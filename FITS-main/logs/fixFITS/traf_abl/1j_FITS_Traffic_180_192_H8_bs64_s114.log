Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j192_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j192_H8_FITS_custom_ftM_sl180_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11909
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=74, out_features=152, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1241059328.0
params:  11400.0
Trainable parameters:  11400
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 103.42737007141113
Epoch: 1, Steps: 93 | Train Loss: 0.8905512 Vali Loss: 0.7316654 Test Loss: 0.8860003
Validation loss decreased (inf --> 0.731665).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 101.39861249923706
Epoch: 2, Steps: 93 | Train Loss: 0.4796879 Vali Loss: 0.5378987 Test Loss: 0.6610402
Validation loss decreased (0.731665 --> 0.537899).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 102.46688652038574
Epoch: 3, Steps: 93 | Train Loss: 0.3856134 Vali Loss: 0.4716615 Test Loss: 0.5809214
Validation loss decreased (0.537899 --> 0.471661).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 73.54641151428223
Epoch: 4, Steps: 93 | Train Loss: 0.3447393 Vali Loss: 0.4355961 Test Loss: 0.5371237
Validation loss decreased (0.471661 --> 0.435596).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 73.54359889030457
Epoch: 5, Steps: 93 | Train Loss: 0.3211351 Vali Loss: 0.4140365 Test Loss: 0.5107019
Validation loss decreased (0.435596 --> 0.414037).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 82.65897846221924
Epoch: 6, Steps: 93 | Train Loss: 0.3065593 Vali Loss: 0.4003601 Test Loss: 0.4944106
Validation loss decreased (0.414037 --> 0.400360).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 73.53001761436462
Epoch: 7, Steps: 93 | Train Loss: 0.2973813 Vali Loss: 0.3915104 Test Loss: 0.4842577
Validation loss decreased (0.400360 --> 0.391510).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 66.53733038902283
Epoch: 8, Steps: 93 | Train Loss: 0.2914562 Vali Loss: 0.3863132 Test Loss: 0.4777207
Validation loss decreased (0.391510 --> 0.386313).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 65.56237244606018
Epoch: 9, Steps: 93 | Train Loss: 0.2875864 Vali Loss: 0.3823386 Test Loss: 0.4736045
Validation loss decreased (0.386313 --> 0.382339).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 69.38413119316101
Epoch: 10, Steps: 93 | Train Loss: 0.2850354 Vali Loss: 0.3798202 Test Loss: 0.4709852
Validation loss decreased (0.382339 --> 0.379820).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 69.84573864936829
Epoch: 11, Steps: 93 | Train Loss: 0.2832990 Vali Loss: 0.3783856 Test Loss: 0.4690941
Validation loss decreased (0.379820 --> 0.378386).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 61.81563997268677
Epoch: 12, Steps: 93 | Train Loss: 0.2821240 Vali Loss: 0.3770460 Test Loss: 0.4679167
Validation loss decreased (0.378386 --> 0.377046).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 62.42429041862488
Epoch: 13, Steps: 93 | Train Loss: 0.2812857 Vali Loss: 0.3764001 Test Loss: 0.4670228
Validation loss decreased (0.377046 --> 0.376400).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 65.3564031124115
Epoch: 14, Steps: 93 | Train Loss: 0.2806790 Vali Loss: 0.3757476 Test Loss: 0.4664968
Validation loss decreased (0.376400 --> 0.375748).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 84.63765501976013
Epoch: 15, Steps: 93 | Train Loss: 0.2802774 Vali Loss: 0.3748333 Test Loss: 0.4660419
Validation loss decreased (0.375748 --> 0.374833).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 63.418442726135254
Epoch: 16, Steps: 93 | Train Loss: 0.2799167 Vali Loss: 0.3746807 Test Loss: 0.4657333
Validation loss decreased (0.374833 --> 0.374681).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 80.48340034484863
Epoch: 17, Steps: 93 | Train Loss: 0.2796759 Vali Loss: 0.3739056 Test Loss: 0.4654824
Validation loss decreased (0.374681 --> 0.373906).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 77.49384617805481
Epoch: 18, Steps: 93 | Train Loss: 0.2795062 Vali Loss: 0.3739863 Test Loss: 0.4653043
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 62.298712730407715
Epoch: 19, Steps: 93 | Train Loss: 0.2793514 Vali Loss: 0.3742056 Test Loss: 0.4651378
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 63.83256196975708
Epoch: 20, Steps: 93 | Train Loss: 0.2792284 Vali Loss: 0.3739591 Test Loss: 0.4650402
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 69.01693177223206
Epoch: 21, Steps: 93 | Train Loss: 0.2791143 Vali Loss: 0.3738073 Test Loss: 0.4649308
Validation loss decreased (0.373906 --> 0.373807).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 68.06996297836304
Epoch: 22, Steps: 93 | Train Loss: 0.2790426 Vali Loss: 0.3736573 Test Loss: 0.4648352
Validation loss decreased (0.373807 --> 0.373657).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 64.27412843704224
Epoch: 23, Steps: 93 | Train Loss: 0.2789596 Vali Loss: 0.3737387 Test Loss: 0.4647450
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 66.91275978088379
Epoch: 24, Steps: 93 | Train Loss: 0.2788903 Vali Loss: 0.3741078 Test Loss: 0.4646251
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 65.00683403015137
Epoch: 25, Steps: 93 | Train Loss: 0.2788263 Vali Loss: 0.3734371 Test Loss: 0.4645987
Validation loss decreased (0.373657 --> 0.373437).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 65.18350005149841
Epoch: 26, Steps: 93 | Train Loss: 0.2787840 Vali Loss: 0.3736173 Test Loss: 0.4645214
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 71.93672704696655
Epoch: 27, Steps: 93 | Train Loss: 0.2787398 Vali Loss: 0.3731670 Test Loss: 0.4645038
Validation loss decreased (0.373437 --> 0.373167).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 64.65367865562439
Epoch: 28, Steps: 93 | Train Loss: 0.2786911 Vali Loss: 0.3734536 Test Loss: 0.4643970
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 69.4885802268982
Epoch: 29, Steps: 93 | Train Loss: 0.2786620 Vali Loss: 0.3734540 Test Loss: 0.4643763
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 65.7189838886261
Epoch: 30, Steps: 93 | Train Loss: 0.2786331 Vali Loss: 0.3732578 Test Loss: 0.4643440
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 65.88217258453369
Epoch: 31, Steps: 93 | Train Loss: 0.2786078 Vali Loss: 0.3733815 Test Loss: 0.4643147
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 66.79258370399475
Epoch: 32, Steps: 93 | Train Loss: 0.2785420 Vali Loss: 0.3732690 Test Loss: 0.4642984
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 62.05372977256775
Epoch: 33, Steps: 93 | Train Loss: 0.2785412 Vali Loss: 0.3735944 Test Loss: 0.4642281
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 61.37623643875122
Epoch: 34, Steps: 93 | Train Loss: 0.2785129 Vali Loss: 0.3730905 Test Loss: 0.4642213
Validation loss decreased (0.373167 --> 0.373091).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 62.829718828201294
Epoch: 35, Steps: 93 | Train Loss: 0.2784747 Vali Loss: 0.3731630 Test Loss: 0.4642024
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 61.32015109062195
Epoch: 36, Steps: 93 | Train Loss: 0.2784416 Vali Loss: 0.3730733 Test Loss: 0.4641825
Validation loss decreased (0.373091 --> 0.373073).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 63.01417517662048
Epoch: 37, Steps: 93 | Train Loss: 0.2784783 Vali Loss: 0.3732122 Test Loss: 0.4641601
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 57.557997941970825
Epoch: 38, Steps: 93 | Train Loss: 0.2784213 Vali Loss: 0.3729420 Test Loss: 0.4641271
Validation loss decreased (0.373073 --> 0.372942).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 61.48081827163696
Epoch: 39, Steps: 93 | Train Loss: 0.2784172 Vali Loss: 0.3732976 Test Loss: 0.4641223
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 60.42005944252014
Epoch: 40, Steps: 93 | Train Loss: 0.2784127 Vali Loss: 0.3732758 Test Loss: 0.4640694
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 58.2526912689209
Epoch: 41, Steps: 93 | Train Loss: 0.2783666 Vali Loss: 0.3730297 Test Loss: 0.4640681
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 58.69958543777466
Epoch: 42, Steps: 93 | Train Loss: 0.2784033 Vali Loss: 0.3731386 Test Loss: 0.4640525
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 63.94657015800476
Epoch: 43, Steps: 93 | Train Loss: 0.2783604 Vali Loss: 0.3733171 Test Loss: 0.4640095
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 49.20227670669556
Epoch: 44, Steps: 93 | Train Loss: 0.2783354 Vali Loss: 0.3734693 Test Loss: 0.4640314
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 50.43307900428772
Epoch: 45, Steps: 93 | Train Loss: 0.2783400 Vali Loss: 0.3732443 Test Loss: 0.4640338
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 52.878060817718506
Epoch: 46, Steps: 93 | Train Loss: 0.2783374 Vali Loss: 0.3726058 Test Loss: 0.4639702
Validation loss decreased (0.372942 --> 0.372606).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 51.06340551376343
Epoch: 47, Steps: 93 | Train Loss: 0.2783057 Vali Loss: 0.3729397 Test Loss: 0.4639764
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 49.563682317733765
Epoch: 48, Steps: 93 | Train Loss: 0.2782984 Vali Loss: 0.3728634 Test Loss: 0.4639614
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 47.02691841125488
Epoch: 49, Steps: 93 | Train Loss: 0.2782960 Vali Loss: 0.3740605 Test Loss: 0.4639333
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 47.69489669799805
Epoch: 50, Steps: 93 | Train Loss: 0.2782699 Vali Loss: 0.3729924 Test Loss: 0.4639561
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 43.1058623790741
Epoch: 51, Steps: 93 | Train Loss: 0.2782700 Vali Loss: 0.3729399 Test Loss: 0.4639433
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 48.00798439979553
Epoch: 52, Steps: 93 | Train Loss: 0.2782558 Vali Loss: 0.3729230 Test Loss: 0.4639264
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 43.189157247543335
Epoch: 53, Steps: 93 | Train Loss: 0.2782488 Vali Loss: 0.3731193 Test Loss: 0.4639254
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 41.660236120224
Epoch: 54, Steps: 93 | Train Loss: 0.2782397 Vali Loss: 0.3735366 Test Loss: 0.4639123
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 40.099703788757324
Epoch: 55, Steps: 93 | Train Loss: 0.2782704 Vali Loss: 0.3730482 Test Loss: 0.4639220
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 39.84918808937073
Epoch: 56, Steps: 93 | Train Loss: 0.2782475 Vali Loss: 0.3731470 Test Loss: 0.4639060
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j192_H8_FITS_custom_ftM_sl180_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.46311938762664795, mae:0.30308687686920166, rse:0.5616626143455505, corr:[0.27595738 0.288146   0.2873781  0.28607553 0.28787917 0.2863524
 0.2882698  0.28764457 0.28766677 0.28817752 0.28759864 0.28772837
 0.28742063 0.2866017  0.28660914 0.286042   0.28597906 0.28602713
 0.28536737 0.28586426 0.28609866 0.28664428 0.28765753 0.28729513
 0.28725263 0.28705162 0.2863497  0.28644294 0.28655753 0.286582
 0.287022   0.28672612 0.28673866 0.28674585 0.28649813 0.28634474
 0.2859266  0.28535303 0.2853614  0.28562877 0.28555527 0.28559425
 0.28538668 0.28553748 0.28559595 0.28570935 0.28613007 0.2859144
 0.28580266 0.28569338 0.28553355 0.2856798  0.28571078 0.28547987
 0.285709   0.28559548 0.2855059  0.28546774 0.2852529  0.28523034
 0.28513956 0.28479713 0.2846093  0.28474352 0.28451195 0.28448623
 0.28453052 0.28454828 0.28459293 0.28466794 0.2849843  0.28496465
 0.2846818  0.28440803 0.28419378 0.2841236  0.28422612 0.28418463
 0.28426987 0.28417644 0.28399882 0.28409597 0.28394723 0.2838656
 0.28390914 0.28393215 0.2839957  0.28414252 0.28408134 0.2838623
 0.2836935  0.28368852 0.28386593 0.28387046 0.28387007 0.28384703
 0.28387398 0.28388512 0.2837942  0.28376088 0.28375715 0.28360876
 0.28340918 0.28321132 0.28292763 0.2829832  0.2829704  0.28298947
 0.28322554 0.28310576 0.28295302 0.28313175 0.28316838 0.2832756
 0.2832829  0.28315228 0.2831538  0.2829012  0.283029   0.28330183
 0.2834074  0.28349236 0.28351307 0.28339854 0.28348187 0.28346533
 0.28298923 0.28278273 0.28266916 0.28272495 0.28302377 0.2832888
 0.28390768 0.2842869  0.2839298  0.28389487 0.28384972 0.28372988
 0.28376788 0.28375372 0.28392532 0.28424188 0.28483063 0.2848172
 0.2845085  0.28432292 0.28416663 0.28430888 0.28455797 0.28449813
 0.2839599  0.28367475 0.28392693 0.28426453 0.28487882 0.28553244
 0.28667393 0.28743365 0.2868636  0.28648558 0.28606516 0.2859186
 0.2857497  0.28579992 0.28632888 0.28713143 0.2883529  0.28809464
 0.28702885 0.286416   0.2859368  0.2855427  0.28567564 0.28573674
 0.28601503 0.28597227 0.28614208 0.28614178 0.2864146  0.28677288
 0.28674757 0.28678665 0.28660637 0.286115   0.2859124  0.2857029
 0.2843468  0.28472137 0.28401306 0.28456578 0.28653812 0.28672168]
