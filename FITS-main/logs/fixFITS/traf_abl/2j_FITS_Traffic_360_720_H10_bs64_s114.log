Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=170, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j720_H10_FITS_custom_ftM_sl360_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11201
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=170, out_features=510, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9566131200.0
params:  87210.0
Trainable parameters:  87210
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 133.67105746269226
Epoch: 1, Steps: 87 | Train Loss: 1.3509883 Vali Loss: 1.2412968 Test Loss: 1.4696667
Validation loss decreased (inf --> 1.241297).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 140.80139446258545
Epoch: 2, Steps: 87 | Train Loss: 0.8475288 Vali Loss: 1.0167916 Test Loss: 1.1924809
Validation loss decreased (1.241297 --> 1.016792).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 131.63825464248657
Epoch: 3, Steps: 87 | Train Loss: 0.7190975 Vali Loss: 0.9388410 Test Loss: 1.0971334
Validation loss decreased (1.016792 --> 0.938841).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 119.8754210472107
Epoch: 4, Steps: 87 | Train Loss: 0.6551192 Vali Loss: 0.8840687 Test Loss: 1.0306849
Validation loss decreased (0.938841 --> 0.884069).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 119.00156903266907
Epoch: 5, Steps: 87 | Train Loss: 0.6064009 Vali Loss: 0.8374029 Test Loss: 0.9745548
Validation loss decreased (0.884069 --> 0.837403).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 157.0865819454193
Epoch: 6, Steps: 87 | Train Loss: 0.5658154 Vali Loss: 0.7968502 Test Loss: 0.9261963
Validation loss decreased (0.837403 --> 0.796850).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 131.5979723930359
Epoch: 7, Steps: 87 | Train Loss: 0.5311762 Vali Loss: 0.7630195 Test Loss: 0.8851068
Validation loss decreased (0.796850 --> 0.763020).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 122.37766814231873
Epoch: 8, Steps: 87 | Train Loss: 0.5012617 Vali Loss: 0.7312257 Test Loss: 0.8480626
Validation loss decreased (0.763020 --> 0.731226).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 125.76221489906311
Epoch: 9, Steps: 87 | Train Loss: 0.4751153 Vali Loss: 0.7049724 Test Loss: 0.8165098
Validation loss decreased (0.731226 --> 0.704972).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 121.7630627155304
Epoch: 10, Steps: 87 | Train Loss: 0.4521320 Vali Loss: 0.6796983 Test Loss: 0.7873513
Validation loss decreased (0.704972 --> 0.679698).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 124.873051404953
Epoch: 11, Steps: 87 | Train Loss: 0.4317338 Vali Loss: 0.6580877 Test Loss: 0.7615476
Validation loss decreased (0.679698 --> 0.658088).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 107.71623682975769
Epoch: 12, Steps: 87 | Train Loss: 0.4136181 Vali Loss: 0.6390369 Test Loss: 0.7392349
Validation loss decreased (0.658088 --> 0.639037).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 109.37323760986328
Epoch: 13, Steps: 87 | Train Loss: 0.3973892 Vali Loss: 0.6218088 Test Loss: 0.7189087
Validation loss decreased (0.639037 --> 0.621809).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 104.44132900238037
Epoch: 14, Steps: 87 | Train Loss: 0.3828534 Vali Loss: 0.6058931 Test Loss: 0.7007234
Validation loss decreased (0.621809 --> 0.605893).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 109.72591495513916
Epoch: 15, Steps: 87 | Train Loss: 0.3697780 Vali Loss: 0.5913734 Test Loss: 0.6839184
Validation loss decreased (0.605893 --> 0.591373).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 108.1040449142456
Epoch: 16, Steps: 87 | Train Loss: 0.3579177 Vali Loss: 0.5785216 Test Loss: 0.6692328
Validation loss decreased (0.591373 --> 0.578522).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 110.07507562637329
Epoch: 17, Steps: 87 | Train Loss: 0.3472097 Vali Loss: 0.5675845 Test Loss: 0.6561183
Validation loss decreased (0.578522 --> 0.567585).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 107.33974623680115
Epoch: 18, Steps: 87 | Train Loss: 0.3374589 Vali Loss: 0.5565513 Test Loss: 0.6435961
Validation loss decreased (0.567585 --> 0.556551).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 142.48578524589539
Epoch: 19, Steps: 87 | Train Loss: 0.3286145 Vali Loss: 0.5469322 Test Loss: 0.6322384
Validation loss decreased (0.556551 --> 0.546932).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 166.59769415855408
Epoch: 20, Steps: 87 | Train Loss: 0.3205879 Vali Loss: 0.5382628 Test Loss: 0.6220950
Validation loss decreased (0.546932 --> 0.538263).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 193.25248098373413
Epoch: 21, Steps: 87 | Train Loss: 0.3131844 Vali Loss: 0.5305043 Test Loss: 0.6130760
Validation loss decreased (0.538263 --> 0.530504).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 151.59447503089905
Epoch: 22, Steps: 87 | Train Loss: 0.3064322 Vali Loss: 0.5228753 Test Loss: 0.6048574
Validation loss decreased (0.530504 --> 0.522875).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 155.28752899169922
Epoch: 23, Steps: 87 | Train Loss: 0.3002415 Vali Loss: 0.5169500 Test Loss: 0.5970832
Validation loss decreased (0.522875 --> 0.516950).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 155.39729952812195
Epoch: 24, Steps: 87 | Train Loss: 0.2945713 Vali Loss: 0.5099848 Test Loss: 0.5899829
Validation loss decreased (0.516950 --> 0.509985).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 151.71853852272034
Epoch: 25, Steps: 87 | Train Loss: 0.2893021 Vali Loss: 0.5048093 Test Loss: 0.5834534
Validation loss decreased (0.509985 --> 0.504809).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 146.078143119812
Epoch: 26, Steps: 87 | Train Loss: 0.2844557 Vali Loss: 0.4996125 Test Loss: 0.5774330
Validation loss decreased (0.504809 --> 0.499613).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 144.27173376083374
Epoch: 27, Steps: 87 | Train Loss: 0.2800077 Vali Loss: 0.4944000 Test Loss: 0.5717835
Validation loss decreased (0.499613 --> 0.494400).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 152.97124075889587
Epoch: 28, Steps: 87 | Train Loss: 0.2758908 Vali Loss: 0.4897605 Test Loss: 0.5664827
Validation loss decreased (0.494400 --> 0.489760).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 174.56575226783752
Epoch: 29, Steps: 87 | Train Loss: 0.2720589 Vali Loss: 0.4858857 Test Loss: 0.5620633
Validation loss decreased (0.489760 --> 0.485886).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 155.20329976081848
Epoch: 30, Steps: 87 | Train Loss: 0.2685245 Vali Loss: 0.4815784 Test Loss: 0.5574114
Validation loss decreased (0.485886 --> 0.481578).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 138.90917015075684
Epoch: 31, Steps: 87 | Train Loss: 0.2652224 Vali Loss: 0.4787242 Test Loss: 0.5535205
Validation loss decreased (0.481578 --> 0.478724).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 138.73387718200684
Epoch: 32, Steps: 87 | Train Loss: 0.2622001 Vali Loss: 0.4752272 Test Loss: 0.5496694
Validation loss decreased (0.478724 --> 0.475227).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 165.18302202224731
Epoch: 33, Steps: 87 | Train Loss: 0.2593353 Vali Loss: 0.4724502 Test Loss: 0.5461053
Validation loss decreased (0.475227 --> 0.472450).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 168.37030005455017
Epoch: 34, Steps: 87 | Train Loss: 0.2566978 Vali Loss: 0.4692437 Test Loss: 0.5427738
Validation loss decreased (0.472450 --> 0.469244).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 179.35628414154053
Epoch: 35, Steps: 87 | Train Loss: 0.2542821 Vali Loss: 0.4663421 Test Loss: 0.5398029
Validation loss decreased (0.469244 --> 0.466342).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 230.89550471305847
Epoch: 36, Steps: 87 | Train Loss: 0.2519384 Vali Loss: 0.4641742 Test Loss: 0.5370448
Validation loss decreased (0.466342 --> 0.464174).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 207.17361545562744
Epoch: 37, Steps: 87 | Train Loss: 0.2497689 Vali Loss: 0.4615134 Test Loss: 0.5342854
Validation loss decreased (0.464174 --> 0.461513).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 210.76264929771423
Epoch: 38, Steps: 87 | Train Loss: 0.2477530 Vali Loss: 0.4596168 Test Loss: 0.5318677
Validation loss decreased (0.461513 --> 0.459617).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 216.8741273880005
Epoch: 39, Steps: 87 | Train Loss: 0.2458505 Vali Loss: 0.4571585 Test Loss: 0.5294117
Validation loss decreased (0.459617 --> 0.457159).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 207.6118609905243
Epoch: 40, Steps: 87 | Train Loss: 0.2441254 Vali Loss: 0.4554607 Test Loss: 0.5272724
Validation loss decreased (0.457159 --> 0.455461).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 210.56110644340515
Epoch: 41, Steps: 87 | Train Loss: 0.2424780 Vali Loss: 0.4543163 Test Loss: 0.5252494
Validation loss decreased (0.455461 --> 0.454316).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 208.8684139251709
Epoch: 42, Steps: 87 | Train Loss: 0.2409019 Vali Loss: 0.4522621 Test Loss: 0.5233682
Validation loss decreased (0.454316 --> 0.452262).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 205.58436703681946
Epoch: 43, Steps: 87 | Train Loss: 0.2393361 Vali Loss: 0.4505724 Test Loss: 0.5215690
Validation loss decreased (0.452262 --> 0.450572).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 211.02560997009277
Epoch: 44, Steps: 87 | Train Loss: 0.2380378 Vali Loss: 0.4490699 Test Loss: 0.5198498
Validation loss decreased (0.450572 --> 0.449070).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 210.44928455352783
Epoch: 45, Steps: 87 | Train Loss: 0.2367538 Vali Loss: 0.4475946 Test Loss: 0.5183423
Validation loss decreased (0.449070 --> 0.447595).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 211.1988663673401
Epoch: 46, Steps: 87 | Train Loss: 0.2355650 Vali Loss: 0.4463784 Test Loss: 0.5167326
Validation loss decreased (0.447595 --> 0.446378).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 209.8723921775818
Epoch: 47, Steps: 87 | Train Loss: 0.2343872 Vali Loss: 0.4456133 Test Loss: 0.5154654
Validation loss decreased (0.446378 --> 0.445613).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 216.06343340873718
Epoch: 48, Steps: 87 | Train Loss: 0.2333031 Vali Loss: 0.4440472 Test Loss: 0.5141071
Validation loss decreased (0.445613 --> 0.444047).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 213.09894943237305
Epoch: 49, Steps: 87 | Train Loss: 0.2322670 Vali Loss: 0.4427381 Test Loss: 0.5128002
Validation loss decreased (0.444047 --> 0.442738).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 207.44649004936218
Epoch: 50, Steps: 87 | Train Loss: 0.2313155 Vali Loss: 0.4420469 Test Loss: 0.5116684
Validation loss decreased (0.442738 --> 0.442047).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 206.9402952194214
Epoch: 51, Steps: 87 | Train Loss: 0.2304139 Vali Loss: 0.4406455 Test Loss: 0.5105428
Validation loss decreased (0.442047 --> 0.440646).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 197.64972114562988
Epoch: 52, Steps: 87 | Train Loss: 0.2295472 Vali Loss: 0.4398767 Test Loss: 0.5095391
Validation loss decreased (0.440646 --> 0.439877).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 188.92231369018555
Epoch: 53, Steps: 87 | Train Loss: 0.2287319 Vali Loss: 0.4396511 Test Loss: 0.5084957
Validation loss decreased (0.439877 --> 0.439651).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 257.9097566604614
Epoch: 54, Steps: 87 | Train Loss: 0.2280167 Vali Loss: 0.4383546 Test Loss: 0.5075738
Validation loss decreased (0.439651 --> 0.438355).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 259.74073791503906
Epoch: 55, Steps: 87 | Train Loss: 0.2272666 Vali Loss: 0.4376630 Test Loss: 0.5067108
Validation loss decreased (0.438355 --> 0.437663).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 220.60973048210144
Epoch: 56, Steps: 87 | Train Loss: 0.2265843 Vali Loss: 0.4368139 Test Loss: 0.5058671
Validation loss decreased (0.437663 --> 0.436814).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 222.69403409957886
Epoch: 57, Steps: 87 | Train Loss: 0.2259278 Vali Loss: 0.4361760 Test Loss: 0.5050702
Validation loss decreased (0.436814 --> 0.436176).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 219.75485157966614
Epoch: 58, Steps: 87 | Train Loss: 0.2253294 Vali Loss: 0.4356017 Test Loss: 0.5043005
Validation loss decreased (0.436176 --> 0.435602).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 225.9473385810852
Epoch: 59, Steps: 87 | Train Loss: 0.2247792 Vali Loss: 0.4349542 Test Loss: 0.5036384
Validation loss decreased (0.435602 --> 0.434954).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 226.78912734985352
Epoch: 60, Steps: 87 | Train Loss: 0.2242585 Vali Loss: 0.4342059 Test Loss: 0.5029399
Validation loss decreased (0.434954 --> 0.434206).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 193.33483505249023
Epoch: 61, Steps: 87 | Train Loss: 0.2236674 Vali Loss: 0.4338969 Test Loss: 0.5023182
Validation loss decreased (0.434206 --> 0.433897).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 261.28550457954407
Epoch: 62, Steps: 87 | Train Loss: 0.2231676 Vali Loss: 0.4332912 Test Loss: 0.5017086
Validation loss decreased (0.433897 --> 0.433291).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 341.94718074798584
Epoch: 63, Steps: 87 | Train Loss: 0.2227136 Vali Loss: 0.4325827 Test Loss: 0.5011801
Validation loss decreased (0.433291 --> 0.432583).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 267.3642122745514
Epoch: 64, Steps: 87 | Train Loss: 0.2222637 Vali Loss: 0.4319583 Test Loss: 0.5006086
Validation loss decreased (0.432583 --> 0.431958).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 194.49761867523193
Epoch: 65, Steps: 87 | Train Loss: 0.2218160 Vali Loss: 0.4316862 Test Loss: 0.5000904
Validation loss decreased (0.431958 --> 0.431686).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 202.89883613586426
Epoch: 66, Steps: 87 | Train Loss: 0.2214349 Vali Loss: 0.4311088 Test Loss: 0.4996155
Validation loss decreased (0.431686 --> 0.431109).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 201.1082558631897
Epoch: 67, Steps: 87 | Train Loss: 0.2210785 Vali Loss: 0.4310169 Test Loss: 0.4991468
Validation loss decreased (0.431109 --> 0.431017).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 214.79531121253967
Epoch: 68, Steps: 87 | Train Loss: 0.2207389 Vali Loss: 0.4304569 Test Loss: 0.4987136
Validation loss decreased (0.431017 --> 0.430457).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 179.63962078094482
Epoch: 69, Steps: 87 | Train Loss: 0.2203608 Vali Loss: 0.4300728 Test Loss: 0.4983017
Validation loss decreased (0.430457 --> 0.430073).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 185.6365511417389
Epoch: 70, Steps: 87 | Train Loss: 0.2200218 Vali Loss: 0.4297984 Test Loss: 0.4979137
Validation loss decreased (0.430073 --> 0.429798).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 190.9459958076477
Epoch: 71, Steps: 87 | Train Loss: 0.2197681 Vali Loss: 0.4287456 Test Loss: 0.4975597
Validation loss decreased (0.429798 --> 0.428746).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 219.88514018058777
Epoch: 72, Steps: 87 | Train Loss: 0.2194921 Vali Loss: 0.4288319 Test Loss: 0.4971932
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 187.98499464988708
Epoch: 73, Steps: 87 | Train Loss: 0.2191736 Vali Loss: 0.4285563 Test Loss: 0.4968542
Validation loss decreased (0.428746 --> 0.428556).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 218.5980441570282
Epoch: 74, Steps: 87 | Train Loss: 0.2189355 Vali Loss: 0.4283013 Test Loss: 0.4965336
Validation loss decreased (0.428556 --> 0.428301).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 193.34915018081665
Epoch: 75, Steps: 87 | Train Loss: 0.2187062 Vali Loss: 0.4281794 Test Loss: 0.4962404
Validation loss decreased (0.428301 --> 0.428179).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 180.57320070266724
Epoch: 76, Steps: 87 | Train Loss: 0.2184089 Vali Loss: 0.4281774 Test Loss: 0.4959413
Validation loss decreased (0.428179 --> 0.428177).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 184.16186833381653
Epoch: 77, Steps: 87 | Train Loss: 0.2181959 Vali Loss: 0.4274895 Test Loss: 0.4956753
Validation loss decreased (0.428177 --> 0.427489).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 188.2746284008026
Epoch: 78, Steps: 87 | Train Loss: 0.2179848 Vali Loss: 0.4272724 Test Loss: 0.4954058
Validation loss decreased (0.427489 --> 0.427272).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 178.95424127578735
Epoch: 79, Steps: 87 | Train Loss: 0.2177515 Vali Loss: 0.4273683 Test Loss: 0.4951832
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 218.80445790290833
Epoch: 80, Steps: 87 | Train Loss: 0.2175896 Vali Loss: 0.4272140 Test Loss: 0.4949410
Validation loss decreased (0.427272 --> 0.427214).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 143.12580847740173
Epoch: 81, Steps: 87 | Train Loss: 0.2173329 Vali Loss: 0.4268048 Test Loss: 0.4947191
Validation loss decreased (0.427214 --> 0.426805).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 143.9523046016693
Epoch: 82, Steps: 87 | Train Loss: 0.2172436 Vali Loss: 0.4266689 Test Loss: 0.4945191
Validation loss decreased (0.426805 --> 0.426669).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 152.53137850761414
Epoch: 83, Steps: 87 | Train Loss: 0.2170580 Vali Loss: 0.4265524 Test Loss: 0.4943040
Validation loss decreased (0.426669 --> 0.426552).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 182.7582814693451
Epoch: 84, Steps: 87 | Train Loss: 0.2168927 Vali Loss: 0.4263874 Test Loss: 0.4941139
Validation loss decreased (0.426552 --> 0.426387).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 169.04125905036926
Epoch: 85, Steps: 87 | Train Loss: 0.2167323 Vali Loss: 0.4260526 Test Loss: 0.4939367
Validation loss decreased (0.426387 --> 0.426053).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 168.93702268600464
Epoch: 86, Steps: 87 | Train Loss: 0.2165871 Vali Loss: 0.4262385 Test Loss: 0.4937661
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 176.33402585983276
Epoch: 87, Steps: 87 | Train Loss: 0.2164277 Vali Loss: 0.4259702 Test Loss: 0.4935988
Validation loss decreased (0.426053 --> 0.425970).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 177.034423828125
Epoch: 88, Steps: 87 | Train Loss: 0.2163600 Vali Loss: 0.4261924 Test Loss: 0.4934454
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 180.47415947914124
Epoch: 89, Steps: 87 | Train Loss: 0.2162415 Vali Loss: 0.4253553 Test Loss: 0.4933059
Validation loss decreased (0.425970 --> 0.425355).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 174.56295728683472
Epoch: 90, Steps: 87 | Train Loss: 0.2161099 Vali Loss: 0.4257050 Test Loss: 0.4931540
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 159.85852694511414
Epoch: 91, Steps: 87 | Train Loss: 0.2159660 Vali Loss: 0.4255585 Test Loss: 0.4930311
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 175.91967725753784
Epoch: 92, Steps: 87 | Train Loss: 0.2158988 Vali Loss: 0.4253820 Test Loss: 0.4929022
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 174.59380388259888
Epoch: 93, Steps: 87 | Train Loss: 0.2157565 Vali Loss: 0.4250585 Test Loss: 0.4927849
Validation loss decreased (0.425355 --> 0.425059).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 155.02876901626587
Epoch: 94, Steps: 87 | Train Loss: 0.2157087 Vali Loss: 0.4248682 Test Loss: 0.4926725
Validation loss decreased (0.425059 --> 0.424868).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 178.9230592250824
Epoch: 95, Steps: 87 | Train Loss: 0.2156307 Vali Loss: 0.4243779 Test Loss: 0.4925548
Validation loss decreased (0.424868 --> 0.424378).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 206.1100766658783
Epoch: 96, Steps: 87 | Train Loss: 0.2155820 Vali Loss: 0.4247218 Test Loss: 0.4924611
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 218.7722589969635
Epoch: 97, Steps: 87 | Train Loss: 0.2154423 Vali Loss: 0.4248380 Test Loss: 0.4923608
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 227.83966422080994
Epoch: 98, Steps: 87 | Train Loss: 0.2153565 Vali Loss: 0.4251696 Test Loss: 0.4922703
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 220.03713011741638
Epoch: 99, Steps: 87 | Train Loss: 0.2153122 Vali Loss: 0.4244067 Test Loss: 0.4921763
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 226.65315318107605
Epoch: 100, Steps: 87 | Train Loss: 0.2152397 Vali Loss: 0.4247388 Test Loss: 0.4920973
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11201
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=170, out_features=510, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9566131200.0
params:  87210.0
Trainable parameters:  87210
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 225.08744525909424
Epoch: 1, Steps: 87 | Train Loss: 0.2961532 Vali Loss: 0.4034415 Test Loss: 0.4679945
Validation loss decreased (inf --> 0.403442).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 223.55683946609497
Epoch: 2, Steps: 87 | Train Loss: 0.2857659 Vali Loss: 0.3995079 Test Loss: 0.4642009
Validation loss decreased (0.403442 --> 0.399508).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 215.98048162460327
Epoch: 3, Steps: 87 | Train Loss: 0.2838943 Vali Loss: 0.3986688 Test Loss: 0.4637712
Validation loss decreased (0.399508 --> 0.398669).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 210.5384578704834
Epoch: 4, Steps: 87 | Train Loss: 0.2836849 Vali Loss: 0.3984417 Test Loss: 0.4637848
Validation loss decreased (0.398669 --> 0.398442).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 198.1207766532898
Epoch: 5, Steps: 87 | Train Loss: 0.2835686 Vali Loss: 0.3989117 Test Loss: 0.4635189
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 207.14485597610474
Epoch: 6, Steps: 87 | Train Loss: 0.2835956 Vali Loss: 0.3984709 Test Loss: 0.4636357
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 205.24055075645447
Epoch: 7, Steps: 87 | Train Loss: 0.2835131 Vali Loss: 0.3985966 Test Loss: 0.4635574
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 201.1091651916504
Epoch: 8, Steps: 87 | Train Loss: 0.2835274 Vali Loss: 0.3985724 Test Loss: 0.4638132
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 188.16120028495789
Epoch: 9, Steps: 87 | Train Loss: 0.2835113 Vali Loss: 0.3986514 Test Loss: 0.4632648
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 191.12112402915955
Epoch: 10, Steps: 87 | Train Loss: 0.2835465 Vali Loss: 0.3986545 Test Loss: 0.4638021
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 181.67830848693848
Epoch: 11, Steps: 87 | Train Loss: 0.2834313 Vali Loss: 0.3985581 Test Loss: 0.4633505
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 189.9688262939453
Epoch: 12, Steps: 87 | Train Loss: 0.2834354 Vali Loss: 0.3988186 Test Loss: 0.4634863
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 193.24846482276917
Epoch: 13, Steps: 87 | Train Loss: 0.2834302 Vali Loss: 0.3989738 Test Loss: 0.4634083
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 189.3154377937317
Epoch: 14, Steps: 87 | Train Loss: 0.2834307 Vali Loss: 0.3985067 Test Loss: 0.4633796
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j720_H10_FITS_custom_ftM_sl360_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4630914628505707, mae:0.3067833185195923, rse:0.5564603209495544, corr:[0.25428152 0.2665989  0.2663521  0.26598087 0.26572543 0.26608703
 0.26611027 0.2665096  0.26648518 0.26653692 0.26683712 0.26659644
 0.2665805  0.2659944  0.2662661  0.26651114 0.26679343 0.26621795
 0.26557577 0.26606506 0.26606604 0.26608655 0.26570597 0.26571822
 0.26691514 0.26739573 0.267176   0.26704222 0.26707995 0.26651728
 0.2665448  0.26645732 0.26656553 0.26662478 0.266745   0.26692313
 0.26713043 0.26711273 0.26651993 0.26709387 0.26724684 0.26683617
 0.26628846 0.2662434  0.26657915 0.26604465 0.26629287 0.26659855
 0.26736462 0.26735508 0.26662928 0.26659033 0.2665578  0.26644045
 0.26621616 0.2664836  0.26639476 0.26634684 0.2670854  0.26674587
 0.26672003 0.26693603 0.26671523 0.26637742 0.26609498 0.26577458
 0.26540577 0.26554134 0.26564017 0.26558736 0.26545057 0.26599044
 0.26630628 0.26625332 0.2665914  0.26641703 0.26625183 0.26565558
 0.26528174 0.26585248 0.26653385 0.26661918 0.2657002  0.26571482
 0.26611245 0.26611766 0.2659304  0.26581812 0.26573956 0.26569927
 0.26630035 0.2658413  0.26583964 0.26583165 0.26544082 0.26548582
 0.26514205 0.26535156 0.2654586  0.26569396 0.26591468 0.26521203
 0.26480553 0.26519966 0.26554155 0.26506883 0.26513308 0.2657696
 0.26568365 0.26529276 0.2651116  0.26533514 0.2651034  0.26518905
 0.26560497 0.26549122 0.26543406 0.2652976  0.26537928 0.26521462
 0.26509923 0.2653232  0.2657036  0.26553604 0.26495966 0.26520687
 0.265283   0.26569372 0.2661087  0.26573098 0.2656867  0.26550037
 0.2651035  0.26510385 0.26553333 0.2658739  0.26598167 0.26543376
 0.26536343 0.2653898  0.26496512 0.26478148 0.26497465 0.265847
 0.26601842 0.26599604 0.26640883 0.26643366 0.2662379  0.26588362
 0.26591727 0.26606318 0.2660284  0.2658415  0.2658087  0.26574185
 0.26565146 0.26617414 0.2659558  0.26597747 0.26674527 0.2669677
 0.26719368 0.26716074 0.2669955  0.26660612 0.26658532 0.26768866
 0.26829582 0.26783583 0.26803842 0.26825956 0.2679482  0.2676848
 0.26724264 0.26673117 0.26717165 0.26742604 0.26758206 0.2677659
 0.26754174 0.2676937  0.26807776 0.2683565  0.2686358  0.268776
 0.2682846  0.26812217 0.26811203 0.2677791  0.26804373 0.26768902
 0.26787004 0.26806688 0.26819327 0.26833463 0.267935   0.2676905
 0.2678023  0.2678711  0.2680921  0.26824337 0.26801273 0.2677883
 0.26768187 0.26784438 0.26831895 0.2680653  0.26725292 0.26708022
 0.26750576 0.2675989  0.26764414 0.26770738 0.2672268  0.26731208
 0.26762477 0.2672735  0.26749972 0.26772204 0.26749417 0.2672134
 0.26733074 0.26742467 0.26725733 0.2667529  0.26673934 0.26730636
 0.2671712  0.267419   0.26764354 0.26726475 0.26672515 0.2666623
 0.26723695 0.2669367  0.2672362  0.26782477 0.26730025 0.2674708
 0.26782382 0.26731208 0.26689294 0.2668657  0.26708317 0.26718745
 0.26733828 0.26744083 0.2675662  0.26746103 0.2669606  0.26677132
 0.26736185 0.26762766 0.26723608 0.26703253 0.2666447  0.26674452
 0.26699495 0.26704457 0.26690775 0.2667999  0.26691633 0.26644164
 0.26603293 0.26650047 0.2667882  0.26691017 0.26716328 0.26698807
 0.26660556 0.2666093  0.26679364 0.26701063 0.26680446 0.26705006
 0.26761904 0.26732302 0.26685092 0.26663616 0.26691625 0.26691017
 0.26689163 0.2673222  0.2672626  0.266949   0.26636878 0.26604426
 0.2658101  0.26617554 0.26641697 0.266062   0.26638395 0.266201
 0.26583228 0.26597852 0.2663162  0.26668894 0.26654452 0.26671115
 0.26700383 0.2672778  0.26710787 0.26635367 0.26609886 0.26628
 0.26686612 0.26689768 0.26679754 0.2667972  0.26673913 0.26717028
 0.26761004 0.267666   0.2678361  0.26854178 0.26876324 0.26885605
 0.26906005 0.26855895 0.26805258 0.26777333 0.26763022 0.26747668
 0.26759484 0.26815748 0.2682036  0.26782697 0.26785365 0.26824024
 0.26808348 0.2681121  0.26826847 0.26794863 0.268248   0.26857954
 0.26913688 0.2692755  0.26902124 0.26895872 0.26909897 0.26909947
 0.2691289  0.26918277 0.26860598 0.26826763 0.26817322 0.26843184
 0.26867816 0.26857316 0.2690676  0.2689533  0.26889297 0.26925054
 0.26864663 0.26869828 0.26918724 0.26920366 0.2692941  0.26893052
 0.26933235 0.26993552 0.26976305 0.26918608 0.26919886 0.26916775
 0.26917595 0.2688802  0.26791283 0.26808357 0.2684294  0.26834297
 0.2683334  0.26806876 0.2682778  0.26819623 0.26781842 0.26820827
 0.26806143 0.26782557 0.2682679  0.26782626 0.26762503 0.267932
 0.26805386 0.26816824 0.2679393  0.26760244 0.26778552 0.26780024
 0.26785314 0.26783583 0.26778102 0.26765507 0.26777998 0.2683102
 0.2679062  0.26811707 0.26859486 0.26762196 0.2674227  0.26786962
 0.26788253 0.26772264 0.2674607  0.26730654 0.26738313 0.26770613
 0.26743016 0.26725945 0.26763627 0.2676464  0.26774424 0.26790085
 0.26797473 0.2677477  0.2673928  0.2674548  0.2673909  0.2679238
 0.2681595  0.26782635 0.26779506 0.26734516 0.26741245 0.26787975
 0.26806295 0.2675504  0.26753595 0.2679083  0.2676812  0.26792404
 0.26784417 0.26788962 0.26808706 0.2681416  0.2681176  0.26734817
 0.26703826 0.2674889  0.26808453 0.26825795 0.26769182 0.26787254
 0.2679904  0.26780367 0.26778054 0.26718113 0.26649415 0.26660967
 0.26718825 0.26694903 0.26689416 0.2667529  0.2662951  0.26638997
 0.26650095 0.2666241  0.26721427 0.26776922 0.2677048  0.26748303
 0.26699725 0.26705864 0.26754206 0.26744914 0.26740393 0.2677363
 0.26778686 0.26747397 0.26748806 0.26785192 0.26773158 0.26735905
 0.2674516  0.26765117 0.2675164  0.26759523 0.26810923 0.26824054
 0.26843798 0.26849595 0.26834506 0.268946   0.26943955 0.26911727
 0.26870435 0.26838568 0.26796865 0.26801893 0.26818994 0.26825392
 0.2682386  0.26822662 0.26817995 0.26792574 0.2682589  0.26842082
 0.2683217  0.26868883 0.2683923  0.26846346 0.26862133 0.26809815
 0.26865935 0.26891443 0.26846096 0.26842192 0.26866576 0.26813546
 0.26730025 0.26719764 0.26722103 0.26711455 0.26747915 0.26761088
 0.2673559  0.26742238 0.2678815  0.26809955 0.26807573 0.26818597
 0.26794365 0.26832762 0.26890194 0.26853752 0.26808533 0.26768744
 0.26780576 0.26798904 0.26816496 0.26806474 0.26748937 0.267247
 0.26761454 0.2675023  0.26721346 0.26721328 0.26712245 0.26733166
 0.267219   0.26729888 0.26767635 0.26712278 0.26741454 0.26777387
 0.26705453 0.2674467  0.2680278  0.26742777 0.26708636 0.26723596
 0.26740345 0.26757282 0.26744008 0.26699507 0.26674986 0.26661918
 0.26679787 0.26671422 0.26670304 0.26666903 0.266411   0.2674222
 0.26728928 0.26662034 0.2666998  0.2664951  0.26697552 0.26680973
 0.26660332 0.2665461  0.26663944 0.2670252  0.26672265 0.26648742
 0.26616332 0.2659961  0.26619732 0.26640618 0.26658866 0.26649976
 0.2659636  0.2659061  0.2659667  0.26506495 0.2650696  0.26550865
 0.2652803  0.26531768 0.26561463 0.2659578  0.26564878 0.265896
 0.26617068 0.26564202 0.2655195  0.26550442 0.26572764 0.26563743
 0.26552224 0.26515305 0.26533303 0.26585308 0.26560175 0.2656724
 0.26501733 0.26474664 0.26493654 0.2649624  0.2650509  0.26517597
 0.26538908 0.26469728 0.26495582 0.2652988  0.26470992 0.26543617
 0.26642922 0.26628172 0.26577762 0.26542872 0.26494643 0.26493964
 0.26470688 0.26452848 0.26487914 0.2648975  0.26513854 0.2648818
 0.2646194  0.26458347 0.26476812 0.2651239  0.26499093 0.26558086
 0.26544964 0.26516336 0.2651402  0.26513717 0.26574722 0.26551858
 0.26551598 0.26516804 0.26524478 0.2658775  0.2658356  0.2665239
 0.2667416  0.26691    0.2671095  0.267327   0.26758248 0.26722634
 0.266784   0.26637417 0.26584646 0.26511115 0.26544964 0.2654551
 0.26545075 0.2655277  0.26510617 0.26579884 0.26612002 0.2659323
 0.26593807 0.2665195  0.2668847  0.2665105  0.26657408 0.26686895
 0.26785776 0.26785192 0.26768854 0.26762703 0.26734057 0.26721457
 0.26697487 0.26689982 0.2660496  0.2660845  0.26579413 0.26586586
 0.26652235 0.26579037 0.2661881  0.2663723  0.26717192 0.26728347
 0.26719683 0.26845932 0.2680615  0.2675463  0.26742506 0.26776448
 0.26808357 0.26800516 0.2684511  0.2682773  0.26811886 0.26719922
 0.26786014 0.26775646 0.26746434 0.2672257  0.26640245 0.26690447
 0.26750606 0.26825118 0.26777673 0.26889378 0.26904547 0.2694303
 0.2676862  0.26700532 0.26637107 0.26532573 0.26569247 0.26808718]
