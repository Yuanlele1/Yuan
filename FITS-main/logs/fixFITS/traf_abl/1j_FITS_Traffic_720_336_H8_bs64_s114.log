Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j336_H8_FITS_custom_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10760408064.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 170.3963212966919
Epoch: 1, Steps: 87 | Train Loss: 0.8229635 Vali Loss: 0.7153024 Test Loss: 0.8226689
Validation loss decreased (inf --> 0.715302).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 234.0031065940857
Epoch: 2, Steps: 87 | Train Loss: 0.4656222 Vali Loss: 0.5093636 Test Loss: 0.5900921
Validation loss decreased (0.715302 --> 0.509364).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 194.7096083164215
Epoch: 3, Steps: 87 | Train Loss: 0.3444911 Vali Loss: 0.4173498 Test Loss: 0.4886685
Validation loss decreased (0.509364 --> 0.417350).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 195.03763723373413
Epoch: 4, Steps: 87 | Train Loss: 0.2900331 Vali Loss: 0.3757313 Test Loss: 0.4442694
Validation loss decreased (0.417350 --> 0.375731).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 191.8997461795807
Epoch: 5, Steps: 87 | Train Loss: 0.2662577 Vali Loss: 0.3573276 Test Loss: 0.4263409
Validation loss decreased (0.375731 --> 0.357328).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 187.8277804851532
Epoch: 6, Steps: 87 | Train Loss: 0.2562924 Vali Loss: 0.3493762 Test Loss: 0.4192927
Validation loss decreased (0.357328 --> 0.349376).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 186.33909034729004
Epoch: 7, Steps: 87 | Train Loss: 0.2522675 Vali Loss: 0.3458919 Test Loss: 0.4166195
Validation loss decreased (0.349376 --> 0.345892).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 187.34555649757385
Epoch: 8, Steps: 87 | Train Loss: 0.2504661 Vali Loss: 0.3443155 Test Loss: 0.4156486
Validation loss decreased (0.345892 --> 0.344315).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 174.7867786884308
Epoch: 9, Steps: 87 | Train Loss: 0.2497194 Vali Loss: 0.3436660 Test Loss: 0.4153110
Validation loss decreased (0.344315 --> 0.343666).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 186.55447697639465
Epoch: 10, Steps: 87 | Train Loss: 0.2492595 Vali Loss: 0.3424880 Test Loss: 0.4149088
Validation loss decreased (0.343666 --> 0.342488).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 171.42876863479614
Epoch: 11, Steps: 87 | Train Loss: 0.2490567 Vali Loss: 0.3424913 Test Loss: 0.4150185
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 171.86417770385742
Epoch: 12, Steps: 87 | Train Loss: 0.2488888 Vali Loss: 0.3423738 Test Loss: 0.4147097
Validation loss decreased (0.342488 --> 0.342374).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 242.67973804473877
Epoch: 13, Steps: 87 | Train Loss: 0.2488818 Vali Loss: 0.3420866 Test Loss: 0.4143962
Validation loss decreased (0.342374 --> 0.342087).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 116.4677505493164
Epoch: 14, Steps: 87 | Train Loss: 0.2487515 Vali Loss: 0.3417859 Test Loss: 0.4144495
Validation loss decreased (0.342087 --> 0.341786).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 148.01591444015503
Epoch: 15, Steps: 87 | Train Loss: 0.2486575 Vali Loss: 0.3414437 Test Loss: 0.4145673
Validation loss decreased (0.341786 --> 0.341444).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 118.06036615371704
Epoch: 16, Steps: 87 | Train Loss: 0.2486441 Vali Loss: 0.3412438 Test Loss: 0.4142956
Validation loss decreased (0.341444 --> 0.341244).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 116.11660552024841
Epoch: 17, Steps: 87 | Train Loss: 0.2485752 Vali Loss: 0.3412756 Test Loss: 0.4143159
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 103.97539234161377
Epoch: 18, Steps: 87 | Train Loss: 0.2486008 Vali Loss: 0.3414105 Test Loss: 0.4142938
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 96.4815125465393
Epoch: 19, Steps: 87 | Train Loss: 0.2485481 Vali Loss: 0.3406750 Test Loss: 0.4142239
Validation loss decreased (0.341244 --> 0.340675).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 102.15101099014282
Epoch: 20, Steps: 87 | Train Loss: 0.2483756 Vali Loss: 0.3405883 Test Loss: 0.4140792
Validation loss decreased (0.340675 --> 0.340588).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 100.99055790901184
Epoch: 21, Steps: 87 | Train Loss: 0.2484562 Vali Loss: 0.3409930 Test Loss: 0.4141616
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 106.00542116165161
Epoch: 22, Steps: 87 | Train Loss: 0.2483613 Vali Loss: 0.3407229 Test Loss: 0.4139238
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 88.33869242668152
Epoch: 23, Steps: 87 | Train Loss: 0.2484338 Vali Loss: 0.3407356 Test Loss: 0.4141520
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 74.92210698127747
Epoch: 24, Steps: 87 | Train Loss: 0.2483433 Vali Loss: 0.3406571 Test Loss: 0.4141418
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 77.44662570953369
Epoch: 25, Steps: 87 | Train Loss: 0.2483568 Vali Loss: 0.3405533 Test Loss: 0.4141240
Validation loss decreased (0.340588 --> 0.340553).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 84.80044364929199
Epoch: 26, Steps: 87 | Train Loss: 0.2483624 Vali Loss: 0.3407485 Test Loss: 0.4139732
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 101.1069552898407
Epoch: 27, Steps: 87 | Train Loss: 0.2483039 Vali Loss: 0.3405572 Test Loss: 0.4139747
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 107.50134205818176
Epoch: 28, Steps: 87 | Train Loss: 0.2483120 Vali Loss: 0.3405595 Test Loss: 0.4139241
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 107.9162974357605
Epoch: 29, Steps: 87 | Train Loss: 0.2482386 Vali Loss: 0.3407057 Test Loss: 0.4138140
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 95.71557092666626
Epoch: 30, Steps: 87 | Train Loss: 0.2481824 Vali Loss: 0.3406894 Test Loss: 0.4140407
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 90.39802289009094
Epoch: 31, Steps: 87 | Train Loss: 0.2481746 Vali Loss: 0.3406270 Test Loss: 0.4139031
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 89.62076377868652
Epoch: 32, Steps: 87 | Train Loss: 0.2481731 Vali Loss: 0.3402181 Test Loss: 0.4139937
Validation loss decreased (0.340553 --> 0.340218).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 88.06922817230225
Epoch: 33, Steps: 87 | Train Loss: 0.2482240 Vali Loss: 0.3403710 Test Loss: 0.4139230
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 95.25271773338318
Epoch: 34, Steps: 87 | Train Loss: 0.2481327 Vali Loss: 0.3404043 Test Loss: 0.4138680
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 94.69892764091492
Epoch: 35, Steps: 87 | Train Loss: 0.2481605 Vali Loss: 0.3402477 Test Loss: 0.4138938
EarlyStopping counter: 3 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 98.3019666671753
Epoch: 36, Steps: 87 | Train Loss: 0.2482249 Vali Loss: 0.3406882 Test Loss: 0.4138137
EarlyStopping counter: 4 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 94.65776085853577
Epoch: 37, Steps: 87 | Train Loss: 0.2481039 Vali Loss: 0.3402770 Test Loss: 0.4138341
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 95.01852607727051
Epoch: 38, Steps: 87 | Train Loss: 0.2480968 Vali Loss: 0.3407688 Test Loss: 0.4138058
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 100.04819202423096
Epoch: 39, Steps: 87 | Train Loss: 0.2481456 Vali Loss: 0.3407573 Test Loss: 0.4138279
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 94.97300505638123
Epoch: 40, Steps: 87 | Train Loss: 0.2480535 Vali Loss: 0.3402163 Test Loss: 0.4138521
Validation loss decreased (0.340218 --> 0.340216).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 93.45634722709656
Epoch: 41, Steps: 87 | Train Loss: 0.2480644 Vali Loss: 0.3401562 Test Loss: 0.4137127
Validation loss decreased (0.340216 --> 0.340156).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 96.87008619308472
Epoch: 42, Steps: 87 | Train Loss: 0.2481208 Vali Loss: 0.3404571 Test Loss: 0.4138515
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 99.04653286933899
Epoch: 43, Steps: 87 | Train Loss: 0.2480765 Vali Loss: 0.3407050 Test Loss: 0.4138101
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 95.06053137779236
Epoch: 44, Steps: 87 | Train Loss: 0.2480874 Vali Loss: 0.3398829 Test Loss: 0.4138514
Validation loss decreased (0.340156 --> 0.339883).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 93.15638208389282
Epoch: 45, Steps: 87 | Train Loss: 0.2481016 Vali Loss: 0.3401773 Test Loss: 0.4137429
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 94.00169491767883
Epoch: 46, Steps: 87 | Train Loss: 0.2480678 Vali Loss: 0.3405716 Test Loss: 0.4137585
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 93.73653554916382
Epoch: 47, Steps: 87 | Train Loss: 0.2479980 Vali Loss: 0.3404108 Test Loss: 0.4137529
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 83.96625280380249
Epoch: 48, Steps: 87 | Train Loss: 0.2481191 Vali Loss: 0.3402317 Test Loss: 0.4137691
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 104.06125926971436
Epoch: 49, Steps: 87 | Train Loss: 0.2480563 Vali Loss: 0.3404990 Test Loss: 0.4137103
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 91.34290552139282
Epoch: 50, Steps: 87 | Train Loss: 0.2479749 Vali Loss: 0.3399132 Test Loss: 0.4137215
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 86.60290122032166
Epoch: 51, Steps: 87 | Train Loss: 0.2481397 Vali Loss: 0.3401863 Test Loss: 0.4137247
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 84.04114747047424
Epoch: 52, Steps: 87 | Train Loss: 0.2481104 Vali Loss: 0.3404268 Test Loss: 0.4137728
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 79.92122626304626
Epoch: 53, Steps: 87 | Train Loss: 0.2480503 Vali Loss: 0.3397517 Test Loss: 0.4137450
Validation loss decreased (0.339883 --> 0.339752).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 84.63027453422546
Epoch: 54, Steps: 87 | Train Loss: 0.2480730 Vali Loss: 0.3401939 Test Loss: 0.4137048
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 76.89098238945007
Epoch: 55, Steps: 87 | Train Loss: 0.2480379 Vali Loss: 0.3403807 Test Loss: 0.4137630
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 84.33087873458862
Epoch: 56, Steps: 87 | Train Loss: 0.2479717 Vali Loss: 0.3404993 Test Loss: 0.4137263
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 93.77283644676208
Epoch: 57, Steps: 87 | Train Loss: 0.2479634 Vali Loss: 0.3401878 Test Loss: 0.4137307
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 81.01673555374146
Epoch: 58, Steps: 87 | Train Loss: 0.2479869 Vali Loss: 0.3403714 Test Loss: 0.4137784
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 92.7222466468811
Epoch: 59, Steps: 87 | Train Loss: 0.2480452 Vali Loss: 0.3403706 Test Loss: 0.4137255
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 83.45081353187561
Epoch: 60, Steps: 87 | Train Loss: 0.2480118 Vali Loss: 0.3401332 Test Loss: 0.4137174
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 90.51963877677917
Epoch: 61, Steps: 87 | Train Loss: 0.2480227 Vali Loss: 0.3402140 Test Loss: 0.4137113
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 85.35406446456909
Epoch: 62, Steps: 87 | Train Loss: 0.2480232 Vali Loss: 0.3404014 Test Loss: 0.4136944
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 84.17181491851807
Epoch: 63, Steps: 87 | Train Loss: 0.2479910 Vali Loss: 0.3403810 Test Loss: 0.4136828
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j336_H8_FITS_custom_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.41160833835601807, mae:0.28036728501319885, rse:0.5272800326347351, corr:[0.2741234  0.28290772 0.28462812 0.28367427 0.28503346 0.28555158
 0.28502968 0.2855788  0.28547812 0.28496096 0.28517145 0.2846827
 0.28394744 0.28404477 0.2840198  0.28377944 0.28398165 0.28420606
 0.28418386 0.2842616  0.28432038 0.2842066  0.28406984 0.28430805
 0.28562242 0.28603414 0.28603405 0.2856615  0.28533772 0.28518698
 0.28487435 0.28465474 0.28463614 0.2845132  0.28443745 0.2845772
 0.28459013 0.28450423 0.28454846 0.2846787  0.28485268 0.2849627
 0.2850896  0.28516236 0.28507128 0.28486004 0.2847411  0.28495398
 0.28536832 0.28528383 0.285208   0.28509173 0.28478512 0.28461462
 0.28467554 0.28465152 0.28458464 0.2846466  0.2845859  0.28448457
 0.28460133 0.2847509  0.28494054 0.2851836  0.28519827 0.28496048
 0.28491914 0.28483278 0.2846863  0.284599   0.2844783  0.28444684
 0.2844053  0.28428504 0.28413787 0.28404814 0.28405997 0.28406867
 0.28408802 0.2841984  0.28430352 0.2843236  0.28434014 0.28431565
 0.28425804 0.2842884  0.28440478 0.28448227 0.2845412  0.28450283
 0.2844133  0.28434753 0.28435957 0.28433207 0.28421444 0.2841845
 0.28404984 0.2840187  0.28400064 0.28397363 0.28402877 0.28404042
 0.2839956  0.28399712 0.2840587  0.28407314 0.2840867  0.28422192
 0.28434488 0.28426266 0.28419352 0.28422877 0.28416747 0.28411993
 0.284189   0.28409097 0.28394076 0.28386685 0.28383008 0.2838561
 0.2840059  0.2842995  0.28439295 0.2843798  0.28445593 0.2844691
 0.28444174 0.28444758 0.2844608  0.2844825  0.28456497 0.28463963
 0.28471377 0.2848219  0.284777   0.28466073 0.28475088 0.28478608
 0.284582   0.28443345 0.28434846 0.2842185  0.28420225 0.28435966
 0.28463492 0.28472063 0.28472808 0.28469908 0.28476515 0.28491876
 0.28497478 0.2849596  0.28503898 0.28515166 0.2851687  0.285119
 0.28506303 0.285058   0.2850476  0.28489476 0.28476366 0.28481552
 0.28480983 0.28466806 0.28465474 0.28471226 0.2846071  0.2848078
 0.2858945  0.2859459  0.28584468 0.2858101  0.2856775  0.2856815
 0.2857772  0.28576747 0.28579533 0.28587228 0.28580672 0.28568274
 0.28559598 0.28546965 0.28541476 0.28542075 0.28525448 0.28499374
 0.2849422  0.2849948  0.28484064 0.28471512 0.2847089  0.28481722
 0.2854305  0.2854784  0.2853477  0.28541344 0.28558472 0.28561258
 0.28562006 0.2856701  0.2855488  0.28534517 0.28514895 0.2849616
 0.28492352 0.28495374 0.28480846 0.28459647 0.28456637 0.2844907
 0.28424266 0.28416577 0.28421777 0.28415102 0.28410304 0.284289
 0.28445548 0.2844118  0.28458536 0.28467277 0.28460836 0.28464568
 0.28466088 0.28465125 0.2847145  0.284614   0.2844099  0.2843569
 0.28426713 0.28407022 0.2840536  0.28401563 0.2838644  0.28393736
 0.28402564 0.28379458 0.28376743 0.28400365 0.28406373 0.28406537
 0.28426415 0.28442988 0.28444007 0.28441033 0.28433496 0.28426194
 0.28436524 0.28438044 0.28421775 0.28415346 0.2840253  0.28371143
 0.28358823 0.28356436 0.28341052 0.28340313 0.28351593 0.28344837
 0.28338647 0.28343827 0.28340107 0.2834403  0.2836289  0.2836606
 0.2835273  0.2836826  0.28383175 0.28376412 0.28377482 0.2837703
 0.2836664  0.28366777 0.28359747 0.2834243  0.28340998 0.2832019
 0.28279608 0.28280154 0.2829439  0.28275782 0.28276798 0.28297633
 0.28283146 0.2827852  0.283085   0.2831454  0.28313115 0.2833836
 0.28341597 0.28347126 0.28367835 0.28370973 0.28360498 0.28371054
 0.283688   0.28336734 0.28330708 0.28324237 0.2828489  0.28278086
 0.28293326 0.28276578 0.28286004 0.28318903 0.2831428  0.28328016
 0.28363764 0.2835036  0.28353733 0.28397974 0.28378722 0.28334746
 0.2836042  0.28374135 0.28378934 0.2838784  0.2835828  0.28341842
 0.28363013 0.28330147 0.28288668 0.28312415 0.28304088 0.28294042
 0.2838548  0.28428563 0.28406668 0.284773   0.28483042 0.2840063
 0.28442883 0.2835038  0.2814217  0.2826196  0.28115383 0.2858783 ]
