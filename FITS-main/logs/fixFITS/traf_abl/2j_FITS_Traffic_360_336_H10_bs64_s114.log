Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=170, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j336_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j336_H10_FITS_custom_ftM_sl360_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11585
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=170, out_features=328, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6152335360.0
params:  56088.0
Trainable parameters:  56088
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 106.12133288383484
Epoch: 1, Steps: 90 | Train Loss: 1.2787921 Vali Loss: 1.3224894 Test Loss: 1.5451928
Validation loss decreased (inf --> 1.322489).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 88.49944353103638
Epoch: 2, Steps: 90 | Train Loss: 0.9203577 Vali Loss: 1.1468912 Test Loss: 1.3361019
Validation loss decreased (1.322489 --> 1.146891).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 87.89128112792969
Epoch: 3, Steps: 90 | Train Loss: 0.8066776 Vali Loss: 1.0703019 Test Loss: 1.2458814
Validation loss decreased (1.146891 --> 1.070302).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 90.29831981658936
Epoch: 4, Steps: 90 | Train Loss: 0.7364424 Vali Loss: 1.0070071 Test Loss: 1.1723050
Validation loss decreased (1.070302 --> 1.007007).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 85.19717335700989
Epoch: 5, Steps: 90 | Train Loss: 0.6790958 Vali Loss: 0.9504553 Test Loss: 1.1054769
Validation loss decreased (1.007007 --> 0.950455).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 96.35803008079529
Epoch: 6, Steps: 90 | Train Loss: 0.6299806 Vali Loss: 0.9048666 Test Loss: 1.0523442
Validation loss decreased (0.950455 --> 0.904867).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 89.63739514350891
Epoch: 7, Steps: 90 | Train Loss: 0.5872173 Vali Loss: 0.8603112 Test Loss: 1.0004437
Validation loss decreased (0.904867 --> 0.860311).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 90.13305926322937
Epoch: 8, Steps: 90 | Train Loss: 0.5497279 Vali Loss: 0.8222291 Test Loss: 0.9563708
Validation loss decreased (0.860311 --> 0.822229).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 88.24350762367249
Epoch: 9, Steps: 90 | Train Loss: 0.5166127 Vali Loss: 0.7884560 Test Loss: 0.9167270
Validation loss decreased (0.822229 --> 0.788456).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 92.03935933113098
Epoch: 10, Steps: 90 | Train Loss: 0.4872197 Vali Loss: 0.7565180 Test Loss: 0.8798697
Validation loss decreased (0.788456 --> 0.756518).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 97.69579696655273
Epoch: 11, Steps: 90 | Train Loss: 0.4610080 Vali Loss: 0.7290783 Test Loss: 0.8482398
Validation loss decreased (0.756518 --> 0.729078).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 89.48308372497559
Epoch: 12, Steps: 90 | Train Loss: 0.4375511 Vali Loss: 0.7045548 Test Loss: 0.8199375
Validation loss decreased (0.729078 --> 0.704555).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 87.57167887687683
Epoch: 13, Steps: 90 | Train Loss: 0.4164780 Vali Loss: 0.6809616 Test Loss: 0.7934627
Validation loss decreased (0.704555 --> 0.680962).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 86.82664847373962
Epoch: 14, Steps: 90 | Train Loss: 0.3974426 Vali Loss: 0.6609378 Test Loss: 0.7700302
Validation loss decreased (0.680962 --> 0.660938).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 86.21104335784912
Epoch: 15, Steps: 90 | Train Loss: 0.3802908 Vali Loss: 0.6409874 Test Loss: 0.7472092
Validation loss decreased (0.660938 --> 0.640987).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 85.7712676525116
Epoch: 16, Steps: 90 | Train Loss: 0.3647023 Vali Loss: 0.6241734 Test Loss: 0.7279998
Validation loss decreased (0.640987 --> 0.624173).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 85.23591899871826
Epoch: 17, Steps: 90 | Train Loss: 0.3505682 Vali Loss: 0.6086144 Test Loss: 0.7097541
Validation loss decreased (0.624173 --> 0.608614).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 79.23788070678711
Epoch: 18, Steps: 90 | Train Loss: 0.3376767 Vali Loss: 0.5949559 Test Loss: 0.6946735
Validation loss decreased (0.608614 --> 0.594956).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 78.59401154518127
Epoch: 19, Steps: 90 | Train Loss: 0.3258831 Vali Loss: 0.5814566 Test Loss: 0.6791649
Validation loss decreased (0.594956 --> 0.581457).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 75.41806745529175
Epoch: 20, Steps: 90 | Train Loss: 0.3150782 Vali Loss: 0.5687586 Test Loss: 0.6650524
Validation loss decreased (0.581457 --> 0.568759).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 78.69865369796753
Epoch: 21, Steps: 90 | Train Loss: 0.3051364 Vali Loss: 0.5578173 Test Loss: 0.6525483
Validation loss decreased (0.568759 --> 0.557817).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 79.65813612937927
Epoch: 22, Steps: 90 | Train Loss: 0.2960765 Vali Loss: 0.5488344 Test Loss: 0.6423385
Validation loss decreased (0.557817 --> 0.548834).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 76.48521089553833
Epoch: 23, Steps: 90 | Train Loss: 0.2877322 Vali Loss: 0.5393606 Test Loss: 0.6308572
Validation loss decreased (0.548834 --> 0.539361).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 78.41509008407593
Epoch: 24, Steps: 90 | Train Loss: 0.2800496 Vali Loss: 0.5310944 Test Loss: 0.6224119
Validation loss decreased (0.539361 --> 0.531094).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 78.92400693893433
Epoch: 25, Steps: 90 | Train Loss: 0.2729411 Vali Loss: 0.5227640 Test Loss: 0.6128673
Validation loss decreased (0.531094 --> 0.522764).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 99.12351822853088
Epoch: 26, Steps: 90 | Train Loss: 0.2663400 Vali Loss: 0.5150958 Test Loss: 0.6044396
Validation loss decreased (0.522764 --> 0.515096).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 182.78256130218506
Epoch: 27, Steps: 90 | Train Loss: 0.2602497 Vali Loss: 0.5080920 Test Loss: 0.5962195
Validation loss decreased (0.515096 --> 0.508092).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 149.80925798416138
Epoch: 28, Steps: 90 | Train Loss: 0.2546195 Vali Loss: 0.5016647 Test Loss: 0.5894066
Validation loss decreased (0.508092 --> 0.501665).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 175.01424479484558
Epoch: 29, Steps: 90 | Train Loss: 0.2493417 Vali Loss: 0.4958460 Test Loss: 0.5826072
Validation loss decreased (0.501665 --> 0.495846).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 148.68782091140747
Epoch: 30, Steps: 90 | Train Loss: 0.2444734 Vali Loss: 0.4905465 Test Loss: 0.5768483
Validation loss decreased (0.495846 --> 0.490546).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 177.55776691436768
Epoch: 31, Steps: 90 | Train Loss: 0.2399259 Vali Loss: 0.4851366 Test Loss: 0.5708386
Validation loss decreased (0.490546 --> 0.485137).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 137.46990394592285
Epoch: 32, Steps: 90 | Train Loss: 0.2357547 Vali Loss: 0.4803086 Test Loss: 0.5658466
Validation loss decreased (0.485137 --> 0.480309).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 139.8137288093567
Epoch: 33, Steps: 90 | Train Loss: 0.2318115 Vali Loss: 0.4757290 Test Loss: 0.5607399
Validation loss decreased (0.480309 --> 0.475729).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 185.61724138259888
Epoch: 34, Steps: 90 | Train Loss: 0.2281110 Vali Loss: 0.4720570 Test Loss: 0.5563431
Validation loss decreased (0.475729 --> 0.472057).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 181.69666743278503
Epoch: 35, Steps: 90 | Train Loss: 0.2246948 Vali Loss: 0.4675915 Test Loss: 0.5520061
Validation loss decreased (0.472057 --> 0.467591).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 173.86215472221375
Epoch: 36, Steps: 90 | Train Loss: 0.2214628 Vali Loss: 0.4642579 Test Loss: 0.5478915
Validation loss decreased (0.467591 --> 0.464258).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 182.0604648590088
Epoch: 37, Steps: 90 | Train Loss: 0.2184818 Vali Loss: 0.4607176 Test Loss: 0.5440369
Validation loss decreased (0.464258 --> 0.460718).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 153.72371292114258
Epoch: 38, Steps: 90 | Train Loss: 0.2156329 Vali Loss: 0.4574316 Test Loss: 0.5405022
Validation loss decreased (0.460718 --> 0.457432).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 164.29044127464294
Epoch: 39, Steps: 90 | Train Loss: 0.2129632 Vali Loss: 0.4545810 Test Loss: 0.5372791
Validation loss decreased (0.457432 --> 0.454581).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 124.89216494560242
Epoch: 40, Steps: 90 | Train Loss: 0.2105339 Vali Loss: 0.4512532 Test Loss: 0.5339763
Validation loss decreased (0.454581 --> 0.451253).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 164.92344164848328
Epoch: 41, Steps: 90 | Train Loss: 0.2081547 Vali Loss: 0.4489659 Test Loss: 0.5311463
Validation loss decreased (0.451253 --> 0.448966).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 243.0150065422058
Epoch: 42, Steps: 90 | Train Loss: 0.2059759 Vali Loss: 0.4464658 Test Loss: 0.5284589
Validation loss decreased (0.448966 --> 0.446466).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 129.54622387886047
Epoch: 43, Steps: 90 | Train Loss: 0.2039071 Vali Loss: 0.4438066 Test Loss: 0.5257745
Validation loss decreased (0.446466 --> 0.443807).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 126.06047558784485
Epoch: 44, Steps: 90 | Train Loss: 0.2019671 Vali Loss: 0.4418826 Test Loss: 0.5234793
Validation loss decreased (0.443807 --> 0.441883).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 154.31442070007324
Epoch: 45, Steps: 90 | Train Loss: 0.2001567 Vali Loss: 0.4398709 Test Loss: 0.5210063
Validation loss decreased (0.441883 --> 0.439871).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 154.555082321167
Epoch: 46, Steps: 90 | Train Loss: 0.1984034 Vali Loss: 0.4376698 Test Loss: 0.5188816
Validation loss decreased (0.439871 --> 0.437670).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 163.20122265815735
Epoch: 47, Steps: 90 | Train Loss: 0.1968067 Vali Loss: 0.4357644 Test Loss: 0.5170175
Validation loss decreased (0.437670 --> 0.435764).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 153.89080023765564
Epoch: 48, Steps: 90 | Train Loss: 0.1952577 Vali Loss: 0.4343217 Test Loss: 0.5150468
Validation loss decreased (0.435764 --> 0.434322).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 158.89127588272095
Epoch: 49, Steps: 90 | Train Loss: 0.1938225 Vali Loss: 0.4324081 Test Loss: 0.5131749
Validation loss decreased (0.434322 --> 0.432408).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 159.51541233062744
Epoch: 50, Steps: 90 | Train Loss: 0.1924171 Vali Loss: 0.4305928 Test Loss: 0.5114905
Validation loss decreased (0.432408 --> 0.430593).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 164.81517243385315
Epoch: 51, Steps: 90 | Train Loss: 0.1911728 Vali Loss: 0.4291670 Test Loss: 0.5100223
Validation loss decreased (0.430593 --> 0.429167).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 156.7770276069641
Epoch: 52, Steps: 90 | Train Loss: 0.1899293 Vali Loss: 0.4278911 Test Loss: 0.5084612
Validation loss decreased (0.429167 --> 0.427891).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 151.49021577835083
Epoch: 53, Steps: 90 | Train Loss: 0.1887996 Vali Loss: 0.4265903 Test Loss: 0.5070732
Validation loss decreased (0.427891 --> 0.426590).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 158.58549284934998
Epoch: 54, Steps: 90 | Train Loss: 0.1876753 Vali Loss: 0.4254159 Test Loss: 0.5056151
Validation loss decreased (0.426590 --> 0.425416).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 162.343688249588
Epoch: 55, Steps: 90 | Train Loss: 0.1866576 Vali Loss: 0.4242183 Test Loss: 0.5043541
Validation loss decreased (0.425416 --> 0.424218).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 162.056813955307
Epoch: 56, Steps: 90 | Train Loss: 0.1856685 Vali Loss: 0.4233395 Test Loss: 0.5031798
Validation loss decreased (0.424218 --> 0.423339).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 160.46908259391785
Epoch: 57, Steps: 90 | Train Loss: 0.1847604 Vali Loss: 0.4219071 Test Loss: 0.5019493
Validation loss decreased (0.423339 --> 0.421907).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 163.47097086906433
Epoch: 58, Steps: 90 | Train Loss: 0.1838402 Vali Loss: 0.4209752 Test Loss: 0.5009202
Validation loss decreased (0.421907 --> 0.420975).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 156.34062600135803
Epoch: 59, Steps: 90 | Train Loss: 0.1830451 Vali Loss: 0.4201468 Test Loss: 0.4999142
Validation loss decreased (0.420975 --> 0.420147).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 159.63257217407227
Epoch: 60, Steps: 90 | Train Loss: 0.1822460 Vali Loss: 0.4192449 Test Loss: 0.4989215
Validation loss decreased (0.420147 --> 0.419245).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 154.7161853313446
Epoch: 61, Steps: 90 | Train Loss: 0.1815024 Vali Loss: 0.4183955 Test Loss: 0.4980112
Validation loss decreased (0.419245 --> 0.418395).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 155.95646381378174
Epoch: 62, Steps: 90 | Train Loss: 0.1807994 Vali Loss: 0.4173731 Test Loss: 0.4971478
Validation loss decreased (0.418395 --> 0.417373).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 155.75953650474548
Epoch: 63, Steps: 90 | Train Loss: 0.1801130 Vali Loss: 0.4163902 Test Loss: 0.4963050
Validation loss decreased (0.417373 --> 0.416390).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 152.57150959968567
Epoch: 64, Steps: 90 | Train Loss: 0.1794754 Vali Loss: 0.4157459 Test Loss: 0.4954935
Validation loss decreased (0.416390 --> 0.415746).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 140.5315237045288
Epoch: 65, Steps: 90 | Train Loss: 0.1788585 Vali Loss: 0.4153571 Test Loss: 0.4947794
Validation loss decreased (0.415746 --> 0.415357).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 135.70344996452332
Epoch: 66, Steps: 90 | Train Loss: 0.1782960 Vali Loss: 0.4146282 Test Loss: 0.4940509
Validation loss decreased (0.415357 --> 0.414628).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 183.09296989440918
Epoch: 67, Steps: 90 | Train Loss: 0.1777812 Vali Loss: 0.4138902 Test Loss: 0.4933641
Validation loss decreased (0.414628 --> 0.413890).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 186.63593912124634
Epoch: 68, Steps: 90 | Train Loss: 0.1772366 Vali Loss: 0.4131840 Test Loss: 0.4927467
Validation loss decreased (0.413890 --> 0.413184).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 181.46429657936096
Epoch: 69, Steps: 90 | Train Loss: 0.1767234 Vali Loss: 0.4126250 Test Loss: 0.4921455
Validation loss decreased (0.413184 --> 0.412625).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 166.11465883255005
Epoch: 70, Steps: 90 | Train Loss: 0.1762910 Vali Loss: 0.4124593 Test Loss: 0.4915866
Validation loss decreased (0.412625 --> 0.412459).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 161.53589057922363
Epoch: 71, Steps: 90 | Train Loss: 0.1758394 Vali Loss: 0.4114748 Test Loss: 0.4910417
Validation loss decreased (0.412459 --> 0.411475).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 159.9364628791809
Epoch: 72, Steps: 90 | Train Loss: 0.1754328 Vali Loss: 0.4109000 Test Loss: 0.4905252
Validation loss decreased (0.411475 --> 0.410900).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 158.3065071105957
Epoch: 73, Steps: 90 | Train Loss: 0.1750366 Vali Loss: 0.4106106 Test Loss: 0.4900147
Validation loss decreased (0.410900 --> 0.410611).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 160.05879378318787
Epoch: 74, Steps: 90 | Train Loss: 0.1746632 Vali Loss: 0.4100854 Test Loss: 0.4895672
Validation loss decreased (0.410611 --> 0.410085).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 174.0402317047119
Epoch: 75, Steps: 90 | Train Loss: 0.1742491 Vali Loss: 0.4097355 Test Loss: 0.4891219
Validation loss decreased (0.410085 --> 0.409735).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 158.91255927085876
Epoch: 76, Steps: 90 | Train Loss: 0.1739480 Vali Loss: 0.4091125 Test Loss: 0.4887167
Validation loss decreased (0.409735 --> 0.409112).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 218.62609028816223
Epoch: 77, Steps: 90 | Train Loss: 0.1736496 Vali Loss: 0.4091208 Test Loss: 0.4882973
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 230.60183596611023
Epoch: 78, Steps: 90 | Train Loss: 0.1733339 Vali Loss: 0.4089437 Test Loss: 0.4879376
Validation loss decreased (0.409112 --> 0.408944).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 173.5389404296875
Epoch: 79, Steps: 90 | Train Loss: 0.1730609 Vali Loss: 0.4084795 Test Loss: 0.4875569
Validation loss decreased (0.408944 --> 0.408480).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 185.31423377990723
Epoch: 80, Steps: 90 | Train Loss: 0.1727680 Vali Loss: 0.4078658 Test Loss: 0.4872203
Validation loss decreased (0.408480 --> 0.407866).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 141.1093385219574
Epoch: 81, Steps: 90 | Train Loss: 0.1725013 Vali Loss: 0.4078526 Test Loss: 0.4868872
Validation loss decreased (0.407866 --> 0.407853).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 177.9186418056488
Epoch: 82, Steps: 90 | Train Loss: 0.1722098 Vali Loss: 0.4076194 Test Loss: 0.4865815
Validation loss decreased (0.407853 --> 0.407619).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 161.229887008667
Epoch: 83, Steps: 90 | Train Loss: 0.1720291 Vali Loss: 0.4069821 Test Loss: 0.4863023
Validation loss decreased (0.407619 --> 0.406982).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 146.6130132675171
Epoch: 84, Steps: 90 | Train Loss: 0.1717703 Vali Loss: 0.4068369 Test Loss: 0.4860108
Validation loss decreased (0.406982 --> 0.406837).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 147.58402729034424
Epoch: 85, Steps: 90 | Train Loss: 0.1715960 Vali Loss: 0.4068900 Test Loss: 0.4857539
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 132.3544156551361
Epoch: 86, Steps: 90 | Train Loss: 0.1713765 Vali Loss: 0.4064631 Test Loss: 0.4855045
Validation loss decreased (0.406837 --> 0.406463).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 157.39051413536072
Epoch: 87, Steps: 90 | Train Loss: 0.1711264 Vali Loss: 0.4065475 Test Loss: 0.4852793
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 140.30930161476135
Epoch: 88, Steps: 90 | Train Loss: 0.1709806 Vali Loss: 0.4057134 Test Loss: 0.4850486
Validation loss decreased (0.406463 --> 0.405713).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 142.80798816680908
Epoch: 89, Steps: 90 | Train Loss: 0.1707974 Vali Loss: 0.4057967 Test Loss: 0.4848341
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 174.5675973892212
Epoch: 90, Steps: 90 | Train Loss: 0.1706441 Vali Loss: 0.4055507 Test Loss: 0.4846369
Validation loss decreased (0.405713 --> 0.405551).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 150.6205861568451
Epoch: 91, Steps: 90 | Train Loss: 0.1705017 Vali Loss: 0.4055694 Test Loss: 0.4844369
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 154.64486289024353
Epoch: 92, Steps: 90 | Train Loss: 0.1703494 Vali Loss: 0.4053984 Test Loss: 0.4842459
Validation loss decreased (0.405551 --> 0.405398).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 123.25969314575195
Epoch: 93, Steps: 90 | Train Loss: 0.1701844 Vali Loss: 0.4050597 Test Loss: 0.4840860
Validation loss decreased (0.405398 --> 0.405060).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 149.63632655143738
Epoch: 94, Steps: 90 | Train Loss: 0.1700625 Vali Loss: 0.4048334 Test Loss: 0.4839092
Validation loss decreased (0.405060 --> 0.404833).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 136.40157461166382
Epoch: 95, Steps: 90 | Train Loss: 0.1699135 Vali Loss: 0.4045033 Test Loss: 0.4837472
Validation loss decreased (0.404833 --> 0.404503).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 137.32133960723877
Epoch: 96, Steps: 90 | Train Loss: 0.1697764 Vali Loss: 0.4049011 Test Loss: 0.4835987
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 138.41229510307312
Epoch: 97, Steps: 90 | Train Loss: 0.1696726 Vali Loss: 0.4045968 Test Loss: 0.4834526
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 128.59754467010498
Epoch: 98, Steps: 90 | Train Loss: 0.1695639 Vali Loss: 0.4044291 Test Loss: 0.4833181
Validation loss decreased (0.404503 --> 0.404429).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 136.32064580917358
Epoch: 99, Steps: 90 | Train Loss: 0.1694849 Vali Loss: 0.4040839 Test Loss: 0.4831881
Validation loss decreased (0.404429 --> 0.404084).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 138.65051555633545
Epoch: 100, Steps: 90 | Train Loss: 0.1694081 Vali Loss: 0.4040742 Test Loss: 0.4830649
Validation loss decreased (0.404084 --> 0.404074).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 11585
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=170, out_features=328, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6152335360.0
params:  56088.0
Trainable parameters:  56088
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 125.19877815246582
Epoch: 1, Steps: 90 | Train Loss: 0.2827384 Vali Loss: 0.3628066 Test Loss: 0.4427565
Validation loss decreased (inf --> 0.362807).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 146.34873723983765
Epoch: 2, Steps: 90 | Train Loss: 0.2664310 Vali Loss: 0.3548090 Test Loss: 0.4367009
Validation loss decreased (0.362807 --> 0.354809).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 83.26144790649414
Epoch: 3, Steps: 90 | Train Loss: 0.2639850 Vali Loss: 0.3536993 Test Loss: 0.4366999
Validation loss decreased (0.354809 --> 0.353699).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 229.92850518226624
Epoch: 4, Steps: 90 | Train Loss: 0.2637145 Vali Loss: 0.3537143 Test Loss: 0.4365939
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 154.96553564071655
Epoch: 5, Steps: 90 | Train Loss: 0.2635949 Vali Loss: 0.3534655 Test Loss: 0.4363007
Validation loss decreased (0.353699 --> 0.353466).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 102.41441059112549
Epoch: 6, Steps: 90 | Train Loss: 0.2636380 Vali Loss: 0.3531526 Test Loss: 0.4361712
Validation loss decreased (0.353466 --> 0.353153).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 92.81962776184082
Epoch: 7, Steps: 90 | Train Loss: 0.2635628 Vali Loss: 0.3532605 Test Loss: 0.4363705
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 123.99750304222107
Epoch: 8, Steps: 90 | Train Loss: 0.2634798 Vali Loss: 0.3535185 Test Loss: 0.4361620
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 130.17396664619446
Epoch: 9, Steps: 90 | Train Loss: 0.2633904 Vali Loss: 0.3531657 Test Loss: 0.4360938
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 135.94614171981812
Epoch: 10, Steps: 90 | Train Loss: 0.2634476 Vali Loss: 0.3530451 Test Loss: 0.4361079
Validation loss decreased (0.353153 --> 0.353045).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 131.90838527679443
Epoch: 11, Steps: 90 | Train Loss: 0.2634604 Vali Loss: 0.3529356 Test Loss: 0.4361459
Validation loss decreased (0.353045 --> 0.352936).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 129.08772826194763
Epoch: 12, Steps: 90 | Train Loss: 0.2634095 Vali Loss: 0.3532897 Test Loss: 0.4360842
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 127.65727376937866
Epoch: 13, Steps: 90 | Train Loss: 0.2633721 Vali Loss: 0.3530538 Test Loss: 0.4359156
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 133.56757926940918
Epoch: 14, Steps: 90 | Train Loss: 0.2633072 Vali Loss: 0.3533263 Test Loss: 0.4359876
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 129.79489254951477
Epoch: 15, Steps: 90 | Train Loss: 0.2633099 Vali Loss: 0.3528514 Test Loss: 0.4358222
Validation loss decreased (0.352936 --> 0.352851).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 131.7114074230194
Epoch: 16, Steps: 90 | Train Loss: 0.2632908 Vali Loss: 0.3530923 Test Loss: 0.4361016
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 117.28103566169739
Epoch: 17, Steps: 90 | Train Loss: 0.2633539 Vali Loss: 0.3532738 Test Loss: 0.4360291
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 129.4719364643097
Epoch: 18, Steps: 90 | Train Loss: 0.2632062 Vali Loss: 0.3533356 Test Loss: 0.4358583
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 128.7469654083252
Epoch: 19, Steps: 90 | Train Loss: 0.2631966 Vali Loss: 0.3534363 Test Loss: 0.4359455
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 132.17223358154297
Epoch: 20, Steps: 90 | Train Loss: 0.2632423 Vali Loss: 0.3528929 Test Loss: 0.4358804
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 116.48355412483215
Epoch: 21, Steps: 90 | Train Loss: 0.2632633 Vali Loss: 0.3532120 Test Loss: 0.4359812
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 107.93479585647583
Epoch: 22, Steps: 90 | Train Loss: 0.2632263 Vali Loss: 0.3531575 Test Loss: 0.4360329
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 152.09967303276062
Epoch: 23, Steps: 90 | Train Loss: 0.2632101 Vali Loss: 0.3528137 Test Loss: 0.4359676
Validation loss decreased (0.352851 --> 0.352814).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 166.84869956970215
Epoch: 24, Steps: 90 | Train Loss: 0.2631957 Vali Loss: 0.3532965 Test Loss: 0.4359903
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 162.07543087005615
Epoch: 25, Steps: 90 | Train Loss: 0.2632558 Vali Loss: 0.3530884 Test Loss: 0.4358910
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 153.54426836967468
Epoch: 26, Steps: 90 | Train Loss: 0.2631338 Vali Loss: 0.3529522 Test Loss: 0.4359716
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 160.44611191749573
Epoch: 27, Steps: 90 | Train Loss: 0.2632757 Vali Loss: 0.3532399 Test Loss: 0.4358662
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 162.1094515323639
Epoch: 28, Steps: 90 | Train Loss: 0.2631256 Vali Loss: 0.3531609 Test Loss: 0.4358305
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 165.07640027999878
Epoch: 29, Steps: 90 | Train Loss: 0.2630831 Vali Loss: 0.3532032 Test Loss: 0.4358139
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 156.8523828983307
Epoch: 30, Steps: 90 | Train Loss: 0.2631287 Vali Loss: 0.3532718 Test Loss: 0.4358696
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 155.95144486427307
Epoch: 31, Steps: 90 | Train Loss: 0.2631712 Vali Loss: 0.3532453 Test Loss: 0.4358344
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 166.15164732933044
Epoch: 32, Steps: 90 | Train Loss: 0.2631866 Vali Loss: 0.3529880 Test Loss: 0.4358459
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 150.85640478134155
Epoch: 33, Steps: 90 | Train Loss: 0.2630774 Vali Loss: 0.3531857 Test Loss: 0.4358681
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j336_H10_FITS_custom_ftM_sl360_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4334561228752136, mae:0.28919053077697754, rse:0.5410929322242737, corr:[0.26707813 0.28282517 0.2823008  0.28333935 0.28233036 0.282782
 0.28251776 0.283081   0.28310537 0.28312767 0.2830699  0.28280094
 0.28279215 0.2820515  0.28226277 0.28207943 0.28233832 0.28229576
 0.2820468  0.28214175 0.28232428 0.28236267 0.28217527 0.28248215
 0.28356877 0.28392586 0.2836075  0.2835788  0.28337482 0.28299066
 0.28318363 0.28280136 0.2830129  0.28312117 0.28286627 0.28276634
 0.28246722 0.28250644 0.28246576 0.28251648 0.28273198 0.28266552
 0.2824215  0.28273898 0.28293583 0.28289664 0.28272396 0.2823304
 0.28290316 0.28301147 0.28325447 0.28339073 0.28329515 0.28321028
 0.28310928 0.28320658 0.28317076 0.28326583 0.2832762  0.2832608
 0.28306302 0.28292742 0.28306934 0.28266442 0.28266603 0.28250208
 0.28234386 0.2824582  0.2826784  0.28285143 0.282466   0.28268117
 0.28262916 0.28235313 0.2825833  0.28250322 0.28235045 0.28210855
 0.2822053  0.28223512 0.28238285 0.2823799  0.28230613 0.28273568
 0.28258944 0.28254518 0.28249347 0.28246334 0.28245094 0.28235453
 0.28261414 0.28234932 0.28234896 0.28214204 0.28180635 0.28198636
 0.2819024  0.2820006  0.28185174 0.28167376 0.281767   0.2820643
 0.2821158  0.28198534 0.28187418 0.28176886 0.2820228  0.28224608
 0.28223923 0.2822343  0.2823401  0.28237337 0.28215325 0.28197315
 0.28178546 0.2817692  0.28157783 0.2812426  0.28140742 0.2814744
 0.28151157 0.28192803 0.28232375 0.28238922 0.282228   0.28205237
 0.28190377 0.28184617 0.28156018 0.2815483  0.28156483 0.28151587
 0.28156874 0.2815759  0.28173232 0.28193608 0.28228584 0.2821746
 0.28185797 0.28163868 0.2816901  0.2819523  0.2820903  0.2827555
 0.28314525 0.2832693  0.2833867  0.28349778 0.28362328 0.28338474
 0.28356373 0.28358892 0.28354114 0.2837676  0.28349745 0.28341642
 0.283542   0.28360415 0.28356364 0.28344446 0.28355706 0.28358215
 0.28357992 0.28338903 0.28328043 0.2833204  0.28270558 0.2830639
 0.28472498 0.2844696  0.28416288 0.28423157 0.2838128  0.28361318
 0.28381628 0.2840431  0.2842086  0.2839441  0.28342265 0.28352776
 0.28361592 0.28350988 0.2839123  0.28360894 0.28328338 0.2836317
 0.28355277 0.2835308  0.2834719  0.28302363 0.28274402 0.28299603
 0.28391114 0.2836949  0.28338766 0.28347233 0.28328443 0.2832288
 0.2836027  0.28368384 0.28372163 0.28355986 0.28322732 0.28329617
 0.28306335 0.28309727 0.28314406 0.28268263 0.28277972 0.282846
 0.28264758 0.28244564 0.28252614 0.2823497  0.28198615 0.2821416
 0.28239888 0.28249636 0.2823941  0.28235835 0.28243768 0.2825213
 0.28284964 0.2827057  0.28283322 0.28302348 0.2825677  0.2824442
 0.28252184 0.2825851  0.28280294 0.28259352 0.28202483 0.28220353
 0.2823863  0.28216314 0.28237402 0.2822854  0.2822439  0.28243893
 0.28237474 0.28223136 0.28238773 0.28255078 0.2823826  0.28208223
 0.28192684 0.28217775 0.28237623 0.28234813 0.28210923 0.28199685
 0.28197894 0.28177    0.28187725 0.28186294 0.28177983 0.28174534
 0.28151762 0.28151414 0.28141287 0.2812764  0.2810275  0.28131458
 0.28159854 0.28154698 0.28158635 0.28133172 0.28134316 0.28126726
 0.2813085  0.28143296 0.28134057 0.2814244  0.28128773 0.28136927
 0.28135777 0.28153047 0.28176284 0.2812924  0.28097653 0.2809024
 0.280972   0.2809656  0.28113553 0.2810081  0.28078818 0.28102887
 0.28079814 0.28130966 0.2815743  0.28129455 0.28092965 0.28076565
 0.28074938 0.28067553 0.28087664 0.2806315  0.28043002 0.2805119
 0.28074276 0.28089046 0.28112915 0.28126454 0.28106448 0.2813546
 0.2813463  0.28129083 0.28147027 0.28135833 0.2814603  0.28215107
 0.28258005 0.28222054 0.28237942 0.2823083  0.28224728 0.2822604
 0.28216663 0.28252104 0.28252232 0.28262782 0.28204525 0.28211573
 0.28225943 0.2822903  0.2826845  0.2825635  0.2821074  0.28187308
 0.28178826 0.2811652  0.28181776 0.28131244 0.2819597  0.28192717]
