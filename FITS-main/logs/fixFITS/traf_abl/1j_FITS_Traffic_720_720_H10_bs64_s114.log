Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  22596812800.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 130.31293559074402
Epoch: 1, Steps: 84 | Train Loss: 0.9944615 Vali Loss: 0.9167346 Test Loss: 1.0750664
Validation loss decreased (inf --> 0.916735).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 173.83220767974854
Epoch: 2, Steps: 84 | Train Loss: 0.6525001 Vali Loss: 0.7296012 Test Loss: 0.8508936
Validation loss decreased (0.916735 --> 0.729601).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 129.9082634449005
Epoch: 3, Steps: 84 | Train Loss: 0.5216120 Vali Loss: 0.6143773 Test Loss: 0.7131962
Validation loss decreased (0.729601 --> 0.614377).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 211.37967777252197
Epoch: 4, Steps: 84 | Train Loss: 0.4376706 Vali Loss: 0.5383013 Test Loss: 0.6239496
Validation loss decreased (0.614377 --> 0.538301).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 169.91349387168884
Epoch: 5, Steps: 84 | Train Loss: 0.3820750 Vali Loss: 0.4876119 Test Loss: 0.5650613
Validation loss decreased (0.538301 --> 0.487612).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 183.5413784980774
Epoch: 6, Steps: 84 | Train Loss: 0.3448420 Vali Loss: 0.4532082 Test Loss: 0.5248563
Validation loss decreased (0.487612 --> 0.453208).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 133.27694272994995
Epoch: 7, Steps: 84 | Train Loss: 0.3199483 Vali Loss: 0.4303369 Test Loss: 0.4994445
Validation loss decreased (0.453208 --> 0.430337).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 164.9167332649231
Epoch: 8, Steps: 84 | Train Loss: 0.3033137 Vali Loss: 0.4156530 Test Loss: 0.4816999
Validation loss decreased (0.430337 --> 0.415653).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 129.78871297836304
Epoch: 9, Steps: 84 | Train Loss: 0.2922921 Vali Loss: 0.4054735 Test Loss: 0.4708292
Validation loss decreased (0.415653 --> 0.405474).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 211.77483797073364
Epoch: 10, Steps: 84 | Train Loss: 0.2850858 Vali Loss: 0.3994463 Test Loss: 0.4632075
Validation loss decreased (0.405474 --> 0.399446).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 182.21118927001953
Epoch: 11, Steps: 84 | Train Loss: 0.2801473 Vali Loss: 0.3950723 Test Loss: 0.4594010
Validation loss decreased (0.399446 --> 0.395072).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 136.2591474056244
Epoch: 12, Steps: 84 | Train Loss: 0.2770658 Vali Loss: 0.3927721 Test Loss: 0.4561671
Validation loss decreased (0.395072 --> 0.392772).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 135.80383372306824
Epoch: 13, Steps: 84 | Train Loss: 0.2748458 Vali Loss: 0.3898956 Test Loss: 0.4534192
Validation loss decreased (0.392772 --> 0.389896).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 129.76482844352722
Epoch: 14, Steps: 84 | Train Loss: 0.2734966 Vali Loss: 0.3892936 Test Loss: 0.4522955
Validation loss decreased (0.389896 --> 0.389294).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 133.85878801345825
Epoch: 15, Steps: 84 | Train Loss: 0.2725357 Vali Loss: 0.3877844 Test Loss: 0.4518891
Validation loss decreased (0.389294 --> 0.387784).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 131.2507050037384
Epoch: 16, Steps: 84 | Train Loss: 0.2718996 Vali Loss: 0.3873171 Test Loss: 0.4511054
Validation loss decreased (0.387784 --> 0.387317).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 123.61103129386902
Epoch: 17, Steps: 84 | Train Loss: 0.2713798 Vali Loss: 0.3867863 Test Loss: 0.4505496
Validation loss decreased (0.387317 --> 0.386786).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 135.1305902004242
Epoch: 18, Steps: 84 | Train Loss: 0.2710444 Vali Loss: 0.3867022 Test Loss: 0.4504502
Validation loss decreased (0.386786 --> 0.386702).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 161.91444969177246
Epoch: 19, Steps: 84 | Train Loss: 0.2709473 Vali Loss: 0.3868518 Test Loss: 0.4504121
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 145.92102789878845
Epoch: 20, Steps: 84 | Train Loss: 0.2706079 Vali Loss: 0.3863474 Test Loss: 0.4500019
Validation loss decreased (0.386702 --> 0.386347).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 155.14332818984985
Epoch: 21, Steps: 84 | Train Loss: 0.2706111 Vali Loss: 0.3866068 Test Loss: 0.4500573
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 143.8015329837799
Epoch: 22, Steps: 84 | Train Loss: 0.2704794 Vali Loss: 0.3865162 Test Loss: 0.4499379
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 157.13034105300903
Epoch: 23, Steps: 84 | Train Loss: 0.2703745 Vali Loss: 0.3856570 Test Loss: 0.4498142
Validation loss decreased (0.386347 --> 0.385657).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 158.27754640579224
Epoch: 24, Steps: 84 | Train Loss: 0.2702628 Vali Loss: 0.3861719 Test Loss: 0.4496500
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 163.42133164405823
Epoch: 25, Steps: 84 | Train Loss: 0.2701761 Vali Loss: 0.3857718 Test Loss: 0.4497350
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 198.3896872997284
Epoch: 26, Steps: 84 | Train Loss: 0.2702181 Vali Loss: 0.3861913 Test Loss: 0.4499650
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 203.20153546333313
Epoch: 27, Steps: 84 | Train Loss: 0.2701358 Vali Loss: 0.3857264 Test Loss: 0.4497821
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 196.21294450759888
Epoch: 28, Steps: 84 | Train Loss: 0.2701807 Vali Loss: 0.3856065 Test Loss: 0.4497689
Validation loss decreased (0.385657 --> 0.385607).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 212.8189218044281
Epoch: 29, Steps: 84 | Train Loss: 0.2701723 Vali Loss: 0.3860469 Test Loss: 0.4496725
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 191.1928174495697
Epoch: 30, Steps: 84 | Train Loss: 0.2699354 Vali Loss: 0.3855258 Test Loss: 0.4495580
Validation loss decreased (0.385607 --> 0.385526).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 189.3714530467987
Epoch: 31, Steps: 84 | Train Loss: 0.2700494 Vali Loss: 0.3851029 Test Loss: 0.4496050
Validation loss decreased (0.385526 --> 0.385103).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 179.0644989013672
Epoch: 32, Steps: 84 | Train Loss: 0.2699796 Vali Loss: 0.3855911 Test Loss: 0.4498717
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 177.7214913368225
Epoch: 33, Steps: 84 | Train Loss: 0.2699982 Vali Loss: 0.3853828 Test Loss: 0.4495631
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 189.9554784297943
Epoch: 34, Steps: 84 | Train Loss: 0.2699534 Vali Loss: 0.3854409 Test Loss: 0.4494879
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 190.1594636440277
Epoch: 35, Steps: 84 | Train Loss: 0.2699775 Vali Loss: 0.3857471 Test Loss: 0.4495523
EarlyStopping counter: 4 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 191.13101720809937
Epoch: 36, Steps: 84 | Train Loss: 0.2698284 Vali Loss: 0.3856158 Test Loss: 0.4495801
EarlyStopping counter: 5 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 181.779611825943
Epoch: 37, Steps: 84 | Train Loss: 0.2698359 Vali Loss: 0.3859093 Test Loss: 0.4495499
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 198.2923662662506
Epoch: 38, Steps: 84 | Train Loss: 0.2698714 Vali Loss: 0.3851809 Test Loss: 0.4494978
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 175.1041214466095
Epoch: 39, Steps: 84 | Train Loss: 0.2697918 Vali Loss: 0.3854685 Test Loss: 0.4495219
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 169.90150237083435
Epoch: 40, Steps: 84 | Train Loss: 0.2698257 Vali Loss: 0.3850034 Test Loss: 0.4494479
Validation loss decreased (0.385103 --> 0.385003).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 196.03489565849304
Epoch: 41, Steps: 84 | Train Loss: 0.2697968 Vali Loss: 0.3848988 Test Loss: 0.4493577
Validation loss decreased (0.385003 --> 0.384899).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 174.61670017242432
Epoch: 42, Steps: 84 | Train Loss: 0.2698369 Vali Loss: 0.3850331 Test Loss: 0.4493870
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 173.89840126037598
Epoch: 43, Steps: 84 | Train Loss: 0.2697847 Vali Loss: 0.3855392 Test Loss: 0.4493544
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 157.02632069587708
Epoch: 44, Steps: 84 | Train Loss: 0.2698153 Vali Loss: 0.3845952 Test Loss: 0.4495803
Validation loss decreased (0.384899 --> 0.384595).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 181.12772965431213
Epoch: 45, Steps: 84 | Train Loss: 0.2697637 Vali Loss: 0.3851886 Test Loss: 0.4494585
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 217.0048656463623
Epoch: 46, Steps: 84 | Train Loss: 0.2697209 Vali Loss: 0.3856452 Test Loss: 0.4495121
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 174.0571985244751
Epoch: 47, Steps: 84 | Train Loss: 0.2697063 Vali Loss: 0.3855301 Test Loss: 0.4493591
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 173.28079295158386
Epoch: 48, Steps: 84 | Train Loss: 0.2697827 Vali Loss: 0.3851696 Test Loss: 0.4494493
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 209.9190058708191
Epoch: 49, Steps: 84 | Train Loss: 0.2696731 Vali Loss: 0.3851336 Test Loss: 0.4493406
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 210.563866853714
Epoch: 50, Steps: 84 | Train Loss: 0.2697299 Vali Loss: 0.3852214 Test Loss: 0.4493399
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 189.73036575317383
Epoch: 51, Steps: 84 | Train Loss: 0.2696495 Vali Loss: 0.3847684 Test Loss: 0.4494465
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 120.70460081100464
Epoch: 52, Steps: 84 | Train Loss: 0.2697611 Vali Loss: 0.3852429 Test Loss: 0.4494305
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 103.74450540542603
Epoch: 53, Steps: 84 | Train Loss: 0.2697955 Vali Loss: 0.3855860 Test Loss: 0.4493613
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 197.97158575057983
Epoch: 54, Steps: 84 | Train Loss: 0.2696438 Vali Loss: 0.3845592 Test Loss: 0.4494098
Validation loss decreased (0.384595 --> 0.384559).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 170.28488969802856
Epoch: 55, Steps: 84 | Train Loss: 0.2696318 Vali Loss: 0.3850417 Test Loss: 0.4493479
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 164.65787887573242
Epoch: 56, Steps: 84 | Train Loss: 0.2696970 Vali Loss: 0.3851049 Test Loss: 0.4493674
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 169.24749541282654
Epoch: 57, Steps: 84 | Train Loss: 0.2697201 Vali Loss: 0.3848856 Test Loss: 0.4492875
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 195.99859809875488
Epoch: 58, Steps: 84 | Train Loss: 0.2696961 Vali Loss: 0.3849515 Test Loss: 0.4492804
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 180.00580501556396
Epoch: 59, Steps: 84 | Train Loss: 0.2696330 Vali Loss: 0.3852375 Test Loss: 0.4493171
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 185.56753158569336
Epoch: 60, Steps: 84 | Train Loss: 0.2695869 Vali Loss: 0.3852796 Test Loss: 0.4493335
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 164.21551394462585
Epoch: 61, Steps: 84 | Train Loss: 0.2696340 Vali Loss: 0.3852298 Test Loss: 0.4492915
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 115.84433937072754
Epoch: 62, Steps: 84 | Train Loss: 0.2695927 Vali Loss: 0.3849896 Test Loss: 0.4493271
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 202.23691153526306
Epoch: 63, Steps: 84 | Train Loss: 0.2696817 Vali Loss: 0.3848244 Test Loss: 0.4493261
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 110.65500140190125
Epoch: 64, Steps: 84 | Train Loss: 0.2696419 Vali Loss: 0.3849966 Test Loss: 0.4493512
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.44888243079185486, mae:0.29935774207115173, rse:0.547856867313385, corr:[0.2558971  0.26507002 0.26313257 0.26549447 0.2676308  0.26795346
 0.27036095 0.26886386 0.27022716 0.26879808 0.26857266 0.268288
 0.26671147 0.26720512 0.265597   0.26613453 0.2661758  0.266176
 0.26735106 0.26691276 0.26764724 0.26744372 0.26752812 0.26787916
 0.2690946  0.2703163  0.26977682 0.26982    0.2696401  0.2688
 0.26895696 0.26803398 0.2680324  0.267636   0.2671211  0.26750526
 0.26672843 0.26715383 0.26702836 0.26692972 0.26777864 0.26766935
 0.26823193 0.26817176 0.26808932 0.26839253 0.26808617 0.26848698
 0.26873082 0.2689347  0.26900426 0.26833653 0.2684674  0.267906
 0.2677455  0.2675867  0.26693612 0.26736638 0.2670943  0.26708746
 0.26713634 0.26698235 0.2675615  0.2673798  0.267842   0.26807025
 0.26777947 0.26808068 0.26772633 0.2677731  0.26778635 0.26785406
 0.2680461  0.2675533  0.2676703  0.2673497  0.2670157  0.2671121
 0.2666751  0.26678532 0.2665377  0.266587   0.26699474 0.26666465
 0.2669054  0.26689863 0.267112   0.2675316  0.2673994  0.26780698
 0.26784572 0.2677059  0.2677875  0.26757076 0.26763633 0.2674887
 0.26748508 0.2674592  0.267036   0.26715556 0.26675734 0.26663867
 0.26661557 0.26633507 0.26654112 0.26642582 0.2666385  0.26673982
 0.2665923  0.26700902 0.2669832  0.26723608 0.2674844  0.26730695
 0.2676236  0.2674581  0.26738042 0.26715714 0.26697955 0.26712516
 0.266912   0.26732805 0.26726398 0.26698118 0.26705706 0.26673958
 0.26688275 0.26688424 0.26691505 0.26722112 0.26713437 0.2674824
 0.2675822  0.2677611  0.26803988 0.26794383 0.26836777 0.26792285
 0.2678118  0.26786783 0.26750204 0.26767737 0.26759684 0.26745257
 0.26770386 0.26765537 0.2678009  0.26738712 0.26724926 0.26738408
 0.26724955 0.26761067 0.26758814 0.2677746  0.26788676 0.26773015
 0.26813513 0.26826838 0.26845255 0.2684807  0.2684919  0.2688523
 0.26860926 0.26856348 0.2686205  0.26834524 0.26825666 0.26814646
 0.26954302 0.26978722 0.2693031  0.26907176 0.26867995 0.26875946
 0.26866493 0.26858014 0.26887885 0.26875308 0.2690166  0.26909316
 0.2690549  0.26935342 0.26934645 0.26969826 0.26975003 0.2696418
 0.26974782 0.26952562 0.26957256 0.26927096 0.2689788  0.26907435
 0.26962578 0.2698762  0.26959813 0.26939335 0.2692919  0.26903248
 0.26932994 0.26918516 0.26917848 0.26948375 0.26944745 0.2696214
 0.2695243  0.2696695  0.26983005 0.26956394 0.26973534 0.26970655
 0.26963905 0.26948604 0.26907083 0.2691375  0.26896963 0.26886147
 0.2690363  0.26891947 0.26900297 0.2687396  0.2687091  0.26875588
 0.26880348 0.26904356 0.26887435 0.26907006 0.2690813  0.26892412
 0.26918268 0.26908195 0.26929212 0.26927653 0.26908013 0.2693029
 0.26906577 0.26884076 0.2686428  0.26850194 0.26867887 0.26859048
 0.2685827  0.2683705  0.2682579  0.2683394  0.26811734 0.26836336
 0.26852468 0.26868054 0.2688978  0.26871002 0.26884067 0.26879084
 0.26881248 0.26909283 0.26901504 0.26918277 0.2691408  0.26907903
 0.26912653 0.26875076 0.268744   0.26877305 0.2687289  0.26858974
 0.26825514 0.2683711  0.26816836 0.26825753 0.2682927  0.2681046
 0.26837423 0.26829752 0.26840654 0.26873797 0.2686989  0.26874697
 0.2686263  0.26884514 0.2689184  0.2687685  0.2689597  0.26868382
 0.2687901  0.26872197 0.2683192  0.26830816 0.26816234 0.26830378
 0.26821917 0.2682719  0.26859877 0.26842216 0.26861435 0.26870835
 0.2685584  0.26882613 0.26882315 0.2690895  0.26919338 0.2691063
 0.2693604  0.26937154 0.26956427 0.26964977 0.2696401  0.26954824
 0.26928297 0.26930386 0.26919404 0.26901272 0.2690159  0.2688188
 0.269165   0.26910615 0.26910156 0.26933986 0.26919895 0.26945272
 0.26943195 0.2694928  0.2697416  0.26956937 0.26980108 0.26979434
 0.2697979  0.2701132  0.27001238 0.27008224 0.26998574 0.2698741
 0.26992142 0.26959103 0.26960406 0.2695539  0.26945603 0.26936886
 0.27014005 0.27057797 0.2702414  0.27005947 0.27003545 0.26994824
 0.27023068 0.2702389  0.2704616  0.2706063  0.2705266  0.27074745
 0.27060243 0.27061346 0.27060547 0.2705488  0.27083218 0.2706144
 0.27051592 0.27041048 0.27015683 0.27019206 0.26982558 0.2698447
 0.27048016 0.27060017 0.2706415  0.27041972 0.27063444 0.27066922
 0.2705457  0.27071205 0.27071425 0.27091408 0.270746   0.2706582
 0.27078268 0.2704982  0.27068326 0.27065292 0.27051026 0.2706696
 0.2703579  0.27024174 0.2700518  0.2698853  0.26981407 0.26960805
 0.27000153 0.26997212 0.27000496 0.27011976 0.26991203 0.27016369
 0.27014336 0.27019712 0.27046943 0.27045053 0.2706813  0.27051687
 0.270393   0.2704005  0.27015647 0.27028456 0.2701826  0.27010024
 0.26999617 0.26952857 0.2692997  0.26920557 0.26940066 0.26946673
 0.26940066 0.2695143  0.26925635 0.26938197 0.26941988 0.26943043
 0.26985127 0.2697305  0.26985022 0.26999563 0.26987475 0.26997343
 0.26973328 0.26977053 0.26979372 0.26965114 0.26970506 0.26934227
 0.26929447 0.26912236 0.2689457  0.26916137 0.26891783 0.26900694
 0.26891991 0.26885828 0.26896    0.2689306  0.26938936 0.26939303
 0.26936457 0.26975995 0.2697071  0.26987493 0.26977572 0.2696768
 0.26974863 0.26945627 0.26962605 0.26952374 0.26932257 0.26926026
 0.2689762  0.26885816 0.26859328 0.26855814 0.26857993 0.2685119
 0.2689049  0.26894817 0.26906487 0.2693119  0.26929736 0.2696538
 0.26966697 0.26983583 0.26996443 0.26980677 0.2700127  0.26977804
 0.26984304 0.27003998 0.26979375 0.26985082 0.26964504 0.26969147
 0.26975098 0.26937246 0.26935497 0.26926723 0.26946324 0.26953995
 0.26964867 0.2699902  0.26993093 0.2700691  0.2700506  0.27007526
 0.27038586 0.27022707 0.27040175 0.27028385 0.2700213  0.2702063
 0.27001333 0.2701614  0.27008566 0.2699108  0.27002582 0.2697938
 0.2699937  0.2697736  0.2695446  0.26977932 0.26942614 0.2694651
 0.27032298 0.2705101  0.27042332 0.2702428  0.27061874 0.27052858
 0.27048382 0.27072856 0.27053595 0.27066138 0.27042803 0.27029523
 0.27040172 0.27009887 0.27018225 0.2699589  0.26990142 0.2698735
 0.26934615 0.26947752 0.2693692  0.26930836 0.26930583 0.2691997
 0.26998585 0.26987475 0.27006036 0.2702418  0.2700874  0.2705004
 0.270373   0.2703659  0.270491   0.27036262 0.27046427 0.27007672
 0.26993853 0.2698195  0.26951194 0.26950303 0.26907998 0.26906413
 0.26900932 0.26879498 0.26889446 0.26866648 0.26876652 0.26876312
 0.2690901  0.26944998 0.26924473 0.26954815 0.26961747 0.26963794
 0.26987934 0.26966348 0.26988435 0.26972166 0.26948786 0.26942864
 0.2689234  0.269031   0.2687403  0.2684926  0.26868    0.2683809
 0.26842162 0.268089   0.2679426  0.26814356 0.26802456 0.2685201
 0.26857015 0.2685461  0.26866168 0.26857328 0.26891565 0.268817
 0.26892495 0.26886564 0.2685667  0.26867273 0.2681197  0.2680358
 0.2679343  0.26758635 0.26773155 0.26717234 0.26727214 0.26725298
 0.26673004 0.2670337  0.26691914 0.2671736  0.26732576 0.2672955
 0.26751485 0.26728392 0.2677188  0.2678957  0.2678676  0.2681169
 0.26764828 0.26786435 0.2677386  0.2674051  0.26752648 0.267011
 0.2671003  0.26680243 0.26649925 0.2666723  0.2663003  0.2667077
 0.2666983  0.2666786  0.26700404 0.26677975 0.26716805 0.26707044
 0.2671459  0.2676564  0.26773626 0.2682748  0.26804772 0.26834235
 0.26846266 0.26769447 0.26786816 0.2673664  0.26710647 0.26708645
 0.26666352 0.26692498 0.26636276 0.26647654 0.26682457 0.2668043
 0.26735353 0.26700687 0.2672913  0.26754028 0.26755312 0.268156
 0.26823267 0.2687377  0.26876658 0.26866361 0.26906812 0.26862746
 0.26875848 0.26830798 0.26785967 0.26796246 0.2671155  0.26730067
 0.26691478 0.26669303 0.26703796 0.26643264 0.26721743 0.26734528
 0.2675837  0.26810414 0.2677732  0.26852056 0.26825106 0.26849678
 0.2698999  0.26995352 0.27023798 0.26958308 0.26978734 0.26968122
 0.26892096 0.2690644  0.2679551  0.26814267 0.26811823 0.2675535
 0.26819405 0.26750857 0.26824513 0.26828825 0.2683744  0.26952487
 0.26898745 0.2699742  0.269936   0.27000555 0.27051982 0.26964653
 0.27100638 0.27070132 0.2706831  0.27042556 0.26934    0.2697433
 0.26795647 0.26815096 0.26771456 0.2672824  0.2688402  0.26768148
 0.2695859  0.26949263 0.2705682  0.27211794 0.2710842  0.27320158
 0.26996565 0.2701779  0.26671237 0.2643902  0.26361609 0.27177715]
