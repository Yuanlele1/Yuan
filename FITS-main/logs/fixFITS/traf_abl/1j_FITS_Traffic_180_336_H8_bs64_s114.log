Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j336_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j336_H8_FITS_custom_ftM_sl180_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11765
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=74, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1730951168.0
params:  15900.0
Trainable parameters:  15900
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 70.9187023639679
Epoch: 1, Steps: 91 | Train Loss: 1.0956906 Vali Loss: 0.8890417 Test Loss: 1.0748913
Validation loss decreased (inf --> 0.889042).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 68.29176831245422
Epoch: 2, Steps: 91 | Train Loss: 0.5706447 Vali Loss: 0.6047730 Test Loss: 0.7403708
Validation loss decreased (0.889042 --> 0.604773).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 69.2914674282074
Epoch: 3, Steps: 91 | Train Loss: 0.4301544 Vali Loss: 0.5127939 Test Loss: 0.6316717
Validation loss decreased (0.604773 --> 0.512794).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 74.02552795410156
Epoch: 4, Steps: 91 | Train Loss: 0.3772599 Vali Loss: 0.4692873 Test Loss: 0.5794232
Validation loss decreased (0.512794 --> 0.469287).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 73.15785670280457
Epoch: 5, Steps: 91 | Train Loss: 0.3492278 Vali Loss: 0.4438741 Test Loss: 0.5480182
Validation loss decreased (0.469287 --> 0.443874).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 73.13789105415344
Epoch: 6, Steps: 91 | Train Loss: 0.3316641 Vali Loss: 0.4269199 Test Loss: 0.5273307
Validation loss decreased (0.443874 --> 0.426920).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 74.85702276229858
Epoch: 7, Steps: 91 | Train Loss: 0.3199081 Vali Loss: 0.4150943 Test Loss: 0.5133872
Validation loss decreased (0.426920 --> 0.415094).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 70.89988088607788
Epoch: 8, Steps: 91 | Train Loss: 0.3117955 Vali Loss: 0.4074472 Test Loss: 0.5038421
Validation loss decreased (0.415094 --> 0.407447).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 70.13217329978943
Epoch: 9, Steps: 91 | Train Loss: 0.3062141 Vali Loss: 0.4018456 Test Loss: 0.4970458
Validation loss decreased (0.407447 --> 0.401846).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 70.8534164428711
Epoch: 10, Steps: 91 | Train Loss: 0.3021974 Vali Loss: 0.3976159 Test Loss: 0.4924017
Validation loss decreased (0.401846 --> 0.397616).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 70.10019063949585
Epoch: 11, Steps: 91 | Train Loss: 0.2993969 Vali Loss: 0.3950257 Test Loss: 0.4891479
Validation loss decreased (0.397616 --> 0.395026).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 69.87187266349792
Epoch: 12, Steps: 91 | Train Loss: 0.2972426 Vali Loss: 0.3929718 Test Loss: 0.4866990
Validation loss decreased (0.395026 --> 0.392972).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 69.9640896320343
Epoch: 13, Steps: 91 | Train Loss: 0.2957061 Vali Loss: 0.3911663 Test Loss: 0.4849833
Validation loss decreased (0.392972 --> 0.391166).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 74.05837917327881
Epoch: 14, Steps: 91 | Train Loss: 0.2944864 Vali Loss: 0.3901086 Test Loss: 0.4836117
Validation loss decreased (0.391166 --> 0.390109).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 71.00067567825317
Epoch: 15, Steps: 91 | Train Loss: 0.2936770 Vali Loss: 0.3888521 Test Loss: 0.4825799
Validation loss decreased (0.390109 --> 0.388852).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 70.3107078075409
Epoch: 16, Steps: 91 | Train Loss: 0.2929429 Vali Loss: 0.3883282 Test Loss: 0.4818853
Validation loss decreased (0.388852 --> 0.388328).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 73.95005583763123
Epoch: 17, Steps: 91 | Train Loss: 0.2924692 Vali Loss: 0.3880293 Test Loss: 0.4812844
Validation loss decreased (0.388328 --> 0.388029).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 70.10571551322937
Epoch: 18, Steps: 91 | Train Loss: 0.2920682 Vali Loss: 0.3874186 Test Loss: 0.4808612
Validation loss decreased (0.388029 --> 0.387419).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 92.53699493408203
Epoch: 19, Steps: 91 | Train Loss: 0.2917188 Vali Loss: 0.3869571 Test Loss: 0.4804932
Validation loss decreased (0.387419 --> 0.386957).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 74.6629228591919
Epoch: 20, Steps: 91 | Train Loss: 0.2914359 Vali Loss: 0.3869473 Test Loss: 0.4801913
Validation loss decreased (0.386957 --> 0.386947).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 77.44669532775879
Epoch: 21, Steps: 91 | Train Loss: 0.2912700 Vali Loss: 0.3864653 Test Loss: 0.4800274
Validation loss decreased (0.386947 --> 0.386465).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 71.54369306564331
Epoch: 22, Steps: 91 | Train Loss: 0.2910709 Vali Loss: 0.3866620 Test Loss: 0.4798447
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 77.22782635688782
Epoch: 23, Steps: 91 | Train Loss: 0.2910234 Vali Loss: 0.3865584 Test Loss: 0.4796558
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 77.28132390975952
Epoch: 24, Steps: 91 | Train Loss: 0.2908552 Vali Loss: 0.3858702 Test Loss: 0.4795668
Validation loss decreased (0.386465 --> 0.385870).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 68.85417604446411
Epoch: 25, Steps: 91 | Train Loss: 0.2907882 Vali Loss: 0.3857869 Test Loss: 0.4794339
Validation loss decreased (0.385870 --> 0.385787).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 73.33520603179932
Epoch: 26, Steps: 91 | Train Loss: 0.2905411 Vali Loss: 0.3860495 Test Loss: 0.4793766
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 75.56242513656616
Epoch: 27, Steps: 91 | Train Loss: 0.2906045 Vali Loss: 0.3857174 Test Loss: 0.4792773
Validation loss decreased (0.385787 --> 0.385717).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 76.04732370376587
Epoch: 28, Steps: 91 | Train Loss: 0.2905929 Vali Loss: 0.3857962 Test Loss: 0.4791948
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 75.7333595752716
Epoch: 29, Steps: 91 | Train Loss: 0.2905254 Vali Loss: 0.3856786 Test Loss: 0.4791727
Validation loss decreased (0.385717 --> 0.385679).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 74.25178813934326
Epoch: 30, Steps: 91 | Train Loss: 0.2905410 Vali Loss: 0.3855180 Test Loss: 0.4791358
Validation loss decreased (0.385679 --> 0.385518).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 74.26340103149414
Epoch: 31, Steps: 91 | Train Loss: 0.2904530 Vali Loss: 0.3857239 Test Loss: 0.4791014
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 65.47583866119385
Epoch: 32, Steps: 91 | Train Loss: 0.2903294 Vali Loss: 0.3853696 Test Loss: 0.4790676
Validation loss decreased (0.385518 --> 0.385370).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 63.4675989151001
Epoch: 33, Steps: 91 | Train Loss: 0.2904038 Vali Loss: 0.3856932 Test Loss: 0.4790667
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 70.17131471633911
Epoch: 34, Steps: 91 | Train Loss: 0.2903195 Vali Loss: 0.3855176 Test Loss: 0.4790460
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 64.60415959358215
Epoch: 35, Steps: 91 | Train Loss: 0.2902458 Vali Loss: 0.3857009 Test Loss: 0.4790450
EarlyStopping counter: 3 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 74.60083389282227
Epoch: 36, Steps: 91 | Train Loss: 0.2901811 Vali Loss: 0.3854921 Test Loss: 0.4790223
EarlyStopping counter: 4 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 78.42140197753906
Epoch: 37, Steps: 91 | Train Loss: 0.2902789 Vali Loss: 0.3859127 Test Loss: 0.4789992
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 68.05736875534058
Epoch: 38, Steps: 91 | Train Loss: 0.2902376 Vali Loss: 0.3854488 Test Loss: 0.4789620
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 63.19930338859558
Epoch: 39, Steps: 91 | Train Loss: 0.2903412 Vali Loss: 0.3855897 Test Loss: 0.4789672
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 62.16812086105347
Epoch: 40, Steps: 91 | Train Loss: 0.2903066 Vali Loss: 0.3855196 Test Loss: 0.4789519
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 56.492467403411865
Epoch: 41, Steps: 91 | Train Loss: 0.2902223 Vali Loss: 0.3854179 Test Loss: 0.4789289
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 56.88363337516785
Epoch: 42, Steps: 91 | Train Loss: 0.2902891 Vali Loss: 0.3857479 Test Loss: 0.4789286
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j336_H8_FITS_custom_ftM_sl180_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.47711068391799927, mae:0.3098241984844208, rse:0.5676868557929993, corr:[0.27242583 0.2803091  0.28103268 0.27898452 0.28203872 0.28006575
 0.2818722  0.28205973 0.2813204  0.28250685 0.28172806 0.281599
 0.28190163 0.28058195 0.28078067 0.28047293 0.28000423 0.28020704
 0.27995202 0.28017172 0.28036037 0.28043017 0.28160024 0.2814153
 0.28163075 0.2816877  0.2805471  0.28100026 0.28084204 0.2804387
 0.281219   0.28129762 0.28131312 0.2815253  0.28077742 0.28073227
 0.28062132 0.28010082 0.2805611  0.28065407 0.28065413 0.28063568
 0.28035718 0.28046474 0.28053862 0.28053796 0.28125888 0.28094038
 0.28073713 0.28111944 0.2806517  0.2808911  0.28096506 0.2807072
 0.28119045 0.28124878 0.2811496  0.2813123  0.28089055 0.28072983
 0.28072834 0.28038126 0.28059292 0.28047532 0.2805053  0.2806121
 0.28042725 0.28060398 0.28075388 0.28059164 0.28099406 0.2808905
 0.28040865 0.28057703 0.2801204  0.28013337 0.28031132 0.27992284
 0.27997494 0.2800113  0.2799429  0.28030178 0.280291   0.28025797
 0.28070027 0.28035003 0.28033534 0.28026497 0.27984637 0.28005207
 0.28004786 0.27997696 0.27999288 0.2797175  0.2799032  0.28000432
 0.27953976 0.2799592  0.28015336 0.27991873 0.27995327 0.2796412
 0.27954057 0.27949658 0.27918965 0.27933437 0.2794421  0.27907357
 0.27944133 0.2794144  0.2790472  0.27931088 0.2790038  0.27900743
 0.2790863  0.27900925 0.27900708 0.27854756 0.27864066 0.2793068
 0.27938497 0.27973112 0.28007147 0.2795583  0.2795314  0.2793281
 0.2790515  0.27920878 0.27916718 0.27901387 0.2790832  0.2789603
 0.27953222 0.28017527 0.27963883 0.27959788 0.2794162  0.27915132
 0.27916726 0.27939492 0.2797331  0.28003773 0.28073996 0.28110203
 0.28070453 0.28032035 0.28054735 0.28019318 0.28019172 0.28047562
 0.28013188 0.27985892 0.279779   0.27976307 0.28029132 0.28114405
 0.28218982 0.28289157 0.28223103 0.28176162 0.28122687 0.28105772
 0.28102168 0.28126004 0.28222635 0.28303263 0.28400326 0.28404006
 0.28331792 0.2822325  0.2816757  0.28132638 0.28130433 0.28170946
 0.28191873 0.28179118 0.28168908 0.2815891  0.28180665 0.28196204
 0.28193676 0.2822348  0.28200278 0.28182894 0.28136086 0.2810866
 0.28116956 0.2812243  0.2814261  0.28180265 0.28236932 0.2820961
 0.28168234 0.28113788 0.28109473 0.2809469  0.28059638 0.2809082
 0.28102377 0.28091857 0.2811744  0.28114665 0.28081712 0.28074038
 0.2803136  0.28015962 0.2801678  0.28020233 0.28036052 0.28034195
 0.280322   0.28012934 0.28003404 0.28029165 0.28060544 0.28036284
 0.28044766 0.2803687  0.2804129  0.28087723 0.2806429  0.28049514
 0.2806295  0.28051585 0.28036866 0.280325   0.28021154 0.28030235
 0.2801029  0.27991802 0.28001148 0.2798337  0.27989906 0.27978346
 0.27980942 0.2800053  0.27998778 0.2800833  0.28025162 0.279969
 0.28007114 0.28013283 0.27975115 0.28002855 0.2800288  0.27986547
 0.27970618 0.27930054 0.27932328 0.27959767 0.27951902 0.2797032
 0.27974305 0.2793629  0.2795045  0.27920118 0.27901834 0.27914885
 0.2791087  0.27938887 0.27928835 0.2791029  0.2794428  0.27916577
 0.27893555 0.27934515 0.27908194 0.27921006 0.2793376  0.27895907
 0.27885464 0.27872765 0.2784489  0.2784246  0.27832666 0.27809054
 0.2783713  0.27844083 0.278467   0.27854866 0.27866983 0.2789548
 0.27872068 0.27877474 0.27866334 0.2780146  0.27830696 0.27841327
 0.27821192 0.2789917  0.27881843 0.2784582  0.2786079  0.27803034
 0.2778968  0.2779009  0.27767655 0.27760872 0.27775446 0.27782127
 0.27859172 0.27938282 0.27939793 0.2793129  0.27921692 0.27943173
 0.2791989  0.27944598 0.2796686  0.27925092 0.27978757 0.2795121
 0.27867317 0.27937958 0.27903888 0.27861795 0.27939445 0.27888662
 0.2786077  0.27850443 0.27841893 0.27868348 0.27963215 0.28058162
 0.28210393 0.28346464 0.2833356  0.28248328 0.28259465 0.28173324
 0.27970758 0.28043792 0.27818203 0.27949485 0.28004602 0.2831572 ]
