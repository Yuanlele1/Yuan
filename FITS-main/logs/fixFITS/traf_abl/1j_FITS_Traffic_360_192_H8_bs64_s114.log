Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=138, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j192_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j192_H8_FITS_custom_ftM_sl360_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11729
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=138, out_features=211, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3212763648.0
params:  29329.0
Trainable parameters:  29329
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 104.62113690376282
Epoch: 1, Steps: 91 | Train Loss: 0.9229716 Vali Loss: 0.7983136 Test Loss: 0.9291819
Validation loss decreased (inf --> 0.798314).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 95.02357316017151
Epoch: 2, Steps: 91 | Train Loss: 0.5323614 Vali Loss: 0.5586154 Test Loss: 0.6559420
Validation loss decreased (0.798314 --> 0.558615).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 111.98053431510925
Epoch: 3, Steps: 91 | Train Loss: 0.3854703 Vali Loss: 0.4432592 Test Loss: 0.5280248
Validation loss decreased (0.558615 --> 0.443259).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 93.96946406364441
Epoch: 4, Steps: 91 | Train Loss: 0.3145225 Vali Loss: 0.3885112 Test Loss: 0.4688529
Validation loss decreased (0.443259 --> 0.388511).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 126.83365511894226
Epoch: 5, Steps: 91 | Train Loss: 0.2811478 Vali Loss: 0.3626581 Test Loss: 0.4426484
Validation loss decreased (0.388511 --> 0.362658).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 104.2017273902893
Epoch: 6, Steps: 91 | Train Loss: 0.2661567 Vali Loss: 0.3511432 Test Loss: 0.4317854
Validation loss decreased (0.362658 --> 0.351143).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 97.8396954536438
Epoch: 7, Steps: 91 | Train Loss: 0.2596035 Vali Loss: 0.3461101 Test Loss: 0.4275049
Validation loss decreased (0.351143 --> 0.346110).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 117.61827445030212
Epoch: 8, Steps: 91 | Train Loss: 0.2566555 Vali Loss: 0.3439365 Test Loss: 0.4259024
Validation loss decreased (0.346110 --> 0.343936).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 98.33977484703064
Epoch: 9, Steps: 91 | Train Loss: 0.2553380 Vali Loss: 0.3430436 Test Loss: 0.4253228
Validation loss decreased (0.343936 --> 0.343044).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 91.68694567680359
Epoch: 10, Steps: 91 | Train Loss: 0.2546417 Vali Loss: 0.3420413 Test Loss: 0.4249948
Validation loss decreased (0.343044 --> 0.342041).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 91.33848738670349
Epoch: 11, Steps: 91 | Train Loss: 0.2542669 Vali Loss: 0.3415575 Test Loss: 0.4246990
Validation loss decreased (0.342041 --> 0.341558).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 92.73964190483093
Epoch: 12, Steps: 91 | Train Loss: 0.2540031 Vali Loss: 0.3412009 Test Loss: 0.4246624
Validation loss decreased (0.341558 --> 0.341201).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 93.30272388458252
Epoch: 13, Steps: 91 | Train Loss: 0.2538646 Vali Loss: 0.3412620 Test Loss: 0.4245261
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 94.34476923942566
Epoch: 14, Steps: 91 | Train Loss: 0.2536413 Vali Loss: 0.3413019 Test Loss: 0.4244612
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 121.99462747573853
Epoch: 15, Steps: 91 | Train Loss: 0.2537188 Vali Loss: 0.3408628 Test Loss: 0.4243401
Validation loss decreased (0.341201 --> 0.340863).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 93.91582560539246
Epoch: 16, Steps: 91 | Train Loss: 0.2535850 Vali Loss: 0.3411757 Test Loss: 0.4243216
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 98.65758633613586
Epoch: 17, Steps: 91 | Train Loss: 0.2534683 Vali Loss: 0.3410590 Test Loss: 0.4242230
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 92.81245040893555
Epoch: 18, Steps: 91 | Train Loss: 0.2533834 Vali Loss: 0.3410928 Test Loss: 0.4241865
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 94.71612310409546
Epoch: 19, Steps: 91 | Train Loss: 0.2534132 Vali Loss: 0.3404695 Test Loss: 0.4241080
Validation loss decreased (0.340863 --> 0.340469).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 93.67311692237854
Epoch: 20, Steps: 91 | Train Loss: 0.2533109 Vali Loss: 0.3406767 Test Loss: 0.4241381
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 88.26667475700378
Epoch: 21, Steps: 91 | Train Loss: 0.2532248 Vali Loss: 0.3403440 Test Loss: 0.4240916
Validation loss decreased (0.340469 --> 0.340344).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 86.73962879180908
Epoch: 22, Steps: 91 | Train Loss: 0.2532825 Vali Loss: 0.3404485 Test Loss: 0.4239809
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 84.32526326179504
Epoch: 23, Steps: 91 | Train Loss: 0.2532581 Vali Loss: 0.3411362 Test Loss: 0.4239191
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 85.99307298660278
Epoch: 24, Steps: 91 | Train Loss: 0.2531143 Vali Loss: 0.3402738 Test Loss: 0.4239519
Validation loss decreased (0.340344 --> 0.340274).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 90.59159088134766
Epoch: 25, Steps: 91 | Train Loss: 0.2531887 Vali Loss: 0.3404123 Test Loss: 0.4239050
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 97.29665112495422
Epoch: 26, Steps: 91 | Train Loss: 0.2531734 Vali Loss: 0.3405298 Test Loss: 0.4238378
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 95.60901522636414
Epoch: 27, Steps: 91 | Train Loss: 0.2531215 Vali Loss: 0.3407576 Test Loss: 0.4238139
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 94.21665716171265
Epoch: 28, Steps: 91 | Train Loss: 0.2530730 Vali Loss: 0.3408600 Test Loss: 0.4238431
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 95.1111626625061
Epoch: 29, Steps: 91 | Train Loss: 0.2531068 Vali Loss: 0.3407281 Test Loss: 0.4237441
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 97.7872428894043
Epoch: 30, Steps: 91 | Train Loss: 0.2530530 Vali Loss: 0.3404511 Test Loss: 0.4237522
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 96.093026638031
Epoch: 31, Steps: 91 | Train Loss: 0.2530914 Vali Loss: 0.3404367 Test Loss: 0.4237238
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 98.39832854270935
Epoch: 32, Steps: 91 | Train Loss: 0.2530561 Vali Loss: 0.3404621 Test Loss: 0.4237615
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 94.77462983131409
Epoch: 33, Steps: 91 | Train Loss: 0.2529782 Vali Loss: 0.3402682 Test Loss: 0.4237514
Validation loss decreased (0.340274 --> 0.340268).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 99.53475832939148
Epoch: 34, Steps: 91 | Train Loss: 0.2530191 Vali Loss: 0.3404352 Test Loss: 0.4237127
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 100.55579376220703
Epoch: 35, Steps: 91 | Train Loss: 0.2529090 Vali Loss: 0.3401326 Test Loss: 0.4237021
Validation loss decreased (0.340268 --> 0.340133).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 94.7377758026123
Epoch: 36, Steps: 91 | Train Loss: 0.2529698 Vali Loss: 0.3407453 Test Loss: 0.4236543
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 95.293301820755
Epoch: 37, Steps: 91 | Train Loss: 0.2529077 Vali Loss: 0.3402473 Test Loss: 0.4236772
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 89.57580399513245
Epoch: 38, Steps: 91 | Train Loss: 0.2529952 Vali Loss: 0.3402698 Test Loss: 0.4236836
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 93.74448609352112
Epoch: 39, Steps: 91 | Train Loss: 0.2529685 Vali Loss: 0.3405662 Test Loss: 0.4236583
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 88.5003502368927
Epoch: 40, Steps: 91 | Train Loss: 0.2529523 Vali Loss: 0.3401147 Test Loss: 0.4236359
Validation loss decreased (0.340133 --> 0.340115).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 85.97847771644592
Epoch: 41, Steps: 91 | Train Loss: 0.2529031 Vali Loss: 0.3402677 Test Loss: 0.4236431
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 85.78957796096802
Epoch: 42, Steps: 91 | Train Loss: 0.2528538 Vali Loss: 0.3405581 Test Loss: 0.4236397
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 86.03598308563232
Epoch: 43, Steps: 91 | Train Loss: 0.2529251 Vali Loss: 0.3404363 Test Loss: 0.4236290
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 82.94150185585022
Epoch: 44, Steps: 91 | Train Loss: 0.2528580 Vali Loss: 0.3401670 Test Loss: 0.4236163
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 72.0034384727478
Epoch: 45, Steps: 91 | Train Loss: 0.2528623 Vali Loss: 0.3405348 Test Loss: 0.4236320
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 70.71546292304993
Epoch: 46, Steps: 91 | Train Loss: 0.2528841 Vali Loss: 0.3400307 Test Loss: 0.4236058
Validation loss decreased (0.340115 --> 0.340031).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 70.0285120010376
Epoch: 47, Steps: 91 | Train Loss: 0.2528893 Vali Loss: 0.3402193 Test Loss: 0.4235686
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 74.49419212341309
Epoch: 48, Steps: 91 | Train Loss: 0.2528844 Vali Loss: 0.3403604 Test Loss: 0.4235922
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 67.2043662071228
Epoch: 49, Steps: 91 | Train Loss: 0.2528996 Vali Loss: 0.3402276 Test Loss: 0.4235914
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 68.6486701965332
Epoch: 50, Steps: 91 | Train Loss: 0.2527767 Vali Loss: 0.3406474 Test Loss: 0.4235725
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 63.071455240249634
Epoch: 51, Steps: 91 | Train Loss: 0.2527269 Vali Loss: 0.3409765 Test Loss: 0.4235623
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 60.54270839691162
Epoch: 52, Steps: 91 | Train Loss: 0.2528904 Vali Loss: 0.3403382 Test Loss: 0.4235646
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 64.73594427108765
Epoch: 53, Steps: 91 | Train Loss: 0.2528207 Vali Loss: 0.3402689 Test Loss: 0.4235692
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 69.18225359916687
Epoch: 54, Steps: 91 | Train Loss: 0.2527618 Vali Loss: 0.3401913 Test Loss: 0.4235682
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 67.58500170707703
Epoch: 55, Steps: 91 | Train Loss: 0.2527859 Vali Loss: 0.3397803 Test Loss: 0.4235665
Validation loss decreased (0.340031 --> 0.339780).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 68.53621172904968
Epoch: 56, Steps: 91 | Train Loss: 0.2527957 Vali Loss: 0.3397764 Test Loss: 0.4235409
Validation loss decreased (0.339780 --> 0.339776).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 67.7458963394165
Epoch: 57, Steps: 91 | Train Loss: 0.2527721 Vali Loss: 0.3400521 Test Loss: 0.4235581
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 68.01945209503174
Epoch: 58, Steps: 91 | Train Loss: 0.2528013 Vali Loss: 0.3403016 Test Loss: 0.4235553
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 66.1032965183258
Epoch: 59, Steps: 91 | Train Loss: 0.2527510 Vali Loss: 0.3404917 Test Loss: 0.4235392
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 72.4983057975769
Epoch: 60, Steps: 91 | Train Loss: 0.2527791 Vali Loss: 0.3407266 Test Loss: 0.4235474
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 68.63434481620789
Epoch: 61, Steps: 91 | Train Loss: 0.2528427 Vali Loss: 0.3403950 Test Loss: 0.4235578
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 66.72966289520264
Epoch: 62, Steps: 91 | Train Loss: 0.2528338 Vali Loss: 0.3402891 Test Loss: 0.4235473
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 69.98794460296631
Epoch: 63, Steps: 91 | Train Loss: 0.2527968 Vali Loss: 0.3404508 Test Loss: 0.4235452
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 71.26082348823547
Epoch: 64, Steps: 91 | Train Loss: 0.2528368 Vali Loss: 0.3404327 Test Loss: 0.4235378
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 74.19882678985596
Epoch: 65, Steps: 91 | Train Loss: 0.2527995 Vali Loss: 0.3404801 Test Loss: 0.4235334
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 68.74864220619202
Epoch: 66, Steps: 91 | Train Loss: 0.2528307 Vali Loss: 0.3402021 Test Loss: 0.4235320
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j192_H8_FITS_custom_ftM_sl360_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4225165843963623, mae:0.284893274307251, rse:0.5364767909049988, corr:[0.2772213  0.28668728 0.288564   0.28703302 0.2896404  0.2895853
 0.28961325 0.29095218 0.2898616  0.28967264 0.29004446 0.2887753
 0.28890896 0.2889961  0.2879202  0.28819427 0.28827104 0.28781167
 0.28827044 0.28828618 0.2880871  0.2882261  0.2877386  0.28810465
 0.2897742  0.2899435  0.28998953 0.29010043 0.28945494 0.2892656
 0.28929397 0.2887258  0.28876078 0.28888077 0.28834054 0.2884084
 0.2884488  0.2879249  0.2881797  0.28837183 0.288086   0.28836796
 0.28835374 0.2880156  0.2882165  0.28812978 0.2879363  0.2883727
 0.28878832 0.28876862 0.28896803 0.288657   0.28837314 0.28841183
 0.28799215 0.2877302  0.2877984  0.28748232 0.2874637  0.28761545
 0.28724372 0.2872936  0.28760582 0.28736398 0.28749388 0.28776738
 0.28747272 0.2875431  0.28767866 0.28742278 0.28752282 0.287623
 0.2874262  0.28758487 0.28753594 0.28720182 0.28726867 0.28707075
 0.2867408  0.28685573 0.2866966  0.28652117 0.28685078 0.28678533
 0.2865745  0.28680348 0.28678313 0.28671762 0.2868973  0.28670883
 0.28667653 0.286953   0.28674275 0.28663445 0.286802   0.28666174
 0.28655022 0.28660163 0.2862665  0.28627735 0.2863379  0.2859729
 0.2860188  0.28614706 0.28592467 0.28606772 0.2862089  0.2859597
 0.28603014 0.28615922 0.2860826  0.28621623 0.28617465 0.28603384
 0.28618234 0.2860336  0.28579706 0.28599486 0.28602386 0.28599703
 0.2861006  0.28599346 0.28598636 0.28631586 0.28624332 0.2861069
 0.28621313 0.28604704 0.28607294 0.2861711  0.28585514 0.2859797
 0.28630608 0.28606406 0.28614655 0.2864206  0.28618374 0.2862219
 0.2865397  0.2863879  0.28634244 0.2865295  0.28659528 0.2870103
 0.28730705 0.28731903 0.2875746  0.28762156 0.28746277 0.28764898
 0.28757197 0.28741914 0.28758594 0.28738075 0.28736684 0.2877312
 0.28743896 0.28736445 0.28786698 0.28771257 0.28767726 0.28811702
 0.28787777 0.28778985 0.28811595 0.28782794 0.2876836  0.28829813
 0.2890115  0.2888759  0.28877532 0.28846368 0.28861347 0.28838274
 0.2878356  0.28823093 0.28819692 0.28787917 0.28855616 0.28831106
 0.2880699  0.28901762 0.28871587 0.2887011  0.2894479  0.2883393
 0.28825727 0.28820637 0.28595623 0.28696612 0.28585508 0.28959924]
