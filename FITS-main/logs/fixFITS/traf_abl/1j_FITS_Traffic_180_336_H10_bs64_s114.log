Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j336_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j336_H10_FITS_custom_ftM_sl180_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11765
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=90, out_features=258, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2562001920.0
params:  23478.0
Trainable parameters:  23478
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 132.81513738632202
Epoch: 1, Steps: 91 | Train Loss: 1.1339907 Vali Loss: 0.9233103 Test Loss: 1.1096420
Validation loss decreased (inf --> 0.923310).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 139.05115151405334
Epoch: 2, Steps: 91 | Train Loss: 0.5939214 Vali Loss: 0.6344517 Test Loss: 0.7740713
Validation loss decreased (0.923310 --> 0.634452).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 132.55131340026855
Epoch: 3, Steps: 91 | Train Loss: 0.4504943 Vali Loss: 0.5381285 Test Loss: 0.6616625
Validation loss decreased (0.634452 --> 0.538128).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 133.1893949508667
Epoch: 4, Steps: 91 | Train Loss: 0.3939544 Vali Loss: 0.4898903 Test Loss: 0.6037328
Validation loss decreased (0.538128 --> 0.489890).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 131.52620792388916
Epoch: 5, Steps: 91 | Train Loss: 0.3623846 Vali Loss: 0.4596113 Test Loss: 0.5670933
Validation loss decreased (0.489890 --> 0.459611).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 124.20999360084534
Epoch: 6, Steps: 91 | Train Loss: 0.3419278 Vali Loss: 0.4396461 Test Loss: 0.5422626
Validation loss decreased (0.459611 --> 0.439646).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 120.88532781600952
Epoch: 7, Steps: 91 | Train Loss: 0.3278846 Vali Loss: 0.4257568 Test Loss: 0.5251295
Validation loss decreased (0.439646 --> 0.425757).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 127.34145975112915
Epoch: 8, Steps: 91 | Train Loss: 0.3181072 Vali Loss: 0.4157495 Test Loss: 0.5130356
Validation loss decreased (0.425757 --> 0.415750).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 114.4744303226471
Epoch: 9, Steps: 91 | Train Loss: 0.3112323 Vali Loss: 0.4076109 Test Loss: 0.5045644
Validation loss decreased (0.415750 --> 0.407611).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 119.83380722999573
Epoch: 10, Steps: 91 | Train Loss: 0.3061029 Vali Loss: 0.4029846 Test Loss: 0.4983427
Validation loss decreased (0.407611 --> 0.402985).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 111.77559757232666
Epoch: 11, Steps: 91 | Train Loss: 0.3025221 Vali Loss: 0.3988794 Test Loss: 0.4938173
Validation loss decreased (0.402985 --> 0.398879).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 114.54442429542542
Epoch: 12, Steps: 91 | Train Loss: 0.2996387 Vali Loss: 0.3960969 Test Loss: 0.4904844
Validation loss decreased (0.398879 --> 0.396097).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 111.95808482170105
Epoch: 13, Steps: 91 | Train Loss: 0.2976600 Vali Loss: 0.3938259 Test Loss: 0.4879919
Validation loss decreased (0.396097 --> 0.393826).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 100.16712927818298
Epoch: 14, Steps: 91 | Train Loss: 0.2961055 Vali Loss: 0.3919623 Test Loss: 0.4860139
Validation loss decreased (0.393826 --> 0.391962).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 103.13641333580017
Epoch: 15, Steps: 91 | Train Loss: 0.2949267 Vali Loss: 0.3907768 Test Loss: 0.4845905
Validation loss decreased (0.391962 --> 0.390777).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 101.21956586837769
Epoch: 16, Steps: 91 | Train Loss: 0.2940124 Vali Loss: 0.3898977 Test Loss: 0.4833543
Validation loss decreased (0.390777 --> 0.389898).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 104.39829230308533
Epoch: 17, Steps: 91 | Train Loss: 0.2932301 Vali Loss: 0.3886533 Test Loss: 0.4825174
Validation loss decreased (0.389898 --> 0.388653).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 104.0242908000946
Epoch: 18, Steps: 91 | Train Loss: 0.2925754 Vali Loss: 0.3879224 Test Loss: 0.4816765
Validation loss decreased (0.388653 --> 0.387922).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 99.47578310966492
Epoch: 19, Steps: 91 | Train Loss: 0.2921093 Vali Loss: 0.3880286 Test Loss: 0.4811586
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 104.56713199615479
Epoch: 20, Steps: 91 | Train Loss: 0.2916856 Vali Loss: 0.3872634 Test Loss: 0.4806434
Validation loss decreased (0.387922 --> 0.387263).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 103.52092790603638
Epoch: 21, Steps: 91 | Train Loss: 0.2913822 Vali Loss: 0.3869701 Test Loss: 0.4802836
Validation loss decreased (0.387263 --> 0.386970).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 92.13988447189331
Epoch: 22, Steps: 91 | Train Loss: 0.2910893 Vali Loss: 0.3865690 Test Loss: 0.4800111
Validation loss decreased (0.386970 --> 0.386569).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 92.63014817237854
Epoch: 23, Steps: 91 | Train Loss: 0.2909419 Vali Loss: 0.3861214 Test Loss: 0.4797194
Validation loss decreased (0.386569 --> 0.386121).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 78.22936534881592
Epoch: 24, Steps: 91 | Train Loss: 0.2907565 Vali Loss: 0.3864368 Test Loss: 0.4794632
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 62.68372464179993
Epoch: 25, Steps: 91 | Train Loss: 0.2907046 Vali Loss: 0.3861119 Test Loss: 0.4793169
Validation loss decreased (0.386121 --> 0.386112).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 68.33241248130798
Epoch: 26, Steps: 91 | Train Loss: 0.2905935 Vali Loss: 0.3856691 Test Loss: 0.4791553
Validation loss decreased (0.386112 --> 0.385669).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 66.71120977401733
Epoch: 27, Steps: 91 | Train Loss: 0.2902856 Vali Loss: 0.3857260 Test Loss: 0.4790197
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 63.231181144714355
Epoch: 28, Steps: 91 | Train Loss: 0.2902947 Vali Loss: 0.3857165 Test Loss: 0.4789398
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 66.88900637626648
Epoch: 29, Steps: 91 | Train Loss: 0.2902441 Vali Loss: 0.3856870 Test Loss: 0.4788225
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 65.81353807449341
Epoch: 30, Steps: 91 | Train Loss: 0.2902567 Vali Loss: 0.3853088 Test Loss: 0.4786975
Validation loss decreased (0.385669 --> 0.385309).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 68.65226888656616
Epoch: 31, Steps: 91 | Train Loss: 0.2900436 Vali Loss: 0.3853210 Test Loss: 0.4786772
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 79.27085947990417
Epoch: 32, Steps: 91 | Train Loss: 0.2901026 Vali Loss: 0.3852279 Test Loss: 0.4786030
Validation loss decreased (0.385309 --> 0.385228).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 81.74414229393005
Epoch: 33, Steps: 91 | Train Loss: 0.2899896 Vali Loss: 0.3853682 Test Loss: 0.4785828
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 79.65139079093933
Epoch: 34, Steps: 91 | Train Loss: 0.2899727 Vali Loss: 0.3852141 Test Loss: 0.4785545
Validation loss decreased (0.385228 --> 0.385214).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 71.3493070602417
Epoch: 35, Steps: 91 | Train Loss: 0.2899261 Vali Loss: 0.3853314 Test Loss: 0.4784839
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 71.64143395423889
Epoch: 36, Steps: 91 | Train Loss: 0.2898651 Vali Loss: 0.3851084 Test Loss: 0.4784876
Validation loss decreased (0.385214 --> 0.385108).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 71.24980521202087
Epoch: 37, Steps: 91 | Train Loss: 0.2897605 Vali Loss: 0.3852693 Test Loss: 0.4784736
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 62.32227444648743
Epoch: 38, Steps: 91 | Train Loss: 0.2898409 Vali Loss: 0.3850366 Test Loss: 0.4784642
Validation loss decreased (0.385108 --> 0.385037).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 67.4581778049469
Epoch: 39, Steps: 91 | Train Loss: 0.2897389 Vali Loss: 0.3848754 Test Loss: 0.4784161
Validation loss decreased (0.385037 --> 0.384875).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 64.93235778808594
Epoch: 40, Steps: 91 | Train Loss: 0.2897252 Vali Loss: 0.3852076 Test Loss: 0.4783465
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 65.68480086326599
Epoch: 41, Steps: 91 | Train Loss: 0.2897020 Vali Loss: 0.3852158 Test Loss: 0.4783397
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 59.72796964645386
Epoch: 42, Steps: 91 | Train Loss: 0.2898434 Vali Loss: 0.3851989 Test Loss: 0.4783340
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 57.88011932373047
Epoch: 43, Steps: 91 | Train Loss: 0.2896962 Vali Loss: 0.3847607 Test Loss: 0.4783248
Validation loss decreased (0.384875 --> 0.384761).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 56.080323934555054
Epoch: 44, Steps: 91 | Train Loss: 0.2897843 Vali Loss: 0.3852400 Test Loss: 0.4782584
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 61.75534915924072
Epoch: 45, Steps: 91 | Train Loss: 0.2896550 Vali Loss: 0.3850317 Test Loss: 0.4782957
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 59.099934339523315
Epoch: 46, Steps: 91 | Train Loss: 0.2896626 Vali Loss: 0.3848448 Test Loss: 0.4782674
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 91.31007432937622
Epoch: 47, Steps: 91 | Train Loss: 0.2896215 Vali Loss: 0.3848250 Test Loss: 0.4782260
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 96.00533175468445
Epoch: 48, Steps: 91 | Train Loss: 0.2896518 Vali Loss: 0.3853603 Test Loss: 0.4782597
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 89.35934519767761
Epoch: 49, Steps: 91 | Train Loss: 0.2896445 Vali Loss: 0.3846946 Test Loss: 0.4782727
Validation loss decreased (0.384761 --> 0.384695).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 88.02038264274597
Epoch: 50, Steps: 91 | Train Loss: 0.2896832 Vali Loss: 0.3851173 Test Loss: 0.4782245
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 90.19449973106384
Epoch: 51, Steps: 91 | Train Loss: 0.2896734 Vali Loss: 0.3850670 Test Loss: 0.4782390
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 87.35647916793823
Epoch: 52, Steps: 91 | Train Loss: 0.2896946 Vali Loss: 0.3849886 Test Loss: 0.4782523
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 84.89342856407166
Epoch: 53, Steps: 91 | Train Loss: 0.2897012 Vali Loss: 0.3849777 Test Loss: 0.4782592
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 86.14245367050171
Epoch: 54, Steps: 91 | Train Loss: 0.2896481 Vali Loss: 0.3846605 Test Loss: 0.4782549
Validation loss decreased (0.384695 --> 0.384661).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 87.60126686096191
Epoch: 55, Steps: 91 | Train Loss: 0.2896435 Vali Loss: 0.3849788 Test Loss: 0.4782483
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 92.69356393814087
Epoch: 56, Steps: 91 | Train Loss: 0.2895524 Vali Loss: 0.3848261 Test Loss: 0.4782144
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 87.00454759597778
Epoch: 57, Steps: 91 | Train Loss: 0.2895392 Vali Loss: 0.3846302 Test Loss: 0.4782255
Validation loss decreased (0.384661 --> 0.384630).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 88.81560516357422
Epoch: 58, Steps: 91 | Train Loss: 0.2896150 Vali Loss: 0.3850433 Test Loss: 0.4782048
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 89.82466912269592
Epoch: 59, Steps: 91 | Train Loss: 0.2896129 Vali Loss: 0.3849726 Test Loss: 0.4782064
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 83.66891026496887
Epoch: 60, Steps: 91 | Train Loss: 0.2896380 Vali Loss: 0.3848710 Test Loss: 0.4782066
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 83.7774748802185
Epoch: 61, Steps: 91 | Train Loss: 0.2896042 Vali Loss: 0.3849323 Test Loss: 0.4782003
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 77.75199341773987
Epoch: 62, Steps: 91 | Train Loss: 0.2894360 Vali Loss: 0.3850527 Test Loss: 0.4781843
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 79.82042694091797
Epoch: 63, Steps: 91 | Train Loss: 0.2895966 Vali Loss: 0.3847591 Test Loss: 0.4781943
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 78.74019002914429
Epoch: 64, Steps: 91 | Train Loss: 0.2894215 Vali Loss: 0.3848258 Test Loss: 0.4782116
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 78.06536841392517
Epoch: 65, Steps: 91 | Train Loss: 0.2896222 Vali Loss: 0.3847897 Test Loss: 0.4781933
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 78.5462532043457
Epoch: 66, Steps: 91 | Train Loss: 0.2895776 Vali Loss: 0.3849890 Test Loss: 0.4781979
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 84.697589635849
Epoch: 67, Steps: 91 | Train Loss: 0.2896258 Vali Loss: 0.3847916 Test Loss: 0.4781782
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j336_H10_FITS_custom_ftM_sl180_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4762483537197113, mae:0.3082984983921051, rse:0.5671736001968384, corr:[0.26673403 0.28235698 0.2799246  0.281551   0.28061464 0.28194833
 0.2814029  0.2819995  0.28175807 0.28189188 0.2812629  0.2811263
 0.28075638 0.28078356 0.28058872 0.2803336  0.28020886 0.2799015
 0.27992126 0.27994558 0.28030202 0.28057146 0.2813255  0.28129265
 0.28160277 0.2810137  0.2808879  0.2803976  0.28039497 0.28050092
 0.28049284 0.2807856  0.28064504 0.28067157 0.2804126  0.28033623
 0.27995142 0.28002003 0.2799853  0.28020936 0.28022096 0.28028443
 0.2802596  0.28036737 0.28025413 0.28027934 0.28035134 0.28036588
 0.28053528 0.2801191  0.28041437 0.28008196 0.2801752  0.28021047
 0.28020397 0.28049535 0.2804821  0.28050375 0.28056687 0.2804618
 0.2803119  0.2803171  0.2801422  0.2801527  0.28013316 0.28014576
 0.28012058 0.2801244  0.27998912 0.28020403 0.28021067 0.28034183
 0.28018653 0.27976993 0.2798451  0.27958784 0.2796871  0.27955344
 0.27947158 0.27962172 0.27969292 0.2798089  0.27981406 0.2796482
 0.27958462 0.2795442  0.27953377 0.27973574 0.27965865 0.2797504
 0.27964562 0.27971473 0.27966636 0.27977028 0.27981356 0.2797953
 0.27967033 0.27961537 0.2796101  0.2794378  0.27946675 0.2792097
 0.27920672 0.2792555  0.27932203 0.27937835 0.27934164 0.27929035
 0.2794134  0.2793759  0.27916437 0.27935666 0.27923107 0.27919257
 0.27897742 0.27890804 0.27874395 0.27866778 0.2787332  0.27891436
 0.27913564 0.27908796 0.2792643  0.27923045 0.27931997 0.27910852
 0.27898937 0.27883336 0.27872154 0.27881145 0.2790714  0.2792412
 0.27966854 0.2799339  0.27954546 0.27938306 0.2792171  0.27916068
 0.27914113 0.27915576 0.2793714  0.279812   0.28033277 0.28066033
 0.2804953  0.280339   0.28007373 0.28002414 0.2801731  0.27997017
 0.27986172 0.2798282  0.27980238 0.2799618  0.28055435 0.28115448
 0.28236702 0.28273335 0.28204468 0.28150135 0.28106675 0.28079352
 0.2806827  0.28091538 0.28152356 0.2825627  0.28344643 0.28382286
 0.28299743 0.28244147 0.28179565 0.28142637 0.28136632 0.28142515
 0.28153726 0.28172764 0.2817365  0.28170073 0.28182787 0.28179651
 0.28196403 0.28186387 0.28147942 0.2813226  0.28101873 0.28081614
 0.28060052 0.28060818 0.2807346  0.28121144 0.28170088 0.28174058
 0.28135225 0.2810402  0.2807797  0.28057414 0.2806409  0.28079027
 0.2809475  0.28097105 0.28087074 0.280908   0.2809012  0.28072307
 0.28040913 0.28028417 0.28022048 0.28032175 0.28035134 0.2801429
 0.2798985  0.27983984 0.2797795  0.27981254 0.27982017 0.28008318
 0.28007016 0.27985254 0.27980667 0.27976796 0.2798689  0.27997166
 0.28004926 0.27991992 0.27987602 0.27990556 0.28011262 0.28001773
 0.27992034 0.279874   0.27981207 0.27979767 0.27964675 0.2795153
 0.2793741  0.2793625  0.2792774  0.27967903 0.27966735 0.27975246
 0.27959284 0.27948278 0.27960238 0.27963302 0.27977785 0.27950615
 0.27956465 0.279575   0.27950257 0.27933794 0.27954942 0.2794118
 0.27910885 0.2791609  0.2792084  0.27922764 0.27906516 0.27907705
 0.278938   0.27897933 0.27884692 0.2790226  0.27895832 0.2789597
 0.279031   0.27907977 0.27925467 0.27925816 0.27926466 0.27907887
 0.27896437 0.2787446  0.27860954 0.27844295 0.27853274 0.2784451
 0.27848306 0.27828127 0.27838236 0.2784193  0.27831206 0.2783831
 0.27835286 0.27862665 0.2784728  0.27844483 0.278271   0.27845255
 0.27841535 0.27864796 0.27879474 0.27858782 0.27857888 0.27813795
 0.27791575 0.27756196 0.27745396 0.27762103 0.2780047  0.27820605
 0.2787254  0.278906   0.27894798 0.27894726 0.2789361  0.27897882
 0.2789598  0.27934197 0.27957764 0.27970725 0.279804   0.27999604
 0.27944288 0.27952448 0.2792983  0.27919608 0.27930677 0.27893713
 0.2787083  0.278431   0.2783953  0.27918646 0.27960187 0.2806684
 0.2821843  0.2830781  0.28302142 0.2822837  0.28189358 0.28063795
 0.28070047 0.27928236 0.27964318 0.27737406 0.28147337 0.2820521 ]
