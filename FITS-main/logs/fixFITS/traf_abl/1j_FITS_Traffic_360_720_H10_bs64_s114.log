Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=170, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j720_H10_FITS_custom_ftM_sl360_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11201
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=170, out_features=510, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9566131200.0
params:  87210.0
Trainable parameters:  87210
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 77.77836728096008
Epoch: 1, Steps: 87 | Train Loss: 1.1665259 Vali Loss: 0.9733073 Test Loss: 1.1437756
Validation loss decreased (inf --> 0.973307).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 79.16280579566956
Epoch: 2, Steps: 87 | Train Loss: 0.6835201 Vali Loss: 0.7674196 Test Loss: 0.8923051
Validation loss decreased (0.973307 --> 0.767420).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 82.52357125282288
Epoch: 3, Steps: 87 | Train Loss: 0.5568459 Vali Loss: 0.6598752 Test Loss: 0.7636261
Validation loss decreased (0.767420 --> 0.659875).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 76.86926579475403
Epoch: 4, Steps: 87 | Train Loss: 0.4786250 Vali Loss: 0.5865645 Test Loss: 0.6769500
Validation loss decreased (0.659875 --> 0.586565).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 75.90265679359436
Epoch: 5, Steps: 87 | Train Loss: 0.4239378 Vali Loss: 0.5339979 Test Loss: 0.6156749
Validation loss decreased (0.586565 --> 0.533998).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 76.5968849658966
Epoch: 6, Steps: 87 | Train Loss: 0.3846492 Vali Loss: 0.4958552 Test Loss: 0.5715772
Validation loss decreased (0.533998 --> 0.495855).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 76.92405843734741
Epoch: 7, Steps: 87 | Train Loss: 0.3562016 Vali Loss: 0.4689603 Test Loss: 0.5401395
Validation loss decreased (0.495855 --> 0.468960).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 79.67606616020203
Epoch: 8, Steps: 87 | Train Loss: 0.3357182 Vali Loss: 0.4487458 Test Loss: 0.5175207
Validation loss decreased (0.468960 --> 0.448746).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 116.04807901382446
Epoch: 9, Steps: 87 | Train Loss: 0.3208953 Vali Loss: 0.4348393 Test Loss: 0.5015444
Validation loss decreased (0.448746 --> 0.434839).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 122.49641299247742
Epoch: 10, Steps: 87 | Train Loss: 0.3102738 Vali Loss: 0.4243243 Test Loss: 0.4901303
Validation loss decreased (0.434839 --> 0.424324).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 113.1302559375763
Epoch: 11, Steps: 87 | Train Loss: 0.3026253 Vali Loss: 0.4170139 Test Loss: 0.4817146
Validation loss decreased (0.424324 --> 0.417014).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 132.73429822921753
Epoch: 12, Steps: 87 | Train Loss: 0.2971781 Vali Loss: 0.4119248 Test Loss: 0.4762800
Validation loss decreased (0.417014 --> 0.411925).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 134.2464678287506
Epoch: 13, Steps: 87 | Train Loss: 0.2932756 Vali Loss: 0.4084778 Test Loss: 0.4723795
Validation loss decreased (0.411925 --> 0.408478).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 122.68862867355347
Epoch: 14, Steps: 87 | Train Loss: 0.2905208 Vali Loss: 0.4056353 Test Loss: 0.4696271
Validation loss decreased (0.408478 --> 0.405635).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 125.48324012756348
Epoch: 15, Steps: 87 | Train Loss: 0.2885477 Vali Loss: 0.4036013 Test Loss: 0.4676044
Validation loss decreased (0.405635 --> 0.403601).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 129.32839632034302
Epoch: 16, Steps: 87 | Train Loss: 0.2871061 Vali Loss: 0.4019934 Test Loss: 0.4663440
Validation loss decreased (0.403601 --> 0.401993).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 193.63250827789307
Epoch: 17, Steps: 87 | Train Loss: 0.2861128 Vali Loss: 0.4011262 Test Loss: 0.4653477
Validation loss decreased (0.401993 --> 0.401126).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 178.95840096473694
Epoch: 18, Steps: 87 | Train Loss: 0.2853488 Vali Loss: 0.4005980 Test Loss: 0.4648137
Validation loss decreased (0.401126 --> 0.400598).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 122.52201509475708
Epoch: 19, Steps: 87 | Train Loss: 0.2848518 Vali Loss: 0.4001595 Test Loss: 0.4643428
Validation loss decreased (0.400598 --> 0.400160).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 126.76680278778076
Epoch: 20, Steps: 87 | Train Loss: 0.2845422 Vali Loss: 0.3999072 Test Loss: 0.4639979
Validation loss decreased (0.400160 --> 0.399907).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 185.9691081047058
Epoch: 21, Steps: 87 | Train Loss: 0.2842262 Vali Loss: 0.3996498 Test Loss: 0.4638612
Validation loss decreased (0.399907 --> 0.399650).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 121.03468084335327
Epoch: 22, Steps: 87 | Train Loss: 0.2840640 Vali Loss: 0.3989393 Test Loss: 0.4637505
Validation loss decreased (0.399650 --> 0.398939).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 118.67403817176819
Epoch: 23, Steps: 87 | Train Loss: 0.2838879 Vali Loss: 0.3995793 Test Loss: 0.4635482
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 113.06044721603394
Epoch: 24, Steps: 87 | Train Loss: 0.2838121 Vali Loss: 0.3988393 Test Loss: 0.4635024
Validation loss decreased (0.398939 --> 0.398839).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 108.3218286037445
Epoch: 25, Steps: 87 | Train Loss: 0.2836973 Vali Loss: 0.3991409 Test Loss: 0.4635590
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 114.34620642662048
Epoch: 26, Steps: 87 | Train Loss: 0.2836289 Vali Loss: 0.3990776 Test Loss: 0.4634677
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 111.45491194725037
Epoch: 27, Steps: 87 | Train Loss: 0.2835294 Vali Loss: 0.3988332 Test Loss: 0.4634500
Validation loss decreased (0.398839 --> 0.398833).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 116.19514203071594
Epoch: 28, Steps: 87 | Train Loss: 0.2835338 Vali Loss: 0.3987263 Test Loss: 0.4634529
Validation loss decreased (0.398833 --> 0.398726).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 98.8786849975586
Epoch: 29, Steps: 87 | Train Loss: 0.2834864 Vali Loss: 0.3986986 Test Loss: 0.4634596
Validation loss decreased (0.398726 --> 0.398699).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 99.27993845939636
Epoch: 30, Steps: 87 | Train Loss: 0.2834767 Vali Loss: 0.3983628 Test Loss: 0.4634143
Validation loss decreased (0.398699 --> 0.398363).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 97.39578580856323
Epoch: 31, Steps: 87 | Train Loss: 0.2833860 Vali Loss: 0.3989670 Test Loss: 0.4634140
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 83.7177050113678
Epoch: 32, Steps: 87 | Train Loss: 0.2834546 Vali Loss: 0.3986911 Test Loss: 0.4634107
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 66.56302976608276
Epoch: 33, Steps: 87 | Train Loss: 0.2833891 Vali Loss: 0.3990821 Test Loss: 0.4634645
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 56.978034257888794
Epoch: 34, Steps: 87 | Train Loss: 0.2833737 Vali Loss: 0.3988748 Test Loss: 0.4634428
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 55.86878800392151
Epoch: 35, Steps: 87 | Train Loss: 0.2834891 Vali Loss: 0.3985113 Test Loss: 0.4634228
EarlyStopping counter: 5 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 56.5560188293457
Epoch: 36, Steps: 87 | Train Loss: 0.2834059 Vali Loss: 0.3986397 Test Loss: 0.4634223
EarlyStopping counter: 6 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 57.78258156776428
Epoch: 37, Steps: 87 | Train Loss: 0.2833628 Vali Loss: 0.3984681 Test Loss: 0.4634103
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 56.10341119766235
Epoch: 38, Steps: 87 | Train Loss: 0.2833632 Vali Loss: 0.3987710 Test Loss: 0.4634609
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 56.44604706764221
Epoch: 39, Steps: 87 | Train Loss: 0.2833278 Vali Loss: 0.3983062 Test Loss: 0.4633505
Validation loss decreased (0.398363 --> 0.398306).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 58.66891551017761
Epoch: 40, Steps: 87 | Train Loss: 0.2833996 Vali Loss: 0.3986818 Test Loss: 0.4633898
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 64.5589108467102
Epoch: 41, Steps: 87 | Train Loss: 0.2834030 Vali Loss: 0.3991611 Test Loss: 0.4633858
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 58.21779251098633
Epoch: 42, Steps: 87 | Train Loss: 0.2833606 Vali Loss: 0.3986836 Test Loss: 0.4633624
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 61.12045073509216
Epoch: 43, Steps: 87 | Train Loss: 0.2832021 Vali Loss: 0.3985730 Test Loss: 0.4634241
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 58.28019404411316
Epoch: 44, Steps: 87 | Train Loss: 0.2832782 Vali Loss: 0.3984841 Test Loss: 0.4633445
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 53.244096517562866
Epoch: 45, Steps: 87 | Train Loss: 0.2833053 Vali Loss: 0.3986951 Test Loss: 0.4634145
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 106.77176189422607
Epoch: 46, Steps: 87 | Train Loss: 0.2833435 Vali Loss: 0.3987571 Test Loss: 0.4633718
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 102.59665036201477
Epoch: 47, Steps: 87 | Train Loss: 0.2832941 Vali Loss: 0.3990271 Test Loss: 0.4633992
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 86.32934927940369
Epoch: 48, Steps: 87 | Train Loss: 0.2832684 Vali Loss: 0.3986612 Test Loss: 0.4633681
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 80.06200528144836
Epoch: 49, Steps: 87 | Train Loss: 0.2832451 Vali Loss: 0.3984495 Test Loss: 0.4633470
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j720_H10_FITS_custom_ftM_sl360_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4626426696777344, mae:0.3064292371273041, rse:0.5561906099319458, corr:[0.25315785 0.2641974  0.26429856 0.26544568 0.2668446  0.26709837
 0.26830405 0.26736388 0.26821744 0.2666523  0.2670555  0.26597187
 0.26596257 0.26564983 0.2649331  0.26543233 0.26520392 0.26621962
 0.2662666  0.26658446 0.26661965 0.26656988 0.26646078 0.2665677
 0.26809445 0.2682336  0.2683583  0.26779118 0.26782662 0.2674309
 0.26687053 0.2667043  0.26615816 0.26634035 0.26592118 0.2662117
 0.26587975 0.26600072 0.26630202 0.2662596  0.26688743 0.26684263
 0.26706967 0.26696151 0.26692063 0.26683903 0.2667216  0.2664714
 0.26698485 0.26736668 0.2668365  0.2668135  0.26661113 0.26627102
 0.26625988 0.26590756 0.26588494 0.2655594  0.26573583 0.26550475
 0.26547155 0.26581028 0.26566926 0.2659331  0.26606178 0.26637167
 0.26618636 0.26630163 0.2663272  0.26621374 0.26625857 0.26612929
 0.2662652  0.2660319  0.26604432 0.2658531  0.26548275 0.26532757
 0.26523733 0.26529345 0.26517835 0.26537547 0.26530296 0.26533383
 0.26553044 0.26539615 0.26569843 0.2657728  0.26607952 0.2660956
 0.2661243  0.26607215 0.26581135 0.26587233 0.2657025  0.26588055
 0.26559657 0.26554522 0.2654522  0.26512358 0.26509705 0.26483956
 0.26483142 0.26496324 0.2650447  0.2651167  0.2653799  0.26551178
 0.26528856 0.26543343 0.26552963 0.26571387 0.26589906 0.26569164
 0.26570815 0.2656519  0.26554543 0.265294   0.2652997  0.26517904
 0.2650469  0.2652547  0.2653674  0.26552817 0.26532683 0.26530468
 0.26519305 0.26532355 0.26542425 0.2653472  0.26559404 0.26568165
 0.26589844 0.2660357  0.26603362 0.26623613 0.26630515 0.26600802
 0.26605067 0.2659812  0.26587617 0.2656835  0.26569745 0.26608154
 0.26619282 0.2661889  0.26642478 0.26647112 0.26636052 0.26612762
 0.26619312 0.26650393 0.26647833 0.26661488 0.26666868 0.26678228
 0.26683688 0.26687163 0.26711    0.2672707  0.26731747 0.26728997
 0.26743057 0.26746958 0.26719353 0.2669117  0.2666902  0.26715487
 0.26827434 0.26815346 0.2677313  0.2674734  0.2673591  0.26719064
 0.267315   0.26729023 0.2673751  0.26753697 0.26772526 0.2679159
 0.2679229  0.26801634 0.26795945 0.2678623  0.2679206  0.26821628
 0.2683051  0.2681894  0.26792932 0.26766902 0.2675667  0.2674213
 0.26823062 0.2682787  0.2679946  0.26801547 0.26770636 0.26754615
 0.267581   0.26773477 0.26785725 0.26787278 0.26809287 0.2679986
 0.26794356 0.26778948 0.26782182 0.26790482 0.26788253 0.26784217
 0.26774964 0.26784894 0.26751778 0.2672843  0.26720938 0.26711354
 0.26729918 0.26730976 0.26747254 0.26742372 0.2673089  0.2670811
 0.26702183 0.2673168  0.26731533 0.26739258 0.26746294 0.2675774
 0.2675234  0.26733536 0.2674035  0.26737615 0.26753446 0.26752806
 0.26731673 0.26700863 0.2668999  0.2667634  0.26655963 0.26656687
 0.26667595 0.26681438 0.26678768 0.26679415 0.2668061  0.26676455
 0.26678813 0.26704514 0.26731136 0.26734993 0.26747537 0.26755425
 0.26743665 0.26745522 0.2673048  0.26724774 0.26736236 0.2673454
 0.2672032  0.26707008 0.26692757 0.2667572  0.26678446 0.26667532
 0.26666662 0.2667952  0.26669097 0.26667783 0.26672426 0.26667514
 0.2666299  0.26699215 0.26710597 0.2671356  0.2670907  0.267086
 0.26715928 0.26701277 0.2669665  0.2670878  0.2670897  0.2667522
 0.26664147 0.26667708 0.26659253 0.2664759  0.26635292 0.2664657
 0.2662441  0.2662314  0.26638216 0.26647234 0.26656953 0.2665076
 0.26668498 0.26670846 0.26680157 0.26696482 0.2669227  0.2671968
 0.26730695 0.2672034  0.26726958 0.26738086 0.2672996  0.2670842
 0.26713842 0.26707467 0.26707956 0.2669252  0.26699015 0.26743442
 0.26756343 0.26775005 0.26784623 0.2681042  0.26814017 0.26795754
 0.26817355 0.26832533 0.26847687 0.2686687  0.2686692  0.26855844
 0.2686166  0.26863396 0.26859763 0.26865682 0.2687577  0.2686827
 0.26861385 0.2686977  0.2685177  0.26824796 0.26800036 0.26841503
 0.26952243 0.26935294 0.26896483 0.26887414 0.2687525  0.26885924
 0.2688238  0.268885   0.2690211  0.26916414 0.26909682 0.26889336
 0.2688728  0.26876885 0.2688855  0.26890144 0.2688955  0.2689243
 0.26879814 0.26875085 0.2685653  0.26834452 0.26811978 0.26808932
 0.26873308 0.26886177 0.26882702 0.2688728  0.26890492 0.2688364
 0.2688384  0.26896024 0.26901332 0.26897994 0.26886958 0.2690532
 0.26898572 0.26884198 0.2688322  0.26885384 0.26879573 0.26867062
 0.26881835 0.26857558 0.26815695 0.26799503 0.26804963 0.2680237
 0.26827905 0.26834583 0.26830086 0.2684758  0.26853094 0.26853162
 0.2686581  0.26872703 0.26873505 0.26867047 0.2687156  0.2685202
 0.2682952  0.26837602 0.2682119  0.26806512 0.26810423 0.26823327
 0.2680241  0.26781127 0.2675279  0.26723263 0.26731864 0.2673504
 0.26756778 0.267621   0.26753    0.26762646 0.2677458  0.26769677
 0.26783225 0.26797676 0.26792872 0.2681031  0.26793995 0.2677762
 0.2678747  0.26773605 0.26767614 0.26767594 0.26777172 0.2675925
 0.2673903  0.2673771  0.26720265 0.26715437 0.26711375 0.26738104
 0.26752084 0.26763114 0.2675979  0.2675982  0.267648   0.2674098
 0.26759982 0.26783147 0.26807618 0.2680738  0.26784515 0.26784953
 0.2676987  0.26756254 0.2674891  0.267478   0.26735315 0.2670007
 0.2669366  0.26700944 0.26700085 0.2666602  0.26673228 0.26699412
 0.26696408 0.26722983 0.2673303  0.26764345 0.2676087  0.26749712
 0.26752892 0.26758066 0.26768106 0.26747262 0.26737568 0.26740214
 0.26751307 0.2675271  0.2673896  0.26722667 0.26727465 0.26722145
 0.26706707 0.2670194  0.26698095 0.26703694 0.26722908 0.26773039
 0.26808003 0.26823488 0.26845673 0.2686232  0.26873854 0.26865402
 0.26854563 0.26867867 0.2686943  0.26876274 0.26844534 0.2682937
 0.26836693 0.26828784 0.268342   0.2682963  0.26827267 0.26815802
 0.2682075  0.26823774 0.26805907 0.26795802 0.26786438 0.26825064
 0.2691021  0.2689789  0.26873893 0.26875603 0.2687776  0.26875684
 0.26878685 0.26862618 0.26863444 0.26870686 0.26841888 0.26813582
 0.26791054 0.26788855 0.26775318 0.26773232 0.26768616 0.26757976
 0.26760647 0.26758143 0.2674688  0.26745436 0.26758343 0.26741624
 0.26791885 0.26820788 0.26820457 0.2682405  0.26826715 0.26839036
 0.26843643 0.26837218 0.26832724 0.26821917 0.2683062  0.26813453
 0.26781282 0.2676838  0.26750085 0.26737258 0.2673892  0.26729867
 0.267179   0.2672738  0.26688355 0.2668293  0.26693383 0.2668547
 0.26716644 0.26716593 0.26722544 0.26746416 0.26779658 0.2677873
 0.2677574  0.26773342 0.26757142 0.26754183 0.267294   0.26712555
 0.26686144 0.26661882 0.26651314 0.26639196 0.26660046 0.2663889
 0.2664219  0.26645625 0.2663802  0.26645648 0.26632023 0.26652333
 0.26651546 0.26652923 0.2664018  0.2666023  0.2670504  0.26700342
 0.26687032 0.2666983  0.26668444 0.2664207  0.26628655 0.26604998
 0.2656455  0.2655713  0.26530287 0.2653344  0.2653301  0.26508018
 0.2649175  0.26494685 0.26503262 0.2649864  0.26538822 0.26557967
 0.2657861  0.2657645  0.26571634 0.2659703  0.2658152  0.265834
 0.26555732 0.26560876 0.26544318 0.26526266 0.26521602 0.26484457
 0.2647827  0.26452306 0.26447758 0.26424578 0.26423016 0.26416072
 0.26408404 0.26453114 0.26455688 0.26489773 0.26511106 0.26531008
 0.26549432 0.26552537 0.26575473 0.26569268 0.26598418 0.26560706
 0.26531723 0.26530784 0.26503375 0.26496035 0.2644262  0.2644517
 0.2641994  0.26431844 0.26440066 0.2643182  0.2643968  0.2642353
 0.26468655 0.2646616  0.26494208 0.26512152 0.26538786 0.2662963
 0.2665799  0.2668844  0.26696107 0.2671945  0.26710647 0.2667986
 0.26656267 0.26626056 0.26642314 0.2657982  0.26573625 0.2655188
 0.26544636 0.26548684 0.26538885 0.2655965  0.26535046 0.2659632
 0.26596043 0.26646653 0.2666766  0.26654187 0.26689616 0.26703835
 0.26838338 0.26821518 0.26807734 0.26792523 0.26756048 0.26741257
 0.26679656 0.2669085  0.2662511  0.266164   0.26601866 0.26582292
 0.26618224 0.2662006  0.26681966 0.26652214 0.26738992 0.2674182
 0.26810682 0.26862288 0.26827374 0.2689258  0.2682268  0.26836136
 0.26860604 0.26859465 0.26837298 0.26796657 0.26766375 0.26677218
 0.26683846 0.265743   0.26629966 0.26599684 0.26659483 0.26726606
 0.2673981  0.2689659  0.2688185  0.27078086 0.2695418  0.2706796
 0.26870033 0.26786917 0.2648534  0.26196745 0.26113304 0.2692036 ]
