Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=138, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j336_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j336_H8_FITS_custom_ftM_sl360_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11585
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=138, out_features=266, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4050213888.0
params:  36974.0
Trainable parameters:  36974
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 101.72377967834473
Epoch: 1, Steps: 90 | Train Loss: 1.1632327 Vali Loss: 1.2098607 Test Loss: 1.4287808
Validation loss decreased (inf --> 1.209861).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 88.644935131073
Epoch: 2, Steps: 90 | Train Loss: 0.8200405 Vali Loss: 1.0267791 Test Loss: 1.2085617
Validation loss decreased (1.209861 --> 1.026779).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 85.90662813186646
Epoch: 3, Steps: 90 | Train Loss: 0.7038665 Vali Loss: 0.9477149 Test Loss: 1.1145515
Validation loss decreased (1.026779 --> 0.947715).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 90.58513188362122
Epoch: 4, Steps: 90 | Train Loss: 0.6370327 Vali Loss: 0.8902833 Test Loss: 1.0468404
Validation loss decreased (0.947715 --> 0.890283).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 88.73860549926758
Epoch: 5, Steps: 90 | Train Loss: 0.5845850 Vali Loss: 0.8403774 Test Loss: 0.9882041
Validation loss decreased (0.890283 --> 0.840377).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 86.65634965896606
Epoch: 6, Steps: 90 | Train Loss: 0.5402532 Vali Loss: 0.7972462 Test Loss: 0.9373152
Validation loss decreased (0.840377 --> 0.797246).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 85.84788155555725
Epoch: 7, Steps: 90 | Train Loss: 0.5019928 Vali Loss: 0.7579977 Test Loss: 0.8911400
Validation loss decreased (0.797246 --> 0.757998).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 86.08628487586975
Epoch: 8, Steps: 90 | Train Loss: 0.4687719 Vali Loss: 0.7234897 Test Loss: 0.8510134
Validation loss decreased (0.757998 --> 0.723490).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 83.16271233558655
Epoch: 9, Steps: 90 | Train Loss: 0.4396316 Vali Loss: 0.6929042 Test Loss: 0.8150812
Validation loss decreased (0.723490 --> 0.692904).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 84.6733615398407
Epoch: 10, Steps: 90 | Train Loss: 0.4139564 Vali Loss: 0.6674455 Test Loss: 0.7855420
Validation loss decreased (0.692904 --> 0.667446).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 84.58239722251892
Epoch: 11, Steps: 90 | Train Loss: 0.3912478 Vali Loss: 0.6435357 Test Loss: 0.7574158
Validation loss decreased (0.667446 --> 0.643536).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 89.82452249526978
Epoch: 12, Steps: 90 | Train Loss: 0.3709813 Vali Loss: 0.6217943 Test Loss: 0.7327706
Validation loss decreased (0.643536 --> 0.621794).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 87.10235953330994
Epoch: 13, Steps: 90 | Train Loss: 0.3529845 Vali Loss: 0.6023743 Test Loss: 0.7102529
Validation loss decreased (0.621794 --> 0.602374).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 88.45613765716553
Epoch: 14, Steps: 90 | Train Loss: 0.3367563 Vali Loss: 0.5849653 Test Loss: 0.6901033
Validation loss decreased (0.602374 --> 0.584965).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 86.42129802703857
Epoch: 15, Steps: 90 | Train Loss: 0.3222135 Vali Loss: 0.5698323 Test Loss: 0.6723785
Validation loss decreased (0.584965 --> 0.569832).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 81.01037049293518
Epoch: 16, Steps: 90 | Train Loss: 0.3091249 Vali Loss: 0.5547216 Test Loss: 0.6547535
Validation loss decreased (0.569832 --> 0.554722).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 86.24928140640259
Epoch: 17, Steps: 90 | Train Loss: 0.2972918 Vali Loss: 0.5412782 Test Loss: 0.6396419
Validation loss decreased (0.554722 --> 0.541278).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 86.0766248703003
Epoch: 18, Steps: 90 | Train Loss: 0.2865732 Vali Loss: 0.5297719 Test Loss: 0.6261609
Validation loss decreased (0.541278 --> 0.529772).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 81.49741268157959
Epoch: 19, Steps: 90 | Train Loss: 0.2767586 Vali Loss: 0.5191374 Test Loss: 0.6141855
Validation loss decreased (0.529772 --> 0.519137).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 83.83901524543762
Epoch: 20, Steps: 90 | Train Loss: 0.2679070 Vali Loss: 0.5088034 Test Loss: 0.6025625
Validation loss decreased (0.519137 --> 0.508803).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 82.01654291152954
Epoch: 21, Steps: 90 | Train Loss: 0.2597539 Vali Loss: 0.4999052 Test Loss: 0.5922299
Validation loss decreased (0.508803 --> 0.499905).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 79.75522923469543
Epoch: 22, Steps: 90 | Train Loss: 0.2524120 Vali Loss: 0.4912843 Test Loss: 0.5825012
Validation loss decreased (0.499905 --> 0.491284).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 80.67808842658997
Epoch: 23, Steps: 90 | Train Loss: 0.2456399 Vali Loss: 0.4844775 Test Loss: 0.5745989
Validation loss decreased (0.491284 --> 0.484478).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 80.15740060806274
Epoch: 24, Steps: 90 | Train Loss: 0.2394251 Vali Loss: 0.4772736 Test Loss: 0.5668149
Validation loss decreased (0.484478 --> 0.477274).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 80.4659161567688
Epoch: 25, Steps: 90 | Train Loss: 0.2336846 Vali Loss: 0.4711977 Test Loss: 0.5595659
Validation loss decreased (0.477274 --> 0.471198).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 78.32753372192383
Epoch: 26, Steps: 90 | Train Loss: 0.2284320 Vali Loss: 0.4651376 Test Loss: 0.5533994
Validation loss decreased (0.471198 --> 0.465138).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 84.62533259391785
Epoch: 27, Steps: 90 | Train Loss: 0.2235624 Vali Loss: 0.4593707 Test Loss: 0.5466160
Validation loss decreased (0.465138 --> 0.459371).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 80.58821177482605
Epoch: 28, Steps: 90 | Train Loss: 0.2191087 Vali Loss: 0.4547625 Test Loss: 0.5411228
Validation loss decreased (0.459371 --> 0.454762).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 80.71343231201172
Epoch: 29, Steps: 90 | Train Loss: 0.2150173 Vali Loss: 0.4497875 Test Loss: 0.5354804
Validation loss decreased (0.454762 --> 0.449787).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 79.42138075828552
Epoch: 30, Steps: 90 | Train Loss: 0.2111477 Vali Loss: 0.4459216 Test Loss: 0.5308272
Validation loss decreased (0.449787 --> 0.445922).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 81.39424777030945
Epoch: 31, Steps: 90 | Train Loss: 0.2075810 Vali Loss: 0.4413497 Test Loss: 0.5261406
Validation loss decreased (0.445922 --> 0.441350).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 78.08543753623962
Epoch: 32, Steps: 90 | Train Loss: 0.2042890 Vali Loss: 0.4376924 Test Loss: 0.5221658
Validation loss decreased (0.441350 --> 0.437692).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 78.625985622406
Epoch: 33, Steps: 90 | Train Loss: 0.2012645 Vali Loss: 0.4343985 Test Loss: 0.5183612
Validation loss decreased (0.437692 --> 0.434398).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 80.82247948646545
Epoch: 34, Steps: 90 | Train Loss: 0.1983838 Vali Loss: 0.4314883 Test Loss: 0.5149018
Validation loss decreased (0.434398 --> 0.431488).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 79.03500604629517
Epoch: 35, Steps: 90 | Train Loss: 0.1957434 Vali Loss: 0.4283007 Test Loss: 0.5116495
Validation loss decreased (0.431488 --> 0.428301).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 75.61778283119202
Epoch: 36, Steps: 90 | Train Loss: 0.1932927 Vali Loss: 0.4256557 Test Loss: 0.5085307
Validation loss decreased (0.428301 --> 0.425656).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 74.54924321174622
Epoch: 37, Steps: 90 | Train Loss: 0.1909646 Vali Loss: 0.4228787 Test Loss: 0.5055469
Validation loss decreased (0.425656 --> 0.422879).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 75.23444247245789
Epoch: 38, Steps: 90 | Train Loss: 0.1888151 Vali Loss: 0.4203971 Test Loss: 0.5027596
Validation loss decreased (0.422879 --> 0.420397).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 68.79360008239746
Epoch: 39, Steps: 90 | Train Loss: 0.1868435 Vali Loss: 0.4179408 Test Loss: 0.5002817
Validation loss decreased (0.420397 --> 0.417941).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 103.20367050170898
Epoch: 40, Steps: 90 | Train Loss: 0.1849417 Vali Loss: 0.4158045 Test Loss: 0.4978573
Validation loss decreased (0.417941 --> 0.415805).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 114.32840251922607
Epoch: 41, Steps: 90 | Train Loss: 0.1831806 Vali Loss: 0.4136813 Test Loss: 0.4955370
Validation loss decreased (0.415805 --> 0.413681).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 111.76533389091492
Epoch: 42, Steps: 90 | Train Loss: 0.1815040 Vali Loss: 0.4120571 Test Loss: 0.4934736
Validation loss decreased (0.413681 --> 0.412057).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 109.58059048652649
Epoch: 43, Steps: 90 | Train Loss: 0.1799414 Vali Loss: 0.4103011 Test Loss: 0.4915544
Validation loss decreased (0.412057 --> 0.410301).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 113.32699418067932
Epoch: 44, Steps: 90 | Train Loss: 0.1785248 Vali Loss: 0.4085165 Test Loss: 0.4898830
Validation loss decreased (0.410301 --> 0.408517).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 107.84964227676392
Epoch: 45, Steps: 90 | Train Loss: 0.1771313 Vali Loss: 0.4066730 Test Loss: 0.4880556
Validation loss decreased (0.408517 --> 0.406673).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 117.85746693611145
Epoch: 46, Steps: 90 | Train Loss: 0.1758916 Vali Loss: 0.4053518 Test Loss: 0.4865773
Validation loss decreased (0.406673 --> 0.405352).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 114.99867129325867
Epoch: 47, Steps: 90 | Train Loss: 0.1746394 Vali Loss: 0.4039438 Test Loss: 0.4849679
Validation loss decreased (0.405352 --> 0.403944).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 116.69371390342712
Epoch: 48, Steps: 90 | Train Loss: 0.1735250 Vali Loss: 0.4023930 Test Loss: 0.4835593
Validation loss decreased (0.403944 --> 0.402393).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 107.02123236656189
Epoch: 49, Steps: 90 | Train Loss: 0.1724527 Vali Loss: 0.4013347 Test Loss: 0.4822487
Validation loss decreased (0.402393 --> 0.401335).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 135.57505440711975
Epoch: 50, Steps: 90 | Train Loss: 0.1714494 Vali Loss: 0.4005286 Test Loss: 0.4808878
Validation loss decreased (0.401335 --> 0.400529).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 119.14632034301758
Epoch: 51, Steps: 90 | Train Loss: 0.1704737 Vali Loss: 0.3992095 Test Loss: 0.4798070
Validation loss decreased (0.400529 --> 0.399210).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 115.69503784179688
Epoch: 52, Steps: 90 | Train Loss: 0.1695728 Vali Loss: 0.3979782 Test Loss: 0.4786850
Validation loss decreased (0.399210 --> 0.397978).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 111.29878234863281
Epoch: 53, Steps: 90 | Train Loss: 0.1687380 Vali Loss: 0.3971815 Test Loss: 0.4776083
Validation loss decreased (0.397978 --> 0.397182).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 109.44221711158752
Epoch: 54, Steps: 90 | Train Loss: 0.1679192 Vali Loss: 0.3963799 Test Loss: 0.4766454
Validation loss decreased (0.397182 --> 0.396380).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 134.4707794189453
Epoch: 55, Steps: 90 | Train Loss: 0.1671464 Vali Loss: 0.3953070 Test Loss: 0.4757204
Validation loss decreased (0.396380 --> 0.395307).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 115.84046792984009
Epoch: 56, Steps: 90 | Train Loss: 0.1664789 Vali Loss: 0.3947261 Test Loss: 0.4748617
Validation loss decreased (0.395307 --> 0.394726).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 111.30267572402954
Epoch: 57, Steps: 90 | Train Loss: 0.1658037 Vali Loss: 0.3937658 Test Loss: 0.4739861
Validation loss decreased (0.394726 --> 0.393766).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 125.5960795879364
Epoch: 58, Steps: 90 | Train Loss: 0.1651335 Vali Loss: 0.3930245 Test Loss: 0.4731622
Validation loss decreased (0.393766 --> 0.393025).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 117.24788808822632
Epoch: 59, Steps: 90 | Train Loss: 0.1645554 Vali Loss: 0.3926511 Test Loss: 0.4724522
Validation loss decreased (0.393025 --> 0.392651).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 121.33251810073853
Epoch: 60, Steps: 90 | Train Loss: 0.1639540 Vali Loss: 0.3915478 Test Loss: 0.4717390
Validation loss decreased (0.392651 --> 0.391548).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 136.9088921546936
Epoch: 61, Steps: 90 | Train Loss: 0.1634454 Vali Loss: 0.3911777 Test Loss: 0.4710726
Validation loss decreased (0.391548 --> 0.391178).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 115.19726872444153
Epoch: 62, Steps: 90 | Train Loss: 0.1628986 Vali Loss: 0.3904297 Test Loss: 0.4704072
Validation loss decreased (0.391178 --> 0.390430).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 109.88988018035889
Epoch: 63, Steps: 90 | Train Loss: 0.1624359 Vali Loss: 0.3895012 Test Loss: 0.4698022
Validation loss decreased (0.390430 --> 0.389501).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 120.03850054740906
Epoch: 64, Steps: 90 | Train Loss: 0.1619418 Vali Loss: 0.3891332 Test Loss: 0.4692711
Validation loss decreased (0.389501 --> 0.389133).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 114.08688402175903
Epoch: 65, Steps: 90 | Train Loss: 0.1615406 Vali Loss: 0.3887356 Test Loss: 0.4687113
Validation loss decreased (0.389133 --> 0.388736).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 125.35266757011414
Epoch: 66, Steps: 90 | Train Loss: 0.1611120 Vali Loss: 0.3882286 Test Loss: 0.4682068
Validation loss decreased (0.388736 --> 0.388229).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 119.38421559333801
Epoch: 67, Steps: 90 | Train Loss: 0.1606965 Vali Loss: 0.3878350 Test Loss: 0.4677326
Validation loss decreased (0.388229 --> 0.387835).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 104.74720597267151
Epoch: 68, Steps: 90 | Train Loss: 0.1603206 Vali Loss: 0.3876085 Test Loss: 0.4672705
Validation loss decreased (0.387835 --> 0.387609).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 102.5028760433197
Epoch: 69, Steps: 90 | Train Loss: 0.1599810 Vali Loss: 0.3873152 Test Loss: 0.4668373
Validation loss decreased (0.387609 --> 0.387315).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 107.9736852645874
Epoch: 70, Steps: 90 | Train Loss: 0.1596545 Vali Loss: 0.3865757 Test Loss: 0.4664253
Validation loss decreased (0.387315 --> 0.386576).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 100.80116295814514
Epoch: 71, Steps: 90 | Train Loss: 0.1593577 Vali Loss: 0.3864081 Test Loss: 0.4660324
Validation loss decreased (0.386576 --> 0.386408).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 115.11862802505493
Epoch: 72, Steps: 90 | Train Loss: 0.1590326 Vali Loss: 0.3858027 Test Loss: 0.4656608
Validation loss decreased (0.386408 --> 0.385803).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 102.34054040908813
Epoch: 73, Steps: 90 | Train Loss: 0.1587574 Vali Loss: 0.3857754 Test Loss: 0.4652998
Validation loss decreased (0.385803 --> 0.385775).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 121.56807374954224
Epoch: 74, Steps: 90 | Train Loss: 0.1584838 Vali Loss: 0.3853320 Test Loss: 0.4649797
Validation loss decreased (0.385775 --> 0.385332).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 109.86153292655945
Epoch: 75, Steps: 90 | Train Loss: 0.1582185 Vali Loss: 0.3849539 Test Loss: 0.4646504
Validation loss decreased (0.385332 --> 0.384954).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 123.44677400588989
Epoch: 76, Steps: 90 | Train Loss: 0.1579474 Vali Loss: 0.3843955 Test Loss: 0.4643499
Validation loss decreased (0.384954 --> 0.384395).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 116.55212497711182
Epoch: 77, Steps: 90 | Train Loss: 0.1577155 Vali Loss: 0.3843555 Test Loss: 0.4640646
Validation loss decreased (0.384395 --> 0.384356).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 137.8139328956604
Epoch: 78, Steps: 90 | Train Loss: 0.1575053 Vali Loss: 0.3839851 Test Loss: 0.4638008
Validation loss decreased (0.384356 --> 0.383985).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 134.63917994499207
Epoch: 79, Steps: 90 | Train Loss: 0.1572770 Vali Loss: 0.3840114 Test Loss: 0.4635487
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 110.98834037780762
Epoch: 80, Steps: 90 | Train Loss: 0.1570931 Vali Loss: 0.3838303 Test Loss: 0.4632984
Validation loss decreased (0.383985 --> 0.383830).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 91.09323000907898
Epoch: 81, Steps: 90 | Train Loss: 0.1568854 Vali Loss: 0.3832961 Test Loss: 0.4630551
Validation loss decreased (0.383830 --> 0.383296).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 92.69092059135437
Epoch: 82, Steps: 90 | Train Loss: 0.1567421 Vali Loss: 0.3831657 Test Loss: 0.4628447
Validation loss decreased (0.383296 --> 0.383166).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 89.90808463096619
Epoch: 83, Steps: 90 | Train Loss: 0.1565918 Vali Loss: 0.3830060 Test Loss: 0.4626241
Validation loss decreased (0.383166 --> 0.383006).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 92.03872156143188
Epoch: 84, Steps: 90 | Train Loss: 0.1563865 Vali Loss: 0.3827004 Test Loss: 0.4624326
Validation loss decreased (0.383006 --> 0.382700).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 88.12935757637024
Epoch: 85, Steps: 90 | Train Loss: 0.1562273 Vali Loss: 0.3827565 Test Loss: 0.4622512
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 112.88503813743591
Epoch: 86, Steps: 90 | Train Loss: 0.1561125 Vali Loss: 0.3826980 Test Loss: 0.4620688
Validation loss decreased (0.382700 --> 0.382698).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 86.83753371238708
Epoch: 87, Steps: 90 | Train Loss: 0.1559134 Vali Loss: 0.3823993 Test Loss: 0.4619021
Validation loss decreased (0.382698 --> 0.382399).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 89.34050035476685
Epoch: 88, Steps: 90 | Train Loss: 0.1557937 Vali Loss: 0.3820115 Test Loss: 0.4617369
Validation loss decreased (0.382399 --> 0.382012).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 89.66885209083557
Epoch: 89, Steps: 90 | Train Loss: 0.1556735 Vali Loss: 0.3821167 Test Loss: 0.4615766
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 96.43069672584534
Epoch: 90, Steps: 90 | Train Loss: 0.1555630 Vali Loss: 0.3819126 Test Loss: 0.4614373
Validation loss decreased (0.382012 --> 0.381913).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 115.83506369590759
Epoch: 91, Steps: 90 | Train Loss: 0.1554572 Vali Loss: 0.3819615 Test Loss: 0.4612943
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 96.31369805335999
Epoch: 92, Steps: 90 | Train Loss: 0.1553499 Vali Loss: 0.3815309 Test Loss: 0.4611664
Validation loss decreased (0.381913 --> 0.381531).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 77.17756772041321
Epoch: 93, Steps: 90 | Train Loss: 0.1552864 Vali Loss: 0.3813804 Test Loss: 0.4610463
Validation loss decreased (0.381531 --> 0.381380).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 71.07219648361206
Epoch: 94, Steps: 90 | Train Loss: 0.1551619 Vali Loss: 0.3813311 Test Loss: 0.4609278
Validation loss decreased (0.381380 --> 0.381331).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 69.86641383171082
Epoch: 95, Steps: 90 | Train Loss: 0.1550485 Vali Loss: 0.3813084 Test Loss: 0.4608102
Validation loss decreased (0.381331 --> 0.381308).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 68.68942022323608
Epoch: 96, Steps: 90 | Train Loss: 0.1549980 Vali Loss: 0.3812478 Test Loss: 0.4607031
Validation loss decreased (0.381308 --> 0.381248).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 65.8959093093872
Epoch: 97, Steps: 90 | Train Loss: 0.1549084 Vali Loss: 0.3810960 Test Loss: 0.4606030
Validation loss decreased (0.381248 --> 0.381096).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 60.83802318572998
Epoch: 98, Steps: 90 | Train Loss: 0.1548072 Vali Loss: 0.3810077 Test Loss: 0.4605021
Validation loss decreased (0.381096 --> 0.381008).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 60.886895179748535
Epoch: 99, Steps: 90 | Train Loss: 0.1547564 Vali Loss: 0.3806811 Test Loss: 0.4604203
Validation loss decreased (0.381008 --> 0.380681).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 58.7806921005249
Epoch: 100, Steps: 90 | Train Loss: 0.1546664 Vali Loss: 0.3806442 Test Loss: 0.4603262
Validation loss decreased (0.380681 --> 0.380644).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 11585
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=138, out_features=266, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4050213888.0
params:  36974.0
Trainable parameters:  36974
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 48.37528371810913
Epoch: 1, Steps: 90 | Train Loss: 0.2719079 Vali Loss: 0.3563808 Test Loss: 0.4379231
Validation loss decreased (inf --> 0.356381).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 50.09035897254944
Epoch: 2, Steps: 90 | Train Loss: 0.2645820 Vali Loss: 0.3541847 Test Loss: 0.4370192
Validation loss decreased (0.356381 --> 0.354185).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 49.91817355155945
Epoch: 3, Steps: 90 | Train Loss: 0.2641702 Vali Loss: 0.3541467 Test Loss: 0.4373308
Validation loss decreased (0.354185 --> 0.354147).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 50.410247802734375
Epoch: 4, Steps: 90 | Train Loss: 0.2641498 Vali Loss: 0.3541264 Test Loss: 0.4371710
Validation loss decreased (0.354147 --> 0.354126).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 54.98350191116333
Epoch: 5, Steps: 90 | Train Loss: 0.2640714 Vali Loss: 0.3540952 Test Loss: 0.4371711
Validation loss decreased (0.354126 --> 0.354095).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 51.61888146400452
Epoch: 6, Steps: 90 | Train Loss: 0.2639993 Vali Loss: 0.3538515 Test Loss: 0.4371616
Validation loss decreased (0.354095 --> 0.353851).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 45.90437626838684
Epoch: 7, Steps: 90 | Train Loss: 0.2640396 Vali Loss: 0.3541129 Test Loss: 0.4367144
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 42.230698108673096
Epoch: 8, Steps: 90 | Train Loss: 0.2640203 Vali Loss: 0.3538712 Test Loss: 0.4369735
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 74.55675935745239
Epoch: 9, Steps: 90 | Train Loss: 0.2639929 Vali Loss: 0.3540167 Test Loss: 0.4368227
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 70.82587790489197
Epoch: 10, Steps: 90 | Train Loss: 0.2639372 Vali Loss: 0.3540823 Test Loss: 0.4368948
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 70.04290270805359
Epoch: 11, Steps: 90 | Train Loss: 0.2639195 Vali Loss: 0.3535170 Test Loss: 0.4367847
Validation loss decreased (0.353851 --> 0.353517).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 73.45710062980652
Epoch: 12, Steps: 90 | Train Loss: 0.2638109 Vali Loss: 0.3542086 Test Loss: 0.4368022
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 71.82340574264526
Epoch: 13, Steps: 90 | Train Loss: 0.2639379 Vali Loss: 0.3535863 Test Loss: 0.4366477
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 69.71141815185547
Epoch: 14, Steps: 90 | Train Loss: 0.2638048 Vali Loss: 0.3540633 Test Loss: 0.4367580
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 84.87925410270691
Epoch: 15, Steps: 90 | Train Loss: 0.2639194 Vali Loss: 0.3539458 Test Loss: 0.4367115
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 72.1316454410553
Epoch: 16, Steps: 90 | Train Loss: 0.2637921 Vali Loss: 0.3541588 Test Loss: 0.4366100
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 68.20438718795776
Epoch: 17, Steps: 90 | Train Loss: 0.2638070 Vali Loss: 0.3539453 Test Loss: 0.4366250
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 72.07526278495789
Epoch: 18, Steps: 90 | Train Loss: 0.2638695 Vali Loss: 0.3535038 Test Loss: 0.4366348
Validation loss decreased (0.353517 --> 0.353504).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 69.85097670555115
Epoch: 19, Steps: 90 | Train Loss: 0.2637574 Vali Loss: 0.3536449 Test Loss: 0.4367943
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 74.65130758285522
Epoch: 20, Steps: 90 | Train Loss: 0.2637753 Vali Loss: 0.3536583 Test Loss: 0.4365715
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 72.5071210861206
Epoch: 21, Steps: 90 | Train Loss: 0.2636470 Vali Loss: 0.3537668 Test Loss: 0.4365567
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 81.40467762947083
Epoch: 22, Steps: 90 | Train Loss: 0.2637916 Vali Loss: 0.3537572 Test Loss: 0.4366822
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 103.27151894569397
Epoch: 23, Steps: 90 | Train Loss: 0.2637364 Vali Loss: 0.3540268 Test Loss: 0.4366794
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 69.65668654441833
Epoch: 24, Steps: 90 | Train Loss: 0.2637238 Vali Loss: 0.3539999 Test Loss: 0.4365276
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 71.83573698997498
Epoch: 25, Steps: 90 | Train Loss: 0.2637406 Vali Loss: 0.3541091 Test Loss: 0.4364838
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 73.8029568195343
Epoch: 26, Steps: 90 | Train Loss: 0.2636916 Vali Loss: 0.3538870 Test Loss: 0.4365119
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 71.41160678863525
Epoch: 27, Steps: 90 | Train Loss: 0.2636736 Vali Loss: 0.3539885 Test Loss: 0.4365176
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 75.0607042312622
Epoch: 28, Steps: 90 | Train Loss: 0.2637028 Vali Loss: 0.3535824 Test Loss: 0.4365995
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j336_H8_FITS_custom_ftM_sl360_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.43412351608276367, mae:0.2903396189212799, rse:0.5415093302726746, corr:[0.27032495 0.2835028  0.28408912 0.28372034 0.2838925  0.28346333
 0.28328183 0.2832871  0.2831288  0.2832962  0.28330114 0.28303468
 0.28303087 0.2826198  0.28219774 0.2824617  0.28258058 0.28253415
 0.28291532 0.2832484  0.28298792 0.2825418  0.28235134 0.28266236
 0.28364524 0.2839435  0.28390232 0.28369606 0.28358874 0.2834374
 0.28324485 0.28338292 0.28360733 0.2836478  0.2837445  0.2837846
 0.28347617 0.28311676 0.28298238 0.2829218  0.2830112  0.283362
 0.28374034 0.2836813  0.28333208 0.2831524  0.28308964 0.2830666
 0.28345644 0.2837815  0.28401357 0.2839548  0.28362998 0.28334197
 0.2833963  0.2835965  0.28352547 0.2833087  0.28320637 0.28316274
 0.2830824  0.2830076  0.28306186 0.2832341  0.28344718 0.2835933
 0.2836044  0.28326407 0.28310534 0.28322303 0.28299078 0.28274697
 0.28300473 0.2832047  0.28289726 0.28259516 0.28256544 0.28250566
 0.2824756  0.2824355  0.28234252 0.2823662  0.28243637 0.2824628
 0.28247252 0.28239888 0.28238058 0.28246292 0.28268892 0.28303915
 0.28306264 0.28263083 0.28241238 0.28228387 0.28219    0.28263634
 0.28286263 0.28272936 0.28279793 0.28293547 0.28273812 0.28256518
 0.28257677 0.28244624 0.2823579  0.28236037 0.28222692 0.2822871
 0.28264195 0.28274792 0.28259066 0.28234026 0.2819161  0.28174868
 0.2820386  0.2820352  0.2818298  0.28204018 0.2825468  0.28264785
 0.28237376 0.2825976  0.28272438 0.2824092  0.2824867  0.2828337
 0.28278318 0.28260863 0.28240243 0.28206384 0.2820574  0.2823651
 0.2825159  0.2825634  0.28251734 0.2822674  0.28214782 0.28224564
 0.28246504 0.28270558 0.28260815 0.28230885 0.28253964 0.28317901
 0.28329012 0.28327665 0.2834844  0.28346244 0.28339148 0.28356037
 0.28372082 0.28393793 0.28412062 0.28403828 0.2838702  0.28369263
 0.28353253 0.283461   0.28336212 0.28336266 0.28361824 0.28391013
 0.28392768 0.28365132 0.28331935 0.2832217  0.28339112 0.2839472
 0.28482294 0.28470773 0.28444594 0.28417137 0.28414896 0.2843309
 0.28429773 0.28397503 0.28384212 0.2840939  0.2843315  0.2842303
 0.2840121  0.28377694 0.28359678 0.28359792 0.28368726 0.28392062
 0.2841879  0.28422117 0.2841527  0.28421926 0.28399393 0.28365025
 0.2840266  0.28393063 0.28373486 0.28372929 0.28372914 0.28366584
 0.28364924 0.2835908  0.2835169  0.28331745 0.2829361  0.28290293
 0.28318235 0.2830967  0.28302795 0.28329948 0.2833207  0.28316388
 0.2831644  0.2831375  0.28311872 0.28329888 0.28343847 0.2835476
 0.28352538 0.28309563 0.28313202 0.28340462 0.28329667 0.28317463
 0.2832442  0.2831894  0.28305033 0.28300822 0.28310066 0.2830809
 0.2825924  0.28208715 0.28229344 0.28277108 0.2830719  0.28328463
 0.28319398 0.28284788 0.2826523  0.28246817 0.28249735 0.28293726
 0.28309596 0.28277963 0.28264216 0.28271258 0.2826943  0.28268358
 0.28261718 0.28242844 0.28233883 0.28224608 0.28195965 0.28171727
 0.28168622 0.28173932 0.2817152  0.28167632 0.28191024 0.28230658
 0.28244522 0.28225604 0.28189358 0.28168172 0.28183344 0.2820619
 0.2819681  0.2819079  0.28201175 0.2822572  0.28244174 0.2823673
 0.2822053  0.28200272 0.28175792 0.28166056 0.2816842  0.2815722
 0.28144065 0.28128695 0.28093377 0.28077534 0.2811673  0.28144455
 0.28125516 0.28114203 0.28124022 0.2812223  0.2811762  0.28144044
 0.28150675 0.28138104 0.28113368 0.28124437 0.2815876  0.28174698
 0.28166413 0.28141814 0.28120995 0.28106084 0.2810232  0.28128114
 0.2815169  0.28132388 0.2809381  0.28077087 0.2809493  0.28114676
 0.28114137 0.28111482 0.2813666  0.2815854  0.2815547  0.2818213
 0.28194755 0.28173235 0.28193936 0.28253067 0.28250626 0.28213444
 0.28203765 0.28209528 0.28212923 0.28206593 0.2818834  0.28168508
 0.28168058 0.28183097 0.28180236 0.28179988 0.28217277 0.2826152
 0.28283307 0.28259453 0.28228256 0.28270125 0.2822526  0.2824661 ]
