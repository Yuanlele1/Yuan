Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j96_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j96_H10_FITS_custom_ftM_sl180_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 12005
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=90, out_features=138, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1370373120.0
params:  12558.0
Trainable parameters:  12558
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 82.39187979698181
Epoch: 1, Steps: 93 | Train Loss: 0.9746014 Vali Loss: 0.9776955 Test Loss: 1.1559777
Validation loss decreased (inf --> 0.977696).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 74.1952645778656
Epoch: 2, Steps: 93 | Train Loss: 0.6147430 Vali Loss: 0.7446115 Test Loss: 0.8894908
Validation loss decreased (0.977696 --> 0.744612).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 76.91965627670288
Epoch: 3, Steps: 93 | Train Loss: 0.4607321 Vali Loss: 0.6430674 Test Loss: 0.7771721
Validation loss decreased (0.744612 --> 0.643067).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 84.72986698150635
Epoch: 4, Steps: 93 | Train Loss: 0.3783634 Vali Loss: 0.5924022 Test Loss: 0.7163692
Validation loss decreased (0.643067 --> 0.592402).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 76.86170601844788
Epoch: 5, Steps: 93 | Train Loss: 0.3273166 Vali Loss: 0.5582442 Test Loss: 0.6775166
Validation loss decreased (0.592402 --> 0.558244).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 77.33278393745422
Epoch: 6, Steps: 93 | Train Loss: 0.2917836 Vali Loss: 0.5334243 Test Loss: 0.6478509
Validation loss decreased (0.558244 --> 0.533424).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 85.61807942390442
Epoch: 7, Steps: 93 | Train Loss: 0.2647939 Vali Loss: 0.5127893 Test Loss: 0.6254567
Validation loss decreased (0.533424 --> 0.512789).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 83.87546253204346
Epoch: 8, Steps: 93 | Train Loss: 0.2435997 Vali Loss: 0.4982422 Test Loss: 0.6059999
Validation loss decreased (0.512789 --> 0.498242).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 77.60351729393005
Epoch: 9, Steps: 93 | Train Loss: 0.2262011 Vali Loss: 0.4859760 Test Loss: 0.5908062
Validation loss decreased (0.498242 --> 0.485976).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 77.65877532958984
Epoch: 10, Steps: 93 | Train Loss: 0.2115752 Vali Loss: 0.4734622 Test Loss: 0.5764080
Validation loss decreased (0.485976 --> 0.473462).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 77.41388630867004
Epoch: 11, Steps: 93 | Train Loss: 0.1992210 Vali Loss: 0.4636579 Test Loss: 0.5648243
Validation loss decreased (0.473462 --> 0.463658).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 77.71408581733704
Epoch: 12, Steps: 93 | Train Loss: 0.1887869 Vali Loss: 0.4566221 Test Loss: 0.5544586
Validation loss decreased (0.463658 --> 0.456622).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 76.68357539176941
Epoch: 13, Steps: 93 | Train Loss: 0.1797351 Vali Loss: 0.4493900 Test Loss: 0.5454978
Validation loss decreased (0.456622 --> 0.449390).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 82.61193227767944
Epoch: 14, Steps: 93 | Train Loss: 0.1720003 Vali Loss: 0.4430119 Test Loss: 0.5377608
Validation loss decreased (0.449390 --> 0.443012).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 77.88742017745972
Epoch: 15, Steps: 93 | Train Loss: 0.1651215 Vali Loss: 0.4372313 Test Loss: 0.5305937
Validation loss decreased (0.443012 --> 0.437231).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 81.69163632392883
Epoch: 16, Steps: 93 | Train Loss: 0.1592547 Vali Loss: 0.4324252 Test Loss: 0.5246658
Validation loss decreased (0.437231 --> 0.432425).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 79.0337393283844
Epoch: 17, Steps: 93 | Train Loss: 0.1540572 Vali Loss: 0.4269306 Test Loss: 0.5194581
Validation loss decreased (0.432425 --> 0.426931).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 78.94114661216736
Epoch: 18, Steps: 93 | Train Loss: 0.1494955 Vali Loss: 0.4236047 Test Loss: 0.5144759
Validation loss decreased (0.426931 --> 0.423605).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 86.96914792060852
Epoch: 19, Steps: 93 | Train Loss: 0.1454792 Vali Loss: 0.4194801 Test Loss: 0.5102897
Validation loss decreased (0.423605 --> 0.419480).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 93.48865747451782
Epoch: 20, Steps: 93 | Train Loss: 0.1419248 Vali Loss: 0.4177551 Test Loss: 0.5066537
Validation loss decreased (0.419480 --> 0.417755).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 85.26638913154602
Epoch: 21, Steps: 93 | Train Loss: 0.1386894 Vali Loss: 0.4135095 Test Loss: 0.5033715
Validation loss decreased (0.417755 --> 0.413510).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 82.3577401638031
Epoch: 22, Steps: 93 | Train Loss: 0.1358486 Vali Loss: 0.4128122 Test Loss: 0.5001210
Validation loss decreased (0.413510 --> 0.412812).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 82.92189240455627
Epoch: 23, Steps: 93 | Train Loss: 0.1332829 Vali Loss: 0.4097166 Test Loss: 0.4972622
Validation loss decreased (0.412812 --> 0.409717).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 81.27767157554626
Epoch: 24, Steps: 93 | Train Loss: 0.1309219 Vali Loss: 0.4072951 Test Loss: 0.4949320
Validation loss decreased (0.409717 --> 0.407295).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 94.38166570663452
Epoch: 25, Steps: 93 | Train Loss: 0.1288750 Vali Loss: 0.4061342 Test Loss: 0.4927347
Validation loss decreased (0.407295 --> 0.406134).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 84.38154864311218
Epoch: 26, Steps: 93 | Train Loss: 0.1270224 Vali Loss: 0.4043478 Test Loss: 0.4907489
Validation loss decreased (0.406134 --> 0.404348).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 88.31333470344543
Epoch: 27, Steps: 93 | Train Loss: 0.1252794 Vali Loss: 0.4026258 Test Loss: 0.4886602
Validation loss decreased (0.404348 --> 0.402626).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 86.32840609550476
Epoch: 28, Steps: 93 | Train Loss: 0.1237430 Vali Loss: 0.4021529 Test Loss: 0.4870497
Validation loss decreased (0.402626 --> 0.402153).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 86.30051970481873
Epoch: 29, Steps: 93 | Train Loss: 0.1223140 Vali Loss: 0.4008616 Test Loss: 0.4855713
Validation loss decreased (0.402153 --> 0.400862).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 90.3889570236206
Epoch: 30, Steps: 93 | Train Loss: 0.1210271 Vali Loss: 0.3996286 Test Loss: 0.4841046
Validation loss decreased (0.400862 --> 0.399629).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 77.92710518836975
Epoch: 31, Steps: 93 | Train Loss: 0.1198164 Vali Loss: 0.3980296 Test Loss: 0.4829139
Validation loss decreased (0.399629 --> 0.398030).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 75.40420150756836
Epoch: 32, Steps: 93 | Train Loss: 0.1187360 Vali Loss: 0.3983347 Test Loss: 0.4817300
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 77.19972038269043
Epoch: 33, Steps: 93 | Train Loss: 0.1177721 Vali Loss: 0.3952965 Test Loss: 0.4806791
Validation loss decreased (0.398030 --> 0.395296).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 76.66502952575684
Epoch: 34, Steps: 93 | Train Loss: 0.1167975 Vali Loss: 0.3946421 Test Loss: 0.4795997
Validation loss decreased (0.395296 --> 0.394642).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 83.48321843147278
Epoch: 35, Steps: 93 | Train Loss: 0.1160004 Vali Loss: 0.3926992 Test Loss: 0.4787457
Validation loss decreased (0.394642 --> 0.392699).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 84.58087229728699
Epoch: 36, Steps: 93 | Train Loss: 0.1152354 Vali Loss: 0.3943698 Test Loss: 0.4778734
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 74.99200415611267
Epoch: 37, Steps: 93 | Train Loss: 0.1144368 Vali Loss: 0.3937500 Test Loss: 0.4770730
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 67.83352541923523
Epoch: 38, Steps: 93 | Train Loss: 0.1138201 Vali Loss: 0.3913155 Test Loss: 0.4763259
Validation loss decreased (0.392699 --> 0.391315).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 71.94357299804688
Epoch: 39, Steps: 93 | Train Loss: 0.1132104 Vali Loss: 0.3915389 Test Loss: 0.4756615
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 70.69645500183105
Epoch: 40, Steps: 93 | Train Loss: 0.1126086 Vali Loss: 0.3908914 Test Loss: 0.4750350
Validation loss decreased (0.391315 --> 0.390891).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 68.3025631904602
Epoch: 41, Steps: 93 | Train Loss: 0.1120698 Vali Loss: 0.3918458 Test Loss: 0.4744809
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 69.3732419013977
Epoch: 42, Steps: 93 | Train Loss: 0.1115769 Vali Loss: 0.3901626 Test Loss: 0.4739060
Validation loss decreased (0.390891 --> 0.390163).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 75.12076234817505
Epoch: 43, Steps: 93 | Train Loss: 0.1111017 Vali Loss: 0.3897752 Test Loss: 0.4734635
Validation loss decreased (0.390163 --> 0.389775).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 77.69381070137024
Epoch: 44, Steps: 93 | Train Loss: 0.1107065 Vali Loss: 0.3892761 Test Loss: 0.4729404
Validation loss decreased (0.389775 --> 0.389276).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 69.26092505455017
Epoch: 45, Steps: 93 | Train Loss: 0.1102410 Vali Loss: 0.3884622 Test Loss: 0.4725222
Validation loss decreased (0.389276 --> 0.388462).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 69.22997522354126
Epoch: 46, Steps: 93 | Train Loss: 0.1099319 Vali Loss: 0.3900160 Test Loss: 0.4720209
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 67.40717220306396
Epoch: 47, Steps: 93 | Train Loss: 0.1095238 Vali Loss: 0.3879860 Test Loss: 0.4716416
Validation loss decreased (0.388462 --> 0.387986).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 69.4358491897583
Epoch: 48, Steps: 93 | Train Loss: 0.1092084 Vali Loss: 0.3876026 Test Loss: 0.4713434
Validation loss decreased (0.387986 --> 0.387603).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 68.8832037448883
Epoch: 49, Steps: 93 | Train Loss: 0.1089241 Vali Loss: 0.3877661 Test Loss: 0.4709543
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 70.36726880073547
Epoch: 50, Steps: 93 | Train Loss: 0.1086832 Vali Loss: 0.3884755 Test Loss: 0.4706339
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 69.93525171279907
Epoch: 51, Steps: 93 | Train Loss: 0.1083789 Vali Loss: 0.3870092 Test Loss: 0.4703559
Validation loss decreased (0.387603 --> 0.387009).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 65.96971607208252
Epoch: 52, Steps: 93 | Train Loss: 0.1081359 Vali Loss: 0.3874840 Test Loss: 0.4700592
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 65.10420656204224
Epoch: 53, Steps: 93 | Train Loss: 0.1078480 Vali Loss: 0.3876336 Test Loss: 0.4698082
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 56.30852508544922
Epoch: 54, Steps: 93 | Train Loss: 0.1076424 Vali Loss: 0.3871145 Test Loss: 0.4695120
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 51.64649510383606
Epoch: 55, Steps: 93 | Train Loss: 0.1074037 Vali Loss: 0.3870661 Test Loss: 0.4693060
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 54.89318633079529
Epoch: 56, Steps: 93 | Train Loss: 0.1071942 Vali Loss: 0.3864293 Test Loss: 0.4690756
Validation loss decreased (0.387009 --> 0.386429).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 54.916271924972534
Epoch: 57, Steps: 93 | Train Loss: 0.1070195 Vali Loss: 0.3880005 Test Loss: 0.4688634
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 58.227577924728394
Epoch: 58, Steps: 93 | Train Loss: 0.1068021 Vali Loss: 0.3863624 Test Loss: 0.4686884
Validation loss decreased (0.386429 --> 0.386362).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 54.37732481956482
Epoch: 59, Steps: 93 | Train Loss: 0.1066774 Vali Loss: 0.3861403 Test Loss: 0.4684876
Validation loss decreased (0.386362 --> 0.386140).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 55.79064607620239
Epoch: 60, Steps: 93 | Train Loss: 0.1065709 Vali Loss: 0.3865441 Test Loss: 0.4683051
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 51.362557888031006
Epoch: 61, Steps: 93 | Train Loss: 0.1063676 Vali Loss: 0.3850796 Test Loss: 0.4681190
Validation loss decreased (0.386140 --> 0.385080).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 52.10110521316528
Epoch: 62, Steps: 93 | Train Loss: 0.1062291 Vali Loss: 0.3853312 Test Loss: 0.4679778
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 54.323192834854126
Epoch: 63, Steps: 93 | Train Loss: 0.1061180 Vali Loss: 0.3853964 Test Loss: 0.4678323
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 51.87988591194153
Epoch: 64, Steps: 93 | Train Loss: 0.1059328 Vali Loss: 0.3857621 Test Loss: 0.4676908
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 57.564491987228394
Epoch: 65, Steps: 93 | Train Loss: 0.1058735 Vali Loss: 0.3855865 Test Loss: 0.4675505
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 53.859275817871094
Epoch: 66, Steps: 93 | Train Loss: 0.1057759 Vali Loss: 0.3850309 Test Loss: 0.4674001
Validation loss decreased (0.385080 --> 0.385031).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 54.917808294296265
Epoch: 67, Steps: 93 | Train Loss: 0.1055765 Vali Loss: 0.3850179 Test Loss: 0.4672869
Validation loss decreased (0.385031 --> 0.385018).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 52.65714883804321
Epoch: 68, Steps: 93 | Train Loss: 0.1055583 Vali Loss: 0.3845569 Test Loss: 0.4671758
Validation loss decreased (0.385018 --> 0.384557).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 58.066280364990234
Epoch: 69, Steps: 93 | Train Loss: 0.1054566 Vali Loss: 0.3846405 Test Loss: 0.4670559
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 50.6834352016449
Epoch: 70, Steps: 93 | Train Loss: 0.1053565 Vali Loss: 0.3860519 Test Loss: 0.4669466
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 90.12885212898254
Epoch: 71, Steps: 93 | Train Loss: 0.1052968 Vali Loss: 0.3852308 Test Loss: 0.4668632
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 84.8114607334137
Epoch: 72, Steps: 93 | Train Loss: 0.1051968 Vali Loss: 0.3837161 Test Loss: 0.4667724
Validation loss decreased (0.384557 --> 0.383716).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 84.92516112327576
Epoch: 73, Steps: 93 | Train Loss: 0.1050511 Vali Loss: 0.3849773 Test Loss: 0.4666854
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 87.21722531318665
Epoch: 74, Steps: 93 | Train Loss: 0.1050381 Vali Loss: 0.3853184 Test Loss: 0.4665932
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 87.17214560508728
Epoch: 75, Steps: 93 | Train Loss: 0.1049748 Vali Loss: 0.3845172 Test Loss: 0.4665104
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 78.93519306182861
Epoch: 76, Steps: 93 | Train Loss: 0.1049235 Vali Loss: 0.3847958 Test Loss: 0.4664356
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 93.42767143249512
Epoch: 77, Steps: 93 | Train Loss: 0.1048110 Vali Loss: 0.3838438 Test Loss: 0.4663632
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 83.24024438858032
Epoch: 78, Steps: 93 | Train Loss: 0.1047573 Vali Loss: 0.3843893 Test Loss: 0.4663032
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 119.23443484306335
Epoch: 79, Steps: 93 | Train Loss: 0.1047221 Vali Loss: 0.3843977 Test Loss: 0.4662321
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 87.88703799247742
Epoch: 80, Steps: 93 | Train Loss: 0.1047047 Vali Loss: 0.3847364 Test Loss: 0.4661631
EarlyStopping counter: 8 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 90.13219881057739
Epoch: 81, Steps: 93 | Train Loss: 0.1045782 Vali Loss: 0.3843106 Test Loss: 0.4661067
EarlyStopping counter: 9 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 90.02828645706177
Epoch: 82, Steps: 93 | Train Loss: 0.1045276 Vali Loss: 0.3840019 Test Loss: 0.4660467
EarlyStopping counter: 10 out of 10
Early stopping
train 12005
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=90, out_features=138, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1370373120.0
params:  12558.0
Trainable parameters:  12558
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 84.33705472946167
Epoch: 1, Steps: 93 | Train Loss: 0.2780455 Vali Loss: 0.3748344 Test Loss: 0.4544997
Validation loss decreased (inf --> 0.374834).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 78.41539716720581
Epoch: 2, Steps: 93 | Train Loss: 0.2744208 Vali Loss: 0.3739734 Test Loss: 0.4529460
Validation loss decreased (0.374834 --> 0.373973).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 87.72080755233765
Epoch: 3, Steps: 93 | Train Loss: 0.2736431 Vali Loss: 0.3719304 Test Loss: 0.4520251
Validation loss decreased (0.373973 --> 0.371930).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 79.96759796142578
Epoch: 4, Steps: 93 | Train Loss: 0.2733710 Vali Loss: 0.3725764 Test Loss: 0.4519141
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 81.08471846580505
Epoch: 5, Steps: 93 | Train Loss: 0.2732856 Vali Loss: 0.3723868 Test Loss: 0.4517913
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 82.91978645324707
Epoch: 6, Steps: 93 | Train Loss: 0.2730061 Vali Loss: 0.3714262 Test Loss: 0.4513996
Validation loss decreased (0.371930 --> 0.371426).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 75.00234699249268
Epoch: 7, Steps: 93 | Train Loss: 0.2731673 Vali Loss: 0.3719177 Test Loss: 0.4514603
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 83.95331692695618
Epoch: 8, Steps: 93 | Train Loss: 0.2730572 Vali Loss: 0.3716412 Test Loss: 0.4512026
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 84.09192991256714
Epoch: 9, Steps: 93 | Train Loss: 0.2728952 Vali Loss: 0.3734331 Test Loss: 0.4511753
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 83.76939916610718
Epoch: 10, Steps: 93 | Train Loss: 0.2730160 Vali Loss: 0.3714901 Test Loss: 0.4511724
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 93.46147394180298
Epoch: 11, Steps: 93 | Train Loss: 0.2728619 Vali Loss: 0.3710497 Test Loss: 0.4510608
Validation loss decreased (0.371426 --> 0.371050).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 91.12928366661072
Epoch: 12, Steps: 93 | Train Loss: 0.2729101 Vali Loss: 0.3723417 Test Loss: 0.4509175
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 72.51370763778687
Epoch: 13, Steps: 93 | Train Loss: 0.2727164 Vali Loss: 0.3722266 Test Loss: 0.4509874
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 73.96408081054688
Epoch: 14, Steps: 93 | Train Loss: 0.2726399 Vali Loss: 0.3701620 Test Loss: 0.4511037
Validation loss decreased (0.371050 --> 0.370162).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 67.48531556129456
Epoch: 15, Steps: 93 | Train Loss: 0.2727630 Vali Loss: 0.3724165 Test Loss: 0.4509230
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 84.38077211380005
Epoch: 16, Steps: 93 | Train Loss: 0.2728126 Vali Loss: 0.3712631 Test Loss: 0.4508625
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 87.26614475250244
Epoch: 17, Steps: 93 | Train Loss: 0.2726103 Vali Loss: 0.3724456 Test Loss: 0.4508282
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 72.33517742156982
Epoch: 18, Steps: 93 | Train Loss: 0.2727441 Vali Loss: 0.3717056 Test Loss: 0.4507566
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 69.22795367240906
Epoch: 19, Steps: 93 | Train Loss: 0.2727498 Vali Loss: 0.3717164 Test Loss: 0.4507963
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 77.8725335597992
Epoch: 20, Steps: 93 | Train Loss: 0.2726468 Vali Loss: 0.3722620 Test Loss: 0.4507645
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 77.12769556045532
Epoch: 21, Steps: 93 | Train Loss: 0.2725533 Vali Loss: 0.3716286 Test Loss: 0.4507367
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 78.1480975151062
Epoch: 22, Steps: 93 | Train Loss: 0.2727483 Vali Loss: 0.3728504 Test Loss: 0.4508395
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 73.38048315048218
Epoch: 23, Steps: 93 | Train Loss: 0.2726184 Vali Loss: 0.3709716 Test Loss: 0.4507104
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 73.36694312095642
Epoch: 24, Steps: 93 | Train Loss: 0.2725560 Vali Loss: 0.3710009 Test Loss: 0.4507633
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j96_H10_FITS_custom_ftM_sl180_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.45084962248802185, mae:0.29726681113243103, rse:0.5559930205345154, corr:[0.2721127  0.2904282  0.29041588 0.28980923 0.28952473 0.28992695
 0.2900242  0.28977802 0.2893334  0.2897244  0.28955922 0.28934067
 0.28945547 0.28918317 0.2891259  0.28901362 0.28896555 0.28872764
 0.28814465 0.28883705 0.28890675 0.28891745 0.28948313 0.28937912
 0.2895076  0.28910175 0.2884189  0.28829923 0.28855613 0.28908694
 0.2894014  0.28956574 0.2897513  0.28960818 0.289422   0.28900775
 0.2888314  0.2886529  0.28853506 0.2879663  0.28839603 0.2887195
 0.28867516 0.28838757 0.28780672 0.28819323 0.28803444 0.28838912
 0.28847754 0.28789294 0.28748304 0.28769794 0.28850824 0.28906503
 0.28911877 0.28850687 0.28859955 0.2885108  0.28863272 0.28798962
 0.28801754 0.28848955 0.2880988  0.28762615 0.28787374 0.28763828
 0.287897   0.28795367 0.2873503  0.28756067 0.28807926 0.28811848
 0.28688097 0.28710297 0.2870191  0.28702042 0.2872233  0.2872697
 0.28730127 0.2873439  0.287977   0.28767964 0.28830454 0.28789377
 0.28766188 0.2862487  0.28679347 0.2871704  0.28714344 0.28707442
 0.28758153 0.28720224 0.287073   0.28577286 0.28710213 0.28722304]
