Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j336_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j336_H10_FITS_custom_ftM_sl180_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11765
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=90, out_features=258, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2562001920.0
params:  23478.0
Trainable parameters:  23478
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 126.39438605308533
Epoch: 1, Steps: 91 | Train Loss: 1.2764440 Vali Loss: 1.2522912 Test Loss: 1.5064529
Validation loss decreased (inf --> 1.252291).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 116.34746789932251
Epoch: 2, Steps: 91 | Train Loss: 0.7652892 Vali Loss: 0.9290217 Test Loss: 1.1262963
Validation loss decreased (1.252291 --> 0.929022).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 116.9284987449646
Epoch: 3, Steps: 91 | Train Loss: 0.5743427 Vali Loss: 0.7781611 Test Loss: 0.9519185
Validation loss decreased (0.929022 --> 0.778161).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 106.40295481681824
Epoch: 4, Steps: 91 | Train Loss: 0.4787500 Vali Loss: 0.6947567 Test Loss: 0.8553964
Validation loss decreased (0.778161 --> 0.694757).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 108.7034044265747
Epoch: 5, Steps: 91 | Train Loss: 0.4222469 Vali Loss: 0.6415849 Test Loss: 0.7934521
Validation loss decreased (0.694757 --> 0.641585).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 117.30856680870056
Epoch: 6, Steps: 91 | Train Loss: 0.3846678 Vali Loss: 0.6044576 Test Loss: 0.7486963
Validation loss decreased (0.641585 --> 0.604458).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 113.31621646881104
Epoch: 7, Steps: 91 | Train Loss: 0.3571826 Vali Loss: 0.5758976 Test Loss: 0.7140209
Validation loss decreased (0.604458 --> 0.575898).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 111.9430046081543
Epoch: 8, Steps: 91 | Train Loss: 0.3358715 Vali Loss: 0.5529087 Test Loss: 0.6858962
Validation loss decreased (0.575898 --> 0.552909).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 110.08325839042664
Epoch: 9, Steps: 91 | Train Loss: 0.3187961 Vali Loss: 0.5329595 Test Loss: 0.6627066
Validation loss decreased (0.552909 --> 0.532959).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 107.52335333824158
Epoch: 10, Steps: 91 | Train Loss: 0.3045624 Vali Loss: 0.5179756 Test Loss: 0.6431122
Validation loss decreased (0.532959 --> 0.517976).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 108.55173850059509
Epoch: 11, Steps: 91 | Train Loss: 0.2927511 Vali Loss: 0.5040962 Test Loss: 0.6261907
Validation loss decreased (0.517976 --> 0.504096).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 111.93687415122986
Epoch: 12, Steps: 91 | Train Loss: 0.2825233 Vali Loss: 0.4925824 Test Loss: 0.6116183
Validation loss decreased (0.504096 --> 0.492582).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 106.01712918281555
Epoch: 13, Steps: 91 | Train Loss: 0.2738232 Vali Loss: 0.4823709 Test Loss: 0.5990368
Validation loss decreased (0.492582 --> 0.482371).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 117.45592832565308
Epoch: 14, Steps: 91 | Train Loss: 0.2663137 Vali Loss: 0.4736280 Test Loss: 0.5882055
Validation loss decreased (0.482371 --> 0.473628).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 123.70210266113281
Epoch: 15, Steps: 91 | Train Loss: 0.2597160 Vali Loss: 0.4662349 Test Loss: 0.5786225
Validation loss decreased (0.473628 --> 0.466235).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 112.6229190826416
Epoch: 16, Steps: 91 | Train Loss: 0.2539406 Vali Loss: 0.4596852 Test Loss: 0.5700157
Validation loss decreased (0.466235 --> 0.459685).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 121.33771657943726
Epoch: 17, Steps: 91 | Train Loss: 0.2487963 Vali Loss: 0.4532571 Test Loss: 0.5626594
Validation loss decreased (0.459685 --> 0.453257).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 127.8778030872345
Epoch: 18, Steps: 91 | Train Loss: 0.2442760 Vali Loss: 0.4479211 Test Loss: 0.5559544
Validation loss decreased (0.453257 --> 0.447921).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 129.68483448028564
Epoch: 19, Steps: 91 | Train Loss: 0.2402434 Vali Loss: 0.4439193 Test Loss: 0.5500059
Validation loss decreased (0.447921 --> 0.443919).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 116.7983078956604
Epoch: 20, Steps: 91 | Train Loss: 0.2365925 Vali Loss: 0.4394046 Test Loss: 0.5447017
Validation loss decreased (0.443919 --> 0.439405).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 108.58616065979004
Epoch: 21, Steps: 91 | Train Loss: 0.2333854 Vali Loss: 0.4356916 Test Loss: 0.5400364
Validation loss decreased (0.439405 --> 0.435692).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 108.56813311576843
Epoch: 22, Steps: 91 | Train Loss: 0.2304449 Vali Loss: 0.4320855 Test Loss: 0.5356740
Validation loss decreased (0.435692 --> 0.432086).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 105.16685271263123
Epoch: 23, Steps: 91 | Train Loss: 0.2278303 Vali Loss: 0.4288660 Test Loss: 0.5318756
Validation loss decreased (0.432086 --> 0.428866).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 105.02478504180908
Epoch: 24, Steps: 91 | Train Loss: 0.2254259 Vali Loss: 0.4266443 Test Loss: 0.5283465
Validation loss decreased (0.428866 --> 0.426644).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 98.85681366920471
Epoch: 25, Steps: 91 | Train Loss: 0.2232698 Vali Loss: 0.4240464 Test Loss: 0.5252328
Validation loss decreased (0.426644 --> 0.424046).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 95.92313838005066
Epoch: 26, Steps: 91 | Train Loss: 0.2213127 Vali Loss: 0.4213444 Test Loss: 0.5223013
Validation loss decreased (0.424046 --> 0.421344).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 93.50439667701721
Epoch: 27, Steps: 91 | Train Loss: 0.2193945 Vali Loss: 0.4194711 Test Loss: 0.5196491
Validation loss decreased (0.421344 --> 0.419471).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 98.27414608001709
Epoch: 28, Steps: 91 | Train Loss: 0.2177945 Vali Loss: 0.4175829 Test Loss: 0.5172147
Validation loss decreased (0.419471 --> 0.417583).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 99.19660782814026
Epoch: 29, Steps: 91 | Train Loss: 0.2162828 Vali Loss: 0.4157596 Test Loss: 0.5148729
Validation loss decreased (0.417583 --> 0.415760).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 94.24715447425842
Epoch: 30, Steps: 91 | Train Loss: 0.2149247 Vali Loss: 0.4139198 Test Loss: 0.5128542
Validation loss decreased (0.415760 --> 0.413920).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 95.60232901573181
Epoch: 31, Steps: 91 | Train Loss: 0.2135291 Vali Loss: 0.4123890 Test Loss: 0.5110202
Validation loss decreased (0.413920 --> 0.412389).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 95.4678852558136
Epoch: 32, Steps: 91 | Train Loss: 0.2123971 Vali Loss: 0.4110479 Test Loss: 0.5092292
Validation loss decreased (0.412389 --> 0.411048).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 94.81417679786682
Epoch: 33, Steps: 91 | Train Loss: 0.2112580 Vali Loss: 0.4099455 Test Loss: 0.5076587
Validation loss decreased (0.411048 --> 0.409945).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 96.08522510528564
Epoch: 34, Steps: 91 | Train Loss: 0.2102412 Vali Loss: 0.4086583 Test Loss: 0.5062407
Validation loss decreased (0.409945 --> 0.408658).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 92.00189661979675
Epoch: 35, Steps: 91 | Train Loss: 0.2093028 Vali Loss: 0.4076763 Test Loss: 0.5048147
Validation loss decreased (0.408658 --> 0.407676).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 97.8361451625824
Epoch: 36, Steps: 91 | Train Loss: 0.2083736 Vali Loss: 0.4064589 Test Loss: 0.5035264
Validation loss decreased (0.407676 --> 0.406459).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 77.33026552200317
Epoch: 37, Steps: 91 | Train Loss: 0.2075175 Vali Loss: 0.4056952 Test Loss: 0.5024069
Validation loss decreased (0.406459 --> 0.405695).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 80.04681515693665
Epoch: 38, Steps: 91 | Train Loss: 0.2068095 Vali Loss: 0.4045838 Test Loss: 0.5012457
Validation loss decreased (0.405695 --> 0.404584).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 79.84044742584229
Epoch: 39, Steps: 91 | Train Loss: 0.2060559 Vali Loss: 0.4036494 Test Loss: 0.5002423
Validation loss decreased (0.404584 --> 0.403649).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 79.12669014930725
Epoch: 40, Steps: 91 | Train Loss: 0.2054033 Vali Loss: 0.4032497 Test Loss: 0.4992737
Validation loss decreased (0.403649 --> 0.403250).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 79.28631114959717
Epoch: 41, Steps: 91 | Train Loss: 0.2047950 Vali Loss: 0.4025539 Test Loss: 0.4983794
Validation loss decreased (0.403250 --> 0.402554).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 80.16521716117859
Epoch: 42, Steps: 91 | Train Loss: 0.2042817 Vali Loss: 0.4018684 Test Loss: 0.4975581
Validation loss decreased (0.402554 --> 0.401868).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 79.56097555160522
Epoch: 43, Steps: 91 | Train Loss: 0.2036680 Vali Loss: 0.4007897 Test Loss: 0.4967941
Validation loss decreased (0.401868 --> 0.400790).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 81.15657329559326
Epoch: 44, Steps: 91 | Train Loss: 0.2032259 Vali Loss: 0.4007410 Test Loss: 0.4960095
Validation loss decreased (0.400790 --> 0.400741).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 79.66663837432861
Epoch: 45, Steps: 91 | Train Loss: 0.2026731 Vali Loss: 0.3999412 Test Loss: 0.4953917
Validation loss decreased (0.400741 --> 0.399941).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 78.88928699493408
Epoch: 46, Steps: 91 | Train Loss: 0.2022658 Vali Loss: 0.3992842 Test Loss: 0.4947149
Validation loss decreased (0.399941 --> 0.399284).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 81.94616651535034
Epoch: 47, Steps: 91 | Train Loss: 0.2018024 Vali Loss: 0.3987865 Test Loss: 0.4941214
Validation loss decreased (0.399284 --> 0.398787).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 131.63964700698853
Epoch: 48, Steps: 91 | Train Loss: 0.2014511 Vali Loss: 0.3988750 Test Loss: 0.4935987
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 126.62920641899109
Epoch: 49, Steps: 91 | Train Loss: 0.2010629 Vali Loss: 0.3977709 Test Loss: 0.4930924
Validation loss decreased (0.398787 --> 0.397771).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 133.21816730499268
Epoch: 50, Steps: 91 | Train Loss: 0.2007587 Vali Loss: 0.3978514 Test Loss: 0.4925682
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 139.7665102481842
Epoch: 51, Steps: 91 | Train Loss: 0.2004293 Vali Loss: 0.3973719 Test Loss: 0.4921355
Validation loss decreased (0.397771 --> 0.397372).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 151.5073528289795
Epoch: 52, Steps: 91 | Train Loss: 0.2001390 Vali Loss: 0.3969767 Test Loss: 0.4917124
Validation loss decreased (0.397372 --> 0.396977).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 139.87664985656738
Epoch: 53, Steps: 91 | Train Loss: 0.1998572 Vali Loss: 0.3966436 Test Loss: 0.4913183
Validation loss decreased (0.396977 --> 0.396644).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 124.54105806350708
Epoch: 54, Steps: 91 | Train Loss: 0.1995546 Vali Loss: 0.3959721 Test Loss: 0.4909188
Validation loss decreased (0.396644 --> 0.395972).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 126.94319176673889
Epoch: 55, Steps: 91 | Train Loss: 0.1992956 Vali Loss: 0.3960057 Test Loss: 0.4905587
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 120.27018976211548
Epoch: 56, Steps: 91 | Train Loss: 0.1990068 Vali Loss: 0.3956380 Test Loss: 0.4902024
Validation loss decreased (0.395972 --> 0.395638).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 119.094313621521
Epoch: 57, Steps: 91 | Train Loss: 0.1987545 Vali Loss: 0.3951474 Test Loss: 0.4899016
Validation loss decreased (0.395638 --> 0.395147).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 121.39687824249268
Epoch: 58, Steps: 91 | Train Loss: 0.1985986 Vali Loss: 0.3953077 Test Loss: 0.4895809
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 119.39822053909302
Epoch: 59, Steps: 91 | Train Loss: 0.1984046 Vali Loss: 0.3949736 Test Loss: 0.4893000
Validation loss decreased (0.395147 --> 0.394974).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 120.61037111282349
Epoch: 60, Steps: 91 | Train Loss: 0.1982208 Vali Loss: 0.3946536 Test Loss: 0.4890366
Validation loss decreased (0.394974 --> 0.394654).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 116.2068738937378
Epoch: 61, Steps: 91 | Train Loss: 0.1980092 Vali Loss: 0.3945611 Test Loss: 0.4887714
Validation loss decreased (0.394654 --> 0.394561).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 121.92654800415039
Epoch: 62, Steps: 91 | Train Loss: 0.1977398 Vali Loss: 0.3944288 Test Loss: 0.4885084
Validation loss decreased (0.394561 --> 0.394429).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 110.90891695022583
Epoch: 63, Steps: 91 | Train Loss: 0.1976759 Vali Loss: 0.3939807 Test Loss: 0.4883023
Validation loss decreased (0.394429 --> 0.393981).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 103.83006763458252
Epoch: 64, Steps: 91 | Train Loss: 0.1974219 Vali Loss: 0.3938822 Test Loss: 0.4881103
Validation loss decreased (0.393981 --> 0.393882).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 106.43076777458191
Epoch: 65, Steps: 91 | Train Loss: 0.1973907 Vali Loss: 0.3936717 Test Loss: 0.4878871
Validation loss decreased (0.393882 --> 0.393672).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 103.26985383033752
Epoch: 66, Steps: 91 | Train Loss: 0.1972276 Vali Loss: 0.3937035 Test Loss: 0.4876942
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 103.18774247169495
Epoch: 67, Steps: 91 | Train Loss: 0.1971256 Vali Loss: 0.3933784 Test Loss: 0.4875086
Validation loss decreased (0.393672 --> 0.393378).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 102.25549817085266
Epoch: 68, Steps: 91 | Train Loss: 0.1970102 Vali Loss: 0.3933655 Test Loss: 0.4873369
Validation loss decreased (0.393378 --> 0.393365).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 106.57054615020752
Epoch: 69, Steps: 91 | Train Loss: 0.1968379 Vali Loss: 0.3932588 Test Loss: 0.4871637
Validation loss decreased (0.393365 --> 0.393259).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 106.60948085784912
Epoch: 70, Steps: 91 | Train Loss: 0.1967358 Vali Loss: 0.3928900 Test Loss: 0.4870172
Validation loss decreased (0.393259 --> 0.392890).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 106.51600861549377
Epoch: 71, Steps: 91 | Train Loss: 0.1967028 Vali Loss: 0.3932066 Test Loss: 0.4868806
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 101.82955598831177
Epoch: 72, Steps: 91 | Train Loss: 0.1964444 Vali Loss: 0.3929985 Test Loss: 0.4867423
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 97.84188747406006
Epoch: 73, Steps: 91 | Train Loss: 0.1964209 Vali Loss: 0.3926134 Test Loss: 0.4866047
Validation loss decreased (0.392890 --> 0.392613).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 109.85296416282654
Epoch: 74, Steps: 91 | Train Loss: 0.1963098 Vali Loss: 0.3926472 Test Loss: 0.4864725
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 132.48072361946106
Epoch: 75, Steps: 91 | Train Loss: 0.1962215 Vali Loss: 0.3927010 Test Loss: 0.4863743
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 112.81480598449707
Epoch: 76, Steps: 91 | Train Loss: 0.1961887 Vali Loss: 0.3924591 Test Loss: 0.4862596
Validation loss decreased (0.392613 --> 0.392459).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 116.93419575691223
Epoch: 77, Steps: 91 | Train Loss: 0.1961133 Vali Loss: 0.3922390 Test Loss: 0.4861461
Validation loss decreased (0.392459 --> 0.392239).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 109.85129261016846
Epoch: 78, Steps: 91 | Train Loss: 0.1960404 Vali Loss: 0.3924784 Test Loss: 0.4860420
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 102.60906410217285
Epoch: 79, Steps: 91 | Train Loss: 0.1960029 Vali Loss: 0.3919015 Test Loss: 0.4859510
Validation loss decreased (0.392239 --> 0.391902).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 102.87294363975525
Epoch: 80, Steps: 91 | Train Loss: 0.1959313 Vali Loss: 0.3923191 Test Loss: 0.4858637
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 108.21504616737366
Epoch: 81, Steps: 91 | Train Loss: 0.1958263 Vali Loss: 0.3921915 Test Loss: 0.4857786
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 143.21050596237183
Epoch: 82, Steps: 91 | Train Loss: 0.1958197 Vali Loss: 0.3918880 Test Loss: 0.4856969
Validation loss decreased (0.391902 --> 0.391888).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 138.07836413383484
Epoch: 83, Steps: 91 | Train Loss: 0.1957097 Vali Loss: 0.3918620 Test Loss: 0.4856195
Validation loss decreased (0.391888 --> 0.391862).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 137.7133641242981
Epoch: 84, Steps: 91 | Train Loss: 0.1956707 Vali Loss: 0.3917240 Test Loss: 0.4855461
Validation loss decreased (0.391862 --> 0.391724).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 134.43728113174438
Epoch: 85, Steps: 91 | Train Loss: 0.1955579 Vali Loss: 0.3916406 Test Loss: 0.4854800
Validation loss decreased (0.391724 --> 0.391641).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 144.7026743888855
Epoch: 86, Steps: 91 | Train Loss: 0.1955854 Vali Loss: 0.3917703 Test Loss: 0.4854136
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 148.23585438728333
Epoch: 87, Steps: 91 | Train Loss: 0.1955364 Vali Loss: 0.3916490 Test Loss: 0.4853541
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 138.11540722846985
Epoch: 88, Steps: 91 | Train Loss: 0.1954976 Vali Loss: 0.3916852 Test Loss: 0.4852930
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 134.80240488052368
Epoch: 89, Steps: 91 | Train Loss: 0.1954296 Vali Loss: 0.3915266 Test Loss: 0.4852306
Validation loss decreased (0.391641 --> 0.391527).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 135.81147170066833
Epoch: 90, Steps: 91 | Train Loss: 0.1954124 Vali Loss: 0.3913318 Test Loss: 0.4851808
Validation loss decreased (0.391527 --> 0.391332).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 137.43684458732605
Epoch: 91, Steps: 91 | Train Loss: 0.1953744 Vali Loss: 0.3913143 Test Loss: 0.4851333
Validation loss decreased (0.391332 --> 0.391314).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 142.41696453094482
Epoch: 92, Steps: 91 | Train Loss: 0.1952870 Vali Loss: 0.3913334 Test Loss: 0.4850858
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 140.02531933784485
Epoch: 93, Steps: 91 | Train Loss: 0.1952922 Vali Loss: 0.3915055 Test Loss: 0.4850396
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 134.4655020236969
Epoch: 94, Steps: 91 | Train Loss: 0.1952638 Vali Loss: 0.3914993 Test Loss: 0.4849959
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 134.53552675247192
Epoch: 95, Steps: 91 | Train Loss: 0.1952035 Vali Loss: 0.3915552 Test Loss: 0.4849558
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 138.97953414916992
Epoch: 96, Steps: 91 | Train Loss: 0.1951121 Vali Loss: 0.3912749 Test Loss: 0.4849175
Validation loss decreased (0.391314 --> 0.391275).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 138.2396411895752
Epoch: 97, Steps: 91 | Train Loss: 0.1951738 Vali Loss: 0.3914710 Test Loss: 0.4848767
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 135.987895488739
Epoch: 98, Steps: 91 | Train Loss: 0.1950786 Vali Loss: 0.3913075 Test Loss: 0.4848404
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 133.6995723247528
Epoch: 99, Steps: 91 | Train Loss: 0.1950956 Vali Loss: 0.3914417 Test Loss: 0.4848079
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 136.47844862937927
Epoch: 100, Steps: 91 | Train Loss: 0.1950607 Vali Loss: 0.3909809 Test Loss: 0.4847744
Validation loss decreased (0.391275 --> 0.390981).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 11765
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=90, out_features=258, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2562001920.0
params:  23478.0
Trainable parameters:  23478
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 132.25364327430725
Epoch: 1, Steps: 91 | Train Loss: 0.2922642 Vali Loss: 0.3859312 Test Loss: 0.4786162
Validation loss decreased (inf --> 0.385931).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 138.82653975486755
Epoch: 2, Steps: 91 | Train Loss: 0.2901791 Vali Loss: 0.3852150 Test Loss: 0.4782692
Validation loss decreased (0.385931 --> 0.385215).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 124.88191437721252
Epoch: 3, Steps: 91 | Train Loss: 0.2900673 Vali Loss: 0.3850373 Test Loss: 0.4782450
Validation loss decreased (0.385215 --> 0.385037).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 125.09754538536072
Epoch: 4, Steps: 91 | Train Loss: 0.2897420 Vali Loss: 0.3852659 Test Loss: 0.4782268
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 113.26533269882202
Epoch: 5, Steps: 91 | Train Loss: 0.2898571 Vali Loss: 0.3849669 Test Loss: 0.4783850
Validation loss decreased (0.385037 --> 0.384967).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 111.94271421432495
Epoch: 6, Steps: 91 | Train Loss: 0.2898792 Vali Loss: 0.3849155 Test Loss: 0.4783906
Validation loss decreased (0.384967 --> 0.384916).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 113.60533142089844
Epoch: 7, Steps: 91 | Train Loss: 0.2897217 Vali Loss: 0.3853474 Test Loss: 0.4781747
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 111.01056790351868
Epoch: 8, Steps: 91 | Train Loss: 0.2896354 Vali Loss: 0.3848427 Test Loss: 0.4781138
Validation loss decreased (0.384916 --> 0.384843).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 112.11810612678528
Epoch: 9, Steps: 91 | Train Loss: 0.2895630 Vali Loss: 0.3850738 Test Loss: 0.4782261
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 113.15240478515625
Epoch: 10, Steps: 91 | Train Loss: 0.2896501 Vali Loss: 0.3850333 Test Loss: 0.4782097
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 114.02696228027344
Epoch: 11, Steps: 91 | Train Loss: 0.2896185 Vali Loss: 0.3846515 Test Loss: 0.4779577
Validation loss decreased (0.384843 --> 0.384651).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 114.8185224533081
Epoch: 12, Steps: 91 | Train Loss: 0.2894499 Vali Loss: 0.3849981 Test Loss: 0.4781444
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 115.44179654121399
Epoch: 13, Steps: 91 | Train Loss: 0.2895348 Vali Loss: 0.3847687 Test Loss: 0.4781460
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 113.47900819778442
Epoch: 14, Steps: 91 | Train Loss: 0.2895574 Vali Loss: 0.3847751 Test Loss: 0.4780605
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 115.93515300750732
Epoch: 15, Steps: 91 | Train Loss: 0.2895587 Vali Loss: 0.3847243 Test Loss: 0.4780389
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 119.30624914169312
Epoch: 16, Steps: 91 | Train Loss: 0.2895891 Vali Loss: 0.3848783 Test Loss: 0.4779970
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 109.7711570262909
Epoch: 17, Steps: 91 | Train Loss: 0.2895774 Vali Loss: 0.3846600 Test Loss: 0.4779482
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 101.44565510749817
Epoch: 18, Steps: 91 | Train Loss: 0.2895614 Vali Loss: 0.3847891 Test Loss: 0.4779935
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 102.64417958259583
Epoch: 19, Steps: 91 | Train Loss: 0.2895607 Vali Loss: 0.3847102 Test Loss: 0.4779142
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 103.29758977890015
Epoch: 20, Steps: 91 | Train Loss: 0.2895783 Vali Loss: 0.3848085 Test Loss: 0.4780136
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 100.84412407875061
Epoch: 21, Steps: 91 | Train Loss: 0.2895365 Vali Loss: 0.3844138 Test Loss: 0.4780153
Validation loss decreased (0.384651 --> 0.384414).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 110.1721601486206
Epoch: 22, Steps: 91 | Train Loss: 0.2895196 Vali Loss: 0.3847134 Test Loss: 0.4778164
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 105.28077054023743
Epoch: 23, Steps: 91 | Train Loss: 0.2895177 Vali Loss: 0.3848274 Test Loss: 0.4780489
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 96.52431654930115
Epoch: 24, Steps: 91 | Train Loss: 0.2895117 Vali Loss: 0.3849641 Test Loss: 0.4778791
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 93.78879022598267
Epoch: 25, Steps: 91 | Train Loss: 0.2895580 Vali Loss: 0.3846667 Test Loss: 0.4778479
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 81.50527262687683
Epoch: 26, Steps: 91 | Train Loss: 0.2894905 Vali Loss: 0.3847440 Test Loss: 0.4778520
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 102.5657069683075
Epoch: 27, Steps: 91 | Train Loss: 0.2894778 Vali Loss: 0.3849128 Test Loss: 0.4779562
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 104.83260107040405
Epoch: 28, Steps: 91 | Train Loss: 0.2894644 Vali Loss: 0.3849549 Test Loss: 0.4779340
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 97.17210292816162
Epoch: 29, Steps: 91 | Train Loss: 0.2894523 Vali Loss: 0.3848610 Test Loss: 0.4778811
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 94.29177904129028
Epoch: 30, Steps: 91 | Train Loss: 0.2894660 Vali Loss: 0.3848189 Test Loss: 0.4779577
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 102.47980093955994
Epoch: 31, Steps: 91 | Train Loss: 0.2894873 Vali Loss: 0.3845536 Test Loss: 0.4779755
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j336_H10_FITS_custom_ftM_sl180_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4760444760322571, mae:0.3076646327972412, rse:0.5670521855354309, corr:[0.26529673 0.28206998 0.28235087 0.28182182 0.28100455 0.28086567
 0.2814341  0.28159013 0.28092545 0.28105527 0.28121826 0.28143713
 0.28071707 0.28058684 0.2803471  0.2808273  0.2806308  0.28008568
 0.28011882 0.28030938 0.2801776  0.28081423 0.2818384  0.2816419
 0.2810437  0.28034484 0.28041905 0.2804287  0.28028512 0.28018472
 0.28011864 0.28044736 0.28055838 0.28065577 0.28066736 0.28061217
 0.28012574 0.2799684  0.27991384 0.28049454 0.28101218 0.28125605
 0.28089604 0.28061345 0.28071383 0.280976   0.28084973 0.28054875
 0.2802253  0.2801027  0.28057435 0.2805748  0.2803115  0.28041682
 0.28083867 0.28087497 0.28084058 0.280883   0.28112522 0.28087023
 0.28070387 0.28077874 0.28057984 0.28078505 0.28028858 0.2801828
 0.2800756  0.2797806  0.27986348 0.2798231  0.27964035 0.27982923
 0.27969542 0.27945095 0.27989477 0.27968314 0.2794145  0.27906403
 0.27920645 0.2792644  0.27910662 0.27950796 0.27965    0.27942044
 0.27930027 0.27951196 0.27938685 0.27950343 0.2795532  0.27957606
 0.27962923 0.27929395 0.2798974  0.27995917 0.2795912  0.27987373
 0.27976596 0.27925852 0.27926183 0.2791463  0.27952042 0.27962378
 0.2793763  0.2795961  0.2794627  0.27917835 0.27912712 0.27966276
 0.27981874 0.2792184  0.27919814 0.27974972 0.2796758  0.2794541
 0.2793416  0.27889296 0.2787821  0.27860954 0.27846757 0.2786036
 0.27884287 0.27896157 0.27888697 0.27905828 0.2792033  0.27925742
 0.27905634 0.27886042 0.27874306 0.27877855 0.27921316 0.27947614
 0.28007737 0.28015012 0.27964598 0.27976456 0.27980822 0.27976036
 0.2797239  0.27951995 0.27976075 0.2801051  0.28056931 0.2809482
 0.28037906 0.28043112 0.2805105  0.28030428 0.28034535 0.28005075
 0.28008416 0.28043103 0.28036973 0.28049698 0.2811397  0.281281
 0.28204352 0.28273264 0.2820777  0.28190735 0.28181055 0.28132212
 0.28088903 0.28150463 0.28219408 0.2832059  0.28421018 0.28441358
 0.2837373  0.2828224  0.28215265 0.28162643 0.28190657 0.28201643
 0.281924   0.28216112 0.28172854 0.2815056  0.28183833 0.28196618
 0.28188288 0.28213975 0.2813254  0.28145054 0.28138903 0.2808077
 0.28067335 0.28066418 0.28120634 0.28186005 0.2824705  0.28207177
 0.28145364 0.28121743 0.28125784 0.28085792 0.28065118 0.28128073
 0.28118086 0.28133187 0.28137824 0.28127953 0.28096232 0.28081423
 0.2808463  0.28064117 0.2806725  0.28088596 0.28119683 0.28101444
 0.28065926 0.2803647  0.28033972 0.28008375 0.279909   0.2802538
 0.28027427 0.2798539  0.28010374 0.28005004 0.27967474 0.27992058
 0.28022143 0.28024414 0.27991125 0.28018275 0.28023386 0.2800104
 0.2799246  0.27995855 0.28001776 0.27967557 0.27986398 0.28036255
 0.28009358 0.27972123 0.279694   0.27955782 0.27935827 0.27897626
 0.2790706  0.27935424 0.27966842 0.2794016  0.27922648 0.27908018
 0.27904007 0.2793314  0.27916074 0.27944708 0.2796535  0.2792247
 0.2787366  0.27840555 0.27828512 0.2781442  0.27872652 0.27918527
 0.27903023 0.27835423 0.27857783 0.27879643 0.27861124 0.27924916
 0.2793372  0.2791493  0.27936667 0.2796284  0.27973452 0.27926156
 0.27895507 0.27912226 0.27899614 0.27896723 0.27947012 0.27974632
 0.27901417 0.27857515 0.27868655 0.2785874  0.27875385 0.27877146
 0.27825966 0.2779493  0.27828306 0.27825162 0.27775604 0.2780371
 0.27841222 0.2785821  0.2786358  0.27867004 0.27903625 0.27851608
 0.27803513 0.278089   0.27802908 0.2779492  0.2783521  0.27855334
 0.278765   0.27899176 0.27922356 0.27889135 0.27875614 0.27895135
 0.27871302 0.2791077  0.27931005 0.27917343 0.2796961  0.27955306
 0.27894393 0.27905154 0.27910927 0.27937803 0.27957773 0.27931648
 0.279468   0.27944443 0.27921402 0.2795454  0.28032133 0.2806779
 0.28192776 0.28186452 0.28111893 0.28080463 0.2806771  0.28042483
 0.2804241  0.28030178 0.28077137 0.28118503 0.28163487 0.28029802]
