Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14299545600.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 128.43455338478088
Epoch: 1, Steps: 88 | Train Loss: 1.0595264 Vali Loss: 1.1102067 Test Loss: 1.2780925
Validation loss decreased (inf --> 1.110207).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 121.89792513847351
Epoch: 2, Steps: 88 | Train Loss: 0.7968651 Vali Loss: 0.9965819 Test Loss: 1.1458969
Validation loss decreased (1.110207 --> 0.996582).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 126.26291275024414
Epoch: 3, Steps: 88 | Train Loss: 0.6991418 Vali Loss: 0.9373901 Test Loss: 1.0773299
Validation loss decreased (0.996582 --> 0.937390).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 131.64513278007507
Epoch: 4, Steps: 88 | Train Loss: 0.6286066 Vali Loss: 0.8877959 Test Loss: 1.0217334
Validation loss decreased (0.937390 --> 0.887796).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 129.99564671516418
Epoch: 5, Steps: 88 | Train Loss: 0.5710385 Vali Loss: 0.8454658 Test Loss: 0.9733009
Validation loss decreased (0.887796 --> 0.845466).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 119.5865638256073
Epoch: 6, Steps: 88 | Train Loss: 0.5223810 Vali Loss: 0.8053078 Test Loss: 0.9274560
Validation loss decreased (0.845466 --> 0.805308).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 139.83729577064514
Epoch: 7, Steps: 88 | Train Loss: 0.4807109 Vali Loss: 0.7717506 Test Loss: 0.8888556
Validation loss decreased (0.805308 --> 0.771751).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 125.90133762359619
Epoch: 8, Steps: 88 | Train Loss: 0.4444199 Vali Loss: 0.7392086 Test Loss: 0.8517929
Validation loss decreased (0.771751 --> 0.739209).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 123.93684673309326
Epoch: 9, Steps: 88 | Train Loss: 0.4126822 Vali Loss: 0.7098824 Test Loss: 0.8182570
Validation loss decreased (0.739209 --> 0.709882).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 122.43406796455383
Epoch: 10, Steps: 88 | Train Loss: 0.3845794 Vali Loss: 0.6827552 Test Loss: 0.7875454
Validation loss decreased (0.709882 --> 0.682755).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 189.86713027954102
Epoch: 11, Steps: 88 | Train Loss: 0.3597357 Vali Loss: 0.6621239 Test Loss: 0.7636394
Validation loss decreased (0.682755 --> 0.662124).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 119.90796947479248
Epoch: 12, Steps: 88 | Train Loss: 0.3375705 Vali Loss: 0.6388891 Test Loss: 0.7376529
Validation loss decreased (0.662124 --> 0.638889).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 104.18054795265198
Epoch: 13, Steps: 88 | Train Loss: 0.3177032 Vali Loss: 0.6185258 Test Loss: 0.7138448
Validation loss decreased (0.638889 --> 0.618526).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 99.60584139823914
Epoch: 14, Steps: 88 | Train Loss: 0.2998265 Vali Loss: 0.6018557 Test Loss: 0.6953172
Validation loss decreased (0.618526 --> 0.601856).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 95.39859628677368
Epoch: 15, Steps: 88 | Train Loss: 0.2836469 Vali Loss: 0.5866647 Test Loss: 0.6777061
Validation loss decreased (0.601856 --> 0.586665).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 93.16779589653015
Epoch: 16, Steps: 88 | Train Loss: 0.2690888 Vali Loss: 0.5723346 Test Loss: 0.6616193
Validation loss decreased (0.586665 --> 0.572335).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 88.96090483665466
Epoch: 17, Steps: 88 | Train Loss: 0.2559056 Vali Loss: 0.5577240 Test Loss: 0.6455637
Validation loss decreased (0.572335 --> 0.557724).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 96.99160099029541
Epoch: 18, Steps: 88 | Train Loss: 0.2439021 Vali Loss: 0.5459110 Test Loss: 0.6319386
Validation loss decreased (0.557724 --> 0.545911).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 90.39648413658142
Epoch: 19, Steps: 88 | Train Loss: 0.2329410 Vali Loss: 0.5345418 Test Loss: 0.6187083
Validation loss decreased (0.545911 --> 0.534542).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 94.31181526184082
Epoch: 20, Steps: 88 | Train Loss: 0.2229567 Vali Loss: 0.5232727 Test Loss: 0.6058726
Validation loss decreased (0.534542 --> 0.523273).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 90.66373586654663
Epoch: 21, Steps: 88 | Train Loss: 0.2137847 Vali Loss: 0.5127134 Test Loss: 0.5949210
Validation loss decreased (0.523273 --> 0.512713).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 100.40293550491333
Epoch: 22, Steps: 88 | Train Loss: 0.2054194 Vali Loss: 0.5039057 Test Loss: 0.5844988
Validation loss decreased (0.512713 --> 0.503906).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 94.44467973709106
Epoch: 23, Steps: 88 | Train Loss: 0.1976543 Vali Loss: 0.4951524 Test Loss: 0.5747374
Validation loss decreased (0.503906 --> 0.495152).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 96.5409004688263
Epoch: 24, Steps: 88 | Train Loss: 0.1906101 Vali Loss: 0.4888556 Test Loss: 0.5675031
Validation loss decreased (0.495152 --> 0.488856).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 80.14507150650024
Epoch: 25, Steps: 88 | Train Loss: 0.1840659 Vali Loss: 0.4813557 Test Loss: 0.5587267
Validation loss decreased (0.488856 --> 0.481356).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 81.39881253242493
Epoch: 26, Steps: 88 | Train Loss: 0.1780377 Vali Loss: 0.4753840 Test Loss: 0.5515438
Validation loss decreased (0.481356 --> 0.475384).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 78.9572365283966
Epoch: 27, Steps: 88 | Train Loss: 0.1724799 Vali Loss: 0.4686346 Test Loss: 0.5443988
Validation loss decreased (0.475384 --> 0.468635).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 79.36177825927734
Epoch: 28, Steps: 88 | Train Loss: 0.1673142 Vali Loss: 0.4628659 Test Loss: 0.5382566
Validation loss decreased (0.468635 --> 0.462866).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 81.51936078071594
Epoch: 29, Steps: 88 | Train Loss: 0.1624946 Vali Loss: 0.4567499 Test Loss: 0.5315762
Validation loss decreased (0.462866 --> 0.456750).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 78.05300545692444
Epoch: 30, Steps: 88 | Train Loss: 0.1580692 Vali Loss: 0.4533196 Test Loss: 0.5271095
Validation loss decreased (0.456750 --> 0.453320).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 79.56159400939941
Epoch: 31, Steps: 88 | Train Loss: 0.1539289 Vali Loss: 0.4479555 Test Loss: 0.5210847
Validation loss decreased (0.453320 --> 0.447956).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 81.37080216407776
Epoch: 32, Steps: 88 | Train Loss: 0.1500977 Vali Loss: 0.4442735 Test Loss: 0.5168399
Validation loss decreased (0.447956 --> 0.444274).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 77.76954483985901
Epoch: 33, Steps: 88 | Train Loss: 0.1464918 Vali Loss: 0.4390131 Test Loss: 0.5117676
Validation loss decreased (0.444274 --> 0.439013).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 78.60421848297119
Epoch: 34, Steps: 88 | Train Loss: 0.1431702 Vali Loss: 0.4360432 Test Loss: 0.5075378
Validation loss decreased (0.439013 --> 0.436043).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 75.60558605194092
Epoch: 35, Steps: 88 | Train Loss: 0.1400801 Vali Loss: 0.4322304 Test Loss: 0.5040315
Validation loss decreased (0.436043 --> 0.432230).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 108.19749784469604
Epoch: 36, Steps: 88 | Train Loss: 0.1371289 Vali Loss: 0.4282937 Test Loss: 0.5001156
Validation loss decreased (0.432230 --> 0.428294).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 112.70934844017029
Epoch: 37, Steps: 88 | Train Loss: 0.1344248 Vali Loss: 0.4254874 Test Loss: 0.4969541
Validation loss decreased (0.428294 --> 0.425487).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 108.0384726524353
Epoch: 38, Steps: 88 | Train Loss: 0.1318618 Vali Loss: 0.4231133 Test Loss: 0.4940948
Validation loss decreased (0.425487 --> 0.423113).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 124.68198418617249
Epoch: 39, Steps: 88 | Train Loss: 0.1294354 Vali Loss: 0.4195137 Test Loss: 0.4904555
Validation loss decreased (0.423113 --> 0.419514).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 112.2410044670105
Epoch: 40, Steps: 88 | Train Loss: 0.1272126 Vali Loss: 0.4171655 Test Loss: 0.4878631
Validation loss decreased (0.419514 --> 0.417166).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 147.8123002052307
Epoch: 41, Steps: 88 | Train Loss: 0.1250971 Vali Loss: 0.4146992 Test Loss: 0.4849030
Validation loss decreased (0.417166 --> 0.414699).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 141.1016800403595
Epoch: 42, Steps: 88 | Train Loss: 0.1231176 Vali Loss: 0.4126409 Test Loss: 0.4827259
Validation loss decreased (0.414699 --> 0.412641).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 168.15858006477356
Epoch: 43, Steps: 88 | Train Loss: 0.1212752 Vali Loss: 0.4101284 Test Loss: 0.4802363
Validation loss decreased (0.412641 --> 0.410128).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 138.92482829093933
Epoch: 44, Steps: 88 | Train Loss: 0.1195124 Vali Loss: 0.4075381 Test Loss: 0.4776408
Validation loss decreased (0.410128 --> 0.407538).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 149.82617044448853
Epoch: 45, Steps: 88 | Train Loss: 0.1178464 Vali Loss: 0.4069255 Test Loss: 0.4762340
Validation loss decreased (0.407538 --> 0.406926).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 137.01480841636658
Epoch: 46, Steps: 88 | Train Loss: 0.1162997 Vali Loss: 0.4044968 Test Loss: 0.4743147
Validation loss decreased (0.406926 --> 0.404497).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 161.67484498023987
Epoch: 47, Steps: 88 | Train Loss: 0.1148469 Vali Loss: 0.4024334 Test Loss: 0.4720527
Validation loss decreased (0.404497 --> 0.402433).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 141.9062840938568
Epoch: 48, Steps: 88 | Train Loss: 0.1134741 Vali Loss: 0.4014128 Test Loss: 0.4707606
Validation loss decreased (0.402433 --> 0.401413).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 159.7438108921051
Epoch: 49, Steps: 88 | Train Loss: 0.1121627 Vali Loss: 0.3992332 Test Loss: 0.4683666
Validation loss decreased (0.401413 --> 0.399233).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 204.40913200378418
Epoch: 50, Steps: 88 | Train Loss: 0.1109280 Vali Loss: 0.3980486 Test Loss: 0.4671329
Validation loss decreased (0.399233 --> 0.398049).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 219.83745169639587
Epoch: 51, Steps: 88 | Train Loss: 0.1097696 Vali Loss: 0.3966720 Test Loss: 0.4657246
Validation loss decreased (0.398049 --> 0.396672).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 145.47504091262817
Epoch: 52, Steps: 88 | Train Loss: 0.1086716 Vali Loss: 0.3958608 Test Loss: 0.4643269
Validation loss decreased (0.396672 --> 0.395861).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 209.434716463089
Epoch: 53, Steps: 88 | Train Loss: 0.1076464 Vali Loss: 0.3950550 Test Loss: 0.4633455
Validation loss decreased (0.395861 --> 0.395055).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 151.481032371521
Epoch: 54, Steps: 88 | Train Loss: 0.1066537 Vali Loss: 0.3937930 Test Loss: 0.4618168
Validation loss decreased (0.395055 --> 0.393793).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 182.62707662582397
Epoch: 55, Steps: 88 | Train Loss: 0.1057159 Vali Loss: 0.3922596 Test Loss: 0.4607933
Validation loss decreased (0.393793 --> 0.392260).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 147.45692038536072
Epoch: 56, Steps: 88 | Train Loss: 0.1048550 Vali Loss: 0.3912989 Test Loss: 0.4597078
Validation loss decreased (0.392260 --> 0.391299).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 160.89728593826294
Epoch: 57, Steps: 88 | Train Loss: 0.1040121 Vali Loss: 0.3901795 Test Loss: 0.4586387
Validation loss decreased (0.391299 --> 0.390180).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 159.60772275924683
Epoch: 58, Steps: 88 | Train Loss: 0.1032225 Vali Loss: 0.3899390 Test Loss: 0.4576250
Validation loss decreased (0.390180 --> 0.389939).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 162.73186421394348
Epoch: 59, Steps: 88 | Train Loss: 0.1024846 Vali Loss: 0.3889624 Test Loss: 0.4568613
Validation loss decreased (0.389939 --> 0.388962).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 259.42876505851746
Epoch: 60, Steps: 88 | Train Loss: 0.1017750 Vali Loss: 0.3878248 Test Loss: 0.4558494
Validation loss decreased (0.388962 --> 0.387825).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 153.7169373035431
Epoch: 61, Steps: 88 | Train Loss: 0.1011002 Vali Loss: 0.3867573 Test Loss: 0.4551054
Validation loss decreased (0.387825 --> 0.386757).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 158.4900417327881
Epoch: 62, Steps: 88 | Train Loss: 0.1004741 Vali Loss: 0.3863661 Test Loss: 0.4541634
Validation loss decreased (0.386757 --> 0.386366).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 164.60214757919312
Epoch: 63, Steps: 88 | Train Loss: 0.0998535 Vali Loss: 0.3851219 Test Loss: 0.4534659
Validation loss decreased (0.386366 --> 0.385122).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 153.17034220695496
Epoch: 64, Steps: 88 | Train Loss: 0.0992837 Vali Loss: 0.3852473 Test Loss: 0.4527267
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 148.42883777618408
Epoch: 65, Steps: 88 | Train Loss: 0.0987110 Vali Loss: 0.3841054 Test Loss: 0.4520653
Validation loss decreased (0.385122 --> 0.384105).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 211.64621233940125
Epoch: 66, Steps: 88 | Train Loss: 0.0982366 Vali Loss: 0.3832288 Test Loss: 0.4514187
Validation loss decreased (0.384105 --> 0.383229).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 152.74196410179138
Epoch: 67, Steps: 88 | Train Loss: 0.0977348 Vali Loss: 0.3828301 Test Loss: 0.4508035
Validation loss decreased (0.383229 --> 0.382830).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 203.44826650619507
Epoch: 68, Steps: 88 | Train Loss: 0.0972655 Vali Loss: 0.3823421 Test Loss: 0.4502379
Validation loss decreased (0.382830 --> 0.382342).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 211.977525472641
Epoch: 69, Steps: 88 | Train Loss: 0.0968481 Vali Loss: 0.3817005 Test Loss: 0.4496698
Validation loss decreased (0.382342 --> 0.381701).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 149.1330680847168
Epoch: 70, Steps: 88 | Train Loss: 0.0964366 Vali Loss: 0.3810407 Test Loss: 0.4491282
Validation loss decreased (0.381701 --> 0.381041).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 159.55257749557495
Epoch: 71, Steps: 88 | Train Loss: 0.0960237 Vali Loss: 0.3809302 Test Loss: 0.4487079
Validation loss decreased (0.381041 --> 0.380930).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 234.365079164505
Epoch: 72, Steps: 88 | Train Loss: 0.0956468 Vali Loss: 0.3805798 Test Loss: 0.4482625
Validation loss decreased (0.380930 --> 0.380580).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 145.58509230613708
Epoch: 73, Steps: 88 | Train Loss: 0.0952970 Vali Loss: 0.3798540 Test Loss: 0.4478110
Validation loss decreased (0.380580 --> 0.379854).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 130.6000566482544
Epoch: 74, Steps: 88 | Train Loss: 0.0949685 Vali Loss: 0.3796707 Test Loss: 0.4473686
Validation loss decreased (0.379854 --> 0.379671).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 153.4937436580658
Epoch: 75, Steps: 88 | Train Loss: 0.0946432 Vali Loss: 0.3794435 Test Loss: 0.4469524
Validation loss decreased (0.379671 --> 0.379443).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 124.78567504882812
Epoch: 76, Steps: 88 | Train Loss: 0.0943004 Vali Loss: 0.3789010 Test Loss: 0.4466425
Validation loss decreased (0.379443 --> 0.378901).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 159.56218433380127
Epoch: 77, Steps: 88 | Train Loss: 0.0940392 Vali Loss: 0.3782930 Test Loss: 0.4462249
Validation loss decreased (0.378901 --> 0.378293).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 123.20451188087463
Epoch: 78, Steps: 88 | Train Loss: 0.0937528 Vali Loss: 0.3782427 Test Loss: 0.4459028
Validation loss decreased (0.378293 --> 0.378243).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 111.5865707397461
Epoch: 79, Steps: 88 | Train Loss: 0.0935051 Vali Loss: 0.3781672 Test Loss: 0.4455164
Validation loss decreased (0.378243 --> 0.378167).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 117.2449963092804
Epoch: 80, Steps: 88 | Train Loss: 0.0932439 Vali Loss: 0.3778391 Test Loss: 0.4452461
Validation loss decreased (0.378167 --> 0.377839).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 115.61658310890198
Epoch: 81, Steps: 88 | Train Loss: 0.0930124 Vali Loss: 0.3774842 Test Loss: 0.4449490
Validation loss decreased (0.377839 --> 0.377484).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 159.40393567085266
Epoch: 82, Steps: 88 | Train Loss: 0.0927811 Vali Loss: 0.3772264 Test Loss: 0.4446939
Validation loss decreased (0.377484 --> 0.377226).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 112.24119567871094
Epoch: 83, Steps: 88 | Train Loss: 0.0925588 Vali Loss: 0.3766189 Test Loss: 0.4444150
Validation loss decreased (0.377226 --> 0.376619).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 111.7929220199585
Epoch: 84, Steps: 88 | Train Loss: 0.0923706 Vali Loss: 0.3762070 Test Loss: 0.4441330
Validation loss decreased (0.376619 --> 0.376207).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 115.39715218544006
Epoch: 85, Steps: 88 | Train Loss: 0.0921611 Vali Loss: 0.3764445 Test Loss: 0.4438941
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 119.17998051643372
Epoch: 86, Steps: 88 | Train Loss: 0.0919788 Vali Loss: 0.3766291 Test Loss: 0.4436638
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 136.04272150993347
Epoch: 87, Steps: 88 | Train Loss: 0.0918188 Vali Loss: 0.3761052 Test Loss: 0.4434298
Validation loss decreased (0.376207 --> 0.376105).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 121.94159245491028
Epoch: 88, Steps: 88 | Train Loss: 0.0916432 Vali Loss: 0.3759243 Test Loss: 0.4432864
Validation loss decreased (0.376105 --> 0.375924).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 135.0534644126892
Epoch: 89, Steps: 88 | Train Loss: 0.0914753 Vali Loss: 0.3750743 Test Loss: 0.4430668
Validation loss decreased (0.375924 --> 0.375074).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 109.38853526115417
Epoch: 90, Steps: 88 | Train Loss: 0.0913419 Vali Loss: 0.3755524 Test Loss: 0.4428692
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 108.47036457061768
Epoch: 91, Steps: 88 | Train Loss: 0.0911842 Vali Loss: 0.3755088 Test Loss: 0.4427010
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 114.31773519515991
Epoch: 92, Steps: 88 | Train Loss: 0.0910840 Vali Loss: 0.3747475 Test Loss: 0.4424917
Validation loss decreased (0.375074 --> 0.374748).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 110.8187506198883
Epoch: 93, Steps: 88 | Train Loss: 0.0909484 Vali Loss: 0.3750800 Test Loss: 0.4423556
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 107.65895390510559
Epoch: 94, Steps: 88 | Train Loss: 0.0907997 Vali Loss: 0.3749493 Test Loss: 0.4422087
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 114.5454170703888
Epoch: 95, Steps: 88 | Train Loss: 0.0907075 Vali Loss: 0.3746282 Test Loss: 0.4420691
Validation loss decreased (0.374748 --> 0.374628).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 158.20820212364197
Epoch: 96, Steps: 88 | Train Loss: 0.0905907 Vali Loss: 0.3745424 Test Loss: 0.4419262
Validation loss decreased (0.374628 --> 0.374542).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 110.20520448684692
Epoch: 97, Steps: 88 | Train Loss: 0.0904914 Vali Loss: 0.3745744 Test Loss: 0.4417984
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 105.29543256759644
Epoch: 98, Steps: 88 | Train Loss: 0.0903789 Vali Loss: 0.3743709 Test Loss: 0.4416757
Validation loss decreased (0.374542 --> 0.374371).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 107.66895580291748
Epoch: 99, Steps: 88 | Train Loss: 0.0902883 Vali Loss: 0.3746568 Test Loss: 0.4415684
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 107.59708738327026
Epoch: 100, Steps: 88 | Train Loss: 0.0902091 Vali Loss: 0.3744232 Test Loss: 0.4414528
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14299545600.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 120.81738686561584
Epoch: 1, Steps: 88 | Train Loss: 0.2459669 Vali Loss: 0.3294126 Test Loss: 0.3999994
Validation loss decreased (inf --> 0.329413).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 116.7538378238678
Epoch: 2, Steps: 88 | Train Loss: 0.2380013 Vali Loss: 0.3286909 Test Loss: 0.3998073
Validation loss decreased (0.329413 --> 0.328691).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 109.29981231689453
Epoch: 3, Steps: 88 | Train Loss: 0.2378487 Vali Loss: 0.3283649 Test Loss: 0.3992727
Validation loss decreased (0.328691 --> 0.328365).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 112.38465332984924
Epoch: 4, Steps: 88 | Train Loss: 0.2377369 Vali Loss: 0.3281339 Test Loss: 0.3995427
Validation loss decreased (0.328365 --> 0.328134).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 144.7627718448639
Epoch: 5, Steps: 88 | Train Loss: 0.2376250 Vali Loss: 0.3275718 Test Loss: 0.3989401
Validation loss decreased (0.328134 --> 0.327572).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 121.89538359642029
Epoch: 6, Steps: 88 | Train Loss: 0.2374970 Vali Loss: 0.3270350 Test Loss: 0.3989881
Validation loss decreased (0.327572 --> 0.327035).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 123.04955005645752
Epoch: 7, Steps: 88 | Train Loss: 0.2374819 Vali Loss: 0.3283247 Test Loss: 0.3991734
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 138.37539052963257
Epoch: 8, Steps: 88 | Train Loss: 0.2374655 Vali Loss: 0.3279720 Test Loss: 0.3988495
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 91.78497338294983
Epoch: 9, Steps: 88 | Train Loss: 0.2373482 Vali Loss: 0.3274665 Test Loss: 0.3988431
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 87.70111417770386
Epoch: 10, Steps: 88 | Train Loss: 0.2372678 Vali Loss: 0.3271883 Test Loss: 0.3985641
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 84.86111855506897
Epoch: 11, Steps: 88 | Train Loss: 0.2373859 Vali Loss: 0.3280433 Test Loss: 0.3988935
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 87.30401945114136
Epoch: 12, Steps: 88 | Train Loss: 0.2372537 Vali Loss: 0.3273612 Test Loss: 0.3985728
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 90.7374107837677
Epoch: 13, Steps: 88 | Train Loss: 0.2372255 Vali Loss: 0.3270817 Test Loss: 0.3986429
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 82.23906135559082
Epoch: 14, Steps: 88 | Train Loss: 0.2371759 Vali Loss: 0.3277059 Test Loss: 0.3987146
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 125.69640469551086
Epoch: 15, Steps: 88 | Train Loss: 0.2371793 Vali Loss: 0.3271765 Test Loss: 0.3983968
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 125.54473853111267
Epoch: 16, Steps: 88 | Train Loss: 0.2371212 Vali Loss: 0.3276148 Test Loss: 0.3985958
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.3980342745780945, mae:0.2739647328853607, rse:0.5207020044326782, corr:[0.27446637 0.29039    0.29064992 0.2908756  0.29077312 0.29038337
 0.2900791  0.28971833 0.29022387 0.29030946 0.29067644 0.29065675
 0.29018837 0.28981453 0.28913626 0.28952378 0.28960165 0.28942358
 0.28927588 0.28885418 0.2888231  0.2884652  0.2884856  0.28894076
 0.290515   0.29078665 0.29046947 0.29057673 0.29055485 0.2902146
 0.28949004 0.28884044 0.2892544  0.28942588 0.2892989  0.28933364
 0.28915343 0.28915    0.2891926  0.28968304 0.29008305 0.29001024
 0.2896372  0.28862616 0.28823256 0.28831515 0.288125   0.28861496
 0.28888825 0.2887691  0.28916916 0.28953838 0.29017612 0.28996533
 0.2890008  0.28802192 0.28771317 0.28871304 0.2893597  0.28940895
 0.28925925 0.2884874  0.28810573 0.28807348 0.28786048 0.28795335
 0.2879907  0.2878489  0.28765982 0.2870849  0.28643483 0.28644896
 0.28704208 0.28773326 0.2884054  0.28834832 0.2883368  0.28847092
 0.2877561  0.28727284 0.28731298 0.2877107  0.28795043 0.28781638
 0.2877152  0.28757933 0.28802457 0.28799438 0.2873118  0.28736845
 0.28746346 0.28745088 0.28769398 0.28732833 0.28700042 0.28717983
 0.28662562 0.28650844 0.28703547 0.2872055  0.28671753 0.28658733
 0.28672156 0.28668824 0.2871657  0.28815335 0.28866357 0.2883007
 0.28812897 0.28781107 0.28747472 0.28771356 0.28709602 0.28668368
 0.2872282  0.2874511  0.2874622  0.28698504 0.2867954  0.28733775
 0.28745863 0.2876353  0.28775656 0.28781068 0.28759852 0.2871388
 0.28707108 0.28685972 0.28694504 0.28759846 0.28808218 0.28873503
 0.28876057 0.2880753  0.28767076 0.28766158 0.2875727  0.28730297
 0.2879258  0.2885087  0.28799334 0.2875358  0.2873732  0.28743866
 0.2878905  0.2878939  0.2883561  0.28955844 0.2898075  0.28930014
 0.28917822 0.288928   0.28800026 0.2876222  0.2876155  0.2878313
 0.288458   0.28825685 0.28834477 0.2886292  0.28884497 0.2892226
 0.28881645 0.2887068  0.2883432  0.28797257 0.28834465 0.28804436
 0.28946653 0.29016948 0.29008463 0.29071677 0.29065716 0.2904925
 0.29016846 0.2893632  0.28915608 0.2884123  0.28881115 0.28919294
 0.28873843 0.28897372 0.2889335  0.28925997 0.28942847 0.28951836
 0.28879097 0.2883315  0.28770107 0.28680357 0.2871881  0.2900642 ]
