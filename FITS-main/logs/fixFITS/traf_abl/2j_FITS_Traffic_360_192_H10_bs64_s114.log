Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=170, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j192_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j192_H10_FITS_custom_ftM_sl360_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11729
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=170, out_features=260, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4876851200.0
params:  44460.0
Trainable parameters:  44460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 71.42422294616699
Epoch: 1, Steps: 91 | Train Loss: 1.1223592 Vali Loss: 1.1653686 Test Loss: 1.3569020
Validation loss decreased (inf --> 1.165369).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 75.76186084747314
Epoch: 2, Steps: 91 | Train Loss: 0.8137342 Vali Loss: 1.0104465 Test Loss: 1.1759216
Validation loss decreased (1.165369 --> 1.010447).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 68.018385887146
Epoch: 3, Steps: 91 | Train Loss: 0.7044204 Vali Loss: 0.9418092 Test Loss: 1.0968556
Validation loss decreased (1.010447 --> 0.941809).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 75.74283981323242
Epoch: 4, Steps: 91 | Train Loss: 0.6340864 Vali Loss: 0.8840767 Test Loss: 1.0297801
Validation loss decreased (0.941809 --> 0.884077).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 77.58788466453552
Epoch: 5, Steps: 91 | Train Loss: 0.5774131 Vali Loss: 0.8371110 Test Loss: 0.9760141
Validation loss decreased (0.884077 --> 0.837111).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 71.95302486419678
Epoch: 6, Steps: 91 | Train Loss: 0.5295055 Vali Loss: 0.7944658 Test Loss: 0.9269292
Validation loss decreased (0.837111 --> 0.794466).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 70.5996253490448
Epoch: 7, Steps: 91 | Train Loss: 0.4882299 Vali Loss: 0.7571170 Test Loss: 0.8852673
Validation loss decreased (0.794466 --> 0.757117).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 69.62085461616516
Epoch: 8, Steps: 91 | Train Loss: 0.4523240 Vali Loss: 0.7225582 Test Loss: 0.8451856
Validation loss decreased (0.757117 --> 0.722558).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 71.29393196105957
Epoch: 9, Steps: 91 | Train Loss: 0.4209698 Vali Loss: 0.6925771 Test Loss: 0.8105176
Validation loss decreased (0.722558 --> 0.692577).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 72.52632713317871
Epoch: 10, Steps: 91 | Train Loss: 0.3933160 Vali Loss: 0.6647628 Test Loss: 0.7786737
Validation loss decreased (0.692577 --> 0.664763).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 71.51781749725342
Epoch: 11, Steps: 91 | Train Loss: 0.3688879 Vali Loss: 0.6389250 Test Loss: 0.7494330
Validation loss decreased (0.664763 --> 0.638925).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 70.62175559997559
Epoch: 12, Steps: 91 | Train Loss: 0.3471438 Vali Loss: 0.6176179 Test Loss: 0.7255964
Validation loss decreased (0.638925 --> 0.617618).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 85.87365674972534
Epoch: 13, Steps: 91 | Train Loss: 0.3277468 Vali Loss: 0.5997458 Test Loss: 0.7045634
Validation loss decreased (0.617618 --> 0.599746).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 101.07452988624573
Epoch: 14, Steps: 91 | Train Loss: 0.3104309 Vali Loss: 0.5819882 Test Loss: 0.6842317
Validation loss decreased (0.599746 --> 0.581988).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 71.86024284362793
Epoch: 15, Steps: 91 | Train Loss: 0.2948317 Vali Loss: 0.5643027 Test Loss: 0.6647319
Validation loss decreased (0.581988 --> 0.564303).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 70.1315369606018
Epoch: 16, Steps: 91 | Train Loss: 0.2807868 Vali Loss: 0.5509701 Test Loss: 0.6494639
Validation loss decreased (0.564303 --> 0.550970).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 73.519376039505
Epoch: 17, Steps: 91 | Train Loss: 0.2681283 Vali Loss: 0.5366725 Test Loss: 0.6338519
Validation loss decreased (0.550970 --> 0.536672).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 70.03468012809753
Epoch: 18, Steps: 91 | Train Loss: 0.2566559 Vali Loss: 0.5242301 Test Loss: 0.6195116
Validation loss decreased (0.536672 --> 0.524230).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 71.31811547279358
Epoch: 19, Steps: 91 | Train Loss: 0.2461937 Vali Loss: 0.5133146 Test Loss: 0.6069894
Validation loss decreased (0.524230 --> 0.513315).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 67.08633589744568
Epoch: 20, Steps: 91 | Train Loss: 0.2367117 Vali Loss: 0.5030457 Test Loss: 0.5952965
Validation loss decreased (0.513315 --> 0.503046).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 63.392191648483276
Epoch: 21, Steps: 91 | Train Loss: 0.2280739 Vali Loss: 0.4949920 Test Loss: 0.5863750
Validation loss decreased (0.503046 --> 0.494992).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 62.95017766952515
Epoch: 22, Steps: 91 | Train Loss: 0.2201186 Vali Loss: 0.4855771 Test Loss: 0.5756599
Validation loss decreased (0.494992 --> 0.485577).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 62.856837034225464
Epoch: 23, Steps: 91 | Train Loss: 0.2129114 Vali Loss: 0.4781306 Test Loss: 0.5672224
Validation loss decreased (0.485577 --> 0.478131).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 63.89100432395935
Epoch: 24, Steps: 91 | Train Loss: 0.2062607 Vali Loss: 0.4702264 Test Loss: 0.5584207
Validation loss decreased (0.478131 --> 0.470226).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 60.5899932384491
Epoch: 25, Steps: 91 | Train Loss: 0.2001385 Vali Loss: 0.4640730 Test Loss: 0.5514041
Validation loss decreased (0.470226 --> 0.464073).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 63.917953968048096
Epoch: 26, Steps: 91 | Train Loss: 0.1945627 Vali Loss: 0.4577864 Test Loss: 0.5444872
Validation loss decreased (0.464073 --> 0.457786).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 67.85141563415527
Epoch: 27, Steps: 91 | Train Loss: 0.1893575 Vali Loss: 0.4517837 Test Loss: 0.5379831
Validation loss decreased (0.457786 --> 0.451784).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 60.83602237701416
Epoch: 28, Steps: 91 | Train Loss: 0.1845671 Vali Loss: 0.4465566 Test Loss: 0.5319466
Validation loss decreased (0.451784 --> 0.446557).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 67.76501154899597
Epoch: 29, Steps: 91 | Train Loss: 0.1801335 Vali Loss: 0.4419077 Test Loss: 0.5266421
Validation loss decreased (0.446557 --> 0.441908).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 71.78511333465576
Epoch: 30, Steps: 91 | Train Loss: 0.1760130 Vali Loss: 0.4377573 Test Loss: 0.5221652
Validation loss decreased (0.441908 --> 0.437757).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 63.62059259414673
Epoch: 31, Steps: 91 | Train Loss: 0.1722360 Vali Loss: 0.4327663 Test Loss: 0.5169536
Validation loss decreased (0.437757 --> 0.432766).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 84.66590428352356
Epoch: 32, Steps: 91 | Train Loss: 0.1687320 Vali Loss: 0.4294246 Test Loss: 0.5132310
Validation loss decreased (0.432766 --> 0.429425).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 81.04533267021179
Epoch: 33, Steps: 91 | Train Loss: 0.1654133 Vali Loss: 0.4254880 Test Loss: 0.5090765
Validation loss decreased (0.429425 --> 0.425488).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 79.9096028804779
Epoch: 34, Steps: 91 | Train Loss: 0.1623873 Vali Loss: 0.4225166 Test Loss: 0.5051739
Validation loss decreased (0.425488 --> 0.422517).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 87.79023861885071
Epoch: 35, Steps: 91 | Train Loss: 0.1595308 Vali Loss: 0.4188377 Test Loss: 0.5016936
Validation loss decreased (0.422517 --> 0.418838).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 89.24659252166748
Epoch: 36, Steps: 91 | Train Loss: 0.1568607 Vali Loss: 0.4166758 Test Loss: 0.4987970
Validation loss decreased (0.418838 --> 0.416676).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 90.75935220718384
Epoch: 37, Steps: 91 | Train Loss: 0.1544111 Vali Loss: 0.4134306 Test Loss: 0.4957909
Validation loss decreased (0.416676 --> 0.413431).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 80.12558770179749
Epoch: 38, Steps: 91 | Train Loss: 0.1520853 Vali Loss: 0.4113961 Test Loss: 0.4927773
Validation loss decreased (0.413431 --> 0.411396).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 92.95918893814087
Epoch: 39, Steps: 91 | Train Loss: 0.1498929 Vali Loss: 0.4089189 Test Loss: 0.4903097
Validation loss decreased (0.411396 --> 0.408919).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 83.29670453071594
Epoch: 40, Steps: 91 | Train Loss: 0.1478976 Vali Loss: 0.4064732 Test Loss: 0.4879953
Validation loss decreased (0.408919 --> 0.406473).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 88.26977276802063
Epoch: 41, Steps: 91 | Train Loss: 0.1459987 Vali Loss: 0.4046358 Test Loss: 0.4855738
Validation loss decreased (0.406473 --> 0.404636).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 93.51162219047546
Epoch: 42, Steps: 91 | Train Loss: 0.1442103 Vali Loss: 0.4022878 Test Loss: 0.4834011
Validation loss decreased (0.404636 --> 0.402288).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 88.67180228233337
Epoch: 43, Steps: 91 | Train Loss: 0.1425253 Vali Loss: 0.4004818 Test Loss: 0.4813120
Validation loss decreased (0.402288 --> 0.400482).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 167.01711702346802
Epoch: 44, Steps: 91 | Train Loss: 0.1409904 Vali Loss: 0.3984660 Test Loss: 0.4796060
Validation loss decreased (0.400482 --> 0.398466).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 164.94951176643372
Epoch: 45, Steps: 91 | Train Loss: 0.1395288 Vali Loss: 0.3975919 Test Loss: 0.4778643
Validation loss decreased (0.398466 --> 0.397592).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 98.12990140914917
Epoch: 46, Steps: 91 | Train Loss: 0.1381123 Vali Loss: 0.3954854 Test Loss: 0.4761594
Validation loss decreased (0.397592 --> 0.395485).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 85.21880745887756
Epoch: 47, Steps: 91 | Train Loss: 0.1368170 Vali Loss: 0.3938552 Test Loss: 0.4746050
Validation loss decreased (0.395485 --> 0.393855).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 88.43950295448303
Epoch: 48, Steps: 91 | Train Loss: 0.1355965 Vali Loss: 0.3924179 Test Loss: 0.4731552
Validation loss decreased (0.393855 --> 0.392418).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 93.75232815742493
Epoch: 49, Steps: 91 | Train Loss: 0.1344466 Vali Loss: 0.3914686 Test Loss: 0.4718050
Validation loss decreased (0.392418 --> 0.391469).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 86.92700624465942
Epoch: 50, Steps: 91 | Train Loss: 0.1333462 Vali Loss: 0.3898696 Test Loss: 0.4705027
Validation loss decreased (0.391469 --> 0.389870).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 89.77287292480469
Epoch: 51, Steps: 91 | Train Loss: 0.1323199 Vali Loss: 0.3895513 Test Loss: 0.4692649
Validation loss decreased (0.389870 --> 0.389551).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 81.77425241470337
Epoch: 52, Steps: 91 | Train Loss: 0.1313668 Vali Loss: 0.3887449 Test Loss: 0.4680871
Validation loss decreased (0.389551 --> 0.388745).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 105.52859711647034
Epoch: 53, Steps: 91 | Train Loss: 0.1304146 Vali Loss: 0.3875501 Test Loss: 0.4670008
Validation loss decreased (0.388745 --> 0.387550).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 74.30995917320251
Epoch: 54, Steps: 91 | Train Loss: 0.1295745 Vali Loss: 0.3863982 Test Loss: 0.4660301
Validation loss decreased (0.387550 --> 0.386398).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 76.89339780807495
Epoch: 55, Steps: 91 | Train Loss: 0.1287263 Vali Loss: 0.3856727 Test Loss: 0.4650417
Validation loss decreased (0.386398 --> 0.385673).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 86.12018990516663
Epoch: 56, Steps: 91 | Train Loss: 0.1279504 Vali Loss: 0.3840375 Test Loss: 0.4640306
Validation loss decreased (0.385673 --> 0.384037).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 105.18637275695801
Epoch: 57, Steps: 91 | Train Loss: 0.1272603 Vali Loss: 0.3834278 Test Loss: 0.4632089
Validation loss decreased (0.384037 --> 0.383428).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 88.70704984664917
Epoch: 58, Steps: 91 | Train Loss: 0.1265587 Vali Loss: 0.3831103 Test Loss: 0.4624046
Validation loss decreased (0.383428 --> 0.383110).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 94.43287920951843
Epoch: 59, Steps: 91 | Train Loss: 0.1258810 Vali Loss: 0.3825651 Test Loss: 0.4616317
Validation loss decreased (0.383110 --> 0.382565).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 146.09921836853027
Epoch: 60, Steps: 91 | Train Loss: 0.1252620 Vali Loss: 0.3813125 Test Loss: 0.4609216
Validation loss decreased (0.382565 --> 0.381313).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 104.41053175926208
Epoch: 61, Steps: 91 | Train Loss: 0.1246586 Vali Loss: 0.3807302 Test Loss: 0.4602969
Validation loss decreased (0.381313 --> 0.380730).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 108.26534748077393
Epoch: 62, Steps: 91 | Train Loss: 0.1241265 Vali Loss: 0.3803129 Test Loss: 0.4595937
Validation loss decreased (0.380730 --> 0.380313).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 140.5591380596161
Epoch: 63, Steps: 91 | Train Loss: 0.1235770 Vali Loss: 0.3794124 Test Loss: 0.4589799
Validation loss decreased (0.380313 --> 0.379412).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 102.34344220161438
Epoch: 64, Steps: 91 | Train Loss: 0.1231169 Vali Loss: 0.3788104 Test Loss: 0.4584145
Validation loss decreased (0.379412 --> 0.378810).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 146.43708729743958
Epoch: 65, Steps: 91 | Train Loss: 0.1225957 Vali Loss: 0.3785676 Test Loss: 0.4578737
Validation loss decreased (0.378810 --> 0.378568).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 146.71272540092468
Epoch: 66, Steps: 91 | Train Loss: 0.1221976 Vali Loss: 0.3782197 Test Loss: 0.4573003
Validation loss decreased (0.378568 --> 0.378220).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 159.52267360687256
Epoch: 67, Steps: 91 | Train Loss: 0.1217267 Vali Loss: 0.3775274 Test Loss: 0.4568408
Validation loss decreased (0.378220 --> 0.377527).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 146.3925404548645
Epoch: 68, Steps: 91 | Train Loss: 0.1213428 Vali Loss: 0.3768879 Test Loss: 0.4564118
Validation loss decreased (0.377527 --> 0.376888).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 132.5300965309143
Epoch: 69, Steps: 91 | Train Loss: 0.1209730 Vali Loss: 0.3762836 Test Loss: 0.4559202
Validation loss decreased (0.376888 --> 0.376284).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 149.47900676727295
Epoch: 70, Steps: 91 | Train Loss: 0.1206085 Vali Loss: 0.3761924 Test Loss: 0.4555178
Validation loss decreased (0.376284 --> 0.376192).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 144.08136701583862
Epoch: 71, Steps: 91 | Train Loss: 0.1202384 Vali Loss: 0.3761133 Test Loss: 0.4551198
Validation loss decreased (0.376192 --> 0.376113).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 143.63999271392822
Epoch: 72, Steps: 91 | Train Loss: 0.1199250 Vali Loss: 0.3754056 Test Loss: 0.4547404
Validation loss decreased (0.376113 --> 0.375406).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 136.2834074497223
Epoch: 73, Steps: 91 | Train Loss: 0.1196050 Vali Loss: 0.3753916 Test Loss: 0.4543613
Validation loss decreased (0.375406 --> 0.375392).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 141.82846689224243
Epoch: 74, Steps: 91 | Train Loss: 0.1193171 Vali Loss: 0.3745681 Test Loss: 0.4540419
Validation loss decreased (0.375392 --> 0.374568).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 141.94967579841614
Epoch: 75, Steps: 91 | Train Loss: 0.1190361 Vali Loss: 0.3746423 Test Loss: 0.4537279
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 144.4919674396515
Epoch: 76, Steps: 91 | Train Loss: 0.1187875 Vali Loss: 0.3745184 Test Loss: 0.4534148
Validation loss decreased (0.374568 --> 0.374518).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 146.04486393928528
Epoch: 77, Steps: 91 | Train Loss: 0.1185378 Vali Loss: 0.3736677 Test Loss: 0.4531068
Validation loss decreased (0.374518 --> 0.373668).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 146.7564558982849
Epoch: 78, Steps: 91 | Train Loss: 0.1182918 Vali Loss: 0.3743485 Test Loss: 0.4528519
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 145.50870418548584
Epoch: 79, Steps: 91 | Train Loss: 0.1180316 Vali Loss: 0.3736216 Test Loss: 0.4525850
Validation loss decreased (0.373668 --> 0.373622).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 152.59723901748657
Epoch: 80, Steps: 91 | Train Loss: 0.1178390 Vali Loss: 0.3731874 Test Loss: 0.4523418
Validation loss decreased (0.373622 --> 0.373187).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 144.70141696929932
Epoch: 81, Steps: 91 | Train Loss: 0.1176236 Vali Loss: 0.3727741 Test Loss: 0.4521140
Validation loss decreased (0.373187 --> 0.372774).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 142.1256618499756
Epoch: 82, Steps: 91 | Train Loss: 0.1174383 Vali Loss: 0.3729852 Test Loss: 0.4518670
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 132.91770005226135
Epoch: 83, Steps: 91 | Train Loss: 0.1172447 Vali Loss: 0.3729347 Test Loss: 0.4516584
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 136.95049262046814
Epoch: 84, Steps: 91 | Train Loss: 0.1170614 Vali Loss: 0.3727889 Test Loss: 0.4514642
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 141.6755657196045
Epoch: 85, Steps: 91 | Train Loss: 0.1169085 Vali Loss: 0.3722520 Test Loss: 0.4512676
Validation loss decreased (0.372774 --> 0.372252).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 136.559401512146
Epoch: 86, Steps: 91 | Train Loss: 0.1167415 Vali Loss: 0.3722097 Test Loss: 0.4510852
Validation loss decreased (0.372252 --> 0.372210).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 122.8614764213562
Epoch: 87, Steps: 91 | Train Loss: 0.1165923 Vali Loss: 0.3721086 Test Loss: 0.4509114
Validation loss decreased (0.372210 --> 0.372109).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 124.24654603004456
Epoch: 88, Steps: 91 | Train Loss: 0.1164498 Vali Loss: 0.3714765 Test Loss: 0.4507351
Validation loss decreased (0.372109 --> 0.371477).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 124.29131531715393
Epoch: 89, Steps: 91 | Train Loss: 0.1163343 Vali Loss: 0.3718533 Test Loss: 0.4505877
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 160.04864120483398
Epoch: 90, Steps: 91 | Train Loss: 0.1161915 Vali Loss: 0.3715067 Test Loss: 0.4504299
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 161.12597680091858
Epoch: 91, Steps: 91 | Train Loss: 0.1160708 Vali Loss: 0.3716127 Test Loss: 0.4502937
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 166.35844707489014
Epoch: 92, Steps: 91 | Train Loss: 0.1159240 Vali Loss: 0.3708728 Test Loss: 0.4501554
Validation loss decreased (0.371477 --> 0.370873).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 155.3366184234619
Epoch: 93, Steps: 91 | Train Loss: 0.1158699 Vali Loss: 0.3707260 Test Loss: 0.4500362
Validation loss decreased (0.370873 --> 0.370726).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 133.84242153167725
Epoch: 94, Steps: 91 | Train Loss: 0.1157466 Vali Loss: 0.3711677 Test Loss: 0.4499270
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 132.61562299728394
Epoch: 95, Steps: 91 | Train Loss: 0.1155943 Vali Loss: 0.3709333 Test Loss: 0.4498064
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 142.361572265625
Epoch: 96, Steps: 91 | Train Loss: 0.1155078 Vali Loss: 0.3710999 Test Loss: 0.4497018
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 165.12351512908936
Epoch: 97, Steps: 91 | Train Loss: 0.1154342 Vali Loss: 0.3705113 Test Loss: 0.4496027
Validation loss decreased (0.370726 --> 0.370511).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 159.74441742897034
Epoch: 98, Steps: 91 | Train Loss: 0.1153506 Vali Loss: 0.3705006 Test Loss: 0.4495018
Validation loss decreased (0.370511 --> 0.370501).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 147.23137187957764
Epoch: 99, Steps: 91 | Train Loss: 0.1152816 Vali Loss: 0.3704337 Test Loss: 0.4493998
Validation loss decreased (0.370501 --> 0.370434).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 147.1090292930603
Epoch: 100, Steps: 91 | Train Loss: 0.1152212 Vali Loss: 0.3702030 Test Loss: 0.4493127
Validation loss decreased (0.370434 --> 0.370203).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 11729
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=170, out_features=260, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4876851200.0
params:  44460.0
Trainable parameters:  44460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 152.98298525810242
Epoch: 1, Steps: 91 | Train Loss: 0.2597456 Vali Loss: 0.3409804 Test Loss: 0.4231977
Validation loss decreased (inf --> 0.340980).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 252.84793829917908
Epoch: 2, Steps: 91 | Train Loss: 0.2529246 Vali Loss: 0.3396654 Test Loss: 0.4234180
Validation loss decreased (0.340980 --> 0.339665).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 110.30928039550781
Epoch: 3, Steps: 91 | Train Loss: 0.2527135 Vali Loss: 0.3395853 Test Loss: 0.4230154
Validation loss decreased (0.339665 --> 0.339585).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 111.85963368415833
Epoch: 4, Steps: 91 | Train Loss: 0.2527091 Vali Loss: 0.3394899 Test Loss: 0.4231389
Validation loss decreased (0.339585 --> 0.339490).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 166.53866362571716
Epoch: 5, Steps: 91 | Train Loss: 0.2526055 Vali Loss: 0.3395567 Test Loss: 0.4229438
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 90.90597629547119
Epoch: 6, Steps: 91 | Train Loss: 0.2525636 Vali Loss: 0.3397795 Test Loss: 0.4230654
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 108.14611482620239
Epoch: 7, Steps: 91 | Train Loss: 0.2525105 Vali Loss: 0.3397078 Test Loss: 0.4230431
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 138.017231464386
Epoch: 8, Steps: 91 | Train Loss: 0.2524639 Vali Loss: 0.3394131 Test Loss: 0.4228930
Validation loss decreased (0.339490 --> 0.339413).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 144.50925064086914
Epoch: 9, Steps: 91 | Train Loss: 0.2524390 Vali Loss: 0.3398063 Test Loss: 0.4227743
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 138.00941157341003
Epoch: 10, Steps: 91 | Train Loss: 0.2523859 Vali Loss: 0.3395923 Test Loss: 0.4229051
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 123.4903359413147
Epoch: 11, Steps: 91 | Train Loss: 0.2523645 Vali Loss: 0.3395692 Test Loss: 0.4228948
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 126.28273367881775
Epoch: 12, Steps: 91 | Train Loss: 0.2523508 Vali Loss: 0.3395164 Test Loss: 0.4226966
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 122.0221517086029
Epoch: 13, Steps: 91 | Train Loss: 0.2522821 Vali Loss: 0.3389712 Test Loss: 0.4227050
Validation loss decreased (0.339413 --> 0.338971).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 129.7365219593048
Epoch: 14, Steps: 91 | Train Loss: 0.2522431 Vali Loss: 0.3393561 Test Loss: 0.4228891
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 127.29038739204407
Epoch: 15, Steps: 91 | Train Loss: 0.2521360 Vali Loss: 0.3395728 Test Loss: 0.4226689
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 141.71098113059998
Epoch: 16, Steps: 91 | Train Loss: 0.2522828 Vali Loss: 0.3395853 Test Loss: 0.4228373
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 118.25888705253601
Epoch: 17, Steps: 91 | Train Loss: 0.2522800 Vali Loss: 0.3393327 Test Loss: 0.4225637
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 113.69153809547424
Epoch: 18, Steps: 91 | Train Loss: 0.2521717 Vali Loss: 0.3393509 Test Loss: 0.4225999
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 109.33041906356812
Epoch: 19, Steps: 91 | Train Loss: 0.2522998 Vali Loss: 0.3393586 Test Loss: 0.4226351
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 106.2890419960022
Epoch: 20, Steps: 91 | Train Loss: 0.2521874 Vali Loss: 0.3394488 Test Loss: 0.4225949
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 165.56507301330566
Epoch: 21, Steps: 91 | Train Loss: 0.2521438 Vali Loss: 0.3400728 Test Loss: 0.4224596
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 142.53156042099
Epoch: 22, Steps: 91 | Train Loss: 0.2522683 Vali Loss: 0.3395287 Test Loss: 0.4225477
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 121.30192422866821
Epoch: 23, Steps: 91 | Train Loss: 0.2521110 Vali Loss: 0.3394431 Test Loss: 0.4224229
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j192_H10_FITS_custom_ftM_sl360_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4216774106025696, mae:0.2834414839744568, rse:0.5359437465667725, corr:[0.27225977 0.28897402 0.28893912 0.28880784 0.2887347  0.28903666
 0.28887007 0.28861824 0.2883472  0.2884394  0.28872788 0.28863183
 0.2880014  0.28764075 0.28759655 0.2874498  0.28794268 0.28815064
 0.28818217 0.2882764  0.2879359  0.28765553 0.28761625 0.28789365
 0.2890829  0.28928572 0.2885733  0.28860518 0.2885907  0.28842375
 0.28894505 0.28862324 0.28857133 0.2882157  0.28810015 0.2885413
 0.28798562 0.28738675 0.28749782 0.2879922  0.28789213 0.2877572
 0.28800535 0.28813896 0.2878893  0.28792495 0.28815117 0.2878166
 0.28798366 0.28766903 0.28744778 0.28764814 0.28804758 0.28831825
 0.28826517 0.2881099  0.28763762 0.28757453 0.28764778 0.28757668
 0.28768197 0.287646   0.28766364 0.28787047 0.28812888 0.28744724
 0.2869595  0.2872567  0.2870522  0.28718194 0.2871037  0.2867766
 0.28664654 0.2863637  0.2864024  0.28646535 0.286759   0.2872266
 0.28722012 0.2866025  0.28665403 0.28699076 0.2867874  0.2868969
 0.28660107 0.28659287 0.28689793 0.286956   0.28730145 0.28689668
 0.2864153  0.28642145 0.28615004 0.28627235 0.2863882  0.28632894
 0.2860952  0.28584453 0.28564462 0.2855115  0.28582987 0.2862262
 0.28616044 0.28593656 0.28574494 0.28550938 0.28592885 0.28614333
 0.28628203 0.28674337 0.28649637 0.28623834 0.28616354 0.28562707
 0.2856403  0.28599694 0.2855365  0.28549027 0.28591216 0.2860623
 0.28585225 0.28556797 0.2854509  0.28554422 0.28527552 0.2851597
 0.28493392 0.28506103 0.28533396 0.28507018 0.28546065 0.2856307
 0.28577545 0.2858385  0.28555068 0.28559247 0.28590828 0.28605255
 0.28572726 0.28584117 0.28608948 0.28600317 0.28624627 0.2864795
 0.2866219  0.2870208  0.28710514 0.28700295 0.2872379  0.28745174
 0.28684315 0.28691432 0.28724533 0.2870657  0.28726584 0.2875159
 0.28744036 0.28720233 0.28714246 0.2870958  0.2871475  0.28700736
 0.28716865 0.28734922 0.28722477 0.28766295 0.28757897 0.28785923
 0.28914374 0.28859833 0.2884107  0.2883906  0.2883244  0.2881898
 0.28817722 0.2879105  0.2874384  0.28726205 0.28700933 0.2877428
 0.28771755 0.28785276 0.28776577 0.2880194  0.28813174 0.28802633
 0.28835952 0.28742608 0.2871747  0.28657243 0.28674558 0.28782186]
