Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j336_H8_FITS_custom_ftM_sl720_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10760408064.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 257.33978152275085
Epoch: 1, Steps: 87 | Train Loss: 1.0937679 Vali Loss: 1.1489040 Test Loss: 1.3359382
Validation loss decreased (inf --> 1.148904).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 137.0409333705902
Epoch: 2, Steps: 87 | Train Loss: 0.8118962 Vali Loss: 1.0261689 Test Loss: 1.1910701
Validation loss decreased (1.148904 --> 1.026169).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 127.06473445892334
Epoch: 3, Steps: 87 | Train Loss: 0.7132353 Vali Loss: 0.9597570 Test Loss: 1.1136006
Validation loss decreased (1.026169 --> 0.959757).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 118.95266842842102
Epoch: 4, Steps: 87 | Train Loss: 0.6449172 Vali Loss: 0.9036156 Test Loss: 1.0481362
Validation loss decreased (0.959757 --> 0.903616).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 120.0337781906128
Epoch: 5, Steps: 87 | Train Loss: 0.5893841 Vali Loss: 0.8577189 Test Loss: 0.9955137
Validation loss decreased (0.903616 --> 0.857719).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 120.67730331420898
Epoch: 6, Steps: 87 | Train Loss: 0.5424081 Vali Loss: 0.8158394 Test Loss: 0.9468255
Validation loss decreased (0.857719 --> 0.815839).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 119.49406003952026
Epoch: 7, Steps: 87 | Train Loss: 0.5020812 Vali Loss: 0.7808998 Test Loss: 0.9065427
Validation loss decreased (0.815839 --> 0.780900).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 116.80126881599426
Epoch: 8, Steps: 87 | Train Loss: 0.4669319 Vali Loss: 0.7457020 Test Loss: 0.8660477
Validation loss decreased (0.780900 --> 0.745702).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 128.3962960243225
Epoch: 9, Steps: 87 | Train Loss: 0.4362254 Vali Loss: 0.7180399 Test Loss: 0.8341280
Validation loss decreased (0.745702 --> 0.718040).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 134.67717051506042
Epoch: 10, Steps: 87 | Train Loss: 0.4091101 Vali Loss: 0.6909316 Test Loss: 0.8037017
Validation loss decreased (0.718040 --> 0.690932).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 140.8325982093811
Epoch: 11, Steps: 87 | Train Loss: 0.3850519 Vali Loss: 0.6683168 Test Loss: 0.7771481
Validation loss decreased (0.690932 --> 0.668317).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 133.59103775024414
Epoch: 12, Steps: 87 | Train Loss: 0.3636878 Vali Loss: 0.6469139 Test Loss: 0.7521820
Validation loss decreased (0.668317 --> 0.646914).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 125.15647077560425
Epoch: 13, Steps: 87 | Train Loss: 0.3445275 Vali Loss: 0.6268303 Test Loss: 0.7292894
Validation loss decreased (0.646914 --> 0.626830).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 134.04943990707397
Epoch: 14, Steps: 87 | Train Loss: 0.3273402 Vali Loss: 0.6086557 Test Loss: 0.7086593
Validation loss decreased (0.626830 --> 0.608656).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 121.11016631126404
Epoch: 15, Steps: 87 | Train Loss: 0.3118168 Vali Loss: 0.5913189 Test Loss: 0.6890016
Validation loss decreased (0.608656 --> 0.591319).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 119.77334475517273
Epoch: 16, Steps: 87 | Train Loss: 0.2978310 Vali Loss: 0.5768915 Test Loss: 0.6723169
Validation loss decreased (0.591319 --> 0.576892).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 121.05616569519043
Epoch: 17, Steps: 87 | Train Loss: 0.2851397 Vali Loss: 0.5637906 Test Loss: 0.6571448
Validation loss decreased (0.576892 --> 0.563791).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 112.99843096733093
Epoch: 18, Steps: 87 | Train Loss: 0.2736057 Vali Loss: 0.5527073 Test Loss: 0.6444489
Validation loss decreased (0.563791 --> 0.552707).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 123.68540668487549
Epoch: 19, Steps: 87 | Train Loss: 0.2630507 Vali Loss: 0.5401339 Test Loss: 0.6303478
Validation loss decreased (0.552707 --> 0.540134).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 179.8820390701294
Epoch: 20, Steps: 87 | Train Loss: 0.2534570 Vali Loss: 0.5297998 Test Loss: 0.6184283
Validation loss decreased (0.540134 --> 0.529800).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 120.94537472724915
Epoch: 21, Steps: 87 | Train Loss: 0.2446670 Vali Loss: 0.5203394 Test Loss: 0.6070487
Validation loss decreased (0.529800 --> 0.520339).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 117.4058165550232
Epoch: 22, Steps: 87 | Train Loss: 0.2366076 Vali Loss: 0.5113087 Test Loss: 0.5971844
Validation loss decreased (0.520339 --> 0.511309).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 153.03360056877136
Epoch: 23, Steps: 87 | Train Loss: 0.2292009 Vali Loss: 0.5035145 Test Loss: 0.5885404
Validation loss decreased (0.511309 --> 0.503515).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 150.78269290924072
Epoch: 24, Steps: 87 | Train Loss: 0.2223983 Vali Loss: 0.4966120 Test Loss: 0.5805670
Validation loss decreased (0.503515 --> 0.496612).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 128.53446221351624
Epoch: 25, Steps: 87 | Train Loss: 0.2161194 Vali Loss: 0.4895359 Test Loss: 0.5727841
Validation loss decreased (0.496612 --> 0.489536).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 130.1557161808014
Epoch: 26, Steps: 87 | Train Loss: 0.2103253 Vali Loss: 0.4836064 Test Loss: 0.5654352
Validation loss decreased (0.489536 --> 0.483606).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 127.68571758270264
Epoch: 27, Steps: 87 | Train Loss: 0.2049371 Vali Loss: 0.4770461 Test Loss: 0.5585445
Validation loss decreased (0.483606 --> 0.477046).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 125.13757014274597
Epoch: 28, Steps: 87 | Train Loss: 0.1999980 Vali Loss: 0.4712204 Test Loss: 0.5518083
Validation loss decreased (0.477046 --> 0.471220).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 132.27442526817322
Epoch: 29, Steps: 87 | Train Loss: 0.1953570 Vali Loss: 0.4667906 Test Loss: 0.5464891
Validation loss decreased (0.471220 --> 0.466791).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 164.0431113243103
Epoch: 30, Steps: 87 | Train Loss: 0.1910675 Vali Loss: 0.4618094 Test Loss: 0.5408961
Validation loss decreased (0.466791 --> 0.461809).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 137.7375991344452
Epoch: 31, Steps: 87 | Train Loss: 0.1870945 Vali Loss: 0.4574331 Test Loss: 0.5358751
Validation loss decreased (0.461809 --> 0.457433).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 129.60905838012695
Epoch: 32, Steps: 87 | Train Loss: 0.1833756 Vali Loss: 0.4532016 Test Loss: 0.5317498
Validation loss decreased (0.457433 --> 0.453202).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 149.75249981880188
Epoch: 33, Steps: 87 | Train Loss: 0.1799500 Vali Loss: 0.4487478 Test Loss: 0.5266265
Validation loss decreased (0.453202 --> 0.448748).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 174.8200294971466
Epoch: 34, Steps: 87 | Train Loss: 0.1767039 Vali Loss: 0.4457003 Test Loss: 0.5228974
Validation loss decreased (0.448748 --> 0.445700).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 178.39771389961243
Epoch: 35, Steps: 87 | Train Loss: 0.1736767 Vali Loss: 0.4419861 Test Loss: 0.5190364
Validation loss decreased (0.445700 --> 0.441986).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 152.62567234039307
Epoch: 36, Steps: 87 | Train Loss: 0.1708883 Vali Loss: 0.4398383 Test Loss: 0.5158666
Validation loss decreased (0.441986 --> 0.439838).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 147.88859009742737
Epoch: 37, Steps: 87 | Train Loss: 0.1682269 Vali Loss: 0.4361502 Test Loss: 0.5121467
Validation loss decreased (0.439838 --> 0.436150).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 149.11719179153442
Epoch: 38, Steps: 87 | Train Loss: 0.1657436 Vali Loss: 0.4334746 Test Loss: 0.5087720
Validation loss decreased (0.436150 --> 0.433475).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 155.45809483528137
Epoch: 39, Steps: 87 | Train Loss: 0.1634604 Vali Loss: 0.4312117 Test Loss: 0.5062615
Validation loss decreased (0.433475 --> 0.431212).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 134.81605434417725
Epoch: 40, Steps: 87 | Train Loss: 0.1612542 Vali Loss: 0.4281482 Test Loss: 0.5033399
Validation loss decreased (0.431212 --> 0.428148).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 144.11292386054993
Epoch: 41, Steps: 87 | Train Loss: 0.1592037 Vali Loss: 0.4257253 Test Loss: 0.5005502
Validation loss decreased (0.428148 --> 0.425725).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 140.7532947063446
Epoch: 42, Steps: 87 | Train Loss: 0.1573112 Vali Loss: 0.4240029 Test Loss: 0.4984331
Validation loss decreased (0.425725 --> 0.424003).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 140.77929139137268
Epoch: 43, Steps: 87 | Train Loss: 0.1554971 Vali Loss: 0.4219110 Test Loss: 0.4958667
Validation loss decreased (0.424003 --> 0.421911).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 141.63563776016235
Epoch: 44, Steps: 87 | Train Loss: 0.1537863 Vali Loss: 0.4191830 Test Loss: 0.4937185
Validation loss decreased (0.421911 --> 0.419183).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 144.56047987937927
Epoch: 45, Steps: 87 | Train Loss: 0.1522182 Vali Loss: 0.4178361 Test Loss: 0.4919841
Validation loss decreased (0.419183 --> 0.417836).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 138.49052715301514
Epoch: 46, Steps: 87 | Train Loss: 0.1506856 Vali Loss: 0.4161010 Test Loss: 0.4896538
Validation loss decreased (0.417836 --> 0.416101).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 140.59357047080994
Epoch: 47, Steps: 87 | Train Loss: 0.1492387 Vali Loss: 0.4145167 Test Loss: 0.4880508
Validation loss decreased (0.416101 --> 0.414517).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 143.98929595947266
Epoch: 48, Steps: 87 | Train Loss: 0.1479342 Vali Loss: 0.4126447 Test Loss: 0.4862498
Validation loss decreased (0.414517 --> 0.412645).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 141.42740607261658
Epoch: 49, Steps: 87 | Train Loss: 0.1466405 Vali Loss: 0.4117364 Test Loss: 0.4847869
Validation loss decreased (0.412645 --> 0.411736).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 141.56624341011047
Epoch: 50, Steps: 87 | Train Loss: 0.1454186 Vali Loss: 0.4097777 Test Loss: 0.4831862
Validation loss decreased (0.411736 --> 0.409778).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 168.02213549613953
Epoch: 51, Steps: 87 | Train Loss: 0.1443343 Vali Loss: 0.4088556 Test Loss: 0.4818178
Validation loss decreased (0.409778 --> 0.408856).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 209.45247435569763
Epoch: 52, Steps: 87 | Train Loss: 0.1432623 Vali Loss: 0.4077046 Test Loss: 0.4804845
Validation loss decreased (0.408856 --> 0.407705).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 158.76005148887634
Epoch: 53, Steps: 87 | Train Loss: 0.1422305 Vali Loss: 0.4058848 Test Loss: 0.4791700
Validation loss decreased (0.407705 --> 0.405885).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 144.1227068901062
Epoch: 54, Steps: 87 | Train Loss: 0.1412873 Vali Loss: 0.4052714 Test Loss: 0.4779846
Validation loss decreased (0.405885 --> 0.405271).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 140.81142282485962
Epoch: 55, Steps: 87 | Train Loss: 0.1403648 Vali Loss: 0.4044192 Test Loss: 0.4767621
Validation loss decreased (0.405271 --> 0.404419).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 122.55776619911194
Epoch: 56, Steps: 87 | Train Loss: 0.1394973 Vali Loss: 0.4033285 Test Loss: 0.4756127
Validation loss decreased (0.404419 --> 0.403329).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 129.7100064754486
Epoch: 57, Steps: 87 | Train Loss: 0.1386788 Vali Loss: 0.4022942 Test Loss: 0.4747513
Validation loss decreased (0.403329 --> 0.402294).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 124.9828610420227
Epoch: 58, Steps: 87 | Train Loss: 0.1379156 Vali Loss: 0.4016430 Test Loss: 0.4737413
Validation loss decreased (0.402294 --> 0.401643).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 129.35581374168396
Epoch: 59, Steps: 87 | Train Loss: 0.1372126 Vali Loss: 0.4008085 Test Loss: 0.4727692
Validation loss decreased (0.401643 --> 0.400809).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 120.88103556632996
Epoch: 60, Steps: 87 | Train Loss: 0.1365070 Vali Loss: 0.3997390 Test Loss: 0.4719126
Validation loss decreased (0.400809 --> 0.399739).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 125.1236560344696
Epoch: 61, Steps: 87 | Train Loss: 0.1358524 Vali Loss: 0.3989997 Test Loss: 0.4710409
Validation loss decreased (0.399739 --> 0.399000).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 122.34076499938965
Epoch: 62, Steps: 87 | Train Loss: 0.1352190 Vali Loss: 0.3984509 Test Loss: 0.4702342
Validation loss decreased (0.399000 --> 0.398451).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 125.16009449958801
Epoch: 63, Steps: 87 | Train Loss: 0.1346341 Vali Loss: 0.3977389 Test Loss: 0.4695135
Validation loss decreased (0.398451 --> 0.397739).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 124.17033982276917
Epoch: 64, Steps: 87 | Train Loss: 0.1340594 Vali Loss: 0.3969755 Test Loss: 0.4688334
Validation loss decreased (0.397739 --> 0.396976).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 118.92736530303955
Epoch: 65, Steps: 87 | Train Loss: 0.1335555 Vali Loss: 0.3959370 Test Loss: 0.4682262
Validation loss decreased (0.396976 --> 0.395937).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 115.53644442558289
Epoch: 66, Steps: 87 | Train Loss: 0.1330558 Vali Loss: 0.3956505 Test Loss: 0.4675622
Validation loss decreased (0.395937 --> 0.395650).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 115.60723543167114
Epoch: 67, Steps: 87 | Train Loss: 0.1325582 Vali Loss: 0.3953250 Test Loss: 0.4669471
Validation loss decreased (0.395650 --> 0.395325).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 109.81522512435913
Epoch: 68, Steps: 87 | Train Loss: 0.1321038 Vali Loss: 0.3945114 Test Loss: 0.4663313
Validation loss decreased (0.395325 --> 0.394511).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 107.8887345790863
Epoch: 69, Steps: 87 | Train Loss: 0.1316719 Vali Loss: 0.3941651 Test Loss: 0.4658033
Validation loss decreased (0.394511 --> 0.394165).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 90.73130393028259
Epoch: 70, Steps: 87 | Train Loss: 0.1312677 Vali Loss: 0.3937606 Test Loss: 0.4652777
Validation loss decreased (0.394165 --> 0.393761).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 95.27522158622742
Epoch: 71, Steps: 87 | Train Loss: 0.1309043 Vali Loss: 0.3931635 Test Loss: 0.4648014
Validation loss decreased (0.393761 --> 0.393164).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 96.38335347175598
Epoch: 72, Steps: 87 | Train Loss: 0.1304839 Vali Loss: 0.3926149 Test Loss: 0.4643373
Validation loss decreased (0.393164 --> 0.392615).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 93.01660680770874
Epoch: 73, Steps: 87 | Train Loss: 0.1301467 Vali Loss: 0.3925200 Test Loss: 0.4638691
Validation loss decreased (0.392615 --> 0.392520).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 103.06255626678467
Epoch: 74, Steps: 87 | Train Loss: 0.1298006 Vali Loss: 0.3915971 Test Loss: 0.4634680
Validation loss decreased (0.392520 --> 0.391597).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 130.69086384773254
Epoch: 75, Steps: 87 | Train Loss: 0.1295030 Vali Loss: 0.3918931 Test Loss: 0.4630679
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 97.7425184249878
Epoch: 76, Steps: 87 | Train Loss: 0.1292024 Vali Loss: 0.3909993 Test Loss: 0.4626870
Validation loss decreased (0.391597 --> 0.390999).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 104.04893922805786
Epoch: 77, Steps: 87 | Train Loss: 0.1289233 Vali Loss: 0.3913400 Test Loss: 0.4623154
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 99.13110828399658
Epoch: 78, Steps: 87 | Train Loss: 0.1286382 Vali Loss: 0.3907690 Test Loss: 0.4620025
Validation loss decreased (0.390999 --> 0.390769).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 145.22641921043396
Epoch: 79, Steps: 87 | Train Loss: 0.1283919 Vali Loss: 0.3904642 Test Loss: 0.4616312
Validation loss decreased (0.390769 --> 0.390464).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 156.5949981212616
Epoch: 80, Steps: 87 | Train Loss: 0.1281367 Vali Loss: 0.3900151 Test Loss: 0.4613749
Validation loss decreased (0.390464 --> 0.390015).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 150.5199055671692
Epoch: 81, Steps: 87 | Train Loss: 0.1279072 Vali Loss: 0.3899036 Test Loss: 0.4610617
Validation loss decreased (0.390015 --> 0.389904).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 151.7529969215393
Epoch: 82, Steps: 87 | Train Loss: 0.1276682 Vali Loss: 0.3894328 Test Loss: 0.4607782
Validation loss decreased (0.389904 --> 0.389433).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 147.98670434951782
Epoch: 83, Steps: 87 | Train Loss: 0.1274781 Vali Loss: 0.3891438 Test Loss: 0.4605113
Validation loss decreased (0.389433 --> 0.389144).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 154.2631585597992
Epoch: 84, Steps: 87 | Train Loss: 0.1272644 Vali Loss: 0.3891252 Test Loss: 0.4602700
Validation loss decreased (0.389144 --> 0.389125).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 158.4577283859253
Epoch: 85, Steps: 87 | Train Loss: 0.1270882 Vali Loss: 0.3889823 Test Loss: 0.4600061
Validation loss decreased (0.389125 --> 0.388982).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 154.1945834159851
Epoch: 86, Steps: 87 | Train Loss: 0.1269249 Vali Loss: 0.3885133 Test Loss: 0.4597707
Validation loss decreased (0.388982 --> 0.388513).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 140.79651808738708
Epoch: 87, Steps: 87 | Train Loss: 0.1267344 Vali Loss: 0.3881960 Test Loss: 0.4595635
Validation loss decreased (0.388513 --> 0.388196).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 135.0402009487152
Epoch: 88, Steps: 87 | Train Loss: 0.1265776 Vali Loss: 0.3885573 Test Loss: 0.4593513
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 134.64762687683105
Epoch: 89, Steps: 87 | Train Loss: 0.1263981 Vali Loss: 0.3881915 Test Loss: 0.4591481
Validation loss decreased (0.388196 --> 0.388191).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 138.88009428977966
Epoch: 90, Steps: 87 | Train Loss: 0.1262617 Vali Loss: 0.3875941 Test Loss: 0.4589804
Validation loss decreased (0.388191 --> 0.387594).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 139.4729025363922
Epoch: 91, Steps: 87 | Train Loss: 0.1261084 Vali Loss: 0.3877132 Test Loss: 0.4587947
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 128.45526313781738
Epoch: 92, Steps: 87 | Train Loss: 0.1260021 Vali Loss: 0.3875144 Test Loss: 0.4586244
Validation loss decreased (0.387594 --> 0.387514).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 136.95429587364197
Epoch: 93, Steps: 87 | Train Loss: 0.1258522 Vali Loss: 0.3874002 Test Loss: 0.4584717
Validation loss decreased (0.387514 --> 0.387400).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 143.33981323242188
Epoch: 94, Steps: 87 | Train Loss: 0.1257373 Vali Loss: 0.3872955 Test Loss: 0.4583112
Validation loss decreased (0.387400 --> 0.387296).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 136.06182312965393
Epoch: 95, Steps: 87 | Train Loss: 0.1256334 Vali Loss: 0.3870254 Test Loss: 0.4581751
Validation loss decreased (0.387296 --> 0.387025).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 150.60695958137512
Epoch: 96, Steps: 87 | Train Loss: 0.1255315 Vali Loss: 0.3869635 Test Loss: 0.4580404
Validation loss decreased (0.387025 --> 0.386964).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 151.12370252609253
Epoch: 97, Steps: 87 | Train Loss: 0.1254409 Vali Loss: 0.3869559 Test Loss: 0.4579123
Validation loss decreased (0.386964 --> 0.386956).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 150.79550862312317
Epoch: 98, Steps: 87 | Train Loss: 0.1253389 Vali Loss: 0.3866743 Test Loss: 0.4577699
Validation loss decreased (0.386956 --> 0.386674).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 150.53862285614014
Epoch: 99, Steps: 87 | Train Loss: 0.1252489 Vali Loss: 0.3864731 Test Loss: 0.4576640
Validation loss decreased (0.386674 --> 0.386473).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 155.5550057888031
Epoch: 100, Steps: 87 | Train Loss: 0.1251238 Vali Loss: 0.3866832 Test Loss: 0.4575506
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=258, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10760408064.0
params:  97902.0
Trainable parameters:  97902
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 139.7250998020172
Epoch: 1, Steps: 87 | Train Loss: 0.2598893 Vali Loss: 0.3438337 Test Loss: 0.4158100
Validation loss decreased (inf --> 0.343834).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 142.4716513156891
Epoch: 2, Steps: 87 | Train Loss: 0.2493470 Vali Loss: 0.3416262 Test Loss: 0.4152227
Validation loss decreased (0.343834 --> 0.341626).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 140.42394757270813
Epoch: 3, Steps: 87 | Train Loss: 0.2489219 Vali Loss: 0.3415333 Test Loss: 0.4155382
Validation loss decreased (0.341626 --> 0.341533).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 142.9572947025299
Epoch: 4, Steps: 87 | Train Loss: 0.2487911 Vali Loss: 0.3411191 Test Loss: 0.4144675
Validation loss decreased (0.341533 --> 0.341119).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 142.88373708724976
Epoch: 5, Steps: 87 | Train Loss: 0.2487738 Vali Loss: 0.3411424 Test Loss: 0.4143630
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 143.44202375411987
Epoch: 6, Steps: 87 | Train Loss: 0.2487762 Vali Loss: 0.3407716 Test Loss: 0.4146273
Validation loss decreased (0.341119 --> 0.340772).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 125.50773596763611
Epoch: 7, Steps: 87 | Train Loss: 0.2486873 Vali Loss: 0.3408270 Test Loss: 0.4142461
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 129.03986930847168
Epoch: 8, Steps: 87 | Train Loss: 0.2486151 Vali Loss: 0.3408130 Test Loss: 0.4147418
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 127.21867752075195
Epoch: 9, Steps: 87 | Train Loss: 0.2485599 Vali Loss: 0.3408561 Test Loss: 0.4144832
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 124.44055104255676
Epoch: 10, Steps: 87 | Train Loss: 0.2485681 Vali Loss: 0.3405250 Test Loss: 0.4141019
Validation loss decreased (0.340772 --> 0.340525).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 131.85157442092896
Epoch: 11, Steps: 87 | Train Loss: 0.2485065 Vali Loss: 0.3404797 Test Loss: 0.4139286
Validation loss decreased (0.340525 --> 0.340480).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 192.32165026664734
Epoch: 12, Steps: 87 | Train Loss: 0.2485388 Vali Loss: 0.3401154 Test Loss: 0.4142804
Validation loss decreased (0.340480 --> 0.340115).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 105.04137086868286
Epoch: 13, Steps: 87 | Train Loss: 0.2484026 Vali Loss: 0.3404112 Test Loss: 0.4142719
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 106.37302494049072
Epoch: 14, Steps: 87 | Train Loss: 0.2484355 Vali Loss: 0.3404087 Test Loss: 0.4143939
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 104.48102641105652
Epoch: 15, Steps: 87 | Train Loss: 0.2483960 Vali Loss: 0.3406608 Test Loss: 0.4139889
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 130.7730176448822
Epoch: 16, Steps: 87 | Train Loss: 0.2483781 Vali Loss: 0.3406355 Test Loss: 0.4142118
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 122.54895496368408
Epoch: 17, Steps: 87 | Train Loss: 0.2483534 Vali Loss: 0.3405567 Test Loss: 0.4141024
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 129.32294988632202
Epoch: 18, Steps: 87 | Train Loss: 0.2483046 Vali Loss: 0.3404122 Test Loss: 0.4142557
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 129.271479845047
Epoch: 19, Steps: 87 | Train Loss: 0.2483155 Vali Loss: 0.3405274 Test Loss: 0.4140359
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 134.77587270736694
Epoch: 20, Steps: 87 | Train Loss: 0.2482429 Vali Loss: 0.3410430 Test Loss: 0.4139200
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 129.10394167900085
Epoch: 21, Steps: 87 | Train Loss: 0.2482746 Vali Loss: 0.3403218 Test Loss: 0.4138623
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 126.16758036613464
Epoch: 22, Steps: 87 | Train Loss: 0.2481995 Vali Loss: 0.3401298 Test Loss: 0.4140002
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j336_H8_FITS_custom_ftM_sl720_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4121136963367462, mae:0.28099966049194336, rse:0.5276036262512207, corr:[0.2731779  0.28416625 0.28479356 0.28432673 0.2840189  0.28375998
 0.2838003  0.284415   0.28470773 0.28485096 0.28530577 0.28539714
 0.28526536 0.28507885 0.28476036 0.28469646 0.28459287 0.28422052
 0.28403667 0.28391418 0.28359017 0.28350103 0.28354913 0.2837896
 0.2854326  0.28590748 0.28547335 0.2847583  0.2843406  0.28415686
 0.2840438  0.28431505 0.28481627 0.28505072 0.2849571  0.28467444
 0.28431502 0.28440523 0.28485823 0.28485924 0.28453594 0.28458053
 0.28494522 0.28488463 0.28458044 0.28451574 0.28450084 0.28467396
 0.28511527 0.28492516 0.28480187 0.2849443  0.28476354 0.28427497
 0.28418714 0.28454706 0.28492168 0.28528655 0.28546453 0.28517288
 0.2848664  0.28509867 0.28547326 0.28535098 0.28494996 0.28453174
 0.28431332 0.2843856  0.28462696 0.28455177 0.2843983  0.2846232
 0.28451258 0.28410625 0.2842916  0.2846153  0.28426018 0.2838071
 0.28401825 0.28428552 0.28416055 0.28424376 0.28464854 0.2846993
 0.28441083 0.28433082 0.28452864 0.28465065 0.28468245 0.28477368
 0.28480083 0.2843873  0.28394824 0.28403667 0.2844545  0.28481904
 0.28464302 0.28424957 0.28397048 0.28387377 0.28377467 0.28358847
 0.28352487 0.28353423 0.28344306 0.2834823  0.28394833 0.2845065
 0.28460017 0.28418118 0.2837666  0.28370047 0.28380904 0.28385773
 0.28392    0.2839941  0.2840181  0.28405926 0.28437096 0.28459057
 0.28433737 0.28425997 0.28447843 0.28468063 0.28466138 0.28427884
 0.2840556  0.28429905 0.2843149  0.2838253  0.2837424  0.2843403
 0.28479794 0.2848728  0.28494444 0.28505567 0.28503975 0.2847821
 0.28436226 0.2841254  0.28430563 0.28472406 0.2850631  0.28527865
 0.28555736 0.28557602 0.28543642 0.28502506 0.284337   0.2839933
 0.28429854 0.2846225  0.28453574 0.28435454 0.28435245 0.28444907
 0.28456107 0.28464118 0.28463694 0.28458044 0.28448868 0.28439745
 0.28437254 0.28433675 0.28439856 0.2847884  0.28503227 0.2849426
 0.28556257 0.28558612 0.28541514 0.28521594 0.28518304 0.28527054
 0.28553492 0.28614342 0.28643978 0.28574997 0.28472003 0.28428382
 0.2843995  0.2846834  0.28512576 0.2855985  0.2858188  0.28568217
 0.28543314 0.28540656 0.2854046  0.28518647 0.2847858  0.28452945
 0.28498966 0.28535601 0.28554887 0.2853405  0.285238   0.28563246
 0.28583607 0.28544644 0.28488293 0.28435495 0.28398195 0.2841363
 0.2843963  0.28414357 0.28422347 0.2850171  0.2853255  0.28474638
 0.28430817 0.28448355 0.28486273 0.2851687  0.28512642 0.28481278
 0.28475225 0.284776   0.2847272  0.28449884 0.28427953 0.28408605
 0.28388834 0.28395864 0.28418785 0.2840937  0.28388974 0.28416944
 0.28479597 0.285176   0.2852256  0.2849783  0.28450674 0.284167
 0.28405598 0.28381932 0.28367803 0.28393656 0.2844636  0.28496867
 0.28521863 0.28506857 0.284824   0.28457728 0.28397775 0.28319958
 0.28299505 0.2832035  0.28322908 0.2833142  0.28352815 0.2832544
 0.28283793 0.2831679  0.28371066 0.2835081  0.2830944  0.28311706
 0.2831294  0.2829423  0.2828612  0.28294367 0.28329173 0.28389916
 0.28416926 0.28405827 0.2837113  0.2833391  0.2831831  0.2831193
 0.28297383 0.28278396 0.28252    0.2823166  0.28242895 0.28275737
 0.28307667 0.2830771  0.28274915 0.28263587 0.28296754 0.28321505
 0.2831379  0.28307083 0.28311563 0.2832269  0.28342608 0.2836186
 0.2836025  0.28372398 0.28391203 0.28393763 0.2836601  0.2833481
 0.2833683  0.28351334 0.28349608 0.28329262 0.283163   0.2833789
 0.28363532 0.28357455 0.2835415  0.2836063  0.28342074 0.2833455
 0.28370962 0.283916   0.28378853 0.28375816 0.28368923 0.28339148
 0.28344446 0.28352427 0.28347552 0.2832464  0.28285065 0.28263536
 0.282954   0.283308   0.2833377  0.28334397 0.28317043 0.28284222
 0.2831504  0.28375724 0.28387213 0.28400028 0.283977   0.2835068
 0.28349888 0.2833835  0.28312346 0.2841885  0.2840883  0.28482842]
