Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=72, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=72, out_features=91, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5870592.0
params:  6643.0
Trainable parameters:  6643
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.312802076339722
Epoch: 1, Steps: 60 | Train Loss: 0.6230414 Vali Loss: 1.6188645 Test Loss: 0.8091690
Validation loss decreased (inf --> 1.618865).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.690187454223633
Epoch: 2, Steps: 60 | Train Loss: 0.5042371 Vali Loss: 1.4398670 Test Loss: 0.7233729
Validation loss decreased (1.618865 --> 1.439867).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.247276782989502
Epoch: 3, Steps: 60 | Train Loss: 0.4339016 Vali Loss: 1.3406017 Test Loss: 0.6767098
Validation loss decreased (1.439867 --> 1.340602).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.416841745376587
Epoch: 4, Steps: 60 | Train Loss: 0.3895850 Vali Loss: 1.2862737 Test Loss: 0.6521342
Validation loss decreased (1.340602 --> 1.286274).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.374767780303955
Epoch: 5, Steps: 60 | Train Loss: 0.3589840 Vali Loss: 1.2516227 Test Loss: 0.6371481
Validation loss decreased (1.286274 --> 1.251623).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.4282402992248535
Epoch: 6, Steps: 60 | Train Loss: 0.3359823 Vali Loss: 1.2261435 Test Loss: 0.6257352
Validation loss decreased (1.251623 --> 1.226143).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.16526985168457
Epoch: 7, Steps: 60 | Train Loss: 0.3178118 Vali Loss: 1.2062238 Test Loss: 0.6164818
Validation loss decreased (1.226143 --> 1.206224).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.646798610687256
Epoch: 8, Steps: 60 | Train Loss: 0.3024968 Vali Loss: 1.1903653 Test Loss: 0.6079302
Validation loss decreased (1.206224 --> 1.190365).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.296505928039551
Epoch: 9, Steps: 60 | Train Loss: 0.2894688 Vali Loss: 1.1780984 Test Loss: 0.6016393
Validation loss decreased (1.190365 --> 1.178098).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.290112018585205
Epoch: 10, Steps: 60 | Train Loss: 0.2783157 Vali Loss: 1.1644243 Test Loss: 0.5940657
Validation loss decreased (1.178098 --> 1.164424).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.006887197494507
Epoch: 11, Steps: 60 | Train Loss: 0.2684164 Vali Loss: 1.1546233 Test Loss: 0.5881110
Validation loss decreased (1.164424 --> 1.154623).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.644181728363037
Epoch: 12, Steps: 60 | Train Loss: 0.2594803 Vali Loss: 1.1444530 Test Loss: 0.5812057
Validation loss decreased (1.154623 --> 1.144453).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.6297338008880615
Epoch: 13, Steps: 60 | Train Loss: 0.2516452 Vali Loss: 1.1360841 Test Loss: 0.5763646
Validation loss decreased (1.144453 --> 1.136084).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.665530681610107
Epoch: 14, Steps: 60 | Train Loss: 0.2445818 Vali Loss: 1.1265950 Test Loss: 0.5694600
Validation loss decreased (1.136084 --> 1.126595).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.304125785827637
Epoch: 15, Steps: 60 | Train Loss: 0.2381565 Vali Loss: 1.1196102 Test Loss: 0.5646625
Validation loss decreased (1.126595 --> 1.119610).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.57375431060791
Epoch: 16, Steps: 60 | Train Loss: 0.2323637 Vali Loss: 1.1126004 Test Loss: 0.5597969
Validation loss decreased (1.119610 --> 1.112600).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.161013603210449
Epoch: 17, Steps: 60 | Train Loss: 0.2272648 Vali Loss: 1.1064966 Test Loss: 0.5554224
Validation loss decreased (1.112600 --> 1.106497).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.282499074935913
Epoch: 18, Steps: 60 | Train Loss: 0.2224651 Vali Loss: 1.0996100 Test Loss: 0.5507051
Validation loss decreased (1.106497 --> 1.099610).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.442748785018921
Epoch: 19, Steps: 60 | Train Loss: 0.2180931 Vali Loss: 1.0939549 Test Loss: 0.5463730
Validation loss decreased (1.099610 --> 1.093955).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.5614752769470215
Epoch: 20, Steps: 60 | Train Loss: 0.2140702 Vali Loss: 1.0898873 Test Loss: 0.5434282
Validation loss decreased (1.093955 --> 1.089887).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.406875133514404
Epoch: 21, Steps: 60 | Train Loss: 0.2105342 Vali Loss: 1.0850799 Test Loss: 0.5398295
Validation loss decreased (1.089887 --> 1.085080).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.0984790325164795
Epoch: 22, Steps: 60 | Train Loss: 0.2072920 Vali Loss: 1.0795928 Test Loss: 0.5361056
Validation loss decreased (1.085080 --> 1.079593).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.363828659057617
Epoch: 23, Steps: 60 | Train Loss: 0.2042658 Vali Loss: 1.0750128 Test Loss: 0.5324842
Validation loss decreased (1.079593 --> 1.075013).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.702559471130371
Epoch: 24, Steps: 60 | Train Loss: 0.2013371 Vali Loss: 1.0714209 Test Loss: 0.5294917
Validation loss decreased (1.075013 --> 1.071421).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.462191104888916
Epoch: 25, Steps: 60 | Train Loss: 0.1986729 Vali Loss: 1.0676378 Test Loss: 0.5267609
Validation loss decreased (1.071421 --> 1.067638).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.609709024429321
Epoch: 26, Steps: 60 | Train Loss: 0.1963457 Vali Loss: 1.0644083 Test Loss: 0.5244579
Validation loss decreased (1.067638 --> 1.064408).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.902953386306763
Epoch: 27, Steps: 60 | Train Loss: 0.1941598 Vali Loss: 1.0610288 Test Loss: 0.5218242
Validation loss decreased (1.064408 --> 1.061029).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.403537034988403
Epoch: 28, Steps: 60 | Train Loss: 0.1921281 Vali Loss: 1.0575800 Test Loss: 0.5191650
Validation loss decreased (1.061029 --> 1.057580).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.54403281211853
Epoch: 29, Steps: 60 | Train Loss: 0.1902202 Vali Loss: 1.0550299 Test Loss: 0.5172821
Validation loss decreased (1.057580 --> 1.055030).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.373553991317749
Epoch: 30, Steps: 60 | Train Loss: 0.1883359 Vali Loss: 1.0524371 Test Loss: 0.5151044
Validation loss decreased (1.055030 --> 1.052437).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.4158501625061035
Epoch: 31, Steps: 60 | Train Loss: 0.1867396 Vali Loss: 1.0497401 Test Loss: 0.5129197
Validation loss decreased (1.052437 --> 1.049740).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.844598054885864
Epoch: 32, Steps: 60 | Train Loss: 0.1851799 Vali Loss: 1.0474436 Test Loss: 0.5112789
Validation loss decreased (1.049740 --> 1.047444).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.642145395278931
Epoch: 33, Steps: 60 | Train Loss: 0.1837323 Vali Loss: 1.0450401 Test Loss: 0.5092283
Validation loss decreased (1.047444 --> 1.045040).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.447865724563599
Epoch: 34, Steps: 60 | Train Loss: 0.1823570 Vali Loss: 1.0430428 Test Loss: 0.5075842
Validation loss decreased (1.045040 --> 1.043043).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.321367979049683
Epoch: 35, Steps: 60 | Train Loss: 0.1811106 Vali Loss: 1.0403166 Test Loss: 0.5059228
Validation loss decreased (1.043043 --> 1.040317).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.528412818908691
Epoch: 36, Steps: 60 | Train Loss: 0.1799257 Vali Loss: 1.0394499 Test Loss: 0.5047557
Validation loss decreased (1.040317 --> 1.039450).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.2334864139556885
Epoch: 37, Steps: 60 | Train Loss: 0.1788891 Vali Loss: 1.0377817 Test Loss: 0.5033860
Validation loss decreased (1.039450 --> 1.037782).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.477697134017944
Epoch: 38, Steps: 60 | Train Loss: 0.1777307 Vali Loss: 1.0357385 Test Loss: 0.5020849
Validation loss decreased (1.037782 --> 1.035738).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.4145402908325195
Epoch: 39, Steps: 60 | Train Loss: 0.1768127 Vali Loss: 1.0342638 Test Loss: 0.5008154
Validation loss decreased (1.035738 --> 1.034264).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.314938545227051
Epoch: 40, Steps: 60 | Train Loss: 0.1759412 Vali Loss: 1.0329134 Test Loss: 0.4996493
Validation loss decreased (1.034264 --> 1.032913).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.9024059772491455
Epoch: 41, Steps: 60 | Train Loss: 0.1750268 Vali Loss: 1.0316092 Test Loss: 0.4986564
Validation loss decreased (1.032913 --> 1.031609).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.409973382949829
Epoch: 42, Steps: 60 | Train Loss: 0.1742376 Vali Loss: 1.0304056 Test Loss: 0.4975929
Validation loss decreased (1.031609 --> 1.030406).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.614672422409058
Epoch: 43, Steps: 60 | Train Loss: 0.1735103 Vali Loss: 1.0286437 Test Loss: 0.4965343
Validation loss decreased (1.030406 --> 1.028644).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.222153425216675
Epoch: 44, Steps: 60 | Train Loss: 0.1728284 Vali Loss: 1.0279390 Test Loss: 0.4956936
Validation loss decreased (1.028644 --> 1.027939).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.330897092819214
Epoch: 45, Steps: 60 | Train Loss: 0.1720842 Vali Loss: 1.0268282 Test Loss: 0.4948981
Validation loss decreased (1.027939 --> 1.026828).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.27686619758606
Epoch: 46, Steps: 60 | Train Loss: 0.1714158 Vali Loss: 1.0261146 Test Loss: 0.4942175
Validation loss decreased (1.026828 --> 1.026115).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.272329330444336
Epoch: 47, Steps: 60 | Train Loss: 0.1707686 Vali Loss: 1.0248138 Test Loss: 0.4932556
Validation loss decreased (1.026115 --> 1.024814).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.444821357727051
Epoch: 48, Steps: 60 | Train Loss: 0.1702694 Vali Loss: 1.0242165 Test Loss: 0.4925769
Validation loss decreased (1.024814 --> 1.024217).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 4.442817449569702
Epoch: 49, Steps: 60 | Train Loss: 0.1697097 Vali Loss: 1.0231841 Test Loss: 0.4918268
Validation loss decreased (1.024217 --> 1.023184).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.6169373989105225
Epoch: 50, Steps: 60 | Train Loss: 0.1692323 Vali Loss: 1.0216773 Test Loss: 0.4910702
Validation loss decreased (1.023184 --> 1.021677).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.2959558963775635
Epoch: 51, Steps: 60 | Train Loss: 0.1687808 Vali Loss: 1.0210651 Test Loss: 0.4904820
Validation loss decreased (1.021677 --> 1.021065).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.580524444580078
Epoch: 52, Steps: 60 | Train Loss: 0.1682962 Vali Loss: 1.0208268 Test Loss: 0.4900131
Validation loss decreased (1.021065 --> 1.020827).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.304898500442505
Epoch: 53, Steps: 60 | Train Loss: 0.1677954 Vali Loss: 1.0202656 Test Loss: 0.4893866
Validation loss decreased (1.020827 --> 1.020266).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 4.252245903015137
Epoch: 54, Steps: 60 | Train Loss: 0.1674006 Vali Loss: 1.0196633 Test Loss: 0.4889019
Validation loss decreased (1.020266 --> 1.019663).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.409902811050415
Epoch: 55, Steps: 60 | Train Loss: 0.1670822 Vali Loss: 1.0189426 Test Loss: 0.4884056
Validation loss decreased (1.019663 --> 1.018943).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 4.28874945640564
Epoch: 56, Steps: 60 | Train Loss: 0.1666957 Vali Loss: 1.0182600 Test Loss: 0.4878508
Validation loss decreased (1.018943 --> 1.018260).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.323843717575073
Epoch: 57, Steps: 60 | Train Loss: 0.1663197 Vali Loss: 1.0171865 Test Loss: 0.4873668
Validation loss decreased (1.018260 --> 1.017187).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 4.246831655502319
Epoch: 58, Steps: 60 | Train Loss: 0.1661076 Vali Loss: 1.0169798 Test Loss: 0.4868774
Validation loss decreased (1.017187 --> 1.016980).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.20457124710083
Epoch: 59, Steps: 60 | Train Loss: 0.1656964 Vali Loss: 1.0164301 Test Loss: 0.4866036
Validation loss decreased (1.016980 --> 1.016430).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 4.535010814666748
Epoch: 60, Steps: 60 | Train Loss: 0.1654287 Vali Loss: 1.0161463 Test Loss: 0.4861641
Validation loss decreased (1.016430 --> 1.016146).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 4.659522533416748
Epoch: 61, Steps: 60 | Train Loss: 0.1651205 Vali Loss: 1.0154703 Test Loss: 0.4857906
Validation loss decreased (1.016146 --> 1.015470).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 4.599716663360596
Epoch: 62, Steps: 60 | Train Loss: 0.1648790 Vali Loss: 1.0152721 Test Loss: 0.4854363
Validation loss decreased (1.015470 --> 1.015272).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 4.284397602081299
Epoch: 63, Steps: 60 | Train Loss: 0.1646115 Vali Loss: 1.0146147 Test Loss: 0.4851378
Validation loss decreased (1.015272 --> 1.014615).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 4.705557346343994
Epoch: 64, Steps: 60 | Train Loss: 0.1643790 Vali Loss: 1.0144898 Test Loss: 0.4847914
Validation loss decreased (1.014615 --> 1.014490).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 4.603007555007935
Epoch: 65, Steps: 60 | Train Loss: 0.1641234 Vali Loss: 1.0139719 Test Loss: 0.4843588
Validation loss decreased (1.014490 --> 1.013972).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 4.270253658294678
Epoch: 66, Steps: 60 | Train Loss: 0.1639481 Vali Loss: 1.0132997 Test Loss: 0.4841487
Validation loss decreased (1.013972 --> 1.013300).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 4.442067384719849
Epoch: 67, Steps: 60 | Train Loss: 0.1637499 Vali Loss: 1.0133600 Test Loss: 0.4838613
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 4.2669031620025635
Epoch: 68, Steps: 60 | Train Loss: 0.1635911 Vali Loss: 1.0129335 Test Loss: 0.4836239
Validation loss decreased (1.013300 --> 1.012933).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 4.336459398269653
Epoch: 69, Steps: 60 | Train Loss: 0.1633203 Vali Loss: 1.0127501 Test Loss: 0.4833310
Validation loss decreased (1.012933 --> 1.012750).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.407073974609375
Epoch: 70, Steps: 60 | Train Loss: 0.1631486 Vali Loss: 1.0120834 Test Loss: 0.4831471
Validation loss decreased (1.012750 --> 1.012083).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 4.79484486579895
Epoch: 71, Steps: 60 | Train Loss: 0.1630488 Vali Loss: 1.0121939 Test Loss: 0.4829049
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 4.284836769104004
Epoch: 72, Steps: 60 | Train Loss: 0.1628448 Vali Loss: 1.0119178 Test Loss: 0.4826795
Validation loss decreased (1.012083 --> 1.011918).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 4.0856029987335205
Epoch: 73, Steps: 60 | Train Loss: 0.1626716 Vali Loss: 1.0117766 Test Loss: 0.4824966
Validation loss decreased (1.011918 --> 1.011777).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 4.331811428070068
Epoch: 74, Steps: 60 | Train Loss: 0.1625370 Vali Loss: 1.0112808 Test Loss: 0.4822766
Validation loss decreased (1.011777 --> 1.011281).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 4.556210517883301
Epoch: 75, Steps: 60 | Train Loss: 0.1624181 Vali Loss: 1.0112634 Test Loss: 0.4821141
Validation loss decreased (1.011281 --> 1.011263).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 4.168624639511108
Epoch: 76, Steps: 60 | Train Loss: 0.1623044 Vali Loss: 1.0106821 Test Loss: 0.4819390
Validation loss decreased (1.011263 --> 1.010682).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 4.34007716178894
Epoch: 77, Steps: 60 | Train Loss: 0.1621269 Vali Loss: 1.0109930 Test Loss: 0.4817829
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 4.843744993209839
Epoch: 78, Steps: 60 | Train Loss: 0.1619876 Vali Loss: 1.0103054 Test Loss: 0.4816066
Validation loss decreased (1.010682 --> 1.010305).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 4.129927635192871
Epoch: 79, Steps: 60 | Train Loss: 0.1618764 Vali Loss: 1.0106108 Test Loss: 0.4814687
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 4.258517742156982
Epoch: 80, Steps: 60 | Train Loss: 0.1618057 Vali Loss: 1.0104234 Test Loss: 0.4813148
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 4.420856237411499
Epoch: 81, Steps: 60 | Train Loss: 0.1616844 Vali Loss: 1.0102072 Test Loss: 0.4811415
Validation loss decreased (1.010305 --> 1.010207).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 4.605412483215332
Epoch: 82, Steps: 60 | Train Loss: 0.1616190 Vali Loss: 1.0095828 Test Loss: 0.4810672
Validation loss decreased (1.010207 --> 1.009583).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 4.2840187549591064
Epoch: 83, Steps: 60 | Train Loss: 0.1614139 Vali Loss: 1.0097406 Test Loss: 0.4809180
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 4.289656639099121
Epoch: 84, Steps: 60 | Train Loss: 0.1614953 Vali Loss: 1.0093572 Test Loss: 0.4808073
Validation loss decreased (1.009583 --> 1.009357).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.8406291007995605
Epoch: 85, Steps: 60 | Train Loss: 0.1613782 Vali Loss: 1.0095861 Test Loss: 0.4806857
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 4.595497369766235
Epoch: 86, Steps: 60 | Train Loss: 0.1612987 Vali Loss: 1.0092242 Test Loss: 0.4805788
Validation loss decreased (1.009357 --> 1.009224).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 4.425638198852539
Epoch: 87, Steps: 60 | Train Loss: 0.1611288 Vali Loss: 1.0093478 Test Loss: 0.4805026
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 4.2619240283966064
Epoch: 88, Steps: 60 | Train Loss: 0.1611415 Vali Loss: 1.0090592 Test Loss: 0.4804063
Validation loss decreased (1.009224 --> 1.009059).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 4.549601316452026
Epoch: 89, Steps: 60 | Train Loss: 0.1609981 Vali Loss: 1.0086387 Test Loss: 0.4803035
Validation loss decreased (1.009059 --> 1.008639).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 4.124640226364136
Epoch: 90, Steps: 60 | Train Loss: 0.1610242 Vali Loss: 1.0087864 Test Loss: 0.4802310
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 4.044558048248291
Epoch: 91, Steps: 60 | Train Loss: 0.1608948 Vali Loss: 1.0089060 Test Loss: 0.4801435
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 4.509640693664551
Epoch: 92, Steps: 60 | Train Loss: 0.1608734 Vali Loss: 1.0088608 Test Loss: 0.4800673
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 4.258692264556885
Epoch: 93, Steps: 60 | Train Loss: 0.1607718 Vali Loss: 1.0086833 Test Loss: 0.4799984
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 4.374752044677734
Epoch: 94, Steps: 60 | Train Loss: 0.1606710 Vali Loss: 1.0082111 Test Loss: 0.4799326
Validation loss decreased (1.008639 --> 1.008211).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 4.518279075622559
Epoch: 95, Steps: 60 | Train Loss: 0.1607201 Vali Loss: 1.0086809 Test Loss: 0.4798712
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 4.561135292053223
Epoch: 96, Steps: 60 | Train Loss: 0.1606542 Vali Loss: 1.0084044 Test Loss: 0.4798013
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 4.209131479263306
Epoch: 97, Steps: 60 | Train Loss: 0.1605652 Vali Loss: 1.0085113 Test Loss: 0.4797353
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 4.156149625778198
Epoch: 98, Steps: 60 | Train Loss: 0.1605700 Vali Loss: 1.0084311 Test Loss: 0.4796822
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 4.280060291290283
Epoch: 99, Steps: 60 | Train Loss: 0.1605249 Vali Loss: 1.0076742 Test Loss: 0.4796235
Validation loss decreased (1.008211 --> 1.007674).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 4.170628786087036
Epoch: 100, Steps: 60 | Train Loss: 0.1605608 Vali Loss: 1.0083162 Test Loss: 0.4795707
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=72, out_features=91, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5870592.0
params:  6643.0
Trainable parameters:  6643
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.094212770462036
Epoch: 1, Steps: 60 | Train Loss: 0.4116692 Vali Loss: 0.9631058 Test Loss: 0.4424591
Validation loss decreased (inf --> 0.963106).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.46862530708313
Epoch: 2, Steps: 60 | Train Loss: 0.3989846 Vali Loss: 0.9633411 Test Loss: 0.4384771
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.4826765060424805
Epoch: 3, Steps: 60 | Train Loss: 0.3967852 Vali Loss: 0.9658198 Test Loss: 0.4390310
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.264110565185547
Epoch: 4, Steps: 60 | Train Loss: 0.3957716 Vali Loss: 0.9684844 Test Loss: 0.4400142
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.914048194885254
Epoch: 5, Steps: 60 | Train Loss: 0.3951085 Vali Loss: 0.9684077 Test Loss: 0.4400243
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.298290014266968
Epoch: 6, Steps: 60 | Train Loss: 0.3941834 Vali Loss: 0.9704406 Test Loss: 0.4402215
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.464662790298462
Epoch: 7, Steps: 60 | Train Loss: 0.3941595 Vali Loss: 0.9714314 Test Loss: 0.4398881
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.077823162078857
Epoch: 8, Steps: 60 | Train Loss: 0.3936401 Vali Loss: 0.9704605 Test Loss: 0.4400628
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.183613538742065
Epoch: 9, Steps: 60 | Train Loss: 0.3935055 Vali Loss: 0.9700669 Test Loss: 0.4401396
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.415417432785034
Epoch: 10, Steps: 60 | Train Loss: 0.3931402 Vali Loss: 0.9709548 Test Loss: 0.4401009
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.515515327453613
Epoch: 11, Steps: 60 | Train Loss: 0.3929610 Vali Loss: 0.9712540 Test Loss: 0.4399880
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.149288177490234
Epoch: 12, Steps: 60 | Train Loss: 0.3928505 Vali Loss: 0.9728391 Test Loss: 0.4402569
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.188465595245361
Epoch: 13, Steps: 60 | Train Loss: 0.3928776 Vali Loss: 0.9711361 Test Loss: 0.4403661
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.261120080947876
Epoch: 14, Steps: 60 | Train Loss: 0.3926141 Vali Loss: 0.9714900 Test Loss: 0.4402486
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.0430567264556885
Epoch: 15, Steps: 60 | Train Loss: 0.3926504 Vali Loss: 0.9701083 Test Loss: 0.4399963
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.9720394611358643
Epoch: 16, Steps: 60 | Train Loss: 0.3926140 Vali Loss: 0.9714366 Test Loss: 0.4401458
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.242906808853149
Epoch: 17, Steps: 60 | Train Loss: 0.3921282 Vali Loss: 0.9711137 Test Loss: 0.4403337
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.352798700332642
Epoch: 18, Steps: 60 | Train Loss: 0.3921940 Vali Loss: 0.9711347 Test Loss: 0.4401781
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.446816921234131
Epoch: 19, Steps: 60 | Train Loss: 0.3922477 Vali Loss: 0.9709926 Test Loss: 0.4401846
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.330390691757202
Epoch: 20, Steps: 60 | Train Loss: 0.3924966 Vali Loss: 0.9718372 Test Loss: 0.4401967
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.373039960861206
Epoch: 21, Steps: 60 | Train Loss: 0.3924168 Vali Loss: 0.9712532 Test Loss: 0.4399939
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.44050419330596924, mae:0.44497689604759216, rse:0.6302783489227295, corr:[0.2600038  0.26468965 0.26670137 0.26626784 0.26491892 0.263776
 0.26309556 0.2627463  0.26246703 0.26224607 0.26203075 0.26176316
 0.2614922  0.2612651  0.2610843  0.26086125 0.26041108 0.25983435
 0.2592496  0.2588126  0.25860408 0.25869924 0.25875765 0.2587303
 0.25852224 0.25816014 0.25766534 0.25707304 0.25654054 0.25619486
 0.25613993 0.25620982 0.25644162 0.2567187  0.2568106  0.25676927
 0.25675923 0.25677013 0.25673443 0.25663707 0.25659543 0.25656447
 0.25650984 0.25649527 0.25655025 0.25666398 0.25673673 0.25667813
 0.25619337 0.25552526 0.25466862 0.25376824 0.2529775  0.25224692
 0.2516681  0.25122425 0.25076768 0.25029895 0.24977878 0.24930024
 0.24900974 0.24892926 0.24906385 0.24943577 0.24976566 0.24991821
 0.24990055 0.24960443 0.24922967 0.24898416 0.24893552 0.2489424
 0.24888346 0.24871317 0.24837019 0.24800439 0.2475864  0.24711353
 0.2467056  0.24643275 0.24629034 0.24621285 0.24611524 0.24597606
 0.24592188 0.24585932 0.24571328 0.2454249  0.24497806 0.24447645
 0.24398525 0.24361545 0.24345748 0.24362622 0.24411298 0.24462464
 0.24508737 0.24542636 0.24545065 0.2452608  0.24500889 0.24478681
 0.24469964 0.2446896  0.24463375 0.2445764  0.24442124 0.24414411
 0.24392098 0.24379024 0.24377613 0.24389707 0.2438582  0.24359758
 0.24325876 0.2428542  0.24241318 0.24202275 0.24172963 0.24147731
 0.24125756 0.24095877 0.24054559 0.24019624 0.23999201 0.23981853
 0.23971559 0.23957895 0.23923488 0.23881902 0.23838504 0.23800662
 0.23772521 0.23754476 0.23751323 0.2375575  0.23770629 0.23783045
 0.23786871 0.2376654  0.23729081 0.23685947 0.2363855  0.23553059
 0.23439813 0.23311323 0.23175615 0.23050405 0.22970393 0.229297
 0.22924167 0.22938657 0.22953029 0.22963716 0.22951764 0.22922876
 0.22890681 0.22873746 0.22875793 0.2288296  0.22874323 0.22873983
 0.22864012 0.22831385 0.2279031  0.22745375 0.227158   0.2267815
 0.2264596  0.22624356 0.22585265 0.22524829 0.22441319 0.22360188
 0.22302002 0.2228838  0.22283502 0.22289537 0.22277099 0.22227977
 0.22142683 0.22032343 0.21924932 0.2185744  0.21866105 0.2195519
 0.22072208 0.22118978 0.22022678 0.21718585 0.21081838 0.19934265]
