Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  23532544.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.0619378089904785
Epoch: 1, Steps: 59 | Train Loss: 0.7528287 Vali Loss: 1.4748542 Test Loss: 0.6430255
Validation loss decreased (inf --> 1.474854).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.0754449367523193
Epoch: 2, Steps: 59 | Train Loss: 0.5791767 Vali Loss: 1.3180165 Test Loss: 0.5378333
Validation loss decreased (1.474854 --> 1.318017).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.07553768157959
Epoch: 3, Steps: 59 | Train Loss: 0.5231012 Vali Loss: 1.2584393 Test Loss: 0.4915550
Validation loss decreased (1.318017 --> 1.258439).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9450809955596924
Epoch: 4, Steps: 59 | Train Loss: 0.4943995 Vali Loss: 1.2218317 Test Loss: 0.4666623
Validation loss decreased (1.258439 --> 1.221832).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.1056160926818848
Epoch: 5, Steps: 59 | Train Loss: 0.4775084 Vali Loss: 1.2022184 Test Loss: 0.4526245
Validation loss decreased (1.221832 --> 1.202218).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.995309829711914
Epoch: 6, Steps: 59 | Train Loss: 0.4671251 Vali Loss: 1.1952330 Test Loss: 0.4451498
Validation loss decreased (1.202218 --> 1.195233).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.047114849090576
Epoch: 7, Steps: 59 | Train Loss: 0.4606214 Vali Loss: 1.1906900 Test Loss: 0.4409898
Validation loss decreased (1.195233 --> 1.190690).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.0932083129882812
Epoch: 8, Steps: 59 | Train Loss: 0.4561547 Vali Loss: 1.1834837 Test Loss: 0.4391706
Validation loss decreased (1.190690 --> 1.183484).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.053856134414673
Epoch: 9, Steps: 59 | Train Loss: 0.4531278 Vali Loss: 1.1870811 Test Loss: 0.4381651
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.971353530883789
Epoch: 10, Steps: 59 | Train Loss: 0.4509040 Vali Loss: 1.1885505 Test Loss: 0.4382020
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.0399489402770996
Epoch: 11, Steps: 59 | Train Loss: 0.4491875 Vali Loss: 1.1879570 Test Loss: 0.4382926
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.0710039138793945
Epoch: 12, Steps: 59 | Train Loss: 0.4476887 Vali Loss: 1.1919127 Test Loss: 0.4385760
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.2343592643737793
Epoch: 13, Steps: 59 | Train Loss: 0.4468522 Vali Loss: 1.1886746 Test Loss: 0.4389242
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.0709784030914307
Epoch: 14, Steps: 59 | Train Loss: 0.4457468 Vali Loss: 1.1947979 Test Loss: 0.4392125
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.0205912590026855
Epoch: 15, Steps: 59 | Train Loss: 0.4448661 Vali Loss: 1.1955671 Test Loss: 0.4395052
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.102267265319824
Epoch: 16, Steps: 59 | Train Loss: 0.4439474 Vali Loss: 1.1943846 Test Loss: 0.4397859
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.014345407485962
Epoch: 17, Steps: 59 | Train Loss: 0.4433564 Vali Loss: 1.1936281 Test Loss: 0.4401161
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.196436643600464
Epoch: 18, Steps: 59 | Train Loss: 0.4432073 Vali Loss: 1.1951749 Test Loss: 0.4402741
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.1256234645843506
Epoch: 19, Steps: 59 | Train Loss: 0.4425812 Vali Loss: 1.2001919 Test Loss: 0.4405666
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9942095279693604
Epoch: 20, Steps: 59 | Train Loss: 0.4421897 Vali Loss: 1.1967548 Test Loss: 0.4408808
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.2473647594451904
Epoch: 21, Steps: 59 | Train Loss: 0.4415983 Vali Loss: 1.2018293 Test Loss: 0.4408984
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.129417896270752
Epoch: 22, Steps: 59 | Train Loss: 0.4411306 Vali Loss: 1.2014201 Test Loss: 0.4411589
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8886840343475342
Epoch: 23, Steps: 59 | Train Loss: 0.4412283 Vali Loss: 1.1981359 Test Loss: 0.4413377
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.1678481101989746
Epoch: 24, Steps: 59 | Train Loss: 0.4407234 Vali Loss: 1.1964134 Test Loss: 0.4415168
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.1414284706115723
Epoch: 25, Steps: 59 | Train Loss: 0.4403828 Vali Loss: 1.2060537 Test Loss: 0.4416891
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.987297773361206
Epoch: 26, Steps: 59 | Train Loss: 0.4402520 Vali Loss: 1.1997705 Test Loss: 0.4418361
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.23164439201355
Epoch: 27, Steps: 59 | Train Loss: 0.4401842 Vali Loss: 1.2041876 Test Loss: 0.4420095
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.015723705291748
Epoch: 28, Steps: 59 | Train Loss: 0.4398643 Vali Loss: 1.2048897 Test Loss: 0.4420086
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4379706084728241, mae:0.4425084590911865, rse:0.6300492882728577, corr:[0.24912058 0.26029992 0.25775713 0.2518602  0.25085917 0.25234938
 0.25269577 0.25189736 0.2508036  0.2506211  0.25093043 0.25089833
 0.25056684 0.25023618 0.25035658 0.25052845 0.24992426 0.24863367
 0.24750365 0.24722491 0.24776238 0.24794596 0.2473704  0.24617532
 0.24527177 0.24521406 0.24561681 0.24580415 0.2456053  0.24531509
 0.24543521 0.24600568 0.24667637 0.24699648 0.24679852 0.24641286
 0.24615493 0.24595828 0.2458142  0.2457786  0.24598595 0.24650869
 0.24710768 0.24767382 0.24811663 0.24851114 0.24885799 0.2489494
 0.24866864 0.2479794  0.24695744 0.2460503  0.24536434 0.2447523
 0.24423166 0.24379931 0.24341342 0.24309067 0.24293137 0.24290378
 0.2428719  0.24278234 0.24265008 0.24264891 0.24272455 0.24283288
 0.24285586 0.24271451 0.24272932 0.24275538 0.24266429 0.24237017
 0.2418897  0.24137494 0.24106054 0.24099272 0.24091463 0.24070795
 0.24050426 0.24043489 0.24041428 0.24031594 0.24005608 0.23973043
 0.23936827 0.23903993 0.23877072 0.23860376 0.23842242 0.23827815
 0.23813452 0.2380389  0.23782437 0.23757446 0.23744367 0.2377838
 0.2385745  0.23946746 0.24009961 0.24041472 0.24055277 0.2406039
 0.2407329  0.24090093 0.24093106 0.2407536  0.24053915 0.24048229
 0.24053037 0.24053507 0.2403363  0.24008243 0.23991275 0.23994176
 0.24002974 0.23991513 0.23966466 0.23941411 0.23928316 0.23925604
 0.23903653 0.23839065 0.2375953  0.23704313 0.23679918 0.23657517
 0.2362583  0.23599045 0.23579328 0.2357543  0.23577683 0.23573373
 0.23563191 0.23551223 0.23554353 0.2356672  0.23555718 0.23514454
 0.23455095 0.2341971  0.23426832 0.23438306 0.23415188 0.23371959
 0.23322202 0.23278497 0.23244242 0.23201156 0.2315328  0.23104465
 0.23085332 0.23111483 0.23153254 0.23167837 0.2315307  0.23136044
 0.23123717 0.23108704 0.23081511 0.23056595 0.230401   0.2305926
 0.23090632 0.23100923 0.23074792 0.23030008 0.22978896 0.22952898
 0.22952466 0.22962114 0.22962858 0.22961046 0.22956692 0.22947952
 0.22929321 0.22919321 0.22921106 0.22941501 0.22956797 0.22946194
 0.2290407  0.22867312 0.22880048 0.22944097 0.2300881  0.23030813
 0.23013261 0.23003922 0.23040295 0.2309149  0.23094249 0.23032014
 0.22939496 0.22871904 0.22844192 0.22818272 0.22761859 0.22679742
 0.22615212 0.22610164 0.22638266 0.22637613 0.22596812 0.22575028
 0.22614704 0.22708063 0.22774841 0.2275969  0.22690934 0.22632045
 0.22624606 0.22636597 0.2262051  0.22564432 0.22508915 0.22504623
 0.22539274 0.225564   0.22512527 0.22431326 0.22376886 0.2236119
 0.2236495  0.2236423  0.22348842 0.22332685 0.22326578 0.2233966
 0.22331892 0.22289541 0.22255063 0.22266775 0.22293912 0.2230357
 0.22271092 0.22212487 0.22175863 0.22185893 0.22227961 0.22247182
 0.22240914 0.2222566  0.2223447  0.22246647 0.22217526 0.2215556
 0.22110681 0.221255   0.22167498 0.22166166 0.22098926 0.22028133
 0.22016992 0.2205887  0.22104402 0.22089617 0.22034746 0.22008298
 0.220594   0.22129911 0.22149934 0.22081144 0.21974002 0.21902236
 0.21917242 0.21973054 0.21983409 0.21929593 0.21830232 0.21739043
 0.21689233 0.21659276 0.21639092 0.21664006 0.21721493 0.21760923
 0.21719521 0.21604715 0.21492273 0.21480772 0.21564773 0.21640699
 0.2159569  0.21441253 0.21318552 0.21320862 0.21418063 0.21489003
 0.21466617 0.21405038 0.21382442 0.21440092 0.2146886  0.21408981
 0.21297367 0.21235031 0.21269983 0.21342403 0.21336026 0.21261597
 0.21229152 0.21299718 0.21425746 0.21446992 0.21331662 0.21191618
 0.21200879 0.21320425 0.21415918 0.2132208  0.21079025 0.20874304
 0.20866185 0.20972487 0.20960623 0.20769623 0.2055752  0.20468903
 0.20511013 0.2051605  0.20412719 0.20290089 0.20263012 0.20327356
 0.20316696 0.20075546 0.19771251 0.19739439 0.19942996 0.20039546
 0.19625169 0.1880972  0.1832435  0.18675293 0.18993922 0.16553435]
