Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=1919, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=106, out_features=204, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19375104.0
params:  21828.0
Trainable parameters:  21828
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7511205673217773
Epoch: 1, Steps: 62 | Train Loss: 0.7533112 Vali Loss: 1.5323521 Test Loss: 0.6735712
Validation loss decreased (inf --> 1.532352).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8808350563049316
Epoch: 2, Steps: 62 | Train Loss: 0.5888269 Vali Loss: 1.3826597 Test Loss: 0.5753715
Validation loss decreased (1.532352 --> 1.382660).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.6843750476837158
Epoch: 3, Steps: 62 | Train Loss: 0.5418919 Vali Loss: 1.3282263 Test Loss: 0.5331221
Validation loss decreased (1.382660 --> 1.328226).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.364098310470581
Epoch: 4, Steps: 62 | Train Loss: 0.5173688 Vali Loss: 1.2883481 Test Loss: 0.5059644
Validation loss decreased (1.328226 --> 1.288348).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7082114219665527
Epoch: 5, Steps: 62 | Train Loss: 0.5010487 Vali Loss: 1.2638286 Test Loss: 0.4861161
Validation loss decreased (1.288348 --> 1.263829).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.4393057823181152
Epoch: 6, Steps: 62 | Train Loss: 0.4893715 Vali Loss: 1.2416631 Test Loss: 0.4713480
Validation loss decreased (1.263829 --> 1.241663).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5613634586334229
Epoch: 7, Steps: 62 | Train Loss: 0.4807517 Vali Loss: 1.2249579 Test Loss: 0.4604503
Validation loss decreased (1.241663 --> 1.224958).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.4404723644256592
Epoch: 8, Steps: 62 | Train Loss: 0.4742127 Vali Loss: 1.2089624 Test Loss: 0.4521717
Validation loss decreased (1.224958 --> 1.208962).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.5748815536499023
Epoch: 9, Steps: 62 | Train Loss: 0.4694524 Vali Loss: 1.1952248 Test Loss: 0.4461074
Validation loss decreased (1.208962 --> 1.195225).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.5367534160614014
Epoch: 10, Steps: 62 | Train Loss: 0.4658125 Vali Loss: 1.1961794 Test Loss: 0.4416599
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.0897693634033203
Epoch: 11, Steps: 62 | Train Loss: 0.4629222 Vali Loss: 1.1855125 Test Loss: 0.4383513
Validation loss decreased (1.195225 --> 1.185513).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.6062932014465332
Epoch: 12, Steps: 62 | Train Loss: 0.4608152 Vali Loss: 1.1795790 Test Loss: 0.4358381
Validation loss decreased (1.185513 --> 1.179579).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5543615818023682
Epoch: 13, Steps: 62 | Train Loss: 0.4594297 Vali Loss: 1.1785641 Test Loss: 0.4340616
Validation loss decreased (1.179579 --> 1.178564).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6062119007110596
Epoch: 14, Steps: 62 | Train Loss: 0.4580696 Vali Loss: 1.1761656 Test Loss: 0.4327382
Validation loss decreased (1.178564 --> 1.176166).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.621830940246582
Epoch: 15, Steps: 62 | Train Loss: 0.4569483 Vali Loss: 1.1705997 Test Loss: 0.4316348
Validation loss decreased (1.176166 --> 1.170600).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.627556324005127
Epoch: 16, Steps: 62 | Train Loss: 0.4561152 Vali Loss: 1.1726329 Test Loss: 0.4309541
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.606494665145874
Epoch: 17, Steps: 62 | Train Loss: 0.4556468 Vali Loss: 1.1669984 Test Loss: 0.4302890
Validation loss decreased (1.170600 --> 1.166998).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6158716678619385
Epoch: 18, Steps: 62 | Train Loss: 0.4552514 Vali Loss: 1.1706549 Test Loss: 0.4298777
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.2697255611419678
Epoch: 19, Steps: 62 | Train Loss: 0.4547062 Vali Loss: 1.1682376 Test Loss: 0.4296151
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5678038597106934
Epoch: 20, Steps: 62 | Train Loss: 0.4545039 Vali Loss: 1.1649326 Test Loss: 0.4293798
Validation loss decreased (1.166998 --> 1.164933).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.7717564105987549
Epoch: 21, Steps: 62 | Train Loss: 0.4540798 Vali Loss: 1.1663773 Test Loss: 0.4291935
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.7110586166381836
Epoch: 22, Steps: 62 | Train Loss: 0.4538688 Vali Loss: 1.1636686 Test Loss: 0.4290711
Validation loss decreased (1.164933 --> 1.163669).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3945844173431396
Epoch: 23, Steps: 62 | Train Loss: 0.4537756 Vali Loss: 1.1645197 Test Loss: 0.4289463
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.956408977508545
Epoch: 24, Steps: 62 | Train Loss: 0.4535117 Vali Loss: 1.1638614 Test Loss: 0.4288541
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.6087398529052734
Epoch: 25, Steps: 62 | Train Loss: 0.4534265 Vali Loss: 1.1685380 Test Loss: 0.4288364
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5119705200195312
Epoch: 26, Steps: 62 | Train Loss: 0.4532606 Vali Loss: 1.1626430 Test Loss: 0.4287731
Validation loss decreased (1.163669 --> 1.162643).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6167628765106201
Epoch: 27, Steps: 62 | Train Loss: 0.4531632 Vali Loss: 1.1609653 Test Loss: 0.4287607
Validation loss decreased (1.162643 --> 1.160965).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.7323992252349854
Epoch: 28, Steps: 62 | Train Loss: 0.4529871 Vali Loss: 1.1623300 Test Loss: 0.4286806
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7466411590576172
Epoch: 29, Steps: 62 | Train Loss: 0.4529548 Vali Loss: 1.1671866 Test Loss: 0.4287208
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.6013474464416504
Epoch: 30, Steps: 62 | Train Loss: 0.4528293 Vali Loss: 1.1646260 Test Loss: 0.4287098
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.0152242183685303
Epoch: 31, Steps: 62 | Train Loss: 0.4528221 Vali Loss: 1.1622044 Test Loss: 0.4285678
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.7842295169830322
Epoch: 32, Steps: 62 | Train Loss: 0.4527262 Vali Loss: 1.1605083 Test Loss: 0.4286427
Validation loss decreased (1.160965 --> 1.160508).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5324718952178955
Epoch: 33, Steps: 62 | Train Loss: 0.4527279 Vali Loss: 1.1628039 Test Loss: 0.4286003
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.538869857788086
Epoch: 34, Steps: 62 | Train Loss: 0.4526591 Vali Loss: 1.1578240 Test Loss: 0.4286074
Validation loss decreased (1.160508 --> 1.157824).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.6463546752929688
Epoch: 35, Steps: 62 | Train Loss: 0.4527027 Vali Loss: 1.1603251 Test Loss: 0.4286131
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6916918754577637
Epoch: 36, Steps: 62 | Train Loss: 0.4525990 Vali Loss: 1.1584324 Test Loss: 0.4286029
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.743760347366333
Epoch: 37, Steps: 62 | Train Loss: 0.4524637 Vali Loss: 1.1623296 Test Loss: 0.4286143
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.633606195449829
Epoch: 38, Steps: 62 | Train Loss: 0.4523417 Vali Loss: 1.1655455 Test Loss: 0.4286176
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7257120609283447
Epoch: 39, Steps: 62 | Train Loss: 0.4524701 Vali Loss: 1.1619227 Test Loss: 0.4285878
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.74745512008667
Epoch: 40, Steps: 62 | Train Loss: 0.4522751 Vali Loss: 1.1622813 Test Loss: 0.4285945
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.944777011871338
Epoch: 41, Steps: 62 | Train Loss: 0.4522350 Vali Loss: 1.1619664 Test Loss: 0.4285949
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.5979747772216797
Epoch: 42, Steps: 62 | Train Loss: 0.4522510 Vali Loss: 1.1596428 Test Loss: 0.4285974
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.846879482269287
Epoch: 43, Steps: 62 | Train Loss: 0.4522057 Vali Loss: 1.1573189 Test Loss: 0.4286020
Validation loss decreased (1.157824 --> 1.157319).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.691652536392212
Epoch: 44, Steps: 62 | Train Loss: 0.4522275 Vali Loss: 1.1599164 Test Loss: 0.4285993
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.6731958389282227
Epoch: 45, Steps: 62 | Train Loss: 0.4522373 Vali Loss: 1.1585478 Test Loss: 0.4285863
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.7144889831542969
Epoch: 46, Steps: 62 | Train Loss: 0.4522506 Vali Loss: 1.1594371 Test Loss: 0.4286132
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.6304707527160645
Epoch: 47, Steps: 62 | Train Loss: 0.4519094 Vali Loss: 1.1589451 Test Loss: 0.4285831
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.5625460147857666
Epoch: 48, Steps: 62 | Train Loss: 0.4521032 Vali Loss: 1.1593789 Test Loss: 0.4285904
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.2113699913024902
Epoch: 49, Steps: 62 | Train Loss: 0.4521079 Vali Loss: 1.1611681 Test Loss: 0.4285840
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6077446937561035
Epoch: 50, Steps: 62 | Train Loss: 0.4520949 Vali Loss: 1.1597558 Test Loss: 0.4285737
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6319890022277832
Epoch: 51, Steps: 62 | Train Loss: 0.4519335 Vali Loss: 1.1612175 Test Loss: 0.4285851
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.221773147583008
Epoch: 52, Steps: 62 | Train Loss: 0.4519827 Vali Loss: 1.1626645 Test Loss: 0.4285902
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6157152652740479
Epoch: 53, Steps: 62 | Train Loss: 0.4520408 Vali Loss: 1.1560575 Test Loss: 0.4285935
Validation loss decreased (1.157319 --> 1.156057).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.4723560810089111
Epoch: 54, Steps: 62 | Train Loss: 0.4521101 Vali Loss: 1.1583309 Test Loss: 0.4286019
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.526738166809082
Epoch: 55, Steps: 62 | Train Loss: 0.4519830 Vali Loss: 1.1576439 Test Loss: 0.4285921
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.4464058876037598
Epoch: 56, Steps: 62 | Train Loss: 0.4520502 Vali Loss: 1.1600370 Test Loss: 0.4285921
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.5748074054718018
Epoch: 57, Steps: 62 | Train Loss: 0.4518613 Vali Loss: 1.1585420 Test Loss: 0.4285772
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.5511915683746338
Epoch: 58, Steps: 62 | Train Loss: 0.4519678 Vali Loss: 1.1580640 Test Loss: 0.4285833
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.9692888259887695
Epoch: 59, Steps: 62 | Train Loss: 0.4517912 Vali Loss: 1.1595634 Test Loss: 0.4285769
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.6143832206726074
Epoch: 60, Steps: 62 | Train Loss: 0.4519310 Vali Loss: 1.1646923 Test Loss: 0.4285876
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.169297695159912
Epoch: 61, Steps: 62 | Train Loss: 0.4519489 Vali Loss: 1.1594673 Test Loss: 0.4285890
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.8957791328430176
Epoch: 62, Steps: 62 | Train Loss: 0.4519819 Vali Loss: 1.1539444 Test Loss: 0.4285839
Validation loss decreased (1.156057 --> 1.153944).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.6917788982391357
Epoch: 63, Steps: 62 | Train Loss: 0.4519883 Vali Loss: 1.1603338 Test Loss: 0.4285939
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.791506052017212
Epoch: 64, Steps: 62 | Train Loss: 0.4519309 Vali Loss: 1.1637425 Test Loss: 0.4285828
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.9375801086425781
Epoch: 65, Steps: 62 | Train Loss: 0.4517649 Vali Loss: 1.1626228 Test Loss: 0.4285867
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.7131073474884033
Epoch: 66, Steps: 62 | Train Loss: 0.4519393 Vali Loss: 1.1603460 Test Loss: 0.4285936
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.740485668182373
Epoch: 67, Steps: 62 | Train Loss: 0.4518716 Vali Loss: 1.1615471 Test Loss: 0.4285854
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.8679792881011963
Epoch: 68, Steps: 62 | Train Loss: 0.4518176 Vali Loss: 1.1572310 Test Loss: 0.4285866
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.8650588989257812
Epoch: 69, Steps: 62 | Train Loss: 0.4519028 Vali Loss: 1.1566153 Test Loss: 0.4285939
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.67466139793396
Epoch: 70, Steps: 62 | Train Loss: 0.4517406 Vali Loss: 1.1574214 Test Loss: 0.4285907
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.804673671722412
Epoch: 71, Steps: 62 | Train Loss: 0.4516509 Vali Loss: 1.1568021 Test Loss: 0.4285910
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.5312998294830322
Epoch: 72, Steps: 62 | Train Loss: 0.4518068 Vali Loss: 1.1592678 Test Loss: 0.4285948
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.6219446659088135
Epoch: 73, Steps: 62 | Train Loss: 0.4518565 Vali Loss: 1.1585252 Test Loss: 0.4286026
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.804168462753296
Epoch: 74, Steps: 62 | Train Loss: 0.4518433 Vali Loss: 1.1611103 Test Loss: 0.4285998
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.688215970993042
Epoch: 75, Steps: 62 | Train Loss: 0.4517693 Vali Loss: 1.1619623 Test Loss: 0.4285957
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.8698019981384277
Epoch: 76, Steps: 62 | Train Loss: 0.4519634 Vali Loss: 1.1586044 Test Loss: 0.4285885
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.670698642730713
Epoch: 77, Steps: 62 | Train Loss: 0.4516799 Vali Loss: 1.1559316 Test Loss: 0.4285904
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.587339162826538
Epoch: 78, Steps: 62 | Train Loss: 0.4517661 Vali Loss: 1.1589880 Test Loss: 0.4285995
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.5873150825500488
Epoch: 79, Steps: 62 | Train Loss: 0.4516995 Vali Loss: 1.1605172 Test Loss: 0.4285945
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.6431214809417725
Epoch: 80, Steps: 62 | Train Loss: 0.4517546 Vali Loss: 1.1568741 Test Loss: 0.4285947
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.69917631149292
Epoch: 81, Steps: 62 | Train Loss: 0.4517043 Vali Loss: 1.1613917 Test Loss: 0.4285990
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.6122682094573975
Epoch: 82, Steps: 62 | Train Loss: 0.4516148 Vali Loss: 1.1578304 Test Loss: 0.4285991
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.42744046449661255, mae:0.4270782470703125, rse:0.6224290728569031, corr:[0.25063524 0.2572021  0.25711176 0.26004568 0.2567912  0.2539708
 0.25472945 0.2548202  0.25334337 0.25341123 0.25399926 0.25327924
 0.25272444 0.25281155 0.25246164 0.25205082 0.25208578 0.25190884
 0.25167814 0.25168848 0.25160548 0.25146565 0.2521365  0.2527679
 0.25220963 0.25183785 0.25208202 0.25165874 0.25090075 0.25095353
 0.25103304 0.25022948 0.24960092 0.24987073 0.24998476 0.24957564
 0.24958253 0.24989837 0.24988693 0.24985896 0.25018114 0.2503212
 0.25026703 0.25026682 0.2500955  0.24981833 0.24996358 0.25024977
 0.24991925 0.24917954 0.24850976 0.24793503 0.24730869 0.24658893
 0.24601    0.24548623 0.24517322 0.24511014 0.24488601 0.24465956
 0.24445932 0.24439983 0.24424529 0.24418597 0.24439712 0.24476986
 0.2451091  0.2450264  0.24487278 0.24499609 0.24507976 0.24475938
 0.24415652 0.2435715  0.24302332 0.24262165 0.24253866 0.24249265
 0.24202487 0.24139452 0.24118118 0.24107984 0.24074624 0.24045907
 0.24030669 0.24014552 0.23999959 0.24002288 0.24002364 0.2399173
 0.23974895 0.23951681 0.23922296 0.23912366 0.23930494 0.23996648
 0.24077144 0.24117915 0.24134384 0.24136792 0.24150203 0.24154247
 0.24127315 0.24112172 0.24115086 0.24102071 0.24066643 0.24042688
 0.24034154 0.24027628 0.2403233  0.24057719 0.24081564 0.24085787
 0.24081063 0.24064133 0.24033658 0.24001077 0.23969501 0.239635
 0.23974241 0.23945002 0.23880553 0.23824386 0.2380197  0.2376584
 0.23724379 0.2371697  0.23708223 0.23673457 0.23643255 0.23637065
 0.23633088 0.23625201 0.23626654 0.23635295 0.23650925 0.2366646
 0.23684564 0.23686239 0.23675473 0.23669665 0.23644546 0.23623341
 0.23624301 0.23607571 0.23571146 0.2352194  0.23475794 0.23422866
 0.23391904 0.23397182 0.23410738 0.2341928  0.23418169 0.23420374
 0.23416843 0.23416898 0.23418453 0.23406978 0.2338683  0.23387058
 0.23388335 0.23372562 0.23354048 0.2333353  0.2329121  0.23285617
 0.23322725 0.23347545 0.23379254 0.23436856 0.23458974 0.23427323
 0.23417754 0.23449902 0.23455831 0.23439884 0.23444408 0.23449075
 0.234201   0.2340182  0.23419607 0.23420902 0.23412302 0.23448965
 0.23477565 0.23462902 0.23460756 0.23459612 0.23411337 0.23373733
 0.23371719 0.23341781 0.23266575 0.23223379 0.23192728 0.23118694
 0.23064943 0.23092291 0.23105548 0.23071338 0.23052004 0.23060964
 0.23048185 0.23053929 0.23088588 0.23095162 0.23084009 0.230999
 0.23115985 0.230804   0.23047686 0.23027842 0.22973089 0.22963183
 0.23020533 0.23014022 0.2297319  0.22964679 0.22970031 0.2292774
 0.22884181 0.22911504 0.22935054 0.2288482  0.22835983 0.22828844
 0.22794542 0.22775875 0.22816375 0.22823335 0.22794537 0.2280234
 0.22823603 0.22794698 0.22772007 0.22777253 0.22764465 0.2275497
 0.22795266 0.22784267 0.22746176 0.22770442 0.22800295 0.22762704
 0.22743303 0.22803594 0.22834787 0.22785126 0.22753797 0.22747928
 0.2270799  0.22691362 0.22739618 0.22755615 0.22746795 0.2278343
 0.22816578 0.22788696 0.22784103 0.22808406 0.22796945 0.22795361
 0.22814642 0.22774936 0.22704135 0.22695489 0.2267632  0.22596449
 0.22554895 0.22579966 0.22562586 0.2250244  0.2249274  0.2249358
 0.2246107  0.22459188 0.22489412 0.22470127 0.22438543 0.22460753
 0.22481206 0.22446498 0.22455886 0.22492014 0.22482772 0.22506715
 0.22591308 0.22609507 0.22602203 0.22641127 0.22651951 0.22601771
 0.22598471 0.22649476 0.22637354 0.22598834 0.22606179 0.22600965
 0.22551993 0.22557129 0.22612487 0.22582376 0.22572505 0.22620563
 0.22623384 0.22590716 0.22640865 0.22654828 0.22574215 0.22578356
 0.22660282 0.22636865 0.22583044 0.22626615 0.22588897 0.22457112
 0.2243073  0.22454363 0.22379106 0.22356553 0.22372676 0.2230174
 0.22273134 0.22370096 0.22342943 0.22241703 0.22341126 0.2241043
 0.22235914 0.22262058 0.22372065 0.21985814 0.22116488 0.22459695]
