Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=72, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=72, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9289728.0
params:  10512.0
Trainable parameters:  10512
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.630399942398071
Epoch: 1, Steps: 56 | Train Loss: 0.8440920 Vali Loss: 2.2022905 Test Loss: 0.9342545
Validation loss decreased (inf --> 2.202291).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.3460915088653564
Epoch: 2, Steps: 56 | Train Loss: 0.6905555 Vali Loss: 1.9726017 Test Loss: 0.8101211
Validation loss decreased (2.202291 --> 1.972602).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.416596412658691
Epoch: 3, Steps: 56 | Train Loss: 0.6037437 Vali Loss: 1.8509336 Test Loss: 0.7440984
Validation loss decreased (1.972602 --> 1.850934).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.387851715087891
Epoch: 4, Steps: 56 | Train Loss: 0.5522847 Vali Loss: 1.7849524 Test Loss: 0.7050197
Validation loss decreased (1.850934 --> 1.784952).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.521816968917847
Epoch: 5, Steps: 56 | Train Loss: 0.5192061 Vali Loss: 1.7493389 Test Loss: 0.6813817
Validation loss decreased (1.784952 --> 1.749339).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.384369134902954
Epoch: 6, Steps: 56 | Train Loss: 0.4966607 Vali Loss: 1.7215043 Test Loss: 0.6639959
Validation loss decreased (1.749339 --> 1.721504).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.226837396621704
Epoch: 7, Steps: 56 | Train Loss: 0.4793608 Vali Loss: 1.6968383 Test Loss: 0.6511986
Validation loss decreased (1.721504 --> 1.696838).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.559728145599365
Epoch: 8, Steps: 56 | Train Loss: 0.4657067 Vali Loss: 1.6734627 Test Loss: 0.6399871
Validation loss decreased (1.696838 --> 1.673463).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.660492897033691
Epoch: 9, Steps: 56 | Train Loss: 0.4541287 Vali Loss: 1.6680923 Test Loss: 0.6302409
Validation loss decreased (1.673463 --> 1.668092).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.7467474937438965
Epoch: 10, Steps: 56 | Train Loss: 0.4446284 Vali Loss: 1.6535814 Test Loss: 0.6222349
Validation loss decreased (1.668092 --> 1.653581).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.654652118682861
Epoch: 11, Steps: 56 | Train Loss: 0.4360000 Vali Loss: 1.6441154 Test Loss: 0.6146598
Validation loss decreased (1.653581 --> 1.644115).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.264520883560181
Epoch: 12, Steps: 56 | Train Loss: 0.4284008 Vali Loss: 1.6287298 Test Loss: 0.6081872
Validation loss decreased (1.644115 --> 1.628730).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.355849027633667
Epoch: 13, Steps: 56 | Train Loss: 0.4217018 Vali Loss: 1.6249740 Test Loss: 0.6015921
Validation loss decreased (1.628730 --> 1.624974).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.557325124740601
Epoch: 14, Steps: 56 | Train Loss: 0.4158590 Vali Loss: 1.6175113 Test Loss: 0.5955650
Validation loss decreased (1.624974 --> 1.617511).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.363986015319824
Epoch: 15, Steps: 56 | Train Loss: 0.4105097 Vali Loss: 1.6073221 Test Loss: 0.5902981
Validation loss decreased (1.617511 --> 1.607322).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.35847544670105
Epoch: 16, Steps: 56 | Train Loss: 0.4056600 Vali Loss: 1.6005799 Test Loss: 0.5850312
Validation loss decreased (1.607322 --> 1.600580).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.302540302276611
Epoch: 17, Steps: 56 | Train Loss: 0.4011202 Vali Loss: 1.5975186 Test Loss: 0.5803106
Validation loss decreased (1.600580 --> 1.597519).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.416132688522339
Epoch: 18, Steps: 56 | Train Loss: 0.3973249 Vali Loss: 1.5903127 Test Loss: 0.5760859
Validation loss decreased (1.597519 --> 1.590313).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.531222581863403
Epoch: 19, Steps: 56 | Train Loss: 0.3936686 Vali Loss: 1.5883269 Test Loss: 0.5717528
Validation loss decreased (1.590313 --> 1.588327).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.410752773284912
Epoch: 20, Steps: 56 | Train Loss: 0.3904923 Vali Loss: 1.5802584 Test Loss: 0.5680834
Validation loss decreased (1.588327 --> 1.580258).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.690158367156982
Epoch: 21, Steps: 56 | Train Loss: 0.3874346 Vali Loss: 1.5744771 Test Loss: 0.5644121
Validation loss decreased (1.580258 --> 1.574477).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.3600218296051025
Epoch: 22, Steps: 56 | Train Loss: 0.3846277 Vali Loss: 1.5741724 Test Loss: 0.5610718
Validation loss decreased (1.574477 --> 1.574172).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.276016712188721
Epoch: 23, Steps: 56 | Train Loss: 0.3820946 Vali Loss: 1.5689044 Test Loss: 0.5579675
Validation loss decreased (1.574172 --> 1.568904).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.8996126651763916
Epoch: 24, Steps: 56 | Train Loss: 0.3797677 Vali Loss: 1.5642073 Test Loss: 0.5552878
Validation loss decreased (1.568904 --> 1.564207).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.139780759811401
Epoch: 25, Steps: 56 | Train Loss: 0.3774067 Vali Loss: 1.5572211 Test Loss: 0.5525969
Validation loss decreased (1.564207 --> 1.557221).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.132350206375122
Epoch: 26, Steps: 56 | Train Loss: 0.3754623 Vali Loss: 1.5584121 Test Loss: 0.5499244
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.935685634613037
Epoch: 27, Steps: 56 | Train Loss: 0.3737683 Vali Loss: 1.5518494 Test Loss: 0.5476686
Validation loss decreased (1.557221 --> 1.551849).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.297821044921875
Epoch: 28, Steps: 56 | Train Loss: 0.3719080 Vali Loss: 1.5557446 Test Loss: 0.5455328
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.564289093017578
Epoch: 29, Steps: 56 | Train Loss: 0.3703064 Vali Loss: 1.5523771 Test Loss: 0.5433071
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.528487205505371
Epoch: 30, Steps: 56 | Train Loss: 0.3688530 Vali Loss: 1.5457160 Test Loss: 0.5413280
Validation loss decreased (1.551849 --> 1.545716).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.106736421585083
Epoch: 31, Steps: 56 | Train Loss: 0.3675602 Vali Loss: 1.5432386 Test Loss: 0.5394621
Validation loss decreased (1.545716 --> 1.543239).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.956122875213623
Epoch: 32, Steps: 56 | Train Loss: 0.3662744 Vali Loss: 1.5445671 Test Loss: 0.5377490
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.3763182163238525
Epoch: 33, Steps: 56 | Train Loss: 0.3651017 Vali Loss: 1.5372732 Test Loss: 0.5361318
Validation loss decreased (1.543239 --> 1.537273).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.854681968688965
Epoch: 34, Steps: 56 | Train Loss: 0.3637968 Vali Loss: 1.5404462 Test Loss: 0.5346129
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.492669582366943
Epoch: 35, Steps: 56 | Train Loss: 0.3627050 Vali Loss: 1.5354195 Test Loss: 0.5331993
Validation loss decreased (1.537273 --> 1.535419).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.424272060394287
Epoch: 36, Steps: 56 | Train Loss: 0.3618136 Vali Loss: 1.5315921 Test Loss: 0.5318043
Validation loss decreased (1.535419 --> 1.531592).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.42228102684021
Epoch: 37, Steps: 56 | Train Loss: 0.3607910 Vali Loss: 1.5355217 Test Loss: 0.5304683
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.316832780838013
Epoch: 38, Steps: 56 | Train Loss: 0.3600165 Vali Loss: 1.5297972 Test Loss: 0.5292702
Validation loss decreased (1.531592 --> 1.529797).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.459728002548218
Epoch: 39, Steps: 56 | Train Loss: 0.3591280 Vali Loss: 1.5295708 Test Loss: 0.5281321
Validation loss decreased (1.529797 --> 1.529571).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.568150520324707
Epoch: 40, Steps: 56 | Train Loss: 0.3583811 Vali Loss: 1.5328045 Test Loss: 0.5271228
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.9164719581604
Epoch: 41, Steps: 56 | Train Loss: 0.3578029 Vali Loss: 1.5278652 Test Loss: 0.5260882
Validation loss decreased (1.529571 --> 1.527865).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.343808650970459
Epoch: 42, Steps: 56 | Train Loss: 0.3571367 Vali Loss: 1.5250365 Test Loss: 0.5251417
Validation loss decreased (1.527865 --> 1.525036).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.615283966064453
Epoch: 43, Steps: 56 | Train Loss: 0.3565142 Vali Loss: 1.5239398 Test Loss: 0.5242862
Validation loss decreased (1.525036 --> 1.523940).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.213099479675293
Epoch: 44, Steps: 56 | Train Loss: 0.3558782 Vali Loss: 1.5238109 Test Loss: 0.5233516
Validation loss decreased (1.523940 --> 1.523811).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.076176166534424
Epoch: 45, Steps: 56 | Train Loss: 0.3552856 Vali Loss: 1.5229647 Test Loss: 0.5225198
Validation loss decreased (1.523811 --> 1.522965).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.469070196151733
Epoch: 46, Steps: 56 | Train Loss: 0.3546783 Vali Loss: 1.5259297 Test Loss: 0.5217391
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.428661108016968
Epoch: 47, Steps: 56 | Train Loss: 0.3541240 Vali Loss: 1.5196061 Test Loss: 0.5210656
Validation loss decreased (1.522965 --> 1.519606).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.4312145709991455
Epoch: 48, Steps: 56 | Train Loss: 0.3537399 Vali Loss: 1.5258799 Test Loss: 0.5203109
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 4.579224586486816
Epoch: 49, Steps: 56 | Train Loss: 0.3532666 Vali Loss: 1.5156277 Test Loss: 0.5196705
Validation loss decreased (1.519606 --> 1.515628).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.67422890663147
Epoch: 50, Steps: 56 | Train Loss: 0.3527488 Vali Loss: 1.5148776 Test Loss: 0.5190381
Validation loss decreased (1.515628 --> 1.514878).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.623004674911499
Epoch: 51, Steps: 56 | Train Loss: 0.3524926 Vali Loss: 1.5175530 Test Loss: 0.5184461
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.48962926864624
Epoch: 52, Steps: 56 | Train Loss: 0.3519875 Vali Loss: 1.5142676 Test Loss: 0.5178903
Validation loss decreased (1.514878 --> 1.514268).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.411022663116455
Epoch: 53, Steps: 56 | Train Loss: 0.3517477 Vali Loss: 1.5143864 Test Loss: 0.5173632
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 4.1295859813690186
Epoch: 54, Steps: 56 | Train Loss: 0.3512707 Vali Loss: 1.5113088 Test Loss: 0.5168721
Validation loss decreased (1.514268 --> 1.511309).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.573211669921875
Epoch: 55, Steps: 56 | Train Loss: 0.3509799 Vali Loss: 1.5126727 Test Loss: 0.5163529
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 4.129210710525513
Epoch: 56, Steps: 56 | Train Loss: 0.3505345 Vali Loss: 1.5144678 Test Loss: 0.5159495
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.052086353302002
Epoch: 57, Steps: 56 | Train Loss: 0.3502538 Vali Loss: 1.5180783 Test Loss: 0.5154688
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 4.434974908828735
Epoch: 58, Steps: 56 | Train Loss: 0.3500446 Vali Loss: 1.5091338 Test Loss: 0.5150802
Validation loss decreased (1.511309 --> 1.509134).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.616483211517334
Epoch: 59, Steps: 56 | Train Loss: 0.3498307 Vali Loss: 1.5124447 Test Loss: 0.5147067
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 4.297995328903198
Epoch: 60, Steps: 56 | Train Loss: 0.3494009 Vali Loss: 1.5079980 Test Loss: 0.5143319
Validation loss decreased (1.509134 --> 1.507998).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 4.695100545883179
Epoch: 61, Steps: 56 | Train Loss: 0.3493439 Vali Loss: 1.5090344 Test Loss: 0.5140085
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 4.560404539108276
Epoch: 62, Steps: 56 | Train Loss: 0.3491246 Vali Loss: 1.5083350 Test Loss: 0.5136414
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 4.437815189361572
Epoch: 63, Steps: 56 | Train Loss: 0.3488997 Vali Loss: 1.5090907 Test Loss: 0.5133412
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 4.661614894866943
Epoch: 64, Steps: 56 | Train Loss: 0.3486400 Vali Loss: 1.5112116 Test Loss: 0.5130312
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 4.641184329986572
Epoch: 65, Steps: 56 | Train Loss: 0.3483756 Vali Loss: 1.5090959 Test Loss: 0.5127305
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.9623894691467285
Epoch: 66, Steps: 56 | Train Loss: 0.3484078 Vali Loss: 1.5120778 Test Loss: 0.5124478
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 4.311860799789429
Epoch: 67, Steps: 56 | Train Loss: 0.3478140 Vali Loss: 1.5089327 Test Loss: 0.5122423
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 3.827394962310791
Epoch: 68, Steps: 56 | Train Loss: 0.3479661 Vali Loss: 1.5072047 Test Loss: 0.5119940
Validation loss decreased (1.507998 --> 1.507205).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.679598569869995
Epoch: 69, Steps: 56 | Train Loss: 0.3476776 Vali Loss: 1.5113797 Test Loss: 0.5117767
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.0169899463653564
Epoch: 70, Steps: 56 | Train Loss: 0.3476490 Vali Loss: 1.5085919 Test Loss: 0.5114980
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.9417967796325684
Epoch: 71, Steps: 56 | Train Loss: 0.3473959 Vali Loss: 1.5119402 Test Loss: 0.5113077
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 4.028077840805054
Epoch: 72, Steps: 56 | Train Loss: 0.3472821 Vali Loss: 1.5094057 Test Loss: 0.5111185
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.7981784343719482
Epoch: 73, Steps: 56 | Train Loss: 0.3472076 Vali Loss: 1.5061474 Test Loss: 0.5109210
Validation loss decreased (1.507205 --> 1.506147).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.920525550842285
Epoch: 74, Steps: 56 | Train Loss: 0.3470990 Vali Loss: 1.5061755 Test Loss: 0.5107365
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 4.01951789855957
Epoch: 75, Steps: 56 | Train Loss: 0.3469462 Vali Loss: 1.5096248 Test Loss: 0.5105796
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.89418888092041
Epoch: 76, Steps: 56 | Train Loss: 0.3469464 Vali Loss: 1.5022705 Test Loss: 0.5104198
Validation loss decreased (1.506147 --> 1.502270).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.7559003829956055
Epoch: 77, Steps: 56 | Train Loss: 0.3467249 Vali Loss: 1.5054622 Test Loss: 0.5102547
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.8284058570861816
Epoch: 78, Steps: 56 | Train Loss: 0.3466788 Vali Loss: 1.5079902 Test Loss: 0.5101159
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 4.05232572555542
Epoch: 79, Steps: 56 | Train Loss: 0.3464714 Vali Loss: 1.5113304 Test Loss: 0.5099650
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 3.713120937347412
Epoch: 80, Steps: 56 | Train Loss: 0.3465331 Vali Loss: 1.5047265 Test Loss: 0.5098376
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.878420352935791
Epoch: 81, Steps: 56 | Train Loss: 0.3462967 Vali Loss: 1.5083506 Test Loss: 0.5097063
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.7796506881713867
Epoch: 82, Steps: 56 | Train Loss: 0.3462638 Vali Loss: 1.5097975 Test Loss: 0.5095934
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 3.9483444690704346
Epoch: 83, Steps: 56 | Train Loss: 0.3461939 Vali Loss: 1.5000507 Test Loss: 0.5094690
Validation loss decreased (1.502270 --> 1.500051).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.751270055770874
Epoch: 84, Steps: 56 | Train Loss: 0.3460852 Vali Loss: 1.5087624 Test Loss: 0.5093843
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.774040460586548
Epoch: 85, Steps: 56 | Train Loss: 0.3460695 Vali Loss: 1.5017941 Test Loss: 0.5092641
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 3.865243911743164
Epoch: 86, Steps: 56 | Train Loss: 0.3459630 Vali Loss: 1.5114553 Test Loss: 0.5091783
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.7244629859924316
Epoch: 87, Steps: 56 | Train Loss: 0.3461542 Vali Loss: 1.5036526 Test Loss: 0.5090603
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 4.1100475788116455
Epoch: 88, Steps: 56 | Train Loss: 0.3459452 Vali Loss: 1.5090237 Test Loss: 0.5089865
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 4.018650054931641
Epoch: 89, Steps: 56 | Train Loss: 0.3457352 Vali Loss: 1.5072659 Test Loss: 0.5088954
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.946192502975464
Epoch: 90, Steps: 56 | Train Loss: 0.3458027 Vali Loss: 1.5057235 Test Loss: 0.5088167
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 3.7014739513397217
Epoch: 91, Steps: 56 | Train Loss: 0.3457299 Vali Loss: 1.5041847 Test Loss: 0.5087360
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 3.596501588821411
Epoch: 92, Steps: 56 | Train Loss: 0.3457170 Vali Loss: 1.5088243 Test Loss: 0.5086669
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 3.7858023643493652
Epoch: 93, Steps: 56 | Train Loss: 0.3457210 Vali Loss: 1.5020118 Test Loss: 0.5085884
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 3.8340702056884766
Epoch: 94, Steps: 56 | Train Loss: 0.3457008 Vali Loss: 1.4985613 Test Loss: 0.5085289
Validation loss decreased (1.500051 --> 1.498561).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 3.972303867340088
Epoch: 95, Steps: 56 | Train Loss: 0.3454938 Vali Loss: 1.5071853 Test Loss: 0.5084640
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 3.8460874557495117
Epoch: 96, Steps: 56 | Train Loss: 0.3453444 Vali Loss: 1.5078139 Test Loss: 0.5084094
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 3.798729658126831
Epoch: 97, Steps: 56 | Train Loss: 0.3455789 Vali Loss: 1.4981692 Test Loss: 0.5083537
Validation loss decreased (1.498561 --> 1.498169).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 3.9358396530151367
Epoch: 98, Steps: 56 | Train Loss: 0.3453778 Vali Loss: 1.5069300 Test Loss: 0.5082963
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 3.8816006183624268
Epoch: 99, Steps: 56 | Train Loss: 0.3454128 Vali Loss: 1.5038612 Test Loss: 0.5082440
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 3.934628963470459
Epoch: 100, Steps: 56 | Train Loss: 0.3453614 Vali Loss: 1.5014477 Test Loss: 0.5082003
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=72, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9289728.0
params:  10512.0
Trainable parameters:  10512
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.81127667427063
Epoch: 1, Steps: 56 | Train Loss: 0.5895073 Vali Loss: 1.4775523 Test Loss: 0.4829129
Validation loss decreased (inf --> 1.477552).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.8348441123962402
Epoch: 2, Steps: 56 | Train Loss: 0.5780785 Vali Loss: 1.4583547 Test Loss: 0.4676484
Validation loss decreased (1.477552 --> 1.458355).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.9889063835144043
Epoch: 3, Steps: 56 | Train Loss: 0.5709344 Vali Loss: 1.4506347 Test Loss: 0.4595121
Validation loss decreased (1.458355 --> 1.450635).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.9478859901428223
Epoch: 4, Steps: 56 | Train Loss: 0.5670426 Vali Loss: 1.4475126 Test Loss: 0.4551104
Validation loss decreased (1.450635 --> 1.447513).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.0125346183776855
Epoch: 5, Steps: 56 | Train Loss: 0.5647540 Vali Loss: 1.4414907 Test Loss: 0.4533739
Validation loss decreased (1.447513 --> 1.441491).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.886561155319214
Epoch: 6, Steps: 56 | Train Loss: 0.5632971 Vali Loss: 1.4429765 Test Loss: 0.4523258
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.115901231765747
Epoch: 7, Steps: 56 | Train Loss: 0.5623531 Vali Loss: 1.4373612 Test Loss: 0.4520952
Validation loss decreased (1.441491 --> 1.437361).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.88790225982666
Epoch: 8, Steps: 56 | Train Loss: 0.5615454 Vali Loss: 1.4405111 Test Loss: 0.4522659
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.9761881828308105
Epoch: 9, Steps: 56 | Train Loss: 0.5610339 Vali Loss: 1.4453126 Test Loss: 0.4522629
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.808206558227539
Epoch: 10, Steps: 56 | Train Loss: 0.5606948 Vali Loss: 1.4433281 Test Loss: 0.4526123
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.833595037460327
Epoch: 11, Steps: 56 | Train Loss: 0.5601920 Vali Loss: 1.4441936 Test Loss: 0.4526174
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.875056028366089
Epoch: 12, Steps: 56 | Train Loss: 0.5601637 Vali Loss: 1.4500846 Test Loss: 0.4528334
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.773331642150879
Epoch: 13, Steps: 56 | Train Loss: 0.5605154 Vali Loss: 1.4448110 Test Loss: 0.4529800
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.917970657348633
Epoch: 14, Steps: 56 | Train Loss: 0.5604813 Vali Loss: 1.4448838 Test Loss: 0.4532478
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.9773707389831543
Epoch: 15, Steps: 56 | Train Loss: 0.5599533 Vali Loss: 1.4440157 Test Loss: 0.4533514
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.819847583770752
Epoch: 16, Steps: 56 | Train Loss: 0.5598609 Vali Loss: 1.4464593 Test Loss: 0.4533717
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.9357566833496094
Epoch: 17, Steps: 56 | Train Loss: 0.5596973 Vali Loss: 1.4457948 Test Loss: 0.4535370
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.9234135150909424
Epoch: 18, Steps: 56 | Train Loss: 0.5601582 Vali Loss: 1.4455749 Test Loss: 0.4535697
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.782914400100708
Epoch: 19, Steps: 56 | Train Loss: 0.5595859 Vali Loss: 1.4421301 Test Loss: 0.4536791
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.006139039993286
Epoch: 20, Steps: 56 | Train Loss: 0.5596092 Vali Loss: 1.4466035 Test Loss: 0.4537875
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.8299336433410645
Epoch: 21, Steps: 56 | Train Loss: 0.5596008 Vali Loss: 1.4447147 Test Loss: 0.4538548
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.765918731689453
Epoch: 22, Steps: 56 | Train Loss: 0.5592369 Vali Loss: 1.4451042 Test Loss: 0.4539597
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.752166509628296
Epoch: 23, Steps: 56 | Train Loss: 0.5593884 Vali Loss: 1.4439807 Test Loss: 0.4540323
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.8919804096221924
Epoch: 24, Steps: 56 | Train Loss: 0.5596916 Vali Loss: 1.4455835 Test Loss: 0.4540475
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.802482843399048
Epoch: 25, Steps: 56 | Train Loss: 0.5593778 Vali Loss: 1.4422593 Test Loss: 0.4540885
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.885648488998413
Epoch: 26, Steps: 56 | Train Loss: 0.5589401 Vali Loss: 1.4469376 Test Loss: 0.4541675
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.0584211349487305
Epoch: 27, Steps: 56 | Train Loss: 0.5592773 Vali Loss: 1.4387525 Test Loss: 0.4542066
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.45124727487564087, mae:0.4726684093475342, rse:0.6430715322494507, corr:[0.2189648  0.22716731 0.23127244 0.23201242 0.23106983 0.22959173
 0.22838756 0.22765367 0.22703609 0.22661123 0.22630326 0.22602254
 0.22583342 0.22562692 0.22538808 0.22517322 0.22506349 0.2250652
 0.22518052 0.22544864 0.22588651 0.22652994 0.22698343 0.22724964
 0.22726683 0.22714816 0.2269545  0.22667627 0.22640498 0.22619897
 0.22616531 0.22616929 0.22614473 0.22602752 0.22575137 0.2254106
 0.22523008 0.22505371 0.2248792  0.22470102 0.22459759 0.22444765
 0.22438829 0.22449774 0.2247934  0.22518325 0.2255977  0.22593266
 0.22582814 0.22555614 0.2250865  0.22451177 0.22397555 0.22330812
 0.22253755 0.22169203 0.22107746 0.2205694  0.2200679  0.21965465
 0.21942523 0.21937439 0.21937974 0.21948197 0.21953931 0.21950088
 0.219423   0.21929643 0.21917707 0.21915706 0.2191539  0.21927053
 0.21915309 0.21884403 0.21847636 0.21811625 0.21776643 0.21732803
 0.21692827 0.2166888  0.21654926 0.21644288 0.21626332 0.21594894
 0.21562208 0.21525502 0.21486062 0.2144912  0.2141463  0.2139238
 0.21385194 0.21405658 0.21449962 0.21519272 0.21610004 0.21706514
 0.21801507 0.21884029 0.21931805 0.21954618 0.21950655 0.21932398
 0.21907385 0.21881819 0.21855913 0.21825106 0.21787494 0.21750519
 0.21734148 0.21733743 0.21735412 0.21738525 0.21739122 0.21726519
 0.21717173 0.21712093 0.21709734 0.21708237 0.2170532  0.21704797
 0.2170142  0.21685398 0.21650118 0.21616577 0.21577114 0.21535002
 0.2149907  0.21481256 0.2146573  0.21450785 0.21439831 0.21431741
 0.21428889 0.21419051 0.21406457 0.21394466 0.21382476 0.21365619
 0.21351047 0.21339437 0.21331467 0.21321069 0.21303803 0.21277237
 0.21243663 0.21197432 0.21144144 0.2108629  0.21049213 0.21033542
 0.21029745 0.21041843 0.21054067 0.21054941 0.21040508 0.21017696
 0.20993961 0.20974384 0.20968676 0.20978372 0.20986138 0.20999771
 0.21015286 0.21032919 0.21050657 0.21066973 0.21082407 0.21102954
 0.2113172  0.21159595 0.2116859  0.21166305 0.2114531  0.2111151
 0.21073505 0.21046858 0.21020992 0.20998162 0.20977825 0.20961884
 0.20952027 0.20950209 0.20960113 0.20981357 0.21012874 0.21049993
 0.21086559 0.21114376 0.21135084 0.21143848 0.2113399  0.21106185
 0.21068601 0.21029443 0.20986468 0.20941919 0.20903666 0.20873997
 0.20848604 0.20840801 0.20840451 0.20838268 0.20832872 0.20830825
 0.20831873 0.2084314  0.20860825 0.20874019 0.2088196  0.20881367
 0.20872892 0.20847806 0.2082047  0.20791753 0.20763111 0.20739041
 0.20718445 0.20708731 0.20697635 0.20684126 0.20678195 0.20668909
 0.20660676 0.20648876 0.20630677 0.20604715 0.20572147 0.20540772
 0.2051958  0.20499867 0.20486453 0.2047694  0.20457166 0.2043255
 0.2041139  0.20394826 0.20391616 0.20401536 0.20430307 0.20468393
 0.20510432 0.20545118 0.20562114 0.20564683 0.20547786 0.20510526
 0.20476861 0.20453443 0.20437379 0.20421222 0.20407693 0.20404102
 0.2041473  0.20424879 0.20434195 0.20438442 0.2044177  0.2044487
 0.20453359 0.2045345  0.20460266 0.20472185 0.20480752 0.20479448
 0.20468965 0.20455062 0.20440866 0.20428129 0.20407002 0.20384523
 0.20369048 0.20360403 0.20339584 0.20311221 0.20276582 0.2024357
 0.20217073 0.20205301 0.20201047 0.20204006 0.20209639 0.20223083
 0.20235531 0.20249826 0.2027133  0.20301935 0.20350961 0.20409927
 0.20468374 0.20521048 0.20553745 0.2056857  0.20561466 0.20539469
 0.20516708 0.20503314 0.20495574 0.20493877 0.20501341 0.20518148
 0.20541535 0.20558271 0.20567995 0.20570928 0.20558336 0.2053492
 0.20522556 0.20517458 0.20529327 0.20561075 0.20595239 0.20626022
 0.20647304 0.2064327  0.20593616 0.2051099  0.20417285 0.20332956
 0.2027592  0.20252363 0.20251425 0.20254748 0.20254567 0.20251219
 0.20245452 0.20235887 0.20219257 0.20219894 0.20221975 0.20235613
 0.20263155 0.20292734 0.20314983 0.20324486 0.20317522 0.2029118
 0.20254926 0.20214055 0.20165136 0.20124996 0.20101862 0.20087785
 0.20084615 0.20081499 0.20067143 0.20050475 0.20039052 0.20047899
 0.20069316 0.20107746 0.20159861 0.20219845 0.20266281 0.20302987
 0.20320873 0.20313257 0.20287424 0.2025531  0.20218274 0.20180798
 0.2014557  0.20101517 0.2004664  0.19984074 0.19922519 0.19863582
 0.19813451 0.19778773 0.19757476 0.19743037 0.1973406  0.19737191
 0.19739018 0.19734277 0.19721924 0.19697022 0.1966888  0.19641164
 0.19620623 0.19604628 0.19604827 0.19629632 0.19671857 0.19735534
 0.19802997 0.19847213 0.19855039 0.19834487 0.1980969  0.19778799
 0.19753571 0.19733684 0.19714063 0.19689192 0.19657382 0.19635311
 0.196292   0.19629543 0.19640799 0.19663028 0.19691859 0.19732325
 0.19773349 0.19801556 0.1982037  0.19838598 0.19848232 0.19860713
 0.19878612 0.19890518 0.19885759 0.19870019 0.19850206 0.19817163
 0.1978306  0.19749554 0.19722761 0.197035   0.19700785 0.19716981
 0.19746163 0.1978636  0.19811936 0.1982884  0.19835149 0.19838274
 0.19837138 0.1983572  0.19830762 0.19857828 0.19904473 0.19949214
 0.19986276 0.19997074 0.19973272 0.19940092 0.19909091 0.19881378
 0.1986561  0.19874772 0.19891739 0.19911134 0.1992023  0.19918416
 0.19898684 0.19861843 0.19823457 0.19791876 0.1977048  0.1976456
 0.19780304 0.19805908 0.19844212 0.19893987 0.19937822 0.1997276
 0.19983758 0.19973347 0.19942918 0.19905542 0.19876175 0.19855942
 0.19849567 0.19850951 0.19842608 0.19827579 0.19803756 0.1978508
 0.19774981 0.19777867 0.19795522 0.19826558 0.19861251 0.19891363
 0.19905405 0.19905406 0.19894454 0.19879182 0.1988504  0.19903034
 0.19926895 0.19950734 0.19956754 0.19944675 0.19915023 0.19866061
 0.19799882 0.1973743  0.19679022 0.19639023 0.19626592 0.19640222
 0.19673221 0.19705275 0.19724819 0.19723864 0.197063   0.19671446
 0.19636644 0.19616088 0.19623944 0.19669603 0.1974943  0.19842817
 0.19924593 0.1999082  0.2001981  0.2001099  0.19981016 0.19929215
 0.19869833 0.19821046 0.19782999 0.19753408 0.1974476  0.19764975
 0.19793911 0.19816759 0.19839329 0.1985142  0.19866377 0.19881745
 0.19899294 0.19923732 0.19970083 0.20001122 0.20050283 0.20062245
 0.20070753 0.20050684 0.19996612 0.19939534 0.19869752 0.1979825
 0.19727965 0.19677526 0.19643219 0.19627376 0.19627702 0.19658494
 0.19712362 0.19766347 0.19817808 0.19848493 0.1986618  0.19870928
 0.19869459 0.19866495 0.19863966 0.19864179 0.19862884 0.19847894
 0.19806075 0.19745058 0.19651337 0.1953515  0.19413954 0.19311078
 0.19246234 0.19228318 0.19224674 0.19226462 0.19223788 0.19220461
 0.19199632 0.19145818 0.19095232 0.19044474 0.19025701 0.19024344
 0.19044046 0.19053353 0.19061306 0.19058268 0.19049741 0.19024475
 0.18990082 0.18962488 0.189216   0.18886767 0.18858272 0.18825269
 0.18807445 0.18795384 0.18785231 0.18759023 0.18731387 0.18708543
 0.1870464  0.18705218 0.18722878 0.18747121 0.18768756 0.18772922
 0.18760097 0.18724594 0.18676455 0.18636219 0.18608987 0.18591566
 0.1858091  0.18562774 0.18516347 0.18434702 0.18325841 0.1819764
 0.18076192 0.17991038 0.17941685 0.17927295 0.17935808 0.17964141
 0.18005098 0.18038847 0.18060075 0.1806296  0.1805411  0.18046519
 0.18057437 0.18081674 0.1811554  0.18146746 0.18171838 0.18170108
 0.18134862 0.18078057 0.17999633 0.17930195 0.1787813  0.17837301
 0.17816073 0.1781335  0.17807958 0.17791764 0.17766711 0.17746295
 0.17734681 0.1771178  0.17692187 0.17684747 0.17680079 0.17671938
 0.17641796 0.17590818 0.17527363 0.17461337 0.17412739 0.17391483
 0.17390853 0.17409606 0.17426145 0.17449093 0.17448284 0.17408702
 0.17330708 0.17228651 0.17126669 0.17021275 0.16927508 0.16864909
 0.16839737 0.16832037 0.16845942 0.16864696 0.16889097 0.16912618
 0.16934367 0.16961086 0.1698513  0.17007188 0.17001623 0.16953084
 0.16832583 0.16653143 0.16421594 0.1617765  0.15973552 0.15816972
 0.15752397 0.15771471 0.15856193 0.15931655 0.15997764 0.16049603
 0.16074806 0.16056323 0.16035737 0.16036594 0.16067736 0.16117935
 0.16162065 0.16181847 0.16163382 0.16104187 0.16019532 0.15945515
 0.15904881 0.15921201 0.15957774 0.1601618  0.16055208 0.16045137
 0.15993509 0.15899992 0.15772644 0.15620863 0.15495215 0.1543729
 0.15470909 0.15540472 0.15613636 0.15635884 0.15597227 0.1550958
 0.15437245 0.1545096  0.15637147 0.15990308 0.16399486 0.16222064]
