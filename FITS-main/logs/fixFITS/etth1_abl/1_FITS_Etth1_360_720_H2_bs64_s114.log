Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4741632.0
params:  5418.0
Trainable parameters:  5418
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.4297873973846436
Epoch: 1, Steps: 59 | Train Loss: 1.0313720 Vali Loss: 2.2232852 Test Loss: 0.9253225
Validation loss decreased (inf --> 2.223285).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.484243631362915
Epoch: 2, Steps: 59 | Train Loss: 0.8291222 Vali Loss: 1.9098594 Test Loss: 0.7293813
Validation loss decreased (2.223285 --> 1.909859).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.4279694557189941
Epoch: 3, Steps: 59 | Train Loss: 0.7348458 Vali Loss: 1.7587136 Test Loss: 0.6341713
Validation loss decreased (1.909859 --> 1.758714).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3722114562988281
Epoch: 4, Steps: 59 | Train Loss: 0.6881380 Vali Loss: 1.6819746 Test Loss: 0.5823225
Validation loss decreased (1.758714 --> 1.681975).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.2839832305908203
Epoch: 5, Steps: 59 | Train Loss: 0.6617848 Vali Loss: 1.6329930 Test Loss: 0.5500962
Validation loss decreased (1.681975 --> 1.632993).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.441352367401123
Epoch: 6, Steps: 59 | Train Loss: 0.6451068 Vali Loss: 1.5954857 Test Loss: 0.5280874
Validation loss decreased (1.632993 --> 1.595486).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.2836487293243408
Epoch: 7, Steps: 59 | Train Loss: 0.6331345 Vali Loss: 1.5761771 Test Loss: 0.5118949
Validation loss decreased (1.595486 --> 1.576177).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.3502662181854248
Epoch: 8, Steps: 59 | Train Loss: 0.6242608 Vali Loss: 1.5508287 Test Loss: 0.4992776
Validation loss decreased (1.576177 --> 1.550829).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4023256301879883
Epoch: 9, Steps: 59 | Train Loss: 0.6172136 Vali Loss: 1.5349147 Test Loss: 0.4893187
Validation loss decreased (1.550829 --> 1.534915).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.3538453578948975
Epoch: 10, Steps: 59 | Train Loss: 0.6118403 Vali Loss: 1.5225453 Test Loss: 0.4813728
Validation loss decreased (1.534915 --> 1.522545).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.409085988998413
Epoch: 11, Steps: 59 | Train Loss: 0.6070665 Vali Loss: 1.5183666 Test Loss: 0.4749226
Validation loss decreased (1.522545 --> 1.518367).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.347163200378418
Epoch: 12, Steps: 59 | Train Loss: 0.6033397 Vali Loss: 1.5025346 Test Loss: 0.4696767
Validation loss decreased (1.518367 --> 1.502535).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.441894769668579
Epoch: 13, Steps: 59 | Train Loss: 0.6000238 Vali Loss: 1.4954736 Test Loss: 0.4653657
Validation loss decreased (1.502535 --> 1.495474).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.4048347473144531
Epoch: 14, Steps: 59 | Train Loss: 0.5975640 Vali Loss: 1.4915574 Test Loss: 0.4619036
Validation loss decreased (1.495474 --> 1.491557).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.3193328380584717
Epoch: 15, Steps: 59 | Train Loss: 0.5954828 Vali Loss: 1.4790548 Test Loss: 0.4590020
Validation loss decreased (1.491557 --> 1.479055).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.2888116836547852
Epoch: 16, Steps: 59 | Train Loss: 0.5936029 Vali Loss: 1.4787509 Test Loss: 0.4566826
Validation loss decreased (1.479055 --> 1.478751).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3866868019104004
Epoch: 17, Steps: 59 | Train Loss: 0.5920290 Vali Loss: 1.4752474 Test Loss: 0.4547686
Validation loss decreased (1.478751 --> 1.475247).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.3080759048461914
Epoch: 18, Steps: 59 | Train Loss: 0.5906405 Vali Loss: 1.4641672 Test Loss: 0.4532173
Validation loss decreased (1.475247 --> 1.464167).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.3237419128417969
Epoch: 19, Steps: 59 | Train Loss: 0.5896718 Vali Loss: 1.4670467 Test Loss: 0.4519012
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3316662311553955
Epoch: 20, Steps: 59 | Train Loss: 0.5885698 Vali Loss: 1.4634370 Test Loss: 0.4508377
Validation loss decreased (1.464167 --> 1.463437).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.361119031906128
Epoch: 21, Steps: 59 | Train Loss: 0.5879491 Vali Loss: 1.4618461 Test Loss: 0.4499486
Validation loss decreased (1.463437 --> 1.461846).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.4452826976776123
Epoch: 22, Steps: 59 | Train Loss: 0.5871182 Vali Loss: 1.4599078 Test Loss: 0.4492592
Validation loss decreased (1.461846 --> 1.459908).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3964755535125732
Epoch: 23, Steps: 59 | Train Loss: 0.5866046 Vali Loss: 1.4601994 Test Loss: 0.4486498
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4279861450195312
Epoch: 24, Steps: 59 | Train Loss: 0.5858749 Vali Loss: 1.4572101 Test Loss: 0.4481896
Validation loss decreased (1.459908 --> 1.457210).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.4912846088409424
Epoch: 25, Steps: 59 | Train Loss: 0.5855914 Vali Loss: 1.4527087 Test Loss: 0.4477926
Validation loss decreased (1.457210 --> 1.452709).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.3772740364074707
Epoch: 26, Steps: 59 | Train Loss: 0.5851922 Vali Loss: 1.4587049 Test Loss: 0.4474775
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.397815465927124
Epoch: 27, Steps: 59 | Train Loss: 0.5848477 Vali Loss: 1.4572192 Test Loss: 0.4472643
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.3737103939056396
Epoch: 28, Steps: 59 | Train Loss: 0.5843428 Vali Loss: 1.4535971 Test Loss: 0.4470159
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.3947906494140625
Epoch: 29, Steps: 59 | Train Loss: 0.5842013 Vali Loss: 1.4557028 Test Loss: 0.4468724
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.3697004318237305
Epoch: 30, Steps: 59 | Train Loss: 0.5839782 Vali Loss: 1.4472880 Test Loss: 0.4467165
Validation loss decreased (1.452709 --> 1.447288).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.3548531532287598
Epoch: 31, Steps: 59 | Train Loss: 0.5837166 Vali Loss: 1.4562407 Test Loss: 0.4466115
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.329326868057251
Epoch: 32, Steps: 59 | Train Loss: 0.5836130 Vali Loss: 1.4520482 Test Loss: 0.4465501
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4055192470550537
Epoch: 33, Steps: 59 | Train Loss: 0.5835443 Vali Loss: 1.4490793 Test Loss: 0.4464650
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.4631664752960205
Epoch: 34, Steps: 59 | Train Loss: 0.5831624 Vali Loss: 1.4464632 Test Loss: 0.4464028
Validation loss decreased (1.447288 --> 1.446463).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.3278987407684326
Epoch: 35, Steps: 59 | Train Loss: 0.5831349 Vali Loss: 1.4504416 Test Loss: 0.4463870
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3332102298736572
Epoch: 36, Steps: 59 | Train Loss: 0.5831145 Vali Loss: 1.4504440 Test Loss: 0.4463480
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.3866260051727295
Epoch: 37, Steps: 59 | Train Loss: 0.5829949 Vali Loss: 1.4473661 Test Loss: 0.4463516
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.3561348915100098
Epoch: 38, Steps: 59 | Train Loss: 0.5828772 Vali Loss: 1.4453316 Test Loss: 0.4463379
Validation loss decreased (1.446463 --> 1.445332).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.298168659210205
Epoch: 39, Steps: 59 | Train Loss: 0.5826689 Vali Loss: 1.4430315 Test Loss: 0.4463433
Validation loss decreased (1.445332 --> 1.443032).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.4140844345092773
Epoch: 40, Steps: 59 | Train Loss: 0.5827714 Vali Loss: 1.4468071 Test Loss: 0.4463380
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.513338565826416
Epoch: 41, Steps: 59 | Train Loss: 0.5826360 Vali Loss: 1.4514768 Test Loss: 0.4463494
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3124630451202393
Epoch: 42, Steps: 59 | Train Loss: 0.5826825 Vali Loss: 1.4504201 Test Loss: 0.4463498
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.349191427230835
Epoch: 43, Steps: 59 | Train Loss: 0.5825278 Vali Loss: 1.4464805 Test Loss: 0.4463554
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.3761460781097412
Epoch: 44, Steps: 59 | Train Loss: 0.5822741 Vali Loss: 1.4463401 Test Loss: 0.4463657
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.3339650630950928
Epoch: 45, Steps: 59 | Train Loss: 0.5825425 Vali Loss: 1.4493575 Test Loss: 0.4463885
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.3291358947753906
Epoch: 46, Steps: 59 | Train Loss: 0.5824210 Vali Loss: 1.4448159 Test Loss: 0.4463918
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.3623082637786865
Epoch: 47, Steps: 59 | Train Loss: 0.5823308 Vali Loss: 1.4473944 Test Loss: 0.4464112
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.4353160858154297
Epoch: 48, Steps: 59 | Train Loss: 0.5822747 Vali Loss: 1.4438148 Test Loss: 0.4464333
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.5944488048553467
Epoch: 49, Steps: 59 | Train Loss: 0.5821978 Vali Loss: 1.4432398 Test Loss: 0.4464635
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.7338831424713135
Epoch: 50, Steps: 59 | Train Loss: 0.5823427 Vali Loss: 1.4465839 Test Loss: 0.4464574
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.0667004585266113
Epoch: 51, Steps: 59 | Train Loss: 0.5821379 Vali Loss: 1.4406228 Test Loss: 0.4464854
Validation loss decreased (1.443032 --> 1.440623).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.769270420074463
Epoch: 52, Steps: 59 | Train Loss: 0.5822309 Vali Loss: 1.4423172 Test Loss: 0.4465026
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.235429286956787
Epoch: 53, Steps: 59 | Train Loss: 0.5820572 Vali Loss: 1.4475615 Test Loss: 0.4465183
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.629487991333008
Epoch: 54, Steps: 59 | Train Loss: 0.5820431 Vali Loss: 1.4444727 Test Loss: 0.4465376
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.9564437866210938
Epoch: 55, Steps: 59 | Train Loss: 0.5820100 Vali Loss: 1.4394352 Test Loss: 0.4465459
Validation loss decreased (1.440623 --> 1.439435).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.9660184383392334
Epoch: 56, Steps: 59 | Train Loss: 0.5819459 Vali Loss: 1.4476644 Test Loss: 0.4465659
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.8033676147460938
Epoch: 57, Steps: 59 | Train Loss: 0.5819555 Vali Loss: 1.4436874 Test Loss: 0.4465894
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 4.056725025177002
Epoch: 58, Steps: 59 | Train Loss: 0.5821822 Vali Loss: 1.4456100 Test Loss: 0.4465971
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.7639734745025635
Epoch: 59, Steps: 59 | Train Loss: 0.5820167 Vali Loss: 1.4445837 Test Loss: 0.4466118
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.533714532852173
Epoch: 60, Steps: 59 | Train Loss: 0.5820460 Vali Loss: 1.4456317 Test Loss: 0.4466245
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.32456111907959
Epoch: 61, Steps: 59 | Train Loss: 0.5819831 Vali Loss: 1.4435762 Test Loss: 0.4466349
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.9873242378234863
Epoch: 62, Steps: 59 | Train Loss: 0.5817431 Vali Loss: 1.4424262 Test Loss: 0.4466579
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.334758758544922
Epoch: 63, Steps: 59 | Train Loss: 0.5816927 Vali Loss: 1.4405712 Test Loss: 0.4466706
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.844261884689331
Epoch: 64, Steps: 59 | Train Loss: 0.5820417 Vali Loss: 1.4454964 Test Loss: 0.4466790
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.7313201427459717
Epoch: 65, Steps: 59 | Train Loss: 0.5817016 Vali Loss: 1.4450445 Test Loss: 0.4466886
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.1063549518585205
Epoch: 66, Steps: 59 | Train Loss: 0.5817990 Vali Loss: 1.4430778 Test Loss: 0.4467014
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.082789897918701
Epoch: 67, Steps: 59 | Train Loss: 0.5818651 Vali Loss: 1.4420760 Test Loss: 0.4467157
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 3.201348066329956
Epoch: 68, Steps: 59 | Train Loss: 0.5817965 Vali Loss: 1.4477715 Test Loss: 0.4467235
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.816502094268799
Epoch: 69, Steps: 59 | Train Loss: 0.5818209 Vali Loss: 1.4419165 Test Loss: 0.4467365
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.7590277194976807
Epoch: 70, Steps: 59 | Train Loss: 0.5818045 Vali Loss: 1.4444672 Test Loss: 0.4467463
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.5157289505004883
Epoch: 71, Steps: 59 | Train Loss: 0.5817640 Vali Loss: 1.4429175 Test Loss: 0.4467570
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.8039498329162598
Epoch: 72, Steps: 59 | Train Loss: 0.5818381 Vali Loss: 1.4438914 Test Loss: 0.4467663
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.8929169178009033
Epoch: 73, Steps: 59 | Train Loss: 0.5818109 Vali Loss: 1.4431226 Test Loss: 0.4467776
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.8489298820495605
Epoch: 74, Steps: 59 | Train Loss: 0.5817889 Vali Loss: 1.4465368 Test Loss: 0.4467835
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.9678335189819336
Epoch: 75, Steps: 59 | Train Loss: 0.5818922 Vali Loss: 1.4375279 Test Loss: 0.4467913
Validation loss decreased (1.439435 --> 1.437528).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.3803753852844238
Epoch: 76, Steps: 59 | Train Loss: 0.5818911 Vali Loss: 1.4428251 Test Loss: 0.4468009
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.3615107536315918
Epoch: 77, Steps: 59 | Train Loss: 0.5816978 Vali Loss: 1.4335631 Test Loss: 0.4468035
Validation loss decreased (1.437528 --> 1.433563).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.4036452770233154
Epoch: 78, Steps: 59 | Train Loss: 0.5816917 Vali Loss: 1.4480610 Test Loss: 0.4468110
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.3725476264953613
Epoch: 79, Steps: 59 | Train Loss: 0.5816722 Vali Loss: 1.4430606 Test Loss: 0.4468216
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.403346300125122
Epoch: 80, Steps: 59 | Train Loss: 0.5816986 Vali Loss: 1.4422472 Test Loss: 0.4468286
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.335306167602539
Epoch: 81, Steps: 59 | Train Loss: 0.5816439 Vali Loss: 1.4413116 Test Loss: 0.4468358
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.4528076648712158
Epoch: 82, Steps: 59 | Train Loss: 0.5815591 Vali Loss: 1.4419246 Test Loss: 0.4468409
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.400838851928711
Epoch: 83, Steps: 59 | Train Loss: 0.5818471 Vali Loss: 1.4382730 Test Loss: 0.4468466
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.3602213859558105
Epoch: 84, Steps: 59 | Train Loss: 0.5818245 Vali Loss: 1.4443330 Test Loss: 0.4468513
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.367781162261963
Epoch: 85, Steps: 59 | Train Loss: 0.5816592 Vali Loss: 1.4473230 Test Loss: 0.4468586
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.358689546585083
Epoch: 86, Steps: 59 | Train Loss: 0.5819106 Vali Loss: 1.4421148 Test Loss: 0.4468645
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.2612054347991943
Epoch: 87, Steps: 59 | Train Loss: 0.5818151 Vali Loss: 1.4457009 Test Loss: 0.4468681
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.4251625537872314
Epoch: 88, Steps: 59 | Train Loss: 0.5818254 Vali Loss: 1.4422216 Test Loss: 0.4468731
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.2851569652557373
Epoch: 89, Steps: 59 | Train Loss: 0.5816990 Vali Loss: 1.4380696 Test Loss: 0.4468782
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.2597975730895996
Epoch: 90, Steps: 59 | Train Loss: 0.5815912 Vali Loss: 1.4421593 Test Loss: 0.4468828
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.393392562866211
Epoch: 91, Steps: 59 | Train Loss: 0.5817626 Vali Loss: 1.4409308 Test Loss: 0.4468863
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.3230769634246826
Epoch: 92, Steps: 59 | Train Loss: 0.5817459 Vali Loss: 1.4432008 Test Loss: 0.4468925
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.352842092514038
Epoch: 93, Steps: 59 | Train Loss: 0.5816068 Vali Loss: 1.4430416 Test Loss: 0.4468950
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.3292720317840576
Epoch: 94, Steps: 59 | Train Loss: 0.5816248 Vali Loss: 1.4420102 Test Loss: 0.4469004
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.3392791748046875
Epoch: 95, Steps: 59 | Train Loss: 0.5816972 Vali Loss: 1.4415740 Test Loss: 0.4469025
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.3215622901916504
Epoch: 96, Steps: 59 | Train Loss: 0.5818378 Vali Loss: 1.4442343 Test Loss: 0.4469081
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.4200358390808105
Epoch: 97, Steps: 59 | Train Loss: 0.5818223 Vali Loss: 1.4451228 Test Loss: 0.4469112
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4458560645580292, mae:0.46471476554870605, rse:0.6392184495925903, corr:[0.22271915 0.23041657 0.23333053 0.2325544  0.2304368  0.22885767
 0.22842543 0.22883028 0.22919022 0.22925685 0.22894734 0.22832412
 0.22768506 0.22729276 0.2271313  0.22715986 0.22720236 0.22717799
 0.22711533 0.22706704 0.22712056 0.22738951 0.22776154 0.2282381
 0.22861196 0.2287453  0.22863112 0.22829598 0.22783904 0.22740859
 0.22710836 0.22694257 0.22686884 0.2268451  0.22684474 0.22682907
 0.22683765 0.22688322 0.22695951 0.22699991 0.22696027 0.2267619
 0.22659299 0.2265577  0.22662401 0.22673243 0.22686741 0.22692828
 0.2267236  0.22630341 0.22564459 0.22498854 0.22453521 0.2241837
 0.22394758 0.22362247 0.22345376 0.22317934 0.22276987 0.22234689
 0.22197534 0.22173727 0.22161834 0.22169743 0.2218283  0.22195967
 0.22207291 0.22206472 0.22198747 0.22188014 0.22168472 0.22153091
 0.221229   0.22087754 0.22058065 0.22036111 0.22016971 0.21996123
 0.21974795 0.21955143 0.21932271 0.21906148 0.21878505 0.21854526
 0.21834765 0.2181359  0.21785623 0.21753184 0.2171985  0.21690232
 0.21670786 0.21673867 0.21702173 0.21760175 0.2184138  0.21936771
 0.22036159 0.22124115 0.22189571 0.2222671  0.2223227  0.22218549
 0.22194661 0.22173654 0.22154124 0.22134951 0.22117247 0.2210428
 0.2209937  0.22102219 0.2209999  0.220952   0.22076935 0.22047482
 0.22024478 0.22008176 0.22001722 0.21996662 0.21989027 0.21978055
 0.21956995 0.21927744 0.21890576 0.2186442  0.21841703 0.21818224
 0.21791656 0.2176445  0.21732692 0.21697727 0.21668579 0.21654552
 0.2165809  0.21664949 0.21671142 0.2167036  0.21655849 0.21632235
 0.21607727 0.2159347  0.21593381 0.2160176  0.21604723 0.21591796
 0.21561554 0.2151004  0.21449035 0.2139046  0.21352525 0.21335073
 0.21328314 0.21328793 0.21328844 0.21323851 0.21309555 0.21295603
 0.21286367 0.21285501 0.2129443  0.21308267 0.21302247 0.21283521
 0.21258084 0.21235532 0.21230939 0.21247335 0.2128663  0.21346998
 0.21413717 0.21473064 0.215014   0.21508062 0.21501973 0.21498172
 0.21498796 0.21506318 0.21511118 0.21506073 0.21490213 0.2146771
 0.2144518  0.21432264 0.21436389 0.21451168 0.2147244  0.2149187
 0.21502788 0.21501611 0.21495038 0.21481654 0.21461365 0.21434093
 0.21401913 0.21365774 0.2132225  0.21276262 0.21231927 0.21194121
 0.21164766 0.21156086 0.21157855 0.21159182 0.21156581 0.21150848
 0.21140759 0.21133943 0.21129446 0.2112538  0.21126074 0.21128474
 0.2112561  0.21108446 0.21084507 0.21061979 0.21034072 0.21018021
 0.21017914 0.21023498 0.21033007 0.21038118 0.21041483 0.21038726
 0.2102327  0.20999806 0.2097585  0.20954138 0.20935638 0.20924917
 0.2091899  0.20907314 0.20891269 0.20870067 0.20837629 0.20807336
 0.2079008  0.20782751 0.20785716 0.20792724 0.20804365 0.20808321
 0.20814447 0.20816605 0.20818704 0.20831783 0.20850152 0.20864087
 0.20879985 0.20888183 0.2088386  0.20860164 0.2083222  0.20815152
 0.20821148 0.20836316 0.20860797 0.20873444 0.20875703 0.20864536
 0.20847985 0.20822102 0.20810701 0.20817195 0.20831613 0.20842397
 0.208406   0.20819817 0.20784591 0.20747827 0.20714436 0.20695722
 0.20697157 0.20711486 0.20720701 0.20715947 0.20697434 0.20678516
 0.20663263 0.20655073 0.20647256 0.20637263 0.20619279 0.20600131
 0.20576738 0.205594   0.20562533 0.20594005 0.20663531 0.2075406
 0.20848571 0.20922759 0.20965011 0.20969374 0.20953643 0.20933396
 0.20923918 0.20930634 0.20943359 0.20952773 0.20950952 0.20942579
 0.20930135 0.20915121 0.20910051 0.2092011  0.20928295 0.20930387
 0.20932828 0.20927054 0.20919795 0.20925522 0.20944037 0.20978826
 0.2102944  0.2107867  0.21101914 0.21105127 0.21090727 0.21062459
 0.21030931 0.21005557 0.20982602 0.20961496 0.20946552 0.20943378
 0.20946863 0.20951627 0.20948383 0.209509   0.2095076  0.2095094
 0.2095752  0.20968495 0.20980455 0.20989038 0.20989175 0.20977664
 0.20959288 0.20938289 0.20916744 0.20893762 0.20875502 0.20850581
 0.20829159 0.2081474  0.20802027 0.20793277 0.20797643 0.20818576
 0.20843035 0.20867956 0.20884286 0.20891124 0.2087715  0.20862335
 0.20851392 0.20839135 0.20827322 0.20829208 0.2082611  0.20821574
 0.20814061 0.2079136  0.20751628 0.20704055 0.20663774 0.20630634
 0.20613952 0.20606416 0.20594086 0.20570932 0.2053935  0.2051291
 0.20491876 0.20477237 0.20473157 0.20475096 0.20478527 0.20474887
 0.20458621 0.2043191  0.20419572 0.20447564 0.2051242  0.20620844
 0.20744966 0.2083257  0.20866807 0.20837617 0.2077638  0.2069991
 0.20645669 0.20610033 0.20589513 0.20576055 0.20560136 0.20542611
 0.20530239 0.2052271  0.20524853 0.20538269 0.20564426 0.20605531
 0.20646963 0.20674103 0.20686418 0.20703395 0.20725489 0.20768787
 0.20815355 0.20852056 0.20862654 0.20845199 0.20808442 0.20757726
 0.20713666 0.2069257  0.20687114 0.20691589 0.207018   0.20707004
 0.20698778 0.20681483 0.2064928  0.2061553  0.205927   0.20594312
 0.20617798 0.20660205 0.20702527 0.2077025  0.20839441 0.209035
 0.20953262 0.20977071 0.2096763  0.2094579  0.2092355  0.20895627
 0.20865983 0.20853633 0.20844968 0.20836285 0.2082951  0.20823105
 0.20819092 0.20810789 0.20799862 0.20786391 0.20771055 0.20759496
 0.20761852 0.20779157 0.2081886  0.20886111 0.20962122 0.21042354
 0.21099798 0.2111751  0.21087779 0.2104157  0.20990646 0.2094955
 0.20928515 0.20928799 0.20936245 0.2094348  0.20949045 0.20952043
 0.20956823 0.20967267 0.20982665 0.20994195 0.20995873 0.20991457
 0.20979318 0.20966488 0.20958193 0.20952806 0.20969377 0.21002658
 0.21029215 0.21035388 0.21008411 0.20969361 0.20928693 0.20893067
 0.20868301 0.2085502  0.2083871  0.2082158  0.20801952 0.20785554
 0.20772982 0.20763601 0.20757502 0.20754965 0.20758052 0.20759508
 0.20761842 0.20766376 0.20784815 0.2083169  0.20917326 0.21027355
 0.21132578 0.21211012 0.21241082 0.21227527 0.21180297 0.21111476
 0.21044159 0.20994955 0.20960645 0.20936275 0.20928799 0.2094177
 0.20963793 0.20983335 0.20998555 0.21009113 0.21023431 0.21046996
 0.21081161 0.21125291 0.2117409  0.2121524  0.21252444 0.21273118
 0.2127544  0.21251862 0.21209224 0.21162908 0.21110919 0.21051343
 0.21000531 0.20974277 0.20952335 0.20930791 0.20912725 0.20914371
 0.20931935 0.20963895 0.21000136 0.21028659 0.21046892 0.21058929
 0.21060863 0.21056566 0.21046838 0.21040128 0.21034773 0.21034023
 0.21017718 0.20978196 0.20912096 0.20834017 0.20747636 0.20679852
 0.20635135 0.20606492 0.20577896 0.20540322 0.20501168 0.20469819
 0.204512   0.20429899 0.2041958  0.204165   0.20429727 0.2044312
 0.20455143 0.20458245 0.20466787 0.2048479  0.20517848 0.20537418
 0.20534816 0.20520717 0.2046911  0.20394897 0.20322378 0.20243959
 0.20193465 0.20176405 0.2018128  0.20185703 0.20182794 0.20165288
 0.20147684 0.20125759 0.20117934 0.20119737 0.20126358 0.20133331
 0.20137705 0.20129125 0.20113866 0.20099445 0.20090541 0.20091878
 0.20087238 0.20058396 0.20002191 0.19928381 0.19846112 0.19761582
 0.1969136  0.19650263 0.19629605 0.19623575 0.19619916 0.1961191
 0.19607298 0.19600816 0.19583055 0.19567016 0.19552538 0.19542035
 0.19547054 0.19563825 0.19587567 0.19614644 0.19636393 0.1965634
 0.19662373 0.19640093 0.19608615 0.19574955 0.1952843  0.19461048
 0.1939778  0.1935434  0.19320904 0.19299135 0.1928619  0.19281064
 0.19289236 0.19287017 0.1927823  0.19267894 0.1925485  0.19256285
 0.1926404  0.19276002 0.19297698 0.19332334 0.19379146 0.19424102
 0.1944934  0.19447179 0.19402109 0.19364324 0.1930803  0.19251932
 0.19208425 0.1917757  0.19151166 0.19112948 0.19076435 0.19055709
 0.19054052 0.1906019  0.19071272 0.1906998  0.19073379 0.19076855
 0.19079502 0.19082613 0.1909457  0.19113165 0.19116238 0.19107212
 0.19070312 0.18991953 0.18883991 0.18761836 0.18658659 0.1858649
 0.18552843 0.18555607 0.18568972 0.1855948  0.18531005 0.18498628
 0.18477489 0.18455444 0.18445615 0.18463118 0.1848713  0.18519594
 0.18548633 0.18576482 0.18604285 0.18643941 0.1868163  0.18696454
 0.1869969  0.18664585 0.18582477 0.18491472 0.18415636 0.1836681
 0.18374054 0.18417685 0.18452114 0.18434478 0.18374452 0.1829524
 0.1823097  0.18176298 0.1815459  0.18142086 0.18153921 0.18176498
 0.18224317 0.18272267 0.1831699  0.18273741 0.17949769 0.16756937]
