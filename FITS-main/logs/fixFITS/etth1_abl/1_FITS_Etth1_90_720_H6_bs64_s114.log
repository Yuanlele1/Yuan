Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=34, out_features=306, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9321984.0
params:  10710.0
Trainable parameters:  10710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.6268446445465088
Epoch: 1, Steps: 61 | Train Loss: 1.4934378 Vali Loss: 2.7044001 Test Loss: 1.3848679
Validation loss decreased (inf --> 2.704400).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.4610655307769775
Epoch: 2, Steps: 61 | Train Loss: 1.0702365 Vali Loss: 2.1866469 Test Loss: 0.9499052
Validation loss decreased (2.704400 --> 2.186647).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.303426742553711
Epoch: 3, Steps: 61 | Train Loss: 0.8753094 Vali Loss: 1.9505982 Test Loss: 0.7483069
Validation loss decreased (2.186647 --> 1.950598).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3099465370178223
Epoch: 4, Steps: 61 | Train Loss: 0.7744585 Vali Loss: 1.8191624 Test Loss: 0.6422360
Validation loss decreased (1.950598 --> 1.819162).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.6298339366912842
Epoch: 5, Steps: 61 | Train Loss: 0.7183993 Vali Loss: 1.7476803 Test Loss: 0.5811162
Validation loss decreased (1.819162 --> 1.747680).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.3564496040344238
Epoch: 6, Steps: 61 | Train Loss: 0.6846129 Vali Loss: 1.6933744 Test Loss: 0.5441962
Validation loss decreased (1.747680 --> 1.693374).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4589908123016357
Epoch: 7, Steps: 61 | Train Loss: 0.6641982 Vali Loss: 1.6623366 Test Loss: 0.5211275
Validation loss decreased (1.693374 --> 1.662337).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.084996223449707
Epoch: 8, Steps: 61 | Train Loss: 0.6509062 Vali Loss: 1.6498804 Test Loss: 0.5061941
Validation loss decreased (1.662337 --> 1.649880).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3901069164276123
Epoch: 9, Steps: 61 | Train Loss: 0.6420301 Vali Loss: 1.6346360 Test Loss: 0.4961617
Validation loss decreased (1.649880 --> 1.634636).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.3515892028808594
Epoch: 10, Steps: 61 | Train Loss: 0.6356559 Vali Loss: 1.6242132 Test Loss: 0.4893228
Validation loss decreased (1.634636 --> 1.624213).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.3717527389526367
Epoch: 11, Steps: 61 | Train Loss: 0.6309910 Vali Loss: 1.6142814 Test Loss: 0.4845237
Validation loss decreased (1.624213 --> 1.614281).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.414980411529541
Epoch: 12, Steps: 61 | Train Loss: 0.6277592 Vali Loss: 1.6072962 Test Loss: 0.4810743
Validation loss decreased (1.614281 --> 1.607296).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.2374358177185059
Epoch: 13, Steps: 61 | Train Loss: 0.6250101 Vali Loss: 1.6081636 Test Loss: 0.4785570
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.2366411685943604
Epoch: 14, Steps: 61 | Train Loss: 0.6229053 Vali Loss: 1.5973859 Test Loss: 0.4766504
Validation loss decreased (1.607296 --> 1.597386).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.2472853660583496
Epoch: 15, Steps: 61 | Train Loss: 0.6216818 Vali Loss: 1.5983722 Test Loss: 0.4752092
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.350372552871704
Epoch: 16, Steps: 61 | Train Loss: 0.6202942 Vali Loss: 1.5985914 Test Loss: 0.4741339
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.369399070739746
Epoch: 17, Steps: 61 | Train Loss: 0.6189443 Vali Loss: 1.5853479 Test Loss: 0.4732538
Validation loss decreased (1.597386 --> 1.585348).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.2700259685516357
Epoch: 18, Steps: 61 | Train Loss: 0.6178688 Vali Loss: 1.5881741 Test Loss: 0.4725436
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.2034647464752197
Epoch: 19, Steps: 61 | Train Loss: 0.6171611 Vali Loss: 1.5907592 Test Loss: 0.4719842
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3785510063171387
Epoch: 20, Steps: 61 | Train Loss: 0.6165715 Vali Loss: 1.5934044 Test Loss: 0.4715540
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.2931180000305176
Epoch: 21, Steps: 61 | Train Loss: 0.6159201 Vali Loss: 1.5944910 Test Loss: 0.4711975
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.948744535446167
Epoch: 22, Steps: 61 | Train Loss: 0.6150640 Vali Loss: 1.5841057 Test Loss: 0.4708799
Validation loss decreased (1.585348 --> 1.584106).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.7580368518829346
Epoch: 23, Steps: 61 | Train Loss: 0.6148429 Vali Loss: 1.5875164 Test Loss: 0.4706088
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.3237643241882324
Epoch: 24, Steps: 61 | Train Loss: 0.6144423 Vali Loss: 1.5726519 Test Loss: 0.4704228
Validation loss decreased (1.584106 --> 1.572652).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.3664381504058838
Epoch: 25, Steps: 61 | Train Loss: 0.6141863 Vali Loss: 1.5824412 Test Loss: 0.4702594
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7483882904052734
Epoch: 26, Steps: 61 | Train Loss: 0.6133871 Vali Loss: 1.5820265 Test Loss: 0.4700754
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6991510391235352
Epoch: 27, Steps: 61 | Train Loss: 0.6135569 Vali Loss: 1.5805347 Test Loss: 0.4699554
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.818877935409546
Epoch: 28, Steps: 61 | Train Loss: 0.6129123 Vali Loss: 1.5857921 Test Loss: 0.4698330
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.1493613719940186
Epoch: 29, Steps: 61 | Train Loss: 0.6127481 Vali Loss: 1.5764805 Test Loss: 0.4697361
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.6804776191711426
Epoch: 30, Steps: 61 | Train Loss: 0.6125509 Vali Loss: 1.5803547 Test Loss: 0.4696819
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5254795551300049
Epoch: 31, Steps: 61 | Train Loss: 0.6121424 Vali Loss: 1.5841033 Test Loss: 0.4696028
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.8415510654449463
Epoch: 32, Steps: 61 | Train Loss: 0.6120523 Vali Loss: 1.5677606 Test Loss: 0.4695506
Validation loss decreased (1.572652 --> 1.567761).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.146261215209961
Epoch: 33, Steps: 61 | Train Loss: 0.6119997 Vali Loss: 1.5794028 Test Loss: 0.4694863
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.520186424255371
Epoch: 34, Steps: 61 | Train Loss: 0.6116521 Vali Loss: 1.5744320 Test Loss: 0.4694213
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.413905382156372
Epoch: 35, Steps: 61 | Train Loss: 0.6114422 Vali Loss: 1.5717165 Test Loss: 0.4694007
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6054558753967285
Epoch: 36, Steps: 61 | Train Loss: 0.6113285 Vali Loss: 1.5747039 Test Loss: 0.4693612
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5232470035552979
Epoch: 37, Steps: 61 | Train Loss: 0.6112260 Vali Loss: 1.5810487 Test Loss: 0.4693336
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4372248649597168
Epoch: 38, Steps: 61 | Train Loss: 0.6111924 Vali Loss: 1.5737824 Test Loss: 0.4693009
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.4201886653900146
Epoch: 39, Steps: 61 | Train Loss: 0.6114321 Vali Loss: 1.5732954 Test Loss: 0.4692824
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.6823790073394775
Epoch: 40, Steps: 61 | Train Loss: 0.6110163 Vali Loss: 1.5822818 Test Loss: 0.4692742
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.3650119304656982
Epoch: 41, Steps: 61 | Train Loss: 0.6114675 Vali Loss: 1.5764149 Test Loss: 0.4692360
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.522195816040039
Epoch: 42, Steps: 61 | Train Loss: 0.6107108 Vali Loss: 1.5741946 Test Loss: 0.4692121
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.4494132995605469
Epoch: 43, Steps: 61 | Train Loss: 0.6106871 Vali Loss: 1.5742892 Test Loss: 0.4691975
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.6482315063476562
Epoch: 44, Steps: 61 | Train Loss: 0.6105897 Vali Loss: 1.5752752 Test Loss: 0.4691791
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.4706628322601318
Epoch: 45, Steps: 61 | Train Loss: 0.6101922 Vali Loss: 1.5753218 Test Loss: 0.4691707
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6086158752441406
Epoch: 46, Steps: 61 | Train Loss: 0.6103305 Vali Loss: 1.5712457 Test Loss: 0.4691689
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.582158088684082
Epoch: 47, Steps: 61 | Train Loss: 0.6101576 Vali Loss: 1.5689427 Test Loss: 0.4691565
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.6485075950622559
Epoch: 48, Steps: 61 | Train Loss: 0.6101582 Vali Loss: 1.5736585 Test Loss: 0.4691551
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.5038516521453857
Epoch: 49, Steps: 61 | Train Loss: 0.6105015 Vali Loss: 1.5728059 Test Loss: 0.4691442
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6527526378631592
Epoch: 50, Steps: 61 | Train Loss: 0.6101612 Vali Loss: 1.5678704 Test Loss: 0.4691408
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.769303560256958
Epoch: 51, Steps: 61 | Train Loss: 0.6100683 Vali Loss: 1.5759003 Test Loss: 0.4691322
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6990447044372559
Epoch: 52, Steps: 61 | Train Loss: 0.6099226 Vali Loss: 1.5687163 Test Loss: 0.4691311
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4682318866252899, mae:0.46325647830963135, rse:0.6550620794296265, corr:[0.22787225 0.23263784 0.22863463 0.22848989 0.22666202 0.22412185
 0.22529432 0.22531411 0.22500941 0.22526544 0.22467259 0.22353895
 0.22298473 0.22312716 0.22230254 0.22263615 0.22307007 0.22269374
 0.2231637  0.22361895 0.22252637 0.22295901 0.22293796 0.22157153
 0.22093654 0.21950036 0.2187496  0.22003368 0.21961242 0.21985164
 0.220555   0.22033626 0.22052036 0.22030197 0.21942861 0.21933487
 0.21966228 0.21903798 0.21931137 0.22008249 0.21989626 0.2201951
 0.2207037  0.22066522 0.22033311 0.22051665 0.22018555 0.21935637
 0.21852864 0.21693845 0.21633635 0.21637024 0.2155321  0.21517447
 0.2153687  0.21494813 0.21506888 0.21503095 0.21436644 0.21422851
 0.21415834 0.21360978 0.21383637 0.21384525 0.21367891 0.21438244
 0.21464626 0.21454713 0.21440645 0.21432696 0.21327618 0.21213424
 0.21084431 0.20943104 0.20896357 0.20948564 0.20905913 0.20920129
 0.21008791 0.21015286 0.21021602 0.21002899 0.20953372 0.20918064
 0.20920852 0.2088354  0.209028   0.20930642 0.2089558  0.20910886
 0.20915143 0.20908587 0.20905192 0.20934202 0.20912592 0.20917575
 0.20853864 0.20766187 0.20829311 0.20865503 0.20888269 0.2095532
 0.21002126 0.2101296  0.21026629 0.21008085 0.20965801 0.2094055
 0.20951311 0.20919932 0.20944637 0.20939295 0.20940702 0.20977081
 0.20959093 0.20961618 0.20974985 0.20961165 0.20901254 0.20806527
 0.206299   0.2049836  0.20448612 0.20380738 0.20372775 0.20431535
 0.205342   0.205865   0.20620157 0.2060563  0.20568585 0.20550872
 0.20547214 0.2050567  0.2053333  0.20529164 0.20536596 0.2059337
 0.20572358 0.20584409 0.20612091 0.20606323 0.20575893 0.20529407
 0.20354873 0.20225552 0.2023514  0.20147014 0.20046207 0.20098451
 0.20204969 0.20234683 0.20242323 0.20221546 0.20196773 0.20169748
 0.2013031  0.20100617 0.201099   0.20062578 0.20061418 0.20119119
 0.20123723 0.20144022 0.20157379 0.20151486 0.20124103 0.20066953
 0.19885676 0.19779362 0.19860086 0.1986824  0.19751924 0.197641
 0.19859584 0.19897951 0.19916967 0.19889294 0.19878592 0.19881125
 0.19850071 0.1979867  0.19816418 0.19793451 0.19792666 0.19852158
 0.19876997 0.19910711 0.19933335 0.19941577 0.19938506 0.19889066
 0.1970232  0.19571504 0.19509412 0.19422184 0.19341205 0.19403858
 0.19487628 0.19527827 0.19578488 0.19547258 0.19508179 0.19480838
 0.19461904 0.19444874 0.19464733 0.1942521  0.19412374 0.19436105
 0.19425327 0.1944707  0.19447955 0.19437258 0.19439767 0.19376561
 0.19178136 0.19080003 0.1908209  0.19123277 0.19184032 0.19297372
 0.19472475 0.19589435 0.19629304 0.19644608 0.19652247 0.19617929
 0.19618794 0.19606802 0.19610257 0.19609475 0.19595918 0.19572599
 0.19555493 0.19585358 0.19589406 0.19590046 0.1958129  0.19473371
 0.19282302 0.19182447 0.19155401 0.19165148 0.1914998  0.19175683
 0.19355264 0.19458929 0.19482355 0.19483137 0.19440484 0.19413325
 0.19409238 0.19373262 0.1937145  0.19351354 0.19345367 0.19345243
 0.19330427 0.19361185 0.19356416 0.19315156 0.19304392 0.19228023
 0.19021682 0.18961138 0.18954831 0.18993776 0.19040293 0.19130869
 0.19318847 0.19461878 0.19456202 0.19419421 0.19431792 0.1939068
 0.19338195 0.19332072 0.19291796 0.19240095 0.19260423 0.19284627
 0.19309065 0.1937126  0.19388291 0.19404963 0.19433868 0.19393405
 0.19282348 0.19264813 0.19407919 0.19526124 0.19517595 0.1956715
 0.19685271 0.19732386 0.19720815 0.1968431  0.19665053 0.1960734
 0.19566809 0.19560502 0.19554403 0.19530068 0.19529703 0.19527067
 0.19514644 0.1952563  0.19525392 0.19532366 0.19542857 0.19478297
 0.19314931 0.19231139 0.19254476 0.19256845 0.19240265 0.19257699
 0.19385076 0.19461688 0.19475876 0.19436304 0.1940658  0.19373567
 0.19363604 0.19350965 0.19338632 0.19335672 0.19324085 0.19308099
 0.19313738 0.19324279 0.19299304 0.1929201  0.1928126  0.19185588
 0.19012529 0.18892604 0.18774827 0.18789853 0.18822168 0.1888129
 0.19081531 0.19227976 0.19253017 0.19233097 0.192343   0.19177064
 0.19141915 0.19138986 0.1912     0.19126987 0.19149092 0.19156194
 0.19166431 0.19158739 0.19110656 0.1909237  0.19057773 0.189359
 0.18727168 0.1858794  0.18552049 0.1851659  0.18473394 0.18498968
 0.186958   0.18800798 0.18832043 0.18805356 0.18788387 0.1875627
 0.18720424 0.1870631  0.18674977 0.18642932 0.186509   0.18640406
 0.18665402 0.18688424 0.1865379  0.18661146 0.18652317 0.18542957
 0.18406095 0.1836642  0.18398574 0.1851341  0.18554464 0.18585739
 0.1879883  0.18931243 0.18958193 0.18948671 0.1894616  0.18926324
 0.18911773 0.18915933 0.18911697 0.18919706 0.1893658  0.18950278
 0.19005497 0.19040431 0.1903921  0.1906932  0.19059666 0.19012126
 0.18919109 0.18814617 0.18811801 0.1891765  0.18950929 0.19039367
 0.19270203 0.19366045 0.19378057 0.19358617 0.1935551  0.19334973
 0.1932965  0.19341892 0.19317521 0.1929938  0.19323958 0.19331561
 0.19344336 0.1938734  0.19384363 0.19420385 0.19437857 0.19362341
 0.19223353 0.19148631 0.19202802 0.1932487  0.19356327 0.19448663
 0.19661088 0.19769506 0.19775018 0.19767107 0.19764546 0.19710787
 0.19700289 0.19692613 0.19670278 0.19657418 0.19660091 0.19666395
 0.19672434 0.1966522  0.19655009 0.19670802 0.19640005 0.19604582
 0.19538866 0.19482347 0.19550058 0.19650106 0.19641969 0.1968675
 0.19829533 0.19912933 0.19961007 0.19943221 0.19943266 0.19948636
 0.19959344 0.1996226  0.19974288 0.19996265 0.19998781 0.20012315
 0.20026459 0.20035897 0.20033357 0.20009777 0.1996239  0.19899453
 0.1977418  0.19674103 0.19652243 0.19672464 0.19671355 0.19722295
 0.1989177  0.19961698 0.19976269 0.19950807 0.19935207 0.19908313
 0.19920155 0.19923979 0.19894311 0.19879974 0.19874358 0.19867128
 0.1987811  0.19867796 0.198721   0.19931972 0.19944651 0.19968027
 0.19930768 0.19873239 0.19966677 0.20080712 0.2006525  0.2009933
 0.20228052 0.2025449  0.202727   0.20270824 0.2026199  0.20241775
 0.20262189 0.20278676 0.20286295 0.20298716 0.20311336 0.20322093
 0.20337835 0.20372508 0.2039841  0.20427637 0.20443304 0.20453827
 0.20335025 0.20202374 0.20215908 0.20280206 0.2026239  0.20270947
 0.2036295  0.20436475 0.2046174  0.20435794 0.20428503 0.2042026
 0.20410888 0.20418613 0.20424591 0.2039531  0.2040829  0.20437543
 0.20428117 0.2044035  0.2044288  0.20416686 0.20367344 0.20315194
 0.20148839 0.19988833 0.20005544 0.20002685 0.19927287 0.19934322
 0.20037144 0.20077135 0.20103043 0.2008041  0.20045364 0.20041046
 0.20040691 0.20003553 0.19994289 0.19966173 0.1997172  0.2001633
 0.20022832 0.20038469 0.20026429 0.19998538 0.19996084 0.19928843
 0.19743395 0.19662243 0.19651003 0.1966788  0.196858   0.19706985
 0.19826534 0.19887649 0.19913594 0.19908886 0.19893225 0.19893534
 0.1986053  0.19835149 0.19846973 0.198079   0.19815567 0.19830385
 0.19816892 0.1984136  0.19810802 0.197712   0.19769758 0.19685663
 0.19479632 0.19353557 0.19333042 0.1932726  0.19321354 0.19315569
 0.19401895 0.19476873 0.1949376  0.19477756 0.19477192 0.19482242
 0.19469194 0.19446331 0.19452097 0.1943436  0.19451928 0.19474033
 0.19438627 0.19463582 0.19482166 0.19453289 0.19470872 0.19436361
 0.19249469 0.19137365 0.19080211 0.19026454 0.18994907 0.18987773
 0.1907305  0.19127001 0.19125523 0.19089293 0.19078726 0.19096309
 0.19079132 0.1905134  0.1903316  0.18976179 0.18973038 0.18979757
 0.18950103 0.18951654 0.1895805  0.1896561  0.18994676 0.18965155
 0.18816784 0.18733001 0.18659833 0.18651567 0.18685716 0.18689832
 0.1878481  0.18822427 0.18808122 0.18758304 0.18735518 0.18722837
 0.18676217 0.18656747 0.18635008 0.18587795 0.18600352 0.18581302
 0.1854561  0.18567823 0.18547857 0.18520385 0.18544301 0.18464267
 0.18190645 0.17975311 0.17825766 0.17651537 0.17542993 0.17445561
 0.17410861 0.17434528 0.17397295 0.17316997 0.17321138 0.1732366
 0.17286466 0.17275248 0.17241351 0.17200817 0.17192338 0.17179677
 0.17125164 0.17115872 0.1714246  0.1713353  0.17179087 0.17176166
 0.16942607 0.1688537  0.1685652  0.16731234 0.16755292 0.1672207
 0.1678559  0.16852055 0.16799483 0.1672206  0.16642292 0.16571355
 0.16559546 0.1640593  0.16324285 0.16276297 0.16173154 0.16221417
 0.16165727 0.1616775  0.16350639 0.16128077 0.16373329 0.16040538]
