Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.822890281677246
Epoch: 1, Steps: 56 | Train Loss: 0.8184630 Vali Loss: 2.0696192 Test Loss: 0.8643990
Validation loss decreased (inf --> 2.069619).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.861164808273315
Epoch: 2, Steps: 56 | Train Loss: 0.6372918 Vali Loss: 1.8764153 Test Loss: 0.7523523
Validation loss decreased (2.069619 --> 1.876415).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 6.232740163803101
Epoch: 3, Steps: 56 | Train Loss: 0.5638902 Vali Loss: 1.8121651 Test Loss: 0.7125830
Validation loss decreased (1.876415 --> 1.812165).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.836336612701416
Epoch: 4, Steps: 56 | Train Loss: 0.5265481 Vali Loss: 1.7760214 Test Loss: 0.6904148
Validation loss decreased (1.812165 --> 1.776021).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.759429216384888
Epoch: 5, Steps: 56 | Train Loss: 0.5020056 Vali Loss: 1.7501311 Test Loss: 0.6752229
Validation loss decreased (1.776021 --> 1.750131).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 6.248722076416016
Epoch: 6, Steps: 56 | Train Loss: 0.4827942 Vali Loss: 1.7325850 Test Loss: 0.6619548
Validation loss decreased (1.750131 --> 1.732585).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 6.042001008987427
Epoch: 7, Steps: 56 | Train Loss: 0.4669540 Vali Loss: 1.7175539 Test Loss: 0.6511230
Validation loss decreased (1.732585 --> 1.717554).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 6.177769660949707
Epoch: 8, Steps: 56 | Train Loss: 0.4535033 Vali Loss: 1.7055610 Test Loss: 0.6402756
Validation loss decreased (1.717554 --> 1.705561).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 6.3108978271484375
Epoch: 9, Steps: 56 | Train Loss: 0.4416094 Vali Loss: 1.6877644 Test Loss: 0.6305857
Validation loss decreased (1.705561 --> 1.687764).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.936754941940308
Epoch: 10, Steps: 56 | Train Loss: 0.4314231 Vali Loss: 1.6804368 Test Loss: 0.6222580
Validation loss decreased (1.687764 --> 1.680437).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.031160116195679
Epoch: 11, Steps: 56 | Train Loss: 0.4221990 Vali Loss: 1.6658936 Test Loss: 0.6135620
Validation loss decreased (1.680437 --> 1.665894).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.156035661697388
Epoch: 12, Steps: 56 | Train Loss: 0.4140853 Vali Loss: 1.6573943 Test Loss: 0.6060133
Validation loss decreased (1.665894 --> 1.657394).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 6.1688010692596436
Epoch: 13, Steps: 56 | Train Loss: 0.4068034 Vali Loss: 1.6486226 Test Loss: 0.5987916
Validation loss decreased (1.657394 --> 1.648623).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.271404027938843
Epoch: 14, Steps: 56 | Train Loss: 0.4001589 Vali Loss: 1.6448588 Test Loss: 0.5925990
Validation loss decreased (1.648623 --> 1.644859).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 6.543519496917725
Epoch: 15, Steps: 56 | Train Loss: 0.3944876 Vali Loss: 1.6332824 Test Loss: 0.5859108
Validation loss decreased (1.644859 --> 1.633282).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 6.546983242034912
Epoch: 16, Steps: 56 | Train Loss: 0.3890884 Vali Loss: 1.6262410 Test Loss: 0.5805560
Validation loss decreased (1.633282 --> 1.626241).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 6.498218297958374
Epoch: 17, Steps: 56 | Train Loss: 0.3844310 Vali Loss: 1.6107944 Test Loss: 0.5756236
Validation loss decreased (1.626241 --> 1.610794).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 6.309929370880127
Epoch: 18, Steps: 56 | Train Loss: 0.3800061 Vali Loss: 1.6058924 Test Loss: 0.5705850
Validation loss decreased (1.610794 --> 1.605892).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 6.405262470245361
Epoch: 19, Steps: 56 | Train Loss: 0.3759637 Vali Loss: 1.6041858 Test Loss: 0.5661918
Validation loss decreased (1.605892 --> 1.604186).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 6.4026780128479
Epoch: 20, Steps: 56 | Train Loss: 0.3723719 Vali Loss: 1.5965211 Test Loss: 0.5617803
Validation loss decreased (1.604186 --> 1.596521).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 6.167378902435303
Epoch: 21, Steps: 56 | Train Loss: 0.3688836 Vali Loss: 1.5945144 Test Loss: 0.5579640
Validation loss decreased (1.596521 --> 1.594514).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 6.291449308395386
Epoch: 22, Steps: 56 | Train Loss: 0.3660276 Vali Loss: 1.5857265 Test Loss: 0.5541051
Validation loss decreased (1.594514 --> 1.585726).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 6.31982946395874
Epoch: 23, Steps: 56 | Train Loss: 0.3632503 Vali Loss: 1.5793499 Test Loss: 0.5505905
Validation loss decreased (1.585726 --> 1.579350).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 6.276533126831055
Epoch: 24, Steps: 56 | Train Loss: 0.3605286 Vali Loss: 1.5795965 Test Loss: 0.5475148
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 6.302609205245972
Epoch: 25, Steps: 56 | Train Loss: 0.3579152 Vali Loss: 1.5747890 Test Loss: 0.5445672
Validation loss decreased (1.579350 --> 1.574789).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 6.54986834526062
Epoch: 26, Steps: 56 | Train Loss: 0.3558257 Vali Loss: 1.5753232 Test Loss: 0.5416381
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 6.24543309211731
Epoch: 27, Steps: 56 | Train Loss: 0.3540797 Vali Loss: 1.5733875 Test Loss: 0.5390906
Validation loss decreased (1.574789 --> 1.573388).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 6.311377763748169
Epoch: 28, Steps: 56 | Train Loss: 0.3518501 Vali Loss: 1.5662479 Test Loss: 0.5365958
Validation loss decreased (1.573388 --> 1.566248).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 6.365014553070068
Epoch: 29, Steps: 56 | Train Loss: 0.3500566 Vali Loss: 1.5658035 Test Loss: 0.5342110
Validation loss decreased (1.566248 --> 1.565804).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 6.23649787902832
Epoch: 30, Steps: 56 | Train Loss: 0.3484350 Vali Loss: 1.5559396 Test Loss: 0.5319804
Validation loss decreased (1.565804 --> 1.555940).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 6.277735948562622
Epoch: 31, Steps: 56 | Train Loss: 0.3470297 Vali Loss: 1.5573062 Test Loss: 0.5300654
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 6.307294130325317
Epoch: 32, Steps: 56 | Train Loss: 0.3455166 Vali Loss: 1.5585676 Test Loss: 0.5282019
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 6.0932135581970215
Epoch: 33, Steps: 56 | Train Loss: 0.3440953 Vali Loss: 1.5520895 Test Loss: 0.5263165
Validation loss decreased (1.555940 --> 1.552089).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 6.219080209732056
Epoch: 34, Steps: 56 | Train Loss: 0.3430581 Vali Loss: 1.5463097 Test Loss: 0.5244096
Validation loss decreased (1.552089 --> 1.546310).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 6.37404727935791
Epoch: 35, Steps: 56 | Train Loss: 0.3416733 Vali Loss: 1.5464288 Test Loss: 0.5229236
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 6.064818382263184
Epoch: 36, Steps: 56 | Train Loss: 0.3405694 Vali Loss: 1.5420871 Test Loss: 0.5214616
Validation loss decreased (1.546310 --> 1.542087).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 5.999513387680054
Epoch: 37, Steps: 56 | Train Loss: 0.3395812 Vali Loss: 1.5471294 Test Loss: 0.5198900
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 6.03113579750061
Epoch: 38, Steps: 56 | Train Loss: 0.3386586 Vali Loss: 1.5430830 Test Loss: 0.5186669
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 6.109084129333496
Epoch: 39, Steps: 56 | Train Loss: 0.3378511 Vali Loss: 1.5416504 Test Loss: 0.5172791
Validation loss decreased (1.542087 --> 1.541650).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 6.327628135681152
Epoch: 40, Steps: 56 | Train Loss: 0.3369626 Vali Loss: 1.5370498 Test Loss: 0.5161434
Validation loss decreased (1.541650 --> 1.537050).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 6.219975471496582
Epoch: 41, Steps: 56 | Train Loss: 0.3360945 Vali Loss: 1.5384724 Test Loss: 0.5149786
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 6.253998279571533
Epoch: 42, Steps: 56 | Train Loss: 0.3354047 Vali Loss: 1.5408274 Test Loss: 0.5139202
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 5.985755681991577
Epoch: 43, Steps: 56 | Train Loss: 0.3348475 Vali Loss: 1.5330907 Test Loss: 0.5128685
Validation loss decreased (1.537050 --> 1.533091).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 6.384472846984863
Epoch: 44, Steps: 56 | Train Loss: 0.3338163 Vali Loss: 1.5365639 Test Loss: 0.5119092
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 6.232096195220947
Epoch: 45, Steps: 56 | Train Loss: 0.3333411 Vali Loss: 1.5289975 Test Loss: 0.5109681
Validation loss decreased (1.533091 --> 1.528998).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 6.3539817333221436
Epoch: 46, Steps: 56 | Train Loss: 0.3326232 Vali Loss: 1.5331608 Test Loss: 0.5101401
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 6.10237193107605
Epoch: 47, Steps: 56 | Train Loss: 0.3320600 Vali Loss: 1.5297773 Test Loss: 0.5093560
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 6.091954946517944
Epoch: 48, Steps: 56 | Train Loss: 0.3316422 Vali Loss: 1.5298839 Test Loss: 0.5085170
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 6.3795552253723145
Epoch: 49, Steps: 56 | Train Loss: 0.3310141 Vali Loss: 1.5281875 Test Loss: 0.5077292
Validation loss decreased (1.528998 --> 1.528188).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 6.23152494430542
Epoch: 50, Steps: 56 | Train Loss: 0.3307970 Vali Loss: 1.5266888 Test Loss: 0.5070680
Validation loss decreased (1.528188 --> 1.526689).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 6.240453004837036
Epoch: 51, Steps: 56 | Train Loss: 0.3300474 Vali Loss: 1.5243655 Test Loss: 0.5064922
Validation loss decreased (1.526689 --> 1.524366).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 6.307876825332642
Epoch: 52, Steps: 56 | Train Loss: 0.3297853 Vali Loss: 1.5278184 Test Loss: 0.5058093
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 6.519385814666748
Epoch: 53, Steps: 56 | Train Loss: 0.3293461 Vali Loss: 1.5273173 Test Loss: 0.5052317
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 6.4280195236206055
Epoch: 54, Steps: 56 | Train Loss: 0.3289193 Vali Loss: 1.5219421 Test Loss: 0.5046299
Validation loss decreased (1.524366 --> 1.521942).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 6.390235185623169
Epoch: 55, Steps: 56 | Train Loss: 0.3287081 Vali Loss: 1.5270386 Test Loss: 0.5041387
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 6.141178369522095
Epoch: 56, Steps: 56 | Train Loss: 0.3280362 Vali Loss: 1.5243015 Test Loss: 0.5036274
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 6.329676151275635
Epoch: 57, Steps: 56 | Train Loss: 0.3277712 Vali Loss: 1.5187682 Test Loss: 0.5031093
Validation loss decreased (1.521942 --> 1.518768).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 6.296810150146484
Epoch: 58, Steps: 56 | Train Loss: 0.3275484 Vali Loss: 1.5219226 Test Loss: 0.5026087
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 6.290579080581665
Epoch: 59, Steps: 56 | Train Loss: 0.3272536 Vali Loss: 1.5218408 Test Loss: 0.5021483
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 6.240177392959595
Epoch: 60, Steps: 56 | Train Loss: 0.3270909 Vali Loss: 1.5216727 Test Loss: 0.5017998
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 6.279007196426392
Epoch: 61, Steps: 56 | Train Loss: 0.3266882 Vali Loss: 1.5201356 Test Loss: 0.5014111
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 6.380811929702759
Epoch: 62, Steps: 56 | Train Loss: 0.3265768 Vali Loss: 1.5228989 Test Loss: 0.5010254
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 6.274756908416748
Epoch: 63, Steps: 56 | Train Loss: 0.3263040 Vali Loss: 1.5221306 Test Loss: 0.5006631
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 5.9374754428863525
Epoch: 64, Steps: 56 | Train Loss: 0.3260621 Vali Loss: 1.5220129 Test Loss: 0.5003057
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 6.247222661972046
Epoch: 65, Steps: 56 | Train Loss: 0.3257820 Vali Loss: 1.5227032 Test Loss: 0.4999928
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 6.439523220062256
Epoch: 66, Steps: 56 | Train Loss: 0.3255715 Vali Loss: 1.5137883 Test Loss: 0.4996693
Validation loss decreased (1.518768 --> 1.513788).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 6.447170257568359
Epoch: 67, Steps: 56 | Train Loss: 0.3255372 Vali Loss: 1.5196564 Test Loss: 0.4993636
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 6.161010026931763
Epoch: 68, Steps: 56 | Train Loss: 0.3253011 Vali Loss: 1.5219432 Test Loss: 0.4991172
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 6.302807569503784
Epoch: 69, Steps: 56 | Train Loss: 0.3250644 Vali Loss: 1.5173481 Test Loss: 0.4988467
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 6.343682765960693
Epoch: 70, Steps: 56 | Train Loss: 0.3247874 Vali Loss: 1.5166348 Test Loss: 0.4985952
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 5.8590803146362305
Epoch: 71, Steps: 56 | Train Loss: 0.3247717 Vali Loss: 1.5175922 Test Loss: 0.4983473
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 6.009189605712891
Epoch: 72, Steps: 56 | Train Loss: 0.3247749 Vali Loss: 1.5169927 Test Loss: 0.4981472
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 6.231497287750244
Epoch: 73, Steps: 56 | Train Loss: 0.3244316 Vali Loss: 1.5157709 Test Loss: 0.4979358
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 6.14324688911438
Epoch: 74, Steps: 56 | Train Loss: 0.3243347 Vali Loss: 1.5191776 Test Loss: 0.4977202
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 6.2801923751831055
Epoch: 75, Steps: 56 | Train Loss: 0.3241340 Vali Loss: 1.5140362 Test Loss: 0.4975337
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 6.2484636306762695
Epoch: 76, Steps: 56 | Train Loss: 0.3241304 Vali Loss: 1.5154617 Test Loss: 0.4973393
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 6.18241024017334
Epoch: 77, Steps: 56 | Train Loss: 0.3239320 Vali Loss: 1.5191388 Test Loss: 0.4971645
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 6.1819164752960205
Epoch: 78, Steps: 56 | Train Loss: 0.3236509 Vali Loss: 1.5141047 Test Loss: 0.4970044
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 6.385841608047485
Epoch: 79, Steps: 56 | Train Loss: 0.3237432 Vali Loss: 1.5125983 Test Loss: 0.4968328
Validation loss decreased (1.513788 --> 1.512598).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 6.2222511768341064
Epoch: 80, Steps: 56 | Train Loss: 0.3234983 Vali Loss: 1.5143665 Test Loss: 0.4966919
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 6.290472984313965
Epoch: 81, Steps: 56 | Train Loss: 0.3234169 Vali Loss: 1.5150690 Test Loss: 0.4965345
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 6.224853992462158
Epoch: 82, Steps: 56 | Train Loss: 0.3232118 Vali Loss: 1.5157261 Test Loss: 0.4964197
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 6.086611986160278
Epoch: 83, Steps: 56 | Train Loss: 0.3232609 Vali Loss: 1.5205913 Test Loss: 0.4962890
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 6.024943113327026
Epoch: 84, Steps: 56 | Train Loss: 0.3230437 Vali Loss: 1.5133109 Test Loss: 0.4961661
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 6.066535711288452
Epoch: 85, Steps: 56 | Train Loss: 0.3232552 Vali Loss: 1.5114744 Test Loss: 0.4960540
Validation loss decreased (1.512598 --> 1.511474).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 6.023486137390137
Epoch: 86, Steps: 56 | Train Loss: 0.3231416 Vali Loss: 1.5097706 Test Loss: 0.4959320
Validation loss decreased (1.511474 --> 1.509771).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 6.091848373413086
Epoch: 87, Steps: 56 | Train Loss: 0.3232362 Vali Loss: 1.5136406 Test Loss: 0.4958214
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 6.264873027801514
Epoch: 88, Steps: 56 | Train Loss: 0.3230325 Vali Loss: 1.5176101 Test Loss: 0.4957272
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 6.115489482879639
Epoch: 89, Steps: 56 | Train Loss: 0.3228186 Vali Loss: 1.5122470 Test Loss: 0.4956315
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 6.044970273971558
Epoch: 90, Steps: 56 | Train Loss: 0.3228646 Vali Loss: 1.5087376 Test Loss: 0.4955361
Validation loss decreased (1.509771 --> 1.508738).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 5.908074617385864
Epoch: 91, Steps: 56 | Train Loss: 0.3228000 Vali Loss: 1.5151869 Test Loss: 0.4954559
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 6.227630853652954
Epoch: 92, Steps: 56 | Train Loss: 0.3227256 Vali Loss: 1.5156560 Test Loss: 0.4953665
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 6.215818405151367
Epoch: 93, Steps: 56 | Train Loss: 0.3226734 Vali Loss: 1.5114298 Test Loss: 0.4952921
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 6.206101179122925
Epoch: 94, Steps: 56 | Train Loss: 0.3227097 Vali Loss: 1.5045798 Test Loss: 0.4952187
Validation loss decreased (1.508738 --> 1.504580).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 6.207485914230347
Epoch: 95, Steps: 56 | Train Loss: 0.3226177 Vali Loss: 1.5148981 Test Loss: 0.4951446
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 5.99642276763916
Epoch: 96, Steps: 56 | Train Loss: 0.3226002 Vali Loss: 1.5078197 Test Loss: 0.4950795
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 6.154210805892944
Epoch: 97, Steps: 56 | Train Loss: 0.3225961 Vali Loss: 1.5138955 Test Loss: 0.4950158
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 6.427943468093872
Epoch: 98, Steps: 56 | Train Loss: 0.3224414 Vali Loss: 1.5138299 Test Loss: 0.4949616
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 6.598987579345703
Epoch: 99, Steps: 56 | Train Loss: 0.3223948 Vali Loss: 1.5120561 Test Loss: 0.4948967
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 6.846163511276245
Epoch: 100, Steps: 56 | Train Loss: 0.3224673 Vali Loss: 1.5073096 Test Loss: 0.4948461
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 6.052072525024414
Epoch: 1, Steps: 56 | Train Loss: 0.5814796 Vali Loss: 1.4708827 Test Loss: 0.4664493
Validation loss decreased (inf --> 1.470883).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 6.048965692520142
Epoch: 2, Steps: 56 | Train Loss: 0.5683277 Vali Loss: 1.4559212 Test Loss: 0.4490893
Validation loss decreased (1.470883 --> 1.455921).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 6.153242111206055
Epoch: 3, Steps: 56 | Train Loss: 0.5605718 Vali Loss: 1.4453115 Test Loss: 0.4397925
Validation loss decreased (1.455921 --> 1.445312).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.6309356689453125
Epoch: 4, Steps: 56 | Train Loss: 0.5558088 Vali Loss: 1.4386234 Test Loss: 0.4351282
Validation loss decreased (1.445312 --> 1.438623).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 6.012236833572388
Epoch: 5, Steps: 56 | Train Loss: 0.5531661 Vali Loss: 1.4296616 Test Loss: 0.4329192
Validation loss decreased (1.438623 --> 1.429662).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.6232008934021
Epoch: 6, Steps: 56 | Train Loss: 0.5512803 Vali Loss: 1.4361519 Test Loss: 0.4321156
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.750176906585693
Epoch: 7, Steps: 56 | Train Loss: 0.5500789 Vali Loss: 1.4391329 Test Loss: 0.4319805
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.703200817108154
Epoch: 8, Steps: 56 | Train Loss: 0.5499814 Vali Loss: 1.4326714 Test Loss: 0.4320650
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 6.3776469230651855
Epoch: 9, Steps: 56 | Train Loss: 0.5495667 Vali Loss: 1.4380543 Test Loss: 0.4325439
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.695714950561523
Epoch: 10, Steps: 56 | Train Loss: 0.5490586 Vali Loss: 1.4385579 Test Loss: 0.4326139
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.077147006988525
Epoch: 11, Steps: 56 | Train Loss: 0.5489826 Vali Loss: 1.4358877 Test Loss: 0.4328313
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.325331926345825
Epoch: 12, Steps: 56 | Train Loss: 0.5485236 Vali Loss: 1.4354146 Test Loss: 0.4330569
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.974335193634033
Epoch: 13, Steps: 56 | Train Loss: 0.5482473 Vali Loss: 1.4432354 Test Loss: 0.4333332
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 6.2304909229278564
Epoch: 14, Steps: 56 | Train Loss: 0.5481579 Vali Loss: 1.4357910 Test Loss: 0.4336463
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 6.124459981918335
Epoch: 15, Steps: 56 | Train Loss: 0.5480386 Vali Loss: 1.4460044 Test Loss: 0.4336869
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.7793238162994385
Epoch: 16, Steps: 56 | Train Loss: 0.5478109 Vali Loss: 1.4379079 Test Loss: 0.4338016
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.48562216758728
Epoch: 17, Steps: 56 | Train Loss: 0.5478349 Vali Loss: 1.4447675 Test Loss: 0.4339545
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.659536600112915
Epoch: 18, Steps: 56 | Train Loss: 0.5476407 Vali Loss: 1.4413936 Test Loss: 0.4340053
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.872871160507202
Epoch: 19, Steps: 56 | Train Loss: 0.5475042 Vali Loss: 1.4372947 Test Loss: 0.4340490
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 5.69345498085022
Epoch: 20, Steps: 56 | Train Loss: 0.5476969 Vali Loss: 1.4433252 Test Loss: 0.4341893
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.464177846908569
Epoch: 21, Steps: 56 | Train Loss: 0.5471785 Vali Loss: 1.4416945 Test Loss: 0.4340971
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 5.98650050163269
Epoch: 22, Steps: 56 | Train Loss: 0.5469230 Vali Loss: 1.4452767 Test Loss: 0.4342908
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 6.238272428512573
Epoch: 23, Steps: 56 | Train Loss: 0.5471956 Vali Loss: 1.4412699 Test Loss: 0.4344095
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 6.062338590621948
Epoch: 24, Steps: 56 | Train Loss: 0.5470411 Vali Loss: 1.4469724 Test Loss: 0.4344718
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.83569598197937
Epoch: 25, Steps: 56 | Train Loss: 0.5471381 Vali Loss: 1.4429784 Test Loss: 0.4344898
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4319782853126526, mae:0.4581006169319153, rse:0.6291916370391846, corr:[0.22555952 0.23354398 0.23185223 0.23375514 0.23368366 0.23053095
 0.22982007 0.23196706 0.23248911 0.23141149 0.23105696 0.23176602
 0.231711   0.23047093 0.2296384  0.22975186 0.22976048 0.2289939
 0.22802445 0.227697   0.22808614 0.22856313 0.22858766 0.22844219
 0.22843532 0.22850582 0.2287515  0.22927396 0.22975697 0.22966467
 0.22920704 0.22884896 0.2286813  0.22833116 0.22786288 0.22777505
 0.22802562 0.22787935 0.22737244 0.22712772 0.22745065 0.22762865
 0.22735576 0.22689535 0.226864   0.22752519 0.22846477 0.22905795
 0.22916314 0.22909181 0.22863193 0.22767653 0.22652438 0.2255489
 0.22498521 0.22427495 0.22366026 0.22345063 0.2233928  0.2232666
 0.22276571 0.22227004 0.2222627  0.22258984 0.22272474 0.22263965
 0.22263087 0.22253352 0.2223305  0.22218958 0.22214986 0.22222719
 0.22184284 0.22111022 0.22063938 0.2208466  0.22094192 0.22041218
 0.2196988  0.21917713 0.21865016 0.21817277 0.21793857 0.2180139
 0.2179402  0.21732102 0.2165339  0.21610712 0.2159084  0.21568622
 0.21550146 0.2158664  0.21658961 0.21712804 0.21724828 0.21780486
 0.21906072 0.22019434 0.22071701 0.22094224 0.2209201  0.22070725
 0.22048879 0.2204759  0.22042522 0.22007635 0.21944077 0.21896349
 0.21901631 0.2192907  0.2193278  0.21891348 0.21837875 0.21819
 0.21859907 0.21911275 0.21926035 0.21905307 0.21874659 0.21858308
 0.21859647 0.2183759  0.21806028 0.21788219 0.21727125 0.21637373
 0.21618648 0.2166621  0.21666665 0.21614867 0.21585158 0.2159372
 0.21587732 0.21522382 0.21454747 0.21467754 0.21548934 0.21592852
 0.21534695 0.21444854 0.21448286 0.21537545 0.21571428 0.21522792
 0.21441372 0.21390218 0.21381658 0.21365646 0.21358667 0.21371338
 0.21357404 0.21319304 0.21296075 0.21292768 0.21251297 0.21195313
 0.2118488  0.21204595 0.21193513 0.21156484 0.21164808 0.21229786
 0.21274126 0.21265268 0.21249604 0.21277888 0.21296985 0.21291427
 0.21253158 0.21235652 0.21300061 0.21416743 0.21462072 0.21416098
 0.21379001 0.21397124 0.21377635 0.21282443 0.21190006 0.21181354
 0.21199319 0.21180905 0.21154635 0.2115958  0.21173419 0.2115904
 0.21133263 0.21137492 0.21188739 0.21257612 0.21320169 0.21354629
 0.21335459 0.21247679 0.21127853 0.21042879 0.21016842 0.21017578
 0.20997588 0.20970464 0.20982362 0.21037316 0.21066484 0.2104622
 0.21029393 0.21072696 0.21118742 0.21088144 0.21024206 0.21022907
 0.2108754  0.21115768 0.21063241 0.20979023 0.20932382 0.20936899
 0.2094438  0.20937316 0.20912538 0.20872384 0.20832163 0.2083141
 0.20886217 0.20912157 0.20860864 0.20798819 0.20776646 0.20750536
 0.2070085  0.20697828 0.20758985 0.20777752 0.20714442 0.20638108
 0.20605396 0.20604478 0.20624985 0.20647323 0.20637167 0.20594953
 0.20617738 0.20710284 0.20773728 0.2074307  0.20688015 0.20701565
 0.2075412  0.20755571 0.2069558  0.20619953 0.20574789 0.2058556
 0.20634869 0.20660605 0.20658691 0.20668598 0.20712215 0.2073273
 0.20679788 0.2057221  0.20516622 0.2053367  0.2055163  0.20556042
 0.20578541 0.20635621 0.20684382 0.20667826 0.20577501 0.20510375
 0.20525448 0.20545149 0.20515656 0.2049812  0.20530008 0.20560266
 0.20535323 0.20489293 0.20460969 0.20462754 0.20463283 0.20452179
 0.20423767 0.20409168 0.20417632 0.20439136 0.20465708 0.20526528
 0.20575221 0.20548823 0.2050578  0.20551197 0.20646545 0.20686497
 0.20668699 0.2066534  0.20672566 0.20661716 0.20657557 0.20688783
 0.20710781 0.20686069 0.2069308  0.20760462 0.20811646 0.20780642
 0.20719516 0.20694244 0.20709255 0.20726492 0.20715395 0.2070075
 0.20719095 0.20790829 0.2085519  0.20808214 0.20665674 0.20592146
 0.20629892 0.20623668 0.20554073 0.20558918 0.20608824 0.20569912
 0.20486145 0.20517303 0.20598531 0.2056898  0.20465349 0.20464757
 0.2052868  0.20501247 0.2042555  0.2043389  0.20484114 0.20481862
 0.20456038 0.20460987 0.20474201 0.20442775 0.20369503 0.20312414
 0.20314804 0.2034056  0.20321856 0.20255071 0.20230852 0.20311376
 0.20386425 0.20340167 0.20230971 0.2024939  0.20374902 0.20446031
 0.20400211 0.20356089 0.20390609 0.20425116 0.20366849 0.20311685
 0.20338784 0.20362478 0.2029776  0.20173252 0.20063797 0.20011897
 0.19990085 0.19959274 0.19950387 0.19984737 0.19961447 0.19827366
 0.19708583 0.19730288 0.19804826 0.19800618 0.1978155  0.1981499
 0.19825242 0.19755651 0.19694945 0.19764788 0.1987305  0.19945556
 0.20000547 0.20086771 0.20175548 0.20206867 0.20174439 0.20111802
 0.20044753 0.19958703 0.19882539 0.19854231 0.1984909  0.19844148
 0.19812433 0.19789025 0.1982277  0.19872503 0.19889615 0.19904508
 0.19942805 0.19979669 0.19997396 0.20021455 0.20050186 0.20055434
 0.20024396 0.19992208 0.20017655 0.20082271 0.20117971 0.20093817
 0.20054068 0.20011882 0.20004144 0.20036548 0.2004843  0.20009643
 0.19984896 0.20018448 0.20012553 0.1991962  0.19814344 0.19791654
 0.19835466 0.19857572 0.19797666 0.1976955  0.19824432 0.19935656
 0.200086   0.20010155 0.20026217 0.20093252 0.2014392  0.20155393
 0.20141351 0.2006262  0.19900309 0.19852707 0.20013075 0.20179671
 0.20143497 0.1999932  0.19982545 0.20057994 0.2006813  0.20029464
 0.2002283  0.1999507  0.19923571 0.19913784 0.19995788 0.20077664
 0.20094554 0.20130207 0.20167907 0.20074153 0.19915292 0.19913675
 0.20060664 0.2010884  0.20021148 0.19986545 0.20017871 0.20019056
 0.200021   0.20021905 0.2003715  0.20020914 0.20033559 0.20086251
 0.20085284 0.20045651 0.20062326 0.20088749 0.20053075 0.20004852
 0.20059313 0.20152462 0.20125945 0.19987957 0.19877477 0.19886766
 0.19974035 0.20024197 0.1995719  0.19867769 0.19882564 0.19983675
 0.19999377 0.1988509  0.19829105 0.1992316  0.20015883 0.19969742
 0.19847621 0.19782199 0.19796617 0.1983397  0.19867341 0.19941156
 0.20053132 0.20155181 0.20184934 0.20141153 0.20077936 0.20062953
 0.20117618 0.2014015  0.20051996 0.1993422  0.19908962 0.19966295
 0.19987875 0.199649   0.20021419 0.20134974 0.20177624 0.20110205
 0.20068067 0.2010708  0.20127252 0.20046586 0.20035952 0.20117733
 0.2021208  0.20207272 0.20150423 0.20111956 0.20037808 0.19993074
 0.20042385 0.20067705 0.19931012 0.19817747 0.19913588 0.2007225
 0.20042117 0.19903013 0.19890909 0.19960138 0.19975568 0.1994379
 0.19924036 0.19873533 0.19810161 0.19843753 0.19939144 0.19957004
 0.19900516 0.19866687 0.19815151 0.19704053 0.19640747 0.19666722
 0.19614421 0.19445051 0.19379857 0.19474864 0.19456674 0.19247158
 0.19088724 0.19101301 0.19161676 0.19163302 0.19233537 0.19330017
 0.1930533  0.1917557  0.19145496 0.19184041 0.19142315 0.19081172
 0.191622   0.19301854 0.19241662 0.19070339 0.19010697 0.19077352
 0.19096957 0.18955374 0.1880007  0.18778726 0.18908252 0.19049957
 0.19088134 0.18962727 0.18832932 0.1888985  0.19120164 0.1926013
 0.19162765 0.1898317  0.18908949 0.1891261  0.18903863 0.18918824
 0.19009341 0.19044149 0.18941903 0.18779904 0.18627255 0.18484923
 0.18431814 0.1847586  0.18461458 0.18344453 0.18273957 0.18331896
 0.18356106 0.1825663  0.18198729 0.18240452 0.18248948 0.18198547
 0.1822664  0.18294424 0.18273513 0.18211538 0.18219629 0.1822506
 0.18173845 0.18172045 0.1822883  0.18217434 0.1814125  0.18146442
 0.18185651 0.18060413 0.17897151 0.17954929 0.18065745 0.17977527
 0.1783124  0.17859624 0.17946292 0.17884065 0.1784414  0.18002959
 0.18141706 0.18052278 0.1790421  0.17902766 0.1796861  0.17984372
 0.17992094 0.18025023 0.1798946  0.17899978 0.17794628 0.17680372
 0.17558263 0.1751286  0.17608318 0.1769539  0.17639585 0.17501837
 0.17399113 0.1733466  0.17329259 0.17417921 0.1758195  0.1766762
 0.17568019 0.17387454 0.1731643  0.17393123 0.17427398 0.17355786
 0.17283194 0.17226744 0.17055118 0.16862386 0.16876826 0.16927627
 0.16763481 0.1642675  0.16237013 0.1617756  0.16134946 0.16170919
 0.16311531 0.16338867 0.16223495 0.16186574 0.16340317 0.16459246
 0.16370228 0.16183291 0.16035633 0.16028963 0.16245596 0.16492677
 0.16451351 0.16323023 0.1639245  0.16526686 0.16376308 0.16250385
 0.16465925 0.1651721  0.16161479 0.15966813 0.16193543 0.16248432
 0.1599189  0.15858056 0.15959749 0.15940319 0.15934841 0.16007315
 0.15853164 0.15775694 0.16043726 0.15631403 0.15305372 0.1725049 ]
