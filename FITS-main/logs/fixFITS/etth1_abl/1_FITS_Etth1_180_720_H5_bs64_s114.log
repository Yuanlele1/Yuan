Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=50, out_features=250, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11200000.0
params:  12750.0
Trainable parameters:  12750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.260822296142578
Epoch: 1, Steps: 60 | Train Loss: 1.1541755 Vali Loss: 2.2226067 Test Loss: 0.9734799
Validation loss decreased (inf --> 2.222607).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.0259242057800293
Epoch: 2, Steps: 60 | Train Loss: 0.8587608 Vali Loss: 1.8806632 Test Loss: 0.7184168
Validation loss decreased (2.222607 --> 1.880663).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.458275079727173
Epoch: 3, Steps: 60 | Train Loss: 0.7369697 Vali Loss: 1.7253578 Test Loss: 0.5993163
Validation loss decreased (1.880663 --> 1.725358).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.821643829345703
Epoch: 4, Steps: 60 | Train Loss: 0.6768920 Vali Loss: 1.6502161 Test Loss: 0.5367059
Validation loss decreased (1.725358 --> 1.650216).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.553055763244629
Epoch: 5, Steps: 60 | Train Loss: 0.6452513 Vali Loss: 1.5982630 Test Loss: 0.5022709
Validation loss decreased (1.650216 --> 1.598263).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.6422371864318848
Epoch: 6, Steps: 60 | Train Loss: 0.6273761 Vali Loss: 1.5728614 Test Loss: 0.4823141
Validation loss decreased (1.598263 --> 1.572861).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.318864345550537
Epoch: 7, Steps: 60 | Train Loss: 0.6171813 Vali Loss: 1.5504982 Test Loss: 0.4701847
Validation loss decreased (1.572861 --> 1.550498).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.855943202972412
Epoch: 8, Steps: 60 | Train Loss: 0.6104218 Vali Loss: 1.5479290 Test Loss: 0.4621866
Validation loss decreased (1.550498 --> 1.547929).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.1488022804260254
Epoch: 9, Steps: 60 | Train Loss: 0.6057880 Vali Loss: 1.5370293 Test Loss: 0.4568334
Validation loss decreased (1.547929 --> 1.537029).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.8300933837890625
Epoch: 10, Steps: 60 | Train Loss: 0.6022634 Vali Loss: 1.5338109 Test Loss: 0.4527986
Validation loss decreased (1.537029 --> 1.533811).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7696797847747803
Epoch: 11, Steps: 60 | Train Loss: 0.5999927 Vali Loss: 1.5221974 Test Loss: 0.4498882
Validation loss decreased (1.533811 --> 1.522197).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.8263802528381348
Epoch: 12, Steps: 60 | Train Loss: 0.5979473 Vali Loss: 1.5209712 Test Loss: 0.4475313
Validation loss decreased (1.522197 --> 1.520971).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9993195533752441
Epoch: 13, Steps: 60 | Train Loss: 0.5960011 Vali Loss: 1.5176990 Test Loss: 0.4457519
Validation loss decreased (1.520971 --> 1.517699).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.0280585289001465
Epoch: 14, Steps: 60 | Train Loss: 0.5948110 Vali Loss: 1.5145245 Test Loss: 0.4443737
Validation loss decreased (1.517699 --> 1.514524).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.8541197776794434
Epoch: 15, Steps: 60 | Train Loss: 0.5937234 Vali Loss: 1.5144696 Test Loss: 0.4432857
Validation loss decreased (1.514524 --> 1.514470).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.8678863048553467
Epoch: 16, Steps: 60 | Train Loss: 0.5930687 Vali Loss: 1.5139079 Test Loss: 0.4422588
Validation loss decreased (1.514470 --> 1.513908).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.1569578647613525
Epoch: 17, Steps: 60 | Train Loss: 0.5921100 Vali Loss: 1.5078491 Test Loss: 0.4415555
Validation loss decreased (1.513908 --> 1.507849).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.9029271602630615
Epoch: 18, Steps: 60 | Train Loss: 0.5909697 Vali Loss: 1.5066034 Test Loss: 0.4410083
Validation loss decreased (1.507849 --> 1.506603).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0103514194488525
Epoch: 19, Steps: 60 | Train Loss: 0.5912674 Vali Loss: 1.5098163 Test Loss: 0.4404859
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9299290180206299
Epoch: 20, Steps: 60 | Train Loss: 0.5897931 Vali Loss: 1.5072570 Test Loss: 0.4400894
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.8680365085601807
Epoch: 21, Steps: 60 | Train Loss: 0.5899452 Vali Loss: 1.5121285 Test Loss: 0.4397393
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.3699047565460205
Epoch: 22, Steps: 60 | Train Loss: 0.5892198 Vali Loss: 1.5046372 Test Loss: 0.4395494
Validation loss decreased (1.506603 --> 1.504637).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8702945709228516
Epoch: 23, Steps: 60 | Train Loss: 0.5882907 Vali Loss: 1.5010425 Test Loss: 0.4393561
Validation loss decreased (1.504637 --> 1.501042).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.1282637119293213
Epoch: 24, Steps: 60 | Train Loss: 0.5888619 Vali Loss: 1.5030029 Test Loss: 0.4391797
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.8384795188903809
Epoch: 25, Steps: 60 | Train Loss: 0.5882381 Vali Loss: 1.4986259 Test Loss: 0.4390771
Validation loss decreased (1.501042 --> 1.498626).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.403452157974243
Epoch: 26, Steps: 60 | Train Loss: 0.5883293 Vali Loss: 1.5070316 Test Loss: 0.4389557
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.9052026271820068
Epoch: 27, Steps: 60 | Train Loss: 0.5880388 Vali Loss: 1.5048572 Test Loss: 0.4389089
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.4356446266174316
Epoch: 28, Steps: 60 | Train Loss: 0.5883821 Vali Loss: 1.5023437 Test Loss: 0.4388459
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.769181489944458
Epoch: 29, Steps: 60 | Train Loss: 0.5879172 Vali Loss: 1.4954097 Test Loss: 0.4387959
Validation loss decreased (1.498626 --> 1.495410).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.8892812728881836
Epoch: 30, Steps: 60 | Train Loss: 0.5877745 Vali Loss: 1.5020603 Test Loss: 0.4387629
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.8975653648376465
Epoch: 31, Steps: 60 | Train Loss: 0.5875725 Vali Loss: 1.5048783 Test Loss: 0.4387355
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.2572832107543945
Epoch: 32, Steps: 60 | Train Loss: 0.5874576 Vali Loss: 1.5039674 Test Loss: 0.4387172
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.8808012008666992
Epoch: 33, Steps: 60 | Train Loss: 0.5867196 Vali Loss: 1.4991131 Test Loss: 0.4387130
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9822754859924316
Epoch: 34, Steps: 60 | Train Loss: 0.5864403 Vali Loss: 1.4958712 Test Loss: 0.4386975
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.355699300765991
Epoch: 35, Steps: 60 | Train Loss: 0.5871278 Vali Loss: 1.5005357 Test Loss: 0.4387116
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.074253797531128
Epoch: 36, Steps: 60 | Train Loss: 0.5872062 Vali Loss: 1.5030801 Test Loss: 0.4387641
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9022150039672852
Epoch: 37, Steps: 60 | Train Loss: 0.5868195 Vali Loss: 1.4990621 Test Loss: 0.4387761
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.913271188735962
Epoch: 38, Steps: 60 | Train Loss: 0.5869204 Vali Loss: 1.4918165 Test Loss: 0.4387600
Validation loss decreased (1.495410 --> 1.491817).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.0493228435516357
Epoch: 39, Steps: 60 | Train Loss: 0.5867772 Vali Loss: 1.5013616 Test Loss: 0.4387810
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.3137218952178955
Epoch: 40, Steps: 60 | Train Loss: 0.5869022 Vali Loss: 1.5000124 Test Loss: 0.4387993
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9953389167785645
Epoch: 41, Steps: 60 | Train Loss: 0.5870779 Vali Loss: 1.4973793 Test Loss: 0.4388087
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.8048253059387207
Epoch: 42, Steps: 60 | Train Loss: 0.5863552 Vali Loss: 1.4966505 Test Loss: 0.4388521
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.2203826904296875
Epoch: 43, Steps: 60 | Train Loss: 0.5863569 Vali Loss: 1.5008247 Test Loss: 0.4388714
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.5290517807006836
Epoch: 44, Steps: 60 | Train Loss: 0.5860355 Vali Loss: 1.5019631 Test Loss: 0.4388925
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.1951873302459717
Epoch: 45, Steps: 60 | Train Loss: 0.5863941 Vali Loss: 1.4951934 Test Loss: 0.4389137
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.861374855041504
Epoch: 46, Steps: 60 | Train Loss: 0.5864366 Vali Loss: 1.4950966 Test Loss: 0.4389497
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.5648033618927
Epoch: 47, Steps: 60 | Train Loss: 0.5863187 Vali Loss: 1.5000626 Test Loss: 0.4389700
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.590804100036621
Epoch: 48, Steps: 60 | Train Loss: 0.5860611 Vali Loss: 1.4985915 Test Loss: 0.4389822
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.6634116172790527
Epoch: 49, Steps: 60 | Train Loss: 0.5861703 Vali Loss: 1.4956719 Test Loss: 0.4390273
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.074810743331909
Epoch: 50, Steps: 60 | Train Loss: 0.5859834 Vali Loss: 1.4990293 Test Loss: 0.4390213
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.4565677642822266
Epoch: 51, Steps: 60 | Train Loss: 0.5866962 Vali Loss: 1.5020707 Test Loss: 0.4390568
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.3493852615356445
Epoch: 52, Steps: 60 | Train Loss: 0.5868973 Vali Loss: 1.4958003 Test Loss: 0.4390761
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.1952452659606934
Epoch: 53, Steps: 60 | Train Loss: 0.5862319 Vali Loss: 1.4981955 Test Loss: 0.4390799
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9019415378570557
Epoch: 54, Steps: 60 | Train Loss: 0.5857965 Vali Loss: 1.5007722 Test Loss: 0.4391189
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.8321003913879395
Epoch: 55, Steps: 60 | Train Loss: 0.5858491 Vali Loss: 1.4964538 Test Loss: 0.4391355
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.578896999359131
Epoch: 56, Steps: 60 | Train Loss: 0.5864471 Vali Loss: 1.4979944 Test Loss: 0.4391607
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.2113096714019775
Epoch: 57, Steps: 60 | Train Loss: 0.5849948 Vali Loss: 1.4955120 Test Loss: 0.4391683
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.917095422744751
Epoch: 58, Steps: 60 | Train Loss: 0.5861213 Vali Loss: 1.4972612 Test Loss: 0.4391980
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43759775161743164, mae:0.44989919662475586, rse:0.6332709193229675, corr:[0.2264909  0.23247074 0.23204987 0.23272365 0.23175165 0.22841062
 0.22831228 0.23165943 0.23206104 0.2306894  0.23066354 0.23138909
 0.23111013 0.22990704 0.22934063 0.22943573 0.22945598 0.22917067
 0.22897188 0.22927104 0.22969511 0.23004465 0.23065624 0.23111624
 0.23047277 0.229473   0.22893186 0.22893707 0.2285333  0.22769634
 0.22715381 0.22733058 0.22733791 0.22683404 0.22658369 0.22718008
 0.22760448 0.22716539 0.22691442 0.22685711 0.22678405 0.2267944
 0.22698627 0.22717708 0.22751713 0.2283401  0.22919679 0.22937788
 0.22866502 0.22777759 0.22617266 0.22456034 0.2235703  0.2228842
 0.22219516 0.22170512 0.22173384 0.22209416 0.2217947  0.2220303
 0.22249722 0.22228605 0.22214197 0.22218792 0.22195959 0.22174495
 0.22197351 0.22206342 0.2218282  0.22187972 0.22209199 0.22162122
 0.22035432 0.21958353 0.21891691 0.21825701 0.21776058 0.21763843
 0.21789657 0.21773373 0.21716352 0.21725543 0.21736306 0.2173552
 0.21726719 0.21699421 0.21701387 0.21694772 0.21651159 0.21606037
 0.2158768  0.21590383 0.21602857 0.21662518 0.21747257 0.21804701
 0.21791813 0.21770723 0.21754885 0.21728863 0.21708408 0.21674165
 0.21639635 0.21682878 0.21714684 0.21716535 0.216919   0.21716383
 0.21743713 0.21717556 0.21692827 0.21673802 0.21644533 0.21617787
 0.21616244 0.21617244 0.21608728 0.21611446 0.21620779 0.21597531
 0.21522228 0.21456358 0.213549   0.2126334  0.21199618 0.21174596
 0.21162874 0.21170145 0.21181181 0.21212634 0.21216351 0.21263812
 0.21347225 0.21357761 0.21332736 0.21303697 0.21284941 0.21272632
 0.2126711  0.21259812 0.21267696 0.21314976 0.21338984 0.21310987
 0.21251373 0.21204263 0.21124698 0.20993362 0.20907657 0.20878172
 0.20872395 0.20875177 0.20911096 0.209603   0.2096644  0.20987599
 0.21027589 0.21017984 0.21007122 0.2100307  0.2098906  0.20974295
 0.20979753 0.20997158 0.21016422 0.21065813 0.21114776 0.21126844
 0.21076307 0.21045385 0.21022248 0.20999141 0.20968066 0.20925188
 0.20916387 0.20969543 0.21012217 0.21055515 0.2109223  0.21150738
 0.21201746 0.21206948 0.21206613 0.21194011 0.211777   0.21180922
 0.2119504  0.2120217  0.21215592 0.21237613 0.21248044 0.21217893
 0.21137252 0.21071559 0.20980637 0.2088043  0.20808445 0.2075912
 0.20720103 0.20718959 0.20763624 0.20817171 0.20819257 0.208642
 0.20929901 0.20935667 0.20907958 0.20870127 0.20845965 0.20830946
 0.20814747 0.20787759 0.20776889 0.20818426 0.2084415  0.20810422
 0.20737383 0.20720074 0.20701763 0.20637159 0.20575391 0.20530137
 0.20520425 0.20527636 0.20540354 0.20571603 0.20589355 0.20613477
 0.2063998  0.20636779 0.20641798 0.20627078 0.2057462  0.2052582
 0.20512542 0.20508882 0.20495026 0.2049653  0.20517299 0.20516942
 0.20467573 0.20423776 0.20381875 0.2036031  0.203527   0.20331684
 0.20326696 0.20347326 0.20376284 0.20407918 0.20431525 0.2050189
 0.20580275 0.20601776 0.20610258 0.20605241 0.2058287  0.20550238
 0.2052479  0.20495892 0.20478982 0.20490357 0.20492446 0.20463602
 0.2040326  0.20376271 0.2032979  0.20267813 0.20230214 0.20211214
 0.20202512 0.20211394 0.20215641 0.20241708 0.20256299 0.20279707
 0.20338334 0.20357004 0.20345525 0.20304291 0.20260839 0.20248339
 0.20257027 0.20259164 0.20265499 0.20323852 0.20382753 0.2040135
 0.20379955 0.20383745 0.20403652 0.20404927 0.2039142  0.20398787
 0.20435315 0.20489606 0.20534152 0.20564571 0.2057468  0.20614435
 0.20673044 0.2069334  0.207112   0.20723942 0.20704252 0.20666458
 0.20659974 0.2066684  0.2065558  0.20669223 0.20684478 0.20688084
 0.20660642 0.2064883  0.20625593 0.205768   0.20541596 0.2049653
 0.2047287  0.20476113 0.2047983  0.20498066 0.20509782 0.20544748
 0.20596218 0.20617951 0.20634311 0.20632476 0.20614117 0.20604923
 0.20601594 0.20584483 0.20582575 0.20617153 0.20639749 0.20622914
 0.20583674 0.20578139 0.20548701 0.20494226 0.20442338 0.2040426
 0.20386335 0.20386623 0.20418538 0.20463337 0.20489366 0.20532003
 0.2058893  0.20618942 0.20626628 0.20607284 0.20568405 0.2054934
 0.20549935 0.20542215 0.20521532 0.20544478 0.20542775 0.20506671
 0.20447828 0.20435637 0.20430064 0.20386556 0.20324863 0.20286652
 0.20292184 0.20298646 0.20289032 0.20277032 0.20267841 0.20277584
 0.20294516 0.20279308 0.20276432 0.20269918 0.20235498 0.20195131
 0.20206635 0.20247799 0.2027313  0.20304538 0.20343477 0.20413312
 0.20456219 0.20487554 0.20496869 0.20486468 0.2044974  0.203709
 0.20328246 0.20332384 0.20330876 0.20333293 0.20324527 0.20374404
 0.20452575 0.20475015 0.20478243 0.20451096 0.20426941 0.20432276
 0.20448053 0.20452267 0.20481929 0.2054357  0.20567407 0.2056013
 0.20558913 0.20585343 0.20564851 0.20498325 0.20478085 0.2047342
 0.20465921 0.20481808 0.20529798 0.2057063  0.20581746 0.20612992
 0.20661663 0.20664816 0.20655155 0.20636246 0.20596297 0.20577556
 0.20610945 0.20648468 0.20649733 0.2071649  0.2080943  0.20859332
 0.20833297 0.2082752  0.20824194 0.2077974  0.20750713 0.20748849
 0.20766042 0.20787561 0.20796396 0.20831339 0.20857412 0.20903881
 0.2095128  0.20947999 0.20916884 0.2088741  0.2088222  0.20892045
 0.20897976 0.20898189 0.20910221 0.20959531 0.21010469 0.21061982
 0.21077655 0.21081442 0.21038772 0.20982903 0.20946747 0.2089147
 0.20851183 0.2085906  0.20894848 0.2092767  0.20968412 0.21043175
 0.21099356 0.2110153  0.21110733 0.21114047 0.21090555 0.21076128
 0.21084414 0.21090648 0.21090692 0.21092771 0.211022   0.21090469
 0.21055408 0.2103542  0.20973012 0.20889813 0.2086543  0.20869142
 0.20878631 0.20885427 0.2091074  0.20956887 0.20971704 0.20999359
 0.21040975 0.21032912 0.21029079 0.21019162 0.20999515 0.20987016
 0.21010523 0.21044467 0.2107269  0.21126638 0.21197687 0.212552
 0.21261032 0.21273555 0.21275929 0.21261576 0.21245638 0.21206088
 0.21173187 0.21151307 0.2114779  0.21143554 0.21151341 0.2119986
 0.21246044 0.21246941 0.21261457 0.21279623 0.21281725 0.21290216
 0.21321823 0.21359491 0.21391729 0.21446612 0.21506815 0.21515009
 0.21454588 0.21422526 0.21389303 0.21344407 0.21291639 0.212216
 0.21167493 0.21162266 0.21191297 0.21226117 0.2122815  0.21250738
 0.21297729 0.2129963  0.2131027  0.21326907 0.2133185  0.21337965
 0.21346575 0.21346645 0.21337584 0.21333861 0.21325225 0.2126912
 0.2116215  0.2110968  0.21070367 0.20991383 0.20919223 0.2087788
 0.20842178 0.20799825 0.20820141 0.20876756 0.20884077 0.20902969
 0.20970814 0.2096794  0.20949788 0.20922762 0.20885114 0.2086921
 0.20890605 0.20885542 0.20854154 0.20857225 0.20886731 0.2086731
 0.2076537  0.20696028 0.20638154 0.20572469 0.20520253 0.2045188
 0.20410517 0.2037779  0.20358993 0.20366979 0.203596   0.20368168
 0.20371234 0.2032501  0.20277514 0.20242718 0.20211901 0.20196901
 0.2019614  0.20182043 0.2015447  0.20152906 0.2015478  0.2010407
 0.19994974 0.19922611 0.19857308 0.1975456  0.19665357 0.196004
 0.19540338 0.19514127 0.19529174 0.19531788 0.19499005 0.19510789
 0.19580089 0.19564684 0.19516142 0.19501165 0.19492231 0.19461252
 0.19435899 0.1943196  0.19431673 0.1943786  0.1944155  0.19406539
 0.19318622 0.19245471 0.1917066  0.19063978 0.19002336 0.18980585
 0.18972625 0.189241   0.18889368 0.18934947 0.18954417 0.18971676
 0.190329   0.19029103 0.18988241 0.18947329 0.18920329 0.18908678
 0.18892138 0.18884906 0.18898687 0.18926181 0.18930276 0.18903911
 0.18813725 0.18706799 0.18583451 0.18506983 0.18483852 0.18443847
 0.18420821 0.18427746 0.18442598 0.18443383 0.18457314 0.18514162
 0.18561994 0.18534864 0.18519694 0.1851426  0.18480267 0.1845277
 0.18466994 0.18472168 0.18447457 0.18466999 0.1850313  0.18447885
 0.1827864  0.18148705 0.18031958 0.1786653  0.17775205 0.17717792
 0.17651412 0.17612368 0.17685969 0.17740147 0.17707978 0.17770645
 0.17924891 0.17927037 0.1785109  0.17839874 0.1787006  0.17850026
 0.17794612 0.17790058 0.17828508 0.17857729 0.17879087 0.1786919
 0.17798862 0.17751543 0.17707404 0.17657293 0.17597874 0.17530817
 0.17494386 0.17496213 0.17531487 0.1752462  0.17445825 0.17440735
 0.17538126 0.1743817  0.17294079 0.17286985 0.17335346 0.17276171
 0.17227438 0.17284298 0.17281885 0.17139849 0.1723577  0.17047162]
