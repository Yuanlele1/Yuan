Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_192_FITS_ETTh1_ftM_sl90_ll48_pl192_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8359
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=18, out_features=56, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  903168.0
params:  1064.0
Trainable parameters:  1064
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 0.9876954555511475
Epoch: 1, Steps: 65 | Train Loss: 0.8525187 Vali Loss: 1.6140741 Test Loss: 0.9061716
Validation loss decreased (inf --> 1.614074).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.0538742542266846
Epoch: 2, Steps: 65 | Train Loss: 0.6563177 Vali Loss: 1.3399547 Test Loss: 0.6800680
Validation loss decreased (1.614074 --> 1.339955).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 0.9285037517547607
Epoch: 3, Steps: 65 | Train Loss: 0.5600326 Vali Loss: 1.2103795 Test Loss: 0.5752027
Validation loss decreased (1.339955 --> 1.210379).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.075908899307251
Epoch: 4, Steps: 65 | Train Loss: 0.5103296 Vali Loss: 1.1416388 Test Loss: 0.5227641
Validation loss decreased (1.210379 --> 1.141639).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 0.9923436641693115
Epoch: 5, Steps: 65 | Train Loss: 0.4837215 Vali Loss: 1.1036215 Test Loss: 0.4943339
Validation loss decreased (1.141639 --> 1.103621).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.0178914070129395
Epoch: 6, Steps: 65 | Train Loss: 0.4668019 Vali Loss: 1.0801256 Test Loss: 0.4777386
Validation loss decreased (1.103621 --> 1.080126).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 0.9637405872344971
Epoch: 7, Steps: 65 | Train Loss: 0.4570319 Vali Loss: 1.0650001 Test Loss: 0.4677132
Validation loss decreased (1.080126 --> 1.065000).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 0.8780276775360107
Epoch: 8, Steps: 65 | Train Loss: 0.4500803 Vali Loss: 1.0541993 Test Loss: 0.4610195
Validation loss decreased (1.065000 --> 1.054199).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.0889503955841064
Epoch: 9, Steps: 65 | Train Loss: 0.4452298 Vali Loss: 1.0463821 Test Loss: 0.4564083
Validation loss decreased (1.054199 --> 1.046382).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 0.8551962375640869
Epoch: 10, Steps: 65 | Train Loss: 0.4418513 Vali Loss: 1.0403997 Test Loss: 0.4531082
Validation loss decreased (1.046382 --> 1.040400).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 0.9360771179199219
Epoch: 11, Steps: 65 | Train Loss: 0.4395987 Vali Loss: 1.0355641 Test Loss: 0.4507191
Validation loss decreased (1.040400 --> 1.035564).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 0.9999191761016846
Epoch: 12, Steps: 65 | Train Loss: 0.4372617 Vali Loss: 1.0327947 Test Loss: 0.4489171
Validation loss decreased (1.035564 --> 1.032795).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.0545530319213867
Epoch: 13, Steps: 65 | Train Loss: 0.4359754 Vali Loss: 1.0296849 Test Loss: 0.4475086
Validation loss decreased (1.032795 --> 1.029685).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.004127025604248
Epoch: 14, Steps: 65 | Train Loss: 0.4342956 Vali Loss: 1.0277884 Test Loss: 0.4463840
Validation loss decreased (1.029685 --> 1.027788).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 0.9064717292785645
Epoch: 15, Steps: 65 | Train Loss: 0.4337808 Vali Loss: 1.0257916 Test Loss: 0.4455146
Validation loss decreased (1.027788 --> 1.025792).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.1392042636871338
Epoch: 16, Steps: 65 | Train Loss: 0.4328615 Vali Loss: 1.0241405 Test Loss: 0.4448132
Validation loss decreased (1.025792 --> 1.024140).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.0390558242797852
Epoch: 17, Steps: 65 | Train Loss: 0.4320530 Vali Loss: 1.0227091 Test Loss: 0.4441821
Validation loss decreased (1.024140 --> 1.022709).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.0502965450286865
Epoch: 18, Steps: 65 | Train Loss: 0.4316328 Vali Loss: 1.0211211 Test Loss: 0.4436986
Validation loss decreased (1.022709 --> 1.021121).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.0915448665618896
Epoch: 19, Steps: 65 | Train Loss: 0.4308187 Vali Loss: 1.0203352 Test Loss: 0.4432850
Validation loss decreased (1.021121 --> 1.020335).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.0504202842712402
Epoch: 20, Steps: 65 | Train Loss: 0.4306006 Vali Loss: 1.0193340 Test Loss: 0.4429659
Validation loss decreased (1.020335 --> 1.019334).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 0.9810240268707275
Epoch: 21, Steps: 65 | Train Loss: 0.4300386 Vali Loss: 1.0177680 Test Loss: 0.4426580
Validation loss decreased (1.019334 --> 1.017768).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 0.8580315113067627
Epoch: 22, Steps: 65 | Train Loss: 0.4302364 Vali Loss: 1.0179809 Test Loss: 0.4425381
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.1039235591888428
Epoch: 23, Steps: 65 | Train Loss: 0.4301190 Vali Loss: 1.0172987 Test Loss: 0.4423626
Validation loss decreased (1.017768 --> 1.017299).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 0.9984052181243896
Epoch: 24, Steps: 65 | Train Loss: 0.4294320 Vali Loss: 1.0168599 Test Loss: 0.4421856
Validation loss decreased (1.017299 --> 1.016860).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 0.9916465282440186
Epoch: 25, Steps: 65 | Train Loss: 0.4291633 Vali Loss: 1.0162139 Test Loss: 0.4419836
Validation loss decreased (1.016860 --> 1.016214).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.0307347774505615
Epoch: 26, Steps: 65 | Train Loss: 0.4290759 Vali Loss: 1.0158426 Test Loss: 0.4419602
Validation loss decreased (1.016214 --> 1.015843).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 0.9184637069702148
Epoch: 27, Steps: 65 | Train Loss: 0.4289982 Vali Loss: 1.0155406 Test Loss: 0.4418653
Validation loss decreased (1.015843 --> 1.015541).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.0253124237060547
Epoch: 28, Steps: 65 | Train Loss: 0.4287333 Vali Loss: 1.0151128 Test Loss: 0.4417938
Validation loss decreased (1.015541 --> 1.015113).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 0.963043212890625
Epoch: 29, Steps: 65 | Train Loss: 0.4286521 Vali Loss: 1.0148993 Test Loss: 0.4416941
Validation loss decreased (1.015113 --> 1.014899).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.0402979850769043
Epoch: 30, Steps: 65 | Train Loss: 0.4284798 Vali Loss: 1.0143267 Test Loss: 0.4416321
Validation loss decreased (1.014899 --> 1.014327).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 0.9741325378417969
Epoch: 31, Steps: 65 | Train Loss: 0.4283967 Vali Loss: 1.0141966 Test Loss: 0.4415942
Validation loss decreased (1.014327 --> 1.014197).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 0.9970073699951172
Epoch: 32, Steps: 65 | Train Loss: 0.4282898 Vali Loss: 1.0139792 Test Loss: 0.4415625
Validation loss decreased (1.014197 --> 1.013979).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.0036492347717285
Epoch: 33, Steps: 65 | Train Loss: 0.4286946 Vali Loss: 1.0133425 Test Loss: 0.4414986
Validation loss decreased (1.013979 --> 1.013342).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.278055191040039
Epoch: 34, Steps: 65 | Train Loss: 0.4278361 Vali Loss: 1.0135800 Test Loss: 0.4414549
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.0096089839935303
Epoch: 35, Steps: 65 | Train Loss: 0.4282014 Vali Loss: 1.0131843 Test Loss: 0.4414558
Validation loss decreased (1.013342 --> 1.013184).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 0.9617495536804199
Epoch: 36, Steps: 65 | Train Loss: 0.4281111 Vali Loss: 1.0128663 Test Loss: 0.4414459
Validation loss decreased (1.013184 --> 1.012866).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.0669643878936768
Epoch: 37, Steps: 65 | Train Loss: 0.4282164 Vali Loss: 1.0128809 Test Loss: 0.4414517
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 0.9586257934570312
Epoch: 38, Steps: 65 | Train Loss: 0.4281494 Vali Loss: 1.0127003 Test Loss: 0.4413949
Validation loss decreased (1.012866 --> 1.012700).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.0476114749908447
Epoch: 39, Steps: 65 | Train Loss: 0.4282087 Vali Loss: 1.0124625 Test Loss: 0.4413891
Validation loss decreased (1.012700 --> 1.012462).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.0658056735992432
Epoch: 40, Steps: 65 | Train Loss: 0.4273423 Vali Loss: 1.0123281 Test Loss: 0.4413953
Validation loss decreased (1.012462 --> 1.012328).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 0.9629199504852295
Epoch: 41, Steps: 65 | Train Loss: 0.4276588 Vali Loss: 1.0120428 Test Loss: 0.4413812
Validation loss decreased (1.012328 --> 1.012043).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.0373094081878662
Epoch: 42, Steps: 65 | Train Loss: 0.4279912 Vali Loss: 1.0122070 Test Loss: 0.4413484
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.1813163757324219
Epoch: 43, Steps: 65 | Train Loss: 0.4277087 Vali Loss: 1.0121057 Test Loss: 0.4413610
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.0531485080718994
Epoch: 44, Steps: 65 | Train Loss: 0.4278813 Vali Loss: 1.0120231 Test Loss: 0.4413747
Validation loss decreased (1.012043 --> 1.012023).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 0.8448123931884766
Epoch: 45, Steps: 65 | Train Loss: 0.4275398 Vali Loss: 1.0114397 Test Loss: 0.4413989
Validation loss decreased (1.012023 --> 1.011440).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 0.8876352310180664
Epoch: 46, Steps: 65 | Train Loss: 0.4275886 Vali Loss: 1.0116982 Test Loss: 0.4413797
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.0146009922027588
Epoch: 47, Steps: 65 | Train Loss: 0.4275200 Vali Loss: 1.0114667 Test Loss: 0.4413693
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.0209994316101074
Epoch: 48, Steps: 65 | Train Loss: 0.4276748 Vali Loss: 1.0111114 Test Loss: 0.4413577
Validation loss decreased (1.011440 --> 1.011111).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.0212488174438477
Epoch: 49, Steps: 65 | Train Loss: 0.4275335 Vali Loss: 1.0114381 Test Loss: 0.4413493
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.0126605033874512
Epoch: 50, Steps: 65 | Train Loss: 0.4273436 Vali Loss: 1.0108356 Test Loss: 0.4413563
Validation loss decreased (1.011111 --> 1.010836).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.0815074443817139
Epoch: 51, Steps: 65 | Train Loss: 0.4275009 Vali Loss: 1.0112165 Test Loss: 0.4413664
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 0.9317324161529541
Epoch: 52, Steps: 65 | Train Loss: 0.4275028 Vali Loss: 1.0103642 Test Loss: 0.4413700
Validation loss decreased (1.010836 --> 1.010364).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.0444440841674805
Epoch: 53, Steps: 65 | Train Loss: 0.4271781 Vali Loss: 1.0107406 Test Loss: 0.4413619
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.0707206726074219
Epoch: 54, Steps: 65 | Train Loss: 0.4274712 Vali Loss: 1.0113115 Test Loss: 0.4413309
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 0.9585835933685303
Epoch: 55, Steps: 65 | Train Loss: 0.4274886 Vali Loss: 1.0112481 Test Loss: 0.4413653
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.1033306121826172
Epoch: 56, Steps: 65 | Train Loss: 0.4275347 Vali Loss: 1.0102400 Test Loss: 0.4413545
Validation loss decreased (1.010364 --> 1.010240).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.0100572109222412
Epoch: 57, Steps: 65 | Train Loss: 0.4275737 Vali Loss: 1.0110664 Test Loss: 0.4413681
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 0.9719882011413574
Epoch: 58, Steps: 65 | Train Loss: 0.4275025 Vali Loss: 1.0100905 Test Loss: 0.4413753
Validation loss decreased (1.010240 --> 1.010090).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.0292017459869385
Epoch: 59, Steps: 65 | Train Loss: 0.4274322 Vali Loss: 1.0110239 Test Loss: 0.4413609
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.055105447769165
Epoch: 60, Steps: 65 | Train Loss: 0.4277302 Vali Loss: 1.0108855 Test Loss: 0.4413764
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 0.9969446659088135
Epoch: 61, Steps: 65 | Train Loss: 0.4275655 Vali Loss: 1.0109590 Test Loss: 0.4413630
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 0.9731161594390869
Epoch: 62, Steps: 65 | Train Loss: 0.4270833 Vali Loss: 1.0105277 Test Loss: 0.4413615
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 0.9817352294921875
Epoch: 63, Steps: 65 | Train Loss: 0.4274431 Vali Loss: 1.0104911 Test Loss: 0.4413720
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 0.9643230438232422
Epoch: 64, Steps: 65 | Train Loss: 0.4274679 Vali Loss: 1.0108153 Test Loss: 0.4413714
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 0.9849746227264404
Epoch: 65, Steps: 65 | Train Loss: 0.4274973 Vali Loss: 1.0108627 Test Loss: 0.4413627
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.0243897438049316
Epoch: 66, Steps: 65 | Train Loss: 0.4273771 Vali Loss: 1.0108011 Test Loss: 0.4413763
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 0.9937798976898193
Epoch: 67, Steps: 65 | Train Loss: 0.4274780 Vali Loss: 1.0102067 Test Loss: 0.4413788
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 0.9576711654663086
Epoch: 68, Steps: 65 | Train Loss: 0.4273389 Vali Loss: 1.0107818 Test Loss: 0.4413839
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.0191733837127686
Epoch: 69, Steps: 65 | Train Loss: 0.4271292 Vali Loss: 1.0103620 Test Loss: 0.4413791
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 0.9524343013763428
Epoch: 70, Steps: 65 | Train Loss: 0.4275272 Vali Loss: 1.0106999 Test Loss: 0.4413742
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 0.9823596477508545
Epoch: 71, Steps: 65 | Train Loss: 0.4274236 Vali Loss: 1.0096468 Test Loss: 0.4413744
Validation loss decreased (1.010090 --> 1.009647).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 0.9957189559936523
Epoch: 72, Steps: 65 | Train Loss: 0.4272587 Vali Loss: 1.0104166 Test Loss: 0.4413816
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.0013015270233154
Epoch: 73, Steps: 65 | Train Loss: 0.4271259 Vali Loss: 1.0103413 Test Loss: 0.4413802
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.0232481956481934
Epoch: 74, Steps: 65 | Train Loss: 0.4274183 Vali Loss: 1.0096684 Test Loss: 0.4413871
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 0.9574344158172607
Epoch: 75, Steps: 65 | Train Loss: 0.4274792 Vali Loss: 1.0096864 Test Loss: 0.4413941
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.072547197341919
Epoch: 76, Steps: 65 | Train Loss: 0.4273223 Vali Loss: 1.0104182 Test Loss: 0.4413944
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 0.9328491687774658
Epoch: 77, Steps: 65 | Train Loss: 0.4272159 Vali Loss: 1.0105025 Test Loss: 0.4413938
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.0028290748596191
Epoch: 78, Steps: 65 | Train Loss: 0.4272823 Vali Loss: 1.0103453 Test Loss: 0.4413948
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 0.9960243701934814
Epoch: 79, Steps: 65 | Train Loss: 0.4273516 Vali Loss: 1.0105034 Test Loss: 0.4413985
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 0.94970703125
Epoch: 80, Steps: 65 | Train Loss: 0.4268829 Vali Loss: 1.0105308 Test Loss: 0.4413970
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.0089335441589355
Epoch: 81, Steps: 65 | Train Loss: 0.4271719 Vali Loss: 1.0102084 Test Loss: 0.4413963
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.1155719757080078
Epoch: 82, Steps: 65 | Train Loss: 0.4273965 Vali Loss: 1.0101452 Test Loss: 0.4413998
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.0091211795806885
Epoch: 83, Steps: 65 | Train Loss: 0.4272873 Vali Loss: 1.0095185 Test Loss: 0.4413989
Validation loss decreased (1.009647 --> 1.009519).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 0.9306313991546631
Epoch: 84, Steps: 65 | Train Loss: 0.4273055 Vali Loss: 1.0100981 Test Loss: 0.4413960
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 0.9397754669189453
Epoch: 85, Steps: 65 | Train Loss: 0.4270390 Vali Loss: 1.0104769 Test Loss: 0.4413983
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.0040767192840576
Epoch: 86, Steps: 65 | Train Loss: 0.4269708 Vali Loss: 1.0102623 Test Loss: 0.4414040
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 0.9867868423461914
Epoch: 87, Steps: 65 | Train Loss: 0.4273566 Vali Loss: 1.0102193 Test Loss: 0.4414012
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.0059726238250732
Epoch: 88, Steps: 65 | Train Loss: 0.4270360 Vali Loss: 1.0103662 Test Loss: 0.4414036
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 0.9542722702026367
Epoch: 89, Steps: 65 | Train Loss: 0.4270108 Vali Loss: 1.0104872 Test Loss: 0.4414063
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.003392219543457
Epoch: 90, Steps: 65 | Train Loss: 0.4272503 Vali Loss: 1.0103090 Test Loss: 0.4414061
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 0.905691385269165
Epoch: 91, Steps: 65 | Train Loss: 0.4272060 Vali Loss: 1.0103291 Test Loss: 0.4414056
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.0230460166931152
Epoch: 92, Steps: 65 | Train Loss: 0.4272373 Vali Loss: 1.0101812 Test Loss: 0.4414062
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 0.902247428894043
Epoch: 93, Steps: 65 | Train Loss: 0.4274660 Vali Loss: 1.0100679 Test Loss: 0.4414098
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.024268388748169
Epoch: 94, Steps: 65 | Train Loss: 0.4273079 Vali Loss: 1.0103952 Test Loss: 0.4414113
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 0.9776134490966797
Epoch: 95, Steps: 65 | Train Loss: 0.4272024 Vali Loss: 1.0104023 Test Loss: 0.4414080
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.0654432773590088
Epoch: 96, Steps: 65 | Train Loss: 0.4269854 Vali Loss: 1.0101925 Test Loss: 0.4414062
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.1090476512908936
Epoch: 97, Steps: 65 | Train Loss: 0.4274364 Vali Loss: 1.0102431 Test Loss: 0.4414069
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 0.977325439453125
Epoch: 98, Steps: 65 | Train Loss: 0.4271525 Vali Loss: 1.0102524 Test Loss: 0.4414089
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.0877656936645508
Epoch: 99, Steps: 65 | Train Loss: 0.4273550 Vali Loss: 1.0103339 Test Loss: 0.4414088
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 0.9787774085998535
Epoch: 100, Steps: 65 | Train Loss: 0.4270943 Vali Loss: 1.0103575 Test Loss: 0.4414091
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_90_192_FITS_ETTh1_ftM_sl90_ll48_pl192_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.44134271144866943, mae:0.42723435163497925, rse:0.6308779120445251, corr:[0.26218668 0.26472124 0.26395917 0.2614064  0.25856134 0.25765717
 0.25815582 0.25748286 0.25601375 0.25545058 0.25571695 0.25609496
 0.25567427 0.25458512 0.25405243 0.25440022 0.25500122 0.2551663
 0.254908   0.25486228 0.25488845 0.25471896 0.25398663 0.2528499
 0.2513418  0.25108    0.25155303 0.25162587 0.25115335 0.25125012
 0.25200298 0.25206554 0.25158063 0.25116244 0.25099617 0.25118834
 0.2513524  0.25117862 0.2510645  0.25134304 0.25190148 0.25225982
 0.25227615 0.25232723 0.2525054  0.25238758 0.25202584 0.25126478
 0.24988821 0.249293   0.24891557 0.24780151 0.24599072 0.24489139
 0.24534506 0.2454627  0.24494441 0.24460094 0.24450988 0.24469739
 0.24465011 0.24423517 0.24391638 0.24397527 0.24444185 0.24484876
 0.24502088 0.24499194 0.24496673 0.24491246 0.24444821 0.2431146
 0.2411491  0.24025619 0.24013002 0.2398532  0.23906483 0.23852934
 0.23893468 0.23896226 0.23864497 0.2382992  0.23805904 0.23807009
 0.23811762 0.23788781 0.2377331  0.23785946 0.23819736 0.23845942
 0.23841797 0.23838438 0.23845676 0.23846403 0.23827343 0.2375213
 0.23625718 0.2358985  0.23605148 0.23558144 0.23488489 0.2348175
 0.23543355 0.23547979 0.2350712  0.23478311 0.23470363 0.23477809
 0.23476772 0.23447433 0.23436111 0.2345103  0.23488276 0.23502576
 0.23503457 0.23515941 0.2353534  0.23525447 0.23460056 0.23330264
 0.2315014  0.23059145 0.23003286 0.22901346 0.22801603 0.22797455
 0.22903126 0.22944564 0.2292157  0.228919   0.22877629 0.22889718
 0.22892053 0.22855306 0.22827487 0.22827655 0.22862491 0.22881502
 0.22879466 0.22887436 0.22900313 0.22896837 0.22851379 0.22734767
 0.22552145 0.22467771 0.22455004 0.22371067 0.22256397 0.22241537
 0.22364825 0.2242389  0.22420508 0.22405899 0.22401784 0.2241292
 0.22397296 0.22346239 0.22303922 0.2230275  0.22327617 0.22337969
 0.22324331 0.22318445 0.22328833 0.22326498 0.22277944 0.22151059
 0.21978748 0.21930242 0.21963403 0.21937966 0.21817495 0.2177125
 0.21873628 0.21926302 0.21915111 0.2191232  0.21936297 0.21957594
 0.21928674 0.21822923 0.21737716 0.21734768 0.21808317 0.21871684
 0.21826917 0.21709901 0.21665731 0.21774295 0.21915048 0.21653952]
