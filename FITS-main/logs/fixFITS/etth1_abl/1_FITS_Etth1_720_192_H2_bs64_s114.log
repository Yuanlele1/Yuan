Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=72, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=72, out_features=91, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5870592.0
params:  6643.0
Trainable parameters:  6643
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.975341796875
Epoch: 1, Steps: 60 | Train Loss: 0.6718039 Vali Loss: 1.2713408 Test Loss: 0.5994902
Validation loss decreased (inf --> 1.271341).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.8865387439727783
Epoch: 2, Steps: 60 | Train Loss: 0.4985194 Vali Loss: 1.1010027 Test Loss: 0.4944131
Validation loss decreased (1.271341 --> 1.101003).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.915273427963257
Epoch: 3, Steps: 60 | Train Loss: 0.4474794 Vali Loss: 1.0419848 Test Loss: 0.4584673
Validation loss decreased (1.101003 --> 1.041985).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.0180418491363525
Epoch: 4, Steps: 60 | Train Loss: 0.4270077 Vali Loss: 1.0159785 Test Loss: 0.4446507
Validation loss decreased (1.041985 --> 1.015978).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.87288236618042
Epoch: 5, Steps: 60 | Train Loss: 0.4173676 Vali Loss: 1.0028177 Test Loss: 0.4394504
Validation loss decreased (1.015978 --> 1.002818).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.8959736824035645
Epoch: 6, Steps: 60 | Train Loss: 0.4122823 Vali Loss: 0.9956851 Test Loss: 0.4381726
Validation loss decreased (1.002818 --> 0.995685).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.108179330825806
Epoch: 7, Steps: 60 | Train Loss: 0.4090075 Vali Loss: 0.9910348 Test Loss: 0.4373610
Validation loss decreased (0.995685 --> 0.991035).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.050295352935791
Epoch: 8, Steps: 60 | Train Loss: 0.4066228 Vali Loss: 0.9878806 Test Loss: 0.4374598
Validation loss decreased (0.991035 --> 0.987881).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.8350822925567627
Epoch: 9, Steps: 60 | Train Loss: 0.4046154 Vali Loss: 0.9859262 Test Loss: 0.4374377
Validation loss decreased (0.987881 --> 0.985926).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.9475719928741455
Epoch: 10, Steps: 60 | Train Loss: 0.4036900 Vali Loss: 0.9839727 Test Loss: 0.4376278
Validation loss decreased (0.985926 --> 0.983973).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.027559995651245
Epoch: 11, Steps: 60 | Train Loss: 0.4027372 Vali Loss: 0.9825379 Test Loss: 0.4374685
Validation loss decreased (0.983973 --> 0.982538).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.011876106262207
Epoch: 12, Steps: 60 | Train Loss: 0.4017677 Vali Loss: 0.9827302 Test Loss: 0.4376362
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.8134188652038574
Epoch: 13, Steps: 60 | Train Loss: 0.4010530 Vali Loss: 0.9810141 Test Loss: 0.4377911
Validation loss decreased (0.982538 --> 0.981014).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.871845006942749
Epoch: 14, Steps: 60 | Train Loss: 0.4005225 Vali Loss: 0.9808110 Test Loss: 0.4378351
Validation loss decreased (0.981014 --> 0.980811).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.000967264175415
Epoch: 15, Steps: 60 | Train Loss: 0.3995113 Vali Loss: 0.9803134 Test Loss: 0.4380514
Validation loss decreased (0.980811 --> 0.980313).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.9782607555389404
Epoch: 16, Steps: 60 | Train Loss: 0.3990749 Vali Loss: 0.9797279 Test Loss: 0.4382170
Validation loss decreased (0.980313 --> 0.979728).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.022536754608154
Epoch: 17, Steps: 60 | Train Loss: 0.3988352 Vali Loss: 0.9795598 Test Loss: 0.4381344
Validation loss decreased (0.979728 --> 0.979560).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.860313892364502
Epoch: 18, Steps: 60 | Train Loss: 0.3981944 Vali Loss: 0.9786608 Test Loss: 0.4381951
Validation loss decreased (0.979560 --> 0.978661).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.862117052078247
Epoch: 19, Steps: 60 | Train Loss: 0.3976878 Vali Loss: 0.9788496 Test Loss: 0.4383826
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.018240690231323
Epoch: 20, Steps: 60 | Train Loss: 0.3972686 Vali Loss: 0.9782734 Test Loss: 0.4383880
Validation loss decreased (0.978661 --> 0.978273).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.7961671352386475
Epoch: 21, Steps: 60 | Train Loss: 0.3971312 Vali Loss: 0.9783870 Test Loss: 0.4384734
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.9337799549102783
Epoch: 22, Steps: 60 | Train Loss: 0.3970180 Vali Loss: 0.9774910 Test Loss: 0.4384629
Validation loss decreased (0.978273 --> 0.977491).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.834965467453003
Epoch: 23, Steps: 60 | Train Loss: 0.3968472 Vali Loss: 0.9773358 Test Loss: 0.4384143
Validation loss decreased (0.977491 --> 0.977336).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.104532480239868
Epoch: 24, Steps: 60 | Train Loss: 0.3966444 Vali Loss: 0.9777041 Test Loss: 0.4385865
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.9847004413604736
Epoch: 25, Steps: 60 | Train Loss: 0.3959121 Vali Loss: 0.9774272 Test Loss: 0.4385358
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.254683256149292
Epoch: 26, Steps: 60 | Train Loss: 0.3961471 Vali Loss: 0.9766994 Test Loss: 0.4385542
Validation loss decreased (0.977336 --> 0.976699).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.9446704387664795
Epoch: 27, Steps: 60 | Train Loss: 0.3960624 Vali Loss: 0.9769289 Test Loss: 0.4387393
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.9573512077331543
Epoch: 28, Steps: 60 | Train Loss: 0.3959414 Vali Loss: 0.9767876 Test Loss: 0.4387147
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.9877750873565674
Epoch: 29, Steps: 60 | Train Loss: 0.3957417 Vali Loss: 0.9762370 Test Loss: 0.4385897
Validation loss decreased (0.976699 --> 0.976237).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.9987080097198486
Epoch: 30, Steps: 60 | Train Loss: 0.3951938 Vali Loss: 0.9765450 Test Loss: 0.4387493
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.137953042984009
Epoch: 31, Steps: 60 | Train Loss: 0.3954446 Vali Loss: 0.9764991 Test Loss: 0.4386393
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.8616302013397217
Epoch: 32, Steps: 60 | Train Loss: 0.3952977 Vali Loss: 0.9764025 Test Loss: 0.4387098
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.951860189437866
Epoch: 33, Steps: 60 | Train Loss: 0.3950899 Vali Loss: 0.9764142 Test Loss: 0.4386961
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.9046332836151123
Epoch: 34, Steps: 60 | Train Loss: 0.3949666 Vali Loss: 0.9762984 Test Loss: 0.4388742
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.12122654914856
Epoch: 35, Steps: 60 | Train Loss: 0.3949355 Vali Loss: 0.9756535 Test Loss: 0.4388889
Validation loss decreased (0.976237 --> 0.975654).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.7041914463043213
Epoch: 36, Steps: 60 | Train Loss: 0.3949701 Vali Loss: 0.9761913 Test Loss: 0.4388776
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.014228820800781
Epoch: 37, Steps: 60 | Train Loss: 0.3947858 Vali Loss: 0.9763756 Test Loss: 0.4389450
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.8369534015655518
Epoch: 38, Steps: 60 | Train Loss: 0.3944186 Vali Loss: 0.9762247 Test Loss: 0.4389620
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.089975833892822
Epoch: 39, Steps: 60 | Train Loss: 0.3945339 Vali Loss: 0.9760052 Test Loss: 0.4390033
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.8366990089416504
Epoch: 40, Steps: 60 | Train Loss: 0.3945424 Vali Loss: 0.9760312 Test Loss: 0.4389250
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.749166250228882
Epoch: 41, Steps: 60 | Train Loss: 0.3941439 Vali Loss: 0.9759278 Test Loss: 0.4389428
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.9400391578674316
Epoch: 42, Steps: 60 | Train Loss: 0.3944943 Vali Loss: 0.9759991 Test Loss: 0.4390426
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.889740228652954
Epoch: 43, Steps: 60 | Train Loss: 0.3946957 Vali Loss: 0.9756579 Test Loss: 0.4390537
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.8627967834472656
Epoch: 44, Steps: 60 | Train Loss: 0.3946005 Vali Loss: 0.9757983 Test Loss: 0.4390966
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.034712791442871
Epoch: 45, Steps: 60 | Train Loss: 0.3943589 Vali Loss: 0.9756501 Test Loss: 0.4390464
Validation loss decreased (0.975654 --> 0.975650).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.8707351684570312
Epoch: 46, Steps: 60 | Train Loss: 0.3941153 Vali Loss: 0.9757152 Test Loss: 0.4390334
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.1041858196258545
Epoch: 47, Steps: 60 | Train Loss: 0.3938720 Vali Loss: 0.9755951 Test Loss: 0.4390831
Validation loss decreased (0.975650 --> 0.975595).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.8922340869903564
Epoch: 48, Steps: 60 | Train Loss: 0.3941294 Vali Loss: 0.9757560 Test Loss: 0.4391063
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.8752920627593994
Epoch: 49, Steps: 60 | Train Loss: 0.3940242 Vali Loss: 0.9757013 Test Loss: 0.4391154
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.8705596923828125
Epoch: 50, Steps: 60 | Train Loss: 0.3941467 Vali Loss: 0.9749309 Test Loss: 0.4391006
Validation loss decreased (0.975595 --> 0.974931).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.04536247253418
Epoch: 51, Steps: 60 | Train Loss: 0.3942637 Vali Loss: 0.9752091 Test Loss: 0.4391048
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.8891499042510986
Epoch: 52, Steps: 60 | Train Loss: 0.3941565 Vali Loss: 0.9753578 Test Loss: 0.4391085
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.9173271656036377
Epoch: 53, Steps: 60 | Train Loss: 0.3939612 Vali Loss: 0.9757206 Test Loss: 0.4391084
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.8491625785827637
Epoch: 54, Steps: 60 | Train Loss: 0.3938957 Vali Loss: 0.9756439 Test Loss: 0.4391006
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.050134658813477
Epoch: 55, Steps: 60 | Train Loss: 0.3940790 Vali Loss: 0.9757113 Test Loss: 0.4392077
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 4.107676267623901
Epoch: 56, Steps: 60 | Train Loss: 0.3937917 Vali Loss: 0.9755132 Test Loss: 0.4391439
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.2375078201293945
Epoch: 57, Steps: 60 | Train Loss: 0.3940561 Vali Loss: 0.9749715 Test Loss: 0.4391912
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 4.155963897705078
Epoch: 58, Steps: 60 | Train Loss: 0.3942779 Vali Loss: 0.9754228 Test Loss: 0.4391415
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.138892650604248
Epoch: 59, Steps: 60 | Train Loss: 0.3938907 Vali Loss: 0.9752093 Test Loss: 0.4391226
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.8636457920074463
Epoch: 60, Steps: 60 | Train Loss: 0.3939956 Vali Loss: 0.9753647 Test Loss: 0.4391865
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 4.11858344078064
Epoch: 61, Steps: 60 | Train Loss: 0.3939422 Vali Loss: 0.9752033 Test Loss: 0.4391711
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 4.150981426239014
Epoch: 62, Steps: 60 | Train Loss: 0.3940129 Vali Loss: 0.9755525 Test Loss: 0.4391937
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 4.191399097442627
Epoch: 63, Steps: 60 | Train Loss: 0.3937987 Vali Loss: 0.9751838 Test Loss: 0.4391920
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 4.118052959442139
Epoch: 64, Steps: 60 | Train Loss: 0.3936063 Vali Loss: 0.9754627 Test Loss: 0.4391989
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 4.0827696323394775
Epoch: 65, Steps: 60 | Train Loss: 0.3937599 Vali Loss: 0.9755614 Test Loss: 0.4392176
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 4.406361103057861
Epoch: 66, Steps: 60 | Train Loss: 0.3939113 Vali Loss: 0.9751088 Test Loss: 0.4392097
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 4.12421727180481
Epoch: 67, Steps: 60 | Train Loss: 0.3941096 Vali Loss: 0.9754733 Test Loss: 0.4392253
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 4.072055339813232
Epoch: 68, Steps: 60 | Train Loss: 0.3939923 Vali Loss: 0.9752823 Test Loss: 0.4392054
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.9617583751678467
Epoch: 69, Steps: 60 | Train Loss: 0.3937153 Vali Loss: 0.9755000 Test Loss: 0.4392292
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.338363885879517
Epoch: 70, Steps: 60 | Train Loss: 0.3938603 Vali Loss: 0.9750564 Test Loss: 0.4392023
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4358600080013275, mae:0.44371283054351807, rse:0.6269469857215881, corr:[0.26134932 0.265516   0.26728535 0.26636794 0.26394436 0.26174635
 0.26044196 0.25991884 0.25976732 0.25993168 0.26021227 0.26039162
 0.26045397 0.26044568 0.26049784 0.260565   0.26044047 0.26014417
 0.25976092 0.2594276  0.25923878 0.2592937  0.25929147 0.2592507
 0.25909653 0.2588586  0.25850025 0.25804946 0.25760937 0.25727224
 0.25713006 0.25708324 0.25714248 0.25727612 0.2573247  0.25734106
 0.25741464 0.2575058  0.25752366 0.25746536 0.25744632 0.2574605
 0.2575261  0.2576596  0.25785607 0.25802046 0.25800237 0.25768924
 0.2568261  0.25574395 0.25452554 0.25339282 0.25251204 0.2518324
 0.25138864 0.25117037 0.25100994 0.2508681  0.25065964 0.25041124
 0.25023487 0.25015298 0.2501747  0.25035474 0.25053984 0.25065348
 0.25069407 0.25053212 0.25026345 0.25001445 0.24984945 0.24966459
 0.24942419 0.24908487 0.2486496  0.24823499 0.2478416  0.24741353
 0.24702467 0.24672055 0.24649136 0.24628085 0.24605092 0.24579589
 0.24565578 0.2455784  0.24550165 0.24539137 0.24521236 0.24504438
 0.24492513 0.24494714 0.24512674 0.24552205 0.246087   0.2465715
 0.2469309  0.24709165 0.24690382 0.24649991 0.24604492 0.2456497
 0.24541211 0.24532124 0.24526489 0.2452554  0.24516477 0.24494052
 0.24473388 0.24461184 0.24462354 0.24480455 0.2449415  0.24496974
 0.2449888  0.24494632 0.24477789 0.24448648 0.24406096 0.24347198
 0.24277203 0.24196695 0.24115317 0.24050748 0.24010195 0.23983684
 0.23969255 0.2395495  0.23924153 0.238848   0.23842329 0.23805515
 0.23780422 0.23764972 0.23761614 0.23766191 0.23778057 0.2378487
 0.23784028 0.23766184 0.23738748 0.23709379 0.23676147 0.23615123
 0.2353223  0.2343334  0.23321894 0.2320734  0.23120716 0.23059034
 0.23022307 0.2300745  0.23001462 0.2300098  0.2298794  0.22960752
 0.2292534  0.22896364 0.22883677 0.22884332 0.22879039 0.22887449
 0.22894384 0.22884448 0.22858828 0.22814803 0.22761993 0.22690873
 0.22621511 0.22570324 0.22530288 0.2250337  0.22478954 0.22453187
 0.22419988 0.22382447 0.22320056 0.22254504 0.22194248 0.22152132
 0.22134966 0.22129029 0.22114287 0.22079986 0.22034362 0.22000183
 0.21991976 0.22022767 0.22122201 0.22258165 0.22253674 0.21826868]
