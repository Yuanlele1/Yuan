Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=514, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_96_FITS_ETTh1_ftM_sl360_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8185
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12726784.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.3915956020355225
Epoch: 1, Steps: 63 | Train Loss: 0.5832108 Vali Loss: 1.0669012 Test Loss: 0.5490746
Validation loss decreased (inf --> 1.066901).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3329250812530518
Epoch: 2, Steps: 63 | Train Loss: 0.4334642 Vali Loss: 0.8876769 Test Loss: 0.4467412
Validation loss decreased (1.066901 --> 0.887677).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.332153081893921
Epoch: 3, Steps: 63 | Train Loss: 0.3857610 Vali Loss: 0.8101103 Test Loss: 0.4048609
Validation loss decreased (0.887677 --> 0.810110).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4072389602661133
Epoch: 4, Steps: 63 | Train Loss: 0.3647915 Vali Loss: 0.7715813 Test Loss: 0.3867534
Validation loss decreased (0.810110 --> 0.771581).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.2450158596038818
Epoch: 5, Steps: 63 | Train Loss: 0.3547306 Vali Loss: 0.7484370 Test Loss: 0.3796506
Validation loss decreased (0.771581 --> 0.748437).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.4815971851348877
Epoch: 6, Steps: 63 | Train Loss: 0.3494873 Vali Loss: 0.7334068 Test Loss: 0.3770390
Validation loss decreased (0.748437 --> 0.733407).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4946660995483398
Epoch: 7, Steps: 63 | Train Loss: 0.3466803 Vali Loss: 0.7238437 Test Loss: 0.3754231
Validation loss decreased (0.733407 --> 0.723844).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5072438716888428
Epoch: 8, Steps: 63 | Train Loss: 0.3448207 Vali Loss: 0.7191368 Test Loss: 0.3748493
Validation loss decreased (0.723844 --> 0.719137).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.539297342300415
Epoch: 9, Steps: 63 | Train Loss: 0.3438812 Vali Loss: 0.7136140 Test Loss: 0.3745930
Validation loss decreased (0.719137 --> 0.713614).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.3888497352600098
Epoch: 10, Steps: 63 | Train Loss: 0.3423000 Vali Loss: 0.7069480 Test Loss: 0.3742719
Validation loss decreased (0.713614 --> 0.706948).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.3464553356170654
Epoch: 11, Steps: 63 | Train Loss: 0.3418203 Vali Loss: 0.7070069 Test Loss: 0.3742086
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.3874340057373047
Epoch: 12, Steps: 63 | Train Loss: 0.3417831 Vali Loss: 0.7002990 Test Loss: 0.3738904
Validation loss decreased (0.706948 --> 0.700299).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.4035727977752686
Epoch: 13, Steps: 63 | Train Loss: 0.3411443 Vali Loss: 0.6957104 Test Loss: 0.3739078
Validation loss decreased (0.700299 --> 0.695710).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.29295015335083
Epoch: 14, Steps: 63 | Train Loss: 0.3409553 Vali Loss: 0.6946442 Test Loss: 0.3735382
Validation loss decreased (0.695710 --> 0.694644).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.2566821575164795
Epoch: 15, Steps: 63 | Train Loss: 0.3403748 Vali Loss: 0.6966425 Test Loss: 0.3736545
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.396118402481079
Epoch: 16, Steps: 63 | Train Loss: 0.3400305 Vali Loss: 0.6954636 Test Loss: 0.3734286
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.2898445129394531
Epoch: 17, Steps: 63 | Train Loss: 0.3403338 Vali Loss: 0.6964467 Test Loss: 0.3733628
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4325082302093506
Epoch: 18, Steps: 63 | Train Loss: 0.3398253 Vali Loss: 0.6883844 Test Loss: 0.3735132
Validation loss decreased (0.694644 --> 0.688384).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4359815120697021
Epoch: 19, Steps: 63 | Train Loss: 0.3390380 Vali Loss: 0.6924566 Test Loss: 0.3733137
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3827564716339111
Epoch: 20, Steps: 63 | Train Loss: 0.3391131 Vali Loss: 0.6899555 Test Loss: 0.3734608
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4338929653167725
Epoch: 21, Steps: 63 | Train Loss: 0.3384909 Vali Loss: 0.6883762 Test Loss: 0.3732668
Validation loss decreased (0.688384 --> 0.688376).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.397989273071289
Epoch: 22, Steps: 63 | Train Loss: 0.3384409 Vali Loss: 0.6871455 Test Loss: 0.3732678
Validation loss decreased (0.688376 --> 0.687146).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.37034010887146
Epoch: 23, Steps: 63 | Train Loss: 0.3386230 Vali Loss: 0.6913024 Test Loss: 0.3730811
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4006900787353516
Epoch: 24, Steps: 63 | Train Loss: 0.3382692 Vali Loss: 0.6885427 Test Loss: 0.3731647
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.4052250385284424
Epoch: 25, Steps: 63 | Train Loss: 0.3382722 Vali Loss: 0.6854791 Test Loss: 0.3731096
Validation loss decreased (0.687146 --> 0.685479).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4722740650177002
Epoch: 26, Steps: 63 | Train Loss: 0.3385343 Vali Loss: 0.6888401 Test Loss: 0.3732433
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.50555419921875
Epoch: 27, Steps: 63 | Train Loss: 0.3378208 Vali Loss: 0.6868961 Test Loss: 0.3730950
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.344200611114502
Epoch: 28, Steps: 63 | Train Loss: 0.3383421 Vali Loss: 0.6817161 Test Loss: 0.3731614
Validation loss decreased (0.685479 --> 0.681716).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.3986420631408691
Epoch: 29, Steps: 63 | Train Loss: 0.3376506 Vali Loss: 0.6805751 Test Loss: 0.3731308
Validation loss decreased (0.681716 --> 0.680575).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.3912830352783203
Epoch: 30, Steps: 63 | Train Loss: 0.3375302 Vali Loss: 0.6828750 Test Loss: 0.3731249
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.3961188793182373
Epoch: 31, Steps: 63 | Train Loss: 0.3372878 Vali Loss: 0.6857465 Test Loss: 0.3731588
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.3677492141723633
Epoch: 32, Steps: 63 | Train Loss: 0.3375904 Vali Loss: 0.6865035 Test Loss: 0.3730843
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.2929391860961914
Epoch: 33, Steps: 63 | Train Loss: 0.3378436 Vali Loss: 0.6867585 Test Loss: 0.3730795
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3633582592010498
Epoch: 34, Steps: 63 | Train Loss: 0.3380211 Vali Loss: 0.6819843 Test Loss: 0.3730897
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.277369499206543
Epoch: 35, Steps: 63 | Train Loss: 0.3377965 Vali Loss: 0.6809392 Test Loss: 0.3731252
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3312876224517822
Epoch: 36, Steps: 63 | Train Loss: 0.3377743 Vali Loss: 0.6835880 Test Loss: 0.3730483
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.4168977737426758
Epoch: 37, Steps: 63 | Train Loss: 0.3379603 Vali Loss: 0.6795239 Test Loss: 0.3729872
Validation loss decreased (0.680575 --> 0.679524).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.3096225261688232
Epoch: 38, Steps: 63 | Train Loss: 0.3376634 Vali Loss: 0.6821849 Test Loss: 0.3729928
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.340691089630127
Epoch: 39, Steps: 63 | Train Loss: 0.3373292 Vali Loss: 0.6852765 Test Loss: 0.3730052
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.2985007762908936
Epoch: 40, Steps: 63 | Train Loss: 0.3380722 Vali Loss: 0.6818396 Test Loss: 0.3730306
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.2511491775512695
Epoch: 41, Steps: 63 | Train Loss: 0.3366965 Vali Loss: 0.6817846 Test Loss: 0.3730936
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3311564922332764
Epoch: 42, Steps: 63 | Train Loss: 0.3373294 Vali Loss: 0.6846469 Test Loss: 0.3730787
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.2389106750488281
Epoch: 43, Steps: 63 | Train Loss: 0.3374907 Vali Loss: 0.6760251 Test Loss: 0.3730929
Validation loss decreased (0.679524 --> 0.676025).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.3409953117370605
Epoch: 44, Steps: 63 | Train Loss: 0.3373732 Vali Loss: 0.6823144 Test Loss: 0.3730820
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.359952449798584
Epoch: 45, Steps: 63 | Train Loss: 0.3374564 Vali Loss: 0.6831509 Test Loss: 0.3731092
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.3451144695281982
Epoch: 46, Steps: 63 | Train Loss: 0.3372628 Vali Loss: 0.6815940 Test Loss: 0.3730614
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.392697811126709
Epoch: 47, Steps: 63 | Train Loss: 0.3372091 Vali Loss: 0.6778475 Test Loss: 0.3730577
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.3764193058013916
Epoch: 48, Steps: 63 | Train Loss: 0.3368484 Vali Loss: 0.6768550 Test Loss: 0.3731122
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.3982536792755127
Epoch: 49, Steps: 63 | Train Loss: 0.3369417 Vali Loss: 0.6805755 Test Loss: 0.3730688
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.4032673835754395
Epoch: 50, Steps: 63 | Train Loss: 0.3371357 Vali Loss: 0.6811122 Test Loss: 0.3730467
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3976352214813232
Epoch: 51, Steps: 63 | Train Loss: 0.3373691 Vali Loss: 0.6815522 Test Loss: 0.3730631
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.372525691986084
Epoch: 52, Steps: 63 | Train Loss: 0.3372253 Vali Loss: 0.6801854 Test Loss: 0.3731044
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.37060546875
Epoch: 53, Steps: 63 | Train Loss: 0.3372711 Vali Loss: 0.6820552 Test Loss: 0.3730979
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.4937145709991455
Epoch: 54, Steps: 63 | Train Loss: 0.3370716 Vali Loss: 0.6827077 Test Loss: 0.3730620
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.39237380027771
Epoch: 55, Steps: 63 | Train Loss: 0.3369484 Vali Loss: 0.6804712 Test Loss: 0.3731017
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.3464772701263428
Epoch: 56, Steps: 63 | Train Loss: 0.3371875 Vali Loss: 0.6785541 Test Loss: 0.3730874
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.384281873703003
Epoch: 57, Steps: 63 | Train Loss: 0.3371782 Vali Loss: 0.6781736 Test Loss: 0.3730916
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.3631165027618408
Epoch: 58, Steps: 63 | Train Loss: 0.3366240 Vali Loss: 0.6812329 Test Loss: 0.3730747
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.3651981353759766
Epoch: 59, Steps: 63 | Train Loss: 0.3370938 Vali Loss: 0.6782290 Test Loss: 0.3730784
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.4376466274261475
Epoch: 60, Steps: 63 | Train Loss: 0.3372411 Vali Loss: 0.6798081 Test Loss: 0.3730584
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.356813669204712
Epoch: 61, Steps: 63 | Train Loss: 0.3372408 Vali Loss: 0.6820872 Test Loss: 0.3730461
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.4286580085754395
Epoch: 62, Steps: 63 | Train Loss: 0.3370157 Vali Loss: 0.6813288 Test Loss: 0.3730675
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.3628809452056885
Epoch: 63, Steps: 63 | Train Loss: 0.3366145 Vali Loss: 0.6816766 Test Loss: 0.3730782
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_96_FITS_ETTh1_ftM_sl360_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.372130811214447, mae:0.3945476710796356, rse:0.5794362425804138, corr:[0.26902506 0.27800322 0.27822402 0.27955386 0.27693647 0.27477083
 0.2744609  0.2740219  0.27324784 0.2733271  0.27331188 0.2727176
 0.27259716 0.27260864 0.27222264 0.27222434 0.2725357  0.2722631
 0.27183914 0.2717381  0.27160618 0.27127206 0.27145392 0.27193284
 0.2715709  0.2710403  0.2709825  0.2708492  0.2703354  0.26996085
 0.26967573 0.26906237 0.26860437 0.2685807  0.2685437  0.26828736
 0.26839596 0.26866004 0.2686285  0.26856855 0.2689217  0.26913452
 0.2692422  0.2692193  0.26885873 0.2683382  0.2683952  0.2687305
 0.26834676 0.26733896 0.26653728 0.26609454 0.26531926 0.2640922
 0.26342854 0.26309597 0.26269403 0.2625227  0.2623441  0.26233488
 0.26233944 0.26247615 0.26250228 0.2624591  0.26263285 0.26306805
 0.2633888  0.2632019  0.26302797 0.2632814  0.2635581  0.26344866
 0.26282582 0.26192534 0.26109493 0.26052332 0.26009327 0.25964633
 0.25926697 0.2586819  0.25809664 0.25778067 0.25765684 0.25747928
 0.2573399  0.2574702  0.25756264 0.25727856 0.25734267 0.25750735
 0.25673822 0.25611848 0.2562544  0.25503567 0.2549301  0.25762054]
