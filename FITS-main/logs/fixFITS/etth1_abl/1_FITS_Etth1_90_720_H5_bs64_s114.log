Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=30, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  8370.0
Trainable parameters:  8370
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.1462604999542236
Epoch: 1, Steps: 61 | Train Loss: 1.5367856 Vali Loss: 2.8099406 Test Loss: 1.4640911
Validation loss decreased (inf --> 2.809941).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.280843496322632
Epoch: 2, Steps: 61 | Train Loss: 1.1116003 Vali Loss: 2.2545938 Test Loss: 1.0072713
Validation loss decreased (2.809941 --> 2.254594).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.239952564239502
Epoch: 3, Steps: 61 | Train Loss: 0.9064752 Vali Loss: 1.9878578 Test Loss: 0.7895002
Validation loss decreased (2.254594 --> 1.987858).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.541078567504883
Epoch: 4, Steps: 61 | Train Loss: 0.7971763 Vali Loss: 1.8473666 Test Loss: 0.6712289
Validation loss decreased (1.987858 --> 1.847367).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.937131404876709
Epoch: 5, Steps: 61 | Train Loss: 0.7344969 Vali Loss: 1.7678694 Test Loss: 0.6027181
Validation loss decreased (1.847367 --> 1.767869).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2494678497314453
Epoch: 6, Steps: 61 | Train Loss: 0.6965252 Vali Loss: 1.7140574 Test Loss: 0.5604858
Validation loss decreased (1.767869 --> 1.714057).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.4910788536071777
Epoch: 7, Steps: 61 | Train Loss: 0.6724272 Vali Loss: 1.6786268 Test Loss: 0.5336505
Validation loss decreased (1.714057 --> 1.678627).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.1210384368896484
Epoch: 8, Steps: 61 | Train Loss: 0.6564565 Vali Loss: 1.6543832 Test Loss: 0.5160294
Validation loss decreased (1.678627 --> 1.654383).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.937490224838257
Epoch: 9, Steps: 61 | Train Loss: 0.6460158 Vali Loss: 1.6352909 Test Loss: 0.5041646
Validation loss decreased (1.654383 --> 1.635291).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.941401481628418
Epoch: 10, Steps: 61 | Train Loss: 0.6385262 Vali Loss: 1.6225688 Test Loss: 0.4959454
Validation loss decreased (1.635291 --> 1.622569).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.597602367401123
Epoch: 11, Steps: 61 | Train Loss: 0.6333367 Vali Loss: 1.6206968 Test Loss: 0.4901403
Validation loss decreased (1.622569 --> 1.620697).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.7076003551483154
Epoch: 12, Steps: 61 | Train Loss: 0.6293000 Vali Loss: 1.6044891 Test Loss: 0.4859715
Validation loss decreased (1.620697 --> 1.604489).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.248680353164673
Epoch: 13, Steps: 61 | Train Loss: 0.6264203 Vali Loss: 1.6003945 Test Loss: 0.4828431
Validation loss decreased (1.604489 --> 1.600394).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.2539074420928955
Epoch: 14, Steps: 61 | Train Loss: 0.6236801 Vali Loss: 1.5993010 Test Loss: 0.4804837
Validation loss decreased (1.600394 --> 1.599301).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.7875277996063232
Epoch: 15, Steps: 61 | Train Loss: 0.6214880 Vali Loss: 1.5931317 Test Loss: 0.4787479
Validation loss decreased (1.599301 --> 1.593132).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.109158515930176
Epoch: 16, Steps: 61 | Train Loss: 0.6204585 Vali Loss: 1.5982084 Test Loss: 0.4773566
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.773510456085205
Epoch: 17, Steps: 61 | Train Loss: 0.6196606 Vali Loss: 1.5939960 Test Loss: 0.4762941
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.8829851150512695
Epoch: 18, Steps: 61 | Train Loss: 0.6184259 Vali Loss: 1.5876989 Test Loss: 0.4754375
Validation loss decreased (1.593132 --> 1.587699).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.080833673477173
Epoch: 19, Steps: 61 | Train Loss: 0.6175072 Vali Loss: 1.5867969 Test Loss: 0.4747105
Validation loss decreased (1.587699 --> 1.586797).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.8126494884490967
Epoch: 20, Steps: 61 | Train Loss: 0.6167634 Vali Loss: 1.5909449 Test Loss: 0.4741763
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.5019471645355225
Epoch: 21, Steps: 61 | Train Loss: 0.6161182 Vali Loss: 1.5792798 Test Loss: 0.4736762
Validation loss decreased (1.586797 --> 1.579280).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.0775468349456787
Epoch: 22, Steps: 61 | Train Loss: 0.6156419 Vali Loss: 1.5878023 Test Loss: 0.4733174
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.3668205738067627
Epoch: 23, Steps: 61 | Train Loss: 0.6151795 Vali Loss: 1.5819497 Test Loss: 0.4729848
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.6730165481567383
Epoch: 24, Steps: 61 | Train Loss: 0.6146289 Vali Loss: 1.5824986 Test Loss: 0.4727227
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.5819602012634277
Epoch: 25, Steps: 61 | Train Loss: 0.6144326 Vali Loss: 1.5860598 Test Loss: 0.4724505
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.9080069065093994
Epoch: 26, Steps: 61 | Train Loss: 0.6141582 Vali Loss: 1.5779753 Test Loss: 0.4722449
Validation loss decreased (1.579280 --> 1.577975).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.189030885696411
Epoch: 27, Steps: 61 | Train Loss: 0.6134243 Vali Loss: 1.5805707 Test Loss: 0.4721119
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.1014835834503174
Epoch: 28, Steps: 61 | Train Loss: 0.6131950 Vali Loss: 1.5731690 Test Loss: 0.4719021
Validation loss decreased (1.577975 --> 1.573169).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.5753703117370605
Epoch: 29, Steps: 61 | Train Loss: 0.6129236 Vali Loss: 1.5832710 Test Loss: 0.4718301
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.7048804759979248
Epoch: 30, Steps: 61 | Train Loss: 0.6127407 Vali Loss: 1.5647238 Test Loss: 0.4717185
Validation loss decreased (1.573169 --> 1.564724).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.10075044631958
Epoch: 31, Steps: 61 | Train Loss: 0.6124717 Vali Loss: 1.5767754 Test Loss: 0.4716221
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.014187812805176
Epoch: 32, Steps: 61 | Train Loss: 0.6123083 Vali Loss: 1.5787644 Test Loss: 0.4715374
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.5213632583618164
Epoch: 33, Steps: 61 | Train Loss: 0.6124227 Vali Loss: 1.5707676 Test Loss: 0.4714636
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.914018154144287
Epoch: 34, Steps: 61 | Train Loss: 0.6122379 Vali Loss: 1.5724388 Test Loss: 0.4713855
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.3317911624908447
Epoch: 35, Steps: 61 | Train Loss: 0.6119510 Vali Loss: 1.5709054 Test Loss: 0.4713521
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.37312912940979
Epoch: 36, Steps: 61 | Train Loss: 0.6114633 Vali Loss: 1.5841370 Test Loss: 0.4712943
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.562530994415283
Epoch: 37, Steps: 61 | Train Loss: 0.6115532 Vali Loss: 1.5785518 Test Loss: 0.4712527
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.1173250675201416
Epoch: 38, Steps: 61 | Train Loss: 0.6112719 Vali Loss: 1.5743716 Test Loss: 0.4711892
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.9750251770019531
Epoch: 39, Steps: 61 | Train Loss: 0.6109623 Vali Loss: 1.5795724 Test Loss: 0.4711583
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.928900957107544
Epoch: 40, Steps: 61 | Train Loss: 0.6110221 Vali Loss: 1.5810744 Test Loss: 0.4711333
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.245323657989502
Epoch: 41, Steps: 61 | Train Loss: 0.6110101 Vali Loss: 1.5721761 Test Loss: 0.4710868
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.7097880840301514
Epoch: 42, Steps: 61 | Train Loss: 0.6113947 Vali Loss: 1.5814685 Test Loss: 0.4710685
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9981746673583984
Epoch: 43, Steps: 61 | Train Loss: 0.6110583 Vali Loss: 1.5758486 Test Loss: 0.4710534
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.9060845375061035
Epoch: 44, Steps: 61 | Train Loss: 0.6109029 Vali Loss: 1.5748684 Test Loss: 0.4710351
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9589745998382568
Epoch: 45, Steps: 61 | Train Loss: 0.6107881 Vali Loss: 1.5714983 Test Loss: 0.4710031
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.237122058868408
Epoch: 46, Steps: 61 | Train Loss: 0.6110117 Vali Loss: 1.5750945 Test Loss: 0.4709893
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.9508235454559326
Epoch: 47, Steps: 61 | Train Loss: 0.6105048 Vali Loss: 1.5742803 Test Loss: 0.4709726
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.2694246768951416
Epoch: 48, Steps: 61 | Train Loss: 0.6106932 Vali Loss: 1.5727508 Test Loss: 0.4709589
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.6004648208618164
Epoch: 49, Steps: 61 | Train Loss: 0.6106926 Vali Loss: 1.5732495 Test Loss: 0.4709496
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.60915207862854
Epoch: 50, Steps: 61 | Train Loss: 0.6106471 Vali Loss: 1.5769277 Test Loss: 0.4709428
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4703908860683441, mae:0.46552738547325134, rse:0.6565706133842468, corr:[0.22722158 0.22776687 0.22536935 0.22909434 0.22496934 0.22275625
 0.22572453 0.22499023 0.22345567 0.22583035 0.224941   0.22236334
 0.22381327 0.22385742 0.2215819  0.22194748 0.22312002 0.22233258
 0.22231732 0.22320104 0.2221977  0.22159305 0.2219668  0.22173749
 0.2197304  0.21832323 0.21878886 0.21916339 0.21852985 0.21929052
 0.22020368 0.2194955  0.21912217 0.21950702 0.21867543 0.21823163
 0.21875606 0.21813741 0.2176774  0.21830295 0.21876451 0.21866193
 0.21930756 0.21964191 0.21917224 0.21940358 0.21967393 0.21903828
 0.21779864 0.21685821 0.21625759 0.21584459 0.21517123 0.21462265
 0.21465924 0.21433368 0.21436511 0.21445394 0.21364874 0.21334736
 0.21374173 0.21348056 0.21295011 0.21312253 0.21351841 0.21335998
 0.21375349 0.21420328 0.21369153 0.21327335 0.21292989 0.21207932
 0.21023138 0.20914651 0.20872399 0.20865172 0.20846571 0.20897935
 0.20967418 0.20940839 0.20940004 0.2093318  0.20855731 0.20826402
 0.2085258  0.20813833 0.20788486 0.20836648 0.20843309 0.20800598
 0.20829712 0.2086318  0.20835339 0.20856838 0.20855604 0.20801294
 0.2073077  0.2073978  0.2076024  0.20755754 0.20810728 0.20900133
 0.20971376 0.20973995 0.20959105 0.20941445 0.20890416 0.20861506
 0.20877682 0.2085401  0.20831193 0.20828098 0.20849231 0.20854053
 0.20901014 0.20926757 0.20900346 0.20900905 0.20888805 0.20786989
 0.2060534  0.20492397 0.2041192  0.20341335 0.2034379  0.20400369
 0.20513016 0.20564206 0.2058148  0.20571928 0.20523952 0.2050815
 0.20520537 0.20485412 0.20458364 0.20463389 0.20471705 0.20476991
 0.20539129 0.20576513 0.20530364 0.20522398 0.20538965 0.2049028
 0.20329577 0.20192952 0.20124575 0.20065294 0.2000764  0.20017593
 0.20100239 0.20115277 0.2013161  0.20154552 0.20122197 0.20083342
 0.20070481 0.20030019 0.19989017 0.20003788 0.20025198 0.2000548
 0.20030096 0.20060465 0.20045689 0.20070997 0.20052621 0.199314
 0.19785193 0.197542   0.1978839  0.1981033  0.1972709  0.19664678
 0.19747683 0.19821087 0.19844289 0.1981426  0.19778493 0.19783574
 0.19804552 0.19779679 0.19744584 0.19736987 0.19757721 0.19805557
 0.19877903 0.19902177 0.19871877 0.19895616 0.19933023 0.19846639
 0.1962895  0.19530487 0.19463034 0.19357328 0.19285876 0.19309855
 0.19378774 0.19427416 0.1947421  0.19472942 0.19431642 0.19392443
 0.19382587 0.19370748 0.1935769  0.19363621 0.19347437 0.19321682
 0.1936964  0.19417486 0.19392069 0.19372387 0.19337742 0.19238608
 0.19082807 0.1902505  0.1901041  0.19022036 0.19070196 0.19219236
 0.19420332 0.19514424 0.19531345 0.19527537 0.19527528 0.19534473
 0.19531558 0.19484788 0.19453011 0.19490388 0.19510303 0.19479705
 0.19484371 0.19501354 0.19505844 0.19522455 0.1949275  0.19373597
 0.19196874 0.190898   0.19065176 0.1906827  0.1905134  0.1910362
 0.19279422 0.19375966 0.19414343 0.19427821 0.1938624  0.19346179
 0.19353022 0.19334705 0.19269647 0.19250457 0.19286741 0.19305257
 0.19314404 0.19291015 0.1923427  0.19214211 0.19208923 0.19150177
 0.18963674 0.18828857 0.1880868  0.18890627 0.18937328 0.1903284
 0.19193807 0.19293009 0.19336824 0.19339862 0.19287133 0.19206831
 0.1917938  0.1918632  0.19165942 0.19151998 0.19135451 0.19138715
 0.19218008 0.19289465 0.19293496 0.1928499  0.19265993 0.19231656
 0.19187307 0.19202    0.19311221 0.19380054 0.19397573 0.19479844
 0.19603907 0.19635145 0.19614053 0.19584018 0.19568664 0.19527873
 0.19486347 0.1945444  0.19420454 0.19442867 0.19471815 0.19467433
 0.19495843 0.19500467 0.19461104 0.19499585 0.19539602 0.1944498
 0.19262981 0.19171323 0.19203474 0.19246629 0.19253576 0.1922679
 0.19304974 0.19401579 0.19467561 0.19442941 0.19380715 0.1934327
 0.19346997 0.1933066  0.1929418  0.19293958 0.19293079 0.1927493
 0.19295488 0.19320652 0.19286335 0.19259578 0.19249527 0.19179033
 0.19007528 0.18885571 0.18764204 0.1869812  0.18749864 0.1888038
 0.19021732 0.19088292 0.19147116 0.19168481 0.19154082 0.1910489
 0.19060959 0.19034764 0.19029239 0.1904694  0.19032294 0.19000226
 0.1900407  0.1901642  0.1899526  0.18977301 0.18915838 0.18781447
 0.18578573 0.18468404 0.18430287 0.18334232 0.18271595 0.18327901
 0.18546323 0.1864302  0.18653728 0.18632539 0.1861589  0.18633556
 0.18654513 0.18583968 0.18480746 0.18481515 0.18529293 0.18539533
 0.18550022 0.1854158  0.18508005 0.18514341 0.18489072 0.1840334
 0.18254709 0.18194471 0.18249172 0.1836089  0.18424112 0.1845599
 0.18630873 0.18723753 0.18762745 0.18808864 0.18793897 0.1873654
 0.18749966 0.18813287 0.18822874 0.18802816 0.18804204 0.18847509
 0.18944407 0.18993483 0.18947351 0.18925293 0.18949184 0.1894756
 0.18830667 0.18747804 0.1876897  0.18839285 0.1890244  0.1900223
 0.19200402 0.19264638 0.19258425 0.19282018 0.19312374 0.19287004
 0.19250065 0.19224112 0.19207887 0.19246532 0.1926872  0.19253957
 0.19288005 0.19332859 0.19328995 0.19359891 0.19358654 0.19284455
 0.19160081 0.19131695 0.1919309  0.19278868 0.19321199 0.1940777
 0.19582462 0.19683583 0.1972034  0.19721419 0.19692287 0.19665483
 0.19676441 0.19643423 0.1957796  0.19577968 0.19597517 0.1959888
 0.196183   0.19605371 0.19568187 0.195782   0.1957749  0.19526498
 0.1942106  0.19417936 0.19497111 0.19581416 0.19577952 0.19611354
 0.19759096 0.19821459 0.19853206 0.19884266 0.19867794 0.19828813
 0.19863226 0.19910139 0.19896732 0.19891408 0.19911785 0.19942991
 0.19985181 0.20003511 0.20000397 0.19965366 0.19880052 0.1980467
 0.19725247 0.19660923 0.19588342 0.19608739 0.19627702 0.1968819
 0.19877821 0.19899917 0.19842367 0.19863889 0.19914968 0.19902858
 0.1986104  0.19816518 0.19792613 0.19823593 0.19830365 0.19806807
 0.1983442  0.19847113 0.19826625 0.19864668 0.1991319  0.1991695
 0.19853826 0.19853266 0.19945069 0.20073633 0.20088892 0.20074084
 0.20177586 0.20230515 0.20251542 0.20249528 0.20211989 0.20192583
 0.20247693 0.20256256 0.20197956 0.20221113 0.20273873 0.20263126
 0.20291364 0.2034605  0.2034781  0.20365797 0.20401907 0.20392597
 0.20291005 0.20209773 0.2018535  0.20231582 0.202473   0.20257352
 0.20318545 0.2035679  0.20387143 0.20411617 0.20387556 0.20337465
 0.20325476 0.20347422 0.203374   0.2033027  0.20353228 0.20370756
 0.2038673  0.20376989 0.20337087 0.20315453 0.20293249 0.2021637
 0.20056899 0.19942953 0.1992311  0.1989531  0.19842851 0.1985383
 0.19953912 0.19985792 0.1999687  0.1999918  0.19972612 0.199429
 0.1994687  0.1992565  0.19872537 0.19872741 0.1990948  0.19909559
 0.19915813 0.19932634 0.19912656 0.19903949 0.19909178 0.19836807
 0.1965236  0.19570981 0.1957839  0.19589172 0.19574423 0.196068
 0.19725779 0.19768415 0.19798689 0.19827853 0.19798009 0.19760296
 0.19767329 0.19768997 0.19739196 0.19723307 0.19720922 0.19710451
 0.19724184 0.19734867 0.19698139 0.19663289 0.19638933 0.19567035
 0.19380127 0.19227685 0.19199155 0.19209595 0.19194609 0.19226116
 0.19341287 0.1937331  0.19360857 0.19379903 0.19384708 0.19361186
 0.19375356 0.19385923 0.19350095 0.193392   0.19369619 0.19370015
 0.19356532 0.19368038 0.19368535 0.193728   0.19368652 0.19303167
 0.19145486 0.19026963 0.18964005 0.18944547 0.18903045 0.18916744
 0.19035107 0.1904427  0.18991758 0.18998447 0.19002059 0.18983014
 0.18987738 0.18967573 0.18924822 0.18906479 0.1889778  0.18874703
 0.18886073 0.18914872 0.18889982 0.18890315 0.18931301 0.18897705
 0.18736494 0.18623109 0.18587692 0.18592611 0.18577795 0.18607011
 0.18714327 0.18717118 0.18701418 0.18693712 0.18662892 0.18630628
 0.18629101 0.18605334 0.18566018 0.18553706 0.18549503 0.18521196
 0.18516602 0.1853251  0.1849091  0.18454519 0.1845585  0.18364008
 0.18095608 0.17853984 0.17700593 0.17540556 0.17411767 0.17331937
 0.17307308 0.17291743 0.17260663 0.17232099 0.17227253 0.17207952
 0.17202276 0.17209096 0.1713225  0.17093013 0.17145342 0.17090377
 0.17023519 0.17119476 0.17119247 0.1704277  0.17141958 0.17125016
 0.16836257 0.16786589 0.16784154 0.16596709 0.16626835 0.16707796
 0.16649982 0.16661455 0.16758493 0.16670704 0.16577776 0.16567907
 0.16484666 0.16388303 0.16274466 0.16122098 0.1616115  0.16112277
 0.1600207  0.1614872  0.16126089 0.15943177 0.16192055 0.15932123]
