Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=74, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14719488.0
params:  16650.0
Trainable parameters:  16650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.593324661254883
Epoch: 1, Steps: 59 | Train Loss: 0.9545458 Vali Loss: 2.2738385 Test Loss: 0.9970222
Validation loss decreased (inf --> 2.273839).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9439866542816162
Epoch: 2, Steps: 59 | Train Loss: 0.7488437 Vali Loss: 2.0074253 Test Loss: 0.8243976
Validation loss decreased (2.273839 --> 2.007425).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.9766130447387695
Epoch: 3, Steps: 59 | Train Loss: 0.6441730 Vali Loss: 1.8725590 Test Loss: 0.7386700
Validation loss decreased (2.007425 --> 1.872559).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.8553028106689453
Epoch: 4, Steps: 59 | Train Loss: 0.5896664 Vali Loss: 1.8073663 Test Loss: 0.6932536
Validation loss decreased (1.872559 --> 1.807366).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.6173665523529053
Epoch: 5, Steps: 59 | Train Loss: 0.5587481 Vali Loss: 1.7679712 Test Loss: 0.6662993
Validation loss decreased (1.807366 --> 1.767971).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.858854055404663
Epoch: 6, Steps: 59 | Train Loss: 0.5389290 Vali Loss: 1.7368284 Test Loss: 0.6479631
Validation loss decreased (1.767971 --> 1.736828).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.5477449893951416
Epoch: 7, Steps: 59 | Train Loss: 0.5250712 Vali Loss: 1.7182775 Test Loss: 0.6336669
Validation loss decreased (1.736828 --> 1.718277).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.499330997467041
Epoch: 8, Steps: 59 | Train Loss: 0.5142426 Vali Loss: 1.7104998 Test Loss: 0.6223788
Validation loss decreased (1.718277 --> 1.710500).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.434614658355713
Epoch: 9, Steps: 59 | Train Loss: 0.5053467 Vali Loss: 1.6870805 Test Loss: 0.6114628
Validation loss decreased (1.710500 --> 1.687081).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.234414577484131
Epoch: 10, Steps: 59 | Train Loss: 0.4979915 Vali Loss: 1.6750698 Test Loss: 0.6024378
Validation loss decreased (1.687081 --> 1.675070).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.3316903114318848
Epoch: 11, Steps: 59 | Train Loss: 0.4914172 Vali Loss: 1.6659663 Test Loss: 0.5937663
Validation loss decreased (1.675070 --> 1.665966).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.974714756011963
Epoch: 12, Steps: 59 | Train Loss: 0.4860167 Vali Loss: 1.6600080 Test Loss: 0.5863204
Validation loss decreased (1.665966 --> 1.660008).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9595768451690674
Epoch: 13, Steps: 59 | Train Loss: 0.4809398 Vali Loss: 1.6432242 Test Loss: 0.5790181
Validation loss decreased (1.660008 --> 1.643224).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.8605504035949707
Epoch: 14, Steps: 59 | Train Loss: 0.4763920 Vali Loss: 1.6364691 Test Loss: 0.5724789
Validation loss decreased (1.643224 --> 1.636469).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.756255626678467
Epoch: 15, Steps: 59 | Train Loss: 0.4723994 Vali Loss: 1.6338290 Test Loss: 0.5666376
Validation loss decreased (1.636469 --> 1.633829).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.9503278732299805
Epoch: 16, Steps: 59 | Train Loss: 0.4688498 Vali Loss: 1.6204344 Test Loss: 0.5609394
Validation loss decreased (1.633829 --> 1.620434).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.193830966949463
Epoch: 17, Steps: 59 | Train Loss: 0.4655434 Vali Loss: 1.6127801 Test Loss: 0.5558599
Validation loss decreased (1.620434 --> 1.612780).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7840182781219482
Epoch: 18, Steps: 59 | Train Loss: 0.4626131 Vali Loss: 1.6094098 Test Loss: 0.5510607
Validation loss decreased (1.612780 --> 1.609410).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.3318161964416504
Epoch: 19, Steps: 59 | Train Loss: 0.4598850 Vali Loss: 1.5997432 Test Loss: 0.5464935
Validation loss decreased (1.609410 --> 1.599743).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9792470932006836
Epoch: 20, Steps: 59 | Train Loss: 0.4573439 Vali Loss: 1.6033757 Test Loss: 0.5426835
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.2524380683898926
Epoch: 21, Steps: 59 | Train Loss: 0.4550669 Vali Loss: 1.5927227 Test Loss: 0.5387365
Validation loss decreased (1.599743 --> 1.592723).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0916879177093506
Epoch: 22, Steps: 59 | Train Loss: 0.4529936 Vali Loss: 1.5896182 Test Loss: 0.5350787
Validation loss decreased (1.592723 --> 1.589618).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.9618542194366455
Epoch: 23, Steps: 59 | Train Loss: 0.4510994 Vali Loss: 1.5865318 Test Loss: 0.5319799
Validation loss decreased (1.589618 --> 1.586532).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.6788086891174316
Epoch: 24, Steps: 59 | Train Loss: 0.4491750 Vali Loss: 1.5851283 Test Loss: 0.5288338
Validation loss decreased (1.586532 --> 1.585128).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.107407808303833
Epoch: 25, Steps: 59 | Train Loss: 0.4475979 Vali Loss: 1.5726998 Test Loss: 0.5259168
Validation loss decreased (1.585128 --> 1.572700).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.4763023853302
Epoch: 26, Steps: 59 | Train Loss: 0.4460155 Vali Loss: 1.5785232 Test Loss: 0.5233241
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.082561731338501
Epoch: 27, Steps: 59 | Train Loss: 0.4444890 Vali Loss: 1.5732830 Test Loss: 0.5206740
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.7230441570281982
Epoch: 28, Steps: 59 | Train Loss: 0.4431754 Vali Loss: 1.5674806 Test Loss: 0.5183892
Validation loss decreased (1.572700 --> 1.567481).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.2921650409698486
Epoch: 29, Steps: 59 | Train Loss: 0.4418921 Vali Loss: 1.5666263 Test Loss: 0.5162442
Validation loss decreased (1.567481 --> 1.566626).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.0600533485412598
Epoch: 30, Steps: 59 | Train Loss: 0.4406403 Vali Loss: 1.5610694 Test Loss: 0.5141323
Validation loss decreased (1.566626 --> 1.561069).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.411259651184082
Epoch: 31, Steps: 59 | Train Loss: 0.4395334 Vali Loss: 1.5525545 Test Loss: 0.5121971
Validation loss decreased (1.561069 --> 1.552554).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.6947529315948486
Epoch: 32, Steps: 59 | Train Loss: 0.4384652 Vali Loss: 1.5560005 Test Loss: 0.5104156
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.4856343269348145
Epoch: 33, Steps: 59 | Train Loss: 0.4376054 Vali Loss: 1.5546687 Test Loss: 0.5087237
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.684373140335083
Epoch: 34, Steps: 59 | Train Loss: 0.4366294 Vali Loss: 1.5512688 Test Loss: 0.5070688
Validation loss decreased (1.552554 --> 1.551269).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.1979525089263916
Epoch: 35, Steps: 59 | Train Loss: 0.4358659 Vali Loss: 1.5479192 Test Loss: 0.5055630
Validation loss decreased (1.551269 --> 1.547919).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.705371379852295
Epoch: 36, Steps: 59 | Train Loss: 0.4349955 Vali Loss: 1.5452378 Test Loss: 0.5041432
Validation loss decreased (1.547919 --> 1.545238).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.026909351348877
Epoch: 37, Steps: 59 | Train Loss: 0.4342802 Vali Loss: 1.5416536 Test Loss: 0.5028284
Validation loss decreased (1.545238 --> 1.541654).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.649085760116577
Epoch: 38, Steps: 59 | Train Loss: 0.4335161 Vali Loss: 1.5439875 Test Loss: 0.5014841
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.9034063816070557
Epoch: 39, Steps: 59 | Train Loss: 0.4329858 Vali Loss: 1.5404501 Test Loss: 0.5002907
Validation loss decreased (1.541654 --> 1.540450).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.9384922981262207
Epoch: 40, Steps: 59 | Train Loss: 0.4323334 Vali Loss: 1.5396296 Test Loss: 0.4992200
Validation loss decreased (1.540450 --> 1.539630).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.7429158687591553
Epoch: 41, Steps: 59 | Train Loss: 0.4315414 Vali Loss: 1.5360447 Test Loss: 0.4981378
Validation loss decreased (1.539630 --> 1.536045).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.114194393157959
Epoch: 42, Steps: 59 | Train Loss: 0.4310834 Vali Loss: 1.5385351 Test Loss: 0.4970785
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.1198933124542236
Epoch: 43, Steps: 59 | Train Loss: 0.4306494 Vali Loss: 1.5393075 Test Loss: 0.4961915
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.972369432449341
Epoch: 44, Steps: 59 | Train Loss: 0.4301292 Vali Loss: 1.5328904 Test Loss: 0.4952722
Validation loss decreased (1.536045 --> 1.532890).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.0553627014160156
Epoch: 45, Steps: 59 | Train Loss: 0.4295535 Vali Loss: 1.5290222 Test Loss: 0.4944321
Validation loss decreased (1.532890 --> 1.529022).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.359255075454712
Epoch: 46, Steps: 59 | Train Loss: 0.4290307 Vali Loss: 1.5313747 Test Loss: 0.4936075
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.7561326026916504
Epoch: 47, Steps: 59 | Train Loss: 0.4287700 Vali Loss: 1.5329242 Test Loss: 0.4928520
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.1792707443237305
Epoch: 48, Steps: 59 | Train Loss: 0.4282986 Vali Loss: 1.5269573 Test Loss: 0.4921410
Validation loss decreased (1.529022 --> 1.526957).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.4234156608581543
Epoch: 49, Steps: 59 | Train Loss: 0.4279215 Vali Loss: 1.5352951 Test Loss: 0.4914045
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.037748098373413
Epoch: 50, Steps: 59 | Train Loss: 0.4273697 Vali Loss: 1.5289780 Test Loss: 0.4907328
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.774923086166382
Epoch: 51, Steps: 59 | Train Loss: 0.4271902 Vali Loss: 1.5302581 Test Loss: 0.4901339
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.8010001182556152
Epoch: 52, Steps: 59 | Train Loss: 0.4269669 Vali Loss: 1.5223505 Test Loss: 0.4895481
Validation loss decreased (1.526957 --> 1.522351).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.7903051376342773
Epoch: 53, Steps: 59 | Train Loss: 0.4266866 Vali Loss: 1.5224785 Test Loss: 0.4890009
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.7861175537109375
Epoch: 54, Steps: 59 | Train Loss: 0.4263306 Vali Loss: 1.5212300 Test Loss: 0.4885173
Validation loss decreased (1.522351 --> 1.521230).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.0697295665740967
Epoch: 55, Steps: 59 | Train Loss: 0.4259356 Vali Loss: 1.5311129 Test Loss: 0.4879902
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.592092275619507
Epoch: 56, Steps: 59 | Train Loss: 0.4257769 Vali Loss: 1.5273005 Test Loss: 0.4874960
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.8098061084747314
Epoch: 57, Steps: 59 | Train Loss: 0.4254373 Vali Loss: 1.5262402 Test Loss: 0.4870797
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.4696295261383057
Epoch: 58, Steps: 59 | Train Loss: 0.4251210 Vali Loss: 1.5233531 Test Loss: 0.4866269
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.046652317047119
Epoch: 59, Steps: 59 | Train Loss: 0.4249859 Vali Loss: 1.5248444 Test Loss: 0.4862385
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.180046319961548
Epoch: 60, Steps: 59 | Train Loss: 0.4247793 Vali Loss: 1.5213516 Test Loss: 0.4858351
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.678692579269409
Epoch: 61, Steps: 59 | Train Loss: 0.4245174 Vali Loss: 1.5240624 Test Loss: 0.4854807
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.0722808837890625
Epoch: 62, Steps: 59 | Train Loss: 0.4243223 Vali Loss: 1.5239813 Test Loss: 0.4851038
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.396578073501587
Epoch: 63, Steps: 59 | Train Loss: 0.4243080 Vali Loss: 1.5285790 Test Loss: 0.4848008
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.967743396759033
Epoch: 64, Steps: 59 | Train Loss: 0.4241217 Vali Loss: 1.5243937 Test Loss: 0.4844772
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.128419876098633
Epoch: 65, Steps: 59 | Train Loss: 0.4239526 Vali Loss: 1.5188866 Test Loss: 0.4842180
Validation loss decreased (1.521230 --> 1.518887).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.8928308486938477
Epoch: 66, Steps: 59 | Train Loss: 0.4236989 Vali Loss: 1.5280582 Test Loss: 0.4839202
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.9401772022247314
Epoch: 67, Steps: 59 | Train Loss: 0.4235434 Vali Loss: 1.5194814 Test Loss: 0.4836395
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.5144858360290527
Epoch: 68, Steps: 59 | Train Loss: 0.4233350 Vali Loss: 1.5238100 Test Loss: 0.4833724
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.4283790588378906
Epoch: 69, Steps: 59 | Train Loss: 0.4233320 Vali Loss: 1.5219253 Test Loss: 0.4831336
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.462533712387085
Epoch: 70, Steps: 59 | Train Loss: 0.4231555 Vali Loss: 1.5203828 Test Loss: 0.4828946
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.173840045928955
Epoch: 71, Steps: 59 | Train Loss: 0.4230372 Vali Loss: 1.5190804 Test Loss: 0.4826865
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.934598207473755
Epoch: 72, Steps: 59 | Train Loss: 0.4229247 Vali Loss: 1.5192889 Test Loss: 0.4824873
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.408597469329834
Epoch: 73, Steps: 59 | Train Loss: 0.4228492 Vali Loss: 1.5202639 Test Loss: 0.4822760
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.9832170009613037
Epoch: 74, Steps: 59 | Train Loss: 0.4226745 Vali Loss: 1.5172148 Test Loss: 0.4820800
Validation loss decreased (1.518887 --> 1.517215).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.854388952255249
Epoch: 75, Steps: 59 | Train Loss: 0.4225501 Vali Loss: 1.5172749 Test Loss: 0.4819074
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.209968090057373
Epoch: 76, Steps: 59 | Train Loss: 0.4223398 Vali Loss: 1.5205463 Test Loss: 0.4817326
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.148426055908203
Epoch: 77, Steps: 59 | Train Loss: 0.4224099 Vali Loss: 1.5169785 Test Loss: 0.4815704
Validation loss decreased (1.517215 --> 1.516979).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.7533674240112305
Epoch: 78, Steps: 59 | Train Loss: 0.4223143 Vali Loss: 1.5224566 Test Loss: 0.4814137
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.250598430633545
Epoch: 79, Steps: 59 | Train Loss: 0.4222185 Vali Loss: 1.5195341 Test Loss: 0.4812716
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.2658584117889404
Epoch: 80, Steps: 59 | Train Loss: 0.4222050 Vali Loss: 1.5128367 Test Loss: 0.4811233
Validation loss decreased (1.516979 --> 1.512837).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.063959836959839
Epoch: 81, Steps: 59 | Train Loss: 0.4220674 Vali Loss: 1.5144141 Test Loss: 0.4810022
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.929530143737793
Epoch: 82, Steps: 59 | Train Loss: 0.4220611 Vali Loss: 1.5147225 Test Loss: 0.4808720
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.396084785461426
Epoch: 83, Steps: 59 | Train Loss: 0.4219720 Vali Loss: 1.5125134 Test Loss: 0.4807459
Validation loss decreased (1.512837 --> 1.512513).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.7113194465637207
Epoch: 84, Steps: 59 | Train Loss: 0.4219612 Vali Loss: 1.5158861 Test Loss: 0.4806388
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.4145491123199463
Epoch: 85, Steps: 59 | Train Loss: 0.4218197 Vali Loss: 1.5167366 Test Loss: 0.4805317
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.0657687187194824
Epoch: 86, Steps: 59 | Train Loss: 0.4218363 Vali Loss: 1.5104297 Test Loss: 0.4804286
Validation loss decreased (1.512513 --> 1.510430).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.597790002822876
Epoch: 87, Steps: 59 | Train Loss: 0.4216617 Vali Loss: 1.5157192 Test Loss: 0.4803345
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.7711622714996338
Epoch: 88, Steps: 59 | Train Loss: 0.4216580 Vali Loss: 1.5137739 Test Loss: 0.4802330
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.179825782775879
Epoch: 89, Steps: 59 | Train Loss: 0.4214908 Vali Loss: 1.5141867 Test Loss: 0.4801453
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.448254108428955
Epoch: 90, Steps: 59 | Train Loss: 0.4214601 Vali Loss: 1.5187827 Test Loss: 0.4800661
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.3788325786590576
Epoch: 91, Steps: 59 | Train Loss: 0.4215335 Vali Loss: 1.5204804 Test Loss: 0.4799862
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 3.290412425994873
Epoch: 92, Steps: 59 | Train Loss: 0.4214367 Vali Loss: 1.5178674 Test Loss: 0.4799037
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 3.289562225341797
Epoch: 93, Steps: 59 | Train Loss: 0.4214867 Vali Loss: 1.5126591 Test Loss: 0.4798299
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 3.669215440750122
Epoch: 94, Steps: 59 | Train Loss: 0.4214929 Vali Loss: 1.5062358 Test Loss: 0.4797667
Validation loss decreased (1.510430 --> 1.506236).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 3.685145616531372
Epoch: 95, Steps: 59 | Train Loss: 0.4214712 Vali Loss: 1.5204158 Test Loss: 0.4796984
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 3.8030307292938232
Epoch: 96, Steps: 59 | Train Loss: 0.4214044 Vali Loss: 1.5180132 Test Loss: 0.4796340
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 4.098642826080322
Epoch: 97, Steps: 59 | Train Loss: 0.4213096 Vali Loss: 1.5149741 Test Loss: 0.4795808
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.89901065826416
Epoch: 98, Steps: 59 | Train Loss: 0.4211521 Vali Loss: 1.5150818 Test Loss: 0.4795226
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.3593573570251465
Epoch: 99, Steps: 59 | Train Loss: 0.4213768 Vali Loss: 1.5188661 Test Loss: 0.4794682
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 3.2886314392089844
Epoch: 100, Steps: 59 | Train Loss: 0.4211848 Vali Loss: 1.5135713 Test Loss: 0.4794152
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=74, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14719488.0
params:  16650.0
Trainable parameters:  16650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.2377426624298096
Epoch: 1, Steps: 59 | Train Loss: 0.5958852 Vali Loss: 1.4837314 Test Loss: 0.4571357
Validation loss decreased (inf --> 1.483731).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.15627384185791
Epoch: 2, Steps: 59 | Train Loss: 0.5861932 Vali Loss: 1.4651437 Test Loss: 0.4449042
Validation loss decreased (1.483731 --> 1.465144).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0313773155212402
Epoch: 3, Steps: 59 | Train Loss: 0.5807344 Vali Loss: 1.4485807 Test Loss: 0.4377441
Validation loss decreased (1.465144 --> 1.448581).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0938847064971924
Epoch: 4, Steps: 59 | Train Loss: 0.5773762 Vali Loss: 1.4461577 Test Loss: 0.4337904
Validation loss decreased (1.448581 --> 1.446158).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7733724117279053
Epoch: 5, Steps: 59 | Train Loss: 0.5754640 Vali Loss: 1.4463519 Test Loss: 0.4318030
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.9433023929595947
Epoch: 6, Steps: 59 | Train Loss: 0.5743035 Vali Loss: 1.4380510 Test Loss: 0.4305968
Validation loss decreased (1.446158 --> 1.438051).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.194387197494507
Epoch: 7, Steps: 59 | Train Loss: 0.5734188 Vali Loss: 1.4355026 Test Loss: 0.4302978
Validation loss decreased (1.438051 --> 1.435503).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.30208158493042
Epoch: 8, Steps: 59 | Train Loss: 0.5730480 Vali Loss: 1.4371500 Test Loss: 0.4301486
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.2879602909088135
Epoch: 9, Steps: 59 | Train Loss: 0.5728476 Vali Loss: 1.4327817 Test Loss: 0.4301649
Validation loss decreased (1.435503 --> 1.432782).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.521260976791382
Epoch: 10, Steps: 59 | Train Loss: 0.5724375 Vali Loss: 1.4353733 Test Loss: 0.4303493
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7928707599639893
Epoch: 11, Steps: 59 | Train Loss: 0.5722879 Vali Loss: 1.4358294 Test Loss: 0.4304965
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.3794047832489014
Epoch: 12, Steps: 59 | Train Loss: 0.5724476 Vali Loss: 1.4361324 Test Loss: 0.4305502
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.6605355739593506
Epoch: 13, Steps: 59 | Train Loss: 0.5721007 Vali Loss: 1.4323032 Test Loss: 0.4307025
Validation loss decreased (1.432782 --> 1.432303).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.5608022212982178
Epoch: 14, Steps: 59 | Train Loss: 0.5723147 Vali Loss: 1.4317977 Test Loss: 0.4307015
Validation loss decreased (1.432303 --> 1.431798).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.9831666946411133
Epoch: 15, Steps: 59 | Train Loss: 0.5721856 Vali Loss: 1.4377159 Test Loss: 0.4307436
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.7456393241882324
Epoch: 16, Steps: 59 | Train Loss: 0.5721636 Vali Loss: 1.4336064 Test Loss: 0.4308660
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.525932550430298
Epoch: 17, Steps: 59 | Train Loss: 0.5719097 Vali Loss: 1.4354985 Test Loss: 0.4308665
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.6059250831604004
Epoch: 18, Steps: 59 | Train Loss: 0.5719797 Vali Loss: 1.4328547 Test Loss: 0.4309729
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.615039587020874
Epoch: 19, Steps: 59 | Train Loss: 0.5720147 Vali Loss: 1.4372978 Test Loss: 0.4309676
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.045203924179077
Epoch: 20, Steps: 59 | Train Loss: 0.5720935 Vali Loss: 1.4320741 Test Loss: 0.4309711
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0688273906707764
Epoch: 21, Steps: 59 | Train Loss: 0.5720835 Vali Loss: 1.4335178 Test Loss: 0.4310471
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.9265036582946777
Epoch: 22, Steps: 59 | Train Loss: 0.5719162 Vali Loss: 1.4299319 Test Loss: 0.4310604
Validation loss decreased (1.431798 --> 1.429932).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9509358406066895
Epoch: 23, Steps: 59 | Train Loss: 0.5719400 Vali Loss: 1.4318366 Test Loss: 0.4311088
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.8383305072784424
Epoch: 24, Steps: 59 | Train Loss: 0.5719617 Vali Loss: 1.4369822 Test Loss: 0.4310719
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.160365581512451
Epoch: 25, Steps: 59 | Train Loss: 0.5719684 Vali Loss: 1.4367542 Test Loss: 0.4311060
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.7934882640838623
Epoch: 26, Steps: 59 | Train Loss: 0.5718608 Vali Loss: 1.4335164 Test Loss: 0.4311230
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.879854440689087
Epoch: 27, Steps: 59 | Train Loss: 0.5718772 Vali Loss: 1.4329586 Test Loss: 0.4311686
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.4347431659698486
Epoch: 28, Steps: 59 | Train Loss: 0.5718694 Vali Loss: 1.4326279 Test Loss: 0.4311408
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.621518850326538
Epoch: 29, Steps: 59 | Train Loss: 0.5718449 Vali Loss: 1.4298655 Test Loss: 0.4311723
Validation loss decreased (1.429932 --> 1.429865).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.4787306785583496
Epoch: 30, Steps: 59 | Train Loss: 0.5718593 Vali Loss: 1.4284642 Test Loss: 0.4311930
Validation loss decreased (1.429865 --> 1.428464).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.32135009765625
Epoch: 31, Steps: 59 | Train Loss: 0.5718450 Vali Loss: 1.4336529 Test Loss: 0.4312190
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.2027931213378906
Epoch: 32, Steps: 59 | Train Loss: 0.5718016 Vali Loss: 1.4303260 Test Loss: 0.4312046
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.5887699127197266
Epoch: 33, Steps: 59 | Train Loss: 0.5716181 Vali Loss: 1.4344169 Test Loss: 0.4312181
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.900303840637207
Epoch: 34, Steps: 59 | Train Loss: 0.5715659 Vali Loss: 1.4334848 Test Loss: 0.4312420
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.1640565395355225
Epoch: 35, Steps: 59 | Train Loss: 0.5715978 Vali Loss: 1.4328697 Test Loss: 0.4312581
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.0041770935058594
Epoch: 36, Steps: 59 | Train Loss: 0.5717193 Vali Loss: 1.4352365 Test Loss: 0.4312664
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.2074661254882812
Epoch: 37, Steps: 59 | Train Loss: 0.5715826 Vali Loss: 1.4367020 Test Loss: 0.4312427
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9290087223052979
Epoch: 38, Steps: 59 | Train Loss: 0.5716601 Vali Loss: 1.4279981 Test Loss: 0.4312468
Validation loss decreased (1.428464 --> 1.427998).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7763049602508545
Epoch: 39, Steps: 59 | Train Loss: 0.5715928 Vali Loss: 1.4360065 Test Loss: 0.4312640
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.241729736328125
Epoch: 40, Steps: 59 | Train Loss: 0.5716816 Vali Loss: 1.4311893 Test Loss: 0.4312978
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.3574068546295166
Epoch: 41, Steps: 59 | Train Loss: 0.5715531 Vali Loss: 1.4278841 Test Loss: 0.4313059
Validation loss decreased (1.427998 --> 1.427884).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.235161304473877
Epoch: 42, Steps: 59 | Train Loss: 0.5715491 Vali Loss: 1.4329141 Test Loss: 0.4313062
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.169278383255005
Epoch: 43, Steps: 59 | Train Loss: 0.5715871 Vali Loss: 1.4364629 Test Loss: 0.4312980
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.4617726802825928
Epoch: 44, Steps: 59 | Train Loss: 0.5714397 Vali Loss: 1.4339921 Test Loss: 0.4312989
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9704399108886719
Epoch: 45, Steps: 59 | Train Loss: 0.5716213 Vali Loss: 1.4311870 Test Loss: 0.4313271
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.463584661483765
Epoch: 46, Steps: 59 | Train Loss: 0.5714541 Vali Loss: 1.4313265 Test Loss: 0.4313179
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.1662371158599854
Epoch: 47, Steps: 59 | Train Loss: 0.5714706 Vali Loss: 1.4309778 Test Loss: 0.4313160
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.670518398284912
Epoch: 48, Steps: 59 | Train Loss: 0.5715373 Vali Loss: 1.4318309 Test Loss: 0.4313392
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.847209930419922
Epoch: 49, Steps: 59 | Train Loss: 0.5715507 Vali Loss: 1.4328099 Test Loss: 0.4313447
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.936232566833496
Epoch: 50, Steps: 59 | Train Loss: 0.5716517 Vali Loss: 1.4312823 Test Loss: 0.4313512
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.964033603668213
Epoch: 51, Steps: 59 | Train Loss: 0.5716942 Vali Loss: 1.4349649 Test Loss: 0.4313629
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.239360809326172
Epoch: 52, Steps: 59 | Train Loss: 0.5714641 Vali Loss: 1.4285917 Test Loss: 0.4313696
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.3120594024658203
Epoch: 53, Steps: 59 | Train Loss: 0.5715297 Vali Loss: 1.4331601 Test Loss: 0.4313710
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.9029886722564697
Epoch: 54, Steps: 59 | Train Loss: 0.5715343 Vali Loss: 1.4282033 Test Loss: 0.4313758
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.5517468452453613
Epoch: 55, Steps: 59 | Train Loss: 0.5715218 Vali Loss: 1.4341686 Test Loss: 0.4313748
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.181428909301758
Epoch: 56, Steps: 59 | Train Loss: 0.5716247 Vali Loss: 1.4314256 Test Loss: 0.4313844
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.03067946434021
Epoch: 57, Steps: 59 | Train Loss: 0.5715824 Vali Loss: 1.4346520 Test Loss: 0.4313802
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.5007998943328857
Epoch: 58, Steps: 59 | Train Loss: 0.5713144 Vali Loss: 1.4326110 Test Loss: 0.4313826
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.709105968475342
Epoch: 59, Steps: 59 | Train Loss: 0.5713258 Vali Loss: 1.4365928 Test Loss: 0.4313979
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.868756055831909
Epoch: 60, Steps: 59 | Train Loss: 0.5715520 Vali Loss: 1.4334842 Test Loss: 0.4313954
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.153981924057007
Epoch: 61, Steps: 59 | Train Loss: 0.5715573 Vali Loss: 1.4271512 Test Loss: 0.4313962
Validation loss decreased (1.427884 --> 1.427151).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.34238338470459
Epoch: 62, Steps: 59 | Train Loss: 0.5714120 Vali Loss: 1.4268357 Test Loss: 0.4313931
Validation loss decreased (1.427151 --> 1.426836).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.2411742210388184
Epoch: 63, Steps: 59 | Train Loss: 0.5716180 Vali Loss: 1.4277959 Test Loss: 0.4314010
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.344930410385132
Epoch: 64, Steps: 59 | Train Loss: 0.5714118 Vali Loss: 1.4333980 Test Loss: 0.4313980
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.8747317790985107
Epoch: 65, Steps: 59 | Train Loss: 0.5715106 Vali Loss: 1.4288442 Test Loss: 0.4314084
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.005013942718506
Epoch: 66, Steps: 59 | Train Loss: 0.5715880 Vali Loss: 1.4289634 Test Loss: 0.4314106
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.015719413757324
Epoch: 67, Steps: 59 | Train Loss: 0.5714835 Vali Loss: 1.4291549 Test Loss: 0.4314018
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.8537888526916504
Epoch: 68, Steps: 59 | Train Loss: 0.5714725 Vali Loss: 1.4300034 Test Loss: 0.4314082
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.8753747940063477
Epoch: 69, Steps: 59 | Train Loss: 0.5715479 Vali Loss: 1.4356537 Test Loss: 0.4314125
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.985128402709961
Epoch: 70, Steps: 59 | Train Loss: 0.5715119 Vali Loss: 1.4344624 Test Loss: 0.4314135
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.249654769897461
Epoch: 71, Steps: 59 | Train Loss: 0.5714512 Vali Loss: 1.4311767 Test Loss: 0.4314222
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.9643118381500244
Epoch: 72, Steps: 59 | Train Loss: 0.5714069 Vali Loss: 1.4284365 Test Loss: 0.4314120
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.0595803260803223
Epoch: 73, Steps: 59 | Train Loss: 0.5715637 Vali Loss: 1.4331899 Test Loss: 0.4314174
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.3182549476623535
Epoch: 74, Steps: 59 | Train Loss: 0.5713788 Vali Loss: 1.4297554 Test Loss: 0.4314247
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.379937171936035
Epoch: 75, Steps: 59 | Train Loss: 0.5714051 Vali Loss: 1.4351839 Test Loss: 0.4314244
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.270749568939209
Epoch: 76, Steps: 59 | Train Loss: 0.5715386 Vali Loss: 1.4283559 Test Loss: 0.4314236
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.5907063484191895
Epoch: 77, Steps: 59 | Train Loss: 0.5715917 Vali Loss: 1.4292927 Test Loss: 0.4314236
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.048760175704956
Epoch: 78, Steps: 59 | Train Loss: 0.5714596 Vali Loss: 1.4330242 Test Loss: 0.4314239
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 3.236006259918213
Epoch: 79, Steps: 59 | Train Loss: 0.5715644 Vali Loss: 1.4363520 Test Loss: 0.4314246
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.556246280670166
Epoch: 80, Steps: 59 | Train Loss: 0.5714779 Vali Loss: 1.4326487 Test Loss: 0.4314280
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.922895908355713
Epoch: 81, Steps: 59 | Train Loss: 0.5715147 Vali Loss: 1.4336574 Test Loss: 0.4314274
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.0183284282684326
Epoch: 82, Steps: 59 | Train Loss: 0.5713978 Vali Loss: 1.4330070 Test Loss: 0.4314307
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4304065406322479, mae:0.44975852966308594, rse:0.628045916557312, corr:[0.22653137 0.23474735 0.23479627 0.23280255 0.23136705 0.23032601
 0.22946824 0.22925037 0.229222   0.22961892 0.22976893 0.22931245
 0.2288649  0.2288393  0.22895306 0.22892849 0.22861646 0.22825882
 0.22805767 0.22783875 0.22764175 0.22783783 0.22849773 0.22935073
 0.2296297  0.2295519  0.22960414 0.22968778 0.22951435 0.22910756
 0.22857942 0.22817153 0.22795217 0.22776225 0.2276237  0.22751267
 0.22756441 0.2276161  0.2275972  0.22751307 0.22763732 0.22775663
 0.2277719  0.22755863 0.2272437  0.22715992 0.22765121 0.2283111
 0.22838423 0.22801271 0.22737864 0.22683358 0.22645098 0.22586854
 0.22533996 0.22479852 0.22458535 0.22443594 0.22410496 0.22385854
 0.22364248 0.22345217 0.2232262  0.22324122 0.2234425  0.22369464
 0.22386034 0.22377193 0.22368726 0.22384557 0.22403285 0.2239948
 0.22333002 0.2224676  0.22187005 0.22167468 0.22156933 0.22128542
 0.22092645 0.22058003 0.2203282  0.2201208  0.21974538 0.2193986
 0.21925715 0.21924564 0.21910839 0.21880035 0.21854028 0.21842374
 0.21827017 0.21802813 0.21786645 0.21810167 0.21891636 0.2201701
 0.22134204 0.222      0.22247458 0.22293681 0.22324634 0.2232648
 0.22301684 0.22284713 0.22273542 0.2225541  0.222241   0.22183874
 0.22154994 0.22149454 0.22157192 0.22175564 0.2218848  0.22193411
 0.22194605 0.22181432 0.22163033 0.22142798 0.22141293 0.22164991
 0.2217754  0.22146234 0.22078814 0.22027925 0.21999292 0.2197498
 0.21950139 0.21929139 0.21902789 0.21867429 0.21834186 0.21808456
 0.21785505 0.21769473 0.21765035 0.21768539 0.2177982  0.21787016
 0.2179031  0.21786836 0.2178172  0.21777558 0.21770951 0.2176745
 0.21761651 0.21727593 0.21674526 0.2161332  0.21563607 0.21524042
 0.21496874 0.21489559 0.21491018 0.21497843 0.21495895 0.21485734
 0.21471788 0.2146145  0.21454777 0.214512   0.21444477 0.21447362
 0.21452115 0.21445794 0.21433763 0.21415548 0.21404937 0.2143379
 0.21500026 0.21580057 0.21637869 0.21679214 0.21706222 0.21719232
 0.21717347 0.21709989 0.21706377 0.2170869  0.21705875 0.21690965
 0.21675111 0.21664797 0.21667017 0.21673939 0.2168338  0.21702412
 0.21722072 0.2172748  0.21728566 0.2171889  0.21706218 0.2169652
 0.21679178 0.21644424 0.21585602 0.21529801 0.21480848 0.21434322
 0.21397029 0.21388254 0.21385072 0.2137375  0.2135165  0.213367
 0.21336076 0.21352313 0.21365795 0.21366498 0.21364126 0.21363433
 0.21362627 0.21344961 0.21310717 0.2127397  0.21248688 0.2126132
 0.21287906 0.21280453 0.21255876 0.21235093 0.21230166 0.21232723
 0.2122247  0.21206433 0.21189666 0.21166497 0.21127534 0.21090433
 0.21063717 0.21046361 0.21041973 0.21036363 0.21021822 0.21004285
 0.20994207 0.20976934 0.20951858 0.2093128  0.20937422 0.20963284
 0.21003187 0.21016468 0.21009533 0.21014173 0.21035263 0.21052459
 0.21064192 0.21072933 0.21079522 0.21063568 0.21031448 0.21000122
 0.20986639 0.20985407 0.21003816 0.21017544 0.2102591  0.21027622
 0.21024087 0.21004751 0.20998661 0.21003205 0.21001977 0.20995189
 0.20983703 0.2096499  0.20938459 0.20916873 0.20898302 0.20885946
 0.20879422 0.20874673 0.2086006  0.20845215 0.20830035 0.20821424
 0.20822397 0.2082838  0.20821649 0.20802948 0.20776606 0.20755245
 0.20746316 0.20746401 0.20755886 0.20766538 0.20795217 0.20849897
 0.20924966 0.20987195 0.21025099 0.21035857 0.21041551 0.21057841
 0.21079196 0.21091226 0.2108328  0.21070467 0.210552   0.21048659
 0.21047452 0.2104196  0.21049578 0.21060449 0.2106127  0.21053119
 0.21050535 0.21049058 0.21048398 0.21044353 0.21042453 0.21070968
 0.21130864 0.21187478 0.2120311  0.21195379 0.21178208 0.21162905
 0.21148236 0.21137586 0.2111597  0.21086413 0.21051553 0.21028253
 0.21017675 0.2101711  0.21021073 0.2104674  0.2106903  0.21080796
 0.21084945 0.21086435 0.21092932 0.2109386  0.21087845 0.2107622
 0.2107091  0.21067901 0.2105808  0.21030332 0.20997256 0.20958787
 0.20930542 0.2091791  0.20909098 0.20903982 0.20910688 0.2092575
 0.20934244 0.20940702 0.20948896 0.20963316 0.20963335 0.20967717
 0.20974568 0.20968868 0.20946765 0.20922199 0.20885998 0.20870958
 0.20885092 0.20898935 0.20886853 0.20850965 0.20806329 0.20758145
 0.20730723 0.20714702 0.20689757 0.20659754 0.20634088 0.20619705
 0.20609911 0.20585452 0.20553859 0.20525806 0.20517111 0.20529905
 0.20544253 0.20544389 0.20535548 0.20535818 0.20553793 0.20632242
 0.20753787 0.20856783 0.209178   0.20924023 0.20898694 0.20846511
 0.20805986 0.20764548 0.20721072 0.20682909 0.20649603 0.20636867
 0.2064188  0.20646213 0.2064917  0.20653582 0.20666078 0.20697835
 0.2072865  0.20750414 0.20769148 0.20801294 0.208322   0.20872453
 0.20901495 0.20913324 0.20902096 0.20880178 0.20858663 0.20834057
 0.20813012 0.20808801 0.20798753 0.20778686 0.20765167 0.20763819
 0.2076958  0.20773269 0.20752427 0.20720795 0.20701903 0.20709512
 0.20723699 0.2073797  0.20742024 0.20788224 0.20859759 0.20939529
 0.20997068 0.21010672 0.20990075 0.20975217 0.2097374  0.20967682
 0.20944534 0.20933145 0.20918766 0.2090485  0.20896982 0.20892513
 0.20894702 0.20891818 0.20884694 0.20874514 0.20868582 0.20874803
 0.2088747  0.20891033 0.20897095 0.20929162 0.20989789 0.21079911
 0.21149631 0.21171169 0.21154575 0.211458   0.21130565 0.21092285
 0.21045484 0.2102196  0.21024919 0.21041547 0.21049412 0.2104344
 0.21042591 0.21055058 0.21072978 0.21081796 0.210831   0.21091531
 0.2110449  0.21108884 0.21098492 0.21074773 0.21084242 0.21121478
 0.21137826 0.21112686 0.2106453  0.21043716 0.21043287 0.21029107
 0.21003611 0.2097885  0.20956667 0.2094878  0.20935512 0.20916134
 0.20898835 0.20890051 0.20891017 0.20892172 0.20888017 0.20883957
 0.20889857 0.20899664 0.20914564 0.20944922 0.21013018 0.21117146
 0.21217868 0.2128706  0.21316712 0.21326494 0.21315856 0.21275112
 0.21220231 0.21172364 0.21134433 0.21102636 0.21079117 0.21068895
 0.21069224 0.21073443 0.21094081 0.2112367  0.21148862 0.21164706
 0.21173006 0.21191269 0.21226968 0.21268445 0.21311587 0.2133862
 0.21343061 0.21331985 0.21317211 0.21305569 0.21269232 0.21201807
 0.2114066  0.21113847 0.21093911 0.2107858  0.21064134 0.21063662
 0.21063915 0.21070816 0.21089937 0.21121481 0.21153148 0.21172369
 0.21171068 0.21169087 0.21167813 0.21162984 0.21144469 0.2113031
 0.21115203 0.21095157 0.21065646 0.21021193 0.20955169 0.20896527
 0.20849456 0.20803149 0.20758462 0.20708483 0.20670308 0.2065126
 0.2064574  0.2061928  0.20588334 0.2056951  0.205841   0.20609435
 0.20625019 0.20623411 0.206289   0.206384   0.20654646 0.20657414
 0.20648813 0.20648319 0.20627993 0.20604223 0.20575817 0.20511404
 0.20452558 0.20412329 0.20387551 0.20367672 0.20357072 0.20345506
 0.20342875 0.20322344 0.20304959 0.20291543 0.20288102 0.20294893
 0.2030327  0.20302325 0.20301832 0.20298807 0.20290685 0.20285009
 0.20272714 0.20236628 0.2018729  0.20137237 0.20080832 0.2001182
 0.19952784 0.19920589 0.19893932 0.19868875 0.19842921 0.19820113
 0.19811732 0.19797362 0.19771482 0.19758554 0.1975351  0.19750871
 0.19751525 0.19756389 0.19773418 0.19804753 0.19836931 0.19868562
 0.19876856 0.19835527 0.19791168 0.19758885 0.19721094 0.19666019
 0.19620596 0.19591095 0.19553003 0.19511639 0.19474223 0.19464813
 0.19487047 0.1949027  0.19474827 0.1945094  0.19433992 0.19446847
 0.1945839  0.19459292 0.19464234 0.1949019  0.19541833 0.19593199
 0.19609877 0.19592525 0.19550963 0.1954394  0.19512826 0.19461887
 0.19410594 0.19376662 0.19345169 0.1930429  0.19271715 0.19255713
 0.19254161 0.19245821 0.19225694 0.19192798 0.19194828 0.19218738
 0.19236517 0.19233431 0.19229405 0.19232138 0.19233415 0.19235891
 0.19204769 0.19127366 0.19037078 0.18952808 0.18884584 0.18807809
 0.18738599 0.18699566 0.18676808 0.18647946 0.18633176 0.18638195
 0.18658014 0.18650988 0.18626922 0.18616663 0.18618041 0.18646741
 0.18680193 0.18702823 0.1871725  0.1874138  0.1877129  0.187957
 0.18818633 0.18804356 0.18759497 0.18724564 0.18693525 0.18645477
 0.18615061 0.18603784 0.1858505  0.18536943 0.18485396 0.18459035
 0.18464361 0.1844136  0.1839761  0.18324344 0.18302226 0.1833977
 0.18405965 0.183857   0.18300472 0.18254459 0.18353459 0.18209317]
