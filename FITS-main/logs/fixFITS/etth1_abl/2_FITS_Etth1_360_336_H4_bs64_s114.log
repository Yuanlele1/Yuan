Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=74, out_features=143, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9481472.0
params:  10725.0
Trainable parameters:  10725
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.1338207721710205
Epoch: 1, Steps: 62 | Train Loss: 0.7388440 Vali Loss: 1.8303739 Test Loss: 0.8812490
Validation loss decreased (inf --> 1.830374).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.4324660301208496
Epoch: 2, Steps: 62 | Train Loss: 0.5879069 Vali Loss: 1.6480166 Test Loss: 0.7686168
Validation loss decreased (1.830374 --> 1.648017).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.403413772583008
Epoch: 3, Steps: 62 | Train Loss: 0.5077918 Vali Loss: 1.5607527 Test Loss: 0.7139364
Validation loss decreased (1.648017 --> 1.560753).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.063046455383301
Epoch: 4, Steps: 62 | Train Loss: 0.4626806 Vali Loss: 1.5141158 Test Loss: 0.6834894
Validation loss decreased (1.560753 --> 1.514116).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.206418514251709
Epoch: 5, Steps: 62 | Train Loss: 0.4341002 Vali Loss: 1.4810071 Test Loss: 0.6633033
Validation loss decreased (1.514116 --> 1.481007).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.5154035091400146
Epoch: 6, Steps: 62 | Train Loss: 0.4135338 Vali Loss: 1.4538256 Test Loss: 0.6485412
Validation loss decreased (1.481007 --> 1.453826).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.7327229976654053
Epoch: 7, Steps: 62 | Train Loss: 0.3977663 Vali Loss: 1.4365067 Test Loss: 0.6372988
Validation loss decreased (1.453826 --> 1.436507).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.0399763584136963
Epoch: 8, Steps: 62 | Train Loss: 0.3847768 Vali Loss: 1.4223151 Test Loss: 0.6269382
Validation loss decreased (1.436507 --> 1.422315).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7959661483764648
Epoch: 9, Steps: 62 | Train Loss: 0.3736198 Vali Loss: 1.4055433 Test Loss: 0.6178311
Validation loss decreased (1.422315 --> 1.405543).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.70487642288208
Epoch: 10, Steps: 62 | Train Loss: 0.3641522 Vali Loss: 1.3958763 Test Loss: 0.6092601
Validation loss decreased (1.405543 --> 1.395876).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.7815346717834473
Epoch: 11, Steps: 62 | Train Loss: 0.3557648 Vali Loss: 1.3842301 Test Loss: 0.6003124
Validation loss decreased (1.395876 --> 1.384230).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.2830893993377686
Epoch: 12, Steps: 62 | Train Loss: 0.3483309 Vali Loss: 1.3718351 Test Loss: 0.5923724
Validation loss decreased (1.384230 --> 1.371835).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.787681818008423
Epoch: 13, Steps: 62 | Train Loss: 0.3417454 Vali Loss: 1.3642714 Test Loss: 0.5861337
Validation loss decreased (1.371835 --> 1.364271).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.6976377964019775
Epoch: 14, Steps: 62 | Train Loss: 0.3358086 Vali Loss: 1.3519865 Test Loss: 0.5800886
Validation loss decreased (1.364271 --> 1.351987).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.063931941986084
Epoch: 15, Steps: 62 | Train Loss: 0.3305072 Vali Loss: 1.3434088 Test Loss: 0.5737094
Validation loss decreased (1.351987 --> 1.343409).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.2543578147888184
Epoch: 16, Steps: 62 | Train Loss: 0.3257404 Vali Loss: 1.3385401 Test Loss: 0.5685526
Validation loss decreased (1.343409 --> 1.338540).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.00354266166687
Epoch: 17, Steps: 62 | Train Loss: 0.3213890 Vali Loss: 1.3278205 Test Loss: 0.5631629
Validation loss decreased (1.338540 --> 1.327821).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.4519238471984863
Epoch: 18, Steps: 62 | Train Loss: 0.3175113 Vali Loss: 1.3225986 Test Loss: 0.5585176
Validation loss decreased (1.327821 --> 1.322599).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.8727009296417236
Epoch: 19, Steps: 62 | Train Loss: 0.3138770 Vali Loss: 1.3207090 Test Loss: 0.5540386
Validation loss decreased (1.322599 --> 1.320709).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9899377822875977
Epoch: 20, Steps: 62 | Train Loss: 0.3105545 Vali Loss: 1.3120254 Test Loss: 0.5497679
Validation loss decreased (1.320709 --> 1.312025).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.2687134742736816
Epoch: 21, Steps: 62 | Train Loss: 0.3076758 Vali Loss: 1.3029999 Test Loss: 0.5458863
Validation loss decreased (1.312025 --> 1.303000).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.7377395629882812
Epoch: 22, Steps: 62 | Train Loss: 0.3049301 Vali Loss: 1.2996337 Test Loss: 0.5421909
Validation loss decreased (1.303000 --> 1.299634).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.233778476715088
Epoch: 23, Steps: 62 | Train Loss: 0.3024414 Vali Loss: 1.2943288 Test Loss: 0.5389613
Validation loss decreased (1.299634 --> 1.294329).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.382495403289795
Epoch: 24, Steps: 62 | Train Loss: 0.3000671 Vali Loss: 1.2913176 Test Loss: 0.5356140
Validation loss decreased (1.294329 --> 1.291318).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.685166597366333
Epoch: 25, Steps: 62 | Train Loss: 0.2979554 Vali Loss: 1.2870924 Test Loss: 0.5326898
Validation loss decreased (1.291318 --> 1.287092).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.710641860961914
Epoch: 26, Steps: 62 | Train Loss: 0.2959264 Vali Loss: 1.2818028 Test Loss: 0.5299715
Validation loss decreased (1.287092 --> 1.281803).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.2507784366607666
Epoch: 27, Steps: 62 | Train Loss: 0.2941536 Vali Loss: 1.2789718 Test Loss: 0.5273166
Validation loss decreased (1.281803 --> 1.278972).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.030228614807129
Epoch: 28, Steps: 62 | Train Loss: 0.2923831 Vali Loss: 1.2756573 Test Loss: 0.5250085
Validation loss decreased (1.278972 --> 1.275657).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.6105990409851074
Epoch: 29, Steps: 62 | Train Loss: 0.2908425 Vali Loss: 1.2742767 Test Loss: 0.5226442
Validation loss decreased (1.275657 --> 1.274277).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.2551469802856445
Epoch: 30, Steps: 62 | Train Loss: 0.2893988 Vali Loss: 1.2724409 Test Loss: 0.5205038
Validation loss decreased (1.274277 --> 1.272441).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.8560073375701904
Epoch: 31, Steps: 62 | Train Loss: 0.2880784 Vali Loss: 1.2697141 Test Loss: 0.5184146
Validation loss decreased (1.272441 --> 1.269714).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.856076955795288
Epoch: 32, Steps: 62 | Train Loss: 0.2867296 Vali Loss: 1.2638065 Test Loss: 0.5165700
Validation loss decreased (1.269714 --> 1.263806).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.8212883472442627
Epoch: 33, Steps: 62 | Train Loss: 0.2855559 Vali Loss: 1.2627819 Test Loss: 0.5148041
Validation loss decreased (1.263806 --> 1.262782).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.8160488605499268
Epoch: 34, Steps: 62 | Train Loss: 0.2844264 Vali Loss: 1.2599691 Test Loss: 0.5130575
Validation loss decreased (1.262782 --> 1.259969).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.0801734924316406
Epoch: 35, Steps: 62 | Train Loss: 0.2833424 Vali Loss: 1.2604563 Test Loss: 0.5115723
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.6265506744384766
Epoch: 36, Steps: 62 | Train Loss: 0.2822972 Vali Loss: 1.2578958 Test Loss: 0.5100451
Validation loss decreased (1.259969 --> 1.257896).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.623469591140747
Epoch: 37, Steps: 62 | Train Loss: 0.2814543 Vali Loss: 1.2504511 Test Loss: 0.5086569
Validation loss decreased (1.257896 --> 1.250451).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6148173809051514
Epoch: 38, Steps: 62 | Train Loss: 0.2805876 Vali Loss: 1.2484719 Test Loss: 0.5072700
Validation loss decreased (1.250451 --> 1.248472).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.838608741760254
Epoch: 39, Steps: 62 | Train Loss: 0.2798113 Vali Loss: 1.2528223 Test Loss: 0.5061073
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.5709710121154785
Epoch: 40, Steps: 62 | Train Loss: 0.2789121 Vali Loss: 1.2494119 Test Loss: 0.5049564
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.08728289604187
Epoch: 41, Steps: 62 | Train Loss: 0.2782541 Vali Loss: 1.2490386 Test Loss: 0.5037783
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.100731134414673
Epoch: 42, Steps: 62 | Train Loss: 0.2776826 Vali Loss: 1.2511926 Test Loss: 0.5027557
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0682692527770996
Epoch: 43, Steps: 62 | Train Loss: 0.2770388 Vali Loss: 1.2494936 Test Loss: 0.5017276
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.414470672607422
Epoch: 44, Steps: 62 | Train Loss: 0.2764389 Vali Loss: 1.2455878 Test Loss: 0.5007178
Validation loss decreased (1.248472 --> 1.245588).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.990384817123413
Epoch: 45, Steps: 62 | Train Loss: 0.2757840 Vali Loss: 1.2397038 Test Loss: 0.4998975
Validation loss decreased (1.245588 --> 1.239704).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.9008688926696777
Epoch: 46, Steps: 62 | Train Loss: 0.2752766 Vali Loss: 1.2419728 Test Loss: 0.4990548
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.040895700454712
Epoch: 47, Steps: 62 | Train Loss: 0.2747428 Vali Loss: 1.2387117 Test Loss: 0.4982615
Validation loss decreased (1.239704 --> 1.238712).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.8427903652191162
Epoch: 48, Steps: 62 | Train Loss: 0.2743252 Vali Loss: 1.2377526 Test Loss: 0.4974981
Validation loss decreased (1.238712 --> 1.237753).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.7934763431549072
Epoch: 49, Steps: 62 | Train Loss: 0.2738531 Vali Loss: 1.2364889 Test Loss: 0.4967809
Validation loss decreased (1.237753 --> 1.236489).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3646633625030518
Epoch: 50, Steps: 62 | Train Loss: 0.2734530 Vali Loss: 1.2410277 Test Loss: 0.4960769
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.219470739364624
Epoch: 51, Steps: 62 | Train Loss: 0.2730896 Vali Loss: 1.2384915 Test Loss: 0.4954655
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.238557815551758
Epoch: 52, Steps: 62 | Train Loss: 0.2725979 Vali Loss: 1.2380424 Test Loss: 0.4948264
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.8780100345611572
Epoch: 53, Steps: 62 | Train Loss: 0.2722300 Vali Loss: 1.2377863 Test Loss: 0.4942674
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.8757684230804443
Epoch: 54, Steps: 62 | Train Loss: 0.2719845 Vali Loss: 1.2364190 Test Loss: 0.4937312
Validation loss decreased (1.236489 --> 1.236419).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.3503916263580322
Epoch: 55, Steps: 62 | Train Loss: 0.2716166 Vali Loss: 1.2317501 Test Loss: 0.4932129
Validation loss decreased (1.236419 --> 1.231750).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.5895466804504395
Epoch: 56, Steps: 62 | Train Loss: 0.2712847 Vali Loss: 1.2292992 Test Loss: 0.4927068
Validation loss decreased (1.231750 --> 1.229299).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.7574691772460938
Epoch: 57, Steps: 62 | Train Loss: 0.2710968 Vali Loss: 1.2340847 Test Loss: 0.4922501
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.3765931129455566
Epoch: 58, Steps: 62 | Train Loss: 0.2707025 Vali Loss: 1.2265899 Test Loss: 0.4918076
Validation loss decreased (1.229299 --> 1.226590).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.90267014503479
Epoch: 59, Steps: 62 | Train Loss: 0.2705594 Vali Loss: 1.2318544 Test Loss: 0.4913550
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.536379814147949
Epoch: 60, Steps: 62 | Train Loss: 0.2702633 Vali Loss: 1.2317357 Test Loss: 0.4909645
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.9410839080810547
Epoch: 61, Steps: 62 | Train Loss: 0.2700185 Vali Loss: 1.2307100 Test Loss: 0.4905421
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.517751455307007
Epoch: 62, Steps: 62 | Train Loss: 0.2698147 Vali Loss: 1.2292418 Test Loss: 0.4901738
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.8661565780639648
Epoch: 63, Steps: 62 | Train Loss: 0.2695841 Vali Loss: 1.2322804 Test Loss: 0.4898070
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.9292657375335693
Epoch: 64, Steps: 62 | Train Loss: 0.2693513 Vali Loss: 1.2263875 Test Loss: 0.4895259
Validation loss decreased (1.226590 --> 1.226388).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.264923572540283
Epoch: 65, Steps: 62 | Train Loss: 0.2691879 Vali Loss: 1.2283239 Test Loss: 0.4892142
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.6343066692352295
Epoch: 66, Steps: 62 | Train Loss: 0.2690579 Vali Loss: 1.2278349 Test Loss: 0.4889053
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.934880018234253
Epoch: 67, Steps: 62 | Train Loss: 0.2688208 Vali Loss: 1.2244368 Test Loss: 0.4886180
Validation loss decreased (1.226388 --> 1.224437).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.041078567504883
Epoch: 68, Steps: 62 | Train Loss: 0.2687168 Vali Loss: 1.2286745 Test Loss: 0.4883359
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.8024647235870361
Epoch: 69, Steps: 62 | Train Loss: 0.2684746 Vali Loss: 1.2282579 Test Loss: 0.4880939
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.3904712200164795
Epoch: 70, Steps: 62 | Train Loss: 0.2683729 Vali Loss: 1.2291175 Test Loss: 0.4878449
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 4.106217861175537
Epoch: 71, Steps: 62 | Train Loss: 0.2682130 Vali Loss: 1.2262888 Test Loss: 0.4876289
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.8207309246063232
Epoch: 72, Steps: 62 | Train Loss: 0.2679914 Vali Loss: 1.2276844 Test Loss: 0.4873976
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.8930578231811523
Epoch: 73, Steps: 62 | Train Loss: 0.2679629 Vali Loss: 1.2220905 Test Loss: 0.4872079
Validation loss decreased (1.224437 --> 1.222090).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.6039371490478516
Epoch: 74, Steps: 62 | Train Loss: 0.2678857 Vali Loss: 1.2258937 Test Loss: 0.4869944
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.697667360305786
Epoch: 75, Steps: 62 | Train Loss: 0.2677271 Vali Loss: 1.2254037 Test Loss: 0.4867998
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.8042173385620117
Epoch: 76, Steps: 62 | Train Loss: 0.2676156 Vali Loss: 1.2274640 Test Loss: 0.4866235
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.574104070663452
Epoch: 77, Steps: 62 | Train Loss: 0.2674911 Vali Loss: 1.2214504 Test Loss: 0.4864656
Validation loss decreased (1.222090 --> 1.221450).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.760932683944702
Epoch: 78, Steps: 62 | Train Loss: 0.2674470 Vali Loss: 1.2211788 Test Loss: 0.4862913
Validation loss decreased (1.221450 --> 1.221179).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.7021496295928955
Epoch: 79, Steps: 62 | Train Loss: 0.2673583 Vali Loss: 1.2260116 Test Loss: 0.4861509
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.0260708332061768
Epoch: 80, Steps: 62 | Train Loss: 0.2671603 Vali Loss: 1.2238537 Test Loss: 0.4860105
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.0773768424987793
Epoch: 81, Steps: 62 | Train Loss: 0.2670482 Vali Loss: 1.2268809 Test Loss: 0.4858647
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.2481257915496826
Epoch: 82, Steps: 62 | Train Loss: 0.2670846 Vali Loss: 1.2248710 Test Loss: 0.4857331
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.7782158851623535
Epoch: 83, Steps: 62 | Train Loss: 0.2669570 Vali Loss: 1.2195083 Test Loss: 0.4855959
Validation loss decreased (1.221179 --> 1.219508).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.812415361404419
Epoch: 84, Steps: 62 | Train Loss: 0.2668895 Vali Loss: 1.2285162 Test Loss: 0.4854890
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.0330207347869873
Epoch: 85, Steps: 62 | Train Loss: 0.2667992 Vali Loss: 1.2261893 Test Loss: 0.4853653
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 3.1875789165496826
Epoch: 86, Steps: 62 | Train Loss: 0.2668285 Vali Loss: 1.2233603 Test Loss: 0.4852692
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.142519235610962
Epoch: 87, Steps: 62 | Train Loss: 0.2666529 Vali Loss: 1.2237277 Test Loss: 0.4851490
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.5279946327209473
Epoch: 88, Steps: 62 | Train Loss: 0.2666488 Vali Loss: 1.2194334 Test Loss: 0.4850642
Validation loss decreased (1.219508 --> 1.219433).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.6571316719055176
Epoch: 89, Steps: 62 | Train Loss: 0.2665150 Vali Loss: 1.2228924 Test Loss: 0.4849657
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.728590488433838
Epoch: 90, Steps: 62 | Train Loss: 0.2665932 Vali Loss: 1.2218239 Test Loss: 0.4848735
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.2241172790527344
Epoch: 91, Steps: 62 | Train Loss: 0.2665235 Vali Loss: 1.2229369 Test Loss: 0.4848044
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.1658105850219727
Epoch: 92, Steps: 62 | Train Loss: 0.2665326 Vali Loss: 1.2238773 Test Loss: 0.4847121
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.81701397895813
Epoch: 93, Steps: 62 | Train Loss: 0.2663815 Vali Loss: 1.2228391 Test Loss: 0.4846321
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 3.0278849601745605
Epoch: 94, Steps: 62 | Train Loss: 0.2662628 Vali Loss: 1.2241163 Test Loss: 0.4845632
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.6075327396392822
Epoch: 95, Steps: 62 | Train Loss: 0.2663476 Vali Loss: 1.2198147 Test Loss: 0.4844970
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.7979466915130615
Epoch: 96, Steps: 62 | Train Loss: 0.2661920 Vali Loss: 1.2236378 Test Loss: 0.4844308
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.006460428237915
Epoch: 97, Steps: 62 | Train Loss: 0.2661954 Vali Loss: 1.2241608 Test Loss: 0.4843665
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.1306326389312744
Epoch: 98, Steps: 62 | Train Loss: 0.2662410 Vali Loss: 1.2233377 Test Loss: 0.4843088
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.2699832916259766
Epoch: 99, Steps: 62 | Train Loss: 0.2662048 Vali Loss: 1.2255609 Test Loss: 0.4842545
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.2228119373321533
Epoch: 100, Steps: 62 | Train Loss: 0.2660739 Vali Loss: 1.2206941 Test Loss: 0.4842020
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=74, out_features=143, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9481472.0
params:  10725.0
Trainable parameters:  10725
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.0838029384613037
Epoch: 1, Steps: 62 | Train Loss: 0.4795642 Vali Loss: 1.1870461 Test Loss: 0.4578995
Validation loss decreased (inf --> 1.187046).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.8042545318603516
Epoch: 2, Steps: 62 | Train Loss: 0.4681595 Vali Loss: 1.1713673 Test Loss: 0.4443697
Validation loss decreased (1.187046 --> 1.171367).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.2080328464508057
Epoch: 3, Steps: 62 | Train Loss: 0.4627095 Vali Loss: 1.1650234 Test Loss: 0.4382563
Validation loss decreased (1.171367 --> 1.165023).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.4977777004241943
Epoch: 4, Steps: 62 | Train Loss: 0.4600916 Vali Loss: 1.1571023 Test Loss: 0.4354829
Validation loss decreased (1.165023 --> 1.157102).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.639162063598633
Epoch: 5, Steps: 62 | Train Loss: 0.4587098 Vali Loss: 1.1553292 Test Loss: 0.4340440
Validation loss decreased (1.157102 --> 1.155329).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.4553334712982178
Epoch: 6, Steps: 62 | Train Loss: 0.4581400 Vali Loss: 1.1567611 Test Loss: 0.4334961
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1087558269500732
Epoch: 7, Steps: 62 | Train Loss: 0.4579371 Vali Loss: 1.1567323 Test Loss: 0.4333624
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.429112434387207
Epoch: 8, Steps: 62 | Train Loss: 0.4576814 Vali Loss: 1.1557425 Test Loss: 0.4333725
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.8931047916412354
Epoch: 9, Steps: 62 | Train Loss: 0.4575428 Vali Loss: 1.1572068 Test Loss: 0.4332237
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.563777446746826
Epoch: 10, Steps: 62 | Train Loss: 0.4572677 Vali Loss: 1.1572747 Test Loss: 0.4333553
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.1436142921447754
Epoch: 11, Steps: 62 | Train Loss: 0.4572154 Vali Loss: 1.1593778 Test Loss: 0.4331822
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9884002208709717
Epoch: 12, Steps: 62 | Train Loss: 0.4572273 Vali Loss: 1.1631147 Test Loss: 0.4331726
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.6445648670196533
Epoch: 13, Steps: 62 | Train Loss: 0.4570727 Vali Loss: 1.1573030 Test Loss: 0.4332623
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.199937582015991
Epoch: 14, Steps: 62 | Train Loss: 0.4569333 Vali Loss: 1.1605788 Test Loss: 0.4333574
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.9858129024505615
Epoch: 15, Steps: 62 | Train Loss: 0.4569646 Vali Loss: 1.1599659 Test Loss: 0.4333334
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.101436138153076
Epoch: 16, Steps: 62 | Train Loss: 0.4569804 Vali Loss: 1.1589302 Test Loss: 0.4331223
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.3543412685394287
Epoch: 17, Steps: 62 | Train Loss: 0.4567977 Vali Loss: 1.1580427 Test Loss: 0.4332023
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.651782274246216
Epoch: 18, Steps: 62 | Train Loss: 0.4567783 Vali Loss: 1.1583568 Test Loss: 0.4331748
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.193540096282959
Epoch: 19, Steps: 62 | Train Loss: 0.4568613 Vali Loss: 1.1587294 Test Loss: 0.4331929
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.9055933952331543
Epoch: 20, Steps: 62 | Train Loss: 0.4568435 Vali Loss: 1.1578817 Test Loss: 0.4331582
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.289077043533325
Epoch: 21, Steps: 62 | Train Loss: 0.4567051 Vali Loss: 1.1608044 Test Loss: 0.4331810
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.495201826095581
Epoch: 22, Steps: 62 | Train Loss: 0.4566495 Vali Loss: 1.1594040 Test Loss: 0.4331992
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.354020595550537
Epoch: 23, Steps: 62 | Train Loss: 0.4566805 Vali Loss: 1.1570840 Test Loss: 0.4333089
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.903179883956909
Epoch: 24, Steps: 62 | Train Loss: 0.4566111 Vali Loss: 1.1607060 Test Loss: 0.4332413
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.427532196044922
Epoch: 25, Steps: 62 | Train Loss: 0.4564248 Vali Loss: 1.1579847 Test Loss: 0.4333079
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4328486919403076, mae:0.4326068162918091, rse:0.6263543367385864, corr:[0.25174305 0.26102677 0.25997278 0.2563663  0.25440958 0.25380874
 0.25311676 0.25221482 0.25131425 0.25126952 0.25176266 0.2518594
 0.2513829  0.2507554  0.25040963 0.25039265 0.25026855 0.2499183
 0.24950771 0.24910338 0.24889874 0.24894688 0.24928401 0.2497225
 0.24976516 0.24928088 0.24869922 0.24843906 0.24844371 0.24839984
 0.2479997  0.24736504 0.24688248 0.24679855 0.24701257 0.24720193
 0.2471608  0.2469951  0.2469462  0.2470343  0.24726288 0.2474365
 0.2474548  0.24729732 0.24713008 0.24711804 0.24736232 0.24753983
 0.24716583 0.24624664 0.2451272  0.24442871 0.24403821 0.24349649
 0.24295223 0.24251443 0.24216679 0.24193329 0.24169749 0.24160953
 0.24150358 0.24147724 0.24149469 0.24160904 0.24176742 0.2419576
 0.24215944 0.24211715 0.24216168 0.24237105 0.24250872 0.24221891
 0.24140775 0.24038458 0.23969406 0.2395592  0.23966533 0.23955886
 0.23918475 0.23873638 0.2384579  0.23830704 0.23808584 0.237801
 0.23754202 0.23737258 0.2372862  0.23727925 0.23731807 0.2373177
 0.23707595 0.23660982 0.23621267 0.23622635 0.23670553 0.23749709
 0.23820865 0.23837936 0.23840791 0.23858444 0.23890503 0.23904623
 0.23882297 0.23846692 0.23818633 0.23807049 0.23799966 0.23784646
 0.23760675 0.23748302 0.23756692 0.23779768 0.23793313 0.2379744
 0.23795758 0.23779154 0.23747663 0.23711307 0.23690479 0.23702757
 0.23718616 0.23676772 0.23587523 0.2351151  0.23492421 0.23499912
 0.23498905 0.2347997  0.23443066 0.23410079 0.23389558 0.23363271
 0.23320788 0.23293668 0.23313406 0.23358957 0.23394911 0.23399463
 0.23389432 0.23385477 0.23393969 0.23397456 0.23382379 0.23362878
 0.2335095  0.23305929 0.23223525 0.23133953 0.2308914  0.23085812
 0.23101465 0.23106268 0.23087095 0.23081727 0.231058   0.23141862
 0.23150869 0.23136534 0.23117025 0.23112218 0.23108783 0.23111661
 0.2310195  0.23078173 0.23059802 0.23049495 0.2304535  0.2305771
 0.23093173 0.23128591 0.23141605 0.2313474  0.23128206 0.23128729
 0.23126173 0.23117504 0.23110458 0.23126577 0.23149444 0.23149972
 0.23120822 0.23085596 0.23075165 0.23098035 0.23134817 0.23173602
 0.23196836 0.23199868 0.23200648 0.23192993 0.23162065 0.23113322
 0.23049937 0.22971205 0.22878696 0.2281506  0.22798868 0.22807674
 0.22811235 0.22805452 0.2278049  0.22758411 0.22750899 0.22753055
 0.22741129 0.22714795 0.22698641 0.22707961 0.22732037 0.22744428
 0.22744487 0.22732411 0.22723459 0.2270715  0.22675003 0.22663793
 0.22689074 0.22698839 0.22687046 0.22644897 0.22606805 0.22601728
 0.22619466 0.2263609  0.22635287 0.22615041 0.22594057 0.22594997
 0.22581735 0.22527811 0.22473018 0.22450039 0.22456722 0.22462718
 0.22450143 0.2242244  0.22417156 0.22436227 0.22452302 0.22433153
 0.22419357 0.22410373 0.22405148 0.2239995  0.22393292 0.22395036
 0.22413853 0.22447582 0.224698   0.22450352 0.22422983 0.22424789
 0.22442828 0.22443938 0.22440462 0.22447036 0.22481415 0.22515546
 0.22515689 0.2247265  0.22456044 0.22492246 0.22531411 0.2251369
 0.22447689 0.2239184  0.22364926 0.22352372 0.22317208 0.22272372
 0.2225968  0.2227742  0.22282365 0.22243303 0.22183992 0.22154124
 0.22174321 0.22191697 0.2215996  0.22101824 0.22068025 0.22068235
 0.2208847  0.22085021 0.22092777 0.22132085 0.22198333 0.22240803
 0.22263767 0.22278985 0.2230572  0.22317317 0.22298783 0.22269605
 0.22257583 0.22265749 0.22268587 0.22255842 0.22227839 0.22226962
 0.22259432 0.22276144 0.22271778 0.2223942  0.22231118 0.22251119
 0.22278087 0.22273916 0.22275026 0.22293192 0.2231869  0.22317992
 0.22308122 0.22316131 0.22308059 0.22288094 0.22224015 0.22118467
 0.22047153 0.22058852 0.22078319 0.22045547 0.21927705 0.21846063
 0.21897009 0.21975817 0.21954496 0.21901065 0.21908753 0.22010341
 0.2203998  0.21846676 0.2163052  0.21747483 0.22065246 0.21566746]
