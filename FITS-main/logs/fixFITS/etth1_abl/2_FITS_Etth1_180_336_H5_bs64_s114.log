Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=50, out_features=143, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6406400.0
params:  7293.0
Trainable parameters:  7293
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.0658669471740723
Epoch: 1, Steps: 63 | Train Loss: 0.7979632 Vali Loss: 1.8633485 Test Loss: 0.8842901
Validation loss decreased (inf --> 1.863348).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.7551167011260986
Epoch: 2, Steps: 63 | Train Loss: 0.6071511 Vali Loss: 1.6352324 Test Loss: 0.7325414
Validation loss decreased (1.863348 --> 1.635232).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.315056324005127
Epoch: 3, Steps: 63 | Train Loss: 0.5118865 Vali Loss: 1.5175635 Test Loss: 0.6511258
Validation loss decreased (1.635232 --> 1.517563).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.5237865447998047
Epoch: 4, Steps: 63 | Train Loss: 0.4571871 Vali Loss: 1.4440895 Test Loss: 0.6018850
Validation loss decreased (1.517563 --> 1.444090).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.403355121612549
Epoch: 5, Steps: 63 | Train Loss: 0.4229193 Vali Loss: 1.4023640 Test Loss: 0.5699705
Validation loss decreased (1.444090 --> 1.402364).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.3176729679107666
Epoch: 6, Steps: 63 | Train Loss: 0.4005443 Vali Loss: 1.3695769 Test Loss: 0.5488023
Validation loss decreased (1.402364 --> 1.369577).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.4869654178619385
Epoch: 7, Steps: 63 | Train Loss: 0.3852036 Vali Loss: 1.3443526 Test Loss: 0.5342349
Validation loss decreased (1.369577 --> 1.344353).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.5022640228271484
Epoch: 8, Steps: 63 | Train Loss: 0.3739957 Vali Loss: 1.3344634 Test Loss: 0.5238508
Validation loss decreased (1.344353 --> 1.334463).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.0373167991638184
Epoch: 9, Steps: 63 | Train Loss: 0.3657617 Vali Loss: 1.3227903 Test Loss: 0.5159588
Validation loss decreased (1.334463 --> 1.322790).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.464860200881958
Epoch: 10, Steps: 63 | Train Loss: 0.3592538 Vali Loss: 1.3079457 Test Loss: 0.5098872
Validation loss decreased (1.322790 --> 1.307946).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.76745343208313
Epoch: 11, Steps: 63 | Train Loss: 0.3545374 Vali Loss: 1.2989025 Test Loss: 0.5050744
Validation loss decreased (1.307946 --> 1.298903).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.5934722423553467
Epoch: 12, Steps: 63 | Train Loss: 0.3504669 Vali Loss: 1.2921355 Test Loss: 0.5010194
Validation loss decreased (1.298903 --> 1.292135).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9944870471954346
Epoch: 13, Steps: 63 | Train Loss: 0.3471231 Vali Loss: 1.2888756 Test Loss: 0.4975178
Validation loss decreased (1.292135 --> 1.288876).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.434189796447754
Epoch: 14, Steps: 63 | Train Loss: 0.3444399 Vali Loss: 1.2830187 Test Loss: 0.4947634
Validation loss decreased (1.288876 --> 1.283019).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.1458377838134766
Epoch: 15, Steps: 63 | Train Loss: 0.3417432 Vali Loss: 1.2870070 Test Loss: 0.4921556
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.8476841449737549
Epoch: 16, Steps: 63 | Train Loss: 0.3398928 Vali Loss: 1.2770199 Test Loss: 0.4899748
Validation loss decreased (1.283019 --> 1.277020).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.7269763946533203
Epoch: 17, Steps: 63 | Train Loss: 0.3380833 Vali Loss: 1.2806433 Test Loss: 0.4880018
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.9748494625091553
Epoch: 18, Steps: 63 | Train Loss: 0.3363749 Vali Loss: 1.2748320 Test Loss: 0.4861605
Validation loss decreased (1.277020 --> 1.274832).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6466598510742188
Epoch: 19, Steps: 63 | Train Loss: 0.3347747 Vali Loss: 1.2711563 Test Loss: 0.4845809
Validation loss decreased (1.274832 --> 1.271156).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.110801935195923
Epoch: 20, Steps: 63 | Train Loss: 0.3334648 Vali Loss: 1.2696258 Test Loss: 0.4830301
Validation loss decreased (1.271156 --> 1.269626).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.8873026371002197
Epoch: 21, Steps: 63 | Train Loss: 0.3323669 Vali Loss: 1.2659912 Test Loss: 0.4817263
Validation loss decreased (1.269626 --> 1.265991).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.808222770690918
Epoch: 22, Steps: 63 | Train Loss: 0.3313603 Vali Loss: 1.2653151 Test Loss: 0.4805002
Validation loss decreased (1.265991 --> 1.265315).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9161899089813232
Epoch: 23, Steps: 63 | Train Loss: 0.3302584 Vali Loss: 1.2627132 Test Loss: 0.4793542
Validation loss decreased (1.265315 --> 1.262713).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.6123595237731934
Epoch: 24, Steps: 63 | Train Loss: 0.3292704 Vali Loss: 1.2618952 Test Loss: 0.4783283
Validation loss decreased (1.262713 --> 1.261895).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2143471240997314
Epoch: 25, Steps: 63 | Train Loss: 0.3287503 Vali Loss: 1.2644876 Test Loss: 0.4773981
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.705195426940918
Epoch: 26, Steps: 63 | Train Loss: 0.3277720 Vali Loss: 1.2588398 Test Loss: 0.4764616
Validation loss decreased (1.261895 --> 1.258840).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.497140884399414
Epoch: 27, Steps: 63 | Train Loss: 0.3275034 Vali Loss: 1.2585484 Test Loss: 0.4756872
Validation loss decreased (1.258840 --> 1.258548).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.9662034511566162
Epoch: 28, Steps: 63 | Train Loss: 0.3268412 Vali Loss: 1.2608678 Test Loss: 0.4749172
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.699794292449951
Epoch: 29, Steps: 63 | Train Loss: 0.3260095 Vali Loss: 1.2579455 Test Loss: 0.4742737
Validation loss decreased (1.258548 --> 1.257946).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.5865511894226074
Epoch: 30, Steps: 63 | Train Loss: 0.3256983 Vali Loss: 1.2565683 Test Loss: 0.4735642
Validation loss decreased (1.257946 --> 1.256568).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.4012086391448975
Epoch: 31, Steps: 63 | Train Loss: 0.3251269 Vali Loss: 1.2529827 Test Loss: 0.4729888
Validation loss decreased (1.256568 --> 1.252983).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.484790802001953
Epoch: 32, Steps: 63 | Train Loss: 0.3248010 Vali Loss: 1.2485517 Test Loss: 0.4724354
Validation loss decreased (1.252983 --> 1.248552).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.9240243434906006
Epoch: 33, Steps: 63 | Train Loss: 0.3243035 Vali Loss: 1.2473903 Test Loss: 0.4719257
Validation loss decreased (1.248552 --> 1.247390).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6590771675109863
Epoch: 34, Steps: 63 | Train Loss: 0.3239599 Vali Loss: 1.2523012 Test Loss: 0.4713859
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.8009581565856934
Epoch: 35, Steps: 63 | Train Loss: 0.3235901 Vali Loss: 1.2537199 Test Loss: 0.4709436
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.9754300117492676
Epoch: 36, Steps: 63 | Train Loss: 0.3231661 Vali Loss: 1.2511367 Test Loss: 0.4705347
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8505518436431885
Epoch: 37, Steps: 63 | Train Loss: 0.3228939 Vali Loss: 1.2458386 Test Loss: 0.4701543
Validation loss decreased (1.247390 --> 1.245839).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.3224215507507324
Epoch: 38, Steps: 63 | Train Loss: 0.3225288 Vali Loss: 1.2517225 Test Loss: 0.4697768
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.1206111907958984
Epoch: 39, Steps: 63 | Train Loss: 0.3222755 Vali Loss: 1.2479638 Test Loss: 0.4694335
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.261474609375
Epoch: 40, Steps: 63 | Train Loss: 0.3222955 Vali Loss: 1.2478613 Test Loss: 0.4690947
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.714731216430664
Epoch: 41, Steps: 63 | Train Loss: 0.3216708 Vali Loss: 1.2511637 Test Loss: 0.4687788
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.4398298263549805
Epoch: 42, Steps: 63 | Train Loss: 0.3218149 Vali Loss: 1.2546326 Test Loss: 0.4684948
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.029576539993286
Epoch: 43, Steps: 63 | Train Loss: 0.3213458 Vali Loss: 1.2462879 Test Loss: 0.4682563
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.6239378452301025
Epoch: 44, Steps: 63 | Train Loss: 0.3211392 Vali Loss: 1.2514130 Test Loss: 0.4679770
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.145576238632202
Epoch: 45, Steps: 63 | Train Loss: 0.3210982 Vali Loss: 1.2561808 Test Loss: 0.4677460
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.583381414413452
Epoch: 46, Steps: 63 | Train Loss: 0.3209988 Vali Loss: 1.2473618 Test Loss: 0.4675159
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.965684652328491
Epoch: 47, Steps: 63 | Train Loss: 0.3208825 Vali Loss: 1.2473153 Test Loss: 0.4672991
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.7166979312896729
Epoch: 48, Steps: 63 | Train Loss: 0.3201768 Vali Loss: 1.2502528 Test Loss: 0.4671137
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.824418067932129
Epoch: 49, Steps: 63 | Train Loss: 0.3201781 Vali Loss: 1.2480954 Test Loss: 0.4669169
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.7965431213378906
Epoch: 50, Steps: 63 | Train Loss: 0.3202014 Vali Loss: 1.2500299 Test Loss: 0.4667345
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.732060432434082
Epoch: 51, Steps: 63 | Train Loss: 0.3200086 Vali Loss: 1.2492062 Test Loss: 0.4665837
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.5637032985687256
Epoch: 52, Steps: 63 | Train Loss: 0.3197605 Vali Loss: 1.2457772 Test Loss: 0.4664142
Validation loss decreased (1.245839 --> 1.245777).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.101818323135376
Epoch: 53, Steps: 63 | Train Loss: 0.3200774 Vali Loss: 1.2475774 Test Loss: 0.4662603
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.761946439743042
Epoch: 54, Steps: 63 | Train Loss: 0.3198415 Vali Loss: 1.2453545 Test Loss: 0.4661267
Validation loss decreased (1.245777 --> 1.245355).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.6174979209899902
Epoch: 55, Steps: 63 | Train Loss: 0.3196841 Vali Loss: 1.2433405 Test Loss: 0.4660006
Validation loss decreased (1.245355 --> 1.243340).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.3067374229431152
Epoch: 56, Steps: 63 | Train Loss: 0.3194968 Vali Loss: 1.2468499 Test Loss: 0.4658784
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.847944974899292
Epoch: 57, Steps: 63 | Train Loss: 0.3194712 Vali Loss: 1.2496015 Test Loss: 0.4657586
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.7205541133880615
Epoch: 58, Steps: 63 | Train Loss: 0.3192194 Vali Loss: 1.2452750 Test Loss: 0.4656417
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.482808351516724
Epoch: 59, Steps: 63 | Train Loss: 0.3192847 Vali Loss: 1.2443067 Test Loss: 0.4655420
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.690516948699951
Epoch: 60, Steps: 63 | Train Loss: 0.3191857 Vali Loss: 1.2455602 Test Loss: 0.4654302
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.897585153579712
Epoch: 61, Steps: 63 | Train Loss: 0.3190990 Vali Loss: 1.2454138 Test Loss: 0.4653366
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 4.6494879722595215
Epoch: 62, Steps: 63 | Train Loss: 0.3192289 Vali Loss: 1.2447637 Test Loss: 0.4652589
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.6055338382720947
Epoch: 63, Steps: 63 | Train Loss: 0.3192672 Vali Loss: 1.2423557 Test Loss: 0.4651750
Validation loss decreased (1.243340 --> 1.242356).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.7602810859680176
Epoch: 64, Steps: 63 | Train Loss: 0.3188764 Vali Loss: 1.2491689 Test Loss: 0.4650947
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.8563108444213867
Epoch: 65, Steps: 63 | Train Loss: 0.3191483 Vali Loss: 1.2492298 Test Loss: 0.4650182
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.341240882873535
Epoch: 66, Steps: 63 | Train Loss: 0.3188364 Vali Loss: 1.2444249 Test Loss: 0.4649447
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.7206096649169922
Epoch: 67, Steps: 63 | Train Loss: 0.3185630 Vali Loss: 1.2452004 Test Loss: 0.4648792
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.5301191806793213
Epoch: 68, Steps: 63 | Train Loss: 0.3186269 Vali Loss: 1.2408968 Test Loss: 0.4648165
Validation loss decreased (1.242356 --> 1.240897).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.145143985748291
Epoch: 69, Steps: 63 | Train Loss: 0.3187474 Vali Loss: 1.2398406 Test Loss: 0.4647501
Validation loss decreased (1.240897 --> 1.239841).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.5275168418884277
Epoch: 70, Steps: 63 | Train Loss: 0.3185014 Vali Loss: 1.2446157 Test Loss: 0.4646938
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.560615301132202
Epoch: 71, Steps: 63 | Train Loss: 0.3183082 Vali Loss: 1.2450284 Test Loss: 0.4646451
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.5770606994628906
Epoch: 72, Steps: 63 | Train Loss: 0.3182596 Vali Loss: 1.2425625 Test Loss: 0.4645914
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.7134594917297363
Epoch: 73, Steps: 63 | Train Loss: 0.3180980 Vali Loss: 1.2439429 Test Loss: 0.4645384
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.1315507888793945
Epoch: 74, Steps: 63 | Train Loss: 0.3184610 Vali Loss: 1.2418027 Test Loss: 0.4644895
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.6232926845550537
Epoch: 75, Steps: 63 | Train Loss: 0.3183599 Vali Loss: 1.2421072 Test Loss: 0.4644516
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.4682445526123047
Epoch: 76, Steps: 63 | Train Loss: 0.3182421 Vali Loss: 1.2410167 Test Loss: 0.4644073
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.7612197399139404
Epoch: 77, Steps: 63 | Train Loss: 0.3182774 Vali Loss: 1.2448206 Test Loss: 0.4643690
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 4.20158839225769
Epoch: 78, Steps: 63 | Train Loss: 0.3182602 Vali Loss: 1.2473596 Test Loss: 0.4643311
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.527987480163574
Epoch: 79, Steps: 63 | Train Loss: 0.3183809 Vali Loss: 1.2408806 Test Loss: 0.4642964
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.2438042163848877
Epoch: 80, Steps: 63 | Train Loss: 0.3181679 Vali Loss: 1.2450899 Test Loss: 0.4642644
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.72078275680542
Epoch: 81, Steps: 63 | Train Loss: 0.3184426 Vali Loss: 1.2484487 Test Loss: 0.4642310
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.7703673839569092
Epoch: 82, Steps: 63 | Train Loss: 0.3183221 Vali Loss: 1.2461869 Test Loss: 0.4641963
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.6269233226776123
Epoch: 83, Steps: 63 | Train Loss: 0.3180103 Vali Loss: 1.2463154 Test Loss: 0.4641694
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.115217924118042
Epoch: 84, Steps: 63 | Train Loss: 0.3180890 Vali Loss: 1.2427623 Test Loss: 0.4641446
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.8764073848724365
Epoch: 85, Steps: 63 | Train Loss: 0.3181148 Vali Loss: 1.2391301 Test Loss: 0.4641203
Validation loss decreased (1.239841 --> 1.239130).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.4371488094329834
Epoch: 86, Steps: 63 | Train Loss: 0.3183011 Vali Loss: 1.2411065 Test Loss: 0.4640961
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.9781394004821777
Epoch: 87, Steps: 63 | Train Loss: 0.3180112 Vali Loss: 1.2473650 Test Loss: 0.4640724
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.6682043075561523
Epoch: 88, Steps: 63 | Train Loss: 0.3180964 Vali Loss: 1.2434517 Test Loss: 0.4640519
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 3.596012592315674
Epoch: 89, Steps: 63 | Train Loss: 0.3179501 Vali Loss: 1.2418638 Test Loss: 0.4640322
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.509429693222046
Epoch: 90, Steps: 63 | Train Loss: 0.3179011 Vali Loss: 1.2437996 Test Loss: 0.4640136
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 3.660953998565674
Epoch: 91, Steps: 63 | Train Loss: 0.3176677 Vali Loss: 1.2433677 Test Loss: 0.4639946
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.962195873260498
Epoch: 92, Steps: 63 | Train Loss: 0.3181271 Vali Loss: 1.2418166 Test Loss: 0.4639758
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.9353015422821045
Epoch: 93, Steps: 63 | Train Loss: 0.3179637 Vali Loss: 1.2423680 Test Loss: 0.4639588
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.4219443798065186
Epoch: 94, Steps: 63 | Train Loss: 0.3178789 Vali Loss: 1.2433239 Test Loss: 0.4639445
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.233358383178711
Epoch: 95, Steps: 63 | Train Loss: 0.3177831 Vali Loss: 1.2428746 Test Loss: 0.4639295
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 3.009007692337036
Epoch: 96, Steps: 63 | Train Loss: 0.3178335 Vali Loss: 1.2389561 Test Loss: 0.4639145
Validation loss decreased (1.239130 --> 1.238956).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.2477338314056396
Epoch: 97, Steps: 63 | Train Loss: 0.3179133 Vali Loss: 1.2438216 Test Loss: 0.4639030
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.873047113418579
Epoch: 98, Steps: 63 | Train Loss: 0.3177861 Vali Loss: 1.2437019 Test Loss: 0.4638895
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.700005292892456
Epoch: 99, Steps: 63 | Train Loss: 0.3176960 Vali Loss: 1.2379718 Test Loss: 0.4638784
Validation loss decreased (1.238956 --> 1.237972).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.360963821411133
Epoch: 100, Steps: 63 | Train Loss: 0.3179511 Vali Loss: 1.2421052 Test Loss: 0.4638671
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1160680107021042e-06
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=50, out_features=143, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6406400.0
params:  7293.0
Trainable parameters:  7293
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.0487000942230225
Epoch: 1, Steps: 63 | Train Loss: 0.4711300 Vali Loss: 1.2325734 Test Loss: 0.4585987
Validation loss decreased (inf --> 1.232573).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.106736183166504
Epoch: 2, Steps: 63 | Train Loss: 0.4679373 Vali Loss: 1.2385308 Test Loss: 0.4566154
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.6384363174438477
Epoch: 3, Steps: 63 | Train Loss: 0.4674960 Vali Loss: 1.2273926 Test Loss: 0.4558867
Validation loss decreased (1.232573 --> 1.227393).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.963243007659912
Epoch: 4, Steps: 63 | Train Loss: 0.4663903 Vali Loss: 1.2278702 Test Loss: 0.4556569
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.033833980560303
Epoch: 5, Steps: 63 | Train Loss: 0.4659730 Vali Loss: 1.2300005 Test Loss: 0.4558147
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2783002853393555
Epoch: 6, Steps: 63 | Train Loss: 0.4657062 Vali Loss: 1.2334358 Test Loss: 0.4559006
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.868649482727051
Epoch: 7, Steps: 63 | Train Loss: 0.4657663 Vali Loss: 1.2276201 Test Loss: 0.4556647
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.189569473266602
Epoch: 8, Steps: 63 | Train Loss: 0.4659354 Vali Loss: 1.2300026 Test Loss: 0.4556821
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.180326461791992
Epoch: 9, Steps: 63 | Train Loss: 0.4654581 Vali Loss: 1.2303940 Test Loss: 0.4557478
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.4107825756073
Epoch: 10, Steps: 63 | Train Loss: 0.4658997 Vali Loss: 1.2289152 Test Loss: 0.4558446
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.654500961303711
Epoch: 11, Steps: 63 | Train Loss: 0.4655816 Vali Loss: 1.2244412 Test Loss: 0.4560475
Validation loss decreased (1.227393 --> 1.224441).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.6129114627838135
Epoch: 12, Steps: 63 | Train Loss: 0.4659540 Vali Loss: 1.2296129 Test Loss: 0.4558366
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.5073258876800537
Epoch: 13, Steps: 63 | Train Loss: 0.4649279 Vali Loss: 1.2297634 Test Loss: 0.4557929
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.82112455368042
Epoch: 14, Steps: 63 | Train Loss: 0.4655547 Vali Loss: 1.2236276 Test Loss: 0.4559698
Validation loss decreased (1.224441 --> 1.223628).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.001610517501831
Epoch: 15, Steps: 63 | Train Loss: 0.4652876 Vali Loss: 1.2276050 Test Loss: 0.4559032
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.1730103492736816
Epoch: 16, Steps: 63 | Train Loss: 0.4650979 Vali Loss: 1.2259893 Test Loss: 0.4560406
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.2171239852905273
Epoch: 17, Steps: 63 | Train Loss: 0.4652127 Vali Loss: 1.2225877 Test Loss: 0.4559399
Validation loss decreased (1.223628 --> 1.222588).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.66349458694458
Epoch: 18, Steps: 63 | Train Loss: 0.4653750 Vali Loss: 1.2265803 Test Loss: 0.4560339
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0204265117645264
Epoch: 19, Steps: 63 | Train Loss: 0.4651527 Vali Loss: 1.2323543 Test Loss: 0.4559190
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8067352771759033
Epoch: 20, Steps: 63 | Train Loss: 0.4649669 Vali Loss: 1.2258718 Test Loss: 0.4561167
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.2915799617767334
Epoch: 21, Steps: 63 | Train Loss: 0.4651914 Vali Loss: 1.2217567 Test Loss: 0.4560612
Validation loss decreased (1.222588 --> 1.221757).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.4580166339874268
Epoch: 22, Steps: 63 | Train Loss: 0.4655982 Vali Loss: 1.2251042 Test Loss: 0.4560853
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.208118677139282
Epoch: 23, Steps: 63 | Train Loss: 0.4650441 Vali Loss: 1.2258096 Test Loss: 0.4559516
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.258033752441406
Epoch: 24, Steps: 63 | Train Loss: 0.4652726 Vali Loss: 1.2288903 Test Loss: 0.4560493
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.019108772277832
Epoch: 25, Steps: 63 | Train Loss: 0.4654326 Vali Loss: 1.2276174 Test Loss: 0.4560956
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8096799850463867
Epoch: 26, Steps: 63 | Train Loss: 0.4647459 Vali Loss: 1.2208774 Test Loss: 0.4559547
Validation loss decreased (1.221757 --> 1.220877).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.596858024597168
Epoch: 27, Steps: 63 | Train Loss: 0.4651098 Vali Loss: 1.2298173 Test Loss: 0.4560829
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.392286777496338
Epoch: 28, Steps: 63 | Train Loss: 0.4650811 Vali Loss: 1.2226589 Test Loss: 0.4560801
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.8859775066375732
Epoch: 29, Steps: 63 | Train Loss: 0.4651229 Vali Loss: 1.2259283 Test Loss: 0.4560850
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.5040149688720703
Epoch: 30, Steps: 63 | Train Loss: 0.4654999 Vali Loss: 1.2268568 Test Loss: 0.4560982
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.6454451084136963
Epoch: 31, Steps: 63 | Train Loss: 0.4647241 Vali Loss: 1.2269546 Test Loss: 0.4560962
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.9993858337402344
Epoch: 32, Steps: 63 | Train Loss: 0.4652768 Vali Loss: 1.2293876 Test Loss: 0.4561410
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.948350429534912
Epoch: 33, Steps: 63 | Train Loss: 0.4649523 Vali Loss: 1.2256802 Test Loss: 0.4560837
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.4993860721588135
Epoch: 34, Steps: 63 | Train Loss: 0.4651730 Vali Loss: 1.2304038 Test Loss: 0.4561012
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.072756052017212
Epoch: 35, Steps: 63 | Train Loss: 0.4651262 Vali Loss: 1.2280128 Test Loss: 0.4561258
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.2323243618011475
Epoch: 36, Steps: 63 | Train Loss: 0.4650579 Vali Loss: 1.2283748 Test Loss: 0.4561281
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.2113037109375
Epoch: 37, Steps: 63 | Train Loss: 0.4649709 Vali Loss: 1.2300667 Test Loss: 0.4560922
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9249458312988281
Epoch: 38, Steps: 63 | Train Loss: 0.4649517 Vali Loss: 1.2238197 Test Loss: 0.4561432
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.340407609939575
Epoch: 39, Steps: 63 | Train Loss: 0.4649592 Vali Loss: 1.2252373 Test Loss: 0.4561446
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.8327033519744873
Epoch: 40, Steps: 63 | Train Loss: 0.4644009 Vali Loss: 1.2233819 Test Loss: 0.4561513
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.1110641956329346
Epoch: 41, Steps: 63 | Train Loss: 0.4647129 Vali Loss: 1.2305592 Test Loss: 0.4561482
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.72451114654541
Epoch: 42, Steps: 63 | Train Loss: 0.4646918 Vali Loss: 1.2249407 Test Loss: 0.4562012
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.308948040008545
Epoch: 43, Steps: 63 | Train Loss: 0.4647618 Vali Loss: 1.2272590 Test Loss: 0.4562058
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.4550411701202393
Epoch: 44, Steps: 63 | Train Loss: 0.4650383 Vali Loss: 1.2286584 Test Loss: 0.4561614
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.4201393127441406
Epoch: 45, Steps: 63 | Train Loss: 0.4644272 Vali Loss: 1.2307343 Test Loss: 0.4561807
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.420866012573242
Epoch: 46, Steps: 63 | Train Loss: 0.4650153 Vali Loss: 1.2255589 Test Loss: 0.4561858
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.45524752140045166, mae:0.4349759519100189, rse:0.6423560976982117, corr:[0.25359127 0.25905308 0.2586725  0.2576052  0.25565255 0.2526717
 0.25124916 0.25167558 0.2512918  0.2508499  0.25111696 0.25136566
 0.2510982  0.25054076 0.25036895 0.25041652 0.25044474 0.25033453
 0.2501179  0.24978489 0.24955858 0.24976104 0.25014883 0.2499329
 0.24900842 0.24842069 0.2478465  0.2473435  0.24686374 0.24637651
 0.24587154 0.24559072 0.24550332 0.24538167 0.24538274 0.24592471
 0.2462833  0.24601074 0.24605724 0.24628973 0.24643748 0.24670489
 0.2471597  0.24733484 0.24727355 0.24751692 0.24820547 0.2482522
 0.24712496 0.24588251 0.24426092 0.24278611 0.24161407 0.24056873
 0.2397713  0.23941201 0.2394805  0.23980996 0.23970106 0.24034292
 0.24091792 0.2405966  0.24042064 0.24049298 0.24041438 0.24030429
 0.24053726 0.24054453 0.24029787 0.24034002 0.24072485 0.24026407
 0.23879117 0.23761156 0.23658633 0.23602849 0.23585446 0.23561631
 0.23534143 0.23497257 0.23475252 0.23483764 0.23453985 0.23436432
 0.23446147 0.23429124 0.23446053 0.23461454 0.23432937 0.23406294
 0.23410061 0.23406042 0.23392501 0.234265   0.23499833 0.23527436
 0.23468946 0.23418856 0.23364978 0.2329092  0.23256256 0.23231572
 0.23190896 0.2319191  0.23214883 0.23238946 0.2323402  0.23266657
 0.2329343  0.23251772 0.23220886 0.2323169  0.23252521 0.23258676
 0.23254268 0.23248865 0.23249644 0.2327069  0.23290811 0.23243913
 0.23122144 0.2302155  0.22903295 0.2278008  0.22681911 0.22632708
 0.22613305 0.22631945 0.22661203 0.2269495  0.2269859  0.22760107
 0.22851843 0.22853759 0.2284625  0.22847259 0.2283962  0.22839499
 0.22863361 0.22877233 0.2286264  0.22875808 0.22909795 0.2288234
 0.22776188 0.22683538 0.22584192 0.22457296 0.22379626 0.22336778
 0.22317526 0.2232222  0.2237121  0.22415751 0.22403863 0.22423953
 0.22485857 0.22498594 0.22505519 0.2249171  0.22452283 0.22433588
 0.22444278 0.22454801 0.22447976 0.2246198  0.22490604 0.2245917
 0.22356693 0.22290035 0.22231534 0.22163558 0.22115201 0.22078668
 0.22070058 0.22121903 0.22189038 0.22263834 0.22314154 0.22383773
 0.22443104 0.22429271 0.2242053  0.22426988 0.22425842 0.22454591
 0.22499058 0.22517522 0.22523057 0.22553399 0.22583665 0.22543056
 0.22427581 0.22359736 0.22277181 0.22162916 0.22074477 0.22034729
 0.22018139 0.22007003 0.2203203  0.22083426 0.22091433 0.22138433
 0.22201909 0.2219988  0.22187223 0.22166869 0.2213205  0.22115663
 0.22128071 0.22130725 0.22121164 0.22138973 0.22170717 0.22138722
 0.22049871 0.22013746 0.21969008 0.21888109 0.21834292 0.21790004
 0.21779668 0.21804143 0.21850021 0.21853669 0.21815936 0.21853834
 0.21941373 0.21936636 0.21915902 0.21915632 0.21904275 0.21888876
 0.21885104 0.21885362 0.21882822 0.21901086 0.21935888 0.21919438
 0.21835677 0.21764435 0.21687719 0.21617796 0.21587507 0.21584845
 0.21592693 0.21608949 0.21652175 0.217082   0.21740778 0.21809374
 0.21895202 0.21906656 0.21915433 0.21922214 0.2190669  0.2189407
 0.21905181 0.21896839 0.2187562  0.21886364 0.21927129 0.21908858
 0.21821003 0.21773632 0.21704541 0.21626508 0.21591084 0.21574825
 0.21553777 0.21548    0.21568125 0.21602203 0.2160563  0.2162229
 0.21688063 0.21685283 0.2167537  0.21674259 0.21651    0.21628162
 0.2164212  0.21658006 0.2165889  0.21680893 0.21733287 0.21736543
 0.21665327 0.2162631  0.21603885 0.21581148 0.21571264 0.21586992
 0.21589817 0.2160364  0.21660219 0.21708812 0.21705686 0.21743435
 0.21820651 0.21809204 0.21802974 0.21820329 0.21820997 0.21800546
 0.218211   0.21856475 0.2186001  0.21858294 0.21884555 0.21862094
 0.21766512 0.21726105 0.21689382 0.21604967 0.21557099 0.21508996
 0.21485141 0.21487905 0.21522401 0.21575305 0.21561575 0.21584862
 0.21725097 0.21762541 0.21718934 0.21728681 0.21784192 0.21771489
 0.21720016 0.21754041 0.21713129 0.21560805 0.2160971  0.21677992]
