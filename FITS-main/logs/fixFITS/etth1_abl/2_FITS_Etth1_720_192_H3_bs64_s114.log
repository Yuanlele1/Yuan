Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=103, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=103, out_features=130, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11997440.0
params:  13520.0
Trainable parameters:  13520
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.85330867767334
Epoch: 1, Steps: 60 | Train Loss: 0.6717611 Vali Loss: 1.6087899 Test Loss: 0.8172232
Validation loss decreased (inf --> 1.608790).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.630401849746704
Epoch: 2, Steps: 60 | Train Loss: 0.5424510 Vali Loss: 1.4609960 Test Loss: 0.7530664
Validation loss decreased (1.608790 --> 1.460996).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.392091274261475
Epoch: 3, Steps: 60 | Train Loss: 0.4719419 Vali Loss: 1.3944007 Test Loss: 0.7261364
Validation loss decreased (1.460996 --> 1.394401).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.793585300445557
Epoch: 4, Steps: 60 | Train Loss: 0.4284922 Vali Loss: 1.3554946 Test Loss: 0.7109678
Validation loss decreased (1.394401 --> 1.355495).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.497697353363037
Epoch: 5, Steps: 60 | Train Loss: 0.3981862 Vali Loss: 1.3302468 Test Loss: 0.7000476
Validation loss decreased (1.355495 --> 1.330247).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.3895087242126465
Epoch: 6, Steps: 60 | Train Loss: 0.3744164 Vali Loss: 1.3083165 Test Loss: 0.6893316
Validation loss decreased (1.330247 --> 1.308316).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.4883012771606445
Epoch: 7, Steps: 60 | Train Loss: 0.3545629 Vali Loss: 1.2934042 Test Loss: 0.6814723
Validation loss decreased (1.308316 --> 1.293404).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.428866147994995
Epoch: 8, Steps: 60 | Train Loss: 0.3378079 Vali Loss: 1.2777429 Test Loss: 0.6721466
Validation loss decreased (1.293404 --> 1.277743).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.400092124938965
Epoch: 9, Steps: 60 | Train Loss: 0.3230944 Vali Loss: 1.2639253 Test Loss: 0.6636895
Validation loss decreased (1.277743 --> 1.263925).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.310080528259277
Epoch: 10, Steps: 60 | Train Loss: 0.3103806 Vali Loss: 1.2503644 Test Loss: 0.6552691
Validation loss decreased (1.263925 --> 1.250364).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.310852527618408
Epoch: 11, Steps: 60 | Train Loss: 0.2987673 Vali Loss: 1.2381680 Test Loss: 0.6471438
Validation loss decreased (1.250364 --> 1.238168).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.536192893981934
Epoch: 12, Steps: 60 | Train Loss: 0.2886802 Vali Loss: 1.2258844 Test Loss: 0.6390282
Validation loss decreased (1.238168 --> 1.225884).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.387902021408081
Epoch: 13, Steps: 60 | Train Loss: 0.2791945 Vali Loss: 1.2150830 Test Loss: 0.6312001
Validation loss decreased (1.225884 --> 1.215083).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.307743787765503
Epoch: 14, Steps: 60 | Train Loss: 0.2710650 Vali Loss: 1.2058140 Test Loss: 0.6247461
Validation loss decreased (1.215083 --> 1.205814).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.427913665771484
Epoch: 15, Steps: 60 | Train Loss: 0.2636023 Vali Loss: 1.1957475 Test Loss: 0.6177582
Validation loss decreased (1.205814 --> 1.195747).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.265151262283325
Epoch: 16, Steps: 60 | Train Loss: 0.2567479 Vali Loss: 1.1858392 Test Loss: 0.6102361
Validation loss decreased (1.195747 --> 1.185839).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.397679328918457
Epoch: 17, Steps: 60 | Train Loss: 0.2505541 Vali Loss: 1.1777939 Test Loss: 0.6046119
Validation loss decreased (1.185839 --> 1.177794).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.776030778884888
Epoch: 18, Steps: 60 | Train Loss: 0.2447221 Vali Loss: 1.1692368 Test Loss: 0.5983260
Validation loss decreased (1.177794 --> 1.169237).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.790649175643921
Epoch: 19, Steps: 60 | Train Loss: 0.2394554 Vali Loss: 1.1628102 Test Loss: 0.5937368
Validation loss decreased (1.169237 --> 1.162810).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.324182748794556
Epoch: 20, Steps: 60 | Train Loss: 0.2345805 Vali Loss: 1.1561661 Test Loss: 0.5887769
Validation loss decreased (1.162810 --> 1.156166).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.1583571434021
Epoch: 21, Steps: 60 | Train Loss: 0.2303028 Vali Loss: 1.1490912 Test Loss: 0.5834048
Validation loss decreased (1.156166 --> 1.149091).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.313838005065918
Epoch: 22, Steps: 60 | Train Loss: 0.2261931 Vali Loss: 1.1430002 Test Loss: 0.5791140
Validation loss decreased (1.149091 --> 1.143000).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.2550108432769775
Epoch: 23, Steps: 60 | Train Loss: 0.2224623 Vali Loss: 1.1370004 Test Loss: 0.5743303
Validation loss decreased (1.143000 --> 1.137000).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.931889295578003
Epoch: 24, Steps: 60 | Train Loss: 0.2188660 Vali Loss: 1.1322749 Test Loss: 0.5707530
Validation loss decreased (1.137000 --> 1.132275).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.189330339431763
Epoch: 25, Steps: 60 | Train Loss: 0.2157067 Vali Loss: 1.1267226 Test Loss: 0.5663410
Validation loss decreased (1.132275 --> 1.126723).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.286018371582031
Epoch: 26, Steps: 60 | Train Loss: 0.2126663 Vali Loss: 1.1225042 Test Loss: 0.5633488
Validation loss decreased (1.126723 --> 1.122504).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.0682373046875
Epoch: 27, Steps: 60 | Train Loss: 0.2098975 Vali Loss: 1.1174268 Test Loss: 0.5597771
Validation loss decreased (1.122504 --> 1.117427).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.9314870834350586
Epoch: 28, Steps: 60 | Train Loss: 0.2073071 Vali Loss: 1.1140225 Test Loss: 0.5570202
Validation loss decreased (1.117427 --> 1.114022).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.410619258880615
Epoch: 29, Steps: 60 | Train Loss: 0.2047728 Vali Loss: 1.1097481 Test Loss: 0.5536596
Validation loss decreased (1.114022 --> 1.109748).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.375078916549683
Epoch: 30, Steps: 60 | Train Loss: 0.2027015 Vali Loss: 1.1061219 Test Loss: 0.5508266
Validation loss decreased (1.109748 --> 1.106122).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.481307744979858
Epoch: 31, Steps: 60 | Train Loss: 0.2004600 Vali Loss: 1.1026827 Test Loss: 0.5480053
Validation loss decreased (1.106122 --> 1.102683).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.903162956237793
Epoch: 32, Steps: 60 | Train Loss: 0.1985358 Vali Loss: 1.0996869 Test Loss: 0.5460005
Validation loss decreased (1.102683 --> 1.099687).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.8598620891571045
Epoch: 33, Steps: 60 | Train Loss: 0.1967308 Vali Loss: 1.0968777 Test Loss: 0.5437968
Validation loss decreased (1.099687 --> 1.096878).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.883486270904541
Epoch: 34, Steps: 60 | Train Loss: 0.1950089 Vali Loss: 1.0937238 Test Loss: 0.5411576
Validation loss decreased (1.096878 --> 1.093724).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.847264051437378
Epoch: 35, Steps: 60 | Train Loss: 0.1933970 Vali Loss: 1.0909113 Test Loss: 0.5390033
Validation loss decreased (1.093724 --> 1.090911).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.824284791946411
Epoch: 36, Steps: 60 | Train Loss: 0.1918501 Vali Loss: 1.0878603 Test Loss: 0.5367094
Validation loss decreased (1.090911 --> 1.087860).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.039238929748535
Epoch: 37, Steps: 60 | Train Loss: 0.1904840 Vali Loss: 1.0860593 Test Loss: 0.5352020
Validation loss decreased (1.087860 --> 1.086059).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.535788536071777
Epoch: 38, Steps: 60 | Train Loss: 0.1890953 Vali Loss: 1.0835416 Test Loss: 0.5334010
Validation loss decreased (1.086059 --> 1.083542).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 5.45965051651001
Epoch: 39, Steps: 60 | Train Loss: 0.1878451 Vali Loss: 1.0811889 Test Loss: 0.5318890
Validation loss decreased (1.083542 --> 1.081189).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 5.04200005531311
Epoch: 40, Steps: 60 | Train Loss: 0.1866695 Vali Loss: 1.0792856 Test Loss: 0.5301347
Validation loss decreased (1.081189 --> 1.079286).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.137665748596191
Epoch: 41, Steps: 60 | Train Loss: 0.1855178 Vali Loss: 1.0772278 Test Loss: 0.5287144
Validation loss decreased (1.079286 --> 1.077228).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.72533917427063
Epoch: 42, Steps: 60 | Train Loss: 0.1843272 Vali Loss: 1.0758272 Test Loss: 0.5273023
Validation loss decreased (1.077228 --> 1.075827).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.489952087402344
Epoch: 43, Steps: 60 | Train Loss: 0.1833789 Vali Loss: 1.0741398 Test Loss: 0.5259847
Validation loss decreased (1.075827 --> 1.074140).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.357907056808472
Epoch: 44, Steps: 60 | Train Loss: 0.1824935 Vali Loss: 1.0719483 Test Loss: 0.5246368
Validation loss decreased (1.074140 --> 1.071948).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.196467399597168
Epoch: 45, Steps: 60 | Train Loss: 0.1814991 Vali Loss: 1.0704719 Test Loss: 0.5234469
Validation loss decreased (1.071948 --> 1.070472).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.824634313583374
Epoch: 46, Steps: 60 | Train Loss: 0.1806660 Vali Loss: 1.0695237 Test Loss: 0.5224361
Validation loss decreased (1.070472 --> 1.069524).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.868145704269409
Epoch: 47, Steps: 60 | Train Loss: 0.1800026 Vali Loss: 1.0673797 Test Loss: 0.5211083
Validation loss decreased (1.069524 --> 1.067380).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.47203516960144
Epoch: 48, Steps: 60 | Train Loss: 0.1792096 Vali Loss: 1.0666732 Test Loss: 0.5200711
Validation loss decreased (1.067380 --> 1.066673).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 4.491893768310547
Epoch: 49, Steps: 60 | Train Loss: 0.1784303 Vali Loss: 1.0649923 Test Loss: 0.5192308
Validation loss decreased (1.066673 --> 1.064992).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.296982526779175
Epoch: 50, Steps: 60 | Train Loss: 0.1777374 Vali Loss: 1.0641513 Test Loss: 0.5182004
Validation loss decreased (1.064992 --> 1.064151).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.718228101730347
Epoch: 51, Steps: 60 | Train Loss: 0.1772055 Vali Loss: 1.0630734 Test Loss: 0.5174515
Validation loss decreased (1.064151 --> 1.063073).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.097012519836426
Epoch: 52, Steps: 60 | Train Loss: 0.1765145 Vali Loss: 1.0620278 Test Loss: 0.5165497
Validation loss decreased (1.063073 --> 1.062028).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.543800354003906
Epoch: 53, Steps: 60 | Train Loss: 0.1760330 Vali Loss: 1.0604910 Test Loss: 0.5156296
Validation loss decreased (1.062028 --> 1.060491).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 4.67018985748291
Epoch: 54, Steps: 60 | Train Loss: 0.1755629 Vali Loss: 1.0601039 Test Loss: 0.5151173
Validation loss decreased (1.060491 --> 1.060104).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.59043025970459
Epoch: 55, Steps: 60 | Train Loss: 0.1749510 Vali Loss: 1.0588424 Test Loss: 0.5144269
Validation loss decreased (1.060104 --> 1.058842).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 4.1724841594696045
Epoch: 56, Steps: 60 | Train Loss: 0.1744833 Vali Loss: 1.0583194 Test Loss: 0.5135986
Validation loss decreased (1.058842 --> 1.058319).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.658482551574707
Epoch: 57, Steps: 60 | Train Loss: 0.1739648 Vali Loss: 1.0573440 Test Loss: 0.5129706
Validation loss decreased (1.058319 --> 1.057344).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 5.11138916015625
Epoch: 58, Steps: 60 | Train Loss: 0.1736098 Vali Loss: 1.0570643 Test Loss: 0.5124627
Validation loss decreased (1.057344 --> 1.057064).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 5.341367244720459
Epoch: 59, Steps: 60 | Train Loss: 0.1732654 Vali Loss: 1.0556331 Test Loss: 0.5117304
Validation loss decreased (1.057064 --> 1.055633).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 4.454415321350098
Epoch: 60, Steps: 60 | Train Loss: 0.1727043 Vali Loss: 1.0553169 Test Loss: 0.5111790
Validation loss decreased (1.055633 --> 1.055317).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 4.400684118270874
Epoch: 61, Steps: 60 | Train Loss: 0.1723429 Vali Loss: 1.0547099 Test Loss: 0.5106654
Validation loss decreased (1.055317 --> 1.054710).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 4.666652679443359
Epoch: 62, Steps: 60 | Train Loss: 0.1720067 Vali Loss: 1.0540044 Test Loss: 0.5101651
Validation loss decreased (1.054710 --> 1.054004).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 4.7341086864471436
Epoch: 63, Steps: 60 | Train Loss: 0.1717222 Vali Loss: 1.0536615 Test Loss: 0.5097301
Validation loss decreased (1.054004 --> 1.053661).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 4.576727390289307
Epoch: 64, Steps: 60 | Train Loss: 0.1713486 Vali Loss: 1.0530537 Test Loss: 0.5093146
Validation loss decreased (1.053661 --> 1.053054).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 4.767525672912598
Epoch: 65, Steps: 60 | Train Loss: 0.1710017 Vali Loss: 1.0520148 Test Loss: 0.5089015
Validation loss decreased (1.053054 --> 1.052015).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 4.179903745651245
Epoch: 66, Steps: 60 | Train Loss: 0.1707006 Vali Loss: 1.0521097 Test Loss: 0.5084984
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 4.434840202331543
Epoch: 67, Steps: 60 | Train Loss: 0.1704426 Vali Loss: 1.0509499 Test Loss: 0.5080734
Validation loss decreased (1.052015 --> 1.050950).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 4.359672546386719
Epoch: 68, Steps: 60 | Train Loss: 0.1702147 Vali Loss: 1.0510523 Test Loss: 0.5077201
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 4.501356363296509
Epoch: 69, Steps: 60 | Train Loss: 0.1699408 Vali Loss: 1.0503660 Test Loss: 0.5073579
Validation loss decreased (1.050950 --> 1.050366).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.8844287395477295
Epoch: 70, Steps: 60 | Train Loss: 0.1697291 Vali Loss: 1.0499029 Test Loss: 0.5070571
Validation loss decreased (1.050366 --> 1.049903).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 4.692249774932861
Epoch: 71, Steps: 60 | Train Loss: 0.1695143 Vali Loss: 1.0498822 Test Loss: 0.5067437
Validation loss decreased (1.049903 --> 1.049882).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 4.5446226596832275
Epoch: 72, Steps: 60 | Train Loss: 0.1692266 Vali Loss: 1.0495003 Test Loss: 0.5064497
Validation loss decreased (1.049882 --> 1.049500).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 4.696018695831299
Epoch: 73, Steps: 60 | Train Loss: 0.1690057 Vali Loss: 1.0490416 Test Loss: 0.5061691
Validation loss decreased (1.049500 --> 1.049042).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 4.639938592910767
Epoch: 74, Steps: 60 | Train Loss: 0.1689183 Vali Loss: 1.0485133 Test Loss: 0.5058960
Validation loss decreased (1.049042 --> 1.048513).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 4.7195329666137695
Epoch: 75, Steps: 60 | Train Loss: 0.1686360 Vali Loss: 1.0479105 Test Loss: 0.5056840
Validation loss decreased (1.048513 --> 1.047910).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 4.395662546157837
Epoch: 76, Steps: 60 | Train Loss: 0.1685210 Vali Loss: 1.0480387 Test Loss: 0.5053774
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 4.875085830688477
Epoch: 77, Steps: 60 | Train Loss: 0.1684041 Vali Loss: 1.0477667 Test Loss: 0.5051057
Validation loss decreased (1.047910 --> 1.047767).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 4.73369836807251
Epoch: 78, Steps: 60 | Train Loss: 0.1681839 Vali Loss: 1.0473413 Test Loss: 0.5049199
Validation loss decreased (1.047767 --> 1.047341).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 4.658219337463379
Epoch: 79, Steps: 60 | Train Loss: 0.1680552 Vali Loss: 1.0466974 Test Loss: 0.5046992
Validation loss decreased (1.047341 --> 1.046697).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 4.238734483718872
Epoch: 80, Steps: 60 | Train Loss: 0.1679584 Vali Loss: 1.0469716 Test Loss: 0.5044469
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 4.7337870597839355
Epoch: 81, Steps: 60 | Train Loss: 0.1678752 Vali Loss: 1.0462343 Test Loss: 0.5043028
Validation loss decreased (1.046697 --> 1.046234).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 4.977434873580933
Epoch: 82, Steps: 60 | Train Loss: 0.1677033 Vali Loss: 1.0465553 Test Loss: 0.5041154
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 4.5522613525390625
Epoch: 83, Steps: 60 | Train Loss: 0.1676125 Vali Loss: 1.0462188 Test Loss: 0.5039339
Validation loss decreased (1.046234 --> 1.046219).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 4.473211765289307
Epoch: 84, Steps: 60 | Train Loss: 0.1674325 Vali Loss: 1.0456557 Test Loss: 0.5037386
Validation loss decreased (1.046219 --> 1.045656).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 5.017251253128052
Epoch: 85, Steps: 60 | Train Loss: 0.1672982 Vali Loss: 1.0457428 Test Loss: 0.5036231
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 4.62448525428772
Epoch: 86, Steps: 60 | Train Loss: 0.1671411 Vali Loss: 1.0455326 Test Loss: 0.5034805
Validation loss decreased (1.045656 --> 1.045533).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 4.689059734344482
Epoch: 87, Steps: 60 | Train Loss: 0.1670567 Vali Loss: 1.0455842 Test Loss: 0.5033768
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 4.207634449005127
Epoch: 88, Steps: 60 | Train Loss: 0.1669725 Vali Loss: 1.0452696 Test Loss: 0.5032092
Validation loss decreased (1.045533 --> 1.045270).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 4.520141363143921
Epoch: 89, Steps: 60 | Train Loss: 0.1669371 Vali Loss: 1.0447469 Test Loss: 0.5030870
Validation loss decreased (1.045270 --> 1.044747).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 4.389258861541748
Epoch: 90, Steps: 60 | Train Loss: 0.1667746 Vali Loss: 1.0450217 Test Loss: 0.5029737
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 4.690276145935059
Epoch: 91, Steps: 60 | Train Loss: 0.1668108 Vali Loss: 1.0449493 Test Loss: 0.5028487
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 4.46844482421875
Epoch: 92, Steps: 60 | Train Loss: 0.1665748 Vali Loss: 1.0442755 Test Loss: 0.5027460
Validation loss decreased (1.044747 --> 1.044276).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 4.547463417053223
Epoch: 93, Steps: 60 | Train Loss: 0.1666146 Vali Loss: 1.0441916 Test Loss: 0.5026424
Validation loss decreased (1.044276 --> 1.044192).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 5.071601390838623
Epoch: 94, Steps: 60 | Train Loss: 0.1665397 Vali Loss: 1.0441841 Test Loss: 0.5025257
Validation loss decreased (1.044192 --> 1.044184).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 4.627427101135254
Epoch: 95, Steps: 60 | Train Loss: 0.1664393 Vali Loss: 1.0442911 Test Loss: 0.5024239
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 5.095589637756348
Epoch: 96, Steps: 60 | Train Loss: 0.1664599 Vali Loss: 1.0437711 Test Loss: 0.5023220
Validation loss decreased (1.044184 --> 1.043771).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 4.659111261367798
Epoch: 97, Steps: 60 | Train Loss: 0.1663524 Vali Loss: 1.0438635 Test Loss: 0.5022442
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 4.462470531463623
Epoch: 98, Steps: 60 | Train Loss: 0.1661699 Vali Loss: 1.0439059 Test Loss: 0.5021710
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 4.153196811676025
Epoch: 99, Steps: 60 | Train Loss: 0.1661747 Vali Loss: 1.0439379 Test Loss: 0.5020975
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 4.435183525085449
Epoch: 100, Steps: 60 | Train Loss: 0.1661817 Vali Loss: 1.0434713 Test Loss: 0.5020326
Validation loss decreased (1.043771 --> 1.043471).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=103, out_features=130, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11997440.0
params:  13520.0
Trainable parameters:  13520
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.586339712142944
Epoch: 1, Steps: 60 | Train Loss: 0.4182912 Vali Loss: 0.9672477 Test Loss: 0.4386157
Validation loss decreased (inf --> 0.967248).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.505357980728149
Epoch: 2, Steps: 60 | Train Loss: 0.3957213 Vali Loss: 0.9549006 Test Loss: 0.4252838
Validation loss decreased (0.967248 --> 0.954901).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.367610692977905
Epoch: 3, Steps: 60 | Train Loss: 0.3906398 Vali Loss: 0.9554994 Test Loss: 0.4237742
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.241860866546631
Epoch: 4, Steps: 60 | Train Loss: 0.3894362 Vali Loss: 0.9590186 Test Loss: 0.4245451
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.095989942550659
Epoch: 5, Steps: 60 | Train Loss: 0.3882057 Vali Loss: 0.9598527 Test Loss: 0.4247115
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.736761569976807
Epoch: 6, Steps: 60 | Train Loss: 0.3875943 Vali Loss: 0.9603816 Test Loss: 0.4253577
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.745113849639893
Epoch: 7, Steps: 60 | Train Loss: 0.3872771 Vali Loss: 0.9604670 Test Loss: 0.4252543
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.601960897445679
Epoch: 8, Steps: 60 | Train Loss: 0.3871110 Vali Loss: 0.9624479 Test Loss: 0.4253959
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.3936073780059814
Epoch: 9, Steps: 60 | Train Loss: 0.3869561 Vali Loss: 0.9644771 Test Loss: 0.4258648
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.403249979019165
Epoch: 10, Steps: 60 | Train Loss: 0.3866397 Vali Loss: 0.9644352 Test Loss: 0.4255826
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.540016174316406
Epoch: 11, Steps: 60 | Train Loss: 0.3861317 Vali Loss: 0.9643587 Test Loss: 0.4257848
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.668997526168823
Epoch: 12, Steps: 60 | Train Loss: 0.3860958 Vali Loss: 0.9643385 Test Loss: 0.4256506
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.872676134109497
Epoch: 13, Steps: 60 | Train Loss: 0.3860458 Vali Loss: 0.9634532 Test Loss: 0.4255797
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.248585224151611
Epoch: 14, Steps: 60 | Train Loss: 0.3856499 Vali Loss: 0.9636772 Test Loss: 0.4256923
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.5046546459198
Epoch: 15, Steps: 60 | Train Loss: 0.3854444 Vali Loss: 0.9645584 Test Loss: 0.4257933
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.522860050201416
Epoch: 16, Steps: 60 | Train Loss: 0.3857010 Vali Loss: 0.9648433 Test Loss: 0.4258426
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.444422960281372
Epoch: 17, Steps: 60 | Train Loss: 0.3855601 Vali Loss: 0.9643189 Test Loss: 0.4254991
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.483180522918701
Epoch: 18, Steps: 60 | Train Loss: 0.3855707 Vali Loss: 0.9646790 Test Loss: 0.4255365
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.252931594848633
Epoch: 19, Steps: 60 | Train Loss: 0.3855436 Vali Loss: 0.9641746 Test Loss: 0.4258151
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.876568078994751
Epoch: 20, Steps: 60 | Train Loss: 0.3848040 Vali Loss: 0.9641481 Test Loss: 0.4256056
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.446438550949097
Epoch: 21, Steps: 60 | Train Loss: 0.3851421 Vali Loss: 0.9651982 Test Loss: 0.4256962
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.764109134674072
Epoch: 22, Steps: 60 | Train Loss: 0.3853501 Vali Loss: 0.9651355 Test Loss: 0.4257724
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4223479628562927, mae:0.42968273162841797, rse:0.6171525716781616, corr:[0.26085985 0.26991442 0.27065918 0.26715285 0.26310503 0.2605122
 0.25983664 0.26041096 0.26113284 0.26162404 0.26162988 0.26118025
 0.26065844 0.26033935 0.26026198 0.26024988 0.2599764  0.25944066
 0.25870803 0.2580389  0.25762647 0.25760227 0.25770223 0.25786406
 0.25790995 0.25790164 0.25787187 0.25767168 0.25731415 0.25689375
 0.25649643 0.25601652 0.25566357 0.25552663 0.25547847 0.2555568
 0.25578395 0.25605336 0.2561244  0.2559857  0.25582764 0.2555855
 0.2552565  0.25509048 0.25524017 0.25571346 0.25641772 0.25705302
 0.2570137  0.25643006 0.25531858 0.25400206 0.25272557 0.2514629
 0.25051925 0.2501135  0.25014538 0.25046232 0.25067225 0.25067097
 0.2504754  0.250156   0.24982059 0.2497592  0.24996942 0.25030488
 0.25063902 0.25068194 0.25052932 0.25029555 0.25002727 0.24962239
 0.24912114 0.24863979 0.24821392 0.24801221 0.24787828 0.24762456
 0.24723175 0.24673553 0.24614425 0.24549833 0.24485183 0.24434116
 0.24417818 0.24423525 0.24431026 0.24419448 0.24385446 0.24344644
 0.24307558 0.24290003 0.24296434 0.2433301  0.24398091 0.24465325
 0.2452153  0.2455601  0.24557    0.24547428 0.24540254 0.24530472
 0.2451955  0.24505499 0.24478878 0.24447386 0.24401529 0.24342145
 0.24296251 0.2427598  0.24292484 0.24345554 0.24400064 0.24435247
 0.24446635 0.24431211 0.2440157  0.24376968 0.2436235  0.24344271
 0.2431093  0.2423788  0.24118051 0.23988038 0.23881358 0.23809764
 0.23789127 0.23802646 0.23802692 0.23781723 0.23736055 0.23681329
 0.23639043 0.23630261 0.23663354 0.23714274 0.23768231 0.23791263
 0.23770791 0.23713684 0.23660003 0.2363629  0.23637623 0.23613183
 0.23548147 0.23438646 0.2330344  0.2318059  0.23115812 0.23085216
 0.230645   0.23035195 0.22994003 0.2296019  0.22927362 0.22901475
 0.2287657  0.22849154 0.22814786 0.22768651 0.22731309 0.22746082
 0.22790554 0.22830892 0.22855575 0.22850831 0.22833525 0.22789991
 0.22752368 0.22744414 0.22730395 0.22687422 0.22606872 0.22526231
 0.22476757 0.22483139 0.22486185 0.22463216 0.22373274 0.2222859
 0.2207503  0.21964131 0.21921296 0.2192816  0.21940792 0.21929304
 0.2188381  0.21826333 0.21832122 0.21855143 0.215912   0.2056112 ]
