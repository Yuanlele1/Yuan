Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_96_FITS_ETTh1_ftM_sl90_ll48_pl96_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8455
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=26, out_features=53, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1234688.0
params:  1431.0
Trainable parameters:  1431
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.644477128982544
Epoch: 1, Steps: 66 | Train Loss: 0.6523128 Vali Loss: 1.0777085 Test Loss: 0.6736435
Validation loss decreased (inf --> 1.077708).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6719748973846436
Epoch: 2, Steps: 66 | Train Loss: 0.4931175 Vali Loss: 0.8934692 Test Loss: 0.5118573
Validation loss decreased (1.077708 --> 0.893469).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.58046817779541
Epoch: 3, Steps: 66 | Train Loss: 0.4286043 Vali Loss: 0.8147196 Test Loss: 0.4479268
Validation loss decreased (0.893469 --> 0.814720).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.1813149452209473
Epoch: 4, Steps: 66 | Train Loss: 0.4001396 Vali Loss: 0.7742804 Test Loss: 0.4203899
Validation loss decreased (0.814720 --> 0.774280).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.828477621078491
Epoch: 5, Steps: 66 | Train Loss: 0.3865336 Vali Loss: 0.7588290 Test Loss: 0.4076435
Validation loss decreased (0.774280 --> 0.758829).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.9463088512420654
Epoch: 6, Steps: 66 | Train Loss: 0.3790249 Vali Loss: 0.7458355 Test Loss: 0.4010976
Validation loss decreased (0.758829 --> 0.745835).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.291320562362671
Epoch: 7, Steps: 66 | Train Loss: 0.3744776 Vali Loss: 0.7398619 Test Loss: 0.3973039
Validation loss decreased (0.745835 --> 0.739862).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.6993560791015625
Epoch: 8, Steps: 66 | Train Loss: 0.3714962 Vali Loss: 0.7295035 Test Loss: 0.3950033
Validation loss decreased (0.739862 --> 0.729504).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.465681314468384
Epoch: 9, Steps: 66 | Train Loss: 0.3695367 Vali Loss: 0.7302030 Test Loss: 0.3933552
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.0595650672912598
Epoch: 10, Steps: 66 | Train Loss: 0.3679333 Vali Loss: 0.7250164 Test Loss: 0.3922441
Validation loss decreased (0.729504 --> 0.725016).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7747712135314941
Epoch: 11, Steps: 66 | Train Loss: 0.3665699 Vali Loss: 0.7251124 Test Loss: 0.3913148
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.8344879150390625
Epoch: 12, Steps: 66 | Train Loss: 0.3657110 Vali Loss: 0.7206853 Test Loss: 0.3907700
Validation loss decreased (0.725016 --> 0.720685).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.7447614669799805
Epoch: 13, Steps: 66 | Train Loss: 0.3649125 Vali Loss: 0.7220085 Test Loss: 0.3903218
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.366016387939453
Epoch: 14, Steps: 66 | Train Loss: 0.3642978 Vali Loss: 0.7146649 Test Loss: 0.3899449
Validation loss decreased (0.720685 --> 0.714665).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.072471857070923
Epoch: 15, Steps: 66 | Train Loss: 0.3637072 Vali Loss: 0.7223181 Test Loss: 0.3896201
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.223181962966919
Epoch: 16, Steps: 66 | Train Loss: 0.3633043 Vali Loss: 0.7143303 Test Loss: 0.3894310
Validation loss decreased (0.714665 --> 0.714330).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.289395093917847
Epoch: 17, Steps: 66 | Train Loss: 0.3628930 Vali Loss: 0.7203729 Test Loss: 0.3892597
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4923009872436523
Epoch: 18, Steps: 66 | Train Loss: 0.3626347 Vali Loss: 0.7146580 Test Loss: 0.3891255
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.669328212738037
Epoch: 19, Steps: 66 | Train Loss: 0.3622571 Vali Loss: 0.7181884 Test Loss: 0.3890566
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.9477622509002686
Epoch: 20, Steps: 66 | Train Loss: 0.3621048 Vali Loss: 0.7136591 Test Loss: 0.3890118
Validation loss decreased (0.714330 --> 0.713659).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.204632043838501
Epoch: 21, Steps: 66 | Train Loss: 0.3619761 Vali Loss: 0.7125925 Test Loss: 0.3888773
Validation loss decreased (0.713659 --> 0.712592).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.756204843521118
Epoch: 22, Steps: 66 | Train Loss: 0.3617191 Vali Loss: 0.7108578 Test Loss: 0.3888786
Validation loss decreased (0.712592 --> 0.710858).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.3799631595611572
Epoch: 23, Steps: 66 | Train Loss: 0.3616705 Vali Loss: 0.7145231 Test Loss: 0.3888242
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.60001540184021
Epoch: 24, Steps: 66 | Train Loss: 0.3615151 Vali Loss: 0.7133704 Test Loss: 0.3887862
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.3859057426452637
Epoch: 25, Steps: 66 | Train Loss: 0.3613625 Vali Loss: 0.7138323 Test Loss: 0.3887667
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.6751468181610107
Epoch: 26, Steps: 66 | Train Loss: 0.3612970 Vali Loss: 0.7130710 Test Loss: 0.3887623
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.4748404026031494
Epoch: 27, Steps: 66 | Train Loss: 0.3611761 Vali Loss: 0.7117818 Test Loss: 0.3887230
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.9134039878845215
Epoch: 28, Steps: 66 | Train Loss: 0.3608564 Vali Loss: 0.7100644 Test Loss: 0.3887222
Validation loss decreased (0.710858 --> 0.710064).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.5105910301208496
Epoch: 29, Steps: 66 | Train Loss: 0.3609868 Vali Loss: 0.7115666 Test Loss: 0.3887168
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.168450355529785
Epoch: 30, Steps: 66 | Train Loss: 0.3609215 Vali Loss: 0.7121656 Test Loss: 0.3887057
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.6553122997283936
Epoch: 31, Steps: 66 | Train Loss: 0.3609199 Vali Loss: 0.7110258 Test Loss: 0.3887114
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.7291109561920166
Epoch: 32, Steps: 66 | Train Loss: 0.3606686 Vali Loss: 0.7102864 Test Loss: 0.3887404
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.6076927185058594
Epoch: 33, Steps: 66 | Train Loss: 0.3607477 Vali Loss: 0.7114829 Test Loss: 0.3887360
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.042677402496338
Epoch: 34, Steps: 66 | Train Loss: 0.3605817 Vali Loss: 0.7105010 Test Loss: 0.3887333
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.5332448482513428
Epoch: 35, Steps: 66 | Train Loss: 0.3606646 Vali Loss: 0.7104338 Test Loss: 0.3886937
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.0765604972839355
Epoch: 36, Steps: 66 | Train Loss: 0.3605306 Vali Loss: 0.7101399 Test Loss: 0.3887377
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.4043314456939697
Epoch: 37, Steps: 66 | Train Loss: 0.3603530 Vali Loss: 0.7114610 Test Loss: 0.3887085
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.044624090194702
Epoch: 38, Steps: 66 | Train Loss: 0.3605402 Vali Loss: 0.7076028 Test Loss: 0.3887173
Validation loss decreased (0.710064 --> 0.707603).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.989389419555664
Epoch: 39, Steps: 66 | Train Loss: 0.3605421 Vali Loss: 0.7134082 Test Loss: 0.3887398
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.485417604446411
Epoch: 40, Steps: 66 | Train Loss: 0.3604528 Vali Loss: 0.7101005 Test Loss: 0.3887292
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.654538869857788
Epoch: 41, Steps: 66 | Train Loss: 0.3603109 Vali Loss: 0.7105735 Test Loss: 0.3886941
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.9899561405181885
Epoch: 42, Steps: 66 | Train Loss: 0.3601947 Vali Loss: 0.7137017 Test Loss: 0.3887211
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.1505286693573
Epoch: 43, Steps: 66 | Train Loss: 0.3603899 Vali Loss: 0.7127883 Test Loss: 0.3887263
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.3029072284698486
Epoch: 44, Steps: 66 | Train Loss: 0.3604408 Vali Loss: 0.7058098 Test Loss: 0.3887418
Validation loss decreased (0.707603 --> 0.705810).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.763209581375122
Epoch: 45, Steps: 66 | Train Loss: 0.3602752 Vali Loss: 0.7065312 Test Loss: 0.3887378
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.061192274093628
Epoch: 46, Steps: 66 | Train Loss: 0.3603627 Vali Loss: 0.7134807 Test Loss: 0.3887174
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.127364158630371
Epoch: 47, Steps: 66 | Train Loss: 0.3602861 Vali Loss: 0.7045604 Test Loss: 0.3887553
Validation loss decreased (0.705810 --> 0.704560).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.695051908493042
Epoch: 48, Steps: 66 | Train Loss: 0.3601786 Vali Loss: 0.7098846 Test Loss: 0.3887511
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.7154250144958496
Epoch: 49, Steps: 66 | Train Loss: 0.3602207 Vali Loss: 0.7112685 Test Loss: 0.3887547
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.9795291423797607
Epoch: 50, Steps: 66 | Train Loss: 0.3600607 Vali Loss: 0.7088533 Test Loss: 0.3887446
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.438216209411621
Epoch: 51, Steps: 66 | Train Loss: 0.3600444 Vali Loss: 0.7101337 Test Loss: 0.3887474
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.3923768997192383
Epoch: 52, Steps: 66 | Train Loss: 0.3601819 Vali Loss: 0.7133017 Test Loss: 0.3887407
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.8355417251586914
Epoch: 53, Steps: 66 | Train Loss: 0.3601940 Vali Loss: 0.7084370 Test Loss: 0.3887599
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.7046949863433838
Epoch: 54, Steps: 66 | Train Loss: 0.3601823 Vali Loss: 0.7098723 Test Loss: 0.3887542
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.128718137741089
Epoch: 55, Steps: 66 | Train Loss: 0.3601530 Vali Loss: 0.7104530 Test Loss: 0.3887568
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.244891405105591
Epoch: 56, Steps: 66 | Train Loss: 0.3602569 Vali Loss: 0.7091740 Test Loss: 0.3887610
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.1324830055236816
Epoch: 57, Steps: 66 | Train Loss: 0.3601724 Vali Loss: 0.7046587 Test Loss: 0.3887632
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.6312541961669922
Epoch: 58, Steps: 66 | Train Loss: 0.3601842 Vali Loss: 0.7058905 Test Loss: 0.3887643
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.7389090061187744
Epoch: 59, Steps: 66 | Train Loss: 0.3601488 Vali Loss: 0.7080463 Test Loss: 0.3887548
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.3763692378997803
Epoch: 60, Steps: 66 | Train Loss: 0.3601400 Vali Loss: 0.7079903 Test Loss: 0.3887707
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.073857545852661
Epoch: 61, Steps: 66 | Train Loss: 0.3600933 Vali Loss: 0.7098011 Test Loss: 0.3887689
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.8107874393463135
Epoch: 62, Steps: 66 | Train Loss: 0.3600532 Vali Loss: 0.7098364 Test Loss: 0.3887788
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.3644802570343018
Epoch: 63, Steps: 66 | Train Loss: 0.3601112 Vali Loss: 0.7068201 Test Loss: 0.3887711
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.723236560821533
Epoch: 64, Steps: 66 | Train Loss: 0.3600269 Vali Loss: 0.7067137 Test Loss: 0.3887663
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.001593828201294
Epoch: 65, Steps: 66 | Train Loss: 0.3601059 Vali Loss: 0.7092662 Test Loss: 0.3887770
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.4970133304595947
Epoch: 66, Steps: 66 | Train Loss: 0.3599833 Vali Loss: 0.7096780 Test Loss: 0.3887815
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.410625696182251
Epoch: 67, Steps: 66 | Train Loss: 0.3601489 Vali Loss: 0.7106580 Test Loss: 0.3887772
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_96_FITS_ETTh1_ftM_sl90_ll48_pl96_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.38797107338905334, mae:0.3954029977321625, rse:0.5916399955749512, corr:[0.27063674 0.27378163 0.2720604  0.2727678  0.27079833 0.2676367
 0.26716572 0.26724643 0.26653168 0.26573256 0.26556218 0.26552337
 0.26511437 0.2647454  0.26468983 0.26499096 0.26537296 0.26552075
 0.26571992 0.26581317 0.26523924 0.2647086  0.26419398 0.26364118
 0.26228634 0.26140797 0.26140353 0.26192427 0.26189068 0.26154888
 0.26176026 0.26190206 0.26176187 0.261248   0.26095775 0.261225
 0.2612691  0.2610327  0.26110226 0.26139933 0.2617738  0.26203477
 0.26237184 0.26258418 0.26240844 0.2620612  0.26187053 0.26137322
 0.2600718  0.25884658 0.2579546  0.25735444 0.25631726 0.25502864
 0.2548085  0.25472596 0.25472024 0.2547506  0.25445172 0.25459683
 0.2548171  0.2547288  0.25445035 0.2542383  0.25443703 0.25457716
 0.2547615  0.2549411  0.25481823 0.25442538 0.25388706 0.2529647
 0.25150454 0.25038862 0.24952321 0.24923053 0.24901864 0.24877726
 0.24911787 0.24914612 0.2491995  0.24908401 0.24867514 0.2485481
 0.24868195 0.24853028 0.24815255 0.24788056 0.24803986 0.24771701
 0.24705173 0.24714594 0.2467281  0.24502322 0.24516249 0.24795043]
