Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.548724889755249
Epoch: 1, Steps: 56 | Train Loss: 0.8909898 Vali Loss: 1.8278899 Test Loss: 0.7069761
Validation loss decreased (inf --> 1.827890).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6406140327453613
Epoch: 2, Steps: 56 | Train Loss: 0.7193062 Vali Loss: 1.6869625 Test Loss: 0.6141303
Validation loss decreased (1.827890 --> 1.686962).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.5942573547363281
Epoch: 3, Steps: 56 | Train Loss: 0.6714658 Vali Loss: 1.6258272 Test Loss: 0.5679553
Validation loss decreased (1.686962 --> 1.625827).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6946275234222412
Epoch: 4, Steps: 56 | Train Loss: 0.6436845 Vali Loss: 1.5820875 Test Loss: 0.5357041
Validation loss decreased (1.625827 --> 1.582088).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.641012191772461
Epoch: 5, Steps: 56 | Train Loss: 0.6246133 Vali Loss: 1.5477355 Test Loss: 0.5114982
Validation loss decreased (1.582088 --> 1.547735).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.695112705230713
Epoch: 6, Steps: 56 | Train Loss: 0.6103256 Vali Loss: 1.5263253 Test Loss: 0.4931143
Validation loss decreased (1.547735 --> 1.526325).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5516564846038818
Epoch: 7, Steps: 56 | Train Loss: 0.5995674 Vali Loss: 1.5069821 Test Loss: 0.4789072
Validation loss decreased (1.526325 --> 1.506982).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8684535026550293
Epoch: 8, Steps: 56 | Train Loss: 0.5914768 Vali Loss: 1.4946736 Test Loss: 0.4678512
Validation loss decreased (1.506982 --> 1.494674).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7609546184539795
Epoch: 9, Steps: 56 | Train Loss: 0.5846882 Vali Loss: 1.4783266 Test Loss: 0.4592518
Validation loss decreased (1.494674 --> 1.478327).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.7583856582641602
Epoch: 10, Steps: 56 | Train Loss: 0.5796286 Vali Loss: 1.4719582 Test Loss: 0.4526028
Validation loss decreased (1.478327 --> 1.471958).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.8080177307128906
Epoch: 11, Steps: 56 | Train Loss: 0.5753875 Vali Loss: 1.4622680 Test Loss: 0.4475619
Validation loss decreased (1.471958 --> 1.462268).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.6072032451629639
Epoch: 12, Steps: 56 | Train Loss: 0.5719831 Vali Loss: 1.4574306 Test Loss: 0.4435622
Validation loss decreased (1.462268 --> 1.457431).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6606197357177734
Epoch: 13, Steps: 56 | Train Loss: 0.5691879 Vali Loss: 1.4532160 Test Loss: 0.4405725
Validation loss decreased (1.457431 --> 1.453216).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.7105910778045654
Epoch: 14, Steps: 56 | Train Loss: 0.5669144 Vali Loss: 1.4537477 Test Loss: 0.4382860
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6303000450134277
Epoch: 15, Steps: 56 | Train Loss: 0.5651621 Vali Loss: 1.4492157 Test Loss: 0.4364379
Validation loss decreased (1.453216 --> 1.449216).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7059154510498047
Epoch: 16, Steps: 56 | Train Loss: 0.5634131 Vali Loss: 1.4457371 Test Loss: 0.4350529
Validation loss decreased (1.449216 --> 1.445737).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.6373684406280518
Epoch: 17, Steps: 56 | Train Loss: 0.5623190 Vali Loss: 1.4357442 Test Loss: 0.4339714
Validation loss decreased (1.445737 --> 1.435744).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7158563137054443
Epoch: 18, Steps: 56 | Train Loss: 0.5611860 Vali Loss: 1.4350274 Test Loss: 0.4331225
Validation loss decreased (1.435744 --> 1.435027).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.8048710823059082
Epoch: 19, Steps: 56 | Train Loss: 0.5600819 Vali Loss: 1.4375346 Test Loss: 0.4325237
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.7614092826843262
Epoch: 20, Steps: 56 | Train Loss: 0.5593334 Vali Loss: 1.4352129 Test Loss: 0.4320751
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.8384501934051514
Epoch: 21, Steps: 56 | Train Loss: 0.5583849 Vali Loss: 1.4383816 Test Loss: 0.4317590
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.8451673984527588
Epoch: 22, Steps: 56 | Train Loss: 0.5580514 Vali Loss: 1.4327712 Test Loss: 0.4315186
Validation loss decreased (1.435027 --> 1.432771).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6919143199920654
Epoch: 23, Steps: 56 | Train Loss: 0.5576098 Vali Loss: 1.4309393 Test Loss: 0.4312606
Validation loss decreased (1.432771 --> 1.430939).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.5732429027557373
Epoch: 24, Steps: 56 | Train Loss: 0.5570866 Vali Loss: 1.4341936 Test Loss: 0.4311556
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.6619701385498047
Epoch: 25, Steps: 56 | Train Loss: 0.5562125 Vali Loss: 1.4324129 Test Loss: 0.4310929
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.6754298210144043
Epoch: 26, Steps: 56 | Train Loss: 0.5560084 Vali Loss: 1.4371762 Test Loss: 0.4310122
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5450410842895508
Epoch: 27, Steps: 56 | Train Loss: 0.5562552 Vali Loss: 1.4387181 Test Loss: 0.4310186
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.667677640914917
Epoch: 28, Steps: 56 | Train Loss: 0.5553857 Vali Loss: 1.4347196 Test Loss: 0.4310102
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6896111965179443
Epoch: 29, Steps: 56 | Train Loss: 0.5550274 Vali Loss: 1.4368800 Test Loss: 0.4309467
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.8062129020690918
Epoch: 30, Steps: 56 | Train Loss: 0.5546925 Vali Loss: 1.4300151 Test Loss: 0.4309742
Validation loss decreased (1.430939 --> 1.430015).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6016957759857178
Epoch: 31, Steps: 56 | Train Loss: 0.5549169 Vali Loss: 1.4331677 Test Loss: 0.4310343
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.6999626159667969
Epoch: 32, Steps: 56 | Train Loss: 0.5545111 Vali Loss: 1.4365243 Test Loss: 0.4310428
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.7462787628173828
Epoch: 33, Steps: 56 | Train Loss: 0.5541780 Vali Loss: 1.4329947 Test Loss: 0.4310704
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6444759368896484
Epoch: 34, Steps: 56 | Train Loss: 0.5542402 Vali Loss: 1.4300838 Test Loss: 0.4310986
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.633802890777588
Epoch: 35, Steps: 56 | Train Loss: 0.5537556 Vali Loss: 1.4313829 Test Loss: 0.4311078
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.5270013809204102
Epoch: 36, Steps: 56 | Train Loss: 0.5535336 Vali Loss: 1.4291737 Test Loss: 0.4311833
Validation loss decreased (1.430015 --> 1.429174).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9931302070617676
Epoch: 37, Steps: 56 | Train Loss: 0.5536069 Vali Loss: 1.4363269 Test Loss: 0.4312274
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.5736205577850342
Epoch: 38, Steps: 56 | Train Loss: 0.5535456 Vali Loss: 1.4347646 Test Loss: 0.4312883
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.3694231510162354
Epoch: 39, Steps: 56 | Train Loss: 0.5535373 Vali Loss: 1.4339106 Test Loss: 0.4312619
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.8080167770385742
Epoch: 40, Steps: 56 | Train Loss: 0.5533416 Vali Loss: 1.4320736 Test Loss: 0.4313482
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6775915622711182
Epoch: 41, Steps: 56 | Train Loss: 0.5530527 Vali Loss: 1.4336702 Test Loss: 0.4313660
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.8335673809051514
Epoch: 42, Steps: 56 | Train Loss: 0.5531318 Vali Loss: 1.4384958 Test Loss: 0.4314295
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0823583602905273
Epoch: 43, Steps: 56 | Train Loss: 0.5532826 Vali Loss: 1.4320567 Test Loss: 0.4314331
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.6551506519317627
Epoch: 44, Steps: 56 | Train Loss: 0.5525825 Vali Loss: 1.4360449 Test Loss: 0.4314452
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7609317302703857
Epoch: 45, Steps: 56 | Train Loss: 0.5528203 Vali Loss: 1.4312651 Test Loss: 0.4315063
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6135692596435547
Epoch: 46, Steps: 56 | Train Loss: 0.5524833 Vali Loss: 1.4346058 Test Loss: 0.4315107
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.6358325481414795
Epoch: 47, Steps: 56 | Train Loss: 0.5522826 Vali Loss: 1.4340758 Test Loss: 0.4315201
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.663482904434204
Epoch: 48, Steps: 56 | Train Loss: 0.5527154 Vali Loss: 1.4344172 Test Loss: 0.4315588
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.6976642608642578
Epoch: 49, Steps: 56 | Train Loss: 0.5521955 Vali Loss: 1.4342842 Test Loss: 0.4315681
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.7489912509918213
Epoch: 50, Steps: 56 | Train Loss: 0.5526826 Vali Loss: 1.4334433 Test Loss: 0.4316411
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.8551321029663086
Epoch: 51, Steps: 56 | Train Loss: 0.5520768 Vali Loss: 1.4312781 Test Loss: 0.4316368
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.8636939525604248
Epoch: 52, Steps: 56 | Train Loss: 0.5524017 Vali Loss: 1.4361057 Test Loss: 0.4316432
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6980812549591064
Epoch: 53, Steps: 56 | Train Loss: 0.5522816 Vali Loss: 1.4359514 Test Loss: 0.4317025
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.6957433223724365
Epoch: 54, Steps: 56 | Train Loss: 0.5520542 Vali Loss: 1.4318923 Test Loss: 0.4317182
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.68925142288208
Epoch: 55, Steps: 56 | Train Loss: 0.5522998 Vali Loss: 1.4370904 Test Loss: 0.4317208
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.754512071609497
Epoch: 56, Steps: 56 | Train Loss: 0.5516627 Vali Loss: 1.4350970 Test Loss: 0.4317404
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4302580654621124, mae:0.45409634709358215, rse:0.6279376149177551, corr:[0.21811663 0.23006487 0.22879098 0.23339467 0.2338473  0.23027164
 0.23032232 0.23272339 0.23257868 0.23148233 0.23130523 0.23160018
 0.2315058  0.23075624 0.22991565 0.22963941 0.22988696 0.22961777
 0.22899185 0.22914056 0.22990243 0.22994283 0.22949398 0.22991326
 0.23088744 0.23104922 0.23074667 0.23116718 0.23182726 0.2315696
 0.23079458 0.23045833 0.23056726 0.2304682  0.22995792 0.22937894
 0.22931531 0.22955821 0.22948629 0.22896668 0.22878906 0.22932158
 0.22980632 0.22965012 0.22955611 0.23027498 0.23106734 0.23108724
 0.23088388 0.23067679 0.23012249 0.22913493 0.22820894 0.2275295
 0.22694698 0.22609669 0.22562425 0.2254284  0.22530964 0.22518405
 0.2248742  0.22450885 0.22444575 0.22465941 0.22465129 0.22446853
 0.22463952 0.22507656 0.22521731 0.22499514 0.22484358 0.2249
 0.22443569 0.2236679  0.22336826 0.22343175 0.22309168 0.22238918
 0.22206861 0.22211811 0.22197527 0.22152169 0.22106607 0.22079736
 0.22065273 0.22039455 0.21985306 0.21937424 0.2193549  0.21961552
 0.21954988 0.21940957 0.21967377 0.22025152 0.22046779 0.2210036
 0.2222051  0.22320727 0.22360791 0.22391431 0.22429638 0.22441654
 0.22411834 0.223804   0.22380066 0.22375244 0.22337386 0.2229002
 0.2226117  0.22256288 0.22268179 0.2227976  0.22277848 0.22268254
 0.2228569  0.22310825 0.22309634 0.2229129  0.22295915 0.22318807
 0.22309934 0.22253346 0.22204898 0.2218005  0.2211873  0.22052643
 0.22043863 0.22062801 0.22040325 0.21996148 0.21976934 0.21968095
 0.21952336 0.2192445  0.21900922 0.2188778  0.2188567  0.21889913
 0.21888183 0.2188061  0.21895845 0.219114   0.21868488 0.21833538
 0.21838906 0.21838892 0.21798284 0.21736342 0.21702579 0.21691349
 0.21661386 0.21641925 0.21668138 0.21689108 0.21649824 0.2159729
 0.21586597 0.21596742 0.21585791 0.21570124 0.21567382 0.21584214
 0.21608368 0.21629106 0.21633618 0.21636465 0.21610098 0.21606657
 0.2162206  0.21661712 0.2172166  0.21771356 0.2175165  0.21708997
 0.21711247 0.21730693 0.21695875 0.21643524 0.21633668 0.21642394
 0.21623492 0.21602678 0.21619384 0.2164419  0.21656922 0.2167421
 0.21704061 0.21727324 0.21745192 0.21747637 0.21739618 0.21728604
 0.21707135 0.21664426 0.21599424 0.21538073 0.21501234 0.21480581
 0.21458967 0.214555   0.2146742  0.21453525 0.21415162 0.21398635
 0.21424767 0.2145923  0.21475284 0.21476617 0.21477199 0.21473087
 0.21467817 0.21456534 0.21439794 0.21418703 0.21392846 0.21378343
 0.21369259 0.21369623 0.21390194 0.21405761 0.21378787 0.21347994
 0.21363191 0.21373016 0.2132633  0.21278055 0.21266985 0.21250552
 0.21205133 0.21179686 0.21188068 0.21179534 0.21149862 0.21135835
 0.21134976 0.2112517  0.21116254 0.21117003 0.21114254 0.2112214
 0.21162187 0.21190172 0.2118318  0.21171756 0.21182552 0.21186964
 0.21168903 0.21154019 0.2115649  0.21130398 0.21078043 0.21054761
 0.2107097  0.21073233 0.21063676 0.21062437 0.21068172 0.21074459
 0.21090256 0.21093616 0.21079122 0.21068689 0.2107924  0.210911
 0.21064599 0.21030852 0.21034282 0.21033399 0.20981734 0.20947187
 0.20967157 0.20961806 0.20910443 0.20901132 0.2093865  0.2095142
 0.20930283 0.20939565 0.20958099 0.20948927 0.20928055 0.20931059
 0.20927781 0.209332   0.20951207 0.20953201 0.20919122 0.20926984
 0.20990139 0.21041459 0.21052578 0.2107782  0.21131924 0.21138361
 0.21092302 0.2108963  0.21132986 0.21134938 0.21095501 0.21098378
 0.21130905 0.21125507 0.2110871  0.21125719 0.21150802 0.21158016
 0.21171495 0.21175319 0.21156724 0.21157074 0.21197091 0.21242198
 0.21237414 0.21205504 0.2120561  0.21201995 0.21143383 0.2109544
 0.21083659 0.2103595  0.20961055 0.20952061 0.20983407 0.2096402
 0.20914398 0.20919397 0.20954818 0.20965835 0.20960705 0.2097702
 0.20984265 0.20972268 0.20981176 0.21000467 0.20981902 0.20952548
 0.20938489 0.20910113 0.20881055 0.20881265 0.20871861 0.20811793
 0.20747344 0.20728832 0.2071829  0.2068739  0.20688625 0.20740655
 0.20760204 0.20727243 0.2071134  0.20748325 0.20772426 0.2078087
 0.20790309 0.20794457 0.20785782 0.20795806 0.20786458 0.2075015
 0.20713109 0.20700091 0.20693742 0.20646362 0.20577891 0.20550245
 0.20527773 0.20458968 0.20409152 0.20430695 0.20429707 0.2036744
 0.2032639  0.20336056 0.20312011 0.2024593  0.20231214 0.20256056
 0.20249765 0.20226873 0.20241474 0.20276383 0.20287067 0.20356397
 0.20466979 0.20510654 0.2050038  0.20528547 0.20542553 0.20456693
 0.20372882 0.20354879 0.20304915 0.2018703  0.20119739 0.20161618
 0.20177019 0.2013685  0.20152359 0.20198376 0.20185255 0.20163022
 0.20203508 0.20241179 0.20233613 0.20254865 0.20309435 0.2033253
 0.20331897 0.20380694 0.20452167 0.20438664 0.2038417  0.20387483
 0.20411682 0.2035907  0.20313863 0.20347026 0.20361346 0.20313372
 0.20294714 0.20321834 0.2028765  0.20228918 0.20232515 0.202571
 0.20241624 0.20245302 0.20265344 0.20278814 0.20272909 0.20336644
 0.20436624 0.20443256 0.20395285 0.20403625 0.20411363 0.20359582
 0.20342663 0.20378192 0.20335102 0.20247915 0.20251973 0.20314334
 0.20287031 0.20218444 0.20235385 0.20249711 0.20179676 0.2014445
 0.20210215 0.2023546  0.20184487 0.20188424 0.20263177 0.20312105
 0.20332517 0.20411304 0.20481746 0.20420656 0.20317006 0.20292695
 0.20269975 0.20176941 0.201445   0.2022131  0.20231792 0.20164302
 0.20169544 0.20248647 0.20265995 0.20234837 0.20244119 0.20268108
 0.20248596 0.20245044 0.20261282 0.20200276 0.20135511 0.20174403
 0.20255663 0.20243472 0.20182742 0.20174779 0.20158222 0.20103326
 0.2010808  0.2015907  0.2010854  0.20021932 0.20044875 0.20105253
 0.20045151 0.19949384 0.19982742 0.20049214 0.20018458 0.19961289
 0.19950916 0.19935805 0.1992988  0.200012   0.20082095 0.20111316
 0.20156844 0.20269555 0.20337339 0.20311351 0.20297058 0.20303039
 0.20240255 0.20133518 0.20103954 0.20124051 0.20086291 0.20050985
 0.20101115 0.20156033 0.20159976 0.20162171 0.20215793 0.20248199
 0.20246355 0.20258051 0.20298034 0.20301515 0.20383379 0.20463094
 0.20456867 0.20370676 0.2037024  0.2044424  0.20391174 0.20280538
 0.20279266 0.20329157 0.20270105 0.20209089 0.20231077 0.20224477
 0.2013839  0.2013463  0.20260318 0.20312756 0.20281686 0.20297298
 0.20332257 0.20278108 0.20221217 0.20267917 0.20288524 0.20188008
 0.20093095 0.20086849 0.20046356 0.1995168  0.19917886 0.19916898
 0.19845699 0.1977933  0.19791476 0.19774698 0.19666094 0.19611353
 0.19644529 0.19622675 0.195642   0.19536607 0.19547635 0.19512253
 0.19503254 0.19523454 0.19514585 0.19485433 0.19534563 0.19567735
 0.19465283 0.1937269  0.19434406 0.19526444 0.19451776 0.19337912
 0.19354638 0.19319601 0.1920228  0.19195236 0.19294599 0.1924591
 0.19128948 0.19154856 0.19260015 0.1922154  0.1915603  0.19229242
 0.19311734 0.19277258 0.19247997 0.19282137 0.1925049  0.1916152
 0.19144559 0.19117822 0.19002353 0.18927585 0.18932359 0.18845104
 0.18701646 0.18692923 0.18737534 0.18654035 0.18535914 0.18562168
 0.18596557 0.18509637 0.18484981 0.18573284 0.18572736 0.18475416
 0.1849779  0.18576916 0.18546578 0.185414   0.1867746  0.18705261
 0.18528333 0.18461758 0.185684   0.18527314 0.18316959 0.18279378
 0.1839093  0.18309294 0.18147694 0.18209623 0.18281421 0.18150865
 0.18067151 0.1818045  0.18226913 0.18096662 0.18067014 0.18166478
 0.18126345 0.18027598 0.18095846 0.18188247 0.18131678 0.18115468
 0.18202247 0.18141566 0.17948146 0.1797111  0.18084076 0.1795812
 0.17751166 0.17749418 0.1777818  0.17624813 0.17540905 0.17642812
 0.17631473 0.17465039 0.17460492 0.17562845 0.17523415 0.17470582
 0.17556015 0.17560308 0.1740913  0.17428508 0.17557949 0.17432012
 0.17151554 0.17104898 0.17086536 0.1680545  0.16611394 0.16662142
 0.16623013 0.16351955 0.163144   0.16484363 0.16454978 0.16323556
 0.16388077 0.16431764 0.16302118 0.16278297 0.16426113 0.16388483
 0.16242075 0.16365957 0.16527876 0.16339056 0.1620145  0.16361539
 0.16300112 0.1597334  0.15961179 0.1622106  0.16016094 0.15639928
 0.15795279 0.15951996 0.15599188 0.1542484  0.15707377 0.15620092
 0.15265901 0.153416   0.1551999  0.15175104 0.15128848 0.15516171
 0.15153746 0.14806831 0.15606576 0.15124106 0.13911325 0.18084376]
