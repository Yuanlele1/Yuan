Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=34, out_features=160, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4874240.0
params:  5600.0
Trainable parameters:  5600
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.282397747039795
Epoch: 1, Steps: 64 | Train Loss: 0.9629180 Vali Loss: 1.8572842 Test Loss: 0.9136022
Validation loss decreased (inf --> 1.857284).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3235135078430176
Epoch: 2, Steps: 64 | Train Loss: 0.7119673 Vali Loss: 1.5725318 Test Loss: 0.6836179
Validation loss decreased (1.857284 --> 1.572532).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.136953592300415
Epoch: 3, Steps: 64 | Train Loss: 0.6078070 Vali Loss: 1.4672178 Test Loss: 0.5876697
Validation loss decreased (1.572532 --> 1.467218).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.2300987243652344
Epoch: 4, Steps: 64 | Train Loss: 0.5583945 Vali Loss: 1.3993903 Test Loss: 0.5410798
Validation loss decreased (1.467218 --> 1.399390).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.059816598892212
Epoch: 5, Steps: 64 | Train Loss: 0.5326099 Vali Loss: 1.3680435 Test Loss: 0.5163857
Validation loss decreased (1.399390 --> 1.368044).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.201404333114624
Epoch: 6, Steps: 64 | Train Loss: 0.5177879 Vali Loss: 1.3493026 Test Loss: 0.5025757
Validation loss decreased (1.368044 --> 1.349303).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.1936936378479004
Epoch: 7, Steps: 64 | Train Loss: 0.5086878 Vali Loss: 1.3388869 Test Loss: 0.4944108
Validation loss decreased (1.349303 --> 1.338887).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.1725265979766846
Epoch: 8, Steps: 64 | Train Loss: 0.5030473 Vali Loss: 1.3246698 Test Loss: 0.4893490
Validation loss decreased (1.338887 --> 1.324670).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.1206707954406738
Epoch: 9, Steps: 64 | Train Loss: 0.4992961 Vali Loss: 1.3228304 Test Loss: 0.4860700
Validation loss decreased (1.324670 --> 1.322830).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 0.9889695644378662
Epoch: 10, Steps: 64 | Train Loss: 0.4960151 Vali Loss: 1.3217162 Test Loss: 0.4839520
Validation loss decreased (1.322830 --> 1.321716).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.3835785388946533
Epoch: 11, Steps: 64 | Train Loss: 0.4944731 Vali Loss: 1.3059019 Test Loss: 0.4824356
Validation loss decreased (1.321716 --> 1.305902).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.355983018875122
Epoch: 12, Steps: 64 | Train Loss: 0.4925935 Vali Loss: 1.3075420 Test Loss: 0.4814200
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.4915974140167236
Epoch: 13, Steps: 64 | Train Loss: 0.4913255 Vali Loss: 1.3089585 Test Loss: 0.4806418
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.275122880935669
Epoch: 14, Steps: 64 | Train Loss: 0.4905471 Vali Loss: 1.3046610 Test Loss: 0.4801084
Validation loss decreased (1.305902 --> 1.304661).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5180854797363281
Epoch: 15, Steps: 64 | Train Loss: 0.4896008 Vali Loss: 1.3072510 Test Loss: 0.4796302
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4011657238006592
Epoch: 16, Steps: 64 | Train Loss: 0.4891743 Vali Loss: 1.3097115 Test Loss: 0.4793154
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.460278034210205
Epoch: 17, Steps: 64 | Train Loss: 0.4887287 Vali Loss: 1.3058574 Test Loss: 0.4790625
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.759601354598999
Epoch: 18, Steps: 64 | Train Loss: 0.4879941 Vali Loss: 1.3034713 Test Loss: 0.4788621
Validation loss decreased (1.304661 --> 1.303471).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.533508539199829
Epoch: 19, Steps: 64 | Train Loss: 0.4875280 Vali Loss: 1.3033279 Test Loss: 0.4787205
Validation loss decreased (1.303471 --> 1.303328).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3772578239440918
Epoch: 20, Steps: 64 | Train Loss: 0.4871384 Vali Loss: 1.2995151 Test Loss: 0.4786103
Validation loss decreased (1.303328 --> 1.299515).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4049017429351807
Epoch: 21, Steps: 64 | Train Loss: 0.4872614 Vali Loss: 1.2965647 Test Loss: 0.4785706
Validation loss decreased (1.299515 --> 1.296565).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.4023780822753906
Epoch: 22, Steps: 64 | Train Loss: 0.4867949 Vali Loss: 1.3039058 Test Loss: 0.4784819
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9085092544555664
Epoch: 23, Steps: 64 | Train Loss: 0.4863995 Vali Loss: 1.2981561 Test Loss: 0.4783967
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.91986083984375
Epoch: 24, Steps: 64 | Train Loss: 0.4862264 Vali Loss: 1.2995204 Test Loss: 0.4784363
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.230888605117798
Epoch: 25, Steps: 64 | Train Loss: 0.4861958 Vali Loss: 1.2962028 Test Loss: 0.4783524
Validation loss decreased (1.296565 --> 1.296203).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.3768644332885742
Epoch: 26, Steps: 64 | Train Loss: 0.4860673 Vali Loss: 1.3020195 Test Loss: 0.4783347
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.3001430034637451
Epoch: 27, Steps: 64 | Train Loss: 0.4858793 Vali Loss: 1.2974694 Test Loss: 0.4783045
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.2826707363128662
Epoch: 28, Steps: 64 | Train Loss: 0.4857691 Vali Loss: 1.2992481 Test Loss: 0.4783066
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.3557913303375244
Epoch: 29, Steps: 64 | Train Loss: 0.4853656 Vali Loss: 1.3017696 Test Loss: 0.4783093
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.2977747917175293
Epoch: 30, Steps: 64 | Train Loss: 0.4854756 Vali Loss: 1.3014053 Test Loss: 0.4782786
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.3522014617919922
Epoch: 31, Steps: 64 | Train Loss: 0.4853908 Vali Loss: 1.2930989 Test Loss: 0.4782936
Validation loss decreased (1.296203 --> 1.293099).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.2516345977783203
Epoch: 32, Steps: 64 | Train Loss: 0.4853640 Vali Loss: 1.2978547 Test Loss: 0.4783304
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.7150604724884033
Epoch: 33, Steps: 64 | Train Loss: 0.4854892 Vali Loss: 1.2880815 Test Loss: 0.4782813
Validation loss decreased (1.293099 --> 1.288082).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.629694938659668
Epoch: 34, Steps: 64 | Train Loss: 0.4851628 Vali Loss: 1.2975035 Test Loss: 0.4783254
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.3689815998077393
Epoch: 35, Steps: 64 | Train Loss: 0.4851426 Vali Loss: 1.2956139 Test Loss: 0.4783457
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.460449457168579
Epoch: 36, Steps: 64 | Train Loss: 0.4851384 Vali Loss: 1.2937350 Test Loss: 0.4783397
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6302504539489746
Epoch: 37, Steps: 64 | Train Loss: 0.4851916 Vali Loss: 1.2895874 Test Loss: 0.4783500
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4434235095977783
Epoch: 38, Steps: 64 | Train Loss: 0.4848958 Vali Loss: 1.2980291 Test Loss: 0.4783427
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3637797832489014
Epoch: 39, Steps: 64 | Train Loss: 0.4849488 Vali Loss: 1.2963035 Test Loss: 0.4783508
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.326033592224121
Epoch: 40, Steps: 64 | Train Loss: 0.4848988 Vali Loss: 1.2959248 Test Loss: 0.4783866
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.3487417697906494
Epoch: 41, Steps: 64 | Train Loss: 0.4849047 Vali Loss: 1.2932247 Test Loss: 0.4783942
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.4759714603424072
Epoch: 42, Steps: 64 | Train Loss: 0.4847009 Vali Loss: 1.2977744 Test Loss: 0.4783950
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.3675463199615479
Epoch: 43, Steps: 64 | Train Loss: 0.4845913 Vali Loss: 1.2959789 Test Loss: 0.4783991
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.3865771293640137
Epoch: 44, Steps: 64 | Train Loss: 0.4846860 Vali Loss: 1.2946414 Test Loss: 0.4784182
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.1207213401794434
Epoch: 45, Steps: 64 | Train Loss: 0.4846778 Vali Loss: 1.2893486 Test Loss: 0.4784140
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.1658122539520264
Epoch: 46, Steps: 64 | Train Loss: 0.4847447 Vali Loss: 1.2899001 Test Loss: 0.4784355
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.1715164184570312
Epoch: 47, Steps: 64 | Train Loss: 0.4845513 Vali Loss: 1.2908648 Test Loss: 0.4784598
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.2300746440887451
Epoch: 48, Steps: 64 | Train Loss: 0.4845918 Vali Loss: 1.2936088 Test Loss: 0.4784649
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.0612080097198486
Epoch: 49, Steps: 64 | Train Loss: 0.4844614 Vali Loss: 1.2967200 Test Loss: 0.4784576
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.0426204204559326
Epoch: 50, Steps: 64 | Train Loss: 0.4847342 Vali Loss: 1.2910949 Test Loss: 0.4784707
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.0317778587341309
Epoch: 51, Steps: 64 | Train Loss: 0.4847643 Vali Loss: 1.3010378 Test Loss: 0.4784837
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 0.9792232513427734
Epoch: 52, Steps: 64 | Train Loss: 0.4842897 Vali Loss: 1.2974194 Test Loss: 0.4784910
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.0742194652557373
Epoch: 53, Steps: 64 | Train Loss: 0.4846424 Vali Loss: 1.2969348 Test Loss: 0.4785031
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.47776949405670166, mae:0.44378015398979187, rse:0.6580535769462585, corr:[0.25516036 0.25481048 0.25407794 0.2528276  0.25018764 0.24853197
 0.24694078 0.24722354 0.24782334 0.246666   0.24635196 0.24681161
 0.24547744 0.24531269 0.24582876 0.24472095 0.24514131 0.2458666
 0.2454439  0.2455377  0.24529052 0.24490932 0.24465808 0.24413249
 0.24251524 0.24129838 0.2411963  0.2415401  0.24145181 0.2414801
 0.2418655  0.24217533 0.24209753 0.24164297 0.2417152  0.241811
 0.24140573 0.24154016 0.24192446 0.24160552 0.24211244 0.24284387
 0.24288486 0.2431545  0.24308458 0.24292329 0.24294087 0.2422705
 0.24095014 0.23957582 0.23861016 0.23832719 0.23749079 0.23644258
 0.2364691  0.23636657 0.23620798 0.23621458 0.23599857 0.23598279
 0.23584026 0.23589996 0.23605566 0.23571406 0.23587734 0.236227
 0.2363383  0.23651549 0.23631601 0.2359914  0.23562092 0.23458095
 0.2328011  0.23165911 0.23101094 0.2310069  0.23061398 0.23036774
 0.2308014  0.23093826 0.2308054  0.23042358 0.2302658  0.2299905
 0.2297057  0.22982797 0.23006426 0.22989364 0.23002048 0.23017806
 0.23011047 0.23039262 0.23022722 0.23021755 0.2304013  0.22986126
 0.22850412 0.22781709 0.2276767  0.22752857 0.22758508 0.22778255
 0.22814752 0.22816862 0.22820078 0.228122   0.22782177 0.2276286
 0.22751646 0.22734274 0.2274905  0.22737736 0.22763345 0.22787306
 0.2279251  0.22807394 0.22801907 0.22798459 0.2277068  0.22645995
 0.22445327 0.22302423 0.22201017 0.22118574 0.22084202 0.22099501
 0.22187929 0.22221729 0.22234106 0.22232373 0.2220387  0.22188222
 0.22186242 0.22163518 0.2216344  0.22158405 0.22180605 0.22200361
 0.22211137 0.22228467 0.2221144  0.22220525 0.22215396 0.22114715
 0.21927747 0.21792367 0.21738733 0.21684803 0.21612507 0.21611528
 0.2171172  0.21744205 0.21758243 0.21749784 0.21736269 0.21729586
 0.21698818 0.21683776 0.2167774  0.21646905 0.21664655 0.21677807
 0.2167688  0.21703884 0.21682085 0.21672198 0.21670938 0.2155768
 0.21367496 0.21264268 0.21240452 0.21250004 0.21198568 0.21178181
 0.21255183 0.21287125 0.21277598 0.2127119  0.21284963 0.21286371
 0.21272284 0.21257776 0.21264112 0.21235177 0.2124008  0.21289472
 0.21322736 0.21353683 0.2135711  0.21361738 0.21362273 0.21260786
 0.21068014 0.20968683 0.20923342 0.20895974 0.20854504 0.20876247
 0.20966059 0.21007675 0.21015376 0.20978807 0.20953506 0.20939606
 0.20920144 0.20919006 0.20919389 0.2088885  0.20900294 0.20908567
 0.20903146 0.20929548 0.2091171  0.20901448 0.20909053 0.20810045
 0.20634164 0.20568202 0.20565028 0.20598923 0.20665187 0.2076708
 0.20921512 0.21025756 0.21047702 0.21017961 0.21015032 0.20999953
 0.20996776 0.21012832 0.21005    0.20989615 0.20995119 0.20984843
 0.2098683  0.21011356 0.21014279 0.21037792 0.21052913 0.20942485
 0.20746124 0.2061267  0.20566747 0.20576628 0.20566384 0.20630443
 0.20786601 0.20864654 0.20898548 0.20906912 0.20884643 0.20863922
 0.20847327 0.2081192  0.2079497  0.2078735  0.20790619 0.20801142
 0.20824583 0.20837525 0.20815049 0.20797852 0.20785709 0.20692012
 0.20521452 0.2043836  0.20430663 0.20461185 0.20494305 0.20599985
 0.20755994 0.20855294 0.20871152 0.20846818 0.20862961 0.2083505
 0.20798875 0.20808715 0.20763966 0.20733751 0.20774245 0.20775455
 0.20772897 0.20811218 0.208022   0.2080713  0.2084023  0.20778911
 0.20637049 0.20571078 0.20651779 0.20729893 0.20741403 0.20833403
 0.20959976 0.21009342 0.21027265 0.20989698 0.20977075 0.20952241
 0.20896558 0.20871247 0.208512   0.20802487 0.20823672 0.20831019
 0.20820443 0.20846695 0.20842648 0.2083641  0.20846759 0.207285
 0.2054973  0.20460479 0.20446986 0.20449015 0.20463596 0.2050513
 0.20628633 0.20672746 0.20697317 0.20665516 0.20595057 0.20548569
 0.20528828 0.20470607 0.20437819 0.20438886 0.20389666 0.20429972
 0.2048338  0.20379598 0.20417354 0.20442596 0.20355234 0.20582144]
