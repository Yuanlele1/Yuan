Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=103, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=103, out_features=206, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19011328.0
params:  21424.0
Trainable parameters:  21424
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.418605089187622
Epoch: 1, Steps: 56 | Train Loss: 0.8508744 Vali Loss: 2.1993983 Test Loss: 0.9408605
Validation loss decreased (inf --> 2.199398).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.605671167373657
Epoch: 2, Steps: 56 | Train Loss: 0.6830266 Vali Loss: 1.9754760 Test Loss: 0.8148614
Validation loss decreased (2.199398 --> 1.975476).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.324074029922485
Epoch: 3, Steps: 56 | Train Loss: 0.5977351 Vali Loss: 1.8717449 Test Loss: 0.7534890
Validation loss decreased (1.975476 --> 1.871745).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.188636779785156
Epoch: 4, Steps: 56 | Train Loss: 0.5506551 Vali Loss: 1.8159916 Test Loss: 0.7201521
Validation loss decreased (1.871745 --> 1.815992).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.920971155166626
Epoch: 5, Steps: 56 | Train Loss: 0.5210055 Vali Loss: 1.7768376 Test Loss: 0.6998102
Validation loss decreased (1.815992 --> 1.776838).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.070051193237305
Epoch: 6, Steps: 56 | Train Loss: 0.4995090 Vali Loss: 1.7526655 Test Loss: 0.6832429
Validation loss decreased (1.776838 --> 1.752666).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.3768885135650635
Epoch: 7, Steps: 56 | Train Loss: 0.4833214 Vali Loss: 1.7372965 Test Loss: 0.6709632
Validation loss decreased (1.752666 --> 1.737296).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.130061626434326
Epoch: 8, Steps: 56 | Train Loss: 0.4693549 Vali Loss: 1.7146559 Test Loss: 0.6599599
Validation loss decreased (1.737296 --> 1.714656).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.229621171951294
Epoch: 9, Steps: 56 | Train Loss: 0.4574699 Vali Loss: 1.7053436 Test Loss: 0.6494631
Validation loss decreased (1.714656 --> 1.705344).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.230985403060913
Epoch: 10, Steps: 56 | Train Loss: 0.4472961 Vali Loss: 1.6877420 Test Loss: 0.6403159
Validation loss decreased (1.705344 --> 1.687742).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.74443507194519
Epoch: 11, Steps: 56 | Train Loss: 0.4380713 Vali Loss: 1.6832023 Test Loss: 0.6315709
Validation loss decreased (1.687742 --> 1.683202).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.51159143447876
Epoch: 12, Steps: 56 | Train Loss: 0.4303455 Vali Loss: 1.6691861 Test Loss: 0.6233960
Validation loss decreased (1.683202 --> 1.669186).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.848158359527588
Epoch: 13, Steps: 56 | Train Loss: 0.4230713 Vali Loss: 1.6640251 Test Loss: 0.6159698
Validation loss decreased (1.669186 --> 1.664025).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.682170391082764
Epoch: 14, Steps: 56 | Train Loss: 0.4168306 Vali Loss: 1.6523666 Test Loss: 0.6094501
Validation loss decreased (1.664025 --> 1.652367).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.367885112762451
Epoch: 15, Steps: 56 | Train Loss: 0.4109782 Vali Loss: 1.6457405 Test Loss: 0.6031446
Validation loss decreased (1.652367 --> 1.645741).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.583508014678955
Epoch: 16, Steps: 56 | Train Loss: 0.4056755 Vali Loss: 1.6344228 Test Loss: 0.5969155
Validation loss decreased (1.645741 --> 1.634423).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.694837331771851
Epoch: 17, Steps: 56 | Train Loss: 0.4009879 Vali Loss: 1.6292808 Test Loss: 0.5918522
Validation loss decreased (1.634423 --> 1.629281).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.326987266540527
Epoch: 18, Steps: 56 | Train Loss: 0.3968767 Vali Loss: 1.6174158 Test Loss: 0.5867167
Validation loss decreased (1.629281 --> 1.617416).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.757394313812256
Epoch: 19, Steps: 56 | Train Loss: 0.3927208 Vali Loss: 1.6195142 Test Loss: 0.5822280
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.181086778640747
Epoch: 20, Steps: 56 | Train Loss: 0.3891424 Vali Loss: 1.6102488 Test Loss: 0.5777306
Validation loss decreased (1.617416 --> 1.610249).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.677551507949829
Epoch: 21, Steps: 56 | Train Loss: 0.3860495 Vali Loss: 1.6030777 Test Loss: 0.5737503
Validation loss decreased (1.610249 --> 1.603078).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.668225526809692
Epoch: 22, Steps: 56 | Train Loss: 0.3830774 Vali Loss: 1.6033682 Test Loss: 0.5699379
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.448753356933594
Epoch: 23, Steps: 56 | Train Loss: 0.3803585 Vali Loss: 1.5960169 Test Loss: 0.5663947
Validation loss decreased (1.603078 --> 1.596017).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.911132097244263
Epoch: 24, Steps: 56 | Train Loss: 0.3778694 Vali Loss: 1.5909770 Test Loss: 0.5630628
Validation loss decreased (1.596017 --> 1.590977).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.483138084411621
Epoch: 25, Steps: 56 | Train Loss: 0.3752913 Vali Loss: 1.5957534 Test Loss: 0.5597401
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.4327287673950195
Epoch: 26, Steps: 56 | Train Loss: 0.3731775 Vali Loss: 1.5903403 Test Loss: 0.5568846
Validation loss decreased (1.590977 --> 1.590340).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.739060401916504
Epoch: 27, Steps: 56 | Train Loss: 0.3713425 Vali Loss: 1.5796342 Test Loss: 0.5542954
Validation loss decreased (1.590340 --> 1.579634).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.209751129150391
Epoch: 28, Steps: 56 | Train Loss: 0.3692999 Vali Loss: 1.5769334 Test Loss: 0.5516344
Validation loss decreased (1.579634 --> 1.576933).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.952555894851685
Epoch: 29, Steps: 56 | Train Loss: 0.3674835 Vali Loss: 1.5765984 Test Loss: 0.5492749
Validation loss decreased (1.576933 --> 1.576598).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.470672369003296
Epoch: 30, Steps: 56 | Train Loss: 0.3657751 Vali Loss: 1.5685196 Test Loss: 0.5468249
Validation loss decreased (1.576598 --> 1.568520).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.2856903076171875
Epoch: 31, Steps: 56 | Train Loss: 0.3641542 Vali Loss: 1.5702041 Test Loss: 0.5448563
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.141464710235596
Epoch: 32, Steps: 56 | Train Loss: 0.3628435 Vali Loss: 1.5667613 Test Loss: 0.5426233
Validation loss decreased (1.568520 --> 1.566761).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.3779802322387695
Epoch: 33, Steps: 56 | Train Loss: 0.3616480 Vali Loss: 1.5599632 Test Loss: 0.5407950
Validation loss decreased (1.566761 --> 1.559963).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.0252296924591064
Epoch: 34, Steps: 56 | Train Loss: 0.3603365 Vali Loss: 1.5618345 Test Loss: 0.5391422
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.054386615753174
Epoch: 35, Steps: 56 | Train Loss: 0.3592149 Vali Loss: 1.5541106 Test Loss: 0.5373111
Validation loss decreased (1.559963 --> 1.554111).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.404799938201904
Epoch: 36, Steps: 56 | Train Loss: 0.3582626 Vali Loss: 1.5544679 Test Loss: 0.5358133
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.182843923568726
Epoch: 37, Steps: 56 | Train Loss: 0.3571288 Vali Loss: 1.5525935 Test Loss: 0.5342773
Validation loss decreased (1.554111 --> 1.552593).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.467655181884766
Epoch: 38, Steps: 56 | Train Loss: 0.3561414 Vali Loss: 1.5614078 Test Loss: 0.5328767
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.447866678237915
Epoch: 39, Steps: 56 | Train Loss: 0.3551574 Vali Loss: 1.5551126 Test Loss: 0.5314902
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.190955877304077
Epoch: 40, Steps: 56 | Train Loss: 0.3544515 Vali Loss: 1.5464852 Test Loss: 0.5303028
Validation loss decreased (1.552593 --> 1.546485).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.878737688064575
Epoch: 41, Steps: 56 | Train Loss: 0.3535162 Vali Loss: 1.5465970 Test Loss: 0.5290002
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.967604875564575
Epoch: 42, Steps: 56 | Train Loss: 0.3527717 Vali Loss: 1.5449901 Test Loss: 0.5279313
Validation loss decreased (1.546485 --> 1.544990).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.096339702606201
Epoch: 43, Steps: 56 | Train Loss: 0.3521960 Vali Loss: 1.5511178 Test Loss: 0.5268707
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.191803932189941
Epoch: 44, Steps: 56 | Train Loss: 0.3514414 Vali Loss: 1.5437319 Test Loss: 0.5257962
Validation loss decreased (1.544990 --> 1.543732).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.49447774887085
Epoch: 45, Steps: 56 | Train Loss: 0.3508971 Vali Loss: 1.5476217 Test Loss: 0.5248660
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.578884840011597
Epoch: 46, Steps: 56 | Train Loss: 0.3502466 Vali Loss: 1.5475112 Test Loss: 0.5238752
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.388288497924805
Epoch: 47, Steps: 56 | Train Loss: 0.3496974 Vali Loss: 1.5416920 Test Loss: 0.5230981
Validation loss decreased (1.543732 --> 1.541692).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.199303150177002
Epoch: 48, Steps: 56 | Train Loss: 0.3492644 Vali Loss: 1.5385206 Test Loss: 0.5222332
Validation loss decreased (1.541692 --> 1.538521).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 4.549408197402954
Epoch: 49, Steps: 56 | Train Loss: 0.3486023 Vali Loss: 1.5416912 Test Loss: 0.5214641
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.5228681564331055
Epoch: 50, Steps: 56 | Train Loss: 0.3482352 Vali Loss: 1.5382837 Test Loss: 0.5207971
Validation loss decreased (1.538521 --> 1.538284).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.213598012924194
Epoch: 51, Steps: 56 | Train Loss: 0.3478749 Vali Loss: 1.5356327 Test Loss: 0.5200472
Validation loss decreased (1.538284 --> 1.535633).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.467465877532959
Epoch: 52, Steps: 56 | Train Loss: 0.3473133 Vali Loss: 1.5355480 Test Loss: 0.5193684
Validation loss decreased (1.535633 --> 1.535548).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.384713649749756
Epoch: 53, Steps: 56 | Train Loss: 0.3468837 Vali Loss: 1.5404334 Test Loss: 0.5187931
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 4.45796275138855
Epoch: 54, Steps: 56 | Train Loss: 0.3463618 Vali Loss: 1.5365616 Test Loss: 0.5181714
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.412689208984375
Epoch: 55, Steps: 56 | Train Loss: 0.3460674 Vali Loss: 1.5353632 Test Loss: 0.5176320
Validation loss decreased (1.535548 --> 1.535363).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 4.5784101486206055
Epoch: 56, Steps: 56 | Train Loss: 0.3457858 Vali Loss: 1.5313524 Test Loss: 0.5171387
Validation loss decreased (1.535363 --> 1.531352).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.618936777114868
Epoch: 57, Steps: 56 | Train Loss: 0.3455062 Vali Loss: 1.5332402 Test Loss: 0.5165918
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 4.472422361373901
Epoch: 58, Steps: 56 | Train Loss: 0.3450574 Vali Loss: 1.5378222 Test Loss: 0.5160891
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.146990060806274
Epoch: 59, Steps: 56 | Train Loss: 0.3447215 Vali Loss: 1.5299460 Test Loss: 0.5156156
Validation loss decreased (1.531352 --> 1.529946).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 4.595827341079712
Epoch: 60, Steps: 56 | Train Loss: 0.3446272 Vali Loss: 1.5299375 Test Loss: 0.5151688
Validation loss decreased (1.529946 --> 1.529938).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 4.370281457901001
Epoch: 61, Steps: 56 | Train Loss: 0.3442264 Vali Loss: 1.5309336 Test Loss: 0.5147778
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 4.951770782470703
Epoch: 62, Steps: 56 | Train Loss: 0.3442619 Vali Loss: 1.5276980 Test Loss: 0.5144077
Validation loss decreased (1.529938 --> 1.527698).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 4.13529896736145
Epoch: 63, Steps: 56 | Train Loss: 0.3438015 Vali Loss: 1.5318151 Test Loss: 0.5140091
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 4.83723258972168
Epoch: 64, Steps: 56 | Train Loss: 0.3436578 Vali Loss: 1.5283465 Test Loss: 0.5136724
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 4.5574705600738525
Epoch: 65, Steps: 56 | Train Loss: 0.3435199 Vali Loss: 1.5290999 Test Loss: 0.5133073
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 4.432365655899048
Epoch: 66, Steps: 56 | Train Loss: 0.3430427 Vali Loss: 1.5295254 Test Loss: 0.5129899
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 4.024021863937378
Epoch: 67, Steps: 56 | Train Loss: 0.3429588 Vali Loss: 1.5258722 Test Loss: 0.5126850
Validation loss decreased (1.527698 --> 1.525872).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 4.5265820026397705
Epoch: 68, Steps: 56 | Train Loss: 0.3428852 Vali Loss: 1.5276521 Test Loss: 0.5124164
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 4.258922338485718
Epoch: 69, Steps: 56 | Train Loss: 0.3424870 Vali Loss: 1.5269545 Test Loss: 0.5121242
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.620829105377197
Epoch: 70, Steps: 56 | Train Loss: 0.3424267 Vali Loss: 1.5269241 Test Loss: 0.5118627
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 4.216508626937866
Epoch: 71, Steps: 56 | Train Loss: 0.3422478 Vali Loss: 1.5234717 Test Loss: 0.5115997
Validation loss decreased (1.525872 --> 1.523472).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 4.041321754455566
Epoch: 72, Steps: 56 | Train Loss: 0.3421106 Vali Loss: 1.5263917 Test Loss: 0.5113950
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 4.449934244155884
Epoch: 73, Steps: 56 | Train Loss: 0.3419943 Vali Loss: 1.5271486 Test Loss: 0.5111623
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 4.46962571144104
Epoch: 74, Steps: 56 | Train Loss: 0.3418294 Vali Loss: 1.5199656 Test Loss: 0.5109458
Validation loss decreased (1.523472 --> 1.519966).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 4.3433473110198975
Epoch: 75, Steps: 56 | Train Loss: 0.3416663 Vali Loss: 1.5272579 Test Loss: 0.5107676
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 4.05964207649231
Epoch: 76, Steps: 56 | Train Loss: 0.3415491 Vali Loss: 1.5260899 Test Loss: 0.5105816
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 4.765438079833984
Epoch: 77, Steps: 56 | Train Loss: 0.3415095 Vali Loss: 1.5263761 Test Loss: 0.5103928
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 4.279353857040405
Epoch: 78, Steps: 56 | Train Loss: 0.3415098 Vali Loss: 1.5224522 Test Loss: 0.5102175
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 4.266560316085815
Epoch: 79, Steps: 56 | Train Loss: 0.3412093 Vali Loss: 1.5241690 Test Loss: 0.5100431
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 4.3773791790008545
Epoch: 80, Steps: 56 | Train Loss: 0.3411548 Vali Loss: 1.5292950 Test Loss: 0.5098933
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 4.213639736175537
Epoch: 81, Steps: 56 | Train Loss: 0.3410229 Vali Loss: 1.5276947 Test Loss: 0.5097466
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 4.651374578475952
Epoch: 82, Steps: 56 | Train Loss: 0.3411068 Vali Loss: 1.5275552 Test Loss: 0.5095986
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 4.214111328125
Epoch: 83, Steps: 56 | Train Loss: 0.3407941 Vali Loss: 1.5228454 Test Loss: 0.5094641
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 4.380497932434082
Epoch: 84, Steps: 56 | Train Loss: 0.3409347 Vali Loss: 1.5214331 Test Loss: 0.5093410
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 4.729655981063843
Epoch: 85, Steps: 56 | Train Loss: 0.3408035 Vali Loss: 1.5251691 Test Loss: 0.5092164
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 5.098928689956665
Epoch: 86, Steps: 56 | Train Loss: 0.3405968 Vali Loss: 1.5252593 Test Loss: 0.5090930
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 4.74168062210083
Epoch: 87, Steps: 56 | Train Loss: 0.3406547 Vali Loss: 1.5214002 Test Loss: 0.5089871
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 4.296896934509277
Epoch: 88, Steps: 56 | Train Loss: 0.3405382 Vali Loss: 1.5240166 Test Loss: 0.5088913
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 4.574096918106079
Epoch: 89, Steps: 56 | Train Loss: 0.3405165 Vali Loss: 1.5291862 Test Loss: 0.5087830
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 4.403225421905518
Epoch: 90, Steps: 56 | Train Loss: 0.3402218 Vali Loss: 1.5233335 Test Loss: 0.5086925
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 4.3159472942352295
Epoch: 91, Steps: 56 | Train Loss: 0.3402995 Vali Loss: 1.5241089 Test Loss: 0.5085931
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 4.604832172393799
Epoch: 92, Steps: 56 | Train Loss: 0.3404069 Vali Loss: 1.5223628 Test Loss: 0.5085136
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 4.682315111160278
Epoch: 93, Steps: 56 | Train Loss: 0.3403715 Vali Loss: 1.5204477 Test Loss: 0.5084214
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 4.573012113571167
Epoch: 94, Steps: 56 | Train Loss: 0.3402592 Vali Loss: 1.5225129 Test Loss: 0.5083547
EarlyStopping counter: 20 out of 20
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=103, out_features=206, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19011328.0
params:  21424.0
Trainable parameters:  21424
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.403193473815918
Epoch: 1, Steps: 56 | Train Loss: 0.5912949 Vali Loss: 1.4875680 Test Loss: 0.4794774
Validation loss decreased (inf --> 1.487568).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.447723388671875
Epoch: 2, Steps: 56 | Train Loss: 0.5769970 Vali Loss: 1.4689479 Test Loss: 0.4602430
Validation loss decreased (1.487568 --> 1.468948).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.529210805892944
Epoch: 3, Steps: 56 | Train Loss: 0.5686005 Vali Loss: 1.4538389 Test Loss: 0.4490134
Validation loss decreased (1.468948 --> 1.453839).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.413855314254761
Epoch: 4, Steps: 56 | Train Loss: 0.5634844 Vali Loss: 1.4465432 Test Loss: 0.4428305
Validation loss decreased (1.453839 --> 1.446543).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.936678409576416
Epoch: 5, Steps: 56 | Train Loss: 0.5601379 Vali Loss: 1.4447125 Test Loss: 0.4396828
Validation loss decreased (1.446543 --> 1.444713).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.718345403671265
Epoch: 6, Steps: 56 | Train Loss: 0.5584924 Vali Loss: 1.4374812 Test Loss: 0.4381751
Validation loss decreased (1.444713 --> 1.437481).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.155022621154785
Epoch: 7, Steps: 56 | Train Loss: 0.5572048 Vali Loss: 1.4380438 Test Loss: 0.4375701
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.57173490524292
Epoch: 8, Steps: 56 | Train Loss: 0.5561501 Vali Loss: 1.4417498 Test Loss: 0.4372680
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.186737775802612
Epoch: 9, Steps: 56 | Train Loss: 0.5556942 Vali Loss: 1.4387101 Test Loss: 0.4375244
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.950475215911865
Epoch: 10, Steps: 56 | Train Loss: 0.5552809 Vali Loss: 1.4372047 Test Loss: 0.4377163
Validation loss decreased (1.437481 --> 1.437205).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.16128134727478
Epoch: 11, Steps: 56 | Train Loss: 0.5554230 Vali Loss: 1.4373403 Test Loss: 0.4378607
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.488938808441162
Epoch: 12, Steps: 56 | Train Loss: 0.5551645 Vali Loss: 1.4409525 Test Loss: 0.4381337
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.541988849639893
Epoch: 13, Steps: 56 | Train Loss: 0.5546700 Vali Loss: 1.4402666 Test Loss: 0.4381943
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.79338002204895
Epoch: 14, Steps: 56 | Train Loss: 0.5548248 Vali Loss: 1.4428985 Test Loss: 0.4385598
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.562992811203003
Epoch: 15, Steps: 56 | Train Loss: 0.5547512 Vali Loss: 1.4397345 Test Loss: 0.4385678
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.243421792984009
Epoch: 16, Steps: 56 | Train Loss: 0.5545268 Vali Loss: 1.4419379 Test Loss: 0.4386698
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.5051796436309814
Epoch: 17, Steps: 56 | Train Loss: 0.5542217 Vali Loss: 1.4416122 Test Loss: 0.4388795
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.395017147064209
Epoch: 18, Steps: 56 | Train Loss: 0.5541481 Vali Loss: 1.4411452 Test Loss: 0.4391106
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.557833909988403
Epoch: 19, Steps: 56 | Train Loss: 0.5541413 Vali Loss: 1.4428366 Test Loss: 0.4390187
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.8076171875
Epoch: 20, Steps: 56 | Train Loss: 0.5539945 Vali Loss: 1.4457201 Test Loss: 0.4392131
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.310755014419556
Epoch: 21, Steps: 56 | Train Loss: 0.5538868 Vali Loss: 1.4434284 Test Loss: 0.4392232
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.384547233581543
Epoch: 22, Steps: 56 | Train Loss: 0.5540304 Vali Loss: 1.4442611 Test Loss: 0.4393442
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.698362827301025
Epoch: 23, Steps: 56 | Train Loss: 0.5539377 Vali Loss: 1.4471985 Test Loss: 0.4393300
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.504377841949463
Epoch: 24, Steps: 56 | Train Loss: 0.5536023 Vali Loss: 1.4466484 Test Loss: 0.4393298
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.372565031051636
Epoch: 25, Steps: 56 | Train Loss: 0.5538088 Vali Loss: 1.4441490 Test Loss: 0.4393944
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.440386772155762
Epoch: 26, Steps: 56 | Train Loss: 0.5537049 Vali Loss: 1.4449216 Test Loss: 0.4394822
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.503465890884399
Epoch: 27, Steps: 56 | Train Loss: 0.5531458 Vali Loss: 1.4420927 Test Loss: 0.4395693
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.608338356018066
Epoch: 28, Steps: 56 | Train Loss: 0.5531074 Vali Loss: 1.4489026 Test Loss: 0.4395820
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.565431356430054
Epoch: 29, Steps: 56 | Train Loss: 0.5533189 Vali Loss: 1.4457192 Test Loss: 0.4396788
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.498660326004028
Epoch: 30, Steps: 56 | Train Loss: 0.5534099 Vali Loss: 1.4448467 Test Loss: 0.4396766
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4367954432964325, mae:0.459271639585495, rse:0.6326901316642761, corr:[0.22101627 0.23117605 0.2335625  0.23060745 0.22721815 0.22543514
 0.22551431 0.22686777 0.22800249 0.22847302 0.22815105 0.22731702
 0.226574   0.22609317 0.2258594  0.22578399 0.22569948 0.22560503
 0.22543727 0.22530675 0.22533809 0.22563718 0.22584039 0.22609496
 0.22637974 0.22679718 0.22729108 0.22751598 0.22737813 0.22689241
 0.22629221 0.22569562 0.22526407 0.22502883 0.22486179 0.22472125
 0.22473589 0.2247352  0.22472793 0.22477514 0.22500269 0.22517535
 0.2253211  0.22556162 0.22594482 0.22638859 0.22683896 0.2270892
 0.22668478 0.2259203  0.22484204 0.22378092 0.2230584  0.22245356
 0.22193731 0.221466   0.2212312  0.22107013 0.2207066  0.22026223
 0.21987575 0.21962644 0.21949159 0.21961516 0.21989469 0.22018534
 0.22047433 0.22066325 0.22068654 0.22057514 0.22026509 0.21996886
 0.21954729 0.21915561 0.21889894 0.21877412 0.21860896 0.21817653
 0.21758129 0.21703656 0.21659133 0.21631247 0.21610071 0.21585864
 0.2156214  0.21529572 0.2148704  0.21439487 0.21393503 0.21365269
 0.21357876 0.21388417 0.21446674 0.21529083 0.21630785 0.21738519
 0.21839388 0.2191424  0.21941812 0.21943113 0.21927387 0.21908648
 0.21888417 0.21870457 0.21852452 0.21832503 0.21808401 0.21787262
 0.2178631  0.21790454 0.21784057 0.21770753 0.21758972 0.21744734
 0.21749571 0.21776605 0.21813266 0.2184142  0.2185037  0.21841846
 0.21814655 0.21764465 0.21699364 0.2165358  0.21618438 0.21584064
 0.21547365 0.21518649 0.21485779 0.2146115  0.21448274 0.2143578
 0.21418259 0.21388301 0.21361524 0.21345332 0.21342088 0.2133925
 0.21334794 0.21327831 0.21322986 0.21318841 0.21315768 0.21315792
 0.21311428 0.21284609 0.21236104 0.21168883 0.21112387 0.21068782
 0.21037851 0.21032977 0.21044187 0.21057913 0.21059445 0.21046914
 0.2102515  0.21001661 0.20990223 0.20992158 0.2100007  0.21019024
 0.21037401 0.21056502 0.21070959 0.21083084 0.21090378 0.21106859
 0.21138363 0.21175933 0.21202223 0.21222547 0.21222062 0.21198565
 0.21162012 0.21133459 0.2111345  0.21108384 0.21111803 0.21113887
 0.21105836 0.21087702 0.21069331 0.21064714 0.21083204 0.2112109
 0.21165769 0.2120258  0.21228607 0.21236008 0.21223703 0.21195352
 0.21159923 0.21123411 0.21075457 0.21015477 0.20952383 0.2089135
 0.20839433 0.2081365  0.20807725 0.20805606 0.2079448  0.20776163
 0.20757566 0.20752685 0.20761381 0.20778155 0.20805673 0.20830524
 0.20842336 0.20835042 0.20826213 0.208152   0.20804814 0.20797971
 0.20786193 0.2077113  0.20745212 0.20718578 0.20707975 0.20704375
 0.20704904 0.20700543 0.2068538  0.20663202 0.20637494 0.20614658
 0.20599529 0.20578495 0.20549935 0.20514464 0.2047301  0.20439321
 0.20427912 0.2044086  0.20474352 0.20510319 0.20542942 0.20562814
 0.20571168 0.20571008 0.2056857  0.20573017 0.20576699 0.20572296
 0.20571607 0.20574632 0.20570037 0.2054508  0.20506404 0.20467256
 0.20443974 0.2043144  0.20435362 0.20449503 0.20467773 0.20481539
 0.20488407 0.20479353 0.20478801 0.20488207 0.20499492 0.20508565
 0.20511895 0.20505199 0.20486684 0.20457385 0.20413816 0.20370518
 0.20338085 0.20313829 0.20280287 0.20247303 0.20225343 0.20227896
 0.20258427 0.20305592 0.20341495 0.203566   0.20354234 0.20349829
 0.20346053 0.20354    0.20373148 0.20396754 0.20428334 0.20462385
 0.20482874 0.20484518 0.20462197 0.20430328 0.20392808 0.20365752
 0.20358358 0.2037639  0.2040894  0.20448862 0.20488891 0.20522085
 0.20545141 0.20552053 0.20553844 0.20557056 0.20564762 0.20572686
 0.20584437 0.20587412 0.20585047 0.20585567 0.20582159 0.20580709
 0.20575137 0.20550364 0.2049362  0.20426698 0.20372544 0.20339783
 0.20321716 0.20305464 0.20276827 0.20233108 0.20194584 0.20187235
 0.20220017 0.20270908 0.20302552 0.20315425 0.20293702 0.2025622
 0.2023197  0.20241289 0.20281786 0.20325688 0.20339198 0.203046
 0.20234895 0.20153414 0.20090014 0.20072405 0.20092958 0.20119123
 0.20128386 0.2011095  0.20072271 0.20037816 0.2003409  0.20064303
 0.20100331 0.2012631  0.20138872 0.20143819 0.2014366  0.20161694
 0.20187902 0.20204782 0.202046   0.20194471 0.20169498 0.20145215
 0.2012563  0.20095874 0.20051204 0.19994187 0.19933988 0.19877249
 0.19834913 0.19819753 0.19820447 0.19812329 0.1978375  0.19737674
 0.1968078  0.19635408 0.19621313 0.19631481 0.19657797 0.19675054
 0.19669949 0.19643603 0.19624467 0.19645135 0.19699569 0.197889
 0.19876339 0.19923212 0.19920877 0.1989862  0.19887546 0.19873886
 0.1984761  0.19797237 0.1972146  0.19636169 0.19562443 0.1953374
 0.1955382  0.19590588 0.19622149 0.19630381 0.19614574 0.19601217
 0.19601873 0.1962213  0.19668026 0.1973299  0.19788149 0.19838771
 0.19882    0.19908468 0.19912939 0.19902004 0.19879815 0.19835107
 0.19789235 0.1975542  0.19744092 0.1974986  0.19766195 0.19777502
 0.1976935  0.19750522 0.19716421 0.19692877 0.19692919 0.19718975
 0.19755296 0.19792636 0.19811508 0.19854696 0.19901446 0.19931278
 0.19939275 0.19911298 0.19855219 0.1981743  0.19814195 0.19833775
 0.19857629 0.19874105 0.19858006 0.19815548 0.19764535 0.19726135
 0.19707674 0.19704954 0.19711135 0.1970967  0.19687717 0.19654468
 0.19634382 0.19638623 0.19681339 0.19759975 0.19847332 0.19927065
 0.19970779 0.19976345 0.199508   0.1991721  0.19889453 0.19863771
 0.19847901 0.19842686 0.19840297 0.19843261 0.19838455 0.19823751
 0.19791637 0.19750911 0.1972181  0.19721054 0.19747646 0.19793886
 0.19836299 0.19860867 0.1986004  0.19836648 0.19829528 0.19835491
 0.19843446 0.19839236 0.19800806 0.19741946 0.196824   0.1963525
 0.19614175 0.19628204 0.19648321 0.19658995 0.19650425 0.19628827
 0.19608139 0.19593997 0.1959302  0.19598576 0.19604455 0.19599801
 0.19589542 0.1958241  0.19590223 0.19625387 0.19688703 0.19767377
 0.19839065 0.19904347 0.19946244 0.19965985 0.1996813  0.19934066
 0.19866912 0.19789705 0.19716989 0.19665359 0.19663443 0.19716801
 0.19788308 0.19840877 0.19870287 0.19863065 0.19846119 0.19836345
 0.19846286 0.1987939  0.19936259 0.1996708  0.19996072 0.19971289
 0.19937389 0.19892412 0.19842304 0.19818828 0.19803736 0.19798845
 0.19793224 0.19795863 0.19793196 0.19781126 0.19760278 0.19749843
 0.19753419 0.19760671 0.19777186 0.19785714 0.19798587 0.198143
 0.1983757  0.19869182 0.1989916  0.19914751 0.1989924  0.19838578
 0.19728912 0.19597408 0.19456144 0.19325967 0.19225667 0.19163373
 0.19134523 0.19131029 0.19126245 0.19117542 0.19111627 0.1911677
 0.19115788 0.19091055 0.19070888 0.19043285 0.19034407 0.19026525
 0.19021495 0.18999939 0.18990587 0.18998915 0.19032392 0.19070825
 0.19101942 0.19117954 0.19079204 0.19006473 0.1891191  0.18806203
 0.187301   0.18684311 0.18665902 0.18647146 0.18633562 0.18620668
 0.18622965 0.1861577  0.18613386 0.18607895 0.18600881 0.18593906
 0.1860611  0.1864947  0.18724002 0.1881242  0.18868984 0.18857533
 0.18776226 0.18643877 0.18497282 0.1837822  0.1830754  0.1826514
 0.18231447 0.18197124 0.18149072 0.18101604 0.18069841 0.18067843
 0.1808722  0.18096979 0.18081559 0.18029241 0.17958833 0.179033
 0.17894666 0.17932138 0.17998658 0.18066312 0.18115884 0.18125425
 0.18094015 0.18042761 0.17975433 0.17923076 0.17885873 0.1785249
 0.1782679  0.17809553 0.17783573 0.17743029 0.1769367  0.1766049
 0.1765179  0.17647532 0.17656775 0.17673685 0.17681046 0.17672129
 0.17639612 0.17605501 0.175844   0.17574115 0.17566116 0.1754399
 0.17494382 0.1743687  0.17388223 0.17390172 0.17412551 0.17416301
 0.17371388 0.17283235 0.17195989 0.1712951  0.17110024 0.17137446
 0.1718124  0.17182642 0.17136636 0.17060268 0.17011148 0.17024347
 0.17099789 0.17205964 0.1727659  0.17274767 0.17179513 0.1702983
 0.16865459 0.1673131  0.16627543 0.16548666 0.16484274 0.16396818
 0.16324344 0.16267063 0.16233386 0.16184522 0.16152021 0.16153124
 0.16171598 0.16175707 0.16191936 0.16212331 0.16224003 0.1621684
 0.16194408 0.16179204 0.16180913 0.16178277 0.16142559 0.16075824
 0.15994057 0.15970474 0.16012447 0.16139126 0.16257443 0.16267565
 0.1615453  0.1595916  0.15779969 0.15695623 0.15742765 0.15847406
 0.15896381 0.1578988  0.1561361  0.15493134 0.15541321 0.15725265
 0.15872483 0.15816565 0.15658842 0.15684056 0.16281264 0.17088181]
