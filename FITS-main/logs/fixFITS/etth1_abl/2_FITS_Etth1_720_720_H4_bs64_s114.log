Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.92183256149292
Epoch: 1, Steps: 56 | Train Loss: 0.8280275 Vali Loss: 2.1137130 Test Loss: 0.8998618
Validation loss decreased (inf --> 2.113713).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.038787364959717
Epoch: 2, Steps: 56 | Train Loss: 0.6535785 Vali Loss: 1.8997207 Test Loss: 0.7786858
Validation loss decreased (2.113713 --> 1.899721).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.105483055114746
Epoch: 3, Steps: 56 | Train Loss: 0.5721780 Vali Loss: 1.8102229 Test Loss: 0.7250767
Validation loss decreased (1.899721 --> 1.810223).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.13851261138916
Epoch: 4, Steps: 56 | Train Loss: 0.5295996 Vali Loss: 1.7648144 Test Loss: 0.6980392
Validation loss decreased (1.810223 --> 1.764814).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.104893684387207
Epoch: 5, Steps: 56 | Train Loss: 0.5026474 Vali Loss: 1.7331071 Test Loss: 0.6802459
Validation loss decreased (1.764814 --> 1.733107).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.0998830795288086
Epoch: 6, Steps: 56 | Train Loss: 0.4829790 Vali Loss: 1.7116276 Test Loss: 0.6669927
Validation loss decreased (1.733107 --> 1.711628).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.914215087890625
Epoch: 7, Steps: 56 | Train Loss: 0.4671975 Vali Loss: 1.6961119 Test Loss: 0.6554571
Validation loss decreased (1.711628 --> 1.696112).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.281741142272949
Epoch: 8, Steps: 56 | Train Loss: 0.4541799 Vali Loss: 1.6709168 Test Loss: 0.6443466
Validation loss decreased (1.696112 --> 1.670917).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.9986093044281006
Epoch: 9, Steps: 56 | Train Loss: 0.4426746 Vali Loss: 1.6622189 Test Loss: 0.6348861
Validation loss decreased (1.670917 --> 1.662219).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.1385581493377686
Epoch: 10, Steps: 56 | Train Loss: 0.4328792 Vali Loss: 1.6463583 Test Loss: 0.6258349
Validation loss decreased (1.662219 --> 1.646358).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.0950114727020264
Epoch: 11, Steps: 56 | Train Loss: 0.4238068 Vali Loss: 1.6462746 Test Loss: 0.6175901
Validation loss decreased (1.646358 --> 1.646275).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.942775011062622
Epoch: 12, Steps: 56 | Train Loss: 0.4161172 Vali Loss: 1.6320361 Test Loss: 0.6098639
Validation loss decreased (1.646275 --> 1.632036).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.031789779663086
Epoch: 13, Steps: 56 | Train Loss: 0.4092874 Vali Loss: 1.6239793 Test Loss: 0.6025094
Validation loss decreased (1.632036 --> 1.623979).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.087847948074341
Epoch: 14, Steps: 56 | Train Loss: 0.4031139 Vali Loss: 1.6168802 Test Loss: 0.5960041
Validation loss decreased (1.623979 --> 1.616880).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.283972978591919
Epoch: 15, Steps: 56 | Train Loss: 0.3973731 Vali Loss: 1.6062171 Test Loss: 0.5908418
Validation loss decreased (1.616880 --> 1.606217).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.11010479927063
Epoch: 16, Steps: 56 | Train Loss: 0.3924236 Vali Loss: 1.6035471 Test Loss: 0.5851243
Validation loss decreased (1.606217 --> 1.603547).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.150604248046875
Epoch: 17, Steps: 56 | Train Loss: 0.3879384 Vali Loss: 1.5891224 Test Loss: 0.5797405
Validation loss decreased (1.603547 --> 1.589122).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.157201051712036
Epoch: 18, Steps: 56 | Train Loss: 0.3839020 Vali Loss: 1.5885921 Test Loss: 0.5749925
Validation loss decreased (1.589122 --> 1.588592).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9803810119628906
Epoch: 19, Steps: 56 | Train Loss: 0.3801163 Vali Loss: 1.5863316 Test Loss: 0.5703502
Validation loss decreased (1.588592 --> 1.586332).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.0558526515960693
Epoch: 20, Steps: 56 | Train Loss: 0.3766125 Vali Loss: 1.5774763 Test Loss: 0.5660430
Validation loss decreased (1.586332 --> 1.577476).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.139899969100952
Epoch: 21, Steps: 56 | Train Loss: 0.3734993 Vali Loss: 1.5712876 Test Loss: 0.5622353
Validation loss decreased (1.577476 --> 1.571288).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0512988567352295
Epoch: 22, Steps: 56 | Train Loss: 0.3705111 Vali Loss: 1.5671046 Test Loss: 0.5586852
Validation loss decreased (1.571288 --> 1.567105).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.1262903213500977
Epoch: 23, Steps: 56 | Train Loss: 0.3677927 Vali Loss: 1.5685265 Test Loss: 0.5553716
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.1291754245758057
Epoch: 24, Steps: 56 | Train Loss: 0.3653530 Vali Loss: 1.5613661 Test Loss: 0.5520346
Validation loss decreased (1.567105 --> 1.561366).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.0191147327423096
Epoch: 25, Steps: 56 | Train Loss: 0.3631593 Vali Loss: 1.5558212 Test Loss: 0.5487421
Validation loss decreased (1.561366 --> 1.555821).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.1353139877319336
Epoch: 26, Steps: 56 | Train Loss: 0.3611209 Vali Loss: 1.5541745 Test Loss: 0.5463486
Validation loss decreased (1.555821 --> 1.554175).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1350367069244385
Epoch: 27, Steps: 56 | Train Loss: 0.3590254 Vali Loss: 1.5509350 Test Loss: 0.5437928
Validation loss decreased (1.554175 --> 1.550935).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.1272380352020264
Epoch: 28, Steps: 56 | Train Loss: 0.3573539 Vali Loss: 1.5455608 Test Loss: 0.5412590
Validation loss decreased (1.550935 --> 1.545561).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.181931495666504
Epoch: 29, Steps: 56 | Train Loss: 0.3556374 Vali Loss: 1.5499144 Test Loss: 0.5387056
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.955280065536499
Epoch: 30, Steps: 56 | Train Loss: 0.3541382 Vali Loss: 1.5474248 Test Loss: 0.5366002
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.9348597526550293
Epoch: 31, Steps: 56 | Train Loss: 0.3525414 Vali Loss: 1.5443884 Test Loss: 0.5347314
Validation loss decreased (1.545561 --> 1.544388).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.0731496810913086
Epoch: 32, Steps: 56 | Train Loss: 0.3513847 Vali Loss: 1.5383776 Test Loss: 0.5326465
Validation loss decreased (1.544388 --> 1.538378).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.9595293998718262
Epoch: 33, Steps: 56 | Train Loss: 0.3501217 Vali Loss: 1.5395555 Test Loss: 0.5308180
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.079183340072632
Epoch: 34, Steps: 56 | Train Loss: 0.3488259 Vali Loss: 1.5364103 Test Loss: 0.5291020
Validation loss decreased (1.538378 --> 1.536410).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.1632306575775146
Epoch: 35, Steps: 56 | Train Loss: 0.3475422 Vali Loss: 1.5313313 Test Loss: 0.5275664
Validation loss decreased (1.536410 --> 1.531331).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.0438783168792725
Epoch: 36, Steps: 56 | Train Loss: 0.3467208 Vali Loss: 1.5288491 Test Loss: 0.5262187
Validation loss decreased (1.531331 --> 1.528849).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8872582912445068
Epoch: 37, Steps: 56 | Train Loss: 0.3456660 Vali Loss: 1.5258864 Test Loss: 0.5247790
Validation loss decreased (1.528849 --> 1.525886).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.065159320831299
Epoch: 38, Steps: 56 | Train Loss: 0.3445718 Vali Loss: 1.5325261 Test Loss: 0.5233352
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.1484806537628174
Epoch: 39, Steps: 56 | Train Loss: 0.3437117 Vali Loss: 1.5274562 Test Loss: 0.5221233
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.074619770050049
Epoch: 40, Steps: 56 | Train Loss: 0.3429238 Vali Loss: 1.5217844 Test Loss: 0.5208429
Validation loss decreased (1.525886 --> 1.521784).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.181077241897583
Epoch: 41, Steps: 56 | Train Loss: 0.3422749 Vali Loss: 1.5214758 Test Loss: 0.5197104
Validation loss decreased (1.521784 --> 1.521476).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.0735275745391846
Epoch: 42, Steps: 56 | Train Loss: 0.3415028 Vali Loss: 1.5205454 Test Loss: 0.5185669
Validation loss decreased (1.521476 --> 1.520545).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.186218500137329
Epoch: 43, Steps: 56 | Train Loss: 0.3408314 Vali Loss: 1.5222907 Test Loss: 0.5175505
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.070833444595337
Epoch: 44, Steps: 56 | Train Loss: 0.3402344 Vali Loss: 1.5238353 Test Loss: 0.5166807
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9817874431610107
Epoch: 45, Steps: 56 | Train Loss: 0.3397701 Vali Loss: 1.5187796 Test Loss: 0.5157126
Validation loss decreased (1.520545 --> 1.518780).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.0405502319335938
Epoch: 46, Steps: 56 | Train Loss: 0.3390755 Vali Loss: 1.5149328 Test Loss: 0.5148506
Validation loss decreased (1.518780 --> 1.514933).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.2344751358032227
Epoch: 47, Steps: 56 | Train Loss: 0.3385200 Vali Loss: 1.5180809 Test Loss: 0.5140750
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.1792118549346924
Epoch: 48, Steps: 56 | Train Loss: 0.3379161 Vali Loss: 1.5199864 Test Loss: 0.5133075
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.082989454269409
Epoch: 49, Steps: 56 | Train Loss: 0.3375331 Vali Loss: 1.5153575 Test Loss: 0.5125525
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.0794100761413574
Epoch: 50, Steps: 56 | Train Loss: 0.3371904 Vali Loss: 1.5163367 Test Loss: 0.5118498
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9953968524932861
Epoch: 51, Steps: 56 | Train Loss: 0.3366843 Vali Loss: 1.5117259 Test Loss: 0.5111929
Validation loss decreased (1.514933 --> 1.511726).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.135120391845703
Epoch: 52, Steps: 56 | Train Loss: 0.3363502 Vali Loss: 1.5121537 Test Loss: 0.5105103
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.207000255584717
Epoch: 53, Steps: 56 | Train Loss: 0.3357596 Vali Loss: 1.5136313 Test Loss: 0.5100163
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9728808403015137
Epoch: 54, Steps: 56 | Train Loss: 0.3354953 Vali Loss: 1.5166867 Test Loss: 0.5094012
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.937748670578003
Epoch: 55, Steps: 56 | Train Loss: 0.3352179 Vali Loss: 1.5114284 Test Loss: 0.5088738
Validation loss decreased (1.511726 --> 1.511428).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.129138469696045
Epoch: 56, Steps: 56 | Train Loss: 0.3347692 Vali Loss: 1.5129607 Test Loss: 0.5083799
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.092799425125122
Epoch: 57, Steps: 56 | Train Loss: 0.3346889 Vali Loss: 1.5123274 Test Loss: 0.5078669
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.9856767654418945
Epoch: 58, Steps: 56 | Train Loss: 0.3343635 Vali Loss: 1.5097764 Test Loss: 0.5074373
Validation loss decreased (1.511428 --> 1.509776).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.0239713191986084
Epoch: 59, Steps: 56 | Train Loss: 0.3339344 Vali Loss: 1.5111802 Test Loss: 0.5070136
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.0575203895568848
Epoch: 60, Steps: 56 | Train Loss: 0.3337099 Vali Loss: 1.5080270 Test Loss: 0.5065747
Validation loss decreased (1.509776 --> 1.508027).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.046640396118164
Epoch: 61, Steps: 56 | Train Loss: 0.3332686 Vali Loss: 1.5107675 Test Loss: 0.5061970
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.0300822257995605
Epoch: 62, Steps: 56 | Train Loss: 0.3332560 Vali Loss: 1.5062517 Test Loss: 0.5057673
Validation loss decreased (1.508027 --> 1.506252).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.2374792098999023
Epoch: 63, Steps: 56 | Train Loss: 0.3328646 Vali Loss: 1.5091295 Test Loss: 0.5054347
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.137049913406372
Epoch: 64, Steps: 56 | Train Loss: 0.3326674 Vali Loss: 1.5102546 Test Loss: 0.5051162
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.2158076763153076
Epoch: 65, Steps: 56 | Train Loss: 0.3325467 Vali Loss: 1.5105290 Test Loss: 0.5047988
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.9531667232513428
Epoch: 66, Steps: 56 | Train Loss: 0.3322736 Vali Loss: 1.5047889 Test Loss: 0.5045252
Validation loss decreased (1.506252 --> 1.504789).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.0193891525268555
Epoch: 67, Steps: 56 | Train Loss: 0.3322442 Vali Loss: 1.5060343 Test Loss: 0.5042127
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.0891268253326416
Epoch: 68, Steps: 56 | Train Loss: 0.3318838 Vali Loss: 1.5061437 Test Loss: 0.5039476
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.026526689529419
Epoch: 69, Steps: 56 | Train Loss: 0.3319191 Vali Loss: 1.5071726 Test Loss: 0.5036857
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9915084838867188
Epoch: 70, Steps: 56 | Train Loss: 0.3316382 Vali Loss: 1.5018989 Test Loss: 0.5034359
Validation loss decreased (1.504789 --> 1.501899).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.030758857727051
Epoch: 71, Steps: 56 | Train Loss: 0.3314318 Vali Loss: 1.5082709 Test Loss: 0.5032015
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.112643241882324
Epoch: 72, Steps: 56 | Train Loss: 0.3312404 Vali Loss: 1.5010149 Test Loss: 0.5030131
Validation loss decreased (1.501899 --> 1.501015).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.10235857963562
Epoch: 73, Steps: 56 | Train Loss: 0.3311199 Vali Loss: 1.5049967 Test Loss: 0.5027593
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.9620919227600098
Epoch: 74, Steps: 56 | Train Loss: 0.3310210 Vali Loss: 1.5059775 Test Loss: 0.5025610
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.985504388809204
Epoch: 75, Steps: 56 | Train Loss: 0.3310062 Vali Loss: 1.5074531 Test Loss: 0.5023723
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.132476568222046
Epoch: 76, Steps: 56 | Train Loss: 0.3308329 Vali Loss: 1.5033753 Test Loss: 0.5022040
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.0866432189941406
Epoch: 77, Steps: 56 | Train Loss: 0.3307376 Vali Loss: 1.5024874 Test Loss: 0.5020207
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.972172737121582
Epoch: 78, Steps: 56 | Train Loss: 0.3306419 Vali Loss: 1.5051739 Test Loss: 0.5018735
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.086195945739746
Epoch: 79, Steps: 56 | Train Loss: 0.3305694 Vali Loss: 1.5028551 Test Loss: 0.5017054
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.2274057865142822
Epoch: 80, Steps: 56 | Train Loss: 0.3303687 Vali Loss: 1.5058196 Test Loss: 0.5015674
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.2430837154388428
Epoch: 81, Steps: 56 | Train Loss: 0.3302287 Vali Loss: 1.5058365 Test Loss: 0.5014184
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.140364408493042
Epoch: 82, Steps: 56 | Train Loss: 0.3302271 Vali Loss: 1.5026886 Test Loss: 0.5012898
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.31943416595459
Epoch: 83, Steps: 56 | Train Loss: 0.3300848 Vali Loss: 1.4992734 Test Loss: 0.5011582
Validation loss decreased (1.501015 --> 1.499273).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.1033594608306885
Epoch: 84, Steps: 56 | Train Loss: 0.3302179 Vali Loss: 1.5025539 Test Loss: 0.5010231
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.2034685611724854
Epoch: 85, Steps: 56 | Train Loss: 0.3300149 Vali Loss: 1.5053270 Test Loss: 0.5009159
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.100694179534912
Epoch: 86, Steps: 56 | Train Loss: 0.3300271 Vali Loss: 1.5046916 Test Loss: 0.5008207
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.1346421241760254
Epoch: 87, Steps: 56 | Train Loss: 0.3298744 Vali Loss: 1.5061485 Test Loss: 0.5007046
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.065650224685669
Epoch: 88, Steps: 56 | Train Loss: 0.3298623 Vali Loss: 1.5060813 Test Loss: 0.5006023
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.061215877532959
Epoch: 89, Steps: 56 | Train Loss: 0.3297410 Vali Loss: 1.5035257 Test Loss: 0.5005113
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.118591070175171
Epoch: 90, Steps: 56 | Train Loss: 0.3296725 Vali Loss: 1.4989195 Test Loss: 0.5004268
Validation loss decreased (1.499273 --> 1.498919).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.9526302814483643
Epoch: 91, Steps: 56 | Train Loss: 0.3295143 Vali Loss: 1.5043786 Test Loss: 0.5003318
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.094308376312256
Epoch: 92, Steps: 56 | Train Loss: 0.3295906 Vali Loss: 1.4977150 Test Loss: 0.5002577
Validation loss decreased (1.498919 --> 1.497715).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.9948060512542725
Epoch: 93, Steps: 56 | Train Loss: 0.3295608 Vali Loss: 1.5018353 Test Loss: 0.5001734
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.0907695293426514
Epoch: 94, Steps: 56 | Train Loss: 0.3294152 Vali Loss: 1.5020819 Test Loss: 0.5001045
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.236919641494751
Epoch: 95, Steps: 56 | Train Loss: 0.3296763 Vali Loss: 1.5018626 Test Loss: 0.5000365
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.0810344219207764
Epoch: 96, Steps: 56 | Train Loss: 0.3293927 Vali Loss: 1.5028713 Test Loss: 0.4999696
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.9575748443603516
Epoch: 97, Steps: 56 | Train Loss: 0.3293810 Vali Loss: 1.5062186 Test Loss: 0.4998994
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.0823400020599365
Epoch: 98, Steps: 56 | Train Loss: 0.3294221 Vali Loss: 1.5052829 Test Loss: 0.4998472
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.9023640155792236
Epoch: 99, Steps: 56 | Train Loss: 0.3294336 Vali Loss: 1.4995810 Test Loss: 0.4997923
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.032223701477051
Epoch: 100, Steps: 56 | Train Loss: 0.3291648 Vali Loss: 1.5016364 Test Loss: 0.4997356
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.0345425605773926
Epoch: 1, Steps: 56 | Train Loss: 0.5834181 Vali Loss: 1.4734173 Test Loss: 0.4715264
Validation loss decreased (inf --> 1.473417).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.035108804702759
Epoch: 2, Steps: 56 | Train Loss: 0.5704429 Vali Loss: 1.4547195 Test Loss: 0.4541008
Validation loss decreased (1.473417 --> 1.454720).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0700414180755615
Epoch: 3, Steps: 56 | Train Loss: 0.5630785 Vali Loss: 1.4454960 Test Loss: 0.4444196
Validation loss decreased (1.454720 --> 1.445496).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.189009666442871
Epoch: 4, Steps: 56 | Train Loss: 0.5584102 Vali Loss: 1.4365019 Test Loss: 0.4396665
Validation loss decreased (1.445496 --> 1.436502).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.0806074142456055
Epoch: 5, Steps: 56 | Train Loss: 0.5559465 Vali Loss: 1.4383749 Test Loss: 0.4371845
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.121918201446533
Epoch: 6, Steps: 56 | Train Loss: 0.5544221 Vali Loss: 1.4419909 Test Loss: 0.4361328
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.2595114707946777
Epoch: 7, Steps: 56 | Train Loss: 0.5532174 Vali Loss: 1.4399443 Test Loss: 0.4357792
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.0802111625671387
Epoch: 8, Steps: 56 | Train Loss: 0.5524851 Vali Loss: 1.4374036 Test Loss: 0.4358471
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.0893118381500244
Epoch: 9, Steps: 56 | Train Loss: 0.5518680 Vali Loss: 1.4397552 Test Loss: 0.4359219
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.1111598014831543
Epoch: 10, Steps: 56 | Train Loss: 0.5519319 Vali Loss: 1.4388733 Test Loss: 0.4360006
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9375035762786865
Epoch: 11, Steps: 56 | Train Loss: 0.5513130 Vali Loss: 1.4400103 Test Loss: 0.4361644
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9658586978912354
Epoch: 12, Steps: 56 | Train Loss: 0.5511617 Vali Loss: 1.4389291 Test Loss: 0.4362561
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.099829912185669
Epoch: 13, Steps: 56 | Train Loss: 0.5512066 Vali Loss: 1.4414032 Test Loss: 0.4364363
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.1946022510528564
Epoch: 14, Steps: 56 | Train Loss: 0.5510087 Vali Loss: 1.4463656 Test Loss: 0.4366349
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.049179792404175
Epoch: 15, Steps: 56 | Train Loss: 0.5506820 Vali Loss: 1.4434590 Test Loss: 0.4367286
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.1051974296569824
Epoch: 16, Steps: 56 | Train Loss: 0.5506200 Vali Loss: 1.4462157 Test Loss: 0.4366939
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.2671921253204346
Epoch: 17, Steps: 56 | Train Loss: 0.5504626 Vali Loss: 1.4389452 Test Loss: 0.4369319
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.0472168922424316
Epoch: 18, Steps: 56 | Train Loss: 0.5504459 Vali Loss: 1.4404148 Test Loss: 0.4370979
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.1498687267303467
Epoch: 19, Steps: 56 | Train Loss: 0.5503292 Vali Loss: 1.4448986 Test Loss: 0.4371437
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.042043685913086
Epoch: 20, Steps: 56 | Train Loss: 0.5504289 Vali Loss: 1.4437560 Test Loss: 0.4370504
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0117645263671875
Epoch: 21, Steps: 56 | Train Loss: 0.5501122 Vali Loss: 1.4408479 Test Loss: 0.4371262
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0569896697998047
Epoch: 22, Steps: 56 | Train Loss: 0.5502704 Vali Loss: 1.4496560 Test Loss: 0.4373197
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9961302280426025
Epoch: 23, Steps: 56 | Train Loss: 0.5500876 Vali Loss: 1.4455841 Test Loss: 0.4373232
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.9509778022766113
Epoch: 24, Steps: 56 | Train Loss: 0.5496170 Vali Loss: 1.4420819 Test Loss: 0.4374195
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4387299120426178, mae:0.4639001786708832, rse:0.6340895295143127, corr:[0.22217499 0.23428054 0.23499441 0.23083776 0.2289366  0.22956094
 0.2309234  0.2315888  0.23103912 0.23040973 0.23014927 0.23021469
 0.2303821  0.23032995 0.23005782 0.22951974 0.22866437 0.22763509
 0.22670405 0.22624683 0.22630128 0.22664705 0.22677474 0.22679181
 0.22699344 0.2274457  0.22773862 0.22751974 0.22699891 0.22655013
 0.22650811 0.22661236 0.22664414 0.22636585 0.22574268 0.22521512
 0.22513758 0.22521853 0.22523329 0.22507685 0.224985   0.22490178
 0.22480297 0.22482723 0.22508965 0.22562769 0.22645672 0.227213
 0.22730272 0.22681352 0.22587599 0.22497049 0.22443376 0.22390574
 0.22328115 0.22254598 0.2221502  0.22199441 0.22180827 0.22156887
 0.2211941  0.22071807 0.2202865  0.22017597 0.22027706 0.22035527
 0.22034663 0.2203145  0.22036307 0.22046573 0.2203917  0.22014263
 0.21958262 0.21898837 0.21851605 0.2182454  0.21802235 0.21768403
 0.21736833 0.217182   0.21702197 0.21681625 0.2164394  0.21599425
 0.2156659  0.21538839 0.21503448 0.21457137 0.21405308 0.21372594
 0.21369123 0.21412334 0.2147418  0.21528402 0.21568695 0.2162214
 0.2169736  0.21776308 0.21839595 0.21901244 0.2195081  0.21972595
 0.2195757  0.2191625  0.21867822 0.21822865 0.21790709 0.21779656
 0.21796666 0.21820378 0.21828319 0.21824798 0.21811351 0.21787728
 0.21775049 0.21778975 0.21795154 0.21811602 0.21819042 0.21816304
 0.21796739 0.21749082 0.21680845 0.2163326  0.21600252 0.21567686
 0.21530774 0.21498209 0.21457574 0.21420412 0.21392147 0.21370025
 0.213539   0.2133279  0.21314992 0.21307778 0.21306951 0.21300602
 0.2128648  0.21274693 0.21282251 0.21303469 0.213177   0.21313533
 0.21272783 0.2119758  0.2112824  0.21094836 0.21116465 0.21153072
 0.21163498 0.21153758 0.21138361 0.21131334 0.21135794 0.21137851
 0.21111484 0.21051967 0.20991789 0.20961446 0.20952486 0.20963857
 0.20973389 0.20985258 0.21005788 0.21039738 0.210567   0.21069098
 0.21085727 0.21116094 0.21153426 0.2120358  0.21233486 0.2122255
 0.21181734 0.21147694 0.21129093 0.21131624 0.21130966 0.21109201
 0.21063882 0.21008348 0.2096762  0.20956472 0.20977356 0.21023011
 0.21084417 0.21154642 0.21225698 0.21262516 0.21256346 0.21214893
 0.21150918 0.21079977 0.21014749 0.20972711 0.20958562 0.20952001
 0.20931827 0.20900665 0.20850433 0.2078645  0.20735423 0.20731615
 0.20776916 0.20856363 0.20928621 0.20961338 0.2094846  0.20898326
 0.20833583 0.20778283 0.20760553 0.20770161 0.2078534  0.20788328
 0.20765933 0.2073292  0.20704529 0.20697705 0.20711483 0.20714298
 0.20701249 0.20675035 0.20640665 0.20605882 0.20567474 0.20532489
 0.20505479 0.20485021 0.20476466 0.20473905 0.20456237 0.20430258
 0.2041218  0.20407651 0.2041122  0.20404513 0.20393613 0.20392698
 0.20425886 0.20482129 0.20540722 0.20581192 0.20592088 0.20586364
 0.20595275 0.20615588 0.20615382 0.20573309 0.20507704 0.20456547
 0.20443934 0.20453319 0.20473568 0.20491444 0.2050789  0.20518877
 0.20510694 0.20470113 0.20442262 0.20449005 0.20487407 0.20533565
 0.20550463 0.20515478 0.2043714  0.20346801 0.20262922 0.20211071
 0.20203237 0.20224619 0.20242427 0.2025697  0.2025705  0.20241132
 0.202198   0.20223558 0.20254064 0.20304    0.20337558 0.20336896
 0.20298187 0.20264101 0.2026871  0.2030547  0.20341852 0.203652
 0.20372343 0.20374282 0.20391867 0.20445138 0.20502353 0.20540878
 0.20554583 0.20563577 0.20573749 0.20585115 0.20582452 0.2055838
 0.20513548 0.20451076 0.20400874 0.20372656 0.20357774 0.20360118
 0.20396826 0.20451583 0.20517978 0.20580676 0.20623398 0.20650293
 0.20652223 0.20618953 0.20558712 0.20508164 0.2047769  0.2045306
 0.20425142 0.20402248 0.20385982 0.203683   0.20342945 0.20314442
 0.20288666 0.20277311 0.20283858 0.20321229 0.20343892 0.20351666
 0.20355043 0.20372772 0.2042182  0.20477916 0.20497523 0.20457429
 0.20381778 0.20298265 0.20234598 0.20201176 0.20180331 0.2015341
 0.20125471 0.20097408 0.20063154 0.2003403  0.20024258 0.20047347
 0.20093593 0.201543   0.20203182 0.20218661 0.20188075 0.20155804
 0.20143403 0.20151883 0.20172532 0.20190382 0.20175613 0.2014259
 0.20102373 0.2005211  0.20002228 0.1996097  0.19924746 0.19883148
 0.19842666 0.19822946 0.19826661 0.19841282 0.19848932 0.19842006
 0.19809538 0.19753204 0.19690138 0.19625652 0.19579275 0.1956412
 0.19583848 0.19621487 0.19663507 0.19703838 0.19724575 0.19759133
 0.19816631 0.19886263 0.19941156 0.19964343 0.19956705 0.19915482
 0.19874994 0.19852456 0.19831441 0.19790636 0.19718255 0.19649553
 0.19604501 0.19572748 0.1955136  0.19530731 0.19519134 0.19548671
 0.19613211 0.19685215 0.19754194 0.19815862 0.19865854 0.19924915
 0.19982457 0.200059   0.19987851 0.19962417 0.19958511 0.1995563
 0.19941793 0.19902633 0.1985439  0.19830248 0.19848394 0.19882542
 0.19885476 0.19847764 0.19784573 0.19764829 0.19808118 0.19876494
 0.19907245 0.19884674 0.19826296 0.1981899  0.19856663 0.19896547
 0.19912979 0.19897369 0.19879335 0.19893184 0.19907373 0.19878064
 0.19821313 0.1980086  0.19822039 0.19859587 0.19855063 0.19796045
 0.19726603 0.1971574  0.19790773 0.19878213 0.19886483 0.19800018
 0.19688065 0.19626705 0.19649942 0.19726257 0.1979971  0.19858363
 0.19899695 0.19935398 0.19957142 0.1995105  0.19911453 0.19849125
 0.19809137 0.19804975 0.1981463  0.19837005 0.19864573 0.1991008
 0.19958606 0.1998681  0.19984497 0.19972141 0.19974715 0.2000794
 0.20039432 0.20026448 0.1995428  0.19847736 0.19779526 0.19768606
 0.19797976 0.19835605 0.19855699 0.1987052  0.19876763 0.19846815
 0.19788152 0.19745146 0.19731654 0.19764486 0.19808279 0.19813892
 0.19757949 0.19657673 0.19575435 0.1954767  0.19564086 0.19580539
 0.19574295 0.19556697 0.19566938 0.1963144  0.19740115 0.19861864
 0.19958231 0.20022564 0.20047317 0.20047201 0.20030338 0.19975024
 0.1988939  0.19803317 0.19721632 0.19650923 0.19621399 0.19661897
 0.1975377  0.19854663 0.19940093 0.19969869 0.19952571 0.19910437
 0.19886084 0.19906557 0.19980815 0.20033884 0.20071414 0.20036297
 0.1999469  0.19971754 0.19977406 0.2000622  0.19981398 0.19895156
 0.19798806 0.19773272 0.19820864 0.19897537 0.19911864 0.19830033
 0.19678007 0.195295   0.19477582 0.19518511 0.19599658 0.19633181
 0.1959064  0.19510025 0.19455686 0.19471714 0.19555134 0.19667192
 0.19750711 0.19777873 0.19729191 0.19636789 0.19558729 0.19535382
 0.1955519  0.19563738 0.19500501 0.1937764  0.19257158 0.19201626
 0.19195247 0.19167355 0.19100706 0.18975629 0.18871517 0.18833797
 0.18874574 0.18931109 0.18978353 0.18991545 0.1898367  0.18969142
 0.1896385  0.18978484 0.18982534 0.19006592 0.19020675 0.18978517
 0.18905674 0.18828578 0.18798587 0.18823175 0.18893123 0.18939885
 0.18926258 0.1883204  0.18731754 0.18674193 0.18663415 0.18659101
 0.18643388 0.1863362  0.18655054 0.1870199  0.1872823  0.1870107
 0.18638273 0.18565936 0.18487161 0.18395366 0.18283492 0.18161276
 0.18086995 0.18096621 0.18147291 0.181898   0.18188016 0.18155771
 0.18109041 0.18039815 0.1794511  0.1783143  0.17746468 0.17743842
 0.17830722 0.1795075  0.18044011 0.18080674 0.1807732  0.18045388
 0.17987426 0.17907344 0.17822492 0.17805286 0.17876193 0.17985639
 0.1806557  0.18060885 0.17971891 0.17875354 0.17835468 0.1786046
 0.1788136  0.17827205 0.1773686  0.17668536 0.17635721 0.1760961
 0.17557725 0.17514873 0.175329   0.17616987 0.17714679 0.17745025
 0.17669497 0.17553253 0.17481656 0.17523909 0.17611738 0.17672849
 0.17691687 0.17682542 0.17648403 0.17544153 0.17372073 0.17188945
 0.17063673 0.17001335 0.16982351 0.16954303 0.16922337 0.16911721
 0.16930698 0.16956918 0.16938873 0.16885686 0.16842045 0.1685732
 0.16885048 0.16848505 0.16691564 0.16474438 0.16315807 0.16241214
 0.16277705 0.1633639  0.16347376 0.16279016 0.16225259 0.16234878
 0.16248035 0.16197841 0.16141544 0.1610665  0.1608234  0.16029269
 0.15953511 0.15942505 0.16058147 0.16233747 0.16327108 0.16253193
 0.16067538 0.15967274 0.15988266 0.16083379 0.16076641 0.15942088
 0.15854862 0.15917318 0.16060628 0.16089389 0.15934873 0.1569428
 0.15534052 0.15452422 0.15410246 0.1534226  0.15349945 0.15534045
 0.15793    0.15809569 0.15522483 0.15371946 0.15987192 0.16343819]
