Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_96_FITS_ETTh1_ftM_sl360_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8185
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12726784.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.1432585716247559
Epoch: 1, Steps: 63 | Train Loss: 0.6232876 Vali Loss: 1.1045376 Test Loss: 0.6012175
Validation loss decreased (inf --> 1.104538).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.172027349472046
Epoch: 2, Steps: 63 | Train Loss: 0.4512690 Vali Loss: 0.9112193 Test Loss: 0.4749987
Validation loss decreased (1.104538 --> 0.911219).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.1215424537658691
Epoch: 3, Steps: 63 | Train Loss: 0.3965649 Vali Loss: 0.8211767 Test Loss: 0.4197438
Validation loss decreased (0.911219 --> 0.821177).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.0627939701080322
Epoch: 4, Steps: 63 | Train Loss: 0.3698221 Vali Loss: 0.7728140 Test Loss: 0.3940996
Validation loss decreased (0.821177 --> 0.772814).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.0904216766357422
Epoch: 5, Steps: 63 | Train Loss: 0.3568444 Vali Loss: 0.7479108 Test Loss: 0.3825473
Validation loss decreased (0.772814 --> 0.747911).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.0775196552276611
Epoch: 6, Steps: 63 | Train Loss: 0.3496882 Vali Loss: 0.7327144 Test Loss: 0.3776082
Validation loss decreased (0.747911 --> 0.732714).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.0963456630706787
Epoch: 7, Steps: 63 | Train Loss: 0.3465728 Vali Loss: 0.7254277 Test Loss: 0.3756301
Validation loss decreased (0.732714 --> 0.725428).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.185800552368164
Epoch: 8, Steps: 63 | Train Loss: 0.3440963 Vali Loss: 0.7135072 Test Loss: 0.3747745
Validation loss decreased (0.725428 --> 0.713507).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.6595242023468018
Epoch: 9, Steps: 63 | Train Loss: 0.3433090 Vali Loss: 0.7083434 Test Loss: 0.3743429
Validation loss decreased (0.713507 --> 0.708343).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.216651439666748
Epoch: 10, Steps: 63 | Train Loss: 0.3424323 Vali Loss: 0.7061260 Test Loss: 0.3740333
Validation loss decreased (0.708343 --> 0.706126).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.231656551361084
Epoch: 11, Steps: 63 | Train Loss: 0.3418835 Vali Loss: 0.7007731 Test Loss: 0.3739369
Validation loss decreased (0.706126 --> 0.700773).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.177748203277588
Epoch: 12, Steps: 63 | Train Loss: 0.3416833 Vali Loss: 0.7002891 Test Loss: 0.3737867
Validation loss decreased (0.700773 --> 0.700289).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.1630761623382568
Epoch: 13, Steps: 63 | Train Loss: 0.3402523 Vali Loss: 0.6959414 Test Loss: 0.3735102
Validation loss decreased (0.700289 --> 0.695941).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.1436595916748047
Epoch: 14, Steps: 63 | Train Loss: 0.3405423 Vali Loss: 0.6980280 Test Loss: 0.3735353
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1145403385162354
Epoch: 15, Steps: 63 | Train Loss: 0.3402175 Vali Loss: 0.6939577 Test Loss: 0.3734595
Validation loss decreased (0.695941 --> 0.693958).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.341780424118042
Epoch: 16, Steps: 63 | Train Loss: 0.3395578 Vali Loss: 0.6940881 Test Loss: 0.3733432
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.1173443794250488
Epoch: 17, Steps: 63 | Train Loss: 0.3394404 Vali Loss: 0.6922953 Test Loss: 0.3732128
Validation loss decreased (0.693958 --> 0.692295).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.0672059059143066
Epoch: 18, Steps: 63 | Train Loss: 0.3396221 Vali Loss: 0.6913294 Test Loss: 0.3732360
Validation loss decreased (0.692295 --> 0.691329).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.077746868133545
Epoch: 19, Steps: 63 | Train Loss: 0.3390533 Vali Loss: 0.6896713 Test Loss: 0.3731770
Validation loss decreased (0.691329 --> 0.689671).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.013033151626587
Epoch: 20, Steps: 63 | Train Loss: 0.3385003 Vali Loss: 0.6853889 Test Loss: 0.3731442
Validation loss decreased (0.689671 --> 0.685389).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.1239540576934814
Epoch: 21, Steps: 63 | Train Loss: 0.3381973 Vali Loss: 0.6849401 Test Loss: 0.3731393
Validation loss decreased (0.685389 --> 0.684940).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.139089584350586
Epoch: 22, Steps: 63 | Train Loss: 0.3383952 Vali Loss: 0.6891610 Test Loss: 0.3731196
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.1182608604431152
Epoch: 23, Steps: 63 | Train Loss: 0.3380779 Vali Loss: 0.6894373 Test Loss: 0.3731194
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.1495206356048584
Epoch: 24, Steps: 63 | Train Loss: 0.3381468 Vali Loss: 0.6817389 Test Loss: 0.3730221
Validation loss decreased (0.684940 --> 0.681739).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.14939546585083
Epoch: 25, Steps: 63 | Train Loss: 0.3383234 Vali Loss: 0.6817354 Test Loss: 0.3731053
Validation loss decreased (0.681739 --> 0.681735).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.129964828491211
Epoch: 26, Steps: 63 | Train Loss: 0.3382234 Vali Loss: 0.6830229 Test Loss: 0.3731245
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.1806330680847168
Epoch: 27, Steps: 63 | Train Loss: 0.3374707 Vali Loss: 0.6882241 Test Loss: 0.3731561
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.1664402484893799
Epoch: 28, Steps: 63 | Train Loss: 0.3381645 Vali Loss: 0.6840667 Test Loss: 0.3729864
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.1388170719146729
Epoch: 29, Steps: 63 | Train Loss: 0.3377934 Vali Loss: 0.6835299 Test Loss: 0.3730605
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.1033473014831543
Epoch: 30, Steps: 63 | Train Loss: 0.3370422 Vali Loss: 0.6822069 Test Loss: 0.3730362
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.137495517730713
Epoch: 31, Steps: 63 | Train Loss: 0.3378221 Vali Loss: 0.6836563 Test Loss: 0.3730649
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.1741092205047607
Epoch: 32, Steps: 63 | Train Loss: 0.3368489 Vali Loss: 0.6832717 Test Loss: 0.3731351
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.062718152999878
Epoch: 33, Steps: 63 | Train Loss: 0.3376194 Vali Loss: 0.6792086 Test Loss: 0.3730794
Validation loss decreased (0.681735 --> 0.679209).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.2393064498901367
Epoch: 34, Steps: 63 | Train Loss: 0.3374858 Vali Loss: 0.6791312 Test Loss: 0.3730343
Validation loss decreased (0.679209 --> 0.679131).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.1206376552581787
Epoch: 35, Steps: 63 | Train Loss: 0.3370926 Vali Loss: 0.6853178 Test Loss: 0.3730486
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.1458525657653809
Epoch: 36, Steps: 63 | Train Loss: 0.3373280 Vali Loss: 0.6830512 Test Loss: 0.3730724
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.1878173351287842
Epoch: 37, Steps: 63 | Train Loss: 0.3371521 Vali Loss: 0.6818370 Test Loss: 0.3730590
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.0678660869598389
Epoch: 38, Steps: 63 | Train Loss: 0.3375157 Vali Loss: 0.6814543 Test Loss: 0.3730908
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.1521050930023193
Epoch: 39, Steps: 63 | Train Loss: 0.3369633 Vali Loss: 0.6824757 Test Loss: 0.3730258
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.1943817138671875
Epoch: 40, Steps: 63 | Train Loss: 0.3375870 Vali Loss: 0.6833239 Test Loss: 0.3730957
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.1634249687194824
Epoch: 41, Steps: 63 | Train Loss: 0.3371268 Vali Loss: 0.6802627 Test Loss: 0.3730407
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.119544506072998
Epoch: 42, Steps: 63 | Train Loss: 0.3370917 Vali Loss: 0.6798013 Test Loss: 0.3729906
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.1162075996398926
Epoch: 43, Steps: 63 | Train Loss: 0.3368963 Vali Loss: 0.6799348 Test Loss: 0.3730600
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.0834829807281494
Epoch: 44, Steps: 63 | Train Loss: 0.3376986 Vali Loss: 0.6801831 Test Loss: 0.3730257
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.2233161926269531
Epoch: 45, Steps: 63 | Train Loss: 0.3372534 Vali Loss: 0.6806332 Test Loss: 0.3730363
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.2836990356445312
Epoch: 46, Steps: 63 | Train Loss: 0.3373461 Vali Loss: 0.6807870 Test Loss: 0.3730749
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.1385276317596436
Epoch: 47, Steps: 63 | Train Loss: 0.3370879 Vali Loss: 0.6805740 Test Loss: 0.3730290
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.0716688632965088
Epoch: 48, Steps: 63 | Train Loss: 0.3374921 Vali Loss: 0.6797341 Test Loss: 0.3730350
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.088451623916626
Epoch: 49, Steps: 63 | Train Loss: 0.3369547 Vali Loss: 0.6802943 Test Loss: 0.3730669
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.1952111721038818
Epoch: 50, Steps: 63 | Train Loss: 0.3364199 Vali Loss: 0.6814624 Test Loss: 0.3730493
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.102360725402832
Epoch: 51, Steps: 63 | Train Loss: 0.3369869 Vali Loss: 0.6749002 Test Loss: 0.3730467
Validation loss decreased (0.679131 --> 0.674900).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.2083511352539062
Epoch: 52, Steps: 63 | Train Loss: 0.3371431 Vali Loss: 0.6810988 Test Loss: 0.3730508
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.1290614604949951
Epoch: 53, Steps: 63 | Train Loss: 0.3371781 Vali Loss: 0.6782992 Test Loss: 0.3730454
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.172334909439087
Epoch: 54, Steps: 63 | Train Loss: 0.3372201 Vali Loss: 0.6803590 Test Loss: 0.3730376
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.0728037357330322
Epoch: 55, Steps: 63 | Train Loss: 0.3370117 Vali Loss: 0.6791272 Test Loss: 0.3730526
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.1446154117584229
Epoch: 56, Steps: 63 | Train Loss: 0.3372021 Vali Loss: 0.6801887 Test Loss: 0.3730220
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.2045626640319824
Epoch: 57, Steps: 63 | Train Loss: 0.3368776 Vali Loss: 0.6804153 Test Loss: 0.3730493
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.153320074081421
Epoch: 58, Steps: 63 | Train Loss: 0.3369202 Vali Loss: 0.6771709 Test Loss: 0.3730415
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.0742948055267334
Epoch: 59, Steps: 63 | Train Loss: 0.3370877 Vali Loss: 0.6822827 Test Loss: 0.3730308
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.1285545825958252
Epoch: 60, Steps: 63 | Train Loss: 0.3370050 Vali Loss: 0.6776115 Test Loss: 0.3730581
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.0963242053985596
Epoch: 61, Steps: 63 | Train Loss: 0.3368297 Vali Loss: 0.6791387 Test Loss: 0.3730707
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.110588550567627
Epoch: 62, Steps: 63 | Train Loss: 0.3367369 Vali Loss: 0.6817549 Test Loss: 0.3730531
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.1801679134368896
Epoch: 63, Steps: 63 | Train Loss: 0.3370930 Vali Loss: 0.6804458 Test Loss: 0.3730568
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.0702626705169678
Epoch: 64, Steps: 63 | Train Loss: 0.3365865 Vali Loss: 0.6807285 Test Loss: 0.3730721
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.1238486766815186
Epoch: 65, Steps: 63 | Train Loss: 0.3367710 Vali Loss: 0.6741952 Test Loss: 0.3730538
Validation loss decreased (0.674900 --> 0.674195).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.189324140548706
Epoch: 66, Steps: 63 | Train Loss: 0.3367974 Vali Loss: 0.6788157 Test Loss: 0.3730715
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.08933424949646
Epoch: 67, Steps: 63 | Train Loss: 0.3365814 Vali Loss: 0.6783549 Test Loss: 0.3730734
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.1216540336608887
Epoch: 68, Steps: 63 | Train Loss: 0.3365042 Vali Loss: 0.6800715 Test Loss: 0.3730583
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.1102960109710693
Epoch: 69, Steps: 63 | Train Loss: 0.3365995 Vali Loss: 0.6772113 Test Loss: 0.3730479
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.0401031970977783
Epoch: 70, Steps: 63 | Train Loss: 0.3368349 Vali Loss: 0.6817243 Test Loss: 0.3730499
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.0936918258666992
Epoch: 71, Steps: 63 | Train Loss: 0.3368514 Vali Loss: 0.6784343 Test Loss: 0.3730474
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.157428503036499
Epoch: 72, Steps: 63 | Train Loss: 0.3370270 Vali Loss: 0.6769164 Test Loss: 0.3730527
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.1765220165252686
Epoch: 73, Steps: 63 | Train Loss: 0.3365450 Vali Loss: 0.6789681 Test Loss: 0.3730564
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.1131634712219238
Epoch: 74, Steps: 63 | Train Loss: 0.3366197 Vali Loss: 0.6770495 Test Loss: 0.3730564
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.0998542308807373
Epoch: 75, Steps: 63 | Train Loss: 0.3367684 Vali Loss: 0.6813587 Test Loss: 0.3730458
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.0118722915649414
Epoch: 76, Steps: 63 | Train Loss: 0.3369138 Vali Loss: 0.6769394 Test Loss: 0.3730591
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.1387684345245361
Epoch: 77, Steps: 63 | Train Loss: 0.3370545 Vali Loss: 0.6797082 Test Loss: 0.3730455
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.149766445159912
Epoch: 78, Steps: 63 | Train Loss: 0.3368655 Vali Loss: 0.6778192 Test Loss: 0.3730601
EarlyStopping counter: 13 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.1584656238555908
Epoch: 79, Steps: 63 | Train Loss: 0.3366691 Vali Loss: 0.6777879 Test Loss: 0.3730512
EarlyStopping counter: 14 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.1393465995788574
Epoch: 80, Steps: 63 | Train Loss: 0.3364691 Vali Loss: 0.6783959 Test Loss: 0.3730578
EarlyStopping counter: 15 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.0380778312683105
Epoch: 81, Steps: 63 | Train Loss: 0.3365206 Vali Loss: 0.6777287 Test Loss: 0.3730583
EarlyStopping counter: 16 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.1407945156097412
Epoch: 82, Steps: 63 | Train Loss: 0.3367098 Vali Loss: 0.6817019 Test Loss: 0.3730552
EarlyStopping counter: 17 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.2023985385894775
Epoch: 83, Steps: 63 | Train Loss: 0.3367076 Vali Loss: 0.6752678 Test Loss: 0.3730554
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.0859589576721191
Epoch: 84, Steps: 63 | Train Loss: 0.3366321 Vali Loss: 0.6819854 Test Loss: 0.3730557
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.129194736480713
Epoch: 85, Steps: 63 | Train Loss: 0.3365015 Vali Loss: 0.6785839 Test Loss: 0.3730661
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_96_FITS_ETTh1_ftM_sl360_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.372101366519928, mae:0.3945726156234741, rse:0.5794132947921753, corr:[0.2690358  0.2773861  0.27781492 0.27962992 0.27717745 0.2748661
 0.27437112 0.2739489  0.27312317 0.2731352  0.27320826 0.2726251
 0.2723921  0.27241826 0.27217218 0.27220064 0.272426   0.2721204
 0.2717757  0.27172682 0.27158704 0.27123544 0.27146596 0.27202842
 0.2716771  0.27111465 0.27098444 0.27078205 0.2702506  0.269922
 0.26969492 0.2690353  0.2684621  0.26844305 0.26848477 0.26818985
 0.26819065 0.26844332 0.26848516 0.26848492 0.26888838 0.26911962
 0.26919058 0.26915723 0.26886886 0.26841995 0.26849124 0.26884648
 0.26850453 0.26743925 0.2665012  0.26599446 0.2652704  0.26410383
 0.26345974 0.26312587 0.26275772 0.2626378  0.2624275  0.2623492
 0.26236972 0.26255283 0.26256996 0.26248443 0.2626188  0.26303494
 0.263416   0.263325   0.2631281  0.26326707 0.263522   0.2634483
 0.2627367  0.26169795 0.26086122 0.2603787  0.26000935 0.25956115
 0.25919628 0.25870657 0.25823864 0.25789928 0.25763345 0.2574093
 0.25733492 0.25751054 0.2575973  0.25725606 0.25724927 0.25750098
 0.25689712 0.25613675 0.25606456 0.2551323  0.2552008  0.25769952]
