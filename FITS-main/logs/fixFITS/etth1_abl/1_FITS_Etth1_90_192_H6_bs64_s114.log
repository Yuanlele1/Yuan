Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_192_FITS_ETTh1_ftM_sl90_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8359
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=34, out_features=106, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3229184.0
params:  3710.0
Trainable parameters:  3710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.1560521125793457
Epoch: 1, Steps: 65 | Train Loss: 0.7830093 Vali Loss: 1.4567678 Test Loss: 0.7709936
Validation loss decreased (inf --> 1.456768).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.1638565063476562
Epoch: 2, Steps: 65 | Train Loss: 0.5936569 Vali Loss: 1.2460351 Test Loss: 0.5925865
Validation loss decreased (1.456768 --> 1.246035).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.0960588455200195
Epoch: 3, Steps: 65 | Train Loss: 0.5160951 Vali Loss: 1.1518260 Test Loss: 0.5184382
Validation loss decreased (1.246035 --> 1.151826).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.001699686050415
Epoch: 4, Steps: 65 | Train Loss: 0.4797010 Vali Loss: 1.1027997 Test Loss: 0.4829573
Validation loss decreased (1.151826 --> 1.102800).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.0608785152435303
Epoch: 5, Steps: 65 | Train Loss: 0.4607653 Vali Loss: 1.0744514 Test Loss: 0.4645205
Validation loss decreased (1.102800 --> 1.074451).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.059281587600708
Epoch: 6, Steps: 65 | Train Loss: 0.4494313 Vali Loss: 1.0571445 Test Loss: 0.4544223
Validation loss decreased (1.074451 --> 1.057145).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 0.9664790630340576
Epoch: 7, Steps: 65 | Train Loss: 0.4421825 Vali Loss: 1.0454224 Test Loss: 0.4484962
Validation loss decreased (1.057145 --> 1.045422).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.0895771980285645
Epoch: 8, Steps: 65 | Train Loss: 0.4378124 Vali Loss: 1.0376014 Test Loss: 0.4448356
Validation loss decreased (1.045422 --> 1.037601).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.1153607368469238
Epoch: 9, Steps: 65 | Train Loss: 0.4345067 Vali Loss: 1.0319299 Test Loss: 0.4423541
Validation loss decreased (1.037601 --> 1.031930).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.0382373332977295
Epoch: 10, Steps: 65 | Train Loss: 0.4322939 Vali Loss: 1.0271971 Test Loss: 0.4408148
Validation loss decreased (1.031930 --> 1.027197).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.044203281402588
Epoch: 11, Steps: 65 | Train Loss: 0.4303168 Vali Loss: 1.0236036 Test Loss: 0.4395766
Validation loss decreased (1.027197 --> 1.023604).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.110361099243164
Epoch: 12, Steps: 65 | Train Loss: 0.4287022 Vali Loss: 1.0213925 Test Loss: 0.4387101
Validation loss decreased (1.023604 --> 1.021392).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.1043713092803955
Epoch: 13, Steps: 65 | Train Loss: 0.4276818 Vali Loss: 1.0193561 Test Loss: 0.4381209
Validation loss decreased (1.021392 --> 1.019356).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.155651569366455
Epoch: 14, Steps: 65 | Train Loss: 0.4268212 Vali Loss: 1.0173717 Test Loss: 0.4377441
Validation loss decreased (1.019356 --> 1.017372).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.0242185592651367
Epoch: 15, Steps: 65 | Train Loss: 0.4262764 Vali Loss: 1.0159625 Test Loss: 0.4373964
Validation loss decreased (1.017372 --> 1.015962).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.0152246952056885
Epoch: 16, Steps: 65 | Train Loss: 0.4251961 Vali Loss: 1.0146046 Test Loss: 0.4370433
Validation loss decreased (1.015962 --> 1.014605).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.1045172214508057
Epoch: 17, Steps: 65 | Train Loss: 0.4251715 Vali Loss: 1.0136706 Test Loss: 0.4369212
Validation loss decreased (1.014605 --> 1.013671).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.1269822120666504
Epoch: 18, Steps: 65 | Train Loss: 0.4245610 Vali Loss: 1.0118914 Test Loss: 0.4368024
Validation loss decreased (1.013671 --> 1.011891).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.0356225967407227
Epoch: 19, Steps: 65 | Train Loss: 0.4237962 Vali Loss: 1.0120027 Test Loss: 0.4366958
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.151214838027954
Epoch: 20, Steps: 65 | Train Loss: 0.4235998 Vali Loss: 1.0108968 Test Loss: 0.4365526
Validation loss decreased (1.011891 --> 1.010897).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.1536672115325928
Epoch: 21, Steps: 65 | Train Loss: 0.4236490 Vali Loss: 1.0106264 Test Loss: 0.4365281
Validation loss decreased (1.010897 --> 1.010626).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.1325623989105225
Epoch: 22, Steps: 65 | Train Loss: 0.4231307 Vali Loss: 1.0100451 Test Loss: 0.4364235
Validation loss decreased (1.010626 --> 1.010045).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.0175983905792236
Epoch: 23, Steps: 65 | Train Loss: 0.4232114 Vali Loss: 1.0093797 Test Loss: 0.4364825
Validation loss decreased (1.010045 --> 1.009380).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 0.9672584533691406
Epoch: 24, Steps: 65 | Train Loss: 0.4230923 Vali Loss: 1.0091245 Test Loss: 0.4363537
Validation loss decreased (1.009380 --> 1.009125).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.0285992622375488
Epoch: 25, Steps: 65 | Train Loss: 0.4227931 Vali Loss: 1.0085535 Test Loss: 0.4363799
Validation loss decreased (1.009125 --> 1.008554).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.0920500755310059
Epoch: 26, Steps: 65 | Train Loss: 0.4226869 Vali Loss: 1.0084238 Test Loss: 0.4363739
Validation loss decreased (1.008554 --> 1.008424).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.0740745067596436
Epoch: 27, Steps: 65 | Train Loss: 0.4223075 Vali Loss: 1.0083171 Test Loss: 0.4363455
Validation loss decreased (1.008424 --> 1.008317).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.0505290031433105
Epoch: 28, Steps: 65 | Train Loss: 0.4223158 Vali Loss: 1.0080423 Test Loss: 0.4363017
Validation loss decreased (1.008317 --> 1.008042).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.1296732425689697
Epoch: 29, Steps: 65 | Train Loss: 0.4224768 Vali Loss: 1.0076765 Test Loss: 0.4362947
Validation loss decreased (1.008042 --> 1.007676).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.0686726570129395
Epoch: 30, Steps: 65 | Train Loss: 0.4220922 Vali Loss: 1.0073891 Test Loss: 0.4363246
Validation loss decreased (1.007676 --> 1.007389).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.154294490814209
Epoch: 31, Steps: 65 | Train Loss: 0.4221314 Vali Loss: 1.0070295 Test Loss: 0.4363509
Validation loss decreased (1.007389 --> 1.007030).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.085329294204712
Epoch: 32, Steps: 65 | Train Loss: 0.4218886 Vali Loss: 1.0066855 Test Loss: 0.4363628
Validation loss decreased (1.007030 --> 1.006685).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.0497522354125977
Epoch: 33, Steps: 65 | Train Loss: 0.4218028 Vali Loss: 1.0068966 Test Loss: 0.4363566
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.0737512111663818
Epoch: 34, Steps: 65 | Train Loss: 0.4219406 Vali Loss: 1.0066649 Test Loss: 0.4363583
Validation loss decreased (1.006685 --> 1.006665).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.0457372665405273
Epoch: 35, Steps: 65 | Train Loss: 0.4219079 Vali Loss: 1.0063217 Test Loss: 0.4363464
Validation loss decreased (1.006665 --> 1.006322).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.0983572006225586
Epoch: 36, Steps: 65 | Train Loss: 0.4216011 Vali Loss: 1.0059853 Test Loss: 0.4363600
Validation loss decreased (1.006322 --> 1.005985).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.2804365158081055
Epoch: 37, Steps: 65 | Train Loss: 0.4217778 Vali Loss: 1.0058267 Test Loss: 0.4363726
Validation loss decreased (1.005985 --> 1.005827).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.246781587600708
Epoch: 38, Steps: 65 | Train Loss: 0.4217936 Vali Loss: 1.0056483 Test Loss: 0.4363534
Validation loss decreased (1.005827 --> 1.005648).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.1115314960479736
Epoch: 39, Steps: 65 | Train Loss: 0.4217885 Vali Loss: 1.0061079 Test Loss: 0.4363779
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.1442155838012695
Epoch: 40, Steps: 65 | Train Loss: 0.4218132 Vali Loss: 1.0060503 Test Loss: 0.4363704
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.1769614219665527
Epoch: 41, Steps: 65 | Train Loss: 0.4213759 Vali Loss: 1.0059422 Test Loss: 0.4363724
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 0.9910404682159424
Epoch: 42, Steps: 65 | Train Loss: 0.4217836 Vali Loss: 1.0058209 Test Loss: 0.4363822
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.0690975189208984
Epoch: 43, Steps: 65 | Train Loss: 0.4215997 Vali Loss: 1.0057870 Test Loss: 0.4363828
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.0220794677734375
Epoch: 44, Steps: 65 | Train Loss: 0.4219431 Vali Loss: 1.0056971 Test Loss: 0.4364167
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.038088321685791
Epoch: 45, Steps: 65 | Train Loss: 0.4217170 Vali Loss: 1.0053111 Test Loss: 0.4364269
Validation loss decreased (1.005648 --> 1.005311).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.0529630184173584
Epoch: 46, Steps: 65 | Train Loss: 0.4214897 Vali Loss: 1.0054450 Test Loss: 0.4364338
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.0356478691101074
Epoch: 47, Steps: 65 | Train Loss: 0.4213769 Vali Loss: 1.0047001 Test Loss: 0.4364091
Validation loss decreased (1.005311 --> 1.004700).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.07989501953125
Epoch: 48, Steps: 65 | Train Loss: 0.4215677 Vali Loss: 1.0052624 Test Loss: 0.4364471
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.0387306213378906
Epoch: 49, Steps: 65 | Train Loss: 0.4215790 Vali Loss: 1.0049136 Test Loss: 0.4364411
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 0.9815473556518555
Epoch: 50, Steps: 65 | Train Loss: 0.4216260 Vali Loss: 1.0048236 Test Loss: 0.4364868
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 0.9302301406860352
Epoch: 51, Steps: 65 | Train Loss: 0.4212095 Vali Loss: 1.0051615 Test Loss: 0.4364529
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.0718343257904053
Epoch: 52, Steps: 65 | Train Loss: 0.4213159 Vali Loss: 1.0052249 Test Loss: 0.4364683
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.0911762714385986
Epoch: 53, Steps: 65 | Train Loss: 0.4215608 Vali Loss: 1.0052294 Test Loss: 0.4364624
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.0845942497253418
Epoch: 54, Steps: 65 | Train Loss: 0.4214296 Vali Loss: 1.0046952 Test Loss: 0.4364702
Validation loss decreased (1.004700 --> 1.004695).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.0437431335449219
Epoch: 55, Steps: 65 | Train Loss: 0.4211151 Vali Loss: 1.0051348 Test Loss: 0.4364679
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.0622410774230957
Epoch: 56, Steps: 65 | Train Loss: 0.4213306 Vali Loss: 1.0048741 Test Loss: 0.4364828
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.03379487991333
Epoch: 57, Steps: 65 | Train Loss: 0.4210676 Vali Loss: 1.0051084 Test Loss: 0.4364814
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.006382942199707
Epoch: 58, Steps: 65 | Train Loss: 0.4214433 Vali Loss: 1.0050825 Test Loss: 0.4364885
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.0708448886871338
Epoch: 59, Steps: 65 | Train Loss: 0.4215613 Vali Loss: 1.0050386 Test Loss: 0.4364725
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.068378210067749
Epoch: 60, Steps: 65 | Train Loss: 0.4210204 Vali Loss: 1.0047671 Test Loss: 0.4364920
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.1102790832519531
Epoch: 61, Steps: 65 | Train Loss: 0.4212563 Vali Loss: 1.0049437 Test Loss: 0.4364901
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 0.9681129455566406
Epoch: 62, Steps: 65 | Train Loss: 0.4210264 Vali Loss: 1.0041676 Test Loss: 0.4365029
Validation loss decreased (1.004695 --> 1.004168).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.1255855560302734
Epoch: 63, Steps: 65 | Train Loss: 0.4212635 Vali Loss: 1.0049500 Test Loss: 0.4364983
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.0631942749023438
Epoch: 64, Steps: 65 | Train Loss: 0.4212364 Vali Loss: 1.0044827 Test Loss: 0.4364947
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 0.9533441066741943
Epoch: 65, Steps: 65 | Train Loss: 0.4212781 Vali Loss: 1.0048454 Test Loss: 0.4365065
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.0605618953704834
Epoch: 66, Steps: 65 | Train Loss: 0.4212309 Vali Loss: 1.0048462 Test Loss: 0.4365018
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.1336300373077393
Epoch: 67, Steps: 65 | Train Loss: 0.4213433 Vali Loss: 1.0047793 Test Loss: 0.4365059
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 0.9938852787017822
Epoch: 68, Steps: 65 | Train Loss: 0.4213454 Vali Loss: 1.0045817 Test Loss: 0.4364980
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.0171103477478027
Epoch: 69, Steps: 65 | Train Loss: 0.4211535 Vali Loss: 1.0048094 Test Loss: 0.4365126
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.017967700958252
Epoch: 70, Steps: 65 | Train Loss: 0.4211131 Vali Loss: 1.0037371 Test Loss: 0.4365197
Validation loss decreased (1.004168 --> 1.003737).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.0624487400054932
Epoch: 71, Steps: 65 | Train Loss: 0.4212056 Vali Loss: 1.0047389 Test Loss: 0.4365179
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.0503735542297363
Epoch: 72, Steps: 65 | Train Loss: 0.4212507 Vali Loss: 1.0042956 Test Loss: 0.4365122
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.1117560863494873
Epoch: 73, Steps: 65 | Train Loss: 0.4211913 Vali Loss: 1.0036802 Test Loss: 0.4365189
Validation loss decreased (1.003737 --> 1.003680).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.1280550956726074
Epoch: 74, Steps: 65 | Train Loss: 0.4208685 Vali Loss: 1.0046196 Test Loss: 0.4365208
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 0.9912903308868408
Epoch: 75, Steps: 65 | Train Loss: 0.4211388 Vali Loss: 1.0041549 Test Loss: 0.4365138
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 0.9779973030090332
Epoch: 76, Steps: 65 | Train Loss: 0.4212989 Vali Loss: 1.0043093 Test Loss: 0.4365244
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.0408923625946045
Epoch: 77, Steps: 65 | Train Loss: 0.4211576 Vali Loss: 1.0047051 Test Loss: 0.4365195
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.0690948963165283
Epoch: 78, Steps: 65 | Train Loss: 0.4210172 Vali Loss: 1.0046813 Test Loss: 0.4365321
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 0.9781508445739746
Epoch: 79, Steps: 65 | Train Loss: 0.4210148 Vali Loss: 1.0044523 Test Loss: 0.4365249
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 0.9290075302124023
Epoch: 80, Steps: 65 | Train Loss: 0.4211631 Vali Loss: 1.0045973 Test Loss: 0.4365315
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.0438737869262695
Epoch: 81, Steps: 65 | Train Loss: 0.4214829 Vali Loss: 1.0043308 Test Loss: 0.4365318
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.2414608001708984
Epoch: 82, Steps: 65 | Train Loss: 0.4214064 Vali Loss: 1.0042791 Test Loss: 0.4365327
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.1344430446624756
Epoch: 83, Steps: 65 | Train Loss: 0.4211602 Vali Loss: 1.0045099 Test Loss: 0.4365375
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.0921761989593506
Epoch: 84, Steps: 65 | Train Loss: 0.4213528 Vali Loss: 1.0044017 Test Loss: 0.4365331
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.0130658149719238
Epoch: 85, Steps: 65 | Train Loss: 0.4212325 Vali Loss: 1.0040812 Test Loss: 0.4365371
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.0505285263061523
Epoch: 86, Steps: 65 | Train Loss: 0.4210862 Vali Loss: 1.0046190 Test Loss: 0.4365355
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.0607407093048096
Epoch: 87, Steps: 65 | Train Loss: 0.4209170 Vali Loss: 1.0045909 Test Loss: 0.4365365
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 0.9830539226531982
Epoch: 88, Steps: 65 | Train Loss: 0.4209960 Vali Loss: 1.0046033 Test Loss: 0.4365407
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.007026195526123
Epoch: 89, Steps: 65 | Train Loss: 0.4213681 Vali Loss: 1.0037717 Test Loss: 0.4365413
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.0818638801574707
Epoch: 90, Steps: 65 | Train Loss: 0.4213648 Vali Loss: 1.0042078 Test Loss: 0.4365415
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.2125964164733887
Epoch: 91, Steps: 65 | Train Loss: 0.4211981 Vali Loss: 1.0044158 Test Loss: 0.4365432
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.1502768993377686
Epoch: 92, Steps: 65 | Train Loss: 0.4213139 Vali Loss: 1.0045669 Test Loss: 0.4365422
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.066838264465332
Epoch: 93, Steps: 65 | Train Loss: 0.4210354 Vali Loss: 1.0038403 Test Loss: 0.4365444
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_192_FITS_ETTh1_ftM_sl90_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.43642252683639526, mae:0.42249664664268494, rse:0.6273514628410339, corr:[0.26325217 0.2649051  0.26411602 0.26478994 0.2614644  0.26046437
 0.2589323  0.25870344 0.2591279  0.25837618 0.25823542 0.25797892
 0.25727022 0.25714746 0.25718614 0.25689766 0.25715467 0.25754535
 0.25754791 0.25762346 0.25719592 0.25697377 0.256452   0.2561204
 0.25452363 0.25345448 0.25376654 0.2538299  0.2538213  0.25391385
 0.25383905 0.25401515 0.2538964  0.2533577  0.2534057  0.25343314
 0.2532632  0.25331497 0.25369766 0.25357205 0.2538546  0.25461686
 0.25474986 0.25496703 0.25495818 0.2546783  0.2545025  0.2540862
 0.25268295 0.25135556 0.25056884 0.24999642 0.24913767 0.24807401
 0.24781671 0.24764034 0.24751863 0.24741746 0.24720494 0.24722196
 0.24712202 0.24706218 0.24711221 0.24662137 0.24685062 0.24730505
 0.24744985 0.24779104 0.24758205 0.24722902 0.24696289 0.24601668
 0.24418263 0.24307603 0.24245614 0.24214615 0.24170475 0.24142736
 0.24165714 0.24164835 0.24153364 0.24122243 0.24111871 0.24091701
 0.24077381 0.24076772 0.24103348 0.24098308 0.24099399 0.24118893
 0.24125257 0.24145913 0.24125823 0.24123149 0.24124834 0.24077636
 0.23948547 0.23862341 0.23837662 0.23808567 0.23791035 0.23782952
 0.23798677 0.23801546 0.23796786 0.23780398 0.23775743 0.23764199
 0.23748504 0.23740605 0.23759505 0.23742509 0.2377569  0.23799925
 0.23795846 0.23825458 0.23820922 0.23808031 0.23785189 0.23674239
 0.23476785 0.23357755 0.2327449  0.23184861 0.2313772  0.23125532
 0.23189853 0.23233496 0.23241346 0.23222551 0.23209336 0.23193878
 0.23183313 0.23169634 0.23165877 0.23131154 0.23154926 0.23182468
 0.23178479 0.2320886  0.2319507  0.23181917 0.23175907 0.23084041
 0.22890773 0.2277281  0.22720715 0.22644944 0.22590885 0.22583982
 0.22661726 0.22709635 0.22719721 0.2270615  0.22721319 0.227123
 0.2268223  0.22675838 0.22651574 0.22601272 0.2261262  0.22623853
 0.22630155 0.2265352  0.22623853 0.22602789 0.22590216 0.22505197
 0.22317447 0.22197117 0.2219066  0.22191018 0.22132616 0.2210797
 0.22177996 0.22228181 0.22210993 0.22195531 0.22247247 0.22243316
 0.2223099  0.22193867 0.22126037 0.22084673 0.22063197 0.22033072
 0.22084874 0.22066852 0.2204796  0.22120923 0.22110997 0.22296268]
