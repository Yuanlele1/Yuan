Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=58, out_features=166, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8626688.0
params:  9794.0
Trainable parameters:  9794
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.2451748847961426
Epoch: 1, Steps: 63 | Train Loss: 0.8296900 Vali Loss: 1.6585364 Test Loss: 0.7357578
Validation loss decreased (inf --> 1.658536).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.1719698905944824
Epoch: 2, Steps: 63 | Train Loss: 0.6133720 Vali Loss: 1.4383866 Test Loss: 0.5749696
Validation loss decreased (1.658536 --> 1.438387).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2145195007324219
Epoch: 3, Steps: 63 | Train Loss: 0.5390338 Vali Loss: 1.3487066 Test Loss: 0.5127246
Validation loss decreased (1.438387 --> 1.348707).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.2219884395599365
Epoch: 4, Steps: 63 | Train Loss: 0.5083593 Vali Loss: 1.3071856 Test Loss: 0.4858391
Validation loss decreased (1.348707 --> 1.307186).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.2288601398468018
Epoch: 5, Steps: 63 | Train Loss: 0.4936806 Vali Loss: 1.2872112 Test Loss: 0.4728540
Validation loss decreased (1.307186 --> 1.287211).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.2238304615020752
Epoch: 6, Steps: 63 | Train Loss: 0.4856873 Vali Loss: 1.2790602 Test Loss: 0.4655437
Validation loss decreased (1.287211 --> 1.279060).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.306244134902954
Epoch: 7, Steps: 63 | Train Loss: 0.4806010 Vali Loss: 1.2629046 Test Loss: 0.4610370
Validation loss decreased (1.279060 --> 1.262905).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2695865631103516
Epoch: 8, Steps: 63 | Train Loss: 0.4767286 Vali Loss: 1.2601087 Test Loss: 0.4579799
Validation loss decreased (1.262905 --> 1.260109).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.2750635147094727
Epoch: 9, Steps: 63 | Train Loss: 0.4742402 Vali Loss: 1.2544347 Test Loss: 0.4558647
Validation loss decreased (1.260109 --> 1.254435).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.1047823429107666
Epoch: 10, Steps: 63 | Train Loss: 0.4727590 Vali Loss: 1.2479409 Test Loss: 0.4544602
Validation loss decreased (1.254435 --> 1.247941).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.2598662376403809
Epoch: 11, Steps: 63 | Train Loss: 0.4711855 Vali Loss: 1.2578713 Test Loss: 0.4534417
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.2065587043762207
Epoch: 12, Steps: 63 | Train Loss: 0.4702417 Vali Loss: 1.2469671 Test Loss: 0.4527216
Validation loss decreased (1.247941 --> 1.246967).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.2382044792175293
Epoch: 13, Steps: 63 | Train Loss: 0.4687568 Vali Loss: 1.2417520 Test Loss: 0.4522845
Validation loss decreased (1.246967 --> 1.241752).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.1920270919799805
Epoch: 14, Steps: 63 | Train Loss: 0.4682012 Vali Loss: 1.2456098 Test Loss: 0.4519929
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1354153156280518
Epoch: 15, Steps: 63 | Train Loss: 0.4677517 Vali Loss: 1.2446874 Test Loss: 0.4518037
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.197662591934204
Epoch: 16, Steps: 63 | Train Loss: 0.4670386 Vali Loss: 1.2460011 Test Loss: 0.4517412
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.192349910736084
Epoch: 17, Steps: 63 | Train Loss: 0.4668149 Vali Loss: 1.2446971 Test Loss: 0.4516736
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.218292236328125
Epoch: 18, Steps: 63 | Train Loss: 0.4661467 Vali Loss: 1.2382842 Test Loss: 0.4516998
Validation loss decreased (1.241752 --> 1.238284).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.1356582641601562
Epoch: 19, Steps: 63 | Train Loss: 0.4655035 Vali Loss: 1.2427024 Test Loss: 0.4516864
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.2012505531311035
Epoch: 20, Steps: 63 | Train Loss: 0.4659543 Vali Loss: 1.2444012 Test Loss: 0.4517329
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.1867585182189941
Epoch: 21, Steps: 63 | Train Loss: 0.4658447 Vali Loss: 1.2376237 Test Loss: 0.4518101
Validation loss decreased (1.238284 --> 1.237624).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2006325721740723
Epoch: 22, Steps: 63 | Train Loss: 0.4655227 Vali Loss: 1.2350067 Test Loss: 0.4518731
Validation loss decreased (1.237624 --> 1.235007).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.341627836227417
Epoch: 23, Steps: 63 | Train Loss: 0.4654093 Vali Loss: 1.2384067 Test Loss: 0.4519563
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4004576206207275
Epoch: 24, Steps: 63 | Train Loss: 0.4647625 Vali Loss: 1.2324839 Test Loss: 0.4520416
Validation loss decreased (1.235007 --> 1.232484).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.2704570293426514
Epoch: 25, Steps: 63 | Train Loss: 0.4649439 Vali Loss: 1.2447342 Test Loss: 0.4521199
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.227372646331787
Epoch: 26, Steps: 63 | Train Loss: 0.4650403 Vali Loss: 1.2366310 Test Loss: 0.4521690
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.219573974609375
Epoch: 27, Steps: 63 | Train Loss: 0.4649874 Vali Loss: 1.2364053 Test Loss: 0.4522537
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.2199921607971191
Epoch: 28, Steps: 63 | Train Loss: 0.4644443 Vali Loss: 1.2372782 Test Loss: 0.4523592
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.1268513202667236
Epoch: 29, Steps: 63 | Train Loss: 0.4644690 Vali Loss: 1.2376280 Test Loss: 0.4524383
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.0903494358062744
Epoch: 30, Steps: 63 | Train Loss: 0.4645096 Vali Loss: 1.2349730 Test Loss: 0.4524724
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.1834683418273926
Epoch: 31, Steps: 63 | Train Loss: 0.4644144 Vali Loss: 1.2354428 Test Loss: 0.4525294
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.2178876399993896
Epoch: 32, Steps: 63 | Train Loss: 0.4643646 Vali Loss: 1.2332792 Test Loss: 0.4526330
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.2260432243347168
Epoch: 33, Steps: 63 | Train Loss: 0.4643364 Vali Loss: 1.2361277 Test Loss: 0.4526808
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.264240026473999
Epoch: 34, Steps: 63 | Train Loss: 0.4641413 Vali Loss: 1.2324780 Test Loss: 0.4527162
Validation loss decreased (1.232484 --> 1.232478).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.1360118389129639
Epoch: 35, Steps: 63 | Train Loss: 0.4639722 Vali Loss: 1.2341480 Test Loss: 0.4527768
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3134934902191162
Epoch: 36, Steps: 63 | Train Loss: 0.4642783 Vali Loss: 1.2312633 Test Loss: 0.4528264
Validation loss decreased (1.232478 --> 1.231263).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.3187940120697021
Epoch: 37, Steps: 63 | Train Loss: 0.4642721 Vali Loss: 1.2343370 Test Loss: 0.4528787
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.214721441268921
Epoch: 38, Steps: 63 | Train Loss: 0.4641712 Vali Loss: 1.2343283 Test Loss: 0.4529129
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3373889923095703
Epoch: 39, Steps: 63 | Train Loss: 0.4639806 Vali Loss: 1.2375669 Test Loss: 0.4529606
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.2246065139770508
Epoch: 40, Steps: 63 | Train Loss: 0.4640430 Vali Loss: 1.2301962 Test Loss: 0.4529810
Validation loss decreased (1.231263 --> 1.230196).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.258025884628296
Epoch: 41, Steps: 63 | Train Loss: 0.4640176 Vali Loss: 1.2318577 Test Loss: 0.4530666
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.2510240077972412
Epoch: 42, Steps: 63 | Train Loss: 0.4645822 Vali Loss: 1.2264541 Test Loss: 0.4530869
Validation loss decreased (1.230196 --> 1.226454).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.1764986515045166
Epoch: 43, Steps: 63 | Train Loss: 0.4637655 Vali Loss: 1.2310231 Test Loss: 0.4531337
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.311427116394043
Epoch: 44, Steps: 63 | Train Loss: 0.4639251 Vali Loss: 1.2334900 Test Loss: 0.4531293
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.186561107635498
Epoch: 45, Steps: 63 | Train Loss: 0.4635613 Vali Loss: 1.2339902 Test Loss: 0.4531726
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.3151154518127441
Epoch: 46, Steps: 63 | Train Loss: 0.4638077 Vali Loss: 1.2320513 Test Loss: 0.4532126
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.2541403770446777
Epoch: 47, Steps: 63 | Train Loss: 0.4640003 Vali Loss: 1.2303441 Test Loss: 0.4532393
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.2170484066009521
Epoch: 48, Steps: 63 | Train Loss: 0.4633350 Vali Loss: 1.2324418 Test Loss: 0.4532692
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.243995189666748
Epoch: 49, Steps: 63 | Train Loss: 0.4634629 Vali Loss: 1.2349863 Test Loss: 0.4532870
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.3275127410888672
Epoch: 50, Steps: 63 | Train Loss: 0.4636930 Vali Loss: 1.2352800 Test Loss: 0.4533246
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.283008098602295
Epoch: 51, Steps: 63 | Train Loss: 0.4638370 Vali Loss: 1.2280945 Test Loss: 0.4533321
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3773579597473145
Epoch: 52, Steps: 63 | Train Loss: 0.4637454 Vali Loss: 1.2345117 Test Loss: 0.4533617
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6305265426635742
Epoch: 53, Steps: 63 | Train Loss: 0.4632198 Vali Loss: 1.2312140 Test Loss: 0.4533753
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.311288833618164
Epoch: 54, Steps: 63 | Train Loss: 0.4634912 Vali Loss: 1.2367697 Test Loss: 0.4533976
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.2832159996032715
Epoch: 55, Steps: 63 | Train Loss: 0.4638805 Vali Loss: 1.2332729 Test Loss: 0.4534227
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.3414013385772705
Epoch: 56, Steps: 63 | Train Loss: 0.4633146 Vali Loss: 1.2341639 Test Loss: 0.4534252
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.1939074993133545
Epoch: 57, Steps: 63 | Train Loss: 0.4631423 Vali Loss: 1.2324030 Test Loss: 0.4534500
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.1684072017669678
Epoch: 58, Steps: 63 | Train Loss: 0.4635929 Vali Loss: 1.2342367 Test Loss: 0.4534684
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.1261470317840576
Epoch: 59, Steps: 63 | Train Loss: 0.4630312 Vali Loss: 1.2315326 Test Loss: 0.4534883
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.31329345703125
Epoch: 60, Steps: 63 | Train Loss: 0.4633759 Vali Loss: 1.2385670 Test Loss: 0.4535050
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.3159089088439941
Epoch: 61, Steps: 63 | Train Loss: 0.4638324 Vali Loss: 1.2325600 Test Loss: 0.4535117
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.1932504177093506
Epoch: 62, Steps: 63 | Train Loss: 0.4633958 Vali Loss: 1.2276896 Test Loss: 0.4535304
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4523236155509949, mae:0.4332655370235443, rse:0.6402899622917175, corr:[0.25383103 0.25754178 0.25678736 0.25767973 0.2546997  0.2519931
 0.25232384 0.2519801  0.25107542 0.25217152 0.2514625  0.25049004
 0.25164244 0.251171   0.25005394 0.2505676  0.2508812  0.25049487
 0.25052062 0.25046062 0.2502627  0.25041538 0.250874   0.25081143
 0.24998231 0.24899976 0.24825773 0.24784228 0.247199   0.2466752
 0.24661516 0.2463675  0.24573916 0.24588026 0.24622115 0.24614835
 0.2465155  0.24692714 0.24671611 0.24669522 0.24725337 0.24743047
 0.247471   0.24771298 0.24777144 0.24809466 0.24887201 0.24903141
 0.24803509 0.24645649 0.24457598 0.24351099 0.24254352 0.24115814
 0.24044727 0.24041268 0.24011622 0.24024461 0.24044749 0.24083617
 0.24105182 0.24119191 0.24120815 0.24105458 0.24108669 0.24116403
 0.24131517 0.24122232 0.24087209 0.24087717 0.24105708 0.24047558
 0.23938969 0.23845962 0.23730777 0.23679408 0.23667093 0.23624736
 0.2359385  0.23588252 0.23573229 0.23547854 0.23513666 0.2352951
 0.23539631 0.2352641  0.23557632 0.23561329 0.23513386 0.23502079
 0.23520218 0.23505008 0.23469824 0.23510937 0.23589404 0.23609088
 0.23552608 0.23501514 0.23442632 0.23374069 0.23342398 0.23304139
 0.23269753 0.23286937 0.2330279  0.23327287 0.23316222 0.23333004
 0.23367913 0.23337875 0.23308088 0.2332311  0.23329023 0.23304552
 0.23306152 0.23313366 0.23302545 0.23314784 0.23329744 0.23287885
 0.23187664 0.2307282  0.22936422 0.22837307 0.22761996 0.2269637
 0.2266874  0.2270015  0.22709529 0.22736208 0.22758603 0.22801101
 0.22878893 0.22911367 0.22885972 0.228644   0.22895813 0.22901021
 0.22884226 0.22901289 0.2291181  0.22922502 0.22942369 0.2292143
 0.22830068 0.22724898 0.22626145 0.22521968 0.22431423 0.22368042
 0.22365509 0.22386593 0.22412427 0.22448996 0.22481516 0.22523911
 0.22542948 0.2254256  0.22555129 0.22532974 0.22510332 0.22514066
 0.22516602 0.22526067 0.22532164 0.22560653 0.2260198  0.22590986
 0.22490011 0.22384624 0.22307786 0.22263643 0.2220793  0.22139776
 0.22139522 0.2219877  0.22243105 0.22319    0.2238027  0.22456162
 0.22523808 0.22523668 0.22526175 0.2253626  0.22522984 0.225275
 0.22558457 0.22580916 0.225909   0.2262451  0.22662279 0.2263469
 0.22525544 0.22439912 0.223491   0.22262017 0.22192492 0.22137247
 0.22119945 0.2212663  0.22141889 0.22193597 0.22212614 0.22245115
 0.22302903 0.22310863 0.22270776 0.22244501 0.22241718 0.22222209
 0.22207433 0.22206335 0.22197244 0.22210506 0.22245227 0.22231826
 0.22159924 0.22099519 0.22041002 0.2198645  0.21938893 0.21874966
 0.21863577 0.21881922 0.21899761 0.2192545  0.21940975 0.21956202
 0.2199231  0.22019465 0.22019315 0.21990941 0.21990569 0.21990858
 0.21973337 0.21987474 0.22007458 0.22012766 0.22040285 0.22033575
 0.21937133 0.21841471 0.21778208 0.21732019 0.21682033 0.21648929
 0.21655573 0.21676785 0.21715117 0.21771725 0.2181282  0.21881595
 0.21941467 0.21954876 0.21983625 0.21985368 0.21972223 0.21994989
 0.2201172  0.21986745 0.21972662 0.21981756 0.22003323 0.21990564
 0.21920612 0.21843673 0.2176208  0.21726869 0.21681939 0.21603054
 0.21585561 0.21609654 0.21618482 0.21656767 0.21671404 0.21692151
 0.21770038 0.21773484 0.21743894 0.21736766 0.21716793 0.2169505
 0.21720144 0.21746233 0.21743672 0.21773869 0.21836773 0.2183411
 0.21759216 0.21716385 0.21693286 0.21675256 0.21648867 0.21630655
 0.21650559 0.21695893 0.21714674 0.2176227  0.2180505  0.21821888
 0.2188286  0.21924543 0.21911067 0.21894898 0.21919961 0.21912077
 0.21910457 0.21940133 0.21951325 0.21946275 0.21963471 0.21948196
 0.21875669 0.2181296  0.21743463 0.21689792 0.2165671  0.2154935
 0.21514173 0.21571115 0.21576354 0.21597296 0.21669614 0.21710439
 0.21768086 0.21864948 0.21896188 0.21835801 0.2186009  0.21902281
 0.21852279 0.21817338 0.21800552 0.21684782 0.21599351 0.21601959]
