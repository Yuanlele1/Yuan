Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=514, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=106, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  15386112.0
params:  17334.0
Trainable parameters:  17334
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.4450325965881348
Epoch: 1, Steps: 63 | Train Loss: 0.6883512 Vali Loss: 1.3681313 Test Loss: 0.6567736
Validation loss decreased (inf --> 1.368131).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3854029178619385
Epoch: 2, Steps: 63 | Train Loss: 0.5327194 Vali Loss: 1.1941221 Test Loss: 0.5477731
Validation loss decreased (1.368131 --> 1.194122).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.5057830810546875
Epoch: 3, Steps: 63 | Train Loss: 0.4788445 Vali Loss: 1.1036382 Test Loss: 0.4923247
Validation loss decreased (1.194122 --> 1.103638).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4627554416656494
Epoch: 4, Steps: 63 | Train Loss: 0.4485436 Vali Loss: 1.0478684 Test Loss: 0.4586436
Validation loss decreased (1.103638 --> 1.047868).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.490570306777954
Epoch: 5, Steps: 63 | Train Loss: 0.4300312 Vali Loss: 1.0115801 Test Loss: 0.4380780
Validation loss decreased (1.047868 --> 1.011580).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.4125027656555176
Epoch: 6, Steps: 63 | Train Loss: 0.4185761 Vali Loss: 0.9873000 Test Loss: 0.4258522
Validation loss decreased (1.011580 --> 0.987300).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5354247093200684
Epoch: 7, Steps: 63 | Train Loss: 0.4112865 Vali Loss: 0.9717262 Test Loss: 0.4186371
Validation loss decreased (0.987300 --> 0.971726).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.4450273513793945
Epoch: 8, Steps: 63 | Train Loss: 0.4064462 Vali Loss: 0.9611326 Test Loss: 0.4142405
Validation loss decreased (0.971726 --> 0.961133).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4030125141143799
Epoch: 9, Steps: 63 | Train Loss: 0.4034270 Vali Loss: 0.9542477 Test Loss: 0.4120415
Validation loss decreased (0.961133 --> 0.954248).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.4991779327392578
Epoch: 10, Steps: 63 | Train Loss: 0.4012959 Vali Loss: 0.9485243 Test Loss: 0.4105241
Validation loss decreased (0.954248 --> 0.948524).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.4114983081817627
Epoch: 11, Steps: 63 | Train Loss: 0.3997374 Vali Loss: 0.9447436 Test Loss: 0.4100048
Validation loss decreased (0.948524 --> 0.944744).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4565207958221436
Epoch: 12, Steps: 63 | Train Loss: 0.3985788 Vali Loss: 0.9408851 Test Loss: 0.4093469
Validation loss decreased (0.944744 --> 0.940885).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.451702356338501
Epoch: 13, Steps: 63 | Train Loss: 0.3979889 Vali Loss: 0.9393585 Test Loss: 0.4092884
Validation loss decreased (0.940885 --> 0.939358).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.4363768100738525
Epoch: 14, Steps: 63 | Train Loss: 0.3973533 Vali Loss: 0.9370424 Test Loss: 0.4092593
Validation loss decreased (0.939358 --> 0.937042).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.403700590133667
Epoch: 15, Steps: 63 | Train Loss: 0.3972161 Vali Loss: 0.9353415 Test Loss: 0.4090141
Validation loss decreased (0.937042 --> 0.935342).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.5119643211364746
Epoch: 16, Steps: 63 | Train Loss: 0.3967336 Vali Loss: 0.9341016 Test Loss: 0.4088831
Validation loss decreased (0.935342 --> 0.934102).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4188294410705566
Epoch: 17, Steps: 63 | Train Loss: 0.3964336 Vali Loss: 0.9328993 Test Loss: 0.4088304
Validation loss decreased (0.934102 --> 0.932899).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4441723823547363
Epoch: 18, Steps: 63 | Train Loss: 0.3958804 Vali Loss: 0.9319426 Test Loss: 0.4089127
Validation loss decreased (0.932899 --> 0.931943).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4663045406341553
Epoch: 19, Steps: 63 | Train Loss: 0.3957353 Vali Loss: 0.9309891 Test Loss: 0.4087924
Validation loss decreased (0.931943 --> 0.930989).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.465437889099121
Epoch: 20, Steps: 63 | Train Loss: 0.3956267 Vali Loss: 0.9300639 Test Loss: 0.4086311
Validation loss decreased (0.930989 --> 0.930064).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4635882377624512
Epoch: 21, Steps: 63 | Train Loss: 0.3954140 Vali Loss: 0.9292399 Test Loss: 0.4088085
Validation loss decreased (0.930064 --> 0.929240).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.4061400890350342
Epoch: 22, Steps: 63 | Train Loss: 0.3950844 Vali Loss: 0.9289252 Test Loss: 0.4088161
Validation loss decreased (0.929240 --> 0.928925).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5169527530670166
Epoch: 23, Steps: 63 | Train Loss: 0.3950407 Vali Loss: 0.9285625 Test Loss: 0.4089071
Validation loss decreased (0.928925 --> 0.928562).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4454498291015625
Epoch: 24, Steps: 63 | Train Loss: 0.3950063 Vali Loss: 0.9277854 Test Loss: 0.4087632
Validation loss decreased (0.928562 --> 0.927785).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.4408671855926514
Epoch: 25, Steps: 63 | Train Loss: 0.3947458 Vali Loss: 0.9274209 Test Loss: 0.4087133
Validation loss decreased (0.927785 --> 0.927421).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4155123233795166
Epoch: 26, Steps: 63 | Train Loss: 0.3947318 Vali Loss: 0.9270486 Test Loss: 0.4087196
Validation loss decreased (0.927421 --> 0.927049).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4098236560821533
Epoch: 27, Steps: 63 | Train Loss: 0.3948365 Vali Loss: 0.9262295 Test Loss: 0.4086472
Validation loss decreased (0.927049 --> 0.926229).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.4266364574432373
Epoch: 28, Steps: 63 | Train Loss: 0.3946408 Vali Loss: 0.9260152 Test Loss: 0.4086818
Validation loss decreased (0.926229 --> 0.926015).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.5485243797302246
Epoch: 29, Steps: 63 | Train Loss: 0.3943123 Vali Loss: 0.9255374 Test Loss: 0.4086328
Validation loss decreased (0.926015 --> 0.925537).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4334585666656494
Epoch: 30, Steps: 63 | Train Loss: 0.3940697 Vali Loss: 0.9251958 Test Loss: 0.4086388
Validation loss decreased (0.925537 --> 0.925196).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.391371726989746
Epoch: 31, Steps: 63 | Train Loss: 0.3940379 Vali Loss: 0.9250663 Test Loss: 0.4085881
Validation loss decreased (0.925196 --> 0.925066).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.4548511505126953
Epoch: 32, Steps: 63 | Train Loss: 0.3942928 Vali Loss: 0.9250932 Test Loss: 0.4085836
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.489530324935913
Epoch: 33, Steps: 63 | Train Loss: 0.3942021 Vali Loss: 0.9246844 Test Loss: 0.4085815
Validation loss decreased (0.925066 --> 0.924684).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.460249662399292
Epoch: 34, Steps: 63 | Train Loss: 0.3940635 Vali Loss: 0.9246778 Test Loss: 0.4086243
Validation loss decreased (0.924684 --> 0.924678).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.395195722579956
Epoch: 35, Steps: 63 | Train Loss: 0.3941572 Vali Loss: 0.9241647 Test Loss: 0.4086535
Validation loss decreased (0.924678 --> 0.924165).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3743243217468262
Epoch: 36, Steps: 63 | Train Loss: 0.3936957 Vali Loss: 0.9243776 Test Loss: 0.4085715
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.331470251083374
Epoch: 37, Steps: 63 | Train Loss: 0.3940029 Vali Loss: 0.9241568 Test Loss: 0.4085704
Validation loss decreased (0.924165 --> 0.924157).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.3528797626495361
Epoch: 38, Steps: 63 | Train Loss: 0.3938162 Vali Loss: 0.9234590 Test Loss: 0.4086337
Validation loss decreased (0.924157 --> 0.923459).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3903028964996338
Epoch: 39, Steps: 63 | Train Loss: 0.3938975 Vali Loss: 0.9232797 Test Loss: 0.4085538
Validation loss decreased (0.923459 --> 0.923280).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.47318696975708
Epoch: 40, Steps: 63 | Train Loss: 0.3938618 Vali Loss: 0.9236520 Test Loss: 0.4085562
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.4780023097991943
Epoch: 41, Steps: 63 | Train Loss: 0.3937332 Vali Loss: 0.9234827 Test Loss: 0.4085709
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.4470617771148682
Epoch: 42, Steps: 63 | Train Loss: 0.3935107 Vali Loss: 0.9232683 Test Loss: 0.4085270
Validation loss decreased (0.923280 --> 0.923268).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.4314382076263428
Epoch: 43, Steps: 63 | Train Loss: 0.3938650 Vali Loss: 0.9228753 Test Loss: 0.4085576
Validation loss decreased (0.923268 --> 0.922875).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.4777603149414062
Epoch: 44, Steps: 63 | Train Loss: 0.3935956 Vali Loss: 0.9231290 Test Loss: 0.4085241
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.4983735084533691
Epoch: 45, Steps: 63 | Train Loss: 0.3935371 Vali Loss: 0.9231302 Test Loss: 0.4085396
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.407369613647461
Epoch: 46, Steps: 63 | Train Loss: 0.3935478 Vali Loss: 0.9228038 Test Loss: 0.4085555
Validation loss decreased (0.922875 --> 0.922804).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.3684513568878174
Epoch: 47, Steps: 63 | Train Loss: 0.3937760 Vali Loss: 0.9229153 Test Loss: 0.4085001
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.3981420993804932
Epoch: 48, Steps: 63 | Train Loss: 0.3932545 Vali Loss: 0.9227070 Test Loss: 0.4085000
Validation loss decreased (0.922804 --> 0.922707).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.4826383590698242
Epoch: 49, Steps: 63 | Train Loss: 0.3934151 Vali Loss: 0.9227332 Test Loss: 0.4084987
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.3568625450134277
Epoch: 50, Steps: 63 | Train Loss: 0.3934233 Vali Loss: 0.9224923 Test Loss: 0.4085303
Validation loss decreased (0.922707 --> 0.922492).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.5209615230560303
Epoch: 51, Steps: 63 | Train Loss: 0.3936103 Vali Loss: 0.9224772 Test Loss: 0.4085309
Validation loss decreased (0.922492 --> 0.922477).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.476618766784668
Epoch: 52, Steps: 63 | Train Loss: 0.3933628 Vali Loss: 0.9225379 Test Loss: 0.4085241
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.4694843292236328
Epoch: 53, Steps: 63 | Train Loss: 0.3937110 Vali Loss: 0.9222705 Test Loss: 0.4085352
Validation loss decreased (0.922477 --> 0.922271).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.3212602138519287
Epoch: 54, Steps: 63 | Train Loss: 0.3933077 Vali Loss: 0.9219569 Test Loss: 0.4084963
Validation loss decreased (0.922271 --> 0.921957).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.440514087677002
Epoch: 55, Steps: 63 | Train Loss: 0.3933328 Vali Loss: 0.9220796 Test Loss: 0.4085174
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.406510353088379
Epoch: 56, Steps: 63 | Train Loss: 0.3932959 Vali Loss: 0.9222043 Test Loss: 0.4085251
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.4356844425201416
Epoch: 57, Steps: 63 | Train Loss: 0.3933857 Vali Loss: 0.9220038 Test Loss: 0.4085087
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.5299248695373535
Epoch: 58, Steps: 63 | Train Loss: 0.3933027 Vali Loss: 0.9220046 Test Loss: 0.4085258
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.4274044036865234
Epoch: 59, Steps: 63 | Train Loss: 0.3933392 Vali Loss: 0.9215735 Test Loss: 0.4085032
Validation loss decreased (0.921957 --> 0.921574).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.524463415145874
Epoch: 60, Steps: 63 | Train Loss: 0.3932574 Vali Loss: 0.9218961 Test Loss: 0.4085260
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.4852838516235352
Epoch: 61, Steps: 63 | Train Loss: 0.3933026 Vali Loss: 0.9220396 Test Loss: 0.4085100
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.436594009399414
Epoch: 62, Steps: 63 | Train Loss: 0.3929930 Vali Loss: 0.9219713 Test Loss: 0.4085162
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.4322526454925537
Epoch: 63, Steps: 63 | Train Loss: 0.3930973 Vali Loss: 0.9219970 Test Loss: 0.4085193
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.8411839008331299
Epoch: 64, Steps: 63 | Train Loss: 0.3933731 Vali Loss: 0.9219863 Test Loss: 0.4085310
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.5699715614318848
Epoch: 65, Steps: 63 | Train Loss: 0.3933260 Vali Loss: 0.9214666 Test Loss: 0.4085111
Validation loss decreased (0.921574 --> 0.921467).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.432478427886963
Epoch: 66, Steps: 63 | Train Loss: 0.3931208 Vali Loss: 0.9217822 Test Loss: 0.4085075
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.4420785903930664
Epoch: 67, Steps: 63 | Train Loss: 0.3931433 Vali Loss: 0.9216548 Test Loss: 0.4085233
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.565307378768921
Epoch: 68, Steps: 63 | Train Loss: 0.3932555 Vali Loss: 0.9217756 Test Loss: 0.4084982
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.5733282566070557
Epoch: 69, Steps: 63 | Train Loss: 0.3932512 Vali Loss: 0.9213300 Test Loss: 0.4085278
Validation loss decreased (0.921467 --> 0.921330).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5916190147399902
Epoch: 70, Steps: 63 | Train Loss: 0.3932165 Vali Loss: 0.9217739 Test Loss: 0.4085050
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.5226662158966064
Epoch: 71, Steps: 63 | Train Loss: 0.3933373 Vali Loss: 0.9215059 Test Loss: 0.4085098
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.5392367839813232
Epoch: 72, Steps: 63 | Train Loss: 0.3929634 Vali Loss: 0.9217130 Test Loss: 0.4085122
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.5374693870544434
Epoch: 73, Steps: 63 | Train Loss: 0.3930787 Vali Loss: 0.9216513 Test Loss: 0.4085043
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.4015140533447266
Epoch: 74, Steps: 63 | Train Loss: 0.3932577 Vali Loss: 0.9216588 Test Loss: 0.4085010
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.360050916671753
Epoch: 75, Steps: 63 | Train Loss: 0.3931486 Vali Loss: 0.9211249 Test Loss: 0.4084954
Validation loss decreased (0.921330 --> 0.921125).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.388622522354126
Epoch: 76, Steps: 63 | Train Loss: 0.3932623 Vali Loss: 0.9213472 Test Loss: 0.4085090
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.5131289958953857
Epoch: 77, Steps: 63 | Train Loss: 0.3930700 Vali Loss: 0.9216638 Test Loss: 0.4085110
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.495584487915039
Epoch: 78, Steps: 63 | Train Loss: 0.3934805 Vali Loss: 0.9215958 Test Loss: 0.4085025
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.521618366241455
Epoch: 79, Steps: 63 | Train Loss: 0.3931321 Vali Loss: 0.9216926 Test Loss: 0.4085110
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.5015950202941895
Epoch: 80, Steps: 63 | Train Loss: 0.3930820 Vali Loss: 0.9215468 Test Loss: 0.4084996
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.4451425075531006
Epoch: 81, Steps: 63 | Train Loss: 0.3933058 Vali Loss: 0.9215530 Test Loss: 0.4085079
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.5767316818237305
Epoch: 82, Steps: 63 | Train Loss: 0.3933625 Vali Loss: 0.9211925 Test Loss: 0.4085063
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.411015272140503
Epoch: 83, Steps: 63 | Train Loss: 0.3934353 Vali Loss: 0.9210743 Test Loss: 0.4085050
Validation loss decreased (0.921125 --> 0.921074).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.5375862121582031
Epoch: 84, Steps: 63 | Train Loss: 0.3930327 Vali Loss: 0.9210895 Test Loss: 0.4085014
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.4951868057250977
Epoch: 85, Steps: 63 | Train Loss: 0.3932194 Vali Loss: 0.9204125 Test Loss: 0.4085056
Validation loss decreased (0.921074 --> 0.920413).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.4020054340362549
Epoch: 86, Steps: 63 | Train Loss: 0.3931075 Vali Loss: 0.9211627 Test Loss: 0.4085014
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.4070420265197754
Epoch: 87, Steps: 63 | Train Loss: 0.3933177 Vali Loss: 0.9210164 Test Loss: 0.4085023
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.5179288387298584
Epoch: 88, Steps: 63 | Train Loss: 0.3931564 Vali Loss: 0.9213097 Test Loss: 0.4085094
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.4521703720092773
Epoch: 89, Steps: 63 | Train Loss: 0.3931440 Vali Loss: 0.9209207 Test Loss: 0.4085090
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.3490595817565918
Epoch: 90, Steps: 63 | Train Loss: 0.3932716 Vali Loss: 0.9211043 Test Loss: 0.4085100
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.5192954540252686
Epoch: 91, Steps: 63 | Train Loss: 0.3930822 Vali Loss: 0.9212138 Test Loss: 0.4085132
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.501967430114746
Epoch: 92, Steps: 63 | Train Loss: 0.3929872 Vali Loss: 0.9213997 Test Loss: 0.4085073
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.537888765335083
Epoch: 93, Steps: 63 | Train Loss: 0.3930404 Vali Loss: 0.9211825 Test Loss: 0.4085119
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.5040113925933838
Epoch: 94, Steps: 63 | Train Loss: 0.3930369 Vali Loss: 0.9214746 Test Loss: 0.4085140
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.3590915203094482
Epoch: 95, Steps: 63 | Train Loss: 0.3931405 Vali Loss: 0.9212748 Test Loss: 0.4085087
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.5442650318145752
Epoch: 96, Steps: 63 | Train Loss: 0.3932835 Vali Loss: 0.9215062 Test Loss: 0.4085130
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.6976137161254883
Epoch: 97, Steps: 63 | Train Loss: 0.3932950 Vali Loss: 0.9213792 Test Loss: 0.4085114
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.6483113765716553
Epoch: 98, Steps: 63 | Train Loss: 0.3932416 Vali Loss: 0.9214483 Test Loss: 0.4085118
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.6095576286315918
Epoch: 99, Steps: 63 | Train Loss: 0.3931978 Vali Loss: 0.9209697 Test Loss: 0.4085099
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.5140695571899414
Epoch: 100, Steps: 63 | Train Loss: 0.3930475 Vali Loss: 0.9215142 Test Loss: 0.4085120
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.40436452627182007, mae:0.41309213638305664, rse:0.6038705706596375, corr:[0.2610941  0.26921517 0.269461   0.27119064 0.26834208 0.26625708
 0.2662666  0.2657454  0.2648555  0.26524517 0.26535258 0.26448604
 0.26426038 0.26441136 0.26395082 0.26371294 0.2639647  0.26377758
 0.26335388 0.2632227  0.2631338  0.26287714 0.2631285  0.26351655
 0.26311326 0.26284215 0.26286986 0.2625225  0.2620475  0.26198918
 0.2617844  0.26106122 0.26068708 0.26090354 0.26088977 0.26061305
 0.26080534 0.2611355  0.26101586 0.26099488 0.26140344 0.2614927
 0.26135287 0.26136488 0.2612129  0.2607579  0.26070812 0.2609733
 0.2606129  0.25962636 0.258763   0.25833035 0.257758   0.25673783
 0.25608942 0.2557672  0.2554346  0.2551943  0.25495836 0.25488237
 0.25478378 0.25467682 0.25454378 0.2545135  0.25466546 0.25499213
 0.25530884 0.2552305  0.25508133 0.25517285 0.25526652 0.25508788
 0.25455746 0.25375682 0.25307375 0.25280544 0.25269997 0.25236416
 0.25191706 0.2514769  0.25110495 0.2507134  0.25045544 0.25038072
 0.25027505 0.25014675 0.2502461  0.25039726 0.25037032 0.2503459
 0.25028667 0.2499699  0.24957141 0.2494283  0.2495483  0.2500916
 0.25075534 0.25083745 0.2509264  0.25106478 0.25101563 0.250763
 0.2505765  0.25061482 0.25049067 0.2502392  0.25006565 0.24986528
 0.24955425 0.2494309  0.24963251 0.24999383 0.2502892  0.2505261
 0.25053105 0.25028047 0.25002974 0.24982177 0.24950302 0.24943157
 0.24953812 0.2490471  0.24831623 0.24782519 0.24734569 0.24665664
 0.24643995 0.24652864 0.2461751  0.24563985 0.24552801 0.24538054
 0.24497789 0.24503234 0.24532306 0.24514693 0.24532962 0.24579711
 0.2459489  0.24580567 0.24584822 0.24587694 0.24549778 0.24533308
 0.24542871 0.24489309 0.24440044 0.24403532 0.24324141 0.24223264
 0.24216025 0.2423918  0.24204208 0.24210027 0.24251552 0.24239643
 0.24191229 0.24217226 0.24245241 0.2420689  0.2420205  0.24249227
 0.24247234 0.24212626 0.24225293 0.24215768 0.24159677 0.24163519
 0.24199179 0.24177425 0.24198198 0.24258451 0.24229749 0.2416313
 0.24177507 0.24193032 0.24162191 0.24199937 0.2424275  0.24199049
 0.24191633 0.24245188 0.24228017 0.24195416 0.24255086 0.2425346
 0.24174802 0.2425215  0.24248031 0.23992793 0.24147306 0.23857549]
