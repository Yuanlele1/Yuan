Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=18, out_features=85, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1370880.0
params:  1615.0
Trainable parameters:  1615
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.5743846893310547
Epoch: 1, Steps: 64 | Train Loss: 1.0220891 Vali Loss: 2.2830770 Test Loss: 1.2225089
Validation loss decreased (inf --> 2.283077).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.120832920074463
Epoch: 2, Steps: 64 | Train Loss: 0.8040954 Vali Loss: 1.9392409 Test Loss: 0.9519119
Validation loss decreased (2.283077 --> 1.939241).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.7408056259155273
Epoch: 3, Steps: 64 | Train Loss: 0.6708979 Vali Loss: 1.7365128 Test Loss: 0.7944098
Validation loss decreased (1.939241 --> 1.736513).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.1836628913879395
Epoch: 4, Steps: 64 | Train Loss: 0.5877425 Vali Loss: 1.6089805 Test Loss: 0.6976807
Validation loss decreased (1.736513 --> 1.608981).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.410672903060913
Epoch: 5, Steps: 64 | Train Loss: 0.5339577 Vali Loss: 1.5334677 Test Loss: 0.6358432
Validation loss decreased (1.608981 --> 1.533468).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.663174629211426
Epoch: 6, Steps: 64 | Train Loss: 0.4983307 Vali Loss: 1.4689709 Test Loss: 0.5948223
Validation loss decreased (1.533468 --> 1.468971).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.952545166015625
Epoch: 7, Steps: 64 | Train Loss: 0.4735937 Vali Loss: 1.4345188 Test Loss: 0.5667912
Validation loss decreased (1.468971 --> 1.434519).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.034210205078125
Epoch: 8, Steps: 64 | Train Loss: 0.4562623 Vali Loss: 1.4128158 Test Loss: 0.5470679
Validation loss decreased (1.434519 --> 1.412816).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4110445976257324
Epoch: 9, Steps: 64 | Train Loss: 0.4441099 Vali Loss: 1.3924314 Test Loss: 0.5330245
Validation loss decreased (1.412816 --> 1.392431).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.699772834777832
Epoch: 10, Steps: 64 | Train Loss: 0.4349008 Vali Loss: 1.3764857 Test Loss: 0.5225961
Validation loss decreased (1.392431 --> 1.376486).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.7222471237182617
Epoch: 11, Steps: 64 | Train Loss: 0.4282480 Vali Loss: 1.3599643 Test Loss: 0.5148010
Validation loss decreased (1.376486 --> 1.359964).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.5695412158966064
Epoch: 12, Steps: 64 | Train Loss: 0.4230632 Vali Loss: 1.3540177 Test Loss: 0.5089426
Validation loss decreased (1.359964 --> 1.354018).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.546074867248535
Epoch: 13, Steps: 64 | Train Loss: 0.4189089 Vali Loss: 1.3459558 Test Loss: 0.5044405
Validation loss decreased (1.354018 --> 1.345956).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.012127637863159
Epoch: 14, Steps: 64 | Train Loss: 0.4156965 Vali Loss: 1.3402549 Test Loss: 0.5009255
Validation loss decreased (1.345956 --> 1.340255).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.817699670791626
Epoch: 15, Steps: 64 | Train Loss: 0.4131183 Vali Loss: 1.3351921 Test Loss: 0.4980997
Validation loss decreased (1.340255 --> 1.335192).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4741816520690918
Epoch: 16, Steps: 64 | Train Loss: 0.4109522 Vali Loss: 1.3337671 Test Loss: 0.4958816
Validation loss decreased (1.335192 --> 1.333767).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.801915168762207
Epoch: 17, Steps: 64 | Train Loss: 0.4091892 Vali Loss: 1.3328606 Test Loss: 0.4940448
Validation loss decreased (1.333767 --> 1.332861).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.328130483627319
Epoch: 18, Steps: 64 | Train Loss: 0.4076772 Vali Loss: 1.3227305 Test Loss: 0.4925299
Validation loss decreased (1.332861 --> 1.322731).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.728261709213257
Epoch: 19, Steps: 64 | Train Loss: 0.4063253 Vali Loss: 1.3238517 Test Loss: 0.4912936
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.8225550651550293
Epoch: 20, Steps: 64 | Train Loss: 0.4052404 Vali Loss: 1.3240376 Test Loss: 0.4902856
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.93312406539917
Epoch: 21, Steps: 64 | Train Loss: 0.4045339 Vali Loss: 1.3214718 Test Loss: 0.4894120
Validation loss decreased (1.322731 --> 1.321472).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.375239610671997
Epoch: 22, Steps: 64 | Train Loss: 0.4036294 Vali Loss: 1.3182005 Test Loss: 0.4886839
Validation loss decreased (1.321472 --> 1.318200).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.4002621173858643
Epoch: 23, Steps: 64 | Train Loss: 0.4028351 Vali Loss: 1.3206644 Test Loss: 0.4880578
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.1856718063354492
Epoch: 24, Steps: 64 | Train Loss: 0.4021503 Vali Loss: 1.3168710 Test Loss: 0.4875606
Validation loss decreased (1.318200 --> 1.316871).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.3229963779449463
Epoch: 25, Steps: 64 | Train Loss: 0.4014487 Vali Loss: 1.3127171 Test Loss: 0.4871058
Validation loss decreased (1.316871 --> 1.312717).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.1073963642120361
Epoch: 26, Steps: 64 | Train Loss: 0.4009942 Vali Loss: 1.3147006 Test Loss: 0.4867011
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.0855870246887207
Epoch: 27, Steps: 64 | Train Loss: 0.4006606 Vali Loss: 1.3126829 Test Loss: 0.4863718
Validation loss decreased (1.312717 --> 1.312683).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.0269701480865479
Epoch: 28, Steps: 64 | Train Loss: 0.4004375 Vali Loss: 1.3163862 Test Loss: 0.4860496
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.1579031944274902
Epoch: 29, Steps: 64 | Train Loss: 0.3999492 Vali Loss: 1.3080053 Test Loss: 0.4857965
Validation loss decreased (1.312683 --> 1.308005).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 0.9865550994873047
Epoch: 30, Steps: 64 | Train Loss: 0.3995088 Vali Loss: 1.3080571 Test Loss: 0.4855557
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.0601770877838135
Epoch: 31, Steps: 64 | Train Loss: 0.3994677 Vali Loss: 1.3083785 Test Loss: 0.4853609
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.1768286228179932
Epoch: 32, Steps: 64 | Train Loss: 0.3988025 Vali Loss: 1.3105636 Test Loss: 0.4852105
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.1392302513122559
Epoch: 33, Steps: 64 | Train Loss: 0.3985829 Vali Loss: 1.3140421 Test Loss: 0.4850479
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.1882801055908203
Epoch: 34, Steps: 64 | Train Loss: 0.3985808 Vali Loss: 1.3089399 Test Loss: 0.4848856
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.0563313961029053
Epoch: 35, Steps: 64 | Train Loss: 0.3983207 Vali Loss: 1.3049058 Test Loss: 0.4847802
Validation loss decreased (1.308005 --> 1.304906).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.0489752292633057
Epoch: 36, Steps: 64 | Train Loss: 0.3977679 Vali Loss: 1.3075811 Test Loss: 0.4846575
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.1695637702941895
Epoch: 37, Steps: 64 | Train Loss: 0.3980075 Vali Loss: 1.3066943 Test Loss: 0.4845684
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.1474518775939941
Epoch: 38, Steps: 64 | Train Loss: 0.3977901 Vali Loss: 1.3035848 Test Loss: 0.4844963
Validation loss decreased (1.304906 --> 1.303585).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.0056021213531494
Epoch: 39, Steps: 64 | Train Loss: 0.3976323 Vali Loss: 1.3096164 Test Loss: 0.4843960
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.0932543277740479
Epoch: 40, Steps: 64 | Train Loss: 0.3974536 Vali Loss: 1.3084030 Test Loss: 0.4843256
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.0874104499816895
Epoch: 41, Steps: 64 | Train Loss: 0.3971783 Vali Loss: 1.3040507 Test Loss: 0.4842612
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3512156009674072
Epoch: 42, Steps: 64 | Train Loss: 0.3971915 Vali Loss: 1.3050674 Test Loss: 0.4842075
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.1994500160217285
Epoch: 43, Steps: 64 | Train Loss: 0.3970614 Vali Loss: 1.3020020 Test Loss: 0.4841524
Validation loss decreased (1.303585 --> 1.302002).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 0.9452428817749023
Epoch: 44, Steps: 64 | Train Loss: 0.3970873 Vali Loss: 1.3017491 Test Loss: 0.4841130
Validation loss decreased (1.302002 --> 1.301749).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.1570982933044434
Epoch: 45, Steps: 64 | Train Loss: 0.3967704 Vali Loss: 1.3078963 Test Loss: 0.4840701
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.002467155456543
Epoch: 46, Steps: 64 | Train Loss: 0.3968451 Vali Loss: 1.3091202 Test Loss: 0.4840390
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 0.9647204875946045
Epoch: 47, Steps: 64 | Train Loss: 0.3967656 Vali Loss: 1.3096176 Test Loss: 0.4839967
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.088864803314209
Epoch: 48, Steps: 64 | Train Loss: 0.3967216 Vali Loss: 1.3022039 Test Loss: 0.4839830
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.094113826751709
Epoch: 49, Steps: 64 | Train Loss: 0.3965324 Vali Loss: 1.3039445 Test Loss: 0.4839559
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.182570457458496
Epoch: 50, Steps: 64 | Train Loss: 0.3964943 Vali Loss: 1.3031065 Test Loss: 0.4839302
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.1378068923950195
Epoch: 51, Steps: 64 | Train Loss: 0.3962794 Vali Loss: 1.3064343 Test Loss: 0.4839054
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.0131688117980957
Epoch: 52, Steps: 64 | Train Loss: 0.3963819 Vali Loss: 1.3034471 Test Loss: 0.4838785
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.1282668113708496
Epoch: 53, Steps: 64 | Train Loss: 0.3963171 Vali Loss: 1.3046799 Test Loss: 0.4838642
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.067739486694336
Epoch: 54, Steps: 64 | Train Loss: 0.3961918 Vali Loss: 1.3052649 Test Loss: 0.4838581
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.138848066329956
Epoch: 55, Steps: 64 | Train Loss: 0.3961632 Vali Loss: 1.3060788 Test Loss: 0.4838362
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.116457462310791
Epoch: 56, Steps: 64 | Train Loss: 0.3961718 Vali Loss: 1.3001398 Test Loss: 0.4838341
Validation loss decreased (1.301749 --> 1.300140).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.171567440032959
Epoch: 57, Steps: 64 | Train Loss: 0.3959632 Vali Loss: 1.3047407 Test Loss: 0.4838111
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.1234397888183594
Epoch: 58, Steps: 64 | Train Loss: 0.3962328 Vali Loss: 1.3033630 Test Loss: 0.4838079
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.0893518924713135
Epoch: 59, Steps: 64 | Train Loss: 0.3960672 Vali Loss: 1.3036821 Test Loss: 0.4838098
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.0551643371582031
Epoch: 60, Steps: 64 | Train Loss: 0.3957865 Vali Loss: 1.3004347 Test Loss: 0.4837867
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.086867332458496
Epoch: 61, Steps: 64 | Train Loss: 0.3958089 Vali Loss: 1.3016156 Test Loss: 0.4837838
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.053894281387329
Epoch: 62, Steps: 64 | Train Loss: 0.3959624 Vali Loss: 1.3010709 Test Loss: 0.4837810
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.1005299091339111
Epoch: 63, Steps: 64 | Train Loss: 0.3958616 Vali Loss: 1.3033177 Test Loss: 0.4837750
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.081491470336914
Epoch: 64, Steps: 64 | Train Loss: 0.3957566 Vali Loss: 1.3038315 Test Loss: 0.4837748
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.098757266998291
Epoch: 65, Steps: 64 | Train Loss: 0.3957910 Vali Loss: 1.3061051 Test Loss: 0.4837688
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.1032142639160156
Epoch: 66, Steps: 64 | Train Loss: 0.3956506 Vali Loss: 1.3064682 Test Loss: 0.4837622
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.110630750656128
Epoch: 67, Steps: 64 | Train Loss: 0.3955650 Vali Loss: 1.3041426 Test Loss: 0.4837570
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.1341004371643066
Epoch: 68, Steps: 64 | Train Loss: 0.3957493 Vali Loss: 1.2977028 Test Loss: 0.4837535
Validation loss decreased (1.300140 --> 1.297703).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.1760952472686768
Epoch: 69, Steps: 64 | Train Loss: 0.3954988 Vali Loss: 1.2969427 Test Loss: 0.4837571
Validation loss decreased (1.297703 --> 1.296943).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 3.1964008808135986
Epoch: 70, Steps: 64 | Train Loss: 0.3957132 Vali Loss: 1.3012432 Test Loss: 0.4837562
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.0970656871795654
Epoch: 71, Steps: 64 | Train Loss: 0.3956595 Vali Loss: 1.3018261 Test Loss: 0.4837507
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.1139256954193115
Epoch: 72, Steps: 64 | Train Loss: 0.3955335 Vali Loss: 1.3029152 Test Loss: 0.4837456
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.0680458545684814
Epoch: 73, Steps: 64 | Train Loss: 0.3954389 Vali Loss: 1.3045691 Test Loss: 0.4837514
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 0.9928359985351562
Epoch: 74, Steps: 64 | Train Loss: 0.3955848 Vali Loss: 1.3062222 Test Loss: 0.4837495
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.147853136062622
Epoch: 75, Steps: 64 | Train Loss: 0.3956465 Vali Loss: 1.3021845 Test Loss: 0.4837504
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.0383882522583008
Epoch: 76, Steps: 64 | Train Loss: 0.3956121 Vali Loss: 1.3020054 Test Loss: 0.4837487
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.0308172702789307
Epoch: 77, Steps: 64 | Train Loss: 0.3954239 Vali Loss: 1.3005923 Test Loss: 0.4837486
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.120415449142456
Epoch: 78, Steps: 64 | Train Loss: 0.3955310 Vali Loss: 1.3005100 Test Loss: 0.4837521
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.1468050479888916
Epoch: 79, Steps: 64 | Train Loss: 0.3955072 Vali Loss: 1.3012985 Test Loss: 0.4837525
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.165494680404663
Epoch: 80, Steps: 64 | Train Loss: 0.3952296 Vali Loss: 1.3019689 Test Loss: 0.4837511
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.2607395648956299
Epoch: 81, Steps: 64 | Train Loss: 0.3956218 Vali Loss: 1.3055985 Test Loss: 0.4837549
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.2468492984771729
Epoch: 82, Steps: 64 | Train Loss: 0.3955085 Vali Loss: 1.2985352 Test Loss: 0.4837531
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.1928653717041016
Epoch: 83, Steps: 64 | Train Loss: 0.3956504 Vali Loss: 1.3025879 Test Loss: 0.4837524
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.1638429164886475
Epoch: 84, Steps: 64 | Train Loss: 0.3955501 Vali Loss: 1.3017594 Test Loss: 0.4837542
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.0884292125701904
Epoch: 85, Steps: 64 | Train Loss: 0.3955236 Vali Loss: 1.3033998 Test Loss: 0.4837560
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.0216524600982666
Epoch: 86, Steps: 64 | Train Loss: 0.3954989 Vali Loss: 1.3046172 Test Loss: 0.4837562
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.2004070281982422
Epoch: 87, Steps: 64 | Train Loss: 0.3953108 Vali Loss: 1.3004346 Test Loss: 0.4837603
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.1058294773101807
Epoch: 88, Steps: 64 | Train Loss: 0.3951323 Vali Loss: 1.3007046 Test Loss: 0.4837561
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.0478439331054688
Epoch: 89, Steps: 64 | Train Loss: 0.3954682 Vali Loss: 1.3050065 Test Loss: 0.4837619
EarlyStopping counter: 20 out of 20
Early stopping
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=18, out_features=85, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1370880.0
params:  1615.0
Trainable parameters:  1615
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 0.98573899269104
Epoch: 1, Steps: 64 | Train Loss: 0.4909536 Vali Loss: 1.2989916 Test Loss: 0.4838276
Validation loss decreased (inf --> 1.298992).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.7900478839874268
Epoch: 2, Steps: 64 | Train Loss: 0.4906283 Vali Loss: 1.2970387 Test Loss: 0.4836126
Validation loss decreased (1.298992 --> 1.297039).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.215306520462036
Epoch: 3, Steps: 64 | Train Loss: 0.4899556 Vali Loss: 1.3000091 Test Loss: 0.4835357
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.7413747310638428
Epoch: 4, Steps: 64 | Train Loss: 0.4896350 Vali Loss: 1.2978973 Test Loss: 0.4836683
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.243910551071167
Epoch: 5, Steps: 64 | Train Loss: 0.4896968 Vali Loss: 1.2956802 Test Loss: 0.4836605
Validation loss decreased (1.297039 --> 1.295680).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.5505828857421875
Epoch: 6, Steps: 64 | Train Loss: 0.4897632 Vali Loss: 1.2975048 Test Loss: 0.4838136
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.0094053745269775
Epoch: 7, Steps: 64 | Train Loss: 0.4893199 Vali Loss: 1.2934111 Test Loss: 0.4839005
Validation loss decreased (1.295680 --> 1.293411).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.289175271987915
Epoch: 8, Steps: 64 | Train Loss: 0.4894364 Vali Loss: 1.2938083 Test Loss: 0.4840744
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.0697579383850098
Epoch: 9, Steps: 64 | Train Loss: 0.4895607 Vali Loss: 1.2943420 Test Loss: 0.4838809
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.8208978176116943
Epoch: 10, Steps: 64 | Train Loss: 0.4892826 Vali Loss: 1.2913891 Test Loss: 0.4840429
Validation loss decreased (1.293411 --> 1.291389).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.483306646347046
Epoch: 11, Steps: 64 | Train Loss: 0.4892547 Vali Loss: 1.3029234 Test Loss: 0.4840596
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.847996473312378
Epoch: 12, Steps: 64 | Train Loss: 0.4893969 Vali Loss: 1.2958782 Test Loss: 0.4840700
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.4071030616760254
Epoch: 13, Steps: 64 | Train Loss: 0.4894676 Vali Loss: 1.2924839 Test Loss: 0.4841863
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.717151641845703
Epoch: 14, Steps: 64 | Train Loss: 0.4892915 Vali Loss: 1.2948203 Test Loss: 0.4843271
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.510111093521118
Epoch: 15, Steps: 64 | Train Loss: 0.4894499 Vali Loss: 1.2978272 Test Loss: 0.4842398
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.56286358833313
Epoch: 16, Steps: 64 | Train Loss: 0.4896057 Vali Loss: 1.2957882 Test Loss: 0.4843304
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.682828664779663
Epoch: 17, Steps: 64 | Train Loss: 0.4891307 Vali Loss: 1.2952970 Test Loss: 0.4843532
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.785618543624878
Epoch: 18, Steps: 64 | Train Loss: 0.4891454 Vali Loss: 1.2935352 Test Loss: 0.4844105
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.255012273788452
Epoch: 19, Steps: 64 | Train Loss: 0.4890590 Vali Loss: 1.2934257 Test Loss: 0.4844619
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5221595764160156
Epoch: 20, Steps: 64 | Train Loss: 0.4892262 Vali Loss: 1.2967621 Test Loss: 0.4845015
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.953094720840454
Epoch: 21, Steps: 64 | Train Loss: 0.4890600 Vali Loss: 1.2965809 Test Loss: 0.4844932
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.4744393825531006
Epoch: 22, Steps: 64 | Train Loss: 0.4891854 Vali Loss: 1.2915640 Test Loss: 0.4844968
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.4137389659881592
Epoch: 23, Steps: 64 | Train Loss: 0.4892092 Vali Loss: 1.2980191 Test Loss: 0.4844835
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.070652961730957
Epoch: 24, Steps: 64 | Train Loss: 0.4890187 Vali Loss: 1.2938155 Test Loss: 0.4845082
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.7926292419433594
Epoch: 25, Steps: 64 | Train Loss: 0.4894415 Vali Loss: 1.2948030 Test Loss: 0.4845890
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.9750351905822754
Epoch: 26, Steps: 64 | Train Loss: 0.4889716 Vali Loss: 1.2987678 Test Loss: 0.4845936
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.6079976558685303
Epoch: 27, Steps: 64 | Train Loss: 0.4890872 Vali Loss: 1.2925243 Test Loss: 0.4845859
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.9388484954833984
Epoch: 28, Steps: 64 | Train Loss: 0.4891734 Vali Loss: 1.2925762 Test Loss: 0.4846534
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.8050501346588135
Epoch: 29, Steps: 64 | Train Loss: 0.4890748 Vali Loss: 1.2914613 Test Loss: 0.4846459
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.586827039718628
Epoch: 30, Steps: 64 | Train Loss: 0.4890883 Vali Loss: 1.2961359 Test Loss: 0.4847027
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4835524260997772, mae:0.4482373893260956, rse:0.6620241403579712, corr:[0.25181547 0.25449583 0.25484022 0.2521781  0.24801195 0.24556975
 0.24558643 0.24524112 0.244219   0.24388583 0.2440389  0.2441343
 0.24355638 0.24277012 0.24270849 0.24328059 0.24375677 0.24358499
 0.2429688  0.24273473 0.24281015 0.24284977 0.24234994 0.24130666
 0.23975036 0.23937252 0.23974389 0.23972066 0.23916486 0.23912808
 0.23977229 0.23983762 0.23943847 0.2391478  0.23909219 0.2393271
 0.23934273 0.23891126 0.23876064 0.23913926 0.23984548 0.24028744
 0.24031925 0.24039324 0.2407083  0.24075182 0.24055284 0.23989454
 0.23856984 0.23808034 0.23781751 0.23680954 0.23500887 0.2338389
 0.23420043 0.23429953 0.23377618 0.23343676 0.23333462 0.23358242
 0.23348165 0.23298387 0.23261666 0.23269619 0.23327464 0.23377414
 0.23392086 0.23377058 0.23369192 0.23369731 0.23340732 0.23221797
 0.23030673 0.22945966 0.22935976 0.22919358 0.22852667 0.22812536
 0.22864419 0.22864328 0.22815439 0.227619   0.22728978 0.22734727
 0.22740266 0.22708078 0.22680007 0.22681579 0.2271581  0.22752602
 0.22753778 0.22742826 0.22739267 0.22753374 0.22769907 0.22725818
 0.22610028 0.22576845 0.2259056  0.2254954  0.22496758 0.22510143
 0.22592531 0.22608212 0.22559017 0.22511141 0.22481021 0.22470386
 0.22456111 0.2242036  0.2241446  0.22433215 0.22473142 0.2248385
 0.22479871 0.22489929 0.2251816  0.22528432 0.22481313 0.22354166
 0.22168064 0.22074354 0.22025846 0.21934094 0.2184116  0.21831821
 0.21937662 0.21987902 0.2196257  0.2192684  0.2190336  0.21901855
 0.21882461 0.21824373 0.21795835 0.21811603 0.21864109 0.2188789
 0.21884857 0.2189851  0.21923327 0.21941225 0.2192389  0.21827419
 0.21649364 0.21557628 0.21532346 0.2144078  0.21326116 0.21310174
 0.21427904 0.21473648 0.21449031 0.21420172 0.21411611 0.21426849
 0.21416822 0.21364711 0.21318984 0.21316026 0.21345119 0.21363634
 0.21359706 0.21367113 0.21391957 0.21409023 0.21381372 0.21263388
 0.2108708  0.210311   0.21062942 0.2105016  0.2094637  0.2090664
 0.20997614 0.21032308 0.21000504 0.20976761 0.20982471 0.20999935
 0.20988786 0.20927328 0.20874554 0.2086686  0.20912306 0.20978057
 0.2101504  0.21032472 0.21061751 0.21095021 0.21084459 0.20982349
 0.2079445  0.20735824 0.20747103 0.20702793 0.20615368 0.20610566
 0.20710748 0.20757675 0.20735729 0.20702758 0.20672725 0.20667845
 0.20654042 0.20611724 0.20579416 0.20575087 0.20596696 0.2060316
 0.20597303 0.20613796 0.20644835 0.20661396 0.20633721 0.20537719
 0.20389539 0.20346914 0.20379718 0.20393167 0.20392619 0.20479833
 0.2065537  0.20750314 0.2077498  0.20766337 0.20734742 0.2071992
 0.2071837  0.20685977 0.20662944 0.2067136  0.2069554  0.20700519
 0.20691615 0.2070032  0.20731555 0.20753506 0.20732965 0.20632724
 0.20482667 0.20432726 0.20438597 0.20406903 0.20348713 0.20386785
 0.20547079 0.20644991 0.20665559 0.20638593 0.20598114 0.20574605
 0.20544617 0.20484912 0.2044866  0.20441349 0.20459387 0.20471378
 0.2047761  0.20479532 0.20485006 0.20486937 0.20479602 0.20404676
 0.20250377 0.20201138 0.20232977 0.20262729 0.20260254 0.20336705
 0.20508346 0.20600301 0.20596895 0.20556408 0.20516266 0.20507684
 0.2050028  0.20467424 0.2043652  0.20443851 0.20475118 0.2050319
 0.20510775 0.20517696 0.20544372 0.20570676 0.20570962 0.204988
 0.2038162  0.20375554 0.20478433 0.20533496 0.20510831 0.20551138
 0.20677182 0.20714037 0.206974   0.20669971 0.20647274 0.20648016
 0.20630279 0.20564695 0.20509751 0.20497128 0.20531194 0.2054962
 0.20553768 0.20568722 0.20601477 0.20625016 0.20605868 0.20480591
 0.20304915 0.20234512 0.20251705 0.20223342 0.20171106 0.2018424
 0.20329118 0.20383947 0.20342486 0.20292078 0.20255539 0.20254618
 0.20252153 0.20193067 0.20106705 0.2008174  0.2011389  0.20166011
 0.20197603 0.20174745 0.20127836 0.20149618 0.20256473 0.20253482]
