Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.2432947158813477
Epoch: 1, Steps: 60 | Train Loss: 0.6519894 Vali Loss: 1.1970723 Test Loss: 0.5668868
Validation loss decreased (inf --> 1.197072).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.2601723670959473
Epoch: 2, Steps: 60 | Train Loss: 0.4844575 Vali Loss: 1.0596355 Test Loss: 0.4745445
Validation loss decreased (1.197072 --> 1.059636).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.3577916622161865
Epoch: 3, Steps: 60 | Train Loss: 0.4352209 Vali Loss: 1.0059483 Test Loss: 0.4382724
Validation loss decreased (1.059636 --> 1.005948).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.0391809940338135
Epoch: 4, Steps: 60 | Train Loss: 0.4133068 Vali Loss: 0.9826912 Test Loss: 0.4236814
Validation loss decreased (1.005948 --> 0.982691).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.235596179962158
Epoch: 5, Steps: 60 | Train Loss: 0.4029478 Vali Loss: 0.9735073 Test Loss: 0.4186169
Validation loss decreased (0.982691 --> 0.973507).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.5135488510131836
Epoch: 6, Steps: 60 | Train Loss: 0.3975338 Vali Loss: 0.9684221 Test Loss: 0.4169523
Validation loss decreased (0.973507 --> 0.968422).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.8098158836364746
Epoch: 7, Steps: 60 | Train Loss: 0.3938341 Vali Loss: 0.9667459 Test Loss: 0.4162206
Validation loss decreased (0.968422 --> 0.966746).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.6073691844940186
Epoch: 8, Steps: 60 | Train Loss: 0.3917195 Vali Loss: 0.9651375 Test Loss: 0.4164197
Validation loss decreased (0.966746 --> 0.965137).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.1093499660491943
Epoch: 9, Steps: 60 | Train Loss: 0.3895564 Vali Loss: 0.9644850 Test Loss: 0.4167304
Validation loss decreased (0.965137 --> 0.964485).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.1877262592315674
Epoch: 10, Steps: 60 | Train Loss: 0.3885685 Vali Loss: 0.9634025 Test Loss: 0.4165831
Validation loss decreased (0.964485 --> 0.963402).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.4845080375671387
Epoch: 11, Steps: 60 | Train Loss: 0.3872037 Vali Loss: 0.9630748 Test Loss: 0.4166214
Validation loss decreased (0.963402 --> 0.963075).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1094870567321777
Epoch: 12, Steps: 60 | Train Loss: 0.3861598 Vali Loss: 0.9625323 Test Loss: 0.4166940
Validation loss decreased (0.963075 --> 0.962532).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.1275835037231445
Epoch: 13, Steps: 60 | Train Loss: 0.3854372 Vali Loss: 0.9631696 Test Loss: 0.4169256
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.9047949314117432
Epoch: 14, Steps: 60 | Train Loss: 0.3850550 Vali Loss: 0.9623151 Test Loss: 0.4169821
Validation loss decreased (0.962532 --> 0.962315).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.972916841506958
Epoch: 15, Steps: 60 | Train Loss: 0.3845197 Vali Loss: 0.9626672 Test Loss: 0.4172416
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.8736119270324707
Epoch: 16, Steps: 60 | Train Loss: 0.3840096 Vali Loss: 0.9625007 Test Loss: 0.4172169
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.886355400085449
Epoch: 17, Steps: 60 | Train Loss: 0.3834892 Vali Loss: 0.9627578 Test Loss: 0.4172983
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.6595566272735596
Epoch: 18, Steps: 60 | Train Loss: 0.3833772 Vali Loss: 0.9630454 Test Loss: 0.4173028
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.997131586074829
Epoch: 19, Steps: 60 | Train Loss: 0.3828598 Vali Loss: 0.9627242 Test Loss: 0.4173135
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.529721975326538
Epoch: 20, Steps: 60 | Train Loss: 0.3820188 Vali Loss: 0.9623482 Test Loss: 0.4173572
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.7878170013427734
Epoch: 21, Steps: 60 | Train Loss: 0.3820639 Vali Loss: 0.9621012 Test Loss: 0.4174348
Validation loss decreased (0.962315 --> 0.962101).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.4943673610687256
Epoch: 22, Steps: 60 | Train Loss: 0.3819340 Vali Loss: 0.9624462 Test Loss: 0.4173803
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.334821939468384
Epoch: 23, Steps: 60 | Train Loss: 0.3815426 Vali Loss: 0.9626470 Test Loss: 0.4175246
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.523038148880005
Epoch: 24, Steps: 60 | Train Loss: 0.3816097 Vali Loss: 0.9630088 Test Loss: 0.4176979
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.816865921020508
Epoch: 25, Steps: 60 | Train Loss: 0.3811069 Vali Loss: 0.9625556 Test Loss: 0.4174872
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.3801379203796387
Epoch: 26, Steps: 60 | Train Loss: 0.3810050 Vali Loss: 0.9625084 Test Loss: 0.4175808
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.001317262649536
Epoch: 27, Steps: 60 | Train Loss: 0.3805526 Vali Loss: 0.9626769 Test Loss: 0.4175888
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.5356359481811523
Epoch: 28, Steps: 60 | Train Loss: 0.3807705 Vali Loss: 0.9626883 Test Loss: 0.4177511
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.20524001121521
Epoch: 29, Steps: 60 | Train Loss: 0.3806024 Vali Loss: 0.9620157 Test Loss: 0.4177403
Validation loss decreased (0.962101 --> 0.962016).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.7851064205169678
Epoch: 30, Steps: 60 | Train Loss: 0.3805073 Vali Loss: 0.9626874 Test Loss: 0.4177583
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.7453486919403076
Epoch: 31, Steps: 60 | Train Loss: 0.3801052 Vali Loss: 0.9621856 Test Loss: 0.4178494
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.723970413208008
Epoch: 32, Steps: 60 | Train Loss: 0.3802408 Vali Loss: 0.9627401 Test Loss: 0.4176551
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.6024975776672363
Epoch: 33, Steps: 60 | Train Loss: 0.3802122 Vali Loss: 0.9624597 Test Loss: 0.4177914
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.568220853805542
Epoch: 34, Steps: 60 | Train Loss: 0.3800210 Vali Loss: 0.9622744 Test Loss: 0.4178806
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.279125690460205
Epoch: 35, Steps: 60 | Train Loss: 0.3799493 Vali Loss: 0.9625649 Test Loss: 0.4179053
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.4726381301879883
Epoch: 36, Steps: 60 | Train Loss: 0.3798142 Vali Loss: 0.9626083 Test Loss: 0.4178837
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.323442220687866
Epoch: 37, Steps: 60 | Train Loss: 0.3796796 Vali Loss: 0.9618348 Test Loss: 0.4179125
Validation loss decreased (0.962016 --> 0.961835).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6821680068969727
Epoch: 38, Steps: 60 | Train Loss: 0.3795376 Vali Loss: 0.9624963 Test Loss: 0.4178355
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.676905870437622
Epoch: 39, Steps: 60 | Train Loss: 0.3793947 Vali Loss: 0.9621400 Test Loss: 0.4179022
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.847127676010132
Epoch: 40, Steps: 60 | Train Loss: 0.3792121 Vali Loss: 0.9621010 Test Loss: 0.4178704
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.2659971714019775
Epoch: 41, Steps: 60 | Train Loss: 0.3798472 Vali Loss: 0.9624511 Test Loss: 0.4179204
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.584343433380127
Epoch: 42, Steps: 60 | Train Loss: 0.3792080 Vali Loss: 0.9622844 Test Loss: 0.4179516
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.649902582168579
Epoch: 43, Steps: 60 | Train Loss: 0.3792657 Vali Loss: 0.9618948 Test Loss: 0.4179659
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.6798715591430664
Epoch: 44, Steps: 60 | Train Loss: 0.3792417 Vali Loss: 0.9620172 Test Loss: 0.4179595
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.3657000064849854
Epoch: 45, Steps: 60 | Train Loss: 0.3793710 Vali Loss: 0.9622241 Test Loss: 0.4179724
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.489475727081299
Epoch: 46, Steps: 60 | Train Loss: 0.3790333 Vali Loss: 0.9621360 Test Loss: 0.4179426
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.8808531761169434
Epoch: 47, Steps: 60 | Train Loss: 0.3792575 Vali Loss: 0.9621816 Test Loss: 0.4180068
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.560749053955078
Epoch: 48, Steps: 60 | Train Loss: 0.3784020 Vali Loss: 0.9621012 Test Loss: 0.4179764
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.3212695121765137
Epoch: 49, Steps: 60 | Train Loss: 0.3790613 Vali Loss: 0.9625053 Test Loss: 0.4180828
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.61887264251709
Epoch: 50, Steps: 60 | Train Loss: 0.3786305 Vali Loss: 0.9621013 Test Loss: 0.4180181
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.143904447555542
Epoch: 51, Steps: 60 | Train Loss: 0.3789510 Vali Loss: 0.9620031 Test Loss: 0.4180082
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.0497899055480957
Epoch: 52, Steps: 60 | Train Loss: 0.3789664 Vali Loss: 0.9623259 Test Loss: 0.4180580
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.154897689819336
Epoch: 53, Steps: 60 | Train Loss: 0.3790539 Vali Loss: 0.9619443 Test Loss: 0.4180478
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.4273159503936768
Epoch: 54, Steps: 60 | Train Loss: 0.3787465 Vali Loss: 0.9621173 Test Loss: 0.4180788
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.262397289276123
Epoch: 55, Steps: 60 | Train Loss: 0.3786681 Vali Loss: 0.9622170 Test Loss: 0.4180725
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.6162769794464111
Epoch: 56, Steps: 60 | Train Loss: 0.3790188 Vali Loss: 0.9621821 Test Loss: 0.4180760
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.795271635055542
Epoch: 57, Steps: 60 | Train Loss: 0.3784577 Vali Loss: 0.9622203 Test Loss: 0.4180991
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4141504168510437, mae:0.42246779799461365, rse:0.6111339330673218, corr:[0.25752524 0.26871368 0.26870728 0.26967803 0.2679758  0.2648041
 0.26379597 0.2641521  0.26362947 0.26326814 0.26366007 0.2636385
 0.26300082 0.26262617 0.26280513 0.26291928 0.2628754  0.26294708
 0.26281434 0.26226926 0.2618188  0.2618436  0.2618095  0.26174307
 0.2616876  0.2614958  0.26134253 0.26152465 0.26151136 0.26086706
 0.2602884  0.26010543 0.2600266  0.25980803 0.25961867 0.25969645
 0.2597259  0.25963083 0.25967738 0.25988066 0.2601196  0.26030594
 0.26054427 0.26073727 0.26071867 0.2607257  0.2610404  0.2613081
 0.26094693 0.26006123 0.25893077 0.25786316 0.25688267 0.25574532
 0.25485724 0.25449035 0.25440902 0.25427416 0.2538292  0.2536029
 0.25367633 0.25373322 0.25350225 0.25330794 0.25341815 0.25375912
 0.25412327 0.25414735 0.2539845  0.25388733 0.2539251  0.25372636
 0.2531298  0.25230467 0.25170544 0.25142625 0.25095424 0.25040624
 0.25020352 0.25004917 0.24957596 0.24911453 0.24895082 0.24876703
 0.2483995  0.24827664 0.24854909 0.24852224 0.24803981 0.24788769
 0.24816366 0.24811967 0.24765208 0.24749164 0.24782719 0.24847274
 0.24915704 0.24949108 0.24931397 0.24902727 0.2489569  0.248837
 0.2484734  0.24823138 0.24836618 0.2485359  0.24811405 0.24755092
 0.24747488 0.24749164 0.24744745 0.24774383 0.24814723 0.24814409
 0.24795182 0.24799432 0.24807839 0.24783733 0.2475683  0.24752818
 0.24719211 0.24614084 0.24509546 0.24453525 0.24377468 0.24286667
 0.24265188 0.24268845 0.24205905 0.24157812 0.24162342 0.24136949
 0.24056809 0.24036711 0.24089734 0.24095659 0.2407555  0.24105757
 0.24138501 0.24072045 0.24003024 0.24034338 0.2404103  0.2395115
 0.23891947 0.23880649 0.23786016 0.23626398 0.23543791 0.23485887
 0.23371775 0.23299484 0.23348328 0.233733   0.23286496 0.2326751
 0.23324762 0.23280413 0.23206569 0.23267399 0.2332355  0.23222569
 0.23140062 0.23219559 0.23241074 0.23109144 0.23071991 0.2313991
 0.23059665 0.22909835 0.2294176  0.23011859 0.22862783 0.22711651
 0.22746423 0.22721295 0.22555603 0.2257159  0.22670224 0.2253032
 0.22388433 0.22509919 0.22483519 0.22237971 0.2230991  0.22493167
 0.2218722  0.22014157 0.22442532 0.22121225 0.21649244 0.23651484]
