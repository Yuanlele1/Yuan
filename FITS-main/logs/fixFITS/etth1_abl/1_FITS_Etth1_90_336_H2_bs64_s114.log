Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=18, out_features=85, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1370880.0
params:  1615.0
Trainable parameters:  1615
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.121415376663208
Epoch: 1, Steps: 64 | Train Loss: 1.0759980 Vali Loss: 2.1452637 Test Loss: 1.1118287
Validation loss decreased (inf --> 2.145264).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.0956659317016602
Epoch: 2, Steps: 64 | Train Loss: 0.8285321 Vali Loss: 1.7898754 Test Loss: 0.8335279
Validation loss decreased (2.145264 --> 1.789875).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.1133675575256348
Epoch: 3, Steps: 64 | Train Loss: 0.6969139 Vali Loss: 1.6045280 Test Loss: 0.6908535
Validation loss decreased (1.789875 --> 1.604528).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.1090068817138672
Epoch: 4, Steps: 64 | Train Loss: 0.6228439 Vali Loss: 1.4978170 Test Loss: 0.6116951
Validation loss decreased (1.604528 --> 1.497817).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.1239137649536133
Epoch: 5, Steps: 64 | Train Loss: 0.5788398 Vali Loss: 1.4406023 Test Loss: 0.5653840
Validation loss decreased (1.497817 --> 1.440602).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.0980334281921387
Epoch: 6, Steps: 64 | Train Loss: 0.5516355 Vali Loss: 1.3912268 Test Loss: 0.5372132
Validation loss decreased (1.440602 --> 1.391227).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 0.9729735851287842
Epoch: 7, Steps: 64 | Train Loss: 0.5337950 Vali Loss: 1.3688819 Test Loss: 0.5194796
Validation loss decreased (1.391227 --> 1.368882).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.195573329925537
Epoch: 8, Steps: 64 | Train Loss: 0.5218673 Vali Loss: 1.3567626 Test Loss: 0.5079749
Validation loss decreased (1.368882 --> 1.356763).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.2532308101654053
Epoch: 9, Steps: 64 | Train Loss: 0.5140988 Vali Loss: 1.3437662 Test Loss: 0.5004813
Validation loss decreased (1.356763 --> 1.343766).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.1950583457946777
Epoch: 10, Steps: 64 | Train Loss: 0.5082929 Vali Loss: 1.3343815 Test Loss: 0.4952792
Validation loss decreased (1.343766 --> 1.334381).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.151845932006836
Epoch: 11, Steps: 64 | Train Loss: 0.5044798 Vali Loss: 1.3232039 Test Loss: 0.4917060
Validation loss decreased (1.334381 --> 1.323204).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.1673774719238281
Epoch: 12, Steps: 64 | Train Loss: 0.5015999 Vali Loss: 1.3210694 Test Loss: 0.4892645
Validation loss decreased (1.323204 --> 1.321069).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.0954158306121826
Epoch: 13, Steps: 64 | Train Loss: 0.4992797 Vali Loss: 1.3167939 Test Loss: 0.4875683
Validation loss decreased (1.321069 --> 1.316794).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.0831942558288574
Epoch: 14, Steps: 64 | Train Loss: 0.4976201 Vali Loss: 1.3136308 Test Loss: 0.4863234
Validation loss decreased (1.316794 --> 1.313631).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1123754978179932
Epoch: 15, Steps: 64 | Train Loss: 0.4964062 Vali Loss: 1.3109397 Test Loss: 0.4854269
Validation loss decreased (1.313631 --> 1.310940).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.1158125400543213
Epoch: 16, Steps: 64 | Train Loss: 0.4953657 Vali Loss: 1.3119234 Test Loss: 0.4847809
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.1463782787322998
Epoch: 17, Steps: 64 | Train Loss: 0.4945421 Vali Loss: 1.3128858 Test Loss: 0.4842582
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.130725860595703
Epoch: 18, Steps: 64 | Train Loss: 0.4938814 Vali Loss: 1.3040227 Test Loss: 0.4839211
Validation loss decreased (1.310940 --> 1.304023).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.1181511878967285
Epoch: 19, Steps: 64 | Train Loss: 0.4932522 Vali Loss: 1.3066235 Test Loss: 0.4836818
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.2936270236968994
Epoch: 20, Steps: 64 | Train Loss: 0.4928045 Vali Loss: 1.3080362 Test Loss: 0.4834998
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.0905869007110596
Epoch: 21, Steps: 64 | Train Loss: 0.4927485 Vali Loss: 1.3065535 Test Loss: 0.4833159
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.1429226398468018
Epoch: 22, Steps: 64 | Train Loss: 0.4923203 Vali Loss: 1.3039986 Test Loss: 0.4832414
Validation loss decreased (1.304023 --> 1.303999).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.0663042068481445
Epoch: 23, Steps: 64 | Train Loss: 0.4919648 Vali Loss: 1.3074465 Test Loss: 0.4831542
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.071458101272583
Epoch: 24, Steps: 64 | Train Loss: 0.4916906 Vali Loss: 1.3045094 Test Loss: 0.4831386
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.1848325729370117
Epoch: 25, Steps: 64 | Train Loss: 0.4913170 Vali Loss: 1.3010013 Test Loss: 0.4830913
Validation loss decreased (1.303999 --> 1.301001).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.0773468017578125
Epoch: 26, Steps: 64 | Train Loss: 0.4912126 Vali Loss: 1.3035372 Test Loss: 0.4830874
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.22198486328125
Epoch: 27, Steps: 64 | Train Loss: 0.4912333 Vali Loss: 1.3019276 Test Loss: 0.4830647
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.1516039371490479
Epoch: 28, Steps: 64 | Train Loss: 0.4913236 Vali Loss: 1.3061495 Test Loss: 0.4830384
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.0428826808929443
Epoch: 29, Steps: 64 | Train Loss: 0.4910717 Vali Loss: 1.2982363 Test Loss: 0.4830306
Validation loss decreased (1.301001 --> 1.298236).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.1719534397125244
Epoch: 30, Steps: 64 | Train Loss: 0.4908409 Vali Loss: 1.2986829 Test Loss: 0.4830163
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.06038498878479
Epoch: 31, Steps: 64 | Train Loss: 0.4910599 Vali Loss: 1.2993976 Test Loss: 0.4830323
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.1091887950897217
Epoch: 32, Steps: 64 | Train Loss: 0.4904862 Vali Loss: 1.3019258 Test Loss: 0.4830652
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.0455400943756104
Epoch: 33, Steps: 64 | Train Loss: 0.4904582 Vali Loss: 1.3055156 Test Loss: 0.4830739
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.172853708267212
Epoch: 34, Steps: 64 | Train Loss: 0.4906919 Vali Loss: 1.3006959 Test Loss: 0.4830503
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.21368408203125
Epoch: 35, Steps: 64 | Train Loss: 0.4905660 Vali Loss: 1.2971103 Test Loss: 0.4831000
Validation loss decreased (1.298236 --> 1.297110).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.1186814308166504
Epoch: 36, Steps: 64 | Train Loss: 0.4900548 Vali Loss: 1.3000001 Test Loss: 0.4831001
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.1440279483795166
Epoch: 37, Steps: 64 | Train Loss: 0.4905405 Vali Loss: 1.2993912 Test Loss: 0.4831128
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.1363017559051514
Epoch: 38, Steps: 64 | Train Loss: 0.4904263 Vali Loss: 1.2963591 Test Loss: 0.4831389
Validation loss decreased (1.297110 --> 1.296359).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.1478233337402344
Epoch: 39, Steps: 64 | Train Loss: 0.4903765 Vali Loss: 1.3026851 Test Loss: 0.4831294
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.0345721244812012
Epoch: 40, Steps: 64 | Train Loss: 0.4903035 Vali Loss: 1.3015497 Test Loss: 0.4831418
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.1771962642669678
Epoch: 41, Steps: 64 | Train Loss: 0.4900818 Vali Loss: 1.2974252 Test Loss: 0.4831526
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.099961519241333
Epoch: 42, Steps: 64 | Train Loss: 0.4902229 Vali Loss: 1.2987077 Test Loss: 0.4831678
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.1691627502441406
Epoch: 43, Steps: 64 | Train Loss: 0.4901648 Vali Loss: 1.2957690 Test Loss: 0.4831750
Validation loss decreased (1.296359 --> 1.295769).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.1953518390655518
Epoch: 44, Steps: 64 | Train Loss: 0.4903171 Vali Loss: 1.2955395 Test Loss: 0.4831805
Validation loss decreased (1.295769 --> 1.295539).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.139733076095581
Epoch: 45, Steps: 64 | Train Loss: 0.4900109 Vali Loss: 1.3018585 Test Loss: 0.4831982
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.0011117458343506
Epoch: 46, Steps: 64 | Train Loss: 0.4901922 Vali Loss: 1.3032168 Test Loss: 0.4832050
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.1337759494781494
Epoch: 47, Steps: 64 | Train Loss: 0.4901848 Vali Loss: 1.3038507 Test Loss: 0.4831995
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.158015489578247
Epoch: 48, Steps: 64 | Train Loss: 0.4902138 Vali Loss: 1.2964420 Test Loss: 0.4832341
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.1542017459869385
Epoch: 49, Steps: 64 | Train Loss: 0.4900492 Vali Loss: 1.2984303 Test Loss: 0.4832395
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.1663038730621338
Epoch: 50, Steps: 64 | Train Loss: 0.4900686 Vali Loss: 1.2975932 Test Loss: 0.4832452
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.0467352867126465
Epoch: 51, Steps: 64 | Train Loss: 0.4898657 Vali Loss: 1.3010342 Test Loss: 0.4832458
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 0.983525276184082
Epoch: 52, Steps: 64 | Train Loss: 0.4900646 Vali Loss: 1.2979940 Test Loss: 0.4832466
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.1244115829467773
Epoch: 53, Steps: 64 | Train Loss: 0.4900404 Vali Loss: 1.2993968 Test Loss: 0.4832566
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.1138560771942139
Epoch: 54, Steps: 64 | Train Loss: 0.4899291 Vali Loss: 1.3000562 Test Loss: 0.4832750
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.0728275775909424
Epoch: 55, Steps: 64 | Train Loss: 0.4899563 Vali Loss: 1.3008986 Test Loss: 0.4832733
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.1481313705444336
Epoch: 56, Steps: 64 | Train Loss: 0.4900095 Vali Loss: 1.2950457 Test Loss: 0.4832910
Validation loss decreased (1.295539 --> 1.295046).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.1156139373779297
Epoch: 57, Steps: 64 | Train Loss: 0.4897938 Vali Loss: 1.2995868 Test Loss: 0.4832823
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.0398008823394775
Epoch: 58, Steps: 64 | Train Loss: 0.4901848 Vali Loss: 1.2983060 Test Loss: 0.4833017
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.0772478580474854
Epoch: 59, Steps: 64 | Train Loss: 0.4900183 Vali Loss: 1.2987642 Test Loss: 0.4833141
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.1367506980895996
Epoch: 60, Steps: 64 | Train Loss: 0.4896906 Vali Loss: 1.2955784 Test Loss: 0.4833022
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.0733063220977783
Epoch: 61, Steps: 64 | Train Loss: 0.4897640 Vali Loss: 1.2967266 Test Loss: 0.4833095
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.0757803916931152
Epoch: 62, Steps: 64 | Train Loss: 0.4899949 Vali Loss: 1.2962699 Test Loss: 0.4833208
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.174710750579834
Epoch: 63, Steps: 64 | Train Loss: 0.4899006 Vali Loss: 1.2986166 Test Loss: 0.4833252
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.090850591659546
Epoch: 64, Steps: 64 | Train Loss: 0.4898006 Vali Loss: 1.2991018 Test Loss: 0.4833354
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.2098102569580078
Epoch: 65, Steps: 64 | Train Loss: 0.4898831 Vali Loss: 1.3014408 Test Loss: 0.4833396
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.2469961643218994
Epoch: 66, Steps: 64 | Train Loss: 0.4897341 Vali Loss: 1.3018512 Test Loss: 0.4833363
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.1413333415985107
Epoch: 67, Steps: 64 | Train Loss: 0.4896420 Vali Loss: 1.2995816 Test Loss: 0.4833389
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.058316707611084
Epoch: 68, Steps: 64 | Train Loss: 0.4899093 Vali Loss: 1.2930646 Test Loss: 0.4833404
Validation loss decreased (1.295046 --> 1.293065).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.0871086120605469
Epoch: 69, Steps: 64 | Train Loss: 0.4896177 Vali Loss: 1.2924491 Test Loss: 0.4833501
Validation loss decreased (1.293065 --> 1.292449).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.798466444015503
Epoch: 70, Steps: 64 | Train Loss: 0.4899063 Vali Loss: 1.2966430 Test Loss: 0.4833568
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.057807445526123
Epoch: 71, Steps: 64 | Train Loss: 0.4898590 Vali Loss: 1.2973037 Test Loss: 0.4833518
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.0545873641967773
Epoch: 72, Steps: 64 | Train Loss: 0.4897193 Vali Loss: 1.2984536 Test Loss: 0.4833524
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.075089693069458
Epoch: 73, Steps: 64 | Train Loss: 0.4896281 Vali Loss: 1.3002819 Test Loss: 0.4833640
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.2569735050201416
Epoch: 74, Steps: 64 | Train Loss: 0.4898240 Vali Loss: 1.3017828 Test Loss: 0.4833620
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.1449248790740967
Epoch: 75, Steps: 64 | Train Loss: 0.4899275 Vali Loss: 1.2978239 Test Loss: 0.4833667
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.1724047660827637
Epoch: 76, Steps: 64 | Train Loss: 0.4898964 Vali Loss: 1.2976320 Test Loss: 0.4833673
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.0803780555725098
Epoch: 77, Steps: 64 | Train Loss: 0.4896789 Vali Loss: 1.2963368 Test Loss: 0.4833712
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.0783593654632568
Epoch: 78, Steps: 64 | Train Loss: 0.4898288 Vali Loss: 1.2962227 Test Loss: 0.4833770
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.1593108177185059
Epoch: 79, Steps: 64 | Train Loss: 0.4898059 Vali Loss: 1.2970210 Test Loss: 0.4833802
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.1744298934936523
Epoch: 80, Steps: 64 | Train Loss: 0.4894675 Vali Loss: 1.2976632 Test Loss: 0.4833801
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 0.9758648872375488
Epoch: 81, Steps: 64 | Train Loss: 0.4899747 Vali Loss: 1.3015312 Test Loss: 0.4833854
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.1074705123901367
Epoch: 82, Steps: 64 | Train Loss: 0.4898465 Vali Loss: 1.2943588 Test Loss: 0.4833850
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.109565258026123
Epoch: 83, Steps: 64 | Train Loss: 0.4900442 Vali Loss: 1.2983173 Test Loss: 0.4833859
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.1937839984893799
Epoch: 84, Steps: 64 | Train Loss: 0.4899232 Vali Loss: 1.2975980 Test Loss: 0.4833879
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.0979409217834473
Epoch: 85, Steps: 64 | Train Loss: 0.4899003 Vali Loss: 1.2992252 Test Loss: 0.4833907
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.0688741207122803
Epoch: 86, Steps: 64 | Train Loss: 0.4898901 Vali Loss: 1.3004984 Test Loss: 0.4833918
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.0670113563537598
Epoch: 87, Steps: 64 | Train Loss: 0.4896555 Vali Loss: 1.2962430 Test Loss: 0.4833971
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.1657073497772217
Epoch: 88, Steps: 64 | Train Loss: 0.4894368 Vali Loss: 1.2965540 Test Loss: 0.4833929
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.2296206951141357
Epoch: 89, Steps: 64 | Train Loss: 0.4898715 Vali Loss: 1.3008441 Test Loss: 0.4833999
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4828398823738098, mae:0.4477992653846741, rse:0.6615361571311951, corr:[0.25093973 0.2539458  0.25420812 0.25137928 0.24748251 0.24591501
 0.24641821 0.24592096 0.24427979 0.24337585 0.24355632 0.24413542
 0.24381055 0.24266343 0.24201137 0.24236551 0.24311689 0.2433841
 0.24306853 0.24289903 0.24289997 0.24279049 0.24207178 0.24081272
 0.2392342  0.23895873 0.23939447 0.23932089 0.2386596  0.23862207
 0.23942147 0.23972316 0.23944919 0.2390767  0.23887007 0.23907833
 0.23924655 0.2390213  0.23893617 0.23925535 0.239884   0.2403256
 0.24039106 0.24044429 0.24065907 0.2405911  0.24025744 0.23944916
 0.23803179 0.23750545 0.23725463 0.23631525 0.2346197  0.23356959
 0.23399737 0.23412515 0.23358737 0.23318629 0.23308149 0.23339367
 0.2334379  0.23310624 0.23280023 0.23282596 0.23330636 0.23379329
 0.23401767 0.23393579 0.2338451  0.2337523  0.23331408 0.23194242
 0.22982168 0.22881381 0.22874193 0.22871594 0.22815195 0.22773936
 0.22819501 0.22826189 0.22798234 0.2276711  0.22741799 0.22738385
 0.2273675  0.22712657 0.22697528 0.22705889 0.22731213 0.22753443
 0.22749029 0.22743487 0.22747247 0.22759755 0.22761974 0.22702755
 0.22578512 0.22540548 0.2255717  0.22523023 0.22471507 0.2247688
 0.2255171  0.22573482 0.2253912  0.22505763 0.2248733  0.22484678
 0.22475225 0.22443478 0.22434579 0.22449093 0.22484624 0.22496241
 0.22493398 0.22499746 0.2251837  0.22516496 0.224641   0.22345737
 0.2216955  0.22066404 0.21995655 0.21887752 0.21794727 0.21792497
 0.21898974 0.21951337 0.21934116 0.21908826 0.21896185 0.21900111
 0.21886761 0.2184152  0.21822326 0.21842885 0.21890241 0.21909928
 0.21903986 0.21910003 0.2192542  0.2193048  0.21895267 0.2178742
 0.21612644 0.2152807  0.21508157 0.21414724 0.21291764 0.21269399
 0.21389185 0.21456066 0.2145894  0.21444044 0.21433085 0.21436985
 0.21420345 0.21372792 0.21335104 0.2133681  0.21364191 0.21383616
 0.21381938 0.21384594 0.21397471 0.21398929 0.21356359 0.21236521
 0.21069837 0.21014816 0.21034901 0.21005313 0.20893389 0.20858735
 0.20960811 0.21009403 0.20993534 0.20979384 0.20984952 0.20999256
 0.20991607 0.20948988 0.20917301 0.20920467 0.2095882  0.21006517
 0.21031465 0.21049106 0.2108147  0.21112765 0.21096879 0.20988221
 0.20790231 0.20711778 0.20709437 0.20668302 0.20591627 0.20590848
 0.20684789 0.20730287 0.20715885 0.20689932 0.20664829 0.20660165
 0.20647281 0.20607074 0.20575234 0.20571819 0.20591953 0.20600988
 0.2059639  0.20602998 0.20620115 0.20622233 0.20585956 0.20492832
 0.20356409 0.20317598 0.20345289 0.20353006 0.2035426  0.20448564
 0.20630328 0.20732482 0.20761813 0.20754711 0.2072871  0.20720369
 0.20725629 0.20699371 0.20675781 0.20680477 0.2070105  0.20706546
 0.20699358 0.2070692  0.20734715 0.20751238 0.20722172 0.20616272
 0.20462129 0.20400736 0.20391066 0.2035422  0.20303808 0.20352052
 0.20510742 0.20603536 0.20626302 0.20616138 0.20604199 0.20602262
 0.20581163 0.20528904 0.20500392 0.20501311 0.20523013 0.20535858
 0.20541418 0.20543489 0.20550802 0.20552686 0.20531522 0.2043643
 0.20271833 0.20219518 0.20255548 0.20287405 0.20276183 0.20332366
 0.20486647 0.20586562 0.20615007 0.20604305 0.20576763 0.20560482
 0.20538366 0.20499721 0.2046665  0.20464322 0.2047961  0.2049443
 0.20500581 0.20511922 0.20541903 0.20571077 0.20574525 0.20508114
 0.20398413 0.20391855 0.20487744 0.20543197 0.20536573 0.2059678
 0.2073429  0.2077977  0.20772985 0.20753442 0.20727314 0.20704927
 0.206552   0.20566995 0.20508577 0.20513505 0.20561796 0.20580007
 0.20567629 0.20562291 0.20585218 0.20609628 0.20592743 0.20474756
 0.2030776  0.20234343 0.2024231  0.2021895  0.20181336 0.20200954
 0.20327276 0.20366399 0.20332463 0.20303991 0.2028639  0.20280787
 0.20253451 0.20174667 0.20097478 0.20102675 0.20155507 0.20200403
 0.20203654 0.20164838 0.20135514 0.20175348 0.20240712 0.19991949]
