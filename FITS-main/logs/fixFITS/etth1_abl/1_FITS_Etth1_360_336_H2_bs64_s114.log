Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=42, out_features=81, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3048192.0
params:  3483.0
Trainable parameters:  3483
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.397453784942627
Epoch: 1, Steps: 62 | Train Loss: 0.7703124 Vali Loss: 1.6169381 Test Loss: 0.7348085
Validation loss decreased (inf --> 1.616938).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.1958167552947998
Epoch: 2, Steps: 62 | Train Loss: 0.6201247 Vali Loss: 1.4236625 Test Loss: 0.6126504
Validation loss decreased (1.616938 --> 1.423663).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2565970420837402
Epoch: 3, Steps: 62 | Train Loss: 0.5596272 Vali Loss: 1.3470691 Test Loss: 0.5591339
Validation loss decreased (1.423663 --> 1.347069).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.1790528297424316
Epoch: 4, Steps: 62 | Train Loss: 0.5310067 Vali Loss: 1.3045985 Test Loss: 0.5290971
Validation loss decreased (1.347069 --> 1.304598).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.152834415435791
Epoch: 5, Steps: 62 | Train Loss: 0.5141785 Vali Loss: 1.2759650 Test Loss: 0.5092723
Validation loss decreased (1.304598 --> 1.275965).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.1863510608673096
Epoch: 6, Steps: 62 | Train Loss: 0.5027494 Vali Loss: 1.2550019 Test Loss: 0.4949556
Validation loss decreased (1.275965 --> 1.255002).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.2131054401397705
Epoch: 7, Steps: 62 | Train Loss: 0.4947717 Vali Loss: 1.2400051 Test Loss: 0.4844148
Validation loss decreased (1.255002 --> 1.240005).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2282397747039795
Epoch: 8, Steps: 62 | Train Loss: 0.4889418 Vali Loss: 1.2264333 Test Loss: 0.4767249
Validation loss decreased (1.240005 --> 1.226433).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.200972318649292
Epoch: 9, Steps: 62 | Train Loss: 0.4846171 Vali Loss: 1.2116699 Test Loss: 0.4708949
Validation loss decreased (1.226433 --> 1.211670).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.270050287246704
Epoch: 10, Steps: 62 | Train Loss: 0.4811947 Vali Loss: 1.2075802 Test Loss: 0.4663827
Validation loss decreased (1.211670 --> 1.207580).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.2532682418823242
Epoch: 11, Steps: 62 | Train Loss: 0.4785921 Vali Loss: 1.2014424 Test Loss: 0.4631785
Validation loss decreased (1.207580 --> 1.201442).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.3344452381134033
Epoch: 12, Steps: 62 | Train Loss: 0.4766178 Vali Loss: 1.1962074 Test Loss: 0.4604996
Validation loss decreased (1.201442 --> 1.196207).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.1951169967651367
Epoch: 13, Steps: 62 | Train Loss: 0.4749554 Vali Loss: 1.1928526 Test Loss: 0.4584893
Validation loss decreased (1.196207 --> 1.192853).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.2446942329406738
Epoch: 14, Steps: 62 | Train Loss: 0.4737655 Vali Loss: 1.1918756 Test Loss: 0.4569084
Validation loss decreased (1.192853 --> 1.191876).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1694574356079102
Epoch: 15, Steps: 62 | Train Loss: 0.4729609 Vali Loss: 1.1869128 Test Loss: 0.4557123
Validation loss decreased (1.191876 --> 1.186913).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.0785517692565918
Epoch: 16, Steps: 62 | Train Loss: 0.4723790 Vali Loss: 1.1887164 Test Loss: 0.4546820
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.2561161518096924
Epoch: 17, Steps: 62 | Train Loss: 0.4715228 Vali Loss: 1.1834484 Test Loss: 0.4538713
Validation loss decreased (1.186913 --> 1.183448).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.2068612575531006
Epoch: 18, Steps: 62 | Train Loss: 0.4709887 Vali Loss: 1.1807457 Test Loss: 0.4532908
Validation loss decreased (1.183448 --> 1.180746).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.1575672626495361
Epoch: 19, Steps: 62 | Train Loss: 0.4706349 Vali Loss: 1.1840308 Test Loss: 0.4527466
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3001289367675781
Epoch: 20, Steps: 62 | Train Loss: 0.4702744 Vali Loss: 1.1823394 Test Loss: 0.4524139
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.2093467712402344
Epoch: 21, Steps: 62 | Train Loss: 0.4699683 Vali Loss: 1.1813593 Test Loss: 0.4521094
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2769880294799805
Epoch: 22, Steps: 62 | Train Loss: 0.4695900 Vali Loss: 1.1837845 Test Loss: 0.4517543
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.27280592918396
Epoch: 23, Steps: 62 | Train Loss: 0.4695008 Vali Loss: 1.1816452 Test Loss: 0.4516063
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.2174055576324463
Epoch: 24, Steps: 62 | Train Loss: 0.4692196 Vali Loss: 1.1755019 Test Loss: 0.4513454
Validation loss decreased (1.180746 --> 1.175502).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.1771512031555176
Epoch: 25, Steps: 62 | Train Loss: 0.4690799 Vali Loss: 1.1798021 Test Loss: 0.4510994
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2323479652404785
Epoch: 26, Steps: 62 | Train Loss: 0.4689497 Vali Loss: 1.1789160 Test Loss: 0.4509999
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.2485547065734863
Epoch: 27, Steps: 62 | Train Loss: 0.4688253 Vali Loss: 1.1810392 Test Loss: 0.4508997
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.203434705734253
Epoch: 28, Steps: 62 | Train Loss: 0.4687334 Vali Loss: 1.1699883 Test Loss: 0.4507983
Validation loss decreased (1.175502 --> 1.169988).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.2670972347259521
Epoch: 29, Steps: 62 | Train Loss: 0.4686441 Vali Loss: 1.1748719 Test Loss: 0.4507294
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.2113807201385498
Epoch: 30, Steps: 62 | Train Loss: 0.4686342 Vali Loss: 1.1728406 Test Loss: 0.4505790
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.2416386604309082
Epoch: 31, Steps: 62 | Train Loss: 0.4682498 Vali Loss: 1.1750424 Test Loss: 0.4505225
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.1961994171142578
Epoch: 32, Steps: 62 | Train Loss: 0.4683730 Vali Loss: 1.1766975 Test Loss: 0.4504797
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.2030673027038574
Epoch: 33, Steps: 62 | Train Loss: 0.4683673 Vali Loss: 1.1788130 Test Loss: 0.4504048
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.2833952903747559
Epoch: 34, Steps: 62 | Train Loss: 0.4679711 Vali Loss: 1.1756968 Test Loss: 0.4503589
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.0835497379302979
Epoch: 35, Steps: 62 | Train Loss: 0.4681499 Vali Loss: 1.1696707 Test Loss: 0.4503422
Validation loss decreased (1.169988 --> 1.169671).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.4129703044891357
Epoch: 36, Steps: 62 | Train Loss: 0.4681257 Vali Loss: 1.1719933 Test Loss: 0.4503047
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.4646344184875488
Epoch: 37, Steps: 62 | Train Loss: 0.4680820 Vali Loss: 1.1710449 Test Loss: 0.4502644
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.21815824508667
Epoch: 38, Steps: 62 | Train Loss: 0.4681441 Vali Loss: 1.1744984 Test Loss: 0.4502138
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.310889720916748
Epoch: 39, Steps: 62 | Train Loss: 0.4680645 Vali Loss: 1.1710474 Test Loss: 0.4501621
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.1815667152404785
Epoch: 40, Steps: 62 | Train Loss: 0.4678223 Vali Loss: 1.1759623 Test Loss: 0.4501557
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.194077730178833
Epoch: 41, Steps: 62 | Train Loss: 0.4679884 Vali Loss: 1.1758070 Test Loss: 0.4501703
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.1784074306488037
Epoch: 42, Steps: 62 | Train Loss: 0.4679203 Vali Loss: 1.1732988 Test Loss: 0.4501421
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.3394176959991455
Epoch: 43, Steps: 62 | Train Loss: 0.4678643 Vali Loss: 1.1737761 Test Loss: 0.4501386
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.182701826095581
Epoch: 44, Steps: 62 | Train Loss: 0.4677158 Vali Loss: 1.1709704 Test Loss: 0.4501186
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.1363954544067383
Epoch: 45, Steps: 62 | Train Loss: 0.4678198 Vali Loss: 1.1718885 Test Loss: 0.4500785
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.24880051612854
Epoch: 46, Steps: 62 | Train Loss: 0.4676542 Vali Loss: 1.1699877 Test Loss: 0.4500811
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.208897352218628
Epoch: 47, Steps: 62 | Train Loss: 0.4677737 Vali Loss: 1.1730120 Test Loss: 0.4500771
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.165555477142334
Epoch: 48, Steps: 62 | Train Loss: 0.4677850 Vali Loss: 1.1720221 Test Loss: 0.4500590
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.070199728012085
Epoch: 49, Steps: 62 | Train Loss: 0.4675909 Vali Loss: 1.1753753 Test Loss: 0.4500535
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.3129615783691406
Epoch: 50, Steps: 62 | Train Loss: 0.4675593 Vali Loss: 1.1720570 Test Loss: 0.4500495
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.2305407524108887
Epoch: 51, Steps: 62 | Train Loss: 0.4675848 Vali Loss: 1.1671242 Test Loss: 0.4500316
Validation loss decreased (1.169671 --> 1.167124).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.1921405792236328
Epoch: 52, Steps: 62 | Train Loss: 0.4674322 Vali Loss: 1.1763128 Test Loss: 0.4500424
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.239922046661377
Epoch: 53, Steps: 62 | Train Loss: 0.4674869 Vali Loss: 1.1732831 Test Loss: 0.4500262
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.1199355125427246
Epoch: 54, Steps: 62 | Train Loss: 0.4675983 Vali Loss: 1.1732703 Test Loss: 0.4500251
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.2696850299835205
Epoch: 55, Steps: 62 | Train Loss: 0.4676207 Vali Loss: 1.1689100 Test Loss: 0.4500083
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.2720191478729248
Epoch: 56, Steps: 62 | Train Loss: 0.4675475 Vali Loss: 1.1747884 Test Loss: 0.4500162
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.2132833003997803
Epoch: 57, Steps: 62 | Train Loss: 0.4676463 Vali Loss: 1.1695278 Test Loss: 0.4500127
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.2653827667236328
Epoch: 58, Steps: 62 | Train Loss: 0.4675669 Vali Loss: 1.1742419 Test Loss: 0.4499926
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.2649271488189697
Epoch: 59, Steps: 62 | Train Loss: 0.4675126 Vali Loss: 1.1677215 Test Loss: 0.4499935
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.2543919086456299
Epoch: 60, Steps: 62 | Train Loss: 0.4675275 Vali Loss: 1.1699393 Test Loss: 0.4499880
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.2415778636932373
Epoch: 61, Steps: 62 | Train Loss: 0.4674720 Vali Loss: 1.1712798 Test Loss: 0.4499939
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.2424774169921875
Epoch: 62, Steps: 62 | Train Loss: 0.4675884 Vali Loss: 1.1673384 Test Loss: 0.4499844
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.2679741382598877
Epoch: 63, Steps: 62 | Train Loss: 0.4673837 Vali Loss: 1.1714202 Test Loss: 0.4499770
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.2054369449615479
Epoch: 64, Steps: 62 | Train Loss: 0.4673425 Vali Loss: 1.1741931 Test Loss: 0.4499788
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.2251434326171875
Epoch: 65, Steps: 62 | Train Loss: 0.4674751 Vali Loss: 1.1708499 Test Loss: 0.4499747
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.3368654251098633
Epoch: 66, Steps: 62 | Train Loss: 0.4674023 Vali Loss: 1.1731235 Test Loss: 0.4499768
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.3196666240692139
Epoch: 67, Steps: 62 | Train Loss: 0.4674228 Vali Loss: 1.1742222 Test Loss: 0.4499786
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.2935888767242432
Epoch: 68, Steps: 62 | Train Loss: 0.4674565 Vali Loss: 1.1697824 Test Loss: 0.4499729
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3141918182373047
Epoch: 69, Steps: 62 | Train Loss: 0.4674818 Vali Loss: 1.1698859 Test Loss: 0.4499663
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.2750310897827148
Epoch: 70, Steps: 62 | Train Loss: 0.4673437 Vali Loss: 1.1698465 Test Loss: 0.4499723
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.3483071327209473
Epoch: 71, Steps: 62 | Train Loss: 0.4673748 Vali Loss: 1.1676372 Test Loss: 0.4499752
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4489172697067261, mae:0.447919100522995, rse:0.63787442445755, corr:[0.24474308 0.25522965 0.25805098 0.25618753 0.25310776 0.2508971
 0.24994268 0.24999036 0.25021046 0.25021225 0.24988687 0.2492909
 0.24877042 0.2485631  0.24864729 0.24892972 0.24918184 0.24928463
 0.24925548 0.2491237  0.24909608 0.24927053 0.24956743 0.24982992
 0.24990594 0.24966079 0.24915247 0.24852003 0.24785037 0.24724512
 0.24675502 0.24638362 0.24612436 0.24597472 0.24590097 0.24593322
 0.24607074 0.2463441  0.2467168  0.24702416 0.24718928 0.24722797
 0.24723308 0.24728143 0.2474305  0.24765137 0.2478695  0.2478322
 0.24735364 0.24647823 0.24522404 0.24403247 0.24311097 0.24243061
 0.24208948 0.24197234 0.24185556 0.24162991 0.24128808 0.24099243
 0.24074537 0.24069262 0.24080408 0.2410403  0.2412908  0.2415177
 0.24170719 0.2416626  0.24156313 0.24149016 0.24140577 0.2411921
 0.24083905 0.2403604  0.2398493  0.23941348 0.23905517 0.23873395
 0.23850355 0.23830537 0.23808183 0.23780137 0.23747571 0.23720019
 0.23704305 0.23698618 0.23692046 0.23678848 0.23658808 0.23639683
 0.23623754 0.23616882 0.23621045 0.23655578 0.23713113 0.2377456
 0.23834673 0.23877016 0.23899019 0.23899455 0.23886807 0.23865254
 0.23840864 0.23820272 0.23798415 0.23780075 0.23768382 0.23768343
 0.23779775 0.23798905 0.23815969 0.23827624 0.23822078 0.23807052
 0.2379446  0.23780559 0.2376838  0.23759966 0.2374894  0.23724823
 0.23683614 0.23622033 0.23552938 0.2348885  0.2344791  0.23421635
 0.23406363 0.23397687 0.23379482 0.23354459 0.23336343 0.23336223
 0.23350188 0.2336928  0.23391932 0.23404267 0.2340048  0.23386876
 0.23370174 0.2335502  0.23348577 0.23355274 0.23366179 0.23358911
 0.2332962  0.23267312 0.23185956 0.23104924 0.23055236 0.23032363
 0.23032919 0.23050568 0.23065664 0.23070383 0.23061208 0.23048913
 0.23037532 0.230359   0.23046038 0.23060673 0.23055738 0.23049137
 0.2303928  0.23028553 0.2302593  0.23039429 0.23069996 0.23105563
 0.23139724 0.23155607 0.23140004 0.23100159 0.2305899  0.23036394
 0.23036917 0.23053911 0.23067844 0.23076564 0.2307589  0.23066774
 0.23052464 0.23041183 0.23040912 0.23055278 0.23082687 0.23121448
 0.23158728 0.23181486 0.2319076  0.2318825  0.23166712 0.23124678
 0.23063977 0.2299272  0.22910444 0.22828862 0.2275888  0.22705343
 0.22669657 0.22663096 0.22670075 0.22679064 0.22684865 0.2269177
 0.22697811 0.22701707 0.22707576 0.22712612 0.22718915 0.22726054
 0.2273279  0.22724155 0.22707118 0.22690517 0.22672862 0.22657745
 0.22643152 0.22613615 0.22583745 0.22551833 0.2253359  0.22528097
 0.22525567 0.22521305 0.22514026 0.22497803 0.22474173 0.22460039
 0.22456302 0.22452211 0.22454345 0.22455215 0.22441006 0.2242427
 0.22417323 0.22416452 0.22426914 0.22446308 0.2247136  0.22477147
 0.22472247 0.22446367 0.2240296  0.22363983 0.2234464  0.22350366
 0.22373149 0.22401981 0.22420968 0.22412479 0.22385208 0.22359699
 0.22349514 0.22357242 0.22392179 0.22432345 0.22462922 0.22479638
 0.22483446 0.22464466 0.22442731 0.22437023 0.22449175 0.22462451
 0.22461505 0.22431014 0.22364752 0.22282793 0.22202753 0.22146647
 0.22132568 0.22153036 0.22184968 0.2220015  0.22189906 0.22162396
 0.2213206  0.22109625 0.22098863 0.22102584 0.22115175 0.2213065
 0.22143261 0.22134799 0.22121859 0.22124605 0.2216328  0.22217084
 0.2228126  0.22334012 0.22353238 0.22328122 0.22284864 0.22249943
 0.22243372 0.2226579  0.22303145 0.22338459 0.22347644 0.22330655
 0.22293799 0.22246975 0.22218043 0.22214346 0.22234082 0.22267553
 0.22313876 0.22347182 0.22368295 0.22373414 0.22369279 0.22352263
 0.22340749 0.2233613  0.22299658 0.22243774 0.22188297 0.22133482
 0.22090694 0.22065142 0.22042847 0.22031704 0.22025296 0.22026202
 0.22035335 0.22031103 0.21999197 0.2196029  0.21913344 0.21893454
 0.21928832 0.22004962 0.22089939 0.22092907 0.21820082 0.20924479]
