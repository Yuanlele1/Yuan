Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_192_FITS_ETTh1_ftM_sl180_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8269
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=58, out_features=119, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6184192.0
params:  7021.0
Trainable parameters:  7021
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.1860673427581787
Epoch: 1, Steps: 64 | Train Loss: 0.7142968 Vali Loss: 1.3753490 Test Loss: 0.6811063
Validation loss decreased (inf --> 1.375349).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.1087646484375
Epoch: 2, Steps: 64 | Train Loss: 0.5345185 Vali Loss: 1.1824118 Test Loss: 0.5494025
Validation loss decreased (1.375349 --> 1.182412).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.0432145595550537
Epoch: 3, Steps: 64 | Train Loss: 0.4755169 Vali Loss: 1.1081775 Test Loss: 0.5032519
Validation loss decreased (1.182412 --> 1.108178).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 0.9636967182159424
Epoch: 4, Steps: 64 | Train Loss: 0.4515736 Vali Loss: 1.0717070 Test Loss: 0.4817274
Validation loss decreased (1.108178 --> 1.071707).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.0766632556915283
Epoch: 5, Steps: 64 | Train Loss: 0.4397799 Vali Loss: 1.0504344 Test Loss: 0.4690716
Validation loss decreased (1.071707 --> 1.050434).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.1193578243255615
Epoch: 6, Steps: 64 | Train Loss: 0.4310886 Vali Loss: 1.0361097 Test Loss: 0.4599714
Validation loss decreased (1.050434 --> 1.036110).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.1148443222045898
Epoch: 7, Steps: 64 | Train Loss: 0.4256709 Vali Loss: 1.0254501 Test Loss: 0.4534616
Validation loss decreased (1.036110 --> 1.025450).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.1369974613189697
Epoch: 8, Steps: 64 | Train Loss: 0.4217602 Vali Loss: 1.0167655 Test Loss: 0.4481777
Validation loss decreased (1.025450 --> 1.016765).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.1163837909698486
Epoch: 9, Steps: 64 | Train Loss: 0.4181169 Vali Loss: 1.0102223 Test Loss: 0.4443956
Validation loss decreased (1.016765 --> 1.010222).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.0758213996887207
Epoch: 10, Steps: 64 | Train Loss: 0.4160898 Vali Loss: 1.0049589 Test Loss: 0.4413163
Validation loss decreased (1.010222 --> 1.004959).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.247936725616455
Epoch: 11, Steps: 64 | Train Loss: 0.4138151 Vali Loss: 1.0004646 Test Loss: 0.4391636
Validation loss decreased (1.004959 --> 1.000465).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.0417840480804443
Epoch: 12, Steps: 64 | Train Loss: 0.4123287 Vali Loss: 0.9968909 Test Loss: 0.4371055
Validation loss decreased (1.000465 --> 0.996891).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.1637399196624756
Epoch: 13, Steps: 64 | Train Loss: 0.4110058 Vali Loss: 0.9940602 Test Loss: 0.4357973
Validation loss decreased (0.996891 --> 0.994060).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.245368480682373
Epoch: 14, Steps: 64 | Train Loss: 0.4098185 Vali Loss: 0.9912273 Test Loss: 0.4346620
Validation loss decreased (0.994060 --> 0.991227).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.3462562561035156
Epoch: 15, Steps: 64 | Train Loss: 0.4085991 Vali Loss: 0.9896650 Test Loss: 0.4336975
Validation loss decreased (0.991227 --> 0.989665).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.264634370803833
Epoch: 16, Steps: 64 | Train Loss: 0.4080687 Vali Loss: 0.9879764 Test Loss: 0.4331727
Validation loss decreased (0.989665 --> 0.987976).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.1692657470703125
Epoch: 17, Steps: 64 | Train Loss: 0.4079145 Vali Loss: 0.9856460 Test Loss: 0.4328214
Validation loss decreased (0.987976 --> 0.985646).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.1706748008728027
Epoch: 18, Steps: 64 | Train Loss: 0.4069447 Vali Loss: 0.9850434 Test Loss: 0.4324374
Validation loss decreased (0.985646 --> 0.985043).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.1205596923828125
Epoch: 19, Steps: 64 | Train Loss: 0.4069465 Vali Loss: 0.9832781 Test Loss: 0.4321495
Validation loss decreased (0.985043 --> 0.983278).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.0683538913726807
Epoch: 20, Steps: 64 | Train Loss: 0.4064815 Vali Loss: 0.9827153 Test Loss: 0.4320261
Validation loss decreased (0.983278 --> 0.982715).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.303598165512085
Epoch: 21, Steps: 64 | Train Loss: 0.4060932 Vali Loss: 0.9821406 Test Loss: 0.4317437
Validation loss decreased (0.982715 --> 0.982141).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.0384795665740967
Epoch: 22, Steps: 64 | Train Loss: 0.4057162 Vali Loss: 0.9810804 Test Loss: 0.4316447
Validation loss decreased (0.982141 --> 0.981080).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.127445936203003
Epoch: 23, Steps: 64 | Train Loss: 0.4048261 Vali Loss: 0.9806728 Test Loss: 0.4314581
Validation loss decreased (0.981080 --> 0.980673).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.0660533905029297
Epoch: 24, Steps: 64 | Train Loss: 0.4056974 Vali Loss: 0.9797779 Test Loss: 0.4315159
Validation loss decreased (0.980673 --> 0.979778).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.1060471534729004
Epoch: 25, Steps: 64 | Train Loss: 0.4058111 Vali Loss: 0.9797840 Test Loss: 0.4314838
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2031538486480713
Epoch: 26, Steps: 64 | Train Loss: 0.4050750 Vali Loss: 0.9788032 Test Loss: 0.4313999
Validation loss decreased (0.979778 --> 0.978803).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.2901463508605957
Epoch: 27, Steps: 64 | Train Loss: 0.4050826 Vali Loss: 0.9789435 Test Loss: 0.4314089
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.2179956436157227
Epoch: 28, Steps: 64 | Train Loss: 0.4046767 Vali Loss: 0.9783306 Test Loss: 0.4313972
Validation loss decreased (0.978803 --> 0.978331).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.1784489154815674
Epoch: 29, Steps: 64 | Train Loss: 0.4047455 Vali Loss: 0.9780269 Test Loss: 0.4313873
Validation loss decreased (0.978331 --> 0.978027).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.128594160079956
Epoch: 30, Steps: 64 | Train Loss: 0.4050252 Vali Loss: 0.9776939 Test Loss: 0.4314612
Validation loss decreased (0.978027 --> 0.977694).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.1595301628112793
Epoch: 31, Steps: 64 | Train Loss: 0.4046022 Vali Loss: 0.9774063 Test Loss: 0.4314789
Validation loss decreased (0.977694 --> 0.977406).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.1714189052581787
Epoch: 32, Steps: 64 | Train Loss: 0.4045438 Vali Loss: 0.9768422 Test Loss: 0.4314331
Validation loss decreased (0.977406 --> 0.976842).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.1424641609191895
Epoch: 33, Steps: 64 | Train Loss: 0.4048539 Vali Loss: 0.9765614 Test Loss: 0.4313960
Validation loss decreased (0.976842 --> 0.976561).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.0919044017791748
Epoch: 34, Steps: 64 | Train Loss: 0.4042052 Vali Loss: 0.9764730 Test Loss: 0.4314271
Validation loss decreased (0.976561 --> 0.976473).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.1769647598266602
Epoch: 35, Steps: 64 | Train Loss: 0.4037834 Vali Loss: 0.9763875 Test Loss: 0.4314291
Validation loss decreased (0.976473 --> 0.976388).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.189009189605713
Epoch: 36, Steps: 64 | Train Loss: 0.4040682 Vali Loss: 0.9760234 Test Loss: 0.4314268
Validation loss decreased (0.976388 --> 0.976023).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.1320223808288574
Epoch: 37, Steps: 64 | Train Loss: 0.4044827 Vali Loss: 0.9758022 Test Loss: 0.4314100
Validation loss decreased (0.976023 --> 0.975802).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.0156350135803223
Epoch: 38, Steps: 64 | Train Loss: 0.4046141 Vali Loss: 0.9753513 Test Loss: 0.4313929
Validation loss decreased (0.975802 --> 0.975351).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 0.9950978755950928
Epoch: 39, Steps: 64 | Train Loss: 0.4045091 Vali Loss: 0.9754630 Test Loss: 0.4314025
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.2421786785125732
Epoch: 40, Steps: 64 | Train Loss: 0.4039946 Vali Loss: 0.9753133 Test Loss: 0.4315489
Validation loss decreased (0.975351 --> 0.975313).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.1786737442016602
Epoch: 41, Steps: 64 | Train Loss: 0.4040996 Vali Loss: 0.9755172 Test Loss: 0.4314685
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.0565316677093506
Epoch: 42, Steps: 64 | Train Loss: 0.4043266 Vali Loss: 0.9752976 Test Loss: 0.4314295
Validation loss decreased (0.975313 --> 0.975298).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.1007184982299805
Epoch: 43, Steps: 64 | Train Loss: 0.4041913 Vali Loss: 0.9751412 Test Loss: 0.4314518
Validation loss decreased (0.975298 --> 0.975141).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 0.9980370998382568
Epoch: 44, Steps: 64 | Train Loss: 0.4038140 Vali Loss: 0.9748272 Test Loss: 0.4314445
Validation loss decreased (0.975141 --> 0.974827).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.1400010585784912
Epoch: 45, Steps: 64 | Train Loss: 0.4038745 Vali Loss: 0.9745867 Test Loss: 0.4315223
Validation loss decreased (0.974827 --> 0.974587).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.0702552795410156
Epoch: 46, Steps: 64 | Train Loss: 0.4040017 Vali Loss: 0.9744444 Test Loss: 0.4315240
Validation loss decreased (0.974587 --> 0.974444).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.0575287342071533
Epoch: 47, Steps: 64 | Train Loss: 0.4042072 Vali Loss: 0.9747633 Test Loss: 0.4314629
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.112633228302002
Epoch: 48, Steps: 64 | Train Loss: 0.4043450 Vali Loss: 0.9742646 Test Loss: 0.4314620
Validation loss decreased (0.974444 --> 0.974265).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.0394060611724854
Epoch: 49, Steps: 64 | Train Loss: 0.4042300 Vali Loss: 0.9745196 Test Loss: 0.4315214
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2072746753692627
Epoch: 50, Steps: 64 | Train Loss: 0.4037133 Vali Loss: 0.9741086 Test Loss: 0.4315219
Validation loss decreased (0.974265 --> 0.974109).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.1579370498657227
Epoch: 51, Steps: 64 | Train Loss: 0.4040873 Vali Loss: 0.9744143 Test Loss: 0.4315449
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.1324098110198975
Epoch: 52, Steps: 64 | Train Loss: 0.4040189 Vali Loss: 0.9739822 Test Loss: 0.4315158
Validation loss decreased (0.974109 --> 0.973982).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.0881052017211914
Epoch: 53, Steps: 64 | Train Loss: 0.4041969 Vali Loss: 0.9738576 Test Loss: 0.4315328
Validation loss decreased (0.973982 --> 0.973858).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.1748998165130615
Epoch: 54, Steps: 64 | Train Loss: 0.4038158 Vali Loss: 0.9742936 Test Loss: 0.4315366
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.0873816013336182
Epoch: 55, Steps: 64 | Train Loss: 0.4043337 Vali Loss: 0.9739822 Test Loss: 0.4315343
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.055443525314331
Epoch: 56, Steps: 64 | Train Loss: 0.4035984 Vali Loss: 0.9736243 Test Loss: 0.4315815
Validation loss decreased (0.973858 --> 0.973624).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.0392544269561768
Epoch: 57, Steps: 64 | Train Loss: 0.4038481 Vali Loss: 0.9735839 Test Loss: 0.4315190
Validation loss decreased (0.973624 --> 0.973584).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.0041413307189941
Epoch: 58, Steps: 64 | Train Loss: 0.4038697 Vali Loss: 0.9739930 Test Loss: 0.4315189
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.0881116390228271
Epoch: 59, Steps: 64 | Train Loss: 0.4040863 Vali Loss: 0.9735048 Test Loss: 0.4315640
Validation loss decreased (0.973584 --> 0.973505).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.0742261409759521
Epoch: 60, Steps: 64 | Train Loss: 0.4034345 Vali Loss: 0.9740497 Test Loss: 0.4315723
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.1944994926452637
Epoch: 61, Steps: 64 | Train Loss: 0.4040007 Vali Loss: 0.9740084 Test Loss: 0.4315844
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.175783395767212
Epoch: 62, Steps: 64 | Train Loss: 0.4044525 Vali Loss: 0.9738302 Test Loss: 0.4315642
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.068058967590332
Epoch: 63, Steps: 64 | Train Loss: 0.4038941 Vali Loss: 0.9734982 Test Loss: 0.4315660
Validation loss decreased (0.973505 --> 0.973498).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.1720311641693115
Epoch: 64, Steps: 64 | Train Loss: 0.4040800 Vali Loss: 0.9735405 Test Loss: 0.4315782
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.1476852893829346
Epoch: 65, Steps: 64 | Train Loss: 0.4036904 Vali Loss: 0.9733068 Test Loss: 0.4315447
Validation loss decreased (0.973498 --> 0.973307).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.165198802947998
Epoch: 66, Steps: 64 | Train Loss: 0.4038725 Vali Loss: 0.9737554 Test Loss: 0.4315799
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.0670757293701172
Epoch: 67, Steps: 64 | Train Loss: 0.4039213 Vali Loss: 0.9733127 Test Loss: 0.4315666
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.1821668148040771
Epoch: 68, Steps: 64 | Train Loss: 0.4039269 Vali Loss: 0.9737598 Test Loss: 0.4316024
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.1342129707336426
Epoch: 69, Steps: 64 | Train Loss: 0.4041747 Vali Loss: 0.9737129 Test Loss: 0.4315817
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.1294276714324951
Epoch: 70, Steps: 64 | Train Loss: 0.4036514 Vali Loss: 0.9736078 Test Loss: 0.4315970
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.1130039691925049
Epoch: 71, Steps: 64 | Train Loss: 0.4034161 Vali Loss: 0.9735981 Test Loss: 0.4316108
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.1750257015228271
Epoch: 72, Steps: 64 | Train Loss: 0.4037167 Vali Loss: 0.9735895 Test Loss: 0.4315946
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.149444580078125
Epoch: 73, Steps: 64 | Train Loss: 0.4036247 Vali Loss: 0.9732504 Test Loss: 0.4315784
Validation loss decreased (0.973307 --> 0.973250).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.1684770584106445
Epoch: 74, Steps: 64 | Train Loss: 0.4044389 Vali Loss: 0.9735721 Test Loss: 0.4316069
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.057082176208496
Epoch: 75, Steps: 64 | Train Loss: 0.4042580 Vali Loss: 0.9735329 Test Loss: 0.4315934
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.1978909969329834
Epoch: 76, Steps: 64 | Train Loss: 0.4033399 Vali Loss: 0.9735430 Test Loss: 0.4315934
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.0654606819152832
Epoch: 77, Steps: 64 | Train Loss: 0.4037515 Vali Loss: 0.9733909 Test Loss: 0.4315946
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.0973632335662842
Epoch: 78, Steps: 64 | Train Loss: 0.4035436 Vali Loss: 0.9734812 Test Loss: 0.4315893
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.137068748474121
Epoch: 79, Steps: 64 | Train Loss: 0.4032589 Vali Loss: 0.9735005 Test Loss: 0.4315834
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.1066844463348389
Epoch: 80, Steps: 64 | Train Loss: 0.4032702 Vali Loss: 0.9726964 Test Loss: 0.4315882
Validation loss decreased (0.973250 --> 0.972696).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.1339795589447021
Epoch: 81, Steps: 64 | Train Loss: 0.4037880 Vali Loss: 0.9728114 Test Loss: 0.4315762
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.0832698345184326
Epoch: 82, Steps: 64 | Train Loss: 0.4039408 Vali Loss: 0.9734731 Test Loss: 0.4315991
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.0165069103240967
Epoch: 83, Steps: 64 | Train Loss: 0.4037878 Vali Loss: 0.9731188 Test Loss: 0.4316024
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.059424877166748
Epoch: 84, Steps: 64 | Train Loss: 0.4036343 Vali Loss: 0.9731902 Test Loss: 0.4316013
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.0404422283172607
Epoch: 85, Steps: 64 | Train Loss: 0.4033321 Vali Loss: 0.9733471 Test Loss: 0.4316007
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.1380302906036377
Epoch: 86, Steps: 64 | Train Loss: 0.4038279 Vali Loss: 0.9735366 Test Loss: 0.4315947
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.0482397079467773
Epoch: 87, Steps: 64 | Train Loss: 0.4038150 Vali Loss: 0.9734327 Test Loss: 0.4315961
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.1474967002868652
Epoch: 88, Steps: 64 | Train Loss: 0.4039743 Vali Loss: 0.9721254 Test Loss: 0.4315988
Validation loss decreased (0.972696 --> 0.972125).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.0560357570648193
Epoch: 89, Steps: 64 | Train Loss: 0.4037463 Vali Loss: 0.9733551 Test Loss: 0.4315942
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.0477981567382812
Epoch: 90, Steps: 64 | Train Loss: 0.4040946 Vali Loss: 0.9728942 Test Loss: 0.4315971
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.0762813091278076
Epoch: 91, Steps: 64 | Train Loss: 0.4036337 Vali Loss: 0.9725935 Test Loss: 0.4316011
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.1003081798553467
Epoch: 92, Steps: 64 | Train Loss: 0.4038491 Vali Loss: 0.9731627 Test Loss: 0.4316012
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.005488395690918
Epoch: 93, Steps: 64 | Train Loss: 0.4039659 Vali Loss: 0.9728834 Test Loss: 0.4315976
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.0398907661437988
Epoch: 94, Steps: 64 | Train Loss: 0.4038539 Vali Loss: 0.9733334 Test Loss: 0.4316029
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.0729179382324219
Epoch: 95, Steps: 64 | Train Loss: 0.4036497 Vali Loss: 0.9731003 Test Loss: 0.4316022
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.1198978424072266
Epoch: 96, Steps: 64 | Train Loss: 0.4037975 Vali Loss: 0.9733587 Test Loss: 0.4316017
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.0747132301330566
Epoch: 97, Steps: 64 | Train Loss: 0.4037531 Vali Loss: 0.9725993 Test Loss: 0.4315990
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.126894235610962
Epoch: 98, Steps: 64 | Train Loss: 0.4031913 Vali Loss: 0.9728080 Test Loss: 0.4316012
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.0078179836273193
Epoch: 99, Steps: 64 | Train Loss: 0.4036742 Vali Loss: 0.9732639 Test Loss: 0.4316045
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.0749597549438477
Epoch: 100, Steps: 64 | Train Loss: 0.4037463 Vali Loss: 0.9730514 Test Loss: 0.4316092
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_180_192_FITS_ETTh1_ftM_sl180_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.42522600293159485, mae:0.4193587303161621, rse:0.6192517876625061, corr:[0.26376265 0.26772606 0.2677469  0.26850578 0.26578924 0.26349238
 0.2635712  0.26325035 0.26269165 0.26353043 0.2629143  0.2621504
 0.26253855 0.26175594 0.2612178  0.2617015  0.2617308  0.26160458
 0.26186675 0.26172495 0.26133677 0.26147202 0.2619152  0.2616902
 0.26069537 0.25994456 0.25940853 0.25897694 0.25827703 0.2578461
 0.25788972 0.25755444 0.25688532 0.25701916 0.2573311  0.25732937
 0.2575006  0.25773624 0.25775334 0.2578716  0.25824913 0.2584357
 0.2585859  0.25885576 0.25885028 0.25890726 0.259425   0.2595367
 0.25837025 0.25658125 0.25483978 0.25388336 0.25278184 0.25132555
 0.250766   0.2507942  0.25048298 0.25064036 0.25073257 0.25106713
 0.25140333 0.2512724  0.25106555 0.25104403 0.25121045 0.251229
 0.25135115 0.2514267  0.25119993 0.2510925  0.25113016 0.25052276
 0.24934459 0.24820173 0.24692257 0.24640043 0.24630639 0.24589092
 0.24556331 0.24540521 0.24520937 0.24512649 0.24482806 0.24480113
 0.24501926 0.24503055 0.24516492 0.24520306 0.24508661 0.24509472
 0.24513473 0.24502009 0.2447873  0.24498335 0.24545339 0.24538839
 0.24462777 0.24400774 0.24334665 0.2425255  0.24207705 0.24171504
 0.24150768 0.24166125 0.2417572  0.2420812  0.24209683 0.24210519
 0.24219278 0.2419132  0.24174577 0.24191394 0.24209301 0.24204847
 0.2421253  0.24222882 0.24204431 0.24194436 0.24201961 0.24161856
 0.24051838 0.23938638 0.23820566 0.23732316 0.23642506 0.23572466
 0.2356749  0.23600541 0.23596434 0.23613511 0.23632808 0.23675115
 0.2373528  0.23747087 0.23731227 0.23715244 0.23738831 0.23753643
 0.23751478 0.23766236 0.23763368 0.23755689 0.23763752 0.2374092
 0.2364716  0.23527293 0.23419926 0.23331492 0.23247804 0.23166063
 0.23165105 0.23196422 0.23216262 0.23244265 0.23271899 0.23311917
 0.23344329 0.23349054 0.23351781 0.23323955 0.23317239 0.23324275
 0.23309752 0.23305464 0.23307918 0.23314068 0.23325846 0.23298198
 0.23198606 0.2308752  0.22994272 0.22955842 0.22911352 0.22837816
 0.22841644 0.22915694 0.2295479  0.23025861 0.23098694 0.23177242
 0.23248932 0.2325755  0.23269725 0.23237078 0.23185124 0.23221572
 0.23223959 0.23149937 0.23172155 0.23259394 0.23256783 0.23245578]
