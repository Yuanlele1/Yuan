Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=26, out_features=234, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5451264.0
params:  6318.0
Trainable parameters:  6318
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.945519208908081
Epoch: 1, Steps: 61 | Train Loss: 1.5348669 Vali Loss: 3.0037830 Test Loss: 1.6164986
Validation loss decreased (inf --> 3.003783).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.3447141647338867
Epoch: 2, Steps: 61 | Train Loss: 1.1497741 Vali Loss: 2.4474661 Test Loss: 1.1620598
Validation loss decreased (3.003783 --> 2.447466).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.697272539138794
Epoch: 3, Steps: 61 | Train Loss: 0.9343261 Vali Loss: 2.1483173 Test Loss: 0.9172334
Validation loss decreased (2.447466 --> 2.148317).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9946043491363525
Epoch: 4, Steps: 61 | Train Loss: 0.8078119 Vali Loss: 1.9759624 Test Loss: 0.7737838
Validation loss decreased (2.148317 --> 1.975962).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.359577178955078
Epoch: 5, Steps: 61 | Train Loss: 0.7294392 Vali Loss: 1.8761501 Test Loss: 0.6843640
Validation loss decreased (1.975962 --> 1.876150).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.4750235080718994
Epoch: 6, Steps: 61 | Train Loss: 0.6793188 Vali Loss: 1.7935276 Test Loss: 0.6260900
Validation loss decreased (1.876150 --> 1.793528).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.689401865005493
Epoch: 7, Steps: 61 | Train Loss: 0.6457771 Vali Loss: 1.7362610 Test Loss: 0.5872470
Validation loss decreased (1.793528 --> 1.736261).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.8214383125305176
Epoch: 8, Steps: 61 | Train Loss: 0.6229820 Vali Loss: 1.7181462 Test Loss: 0.5598090
Validation loss decreased (1.736261 --> 1.718146).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.982093095779419
Epoch: 9, Steps: 61 | Train Loss: 0.6066407 Vali Loss: 1.6921195 Test Loss: 0.5402483
Validation loss decreased (1.718146 --> 1.692119).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.825148820877075
Epoch: 10, Steps: 61 | Train Loss: 0.5954595 Vali Loss: 1.6707468 Test Loss: 0.5262619
Validation loss decreased (1.692119 --> 1.670747).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.0657167434692383
Epoch: 11, Steps: 61 | Train Loss: 0.5870733 Vali Loss: 1.6569917 Test Loss: 0.5157098
Validation loss decreased (1.670747 --> 1.656992).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.7844367027282715
Epoch: 12, Steps: 61 | Train Loss: 0.5800981 Vali Loss: 1.6455607 Test Loss: 0.5077440
Validation loss decreased (1.656992 --> 1.645561).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.447505950927734
Epoch: 13, Steps: 61 | Train Loss: 0.5756900 Vali Loss: 1.6344994 Test Loss: 0.5014689
Validation loss decreased (1.645561 --> 1.634499).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.3455471992492676
Epoch: 14, Steps: 61 | Train Loss: 0.5720840 Vali Loss: 1.6282204 Test Loss: 0.4966595
Validation loss decreased (1.634499 --> 1.628220).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.5567402839660645
Epoch: 15, Steps: 61 | Train Loss: 0.5687267 Vali Loss: 1.6222359 Test Loss: 0.4929293
Validation loss decreased (1.628220 --> 1.622236).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.4664430618286133
Epoch: 16, Steps: 61 | Train Loss: 0.5659181 Vali Loss: 1.6172749 Test Loss: 0.4898193
Validation loss decreased (1.622236 --> 1.617275).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.0813159942626953
Epoch: 17, Steps: 61 | Train Loss: 0.5644179 Vali Loss: 1.6082199 Test Loss: 0.4872786
Validation loss decreased (1.617275 --> 1.608220).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.3780019283294678
Epoch: 18, Steps: 61 | Train Loss: 0.5623260 Vali Loss: 1.6104349 Test Loss: 0.4852341
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0804247856140137
Epoch: 19, Steps: 61 | Train Loss: 0.5612939 Vali Loss: 1.6116027 Test Loss: 0.4835475
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.4446218013763428
Epoch: 20, Steps: 61 | Train Loss: 0.5602164 Vali Loss: 1.6061687 Test Loss: 0.4820534
Validation loss decreased (1.608220 --> 1.606169).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.966205596923828
Epoch: 21, Steps: 61 | Train Loss: 0.5587084 Vali Loss: 1.5975773 Test Loss: 0.4808880
Validation loss decreased (1.606169 --> 1.597577).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.139007091522217
Epoch: 22, Steps: 61 | Train Loss: 0.5581861 Vali Loss: 1.5924333 Test Loss: 0.4798490
Validation loss decreased (1.597577 --> 1.592433).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.9455018043518066
Epoch: 23, Steps: 61 | Train Loss: 0.5573044 Vali Loss: 1.5983217 Test Loss: 0.4789663
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.7247438430786133
Epoch: 24, Steps: 61 | Train Loss: 0.5565884 Vali Loss: 1.5963068 Test Loss: 0.4782526
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.944751501083374
Epoch: 25, Steps: 61 | Train Loss: 0.5559199 Vali Loss: 1.5897080 Test Loss: 0.4775320
Validation loss decreased (1.592433 --> 1.589708).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.688220262527466
Epoch: 26, Steps: 61 | Train Loss: 0.5547400 Vali Loss: 1.5880593 Test Loss: 0.4769427
Validation loss decreased (1.589708 --> 1.588059).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.603119373321533
Epoch: 27, Steps: 61 | Train Loss: 0.5544764 Vali Loss: 1.5890079 Test Loss: 0.4764516
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.4768528938293457
Epoch: 28, Steps: 61 | Train Loss: 0.5541518 Vali Loss: 1.5884986 Test Loss: 0.4759978
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.6162986755371094
Epoch: 29, Steps: 61 | Train Loss: 0.5539454 Vali Loss: 1.5927296 Test Loss: 0.4756372
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.4648866653442383
Epoch: 30, Steps: 61 | Train Loss: 0.5533816 Vali Loss: 1.5861206 Test Loss: 0.4752675
Validation loss decreased (1.588059 --> 1.586121).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.1350440979003906
Epoch: 31, Steps: 61 | Train Loss: 0.5531142 Vali Loss: 1.5820782 Test Loss: 0.4749254
Validation loss decreased (1.586121 --> 1.582078).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.5741193294525146
Epoch: 32, Steps: 61 | Train Loss: 0.5526546 Vali Loss: 1.5962244 Test Loss: 0.4746735
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.0553159713745117
Epoch: 33, Steps: 61 | Train Loss: 0.5525167 Vali Loss: 1.5881808 Test Loss: 0.4744203
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.4266531467437744
Epoch: 34, Steps: 61 | Train Loss: 0.5521867 Vali Loss: 1.5875374 Test Loss: 0.4741851
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.817601442337036
Epoch: 35, Steps: 61 | Train Loss: 0.5519666 Vali Loss: 1.5809038 Test Loss: 0.4740045
Validation loss decreased (1.582078 --> 1.580904).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.400160551071167
Epoch: 36, Steps: 61 | Train Loss: 0.5518412 Vali Loss: 1.5855186 Test Loss: 0.4738124
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.7153332233428955
Epoch: 37, Steps: 61 | Train Loss: 0.5515319 Vali Loss: 1.5851710 Test Loss: 0.4736625
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.4872071743011475
Epoch: 38, Steps: 61 | Train Loss: 0.5514441 Vali Loss: 1.5874877 Test Loss: 0.4735077
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.8880393505096436
Epoch: 39, Steps: 61 | Train Loss: 0.5511652 Vali Loss: 1.5815711 Test Loss: 0.4733749
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.7550017833709717
Epoch: 40, Steps: 61 | Train Loss: 0.5506672 Vali Loss: 1.5859811 Test Loss: 0.4732386
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.496830463409424
Epoch: 41, Steps: 61 | Train Loss: 0.5507177 Vali Loss: 1.5865715 Test Loss: 0.4731237
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.7791686058044434
Epoch: 42, Steps: 61 | Train Loss: 0.5505161 Vali Loss: 1.5805347 Test Loss: 0.4730397
Validation loss decreased (1.580904 --> 1.580535).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.214897394180298
Epoch: 43, Steps: 61 | Train Loss: 0.5503559 Vali Loss: 1.5818459 Test Loss: 0.4729509
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.0414230823516846
Epoch: 44, Steps: 61 | Train Loss: 0.5502272 Vali Loss: 1.5837822 Test Loss: 0.4728565
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.0463109016418457
Epoch: 45, Steps: 61 | Train Loss: 0.5501317 Vali Loss: 1.5772582 Test Loss: 0.4727655
Validation loss decreased (1.580535 --> 1.577258).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.117814064025879
Epoch: 46, Steps: 61 | Train Loss: 0.5500491 Vali Loss: 1.5771639 Test Loss: 0.4726997
Validation loss decreased (1.577258 --> 1.577164).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.4164037704467773
Epoch: 47, Steps: 61 | Train Loss: 0.5498082 Vali Loss: 1.5795479 Test Loss: 0.4726279
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.288603067398071
Epoch: 48, Steps: 61 | Train Loss: 0.5497644 Vali Loss: 1.5869294 Test Loss: 0.4725711
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.2474896907806396
Epoch: 49, Steps: 61 | Train Loss: 0.5500198 Vali Loss: 1.5867527 Test Loss: 0.4725130
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.98663330078125
Epoch: 50, Steps: 61 | Train Loss: 0.5496535 Vali Loss: 1.5806589 Test Loss: 0.4724712
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.3001418113708496
Epoch: 51, Steps: 61 | Train Loss: 0.5494888 Vali Loss: 1.5777943 Test Loss: 0.4724119
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.938307523727417
Epoch: 52, Steps: 61 | Train Loss: 0.5493914 Vali Loss: 1.5776436 Test Loss: 0.4723723
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.59836745262146
Epoch: 53, Steps: 61 | Train Loss: 0.5494598 Vali Loss: 1.5805991 Test Loss: 0.4723252
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.1280927658081055
Epoch: 54, Steps: 61 | Train Loss: 0.5495061 Vali Loss: 1.5771707 Test Loss: 0.4722940
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.547975540161133
Epoch: 55, Steps: 61 | Train Loss: 0.5493008 Vali Loss: 1.5794966 Test Loss: 0.4722520
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9336388111114502
Epoch: 56, Steps: 61 | Train Loss: 0.5495205 Vali Loss: 1.5869473 Test Loss: 0.4722092
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.9994730949401855
Epoch: 57, Steps: 61 | Train Loss: 0.5491962 Vali Loss: 1.5758890 Test Loss: 0.4721933
Validation loss decreased (1.577164 --> 1.575889).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.3961541652679443
Epoch: 58, Steps: 61 | Train Loss: 0.5491478 Vali Loss: 1.5784671 Test Loss: 0.4721595
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.9711592197418213
Epoch: 59, Steps: 61 | Train Loss: 0.5490472 Vali Loss: 1.5785856 Test Loss: 0.4721308
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.7717864513397217
Epoch: 60, Steps: 61 | Train Loss: 0.5491171 Vali Loss: 1.5880351 Test Loss: 0.4721099
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.995985269546509
Epoch: 61, Steps: 61 | Train Loss: 0.5490150 Vali Loss: 1.5753362 Test Loss: 0.4720905
Validation loss decreased (1.575889 --> 1.575336).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.5962131023406982
Epoch: 62, Steps: 61 | Train Loss: 0.5486841 Vali Loss: 1.5829641 Test Loss: 0.4720629
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.433582067489624
Epoch: 63, Steps: 61 | Train Loss: 0.5489411 Vali Loss: 1.5812906 Test Loss: 0.4720445
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.188035011291504
Epoch: 64, Steps: 61 | Train Loss: 0.5487228 Vali Loss: 1.5848356 Test Loss: 0.4720220
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.4876346588134766
Epoch: 65, Steps: 61 | Train Loss: 0.5486493 Vali Loss: 1.5820835 Test Loss: 0.4720082
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.205644369125366
Epoch: 66, Steps: 61 | Train Loss: 0.5489401 Vali Loss: 1.5794386 Test Loss: 0.4719909
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.3671061992645264
Epoch: 67, Steps: 61 | Train Loss: 0.5487533 Vali Loss: 1.5745119 Test Loss: 0.4719674
Validation loss decreased (1.575336 --> 1.574512).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.471855640411377
Epoch: 68, Steps: 61 | Train Loss: 0.5487293 Vali Loss: 1.5770380 Test Loss: 0.4719556
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.1136679649353027
Epoch: 69, Steps: 61 | Train Loss: 0.5486411 Vali Loss: 1.5770283 Test Loss: 0.4719385
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.8385331630706787
Epoch: 70, Steps: 61 | Train Loss: 0.5486761 Vali Loss: 1.5793358 Test Loss: 0.4719292
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.84323787689209
Epoch: 71, Steps: 61 | Train Loss: 0.5485852 Vali Loss: 1.5800519 Test Loss: 0.4719156
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 4.105630159378052
Epoch: 72, Steps: 61 | Train Loss: 0.5487592 Vali Loss: 1.5796335 Test Loss: 0.4719050
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 4.217704772949219
Epoch: 73, Steps: 61 | Train Loss: 0.5486472 Vali Loss: 1.5779036 Test Loss: 0.4718911
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 5.542337894439697
Epoch: 74, Steps: 61 | Train Loss: 0.5484683 Vali Loss: 1.5761597 Test Loss: 0.4718797
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.930259943008423
Epoch: 75, Steps: 61 | Train Loss: 0.5482495 Vali Loss: 1.5744452 Test Loss: 0.4718722
Validation loss decreased (1.574512 --> 1.574445).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.9725444316864014
Epoch: 76, Steps: 61 | Train Loss: 0.5484605 Vali Loss: 1.5806849 Test Loss: 0.4718603
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.6120378971099854
Epoch: 77, Steps: 61 | Train Loss: 0.5486769 Vali Loss: 1.5763531 Test Loss: 0.4718512
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 4.95365834236145
Epoch: 78, Steps: 61 | Train Loss: 0.5485547 Vali Loss: 1.5815381 Test Loss: 0.4718479
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 3.1743547916412354
Epoch: 79, Steps: 61 | Train Loss: 0.5486210 Vali Loss: 1.5794477 Test Loss: 0.4718380
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.393386125564575
Epoch: 80, Steps: 61 | Train Loss: 0.5482392 Vali Loss: 1.5801871 Test Loss: 0.4718322
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.5847668647766113
Epoch: 81, Steps: 61 | Train Loss: 0.5485381 Vali Loss: 1.5745541 Test Loss: 0.4718230
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.789752960205078
Epoch: 82, Steps: 61 | Train Loss: 0.5484320 Vali Loss: 1.5751615 Test Loss: 0.4718193
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.993373394012451
Epoch: 83, Steps: 61 | Train Loss: 0.5481406 Vali Loss: 1.5768206 Test Loss: 0.4718124
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.1708984375
Epoch: 84, Steps: 61 | Train Loss: 0.5483404 Vali Loss: 1.5795600 Test Loss: 0.4718049
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.8024077415466309
Epoch: 85, Steps: 61 | Train Loss: 0.5479986 Vali Loss: 1.5827054 Test Loss: 0.4718009
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.8381438255310059
Epoch: 86, Steps: 61 | Train Loss: 0.5483683 Vali Loss: 1.5798388 Test Loss: 0.4717943
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.708369016647339
Epoch: 87, Steps: 61 | Train Loss: 0.5482172 Vali Loss: 1.5774384 Test Loss: 0.4717896
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.9517934322357178
Epoch: 88, Steps: 61 | Train Loss: 0.5482840 Vali Loss: 1.5716903 Test Loss: 0.4717848
Validation loss decreased (1.574445 --> 1.571690).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.657020330429077
Epoch: 89, Steps: 61 | Train Loss: 0.5480599 Vali Loss: 1.5769086 Test Loss: 0.4717815
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.78916597366333
Epoch: 90, Steps: 61 | Train Loss: 0.5483658 Vali Loss: 1.5734400 Test Loss: 0.4717775
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.592773914337158
Epoch: 91, Steps: 61 | Train Loss: 0.5481519 Vali Loss: 1.5853081 Test Loss: 0.4717733
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.6261301040649414
Epoch: 92, Steps: 61 | Train Loss: 0.5480968 Vali Loss: 1.5744796 Test Loss: 0.4717691
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 3.3313047885894775
Epoch: 93, Steps: 61 | Train Loss: 0.5483027 Vali Loss: 1.5707259 Test Loss: 0.4717673
Validation loss decreased (1.571690 --> 1.570726).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.848245859146118
Epoch: 94, Steps: 61 | Train Loss: 0.5483733 Vali Loss: 1.5749552 Test Loss: 0.4717642
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 4.098348617553711
Epoch: 95, Steps: 61 | Train Loss: 0.5483162 Vali Loss: 1.5804056 Test Loss: 0.4717587
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.0287225246429443
Epoch: 96, Steps: 61 | Train Loss: 0.5479742 Vali Loss: 1.5736949 Test Loss: 0.4717576
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.829740285873413
Epoch: 97, Steps: 61 | Train Loss: 0.5481588 Vali Loss: 1.5795119 Test Loss: 0.4717546
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.4603583812713623
Epoch: 98, Steps: 61 | Train Loss: 0.5481117 Vali Loss: 1.5737956 Test Loss: 0.4717520
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 3.905184745788574
Epoch: 99, Steps: 61 | Train Loss: 0.5482379 Vali Loss: 1.5794303 Test Loss: 0.4717487
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 3.189918279647827
Epoch: 100, Steps: 61 | Train Loss: 0.5482309 Vali Loss: 1.5745010 Test Loss: 0.4717469
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=26, out_features=234, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5451264.0
params:  6318.0
Trainable parameters:  6318
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.444694757461548
Epoch: 1, Steps: 61 | Train Loss: 0.6133669 Vali Loss: 1.5761549 Test Loss: 0.4715369
Validation loss decreased (inf --> 1.576155).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.922898769378662
Epoch: 2, Steps: 61 | Train Loss: 0.6119713 Vali Loss: 1.5698850 Test Loss: 0.4715213
Validation loss decreased (1.576155 --> 1.569885).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.2545523643493652
Epoch: 3, Steps: 61 | Train Loss: 0.6111313 Vali Loss: 1.5662173 Test Loss: 0.4713652
Validation loss decreased (1.569885 --> 1.566217).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.3314926624298096
Epoch: 4, Steps: 61 | Train Loss: 0.6111951 Vali Loss: 1.5671878 Test Loss: 0.4714334
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.2724578380584717
Epoch: 5, Steps: 61 | Train Loss: 0.6106351 Vali Loss: 1.5702941 Test Loss: 0.4713980
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.7026636600494385
Epoch: 6, Steps: 61 | Train Loss: 0.6107884 Vali Loss: 1.5711700 Test Loss: 0.4714046
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.30027437210083
Epoch: 7, Steps: 61 | Train Loss: 0.6103190 Vali Loss: 1.5647161 Test Loss: 0.4716249
Validation loss decreased (1.566217 --> 1.564716).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.0145130157470703
Epoch: 8, Steps: 61 | Train Loss: 0.6105014 Vali Loss: 1.5715410 Test Loss: 0.4717868
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.244108200073242
Epoch: 9, Steps: 61 | Train Loss: 0.6102334 Vali Loss: 1.5702146 Test Loss: 0.4718846
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.511767864227295
Epoch: 10, Steps: 61 | Train Loss: 0.6103433 Vali Loss: 1.5709826 Test Loss: 0.4720118
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.37036395072937
Epoch: 11, Steps: 61 | Train Loss: 0.6103266 Vali Loss: 1.5692245 Test Loss: 0.4720363
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3352890014648438
Epoch: 12, Steps: 61 | Train Loss: 0.6100742 Vali Loss: 1.5611390 Test Loss: 0.4723020
Validation loss decreased (1.564716 --> 1.561139).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.846850395202637
Epoch: 13, Steps: 61 | Train Loss: 0.6101352 Vali Loss: 1.5619268 Test Loss: 0.4722652
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.458242654800415
Epoch: 14, Steps: 61 | Train Loss: 0.6099224 Vali Loss: 1.5666271 Test Loss: 0.4724216
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.5093319416046143
Epoch: 15, Steps: 61 | Train Loss: 0.6100021 Vali Loss: 1.5617154 Test Loss: 0.4724346
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 6.775251626968384
Epoch: 16, Steps: 61 | Train Loss: 0.6098535 Vali Loss: 1.5618708 Test Loss: 0.4725817
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.149831056594849
Epoch: 17, Steps: 61 | Train Loss: 0.6098441 Vali Loss: 1.5606405 Test Loss: 0.4726686
Validation loss decreased (1.561139 --> 1.560640).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8028311729431152
Epoch: 18, Steps: 61 | Train Loss: 0.6097529 Vali Loss: 1.5702481 Test Loss: 0.4728371
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.783036470413208
Epoch: 19, Steps: 61 | Train Loss: 0.6099827 Vali Loss: 1.5650973 Test Loss: 0.4728252
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.570990562438965
Epoch: 20, Steps: 61 | Train Loss: 0.6094397 Vali Loss: 1.5722303 Test Loss: 0.4728849
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.209195375442505
Epoch: 21, Steps: 61 | Train Loss: 0.6096436 Vali Loss: 1.5717716 Test Loss: 0.4729393
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.536959648132324
Epoch: 22, Steps: 61 | Train Loss: 0.6097924 Vali Loss: 1.5617194 Test Loss: 0.4730022
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.917790412902832
Epoch: 23, Steps: 61 | Train Loss: 0.6095570 Vali Loss: 1.5656264 Test Loss: 0.4730714
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.953524351119995
Epoch: 24, Steps: 61 | Train Loss: 0.6096565 Vali Loss: 1.5676771 Test Loss: 0.4731408
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.1085121631622314
Epoch: 25, Steps: 61 | Train Loss: 0.6096049 Vali Loss: 1.5687258 Test Loss: 0.4731641
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.4776501655578613
Epoch: 26, Steps: 61 | Train Loss: 0.6098667 Vali Loss: 1.5644144 Test Loss: 0.4732122
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.026209592819214
Epoch: 27, Steps: 61 | Train Loss: 0.6095333 Vali Loss: 1.5697809 Test Loss: 0.4732886
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.8926639556884766
Epoch: 28, Steps: 61 | Train Loss: 0.6097919 Vali Loss: 1.5697234 Test Loss: 0.4732536
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.917603015899658
Epoch: 29, Steps: 61 | Train Loss: 0.6096669 Vali Loss: 1.5696726 Test Loss: 0.4733678
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.8750312328338623
Epoch: 30, Steps: 61 | Train Loss: 0.6096483 Vali Loss: 1.5712843 Test Loss: 0.4733661
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.721182346343994
Epoch: 31, Steps: 61 | Train Loss: 0.6097649 Vali Loss: 1.5666599 Test Loss: 0.4734108
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.701530694961548
Epoch: 32, Steps: 61 | Train Loss: 0.6094982 Vali Loss: 1.5648313 Test Loss: 0.4734790
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.1511433124542236
Epoch: 33, Steps: 61 | Train Loss: 0.6094879 Vali Loss: 1.5655642 Test Loss: 0.4734538
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.420346260070801
Epoch: 34, Steps: 61 | Train Loss: 0.6095103 Vali Loss: 1.5660007 Test Loss: 0.4735085
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.197695732116699
Epoch: 35, Steps: 61 | Train Loss: 0.6096526 Vali Loss: 1.5725001 Test Loss: 0.4735528
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.119839906692505
Epoch: 36, Steps: 61 | Train Loss: 0.6093736 Vali Loss: 1.5628698 Test Loss: 0.4735572
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.9464313983917236
Epoch: 37, Steps: 61 | Train Loss: 0.6096303 Vali Loss: 1.5674736 Test Loss: 0.4735722
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4714386761188507, mae:0.462190181016922, rse:0.6573014259338379, corr:[0.22751755 0.23036495 0.22885633 0.22886689 0.22664292 0.2239072
 0.22434205 0.22473684 0.22410217 0.22437072 0.2244247  0.22361512
 0.22317614 0.22300847 0.22257303 0.22263493 0.22297838 0.2227753
 0.22258517 0.22274671 0.22261006 0.22254089 0.22239101 0.22218919
 0.22070329 0.21954143 0.21954286 0.22009856 0.22005807 0.22016943
 0.22066066 0.22044513 0.21995138 0.21989614 0.21964395 0.21902114
 0.21890885 0.21906282 0.21912302 0.21893829 0.21924779 0.21949543
 0.21980724 0.21999425 0.22007076 0.22010227 0.22013135 0.22006443
 0.21912724 0.21800801 0.21709234 0.21650451 0.21577851 0.21494764
 0.21501085 0.2148492  0.21454287 0.2146407  0.21419798 0.2135709
 0.21321137 0.21318296 0.21301748 0.21265322 0.21290861 0.21328412
 0.21364719 0.21382464 0.21364881 0.21369226 0.21351407 0.21280105
 0.21127811 0.21040006 0.20996016 0.2100807  0.21001266 0.21016271
 0.21093716 0.21088398 0.2104017  0.21008116 0.209828   0.20943521
 0.2090305  0.20899479 0.20915161 0.20883149 0.20867261 0.20887573
 0.20898998 0.20900933 0.20896557 0.20924318 0.20964758 0.20968196
 0.20917906 0.20917358 0.20924468 0.20916538 0.20945841 0.20991026
 0.21051605 0.21061887 0.21030405 0.20994754 0.20957883 0.20916285
 0.20885897 0.2086038  0.20870103 0.2084797  0.20862776 0.20876108
 0.20906655 0.20934542 0.20942321 0.209461   0.20939034 0.20877147
 0.20743653 0.20639965 0.20549592 0.20505473 0.20516323 0.20533895
 0.20604736 0.20654239 0.20659007 0.20624587 0.20571193 0.20564157
 0.20539258 0.20489743 0.20489584 0.20477517 0.2048966  0.20491835
 0.20486934 0.20512217 0.20523514 0.20538466 0.20563772 0.20541514
 0.2043305  0.20355539 0.20314568 0.20238905 0.20156233 0.20150697
 0.20226543 0.20234151 0.20226228 0.20234558 0.20217808 0.20199251
 0.20170085 0.2013597  0.20131022 0.20117587 0.20127086 0.20132802
 0.20133853 0.20150147 0.20182735 0.20214523 0.20215267 0.2015155
 0.20048013 0.20037368 0.20066635 0.20056985 0.19981861 0.19942382
 0.20009184 0.20024978 0.20015287 0.20013192 0.20008728 0.20006768
 0.19989109 0.19937135 0.19921902 0.19894685 0.1988091  0.19919443
 0.19975004 0.2001416  0.2005029  0.20072187 0.20079218 0.20039147
 0.19897076 0.19815508 0.19753422 0.19658436 0.19561464 0.19556619
 0.19613406 0.19616973 0.1958754  0.1956634  0.19538255 0.19527791
 0.19503768 0.19484113 0.19490613 0.19471356 0.19460249 0.1947138
 0.1950384  0.1953683  0.19531246 0.19530103 0.19528218 0.19482785
 0.19362138 0.19305283 0.19311857 0.19325191 0.19346593 0.1943832
 0.19593206 0.19671713 0.19700582 0.1968406  0.1961206  0.19581176
 0.19582029 0.19561864 0.19565912 0.19553377 0.19520412 0.19502722
 0.1951764  0.19537659 0.19537008 0.19537914 0.1955425  0.19511004
 0.19384785 0.19301026 0.1928217  0.19282235 0.19298816 0.19350572
 0.19454642 0.19492602 0.19514492 0.19516005 0.19470371 0.19447811
 0.19424088 0.19355093 0.19321224 0.19305603 0.19286524 0.19270033
 0.192823   0.1930478  0.19332151 0.19348164 0.19350769 0.1931796
 0.1919658  0.19140336 0.19153897 0.19204164 0.19243364 0.19312207
 0.19424944 0.19488902 0.19516213 0.19521528 0.19478983 0.19429746
 0.19393921 0.19354592 0.19333392 0.1933747  0.19332519 0.19332106
 0.19358459 0.19402851 0.19429962 0.1944988  0.19489715 0.19500773
 0.19444507 0.19432627 0.19527334 0.19610174 0.19641082 0.19697472
 0.19793974 0.19815311 0.19813308 0.19803628 0.19763996 0.19725043
 0.1968548  0.19642456 0.19625722 0.19615467 0.19626597 0.19629073
 0.1965356  0.1967953  0.19674131 0.19676442 0.19680572 0.19617489
 0.19499213 0.19451399 0.19478925 0.19479357 0.19444744 0.19418679
 0.19501019 0.19538651 0.19531284 0.19498041 0.1942375  0.19367854
 0.1934907  0.19337587 0.19322492 0.19321476 0.19326264 0.193315
 0.19352558 0.19389756 0.19397555 0.1939003  0.19381182 0.19343008
 0.1923056  0.19151995 0.19094491 0.1908383  0.19100168 0.19157405
 0.19281206 0.1935081  0.193772   0.19364245 0.19333398 0.19314714
 0.19295256 0.1926051  0.1924756  0.1925485  0.19263364 0.19270156
 0.19285701 0.19313222 0.19311374 0.19301696 0.1927241  0.19194147
 0.19066085 0.19002748 0.18972509 0.1891098  0.18868326 0.18856546
 0.18948774 0.18981913 0.19001669 0.19001497 0.18959197 0.18902114
 0.18880299 0.18865661 0.18836038 0.18785234 0.18776323 0.18801944
 0.18837659 0.18868136 0.18857278 0.18855974 0.18863086 0.18873882
 0.1884382  0.18866934 0.18905029 0.18931894 0.1895898  0.18996122
 0.19118291 0.19157526 0.19134079 0.19100504 0.19063877 0.19050342
 0.19038528 0.1904009  0.19055074 0.19047779 0.1905383  0.19083163
 0.19110711 0.19157548 0.19174911 0.19175954 0.19193573 0.19193448
 0.19122247 0.19101885 0.19126387 0.19140778 0.1915855  0.19214475
 0.19349657 0.1940016  0.1940807  0.19398998 0.19398104 0.19398566
 0.19388199 0.19383489 0.1934758  0.19304888 0.193038   0.19323213
 0.19348195 0.19394624 0.19419011 0.19451918 0.19466543 0.19440235
 0.19354998 0.19313018 0.19320774 0.1936378  0.19418836 0.19506413
 0.19642125 0.19712907 0.1973082  0.1972119  0.19703855 0.19700213
 0.196786   0.19649342 0.19629751 0.19607067 0.19604614 0.19631656
 0.1967296  0.19690415 0.19677144 0.19690724 0.19715795 0.19727646
 0.19706285 0.1975542  0.19783704 0.19777663 0.19777223 0.19823532
 0.1992757  0.19962463 0.1998224  0.1998686  0.1996696  0.19965811
 0.1997424  0.1998305  0.19993946 0.19992153 0.20003076 0.20019776
 0.2004973  0.20087695 0.20098418 0.20074384 0.20063035 0.2002721
 0.19919796 0.19867815 0.19841939 0.19839765 0.19853118 0.19892137
 0.2001272  0.20044369 0.20038356 0.2000823  0.19964917 0.19963922
 0.1995656  0.19915228 0.19895025 0.1989924  0.19918288 0.19935748
 0.19957666 0.19974335 0.19970934 0.1998225  0.20032018 0.20063919
 0.20025167 0.20062153 0.20138179 0.20182589 0.20189303 0.20199464
 0.20275065 0.20276257 0.20267561 0.20225826 0.20158055 0.20148174
 0.20170227 0.20164792 0.20180695 0.20200962 0.20213301 0.20220962
 0.20262589 0.2033032  0.20362188 0.20384794 0.20434067 0.20444441
 0.2034445  0.20274319 0.20261876 0.2027913  0.20269084 0.20262572
 0.20318659 0.20335937 0.20342815 0.20338918 0.2030107  0.20294878
 0.2030688  0.20309809 0.20320378 0.20331997 0.20361751 0.20377636
 0.20393017 0.20419009 0.20416147 0.20401293 0.20390004 0.20329611
 0.20177488 0.2008229  0.2007225  0.20038112 0.1997572  0.19949868
 0.20018432 0.20030743 0.20018995 0.19983414 0.19931963 0.19919826
 0.1992661  0.19903524 0.19886214 0.19879547 0.1988023  0.19889295
 0.1992689  0.19960016 0.19962673 0.19960052 0.19975063 0.19929774
 0.19779561 0.1971253  0.1970296  0.19706096 0.19708239 0.1972467
 0.19816183 0.19831958 0.19818197 0.19821468 0.19789976 0.19767584
 0.19753835 0.19728969 0.19711468 0.19708115 0.19718988 0.19728807
 0.1975407  0.19790772 0.19787824 0.19756775 0.19731183 0.19670413
 0.19513363 0.19389868 0.19343473 0.19319966 0.19306432 0.19320662
 0.19386403 0.19384633 0.19370754 0.1937833  0.19352868 0.19328375
 0.1933371  0.19326112 0.19311117 0.19315723 0.19331096 0.19337626
 0.19355531 0.19393554 0.19386122 0.1937712  0.19387531 0.19339842
 0.19197275 0.19105986 0.19060579 0.19029315 0.18976398 0.18967494
 0.19059594 0.19057056 0.19018084 0.19005367 0.18960094 0.18934385
 0.18945323 0.18926533 0.18892843 0.1886317  0.18853709 0.18838483
 0.18817754 0.18840258 0.1886208  0.18883456 0.18909973 0.18905565
 0.18802187 0.1871231  0.18653613 0.18643086 0.18626085 0.18635422
 0.18737231 0.18709895 0.18663634 0.18647796 0.18593995 0.18516834
 0.18492825 0.18480045 0.18473238 0.18462041 0.1845419  0.18448946
 0.18443966 0.1844813  0.18435445 0.18439159 0.18449016 0.18358348
 0.18086661 0.17849194 0.1770747  0.17565797 0.17396747 0.17282106
 0.17268889 0.17203178 0.17128927 0.17092527 0.17081392 0.1709073
 0.17102453 0.17090511 0.1705185  0.17042923 0.17067404 0.17067598
 0.17040378 0.17057075 0.17095327 0.17127681 0.17155185 0.17104016
 0.1691836  0.16826534 0.1675712  0.16656187 0.16597211 0.16586259
 0.16659875 0.16647527 0.16604328 0.16572829 0.1650724  0.16433069
 0.16409908 0.16364175 0.16264917 0.16147895 0.16144991 0.16169685
 0.16158041 0.16201816 0.16333504 0.16296947 0.16267297 0.1643862 ]
