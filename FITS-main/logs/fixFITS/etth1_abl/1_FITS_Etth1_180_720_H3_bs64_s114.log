Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=34, out_features=170, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5178880.0
params:  5950.0
Trainable parameters:  5950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9261138439178467
Epoch: 1, Steps: 60 | Train Loss: 1.2077061 Vali Loss: 2.4004245 Test Loss: 1.1246808
Validation loss decreased (inf --> 2.400424).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.594588041305542
Epoch: 2, Steps: 60 | Train Loss: 0.9235015 Vali Loss: 2.0227146 Test Loss: 0.8361027
Validation loss decreased (2.400424 --> 2.022715).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.541438341140747
Epoch: 3, Steps: 60 | Train Loss: 0.7921147 Vali Loss: 1.8276907 Test Loss: 0.6907503
Validation loss decreased (2.022715 --> 1.827691).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.485328197479248
Epoch: 4, Steps: 60 | Train Loss: 0.7219372 Vali Loss: 1.7226481 Test Loss: 0.6068760
Validation loss decreased (1.827691 --> 1.722648).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.4591875076293945
Epoch: 5, Steps: 60 | Train Loss: 0.6804167 Vali Loss: 1.6655402 Test Loss: 0.5553939
Validation loss decreased (1.722648 --> 1.665540).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.6977818012237549
Epoch: 6, Steps: 60 | Train Loss: 0.6547008 Vali Loss: 1.6266723 Test Loss: 0.5230508
Validation loss decreased (1.665540 --> 1.626672).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.3919107913970947
Epoch: 7, Steps: 60 | Train Loss: 0.6391475 Vali Loss: 1.5883682 Test Loss: 0.5017750
Validation loss decreased (1.626672 --> 1.588368).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5678601264953613
Epoch: 8, Steps: 60 | Train Loss: 0.6278775 Vali Loss: 1.5777273 Test Loss: 0.4875402
Validation loss decreased (1.588368 --> 1.577727).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.5606701374053955
Epoch: 9, Steps: 60 | Train Loss: 0.6204370 Vali Loss: 1.5637162 Test Loss: 0.4776752
Validation loss decreased (1.577727 --> 1.563716).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.4766833782196045
Epoch: 10, Steps: 60 | Train Loss: 0.6149182 Vali Loss: 1.5498997 Test Loss: 0.4704690
Validation loss decreased (1.563716 --> 1.549900).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.5939891338348389
Epoch: 11, Steps: 60 | Train Loss: 0.6111836 Vali Loss: 1.5434229 Test Loss: 0.4651133
Validation loss decreased (1.549900 --> 1.543423).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.6038308143615723
Epoch: 12, Steps: 60 | Train Loss: 0.6075769 Vali Loss: 1.5421102 Test Loss: 0.4610518
Validation loss decreased (1.543423 --> 1.542110).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5124783515930176
Epoch: 13, Steps: 60 | Train Loss: 0.6053866 Vali Loss: 1.5304499 Test Loss: 0.4579290
Validation loss decreased (1.542110 --> 1.530450).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6339046955108643
Epoch: 14, Steps: 60 | Train Loss: 0.6029715 Vali Loss: 1.5267252 Test Loss: 0.4553603
Validation loss decreased (1.530450 --> 1.526725).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5736348628997803
Epoch: 15, Steps: 60 | Train Loss: 0.6021886 Vali Loss: 1.5310577 Test Loss: 0.4532423
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.6287593841552734
Epoch: 16, Steps: 60 | Train Loss: 0.6008019 Vali Loss: 1.5259506 Test Loss: 0.4516099
Validation loss decreased (1.526725 --> 1.525951).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.6194453239440918
Epoch: 17, Steps: 60 | Train Loss: 0.5984836 Vali Loss: 1.5166990 Test Loss: 0.4501536
Validation loss decreased (1.525951 --> 1.516699).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6035699844360352
Epoch: 18, Steps: 60 | Train Loss: 0.5979573 Vali Loss: 1.5182335 Test Loss: 0.4489444
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6092829704284668
Epoch: 19, Steps: 60 | Train Loss: 0.5972490 Vali Loss: 1.5198905 Test Loss: 0.4479305
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5689327716827393
Epoch: 20, Steps: 60 | Train Loss: 0.5965146 Vali Loss: 1.5140550 Test Loss: 0.4470768
Validation loss decreased (1.516699 --> 1.514055).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.5096454620361328
Epoch: 21, Steps: 60 | Train Loss: 0.5957974 Vali Loss: 1.5120862 Test Loss: 0.4463361
Validation loss decreased (1.514055 --> 1.512086).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.4955682754516602
Epoch: 22, Steps: 60 | Train Loss: 0.5953558 Vali Loss: 1.5121222 Test Loss: 0.4457309
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3909111022949219
Epoch: 23, Steps: 60 | Train Loss: 0.5949279 Vali Loss: 1.5162691 Test Loss: 0.4451790
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6840288639068604
Epoch: 24, Steps: 60 | Train Loss: 0.5939313 Vali Loss: 1.5113666 Test Loss: 0.4447060
Validation loss decreased (1.512086 --> 1.511367).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.5864837169647217
Epoch: 25, Steps: 60 | Train Loss: 0.5936515 Vali Loss: 1.5121424 Test Loss: 0.4443086
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4865148067474365
Epoch: 26, Steps: 60 | Train Loss: 0.5933456 Vali Loss: 1.5117573 Test Loss: 0.4439728
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4649474620819092
Epoch: 27, Steps: 60 | Train Loss: 0.5926647 Vali Loss: 1.5127521 Test Loss: 0.4436499
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6972787380218506
Epoch: 28, Steps: 60 | Train Loss: 0.5924898 Vali Loss: 1.5019286 Test Loss: 0.4433925
Validation loss decreased (1.511367 --> 1.501929).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.0778486728668213
Epoch: 29, Steps: 60 | Train Loss: 0.5925839 Vali Loss: 1.5109268 Test Loss: 0.4431617
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.419358253479004
Epoch: 30, Steps: 60 | Train Loss: 0.5924375 Vali Loss: 1.5075548 Test Loss: 0.4429877
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5947349071502686
Epoch: 31, Steps: 60 | Train Loss: 0.5924154 Vali Loss: 1.5095326 Test Loss: 0.4428071
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.520582914352417
Epoch: 32, Steps: 60 | Train Loss: 0.5918600 Vali Loss: 1.5010887 Test Loss: 0.4426692
Validation loss decreased (1.501929 --> 1.501089).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4509432315826416
Epoch: 33, Steps: 60 | Train Loss: 0.5916212 Vali Loss: 1.5052028 Test Loss: 0.4425251
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6244542598724365
Epoch: 34, Steps: 60 | Train Loss: 0.5910977 Vali Loss: 1.5077918 Test Loss: 0.4424223
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.6791555881500244
Epoch: 35, Steps: 60 | Train Loss: 0.5909715 Vali Loss: 1.5066941 Test Loss: 0.4423283
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.521331548690796
Epoch: 36, Steps: 60 | Train Loss: 0.5910179 Vali Loss: 1.5048665 Test Loss: 0.4422446
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5030021667480469
Epoch: 37, Steps: 60 | Train Loss: 0.5911984 Vali Loss: 1.5043875 Test Loss: 0.4421871
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4904019832611084
Epoch: 38, Steps: 60 | Train Loss: 0.5907662 Vali Loss: 1.5037373 Test Loss: 0.4421152
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.5194199085235596
Epoch: 39, Steps: 60 | Train Loss: 0.5901600 Vali Loss: 1.5044440 Test Loss: 0.4420658
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.5522210597991943
Epoch: 40, Steps: 60 | Train Loss: 0.5906141 Vali Loss: 1.5089457 Test Loss: 0.4420270
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.6746625900268555
Epoch: 41, Steps: 60 | Train Loss: 0.5907163 Vali Loss: 1.5081567 Test Loss: 0.4419934
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.642751932144165
Epoch: 42, Steps: 60 | Train Loss: 0.5911805 Vali Loss: 1.5037915 Test Loss: 0.4419723
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.5996549129486084
Epoch: 43, Steps: 60 | Train Loss: 0.5905093 Vali Loss: 1.5014679 Test Loss: 0.4419256
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.6044652462005615
Epoch: 44, Steps: 60 | Train Loss: 0.5899072 Vali Loss: 1.4981403 Test Loss: 0.4418934
Validation loss decreased (1.501089 --> 1.498140).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5446560382843018
Epoch: 45, Steps: 60 | Train Loss: 0.5900556 Vali Loss: 1.5000744 Test Loss: 0.4418778
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.656334400177002
Epoch: 46, Steps: 60 | Train Loss: 0.5902020 Vali Loss: 1.4961754 Test Loss: 0.4418621
Validation loss decreased (1.498140 --> 1.496175).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.508720874786377
Epoch: 47, Steps: 60 | Train Loss: 0.5898727 Vali Loss: 1.5034906 Test Loss: 0.4418520
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.647942304611206
Epoch: 48, Steps: 60 | Train Loss: 0.5899774 Vali Loss: 1.5044947 Test Loss: 0.4418557
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.4979569911956787
Epoch: 49, Steps: 60 | Train Loss: 0.5894798 Vali Loss: 1.5053103 Test Loss: 0.4418277
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.5252258777618408
Epoch: 50, Steps: 60 | Train Loss: 0.5894979 Vali Loss: 1.5035543 Test Loss: 0.4418247
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.645824670791626
Epoch: 51, Steps: 60 | Train Loss: 0.5896534 Vali Loss: 1.5051851 Test Loss: 0.4418206
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.5706532001495361
Epoch: 52, Steps: 60 | Train Loss: 0.5897104 Vali Loss: 1.5083774 Test Loss: 0.4418354
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6139318943023682
Epoch: 53, Steps: 60 | Train Loss: 0.5898391 Vali Loss: 1.5040333 Test Loss: 0.4418287
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.6116611957550049
Epoch: 54, Steps: 60 | Train Loss: 0.5896012 Vali Loss: 1.5048488 Test Loss: 0.4418224
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.558098316192627
Epoch: 55, Steps: 60 | Train Loss: 0.5895199 Vali Loss: 1.4913120 Test Loss: 0.4418325
Validation loss decreased (1.496175 --> 1.491312).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.4737393856048584
Epoch: 56, Steps: 60 | Train Loss: 0.5895826 Vali Loss: 1.4973998 Test Loss: 0.4418317
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.6248340606689453
Epoch: 57, Steps: 60 | Train Loss: 0.5893462 Vali Loss: 1.5009968 Test Loss: 0.4418255
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.6351988315582275
Epoch: 58, Steps: 60 | Train Loss: 0.5894574 Vali Loss: 1.5001156 Test Loss: 0.4418300
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.5225224494934082
Epoch: 59, Steps: 60 | Train Loss: 0.5895804 Vali Loss: 1.5050278 Test Loss: 0.4418380
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.6673359870910645
Epoch: 60, Steps: 60 | Train Loss: 0.5894656 Vali Loss: 1.5032938 Test Loss: 0.4418489
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.5757160186767578
Epoch: 61, Steps: 60 | Train Loss: 0.5891197 Vali Loss: 1.5023732 Test Loss: 0.4418509
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6499412059783936
Epoch: 62, Steps: 60 | Train Loss: 0.5894120 Vali Loss: 1.4951768 Test Loss: 0.4418502
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.621011734008789
Epoch: 63, Steps: 60 | Train Loss: 0.5892117 Vali Loss: 1.5054708 Test Loss: 0.4418609
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.599437952041626
Epoch: 64, Steps: 60 | Train Loss: 0.5896475 Vali Loss: 1.4997323 Test Loss: 0.4418585
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.5894722938537598
Epoch: 65, Steps: 60 | Train Loss: 0.5889491 Vali Loss: 1.4978578 Test Loss: 0.4418681
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.4519615173339844
Epoch: 66, Steps: 60 | Train Loss: 0.5898234 Vali Loss: 1.5001571 Test Loss: 0.4418794
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.5280461311340332
Epoch: 67, Steps: 60 | Train Loss: 0.5892542 Vali Loss: 1.5009451 Test Loss: 0.4418784
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.5796570777893066
Epoch: 68, Steps: 60 | Train Loss: 0.5893096 Vali Loss: 1.4987793 Test Loss: 0.4418851
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.6125731468200684
Epoch: 69, Steps: 60 | Train Loss: 0.5890824 Vali Loss: 1.5020829 Test Loss: 0.4418929
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5433382987976074
Epoch: 70, Steps: 60 | Train Loss: 0.5893072 Vali Loss: 1.5026929 Test Loss: 0.4418946
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.629817008972168
Epoch: 71, Steps: 60 | Train Loss: 0.5890399 Vali Loss: 1.4927205 Test Loss: 0.4419077
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.60823655128479
Epoch: 72, Steps: 60 | Train Loss: 0.5891718 Vali Loss: 1.4999797 Test Loss: 0.4419088
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.4896321296691895
Epoch: 73, Steps: 60 | Train Loss: 0.5890580 Vali Loss: 1.5053135 Test Loss: 0.4419221
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.7060096263885498
Epoch: 74, Steps: 60 | Train Loss: 0.5889507 Vali Loss: 1.5003879 Test Loss: 0.4419272
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.626526117324829
Epoch: 75, Steps: 60 | Train Loss: 0.5892484 Vali Loss: 1.5066888 Test Loss: 0.4419304
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4406747817993164, mae:0.4530518352985382, rse:0.6354934573173523, corr:[0.22588651 0.23238383 0.23226444 0.22979726 0.22807074 0.22793962
 0.229229   0.2306471  0.23046091 0.22988573 0.2295601  0.22951584
 0.229201   0.22817674 0.22734065 0.22722483 0.22730634 0.22720204
 0.22703783 0.22713536 0.22763042 0.2283012  0.22867863 0.22851022
 0.2278589  0.2276012  0.22743814 0.22697987 0.22608426 0.22536625
 0.22517854 0.22533202 0.22511533 0.22464234 0.22445358 0.22500932
 0.22581846 0.22567326 0.22503845 0.22470877 0.22501968 0.22540514
 0.2253786  0.2253873  0.225906   0.22674958 0.2274649  0.22733566
 0.22609775 0.22519262 0.22427616 0.22315645 0.2218599  0.22068523
 0.2201002  0.22002964 0.21991536 0.21991566 0.21985294 0.22033852
 0.2208765  0.22072178 0.22029397 0.22006978 0.22010288 0.22021316
 0.22015315 0.21998203 0.2199718  0.22005941 0.21995021 0.21932872
 0.21805546 0.21715695 0.21648055 0.21603547 0.21568212 0.21528345
 0.21530774 0.21570767 0.21576099 0.21554671 0.21509388 0.21502362
 0.21529038 0.21508296 0.21474634 0.21457632 0.21450819 0.2142877
 0.21382846 0.21369998 0.21419826 0.2151305  0.21601193 0.21637292
 0.2160182  0.21591747 0.21606308 0.21587385 0.21534024 0.21477619
 0.21450761 0.21495694 0.21525836 0.21526815 0.21501236 0.21521215
 0.21568681 0.21567766 0.21532236 0.214926   0.21452333 0.21417142
 0.21401273 0.21410732 0.21442318 0.21466266 0.21458143 0.21409652
 0.21312536 0.21243322 0.21158841 0.21075822 0.20993312 0.209484
 0.20966683 0.21028689 0.2103899  0.21016277 0.20996532 0.21049245
 0.2113505  0.21151483 0.211357   0.21118689 0.21109712 0.21102121
 0.21086186 0.21081968 0.21106951 0.21144228 0.21159957 0.21134202
 0.21060564 0.21001267 0.20942123 0.208353   0.207306   0.20664258
 0.2066331  0.20704502 0.207493   0.20775959 0.2077722  0.20801538
 0.2084215  0.20835435 0.20817691 0.20813149 0.2081212  0.20817477
 0.20819025 0.20829614 0.20869863 0.20922022 0.20949276 0.20939863
 0.20896178 0.20889059 0.20879617 0.20841858 0.20790622 0.2075228
 0.20766601 0.2083801  0.2088397  0.20913438 0.20930324 0.20978472
 0.21043947 0.2106055  0.21058096 0.21053638 0.21047918 0.21038178
 0.21032304 0.21050367 0.21096808 0.21129352 0.21119894 0.21058828
 0.20957498 0.20903216 0.20859285 0.20785893 0.20686276 0.20597439
 0.20567875 0.20607355 0.20655465 0.20686494 0.20690599 0.20734829
 0.20787549 0.20787662 0.20758547 0.20723878 0.20698486 0.20676619
 0.20654154 0.20642589 0.20655273 0.20677412 0.20677832 0.20647764
 0.20585352 0.20554551 0.20528156 0.20479982 0.20427608 0.2037425
 0.20367596 0.2040233  0.2042648  0.20432033 0.20426604 0.2045563
 0.20501989 0.204956   0.20474765 0.20456545 0.20424345 0.2037689
 0.20333321 0.20321842 0.20348117 0.20375599 0.20370306 0.20322238
 0.20253326 0.20226227 0.20214047 0.20190904 0.20149532 0.20111203
 0.20128594 0.20187664 0.20237334 0.2026584  0.20276216 0.2032494
 0.20403169 0.20439397 0.20451702 0.20443821 0.20423923 0.2039133
 0.20353442 0.20317353 0.20312113 0.20322855 0.20311782 0.20274332
 0.20212495 0.20185103 0.20158808 0.20111774 0.20054163 0.2000854
 0.20013611 0.20063584 0.20075716 0.20067571 0.20057051 0.2007744
 0.20131437 0.20147754 0.2014011  0.20115827 0.20082739 0.20058645
 0.20048796 0.20072085 0.20123887 0.20177332 0.20204875 0.20207323
 0.20191775 0.20212895 0.20249954 0.2026119  0.2024415  0.20239212
 0.2027457  0.20337023 0.20374724 0.20388599 0.20391038 0.2043227
 0.20497589 0.20516798 0.2052032  0.20525506 0.20514238 0.20493433
 0.20474859 0.20469934 0.2047847  0.20499063 0.20493203 0.20470683
 0.20438427 0.20443667 0.20444456 0.20405412 0.20352802 0.20296814
 0.20290767 0.20325719 0.2033505  0.20321542 0.20302624 0.20329992
 0.2039467  0.20423496 0.20443074 0.20464084 0.2045178  0.2041506
 0.20381011 0.20376241 0.20413557 0.20457031 0.2046526  0.20435534
 0.20385934 0.20374954 0.20366174 0.20330505 0.2026774  0.20209381
 0.20203456 0.20240352 0.20271602 0.20285127 0.20304163 0.20357507
 0.20416021 0.20435177 0.2044204  0.204472   0.20436813 0.20416063
 0.20384687 0.20369917 0.2037975  0.20397261 0.20367733 0.2031586
 0.2026065  0.20246118 0.20233496 0.20190474 0.201256   0.20076583
 0.20084712 0.20114523 0.20117778 0.20097467 0.20076971 0.20083849
 0.20108047 0.20098928 0.20090473 0.20084777 0.20076051 0.20065114
 0.20067525 0.20085464 0.20126647 0.20176373 0.2019951  0.20228201
 0.20261973 0.20325424 0.20361058 0.20327449 0.20253985 0.20171711
 0.20148271 0.20175976 0.2017813  0.20160177 0.20141615 0.20195143
 0.20290655 0.20309812 0.20295626 0.20283142 0.20288716 0.2030113
 0.20312235 0.20326948 0.20369522 0.20419647 0.20432885 0.20412414
 0.20367284 0.20363832 0.20376208 0.20353778 0.20311172 0.20267044
 0.20273632 0.20323354 0.20354325 0.20352924 0.2035522  0.20400211
 0.204699   0.20488954 0.20471416 0.20442986 0.20409195 0.20389096
 0.20402545 0.20454995 0.2051671  0.20592976 0.20636487 0.20647246
 0.2062019  0.20622835 0.20625131 0.20590967 0.20557186 0.20537686
 0.20555995 0.20609224 0.20631613 0.20633599 0.20631787 0.20679167
 0.20740294 0.20733002 0.20704518 0.20696664 0.20701976 0.20703033
 0.20694661 0.20696568 0.20732962 0.20795007 0.20839481 0.20859806
 0.20845138 0.20864007 0.20868342 0.20817521 0.20732944 0.2063688
 0.2059505  0.20623776 0.20677328 0.20724851 0.20759106 0.2081282
 0.20878154 0.20893016 0.20892848 0.2089185  0.20887098 0.20882711
 0.20876119 0.20884006 0.20908144 0.2091632  0.20904677 0.20869842
 0.20811601 0.20790665 0.20773977 0.20725586 0.20666157 0.20611201
 0.20618713 0.20678182 0.20729381 0.20756933 0.20756839 0.20782882
 0.20836847 0.20835768 0.20821741 0.20816268 0.20816262 0.20805298
 0.20795925 0.20819978 0.20890427 0.20974153 0.21036223 0.21058895
 0.2104079  0.2106232  0.21087185 0.21071161 0.21025045 0.20964381
 0.20937835 0.20945707 0.20960222 0.20954916 0.20950323 0.20995565
 0.21060032 0.21072702 0.21077348 0.21093327 0.21109618 0.21117884
 0.21129774 0.21177377 0.21259008 0.21336837 0.21373235 0.2134194
 0.21254304 0.21220046 0.21213205 0.21173745 0.2109131  0.21003841
 0.20981044 0.21024399 0.21055643 0.21059884 0.21048261 0.21078235
 0.21145461 0.21157245 0.21153608 0.21160764 0.21175867 0.21187831
 0.21178472 0.21167114 0.2117287  0.21179868 0.211646   0.21103425
 0.20988294 0.2092963  0.20911641 0.20864521 0.20775053 0.2069346
 0.20664202 0.20677373 0.20709991 0.20729223 0.20723283 0.20758112
 0.20839557 0.20836084 0.20795542 0.20762417 0.20755349 0.20757854
 0.20748295 0.20728597 0.20734255 0.207523   0.20752801 0.20700042
 0.20590517 0.20539837 0.20522046 0.204797   0.20410568 0.20303448
 0.20249191 0.20248632 0.20263797 0.20260257 0.20224306 0.2022326
 0.20242128 0.20202845 0.20141141 0.20096117 0.20072937 0.20056306
 0.20037521 0.20024034 0.200277   0.2003638  0.20017591 0.19951114
 0.19825166 0.19747144 0.19710119 0.19643205 0.19541164 0.19439985
 0.19382234 0.19383723 0.19399042 0.19393069 0.19368425 0.19371901
 0.19423684 0.19420528 0.19385654 0.19354913 0.1932984  0.1930696
 0.19280654 0.19269402 0.19286053 0.19308989 0.19302548 0.19239162
 0.19124317 0.19050436 0.19005248 0.18929063 0.18844913 0.18774827
 0.18763699 0.18779172 0.18775564 0.18768616 0.18758103 0.18799788
 0.188856   0.18883319 0.18834215 0.1879454  0.18763569 0.18741596
 0.18720731 0.1871699  0.18737645 0.18768182 0.18777774 0.18725504
 0.18597774 0.1850383  0.18450409 0.18408832 0.18350568 0.18273403
 0.18241563 0.18265931 0.18314634 0.18336503 0.18322648 0.18337524
 0.18397737 0.18405195 0.18385181 0.18360178 0.18338995 0.1831834
 0.1830044  0.1830331  0.18334036 0.18367453 0.18358342 0.18278588
 0.18118663 0.18003173 0.17921063 0.17790468 0.17648806 0.17539217
 0.17523234 0.17552826 0.17583081 0.17590035 0.17602846 0.17684393
 0.17795679 0.17798968 0.1775167  0.17721397 0.17718214 0.17723162
 0.17714815 0.177078   0.17720324 0.17756796 0.17787865 0.17756343
 0.17654943 0.17605716 0.17586234 0.17533739 0.17441927 0.17355032
 0.1732278  0.17343813 0.1736707  0.17333283 0.17261627 0.17259572
 0.17338699 0.17301784 0.17183645 0.17080814 0.17062318 0.17115037
 0.17166127 0.1714317  0.17078714 0.1707652  0.17191198 0.16947974]
