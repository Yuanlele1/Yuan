Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=90, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14031360.0
params:  15834.0
Trainable parameters:  15834
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9456567764282227
Epoch: 1, Steps: 62 | Train Loss: 0.7357250 Vali Loss: 1.8537583 Test Loss: 0.8863559
Validation loss decreased (inf --> 1.853758).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3738741874694824
Epoch: 2, Steps: 62 | Train Loss: 0.5815694 Vali Loss: 1.6765964 Test Loss: 0.7810776
Validation loss decreased (1.853758 --> 1.676596).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2132289409637451
Epoch: 3, Steps: 62 | Train Loss: 0.5044728 Vali Loss: 1.5871910 Test Loss: 0.7302321
Validation loss decreased (1.676596 --> 1.587191).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4734253883361816
Epoch: 4, Steps: 62 | Train Loss: 0.4620503 Vali Loss: 1.5415603 Test Loss: 0.7034779
Validation loss decreased (1.587191 --> 1.541560).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.250849962234497
Epoch: 5, Steps: 62 | Train Loss: 0.4352418 Vali Loss: 1.5066892 Test Loss: 0.6844734
Validation loss decreased (1.541560 --> 1.506689).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.1068670749664307
Epoch: 6, Steps: 62 | Train Loss: 0.4156359 Vali Loss: 1.4882790 Test Loss: 0.6701677
Validation loss decreased (1.506689 --> 1.488279).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.349468469619751
Epoch: 7, Steps: 62 | Train Loss: 0.4002725 Vali Loss: 1.4646661 Test Loss: 0.6576250
Validation loss decreased (1.488279 --> 1.464666).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2873518466949463
Epoch: 8, Steps: 62 | Train Loss: 0.3874588 Vali Loss: 1.4509782 Test Loss: 0.6456965
Validation loss decreased (1.464666 --> 1.450978).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4150993824005127
Epoch: 9, Steps: 62 | Train Loss: 0.3764062 Vali Loss: 1.4390463 Test Loss: 0.6357970
Validation loss decreased (1.450978 --> 1.439046).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.178579330444336
Epoch: 10, Steps: 62 | Train Loss: 0.3668205 Vali Loss: 1.4188446 Test Loss: 0.6265561
Validation loss decreased (1.439046 --> 1.418845).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.2380836009979248
Epoch: 11, Steps: 62 | Train Loss: 0.3583469 Vali Loss: 1.4100492 Test Loss: 0.6183105
Validation loss decreased (1.418845 --> 1.410049).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.2209513187408447
Epoch: 12, Steps: 62 | Train Loss: 0.3508172 Vali Loss: 1.4004090 Test Loss: 0.6101016
Validation loss decreased (1.410049 --> 1.400409).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.606706142425537
Epoch: 13, Steps: 62 | Train Loss: 0.3442750 Vali Loss: 1.3855779 Test Loss: 0.6021185
Validation loss decreased (1.400409 --> 1.385578).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.2499077320098877
Epoch: 14, Steps: 62 | Train Loss: 0.3382900 Vali Loss: 1.3770868 Test Loss: 0.5955523
Validation loss decreased (1.385578 --> 1.377087).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.412602186203003
Epoch: 15, Steps: 62 | Train Loss: 0.3329425 Vali Loss: 1.3651953 Test Loss: 0.5888765
Validation loss decreased (1.377087 --> 1.365195).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.6210806369781494
Epoch: 16, Steps: 62 | Train Loss: 0.3280213 Vali Loss: 1.3618225 Test Loss: 0.5831326
Validation loss decreased (1.365195 --> 1.361822).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.328338623046875
Epoch: 17, Steps: 62 | Train Loss: 0.3237181 Vali Loss: 1.3511481 Test Loss: 0.5776235
Validation loss decreased (1.361822 --> 1.351148).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4792368412017822
Epoch: 18, Steps: 62 | Train Loss: 0.3196992 Vali Loss: 1.3406016 Test Loss: 0.5721615
Validation loss decreased (1.351148 --> 1.340602).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4816186428070068
Epoch: 19, Steps: 62 | Train Loss: 0.3160738 Vali Loss: 1.3420671 Test Loss: 0.5673195
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3152143955230713
Epoch: 20, Steps: 62 | Train Loss: 0.3126889 Vali Loss: 1.3305620 Test Loss: 0.5626234
Validation loss decreased (1.340602 --> 1.330562).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.5645911693572998
Epoch: 21, Steps: 62 | Train Loss: 0.3097052 Vali Loss: 1.3247881 Test Loss: 0.5585967
Validation loss decreased (1.330562 --> 1.324788).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.642888069152832
Epoch: 22, Steps: 62 | Train Loss: 0.3069399 Vali Loss: 1.3223581 Test Loss: 0.5546247
Validation loss decreased (1.324788 --> 1.322358).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.6034181118011475
Epoch: 23, Steps: 62 | Train Loss: 0.3042945 Vali Loss: 1.3116916 Test Loss: 0.5510800
Validation loss decreased (1.322358 --> 1.311692).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.3642358779907227
Epoch: 24, Steps: 62 | Train Loss: 0.3020081 Vali Loss: 1.3095324 Test Loss: 0.5474080
Validation loss decreased (1.311692 --> 1.309532).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.6310296058654785
Epoch: 25, Steps: 62 | Train Loss: 0.2997654 Vali Loss: 1.2992719 Test Loss: 0.5443384
Validation loss decreased (1.309532 --> 1.299272).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.6477739810943604
Epoch: 26, Steps: 62 | Train Loss: 0.2976579 Vali Loss: 1.3043314 Test Loss: 0.5414780
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.363300085067749
Epoch: 27, Steps: 62 | Train Loss: 0.2957588 Vali Loss: 1.2958695 Test Loss: 0.5385400
Validation loss decreased (1.299272 --> 1.295869).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.012960910797119
Epoch: 28, Steps: 62 | Train Loss: 0.2940228 Vali Loss: 1.2937309 Test Loss: 0.5359246
Validation loss decreased (1.295869 --> 1.293731).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.5981762409210205
Epoch: 29, Steps: 62 | Train Loss: 0.2923767 Vali Loss: 1.2943643 Test Loss: 0.5335298
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.820631742477417
Epoch: 30, Steps: 62 | Train Loss: 0.2908591 Vali Loss: 1.2898544 Test Loss: 0.5309742
Validation loss decreased (1.293731 --> 1.289854).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.8987603187561035
Epoch: 31, Steps: 62 | Train Loss: 0.2895248 Vali Loss: 1.2872665 Test Loss: 0.5288207
Validation loss decreased (1.289854 --> 1.287266).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.6226308345794678
Epoch: 32, Steps: 62 | Train Loss: 0.2881864 Vali Loss: 1.2805047 Test Loss: 0.5267023
Validation loss decreased (1.287266 --> 1.280505).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.886228084564209
Epoch: 33, Steps: 62 | Train Loss: 0.2869023 Vali Loss: 1.2819679 Test Loss: 0.5250821
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.470531702041626
Epoch: 34, Steps: 62 | Train Loss: 0.2857551 Vali Loss: 1.2761878 Test Loss: 0.5231075
Validation loss decreased (1.280505 --> 1.276188).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.0412566661834717
Epoch: 35, Steps: 62 | Train Loss: 0.2846340 Vali Loss: 1.2754675 Test Loss: 0.5213372
Validation loss decreased (1.276188 --> 1.275468).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.991960048675537
Epoch: 36, Steps: 62 | Train Loss: 0.2836166 Vali Loss: 1.2745949 Test Loss: 0.5198186
Validation loss decreased (1.275468 --> 1.274595).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9658823013305664
Epoch: 37, Steps: 62 | Train Loss: 0.2826746 Vali Loss: 1.2688718 Test Loss: 0.5183269
Validation loss decreased (1.274595 --> 1.268872).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9637174606323242
Epoch: 38, Steps: 62 | Train Loss: 0.2818410 Vali Loss: 1.2707618 Test Loss: 0.5168039
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.287353992462158
Epoch: 39, Steps: 62 | Train Loss: 0.2809111 Vali Loss: 1.2673661 Test Loss: 0.5155334
Validation loss decreased (1.268872 --> 1.267366).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.360553503036499
Epoch: 40, Steps: 62 | Train Loss: 0.2801406 Vali Loss: 1.2675427 Test Loss: 0.5142251
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.3620588779449463
Epoch: 41, Steps: 62 | Train Loss: 0.2793266 Vali Loss: 1.2614323 Test Loss: 0.5129290
Validation loss decreased (1.267366 --> 1.261432).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.6211981773376465
Epoch: 42, Steps: 62 | Train Loss: 0.2786885 Vali Loss: 1.2634497 Test Loss: 0.5118287
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.560947895050049
Epoch: 43, Steps: 62 | Train Loss: 0.2780446 Vali Loss: 1.2633435 Test Loss: 0.5108069
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.3597216606140137
Epoch: 44, Steps: 62 | Train Loss: 0.2774610 Vali Loss: 1.2631203 Test Loss: 0.5097245
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.8714990615844727
Epoch: 45, Steps: 62 | Train Loss: 0.2767999 Vali Loss: 1.2568189 Test Loss: 0.5087031
Validation loss decreased (1.261432 --> 1.256819).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.651690721511841
Epoch: 46, Steps: 62 | Train Loss: 0.2762070 Vali Loss: 1.2604246 Test Loss: 0.5078089
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.189480781555176
Epoch: 47, Steps: 62 | Train Loss: 0.2756704 Vali Loss: 1.2555716 Test Loss: 0.5069231
Validation loss decreased (1.256819 --> 1.255572).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.0123541355133057
Epoch: 48, Steps: 62 | Train Loss: 0.2751846 Vali Loss: 1.2562473 Test Loss: 0.5061390
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.375046491622925
Epoch: 49, Steps: 62 | Train Loss: 0.2746585 Vali Loss: 1.2533227 Test Loss: 0.5053386
Validation loss decreased (1.255572 --> 1.253323).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.762578010559082
Epoch: 50, Steps: 62 | Train Loss: 0.2742749 Vali Loss: 1.2550938 Test Loss: 0.5046236
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.7784879207611084
Epoch: 51, Steps: 62 | Train Loss: 0.2738134 Vali Loss: 1.2519238 Test Loss: 0.5038053
Validation loss decreased (1.253323 --> 1.251924).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.0063259601593018
Epoch: 52, Steps: 62 | Train Loss: 0.2734492 Vali Loss: 1.2553123 Test Loss: 0.5032251
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.910308837890625
Epoch: 53, Steps: 62 | Train Loss: 0.2730681 Vali Loss: 1.2515540 Test Loss: 0.5025505
Validation loss decreased (1.251924 --> 1.251554).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9236857891082764
Epoch: 54, Steps: 62 | Train Loss: 0.2727179 Vali Loss: 1.2523063 Test Loss: 0.5019827
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.7800421714782715
Epoch: 55, Steps: 62 | Train Loss: 0.2723848 Vali Loss: 1.2481718 Test Loss: 0.5014149
Validation loss decreased (1.251554 --> 1.248172).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.1003451347351074
Epoch: 56, Steps: 62 | Train Loss: 0.2719906 Vali Loss: 1.2513677 Test Loss: 0.5008566
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.643643379211426
Epoch: 57, Steps: 62 | Train Loss: 0.2717318 Vali Loss: 1.2461079 Test Loss: 0.5003045
Validation loss decreased (1.248172 --> 1.246108).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.8447835445404053
Epoch: 58, Steps: 62 | Train Loss: 0.2714701 Vali Loss: 1.2525799 Test Loss: 0.4998464
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.561300754547119
Epoch: 59, Steps: 62 | Train Loss: 0.2712029 Vali Loss: 1.2468543 Test Loss: 0.4993865
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.0283472537994385
Epoch: 60, Steps: 62 | Train Loss: 0.2709351 Vali Loss: 1.2450424 Test Loss: 0.4989098
Validation loss decreased (1.246108 --> 1.245042).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.1032233238220215
Epoch: 61, Steps: 62 | Train Loss: 0.2705579 Vali Loss: 1.2461305 Test Loss: 0.4985208
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.265169382095337
Epoch: 62, Steps: 62 | Train Loss: 0.2703814 Vali Loss: 1.2457093 Test Loss: 0.4981256
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.9764533042907715
Epoch: 63, Steps: 62 | Train Loss: 0.2702295 Vali Loss: 1.2475914 Test Loss: 0.4977643
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.859489679336548
Epoch: 64, Steps: 62 | Train Loss: 0.2700290 Vali Loss: 1.2408807 Test Loss: 0.4973971
Validation loss decreased (1.245042 --> 1.240881).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.988992691040039
Epoch: 65, Steps: 62 | Train Loss: 0.2697314 Vali Loss: 1.2429017 Test Loss: 0.4970312
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.2870755195617676
Epoch: 66, Steps: 62 | Train Loss: 0.2696233 Vali Loss: 1.2399741 Test Loss: 0.4967065
Validation loss decreased (1.240881 --> 1.239974).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.9108657836914062
Epoch: 67, Steps: 62 | Train Loss: 0.2693205 Vali Loss: 1.2417141 Test Loss: 0.4964066
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.8244354724884033
Epoch: 68, Steps: 62 | Train Loss: 0.2692811 Vali Loss: 1.2437748 Test Loss: 0.4961008
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.9364731311798096
Epoch: 69, Steps: 62 | Train Loss: 0.2689399 Vali Loss: 1.2384356 Test Loss: 0.4958479
Validation loss decreased (1.239974 --> 1.238436).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9273333549499512
Epoch: 70, Steps: 62 | Train Loss: 0.2688064 Vali Loss: 1.2468388 Test Loss: 0.4955631
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.840320348739624
Epoch: 71, Steps: 62 | Train Loss: 0.2687728 Vali Loss: 1.2434052 Test Loss: 0.4952992
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.7220454216003418
Epoch: 72, Steps: 62 | Train Loss: 0.2685796 Vali Loss: 1.2389686 Test Loss: 0.4950697
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.0809731483459473
Epoch: 73, Steps: 62 | Train Loss: 0.2685342 Vali Loss: 1.2416366 Test Loss: 0.4948556
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.2723748683929443
Epoch: 74, Steps: 62 | Train Loss: 0.2683147 Vali Loss: 1.2430329 Test Loss: 0.4946068
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.5882174968719482
Epoch: 75, Steps: 62 | Train Loss: 0.2682347 Vali Loss: 1.2391341 Test Loss: 0.4944184
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.7564401626586914
Epoch: 76, Steps: 62 | Train Loss: 0.2681382 Vali Loss: 1.2423054 Test Loss: 0.4942214
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.835162401199341
Epoch: 77, Steps: 62 | Train Loss: 0.2680007 Vali Loss: 1.2388004 Test Loss: 0.4940338
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.106541633605957
Epoch: 78, Steps: 62 | Train Loss: 0.2679521 Vali Loss: 1.2398914 Test Loss: 0.4938454
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.1543283462524414
Epoch: 79, Steps: 62 | Train Loss: 0.2678359 Vali Loss: 1.2385741 Test Loss: 0.4936770
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.4154860973358154
Epoch: 80, Steps: 62 | Train Loss: 0.2676275 Vali Loss: 1.2401068 Test Loss: 0.4935301
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.9464335441589355
Epoch: 81, Steps: 62 | Train Loss: 0.2676097 Vali Loss: 1.2389615 Test Loss: 0.4933721
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 4.067620277404785
Epoch: 82, Steps: 62 | Train Loss: 0.2675156 Vali Loss: 1.2384495 Test Loss: 0.4932267
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 5.128495931625366
Epoch: 83, Steps: 62 | Train Loss: 0.2674544 Vali Loss: 1.2332133 Test Loss: 0.4930957
Validation loss decreased (1.238436 --> 1.233213).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.166073799133301
Epoch: 84, Steps: 62 | Train Loss: 0.2673540 Vali Loss: 1.2374684 Test Loss: 0.4929598
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.7377679347991943
Epoch: 85, Steps: 62 | Train Loss: 0.2673929 Vali Loss: 1.2422360 Test Loss: 0.4928354
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.4079813957214355
Epoch: 86, Steps: 62 | Train Loss: 0.2671032 Vali Loss: 1.2356077 Test Loss: 0.4927199
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.292510986328125
Epoch: 87, Steps: 62 | Train Loss: 0.2671141 Vali Loss: 1.2356074 Test Loss: 0.4926196
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.2590651512145996
Epoch: 88, Steps: 62 | Train Loss: 0.2671021 Vali Loss: 1.2329124 Test Loss: 0.4924979
Validation loss decreased (1.233213 --> 1.232912).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.8821382522583008
Epoch: 89, Steps: 62 | Train Loss: 0.2669863 Vali Loss: 1.2381195 Test Loss: 0.4924026
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.0403547286987305
Epoch: 90, Steps: 62 | Train Loss: 0.2669594 Vali Loss: 1.2397515 Test Loss: 0.4923047
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.9597430229187012
Epoch: 91, Steps: 62 | Train Loss: 0.2668642 Vali Loss: 1.2393075 Test Loss: 0.4922066
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.749089241027832
Epoch: 92, Steps: 62 | Train Loss: 0.2668322 Vali Loss: 1.2396387 Test Loss: 0.4921187
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.386815309524536
Epoch: 93, Steps: 62 | Train Loss: 0.2668762 Vali Loss: 1.2383102 Test Loss: 0.4920463
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.4349048137664795
Epoch: 94, Steps: 62 | Train Loss: 0.2667546 Vali Loss: 1.2363323 Test Loss: 0.4919640
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.554168224334717
Epoch: 95, Steps: 62 | Train Loss: 0.2667624 Vali Loss: 1.2356471 Test Loss: 0.4918908
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.9872825145721436
Epoch: 96, Steps: 62 | Train Loss: 0.2667809 Vali Loss: 1.2426167 Test Loss: 0.4918181
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.660923957824707
Epoch: 97, Steps: 62 | Train Loss: 0.2665952 Vali Loss: 1.2368903 Test Loss: 0.4917478
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.028534173965454
Epoch: 98, Steps: 62 | Train Loss: 0.2666407 Vali Loss: 1.2406434 Test Loss: 0.4916801
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.0339322090148926
Epoch: 99, Steps: 62 | Train Loss: 0.2665277 Vali Loss: 1.2370983 Test Loss: 0.4916236
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.037539482116699
Epoch: 100, Steps: 62 | Train Loss: 0.2665866 Vali Loss: 1.2323062 Test Loss: 0.4915661
Validation loss decreased (1.232912 --> 1.232306).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=90, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14031360.0
params:  15834.0
Trainable parameters:  15834
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.6934432983398438
Epoch: 1, Steps: 62 | Train Loss: 0.4812033 Vali Loss: 1.1967958 Test Loss: 0.4609476
Validation loss decreased (inf --> 1.196796).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.4795501232147217
Epoch: 2, Steps: 62 | Train Loss: 0.4683137 Vali Loss: 1.1710844 Test Loss: 0.4455408
Validation loss decreased (1.196796 --> 1.171084).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.459864377975464
Epoch: 3, Steps: 62 | Train Loss: 0.4617649 Vali Loss: 1.1657196 Test Loss: 0.4376164
Validation loss decreased (1.171084 --> 1.165720).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9395394325256348
Epoch: 4, Steps: 62 | Train Loss: 0.4583363 Vali Loss: 1.1623913 Test Loss: 0.4343194
Validation loss decreased (1.165720 --> 1.162391).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.066243886947632
Epoch: 5, Steps: 62 | Train Loss: 0.4565079 Vali Loss: 1.1562897 Test Loss: 0.4323950
Validation loss decreased (1.162391 --> 1.156290).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.0581464767456055
Epoch: 6, Steps: 62 | Train Loss: 0.4556741 Vali Loss: 1.1596183 Test Loss: 0.4318665
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.148817300796509
Epoch: 7, Steps: 62 | Train Loss: 0.4552051 Vali Loss: 1.1601943 Test Loss: 0.4314377
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.5551717281341553
Epoch: 8, Steps: 62 | Train Loss: 0.4547187 Vali Loss: 1.1570835 Test Loss: 0.4315283
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.332124948501587
Epoch: 9, Steps: 62 | Train Loss: 0.4548623 Vali Loss: 1.1569264 Test Loss: 0.4312132
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.8891358375549316
Epoch: 10, Steps: 62 | Train Loss: 0.4545605 Vali Loss: 1.1567799 Test Loss: 0.4311551
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.0583243370056152
Epoch: 11, Steps: 62 | Train Loss: 0.4545989 Vali Loss: 1.1558911 Test Loss: 0.4311499
Validation loss decreased (1.156290 --> 1.155891).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.7784101963043213
Epoch: 12, Steps: 62 | Train Loss: 0.4544218 Vali Loss: 1.1571195 Test Loss: 0.4312429
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.002264976501465
Epoch: 13, Steps: 62 | Train Loss: 0.4542333 Vali Loss: 1.1482022 Test Loss: 0.4311578
Validation loss decreased (1.155891 --> 1.148202).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.4458842277526855
Epoch: 14, Steps: 62 | Train Loss: 0.4542146 Vali Loss: 1.1556711 Test Loss: 0.4312438
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.0761289596557617
Epoch: 15, Steps: 62 | Train Loss: 0.4544150 Vali Loss: 1.1548135 Test Loss: 0.4312274
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.573408365249634
Epoch: 16, Steps: 62 | Train Loss: 0.4541176 Vali Loss: 1.1599575 Test Loss: 0.4311751
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.512303590774536
Epoch: 17, Steps: 62 | Train Loss: 0.4542780 Vali Loss: 1.1550040 Test Loss: 0.4311875
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.806760787963867
Epoch: 18, Steps: 62 | Train Loss: 0.4540832 Vali Loss: 1.1579320 Test Loss: 0.4312246
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0242621898651123
Epoch: 19, Steps: 62 | Train Loss: 0.4540616 Vali Loss: 1.1562504 Test Loss: 0.4312812
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.6970360279083252
Epoch: 20, Steps: 62 | Train Loss: 0.4540908 Vali Loss: 1.1586697 Test Loss: 0.4309943
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.8884644508361816
Epoch: 21, Steps: 62 | Train Loss: 0.4540651 Vali Loss: 1.1550413 Test Loss: 0.4311394
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.186988353729248
Epoch: 22, Steps: 62 | Train Loss: 0.4539639 Vali Loss: 1.1559700 Test Loss: 0.4312692
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.1624550819396973
Epoch: 23, Steps: 62 | Train Loss: 0.4538965 Vali Loss: 1.1584762 Test Loss: 0.4312341
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.6764464378356934
Epoch: 24, Steps: 62 | Train Loss: 0.4539927 Vali Loss: 1.1563591 Test Loss: 0.4312121
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9656620025634766
Epoch: 25, Steps: 62 | Train Loss: 0.4539002 Vali Loss: 1.1566756 Test Loss: 0.4311194
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.667654037475586
Epoch: 26, Steps: 62 | Train Loss: 0.4537969 Vali Loss: 1.1544600 Test Loss: 0.4311244
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.458098888397217
Epoch: 27, Steps: 62 | Train Loss: 0.4536640 Vali Loss: 1.1540956 Test Loss: 0.4311849
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.307724714279175
Epoch: 28, Steps: 62 | Train Loss: 0.4538524 Vali Loss: 1.1547165 Test Loss: 0.4311580
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.079996347427368
Epoch: 29, Steps: 62 | Train Loss: 0.4538406 Vali Loss: 1.1554621 Test Loss: 0.4312790
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9606854915618896
Epoch: 30, Steps: 62 | Train Loss: 0.4536910 Vali Loss: 1.1532583 Test Loss: 0.4312270
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.1444876194000244
Epoch: 31, Steps: 62 | Train Loss: 0.4536224 Vali Loss: 1.1562929 Test Loss: 0.4312627
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.846797466278076
Epoch: 32, Steps: 62 | Train Loss: 0.4536770 Vali Loss: 1.1569777 Test Loss: 0.4312510
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.395983934402466
Epoch: 33, Steps: 62 | Train Loss: 0.4536348 Vali Loss: 1.1552638 Test Loss: 0.4312366
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4300363063812256, mae:0.4293206036090851, rse:0.6243161559104919, corr:[0.25306842 0.25960237 0.25852573 0.25872347 0.2576256  0.25464186
 0.25235903 0.25266126 0.25326732 0.25287554 0.25207034 0.25177968
 0.2519944  0.25187156 0.25139618 0.25118786 0.25116795 0.25111562
 0.25103593 0.25077528 0.25047356 0.25042525 0.2510409  0.25180495
 0.25177953 0.2512272  0.25082353 0.2505353  0.25005552 0.24958843
 0.24940322 0.2492785  0.24891524 0.2484823  0.2484137  0.2486537
 0.24881963 0.24872692 0.24868605 0.24882776 0.24907286 0.24920434
 0.24924804 0.24914339 0.24901459 0.24908932 0.2495086  0.24971919
 0.2491424  0.2482892  0.2476285  0.24716213 0.24642083 0.24539812
 0.24480517 0.24456468 0.24431899 0.24406305 0.2438176  0.24382854
 0.24371427 0.24354471 0.2433846  0.24339074 0.24353854 0.2438326
 0.24413122 0.24399187 0.24381271 0.24387048 0.24397463 0.24370432
 0.2430983  0.24255352 0.24213627 0.2417024  0.24126804 0.24099125
 0.2408524  0.24057561 0.2401838  0.23974214 0.23936309 0.23922703
 0.23926097 0.23923275 0.23909768 0.2389543  0.23894653 0.23898691
 0.23872666 0.23821238 0.23793802 0.23820211 0.23877849 0.23941894
 0.2399624  0.2402447  0.24057668 0.24078234 0.24076232 0.24057586
 0.24038249 0.2403077  0.24015458 0.23992684 0.23965117 0.23938318
 0.23919071 0.23915707 0.23934205 0.23973738 0.24004886 0.24015178
 0.24003053 0.23973452 0.23950744 0.23941737 0.23931688 0.23920771
 0.23912019 0.23885714 0.23836514 0.23767254 0.23708497 0.23664628
 0.23647247 0.2364707  0.23630258 0.2359648  0.23564665 0.23539151
 0.23525222 0.23542589 0.23574416 0.23583099 0.2358267  0.23588838
 0.23604457 0.23606643 0.23599121 0.23594393 0.23584758 0.23572339
 0.23563688 0.23539178 0.23500803 0.23443542 0.23389307 0.23348913
 0.23339036 0.23346342 0.23344888 0.23347399 0.23349164 0.23349458
 0.23342209 0.2333774  0.23328082 0.23313558 0.23295166 0.23296066
 0.23299482 0.23290521 0.23280413 0.23264812 0.23224191 0.23186609
 0.23197804 0.23257113 0.23313884 0.23327814 0.23317209 0.23315708
 0.2333172  0.23343438 0.2334097  0.23345259 0.23354383 0.23355024
 0.23346604 0.23341563 0.23350763 0.23367353 0.23381922 0.23409155
 0.23433475 0.23431627 0.23422101 0.23413317 0.23392713 0.23343444
 0.23275971 0.23236999 0.23220366 0.23196125 0.2313392  0.2305305
 0.23005304 0.23012714 0.23014347 0.22994187 0.22968106 0.22964364
 0.22974838 0.22983839 0.22980854 0.22974502 0.2297815  0.22993177
 0.23014425 0.2300978  0.22974825 0.22929136 0.22896832 0.22895455
 0.2290363  0.22885516 0.22880086 0.2286668  0.2283565  0.22805282
 0.22801605 0.22824726 0.22835895 0.22804438 0.22760388 0.22751237
 0.22744174 0.22716352 0.22715224 0.22743936 0.22758922 0.22733568
 0.22698869 0.22681203 0.22682065 0.22688368 0.2271018  0.22719243
 0.22699335 0.22644441 0.22624515 0.22665834 0.2270396  0.22678073
 0.22631417 0.22643937 0.22705808 0.22724909 0.22681864 0.22634678
 0.22612783 0.22602841 0.22608155 0.22624765 0.22656825 0.22690105
 0.22712524 0.22701982 0.22684298 0.22675763 0.22695109 0.22739959
 0.22755736 0.22707395 0.22619629 0.22562684 0.22531718 0.22497267
 0.22470866 0.22465616 0.22462752 0.22437435 0.22410281 0.2239987
 0.22392301 0.22350995 0.22311702 0.22333291 0.22369808 0.2234288
 0.22298166 0.22304791 0.2237936  0.22430606 0.22423619 0.22406437
 0.22428335 0.22453761 0.22473766 0.22491328 0.22503139 0.22490281
 0.22484048 0.22522812 0.22563852 0.22543986 0.22474442 0.22448128
 0.22468206 0.22454071 0.22428209 0.2242525  0.22451168 0.22444372
 0.22436994 0.22476117 0.2253268  0.22497623 0.22419494 0.22440672
 0.22553562 0.22605173 0.22534911 0.22481593 0.22457501 0.22366013
 0.22256453 0.22260801 0.22325079 0.22320059 0.22205718 0.22130534
 0.22131331 0.22091356 0.22056553 0.22139528 0.22180642 0.22094846
 0.22101715 0.22252882 0.22206098 0.21948747 0.22152887 0.22380197]
