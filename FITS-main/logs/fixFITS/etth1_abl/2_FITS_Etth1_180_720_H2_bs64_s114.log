Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=26, out_features=130, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3028480.0
params:  3510.0
Trainable parameters:  3510
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.5410823822021484
Epoch: 1, Steps: 60 | Train Loss: 1.1623178 Vali Loss: 2.6016991 Test Loss: 1.2515352
Validation loss decreased (inf --> 2.601699).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.542778491973877
Epoch: 2, Steps: 60 | Train Loss: 0.9164754 Vali Loss: 2.2047548 Test Loss: 0.9727250
Validation loss decreased (2.601699 --> 2.204755).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.1359355449676514
Epoch: 3, Steps: 60 | Train Loss: 0.7764167 Vali Loss: 1.9885983 Test Loss: 0.8143041
Validation loss decreased (2.204755 --> 1.988598).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.4597976207733154
Epoch: 4, Steps: 60 | Train Loss: 0.6911541 Vali Loss: 1.8597697 Test Loss: 0.7167963
Validation loss decreased (1.988598 --> 1.859770).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.6273274421691895
Epoch: 5, Steps: 60 | Train Loss: 0.6369475 Vali Loss: 1.7762296 Test Loss: 0.6527265
Validation loss decreased (1.859770 --> 1.776230).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2331037521362305
Epoch: 6, Steps: 60 | Train Loss: 0.6016036 Vali Loss: 1.7153571 Test Loss: 0.6092204
Validation loss decreased (1.776230 --> 1.715357).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.9518961906433105
Epoch: 7, Steps: 60 | Train Loss: 0.5769367 Vali Loss: 1.6801556 Test Loss: 0.5786877
Validation loss decreased (1.715357 --> 1.680156).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.535653829574585
Epoch: 8, Steps: 60 | Train Loss: 0.5600440 Vali Loss: 1.6462522 Test Loss: 0.5569081
Validation loss decreased (1.680156 --> 1.646252).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.717395305633545
Epoch: 9, Steps: 60 | Train Loss: 0.5466608 Vali Loss: 1.6313102 Test Loss: 0.5406641
Validation loss decreased (1.646252 --> 1.631310).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.80314564704895
Epoch: 10, Steps: 60 | Train Loss: 0.5379925 Vali Loss: 1.6127070 Test Loss: 0.5286086
Validation loss decreased (1.631310 --> 1.612707).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.951591968536377
Epoch: 11, Steps: 60 | Train Loss: 0.5308148 Vali Loss: 1.5937665 Test Loss: 0.5192056
Validation loss decreased (1.612707 --> 1.593766).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.2605233192443848
Epoch: 12, Steps: 60 | Train Loss: 0.5252414 Vali Loss: 1.5827816 Test Loss: 0.5117876
Validation loss decreased (1.593766 --> 1.582782).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.844038963317871
Epoch: 13, Steps: 60 | Train Loss: 0.5209793 Vali Loss: 1.5843062 Test Loss: 0.5059377
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.2260756492614746
Epoch: 14, Steps: 60 | Train Loss: 0.5175063 Vali Loss: 1.5714262 Test Loss: 0.5010771
Validation loss decreased (1.582782 --> 1.571426).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6215713024139404
Epoch: 15, Steps: 60 | Train Loss: 0.5147278 Vali Loss: 1.5646544 Test Loss: 0.4970481
Validation loss decreased (1.571426 --> 1.564654).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.063702344894409
Epoch: 16, Steps: 60 | Train Loss: 0.5123955 Vali Loss: 1.5648060 Test Loss: 0.4936330
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.321310520172119
Epoch: 17, Steps: 60 | Train Loss: 0.5104209 Vali Loss: 1.5629456 Test Loss: 0.4907342
Validation loss decreased (1.564654 --> 1.562946).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.598346471786499
Epoch: 18, Steps: 60 | Train Loss: 0.5086027 Vali Loss: 1.5490878 Test Loss: 0.4881454
Validation loss decreased (1.562946 --> 1.549088).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.6244449615478516
Epoch: 19, Steps: 60 | Train Loss: 0.5074530 Vali Loss: 1.5537367 Test Loss: 0.4859095
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.9422554969787598
Epoch: 20, Steps: 60 | Train Loss: 0.5057978 Vali Loss: 1.5443599 Test Loss: 0.4839026
Validation loss decreased (1.549088 --> 1.544360).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.1940202713012695
Epoch: 21, Steps: 60 | Train Loss: 0.5052145 Vali Loss: 1.5425535 Test Loss: 0.4821969
Validation loss decreased (1.544360 --> 1.542554).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.7388663291931152
Epoch: 22, Steps: 60 | Train Loss: 0.5037755 Vali Loss: 1.5483837 Test Loss: 0.4805416
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.8280715942382812
Epoch: 23, Steps: 60 | Train Loss: 0.5029153 Vali Loss: 1.5454895 Test Loss: 0.4791094
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.799919605255127
Epoch: 24, Steps: 60 | Train Loss: 0.5014539 Vali Loss: 1.5424701 Test Loss: 0.4777892
Validation loss decreased (1.542554 --> 1.542470).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.0043070316314697
Epoch: 25, Steps: 60 | Train Loss: 0.5013567 Vali Loss: 1.5423257 Test Loss: 0.4766066
Validation loss decreased (1.542470 --> 1.542326).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.854339122772217
Epoch: 26, Steps: 60 | Train Loss: 0.5001736 Vali Loss: 1.5435748 Test Loss: 0.4755238
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.2426049709320068
Epoch: 27, Steps: 60 | Train Loss: 0.5002164 Vali Loss: 1.5381200 Test Loss: 0.4745072
Validation loss decreased (1.542326 --> 1.538120).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.259840488433838
Epoch: 28, Steps: 60 | Train Loss: 0.4993614 Vali Loss: 1.5344324 Test Loss: 0.4735898
Validation loss decreased (1.538120 --> 1.534432).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.328174114227295
Epoch: 29, Steps: 60 | Train Loss: 0.4986455 Vali Loss: 1.5383794 Test Loss: 0.4727429
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.2513010501861572
Epoch: 30, Steps: 60 | Train Loss: 0.4978820 Vali Loss: 1.5347769 Test Loss: 0.4719096
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.250206708908081
Epoch: 31, Steps: 60 | Train Loss: 0.4977979 Vali Loss: 1.5342358 Test Loss: 0.4711628
Validation loss decreased (1.534432 --> 1.534236).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.2531440258026123
Epoch: 32, Steps: 60 | Train Loss: 0.4974927 Vali Loss: 1.5334628 Test Loss: 0.4705224
Validation loss decreased (1.534236 --> 1.533463).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.157562255859375
Epoch: 33, Steps: 60 | Train Loss: 0.4968812 Vali Loss: 1.5243702 Test Loss: 0.4698530
Validation loss decreased (1.533463 --> 1.524370).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3314604759216309
Epoch: 34, Steps: 60 | Train Loss: 0.4963868 Vali Loss: 1.5285678 Test Loss: 0.4692881
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.1895124912261963
Epoch: 35, Steps: 60 | Train Loss: 0.4965668 Vali Loss: 1.5302352 Test Loss: 0.4687382
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.2558467388153076
Epoch: 36, Steps: 60 | Train Loss: 0.4960199 Vali Loss: 1.5256401 Test Loss: 0.4682084
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.3255276679992676
Epoch: 37, Steps: 60 | Train Loss: 0.4957264 Vali Loss: 1.5294535 Test Loss: 0.4677449
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.3338418006896973
Epoch: 38, Steps: 60 | Train Loss: 0.4951974 Vali Loss: 1.5302489 Test Loss: 0.4672471
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.2944307327270508
Epoch: 39, Steps: 60 | Train Loss: 0.4948525 Vali Loss: 1.5186038 Test Loss: 0.4668266
Validation loss decreased (1.524370 --> 1.518604).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.235032320022583
Epoch: 40, Steps: 60 | Train Loss: 0.4946165 Vali Loss: 1.5177617 Test Loss: 0.4664278
Validation loss decreased (1.518604 --> 1.517762).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.2428557872772217
Epoch: 41, Steps: 60 | Train Loss: 0.4948755 Vali Loss: 1.5248299 Test Loss: 0.4660470
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3193120956420898
Epoch: 42, Steps: 60 | Train Loss: 0.4943551 Vali Loss: 1.5176985 Test Loss: 0.4657071
Validation loss decreased (1.517762 --> 1.517699).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.189049243927002
Epoch: 43, Steps: 60 | Train Loss: 0.4940332 Vali Loss: 1.5269367 Test Loss: 0.4653528
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.275381326675415
Epoch: 44, Steps: 60 | Train Loss: 0.4939617 Vali Loss: 1.5284504 Test Loss: 0.4650537
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.1878931522369385
Epoch: 45, Steps: 60 | Train Loss: 0.4936897 Vali Loss: 1.5195775 Test Loss: 0.4647592
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.2806396484375
Epoch: 46, Steps: 60 | Train Loss: 0.4934403 Vali Loss: 1.5186608 Test Loss: 0.4644837
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.1742236614227295
Epoch: 47, Steps: 60 | Train Loss: 0.4936641 Vali Loss: 1.5285411 Test Loss: 0.4642199
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.263735294342041
Epoch: 48, Steps: 60 | Train Loss: 0.4934335 Vali Loss: 1.5214189 Test Loss: 0.4639471
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.2300758361816406
Epoch: 49, Steps: 60 | Train Loss: 0.4933301 Vali Loss: 1.5219702 Test Loss: 0.4637272
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.3178117275238037
Epoch: 50, Steps: 60 | Train Loss: 0.4930102 Vali Loss: 1.5226421 Test Loss: 0.4634966
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3042550086975098
Epoch: 51, Steps: 60 | Train Loss: 0.4924168 Vali Loss: 1.5254401 Test Loss: 0.4632937
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.226386308670044
Epoch: 52, Steps: 60 | Train Loss: 0.4924903 Vali Loss: 1.5207136 Test Loss: 0.4630778
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.2729496955871582
Epoch: 53, Steps: 60 | Train Loss: 0.4926204 Vali Loss: 1.5209281 Test Loss: 0.4628949
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.1969435214996338
Epoch: 54, Steps: 60 | Train Loss: 0.4922586 Vali Loss: 1.5231056 Test Loss: 0.4627056
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.295403003692627
Epoch: 55, Steps: 60 | Train Loss: 0.4923718 Vali Loss: 1.5260260 Test Loss: 0.4625374
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.2994909286499023
Epoch: 56, Steps: 60 | Train Loss: 0.4922800 Vali Loss: 1.5172602 Test Loss: 0.4623756
Validation loss decreased (1.517699 --> 1.517260).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.0792787075042725
Epoch: 57, Steps: 60 | Train Loss: 0.4920740 Vali Loss: 1.5214572 Test Loss: 0.4622158
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.258368968963623
Epoch: 58, Steps: 60 | Train Loss: 0.4915891 Vali Loss: 1.5212606 Test Loss: 0.4620919
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.3432862758636475
Epoch: 59, Steps: 60 | Train Loss: 0.4919647 Vali Loss: 1.5227947 Test Loss: 0.4619361
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.2634246349334717
Epoch: 60, Steps: 60 | Train Loss: 0.4919341 Vali Loss: 1.5241961 Test Loss: 0.4618175
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.30244779586792
Epoch: 61, Steps: 60 | Train Loss: 0.4917170 Vali Loss: 1.5167501 Test Loss: 0.4616892
Validation loss decreased (1.517260 --> 1.516750).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.3018484115600586
Epoch: 62, Steps: 60 | Train Loss: 0.4917969 Vali Loss: 1.5232745 Test Loss: 0.4615733
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.2909314632415771
Epoch: 63, Steps: 60 | Train Loss: 0.4918757 Vali Loss: 1.5196114 Test Loss: 0.4614597
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.3342475891113281
Epoch: 64, Steps: 60 | Train Loss: 0.4914845 Vali Loss: 1.5240995 Test Loss: 0.4613588
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.193037748336792
Epoch: 65, Steps: 60 | Train Loss: 0.4915404 Vali Loss: 1.5220565 Test Loss: 0.4612495
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.1690382957458496
Epoch: 66, Steps: 60 | Train Loss: 0.4912508 Vali Loss: 1.5268335 Test Loss: 0.4611592
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.2596805095672607
Epoch: 67, Steps: 60 | Train Loss: 0.4913476 Vali Loss: 1.5228531 Test Loss: 0.4610727
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.892176389694214
Epoch: 68, Steps: 60 | Train Loss: 0.4918268 Vali Loss: 1.5133314 Test Loss: 0.4609829
Validation loss decreased (1.516750 --> 1.513331).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.36405873298645
Epoch: 69, Steps: 60 | Train Loss: 0.4913280 Vali Loss: 1.5197136 Test Loss: 0.4609039
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.574446678161621
Epoch: 70, Steps: 60 | Train Loss: 0.4913536 Vali Loss: 1.5184907 Test Loss: 0.4608294
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.2594425678253174
Epoch: 71, Steps: 60 | Train Loss: 0.4911571 Vali Loss: 1.5182953 Test Loss: 0.4607517
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.6162962913513184
Epoch: 72, Steps: 60 | Train Loss: 0.4910906 Vali Loss: 1.5167456 Test Loss: 0.4606848
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.9628846645355225
Epoch: 73, Steps: 60 | Train Loss: 0.4912244 Vali Loss: 1.5235877 Test Loss: 0.4606195
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 4.489825010299683
Epoch: 74, Steps: 60 | Train Loss: 0.4906718 Vali Loss: 1.5187557 Test Loss: 0.4605520
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.0394351482391357
Epoch: 75, Steps: 60 | Train Loss: 0.4907434 Vali Loss: 1.5181186 Test Loss: 0.4604960
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.693470001220703
Epoch: 76, Steps: 60 | Train Loss: 0.4907519 Vali Loss: 1.5189960 Test Loss: 0.4604410
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.372429609298706
Epoch: 77, Steps: 60 | Train Loss: 0.4904878 Vali Loss: 1.5201266 Test Loss: 0.4603874
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.8987739086151123
Epoch: 78, Steps: 60 | Train Loss: 0.4909548 Vali Loss: 1.5195423 Test Loss: 0.4603342
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.206467390060425
Epoch: 79, Steps: 60 | Train Loss: 0.4908695 Vali Loss: 1.5230291 Test Loss: 0.4602867
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.6707308292388916
Epoch: 80, Steps: 60 | Train Loss: 0.4910970 Vali Loss: 1.5197217 Test Loss: 0.4602423
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.4327147006988525
Epoch: 81, Steps: 60 | Train Loss: 0.4908657 Vali Loss: 1.5133202 Test Loss: 0.4601980
Validation loss decreased (1.513331 --> 1.513320).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.9747166633605957
Epoch: 82, Steps: 60 | Train Loss: 0.4910970 Vali Loss: 1.5169106 Test Loss: 0.4601534
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.8716092109680176
Epoch: 83, Steps: 60 | Train Loss: 0.4907907 Vali Loss: 1.5216575 Test Loss: 0.4601171
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 4.777231216430664
Epoch: 84, Steps: 60 | Train Loss: 0.4906763 Vali Loss: 1.5169234 Test Loss: 0.4600775
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.866384744644165
Epoch: 85, Steps: 60 | Train Loss: 0.4907324 Vali Loss: 1.5174608 Test Loss: 0.4600447
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.6569547653198242
Epoch: 86, Steps: 60 | Train Loss: 0.4906201 Vali Loss: 1.5187477 Test Loss: 0.4600074
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.5956778526306152
Epoch: 87, Steps: 60 | Train Loss: 0.4909137 Vali Loss: 1.5167080 Test Loss: 0.4599738
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.9816930294036865
Epoch: 88, Steps: 60 | Train Loss: 0.4910931 Vali Loss: 1.5166528 Test Loss: 0.4599427
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.5803914070129395
Epoch: 89, Steps: 60 | Train Loss: 0.4904621 Vali Loss: 1.5138009 Test Loss: 0.4599149
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.733755111694336
Epoch: 90, Steps: 60 | Train Loss: 0.4908323 Vali Loss: 1.5159354 Test Loss: 0.4598870
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.614996910095215
Epoch: 91, Steps: 60 | Train Loss: 0.4904155 Vali Loss: 1.5143518 Test Loss: 0.4598625
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 3.647810220718384
Epoch: 92, Steps: 60 | Train Loss: 0.4908110 Vali Loss: 1.5159054 Test Loss: 0.4598375
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.666438341140747
Epoch: 93, Steps: 60 | Train Loss: 0.4905304 Vali Loss: 1.5159880 Test Loss: 0.4598125
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.1222634315490723
Epoch: 94, Steps: 60 | Train Loss: 0.4907890 Vali Loss: 1.5205314 Test Loss: 0.4597908
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 4.512179851531982
Epoch: 95, Steps: 60 | Train Loss: 0.4907546 Vali Loss: 1.5224078 Test Loss: 0.4597702
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.651320457458496
Epoch: 96, Steps: 60 | Train Loss: 0.4903885 Vali Loss: 1.5160217 Test Loss: 0.4597502
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.343020439147949
Epoch: 97, Steps: 60 | Train Loss: 0.4906721 Vali Loss: 1.5192590 Test Loss: 0.4597302
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.2827346324920654
Epoch: 98, Steps: 60 | Train Loss: 0.4905004 Vali Loss: 1.5232729 Test Loss: 0.4597140
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.2253715991973877
Epoch: 99, Steps: 60 | Train Loss: 0.4906015 Vali Loss: 1.5191426 Test Loss: 0.4596955
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.2104723453521729
Epoch: 100, Steps: 60 | Train Loss: 0.4906417 Vali Loss: 1.5169017 Test Loss: 0.4596783
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=26, out_features=130, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3028480.0
params:  3510.0
Trainable parameters:  3510
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.255842685699463
Epoch: 1, Steps: 60 | Train Loss: 0.5979793 Vali Loss: 1.5081272 Test Loss: 0.4537036
Validation loss decreased (inf --> 1.508127).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.2659482955932617
Epoch: 2, Steps: 60 | Train Loss: 0.5952503 Vali Loss: 1.5108232 Test Loss: 0.4498730
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.3155100345611572
Epoch: 3, Steps: 60 | Train Loss: 0.5935343 Vali Loss: 1.5050402 Test Loss: 0.4477169
Validation loss decreased (1.508127 --> 1.505040).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.2176926136016846
Epoch: 4, Steps: 60 | Train Loss: 0.5927730 Vali Loss: 1.5044161 Test Loss: 0.4466771
Validation loss decreased (1.505040 --> 1.504416).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.284860372543335
Epoch: 5, Steps: 60 | Train Loss: 0.5925547 Vali Loss: 1.4979650 Test Loss: 0.4460854
Validation loss decreased (1.504416 --> 1.497965).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.253204584121704
Epoch: 6, Steps: 60 | Train Loss: 0.5918251 Vali Loss: 1.4948555 Test Loss: 0.4458786
Validation loss decreased (1.497965 --> 1.494856).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.3388128280639648
Epoch: 7, Steps: 60 | Train Loss: 0.5917728 Vali Loss: 1.4966891 Test Loss: 0.4455891
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.4009740352630615
Epoch: 8, Steps: 60 | Train Loss: 0.5915259 Vali Loss: 1.5001671 Test Loss: 0.4456946
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3136265277862549
Epoch: 9, Steps: 60 | Train Loss: 0.5916499 Vali Loss: 1.4967840 Test Loss: 0.4456560
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.2983756065368652
Epoch: 10, Steps: 60 | Train Loss: 0.5912947 Vali Loss: 1.5006545 Test Loss: 0.4458538
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.3187015056610107
Epoch: 11, Steps: 60 | Train Loss: 0.5912079 Vali Loss: 1.4840522 Test Loss: 0.4460722
Validation loss decreased (1.494856 --> 1.484052).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.2086637020111084
Epoch: 12, Steps: 60 | Train Loss: 0.5911578 Vali Loss: 1.4899747 Test Loss: 0.4462146
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.4007282257080078
Epoch: 13, Steps: 60 | Train Loss: 0.5910002 Vali Loss: 1.4959238 Test Loss: 0.4461273
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.4018149375915527
Epoch: 14, Steps: 60 | Train Loss: 0.5915328 Vali Loss: 1.5046754 Test Loss: 0.4463410
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.321274995803833
Epoch: 15, Steps: 60 | Train Loss: 0.5903051 Vali Loss: 1.4959097 Test Loss: 0.4463309
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.2368688583374023
Epoch: 16, Steps: 60 | Train Loss: 0.5902212 Vali Loss: 1.4961579 Test Loss: 0.4463791
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.2817578315734863
Epoch: 17, Steps: 60 | Train Loss: 0.5906064 Vali Loss: 1.4917716 Test Loss: 0.4466368
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.2264471054077148
Epoch: 18, Steps: 60 | Train Loss: 0.5907196 Vali Loss: 1.4957136 Test Loss: 0.4466309
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4044110774993896
Epoch: 19, Steps: 60 | Train Loss: 0.5908060 Vali Loss: 1.4962778 Test Loss: 0.4466503
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3047995567321777
Epoch: 20, Steps: 60 | Train Loss: 0.5907722 Vali Loss: 1.4882784 Test Loss: 0.4466202
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.2507977485656738
Epoch: 21, Steps: 60 | Train Loss: 0.5904562 Vali Loss: 1.5001059 Test Loss: 0.4467675
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2108433246612549
Epoch: 22, Steps: 60 | Train Loss: 0.5907677 Vali Loss: 1.4993021 Test Loss: 0.4468164
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.369184970855713
Epoch: 23, Steps: 60 | Train Loss: 0.5907704 Vali Loss: 1.4971917 Test Loss: 0.4468305
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.2558801174163818
Epoch: 24, Steps: 60 | Train Loss: 0.5910090 Vali Loss: 1.4924706 Test Loss: 0.4468692
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.2565336227416992
Epoch: 25, Steps: 60 | Train Loss: 0.5902477 Vali Loss: 1.4970956 Test Loss: 0.4469047
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.273329496383667
Epoch: 26, Steps: 60 | Train Loss: 0.5903415 Vali Loss: 1.4911660 Test Loss: 0.4468157
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.353323221206665
Epoch: 27, Steps: 60 | Train Loss: 0.5906286 Vali Loss: 1.4951134 Test Loss: 0.4468904
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.3334364891052246
Epoch: 28, Steps: 60 | Train Loss: 0.5904106 Vali Loss: 1.4987489 Test Loss: 0.4469145
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.2741940021514893
Epoch: 29, Steps: 60 | Train Loss: 0.5903762 Vali Loss: 1.4949931 Test Loss: 0.4468789
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.361772060394287
Epoch: 30, Steps: 60 | Train Loss: 0.5901718 Vali Loss: 1.4942175 Test Loss: 0.4469613
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6164801120758057
Epoch: 31, Steps: 60 | Train Loss: 0.5902168 Vali Loss: 1.4947829 Test Loss: 0.4470252
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4449608623981476, mae:0.45415133237838745, rse:0.6385764479637146, corr:[0.23062229 0.23285556 0.23290141 0.23116273 0.22888337 0.22735687
 0.22731565 0.22845829 0.22898844 0.22881961 0.22830023 0.22800936
 0.22809662 0.22780207 0.22717214 0.22661148 0.22614475 0.22584425
 0.22567049 0.22570904 0.22603376 0.22659455 0.22694631 0.22668453
 0.22591417 0.22566965 0.22578338 0.22579998 0.22529596 0.22459058
 0.22396596 0.22361082 0.22342691 0.22345522 0.22367178 0.22400226
 0.2243171  0.22412017 0.2238269  0.2237447  0.22392678 0.22401577
 0.22398883 0.22416696 0.22473875 0.2254327  0.22606924 0.2259733
 0.2248276  0.22386552 0.22275823 0.22152281 0.22019474 0.21901578
 0.21840899 0.2184314  0.21853681 0.21863328 0.21851328 0.21882406
 0.21941835 0.21956317 0.21935458 0.21910444 0.21894948 0.21888301
 0.21889848 0.2189905  0.2192081  0.21938223 0.21925306 0.21846922
 0.21705157 0.21617635 0.21567707 0.21545574 0.21525022 0.21489236
 0.21471606 0.21472837 0.21457624 0.21436891 0.21399587 0.21393082
 0.21429698 0.21434881 0.21419854 0.21393554 0.2136289  0.21331273
 0.21299426 0.21292236 0.21321864 0.21381408 0.21462433 0.21515557
 0.21503812 0.21510737 0.21527928 0.21507175 0.21453738 0.21403062
 0.21375762 0.21405344 0.21423496 0.21416415 0.213885   0.21400125
 0.21450485 0.2146711  0.21449575 0.21427032 0.21403652 0.21370901
 0.21347393 0.21345672 0.21368682 0.21390958 0.2139912  0.21357304
 0.21252213 0.21170977 0.21085337 0.21018466 0.20961578 0.20933276
 0.2093931  0.20972471 0.20976871 0.20963521 0.20950177 0.20984629
 0.21049829 0.21069597 0.21075182 0.21080075 0.2107921  0.21066928
 0.21048498 0.21042562 0.21059662 0.21085966 0.21102288 0.21067888
 0.20973484 0.20899366 0.20846462 0.20769061 0.20697427 0.20652872
 0.20651323 0.20681307 0.20718186 0.20735563 0.20721403 0.20723271
 0.20755291 0.20758997 0.2075231  0.20737003 0.20717725 0.20707147
 0.20707035 0.20718731 0.20745227 0.20770223 0.20785761 0.20761491
 0.20705363 0.2069286  0.2069183  0.2067541  0.20650844 0.20633425
 0.20645483 0.20695314 0.20734832 0.20768452 0.20793827 0.20843107
 0.20910849 0.20924734 0.20910394 0.20899466 0.20894292 0.20898332
 0.20910197 0.20927703 0.2095769  0.20985113 0.2098788  0.20937288
 0.20830368 0.20753372 0.2068821  0.20617595 0.2054492  0.204877
 0.20466334 0.20485851 0.20512547 0.20530456 0.20532297 0.20575322
 0.20641054 0.20669714 0.20665768 0.20639184 0.20605823 0.20572798
 0.2054454  0.20531403 0.20538351 0.20548427 0.20541199 0.20499462
 0.20429258 0.20396866 0.20379668 0.2035057  0.20325498 0.20300733
 0.20301512 0.20319517 0.20325112 0.20313358 0.2029217  0.20305614
 0.20357601 0.2036455  0.20336764 0.20301053 0.20263728 0.20233978
 0.202193   0.20219582 0.20235124 0.20247842 0.20246547 0.20208311
 0.2014174  0.2011247  0.20100945 0.20093958 0.20083135 0.20076638
 0.20101674 0.20138498 0.20159364 0.20161916 0.20163545 0.2021251
 0.20297392 0.20336473 0.2035092  0.20349063 0.2033903  0.20324068
 0.20306975 0.20281251 0.20273633 0.20275861 0.20272107 0.20241146
 0.20181255 0.20157245 0.20141904 0.20112544 0.20076786 0.20051578
 0.20058972 0.20094448 0.20106854 0.20099422 0.20084123 0.2009168
 0.20139575 0.20159973 0.20152825 0.2013074  0.20100239 0.2007632
 0.20068185 0.20084018 0.20121087 0.20159084 0.20194548 0.20203975
 0.20180255 0.20176822 0.20186742 0.2018812  0.20186538 0.20207919
 0.20250471 0.20297617 0.20323038 0.2032867  0.20323533 0.20347796
 0.20404784 0.20432504 0.2043945  0.20438161 0.20425622 0.20404321
 0.20387767 0.20382851 0.20391615 0.20412767 0.20431915 0.20432214
 0.20403802 0.20396896 0.20386332 0.20347987 0.20303567 0.20254062
 0.20228729 0.20231248 0.20224676 0.20209613 0.20199217 0.20228307
 0.20294465 0.20319033 0.20318873 0.20328623 0.2033786  0.20335317
 0.2032156  0.20310658 0.20303224 0.20299342 0.20295034 0.20271112
 0.20222704 0.20203488 0.20183748 0.20148245 0.2010112  0.20060928
 0.20057866 0.200865   0.20121005 0.20145509 0.2016702  0.20204036
 0.20246747 0.20261125 0.2026515  0.20270212 0.20263866 0.20252842
 0.2023802  0.20230608 0.20232683 0.20243816 0.20236789 0.20204356
 0.20152058 0.20120527 0.2009381  0.20053953 0.20008199 0.1997621
 0.19973867 0.19984819 0.19981065 0.1995793  0.19933519 0.19932474
 0.19952157 0.19935012 0.19895898 0.19859906 0.19842969 0.19840159
 0.19854917 0.19886456 0.1993215  0.19988403 0.20036943 0.20074145
 0.2009738  0.20144846 0.20180461 0.20172761 0.2013028  0.20061949
 0.20014861 0.19988473 0.1995591  0.19925383 0.19902116 0.1993759
 0.20027845 0.20070556 0.20083693 0.2008298  0.20080087 0.20086084
 0.20095854 0.2011408  0.20147169 0.20193715 0.20235536 0.2025731
 0.20245071 0.20248784 0.20253539 0.20228437 0.20190352 0.20157613
 0.20164354 0.20197098 0.2021633  0.20211549 0.20211315 0.20251928
 0.20325513 0.20357235 0.20340602 0.20301999 0.2025561  0.20220178
 0.20211849 0.20247541 0.20303379 0.20387255 0.20454966 0.20473863
 0.20437948 0.20428634 0.20427568 0.20408905 0.203913   0.20377797
 0.20379679 0.20404558 0.20423858 0.2044334  0.20461038 0.20509885
 0.20574519 0.20586786 0.20566851 0.20546539 0.2053043  0.20521398
 0.20524019 0.20541883 0.2058141  0.20638567 0.20691067 0.20720014
 0.20702997 0.20704006 0.20689604 0.20638017 0.20568521 0.20501465
 0.2047388  0.2048962  0.20521206 0.20549417 0.20577896 0.20637542
 0.2072463  0.20766492 0.2077962  0.20774244 0.20764184 0.20755665
 0.20749623 0.20760347 0.20784637 0.20801666 0.20812969 0.20791647
 0.20724407 0.20681736 0.20655468 0.20624918 0.20598155 0.20571306
 0.20568338 0.20586637 0.2060579  0.20619427 0.20619522 0.20647235
 0.20708631 0.20724481 0.2070616  0.206878   0.20675945 0.20672101
 0.20684652 0.20725018 0.2079046  0.20871928 0.20952179 0.20995349
 0.20984453 0.20996135 0.21004978 0.20975986 0.20925876 0.20872742
 0.20842454 0.20830026 0.20818643 0.20784973 0.20759141 0.20794019
 0.20876622 0.20921408 0.20940836 0.209521   0.2095929  0.20963998
 0.20978127 0.21022207 0.21091257 0.21168979 0.21237595 0.21246496
 0.21180193 0.21132745 0.21095099 0.2103782  0.20961756 0.20893966
 0.20872386 0.20891291 0.20900224 0.20889002 0.2087092  0.20898847
 0.20979379 0.21020044 0.21028341 0.21025327 0.2102568  0.21032375
 0.21033604 0.21035579 0.21043125 0.21046115 0.21035877 0.2098451
 0.20875224 0.20811169 0.20783904 0.2073656  0.20656772 0.20582485
 0.20537204 0.20514633 0.20517087 0.20525722 0.20530587 0.20577092
 0.2066693  0.20681351 0.20650716 0.20611459 0.20587666 0.20573606
 0.20562696 0.20551099 0.20559175 0.20578234 0.20598266 0.20567518
 0.20471257 0.2041541  0.20374677 0.20307234 0.20219533 0.20112476
 0.20056234 0.20048918 0.20061792 0.20062503 0.20038633 0.2004454
 0.2007587  0.20056665 0.20000863 0.19954853 0.19932184 0.19923234
 0.19921133 0.19921319 0.19916488 0.19903584 0.19873866 0.19801055
 0.1966445  0.19571191 0.19521417 0.19454269 0.19369207 0.1929472
 0.19254656 0.19251806 0.19255996 0.19245142 0.19223873 0.192323
 0.19287746 0.19290423 0.1925427  0.19222452 0.19202001 0.19190754
 0.19179021 0.19171497 0.1916957  0.19164503 0.19152512 0.19097209
 0.18990178 0.18920414 0.18879566 0.1881342  0.18734667 0.18661487
 0.18636699 0.18651925 0.18673137 0.18688707 0.18686019 0.18715493
 0.18797925 0.18816373 0.18782318 0.18747213 0.18707904 0.18674543
 0.18650426 0.18649799 0.18668939 0.18701461 0.18724105 0.18687487
 0.18570705 0.18466571 0.18382965 0.1831181  0.18249467 0.18202272
 0.1819604  0.18211515 0.1823027  0.18228383 0.18216626 0.18244219
 0.18313715 0.18326432 0.1829822  0.18267395 0.18256913 0.18257399
 0.18262842 0.1827377  0.18282567 0.18290289 0.18277301 0.18212938
 0.18068457 0.179467   0.1786202  0.17753044 0.17639023 0.17540881
 0.17505032 0.17515509 0.17559987 0.17593819 0.17617166 0.17674555
 0.17754354 0.177486   0.1770062  0.17671573 0.17666589 0.17671882
 0.1767241  0.17676558 0.17689776 0.17716387 0.17739671 0.1771034
 0.17622255 0.17575604 0.17544927 0.17488101 0.17410776 0.1734881
 0.17320336 0.17313401 0.17312105 0.17280388 0.17224    0.17206918
 0.17244205 0.17202245 0.17114377 0.17044273 0.17016248 0.16999894
 0.1697616  0.16952653 0.16964526 0.17002231 0.17006847 0.16758968]
