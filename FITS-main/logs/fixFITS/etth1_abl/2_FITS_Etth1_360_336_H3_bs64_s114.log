Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=58, out_features=112, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5820416.0
params:  6608.0
Trainable parameters:  6608
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0908608436584473
Epoch: 1, Steps: 62 | Train Loss: 0.7288274 Vali Loss: 1.8318098 Test Loss: 0.8702171
Validation loss decreased (inf --> 1.831810).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.473706007003784
Epoch: 2, Steps: 62 | Train Loss: 0.5874245 Vali Loss: 1.6469575 Test Loss: 0.7621181
Validation loss decreased (1.831810 --> 1.646958).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.9764721393585205
Epoch: 3, Steps: 62 | Train Loss: 0.5071345 Vali Loss: 1.5436668 Test Loss: 0.7033066
Validation loss decreased (1.646958 --> 1.543667).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.7779028415679932
Epoch: 4, Steps: 62 | Train Loss: 0.4596727 Vali Loss: 1.4899659 Test Loss: 0.6706699
Validation loss decreased (1.543667 --> 1.489966).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7472755908966064
Epoch: 5, Steps: 62 | Train Loss: 0.4291161 Vali Loss: 1.4530326 Test Loss: 0.6480142
Validation loss decreased (1.489966 --> 1.453033).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.5418775081634521
Epoch: 6, Steps: 62 | Train Loss: 0.4077572 Vali Loss: 1.4307494 Test Loss: 0.6332299
Validation loss decreased (1.453033 --> 1.430749).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5170211791992188
Epoch: 7, Steps: 62 | Train Loss: 0.3917313 Vali Loss: 1.4126343 Test Loss: 0.6212686
Validation loss decreased (1.430749 --> 1.412634).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.7780983448028564
Epoch: 8, Steps: 62 | Train Loss: 0.3788232 Vali Loss: 1.3941966 Test Loss: 0.6104429
Validation loss decreased (1.412634 --> 1.394197).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.1408793926239014
Epoch: 9, Steps: 62 | Train Loss: 0.3680624 Vali Loss: 1.3857415 Test Loss: 0.6009943
Validation loss decreased (1.394197 --> 1.385741).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.9902091026306152
Epoch: 10, Steps: 62 | Train Loss: 0.3589734 Vali Loss: 1.3691850 Test Loss: 0.5930070
Validation loss decreased (1.385741 --> 1.369185).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.2093007564544678
Epoch: 11, Steps: 62 | Train Loss: 0.3510047 Vali Loss: 1.3567325 Test Loss: 0.5853530
Validation loss decreased (1.369185 --> 1.356732).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.108431339263916
Epoch: 12, Steps: 62 | Train Loss: 0.3439264 Vali Loss: 1.3523601 Test Loss: 0.5790338
Validation loss decreased (1.356732 --> 1.352360).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.255979299545288
Epoch: 13, Steps: 62 | Train Loss: 0.3378540 Vali Loss: 1.3443378 Test Loss: 0.5720026
Validation loss decreased (1.352360 --> 1.344338).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.771754264831543
Epoch: 14, Steps: 62 | Train Loss: 0.3322004 Vali Loss: 1.3332361 Test Loss: 0.5660219
Validation loss decreased (1.344338 --> 1.333236).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.317237377166748
Epoch: 15, Steps: 62 | Train Loss: 0.3272685 Vali Loss: 1.3219939 Test Loss: 0.5605026
Validation loss decreased (1.333236 --> 1.321994).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.783324956893921
Epoch: 16, Steps: 62 | Train Loss: 0.3228068 Vali Loss: 1.3202229 Test Loss: 0.5554353
Validation loss decreased (1.321994 --> 1.320223).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.332941770553589
Epoch: 17, Steps: 62 | Train Loss: 0.3187836 Vali Loss: 1.3090658 Test Loss: 0.5506216
Validation loss decreased (1.320223 --> 1.309066).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.9463911056518555
Epoch: 18, Steps: 62 | Train Loss: 0.3151921 Vali Loss: 1.3027233 Test Loss: 0.5462939
Validation loss decreased (1.309066 --> 1.302723).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.427102565765381
Epoch: 19, Steps: 62 | Train Loss: 0.3118691 Vali Loss: 1.2964590 Test Loss: 0.5418768
Validation loss decreased (1.302723 --> 1.296459).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.5847995281219482
Epoch: 20, Steps: 62 | Train Loss: 0.3088199 Vali Loss: 1.2951298 Test Loss: 0.5380262
Validation loss decreased (1.296459 --> 1.295130).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.4648454189300537
Epoch: 21, Steps: 62 | Train Loss: 0.3061421 Vali Loss: 1.2894728 Test Loss: 0.5346389
Validation loss decreased (1.295130 --> 1.289473).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.4242708683013916
Epoch: 22, Steps: 62 | Train Loss: 0.3035581 Vali Loss: 1.2873138 Test Loss: 0.5311414
Validation loss decreased (1.289473 --> 1.287314).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.1849381923675537
Epoch: 23, Steps: 62 | Train Loss: 0.3012065 Vali Loss: 1.2822497 Test Loss: 0.5282301
Validation loss decreased (1.287314 --> 1.282250).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8200604915618896
Epoch: 24, Steps: 62 | Train Loss: 0.2990541 Vali Loss: 1.2763530 Test Loss: 0.5252990
Validation loss decreased (1.282250 --> 1.276353).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.0760350227355957
Epoch: 25, Steps: 62 | Train Loss: 0.2971317 Vali Loss: 1.2708359 Test Loss: 0.5224172
Validation loss decreased (1.276353 --> 1.270836).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7919948101043701
Epoch: 26, Steps: 62 | Train Loss: 0.2953145 Vali Loss: 1.2695807 Test Loss: 0.5199651
Validation loss decreased (1.270836 --> 1.269581).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.9372124671936035
Epoch: 27, Steps: 62 | Train Loss: 0.2936152 Vali Loss: 1.2673800 Test Loss: 0.5175956
Validation loss decreased (1.269581 --> 1.267380).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.0223612785339355
Epoch: 28, Steps: 62 | Train Loss: 0.2920995 Vali Loss: 1.2621621 Test Loss: 0.5153383
Validation loss decreased (1.267380 --> 1.262162).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.999089002609253
Epoch: 29, Steps: 62 | Train Loss: 0.2905514 Vali Loss: 1.2553524 Test Loss: 0.5130786
Validation loss decreased (1.262162 --> 1.255352).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.956489086151123
Epoch: 30, Steps: 62 | Train Loss: 0.2893089 Vali Loss: 1.2548469 Test Loss: 0.5112596
Validation loss decreased (1.255352 --> 1.254847).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.1300265789031982
Epoch: 31, Steps: 62 | Train Loss: 0.2880489 Vali Loss: 1.2540704 Test Loss: 0.5093665
Validation loss decreased (1.254847 --> 1.254070).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.20125675201416
Epoch: 32, Steps: 62 | Train Loss: 0.2868425 Vali Loss: 1.2494788 Test Loss: 0.5078543
Validation loss decreased (1.254070 --> 1.249479).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.1939713954925537
Epoch: 33, Steps: 62 | Train Loss: 0.2858533 Vali Loss: 1.2487367 Test Loss: 0.5060216
Validation loss decreased (1.249479 --> 1.248737).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7735850811004639
Epoch: 34, Steps: 62 | Train Loss: 0.2847725 Vali Loss: 1.2463284 Test Loss: 0.5043463
Validation loss decreased (1.248737 --> 1.246328).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.6670613288879395
Epoch: 35, Steps: 62 | Train Loss: 0.2839002 Vali Loss: 1.2412258 Test Loss: 0.5030433
Validation loss decreased (1.246328 --> 1.241226).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.809119701385498
Epoch: 36, Steps: 62 | Train Loss: 0.2830001 Vali Loss: 1.2444973 Test Loss: 0.5016177
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8901996612548828
Epoch: 37, Steps: 62 | Train Loss: 0.2821726 Vali Loss: 1.2430269 Test Loss: 0.5003659
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.234736442565918
Epoch: 38, Steps: 62 | Train Loss: 0.2813974 Vali Loss: 1.2427789 Test Loss: 0.4991386
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.6575794219970703
Epoch: 39, Steps: 62 | Train Loss: 0.2806356 Vali Loss: 1.2374016 Test Loss: 0.4981009
Validation loss decreased (1.241226 --> 1.237402).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.996687650680542
Epoch: 40, Steps: 62 | Train Loss: 0.2800388 Vali Loss: 1.2355179 Test Loss: 0.4969447
Validation loss decreased (1.237402 --> 1.235518).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.0831825733184814
Epoch: 41, Steps: 62 | Train Loss: 0.2792905 Vali Loss: 1.2323128 Test Loss: 0.4959045
Validation loss decreased (1.235518 --> 1.232313).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.7646145820617676
Epoch: 42, Steps: 62 | Train Loss: 0.2786641 Vali Loss: 1.2303178 Test Loss: 0.4949600
Validation loss decreased (1.232313 --> 1.230318).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0042192935943604
Epoch: 43, Steps: 62 | Train Loss: 0.2781925 Vali Loss: 1.2331153 Test Loss: 0.4940454
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.4596471786499023
Epoch: 44, Steps: 62 | Train Loss: 0.2776614 Vali Loss: 1.2391485 Test Loss: 0.4931937
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.650724172592163
Epoch: 45, Steps: 62 | Train Loss: 0.2770412 Vali Loss: 1.2277117 Test Loss: 0.4923639
Validation loss decreased (1.230318 --> 1.227712).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.5347325801849365
Epoch: 46, Steps: 62 | Train Loss: 0.2765552 Vali Loss: 1.2288525 Test Loss: 0.4915941
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.9059290885925293
Epoch: 47, Steps: 62 | Train Loss: 0.2761855 Vali Loss: 1.2310888 Test Loss: 0.4908862
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.210852861404419
Epoch: 48, Steps: 62 | Train Loss: 0.2758011 Vali Loss: 1.2271172 Test Loss: 0.4901691
Validation loss decreased (1.227712 --> 1.227117).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.1872451305389404
Epoch: 49, Steps: 62 | Train Loss: 0.2754124 Vali Loss: 1.2264762 Test Loss: 0.4895266
Validation loss decreased (1.227117 --> 1.226476).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.545905828475952
Epoch: 50, Steps: 62 | Train Loss: 0.2748932 Vali Loss: 1.2245774 Test Loss: 0.4889413
Validation loss decreased (1.226476 --> 1.224577).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.4866859912872314
Epoch: 51, Steps: 62 | Train Loss: 0.2745812 Vali Loss: 1.2271934 Test Loss: 0.4882867
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.773191452026367
Epoch: 52, Steps: 62 | Train Loss: 0.2742239 Vali Loss: 1.2206782 Test Loss: 0.4877149
Validation loss decreased (1.224577 --> 1.220678).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.2126448154449463
Epoch: 53, Steps: 62 | Train Loss: 0.2739591 Vali Loss: 1.2238308 Test Loss: 0.4871873
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5988075733184814
Epoch: 54, Steps: 62 | Train Loss: 0.2735414 Vali Loss: 1.2243340 Test Loss: 0.4866951
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.6367549896240234
Epoch: 55, Steps: 62 | Train Loss: 0.2733120 Vali Loss: 1.2267338 Test Loss: 0.4862409
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7545363903045654
Epoch: 56, Steps: 62 | Train Loss: 0.2731025 Vali Loss: 1.2196733 Test Loss: 0.4857477
Validation loss decreased (1.220678 --> 1.219673).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.7173779010772705
Epoch: 57, Steps: 62 | Train Loss: 0.2727798 Vali Loss: 1.2221942 Test Loss: 0.4853142
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.174642562866211
Epoch: 58, Steps: 62 | Train Loss: 0.2724430 Vali Loss: 1.2234404 Test Loss: 0.4849446
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.7942066192626953
Epoch: 59, Steps: 62 | Train Loss: 0.2723093 Vali Loss: 1.2178732 Test Loss: 0.4845961
Validation loss decreased (1.219673 --> 1.217873).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.6267752647399902
Epoch: 60, Steps: 62 | Train Loss: 0.2720341 Vali Loss: 1.2195107 Test Loss: 0.4841824
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6430695056915283
Epoch: 61, Steps: 62 | Train Loss: 0.2719037 Vali Loss: 1.2178072 Test Loss: 0.4838454
Validation loss decreased (1.217873 --> 1.217807).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6480906009674072
Epoch: 62, Steps: 62 | Train Loss: 0.2716427 Vali Loss: 1.2184952 Test Loss: 0.4835070
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.7270147800445557
Epoch: 63, Steps: 62 | Train Loss: 0.2715139 Vali Loss: 1.2192829 Test Loss: 0.4832275
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.71618914604187
Epoch: 64, Steps: 62 | Train Loss: 0.2713557 Vali Loss: 1.2204833 Test Loss: 0.4829106
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.7005715370178223
Epoch: 65, Steps: 62 | Train Loss: 0.2711189 Vali Loss: 1.2211809 Test Loss: 0.4826092
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.009612560272217
Epoch: 66, Steps: 62 | Train Loss: 0.2709929 Vali Loss: 1.2149419 Test Loss: 0.4823513
Validation loss decreased (1.217807 --> 1.214942).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.1208336353302
Epoch: 67, Steps: 62 | Train Loss: 0.2708375 Vali Loss: 1.2152102 Test Loss: 0.4820965
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.808835506439209
Epoch: 68, Steps: 62 | Train Loss: 0.2706575 Vali Loss: 1.2168700 Test Loss: 0.4818302
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.7839086055755615
Epoch: 69, Steps: 62 | Train Loss: 0.2706401 Vali Loss: 1.2232580 Test Loss: 0.4815972
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 3.106973171234131
Epoch: 70, Steps: 62 | Train Loss: 0.2703995 Vali Loss: 1.2166362 Test Loss: 0.4813779
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.7411751747131348
Epoch: 71, Steps: 62 | Train Loss: 0.2703188 Vali Loss: 1.2133259 Test Loss: 0.4811830
Validation loss decreased (1.214942 --> 1.213326).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.6381356716156006
Epoch: 72, Steps: 62 | Train Loss: 0.2702047 Vali Loss: 1.2126791 Test Loss: 0.4809991
Validation loss decreased (1.213326 --> 1.212679).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.6887266635894775
Epoch: 73, Steps: 62 | Train Loss: 0.2699819 Vali Loss: 1.2159901 Test Loss: 0.4807846
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.073129653930664
Epoch: 74, Steps: 62 | Train Loss: 0.2699288 Vali Loss: 1.2170823 Test Loss: 0.4806005
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.07161283493042
Epoch: 75, Steps: 62 | Train Loss: 0.2698112 Vali Loss: 1.2138133 Test Loss: 0.4804371
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.531602382659912
Epoch: 76, Steps: 62 | Train Loss: 0.2697268 Vali Loss: 1.2142299 Test Loss: 0.4802735
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.9759018421173096
Epoch: 77, Steps: 62 | Train Loss: 0.2696271 Vali Loss: 1.2158755 Test Loss: 0.4801141
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.201582908630371
Epoch: 78, Steps: 62 | Train Loss: 0.2694708 Vali Loss: 1.2192001 Test Loss: 0.4799746
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.66853666305542
Epoch: 79, Steps: 62 | Train Loss: 0.2694401 Vali Loss: 1.2160717 Test Loss: 0.4798325
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.6100153923034668
Epoch: 80, Steps: 62 | Train Loss: 0.2693996 Vali Loss: 1.2188778 Test Loss: 0.4797082
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.8097758293151855
Epoch: 81, Steps: 62 | Train Loss: 0.2692972 Vali Loss: 1.2195207 Test Loss: 0.4795760
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.6170837879180908
Epoch: 82, Steps: 62 | Train Loss: 0.2692658 Vali Loss: 1.2147366 Test Loss: 0.4794442
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.6597349643707275
Epoch: 83, Steps: 62 | Train Loss: 0.2690935 Vali Loss: 1.2133656 Test Loss: 0.4793347
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.7183172702789307
Epoch: 84, Steps: 62 | Train Loss: 0.2690530 Vali Loss: 1.2114716 Test Loss: 0.4792362
Validation loss decreased (1.212679 --> 1.211472).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.779998779296875
Epoch: 85, Steps: 62 | Train Loss: 0.2690075 Vali Loss: 1.2125229 Test Loss: 0.4791219
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.410698175430298
Epoch: 86, Steps: 62 | Train Loss: 0.2689890 Vali Loss: 1.2110730 Test Loss: 0.4790321
Validation loss decreased (1.211472 --> 1.211073).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.2659823894500732
Epoch: 87, Steps: 62 | Train Loss: 0.2689221 Vali Loss: 1.2205385 Test Loss: 0.4789337
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.9783148765563965
Epoch: 88, Steps: 62 | Train Loss: 0.2688072 Vali Loss: 1.2113446 Test Loss: 0.4788413
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.493736982345581
Epoch: 89, Steps: 62 | Train Loss: 0.2689031 Vali Loss: 1.2161887 Test Loss: 0.4787607
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.487616777420044
Epoch: 90, Steps: 62 | Train Loss: 0.2687577 Vali Loss: 1.2127395 Test Loss: 0.4786832
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.4337408542633057
Epoch: 91, Steps: 62 | Train Loss: 0.2687580 Vali Loss: 1.2121069 Test Loss: 0.4786012
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 3.0200016498565674
Epoch: 92, Steps: 62 | Train Loss: 0.2687221 Vali Loss: 1.2138077 Test Loss: 0.4785249
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.9660325050354004
Epoch: 93, Steps: 62 | Train Loss: 0.2686177 Vali Loss: 1.2133650 Test Loss: 0.4784614
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.4051308631896973
Epoch: 94, Steps: 62 | Train Loss: 0.2686825 Vali Loss: 1.2100840 Test Loss: 0.4783916
Validation loss decreased (1.211073 --> 1.210084).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.227027416229248
Epoch: 95, Steps: 62 | Train Loss: 0.2686009 Vali Loss: 1.2143326 Test Loss: 0.4783312
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.7741384506225586
Epoch: 96, Steps: 62 | Train Loss: 0.2684957 Vali Loss: 1.2131320 Test Loss: 0.4782731
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.5842843055725098
Epoch: 97, Steps: 62 | Train Loss: 0.2685563 Vali Loss: 1.2127079 Test Loss: 0.4782172
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.7751789093017578
Epoch: 98, Steps: 62 | Train Loss: 0.2685370 Vali Loss: 1.2106901 Test Loss: 0.4781625
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.1988680362701416
Epoch: 99, Steps: 62 | Train Loss: 0.2684638 Vali Loss: 1.2115765 Test Loss: 0.4781061
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.915381908416748
Epoch: 100, Steps: 62 | Train Loss: 0.2683633 Vali Loss: 1.2138705 Test Loss: 0.4780594
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=58, out_features=112, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5820416.0
params:  6608.0
Trainable parameters:  6608
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8045382499694824
Epoch: 1, Steps: 62 | Train Loss: 0.4788924 Vali Loss: 1.1812892 Test Loss: 0.4545269
Validation loss decreased (inf --> 1.181289).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.695345401763916
Epoch: 2, Steps: 62 | Train Loss: 0.4693060 Vali Loss: 1.1690620 Test Loss: 0.4436737
Validation loss decreased (1.181289 --> 1.169062).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0469751358032227
Epoch: 3, Steps: 62 | Train Loss: 0.4650535 Vali Loss: 1.1610101 Test Loss: 0.4388195
Validation loss decreased (1.169062 --> 1.161010).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.7431049346923828
Epoch: 4, Steps: 62 | Train Loss: 0.4630805 Vali Loss: 1.1639181 Test Loss: 0.4368603
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5948922634124756
Epoch: 5, Steps: 62 | Train Loss: 0.4620610 Vali Loss: 1.1610962 Test Loss: 0.4359674
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.7514231204986572
Epoch: 6, Steps: 62 | Train Loss: 0.4617294 Vali Loss: 1.1571372 Test Loss: 0.4355963
Validation loss decreased (1.161010 --> 1.157137).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.852952003479004
Epoch: 7, Steps: 62 | Train Loss: 0.4614189 Vali Loss: 1.1594580 Test Loss: 0.4356129
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.11860990524292
Epoch: 8, Steps: 62 | Train Loss: 0.4612540 Vali Loss: 1.1632118 Test Loss: 0.4354095
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.5807278156280518
Epoch: 9, Steps: 62 | Train Loss: 0.4612186 Vali Loss: 1.1579542 Test Loss: 0.4353677
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.637626886367798
Epoch: 10, Steps: 62 | Train Loss: 0.4610229 Vali Loss: 1.1596729 Test Loss: 0.4352277
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.464383125305176
Epoch: 11, Steps: 62 | Train Loss: 0.4607713 Vali Loss: 1.1589035 Test Loss: 0.4352249
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.7336068153381348
Epoch: 12, Steps: 62 | Train Loss: 0.4609927 Vali Loss: 1.1595263 Test Loss: 0.4352097
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.0027878284454346
Epoch: 13, Steps: 62 | Train Loss: 0.4606067 Vali Loss: 1.1585439 Test Loss: 0.4352496
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.550981283187866
Epoch: 14, Steps: 62 | Train Loss: 0.4604443 Vali Loss: 1.1580343 Test Loss: 0.4351974
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.619020462036133
Epoch: 15, Steps: 62 | Train Loss: 0.4607560 Vali Loss: 1.1616364 Test Loss: 0.4353333
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.1995012760162354
Epoch: 16, Steps: 62 | Train Loss: 0.4605455 Vali Loss: 1.1563457 Test Loss: 0.4353575
Validation loss decreased (1.157137 --> 1.156346).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.240431308746338
Epoch: 17, Steps: 62 | Train Loss: 0.4605789 Vali Loss: 1.1605617 Test Loss: 0.4353208
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.102930784225464
Epoch: 18, Steps: 62 | Train Loss: 0.4605426 Vali Loss: 1.1583662 Test Loss: 0.4353411
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.620772123336792
Epoch: 19, Steps: 62 | Train Loss: 0.4604439 Vali Loss: 1.1607504 Test Loss: 0.4353657
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9175381660461426
Epoch: 20, Steps: 62 | Train Loss: 0.4604707 Vali Loss: 1.1614397 Test Loss: 0.4352947
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4989678859710693
Epoch: 21, Steps: 62 | Train Loss: 0.4601829 Vali Loss: 1.1604300 Test Loss: 0.4353877
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5723345279693604
Epoch: 22, Steps: 62 | Train Loss: 0.4601078 Vali Loss: 1.1615992 Test Loss: 0.4353187
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9574062824249268
Epoch: 23, Steps: 62 | Train Loss: 0.4602912 Vali Loss: 1.1617447 Test Loss: 0.4352742
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.179253578186035
Epoch: 24, Steps: 62 | Train Loss: 0.4603421 Vali Loss: 1.1574372 Test Loss: 0.4352977
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.7421503067016602
Epoch: 25, Steps: 62 | Train Loss: 0.4603020 Vali Loss: 1.1597415 Test Loss: 0.4353567
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.716254472732544
Epoch: 26, Steps: 62 | Train Loss: 0.4600294 Vali Loss: 1.1652644 Test Loss: 0.4354128
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.7472457885742188
Epoch: 27, Steps: 62 | Train Loss: 0.4602779 Vali Loss: 1.1525102 Test Loss: 0.4354159
Validation loss decreased (1.156346 --> 1.152510).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.8052828311920166
Epoch: 28, Steps: 62 | Train Loss: 0.4601339 Vali Loss: 1.1626389 Test Loss: 0.4353860
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.5147156715393066
Epoch: 29, Steps: 62 | Train Loss: 0.4600082 Vali Loss: 1.1587102 Test Loss: 0.4354340
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5612668991088867
Epoch: 30, Steps: 62 | Train Loss: 0.4602721 Vali Loss: 1.1612395 Test Loss: 0.4354183
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.7571520805358887
Epoch: 31, Steps: 62 | Train Loss: 0.4600715 Vali Loss: 1.1606182 Test Loss: 0.4354608
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.1901776790618896
Epoch: 32, Steps: 62 | Train Loss: 0.4599415 Vali Loss: 1.1563965 Test Loss: 0.4354280
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.0996580123901367
Epoch: 33, Steps: 62 | Train Loss: 0.4599740 Vali Loss: 1.1600674 Test Loss: 0.4354519
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9653551578521729
Epoch: 34, Steps: 62 | Train Loss: 0.4601114 Vali Loss: 1.1549705 Test Loss: 0.4354581
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9823670387268066
Epoch: 35, Steps: 62 | Train Loss: 0.4600662 Vali Loss: 1.1575357 Test Loss: 0.4354092
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.241999387741089
Epoch: 36, Steps: 62 | Train Loss: 0.4600770 Vali Loss: 1.1610292 Test Loss: 0.4354356
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.014068603515625
Epoch: 37, Steps: 62 | Train Loss: 0.4601161 Vali Loss: 1.1582005 Test Loss: 0.4354518
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.8631517887115479
Epoch: 38, Steps: 62 | Train Loss: 0.4600438 Vali Loss: 1.1576948 Test Loss: 0.4354862
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.5437443256378174
Epoch: 39, Steps: 62 | Train Loss: 0.4600877 Vali Loss: 1.1604033 Test Loss: 0.4354626
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.8405044078826904
Epoch: 40, Steps: 62 | Train Loss: 0.4600385 Vali Loss: 1.1620245 Test Loss: 0.4354511
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.3138105869293213
Epoch: 41, Steps: 62 | Train Loss: 0.4599252 Vali Loss: 1.1619647 Test Loss: 0.4355095
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.4056379795074463
Epoch: 42, Steps: 62 | Train Loss: 0.4599657 Vali Loss: 1.1645787 Test Loss: 0.4355133
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.7733619213104248
Epoch: 43, Steps: 62 | Train Loss: 0.4600400 Vali Loss: 1.1587723 Test Loss: 0.4354748
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.378734588623047
Epoch: 44, Steps: 62 | Train Loss: 0.4598919 Vali Loss: 1.1636803 Test Loss: 0.4354727
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.189321994781494
Epoch: 45, Steps: 62 | Train Loss: 0.4598037 Vali Loss: 1.1597110 Test Loss: 0.4354726
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.383305072784424
Epoch: 46, Steps: 62 | Train Loss: 0.4600383 Vali Loss: 1.1604227 Test Loss: 0.4354826
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.9113507270812988
Epoch: 47, Steps: 62 | Train Loss: 0.4600571 Vali Loss: 1.1626328 Test Loss: 0.4354722
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43429648876190186, mae:0.4326430857181549, rse:0.6274009943008423, corr:[0.25193247 0.25877196 0.25906903 0.25583404 0.2523832  0.25042742
 0.24968976 0.24975218 0.24959448 0.24914731 0.2487135  0.24843173
 0.24851637 0.24882375 0.24903621 0.24899761 0.24872814 0.24838574
 0.24811931 0.24785277 0.24772516 0.2478484  0.24817775 0.24856165
 0.24865948 0.24841371 0.2480008  0.247545   0.24704677 0.24658245
 0.24618769 0.24589151 0.2456896  0.24551861 0.24542199 0.24542516
 0.24558109 0.24589328 0.24627905 0.24654035 0.24675088 0.24680898
 0.24677144 0.24665387 0.24656121 0.24656269 0.24677232 0.24699181
 0.24683517 0.24617948 0.2450224  0.24393308 0.24312016 0.2424295
 0.24200688 0.24182771 0.24171312 0.24156033 0.24129395 0.2411668
 0.24108942 0.24117732 0.24127077 0.24133997 0.24136144 0.24139422
 0.24151516 0.24151097 0.24157147 0.24172476 0.24181294 0.2416047
 0.24100457 0.24015944 0.23933364 0.23879331 0.23851678 0.23833936
 0.23820253 0.23799239 0.23772915 0.23740661 0.23702078 0.23674016
 0.23667285 0.23673984 0.23680288 0.23673244 0.23658434 0.23648742
 0.23632786 0.23607448 0.23582754 0.23591784 0.23642324 0.23720612
 0.23807521 0.23856094 0.23864284 0.23837642 0.23805054 0.23783429
 0.23776716 0.23784101 0.23783419 0.23765865 0.23736867 0.23709853
 0.23697628 0.23710035 0.23739965 0.23777083 0.23803203 0.23814194
 0.23807263 0.23779315 0.23746172 0.23725493 0.23722975 0.23728788
 0.23721553 0.23672378 0.23586798 0.23495705 0.23436807 0.23406717
 0.23396367 0.23392099 0.23370104 0.23330988 0.23302728 0.23291676
 0.23285547 0.23282641 0.23291835 0.23300838 0.23315322 0.23332278
 0.23342387 0.2333756  0.23324184 0.23313005 0.23308897 0.23306468
 0.23301834 0.23264098 0.23197396 0.23120469 0.23067063 0.23029378
 0.23010974 0.2301234  0.2301697  0.23023838 0.23029613 0.23039998
 0.23045939 0.23049241 0.23045895 0.23031674 0.23009004 0.23005013
 0.23009557 0.23008294 0.23001085 0.22992948 0.22992334 0.23005462
 0.23036636 0.23066618 0.2307869  0.23075707 0.23076384 0.23087934
 0.2310341  0.23112893 0.23107542 0.23100553 0.23096126 0.23095673
 0.23096025 0.23097223 0.23098588 0.23102593 0.23109324 0.2312807
 0.23147775 0.23153919 0.23153377 0.23150472 0.23139612 0.23115261
 0.23066585 0.22992274 0.22893049 0.22804335 0.22744966 0.2271211
 0.22699694 0.22709624 0.22715129 0.22705455 0.2268558  0.22675814
 0.22682269 0.22699724 0.22718097 0.2272816  0.22732817 0.22730255
 0.22721724 0.2269224  0.22656022 0.22630477 0.22618702 0.22623335
 0.22630647 0.22605604 0.22568396 0.22526672 0.22504497 0.22503991
 0.22509603 0.22514184 0.22514173 0.22500521 0.22470263 0.22447678
 0.22436847 0.2242987  0.2243452  0.22438172 0.22431523 0.22418539
 0.22409098 0.22393037 0.22380558 0.22379549 0.22397695 0.2241218
 0.22425021 0.22413939 0.22383066 0.22359082 0.22356752 0.22372545
 0.22392847 0.22410695 0.22416225 0.22393309 0.22356756 0.22329158
 0.22317806 0.22321881 0.22348505 0.22375052 0.22389987 0.22393794
 0.22393031 0.22379135 0.22375257 0.22392172 0.2241844  0.22429167
 0.22408646 0.22359292 0.22290963 0.22234502 0.22197564 0.22176878
 0.22169322 0.22160634 0.22142993 0.22116533 0.22092803 0.22083148
 0.22092487 0.22102481 0.22098759 0.2208827  0.22080219 0.22074783
 0.22079484 0.22074586 0.22075444 0.22092557 0.2213691  0.2218256
 0.22229244 0.22258073 0.22264737 0.22249593 0.22238547 0.2224468
 0.22259021 0.22267611 0.22262491 0.22254415 0.22238599 0.22221303
 0.22203568 0.22181651 0.2217537  0.22174701 0.22188266 0.22201739
 0.22213529 0.22208938 0.22207914 0.22216934 0.22245054 0.2227451
 0.22293726 0.22288448 0.22233523 0.22182372 0.2216567  0.22158234
 0.22130553 0.2207688  0.22005518 0.21967407 0.21957469 0.21957353
 0.21945027 0.21904063 0.21868442 0.21911086 0.22009611 0.22090946
 0.22073168 0.21960038 0.21892923 0.22018306 0.22252397 0.2213392 ]
