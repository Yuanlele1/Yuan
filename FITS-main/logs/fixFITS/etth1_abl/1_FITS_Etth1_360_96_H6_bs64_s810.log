Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=810, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_96_FITS_ETTh1_ftM_sl360_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8185
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=106, out_features=134, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12726784.0
params:  14338.0
Trainable parameters:  14338
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.6272587776184082
Epoch: 1, Steps: 63 | Train Loss: 0.5923715 Vali Loss: 1.0531108 Test Loss: 0.5715454
Validation loss decreased (inf --> 1.053111).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3944063186645508
Epoch: 2, Steps: 63 | Train Loss: 0.4365730 Vali Loss: 0.8759540 Test Loss: 0.4561079
Validation loss decreased (1.053111 --> 0.875954).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8354952335357666
Epoch: 3, Steps: 63 | Train Loss: 0.3867431 Vali Loss: 0.7903934 Test Loss: 0.4078924
Validation loss decreased (0.875954 --> 0.790393).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4844391345977783
Epoch: 4, Steps: 63 | Train Loss: 0.3635294 Vali Loss: 0.7559701 Test Loss: 0.3877535
Validation loss decreased (0.790393 --> 0.755970).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5277268886566162
Epoch: 5, Steps: 63 | Train Loss: 0.3529610 Vali Loss: 0.7345367 Test Loss: 0.3795228
Validation loss decreased (0.755970 --> 0.734537).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.5626039505004883
Epoch: 6, Steps: 63 | Train Loss: 0.3476701 Vali Loss: 0.7221886 Test Loss: 0.3763594
Validation loss decreased (0.734537 --> 0.722189).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5177228450775146
Epoch: 7, Steps: 63 | Train Loss: 0.3446159 Vali Loss: 0.7113692 Test Loss: 0.3749479
Validation loss decreased (0.722189 --> 0.711369).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.638671875
Epoch: 8, Steps: 63 | Train Loss: 0.3430387 Vali Loss: 0.7086469 Test Loss: 0.3746745
Validation loss decreased (0.711369 --> 0.708647).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.611466884613037
Epoch: 9, Steps: 63 | Train Loss: 0.3420354 Vali Loss: 0.7044940 Test Loss: 0.3743787
Validation loss decreased (0.708647 --> 0.704494).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.5422999858856201
Epoch: 10, Steps: 63 | Train Loss: 0.3411679 Vali Loss: 0.6967343 Test Loss: 0.3739850
Validation loss decreased (0.704494 --> 0.696734).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.6709303855895996
Epoch: 11, Steps: 63 | Train Loss: 0.3411218 Vali Loss: 0.6987786 Test Loss: 0.3739016
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4855844974517822
Epoch: 12, Steps: 63 | Train Loss: 0.3410826 Vali Loss: 0.6988192 Test Loss: 0.3737738
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6644601821899414
Epoch: 13, Steps: 63 | Train Loss: 0.3404996 Vali Loss: 0.6958587 Test Loss: 0.3736346
Validation loss decreased (0.696734 --> 0.695859).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.603933334350586
Epoch: 14, Steps: 63 | Train Loss: 0.3399346 Vali Loss: 0.6921061 Test Loss: 0.3734468
Validation loss decreased (0.695859 --> 0.692106).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.40798020362854
Epoch: 15, Steps: 63 | Train Loss: 0.3394013 Vali Loss: 0.6907452 Test Loss: 0.3735407
Validation loss decreased (0.692106 --> 0.690745).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4688777923583984
Epoch: 16, Steps: 63 | Train Loss: 0.3391415 Vali Loss: 0.6855630 Test Loss: 0.3735735
Validation loss decreased (0.690745 --> 0.685563).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4604392051696777
Epoch: 17, Steps: 63 | Train Loss: 0.3390419 Vali Loss: 0.6867321 Test Loss: 0.3732148
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.47676420211792
Epoch: 18, Steps: 63 | Train Loss: 0.3387387 Vali Loss: 0.6851236 Test Loss: 0.3733137
Validation loss decreased (0.685563 --> 0.685124).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.9267680644989014
Epoch: 19, Steps: 63 | Train Loss: 0.3385626 Vali Loss: 0.6912714 Test Loss: 0.3733572
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5328433513641357
Epoch: 20, Steps: 63 | Train Loss: 0.3388957 Vali Loss: 0.6897035 Test Loss: 0.3732556
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4879591464996338
Epoch: 21, Steps: 63 | Train Loss: 0.3378167 Vali Loss: 0.6875203 Test Loss: 0.3732705
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5105843544006348
Epoch: 22, Steps: 63 | Train Loss: 0.3383314 Vali Loss: 0.6844977 Test Loss: 0.3732055
Validation loss decreased (0.685124 --> 0.684498).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.4765987396240234
Epoch: 23, Steps: 63 | Train Loss: 0.3382340 Vali Loss: 0.6883271 Test Loss: 0.3732179
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4507014751434326
Epoch: 24, Steps: 63 | Train Loss: 0.3382053 Vali Loss: 0.6836848 Test Loss: 0.3731774
Validation loss decreased (0.684498 --> 0.683685).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.5175788402557373
Epoch: 25, Steps: 63 | Train Loss: 0.3374972 Vali Loss: 0.6830469 Test Loss: 0.3733411
Validation loss decreased (0.683685 --> 0.683047).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.0100650787353516
Epoch: 26, Steps: 63 | Train Loss: 0.3374210 Vali Loss: 0.6862098 Test Loss: 0.3732608
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5214042663574219
Epoch: 27, Steps: 63 | Train Loss: 0.3381358 Vali Loss: 0.6792276 Test Loss: 0.3732275
Validation loss decreased (0.683047 --> 0.679228).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.84190034866333
Epoch: 28, Steps: 63 | Train Loss: 0.3372516 Vali Loss: 0.6805658 Test Loss: 0.3732165
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.5600824356079102
Epoch: 29, Steps: 63 | Train Loss: 0.3376095 Vali Loss: 0.6818968 Test Loss: 0.3731266
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5470843315124512
Epoch: 30, Steps: 63 | Train Loss: 0.3370519 Vali Loss: 0.6814288 Test Loss: 0.3731760
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6905763149261475
Epoch: 31, Steps: 63 | Train Loss: 0.3375754 Vali Loss: 0.6789762 Test Loss: 0.3730863
Validation loss decreased (0.679228 --> 0.678976).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.6231048107147217
Epoch: 32, Steps: 63 | Train Loss: 0.3371617 Vali Loss: 0.6782644 Test Loss: 0.3731669
Validation loss decreased (0.678976 --> 0.678264).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5717370510101318
Epoch: 33, Steps: 63 | Train Loss: 0.3376265 Vali Loss: 0.6746458 Test Loss: 0.3730489
Validation loss decreased (0.678264 --> 0.674646).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.1025218963623047
Epoch: 34, Steps: 63 | Train Loss: 0.3374150 Vali Loss: 0.6828000 Test Loss: 0.3731074
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.1393654346466064
Epoch: 35, Steps: 63 | Train Loss: 0.3370968 Vali Loss: 0.6821996 Test Loss: 0.3730772
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.332566261291504
Epoch: 36, Steps: 63 | Train Loss: 0.3368179 Vali Loss: 0.6780684 Test Loss: 0.3730251
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.4376256465911865
Epoch: 37, Steps: 63 | Train Loss: 0.3365509 Vali Loss: 0.6822755 Test Loss: 0.3731250
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.5262494087219238
Epoch: 38, Steps: 63 | Train Loss: 0.3366326 Vali Loss: 0.6800852 Test Loss: 0.3731399
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.5626838207244873
Epoch: 39, Steps: 63 | Train Loss: 0.3372258 Vali Loss: 0.6764220 Test Loss: 0.3731454
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.5849316120147705
Epoch: 40, Steps: 63 | Train Loss: 0.3377506 Vali Loss: 0.6788776 Test Loss: 0.3730720
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.4822556972503662
Epoch: 41, Steps: 63 | Train Loss: 0.3366767 Vali Loss: 0.6781201 Test Loss: 0.3731268
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.5090587139129639
Epoch: 42, Steps: 63 | Train Loss: 0.3369399 Vali Loss: 0.6805100 Test Loss: 0.3731304
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.053279399871826
Epoch: 43, Steps: 63 | Train Loss: 0.3367334 Vali Loss: 0.6760436 Test Loss: 0.3731068
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.707796335220337
Epoch: 44, Steps: 63 | Train Loss: 0.3370216 Vali Loss: 0.6754309 Test Loss: 0.3731369
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9820334911346436
Epoch: 45, Steps: 63 | Train Loss: 0.3369465 Vali Loss: 0.6814077 Test Loss: 0.3730757
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.538259744644165
Epoch: 46, Steps: 63 | Train Loss: 0.3363523 Vali Loss: 0.6832315 Test Loss: 0.3730839
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.6175079345703125
Epoch: 47, Steps: 63 | Train Loss: 0.3372627 Vali Loss: 0.6801366 Test Loss: 0.3730946
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9660770893096924
Epoch: 48, Steps: 63 | Train Loss: 0.3366193 Vali Loss: 0.6786823 Test Loss: 0.3731274
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.7777819633483887
Epoch: 49, Steps: 63 | Train Loss: 0.3369592 Vali Loss: 0.6807266 Test Loss: 0.3731429
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6248764991760254
Epoch: 50, Steps: 63 | Train Loss: 0.3369662 Vali Loss: 0.6811824 Test Loss: 0.3731588
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.5382816791534424
Epoch: 51, Steps: 63 | Train Loss: 0.3372994 Vali Loss: 0.6804063 Test Loss: 0.3730821
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.8485455513000488
Epoch: 52, Steps: 63 | Train Loss: 0.3367486 Vali Loss: 0.6764880 Test Loss: 0.3731476
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.247753858566284
Epoch: 53, Steps: 63 | Train Loss: 0.3369106 Vali Loss: 0.6772304 Test Loss: 0.3731222
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_96_FITS_ETTh1_ftM_sl360_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.37209591269493103, mae:0.3945537507534027, rse:0.5794090628623962, corr:[0.26941714 0.2773364  0.27792925 0.2798012  0.2769374  0.2747294
 0.27467555 0.2741614  0.27312317 0.2732731  0.2734637  0.27272716
 0.2724306  0.27263033 0.2724554  0.27232113 0.2724817  0.27229878
 0.27197987 0.27186263 0.27172512 0.2714274  0.2715755  0.2719342
 0.27155107 0.2710911  0.270974   0.2707314  0.27027068 0.27008727
 0.26987582 0.2691198  0.26853818 0.26854846 0.2685253  0.26820084
 0.268293   0.26856372 0.2684688  0.2683937  0.26884648 0.26913697
 0.2692494  0.26925257 0.26889104 0.26832095 0.2684412  0.26891682
 0.26849833 0.26734224 0.2664529  0.26606047 0.2653903  0.2641735
 0.26342288 0.2630829  0.26278952 0.2626291  0.2623453  0.26236793
 0.26255983 0.2627107  0.262569   0.2624706  0.26274088 0.26323897
 0.26355398 0.26331738 0.26303935 0.26318434 0.26345757 0.2633729
 0.26263815 0.2615436  0.26069638 0.2603909  0.2601738  0.25960085
 0.25903305 0.25859258 0.25822037 0.25775754 0.25738382 0.2572674
 0.2572993  0.25742862 0.2575351  0.25738785 0.25750178 0.25757557
 0.25667256 0.25588742 0.25589904 0.2547514  0.25468183 0.25778607]
