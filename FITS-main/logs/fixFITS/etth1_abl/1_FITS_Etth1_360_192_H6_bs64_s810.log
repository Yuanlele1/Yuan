Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=810, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=106, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  15386112.0
params:  17334.0
Trainable parameters:  17334
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7115070819854736
Epoch: 1, Steps: 63 | Train Loss: 0.6923182 Vali Loss: 1.3564126 Test Loss: 0.6630408
Validation loss decreased (inf --> 1.356413).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6294422149658203
Epoch: 2, Steps: 63 | Train Loss: 0.5303622 Vali Loss: 1.1814196 Test Loss: 0.5463206
Validation loss decreased (1.356413 --> 1.181420).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.5507607460021973
Epoch: 3, Steps: 63 | Train Loss: 0.4761062 Vali Loss: 1.0940603 Test Loss: 0.4899361
Validation loss decreased (1.181420 --> 1.094060).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.7039170265197754
Epoch: 4, Steps: 63 | Train Loss: 0.4468623 Vali Loss: 1.0399064 Test Loss: 0.4570141
Validation loss decreased (1.094060 --> 1.039906).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5559442043304443
Epoch: 5, Steps: 63 | Train Loss: 0.4283683 Vali Loss: 1.0048658 Test Loss: 0.4370326
Validation loss decreased (1.039906 --> 1.004866).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.567403793334961
Epoch: 6, Steps: 63 | Train Loss: 0.4176009 Vali Loss: 0.9821848 Test Loss: 0.4249522
Validation loss decreased (1.004866 --> 0.982185).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.1589317321777344
Epoch: 7, Steps: 63 | Train Loss: 0.4099127 Vali Loss: 0.9675363 Test Loss: 0.4180357
Validation loss decreased (0.982185 --> 0.967536).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.619105577468872
Epoch: 8, Steps: 63 | Train Loss: 0.4053356 Vali Loss: 0.9573267 Test Loss: 0.4141463
Validation loss decreased (0.967536 --> 0.957327).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.5504119396209717
Epoch: 9, Steps: 63 | Train Loss: 0.4024782 Vali Loss: 0.9500421 Test Loss: 0.4117284
Validation loss decreased (0.957327 --> 0.950042).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.8619627952575684
Epoch: 10, Steps: 63 | Train Loss: 0.4005631 Vali Loss: 0.9449074 Test Loss: 0.4105995
Validation loss decreased (0.950042 --> 0.944907).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.039562940597534
Epoch: 11, Steps: 63 | Train Loss: 0.3992392 Vali Loss: 0.9417899 Test Loss: 0.4099927
Validation loss decreased (0.944907 --> 0.941790).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.703096628189087
Epoch: 12, Steps: 63 | Train Loss: 0.3984541 Vali Loss: 0.9394923 Test Loss: 0.4096423
Validation loss decreased (0.941790 --> 0.939492).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5777735710144043
Epoch: 13, Steps: 63 | Train Loss: 0.3976222 Vali Loss: 0.9372937 Test Loss: 0.4093115
Validation loss decreased (0.939492 --> 0.937294).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.593172550201416
Epoch: 14, Steps: 63 | Train Loss: 0.3970825 Vali Loss: 0.9353908 Test Loss: 0.4092302
Validation loss decreased (0.937294 --> 0.935391).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4126734733581543
Epoch: 15, Steps: 63 | Train Loss: 0.3963380 Vali Loss: 0.9339270 Test Loss: 0.4092795
Validation loss decreased (0.935391 --> 0.933927).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.6303637027740479
Epoch: 16, Steps: 63 | Train Loss: 0.3961912 Vali Loss: 0.9328433 Test Loss: 0.4091597
Validation loss decreased (0.933927 --> 0.932843).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.1646645069122314
Epoch: 17, Steps: 63 | Train Loss: 0.3959601 Vali Loss: 0.9314645 Test Loss: 0.4090795
Validation loss decreased (0.932843 --> 0.931464).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.5226705074310303
Epoch: 18, Steps: 63 | Train Loss: 0.3957469 Vali Loss: 0.9303809 Test Loss: 0.4089847
Validation loss decreased (0.931464 --> 0.930381).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6591854095458984
Epoch: 19, Steps: 63 | Train Loss: 0.3956420 Vali Loss: 0.9296312 Test Loss: 0.4089851
Validation loss decreased (0.930381 --> 0.929631).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.6908900737762451
Epoch: 20, Steps: 63 | Train Loss: 0.3952978 Vali Loss: 0.9289960 Test Loss: 0.4088378
Validation loss decreased (0.929631 --> 0.928996).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4816370010375977
Epoch: 21, Steps: 63 | Train Loss: 0.3951764 Vali Loss: 0.9282107 Test Loss: 0.4089483
Validation loss decreased (0.928996 --> 0.928211).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5709142684936523
Epoch: 22, Steps: 63 | Train Loss: 0.3950006 Vali Loss: 0.9278481 Test Loss: 0.4088748
Validation loss decreased (0.928211 --> 0.927848).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5818722248077393
Epoch: 23, Steps: 63 | Train Loss: 0.3947812 Vali Loss: 0.9266837 Test Loss: 0.4087927
Validation loss decreased (0.927848 --> 0.926684).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.502140760421753
Epoch: 24, Steps: 63 | Train Loss: 0.3946445 Vali Loss: 0.9263605 Test Loss: 0.4087690
Validation loss decreased (0.926684 --> 0.926361).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.4437062740325928
Epoch: 25, Steps: 63 | Train Loss: 0.3947129 Vali Loss: 0.9263810 Test Loss: 0.4087779
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5668830871582031
Epoch: 26, Steps: 63 | Train Loss: 0.3944551 Vali Loss: 0.9262447 Test Loss: 0.4087957
Validation loss decreased (0.926361 --> 0.926245).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4885094165802002
Epoch: 27, Steps: 63 | Train Loss: 0.3944362 Vali Loss: 0.9253988 Test Loss: 0.4088087
Validation loss decreased (0.926245 --> 0.925399).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5047409534454346
Epoch: 28, Steps: 63 | Train Loss: 0.3944280 Vali Loss: 0.9254338 Test Loss: 0.4087309
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.0167484283447266
Epoch: 29, Steps: 63 | Train Loss: 0.3938310 Vali Loss: 0.9248711 Test Loss: 0.4086592
Validation loss decreased (0.925399 --> 0.924871).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.7152624130249023
Epoch: 30, Steps: 63 | Train Loss: 0.3937196 Vali Loss: 0.9244220 Test Loss: 0.4087396
Validation loss decreased (0.924871 --> 0.924422).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6528031826019287
Epoch: 31, Steps: 63 | Train Loss: 0.3942256 Vali Loss: 0.9242557 Test Loss: 0.4087208
Validation loss decreased (0.924422 --> 0.924256).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.133784294128418
Epoch: 32, Steps: 63 | Train Loss: 0.3941823 Vali Loss: 0.9240263 Test Loss: 0.4086758
Validation loss decreased (0.924256 --> 0.924026).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4947400093078613
Epoch: 33, Steps: 63 | Train Loss: 0.3938520 Vali Loss: 0.9236743 Test Loss: 0.4086628
Validation loss decreased (0.924026 --> 0.923674).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.480588436126709
Epoch: 34, Steps: 63 | Train Loss: 0.3938171 Vali Loss: 0.9236145 Test Loss: 0.4086699
Validation loss decreased (0.923674 --> 0.923615).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.637366771697998
Epoch: 35, Steps: 63 | Train Loss: 0.3938499 Vali Loss: 0.9236682 Test Loss: 0.4086681
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.4110820293426514
Epoch: 36, Steps: 63 | Train Loss: 0.3938968 Vali Loss: 0.9230386 Test Loss: 0.4086799
Validation loss decreased (0.923615 --> 0.923039).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.538095235824585
Epoch: 37, Steps: 63 | Train Loss: 0.3939020 Vali Loss: 0.9231147 Test Loss: 0.4086388
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.572509527206421
Epoch: 38, Steps: 63 | Train Loss: 0.3938873 Vali Loss: 0.9228009 Test Loss: 0.4086189
Validation loss decreased (0.923039 --> 0.922801).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.5844690799713135
Epoch: 39, Steps: 63 | Train Loss: 0.3934940 Vali Loss: 0.9231119 Test Loss: 0.4085931
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.6204564571380615
Epoch: 40, Steps: 63 | Train Loss: 0.3935440 Vali Loss: 0.9227676 Test Loss: 0.4086691
Validation loss decreased (0.922801 --> 0.922768).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6269612312316895
Epoch: 41, Steps: 63 | Train Loss: 0.3935828 Vali Loss: 0.9225602 Test Loss: 0.4085661
Validation loss decreased (0.922768 --> 0.922560).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.034343719482422
Epoch: 42, Steps: 63 | Train Loss: 0.3936438 Vali Loss: 0.9223319 Test Loss: 0.4085634
Validation loss decreased (0.922560 --> 0.922332).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.1582190990448
Epoch: 43, Steps: 63 | Train Loss: 0.3934552 Vali Loss: 0.9224696 Test Loss: 0.4085868
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.6481850147247314
Epoch: 44, Steps: 63 | Train Loss: 0.3935285 Vali Loss: 0.9222335 Test Loss: 0.4085574
Validation loss decreased (0.922332 --> 0.922233).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5651044845581055
Epoch: 45, Steps: 63 | Train Loss: 0.3933923 Vali Loss: 0.9223745 Test Loss: 0.4085803
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.571709156036377
Epoch: 46, Steps: 63 | Train Loss: 0.3936445 Vali Loss: 0.9219121 Test Loss: 0.4085960
Validation loss decreased (0.922233 --> 0.921912).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.3172903060913086
Epoch: 47, Steps: 63 | Train Loss: 0.3931682 Vali Loss: 0.9216449 Test Loss: 0.4085741
Validation loss decreased (0.921912 --> 0.921645).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.5579488277435303
Epoch: 48, Steps: 63 | Train Loss: 0.3931658 Vali Loss: 0.9217070 Test Loss: 0.4085395
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.5126030445098877
Epoch: 49, Steps: 63 | Train Loss: 0.3934115 Vali Loss: 0.9220519 Test Loss: 0.4085565
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6918742656707764
Epoch: 50, Steps: 63 | Train Loss: 0.3932323 Vali Loss: 0.9216165 Test Loss: 0.4085317
Validation loss decreased (0.921645 --> 0.921616).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6517555713653564
Epoch: 51, Steps: 63 | Train Loss: 0.3934207 Vali Loss: 0.9217883 Test Loss: 0.4085543
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.5358178615570068
Epoch: 52, Steps: 63 | Train Loss: 0.3934437 Vali Loss: 0.9214990 Test Loss: 0.4085570
Validation loss decreased (0.921616 --> 0.921499).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.533679723739624
Epoch: 53, Steps: 63 | Train Loss: 0.3932185 Vali Loss: 0.9212841 Test Loss: 0.4085583
Validation loss decreased (0.921499 --> 0.921284).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.4729108810424805
Epoch: 54, Steps: 63 | Train Loss: 0.3932546 Vali Loss: 0.9213012 Test Loss: 0.4085819
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.6079678535461426
Epoch: 55, Steps: 63 | Train Loss: 0.3930147 Vali Loss: 0.9216157 Test Loss: 0.4085683
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.5748839378356934
Epoch: 56, Steps: 63 | Train Loss: 0.3932983 Vali Loss: 0.9212326 Test Loss: 0.4085292
Validation loss decreased (0.921284 --> 0.921233).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.59726619720459
Epoch: 57, Steps: 63 | Train Loss: 0.3932105 Vali Loss: 0.9216319 Test Loss: 0.4085487
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.6165351867675781
Epoch: 58, Steps: 63 | Train Loss: 0.3933982 Vali Loss: 0.9215542 Test Loss: 0.4085420
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.7277815341949463
Epoch: 59, Steps: 63 | Train Loss: 0.3930998 Vali Loss: 0.9214619 Test Loss: 0.4085575
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.5620949268341064
Epoch: 60, Steps: 63 | Train Loss: 0.3929867 Vali Loss: 0.9211709 Test Loss: 0.4085456
Validation loss decreased (0.921233 --> 0.921171).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6942532062530518
Epoch: 61, Steps: 63 | Train Loss: 0.3929135 Vali Loss: 0.9213428 Test Loss: 0.4085474
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6072962284088135
Epoch: 62, Steps: 63 | Train Loss: 0.3931681 Vali Loss: 0.9211889 Test Loss: 0.4085437
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.5855066776275635
Epoch: 63, Steps: 63 | Train Loss: 0.3932355 Vali Loss: 0.9209387 Test Loss: 0.4085471
Validation loss decreased (0.921171 --> 0.920939).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.569932222366333
Epoch: 64, Steps: 63 | Train Loss: 0.3931766 Vali Loss: 0.9210958 Test Loss: 0.4085594
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.6655852794647217
Epoch: 65, Steps: 63 | Train Loss: 0.3931706 Vali Loss: 0.9209788 Test Loss: 0.4085276
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.0198349952697754
Epoch: 66, Steps: 63 | Train Loss: 0.3930712 Vali Loss: 0.9212274 Test Loss: 0.4085679
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.6239826679229736
Epoch: 67, Steps: 63 | Train Loss: 0.3931900 Vali Loss: 0.9210004 Test Loss: 0.4085550
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.5632236003875732
Epoch: 68, Steps: 63 | Train Loss: 0.3930987 Vali Loss: 0.9208272 Test Loss: 0.4085332
Validation loss decreased (0.920939 --> 0.920827).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.7255277633666992
Epoch: 69, Steps: 63 | Train Loss: 0.3930328 Vali Loss: 0.9210790 Test Loss: 0.4085486
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5465044975280762
Epoch: 70, Steps: 63 | Train Loss: 0.3930856 Vali Loss: 0.9212310 Test Loss: 0.4085433
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.6786551475524902
Epoch: 71, Steps: 63 | Train Loss: 0.3928591 Vali Loss: 0.9210553 Test Loss: 0.4085607
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.5856146812438965
Epoch: 72, Steps: 63 | Train Loss: 0.3931671 Vali Loss: 0.9207472 Test Loss: 0.4085541
Validation loss decreased (0.920827 --> 0.920747).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.4907495975494385
Epoch: 73, Steps: 63 | Train Loss: 0.3929918 Vali Loss: 0.9210200 Test Loss: 0.4085470
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.397886037826538
Epoch: 74, Steps: 63 | Train Loss: 0.3928496 Vali Loss: 0.9211271 Test Loss: 0.4085455
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.5087177753448486
Epoch: 75, Steps: 63 | Train Loss: 0.3930772 Vali Loss: 0.9207779 Test Loss: 0.4085531
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.5559918880462646
Epoch: 76, Steps: 63 | Train Loss: 0.3928623 Vali Loss: 0.9210302 Test Loss: 0.4085445
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.841639518737793
Epoch: 77, Steps: 63 | Train Loss: 0.3931399 Vali Loss: 0.9209523 Test Loss: 0.4085402
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.7948763370513916
Epoch: 78, Steps: 63 | Train Loss: 0.3930822 Vali Loss: 0.9206731 Test Loss: 0.4085585
Validation loss decreased (0.920747 --> 0.920673).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.5261375904083252
Epoch: 79, Steps: 63 | Train Loss: 0.3930485 Vali Loss: 0.9204776 Test Loss: 0.4085450
Validation loss decreased (0.920673 --> 0.920478).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.5212526321411133
Epoch: 80, Steps: 63 | Train Loss: 0.3930263 Vali Loss: 0.9207969 Test Loss: 0.4085489
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.6370065212249756
Epoch: 81, Steps: 63 | Train Loss: 0.3929336 Vali Loss: 0.9209782 Test Loss: 0.4085490
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.6857900619506836
Epoch: 82, Steps: 63 | Train Loss: 0.3930731 Vali Loss: 0.9204967 Test Loss: 0.4085528
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.6108613014221191
Epoch: 83, Steps: 63 | Train Loss: 0.3929345 Vali Loss: 0.9209784 Test Loss: 0.4085440
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.6093459129333496
Epoch: 84, Steps: 63 | Train Loss: 0.3929022 Vali Loss: 0.9204490 Test Loss: 0.4085431
Validation loss decreased (0.920478 --> 0.920449).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.6175434589385986
Epoch: 85, Steps: 63 | Train Loss: 0.3931386 Vali Loss: 0.9206366 Test Loss: 0.4085522
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.191566228866577
Epoch: 86, Steps: 63 | Train Loss: 0.3931890 Vali Loss: 0.9207423 Test Loss: 0.4085520
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.5847768783569336
Epoch: 87, Steps: 63 | Train Loss: 0.3931761 Vali Loss: 0.9208596 Test Loss: 0.4085457
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.611074447631836
Epoch: 88, Steps: 63 | Train Loss: 0.3932537 Vali Loss: 0.9205726 Test Loss: 0.4085460
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.5252878665924072
Epoch: 89, Steps: 63 | Train Loss: 0.3929174 Vali Loss: 0.9206337 Test Loss: 0.4085470
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.711540937423706
Epoch: 90, Steps: 63 | Train Loss: 0.3930242 Vali Loss: 0.9203467 Test Loss: 0.4085478
Validation loss decreased (0.920449 --> 0.920347).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.5228772163391113
Epoch: 91, Steps: 63 | Train Loss: 0.3929014 Vali Loss: 0.9204982 Test Loss: 0.4085467
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.679018497467041
Epoch: 92, Steps: 63 | Train Loss: 0.3931279 Vali Loss: 0.9208767 Test Loss: 0.4085447
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.6431922912597656
Epoch: 93, Steps: 63 | Train Loss: 0.3929814 Vali Loss: 0.9203998 Test Loss: 0.4085423
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.5869498252868652
Epoch: 94, Steps: 63 | Train Loss: 0.3930729 Vali Loss: 0.9203649 Test Loss: 0.4085469
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.7056732177734375
Epoch: 95, Steps: 63 | Train Loss: 0.3929851 Vali Loss: 0.9207956 Test Loss: 0.4085450
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.26401948928833
Epoch: 96, Steps: 63 | Train Loss: 0.3930983 Vali Loss: 0.9208322 Test Loss: 0.4085457
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.4974949359893799
Epoch: 97, Steps: 63 | Train Loss: 0.3930202 Vali Loss: 0.9204461 Test Loss: 0.4085455
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.5705139636993408
Epoch: 98, Steps: 63 | Train Loss: 0.3929874 Vali Loss: 0.9207343 Test Loss: 0.4085437
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.5287747383117676
Epoch: 99, Steps: 63 | Train Loss: 0.3929946 Vali Loss: 0.9204908 Test Loss: 0.4085436
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.3273825645446777
Epoch: 100, Steps: 63 | Train Loss: 0.3928472 Vali Loss: 0.9208670 Test Loss: 0.4085404
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4044424891471863, mae:0.41312098503112793, rse:0.6039287447929382, corr:[0.260611   0.26902825 0.2692395  0.27131933 0.26864    0.2662332
 0.26608717 0.26569977 0.26475316 0.26495913 0.26520035 0.26457855
 0.264284   0.2642461  0.26389164 0.26384038 0.2639132  0.26342076
 0.2630454  0.26316088 0.2631141  0.26279247 0.26311558 0.26360667
 0.26313016 0.26271763 0.26275754 0.26244748 0.2619028  0.26181823
 0.2617029  0.26104763 0.26061037 0.26076853 0.26078993 0.26054406
 0.2606685  0.26095885 0.26092696 0.2609697  0.26129147 0.26130572
 0.26127636 0.26144582 0.26132497 0.26083478 0.26078793 0.261059
 0.26067212 0.25965747 0.25877848 0.25830463 0.25771248 0.25670734
 0.25603744 0.25567257 0.25537828 0.25523075 0.25500467 0.25488272
 0.25482595 0.25483045 0.2547186  0.2545828  0.2546194  0.25491038
 0.25528383 0.2552911  0.25515717 0.25518653 0.25522605 0.25503802
 0.25448546 0.25365308 0.2529502  0.25268102 0.25261763 0.25235474
 0.25193146 0.25147554 0.25115034 0.250853   0.2505583  0.25032356
 0.25018644 0.25020313 0.25034595 0.2503429  0.25018388 0.25020343
 0.25026578 0.25002524 0.24962938 0.24945268 0.24955718 0.25011662
 0.2507862  0.25085983 0.25093725 0.25107068 0.25103807 0.25080797
 0.25062573 0.25065067 0.25051966 0.25026834 0.25007567 0.24983504
 0.24952509 0.24946645 0.24970333 0.2499922  0.2501851  0.2504407
 0.25056925 0.25037906 0.2500815  0.24983825 0.24955267 0.2495194
 0.24960814 0.24908762 0.24833374 0.24786012 0.24741481 0.24666707
 0.24636438 0.2465067  0.2462645  0.24571286 0.24550946 0.24534297
 0.2449709  0.24502213 0.24526219 0.2449879  0.24509004 0.24565472
 0.24599063 0.24584095 0.24575317 0.24579339 0.24549662 0.24531353
 0.24539153 0.244871   0.24429594 0.243902   0.243266   0.24234404
 0.2421384  0.24232988 0.24210137 0.24214254 0.24241103 0.24226409
 0.24186581 0.24214247 0.24234812 0.24190426 0.24190448 0.24251306
 0.24252824 0.24207875 0.24218114 0.24217573 0.24161296 0.24160653
 0.24206014 0.24188085 0.24195586 0.24256839 0.24234384 0.24150278
 0.241495   0.24180311 0.24154326 0.24171369 0.24211282 0.24192806
 0.24196455 0.24238321 0.24209523 0.24178173 0.24261636 0.24275333
 0.2416395  0.24217089 0.24242342 0.23961361 0.2409358  0.23774423]
