Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=90, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21772800.0
params:  24570.0
Trainable parameters:  24570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.634345293045044
Epoch: 1, Steps: 59 | Train Loss: 0.9375412 Vali Loss: 2.2225223 Test Loss: 0.9631925
Validation loss decreased (inf --> 2.222522).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.0406556129455566
Epoch: 2, Steps: 59 | Train Loss: 0.7287736 Vali Loss: 1.9621520 Test Loss: 0.7962629
Validation loss decreased (2.222522 --> 1.962152).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.9375731945037842
Epoch: 3, Steps: 59 | Train Loss: 0.6300961 Vali Loss: 1.8501126 Test Loss: 0.7204807
Validation loss decreased (1.962152 --> 1.850113).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.3918120861053467
Epoch: 4, Steps: 59 | Train Loss: 0.5810085 Vali Loss: 1.7920947 Test Loss: 0.6813145
Validation loss decreased (1.850113 --> 1.792095).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.1075336933135986
Epoch: 5, Steps: 59 | Train Loss: 0.5533207 Vali Loss: 1.7579261 Test Loss: 0.6581481
Validation loss decreased (1.792095 --> 1.757926).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.5378613471984863
Epoch: 6, Steps: 59 | Train Loss: 0.5355098 Vali Loss: 1.7422783 Test Loss: 0.6420520
Validation loss decreased (1.757926 --> 1.742278).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9028165340423584
Epoch: 7, Steps: 59 | Train Loss: 0.5221468 Vali Loss: 1.7149408 Test Loss: 0.6282716
Validation loss decreased (1.742278 --> 1.714941).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.1800923347473145
Epoch: 8, Steps: 59 | Train Loss: 0.5117559 Vali Loss: 1.7019298 Test Loss: 0.6166785
Validation loss decreased (1.714941 --> 1.701930).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.914691209793091
Epoch: 9, Steps: 59 | Train Loss: 0.5030973 Vali Loss: 1.6880107 Test Loss: 0.6064637
Validation loss decreased (1.701930 --> 1.688011).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.7133498191833496
Epoch: 10, Steps: 59 | Train Loss: 0.4954869 Vali Loss: 1.6783564 Test Loss: 0.5969913
Validation loss decreased (1.688011 --> 1.678356).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.098576784133911
Epoch: 11, Steps: 59 | Train Loss: 0.4890060 Vali Loss: 1.6687469 Test Loss: 0.5885278
Validation loss decreased (1.678356 --> 1.668747).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.067835807800293
Epoch: 12, Steps: 59 | Train Loss: 0.4833004 Vali Loss: 1.6526066 Test Loss: 0.5808591
Validation loss decreased (1.668747 --> 1.652607).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9552974700927734
Epoch: 13, Steps: 59 | Train Loss: 0.4781776 Vali Loss: 1.6468015 Test Loss: 0.5738053
Validation loss decreased (1.652607 --> 1.646801).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.6903092861175537
Epoch: 14, Steps: 59 | Train Loss: 0.4737331 Vali Loss: 1.6342441 Test Loss: 0.5672126
Validation loss decreased (1.646801 --> 1.634244).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.0540027618408203
Epoch: 15, Steps: 59 | Train Loss: 0.4696829 Vali Loss: 1.6315727 Test Loss: 0.5610280
Validation loss decreased (1.634244 --> 1.631573).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.0183897018432617
Epoch: 16, Steps: 59 | Train Loss: 0.4658561 Vali Loss: 1.6232851 Test Loss: 0.5554209
Validation loss decreased (1.631573 --> 1.623285).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.110846757888794
Epoch: 17, Steps: 59 | Train Loss: 0.4624635 Vali Loss: 1.6127923 Test Loss: 0.5503192
Validation loss decreased (1.623285 --> 1.612792).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.272822380065918
Epoch: 18, Steps: 59 | Train Loss: 0.4594466 Vali Loss: 1.6087875 Test Loss: 0.5457025
Validation loss decreased (1.612792 --> 1.608788).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.165818452835083
Epoch: 19, Steps: 59 | Train Loss: 0.4566768 Vali Loss: 1.6017776 Test Loss: 0.5415642
Validation loss decreased (1.608788 --> 1.601778).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.099174976348877
Epoch: 20, Steps: 59 | Train Loss: 0.4540421 Vali Loss: 1.5999730 Test Loss: 0.5374380
Validation loss decreased (1.601778 --> 1.599973).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.38382887840271
Epoch: 21, Steps: 59 | Train Loss: 0.4516739 Vali Loss: 1.5914569 Test Loss: 0.5336559
Validation loss decreased (1.599973 --> 1.591457).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.527338743209839
Epoch: 22, Steps: 59 | Train Loss: 0.4494896 Vali Loss: 1.5885763 Test Loss: 0.5301079
Validation loss decreased (1.591457 --> 1.588576).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.3780665397644043
Epoch: 23, Steps: 59 | Train Loss: 0.4474631 Vali Loss: 1.5801756 Test Loss: 0.5267938
Validation loss decreased (1.588576 --> 1.580176).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.519503593444824
Epoch: 24, Steps: 59 | Train Loss: 0.4457213 Vali Loss: 1.5795658 Test Loss: 0.5237946
Validation loss decreased (1.580176 --> 1.579566).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.451849937438965
Epoch: 25, Steps: 59 | Train Loss: 0.4439230 Vali Loss: 1.5717022 Test Loss: 0.5209939
Validation loss decreased (1.579566 --> 1.571702).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.4643051624298096
Epoch: 26, Steps: 59 | Train Loss: 0.4423238 Vali Loss: 1.5713433 Test Loss: 0.5183396
Validation loss decreased (1.571702 --> 1.571343).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.785968780517578
Epoch: 27, Steps: 59 | Train Loss: 0.4409375 Vali Loss: 1.5703909 Test Loss: 0.5160291
Validation loss decreased (1.571343 --> 1.570391).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.4390158653259277
Epoch: 28, Steps: 59 | Train Loss: 0.4395864 Vali Loss: 1.5668467 Test Loss: 0.5135621
Validation loss decreased (1.570391 --> 1.566847).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.9362525939941406
Epoch: 29, Steps: 59 | Train Loss: 0.4382743 Vali Loss: 1.5631089 Test Loss: 0.5114350
Validation loss decreased (1.566847 --> 1.563109).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.4698376655578613
Epoch: 30, Steps: 59 | Train Loss: 0.4370552 Vali Loss: 1.5525817 Test Loss: 0.5094062
Validation loss decreased (1.563109 --> 1.552582).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.2938358783721924
Epoch: 31, Steps: 59 | Train Loss: 0.4359887 Vali Loss: 1.5566933 Test Loss: 0.5076061
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.944896697998047
Epoch: 32, Steps: 59 | Train Loss: 0.4348163 Vali Loss: 1.5484345 Test Loss: 0.5057430
Validation loss decreased (1.552582 --> 1.548434).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.5776257514953613
Epoch: 33, Steps: 59 | Train Loss: 0.4338945 Vali Loss: 1.5568678 Test Loss: 0.5040250
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.3693811893463135
Epoch: 34, Steps: 59 | Train Loss: 0.4329115 Vali Loss: 1.5552654 Test Loss: 0.5024770
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.1344988346099854
Epoch: 35, Steps: 59 | Train Loss: 0.4320501 Vali Loss: 1.5509224 Test Loss: 0.5009975
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.292353630065918
Epoch: 36, Steps: 59 | Train Loss: 0.4312133 Vali Loss: 1.5435475 Test Loss: 0.4995426
Validation loss decreased (1.548434 --> 1.543548).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.443223714828491
Epoch: 37, Steps: 59 | Train Loss: 0.4305370 Vali Loss: 1.5489748 Test Loss: 0.4982480
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9427897930145264
Epoch: 38, Steps: 59 | Train Loss: 0.4296823 Vali Loss: 1.5447868 Test Loss: 0.4970117
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.4468228816986084
Epoch: 39, Steps: 59 | Train Loss: 0.4290669 Vali Loss: 1.5386941 Test Loss: 0.4957896
Validation loss decreased (1.543548 --> 1.538694).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.5551624298095703
Epoch: 40, Steps: 59 | Train Loss: 0.4283509 Vali Loss: 1.5445486 Test Loss: 0.4946754
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.2363736629486084
Epoch: 41, Steps: 59 | Train Loss: 0.4277757 Vali Loss: 1.5395265 Test Loss: 0.4936347
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.3755152225494385
Epoch: 42, Steps: 59 | Train Loss: 0.4271630 Vali Loss: 1.5378335 Test Loss: 0.4926192
Validation loss decreased (1.538694 --> 1.537833).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0936012268066406
Epoch: 43, Steps: 59 | Train Loss: 0.4267984 Vali Loss: 1.5356288 Test Loss: 0.4917533
Validation loss decreased (1.537833 --> 1.535629).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.8606083393096924
Epoch: 44, Steps: 59 | Train Loss: 0.4261962 Vali Loss: 1.5348520 Test Loss: 0.4907913
Validation loss decreased (1.535629 --> 1.534852).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.1596062183380127
Epoch: 45, Steps: 59 | Train Loss: 0.4255985 Vali Loss: 1.5323826 Test Loss: 0.4900167
Validation loss decreased (1.534852 --> 1.532383).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.137343406677246
Epoch: 46, Steps: 59 | Train Loss: 0.4251901 Vali Loss: 1.5300505 Test Loss: 0.4892232
Validation loss decreased (1.532383 --> 1.530051).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.8199193477630615
Epoch: 47, Steps: 59 | Train Loss: 0.4248304 Vali Loss: 1.5290492 Test Loss: 0.4884731
Validation loss decreased (1.530051 --> 1.529049).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.10497784614563
Epoch: 48, Steps: 59 | Train Loss: 0.4242901 Vali Loss: 1.5316478 Test Loss: 0.4877549
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.747889280319214
Epoch: 49, Steps: 59 | Train Loss: 0.4239869 Vali Loss: 1.5265226 Test Loss: 0.4870743
Validation loss decreased (1.529049 --> 1.526523).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.399585485458374
Epoch: 50, Steps: 59 | Train Loss: 0.4236276 Vali Loss: 1.5223212 Test Loss: 0.4864939
Validation loss decreased (1.526523 --> 1.522321).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.9648900032043457
Epoch: 51, Steps: 59 | Train Loss: 0.4232232 Vali Loss: 1.5257734 Test Loss: 0.4858536
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.453338146209717
Epoch: 52, Steps: 59 | Train Loss: 0.4229271 Vali Loss: 1.5237683 Test Loss: 0.4852974
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.1594455242156982
Epoch: 53, Steps: 59 | Train Loss: 0.4226497 Vali Loss: 1.5247293 Test Loss: 0.4847274
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.8984529972076416
Epoch: 54, Steps: 59 | Train Loss: 0.4223753 Vali Loss: 1.5335666 Test Loss: 0.4842262
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.3296759128570557
Epoch: 55, Steps: 59 | Train Loss: 0.4220720 Vali Loss: 1.5287943 Test Loss: 0.4837368
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.69073486328125
Epoch: 56, Steps: 59 | Train Loss: 0.4217254 Vali Loss: 1.5190498 Test Loss: 0.4832695
Validation loss decreased (1.522321 --> 1.519050).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.4254653453826904
Epoch: 57, Steps: 59 | Train Loss: 0.4215142 Vali Loss: 1.5193007 Test Loss: 0.4828119
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.102715253829956
Epoch: 58, Steps: 59 | Train Loss: 0.4212651 Vali Loss: 1.5260839 Test Loss: 0.4823979
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.5473899841308594
Epoch: 59, Steps: 59 | Train Loss: 0.4211326 Vali Loss: 1.5209212 Test Loss: 0.4820085
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.080798387527466
Epoch: 60, Steps: 59 | Train Loss: 0.4207715 Vali Loss: 1.5234628 Test Loss: 0.4816113
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.85711669921875
Epoch: 61, Steps: 59 | Train Loss: 0.4207375 Vali Loss: 1.5228322 Test Loss: 0.4812798
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.817986011505127
Epoch: 62, Steps: 59 | Train Loss: 0.4204402 Vali Loss: 1.5181131 Test Loss: 0.4809291
Validation loss decreased (1.519050 --> 1.518113).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.3952372074127197
Epoch: 63, Steps: 59 | Train Loss: 0.4203725 Vali Loss: 1.5200440 Test Loss: 0.4806129
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.4821736812591553
Epoch: 64, Steps: 59 | Train Loss: 0.4200614 Vali Loss: 1.5168790 Test Loss: 0.4802891
Validation loss decreased (1.518113 --> 1.516879).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.4267139434814453
Epoch: 65, Steps: 59 | Train Loss: 0.4200025 Vali Loss: 1.5250106 Test Loss: 0.4800138
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.1971607208251953
Epoch: 66, Steps: 59 | Train Loss: 0.4198588 Vali Loss: 1.5191894 Test Loss: 0.4797345
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.4115359783172607
Epoch: 67, Steps: 59 | Train Loss: 0.4195604 Vali Loss: 1.5185263 Test Loss: 0.4794615
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.3050594329833984
Epoch: 68, Steps: 59 | Train Loss: 0.4195370 Vali Loss: 1.5118173 Test Loss: 0.4792263
Validation loss decreased (1.516879 --> 1.511817).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.339359760284424
Epoch: 69, Steps: 59 | Train Loss: 0.4193410 Vali Loss: 1.5212703 Test Loss: 0.4789917
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.7128591537475586
Epoch: 70, Steps: 59 | Train Loss: 0.4191976 Vali Loss: 1.5223815 Test Loss: 0.4787457
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.328862190246582
Epoch: 71, Steps: 59 | Train Loss: 0.4190998 Vali Loss: 1.5210919 Test Loss: 0.4785520
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.680166721343994
Epoch: 72, Steps: 59 | Train Loss: 0.4190732 Vali Loss: 1.5175154 Test Loss: 0.4783337
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.553346872329712
Epoch: 73, Steps: 59 | Train Loss: 0.4186933 Vali Loss: 1.5151196 Test Loss: 0.4781433
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.123128890991211
Epoch: 74, Steps: 59 | Train Loss: 0.4186936 Vali Loss: 1.5211456 Test Loss: 0.4779605
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.7099790573120117
Epoch: 75, Steps: 59 | Train Loss: 0.4187096 Vali Loss: 1.5142852 Test Loss: 0.4777995
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.8989500999450684
Epoch: 76, Steps: 59 | Train Loss: 0.4185502 Vali Loss: 1.5140834 Test Loss: 0.4775959
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.423907995223999
Epoch: 77, Steps: 59 | Train Loss: 0.4185635 Vali Loss: 1.5196364 Test Loss: 0.4774480
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.5765843391418457
Epoch: 78, Steps: 59 | Train Loss: 0.4184265 Vali Loss: 1.5136247 Test Loss: 0.4773028
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.4129695892333984
Epoch: 79, Steps: 59 | Train Loss: 0.4183739 Vali Loss: 1.5140963 Test Loss: 0.4771601
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.286010265350342
Epoch: 80, Steps: 59 | Train Loss: 0.4182276 Vali Loss: 1.5094781 Test Loss: 0.4770277
Validation loss decreased (1.511817 --> 1.509478).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.7680063247680664
Epoch: 81, Steps: 59 | Train Loss: 0.4181890 Vali Loss: 1.5180129 Test Loss: 0.4768906
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.2992169857025146
Epoch: 82, Steps: 59 | Train Loss: 0.4180991 Vali Loss: 1.5173675 Test Loss: 0.4767670
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.6461973190307617
Epoch: 83, Steps: 59 | Train Loss: 0.4180450 Vali Loss: 1.5144260 Test Loss: 0.4766502
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.0779342651367188
Epoch: 84, Steps: 59 | Train Loss: 0.4179394 Vali Loss: 1.5164728 Test Loss: 0.4765382
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.6257104873657227
Epoch: 85, Steps: 59 | Train Loss: 0.4177996 Vali Loss: 1.5102515 Test Loss: 0.4764320
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.093456983566284
Epoch: 86, Steps: 59 | Train Loss: 0.4177010 Vali Loss: 1.5167661 Test Loss: 0.4763330
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.8739047050476074
Epoch: 87, Steps: 59 | Train Loss: 0.4177541 Vali Loss: 1.5171111 Test Loss: 0.4762373
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.5809988975524902
Epoch: 88, Steps: 59 | Train Loss: 0.4177353 Vali Loss: 1.5171227 Test Loss: 0.4761461
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 3.1424500942230225
Epoch: 89, Steps: 59 | Train Loss: 0.4176323 Vali Loss: 1.5135003 Test Loss: 0.4760562
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.7235636711120605
Epoch: 90, Steps: 59 | Train Loss: 0.4176526 Vali Loss: 1.5182366 Test Loss: 0.4759776
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.1815075874328613
Epoch: 91, Steps: 59 | Train Loss: 0.4175358 Vali Loss: 1.5155860 Test Loss: 0.4758990
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.8871030807495117
Epoch: 92, Steps: 59 | Train Loss: 0.4175224 Vali Loss: 1.5138924 Test Loss: 0.4758219
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.1742067337036133
Epoch: 93, Steps: 59 | Train Loss: 0.4175349 Vali Loss: 1.5155675 Test Loss: 0.4757510
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.104627847671509
Epoch: 94, Steps: 59 | Train Loss: 0.4172848 Vali Loss: 1.5124724 Test Loss: 0.4756830
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.7196273803710938
Epoch: 95, Steps: 59 | Train Loss: 0.4174286 Vali Loss: 1.5105317 Test Loss: 0.4756187
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.1175477504730225
Epoch: 96, Steps: 59 | Train Loss: 0.4174590 Vali Loss: 1.5178790 Test Loss: 0.4755586
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.951523780822754
Epoch: 97, Steps: 59 | Train Loss: 0.4174327 Vali Loss: 1.5107065 Test Loss: 0.4755045
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 4.108783006668091
Epoch: 98, Steps: 59 | Train Loss: 0.4172394 Vali Loss: 1.5112247 Test Loss: 0.4754473
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 4.991821765899658
Epoch: 99, Steps: 59 | Train Loss: 0.4172582 Vali Loss: 1.5110312 Test Loss: 0.4753942
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 4.00933313369751
Epoch: 100, Steps: 59 | Train Loss: 0.4172837 Vali Loss: 1.5131078 Test Loss: 0.4753430
EarlyStopping counter: 20 out of 20
Early stopping
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=90, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21772800.0
params:  24570.0
Trainable parameters:  24570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.272503614425659
Epoch: 1, Steps: 59 | Train Loss: 0.5942905 Vali Loss: 1.4807179 Test Loss: 0.4542915
Validation loss decreased (inf --> 1.480718).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.6877405643463135
Epoch: 2, Steps: 59 | Train Loss: 0.5843933 Vali Loss: 1.4634097 Test Loss: 0.4422840
Validation loss decreased (1.480718 --> 1.463410).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.8306543827056885
Epoch: 3, Steps: 59 | Train Loss: 0.5787920 Vali Loss: 1.4505761 Test Loss: 0.4354172
Validation loss decreased (1.463410 --> 1.450576).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.39750075340271
Epoch: 4, Steps: 59 | Train Loss: 0.5754686 Vali Loss: 1.4427286 Test Loss: 0.4314614
Validation loss decreased (1.450576 --> 1.442729).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.6236886978149414
Epoch: 5, Steps: 59 | Train Loss: 0.5731980 Vali Loss: 1.4416820 Test Loss: 0.4294741
Validation loss decreased (1.442729 --> 1.441682).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.8625051975250244
Epoch: 6, Steps: 59 | Train Loss: 0.5720790 Vali Loss: 1.4381156 Test Loss: 0.4285498
Validation loss decreased (1.441682 --> 1.438116).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.9461185932159424
Epoch: 7, Steps: 59 | Train Loss: 0.5711474 Vali Loss: 1.4339588 Test Loss: 0.4281096
Validation loss decreased (1.438116 --> 1.433959).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8913733959197998
Epoch: 8, Steps: 59 | Train Loss: 0.5707421 Vali Loss: 1.4331844 Test Loss: 0.4280426
Validation loss decreased (1.433959 --> 1.433184).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.8782086372375488
Epoch: 9, Steps: 59 | Train Loss: 0.5705630 Vali Loss: 1.4327739 Test Loss: 0.4281621
Validation loss decreased (1.433184 --> 1.432774).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.3703858852386475
Epoch: 10, Steps: 59 | Train Loss: 0.5702713 Vali Loss: 1.4315856 Test Loss: 0.4282328
Validation loss decreased (1.432774 --> 1.431586).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6925055980682373
Epoch: 11, Steps: 59 | Train Loss: 0.5702227 Vali Loss: 1.4352679 Test Loss: 0.4283941
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.4729487895965576
Epoch: 12, Steps: 59 | Train Loss: 0.5700608 Vali Loss: 1.4336146 Test Loss: 0.4285698
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.8477323055267334
Epoch: 13, Steps: 59 | Train Loss: 0.5700975 Vali Loss: 1.4295826 Test Loss: 0.4286632
Validation loss decreased (1.431586 --> 1.429583).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.4785537719726562
Epoch: 14, Steps: 59 | Train Loss: 0.5699587 Vali Loss: 1.4295754 Test Loss: 0.4287451
Validation loss decreased (1.429583 --> 1.429575).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.212644100189209
Epoch: 15, Steps: 59 | Train Loss: 0.5698686 Vali Loss: 1.4302491 Test Loss: 0.4287862
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.2687180042266846
Epoch: 16, Steps: 59 | Train Loss: 0.5698433 Vali Loss: 1.4276690 Test Loss: 0.4288664
Validation loss decreased (1.429575 --> 1.427669).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.272260904312134
Epoch: 17, Steps: 59 | Train Loss: 0.5697799 Vali Loss: 1.4299953 Test Loss: 0.4288847
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.6455631256103516
Epoch: 18, Steps: 59 | Train Loss: 0.5697304 Vali Loss: 1.4364090 Test Loss: 0.4289450
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.5485315322875977
Epoch: 19, Steps: 59 | Train Loss: 0.5697944 Vali Loss: 1.4323514 Test Loss: 0.4289730
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.301086187362671
Epoch: 20, Steps: 59 | Train Loss: 0.5695543 Vali Loss: 1.4359059 Test Loss: 0.4290857
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0637950897216797
Epoch: 21, Steps: 59 | Train Loss: 0.5695900 Vali Loss: 1.4268506 Test Loss: 0.4290263
Validation loss decreased (1.427669 --> 1.426851).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.757915735244751
Epoch: 22, Steps: 59 | Train Loss: 0.5697299 Vali Loss: 1.4306715 Test Loss: 0.4290877
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.975280523300171
Epoch: 23, Steps: 59 | Train Loss: 0.5694923 Vali Loss: 1.4353360 Test Loss: 0.4291771
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8769862651824951
Epoch: 24, Steps: 59 | Train Loss: 0.5693996 Vali Loss: 1.4313209 Test Loss: 0.4291036
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.78853178024292
Epoch: 25, Steps: 59 | Train Loss: 0.5695458 Vali Loss: 1.4288753 Test Loss: 0.4291257
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.7839362621307373
Epoch: 26, Steps: 59 | Train Loss: 0.5695648 Vali Loss: 1.4311218 Test Loss: 0.4291412
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.586669921875
Epoch: 27, Steps: 59 | Train Loss: 0.5694517 Vali Loss: 1.4289035 Test Loss: 0.4291911
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.609792470932007
Epoch: 28, Steps: 59 | Train Loss: 0.5693006 Vali Loss: 1.4311216 Test Loss: 0.4291824
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.37410044670105
Epoch: 29, Steps: 59 | Train Loss: 0.5693727 Vali Loss: 1.4294751 Test Loss: 0.4292324
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.8120527267456055
Epoch: 30, Steps: 59 | Train Loss: 0.5694199 Vali Loss: 1.4295237 Test Loss: 0.4292787
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.21258282661438
Epoch: 31, Steps: 59 | Train Loss: 0.5694314 Vali Loss: 1.4316431 Test Loss: 0.4292428
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.301576852798462
Epoch: 32, Steps: 59 | Train Loss: 0.5693475 Vali Loss: 1.4355795 Test Loss: 0.4292440
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.758723735809326
Epoch: 33, Steps: 59 | Train Loss: 0.5694866 Vali Loss: 1.4292656 Test Loss: 0.4292819
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9886445999145508
Epoch: 34, Steps: 59 | Train Loss: 0.5693879 Vali Loss: 1.4305384 Test Loss: 0.4292506
EarlyStopping counter: 13 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.5072507858276367
Epoch: 35, Steps: 59 | Train Loss: 0.5693554 Vali Loss: 1.4259479 Test Loss: 0.4292985
Validation loss decreased (1.426851 --> 1.425948).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.804830551147461
Epoch: 36, Steps: 59 | Train Loss: 0.5692021 Vali Loss: 1.4306149 Test Loss: 0.4292721
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8919332027435303
Epoch: 37, Steps: 59 | Train Loss: 0.5691903 Vali Loss: 1.4297814 Test Loss: 0.4293185
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.517308235168457
Epoch: 38, Steps: 59 | Train Loss: 0.5693904 Vali Loss: 1.4306276 Test Loss: 0.4293287
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.026928186416626
Epoch: 39, Steps: 59 | Train Loss: 0.5693363 Vali Loss: 1.4294655 Test Loss: 0.4293205
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.0466203689575195
Epoch: 40, Steps: 59 | Train Loss: 0.5691544 Vali Loss: 1.4320985 Test Loss: 0.4293214
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.219693183898926
Epoch: 41, Steps: 59 | Train Loss: 0.5689987 Vali Loss: 1.4311695 Test Loss: 0.4293555
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.3316493034362793
Epoch: 42, Steps: 59 | Train Loss: 0.5692654 Vali Loss: 1.4347818 Test Loss: 0.4293470
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.7256178855895996
Epoch: 43, Steps: 59 | Train Loss: 0.5690425 Vali Loss: 1.4261560 Test Loss: 0.4293777
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.4918413162231445
Epoch: 44, Steps: 59 | Train Loss: 0.5692118 Vali Loss: 1.4293914 Test Loss: 0.4293781
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9708685874938965
Epoch: 45, Steps: 59 | Train Loss: 0.5692680 Vali Loss: 1.4346567 Test Loss: 0.4293838
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.575192928314209
Epoch: 46, Steps: 59 | Train Loss: 0.5690551 Vali Loss: 1.4286630 Test Loss: 0.4293717
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.255785226821899
Epoch: 47, Steps: 59 | Train Loss: 0.5691728 Vali Loss: 1.4304376 Test Loss: 0.4293844
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.291069746017456
Epoch: 48, Steps: 59 | Train Loss: 0.5690230 Vali Loss: 1.4320874 Test Loss: 0.4293987
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.4988338947296143
Epoch: 49, Steps: 59 | Train Loss: 0.5692411 Vali Loss: 1.4313700 Test Loss: 0.4294001
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.812373638153076
Epoch: 50, Steps: 59 | Train Loss: 0.5691320 Vali Loss: 1.4274583 Test Loss: 0.4293915
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.2883410453796387
Epoch: 51, Steps: 59 | Train Loss: 0.5690353 Vali Loss: 1.4304328 Test Loss: 0.4293911
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.041701078414917
Epoch: 52, Steps: 59 | Train Loss: 0.5689602 Vali Loss: 1.4342779 Test Loss: 0.4294126
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.2443838119506836
Epoch: 53, Steps: 59 | Train Loss: 0.5692194 Vali Loss: 1.4307388 Test Loss: 0.4294095
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.0315401554107666
Epoch: 54, Steps: 59 | Train Loss: 0.5691123 Vali Loss: 1.4339981 Test Loss: 0.4294151
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.1815896034240723
Epoch: 55, Steps: 59 | Train Loss: 0.5692517 Vali Loss: 1.4265645 Test Loss: 0.4294199
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.42830419540405273, mae:0.44792675971984863, rse:0.6265101432800293, corr:[0.22699456 0.2346342  0.2345347  0.23487417 0.23386724 0.23152934
 0.23056509 0.23160523 0.2317573  0.23113967 0.23081681 0.23093274
 0.23098806 0.23074493 0.23056446 0.23063053 0.23037805 0.22995302
 0.22978775 0.22963607 0.22941667 0.22959873 0.23038223 0.23107941
 0.23098527 0.23096602 0.231357   0.23135671 0.23077175 0.23035236
 0.23032784 0.23014677 0.2295764  0.22908737 0.22914746 0.22929677
 0.22924055 0.22906446 0.22908701 0.22911151 0.22909181 0.22904535
 0.22922322 0.22922318 0.22885792 0.22864164 0.22919247 0.22990373
 0.22985213 0.22949234 0.22911519 0.22872558 0.22816211 0.22740795
 0.22699106 0.22650366 0.22612254 0.22582296 0.22552602 0.2254543
 0.22527033 0.22499758 0.22469884 0.22466561 0.22479309 0.2250468
 0.22530422 0.22523335 0.2250946  0.22526854 0.22549726 0.22541434
 0.22477245 0.22411421 0.22357954 0.2231693  0.22290196 0.22282365
 0.22273305 0.2222883  0.22177593 0.22149889 0.22127607 0.22102907
 0.2208417  0.22076808 0.22069223 0.22041097 0.22011277 0.22001131
 0.21990585 0.21961832 0.21933614 0.21946867 0.2201468  0.2212709
 0.2224445  0.22331685 0.22400387 0.22441952 0.22456813 0.22463222
 0.22461756 0.22455764 0.22426584 0.22390369 0.22361574 0.22336076
 0.22314943 0.22301744 0.22300482 0.22317168 0.22333685 0.22342268
 0.22343472 0.22322965 0.22299953 0.22284006 0.22282797 0.2229532
 0.22303152 0.22286357 0.22238207 0.22190303 0.22157791 0.22130796
 0.22109373 0.22090866 0.22067282 0.22037067 0.22003524 0.21964152
 0.21929225 0.21921688 0.21930128 0.21929209 0.21929777 0.2193603
 0.21945326 0.21940184 0.21933196 0.21935287 0.2192608  0.21911336
 0.21906132 0.21892837 0.218549   0.2178386  0.217139   0.21671659
 0.21659619 0.21661513 0.21660145 0.21665664 0.21661504 0.21647555
 0.21637636 0.21638513 0.21634047 0.21619703 0.21607578 0.21615158
 0.21623117 0.21608797 0.2158872  0.21572128 0.2155269  0.21563457
 0.21621999 0.21716565 0.21792589 0.2183452  0.21854152 0.21868192
 0.21878442 0.21876863 0.21874587 0.2187686  0.21872132 0.21851484
 0.21831512 0.21822366 0.21829578 0.21834482 0.21834911 0.21849029
 0.21872303 0.21885245 0.21894655 0.21886015 0.21862483 0.2183653
 0.21818437 0.21806672 0.21765901 0.217058   0.21644804 0.21597108
 0.2156892  0.2156325  0.2154909  0.21527877 0.2150497  0.2149332
 0.21491882 0.21504568 0.21514826 0.21520151 0.21529163 0.21534286
 0.21526438 0.21500853 0.2146849  0.2144035  0.21412686 0.21411198
 0.21430328 0.21434097 0.21432264 0.21420649 0.21407327 0.21397714
 0.21383779 0.21376339 0.21372844 0.21353497 0.21302614 0.2124959
 0.21215992 0.21204114 0.21204618 0.21193926 0.21174978 0.21162328
 0.2115488  0.21133997 0.2110538  0.21086764 0.21091156 0.21103711
 0.21131395 0.2115219  0.21164581 0.21178348 0.21199071 0.21213685
 0.21223384 0.21225753 0.21228758 0.21215177 0.21185775 0.21153484
 0.21136731 0.21140516 0.21164455 0.21169308 0.21168324 0.21175426
 0.2117796  0.21149778 0.21134382 0.21148112 0.21156925 0.21145394
 0.21120736 0.2110672  0.21101469 0.21094991 0.21073288 0.21049207
 0.21032964 0.21019073 0.21001036 0.20994903 0.2099355  0.20982935
 0.20970494 0.20966375 0.20964432 0.20957552 0.20934244 0.2090999
 0.20900878 0.20903064 0.20911653 0.20917669 0.20935082 0.20985796
 0.21057574 0.21115695 0.21152407 0.21170726 0.21191706 0.21215977
 0.2123715  0.21248057 0.21246001 0.21234618 0.21207306 0.2118872
 0.21192865 0.21198593 0.21203473 0.21201001 0.2120292  0.21208513
 0.21212111 0.21200943 0.21192805 0.21195073 0.21194424 0.21215801
 0.21265303 0.21318324 0.21345387 0.21351475 0.21338513 0.21316592
 0.21294759 0.21280792 0.21264571 0.21243684 0.21204494 0.21168232
 0.21158636 0.21174635 0.2118642  0.21197489 0.21205336 0.21223971
 0.21246682 0.21254644 0.2125565  0.21253385 0.21238649 0.2121506
 0.21210387 0.21223855 0.21225184 0.21187694 0.2114163  0.2110762
 0.21098831 0.2109234  0.2106961  0.2105234  0.21064459 0.21088266
 0.21095033 0.21092752 0.2109636  0.21113032 0.21117449 0.2112715
 0.21130128 0.21115841 0.21096979 0.21089692 0.21057338 0.2102934
 0.210292   0.21040379 0.2104109  0.2102384  0.20989841 0.20932649
 0.208901   0.20867814 0.20855789 0.2083827  0.2080491  0.20767847
 0.20755163 0.20753399 0.20738551 0.20697437 0.20668425 0.20679945
 0.20707868 0.20715524 0.20706007 0.20707822 0.20720942 0.20784555
 0.20890638 0.20995757 0.21070544 0.210832   0.2106045  0.2101694
 0.20981252 0.20926875 0.20872538 0.20839795 0.20817794 0.20803136
 0.20794128 0.20791072 0.2080201  0.20816217 0.20830087 0.2085912
 0.20886993 0.20910653 0.20930955 0.20966323 0.20992862 0.21025684
 0.21055219 0.21074127 0.21064942 0.21037619 0.21015903 0.20999639
 0.20982942 0.2097322  0.2096034  0.20947418 0.20940648 0.20931734
 0.20928255 0.20937634 0.20929381 0.20898663 0.20874134 0.20879878
 0.20894016 0.20907056 0.20909113 0.2095427  0.2101553  0.21075128
 0.21124928 0.21158971 0.21165189 0.21152082 0.21138287 0.21130405
 0.21122095 0.21118546 0.21091218 0.21061452 0.21050586 0.21054456
 0.21065572 0.21062377 0.21046077 0.21027242 0.21027267 0.21048574
 0.21062098 0.21050006 0.21048471 0.21085714 0.21139964 0.21213852
 0.21277797 0.21317574 0.21323991 0.21309996 0.21267867 0.21223316
 0.21196379 0.21181536 0.21170345 0.21175885 0.21191102 0.21193628
 0.2119095  0.2120342  0.21231264 0.21244733 0.2123914  0.2124112
 0.21255025 0.2126326  0.21257609 0.2123418  0.212421   0.21266143
 0.21268295 0.21252578 0.21232513 0.21223217 0.21206415 0.21175645
 0.21158369 0.21142986 0.21112005 0.2109202  0.21076074 0.21063296
 0.21052074 0.21043774 0.21045457 0.2104582  0.21038787 0.21037759
 0.21051428 0.21065053 0.21076778 0.21099378 0.21153139 0.21243165
 0.2133163  0.21401004 0.21450862 0.21479733 0.2146633  0.21414407
 0.21368986 0.21336466 0.21299358 0.21256948 0.21227336 0.2121582
 0.21214491 0.21222384 0.21252176 0.21280561 0.21299206 0.21319515
 0.21340145 0.21357778 0.21374781 0.21405256 0.21459    0.21493784
 0.21484058 0.21462767 0.21464254 0.21472195 0.21426448 0.21342036
 0.21285404 0.21274026 0.21257155 0.2123286  0.21214384 0.21216212
 0.21213743 0.21215633 0.2123557  0.21273117 0.2130516  0.21319334
 0.2131787  0.21320878 0.21320032 0.21312915 0.21295792 0.21286167
 0.21265408 0.21236056 0.2121314  0.21180326 0.21115589 0.21051809
 0.2100192  0.20952585 0.209081   0.20858195 0.20819879 0.20794712
 0.20780171 0.20753026 0.20732594 0.20718153 0.20722507 0.20736201
 0.207588   0.20770004 0.20775358 0.20773472 0.20787486 0.20803405
 0.20801666 0.20790304 0.20756204 0.20737787 0.2072526  0.20667432
 0.2060433  0.20558347 0.20534979 0.20520292 0.20506617 0.20486204
 0.20477396 0.20454761 0.20445527 0.20434585 0.204243   0.20428544
 0.20446193 0.2045216  0.20447005 0.2043954  0.20432186 0.20423856
 0.20403126 0.20364168 0.20331436 0.20291887 0.20221086 0.20136799
 0.20089139 0.20073259 0.20035975 0.19992597 0.19974478 0.19974317
 0.199664   0.19925515 0.19892012 0.19902131 0.19911234 0.19895038
 0.19878782 0.19884546 0.19909036 0.19940548 0.19973898 0.20014888
 0.20028566 0.19983323 0.19946146 0.1991791  0.19868055 0.19799314
 0.19767743 0.19755077 0.19710504 0.19657253 0.19624124 0.19630809
 0.19650331 0.19630712 0.19607416 0.19596846 0.19589876 0.19595902
 0.19598772 0.19601843 0.1961172  0.19629855 0.19672915 0.19731611
 0.19758292 0.19737962 0.196891   0.1968106  0.19653513 0.19611543
 0.19565874 0.19525057 0.1947557  0.19429854 0.19414113 0.19408733
 0.1939551  0.19372559 0.19358814 0.19340964 0.19348796 0.19361372
 0.19371557 0.19372909 0.19376643 0.19378082 0.19381268 0.1939566
 0.19362824 0.19271983 0.19184557 0.19117144 0.19053586 0.18957411
 0.1887192  0.18837358 0.18825036 0.18795873 0.18778153 0.18785411
 0.18805301 0.18789129 0.18763778 0.18770821 0.18777572 0.18791933
 0.18813044 0.18845633 0.18874599 0.18897875 0.18917738 0.18939526
 0.18960819 0.18943112 0.18903562 0.18884197 0.18860446 0.18794495
 0.18750429 0.18740834 0.18727234 0.18683168 0.18642946 0.18627222
 0.18616629 0.18560895 0.18534397 0.18501453 0.18472143 0.1843762
 0.18476713 0.1853659  0.18499424 0.18353008 0.18417627 0.18404052]
