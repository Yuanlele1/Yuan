Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  48787200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8234078884124756
Epoch: 1, Steps: 56 | Train Loss: 0.9265040 Vali Loss: 1.8900328 Test Loss: 0.7505700
Validation loss decreased (inf --> 1.890033).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.7778689861297607
Epoch: 2, Steps: 56 | Train Loss: 0.7415842 Vali Loss: 1.7243936 Test Loss: 0.6431875
Validation loss decreased (1.890033 --> 1.724394).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.7321665287017822
Epoch: 3, Steps: 56 | Train Loss: 0.6869100 Vali Loss: 1.6487675 Test Loss: 0.5915476
Validation loss decreased (1.724394 --> 1.648767).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9046103954315186
Epoch: 4, Steps: 56 | Train Loss: 0.6558743 Vali Loss: 1.6008215 Test Loss: 0.5551286
Validation loss decreased (1.648767 --> 1.600821).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8914995193481445
Epoch: 5, Steps: 56 | Train Loss: 0.6345485 Vali Loss: 1.5657185 Test Loss: 0.5278399
Validation loss decreased (1.600821 --> 1.565719).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.818481683731079
Epoch: 6, Steps: 56 | Train Loss: 0.6188610 Vali Loss: 1.5383283 Test Loss: 0.5066826
Validation loss decreased (1.565719 --> 1.538328).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.7363612651824951
Epoch: 7, Steps: 56 | Train Loss: 0.6065331 Vali Loss: 1.5164216 Test Loss: 0.4900827
Validation loss decreased (1.538328 --> 1.516422).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.7042901515960693
Epoch: 8, Steps: 56 | Train Loss: 0.5972277 Vali Loss: 1.4982245 Test Loss: 0.4773948
Validation loss decreased (1.516422 --> 1.498224).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7679378986358643
Epoch: 9, Steps: 56 | Train Loss: 0.5898400 Vali Loss: 1.4779388 Test Loss: 0.4673243
Validation loss decreased (1.498224 --> 1.477939).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.813835859298706
Epoch: 10, Steps: 56 | Train Loss: 0.5837753 Vali Loss: 1.4784098 Test Loss: 0.4594252
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7328410148620605
Epoch: 11, Steps: 56 | Train Loss: 0.5793377 Vali Loss: 1.4600226 Test Loss: 0.4535217
Validation loss decreased (1.477939 --> 1.460023).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.7809782028198242
Epoch: 12, Steps: 56 | Train Loss: 0.5753025 Vali Loss: 1.4543676 Test Loss: 0.4485788
Validation loss decreased (1.460023 --> 1.454368).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.8457982540130615
Epoch: 13, Steps: 56 | Train Loss: 0.5722224 Vali Loss: 1.4563439 Test Loss: 0.4448848
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.7067830562591553
Epoch: 14, Steps: 56 | Train Loss: 0.5696205 Vali Loss: 1.4469030 Test Loss: 0.4420260
Validation loss decreased (1.454368 --> 1.446903).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.8097033500671387
Epoch: 15, Steps: 56 | Train Loss: 0.5673553 Vali Loss: 1.4457124 Test Loss: 0.4398244
Validation loss decreased (1.446903 --> 1.445712).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7382769584655762
Epoch: 16, Steps: 56 | Train Loss: 0.5658190 Vali Loss: 1.4413614 Test Loss: 0.4381043
Validation loss decreased (1.445712 --> 1.441361).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.7235655784606934
Epoch: 17, Steps: 56 | Train Loss: 0.5641115 Vali Loss: 1.4431896 Test Loss: 0.4367493
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.9235808849334717
Epoch: 18, Steps: 56 | Train Loss: 0.5627510 Vali Loss: 1.4371471 Test Loss: 0.4356960
Validation loss decreased (1.441361 --> 1.437147).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.7358129024505615
Epoch: 19, Steps: 56 | Train Loss: 0.5618618 Vali Loss: 1.4396406 Test Loss: 0.4348140
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8259835243225098
Epoch: 20, Steps: 56 | Train Loss: 0.5610820 Vali Loss: 1.4415460 Test Loss: 0.4342808
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.7912523746490479
Epoch: 21, Steps: 56 | Train Loss: 0.5600226 Vali Loss: 1.4300637 Test Loss: 0.4338412
Validation loss decreased (1.437147 --> 1.430064).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.6780602931976318
Epoch: 22, Steps: 56 | Train Loss: 0.5596195 Vali Loss: 1.4331182 Test Loss: 0.4334532
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8185484409332275
Epoch: 23, Steps: 56 | Train Loss: 0.5589932 Vali Loss: 1.4352994 Test Loss: 0.4331543
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.896571397781372
Epoch: 24, Steps: 56 | Train Loss: 0.5582174 Vali Loss: 1.4363594 Test Loss: 0.4330052
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.6353037357330322
Epoch: 25, Steps: 56 | Train Loss: 0.5578584 Vali Loss: 1.4323356 Test Loss: 0.4328151
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7314872741699219
Epoch: 26, Steps: 56 | Train Loss: 0.5573751 Vali Loss: 1.4365230 Test Loss: 0.4327172
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.836425542831421
Epoch: 27, Steps: 56 | Train Loss: 0.5571031 Vali Loss: 1.4312052 Test Loss: 0.4326721
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6410980224609375
Epoch: 28, Steps: 56 | Train Loss: 0.5564535 Vali Loss: 1.4356027 Test Loss: 0.4325726
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7527523040771484
Epoch: 29, Steps: 56 | Train Loss: 0.5560635 Vali Loss: 1.4363854 Test Loss: 0.4325452
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.7376346588134766
Epoch: 30, Steps: 56 | Train Loss: 0.5559833 Vali Loss: 1.4336922 Test Loss: 0.4325314
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.7752044200897217
Epoch: 31, Steps: 56 | Train Loss: 0.5557032 Vali Loss: 1.4374856 Test Loss: 0.4325155
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.738529920578003
Epoch: 32, Steps: 56 | Train Loss: 0.5558579 Vali Loss: 1.4339495 Test Loss: 0.4325384
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.661752462387085
Epoch: 33, Steps: 56 | Train Loss: 0.5552733 Vali Loss: 1.4339662 Test Loss: 0.4325071
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6843202114105225
Epoch: 34, Steps: 56 | Train Loss: 0.5552286 Vali Loss: 1.4339783 Test Loss: 0.4325090
EarlyStopping counter: 13 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.8075032234191895
Epoch: 35, Steps: 56 | Train Loss: 0.5554117 Vali Loss: 1.4299779 Test Loss: 0.4325929
Validation loss decreased (1.430064 --> 1.429978).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.771782636642456
Epoch: 36, Steps: 56 | Train Loss: 0.5547211 Vali Loss: 1.4409606 Test Loss: 0.4326071
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.825674295425415
Epoch: 37, Steps: 56 | Train Loss: 0.5545886 Vali Loss: 1.4334348 Test Loss: 0.4326174
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7413227558135986
Epoch: 38, Steps: 56 | Train Loss: 0.5542488 Vali Loss: 1.4351959 Test Loss: 0.4326721
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.639540672302246
Epoch: 39, Steps: 56 | Train Loss: 0.5544600 Vali Loss: 1.4355584 Test Loss: 0.4326898
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.9045524597167969
Epoch: 40, Steps: 56 | Train Loss: 0.5542062 Vali Loss: 1.4325631 Test Loss: 0.4327445
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.659156084060669
Epoch: 41, Steps: 56 | Train Loss: 0.5541920 Vali Loss: 1.4334840 Test Loss: 0.4327520
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.8633673191070557
Epoch: 42, Steps: 56 | Train Loss: 0.5539402 Vali Loss: 1.4360389 Test Loss: 0.4328074
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.8221120834350586
Epoch: 43, Steps: 56 | Train Loss: 0.5537317 Vali Loss: 1.4387033 Test Loss: 0.4328141
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.7869796752929688
Epoch: 44, Steps: 56 | Train Loss: 0.5541586 Vali Loss: 1.4300201 Test Loss: 0.4328222
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7458302974700928
Epoch: 45, Steps: 56 | Train Loss: 0.5536225 Vali Loss: 1.4257236 Test Loss: 0.4328536
Validation loss decreased (1.429978 --> 1.425724).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.7629034519195557
Epoch: 46, Steps: 56 | Train Loss: 0.5538365 Vali Loss: 1.4374284 Test Loss: 0.4328438
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.7055230140686035
Epoch: 47, Steps: 56 | Train Loss: 0.5534695 Vali Loss: 1.4347975 Test Loss: 0.4328817
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.7194337844848633
Epoch: 48, Steps: 56 | Train Loss: 0.5534226 Vali Loss: 1.4316258 Test Loss: 0.4329159
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.8237686157226562
Epoch: 49, Steps: 56 | Train Loss: 0.5536991 Vali Loss: 1.4290935 Test Loss: 0.4329202
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6990351676940918
Epoch: 50, Steps: 56 | Train Loss: 0.5533740 Vali Loss: 1.4329059 Test Loss: 0.4329346
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6429557800292969
Epoch: 51, Steps: 56 | Train Loss: 0.5532086 Vali Loss: 1.4296831 Test Loss: 0.4329644
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.5939476490020752
Epoch: 52, Steps: 56 | Train Loss: 0.5528749 Vali Loss: 1.4300833 Test Loss: 0.4329892
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.770629644393921
Epoch: 53, Steps: 56 | Train Loss: 0.5532531 Vali Loss: 1.4299488 Test Loss: 0.4329978
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.748504400253296
Epoch: 54, Steps: 56 | Train Loss: 0.5529659 Vali Loss: 1.4425550 Test Loss: 0.4329958
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.8512187004089355
Epoch: 55, Steps: 56 | Train Loss: 0.5531354 Vali Loss: 1.4350028 Test Loss: 0.4330172
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7402164936065674
Epoch: 56, Steps: 56 | Train Loss: 0.5531048 Vali Loss: 1.4354906 Test Loss: 0.4330408
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.779918909072876
Epoch: 57, Steps: 56 | Train Loss: 0.5531134 Vali Loss: 1.4316088 Test Loss: 0.4330444
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.6213600635528564
Epoch: 58, Steps: 56 | Train Loss: 0.5530663 Vali Loss: 1.4360102 Test Loss: 0.4330699
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.6088507175445557
Epoch: 59, Steps: 56 | Train Loss: 0.5530001 Vali Loss: 1.4357499 Test Loss: 0.4330871
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.6864216327667236
Epoch: 60, Steps: 56 | Train Loss: 0.5527694 Vali Loss: 1.4314576 Test Loss: 0.4330966
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6292967796325684
Epoch: 61, Steps: 56 | Train Loss: 0.5527432 Vali Loss: 1.4344094 Test Loss: 0.4331038
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6110382080078125
Epoch: 62, Steps: 56 | Train Loss: 0.5525057 Vali Loss: 1.4364820 Test Loss: 0.4331133
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.668114423751831
Epoch: 63, Steps: 56 | Train Loss: 0.5527853 Vali Loss: 1.4332473 Test Loss: 0.4331276
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.6727244853973389
Epoch: 64, Steps: 56 | Train Loss: 0.5524492 Vali Loss: 1.4317442 Test Loss: 0.4331439
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.6714510917663574
Epoch: 65, Steps: 56 | Train Loss: 0.5524950 Vali Loss: 1.4325067 Test Loss: 0.4331521
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4319343864917755, mae:0.4551682472229004, rse:0.6291596293449402, corr:[0.21715938 0.23073852 0.22968984 0.23063977 0.2327956  0.23201236
 0.23032787 0.23051934 0.23185693 0.23273413 0.23210298 0.23075448
 0.22992304 0.22982354 0.23007482 0.23003894 0.2294341  0.22874676
 0.22865114 0.2292518  0.22972865 0.22966702 0.22941464 0.22974621
 0.23044479 0.23077673 0.2305808  0.23039441 0.2306755  0.23102932
 0.230909   0.23031309 0.229774   0.22951625 0.22940324 0.2293111
 0.22923183 0.2288643  0.22848955 0.22848636 0.2289728  0.2293309
 0.22926648 0.22912212 0.2294516  0.23018661 0.23073244 0.23072274
 0.23045734 0.23019604 0.2297234  0.22883092 0.22768629 0.22671367
 0.22626592 0.22601281 0.22589292 0.22548379 0.2249291  0.22460544
 0.22453317 0.22447641 0.22430074 0.22412418 0.22403866 0.22418931
 0.22454637 0.2247899  0.22475818 0.2245438  0.22436315 0.22444706
 0.2242418  0.22362198 0.222949   0.22266857 0.22266327 0.22247091
 0.22196442 0.2214337  0.22117227 0.22109395 0.2208673  0.22043182
 0.2199925  0.21963748 0.21933605 0.21920022 0.2191515  0.2190874
 0.21893033 0.21900472 0.2194032  0.21997848 0.22038351 0.22100574
 0.22207187 0.22311367 0.22371839 0.22396657 0.22390369 0.22378078
 0.22381857 0.2239654  0.22390762 0.22348446 0.22298883 0.22275987
 0.22274686 0.22275129 0.22265537 0.22256094 0.22257356 0.2226166
 0.22272073 0.2227679  0.22274555 0.2226394  0.2225788  0.2227126
 0.22275606 0.22234628 0.22165811 0.22124471 0.22102901 0.22076145
 0.22033848 0.21998891 0.21983308 0.21977217 0.2196197  0.21930361
 0.21903104 0.21886124 0.21878122 0.21875763 0.21871209 0.21862409
 0.21853274 0.218478   0.21853428 0.21856603 0.21833879 0.218186
 0.2181246  0.21795297 0.21768811 0.21728878 0.21678837 0.21634793
 0.21618532 0.21637832 0.21647191 0.21614832 0.2157059  0.2156099
 0.21567087 0.21551113 0.21513745 0.21501599 0.21519662 0.21551341
 0.21566746 0.21573171 0.21580575 0.21594411 0.21579035 0.21582134
 0.2161079  0.21651289 0.216812   0.21707003 0.21712632 0.21699126
 0.21674493 0.21655056 0.21639691 0.21634708 0.21630548 0.21616086
 0.21593109 0.21579514 0.21589963 0.21614839 0.21640818 0.21658817
 0.21673748 0.21691595 0.21716067 0.21724108 0.21716863 0.2170161
 0.21675022 0.21627864 0.21569301 0.21515137 0.21473286 0.21447626
 0.21443442 0.21456096 0.21450652 0.21408977 0.21371713 0.21372579
 0.21405496 0.21438353 0.21451555 0.21451765 0.21452352 0.21452646
 0.21442696 0.21420407 0.21398632 0.21374239 0.2135182  0.21346937
 0.21350126 0.21345352 0.21330397 0.21323703 0.21328022 0.21325299
 0.21311206 0.21293269 0.21279532 0.21265787 0.2123626  0.21199624
 0.2117245  0.21159299 0.21151058 0.21140014 0.21123897 0.21113093
 0.21107347 0.21097076 0.21085048 0.21079957 0.21088396 0.2110412
 0.2112814  0.2114896  0.21169467 0.21176465 0.21152465 0.21118087
 0.21120332 0.21145794 0.21142438 0.21088603 0.21034731 0.21019885
 0.21030913 0.21027835 0.21018438 0.21015467 0.21026513 0.21040815
 0.21047139 0.21037655 0.21032253 0.21021499 0.21006481 0.2100921
 0.2102559  0.21022029 0.20986266 0.20949554 0.20933172 0.20930433
 0.20916192 0.20888111 0.20875274 0.20892058 0.20903188 0.20892552
 0.2088085  0.20894863 0.20902973 0.20891204 0.20869997 0.20871517
 0.20882565 0.20895222 0.20894597 0.20883718 0.20876132 0.20905359
 0.20959459 0.21012251 0.21055852 0.21077192 0.21065354 0.21050012
 0.21067667 0.21104328 0.21104974 0.21070878 0.21053377 0.21081106
 0.21112306 0.21104033 0.21081407 0.2108884  0.21120021 0.21141976
 0.21148294 0.21139118 0.21134494 0.21137859 0.21141878 0.21166171
 0.21202068 0.21206592 0.2117045  0.21134923 0.21112575 0.21080157
 0.21017408 0.20951779 0.20923786 0.20929225 0.2092313  0.2088918
 0.20862542 0.20876953 0.20905548 0.2092066  0.20906997 0.20902532
 0.20916682 0.20925306 0.20929116 0.20943895 0.20958386 0.20952642
 0.2091901  0.20873673 0.2085632  0.20852336 0.20815782 0.2075555
 0.20717546 0.20702194 0.20675729 0.20643686 0.20637882 0.20661958
 0.20676751 0.20669012 0.20654432 0.20662475 0.20679249 0.20699735
 0.20705347 0.20707582 0.20718454 0.20730652 0.20709634 0.20687816
 0.20677382 0.20654409 0.20610622 0.20568956 0.2053902  0.20503575
 0.20450588 0.2039667  0.2036836  0.20366137 0.20358038 0.2032313
 0.20270461 0.20234525 0.20220621 0.20202062 0.20179929 0.20168789
 0.20173906 0.2017369  0.20176233 0.20212214 0.20254824 0.20310542
 0.20365275 0.20417282 0.20470785 0.2049295  0.20443816 0.20340635
 0.20274614 0.20256343 0.20221113 0.2014545  0.20075744 0.20072338
 0.20104416 0.20112377 0.20101517 0.20098959 0.20115909 0.20140308
 0.2015747  0.20172197 0.20198649 0.20220874 0.20222804 0.20245886
 0.20296386 0.20328952 0.20324391 0.2031841  0.20343426 0.20355041
 0.20325391 0.20265184 0.20231149 0.2023782  0.2024815  0.20229013
 0.20201106 0.20204234 0.2020643  0.20183331 0.2014545  0.20139962
 0.20169353 0.20202817 0.20199288 0.20223577 0.20267561 0.20312046
 0.20339458 0.20346673 0.20351368 0.20354614 0.20326945 0.20285036
 0.20282057 0.20306545 0.20280764 0.20217983 0.20184799 0.20215602
 0.20246533 0.20218515 0.20163074 0.20134312 0.2013846  0.20144632
 0.20140722 0.20139764 0.20157239 0.20180999 0.20206597 0.20265825
 0.20336744 0.20367414 0.20340826 0.20296453 0.20275585 0.20251611
 0.20202665 0.20146717 0.20127752 0.20159271 0.20175472 0.20155808
 0.20135659 0.20155628 0.20186636 0.20183733 0.20155226 0.20160812
 0.20189254 0.20189486 0.2014916  0.20109463 0.20134854 0.20178312
 0.20187137 0.20169541 0.20157102 0.20149817 0.20116833 0.20071629
 0.20062053 0.20083006 0.20058411 0.20002598 0.19974104 0.19993779
 0.20005563 0.19966088 0.1991843  0.19909593 0.19928253 0.19930485
 0.19913454 0.19908176 0.19929531 0.19961236 0.19998033 0.20067453
 0.20157224 0.20217983 0.20224151 0.20229737 0.20260695 0.20248231
 0.20171292 0.2009261  0.20064497 0.20066231 0.20053935 0.20031738
 0.20036617 0.20079361 0.20127977 0.20128997 0.2013447  0.20182657
 0.20235041 0.20243676 0.20259519 0.20290117 0.20374936 0.20386402
 0.2034868  0.20313965 0.20326215 0.20343734 0.20279327 0.20202096
 0.20190167 0.20227835 0.2020445  0.20139755 0.20115942 0.20156263
 0.20171703 0.20125559 0.2011525  0.20176607 0.20245409 0.2023348
 0.20189077 0.20203905 0.20254612 0.20248583 0.20175028 0.2011672
 0.20100114 0.20065123 0.19961227 0.19865978 0.19851004 0.1985066
 0.19776224 0.196763   0.19646175 0.19676182 0.19656256 0.19564706
 0.19490616 0.19481383 0.1950469  0.19456254 0.1941385  0.19439252
 0.19501294 0.19484876 0.19439876 0.19445989 0.19494396 0.19485088
 0.19403309 0.1935816  0.19379619 0.19411054 0.19353373 0.1924963
 0.19242623 0.19276728 0.19241987 0.19132352 0.19096561 0.19156
 0.19205944 0.1915002  0.19091977 0.1910832  0.19157773 0.1915928
 0.19132839 0.19145663 0.19181627 0.19174878 0.19127175 0.1910278
 0.19106619 0.19049266 0.18918318 0.18826745 0.18817644 0.18769689
 0.18627237 0.18501104 0.18493557 0.18553983 0.1853151  0.18427747
 0.18372838 0.18412587 0.18459512 0.18425432 0.18371783 0.18383326
 0.18430124 0.18441507 0.18449306 0.18508045 0.1858093  0.18570931
 0.18486696 0.18442331 0.18451074 0.18417905 0.18294314 0.18191038
 0.18214361 0.18253146 0.18163821 0.18033199 0.18020678 0.1811507
 0.18140346 0.18037984 0.17968334 0.18009228 0.18058717 0.18029226
 0.17970121 0.17978835 0.18017422 0.18003443 0.17987962 0.18044555
 0.18098935 0.18037853 0.17897101 0.17869994 0.17925401 0.17881845
 0.17704265 0.17573276 0.17610675 0.17654763 0.17552431 0.17408134
 0.17401922 0.17482242 0.17485423 0.17385042 0.17347032 0.17428258
 0.17489903 0.17456548 0.17407185 0.17433842 0.17439707 0.17327853
 0.17145602 0.17036025 0.16956575 0.16784753 0.16574174 0.16449007
 0.1647305  0.16441339 0.1630213  0.16187362 0.16241972 0.16331606
 0.16267043 0.16118847 0.1610432  0.1618622  0.16206089 0.16175348
 0.16201217 0.16266622 0.16249429 0.16178711 0.16197369 0.16251019
 0.161272   0.15905686 0.15824297 0.15967746 0.1595925  0.15637445
 0.15414545 0.15538083 0.15701096 0.15527125 0.15269396 0.15298401
 0.15472674 0.15333806 0.15095624 0.15164918 0.15393078 0.15241204
 0.15034981 0.15321423 0.15522192 0.14467011 0.14472376 0.17649984]
