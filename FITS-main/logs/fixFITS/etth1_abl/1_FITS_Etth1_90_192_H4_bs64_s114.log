Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_192_FITS_ETTh1_ftM_sl90_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8359
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=26, out_features=81, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1886976.0
params:  2187.0
Trainable parameters:  2187
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.6767220497131348
Epoch: 1, Steps: 65 | Train Loss: 0.8474080 Vali Loss: 1.5788363 Test Loss: 0.8914230
Validation loss decreased (inf --> 1.578836).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.5012333393096924
Epoch: 2, Steps: 65 | Train Loss: 0.6459578 Vali Loss: 1.3244419 Test Loss: 0.6702920
Validation loss decreased (1.578836 --> 1.324442).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.7468600273132324
Epoch: 3, Steps: 65 | Train Loss: 0.5527171 Vali Loss: 1.2036823 Test Loss: 0.5689328
Validation loss decreased (1.324442 --> 1.203682).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.370995044708252
Epoch: 4, Steps: 65 | Train Loss: 0.5058282 Vali Loss: 1.1390184 Test Loss: 0.5173225
Validation loss decreased (1.203682 --> 1.139018).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.67616605758667
Epoch: 5, Steps: 65 | Train Loss: 0.4795203 Vali Loss: 1.1011394 Test Loss: 0.4895374
Validation loss decreased (1.139018 --> 1.101139).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.648202419281006
Epoch: 6, Steps: 65 | Train Loss: 0.4645072 Vali Loss: 1.0780382 Test Loss: 0.4737192
Validation loss decreased (1.101139 --> 1.078038).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2247049808502197
Epoch: 7, Steps: 65 | Train Loss: 0.4549618 Vali Loss: 1.0631285 Test Loss: 0.4642207
Validation loss decreased (1.078038 --> 1.063128).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.6459145545959473
Epoch: 8, Steps: 65 | Train Loss: 0.4484980 Vali Loss: 1.0526544 Test Loss: 0.4582161
Validation loss decreased (1.063128 --> 1.052654).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.935696840286255
Epoch: 9, Steps: 65 | Train Loss: 0.4440994 Vali Loss: 1.0451782 Test Loss: 0.4540791
Validation loss decreased (1.052654 --> 1.045178).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.3892838954925537
Epoch: 10, Steps: 65 | Train Loss: 0.4408596 Vali Loss: 1.0390109 Test Loss: 0.4513368
Validation loss decreased (1.045178 --> 1.039011).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.148555278778076
Epoch: 11, Steps: 65 | Train Loss: 0.4386911 Vali Loss: 1.0352346 Test Loss: 0.4491721
Validation loss decreased (1.039011 --> 1.035235).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9533298015594482
Epoch: 12, Steps: 65 | Train Loss: 0.4361771 Vali Loss: 1.0308294 Test Loss: 0.4474664
Validation loss decreased (1.035235 --> 1.030829).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.7833616733551025
Epoch: 13, Steps: 65 | Train Loss: 0.4346513 Vali Loss: 1.0286658 Test Loss: 0.4461789
Validation loss decreased (1.030829 --> 1.028666).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.803335428237915
Epoch: 14, Steps: 65 | Train Loss: 0.4335919 Vali Loss: 1.0262064 Test Loss: 0.4450566
Validation loss decreased (1.028666 --> 1.026206).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.9276196956634521
Epoch: 15, Steps: 65 | Train Loss: 0.4322070 Vali Loss: 1.0241457 Test Loss: 0.4441445
Validation loss decreased (1.026206 --> 1.024146).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.355931282043457
Epoch: 16, Steps: 65 | Train Loss: 0.4314842 Vali Loss: 1.0221770 Test Loss: 0.4434136
Validation loss decreased (1.024146 --> 1.022177).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.7151100635528564
Epoch: 17, Steps: 65 | Train Loss: 0.4304347 Vali Loss: 1.0211213 Test Loss: 0.4427276
Validation loss decreased (1.022177 --> 1.021121).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7463362216949463
Epoch: 18, Steps: 65 | Train Loss: 0.4300603 Vali Loss: 1.0193419 Test Loss: 0.4422126
Validation loss decreased (1.021121 --> 1.019342).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5008716583251953
Epoch: 19, Steps: 65 | Train Loss: 0.4292453 Vali Loss: 1.0183214 Test Loss: 0.4417051
Validation loss decreased (1.019342 --> 1.018321).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9147584438323975
Epoch: 20, Steps: 65 | Train Loss: 0.4286898 Vali Loss: 1.0174345 Test Loss: 0.4413056
Validation loss decreased (1.018321 --> 1.017434).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.0320472717285156
Epoch: 21, Steps: 65 | Train Loss: 0.4283589 Vali Loss: 1.0164754 Test Loss: 0.4410116
Validation loss decreased (1.017434 --> 1.016475).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.679286956787109
Epoch: 22, Steps: 65 | Train Loss: 0.4278662 Vali Loss: 1.0157354 Test Loss: 0.4406500
Validation loss decreased (1.016475 --> 1.015735).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.9683549404144287
Epoch: 23, Steps: 65 | Train Loss: 0.4277020 Vali Loss: 1.0145530 Test Loss: 0.4403379
Validation loss decreased (1.015735 --> 1.014553).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.318552255630493
Epoch: 24, Steps: 65 | Train Loss: 0.4273270 Vali Loss: 1.0143793 Test Loss: 0.4401856
Validation loss decreased (1.014553 --> 1.014379).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.011305332183838
Epoch: 25, Steps: 65 | Train Loss: 0.4274150 Vali Loss: 1.0134159 Test Loss: 0.4399858
Validation loss decreased (1.014379 --> 1.013416).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.663818359375
Epoch: 26, Steps: 65 | Train Loss: 0.4267223 Vali Loss: 1.0128088 Test Loss: 0.4398019
Validation loss decreased (1.013416 --> 1.012809).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.1421761512756348
Epoch: 27, Steps: 65 | Train Loss: 0.4263241 Vali Loss: 1.0127561 Test Loss: 0.4395748
Validation loss decreased (1.012809 --> 1.012756).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.1470611095428467
Epoch: 28, Steps: 65 | Train Loss: 0.4262317 Vali Loss: 1.0121229 Test Loss: 0.4394459
Validation loss decreased (1.012756 --> 1.012123).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.269473075866699
Epoch: 29, Steps: 65 | Train Loss: 0.4261894 Vali Loss: 1.0120007 Test Loss: 0.4393021
Validation loss decreased (1.012123 --> 1.012001).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.839613914489746
Epoch: 30, Steps: 65 | Train Loss: 0.4257545 Vali Loss: 1.0113748 Test Loss: 0.4392246
Validation loss decreased (1.012001 --> 1.011375).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.858163833618164
Epoch: 31, Steps: 65 | Train Loss: 0.4257812 Vali Loss: 1.0110865 Test Loss: 0.4390902
Validation loss decreased (1.011375 --> 1.011086).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.3261749744415283
Epoch: 32, Steps: 65 | Train Loss: 0.4254137 Vali Loss: 1.0107650 Test Loss: 0.4389861
Validation loss decreased (1.011086 --> 1.010765).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.5744388103485107
Epoch: 33, Steps: 65 | Train Loss: 0.4255672 Vali Loss: 1.0103395 Test Loss: 0.4388884
Validation loss decreased (1.010765 --> 1.010339).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.4219179153442383
Epoch: 34, Steps: 65 | Train Loss: 0.4255831 Vali Loss: 1.0103772 Test Loss: 0.4388425
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.5562479496002197
Epoch: 35, Steps: 65 | Train Loss: 0.4252209 Vali Loss: 1.0097225 Test Loss: 0.4387579
Validation loss decreased (1.010339 --> 1.009722).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.9864068031311035
Epoch: 36, Steps: 65 | Train Loss: 0.4252932 Vali Loss: 1.0099493 Test Loss: 0.4387255
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.5209221839904785
Epoch: 37, Steps: 65 | Train Loss: 0.4250518 Vali Loss: 1.0094041 Test Loss: 0.4386521
Validation loss decreased (1.009722 --> 1.009404).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.630305290222168
Epoch: 38, Steps: 65 | Train Loss: 0.4252569 Vali Loss: 1.0091993 Test Loss: 0.4386533
Validation loss decreased (1.009404 --> 1.009199).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.936347246170044
Epoch: 39, Steps: 65 | Train Loss: 0.4249384 Vali Loss: 1.0090717 Test Loss: 0.4386312
Validation loss decreased (1.009199 --> 1.009072).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.762389659881592
Epoch: 40, Steps: 65 | Train Loss: 0.4252500 Vali Loss: 1.0086782 Test Loss: 0.4385630
Validation loss decreased (1.009072 --> 1.008678).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.5378048419952393
Epoch: 41, Steps: 65 | Train Loss: 0.4249923 Vali Loss: 1.0089648 Test Loss: 0.4385459
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.7954299449920654
Epoch: 42, Steps: 65 | Train Loss: 0.4248649 Vali Loss: 1.0088626 Test Loss: 0.4385209
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.555415391921997
Epoch: 43, Steps: 65 | Train Loss: 0.4246312 Vali Loss: 1.0088372 Test Loss: 0.4384550
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.9393208026885986
Epoch: 44, Steps: 65 | Train Loss: 0.4244751 Vali Loss: 1.0087440 Test Loss: 0.4384516
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.6854703426361084
Epoch: 45, Steps: 65 | Train Loss: 0.4248188 Vali Loss: 1.0085479 Test Loss: 0.4384349
Validation loss decreased (1.008678 --> 1.008548).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6696045398712158
Epoch: 46, Steps: 65 | Train Loss: 0.4249490 Vali Loss: 1.0083873 Test Loss: 0.4384007
Validation loss decreased (1.008548 --> 1.008387).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.7818107604980469
Epoch: 47, Steps: 65 | Train Loss: 0.4243777 Vali Loss: 1.0083352 Test Loss: 0.4383825
Validation loss decreased (1.008387 --> 1.008335).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.3967444896698
Epoch: 48, Steps: 65 | Train Loss: 0.4243676 Vali Loss: 1.0081568 Test Loss: 0.4383698
Validation loss decreased (1.008335 --> 1.008157).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.686471700668335
Epoch: 49, Steps: 65 | Train Loss: 0.4244313 Vali Loss: 1.0081223 Test Loss: 0.4383569
Validation loss decreased (1.008157 --> 1.008122).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.105135679244995
Epoch: 50, Steps: 65 | Train Loss: 0.4245492 Vali Loss: 1.0078616 Test Loss: 0.4383448
Validation loss decreased (1.008122 --> 1.007862).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.7343950271606445
Epoch: 51, Steps: 65 | Train Loss: 0.4242600 Vali Loss: 1.0081092 Test Loss: 0.4383259
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.6028940677642822
Epoch: 52, Steps: 65 | Train Loss: 0.4245021 Vali Loss: 1.0079466 Test Loss: 0.4383341
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.841660499572754
Epoch: 53, Steps: 65 | Train Loss: 0.4245170 Vali Loss: 1.0073195 Test Loss: 0.4383143
Validation loss decreased (1.007862 --> 1.007319).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.94921875
Epoch: 54, Steps: 65 | Train Loss: 0.4243633 Vali Loss: 1.0075164 Test Loss: 0.4382985
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.6369988918304443
Epoch: 55, Steps: 65 | Train Loss: 0.4241446 Vali Loss: 1.0078433 Test Loss: 0.4383247
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.8086740970611572
Epoch: 56, Steps: 65 | Train Loss: 0.4243689 Vali Loss: 1.0077673 Test Loss: 0.4383022
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.007370948791504
Epoch: 57, Steps: 65 | Train Loss: 0.4244454 Vali Loss: 1.0076841 Test Loss: 0.4382892
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.8845207691192627
Epoch: 58, Steps: 65 | Train Loss: 0.4244035 Vali Loss: 1.0073496 Test Loss: 0.4382780
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.421544313430786
Epoch: 59, Steps: 65 | Train Loss: 0.4242351 Vali Loss: 1.0071509 Test Loss: 0.4382778
Validation loss decreased (1.007319 --> 1.007151).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.221863269805908
Epoch: 60, Steps: 65 | Train Loss: 0.4243479 Vali Loss: 1.0076097 Test Loss: 0.4382701
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.7693514823913574
Epoch: 61, Steps: 65 | Train Loss: 0.4242712 Vali Loss: 1.0073893 Test Loss: 0.4382854
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.740940809249878
Epoch: 62, Steps: 65 | Train Loss: 0.4239537 Vali Loss: 1.0074587 Test Loss: 0.4382914
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.2357029914855957
Epoch: 63, Steps: 65 | Train Loss: 0.4241819 Vali Loss: 1.0074501 Test Loss: 0.4382742
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.5190660953521729
Epoch: 64, Steps: 65 | Train Loss: 0.4242193 Vali Loss: 1.0074862 Test Loss: 0.4382658
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.2991161346435547
Epoch: 65, Steps: 65 | Train Loss: 0.4242374 Vali Loss: 1.0063425 Test Loss: 0.4382725
Validation loss decreased (1.007151 --> 1.006343).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.6541712284088135
Epoch: 66, Steps: 65 | Train Loss: 0.4242626 Vali Loss: 1.0072645 Test Loss: 0.4382747
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.339318037033081
Epoch: 67, Steps: 65 | Train Loss: 0.4241873 Vali Loss: 1.0070387 Test Loss: 0.4382673
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.16849946975708
Epoch: 68, Steps: 65 | Train Loss: 0.4241139 Vali Loss: 1.0069191 Test Loss: 0.4382618
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.7457287311553955
Epoch: 69, Steps: 65 | Train Loss: 0.4242050 Vali Loss: 1.0070959 Test Loss: 0.4382561
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.8169517517089844
Epoch: 70, Steps: 65 | Train Loss: 0.4241454 Vali Loss: 1.0068641 Test Loss: 0.4382626
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.180697202682495
Epoch: 71, Steps: 65 | Train Loss: 0.4242100 Vali Loss: 1.0068057 Test Loss: 0.4382556
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.8390355110168457
Epoch: 72, Steps: 65 | Train Loss: 0.4241552 Vali Loss: 1.0072855 Test Loss: 0.4382662
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.592327117919922
Epoch: 73, Steps: 65 | Train Loss: 0.4236782 Vali Loss: 1.0072385 Test Loss: 0.4382715
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 4.025252342224121
Epoch: 74, Steps: 65 | Train Loss: 0.4243040 Vali Loss: 1.0062354 Test Loss: 0.4382629
Validation loss decreased (1.006343 --> 1.006235).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.89723539352417
Epoch: 75, Steps: 65 | Train Loss: 0.4239318 Vali Loss: 1.0069463 Test Loss: 0.4382629
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.083289623260498
Epoch: 76, Steps: 65 | Train Loss: 0.4238523 Vali Loss: 1.0070018 Test Loss: 0.4382749
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.9600155353546143
Epoch: 77, Steps: 65 | Train Loss: 0.4241146 Vali Loss: 1.0071734 Test Loss: 0.4382651
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.8326809406280518
Epoch: 78, Steps: 65 | Train Loss: 0.4239860 Vali Loss: 1.0070578 Test Loss: 0.4382644
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 4.561370134353638
Epoch: 79, Steps: 65 | Train Loss: 0.4239235 Vali Loss: 1.0067543 Test Loss: 0.4382685
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.3759171962738037
Epoch: 80, Steps: 65 | Train Loss: 0.4234504 Vali Loss: 1.0067078 Test Loss: 0.4382719
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.9741604328155518
Epoch: 81, Steps: 65 | Train Loss: 0.4239053 Vali Loss: 1.0071447 Test Loss: 0.4382745
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.0660228729248047
Epoch: 82, Steps: 65 | Train Loss: 0.4239792 Vali Loss: 1.0070803 Test Loss: 0.4382674
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.315720558166504
Epoch: 83, Steps: 65 | Train Loss: 0.4238625 Vali Loss: 1.0065898 Test Loss: 0.4382689
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.403902053833008
Epoch: 84, Steps: 65 | Train Loss: 0.4240574 Vali Loss: 1.0070788 Test Loss: 0.4382711
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.816859722137451
Epoch: 85, Steps: 65 | Train Loss: 0.4242202 Vali Loss: 1.0065143 Test Loss: 0.4382688
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 3.1869373321533203
Epoch: 86, Steps: 65 | Train Loss: 0.4237068 Vali Loss: 1.0068516 Test Loss: 0.4382718
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.876798629760742
Epoch: 87, Steps: 65 | Train Loss: 0.4239390 Vali Loss: 1.0067724 Test Loss: 0.4382711
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 4.210446119308472
Epoch: 88, Steps: 65 | Train Loss: 0.4241808 Vali Loss: 1.0069658 Test Loss: 0.4382730
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.842874050140381
Epoch: 89, Steps: 65 | Train Loss: 0.4240690 Vali Loss: 1.0062183 Test Loss: 0.4382735
Validation loss decreased (1.006235 --> 1.006218).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.711310625076294
Epoch: 90, Steps: 65 | Train Loss: 0.4241948 Vali Loss: 1.0070283 Test Loss: 0.4382743
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.9737014770507812
Epoch: 91, Steps: 65 | Train Loss: 0.4239515 Vali Loss: 1.0064299 Test Loss: 0.4382721
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 3.6781065464019775
Epoch: 92, Steps: 65 | Train Loss: 0.4242168 Vali Loss: 1.0068146 Test Loss: 0.4382772
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 3.383060932159424
Epoch: 93, Steps: 65 | Train Loss: 0.4242835 Vali Loss: 1.0069969 Test Loss: 0.4382742
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.9938220977783203
Epoch: 94, Steps: 65 | Train Loss: 0.4239927 Vali Loss: 1.0066172 Test Loss: 0.4382752
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.1246609687805176
Epoch: 95, Steps: 65 | Train Loss: 0.4243724 Vali Loss: 1.0069915 Test Loss: 0.4382762
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.922372579574585
Epoch: 96, Steps: 65 | Train Loss: 0.4239927 Vali Loss: 1.0068287 Test Loss: 0.4382775
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.9455430507659912
Epoch: 97, Steps: 65 | Train Loss: 0.4239423 Vali Loss: 1.0065647 Test Loss: 0.4382753
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.7614777088165283
Epoch: 98, Steps: 65 | Train Loss: 0.4237552 Vali Loss: 1.0067179 Test Loss: 0.4382786
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.3149404525756836
Epoch: 99, Steps: 65 | Train Loss: 0.4240130 Vali Loss: 1.0069449 Test Loss: 0.4382808
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 3.3507418632507324
Epoch: 100, Steps: 65 | Train Loss: 0.4240196 Vali Loss: 1.0058073 Test Loss: 0.4382806
Validation loss decreased (1.006218 --> 1.005807).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_90_192_FITS_ETTh1_ftM_sl90_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4387458860874176, mae:0.42467761039733887, rse:0.6290191411972046, corr:[0.2629628  0.26512805 0.26382527 0.26394346 0.26125544 0.2587863
 0.2588283  0.25847387 0.2575351  0.25746608 0.2577251  0.25702685
 0.25619808 0.25621638 0.25617382 0.25613004 0.256478   0.25666255
 0.25676104 0.25676623 0.25643387 0.25620207 0.2554994  0.25468278
 0.25333863 0.2526422  0.25266963 0.25314194 0.25306216 0.25294542
 0.25337678 0.25336486 0.25293934 0.25265893 0.2527525  0.25271624
 0.25236434 0.25246245 0.25278336 0.25283015 0.2530639  0.25351226
 0.25394976 0.25397667 0.25373697 0.25370985 0.2536226  0.25284436
 0.2513921  0.25046465 0.24986562 0.24919192 0.24807012 0.24715498
 0.24714337 0.24681081 0.24648157 0.24649522 0.24631058 0.24615957
 0.24595352 0.24597524 0.24595088 0.24563585 0.2457184  0.24602695
 0.24643026 0.24662118 0.24633968 0.24603459 0.24576955 0.24480739
 0.24302115 0.24194992 0.24139829 0.24107471 0.240578   0.24031252
 0.24066165 0.24047962 0.24020165 0.23998284 0.23974459 0.23962
 0.23946102 0.2394664  0.23973934 0.23967342 0.23959629 0.23980935
 0.2400146  0.24000348 0.23976302 0.23974149 0.23991503 0.23943438
 0.23804675 0.23739345 0.23736751 0.23698837 0.23668262 0.23670818
 0.23707782 0.23694628 0.23671661 0.23662598 0.23642012 0.23632519
 0.23622152 0.23603289 0.2362126  0.23632166 0.23645619 0.23660943
 0.23686354 0.23703586 0.23698904 0.23679869 0.23650555 0.23564251
 0.23379098 0.23240034 0.23162505 0.23081724 0.23023227 0.23021665
 0.23092283 0.23114985 0.23116453 0.23100248 0.23055984 0.23048933
 0.23052335 0.23022692 0.23011552 0.23011646 0.23028204 0.23042911
 0.2307448  0.23100941 0.23086528 0.23068851 0.23054515 0.22982559
 0.22799632 0.22657596 0.22610971 0.22542499 0.22461852 0.22462322
 0.22564057 0.22578366 0.22582155 0.22593383 0.22565903 0.22546145
 0.22536275 0.22504453 0.2247296  0.22464293 0.22474405 0.22478929
 0.22509615 0.22538406 0.22517721 0.22482634 0.22460191 0.22395572
 0.22215302 0.22068112 0.22060487 0.2208176  0.2199382  0.2195434
 0.22070348 0.22100864 0.22074236 0.2207546  0.22091553 0.2209953
 0.22097747 0.2203821  0.21980606 0.21963698 0.21981369 0.2196712
 0.21950315 0.22015716 0.22011654 0.21837762 0.2191863  0.2207759 ]
