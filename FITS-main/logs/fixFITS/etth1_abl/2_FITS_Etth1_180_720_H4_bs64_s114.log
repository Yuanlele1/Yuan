Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=210, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7902720.0
params:  9030.0
Trainable parameters:  9030
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.6177897453308105
Epoch: 1, Steps: 60 | Train Loss: 1.1696807 Vali Loss: 2.5097861 Test Loss: 1.1838386
Validation loss decreased (inf --> 2.509786).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9061980247497559
Epoch: 2, Steps: 60 | Train Loss: 0.8803816 Vali Loss: 2.1199031 Test Loss: 0.8995491
Validation loss decreased (2.509786 --> 2.119903).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.9171974658966064
Epoch: 3, Steps: 60 | Train Loss: 0.7328322 Vali Loss: 1.9161825 Test Loss: 0.7504525
Validation loss decreased (2.119903 --> 1.916183).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.7229056358337402
Epoch: 4, Steps: 60 | Train Loss: 0.6504188 Vali Loss: 1.7928448 Test Loss: 0.6619568
Validation loss decreased (1.916183 --> 1.792845).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.702509880065918
Epoch: 5, Steps: 60 | Train Loss: 0.6004792 Vali Loss: 1.7223169 Test Loss: 0.6060854
Validation loss decreased (1.792845 --> 1.722317).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.49078106880188
Epoch: 6, Steps: 60 | Train Loss: 0.5692711 Vali Loss: 1.6810197 Test Loss: 0.5699204
Validation loss decreased (1.722317 --> 1.681020).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.6327626705169678
Epoch: 7, Steps: 60 | Train Loss: 0.5495253 Vali Loss: 1.6424603 Test Loss: 0.5456407
Validation loss decreased (1.681020 --> 1.642460).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.7963030338287354
Epoch: 8, Steps: 60 | Train Loss: 0.5356415 Vali Loss: 1.6126757 Test Loss: 0.5284629
Validation loss decreased (1.642460 --> 1.612676).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.623859405517578
Epoch: 9, Steps: 60 | Train Loss: 0.5260252 Vali Loss: 1.6011686 Test Loss: 0.5164647
Validation loss decreased (1.612676 --> 1.601169).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.038444757461548
Epoch: 10, Steps: 60 | Train Loss: 0.5194943 Vali Loss: 1.5886185 Test Loss: 0.5074732
Validation loss decreased (1.601169 --> 1.588619).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9843988418579102
Epoch: 11, Steps: 60 | Train Loss: 0.5141182 Vali Loss: 1.5727799 Test Loss: 0.5005919
Validation loss decreased (1.588619 --> 1.572780).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.2885820865631104
Epoch: 12, Steps: 60 | Train Loss: 0.5098349 Vali Loss: 1.5708256 Test Loss: 0.4952486
Validation loss decreased (1.572780 --> 1.570826).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.22914719581604
Epoch: 13, Steps: 60 | Train Loss: 0.5068090 Vali Loss: 1.5626624 Test Loss: 0.4908637
Validation loss decreased (1.570826 --> 1.562662).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.136728286743164
Epoch: 14, Steps: 60 | Train Loss: 0.5047630 Vali Loss: 1.5501715 Test Loss: 0.4872649
Validation loss decreased (1.562662 --> 1.550171).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.7189009189605713
Epoch: 15, Steps: 60 | Train Loss: 0.5026866 Vali Loss: 1.5523608 Test Loss: 0.4843242
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.9541561603546143
Epoch: 16, Steps: 60 | Train Loss: 0.5002647 Vali Loss: 1.5587468 Test Loss: 0.4816417
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.2787888050079346
Epoch: 17, Steps: 60 | Train Loss: 0.4985315 Vali Loss: 1.5486182 Test Loss: 0.4793542
Validation loss decreased (1.550171 --> 1.548618).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.132755756378174
Epoch: 18, Steps: 60 | Train Loss: 0.4973728 Vali Loss: 1.5482378 Test Loss: 0.4773531
Validation loss decreased (1.548618 --> 1.548238).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.012284517288208
Epoch: 19, Steps: 60 | Train Loss: 0.4963352 Vali Loss: 1.5399406 Test Loss: 0.4755891
Validation loss decreased (1.548238 --> 1.539941).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.6828043460845947
Epoch: 20, Steps: 60 | Train Loss: 0.4951417 Vali Loss: 1.5471462 Test Loss: 0.4739783
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.9531385898590088
Epoch: 21, Steps: 60 | Train Loss: 0.4941044 Vali Loss: 1.5359151 Test Loss: 0.4725544
Validation loss decreased (1.539941 --> 1.535915).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.60429048538208
Epoch: 22, Steps: 60 | Train Loss: 0.4934371 Vali Loss: 1.5390055 Test Loss: 0.4712597
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.3334007263183594
Epoch: 23, Steps: 60 | Train Loss: 0.4924701 Vali Loss: 1.5320532 Test Loss: 0.4700727
Validation loss decreased (1.535915 --> 1.532053).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.349842071533203
Epoch: 24, Steps: 60 | Train Loss: 0.4917372 Vali Loss: 1.5331054 Test Loss: 0.4689720
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9250986576080322
Epoch: 25, Steps: 60 | Train Loss: 0.4912782 Vali Loss: 1.5423512 Test Loss: 0.4680049
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.957857608795166
Epoch: 26, Steps: 60 | Train Loss: 0.4903221 Vali Loss: 1.5277886 Test Loss: 0.4670308
Validation loss decreased (1.532053 --> 1.527789).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.0584299564361572
Epoch: 27, Steps: 60 | Train Loss: 0.4900204 Vali Loss: 1.5231082 Test Loss: 0.4661610
Validation loss decreased (1.527789 --> 1.523108).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.7337477207183838
Epoch: 28, Steps: 60 | Train Loss: 0.4896323 Vali Loss: 1.5287943 Test Loss: 0.4654120
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.8640494346618652
Epoch: 29, Steps: 60 | Train Loss: 0.4888159 Vali Loss: 1.5172009 Test Loss: 0.4646621
Validation loss decreased (1.523108 --> 1.517201).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.725811004638672
Epoch: 30, Steps: 60 | Train Loss: 0.4881953 Vali Loss: 1.5292175 Test Loss: 0.4640148
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.7457613945007324
Epoch: 31, Steps: 60 | Train Loss: 0.4878833 Vali Loss: 1.5253808 Test Loss: 0.4633715
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.9436726570129395
Epoch: 32, Steps: 60 | Train Loss: 0.4877305 Vali Loss: 1.5264952 Test Loss: 0.4627863
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.2643380165100098
Epoch: 33, Steps: 60 | Train Loss: 0.4874625 Vali Loss: 1.5285938 Test Loss: 0.4622505
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.140469789505005
Epoch: 34, Steps: 60 | Train Loss: 0.4867395 Vali Loss: 1.5259473 Test Loss: 0.4617338
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.892099618911743
Epoch: 35, Steps: 60 | Train Loss: 0.4865314 Vali Loss: 1.5199637 Test Loss: 0.4612513
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.968763589859009
Epoch: 36, Steps: 60 | Train Loss: 0.4863103 Vali Loss: 1.5240916 Test Loss: 0.4607965
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8287665843963623
Epoch: 37, Steps: 60 | Train Loss: 0.4857502 Vali Loss: 1.5233407 Test Loss: 0.4603943
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6296961307525635
Epoch: 38, Steps: 60 | Train Loss: 0.4859067 Vali Loss: 1.5201988 Test Loss: 0.4599878
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.2761390209198
Epoch: 39, Steps: 60 | Train Loss: 0.4852553 Vali Loss: 1.5286705 Test Loss: 0.4596242
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.27101469039917
Epoch: 40, Steps: 60 | Train Loss: 0.4853955 Vali Loss: 1.5194322 Test Loss: 0.4592696
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.263321876525879
Epoch: 41, Steps: 60 | Train Loss: 0.4849068 Vali Loss: 1.5196285 Test Loss: 0.4589548
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.682290554046631
Epoch: 42, Steps: 60 | Train Loss: 0.4846531 Vali Loss: 1.5231274 Test Loss: 0.4586385
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.217822313308716
Epoch: 43, Steps: 60 | Train Loss: 0.4845756 Vali Loss: 1.5178179 Test Loss: 0.4583569
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.5319297313690186
Epoch: 44, Steps: 60 | Train Loss: 0.4842975 Vali Loss: 1.5142949 Test Loss: 0.4580842
Validation loss decreased (1.517201 --> 1.514295).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.5923404693603516
Epoch: 45, Steps: 60 | Train Loss: 0.4842136 Vali Loss: 1.5162700 Test Loss: 0.4578181
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.5357842445373535
Epoch: 46, Steps: 60 | Train Loss: 0.4840807 Vali Loss: 1.5117993 Test Loss: 0.4575846
Validation loss decreased (1.514295 --> 1.511799).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.39377760887146
Epoch: 47, Steps: 60 | Train Loss: 0.4839732 Vali Loss: 1.5148419 Test Loss: 0.4573512
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.3081750869750977
Epoch: 48, Steps: 60 | Train Loss: 0.4840263 Vali Loss: 1.5170629 Test Loss: 0.4571494
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.1263725757598877
Epoch: 49, Steps: 60 | Train Loss: 0.4841150 Vali Loss: 1.5162252 Test Loss: 0.4569370
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.4672727584838867
Epoch: 50, Steps: 60 | Train Loss: 0.4837134 Vali Loss: 1.5191031 Test Loss: 0.4567577
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.127704381942749
Epoch: 51, Steps: 60 | Train Loss: 0.4836693 Vali Loss: 1.5131695 Test Loss: 0.4565741
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.7309374809265137
Epoch: 52, Steps: 60 | Train Loss: 0.4832909 Vali Loss: 1.5109173 Test Loss: 0.4563950
Validation loss decreased (1.511799 --> 1.510917).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.068833827972412
Epoch: 53, Steps: 60 | Train Loss: 0.4834206 Vali Loss: 1.5180786 Test Loss: 0.4562321
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9693636894226074
Epoch: 54, Steps: 60 | Train Loss: 0.4829850 Vali Loss: 1.5195885 Test Loss: 0.4560807
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.350313425064087
Epoch: 55, Steps: 60 | Train Loss: 0.4831189 Vali Loss: 1.5180321 Test Loss: 0.4559385
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.982058048248291
Epoch: 56, Steps: 60 | Train Loss: 0.4828749 Vali Loss: 1.5140115 Test Loss: 0.4557817
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.868699312210083
Epoch: 57, Steps: 60 | Train Loss: 0.4827512 Vali Loss: 1.5138148 Test Loss: 0.4556580
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.001065731048584
Epoch: 58, Steps: 60 | Train Loss: 0.4825094 Vali Loss: 1.5159297 Test Loss: 0.4555481
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.068648338317871
Epoch: 59, Steps: 60 | Train Loss: 0.4825903 Vali Loss: 1.5114145 Test Loss: 0.4554227
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.516676902770996
Epoch: 60, Steps: 60 | Train Loss: 0.4824755 Vali Loss: 1.5107574 Test Loss: 0.4553084
Validation loss decreased (1.510917 --> 1.510757).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.551074504852295
Epoch: 61, Steps: 60 | Train Loss: 0.4823348 Vali Loss: 1.5106099 Test Loss: 0.4552044
Validation loss decreased (1.510757 --> 1.510610).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.470491409301758
Epoch: 62, Steps: 60 | Train Loss: 0.4823697 Vali Loss: 1.5097754 Test Loss: 0.4551079
Validation loss decreased (1.510610 --> 1.509775).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.3930745124816895
Epoch: 63, Steps: 60 | Train Loss: 0.4824792 Vali Loss: 1.5128455 Test Loss: 0.4550131
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.932051420211792
Epoch: 64, Steps: 60 | Train Loss: 0.4820544 Vali Loss: 1.5104966 Test Loss: 0.4549249
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.4275686740875244
Epoch: 65, Steps: 60 | Train Loss: 0.4823328 Vali Loss: 1.5111769 Test Loss: 0.4548389
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.4754788875579834
Epoch: 66, Steps: 60 | Train Loss: 0.4820504 Vali Loss: 1.5100266 Test Loss: 0.4547589
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.0401554107666016
Epoch: 67, Steps: 60 | Train Loss: 0.4826274 Vali Loss: 1.5085120 Test Loss: 0.4546854
Validation loss decreased (1.509775 --> 1.508512).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 3.2366607189178467
Epoch: 68, Steps: 60 | Train Loss: 0.4820107 Vali Loss: 1.5103161 Test Loss: 0.4546117
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.784769058227539
Epoch: 69, Steps: 60 | Train Loss: 0.4823791 Vali Loss: 1.5063146 Test Loss: 0.4545487
Validation loss decreased (1.508512 --> 1.506315).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9847514629364014
Epoch: 70, Steps: 60 | Train Loss: 0.4813442 Vali Loss: 1.5141919 Test Loss: 0.4544775
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.059786081314087
Epoch: 71, Steps: 60 | Train Loss: 0.4816493 Vali Loss: 1.5131433 Test Loss: 0.4544176
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.2419209480285645
Epoch: 72, Steps: 60 | Train Loss: 0.4819213 Vali Loss: 1.5133797 Test Loss: 0.4543604
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.2075326442718506
Epoch: 73, Steps: 60 | Train Loss: 0.4822043 Vali Loss: 1.5160112 Test Loss: 0.4543068
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.069209337234497
Epoch: 74, Steps: 60 | Train Loss: 0.4816728 Vali Loss: 1.5150459 Test Loss: 0.4542516
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.471935510635376
Epoch: 75, Steps: 60 | Train Loss: 0.4821624 Vali Loss: 1.5089794 Test Loss: 0.4541996
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.8900020122528076
Epoch: 76, Steps: 60 | Train Loss: 0.4819574 Vali Loss: 1.5103663 Test Loss: 0.4541512
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.2565712928771973
Epoch: 77, Steps: 60 | Train Loss: 0.4813947 Vali Loss: 1.5075932 Test Loss: 0.4541094
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.159623384475708
Epoch: 78, Steps: 60 | Train Loss: 0.4817593 Vali Loss: 1.5144935 Test Loss: 0.4540639
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.421449899673462
Epoch: 79, Steps: 60 | Train Loss: 0.4817323 Vali Loss: 1.5132084 Test Loss: 0.4540217
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.68324875831604
Epoch: 80, Steps: 60 | Train Loss: 0.4816338 Vali Loss: 1.5122291 Test Loss: 0.4539824
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.495252847671509
Epoch: 81, Steps: 60 | Train Loss: 0.4815465 Vali Loss: 1.5142019 Test Loss: 0.4539451
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.6139466762542725
Epoch: 82, Steps: 60 | Train Loss: 0.4816561 Vali Loss: 1.5108616 Test Loss: 0.4539100
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.1088335514068604
Epoch: 83, Steps: 60 | Train Loss: 0.4816250 Vali Loss: 1.5136266 Test Loss: 0.4538767
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.517774820327759
Epoch: 84, Steps: 60 | Train Loss: 0.4816748 Vali Loss: 1.5105129 Test Loss: 0.4538450
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.063502311706543
Epoch: 85, Steps: 60 | Train Loss: 0.4814075 Vali Loss: 1.5161066 Test Loss: 0.4538182
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.9440414905548096
Epoch: 86, Steps: 60 | Train Loss: 0.4813789 Vali Loss: 1.5139898 Test Loss: 0.4537885
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.170440673828125
Epoch: 87, Steps: 60 | Train Loss: 0.4814850 Vali Loss: 1.5069950 Test Loss: 0.4537613
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.74550199508667
Epoch: 88, Steps: 60 | Train Loss: 0.4813187 Vali Loss: 1.5102000 Test Loss: 0.4537351
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.538536787033081
Epoch: 89, Steps: 60 | Train Loss: 0.4815608 Vali Loss: 1.5075343 Test Loss: 0.4537096
EarlyStopping counter: 20 out of 20
Early stopping
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=210, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7902720.0
params:  9030.0
Trainable parameters:  9030
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.139355182647705
Epoch: 1, Steps: 60 | Train Loss: 0.5924309 Vali Loss: 1.5047009 Test Loss: 0.4488475
Validation loss decreased (inf --> 1.504701).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.832626581192017
Epoch: 2, Steps: 60 | Train Loss: 0.5898288 Vali Loss: 1.4997213 Test Loss: 0.4454054
Validation loss decreased (1.504701 --> 1.499721).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.328578233718872
Epoch: 3, Steps: 60 | Train Loss: 0.5885556 Vali Loss: 1.4953829 Test Loss: 0.4435962
Validation loss decreased (1.499721 --> 1.495383).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.183074474334717
Epoch: 4, Steps: 60 | Train Loss: 0.5875700 Vali Loss: 1.4974194 Test Loss: 0.4431203
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.898175001144409
Epoch: 5, Steps: 60 | Train Loss: 0.5864453 Vali Loss: 1.4958674 Test Loss: 0.4423452
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.456334114074707
Epoch: 6, Steps: 60 | Train Loss: 0.5865122 Vali Loss: 1.4925805 Test Loss: 0.4424227
Validation loss decreased (1.495383 --> 1.492581).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.4237613677978516
Epoch: 7, Steps: 60 | Train Loss: 0.5865002 Vali Loss: 1.4931049 Test Loss: 0.4423379
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.0302445888519287
Epoch: 8, Steps: 60 | Train Loss: 0.5857607 Vali Loss: 1.4894702 Test Loss: 0.4425080
Validation loss decreased (1.492581 --> 1.489470).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.544881820678711
Epoch: 9, Steps: 60 | Train Loss: 0.5853380 Vali Loss: 1.4946404 Test Loss: 0.4423968
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.958280086517334
Epoch: 10, Steps: 60 | Train Loss: 0.5859239 Vali Loss: 1.4902064 Test Loss: 0.4426300
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.262497663497925
Epoch: 11, Steps: 60 | Train Loss: 0.5855390 Vali Loss: 1.4974440 Test Loss: 0.4426114
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.076154947280884
Epoch: 12, Steps: 60 | Train Loss: 0.5850599 Vali Loss: 1.4958000 Test Loss: 0.4427278
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.4117143154144287
Epoch: 13, Steps: 60 | Train Loss: 0.5854690 Vali Loss: 1.4962510 Test Loss: 0.4426643
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.4513328075408936
Epoch: 14, Steps: 60 | Train Loss: 0.5850542 Vali Loss: 1.4949791 Test Loss: 0.4427888
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.222564458847046
Epoch: 15, Steps: 60 | Train Loss: 0.5854575 Vali Loss: 1.4880711 Test Loss: 0.4428035
Validation loss decreased (1.489470 --> 1.488071).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.2327892780303955
Epoch: 16, Steps: 60 | Train Loss: 0.5851352 Vali Loss: 1.4941434 Test Loss: 0.4429278
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.8380279541015625
Epoch: 17, Steps: 60 | Train Loss: 0.5848066 Vali Loss: 1.4954840 Test Loss: 0.4429081
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.817786931991577
Epoch: 18, Steps: 60 | Train Loss: 0.5852416 Vali Loss: 1.4893608 Test Loss: 0.4429651
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9680907726287842
Epoch: 19, Steps: 60 | Train Loss: 0.5849630 Vali Loss: 1.4926982 Test Loss: 0.4429815
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.2913107872009277
Epoch: 20, Steps: 60 | Train Loss: 0.5851877 Vali Loss: 1.4921067 Test Loss: 0.4430166
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.963771104812622
Epoch: 21, Steps: 60 | Train Loss: 0.5853107 Vali Loss: 1.4973257 Test Loss: 0.4430902
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.135957956314087
Epoch: 22, Steps: 60 | Train Loss: 0.5855925 Vali Loss: 1.4954824 Test Loss: 0.4431403
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.9175355434417725
Epoch: 23, Steps: 60 | Train Loss: 0.5851184 Vali Loss: 1.4934750 Test Loss: 0.4431159
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.041071891784668
Epoch: 24, Steps: 60 | Train Loss: 0.5852653 Vali Loss: 1.4900880 Test Loss: 0.4431336
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.260141134262085
Epoch: 25, Steps: 60 | Train Loss: 0.5850687 Vali Loss: 1.4932182 Test Loss: 0.4431795
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.9833741188049316
Epoch: 26, Steps: 60 | Train Loss: 0.5849915 Vali Loss: 1.4961840 Test Loss: 0.4431575
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.6535019874572754
Epoch: 27, Steps: 60 | Train Loss: 0.5847359 Vali Loss: 1.4878592 Test Loss: 0.4431964
Validation loss decreased (1.488071 --> 1.487859).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.3221263885498047
Epoch: 28, Steps: 60 | Train Loss: 0.5848513 Vali Loss: 1.4920868 Test Loss: 0.4432458
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.7614455223083496
Epoch: 29, Steps: 60 | Train Loss: 0.5852260 Vali Loss: 1.4932396 Test Loss: 0.4432757
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.0774242877960205
Epoch: 30, Steps: 60 | Train Loss: 0.5851964 Vali Loss: 1.4868205 Test Loss: 0.4432628
Validation loss decreased (1.487859 --> 1.486820).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.266606092453003
Epoch: 31, Steps: 60 | Train Loss: 0.5848701 Vali Loss: 1.4840690 Test Loss: 0.4433560
Validation loss decreased (1.486820 --> 1.484069).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.7874326705932617
Epoch: 32, Steps: 60 | Train Loss: 0.5851027 Vali Loss: 1.4952590 Test Loss: 0.4433247
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.217111349105835
Epoch: 33, Steps: 60 | Train Loss: 0.5844558 Vali Loss: 1.4962257 Test Loss: 0.4433149
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6864521503448486
Epoch: 34, Steps: 60 | Train Loss: 0.5846376 Vali Loss: 1.4947306 Test Loss: 0.4433602
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9272394180297852
Epoch: 35, Steps: 60 | Train Loss: 0.5847286 Vali Loss: 1.4910280 Test Loss: 0.4433757
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.115893602371216
Epoch: 36, Steps: 60 | Train Loss: 0.5853934 Vali Loss: 1.4927599 Test Loss: 0.4433445
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.935286045074463
Epoch: 37, Steps: 60 | Train Loss: 0.5850415 Vali Loss: 1.4919945 Test Loss: 0.4433946
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.24403977394104
Epoch: 38, Steps: 60 | Train Loss: 0.5847798 Vali Loss: 1.4863408 Test Loss: 0.4434034
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.585659980773926
Epoch: 39, Steps: 60 | Train Loss: 0.5846894 Vali Loss: 1.4935708 Test Loss: 0.4433936
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.748257875442505
Epoch: 40, Steps: 60 | Train Loss: 0.5848212 Vali Loss: 1.4839811 Test Loss: 0.4434085
Validation loss decreased (1.484069 --> 1.483981).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.1826083660125732
Epoch: 41, Steps: 60 | Train Loss: 0.5850221 Vali Loss: 1.4905065 Test Loss: 0.4434192
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.5449795722961426
Epoch: 42, Steps: 60 | Train Loss: 0.5849685 Vali Loss: 1.4884053 Test Loss: 0.4434230
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.375802993774414
Epoch: 43, Steps: 60 | Train Loss: 0.5841451 Vali Loss: 1.4923935 Test Loss: 0.4434602
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.26236891746521
Epoch: 44, Steps: 60 | Train Loss: 0.5851705 Vali Loss: 1.4911568 Test Loss: 0.4434728
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9897828102111816
Epoch: 45, Steps: 60 | Train Loss: 0.5846658 Vali Loss: 1.4919164 Test Loss: 0.4434629
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.1308183670043945
Epoch: 46, Steps: 60 | Train Loss: 0.5843007 Vali Loss: 1.4933829 Test Loss: 0.4434585
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.820791006088257
Epoch: 47, Steps: 60 | Train Loss: 0.5844728 Vali Loss: 1.4917436 Test Loss: 0.4434755
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.8004109859466553
Epoch: 48, Steps: 60 | Train Loss: 0.5851295 Vali Loss: 1.4934609 Test Loss: 0.4434849
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.5723702907562256
Epoch: 49, Steps: 60 | Train Loss: 0.5851978 Vali Loss: 1.4905474 Test Loss: 0.4435256
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.2096469402313232
Epoch: 50, Steps: 60 | Train Loss: 0.5845643 Vali Loss: 1.4900914 Test Loss: 0.4435022
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.5637154579162598
Epoch: 51, Steps: 60 | Train Loss: 0.5848951 Vali Loss: 1.4974744 Test Loss: 0.4434885
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.83427095413208
Epoch: 52, Steps: 60 | Train Loss: 0.5851760 Vali Loss: 1.4971247 Test Loss: 0.4435414
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.0444531440734863
Epoch: 53, Steps: 60 | Train Loss: 0.5849429 Vali Loss: 1.4943916 Test Loss: 0.4435188
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.8013863563537598
Epoch: 54, Steps: 60 | Train Loss: 0.5847879 Vali Loss: 1.4900879 Test Loss: 0.4435464
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.182239294052124
Epoch: 55, Steps: 60 | Train Loss: 0.5846402 Vali Loss: 1.4906243 Test Loss: 0.4435456
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.841001033782959
Epoch: 56, Steps: 60 | Train Loss: 0.5853389 Vali Loss: 1.4872742 Test Loss: 0.4435405
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.4022157192230225
Epoch: 57, Steps: 60 | Train Loss: 0.5847724 Vali Loss: 1.4907954 Test Loss: 0.4435520
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.9549710750579834
Epoch: 58, Steps: 60 | Train Loss: 0.5847861 Vali Loss: 1.4980267 Test Loss: 0.4435472
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.3211793899536133
Epoch: 59, Steps: 60 | Train Loss: 0.5850724 Vali Loss: 1.4898138 Test Loss: 0.4435638
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.4078543186187744
Epoch: 60, Steps: 60 | Train Loss: 0.5849386 Vali Loss: 1.4953086 Test Loss: 0.4435555
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4422820508480072, mae:0.4514925479888916, rse:0.6366512775421143, corr:[0.22772458 0.23423736 0.23501058 0.23319326 0.23149307 0.22996843
 0.22926761 0.2301918  0.23093367 0.23102117 0.23020071 0.23011896
 0.2307835  0.23012921 0.22904928 0.22907683 0.22955444 0.22929582
 0.2284881  0.2280775  0.22836885 0.22896402 0.22943364 0.22938812
 0.22879076 0.22835697 0.22787884 0.22747166 0.22712573 0.22694828
 0.22659856 0.22606815 0.22564322 0.22573431 0.22587322 0.22595993
 0.22627166 0.22632748 0.22620381 0.22595388 0.22600006 0.22636312
 0.22658397 0.22644468 0.22651169 0.22705539 0.22795868 0.22820236
 0.22719187 0.22618426 0.2248953  0.22361785 0.22244686 0.22137506
 0.22075512 0.22062854 0.22054791 0.22072725 0.22061624 0.22105817
 0.22160688 0.22138357 0.22099365 0.22082287 0.22082323 0.22088932
 0.2209732  0.22097026 0.22099145 0.22102016 0.22102171 0.22054082
 0.21936615 0.21851444 0.21766004 0.21721394 0.21717654 0.21705078
 0.2169725  0.21693651 0.21675354 0.21665752 0.21611679 0.21592024
 0.21632524 0.21638349 0.21621779 0.21588595 0.21553193 0.21535897
 0.21513967 0.21487018 0.21497259 0.21554431 0.21645689 0.21711957
 0.21702242 0.21690196 0.21672378 0.216324   0.2160969  0.21600352
 0.21581997 0.21601091 0.21601687 0.21604127 0.21589649 0.2161408
 0.21658365 0.21642014 0.21604241 0.2158847  0.21593565 0.21597017
 0.21593587 0.21584232 0.21590243 0.21599218 0.21605985 0.21580017
 0.2150139  0.2143511  0.21326414 0.2122877  0.21163806 0.21141353
 0.21152753 0.21187592 0.21185985 0.21184482 0.21179274 0.21228048
 0.21304543 0.21314889 0.21303934 0.21289995 0.21276982 0.2127294
 0.21274965 0.21276754 0.21287023 0.21306993 0.21330251 0.21325195
 0.21270406 0.21203037 0.21116994 0.21005614 0.20934756 0.20899647
 0.20887876 0.20892125 0.20921047 0.2095724  0.20951635 0.20968464
 0.2102299  0.21022905 0.210002   0.20969407 0.20948009 0.2094722
 0.20946    0.2093293  0.20941053 0.20979352 0.21017522 0.2101489
 0.2096279  0.20940027 0.20911026 0.20872661 0.20847848 0.20833094
 0.2083851  0.20881104 0.20914705 0.20962344 0.20995024 0.2104341
 0.21109575 0.21116877 0.21097293 0.21078739 0.21072091 0.2108922
 0.21105386 0.21104725 0.21123733 0.2116015  0.21189392 0.21165201
 0.21070555 0.20998935 0.20917463 0.20828554 0.20757553 0.20709762
 0.2068948  0.20704837 0.20728438 0.20751996 0.20745523 0.20790178
 0.20862003 0.20881897 0.20865342 0.2083756  0.2081735  0.20806763
 0.20785111 0.20758373 0.20759912 0.20783213 0.20791607 0.20760585
 0.20694849 0.20667419 0.20634887 0.20580599 0.20539638 0.20502949
 0.20499438 0.20517175 0.2052926  0.20530084 0.20507775 0.20518368
 0.20568575 0.2057188  0.20557798 0.20545864 0.20524822 0.20501043
 0.20474246 0.20446436 0.2043834  0.20451526 0.2047249  0.20457917
 0.20401916 0.20369959 0.20338307 0.20316619 0.20309572 0.20299894
 0.20308687 0.20327073 0.2034614  0.20372838 0.20392534 0.20447524
 0.20520069 0.2054011  0.20544335 0.20534702 0.20523137 0.20523016
 0.20525466 0.20502445 0.20488845 0.20490885 0.20498708 0.20483889
 0.20429797 0.20399654 0.20357901 0.20316602 0.20298141 0.20281948
 0.20274615 0.20292388 0.20299955 0.20313169 0.20311077 0.20321399
 0.20383412 0.20405142 0.20390642 0.20355988 0.20324557 0.20317037
 0.2031404  0.2030557  0.20313354 0.20346487 0.20389628 0.20410219
 0.20393482 0.20392106 0.20397916 0.20400725 0.20405626 0.20425749
 0.2045691  0.20487422 0.20505589 0.20524584 0.20530061 0.20556736
 0.20608585 0.20622264 0.20626082 0.20625028 0.20614317 0.20607929
 0.2060985  0.20606302 0.20601681 0.20610853 0.20618017 0.2060502
 0.20564134 0.20550837 0.2053332  0.20494954 0.20465456 0.20419073
 0.20390485 0.20393763 0.20394357 0.20392883 0.20382275 0.20411153
 0.2048462  0.20510262 0.2051393  0.2052598  0.20535348 0.20539613
 0.20533039 0.20524448 0.20517744 0.20525567 0.20536987 0.20518263
 0.2046182  0.20437776 0.20406725 0.20367047 0.20330676 0.20306025
 0.20301832 0.20300858 0.2030325  0.2032583  0.2035985  0.204074
 0.20450252 0.20466629 0.20476897 0.20476344 0.20456035 0.20450333
 0.20454668 0.20455343 0.20460539 0.20474136 0.20453578 0.20415097
 0.20374756 0.20367649 0.20347407 0.20300288 0.20254995 0.20228443
 0.20226519 0.20220548 0.20195931 0.20158313 0.20121142 0.2011646
 0.20146213 0.20131816 0.20104401 0.20082529 0.20071523 0.20080364
 0.20106903 0.20132864 0.20149566 0.20187856 0.20238551 0.20302789
 0.20344128 0.20384373 0.20392133 0.20377256 0.20356695 0.20297861
 0.20242794 0.20216802 0.20201202 0.20202991 0.20186542 0.20219678
 0.20304412 0.20328648 0.20327847 0.2031206  0.20300424 0.2032796
 0.20368057 0.20404032 0.20439178 0.20483334 0.20519407 0.20538522
 0.20533313 0.20548595 0.20545731 0.20501363 0.2046899  0.20449993
 0.20460504 0.20491832 0.20513645 0.20519018 0.2052156  0.20550841
 0.20601955 0.20609735 0.20595919 0.20573992 0.20547724 0.20542301
 0.20555492 0.2058585  0.20613489 0.20674892 0.20742893 0.20786981
 0.20776105 0.20766534 0.20736474 0.20690355 0.20677438 0.20672128
 0.20669538 0.20686068 0.20702463 0.20725873 0.20728917 0.20768292
 0.20835485 0.20839293 0.20802549 0.2076838  0.2075856  0.20779169
 0.20797296 0.20799209 0.20812719 0.20857814 0.20912336 0.20958059
 0.20962027 0.20973611 0.20940736 0.20872529 0.20825402 0.20787029
 0.20758837 0.20745966 0.20755553 0.20793931 0.20841464 0.20911203
 0.20986241 0.21003622 0.21004277 0.2100064  0.20999843 0.21008794
 0.21015619 0.21022204 0.21031798 0.21037796 0.21056823 0.21052316
 0.20998181 0.2095635  0.2090369  0.20846872 0.20821851 0.20803231
 0.20807035 0.20827518 0.20852825 0.20884141 0.20886081 0.2090921
 0.20968942 0.20975672 0.20958818 0.20946352 0.20932983 0.20923615
 0.20922051 0.20939147 0.20983806 0.21052517 0.21126314 0.21176313
 0.2118536  0.21213055 0.21211027 0.21171205 0.21141754 0.21112582
 0.21085742 0.21056029 0.21042758 0.21026836 0.21011193 0.2104098
 0.21104771 0.21128191 0.21146947 0.21168165 0.21188265 0.212128
 0.21236257 0.21261783 0.21297765 0.21356565 0.21428482 0.21449855
 0.21387969 0.21345325 0.21306278 0.2125036  0.21182866 0.21113637
 0.21072653 0.21069673 0.210764   0.21090467 0.21091539 0.21123214
 0.21193813 0.212144   0.21222356 0.2123546  0.21245988 0.21254855
 0.21253164 0.21249485 0.2125179  0.21250232 0.21245044 0.21202683
 0.21095413 0.21029459 0.20983928 0.20916335 0.20845151 0.20795217
 0.20767336 0.20746171 0.2075938  0.20783046 0.20775539 0.20796731
 0.20877168 0.20879613 0.20854664 0.20823944 0.20803091 0.20800735
 0.20802855 0.2077949  0.20767583 0.20779353 0.20807688 0.207894
 0.20691898 0.2061932  0.20550725 0.20481692 0.20435975 0.20371021
 0.20319884 0.20287627 0.20289509 0.20303033 0.20279434 0.20277591
 0.202964   0.2025831  0.20197748 0.20161676 0.20152354 0.20152852
 0.20143779 0.20127243 0.20118158 0.20122558 0.20115778 0.20062375
 0.1994714  0.1987209  0.19812375 0.19713043 0.19617732 0.19559519
 0.19532296 0.19519469 0.19502348 0.19495517 0.19500549 0.19525065
 0.1957019  0.19547476 0.19514053 0.19505692 0.19488168 0.19457681
 0.19425626 0.19409421 0.19412227 0.19425432 0.19433539 0.19396451
 0.19298725 0.19223005 0.19157702 0.19069749 0.19011521 0.18974245
 0.18958326 0.1893616  0.18920374 0.18948045 0.18954945 0.18977536
 0.19043536 0.190384   0.18996683 0.18971369 0.1894768  0.18929368
 0.18906324 0.18893659 0.18900593 0.18925357 0.18946213 0.18920873
 0.18819901 0.1872515  0.18631303 0.18555948 0.18516009 0.18486768
 0.18472122 0.18459146 0.18466952 0.18486531 0.18498258 0.18528703
 0.18581133 0.18570112 0.18537891 0.18515424 0.18506686 0.18502437
 0.18504056 0.18506452 0.18504137 0.18516392 0.1852614  0.18481891
 0.18334545 0.18196946 0.1808401  0.17953521 0.1785872  0.17780973
 0.17742398 0.17728893 0.17762995 0.1780684  0.17831795 0.17897314
 0.17995779 0.1798834  0.17938219 0.1790933  0.17899257 0.17892483
 0.17875007 0.17867616 0.17882617 0.17917743 0.17952749 0.17938663
 0.17865743 0.17821558 0.17758358 0.1766291  0.17593072 0.17561443
 0.17535134 0.17502691 0.17493181 0.17490084 0.17461479 0.17453012
 0.17484911 0.17422792 0.17360057 0.17338352 0.17313406 0.17258653
 0.17239796 0.17271562 0.17279463 0.17223196 0.17232066 0.17259966]
