Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=90, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14031360.0
params:  15834.0
Trainable parameters:  15834
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.949871301651001
Epoch: 1, Steps: 62 | Train Loss: 0.7851830 Vali Loss: 1.6088660 Test Loss: 0.7220724
Validation loss decreased (inf --> 1.608866).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.1902081966400146
Epoch: 2, Steps: 62 | Train Loss: 0.6187635 Vali Loss: 1.4443663 Test Loss: 0.6122230
Validation loss decreased (1.608866 --> 1.444366).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0368001461029053
Epoch: 3, Steps: 62 | Train Loss: 0.5636966 Vali Loss: 1.3671112 Test Loss: 0.5623053
Validation loss decreased (1.444366 --> 1.367111).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.88728404045105
Epoch: 4, Steps: 62 | Train Loss: 0.5343869 Vali Loss: 1.3184582 Test Loss: 0.5292870
Validation loss decreased (1.367111 --> 1.318458).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.4846885204315186
Epoch: 5, Steps: 62 | Train Loss: 0.5147830 Vali Loss: 1.2808043 Test Loss: 0.5053036
Validation loss decreased (1.318458 --> 1.280804).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.5656769275665283
Epoch: 6, Steps: 62 | Train Loss: 0.5005354 Vali Loss: 1.2588204 Test Loss: 0.4872121
Validation loss decreased (1.280804 --> 1.258820).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.726499319076538
Epoch: 7, Steps: 62 | Train Loss: 0.4900658 Vali Loss: 1.2354592 Test Loss: 0.4738721
Validation loss decreased (1.258820 --> 1.235459).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.7424421310424805
Epoch: 8, Steps: 62 | Train Loss: 0.4823289 Vali Loss: 1.2228860 Test Loss: 0.4634824
Validation loss decreased (1.235459 --> 1.222886).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.655384063720703
Epoch: 9, Steps: 62 | Train Loss: 0.4764489 Vali Loss: 1.2145603 Test Loss: 0.4557721
Validation loss decreased (1.222886 --> 1.214560).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.402642250061035
Epoch: 10, Steps: 62 | Train Loss: 0.4719506 Vali Loss: 1.1990354 Test Loss: 0.4499730
Validation loss decreased (1.214560 --> 1.199035).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.623018980026245
Epoch: 11, Steps: 62 | Train Loss: 0.4685018 Vali Loss: 1.1957189 Test Loss: 0.4455308
Validation loss decreased (1.199035 --> 1.195719).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.7541782855987549
Epoch: 12, Steps: 62 | Train Loss: 0.4658012 Vali Loss: 1.1923198 Test Loss: 0.4422323
Validation loss decreased (1.195719 --> 1.192320).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.1671485900878906
Epoch: 13, Steps: 62 | Train Loss: 0.4639655 Vali Loss: 1.1846031 Test Loss: 0.4395428
Validation loss decreased (1.192320 --> 1.184603).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.6120405197143555
Epoch: 14, Steps: 62 | Train Loss: 0.4623032 Vali Loss: 1.1824794 Test Loss: 0.4377347
Validation loss decreased (1.184603 --> 1.182479).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.583645820617676
Epoch: 15, Steps: 62 | Train Loss: 0.4611745 Vali Loss: 1.1765214 Test Loss: 0.4361661
Validation loss decreased (1.182479 --> 1.176521).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.1296937465667725
Epoch: 16, Steps: 62 | Train Loss: 0.4600804 Vali Loss: 1.1778325 Test Loss: 0.4350251
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.987335681915283
Epoch: 17, Steps: 62 | Train Loss: 0.4593909 Vali Loss: 1.1735247 Test Loss: 0.4341235
Validation loss decreased (1.176521 --> 1.173525).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.9936881065368652
Epoch: 18, Steps: 62 | Train Loss: 0.4587498 Vali Loss: 1.1688509 Test Loss: 0.4334835
Validation loss decreased (1.173525 --> 1.168851).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.000731468200684
Epoch: 19, Steps: 62 | Train Loss: 0.4583233 Vali Loss: 1.1758105 Test Loss: 0.4330640
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.0149877071380615
Epoch: 20, Steps: 62 | Train Loss: 0.4578409 Vali Loss: 1.1703234 Test Loss: 0.4325872
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.2104172706604004
Epoch: 21, Steps: 62 | Train Loss: 0.4575555 Vali Loss: 1.1688268 Test Loss: 0.4323213
Validation loss decreased (1.168851 --> 1.168827).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.012038230895996
Epoch: 22, Steps: 62 | Train Loss: 0.4573430 Vali Loss: 1.1712828 Test Loss: 0.4320641
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.4560141563415527
Epoch: 23, Steps: 62 | Train Loss: 0.4570559 Vali Loss: 1.1649976 Test Loss: 0.4318626
Validation loss decreased (1.168827 --> 1.164998).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.5066311359405518
Epoch: 24, Steps: 62 | Train Loss: 0.4569460 Vali Loss: 1.1665746 Test Loss: 0.4317021
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.3895678520202637
Epoch: 25, Steps: 62 | Train Loss: 0.4567046 Vali Loss: 1.1605692 Test Loss: 0.4315701
Validation loss decreased (1.164998 --> 1.160569).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.9079680442810059
Epoch: 26, Steps: 62 | Train Loss: 0.4564545 Vali Loss: 1.1681637 Test Loss: 0.4314573
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.349055290222168
Epoch: 27, Steps: 62 | Train Loss: 0.4563021 Vali Loss: 1.1638010 Test Loss: 0.4314294
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6864228248596191
Epoch: 28, Steps: 62 | Train Loss: 0.4561270 Vali Loss: 1.1642212 Test Loss: 0.4313248
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.833876132965088
Epoch: 29, Steps: 62 | Train Loss: 0.4560203 Vali Loss: 1.1684474 Test Loss: 0.4312862
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.8134140968322754
Epoch: 30, Steps: 62 | Train Loss: 0.4559432 Vali Loss: 1.1661991 Test Loss: 0.4311930
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.400787830352783
Epoch: 31, Steps: 62 | Train Loss: 0.4560454 Vali Loss: 1.1675527 Test Loss: 0.4312019
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.0429165363311768
Epoch: 32, Steps: 62 | Train Loss: 0.4559444 Vali Loss: 1.1634109 Test Loss: 0.4311702
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.757685661315918
Epoch: 33, Steps: 62 | Train Loss: 0.4557568 Vali Loss: 1.1668915 Test Loss: 0.4311698
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.5976853370666504
Epoch: 34, Steps: 62 | Train Loss: 0.4557462 Vali Loss: 1.1632479 Test Loss: 0.4310987
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.2462029457092285
Epoch: 35, Steps: 62 | Train Loss: 0.4555645 Vali Loss: 1.1661271 Test Loss: 0.4310496
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.610879421234131
Epoch: 36, Steps: 62 | Train Loss: 0.4554926 Vali Loss: 1.1654207 Test Loss: 0.4310580
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.54826283454895
Epoch: 37, Steps: 62 | Train Loss: 0.4555541 Vali Loss: 1.1624414 Test Loss: 0.4311430
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.3889589309692383
Epoch: 38, Steps: 62 | Train Loss: 0.4556128 Vali Loss: 1.1658036 Test Loss: 0.4310611
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.358715534210205
Epoch: 39, Steps: 62 | Train Loss: 0.4554659 Vali Loss: 1.1629066 Test Loss: 0.4310313
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.467085599899292
Epoch: 40, Steps: 62 | Train Loss: 0.4554075 Vali Loss: 1.1659498 Test Loss: 0.4310308
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.9176790714263916
Epoch: 41, Steps: 62 | Train Loss: 0.4552449 Vali Loss: 1.1610613 Test Loss: 0.4310138
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.8790013790130615
Epoch: 42, Steps: 62 | Train Loss: 0.4553864 Vali Loss: 1.1647265 Test Loss: 0.4310315
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.0665817260742188
Epoch: 43, Steps: 62 | Train Loss: 0.4553440 Vali Loss: 1.1661094 Test Loss: 0.4310269
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.7071459293365479
Epoch: 44, Steps: 62 | Train Loss: 0.4553967 Vali Loss: 1.1670573 Test Loss: 0.4310046
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.8938724994659424
Epoch: 45, Steps: 62 | Train Loss: 0.4552275 Vali Loss: 1.1610719 Test Loss: 0.4310181
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43036866188049316, mae:0.43008899688720703, rse:0.624557375907898, corr:[0.24746065 0.25869083 0.2548613  0.25715572 0.25809345 0.25513893
 0.25303957 0.25409287 0.25518405 0.25458255 0.25317845 0.25259414
 0.25268587 0.2523718  0.25147587 0.25079468 0.25051925 0.2506006
 0.2507626  0.25081593 0.25083667 0.2509937  0.25171837 0.25224257
 0.2517993  0.25114742 0.25122216 0.2514427  0.2509549  0.25005808
 0.24968761 0.24987482 0.24983767 0.24931337 0.24888742 0.24910963
 0.24950552 0.2494526  0.24911794 0.2489951  0.24913059 0.24931951
 0.24933346 0.24919912 0.24904582 0.24912201 0.24937208 0.24922667
 0.24842387 0.24762292 0.24726622 0.2469698  0.24614145 0.24511014
 0.24469188 0.24465045 0.24437974 0.24382327 0.24340987 0.24345398
 0.24342555 0.24312368 0.24279138 0.24284169 0.24324962 0.24376102
 0.24405801 0.24388467 0.24378161 0.24395773 0.24410383 0.24373016
 0.24298923 0.24238384 0.24211735 0.24192089 0.24159698 0.24127313
 0.24104877 0.24079421 0.24040955 0.23986532 0.23942684 0.23932849
 0.23934373 0.23916644 0.23889902 0.23879693 0.23887937 0.23890844
 0.23862182 0.23819235 0.23793529 0.23800795 0.23833211 0.23890615
 0.23951371 0.23992752 0.24027014 0.24034555 0.24020883 0.2401337
 0.2402499  0.24036238 0.24016707 0.23984455 0.23964517 0.23956907
 0.23945008 0.23932691 0.23939544 0.23965526 0.23982339 0.23987457
 0.2398459  0.23960836 0.23919694 0.23875356 0.23841572 0.2383276
 0.23832174 0.23810609 0.23768951 0.23706704 0.2364548  0.23605885
 0.2360345  0.2360736  0.23572667 0.2352488  0.23503238 0.23501806
 0.23498265 0.23496138 0.23504251 0.23519847 0.23539111 0.23561458
 0.23580617 0.23579694 0.23565957 0.23547138 0.23512143 0.2348035
 0.23470002 0.2346175  0.23435897 0.23373483 0.23299806 0.23249246
 0.23244119 0.23258755 0.23267373 0.23284197 0.23301692 0.23305358
 0.23292226 0.23289947 0.23296085 0.2329345  0.23271899 0.23270243
 0.23279431 0.23268713 0.2323396  0.2319258  0.23154847 0.23142375
 0.23160718 0.23203786 0.23251644 0.23277345 0.23281848 0.23290175
 0.2331628  0.23334593 0.23324472 0.23308785 0.23308165 0.23314728
 0.23306295 0.23282008 0.2326922  0.2327978  0.23303472 0.23330884
 0.23340164 0.23332553 0.23331767 0.23326132 0.23297551 0.23255785
 0.23216197 0.23186502 0.2313392  0.23054579 0.22978817 0.22946551
 0.22953165 0.22961026 0.22936718 0.22921392 0.2293303  0.22945102
 0.22936839 0.22935703 0.22961627 0.22989191 0.2299673  0.22998945
 0.23006667 0.22984603 0.229311   0.22879672 0.2285864  0.22864854
 0.22861478 0.22831342 0.22825582 0.22817469 0.22791629 0.22771676
 0.22779615 0.22800434 0.22799855 0.22766891 0.22726591 0.22699903
 0.22668523 0.22646762 0.22670597 0.2270503  0.22709323 0.22698243
 0.22696914 0.22684503 0.22651681 0.22620788 0.22621898 0.22627948
 0.22631305 0.22622995 0.22627969 0.22635071 0.22623566 0.22609027
 0.22628106 0.22676244 0.22705004 0.22686557 0.22651899 0.22626571
 0.22604828 0.22594708 0.2262271  0.22661832 0.22689931 0.22710606
 0.22732562 0.2272792  0.2271372  0.22710589 0.22721127 0.22723882
 0.2270108  0.22670312 0.22639039 0.22602762 0.22545338 0.22497144
 0.22485182 0.22476764 0.2244835  0.22425655 0.22434208 0.2244165
 0.22424647 0.22395216 0.22391102 0.22410218 0.22407368 0.22382872
 0.22386234 0.22394712 0.22390758 0.22372654 0.22376963 0.22420467
 0.22487505 0.22534792 0.22556654 0.22544599 0.2252509  0.2252731
 0.22550558 0.22562295 0.22552978 0.22559267 0.22576486 0.22572726
 0.22529745 0.22484943 0.22514145 0.22559413 0.22572698 0.22560754
 0.22577694 0.22588241 0.22558343 0.22496785 0.22480388 0.2252148
 0.22559766 0.22568868 0.2255047  0.22538508 0.22505312 0.22448044
 0.22411636 0.22395083 0.22348979 0.22324687 0.22314443 0.22286671
 0.22226177 0.22177379 0.22223663 0.22294718 0.22213393 0.22122547
 0.22218277 0.22233163 0.218652   0.21566945 0.2206023  0.21781583]
