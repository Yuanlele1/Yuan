Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=22, out_features=104, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2050048.0
params:  2392.0
Trainable parameters:  2392
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.3816497325897217
Epoch: 1, Steps: 64 | Train Loss: 1.0671103 Vali Loss: 2.1284645 Test Loss: 1.0870793
Validation loss decreased (inf --> 2.128464).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.4533934593200684
Epoch: 2, Steps: 64 | Train Loss: 0.8232194 Vali Loss: 1.7953676 Test Loss: 0.8310747
Validation loss decreased (2.128464 --> 1.795368).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.3607757091522217
Epoch: 3, Steps: 64 | Train Loss: 0.6986662 Vali Loss: 1.6298034 Test Loss: 0.7017339
Validation loss decreased (1.795368 --> 1.629803).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3473765850067139
Epoch: 4, Steps: 64 | Train Loss: 0.6292991 Vali Loss: 1.5313439 Test Loss: 0.6276817
Validation loss decreased (1.629803 --> 1.531344).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.3726427555084229
Epoch: 5, Steps: 64 | Train Loss: 0.5876606 Vali Loss: 1.4643797 Test Loss: 0.5823717
Validation loss decreased (1.531344 --> 1.464380).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.3492836952209473
Epoch: 6, Steps: 64 | Train Loss: 0.5615292 Vali Loss: 1.4286531 Test Loss: 0.5536214
Validation loss decreased (1.464380 --> 1.428653).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4281189441680908
Epoch: 7, Steps: 64 | Train Loss: 0.5443428 Vali Loss: 1.3998653 Test Loss: 0.5347887
Validation loss decreased (1.428653 --> 1.399865).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.3775973320007324
Epoch: 8, Steps: 64 | Train Loss: 0.5326319 Vali Loss: 1.3813890 Test Loss: 0.5222525
Validation loss decreased (1.399865 --> 1.381389).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4857802391052246
Epoch: 9, Steps: 64 | Train Loss: 0.5243284 Vali Loss: 1.3670465 Test Loss: 0.5135891
Validation loss decreased (1.381389 --> 1.367046).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.4022974967956543
Epoch: 10, Steps: 64 | Train Loss: 0.5184595 Vali Loss: 1.3535223 Test Loss: 0.5073981
Validation loss decreased (1.367046 --> 1.353522).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.4731404781341553
Epoch: 11, Steps: 64 | Train Loss: 0.5136588 Vali Loss: 1.3421535 Test Loss: 0.5029365
Validation loss decreased (1.353522 --> 1.342154).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.3275904655456543
Epoch: 12, Steps: 64 | Train Loss: 0.5104831 Vali Loss: 1.3424965 Test Loss: 0.4996482
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.333681583404541
Epoch: 13, Steps: 64 | Train Loss: 0.5075341 Vali Loss: 1.3340899 Test Loss: 0.4969819
Validation loss decreased (1.342154 --> 1.334090).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.3343863487243652
Epoch: 14, Steps: 64 | Train Loss: 0.5051566 Vali Loss: 1.3346981 Test Loss: 0.4949809
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4157402515411377
Epoch: 15, Steps: 64 | Train Loss: 0.5036341 Vali Loss: 1.3314432 Test Loss: 0.4933387
Validation loss decreased (1.334090 --> 1.331443).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.3431878089904785
Epoch: 16, Steps: 64 | Train Loss: 0.5017461 Vali Loss: 1.3191640 Test Loss: 0.4919906
Validation loss decreased (1.331443 --> 1.319164).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.408280611038208
Epoch: 17, Steps: 64 | Train Loss: 0.5004885 Vali Loss: 1.3204074 Test Loss: 0.4909412
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.449251651763916
Epoch: 18, Steps: 64 | Train Loss: 0.4993854 Vali Loss: 1.3186235 Test Loss: 0.4900203
Validation loss decreased (1.319164 --> 1.318624).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4686191082000732
Epoch: 19, Steps: 64 | Train Loss: 0.4986913 Vali Loss: 1.3209107 Test Loss: 0.4892060
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3787074089050293
Epoch: 20, Steps: 64 | Train Loss: 0.4977351 Vali Loss: 1.3178440 Test Loss: 0.4885607
Validation loss decreased (1.318624 --> 1.317844).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.3873159885406494
Epoch: 21, Steps: 64 | Train Loss: 0.4969420 Vali Loss: 1.3141577 Test Loss: 0.4879886
Validation loss decreased (1.317844 --> 1.314158).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.3852651119232178
Epoch: 22, Steps: 64 | Train Loss: 0.4961327 Vali Loss: 1.3158722 Test Loss: 0.4874379
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3737094402313232
Epoch: 23, Steps: 64 | Train Loss: 0.4954638 Vali Loss: 1.3163817 Test Loss: 0.4870067
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.3347418308258057
Epoch: 24, Steps: 64 | Train Loss: 0.4950624 Vali Loss: 1.3011999 Test Loss: 0.4866163
Validation loss decreased (1.314158 --> 1.301200).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.349654197692871
Epoch: 25, Steps: 64 | Train Loss: 0.4943303 Vali Loss: 1.3073517 Test Loss: 0.4862633
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4236652851104736
Epoch: 26, Steps: 64 | Train Loss: 0.4939449 Vali Loss: 1.3059549 Test Loss: 0.4859840
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.2585780620574951
Epoch: 27, Steps: 64 | Train Loss: 0.4934510 Vali Loss: 1.3025116 Test Loss: 0.4857265
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.42146897315979
Epoch: 28, Steps: 64 | Train Loss: 0.4934405 Vali Loss: 1.3097442 Test Loss: 0.4854472
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.422804594039917
Epoch: 29, Steps: 64 | Train Loss: 0.4929749 Vali Loss: 1.3075534 Test Loss: 0.4852077
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4392554759979248
Epoch: 30, Steps: 64 | Train Loss: 0.4924212 Vali Loss: 1.3085792 Test Loss: 0.4850323
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.3505733013153076
Epoch: 31, Steps: 64 | Train Loss: 0.4924305 Vali Loss: 1.3048453 Test Loss: 0.4848543
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.4916572570800781
Epoch: 32, Steps: 64 | Train Loss: 0.4917936 Vali Loss: 1.3009180 Test Loss: 0.4846964
Validation loss decreased (1.301200 --> 1.300918).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4285147190093994
Epoch: 33, Steps: 64 | Train Loss: 0.4916715 Vali Loss: 1.3029981 Test Loss: 0.4845530
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7975685596466064
Epoch: 34, Steps: 64 | Train Loss: 0.4916790 Vali Loss: 1.2978116 Test Loss: 0.4844283
Validation loss decreased (1.300918 --> 1.297812).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.6836659908294678
Epoch: 35, Steps: 64 | Train Loss: 0.4912373 Vali Loss: 1.3005866 Test Loss: 0.4843063
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.4606544971466064
Epoch: 36, Steps: 64 | Train Loss: 0.4910585 Vali Loss: 1.3025714 Test Loss: 0.4842055
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.344148874282837
Epoch: 37, Steps: 64 | Train Loss: 0.4907784 Vali Loss: 1.3070874 Test Loss: 0.4841005
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.299121379852295
Epoch: 38, Steps: 64 | Train Loss: 0.4907908 Vali Loss: 1.3087540 Test Loss: 0.4840124
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3332898616790771
Epoch: 39, Steps: 64 | Train Loss: 0.4907221 Vali Loss: 1.3047483 Test Loss: 0.4839306
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.4091651439666748
Epoch: 40, Steps: 64 | Train Loss: 0.4906570 Vali Loss: 1.3034008 Test Loss: 0.4838406
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.314753532409668
Epoch: 41, Steps: 64 | Train Loss: 0.4906411 Vali Loss: 1.3058646 Test Loss: 0.4837778
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3265025615692139
Epoch: 42, Steps: 64 | Train Loss: 0.4902554 Vali Loss: 1.3008478 Test Loss: 0.4837219
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.4487156867980957
Epoch: 43, Steps: 64 | Train Loss: 0.4901018 Vali Loss: 1.3081105 Test Loss: 0.4836508
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.4211053848266602
Epoch: 44, Steps: 64 | Train Loss: 0.4901140 Vali Loss: 1.3008095 Test Loss: 0.4835979
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.388265609741211
Epoch: 45, Steps: 64 | Train Loss: 0.4896751 Vali Loss: 1.2995285 Test Loss: 0.4835665
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.3352432250976562
Epoch: 46, Steps: 64 | Train Loss: 0.4898975 Vali Loss: 1.3024848 Test Loss: 0.4835041
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.3636221885681152
Epoch: 47, Steps: 64 | Train Loss: 0.4897072 Vali Loss: 1.3049685 Test Loss: 0.4834680
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.3378357887268066
Epoch: 48, Steps: 64 | Train Loss: 0.4891545 Vali Loss: 1.3020413 Test Loss: 0.4834186
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.4266350269317627
Epoch: 49, Steps: 64 | Train Loss: 0.4895931 Vali Loss: 1.3003894 Test Loss: 0.4833849
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2911975383758545
Epoch: 50, Steps: 64 | Train Loss: 0.4896746 Vali Loss: 1.2942208 Test Loss: 0.4833664
Validation loss decreased (1.297812 --> 1.294221).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.4410247802734375
Epoch: 51, Steps: 64 | Train Loss: 0.4892532 Vali Loss: 1.2991718 Test Loss: 0.4833109
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.4409408569335938
Epoch: 52, Steps: 64 | Train Loss: 0.4893994 Vali Loss: 1.2952992 Test Loss: 0.4832847
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.3730518817901611
Epoch: 53, Steps: 64 | Train Loss: 0.4890195 Vali Loss: 1.3018390 Test Loss: 0.4832565
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.2148408889770508
Epoch: 54, Steps: 64 | Train Loss: 0.4893398 Vali Loss: 1.2997407 Test Loss: 0.4832360
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.4895474910736084
Epoch: 55, Steps: 64 | Train Loss: 0.4891408 Vali Loss: 1.2974104 Test Loss: 0.4832136
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.445084571838379
Epoch: 56, Steps: 64 | Train Loss: 0.4887046 Vali Loss: 1.3005794 Test Loss: 0.4831850
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.4183359146118164
Epoch: 57, Steps: 64 | Train Loss: 0.4889865 Vali Loss: 1.2939339 Test Loss: 0.4831675
Validation loss decreased (1.294221 --> 1.293934).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.4331493377685547
Epoch: 58, Steps: 64 | Train Loss: 0.4893060 Vali Loss: 1.3044332 Test Loss: 0.4831428
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.405090570449829
Epoch: 59, Steps: 64 | Train Loss: 0.4892286 Vali Loss: 1.2994664 Test Loss: 0.4831271
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.4286563396453857
Epoch: 60, Steps: 64 | Train Loss: 0.4892227 Vali Loss: 1.3065403 Test Loss: 0.4831107
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.470407247543335
Epoch: 61, Steps: 64 | Train Loss: 0.4891661 Vali Loss: 1.2962909 Test Loss: 0.4830866
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.3505446910858154
Epoch: 62, Steps: 64 | Train Loss: 0.4887925 Vali Loss: 1.3000674 Test Loss: 0.4830695
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.3663759231567383
Epoch: 63, Steps: 64 | Train Loss: 0.4888615 Vali Loss: 1.3052621 Test Loss: 0.4830588
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.4425783157348633
Epoch: 64, Steps: 64 | Train Loss: 0.4888221 Vali Loss: 1.3009826 Test Loss: 0.4830395
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.381702184677124
Epoch: 65, Steps: 64 | Train Loss: 0.4888159 Vali Loss: 1.2978264 Test Loss: 0.4830319
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.3917126655578613
Epoch: 66, Steps: 64 | Train Loss: 0.4891334 Vali Loss: 1.2998257 Test Loss: 0.4830235
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.4499025344848633
Epoch: 67, Steps: 64 | Train Loss: 0.4886389 Vali Loss: 1.3010031 Test Loss: 0.4830150
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.3577039241790771
Epoch: 68, Steps: 64 | Train Loss: 0.4887567 Vali Loss: 1.2942719 Test Loss: 0.4829973
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.322664737701416
Epoch: 69, Steps: 64 | Train Loss: 0.4888835 Vali Loss: 1.2969197 Test Loss: 0.4829888
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.4162094593048096
Epoch: 70, Steps: 64 | Train Loss: 0.4888275 Vali Loss: 1.3049759 Test Loss: 0.4829774
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.468264102935791
Epoch: 71, Steps: 64 | Train Loss: 0.4887123 Vali Loss: 1.3014824 Test Loss: 0.4829701
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.4363181591033936
Epoch: 72, Steps: 64 | Train Loss: 0.4885410 Vali Loss: 1.3053395 Test Loss: 0.4829591
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.3832645416259766
Epoch: 73, Steps: 64 | Train Loss: 0.4884938 Vali Loss: 1.3002397 Test Loss: 0.4829551
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.329864501953125
Epoch: 74, Steps: 64 | Train Loss: 0.4887053 Vali Loss: 1.3017498 Test Loss: 0.4829399
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.3491063117980957
Epoch: 75, Steps: 64 | Train Loss: 0.4885616 Vali Loss: 1.2950646 Test Loss: 0.4829372
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.4139325618743896
Epoch: 76, Steps: 64 | Train Loss: 0.4886313 Vali Loss: 1.2909311 Test Loss: 0.4829301
Validation loss decreased (1.293934 --> 1.290931).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.4941387176513672
Epoch: 77, Steps: 64 | Train Loss: 0.4884701 Vali Loss: 1.3026173 Test Loss: 0.4829206
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.4241259098052979
Epoch: 78, Steps: 64 | Train Loss: 0.4885129 Vali Loss: 1.2946272 Test Loss: 0.4829177
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.4738152027130127
Epoch: 79, Steps: 64 | Train Loss: 0.4886360 Vali Loss: 1.2942196 Test Loss: 0.4829096
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.502690315246582
Epoch: 80, Steps: 64 | Train Loss: 0.4884118 Vali Loss: 1.3008330 Test Loss: 0.4829039
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.4382319450378418
Epoch: 81, Steps: 64 | Train Loss: 0.4883712 Vali Loss: 1.3004544 Test Loss: 0.4829032
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.3993237018585205
Epoch: 82, Steps: 64 | Train Loss: 0.4887610 Vali Loss: 1.2971588 Test Loss: 0.4828945
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.3927326202392578
Epoch: 83, Steps: 64 | Train Loss: 0.4885326 Vali Loss: 1.2983198 Test Loss: 0.4828926
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.3271033763885498
Epoch: 84, Steps: 64 | Train Loss: 0.4886975 Vali Loss: 1.2967117 Test Loss: 0.4828857
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.3771030902862549
Epoch: 85, Steps: 64 | Train Loss: 0.4885899 Vali Loss: 1.2978122 Test Loss: 0.4828830
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.3477783203125
Epoch: 86, Steps: 64 | Train Loss: 0.4885274 Vali Loss: 1.2946926 Test Loss: 0.4828788
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.483452320098877
Epoch: 87, Steps: 64 | Train Loss: 0.4886986 Vali Loss: 1.2963204 Test Loss: 0.4828765
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.4510838985443115
Epoch: 88, Steps: 64 | Train Loss: 0.4884875 Vali Loss: 1.2954639 Test Loss: 0.4828719
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.3236315250396729
Epoch: 89, Steps: 64 | Train Loss: 0.4884897 Vali Loss: 1.3024871 Test Loss: 0.4828688
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.3681917190551758
Epoch: 90, Steps: 64 | Train Loss: 0.4884037 Vali Loss: 1.3030300 Test Loss: 0.4828665
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.5050928592681885
Epoch: 91, Steps: 64 | Train Loss: 0.4882460 Vali Loss: 1.2965240 Test Loss: 0.4828586
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.4206528663635254
Epoch: 92, Steps: 64 | Train Loss: 0.4881811 Vali Loss: 1.2998854 Test Loss: 0.4828576
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.361436367034912
Epoch: 93, Steps: 64 | Train Loss: 0.4883098 Vali Loss: 1.2931046 Test Loss: 0.4828572
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.413980484008789
Epoch: 94, Steps: 64 | Train Loss: 0.4883155 Vali Loss: 1.3037876 Test Loss: 0.4828528
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.3886723518371582
Epoch: 95, Steps: 64 | Train Loss: 0.4884134 Vali Loss: 1.2972416 Test Loss: 0.4828502
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.4163904190063477
Epoch: 96, Steps: 64 | Train Loss: 0.4883219 Vali Loss: 1.2977664 Test Loss: 0.4828496
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.48241859674453735, mae:0.448169469833374, rse:0.6612474918365479, corr:[0.25310037 0.25329638 0.25038606 0.25042483 0.24915071 0.24688882
 0.2460687  0.24603266 0.24638309 0.24663866 0.24586686 0.24502343
 0.24452642 0.24425702 0.24385683 0.24333882 0.24338555 0.2439851
 0.2441957  0.24371807 0.24304424 0.2429035  0.24278757 0.2419873
 0.24007685 0.23926343 0.23966832 0.23997766 0.23965536 0.23967592
 0.24028417 0.24044845 0.2403188  0.24015808 0.23993602 0.23982908
 0.2396007  0.23935333 0.23939495 0.23955089 0.2399374  0.24062954
 0.24126874 0.24128957 0.24101074 0.24084963 0.24089412 0.24045333
 0.23877835 0.2375556  0.23709655 0.2367177  0.23574294 0.23486866
 0.23503742 0.23485805 0.2344592  0.23439966 0.23417847 0.2339421
 0.23365706 0.2335851  0.23356248 0.23333898 0.23342271 0.23398839
 0.23463862 0.23462024 0.23410901 0.23384078 0.23370294 0.2327591
 0.23066378 0.22937708 0.229007   0.22890405 0.22842807 0.2281816
 0.22876129 0.22878744 0.22851266 0.2282754  0.22800545 0.2276676
 0.22739808 0.22736104 0.22747082 0.22741042 0.22737224 0.22766326
 0.2279263  0.22783254 0.22758915 0.22775915 0.22804303 0.22757244
 0.22607371 0.22542968 0.22561237 0.22543117 0.225203   0.22538301
 0.22611171 0.22622687 0.22592758 0.22578716 0.22563826 0.22533171
 0.22501573 0.22489701 0.2250829  0.22510755 0.22522968 0.22550093
 0.2258966  0.2260296  0.22588459 0.22578508 0.22550592 0.2244205
 0.22227666 0.220777   0.22003838 0.21929945 0.2188014  0.21890008
 0.21988699 0.22022626 0.22005185 0.21995595 0.21984522 0.21961941
 0.21927531 0.21903998 0.21905279 0.21907291 0.21925765 0.21958134
 0.21994367 0.2201497  0.22012192 0.22017673 0.22003171 0.21906751
 0.2170764  0.21578012 0.21528827 0.21445876 0.21363233 0.21371004
 0.21485965 0.21510836 0.21496364 0.21493724 0.21480481 0.2145024
 0.21407095 0.2137659  0.2136204  0.21357198 0.2136661  0.2139504
 0.21428682 0.2144224  0.21439324 0.21447559 0.21425861 0.21308167
 0.21111406 0.21027689 0.21039541 0.2101055  0.20907763 0.20899501
 0.21017687 0.21043672 0.21014632 0.21012111 0.21022195 0.21006778
 0.20972483 0.2094861  0.20945331 0.20936532 0.2094546  0.20996743
 0.21056406 0.21088243 0.21107908 0.21138416 0.21141717 0.21041416
 0.2082329  0.20736653 0.20739606 0.20700867 0.20624955 0.20644818
 0.2075242  0.2078606  0.20762815 0.20740184 0.20718974 0.2070351
 0.2067568  0.20648023 0.20635425 0.20622537 0.20624816 0.20647076
 0.2067505  0.20685054 0.20683672 0.20699005 0.20692414 0.20591472
 0.20389996 0.20310521 0.20355909 0.20396641 0.20412418 0.20517465
 0.20716172 0.20813292 0.20834163 0.20822398 0.20797235 0.20797922
 0.20801683 0.20776889 0.20757711 0.20758846 0.20774822 0.20792386
 0.20807123 0.20815001 0.20832613 0.20872791 0.208828   0.20775996
 0.20554686 0.20425937 0.20406199 0.20390102 0.2035811  0.20417874
 0.20592746 0.20681623 0.20699014 0.2069121  0.20670077 0.20645513
 0.20609935 0.20566724 0.20551024 0.20548305 0.20556039 0.20577744
 0.20613582 0.20625417 0.2060644  0.20589776 0.20586245 0.20514528
 0.2032509  0.20218435 0.20226942 0.20272319 0.20283389 0.203518
 0.20518734 0.20616537 0.20642209 0.206346   0.20607057 0.20585397
 0.20572567 0.20559661 0.20535593 0.20521227 0.20536393 0.2057825
 0.20608102 0.20608842 0.20617157 0.20650469 0.20667107 0.20598704
 0.20455782 0.20403585 0.20487276 0.2055896  0.2057657  0.20642349
 0.20778377 0.20828275 0.208471   0.2084947  0.20816334 0.20769617
 0.2072801  0.20708206 0.20693263 0.2065888  0.20650885 0.2067684
 0.20716068 0.20723468 0.20708536 0.2071053  0.20705073 0.20597349
 0.20405537 0.20280758 0.20266551 0.2027315  0.2029125  0.20311965
 0.20413436 0.20446545 0.20463978 0.20490068 0.20449829 0.20362592
 0.20309198 0.20328368 0.20346157 0.2031346  0.20264271 0.20308712
 0.20409738 0.20370065 0.20134634 0.20023136 0.20247938 0.19976242]
