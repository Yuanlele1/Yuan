Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_96_FITS_ETTh1_ftM_sl360_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8185
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9192960.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.180945873260498
Epoch: 1, Steps: 63 | Train Loss: 0.5695276 Vali Loss: 1.0257018 Test Loss: 0.5225095
Validation loss decreased (inf --> 1.025702).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9009509086608887
Epoch: 2, Steps: 63 | Train Loss: 0.4220227 Vali Loss: 0.8616791 Test Loss: 0.4298834
Validation loss decreased (1.025702 --> 0.861679).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.875365972518921
Epoch: 3, Steps: 63 | Train Loss: 0.3801064 Vali Loss: 0.7942285 Test Loss: 0.3968899
Validation loss decreased (0.861679 --> 0.794228).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.387631416320801
Epoch: 4, Steps: 63 | Train Loss: 0.3624148 Vali Loss: 0.7604759 Test Loss: 0.3845071
Validation loss decreased (0.794228 --> 0.760476).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.008173704147339
Epoch: 5, Steps: 63 | Train Loss: 0.3545574 Vali Loss: 0.7476937 Test Loss: 0.3799159
Validation loss decreased (0.760476 --> 0.747694).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.7106599807739258
Epoch: 6, Steps: 63 | Train Loss: 0.3507092 Vali Loss: 0.7327827 Test Loss: 0.3782232
Validation loss decreased (0.747694 --> 0.732783).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.7655537128448486
Epoch: 7, Steps: 63 | Train Loss: 0.3487929 Vali Loss: 0.7240506 Test Loss: 0.3773166
Validation loss decreased (0.732783 --> 0.724051).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.4252047538757324
Epoch: 8, Steps: 63 | Train Loss: 0.3470624 Vali Loss: 0.7156447 Test Loss: 0.3770072
Validation loss decreased (0.724051 --> 0.715645).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.0686628818511963
Epoch: 9, Steps: 63 | Train Loss: 0.3464219 Vali Loss: 0.7142780 Test Loss: 0.3766234
Validation loss decreased (0.715645 --> 0.714278).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.825368881225586
Epoch: 10, Steps: 63 | Train Loss: 0.3454547 Vali Loss: 0.7111403 Test Loss: 0.3765296
Validation loss decreased (0.714278 --> 0.711140).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.535196304321289
Epoch: 11, Steps: 63 | Train Loss: 0.3451294 Vali Loss: 0.7080294 Test Loss: 0.3763700
Validation loss decreased (0.711140 --> 0.708029).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1871137619018555
Epoch: 12, Steps: 63 | Train Loss: 0.3438184 Vali Loss: 0.7044407 Test Loss: 0.3762009
Validation loss decreased (0.708029 --> 0.704441).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.8241369724273682
Epoch: 13, Steps: 63 | Train Loss: 0.3438420 Vali Loss: 0.7045211 Test Loss: 0.3762560
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6384062767028809
Epoch: 14, Steps: 63 | Train Loss: 0.3431842 Vali Loss: 0.6977325 Test Loss: 0.3759883
Validation loss decreased (0.704441 --> 0.697733).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.7205314636230469
Epoch: 15, Steps: 63 | Train Loss: 0.3427658 Vali Loss: 0.6964245 Test Loss: 0.3759268
Validation loss decreased (0.697733 --> 0.696425).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.728268623352051
Epoch: 16, Steps: 63 | Train Loss: 0.3429243 Vali Loss: 0.6938071 Test Loss: 0.3758518
Validation loss decreased (0.696425 --> 0.693807).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.888620138168335
Epoch: 17, Steps: 63 | Train Loss: 0.3421635 Vali Loss: 0.6934745 Test Loss: 0.3759681
Validation loss decreased (0.693807 --> 0.693474).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.587322473526001
Epoch: 18, Steps: 63 | Train Loss: 0.3418428 Vali Loss: 0.6915273 Test Loss: 0.3758042
Validation loss decreased (0.693474 --> 0.691527).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.3181393146514893
Epoch: 19, Steps: 63 | Train Loss: 0.3423131 Vali Loss: 0.6911217 Test Loss: 0.3757271
Validation loss decreased (0.691527 --> 0.691122).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.1434621810913086
Epoch: 20, Steps: 63 | Train Loss: 0.3417898 Vali Loss: 0.6936653 Test Loss: 0.3755832
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.7554192543029785
Epoch: 21, Steps: 63 | Train Loss: 0.3417400 Vali Loss: 0.6907691 Test Loss: 0.3756350
Validation loss decreased (0.691122 --> 0.690769).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.2412993907928467
Epoch: 22, Steps: 63 | Train Loss: 0.3419081 Vali Loss: 0.6879775 Test Loss: 0.3758062
Validation loss decreased (0.690769 --> 0.687977).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9653637409210205
Epoch: 23, Steps: 63 | Train Loss: 0.3418225 Vali Loss: 0.6892008 Test Loss: 0.3756152
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6878266334533691
Epoch: 24, Steps: 63 | Train Loss: 0.3415711 Vali Loss: 0.6896384 Test Loss: 0.3758170
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.0023443698883057
Epoch: 25, Steps: 63 | Train Loss: 0.3409575 Vali Loss: 0.6928321 Test Loss: 0.3755461
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8391149044036865
Epoch: 26, Steps: 63 | Train Loss: 0.3410106 Vali Loss: 0.6871789 Test Loss: 0.3756267
Validation loss decreased (0.687977 --> 0.687179).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8741559982299805
Epoch: 27, Steps: 63 | Train Loss: 0.3409189 Vali Loss: 0.6861340 Test Loss: 0.3754958
Validation loss decreased (0.687179 --> 0.686134).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.8868849277496338
Epoch: 28, Steps: 63 | Train Loss: 0.3407035 Vali Loss: 0.6830150 Test Loss: 0.3754918
Validation loss decreased (0.686134 --> 0.683015).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.616304636001587
Epoch: 29, Steps: 63 | Train Loss: 0.3409925 Vali Loss: 0.6845397 Test Loss: 0.3755483
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.1960391998291016
Epoch: 30, Steps: 63 | Train Loss: 0.3406659 Vali Loss: 0.6834711 Test Loss: 0.3756226
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.072932243347168
Epoch: 31, Steps: 63 | Train Loss: 0.3408435 Vali Loss: 0.6836168 Test Loss: 0.3755309
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.151329517364502
Epoch: 32, Steps: 63 | Train Loss: 0.3404952 Vali Loss: 0.6846057 Test Loss: 0.3754797
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.335089683532715
Epoch: 33, Steps: 63 | Train Loss: 0.3410173 Vali Loss: 0.6879320 Test Loss: 0.3755097
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.8762378692626953
Epoch: 34, Steps: 63 | Train Loss: 0.3408663 Vali Loss: 0.6845743 Test Loss: 0.3755687
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.137504816055298
Epoch: 35, Steps: 63 | Train Loss: 0.3408628 Vali Loss: 0.6838937 Test Loss: 0.3754859
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.9119410514831543
Epoch: 36, Steps: 63 | Train Loss: 0.3406047 Vali Loss: 0.6818595 Test Loss: 0.3755058
Validation loss decreased (0.683015 --> 0.681859).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.325688362121582
Epoch: 37, Steps: 63 | Train Loss: 0.3403490 Vali Loss: 0.6868274 Test Loss: 0.3755261
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.862677574157715
Epoch: 38, Steps: 63 | Train Loss: 0.3403531 Vali Loss: 0.6823390 Test Loss: 0.3755184
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.194943428039551
Epoch: 39, Steps: 63 | Train Loss: 0.3406720 Vali Loss: 0.6823387 Test Loss: 0.3756029
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.248952388763428
Epoch: 40, Steps: 63 | Train Loss: 0.3404452 Vali Loss: 0.6834548 Test Loss: 0.3755323
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.2151267528533936
Epoch: 41, Steps: 63 | Train Loss: 0.3397661 Vali Loss: 0.6835970 Test Loss: 0.3754643
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.0039570331573486
Epoch: 42, Steps: 63 | Train Loss: 0.3403508 Vali Loss: 0.6826354 Test Loss: 0.3754893
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.8353309631347656
Epoch: 43, Steps: 63 | Train Loss: 0.3399386 Vali Loss: 0.6828176 Test Loss: 0.3755250
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.666520118713379
Epoch: 44, Steps: 63 | Train Loss: 0.3404806 Vali Loss: 0.6863977 Test Loss: 0.3755511
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.295969009399414
Epoch: 45, Steps: 63 | Train Loss: 0.3403202 Vali Loss: 0.6794938 Test Loss: 0.3755258
Validation loss decreased (0.681859 --> 0.679494).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.831725835800171
Epoch: 46, Steps: 63 | Train Loss: 0.3405554 Vali Loss: 0.6799216 Test Loss: 0.3755260
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.5635557174682617
Epoch: 47, Steps: 63 | Train Loss: 0.3404281 Vali Loss: 0.6829579 Test Loss: 0.3754851
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.686244010925293
Epoch: 48, Steps: 63 | Train Loss: 0.3400409 Vali Loss: 0.6817963 Test Loss: 0.3755423
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.8168277740478516
Epoch: 49, Steps: 63 | Train Loss: 0.3405313 Vali Loss: 0.6817637 Test Loss: 0.3755270
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.8230934143066406
Epoch: 50, Steps: 63 | Train Loss: 0.3401497 Vali Loss: 0.6862669 Test Loss: 0.3755615
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.583826780319214
Epoch: 51, Steps: 63 | Train Loss: 0.3401509 Vali Loss: 0.6820548 Test Loss: 0.3755224
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.471656322479248
Epoch: 52, Steps: 63 | Train Loss: 0.3403682 Vali Loss: 0.6863310 Test Loss: 0.3755226
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.9131035804748535
Epoch: 53, Steps: 63 | Train Loss: 0.3398587 Vali Loss: 0.6819289 Test Loss: 0.3755228
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.1861705780029297
Epoch: 54, Steps: 63 | Train Loss: 0.3398525 Vali Loss: 0.6817694 Test Loss: 0.3755279
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.3368592262268066
Epoch: 55, Steps: 63 | Train Loss: 0.3401812 Vali Loss: 0.6849244 Test Loss: 0.3755035
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9075868129730225
Epoch: 56, Steps: 63 | Train Loss: 0.3403691 Vali Loss: 0.6826366 Test Loss: 0.3754846
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.486151933670044
Epoch: 57, Steps: 63 | Train Loss: 0.3402270 Vali Loss: 0.6799964 Test Loss: 0.3755161
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.590095043182373
Epoch: 58, Steps: 63 | Train Loss: 0.3395145 Vali Loss: 0.6831084 Test Loss: 0.3755287
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.674705982208252
Epoch: 59, Steps: 63 | Train Loss: 0.3402117 Vali Loss: 0.6839868 Test Loss: 0.3755427
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.066927909851074
Epoch: 60, Steps: 63 | Train Loss: 0.3397950 Vali Loss: 0.6804023 Test Loss: 0.3754966
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.456413507461548
Epoch: 61, Steps: 63 | Train Loss: 0.3402205 Vali Loss: 0.6816663 Test Loss: 0.3755006
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.8024699687957764
Epoch: 62, Steps: 63 | Train Loss: 0.3401632 Vali Loss: 0.6806836 Test Loss: 0.3755256
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.9049196243286133
Epoch: 63, Steps: 63 | Train Loss: 0.3396868 Vali Loss: 0.6829982 Test Loss: 0.3755314
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.7195854187011719
Epoch: 64, Steps: 63 | Train Loss: 0.3399843 Vali Loss: 0.6841038 Test Loss: 0.3755312
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.6236042976379395
Epoch: 65, Steps: 63 | Train Loss: 0.3397352 Vali Loss: 0.6835598 Test Loss: 0.3755248
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_96_FITS_ETTh1_ftM_sl360_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.37456798553466797, mae:0.396562784910202, rse:0.5813305974006653, corr:[0.26910317 0.27821812 0.27785298 0.27823424 0.27713367 0.27455705
 0.27291533 0.2730122  0.2729356  0.27245778 0.27223864 0.27227432
 0.27227598 0.2720042  0.2716947  0.27167043 0.27169117 0.27158144
 0.27136615 0.27102053 0.27082747 0.2708138  0.27091405 0.27097574
 0.2706969  0.27042904 0.27032796 0.27013144 0.2696107  0.2690959
 0.26883623 0.26857907 0.26818103 0.267821   0.26782787 0.26800033
 0.26805857 0.26791334 0.26784214 0.26799992 0.26837376 0.2685441
 0.26855835 0.26844317 0.26823115 0.267894   0.2677615  0.2678259
 0.267495   0.26664513 0.26561186 0.264906   0.26423937 0.26329166
 0.26266202 0.2623431  0.2620857  0.26191396 0.26167685 0.26169595
 0.26167393 0.2615726  0.2614563  0.2615622  0.2618547  0.26220775
 0.26247385 0.26237544 0.2622966  0.2624937  0.26268494 0.2624447
 0.2617705  0.26095372 0.26011655 0.25948805 0.2592037  0.25898945
 0.25860125 0.25787997 0.25733933 0.25718716 0.2569958  0.25661123
 0.25640345 0.25651795 0.25680774 0.25672388 0.25637197 0.25621918
 0.2562657  0.25587344 0.25470793 0.2535666  0.25471237 0.2569784 ]
