Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=58, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9042432.0
params:  10266.0
Trainable parameters:  10266
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.297309398651123
Epoch: 1, Steps: 59 | Train Loss: 1.0229250 Vali Loss: 2.1170149 Test Loss: 0.9034621
Validation loss decreased (inf --> 2.117015).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.977888822555542
Epoch: 2, Steps: 59 | Train Loss: 0.8045303 Vali Loss: 1.8225856 Test Loss: 0.7010397
Validation loss decreased (2.117015 --> 1.822586).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.8268628120422363
Epoch: 3, Steps: 59 | Train Loss: 0.7149369 Vali Loss: 1.7015343 Test Loss: 0.6130531
Validation loss decreased (1.822586 --> 1.701534).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.2735674381256104
Epoch: 4, Steps: 59 | Train Loss: 0.6741226 Vali Loss: 1.6441660 Test Loss: 0.5665050
Validation loss decreased (1.701534 --> 1.644166).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.6068837642669678
Epoch: 5, Steps: 59 | Train Loss: 0.6513758 Vali Loss: 1.6026123 Test Loss: 0.5367606
Validation loss decreased (1.644166 --> 1.602612).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.8919131755828857
Epoch: 6, Steps: 59 | Train Loss: 0.6363919 Vali Loss: 1.5712359 Test Loss: 0.5154338
Validation loss decreased (1.602612 --> 1.571236).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9337449073791504
Epoch: 7, Steps: 59 | Train Loss: 0.6253000 Vali Loss: 1.5517396 Test Loss: 0.4992390
Validation loss decreased (1.571236 --> 1.551740).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8014230728149414
Epoch: 8, Steps: 59 | Train Loss: 0.6169024 Vali Loss: 1.5404544 Test Loss: 0.4864372
Validation loss decreased (1.551740 --> 1.540454).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.1270954608917236
Epoch: 9, Steps: 59 | Train Loss: 0.6098302 Vali Loss: 1.5182695 Test Loss: 0.4761790
Validation loss decreased (1.540454 --> 1.518270).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6745431423187256
Epoch: 10, Steps: 59 | Train Loss: 0.6044923 Vali Loss: 1.5072510 Test Loss: 0.4679117
Validation loss decreased (1.518270 --> 1.507251).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.8929650783538818
Epoch: 11, Steps: 59 | Train Loss: 0.5999801 Vali Loss: 1.4971724 Test Loss: 0.4612409
Validation loss decreased (1.507251 --> 1.497172).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1762921810150146
Epoch: 12, Steps: 59 | Train Loss: 0.5964555 Vali Loss: 1.4884440 Test Loss: 0.4557681
Validation loss decreased (1.497172 --> 1.488444).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9167590141296387
Epoch: 13, Steps: 59 | Train Loss: 0.5933430 Vali Loss: 1.4820849 Test Loss: 0.4513746
Validation loss decreased (1.488444 --> 1.482085).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.7109403610229492
Epoch: 14, Steps: 59 | Train Loss: 0.5908158 Vali Loss: 1.4807802 Test Loss: 0.4477932
Validation loss decreased (1.482085 --> 1.480780).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.129438877105713
Epoch: 15, Steps: 59 | Train Loss: 0.5884965 Vali Loss: 1.4756694 Test Loss: 0.4448237
Validation loss decreased (1.480780 --> 1.475669).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.6916260719299316
Epoch: 16, Steps: 59 | Train Loss: 0.5868887 Vali Loss: 1.4684639 Test Loss: 0.4423783
Validation loss decreased (1.475669 --> 1.468464).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.9055614471435547
Epoch: 17, Steps: 59 | Train Loss: 0.5854168 Vali Loss: 1.4683096 Test Loss: 0.4404546
Validation loss decreased (1.468464 --> 1.468310).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7800915241241455
Epoch: 18, Steps: 59 | Train Loss: 0.5842748 Vali Loss: 1.4613395 Test Loss: 0.4388678
Validation loss decreased (1.468310 --> 1.461339).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6912858486175537
Epoch: 19, Steps: 59 | Train Loss: 0.5831290 Vali Loss: 1.4612532 Test Loss: 0.4375153
Validation loss decreased (1.461339 --> 1.461253).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8607659339904785
Epoch: 20, Steps: 59 | Train Loss: 0.5822168 Vali Loss: 1.4545709 Test Loss: 0.4364372
Validation loss decreased (1.461253 --> 1.454571).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.9417517185211182
Epoch: 21, Steps: 59 | Train Loss: 0.5815700 Vali Loss: 1.4554459 Test Loss: 0.4355016
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.7759058475494385
Epoch: 22, Steps: 59 | Train Loss: 0.5807930 Vali Loss: 1.4536178 Test Loss: 0.4347951
Validation loss decreased (1.454571 --> 1.453618).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.692993402481079
Epoch: 23, Steps: 59 | Train Loss: 0.5802809 Vali Loss: 1.4562118 Test Loss: 0.4342388
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.3852508068084717
Epoch: 24, Steps: 59 | Train Loss: 0.5797659 Vali Loss: 1.4497479 Test Loss: 0.4337676
Validation loss decreased (1.453618 --> 1.449748).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8220651149749756
Epoch: 25, Steps: 59 | Train Loss: 0.5795937 Vali Loss: 1.4507129 Test Loss: 0.4333553
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.3772470951080322
Epoch: 26, Steps: 59 | Train Loss: 0.5790779 Vali Loss: 1.4520776 Test Loss: 0.4330122
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.5243234634399414
Epoch: 27, Steps: 59 | Train Loss: 0.5787975 Vali Loss: 1.4535487 Test Loss: 0.4327197
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.18032169342041
Epoch: 28, Steps: 59 | Train Loss: 0.5785695 Vali Loss: 1.4489138 Test Loss: 0.4325109
Validation loss decreased (1.449748 --> 1.448914).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.3683440685272217
Epoch: 29, Steps: 59 | Train Loss: 0.5782546 Vali Loss: 1.4449313 Test Loss: 0.4323530
Validation loss decreased (1.448914 --> 1.444931).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.163177251815796
Epoch: 30, Steps: 59 | Train Loss: 0.5780217 Vali Loss: 1.4488926 Test Loss: 0.4321806
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.5527448654174805
Epoch: 31, Steps: 59 | Train Loss: 0.5779483 Vali Loss: 1.4497573 Test Loss: 0.4320504
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.2800893783569336
Epoch: 32, Steps: 59 | Train Loss: 0.5779399 Vali Loss: 1.4453158 Test Loss: 0.4319779
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.7537286281585693
Epoch: 33, Steps: 59 | Train Loss: 0.5777388 Vali Loss: 1.4422619 Test Loss: 0.4319070
Validation loss decreased (1.444931 --> 1.442262).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.2332046031951904
Epoch: 34, Steps: 59 | Train Loss: 0.5774654 Vali Loss: 1.4433640 Test Loss: 0.4318361
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9076118469238281
Epoch: 35, Steps: 59 | Train Loss: 0.5772558 Vali Loss: 1.4434485 Test Loss: 0.4317891
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6637649536132812
Epoch: 36, Steps: 59 | Train Loss: 0.5773202 Vali Loss: 1.4433980 Test Loss: 0.4317303
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.927490234375
Epoch: 37, Steps: 59 | Train Loss: 0.5770517 Vali Loss: 1.4403405 Test Loss: 0.4317112
Validation loss decreased (1.442262 --> 1.440341).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.101210594177246
Epoch: 38, Steps: 59 | Train Loss: 0.5771736 Vali Loss: 1.4422977 Test Loss: 0.4316976
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.8933844566345215
Epoch: 39, Steps: 59 | Train Loss: 0.5770750 Vali Loss: 1.4414641 Test Loss: 0.4316643
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.6378488540649414
Epoch: 40, Steps: 59 | Train Loss: 0.5769409 Vali Loss: 1.4427708 Test Loss: 0.4316633
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.3906469345092773
Epoch: 41, Steps: 59 | Train Loss: 0.5768721 Vali Loss: 1.4406574 Test Loss: 0.4316660
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.292130708694458
Epoch: 42, Steps: 59 | Train Loss: 0.5770109 Vali Loss: 1.4431636 Test Loss: 0.4316719
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.193117141723633
Epoch: 43, Steps: 59 | Train Loss: 0.5768704 Vali Loss: 1.4404938 Test Loss: 0.4316692
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.5716118812561035
Epoch: 44, Steps: 59 | Train Loss: 0.5768466 Vali Loss: 1.4463899 Test Loss: 0.4316683
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7541782855987549
Epoch: 45, Steps: 59 | Train Loss: 0.5767928 Vali Loss: 1.4407084 Test Loss: 0.4316659
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.1622231006622314
Epoch: 46, Steps: 59 | Train Loss: 0.5767788 Vali Loss: 1.4436836 Test Loss: 0.4316832
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.8339107036590576
Epoch: 47, Steps: 59 | Train Loss: 0.5764781 Vali Loss: 1.4462194 Test Loss: 0.4316848
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9742209911346436
Epoch: 48, Steps: 59 | Train Loss: 0.5766674 Vali Loss: 1.4407586 Test Loss: 0.4316821
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.5008788108825684
Epoch: 49, Steps: 59 | Train Loss: 0.5765606 Vali Loss: 1.4398801 Test Loss: 0.4317023
Validation loss decreased (1.440341 --> 1.439880).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.4539473056793213
Epoch: 50, Steps: 59 | Train Loss: 0.5764998 Vali Loss: 1.4425081 Test Loss: 0.4317133
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.038506031036377
Epoch: 51, Steps: 59 | Train Loss: 0.5767125 Vali Loss: 1.4448323 Test Loss: 0.4317137
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.3536510467529297
Epoch: 52, Steps: 59 | Train Loss: 0.5765811 Vali Loss: 1.4425958 Test Loss: 0.4317189
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.0858747959136963
Epoch: 53, Steps: 59 | Train Loss: 0.5765139 Vali Loss: 1.4434612 Test Loss: 0.4317334
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.8435955047607422
Epoch: 54, Steps: 59 | Train Loss: 0.5764174 Vali Loss: 1.4431109 Test Loss: 0.4317312
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.069674015045166
Epoch: 55, Steps: 59 | Train Loss: 0.5765639 Vali Loss: 1.4438424 Test Loss: 0.4317415
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.099083662033081
Epoch: 56, Steps: 59 | Train Loss: 0.5764678 Vali Loss: 1.4431109 Test Loss: 0.4317606
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.041222095489502
Epoch: 57, Steps: 59 | Train Loss: 0.5762765 Vali Loss: 1.4431448 Test Loss: 0.4317616
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.44587779045105
Epoch: 58, Steps: 59 | Train Loss: 0.5765496 Vali Loss: 1.4450071 Test Loss: 0.4317736
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.979039430618286
Epoch: 59, Steps: 59 | Train Loss: 0.5762363 Vali Loss: 1.4394743 Test Loss: 0.4317830
Validation loss decreased (1.439880 --> 1.439474).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.7527310848236084
Epoch: 60, Steps: 59 | Train Loss: 0.5763452 Vali Loss: 1.4382640 Test Loss: 0.4317949
Validation loss decreased (1.439474 --> 1.438264).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.0199825763702393
Epoch: 61, Steps: 59 | Train Loss: 0.5762110 Vali Loss: 1.4418166 Test Loss: 0.4317946
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.8452606201171875
Epoch: 62, Steps: 59 | Train Loss: 0.5764555 Vali Loss: 1.4434993 Test Loss: 0.4318109
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.7922236919403076
Epoch: 63, Steps: 59 | Train Loss: 0.5764827 Vali Loss: 1.4401733 Test Loss: 0.4318119
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.9207818508148193
Epoch: 64, Steps: 59 | Train Loss: 0.5763126 Vali Loss: 1.4404879 Test Loss: 0.4318156
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.0963945388793945
Epoch: 65, Steps: 59 | Train Loss: 0.5762938 Vali Loss: 1.4424572 Test Loss: 0.4318265
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.8830020427703857
Epoch: 66, Steps: 59 | Train Loss: 0.5761513 Vali Loss: 1.4383708 Test Loss: 0.4318285
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.8502843379974365
Epoch: 67, Steps: 59 | Train Loss: 0.5763257 Vali Loss: 1.4374917 Test Loss: 0.4318361
Validation loss decreased (1.438264 --> 1.437492).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.1761481761932373
Epoch: 68, Steps: 59 | Train Loss: 0.5763138 Vali Loss: 1.4414666 Test Loss: 0.4318413
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.496472120285034
Epoch: 69, Steps: 59 | Train Loss: 0.5761451 Vali Loss: 1.4418280 Test Loss: 0.4318473
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.185185194015503
Epoch: 70, Steps: 59 | Train Loss: 0.5760129 Vali Loss: 1.4435406 Test Loss: 0.4318583
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.8118908405303955
Epoch: 71, Steps: 59 | Train Loss: 0.5762350 Vali Loss: 1.4433618 Test Loss: 0.4318572
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.1279728412628174
Epoch: 72, Steps: 59 | Train Loss: 0.5761695 Vali Loss: 1.4397080 Test Loss: 0.4318655
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.4814059734344482
Epoch: 73, Steps: 59 | Train Loss: 0.5762186 Vali Loss: 1.4420532 Test Loss: 0.4318700
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.7492785453796387
Epoch: 74, Steps: 59 | Train Loss: 0.5762970 Vali Loss: 1.4389329 Test Loss: 0.4318764
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.4868428707122803
Epoch: 75, Steps: 59 | Train Loss: 0.5761239 Vali Loss: 1.4384738 Test Loss: 0.4318787
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.9731082916259766
Epoch: 76, Steps: 59 | Train Loss: 0.5763139 Vali Loss: 1.4401913 Test Loss: 0.4318815
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.974391460418701
Epoch: 77, Steps: 59 | Train Loss: 0.5761731 Vali Loss: 1.4428427 Test Loss: 0.4318887
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.8924479484558105
Epoch: 78, Steps: 59 | Train Loss: 0.5760755 Vali Loss: 1.4424062 Test Loss: 0.4318923
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.4109201431274414
Epoch: 79, Steps: 59 | Train Loss: 0.5762765 Vali Loss: 1.4383268 Test Loss: 0.4318962
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.6402318477630615
Epoch: 80, Steps: 59 | Train Loss: 0.5762415 Vali Loss: 1.4417835 Test Loss: 0.4318979
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.7963416576385498
Epoch: 81, Steps: 59 | Train Loss: 0.5761000 Vali Loss: 1.4391980 Test Loss: 0.4319074
EarlyStopping counter: 14 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.783318281173706
Epoch: 82, Steps: 59 | Train Loss: 0.5762463 Vali Loss: 1.4426057 Test Loss: 0.4319078
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.2347323894500732
Epoch: 83, Steps: 59 | Train Loss: 0.5761572 Vali Loss: 1.4391556 Test Loss: 0.4319128
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.8108174800872803
Epoch: 84, Steps: 59 | Train Loss: 0.5762109 Vali Loss: 1.4352612 Test Loss: 0.4319123
Validation loss decreased (1.437492 --> 1.435261).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.9070603847503662
Epoch: 85, Steps: 59 | Train Loss: 0.5761484 Vali Loss: 1.4390161 Test Loss: 0.4319198
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.6952362060546875
Epoch: 86, Steps: 59 | Train Loss: 0.5759750 Vali Loss: 1.4407722 Test Loss: 0.4319223
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.039344549179077
Epoch: 87, Steps: 59 | Train Loss: 0.5762516 Vali Loss: 1.4444833 Test Loss: 0.4319255
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.4803268909454346
Epoch: 88, Steps: 59 | Train Loss: 0.5761891 Vali Loss: 1.4411585 Test Loss: 0.4319264
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.9221177101135254
Epoch: 89, Steps: 59 | Train Loss: 0.5761741 Vali Loss: 1.4436259 Test Loss: 0.4319291
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.085895299911499
Epoch: 90, Steps: 59 | Train Loss: 0.5761533 Vali Loss: 1.4442401 Test Loss: 0.4319304
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.9900808334350586
Epoch: 91, Steps: 59 | Train Loss: 0.5762566 Vali Loss: 1.4340065 Test Loss: 0.4319331
Validation loss decreased (1.435261 --> 1.434006).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.1078624725341797
Epoch: 92, Steps: 59 | Train Loss: 0.5761474 Vali Loss: 1.4399049 Test Loss: 0.4319352
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.872041702270508
Epoch: 93, Steps: 59 | Train Loss: 0.5763241 Vali Loss: 1.4406077 Test Loss: 0.4319381
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.5538692474365234
Epoch: 94, Steps: 59 | Train Loss: 0.5763339 Vali Loss: 1.4417651 Test Loss: 0.4319403
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.8850352764129639
Epoch: 95, Steps: 59 | Train Loss: 0.5761864 Vali Loss: 1.4409100 Test Loss: 0.4319429
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.904205322265625
Epoch: 96, Steps: 59 | Train Loss: 0.5760800 Vali Loss: 1.4354995 Test Loss: 0.4319442
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.887521743774414
Epoch: 97, Steps: 59 | Train Loss: 0.5760590 Vali Loss: 1.4344928 Test Loss: 0.4319459
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.303632974624634
Epoch: 98, Steps: 59 | Train Loss: 0.5760842 Vali Loss: 1.4470739 Test Loss: 0.4319475
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.0073025226593018
Epoch: 99, Steps: 59 | Train Loss: 0.5762237 Vali Loss: 1.4418280 Test Loss: 0.4319489
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.7906849384307861
Epoch: 100, Steps: 59 | Train Loss: 0.5762126 Vali Loss: 1.4391552 Test Loss: 0.4319505
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43092042207717896, mae:0.4504207968711853, rse:0.6284207105636597, corr:[0.21962221 0.2328435  0.23517115 0.23175827 0.22877343 0.22797567
 0.22847514 0.22923538 0.22937894 0.22919795 0.22890218 0.22843732
 0.22787486 0.2273349  0.22693644 0.22686435 0.227029   0.22723751
 0.22730647 0.22718456 0.2270743  0.22733425 0.22799887 0.22884107
 0.22931008 0.2292613  0.22894923 0.22857253 0.22826862 0.22808164
 0.2279696  0.22786877 0.227757   0.22760467 0.22746171 0.22730434
 0.22720249 0.22710946 0.22705154 0.22706583 0.22720583 0.22724092
 0.22718398 0.22705248 0.22681212 0.22663969 0.22676648 0.22710757
 0.22726518 0.22704174 0.22638413 0.22569154 0.22522716 0.22483362
 0.22452459 0.22410536 0.22389144 0.22365853 0.2233336  0.22301705
 0.22271717 0.22252795 0.22244199 0.2225763  0.22278146 0.22295557
 0.22305942 0.22299637 0.22285312 0.22269832 0.22249924 0.2223357
 0.22198874 0.22155124 0.22115457 0.22086008 0.2206058  0.22033428
 0.22003773 0.21976337 0.21952514 0.21932603 0.21910295 0.21889253
 0.2186609  0.21833979 0.21799798 0.21769091 0.21748708 0.2173814
 0.2172734  0.2172114  0.21725875 0.21758197 0.21826762 0.2192969
 0.22046494 0.2213981  0.22195448 0.22213869 0.22208627 0.22198732
 0.22186343 0.22177134 0.22161438 0.22137043 0.22111847 0.22089522
 0.22076623 0.22075887 0.22077854 0.22083052 0.2208175  0.2206776
 0.22051106 0.22032213 0.22018066 0.22007155 0.22002542 0.22005235
 0.21997346 0.21968809 0.2192177  0.218862   0.21861403 0.21841268
 0.21815753 0.21784748 0.21745637 0.21704423 0.21675089 0.21657272
 0.21649191 0.21641451 0.21640158 0.21642959 0.21645018 0.21642303
 0.21636677 0.21634308 0.21636455 0.21634139 0.21616167 0.21587521
 0.21553293 0.21507367 0.21457884 0.21411663 0.21380919 0.21360561
 0.21344726 0.21335547 0.21331906 0.21333343 0.2133377  0.21334805
 0.21332216 0.21326643 0.21321839 0.21319212 0.21310659 0.21302006
 0.21288839 0.21273056 0.21263044 0.21258907 0.21263517 0.2129454
 0.21348517 0.21413922 0.2146633  0.21509111 0.2154018  0.21558473
 0.21560283 0.2155274  0.21544859 0.21540527 0.21536843 0.21528856
 0.21515651 0.21504152 0.2150364  0.21512789 0.21527101 0.21538465
 0.2153806  0.21527177 0.21520536 0.21516788 0.21513598 0.21499635
 0.2146289  0.21405093 0.21336605 0.21284768 0.21253549 0.2123489
 0.21221006 0.21216205 0.21210222 0.21196389 0.21180904 0.21169828
 0.21167599 0.21178815 0.21195734 0.21210544 0.21225052 0.2122674
 0.21204025 0.21155088 0.21106306 0.2108046  0.21075006 0.21091679
 0.21107234 0.2109493  0.21067685 0.21044308 0.21043825 0.21060379
 0.21070592 0.21064745 0.21041028 0.21007276 0.20969467 0.20939884
 0.20921077 0.20907745 0.20901534 0.20897576 0.20890957 0.20880565
 0.20864706 0.20832472 0.20796552 0.20773971 0.20783901 0.2081447
 0.20854387 0.20872562 0.2086775  0.2086355  0.20870821 0.20888223
 0.209162   0.20939377 0.20943215 0.20916489 0.2088334  0.20862041
 0.20862463 0.20870757 0.20888965 0.20900655 0.20910928 0.20917113
 0.20917201 0.20898588 0.20883653 0.20875552 0.20871654 0.2087055
 0.20866144 0.20850168 0.20821717 0.20792416 0.20765926 0.20751485
 0.20751804 0.20758992 0.2075696  0.20743483 0.20727223 0.20720622
 0.20722443 0.20720595 0.20703794 0.20676519 0.20648885 0.20633891
 0.20627946 0.20628726 0.20638971 0.20657182 0.20697933 0.20759898
 0.20835082 0.20899557 0.20942594 0.20956436 0.20957085 0.20956716
 0.20960423 0.20968252 0.20970334 0.20964956 0.20953159 0.20948832
 0.20954649 0.20962995 0.20972387 0.20975375 0.20969436 0.20959249
 0.20955156 0.20952098 0.20950258 0.2095618  0.20968454 0.20997253
 0.21042688 0.21088678 0.2111325  0.21126413 0.21125631 0.21108791
 0.21081649 0.21057446 0.21034119 0.21011315 0.20988223 0.20968723
 0.20956333 0.20956615 0.20964229 0.20988587 0.21011591 0.21021806
 0.21022418 0.21022062 0.2102739  0.21034697 0.21035986 0.21026218
 0.21011525 0.20996246 0.20984358 0.20971277 0.20954831 0.20921375
 0.20884228 0.20860544 0.20852953 0.2085544  0.20868023 0.20881143
 0.20883921 0.20885676 0.20894139 0.20912191 0.20920266 0.2092756
 0.20925587 0.2090368  0.20871538 0.2085489  0.20842004 0.20841596
 0.20844203 0.20829773 0.2079789  0.2076308  0.20739345 0.2071312
 0.20687015 0.20660627 0.2063254  0.20609263 0.20594686 0.20584774
 0.20566458 0.2053667  0.2050942  0.20490995 0.20490335 0.20498462
 0.20496647 0.20478483 0.20462835 0.20478353 0.20524819 0.20617074
 0.20727187 0.20806326 0.20844126 0.20837922 0.20815182 0.20779814
 0.20747396 0.20703363 0.20656048 0.20616633 0.20587747 0.20575008
 0.20581281 0.2059152  0.20600095 0.20607437 0.20621115 0.20650308
 0.2068034  0.20700383 0.207146   0.20743638 0.20781793 0.20839566
 0.2088621  0.2090587  0.2088974  0.20853274 0.20817104 0.20783527
 0.20761088 0.20755932 0.20753032 0.20743759 0.20734689 0.20726375
 0.2071805  0.20714504 0.20698404 0.20675075 0.2065478  0.20652011
 0.20663638 0.20689327 0.20709226 0.2076003  0.2082143  0.20890199
 0.2095169  0.20982449 0.20973036 0.20950018 0.20931543 0.209129
 0.20892689 0.20887981 0.20884073 0.20876758 0.20869306 0.20858549
 0.20848984 0.2083826  0.20830715 0.20825322 0.20819803 0.20815066
 0.20816416 0.20822479 0.20840314 0.20879067 0.20933741 0.21012408
 0.21086305 0.21128663 0.21120243 0.21091107 0.2104717  0.20999898
 0.209653   0.20955332 0.20962991 0.20976335 0.20985107 0.20984793
 0.20983453 0.20987447 0.21000165 0.21010916 0.21013172 0.21012765
 0.21012193 0.21014337 0.21016367 0.2100857  0.21012893 0.21028017
 0.21036097 0.21030553 0.21006393 0.20986089 0.20970319 0.20949128
 0.2092603  0.20903699 0.20874064 0.20846269 0.20820522 0.20806721
 0.2080076  0.20795979 0.2079124  0.20784076 0.20777853 0.20771514
 0.20768835 0.20772539 0.20793262 0.20842226 0.20925301 0.21028036
 0.2112038  0.21186349 0.21218301 0.21225354 0.21208432 0.21166433
 0.21114105 0.21066076 0.21023683 0.20984577 0.20956972 0.20948268
 0.2095462  0.20970877 0.20997742 0.21021464 0.21038349 0.21052995
 0.21068956 0.21095593 0.21133341 0.21174222 0.21219455 0.2125097
 0.21258363 0.2124035  0.21207339 0.2117552  0.21132463 0.21074261
 0.2102419  0.21001774 0.20982158 0.20958096 0.2093201  0.20923907
 0.2092999  0.20952283 0.2098279  0.21009079 0.2103153  0.2104944
 0.21054815 0.21051802 0.21037547 0.21021159 0.21008345 0.21008314
 0.20999128 0.20968916 0.20920885 0.20866688 0.20807043 0.20757425
 0.20712689 0.20664628 0.20615149 0.20560439 0.20518273 0.20491588
 0.20479324 0.20460592 0.20444615 0.20431697 0.2043877  0.204527
 0.2046942  0.20477448 0.20484762 0.20488277 0.2049644  0.20496786
 0.20490582 0.20492189 0.2046938  0.20432359 0.20391896 0.20327643
 0.20271134 0.20234667 0.20220868 0.20215729 0.20209949 0.20192447
 0.20178927 0.20156465 0.20148137 0.2014651  0.20146531 0.20148788
 0.2015294  0.20153515 0.20154436 0.2015304  0.20146206 0.20141238
 0.20128477 0.20093423 0.20040452 0.19980237 0.19912682 0.19834624
 0.19763975 0.19722858 0.19704637 0.19697534 0.19677307 0.1963767
 0.19604352 0.19585134 0.19580916 0.19593474 0.19597597 0.19584171
 0.19568737 0.19568372 0.19592254 0.1963625  0.19677424 0.19708486
 0.1971021  0.19668323 0.1962309  0.19591549 0.19557422 0.19506069
 0.19455244 0.1941429  0.1937562  0.19345932 0.19322139 0.19306818
 0.1930761  0.19298278 0.19288774 0.19277292 0.19264862 0.19269286
 0.19272251 0.19273807 0.19284236 0.19314967 0.19368255 0.19425185
 0.19459088 0.19461243 0.19419053 0.19390182 0.19339748 0.19286487
 0.19239597 0.19203247 0.19170403 0.19129592 0.19097161 0.19079366
 0.19074406 0.19069357 0.19064426 0.19046807 0.19050442 0.19068223
 0.19087774 0.19100004 0.19109127 0.19114344 0.19103289 0.19096695
 0.19076313 0.19021215 0.18935296 0.1883212  0.18741302 0.18660554
 0.18595651 0.18552059 0.1852136  0.18486202 0.18467753 0.1847324
 0.18498464 0.18501478 0.184848   0.18472078 0.18463983 0.18488157
 0.18533161 0.18582475 0.18621801 0.18653494 0.18673143 0.18682516
 0.1870524  0.18704839 0.18668047 0.18623872 0.18579708 0.18532187
 0.1851204  0.18508632 0.18489994 0.18431793 0.18365724 0.18317291
 0.18296224 0.18268785 0.18256775 0.18225114 0.18217577 0.1822264
 0.18242185 0.18252249 0.18299066 0.18352222 0.18274562 0.17442732]
