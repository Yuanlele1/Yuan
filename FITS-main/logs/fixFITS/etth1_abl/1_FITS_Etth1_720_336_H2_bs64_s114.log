Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=72, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=72, out_features=105, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6773760.0
params:  7665.0
Trainable parameters:  7665
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.185069561004639
Epoch: 1, Steps: 59 | Train Loss: 0.8087844 Vali Loss: 1.6123849 Test Loss: 0.7402875
Validation loss decreased (inf --> 1.612385).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.106571674346924
Epoch: 2, Steps: 59 | Train Loss: 0.6320128 Vali Loss: 1.4159672 Test Loss: 0.6118989
Validation loss decreased (1.612385 --> 1.415967).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.034832239151001
Epoch: 3, Steps: 59 | Train Loss: 0.5640099 Vali Loss: 1.3293443 Test Loss: 0.5509477
Validation loss decreased (1.415967 --> 1.329344).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.0607922077178955
Epoch: 4, Steps: 59 | Train Loss: 0.5271437 Vali Loss: 1.2782319 Test Loss: 0.5148875
Validation loss decreased (1.329344 --> 1.278232).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.114584922790527
Epoch: 5, Steps: 59 | Train Loss: 0.5045151 Vali Loss: 1.2421515 Test Loss: 0.4921507
Validation loss decreased (1.278232 --> 1.242151).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.242511034011841
Epoch: 6, Steps: 59 | Train Loss: 0.4898389 Vali Loss: 1.2270193 Test Loss: 0.4777857
Validation loss decreased (1.242151 --> 1.227019).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.896061420440674
Epoch: 7, Steps: 59 | Train Loss: 0.4797396 Vali Loss: 1.2145803 Test Loss: 0.4686943
Validation loss decreased (1.227019 --> 1.214580).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.004374027252197
Epoch: 8, Steps: 59 | Train Loss: 0.4733201 Vali Loss: 1.2094628 Test Loss: 0.4632078
Validation loss decreased (1.214580 --> 1.209463).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.963334083557129
Epoch: 9, Steps: 59 | Train Loss: 0.4681435 Vali Loss: 1.2063185 Test Loss: 0.4598936
Validation loss decreased (1.209463 --> 1.206318).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.9807779788970947
Epoch: 10, Steps: 59 | Train Loss: 0.4648904 Vali Loss: 1.2016292 Test Loss: 0.4580628
Validation loss decreased (1.206318 --> 1.201629).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.8952250480651855
Epoch: 11, Steps: 59 | Train Loss: 0.4621319 Vali Loss: 1.2000643 Test Loss: 0.4570835
Validation loss decreased (1.201629 --> 1.200064).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.869680643081665
Epoch: 12, Steps: 59 | Train Loss: 0.4600913 Vali Loss: 1.1989299 Test Loss: 0.4565944
Validation loss decreased (1.200064 --> 1.198930).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.288790702819824
Epoch: 13, Steps: 59 | Train Loss: 0.4587100 Vali Loss: 1.2001303 Test Loss: 0.4565298
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.304014205932617
Epoch: 14, Steps: 59 | Train Loss: 0.4574953 Vali Loss: 1.2049067 Test Loss: 0.4564884
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.117141962051392
Epoch: 15, Steps: 59 | Train Loss: 0.4568067 Vali Loss: 1.1995586 Test Loss: 0.4566832
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.142247676849365
Epoch: 16, Steps: 59 | Train Loss: 0.4556141 Vali Loss: 1.2001617 Test Loss: 0.4568649
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.218743085861206
Epoch: 17, Steps: 59 | Train Loss: 0.4549301 Vali Loss: 1.2055159 Test Loss: 0.4571571
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.094540119171143
Epoch: 18, Steps: 59 | Train Loss: 0.4541449 Vali Loss: 1.2077035 Test Loss: 0.4572958
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.063335657119751
Epoch: 19, Steps: 59 | Train Loss: 0.4537808 Vali Loss: 1.2034134 Test Loss: 0.4575878
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.0162482261657715
Epoch: 20, Steps: 59 | Train Loss: 0.4530539 Vali Loss: 1.2061478 Test Loss: 0.4578112
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.030360460281372
Epoch: 21, Steps: 59 | Train Loss: 0.4526950 Vali Loss: 1.2079104 Test Loss: 0.4578153
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.978947639465332
Epoch: 22, Steps: 59 | Train Loss: 0.4524294 Vali Loss: 1.2026142 Test Loss: 0.4580600
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.092709064483643
Epoch: 23, Steps: 59 | Train Loss: 0.4520762 Vali Loss: 1.2086442 Test Loss: 0.4583016
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.0509655475616455
Epoch: 24, Steps: 59 | Train Loss: 0.4518214 Vali Loss: 1.2116894 Test Loss: 0.4583735
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.032894611358643
Epoch: 25, Steps: 59 | Train Loss: 0.4514290 Vali Loss: 1.2116680 Test Loss: 0.4586256
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.181999683380127
Epoch: 26, Steps: 59 | Train Loss: 0.4511526 Vali Loss: 1.2069083 Test Loss: 0.4587732
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.302947759628296
Epoch: 27, Steps: 59 | Train Loss: 0.4507805 Vali Loss: 1.2071805 Test Loss: 0.4588563
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.584238290786743
Epoch: 28, Steps: 59 | Train Loss: 0.4508371 Vali Loss: 1.2113807 Test Loss: 0.4590829
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.413775682449341
Epoch: 29, Steps: 59 | Train Loss: 0.4505212 Vali Loss: 1.2093769 Test Loss: 0.4591804
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.535133600234985
Epoch: 30, Steps: 59 | Train Loss: 0.4503638 Vali Loss: 1.2087578 Test Loss: 0.4591984
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.582918643951416
Epoch: 31, Steps: 59 | Train Loss: 0.4500911 Vali Loss: 1.2140714 Test Loss: 0.4593100
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.398822546005249
Epoch: 32, Steps: 59 | Train Loss: 0.4501165 Vali Loss: 1.2137442 Test Loss: 0.4594240
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.45547428727149963, mae:0.45662710070610046, rse:0.6425160765647888, corr:[0.24346973 0.25382558 0.2572055  0.25658512 0.2544498  0.25206286
 0.2504035  0.24980928 0.2499358  0.25022304 0.25030428 0.25015718
 0.24985154 0.24933696 0.24864292 0.24788482 0.24731618 0.24703313
 0.24698462 0.24703857 0.24710284 0.24707799 0.24690215 0.24653707
 0.24605753 0.245545   0.24516527 0.24499002 0.24502139 0.24520019
 0.24541456 0.24553317 0.24555938 0.2454606  0.24520081 0.24503124
 0.24521218 0.24552594 0.24587367 0.2460991  0.24620895 0.2462884
 0.24635403 0.24641736 0.24645898 0.24656215 0.24661763 0.2465183
 0.24611796 0.2455569  0.2448604  0.24419175 0.24358793 0.2429852
 0.24246341 0.24205148 0.24172033 0.24142353 0.2412015  0.2410953
 0.24112988 0.24132532 0.24148385 0.24156635 0.24146158 0.2412355
 0.24099933 0.24068768 0.24051568 0.24047944 0.24051936 0.24056847
 0.24055508 0.24035662 0.23996139 0.23948364 0.2389663  0.23848493
 0.23817746 0.23807433 0.23808467 0.23812574 0.23809189 0.23797984
 0.23777431 0.23747696 0.23709337 0.23673086 0.23636001 0.2361004
 0.23604035 0.2362569  0.23663081 0.23721267 0.23786893 0.2384584
 0.23892355 0.23919599 0.23925939 0.23925619 0.23932675 0.23947665
 0.23969659 0.23989338 0.23996341 0.23987336 0.23966527 0.23945376
 0.23935197 0.23939236 0.23942567 0.23942927 0.23936008 0.239232
 0.23909381 0.23890443 0.23870958 0.23854634 0.2383786  0.23819655
 0.23798719 0.23770799 0.23738952 0.23703334 0.2366609  0.23621032
 0.23572475 0.23529613 0.23483926 0.23442371 0.23415662 0.23410104
 0.23420198 0.23427577 0.23429395 0.23422611 0.23401514 0.23376301
 0.23353635 0.23341213 0.2334052  0.23346506 0.23346078 0.23325184
 0.23280595 0.23213641 0.23135386 0.23056448 0.23002476 0.22971866
 0.22965209 0.22982113 0.23006913 0.23022273 0.23018868 0.23004669
 0.22982073 0.22956952 0.22938633 0.22933725 0.2292489  0.2293222
 0.22950637 0.22969706 0.22980547 0.22976694 0.22953351 0.22910278
 0.22859973 0.22818471 0.22791134 0.22783457 0.22790752 0.22807887
 0.22825193 0.22840679 0.22833042 0.22806291 0.22764476 0.2272359
 0.22689985 0.2267038  0.22669813 0.22687984 0.2271951  0.2275523
 0.22782305 0.22794403 0.22796713 0.22793359 0.22780742 0.22759213
 0.22731245 0.22701742 0.22661723 0.22604905 0.22537813 0.22465864
 0.22390899 0.22328775 0.22282897 0.22255464 0.2224259  0.22252348
 0.22273237 0.22304845 0.22343242 0.2237782  0.22407846 0.22431533
 0.22451587 0.22459146 0.22462113 0.22462368 0.2245282  0.22424228
 0.2236809  0.2229405  0.22207594 0.22122711 0.2206654  0.22034292
 0.22021724 0.22026065 0.22036974 0.22037445 0.22015505 0.2198765
 0.219615   0.2193784  0.21932173 0.21948196 0.21958502 0.21964343
 0.21961507 0.21944219 0.21919908 0.21894169 0.21879713 0.21869756
 0.21877111 0.21900262 0.2193832  0.21980828 0.22006369 0.22002313
 0.21964894 0.21908814 0.21837534 0.21759354 0.21687981 0.21644145
 0.21634111 0.21640836 0.21658505 0.21667068 0.21660955 0.21644486
 0.2163379  0.21628171 0.21646816 0.21693581 0.2176171  0.21810879
 0.21823624 0.21793415 0.21719186 0.21615072 0.21487172 0.21361563
 0.21273488 0.21231402 0.2122446  0.21235554 0.2123893  0.21229126
 0.21206217 0.21179199 0.21150859 0.21136165 0.21144398 0.21189748
 0.21253204 0.21306215 0.21346635 0.21365903 0.21364766 0.21325819
 0.2125694  0.21194424 0.21140273 0.21117778 0.21123631 0.21147595
 0.21172102 0.2117754  0.2114545  0.2107777  0.209853   0.20906088
 0.20869638 0.2088338  0.20952785 0.2103863  0.2110154  0.21111749
 0.2108232  0.20995757 0.20901442 0.20823446 0.2078869  0.20800073
 0.2085681  0.20912607 0.20893812 0.20762041 0.20542002 0.20268567
 0.20008662 0.19813558 0.1973492  0.19766964 0.19851552 0.19930904
 0.19956541 0.19870712 0.19693567 0.19512622 0.19350027 0.19300678
 0.19387023 0.19526376 0.19528049 0.19076677 0.17778656 0.1512805 ]
