Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=30, out_features=142, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3816960.0
params:  4402.0
Trainable parameters:  4402
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.7203712463378906
Epoch: 1, Steps: 64 | Train Loss: 0.9951158 Vali Loss: 1.9300985 Test Loss: 0.9652699
Validation loss decreased (inf --> 1.930099).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9817349910736084
Epoch: 2, Steps: 64 | Train Loss: 0.7492402 Vali Loss: 1.6424012 Test Loss: 0.7336160
Validation loss decreased (1.930099 --> 1.642401).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.283764362335205
Epoch: 3, Steps: 64 | Train Loss: 0.6389938 Vali Loss: 1.5112041 Test Loss: 0.6282065
Validation loss decreased (1.642401 --> 1.511204).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.055528402328491
Epoch: 4, Steps: 64 | Train Loss: 0.5826389 Vali Loss: 1.4403107 Test Loss: 0.5723701
Validation loss decreased (1.511204 --> 1.440311).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.6998472213745117
Epoch: 5, Steps: 64 | Train Loss: 0.5510501 Vali Loss: 1.4025825 Test Loss: 0.5409921
Validation loss decreased (1.440311 --> 1.402583).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.7896788120269775
Epoch: 6, Steps: 64 | Train Loss: 0.5321747 Vali Loss: 1.3715404 Test Loss: 0.5223157
Validation loss decreased (1.402583 --> 1.371540).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9322795867919922
Epoch: 7, Steps: 64 | Train Loss: 0.5206696 Vali Loss: 1.3632193 Test Loss: 0.5109549
Validation loss decreased (1.371540 --> 1.363219).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.7653238773345947
Epoch: 8, Steps: 64 | Train Loss: 0.5130255 Vali Loss: 1.3459886 Test Loss: 0.5037061
Validation loss decreased (1.363219 --> 1.345989).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.225519895553589
Epoch: 9, Steps: 64 | Train Loss: 0.5077121 Vali Loss: 1.3390757 Test Loss: 0.4989120
Validation loss decreased (1.345989 --> 1.339076).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.8148753643035889
Epoch: 10, Steps: 64 | Train Loss: 0.5043319 Vali Loss: 1.3317175 Test Loss: 0.4956511
Validation loss decreased (1.339076 --> 1.331717).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9541716575622559
Epoch: 11, Steps: 64 | Train Loss: 0.5013441 Vali Loss: 1.3311249 Test Loss: 0.4930964
Validation loss decreased (1.331717 --> 1.331125).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.0437328815460205
Epoch: 12, Steps: 64 | Train Loss: 0.4991668 Vali Loss: 1.3253888 Test Loss: 0.4912651
Validation loss decreased (1.331125 --> 1.325389).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.1891157627105713
Epoch: 13, Steps: 64 | Train Loss: 0.4972746 Vali Loss: 1.3187256 Test Loss: 0.4899053
Validation loss decreased (1.325389 --> 1.318726).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.196939468383789
Epoch: 14, Steps: 64 | Train Loss: 0.4960417 Vali Loss: 1.3237896 Test Loss: 0.4887672
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.316042423248291
Epoch: 15, Steps: 64 | Train Loss: 0.4948712 Vali Loss: 1.3169292 Test Loss: 0.4877578
Validation loss decreased (1.318726 --> 1.316929).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5251379013061523
Epoch: 16, Steps: 64 | Train Loss: 0.4938922 Vali Loss: 1.3100967 Test Loss: 0.4870075
Validation loss decreased (1.316929 --> 1.310097).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.7720870971679688
Epoch: 17, Steps: 64 | Train Loss: 0.4928648 Vali Loss: 1.3091360 Test Loss: 0.4863519
Validation loss decreased (1.310097 --> 1.309136).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.8225996494293213
Epoch: 18, Steps: 64 | Train Loss: 0.4922821 Vali Loss: 1.3153254 Test Loss: 0.4857599
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9087464809417725
Epoch: 19, Steps: 64 | Train Loss: 0.4917426 Vali Loss: 1.3080374 Test Loss: 0.4852922
Validation loss decreased (1.309136 --> 1.308037).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.1809279918670654
Epoch: 20, Steps: 64 | Train Loss: 0.4910549 Vali Loss: 1.3094721 Test Loss: 0.4848386
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.9638726711273193
Epoch: 21, Steps: 64 | Train Loss: 0.4905661 Vali Loss: 1.3086174 Test Loss: 0.4844479
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0987515449523926
Epoch: 22, Steps: 64 | Train Loss: 0.4900679 Vali Loss: 1.3149763 Test Loss: 0.4841066
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8867592811584473
Epoch: 23, Steps: 64 | Train Loss: 0.4896133 Vali Loss: 1.3063003 Test Loss: 0.4838182
Validation loss decreased (1.308037 --> 1.306300).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.0121612548828125
Epoch: 24, Steps: 64 | Train Loss: 0.4894454 Vali Loss: 1.3072659 Test Loss: 0.4835238
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.687910795211792
Epoch: 25, Steps: 64 | Train Loss: 0.4890759 Vali Loss: 1.3029693 Test Loss: 0.4832766
Validation loss decreased (1.306300 --> 1.302969).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8072569370269775
Epoch: 26, Steps: 64 | Train Loss: 0.4892010 Vali Loss: 1.3083338 Test Loss: 0.4830771
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.7798945903778076
Epoch: 27, Steps: 64 | Train Loss: 0.4887118 Vali Loss: 1.3080865 Test Loss: 0.4828653
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.633629083633423
Epoch: 28, Steps: 64 | Train Loss: 0.4882018 Vali Loss: 1.3038410 Test Loss: 0.4826900
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.4404947757720947
Epoch: 29, Steps: 64 | Train Loss: 0.4880198 Vali Loss: 1.2984957 Test Loss: 0.4825655
Validation loss decreased (1.302969 --> 1.298496).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.877168893814087
Epoch: 30, Steps: 64 | Train Loss: 0.4879126 Vali Loss: 1.3029132 Test Loss: 0.4824100
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.6957082748413086
Epoch: 31, Steps: 64 | Train Loss: 0.4875624 Vali Loss: 1.3008758 Test Loss: 0.4823039
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.803926706314087
Epoch: 32, Steps: 64 | Train Loss: 0.4876892 Vali Loss: 1.3013932 Test Loss: 0.4821731
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.5443198680877686
Epoch: 33, Steps: 64 | Train Loss: 0.4874141 Vali Loss: 1.3020159 Test Loss: 0.4820564
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.586322069168091
Epoch: 34, Steps: 64 | Train Loss: 0.4869982 Vali Loss: 1.2935189 Test Loss: 0.4819581
Validation loss decreased (1.298496 --> 1.293519).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9367198944091797
Epoch: 35, Steps: 64 | Train Loss: 0.4869621 Vali Loss: 1.2972964 Test Loss: 0.4818790
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.2881510257720947
Epoch: 36, Steps: 64 | Train Loss: 0.4870576 Vali Loss: 1.2993307 Test Loss: 0.4818011
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.091933012008667
Epoch: 37, Steps: 64 | Train Loss: 0.4868966 Vali Loss: 1.2977197 Test Loss: 0.4817115
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.213500738143921
Epoch: 38, Steps: 64 | Train Loss: 0.4867807 Vali Loss: 1.2998737 Test Loss: 0.4816383
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.3053224086761475
Epoch: 39, Steps: 64 | Train Loss: 0.4869415 Vali Loss: 1.3025844 Test Loss: 0.4815821
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.598457098007202
Epoch: 40, Steps: 64 | Train Loss: 0.4864608 Vali Loss: 1.2966185 Test Loss: 0.4815200
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.661839485168457
Epoch: 41, Steps: 64 | Train Loss: 0.4866235 Vali Loss: 1.2970794 Test Loss: 0.4814671
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.1717114448547363
Epoch: 42, Steps: 64 | Train Loss: 0.4865254 Vali Loss: 1.2986355 Test Loss: 0.4814251
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.987349033355713
Epoch: 43, Steps: 64 | Train Loss: 0.4864260 Vali Loss: 1.2981683 Test Loss: 0.4813673
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.5399317741394043
Epoch: 44, Steps: 64 | Train Loss: 0.4865999 Vali Loss: 1.2937870 Test Loss: 0.4813332
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.8604331016540527
Epoch: 45, Steps: 64 | Train Loss: 0.4862243 Vali Loss: 1.3002087 Test Loss: 0.4812836
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.5862512588500977
Epoch: 46, Steps: 64 | Train Loss: 0.4861580 Vali Loss: 1.2963725 Test Loss: 0.4812467
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 6.6093950271606445
Epoch: 47, Steps: 64 | Train Loss: 0.4860429 Vali Loss: 1.3011489 Test Loss: 0.4812142
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.134053468704224
Epoch: 48, Steps: 64 | Train Loss: 0.4864242 Vali Loss: 1.2960325 Test Loss: 0.4811919
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.537743091583252
Epoch: 49, Steps: 64 | Train Loss: 0.4858554 Vali Loss: 1.3026795 Test Loss: 0.4811496
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3806536197662354
Epoch: 50, Steps: 64 | Train Loss: 0.4856335 Vali Loss: 1.3058093 Test Loss: 0.4811233
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.0885326862335205
Epoch: 51, Steps: 64 | Train Loss: 0.4859957 Vali Loss: 1.2954214 Test Loss: 0.4811021
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.8754448890686035
Epoch: 52, Steps: 64 | Train Loss: 0.4858282 Vali Loss: 1.2968923 Test Loss: 0.4810671
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.0208780765533447
Epoch: 53, Steps: 64 | Train Loss: 0.4858298 Vali Loss: 1.2916415 Test Loss: 0.4810469
Validation loss decreased (1.293519 --> 1.291641).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9651379585266113
Epoch: 54, Steps: 64 | Train Loss: 0.4861205 Vali Loss: 1.2979738 Test Loss: 0.4810283
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.7656457424163818
Epoch: 55, Steps: 64 | Train Loss: 0.4859289 Vali Loss: 1.2956651 Test Loss: 0.4810185
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7379848957061768
Epoch: 56, Steps: 64 | Train Loss: 0.4855719 Vali Loss: 1.2889383 Test Loss: 0.4809893
Validation loss decreased (1.291641 --> 1.288938).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.041727066040039
Epoch: 57, Steps: 64 | Train Loss: 0.4859704 Vali Loss: 1.2930638 Test Loss: 0.4809849
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.577308177947998
Epoch: 58, Steps: 64 | Train Loss: 0.4858265 Vali Loss: 1.2981453 Test Loss: 0.4809631
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.93679141998291
Epoch: 59, Steps: 64 | Train Loss: 0.4858916 Vali Loss: 1.2928749 Test Loss: 0.4809503
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.7419471740722656
Epoch: 60, Steps: 64 | Train Loss: 0.4856881 Vali Loss: 1.2902755 Test Loss: 0.4809278
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.563117027282715
Epoch: 61, Steps: 64 | Train Loss: 0.4858661 Vali Loss: 1.2972040 Test Loss: 0.4809232
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.9790735244750977
Epoch: 62, Steps: 64 | Train Loss: 0.4853792 Vali Loss: 1.2946529 Test Loss: 0.4809107
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.8440115451812744
Epoch: 63, Steps: 64 | Train Loss: 0.4856684 Vali Loss: 1.2974707 Test Loss: 0.4809008
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.409862756729126
Epoch: 64, Steps: 64 | Train Loss: 0.4854782 Vali Loss: 1.3009064 Test Loss: 0.4808870
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.939234733581543
Epoch: 65, Steps: 64 | Train Loss: 0.4857410 Vali Loss: 1.2961367 Test Loss: 0.4808744
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.500269651412964
Epoch: 66, Steps: 64 | Train Loss: 0.4858688 Vali Loss: 1.2984709 Test Loss: 0.4808650
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.217679262161255
Epoch: 67, Steps: 64 | Train Loss: 0.4856923 Vali Loss: 1.2914480 Test Loss: 0.4808636
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.3819336891174316
Epoch: 68, Steps: 64 | Train Loss: 0.4854682 Vali Loss: 1.2952157 Test Loss: 0.4808446
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.6254355907440186
Epoch: 69, Steps: 64 | Train Loss: 0.4854107 Vali Loss: 1.2967272 Test Loss: 0.4808405
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.829042911529541
Epoch: 70, Steps: 64 | Train Loss: 0.4853731 Vali Loss: 1.2965378 Test Loss: 0.4808306
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.1564981937408447
Epoch: 71, Steps: 64 | Train Loss: 0.4853116 Vali Loss: 1.3019123 Test Loss: 0.4808253
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.5748913288116455
Epoch: 72, Steps: 64 | Train Loss: 0.4852306 Vali Loss: 1.2971821 Test Loss: 0.4808291
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.888185501098633
Epoch: 73, Steps: 64 | Train Loss: 0.4855334 Vali Loss: 1.2938019 Test Loss: 0.4808136
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.166081428527832
Epoch: 74, Steps: 64 | Train Loss: 0.4855365 Vali Loss: 1.2936221 Test Loss: 0.4808100
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.536543130874634
Epoch: 75, Steps: 64 | Train Loss: 0.4855762 Vali Loss: 1.3042626 Test Loss: 0.4808006
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.42258358001709
Epoch: 76, Steps: 64 | Train Loss: 0.4852332 Vali Loss: 1.2992243 Test Loss: 0.4807969
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.48046645522117615, mae:0.4459024965763092, rse:0.6599082946777344, corr:[0.25409773 0.25378233 0.251597   0.25252742 0.24893811 0.2471227
 0.24758837 0.24645458 0.24595898 0.24653138 0.24544917 0.24478048
 0.24495985 0.24442296 0.2442478  0.24471052 0.24491842 0.24495213
 0.24535632 0.2450514  0.24392346 0.24364105 0.24336107 0.24238352
 0.24077885 0.24019298 0.24013875 0.24024566 0.24039258 0.24083963
 0.24113281 0.24091166 0.24097651 0.24084292 0.24040942 0.24040964
 0.24029998 0.24014735 0.2403638  0.24045694 0.24081497 0.24138658
 0.24180774 0.24171884 0.24149166 0.24143583 0.24124195 0.24069966
 0.23958535 0.23856442 0.23781477 0.23732142 0.23635136 0.23544097
 0.23548272 0.23526011 0.23503341 0.2350172  0.23466535 0.23454742
 0.23459348 0.23467176 0.23457767 0.23450826 0.23477815 0.23498246
 0.23532186 0.23523939 0.2348199  0.23473601 0.23432845 0.2331495
 0.23154922 0.23056991 0.22982484 0.2297418  0.22954547 0.22938295
 0.22978714 0.22983839 0.22976442 0.22947007 0.229167   0.22895654
 0.22870637 0.22867908 0.22890864 0.22896218 0.22890963 0.2289944
 0.2292198  0.22915816 0.22880735 0.22901058 0.2290939  0.22857407
 0.22754155 0.22688358 0.22645582 0.22628681 0.22644915 0.22651567
 0.22693025 0.22703227 0.2268783  0.22677425 0.22657715 0.22626407
 0.226108   0.2260771  0.22611415 0.22601925 0.22632137 0.22642973
 0.22659305 0.22674496 0.22670045 0.22676563 0.22642206 0.22523458
 0.2233912  0.22197978 0.22084492 0.22002012 0.219722   0.219786
 0.22067173 0.22102648 0.22099526 0.22093476 0.2207039  0.22045922
 0.22027697 0.22015429 0.22015125 0.22008604 0.22032174 0.2205036
 0.22071323 0.2208128  0.22069311 0.22093692 0.2208299  0.21979488
 0.21811324 0.2168773  0.21592736 0.21523029 0.21481386 0.21480137
 0.21585199 0.21622938 0.21616915 0.21614869 0.21609205 0.2158866
 0.2155505  0.2153301  0.2151916  0.21506146 0.21516064 0.21527359
 0.21553066 0.2156789  0.21546213 0.21555802 0.21545365 0.21438098
 0.21260528 0.21141748 0.21093418 0.21100084 0.2104182  0.21012029
 0.21109785 0.21142444 0.21116708 0.21109556 0.21125107 0.21124502
 0.21105497 0.210835   0.21080504 0.21071376 0.21079199 0.21119855
 0.21167691 0.21195753 0.21204269 0.21237214 0.21241744 0.21137282
 0.20952772 0.20869869 0.20808962 0.20761463 0.2073387  0.20767109
 0.20865358 0.20899473 0.20889586 0.20864837 0.20842071 0.20825624
 0.20807342 0.20794037 0.20782678 0.2076663  0.2076593  0.20773952
 0.20802172 0.20812929 0.20800066 0.20819037 0.20810734 0.20710431
 0.20561245 0.20490426 0.20463067 0.20502248 0.2056421  0.20658676
 0.20825683 0.20922504 0.20946492 0.20931649 0.20912713 0.20905912
 0.20896707 0.20888719 0.20890461 0.20879664 0.2087633  0.20879804
 0.20894882 0.20906816 0.20926404 0.20961131 0.2095087  0.20847079
 0.20671587 0.2053221  0.20459238 0.20460738 0.2046441  0.20517519
 0.2066782  0.20746784 0.20773916 0.20788898 0.20769563 0.20737916
 0.20710605 0.20683597 0.20674169 0.20665339 0.2065701  0.20660913
 0.20696336 0.20703106 0.2068495  0.20695184 0.20681104 0.20583375
 0.20444383 0.20360263 0.20303121 0.20352693 0.20402211 0.20475994
 0.20646    0.20736308 0.20741369 0.20744218 0.20744066 0.20713991
 0.20679495 0.20665492 0.20649403 0.20637691 0.20636797 0.20655178
 0.20695688 0.20715758 0.20715536 0.20755659 0.20790969 0.20721248
 0.20589823 0.20535105 0.20577757 0.20656678 0.20692168 0.20759657
 0.20900053 0.20945415 0.20938778 0.20911779 0.20877337 0.20848188
 0.20812179 0.20770559 0.20748577 0.2072862  0.20730008 0.20721191
 0.20737486 0.20749761 0.20746507 0.20776035 0.20775515 0.20653692
 0.20514436 0.2041105  0.20332034 0.20375842 0.20435339 0.20418215
 0.20557772 0.20621091 0.20581567 0.20566778 0.20542371 0.20462263
 0.20421849 0.20440231 0.2039267  0.20311886 0.2030748  0.2035393
 0.20333126 0.20319696 0.2038876  0.20302978 0.20314907 0.20420973]
