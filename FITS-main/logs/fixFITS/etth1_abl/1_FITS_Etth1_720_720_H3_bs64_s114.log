Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=103, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=103, out_features=206, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19011328.0
params:  21424.0
Trainable parameters:  21424
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.319509267807007
Epoch: 1, Steps: 56 | Train Loss: 0.9336379 Vali Loss: 1.9340935 Test Loss: 0.7751529
Validation loss decreased (inf --> 1.934093).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.353042364120483
Epoch: 2, Steps: 56 | Train Loss: 0.7519445 Vali Loss: 1.7359438 Test Loss: 0.6494613
Validation loss decreased (1.934093 --> 1.735944).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.323793649673462
Epoch: 3, Steps: 56 | Train Loss: 0.6915069 Vali Loss: 1.6575899 Test Loss: 0.5947843
Validation loss decreased (1.735944 --> 1.657590).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.407177209854126
Epoch: 4, Steps: 56 | Train Loss: 0.6602945 Vali Loss: 1.6079412 Test Loss: 0.5589486
Validation loss decreased (1.657590 --> 1.607941).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.744294166564941
Epoch: 5, Steps: 56 | Train Loss: 0.6393737 Vali Loss: 1.5663974 Test Loss: 0.5322514
Validation loss decreased (1.607941 --> 1.566397).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.751540660858154
Epoch: 6, Steps: 56 | Train Loss: 0.6235175 Vali Loss: 1.5397412 Test Loss: 0.5115370
Validation loss decreased (1.566397 --> 1.539741).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.2907326221466064
Epoch: 7, Steps: 56 | Train Loss: 0.6123936 Vali Loss: 1.5207951 Test Loss: 0.4954604
Validation loss decreased (1.539741 --> 1.520795).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.426039457321167
Epoch: 8, Steps: 56 | Train Loss: 0.6031022 Vali Loss: 1.4975239 Test Loss: 0.4829957
Validation loss decreased (1.520795 --> 1.497524).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.36717677116394
Epoch: 9, Steps: 56 | Train Loss: 0.5957532 Vali Loss: 1.4907818 Test Loss: 0.4730496
Validation loss decreased (1.497524 --> 1.490782).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.781267404556274
Epoch: 10, Steps: 56 | Train Loss: 0.5899204 Vali Loss: 1.4741150 Test Loss: 0.4652539
Validation loss decreased (1.490782 --> 1.474115).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.2426722049713135
Epoch: 11, Steps: 56 | Train Loss: 0.5849692 Vali Loss: 1.4725618 Test Loss: 0.4590285
Validation loss decreased (1.474115 --> 1.472562).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.154006004333496
Epoch: 12, Steps: 56 | Train Loss: 0.5816438 Vali Loss: 1.4641430 Test Loss: 0.4542401
Validation loss decreased (1.472562 --> 1.464143).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.651662588119507
Epoch: 13, Steps: 56 | Train Loss: 0.5783166 Vali Loss: 1.4620013 Test Loss: 0.4503324
Validation loss decreased (1.464143 --> 1.462001).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.276424884796143
Epoch: 14, Steps: 56 | Train Loss: 0.5761701 Vali Loss: 1.4551222 Test Loss: 0.4473527
Validation loss decreased (1.462001 --> 1.455122).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.501357555389404
Epoch: 15, Steps: 56 | Train Loss: 0.5737928 Vali Loss: 1.4530679 Test Loss: 0.4449351
Validation loss decreased (1.455122 --> 1.453068).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.028564691543579
Epoch: 16, Steps: 56 | Train Loss: 0.5716311 Vali Loss: 1.4477340 Test Loss: 0.4432448
Validation loss decreased (1.453068 --> 1.447734).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.091314077377319
Epoch: 17, Steps: 56 | Train Loss: 0.5702715 Vali Loss: 1.4455882 Test Loss: 0.4416929
Validation loss decreased (1.447734 --> 1.445588).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.326496839523315
Epoch: 18, Steps: 56 | Train Loss: 0.5692293 Vali Loss: 1.4393234 Test Loss: 0.4405521
Validation loss decreased (1.445588 --> 1.439323).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.730190277099609
Epoch: 19, Steps: 56 | Train Loss: 0.5677577 Vali Loss: 1.4450412 Test Loss: 0.4397854
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.137536525726318
Epoch: 20, Steps: 56 | Train Loss: 0.5667269 Vali Loss: 1.4405653 Test Loss: 0.4390538
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.87231183052063
Epoch: 21, Steps: 56 | Train Loss: 0.5661875 Vali Loss: 1.4374019 Test Loss: 0.4385197
Validation loss decreased (1.439323 --> 1.437402).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.539626836776733
Epoch: 22, Steps: 56 | Train Loss: 0.5656322 Vali Loss: 1.4417970 Test Loss: 0.4379926
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.307286977767944
Epoch: 23, Steps: 56 | Train Loss: 0.5650084 Vali Loss: 1.4388789 Test Loss: 0.4376976
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.33363151550293
Epoch: 24, Steps: 56 | Train Loss: 0.5647356 Vali Loss: 1.4372318 Test Loss: 0.4375142
Validation loss decreased (1.437402 --> 1.437232).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.098790645599365
Epoch: 25, Steps: 56 | Train Loss: 0.5638232 Vali Loss: 1.4453089 Test Loss: 0.4372682
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.635058879852295
Epoch: 26, Steps: 56 | Train Loss: 0.5634181 Vali Loss: 1.4440088 Test Loss: 0.4370960
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.617276668548584
Epoch: 27, Steps: 56 | Train Loss: 0.5633794 Vali Loss: 1.4361064 Test Loss: 0.4370377
Validation loss decreased (1.437232 --> 1.436106).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.281764030456543
Epoch: 28, Steps: 56 | Train Loss: 0.5628247 Vali Loss: 1.4365268 Test Loss: 0.4369777
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.463169574737549
Epoch: 29, Steps: 56 | Train Loss: 0.5622819 Vali Loss: 1.4382428 Test Loss: 0.4369070
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.455125570297241
Epoch: 30, Steps: 56 | Train Loss: 0.5618979 Vali Loss: 1.4335964 Test Loss: 0.4368833
Validation loss decreased (1.436106 --> 1.433596).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.611696720123291
Epoch: 31, Steps: 56 | Train Loss: 0.5613215 Vali Loss: 1.4379168 Test Loss: 0.4368782
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.299082279205322
Epoch: 32, Steps: 56 | Train Loss: 0.5616320 Vali Loss: 1.4383878 Test Loss: 0.4367748
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.52396559715271
Epoch: 33, Steps: 56 | Train Loss: 0.5614080 Vali Loss: 1.4330544 Test Loss: 0.4368441
Validation loss decreased (1.433596 --> 1.433054).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.169316291809082
Epoch: 34, Steps: 56 | Train Loss: 0.5612627 Vali Loss: 1.4359903 Test Loss: 0.4368133
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.0765910148620605
Epoch: 35, Steps: 56 | Train Loss: 0.5609500 Vali Loss: 1.4308878 Test Loss: 0.4368235
Validation loss decreased (1.433054 --> 1.430888).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.398701429367065
Epoch: 36, Steps: 56 | Train Loss: 0.5611496 Vali Loss: 1.4329307 Test Loss: 0.4368563
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.130456209182739
Epoch: 37, Steps: 56 | Train Loss: 0.5606873 Vali Loss: 1.4331474 Test Loss: 0.4368698
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.307723045349121
Epoch: 38, Steps: 56 | Train Loss: 0.5604726 Vali Loss: 1.4435605 Test Loss: 0.4369195
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.781853914260864
Epoch: 39, Steps: 56 | Train Loss: 0.5602896 Vali Loss: 1.4380525 Test Loss: 0.4368855
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.412599802017212
Epoch: 40, Steps: 56 | Train Loss: 0.5602387 Vali Loss: 1.4321704 Test Loss: 0.4369313
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.418895244598389
Epoch: 41, Steps: 56 | Train Loss: 0.5600889 Vali Loss: 1.4342320 Test Loss: 0.4369266
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.279223680496216
Epoch: 42, Steps: 56 | Train Loss: 0.5599227 Vali Loss: 1.4334197 Test Loss: 0.4369474
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.243537425994873
Epoch: 43, Steps: 56 | Train Loss: 0.5601645 Vali Loss: 1.4416759 Test Loss: 0.4369272
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.26222562789917
Epoch: 44, Steps: 56 | Train Loss: 0.5598037 Vali Loss: 1.4354731 Test Loss: 0.4369582
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.536147117614746
Epoch: 45, Steps: 56 | Train Loss: 0.5599339 Vali Loss: 1.4401993 Test Loss: 0.4369866
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.994588851928711
Epoch: 46, Steps: 56 | Train Loss: 0.5595943 Vali Loss: 1.4411056 Test Loss: 0.4369928
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.572717905044556
Epoch: 47, Steps: 56 | Train Loss: 0.5595921 Vali Loss: 1.4368608 Test Loss: 0.4370297
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.0966198444366455
Epoch: 48, Steps: 56 | Train Loss: 0.5596095 Vali Loss: 1.4346142 Test Loss: 0.4370497
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 4.0614025592803955
Epoch: 49, Steps: 56 | Train Loss: 0.5592863 Vali Loss: 1.4391646 Test Loss: 0.4370629
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.882026672363281
Epoch: 50, Steps: 56 | Train Loss: 0.5594705 Vali Loss: 1.4361746 Test Loss: 0.4370790
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.435299634933472
Epoch: 51, Steps: 56 | Train Loss: 0.5596013 Vali Loss: 1.4339639 Test Loss: 0.4371035
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.6240074634552
Epoch: 52, Steps: 56 | Train Loss: 0.5592521 Vali Loss: 1.4350536 Test Loss: 0.4370906
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.375483512878418
Epoch: 53, Steps: 56 | Train Loss: 0.5591735 Vali Loss: 1.4409950 Test Loss: 0.4371444
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 4.497628688812256
Epoch: 54, Steps: 56 | Train Loss: 0.5588198 Vali Loss: 1.4382622 Test Loss: 0.4371584
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 5.049553155899048
Epoch: 55, Steps: 56 | Train Loss: 0.5589349 Vali Loss: 1.4369597 Test Loss: 0.4371468
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43592435121536255, mae:0.4589085578918457, rse:0.6320589184761047, corr:[0.21575738 0.22961439 0.23402368 0.23111644 0.2278677  0.22687565
 0.22757436 0.22891054 0.2296692  0.22952035 0.22877313 0.22796035
 0.22741681 0.22709318 0.22686476 0.22658092 0.22633341 0.22626348
 0.2262923  0.22635359 0.22643778 0.22651821 0.2264722  0.22656979
 0.22697525 0.22762075 0.22821896 0.22844288 0.2282134  0.22759731
 0.22692892 0.22648494 0.22636755 0.22643916 0.2264662  0.22635105
 0.22626019 0.22612016 0.22600085 0.2259668  0.22603132 0.22605665
 0.22613473 0.2264192  0.22692837 0.22752328 0.2280472  0.2282968
 0.2279939  0.22726008 0.22625965 0.22533159 0.22470392 0.22420889
 0.22375606 0.22319895 0.22283803 0.22248581 0.22213465 0.22185491
 0.22172809 0.22170715 0.22168508 0.22167528 0.22165012 0.22163156
 0.22167341 0.22175616 0.22183567 0.22181323 0.22157317 0.22135441
 0.22098604 0.22058211 0.22027312 0.22000231 0.21967833 0.21917999
 0.21862823 0.2182165  0.21798788 0.21790832 0.21786349 0.21773455
 0.21748553 0.21709734 0.21664062 0.21627885 0.21601255 0.21587549
 0.21586488 0.21613804 0.21654865 0.21711162 0.21778026 0.21860717
 0.21951874 0.22028215 0.22071502 0.2209238  0.22096866 0.22099613
 0.221021   0.22101405 0.22092478 0.22062224 0.22015043 0.21968
 0.21941526 0.2194128  0.21950927 0.2196323  0.21971385 0.2196342
 0.21955855 0.21955805 0.21963246 0.21972227 0.21977414 0.21978489
 0.21963121 0.2192271  0.21866378 0.2182349  0.2178592  0.21751994
 0.21716775 0.21688934 0.21663281 0.21640779 0.2162705  0.21614948
 0.21603349 0.21583332 0.21566015 0.21555246 0.21546578 0.21534824
 0.21522318 0.21518101 0.21520658 0.21517675 0.21497728 0.21471964
 0.21438725 0.21393463 0.21344651 0.21298581 0.21272203 0.21262282
 0.21259934 0.2126899  0.21279825 0.2127615  0.21253267 0.2121972
 0.21182437 0.21155946 0.21154535 0.21178178 0.2120373  0.21224043
 0.21232502 0.21238096 0.21242395 0.21257046 0.2127379  0.2130989
 0.21355486 0.21397938 0.2141911  0.21429025 0.21422201 0.21403497
 0.21377185 0.2135668  0.21338777 0.21324131 0.21312189 0.21302457
 0.21293743 0.21288615 0.2128986  0.21298431 0.21315128 0.21334562
 0.213557   0.21379235 0.21404478 0.21416627 0.21410733 0.21385339
 0.21346004 0.21297239 0.21244454 0.21194646 0.21153675 0.21123219
 0.21103331 0.21102163 0.21112613 0.21116245 0.21112455 0.21100885
 0.21093664 0.21102937 0.21126403 0.21151972 0.21174161 0.21179533
 0.21159284 0.21117131 0.21075313 0.21042916 0.21021341 0.21016863
 0.21017054 0.21011394 0.20994154 0.20977458 0.20974168 0.20979044
 0.20983852 0.20979343 0.20957997 0.20927174 0.20892787 0.20868514
 0.20860197 0.2085882  0.20853315 0.20838057 0.20805189 0.20766312
 0.20740199 0.20737171 0.20756012 0.2078415  0.20810883 0.20826396
 0.20833679 0.20836149 0.20845339 0.2086384  0.20872895 0.2086157
 0.20841219 0.2081715  0.20790346 0.20758584 0.20732453 0.20716517
 0.20712158 0.20705043 0.20702682 0.20705457 0.20718664 0.20738386
 0.20758937 0.20762467 0.2076135  0.20751098 0.20736179 0.2072199
 0.2071317  0.20704852 0.20688875 0.20664811 0.20631303 0.20606096
 0.20598462 0.20601018 0.20595163 0.2057921  0.20561723 0.20555384
 0.2056587  0.20594797 0.20616716 0.20625935 0.20617573 0.20604053
 0.20584089 0.20579046 0.20588869 0.20612125 0.20649801 0.2069671
 0.20735538 0.20758002 0.20766711 0.20775735 0.2078975  0.20801647
 0.20803677 0.20800227 0.20789814 0.20782454 0.20788623 0.20807277
 0.20824194 0.20826128 0.20815939 0.20806573 0.2080405  0.20811677
 0.20837352 0.20858587 0.208694   0.20877822 0.20880565 0.2089278
 0.20918025 0.20937504 0.20927408 0.20889726 0.20833237 0.20774011
 0.20723356 0.20688912 0.20665026 0.2063926  0.20612834 0.20590132
 0.20582157 0.20591022 0.20608534 0.20640782 0.20664768 0.20672984
 0.20667593 0.2065545  0.20649113 0.20653515 0.20656429 0.2065455
 0.206397   0.2060492  0.20559317 0.2052107  0.20492662 0.20469075
 0.20445238 0.20416819 0.20382324 0.20355137 0.203515   0.20372148
 0.20397311 0.2041521  0.20421277 0.20420124 0.2041597  0.20428906
 0.2044729  0.20461927 0.20469692 0.20472515 0.2045494  0.20429018
 0.20405056 0.20378296 0.2034817  0.20313586 0.20269708 0.20213959
 0.20157546 0.20120595 0.20105726 0.20097747 0.20082018 0.20050828
 0.19993207 0.19927523 0.19882396 0.198663   0.19882154 0.19907454
 0.19917668 0.19898403 0.1987507  0.19884194 0.19930923 0.20026752
 0.20133272 0.20199423 0.2020375  0.20163086 0.20112303 0.20056558
 0.2000537  0.19953609 0.19894838 0.19828442 0.19763009 0.19725919
 0.19731277 0.1976494  0.19811372 0.19844511 0.19853373 0.19846624
 0.19838063 0.19840775 0.19872975 0.1993279  0.19981447 0.20015039
 0.200262   0.20016012 0.19999777 0.19993694 0.20003402 0.20003915
 0.19994985 0.19972594 0.19946244 0.19921604 0.19909999 0.19912632
 0.19911303 0.19901337 0.1986503  0.19825898 0.1980255  0.19812348
 0.19842349 0.19880475 0.1989922  0.19937012 0.19973361 0.20000213
 0.20022795 0.20027782 0.20009159 0.19994485 0.19986017 0.19972982
 0.1995785  0.19947778 0.19935542 0.19924161 0.19908457 0.1989259
 0.19874115 0.19859838 0.1985632  0.19855092 0.19845857 0.198243
 0.19803765 0.19793895 0.19815479 0.19877058 0.19963498 0.20050022
 0.20099777 0.20099328 0.20058247 0.19998296 0.1994727  0.19908473
 0.19884424 0.19869342 0.19852015 0.19842313 0.19837113 0.19847448
 0.1986465  0.19880567 0.19888544 0.19889648 0.19885868 0.19887905
 0.19893816 0.19905385 0.19912054 0.19901884 0.1989822  0.19890364
 0.19879866 0.19870685 0.19852892 0.19830698 0.19804433 0.19771251
 0.19729707 0.19702618 0.19681673 0.19670156 0.19666748 0.1966678
 0.19663522 0.1965121  0.19636387 0.19624023 0.19619876 0.19613674
 0.19605438 0.195981   0.19604187 0.19641723 0.19714737 0.19810016
 0.19899774 0.19968915 0.19997816 0.19988829 0.19958898 0.19905233
 0.1984281  0.19793838 0.19752885 0.19716747 0.19703105 0.19721973
 0.19753401 0.197812   0.19812961 0.19825888 0.19841053 0.19863738
 0.198961   0.19944327 0.20025155 0.20070232 0.20117123 0.2010046
 0.20079297 0.20045246 0.20002532 0.19983493 0.19965543 0.19952056
 0.19923231 0.19894506 0.19858943 0.19824801 0.19793446 0.19786863
 0.1980264  0.19829956 0.19878225 0.19924814 0.1997449  0.20008469
 0.20020223 0.20008336 0.19973727 0.19933166 0.19898772 0.19871707
 0.19842508 0.1981053  0.1975198  0.1966535  0.19566303 0.19476776
 0.19414373 0.19391435 0.19377126 0.19363175 0.19344842 0.19325998
 0.19294481 0.19243704 0.19219173 0.19200245 0.19212951 0.19222637
 0.19232221 0.19219518 0.19215174 0.19220342 0.19231881 0.19226354
 0.19202301 0.19170183 0.19108549 0.1904868  0.18999666 0.18954036
 0.18935007 0.18916795 0.18902591 0.18871915 0.18845026 0.18826248
 0.18828931 0.18825728 0.18834771 0.18849258 0.1886639  0.18878916
 0.18888627 0.18891983 0.1888908  0.18884641 0.18868831 0.18840398
 0.18806356 0.1875656  0.18680023 0.18582922 0.18479757 0.18373136
 0.18282467 0.1822887  0.18207198 0.18212557 0.18218364 0.18219641
 0.18211587 0.18189259 0.18165547 0.18135318 0.18105978 0.1808446
 0.18082789 0.18103197 0.18144327 0.18194978 0.18241411 0.18251899
 0.1821343  0.18144049 0.18058874 0.18002023 0.17976427 0.17962387
 0.17951867 0.17932919 0.17892177 0.17836195 0.17783162 0.17764084
 0.17776729 0.17772736 0.17755203 0.17731258 0.17709714 0.17708638
 0.17722873 0.17749599 0.17771837 0.17767003 0.17731881 0.1768119
 0.1763064  0.17603822 0.1759339  0.1761111  0.17607549 0.17550449
 0.17438538 0.17307921 0.17216444 0.17169333 0.17156747 0.17164852
 0.17173448 0.17157814 0.17141207 0.17134483 0.17154302 0.17188501
 0.17206857 0.17203559 0.17175001 0.17146689 0.17107277 0.17052236
 0.16939053 0.16776319 0.16572452 0.16370317 0.16221695 0.16116166
 0.16089511 0.16088232 0.16079965 0.16006425 0.15927722 0.15898076
 0.15909113 0.15908986 0.1590288  0.15878299 0.15846643 0.1583807
 0.15871657 0.15946485 0.16019048 0.16015673 0.15907101 0.15743507
 0.1560007  0.15564291 0.15596554 0.1566172  0.15644996 0.15483119
 0.1524979  0.15041888 0.14948712 0.1497415  0.15065739 0.15094467
 0.15001054 0.14809966 0.14672509 0.14674553 0.14798567 0.14857261
 0.14653382 0.142639   0.14064345 0.14303169 0.1499224  0.14471893]
