Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=514, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=106, out_features=318, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30202368.0
params:  34026.0
Trainable parameters:  34026
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8754761219024658
Epoch: 1, Steps: 59 | Train Loss: 1.0055285 Vali Loss: 2.0435264 Test Loss: 0.8498710
Validation loss decreased (inf --> 2.043526).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.00407075881958
Epoch: 2, Steps: 59 | Train Loss: 0.7788252 Vali Loss: 1.8043746 Test Loss: 0.6803476
Validation loss decreased (2.043526 --> 1.804375).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.1185219287872314
Epoch: 3, Steps: 59 | Train Loss: 0.7066447 Vali Loss: 1.7147738 Test Loss: 0.6129789
Validation loss decreased (1.804375 --> 1.714774).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6086463928222656
Epoch: 4, Steps: 59 | Train Loss: 0.6735469 Vali Loss: 1.6563752 Test Loss: 0.5729104
Validation loss decreased (1.714774 --> 1.656375).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5931103229522705
Epoch: 5, Steps: 59 | Train Loss: 0.6525330 Vali Loss: 1.6124603 Test Loss: 0.5442106
Validation loss decreased (1.656375 --> 1.612460).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.593653917312622
Epoch: 6, Steps: 59 | Train Loss: 0.6371970 Vali Loss: 1.5863781 Test Loss: 0.5221408
Validation loss decreased (1.612460 --> 1.586378).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.6376299858093262
Epoch: 7, Steps: 59 | Train Loss: 0.6252878 Vali Loss: 1.5660242 Test Loss: 0.5047692
Validation loss decreased (1.586378 --> 1.566024).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.6758244037628174
Epoch: 8, Steps: 59 | Train Loss: 0.6160318 Vali Loss: 1.5456252 Test Loss: 0.4907256
Validation loss decreased (1.566024 --> 1.545625).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7200021743774414
Epoch: 9, Steps: 59 | Train Loss: 0.6082638 Vali Loss: 1.5336740 Test Loss: 0.4792995
Validation loss decreased (1.545625 --> 1.533674).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.7870526313781738
Epoch: 10, Steps: 59 | Train Loss: 0.6021411 Vali Loss: 1.5160663 Test Loss: 0.4699468
Validation loss decreased (1.533674 --> 1.516066).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.5704445838928223
Epoch: 11, Steps: 59 | Train Loss: 0.5968928 Vali Loss: 1.5081856 Test Loss: 0.4623049
Validation loss decreased (1.516066 --> 1.508186).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.5685579776763916
Epoch: 12, Steps: 59 | Train Loss: 0.5927940 Vali Loss: 1.4936023 Test Loss: 0.4559884
Validation loss decreased (1.508186 --> 1.493602).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.585364580154419
Epoch: 13, Steps: 59 | Train Loss: 0.5891695 Vali Loss: 1.4866832 Test Loss: 0.4507655
Validation loss decreased (1.493602 --> 1.486683).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6165771484375
Epoch: 14, Steps: 59 | Train Loss: 0.5862800 Vali Loss: 1.4765835 Test Loss: 0.4464709
Validation loss decreased (1.486683 --> 1.476583).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6498241424560547
Epoch: 15, Steps: 59 | Train Loss: 0.5835847 Vali Loss: 1.4802434 Test Loss: 0.4429621
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.5902836322784424
Epoch: 16, Steps: 59 | Train Loss: 0.5816454 Vali Loss: 1.4723008 Test Loss: 0.4400185
Validation loss decreased (1.476583 --> 1.472301).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.9564547538757324
Epoch: 17, Steps: 59 | Train Loss: 0.5798704 Vali Loss: 1.4636354 Test Loss: 0.4374912
Validation loss decreased (1.472301 --> 1.463635).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.778815746307373
Epoch: 18, Steps: 59 | Train Loss: 0.5782831 Vali Loss: 1.4643383 Test Loss: 0.4355276
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6203832626342773
Epoch: 19, Steps: 59 | Train Loss: 0.5768589 Vali Loss: 1.4666908 Test Loss: 0.4337807
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.3004331588745117
Epoch: 20, Steps: 59 | Train Loss: 0.5761462 Vali Loss: 1.4532273 Test Loss: 0.4324028
Validation loss decreased (1.463635 --> 1.453227).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0864338874816895
Epoch: 21, Steps: 59 | Train Loss: 0.5750182 Vali Loss: 1.4559395 Test Loss: 0.4311845
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.6662776470184326
Epoch: 22, Steps: 59 | Train Loss: 0.5742025 Vali Loss: 1.4529892 Test Loss: 0.4301662
Validation loss decreased (1.453227 --> 1.452989).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.676647424697876
Epoch: 23, Steps: 59 | Train Loss: 0.5734150 Vali Loss: 1.4554563 Test Loss: 0.4293903
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.844724178314209
Epoch: 24, Steps: 59 | Train Loss: 0.5728102 Vali Loss: 1.4526583 Test Loss: 0.4287043
Validation loss decreased (1.452989 --> 1.452658).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.2050766944885254
Epoch: 25, Steps: 59 | Train Loss: 0.5724396 Vali Loss: 1.4538682 Test Loss: 0.4281135
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7030704021453857
Epoch: 26, Steps: 59 | Train Loss: 0.5719320 Vali Loss: 1.4511817 Test Loss: 0.4276470
Validation loss decreased (1.452658 --> 1.451182).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.7552685737609863
Epoch: 27, Steps: 59 | Train Loss: 0.5715758 Vali Loss: 1.4474797 Test Loss: 0.4272276
Validation loss decreased (1.451182 --> 1.447480).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6950557231903076
Epoch: 28, Steps: 59 | Train Loss: 0.5711371 Vali Loss: 1.4474771 Test Loss: 0.4269187
Validation loss decreased (1.447480 --> 1.447477).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.4548981189727783
Epoch: 29, Steps: 59 | Train Loss: 0.5707369 Vali Loss: 1.4434656 Test Loss: 0.4266142
Validation loss decreased (1.447477 --> 1.443466).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.313876152038574
Epoch: 30, Steps: 59 | Train Loss: 0.5706368 Vali Loss: 1.4476146 Test Loss: 0.4263868
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.7708792686462402
Epoch: 31, Steps: 59 | Train Loss: 0.5703504 Vali Loss: 1.4434856 Test Loss: 0.4261896
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.9008967876434326
Epoch: 32, Steps: 59 | Train Loss: 0.5700876 Vali Loss: 1.4442811 Test Loss: 0.4260362
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.88975191116333
Epoch: 33, Steps: 59 | Train Loss: 0.5699625 Vali Loss: 1.4441698 Test Loss: 0.4259333
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.2001304626464844
Epoch: 34, Steps: 59 | Train Loss: 0.5697029 Vali Loss: 1.4408852 Test Loss: 0.4257864
Validation loss decreased (1.443466 --> 1.440885).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.5896081924438477
Epoch: 35, Steps: 59 | Train Loss: 0.5694910 Vali Loss: 1.4398415 Test Loss: 0.4256954
Validation loss decreased (1.440885 --> 1.439842).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.1371541023254395
Epoch: 36, Steps: 59 | Train Loss: 0.5694553 Vali Loss: 1.4447286 Test Loss: 0.4256330
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6943657398223877
Epoch: 37, Steps: 59 | Train Loss: 0.5693120 Vali Loss: 1.4396631 Test Loss: 0.4255862
Validation loss decreased (1.439842 --> 1.439663).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.1889004707336426
Epoch: 38, Steps: 59 | Train Loss: 0.5692732 Vali Loss: 1.4379706 Test Loss: 0.4255357
Validation loss decreased (1.439663 --> 1.437971).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.4129693508148193
Epoch: 39, Steps: 59 | Train Loss: 0.5689909 Vali Loss: 1.4396274 Test Loss: 0.4254952
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.6843101978302002
Epoch: 40, Steps: 59 | Train Loss: 0.5688690 Vali Loss: 1.4365358 Test Loss: 0.4254746
Validation loss decreased (1.437971 --> 1.436536).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.7245020866394043
Epoch: 41, Steps: 59 | Train Loss: 0.5689116 Vali Loss: 1.4407564 Test Loss: 0.4254534
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.5380139350891113
Epoch: 42, Steps: 59 | Train Loss: 0.5690167 Vali Loss: 1.4385970 Test Loss: 0.4254353
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.7073311805725098
Epoch: 43, Steps: 59 | Train Loss: 0.5688777 Vali Loss: 1.4357418 Test Loss: 0.4254210
Validation loss decreased (1.436536 --> 1.435742).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.0146100521087646
Epoch: 44, Steps: 59 | Train Loss: 0.5686988 Vali Loss: 1.4394946 Test Loss: 0.4254237
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.6291823387145996
Epoch: 45, Steps: 59 | Train Loss: 0.5684627 Vali Loss: 1.4365797 Test Loss: 0.4254234
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.937547206878662
Epoch: 46, Steps: 59 | Train Loss: 0.5686924 Vali Loss: 1.4397948 Test Loss: 0.4254275
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.7241499423980713
Epoch: 47, Steps: 59 | Train Loss: 0.5684860 Vali Loss: 1.4424658 Test Loss: 0.4254256
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.5941903591156006
Epoch: 48, Steps: 59 | Train Loss: 0.5685582 Vali Loss: 1.4368248 Test Loss: 0.4254252
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.617762565612793
Epoch: 49, Steps: 59 | Train Loss: 0.5683775 Vali Loss: 1.4414880 Test Loss: 0.4254395
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.5281901359558105
Epoch: 50, Steps: 59 | Train Loss: 0.5684188 Vali Loss: 1.4367712 Test Loss: 0.4254552
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.700857162475586
Epoch: 51, Steps: 59 | Train Loss: 0.5684437 Vali Loss: 1.4362152 Test Loss: 0.4254556
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6713941097259521
Epoch: 52, Steps: 59 | Train Loss: 0.5684168 Vali Loss: 1.4400203 Test Loss: 0.4254660
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6106154918670654
Epoch: 53, Steps: 59 | Train Loss: 0.5682603 Vali Loss: 1.4400896 Test Loss: 0.4254591
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.1114442348480225
Epoch: 54, Steps: 59 | Train Loss: 0.5682222 Vali Loss: 1.4435604 Test Loss: 0.4254853
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.723487377166748
Epoch: 55, Steps: 59 | Train Loss: 0.5681501 Vali Loss: 1.4355845 Test Loss: 0.4254921
Validation loss decreased (1.435742 --> 1.435585).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9909331798553467
Epoch: 56, Steps: 59 | Train Loss: 0.5681355 Vali Loss: 1.4320811 Test Loss: 0.4254981
Validation loss decreased (1.435585 --> 1.432081).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.6800620555877686
Epoch: 57, Steps: 59 | Train Loss: 0.5682201 Vali Loss: 1.4396968 Test Loss: 0.4255117
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.7439289093017578
Epoch: 58, Steps: 59 | Train Loss: 0.5681906 Vali Loss: 1.4374795 Test Loss: 0.4255252
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.6501023769378662
Epoch: 59, Steps: 59 | Train Loss: 0.5681111 Vali Loss: 1.4375466 Test Loss: 0.4255326
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.839812994003296
Epoch: 60, Steps: 59 | Train Loss: 0.5679776 Vali Loss: 1.4370081 Test Loss: 0.4255480
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.9534821510314941
Epoch: 61, Steps: 59 | Train Loss: 0.5681961 Vali Loss: 1.4356875 Test Loss: 0.4255554
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.7142813205718994
Epoch: 62, Steps: 59 | Train Loss: 0.5681438 Vali Loss: 1.4385459 Test Loss: 0.4255727
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.917994737625122
Epoch: 63, Steps: 59 | Train Loss: 0.5679541 Vali Loss: 1.4394413 Test Loss: 0.4255860
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.695521354675293
Epoch: 64, Steps: 59 | Train Loss: 0.5678178 Vali Loss: 1.4324558 Test Loss: 0.4255908
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.6897599697113037
Epoch: 65, Steps: 59 | Train Loss: 0.5679855 Vali Loss: 1.4382168 Test Loss: 0.4256025
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.7262585163116455
Epoch: 66, Steps: 59 | Train Loss: 0.5679564 Vali Loss: 1.4375452 Test Loss: 0.4256095
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.8075637817382812
Epoch: 67, Steps: 59 | Train Loss: 0.5681139 Vali Loss: 1.4362111 Test Loss: 0.4256227
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.7208197116851807
Epoch: 68, Steps: 59 | Train Loss: 0.5680011 Vali Loss: 1.4390073 Test Loss: 0.4256341
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.7590761184692383
Epoch: 69, Steps: 59 | Train Loss: 0.5680028 Vali Loss: 1.4370530 Test Loss: 0.4256406
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.7098643779754639
Epoch: 70, Steps: 59 | Train Loss: 0.5680480 Vali Loss: 1.4384023 Test Loss: 0.4256485
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.5902423858642578
Epoch: 71, Steps: 59 | Train Loss: 0.5679557 Vali Loss: 1.4367065 Test Loss: 0.4256562
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.749281406402588
Epoch: 72, Steps: 59 | Train Loss: 0.5681289 Vali Loss: 1.4408457 Test Loss: 0.4256606
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.5971951484680176
Epoch: 73, Steps: 59 | Train Loss: 0.5678987 Vali Loss: 1.4338191 Test Loss: 0.4256718
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.610588788986206
Epoch: 74, Steps: 59 | Train Loss: 0.5679721 Vali Loss: 1.4371299 Test Loss: 0.4256811
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.060744524002075
Epoch: 75, Steps: 59 | Train Loss: 0.5678892 Vali Loss: 1.4384311 Test Loss: 0.4256918
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.8160035610198975
Epoch: 76, Steps: 59 | Train Loss: 0.5677878 Vali Loss: 1.4347079 Test Loss: 0.4256984
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4244462549686432, mae:0.4464549720287323, rse:0.6236821413040161, corr:[0.22538482 0.23157138 0.23185563 0.23636205 0.23399337 0.23138687
 0.23284513 0.23350938 0.23205988 0.23230118 0.23302707 0.2320464
 0.23104131 0.23117512 0.2311221  0.23057368 0.23036447 0.2304509
 0.23049681 0.23030853 0.23014204 0.23048292 0.23154528 0.2321565
 0.23173444 0.23192403 0.23262218 0.23232965 0.23178302 0.23203272
 0.23195697 0.23100942 0.23058544 0.2310107  0.23084828 0.2301166
 0.23019266 0.2305901  0.23024416 0.22981898 0.23012727 0.23041065
 0.23020887 0.22996674 0.22985476 0.2298815  0.23012252 0.23035528
 0.23028798 0.23005807 0.22967558 0.22916335 0.22878458 0.22841257
 0.22791247 0.22706729 0.22689141 0.22681753 0.22625218 0.22576168
 0.2256792  0.22561687 0.22520858 0.22508165 0.22546093 0.22581287
 0.22582528 0.22571073 0.22581668 0.22598936 0.22578356 0.22540721
 0.22501028 0.22459611 0.22410946 0.22386926 0.2239562  0.22387056
 0.22334787 0.2229182  0.22292311 0.22268023 0.2221023  0.22191343
 0.22193787 0.22158602 0.22118653 0.22129226 0.22144479 0.2211543
 0.2206961  0.22052534 0.22057165 0.22070062 0.22097577 0.2219167
 0.22312205 0.22380483 0.22429748 0.22488928 0.22533292 0.22538112
 0.22515059 0.2251356  0.22511967 0.2247133  0.22414114 0.22388105
 0.22377975 0.22351907 0.22334105 0.22352631 0.22383752 0.22377628
 0.22354443 0.22344585 0.22342098 0.2231756  0.22275874 0.22271022
 0.2228552  0.22259463 0.22217242 0.22205015 0.22189464 0.22150046
 0.22122738 0.22116984 0.22083789 0.22022793 0.21985982 0.2198093
 0.21975549 0.21952693 0.2193973  0.21956481 0.21976556 0.21972966
 0.21967448 0.21976195 0.219833   0.2196912  0.21925367 0.21905316
 0.21898653 0.21870027 0.21846692 0.21817678 0.21761067 0.21711154
 0.21706176 0.21711515 0.21699914 0.21700104 0.21713297 0.2171368
 0.21694906 0.21683763 0.21684545 0.21686645 0.21684088 0.21681356
 0.21668687 0.2165445  0.21637817 0.21607135 0.21553285 0.21563949
 0.21613629 0.21661495 0.21741652 0.21841298 0.21870069 0.21858294
 0.21875538 0.21895634 0.21881181 0.21860872 0.21860702 0.21852079
 0.2182529  0.21818273 0.21833383 0.21832132 0.2183676  0.21854523
 0.21847706 0.21835703 0.21842636 0.21823955 0.21781312 0.2177232
 0.2176949  0.21735731 0.21699895 0.21681271 0.21633895 0.21568148
 0.21543264 0.21548402 0.21527392 0.21503973 0.21503662 0.21497034
 0.21486765 0.21515438 0.21545623 0.21543166 0.21535897 0.21542114
 0.21534139 0.21494439 0.21457036 0.2143193  0.2139916  0.2139732
 0.2141344  0.21416757 0.21434525 0.21435922 0.21408868 0.21393253
 0.21401061 0.21403967 0.21375197 0.2133258  0.212988   0.21267606
 0.21230836 0.21221492 0.21229619 0.21225889 0.21218586 0.21217646
 0.21207891 0.21167926 0.21125078 0.21101913 0.21101741 0.21112333
 0.2113453  0.21147588 0.21178222 0.21211614 0.21223643 0.21229336
 0.2124396  0.21248215 0.21237294 0.21220575 0.21202604 0.21175286
 0.21157987 0.21169828 0.21202111 0.21215574 0.21219341 0.21218719
 0.2121168  0.21196336 0.21184644 0.21179765 0.21172777 0.2117095
 0.21153274 0.21136217 0.21141033 0.21138765 0.21113446 0.21103658
 0.21096945 0.21071413 0.21047851 0.2104811  0.2103771  0.21021716
 0.21021777 0.2102604  0.21007273 0.20987484 0.2097516  0.20967086
 0.2096066  0.20962316 0.20964803 0.20963205 0.20971598 0.2102141
 0.2108453  0.21143185 0.2120439  0.21235803 0.2124458  0.21261382
 0.21277162 0.2127579  0.21271282 0.21270996 0.21253811 0.21236417
 0.21237019 0.21250713 0.21263412 0.21267796 0.21280386 0.21285434
 0.21279332 0.21258844 0.21237795 0.21229032 0.21222042 0.21251434
 0.21313846 0.21374744 0.21418712 0.2143436  0.21417907 0.21398702
 0.21381006 0.2136777  0.21357125 0.21328029 0.21277088 0.21254507
 0.21256347 0.21255113 0.21255262 0.21281298 0.2130168  0.21299957
 0.213019   0.21306787 0.21300538 0.21300626 0.2130235  0.21282612
 0.21268457 0.21285142 0.21303119 0.21270275 0.21223031 0.21190606
 0.2116858  0.21151666 0.21143213 0.21135712 0.21133274 0.21152744
 0.21177138 0.21180175 0.21180648 0.21209018 0.21223354 0.21220095
 0.21213633 0.21197581 0.2116553  0.21150438 0.21121585 0.21092916
 0.21091318 0.21124707 0.21150966 0.21123105 0.2107607  0.21042581
 0.21014601 0.20978835 0.20964421 0.20956725 0.20916484 0.20887838
 0.20899747 0.20887202 0.20840603 0.20806736 0.20804879 0.20802988
 0.20793271 0.20793438 0.20788966 0.2077673  0.20774166 0.20845973
 0.20955087 0.21053402 0.21143962 0.21179448 0.21164605 0.21121477
 0.21074297 0.21016878 0.20986585 0.20958805 0.2091153  0.20899722
 0.2091972  0.2092235  0.20911367 0.20922238 0.20948671 0.20970564
 0.2098511  0.2101211  0.21036625 0.21068105 0.21098416 0.21136668
 0.21163234 0.2118277  0.2119016  0.2117601  0.21154033 0.21127033
 0.21097639 0.21092477 0.21092898 0.21071805 0.21045688 0.21054092
 0.21077344 0.21067291 0.21027859 0.21011676 0.2101543  0.2101222
 0.21003689 0.21016912 0.21028015 0.2106949  0.21117833 0.21174182
 0.21229732 0.21266344 0.21279614 0.21280172 0.21276122 0.21253234
 0.21219227 0.21220228 0.21219568 0.21188259 0.21159972 0.21174733
 0.21199054 0.21176365 0.21146026 0.21147491 0.21154268 0.21143739
 0.211344   0.21137942 0.21150868 0.21170713 0.21194628 0.21264207
 0.21343921 0.21396093 0.21415125 0.21402724 0.21358415 0.2132516
 0.21300411 0.21279952 0.21272223 0.21269552 0.21267696 0.21280639
 0.21301523 0.21297224 0.21289958 0.21308376 0.21326742 0.21321811
 0.21313715 0.21316013 0.21312343 0.21284713 0.21283369 0.21310644
 0.21332166 0.21333092 0.21318465 0.2129991  0.21276444 0.21254376
 0.21240374 0.21226484 0.2121283  0.21194173 0.21155964 0.21148726
 0.21162745 0.21140242 0.2110968  0.21118717 0.21127288 0.2110153
 0.21085007 0.21102217 0.2112198  0.21133249 0.21176076 0.21282095
 0.21379608 0.21440762 0.21499723 0.21536149 0.21517314 0.2146691
 0.21422547 0.21388997 0.21355733 0.21306252 0.21261013 0.21264201
 0.21286449 0.21277922 0.21278867 0.21321821 0.21363226 0.21356766
 0.21348177 0.21377786 0.21400316 0.21404351 0.21446393 0.21502024
 0.21498407 0.21473165 0.21493353 0.21508989 0.21448652 0.21371333
 0.21327609 0.21307129 0.21278486 0.21255727 0.21231249 0.21231517
 0.21236011 0.21244779 0.21257293 0.21296875 0.21336763 0.21337865
 0.21321529 0.21326593 0.2132668  0.21307036 0.2128067  0.21282776
 0.21267293 0.21235204 0.21227728 0.2119481  0.21111637 0.21046041
 0.2099985  0.20950298 0.20908785 0.20857085 0.20811349 0.20797954
 0.20791636 0.20752451 0.20731936 0.2073616  0.20733643 0.20719308
 0.20737273 0.20758282 0.20753558 0.20740402 0.20759302 0.2077512
 0.20757295 0.2074918  0.20740427 0.2072178  0.2068372  0.20630527
 0.20595758 0.20554818 0.20520605 0.20500728 0.20489824 0.2047607
 0.20467438 0.2043565  0.20428011 0.20428972 0.2041721  0.20407027
 0.2041626  0.20420337 0.20406264 0.20394327 0.20400135 0.20400187
 0.20375451 0.2033988  0.20334472 0.20297725 0.20207182 0.20130229
 0.20101517 0.20077509 0.20019735 0.19984423 0.19979726 0.19969913
 0.19948104 0.19912685 0.1989556  0.19908853 0.19908874 0.19892181
 0.19884783 0.19878173 0.19882135 0.19913684 0.19949596 0.19973819
 0.19971363 0.19947635 0.19947408 0.19910862 0.19833983 0.19788228
 0.1978488  0.19741265 0.19668256 0.19645506 0.19638576 0.19628406
 0.19629887 0.19623554 0.19622667 0.19607325 0.19590005 0.19605565
 0.19603404 0.19576736 0.19579853 0.19618063 0.19656238 0.19701047
 0.19741088 0.19745585 0.19710316 0.19698673 0.19654323 0.19627969
 0.19599882 0.19547053 0.19478595 0.19441438 0.19434187 0.19413052
 0.1938944  0.19385156 0.19384933 0.19353998 0.19368143 0.1940463
 0.19412641 0.19382036 0.1938434  0.19413751 0.19419932 0.194225
 0.19398853 0.19321668 0.1923982  0.19158837 0.19087358 0.18999158
 0.18914679 0.1886472  0.18853194 0.18834612 0.18806091 0.18790756
 0.18818146 0.18828064 0.1880943  0.18804559 0.18825893 0.18872319
 0.18874103 0.1885706  0.18901868 0.18972959 0.18974747 0.18959408
 0.19002263 0.19003116 0.18945074 0.18924524 0.18928997 0.18862803
 0.1879721  0.18814653 0.18843333 0.18783495 0.18708818 0.18700293
 0.1870842  0.18660797 0.1863422  0.1856766  0.18574613 0.18592481
 0.18532619 0.18513362 0.18654053 0.18388675 0.18263833 0.18406421]
