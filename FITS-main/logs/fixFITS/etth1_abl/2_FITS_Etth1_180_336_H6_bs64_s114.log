Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=58, out_features=166, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8626688.0
params:  9794.0
Trainable parameters:  9794
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.2940659523010254
Epoch: 1, Steps: 63 | Train Loss: 0.8163899 Vali Loss: 1.8699274 Test Loss: 0.8907455
Validation loss decreased (inf --> 1.869927).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3120105266571045
Epoch: 2, Steps: 63 | Train Loss: 0.6026669 Vali Loss: 1.6232620 Test Loss: 0.7184969
Validation loss decreased (1.869927 --> 1.623262).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.3372297286987305
Epoch: 3, Steps: 63 | Train Loss: 0.5010566 Vali Loss: 1.4966710 Test Loss: 0.6315945
Validation loss decreased (1.623262 --> 1.496671).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.2134499549865723
Epoch: 4, Steps: 63 | Train Loss: 0.4449762 Vali Loss: 1.4234678 Test Loss: 0.5818841
Validation loss decreased (1.496671 --> 1.423468).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.1405751705169678
Epoch: 5, Steps: 63 | Train Loss: 0.4108102 Vali Loss: 1.3791965 Test Loss: 0.5512022
Validation loss decreased (1.423468 --> 1.379197).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.1482813358306885
Epoch: 6, Steps: 63 | Train Loss: 0.3889912 Vali Loss: 1.3544071 Test Loss: 0.5318620
Validation loss decreased (1.379197 --> 1.354407).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.1850924491882324
Epoch: 7, Steps: 63 | Train Loss: 0.3741504 Vali Loss: 1.3267612 Test Loss: 0.5185499
Validation loss decreased (1.354407 --> 1.326761).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2011733055114746
Epoch: 8, Steps: 63 | Train Loss: 0.3633606 Vali Loss: 1.3158468 Test Loss: 0.5094666
Validation loss decreased (1.326761 --> 1.315847).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3777368068695068
Epoch: 9, Steps: 63 | Train Loss: 0.3555936 Vali Loss: 1.3039936 Test Loss: 0.5023052
Validation loss decreased (1.315847 --> 1.303994).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.2141828536987305
Epoch: 10, Steps: 63 | Train Loss: 0.3498609 Vali Loss: 1.2922190 Test Loss: 0.4967594
Validation loss decreased (1.303994 --> 1.292219).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.1681113243103027
Epoch: 11, Steps: 63 | Train Loss: 0.3450993 Vali Loss: 1.2990847 Test Loss: 0.4924639
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.2029399871826172
Epoch: 12, Steps: 63 | Train Loss: 0.3413955 Vali Loss: 1.2846637 Test Loss: 0.4887456
Validation loss decreased (1.292219 --> 1.284664).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.1697769165039062
Epoch: 13, Steps: 63 | Train Loss: 0.3378524 Vali Loss: 1.2767404 Test Loss: 0.4857116
Validation loss decreased (1.284664 --> 1.276740).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.1928560733795166
Epoch: 14, Steps: 63 | Train Loss: 0.3353305 Vali Loss: 1.2782905 Test Loss: 0.4830030
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.2723796367645264
Epoch: 15, Steps: 63 | Train Loss: 0.3331656 Vali Loss: 1.2750639 Test Loss: 0.4806898
Validation loss decreased (1.276740 --> 1.275064).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.1750409603118896
Epoch: 16, Steps: 63 | Train Loss: 0.3310661 Vali Loss: 1.2744312 Test Loss: 0.4786552
Validation loss decreased (1.275064 --> 1.274431).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.2247414588928223
Epoch: 17, Steps: 63 | Train Loss: 0.3295068 Vali Loss: 1.2719533 Test Loss: 0.4769557
Validation loss decreased (1.274431 --> 1.271953).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.211778163909912
Epoch: 18, Steps: 63 | Train Loss: 0.3278245 Vali Loss: 1.2636162 Test Loss: 0.4753096
Validation loss decreased (1.271953 --> 1.263616).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.157360553741455
Epoch: 19, Steps: 63 | Train Loss: 0.3262942 Vali Loss: 1.2667869 Test Loss: 0.4737447
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.1527860164642334
Epoch: 20, Steps: 63 | Train Loss: 0.3255970 Vali Loss: 1.2674804 Test Loss: 0.4724753
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.1799323558807373
Epoch: 21, Steps: 63 | Train Loss: 0.3246332 Vali Loss: 1.2594347 Test Loss: 0.4713026
Validation loss decreased (1.263616 --> 1.259435).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2738096714019775
Epoch: 22, Steps: 63 | Train Loss: 0.3236219 Vali Loss: 1.2559741 Test Loss: 0.4702865
Validation loss decreased (1.259435 --> 1.255974).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3722681999206543
Epoch: 23, Steps: 63 | Train Loss: 0.3228106 Vali Loss: 1.2584755 Test Loss: 0.4692542
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.2742860317230225
Epoch: 24, Steps: 63 | Train Loss: 0.3217088 Vali Loss: 1.2517538 Test Loss: 0.4684308
Validation loss decreased (1.255974 --> 1.251754).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.781170129776001
Epoch: 25, Steps: 63 | Train Loss: 0.3212635 Vali Loss: 1.2629406 Test Loss: 0.4675988
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.1658475399017334
Epoch: 26, Steps: 63 | Train Loss: 0.3207435 Vali Loss: 1.2542300 Test Loss: 0.4668690
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1834776401519775
Epoch: 27, Steps: 63 | Train Loss: 0.3201989 Vali Loss: 1.2534344 Test Loss: 0.4662217
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.644002676010132
Epoch: 28, Steps: 63 | Train Loss: 0.3194241 Vali Loss: 1.2536361 Test Loss: 0.4655850
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6740431785583496
Epoch: 29, Steps: 63 | Train Loss: 0.3190192 Vali Loss: 1.2533931 Test Loss: 0.4650186
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5985362529754639
Epoch: 30, Steps: 63 | Train Loss: 0.3186313 Vali Loss: 1.2501822 Test Loss: 0.4644971
Validation loss decreased (1.251754 --> 1.250182).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.330735445022583
Epoch: 31, Steps: 63 | Train Loss: 0.3182117 Vali Loss: 1.2503608 Test Loss: 0.4640198
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.6563386917114258
Epoch: 32, Steps: 63 | Train Loss: 0.3178612 Vali Loss: 1.2477293 Test Loss: 0.4635852
Validation loss decreased (1.250182 --> 1.247729).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.571018934249878
Epoch: 33, Steps: 63 | Train Loss: 0.3175260 Vali Loss: 1.2502875 Test Loss: 0.4631712
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5411648750305176
Epoch: 34, Steps: 63 | Train Loss: 0.3170897 Vali Loss: 1.2463560 Test Loss: 0.4627793
Validation loss decreased (1.247729 --> 1.246356).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.6971938610076904
Epoch: 35, Steps: 63 | Train Loss: 0.3167231 Vali Loss: 1.2473427 Test Loss: 0.4624197
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6806321144104004
Epoch: 36, Steps: 63 | Train Loss: 0.3166700 Vali Loss: 1.2441099 Test Loss: 0.4620999
Validation loss decreased (1.246356 --> 1.244110).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.0996928215026855
Epoch: 37, Steps: 63 | Train Loss: 0.3164396 Vali Loss: 1.2467679 Test Loss: 0.4618011
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4392085075378418
Epoch: 38, Steps: 63 | Train Loss: 0.3161709 Vali Loss: 1.2467031 Test Loss: 0.4615062
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.8588457107543945
Epoch: 39, Steps: 63 | Train Loss: 0.3158493 Vali Loss: 1.2495235 Test Loss: 0.4612599
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.2755982875823975
Epoch: 40, Steps: 63 | Train Loss: 0.3156901 Vali Loss: 1.2421379 Test Loss: 0.4609855
Validation loss decreased (1.244110 --> 1.242138).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.2265164852142334
Epoch: 41, Steps: 63 | Train Loss: 0.3155026 Vali Loss: 1.2436160 Test Loss: 0.4607998
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.203650712966919
Epoch: 42, Steps: 63 | Train Loss: 0.3156763 Vali Loss: 1.2383412 Test Loss: 0.4605805
Validation loss decreased (1.242138 --> 1.238341).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.3994598388671875
Epoch: 43, Steps: 63 | Train Loss: 0.3149869 Vali Loss: 1.2422017 Test Loss: 0.4603930
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.3087282180786133
Epoch: 44, Steps: 63 | Train Loss: 0.3149496 Vali Loss: 1.2444572 Test Loss: 0.4601737
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.262272834777832
Epoch: 45, Steps: 63 | Train Loss: 0.3145745 Vali Loss: 1.2447414 Test Loss: 0.4600138
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.252967119216919
Epoch: 46, Steps: 63 | Train Loss: 0.3146253 Vali Loss: 1.2427627 Test Loss: 0.4598463
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.2513482570648193
Epoch: 47, Steps: 63 | Train Loss: 0.3146206 Vali Loss: 1.2408078 Test Loss: 0.4597023
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.1609947681427002
Epoch: 48, Steps: 63 | Train Loss: 0.3140783 Vali Loss: 1.2428896 Test Loss: 0.4595578
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.237677812576294
Epoch: 49, Steps: 63 | Train Loss: 0.3140468 Vali Loss: 1.2452093 Test Loss: 0.4594196
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.204413652420044
Epoch: 50, Steps: 63 | Train Loss: 0.3140858 Vali Loss: 1.2452586 Test Loss: 0.4593051
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.2183313369750977
Epoch: 51, Steps: 63 | Train Loss: 0.3140813 Vali Loss: 1.2380592 Test Loss: 0.4591674
Validation loss decreased (1.238341 --> 1.238059).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.2896041870117188
Epoch: 52, Steps: 63 | Train Loss: 0.3139351 Vali Loss: 1.2446522 Test Loss: 0.4590657
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.1333255767822266
Epoch: 53, Steps: 63 | Train Loss: 0.3134928 Vali Loss: 1.2410752 Test Loss: 0.4589610
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.15407395362854
Epoch: 54, Steps: 63 | Train Loss: 0.3136052 Vali Loss: 1.2462552 Test Loss: 0.4588605
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.1865949630737305
Epoch: 55, Steps: 63 | Train Loss: 0.3137681 Vali Loss: 1.2428416 Test Loss: 0.4587835
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.184755563735962
Epoch: 56, Steps: 63 | Train Loss: 0.3133368 Vali Loss: 1.2435603 Test Loss: 0.4586715
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.2398195266723633
Epoch: 57, Steps: 63 | Train Loss: 0.3131620 Vali Loss: 1.2418144 Test Loss: 0.4586034
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.3089547157287598
Epoch: 58, Steps: 63 | Train Loss: 0.3133989 Vali Loss: 1.2434163 Test Loss: 0.4585265
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.3112995624542236
Epoch: 59, Steps: 63 | Train Loss: 0.3129516 Vali Loss: 1.2407823 Test Loss: 0.4584533
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.4172050952911377
Epoch: 60, Steps: 63 | Train Loss: 0.3131058 Vali Loss: 1.2475698 Test Loss: 0.4583878
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.2702364921569824
Epoch: 61, Steps: 63 | Train Loss: 0.3133413 Vali Loss: 1.2416044 Test Loss: 0.4583153
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.2113733291625977
Epoch: 62, Steps: 63 | Train Loss: 0.3130118 Vali Loss: 1.2365736 Test Loss: 0.4582601
Validation loss decreased (1.238059 --> 1.236574).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.2484025955200195
Epoch: 63, Steps: 63 | Train Loss: 0.3130692 Vali Loss: 1.2368828 Test Loss: 0.4582016
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.1710026264190674
Epoch: 64, Steps: 63 | Train Loss: 0.3133182 Vali Loss: 1.2369598 Test Loss: 0.4581528
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.2388193607330322
Epoch: 65, Steps: 63 | Train Loss: 0.3130759 Vali Loss: 1.2427257 Test Loss: 0.4580967
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.2547144889831543
Epoch: 66, Steps: 63 | Train Loss: 0.3130845 Vali Loss: 1.2374294 Test Loss: 0.4580544
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.1929278373718262
Epoch: 67, Steps: 63 | Train Loss: 0.3127854 Vali Loss: 1.2417614 Test Loss: 0.4580133
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.2594947814941406
Epoch: 68, Steps: 63 | Train Loss: 0.3130854 Vali Loss: 1.2458429 Test Loss: 0.4579686
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.2547080516815186
Epoch: 69, Steps: 63 | Train Loss: 0.3127847 Vali Loss: 1.2400541 Test Loss: 0.4579271
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.169088363647461
Epoch: 70, Steps: 63 | Train Loss: 0.3127969 Vali Loss: 1.2388684 Test Loss: 0.4578938
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.24407958984375
Epoch: 71, Steps: 63 | Train Loss: 0.3126214 Vali Loss: 1.2367048 Test Loss: 0.4578573
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.2219419479370117
Epoch: 72, Steps: 63 | Train Loss: 0.3130243 Vali Loss: 1.2340046 Test Loss: 0.4578244
Validation loss decreased (1.236574 --> 1.234005).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.205819845199585
Epoch: 73, Steps: 63 | Train Loss: 0.3126903 Vali Loss: 1.2438025 Test Loss: 0.4577901
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.156613826751709
Epoch: 74, Steps: 63 | Train Loss: 0.3126269 Vali Loss: 1.2326964 Test Loss: 0.4577630
Validation loss decreased (1.234005 --> 1.232696).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.2278354167938232
Epoch: 75, Steps: 63 | Train Loss: 0.3124096 Vali Loss: 1.2383324 Test Loss: 0.4577258
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.2268986701965332
Epoch: 76, Steps: 63 | Train Loss: 0.3127545 Vali Loss: 1.2361410 Test Loss: 0.4577039
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.224186658859253
Epoch: 77, Steps: 63 | Train Loss: 0.3124333 Vali Loss: 1.2390749 Test Loss: 0.4576784
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.2372634410858154
Epoch: 78, Steps: 63 | Train Loss: 0.3124880 Vali Loss: 1.2366617 Test Loss: 0.4576541
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.4082088470458984
Epoch: 79, Steps: 63 | Train Loss: 0.3125924 Vali Loss: 1.2436795 Test Loss: 0.4576302
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.185241460800171
Epoch: 80, Steps: 63 | Train Loss: 0.3127095 Vali Loss: 1.2392714 Test Loss: 0.4576119
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.1775307655334473
Epoch: 81, Steps: 63 | Train Loss: 0.3125811 Vali Loss: 1.2378913 Test Loss: 0.4575910
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.3288226127624512
Epoch: 82, Steps: 63 | Train Loss: 0.3124857 Vali Loss: 1.2389077 Test Loss: 0.4575701
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.4867630004882812
Epoch: 83, Steps: 63 | Train Loss: 0.3124702 Vali Loss: 1.2393967 Test Loss: 0.4575495
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.3739104270935059
Epoch: 84, Steps: 63 | Train Loss: 0.3122632 Vali Loss: 1.2410184 Test Loss: 0.4575358
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.2609279155731201
Epoch: 85, Steps: 63 | Train Loss: 0.3123674 Vali Loss: 1.2406157 Test Loss: 0.4575223
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.3005855083465576
Epoch: 86, Steps: 63 | Train Loss: 0.3122188 Vali Loss: 1.2408088 Test Loss: 0.4575041
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.1810176372528076
Epoch: 87, Steps: 63 | Train Loss: 0.3122022 Vali Loss: 1.2389059 Test Loss: 0.4574910
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.280519723892212
Epoch: 88, Steps: 63 | Train Loss: 0.3123808 Vali Loss: 1.2411489 Test Loss: 0.4574787
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.2400541305541992
Epoch: 89, Steps: 63 | Train Loss: 0.3123234 Vali Loss: 1.2408377 Test Loss: 0.4574630
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.336463212966919
Epoch: 90, Steps: 63 | Train Loss: 0.3123650 Vali Loss: 1.2449260 Test Loss: 0.4574541
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.3156232833862305
Epoch: 91, Steps: 63 | Train Loss: 0.3124455 Vali Loss: 1.2408196 Test Loss: 0.4574418
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.2262279987335205
Epoch: 92, Steps: 63 | Train Loss: 0.3124591 Vali Loss: 1.2385857 Test Loss: 0.4574286
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.1878306865692139
Epoch: 93, Steps: 63 | Train Loss: 0.3123725 Vali Loss: 1.2397332 Test Loss: 0.4574223
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.1362383365631104
Epoch: 94, Steps: 63 | Train Loss: 0.3122539 Vali Loss: 1.2387737 Test Loss: 0.4574093
EarlyStopping counter: 20 out of 20
Early stopping
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=58, out_features=166, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8626688.0
params:  9794.0
Trainable parameters:  9794
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.2823991775512695
Epoch: 1, Steps: 63 | Train Loss: 0.4674960 Vali Loss: 1.2274381 Test Loss: 0.4543176
Validation loss decreased (inf --> 1.227438).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.2564709186553955
Epoch: 2, Steps: 63 | Train Loss: 0.4654105 Vali Loss: 1.2331653 Test Loss: 0.4536005
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.199115514755249
Epoch: 3, Steps: 63 | Train Loss: 0.4644296 Vali Loss: 1.2323370 Test Loss: 0.4540993
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.156693696975708
Epoch: 4, Steps: 63 | Train Loss: 0.4640679 Vali Loss: 1.2236506 Test Loss: 0.4537823
Validation loss decreased (1.227438 --> 1.223651).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.235936164855957
Epoch: 5, Steps: 63 | Train Loss: 0.4641085 Vali Loss: 1.2233242 Test Loss: 0.4543248
Validation loss decreased (1.223651 --> 1.223324).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.1780400276184082
Epoch: 6, Steps: 63 | Train Loss: 0.4635642 Vali Loss: 1.2273660 Test Loss: 0.4546455
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.2531559467315674
Epoch: 7, Steps: 63 | Train Loss: 0.4634425 Vali Loss: 1.2269580 Test Loss: 0.4543882
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2523105144500732
Epoch: 8, Steps: 63 | Train Loss: 0.4635211 Vali Loss: 1.2313259 Test Loss: 0.4545293
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.431396484375
Epoch: 9, Steps: 63 | Train Loss: 0.4633187 Vali Loss: 1.2289907 Test Loss: 0.4545649
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.4394090175628662
Epoch: 10, Steps: 63 | Train Loss: 0.4632463 Vali Loss: 1.2279172 Test Loss: 0.4549563
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.747779369354248
Epoch: 11, Steps: 63 | Train Loss: 0.4632947 Vali Loss: 1.2250252 Test Loss: 0.4547997
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.8542046546936035
Epoch: 12, Steps: 63 | Train Loss: 0.4633967 Vali Loss: 1.2250929 Test Loss: 0.4548486
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.1662213802337646
Epoch: 13, Steps: 63 | Train Loss: 0.4634989 Vali Loss: 1.2274415 Test Loss: 0.4547600
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.151559829711914
Epoch: 14, Steps: 63 | Train Loss: 0.4634463 Vali Loss: 1.2203354 Test Loss: 0.4550906
Validation loss decreased (1.223324 --> 1.220335).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.7355713844299316
Epoch: 15, Steps: 63 | Train Loss: 0.4629395 Vali Loss: 1.2256145 Test Loss: 0.4548799
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.6228303909301758
Epoch: 16, Steps: 63 | Train Loss: 0.4630075 Vali Loss: 1.2265284 Test Loss: 0.4547825
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.18519926071167
Epoch: 17, Steps: 63 | Train Loss: 0.4629735 Vali Loss: 1.2241083 Test Loss: 0.4549591
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.2384507656097412
Epoch: 18, Steps: 63 | Train Loss: 0.4632293 Vali Loss: 1.2301166 Test Loss: 0.4549745
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.2245032787322998
Epoch: 19, Steps: 63 | Train Loss: 0.4629311 Vali Loss: 1.2268432 Test Loss: 0.4550074
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.214271068572998
Epoch: 20, Steps: 63 | Train Loss: 0.4629476 Vali Loss: 1.2264951 Test Loss: 0.4548786
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.3053703308105469
Epoch: 21, Steps: 63 | Train Loss: 0.4629226 Vali Loss: 1.2265162 Test Loss: 0.4548977
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2844882011413574
Epoch: 22, Steps: 63 | Train Loss: 0.4632962 Vali Loss: 1.2243240 Test Loss: 0.4549767
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.1226270198822021
Epoch: 23, Steps: 63 | Train Loss: 0.4628851 Vali Loss: 1.2272835 Test Loss: 0.4550265
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.1401026248931885
Epoch: 24, Steps: 63 | Train Loss: 0.4631237 Vali Loss: 1.2257977 Test Loss: 0.4549820
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.1599640846252441
Epoch: 25, Steps: 63 | Train Loss: 0.4631286 Vali Loss: 1.2259746 Test Loss: 0.4550735
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2133102416992188
Epoch: 26, Steps: 63 | Train Loss: 0.4629643 Vali Loss: 1.2218181 Test Loss: 0.4550614
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.119551658630371
Epoch: 27, Steps: 63 | Train Loss: 0.4627621 Vali Loss: 1.2222385 Test Loss: 0.4550380
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.1512529850006104
Epoch: 28, Steps: 63 | Train Loss: 0.4629454 Vali Loss: 1.2270679 Test Loss: 0.4549863
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.1784389019012451
Epoch: 29, Steps: 63 | Train Loss: 0.4627831 Vali Loss: 1.2258635 Test Loss: 0.4550273
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.144052267074585
Epoch: 30, Steps: 63 | Train Loss: 0.4630378 Vali Loss: 1.2309093 Test Loss: 0.4550900
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.244136095046997
Epoch: 31, Steps: 63 | Train Loss: 0.4628526 Vali Loss: 1.2260554 Test Loss: 0.4550998
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.2260513305664062
Epoch: 32, Steps: 63 | Train Loss: 0.4630327 Vali Loss: 1.2216223 Test Loss: 0.4551075
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.163118600845337
Epoch: 33, Steps: 63 | Train Loss: 0.4629092 Vali Loss: 1.2227277 Test Loss: 0.4551152
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.1968774795532227
Epoch: 34, Steps: 63 | Train Loss: 0.4625482 Vali Loss: 1.2252624 Test Loss: 0.4551156
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4543779194355011, mae:0.4342164099216461, rse:0.6417422890663147, corr:[0.25462553 0.2598937  0.25843287 0.2586502  0.25614506 0.2523389
 0.25173244 0.2521111  0.25133118 0.2516324  0.251214   0.2511078
 0.2519617  0.25115672 0.25026217 0.25069487 0.25088647 0.25073624
 0.25062788 0.24997258 0.24955007 0.24979618 0.25013334 0.25004557
 0.24956734 0.24881727 0.24788803 0.24751708 0.2469518  0.24607217
 0.24577387 0.24594125 0.24563903 0.24552767 0.24592666 0.24635452
 0.24653895 0.2464482  0.24638109 0.24662699 0.247065   0.24706537
 0.24698353 0.2471832  0.24739505 0.247833   0.24862857 0.24869418
 0.24749982 0.24596766 0.24436794 0.24344696 0.24233486 0.2407967
 0.24015184 0.24008922 0.2397321  0.2400207  0.24023472 0.24074231
 0.24118623 0.24118805 0.24106139 0.24100742 0.24107149 0.24097644
 0.24090785 0.24080537 0.24066691 0.24069986 0.24086191 0.2403229
 0.2391184  0.23801218 0.2367888  0.23633108 0.23620681 0.23562427
 0.23519546 0.23518321 0.2351164  0.23508422 0.2348125  0.23481153
 0.23497745 0.23488341 0.23500136 0.23512216 0.23490247 0.23459765
 0.23462248 0.23485425 0.23471658 0.23476107 0.23546594 0.23591153
 0.23530029 0.23473196 0.23442577 0.23386025 0.23324023 0.23271175
 0.23252283 0.23274429 0.23282133 0.23303206 0.23289308 0.23301122
 0.23330916 0.2330167  0.23268831 0.23274322 0.232999   0.23305967
 0.23303308 0.23307528 0.23315261 0.23326822 0.23334102 0.23303202
 0.23218292 0.23104908 0.22967465 0.22879145 0.22793457 0.2270918
 0.22700275 0.2275918  0.22755848 0.2275978  0.22777738 0.2282655
 0.22890538 0.22901775 0.22904833 0.22907665 0.22908735 0.22895154
 0.22892818 0.22903834 0.22893147 0.22917636 0.22961225 0.22929485
 0.22826715 0.22738281 0.22642504 0.22532956 0.22454603 0.22382228
 0.22362086 0.22388086 0.2242119  0.22446203 0.22460085 0.22524217
 0.22584842 0.22579698 0.2258135  0.22575068 0.22546594 0.22509447
 0.2249594  0.22512546 0.22503296 0.22503397 0.22547683 0.22539489
 0.22413138 0.2230216  0.22261655 0.22246173 0.22185914 0.22113372
 0.22117433 0.22173217 0.22208272 0.22300538 0.22380458 0.22443439
 0.22507757 0.22527172 0.22528882 0.22522217 0.22517055 0.225334
 0.22548445 0.22553198 0.22561266 0.22601654 0.22646226 0.22617814
 0.22499128 0.22411649 0.22330731 0.22251932 0.22171512 0.22107434
 0.22099638 0.22107011 0.22107077 0.22167474 0.22216047 0.22243808
 0.22253889 0.22255151 0.22275859 0.22270846 0.222221   0.22182016
 0.22187832 0.2219837  0.22177336 0.22173475 0.22212216 0.2220603
 0.22121933 0.22046009 0.21992794 0.21959978 0.21922506 0.21849203
 0.21832559 0.2185462  0.21890806 0.21905655 0.21887524 0.21933794
 0.22004394 0.21980861 0.21965504 0.21983652 0.21974456 0.219438
 0.2193573  0.21945813 0.2194971  0.2197366  0.22019763 0.21991637
 0.21872051 0.21773417 0.21698953 0.2164154  0.2160123  0.21585491
 0.21612497 0.21645786 0.2167541  0.21725221 0.21754625 0.2182938
 0.2193849  0.21938366 0.218913   0.21903558 0.21949933 0.21943124
 0.21937521 0.21961276 0.2195969  0.21938886 0.21989475 0.22015871
 0.21924123 0.21832702 0.21778792 0.21756698 0.21698043 0.21611562
 0.21592122 0.21634524 0.21660542 0.21699418 0.2172524  0.21735327
 0.21772434 0.21784994 0.21798448 0.21777588 0.2173888  0.21725637
 0.21735398 0.21750021 0.21761936 0.21787885 0.21829832 0.2182718
 0.2175626  0.21696581 0.21668972 0.21688463 0.21692003 0.21667917
 0.21675806 0.21713796 0.21735042 0.21781877 0.21826686 0.21864328
 0.2190454  0.21899115 0.21915309 0.21931225 0.21918367 0.21889627
 0.21903682 0.21932684 0.21929981 0.21916324 0.21946725 0.21943346
 0.21878271 0.21844326 0.21779723 0.21686499 0.21641344 0.21544799
 0.21511379 0.21591434 0.21636397 0.21646783 0.21662243 0.21722277
 0.21820985 0.21853322 0.21852967 0.21822686 0.21832524 0.21849298
 0.21785004 0.21753782 0.21776386 0.21649037 0.21602689 0.21738651]
