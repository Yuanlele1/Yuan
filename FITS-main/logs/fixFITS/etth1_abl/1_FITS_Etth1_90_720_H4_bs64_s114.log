Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=26, out_features=234, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5451264.0
params:  6318.0
Trainable parameters:  6318
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.0489373207092285
Epoch: 1, Steps: 61 | Train Loss: 1.5238317 Vali Loss: 2.8299787 Test Loss: 1.4754908
Validation loss decreased (inf --> 2.829979).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.7580010890960693
Epoch: 2, Steps: 61 | Train Loss: 1.1222666 Vali Loss: 2.2808635 Test Loss: 1.0269793
Validation loss decreased (2.829979 --> 2.280864).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.924149513244629
Epoch: 3, Steps: 61 | Train Loss: 0.9200545 Vali Loss: 2.0101233 Test Loss: 0.8059001
Validation loss decreased (2.280864 --> 2.010123).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0571322441101074
Epoch: 4, Steps: 61 | Train Loss: 0.8087878 Vali Loss: 1.8618791 Test Loss: 0.6833364
Validation loss decreased (2.010123 --> 1.861879).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8972721099853516
Epoch: 5, Steps: 61 | Train Loss: 0.7431344 Vali Loss: 1.7820851 Test Loss: 0.6109079
Validation loss decreased (1.861879 --> 1.782085).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.459850549697876
Epoch: 6, Steps: 61 | Train Loss: 0.7030450 Vali Loss: 1.7148421 Test Loss: 0.5660001
Validation loss decreased (1.782085 --> 1.714842).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.001931667327881
Epoch: 7, Steps: 61 | Train Loss: 0.6771961 Vali Loss: 1.6698389 Test Loss: 0.5374538
Validation loss decreased (1.714842 --> 1.669839).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.081841468811035
Epoch: 8, Steps: 61 | Train Loss: 0.6603820 Vali Loss: 1.6610893 Test Loss: 0.5184764
Validation loss decreased (1.669839 --> 1.661089).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.983712673187256
Epoch: 9, Steps: 61 | Train Loss: 0.6485931 Vali Loss: 1.6428713 Test Loss: 0.5056970
Validation loss decreased (1.661089 --> 1.642871).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.9671669006347656
Epoch: 10, Steps: 61 | Train Loss: 0.6410677 Vali Loss: 1.6277215 Test Loss: 0.4970601
Validation loss decreased (1.642871 --> 1.627722).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.9856739044189453
Epoch: 11, Steps: 61 | Train Loss: 0.6354555 Vali Loss: 1.6193042 Test Loss: 0.4909217
Validation loss decreased (1.627722 --> 1.619304).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.3478736877441406
Epoch: 12, Steps: 61 | Train Loss: 0.6305898 Vali Loss: 1.6118782 Test Loss: 0.4865224
Validation loss decreased (1.619304 --> 1.611878).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.474807024002075
Epoch: 13, Steps: 61 | Train Loss: 0.6280135 Vali Loss: 1.6042376 Test Loss: 0.4832262
Validation loss decreased (1.611878 --> 1.604238).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.2686989307403564
Epoch: 14, Steps: 61 | Train Loss: 0.6258264 Vali Loss: 1.6012362 Test Loss: 0.4808449
Validation loss decreased (1.604238 --> 1.601236).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.177492618560791
Epoch: 15, Steps: 61 | Train Loss: 0.6235928 Vali Loss: 1.5976076 Test Loss: 0.4790471
Validation loss decreased (1.601236 --> 1.597608).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.6952943801879883
Epoch: 16, Steps: 61 | Train Loss: 0.6216625 Vali Loss: 1.5946696 Test Loss: 0.4776353
Validation loss decreased (1.597608 --> 1.594670).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.302875518798828
Epoch: 17, Steps: 61 | Train Loss: 0.6210367 Vali Loss: 1.5876938 Test Loss: 0.4765175
Validation loss decreased (1.594670 --> 1.587694).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.445970296859741
Epoch: 18, Steps: 61 | Train Loss: 0.6195420 Vali Loss: 1.5913652 Test Loss: 0.4756435
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.7654991149902344
Epoch: 19, Steps: 61 | Train Loss: 0.6191295 Vali Loss: 1.5938210 Test Loss: 0.4749452
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.8703877925872803
Epoch: 20, Steps: 61 | Train Loss: 0.6185575 Vali Loss: 1.5896347 Test Loss: 0.4743631
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.6669628620147705
Epoch: 21, Steps: 61 | Train Loss: 0.6174340 Vali Loss: 1.5821222 Test Loss: 0.4739206
Validation loss decreased (1.587694 --> 1.582122).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.1160812377929688
Epoch: 22, Steps: 61 | Train Loss: 0.6173290 Vali Loss: 1.5779326 Test Loss: 0.4735098
Validation loss decreased (1.582122 --> 1.577933).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.7517342567443848
Epoch: 23, Steps: 61 | Train Loss: 0.6167658 Vali Loss: 1.5844994 Test Loss: 0.4732066
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.0001156330108643
Epoch: 24, Steps: 61 | Train Loss: 0.6163557 Vali Loss: 1.5832939 Test Loss: 0.4729488
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.4646146297454834
Epoch: 25, Steps: 61 | Train Loss: 0.6159338 Vali Loss: 1.5774307 Test Loss: 0.4726813
Validation loss decreased (1.577933 --> 1.577431).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.9075160026550293
Epoch: 26, Steps: 61 | Train Loss: 0.6149006 Vali Loss: 1.5763140 Test Loss: 0.4724666
Validation loss decreased (1.577431 --> 1.576314).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.4844906330108643
Epoch: 27, Steps: 61 | Train Loss: 0.6148779 Vali Loss: 1.5777707 Test Loss: 0.4723182
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.8447318077087402
Epoch: 28, Steps: 61 | Train Loss: 0.6147565 Vali Loss: 1.5778205 Test Loss: 0.4721533
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.55596923828125
Epoch: 29, Steps: 61 | Train Loss: 0.6147544 Vali Loss: 1.5822800 Test Loss: 0.4720507
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.7323451042175293
Epoch: 30, Steps: 61 | Train Loss: 0.6143224 Vali Loss: 1.5761966 Test Loss: 0.4719252
Validation loss decreased (1.576314 --> 1.576197).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.0423502922058105
Epoch: 31, Steps: 61 | Train Loss: 0.6142151 Vali Loss: 1.5727327 Test Loss: 0.4718043
Validation loss decreased (1.576197 --> 1.572733).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.033902645111084
Epoch: 32, Steps: 61 | Train Loss: 0.6138680 Vali Loss: 1.5870385 Test Loss: 0.4717422
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.236246109008789
Epoch: 33, Steps: 61 | Train Loss: 0.6138843 Vali Loss: 1.5794330 Test Loss: 0.4716573
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.7596096992492676
Epoch: 34, Steps: 61 | Train Loss: 0.6136534 Vali Loss: 1.5788841 Test Loss: 0.4715905
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.383288860321045
Epoch: 35, Steps: 61 | Train Loss: 0.6135464 Vali Loss: 1.5727820 Test Loss: 0.4715379
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.6041269302368164
Epoch: 36, Steps: 61 | Train Loss: 0.6135253 Vali Loss: 1.5774117 Test Loss: 0.4714772
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.245544910430908
Epoch: 37, Steps: 61 | Train Loss: 0.6132920 Vali Loss: 1.5772696 Test Loss: 0.4714389
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.275775671005249
Epoch: 38, Steps: 61 | Train Loss: 0.6132937 Vali Loss: 1.5798761 Test Loss: 0.4714072
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.1118688583374023
Epoch: 39, Steps: 61 | Train Loss: 0.6130981 Vali Loss: 1.5741265 Test Loss: 0.4713607
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.7336509227752686
Epoch: 40, Steps: 61 | Train Loss: 0.6126280 Vali Loss: 1.5787513 Test Loss: 0.4713209
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.355743646621704
Epoch: 41, Steps: 61 | Train Loss: 0.6127764 Vali Loss: 1.5796328 Test Loss: 0.4712980
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.660126209259033
Epoch: 42, Steps: 61 | Train Loss: 0.6126235 Vali Loss: 1.5736201 Test Loss: 0.4712821
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.418783664703369
Epoch: 43, Steps: 61 | Train Loss: 0.6125264 Vali Loss: 1.5749508 Test Loss: 0.4712558
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.625232219696045
Epoch: 44, Steps: 61 | Train Loss: 0.6124501 Vali Loss: 1.5771673 Test Loss: 0.4712368
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.8384859561920166
Epoch: 45, Steps: 61 | Train Loss: 0.6124158 Vali Loss: 1.5706582 Test Loss: 0.4712022
Validation loss decreased (1.572733 --> 1.570658).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.358842611312866
Epoch: 46, Steps: 61 | Train Loss: 0.6123833 Vali Loss: 1.5707470 Test Loss: 0.4711943
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.5058326721191406
Epoch: 47, Steps: 61 | Train Loss: 0.6121701 Vali Loss: 1.5732076 Test Loss: 0.4711734
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.6882803440093994
Epoch: 48, Steps: 61 | Train Loss: 0.6121777 Vali Loss: 1.5807481 Test Loss: 0.4711645
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.996732473373413
Epoch: 49, Steps: 61 | Train Loss: 0.6125243 Vali Loss: 1.5806673 Test Loss: 0.4711529
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.1456286907196045
Epoch: 50, Steps: 61 | Train Loss: 0.6121620 Vali Loss: 1.5746465 Test Loss: 0.4711539
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.978696346282959
Epoch: 51, Steps: 61 | Train Loss: 0.6120195 Vali Loss: 1.5720034 Test Loss: 0.4711367
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.8577845096588135
Epoch: 52, Steps: 61 | Train Loss: 0.6119609 Vali Loss: 1.5719447 Test Loss: 0.4711278
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.016407251358032
Epoch: 53, Steps: 61 | Train Loss: 0.6120733 Vali Loss: 1.5747523 Test Loss: 0.4711175
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.9628350734710693
Epoch: 54, Steps: 61 | Train Loss: 0.6121731 Vali Loss: 1.5714315 Test Loss: 0.4711179
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.5224223136901855
Epoch: 55, Steps: 61 | Train Loss: 0.6119782 Vali Loss: 1.5739863 Test Loss: 0.4711100
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.654503345489502
Epoch: 56, Steps: 61 | Train Loss: 0.6122524 Vali Loss: 1.5813284 Test Loss: 0.4710909
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.5986318588256836
Epoch: 57, Steps: 61 | Train Loss: 0.6119324 Vali Loss: 1.5705534 Test Loss: 0.4711020
Validation loss decreased (1.570658 --> 1.570553).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.137613296508789
Epoch: 58, Steps: 61 | Train Loss: 0.6119159 Vali Loss: 1.5731496 Test Loss: 0.4710976
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.3666136264801025
Epoch: 59, Steps: 61 | Train Loss: 0.6118260 Vali Loss: 1.5733116 Test Loss: 0.4710855
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.1479287147521973
Epoch: 60, Steps: 61 | Train Loss: 0.6119348 Vali Loss: 1.5827661 Test Loss: 0.4710892
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 4.193955183029175
Epoch: 61, Steps: 61 | Train Loss: 0.6118471 Vali Loss: 1.5701070 Test Loss: 0.4710893
Validation loss decreased (1.570553 --> 1.570107).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.856783390045166
Epoch: 62, Steps: 61 | Train Loss: 0.6114988 Vali Loss: 1.5777669 Test Loss: 0.4710812
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.614258050918579
Epoch: 63, Steps: 61 | Train Loss: 0.6118254 Vali Loss: 1.5762192 Test Loss: 0.4710840
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.9670219421386719
Epoch: 64, Steps: 61 | Train Loss: 0.6115986 Vali Loss: 1.5797036 Test Loss: 0.4710772
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.3624303340911865
Epoch: 65, Steps: 61 | Train Loss: 0.6115378 Vali Loss: 1.5770619 Test Loss: 0.4710821
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.3108177185058594
Epoch: 66, Steps: 61 | Train Loss: 0.6118884 Vali Loss: 1.5744148 Test Loss: 0.4710818
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.1481668949127197
Epoch: 67, Steps: 61 | Train Loss: 0.6116924 Vali Loss: 1.5696647 Test Loss: 0.4710714
Validation loss decreased (1.570107 --> 1.569665).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.980915069580078
Epoch: 68, Steps: 61 | Train Loss: 0.6116911 Vali Loss: 1.5721776 Test Loss: 0.4710742
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.4472317695617676
Epoch: 69, Steps: 61 | Train Loss: 0.6116065 Vali Loss: 1.5721629 Test Loss: 0.4710697
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.9553170204162598
Epoch: 70, Steps: 61 | Train Loss: 0.6116719 Vali Loss: 1.5744779 Test Loss: 0.4710733
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.8032889366149902
Epoch: 71, Steps: 61 | Train Loss: 0.6115804 Vali Loss: 1.5752684 Test Loss: 0.4710716
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.952277898788452
Epoch: 72, Steps: 61 | Train Loss: 0.6117865 Vali Loss: 1.5747576 Test Loss: 0.4710702
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.080791711807251
Epoch: 73, Steps: 61 | Train Loss: 0.6116827 Vali Loss: 1.5730373 Test Loss: 0.4710684
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.9433140754699707
Epoch: 74, Steps: 61 | Train Loss: 0.6114941 Vali Loss: 1.5714815 Test Loss: 0.4710674
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.0103089809417725
Epoch: 75, Steps: 61 | Train Loss: 0.6112600 Vali Loss: 1.5698025 Test Loss: 0.4710696
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.6997992992401123
Epoch: 76, Steps: 61 | Train Loss: 0.6115127 Vali Loss: 1.5759255 Test Loss: 0.4710670
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.1006834506988525
Epoch: 77, Steps: 61 | Train Loss: 0.6117714 Vali Loss: 1.5716705 Test Loss: 0.4710682
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.0847432613372803
Epoch: 78, Steps: 61 | Train Loss: 0.6116453 Vali Loss: 1.5769795 Test Loss: 0.4710717
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 3.243584394454956
Epoch: 79, Steps: 61 | Train Loss: 0.6117299 Vali Loss: 1.5747021 Test Loss: 0.4710709
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 3.000565528869629
Epoch: 80, Steps: 61 | Train Loss: 0.6113110 Vali Loss: 1.5756383 Test Loss: 0.4710720
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.600201368331909
Epoch: 81, Steps: 61 | Train Loss: 0.6116605 Vali Loss: 1.5699117 Test Loss: 0.4710711
EarlyStopping counter: 14 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.430082082748413
Epoch: 82, Steps: 61 | Train Loss: 0.6115562 Vali Loss: 1.5705993 Test Loss: 0.4710740
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.9896366596221924
Epoch: 83, Steps: 61 | Train Loss: 0.6112339 Vali Loss: 1.5721520 Test Loss: 0.4710746
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.616126775741577
Epoch: 84, Steps: 61 | Train Loss: 0.6114740 Vali Loss: 1.5751320 Test Loss: 0.4710730
EarlyStopping counter: 17 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.376338481903076
Epoch: 85, Steps: 61 | Train Loss: 0.6110832 Vali Loss: 1.5781958 Test Loss: 0.4710756
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.136631488800049
Epoch: 86, Steps: 61 | Train Loss: 0.6115115 Vali Loss: 1.5752912 Test Loss: 0.4710745
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.96256685256958
Epoch: 87, Steps: 61 | Train Loss: 0.6113521 Vali Loss: 1.5729098 Test Loss: 0.4710757
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4697934687137604, mae:0.462395042181015, rse:0.6561535596847534, corr:[0.2252209  0.22706799 0.22441934 0.2281297  0.22632799 0.22317804
 0.22491807 0.22640085 0.22556478 0.22437711 0.22401956 0.22355233
 0.2225036  0.22150968 0.22130893 0.22175473 0.22187246 0.22182554
 0.22246578 0.22263637 0.22188333 0.22171777 0.22196534 0.22181477
 0.22000588 0.21893762 0.2196055  0.22035564 0.21976012 0.21977392
 0.22091812 0.22087076 0.21966842 0.21921556 0.21961887 0.21934468
 0.21854466 0.21828362 0.21869028 0.2188022  0.21893999 0.21924062
 0.21989635 0.2200861  0.21993318 0.22001092 0.22010875 0.21989292
 0.21875912 0.21767746 0.2171403  0.2167518  0.21579996 0.21493657
 0.21521802 0.21508205 0.21452157 0.21419045 0.21387516 0.21359211
 0.21326862 0.21308653 0.21301672 0.21294424 0.21318369 0.21345438
 0.21391553 0.21411923 0.21376128 0.21351564 0.21311389 0.2122202
 0.21048659 0.20947397 0.20923227 0.20952486 0.20925914 0.2092212
 0.21012798 0.21025099 0.2096701  0.20917615 0.20912649 0.20895748
 0.20838785 0.20817457 0.20850918 0.20855618 0.20843649 0.20852126
 0.20873308 0.20884128 0.20878437 0.2090469  0.20933427 0.209256
 0.20858522 0.2083869  0.20857964 0.20875546 0.2090094  0.20930783
 0.20996581 0.21024163 0.21001771 0.2096382  0.20934112 0.20904766
 0.20873562 0.20852667 0.2087187  0.20864706 0.20877974 0.20886964
 0.20910311 0.20930557 0.20931408 0.2092685  0.20904313 0.20826405
 0.20680831 0.20571607 0.20480657 0.2042539  0.20426589 0.20450129
 0.20540217 0.2059086  0.20587277 0.20555362 0.20522809 0.20517029
 0.20493107 0.20466869 0.20476788 0.20463938 0.20472631 0.20494938
 0.2051468  0.20534536 0.20539445 0.20561658 0.20572945 0.20515612
 0.20376495 0.20280115 0.2023512  0.2016939  0.20092337 0.20083202
 0.20166002 0.20196393 0.20194197 0.20176421 0.20150691 0.20149347
 0.2013386  0.20094419 0.20076528 0.20066717 0.20075148 0.20089267
 0.20116109 0.20144565 0.20157841 0.201681   0.20162207 0.2010027
 0.19977319 0.19931418 0.19963123 0.19986425 0.19916956 0.19857328
 0.19919981 0.1994719  0.19942172 0.19927347 0.19912122 0.19914071
 0.19906725 0.19871306 0.19864576 0.19859895 0.19864444 0.1989653
 0.1994272  0.19984585 0.20014451 0.2002247  0.20019636 0.1997174
 0.19814822 0.197075   0.19638887 0.19564594 0.19479915 0.1947029
 0.19530971 0.19565275 0.19569942 0.19543035 0.19498995 0.19489047
 0.1947889  0.1945973  0.19455226 0.19442779 0.19442758 0.19452864
 0.19464551 0.19477919 0.19487567 0.19499795 0.19488783 0.19423501
 0.19286813 0.19220464 0.19221112 0.19241919 0.19277294 0.19372824
 0.19523509 0.1961238  0.19656725 0.19646928 0.19596344 0.19584835
 0.19587371 0.19562128 0.19557525 0.19557737 0.19544384 0.19533399
 0.19535409 0.19541042 0.19546127 0.19557555 0.19559413 0.19492334
 0.19350985 0.19266537 0.19245665 0.19230103 0.19228815 0.1927591
 0.19389947 0.19448239 0.19484189 0.19482595 0.19431864 0.19402622
 0.19392101 0.19364102 0.19355509 0.19343053 0.19325045 0.19326334
 0.19350168 0.1935594  0.19345012 0.19336767 0.1932258  0.1926994
 0.19131689 0.19068591 0.19078623 0.19107848 0.19125417 0.1921089
 0.19351947 0.19425127 0.19454762 0.19459833 0.19426817 0.19388658
 0.19367705 0.19345368 0.19319712 0.19312681 0.19316925 0.19337606
 0.19371288 0.1940568  0.19439404 0.19474551 0.19501376 0.1948877
 0.19423099 0.1942433  0.19537094 0.19606557 0.19606067 0.19644602
 0.1974128  0.19770324 0.19772795 0.19753037 0.19708769 0.19665292
 0.19634214 0.1960972  0.1959446  0.19582164 0.19586033 0.19589487
 0.19599801 0.19598834 0.19597444 0.19624922 0.19621566 0.19546737
 0.19426614 0.1936955  0.19381544 0.19375116 0.1936108  0.1936085
 0.1944664  0.19482641 0.19483908 0.1945833  0.19406633 0.1936647
 0.19349794 0.1933498  0.19317767 0.19316456 0.1932031  0.19327322
 0.19340451 0.19349138 0.19339626 0.19335073 0.1932434  0.19268666
 0.19130504 0.19039108 0.18981628 0.18955293 0.18955442 0.19029605
 0.19183187 0.19267705 0.19292301 0.19273104 0.19252868 0.1923549
 0.1921645  0.19202518 0.19201192 0.19204198 0.1920615  0.19216357
 0.19225253 0.19215702 0.1918514  0.19178848 0.19147296 0.19045411
 0.18877414 0.1878689  0.1876524  0.18704881 0.18654445 0.18660954
 0.18793017 0.18851274 0.18868208 0.18861635 0.18841603 0.18812557
 0.1878731  0.18761113 0.18732758 0.18700702 0.18702435 0.18723468
 0.18745348 0.18750732 0.18742052 0.1875691  0.187539   0.18721353
 0.18633865 0.18621808 0.18680267 0.18735851 0.18752584 0.18775435
 0.1892079  0.18993227 0.18988957 0.18967447 0.18963575 0.1896233
 0.18942629 0.18947382 0.1897528  0.1898158  0.18994205 0.1903075
 0.19070639 0.19102854 0.19113052 0.19128095 0.1914041  0.19120736
 0.19023393 0.18978535 0.19001369 0.19030428 0.1905515  0.1911325
 0.19265477 0.19340912 0.19361907 0.19348939 0.19343789 0.19344905
 0.1933517  0.1933767  0.19328275 0.19310628 0.1930869  0.19326305
 0.19356394 0.19378147 0.19372053 0.19401592 0.19427173 0.19403505
 0.19302471 0.19247545 0.19271486 0.1933388  0.19377728 0.1945632
 0.19608854 0.19701082 0.19723204 0.19703063 0.19685017 0.19687481
 0.19678034 0.19661498 0.19644918 0.19621348 0.19614868 0.1963658
 0.19666171 0.19666032 0.19656742 0.19680542 0.19690475 0.19667499
 0.19616409 0.19650333 0.19688411 0.19708899 0.1971089  0.19737945
 0.19844148 0.19901323 0.19923574 0.19907424 0.19892928 0.19919859
 0.19937208 0.19943066 0.19960733 0.1996823  0.1997741  0.20003146
 0.20040223 0.20053847 0.20032597 0.20002285 0.19984467 0.19924341
 0.19790561 0.19727033 0.1969533  0.19691584 0.19706203 0.19745228
 0.19876148 0.19934525 0.19951114 0.19923069 0.19887553 0.19894814
 0.19889084 0.19851136 0.19828935 0.19826648 0.19833171 0.19843668
 0.19864398 0.19872676 0.19869113 0.19899356 0.19952208 0.19963662
 0.19910021 0.19945444 0.20023355 0.20072351 0.20092712 0.20103616
 0.20179635 0.20207678 0.20233828 0.20209    0.2014954  0.20158789
 0.20196652 0.20193283 0.20201513 0.2022969  0.2024969  0.2025928
 0.20295209 0.20342037 0.20354952 0.20375675 0.20429103 0.20433475
 0.20308264 0.2021501  0.20206317 0.20238124 0.20229135 0.20223661
 0.20289417 0.20331162 0.20355792 0.20348626 0.20311098 0.20310333
 0.20324557 0.2032503  0.20325826 0.20330104 0.20356077 0.20375457
 0.20387237 0.20395418 0.20381372 0.20366071 0.20349762 0.20275961
 0.20108989 0.20001888 0.19987944 0.1995418  0.19898836 0.19887908
 0.1996639  0.19996095 0.20011084 0.20000356 0.19962299 0.1994495
 0.19947128 0.1993067  0.19915146 0.19910616 0.19919416 0.19929239
 0.19947413 0.19957192 0.19951858 0.19947508 0.19946626 0.198849
 0.19726408 0.19653508 0.19636348 0.19636808 0.19644375 0.19660303
 0.19751905 0.19790605 0.198061   0.19814403 0.19786194 0.19763994
 0.19752543 0.19736934 0.1972614  0.19720638 0.19725129 0.19730929
 0.19744512 0.19754958 0.19738002 0.19714655 0.19689189 0.19612119
 0.19435407 0.19312704 0.19285451 0.19266786 0.19246568 0.19261618
 0.19341479 0.19366261 0.19373792 0.19376703 0.19347747 0.19335665
 0.19351454 0.19343342 0.19324091 0.19334432 0.19355538 0.19356477
 0.19356859 0.19375505 0.19372755 0.19382109 0.19395284 0.1933647
 0.1916963  0.1905741  0.19013567 0.18989648 0.18943636 0.18943888
 0.1903329  0.19034979 0.19007526 0.19004026 0.18979205 0.1896442
 0.18972121 0.18945873 0.18899736 0.18865478 0.1886806  0.18872757
 0.18853185 0.18850069 0.1886669  0.18901065 0.18919577 0.18884145
 0.1874404  0.18634401 0.18587752 0.18590768 0.18573268 0.1858099
 0.18681012 0.18668027 0.18645962 0.18647034 0.18614285 0.18566324
 0.18558322 0.1854137  0.18503389 0.18474497 0.18482119 0.18495077
 0.18478315 0.18455972 0.18445294 0.18455692 0.18438993 0.1832699
 0.1806878  0.1782412  0.17656404 0.17521101 0.17384776 0.17272928
 0.17251825 0.17230439 0.17205301 0.1715816  0.1712528  0.17141774
 0.17153372 0.17131662 0.17100106 0.17087415 0.1707924  0.17064573
 0.17046934 0.17028604 0.17004491 0.17024137 0.17070124 0.1702425
 0.16818053 0.16679838 0.1662527  0.16597348 0.16554624 0.1649553
 0.16566953 0.16624641 0.16629794 0.16567776 0.1649725  0.16466457
 0.16449751 0.16380188 0.16268311 0.16156918 0.1617628  0.1614485
 0.15963224 0.15953885 0.1611666  0.15808332 0.15841623 0.16143386]
