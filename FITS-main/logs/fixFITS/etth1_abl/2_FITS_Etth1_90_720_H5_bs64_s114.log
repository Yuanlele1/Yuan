Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=30, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  8370.0
Trainable parameters:  8370
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.3636066913604736
Epoch: 1, Steps: 61 | Train Loss: 1.5563592 Vali Loss: 2.9902909 Test Loss: 1.6157427
Validation loss decreased (inf --> 2.990291).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.0582809448242188
Epoch: 2, Steps: 61 | Train Loss: 1.1441202 Vali Loss: 2.4205456 Test Loss: 1.1483648
Validation loss decreased (2.990291 --> 2.420546).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8178026676177979
Epoch: 3, Steps: 61 | Train Loss: 0.9201944 Vali Loss: 2.1187959 Test Loss: 0.9012581
Validation loss decreased (2.420546 --> 2.118796).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.8544645309448242
Epoch: 4, Steps: 61 | Train Loss: 0.7921222 Vali Loss: 1.9483767 Test Loss: 0.7570186
Validation loss decreased (2.118796 --> 1.948377).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.417990207672119
Epoch: 5, Steps: 61 | Train Loss: 0.7147413 Vali Loss: 1.8472375 Test Loss: 0.6694825
Validation loss decreased (1.948377 --> 1.847237).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.8755254745483398
Epoch: 6, Steps: 61 | Train Loss: 0.6661757 Vali Loss: 1.7775111 Test Loss: 0.6132792
Validation loss decreased (1.847237 --> 1.777511).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9908032417297363
Epoch: 7, Steps: 61 | Train Loss: 0.6344997 Vali Loss: 1.7304776 Test Loss: 0.5761979
Validation loss decreased (1.777511 --> 1.730478).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.5220632553100586
Epoch: 8, Steps: 61 | Train Loss: 0.6130854 Vali Loss: 1.6975547 Test Loss: 0.5508246
Validation loss decreased (1.730478 --> 1.697555).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.8726623058319092
Epoch: 9, Steps: 61 | Train Loss: 0.5986385 Vali Loss: 1.6719977 Test Loss: 0.5332187
Validation loss decreased (1.697555 --> 1.671998).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.913339614868164
Epoch: 10, Steps: 61 | Train Loss: 0.5882233 Vali Loss: 1.6539706 Test Loss: 0.5205476
Validation loss decreased (1.671998 --> 1.653971).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9745213985443115
Epoch: 11, Steps: 61 | Train Loss: 0.5807834 Vali Loss: 1.6482427 Test Loss: 0.5111971
Validation loss decreased (1.653971 --> 1.648243).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9776434898376465
Epoch: 12, Steps: 61 | Train Loss: 0.5750976 Vali Loss: 1.6287014 Test Loss: 0.5042077
Validation loss decreased (1.648243 --> 1.628701).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9930622577667236
Epoch: 13, Steps: 61 | Train Loss: 0.5708847 Vali Loss: 1.6219368 Test Loss: 0.4987202
Validation loss decreased (1.628701 --> 1.621937).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.7229092121124268
Epoch: 14, Steps: 61 | Train Loss: 0.5671615 Vali Loss: 1.6185682 Test Loss: 0.4945440
Validation loss decreased (1.621937 --> 1.618568).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.719592809677124
Epoch: 15, Steps: 61 | Train Loss: 0.5641579 Vali Loss: 1.6106807 Test Loss: 0.4912502
Validation loss decreased (1.618568 --> 1.610681).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.8490679264068604
Epoch: 16, Steps: 61 | Train Loss: 0.5623899 Vali Loss: 1.6142125 Test Loss: 0.4885097
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.9674372673034668
Epoch: 17, Steps: 61 | Train Loss: 0.5609592 Vali Loss: 1.6086618 Test Loss: 0.4863850
Validation loss decreased (1.610681 --> 1.608662).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.143937587738037
Epoch: 18, Steps: 61 | Train Loss: 0.5592632 Vali Loss: 1.6011939 Test Loss: 0.4845928
Validation loss decreased (1.608662 --> 1.601194).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.8310692310333252
Epoch: 19, Steps: 61 | Train Loss: 0.5579351 Vali Loss: 1.5993850 Test Loss: 0.4830373
Validation loss decreased (1.601194 --> 1.599385).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9518990516662598
Epoch: 20, Steps: 61 | Train Loss: 0.5568293 Vali Loss: 1.6026500 Test Loss: 0.4818064
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.744917869567871
Epoch: 21, Steps: 61 | Train Loss: 0.5558590 Vali Loss: 1.5902380 Test Loss: 0.4806638
Validation loss decreased (1.599385 --> 1.590238).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.8919265270233154
Epoch: 22, Steps: 61 | Train Loss: 0.5551020 Vali Loss: 1.5983349 Test Loss: 0.4797674
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.838210105895996
Epoch: 23, Steps: 61 | Train Loss: 0.5543836 Vali Loss: 1.5917375 Test Loss: 0.4789549
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.374856472015381
Epoch: 24, Steps: 61 | Train Loss: 0.5536122 Vali Loss: 1.5917053 Test Loss: 0.4782425
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.8104207515716553
Epoch: 25, Steps: 61 | Train Loss: 0.5531798 Vali Loss: 1.5949020 Test Loss: 0.4775600
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.4553422927856445
Epoch: 26, Steps: 61 | Train Loss: 0.5527076 Vali Loss: 1.5863743 Test Loss: 0.4770302
Validation loss decreased (1.590238 --> 1.586374).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.3487114906311035
Epoch: 27, Steps: 61 | Train Loss: 0.5518526 Vali Loss: 1.5885304 Test Loss: 0.4765750
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.7624590396881104
Epoch: 28, Steps: 61 | Train Loss: 0.5514598 Vali Loss: 1.5806659 Test Loss: 0.4760734
Validation loss decreased (1.586374 --> 1.580666).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.530576705932617
Epoch: 29, Steps: 61 | Train Loss: 0.5510427 Vali Loss: 1.5905716 Test Loss: 0.4757371
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.790297508239746
Epoch: 30, Steps: 61 | Train Loss: 0.5507162 Vali Loss: 1.5717806 Test Loss: 0.4753871
Validation loss decreased (1.580666 --> 1.571781).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.8637046813964844
Epoch: 31, Steps: 61 | Train Loss: 0.5503305 Vali Loss: 1.5835690 Test Loss: 0.4750830
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.814026117324829
Epoch: 32, Steps: 61 | Train Loss: 0.5500471 Vali Loss: 1.5853202 Test Loss: 0.4748038
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.7502882480621338
Epoch: 33, Steps: 61 | Train Loss: 0.5500272 Vali Loss: 1.5771588 Test Loss: 0.4745269
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.035473108291626
Epoch: 34, Steps: 61 | Train Loss: 0.5497287 Vali Loss: 1.5785530 Test Loss: 0.4742899
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.2597227096557617
Epoch: 35, Steps: 61 | Train Loss: 0.5493728 Vali Loss: 1.5767586 Test Loss: 0.4741080
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.1143765449523926
Epoch: 36, Steps: 61 | Train Loss: 0.5488319 Vali Loss: 1.5899234 Test Loss: 0.4739033
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.7893321514129639
Epoch: 37, Steps: 61 | Train Loss: 0.5488199 Vali Loss: 1.5841398 Test Loss: 0.4737345
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.8306894302368164
Epoch: 38, Steps: 61 | Train Loss: 0.5484785 Vali Loss: 1.5798192 Test Loss: 0.4735493
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7578201293945312
Epoch: 39, Steps: 61 | Train Loss: 0.5481230 Vali Loss: 1.5847033 Test Loss: 0.4733990
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.0060691833496094
Epoch: 40, Steps: 61 | Train Loss: 0.5480939 Vali Loss: 1.5862640 Test Loss: 0.4732710
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.1968939304351807
Epoch: 41, Steps: 61 | Train Loss: 0.5480131 Vali Loss: 1.5771760 Test Loss: 0.4731378
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.2223892211914062
Epoch: 42, Steps: 61 | Train Loss: 0.5482773 Vali Loss: 1.5863280 Test Loss: 0.4730332
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.073896884918213
Epoch: 43, Steps: 61 | Train Loss: 0.5479152 Vali Loss: 1.5806464 Test Loss: 0.4729286
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.6251420974731445
Epoch: 44, Steps: 61 | Train Loss: 0.5477157 Vali Loss: 1.5795003 Test Loss: 0.4728385
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9462089538574219
Epoch: 45, Steps: 61 | Train Loss: 0.5475635 Vali Loss: 1.5761755 Test Loss: 0.4727366
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.0768473148345947
Epoch: 46, Steps: 61 | Train Loss: 0.5477108 Vali Loss: 1.5794964 Test Loss: 0.4726540
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.4835424423217773
Epoch: 47, Steps: 61 | Train Loss: 0.5472011 Vali Loss: 1.5786312 Test Loss: 0.4725760
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.866196632385254
Epoch: 48, Steps: 61 | Train Loss: 0.5473269 Vali Loss: 1.5770304 Test Loss: 0.4725030
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.7944293022155762
Epoch: 49, Steps: 61 | Train Loss: 0.5472819 Vali Loss: 1.5773886 Test Loss: 0.4724380
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.124445676803589
Epoch: 50, Steps: 61 | Train Loss: 0.5471939 Vali Loss: 1.5810443 Test Loss: 0.4723839
EarlyStopping counter: 20 out of 20
Early stopping
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=30, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  8370.0
Trainable parameters:  8370
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.3993561267852783
Epoch: 1, Steps: 61 | Train Loss: 0.6154566 Vali Loss: 1.5853211 Test Loss: 0.4723157
Validation loss decreased (inf --> 1.585321).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.2870893478393555
Epoch: 2, Steps: 61 | Train Loss: 0.6126394 Vali Loss: 1.5750022 Test Loss: 0.4711478
Validation loss decreased (1.585321 --> 1.575002).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.2639710903167725
Epoch: 3, Steps: 61 | Train Loss: 0.6111926 Vali Loss: 1.5793059 Test Loss: 0.4707040
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9548780918121338
Epoch: 4, Steps: 61 | Train Loss: 0.6106319 Vali Loss: 1.5710027 Test Loss: 0.4705083
Validation loss decreased (1.575002 --> 1.571003).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.3951146602630615
Epoch: 5, Steps: 61 | Train Loss: 0.6097727 Vali Loss: 1.5731854 Test Loss: 0.4704084
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.964695692062378
Epoch: 6, Steps: 61 | Train Loss: 0.6095386 Vali Loss: 1.5643151 Test Loss: 0.4704458
Validation loss decreased (1.571003 --> 1.564315).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9931342601776123
Epoch: 7, Steps: 61 | Train Loss: 0.6090161 Vali Loss: 1.5683421 Test Loss: 0.4706276
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.2595057487487793
Epoch: 8, Steps: 61 | Train Loss: 0.6091683 Vali Loss: 1.5703241 Test Loss: 0.4705820
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.134915828704834
Epoch: 9, Steps: 61 | Train Loss: 0.6088904 Vali Loss: 1.5759195 Test Loss: 0.4707814
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.0125372409820557
Epoch: 10, Steps: 61 | Train Loss: 0.6085504 Vali Loss: 1.5656784 Test Loss: 0.4709306
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.0649657249450684
Epoch: 11, Steps: 61 | Train Loss: 0.6087413 Vali Loss: 1.5749483 Test Loss: 0.4710253
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.110499620437622
Epoch: 12, Steps: 61 | Train Loss: 0.6084664 Vali Loss: 1.5616100 Test Loss: 0.4710606
Validation loss decreased (1.564315 --> 1.561610).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.296548843383789
Epoch: 13, Steps: 61 | Train Loss: 0.6087573 Vali Loss: 1.5716287 Test Loss: 0.4712813
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.443263530731201
Epoch: 14, Steps: 61 | Train Loss: 0.6085817 Vali Loss: 1.5688412 Test Loss: 0.4712675
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.161102771759033
Epoch: 15, Steps: 61 | Train Loss: 0.6083368 Vali Loss: 1.5746677 Test Loss: 0.4713223
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5601422786712646
Epoch: 16, Steps: 61 | Train Loss: 0.6080861 Vali Loss: 1.5622659 Test Loss: 0.4714069
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.338966131210327
Epoch: 17, Steps: 61 | Train Loss: 0.6082425 Vali Loss: 1.5649688 Test Loss: 0.4715294
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.3229196071624756
Epoch: 18, Steps: 61 | Train Loss: 0.6084801 Vali Loss: 1.5693297 Test Loss: 0.4716564
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0984718799591064
Epoch: 19, Steps: 61 | Train Loss: 0.6079810 Vali Loss: 1.5719005 Test Loss: 0.4717667
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.99367356300354
Epoch: 20, Steps: 61 | Train Loss: 0.6084199 Vali Loss: 1.5664358 Test Loss: 0.4717960
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.925777435302734
Epoch: 21, Steps: 61 | Train Loss: 0.6081499 Vali Loss: 1.5642107 Test Loss: 0.4717906
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.554415225982666
Epoch: 22, Steps: 61 | Train Loss: 0.6082303 Vali Loss: 1.5788021 Test Loss: 0.4718644
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.869422674179077
Epoch: 23, Steps: 61 | Train Loss: 0.6083681 Vali Loss: 1.5683064 Test Loss: 0.4719632
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.9407899379730225
Epoch: 24, Steps: 61 | Train Loss: 0.6080668 Vali Loss: 1.5620265 Test Loss: 0.4720484
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.1059272289276123
Epoch: 25, Steps: 61 | Train Loss: 0.6084255 Vali Loss: 1.5678663 Test Loss: 0.4720657
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.191343069076538
Epoch: 26, Steps: 61 | Train Loss: 0.6081939 Vali Loss: 1.5681295 Test Loss: 0.4721323
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.9323484897613525
Epoch: 27, Steps: 61 | Train Loss: 0.6076104 Vali Loss: 1.5585725 Test Loss: 0.4721346
Validation loss decreased (1.561610 --> 1.558573).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.7936668395996094
Epoch: 28, Steps: 61 | Train Loss: 0.6083356 Vali Loss: 1.5690295 Test Loss: 0.4721728
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.79752779006958
Epoch: 29, Steps: 61 | Train Loss: 0.6079050 Vali Loss: 1.5628748 Test Loss: 0.4722835
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.561774253845215
Epoch: 30, Steps: 61 | Train Loss: 0.6081043 Vali Loss: 1.5569021 Test Loss: 0.4722669
Validation loss decreased (1.558573 --> 1.556902).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.0010528564453125
Epoch: 31, Steps: 61 | Train Loss: 0.6080035 Vali Loss: 1.5640039 Test Loss: 0.4722781
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.7857656478881836
Epoch: 32, Steps: 61 | Train Loss: 0.6078569 Vali Loss: 1.5759265 Test Loss: 0.4723553
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.3982019424438477
Epoch: 33, Steps: 61 | Train Loss: 0.6080337 Vali Loss: 1.5705702 Test Loss: 0.4723700
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.968437433242798
Epoch: 34, Steps: 61 | Train Loss: 0.6079564 Vali Loss: 1.5697671 Test Loss: 0.4724106
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.3597474098205566
Epoch: 35, Steps: 61 | Train Loss: 0.6077473 Vali Loss: 1.5652736 Test Loss: 0.4724452
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.795851469039917
Epoch: 36, Steps: 61 | Train Loss: 0.6074612 Vali Loss: 1.5750792 Test Loss: 0.4724470
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.4438416957855225
Epoch: 37, Steps: 61 | Train Loss: 0.6077791 Vali Loss: 1.5621377 Test Loss: 0.4724815
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.804593801498413
Epoch: 38, Steps: 61 | Train Loss: 0.6077372 Vali Loss: 1.5713799 Test Loss: 0.4725139
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.236412286758423
Epoch: 39, Steps: 61 | Train Loss: 0.6077567 Vali Loss: 1.5633955 Test Loss: 0.4725488
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7721006870269775
Epoch: 40, Steps: 61 | Train Loss: 0.6078191 Vali Loss: 1.5663630 Test Loss: 0.4725935
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.8534271717071533
Epoch: 41, Steps: 61 | Train Loss: 0.6077568 Vali Loss: 1.5796278 Test Loss: 0.4725800
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.910140037536621
Epoch: 42, Steps: 61 | Train Loss: 0.6079372 Vali Loss: 1.5629134 Test Loss: 0.4726135
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.7951607704162598
Epoch: 43, Steps: 61 | Train Loss: 0.6078777 Vali Loss: 1.5590236 Test Loss: 0.4726500
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.821016788482666
Epoch: 44, Steps: 61 | Train Loss: 0.6076855 Vali Loss: 1.5683523 Test Loss: 0.4726622
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.8490712642669678
Epoch: 45, Steps: 61 | Train Loss: 0.6078250 Vali Loss: 1.5578127 Test Loss: 0.4726819
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8134100437164307
Epoch: 46, Steps: 61 | Train Loss: 0.6076176 Vali Loss: 1.5733554 Test Loss: 0.4726843
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.4529104232788086
Epoch: 47, Steps: 61 | Train Loss: 0.6075348 Vali Loss: 1.5673956 Test Loss: 0.4727141
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9347796440124512
Epoch: 48, Steps: 61 | Train Loss: 0.6075632 Vali Loss: 1.5694871 Test Loss: 0.4727263
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9538183212280273
Epoch: 49, Steps: 61 | Train Loss: 0.6077201 Vali Loss: 1.5716919 Test Loss: 0.4727474
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.0601565837860107
Epoch: 50, Steps: 61 | Train Loss: 0.6078785 Vali Loss: 1.5707800 Test Loss: 0.4727582
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.47103771567344666, mae:0.46211764216423035, rse:0.6570218801498413, corr:[0.22720432 0.23003006 0.22805776 0.22875787 0.22628625 0.22394834
 0.22441618 0.22428568 0.22381854 0.22413792 0.22347888 0.22348268
 0.22346081 0.22260554 0.2224098  0.22260745 0.22261837 0.22251958
 0.22254528 0.22264902 0.22235976 0.22237596 0.22244762 0.22232148
 0.22085135 0.21993585 0.21981473 0.21987428 0.21991985 0.22030741
 0.22050577 0.22024924 0.21991672 0.21944675 0.21914198 0.21914831
 0.21881057 0.21859404 0.21896261 0.21875021 0.21883968 0.2191661
 0.21961085 0.21988782 0.21997133 0.21995455 0.22000349 0.2198547
 0.21901739 0.21819632 0.21723756 0.2165163  0.21591882 0.21520615
 0.21525268 0.2150178  0.21468395 0.21466768 0.21420208 0.2139134
 0.21369447 0.21340445 0.21308416 0.2128577  0.21315259 0.21324116
 0.21369007 0.21414606 0.21398433 0.2139339  0.21363539 0.21285692
 0.21138063 0.21057418 0.21001983 0.21017246 0.21027008 0.210329
 0.21082585 0.21085641 0.21064192 0.2103422  0.20994633 0.20964807
 0.20939039 0.20933217 0.20938383 0.20906785 0.20899847 0.20911328
 0.20919958 0.2093293  0.20923747 0.2093718  0.20964263 0.20984568
 0.20950292 0.20928846 0.20923391 0.2094248  0.20987898 0.21013746
 0.21055184 0.21066533 0.2103226  0.2100374  0.20974241 0.2092476
 0.20893618 0.20874394 0.20888591 0.2086764  0.20888378 0.20886067
 0.20907308 0.20930877 0.20936811 0.2094856  0.20935713 0.20862234
 0.20741588 0.20658694 0.20560266 0.20497084 0.20506567 0.20537676
 0.20628251 0.2067714  0.20671566 0.2064684  0.20607117 0.20575789
 0.20545155 0.20518361 0.20514362 0.20494029 0.20514846 0.20511214
 0.20517194 0.20545745 0.20539974 0.20559162 0.20584325 0.20550607
 0.20435405 0.2035687  0.20304306 0.20232038 0.201653   0.20161909
 0.20240575 0.20256299 0.20253097 0.20259507 0.20243914 0.2021908
 0.2019199  0.20176762 0.20164949 0.20126571 0.20131475 0.20133495
 0.20159245 0.20205888 0.20216085 0.202273   0.20231292 0.20187953
 0.20084102 0.20034511 0.20054653 0.2008851  0.200284   0.19977647
 0.20035198 0.20046452 0.2002687  0.20017897 0.2001749  0.20016177
 0.20002222 0.19947682 0.19922552 0.19907086 0.1991937  0.19952819
 0.20002912 0.20054361 0.20077695 0.20093843 0.20110624 0.20074008
 0.19933414 0.19844076 0.1975893  0.19667755 0.19605203 0.19605295
 0.19646844 0.19666678 0.19668977 0.19654848 0.19621184 0.19588163
 0.19551535 0.19544086 0.19537923 0.19511099 0.19525264 0.1953497
 0.1955215  0.19579066 0.19573948 0.19579194 0.19570325 0.19524571
 0.1942316  0.19360566 0.19339077 0.1937405  0.19416761 0.19489339
 0.19630262 0.19715951 0.1974287  0.19718689 0.19658533 0.19625895
 0.19620956 0.196098   0.19600357 0.19582112 0.19572274 0.19542973
 0.19542237 0.19563171 0.19570257 0.19579834 0.1959173  0.19546475
 0.19435394 0.19362253 0.1933079  0.19326001 0.19332285 0.19365354
 0.19475646 0.19554918 0.19588043 0.19560868 0.19501129 0.19467793
 0.19431306 0.1939079  0.1937209  0.19339998 0.19325817 0.19322641
 0.19352432 0.19368316 0.19353183 0.19350526 0.19361654 0.1933077
 0.19212417 0.19167337 0.19177938 0.19224511 0.19257586 0.19331667
 0.19463296 0.1953042  0.19524378 0.19523281 0.19516096 0.19476284
 0.19438036 0.19424067 0.19389522 0.19353183 0.19359422 0.19376631
 0.19398941 0.19447361 0.19487776 0.1950882  0.19539317 0.19553706
 0.19498892 0.19471799 0.19550756 0.19635987 0.19665141 0.1972215
 0.19819763 0.19849727 0.19850037 0.19835815 0.19804555 0.19760135
 0.1970296  0.19670454 0.19662338 0.19641805 0.19660884 0.19659738
 0.19673847 0.19697595 0.19695476 0.19708705 0.19721273 0.19667676
 0.19566447 0.1949649  0.19481118 0.19481362 0.19466108 0.19455828
 0.19557054 0.19594854 0.19562422 0.19525044 0.19479036 0.19426754
 0.19392319 0.19385196 0.19373813 0.19359694 0.19368087 0.19391598
 0.19417962 0.19431674 0.19420667 0.194257   0.1941959  0.19365054
 0.19253433 0.19194421 0.19128402 0.19104135 0.19125056 0.19190428
 0.19316375 0.19382389 0.1939572  0.19376858 0.19354674 0.19335361
 0.19322628 0.19305764 0.1929778  0.19288127 0.19286723 0.19295794
 0.19318254 0.19346103 0.19342062 0.19337186 0.19311777 0.19228928
 0.19097692 0.19025438 0.18989865 0.18951266 0.18923372 0.18894961
 0.18981424 0.19029468 0.1904554  0.19026001 0.1898821  0.18947086
 0.18922208 0.18890673 0.18853216 0.18813205 0.18815641 0.18841921
 0.1887272  0.1889581  0.18882315 0.18896188 0.18910007 0.18913992
 0.18888879 0.18904263 0.1893109  0.18979095 0.19019285 0.19027492
 0.1912263  0.1916351  0.19143045 0.19112828 0.19079791 0.19065125
 0.19046548 0.1904929  0.19068834 0.19066736 0.19081786 0.19110858
 0.19128366 0.19177023 0.19197766 0.19216405 0.19251499 0.19251369
 0.1917457  0.19156499 0.19170238 0.19177164 0.19204228 0.19261977
 0.19400162 0.19472973 0.19493745 0.1946631  0.19454992 0.19451672
 0.19428028 0.19418539 0.19389154 0.19355085 0.19355816 0.19375291
 0.19403028 0.19439428 0.19448793 0.19490619 0.19521129 0.19503893
 0.19425297 0.19384414 0.19391508 0.19431315 0.19463556 0.19542786
 0.19696026 0.19771059 0.19784962 0.19779582 0.1975993  0.19739276
 0.19732715 0.19714229 0.19686253 0.19670795 0.19678997 0.19690576
 0.19701174 0.19715285 0.19735758 0.19771183 0.19773497 0.19780691
 0.19779189 0.1980869  0.19810139 0.19827189 0.1984416  0.19879098
 0.19978088 0.20016739 0.20018306 0.20014603 0.20026727 0.20035397
 0.20030105 0.20043704 0.20063904 0.20063609 0.20069027 0.20078267
 0.20117603 0.20151941 0.20148891 0.20125121 0.20109183 0.20073001
 0.19973312 0.19910917 0.19862211 0.19869928 0.19903082 0.19954294
 0.20075117 0.2010332  0.20108014 0.20094556 0.20058829 0.20038931
 0.20016009 0.19981161 0.19959664 0.19962642 0.19983743 0.1999711
 0.20009017 0.20019574 0.20035034 0.20060627 0.20094997 0.20119002
 0.20098847 0.20135777 0.20190069 0.20233256 0.20237258 0.20245014
 0.20313668 0.20320109 0.20317808 0.20278229 0.20216735 0.20203717
 0.20222738 0.20219997 0.20217885 0.20229949 0.20267898 0.20292123
 0.2032552  0.20372406 0.20393544 0.20419158 0.20469172 0.20495607
 0.20406601 0.20315796 0.20280507 0.20309268 0.20302585 0.20298906
 0.20368972 0.20395465 0.20391747 0.20387685 0.2036082  0.20346022
 0.20344791 0.20355715 0.20356067 0.20350622 0.20387605 0.20410976
 0.20436366 0.20461856 0.20456956 0.20446163 0.2043037  0.20373923
 0.2023941  0.20141706 0.20105618 0.20065774 0.20016    0.20006871
 0.20071867 0.20079763 0.20074049 0.20048627 0.19999117 0.19970874
 0.19969907 0.19957225 0.19936776 0.19923538 0.1993262  0.19943897
 0.19969247 0.20004192 0.20017822 0.20017059 0.20020741 0.19976848
 0.19837297 0.19763942 0.197335   0.19744624 0.19746944 0.19751829
 0.19830978 0.19846265 0.19839549 0.19860643 0.19839635 0.19813505
 0.19793122 0.19765584 0.19746767 0.19749029 0.19760747 0.19761188
 0.19783102 0.1981807  0.19808282 0.19783635 0.19767934 0.197058
 0.19551092 0.1943034  0.19373986 0.19360612 0.19348311 0.19349977
 0.1942299  0.19436763 0.19409516 0.1939982  0.1939728  0.1939343
 0.19380154 0.19354214 0.193428   0.19354509 0.19370945 0.19371802
 0.19382013 0.19419268 0.19417833 0.19415446 0.19435543 0.19398744
 0.19257572 0.19156887 0.1909103  0.19067223 0.19029143 0.19016476
 0.19096644 0.19088577 0.19033451 0.19009279 0.18978007 0.18971172
 0.18976426 0.18936093 0.1889154  0.18866794 0.18868276 0.18871255
 0.18866487 0.18876268 0.18875964 0.18906267 0.18951647 0.18946439
 0.18823412 0.18722665 0.186587   0.18665023 0.18672739 0.18687098
 0.18770054 0.18746492 0.18708523 0.18680795 0.18634695 0.1859263
 0.18570527 0.18523115 0.18485717 0.184752   0.18473941 0.18464616
 0.18459105 0.18467897 0.18460386 0.18466678 0.18475617 0.18389313
 0.18140157 0.1790436  0.17738952 0.1758167  0.17416422 0.17313781
 0.17308299 0.17238516 0.17179704 0.17168836 0.17132911 0.17115851
 0.17147765 0.17148238 0.17098771 0.17088147 0.17099962 0.17083634
 0.17074013 0.17087291 0.17086883 0.17129925 0.17199421 0.17169213
 0.17012207 0.16892585 0.16779643 0.16716884 0.16680965 0.16630766
 0.16713881 0.16721731 0.16676927 0.16624321 0.16542915 0.16492113
 0.16473758 0.16365862 0.16286501 0.16253366 0.16220146 0.16197094
 0.16272922 0.16306761 0.16387889 0.16454323 0.16373773 0.16516593]
