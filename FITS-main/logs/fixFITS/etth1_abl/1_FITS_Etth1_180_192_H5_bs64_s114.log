Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_192_FITS_ETTh1_ftM_sl180_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8269
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=50, out_features=103, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4614400.0
params:  5253.0
Trainable parameters:  5253
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.497032880783081
Epoch: 1, Steps: 64 | Train Loss: 0.7161350 Vali Loss: 1.4037303 Test Loss: 0.6844537
Validation loss decreased (inf --> 1.403730).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.2035088539123535
Epoch: 2, Steps: 64 | Train Loss: 0.5405298 Vali Loss: 1.1964067 Test Loss: 0.5451381
Validation loss decreased (1.403730 --> 1.196407).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.7585365772247314
Epoch: 3, Steps: 64 | Train Loss: 0.4780072 Vali Loss: 1.1115865 Test Loss: 0.4938992
Validation loss decreased (1.196407 --> 1.111586).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.3942346572875977
Epoch: 4, Steps: 64 | Train Loss: 0.4524659 Vali Loss: 1.0712132 Test Loss: 0.4717250
Validation loss decreased (1.111586 --> 1.071213).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.528167247772217
Epoch: 5, Steps: 64 | Train Loss: 0.4394950 Vali Loss: 1.0485967 Test Loss: 0.4597732
Validation loss decreased (1.071213 --> 1.048597).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.87532639503479
Epoch: 6, Steps: 64 | Train Loss: 0.4315428 Vali Loss: 1.0331450 Test Loss: 0.4519886
Validation loss decreased (1.048597 --> 1.033145).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.4571690559387207
Epoch: 7, Steps: 64 | Train Loss: 0.4259652 Vali Loss: 1.0234156 Test Loss: 0.4463388
Validation loss decreased (1.033145 --> 1.023416).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.558448553085327
Epoch: 8, Steps: 64 | Train Loss: 0.4220027 Vali Loss: 1.0148386 Test Loss: 0.4424079
Validation loss decreased (1.023416 --> 1.014839).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.242377996444702
Epoch: 9, Steps: 64 | Train Loss: 0.4193198 Vali Loss: 1.0092970 Test Loss: 0.4396074
Validation loss decreased (1.014839 --> 1.009297).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.1325948238372803
Epoch: 10, Steps: 64 | Train Loss: 0.4169990 Vali Loss: 1.0037261 Test Loss: 0.4373975
Validation loss decreased (1.009297 --> 1.003726).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.2071712017059326
Epoch: 11, Steps: 64 | Train Loss: 0.4156432 Vali Loss: 1.0003558 Test Loss: 0.4359410
Validation loss decreased (1.003726 --> 1.000356).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.176283836364746
Epoch: 12, Steps: 64 | Train Loss: 0.4137940 Vali Loss: 0.9971229 Test Loss: 0.4345757
Validation loss decreased (1.000356 --> 0.997123).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6956207752227783
Epoch: 13, Steps: 64 | Train Loss: 0.4121029 Vali Loss: 0.9944318 Test Loss: 0.4338517
Validation loss decreased (0.997123 --> 0.994432).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.9970026016235352
Epoch: 14, Steps: 64 | Train Loss: 0.4114915 Vali Loss: 0.9926267 Test Loss: 0.4330938
Validation loss decreased (0.994432 --> 0.992627).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.018822193145752
Epoch: 15, Steps: 64 | Train Loss: 0.4110994 Vali Loss: 0.9908149 Test Loss: 0.4327291
Validation loss decreased (0.992627 --> 0.990815).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.360466957092285
Epoch: 16, Steps: 64 | Train Loss: 0.4107018 Vali Loss: 0.9891813 Test Loss: 0.4324630
Validation loss decreased (0.990815 --> 0.989181).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.885864734649658
Epoch: 17, Steps: 64 | Train Loss: 0.4098169 Vali Loss: 0.9879329 Test Loss: 0.4323291
Validation loss decreased (0.989181 --> 0.987933).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.556166887283325
Epoch: 18, Steps: 64 | Train Loss: 0.4094650 Vali Loss: 0.9862165 Test Loss: 0.4322175
Validation loss decreased (0.987933 --> 0.986216).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.7958595752716064
Epoch: 19, Steps: 64 | Train Loss: 0.4092250 Vali Loss: 0.9847051 Test Loss: 0.4319706
Validation loss decreased (0.986216 --> 0.984705).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.2237257957458496
Epoch: 20, Steps: 64 | Train Loss: 0.4083291 Vali Loss: 0.9848645 Test Loss: 0.4319980
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.4519543647766113
Epoch: 21, Steps: 64 | Train Loss: 0.4086160 Vali Loss: 0.9839850 Test Loss: 0.4318292
Validation loss decreased (0.984705 --> 0.983985).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.9052667617797852
Epoch: 22, Steps: 64 | Train Loss: 0.4081831 Vali Loss: 0.9828615 Test Loss: 0.4320803
Validation loss decreased (0.983985 --> 0.982861).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9546797275543213
Epoch: 23, Steps: 64 | Train Loss: 0.4080234 Vali Loss: 0.9820184 Test Loss: 0.4319003
Validation loss decreased (0.982861 --> 0.982018).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.4214866161346436
Epoch: 24, Steps: 64 | Train Loss: 0.4080021 Vali Loss: 0.9819180 Test Loss: 0.4319885
Validation loss decreased (0.982018 --> 0.981918).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.5691769123077393
Epoch: 25, Steps: 64 | Train Loss: 0.4070293 Vali Loss: 0.9809959 Test Loss: 0.4319770
Validation loss decreased (0.981918 --> 0.980996).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7772095203399658
Epoch: 26, Steps: 64 | Train Loss: 0.4078546 Vali Loss: 0.9807035 Test Loss: 0.4319552
Validation loss decreased (0.980996 --> 0.980704).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.443836212158203
Epoch: 27, Steps: 64 | Train Loss: 0.4072869 Vali Loss: 0.9803284 Test Loss: 0.4320245
Validation loss decreased (0.980704 --> 0.980328).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.7870957851409912
Epoch: 28, Steps: 64 | Train Loss: 0.4072311 Vali Loss: 0.9799303 Test Loss: 0.4320296
Validation loss decreased (0.980328 --> 0.979930).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7098388671875
Epoch: 29, Steps: 64 | Train Loss: 0.4073687 Vali Loss: 0.9797147 Test Loss: 0.4319992
Validation loss decreased (0.979930 --> 0.979715).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.1324222087860107
Epoch: 30, Steps: 64 | Train Loss: 0.4069974 Vali Loss: 0.9786429 Test Loss: 0.4321970
Validation loss decreased (0.979715 --> 0.978643).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.3817455768585205
Epoch: 31, Steps: 64 | Train Loss: 0.4067416 Vali Loss: 0.9789444 Test Loss: 0.4321942
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.1502397060394287
Epoch: 32, Steps: 64 | Train Loss: 0.4070439 Vali Loss: 0.9785451 Test Loss: 0.4321109
Validation loss decreased (0.978643 --> 0.978545).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.708505392074585
Epoch: 33, Steps: 64 | Train Loss: 0.4066230 Vali Loss: 0.9786656 Test Loss: 0.4321502
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.016310214996338
Epoch: 34, Steps: 64 | Train Loss: 0.4067000 Vali Loss: 0.9784872 Test Loss: 0.4321208
Validation loss decreased (0.978545 --> 0.978487).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.2108139991760254
Epoch: 35, Steps: 64 | Train Loss: 0.4069867 Vali Loss: 0.9781529 Test Loss: 0.4321869
Validation loss decreased (0.978487 --> 0.978153).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.2392067909240723
Epoch: 36, Steps: 64 | Train Loss: 0.4068656 Vali Loss: 0.9779533 Test Loss: 0.4322710
Validation loss decreased (0.978153 --> 0.977953).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.1653528213500977
Epoch: 37, Steps: 64 | Train Loss: 0.4070715 Vali Loss: 0.9775588 Test Loss: 0.4322698
Validation loss decreased (0.977953 --> 0.977559).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.321909189224243
Epoch: 38, Steps: 64 | Train Loss: 0.4064406 Vali Loss: 0.9775340 Test Loss: 0.4322928
Validation loss decreased (0.977559 --> 0.977534).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.8158867359161377
Epoch: 39, Steps: 64 | Train Loss: 0.4066687 Vali Loss: 0.9776084 Test Loss: 0.4322779
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.4360077381134033
Epoch: 40, Steps: 64 | Train Loss: 0.4066015 Vali Loss: 0.9773946 Test Loss: 0.4323714
Validation loss decreased (0.977534 --> 0.977395).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.5856783390045166
Epoch: 41, Steps: 64 | Train Loss: 0.4067749 Vali Loss: 0.9766785 Test Loss: 0.4324048
Validation loss decreased (0.977395 --> 0.976678).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.7665693759918213
Epoch: 42, Steps: 64 | Train Loss: 0.4065117 Vali Loss: 0.9770442 Test Loss: 0.4323554
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.782322645187378
Epoch: 43, Steps: 64 | Train Loss: 0.4065587 Vali Loss: 0.9769732 Test Loss: 0.4323612
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.1291682720184326
Epoch: 44, Steps: 64 | Train Loss: 0.4064282 Vali Loss: 0.9767711 Test Loss: 0.4323660
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.182443141937256
Epoch: 45, Steps: 64 | Train Loss: 0.4065198 Vali Loss: 0.9765872 Test Loss: 0.4323663
Validation loss decreased (0.976678 --> 0.976587).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.018476724624634
Epoch: 46, Steps: 64 | Train Loss: 0.4064131 Vali Loss: 0.9765255 Test Loss: 0.4323778
Validation loss decreased (0.976587 --> 0.976526).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.6000866889953613
Epoch: 47, Steps: 64 | Train Loss: 0.4064089 Vali Loss: 0.9764318 Test Loss: 0.4323519
Validation loss decreased (0.976526 --> 0.976432).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.556220769882202
Epoch: 48, Steps: 64 | Train Loss: 0.4064543 Vali Loss: 0.9762279 Test Loss: 0.4324004
Validation loss decreased (0.976432 --> 0.976228).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.1877036094665527
Epoch: 49, Steps: 64 | Train Loss: 0.4060316 Vali Loss: 0.9761533 Test Loss: 0.4324076
Validation loss decreased (0.976228 --> 0.976153).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.631850481033325
Epoch: 50, Steps: 64 | Train Loss: 0.4062279 Vali Loss: 0.9759629 Test Loss: 0.4323976
Validation loss decreased (0.976153 --> 0.975963).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.8078503608703613
Epoch: 51, Steps: 64 | Train Loss: 0.4063615 Vali Loss: 0.9761961 Test Loss: 0.4324294
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.1426732540130615
Epoch: 52, Steps: 64 | Train Loss: 0.4062445 Vali Loss: 0.9761713 Test Loss: 0.4324889
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.1013126373291016
Epoch: 53, Steps: 64 | Train Loss: 0.4065717 Vali Loss: 0.9760858 Test Loss: 0.4324262
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.917154312133789
Epoch: 54, Steps: 64 | Train Loss: 0.4062899 Vali Loss: 0.9756920 Test Loss: 0.4324423
Validation loss decreased (0.975963 --> 0.975692).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.3866004943847656
Epoch: 55, Steps: 64 | Train Loss: 0.4060180 Vali Loss: 0.9758830 Test Loss: 0.4324363
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.3197591304779053
Epoch: 56, Steps: 64 | Train Loss: 0.4061046 Vali Loss: 0.9755475 Test Loss: 0.4324823
Validation loss decreased (0.975692 --> 0.975547).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.9486629962921143
Epoch: 57, Steps: 64 | Train Loss: 0.4063171 Vali Loss: 0.9754445 Test Loss: 0.4324761
Validation loss decreased (0.975547 --> 0.975444).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.903724431991577
Epoch: 58, Steps: 64 | Train Loss: 0.4061789 Vali Loss: 0.9757869 Test Loss: 0.4324744
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.7286369800567627
Epoch: 59, Steps: 64 | Train Loss: 0.4059877 Vali Loss: 0.9758410 Test Loss: 0.4324773
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.612701416015625
Epoch: 60, Steps: 64 | Train Loss: 0.4064046 Vali Loss: 0.9755799 Test Loss: 0.4324590
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.5991134643554688
Epoch: 61, Steps: 64 | Train Loss: 0.4068337 Vali Loss: 0.9752613 Test Loss: 0.4324968
Validation loss decreased (0.975444 --> 0.975261).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.070234775543213
Epoch: 62, Steps: 64 | Train Loss: 0.4064385 Vali Loss: 0.9757668 Test Loss: 0.4324595
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.0425868034362793
Epoch: 63, Steps: 64 | Train Loss: 0.4061603 Vali Loss: 0.9756327 Test Loss: 0.4324681
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.166541814804077
Epoch: 64, Steps: 64 | Train Loss: 0.4061536 Vali Loss: 0.9748664 Test Loss: 0.4324875
Validation loss decreased (0.975261 --> 0.974866).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.7709617614746094
Epoch: 65, Steps: 64 | Train Loss: 0.4058894 Vali Loss: 0.9754657 Test Loss: 0.4325071
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.8779821395874023
Epoch: 66, Steps: 64 | Train Loss: 0.4061100 Vali Loss: 0.9753297 Test Loss: 0.4325098
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.8332433700561523
Epoch: 67, Steps: 64 | Train Loss: 0.4058728 Vali Loss: 0.9754710 Test Loss: 0.4324894
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.5251874923706055
Epoch: 68, Steps: 64 | Train Loss: 0.4057700 Vali Loss: 0.9751927 Test Loss: 0.4324904
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.8090693950653076
Epoch: 69, Steps: 64 | Train Loss: 0.4059158 Vali Loss: 0.9749536 Test Loss: 0.4324962
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.884809732437134
Epoch: 70, Steps: 64 | Train Loss: 0.4064731 Vali Loss: 0.9748895 Test Loss: 0.4325169
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.4959208965301514
Epoch: 71, Steps: 64 | Train Loss: 0.4061693 Vali Loss: 0.9753667 Test Loss: 0.4324813
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.639955759048462
Epoch: 72, Steps: 64 | Train Loss: 0.4057709 Vali Loss: 0.9753925 Test Loss: 0.4325039
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.783524513244629
Epoch: 73, Steps: 64 | Train Loss: 0.4060691 Vali Loss: 0.9742602 Test Loss: 0.4325022
Validation loss decreased (0.974866 --> 0.974260).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.766160726547241
Epoch: 74, Steps: 64 | Train Loss: 0.4064000 Vali Loss: 0.9748880 Test Loss: 0.4325123
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.378359794616699
Epoch: 75, Steps: 64 | Train Loss: 0.4060435 Vali Loss: 0.9753148 Test Loss: 0.4324890
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.8315200805664062
Epoch: 76, Steps: 64 | Train Loss: 0.4060636 Vali Loss: 0.9747457 Test Loss: 0.4325176
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.718991994857788
Epoch: 77, Steps: 64 | Train Loss: 0.4061708 Vali Loss: 0.9751990 Test Loss: 0.4325009
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.9820172786712646
Epoch: 78, Steps: 64 | Train Loss: 0.4062138 Vali Loss: 0.9747534 Test Loss: 0.4325029
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.008995532989502
Epoch: 79, Steps: 64 | Train Loss: 0.4062287 Vali Loss: 0.9751656 Test Loss: 0.4325091
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.9686894416809082
Epoch: 80, Steps: 64 | Train Loss: 0.4062030 Vali Loss: 0.9744061 Test Loss: 0.4325039
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.524010419845581
Epoch: 81, Steps: 64 | Train Loss: 0.4059257 Vali Loss: 0.9750240 Test Loss: 0.4324882
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.2977705001831055
Epoch: 82, Steps: 64 | Train Loss: 0.4055125 Vali Loss: 0.9752456 Test Loss: 0.4324971
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.8226985931396484
Epoch: 83, Steps: 64 | Train Loss: 0.4059110 Vali Loss: 0.9746151 Test Loss: 0.4325052
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.9135301113128662
Epoch: 84, Steps: 64 | Train Loss: 0.4059194 Vali Loss: 0.9748467 Test Loss: 0.4325030
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.7587802410125732
Epoch: 85, Steps: 64 | Train Loss: 0.4059145 Vali Loss: 0.9747416 Test Loss: 0.4325008
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.9009840488433838
Epoch: 86, Steps: 64 | Train Loss: 0.4062951 Vali Loss: 0.9749149 Test Loss: 0.4325103
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.7257208824157715
Epoch: 87, Steps: 64 | Train Loss: 0.4063059 Vali Loss: 0.9750776 Test Loss: 0.4324940
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.3751730918884277
Epoch: 88, Steps: 64 | Train Loss: 0.4061481 Vali Loss: 0.9751748 Test Loss: 0.4325002
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.0789170265197754
Epoch: 89, Steps: 64 | Train Loss: 0.4061621 Vali Loss: 0.9750800 Test Loss: 0.4325038
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.750519037246704
Epoch: 90, Steps: 64 | Train Loss: 0.4060815 Vali Loss: 0.9750165 Test Loss: 0.4325068
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.6671934127807617
Epoch: 91, Steps: 64 | Train Loss: 0.4062158 Vali Loss: 0.9747275 Test Loss: 0.4325111
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.9024126529693604
Epoch: 92, Steps: 64 | Train Loss: 0.4063557 Vali Loss: 0.9743379 Test Loss: 0.4325086
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.8469431400299072
Epoch: 93, Steps: 64 | Train Loss: 0.4059503 Vali Loss: 0.9750128 Test Loss: 0.4325184
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_192_FITS_ETTh1_ftM_sl180_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.42606985569000244, mae:0.41989821195602417, rse:0.619865894317627, corr:[0.26279637 0.26853374 0.26778668 0.26774198 0.2661331  0.26321375
 0.26240817 0.26348683 0.26327378 0.26248446 0.26260027 0.26283833
 0.2622634  0.26131493 0.26105148 0.26131746 0.26152188 0.26150995
 0.26134565 0.2612502  0.261261   0.26147726 0.26155755 0.26111093
 0.26020762 0.25960097 0.2589772  0.2585275  0.25816905 0.2577748
 0.25727725 0.25707534 0.2571326  0.25689945 0.25663927 0.25717542
 0.25767374 0.2574822  0.2573807  0.2576266  0.257964   0.25824216
 0.2585135  0.2586441  0.25865895 0.25893366 0.25935084 0.25910008
 0.25774142 0.25629488 0.25468367 0.25331068 0.25227708 0.25132865
 0.2505306  0.25010884 0.25021875 0.250611   0.250438   0.25069794
 0.25113162 0.25095034 0.25082353 0.25073826 0.2506072  0.25069788
 0.25111082 0.2511325  0.25071827 0.25062007 0.25083455 0.2502001
 0.24866338 0.24754907 0.24667938 0.24607794 0.24578336 0.24555385
 0.24538387 0.24508606 0.24475268 0.24464107 0.244452   0.24448873
 0.24464624 0.24448477 0.24466135 0.2448646  0.24465708 0.2444321
 0.24450439 0.2445099  0.24439147 0.24463205 0.24507014 0.24498504
 0.24415027 0.24348672 0.24290805 0.24213713 0.24165998 0.24138746
 0.24113317 0.24124745 0.24151386 0.24165633 0.24148051 0.2416981
 0.24193867 0.24153225 0.2412435  0.24143839 0.2416855  0.24167024
 0.24163875 0.24170999 0.24168454 0.24166763 0.24164508 0.24115801
 0.2399884  0.23893945 0.23774551 0.2366727  0.23584734 0.23536178
 0.2351266  0.23521619 0.23547207 0.23571353 0.23563291 0.23605178
 0.23682377 0.23694824 0.23691998 0.23681574 0.2367534  0.23690446
 0.23720218 0.2372644  0.23706393 0.23718333 0.23736356 0.23684113
 0.23566364 0.23482735 0.23391251 0.23257014 0.23175772 0.23145853
 0.23132607 0.231243   0.2316243  0.23208493 0.23217858 0.23255636
 0.23298319 0.23282069 0.23288096 0.23291996 0.23259749 0.23239765
 0.23261963 0.23274593 0.23251216 0.23252328 0.23285662 0.23253205
 0.23125431 0.23026156 0.22961317 0.22900726 0.22839206 0.22788076
 0.22800241 0.22867279 0.22914602 0.22974335 0.230339   0.23134255
 0.23236921 0.23211385 0.23166336 0.23182641 0.23207845 0.23182946
 0.231338   0.23151593 0.23182388 0.23121673 0.23139125 0.23287484]
