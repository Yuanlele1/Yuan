Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20290816.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.082906484603882
Epoch: 1, Steps: 60 | Train Loss: 0.6408295 Vali Loss: 1.5990659 Test Loss: 0.8324018
Validation loss decreased (inf --> 1.599066).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.0215795040130615
Epoch: 2, Steps: 60 | Train Loss: 0.5058687 Vali Loss: 1.4245391 Test Loss: 0.7400550
Validation loss decreased (1.599066 --> 1.424539).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.1439061164855957
Epoch: 3, Steps: 60 | Train Loss: 0.4370741 Vali Loss: 1.3495300 Test Loss: 0.7036313
Validation loss decreased (1.424539 --> 1.349530).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9916276931762695
Epoch: 4, Steps: 60 | Train Loss: 0.3964639 Vali Loss: 1.3098321 Test Loss: 0.6847059
Validation loss decreased (1.349530 --> 1.309832).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.0326108932495117
Epoch: 5, Steps: 60 | Train Loss: 0.3677768 Vali Loss: 1.2854153 Test Loss: 0.6730330
Validation loss decreased (1.309832 --> 1.285415).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.066638469696045
Epoch: 6, Steps: 60 | Train Loss: 0.3456023 Vali Loss: 1.2687734 Test Loss: 0.6656913
Validation loss decreased (1.285415 --> 1.268773).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.8933658599853516
Epoch: 7, Steps: 60 | Train Loss: 0.3267504 Vali Loss: 1.2533350 Test Loss: 0.6568530
Validation loss decreased (1.268773 --> 1.253335).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.4884777069091797
Epoch: 8, Steps: 60 | Train Loss: 0.3106094 Vali Loss: 1.2378520 Test Loss: 0.6476460
Validation loss decreased (1.253335 --> 1.237852).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.9765591621398926
Epoch: 9, Steps: 60 | Train Loss: 0.2967293 Vali Loss: 1.2233591 Test Loss: 0.6383215
Validation loss decreased (1.237852 --> 1.223359).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.021291494369507
Epoch: 10, Steps: 60 | Train Loss: 0.2843834 Vali Loss: 1.2112745 Test Loss: 0.6304650
Validation loss decreased (1.223359 --> 1.211275).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9336764812469482
Epoch: 11, Steps: 60 | Train Loss: 0.2733854 Vali Loss: 1.1995440 Test Loss: 0.6221380
Validation loss decreased (1.211275 --> 1.199544).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.0306148529052734
Epoch: 12, Steps: 60 | Train Loss: 0.2634359 Vali Loss: 1.1890010 Test Loss: 0.6149542
Validation loss decreased (1.199544 --> 1.189001).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.987198829650879
Epoch: 13, Steps: 60 | Train Loss: 0.2547466 Vali Loss: 1.1788458 Test Loss: 0.6075833
Validation loss decreased (1.189001 --> 1.178846).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.9401235580444336
Epoch: 14, Steps: 60 | Train Loss: 0.2467219 Vali Loss: 1.1679678 Test Loss: 0.5995624
Validation loss decreased (1.178846 --> 1.167968).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.9599080085754395
Epoch: 15, Steps: 60 | Train Loss: 0.2395245 Vali Loss: 1.1601728 Test Loss: 0.5935956
Validation loss decreased (1.167968 --> 1.160173).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.187349557876587
Epoch: 16, Steps: 60 | Train Loss: 0.2328678 Vali Loss: 1.1514610 Test Loss: 0.5875358
Validation loss decreased (1.160173 --> 1.151461).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.9305906295776367
Epoch: 17, Steps: 60 | Train Loss: 0.2269256 Vali Loss: 1.1437565 Test Loss: 0.5815925
Validation loss decreased (1.151461 --> 1.143757).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.2945566177368164
Epoch: 18, Steps: 60 | Train Loss: 0.2214800 Vali Loss: 1.1370314 Test Loss: 0.5765210
Validation loss decreased (1.143757 --> 1.137031).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0420219898223877
Epoch: 19, Steps: 60 | Train Loss: 0.2164832 Vali Loss: 1.1299261 Test Loss: 0.5710417
Validation loss decreased (1.137031 --> 1.129926).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.1852893829345703
Epoch: 20, Steps: 60 | Train Loss: 0.2118493 Vali Loss: 1.1238215 Test Loss: 0.5661582
Validation loss decreased (1.129926 --> 1.123821).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.078983783721924
Epoch: 21, Steps: 60 | Train Loss: 0.2076839 Vali Loss: 1.1184411 Test Loss: 0.5619504
Validation loss decreased (1.123821 --> 1.118441).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0351428985595703
Epoch: 22, Steps: 60 | Train Loss: 0.2037152 Vali Loss: 1.1122886 Test Loss: 0.5574079
Validation loss decreased (1.118441 --> 1.112289).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.1300063133239746
Epoch: 23, Steps: 60 | Train Loss: 0.2002086 Vali Loss: 1.1073090 Test Loss: 0.5533656
Validation loss decreased (1.112289 --> 1.107309).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.0380425453186035
Epoch: 24, Steps: 60 | Train Loss: 0.1968467 Vali Loss: 1.1026950 Test Loss: 0.5494857
Validation loss decreased (1.107309 --> 1.102695).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9944608211517334
Epoch: 25, Steps: 60 | Train Loss: 0.1938462 Vali Loss: 1.0976588 Test Loss: 0.5456083
Validation loss decreased (1.102695 --> 1.097659).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.964442253112793
Epoch: 26, Steps: 60 | Train Loss: 0.1910095 Vali Loss: 1.0937208 Test Loss: 0.5424439
Validation loss decreased (1.097659 --> 1.093721).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.944498062133789
Epoch: 27, Steps: 60 | Train Loss: 0.1883893 Vali Loss: 1.0895970 Test Loss: 0.5390213
Validation loss decreased (1.093721 --> 1.089597).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.856780767440796
Epoch: 28, Steps: 60 | Train Loss: 0.1859738 Vali Loss: 1.0859466 Test Loss: 0.5362580
Validation loss decreased (1.089597 --> 1.085947).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.0240964889526367
Epoch: 29, Steps: 60 | Train Loss: 0.1836536 Vali Loss: 1.0820563 Test Loss: 0.5333560
Validation loss decreased (1.085947 --> 1.082056).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.0964198112487793
Epoch: 30, Steps: 60 | Train Loss: 0.1815474 Vali Loss: 1.0794352 Test Loss: 0.5310415
Validation loss decreased (1.082056 --> 1.079435).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.9261367321014404
Epoch: 31, Steps: 60 | Train Loss: 0.1795858 Vali Loss: 1.0757718 Test Loss: 0.5282786
Validation loss decreased (1.079435 --> 1.075772).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.9192757606506348
Epoch: 32, Steps: 60 | Train Loss: 0.1777376 Vali Loss: 1.0726588 Test Loss: 0.5261430
Validation loss decreased (1.075772 --> 1.072659).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.016695499420166
Epoch: 33, Steps: 60 | Train Loss: 0.1760441 Vali Loss: 1.0697863 Test Loss: 0.5236557
Validation loss decreased (1.072659 --> 1.069786).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.103449583053589
Epoch: 34, Steps: 60 | Train Loss: 0.1745023 Vali Loss: 1.0671332 Test Loss: 0.5216398
Validation loss decreased (1.069786 --> 1.067133).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.0969250202178955
Epoch: 35, Steps: 60 | Train Loss: 0.1729440 Vali Loss: 1.0651473 Test Loss: 0.5195992
Validation loss decreased (1.067133 --> 1.065147).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.130380630493164
Epoch: 36, Steps: 60 | Train Loss: 0.1713826 Vali Loss: 1.0623319 Test Loss: 0.5178373
Validation loss decreased (1.065147 --> 1.062332).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9864611625671387
Epoch: 37, Steps: 60 | Train Loss: 0.1700229 Vali Loss: 1.0604261 Test Loss: 0.5160514
Validation loss decreased (1.062332 --> 1.060426).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9166879653930664
Epoch: 38, Steps: 60 | Train Loss: 0.1688524 Vali Loss: 1.0583190 Test Loss: 0.5141470
Validation loss decreased (1.060426 --> 1.058319).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.160743236541748
Epoch: 39, Steps: 60 | Train Loss: 0.1676597 Vali Loss: 1.0565468 Test Loss: 0.5127283
Validation loss decreased (1.058319 --> 1.056547).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.043043851852417
Epoch: 40, Steps: 60 | Train Loss: 0.1666026 Vali Loss: 1.0545347 Test Loss: 0.5110695
Validation loss decreased (1.056547 --> 1.054535).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.0359373092651367
Epoch: 41, Steps: 60 | Train Loss: 0.1655642 Vali Loss: 1.0527856 Test Loss: 0.5100276
Validation loss decreased (1.054535 --> 1.052786).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.179150104522705
Epoch: 42, Steps: 60 | Train Loss: 0.1645364 Vali Loss: 1.0511893 Test Loss: 0.5083959
Validation loss decreased (1.052786 --> 1.051189).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.078014373779297
Epoch: 43, Steps: 60 | Train Loss: 0.1636601 Vali Loss: 1.0498941 Test Loss: 0.5072605
Validation loss decreased (1.051189 --> 1.049894).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.9125547409057617
Epoch: 44, Steps: 60 | Train Loss: 0.1627342 Vali Loss: 1.0485142 Test Loss: 0.5060080
Validation loss decreased (1.049894 --> 1.048514).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.054814577102661
Epoch: 45, Steps: 60 | Train Loss: 0.1618458 Vali Loss: 1.0471866 Test Loss: 0.5049579
Validation loss decreased (1.048514 --> 1.047187).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.118443489074707
Epoch: 46, Steps: 60 | Train Loss: 0.1610937 Vali Loss: 1.0453035 Test Loss: 0.5039341
Validation loss decreased (1.047187 --> 1.045303).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.1046130657196045
Epoch: 47, Steps: 60 | Train Loss: 0.1604145 Vali Loss: 1.0446066 Test Loss: 0.5028571
Validation loss decreased (1.045303 --> 1.044607).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.103243112564087
Epoch: 48, Steps: 60 | Train Loss: 0.1596781 Vali Loss: 1.0432605 Test Loss: 0.5018857
Validation loss decreased (1.044607 --> 1.043260).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.0545971393585205
Epoch: 49, Steps: 60 | Train Loss: 0.1589928 Vali Loss: 1.0423535 Test Loss: 0.5010604
Validation loss decreased (1.043260 --> 1.042354).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.9476227760314941
Epoch: 50, Steps: 60 | Train Loss: 0.1584175 Vali Loss: 1.0405359 Test Loss: 0.5000396
Validation loss decreased (1.042354 --> 1.040536).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.0061705112457275
Epoch: 51, Steps: 60 | Train Loss: 0.1577743 Vali Loss: 1.0400851 Test Loss: 0.4993638
Validation loss decreased (1.040536 --> 1.040085).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.178212881088257
Epoch: 52, Steps: 60 | Train Loss: 0.1572513 Vali Loss: 1.0388837 Test Loss: 0.4985412
Validation loss decreased (1.040085 --> 1.038884).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.1781091690063477
Epoch: 53, Steps: 60 | Train Loss: 0.1567185 Vali Loss: 1.0383558 Test Loss: 0.4977003
Validation loss decreased (1.038884 --> 1.038356).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9620459079742432
Epoch: 54, Steps: 60 | Train Loss: 0.1562002 Vali Loss: 1.0370295 Test Loss: 0.4971130
Validation loss decreased (1.038356 --> 1.037030).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.9394564628601074
Epoch: 55, Steps: 60 | Train Loss: 0.1557137 Vali Loss: 1.0362996 Test Loss: 0.4964549
Validation loss decreased (1.037030 --> 1.036300).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.999284029006958
Epoch: 56, Steps: 60 | Train Loss: 0.1552921 Vali Loss: 1.0359981 Test Loss: 0.4957170
Validation loss decreased (1.036300 --> 1.035998).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.1063835620880127
Epoch: 57, Steps: 60 | Train Loss: 0.1548396 Vali Loss: 1.0349572 Test Loss: 0.4951278
Validation loss decreased (1.035998 --> 1.034957).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.8764820098876953
Epoch: 58, Steps: 60 | Train Loss: 0.1544895 Vali Loss: 1.0344211 Test Loss: 0.4945416
Validation loss decreased (1.034957 --> 1.034421).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.0384984016418457
Epoch: 59, Steps: 60 | Train Loss: 0.1539828 Vali Loss: 1.0336452 Test Loss: 0.4939756
Validation loss decreased (1.034421 --> 1.033645).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9335150718688965
Epoch: 60, Steps: 60 | Train Loss: 0.1536886 Vali Loss: 1.0332954 Test Loss: 0.4935373
Validation loss decreased (1.033645 --> 1.033295).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.977917194366455
Epoch: 61, Steps: 60 | Train Loss: 0.1533712 Vali Loss: 1.0326875 Test Loss: 0.4930260
Validation loss decreased (1.033295 --> 1.032688).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.1092710494995117
Epoch: 62, Steps: 60 | Train Loss: 0.1530885 Vali Loss: 1.0319946 Test Loss: 0.4925751
Validation loss decreased (1.032688 --> 1.031995).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.0245108604431152
Epoch: 63, Steps: 60 | Train Loss: 0.1527372 Vali Loss: 1.0315809 Test Loss: 0.4920547
Validation loss decreased (1.031995 --> 1.031581).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.140673875808716
Epoch: 64, Steps: 60 | Train Loss: 0.1523979 Vali Loss: 1.0306177 Test Loss: 0.4916704
Validation loss decreased (1.031581 --> 1.030618).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.195185899734497
Epoch: 65, Steps: 60 | Train Loss: 0.1520863 Vali Loss: 1.0305152 Test Loss: 0.4913185
Validation loss decreased (1.030618 --> 1.030515).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.9803271293640137
Epoch: 66, Steps: 60 | Train Loss: 0.1519677 Vali Loss: 1.0298834 Test Loss: 0.4909767
Validation loss decreased (1.030515 --> 1.029883).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.1773667335510254
Epoch: 67, Steps: 60 | Train Loss: 0.1516499 Vali Loss: 1.0293367 Test Loss: 0.4906056
Validation loss decreased (1.029883 --> 1.029337).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.889204978942871
Epoch: 68, Steps: 60 | Train Loss: 0.1513952 Vali Loss: 1.0288075 Test Loss: 0.4902010
Validation loss decreased (1.029337 --> 1.028808).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.9892358779907227
Epoch: 69, Steps: 60 | Train Loss: 0.1511300 Vali Loss: 1.0290254 Test Loss: 0.4899471
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.018873691558838
Epoch: 70, Steps: 60 | Train Loss: 0.1509865 Vali Loss: 1.0281777 Test Loss: 0.4896188
Validation loss decreased (1.028808 --> 1.028178).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.045095205307007
Epoch: 71, Steps: 60 | Train Loss: 0.1508193 Vali Loss: 1.0279756 Test Loss: 0.4892846
Validation loss decreased (1.028178 --> 1.027976).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.976696491241455
Epoch: 72, Steps: 60 | Train Loss: 0.1506692 Vali Loss: 1.0279093 Test Loss: 0.4889968
Validation loss decreased (1.027976 --> 1.027909).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.8921568393707275
Epoch: 73, Steps: 60 | Train Loss: 0.1503657 Vali Loss: 1.0271596 Test Loss: 0.4887249
Validation loss decreased (1.027909 --> 1.027160).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.0782172679901123
Epoch: 74, Steps: 60 | Train Loss: 0.1502186 Vali Loss: 1.0271903 Test Loss: 0.4884585
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.9438345432281494
Epoch: 75, Steps: 60 | Train Loss: 0.1500433 Vali Loss: 1.0264894 Test Loss: 0.4882716
Validation loss decreased (1.027160 --> 1.026489).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.025510787963867
Epoch: 76, Steps: 60 | Train Loss: 0.1498174 Vali Loss: 1.0264728 Test Loss: 0.4880258
Validation loss decreased (1.026489 --> 1.026473).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.9777886867523193
Epoch: 77, Steps: 60 | Train Loss: 0.1497097 Vali Loss: 1.0265008 Test Loss: 0.4877874
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.100898504257202
Epoch: 78, Steps: 60 | Train Loss: 0.1495521 Vali Loss: 1.0256609 Test Loss: 0.4875501
Validation loss decreased (1.026473 --> 1.025661).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.137732982635498
Epoch: 79, Steps: 60 | Train Loss: 0.1493587 Vali Loss: 1.0257320 Test Loss: 0.4874206
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.7910678386688232
Epoch: 80, Steps: 60 | Train Loss: 0.1493171 Vali Loss: 1.0257459 Test Loss: 0.4872095
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.076554775238037
Epoch: 81, Steps: 60 | Train Loss: 0.1491198 Vali Loss: 1.0255053 Test Loss: 0.4870495
Validation loss decreased (1.025661 --> 1.025505).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.1293554306030273
Epoch: 82, Steps: 60 | Train Loss: 0.1490024 Vali Loss: 1.0249830 Test Loss: 0.4868651
Validation loss decreased (1.025505 --> 1.024983).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.022066831588745
Epoch: 83, Steps: 60 | Train Loss: 0.1488675 Vali Loss: 1.0250349 Test Loss: 0.4867031
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.2471654415130615
Epoch: 84, Steps: 60 | Train Loss: 0.1488114 Vali Loss: 1.0249448 Test Loss: 0.4865351
Validation loss decreased (1.024983 --> 1.024945).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.910020351409912
Epoch: 85, Steps: 60 | Train Loss: 0.1487636 Vali Loss: 1.0248690 Test Loss: 0.4864013
Validation loss decreased (1.024945 --> 1.024869).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.9447245597839355
Epoch: 86, Steps: 60 | Train Loss: 0.1485825 Vali Loss: 1.0240239 Test Loss: 0.4862569
Validation loss decreased (1.024869 --> 1.024024).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.0552897453308105
Epoch: 87, Steps: 60 | Train Loss: 0.1484575 Vali Loss: 1.0240083 Test Loss: 0.4861379
Validation loss decreased (1.024024 --> 1.024008).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.1542773246765137
Epoch: 88, Steps: 60 | Train Loss: 0.1484085 Vali Loss: 1.0239207 Test Loss: 0.4859810
Validation loss decreased (1.024008 --> 1.023921).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.9522666931152344
Epoch: 89, Steps: 60 | Train Loss: 0.1483797 Vali Loss: 1.0240520 Test Loss: 0.4858889
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.9547436237335205
Epoch: 90, Steps: 60 | Train Loss: 0.1482653 Vali Loss: 1.0240196 Test Loss: 0.4857587
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.036750078201294
Epoch: 91, Steps: 60 | Train Loss: 0.1481606 Vali Loss: 1.0237440 Test Loss: 0.4856786
Validation loss decreased (1.023921 --> 1.023744).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.0951521396636963
Epoch: 92, Steps: 60 | Train Loss: 0.1481247 Vali Loss: 1.0236840 Test Loss: 0.4855633
Validation loss decreased (1.023744 --> 1.023684).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.045623779296875
Epoch: 93, Steps: 60 | Train Loss: 0.1480673 Vali Loss: 1.0236915 Test Loss: 0.4854687
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.9949142932891846
Epoch: 94, Steps: 60 | Train Loss: 0.1481264 Vali Loss: 1.0233287 Test Loss: 0.4853717
Validation loss decreased (1.023684 --> 1.023329).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.9286248683929443
Epoch: 95, Steps: 60 | Train Loss: 0.1479017 Vali Loss: 1.0234805 Test Loss: 0.4852813
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.9513874053955078
Epoch: 96, Steps: 60 | Train Loss: 0.1478580 Vali Loss: 1.0233873 Test Loss: 0.4851947
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.9816033840179443
Epoch: 97, Steps: 60 | Train Loss: 0.1478462 Vali Loss: 1.0227123 Test Loss: 0.4851134
Validation loss decreased (1.023329 --> 1.022712).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.0708224773406982
Epoch: 98, Steps: 60 | Train Loss: 0.1477141 Vali Loss: 1.0227187 Test Loss: 0.4850416
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.113924503326416
Epoch: 99, Steps: 60 | Train Loss: 0.1477647 Vali Loss: 1.0231440 Test Loss: 0.4849595
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.9955024719238281
Epoch: 100, Steps: 60 | Train Loss: 0.1475900 Vali Loss: 1.0228592 Test Loss: 0.4848937
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20290816.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7886369228363037
Epoch: 1, Steps: 60 | Train Loss: 0.4078668 Vali Loss: 0.9595071 Test Loss: 0.4290343
Validation loss decreased (inf --> 0.959507).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9159481525421143
Epoch: 2, Steps: 60 | Train Loss: 0.3892143 Vali Loss: 0.9536653 Test Loss: 0.4211025
Validation loss decreased (0.959507 --> 0.953665).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8419253826141357
Epoch: 3, Steps: 60 | Train Loss: 0.3857231 Vali Loss: 0.9557972 Test Loss: 0.4208772
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9158191680908203
Epoch: 4, Steps: 60 | Train Loss: 0.3847033 Vali Loss: 0.9590037 Test Loss: 0.4215219
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7524189949035645
Epoch: 5, Steps: 60 | Train Loss: 0.3835098 Vali Loss: 0.9598321 Test Loss: 0.4216691
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.9769642353057861
Epoch: 6, Steps: 60 | Train Loss: 0.3829739 Vali Loss: 0.9616631 Test Loss: 0.4225172
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9410815238952637
Epoch: 7, Steps: 60 | Train Loss: 0.3830195 Vali Loss: 0.9611229 Test Loss: 0.4226455
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.930356502532959
Epoch: 8, Steps: 60 | Train Loss: 0.3824108 Vali Loss: 0.9619255 Test Loss: 0.4225039
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.9997591972351074
Epoch: 9, Steps: 60 | Train Loss: 0.3823080 Vali Loss: 0.9623245 Test Loss: 0.4224752
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.8788700103759766
Epoch: 10, Steps: 60 | Train Loss: 0.3821363 Vali Loss: 0.9615830 Test Loss: 0.4226152
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9971330165863037
Epoch: 11, Steps: 60 | Train Loss: 0.3821763 Vali Loss: 0.9617335 Test Loss: 0.4223911
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.8596971035003662
Epoch: 12, Steps: 60 | Train Loss: 0.3816874 Vali Loss: 0.9617079 Test Loss: 0.4223638
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.0840022563934326
Epoch: 13, Steps: 60 | Train Loss: 0.3814970 Vali Loss: 0.9630800 Test Loss: 0.4229469
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.871953010559082
Epoch: 14, Steps: 60 | Train Loss: 0.3816570 Vali Loss: 0.9618784 Test Loss: 0.4224540
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.8780391216278076
Epoch: 15, Steps: 60 | Train Loss: 0.3812422 Vali Loss: 0.9623379 Test Loss: 0.4225289
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.8735439777374268
Epoch: 16, Steps: 60 | Train Loss: 0.3812421 Vali Loss: 0.9621485 Test Loss: 0.4228156
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.931544303894043
Epoch: 17, Steps: 60 | Train Loss: 0.3812419 Vali Loss: 0.9619430 Test Loss: 0.4225889
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.9647345542907715
Epoch: 18, Steps: 60 | Train Loss: 0.3808727 Vali Loss: 0.9617985 Test Loss: 0.4224625
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9158055782318115
Epoch: 19, Steps: 60 | Train Loss: 0.3806981 Vali Loss: 0.9623222 Test Loss: 0.4227034
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.892500638961792
Epoch: 20, Steps: 60 | Train Loss: 0.3810542 Vali Loss: 0.9628079 Test Loss: 0.4227140
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.8947875499725342
Epoch: 21, Steps: 60 | Train Loss: 0.3807648 Vali Loss: 0.9623157 Test Loss: 0.4227813
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.9141440391540527
Epoch: 22, Steps: 60 | Train Loss: 0.3808377 Vali Loss: 0.9621004 Test Loss: 0.4228654
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41820231080055237, mae:0.42704296112060547, rse:0.6141161918640137, corr:[0.2646561  0.27230698 0.271992   0.2684079  0.26564983 0.26462454
 0.26437804 0.26429072 0.26379776 0.26325354 0.26285535 0.26255533
 0.2624441  0.26245522 0.26255232 0.2625903  0.26227158 0.26163816
 0.26081508 0.26011887 0.25972462 0.25955927 0.2592871  0.25895232
 0.25866488 0.2587595  0.25917822 0.25952175 0.25954458 0.25916213
 0.2586086  0.258092   0.25793988 0.25805286 0.258007   0.257802
 0.25757    0.2574432  0.25746158 0.2576389  0.257997   0.25827032
 0.25825205 0.25811693 0.25807482 0.25822684 0.2586137  0.25890225
 0.258519   0.25779265 0.256796   0.2557115  0.25468224 0.25357312
 0.25265542 0.2520762  0.2517274  0.2516354  0.25160283 0.25170976
 0.25186223 0.2519196  0.251853   0.25181714 0.25179535 0.25182506
 0.251942   0.2519443  0.25197402 0.25207463 0.25206563 0.25167176
 0.2510634  0.25047636 0.25001904 0.2498385  0.24961273 0.24903165
 0.24816632 0.24732998 0.24677794 0.24658382 0.24657826 0.24657232
 0.24645662 0.24610303 0.24563308 0.24522893 0.24497207 0.2449052
 0.24486421 0.24474196 0.24456127 0.24451326 0.2447749  0.24524793
 0.2458504  0.2464236  0.24669565 0.24675216 0.2466823  0.24648929
 0.24628602 0.2460681  0.24575976 0.24540243 0.24489307 0.24429838
 0.24397482 0.2439842  0.24436764 0.24499223 0.24536937 0.24538115
 0.24520577 0.244983   0.24485636 0.24486783 0.24491596 0.2448628
 0.24469972 0.24429132 0.24359526 0.24281941 0.24204315 0.24121836
 0.24064808 0.2404181  0.24018174 0.23982869 0.2391612  0.2383122
 0.23758419 0.23726739 0.23729755 0.2373822  0.23752104 0.23761989
 0.23774226 0.23783554 0.23791797 0.23783685 0.23750927 0.23684566
 0.23619384 0.23580296 0.23550126 0.2349479  0.23424128 0.23336892
 0.23262495 0.23222102 0.23210314 0.23213784 0.23191197 0.23147935
 0.23111692 0.2309994  0.23102367 0.23081823 0.23023123 0.22967935
 0.22923602 0.22901273 0.22907245 0.22906463 0.22888035 0.22844084
 0.2284392  0.22929509 0.23016432 0.230108   0.22873048 0.22676511
 0.22527835 0.22501534 0.22533117 0.22585176 0.22586884 0.22533578
 0.22445653 0.2232872  0.22198735 0.2211941  0.22137526 0.22210611
 0.22197305 0.21996622 0.21774095 0.21756169 0.21853484 0.21313663]
