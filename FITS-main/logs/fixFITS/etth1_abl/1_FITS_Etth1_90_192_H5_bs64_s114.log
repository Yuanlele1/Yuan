Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_192_FITS_ETTh1_ftM_sl90_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8359
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=30, out_features=94, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2526720.0
params:  2914.0
Trainable parameters:  2914
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8177359104156494
Epoch: 1, Steps: 65 | Train Loss: 0.8140317 Vali Loss: 1.5026484 Test Loss: 0.8242313
Validation loss decreased (inf --> 1.502648).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6875603199005127
Epoch: 2, Steps: 65 | Train Loss: 0.6167363 Vali Loss: 1.2735560 Test Loss: 0.6255868
Validation loss decreased (1.502648 --> 1.273556).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.1019723415374756
Epoch: 3, Steps: 65 | Train Loss: 0.5309627 Vali Loss: 1.1694947 Test Loss: 0.5387921
Validation loss decreased (1.273556 --> 1.169495).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0461692810058594
Epoch: 4, Steps: 65 | Train Loss: 0.4888475 Vali Loss: 1.1142081 Test Loss: 0.4956878
Validation loss decreased (1.169495 --> 1.114208).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.026042938232422
Epoch: 5, Steps: 65 | Train Loss: 0.4658801 Vali Loss: 1.0824373 Test Loss: 0.4731095
Validation loss decreased (1.114208 --> 1.082437).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.3212106227874756
Epoch: 6, Steps: 65 | Train Loss: 0.4527646 Vali Loss: 1.0629722 Test Loss: 0.4606785
Validation loss decreased (1.082437 --> 1.062972).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.6295828819274902
Epoch: 7, Steps: 65 | Train Loss: 0.4453067 Vali Loss: 1.0503817 Test Loss: 0.4534129
Validation loss decreased (1.062972 --> 1.050382).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8437867164611816
Epoch: 8, Steps: 65 | Train Loss: 0.4397327 Vali Loss: 1.0418634 Test Loss: 0.4491284
Validation loss decreased (1.050382 --> 1.041863).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.9729926586151123
Epoch: 9, Steps: 65 | Train Loss: 0.4362805 Vali Loss: 1.0350835 Test Loss: 0.4462652
Validation loss decreased (1.041863 --> 1.035084).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.9524166584014893
Epoch: 10, Steps: 65 | Train Loss: 0.4337800 Vali Loss: 1.0308102 Test Loss: 0.4442616
Validation loss decreased (1.035084 --> 1.030810).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.370084047317505
Epoch: 11, Steps: 65 | Train Loss: 0.4314692 Vali Loss: 1.0272425 Test Loss: 0.4429612
Validation loss decreased (1.030810 --> 1.027243).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.0483615398406982
Epoch: 12, Steps: 65 | Train Loss: 0.4306399 Vali Loss: 1.0246123 Test Loss: 0.4419326
Validation loss decreased (1.027243 --> 1.024612).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.2414329051971436
Epoch: 13, Steps: 65 | Train Loss: 0.4293081 Vali Loss: 1.0225766 Test Loss: 0.4410835
Validation loss decreased (1.024612 --> 1.022577).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.2896852493286133
Epoch: 14, Steps: 65 | Train Loss: 0.4281442 Vali Loss: 1.0203383 Test Loss: 0.4405788
Validation loss decreased (1.022577 --> 1.020338).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.343294143676758
Epoch: 15, Steps: 65 | Train Loss: 0.4275938 Vali Loss: 1.0191381 Test Loss: 0.4400568
Validation loss decreased (1.020338 --> 1.019138).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.82950496673584
Epoch: 16, Steps: 65 | Train Loss: 0.4266662 Vali Loss: 1.0175632 Test Loss: 0.4396037
Validation loss decreased (1.019138 --> 1.017563).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.355541944503784
Epoch: 17, Steps: 65 | Train Loss: 0.4263954 Vali Loss: 1.0161043 Test Loss: 0.4392734
Validation loss decreased (1.017563 --> 1.016104).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.211043119430542
Epoch: 18, Steps: 65 | Train Loss: 0.4257008 Vali Loss: 1.0154139 Test Loss: 0.4390756
Validation loss decreased (1.016104 --> 1.015414).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9010567665100098
Epoch: 19, Steps: 65 | Train Loss: 0.4254586 Vali Loss: 1.0145247 Test Loss: 0.4389125
Validation loss decreased (1.015414 --> 1.014525).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.5992321968078613
Epoch: 20, Steps: 65 | Train Loss: 0.4249985 Vali Loss: 1.0137511 Test Loss: 0.4387263
Validation loss decreased (1.014525 --> 1.013751).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.142983675003052
Epoch: 21, Steps: 65 | Train Loss: 0.4249346 Vali Loss: 1.0128090 Test Loss: 0.4385794
Validation loss decreased (1.013751 --> 1.012809).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.470635414123535
Epoch: 22, Steps: 65 | Train Loss: 0.4248860 Vali Loss: 1.0124156 Test Loss: 0.4383899
Validation loss decreased (1.012809 --> 1.012416).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9897897243499756
Epoch: 23, Steps: 65 | Train Loss: 0.4243104 Vali Loss: 1.0110666 Test Loss: 0.4383430
Validation loss decreased (1.012416 --> 1.011067).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.420961380004883
Epoch: 24, Steps: 65 | Train Loss: 0.4241626 Vali Loss: 1.0107374 Test Loss: 0.4381760
Validation loss decreased (1.011067 --> 1.010737).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.731008529663086
Epoch: 25, Steps: 65 | Train Loss: 0.4235642 Vali Loss: 1.0108215 Test Loss: 0.4381646
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.9400269985198975
Epoch: 26, Steps: 65 | Train Loss: 0.4237329 Vali Loss: 1.0107429 Test Loss: 0.4380907
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.9590234756469727
Epoch: 27, Steps: 65 | Train Loss: 0.4234199 Vali Loss: 1.0101438 Test Loss: 0.4380067
Validation loss decreased (1.010737 --> 1.010144).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.6245198249816895
Epoch: 28, Steps: 65 | Train Loss: 0.4236594 Vali Loss: 1.0094877 Test Loss: 0.4379883
Validation loss decreased (1.010144 --> 1.009488).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.3577654361724854
Epoch: 29, Steps: 65 | Train Loss: 0.4233292 Vali Loss: 1.0096776 Test Loss: 0.4379158
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.643292188644409
Epoch: 30, Steps: 65 | Train Loss: 0.4235229 Vali Loss: 1.0094781 Test Loss: 0.4378862
Validation loss decreased (1.009488 --> 1.009478).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.040105104446411
Epoch: 31, Steps: 65 | Train Loss: 0.4227244 Vali Loss: 1.0088016 Test Loss: 0.4378672
Validation loss decreased (1.009478 --> 1.008802).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.750234603881836
Epoch: 32, Steps: 65 | Train Loss: 0.4230516 Vali Loss: 1.0090473 Test Loss: 0.4378768
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.7401654720306396
Epoch: 33, Steps: 65 | Train Loss: 0.4226748 Vali Loss: 1.0088590 Test Loss: 0.4378365
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5765740871429443
Epoch: 34, Steps: 65 | Train Loss: 0.4227823 Vali Loss: 1.0083071 Test Loss: 0.4378111
Validation loss decreased (1.008802 --> 1.008307).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.6843066215515137
Epoch: 35, Steps: 65 | Train Loss: 0.4227970 Vali Loss: 1.0079676 Test Loss: 0.4377584
Validation loss decreased (1.008307 --> 1.007968).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.96144437789917
Epoch: 36, Steps: 65 | Train Loss: 0.4221211 Vali Loss: 1.0078212 Test Loss: 0.4378081
Validation loss decreased (1.007968 --> 1.007821).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9266762733459473
Epoch: 37, Steps: 65 | Train Loss: 0.4224043 Vali Loss: 1.0077155 Test Loss: 0.4377701
Validation loss decreased (1.007821 --> 1.007715).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.8878412246704102
Epoch: 38, Steps: 65 | Train Loss: 0.4222505 Vali Loss: 1.0077949 Test Loss: 0.4377373
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.192366123199463
Epoch: 39, Steps: 65 | Train Loss: 0.4222305 Vali Loss: 1.0078303 Test Loss: 0.4377228
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.8298959732055664
Epoch: 40, Steps: 65 | Train Loss: 0.4229223 Vali Loss: 1.0074618 Test Loss: 0.4377665
Validation loss decreased (1.007715 --> 1.007462).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 9.553471326828003
Epoch: 41, Steps: 65 | Train Loss: 0.4222350 Vali Loss: 1.0071273 Test Loss: 0.4377542
Validation loss decreased (1.007462 --> 1.007127).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 6.383406400680542
Epoch: 42, Steps: 65 | Train Loss: 0.4223652 Vali Loss: 1.0069978 Test Loss: 0.4377286
Validation loss decreased (1.007127 --> 1.006998).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.758495807647705
Epoch: 43, Steps: 65 | Train Loss: 0.4225487 Vali Loss: 1.0074892 Test Loss: 0.4377521
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.967594861984253
Epoch: 44, Steps: 65 | Train Loss: 0.4221020 Vali Loss: 1.0073383 Test Loss: 0.4377210
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.68812894821167
Epoch: 45, Steps: 65 | Train Loss: 0.4223436 Vali Loss: 1.0072397 Test Loss: 0.4377514
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.095167398452759
Epoch: 46, Steps: 65 | Train Loss: 0.4223571 Vali Loss: 1.0072047 Test Loss: 0.4377221
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.7765295505523682
Epoch: 47, Steps: 65 | Train Loss: 0.4221967 Vali Loss: 1.0067115 Test Loss: 0.4377343
Validation loss decreased (1.006998 --> 1.006711).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.8187425136566162
Epoch: 48, Steps: 65 | Train Loss: 0.4225176 Vali Loss: 1.0068141 Test Loss: 0.4377045
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.0507726669311523
Epoch: 49, Steps: 65 | Train Loss: 0.4222636 Vali Loss: 1.0067815 Test Loss: 0.4377381
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.965148687362671
Epoch: 50, Steps: 65 | Train Loss: 0.4223053 Vali Loss: 1.0060190 Test Loss: 0.4377319
Validation loss decreased (1.006711 --> 1.006019).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9536349773406982
Epoch: 51, Steps: 65 | Train Loss: 0.4221569 Vali Loss: 1.0067858 Test Loss: 0.4377386
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.7172255516052246
Epoch: 52, Steps: 65 | Train Loss: 0.4217746 Vali Loss: 1.0068066 Test Loss: 0.4377224
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.8115665912628174
Epoch: 53, Steps: 65 | Train Loss: 0.4221891 Vali Loss: 1.0063717 Test Loss: 0.4377285
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.824934482574463
Epoch: 54, Steps: 65 | Train Loss: 0.4221836 Vali Loss: 1.0067278 Test Loss: 0.4377057
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.0425310134887695
Epoch: 55, Steps: 65 | Train Loss: 0.4220907 Vali Loss: 1.0062735 Test Loss: 0.4377352
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7263145446777344
Epoch: 56, Steps: 65 | Train Loss: 0.4219363 Vali Loss: 1.0061754 Test Loss: 0.4377452
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.3931522369384766
Epoch: 57, Steps: 65 | Train Loss: 0.4217210 Vali Loss: 1.0065942 Test Loss: 0.4377517
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.6367356777191162
Epoch: 58, Steps: 65 | Train Loss: 0.4221112 Vali Loss: 1.0064760 Test Loss: 0.4377491
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.113341808319092
Epoch: 59, Steps: 65 | Train Loss: 0.4220019 Vali Loss: 1.0064675 Test Loss: 0.4377354
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.2766170501708984
Epoch: 60, Steps: 65 | Train Loss: 0.4222166 Vali Loss: 1.0064702 Test Loss: 0.4377470
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.044672966003418
Epoch: 61, Steps: 65 | Train Loss: 0.4219549 Vali Loss: 1.0059726 Test Loss: 0.4377555
Validation loss decreased (1.006019 --> 1.005973).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.9392359256744385
Epoch: 62, Steps: 65 | Train Loss: 0.4220595 Vali Loss: 1.0061980 Test Loss: 0.4377459
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.9930126667022705
Epoch: 63, Steps: 65 | Train Loss: 0.4219140 Vali Loss: 1.0063457 Test Loss: 0.4377365
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.018456220626831
Epoch: 64, Steps: 65 | Train Loss: 0.4222649 Vali Loss: 1.0062933 Test Loss: 0.4377518
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.3488616943359375
Epoch: 65, Steps: 65 | Train Loss: 0.4221968 Vali Loss: 1.0062799 Test Loss: 0.4377515
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.349891424179077
Epoch: 66, Steps: 65 | Train Loss: 0.4222003 Vali Loss: 1.0062770 Test Loss: 0.4377574
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.4395930767059326
Epoch: 67, Steps: 65 | Train Loss: 0.4223634 Vali Loss: 1.0063184 Test Loss: 0.4377518
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.6527776718139648
Epoch: 68, Steps: 65 | Train Loss: 0.4217657 Vali Loss: 1.0062568 Test Loss: 0.4377545
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.8180932998657227
Epoch: 69, Steps: 65 | Train Loss: 0.4219144 Vali Loss: 1.0061991 Test Loss: 0.4377540
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.6122229099273682
Epoch: 70, Steps: 65 | Train Loss: 0.4221995 Vali Loss: 1.0060139 Test Loss: 0.4377547
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.7485570907592773
Epoch: 71, Steps: 65 | Train Loss: 0.4217968 Vali Loss: 1.0057765 Test Loss: 0.4377592
Validation loss decreased (1.005973 --> 1.005777).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.8347241878509521
Epoch: 72, Steps: 65 | Train Loss: 0.4218214 Vali Loss: 1.0060422 Test Loss: 0.4377551
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.9090774059295654
Epoch: 73, Steps: 65 | Train Loss: 0.4220869 Vali Loss: 1.0052614 Test Loss: 0.4377617
Validation loss decreased (1.005777 --> 1.005261).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.897998571395874
Epoch: 74, Steps: 65 | Train Loss: 0.4218714 Vali Loss: 1.0060793 Test Loss: 0.4377639
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.5032455921173096
Epoch: 75, Steps: 65 | Train Loss: 0.4218230 Vali Loss: 1.0055661 Test Loss: 0.4377683
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.9183990955352783
Epoch: 76, Steps: 65 | Train Loss: 0.4219797 Vali Loss: 1.0059528 Test Loss: 0.4377602
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.6540756225585938
Epoch: 77, Steps: 65 | Train Loss: 0.4219285 Vali Loss: 1.0061476 Test Loss: 0.4377625
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.1746842861175537
Epoch: 78, Steps: 65 | Train Loss: 0.4211397 Vali Loss: 1.0059128 Test Loss: 0.4377671
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.6260707378387451
Epoch: 79, Steps: 65 | Train Loss: 0.4220913 Vali Loss: 1.0057697 Test Loss: 0.4377714
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.7052552700042725
Epoch: 80, Steps: 65 | Train Loss: 0.4219064 Vali Loss: 1.0058317 Test Loss: 0.4377732
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.265784740447998
Epoch: 81, Steps: 65 | Train Loss: 0.4220721 Vali Loss: 1.0051222 Test Loss: 0.4377807
Validation loss decreased (1.005261 --> 1.005122).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.8360793590545654
Epoch: 82, Steps: 65 | Train Loss: 0.4220253 Vali Loss: 1.0059001 Test Loss: 0.4377818
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.6703720092773438
Epoch: 83, Steps: 65 | Train Loss: 0.4216454 Vali Loss: 1.0060514 Test Loss: 0.4377778
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.5667529106140137
Epoch: 84, Steps: 65 | Train Loss: 0.4215415 Vali Loss: 1.0054724 Test Loss: 0.4377826
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.2863290309906006
Epoch: 85, Steps: 65 | Train Loss: 0.4221587 Vali Loss: 1.0050865 Test Loss: 0.4377797
Validation loss decreased (1.005122 --> 1.005087).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.6714909076690674
Epoch: 86, Steps: 65 | Train Loss: 0.4220512 Vali Loss: 1.0059755 Test Loss: 0.4377819
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.823580741882324
Epoch: 87, Steps: 65 | Train Loss: 0.4217404 Vali Loss: 1.0059106 Test Loss: 0.4377842
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.8322649002075195
Epoch: 88, Steps: 65 | Train Loss: 0.4217133 Vali Loss: 1.0049509 Test Loss: 0.4377840
Validation loss decreased (1.005087 --> 1.004951).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.832533836364746
Epoch: 89, Steps: 65 | Train Loss: 0.4219025 Vali Loss: 1.0054096 Test Loss: 0.4377896
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.7455012798309326
Epoch: 90, Steps: 65 | Train Loss: 0.4220715 Vali Loss: 1.0050402 Test Loss: 0.4377854
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.886234998703003
Epoch: 91, Steps: 65 | Train Loss: 0.4221631 Vali Loss: 1.0056465 Test Loss: 0.4377863
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.978135108947754
Epoch: 92, Steps: 65 | Train Loss: 0.4218915 Vali Loss: 1.0060145 Test Loss: 0.4377864
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.705589532852173
Epoch: 93, Steps: 65 | Train Loss: 0.4220443 Vali Loss: 1.0057967 Test Loss: 0.4377843
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.719405174255371
Epoch: 94, Steps: 65 | Train Loss: 0.4216425 Vali Loss: 1.0054178 Test Loss: 0.4377888
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.7703189849853516
Epoch: 95, Steps: 65 | Train Loss: 0.4222005 Vali Loss: 1.0059354 Test Loss: 0.4377879
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 3.6675734519958496
Epoch: 96, Steps: 65 | Train Loss: 0.4220858 Vali Loss: 1.0059215 Test Loss: 0.4377883
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 7.426256895065308
Epoch: 97, Steps: 65 | Train Loss: 0.4218842 Vali Loss: 1.0060209 Test Loss: 0.4377877
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.58937668800354
Epoch: 98, Steps: 65 | Train Loss: 0.4218386 Vali Loss: 1.0057856 Test Loss: 0.4377889
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 3.3766016960144043
Epoch: 99, Steps: 65 | Train Loss: 0.4216175 Vali Loss: 1.0059210 Test Loss: 0.4377903
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.6061429977416992
Epoch: 100, Steps: 65 | Train Loss: 0.4219382 Vali Loss: 1.0057771 Test Loss: 0.4377875
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_90_192_FITS_ETTh1_ftM_sl90_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4377841055393219, mae:0.4238910675048828, rse:0.6283292770385742, corr:[0.2632519  0.26425934 0.26262233 0.26393312 0.26068705 0.25909573
 0.25931045 0.25830156 0.25811517 0.2586794  0.25765818 0.25675902
 0.25648832 0.25611928 0.25598314 0.25615585 0.2566116  0.25683498
 0.25713855 0.25726944 0.25660124 0.25599772 0.2555911  0.25522587
 0.25357273 0.252707   0.2531023  0.2533341  0.25316453 0.2534659
 0.2536322  0.2532797  0.2532948  0.2531068  0.25258732 0.25256163
 0.2525465  0.25258988 0.25280786 0.25289205 0.25326324 0.253727
 0.25416476 0.25426787 0.25409093 0.25389656 0.25365007 0.2532433
 0.2520198  0.25075662 0.25000674 0.24951112 0.24848363 0.24741052
 0.24727763 0.2469433  0.24668396 0.24674195 0.24634708 0.24609548
 0.24623056 0.24635975 0.2461101  0.24601373 0.24632962 0.24637516
 0.24681102 0.2470701  0.24661106 0.24629538 0.24601196 0.2450655
 0.24332799 0.24225245 0.24168867 0.2413651  0.24090104 0.24068393
 0.24092402 0.24074957 0.24064493 0.24048263 0.24017961 0.2399307
 0.23983438 0.23997036 0.24019796 0.24023996 0.24024095 0.2402865
 0.24052368 0.24056605 0.24027479 0.24034426 0.24030954 0.23980159
 0.2387141  0.23795375 0.23758101 0.23728012 0.23713195 0.23703082
 0.23727205 0.2372255  0.237081   0.23706704 0.23691379 0.23663665
 0.23659086 0.23664097 0.23666479 0.23665677 0.23699121 0.23696786
 0.23713236 0.23736233 0.2372162  0.23718072 0.23687692 0.23580578
 0.23400736 0.23275521 0.2318136  0.2308725  0.23039171 0.23042947
 0.23109391 0.23134883 0.23147291 0.23141657 0.2310609  0.23087962
 0.23090887 0.23088282 0.2307493  0.23059957 0.2308592  0.23086257
 0.23101419 0.23130202 0.23110421 0.23107485 0.23092847 0.23000333
 0.2282527  0.22705932 0.22638962 0.22558336 0.22502838 0.2251814
 0.22602755 0.22617722 0.22634411 0.22644311 0.22620456 0.22606729
 0.22599262 0.22583126 0.22554809 0.22535034 0.22535321 0.22527695
 0.22552852 0.2256841  0.22534949 0.22533527 0.22516009 0.22415307
 0.22248681 0.22134505 0.22089791 0.22083154 0.22038664 0.22036752
 0.2211829  0.22138685 0.22143032 0.22145334 0.22148497 0.22170225
 0.22161886 0.22104175 0.22080368 0.22028528 0.21951304 0.21972106
 0.22001144 0.2194163  0.21983883 0.22002329 0.21973377 0.22187029]
