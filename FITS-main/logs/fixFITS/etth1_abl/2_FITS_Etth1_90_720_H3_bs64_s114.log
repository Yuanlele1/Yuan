Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=22, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3902976.0
params:  4554.0
Trainable parameters:  4554
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.5697829723358154
Epoch: 1, Steps: 61 | Train Loss: 1.5129744 Vali Loss: 2.9983916 Test Loss: 1.5884750
Validation loss decreased (inf --> 2.998392).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.085571765899658
Epoch: 2, Steps: 61 | Train Loss: 1.1549492 Vali Loss: 2.4469607 Test Loss: 1.1542504
Validation loss decreased (2.998392 --> 2.446961).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.4968993663787842
Epoch: 3, Steps: 61 | Train Loss: 0.9424793 Vali Loss: 2.1481814 Test Loss: 0.9119277
Validation loss decreased (2.446961 --> 2.148181).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.5005083084106445
Epoch: 4, Steps: 61 | Train Loss: 0.8144945 Vali Loss: 1.9655921 Test Loss: 0.7663680
Validation loss decreased (2.148181 --> 1.965592).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.4806902408599854
Epoch: 5, Steps: 61 | Train Loss: 0.7335471 Vali Loss: 1.8558275 Test Loss: 0.6760557
Validation loss decreased (1.965592 --> 1.855827).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.4466192722320557
Epoch: 6, Steps: 61 | Train Loss: 0.6809724 Vali Loss: 1.7756510 Test Loss: 0.6167566
Validation loss decreased (1.855827 --> 1.775651).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.46635103225708
Epoch: 7, Steps: 61 | Train Loss: 0.6457563 Vali Loss: 1.7259142 Test Loss: 0.5775244
Validation loss decreased (1.775651 --> 1.725914).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.4495599269866943
Epoch: 8, Steps: 61 | Train Loss: 0.6215544 Vali Loss: 1.6870108 Test Loss: 0.5502506
Validation loss decreased (1.725914 --> 1.687011).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4932570457458496
Epoch: 9, Steps: 61 | Train Loss: 0.6045648 Vali Loss: 1.6679456 Test Loss: 0.5311328
Validation loss decreased (1.687011 --> 1.667946).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.493070363998413
Epoch: 10, Steps: 61 | Train Loss: 0.5923881 Vali Loss: 1.6508925 Test Loss: 0.5174640
Validation loss decreased (1.667946 --> 1.650892).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.4997670650482178
Epoch: 11, Steps: 61 | Train Loss: 0.5837791 Vali Loss: 1.6364172 Test Loss: 0.5074270
Validation loss decreased (1.650892 --> 1.636417).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4557349681854248
Epoch: 12, Steps: 61 | Train Loss: 0.5765965 Vali Loss: 1.6233709 Test Loss: 0.4999786
Validation loss decreased (1.636417 --> 1.623371).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5412328243255615
Epoch: 13, Steps: 61 | Train Loss: 0.5720705 Vali Loss: 1.6226526 Test Loss: 0.4944674
Validation loss decreased (1.623371 --> 1.622653).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.4784679412841797
Epoch: 14, Steps: 61 | Train Loss: 0.5683271 Vali Loss: 1.6140506 Test Loss: 0.4901723
Validation loss decreased (1.622653 --> 1.614051).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4064385890960693
Epoch: 15, Steps: 61 | Train Loss: 0.5650930 Vali Loss: 1.6079748 Test Loss: 0.4869218
Validation loss decreased (1.614051 --> 1.607975).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4649174213409424
Epoch: 16, Steps: 61 | Train Loss: 0.5627685 Vali Loss: 1.6037691 Test Loss: 0.4843560
Validation loss decreased (1.607975 --> 1.603769).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4721577167510986
Epoch: 17, Steps: 61 | Train Loss: 0.5609529 Vali Loss: 1.5954298 Test Loss: 0.4823381
Validation loss decreased (1.603769 --> 1.595430).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4876947402954102
Epoch: 18, Steps: 61 | Train Loss: 0.5596277 Vali Loss: 1.6019782 Test Loss: 0.4806997
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4988560676574707
Epoch: 19, Steps: 61 | Train Loss: 0.5578932 Vali Loss: 1.5916960 Test Loss: 0.4794062
Validation loss decreased (1.595430 --> 1.591696).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5271692276000977
Epoch: 20, Steps: 61 | Train Loss: 0.5570848 Vali Loss: 1.5943139 Test Loss: 0.4783083
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.540930986404419
Epoch: 21, Steps: 61 | Train Loss: 0.5561340 Vali Loss: 1.5945976 Test Loss: 0.4774306
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5858993530273438
Epoch: 22, Steps: 61 | Train Loss: 0.5552071 Vali Loss: 1.5878510 Test Loss: 0.4766556
Validation loss decreased (1.591696 --> 1.587851).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5871539115905762
Epoch: 23, Steps: 61 | Train Loss: 0.5542962 Vali Loss: 1.5938443 Test Loss: 0.4760595
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.557908296585083
Epoch: 24, Steps: 61 | Train Loss: 0.5540756 Vali Loss: 1.5914359 Test Loss: 0.4755372
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.569720983505249
Epoch: 25, Steps: 61 | Train Loss: 0.5536011 Vali Loss: 1.5931258 Test Loss: 0.4750881
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4580762386322021
Epoch: 26, Steps: 61 | Train Loss: 0.5530271 Vali Loss: 1.5880318 Test Loss: 0.4747123
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4448847770690918
Epoch: 27, Steps: 61 | Train Loss: 0.5524318 Vali Loss: 1.5842806 Test Loss: 0.4744420
Validation loss decreased (1.587851 --> 1.584281).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5480079650878906
Epoch: 28, Steps: 61 | Train Loss: 0.5525315 Vali Loss: 1.5829766 Test Loss: 0.4741523
Validation loss decreased (1.584281 --> 1.582977).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.3968567848205566
Epoch: 29, Steps: 61 | Train Loss: 0.5521241 Vali Loss: 1.5857515 Test Loss: 0.4739098
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4606356620788574
Epoch: 30, Steps: 61 | Train Loss: 0.5517372 Vali Loss: 1.5832024 Test Loss: 0.4737116
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.482719898223877
Epoch: 31, Steps: 61 | Train Loss: 0.5513541 Vali Loss: 1.5826132 Test Loss: 0.4735329
Validation loss decreased (1.582977 --> 1.582613).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.8253412246704102
Epoch: 32, Steps: 61 | Train Loss: 0.5509836 Vali Loss: 1.5843375 Test Loss: 0.4733825
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.891238212585449
Epoch: 33, Steps: 61 | Train Loss: 0.5511179 Vali Loss: 1.5865206 Test Loss: 0.4732288
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.4948129653930664
Epoch: 34, Steps: 61 | Train Loss: 0.5509154 Vali Loss: 1.5828786 Test Loss: 0.4730970
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.4660859107971191
Epoch: 35, Steps: 61 | Train Loss: 0.5505015 Vali Loss: 1.5848376 Test Loss: 0.4729907
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.4619109630584717
Epoch: 36, Steps: 61 | Train Loss: 0.5504676 Vali Loss: 1.5747266 Test Loss: 0.4729074
Validation loss decreased (1.582613 --> 1.574727).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.4575934410095215
Epoch: 37, Steps: 61 | Train Loss: 0.5502536 Vali Loss: 1.5780573 Test Loss: 0.4728022
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.173884868621826
Epoch: 38, Steps: 61 | Train Loss: 0.5501003 Vali Loss: 1.5857363 Test Loss: 0.4727592
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.753749132156372
Epoch: 39, Steps: 61 | Train Loss: 0.5496657 Vali Loss: 1.5747061 Test Loss: 0.4726903
Validation loss decreased (1.574727 --> 1.574706).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.4893312454223633
Epoch: 40, Steps: 61 | Train Loss: 0.5496971 Vali Loss: 1.5844648 Test Loss: 0.4726233
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.4632728099822998
Epoch: 41, Steps: 61 | Train Loss: 0.5496321 Vali Loss: 1.5790160 Test Loss: 0.4725486
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.4355342388153076
Epoch: 42, Steps: 61 | Train Loss: 0.5498220 Vali Loss: 1.5857711 Test Loss: 0.4724915
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.4681673049926758
Epoch: 43, Steps: 61 | Train Loss: 0.5492249 Vali Loss: 1.5762105 Test Loss: 0.4724536
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.4889307022094727
Epoch: 44, Steps: 61 | Train Loss: 0.5492194 Vali Loss: 1.5800617 Test Loss: 0.4724210
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.4395618438720703
Epoch: 45, Steps: 61 | Train Loss: 0.5490928 Vali Loss: 1.5786362 Test Loss: 0.4723785
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.4806864261627197
Epoch: 46, Steps: 61 | Train Loss: 0.5489946 Vali Loss: 1.5666504 Test Loss: 0.4723438
Validation loss decreased (1.574706 --> 1.566650).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.5916664600372314
Epoch: 47, Steps: 61 | Train Loss: 0.5489679 Vali Loss: 1.5788301 Test Loss: 0.4723080
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.458448886871338
Epoch: 48, Steps: 61 | Train Loss: 0.5492700 Vali Loss: 1.5775411 Test Loss: 0.4722874
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.4904866218566895
Epoch: 49, Steps: 61 | Train Loss: 0.5489715 Vali Loss: 1.5821151 Test Loss: 0.4722624
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.4778709411621094
Epoch: 50, Steps: 61 | Train Loss: 0.5489397 Vali Loss: 1.5775261 Test Loss: 0.4722355
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.4013996124267578
Epoch: 51, Steps: 61 | Train Loss: 0.5485474 Vali Loss: 1.5739093 Test Loss: 0.4722218
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.141977071762085
Epoch: 52, Steps: 61 | Train Loss: 0.5485597 Vali Loss: 1.5765705 Test Loss: 0.4721920
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6850647926330566
Epoch: 53, Steps: 61 | Train Loss: 0.5487483 Vali Loss: 1.5798320 Test Loss: 0.4721957
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.393705129623413
Epoch: 54, Steps: 61 | Train Loss: 0.5486046 Vali Loss: 1.5694246 Test Loss: 0.4721720
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.513892412185669
Epoch: 55, Steps: 61 | Train Loss: 0.5486783 Vali Loss: 1.5756409 Test Loss: 0.4721441
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.480271577835083
Epoch: 56, Steps: 61 | Train Loss: 0.5488148 Vali Loss: 1.5733382 Test Loss: 0.4721293
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.5116462707519531
Epoch: 57, Steps: 61 | Train Loss: 0.5486592 Vali Loss: 1.5764321 Test Loss: 0.4721189
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.5161406993865967
Epoch: 58, Steps: 61 | Train Loss: 0.5485979 Vali Loss: 1.5697827 Test Loss: 0.4721077
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.6399164199829102
Epoch: 59, Steps: 61 | Train Loss: 0.5483601 Vali Loss: 1.5781758 Test Loss: 0.4720977
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.616163969039917
Epoch: 60, Steps: 61 | Train Loss: 0.5482430 Vali Loss: 1.5811660 Test Loss: 0.4720868
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.5753917694091797
Epoch: 61, Steps: 61 | Train Loss: 0.5482934 Vali Loss: 1.5764675 Test Loss: 0.4720751
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6171746253967285
Epoch: 62, Steps: 61 | Train Loss: 0.5482168 Vali Loss: 1.5752368 Test Loss: 0.4720702
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.6130762100219727
Epoch: 63, Steps: 61 | Train Loss: 0.5484261 Vali Loss: 1.5724685 Test Loss: 0.4720650
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.007622480392456
Epoch: 64, Steps: 61 | Train Loss: 0.5480510 Vali Loss: 1.5802195 Test Loss: 0.4720574
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.5361461639404297
Epoch: 65, Steps: 61 | Train Loss: 0.5482167 Vali Loss: 1.5714283 Test Loss: 0.4720442
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.609379529953003
Epoch: 66, Steps: 61 | Train Loss: 0.5484228 Vali Loss: 1.5701540 Test Loss: 0.4720349
EarlyStopping counter: 20 out of 20
Early stopping
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=22, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3902976.0
params:  4554.0
Trainable parameters:  4554
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.639418363571167
Epoch: 1, Steps: 61 | Train Loss: 0.6133897 Vali Loss: 1.5701818 Test Loss: 0.4720079
Validation loss decreased (inf --> 1.570182).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.4105725288391113
Epoch: 2, Steps: 61 | Train Loss: 0.6126028 Vali Loss: 1.5695912 Test Loss: 0.4716138
Validation loss decreased (1.570182 --> 1.569591).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.5644481182098389
Epoch: 3, Steps: 61 | Train Loss: 0.6120424 Vali Loss: 1.5746744 Test Loss: 0.4717663
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.483720064163208
Epoch: 4, Steps: 61 | Train Loss: 0.6115839 Vali Loss: 1.5710125 Test Loss: 0.4716880
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5202100276947021
Epoch: 5, Steps: 61 | Train Loss: 0.6113960 Vali Loss: 1.5787289 Test Loss: 0.4721922
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.4960343837738037
Epoch: 6, Steps: 61 | Train Loss: 0.6109459 Vali Loss: 1.5702814 Test Loss: 0.4720545
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4726064205169678
Epoch: 7, Steps: 61 | Train Loss: 0.6109915 Vali Loss: 1.5689695 Test Loss: 0.4722283
Validation loss decreased (1.569591 --> 1.568969).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.4755194187164307
Epoch: 8, Steps: 61 | Train Loss: 0.6108987 Vali Loss: 1.5719092 Test Loss: 0.4724433
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4730403423309326
Epoch: 9, Steps: 61 | Train Loss: 0.6108914 Vali Loss: 1.5676293 Test Loss: 0.4725007
Validation loss decreased (1.568969 --> 1.567629).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.5573534965515137
Epoch: 10, Steps: 61 | Train Loss: 0.6105725 Vali Loss: 1.5682530 Test Loss: 0.4727973
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.556591272354126
Epoch: 11, Steps: 61 | Train Loss: 0.6108654 Vali Loss: 1.5686430 Test Loss: 0.4726025
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.524256706237793
Epoch: 12, Steps: 61 | Train Loss: 0.6106440 Vali Loss: 1.5742011 Test Loss: 0.4729354
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5901129245758057
Epoch: 13, Steps: 61 | Train Loss: 0.6102890 Vali Loss: 1.5770972 Test Loss: 0.4729278
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5418379306793213
Epoch: 14, Steps: 61 | Train Loss: 0.6104942 Vali Loss: 1.5634215 Test Loss: 0.4730940
Validation loss decreased (1.567629 --> 1.563421).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4859445095062256
Epoch: 15, Steps: 61 | Train Loss: 0.6100897 Vali Loss: 1.5661961 Test Loss: 0.4731795
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.6150805950164795
Epoch: 16, Steps: 61 | Train Loss: 0.6102351 Vali Loss: 1.5659767 Test Loss: 0.4732370
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.47810959815979
Epoch: 17, Steps: 61 | Train Loss: 0.6104163 Vali Loss: 1.5689830 Test Loss: 0.4733865
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4841346740722656
Epoch: 18, Steps: 61 | Train Loss: 0.6104350 Vali Loss: 1.5628333 Test Loss: 0.4735188
Validation loss decreased (1.563421 --> 1.562833).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5404176712036133
Epoch: 19, Steps: 61 | Train Loss: 0.6100397 Vali Loss: 1.5691261 Test Loss: 0.4735406
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.4596340656280518
Epoch: 20, Steps: 61 | Train Loss: 0.6103209 Vali Loss: 1.5702083 Test Loss: 0.4735551
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.5224850177764893
Epoch: 21, Steps: 61 | Train Loss: 0.6101722 Vali Loss: 1.5719342 Test Loss: 0.4736615
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5719823837280273
Epoch: 22, Steps: 61 | Train Loss: 0.6103516 Vali Loss: 1.5676417 Test Loss: 0.4736978
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5015711784362793
Epoch: 23, Steps: 61 | Train Loss: 0.6100165 Vali Loss: 1.5652574 Test Loss: 0.4737504
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4651377201080322
Epoch: 24, Steps: 61 | Train Loss: 0.6101949 Vali Loss: 1.5697956 Test Loss: 0.4737685
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.5040075778961182
Epoch: 25, Steps: 61 | Train Loss: 0.6100709 Vali Loss: 1.5715185 Test Loss: 0.4738412
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4591107368469238
Epoch: 26, Steps: 61 | Train Loss: 0.6098488 Vali Loss: 1.5579052 Test Loss: 0.4738857
Validation loss decreased (1.562833 --> 1.557905).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5689198970794678
Epoch: 27, Steps: 61 | Train Loss: 0.6103622 Vali Loss: 1.5706992 Test Loss: 0.4739466
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.4714770317077637
Epoch: 28, Steps: 61 | Train Loss: 0.6100296 Vali Loss: 1.5728548 Test Loss: 0.4739817
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.4419584274291992
Epoch: 29, Steps: 61 | Train Loss: 0.6101313 Vali Loss: 1.5687863 Test Loss: 0.4740472
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.516624927520752
Epoch: 30, Steps: 61 | Train Loss: 0.6098666 Vali Loss: 1.5655477 Test Loss: 0.4740603
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.540632724761963
Epoch: 31, Steps: 61 | Train Loss: 0.6100702 Vali Loss: 1.5782125 Test Loss: 0.4741144
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.5278747081756592
Epoch: 32, Steps: 61 | Train Loss: 0.6100148 Vali Loss: 1.5678715 Test Loss: 0.4742071
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4171485900878906
Epoch: 33, Steps: 61 | Train Loss: 0.6097428 Vali Loss: 1.5608575 Test Loss: 0.4742125
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.4936416149139404
Epoch: 34, Steps: 61 | Train Loss: 0.6099080 Vali Loss: 1.5677629 Test Loss: 0.4742023
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.456592321395874
Epoch: 35, Steps: 61 | Train Loss: 0.6102494 Vali Loss: 1.5676448 Test Loss: 0.4742815
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.4455599784851074
Epoch: 36, Steps: 61 | Train Loss: 0.6100252 Vali Loss: 1.5640874 Test Loss: 0.4742850
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5505108833312988
Epoch: 37, Steps: 61 | Train Loss: 0.6100805 Vali Loss: 1.5688682 Test Loss: 0.4743137
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.5602879524230957
Epoch: 38, Steps: 61 | Train Loss: 0.6100923 Vali Loss: 1.5616925 Test Loss: 0.4743656
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.504967212677002
Epoch: 39, Steps: 61 | Train Loss: 0.6098888 Vali Loss: 1.5661299 Test Loss: 0.4743388
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.577770709991455
Epoch: 40, Steps: 61 | Train Loss: 0.6099557 Vali Loss: 1.5697029 Test Loss: 0.4743666
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.44423246383667
Epoch: 41, Steps: 61 | Train Loss: 0.6097569 Vali Loss: 1.5670526 Test Loss: 0.4744106
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.5309085845947266
Epoch: 42, Steps: 61 | Train Loss: 0.6099848 Vali Loss: 1.5669718 Test Loss: 0.4744248
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.5839810371398926
Epoch: 43, Steps: 61 | Train Loss: 0.6096504 Vali Loss: 1.5641649 Test Loss: 0.4744495
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.5150983333587646
Epoch: 44, Steps: 61 | Train Loss: 0.6095823 Vali Loss: 1.5742774 Test Loss: 0.4744663
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5224366188049316
Epoch: 45, Steps: 61 | Train Loss: 0.6100247 Vali Loss: 1.5680285 Test Loss: 0.4744916
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.5778257846832275
Epoch: 46, Steps: 61 | Train Loss: 0.6097863 Vali Loss: 1.5684301 Test Loss: 0.4745107
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4726596772670746, mae:0.4632488489151001, rse:0.6581520438194275, corr:[0.22673504 0.23013034 0.2295524  0.22801034 0.22569858 0.2238856
 0.22413391 0.22433019 0.22383718 0.22376482 0.22347263 0.22340192
 0.2230586  0.22233354 0.22207536 0.22217362 0.22227658 0.22241718
 0.22238424 0.22244461 0.22229801 0.22224782 0.22211045 0.2217106
 0.22020307 0.21936609 0.21956134 0.21996853 0.21980166 0.21974477
 0.22012933 0.22014172 0.21977122 0.21931198 0.21897155 0.21904048
 0.21908557 0.2187318  0.21866181 0.21865466 0.21892153 0.21930075
 0.2196576  0.21973477 0.21981984 0.21995804 0.22006384 0.21975133
 0.21842593 0.21748082 0.21698748 0.21644995 0.21551988 0.21465847
 0.21484415 0.21464856 0.21418278 0.21418521 0.21392305 0.21356517
 0.2131287  0.21282786 0.21272227 0.21241651 0.21247202 0.21291728
 0.21349925 0.21374634 0.21353845 0.21344045 0.21325901 0.21254319
 0.21091083 0.2100184  0.20962985 0.20965685 0.20960514 0.20971373
 0.21036176 0.21029067 0.20990454 0.20975155 0.20950285 0.20914698
 0.20883487 0.20866273 0.20871952 0.20844743 0.20822063 0.20847234
 0.20875001 0.20879556 0.20868425 0.20888986 0.20935182 0.20936044
 0.20859878 0.20858261 0.20899767 0.20902999 0.20911996 0.20945844
 0.21009973 0.2101418  0.2097634  0.20957275 0.20931019 0.20882079
 0.20847791 0.20826527 0.20849933 0.20838761 0.20833005 0.20830444
 0.20857991 0.20882507 0.20899135 0.20909181 0.208943   0.20815158
 0.20661385 0.20574692 0.20522405 0.20469852 0.20442867 0.20466287
 0.20570765 0.20612179 0.20595661 0.205803   0.2054776  0.20512186
 0.20476672 0.20451784 0.20461541 0.2044362  0.20436876 0.20439866
 0.20457454 0.20485285 0.2049586  0.20520975 0.20542653 0.20503527
 0.20375331 0.20303372 0.20277236 0.2019653  0.20099033 0.2009387
 0.20189205 0.20202716 0.20184329 0.20192026 0.20191392 0.20174652
 0.20140147 0.20102216 0.20085397 0.20063275 0.20063202 0.20074868
 0.200993   0.20125942 0.20140834 0.20156713 0.20159361 0.20101745
 0.19988751 0.19973934 0.20030466 0.20047385 0.19967733 0.19916879
 0.19984439 0.19999176 0.19977488 0.19978197 0.19986936 0.19974044
 0.19944635 0.19898973 0.1988011  0.1985111  0.19849515 0.19906558
 0.1997636  0.20015009 0.20039126 0.20063673 0.20075242 0.20022263
 0.19852294 0.1975444  0.19705494 0.19639052 0.1955202  0.1953344
 0.19590911 0.19610156 0.19586222 0.19560018 0.19530691 0.19514953
 0.19482328 0.19451754 0.19446155 0.19430691 0.19434139 0.19450791
 0.19469343 0.19485891 0.1948752  0.19502573 0.19502987 0.19450177
 0.19315839 0.19258118 0.19276823 0.19305906 0.1932642  0.19406602
 0.19563678 0.1964789  0.19666821 0.19646433 0.19589207 0.19561751
 0.1956019  0.19535547 0.19524883 0.19515169 0.19498597 0.19483824
 0.19490476 0.19508654 0.1952282  0.19536467 0.19541867 0.19487101
 0.19366305 0.19303288 0.19293244 0.19285433 0.1927577  0.19310464
 0.1943197  0.19492432 0.19511093 0.19499919 0.19450116 0.19405435
 0.1936871  0.19328153 0.19315012 0.19291121 0.19275273 0.19283344
 0.19314283 0.19324541 0.19309139 0.19296566 0.19303808 0.19277313
 0.19152287 0.1910558  0.19139624 0.19194019 0.19214508 0.1926669
 0.19390142 0.19469318 0.1949349  0.19495624 0.1946166  0.19411495
 0.1937675  0.19357827 0.19340746 0.19320008 0.19305654 0.19322841
 0.1935673  0.19394745 0.1942434  0.194497   0.19478668 0.194731
 0.19414964 0.19421293 0.19524173 0.19605123 0.19629297 0.19674173
 0.19771087 0.19800173 0.1980362  0.19795881 0.19755475 0.1970792
 0.19660917 0.19627306 0.19621076 0.19605604 0.19604936 0.19605844
 0.19621642 0.19645879 0.19657789 0.19669221 0.19666958 0.19598858
 0.19483317 0.19438608 0.1945895  0.19441366 0.19397397 0.1938261
 0.1948038  0.19518648 0.19493671 0.19458474 0.1941385  0.1937239
 0.1933588  0.19309779 0.19296    0.19295366 0.193016   0.19324823
 0.19356044 0.19374904 0.193633   0.19358338 0.19358248 0.19311863
 0.19179273 0.19106995 0.19074981 0.19059354 0.1904695  0.19096619
 0.19233432 0.19304524 0.19317967 0.19305767 0.19289522 0.19277093
 0.19256632 0.19236034 0.19238119 0.19234395 0.1922116  0.19236012
 0.19269776 0.19294643 0.19281314 0.19272473 0.19250832 0.1917351
 0.1902788  0.18948793 0.18932165 0.1889588  0.18853487 0.18833439
 0.1893048  0.18968429 0.18974496 0.18957889 0.18926498 0.18891254
 0.18859865 0.18819067 0.1878928  0.18756872 0.18754454 0.1877569
 0.18798786 0.18821649 0.18827923 0.18847612 0.18854018 0.18846302
 0.18800421 0.1884218  0.18918407 0.18951382 0.18942401 0.18940766
 0.19055693 0.19108762 0.19092222 0.19055039 0.19015996 0.19007371
 0.19006969 0.19008881 0.19017908 0.19005673 0.190065   0.1904669
 0.19086172 0.19125049 0.19140148 0.19161373 0.19194807 0.19191955
 0.19100673 0.19073606 0.19110829 0.19127358 0.19132264 0.19182138
 0.19332917 0.1939185  0.19390075 0.19371186 0.1936888  0.19377796
 0.19372055 0.19362874 0.19329531 0.19290864 0.19284883 0.19304906
 0.19323686 0.1934931  0.19363308 0.19409086 0.1944903  0.19440237
 0.19350271 0.1930527  0.1932581  0.19361886 0.19393624 0.19477077
 0.19621584 0.19687001 0.19693498 0.19689268 0.1967871  0.19674511
 0.19663726 0.19633423 0.19606546 0.19587953 0.19590275 0.19612339
 0.19642335 0.19659656 0.1966561  0.19692186 0.1971373  0.19727884
 0.19695114 0.1972665  0.19758436 0.19757389 0.19750544 0.19790544
 0.1989941  0.19919309 0.19917722 0.19950405 0.19982038 0.19982557
 0.1996965  0.19980028 0.19993182 0.19983332 0.19993646 0.20026241
 0.20061636 0.20080213 0.20073648 0.20053954 0.20049703 0.20005584
 0.19879338 0.19827329 0.19821742 0.19819437 0.19811858 0.19845016
 0.1998154  0.20020992 0.20015542 0.20005494 0.19983414 0.19964367
 0.19939798 0.199138   0.199046   0.19898273 0.19898906 0.19906431
 0.1992531  0.19947332 0.19960447 0.19984813 0.20033182 0.20066467
 0.20029923 0.20062646 0.20134656 0.20172319 0.20165235 0.20178896
 0.20272726 0.20273927 0.2023608  0.20190501 0.2015775  0.20152797
 0.20156354 0.2015631  0.20178434 0.20191412 0.2020151  0.20215505
 0.20248498 0.20299055 0.20333578 0.20373672 0.20433518 0.20439877
 0.20318797 0.20238893 0.20237087 0.20256317 0.20227692 0.20214756
 0.20292367 0.20329402 0.2032381  0.20308742 0.20285365 0.20279196
 0.20271376 0.20268628 0.20282084 0.20288926 0.203143   0.20342034
 0.20363119 0.203813   0.20376968 0.2037081  0.20363142 0.20306426
 0.20154342 0.20052348 0.20037754 0.20007485 0.19941823 0.19917707
 0.1999663  0.20012069 0.19996303 0.19966953 0.19932467 0.19912794
 0.19905478 0.1989383  0.19884446 0.19862324 0.19857128 0.19883299
 0.1992817  0.19954267 0.1995899  0.19960806 0.19967845 0.1991268
 0.19756821 0.1969103  0.19686045 0.19688903 0.19677205 0.19682617
 0.19777389 0.1980569  0.19788848 0.19793333 0.197816   0.19761285
 0.19731241 0.19708371 0.19707559 0.19702749 0.19699126 0.19708249
 0.19738027 0.1976286  0.19749908 0.19729887 0.19716106 0.1965283
 0.19476196 0.19354822 0.19335443 0.19325897 0.1929808  0.19299264
 0.19372079 0.19379377 0.1935185  0.19348086 0.19345959 0.1933279
 0.19311374 0.19293016 0.19295451 0.19310287 0.19320996 0.19332485
 0.19349156 0.19375078 0.19373254 0.193782   0.19396041 0.19351438
 0.1919244  0.1908724  0.19047888 0.19027187 0.18980844 0.18963104
 0.19035694 0.19029735 0.18980974 0.18965067 0.18942168 0.18922287
 0.1890941  0.18881063 0.18860425 0.188364   0.18818045 0.18815753
 0.18815562 0.188333   0.18841399 0.18865088 0.18903054 0.18901123
 0.18776295 0.18667135 0.18619378 0.18622541 0.1860858  0.18614838
 0.18716003 0.18696426 0.18635827 0.18600515 0.18573843 0.18533894
 0.18495268 0.18457715 0.18443425 0.18437862 0.18429518 0.18421768
 0.18418474 0.18426463 0.18422039 0.18421139 0.18421489 0.1833698
 0.18071032 0.1780811  0.17667395 0.17549926 0.17387465 0.17264761
 0.17256331 0.17209408 0.17142168 0.17085814 0.17061093 0.1707147
 0.17071678 0.17049927 0.17028709 0.17035481 0.17041269 0.1702604
 0.17017591 0.1704527  0.17070188 0.17091186 0.17122869 0.17101161
 0.16930409 0.16811326 0.16765882 0.16698179 0.1661398  0.165735
 0.16653591 0.1664581  0.16590177 0.16533016 0.16470203 0.16422683
 0.16382128 0.16293631 0.16220844 0.16181707 0.16182463 0.16147293
 0.16163264 0.1627614  0.1637065  0.16299197 0.16298753 0.16464242]
