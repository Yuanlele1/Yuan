Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=1919, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=106, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  15386112.0
params:  17334.0
Trainable parameters:  17334
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.672426700592041
Epoch: 1, Steps: 63 | Train Loss: 0.6957647 Vali Loss: 1.3634410 Test Loss: 0.6687547
Validation loss decreased (inf --> 1.363441).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5335276126861572
Epoch: 2, Steps: 63 | Train Loss: 0.5356360 Vali Loss: 1.1942271 Test Loss: 0.5535113
Validation loss decreased (1.363441 --> 1.194227).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.925875186920166
Epoch: 3, Steps: 63 | Train Loss: 0.4813462 Vali Loss: 1.1058381 Test Loss: 0.4960628
Validation loss decreased (1.194227 --> 1.105838).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6063342094421387
Epoch: 4, Steps: 63 | Train Loss: 0.4510502 Vali Loss: 1.0490408 Test Loss: 0.4616042
Validation loss decreased (1.105838 --> 1.049041).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.6166341304779053
Epoch: 5, Steps: 63 | Train Loss: 0.4316223 Vali Loss: 1.0114034 Test Loss: 0.4402249
Validation loss decreased (1.049041 --> 1.011403).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.7919509410858154
Epoch: 6, Steps: 63 | Train Loss: 0.4195031 Vali Loss: 0.9866175 Test Loss: 0.4270439
Validation loss decreased (1.011403 --> 0.986618).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5688738822937012
Epoch: 7, Steps: 63 | Train Loss: 0.4115089 Vali Loss: 0.9708263 Test Loss: 0.4194834
Validation loss decreased (0.986618 --> 0.970826).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5653092861175537
Epoch: 8, Steps: 63 | Train Loss: 0.4063060 Vali Loss: 0.9589331 Test Loss: 0.4147644
Validation loss decreased (0.970826 --> 0.958933).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.776212215423584
Epoch: 9, Steps: 63 | Train Loss: 0.4031141 Vali Loss: 0.9515275 Test Loss: 0.4121784
Validation loss decreased (0.958933 --> 0.951528).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.0712177753448486
Epoch: 10, Steps: 63 | Train Loss: 0.4009513 Vali Loss: 0.9461084 Test Loss: 0.4106479
Validation loss decreased (0.951528 --> 0.946108).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.5804309844970703
Epoch: 11, Steps: 63 | Train Loss: 0.3992619 Vali Loss: 0.9417964 Test Loss: 0.4096507
Validation loss decreased (0.946108 --> 0.941796).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.8066976070404053
Epoch: 12, Steps: 63 | Train Loss: 0.3982661 Vali Loss: 0.9389857 Test Loss: 0.4094248
Validation loss decreased (0.941796 --> 0.938986).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.8916373252868652
Epoch: 13, Steps: 63 | Train Loss: 0.3974635 Vali Loss: 0.9365556 Test Loss: 0.4090119
Validation loss decreased (0.938986 --> 0.936556).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5909850597381592
Epoch: 14, Steps: 63 | Train Loss: 0.3968242 Vali Loss: 0.9343378 Test Loss: 0.4088717
Validation loss decreased (0.936556 --> 0.934338).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5119714736938477
Epoch: 15, Steps: 63 | Train Loss: 0.3965864 Vali Loss: 0.9329356 Test Loss: 0.4090296
Validation loss decreased (0.934338 --> 0.932936).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.4725141525268555
Epoch: 16, Steps: 63 | Train Loss: 0.3958718 Vali Loss: 0.9314769 Test Loss: 0.4089273
Validation loss decreased (0.932936 --> 0.931477).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3816914558410645
Epoch: 17, Steps: 63 | Train Loss: 0.3955458 Vali Loss: 0.9307839 Test Loss: 0.4089409
Validation loss decreased (0.931477 --> 0.930784).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.548980712890625
Epoch: 18, Steps: 63 | Train Loss: 0.3955940 Vali Loss: 0.9300405 Test Loss: 0.4088656
Validation loss decreased (0.930784 --> 0.930040).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5355520248413086
Epoch: 19, Steps: 63 | Train Loss: 0.3954071 Vali Loss: 0.9290511 Test Loss: 0.4087858
Validation loss decreased (0.930040 --> 0.929051).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.604294776916504
Epoch: 20, Steps: 63 | Train Loss: 0.3950371 Vali Loss: 0.9285429 Test Loss: 0.4087909
Validation loss decreased (0.929051 --> 0.928543).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.7335453033447266
Epoch: 21, Steps: 63 | Train Loss: 0.3948041 Vali Loss: 0.9279267 Test Loss: 0.4087291
Validation loss decreased (0.928543 --> 0.927927).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.61763596534729
Epoch: 22, Steps: 63 | Train Loss: 0.3949309 Vali Loss: 0.9271898 Test Loss: 0.4088119
Validation loss decreased (0.927927 --> 0.927190).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9130964279174805
Epoch: 23, Steps: 63 | Train Loss: 0.3945074 Vali Loss: 0.9265649 Test Loss: 0.4087540
Validation loss decreased (0.927190 --> 0.926565).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6453583240509033
Epoch: 24, Steps: 63 | Train Loss: 0.3943695 Vali Loss: 0.9264150 Test Loss: 0.4087494
Validation loss decreased (0.926565 --> 0.926415).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.5482242107391357
Epoch: 25, Steps: 63 | Train Loss: 0.3944792 Vali Loss: 0.9262167 Test Loss: 0.4087788
Validation loss decreased (0.926415 --> 0.926217).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8910400867462158
Epoch: 26, Steps: 63 | Train Loss: 0.3943572 Vali Loss: 0.9258669 Test Loss: 0.4088121
Validation loss decreased (0.926217 --> 0.925867).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6687748432159424
Epoch: 27, Steps: 63 | Train Loss: 0.3943158 Vali Loss: 0.9253291 Test Loss: 0.4087822
Validation loss decreased (0.925867 --> 0.925329).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6210196018218994
Epoch: 28, Steps: 63 | Train Loss: 0.3943218 Vali Loss: 0.9249452 Test Loss: 0.4086974
Validation loss decreased (0.925329 --> 0.924945).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6017646789550781
Epoch: 29, Steps: 63 | Train Loss: 0.3940689 Vali Loss: 0.9247423 Test Loss: 0.4086892
Validation loss decreased (0.924945 --> 0.924742).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5017917156219482
Epoch: 30, Steps: 63 | Train Loss: 0.3939084 Vali Loss: 0.9241364 Test Loss: 0.4087033
Validation loss decreased (0.924742 --> 0.924136).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6465108394622803
Epoch: 31, Steps: 63 | Train Loss: 0.3939117 Vali Loss: 0.9238219 Test Loss: 0.4086628
Validation loss decreased (0.924136 --> 0.923822).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.5246686935424805
Epoch: 32, Steps: 63 | Train Loss: 0.3939161 Vali Loss: 0.9232966 Test Loss: 0.4086401
Validation loss decreased (0.923822 --> 0.923297).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5305359363555908
Epoch: 33, Steps: 63 | Train Loss: 0.3937531 Vali Loss: 0.9235510 Test Loss: 0.4085837
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.2999868392944336
Epoch: 34, Steps: 63 | Train Loss: 0.3938160 Vali Loss: 0.9231508 Test Loss: 0.4086854
Validation loss decreased (0.923297 --> 0.923151).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.8232054710388184
Epoch: 35, Steps: 63 | Train Loss: 0.3936678 Vali Loss: 0.9233728 Test Loss: 0.4086107
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.9467899799346924
Epoch: 36, Steps: 63 | Train Loss: 0.3937707 Vali Loss: 0.9233615 Test Loss: 0.4086133
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5532078742980957
Epoch: 37, Steps: 63 | Train Loss: 0.3937703 Vali Loss: 0.9228650 Test Loss: 0.4085917
Validation loss decreased (0.923151 --> 0.922865).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.587658166885376
Epoch: 38, Steps: 63 | Train Loss: 0.3935835 Vali Loss: 0.9223919 Test Loss: 0.4085716
Validation loss decreased (0.922865 --> 0.922392).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.6413629055023193
Epoch: 39, Steps: 63 | Train Loss: 0.3934104 Vali Loss: 0.9227743 Test Loss: 0.4086067
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.5509703159332275
Epoch: 40, Steps: 63 | Train Loss: 0.3935746 Vali Loss: 0.9224275 Test Loss: 0.4086214
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.7352421283721924
Epoch: 41, Steps: 63 | Train Loss: 0.3936798 Vali Loss: 0.9221001 Test Loss: 0.4086358
Validation loss decreased (0.922392 --> 0.922100).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.590280294418335
Epoch: 42, Steps: 63 | Train Loss: 0.3934366 Vali Loss: 0.9223353 Test Loss: 0.4085760
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9787554740905762
Epoch: 43, Steps: 63 | Train Loss: 0.3934934 Vali Loss: 0.9222896 Test Loss: 0.4086052
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.6193697452545166
Epoch: 44, Steps: 63 | Train Loss: 0.3934344 Vali Loss: 0.9220290 Test Loss: 0.4085838
Validation loss decreased (0.922100 --> 0.922029).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5743522644042969
Epoch: 45, Steps: 63 | Train Loss: 0.3931867 Vali Loss: 0.9221694 Test Loss: 0.4085596
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8122713565826416
Epoch: 46, Steps: 63 | Train Loss: 0.3934407 Vali Loss: 0.9220905 Test Loss: 0.4085940
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.592250108718872
Epoch: 47, Steps: 63 | Train Loss: 0.3930467 Vali Loss: 0.9220047 Test Loss: 0.4085247
Validation loss decreased (0.922029 --> 0.922005).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.5400545597076416
Epoch: 48, Steps: 63 | Train Loss: 0.3933067 Vali Loss: 0.9219137 Test Loss: 0.4085717
Validation loss decreased (0.922005 --> 0.921914).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.8360393047332764
Epoch: 49, Steps: 63 | Train Loss: 0.3936097 Vali Loss: 0.9210177 Test Loss: 0.4085453
Validation loss decreased (0.921914 --> 0.921018).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6013224124908447
Epoch: 50, Steps: 63 | Train Loss: 0.3927914 Vali Loss: 0.9212050 Test Loss: 0.4085565
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.5530996322631836
Epoch: 51, Steps: 63 | Train Loss: 0.3932061 Vali Loss: 0.9213427 Test Loss: 0.4085409
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.4839258193969727
Epoch: 52, Steps: 63 | Train Loss: 0.3931480 Vali Loss: 0.9215044 Test Loss: 0.4085366
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.518765926361084
Epoch: 53, Steps: 63 | Train Loss: 0.3931069 Vali Loss: 0.9212756 Test Loss: 0.4085457
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.6228020191192627
Epoch: 54, Steps: 63 | Train Loss: 0.3930855 Vali Loss: 0.9215218 Test Loss: 0.4085584
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.615206241607666
Epoch: 55, Steps: 63 | Train Loss: 0.3932000 Vali Loss: 0.9212922 Test Loss: 0.4085792
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.5146152973175049
Epoch: 56, Steps: 63 | Train Loss: 0.3933090 Vali Loss: 0.9214476 Test Loss: 0.4085465
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.6109719276428223
Epoch: 57, Steps: 63 | Train Loss: 0.3930887 Vali Loss: 0.9207892 Test Loss: 0.4085447
Validation loss decreased (0.921018 --> 0.920789).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.5860226154327393
Epoch: 58, Steps: 63 | Train Loss: 0.3933432 Vali Loss: 0.9206607 Test Loss: 0.4085370
Validation loss decreased (0.920789 --> 0.920661).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.4946415424346924
Epoch: 59, Steps: 63 | Train Loss: 0.3931278 Vali Loss: 0.9208157 Test Loss: 0.4085510
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.5692262649536133
Epoch: 60, Steps: 63 | Train Loss: 0.3933416 Vali Loss: 0.9211870 Test Loss: 0.4085244
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6017074584960938
Epoch: 61, Steps: 63 | Train Loss: 0.3932863 Vali Loss: 0.9210784 Test Loss: 0.4085365
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.4787423610687256
Epoch: 62, Steps: 63 | Train Loss: 0.3932165 Vali Loss: 0.9212201 Test Loss: 0.4085416
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.5900425910949707
Epoch: 63, Steps: 63 | Train Loss: 0.3928613 Vali Loss: 0.9210067 Test Loss: 0.4085243
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.6170587539672852
Epoch: 64, Steps: 63 | Train Loss: 0.3930624 Vali Loss: 0.9209788 Test Loss: 0.4085363
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.471191644668579
Epoch: 65, Steps: 63 | Train Loss: 0.3929836 Vali Loss: 0.9211438 Test Loss: 0.4085473
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.4905378818511963
Epoch: 66, Steps: 63 | Train Loss: 0.3927481 Vali Loss: 0.9206642 Test Loss: 0.4085395
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.7022569179534912
Epoch: 67, Steps: 63 | Train Loss: 0.3930741 Vali Loss: 0.9206498 Test Loss: 0.4085360
Validation loss decreased (0.920661 --> 0.920650).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.714726448059082
Epoch: 68, Steps: 63 | Train Loss: 0.3928948 Vali Loss: 0.9204282 Test Loss: 0.4085302
Validation loss decreased (0.920650 --> 0.920428).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.759732961654663
Epoch: 69, Steps: 63 | Train Loss: 0.3928974 Vali Loss: 0.9208962 Test Loss: 0.4085239
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.631962537765503
Epoch: 70, Steps: 63 | Train Loss: 0.3930915 Vali Loss: 0.9209740 Test Loss: 0.4085304
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.8603744506835938
Epoch: 71, Steps: 63 | Train Loss: 0.3930649 Vali Loss: 0.9207885 Test Loss: 0.4085334
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.085452079772949
Epoch: 72, Steps: 63 | Train Loss: 0.3931398 Vali Loss: 0.9208493 Test Loss: 0.4085333
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.6402385234832764
Epoch: 73, Steps: 63 | Train Loss: 0.3927029 Vali Loss: 0.9200100 Test Loss: 0.4085384
Validation loss decreased (0.920428 --> 0.920010).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.6197540760040283
Epoch: 74, Steps: 63 | Train Loss: 0.3930223 Vali Loss: 0.9209024 Test Loss: 0.4085320
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.8722705841064453
Epoch: 75, Steps: 63 | Train Loss: 0.3930819 Vali Loss: 0.9209211 Test Loss: 0.4085290
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.5134236812591553
Epoch: 76, Steps: 63 | Train Loss: 0.3930758 Vali Loss: 0.9207946 Test Loss: 0.4085300
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.6114821434020996
Epoch: 77, Steps: 63 | Train Loss: 0.3928502 Vali Loss: 0.9207231 Test Loss: 0.4085316
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.5534842014312744
Epoch: 78, Steps: 63 | Train Loss: 0.3930626 Vali Loss: 0.9208193 Test Loss: 0.4085329
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.506204605102539
Epoch: 79, Steps: 63 | Train Loss: 0.3929602 Vali Loss: 0.9207795 Test Loss: 0.4085285
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.6761577129364014
Epoch: 80, Steps: 63 | Train Loss: 0.3928860 Vali Loss: 0.9208884 Test Loss: 0.4085275
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.3621673583984375
Epoch: 81, Steps: 63 | Train Loss: 0.3930430 Vali Loss: 0.9204462 Test Loss: 0.4085238
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.683819055557251
Epoch: 82, Steps: 63 | Train Loss: 0.3928415 Vali Loss: 0.9205173 Test Loss: 0.4085310
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.5067110061645508
Epoch: 83, Steps: 63 | Train Loss: 0.3930149 Vali Loss: 0.9205751 Test Loss: 0.4085318
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.5663230419158936
Epoch: 84, Steps: 63 | Train Loss: 0.3928530 Vali Loss: 0.9208135 Test Loss: 0.4085234
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.559018850326538
Epoch: 85, Steps: 63 | Train Loss: 0.3927922 Vali Loss: 0.9204589 Test Loss: 0.4085320
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.6180369853973389
Epoch: 86, Steps: 63 | Train Loss: 0.3929947 Vali Loss: 0.9202284 Test Loss: 0.4085278
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.48189115524292
Epoch: 87, Steps: 63 | Train Loss: 0.3931891 Vali Loss: 0.9203737 Test Loss: 0.4085272
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.7750773429870605
Epoch: 88, Steps: 63 | Train Loss: 0.3930754 Vali Loss: 0.9207041 Test Loss: 0.4085231
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.5474796295166016
Epoch: 89, Steps: 63 | Train Loss: 0.3928313 Vali Loss: 0.9203382 Test Loss: 0.4085273
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.582573413848877
Epoch: 90, Steps: 63 | Train Loss: 0.3928331 Vali Loss: 0.9206014 Test Loss: 0.4085268
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.501357078552246
Epoch: 91, Steps: 63 | Train Loss: 0.3929211 Vali Loss: 0.9206836 Test Loss: 0.4085279
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.6614222526550293
Epoch: 92, Steps: 63 | Train Loss: 0.3929986 Vali Loss: 0.9202333 Test Loss: 0.4085293
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.935424566268921
Epoch: 93, Steps: 63 | Train Loss: 0.3931918 Vali Loss: 0.9207287 Test Loss: 0.4085299
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.404401957988739, mae:0.4131086468696594, rse:0.6038985252380371, corr:[0.26047325 0.26911026 0.26920742 0.27134612 0.2685706  0.2660264
 0.26600164 0.26575395 0.26473543 0.2648141  0.26507077 0.26450524
 0.26416713 0.26407412 0.26367924 0.2636346  0.26387239 0.26358467
 0.2632184  0.26321056 0.26313105 0.26283187 0.26311767 0.26360798
 0.26320696 0.26286927 0.2629337  0.26260686 0.26201487 0.26190254
 0.26180527 0.26111603 0.26059803 0.26077572 0.26089913 0.26063448
 0.26066163 0.26097146 0.26102087 0.2610541  0.2613673  0.26144898
 0.26141205 0.26145312 0.2612774  0.26083767 0.2607986  0.26104987
 0.2606894  0.2597071  0.2587975  0.25828883 0.25770873 0.25670362
 0.25602895 0.25569013 0.25539944 0.25524437 0.2550522  0.25494003
 0.2547983  0.2547438  0.25468656 0.2546109  0.2546567  0.25498948
 0.25539184 0.25532427 0.25513437 0.25524274 0.2553367  0.25508925
 0.25453597 0.2538022  0.25306466 0.25263643 0.25253314 0.25232127
 0.251873   0.25137743 0.25110447 0.25086865 0.25059366 0.2504063
 0.25030398 0.25025588 0.25034097 0.2503963  0.25029773 0.25026652
 0.25025293 0.25000453 0.24964343 0.2494887  0.24959762 0.25015715
 0.25084406 0.25093493 0.25095263 0.25099337 0.2509372  0.25072342
 0.2505151  0.25052428 0.2504636  0.2502631  0.25003514 0.24978647
 0.24952178 0.24944791 0.24962704 0.24995975 0.2502384  0.25044364
 0.25046748 0.25030205 0.25008342 0.24981584 0.24948108 0.24947861
 0.24963178 0.24911374 0.24830788 0.24780214 0.24735792 0.24662776
 0.24635918 0.24653319 0.24629167 0.24569452 0.245466   0.24535319
 0.24501346 0.24499416 0.24522887 0.24510415 0.2452676  0.24566287
 0.24588205 0.24583845 0.24584451 0.24581876 0.24548589 0.245363
 0.24546593 0.24491866 0.24435133 0.24396375 0.24325855 0.24226607
 0.24209759 0.24238041 0.2421577  0.24211325 0.24235736 0.24227707
 0.24188831 0.24212179 0.2423924  0.24202998 0.24192174 0.24235348
 0.24241054 0.2421031  0.242216   0.24220112 0.2417049  0.24168035
 0.24205074 0.24187578 0.24197009 0.2425096  0.24227579 0.24154568
 0.24160552 0.2418525  0.24151266 0.24165604 0.24206045 0.24182446
 0.24178152 0.24228212 0.24220498 0.2418659  0.24240848 0.24260491
 0.24182299 0.24225882 0.24229696 0.23993608 0.24115334 0.2384665 ]
