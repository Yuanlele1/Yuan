Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=58, out_features=112, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5820416.0
params:  6608.0
Trainable parameters:  6608
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.3398537635803223
Epoch: 1, Steps: 62 | Train Loss: 0.7778553 Vali Loss: 1.6093655 Test Loss: 0.7206683
Validation loss decreased (inf --> 1.609365).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.3651251792907715
Epoch: 2, Steps: 62 | Train Loss: 0.6187614 Vali Loss: 1.4234295 Test Loss: 0.5999382
Validation loss decreased (1.609365 --> 1.423429).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.1617705821990967
Epoch: 3, Steps: 62 | Train Loss: 0.5599940 Vali Loss: 1.3430228 Test Loss: 0.5491122
Validation loss decreased (1.423429 --> 1.343023).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.362445116043091
Epoch: 4, Steps: 62 | Train Loss: 0.5313724 Vali Loss: 1.2977945 Test Loss: 0.5185938
Validation loss decreased (1.343023 --> 1.297794).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.4009408950805664
Epoch: 5, Steps: 62 | Train Loss: 0.5133267 Vali Loss: 1.2663654 Test Loss: 0.4972914
Validation loss decreased (1.297794 --> 1.266365).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.502636432647705
Epoch: 6, Steps: 62 | Train Loss: 0.5006788 Vali Loss: 1.2452101 Test Loss: 0.4818565
Validation loss decreased (1.266365 --> 1.245210).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.2102763652801514
Epoch: 7, Steps: 62 | Train Loss: 0.4915240 Vali Loss: 1.2287468 Test Loss: 0.4701553
Validation loss decreased (1.245210 --> 1.228747).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.9808974266052246
Epoch: 8, Steps: 62 | Train Loss: 0.4846355 Vali Loss: 1.2135810 Test Loss: 0.4615695
Validation loss decreased (1.228747 --> 1.213581).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.318117141723633
Epoch: 9, Steps: 62 | Train Loss: 0.4795052 Vali Loss: 1.2074833 Test Loss: 0.4549981
Validation loss decreased (1.213581 --> 1.207483).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.655590534210205
Epoch: 10, Steps: 62 | Train Loss: 0.4757826 Vali Loss: 1.1953714 Test Loss: 0.4499382
Validation loss decreased (1.207483 --> 1.195371).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7009680271148682
Epoch: 11, Steps: 62 | Train Loss: 0.4728558 Vali Loss: 1.1891406 Test Loss: 0.4463044
Validation loss decreased (1.195371 --> 1.189141).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3354852199554443
Epoch: 12, Steps: 62 | Train Loss: 0.4705756 Vali Loss: 1.1885124 Test Loss: 0.4436207
Validation loss decreased (1.189141 --> 1.188512).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.9896531105041504
Epoch: 13, Steps: 62 | Train Loss: 0.4689733 Vali Loss: 1.1866765 Test Loss: 0.4414787
Validation loss decreased (1.188512 --> 1.186677).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6926352977752686
Epoch: 14, Steps: 62 | Train Loss: 0.4674706 Vali Loss: 1.1821417 Test Loss: 0.4400540
Validation loss decreased (1.186677 --> 1.182142).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5451624393463135
Epoch: 15, Steps: 62 | Train Loss: 0.4665444 Vali Loss: 1.1761730 Test Loss: 0.4388157
Validation loss decreased (1.182142 --> 1.176173).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.834590196609497
Epoch: 16, Steps: 62 | Train Loss: 0.4656757 Vali Loss: 1.1795167 Test Loss: 0.4379725
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.0344345569610596
Epoch: 17, Steps: 62 | Train Loss: 0.4650953 Vali Loss: 1.1739225 Test Loss: 0.4373304
Validation loss decreased (1.176173 --> 1.173923).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.9789066314697266
Epoch: 18, Steps: 62 | Train Loss: 0.4646406 Vali Loss: 1.1724163 Test Loss: 0.4368395
Validation loss decreased (1.173923 --> 1.172416).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.890153408050537
Epoch: 19, Steps: 62 | Train Loss: 0.4642657 Vali Loss: 1.1709143 Test Loss: 0.4363472
Validation loss decreased (1.172416 --> 1.170914).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.308642864227295
Epoch: 20, Steps: 62 | Train Loss: 0.4637973 Vali Loss: 1.1749282 Test Loss: 0.4361368
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.923497200012207
Epoch: 21, Steps: 62 | Train Loss: 0.4636619 Vali Loss: 1.1720011 Test Loss: 0.4359467
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.128868579864502
Epoch: 22, Steps: 62 | Train Loss: 0.4633900 Vali Loss: 1.1751424 Test Loss: 0.4357396
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.7950942516326904
Epoch: 23, Steps: 62 | Train Loss: 0.4630287 Vali Loss: 1.1740609 Test Loss: 0.4356249
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.1314334869384766
Epoch: 24, Steps: 62 | Train Loss: 0.4628512 Vali Loss: 1.1705223 Test Loss: 0.4354892
Validation loss decreased (1.170914 --> 1.170522).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.4129531383514404
Epoch: 25, Steps: 62 | Train Loss: 0.4627569 Vali Loss: 1.1690736 Test Loss: 0.4354065
Validation loss decreased (1.170522 --> 1.169074).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.988006353378296
Epoch: 26, Steps: 62 | Train Loss: 0.4626383 Vali Loss: 1.1708010 Test Loss: 0.4353050
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.6253397464752197
Epoch: 27, Steps: 62 | Train Loss: 0.4624634 Vali Loss: 1.1717080 Test Loss: 0.4352955
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.2002296447753906
Epoch: 28, Steps: 62 | Train Loss: 0.4624599 Vali Loss: 1.1689326 Test Loss: 0.4352153
Validation loss decreased (1.169074 --> 1.168933).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.330868721008301
Epoch: 29, Steps: 62 | Train Loss: 0.4621412 Vali Loss: 1.1649090 Test Loss: 0.4351940
Validation loss decreased (1.168933 --> 1.164909).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.463627815246582
Epoch: 30, Steps: 62 | Train Loss: 0.4622369 Vali Loss: 1.1669241 Test Loss: 0.4351742
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.866830348968506
Epoch: 31, Steps: 62 | Train Loss: 0.4620732 Vali Loss: 1.1683760 Test Loss: 0.4352131
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.7627263069152832
Epoch: 32, Steps: 62 | Train Loss: 0.4620139 Vali Loss: 1.1660941 Test Loss: 0.4351704
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.0636181831359863
Epoch: 33, Steps: 62 | Train Loss: 0.4620797 Vali Loss: 1.1667871 Test Loss: 0.4351163
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.065885305404663
Epoch: 34, Steps: 62 | Train Loss: 0.4619140 Vali Loss: 1.1662271 Test Loss: 0.4350796
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.4895405769348145
Epoch: 35, Steps: 62 | Train Loss: 0.4619533 Vali Loss: 1.1628133 Test Loss: 0.4350758
Validation loss decreased (1.164909 --> 1.162813).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.064389705657959
Epoch: 36, Steps: 62 | Train Loss: 0.4618902 Vali Loss: 1.1686119 Test Loss: 0.4350570
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.711003303527832
Epoch: 37, Steps: 62 | Train Loss: 0.4618500 Vali Loss: 1.1683215 Test Loss: 0.4350196
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.6568543910980225
Epoch: 38, Steps: 62 | Train Loss: 0.4617916 Vali Loss: 1.1698421 Test Loss: 0.4350258
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.6977617740631104
Epoch: 39, Steps: 62 | Train Loss: 0.4616938 Vali Loss: 1.1654348 Test Loss: 0.4350144
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.148435592651367
Epoch: 40, Steps: 62 | Train Loss: 0.4618290 Vali Loss: 1.1649294 Test Loss: 0.4350237
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.7450239658355713
Epoch: 41, Steps: 62 | Train Loss: 0.4615718 Vali Loss: 1.1638294 Test Loss: 0.4350203
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.2091715335845947
Epoch: 42, Steps: 62 | Train Loss: 0.4615059 Vali Loss: 1.1620661 Test Loss: 0.4350510
Validation loss decreased (1.162813 --> 1.162066).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9622151851654053
Epoch: 43, Steps: 62 | Train Loss: 0.4616755 Vali Loss: 1.1659424 Test Loss: 0.4349909
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.519885301589966
Epoch: 44, Steps: 62 | Train Loss: 0.4616506 Vali Loss: 1.1741253 Test Loss: 0.4350113
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7908000946044922
Epoch: 45, Steps: 62 | Train Loss: 0.4614604 Vali Loss: 1.1623862 Test Loss: 0.4350102
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8016424179077148
Epoch: 46, Steps: 62 | Train Loss: 0.4613805 Vali Loss: 1.1643157 Test Loss: 0.4349999
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.9825150966644287
Epoch: 47, Steps: 62 | Train Loss: 0.4615445 Vali Loss: 1.1683328 Test Loss: 0.4350134
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.112342596054077
Epoch: 48, Steps: 62 | Train Loss: 0.4616067 Vali Loss: 1.1650307 Test Loss: 0.4349852
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.4979991912841797
Epoch: 49, Steps: 62 | Train Loss: 0.4615762 Vali Loss: 1.1651702 Test Loss: 0.4350015
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.7752041816711426
Epoch: 50, Steps: 62 | Train Loss: 0.4613194 Vali Loss: 1.1638571 Test Loss: 0.4349897
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.385026454925537
Epoch: 51, Steps: 62 | Train Loss: 0.4613686 Vali Loss: 1.1675687 Test Loss: 0.4350005
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.4431896209716797
Epoch: 52, Steps: 62 | Train Loss: 0.4613013 Vali Loss: 1.1612141 Test Loss: 0.4349867
Validation loss decreased (1.162066 --> 1.161214).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.521716356277466
Epoch: 53, Steps: 62 | Train Loss: 0.4614722 Vali Loss: 1.1654837 Test Loss: 0.4350050
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.2874910831451416
Epoch: 54, Steps: 62 | Train Loss: 0.4611565 Vali Loss: 1.1656055 Test Loss: 0.4349880
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.365787982940674
Epoch: 55, Steps: 62 | Train Loss: 0.4613117 Vali Loss: 1.1694999 Test Loss: 0.4349909
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.025162935256958
Epoch: 56, Steps: 62 | Train Loss: 0.4614076 Vali Loss: 1.1629111 Test Loss: 0.4349925
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.05072283744812
Epoch: 57, Steps: 62 | Train Loss: 0.4612435 Vali Loss: 1.1660321 Test Loss: 0.4349862
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.9113669395446777
Epoch: 58, Steps: 62 | Train Loss: 0.4610438 Vali Loss: 1.1678263 Test Loss: 0.4349853
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.9130303859710693
Epoch: 59, Steps: 62 | Train Loss: 0.4612861 Vali Loss: 1.1625509 Test Loss: 0.4350012
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.8627262115478516
Epoch: 60, Steps: 62 | Train Loss: 0.4611681 Vali Loss: 1.1644140 Test Loss: 0.4350006
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.0247204303741455
Epoch: 61, Steps: 62 | Train Loss: 0.4613320 Vali Loss: 1.1632003 Test Loss: 0.4350015
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.8086729049682617
Epoch: 62, Steps: 62 | Train Loss: 0.4611723 Vali Loss: 1.1643127 Test Loss: 0.4349990
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.215994119644165
Epoch: 63, Steps: 62 | Train Loss: 0.4612608 Vali Loss: 1.1656638 Test Loss: 0.4350050
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.2273736000061035
Epoch: 64, Steps: 62 | Train Loss: 0.4613067 Vali Loss: 1.1669462 Test Loss: 0.4349931
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.3534271717071533
Epoch: 65, Steps: 62 | Train Loss: 0.4611594 Vali Loss: 1.1680765 Test Loss: 0.4349985
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.9135756492614746
Epoch: 66, Steps: 62 | Train Loss: 0.4612024 Vali Loss: 1.1619879 Test Loss: 0.4350037
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.2797462940216064
Epoch: 67, Steps: 62 | Train Loss: 0.4612748 Vali Loss: 1.1630303 Test Loss: 0.4349898
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 3.072629928588867
Epoch: 68, Steps: 62 | Train Loss: 0.4611525 Vali Loss: 1.1651710 Test Loss: 0.4349968
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.487149953842163
Epoch: 69, Steps: 62 | Train Loss: 0.4613896 Vali Loss: 1.1715100 Test Loss: 0.4350024
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.8631956577301025
Epoch: 70, Steps: 62 | Train Loss: 0.4611332 Vali Loss: 1.1647615 Test Loss: 0.4350010
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.8797061443328857
Epoch: 71, Steps: 62 | Train Loss: 0.4612228 Vali Loss: 1.1613544 Test Loss: 0.4350019
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.5708858966827393
Epoch: 72, Steps: 62 | Train Loss: 0.4612846 Vali Loss: 1.1613522 Test Loss: 0.4350032
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4338264465332031, mae:0.4322148561477661, rse:0.6270613670349121, corr:[0.24668325 0.25816807 0.25859424 0.25440934 0.2512367  0.25047436
 0.25090706 0.2514799  0.2512729  0.25044718 0.2495436  0.24892898
 0.24875668 0.2488324  0.24882798 0.24865259 0.24844564 0.24839275
 0.24851973 0.24861693 0.24864414 0.24866971 0.24884683 0.24913669
 0.24926686 0.24902584 0.24851733 0.2479405  0.24744518 0.24716984
 0.24709478 0.24707367 0.24692968 0.24658968 0.24616998 0.24594262
 0.24605052 0.24646357 0.2469518  0.24724227 0.24731922 0.24724971
 0.24716909 0.2471292  0.24708523 0.24702436 0.24699825 0.24691933
 0.24659778 0.24592309 0.24487069 0.24392366 0.24324079 0.24269363
 0.24236277 0.2421918  0.24200296 0.24168965 0.24127267 0.24099422
 0.24084187 0.24093051 0.24109218 0.24124286 0.24134326 0.24146727
 0.24167418 0.24174711 0.24181734 0.24183828 0.24171437 0.24137187
 0.24085025 0.24022055 0.23961166 0.23915517 0.23882823 0.23856096
 0.23835386 0.23816547 0.23797582 0.23772232 0.23738146 0.2370987
 0.2369624  0.2369393  0.23694928 0.23688334 0.23672616 0.23655471
 0.23631291 0.23607264 0.23593168 0.23612466 0.23662889 0.23730414
 0.23801042 0.23840852 0.23848282 0.23828064 0.23803052 0.23786676
 0.23782675 0.23789819 0.23788197 0.23769483 0.23740329 0.23714447
 0.237038   0.23717086 0.23745282 0.23775434 0.23790346 0.23788217
 0.23771901 0.23741044 0.23709898 0.23690698 0.2368449  0.23683536
 0.2367164  0.23627563 0.23556152 0.23477687 0.234202   0.23384054
 0.2336819  0.23366977 0.23357876 0.23335165 0.23312572 0.23294076
 0.23277678 0.23270015 0.23283873 0.23305745 0.23325023 0.23335727
 0.23333149 0.23321325 0.23312762 0.23311052 0.2330845  0.23294769
 0.23269647 0.23219655 0.23158877 0.23101543 0.23064592 0.23032771
 0.23009782 0.230041   0.23011114 0.23027952 0.23042995 0.23053426
 0.23049092 0.23038922 0.23031764 0.2302883  0.23022002 0.23024188
 0.2301744  0.22995067 0.22969155 0.22952987 0.22954363 0.22974655
 0.2300769  0.23034124 0.23045199 0.23048125 0.23056133 0.23067962
 0.23075029 0.23075962 0.23073357 0.2308057  0.23092683 0.23099457
 0.23089992 0.2307118  0.23057634 0.23064402 0.23090915 0.23129602
 0.23154123 0.23148687 0.23129396 0.2311273  0.23101698 0.23087332
 0.23047195 0.22973773 0.22875643 0.22794665 0.22747807 0.22724302
 0.22709028 0.22703144 0.22694875 0.22687347 0.2268869  0.22704202
 0.22721736 0.22727746 0.22722854 0.22718614 0.22731586 0.2275527
 0.22768858 0.22740625 0.22686954 0.22640531 0.22623247 0.22640564
 0.22663724 0.22643283 0.22594172 0.22538203 0.2251431  0.2252527
 0.22543485 0.22547823 0.2253164  0.22498348 0.22463064 0.22452743
 0.2246008  0.22460368 0.22451752 0.22433162 0.22415374 0.2241461
 0.22430652 0.22432321 0.2241626  0.22395489 0.22396083 0.22412603
 0.2244288  0.22444606 0.22405116 0.22355975 0.22335115 0.22358699
 0.22409807 0.22458857 0.22472525 0.2243246  0.22372738 0.22339644
 0.22346973 0.22378841 0.22418188 0.22433488 0.22426069 0.22421156
 0.22437976 0.22457449 0.22475374 0.22485666 0.22485712 0.2247598
 0.22460558 0.22434099 0.22380842 0.22312295 0.22244099 0.22200625
 0.2219869  0.22217897 0.22224353 0.22193767 0.22138089 0.22095196
 0.2209722  0.22131398 0.22160976 0.22160919 0.22128265 0.22082196
 0.22061004 0.22063416 0.22094639 0.22140653 0.22195993 0.22239679
 0.22288196 0.22331531 0.22352606 0.22331731 0.22291249 0.22266261
 0.22275986 0.2231338  0.2234743  0.22353359 0.22311346 0.22248922
 0.22209841 0.22217515 0.2227539  0.22325931 0.2233962  0.22308074
 0.22272152 0.22257124 0.22285788 0.22330797 0.22364198 0.22363599
 0.22351244 0.22347917 0.22326827 0.22301233 0.22265428 0.22200269
 0.22126989 0.22085409 0.22075103 0.2209065  0.22073139 0.22006038
 0.2193314  0.21907166 0.21959583 0.2207847  0.22147854 0.220903
 0.21925525 0.21777886 0.21801451 0.21990453 0.22045898 0.21338637]
