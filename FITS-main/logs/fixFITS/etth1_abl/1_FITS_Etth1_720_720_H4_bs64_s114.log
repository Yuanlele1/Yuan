Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.0898549556732178
Epoch: 1, Steps: 56 | Train Loss: 0.9131721 Vali Loss: 1.8568994 Test Loss: 0.7365046
Validation loss decreased (inf --> 1.856899).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.1753227710723877
Epoch: 2, Steps: 56 | Train Loss: 0.7329053 Vali Loss: 1.6890467 Test Loss: 0.6246353
Validation loss decreased (1.856899 --> 1.689047).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0479164123535156
Epoch: 3, Steps: 56 | Train Loss: 0.6772893 Vali Loss: 1.6230271 Test Loss: 0.5742725
Validation loss decreased (1.689047 --> 1.623027).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.207817792892456
Epoch: 4, Steps: 56 | Train Loss: 0.6481621 Vali Loss: 1.5799032 Test Loss: 0.5406951
Validation loss decreased (1.623027 --> 1.579903).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.133362054824829
Epoch: 5, Steps: 56 | Train Loss: 0.6280072 Vali Loss: 1.5440468 Test Loss: 0.5155368
Validation loss decreased (1.579903 --> 1.544047).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.0642573833465576
Epoch: 6, Steps: 56 | Train Loss: 0.6135324 Vali Loss: 1.5199640 Test Loss: 0.4965321
Validation loss decreased (1.544047 --> 1.519964).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.2299904823303223
Epoch: 7, Steps: 56 | Train Loss: 0.6025513 Vali Loss: 1.5019637 Test Loss: 0.4819816
Validation loss decreased (1.519964 --> 1.501964).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.0876781940460205
Epoch: 8, Steps: 56 | Train Loss: 0.5942984 Vali Loss: 1.4781860 Test Loss: 0.4707182
Validation loss decreased (1.501964 --> 1.478186).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.061520576477051
Epoch: 9, Steps: 56 | Train Loss: 0.5874686 Vali Loss: 1.4711113 Test Loss: 0.4621887
Validation loss decreased (1.478186 --> 1.471111).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.9780561923980713
Epoch: 10, Steps: 56 | Train Loss: 0.5825089 Vali Loss: 1.4591640 Test Loss: 0.4555941
Validation loss decreased (1.471111 --> 1.459164).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.2016050815582275
Epoch: 11, Steps: 56 | Train Loss: 0.5779159 Vali Loss: 1.4626698 Test Loss: 0.4504442
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.224637985229492
Epoch: 12, Steps: 56 | Train Loss: 0.5747124 Vali Loss: 1.4534192 Test Loss: 0.4466127
Validation loss decreased (1.459164 --> 1.453419).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9640412330627441
Epoch: 13, Steps: 56 | Train Loss: 0.5717748 Vali Loss: 1.4502180 Test Loss: 0.4434662
Validation loss decreased (1.453419 --> 1.450218).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.025757074356079
Epoch: 14, Steps: 56 | Train Loss: 0.5696133 Vali Loss: 1.4485943 Test Loss: 0.4412154
Validation loss decreased (1.450218 --> 1.448594).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.2547218799591064
Epoch: 15, Steps: 56 | Train Loss: 0.5676208 Vali Loss: 1.4417540 Test Loss: 0.4394591
Validation loss decreased (1.448594 --> 1.441754).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.1096725463867188
Epoch: 16, Steps: 56 | Train Loss: 0.5663551 Vali Loss: 1.4433885 Test Loss: 0.4380037
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.1388943195343018
Epoch: 17, Steps: 56 | Train Loss: 0.5651533 Vali Loss: 1.4341362 Test Loss: 0.4370582
Validation loss decreased (1.441754 --> 1.434136).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.1376688480377197
Epoch: 18, Steps: 56 | Train Loss: 0.5640685 Vali Loss: 1.4385171 Test Loss: 0.4362387
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.2233686447143555
Epoch: 19, Steps: 56 | Train Loss: 0.5630585 Vali Loss: 1.4400852 Test Loss: 0.4357056
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.180941581726074
Epoch: 20, Steps: 56 | Train Loss: 0.5622879 Vali Loss: 1.4368570 Test Loss: 0.4352001
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.2327001094818115
Epoch: 21, Steps: 56 | Train Loss: 0.5616982 Vali Loss: 1.4339707 Test Loss: 0.4348006
Validation loss decreased (1.434136 --> 1.433971).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0825178623199463
Epoch: 22, Steps: 56 | Train Loss: 0.5608531 Vali Loss: 1.4333782 Test Loss: 0.4346325
Validation loss decreased (1.433971 --> 1.433378).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.056211233139038
Epoch: 23, Steps: 56 | Train Loss: 0.5602134 Vali Loss: 1.4393318 Test Loss: 0.4344071
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.3172085285186768
Epoch: 24, Steps: 56 | Train Loss: 0.5596932 Vali Loss: 1.4355340 Test Loss: 0.4342780
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.0601303577423096
Epoch: 25, Steps: 56 | Train Loss: 0.5595105 Vali Loss: 1.4335649 Test Loss: 0.4341797
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.2735331058502197
Epoch: 26, Steps: 56 | Train Loss: 0.5592598 Vali Loss: 1.4353404 Test Loss: 0.4341305
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1054728031158447
Epoch: 27, Steps: 56 | Train Loss: 0.5585643 Vali Loss: 1.4336414 Test Loss: 0.4341347
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.1416237354278564
Epoch: 28, Steps: 56 | Train Loss: 0.5586275 Vali Loss: 1.4324439 Test Loss: 0.4341030
Validation loss decreased (1.433378 --> 1.432444).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.2032155990600586
Epoch: 29, Steps: 56 | Train Loss: 0.5583361 Vali Loss: 1.4396964 Test Loss: 0.4340867
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.295544385910034
Epoch: 30, Steps: 56 | Train Loss: 0.5580710 Vali Loss: 1.4388924 Test Loss: 0.4340455
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.135390043258667
Epoch: 31, Steps: 56 | Train Loss: 0.5577198 Vali Loss: 1.4375315 Test Loss: 0.4340558
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.1017301082611084
Epoch: 32, Steps: 56 | Train Loss: 0.5579421 Vali Loss: 1.4343597 Test Loss: 0.4340778
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.076841354370117
Epoch: 33, Steps: 56 | Train Loss: 0.5575526 Vali Loss: 1.4371039 Test Loss: 0.4340729
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.109557867050171
Epoch: 34, Steps: 56 | Train Loss: 0.5572433 Vali Loss: 1.4372991 Test Loss: 0.4340814
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.0801188945770264
Epoch: 35, Steps: 56 | Train Loss: 0.5565777 Vali Loss: 1.4329325 Test Loss: 0.4341096
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.016328811645508
Epoch: 36, Steps: 56 | Train Loss: 0.5570721 Vali Loss: 1.4322295 Test Loss: 0.4341818
Validation loss decreased (1.432444 --> 1.432230).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.12707257270813
Epoch: 37, Steps: 56 | Train Loss: 0.5566512 Vali Loss: 1.4311781 Test Loss: 0.4341997
Validation loss decreased (1.432230 --> 1.431178).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9039647579193115
Epoch: 38, Steps: 56 | Train Loss: 0.5561117 Vali Loss: 1.4391379 Test Loss: 0.4341912
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.118536949157715
Epoch: 39, Steps: 56 | Train Loss: 0.5561491 Vali Loss: 1.4357027 Test Loss: 0.4342522
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.069945812225342
Epoch: 40, Steps: 56 | Train Loss: 0.5560973 Vali Loss: 1.4317319 Test Loss: 0.4342238
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.1996757984161377
Epoch: 41, Steps: 56 | Train Loss: 0.5562468 Vali Loss: 1.4324971 Test Loss: 0.4342777
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.0702693462371826
Epoch: 42, Steps: 56 | Train Loss: 0.5559005 Vali Loss: 1.4338517 Test Loss: 0.4342957
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.8714072704315186
Epoch: 43, Steps: 56 | Train Loss: 0.5558413 Vali Loss: 1.4359857 Test Loss: 0.4342961
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.9674644470214844
Epoch: 44, Steps: 56 | Train Loss: 0.5559352 Vali Loss: 1.4383147 Test Loss: 0.4343573
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.017315626144409
Epoch: 45, Steps: 56 | Train Loss: 0.5560968 Vali Loss: 1.4335129 Test Loss: 0.4343754
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.140176296234131
Epoch: 46, Steps: 56 | Train Loss: 0.5557455 Vali Loss: 1.4309993 Test Loss: 0.4343608
Validation loss decreased (1.431178 --> 1.430999).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.02405047416687
Epoch: 47, Steps: 56 | Train Loss: 0.5557036 Vali Loss: 1.4355574 Test Loss: 0.4343873
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.14009952545166
Epoch: 48, Steps: 56 | Train Loss: 0.5554975 Vali Loss: 1.4378941 Test Loss: 0.4344220
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.0785775184631348
Epoch: 49, Steps: 56 | Train Loss: 0.5554504 Vali Loss: 1.4350173 Test Loss: 0.4344219
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.0109825134277344
Epoch: 50, Steps: 56 | Train Loss: 0.5556985 Vali Loss: 1.4372766 Test Loss: 0.4344407
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.180004835128784
Epoch: 51, Steps: 56 | Train Loss: 0.5554714 Vali Loss: 1.4331400 Test Loss: 0.4344722
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.172034740447998
Epoch: 52, Steps: 56 | Train Loss: 0.5554856 Vali Loss: 1.4346758 Test Loss: 0.4344974
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.995450496673584
Epoch: 53, Steps: 56 | Train Loss: 0.5550748 Vali Loss: 1.4363337 Test Loss: 0.4345174
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.0114145278930664
Epoch: 54, Steps: 56 | Train Loss: 0.5552610 Vali Loss: 1.4393202 Test Loss: 0.4345075
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.0815954208374023
Epoch: 55, Steps: 56 | Train Loss: 0.5554089 Vali Loss: 1.4352868 Test Loss: 0.4345257
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.188822031021118
Epoch: 56, Steps: 56 | Train Loss: 0.5550297 Vali Loss: 1.4383337 Test Loss: 0.4345518
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.114651918411255
Epoch: 57, Steps: 56 | Train Loss: 0.5553593 Vali Loss: 1.4368367 Test Loss: 0.4345557
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.1504528522491455
Epoch: 58, Steps: 56 | Train Loss: 0.5553758 Vali Loss: 1.4356163 Test Loss: 0.4345808
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.0623443126678467
Epoch: 59, Steps: 56 | Train Loss: 0.5550767 Vali Loss: 1.4374199 Test Loss: 0.4345951
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.095013380050659
Epoch: 60, Steps: 56 | Train Loss: 0.5549936 Vali Loss: 1.4336034 Test Loss: 0.4346019
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.1083619594573975
Epoch: 61, Steps: 56 | Train Loss: 0.5546886 Vali Loss: 1.4384987 Test Loss: 0.4346099
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.22672438621521
Epoch: 62, Steps: 56 | Train Loss: 0.5550783 Vali Loss: 1.4338281 Test Loss: 0.4346331
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.1523375511169434
Epoch: 63, Steps: 56 | Train Loss: 0.5546861 Vali Loss: 1.4370492 Test Loss: 0.4346366
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.985675573348999
Epoch: 64, Steps: 56 | Train Loss: 0.5547360 Vali Loss: 1.4380299 Test Loss: 0.4346476
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.1973893642425537
Epoch: 65, Steps: 56 | Train Loss: 0.5548022 Vali Loss: 1.4390202 Test Loss: 0.4346504
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.004281997680664
Epoch: 66, Steps: 56 | Train Loss: 0.5546893 Vali Loss: 1.4345993 Test Loss: 0.4346644
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.43344590067863464, mae:0.4564138352870941, rse:0.6302595138549805, corr:[0.2160416  0.23132937 0.23216549 0.228593   0.22841619 0.230121
 0.2310023  0.23037204 0.22929622 0.22900985 0.22936977 0.22956361
 0.22945176 0.22916408 0.2287687  0.22834527 0.22792904 0.2277543
 0.22797212 0.22829287 0.22845872 0.22842874 0.22826083 0.22847101
 0.22906451 0.22965802 0.229838   0.22944711 0.22895697 0.22879772
 0.2289888  0.22914697 0.2290256  0.22860688 0.22807573 0.22765405
 0.22753625 0.22754492 0.2276703  0.22788943 0.22814038 0.22817776
 0.22806333 0.2281136  0.22856985 0.22925676 0.22982614 0.22995527
 0.22951412 0.22882523 0.22811097 0.2274935  0.22693509 0.22613382
 0.22518678 0.22429551 0.22397532 0.22393541 0.22387178 0.22371364
 0.22351162 0.22326614 0.22299373 0.22286703 0.22291097 0.223134
 0.22341622 0.22357062 0.22353008 0.22329071 0.22295338 0.22288333
 0.22277492 0.22250536 0.22204944 0.22148988 0.22102964 0.22072208
 0.22060354 0.22055694 0.22033687 0.21991178 0.2194223  0.21903484
 0.21881528 0.21862842 0.21840192 0.21823263 0.21805161 0.21789902
 0.21776932 0.2179326  0.2183284  0.21887371 0.21941732 0.22009261
 0.22090864 0.22169071 0.22231485 0.22281152 0.22305237 0.22298114
 0.22269972 0.2224353  0.22234453 0.22227001 0.22205359 0.22170064
 0.22141232 0.22128026 0.22122952 0.22126308 0.2213454  0.22136928
 0.22143823 0.2215509  0.22163616 0.22158557 0.22148326 0.22149196
 0.22146678 0.22117794 0.22058168 0.21996218 0.21939054 0.21901342
 0.21885303 0.21886349 0.2187405  0.21839315 0.21798664 0.21768954
 0.21762861 0.2176184  0.21757932 0.21752752 0.2174336  0.21731202
 0.21719997 0.21716543 0.21723819 0.21726884 0.21710327 0.21694104
 0.21673326 0.21637028 0.21595779 0.21558088 0.215362   0.21516874
 0.2149097  0.21477558 0.21480604 0.21487299 0.2148512  0.21468458
 0.21436518 0.21402805 0.21388936 0.21404998 0.21423772 0.21440601
 0.21450716 0.21465799 0.21479726 0.21490534 0.21479265 0.21484724
 0.21516463 0.21566242 0.21605024 0.21624097 0.21609415 0.21577348
 0.21554835 0.21559054 0.2156116  0.21542053 0.21506698 0.21478936
 0.21473451 0.21486242 0.21504594 0.21519077 0.21532552 0.21548893
 0.21568532 0.21589096 0.21609804 0.21610829 0.21597904 0.21577604
 0.2154712  0.21498203 0.21437229 0.2138799  0.21363866 0.21350312
 0.21325663 0.21297182 0.21274318 0.21263643 0.21273446 0.21293265
 0.21310298 0.2132311  0.2133369  0.21339543 0.21340881 0.21337369
 0.21325994 0.21307383 0.21290623 0.21267697 0.21242745 0.21233307
 0.2123467  0.21239413 0.21239392 0.2123186  0.21216138 0.21195313
 0.21188028 0.21197572 0.2119833  0.21171436 0.21120402 0.21076494
 0.21060318 0.21062697 0.2106248  0.21047117 0.21013413 0.20982894
 0.20973949 0.20981048 0.20988461 0.20987228 0.2098718  0.2099831
 0.21025406 0.2104779  0.21057516 0.21059255 0.21055326 0.2104865
 0.21043316 0.21026489 0.2099265  0.20953289 0.20932077 0.209314
 0.20938528 0.20930333 0.20920028 0.20917179 0.20928724 0.2094516
 0.20953669 0.2094309  0.20942152 0.2094526  0.20941631 0.20932192
 0.20921007 0.20910537 0.20900945 0.20885961 0.20849489 0.2080689
 0.20785053 0.20792954 0.20805234 0.20806986 0.20791498 0.2077531
 0.20772582 0.20790516 0.20797276 0.20787615 0.20765962 0.20757458
 0.20758721 0.20773412 0.20783663 0.20784749 0.20793322 0.20834155
 0.20895858 0.20946485 0.2096524  0.20964718 0.20967007 0.20982984
 0.2099573  0.20992814 0.20968366 0.20950027 0.20960815 0.20995267
 0.21022187 0.21018127 0.209991   0.2099294  0.21000019 0.21013133
 0.21026993 0.21024093 0.21018921 0.21028121 0.2104649  0.21075505
 0.21098697 0.21096496 0.21070358 0.21042098 0.21008426 0.2096214
 0.20909394 0.20873916 0.20856741 0.20840001 0.20814736 0.20789292
 0.20775755 0.20782124 0.20796691 0.20818584 0.2082094  0.20814599
 0.20812199 0.2081468  0.20823175 0.20829579 0.20826593 0.20821819
 0.20814463 0.20785788 0.20738693 0.2069055  0.20655961 0.20637332
 0.20619552 0.20585155 0.20536235 0.20507741 0.20524068 0.20569915
 0.20601147 0.20604177 0.20589639 0.20585981 0.20596923 0.20626184
 0.20640859 0.20630111 0.20615976 0.20621915 0.20622805 0.20615496
 0.20588344 0.20539811 0.20495306 0.2046904  0.20439768 0.20388882
 0.20332377 0.20305207 0.20308475 0.20310436 0.20283239 0.20236242
 0.20188154 0.20160997 0.2015691  0.20148426 0.20132488 0.20120509
 0.20124777 0.2013277  0.20139135 0.20151629 0.20169553 0.20229574
 0.20317048 0.20387316 0.20403501 0.20369154 0.20319489 0.20271169
 0.2023642  0.20197569 0.20140661 0.20081441 0.2004337  0.20046759
 0.20065331 0.20062251 0.20043714 0.20031765 0.2004918  0.20095178
 0.20134084 0.20141037 0.20139684 0.20157701 0.2018663  0.20224446
 0.20252305 0.20264632 0.20279647 0.20307618 0.20329039 0.20303385
 0.20247541 0.20197637 0.20182447 0.20190753 0.20196386 0.20180517
 0.20146254 0.20122264 0.20106156 0.20100911 0.20094416 0.20093893
 0.2009994  0.201168   0.2012208  0.20153844 0.20193136 0.20238036
 0.2028445  0.20301752 0.2027484  0.20238742 0.20219211 0.20219941
 0.20227925 0.20225865 0.20186426 0.2014006  0.20122516 0.20146605
 0.20171894 0.20160548 0.20117578 0.20073318 0.20058322 0.20076145
 0.20101693 0.20106138 0.20101339 0.20119002 0.20172426 0.20252874
 0.20310476 0.20315999 0.20287414 0.20256789 0.20229085 0.2017734
 0.20109576 0.20059551 0.2004877  0.20081426 0.20111085 0.20114003
 0.20092838 0.20081435 0.2009823  0.20128274 0.20136087 0.20122027
 0.20104308 0.2010822  0.20124273 0.20117113 0.20102899 0.20090295
 0.20097011 0.2011941  0.20121133 0.20087208 0.20030607 0.19987245
 0.19982666 0.20012954 0.20010342 0.19964719 0.19911693 0.19901516
 0.19937196 0.1996841  0.19958363 0.19909851 0.19864924 0.19849965
 0.19862284 0.19876999 0.1989114  0.19923796 0.19988343 0.20072706
 0.20137604 0.20169367 0.20178407 0.20194164 0.20214662 0.20191945
 0.20121643 0.20049524 0.20004927 0.19994704 0.2000189  0.2000371
 0.19984736 0.19968198 0.20000301 0.20053136 0.2009909  0.20112337
 0.20111829 0.20135091 0.20213994 0.20264326 0.20307454 0.20293176
 0.2029598  0.20311934 0.20311488 0.20281999 0.20201948 0.20132944
 0.20115034 0.20149857 0.2015664  0.201079   0.20030305 0.20001131
 0.2003787  0.20091584 0.20128863 0.20130542 0.20135924 0.20159002
 0.20186894 0.20189959 0.20157687 0.20117693 0.20092939 0.20074616
 0.20025267 0.19943187 0.19853164 0.19801548 0.19788496 0.19767168
 0.19702254 0.1961989  0.19558887 0.19553609 0.19570872 0.19561672
 0.19496815 0.19404836 0.19378453 0.1939406  0.19429731 0.1942716
 0.19403198 0.1937564  0.19394557 0.19429582 0.19436891 0.19397238
 0.19347213 0.19332846 0.19324265 0.19312297 0.19269748 0.19200784
 0.19173107 0.19174401 0.19171275 0.19120464 0.19065206 0.190468
 0.19082536 0.19102022 0.190863   0.19044434 0.19024049 0.19045742
 0.19089204 0.19117048 0.19119197 0.19114214 0.1910599  0.19082956
 0.19030663 0.18945128 0.18855429 0.18799728 0.18763593 0.18689738
 0.18572667 0.18467249 0.1842139  0.18447644 0.18482763 0.18474703
 0.18419233 0.18361068 0.18352407 0.18372884 0.18377502 0.18348534
 0.18321677 0.18342075 0.18412691 0.1847891  0.1849732  0.18465036
 0.18420665 0.1839499  0.18362331 0.18305765 0.18220149 0.18141605
 0.18121648 0.1813794  0.18114989 0.18036848 0.17962289 0.17968532
 0.18037282 0.18068384 0.18033536 0.17967203 0.17929292 0.17945382
 0.17970502 0.17969963 0.17951122 0.17953983 0.1800211  0.18051435
 0.18021578 0.1791331  0.17802458 0.17798753 0.17850418 0.17844084
 0.17729217 0.17574327 0.174987   0.17513658 0.17541607 0.17506033
 0.17410761 0.17322096 0.17318727 0.17368898 0.17402704 0.17384905
 0.17349154 0.17355505 0.17386971 0.17390956 0.17320056 0.1721838
 0.17121637 0.17045458 0.1692494  0.16724499 0.16506387 0.16343641
 0.16333118 0.16383418 0.16365126 0.16223955 0.16107206 0.16133131
 0.16236171 0.16266626 0.16218542 0.16160122 0.16165866 0.16222417
 0.16248369 0.16223662 0.16216765 0.1626329  0.163156   0.16279857
 0.16110548 0.1594859  0.15894872 0.15987334 0.16028371 0.15862855
 0.15611199 0.15489344 0.1558559  0.15718465 0.15674804 0.1546232
 0.15314665 0.15336373 0.1545865  0.15423143 0.15250863 0.15212496
 0.15430719 0.15572898 0.15172498 0.14631952 0.15232149 0.16740568]
