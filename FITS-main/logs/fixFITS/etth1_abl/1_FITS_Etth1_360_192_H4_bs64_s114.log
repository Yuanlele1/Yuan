Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=74, out_features=113, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7492352.0
params:  8475.0
Trainable parameters:  8475
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.975099802017212
Epoch: 1, Steps: 63 | Train Loss: 0.7244400 Vali Loss: 1.4286035 Test Loss: 0.7178912
Validation loss decreased (inf --> 1.428604).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.633657932281494
Epoch: 2, Steps: 63 | Train Loss: 0.5590280 Vali Loss: 1.2321142 Test Loss: 0.5827955
Validation loss decreased (1.428604 --> 1.232114).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.5662033557891846
Epoch: 3, Steps: 63 | Train Loss: 0.4969311 Vali Loss: 1.1329298 Test Loss: 0.5168266
Validation loss decreased (1.232114 --> 1.132930).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.229086399078369
Epoch: 4, Steps: 63 | Train Loss: 0.4634064 Vali Loss: 1.0710633 Test Loss: 0.4768870
Validation loss decreased (1.132930 --> 1.071063).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.5945496559143066
Epoch: 5, Steps: 63 | Train Loss: 0.4424355 Vali Loss: 1.0296888 Test Loss: 0.4520288
Validation loss decreased (1.071063 --> 1.029689).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.0346579551696777
Epoch: 6, Steps: 63 | Train Loss: 0.4288971 Vali Loss: 1.0022669 Test Loss: 0.4364745
Validation loss decreased (1.029689 --> 1.002267).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.7175796031951904
Epoch: 7, Steps: 63 | Train Loss: 0.4200062 Vali Loss: 0.9833155 Test Loss: 0.4269055
Validation loss decreased (1.002267 --> 0.983316).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.6552305221557617
Epoch: 8, Steps: 63 | Train Loss: 0.4140981 Vali Loss: 0.9697940 Test Loss: 0.4211842
Validation loss decreased (0.983316 --> 0.969794).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.197819709777832
Epoch: 9, Steps: 63 | Train Loss: 0.4104785 Vali Loss: 0.9604724 Test Loss: 0.4174028
Validation loss decreased (0.969794 --> 0.960472).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.1881980895996094
Epoch: 10, Steps: 63 | Train Loss: 0.4075588 Vali Loss: 0.9547641 Test Loss: 0.4157462
Validation loss decreased (0.960472 --> 0.954764).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.451862096786499
Epoch: 11, Steps: 63 | Train Loss: 0.4060133 Vali Loss: 0.9501029 Test Loss: 0.4146043
Validation loss decreased (0.954764 --> 0.950103).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.8190879821777344
Epoch: 12, Steps: 63 | Train Loss: 0.4047472 Vali Loss: 0.9468065 Test Loss: 0.4138700
Validation loss decreased (0.950103 --> 0.946806).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.38360595703125
Epoch: 13, Steps: 63 | Train Loss: 0.4039321 Vali Loss: 0.9434748 Test Loss: 0.4135845
Validation loss decreased (0.946806 --> 0.943475).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.055974245071411
Epoch: 14, Steps: 63 | Train Loss: 0.4029327 Vali Loss: 0.9416763 Test Loss: 0.4133810
Validation loss decreased (0.943475 --> 0.941676).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.1075234413146973
Epoch: 15, Steps: 63 | Train Loss: 0.4027692 Vali Loss: 0.9400855 Test Loss: 0.4133323
Validation loss decreased (0.941676 --> 0.940085).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.0322039127349854
Epoch: 16, Steps: 63 | Train Loss: 0.4022693 Vali Loss: 0.9386720 Test Loss: 0.4133530
Validation loss decreased (0.940085 --> 0.938672).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.875298261642456
Epoch: 17, Steps: 63 | Train Loss: 0.4018401 Vali Loss: 0.9374018 Test Loss: 0.4134189
Validation loss decreased (0.938672 --> 0.937402).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.4497671127319336
Epoch: 18, Steps: 63 | Train Loss: 0.4018375 Vali Loss: 0.9357083 Test Loss: 0.4132327
Validation loss decreased (0.937402 --> 0.935708).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.8548932075500488
Epoch: 19, Steps: 63 | Train Loss: 0.4013025 Vali Loss: 0.9356990 Test Loss: 0.4133564
Validation loss decreased (0.935708 --> 0.935699).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.7961273193359375
Epoch: 20, Steps: 63 | Train Loss: 0.4012332 Vali Loss: 0.9341386 Test Loss: 0.4132611
Validation loss decreased (0.935699 --> 0.934139).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.7327017784118652
Epoch: 21, Steps: 63 | Train Loss: 0.4009819 Vali Loss: 0.9338020 Test Loss: 0.4132962
Validation loss decreased (0.934139 --> 0.933802).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.1361992359161377
Epoch: 22, Steps: 63 | Train Loss: 0.4007220 Vali Loss: 0.9331039 Test Loss: 0.4133079
Validation loss decreased (0.933802 --> 0.933104).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.7549166679382324
Epoch: 23, Steps: 63 | Train Loss: 0.4007623 Vali Loss: 0.9323079 Test Loss: 0.4132348
Validation loss decreased (0.933104 --> 0.932308).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.9809746742248535
Epoch: 24, Steps: 63 | Train Loss: 0.4005226 Vali Loss: 0.9322909 Test Loss: 0.4132684
Validation loss decreased (0.932308 --> 0.932291).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.8953266143798828
Epoch: 25, Steps: 63 | Train Loss: 0.4003469 Vali Loss: 0.9314694 Test Loss: 0.4132882
Validation loss decreased (0.932291 --> 0.931469).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.249912738800049
Epoch: 26, Steps: 63 | Train Loss: 0.4000931 Vali Loss: 0.9313072 Test Loss: 0.4133388
Validation loss decreased (0.931469 --> 0.931307).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.4768574237823486
Epoch: 27, Steps: 63 | Train Loss: 0.4003711 Vali Loss: 0.9309316 Test Loss: 0.4132810
Validation loss decreased (0.931307 --> 0.930932).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.036290407180786
Epoch: 28, Steps: 63 | Train Loss: 0.4002189 Vali Loss: 0.9302330 Test Loss: 0.4132796
Validation loss decreased (0.930932 --> 0.930233).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.1930594444274902
Epoch: 29, Steps: 63 | Train Loss: 0.3999554 Vali Loss: 0.9302589 Test Loss: 0.4132715
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.775421142578125
Epoch: 30, Steps: 63 | Train Loss: 0.3999994 Vali Loss: 0.9294867 Test Loss: 0.4131734
Validation loss decreased (0.930233 --> 0.929487).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.8637404441833496
Epoch: 31, Steps: 63 | Train Loss: 0.3997845 Vali Loss: 0.9293653 Test Loss: 0.4132935
Validation loss decreased (0.929487 --> 0.929365).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.0682036876678467
Epoch: 32, Steps: 63 | Train Loss: 0.3999040 Vali Loss: 0.9293042 Test Loss: 0.4132371
Validation loss decreased (0.929365 --> 0.929304).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.8917078971862793
Epoch: 33, Steps: 63 | Train Loss: 0.3996088 Vali Loss: 0.9291121 Test Loss: 0.4132206
Validation loss decreased (0.929304 --> 0.929112).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.6513712406158447
Epoch: 34, Steps: 63 | Train Loss: 0.3994926 Vali Loss: 0.9289109 Test Loss: 0.4132049
Validation loss decreased (0.929112 --> 0.928911).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.8170642852783203
Epoch: 35, Steps: 63 | Train Loss: 0.3998332 Vali Loss: 0.9286343 Test Loss: 0.4132190
Validation loss decreased (0.928911 --> 0.928634).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.9449727535247803
Epoch: 36, Steps: 63 | Train Loss: 0.3995171 Vali Loss: 0.9281743 Test Loss: 0.4131449
Validation loss decreased (0.928634 --> 0.928174).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.0789971351623535
Epoch: 37, Steps: 63 | Train Loss: 0.3997470 Vali Loss: 0.9279912 Test Loss: 0.4131777
Validation loss decreased (0.928174 --> 0.927991).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.146864414215088
Epoch: 38, Steps: 63 | Train Loss: 0.3996165 Vali Loss: 0.9280038 Test Loss: 0.4131956
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.9119007587432861
Epoch: 39, Steps: 63 | Train Loss: 0.3992231 Vali Loss: 0.9280493 Test Loss: 0.4132115
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.8538508415222168
Epoch: 40, Steps: 63 | Train Loss: 0.3994882 Vali Loss: 0.9274609 Test Loss: 0.4131741
Validation loss decreased (0.927991 --> 0.927461).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9081752300262451
Epoch: 41, Steps: 63 | Train Loss: 0.3996349 Vali Loss: 0.9272440 Test Loss: 0.4131859
Validation loss decreased (0.927461 --> 0.927244).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.098292350769043
Epoch: 42, Steps: 63 | Train Loss: 0.3994062 Vali Loss: 0.9273933 Test Loss: 0.4131376
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9001593589782715
Epoch: 43, Steps: 63 | Train Loss: 0.3993986 Vali Loss: 0.9272964 Test Loss: 0.4131927
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.0209596157073975
Epoch: 44, Steps: 63 | Train Loss: 0.3994866 Vali Loss: 0.9269435 Test Loss: 0.4131381
Validation loss decreased (0.927244 --> 0.926944).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.13086199760437
Epoch: 45, Steps: 63 | Train Loss: 0.3991459 Vali Loss: 0.9270797 Test Loss: 0.4131823
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.640977144241333
Epoch: 46, Steps: 63 | Train Loss: 0.3991503 Vali Loss: 0.9271575 Test Loss: 0.4131760
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.000478506088257
Epoch: 47, Steps: 63 | Train Loss: 0.3991454 Vali Loss: 0.9270266 Test Loss: 0.4131703
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.228278636932373
Epoch: 48, Steps: 63 | Train Loss: 0.3989348 Vali Loss: 0.9269990 Test Loss: 0.4131458
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.5092029571533203
Epoch: 49, Steps: 63 | Train Loss: 0.3993296 Vali Loss: 0.9266270 Test Loss: 0.4131544
Validation loss decreased (0.926944 --> 0.926627).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.4014201164245605
Epoch: 50, Steps: 63 | Train Loss: 0.3993694 Vali Loss: 0.9267701 Test Loss: 0.4131218
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9050252437591553
Epoch: 51, Steps: 63 | Train Loss: 0.3992236 Vali Loss: 0.9265008 Test Loss: 0.4131635
Validation loss decreased (0.926627 --> 0.926501).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.503429412841797
Epoch: 52, Steps: 63 | Train Loss: 0.3989848 Vali Loss: 0.9261365 Test Loss: 0.4131592
Validation loss decreased (0.926501 --> 0.926136).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.9010424613952637
Epoch: 53, Steps: 63 | Train Loss: 0.3992468 Vali Loss: 0.9264081 Test Loss: 0.4131643
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.189241886138916
Epoch: 54, Steps: 63 | Train Loss: 0.3993055 Vali Loss: 0.9266081 Test Loss: 0.4131782
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.9370906352996826
Epoch: 55, Steps: 63 | Train Loss: 0.3990692 Vali Loss: 0.9264532 Test Loss: 0.4131584
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.305682420730591
Epoch: 56, Steps: 63 | Train Loss: 0.3991570 Vali Loss: 0.9264631 Test Loss: 0.4131531
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.8884503841400146
Epoch: 57, Steps: 63 | Train Loss: 0.3992311 Vali Loss: 0.9263795 Test Loss: 0.4131587
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.7929027080535889
Epoch: 58, Steps: 63 | Train Loss: 0.3991491 Vali Loss: 0.9262782 Test Loss: 0.4131499
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.5624189376831055
Epoch: 59, Steps: 63 | Train Loss: 0.3992851 Vali Loss: 0.9259242 Test Loss: 0.4131523
Validation loss decreased (0.926136 --> 0.925924).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.9814136028289795
Epoch: 60, Steps: 63 | Train Loss: 0.3994204 Vali Loss: 0.9258688 Test Loss: 0.4131681
Validation loss decreased (0.925924 --> 0.925869).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.967738389968872
Epoch: 61, Steps: 63 | Train Loss: 0.3991470 Vali Loss: 0.9260793 Test Loss: 0.4131524
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.4018661975860596
Epoch: 62, Steps: 63 | Train Loss: 0.3991325 Vali Loss: 0.9262266 Test Loss: 0.4131506
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.1064014434814453
Epoch: 63, Steps: 63 | Train Loss: 0.3990457 Vali Loss: 0.9256609 Test Loss: 0.4131465
Validation loss decreased (0.925869 --> 0.925661).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.922375202178955
Epoch: 64, Steps: 63 | Train Loss: 0.3991231 Vali Loss: 0.9262441 Test Loss: 0.4131393
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.5948567390441895
Epoch: 65, Steps: 63 | Train Loss: 0.3992103 Vali Loss: 0.9258149 Test Loss: 0.4131571
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.853219747543335
Epoch: 66, Steps: 63 | Train Loss: 0.3991217 Vali Loss: 0.9261312 Test Loss: 0.4131489
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.642712354660034
Epoch: 67, Steps: 63 | Train Loss: 0.3989849 Vali Loss: 0.9255763 Test Loss: 0.4131411
Validation loss decreased (0.925661 --> 0.925576).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.7094168663024902
Epoch: 68, Steps: 63 | Train Loss: 0.3992024 Vali Loss: 0.9261338 Test Loss: 0.4131494
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.8333218097686768
Epoch: 69, Steps: 63 | Train Loss: 0.3990026 Vali Loss: 0.9260053 Test Loss: 0.4131471
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.7200241088867188
Epoch: 70, Steps: 63 | Train Loss: 0.3989961 Vali Loss: 0.9259902 Test Loss: 0.4131411
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.7867424488067627
Epoch: 71, Steps: 63 | Train Loss: 0.3989806 Vali Loss: 0.9259047 Test Loss: 0.4131418
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.9371778964996338
Epoch: 72, Steps: 63 | Train Loss: 0.3992066 Vali Loss: 0.9256995 Test Loss: 0.4131444
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.001131534576416
Epoch: 73, Steps: 63 | Train Loss: 0.3991597 Vali Loss: 0.9258143 Test Loss: 0.4131380
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.1969847679138184
Epoch: 74, Steps: 63 | Train Loss: 0.3989099 Vali Loss: 0.9257727 Test Loss: 0.4131466
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.1893599033355713
Epoch: 75, Steps: 63 | Train Loss: 0.3990450 Vali Loss: 0.9258503 Test Loss: 0.4131489
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.650423526763916
Epoch: 76, Steps: 63 | Train Loss: 0.3987630 Vali Loss: 0.9255574 Test Loss: 0.4131440
Validation loss decreased (0.925576 --> 0.925557).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.220341205596924
Epoch: 77, Steps: 63 | Train Loss: 0.3988340 Vali Loss: 0.9257965 Test Loss: 0.4131449
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.3920295238494873
Epoch: 78, Steps: 63 | Train Loss: 0.3990687 Vali Loss: 0.9257859 Test Loss: 0.4131487
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 3.089134454727173
Epoch: 79, Steps: 63 | Train Loss: 0.3987335 Vali Loss: 0.9258017 Test Loss: 0.4131571
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.8548576831817627
Epoch: 80, Steps: 63 | Train Loss: 0.3992797 Vali Loss: 0.9255096 Test Loss: 0.4131568
Validation loss decreased (0.925557 --> 0.925510).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.670828342437744
Epoch: 81, Steps: 63 | Train Loss: 0.3990237 Vali Loss: 0.9258777 Test Loss: 0.4131604
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.0411059856414795
Epoch: 82, Steps: 63 | Train Loss: 0.3988187 Vali Loss: 0.9249127 Test Loss: 0.4131557
Validation loss decreased (0.925510 --> 0.924913).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.4669442176818848
Epoch: 83, Steps: 63 | Train Loss: 0.3987271 Vali Loss: 0.9253861 Test Loss: 0.4131502
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.4984824657440186
Epoch: 84, Steps: 63 | Train Loss: 0.3990250 Vali Loss: 0.9255772 Test Loss: 0.4131537
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.368931770324707
Epoch: 85, Steps: 63 | Train Loss: 0.3989441 Vali Loss: 0.9252985 Test Loss: 0.4131514
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.6285197734832764
Epoch: 86, Steps: 63 | Train Loss: 0.3989945 Vali Loss: 0.9253831 Test Loss: 0.4131523
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.117244005203247
Epoch: 87, Steps: 63 | Train Loss: 0.3991345 Vali Loss: 0.9253860 Test Loss: 0.4131460
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.4820966720581055
Epoch: 88, Steps: 63 | Train Loss: 0.3990905 Vali Loss: 0.9255499 Test Loss: 0.4131513
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.9603569507598877
Epoch: 89, Steps: 63 | Train Loss: 0.3990246 Vali Loss: 0.9256911 Test Loss: 0.4131470
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.9132888317108154
Epoch: 90, Steps: 63 | Train Loss: 0.3989721 Vali Loss: 0.9250806 Test Loss: 0.4131508
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.5188589096069336
Epoch: 91, Steps: 63 | Train Loss: 0.3991722 Vali Loss: 0.9251577 Test Loss: 0.4131472
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.8792104721069336
Epoch: 92, Steps: 63 | Train Loss: 0.3990178 Vali Loss: 0.9257361 Test Loss: 0.4131522
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.0292282104492188
Epoch: 93, Steps: 63 | Train Loss: 0.3990013 Vali Loss: 0.9252061 Test Loss: 0.4131508
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.1750266551971436
Epoch: 94, Steps: 63 | Train Loss: 0.3990107 Vali Loss: 0.9256921 Test Loss: 0.4131524
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.8876612186431885
Epoch: 95, Steps: 63 | Train Loss: 0.3987249 Vali Loss: 0.9257194 Test Loss: 0.4131531
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.223191499710083
Epoch: 96, Steps: 63 | Train Loss: 0.3989141 Vali Loss: 0.9254731 Test Loss: 0.4131538
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.9780282974243164
Epoch: 97, Steps: 63 | Train Loss: 0.3989420 Vali Loss: 0.9257655 Test Loss: 0.4131522
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.1282007694244385
Epoch: 98, Steps: 63 | Train Loss: 0.3987498 Vali Loss: 0.9253495 Test Loss: 0.4131518
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.154552698135376
Epoch: 99, Steps: 63 | Train Loss: 0.3989997 Vali Loss: 0.9250450 Test Loss: 0.4131543
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.6985538005828857
Epoch: 100, Steps: 63 | Train Loss: 0.3987604 Vali Loss: 0.9253497 Test Loss: 0.4131504
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4090098738670349, mae:0.4170312285423279, rse:0.6073293089866638, corr:[0.2589574  0.26957962 0.26923493 0.2672285  0.26614913 0.26512948
 0.26367924 0.26265746 0.26217145 0.2623071  0.26254326 0.26233098
 0.26180857 0.26128298 0.26093403 0.2608089  0.2607703  0.26081103
 0.2608543  0.26072118 0.2605315  0.26051396 0.2607324  0.26100144
 0.26086944 0.26046112 0.26012716 0.25996584 0.25974932 0.25936797
 0.25881338 0.25826168 0.257964   0.2579708  0.25816712 0.258315
 0.25827911 0.25818235 0.25812742 0.25825778 0.25866473 0.2590187
 0.25908518 0.25887597 0.25862217 0.25849855 0.25856256 0.25845432
 0.2578329  0.25692096 0.25600022 0.2553508  0.2547868  0.25400704
 0.25332642 0.2528685  0.25258908 0.25247774 0.25237954 0.2523647
 0.25227737 0.25201437 0.25171238 0.2517033  0.25203726 0.25251573
 0.25279805 0.25259963 0.25238797 0.25252116 0.25278902 0.2525974
 0.25178036 0.2507407  0.25005752 0.24986322 0.24982779 0.24954484
 0.2490587  0.24855237 0.24826132 0.2481061  0.24792318 0.24773772
 0.24769875 0.24772625 0.24767062 0.2475398  0.24753204 0.24770692
 0.24774764 0.24744919 0.24706182 0.24695192 0.24730228 0.24793538
 0.24845065 0.24841718 0.24831481 0.24831618 0.2483436  0.24822725
 0.24794021 0.24770704 0.24762355 0.24762891 0.2475713  0.24734575
 0.24703313 0.24689081 0.2470017  0.24733166 0.24763156 0.24789551
 0.24801868 0.24783243 0.24741633 0.24703921 0.24693742 0.24705628
 0.2470149  0.2464323  0.24551359 0.24471907 0.24427366 0.2439584
 0.24375135 0.24357451 0.24332398 0.24297833 0.24273205 0.24262303
 0.24252531 0.24243969 0.24239126 0.24231306 0.24250951 0.24290487
 0.24327968 0.24330108 0.24303329 0.24276066 0.2426722  0.24268529
 0.24260433 0.24201857 0.24119447 0.24047913 0.24010965 0.23974612
 0.23941295 0.23925231 0.23921695 0.2393808  0.2395165  0.23958085
 0.23950654 0.23947082 0.23935215 0.23911862 0.23898342 0.23934017
 0.2398496  0.23982573 0.23934762 0.23887637 0.2388936  0.23926361
 0.23956253 0.23949802 0.23931077 0.23931675 0.23945571 0.23939668
 0.23907842 0.23882091 0.2387935  0.23908609 0.23935273 0.23938738
 0.23951375 0.23965715 0.23948704 0.2389964  0.23895936 0.24012399
 0.24106273 0.23986395 0.23758808 0.23788005 0.24032657 0.2344219 ]
