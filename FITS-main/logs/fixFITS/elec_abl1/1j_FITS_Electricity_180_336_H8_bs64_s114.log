Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_180_j336_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_180_j336_H8_FITS_custom_ftM_sl180_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17897
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=74, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  644588544.0
params:  15900.0
Trainable parameters:  15900
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5204044
	speed: 0.6553s/iter; left time: 9043.2210s
Epoch: 1 cost time: 89.49594163894653
Epoch: 1, Steps: 139 | Train Loss: 0.7443399 Vali Loss: 0.3579448 Test Loss: 0.3983658
Validation loss decreased (inf --> 0.357945).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2933443
	speed: 1.6294s/iter; left time: 22260.5658s
Epoch: 2 cost time: 90.38924431800842
Epoch: 2, Steps: 139 | Train Loss: 0.3287631 Vali Loss: 0.2384110 Test Loss: 0.2664615
Validation loss decreased (0.357945 --> 0.238411).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2398136
	speed: 1.4666s/iter; left time: 19832.7037s
Epoch: 3 cost time: 88.73561310768127
Epoch: 3, Steps: 139 | Train Loss: 0.2553822 Vali Loss: 0.2037208 Test Loss: 0.2307259
Validation loss decreased (0.238411 --> 0.203721).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2219654
	speed: 1.5859s/iter; left time: 21225.4187s
Epoch: 4 cost time: 89.45425724983215
Epoch: 4, Steps: 139 | Train Loss: 0.2282844 Vali Loss: 0.1865889 Test Loss: 0.2134779
Validation loss decreased (0.203721 --> 0.186589).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2157142
	speed: 1.5813s/iter; left time: 20944.9069s
Epoch: 5 cost time: 88.86703586578369
Epoch: 5, Steps: 139 | Train Loss: 0.2140612 Vali Loss: 0.1770884 Test Loss: 0.2038421
Validation loss decreased (0.186589 --> 0.177088).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2223848
	speed: 1.4894s/iter; left time: 19520.5926s
Epoch: 6 cost time: 89.34743857383728
Epoch: 6, Steps: 139 | Train Loss: 0.2061677 Vali Loss: 0.1716079 Test Loss: 0.1983243
Validation loss decreased (0.177088 --> 0.171608).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1998915
	speed: 1.2605s/iter; left time: 16345.2707s
Epoch: 7 cost time: 60.17904353141785
Epoch: 7, Steps: 139 | Train Loss: 0.2015556 Vali Loss: 0.1687861 Test Loss: 0.1950243
Validation loss decreased (0.171608 --> 0.168786).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1952805
	speed: 0.9987s/iter; left time: 12811.8858s
Epoch: 8 cost time: 56.88723015785217
Epoch: 8, Steps: 139 | Train Loss: 0.1989253 Vali Loss: 0.1669317 Test Loss: 0.1930004
Validation loss decreased (0.168786 --> 0.166932).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1893431
	speed: 1.0061s/iter; left time: 12766.6013s
Epoch: 9 cost time: 58.90306115150452
Epoch: 9, Steps: 139 | Train Loss: 0.1972103 Vali Loss: 0.1658311 Test Loss: 0.1917062
Validation loss decreased (0.166932 --> 0.165831).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1876329
	speed: 1.0394s/iter; left time: 13044.4459s
Epoch: 10 cost time: 58.62566900253296
Epoch: 10, Steps: 139 | Train Loss: 0.1961086 Vali Loss: 0.1648299 Test Loss: 0.1908584
Validation loss decreased (0.165831 --> 0.164830).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1908191
	speed: 1.0153s/iter; left time: 12600.8216s
Epoch: 11 cost time: 59.10583162307739
Epoch: 11, Steps: 139 | Train Loss: 0.1953573 Vali Loss: 0.1644712 Test Loss: 0.1901968
Validation loss decreased (0.164830 --> 0.164471).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1894275
	speed: 1.0053s/iter; left time: 12337.6436s
Epoch: 12 cost time: 56.544195890426636
Epoch: 12, Steps: 139 | Train Loss: 0.1948417 Vali Loss: 0.1640623 Test Loss: 0.1897337
Validation loss decreased (0.164471 --> 0.164062).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1907539
	speed: 1.3276s/iter; left time: 16108.1196s
Epoch: 13 cost time: 88.6569287776947
Epoch: 13, Steps: 139 | Train Loss: 0.1943416 Vali Loss: 0.1635475 Test Loss: 0.1893643
Validation loss decreased (0.164062 --> 0.163548).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1977568
	speed: 1.5807s/iter; left time: 18958.5247s
Epoch: 14 cost time: 90.28416061401367
Epoch: 14, Steps: 139 | Train Loss: 0.1940869 Vali Loss: 0.1633774 Test Loss: 0.1890643
Validation loss decreased (0.163548 --> 0.163377).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2044638
	speed: 1.2427s/iter; left time: 14731.7325s
Epoch: 15 cost time: 56.67353296279907
Epoch: 15, Steps: 139 | Train Loss: 0.1938032 Vali Loss: 0.1632708 Test Loss: 0.1888128
Validation loss decreased (0.163377 --> 0.163271).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1932945
	speed: 1.0299s/iter; left time: 12066.8038s
Epoch: 16 cost time: 60.02062106132507
Epoch: 16, Steps: 139 | Train Loss: 0.1936310 Vali Loss: 0.1627933 Test Loss: 0.1886190
Validation loss decreased (0.163271 --> 0.162793).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1934030
	speed: 1.0348s/iter; left time: 11980.1588s
Epoch: 17 cost time: 57.420475244522095
Epoch: 17, Steps: 139 | Train Loss: 0.1932471 Vali Loss: 0.1629006 Test Loss: 0.1884521
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1960250
	speed: 1.0067s/iter; left time: 11514.6009s
Epoch: 18 cost time: 61.625513553619385
Epoch: 18, Steps: 139 | Train Loss: 0.1932608 Vali Loss: 0.1626964 Test Loss: 0.1883119
Validation loss decreased (0.162793 --> 0.162696).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1924137
	speed: 0.9971s/iter; left time: 11265.7110s
Epoch: 19 cost time: 57.06536912918091
Epoch: 19, Steps: 139 | Train Loss: 0.1930748 Vali Loss: 0.1626133 Test Loss: 0.1881864
Validation loss decreased (0.162696 --> 0.162613).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1974045
	speed: 1.0401s/iter; left time: 11607.3064s
Epoch: 20 cost time: 58.89710259437561
Epoch: 20, Steps: 139 | Train Loss: 0.1929638 Vali Loss: 0.1622497 Test Loss: 0.1881004
Validation loss decreased (0.162613 --> 0.162250).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1975227
	speed: 1.0715s/iter; left time: 11809.4456s
Epoch: 21 cost time: 62.754974365234375
Epoch: 21, Steps: 139 | Train Loss: 0.1928481 Vali Loss: 0.1624165 Test Loss: 0.1880149
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1899294
	speed: 1.0521s/iter; left time: 11448.6858s
Epoch: 22 cost time: 59.42766308784485
Epoch: 22, Steps: 139 | Train Loss: 0.1927995 Vali Loss: 0.1623171 Test Loss: 0.1879345
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1960460
	speed: 1.0538s/iter; left time: 11320.6963s
Epoch: 23 cost time: 61.82045865058899
Epoch: 23, Steps: 139 | Train Loss: 0.1927686 Vali Loss: 0.1622725 Test Loss: 0.1878645
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1822652
	speed: 1.0980s/iter; left time: 11643.0656s
Epoch: 24 cost time: 59.00134825706482
Epoch: 24, Steps: 139 | Train Loss: 0.1925877 Vali Loss: 0.1621171 Test Loss: 0.1877975
Validation loss decreased (0.162250 --> 0.162117).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1861680
	speed: 1.0108s/iter; left time: 10578.3651s
Epoch: 25 cost time: 58.51899456977844
Epoch: 25, Steps: 139 | Train Loss: 0.1925413 Vali Loss: 0.1618064 Test Loss: 0.1877470
Validation loss decreased (0.162117 --> 0.161806).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1790928
	speed: 1.0177s/iter; left time: 10509.1185s
Epoch: 26 cost time: 55.748337507247925
Epoch: 26, Steps: 139 | Train Loss: 0.1925240 Vali Loss: 0.1620792 Test Loss: 0.1877008
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1997180
	speed: 0.9775s/iter; left time: 9957.7832s
Epoch: 27 cost time: 56.80122780799866
Epoch: 27, Steps: 139 | Train Loss: 0.1924871 Vali Loss: 0.1618585 Test Loss: 0.1876687
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1844167
	speed: 0.9542s/iter; left time: 9587.7314s
Epoch: 28 cost time: 56.26284193992615
Epoch: 28, Steps: 139 | Train Loss: 0.1924894 Vali Loss: 0.1617926 Test Loss: 0.1876495
Validation loss decreased (0.161806 --> 0.161793).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1925117
	speed: 0.9598s/iter; left time: 9510.8545s
Epoch: 29 cost time: 53.8200044631958
Epoch: 29, Steps: 139 | Train Loss: 0.1924499 Vali Loss: 0.1618190 Test Loss: 0.1876190
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1867527
	speed: 0.9960s/iter; left time: 9730.6971s
Epoch: 30 cost time: 61.349130630493164
Epoch: 30, Steps: 139 | Train Loss: 0.1924409 Vali Loss: 0.1616342 Test Loss: 0.1875936
Validation loss decreased (0.161793 --> 0.161634).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1899545
	speed: 1.0638s/iter; left time: 10245.7444s
Epoch: 31 cost time: 71.53707671165466
Epoch: 31, Steps: 139 | Train Loss: 0.1924170 Vali Loss: 0.1616536 Test Loss: 0.1875580
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1945891
	speed: 1.5032s/iter; left time: 14268.4794s
Epoch: 32 cost time: 84.46246266365051
Epoch: 32, Steps: 139 | Train Loss: 0.1923635 Vali Loss: 0.1616180 Test Loss: 0.1875483
Validation loss decreased (0.161634 --> 0.161618).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1867612
	speed: 1.4953s/iter; left time: 13985.9241s
Epoch: 33 cost time: 82.04182696342468
Epoch: 33, Steps: 139 | Train Loss: 0.1924095 Vali Loss: 0.1619153 Test Loss: 0.1875311
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1819504
	speed: 1.4861s/iter; left time: 13693.1030s
Epoch: 34 cost time: 85.12197184562683
Epoch: 34, Steps: 139 | Train Loss: 0.1923663 Vali Loss: 0.1616619 Test Loss: 0.1875159
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1982217
	speed: 1.4113s/iter; left time: 12807.5334s
Epoch: 35 cost time: 78.87224626541138
Epoch: 35, Steps: 139 | Train Loss: 0.1923764 Vali Loss: 0.1617019 Test Loss: 0.1874956
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2011711
	speed: 1.3337s/iter; left time: 11918.1960s
Epoch: 36 cost time: 75.3212902545929
Epoch: 36, Steps: 139 | Train Loss: 0.1921720 Vali Loss: 0.1617545 Test Loss: 0.1874910
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1793705
	speed: 1.2848s/iter; left time: 11302.6925s
Epoch: 37 cost time: 74.80912780761719
Epoch: 37, Steps: 139 | Train Loss: 0.1921604 Vali Loss: 0.1616098 Test Loss: 0.1874776
Validation loss decreased (0.161618 --> 0.161610).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1874945
	speed: 0.9720s/iter; left time: 8416.0047s
Epoch: 38 cost time: 43.51546359062195
Epoch: 38, Steps: 139 | Train Loss: 0.1922640 Vali Loss: 0.1617260 Test Loss: 0.1874777
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1940751
	speed: 0.9255s/iter; left time: 7884.7593s
Epoch: 39 cost time: 43.273927211761475
Epoch: 39, Steps: 139 | Train Loss: 0.1922284 Vali Loss: 0.1620241 Test Loss: 0.1874628
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1972372
	speed: 0.8938s/iter; left time: 7490.4041s
Epoch: 40 cost time: 67.39211106300354
Epoch: 40, Steps: 139 | Train Loss: 0.1921544 Vali Loss: 0.1618351 Test Loss: 0.1874509
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1990555
	speed: 1.4484s/iter; left time: 11936.5089s
Epoch: 41 cost time: 82.08595776557922
Epoch: 41, Steps: 139 | Train Loss: 0.1921911 Vali Loss: 0.1619083 Test Loss: 0.1874440
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1952941
	speed: 1.3901s/iter; left time: 11262.2824s
Epoch: 42 cost time: 75.66572070121765
Epoch: 42, Steps: 139 | Train Loss: 0.1921835 Vali Loss: 0.1617518 Test Loss: 0.1874389
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1857718
	speed: 1.4052s/iter; left time: 11189.5690s
Epoch: 43 cost time: 82.46495079994202
Epoch: 43, Steps: 139 | Train Loss: 0.1922319 Vali Loss: 0.1618986 Test Loss: 0.1874392
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1828081
	speed: 1.4443s/iter; left time: 11300.2711s
Epoch: 44 cost time: 81.63863039016724
Epoch: 44, Steps: 139 | Train Loss: 0.1922597 Vali Loss: 0.1619931 Test Loss: 0.1874254
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1933211
	speed: 1.4293s/iter; left time: 10983.9250s
Epoch: 45 cost time: 80.50883102416992
Epoch: 45, Steps: 139 | Train Loss: 0.1922595 Vali Loss: 0.1618655 Test Loss: 0.1874344
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1852817
	speed: 1.4026s/iter; left time: 10584.3649s
Epoch: 46 cost time: 79.78237652778625
Epoch: 46, Steps: 139 | Train Loss: 0.1921511 Vali Loss: 0.1617458 Test Loss: 0.1874182
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1922932
	speed: 1.4093s/iter; left time: 10438.3162s
Epoch: 47 cost time: 83.31512904167175
Epoch: 47, Steps: 139 | Train Loss: 0.1921912 Vali Loss: 0.1617524 Test Loss: 0.1874154
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1913832
	speed: 1.4530s/iter; left time: 10560.0738s
Epoch: 48 cost time: 79.60578227043152
Epoch: 48, Steps: 139 | Train Loss: 0.1921545 Vali Loss: 0.1617410 Test Loss: 0.1874092
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1944492
	speed: 1.2599s/iter; left time: 8981.8602s
Epoch: 49 cost time: 73.82563161849976
Epoch: 49, Steps: 139 | Train Loss: 0.1921894 Vali Loss: 0.1616617 Test Loss: 0.1874108
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2075332
	speed: 1.2696s/iter; left time: 8874.2262s
Epoch: 50 cost time: 71.93568515777588
Epoch: 50, Steps: 139 | Train Loss: 0.1921656 Vali Loss: 0.1616773 Test Loss: 0.1874124
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.1821470
	speed: 1.4677s/iter; left time: 10054.8997s
Epoch: 51 cost time: 82.90483927726746
Epoch: 51, Steps: 139 | Train Loss: 0.1921082 Vali Loss: 0.1617710 Test Loss: 0.1874027
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2001311
	speed: 1.3621s/iter; left time: 9142.5089s
Epoch: 52 cost time: 76.30684351921082
Epoch: 52, Steps: 139 | Train Loss: 0.1921956 Vali Loss: 0.1616853 Test Loss: 0.1874040
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.1831071
	speed: 1.0808s/iter; left time: 7103.8656s
Epoch: 53 cost time: 60.864415645599365
Epoch: 53, Steps: 139 | Train Loss: 0.1921687 Vali Loss: 0.1616798 Test Loss: 0.1873990
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.1987705
	speed: 0.9876s/iter; left time: 6354.4169s
Epoch: 54 cost time: 53.23502326011658
Epoch: 54, Steps: 139 | Train Loss: 0.1921677 Vali Loss: 0.1616691 Test Loss: 0.1873954
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.1813073
	speed: 0.9133s/iter; left time: 5749.2416s
Epoch: 55 cost time: 53.9162917137146
Epoch: 55, Steps: 139 | Train Loss: 0.1922181 Vali Loss: 0.1618593 Test Loss: 0.1873912
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.1908474
	speed: 0.9552s/iter; left time: 5880.2151s
Epoch: 56 cost time: 54.67180156707764
Epoch: 56, Steps: 139 | Train Loss: 0.1921879 Vali Loss: 0.1617221 Test Loss: 0.1873957
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.1848807
	speed: 0.9506s/iter; left time: 5719.5183s
Epoch: 57 cost time: 50.816444873809814
Epoch: 57, Steps: 139 | Train Loss: 0.1921971 Vali Loss: 0.1618940 Test Loss: 0.1873958
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_180_j336_H8_FITS_custom_ftM_sl180_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.18552972376346588, mae:0.27781394124031067, rse:0.42869463562965393, corr:[0.4592486  0.46067584 0.4612433  0.4610295  0.46173784 0.461572
 0.46159732 0.46160248 0.46103477 0.46102846 0.46071425 0.46030033
 0.46037582 0.46005023 0.46023804 0.460048   0.46007296 0.46012697
 0.45994127 0.45996806 0.45987546 0.45983288 0.4605165  0.46046638
 0.46004233 0.46002078 0.45974118 0.45977268 0.4596506  0.45945457
 0.45935944 0.45918423 0.45898536 0.45890447 0.4587628  0.45861495
 0.45854765 0.45844322 0.45853356 0.45834628 0.4583809  0.4583737
 0.4581837  0.45814246 0.45805317 0.45793003 0.45837212 0.45841846
 0.45813578 0.45826232 0.45803317 0.45793518 0.45787916 0.4577821
 0.4577569  0.45775023 0.45765623 0.45763317 0.45760745 0.45753646
 0.45763573 0.45752215 0.45755973 0.4574628  0.45740753 0.4573901
 0.45721507 0.45702484 0.45694944 0.4568423  0.45703325 0.4570847
 0.45676616 0.45684773 0.45677188 0.45672217 0.45668656 0.45663837
 0.4566053  0.4565354  0.45642695 0.4563991  0.45646536 0.45636874
 0.456384   0.4562861  0.45635435 0.45630237 0.4562775  0.45629644
 0.45618838 0.45615938 0.45625865 0.45623428 0.45641267 0.4565907
 0.45629624 0.45631534 0.4562716  0.4561964  0.4561662  0.4561277
 0.4560918  0.45610783 0.4561284  0.4560932  0.45613495 0.45607778
 0.45613304 0.45598754 0.45600802 0.4560408  0.455967   0.45596278
 0.45590067 0.45574316 0.45565176 0.45558187 0.45563874 0.45586395
 0.45568308 0.45566905 0.45573905 0.45581588 0.45586875 0.45592162
 0.45598686 0.45600352 0.45604214 0.4560331  0.4560826  0.45609114
 0.4561998  0.45614275 0.45610777 0.45616847 0.45607626 0.45609096
 0.4560506  0.45583513 0.45584446 0.4559376  0.45611086 0.4563671
 0.45624992 0.45625946 0.4563599  0.4563724  0.45639175 0.45648012
 0.4564957  0.4565656  0.45665726 0.4567044  0.4568046  0.4568246
 0.45711485 0.4573318  0.45718768 0.45722008 0.45725808 0.45738846
 0.45755503 0.45751524 0.45771736 0.45807996 0.45863292 0.45843634
 0.45774215 0.45715386 0.45677486 0.45641172 0.4560495  0.45587596
 0.4557006  0.4555079  0.4553333  0.45506603 0.4548412  0.45464247
 0.45457113 0.45438212 0.4542483  0.45420486 0.45412222 0.4540403
 0.45405924 0.45396376 0.4539293  0.4541139  0.45431873 0.4540968
 0.45380515 0.45359305 0.4533189  0.4532603  0.45305946 0.45299
 0.4529088  0.4527234  0.45261848 0.452434   0.45233902 0.45222613
 0.45222345 0.45220715 0.45225742 0.45225647 0.45236033 0.45236018
 0.45231932 0.45226282 0.45220056 0.45227882 0.4524215  0.4523775
 0.4522463  0.45220214 0.45202407 0.4520206  0.4518929  0.45184305
 0.4518212  0.4516902  0.4516895  0.4516906  0.45166677 0.45161876
 0.45168552 0.4516571  0.451684   0.45160022 0.45164534 0.45164913
 0.45158026 0.45150146 0.45141634 0.4513695  0.45154905 0.45155635
 0.4513086  0.45139098 0.45121515 0.45120353 0.45124832 0.45118845
 0.45119366 0.45114312 0.4510488  0.45102325 0.45106703 0.451026
 0.45108706 0.45098367 0.45098844 0.45095623 0.45108128 0.45111528
 0.4510565  0.4511377  0.45111784 0.45109087 0.45132127 0.45143965
 0.45115837 0.45121622 0.45112327 0.45110497 0.45109034 0.45103896
 0.45103553 0.4509682  0.45094866 0.4509335  0.45089716 0.45084518
 0.45093802 0.45080248 0.45093566 0.4509155  0.45089543 0.45104784
 0.45094788 0.45073494 0.45064422 0.45061553 0.45072535 0.45090103
 0.45061785 0.45073584 0.45074022 0.45075774 0.4508697  0.4508993
 0.45091495 0.45080814 0.45083106 0.4508829  0.4509614  0.45103985
 0.4512846  0.4512638  0.4513456  0.45140547 0.45130745 0.45148706
 0.45137936 0.4510883  0.4510585  0.45099232 0.45111755 0.45135418
 0.45104846 0.45119894 0.45129415 0.45120338 0.45120707 0.45128566
 0.45134667 0.4512485  0.45141804 0.45152155 0.45185733 0.45218647
 0.4527393  0.45304912 0.45314738 0.45314685 0.45286146 0.45323223
 0.45272404 0.45271295 0.45296118 0.4530666  0.4537815  0.45304245]
