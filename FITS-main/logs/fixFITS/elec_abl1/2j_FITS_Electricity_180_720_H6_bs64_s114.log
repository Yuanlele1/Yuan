Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_180_j720_H6', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_180_j720_H6_FITS_custom_ftM_sl180_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17513
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=58, out_features=290, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  691100160.0
params:  17110.0
Trainable parameters:  17110
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8063915
	speed: 0.9439s/iter; left time: 12744.0334s
Epoch: 1 cost time: 127.55595064163208
Epoch: 1, Steps: 136 | Train Loss: 1.1623042 Vali Loss: 0.6363435 Test Loss: 0.6996897
Validation loss decreased (inf --> 0.636343).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4845050
	speed: 2.2081s/iter; left time: 29510.6365s
Epoch: 2 cost time: 129.04378032684326
Epoch: 2, Steps: 136 | Train Loss: 0.5458368 Vali Loss: 0.4171279 Test Loss: 0.4629999
Validation loss decreased (0.636343 --> 0.417128).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3681683
	speed: 2.2020s/iter; left time: 29130.7372s
Epoch: 3 cost time: 124.60058403015137
Epoch: 3, Steps: 136 | Train Loss: 0.3922572 Vali Loss: 0.3370808 Test Loss: 0.3742638
Validation loss decreased (0.417128 --> 0.337081).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3091786
	speed: 1.9520s/iter; left time: 25557.9208s
Epoch: 4 cost time: 115.40543532371521
Epoch: 4, Steps: 136 | Train Loss: 0.3301308 Vali Loss: 0.2988656 Test Loss: 0.3332402
Validation loss decreased (0.337081 --> 0.298866).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2965471
	speed: 2.2299s/iter; left time: 28893.4538s
Epoch: 5 cost time: 132.40240049362183
Epoch: 5, Steps: 136 | Train Loss: 0.2979964 Vali Loss: 0.2764221 Test Loss: 0.3102645
Validation loss decreased (0.298866 --> 0.276422).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2710862
	speed: 2.1786s/iter; left time: 27931.4395s
Epoch: 6 cost time: 122.18949127197266
Epoch: 6, Steps: 136 | Train Loss: 0.2779649 Vali Loss: 0.2613781 Test Loss: 0.2953285
Validation loss decreased (0.276422 --> 0.261378).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2593356
	speed: 2.1380s/iter; left time: 27120.0146s
Epoch: 7 cost time: 123.72981715202332
Epoch: 7, Steps: 136 | Train Loss: 0.2640084 Vali Loss: 0.2505294 Test Loss: 0.2845372
Validation loss decreased (0.261378 --> 0.250529).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2493910
	speed: 2.1220s/iter; left time: 26629.4762s
Epoch: 8 cost time: 122.49601244926453
Epoch: 8, Steps: 136 | Train Loss: 0.2533608 Vali Loss: 0.2419838 Test Loss: 0.2762512
Validation loss decreased (0.250529 --> 0.241984).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2511561
	speed: 2.1462s/iter; left time: 26640.1627s
Epoch: 9 cost time: 125.7687087059021
Epoch: 9, Steps: 136 | Train Loss: 0.2450169 Vali Loss: 0.2350300 Test Loss: 0.2695683
Validation loss decreased (0.241984 --> 0.235030).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2387957
	speed: 2.1221s/iter; left time: 26052.8355s
Epoch: 10 cost time: 120.16974377632141
Epoch: 10, Steps: 136 | Train Loss: 0.2381487 Vali Loss: 0.2294152 Test Loss: 0.2642325
Validation loss decreased (0.235030 --> 0.229415).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2322356
	speed: 2.1403s/iter; left time: 25985.2483s
Epoch: 11 cost time: 123.59335470199585
Epoch: 11, Steps: 136 | Train Loss: 0.2326502 Vali Loss: 0.2245934 Test Loss: 0.2597612
Validation loss decreased (0.229415 --> 0.224593).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2357872
	speed: 2.1266s/iter; left time: 25529.6081s
Epoch: 12 cost time: 121.22961521148682
Epoch: 12, Steps: 136 | Train Loss: 0.2279491 Vali Loss: 0.2207160 Test Loss: 0.2560003
Validation loss decreased (0.224593 --> 0.220716).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2236776
	speed: 2.1629s/iter; left time: 25671.4937s
Epoch: 13 cost time: 122.96307277679443
Epoch: 13, Steps: 136 | Train Loss: 0.2239832 Vali Loss: 0.2173284 Test Loss: 0.2528313
Validation loss decreased (0.220716 --> 0.217328).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2067578
	speed: 2.0694s/iter; left time: 24280.0444s
Epoch: 14 cost time: 119.35006427764893
Epoch: 14, Steps: 136 | Train Loss: 0.2206766 Vali Loss: 0.2145897 Test Loss: 0.2500287
Validation loss decreased (0.217328 --> 0.214590).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2086217
	speed: 2.0803s/iter; left time: 24125.6943s
Epoch: 15 cost time: 121.78464865684509
Epoch: 15, Steps: 136 | Train Loss: 0.2178346 Vali Loss: 0.2123243 Test Loss: 0.2477005
Validation loss decreased (0.214590 --> 0.212324).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2072773
	speed: 2.1800s/iter; left time: 24985.3183s
Epoch: 16 cost time: 125.12297105789185
Epoch: 16, Steps: 136 | Train Loss: 0.2153710 Vali Loss: 0.2101692 Test Loss: 0.2456707
Validation loss decreased (0.212324 --> 0.210169).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2217201
	speed: 2.1304s/iter; left time: 24127.0611s
Epoch: 17 cost time: 120.44186663627625
Epoch: 17, Steps: 136 | Train Loss: 0.2132322 Vali Loss: 0.2086589 Test Loss: 0.2439240
Validation loss decreased (0.210169 --> 0.208659).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2110892
	speed: 2.0945s/iter; left time: 23435.3896s
Epoch: 18 cost time: 122.43146085739136
Epoch: 18, Steps: 136 | Train Loss: 0.2114469 Vali Loss: 0.2070085 Test Loss: 0.2423999
Validation loss decreased (0.208659 --> 0.207008).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2209016
	speed: 2.0892s/iter; left time: 23092.2394s
Epoch: 19 cost time: 121.72664833068848
Epoch: 19, Steps: 136 | Train Loss: 0.2098140 Vali Loss: 0.2056212 Test Loss: 0.2410755
Validation loss decreased (0.207008 --> 0.205621).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2108753
	speed: 2.0981s/iter; left time: 22905.2905s
Epoch: 20 cost time: 120.70488786697388
Epoch: 20, Steps: 136 | Train Loss: 0.2083924 Vali Loss: 0.2043243 Test Loss: 0.2398797
Validation loss decreased (0.205621 --> 0.204324).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2073058
	speed: 2.1363s/iter; left time: 23031.6284s
Epoch: 21 cost time: 122.32161331176758
Epoch: 21, Steps: 136 | Train Loss: 0.2071338 Vali Loss: 0.2033639 Test Loss: 0.2388104
Validation loss decreased (0.204324 --> 0.203364).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2070542
	speed: 2.1827s/iter; left time: 23235.1875s
Epoch: 22 cost time: 131.29037070274353
Epoch: 22, Steps: 136 | Train Loss: 0.2060336 Vali Loss: 0.2026678 Test Loss: 0.2378892
Validation loss decreased (0.203364 --> 0.202668).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2049349
	speed: 2.3049s/iter; left time: 24222.4606s
Epoch: 23 cost time: 141.18770694732666
Epoch: 23, Steps: 136 | Train Loss: 0.2050677 Vali Loss: 0.2017871 Test Loss: 0.2370598
Validation loss decreased (0.202668 --> 0.201787).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2031489
	speed: 2.4515s/iter; left time: 25429.6213s
Epoch: 24 cost time: 143.13906621932983
Epoch: 24, Steps: 136 | Train Loss: 0.2042801 Vali Loss: 0.2013361 Test Loss: 0.2363798
Validation loss decreased (0.201787 --> 0.201336).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2127303
	speed: 2.4642s/iter; left time: 25225.8231s
Epoch: 25 cost time: 144.4880268573761
Epoch: 25, Steps: 136 | Train Loss: 0.2035315 Vali Loss: 0.2006450 Test Loss: 0.2357030
Validation loss decreased (0.201336 --> 0.200645).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2052226
	speed: 2.2470s/iter; left time: 22696.9851s
Epoch: 26 cost time: 124.1422860622406
Epoch: 26, Steps: 136 | Train Loss: 0.2028196 Vali Loss: 0.2001667 Test Loss: 0.2351131
Validation loss decreased (0.200645 --> 0.200167).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2017998
	speed: 2.1427s/iter; left time: 21351.8058s
Epoch: 27 cost time: 124.71006774902344
Epoch: 27, Steps: 136 | Train Loss: 0.2022882 Vali Loss: 0.1995171 Test Loss: 0.2345995
Validation loss decreased (0.200167 --> 0.199517).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2002752
	speed: 2.0764s/iter; left time: 20408.9494s
Epoch: 28 cost time: 119.07509183883667
Epoch: 28, Steps: 136 | Train Loss: 0.2016242 Vali Loss: 0.1991388 Test Loss: 0.2341319
Validation loss decreased (0.199517 --> 0.199139).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2049221
	speed: 2.0599s/iter; left time: 19966.6054s
Epoch: 29 cost time: 118.37669110298157
Epoch: 29, Steps: 136 | Train Loss: 0.2012600 Vali Loss: 0.1987895 Test Loss: 0.2337405
Validation loss decreased (0.199139 --> 0.198789).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2071863
	speed: 2.0507s/iter; left time: 19598.3089s
Epoch: 30 cost time: 119.61311149597168
Epoch: 30, Steps: 136 | Train Loss: 0.2008869 Vali Loss: 0.1986526 Test Loss: 0.2333620
Validation loss decreased (0.198789 --> 0.198653).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1995150
	speed: 2.0163s/iter; left time: 18995.2779s
Epoch: 31 cost time: 113.9092767238617
Epoch: 31, Steps: 136 | Train Loss: 0.2005221 Vali Loss: 0.1985959 Test Loss: 0.2330056
Validation loss decreased (0.198653 --> 0.198596).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1994445
	speed: 2.0624s/iter; left time: 19149.3754s
Epoch: 32 cost time: 117.74565052986145
Epoch: 32, Steps: 136 | Train Loss: 0.2002316 Vali Loss: 0.1981762 Test Loss: 0.2327295
Validation loss decreased (0.198596 --> 0.198176).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1976099
	speed: 1.9756s/iter; left time: 18074.4491s
Epoch: 33 cost time: 116.08937621116638
Epoch: 33, Steps: 136 | Train Loss: 0.1999133 Vali Loss: 0.1978808 Test Loss: 0.2324207
Validation loss decreased (0.198176 --> 0.197881).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1992699
	speed: 1.9716s/iter; left time: 17770.2763s
Epoch: 34 cost time: 114.7543272972107
Epoch: 34, Steps: 136 | Train Loss: 0.1995413 Vali Loss: 0.1979303 Test Loss: 0.2321540
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1968943
	speed: 2.0248s/iter; left time: 17974.1434s
Epoch: 35 cost time: 114.62902760505676
Epoch: 35, Steps: 136 | Train Loss: 0.1994159 Vali Loss: 0.1978649 Test Loss: 0.2319269
Validation loss decreased (0.197881 --> 0.197865).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1912745
	speed: 1.8528s/iter; left time: 16195.1442s
Epoch: 36 cost time: 114.56857371330261
Epoch: 36, Steps: 136 | Train Loss: 0.1991218 Vali Loss: 0.1975842 Test Loss: 0.2317466
Validation loss decreased (0.197865 --> 0.197584).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2053232
	speed: 2.0349s/iter; left time: 17510.5733s
Epoch: 37 cost time: 118.10531973838806
Epoch: 37, Steps: 136 | Train Loss: 0.1989731 Vali Loss: 0.1975854 Test Loss: 0.2315735
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2009354
	speed: 2.0010s/iter; left time: 16946.6354s
Epoch: 38 cost time: 115.58087372779846
Epoch: 38, Steps: 136 | Train Loss: 0.1987296 Vali Loss: 0.1975842 Test Loss: 0.2313947
Validation loss decreased (0.197584 --> 0.197584).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2015786
	speed: 2.0441s/iter; left time: 17033.7940s
Epoch: 39 cost time: 115.40565538406372
Epoch: 39, Steps: 136 | Train Loss: 0.1986413 Vali Loss: 0.1974541 Test Loss: 0.2312447
Validation loss decreased (0.197584 --> 0.197454).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1960546
	speed: 2.0819s/iter; left time: 17065.3961s
Epoch: 40 cost time: 118.36374974250793
Epoch: 40, Steps: 136 | Train Loss: 0.1984903 Vali Loss: 0.1971673 Test Loss: 0.2310999
Validation loss decreased (0.197454 --> 0.197167).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1991045
	speed: 2.0290s/iter; left time: 16356.0795s
Epoch: 41 cost time: 116.79394698143005
Epoch: 41, Steps: 136 | Train Loss: 0.1983500 Vali Loss: 0.1969139 Test Loss: 0.2309610
Validation loss decreased (0.197167 --> 0.196914).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1930498
	speed: 2.0603s/iter; left time: 16327.6142s
Epoch: 42 cost time: 117.25238513946533
Epoch: 42, Steps: 136 | Train Loss: 0.1983036 Vali Loss: 0.1968732 Test Loss: 0.2308585
Validation loss decreased (0.196914 --> 0.196873).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1975089
	speed: 2.0803s/iter; left time: 16203.5003s
Epoch: 43 cost time: 121.96790885925293
Epoch: 43, Steps: 136 | Train Loss: 0.1981536 Vali Loss: 0.1968563 Test Loss: 0.2307352
Validation loss decreased (0.196873 --> 0.196856).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2035947
	speed: 1.9705s/iter; left time: 15079.8858s
Epoch: 44 cost time: 112.47823166847229
Epoch: 44, Steps: 136 | Train Loss: 0.1980673 Vali Loss: 0.1966937 Test Loss: 0.2306313
Validation loss decreased (0.196856 --> 0.196694).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1974179
	speed: 2.0870s/iter; left time: 15687.8216s
Epoch: 45 cost time: 118.94348692893982
Epoch: 45, Steps: 136 | Train Loss: 0.1979761 Vali Loss: 0.1968270 Test Loss: 0.2305496
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1943612
	speed: 2.0300s/iter; left time: 14983.1819s
Epoch: 46 cost time: 117.73827886581421
Epoch: 46, Steps: 136 | Train Loss: 0.1979117 Vali Loss: 0.1970086 Test Loss: 0.2304621
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2004705
	speed: 2.0566s/iter; left time: 14900.0616s
Epoch: 47 cost time: 114.41353511810303
Epoch: 47, Steps: 136 | Train Loss: 0.1977640 Vali Loss: 0.1967736 Test Loss: 0.2303852
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1957349
	speed: 2.0237s/iter; left time: 14386.8043s
Epoch: 48 cost time: 112.13862133026123
Epoch: 48, Steps: 136 | Train Loss: 0.1976929 Vali Loss: 0.1965090 Test Loss: 0.2303165
Validation loss decreased (0.196694 --> 0.196509).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1926221
	speed: 1.7847s/iter; left time: 12444.4650s
Epoch: 49 cost time: 99.30972838401794
Epoch: 49, Steps: 136 | Train Loss: 0.1976697 Vali Loss: 0.1965663 Test Loss: 0.2302450
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1934545
	speed: 1.7304s/iter; left time: 11831.0023s
Epoch: 50 cost time: 96.68580961227417
Epoch: 50, Steps: 136 | Train Loss: 0.1976393 Vali Loss: 0.1966744 Test Loss: 0.2301843
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.1999675
	speed: 1.8123s/iter; left time: 12144.1783s
Epoch: 51 cost time: 104.88292622566223
Epoch: 51, Steps: 136 | Train Loss: 0.1976132 Vali Loss: 0.1964464 Test Loss: 0.2301403
Validation loss decreased (0.196509 --> 0.196446).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.1918337
	speed: 1.8159s/iter; left time: 11921.3766s
Epoch: 52 cost time: 100.9298803806305
Epoch: 52, Steps: 136 | Train Loss: 0.1976030 Vali Loss: 0.1965576 Test Loss: 0.2300861
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2044997
	speed: 1.7515s/iter; left time: 11260.4625s
Epoch: 53 cost time: 95.87046384811401
Epoch: 53, Steps: 136 | Train Loss: 0.1975508 Vali Loss: 0.1966353 Test Loss: 0.2300487
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2064476
	speed: 1.7206s/iter; left time: 10827.9933s
Epoch: 54 cost time: 98.49703001976013
Epoch: 54, Steps: 136 | Train Loss: 0.1975188 Vali Loss: 0.1964022 Test Loss: 0.2300004
Validation loss decreased (0.196446 --> 0.196402).  Saving model ...
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2011520
	speed: 1.8239s/iter; left time: 11229.9247s
Epoch: 55 cost time: 99.89031839370728
Epoch: 55, Steps: 136 | Train Loss: 0.1974846 Vali Loss: 0.1962661 Test Loss: 0.2299678
Validation loss decreased (0.196402 --> 0.196266).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2111374
	speed: 1.7667s/iter; left time: 10637.1077s
Epoch: 56 cost time: 99.24897336959839
Epoch: 56, Steps: 136 | Train Loss: 0.1973907 Vali Loss: 0.1966586 Test Loss: 0.2299334
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2035767
	speed: 1.8172s/iter; left time: 10693.9415s
Epoch: 57 cost time: 104.86649346351624
Epoch: 57, Steps: 136 | Train Loss: 0.1973945 Vali Loss: 0.1965244 Test Loss: 0.2298872
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.1860232
	speed: 1.8070s/iter; left time: 10388.3433s
Epoch: 58 cost time: 100.60184383392334
Epoch: 58, Steps: 136 | Train Loss: 0.1974830 Vali Loss: 0.1963754 Test Loss: 0.2298585
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.1952524
	speed: 1.7761s/iter; left time: 9969.3947s
Epoch: 59 cost time: 98.77131581306458
Epoch: 59, Steps: 136 | Train Loss: 0.1974132 Vali Loss: 0.1965614 Test Loss: 0.2298297
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2000743
	speed: 1.7905s/iter; left time: 9806.5461s
Epoch: 60 cost time: 99.27634835243225
Epoch: 60, Steps: 136 | Train Loss: 0.1973697 Vali Loss: 0.1967731 Test Loss: 0.2298105
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.1977691
	speed: 1.8453s/iter; left time: 9855.4980s
Epoch: 61 cost time: 106.02232623100281
Epoch: 61, Steps: 136 | Train Loss: 0.1972853 Vali Loss: 0.1965696 Test Loss: 0.2297868
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.1976293
	speed: 1.6818s/iter; left time: 8753.5391s
Epoch: 62 cost time: 91.29740405082703
Epoch: 62, Steps: 136 | Train Loss: 0.1972263 Vali Loss: 0.1962157 Test Loss: 0.2297689
Validation loss decreased (0.196266 --> 0.196216).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.1950646
	speed: 2.0098s/iter; left time: 10187.8133s
Epoch: 63 cost time: 120.00080847740173
Epoch: 63, Steps: 136 | Train Loss: 0.1972725 Vali Loss: 0.1963709 Test Loss: 0.2297432
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.1906128
	speed: 2.0342s/iter; left time: 10034.8317s
Epoch: 64 cost time: 119.23430633544922
Epoch: 64, Steps: 136 | Train Loss: 0.1972439 Vali Loss: 0.1965145 Test Loss: 0.2297231
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.1978252
	speed: 2.0663s/iter; left time: 9911.9105s
Epoch: 65 cost time: 118.85007691383362
Epoch: 65, Steps: 136 | Train Loss: 0.1973322 Vali Loss: 0.1962603 Test Loss: 0.2297120
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.1866897
	speed: 1.9126s/iter; left time: 8914.4911s
Epoch: 66 cost time: 99.94313454627991
Epoch: 66, Steps: 136 | Train Loss: 0.1972829 Vali Loss: 0.1963877 Test Loss: 0.2296887
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2001046
	speed: 1.7838s/iter; left time: 8071.5555s
Epoch: 67 cost time: 100.49316763877869
Epoch: 67, Steps: 136 | Train Loss: 0.1972370 Vali Loss: 0.1962450 Test Loss: 0.2296749
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2044067
	speed: 1.7730s/iter; left time: 7781.7810s
Epoch: 68 cost time: 100.89242053031921
Epoch: 68, Steps: 136 | Train Loss: 0.1972287 Vali Loss: 0.1966442 Test Loss: 0.2296623
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2001359
	speed: 1.8116s/iter; left time: 7704.7346s
Epoch: 69 cost time: 107.12821459770203
Epoch: 69, Steps: 136 | Train Loss: 0.1972159 Vali Loss: 0.1968443 Test Loss: 0.2296502
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2084547
	speed: 1.8080s/iter; left time: 7443.5656s
Epoch: 70 cost time: 101.97726655006409
Epoch: 70, Steps: 136 | Train Loss: 0.1971818 Vali Loss: 0.1967852 Test Loss: 0.2296362
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.1978135
	speed: 1.7060s/iter; left time: 6791.6025s
Epoch: 71 cost time: 95.53711462020874
Epoch: 71, Steps: 136 | Train Loss: 0.1972075 Vali Loss: 0.1963240 Test Loss: 0.2296283
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.1903297
	speed: 1.6702s/iter; left time: 6422.0219s
Epoch: 72 cost time: 93.58655548095703
Epoch: 72, Steps: 136 | Train Loss: 0.1972100 Vali Loss: 0.1964604 Test Loss: 0.2296190
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2051542
	speed: 1.6955s/iter; left time: 6288.7166s
Epoch: 73 cost time: 99.83326578140259
Epoch: 73, Steps: 136 | Train Loss: 0.1971277 Vali Loss: 0.1962874 Test Loss: 0.2296133
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.1925355
	speed: 1.6226s/iter; left time: 5797.6646s
Epoch: 74 cost time: 88.81201934814453
Epoch: 74, Steps: 136 | Train Loss: 0.1971847 Vali Loss: 0.1965624 Test Loss: 0.2296020
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.1913252
	speed: 1.7264s/iter; left time: 5933.5451s
Epoch: 75 cost time: 97.13335371017456
Epoch: 75, Steps: 136 | Train Loss: 0.1972552 Vali Loss: 0.1963647 Test Loss: 0.2295938
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2065907
	speed: 1.7274s/iter; left time: 5702.1338s
Epoch: 76 cost time: 95.49439764022827
Epoch: 76, Steps: 136 | Train Loss: 0.1971445 Vali Loss: 0.1966788 Test Loss: 0.2295823
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.1926008
	speed: 1.5435s/iter; left time: 4885.1463s
Epoch: 77 cost time: 91.22486472129822
Epoch: 77, Steps: 136 | Train Loss: 0.1971667 Vali Loss: 0.1967032 Test Loss: 0.2295815
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.1955876
	speed: 1.6317s/iter; left time: 4942.5244s
Epoch: 78 cost time: 95.03071665763855
Epoch: 78, Steps: 136 | Train Loss: 0.1971578 Vali Loss: 0.1964968 Test Loss: 0.2295713
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.1852541
	speed: 1.7045s/iter; left time: 4931.1776s
Epoch: 79 cost time: 95.83687734603882
Epoch: 79, Steps: 136 | Train Loss: 0.1971152 Vali Loss: 0.1962715 Test Loss: 0.2295684
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.1915020
	speed: 1.6119s/iter; left time: 4443.9902s
Epoch: 80 cost time: 93.32212662696838
Epoch: 80, Steps: 136 | Train Loss: 0.1971699 Vali Loss: 0.1962800 Test Loss: 0.2295637
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.1923410
	speed: 1.6410s/iter; left time: 4300.9906s
Epoch: 81 cost time: 94.73657250404358
Epoch: 81, Steps: 136 | Train Loss: 0.1971425 Vali Loss: 0.1965070 Test Loss: 0.2295552
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.1880728
	speed: 1.6787s/iter; left time: 4171.5114s
Epoch: 82 cost time: 96.55117058753967
Epoch: 82, Steps: 136 | Train Loss: 0.1971883 Vali Loss: 0.1965954 Test Loss: 0.2295533
EarlyStopping counter: 20 out of 20
Early stopping
train 17513
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=58, out_features=290, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  691100160.0
params:  17110.0
Trainable parameters:  17110
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2363398
	speed: 0.6760s/iter; left time: 9126.4748s
Epoch: 1 cost time: 90.15309429168701
Epoch: 1, Steps: 136 | Train Loss: 0.2419368 Vali Loss: 0.1958853 Test Loss: 0.2288502
Validation loss decreased (inf --> 0.195885).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2380764
	speed: 1.4132s/iter; left time: 18887.1603s
Epoch: 2 cost time: 80.00292992591858
Epoch: 2, Steps: 136 | Train Loss: 0.2416123 Vali Loss: 0.1956486 Test Loss: 0.2287330
Validation loss decreased (0.195885 --> 0.195649).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2423836
	speed: 1.4625s/iter; left time: 19347.8558s
Epoch: 3 cost time: 84.18204998970032
Epoch: 3, Steps: 136 | Train Loss: 0.2414428 Vali Loss: 0.1958266 Test Loss: 0.2286297
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2553312
	speed: 1.4449s/iter; left time: 18917.4664s
Epoch: 4 cost time: 84.38871693611145
Epoch: 4, Steps: 136 | Train Loss: 0.2413720 Vali Loss: 0.1956282 Test Loss: 0.2286871
Validation loss decreased (0.195649 --> 0.195628).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2570745
	speed: 1.5089s/iter; left time: 19550.5125s
Epoch: 5 cost time: 87.87927675247192
Epoch: 5, Steps: 136 | Train Loss: 0.2413608 Vali Loss: 0.1956323 Test Loss: 0.2287216
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2463418
	speed: 1.5257s/iter; left time: 19561.2217s
Epoch: 6 cost time: 89.2738893032074
Epoch: 6, Steps: 136 | Train Loss: 0.2414351 Vali Loss: 0.1957115 Test Loss: 0.2287076
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2382878
	speed: 1.2705s/iter; left time: 16115.6979s
Epoch: 7 cost time: 63.5015344619751
Epoch: 7, Steps: 136 | Train Loss: 0.2413337 Vali Loss: 0.1954815 Test Loss: 0.2286464
Validation loss decreased (0.195628 --> 0.195482).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2404421
	speed: 1.0831s/iter; left time: 13591.8506s
Epoch: 8 cost time: 62.412378787994385
Epoch: 8, Steps: 136 | Train Loss: 0.2413982 Vali Loss: 0.1957927 Test Loss: 0.2286013
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2399186
	speed: 1.0780s/iter; left time: 13380.6681s
Epoch: 9 cost time: 61.82070231437683
Epoch: 9, Steps: 136 | Train Loss: 0.2414136 Vali Loss: 0.1959307 Test Loss: 0.2286378
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2527486
	speed: 1.0663s/iter; left time: 13091.3475s
Epoch: 10 cost time: 60.45842695236206
Epoch: 10, Steps: 136 | Train Loss: 0.2413645 Vali Loss: 0.1954037 Test Loss: 0.2286393
Validation loss decreased (0.195482 --> 0.195404).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2446970
	speed: 1.0746s/iter; left time: 13047.0784s
Epoch: 11 cost time: 62.284443855285645
Epoch: 11, Steps: 136 | Train Loss: 0.2411919 Vali Loss: 0.1955645 Test Loss: 0.2286446
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2302980
	speed: 1.0708s/iter; left time: 12855.2042s
Epoch: 12 cost time: 62.04246401786804
Epoch: 12, Steps: 136 | Train Loss: 0.2413116 Vali Loss: 0.1954548 Test Loss: 0.2286294
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2350433
	speed: 1.0676s/iter; left time: 12670.7687s
Epoch: 13 cost time: 62.248650789260864
Epoch: 13, Steps: 136 | Train Loss: 0.2412766 Vali Loss: 0.1954375 Test Loss: 0.2286313
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2404367
	speed: 1.0921s/iter; left time: 12813.2545s
Epoch: 14 cost time: 63.98559641838074
Epoch: 14, Steps: 136 | Train Loss: 0.2411733 Vali Loss: 0.1954952 Test Loss: 0.2285705
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2305725
	speed: 1.0942s/iter; left time: 12689.8655s
Epoch: 15 cost time: 63.70949172973633
Epoch: 15, Steps: 136 | Train Loss: 0.2412500 Vali Loss: 0.1952149 Test Loss: 0.2286846
Validation loss decreased (0.195404 --> 0.195215).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2344326
	speed: 1.1107s/iter; left time: 12729.8442s
Epoch: 16 cost time: 62.85986351966858
Epoch: 16, Steps: 136 | Train Loss: 0.2412165 Vali Loss: 0.1953246 Test Loss: 0.2286255
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2359670
	speed: 1.0878s/iter; left time: 12319.3784s
Epoch: 17 cost time: 62.30065584182739
Epoch: 17, Steps: 136 | Train Loss: 0.2412206 Vali Loss: 0.1950261 Test Loss: 0.2285903
Validation loss decreased (0.195215 --> 0.195026).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2448417
	speed: 1.1487s/iter; left time: 12852.3586s
Epoch: 18 cost time: 66.99495267868042
Epoch: 18, Steps: 136 | Train Loss: 0.2412186 Vali Loss: 0.1951655 Test Loss: 0.2286056
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2454417
	speed: 1.1042s/iter; left time: 12205.2464s
Epoch: 19 cost time: 60.764458417892456
Epoch: 19, Steps: 136 | Train Loss: 0.2412891 Vali Loss: 0.1951191 Test Loss: 0.2286155
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2436250
	speed: 1.1416s/iter; left time: 12462.6513s
Epoch: 20 cost time: 66.36607027053833
Epoch: 20, Steps: 136 | Train Loss: 0.2413145 Vali Loss: 0.1952481 Test Loss: 0.2286367
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2305232
	speed: 1.1291s/iter; left time: 12173.1122s
Epoch: 21 cost time: 66.25296425819397
Epoch: 21, Steps: 136 | Train Loss: 0.2412221 Vali Loss: 0.1952902 Test Loss: 0.2286209
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2473128
	speed: 1.2119s/iter; left time: 12900.3173s
Epoch: 22 cost time: 69.20634746551514
Epoch: 22, Steps: 136 | Train Loss: 0.2411447 Vali Loss: 0.1953162 Test Loss: 0.2286363
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2408090
	speed: 1.1505s/iter; left time: 12090.9729s
Epoch: 23 cost time: 67.50170922279358
Epoch: 23, Steps: 136 | Train Loss: 0.2411511 Vali Loss: 0.1953922 Test Loss: 0.2285857
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2452197
	speed: 1.1759s/iter; left time: 12197.2128s
Epoch: 24 cost time: 66.90681195259094
Epoch: 24, Steps: 136 | Train Loss: 0.2411294 Vali Loss: 0.1950639 Test Loss: 0.2285771
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2635516
	speed: 1.1690s/iter; left time: 11967.3341s
Epoch: 25 cost time: 67.34653306007385
Epoch: 25, Steps: 136 | Train Loss: 0.2410856 Vali Loss: 0.1954554 Test Loss: 0.2286008
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2322116
	speed: 1.1840s/iter; left time: 11959.4011s
Epoch: 26 cost time: 70.12449669837952
Epoch: 26, Steps: 136 | Train Loss: 0.2412312 Vali Loss: 0.1952832 Test Loss: 0.2286165
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2313714
	speed: 1.2000s/iter; left time: 11957.8413s
Epoch: 27 cost time: 69.03011965751648
Epoch: 27, Steps: 136 | Train Loss: 0.2412280 Vali Loss: 0.1952048 Test Loss: 0.2285943
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2423982
	speed: 1.1823s/iter; left time: 11620.3386s
Epoch: 28 cost time: 70.67223453521729
Epoch: 28, Steps: 136 | Train Loss: 0.2411951 Vali Loss: 0.1953968 Test Loss: 0.2286127
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2354649
	speed: 1.1779s/iter; left time: 11417.7598s
Epoch: 29 cost time: 67.93219447135925
Epoch: 29, Steps: 136 | Train Loss: 0.2411350 Vali Loss: 0.1953026 Test Loss: 0.2285907
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2390293
	speed: 1.1936s/iter; left time: 11407.1642s
Epoch: 30 cost time: 67.67990756034851
Epoch: 30, Steps: 136 | Train Loss: 0.2412132 Vali Loss: 0.1952627 Test Loss: 0.2285994
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2382219
	speed: 1.1577s/iter; left time: 10906.3664s
Epoch: 31 cost time: 66.04783010482788
Epoch: 31, Steps: 136 | Train Loss: 0.2411271 Vali Loss: 0.1953438 Test Loss: 0.2285690
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2396761
	speed: 1.1483s/iter; left time: 10661.9848s
Epoch: 32 cost time: 63.999836921691895
Epoch: 32, Steps: 136 | Train Loss: 0.2411967 Vali Loss: 0.1954199 Test Loss: 0.2285748
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2420845
	speed: 1.1658s/iter; left time: 10666.2250s
Epoch: 33 cost time: 67.56570816040039
Epoch: 33, Steps: 136 | Train Loss: 0.2411843 Vali Loss: 0.1953956 Test Loss: 0.2286055
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2399061
	speed: 0.9672s/iter; left time: 8717.1294s
Epoch: 34 cost time: 49.12540316581726
Epoch: 34, Steps: 136 | Train Loss: 0.2410952 Vali Loss: 0.1953425 Test Loss: 0.2285873
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2599074
	speed: 0.8089s/iter; left time: 7180.9160s
Epoch: 35 cost time: 47.33702802658081
Epoch: 35, Steps: 136 | Train Loss: 0.2411495 Vali Loss: 0.1954211 Test Loss: 0.2285865
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2430442
	speed: 0.8683s/iter; left time: 7589.6559s
Epoch: 36 cost time: 49.381232023239136
Epoch: 36, Steps: 136 | Train Loss: 0.2410488 Vali Loss: 0.1952059 Test Loss: 0.2285987
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2311091
	speed: 0.8037s/iter; left time: 6916.2299s
Epoch: 37 cost time: 45.786295890808105
Epoch: 37, Steps: 136 | Train Loss: 0.2411303 Vali Loss: 0.1947738 Test Loss: 0.2286109
Validation loss decreased (0.195026 --> 0.194774).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2571227
	speed: 0.8371s/iter; left time: 7089.4745s
Epoch: 38 cost time: 49.9983332157135
Epoch: 38, Steps: 136 | Train Loss: 0.2411024 Vali Loss: 0.1952157 Test Loss: 0.2285845
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2341793
	speed: 0.9010s/iter; left time: 7507.8582s
Epoch: 39 cost time: 49.152265310287476
Epoch: 39, Steps: 136 | Train Loss: 0.2411104 Vali Loss: 0.1953113 Test Loss: 0.2285821
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2387717
	speed: 0.8808s/iter; left time: 7219.9826s
Epoch: 40 cost time: 50.686378717422485
Epoch: 40, Steps: 136 | Train Loss: 0.2411369 Vali Loss: 0.1953269 Test Loss: 0.2285752
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2333983
	speed: 0.8837s/iter; left time: 7123.1232s
Epoch: 41 cost time: 50.26752424240112
Epoch: 41, Steps: 136 | Train Loss: 0.2411750 Vali Loss: 0.1951237 Test Loss: 0.2285958
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2384053
	speed: 0.8560s/iter; left time: 6783.6384s
Epoch: 42 cost time: 49.03579378128052
Epoch: 42, Steps: 136 | Train Loss: 0.2411857 Vali Loss: 0.1953416 Test Loss: 0.2285835
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2376396
	speed: 0.8774s/iter; left time: 6833.8930s
Epoch: 43 cost time: 49.720048904418945
Epoch: 43, Steps: 136 | Train Loss: 0.2410864 Vali Loss: 0.1953350 Test Loss: 0.2285748
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2326646
	speed: 0.8695s/iter; left time: 6654.3744s
Epoch: 44 cost time: 48.810038328170776
Epoch: 44, Steps: 136 | Train Loss: 0.2411799 Vali Loss: 0.1952326 Test Loss: 0.2285855
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2308090
	speed: 0.8830s/iter; left time: 6637.3077s
Epoch: 45 cost time: 50.70222187042236
Epoch: 45, Steps: 136 | Train Loss: 0.2411730 Vali Loss: 0.1951555 Test Loss: 0.2285813
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2219215
	speed: 0.8664s/iter; left time: 6394.9628s
Epoch: 46 cost time: 49.43168020248413
Epoch: 46, Steps: 136 | Train Loss: 0.2411424 Vali Loss: 0.1951355 Test Loss: 0.2285862
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2325999
	speed: 0.8680s/iter; left time: 6288.5335s
Epoch: 47 cost time: 48.63547682762146
Epoch: 47, Steps: 136 | Train Loss: 0.2411859 Vali Loss: 0.1949403 Test Loss: 0.2285841
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2296547
	speed: 0.8866s/iter; left time: 6302.6140s
Epoch: 48 cost time: 49.42013907432556
Epoch: 48, Steps: 136 | Train Loss: 0.2411010 Vali Loss: 0.1951130 Test Loss: 0.2285852
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2421392
	speed: 0.8797s/iter; left time: 6133.9904s
Epoch: 49 cost time: 51.73260140419006
Epoch: 49, Steps: 136 | Train Loss: 0.2411736 Vali Loss: 0.1948606 Test Loss: 0.2285875
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2469939
	speed: 0.8700s/iter; left time: 5947.9336s
Epoch: 50 cost time: 49.02894735336304
Epoch: 50, Steps: 136 | Train Loss: 0.2411814 Vali Loss: 0.1952316 Test Loss: 0.2285763
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2491475
	speed: 0.8713s/iter; left time: 5838.7180s
Epoch: 51 cost time: 49.65456557273865
Epoch: 51, Steps: 136 | Train Loss: 0.2410701 Vali Loss: 0.1950092 Test Loss: 0.2285690
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2431091
	speed: 0.8729s/iter; left time: 5730.6461s
Epoch: 52 cost time: 49.27260375022888
Epoch: 52, Steps: 136 | Train Loss: 0.2410846 Vali Loss: 0.1950509 Test Loss: 0.2285742
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2309316
	speed: 0.8787s/iter; left time: 5648.9531s
Epoch: 53 cost time: 49.76261568069458
Epoch: 53, Steps: 136 | Train Loss: 0.2411479 Vali Loss: 0.1950482 Test Loss: 0.2285728
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2371425
	speed: 0.8735s/iter; left time: 5496.7644s
Epoch: 54 cost time: 49.70589566230774
Epoch: 54, Steps: 136 | Train Loss: 0.2411003 Vali Loss: 0.1948123 Test Loss: 0.2285775
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2370726
	speed: 0.8550s/iter; left time: 5264.3053s
Epoch: 55 cost time: 47.99411201477051
Epoch: 55, Steps: 136 | Train Loss: 0.2410586 Vali Loss: 0.1951516 Test Loss: 0.2285762
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2450207
	speed: 0.8663s/iter; left time: 5215.7313s
Epoch: 56 cost time: 49.68658638000488
Epoch: 56, Steps: 136 | Train Loss: 0.2411418 Vali Loss: 0.1949931 Test Loss: 0.2285805
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2413235
	speed: 0.8759s/iter; left time: 5154.7234s
Epoch: 57 cost time: 48.98076605796814
Epoch: 57, Steps: 136 | Train Loss: 0.2412065 Vali Loss: 0.1951427 Test Loss: 0.2285763
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_180_j720_H6_FITS_custom_ftM_sl180_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.22701409459114075, mae:0.3125440776348114, rse:0.4752846658229828, corr:[0.44523302 0.44847506 0.44880638 0.4488777  0.44886357 0.44873095
 0.44882548 0.4485792  0.44840172 0.44819772 0.44773927 0.44759947
 0.44749102 0.4470886  0.44717234 0.44710445 0.4469474  0.44697797
 0.44701883 0.44706276 0.4470793  0.44729784 0.44775778 0.44755492
 0.44712806 0.447096   0.44690412 0.4468181  0.44675982 0.4465793
 0.44649437 0.44632912 0.44612592 0.44605845 0.44584727 0.44564086
 0.44571024 0.44555733 0.44554874 0.4455014  0.44541007 0.44543204
 0.4454102  0.4453462  0.44531226 0.44535145 0.44561416 0.44564566
 0.4454365  0.44548142 0.44534624 0.44517508 0.44512114 0.44502825
 0.44497332 0.4449041  0.4448382  0.44490233 0.44485313 0.4446688
 0.44472346 0.4445762  0.4445516  0.44458473 0.44447604 0.4444033
 0.44438392 0.44429073 0.44423422 0.44424874 0.4444296  0.44450352
 0.44430465 0.44428805 0.44422343 0.444133   0.4440752  0.44399178
 0.4439545  0.44389677 0.44379374 0.44380134 0.44384336 0.4437224
 0.44376364 0.44368666 0.44361144 0.44356617 0.44355664 0.44356292
 0.44354847 0.44342443 0.44333103 0.44341338 0.4435895  0.4435779
 0.4433323  0.44330046 0.44328594 0.4433064  0.44333148 0.44329962
 0.443289   0.44326743 0.4432097  0.44320467 0.44321418 0.44315088
 0.44320422 0.4431571  0.44314402 0.4430751  0.44300237 0.4429819
 0.44294658 0.4428324  0.44273132 0.44274527 0.44292018 0.44296855
 0.4428267  0.44293746 0.44298273 0.44300547 0.44302258 0.44300306
 0.44305897 0.44306648 0.44304228 0.44311374 0.4431581  0.44311884
 0.44318172 0.4431489  0.4431704  0.4431608  0.44306052 0.44301426
 0.44304705 0.44307807 0.4431105  0.4432597  0.44359174 0.4437373
 0.4436342  0.44374183 0.44380942 0.4438617  0.44394746 0.443976
 0.44400534 0.44401672 0.44404948 0.44413772 0.44416958 0.4442561
 0.44453752 0.44466278 0.44453007 0.44445595 0.4445046  0.4445902
 0.44478443 0.44496027 0.4450963  0.44538233 0.4457161  0.4452376
 0.44449985 0.444068   0.44377348 0.44349152 0.44321775 0.4429707
 0.44285476 0.4427504  0.44255632 0.4423835  0.44226202 0.4421371
 0.44215637 0.44202912 0.44196823 0.44197163 0.44199646 0.44205597
 0.44208312 0.44197622 0.44190016 0.44198298 0.44213554 0.44187254
 0.4414228  0.44126204 0.44110456 0.44094598 0.4408199  0.44069898
 0.44064337 0.44052914 0.4403613  0.44028282 0.44021094 0.4400803
 0.44015503 0.44011456 0.4401523  0.44018546 0.4401919  0.4402158
 0.44022405 0.44011566 0.43998528 0.4399324  0.4400408  0.44002697
 0.43985686 0.43984857 0.43972224 0.4395923  0.43951774 0.43942267
 0.4393884  0.439354   0.43929535 0.439295   0.43932304 0.4392954
 0.4393654  0.43926764 0.43927848 0.43930972 0.43928042 0.4392564
 0.43925613 0.4391718  0.4391109  0.43910587 0.4392351  0.4391864
 0.43890136 0.43885648 0.43878627 0.4386583  0.43857306 0.43853077
 0.43856382 0.43853763 0.43844828 0.43844372 0.43846598 0.43840203
 0.43850467 0.43851382 0.43855318 0.43855143 0.43850198 0.4384962
 0.438465   0.43835565 0.4382992  0.43834278 0.43848476 0.4384661
 0.43823496 0.438217   0.4381744  0.43812373 0.43813318 0.43809617
 0.43805534 0.43802106 0.43795443 0.4379613  0.43798378 0.4379078
 0.43802983 0.43807867 0.43807977 0.43800756 0.43795627 0.43800265
 0.43802315 0.43787047 0.43773958 0.43776783 0.43794426 0.43799603
 0.43783316 0.43788588 0.4379009  0.4379404  0.43802822 0.43804455
 0.43802887 0.4380128  0.43802947 0.43807888 0.4380746  0.4380943
 0.43829525 0.4383525  0.43830246 0.43825683 0.4382439  0.4382239
 0.43824986 0.43834987 0.43845016 0.4385333  0.43874353 0.4388786
 0.43883026 0.43891764 0.43895304 0.43897647 0.43899047 0.43896696
 0.43901655 0.439094   0.4391756  0.43928576 0.43936908 0.4395043
 0.4397707  0.43991444 0.43979526 0.43973032 0.4398042  0.43986621
 0.43994176 0.4399867  0.44002768 0.44022796 0.44045952 0.43997318
 0.4393972  0.43911573 0.43878964 0.43847492 0.43824714 0.4380141
 0.437854   0.43771312 0.43750393 0.43732533 0.4371702  0.43707418
 0.43713114 0.43695095 0.43682376 0.43682602 0.43683794 0.43685108
 0.43688154 0.4368628  0.43685353 0.4369332  0.43709627 0.43690902
 0.43644223 0.43624067 0.4361454  0.43607488 0.43598732 0.4358961
 0.43584555 0.43570906 0.4354953  0.43537182 0.43527168 0.43510464
 0.4351401  0.4350714  0.43507445 0.43508467 0.43506214 0.43507048
 0.4350769  0.43498272 0.4349058  0.43494785 0.43510014 0.43509048
 0.43492544 0.43499333 0.43494868 0.4348064  0.4347466  0.4347468
 0.43471017 0.4345877  0.43446863 0.43443182 0.43439147 0.43432653
 0.434453   0.43443668 0.4344013  0.43434593 0.4343294  0.43438
 0.43439063 0.43430123 0.4342128  0.43420777 0.43438768 0.43448573
 0.43436658 0.43437538 0.43434653 0.43436983 0.43437254 0.43427128
 0.43426734 0.43426764 0.4341138  0.43404445 0.43409583 0.4340996
 0.43417117 0.4340457  0.434021   0.43406677 0.43402794 0.43401608
 0.4340795  0.4340321  0.43392557 0.43395102 0.43419328 0.4342352
 0.43399224 0.43403915 0.43409947 0.434056   0.4340217  0.43401518
 0.43404567 0.43400022 0.43387148 0.43386012 0.43387648 0.43376514
 0.4338086  0.43376464 0.43373188 0.4337469  0.43376654 0.4337684
 0.43369025 0.43356064 0.4335298  0.43356383 0.43373495 0.43388018
 0.43380103 0.4338789  0.4339525  0.43401885 0.43403816 0.43400607
 0.4340547  0.4341105  0.43404788 0.43398356 0.43402365 0.43407527
 0.4342074  0.43420863 0.4341881  0.43421325 0.4342492  0.43427783
 0.43431526 0.43432978 0.43437582 0.4345444  0.4347995  0.4348536
 0.43473372 0.43483588 0.43493998 0.43501022 0.43509173 0.43514714
 0.4351826  0.43520874 0.4352381  0.43524435 0.43533882 0.4356404
 0.43592933 0.43599442 0.43590158 0.43589717 0.43593276 0.43594423
 0.43605492 0.43611804 0.4360937  0.43624523 0.43650195 0.4359566
 0.43516967 0.43475735 0.43449095 0.43420795 0.4339218  0.43367553
 0.43353686 0.43335474 0.43308607 0.432908   0.43277666 0.43267632
 0.43279797 0.43272147 0.43260974 0.43257368 0.4326153  0.43268645
 0.43269327 0.43264025 0.43265197 0.4326892  0.43273985 0.4324363
 0.43191248 0.43170106 0.4315784  0.4314321  0.43127704 0.43109593
 0.43094793 0.43078    0.43055058 0.43042633 0.43036452 0.43021524
 0.43023294 0.43019786 0.43027565 0.43034002 0.4303449  0.4304333
 0.43053603 0.430473   0.43040317 0.43044204 0.43057093 0.4305136
 0.43023396 0.43009472 0.4299319  0.4297502  0.4296283  0.42953205
 0.42947254 0.42936426 0.42921352 0.42913538 0.4291048  0.42901903
 0.42910385 0.42908946 0.42908683 0.42903957 0.42896768 0.42891166
 0.4289181  0.42892158 0.42885303 0.42886484 0.42902428 0.42894176
 0.42867556 0.42867717 0.42865008 0.4285584  0.42848998 0.42845064
 0.42847693 0.42840767 0.4282349  0.42818314 0.42825335 0.42829123
 0.428407   0.4283438  0.42833868 0.42835048 0.42834038 0.42836434
 0.4283878  0.4283614  0.42833    0.42832667 0.42844772 0.42842636
 0.42813954 0.4280385  0.4280025  0.4279873  0.42797503 0.42793185
 0.42792097 0.42785186 0.427734   0.427792   0.42790145 0.427836
 0.42784977 0.42779887 0.42782065 0.42786318 0.42785862 0.4278868
 0.42786995 0.42778513 0.4277418  0.42566672 0.42785916 0.4237558
 0.42368248 0.42375758 0.42377147 0.42378595 0.42387024 0.4238979
 0.42393747 0.4239783  0.4239503  0.42397678 0.42409113 0.42413533
 0.42427143 0.4243346  0.42434147 0.4242929  0.424193   0.4241752
 0.42432097 0.42442042 0.42441753 0.4245065  0.4247099  0.42477214
 0.42466387 0.42477453 0.42490542 0.42495054 0.42499223 0.4250831
 0.42514467 0.42512423 0.42514846 0.4252782  0.42551005 0.4257825
 0.42604724 0.42622858 0.4261572  0.4261194  0.42622027 0.42628688
 0.42641348 0.42648375 0.42647803 0.42663294 0.42685637 0.42629406
 0.42558348 0.4251893  0.4248991  0.42467555 0.42443535 0.42419815
 0.42404148 0.42379996 0.42355168 0.42341208 0.4232487  0.4231314
 0.423167   0.42302337 0.4229357  0.4229447  0.4230106  0.4230403
 0.42302844 0.42301327 0.4229568  0.42296532 0.4231205  0.42281967
 0.42231098 0.4221745  0.42198133 0.42179543 0.42169413 0.42154175
 0.42147025 0.42132935 0.42104724 0.42094824 0.42094702 0.4208782
 0.420897   0.420812   0.42088285 0.42086983 0.42084163 0.4209295
 0.42088026 0.42066157 0.42043522 0.42062414 0.42091373 0.42101732]
