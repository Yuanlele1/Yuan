Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5469963264.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8881687
	speed: 0.9272s/iter; left time: 12147.1998s
Epoch: 1 cost time: 121.67239260673523
Epoch: 1, Steps: 132 | Train Loss: 1.0254582 Vali Loss: 0.7407526 Test Loss: 0.8706830
Validation loss decreased (inf --> 0.740753).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7258812
	speed: 2.0090s/iter; left time: 26054.1411s
Epoch: 2 cost time: 119.71106338500977
Epoch: 2, Steps: 132 | Train Loss: 0.7543619 Vali Loss: 0.6535943 Test Loss: 0.7718810
Validation loss decreased (0.740753 --> 0.653594).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6249641
	speed: 2.0194s/iter; left time: 25923.0930s
Epoch: 3 cost time: 120.50687766075134
Epoch: 3, Steps: 132 | Train Loss: 0.6610382 Vali Loss: 0.5887181 Test Loss: 0.6985050
Validation loss decreased (0.653594 --> 0.588718).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5791083
	speed: 1.8976s/iter; left time: 24108.9741s
Epoch: 4 cost time: 110.8510537147522
Epoch: 4, Steps: 132 | Train Loss: 0.5871813 Vali Loss: 0.5359467 Test Loss: 0.6387386
Validation loss decreased (0.588718 --> 0.535947).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5110307
	speed: 1.8484s/iter; left time: 23240.2832s
Epoch: 5 cost time: 113.73961591720581
Epoch: 5, Steps: 132 | Train Loss: 0.5256124 Vali Loss: 0.4878843 Test Loss: 0.5841045
Validation loss decreased (0.535947 --> 0.487884).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4695165
	speed: 1.9137s/iter; left time: 23808.5483s
Epoch: 6 cost time: 110.6750328540802
Epoch: 6, Steps: 132 | Train Loss: 0.4738088 Vali Loss: 0.4513641 Test Loss: 0.5426873
Validation loss decreased (0.487884 --> 0.451364).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4123043
	speed: 1.8720s/iter; left time: 23042.3946s
Epoch: 7 cost time: 115.64937973022461
Epoch: 7, Steps: 132 | Train Loss: 0.4296723 Vali Loss: 0.4163552 Test Loss: 0.5025160
Validation loss decreased (0.451364 --> 0.416355).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3903944
	speed: 1.8832s/iter; left time: 22931.4147s
Epoch: 8 cost time: 112.93317866325378
Epoch: 8, Steps: 132 | Train Loss: 0.3917566 Vali Loss: 0.3864928 Test Loss: 0.4679616
Validation loss decreased (0.416355 --> 0.386493).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3538339
	speed: 1.9360s/iter; left time: 23318.5375s
Epoch: 9 cost time: 118.4637188911438
Epoch: 9, Steps: 132 | Train Loss: 0.3591270 Vali Loss: 0.3623885 Test Loss: 0.4399734
Validation loss decreased (0.386493 --> 0.362388).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3291632
	speed: 2.1820s/iter; left time: 25993.7763s
Epoch: 10 cost time: 137.15859484672546
Epoch: 10, Steps: 132 | Train Loss: 0.3308020 Vali Loss: 0.3383028 Test Loss: 0.4121932
Validation loss decreased (0.362388 --> 0.338303).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3050695
	speed: 2.3499s/iter; left time: 27684.3476s
Epoch: 11 cost time: 144.14725494384766
Epoch: 11, Steps: 132 | Train Loss: 0.3062846 Vali Loss: 0.3199444 Test Loss: 0.3906647
Validation loss decreased (0.338303 --> 0.319944).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2730496
	speed: 2.3364s/iter; left time: 27216.2785s
Epoch: 12 cost time: 137.53311109542847
Epoch: 12, Steps: 132 | Train Loss: 0.2848707 Vali Loss: 0.3037711 Test Loss: 0.3716141
Validation loss decreased (0.319944 --> 0.303771).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2612461
	speed: 2.2571s/iter; left time: 25995.1224s
Epoch: 13 cost time: 134.41048169136047
Epoch: 13, Steps: 132 | Train Loss: 0.2661019 Vali Loss: 0.2888566 Test Loss: 0.3544261
Validation loss decreased (0.303771 --> 0.288857).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2420403
	speed: 2.1718s/iter; left time: 24726.0959s
Epoch: 14 cost time: 129.31575846672058
Epoch: 14, Steps: 132 | Train Loss: 0.2496354 Vali Loss: 0.2753852 Test Loss: 0.3380744
Validation loss decreased (0.288857 --> 0.275385).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2378599
	speed: 2.1353s/iter; left time: 24028.8384s
Epoch: 15 cost time: 127.56330299377441
Epoch: 15, Steps: 132 | Train Loss: 0.2352632 Vali Loss: 0.2641758 Test Loss: 0.3247435
Validation loss decreased (0.275385 --> 0.264176).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2181494
	speed: 2.2788s/iter; left time: 25342.1301s
Epoch: 16 cost time: 138.70800042152405
Epoch: 16, Steps: 132 | Train Loss: 0.2225193 Vali Loss: 0.2549800 Test Loss: 0.3132819
Validation loss decreased (0.264176 --> 0.254980).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2095164
	speed: 2.3502s/iter; left time: 25826.5857s
Epoch: 17 cost time: 137.49658799171448
Epoch: 17, Steps: 132 | Train Loss: 0.2112654 Vali Loss: 0.2458272 Test Loss: 0.3023930
Validation loss decreased (0.254980 --> 0.245827).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2030916
	speed: 2.2187s/iter; left time: 24088.2671s
Epoch: 18 cost time: 131.1911404132843
Epoch: 18, Steps: 132 | Train Loss: 0.2013856 Vali Loss: 0.2385864 Test Loss: 0.2934521
Validation loss decreased (0.245827 --> 0.238586).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1853322
	speed: 2.2291s/iter; left time: 23906.9412s
Epoch: 19 cost time: 131.72597241401672
Epoch: 19, Steps: 132 | Train Loss: 0.1925538 Vali Loss: 0.2316930 Test Loss: 0.2852114
Validation loss decreased (0.238586 --> 0.231693).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1812026
	speed: 2.1445s/iter; left time: 22716.9605s
Epoch: 20 cost time: 135.88212418556213
Epoch: 20, Steps: 132 | Train Loss: 0.1846930 Vali Loss: 0.2254851 Test Loss: 0.2773499
Validation loss decreased (0.231693 --> 0.225485).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1728708
	speed: 2.2625s/iter; left time: 23667.5711s
Epoch: 21 cost time: 138.62409687042236
Epoch: 21, Steps: 132 | Train Loss: 0.1778003 Vali Loss: 0.2207113 Test Loss: 0.2714751
Validation loss decreased (0.225485 --> 0.220711).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1649991
	speed: 2.1861s/iter; left time: 22580.0825s
Epoch: 22 cost time: 132.1815276145935
Epoch: 22, Steps: 132 | Train Loss: 0.1715488 Vali Loss: 0.2154440 Test Loss: 0.2646876
Validation loss decreased (0.220711 --> 0.215444).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1603063
	speed: 2.4616s/iter; left time: 25101.3370s
Epoch: 23 cost time: 159.76367044448853
Epoch: 23, Steps: 132 | Train Loss: 0.1661137 Vali Loss: 0.2114139 Test Loss: 0.2598076
Validation loss decreased (0.215444 --> 0.211414).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1618500
	speed: 2.8215s/iter; left time: 28398.8292s
Epoch: 24 cost time: 158.83773255348206
Epoch: 24, Steps: 132 | Train Loss: 0.1611429 Vali Loss: 0.2076335 Test Loss: 0.2549025
Validation loss decreased (0.211414 --> 0.207634).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1592322
	speed: 2.4993s/iter; left time: 24825.7373s
Epoch: 25 cost time: 150.37923669815063
Epoch: 25, Steps: 132 | Train Loss: 0.1567616 Vali Loss: 0.2044354 Test Loss: 0.2505450
Validation loss decreased (0.207634 --> 0.204435).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1491946
	speed: 2.4776s/iter; left time: 24283.2440s
Epoch: 26 cost time: 148.57534337043762
Epoch: 26, Steps: 132 | Train Loss: 0.1527880 Vali Loss: 0.2014169 Test Loss: 0.2467691
Validation loss decreased (0.204435 --> 0.201417).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1518914
	speed: 2.4781s/iter; left time: 23960.8147s
Epoch: 27 cost time: 149.7852680683136
Epoch: 27, Steps: 132 | Train Loss: 0.1491921 Vali Loss: 0.1989949 Test Loss: 0.2434583
Validation loss decreased (0.201417 --> 0.198995).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1406262
	speed: 2.4934s/iter; left time: 23779.3644s
Epoch: 28 cost time: 149.33985376358032
Epoch: 28, Steps: 132 | Train Loss: 0.1460518 Vali Loss: 0.1966628 Test Loss: 0.2401824
Validation loss decreased (0.198995 --> 0.196663).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1445037
	speed: 2.5184s/iter; left time: 23685.1937s
Epoch: 29 cost time: 154.54625415802002
Epoch: 29, Steps: 132 | Train Loss: 0.1432522 Vali Loss: 0.1945278 Test Loss: 0.2375702
Validation loss decreased (0.196663 --> 0.194528).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1407993
	speed: 2.5303s/iter; left time: 23463.2025s
Epoch: 30 cost time: 154.69521188735962
Epoch: 30, Steps: 132 | Train Loss: 0.1406287 Vali Loss: 0.1925607 Test Loss: 0.2348816
Validation loss decreased (0.194528 --> 0.192561).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1389445
	speed: 2.6998s/iter; left time: 24679.2353s
Epoch: 31 cost time: 164.14366507530212
Epoch: 31, Steps: 132 | Train Loss: 0.1383546 Vali Loss: 0.1911904 Test Loss: 0.2328951
Validation loss decreased (0.192561 --> 0.191190).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1278161
	speed: 2.3454s/iter; left time: 21130.0634s
Epoch: 32 cost time: 140.6595230102539
Epoch: 32, Steps: 132 | Train Loss: 0.1362294 Vali Loss: 0.1900443 Test Loss: 0.2308382
Validation loss decreased (0.191190 --> 0.190044).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1367583
	speed: 2.5106s/iter; left time: 22286.1891s
Epoch: 33 cost time: 148.67021107673645
Epoch: 33, Steps: 132 | Train Loss: 0.1343411 Vali Loss: 0.1883125 Test Loss: 0.2289298
Validation loss decreased (0.190044 --> 0.188312).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1333324
	speed: 2.4522s/iter; left time: 21444.2534s
Epoch: 34 cost time: 150.95579433441162
Epoch: 34, Steps: 132 | Train Loss: 0.1326875 Vali Loss: 0.1871089 Test Loss: 0.2269487
Validation loss decreased (0.188312 --> 0.187109).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1350367
	speed: 2.4807s/iter; left time: 21366.3941s
Epoch: 35 cost time: 152.01893424987793
Epoch: 35, Steps: 132 | Train Loss: 0.1311833 Vali Loss: 0.1859009 Test Loss: 0.2257189
Validation loss decreased (0.187109 --> 0.185901).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1297898
	speed: 2.4725s/iter; left time: 20968.9474s
Epoch: 36 cost time: 153.54741787910461
Epoch: 36, Steps: 132 | Train Loss: 0.1297580 Vali Loss: 0.1853526 Test Loss: 0.2243333
Validation loss decreased (0.185901 --> 0.185353).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1225378
	speed: 2.4395s/iter; left time: 20367.3839s
Epoch: 37 cost time: 146.09259581565857
Epoch: 37, Steps: 132 | Train Loss: 0.1285503 Vali Loss: 0.1842875 Test Loss: 0.2231695
Validation loss decreased (0.185353 --> 0.184288).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1295836
	speed: 2.4817s/iter; left time: 20392.4334s
Epoch: 38 cost time: 151.05635118484497
Epoch: 38, Steps: 132 | Train Loss: 0.1273678 Vali Loss: 0.1835368 Test Loss: 0.2219807
Validation loss decreased (0.184288 --> 0.183537).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1194746
	speed: 2.5581s/iter; left time: 20682.2889s
Epoch: 39 cost time: 156.3259699344635
Epoch: 39, Steps: 132 | Train Loss: 0.1263671 Vali Loss: 0.1827278 Test Loss: 0.2209037
Validation loss decreased (0.183537 --> 0.182728).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1275228
	speed: 2.5465s/iter; left time: 20252.0827s
Epoch: 40 cost time: 155.5944766998291
Epoch: 40, Steps: 132 | Train Loss: 0.1254120 Vali Loss: 0.1822606 Test Loss: 0.2199012
Validation loss decreased (0.182728 --> 0.182261).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1218195
	speed: 2.4615s/iter; left time: 19251.0960s
Epoch: 41 cost time: 154.37546730041504
Epoch: 41, Steps: 132 | Train Loss: 0.1245628 Vali Loss: 0.1818210 Test Loss: 0.2190411
Validation loss decreased (0.182261 --> 0.181821).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1190527
	speed: 2.5039s/iter; left time: 19252.1579s
Epoch: 42 cost time: 155.4211823940277
Epoch: 42, Steps: 132 | Train Loss: 0.1238096 Vali Loss: 0.1809514 Test Loss: 0.2182131
Validation loss decreased (0.181821 --> 0.180951).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1241301
	speed: 2.4726s/iter; left time: 18685.4209s
Epoch: 43 cost time: 149.355717420578
Epoch: 43, Steps: 132 | Train Loss: 0.1230497 Vali Loss: 0.1804691 Test Loss: 0.2174840
Validation loss decreased (0.180951 --> 0.180469).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1159543
	speed: 2.4907s/iter; left time: 18493.7619s
Epoch: 44 cost time: 152.4575560092926
Epoch: 44, Steps: 132 | Train Loss: 0.1224515 Vali Loss: 0.1804468 Test Loss: 0.2168271
Validation loss decreased (0.180469 --> 0.180447).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1251309
	speed: 2.5411s/iter; left time: 18531.9133s
Epoch: 45 cost time: 151.2597107887268
Epoch: 45, Steps: 132 | Train Loss: 0.1219271 Vali Loss: 0.1796863 Test Loss: 0.2163224
Validation loss decreased (0.180447 --> 0.179686).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1189718
	speed: 2.4959s/iter; left time: 17873.1582s
Epoch: 46 cost time: 153.16346883773804
Epoch: 46, Steps: 132 | Train Loss: 0.1213992 Vali Loss: 0.1796722 Test Loss: 0.2157005
Validation loss decreased (0.179686 --> 0.179672).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1249831
	speed: 2.5532s/iter; left time: 17946.3053s
Epoch: 47 cost time: 163.92809438705444
Epoch: 47, Steps: 132 | Train Loss: 0.1208797 Vali Loss: 0.1792669 Test Loss: 0.2152129
Validation loss decreased (0.179672 --> 0.179267).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1165605
	speed: 2.6364s/iter; left time: 18182.9434s
Epoch: 48 cost time: 158.70996737480164
Epoch: 48, Steps: 132 | Train Loss: 0.1204357 Vali Loss: 0.1791356 Test Loss: 0.2147259
Validation loss decreased (0.179267 --> 0.179136).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1191932
	speed: 2.6115s/iter; left time: 17666.4715s
Epoch: 49 cost time: 155.37462639808655
Epoch: 49, Steps: 132 | Train Loss: 0.1200625 Vali Loss: 0.1792473 Test Loss: 0.2143320
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1150119
	speed: 2.4891s/iter; left time: 16510.0088s
Epoch: 50 cost time: 145.2717740535736
Epoch: 50, Steps: 132 | Train Loss: 0.1196505 Vali Loss: 0.1786514 Test Loss: 0.2139045
Validation loss decreased (0.179136 --> 0.178651).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.1148527
	speed: 2.4637s/iter; left time: 16016.4259s
Epoch: 51 cost time: 152.8549177646637
Epoch: 51, Steps: 132 | Train Loss: 0.1193398 Vali Loss: 0.1786691 Test Loss: 0.2135655
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.1169391
	speed: 2.4065s/iter; left time: 15327.0289s
Epoch: 52 cost time: 136.5243422985077
Epoch: 52, Steps: 132 | Train Loss: 0.1189758 Vali Loss: 0.1784457 Test Loss: 0.2132753
Validation loss decreased (0.178651 --> 0.178446).  Saving model ...
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.1185064
	speed: 2.3468s/iter; left time: 14636.7897s
Epoch: 53 cost time: 146.18511605262756
Epoch: 53, Steps: 132 | Train Loss: 0.1187539 Vali Loss: 0.1782804 Test Loss: 0.2129547
Validation loss decreased (0.178446 --> 0.178280).  Saving model ...
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.1145115
	speed: 2.4447s/iter; left time: 14924.7685s
Epoch: 54 cost time: 148.12592554092407
Epoch: 54, Steps: 132 | Train Loss: 0.1184893 Vali Loss: 0.1782725 Test Loss: 0.2126721
Validation loss decreased (0.178280 --> 0.178272).  Saving model ...
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.1177068
	speed: 2.3979s/iter; left time: 14322.5854s
Epoch: 55 cost time: 148.94828844070435
Epoch: 55, Steps: 132 | Train Loss: 0.1182553 Vali Loss: 0.1780522 Test Loss: 0.2124000
Validation loss decreased (0.178272 --> 0.178052).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.1127899
	speed: 2.4862s/iter; left time: 14521.6088s
Epoch: 56 cost time: 152.57807230949402
Epoch: 56, Steps: 132 | Train Loss: 0.1180734 Vali Loss: 0.1777215 Test Loss: 0.2121644
Validation loss decreased (0.178052 --> 0.177722).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.1142574
	speed: 2.5120s/iter; left time: 14340.7987s
Epoch: 57 cost time: 147.99384331703186
Epoch: 57, Steps: 132 | Train Loss: 0.1178402 Vali Loss: 0.1773985 Test Loss: 0.2119425
Validation loss decreased (0.177722 --> 0.177399).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.1160944
	speed: 2.3918s/iter; left time: 13339.3216s
Epoch: 58 cost time: 146.43051671981812
Epoch: 58, Steps: 132 | Train Loss: 0.1177146 Vali Loss: 0.1779340 Test Loss: 0.2117315
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.1150903
	speed: 2.5120s/iter; left time: 13677.6521s
Epoch: 59 cost time: 147.62226963043213
Epoch: 59, Steps: 132 | Train Loss: 0.1175180 Vali Loss: 0.1776101 Test Loss: 0.2115589
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.1185280
	speed: 2.5764s/iter; left time: 13688.3955s
Epoch: 60 cost time: 163.11336040496826
Epoch: 60, Steps: 132 | Train Loss: 0.1173790 Vali Loss: 0.1776634 Test Loss: 0.2113853
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.1168299
	speed: 2.6910s/iter; left time: 13942.1301s
Epoch: 61 cost time: 168.79766035079956
Epoch: 61, Steps: 132 | Train Loss: 0.1172197 Vali Loss: 0.1776039 Test Loss: 0.2112177
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.1166214
	speed: 2.6675s/iter; left time: 13468.1950s
Epoch: 62 cost time: 158.19150567054749
Epoch: 62, Steps: 132 | Train Loss: 0.1170929 Vali Loss: 0.1777979 Test Loss: 0.2110639
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.1160164
	speed: 2.6179s/iter; left time: 12872.0937s
Epoch: 63 cost time: 156.0084147453308
Epoch: 63, Steps: 132 | Train Loss: 0.1169750 Vali Loss: 0.1773567 Test Loss: 0.2109244
Validation loss decreased (0.177399 --> 0.177357).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.1179429
	speed: 2.5523s/iter; left time: 12212.9004s
Epoch: 64 cost time: 156.79860997200012
Epoch: 64, Steps: 132 | Train Loss: 0.1168444 Vali Loss: 0.1775319 Test Loss: 0.2107995
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.1136822
	speed: 2.7883s/iter; left time: 12974.1676s
Epoch: 65 cost time: 165.02688884735107
Epoch: 65, Steps: 132 | Train Loss: 0.1167842 Vali Loss: 0.1774938 Test Loss: 0.2106916
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.1216366
	speed: 2.7911s/iter; left time: 12618.7651s
Epoch: 66 cost time: 171.4605095386505
Epoch: 66, Steps: 132 | Train Loss: 0.1166749 Vali Loss: 0.1774059 Test Loss: 0.2105816
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.1125267
	speed: 2.7689s/iter; left time: 12152.5873s
Epoch: 67 cost time: 169.67172622680664
Epoch: 67, Steps: 132 | Train Loss: 0.1166012 Vali Loss: 0.1771055 Test Loss: 0.2104917
Validation loss decreased (0.177357 --> 0.177106).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.1175776
	speed: 2.8729s/iter; left time: 12230.1423s
Epoch: 68 cost time: 175.74363231658936
Epoch: 68, Steps: 132 | Train Loss: 0.1165039 Vali Loss: 0.1773385 Test Loss: 0.2104001
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.1170312
	speed: 2.7907s/iter; left time: 11511.5119s
Epoch: 69 cost time: 173.6627631187439
Epoch: 69, Steps: 132 | Train Loss: 0.1164351 Vali Loss: 0.1772796 Test Loss: 0.2102988
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.1254370
	speed: 2.8233s/iter; left time: 11273.3113s
Epoch: 70 cost time: 167.04869890213013
Epoch: 70, Steps: 132 | Train Loss: 0.1163398 Vali Loss: 0.1774166 Test Loss: 0.2102243
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.1166557
	speed: 2.0375s/iter; left time: 7866.7478s
Epoch: 71 cost time: 112.14629197120667
Epoch: 71, Steps: 132 | Train Loss: 0.1163248 Vali Loss: 0.1772490 Test Loss: 0.2101457
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.1181550
	speed: 1.8853s/iter; left time: 7030.2714s
Epoch: 72 cost time: 114.0622673034668
Epoch: 72, Steps: 132 | Train Loss: 0.1162620 Vali Loss: 0.1773032 Test Loss: 0.2100708
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.1186469
	speed: 1.9368s/iter; left time: 6966.7211s
Epoch: 73 cost time: 118.74367308616638
Epoch: 73, Steps: 132 | Train Loss: 0.1162195 Vali Loss: 0.1773272 Test Loss: 0.2100122
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.1065822
	speed: 1.9751s/iter; left time: 6843.5944s
Epoch: 74 cost time: 120.97028613090515
Epoch: 74, Steps: 132 | Train Loss: 0.1161331 Vali Loss: 0.1771243 Test Loss: 0.2099472
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.1175983
	speed: 2.3854s/iter; left time: 7950.5386s
Epoch: 75 cost time: 173.9849956035614
Epoch: 75, Steps: 132 | Train Loss: 0.1161043 Vali Loss: 0.1771531 Test Loss: 0.2098865
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.1178450
	speed: 3.0299s/iter; left time: 9698.6070s
Epoch: 76 cost time: 182.5953230857849
Epoch: 76, Steps: 132 | Train Loss: 0.1160789 Vali Loss: 0.1774634 Test Loss: 0.2098306
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.1126727
	speed: 2.9946s/iter; left time: 9190.5733s
Epoch: 77 cost time: 180.57904481887817
Epoch: 77, Steps: 132 | Train Loss: 0.1160521 Vali Loss: 0.1771757 Test Loss: 0.2097825
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.1119093
	speed: 2.6912s/iter; left time: 7904.0582s
Epoch: 78 cost time: 140.4038269519806
Epoch: 78, Steps: 132 | Train Loss: 0.1159309 Vali Loss: 0.1775081 Test Loss: 0.2097377
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.1201666
	speed: 2.6941s/iter; left time: 7557.0202s
Epoch: 79 cost time: 154.68072628974915
Epoch: 79, Steps: 132 | Train Loss: 0.1159622 Vali Loss: 0.1772274 Test Loss: 0.2096969
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.1142571
	speed: 2.4820s/iter; left time: 6634.3017s
Epoch: 80 cost time: 133.3163182735443
Epoch: 80, Steps: 132 | Train Loss: 0.1159371 Vali Loss: 0.1769500 Test Loss: 0.2096574
Validation loss decreased (0.177106 --> 0.176950).  Saving model ...
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.1129018
	speed: 1.9638s/iter; left time: 4990.0262s
Epoch: 81 cost time: 121.54490351676941
Epoch: 81, Steps: 132 | Train Loss: 0.1158961 Vali Loss: 0.1771645 Test Loss: 0.2096188
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.1179891
	speed: 2.5213s/iter; left time: 6073.7134s
Epoch: 82 cost time: 152.00738191604614
Epoch: 82, Steps: 132 | Train Loss: 0.1158734 Vali Loss: 0.1772606 Test Loss: 0.2095809
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.1182069
	speed: 2.3659s/iter; left time: 5387.2188s
Epoch: 83 cost time: 132.68344020843506
Epoch: 83, Steps: 132 | Train Loss: 0.1158580 Vali Loss: 0.1770216 Test Loss: 0.2095462
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.1153668
	speed: 2.2621s/iter; left time: 4852.1673s
Epoch: 84 cost time: 144.95935893058777
Epoch: 84, Steps: 132 | Train Loss: 0.1158283 Vali Loss: 0.1771743 Test Loss: 0.2095150
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.1114804
	speed: 2.5472s/iter; left time: 5127.5727s
Epoch: 85 cost time: 148.6014847755432
Epoch: 85, Steps: 132 | Train Loss: 0.1157780 Vali Loss: 0.1771434 Test Loss: 0.2094874
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.1141135
	speed: 2.5697s/iter; left time: 4833.5937s
Epoch: 86 cost time: 157.13083910942078
Epoch: 86, Steps: 132 | Train Loss: 0.1157591 Vali Loss: 0.1770810 Test Loss: 0.2094579
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.1132649
	speed: 2.5585s/iter; left time: 4474.8108s
Epoch: 87 cost time: 154.75030851364136
Epoch: 87, Steps: 132 | Train Loss: 0.1157849 Vali Loss: 0.1772919 Test Loss: 0.2094310
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.1199008
	speed: 2.6023s/iter; left time: 4207.8388s
Epoch: 88 cost time: 161.47743606567383
Epoch: 88, Steps: 132 | Train Loss: 0.1157652 Vali Loss: 0.1771326 Test Loss: 0.2094084
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.766650946003179e-06
	iters: 100, epoch: 89 | loss: 0.1102658
	speed: 2.4191s/iter; left time: 3592.4145s
Epoch: 89 cost time: 144.5125253200531
Epoch: 89, Steps: 132 | Train Loss: 0.1157366 Vali Loss: 0.1770824 Test Loss: 0.2093852
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.47831839870302e-06
	iters: 100, epoch: 90 | loss: 0.1131118
	speed: 2.4430s/iter; left time: 3305.4370s
Epoch: 90 cost time: 144.24444842338562
Epoch: 90, Steps: 132 | Train Loss: 0.1157048 Vali Loss: 0.1774520 Test Loss: 0.2093629
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.204402478767869e-06
	iters: 100, epoch: 91 | loss: 0.1149983
	speed: 2.3851s/iter; left time: 2912.1768s
Epoch: 91 cost time: 140.70339465141296
Epoch: 91, Steps: 132 | Train Loss: 0.1156866 Vali Loss: 0.1773852 Test Loss: 0.2093417
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.944182354829475e-06
	iters: 100, epoch: 92 | loss: 0.1135718
	speed: 2.3954s/iter; left time: 2608.5663s
Epoch: 92 cost time: 146.679048538208
Epoch: 92, Steps: 132 | Train Loss: 0.1156664 Vali Loss: 0.1772616 Test Loss: 0.2093249
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.696973237088e-06
	iters: 100, epoch: 93 | loss: 0.1120832
	speed: 2.4166s/iter; left time: 2312.6864s
Epoch: 93 cost time: 143.1850552558899
Epoch: 93, Steps: 132 | Train Loss: 0.1156537 Vali Loss: 0.1772645 Test Loss: 0.2093055
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.462124575233601e-06
	iters: 100, epoch: 94 | loss: 0.1152657
	speed: 2.2989s/iter; left time: 1896.6143s
Epoch: 94 cost time: 134.17758107185364
Epoch: 94, Steps: 132 | Train Loss: 0.1156065 Vali Loss: 0.1775240 Test Loss: 0.2092887
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.239018346471921e-06
	iters: 100, epoch: 95 | loss: 0.1166311
	speed: 2.3945s/iter; left time: 1659.3723s
Epoch: 95 cost time: 146.08066582679749
Epoch: 95, Steps: 132 | Train Loss: 0.1156622 Vali Loss: 0.1770336 Test Loss: 0.2092728
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.027067429148324e-06
	iters: 100, epoch: 96 | loss: 0.1127055
	speed: 2.3785s/iter; left time: 1334.3107s
Epoch: 96 cost time: 143.62170147895813
Epoch: 96, Steps: 132 | Train Loss: 0.1156255 Vali Loss: 0.1773049 Test Loss: 0.2092578
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.825714057690908e-06
	iters: 100, epoch: 97 | loss: 0.1179172
	speed: 2.3166s/iter; left time: 993.8291s
Epoch: 97 cost time: 139.92473888397217
Epoch: 97, Steps: 132 | Train Loss: 0.1156326 Vali Loss: 0.1772820 Test Loss: 0.2092438
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.6344283548063623e-06
	iters: 100, epoch: 98 | loss: 0.1176750
	speed: 2.0904s/iter; left time: 620.8509s
Epoch: 98 cost time: 127.84910559654236
Epoch: 98, Steps: 132 | Train Loss: 0.1155815 Vali Loss: 0.1773315 Test Loss: 0.2092305
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.452706937066044e-06
	iters: 100, epoch: 99 | loss: 0.1148737
	speed: 2.1924s/iter; left time: 361.7443s
Epoch: 99 cost time: 132.8022472858429
Epoch: 99, Steps: 132 | Train Loss: 0.1156100 Vali Loss: 0.1770285 Test Loss: 0.2092185
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.2800715902127414e-06
	iters: 100, epoch: 100 | loss: 0.1222168
	speed: 2.2081s/iter; left time: 72.8662s
Epoch: 100 cost time: 131.7795968055725
Epoch: 100, Steps: 132 | Train Loss: 0.1156101 Vali Loss: 0.1769874 Test Loss: 0.2092052
EarlyStopping counter: 20 out of 20
Early stopping
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5469963264.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2212180
	speed: 0.9590s/iter; left time: 12564.3779s
Epoch: 1 cost time: 126.6178834438324
Epoch: 1, Steps: 132 | Train Loss: 0.2201157 Vali Loss: 0.1767983 Test Loss: 0.2078955
Validation loss decreased (inf --> 0.176798).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2153635
	speed: 1.8490s/iter; left time: 23980.1468s
Epoch: 2 cost time: 108.76265072822571
Epoch: 2, Steps: 132 | Train Loss: 0.2197055 Vali Loss: 0.1767864 Test Loss: 0.2076854
Validation loss decreased (0.176798 --> 0.176786).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2211631
	speed: 1.8971s/iter; left time: 24353.0468s
Epoch: 3 cost time: 114.87943506240845
Epoch: 3, Steps: 132 | Train Loss: 0.2194705 Vali Loss: 0.1766267 Test Loss: 0.2075149
Validation loss decreased (0.176786 --> 0.176627).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2137466
	speed: 1.7834s/iter; left time: 22658.6267s
Epoch: 4 cost time: 106.14289236068726
Epoch: 4, Steps: 132 | Train Loss: 0.2193481 Vali Loss: 0.1764929 Test Loss: 0.2075780
Validation loss decreased (0.176627 --> 0.176493).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2230760
	speed: 1.7617s/iter; left time: 22149.3751s
Epoch: 5 cost time: 107.580979347229
Epoch: 5, Steps: 132 | Train Loss: 0.2192760 Vali Loss: 0.1769831 Test Loss: 0.2074280
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2012636
	speed: 1.7159s/iter; left time: 21347.1460s
Epoch: 6 cost time: 101.88893914222717
Epoch: 6, Steps: 132 | Train Loss: 0.2193211 Vali Loss: 0.1765455 Test Loss: 0.2075120
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2133886
	speed: 1.5819s/iter; left time: 19472.1530s
Epoch: 7 cost time: 95.16681694984436
Epoch: 7, Steps: 132 | Train Loss: 0.2192579 Vali Loss: 0.1767707 Test Loss: 0.2074688
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2087151
	speed: 1.5839s/iter; left time: 19287.4695s
Epoch: 8 cost time: 96.3421802520752
Epoch: 8, Steps: 132 | Train Loss: 0.2191199 Vali Loss: 0.1764892 Test Loss: 0.2074253
Validation loss decreased (0.176493 --> 0.176489).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2260058
	speed: 1.5740s/iter; left time: 18958.7762s
Epoch: 9 cost time: 86.44202613830566
Epoch: 9, Steps: 132 | Train Loss: 0.2191749 Vali Loss: 0.1768030 Test Loss: 0.2074262
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2222418
	speed: 1.4786s/iter; left time: 17614.3583s
Epoch: 10 cost time: 88.48685836791992
Epoch: 10, Steps: 132 | Train Loss: 0.2191561 Vali Loss: 0.1763362 Test Loss: 0.2074788
Validation loss decreased (0.176489 --> 0.176336).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2223820
	speed: 1.5557s/iter; left time: 18327.5161s
Epoch: 11 cost time: 93.5200309753418
Epoch: 11, Steps: 132 | Train Loss: 0.2190292 Vali Loss: 0.1763881 Test Loss: 0.2073816
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2207065
	speed: 1.4270s/iter; left time: 16623.3592s
Epoch: 12 cost time: 84.06678819656372
Epoch: 12, Steps: 132 | Train Loss: 0.2190605 Vali Loss: 0.1766542 Test Loss: 0.2074215
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2278598
	speed: 1.5799s/iter; left time: 18195.6755s
Epoch: 13 cost time: 92.8803243637085
Epoch: 13, Steps: 132 | Train Loss: 0.2190685 Vali Loss: 0.1765650 Test Loss: 0.2074476
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2306735
	speed: 1.5409s/iter; left time: 17542.9636s
Epoch: 14 cost time: 94.12829113006592
Epoch: 14, Steps: 132 | Train Loss: 0.2190765 Vali Loss: 0.1763697 Test Loss: 0.2073434
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2154990
	speed: 1.5038s/iter; left time: 16922.1534s
Epoch: 15 cost time: 89.82545232772827
Epoch: 15, Steps: 132 | Train Loss: 0.2189895 Vali Loss: 0.1762420 Test Loss: 0.2074149
Validation loss decreased (0.176336 --> 0.176242).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2193993
	speed: 1.5068s/iter; left time: 16757.1189s
Epoch: 16 cost time: 90.55137419700623
Epoch: 16, Steps: 132 | Train Loss: 0.2190026 Vali Loss: 0.1763395 Test Loss: 0.2074096
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2186899
	speed: 1.3805s/iter; left time: 15170.2387s
Epoch: 17 cost time: 77.81146931648254
Epoch: 17, Steps: 132 | Train Loss: 0.2189561 Vali Loss: 0.1765272 Test Loss: 0.2074208
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2095942
	speed: 1.2412s/iter; left time: 13475.2203s
Epoch: 18 cost time: 77.54508352279663
Epoch: 18, Steps: 132 | Train Loss: 0.2189625 Vali Loss: 0.1760980 Test Loss: 0.2073927
Validation loss decreased (0.176242 --> 0.176098).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2329861
	speed: 1.3475s/iter; left time: 14452.1730s
Epoch: 19 cost time: 78.71598672866821
Epoch: 19, Steps: 132 | Train Loss: 0.2188756 Vali Loss: 0.1762762 Test Loss: 0.2074108
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2248750
	speed: 1.3754s/iter; left time: 14569.6400s
Epoch: 20 cost time: 83.30808234214783
Epoch: 20, Steps: 132 | Train Loss: 0.2189635 Vali Loss: 0.1761930 Test Loss: 0.2072945
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2174437
	speed: 1.2921s/iter; left time: 13516.7251s
Epoch: 21 cost time: 74.72557258605957
Epoch: 21, Steps: 132 | Train Loss: 0.2188855 Vali Loss: 0.1761655 Test Loss: 0.2074069
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2126926
	speed: 1.3324s/iter; left time: 13762.3585s
Epoch: 22 cost time: 79.52969789505005
Epoch: 22, Steps: 132 | Train Loss: 0.2189417 Vali Loss: 0.1763151 Test Loss: 0.2073665
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2157487
	speed: 1.2792s/iter; left time: 13043.8763s
Epoch: 23 cost time: 76.33974623680115
Epoch: 23, Steps: 132 | Train Loss: 0.2189054 Vali Loss: 0.1765727 Test Loss: 0.2072949
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2162962
	speed: 1.3163s/iter; left time: 13248.3184s
Epoch: 24 cost time: 84.57948350906372
Epoch: 24, Steps: 132 | Train Loss: 0.2189302 Vali Loss: 0.1760762 Test Loss: 0.2073600
Validation loss decreased (0.176098 --> 0.176076).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2227145
	speed: 1.2533s/iter; left time: 12448.6303s
Epoch: 25 cost time: 74.62177777290344
Epoch: 25, Steps: 132 | Train Loss: 0.2189153 Vali Loss: 0.1761205 Test Loss: 0.2073353
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2274855
	speed: 1.3604s/iter; left time: 13332.8245s
Epoch: 26 cost time: 82.21940898895264
Epoch: 26, Steps: 132 | Train Loss: 0.2188173 Vali Loss: 0.1761731 Test Loss: 0.2073415
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2324615
	speed: 1.4309s/iter; left time: 13835.8236s
Epoch: 27 cost time: 85.08220505714417
Epoch: 27, Steps: 132 | Train Loss: 0.2189514 Vali Loss: 0.1760189 Test Loss: 0.2073095
Validation loss decreased (0.176076 --> 0.176019).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2257422
	speed: 1.3960s/iter; left time: 13313.7392s
Epoch: 28 cost time: 85.1362087726593
Epoch: 28, Steps: 132 | Train Loss: 0.2189293 Vali Loss: 0.1761404 Test Loss: 0.2073304
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2136450
	speed: 1.5038s/iter; left time: 14143.3649s
Epoch: 29 cost time: 91.34116578102112
Epoch: 29, Steps: 132 | Train Loss: 0.2189208 Vali Loss: 0.1763418 Test Loss: 0.2073523
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2263360
	speed: 1.4281s/iter; left time: 13243.0104s
Epoch: 30 cost time: 82.80178999900818
Epoch: 30, Steps: 132 | Train Loss: 0.2188216 Vali Loss: 0.1759751 Test Loss: 0.2073021
Validation loss decreased (0.176019 --> 0.175975).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2222859
	speed: 1.3299s/iter; left time: 12156.7611s
Epoch: 31 cost time: 75.66335582733154
Epoch: 31, Steps: 132 | Train Loss: 0.2188827 Vali Loss: 0.1765097 Test Loss: 0.2073181
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2157582
	speed: 1.1898s/iter; left time: 10719.2098s
Epoch: 32 cost time: 69.6883475780487
Epoch: 32, Steps: 132 | Train Loss: 0.2188695 Vali Loss: 0.1763177 Test Loss: 0.2073525
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2216145
	speed: 1.1523s/iter; left time: 10228.8865s
Epoch: 33 cost time: 71.24132323265076
Epoch: 33, Steps: 132 | Train Loss: 0.2188770 Vali Loss: 0.1761071 Test Loss: 0.2073287
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2188974
	speed: 1.2483s/iter; left time: 10916.4933s
Epoch: 34 cost time: 75.60022687911987
Epoch: 34, Steps: 132 | Train Loss: 0.2187905 Vali Loss: 0.1764237 Test Loss: 0.2072964
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2165169
	speed: 1.2076s/iter; left time: 10400.8632s
Epoch: 35 cost time: 71.17318916320801
Epoch: 35, Steps: 132 | Train Loss: 0.2187916 Vali Loss: 0.1761001 Test Loss: 0.2073036
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2327527
	speed: 1.3270s/iter; left time: 11253.8723s
Epoch: 36 cost time: 77.49298977851868
Epoch: 36, Steps: 132 | Train Loss: 0.2189209 Vali Loss: 0.1760758 Test Loss: 0.2072938
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2107474
	speed: 1.2983s/iter; left time: 10839.7811s
Epoch: 37 cost time: 78.51708102226257
Epoch: 37, Steps: 132 | Train Loss: 0.2188539 Vali Loss: 0.1761143 Test Loss: 0.2073151
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2079964
	speed: 1.2791s/iter; left time: 10510.3853s
Epoch: 38 cost time: 74.41933345794678
Epoch: 38, Steps: 132 | Train Loss: 0.2188289 Vali Loss: 0.1760700 Test Loss: 0.2072607
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2160063
	speed: 1.1872s/iter; left time: 9598.8028s
Epoch: 39 cost time: 72.19535899162292
Epoch: 39, Steps: 132 | Train Loss: 0.2187988 Vali Loss: 0.1761888 Test Loss: 0.2072918
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2281120
	speed: 1.3024s/iter; left time: 10357.6502s
Epoch: 40 cost time: 75.97544240951538
Epoch: 40, Steps: 132 | Train Loss: 0.2188359 Vali Loss: 0.1761488 Test Loss: 0.2072998
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2090425
	speed: 1.2429s/iter; left time: 9720.3569s
Epoch: 41 cost time: 77.88172912597656
Epoch: 41, Steps: 132 | Train Loss: 0.2189111 Vali Loss: 0.1761238 Test Loss: 0.2072954
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2196281
	speed: 1.3538s/iter; left time: 10409.2812s
Epoch: 42 cost time: 78.83172154426575
Epoch: 42, Steps: 132 | Train Loss: 0.2187444 Vali Loss: 0.1762865 Test Loss: 0.2073078
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2164636
	speed: 1.2404s/iter; left time: 9373.8311s
Epoch: 43 cost time: 76.37079668045044
Epoch: 43, Steps: 132 | Train Loss: 0.2188635 Vali Loss: 0.1761639 Test Loss: 0.2072904
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2228680
	speed: 1.2652s/iter; left time: 9394.1894s
Epoch: 44 cost time: 74.95803022384644
Epoch: 44, Steps: 132 | Train Loss: 0.2187884 Vali Loss: 0.1761943 Test Loss: 0.2072771
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2321578
	speed: 1.2880s/iter; left time: 9393.6153s
Epoch: 45 cost time: 77.90877318382263
Epoch: 45, Steps: 132 | Train Loss: 0.2188226 Vali Loss: 0.1762135 Test Loss: 0.2073045
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2116150
	speed: 1.2222s/iter; left time: 8752.3978s
Epoch: 46 cost time: 72.4431381225586
Epoch: 46, Steps: 132 | Train Loss: 0.2187690 Vali Loss: 0.1764291 Test Loss: 0.2072991
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2278607
	speed: 1.3056s/iter; left time: 9177.2102s
Epoch: 47 cost time: 80.51994681358337
Epoch: 47, Steps: 132 | Train Loss: 0.2187097 Vali Loss: 0.1762628 Test Loss: 0.2072813
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2190713
	speed: 1.3308s/iter; left time: 9178.8714s
Epoch: 48 cost time: 78.8375174999237
Epoch: 48, Steps: 132 | Train Loss: 0.2188252 Vali Loss: 0.1761216 Test Loss: 0.2072847
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2178765
	speed: 1.2822s/iter; left time: 8674.1913s
Epoch: 49 cost time: 71.78729057312012
Epoch: 49, Steps: 132 | Train Loss: 0.2188137 Vali Loss: 0.1760598 Test Loss: 0.2072812
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2182705
	speed: 1.2247s/iter; left time: 8123.5505s
Epoch: 50 cost time: 78.33075499534607
Epoch: 50, Steps: 132 | Train Loss: 0.2187334 Vali Loss: 0.1760073 Test Loss: 0.2072833
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.20560377836227417, mae:0.2956766188144684, rse:0.4523169994354248, corr:[0.44676962 0.44953775 0.45061234 0.45109126 0.45116895 0.45113212
 0.45120528 0.45107624 0.4509286  0.45090175 0.45074728 0.45072052
 0.45068395 0.45059124 0.45059866 0.4506156  0.45059383 0.4505662
 0.4505114  0.4503815  0.45022917 0.4502366  0.450321   0.45041057
 0.4506118  0.45091385 0.45102236 0.4510407  0.45097438 0.45085388
 0.45074552 0.45059952 0.45044538 0.4503859  0.4502921  0.45020258
 0.4501896  0.45021474 0.45020443 0.450165   0.45014912 0.4501188
 0.4500156  0.4498371  0.4497195  0.44975036 0.44983995 0.449882
 0.45002285 0.4502946  0.4503795  0.45033285 0.45029625 0.45028389
 0.45022932 0.4500979  0.44998085 0.44992802 0.44984573 0.44978023
 0.4497849  0.44978946 0.44981733 0.4498489  0.4497875  0.44974956
 0.44975266 0.4495975  0.4494909  0.44950202 0.44950038 0.44948664
 0.44954345 0.44973046 0.44983828 0.4498     0.44971615 0.4496592
 0.4495614  0.44947165 0.44944203 0.44935662 0.4492462  0.44917986
 0.44914114 0.449194   0.44926482 0.44923714 0.449176   0.44915485
 0.44917113 0.4491068  0.44901806 0.44902527 0.448996   0.44893783
 0.44902426 0.4491304  0.44909874 0.44907492 0.4490148  0.44893366
 0.44888648 0.4488049  0.44870746 0.4486699  0.44860062 0.4485452
 0.4485358  0.44854185 0.44858962 0.44864362 0.44859818 0.4485339
 0.44852126 0.44843665 0.4483522  0.44838184 0.44844472 0.44852173
 0.44876167 0.4489682  0.44897822 0.44889748 0.44884825 0.44885343
 0.44882298 0.4487708  0.44870764 0.44857633 0.4484204  0.44833872
 0.44831324 0.44832405 0.44839698 0.44842967 0.44838688 0.4483811
 0.44845834 0.4484742  0.4484962  0.4485365  0.44847828 0.44843736
 0.44849104 0.44855812 0.4485834  0.44860548 0.4485907  0.44858503
 0.4485304  0.44841126 0.4483572  0.44834742 0.44825485 0.44812557
 0.44806275 0.44804886 0.44808033 0.44814047 0.44817787 0.44815177
 0.44823    0.44827715 0.44818217 0.4481296  0.44807944 0.44791487
 0.44778264 0.4478858  0.44798    0.44799635 0.44794372 0.44786227
 0.44777304 0.44769275 0.44758272 0.4474071  0.44723982 0.44717053
 0.44712043 0.44706827 0.44704333 0.44704846 0.44707793 0.44705656
 0.44693923 0.44672477 0.44657925 0.4465145  0.446458   0.446414
 0.44646814 0.44666803 0.4467354  0.44666472 0.44655392 0.4465286
 0.44650427 0.44639307 0.44631562 0.4462496  0.44610617 0.4460548
 0.44614428 0.4461824  0.4461807  0.44617862 0.4461189  0.4460484
 0.44601524 0.4458144  0.4456054  0.4455549  0.44556972 0.44560963
 0.44568694 0.4458038  0.4458845  0.44589674 0.44578707 0.4457256
 0.44575873 0.44569483 0.44558796 0.44554886 0.44552454 0.4454866
 0.4454947  0.44548213 0.4454454  0.44547266 0.44550097 0.44535404
 0.44521195 0.4451285  0.44502822 0.4449083  0.4449541  0.44508567
 0.4451617  0.445295   0.44544208 0.445464   0.44535196 0.44530863
 0.4452541  0.44506946 0.44500527 0.445014   0.4448862  0.44479617
 0.44482216 0.44482884 0.4448249  0.4448382  0.44479525 0.44472072
 0.44467136 0.44457948 0.44449934 0.44449738 0.44454828 0.44457206
 0.44467327 0.44489792 0.44499537 0.44494465 0.4448949  0.4448557
 0.44476447 0.4446789  0.44456404 0.44444126 0.44439423 0.4443424
 0.4442612  0.44425654 0.44436434 0.4443975  0.4442966  0.44423258
 0.44423983 0.44414303 0.44400752 0.44401553 0.4441356  0.44429973
 0.44452575 0.44465452 0.44470623 0.44469854 0.44462994 0.44463974
 0.4446398  0.4445397  0.4444484  0.44439715 0.4443381  0.4442702
 0.4442171  0.4442286  0.44425964 0.44426128 0.44429138 0.44429126
 0.4442725  0.4442991  0.44434988 0.4443638  0.4443443  0.4443089
 0.44430205 0.44440657 0.44451603 0.44455835 0.44450834 0.4444535
 0.44438908 0.44426692 0.44422457 0.44423124 0.44413248 0.44402403
 0.4440333  0.4440486  0.44404167 0.44405115 0.44405413 0.4440386
 0.44410163 0.44411054 0.44401824 0.44394174 0.44380927 0.44358048
 0.44342655 0.44354275 0.44358927 0.4435065  0.44339412 0.44331715
 0.44318187 0.4430204  0.4429227  0.4428302  0.4426785  0.44258818
 0.44259083 0.4425699  0.44251192 0.44249907 0.4425114  0.44243586
 0.44239116 0.4423831  0.4423349  0.4423135  0.4423337  0.44227126
 0.44221282 0.44234443 0.44239044 0.44233793 0.44226143 0.44218576
 0.44209003 0.44200006 0.4419155  0.44178653 0.44165924 0.44165224
 0.44171396 0.4417123  0.44172385 0.44174856 0.44174868 0.44172415
 0.4416624  0.44150025 0.44141608 0.44141954 0.44138896 0.44135502
 0.44136485 0.4414052  0.44140598 0.4413823  0.4413095  0.441238
 0.44118023 0.44112575 0.44106647 0.44100994 0.44091293 0.44084963
 0.44088078 0.44086733 0.44081    0.44087943 0.44099343 0.44093862
 0.4408484  0.4407834  0.4407184  0.44068432 0.44066477 0.44064155
 0.44069526 0.44087958 0.4410089  0.4410205  0.44097722 0.44096482
 0.44088858 0.44076458 0.44067103 0.44054717 0.44041678 0.44039452
 0.44041118 0.44037345 0.44043475 0.4405379  0.44055563 0.44054392
 0.44053978 0.44045666 0.4404167  0.44045004 0.44044447 0.44040328
 0.4404759  0.4406361  0.44067833 0.44064343 0.44059393 0.44053993
 0.44050604 0.44047385 0.44041067 0.44031256 0.44019648 0.44013584
 0.44016716 0.44017494 0.44015157 0.4401829  0.44020334 0.4401515
 0.44010997 0.4400756  0.4400196  0.43994093 0.43993935 0.44010863
 0.44038612 0.44053227 0.44060448 0.4406515  0.4406248  0.44060656
 0.44053254 0.44037592 0.440272   0.44022644 0.44017658 0.44014212
 0.44018745 0.44024634 0.4402444  0.4402376  0.44027105 0.4402226
 0.44021428 0.44031286 0.44036543 0.44036338 0.44035947 0.44032663
 0.44029966 0.4403868  0.44048014 0.44051674 0.44048163 0.44041744
 0.44034564 0.4403249  0.44034675 0.44025755 0.44010994 0.44008857
 0.4401315  0.4401301  0.44016123 0.44023997 0.4402814  0.4402292
 0.4401891  0.44017404 0.44018042 0.4401398  0.44000694 0.43980217
 0.43963054 0.43968385 0.43974707 0.43974826 0.43964446 0.43954003
 0.43940452 0.43923408 0.43916392 0.4391188  0.43891415 0.4387476
 0.4387691  0.43879336 0.4387562  0.438774   0.4388093  0.4387688
 0.43870243 0.4385984  0.43852985 0.43849596 0.43841788 0.43831193
 0.43826887 0.43837696 0.4384138  0.43835115 0.43826434 0.43820933
 0.4380985  0.43795657 0.43785435 0.43772894 0.43755534 0.4374782
 0.4375214  0.43758163 0.43758985 0.43756902 0.43761593 0.4376709
 0.43761337 0.4374601  0.4373609  0.43728927 0.4372703  0.43727982
 0.43727243 0.4372916  0.43731892 0.4372807  0.43714762 0.43709728
 0.43709862 0.43696406 0.43683058 0.43673941 0.43653613 0.43638927
 0.43641403 0.4363538  0.43621352 0.43615806 0.43613115 0.43597862
 0.4358414  0.4358306  0.43583748 0.43579176 0.43578056 0.43577173
 0.43578205 0.43591213 0.43600443 0.43597308 0.43593863 0.4359275
 0.43588012 0.4358139  0.43572402 0.4355757  0.4354209  0.4353566
 0.43533644 0.4352859  0.4352718  0.43528056 0.4352807  0.4352693
 0.43527374 0.43522835 0.43519092 0.43517977 0.43516353 0.43516052
 0.4352041  0.43528366 0.43536437 0.43537793 0.43524504 0.43514225
 0.4351096  0.4350289  0.43489653 0.43478638 0.43467522 0.43458575
 0.43456924 0.43460792 0.4346155  0.43459377 0.43462026 0.43461472
 0.43455136 0.43452647 0.43451554 0.4344893  0.43452334 0.43465438
 0.43490976 0.43506056 0.43507588 0.4350419  0.43496042 0.43493915
 0.4349353  0.43480057 0.4346671  0.43461108 0.43450484 0.43443152
 0.43446076 0.4344193  0.43434623 0.43436003 0.43441096 0.43440524
 0.4344208  0.43445587 0.43446067 0.43442938 0.43443283 0.43450508
 0.4345209  0.43455932 0.4346375  0.43460009 0.4344729  0.43447617
 0.4344889  0.43434042 0.43418095 0.4341165  0.4340706  0.43398955
 0.43395633 0.43397677 0.43397984 0.43403918 0.43409348 0.43399483
 0.43396065 0.43397713 0.4339654  0.4339787  0.43394932 0.43378794
 0.43361    0.43363973 0.43364033 0.43356574 0.433433   0.4333424
 0.4332338  0.43302783 0.43288386 0.4327364  0.43248683 0.43232197
 0.43226314 0.43228415 0.4323119  0.43223384 0.43226555 0.4323597
 0.43223587 0.4320866  0.43207338 0.43203878 0.43203655 0.4320702
 0.43209067 0.4321456  0.43211997 0.43212032 0.43203643 0.43186316
 0.43181616 0.43172333 0.43153414 0.43134704 0.43109143 0.43107235
 0.42905566 0.4310759  0.42696643 0.42702353 0.43123454 0.42704487
 0.42686653 0.42685118 0.42678222 0.43110573 0.4268773  0.42666712]
