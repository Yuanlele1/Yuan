Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_180_j720_H4', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_180_j720_H4_FITS_custom_ftM_sl180_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17513
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=42, out_features=210, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  362396160.0
params:  9030.0
Trainable parameters:  9030
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8444802
	speed: 0.9568s/iter; left time: 12918.2767s
Epoch: 1 cost time: 130.8081877231598
Epoch: 1, Steps: 136 | Train Loss: 1.1903039 Vali Loss: 0.6002188 Test Loss: 0.6681637
Validation loss decreased (inf --> 0.600219).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4759336
	speed: 2.0981s/iter; left time: 28040.5276s
Epoch: 2 cost time: 114.50531578063965
Epoch: 2, Steps: 136 | Train Loss: 0.5467055 Vali Loss: 0.3660790 Test Loss: 0.4121307
Validation loss decreased (0.600219 --> 0.366079).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3611422
	speed: 2.0157s/iter; left time: 26665.1801s
Epoch: 3 cost time: 113.82976007461548
Epoch: 3, Steps: 136 | Train Loss: 0.3806261 Vali Loss: 0.2843228 Test Loss: 0.3236585
Validation loss decreased (0.366079 --> 0.284323).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3202355
	speed: 2.0375s/iter; left time: 26677.4678s
Epoch: 4 cost time: 116.67915534973145
Epoch: 4, Steps: 136 | Train Loss: 0.3193745 Vali Loss: 0.2498722 Test Loss: 0.2874555
Validation loss decreased (0.284323 --> 0.249872).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2914309
	speed: 2.0046s/iter; left time: 25973.6459s
Epoch: 5 cost time: 117.0018413066864
Epoch: 5, Steps: 136 | Train Loss: 0.2918655 Vali Loss: 0.2332508 Test Loss: 0.2692914
Validation loss decreased (0.249872 --> 0.233251).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2825632
	speed: 2.0727s/iter; left time: 26574.6756s
Epoch: 6 cost time: 119.40158033370972
Epoch: 6, Steps: 136 | Train Loss: 0.2771114 Vali Loss: 0.2225372 Test Loss: 0.2587251
Validation loss decreased (0.233251 --> 0.222537).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2691720
	speed: 2.0215s/iter; left time: 25642.5425s
Epoch: 7 cost time: 116.6354730129242
Epoch: 7, Steps: 136 | Train Loss: 0.2682174 Vali Loss: 0.2166569 Test Loss: 0.2521218
Validation loss decreased (0.222537 --> 0.216657).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2589232
	speed: 1.9958s/iter; left time: 25045.1820s
Epoch: 8 cost time: 113.364590883255
Epoch: 8, Steps: 136 | Train Loss: 0.2625071 Vali Loss: 0.2123648 Test Loss: 0.2477480
Validation loss decreased (0.216657 --> 0.212365).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2598158
	speed: 2.0379s/iter; left time: 25296.8478s
Epoch: 9 cost time: 122.07004070281982
Epoch: 9, Steps: 136 | Train Loss: 0.2589477 Vali Loss: 0.2102761 Test Loss: 0.2447784
Validation loss decreased (0.212365 --> 0.210276).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2501535
	speed: 2.4629s/iter; left time: 30237.2883s
Epoch: 10 cost time: 145.26261949539185
Epoch: 10, Steps: 136 | Train Loss: 0.2563394 Vali Loss: 0.2083902 Test Loss: 0.2427070
Validation loss decreased (0.210276 --> 0.208390).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2582968
	speed: 2.6403s/iter; left time: 32056.3211s
Epoch: 11 cost time: 152.4467122554779
Epoch: 11, Steps: 136 | Train Loss: 0.2545318 Vali Loss: 0.2070600 Test Loss: 0.2411701
Validation loss decreased (0.208390 --> 0.207060).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2535689
	speed: 2.6038s/iter; left time: 31258.7093s
Epoch: 12 cost time: 152.63430261611938
Epoch: 12, Steps: 136 | Train Loss: 0.2530851 Vali Loss: 0.2060452 Test Loss: 0.2400154
Validation loss decreased (0.207060 --> 0.206045).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2361909
	speed: 2.2943s/iter; left time: 27230.8337s
Epoch: 13 cost time: 113.47796702384949
Epoch: 13, Steps: 136 | Train Loss: 0.2522604 Vali Loss: 0.2049593 Test Loss: 0.2391324
Validation loss decreased (0.206045 --> 0.204959).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2480224
	speed: 1.4992s/iter; left time: 17590.0083s
Epoch: 14 cost time: 86.01672458648682
Epoch: 14, Steps: 136 | Train Loss: 0.2515182 Vali Loss: 0.2045346 Test Loss: 0.2384344
Validation loss decreased (0.204959 --> 0.204535).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2562144
	speed: 1.4998s/iter; left time: 17392.8434s
Epoch: 15 cost time: 86.01768851280212
Epoch: 15, Steps: 136 | Train Loss: 0.2507920 Vali Loss: 0.2039427 Test Loss: 0.2378543
Validation loss decreased (0.204535 --> 0.203943).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2707006
	speed: 1.6114s/iter; left time: 18468.6947s
Epoch: 16 cost time: 91.15083265304565
Epoch: 16, Steps: 136 | Train Loss: 0.2502278 Vali Loss: 0.2036618 Test Loss: 0.2373804
Validation loss decreased (0.203943 --> 0.203662).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2623319
	speed: 2.1126s/iter; left time: 23924.8248s
Epoch: 17 cost time: 141.92350387573242
Epoch: 17, Steps: 136 | Train Loss: 0.2498893 Vali Loss: 0.2035023 Test Loss: 0.2369641
Validation loss decreased (0.203662 --> 0.203502).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2621624
	speed: 2.5282s/iter; left time: 28287.7064s
Epoch: 18 cost time: 135.95551896095276
Epoch: 18, Steps: 136 | Train Loss: 0.2494622 Vali Loss: 0.2029414 Test Loss: 0.2366295
Validation loss decreased (0.203502 --> 0.202941).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2493551
	speed: 2.1399s/iter; left time: 23652.5947s
Epoch: 19 cost time: 115.56513690948486
Epoch: 19, Steps: 136 | Train Loss: 0.2491505 Vali Loss: 0.2029932 Test Loss: 0.2363151
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2549235
	speed: 1.7295s/iter; left time: 18880.6919s
Epoch: 20 cost time: 101.26084280014038
Epoch: 20, Steps: 136 | Train Loss: 0.2487799 Vali Loss: 0.2022866 Test Loss: 0.2360595
Validation loss decreased (0.202941 --> 0.202287).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2490869
	speed: 1.6883s/iter; left time: 18201.1708s
Epoch: 21 cost time: 92.62024116516113
Epoch: 21, Steps: 136 | Train Loss: 0.2485718 Vali Loss: 0.2021667 Test Loss: 0.2358201
Validation loss decreased (0.202287 --> 0.202167).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2311035
	speed: 1.4255s/iter; left time: 15173.9444s
Epoch: 22 cost time: 79.79176831245422
Epoch: 22, Steps: 136 | Train Loss: 0.2482750 Vali Loss: 0.2020801 Test Loss: 0.2356230
Validation loss decreased (0.202167 --> 0.202080).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2438002
	speed: 1.3682s/iter; left time: 14378.1739s
Epoch: 23 cost time: 77.82822155952454
Epoch: 23, Steps: 136 | Train Loss: 0.2481292 Vali Loss: 0.2020993 Test Loss: 0.2354242
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2537242
	speed: 1.3460s/iter; left time: 13961.9055s
Epoch: 24 cost time: 80.17024445533752
Epoch: 24, Steps: 136 | Train Loss: 0.2478588 Vali Loss: 0.2014059 Test Loss: 0.2352598
Validation loss decreased (0.202080 --> 0.201406).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2500999
	speed: 1.3564s/iter; left time: 13885.0607s
Epoch: 25 cost time: 79.28531241416931
Epoch: 25, Steps: 136 | Train Loss: 0.2477677 Vali Loss: 0.2020817 Test Loss: 0.2351092
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2379892
	speed: 1.3974s/iter; left time: 14115.4178s
Epoch: 26 cost time: 79.93617033958435
Epoch: 26, Steps: 136 | Train Loss: 0.2475747 Vali Loss: 0.2016604 Test Loss: 0.2349681
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2401889
	speed: 1.3841s/iter; left time: 13793.0156s
Epoch: 27 cost time: 81.4584276676178
Epoch: 27, Steps: 136 | Train Loss: 0.2474865 Vali Loss: 0.2014427 Test Loss: 0.2348518
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2512258
	speed: 1.3341s/iter; left time: 13112.4258s
Epoch: 28 cost time: 79.41269636154175
Epoch: 28, Steps: 136 | Train Loss: 0.2474159 Vali Loss: 0.2014136 Test Loss: 0.2347411
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2460542
	speed: 1.3032s/iter; left time: 12631.8011s
Epoch: 29 cost time: 76.8264901638031
Epoch: 29, Steps: 136 | Train Loss: 0.2471526 Vali Loss: 0.2011253 Test Loss: 0.2346403
Validation loss decreased (0.201406 --> 0.201125).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2485972
	speed: 1.3377s/iter; left time: 12783.9629s
Epoch: 30 cost time: 79.6632490158081
Epoch: 30, Steps: 136 | Train Loss: 0.2470904 Vali Loss: 0.2010441 Test Loss: 0.2345473
Validation loss decreased (0.201125 --> 0.201044).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2466572
	speed: 1.1998s/iter; left time: 11302.8505s
Epoch: 31 cost time: 68.85593152046204
Epoch: 31, Steps: 136 | Train Loss: 0.2470068 Vali Loss: 0.2010334 Test Loss: 0.2344633
Validation loss decreased (0.201044 --> 0.201033).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2564648
	speed: 1.2705s/iter; left time: 11796.2728s
Epoch: 32 cost time: 73.95760440826416
Epoch: 32, Steps: 136 | Train Loss: 0.2469554 Vali Loss: 0.2006915 Test Loss: 0.2343884
Validation loss decreased (0.201033 --> 0.200691).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2692536
	speed: 1.2633s/iter; left time: 11558.1298s
Epoch: 33 cost time: 74.65433692932129
Epoch: 33, Steps: 136 | Train Loss: 0.2467927 Vali Loss: 0.2010323 Test Loss: 0.2343118
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2265873
	speed: 1.3405s/iter; left time: 12082.3001s
Epoch: 34 cost time: 75.98407888412476
Epoch: 34, Steps: 136 | Train Loss: 0.2468121 Vali Loss: 0.2006395 Test Loss: 0.2342639
Validation loss decreased (0.200691 --> 0.200640).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2518868
	speed: 1.3262s/iter; left time: 11772.2548s
Epoch: 35 cost time: 75.32043671607971
Epoch: 35, Steps: 136 | Train Loss: 0.2466919 Vali Loss: 0.2005956 Test Loss: 0.2341961
Validation loss decreased (0.200640 --> 0.200596).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2433100
	speed: 1.3184s/iter; left time: 11524.1496s
Epoch: 36 cost time: 78.05140995979309
Epoch: 36, Steps: 136 | Train Loss: 0.2466102 Vali Loss: 0.2005848 Test Loss: 0.2341447
Validation loss decreased (0.200596 --> 0.200585).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2473992
	speed: 1.3790s/iter; left time: 11865.9533s
Epoch: 37 cost time: 79.15129399299622
Epoch: 37, Steps: 136 | Train Loss: 0.2466734 Vali Loss: 0.2005380 Test Loss: 0.2341109
Validation loss decreased (0.200585 --> 0.200538).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2462111
	speed: 1.3261s/iter; left time: 11230.8810s
Epoch: 38 cost time: 73.50297975540161
Epoch: 38, Steps: 136 | Train Loss: 0.2466121 Vali Loss: 0.2004850 Test Loss: 0.2340620
Validation loss decreased (0.200538 --> 0.200485).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2355102
	speed: 1.2882s/iter; left time: 10734.2082s
Epoch: 39 cost time: 78.16243028640747
Epoch: 39, Steps: 136 | Train Loss: 0.2466209 Vali Loss: 0.2002771 Test Loss: 0.2340259
Validation loss decreased (0.200485 --> 0.200277).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2416275
	speed: 1.5663s/iter; left time: 12838.7887s
Epoch: 40 cost time: 99.34774231910706
Epoch: 40, Steps: 136 | Train Loss: 0.2464904 Vali Loss: 0.2006458 Test Loss: 0.2339907
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2397819
	speed: 1.9303s/iter; left time: 15560.0025s
Epoch: 41 cost time: 110.57350492477417
Epoch: 41, Steps: 136 | Train Loss: 0.2463712 Vali Loss: 0.2002742 Test Loss: 0.2339620
Validation loss decreased (0.200277 --> 0.200274).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2468811
	speed: 1.8797s/iter; left time: 14896.2803s
Epoch: 42 cost time: 105.80503726005554
Epoch: 42, Steps: 136 | Train Loss: 0.2464203 Vali Loss: 0.2007090 Test Loss: 0.2339225
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2547619
	speed: 1.8905s/iter; left time: 14725.1973s
Epoch: 43 cost time: 106.38051962852478
Epoch: 43, Steps: 136 | Train Loss: 0.2463622 Vali Loss: 0.1998895 Test Loss: 0.2339045
Validation loss decreased (0.200274 --> 0.199889).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2419167
	speed: 1.8396s/iter; left time: 14078.7668s
Epoch: 44 cost time: 103.31015706062317
Epoch: 44, Steps: 136 | Train Loss: 0.2463952 Vali Loss: 0.2000607 Test Loss: 0.2338688
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2492371
	speed: 1.7501s/iter; left time: 13155.7439s
Epoch: 45 cost time: 100.30708909034729
Epoch: 45, Steps: 136 | Train Loss: 0.2462906 Vali Loss: 0.2002310 Test Loss: 0.2338446
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2398587
	speed: 1.5289s/iter; left time: 11284.8059s
Epoch: 46 cost time: 80.76580214500427
Epoch: 46, Steps: 136 | Train Loss: 0.2462778 Vali Loss: 0.2002184 Test Loss: 0.2338250
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2481163
	speed: 1.4203s/iter; left time: 10290.1289s
Epoch: 47 cost time: 79.30917501449585
Epoch: 47, Steps: 136 | Train Loss: 0.2462737 Vali Loss: 0.2003879 Test Loss: 0.2338062
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2493680
	speed: 1.3836s/iter; left time: 9835.9281s
Epoch: 48 cost time: 73.00237131118774
Epoch: 48, Steps: 136 | Train Loss: 0.2462865 Vali Loss: 0.2002375 Test Loss: 0.2337804
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2423067
	speed: 1.3260s/iter; left time: 9246.2460s
Epoch: 49 cost time: 78.03168201446533
Epoch: 49, Steps: 136 | Train Loss: 0.2461718 Vali Loss: 0.2000480 Test Loss: 0.2337737
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2507715
	speed: 1.3125s/iter; left time: 8973.2517s
Epoch: 50 cost time: 74.84207963943481
Epoch: 50, Steps: 136 | Train Loss: 0.2461878 Vali Loss: 0.1999704 Test Loss: 0.2337594
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2441627
	speed: 1.3421s/iter; left time: 8993.5286s
Epoch: 51 cost time: 78.83176279067993
Epoch: 51, Steps: 136 | Train Loss: 0.2461638 Vali Loss: 0.2003191 Test Loss: 0.2337462
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2590634
	speed: 1.3131s/iter; left time: 8620.2173s
Epoch: 52 cost time: 76.36093020439148
Epoch: 52, Steps: 136 | Train Loss: 0.2460959 Vali Loss: 0.2002250 Test Loss: 0.2337323
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2495557
	speed: 1.2597s/iter; left time: 8098.3731s
Epoch: 53 cost time: 72.813232421875
Epoch: 53, Steps: 136 | Train Loss: 0.2461755 Vali Loss: 0.2001980 Test Loss: 0.2337139
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2423190
	speed: 1.3003s/iter; left time: 8182.5733s
Epoch: 54 cost time: 73.58667707443237
Epoch: 54, Steps: 136 | Train Loss: 0.2461324 Vali Loss: 0.2000987 Test Loss: 0.2337082
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2509183
	speed: 1.2801s/iter; left time: 7881.7969s
Epoch: 55 cost time: 71.00175547599792
Epoch: 55, Steps: 136 | Train Loss: 0.2461358 Vali Loss: 0.2000502 Test Loss: 0.2337015
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2529556
	speed: 1.1349s/iter; left time: 6833.1030s
Epoch: 56 cost time: 63.91810750961304
Epoch: 56, Steps: 136 | Train Loss: 0.2461096 Vali Loss: 0.2001835 Test Loss: 0.2336935
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2400586
	speed: 1.4402s/iter; left time: 8475.8214s
Epoch: 57 cost time: 97.4012815952301
Epoch: 57, Steps: 136 | Train Loss: 0.2460243 Vali Loss: 0.2002153 Test Loss: 0.2336887
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2381629
	speed: 1.7029s/iter; left time: 9790.2086s
Epoch: 58 cost time: 98.7420494556427
Epoch: 58, Steps: 136 | Train Loss: 0.2461649 Vali Loss: 0.2000908 Test Loss: 0.2336788
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2440123
	speed: 1.7321s/iter; left time: 9722.2525s
Epoch: 59 cost time: 99.99322938919067
Epoch: 59, Steps: 136 | Train Loss: 0.2460189 Vali Loss: 0.2000424 Test Loss: 0.2336741
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2513093
	speed: 1.6595s/iter; left time: 9089.0236s
Epoch: 60 cost time: 93.9627320766449
Epoch: 60, Steps: 136 | Train Loss: 0.2460915 Vali Loss: 0.2002252 Test Loss: 0.2336600
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2368913
	speed: 1.6001s/iter; left time: 8546.0765s
Epoch: 61 cost time: 92.6151614189148
Epoch: 61, Steps: 136 | Train Loss: 0.2460801 Vali Loss: 0.2001366 Test Loss: 0.2336592
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2409061
	speed: 1.4618s/iter; left time: 7608.4106s
Epoch: 62 cost time: 82.26224207878113
Epoch: 62, Steps: 136 | Train Loss: 0.2460413 Vali Loss: 0.2002222 Test Loss: 0.2336522
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2441330
	speed: 1.4279s/iter; left time: 7238.2288s
Epoch: 63 cost time: 84.28905367851257
Epoch: 63, Steps: 136 | Train Loss: 0.2460444 Vali Loss: 0.2004513 Test Loss: 0.2336520
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_180_j720_H4_FITS_custom_ftM_sl180_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.23232410848140717, mae:0.3199554681777954, rse:0.48081114888191223, corr:[0.44654274 0.44633892 0.4468376  0.44785178 0.44844335 0.44847557
 0.4482113  0.44810498 0.44810975 0.44771323 0.44708022 0.44699895
 0.44724974 0.4468907  0.44666624 0.44683418 0.44693232 0.44685453
 0.4467768  0.44670707 0.44673297 0.44709343 0.4474454  0.4472652
 0.44673705 0.44665748 0.44665056 0.44647673 0.44627532 0.4461608
 0.44599354 0.445777   0.4456708  0.44558388 0.44533992 0.44517598
 0.44531146 0.44526073 0.44513395 0.44504225 0.44504634 0.4450921
 0.445074   0.444942   0.44486606 0.44499308 0.44514453 0.44517058
 0.44504905 0.44500938 0.4448831  0.4447556  0.44469866 0.4446538
 0.44456923 0.44450152 0.44452238 0.44453537 0.44447783 0.44439805
 0.44441745 0.44437766 0.44441926 0.44439733 0.44425932 0.44418857
 0.4441996  0.44412062 0.4440507  0.44416118 0.44425875 0.44415435
 0.44397986 0.44402033 0.44394165 0.44375968 0.44368738 0.44368908
 0.44360965 0.44349554 0.44351214 0.44354102 0.4434925  0.4434975
 0.44359455 0.44348451 0.4433958  0.44340384 0.4434003  0.44333628
 0.4432932  0.44323856 0.44319978 0.4433092  0.44343188 0.44337708
 0.44315237 0.44312507 0.4431124  0.4430368  0.44296923 0.44293925
 0.44286904 0.4427708  0.44276795 0.44282976 0.44281542 0.4427628
 0.44283596 0.4428334  0.44281444 0.44274798 0.4426874  0.44267628
 0.44261402 0.44243762 0.44232044 0.4424051  0.44252697 0.44255024
 0.44246134 0.44252905 0.4425523  0.44252893 0.44254786 0.44257817
 0.44256607 0.44254795 0.44261625 0.44269276 0.44271475 0.44273874
 0.44282547 0.44281992 0.4428316  0.4428056  0.4427132  0.44262972
 0.4426422  0.44269496 0.44279724 0.44304425 0.44328627 0.44333053
 0.44324288 0.4433571  0.44344935 0.44342858 0.4434018  0.44344556
 0.44347394 0.44348037 0.44354847 0.4436204  0.44365707 0.44377083
 0.4440041  0.4441988  0.44418895 0.44403484 0.44407722 0.44423214
 0.44432983 0.44435966 0.44461077 0.4451198  0.44523337 0.44468924
 0.44402194 0.4435591  0.44319525 0.4429266  0.44274482 0.4425867
 0.44241843 0.44227344 0.44218996 0.44206744 0.44190383 0.44182014
 0.44185668 0.44177368 0.44171655 0.44165424 0.4415849  0.44161034
 0.44167194 0.4415909  0.44152334 0.44166237 0.4417074  0.44141695
 0.44101816 0.44088072 0.4407085  0.44048285 0.44038248 0.4403549
 0.4402378  0.44006744 0.44001034 0.43997535 0.4398607  0.43979704
 0.43987033 0.43977055 0.4397314  0.43979046 0.4398199  0.4397985
 0.43977368 0.43966514 0.4395906  0.4396991  0.43982562 0.43975478
 0.43950334 0.4394517  0.43938667 0.4392311  0.43911543 0.43910265
 0.4390859  0.43904144 0.43907022 0.4390969  0.43905342 0.4390127
 0.43909577 0.43903598 0.43895897 0.43888763 0.4388393  0.43883026
 0.43883204 0.43870747 0.43858993 0.43862718 0.4387059  0.43866336
 0.4384901  0.43847212 0.4384003  0.4382714  0.43823215 0.43825728
 0.4382194  0.43812987 0.43813887 0.4381742  0.43815517 0.43814677
 0.43823183 0.43816146 0.43811157 0.43808654 0.4380248  0.4379727
 0.43797675 0.43794525 0.43788394 0.43791336 0.43797123 0.4378985
 0.43769908 0.4377197  0.43771893 0.437626   0.43758616 0.43762156
 0.43759325 0.43751407 0.43752334 0.43757948 0.43757322 0.4375722
 0.43770134 0.4376597  0.4375717  0.43750155 0.43745497 0.43742275
 0.4373521  0.4372021  0.43710068 0.4371494  0.43723926 0.43723083
 0.43708342 0.43715233 0.43722168 0.43723482 0.43728474 0.43738192
 0.4374322  0.43742138 0.4374598  0.43753752 0.43758664 0.43765152
 0.43780825 0.43784344 0.43781134 0.43772063 0.43764117 0.43761852
 0.43764865 0.4376543  0.4376732  0.43782923 0.4379977  0.4380338
 0.4379515  0.43804225 0.43810838 0.4381015  0.4381355  0.43823624
 0.43829975 0.4383379  0.43846    0.43861473 0.43876165 0.4389536
 0.43920094 0.43938515 0.43940794 0.43928    0.43924838 0.43931198
 0.43936053 0.4393356  0.4394385  0.43978435 0.4398181  0.43926418
 0.4386451  0.43833604 0.43810356 0.43783855 0.43761227 0.4374646
 0.43733084 0.43719196 0.43708146 0.43693018 0.4367172  0.43658045
 0.43659347 0.43647352 0.43635544 0.43627033 0.43623945 0.43628505
 0.4363492  0.43632093 0.43632007 0.43650353 0.4365831  0.43635294
 0.43599436 0.43588272 0.43576208 0.43558326 0.4354578  0.4353701
 0.43521792 0.43505818 0.43501207 0.43496475 0.4348044  0.434656
 0.43467677 0.43457958 0.43454146 0.43455023 0.434512   0.43449348
 0.43454614 0.4345397  0.43451667 0.43462357 0.43473637 0.43468177
 0.43449464 0.43454513 0.43454388 0.43439594 0.4342976  0.43432602
 0.43432662 0.43426943 0.43426192 0.43424818 0.43414626 0.4340729
 0.43417385 0.43410566 0.4339834  0.4338994  0.43386778 0.43387476
 0.43388954 0.43384022 0.43380237 0.4339038  0.43401873 0.433987
 0.4337956  0.4337955  0.43378928 0.4337242  0.4337095  0.43374056
 0.43370113 0.43359408 0.43355224 0.43356577 0.43352544 0.43345985
 0.4335259  0.43347168 0.43344033 0.43344578 0.43344653 0.43347648
 0.43352398 0.43346027 0.43337122 0.43344736 0.43359923 0.43362662
 0.4334836  0.4335225  0.43353343 0.43347135 0.43347666 0.43353543
 0.43350288 0.4334011  0.43337086 0.43338016 0.43333542 0.43330294
 0.43338466 0.43328175 0.4331806  0.43316546 0.43315914 0.4331435
 0.433138   0.43310207 0.43307793 0.4331219  0.4332021  0.4332302
 0.43315467 0.43329936 0.43340495 0.43339506 0.43340212 0.4335002
 0.43356627 0.43353587 0.4335281  0.43354696 0.43353575 0.43355808
 0.43374038 0.43378046 0.43371418 0.4336055  0.43356258 0.4335916
 0.43365672 0.43370014 0.43376943 0.43398762 0.43421558 0.43429047
 0.43419936 0.434275   0.43437797 0.43444142 0.4345142  0.4345992
 0.43462944 0.43463707 0.43472788 0.43484646 0.43494102 0.43505985
 0.4352444  0.4353825  0.43540874 0.4353082  0.4352987  0.43540445
 0.43550232 0.43548706 0.43557143 0.43588197 0.43589175 0.43529785
 0.4346013  0.4342153  0.43392557 0.43362543 0.43339625 0.43327418
 0.43314472 0.4329685  0.4328131  0.43264008 0.43242905 0.43230444
 0.43234614 0.43219927 0.43204066 0.4319735  0.43199125 0.43208185
 0.4321778  0.43216795 0.43216646 0.43228748 0.43228194 0.43194938
 0.4314448  0.43122944 0.4311073  0.43096533 0.43086565 0.430799
 0.43066233 0.43047032 0.43032536 0.43018696 0.42998537 0.4298381
 0.4299008  0.4298323  0.42979804 0.42983252 0.4298824  0.42997906
 0.43007967 0.43005815 0.43002173 0.43009624 0.43017283 0.43009123
 0.42982817 0.42973784 0.42961067 0.42940372 0.42930952 0.4293059
 0.42924893 0.4291434  0.42910838 0.42907783 0.4289553  0.42881805
 0.4288489  0.4287227  0.42860854 0.42855594 0.42848444 0.4283922
 0.42836562 0.42837253 0.4283728  0.42844734 0.42850593 0.42839146
 0.42814627 0.42817947 0.42822513 0.4281156  0.42802218 0.42805588
 0.42808735 0.42801687 0.42797437 0.4279757  0.427934   0.42789835
 0.4280456  0.42801464 0.42792323 0.42786443 0.4278753  0.4279365
 0.42798194 0.42793396 0.4278707  0.42791864 0.42797735 0.4279206
 0.42769    0.4276347  0.42760038 0.42755964 0.4276144  0.42770535
 0.427677   0.42755115 0.42748478 0.42748618 0.42748496 0.42748228
 0.4275814  0.42751074 0.42747828 0.42748886 0.42742464 0.4273464
 0.4273341  0.42729396 0.42720142 0.42717582 0.4272193  0.42724872
 0.42719254 0.42737776 0.42750755 0.42749298 0.42754057 0.4276969
 0.42778018 0.42775187 0.4277884  0.4278584  0.42785466 0.42788386
 0.4281132  0.42815733 0.4280863  0.4280423  0.428076   0.42807844
 0.42805094 0.4280573  0.42815053 0.42836535 0.42854616 0.4285749
 0.42842647 0.4285089  0.4286742  0.42876533 0.42880115 0.42884758
 0.42889735 0.42892995 0.4290204  0.42913726 0.42926395 0.429446
 0.42976558 0.4300223  0.4300689  0.42994356 0.429977   0.4301685
 0.43028766 0.43021685 0.43025523 0.43054062 0.43051922 0.42992812
 0.42924422 0.42888004 0.42858344 0.42830324 0.42815384 0.4280446
 0.42782706 0.42755726 0.42738497 0.42722756 0.4270118  0.42688692
 0.4269976  0.42695406 0.4269122  0.42688197 0.42683452 0.42685398
 0.42692327 0.42685053 0.42669487 0.4267027  0.42669967 0.4264134
 0.42595544 0.42163393 0.42153537 0.4213184  0.4211733  0.42114842
 0.42101622 0.42075947 0.42060423 0.42055595 0.42048275 0.42042658
 0.42055002 0.42058182 0.420717   0.42083058 0.42063498 0.4203305
 0.42027104 0.42028797 0.4200725  0.41982085 0.42010543 0.42439383]
