Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H6', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_720_j720_H6_FITS_custom_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3156873216.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6750780
	speed: 1.0732s/iter; left time: 14060.0928s
Epoch: 1 cost time: 139.2410249710083
Epoch: 1, Steps: 132 | Train Loss: 0.8452200 Vali Loss: 0.4998002 Test Loss: 0.6020606
Validation loss decreased (inf --> 0.499800).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4785627
	speed: 2.2783s/iter; left time: 29546.7826s
Epoch: 2 cost time: 139.49660730361938
Epoch: 2, Steps: 132 | Train Loss: 0.5125463 Vali Loss: 0.3439075 Test Loss: 0.4201947
Validation loss decreased (0.499800 --> 0.343908).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3373251
	speed: 2.3204s/iter; left time: 29787.3274s
Epoch: 3 cost time: 141.79655075073242
Epoch: 3, Steps: 132 | Train Loss: 0.3752183 Vali Loss: 0.2617055 Test Loss: 0.3214995
Validation loss decreased (0.343908 --> 0.261706).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2769940
	speed: 2.3627s/iter; left time: 30018.1192s
Epoch: 4 cost time: 138.78689646720886
Epoch: 4, Steps: 132 | Train Loss: 0.3011963 Vali Loss: 0.2192002 Test Loss: 0.2681992
Validation loss decreased (0.261706 --> 0.219200).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2489414
	speed: 2.2850s/iter; left time: 28729.4035s
Epoch: 5 cost time: 141.35971760749817
Epoch: 5, Steps: 132 | Train Loss: 0.2624766 Vali Loss: 0.1978492 Test Loss: 0.2403370
Validation loss decreased (0.219200 --> 0.197849).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2461043
	speed: 2.3133s/iter; left time: 28780.0966s
Epoch: 6 cost time: 137.51039958000183
Epoch: 6, Steps: 132 | Train Loss: 0.2425797 Vali Loss: 0.1879883 Test Loss: 0.2260312
Validation loss decreased (0.197849 --> 0.187988).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2195652
	speed: 2.2598s/iter; left time: 27815.5208s
Epoch: 7 cost time: 135.41272473335266
Epoch: 7, Steps: 132 | Train Loss: 0.2328111 Vali Loss: 0.1838711 Test Loss: 0.2189927
Validation loss decreased (0.187988 --> 0.183871).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2201137
	speed: 2.2291s/iter; left time: 27143.6006s
Epoch: 8 cost time: 135.46672821044922
Epoch: 8, Steps: 132 | Train Loss: 0.2281789 Vali Loss: 0.1817143 Test Loss: 0.2156311
Validation loss decreased (0.183871 --> 0.181714).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2360224
	speed: 2.4097s/iter; left time: 29025.3981s
Epoch: 9 cost time: 146.7836673259735
Epoch: 9, Steps: 132 | Train Loss: 0.2259385 Vali Loss: 0.1810588 Test Loss: 0.2139036
Validation loss decreased (0.181714 --> 0.181059).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2224623
	speed: 2.2874s/iter; left time: 27249.2964s
Epoch: 10 cost time: 138.4915747642517
Epoch: 10, Steps: 132 | Train Loss: 0.2249181 Vali Loss: 0.1811536 Test Loss: 0.2130579
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2180560
	speed: 2.2719s/iter; left time: 26765.1632s
Epoch: 11 cost time: 134.91538858413696
Epoch: 11, Steps: 132 | Train Loss: 0.2244036 Vali Loss: 0.1806979 Test Loss: 0.2125607
Validation loss decreased (0.181059 --> 0.180698).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2202670
	speed: 2.2793s/iter; left time: 26551.6140s
Epoch: 12 cost time: 133.8328037261963
Epoch: 12, Steps: 132 | Train Loss: 0.2240001 Vali Loss: 0.1811118 Test Loss: 0.2122753
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2305719
	speed: 2.2446s/iter; left time: 25850.9762s
Epoch: 13 cost time: 135.80783033370972
Epoch: 13, Steps: 132 | Train Loss: 0.2237997 Vali Loss: 0.1811350 Test Loss: 0.2120294
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2294637
	speed: 2.2695s/iter; left time: 25837.7214s
Epoch: 14 cost time: 136.31881713867188
Epoch: 14, Steps: 132 | Train Loss: 0.2235686 Vali Loss: 0.1807930 Test Loss: 0.2119700
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2231194
	speed: 2.2484s/iter; left time: 25301.5629s
Epoch: 15 cost time: 134.60582518577576
Epoch: 15, Steps: 132 | Train Loss: 0.2235275 Vali Loss: 0.1808597 Test Loss: 0.2118691
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2231851
	speed: 2.1872s/iter; left time: 24324.0879s
Epoch: 16 cost time: 134.59841465950012
Epoch: 16, Steps: 132 | Train Loss: 0.2235160 Vali Loss: 0.1806762 Test Loss: 0.2118304
Validation loss decreased (0.180698 --> 0.180676).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2295392
	speed: 2.2170s/iter; left time: 24363.0662s
Epoch: 17 cost time: 130.98503875732422
Epoch: 17, Steps: 132 | Train Loss: 0.2233955 Vali Loss: 0.1808201 Test Loss: 0.2118078
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2310059
	speed: 2.2377s/iter; left time: 24294.5491s
Epoch: 18 cost time: 140.51801538467407
Epoch: 18, Steps: 132 | Train Loss: 0.2233754 Vali Loss: 0.1808045 Test Loss: 0.2117605
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2160521
	speed: 2.2759s/iter; left time: 24409.2729s
Epoch: 19 cost time: 138.829425573349
Epoch: 19, Steps: 132 | Train Loss: 0.2232502 Vali Loss: 0.1806672 Test Loss: 0.2117069
Validation loss decreased (0.180676 --> 0.180667).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2102836
	speed: 2.3015s/iter; left time: 24380.2143s
Epoch: 20 cost time: 137.78593850135803
Epoch: 20, Steps: 132 | Train Loss: 0.2232578 Vali Loss: 0.1805520 Test Loss: 0.2117150
Validation loss decreased (0.180667 --> 0.180552).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2315267
	speed: 2.2558s/iter; left time: 23598.3721s
Epoch: 21 cost time: 137.11840391159058
Epoch: 21, Steps: 132 | Train Loss: 0.2231711 Vali Loss: 0.1803513 Test Loss: 0.2116511
Validation loss decreased (0.180552 --> 0.180351).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2174941
	speed: 2.3015s/iter; left time: 23772.5179s
Epoch: 22 cost time: 135.667138338089
Epoch: 22, Steps: 132 | Train Loss: 0.2231558 Vali Loss: 0.1803721 Test Loss: 0.2116488
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2191041
	speed: 2.2597s/iter; left time: 23042.0770s
Epoch: 23 cost time: 135.30219435691833
Epoch: 23, Steps: 132 | Train Loss: 0.2231449 Vali Loss: 0.1808203 Test Loss: 0.2116237
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2258327
	speed: 2.3166s/iter; left time: 23316.1741s
Epoch: 24 cost time: 140.83692741394043
Epoch: 24, Steps: 132 | Train Loss: 0.2230879 Vali Loss: 0.1803982 Test Loss: 0.2116229
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2171320
	speed: 2.2895s/iter; left time: 22741.1689s
Epoch: 25 cost time: 139.68406987190247
Epoch: 25, Steps: 132 | Train Loss: 0.2230923 Vali Loss: 0.1803059 Test Loss: 0.2116172
Validation loss decreased (0.180351 --> 0.180306).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2232339
	speed: 2.3335s/iter; left time: 22870.8234s
Epoch: 26 cost time: 143.8001630306244
Epoch: 26, Steps: 132 | Train Loss: 0.2230891 Vali Loss: 0.1806346 Test Loss: 0.2115737
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2219110
	speed: 2.2909s/iter; left time: 22150.6073s
Epoch: 27 cost time: 137.19883179664612
Epoch: 27, Steps: 132 | Train Loss: 0.2230097 Vali Loss: 0.1806436 Test Loss: 0.2115802
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2339161
	speed: 2.3079s/iter; left time: 22010.2199s
Epoch: 28 cost time: 137.82902026176453
Epoch: 28, Steps: 132 | Train Loss: 0.2230178 Vali Loss: 0.1803204 Test Loss: 0.2115875
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2405472
	speed: 2.3186s/iter; left time: 21806.6492s
Epoch: 29 cost time: 140.88394713401794
Epoch: 29, Steps: 132 | Train Loss: 0.2229287 Vali Loss: 0.1804552 Test Loss: 0.2115900
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2289714
	speed: 2.3489s/iter; left time: 21781.6812s
Epoch: 30 cost time: 138.1931574344635
Epoch: 30, Steps: 132 | Train Loss: 0.2229633 Vali Loss: 0.1804706 Test Loss: 0.2115431
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2353459
	speed: 2.3087s/iter; left time: 21104.0095s
Epoch: 31 cost time: 138.8511984348297
Epoch: 31, Steps: 132 | Train Loss: 0.2229145 Vali Loss: 0.1807144 Test Loss: 0.2115420
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2214370
	speed: 2.2956s/iter; left time: 20680.8076s
Epoch: 32 cost time: 137.07836365699768
Epoch: 32, Steps: 132 | Train Loss: 0.2228567 Vali Loss: 0.1804073 Test Loss: 0.2115443
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2148135
	speed: 2.2741s/iter; left time: 20187.2653s
Epoch: 33 cost time: 136.95508551597595
Epoch: 33, Steps: 132 | Train Loss: 0.2229302 Vali Loss: 0.1805672 Test Loss: 0.2115314
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2190978
	speed: 2.0761s/iter; left time: 18155.5542s
Epoch: 34 cost time: 111.38407158851624
Epoch: 34, Steps: 132 | Train Loss: 0.2229099 Vali Loss: 0.1804712 Test Loss: 0.2115197
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2171441
	speed: 1.7383s/iter; left time: 14971.8521s
Epoch: 35 cost time: 104.01106548309326
Epoch: 35, Steps: 132 | Train Loss: 0.2228463 Vali Loss: 0.1800650 Test Loss: 0.2114923
Validation loss decreased (0.180306 --> 0.180065).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2301761
	speed: 1.7074s/iter; left time: 14480.2744s
Epoch: 36 cost time: 102.60460615158081
Epoch: 36, Steps: 132 | Train Loss: 0.2228593 Vali Loss: 0.1801560 Test Loss: 0.2115221
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2223896
	speed: 1.7233s/iter; left time: 14387.5633s
Epoch: 37 cost time: 106.61391139030457
Epoch: 37, Steps: 132 | Train Loss: 0.2228325 Vali Loss: 0.1802063 Test Loss: 0.2114961
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2226407
	speed: 1.7818s/iter; left time: 14641.0676s
Epoch: 38 cost time: 108.86935091018677
Epoch: 38, Steps: 132 | Train Loss: 0.2229003 Vali Loss: 0.1799146 Test Loss: 0.2114977
Validation loss decreased (0.180065 --> 0.179915).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2136947
	speed: 1.7477s/iter; left time: 14130.4870s
Epoch: 39 cost time: 104.08624172210693
Epoch: 39, Steps: 132 | Train Loss: 0.2228331 Vali Loss: 0.1803472 Test Loss: 0.2114712
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2278098
	speed: 1.6901s/iter; left time: 13441.1655s
Epoch: 40 cost time: 104.18770909309387
Epoch: 40, Steps: 132 | Train Loss: 0.2227772 Vali Loss: 0.1802687 Test Loss: 0.2114707
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2088639
	speed: 1.6890s/iter; left time: 13209.5003s
Epoch: 41 cost time: 94.34244275093079
Epoch: 41, Steps: 132 | Train Loss: 0.2228336 Vali Loss: 0.1801399 Test Loss: 0.2114674
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2271342
	speed: 1.2826s/iter; left time: 9862.0107s
Epoch: 42 cost time: 73.18328523635864
Epoch: 42, Steps: 132 | Train Loss: 0.2228342 Vali Loss: 0.1804436 Test Loss: 0.2114744
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2270190
	speed: 1.2304s/iter; left time: 9298.0529s
Epoch: 43 cost time: 72.77350878715515
Epoch: 43, Steps: 132 | Train Loss: 0.2228367 Vali Loss: 0.1800174 Test Loss: 0.2114594
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2183859
	speed: 1.2042s/iter; left time: 8940.8352s
Epoch: 44 cost time: 73.23376059532166
Epoch: 44, Steps: 132 | Train Loss: 0.2227571 Vali Loss: 0.1802170 Test Loss: 0.2114598
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2265239
	speed: 1.2272s/iter; left time: 8949.7208s
Epoch: 45 cost time: 73.68617582321167
Epoch: 45, Steps: 132 | Train Loss: 0.2228372 Vali Loss: 0.1803416 Test Loss: 0.2114596
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2189144
	speed: 1.2230s/iter; left time: 8758.0798s
Epoch: 46 cost time: 73.94839906692505
Epoch: 46, Steps: 132 | Train Loss: 0.2227687 Vali Loss: 0.1802415 Test Loss: 0.2114600
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2265391
	speed: 1.2274s/iter; left time: 8627.3520s
Epoch: 47 cost time: 72.9279625415802
Epoch: 47, Steps: 132 | Train Loss: 0.2227849 Vali Loss: 0.1800012 Test Loss: 0.2114453
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2301333
	speed: 1.2143s/iter; left time: 8375.0929s
Epoch: 48 cost time: 73.26481103897095
Epoch: 48, Steps: 132 | Train Loss: 0.2227718 Vali Loss: 0.1800898 Test Loss: 0.2114460
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2275233
	speed: 1.1702s/iter; left time: 7916.5568s
Epoch: 49 cost time: 69.3512773513794
Epoch: 49, Steps: 132 | Train Loss: 0.2227081 Vali Loss: 0.1802720 Test Loss: 0.2114476
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2159331
	speed: 1.2084s/iter; left time: 8015.0063s
Epoch: 50 cost time: 73.54044556617737
Epoch: 50, Steps: 132 | Train Loss: 0.2227432 Vali Loss: 0.1803085 Test Loss: 0.2114437
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2291357
	speed: 1.2332s/iter; left time: 8017.3099s
Epoch: 51 cost time: 74.53828763961792
Epoch: 51, Steps: 132 | Train Loss: 0.2227117 Vali Loss: 0.1800839 Test Loss: 0.2114527
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2345067
	speed: 1.2528s/iter; left time: 7978.9219s
Epoch: 52 cost time: 77.19945406913757
Epoch: 52, Steps: 132 | Train Loss: 0.2227656 Vali Loss: 0.1802046 Test Loss: 0.2114399
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2195204
	speed: 1.2927s/iter; left time: 8062.3553s
Epoch: 53 cost time: 76.827077627182
Epoch: 53, Steps: 132 | Train Loss: 0.2227887 Vali Loss: 0.1802429 Test Loss: 0.2114352
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2227302
	speed: 1.2670s/iter; left time: 7734.8360s
Epoch: 54 cost time: 77.28657269477844
Epoch: 54, Steps: 132 | Train Loss: 0.2227132 Vali Loss: 0.1798657 Test Loss: 0.2114397
Validation loss decreased (0.179915 --> 0.179866).  Saving model ...
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2239499
	speed: 1.3064s/iter; left time: 7803.3605s
Epoch: 55 cost time: 78.51517248153687
Epoch: 55, Steps: 132 | Train Loss: 0.2227547 Vali Loss: 0.1799997 Test Loss: 0.2114324
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2150728
	speed: 1.2777s/iter; left time: 7462.8240s
Epoch: 56 cost time: 76.92411756515503
Epoch: 56, Steps: 132 | Train Loss: 0.2227703 Vali Loss: 0.1802689 Test Loss: 0.2114275
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2390417
	speed: 1.2811s/iter; left time: 7313.9432s
Epoch: 57 cost time: 76.99485421180725
Epoch: 57, Steps: 132 | Train Loss: 0.2227909 Vali Loss: 0.1801336 Test Loss: 0.2114296
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2166319
	speed: 1.2698s/iter; left time: 7081.6644s
Epoch: 58 cost time: 75.69551086425781
Epoch: 58, Steps: 132 | Train Loss: 0.2226926 Vali Loss: 0.1801537 Test Loss: 0.2114325
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2220275
	speed: 1.2523s/iter; left time: 6818.6105s
Epoch: 59 cost time: 76.82833003997803
Epoch: 59, Steps: 132 | Train Loss: 0.2227151 Vali Loss: 0.1802279 Test Loss: 0.2114231
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2207435
	speed: 1.3023s/iter; left time: 6919.1364s
Epoch: 60 cost time: 81.38992619514465
Epoch: 60, Steps: 132 | Train Loss: 0.2227223 Vali Loss: 0.1801614 Test Loss: 0.2114276
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2168841
	speed: 1.2838s/iter; left time: 6651.1299s
Epoch: 61 cost time: 76.09079360961914
Epoch: 61, Steps: 132 | Train Loss: 0.2227040 Vali Loss: 0.1801192 Test Loss: 0.2114269
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2189395
	speed: 1.2689s/iter; left time: 6406.7939s
Epoch: 62 cost time: 76.50124025344849
Epoch: 62, Steps: 132 | Train Loss: 0.2227305 Vali Loss: 0.1800780 Test Loss: 0.2114260
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2184919
	speed: 1.2562s/iter; left time: 6176.9279s
Epoch: 63 cost time: 74.4032871723175
Epoch: 63, Steps: 132 | Train Loss: 0.2227222 Vali Loss: 0.1800449 Test Loss: 0.2114225
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2310224
	speed: 1.2356s/iter; left time: 5912.3017s
Epoch: 64 cost time: 74.08803105354309
Epoch: 64, Steps: 132 | Train Loss: 0.2227218 Vali Loss: 0.1802430 Test Loss: 0.2114224
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2195823
	speed: 1.1964s/iter; left time: 5567.0373s
Epoch: 65 cost time: 71.87789463996887
Epoch: 65, Steps: 132 | Train Loss: 0.2227052 Vali Loss: 0.1801574 Test Loss: 0.2114211
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2244983
	speed: 1.1902s/iter; left time: 5380.9180s
Epoch: 66 cost time: 73.39961194992065
Epoch: 66, Steps: 132 | Train Loss: 0.2226922 Vali Loss: 0.1801421 Test Loss: 0.2114146
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2363849
	speed: 1.2897s/iter; left time: 5660.3351s
Epoch: 67 cost time: 77.07440400123596
Epoch: 67, Steps: 132 | Train Loss: 0.2226601 Vali Loss: 0.1800997 Test Loss: 0.2114188
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2296711
	speed: 1.2416s/iter; left time: 5285.2876s
Epoch: 68 cost time: 74.11629557609558
Epoch: 68, Steps: 132 | Train Loss: 0.2226967 Vali Loss: 0.1798562 Test Loss: 0.2114199
Validation loss decreased (0.179866 --> 0.179856).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2126140
	speed: 1.2530s/iter; left time: 5168.5317s
Epoch: 69 cost time: 75.82551550865173
Epoch: 69, Steps: 132 | Train Loss: 0.2227024 Vali Loss: 0.1802568 Test Loss: 0.2114197
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2321074
	speed: 1.2634s/iter; left time: 5044.9342s
Epoch: 70 cost time: 76.03385019302368
Epoch: 70, Steps: 132 | Train Loss: 0.2227106 Vali Loss: 0.1804554 Test Loss: 0.2114188
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2252838
	speed: 1.2492s/iter; left time: 4823.1436s
Epoch: 71 cost time: 75.49164271354675
Epoch: 71, Steps: 132 | Train Loss: 0.2226809 Vali Loss: 0.1799534 Test Loss: 0.2114158
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2204857
	speed: 1.2428s/iter; left time: 4634.3701s
Epoch: 72 cost time: 76.36574578285217
Epoch: 72, Steps: 132 | Train Loss: 0.2226581 Vali Loss: 0.1799568 Test Loss: 0.2114138
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2270402
	speed: 1.2809s/iter; left time: 4607.4263s
Epoch: 73 cost time: 78.60172939300537
Epoch: 73, Steps: 132 | Train Loss: 0.2226531 Vali Loss: 0.1803949 Test Loss: 0.2114176
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.2228924
	speed: 1.2479s/iter; left time: 4323.8165s
Epoch: 74 cost time: 74.74351048469543
Epoch: 74, Steps: 132 | Train Loss: 0.2226814 Vali Loss: 0.1801498 Test Loss: 0.2114124
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2286637
	speed: 1.2489s/iter; left time: 4162.6100s
Epoch: 75 cost time: 73.4434564113617
Epoch: 75, Steps: 132 | Train Loss: 0.2226942 Vali Loss: 0.1800704 Test Loss: 0.2114151
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2094549
	speed: 1.1859s/iter; left time: 3796.1294s
Epoch: 76 cost time: 71.53155565261841
Epoch: 76, Steps: 132 | Train Loss: 0.2227103 Vali Loss: 0.1802315 Test Loss: 0.2114087
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2217801
	speed: 1.2301s/iter; left time: 3775.1917s
Epoch: 77 cost time: 72.7772068977356
Epoch: 77, Steps: 132 | Train Loss: 0.2226806 Vali Loss: 0.1802921 Test Loss: 0.2114091
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.2123873
	speed: 1.2319s/iter; left time: 3618.0767s
Epoch: 78 cost time: 73.95519971847534
Epoch: 78, Steps: 132 | Train Loss: 0.2226107 Vali Loss: 0.1799282 Test Loss: 0.2114093
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.2157464
	speed: 1.1819s/iter; left time: 3315.1145s
Epoch: 79 cost time: 71.97041964530945
Epoch: 79, Steps: 132 | Train Loss: 0.2226864 Vali Loss: 0.1801587 Test Loss: 0.2114101
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.2208949
	speed: 1.1980s/iter; left time: 3202.3239s
Epoch: 80 cost time: 73.40549993515015
Epoch: 80, Steps: 132 | Train Loss: 0.2226945 Vali Loss: 0.1802069 Test Loss: 0.2114091
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.2358692
	speed: 1.2630s/iter; left time: 3209.2127s
Epoch: 81 cost time: 78.04919028282166
Epoch: 81, Steps: 132 | Train Loss: 0.2226599 Vali Loss: 0.1801932 Test Loss: 0.2114096
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.2150504
	speed: 1.1525s/iter; left time: 2776.3162s
Epoch: 82 cost time: 64.64906716346741
Epoch: 82, Steps: 132 | Train Loss: 0.2226547 Vali Loss: 0.1799830 Test Loss: 0.2114112
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.2323822
	speed: 1.0035s/iter; left time: 2285.0328s
Epoch: 83 cost time: 58.31779742240906
Epoch: 83, Steps: 132 | Train Loss: 0.2227066 Vali Loss: 0.1800918 Test Loss: 0.2114107
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.2288273
	speed: 0.9572s/iter; left time: 2053.2140s
Epoch: 84 cost time: 57.99807119369507
Epoch: 84, Steps: 132 | Train Loss: 0.2226531 Vali Loss: 0.1801887 Test Loss: 0.2114079
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.2245861
	speed: 0.9464s/iter; left time: 1905.0401s
Epoch: 85 cost time: 56.56423902511597
Epoch: 85, Steps: 132 | Train Loss: 0.2225992 Vali Loss: 0.1801563 Test Loss: 0.2114080
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.2167864
	speed: 0.9765s/iter; left time: 1836.7458s
Epoch: 86 cost time: 59.74578332901001
Epoch: 86, Steps: 132 | Train Loss: 0.2226749 Vali Loss: 0.1800284 Test Loss: 0.2114072
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.2277166
	speed: 0.9712s/iter; left time: 1698.5550s
Epoch: 87 cost time: 59.52650213241577
Epoch: 87, Steps: 132 | Train Loss: 0.2227267 Vali Loss: 0.1800404 Test Loss: 0.2114090
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.2478528
	speed: 0.9500s/iter; left time: 1536.1497s
Epoch: 88 cost time: 55.799357414245605
Epoch: 88, Steps: 132 | Train Loss: 0.2226182 Vali Loss: 0.1799779 Test Loss: 0.2114072
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_720_j720_H6_FITS_custom_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2097349762916565, mae:0.301852285861969, rse:0.4568386375904083, corr:[0.44764245 0.4465688  0.449238   0.4501141  0.4501864  0.45083594
 0.45109048 0.45081985 0.45044878 0.45006657 0.449806   0.44973093
 0.44953227 0.4492932  0.4493587  0.449532   0.44945693 0.44942
 0.449624   0.44967943 0.4495652  0.4496681  0.4500059  0.45024866
 0.45034328 0.45063326 0.4507984  0.4507453  0.45055366 0.45032644
 0.4501114  0.44993946 0.44976673 0.44958603 0.44943202 0.44941175
 0.44946882 0.44947878 0.44947302 0.44953573 0.44961265 0.4496287
 0.44962177 0.4495929  0.449583   0.44956487 0.44956604 0.44968686
 0.44987148 0.45000556 0.4499452  0.44985348 0.4497817  0.44965315
 0.44948187 0.44935206 0.44928026 0.4492455  0.449208   0.44914642
 0.4491216  0.44919777 0.4493023  0.4493209  0.44930503 0.4493711
 0.44941947 0.44926855 0.44913918 0.44915542 0.4491834  0.44913515
 0.4491383  0.44926548 0.44927108 0.44916618 0.44905615 0.44896677
 0.44887286 0.44876269 0.44867224 0.44861332 0.44860366 0.44864345
 0.44865367 0.4486263  0.44865233 0.44873148 0.44876757 0.448756
 0.44875786 0.44871005 0.44862738 0.44857576 0.44856167 0.44859087
 0.4486387  0.4486756  0.44863743 0.44859722 0.44852763 0.44842678
 0.4483348  0.44827574 0.44823158 0.44819915 0.4481548  0.44812733
 0.44814262 0.44819576 0.4482234  0.4481928  0.4481786  0.44823638
 0.4482319  0.44807264 0.4479785  0.4479879  0.44798407 0.44800758
 0.44815615 0.4482838  0.4482742  0.44821537 0.44814125 0.44809663
 0.44808164 0.44805297 0.4480012  0.4479526  0.44794196 0.44796467
 0.44797242 0.4479648  0.44800484 0.4480672  0.44809353 0.4481042
 0.44814655 0.44816405 0.44815013 0.44811034 0.44806936 0.44805965
 0.4480444  0.44803387 0.4480178  0.44804522 0.44802123 0.4479413
 0.44788313 0.44788292 0.44788897 0.44785148 0.44779962 0.44778824
 0.44782725 0.4478738  0.44789848 0.4478742  0.447882   0.447947
 0.4479858  0.44789013 0.44780558 0.44776434 0.44760472 0.4474198
 0.4473376  0.44741005 0.4473601  0.44727847 0.4472432  0.44720203
 0.4471143  0.44700053 0.44691133 0.44688556 0.44688186 0.44686392
 0.44682175 0.44680464 0.44684267 0.44685066 0.446796   0.44676602
 0.44673705 0.44653744 0.4463216  0.44620302 0.4461289  0.44608647
 0.44609755 0.44621122 0.44623855 0.44621766 0.4461826  0.4461165
 0.44603583 0.44598517 0.44595584 0.44588438 0.4457989  0.44578987
 0.4458324  0.4458324  0.4458162  0.44580927 0.44580925 0.44579577
 0.44572175 0.44549364 0.4453064  0.44523638 0.44518542 0.44517043
 0.44524685 0.4453994  0.44543242 0.44541556 0.4454163  0.4454016
 0.44534504 0.44528437 0.44526827 0.4452564  0.4452255  0.44517496
 0.4451383  0.44514084 0.4451676  0.44513008 0.44505528 0.44501868
 0.44500384 0.44482926 0.44464263 0.4445663  0.4445704  0.4445835
 0.4446251  0.44474947 0.44480652 0.44480664 0.4447898  0.4447639
 0.44473463 0.44469753 0.4446632  0.44460452 0.44454443 0.4445368
 0.4445422  0.4445114  0.44446814 0.44444275 0.44442117 0.44439414
 0.44434965 0.44424942 0.44417116 0.44412416 0.444089   0.44410643
 0.4442145  0.44433635 0.44435138 0.44434705 0.44436038 0.44435397
 0.44429362 0.44421414 0.444161   0.44413674 0.44410166 0.44404423
 0.44401395 0.44402674 0.44406012 0.4440259  0.44396394 0.44396758
 0.44394478 0.44377825 0.44365394 0.4436428  0.44367388 0.44374827
 0.44393075 0.444111   0.4441913  0.44422373 0.44420576 0.44418615
 0.4441949  0.44419008 0.44413927 0.44405127 0.44398582 0.44397765
 0.443971   0.4439267  0.44389254 0.44389153 0.44389072 0.4438722
 0.44386277 0.44384745 0.44384363 0.44382933 0.44378212 0.44377053
 0.44380736 0.44386646 0.44388312 0.44391444 0.44393143 0.44391036
 0.4438673  0.44383326 0.44382152 0.44379017 0.44371933 0.44363806
 0.4436097  0.44363362 0.4436495  0.44359848 0.44356924 0.44362795
 0.4436601  0.4435185  0.4433591  0.44329268 0.44314682 0.44294977
 0.44284922 0.44294554 0.44293582 0.4428817  0.44284594 0.44279903
 0.44271693 0.44262812 0.4425402  0.44245246 0.4423642  0.44229847
 0.44224632 0.4421927  0.44218    0.44215912 0.44209677 0.44205138
 0.44203883 0.4419391  0.44181484 0.4417442  0.4416826  0.44165242
 0.44167626 0.4417932  0.44183856 0.44185176 0.4418379  0.4417758
 0.44168818 0.4416311  0.44159406 0.44151625 0.44140318 0.4413288
 0.44132414 0.44131523 0.4412775  0.44120932 0.44116655 0.44116607
 0.44112268 0.4409349  0.4408063  0.4408015  0.4407807  0.44074553
 0.44080925 0.44097793 0.441051   0.4410455  0.44104457 0.44106305
 0.44104263 0.44098732 0.44091278 0.440836   0.44077465 0.4407373
 0.44068602 0.44062132 0.44060743 0.44060138 0.4405538  0.44051313
 0.44050017 0.44038513 0.44023302 0.44016525 0.4401688  0.4402032
 0.4402485  0.4403435  0.44042945 0.44051158 0.44054624 0.4405176
 0.44045317 0.44039515 0.44035175 0.44031176 0.4402616  0.44020516
 0.4402053  0.4402042  0.44016513 0.44009852 0.44006765 0.44008926
 0.440069   0.43991777 0.43982622 0.4398471  0.43985456 0.439851
 0.43994528 0.44009185 0.44014564 0.44017145 0.44021854 0.44024897
 0.4402269  0.4401583  0.44008735 0.44001314 0.43993875 0.43988
 0.43983054 0.43979678 0.43980375 0.4397868  0.43971446 0.4396784
 0.43965754 0.43953672 0.43942004 0.43937522 0.43943125 0.4395806
 0.43978876 0.43993118 0.440019   0.44012138 0.44014335 0.44010234
 0.4400941  0.4401012  0.44006455 0.43996075 0.43984488 0.4397788
 0.43977758 0.43977097 0.43974975 0.43971777 0.43970641 0.43971488
 0.4397161  0.43968692 0.4396966  0.4397367  0.43971646 0.43971297
 0.43980566 0.4399271  0.43995625 0.43999812 0.44007167 0.44010466
 0.44005358 0.43998644 0.43996075 0.4399343  0.439873   0.43978474
 0.43971312 0.4396999  0.43973342 0.43971837 0.4396752  0.4397012
 0.43974537 0.4396382  0.43950284 0.4394378  0.43934622 0.43919897
 0.43906993 0.4391263  0.43915525 0.43915516 0.43911526 0.43904808
 0.43898427 0.43892404 0.43880737 0.43864024 0.43849877 0.43844372
 0.43841383 0.43832988 0.43825397 0.43822283 0.43819556 0.4381695
 0.4381126  0.43795156 0.43782642 0.43777034 0.43769822 0.43764958
 0.4376871  0.43781993 0.43785512 0.4378592  0.43788543 0.43787637
 0.43776733 0.43762645 0.43751118 0.4374003  0.437242   0.43709147
 0.43701622 0.43700168 0.43699327 0.43691507 0.43682194 0.43683505
 0.43685195 0.43669027 0.4365615  0.43655744 0.43660483 0.43662292
 0.43666148 0.43679184 0.4368711  0.4368627  0.43681148 0.43676266
 0.43673316 0.43667752 0.4365442  0.4363443  0.4361649  0.43605384
 0.43596157 0.43581623 0.4357085  0.4356487  0.4355611  0.43542716
 0.4353298  0.43527168 0.4352333  0.43520698 0.43521145 0.43526697
 0.43539822 0.43555227 0.4356187  0.43566248 0.43570104 0.43565857
 0.43555424 0.4354544  0.43536472 0.43524572 0.4350665  0.43491125
 0.43485862 0.43484855 0.43482038 0.4347289  0.4346639  0.43468535
 0.43470585 0.4345916  0.4345115  0.4345597  0.434613   0.4346178
 0.43472075 0.43491352 0.43499926 0.4349909  0.43496695 0.43494797
 0.43488646 0.43475276 0.43457058 0.43441328 0.43430382 0.43422917
 0.43413243 0.43402666 0.4340159  0.43406016 0.43403304 0.43398342
 0.43398863 0.43398026 0.43396598 0.43397793 0.43405658 0.4342413
 0.43450558 0.4346503  0.4347061  0.43478614 0.43478298 0.43465862
 0.43453777 0.43445688 0.43434995 0.43414503 0.43390858 0.43377104
 0.43375427 0.43375614 0.43372107 0.43366748 0.43369007 0.43377706
 0.43383178 0.43383452 0.4339051  0.43403614 0.43406698 0.43404856
 0.4341698  0.43437612 0.43438995 0.43430218 0.4342768  0.43426317
 0.43412936 0.4339069  0.43374497 0.43365112 0.4335415  0.4333736
 0.4332404  0.43326256 0.43339002 0.43343425 0.43339926 0.43347225
 0.4336434  0.43364656 0.4335562  0.43355736 0.4336186  0.43354905
 0.43332186 0.43326303 0.4332503  0.43318155 0.43295783 0.43264487
 0.432446   0.43234575 0.43210447 0.43172583 0.43150157 0.43153417
 0.4315885  0.43152365 0.43153864 0.4317111  0.4318706  0.43191555
 0.4318967  0.43183398 0.43182757 0.4318265  0.43173715 0.4316841
 0.4317513  0.4317439  0.4314167  0.43116248 0.43115017 0.43092975
 0.4304314  0.4302396  0.43038946 0.430278   0.42998168 0.4301691
 0.4307323  0.4310604  0.431057   0.43105933 0.43122312 0.4314025
 0.4308933  0.42556185 0.429859   0.42985716 0.42386168 0.43093154]
