Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_90_j720_H6', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_90_j720_H6_FITS_custom_ftM_sl90_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17603
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=34, out_features=306, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  427479552.0
params:  10710.0
Trainable parameters:  10710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.1286688
	speed: 0.8016s/iter; left time: 10903.1380s
Epoch: 1 cost time: 106.55227828025818
Epoch: 1, Steps: 137 | Train Loss: 1.6267902 Vali Loss: 0.7945746 Test Loss: 0.8785440
Validation loss decreased (inf --> 0.794575).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6374929
	speed: 1.8399s/iter; left time: 24772.2238s
Epoch: 2 cost time: 110.73806309700012
Epoch: 2, Steps: 137 | Train Loss: 0.7159106 Vali Loss: 0.4856268 Test Loss: 0.5452188
Validation loss decreased (0.794575 --> 0.485627).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4650989
	speed: 1.9606s/iter; left time: 26128.8541s
Epoch: 3 cost time: 112.0048611164093
Epoch: 3, Steps: 137 | Train Loss: 0.4942121 Vali Loss: 0.3668094 Test Loss: 0.4142938
Validation loss decreased (0.485627 --> 0.366809).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3795920
	speed: 1.9585s/iter; left time: 25832.2658s
Epoch: 4 cost time: 107.89142060279846
Epoch: 4, Steps: 137 | Train Loss: 0.3978013 Vali Loss: 0.3104789 Test Loss: 0.3515716
Validation loss decreased (0.366809 --> 0.310479).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3466503
	speed: 1.9761s/iter; left time: 25793.9097s
Epoch: 5 cost time: 113.9761962890625
Epoch: 5, Steps: 137 | Train Loss: 0.3505341 Vali Loss: 0.2824244 Test Loss: 0.3195308
Validation loss decreased (0.310479 --> 0.282424).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3172655
	speed: 1.8664s/iter; left time: 24106.6393s
Epoch: 6 cost time: 108.60611033439636
Epoch: 6, Steps: 137 | Train Loss: 0.3258439 Vali Loss: 0.2666197 Test Loss: 0.3020876
Validation loss decreased (0.282424 --> 0.266620).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3022641
	speed: 1.9526s/iter; left time: 24952.1004s
Epoch: 7 cost time: 112.07874011993408
Epoch: 7, Steps: 137 | Train Loss: 0.3119167 Vali Loss: 0.2578827 Test Loss: 0.2918535
Validation loss decreased (0.266620 --> 0.257883).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3169173
	speed: 1.8864s/iter; left time: 23847.9656s
Epoch: 8 cost time: 108.04091024398804
Epoch: 8, Steps: 137 | Train Loss: 0.3034715 Vali Loss: 0.2523242 Test Loss: 0.2853840
Validation loss decreased (0.257883 --> 0.252324).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2919911
	speed: 1.9027s/iter; left time: 23793.5095s
Epoch: 9 cost time: 104.10179376602173
Epoch: 9, Steps: 137 | Train Loss: 0.2979744 Vali Loss: 0.2483328 Test Loss: 0.2809913
Validation loss decreased (0.252324 --> 0.248333).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3030028
	speed: 1.9059s/iter; left time: 23572.7007s
Epoch: 10 cost time: 111.7763500213623
Epoch: 10, Steps: 137 | Train Loss: 0.2940872 Vali Loss: 0.2450709 Test Loss: 0.2778316
Validation loss decreased (0.248333 --> 0.245071).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2879471
	speed: 2.0328s/iter; left time: 24862.6912s
Epoch: 11 cost time: 117.16745162010193
Epoch: 11, Steps: 137 | Train Loss: 0.2911230 Vali Loss: 0.2424821 Test Loss: 0.2754689
Validation loss decreased (0.245071 --> 0.242482).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2902735
	speed: 1.9980s/iter; left time: 24164.1629s
Epoch: 12 cost time: 112.59900069236755
Epoch: 12, Steps: 137 | Train Loss: 0.2890532 Vali Loss: 0.2411439 Test Loss: 0.2736743
Validation loss decreased (0.242482 --> 0.241144).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2879125
	speed: 1.9286s/iter; left time: 23060.1840s
Epoch: 13 cost time: 110.28815484046936
Epoch: 13, Steps: 137 | Train Loss: 0.2872849 Vali Loss: 0.2394777 Test Loss: 0.2722470
Validation loss decreased (0.241144 --> 0.239478).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2931021
	speed: 1.8875s/iter; left time: 22309.9370s
Epoch: 14 cost time: 116.15198969841003
Epoch: 14, Steps: 137 | Train Loss: 0.2859235 Vali Loss: 0.2389261 Test Loss: 0.2711097
Validation loss decreased (0.239478 --> 0.238926).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2781073
	speed: 1.9132s/iter; left time: 22351.3396s
Epoch: 15 cost time: 100.10095000267029
Epoch: 15, Steps: 137 | Train Loss: 0.2848550 Vali Loss: 0.2381614 Test Loss: 0.2701734
Validation loss decreased (0.238926 --> 0.238161).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2756285
	speed: 1.7904s/iter; left time: 20671.8018s
Epoch: 16 cost time: 99.31059622764587
Epoch: 16, Steps: 137 | Train Loss: 0.2838249 Vali Loss: 0.2376993 Test Loss: 0.2693789
Validation loss decreased (0.238161 --> 0.237699).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2883615
	speed: 1.8307s/iter; left time: 20886.3367s
Epoch: 17 cost time: 106.08627200126648
Epoch: 17, Steps: 137 | Train Loss: 0.2831163 Vali Loss: 0.2369585 Test Loss: 0.2687367
Validation loss decreased (0.237699 --> 0.236958).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2705061
	speed: 1.8691s/iter; left time: 21069.0197s
Epoch: 18 cost time: 106.07597994804382
Epoch: 18, Steps: 137 | Train Loss: 0.2825406 Vali Loss: 0.2367119 Test Loss: 0.2681791
Validation loss decreased (0.236958 --> 0.236712).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2970542
	speed: 1.8321s/iter; left time: 20400.5242s
Epoch: 19 cost time: 101.76466298103333
Epoch: 19, Steps: 137 | Train Loss: 0.2819366 Vali Loss: 0.2358283 Test Loss: 0.2677116
Validation loss decreased (0.236712 --> 0.235828).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2803335
	speed: 1.8168s/iter; left time: 19980.6821s
Epoch: 20 cost time: 110.20474410057068
Epoch: 20, Steps: 137 | Train Loss: 0.2814278 Vali Loss: 0.2354512 Test Loss: 0.2672877
Validation loss decreased (0.235828 --> 0.235451).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2760885
	speed: 2.1324s/iter; left time: 23160.1894s
Epoch: 21 cost time: 122.92715668678284
Epoch: 21, Steps: 137 | Train Loss: 0.2810874 Vali Loss: 0.2352731 Test Loss: 0.2669104
Validation loss decreased (0.235451 --> 0.235273).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2766550
	speed: 2.1130s/iter; left time: 22660.2944s
Epoch: 22 cost time: 121.99722599983215
Epoch: 22, Steps: 137 | Train Loss: 0.2806976 Vali Loss: 0.2346839 Test Loss: 0.2665673
Validation loss decreased (0.235273 --> 0.234684).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2661615
	speed: 2.2009s/iter; left time: 23301.0741s
Epoch: 23 cost time: 123.08287405967712
Epoch: 23, Steps: 137 | Train Loss: 0.2803662 Vali Loss: 0.2346467 Test Loss: 0.2662808
Validation loss decreased (0.234684 --> 0.234647).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2794116
	speed: 2.0583s/iter; left time: 21509.1859s
Epoch: 24 cost time: 109.06430149078369
Epoch: 24, Steps: 137 | Train Loss: 0.2800243 Vali Loss: 0.2343008 Test Loss: 0.2660164
Validation loss decreased (0.234647 --> 0.234301).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2865242
	speed: 1.7733s/iter; left time: 18287.8193s
Epoch: 25 cost time: 109.04063749313354
Epoch: 25, Steps: 137 | Train Loss: 0.2797867 Vali Loss: 0.2343169 Test Loss: 0.2657782
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2713962
	speed: 1.9381s/iter; left time: 19722.2123s
Epoch: 26 cost time: 104.19919466972351
Epoch: 26, Steps: 137 | Train Loss: 0.2795440 Vali Loss: 0.2338774 Test Loss: 0.2655552
Validation loss decreased (0.234301 --> 0.233877).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2892005
	speed: 1.7299s/iter; left time: 17366.4757s
Epoch: 27 cost time: 101.05364346504211
Epoch: 27, Steps: 137 | Train Loss: 0.2792797 Vali Loss: 0.2340151 Test Loss: 0.2653351
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2900513
	speed: 1.8009s/iter; left time: 17832.5360s
Epoch: 28 cost time: 100.15542650222778
Epoch: 28, Steps: 137 | Train Loss: 0.2790023 Vali Loss: 0.2331247 Test Loss: 0.2651711
Validation loss decreased (0.233877 --> 0.233125).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2520241
	speed: 1.7654s/iter; left time: 17239.1798s
Epoch: 29 cost time: 101.4553337097168
Epoch: 29, Steps: 137 | Train Loss: 0.2788362 Vali Loss: 0.2325441 Test Loss: 0.2649947
Validation loss decreased (0.233125 --> 0.232544).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2777522
	speed: 1.7220s/iter; left time: 16579.2451s
Epoch: 30 cost time: 101.0420413017273
Epoch: 30, Steps: 137 | Train Loss: 0.2786977 Vali Loss: 0.2331404 Test Loss: 0.2648352
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2759024
	speed: 1.7528s/iter; left time: 16635.7824s
Epoch: 31 cost time: 101.7088074684143
Epoch: 31, Steps: 137 | Train Loss: 0.2785232 Vali Loss: 0.2326136 Test Loss: 0.2646947
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2774491
	speed: 1.8778s/iter; left time: 17564.8970s
Epoch: 32 cost time: 105.53887605667114
Epoch: 32, Steps: 137 | Train Loss: 0.2783071 Vali Loss: 0.2329614 Test Loss: 0.2645583
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2626617
	speed: 1.7664s/iter; left time: 16281.3046s
Epoch: 33 cost time: 95.78044056892395
Epoch: 33, Steps: 137 | Train Loss: 0.2781944 Vali Loss: 0.2331461 Test Loss: 0.2644309
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2687684
	speed: 1.6638s/iter; left time: 15107.1259s
Epoch: 34 cost time: 97.34779214859009
Epoch: 34, Steps: 137 | Train Loss: 0.2780879 Vali Loss: 0.2325238 Test Loss: 0.2643047
Validation loss decreased (0.232544 --> 0.232524).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2830887
	speed: 1.6817s/iter; left time: 15039.2710s
Epoch: 35 cost time: 93.9295608997345
Epoch: 35, Steps: 137 | Train Loss: 0.2779259 Vali Loss: 0.2326757 Test Loss: 0.2642009
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2845350
	speed: 1.6560s/iter; left time: 14582.9383s
Epoch: 36 cost time: 96.59340906143188
Epoch: 36, Steps: 137 | Train Loss: 0.2778130 Vali Loss: 0.2324048 Test Loss: 0.2641001
Validation loss decreased (0.232524 --> 0.232405).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2768412
	speed: 1.5847s/iter; left time: 13737.8054s
Epoch: 37 cost time: 87.39677453041077
Epoch: 37, Steps: 137 | Train Loss: 0.2777167 Vali Loss: 0.2319014 Test Loss: 0.2639997
Validation loss decreased (0.232405 --> 0.231901).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2827231
	speed: 1.5460s/iter; left time: 13190.6855s
Epoch: 38 cost time: 87.74548768997192
Epoch: 38, Steps: 137 | Train Loss: 0.2776464 Vali Loss: 0.2324659 Test Loss: 0.2639143
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2712233
	speed: 1.5701s/iter; left time: 13181.2064s
Epoch: 39 cost time: 89.6508378982544
Epoch: 39, Steps: 137 | Train Loss: 0.2775769 Vali Loss: 0.2322875 Test Loss: 0.2638290
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2897795
	speed: 1.5320s/iter; left time: 12651.4196s
Epoch: 40 cost time: 87.62198424339294
Epoch: 40, Steps: 137 | Train Loss: 0.2773929 Vali Loss: 0.2315591 Test Loss: 0.2637617
Validation loss decreased (0.231901 --> 0.231559).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2786480
	speed: 1.5066s/iter; left time: 12234.8832s
Epoch: 41 cost time: 84.32175421714783
Epoch: 41, Steps: 137 | Train Loss: 0.2773845 Vali Loss: 0.2317073 Test Loss: 0.2636859
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2770194
	speed: 1.1558s/iter; left time: 9228.0751s
Epoch: 42 cost time: 59.74144625663757
Epoch: 42, Steps: 137 | Train Loss: 0.2772323 Vali Loss: 0.2320163 Test Loss: 0.2636169
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2747834
	speed: 1.0537s/iter; left time: 8268.5873s
Epoch: 43 cost time: 59.09596300125122
Epoch: 43, Steps: 137 | Train Loss: 0.2772496 Vali Loss: 0.2317879 Test Loss: 0.2635526
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2682481
	speed: 1.0849s/iter; left time: 8364.5038s
Epoch: 44 cost time: 61.5865433216095
Epoch: 44, Steps: 137 | Train Loss: 0.2771928 Vali Loss: 0.2320979 Test Loss: 0.2634903
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2830383
	speed: 1.0869s/iter; left time: 8231.2756s
Epoch: 45 cost time: 60.6127667427063
Epoch: 45, Steps: 137 | Train Loss: 0.2771343 Vali Loss: 0.2319482 Test Loss: 0.2634363
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2826298
	speed: 1.0842s/iter; left time: 8061.8398s
Epoch: 46 cost time: 61.42275428771973
Epoch: 46, Steps: 137 | Train Loss: 0.2769593 Vali Loss: 0.2314234 Test Loss: 0.2633884
Validation loss decreased (0.231559 --> 0.231423).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2767251
	speed: 1.0844s/iter; left time: 7914.8092s
Epoch: 47 cost time: 61.719764709472656
Epoch: 47, Steps: 137 | Train Loss: 0.2770080 Vali Loss: 0.2311207 Test Loss: 0.2633436
Validation loss decreased (0.231423 --> 0.231121).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2702306
	speed: 1.0958s/iter; left time: 7848.0592s
Epoch: 48 cost time: 62.737274169921875
Epoch: 48, Steps: 137 | Train Loss: 0.2769290 Vali Loss: 0.2318528 Test Loss: 0.2632920
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2756482
	speed: 1.0731s/iter; left time: 7538.2065s
Epoch: 49 cost time: 60.373169898986816
Epoch: 49, Steps: 137 | Train Loss: 0.2769022 Vali Loss: 0.2311075 Test Loss: 0.2632417
Validation loss decreased (0.231121 --> 0.231108).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2721318
	speed: 1.0859s/iter; left time: 7479.4509s
Epoch: 50 cost time: 61.60638093948364
Epoch: 50, Steps: 137 | Train Loss: 0.2768227 Vali Loss: 0.2315241 Test Loss: 0.2632063
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2675645
	speed: 1.0845s/iter; left time: 7321.4794s
Epoch: 51 cost time: 62.73904871940613
Epoch: 51, Steps: 137 | Train Loss: 0.2768132 Vali Loss: 0.2316315 Test Loss: 0.2631742
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2818135
	speed: 1.0736s/iter; left time: 7100.5834s
Epoch: 52 cost time: 59.56269979476929
Epoch: 52, Steps: 137 | Train Loss: 0.2767612 Vali Loss: 0.2312444 Test Loss: 0.2631307
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2747962
	speed: 1.0477s/iter; left time: 6785.7431s
Epoch: 53 cost time: 60.13559675216675
Epoch: 53, Steps: 137 | Train Loss: 0.2766329 Vali Loss: 0.2309656 Test Loss: 0.2631038
Validation loss decreased (0.231108 --> 0.230966).  Saving model ...
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2708250
	speed: 1.0369s/iter; left time: 6574.1215s
Epoch: 54 cost time: 58.59822463989258
Epoch: 54, Steps: 137 | Train Loss: 0.2766407 Vali Loss: 0.2312768 Test Loss: 0.2630795
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2755595
	speed: 1.0220s/iter; left time: 6339.5498s
Epoch: 55 cost time: 59.06817364692688
Epoch: 55, Steps: 137 | Train Loss: 0.2766007 Vali Loss: 0.2311974 Test Loss: 0.2630465
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2876836
	speed: 1.0383s/iter; left time: 6298.3645s
Epoch: 56 cost time: 59.581356048583984
Epoch: 56, Steps: 137 | Train Loss: 0.2765776 Vali Loss: 0.2312393 Test Loss: 0.2630257
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2821214
	speed: 1.0469s/iter; left time: 6206.7997s
Epoch: 57 cost time: 59.564387798309326
Epoch: 57, Steps: 137 | Train Loss: 0.2766045 Vali Loss: 0.2313351 Test Loss: 0.2630037
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2667536
	speed: 1.0618s/iter; left time: 6149.8276s
Epoch: 58 cost time: 60.84521842002869
Epoch: 58, Steps: 137 | Train Loss: 0.2765995 Vali Loss: 0.2311823 Test Loss: 0.2629745
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2812779
	speed: 0.9693s/iter; left time: 5481.4154s
Epoch: 59 cost time: 50.011706829071045
Epoch: 59, Steps: 137 | Train Loss: 0.2765277 Vali Loss: 0.2310235 Test Loss: 0.2629563
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2887310
	speed: 0.7934s/iter; left time: 4377.7393s
Epoch: 60 cost time: 43.73587393760681
Epoch: 60, Steps: 137 | Train Loss: 0.2765087 Vali Loss: 0.2311557 Test Loss: 0.2629325
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2768727
	speed: 0.7936s/iter; left time: 4270.2958s
Epoch: 61 cost time: 44.92901635169983
Epoch: 61, Steps: 137 | Train Loss: 0.2764688 Vali Loss: 0.2315054 Test Loss: 0.2629161
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2645195
	speed: 0.7965s/iter; left time: 4176.8716s
Epoch: 62 cost time: 44.14215850830078
Epoch: 62, Steps: 137 | Train Loss: 0.2764676 Vali Loss: 0.2309840 Test Loss: 0.2628958
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2895798
	speed: 0.7922s/iter; left time: 4045.9299s
Epoch: 63 cost time: 44.48464107513428
Epoch: 63, Steps: 137 | Train Loss: 0.2765272 Vali Loss: 0.2312901 Test Loss: 0.2628830
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2734511
	speed: 0.7861s/iter; left time: 3906.7125s
Epoch: 64 cost time: 44.67615032196045
Epoch: 64, Steps: 137 | Train Loss: 0.2763619 Vali Loss: 0.2309128 Test Loss: 0.2628762
Validation loss decreased (0.230966 --> 0.230913).  Saving model ...
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2826816
	speed: 0.7917s/iter; left time: 3826.4001s
Epoch: 65 cost time: 43.975865602493286
Epoch: 65, Steps: 137 | Train Loss: 0.2763858 Vali Loss: 0.2314190 Test Loss: 0.2628593
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2837515
	speed: 0.7859s/iter; left time: 3690.5220s
Epoch: 66 cost time: 44.34335255622864
Epoch: 66, Steps: 137 | Train Loss: 0.2764418 Vali Loss: 0.2314565 Test Loss: 0.2628483
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2799351
	speed: 0.7830s/iter; left time: 3569.5735s
Epoch: 67 cost time: 43.97399616241455
Epoch: 67, Steps: 137 | Train Loss: 0.2763560 Vali Loss: 0.2311747 Test Loss: 0.2628361
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2749719
	speed: 0.7833s/iter; left time: 3463.5999s
Epoch: 68 cost time: 43.91753053665161
Epoch: 68, Steps: 137 | Train Loss: 0.2763800 Vali Loss: 0.2315700 Test Loss: 0.2628229
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2847558
	speed: 0.7912s/iter; left time: 3390.4305s
Epoch: 69 cost time: 44.247196435928345
Epoch: 69, Steps: 137 | Train Loss: 0.2764410 Vali Loss: 0.2312891 Test Loss: 0.2628140
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2774619
	speed: 0.7827s/iter; left time: 3246.5523s
Epoch: 70 cost time: 44.674323081970215
Epoch: 70, Steps: 137 | Train Loss: 0.2763860 Vali Loss: 0.2308793 Test Loss: 0.2628081
Validation loss decreased (0.230913 --> 0.230879).  Saving model ...
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2791342
	speed: 0.7834s/iter; left time: 3142.3815s
Epoch: 71 cost time: 43.70656132698059
Epoch: 71, Steps: 137 | Train Loss: 0.2763742 Vali Loss: 0.2311286 Test Loss: 0.2628022
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2698748
	speed: 0.7652s/iter; left time: 2964.4699s
Epoch: 72 cost time: 43.27434802055359
Epoch: 72, Steps: 137 | Train Loss: 0.2763245 Vali Loss: 0.2309904 Test Loss: 0.2627920
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2653261
	speed: 0.7786s/iter; left time: 2909.5680s
Epoch: 73 cost time: 44.606913566589355
Epoch: 73, Steps: 137 | Train Loss: 0.2763705 Vali Loss: 0.2312364 Test Loss: 0.2627839
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.2827207
	speed: 0.7850s/iter; left time: 2825.8844s
Epoch: 74 cost time: 43.88319778442383
Epoch: 74, Steps: 137 | Train Loss: 0.2763377 Vali Loss: 0.2312873 Test Loss: 0.2627778
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2805384
	speed: 0.7879s/iter; left time: 2728.5075s
Epoch: 75 cost time: 44.41799807548523
Epoch: 75, Steps: 137 | Train Loss: 0.2764221 Vali Loss: 0.2312032 Test Loss: 0.2627710
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2682815
	speed: 0.7923s/iter; left time: 2635.2369s
Epoch: 76 cost time: 44.60400938987732
Epoch: 76, Steps: 137 | Train Loss: 0.2763472 Vali Loss: 0.2308079 Test Loss: 0.2627625
Validation loss decreased (0.230879 --> 0.230808).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2790099
	speed: 0.7924s/iter; left time: 2527.0809s
Epoch: 77 cost time: 44.72625017166138
Epoch: 77, Steps: 137 | Train Loss: 0.2762716 Vali Loss: 0.2314152 Test Loss: 0.2627574
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.2885334
	speed: 0.7797s/iter; left time: 2379.5912s
Epoch: 78 cost time: 43.73784399032593
Epoch: 78, Steps: 137 | Train Loss: 0.2762324 Vali Loss: 0.2310704 Test Loss: 0.2627514
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.2800083
	speed: 0.7876s/iter; left time: 2295.9938s
Epoch: 79 cost time: 44.96001720428467
Epoch: 79, Steps: 137 | Train Loss: 0.2761920 Vali Loss: 0.2309100 Test Loss: 0.2627485
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.2794166
	speed: 0.7858s/iter; left time: 2182.9972s
Epoch: 80 cost time: 43.83977818489075
Epoch: 80, Steps: 137 | Train Loss: 0.2762726 Vali Loss: 0.2314621 Test Loss: 0.2627430
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.2692493
	speed: 0.7744s/iter; left time: 2045.2821s
Epoch: 81 cost time: 43.47710418701172
Epoch: 81, Steps: 137 | Train Loss: 0.2763721 Vali Loss: 0.2312764 Test Loss: 0.2627386
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.2672350
	speed: 0.7870s/iter; left time: 1970.6420s
Epoch: 82 cost time: 44.804234981536865
Epoch: 82, Steps: 137 | Train Loss: 0.2762129 Vali Loss: 0.2311125 Test Loss: 0.2627361
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.2720484
	speed: 0.7948s/iter; left time: 1881.3006s
Epoch: 83 cost time: 44.30687355995178
Epoch: 83, Steps: 137 | Train Loss: 0.2762841 Vali Loss: 0.2308280 Test Loss: 0.2627325
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.2832231
	speed: 0.7548s/iter; left time: 1683.3099s
Epoch: 84 cost time: 43.72356820106506
Epoch: 84, Steps: 137 | Train Loss: 0.2761801 Vali Loss: 0.2311304 Test Loss: 0.2627307
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.2917970
	speed: 0.7841s/iter; left time: 1641.0901s
Epoch: 85 cost time: 44.030322313308716
Epoch: 85, Steps: 137 | Train Loss: 0.2761768 Vali Loss: 0.2307440 Test Loss: 0.2627264
Validation loss decreased (0.230808 --> 0.230744).  Saving model ...
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.2893567
	speed: 0.7966s/iter; left time: 1558.1138s
Epoch: 86 cost time: 45.13588547706604
Epoch: 86, Steps: 137 | Train Loss: 0.2761515 Vali Loss: 0.2309180 Test Loss: 0.2627254
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.2652648
	speed: 0.7840s/iter; left time: 1426.1848s
Epoch: 87 cost time: 43.77819561958313
Epoch: 87, Steps: 137 | Train Loss: 0.2763015 Vali Loss: 0.2308111 Test Loss: 0.2627224
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.2758240
	speed: 0.7870s/iter; left time: 1323.7426s
Epoch: 88 cost time: 44.21044063568115
Epoch: 88, Steps: 137 | Train Loss: 0.2761904 Vali Loss: 0.2313104 Test Loss: 0.2627192
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.766650946003179e-06
	iters: 100, epoch: 89 | loss: 0.2845458
	speed: 0.7965s/iter; left time: 1230.6146s
Epoch: 89 cost time: 44.08326840400696
Epoch: 89, Steps: 137 | Train Loss: 0.2762325 Vali Loss: 0.2312042 Test Loss: 0.2627183
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.47831839870302e-06
	iters: 100, epoch: 90 | loss: 0.2773794
	speed: 0.7813s/iter; left time: 1100.0342s
Epoch: 90 cost time: 43.70214557647705
Epoch: 90, Steps: 137 | Train Loss: 0.2762281 Vali Loss: 0.2307006 Test Loss: 0.2627156
Validation loss decreased (0.230744 --> 0.230701).  Saving model ...
Updating learning rate to 5.204402478767869e-06
	iters: 100, epoch: 91 | loss: 0.2779594
	speed: 0.7740s/iter; left time: 983.6916s
Epoch: 91 cost time: 43.95462512969971
Epoch: 91, Steps: 137 | Train Loss: 0.2761388 Vali Loss: 0.2307385 Test Loss: 0.2627147
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.944182354829475e-06
	iters: 100, epoch: 92 | loss: 0.2810991
	speed: 0.7889s/iter; left time: 894.6150s
Epoch: 92 cost time: 44.36541724205017
Epoch: 92, Steps: 137 | Train Loss: 0.2761379 Vali Loss: 0.2308119 Test Loss: 0.2627118
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.696973237088e-06
	iters: 100, epoch: 93 | loss: 0.2740157
	speed: 0.7950s/iter; left time: 792.6040s
Epoch: 93 cost time: 44.36180877685547
Epoch: 93, Steps: 137 | Train Loss: 0.2762071 Vali Loss: 0.2311395 Test Loss: 0.2627099
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.462124575233601e-06
	iters: 100, epoch: 94 | loss: 0.2646753
	speed: 0.7878s/iter; left time: 677.5336s
Epoch: 94 cost time: 45.024919748306274
Epoch: 94, Steps: 137 | Train Loss: 0.2762993 Vali Loss: 0.2305809 Test Loss: 0.2627085
Validation loss decreased (0.230701 --> 0.230581).  Saving model ...
Updating learning rate to 4.239018346471921e-06
	iters: 100, epoch: 95 | loss: 0.2668577
	speed: 0.7931s/iter; left time: 573.4108s
Epoch: 95 cost time: 44.38700342178345
Epoch: 95, Steps: 137 | Train Loss: 0.2762517 Vali Loss: 0.2312363 Test Loss: 0.2627063
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
	iters: 100, epoch: 96 | loss: 0.2931013
	speed: 0.7851s/iter; left time: 460.0543s
Epoch: 96 cost time: 44.54089689254761
Epoch: 96, Steps: 137 | Train Loss: 0.2761593 Vali Loss: 0.2308778 Test Loss: 0.2627048
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.825714057690908e-06
	iters: 100, epoch: 97 | loss: 0.2777226
	speed: 0.7847s/iter; left time: 352.3267s
Epoch: 97 cost time: 43.5254693031311
Epoch: 97, Steps: 137 | Train Loss: 0.2762091 Vali Loss: 0.2311675 Test Loss: 0.2627034
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.6344283548063623e-06
	iters: 100, epoch: 98 | loss: 0.2771156
	speed: 0.7955s/iter; left time: 248.1960s
Epoch: 98 cost time: 45.39224433898926
Epoch: 98, Steps: 137 | Train Loss: 0.2762099 Vali Loss: 0.2308327 Test Loss: 0.2627018
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.452706937066044e-06
	iters: 100, epoch: 99 | loss: 0.2720945
	speed: 0.7963s/iter; left time: 139.3467s
Epoch: 99 cost time: 44.42705798149109
Epoch: 99, Steps: 137 | Train Loss: 0.2762310 Vali Loss: 0.2305969 Test Loss: 0.2627005
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.2800715902127414e-06
	iters: 100, epoch: 100 | loss: 0.2747132
	speed: 0.7853s/iter; left time: 29.8403s
Epoch: 100 cost time: 44.621878147125244
Epoch: 100, Steps: 137 | Train Loss: 0.2761740 Vali Loss: 0.2313514 Test Loss: 0.2626997
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : Electricity_90_j720_H6_FITS_custom_ftM_sl90_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.26138922572135925, mae:0.33226874470710754, rse:0.5100011825561523, corr:[0.4429038  0.44120836 0.4401766  0.4384906  0.43839526 0.43739134
 0.43740857 0.43689477 0.43575844 0.43476135 0.43418437 0.43343034
 0.43325377 0.432717   0.43249992 0.4321819  0.43211696 0.4323503
 0.43240404 0.4323838  0.43224418 0.43242255 0.43230346 0.43119055
 0.4294194  0.42808473 0.42681766 0.42609608 0.42544955 0.42555115
 0.4264771  0.4264849  0.42603332 0.42577964 0.42526    0.42490208
 0.42509487 0.42459095 0.42464304 0.42435524 0.42418787 0.4244407
 0.42444763 0.42455557 0.4246946  0.42504317 0.42506656 0.4247769
 0.42393413 0.42345363 0.42339864 0.42358166 0.42425817 0.42581207
 0.4277956  0.42913067 0.42929643 0.42926928 0.42902678 0.4287765
 0.42878565 0.4283262  0.4286071  0.4286366  0.42869085 0.42904028
 0.42926952 0.4294879  0.4298593  0.43045074 0.43079013 0.43120363
 0.43140677 0.43185702 0.43256235 0.43359855 0.43535727 0.4379391
 0.44088778 0.44326127 0.4435633  0.4432782  0.4428745  0.44250414
 0.44236243 0.44186342 0.4421196  0.44206804 0.44224453 0.4426491
 0.44271913 0.44287765 0.44302398 0.4431437  0.4433607  0.4434403
 0.44315556 0.44316474 0.44305313 0.44292772 0.44295317 0.44308975
 0.4433913  0.44346273 0.44304582 0.44272026 0.44233498 0.44206774
 0.44203198 0.44166934 0.44194692 0.44190824 0.4420376  0.44234186
 0.44234142 0.44244444 0.44254336 0.4426554  0.4428401  0.4429618
 0.44272974 0.44278404 0.4426871  0.44258463 0.44261935 0.4425982
 0.44272795 0.44269466 0.44234812 0.44211146 0.44175097 0.44151407
 0.44155487 0.44131804 0.4416388  0.4416213  0.44183242 0.44218054
 0.4422964  0.44257608 0.44274214 0.4428754  0.44299465 0.44291976
 0.4425199  0.44241863 0.44229275 0.4421578  0.4422488  0.44229925
 0.442465   0.44251028 0.44208267 0.44178692 0.44141513 0.44109932
 0.44116622 0.4410312  0.44134557 0.44139856 0.44173616 0.44213608
 0.44234398 0.442548   0.44230446 0.44209707 0.44134757 0.43894458
 0.43611544 0.43385294 0.4320712  0.43053523 0.42959282 0.42904726
 0.42883718 0.4286457  0.42813697 0.4276131  0.42732626 0.42706272
 0.4269863  0.42669794 0.4267717  0.42640713 0.42638004 0.42655405
 0.42645717 0.42642206 0.4260896  0.42590734 0.42536753 0.42371604
 0.42153344 0.42011058 0.41894755 0.41800624 0.4177404  0.41805896
 0.41895592 0.41956854 0.419374   0.4191358  0.41897896 0.4187453
 0.41886607 0.41867292 0.41875893 0.4184547  0.41842467 0.41857564
 0.41849157 0.41855133 0.41850927 0.4186485  0.4185629  0.41799074
 0.41702065 0.41657984 0.41649714 0.416733   0.4176669  0.41918746
 0.4212557  0.42297965 0.42327917 0.4232979  0.423291   0.4230288
 0.42298734 0.4227421  0.42288992 0.42288375 0.42305622 0.4232838
 0.42347    0.42374042 0.42395622 0.42444405 0.42479467 0.42500144
 0.4251535  0.42568982 0.42635268 0.4274344  0.42926937 0.43178013
 0.43487895 0.43746483 0.43788227 0.43766546 0.43746656 0.43708828
 0.43694204 0.43660533 0.43674606 0.43669748 0.4369331  0.43723288
 0.43731204 0.43750876 0.43758896 0.43774092 0.43794522 0.43791625
 0.4376688  0.43765044 0.43748337 0.43745825 0.43754813 0.43767536
 0.43809026 0.43825844 0.43786478 0.43759677 0.43729514 0.4369308
 0.43691763 0.4366093  0.43674976 0.436716   0.43682802 0.4370735
 0.43711552 0.43721732 0.43725914 0.43738148 0.43754828 0.43757114
 0.43736702 0.4374225  0.4372883  0.4372269  0.43731052 0.43729994
 0.43753582 0.43753943 0.43714732 0.4369796  0.43668956 0.43636164
 0.4364661  0.43624946 0.4364585  0.4365164  0.43668696 0.43700397
 0.43717623 0.43741146 0.4375406  0.43770492 0.4377695  0.43757597
 0.43720496 0.4370492  0.43685564 0.43679896 0.43690616 0.43694472
 0.43723014 0.43730673 0.436881   0.43666068 0.4363221  0.4359285
 0.43605477 0.43585524 0.43607196 0.43617386 0.43643558 0.4368299
 0.43702275 0.4371124  0.4367582  0.43646988 0.43556884 0.4329295
 0.4300678  0.42777172 0.42589527 0.42446646 0.42349344 0.42298213
 0.42290005 0.42265165 0.42215705 0.42168278 0.42129198 0.4209659
 0.42094424 0.4205726  0.42057386 0.42029348 0.42024538 0.42037264
 0.42027533 0.42023304 0.4198925  0.41972086 0.41906768 0.41718218
 0.41494116 0.4133309  0.41209263 0.4112669  0.41094187 0.41135973
 0.41232547 0.41288438 0.41275626 0.41253075 0.4122977  0.41202635
 0.41217914 0.41190997 0.41195658 0.41171545 0.41158783 0.411744
 0.41168746 0.4117477  0.41171867 0.41190353 0.41174877 0.41105992
 0.41012514 0.40960303 0.4096116  0.4100413  0.41105747 0.4128292
 0.41503334 0.41680652 0.41722685 0.4172785  0.41722712 0.4169793
 0.4170028  0.4166992  0.41688094 0.41690588 0.4169691  0.41725725
 0.41746852 0.41770092 0.41794416 0.41850162 0.41881678 0.41899744
 0.41921213 0.41971493 0.42052448 0.42180124 0.42377603 0.42659083
 0.4298311  0.43247184 0.43308365 0.43300715 0.43283284 0.4324725
 0.43237236 0.43195564 0.4321362  0.432131   0.43227485 0.43263727
 0.43274352 0.4329169  0.43303248 0.43317115 0.4333941  0.4333851
 0.4331214  0.43306312 0.43299758 0.43301603 0.4331404  0.43339682
 0.43383026 0.43399805 0.43366104 0.4333853  0.43301192 0.43269274
 0.43267885 0.43230343 0.43248832 0.4324424  0.43249363 0.43281236
 0.43282795 0.43290588 0.43299747 0.43311134 0.43329906 0.43336898
 0.43319246 0.4332255  0.4331841  0.4331609  0.4332424  0.43332338
 0.4335491  0.43353656 0.4331842  0.43299136 0.43265185 0.43238318
 0.43246597 0.43219116 0.43248525 0.4325338  0.4326631  0.43302158
 0.43314493 0.433384   0.43358222 0.433746   0.433862   0.4337259
 0.43336365 0.433224   0.43314174 0.43307266 0.43320233 0.4333386
 0.4335604  0.43363196 0.433248   0.43299583 0.4326625  0.43237698
 0.4324462  0.43224844 0.43253264 0.4325941  0.4328774  0.43326786
 0.43340608 0.43351477 0.43322223 0.43293694 0.4320648  0.4294745
 0.42653894 0.42411238 0.4223689  0.42096007 0.420041   0.41964626
 0.41945451 0.41920784 0.4187573  0.41823086 0.4178872  0.41763633
 0.41757277 0.41723818 0.41726783 0.41691443 0.41688448 0.4170368
 0.41689235 0.41682723 0.4164928  0.41624975 0.4155782  0.4137041
 0.41132063 0.40965292 0.4085585  0.40767056 0.40743    0.40787464
 0.4086519  0.4091748  0.40899664 0.40867034 0.40847495 0.40829924
 0.4083748  0.40812716 0.4082125  0.4079093  0.4078256  0.407987
 0.4078585  0.40791237 0.4079352  0.40808338 0.40795618 0.40726545
 0.40621257 0.40569964 0.4056934  0.4059514  0.40704212 0.40873945
 0.41083705 0.41259322 0.41296312 0.41292533 0.41284263 0.41259006
 0.41248915 0.41219795 0.4123342  0.4122188  0.4122931  0.41244265
 0.4125113  0.4128046  0.41307035 0.4135337  0.41389838 0.41407302
 0.41419753 0.41475344 0.41556257 0.41462994 0.41453683 0.42136246
 0.4244986  0.42712626 0.42766818 0.42750487 0.42726767 0.4269401
 0.42674506 0.42643765 0.42661148 0.42650226 0.4267344  0.4249406
 0.42709067 0.42313093 0.42323658 0.42338574 0.42364126 0.42565954
 0.42327812 0.4232784  0.42316958 0.42314565 0.4233282  0.42349422
 0.42387107 0.42400593 0.4236091  0.42329332 0.42294106 0.42263025
 0.42255318 0.42226636 0.42238745 0.42232093 0.4224577  0.42266458
 0.42269892 0.42284387 0.42289123 0.42301    0.42322257 0.42325428
 0.42308494 0.42317617 0.42309964 0.42308012 0.4232432  0.42324615
 0.42346716 0.4234566  0.42304388 0.42281812 0.42250022 0.4221892
 0.42221084 0.42204574 0.42219606 0.42222273 0.42241725 0.4226222
 0.42278102 0.4230618  0.4231539  0.42334276 0.42350253 0.42329124
 0.42293698 0.422879   0.422708   0.4227079  0.4229207  0.42294508
 0.4232384  0.4232743  0.42279196 0.42256826 0.42224917 0.42185068
 0.42191273 0.42176673 0.42187503 0.422029   0.4222461  0.42245543
 0.42263752 0.4226542  0.4223415  0.4221144  0.42126238 0.41860297
 0.41559672 0.41323432 0.41131532 0.41009128 0.40923473 0.4086856
 0.40874466 0.40838954 0.40780205 0.40734702 0.40687186 0.40648025
 0.40645462 0.40600714 0.4058247  0.40557683 0.4053723  0.40546843
 0.40545374 0.40526828 0.40499914 0.4049581  0.40439266 0.40263483
 0.40053275 0.39888468 0.39771846 0.3971924  0.39676127 0.39727306
 0.39825323 0.39834544 0.39819697 0.39774254 0.3971664  0.3968933
 0.39686108 0.3961915  0.39629763 0.3959651  0.3956336  0.39626697
 0.39602473 0.396373   0.39686602 0.39697105 0.39788634 0.39702553]
