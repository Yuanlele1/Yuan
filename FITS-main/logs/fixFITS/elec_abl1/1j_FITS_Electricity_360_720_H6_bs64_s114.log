Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_360_j720_H6', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_360_j720_H6_FITS_custom_ftM_sl360_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17333
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=106, out_features=318, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1384994304.0
params:  34026.0
Trainable parameters:  34026
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6817639
	speed: 0.9343s/iter; left time: 12520.6168s
Epoch: 1 cost time: 128.24820494651794
Epoch: 1, Steps: 135 | Train Loss: 0.9230218 Vali Loss: 0.4961085 Test Loss: 0.5915710
Validation loss decreased (inf --> 0.496109).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4823600
	speed: 2.1304s/iter; left time: 28262.0978s
Epoch: 2 cost time: 121.32764720916748
Epoch: 2, Steps: 135 | Train Loss: 0.5108261 Vali Loss: 0.3514331 Test Loss: 0.4255033
Validation loss decreased (0.496109 --> 0.351433).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3620396
	speed: 2.1754s/iter; left time: 28565.7756s
Epoch: 3 cost time: 125.82115936279297
Epoch: 3, Steps: 135 | Train Loss: 0.3882396 Vali Loss: 0.2759503 Test Loss: 0.3363540
Validation loss decreased (0.351433 --> 0.275950).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3101403
	speed: 2.2101s/iter; left time: 28722.9802s
Epoch: 4 cost time: 131.19150829315186
Epoch: 4, Steps: 135 | Train Loss: 0.3183656 Vali Loss: 0.2328465 Test Loss: 0.2836034
Validation loss decreased (0.275950 --> 0.232846).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2670628
	speed: 2.2616s/iter; left time: 29086.2419s
Epoch: 5 cost time: 132.1736078262329
Epoch: 5, Steps: 135 | Train Loss: 0.2778575 Vali Loss: 0.2088961 Test Loss: 0.2529407
Validation loss decreased (0.232846 --> 0.208896).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2447698
	speed: 2.2617s/iter; left time: 28782.5479s
Epoch: 6 cost time: 132.81262040138245
Epoch: 6, Steps: 135 | Train Loss: 0.2549362 Vali Loss: 0.1957127 Test Loss: 0.2356037
Validation loss decreased (0.208896 --> 0.195713).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2373799
	speed: 2.2917s/iter; left time: 28854.5556s
Epoch: 7 cost time: 134.2114987373352
Epoch: 7, Steps: 135 | Train Loss: 0.2422791 Vali Loss: 0.1891786 Test Loss: 0.2260573
Validation loss decreased (0.195713 --> 0.189179).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2244527
	speed: 2.3112s/iter; left time: 28788.9243s
Epoch: 8 cost time: 135.2922079563141
Epoch: 8, Steps: 135 | Train Loss: 0.2358114 Vali Loss: 0.1861072 Test Loss: 0.2209418
Validation loss decreased (0.189179 --> 0.186107).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2318348
	speed: 2.3269s/iter; left time: 28669.4181s
Epoch: 9 cost time: 137.92909622192383
Epoch: 9, Steps: 135 | Train Loss: 0.2324275 Vali Loss: 0.1849755 Test Loss: 0.2182500
Validation loss decreased (0.186107 --> 0.184976).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2329857
	speed: 2.2647s/iter; left time: 27597.5017s
Epoch: 10 cost time: 131.07952189445496
Epoch: 10, Steps: 135 | Train Loss: 0.2307553 Vali Loss: 0.1841759 Test Loss: 0.2168486
Validation loss decreased (0.184976 --> 0.184176).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2238694
	speed: 2.2222s/iter; left time: 26779.2027s
Epoch: 11 cost time: 131.48335480690002
Epoch: 11, Steps: 135 | Train Loss: 0.2299019 Vali Loss: 0.1842415 Test Loss: 0.2160833
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2338295
	speed: 2.2471s/iter; left time: 26776.6178s
Epoch: 12 cost time: 133.05902862548828
Epoch: 12, Steps: 135 | Train Loss: 0.2295603 Vali Loss: 0.1845481 Test Loss: 0.2156923
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2413759
	speed: 2.2505s/iter; left time: 26512.5576s
Epoch: 13 cost time: 130.5823051929474
Epoch: 13, Steps: 135 | Train Loss: 0.2292010 Vali Loss: 0.1844255 Test Loss: 0.2154616
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2182665
	speed: 2.1696s/iter; left time: 25266.6643s
Epoch: 14 cost time: 125.26184463500977
Epoch: 14, Steps: 135 | Train Loss: 0.2290966 Vali Loss: 0.1843942 Test Loss: 0.2153273
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2458691
	speed: 2.1830s/iter; left time: 25128.7385s
Epoch: 15 cost time: 125.99275350570679
Epoch: 15, Steps: 135 | Train Loss: 0.2290626 Vali Loss: 0.1841670 Test Loss: 0.2152411
Validation loss decreased (0.184176 --> 0.184167).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2301437
	speed: 2.1345s/iter; left time: 24282.0914s
Epoch: 16 cost time: 123.9907557964325
Epoch: 16, Steps: 135 | Train Loss: 0.2290276 Vali Loss: 0.1842816 Test Loss: 0.2151697
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2226715
	speed: 2.1244s/iter; left time: 23880.0936s
Epoch: 17 cost time: 125.51053857803345
Epoch: 17, Steps: 135 | Train Loss: 0.2290001 Vali Loss: 0.1844146 Test Loss: 0.2151389
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2292463
	speed: 2.2501s/iter; left time: 24989.3401s
Epoch: 18 cost time: 129.92857241630554
Epoch: 18, Steps: 135 | Train Loss: 0.2289630 Vali Loss: 0.1840384 Test Loss: 0.2150975
Validation loss decreased (0.184167 --> 0.184038).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2271702
	speed: 2.1380s/iter; left time: 23455.8473s
Epoch: 19 cost time: 125.84501504898071
Epoch: 19, Steps: 135 | Train Loss: 0.2289251 Vali Loss: 0.1842083 Test Loss: 0.2150787
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2279776
	speed: 2.2467s/iter; left time: 24345.1952s
Epoch: 20 cost time: 131.86681175231934
Epoch: 20, Steps: 135 | Train Loss: 0.2288779 Vali Loss: 0.1840855 Test Loss: 0.2150718
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2304226
	speed: 2.1007s/iter; left time: 22479.1475s
Epoch: 21 cost time: 119.95709013938904
Epoch: 21, Steps: 135 | Train Loss: 0.2288141 Vali Loss: 0.1841010 Test Loss: 0.2150442
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2277401
	speed: 2.1176s/iter; left time: 22374.9351s
Epoch: 22 cost time: 123.24735617637634
Epoch: 22, Steps: 135 | Train Loss: 0.2288626 Vali Loss: 0.1840079 Test Loss: 0.2150385
Validation loss decreased (0.184038 --> 0.184008).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2213109
	speed: 2.0959s/iter; left time: 21862.7127s
Epoch: 23 cost time: 124.94875574111938
Epoch: 23, Steps: 135 | Train Loss: 0.2288163 Vali Loss: 0.1842464 Test Loss: 0.2150255
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2211289
	speed: 2.1553s/iter; left time: 22191.2175s
Epoch: 24 cost time: 119.42878079414368
Epoch: 24, Steps: 135 | Train Loss: 0.2288027 Vali Loss: 0.1837905 Test Loss: 0.2150250
Validation loss decreased (0.184008 --> 0.183790).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2431028
	speed: 2.0010s/iter; left time: 20331.8533s
Epoch: 25 cost time: 115.6883819103241
Epoch: 25, Steps: 135 | Train Loss: 0.2288104 Vali Loss: 0.1842284 Test Loss: 0.2150217
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2305304
	speed: 2.0030s/iter; left time: 20081.9776s
Epoch: 26 cost time: 116.92250204086304
Epoch: 26, Steps: 135 | Train Loss: 0.2287715 Vali Loss: 0.1840446 Test Loss: 0.2150140
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2209422
	speed: 2.0462s/iter; left time: 20238.4718s
Epoch: 27 cost time: 117.16648507118225
Epoch: 27, Steps: 135 | Train Loss: 0.2287871 Vali Loss: 0.1840105 Test Loss: 0.2149917
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2191516
	speed: 2.0063s/iter; left time: 19573.8538s
Epoch: 28 cost time: 120.94574356079102
Epoch: 28, Steps: 135 | Train Loss: 0.2287214 Vali Loss: 0.1840484 Test Loss: 0.2149974
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2313605
	speed: 2.0932s/iter; left time: 20138.9864s
Epoch: 29 cost time: 121.34700417518616
Epoch: 29, Steps: 135 | Train Loss: 0.2287209 Vali Loss: 0.1840668 Test Loss: 0.2149867
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2186001
	speed: 2.1046s/iter; left time: 19964.6806s
Epoch: 30 cost time: 122.85133457183838
Epoch: 30, Steps: 135 | Train Loss: 0.2286060 Vali Loss: 0.1838395 Test Loss: 0.2149958
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2182603
	speed: 2.1012s/iter; left time: 19648.6143s
Epoch: 31 cost time: 122.8164553642273
Epoch: 31, Steps: 135 | Train Loss: 0.2286532 Vali Loss: 0.1840072 Test Loss: 0.2149762
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2121295
	speed: 2.1034s/iter; left time: 19384.5655s
Epoch: 32 cost time: 112.7657425403595
Epoch: 32, Steps: 135 | Train Loss: 0.2286704 Vali Loss: 0.1834017 Test Loss: 0.2149755
Validation loss decreased (0.183790 --> 0.183402).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2262269
	speed: 1.6737s/iter; left time: 15198.5537s
Epoch: 33 cost time: 96.85186243057251
Epoch: 33, Steps: 135 | Train Loss: 0.2286719 Vali Loss: 0.1837338 Test Loss: 0.2149659
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2323350
	speed: 1.5621s/iter; left time: 13974.3900s
Epoch: 34 cost time: 87.90541768074036
Epoch: 34, Steps: 135 | Train Loss: 0.2287046 Vali Loss: 0.1840597 Test Loss: 0.2149613
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2177347
	speed: 1.5225s/iter; left time: 13414.4822s
Epoch: 35 cost time: 87.04556488990784
Epoch: 35, Steps: 135 | Train Loss: 0.2287448 Vali Loss: 0.1838710 Test Loss: 0.2149570
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2290376
	speed: 1.4736s/iter; left time: 12784.5224s
Epoch: 36 cost time: 86.04186868667603
Epoch: 36, Steps: 135 | Train Loss: 0.2287405 Vali Loss: 0.1841080 Test Loss: 0.2149570
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2285541
	speed: 1.5010s/iter; left time: 12820.2213s
Epoch: 37 cost time: 87.28251051902771
Epoch: 37, Steps: 135 | Train Loss: 0.2286788 Vali Loss: 0.1836513 Test Loss: 0.2149525
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2352080
	speed: 1.5529s/iter; left time: 13054.0844s
Epoch: 38 cost time: 93.957848072052
Epoch: 38, Steps: 135 | Train Loss: 0.2286871 Vali Loss: 0.1838772 Test Loss: 0.2149498
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2370058
	speed: 1.6456s/iter; left time: 13610.6326s
Epoch: 39 cost time: 97.53289008140564
Epoch: 39, Steps: 135 | Train Loss: 0.2286352 Vali Loss: 0.1841158 Test Loss: 0.2149398
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2341190
	speed: 1.5797s/iter; left time: 12852.8101s
Epoch: 40 cost time: 89.36964178085327
Epoch: 40, Steps: 135 | Train Loss: 0.2286951 Vali Loss: 0.1841045 Test Loss: 0.2149386
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2212828
	speed: 1.5517s/iter; left time: 12414.8914s
Epoch: 41 cost time: 93.66643905639648
Epoch: 41, Steps: 135 | Train Loss: 0.2286495 Vali Loss: 0.1835459 Test Loss: 0.2149519
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2265212
	speed: 1.5306s/iter; left time: 12039.8752s
Epoch: 42 cost time: 87.43881583213806
Epoch: 42, Steps: 135 | Train Loss: 0.2287226 Vali Loss: 0.1839777 Test Loss: 0.2149365
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2104863
	speed: 1.6190s/iter; left time: 12516.5260s
Epoch: 43 cost time: 95.18791389465332
Epoch: 43, Steps: 135 | Train Loss: 0.2286230 Vali Loss: 0.1842579 Test Loss: 0.2149352
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2361894
	speed: 1.5866s/iter; left time: 12052.1845s
Epoch: 44 cost time: 94.81202793121338
Epoch: 44, Steps: 135 | Train Loss: 0.2286528 Vali Loss: 0.1843936 Test Loss: 0.2149354
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2403831
	speed: 1.5865s/iter; left time: 11837.1161s
Epoch: 45 cost time: 92.6446123123169
Epoch: 45, Steps: 135 | Train Loss: 0.2286644 Vali Loss: 0.1840261 Test Loss: 0.2149232
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2311306
	speed: 1.5811s/iter; left time: 11582.8982s
Epoch: 46 cost time: 94.88990545272827
Epoch: 46, Steps: 135 | Train Loss: 0.2286021 Vali Loss: 0.1840628 Test Loss: 0.2149289
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2230191
	speed: 1.5561s/iter; left time: 11189.7826s
Epoch: 47 cost time: 91.04540371894836
Epoch: 47, Steps: 135 | Train Loss: 0.2286518 Vali Loss: 0.1835384 Test Loss: 0.2149318
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2211658
	speed: 1.4442s/iter; left time: 10190.3386s
Epoch: 48 cost time: 70.96260261535645
Epoch: 48, Steps: 135 | Train Loss: 0.2286208 Vali Loss: 0.1834926 Test Loss: 0.2149271
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2194773
	speed: 1.0772s/iter; left time: 7455.4434s
Epoch: 49 cost time: 63.257590532302856
Epoch: 49, Steps: 135 | Train Loss: 0.2286592 Vali Loss: 0.1836578 Test Loss: 0.2149323
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2227020
	speed: 1.0634s/iter; left time: 7216.0145s
Epoch: 50 cost time: 61.35431098937988
Epoch: 50, Steps: 135 | Train Loss: 0.2286630 Vali Loss: 0.1837559 Test Loss: 0.2149257
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2260917
	speed: 1.0216s/iter; left time: 6794.4579s
Epoch: 51 cost time: 56.05083131790161
Epoch: 51, Steps: 135 | Train Loss: 0.2286255 Vali Loss: 0.1836256 Test Loss: 0.2149266
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2280168
	speed: 0.8321s/iter; left time: 5422.2577s
Epoch: 52 cost time: 45.078285455703735
Epoch: 52, Steps: 135 | Train Loss: 0.2286327 Vali Loss: 0.1837399 Test Loss: 0.2149270
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_360_j720_H6_FITS_custom_ftM_sl360_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.21338847279548645, mae:0.3033212721347809, rse:0.4608004093170166, corr:[0.4466921  0.44659862 0.4487711  0.44944036 0.44921944 0.44991922
 0.45022273 0.4497415  0.44947547 0.4493926  0.4491024  0.44887537
 0.44879958 0.4487011  0.4486384  0.44872436 0.44878498 0.44872087
 0.4485801  0.44846207 0.4485293  0.44862115 0.4485975  0.44872668
 0.44904697 0.449315   0.4491676  0.4490214  0.4490686  0.44895545
 0.4486151  0.44840243 0.44833696 0.44819757 0.44804063 0.44802952
 0.4480445  0.44800085 0.44802052 0.44809112 0.44805896 0.44796535
 0.4479428  0.4479041  0.44787383 0.44784427 0.44786188 0.44802406
 0.44818428 0.44823572 0.4481696  0.44814193 0.44809115 0.44789177
 0.44771403 0.44764936 0.44757715 0.44746348 0.4474011  0.44741306
 0.4473771  0.44736058 0.4474508  0.4475101  0.44743294 0.44738048
 0.44743422 0.4473659  0.4472436  0.44717407 0.4472198  0.44733652
 0.44737315 0.44736513 0.44734713 0.4473272  0.44719875 0.44701782
 0.4469249  0.44687742 0.44677696 0.44669604 0.44667706 0.44664478
 0.44656903 0.44655806 0.4466262  0.44664383 0.44660023 0.44661877
 0.44669116 0.44661826 0.44650093 0.44649744 0.44658354 0.44667956
 0.44668028 0.44667968 0.4467076  0.44669428 0.44657278 0.44644594
 0.44641986 0.446399   0.4462792  0.4461775  0.44615257 0.44611534
 0.44605055 0.44606242 0.4461165  0.4461102  0.44605127 0.44606322
 0.44609082 0.44596064 0.44585815 0.44589207 0.44600365 0.44612983
 0.44619024 0.4462481  0.44631326 0.44632733 0.44625273 0.44619644
 0.4461841  0.44612196 0.44604275 0.446032   0.44603416 0.44598198
 0.44595057 0.44599766 0.44603762 0.4460088  0.4459819  0.44601747
 0.4460728  0.44608    0.4461413  0.44623572 0.44632196 0.44652155
 0.44678634 0.4469852  0.44709837 0.44713718 0.4471168  0.44713545
 0.4471429  0.44705978 0.44697016 0.44696888 0.44697428 0.44692147
 0.44689056 0.44691065 0.4469381  0.44689986 0.4468786  0.4469432
 0.4470118  0.44691157 0.44682506 0.4467856  0.4466362  0.44644386
 0.44626582 0.4461917  0.44604433 0.44586757 0.44575816 0.44569582
 0.4455835  0.44540995 0.44528887 0.4452659  0.4452415  0.44516757
 0.44512516 0.44514763 0.4451849  0.44517836 0.44514567 0.44514668
 0.44508955 0.44482672 0.4446583  0.44462863 0.44454077 0.44440925
 0.44435164 0.44443202 0.4444188  0.44430935 0.44422245 0.4441792
 0.4440943  0.44398803 0.44393867 0.44391155 0.44382825 0.4437624
 0.44376454 0.44377536 0.44375807 0.4437514  0.44378462 0.44381732
 0.4437337  0.44345623 0.4433034  0.44330114 0.44327375 0.44326612
 0.44335392 0.44349727 0.4434778  0.4433608  0.44331095 0.4433203
 0.44325128 0.4431342  0.44308737 0.44307512 0.4430504  0.44300646
 0.442973   0.4429537  0.4429461  0.44294468 0.44295302 0.44295108
 0.44291067 0.44272304 0.44262165 0.44261882 0.44261378 0.44261354
 0.44265014 0.44273078 0.44272986 0.4427064  0.4427194  0.44270357
 0.44260234 0.44249523 0.4424591  0.44243038 0.4423713  0.44233146
 0.44230968 0.44228607 0.44226995 0.4422735  0.4422594  0.4422266
 0.44218257 0.4420842  0.442036   0.44202095 0.4419934  0.44200203
 0.4420663  0.44214717 0.44214392 0.44213107 0.4421465  0.44213772
 0.44206205 0.44199976 0.4419503  0.44186214 0.44178697 0.4417541
 0.44173038 0.44168133 0.44166958 0.4416962  0.44168845 0.4416335
 0.441553   0.44143137 0.44141176 0.44142672 0.44143233 0.4415285
 0.4416948  0.44182113 0.44185084 0.44189143 0.44195557 0.44196713
 0.4419044  0.44183928 0.44179943 0.44175768 0.44170964 0.4416801
 0.44164416 0.4415985  0.4416035  0.44164822 0.44164306 0.44158947
 0.44158316 0.44162914 0.44171733 0.4417579  0.44179705 0.4420277
 0.44235462 0.44253775 0.44261292 0.44269335 0.44272587 0.442704
 0.44266215 0.44262567 0.44259486 0.4425566  0.44252247 0.44249964
 0.44248053 0.44246432 0.44248232 0.44247785 0.44242686 0.44239253
 0.44237536 0.44225112 0.4421159  0.44199535 0.44179797 0.44161677
 0.4414672  0.4413929  0.4412765  0.4412059  0.44118208 0.441103
 0.44096804 0.44081664 0.440676   0.44055843 0.44046342 0.44039088
 0.44032833 0.44028962 0.44030482 0.44029891 0.44023234 0.4402017
 0.44021484 0.4400902  0.43993658 0.43987224 0.43984598 0.4398196
 0.43976313 0.43978718 0.4397953  0.43977055 0.43970153 0.4396216
 0.43953833 0.43943545 0.43931407 0.43919402 0.4390869  0.4390014
 0.43892923 0.4388953  0.43891954 0.43891203 0.43883026 0.4387818
 0.43878874 0.4386517  0.4384799  0.43840632 0.43842474 0.43848833
 0.4385319  0.43859953 0.43865702 0.43867308 0.43864393 0.43858925
 0.4385342  0.43849933 0.43845916 0.43841654 0.43835777 0.43826857
 0.4381949  0.43818668 0.4382249  0.43819946 0.43810058 0.4380534
 0.43806872 0.4379532  0.43782383 0.4378049  0.43784788 0.43789372
 0.43793255 0.438021   0.43811202 0.43815517 0.43814188 0.43813378
 0.43809825 0.4379948  0.43787578 0.43783692 0.43782488 0.43773562
 0.43765262 0.43765044 0.43769228 0.43765438 0.43756124 0.43754658
 0.43757266 0.43745428 0.43733677 0.43732363 0.43736532 0.43741512
 0.4374689  0.43757772 0.43767035 0.4376909  0.43767017 0.43766466
 0.43765536 0.43757835 0.43747574 0.4374041  0.43734366 0.43724436
 0.4371699  0.43720955 0.43728325 0.43725738 0.4371812  0.43719515
 0.43720117 0.43705958 0.4369999  0.43705985 0.43713057 0.43722492
 0.43740427 0.43760753 0.4377178  0.4377436  0.4377484  0.4377893
 0.4378145  0.43775553 0.43766716 0.43760723 0.43754497 0.43745273
 0.4373976  0.437407   0.43742034 0.43735683 0.43729252 0.43731096
 0.4373646  0.4373811  0.4374639  0.43759352 0.43770155 0.43792832
 0.43826324 0.4385089  0.43862793 0.43871108 0.43877873 0.4388272
 0.43879408 0.43870562 0.4386499  0.43863177 0.43857646 0.43847677
 0.43840286 0.4383968  0.43842033 0.43837455 0.4383058  0.43832073
 0.43832713 0.4381641  0.43806377 0.43803614 0.4378765  0.43767226
 0.43751913 0.43749171 0.43739223 0.4372675  0.43721    0.43717194
 0.43704954 0.43688637 0.43676746 0.4366547  0.4365064  0.43639937
 0.4363905  0.43638623 0.436327   0.4362592  0.43624604 0.43630084
 0.4362694  0.43602794 0.43588573 0.43589866 0.43586448 0.4357838
 0.435758   0.43582624 0.4357938  0.43570796 0.43569434 0.43567774
 0.4355232  0.43533358 0.4352259  0.43513483 0.43496212 0.43482107
 0.4347844  0.4347771  0.43472856 0.4346789  0.43469447 0.43475103
 0.43469274 0.43446597 0.4343939  0.43444878 0.43446872 0.4344713
 0.4345383  0.43463835 0.43462738 0.43456396 0.43456817 0.43453428
 0.43440944 0.43430644 0.43426383 0.4341345  0.4339033  0.4337284
 0.43366203 0.43356767 0.4334484  0.43336478 0.43330157 0.43320155
 0.43309882 0.4330093  0.43298334 0.43298277 0.4329769  0.43303537
 0.43318847 0.43333966 0.4333623  0.43335533 0.43340302 0.43338165
 0.43324435 0.43309718 0.432993   0.4328536  0.43267658 0.43259162
 0.4325819  0.4325043  0.4324126  0.4323907  0.43239084 0.43234575
 0.43229365 0.43223873 0.43224326 0.43225205 0.43225846 0.432336
 0.43247482 0.4326016  0.43263438 0.43266854 0.4327153  0.43265554
 0.4325224  0.43243396 0.43235397 0.43218508 0.43201312 0.4319527
 0.43191618 0.43181387 0.43176576 0.43183267 0.43184152 0.43178344
 0.4317721  0.43176365 0.43177122 0.43179122 0.4318796  0.43210733
 0.43236834 0.4325062  0.43254977 0.43259612 0.43262625 0.43256888
 0.4324772  0.43241683 0.4323252  0.43215016 0.43199933 0.43195546
 0.4318922  0.43175426 0.43173042 0.43181506 0.4318169  0.4317577
 0.43183973 0.43201151 0.43213022 0.43223107 0.43246332 0.4328437
 0.4331511  0.43326503 0.43336713 0.43349296 0.4334613  0.43332604
 0.4332402  0.43317646 0.4330297  0.4328409  0.43274498 0.43270144
 0.43258816 0.43250102 0.43258455 0.43267757 0.432615   0.43258914
 0.43272963 0.4327355  0.4326123  0.4325454  0.43255714 0.43247378
 0.4321401  0.43190956 0.4318514  0.4317471  0.43147162 0.4312232
 0.43111914 0.43090633 0.43054643 0.43034294 0.43033314 0.4302208
 0.43004757 0.43007866 0.43029734 0.43038675 0.43029997 0.43033466
 0.4304766  0.43035296 0.4301086  0.43008342 0.43020916 0.43010375
 0.42976338 0.4296941  0.4297401  0.42943427 0.42902723 0.42897567
 0.42889038 0.4284473  0.42826462 0.42848578 0.42840648 0.42818132
 0.42849872 0.4290254  0.4290898  0.4289145  0.426849   0.42920563
 0.4288367  0.42345825 0.42351755 0.42791113 0.42212668 0.4287776 ]
