Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_180_j336_H6', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_180_j336_H6_FITS_custom_ftM_sl180_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17897
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=58, out_features=166, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  395595264.0
params:  9794.0
Trainable parameters:  9794
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5366887
	speed: 0.6551s/iter; left time: 9041.6884s
Epoch: 1 cost time: 90.28095507621765
Epoch: 1, Steps: 139 | Train Loss: 0.7828472 Vali Loss: 0.3661126 Test Loss: 0.4104509
Validation loss decreased (inf --> 0.366113).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2943173
	speed: 1.5020s/iter; left time: 20520.6010s
Epoch: 2 cost time: 88.82478189468384
Epoch: 2, Steps: 139 | Train Loss: 0.3324715 Vali Loss: 0.2391623 Test Loss: 0.2684049
Validation loss decreased (0.366113 --> 0.239162).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2379891
	speed: 1.5112s/iter; left time: 20436.4929s
Epoch: 3 cost time: 86.27729916572571
Epoch: 3, Steps: 139 | Train Loss: 0.2559610 Vali Loss: 0.2056189 Test Loss: 0.2328895
Validation loss decreased (0.239162 --> 0.205619).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2424000
	speed: 1.4798s/iter; left time: 19805.9474s
Epoch: 4 cost time: 84.71402287483215
Epoch: 4, Steps: 139 | Train Loss: 0.2301969 Vali Loss: 0.1896777 Test Loss: 0.2167355
Validation loss decreased (0.205619 --> 0.189678).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2107226
	speed: 1.4139s/iter; left time: 18727.0299s
Epoch: 5 cost time: 83.51411366462708
Epoch: 5, Steps: 139 | Train Loss: 0.2172128 Vali Loss: 0.1809601 Test Loss: 0.2078560
Validation loss decreased (0.189678 --> 0.180960).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2155448
	speed: 1.3714s/iter; left time: 17972.9189s
Epoch: 6 cost time: 77.83781909942627
Epoch: 6, Steps: 139 | Train Loss: 0.2099390 Vali Loss: 0.1759294 Test Loss: 0.2026489
Validation loss decreased (0.180960 --> 0.175929).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2125563
	speed: 1.3987s/iter; left time: 18137.5321s
Epoch: 7 cost time: 77.9559874534607
Epoch: 7, Steps: 139 | Train Loss: 0.2056290 Vali Loss: 0.1729521 Test Loss: 0.1994228
Validation loss decreased (0.175929 --> 0.172952).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2017813
	speed: 1.3448s/iter; left time: 17250.8402s
Epoch: 8 cost time: 77.76854419708252
Epoch: 8, Steps: 139 | Train Loss: 0.2029712 Vali Loss: 0.1711868 Test Loss: 0.1973102
Validation loss decreased (0.172952 --> 0.171187).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1947432
	speed: 1.3263s/iter; left time: 16829.5389s
Epoch: 9 cost time: 77.7730143070221
Epoch: 9, Steps: 139 | Train Loss: 0.2012932 Vali Loss: 0.1699237 Test Loss: 0.1958610
Validation loss decreased (0.171187 --> 0.169924).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1991682
	speed: 1.3867s/iter; left time: 17402.5221s
Epoch: 10 cost time: 78.33536577224731
Epoch: 10, Steps: 139 | Train Loss: 0.2000285 Vali Loss: 0.1690541 Test Loss: 0.1948034
Validation loss decreased (0.169924 --> 0.169054).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1978910
	speed: 1.3606s/iter; left time: 16886.2754s
Epoch: 11 cost time: 80.43536496162415
Epoch: 11, Steps: 139 | Train Loss: 0.1991291 Vali Loss: 0.1680867 Test Loss: 0.1940053
Validation loss decreased (0.169054 --> 0.168087).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1950089
	speed: 1.3634s/iter; left time: 16731.6791s
Epoch: 12 cost time: 84.31240725517273
Epoch: 12, Steps: 139 | Train Loss: 0.1983718 Vali Loss: 0.1674561 Test Loss: 0.1933704
Validation loss decreased (0.168087 --> 0.167456).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1917798
	speed: 1.4648s/iter; left time: 17773.0022s
Epoch: 13 cost time: 85.6741189956665
Epoch: 13, Steps: 139 | Train Loss: 0.1977525 Vali Loss: 0.1670665 Test Loss: 0.1928698
Validation loss decreased (0.167456 --> 0.167067).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1998195
	speed: 1.4758s/iter; left time: 17701.0689s
Epoch: 14 cost time: 84.2580292224884
Epoch: 14, Steps: 139 | Train Loss: 0.1973668 Vali Loss: 0.1669300 Test Loss: 0.1924840
Validation loss decreased (0.167067 --> 0.166930).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2011200
	speed: 1.4117s/iter; left time: 16735.5322s
Epoch: 15 cost time: 82.01280474662781
Epoch: 15, Steps: 139 | Train Loss: 0.1970081 Vali Loss: 0.1663744 Test Loss: 0.1921478
Validation loss decreased (0.166930 --> 0.166374).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2058454
	speed: 1.3948s/iter; left time: 16340.9274s
Epoch: 16 cost time: 80.83837342262268
Epoch: 16, Steps: 139 | Train Loss: 0.1967201 Vali Loss: 0.1660949 Test Loss: 0.1918954
Validation loss decreased (0.166374 --> 0.166095).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1860826
	speed: 1.3775s/iter; left time: 15946.9790s
Epoch: 17 cost time: 78.35176110267639
Epoch: 17, Steps: 139 | Train Loss: 0.1963629 Vali Loss: 0.1657919 Test Loss: 0.1916379
Validation loss decreased (0.166095 --> 0.165792).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1929536
	speed: 1.3606s/iter; left time: 15562.5856s
Epoch: 18 cost time: 80.53665113449097
Epoch: 18, Steps: 139 | Train Loss: 0.1961310 Vali Loss: 0.1655131 Test Loss: 0.1914574
Validation loss decreased (0.165792 --> 0.165513).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1959959
	speed: 1.3971s/iter; left time: 15785.4291s
Epoch: 19 cost time: 83.33820796012878
Epoch: 19, Steps: 139 | Train Loss: 0.1960457 Vali Loss: 0.1653456 Test Loss: 0.1913083
Validation loss decreased (0.165513 --> 0.165346).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2006909
	speed: 1.4105s/iter; left time: 15740.6713s
Epoch: 20 cost time: 78.74232172966003
Epoch: 20, Steps: 139 | Train Loss: 0.1958593 Vali Loss: 0.1652687 Test Loss: 0.1911734
Validation loss decreased (0.165346 --> 0.165269).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2056209
	speed: 1.4245s/iter; left time: 15699.3655s
Epoch: 21 cost time: 84.7346510887146
Epoch: 21, Steps: 139 | Train Loss: 0.1956454 Vali Loss: 0.1650502 Test Loss: 0.1910480
Validation loss decreased (0.165269 --> 0.165050).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1916108
	speed: 1.3925s/iter; left time: 15153.0307s
Epoch: 22 cost time: 80.59967255592346
Epoch: 22, Steps: 139 | Train Loss: 0.1955409 Vali Loss: 0.1650897 Test Loss: 0.1909498
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1905503
	speed: 1.3918s/iter; left time: 14952.1976s
Epoch: 23 cost time: 78.95154523849487
Epoch: 23, Steps: 139 | Train Loss: 0.1954960 Vali Loss: 0.1649612 Test Loss: 0.1908865
Validation loss decreased (0.165050 --> 0.164961).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2086982
	speed: 1.3798s/iter; left time: 14631.0077s
Epoch: 24 cost time: 79.57336783409119
Epoch: 24, Steps: 139 | Train Loss: 0.1954365 Vali Loss: 0.1651041 Test Loss: 0.1908003
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1936988
	speed: 1.3186s/iter; left time: 13798.9601s
Epoch: 25 cost time: 74.42267203330994
Epoch: 25, Steps: 139 | Train Loss: 0.1953125 Vali Loss: 0.1650510 Test Loss: 0.1907394
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1980258
	speed: 1.3446s/iter; left time: 13884.4531s
Epoch: 26 cost time: 78.4534957408905
Epoch: 26, Steps: 139 | Train Loss: 0.1952260 Vali Loss: 0.1645524 Test Loss: 0.1906792
Validation loss decreased (0.164961 --> 0.164552).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1899024
	speed: 1.3599s/iter; left time: 13853.5378s
Epoch: 27 cost time: 79.5555374622345
Epoch: 27, Steps: 139 | Train Loss: 0.1952849 Vali Loss: 0.1645859 Test Loss: 0.1906474
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2070102
	speed: 1.3638s/iter; left time: 13703.4038s
Epoch: 28 cost time: 74.166499376297
Epoch: 28, Steps: 139 | Train Loss: 0.1952509 Vali Loss: 0.1648679 Test Loss: 0.1906028
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1930973
	speed: 1.2949s/iter; left time: 12831.3530s
Epoch: 29 cost time: 75.13477754592896
Epoch: 29, Steps: 139 | Train Loss: 0.1952443 Vali Loss: 0.1648395 Test Loss: 0.1905715
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1956919
	speed: 1.2869s/iter; left time: 12573.3932s
Epoch: 30 cost time: 77.69262957572937
Epoch: 30, Steps: 139 | Train Loss: 0.1950783 Vali Loss: 0.1645455 Test Loss: 0.1905269
Validation loss decreased (0.164552 --> 0.164545).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2004322
	speed: 1.3344s/iter; left time: 12851.7839s
Epoch: 31 cost time: 74.48065280914307
Epoch: 31, Steps: 139 | Train Loss: 0.1950409 Vali Loss: 0.1645318 Test Loss: 0.1905181
Validation loss decreased (0.164545 --> 0.164532).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2085907
	speed: 1.0911s/iter; left time: 10356.4038s
Epoch: 32 cost time: 62.13213229179382
Epoch: 32, Steps: 139 | Train Loss: 0.1950410 Vali Loss: 0.1644206 Test Loss: 0.1904909
Validation loss decreased (0.164532 --> 0.164421).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2161841
	speed: 1.0617s/iter; left time: 9930.0275s
Epoch: 33 cost time: 64.50792527198792
Epoch: 33, Steps: 139 | Train Loss: 0.1949585 Vali Loss: 0.1645461 Test Loss: 0.1904728
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2026363
	speed: 1.1025s/iter; left time: 10158.4887s
Epoch: 34 cost time: 62.98295450210571
Epoch: 34, Steps: 139 | Train Loss: 0.1950308 Vali Loss: 0.1647367 Test Loss: 0.1904550
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2102178
	speed: 1.1053s/iter; left time: 10030.9631s
Epoch: 35 cost time: 65.84276747703552
Epoch: 35, Steps: 139 | Train Loss: 0.1950596 Vali Loss: 0.1643601 Test Loss: 0.1904325
Validation loss decreased (0.164421 --> 0.164360).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1905937
	speed: 1.1229s/iter; left time: 10034.0987s
Epoch: 36 cost time: 64.80802464485168
Epoch: 36, Steps: 139 | Train Loss: 0.1950000 Vali Loss: 0.1645626 Test Loss: 0.1904199
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1907899
	speed: 1.0308s/iter; left time: 9068.3331s
Epoch: 37 cost time: 58.108076095581055
Epoch: 37, Steps: 139 | Train Loss: 0.1949689 Vali Loss: 0.1640635 Test Loss: 0.1904132
Validation loss decreased (0.164360 --> 0.164063).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1937678
	speed: 1.0620s/iter; left time: 9194.3806s
Epoch: 38 cost time: 59.25419902801514
Epoch: 38, Steps: 139 | Train Loss: 0.1949722 Vali Loss: 0.1643090 Test Loss: 0.1904080
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1872485
	speed: 1.0641s/iter; left time: 9064.6747s
Epoch: 39 cost time: 63.40458369255066
Epoch: 39, Steps: 139 | Train Loss: 0.1948414 Vali Loss: 0.1643103 Test Loss: 0.1904084
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2064635
	speed: 1.0931s/iter; left time: 9159.8211s
Epoch: 40 cost time: 66.74579977989197
Epoch: 40, Steps: 139 | Train Loss: 0.1948546 Vali Loss: 0.1644571 Test Loss: 0.1903808
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2032627
	speed: 1.0874s/iter; left time: 8960.9302s
Epoch: 41 cost time: 63.148741483688354
Epoch: 41, Steps: 139 | Train Loss: 0.1949772 Vali Loss: 0.1645051 Test Loss: 0.1903798
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2031480
	speed: 1.0389s/iter; left time: 8416.8858s
Epoch: 42 cost time: 55.957666873931885
Epoch: 42, Steps: 139 | Train Loss: 0.1950178 Vali Loss: 0.1645110 Test Loss: 0.1903775
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1958098
	speed: 1.0032s/iter; left time: 7988.4018s
Epoch: 43 cost time: 56.07379508018494
Epoch: 43, Steps: 139 | Train Loss: 0.1948712 Vali Loss: 0.1644082 Test Loss: 0.1903668
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1985532
	speed: 0.8906s/iter; left time: 6968.2058s
Epoch: 44 cost time: 52.04900598526001
Epoch: 44, Steps: 139 | Train Loss: 0.1948497 Vali Loss: 0.1642679 Test Loss: 0.1903585
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1979607
	speed: 0.9117s/iter; left time: 7006.4959s
Epoch: 45 cost time: 52.24218487739563
Epoch: 45, Steps: 139 | Train Loss: 0.1949180 Vali Loss: 0.1643469 Test Loss: 0.1903566
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2007260
	speed: 0.8716s/iter; left time: 6576.7821s
Epoch: 46 cost time: 49.80381655693054
Epoch: 46, Steps: 139 | Train Loss: 0.1948661 Vali Loss: 0.1642816 Test Loss: 0.1903459
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1955257
	speed: 0.9309s/iter; left time: 6894.9772s
Epoch: 47 cost time: 50.32221841812134
Epoch: 47, Steps: 139 | Train Loss: 0.1948333 Vali Loss: 0.1644265 Test Loss: 0.1903476
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1933245
	speed: 0.9009s/iter; left time: 6547.9425s
Epoch: 48 cost time: 48.797794580459595
Epoch: 48, Steps: 139 | Train Loss: 0.1949039 Vali Loss: 0.1643158 Test Loss: 0.1903448
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1983147
	speed: 0.8381s/iter; left time: 5974.6363s
Epoch: 49 cost time: 49.634095907211304
Epoch: 49, Steps: 139 | Train Loss: 0.1949280 Vali Loss: 0.1645255 Test Loss: 0.1903388
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1955596
	speed: 0.8977s/iter; left time: 6274.9773s
Epoch: 50 cost time: 51.16948390007019
Epoch: 50, Steps: 139 | Train Loss: 0.1948937 Vali Loss: 0.1644429 Test Loss: 0.1903359
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.1846212
	speed: 0.8535s/iter; left time: 5847.5725s
Epoch: 51 cost time: 50.117058753967285
Epoch: 51, Steps: 139 | Train Loss: 0.1949190 Vali Loss: 0.1642744 Test Loss: 0.1903391
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.1988917
	speed: 0.8704s/iter; left time: 5842.4036s
Epoch: 52 cost time: 50.785152196884155
Epoch: 52, Steps: 139 | Train Loss: 0.1948973 Vali Loss: 0.1642784 Test Loss: 0.1903306
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2048419
	speed: 0.9785s/iter; left time: 6431.8521s
Epoch: 53 cost time: 59.12218451499939
Epoch: 53, Steps: 139 | Train Loss: 0.1948805 Vali Loss: 0.1644956 Test Loss: 0.1903264
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.1951560
	speed: 1.0051s/iter; left time: 6466.8179s
Epoch: 54 cost time: 59.61777663230896
Epoch: 54, Steps: 139 | Train Loss: 0.1948881 Vali Loss: 0.1642519 Test Loss: 0.1903253
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.1914072
	speed: 1.0583s/iter; left time: 6662.0783s
Epoch: 55 cost time: 58.17042922973633
Epoch: 55, Steps: 139 | Train Loss: 0.1948798 Vali Loss: 0.1641680 Test Loss: 0.1903273
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.1820659
	speed: 1.0547s/iter; left time: 6492.7117s
Epoch: 56 cost time: 60.4566855430603
Epoch: 56, Steps: 139 | Train Loss: 0.1948998 Vali Loss: 0.1641522 Test Loss: 0.1903181
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.1863444
	speed: 1.0695s/iter; left time: 6435.1534s
Epoch: 57 cost time: 63.23944854736328
Epoch: 57, Steps: 139 | Train Loss: 0.1948112 Vali Loss: 0.1642192 Test Loss: 0.1903232
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_180_j336_H6_FITS_custom_ftM_sl180_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.18846909701824188, mae:0.2824127674102783, rse:0.43207719922065735, corr:[0.45970577 0.4599924  0.4610343  0.46105644 0.4609009  0.4612043
 0.46137798 0.46082836 0.46056488 0.46058166 0.4600845  0.459834
 0.45996237 0.45959422 0.4596289  0.45972228 0.45955116 0.45949343
 0.45963266 0.45964083 0.45940956 0.4594646  0.46001163 0.4599141
 0.45940527 0.45938376 0.4592892  0.4590877  0.45892718 0.45883936
 0.45878047 0.4585424  0.4583674  0.45844215 0.45826992 0.45802313
 0.45813864 0.4580434  0.45801258 0.45800748 0.4579407  0.45789146
 0.45787424 0.45783484 0.45768413 0.45758155 0.45782697 0.4578976
 0.45762572 0.4575991  0.45757392 0.45744407 0.45728698 0.4572249
 0.45727095 0.45719534 0.45705596 0.4570926  0.45707777 0.45693797
 0.45702758 0.457      0.45698866 0.45696816 0.45693755 0.45691752
 0.4567915  0.45661268 0.4565317  0.45648783 0.45656067 0.45652875
 0.45630625 0.4563005  0.4562998  0.45626417 0.45614263 0.45600885
 0.45602623 0.45604378 0.4559544  0.45592362 0.45595366 0.4559051
 0.45595336 0.45587927 0.45581898 0.45574406 0.45570964 0.45571285
 0.4557301  0.45570064 0.4556539  0.4556949  0.45582366 0.455803
 0.4556888  0.45574573 0.45566082 0.45561832 0.45564297 0.45562804
 0.45563033 0.4556001  0.45557266 0.45565286 0.45568684 0.45565373
 0.4557162  0.45567164 0.45571467 0.4556761  0.45555228 0.45547765
 0.45542145 0.4552831  0.45513397 0.45507896 0.45519632 0.45525864
 0.45517072 0.45529264 0.45526907 0.45526028 0.4553357  0.4553494
 0.4553786  0.45540515 0.45544094 0.4555358  0.45553902 0.45553008
 0.45566925 0.455635   0.45561153 0.45565382 0.45558754 0.45549092
 0.45549378 0.4554407  0.45526782 0.45524576 0.4555688  0.4557203
 0.45553866 0.4557064  0.4559184  0.45594978 0.45592886 0.45597
 0.45609728 0.45618403 0.45622212 0.45632282 0.45641702 0.4565178
 0.45673808 0.45688346 0.45683128 0.45682552 0.45691872 0.45698234
 0.45709637 0.45718265 0.4572829  0.4576014  0.45805612 0.45767614
 0.45691475 0.45636344 0.45600468 0.45573053 0.45540074 0.4551208
 0.45507467 0.45498836 0.45472294 0.45451567 0.45432535 0.4540831
 0.4540166  0.4538761  0.45382392 0.45373237 0.45364544 0.45365873
 0.45364234 0.4535264  0.45345977 0.45356163 0.45375174 0.4535513
 0.45317084 0.45301536 0.45275554 0.45260224 0.45256278 0.45241535
 0.4523239  0.45229232 0.45216233 0.45203182 0.45195034 0.45188868
 0.45194584 0.45179603 0.45182195 0.4518692  0.45186064 0.45194533
 0.4520301  0.45197475 0.45186937 0.45187202 0.45206377 0.4520425
 0.45175096 0.45180854 0.45177215 0.45156616 0.45145184 0.45144108
 0.4514535  0.4513776  0.45126614 0.45123172 0.45116007 0.45104852
 0.45119035 0.45111492 0.4510825  0.45112965 0.45117855 0.45123538
 0.45124084 0.4511563  0.45109388 0.45106587 0.45119274 0.4512534
 0.4509954  0.4509599  0.4509656  0.45094267 0.4508611  0.45075348
 0.45078877 0.45076725 0.45058432 0.4505019  0.45052636 0.4504587
 0.4505715  0.4505121  0.45046562 0.45046356 0.4504685  0.45051986
 0.4505936  0.45066583 0.4507214  0.45072562 0.45077938 0.45083782
 0.45066455 0.45061308 0.45052502 0.4505582  0.4506387  0.45058906
 0.45058414 0.4505858  0.45045722 0.45042264 0.4504718  0.45038006
 0.45045424 0.45045137 0.4504962  0.45045412 0.45037913 0.45042038
 0.45045027 0.45033336 0.45025134 0.45022944 0.45026726 0.4503057
 0.45020545 0.45037585 0.4503617  0.45034003 0.4504147  0.45038
 0.4503662  0.45037082 0.45033324 0.45041347 0.450523   0.45056617
 0.45075047 0.4507714  0.45080715 0.4508605  0.45081982 0.45087305
 0.45096716 0.45079544 0.45056978 0.4506535  0.45095977 0.45103717
 0.45083863 0.4510163  0.45110834 0.4510464  0.45098498 0.45087764
 0.45090643 0.4509974  0.45098898 0.4511655  0.4515466  0.4519355
 0.452317   0.45255566 0.45268646 0.45278403 0.45256758 0.45253345
 0.45278582 0.45245695 0.45221812 0.45258602 0.45307398 0.45245987]
