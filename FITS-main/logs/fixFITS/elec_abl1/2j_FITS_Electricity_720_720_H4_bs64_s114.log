Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H4', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_720_j720_H4_FITS_custom_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1475552256.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8375545
	speed: 1.5936s/iter; left time: 20877.6307s
Epoch: 1 cost time: 209.46684288978577
Epoch: 1, Steps: 132 | Train Loss: 1.0043714 Vali Loss: 0.7154537 Test Loss: 0.8330053
Validation loss decreased (inf --> 0.715454).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6881462
	speed: 3.4853s/iter; left time: 45201.0330s
Epoch: 2 cost time: 211.37021660804749
Epoch: 2, Steps: 132 | Train Loss: 0.7107426 Vali Loss: 0.6123198 Test Loss: 0.7177486
Validation loss decreased (0.715454 --> 0.612320).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5994374
	speed: 3.4842s/iter; left time: 44726.2715s
Epoch: 3 cost time: 191.72497057914734
Epoch: 3, Steps: 132 | Train Loss: 0.6151839 Vali Loss: 0.5530648 Test Loss: 0.6509410
Validation loss decreased (0.612320 --> 0.553065).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5269564
	speed: 3.0102s/iter; left time: 38244.2654s
Epoch: 4 cost time: 180.98990416526794
Epoch: 4, Steps: 132 | Train Loss: 0.5467567 Vali Loss: 0.5009292 Test Loss: 0.5925582
Validation loss decreased (0.553065 --> 0.500929).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4709495
	speed: 2.8840s/iter; left time: 36260.9637s
Epoch: 5 cost time: 173.45474481582642
Epoch: 5, Steps: 132 | Train Loss: 0.4903505 Vali Loss: 0.4591615 Test Loss: 0.5453135
Validation loss decreased (0.500929 --> 0.459161).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4329290
	speed: 2.7702s/iter; left time: 34464.6046s
Epoch: 6 cost time: 164.41739535331726
Epoch: 6, Steps: 132 | Train Loss: 0.4429551 Vali Loss: 0.4227569 Test Loss: 0.5039954
Validation loss decreased (0.459161 --> 0.422757).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3861746
	speed: 2.7159s/iter; left time: 33429.4409s
Epoch: 7 cost time: 159.6010046005249
Epoch: 7, Steps: 132 | Train Loss: 0.4029168 Vali Loss: 0.3917064 Test Loss: 0.4687520
Validation loss decreased (0.422757 --> 0.391706).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3542544
	speed: 2.6671s/iter; left time: 32477.6510s
Epoch: 8 cost time: 160.93194031715393
Epoch: 8, Steps: 132 | Train Loss: 0.3686275 Vali Loss: 0.3662677 Test Loss: 0.4398848
Validation loss decreased (0.391706 --> 0.366268).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3405693
	speed: 2.5675s/iter; left time: 30925.2657s
Epoch: 9 cost time: 149.97075128555298
Epoch: 9, Steps: 132 | Train Loss: 0.3394054 Vali Loss: 0.3443837 Test Loss: 0.4147096
Validation loss decreased (0.366268 --> 0.344384).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3159277
	speed: 2.6089s/iter; left time: 31079.8430s
Epoch: 10 cost time: 159.4061086177826
Epoch: 10, Steps: 132 | Train Loss: 0.3141783 Vali Loss: 0.3234569 Test Loss: 0.3902710
Validation loss decreased (0.344384 --> 0.323457).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2850411
	speed: 2.6543s/iter; left time: 31270.2244s
Epoch: 11 cost time: 159.98281073570251
Epoch: 11, Steps: 132 | Train Loss: 0.2923563 Vali Loss: 0.3073664 Test Loss: 0.3716188
Validation loss decreased (0.323457 --> 0.307366).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2714835
	speed: 2.6544s/iter; left time: 30920.7703s
Epoch: 12 cost time: 150.79221367835999
Epoch: 12, Steps: 132 | Train Loss: 0.2734220 Vali Loss: 0.2917096 Test Loss: 0.3534318
Validation loss decreased (0.307366 --> 0.291710).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2568159
	speed: 2.4499s/iter; left time: 28215.3574s
Epoch: 13 cost time: 140.7447350025177
Epoch: 13, Steps: 132 | Train Loss: 0.2569858 Vali Loss: 0.2786913 Test Loss: 0.3386456
Validation loss decreased (0.291710 --> 0.278691).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2312919
	speed: 2.3298s/iter; left time: 26524.7767s
Epoch: 14 cost time: 153.03921723365784
Epoch: 14, Steps: 132 | Train Loss: 0.2426397 Vali Loss: 0.2678182 Test Loss: 0.3256077
Validation loss decreased (0.278691 --> 0.267818).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2291324
	speed: 3.5139s/iter; left time: 39542.0548s
Epoch: 15 cost time: 206.37397933006287
Epoch: 15, Steps: 132 | Train Loss: 0.2301101 Vali Loss: 0.2581786 Test Loss: 0.3141097
Validation loss decreased (0.267818 --> 0.258179).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2137192
	speed: 3.7471s/iter; left time: 41671.3704s
Epoch: 16 cost time: 216.71215724945068
Epoch: 16, Steps: 132 | Train Loss: 0.2191710 Vali Loss: 0.2493476 Test Loss: 0.3034665
Validation loss decreased (0.258179 --> 0.249348).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2084792
	speed: 3.6053s/iter; left time: 39618.4172s
Epoch: 17 cost time: 223.4471824169159
Epoch: 17, Steps: 132 | Train Loss: 0.2095337 Vali Loss: 0.2417877 Test Loss: 0.2947523
Validation loss decreased (0.249348 --> 0.241788).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1999377
	speed: 3.7472s/iter; left time: 40682.9102s
Epoch: 18 cost time: 219.17384552955627
Epoch: 18, Steps: 132 | Train Loss: 0.2010100 Vali Loss: 0.2355702 Test Loss: 0.2871439
Validation loss decreased (0.241788 --> 0.235570).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1929620
	speed: 3.9673s/iter; left time: 42549.0538s
Epoch: 19 cost time: 237.34456253051758
Epoch: 19, Steps: 132 | Train Loss: 0.1935805 Vali Loss: 0.2296750 Test Loss: 0.2796351
Validation loss decreased (0.235570 --> 0.229675).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1849130
	speed: 3.5581s/iter; left time: 37690.5647s
Epoch: 20 cost time: 214.85031700134277
Epoch: 20, Steps: 132 | Train Loss: 0.1870325 Vali Loss: 0.2246472 Test Loss: 0.2737458
Validation loss decreased (0.229675 --> 0.224647).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1745590
	speed: 3.8408s/iter; left time: 40178.9753s
Epoch: 21 cost time: 223.35259580612183
Epoch: 21, Steps: 132 | Train Loss: 0.1812150 Vali Loss: 0.2203022 Test Loss: 0.2681648
Validation loss decreased (0.224647 --> 0.220302).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1743833
	speed: 3.6447s/iter; left time: 37645.7148s
Epoch: 22 cost time: 218.72104024887085
Epoch: 22, Steps: 132 | Train Loss: 0.1760919 Vali Loss: 0.2161009 Test Loss: 0.2634411
Validation loss decreased (0.220302 --> 0.216101).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1715486
	speed: 3.7326s/iter; left time: 38061.7282s
Epoch: 23 cost time: 225.0142948627472
Epoch: 23, Steps: 132 | Train Loss: 0.1715019 Vali Loss: 0.2127141 Test Loss: 0.2588702
Validation loss decreased (0.216101 --> 0.212714).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1592488
	speed: 3.6467s/iter; left time: 36704.1413s
Epoch: 24 cost time: 215.67263007164001
Epoch: 24, Steps: 132 | Train Loss: 0.1675219 Vali Loss: 0.2095664 Test Loss: 0.2550718
Validation loss decreased (0.212714 --> 0.209566).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1642643
	speed: 3.5773s/iter; left time: 35532.8545s
Epoch: 25 cost time: 214.77800512313843
Epoch: 25, Steps: 132 | Train Loss: 0.1638848 Vali Loss: 0.2068910 Test Loss: 0.2517890
Validation loss decreased (0.209566 --> 0.206891).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1609855
	speed: 3.5726s/iter; left time: 35015.0277s
Epoch: 26 cost time: 210.5048644542694
Epoch: 26, Steps: 132 | Train Loss: 0.1607404 Vali Loss: 0.2048412 Test Loss: 0.2490267
Validation loss decreased (0.206891 --> 0.204841).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1518947
	speed: 3.7345s/iter; left time: 36108.7865s
Epoch: 27 cost time: 220.8726909160614
Epoch: 27, Steps: 132 | Train Loss: 0.1578572 Vali Loss: 0.2028769 Test Loss: 0.2461366
Validation loss decreased (0.204841 --> 0.202877).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1454570
	speed: 3.6165s/iter; left time: 34490.2460s
Epoch: 28 cost time: 220.50928854942322
Epoch: 28, Steps: 132 | Train Loss: 0.1554028 Vali Loss: 0.2005323 Test Loss: 0.2435397
Validation loss decreased (0.202877 --> 0.200532).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1556221
	speed: 3.3529s/iter; left time: 31533.8022s
Epoch: 29 cost time: 207.68596148490906
Epoch: 29, Steps: 132 | Train Loss: 0.1531574 Vali Loss: 0.1995766 Test Loss: 0.2413696
Validation loss decreased (0.200532 --> 0.199577).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1517786
	speed: 3.5944s/iter; left time: 33330.6521s
Epoch: 30 cost time: 220.16070199012756
Epoch: 30, Steps: 132 | Train Loss: 0.1511180 Vali Loss: 0.1978363 Test Loss: 0.2396373
Validation loss decreased (0.199577 --> 0.197836).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1495267
	speed: 3.6707s/iter; left time: 33554.3059s
Epoch: 31 cost time: 224.39621233940125
Epoch: 31, Steps: 132 | Train Loss: 0.1492804 Vali Loss: 0.1970855 Test Loss: 0.2377217
Validation loss decreased (0.197836 --> 0.197085).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1485651
	speed: 3.8118s/iter; left time: 34340.4849s
Epoch: 32 cost time: 232.76338124275208
Epoch: 32, Steps: 132 | Train Loss: 0.1476956 Vali Loss: 0.1954154 Test Loss: 0.2362733
Validation loss decreased (0.197085 --> 0.195415).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1414657
	speed: 3.1616s/iter; left time: 28065.2985s
Epoch: 33 cost time: 160.35376453399658
Epoch: 33, Steps: 132 | Train Loss: 0.1463217 Vali Loss: 0.1944019 Test Loss: 0.2347509
Validation loss decreased (0.195415 --> 0.194402).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1434660
	speed: 3.0678s/iter; left time: 26828.1456s
Epoch: 34 cost time: 212.36457300186157
Epoch: 34, Steps: 132 | Train Loss: 0.1450048 Vali Loss: 0.1937234 Test Loss: 0.2334773
Validation loss decreased (0.194402 --> 0.193723).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1423391
	speed: 3.5623s/iter; left time: 30681.8550s
Epoch: 35 cost time: 192.52085733413696
Epoch: 35, Steps: 132 | Train Loss: 0.1438523 Vali Loss: 0.1927817 Test Loss: 0.2322010
Validation loss decreased (0.193723 --> 0.192782).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1372775
	speed: 2.7508s/iter; left time: 23329.8567s
Epoch: 36 cost time: 165.61779618263245
Epoch: 36, Steps: 132 | Train Loss: 0.1427951 Vali Loss: 0.1923028 Test Loss: 0.2313327
Validation loss decreased (0.192782 --> 0.192303).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1445429
	speed: 3.3049s/iter; left time: 27592.2044s
Epoch: 37 cost time: 229.1260266304016
Epoch: 37, Steps: 132 | Train Loss: 0.1418892 Vali Loss: 0.1916175 Test Loss: 0.2303431
Validation loss decreased (0.192303 --> 0.191617).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1425600
	speed: 3.8185s/iter; left time: 31376.3784s
Epoch: 38 cost time: 227.61484551429749
Epoch: 38, Steps: 132 | Train Loss: 0.1410409 Vali Loss: 0.1912398 Test Loss: 0.2295757
Validation loss decreased (0.191617 --> 0.191240).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1434206
	speed: 3.8871s/iter; left time: 31426.8151s
Epoch: 39 cost time: 230.62019395828247
Epoch: 39, Steps: 132 | Train Loss: 0.1403210 Vali Loss: 0.1905641 Test Loss: 0.2287749
Validation loss decreased (0.191240 --> 0.190564).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1383711
	speed: 3.6456s/iter; left time: 28993.0743s
Epoch: 40 cost time: 192.70593738555908
Epoch: 40, Steps: 132 | Train Loss: 0.1396672 Vali Loss: 0.1902712 Test Loss: 0.2281497
Validation loss decreased (0.190564 --> 0.190271).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1431753
	speed: 2.9249s/iter; left time: 22875.2939s
Epoch: 41 cost time: 173.4654815196991
Epoch: 41, Steps: 132 | Train Loss: 0.1390631 Vali Loss: 0.1899090 Test Loss: 0.2275250
Validation loss decreased (0.190271 --> 0.189909).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1429249
	speed: 3.0383s/iter; left time: 23361.8303s
Epoch: 42 cost time: 204.19898986816406
Epoch: 42, Steps: 132 | Train Loss: 0.1385112 Vali Loss: 0.1893227 Test Loss: 0.2270400
Validation loss decreased (0.189909 --> 0.189323).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1342003
	speed: 3.7962s/iter; left time: 28688.1614s
Epoch: 43 cost time: 225.72884440422058
Epoch: 43, Steps: 132 | Train Loss: 0.1379878 Vali Loss: 0.1891101 Test Loss: 0.2264404
Validation loss decreased (0.189323 --> 0.189110).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1386109
	speed: 3.6347s/iter; left time: 26987.3566s
Epoch: 44 cost time: 216.32891082763672
Epoch: 44, Steps: 132 | Train Loss: 0.1375619 Vali Loss: 0.1889995 Test Loss: 0.2260089
Validation loss decreased (0.189110 --> 0.188999).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1391093
	speed: 3.4924s/iter; left time: 25469.9183s
Epoch: 45 cost time: 187.79365348815918
Epoch: 45, Steps: 132 | Train Loss: 0.1371648 Vali Loss: 0.1890288 Test Loss: 0.2255750
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1318853
	speed: 2.9117s/iter; left time: 20850.5546s
Epoch: 46 cost time: 174.4060573577881
Epoch: 46, Steps: 132 | Train Loss: 0.1368042 Vali Loss: 0.1886973 Test Loss: 0.2252171
Validation loss decreased (0.188999 --> 0.188697).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1388037
	speed: 3.0159s/iter; left time: 21198.8267s
Epoch: 47 cost time: 179.9239640235901
Epoch: 47, Steps: 132 | Train Loss: 0.1364689 Vali Loss: 0.1884869 Test Loss: 0.2248296
Validation loss decreased (0.188697 --> 0.188487).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1339957
	speed: 3.5855s/iter; left time: 24729.4231s
Epoch: 48 cost time: 228.45844507217407
Epoch: 48, Steps: 132 | Train Loss: 0.1362109 Vali Loss: 0.1884808 Test Loss: 0.2245458
Validation loss decreased (0.188487 --> 0.188481).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1333586
	speed: 3.8954s/iter; left time: 26352.0899s
Epoch: 49 cost time: 261.59898495674133
Epoch: 49, Steps: 132 | Train Loss: 0.1359320 Vali Loss: 0.1879629 Test Loss: 0.2242505
Validation loss decreased (0.188481 --> 0.187963).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1336939
	speed: 4.5765s/iter; left time: 30355.7713s
Epoch: 50 cost time: 267.84912633895874
Epoch: 50, Steps: 132 | Train Loss: 0.1356828 Vali Loss: 0.1880950 Test Loss: 0.2240267
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.1369343
	speed: 4.0444s/iter; left time: 26292.3977s
Epoch: 51 cost time: 214.9615032672882
Epoch: 51, Steps: 132 | Train Loss: 0.1354784 Vali Loss: 0.1880554 Test Loss: 0.2237676
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.1264746
	speed: 3.6240s/iter; left time: 23080.9921s
Epoch: 52 cost time: 207.09855699539185
Epoch: 52, Steps: 132 | Train Loss: 0.1353175 Vali Loss: 0.1882996 Test Loss: 0.2235517
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.1361579
	speed: 3.5137s/iter; left time: 21914.8431s
Epoch: 53 cost time: 216.35853338241577
Epoch: 53, Steps: 132 | Train Loss: 0.1351465 Vali Loss: 0.1878386 Test Loss: 0.2233481
Validation loss decreased (0.187963 --> 0.187839).  Saving model ...
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.1345006
	speed: 3.7270s/iter; left time: 22753.0312s
Epoch: 54 cost time: 221.18652772903442
Epoch: 54, Steps: 132 | Train Loss: 0.1349244 Vali Loss: 0.1878984 Test Loss: 0.2231749
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.1361598
	speed: 3.9611s/iter; left time: 23659.8605s
Epoch: 55 cost time: 254.17579078674316
Epoch: 55, Steps: 132 | Train Loss: 0.1347928 Vali Loss: 0.1877759 Test Loss: 0.2230092
Validation loss decreased (0.187839 --> 0.187776).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.1339776
	speed: 3.9545s/iter; left time: 23098.1383s
Epoch: 56 cost time: 240.03709268569946
Epoch: 56, Steps: 132 | Train Loss: 0.1346143 Vali Loss: 0.1880020 Test Loss: 0.2228559
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.1299472
	speed: 3.7719s/iter; left time: 21533.8784s
Epoch: 57 cost time: 225.86248254776
Epoch: 57, Steps: 132 | Train Loss: 0.1345735 Vali Loss: 0.1877549 Test Loss: 0.2227243
Validation loss decreased (0.187776 --> 0.187755).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.1352683
	speed: 3.6927s/iter; left time: 20594.0345s
Epoch: 58 cost time: 223.1466782093048
Epoch: 58, Steps: 132 | Train Loss: 0.1344398 Vali Loss: 0.1875395 Test Loss: 0.2225881
Validation loss decreased (0.187755 --> 0.187540).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.1276627
	speed: 3.5530s/iter; left time: 19345.8260s
Epoch: 59 cost time: 209.9378080368042
Epoch: 59, Steps: 132 | Train Loss: 0.1343439 Vali Loss: 0.1879615 Test Loss: 0.2224819
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.1280519
	speed: 3.4285s/iter; left time: 18215.3880s
Epoch: 60 cost time: 214.5097885131836
Epoch: 60, Steps: 132 | Train Loss: 0.1342520 Vali Loss: 0.1877637 Test Loss: 0.2223657
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.1387766
	speed: 3.3528s/iter; left time: 17370.8512s
Epoch: 61 cost time: 199.77734994888306
Epoch: 61, Steps: 132 | Train Loss: 0.1341789 Vali Loss: 0.1875089 Test Loss: 0.2222768
Validation loss decreased (0.187540 --> 0.187509).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.1354169
	speed: 3.3216s/iter; left time: 16770.6307s
Epoch: 62 cost time: 200.92051815986633
Epoch: 62, Steps: 132 | Train Loss: 0.1340901 Vali Loss: 0.1875254 Test Loss: 0.2221893
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.1299635
	speed: 3.2063s/iter; left time: 15765.3960s
Epoch: 63 cost time: 183.03123545646667
Epoch: 63, Steps: 132 | Train Loss: 0.1340665 Vali Loss: 0.1876361 Test Loss: 0.2220977
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.1363748
	speed: 2.9729s/iter; left time: 14225.3281s
Epoch: 64 cost time: 189.05853700637817
Epoch: 64, Steps: 132 | Train Loss: 0.1339336 Vali Loss: 0.1876601 Test Loss: 0.2220179
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.1389734
	speed: 3.1888s/iter; left time: 14837.5229s
Epoch: 65 cost time: 185.16679668426514
Epoch: 65, Steps: 132 | Train Loss: 0.1339092 Vali Loss: 0.1877062 Test Loss: 0.2219449
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.1275280
	speed: 3.2430s/iter; left time: 14661.5255s
Epoch: 66 cost time: 196.87553453445435
Epoch: 66, Steps: 132 | Train Loss: 0.1338763 Vali Loss: 0.1875785 Test Loss: 0.2218839
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.1352594
	speed: 3.3701s/iter; left time: 14791.4839s
Epoch: 67 cost time: 201.94014620780945
Epoch: 67, Steps: 132 | Train Loss: 0.1338265 Vali Loss: 0.1878935 Test Loss: 0.2218286
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.1312885
	speed: 3.3435s/iter; left time: 14233.2949s
Epoch: 68 cost time: 195.08775210380554
Epoch: 68, Steps: 132 | Train Loss: 0.1337917 Vali Loss: 0.1875589 Test Loss: 0.2217704
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.1386437
	speed: 3.2928s/iter; left time: 13582.9815s
Epoch: 69 cost time: 204.77296257019043
Epoch: 69, Steps: 132 | Train Loss: 0.1337523 Vali Loss: 0.1875349 Test Loss: 0.2217279
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.1284294
	speed: 3.5370s/iter; left time: 14123.2953s
Epoch: 70 cost time: 205.09815216064453
Epoch: 70, Steps: 132 | Train Loss: 0.1336979 Vali Loss: 0.1880428 Test Loss: 0.2216722
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.1363876
	speed: 3.6291s/iter; left time: 14012.1451s
Epoch: 71 cost time: 223.11655473709106
Epoch: 71, Steps: 132 | Train Loss: 0.1337042 Vali Loss: 0.1876765 Test Loss: 0.2216223
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.1322351
	speed: 3.4746s/iter; left time: 12956.7000s
Epoch: 72 cost time: 202.9798140525818
Epoch: 72, Steps: 132 | Train Loss: 0.1336830 Vali Loss: 0.1876080 Test Loss: 0.2215842
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.1362703
	speed: 3.4237s/iter; left time: 12315.1182s
Epoch: 73 cost time: 208.25908732414246
Epoch: 73, Steps: 132 | Train Loss: 0.1336248 Vali Loss: 0.1877176 Test Loss: 0.2215500
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.1303038
	speed: 3.3370s/iter; left time: 11562.6675s
Epoch: 74 cost time: 209.71589350700378
Epoch: 74, Steps: 132 | Train Loss: 0.1336067 Vali Loss: 0.1873941 Test Loss: 0.2215085
Validation loss decreased (0.187509 --> 0.187394).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.1264495
	speed: 3.4086s/iter; left time: 11360.7505s
Epoch: 75 cost time: 200.9860715866089
Epoch: 75, Steps: 132 | Train Loss: 0.1335564 Vali Loss: 0.1878114 Test Loss: 0.2214759
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.1361292
	speed: 3.2176s/iter; left time: 10299.5069s
Epoch: 76 cost time: 192.87889528274536
Epoch: 76, Steps: 132 | Train Loss: 0.1335338 Vali Loss: 0.1877496 Test Loss: 0.2214470
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.1344794
	speed: 3.0185s/iter; left time: 9263.7555s
Epoch: 77 cost time: 170.48001337051392
Epoch: 77, Steps: 132 | Train Loss: 0.1335198 Vali Loss: 0.1877047 Test Loss: 0.2214156
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.1391843
	speed: 2.8490s/iter; left time: 8367.6399s
Epoch: 78 cost time: 168.33004546165466
Epoch: 78, Steps: 132 | Train Loss: 0.1335375 Vali Loss: 0.1878523 Test Loss: 0.2213926
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.1377145
	speed: 2.8738s/iter; left time: 8060.9493s
Epoch: 79 cost time: 172.96612215042114
Epoch: 79, Steps: 132 | Train Loss: 0.1335475 Vali Loss: 0.1880895 Test Loss: 0.2213663
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.1358551
	speed: 2.5613s/iter; left time: 6846.3432s
Epoch: 80 cost time: 116.30637001991272
Epoch: 80, Steps: 132 | Train Loss: 0.1335068 Vali Loss: 0.1878807 Test Loss: 0.2213458
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.1390062
	speed: 2.7669s/iter; left time: 7030.5956s
Epoch: 81 cost time: 167.60447788238525
Epoch: 81, Steps: 132 | Train Loss: 0.1334736 Vali Loss: 0.1877158 Test Loss: 0.2213247
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.1268867
	speed: 2.8208s/iter; left time: 6795.4138s
Epoch: 82 cost time: 161.64706182479858
Epoch: 82, Steps: 132 | Train Loss: 0.1334648 Vali Loss: 0.1879988 Test Loss: 0.2213045
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.1316007
	speed: 1.9965s/iter; left time: 4546.0594s
Epoch: 83 cost time: 107.8938524723053
Epoch: 83, Steps: 132 | Train Loss: 0.1334622 Vali Loss: 0.1880924 Test Loss: 0.2212853
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.1373339
	speed: 2.4275s/iter; left time: 5207.0909s
Epoch: 84 cost time: 160.5401885509491
Epoch: 84, Steps: 132 | Train Loss: 0.1334436 Vali Loss: 0.1880535 Test Loss: 0.2212691
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.1320214
	speed: 2.6676s/iter; left time: 5369.7815s
Epoch: 85 cost time: 159.73289275169373
Epoch: 85, Steps: 132 | Train Loss: 0.1334530 Vali Loss: 0.1877582 Test Loss: 0.2212527
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.1338481
	speed: 2.5948s/iter; left time: 4880.8468s
Epoch: 86 cost time: 158.22676825523376
Epoch: 86, Steps: 132 | Train Loss: 0.1334073 Vali Loss: 0.1877588 Test Loss: 0.2212365
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.1299169
	speed: 2.5592s/iter; left time: 4476.1150s
Epoch: 87 cost time: 135.41167306900024
Epoch: 87, Steps: 132 | Train Loss: 0.1334346 Vali Loss: 0.1877380 Test Loss: 0.2212235
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.1334851
	speed: 1.8046s/iter; left time: 2918.0321s
Epoch: 88 cost time: 120.66969919204712
Epoch: 88, Steps: 132 | Train Loss: 0.1334023 Vali Loss: 0.1879872 Test Loss: 0.2212082
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.766650946003179e-06
	iters: 100, epoch: 89 | loss: 0.1329888
	speed: 2.4478s/iter; left time: 3634.9876s
Epoch: 89 cost time: 146.94067478179932
Epoch: 89, Steps: 132 | Train Loss: 0.1334156 Vali Loss: 0.1880541 Test Loss: 0.2211963
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.47831839870302e-06
	iters: 100, epoch: 90 | loss: 0.1304978
	speed: 2.4222s/iter; left time: 3277.1869s
Epoch: 90 cost time: 141.72832560539246
Epoch: 90, Steps: 132 | Train Loss: 0.1334112 Vali Loss: 0.1879599 Test Loss: 0.2211826
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.204402478767869e-06
	iters: 100, epoch: 91 | loss: 0.1340001
	speed: 2.5598s/iter; left time: 3125.5329s
Epoch: 91 cost time: 157.12858414649963
Epoch: 91, Steps: 132 | Train Loss: 0.1334211 Vali Loss: 0.1878718 Test Loss: 0.2211739
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.944182354829475e-06
	iters: 100, epoch: 92 | loss: 0.1335324
	speed: 2.3360s/iter; left time: 2543.9579s
Epoch: 92 cost time: 125.77998638153076
Epoch: 92, Steps: 132 | Train Loss: 0.1333422 Vali Loss: 0.1877460 Test Loss: 0.2211625
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.696973237088e-06
	iters: 100, epoch: 93 | loss: 0.1355802
	speed: 1.8224s/iter; left time: 1744.0800s
Epoch: 93 cost time: 110.08572030067444
Epoch: 93, Steps: 132 | Train Loss: 0.1333766 Vali Loss: 0.1878895 Test Loss: 0.2211530
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.462124575233601e-06
	iters: 100, epoch: 94 | loss: 0.1308614
	speed: 1.5418s/iter; left time: 1271.9721s
Epoch: 94 cost time: 87.35937786102295
Epoch: 94, Steps: 132 | Train Loss: 0.1333870 Vali Loss: 0.1881133 Test Loss: 0.2211435
EarlyStopping counter: 20 out of 20
Early stopping
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1475552256.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2275106
	speed: 0.7226s/iter; left time: 9466.8245s
Epoch: 1 cost time: 95.41848349571228
Epoch: 1, Steps: 132 | Train Loss: 0.2312791 Vali Loss: 0.1875478 Test Loss: 0.2199048
Validation loss decreased (inf --> 0.187548).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2440414
	speed: 1.5723s/iter; left time: 20391.0486s
Epoch: 2 cost time: 94.25342655181885
Epoch: 2, Steps: 132 | Train Loss: 0.2309391 Vali Loss: 0.1872691 Test Loss: 0.2196614
Validation loss decreased (0.187548 --> 0.187269).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2303731
	speed: 2.4165s/iter; left time: 31020.6004s
Epoch: 3 cost time: 151.22329425811768
Epoch: 3, Steps: 132 | Train Loss: 0.2307188 Vali Loss: 0.1873483 Test Loss: 0.2195894
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2248090
	speed: 2.6862s/iter; left time: 34128.4234s
Epoch: 4 cost time: 159.98287343978882
Epoch: 4, Steps: 132 | Train Loss: 0.2305740 Vali Loss: 0.1870690 Test Loss: 0.2195041
Validation loss decreased (0.187269 --> 0.187069).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2344873
	speed: 2.7630s/iter; left time: 34738.8592s
Epoch: 5 cost time: 164.89912819862366
Epoch: 5, Steps: 132 | Train Loss: 0.2305749 Vali Loss: 0.1871351 Test Loss: 0.2195776
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2308404
	speed: 2.7196s/iter; left time: 33834.0131s
Epoch: 6 cost time: 162.16047835350037
Epoch: 6, Steps: 132 | Train Loss: 0.2305339 Vali Loss: 0.1869046 Test Loss: 0.2194176
Validation loss decreased (0.187069 --> 0.186905).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2301973
	speed: 2.6681s/iter; left time: 32841.0899s
Epoch: 7 cost time: 165.45794916152954
Epoch: 7, Steps: 132 | Train Loss: 0.2304009 Vali Loss: 0.1872283 Test Loss: 0.2194310
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2231596
	speed: 2.7260s/iter; left time: 33194.1479s
Epoch: 8 cost time: 159.68535375595093
Epoch: 8, Steps: 132 | Train Loss: 0.2304913 Vali Loss: 0.1870201 Test Loss: 0.2194529
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2309891
	speed: 2.7227s/iter; left time: 32795.0906s
Epoch: 9 cost time: 165.56714010238647
Epoch: 9, Steps: 132 | Train Loss: 0.2303064 Vali Loss: 0.1871420 Test Loss: 0.2193525
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2396986
	speed: 2.8822s/iter; left time: 34335.2554s
Epoch: 10 cost time: 169.79620838165283
Epoch: 10, Steps: 132 | Train Loss: 0.2303635 Vali Loss: 0.1872186 Test Loss: 0.2192529
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2359369
	speed: 2.7575s/iter; left time: 32486.3107s
Epoch: 11 cost time: 161.91716122627258
Epoch: 11, Steps: 132 | Train Loss: 0.2303653 Vali Loss: 0.1869610 Test Loss: 0.2193827
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2380233
	speed: 2.8824s/iter; left time: 33576.6098s
Epoch: 12 cost time: 182.98559141159058
Epoch: 12, Steps: 132 | Train Loss: 0.2303257 Vali Loss: 0.1869031 Test Loss: 0.2193882
Validation loss decreased (0.186905 --> 0.186903).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2387851
	speed: 3.0834s/iter; left time: 35511.0430s
Epoch: 13 cost time: 177.9489541053772
Epoch: 13, Steps: 132 | Train Loss: 0.2303798 Vali Loss: 0.1868881 Test Loss: 0.2193853
Validation loss decreased (0.186903 --> 0.186888).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2236962
	speed: 2.8988s/iter; left time: 33002.8598s
Epoch: 14 cost time: 188.70913314819336
Epoch: 14, Steps: 132 | Train Loss: 0.2302705 Vali Loss: 0.1866017 Test Loss: 0.2194193
Validation loss decreased (0.186888 --> 0.186602).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2218633
	speed: 2.9820s/iter; left time: 33556.8838s
Epoch: 15 cost time: 179.2203254699707
Epoch: 15, Steps: 132 | Train Loss: 0.2302471 Vali Loss: 0.1865507 Test Loss: 0.2193381
Validation loss decreased (0.186602 --> 0.186551).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2303801
	speed: 3.0804s/iter; left time: 34257.2319s
Epoch: 16 cost time: 186.61570620536804
Epoch: 16, Steps: 132 | Train Loss: 0.2303228 Vali Loss: 0.1871041 Test Loss: 0.2193557
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2293991
	speed: 3.0074s/iter; left time: 33048.0682s
Epoch: 17 cost time: 193.56100368499756
Epoch: 17, Steps: 132 | Train Loss: 0.2302692 Vali Loss: 0.1868325 Test Loss: 0.2193026
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2440568
	speed: 3.0155s/iter; left time: 32739.5532s
Epoch: 18 cost time: 177.67137122154236
Epoch: 18, Steps: 132 | Train Loss: 0.2302384 Vali Loss: 0.1868898 Test Loss: 0.2193453
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2202118
	speed: 2.6744s/iter; left time: 28682.6568s
Epoch: 19 cost time: 155.0597608089447
Epoch: 19, Steps: 132 | Train Loss: 0.2302971 Vali Loss: 0.1869833 Test Loss: 0.2192840
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2127099
	speed: 2.5279s/iter; left time: 26778.4773s
Epoch: 20 cost time: 160.33531665802002
Epoch: 20, Steps: 132 | Train Loss: 0.2302881 Vali Loss: 0.1868215 Test Loss: 0.2193484
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2177784
	speed: 2.6188s/iter; left time: 27395.7268s
Epoch: 21 cost time: 161.4810085296631
Epoch: 21, Steps: 132 | Train Loss: 0.2301836 Vali Loss: 0.1868738 Test Loss: 0.2193395
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2322565
	speed: 2.6953s/iter; left time: 27839.7409s
Epoch: 22 cost time: 162.2228057384491
Epoch: 22, Steps: 132 | Train Loss: 0.2302076 Vali Loss: 0.1867376 Test Loss: 0.2192982
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2450825
	speed: 2.6315s/iter; left time: 26833.3449s
Epoch: 23 cost time: 158.87121891975403
Epoch: 23, Steps: 132 | Train Loss: 0.2302070 Vali Loss: 0.1869945 Test Loss: 0.2192516
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2243566
	speed: 2.5840s/iter; left time: 26007.7760s
Epoch: 24 cost time: 151.6253924369812
Epoch: 24, Steps: 132 | Train Loss: 0.2300099 Vali Loss: 0.1866578 Test Loss: 0.2193504
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2242538
	speed: 2.5737s/iter; left time: 25564.3149s
Epoch: 25 cost time: 152.3836727142334
Epoch: 25, Steps: 132 | Train Loss: 0.2302502 Vali Loss: 0.1866279 Test Loss: 0.2192848
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2120927
	speed: 2.5653s/iter; left time: 25142.1265s
Epoch: 26 cost time: 158.1750009059906
Epoch: 26, Steps: 132 | Train Loss: 0.2301444 Vali Loss: 0.1869669 Test Loss: 0.2192888
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2171024
	speed: 2.6445s/iter; left time: 25569.7187s
Epoch: 27 cost time: 163.71018290519714
Epoch: 27, Steps: 132 | Train Loss: 0.2301058 Vali Loss: 0.1866117 Test Loss: 0.2193037
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2371802
	speed: 2.7594s/iter; left time: 26316.4946s
Epoch: 28 cost time: 171.27934789657593
Epoch: 28, Steps: 132 | Train Loss: 0.2301377 Vali Loss: 0.1866635 Test Loss: 0.2193100
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2305568
	speed: 2.7704s/iter; left time: 26055.6761s
Epoch: 29 cost time: 165.78023982048035
Epoch: 29, Steps: 132 | Train Loss: 0.2302836 Vali Loss: 0.1869374 Test Loss: 0.2193117
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2308264
	speed: 2.6446s/iter; left time: 24523.4966s
Epoch: 30 cost time: 155.6533329486847
Epoch: 30, Steps: 132 | Train Loss: 0.2301251 Vali Loss: 0.1867937 Test Loss: 0.2192949
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2224441
	speed: 2.5417s/iter; left time: 23233.3485s
Epoch: 31 cost time: 159.64756846427917
Epoch: 31, Steps: 132 | Train Loss: 0.2302078 Vali Loss: 0.1866821 Test Loss: 0.2192940
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2374495
	speed: 2.4032s/iter; left time: 21650.1702s
Epoch: 32 cost time: 146.59149360656738
Epoch: 32, Steps: 132 | Train Loss: 0.2301236 Vali Loss: 0.1867133 Test Loss: 0.2192796
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2310567
	speed: 2.3883s/iter; left time: 21200.6156s
Epoch: 33 cost time: 146.13203144073486
Epoch: 33, Steps: 132 | Train Loss: 0.2301951 Vali Loss: 0.1866173 Test Loss: 0.2192964
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2490505
	speed: 2.5032s/iter; left time: 21890.6501s
Epoch: 34 cost time: 154.3355631828308
Epoch: 34, Steps: 132 | Train Loss: 0.2300665 Vali Loss: 0.1866208 Test Loss: 0.2192798
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2168695
	speed: 2.3426s/iter; left time: 20176.4699s
Epoch: 35 cost time: 140.28527855873108
Epoch: 35, Steps: 132 | Train Loss: 0.2300975 Vali Loss: 0.1870135 Test Loss: 0.2192845
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_720_j720_H4_FITS_custom_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2176436483860016, mae:0.3129335045814514, rse:0.46537214517593384, corr:[0.4465943  0.44913605 0.44976306 0.45003852 0.45034778 0.4504699
 0.45038593 0.45025265 0.45015076 0.4500625  0.44995064 0.44988048
 0.44985262 0.44982463 0.449771   0.44972718 0.44971767 0.4497732
 0.44983855 0.44977352 0.44962305 0.44955194 0.4496411  0.449844
 0.45011008 0.45046264 0.45055178 0.45044267 0.45036814 0.45032385
 0.45022693 0.45008734 0.44994572 0.44984987 0.4497744  0.44969535
 0.44961315 0.44955027 0.44954342 0.44955805 0.44953462 0.4494824
 0.44944012 0.4493628  0.44929823 0.44930547 0.44933948 0.44937378
 0.44948938 0.44971487 0.4498105  0.44975162 0.44969657 0.44968706
 0.44965705 0.44956037 0.44941893 0.44930205 0.4492537  0.44924963
 0.44923612 0.44916934 0.4490821  0.4490259  0.44900858 0.44901863
 0.4490133  0.4489032  0.44879112 0.4487684  0.44881022 0.44885227
 0.4489257  0.4490523  0.44906616 0.44898036 0.44888896 0.44882137
 0.44877365 0.44872808 0.44870135 0.44867802 0.44862586 0.44855466
 0.448506   0.44850805 0.4485569  0.4485962  0.44857582 0.44851604
 0.4484542  0.4483607  0.4482657  0.44825253 0.44827047 0.44828933
 0.44836384 0.44847602 0.448477   0.4484068  0.4483571  0.44832945
 0.44827506 0.44817537 0.4480546  0.4479736  0.44794673 0.44794708
 0.44792557 0.44787753 0.44783503 0.44781023 0.4477922  0.44777405
 0.44770873 0.4475552  0.44743606 0.44743013 0.4475042  0.4476105
 0.44776297 0.44786635 0.4478637  0.44783702 0.44782692 0.44781786
 0.44778895 0.44773224 0.4476594  0.44757453 0.44748402 0.44741008
 0.44738367 0.44739935 0.44745398 0.44750562 0.4475202  0.4475229
 0.44757372 0.44763464 0.44766974 0.44766933 0.44766983 0.44771212
 0.44780883 0.44791883 0.4479086  0.44783837 0.44780383 0.4478204
 0.44782564 0.44776633 0.44765052 0.44751784 0.44740844 0.4473466
 0.4473375  0.44735748 0.44741622 0.4474673  0.44748402 0.44749075
 0.44753894 0.44756016 0.4475362  0.44749892 0.4474376  0.44736624
 0.44732442 0.4474227  0.4474178  0.44731933 0.44725466 0.4472092
 0.44711843 0.4469613  0.44678652 0.4466524  0.4465593  0.44650576
 0.44648597 0.44648308 0.44651416 0.44654563 0.4465306  0.44649902
 0.44646698 0.44633865 0.44618353 0.44609705 0.4460731  0.44607008
 0.4461366  0.4463101  0.44637474 0.44631007 0.44622898 0.44616923
 0.44609743 0.4459759  0.4458287  0.44568595 0.44558164 0.44553
 0.44550547 0.4454548  0.44541323 0.44539845 0.44539818 0.4454007
 0.44536513 0.44517356 0.4449429  0.44484118 0.44485733 0.44492984
 0.44507614 0.44530118 0.44540754 0.4453804  0.44532833 0.4452932
 0.4452595  0.4452046  0.44514942 0.4450775  0.44499332 0.44490272
 0.4448323  0.44478124 0.44476616 0.44475472 0.44472778 0.44467843
 0.44462082 0.4444691  0.44432163 0.444267   0.44430172 0.44435686
 0.44445765 0.44462535 0.4446924  0.44465572 0.44462046 0.44461933
 0.4446114  0.4445248  0.44438913 0.44425023 0.4441544  0.44413063
 0.44415453 0.44418022 0.44419044 0.44418585 0.44416332 0.44414115
 0.44411215 0.4440309  0.4439524  0.44393843 0.44397485 0.44402117
 0.44412395 0.44427788 0.44436797 0.44439116 0.44439867 0.44437888
 0.44430068 0.44417197 0.44402558 0.44388956 0.44377875 0.44369277
 0.44364744 0.44363058 0.44365856 0.4436816  0.4436729  0.44364473
 0.44357294 0.44343412 0.4433343  0.44334215 0.4434426  0.4435937
 0.4438124  0.44398546 0.44402742 0.44402343 0.44405085 0.44410697
 0.44414696 0.4441177  0.444014   0.44386786 0.44374037 0.44367108
 0.44364753 0.44363105 0.44361776 0.4436044  0.44358474 0.44356868
 0.44357863 0.44357157 0.4435438  0.4435321  0.44354662 0.44358096
 0.44364417 0.44373047 0.44375494 0.44374874 0.44376314 0.44378924
 0.44378188 0.44371954 0.44364595 0.44358656 0.4435338  0.44346747
 0.44341457 0.4433887  0.44341147 0.4434299  0.4433984  0.44334066
 0.44330963 0.44324526 0.44313145 0.44304165 0.44294357 0.4428369
 0.44276795 0.44283566 0.44277927 0.44261298 0.44249016 0.44245055
 0.44243315 0.44236988 0.4422377  0.4420789  0.441951   0.44189972
 0.44192368 0.44195244 0.44197294 0.44195452 0.4418958  0.44185188
 0.44185707 0.44182763 0.4417741  0.44176415 0.4417367  0.4416658
 0.44162717 0.44170725 0.44172823 0.441663   0.44159064 0.44152844
 0.44144306 0.44133013 0.44122413 0.44115603 0.44113055 0.44111755
 0.44109502 0.44102958 0.44096744 0.44092482 0.44089565 0.44088107
 0.44087443 0.44079378 0.44071326 0.4407044  0.44071704 0.44071642
 0.4407662  0.4408958  0.44095725 0.44090843 0.44083726 0.4408081
 0.44080108 0.44077832 0.44070956 0.44059357 0.4404547  0.44034344
 0.44028232 0.4402388  0.44020674 0.44016716 0.44011757 0.4400863
 0.4400744  0.44000548 0.43993396 0.43994826 0.43999594 0.440019
 0.4400548  0.44013518 0.44015774 0.44012177 0.44009802 0.4401047
 0.4400872  0.44000643 0.43990108 0.43982828 0.4398025  0.43978953
 0.43980598 0.43981737 0.43983376 0.43985212 0.4398538  0.439847
 0.4398275  0.4397354  0.43964404 0.4396268  0.43965632 0.43969718
 0.43979815 0.43994176 0.44002363 0.44002998 0.44001102 0.43997744
 0.439921   0.43982685 0.43971956 0.4396181  0.4395489  0.4395289
 0.43955192 0.4395843  0.43961656 0.4396153  0.43956894 0.43951434
 0.43941844 0.43924332 0.43910858 0.43908858 0.43917936 0.4393388
 0.43958318 0.43978474 0.43986294 0.43988135 0.43989128 0.43989238
 0.43987268 0.43980587 0.4397123  0.4396171  0.4395439  0.43950698
 0.43951598 0.439525   0.43954304 0.43954474 0.4395025  0.43943596
 0.43940476 0.4393934  0.4393849  0.43939522 0.4394194  0.43945915
 0.4395439  0.43964416 0.43965018 0.4395924  0.4395582  0.43957847
 0.43960357 0.43957472 0.4394981  0.4394063  0.43934065 0.43932742
 0.43936896 0.4394334  0.43950558 0.43953162 0.43949115 0.43943468
 0.4394026  0.4393202  0.4392117  0.43913695 0.4390653  0.43897527
 0.4389054  0.43896884 0.43894932 0.4388506  0.4387765  0.43874767
 0.4386906  0.4385553  0.43837363 0.43821505 0.4381037  0.43804145
 0.4380127  0.4379699  0.4379386  0.43793222 0.43791723 0.43789858
 0.43785155 0.43769276 0.4375089  0.43741095 0.4373657  0.43732965
 0.43735328 0.43750566 0.43759447 0.43755    0.43745634 0.43738723
 0.437333   0.437268   0.43717268 0.4370623  0.43695575 0.43688604
 0.43685946 0.4368376  0.43682766 0.43681934 0.43678564 0.4367337
 0.4366297  0.4364232  0.43627742 0.43624407 0.43627357 0.43630055
 0.43633887 0.4364089  0.4364139  0.4363394  0.43627974 0.4362763
 0.43630818 0.43630594 0.43622813 0.43610373 0.43599144 0.43591338
 0.4358635  0.4357943  0.43573964 0.43568668 0.43559703 0.43545943
 0.43531418 0.43516144 0.43504363 0.43500084 0.43499333 0.43496352
 0.4349911  0.43510136 0.43515083 0.43508208 0.43499202 0.43493846
 0.43493193 0.43490905 0.43483618 0.43473968 0.43465045 0.43461275
 0.4346358  0.43465608 0.43467253 0.43466    0.43461746 0.43457744
 0.43456352 0.43450648 0.43443382 0.43438902 0.4343548  0.43432266
 0.434364   0.43444526 0.4344445  0.43437943 0.4343352  0.43433058
 0.43430343 0.43421304 0.43407375 0.43396312 0.43391788 0.4339377
 0.43398714 0.43401843 0.4340348  0.4340248  0.43396595 0.4338836
 0.43377727 0.43362787 0.43353108 0.43356022 0.43366063 0.4337924
 0.43401802 0.4342036  0.43424976 0.43419394 0.4341194  0.43406194
 0.4340274  0.43398738 0.43393397 0.43385378 0.43376404 0.43370616
 0.43369517 0.4337003  0.4337238  0.43371737 0.43365845 0.43358067
 0.43355232 0.4335596  0.43356434 0.43357322 0.43360293 0.43366492
 0.4337831  0.433909   0.43389922 0.43379027 0.43370715 0.4337017
 0.4337189  0.43367493 0.4335739  0.4334649  0.43338647 0.43333393
 0.43328726 0.433249   0.43325865 0.43330324 0.43332407 0.433307
 0.43327078 0.43316516 0.4330461  0.4329506  0.4328352  0.43268502
 0.4325941  0.4327223  0.43278357 0.4326581  0.43246147 0.43229094
 0.43218458 0.43210447 0.43202224 0.43191323 0.4317797  0.4316661
 0.4316057  0.431563   0.43153742 0.43150792 0.43147504 0.43147907
 0.43148357 0.43135104 0.43110266 0.43090874 0.43083465 0.4308601
 0.43098712 0.4311874  0.43122718 0.43108872 0.43096122 0.4309108
 0.43086815 0.43077055 0.43065315 0.43057215 0.43052214 0.43046364
 0.43038496 0.43035042 0.43039888 0.4304463  0.43037608 0.42605674
 0.43023926 0.43026504 0.43024108 0.4258548  0.4257569  0.4297908 ]
