Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_90_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_90_j720_H8_FITS_custom_ftM_sl90_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17603
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=42, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  652313088.0
params:  16254.0
Trainable parameters:  16254
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.9644446
	speed: 0.1998s/iter; left time: 2716.8673s
Epoch: 1 cost time: 27.078075885772705
Epoch: 1, Steps: 137 | Train Loss: 1.4456315 Vali Loss: 0.6554478 Test Loss: 0.7203459
Validation loss decreased (inf --> 0.655448).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5129743
	speed: 0.4724s/iter; left time: 6360.9133s
Epoch: 2 cost time: 26.656599044799805
Epoch: 2, Steps: 137 | Train Loss: 0.5819833 Vali Loss: 0.4005896 Test Loss: 0.4455000
Validation loss decreased (0.655448 --> 0.400590).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3765581
	speed: 0.4861s/iter; left time: 6477.9163s
Epoch: 3 cost time: 26.996103286743164
Epoch: 3, Steps: 137 | Train Loss: 0.4140637 Vali Loss: 0.3179388 Test Loss: 0.3563461
Validation loss decreased (0.400590 --> 0.317939).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3366711
	speed: 0.4938s/iter; left time: 6512.8037s
Epoch: 4 cost time: 27.572229146957397
Epoch: 4, Steps: 137 | Train Loss: 0.3518888 Vali Loss: 0.2826306 Test Loss: 0.3177567
Validation loss decreased (0.317939 --> 0.282631).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3206488
	speed: 0.4885s/iter; left time: 6376.6226s
Epoch: 5 cost time: 26.547043800354004
Epoch: 5, Steps: 137 | Train Loss: 0.3228817 Vali Loss: 0.2644656 Test Loss: 0.2980561
Validation loss decreased (0.282631 --> 0.264466).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2930573
	speed: 0.4791s/iter; left time: 6187.8379s
Epoch: 6 cost time: 26.135493278503418
Epoch: 6, Steps: 137 | Train Loss: 0.3072377 Vali Loss: 0.2539424 Test Loss: 0.2867265
Validation loss decreased (0.264466 --> 0.253942).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2940882
	speed: 0.4926s/iter; left time: 6295.0142s
Epoch: 7 cost time: 26.992104530334473
Epoch: 7, Steps: 137 | Train Loss: 0.2978508 Vali Loss: 0.2470885 Test Loss: 0.2796694
Validation loss decreased (0.253942 --> 0.247089).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2779841
	speed: 0.4865s/iter; left time: 6150.1364s
Epoch: 8 cost time: 26.66529369354248
Epoch: 8, Steps: 137 | Train Loss: 0.2918245 Vali Loss: 0.2428158 Test Loss: 0.2750400
Validation loss decreased (0.247089 --> 0.242816).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2942191
	speed: 0.4833s/iter; left time: 6043.7098s
Epoch: 9 cost time: 26.25774621963501
Epoch: 9, Steps: 137 | Train Loss: 0.2878984 Vali Loss: 0.2397133 Test Loss: 0.2718606
Validation loss decreased (0.242816 --> 0.239713).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2998489
	speed: 0.4885s/iter; left time: 6041.2245s
Epoch: 10 cost time: 26.460984230041504
Epoch: 10, Steps: 137 | Train Loss: 0.2850913 Vali Loss: 0.2377206 Test Loss: 0.2696280
Validation loss decreased (0.239713 --> 0.237721).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2855530
	speed: 0.4768s/iter; left time: 5832.0487s
Epoch: 11 cost time: 26.224661588668823
Epoch: 11, Steps: 137 | Train Loss: 0.2830641 Vali Loss: 0.2360771 Test Loss: 0.2680209
Validation loss decreased (0.237721 --> 0.236077).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2685826
	speed: 0.4854s/iter; left time: 5870.4783s
Epoch: 12 cost time: 26.63675832748413
Epoch: 12, Steps: 137 | Train Loss: 0.2815894 Vali Loss: 0.2348726 Test Loss: 0.2668176
Validation loss decreased (0.236077 --> 0.234873).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2678746
	speed: 0.4895s/iter; left time: 5853.1911s
Epoch: 13 cost time: 27.17994523048401
Epoch: 13, Steps: 137 | Train Loss: 0.2803868 Vali Loss: 0.2339971 Test Loss: 0.2659049
Validation loss decreased (0.234873 --> 0.233997).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2743731
	speed: 0.4897s/iter; left time: 5788.8283s
Epoch: 14 cost time: 26.896446228027344
Epoch: 14, Steps: 137 | Train Loss: 0.2796407 Vali Loss: 0.2335692 Test Loss: 0.2652018
Validation loss decreased (0.233997 --> 0.233569).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2761074
	speed: 0.4874s/iter; left time: 5694.6393s
Epoch: 15 cost time: 27.01461672782898
Epoch: 15, Steps: 137 | Train Loss: 0.2789473 Vali Loss: 0.2333287 Test Loss: 0.2646241
Validation loss decreased (0.233569 --> 0.233329).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2961639
	speed: 0.4937s/iter; left time: 5699.9146s
Epoch: 16 cost time: 26.727132081985474
Epoch: 16, Steps: 137 | Train Loss: 0.2784366 Vali Loss: 0.2326276 Test Loss: 0.2641885
Validation loss decreased (0.233329 --> 0.232628).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2800149
	speed: 0.4780s/iter; left time: 5453.9898s
Epoch: 17 cost time: 26.463987350463867
Epoch: 17, Steps: 137 | Train Loss: 0.2779900 Vali Loss: 0.2325688 Test Loss: 0.2638246
Validation loss decreased (0.232628 --> 0.232569).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2854527
	speed: 0.4730s/iter; left time: 5332.0283s
Epoch: 18 cost time: 26.333216190338135
Epoch: 18, Steps: 137 | Train Loss: 0.2775857 Vali Loss: 0.2320245 Test Loss: 0.2634890
Validation loss decreased (0.232569 --> 0.232025).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2788529
	speed: 0.4831s/iter; left time: 5379.7080s
Epoch: 19 cost time: 26.906449794769287
Epoch: 19, Steps: 137 | Train Loss: 0.2772868 Vali Loss: 0.2313079 Test Loss: 0.2632102
Validation loss decreased (0.232025 --> 0.231308).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2707737
	speed: 0.4787s/iter; left time: 5264.7253s
Epoch: 20 cost time: 26.98293423652649
Epoch: 20, Steps: 137 | Train Loss: 0.2770639 Vali Loss: 0.2320414 Test Loss: 0.2629887
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2692341
	speed: 0.4786s/iter; left time: 5197.6804s
Epoch: 21 cost time: 27.17405605316162
Epoch: 21, Steps: 137 | Train Loss: 0.2768464 Vali Loss: 0.2312639 Test Loss: 0.2627999
Validation loss decreased (0.231308 --> 0.231264).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2719184
	speed: 0.4778s/iter; left time: 5123.9194s
Epoch: 22 cost time: 27.31405544281006
Epoch: 22, Steps: 137 | Train Loss: 0.2765002 Vali Loss: 0.2308303 Test Loss: 0.2626028
Validation loss decreased (0.231264 --> 0.230830).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2695518
	speed: 0.4759s/iter; left time: 5038.8028s
Epoch: 23 cost time: 26.991264820098877
Epoch: 23, Steps: 137 | Train Loss: 0.2763785 Vali Loss: 0.2311109 Test Loss: 0.2624483
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2784345
	speed: 0.4772s/iter; left time: 4986.5699s
Epoch: 24 cost time: 26.981032848358154
Epoch: 24, Steps: 137 | Train Loss: 0.2762215 Vali Loss: 0.2310750 Test Loss: 0.2623221
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2699310
	speed: 0.4884s/iter; left time: 5036.4728s
Epoch: 25 cost time: 27.42463994026184
Epoch: 25, Steps: 137 | Train Loss: 0.2760933 Vali Loss: 0.2308649 Test Loss: 0.2621743
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2873017
	speed: 0.4748s/iter; left time: 4831.3514s
Epoch: 26 cost time: 26.301205158233643
Epoch: 26, Steps: 137 | Train Loss: 0.2759038 Vali Loss: 0.2312353 Test Loss: 0.2620629
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2750017
	speed: 0.4778s/iter; left time: 4796.8112s
Epoch: 27 cost time: 26.65675973892212
Epoch: 27, Steps: 137 | Train Loss: 0.2758376 Vali Loss: 0.2309700 Test Loss: 0.2619701
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2671946
	speed: 0.4837s/iter; left time: 4789.4645s
Epoch: 28 cost time: 26.540685176849365
Epoch: 28, Steps: 137 | Train Loss: 0.2756570 Vali Loss: 0.2296283 Test Loss: 0.2618794
Validation loss decreased (0.230830 --> 0.229628).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2873811
	speed: 0.4753s/iter; left time: 4641.1100s
Epoch: 29 cost time: 26.39944362640381
Epoch: 29, Steps: 137 | Train Loss: 0.2756045 Vali Loss: 0.2304049 Test Loss: 0.2617768
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2805017
	speed: 0.4820s/iter; left time: 4640.7106s
Epoch: 30 cost time: 26.55197525024414
Epoch: 30, Steps: 137 | Train Loss: 0.2755082 Vali Loss: 0.2304276 Test Loss: 0.2617065
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2640413
	speed: 0.4932s/iter; left time: 4681.2582s
Epoch: 31 cost time: 27.4095778465271
Epoch: 31, Steps: 137 | Train Loss: 0.2754518 Vali Loss: 0.2300952 Test Loss: 0.2616494
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2729318
	speed: 0.4839s/iter; left time: 4526.3042s
Epoch: 32 cost time: 26.395354747772217
Epoch: 32, Steps: 137 | Train Loss: 0.2753921 Vali Loss: 0.2302821 Test Loss: 0.2615854
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2721934
	speed: 0.4848s/iter; left time: 4468.3595s
Epoch: 33 cost time: 26.213509798049927
Epoch: 33, Steps: 137 | Train Loss: 0.2753158 Vali Loss: 0.2301260 Test Loss: 0.2615151
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2782992
	speed: 0.4879s/iter; left time: 4429.9663s
Epoch: 34 cost time: 27.179888248443604
Epoch: 34, Steps: 137 | Train Loss: 0.2752233 Vali Loss: 0.2305189 Test Loss: 0.2614721
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2798493
	speed: 0.4952s/iter; left time: 4428.4570s
Epoch: 35 cost time: 26.545255184173584
Epoch: 35, Steps: 137 | Train Loss: 0.2751902 Vali Loss: 0.2302705 Test Loss: 0.2614388
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2741173
	speed: 0.4895s/iter; left time: 4310.8069s
Epoch: 36 cost time: 26.666160583496094
Epoch: 36, Steps: 137 | Train Loss: 0.2751156 Vali Loss: 0.2299561 Test Loss: 0.2613851
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2706984
	speed: 0.4979s/iter; left time: 4316.1048s
Epoch: 37 cost time: 26.958723545074463
Epoch: 37, Steps: 137 | Train Loss: 0.2751305 Vali Loss: 0.2299030 Test Loss: 0.2613472
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2752366
	speed: 0.4897s/iter; left time: 4178.1289s
Epoch: 38 cost time: 27.1766095161438
Epoch: 38, Steps: 137 | Train Loss: 0.2749869 Vali Loss: 0.2302936 Test Loss: 0.2613188
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2747755
	speed: 0.4808s/iter; left time: 4036.0095s
Epoch: 39 cost time: 26.15774369239807
Epoch: 39, Steps: 137 | Train Loss: 0.2750264 Vali Loss: 0.2298833 Test Loss: 0.2612728
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2832722
	speed: 0.4831s/iter; left time: 3989.4231s
Epoch: 40 cost time: 27.437414407730103
Epoch: 40, Steps: 137 | Train Loss: 0.2749450 Vali Loss: 0.2300195 Test Loss: 0.2612438
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2723058
	speed: 0.4847s/iter; left time: 3936.3430s
Epoch: 41 cost time: 26.573112964630127
Epoch: 41, Steps: 137 | Train Loss: 0.2749129 Vali Loss: 0.2296352 Test Loss: 0.2612256
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2726767
	speed: 0.4901s/iter; left time: 3912.9638s
Epoch: 42 cost time: 26.593553066253662
Epoch: 42, Steps: 137 | Train Loss: 0.2748756 Vali Loss: 0.2294395 Test Loss: 0.2612076
Validation loss decreased (0.229628 --> 0.229439).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2782572
	speed: 0.4787s/iter; left time: 3756.5747s
Epoch: 43 cost time: 27.50585675239563
Epoch: 43, Steps: 137 | Train Loss: 0.2749270 Vali Loss: 0.2298641 Test Loss: 0.2611789
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2637198
	speed: 0.4874s/iter; left time: 3757.4906s
Epoch: 44 cost time: 27.356210708618164
Epoch: 44, Steps: 137 | Train Loss: 0.2748034 Vali Loss: 0.2296427 Test Loss: 0.2611636
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2849743
	speed: 0.4729s/iter; left time: 3581.4275s
Epoch: 45 cost time: 26.722376823425293
Epoch: 45, Steps: 137 | Train Loss: 0.2748459 Vali Loss: 0.2296784 Test Loss: 0.2611330
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2606489
	speed: 0.4763s/iter; left time: 3541.8335s
Epoch: 46 cost time: 28.018990755081177
Epoch: 46, Steps: 137 | Train Loss: 0.2748084 Vali Loss: 0.2296842 Test Loss: 0.2611213
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2786052
	speed: 0.4826s/iter; left time: 3522.3180s
Epoch: 47 cost time: 26.937389612197876
Epoch: 47, Steps: 137 | Train Loss: 0.2747659 Vali Loss: 0.2294168 Test Loss: 0.2611069
Validation loss decreased (0.229439 --> 0.229417).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2751386
	speed: 0.4792s/iter; left time: 3432.0958s
Epoch: 48 cost time: 27.15979766845703
Epoch: 48, Steps: 137 | Train Loss: 0.2748036 Vali Loss: 0.2295422 Test Loss: 0.2610925
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2726937
	speed: 0.4803s/iter; left time: 3373.7826s
Epoch: 49 cost time: 26.546440601348877
Epoch: 49, Steps: 137 | Train Loss: 0.2747801 Vali Loss: 0.2293001 Test Loss: 0.2610863
Validation loss decreased (0.229417 --> 0.229300).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2773040
	speed: 0.4812s/iter; left time: 3314.6702s
Epoch: 50 cost time: 26.892983198165894
Epoch: 50, Steps: 137 | Train Loss: 0.2746675 Vali Loss: 0.2294248 Test Loss: 0.2610705
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2769447
	speed: 0.4788s/iter; left time: 3232.5858s
Epoch: 51 cost time: 26.263938188552856
Epoch: 51, Steps: 137 | Train Loss: 0.2747902 Vali Loss: 0.2299107 Test Loss: 0.2610543
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2681621
	speed: 0.4834s/iter; left time: 3197.1835s
Epoch: 52 cost time: 26.925841093063354
Epoch: 52, Steps: 137 | Train Loss: 0.2747080 Vali Loss: 0.2295945 Test Loss: 0.2610505
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2724560
	speed: 0.4729s/iter; left time: 3063.2465s
Epoch: 53 cost time: 26.269446849822998
Epoch: 53, Steps: 137 | Train Loss: 0.2746863 Vali Loss: 0.2299568 Test Loss: 0.2610496
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2890999
	speed: 0.4784s/iter; left time: 3033.1319s
Epoch: 54 cost time: 26.32997703552246
Epoch: 54, Steps: 137 | Train Loss: 0.2746868 Vali Loss: 0.2300658 Test Loss: 0.2610398
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2737980
	speed: 0.4930s/iter; left time: 3057.7774s
Epoch: 55 cost time: 27.22437071800232
Epoch: 55, Steps: 137 | Train Loss: 0.2747960 Vali Loss: 0.2292165 Test Loss: 0.2610254
Validation loss decreased (0.229300 --> 0.229216).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2765833
	speed: 0.4944s/iter; left time: 2998.8875s
Epoch: 56 cost time: 26.74628257751465
Epoch: 56, Steps: 137 | Train Loss: 0.2746610 Vali Loss: 0.2295392 Test Loss: 0.2610211
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2887988
	speed: 0.4863s/iter; left time: 2883.5165s
Epoch: 57 cost time: 26.615260124206543
Epoch: 57, Steps: 137 | Train Loss: 0.2747102 Vali Loss: 0.2298911 Test Loss: 0.2610186
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2677337
	speed: 0.4879s/iter; left time: 2825.7591s
Epoch: 58 cost time: 26.339894771575928
Epoch: 58, Steps: 137 | Train Loss: 0.2747196 Vali Loss: 0.2295743 Test Loss: 0.2610165
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2722706
	speed: 0.4804s/iter; left time: 2716.7117s
Epoch: 59 cost time: 26.114311695098877
Epoch: 59, Steps: 137 | Train Loss: 0.2746770 Vali Loss: 0.2295442 Test Loss: 0.2610015
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2654566
	speed: 0.4794s/iter; left time: 2645.3928s
Epoch: 60 cost time: 26.78742027282715
Epoch: 60, Steps: 137 | Train Loss: 0.2747021 Vali Loss: 0.2293690 Test Loss: 0.2610038
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2815430
	speed: 0.4910s/iter; left time: 2642.2284s
Epoch: 61 cost time: 27.33673405647278
Epoch: 61, Steps: 137 | Train Loss: 0.2746440 Vali Loss: 0.2295833 Test Loss: 0.2610005
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2798947
	speed: 0.4828s/iter; left time: 2531.9279s
Epoch: 62 cost time: 26.63091468811035
Epoch: 62, Steps: 137 | Train Loss: 0.2746610 Vali Loss: 0.2291892 Test Loss: 0.2609968
Validation loss decreased (0.229216 --> 0.229189).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2719743
	speed: 0.4765s/iter; left time: 2433.7206s
Epoch: 63 cost time: 26.849376916885376
Epoch: 63, Steps: 137 | Train Loss: 0.2746799 Vali Loss: 0.2295241 Test Loss: 0.2609910
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2592145
	speed: 0.5007s/iter; left time: 2488.3531s
Epoch: 64 cost time: 27.741668701171875
Epoch: 64, Steps: 137 | Train Loss: 0.2746704 Vali Loss: 0.2289929 Test Loss: 0.2609862
Validation loss decreased (0.229189 --> 0.228993).  Saving model ...
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2760490
	speed: 0.4872s/iter; left time: 2354.6161s
Epoch: 65 cost time: 26.695493459701538
Epoch: 65, Steps: 137 | Train Loss: 0.2746958 Vali Loss: 0.2294027 Test Loss: 0.2609871
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2824328
	speed: 0.4788s/iter; left time: 2248.6601s
Epoch: 66 cost time: 27.29655647277832
Epoch: 66, Steps: 137 | Train Loss: 0.2746745 Vali Loss: 0.2297842 Test Loss: 0.2609814
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2667646
	speed: 0.4993s/iter; left time: 2276.1239s
Epoch: 67 cost time: 27.627081155776978
Epoch: 67, Steps: 137 | Train Loss: 0.2746483 Vali Loss: 0.2291507 Test Loss: 0.2609800
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2858779
	speed: 0.4847s/iter; left time: 2143.1654s
Epoch: 68 cost time: 26.666677951812744
Epoch: 68, Steps: 137 | Train Loss: 0.2746439 Vali Loss: 0.2296729 Test Loss: 0.2609771
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2794500
	speed: 0.4722s/iter; left time: 2023.4621s
Epoch: 69 cost time: 26.84073758125305
Epoch: 69, Steps: 137 | Train Loss: 0.2745386 Vali Loss: 0.2294054 Test Loss: 0.2609748
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2834579
	speed: 0.4847s/iter; left time: 2010.5636s
Epoch: 70 cost time: 27.00974726676941
Epoch: 70, Steps: 137 | Train Loss: 0.2746447 Vali Loss: 0.2298163 Test Loss: 0.2609695
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2732594
	speed: 0.4780s/iter; left time: 1917.2940s
Epoch: 71 cost time: 26.983407497406006
Epoch: 71, Steps: 137 | Train Loss: 0.2746511 Vali Loss: 0.2293609 Test Loss: 0.2609703
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2701533
	speed: 0.4752s/iter; left time: 1840.9145s
Epoch: 72 cost time: 26.593905448913574
Epoch: 72, Steps: 137 | Train Loss: 0.2746419 Vali Loss: 0.2296180 Test Loss: 0.2609711
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2722064
	speed: 0.4695s/iter; left time: 1754.6747s
Epoch: 73 cost time: 26.59714150428772
Epoch: 73, Steps: 137 | Train Loss: 0.2746136 Vali Loss: 0.2295550 Test Loss: 0.2609649
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.2765365
	speed: 0.4736s/iter; left time: 1705.0129s
Epoch: 74 cost time: 26.338214874267578
Epoch: 74, Steps: 137 | Train Loss: 0.2746919 Vali Loss: 0.2290531 Test Loss: 0.2609685
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2858013
	speed: 0.4601s/iter; left time: 1593.3305s
Epoch: 75 cost time: 25.84639596939087
Epoch: 75, Steps: 137 | Train Loss: 0.2746203 Vali Loss: 0.2291837 Test Loss: 0.2609621
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2687920
	speed: 0.4716s/iter; left time: 1568.5129s
Epoch: 76 cost time: 26.422647953033447
Epoch: 76, Steps: 137 | Train Loss: 0.2745478 Vali Loss: 0.2294633 Test Loss: 0.2609635
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2802262
	speed: 0.4657s/iter; left time: 1485.1876s
Epoch: 77 cost time: 26.114482879638672
Epoch: 77, Steps: 137 | Train Loss: 0.2745720 Vali Loss: 0.2292258 Test Loss: 0.2609628
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.2796860
	speed: 0.4786s/iter; left time: 1460.8303s
Epoch: 78 cost time: 26.273366928100586
Epoch: 78, Steps: 137 | Train Loss: 0.2746011 Vali Loss: 0.2296777 Test Loss: 0.2609614
EarlyStopping counter: 14 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.2688865
	speed: 0.4740s/iter; left time: 1381.6315s
Epoch: 79 cost time: 25.286701917648315
Epoch: 79, Steps: 137 | Train Loss: 0.2745825 Vali Loss: 0.2291363 Test Loss: 0.2609608
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.2663310
	speed: 0.4678s/iter; left time: 1299.6668s
Epoch: 80 cost time: 25.863529205322266
Epoch: 80, Steps: 137 | Train Loss: 0.2745326 Vali Loss: 0.2296367 Test Loss: 0.2609590
EarlyStopping counter: 16 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.2576066
	speed: 0.4770s/iter; left time: 1259.6970s
Epoch: 81 cost time: 25.560721158981323
Epoch: 81, Steps: 137 | Train Loss: 0.2746682 Vali Loss: 0.2293817 Test Loss: 0.2609593
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.2735911
	speed: 0.4688s/iter; left time: 1173.8699s
Epoch: 82 cost time: 25.755645036697388
Epoch: 82, Steps: 137 | Train Loss: 0.2745027 Vali Loss: 0.2294622 Test Loss: 0.2609575
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.2766788
	speed: 0.4707s/iter; left time: 1114.1994s
Epoch: 83 cost time: 25.662981271743774
Epoch: 83, Steps: 137 | Train Loss: 0.2745541 Vali Loss: 0.2293016 Test Loss: 0.2609562
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.2795431
	speed: 0.4704s/iter; left time: 1048.9264s
Epoch: 84 cost time: 25.86794352531433
Epoch: 84, Steps: 137 | Train Loss: 0.2746159 Vali Loss: 0.2292934 Test Loss: 0.2609576
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_90_j720_H8_FITS_custom_ftM_sl90_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2618904709815979, mae:0.3321530818939209, rse:0.5104899406433105, corr:[0.44267857 0.44176462 0.44005966 0.43896544 0.4377355  0.43728945
 0.43736288 0.43685466 0.43611556 0.4351102  0.43441984 0.43384704
 0.4336118  0.43293804 0.43276286 0.4323238  0.4326438  0.43266863
 0.43287826 0.43276745 0.43250263 0.43259907 0.4321424  0.43114355
 0.42909297 0.4275388  0.42648384 0.42520195 0.4250455  0.42516696
 0.42617977 0.4267155  0.42629045 0.4259186  0.42553276 0.4250761
 0.4251433  0.42504737 0.42477518 0.42459923 0.42438576 0.4246713
 0.42493278 0.42475882 0.4250362  0.42510998 0.42495063 0.42474684
 0.4232448  0.42316785 0.42281803 0.4229771  0.42386755 0.42517585
 0.4275881  0.429125   0.42912883 0.42917162 0.42898247 0.42872095
 0.42855415 0.42834675 0.42854097 0.42863333 0.4287176  0.42907864
 0.42941803 0.42944342 0.4298918  0.43003792 0.43070474 0.43065238
 0.43062508 0.43097863 0.43119538 0.43265173 0.43447375 0.43726408
 0.44068182 0.44322395 0.44328868 0.44321272 0.44263068 0.4422334
 0.4419987  0.4417063  0.44184074 0.44184998 0.4420938  0.44256994
 0.44276926 0.44298652 0.44302046 0.443148   0.44336376 0.443176
 0.44307    0.44271857 0.44249755 0.44252798 0.44240528 0.44282395
 0.44336268 0.44358668 0.44326177 0.44284213 0.4424664  0.44202918
 0.4419115  0.44179308 0.44180995 0.44179338 0.44208938 0.44226068
 0.44246325 0.44248343 0.44243953 0.44273037 0.44270828 0.4427681
 0.44260296 0.44233367 0.44233444 0.4421143  0.44223732 0.442357
 0.4426953  0.44280708 0.44258663 0.44214788 0.44181183 0.44156027
 0.44142276 0.44138888 0.4415019  0.4415901  0.44189414 0.4421681
 0.4425416  0.44262385 0.44277635 0.44302437 0.44299278 0.4429066
 0.44217417 0.44197702 0.44191444 0.44153014 0.441934   0.4419721
 0.4423988  0.44246975 0.44215006 0.44181252 0.4413564  0.44098338
 0.44085386 0.44079322 0.44101027 0.44126472 0.44159347 0.44208643
 0.44240245 0.4422778  0.4422161  0.44160944 0.44104415 0.4387404
 0.43519673 0.43279418 0.43066746 0.4290133  0.4284002  0.428054
 0.42835182 0.4285328  0.4278518  0.42746902 0.4269679  0.42686394
 0.42670116 0.42647108 0.42644885 0.42619282 0.42615348 0.4265246
 0.42642164 0.42615443 0.42593023 0.4252313  0.42478043 0.42275462
 0.4205862  0.41893908 0.41741142 0.41676342 0.4164025  0.41715896
 0.41847384 0.41939563 0.41932803 0.4191911  0.41882068 0.41851565
 0.4184637  0.4183764  0.41842297 0.41824353 0.41820797 0.41846988
 0.41844523 0.4183909  0.41838244 0.41819564 0.41834322 0.4173917
 0.4162576  0.41532546 0.4149594  0.41540357 0.41629714 0.41820303
 0.4206189  0.42244807 0.42275977 0.42293987 0.42280173 0.42253697
 0.42240328 0.42230427 0.42226416 0.42234683 0.42260608 0.42294788
 0.4233049  0.42341155 0.42356777 0.42381847 0.4242775  0.42399123
 0.42391416 0.42421436 0.4247693  0.42562988 0.42775065 0.4304828
 0.43424106 0.43682873 0.43718475 0.43707335 0.43675816 0.43655157
 0.43633845 0.43614054 0.43630135 0.43637386 0.43670002 0.43700197
 0.4373972  0.43743506 0.43747437 0.4376762  0.43760714 0.43752924
 0.43707046 0.436868   0.4367382  0.4364908  0.43679586 0.43702978
 0.43768573 0.43804199 0.43772486 0.43739712 0.43698624 0.43669212
 0.4364759  0.43610793 0.43628576 0.43644676 0.43669525 0.436894
 0.4369751  0.4370387  0.43712717 0.43719897 0.43730095 0.43723482
 0.4368769  0.43673992 0.4363869  0.43618047 0.43631735 0.43656722
 0.4370795  0.43710357 0.4369145  0.43664312 0.43623936 0.4359781
 0.43591964 0.43570974 0.435856   0.43606547 0.43629497 0.43668094
 0.43692973 0.437135   0.43733475 0.43741432 0.43753436 0.4371664
 0.43673813 0.4365014  0.43606275 0.4360742  0.4361301  0.4365154
 0.4370248  0.43691587 0.43671596 0.43634966 0.43588176 0.4354462
 0.43528533 0.43523258 0.4353141  0.43570453 0.43599135 0.43641362
 0.4367454  0.43661335 0.43631136 0.43572652 0.43493542 0.43206337
 0.42844403 0.42574698 0.42370394 0.42220655 0.4214397  0.42145625
 0.4218289  0.42201445 0.42168108 0.42126742 0.42075095 0.42047775
 0.4202229  0.42004824 0.41999012 0.41979674 0.41992453 0.41988692
 0.41989136 0.41981304 0.41940463 0.41902277 0.41839752 0.41626254
 0.41347325 0.411515   0.41037506 0.409211   0.4091624  0.40990233
 0.41125584 0.4122992  0.4122673  0.41212463 0.41198492 0.4119165
 0.41166365 0.41139662 0.41146782 0.41132274 0.41143516 0.41150597
 0.41156796 0.4114996  0.41131496 0.41120058 0.41107246 0.41024256
 0.4089541  0.40816247 0.40805727 0.40820614 0.40952983 0.41157863
 0.4142636  0.41614878 0.41656905 0.41675884 0.4167029  0.41663122
 0.41654223 0.41635916 0.41645893 0.41644838 0.4167126  0.41716373
 0.41745296 0.417524   0.4177958  0.41814372 0.41846296 0.41814867
 0.41789356 0.4182714  0.4187693  0.42021358 0.42212337 0.42527702
 0.42904335 0.43190363 0.4325992  0.43243673 0.4322291  0.4318837
 0.43163398 0.43147692 0.4317449  0.43191755 0.43210074 0.43250984
 0.4327287  0.4328084  0.43303463 0.43315905 0.43336976 0.43311983
 0.4326186  0.43248373 0.43211904 0.43212402 0.43217495 0.4327935
 0.43334773 0.43351895 0.4333777  0.4330394  0.4327702  0.43252322
 0.43226358 0.43211025 0.43211392 0.43230948 0.4324986  0.43272454
 0.4329018  0.43295404 0.43295035 0.43306422 0.4332209  0.43321148
 0.43287638 0.43267012 0.43244925 0.43227994 0.43244803 0.43271267
 0.43312427 0.43325382 0.43281353 0.4325143  0.43223095 0.4319843
 0.43205655 0.4318866  0.43197802 0.43209702 0.4324112  0.43269306
 0.43297714 0.43335548 0.43348762 0.4337136  0.4338872  0.4335922
 0.43304354 0.43282345 0.43277857 0.4324945  0.43280444 0.4328617
 0.43320936 0.43351215 0.4331611  0.43288824 0.43249628 0.43209547
 0.43219158 0.43193522 0.4321208  0.43230784 0.4326904  0.4330695
 0.43332472 0.43329138 0.4329186  0.43245077 0.43182626 0.42902702
 0.42553133 0.4229607  0.4209849  0.41954768 0.4188432  0.41841805
 0.4185844  0.41875365 0.41823378 0.4179544  0.41746235 0.4171139
 0.41710725 0.41682714 0.4167356  0.41666922 0.41669875 0.41680545
 0.4169647  0.41656113 0.41632667 0.41590017 0.41536984 0.41324365
 0.410445   0.4085429  0.40717283 0.40665996 0.40628588 0.4068166
 0.40821886 0.40884176 0.4087104  0.4085978  0.4083282  0.40819445
 0.40816456 0.40808713 0.4078033  0.40776217 0.4077621  0.40783834
 0.40802807 0.40782344 0.40792418 0.40791786 0.40779278 0.40710396
 0.40583465 0.40512413 0.40486765 0.40522325 0.40598413 0.4079406
 0.41047153 0.41229662 0.41257504 0.41281095 0.41269192 0.4124694
 0.41243023 0.41233748 0.41227233 0.41224262 0.41226605 0.41247007
 0.4127027  0.41270584 0.41312993 0.41330215 0.4138509  0.413648
 0.4135836  0.4137772  0.41462544 0.4159309  0.4177715  0.42077544
 0.4238925  0.42684126 0.42712247 0.4272535  0.4269783  0.42656893
 0.42649427 0.42627406 0.426399   0.426682   0.42689052 0.42721736
 0.42735684 0.42746606 0.42755446 0.42771733 0.42797613 0.42774257
 0.42737556 0.4271688  0.4271796  0.42707142 0.42721176 0.42761528
 0.42796794 0.42816803 0.42783782 0.4274958  0.42729408 0.42691615
 0.42678902 0.42660403 0.42663723 0.42668247 0.4268389  0.42700893
 0.4271712  0.42718995 0.42716897 0.42736143 0.4273696  0.42741856
 0.4271537  0.42705008 0.42703605 0.42674917 0.42700067 0.42707327
 0.42738783 0.42727757 0.42710304 0.42680115 0.4265298  0.4262275
 0.4262603  0.4262113  0.42637098 0.4264858  0.4267052  0.4268536
 0.4271934  0.42724097 0.42744845 0.42765227 0.42772934 0.42771125
 0.42710897 0.42698008 0.42692733 0.4268514  0.4271772  0.4271817
 0.4276497  0.42748886 0.42723852 0.42699465 0.4264426  0.42622387
 0.4260059  0.4260534  0.4260009  0.42622882 0.4264247  0.4266088
 0.4268585  0.4265596  0.42648044 0.4257384  0.42521688 0.42257774
 0.41904905 0.416606   0.41450968 0.41355437 0.41258976 0.4124559
 0.41246313 0.41240802 0.41187817 0.41141665 0.4109754  0.4107453
 0.4102683  0.41022396 0.40976968 0.4096803  0.4092654  0.4097403
 0.40946445 0.409394   0.40908104 0.40860298 0.4086041  0.40211695
 0.40009102 0.3982385  0.3972322  0.3966235  0.39621174 0.3971371
 0.3976546  0.3984683  0.39811057 0.39768782 0.39761156 0.3967275
 0.39714113 0.39638865 0.39651215 0.39580458 0.39590034 0.3959877
 0.39562804 0.39665452 0.3952909  0.39737257 0.3926065  0.3964538 ]
