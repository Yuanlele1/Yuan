Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j192_H5_FITS_custom_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3804936960.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 93.65030241012573
Epoch: 1, Steps: 88 | Train Loss: 0.7861964 Vali Loss: 0.6314918 Test Loss: 0.7253131
Validation loss decreased (inf --> 0.631492).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 96.65090942382812
Epoch: 2, Steps: 88 | Train Loss: 0.3910853 Vali Loss: 0.4233439 Test Loss: 0.4958225
Validation loss decreased (0.631492 --> 0.423344).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 101.1877703666687
Epoch: 3, Steps: 88 | Train Loss: 0.2856180 Vali Loss: 0.3626218 Test Loss: 0.4316581
Validation loss decreased (0.423344 --> 0.362622).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 101.27049016952515
Epoch: 4, Steps: 88 | Train Loss: 0.2561083 Vali Loss: 0.3461613 Test Loss: 0.4158424
Validation loss decreased (0.362622 --> 0.346161).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 101.44134855270386
Epoch: 5, Steps: 88 | Train Loss: 0.2485532 Vali Loss: 0.3411809 Test Loss: 0.4122653
Validation loss decreased (0.346161 --> 0.341181).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 98.48099946975708
Epoch: 6, Steps: 88 | Train Loss: 0.2462664 Vali Loss: 0.3393868 Test Loss: 0.4111826
Validation loss decreased (0.341181 --> 0.339387).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 103.76864814758301
Epoch: 7, Steps: 88 | Train Loss: 0.2455486 Vali Loss: 0.3385250 Test Loss: 0.4104581
Validation loss decreased (0.339387 --> 0.338525).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 103.29787755012512
Epoch: 8, Steps: 88 | Train Loss: 0.2451036 Vali Loss: 0.3379564 Test Loss: 0.4101149
Validation loss decreased (0.338525 --> 0.337956).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 113.06735920906067
Epoch: 9, Steps: 88 | Train Loss: 0.2448091 Vali Loss: 0.3374020 Test Loss: 0.4100839
Validation loss decreased (0.337956 --> 0.337402).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 123.62029719352722
Epoch: 10, Steps: 88 | Train Loss: 0.2446928 Vali Loss: 0.3368699 Test Loss: 0.4095590
Validation loss decreased (0.337402 --> 0.336870).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 126.43582344055176
Epoch: 11, Steps: 88 | Train Loss: 0.2444905 Vali Loss: 0.3369382 Test Loss: 0.4094734
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 122.96939516067505
Epoch: 12, Steps: 88 | Train Loss: 0.2444378 Vali Loss: 0.3368626 Test Loss: 0.4094467
Validation loss decreased (0.336870 --> 0.336863).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 124.61485457420349
Epoch: 13, Steps: 88 | Train Loss: 0.2443474 Vali Loss: 0.3363848 Test Loss: 0.4091532
Validation loss decreased (0.336863 --> 0.336385).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 124.07667565345764
Epoch: 14, Steps: 88 | Train Loss: 0.2443004 Vali Loss: 0.3360286 Test Loss: 0.4093813
Validation loss decreased (0.336385 --> 0.336029).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 125.23551893234253
Epoch: 15, Steps: 88 | Train Loss: 0.2441132 Vali Loss: 0.3364666 Test Loss: 0.4092000
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 121.76070547103882
Epoch: 16, Steps: 88 | Train Loss: 0.2441232 Vali Loss: 0.3365616 Test Loss: 0.4091504
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 133.24214887619019
Epoch: 17, Steps: 88 | Train Loss: 0.2440681 Vali Loss: 0.3362763 Test Loss: 0.4090740
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 128.10209226608276
Epoch: 18, Steps: 88 | Train Loss: 0.2440286 Vali Loss: 0.3359318 Test Loss: 0.4089155
Validation loss decreased (0.336029 --> 0.335932).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 125.84265279769897
Epoch: 19, Steps: 88 | Train Loss: 0.2439317 Vali Loss: 0.3364612 Test Loss: 0.4091016
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 126.9660267829895
Epoch: 20, Steps: 88 | Train Loss: 0.2440345 Vali Loss: 0.3357669 Test Loss: 0.4089378
Validation loss decreased (0.335932 --> 0.335767).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 125.67961239814758
Epoch: 21, Steps: 88 | Train Loss: 0.2439192 Vali Loss: 0.3356032 Test Loss: 0.4088266
Validation loss decreased (0.335767 --> 0.335603).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 122.63836789131165
Epoch: 22, Steps: 88 | Train Loss: 0.2439153 Vali Loss: 0.3359778 Test Loss: 0.4090123
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 125.10169792175293
Epoch: 23, Steps: 88 | Train Loss: 0.2438650 Vali Loss: 0.3356310 Test Loss: 0.4091052
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 132.09748649597168
Epoch: 24, Steps: 88 | Train Loss: 0.2438449 Vali Loss: 0.3362240 Test Loss: 0.4088830
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 123.17542862892151
Epoch: 25, Steps: 88 | Train Loss: 0.2439148 Vali Loss: 0.3361017 Test Loss: 0.4088128
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 127.50697636604309
Epoch: 26, Steps: 88 | Train Loss: 0.2437659 Vali Loss: 0.3357836 Test Loss: 0.4089948
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 124.28355264663696
Epoch: 27, Steps: 88 | Train Loss: 0.2438213 Vali Loss: 0.3357542 Test Loss: 0.4088896
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 115.14698481559753
Epoch: 28, Steps: 88 | Train Loss: 0.2438407 Vali Loss: 0.3358775 Test Loss: 0.4088473
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 99.22922611236572
Epoch: 29, Steps: 88 | Train Loss: 0.2438292 Vali Loss: 0.3361523 Test Loss: 0.4088449
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 105.07587146759033
Epoch: 30, Steps: 88 | Train Loss: 0.2438144 Vali Loss: 0.3360457 Test Loss: 0.4088716
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 99.09971928596497
Epoch: 31, Steps: 88 | Train Loss: 0.2437718 Vali Loss: 0.3357463 Test Loss: 0.4088691
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j192_H5_FITS_custom_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4079081118106842, mae:0.2875972390174866, rse:0.5271208882331848, corr:[0.2883431  0.29097182 0.29095063 0.29351214 0.29498506 0.2945085
 0.29380435 0.29403087 0.29461974 0.29446048 0.29345843 0.29248962
 0.29223007 0.29240608 0.29245323 0.29226968 0.29227206 0.29272664
 0.29334873 0.293649   0.2934464  0.29302332 0.29291296 0.29372564
 0.2949633  0.29485375 0.29428038 0.29404804 0.2940461  0.29390803
 0.29352468 0.293156   0.2931224  0.2933462  0.29340214 0.29313666
 0.29288363 0.29291126 0.2931775  0.29338455 0.29335743 0.29319277
 0.29310527 0.29316723 0.29321852 0.29305443 0.2927725  0.29287502
 0.2933896  0.29367322 0.2937089  0.29346222 0.29306823 0.29280597
 0.29275742 0.29273984 0.29259443 0.29235932 0.29215255 0.29205868
 0.29207972 0.29205105 0.2919579  0.29190466 0.29198182 0.29214716
 0.29224902 0.29223004 0.2921403  0.2920479  0.2919317  0.29184356
 0.2916995  0.29163232 0.29174742 0.2918825  0.29191616 0.29181516
 0.29162848 0.2915005  0.29157245 0.2917931  0.29193223 0.2918445
 0.29162568 0.2914414  0.2914017  0.2914297  0.29138523 0.29121423
 0.29100004 0.29087102 0.29091617 0.2910758  0.29117322 0.29115564
 0.29093972 0.2908259  0.29091895 0.29102585 0.29108    0.2911436
 0.29122573 0.29128712 0.2913169  0.29131937 0.29129478 0.29121235
 0.29104185 0.29080456 0.29063636 0.29063818 0.29076424 0.29088157
 0.2908214  0.2906031  0.29042605 0.29039034 0.29057932 0.2907816
 0.2908218  0.290906   0.29110596 0.29129705 0.29147753 0.29153457
 0.29139438 0.29113236 0.29091424 0.29084185 0.29090828 0.29101384
 0.29107204 0.2910986  0.2911503  0.2912214  0.2912492  0.2911773
 0.2911088  0.29114425 0.29130164 0.29148504 0.29149297 0.2913487
 0.29121935 0.29111308 0.2912281  0.2915454  0.29185098 0.2919182
 0.29171425 0.2914072  0.29113957 0.29096255 0.29087943 0.29091033
 0.29100358 0.29105297 0.29106948 0.2912049  0.29157    0.2919373
 0.2920335  0.29192036 0.2919229  0.29216218 0.29242003 0.29277846
 0.29315522 0.29268372 0.29232985 0.2923828  0.29234788 0.29200414
 0.2915995  0.29153427 0.29186141 0.29219076 0.2920938  0.29171416
 0.2917005  0.29229814 0.29305574 0.29329342 0.29305205 0.29309976
 0.29362565 0.29369816 0.29215184 0.2896529  0.28940567 0.2932137 ]
