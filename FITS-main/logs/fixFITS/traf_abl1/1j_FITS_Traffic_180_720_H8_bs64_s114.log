Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j720_H8_FITS_custom_ftM_sl180_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11381
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=74, out_features=370, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3020999680.0
params:  27750.0
Trainable parameters:  27750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 164.73881077766418
Epoch: 1, Steps: 88 | Train Loss: 1.5540890 Vali Loss: 1.2321784 Test Loss: 1.5242989
Validation loss decreased (inf --> 1.232178).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 136.31374669075012
Epoch: 2, Steps: 88 | Train Loss: 0.7978527 Vali Loss: 0.8055857 Test Loss: 0.9869740
Validation loss decreased (1.232178 --> 0.805586).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 124.34478569030762
Epoch: 3, Steps: 88 | Train Loss: 0.5609735 Vali Loss: 0.6412042 Test Loss: 0.7816664
Validation loss decreased (0.805586 --> 0.641204).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 135.88471221923828
Epoch: 4, Steps: 88 | Train Loss: 0.4627644 Vali Loss: 0.5664496 Test Loss: 0.6878452
Validation loss decreased (0.641204 --> 0.566450).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 152.64967465400696
Epoch: 5, Steps: 88 | Train Loss: 0.4146229 Vali Loss: 0.5276434 Test Loss: 0.6370177
Validation loss decreased (0.566450 --> 0.527643).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 127.13747668266296
Epoch: 6, Steps: 88 | Train Loss: 0.3869769 Vali Loss: 0.5027709 Test Loss: 0.6050146
Validation loss decreased (0.527643 --> 0.502771).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 111.0539481639862
Epoch: 7, Steps: 88 | Train Loss: 0.3688715 Vali Loss: 0.4866109 Test Loss: 0.5828159
Validation loss decreased (0.502771 --> 0.486611).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 112.3532440662384
Epoch: 8, Steps: 88 | Train Loss: 0.3562534 Vali Loss: 0.4752823 Test Loss: 0.5665600
Validation loss decreased (0.486611 --> 0.475282).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 101.02811288833618
Epoch: 9, Steps: 88 | Train Loss: 0.3469644 Vali Loss: 0.4655044 Test Loss: 0.5546160
Validation loss decreased (0.475282 --> 0.465504).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 145.09046721458435
Epoch: 10, Steps: 88 | Train Loss: 0.3398331 Vali Loss: 0.4586715 Test Loss: 0.5454767
Validation loss decreased (0.465504 --> 0.458671).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 181.30600547790527
Epoch: 11, Steps: 88 | Train Loss: 0.3344540 Vali Loss: 0.4538098 Test Loss: 0.5383490
Validation loss decreased (0.458671 --> 0.453810).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 175.1158208847046
Epoch: 12, Steps: 88 | Train Loss: 0.3303024 Vali Loss: 0.4494744 Test Loss: 0.5327743
Validation loss decreased (0.453810 --> 0.449474).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 160.56978154182434
Epoch: 13, Steps: 88 | Train Loss: 0.3269468 Vali Loss: 0.4466357 Test Loss: 0.5282841
Validation loss decreased (0.449474 --> 0.446636).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 163.10375428199768
Epoch: 14, Steps: 88 | Train Loss: 0.3243192 Vali Loss: 0.4436927 Test Loss: 0.5248813
Validation loss decreased (0.446636 --> 0.443693).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 159.3985607624054
Epoch: 15, Steps: 88 | Train Loss: 0.3222122 Vali Loss: 0.4416076 Test Loss: 0.5219544
Validation loss decreased (0.443693 --> 0.441608).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 153.68702745437622
Epoch: 16, Steps: 88 | Train Loss: 0.3204670 Vali Loss: 0.4402661 Test Loss: 0.5196628
Validation loss decreased (0.441608 --> 0.440266).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 164.8373143672943
Epoch: 17, Steps: 88 | Train Loss: 0.3190226 Vali Loss: 0.4389630 Test Loss: 0.5177484
Validation loss decreased (0.440266 --> 0.438963).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 170.81315803527832
Epoch: 18, Steps: 88 | Train Loss: 0.3178492 Vali Loss: 0.4379253 Test Loss: 0.5163489
Validation loss decreased (0.438963 --> 0.437925).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 153.98568487167358
Epoch: 19, Steps: 88 | Train Loss: 0.3169509 Vali Loss: 0.4371636 Test Loss: 0.5149418
Validation loss decreased (0.437925 --> 0.437164).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 132.29700994491577
Epoch: 20, Steps: 88 | Train Loss: 0.3161580 Vali Loss: 0.4358985 Test Loss: 0.5138855
Validation loss decreased (0.437164 --> 0.435898).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 218.6565821170807
Epoch: 21, Steps: 88 | Train Loss: 0.3154673 Vali Loss: 0.4356719 Test Loss: 0.5129790
Validation loss decreased (0.435898 --> 0.435672).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 211.29970574378967
Epoch: 22, Steps: 88 | Train Loss: 0.3147917 Vali Loss: 0.4353130 Test Loss: 0.5121716
Validation loss decreased (0.435672 --> 0.435313).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 221.8152358531952
Epoch: 23, Steps: 88 | Train Loss: 0.3143772 Vali Loss: 0.4344239 Test Loss: 0.5114715
Validation loss decreased (0.435313 --> 0.434424).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 231.48234367370605
Epoch: 24, Steps: 88 | Train Loss: 0.3139511 Vali Loss: 0.4341274 Test Loss: 0.5109062
Validation loss decreased (0.434424 --> 0.434127).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 179.9151110649109
Epoch: 25, Steps: 88 | Train Loss: 0.3135286 Vali Loss: 0.4333411 Test Loss: 0.5103696
Validation loss decreased (0.434127 --> 0.433341).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 195.05864071846008
Epoch: 26, Steps: 88 | Train Loss: 0.3132208 Vali Loss: 0.4337790 Test Loss: 0.5099455
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 217.61860346794128
Epoch: 27, Steps: 88 | Train Loss: 0.3129607 Vali Loss: 0.4329897 Test Loss: 0.5095435
Validation loss decreased (0.433341 --> 0.432990).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 214.94977474212646
Epoch: 28, Steps: 88 | Train Loss: 0.3126982 Vali Loss: 0.4333698 Test Loss: 0.5092666
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 208.1402735710144
Epoch: 29, Steps: 88 | Train Loss: 0.3123796 Vali Loss: 0.4332137 Test Loss: 0.5090128
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 196.87260365486145
Epoch: 30, Steps: 88 | Train Loss: 0.3122640 Vali Loss: 0.4324932 Test Loss: 0.5087578
Validation loss decreased (0.432990 --> 0.432493).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 180.3460876941681
Epoch: 31, Steps: 88 | Train Loss: 0.3121916 Vali Loss: 0.4329150 Test Loss: 0.5085020
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 156.2170193195343
Epoch: 32, Steps: 88 | Train Loss: 0.3119384 Vali Loss: 0.4321581 Test Loss: 0.5082794
Validation loss decreased (0.432493 --> 0.432158).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 136.50010108947754
Epoch: 33, Steps: 88 | Train Loss: 0.3118400 Vali Loss: 0.4319614 Test Loss: 0.5081437
Validation loss decreased (0.432158 --> 0.431961).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 134.89830136299133
Epoch: 34, Steps: 88 | Train Loss: 0.3117817 Vali Loss: 0.4320701 Test Loss: 0.5079491
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 143.0314121246338
Epoch: 35, Steps: 88 | Train Loss: 0.3116774 Vali Loss: 0.4317385 Test Loss: 0.5078058
Validation loss decreased (0.431961 --> 0.431738).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 142.36919522285461
Epoch: 36, Steps: 88 | Train Loss: 0.3115608 Vali Loss: 0.4320692 Test Loss: 0.5077585
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 138.73224353790283
Epoch: 37, Steps: 88 | Train Loss: 0.3114774 Vali Loss: 0.4317100 Test Loss: 0.5075937
Validation loss decreased (0.431738 --> 0.431710).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 137.4652760028839
Epoch: 38, Steps: 88 | Train Loss: 0.3113264 Vali Loss: 0.4318519 Test Loss: 0.5074707
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 131.53975009918213
Epoch: 39, Steps: 88 | Train Loss: 0.3112759 Vali Loss: 0.4315130 Test Loss: 0.5073776
Validation loss decreased (0.431710 --> 0.431513).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 126.91346740722656
Epoch: 40, Steps: 88 | Train Loss: 0.3112505 Vali Loss: 0.4318701 Test Loss: 0.5073393
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 122.48351764678955
Epoch: 41, Steps: 88 | Train Loss: 0.3111585 Vali Loss: 0.4318989 Test Loss: 0.5072759
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 119.16555333137512
Epoch: 42, Steps: 88 | Train Loss: 0.3111227 Vali Loss: 0.4319781 Test Loss: 0.5071940
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 126.5875985622406
Epoch: 43, Steps: 88 | Train Loss: 0.3110542 Vali Loss: 0.4317166 Test Loss: 0.5071442
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 113.92710566520691
Epoch: 44, Steps: 88 | Train Loss: 0.3110144 Vali Loss: 0.4316640 Test Loss: 0.5070670
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 118.26744532585144
Epoch: 45, Steps: 88 | Train Loss: 0.3109441 Vali Loss: 0.4316425 Test Loss: 0.5070483
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 121.11835718154907
Epoch: 46, Steps: 88 | Train Loss: 0.3110124 Vali Loss: 0.4313754 Test Loss: 0.5069997
Validation loss decreased (0.431513 --> 0.431375).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 121.53435277938843
Epoch: 47, Steps: 88 | Train Loss: 0.3109574 Vali Loss: 0.4315907 Test Loss: 0.5069754
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 117.69834113121033
Epoch: 48, Steps: 88 | Train Loss: 0.3108760 Vali Loss: 0.4313006 Test Loss: 0.5069584
Validation loss decreased (0.431375 --> 0.431301).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 115.6790726184845
Epoch: 49, Steps: 88 | Train Loss: 0.3108496 Vali Loss: 0.4313452 Test Loss: 0.5069315
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 113.93466567993164
Epoch: 50, Steps: 88 | Train Loss: 0.3108451 Vali Loss: 0.4311245 Test Loss: 0.5068920
Validation loss decreased (0.431301 --> 0.431125).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 119.74792790412903
Epoch: 51, Steps: 88 | Train Loss: 0.3108273 Vali Loss: 0.4314765 Test Loss: 0.5068581
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 115.90585994720459
Epoch: 52, Steps: 88 | Train Loss: 0.3108943 Vali Loss: 0.4315104 Test Loss: 0.5068370
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 105.95471668243408
Epoch: 53, Steps: 88 | Train Loss: 0.3108238 Vali Loss: 0.4317212 Test Loss: 0.5067904
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 110.99855875968933
Epoch: 54, Steps: 88 | Train Loss: 0.3107310 Vali Loss: 0.4313101 Test Loss: 0.5067494
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 117.85312914848328
Epoch: 55, Steps: 88 | Train Loss: 0.3107010 Vali Loss: 0.4316904 Test Loss: 0.5067385
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 114.52957367897034
Epoch: 56, Steps: 88 | Train Loss: 0.3107428 Vali Loss: 0.4310763 Test Loss: 0.5067312
Validation loss decreased (0.431125 --> 0.431076).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 115.34694910049438
Epoch: 57, Steps: 88 | Train Loss: 0.3107692 Vali Loss: 0.4313760 Test Loss: 0.5066916
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 114.00881433486938
Epoch: 58, Steps: 88 | Train Loss: 0.3107191 Vali Loss: 0.4317308 Test Loss: 0.5066750
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 141.14095091819763
Epoch: 59, Steps: 88 | Train Loss: 0.3106485 Vali Loss: 0.4315143 Test Loss: 0.5066894
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 141.2853934764862
Epoch: 60, Steps: 88 | Train Loss: 0.3106605 Vali Loss: 0.4316159 Test Loss: 0.5066824
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 144.26570773124695
Epoch: 61, Steps: 88 | Train Loss: 0.3107457 Vali Loss: 0.4312977 Test Loss: 0.5066666
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 145.9158194065094
Epoch: 62, Steps: 88 | Train Loss: 0.3105672 Vali Loss: 0.4314265 Test Loss: 0.5066381
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 155.33724641799927
Epoch: 63, Steps: 88 | Train Loss: 0.3106689 Vali Loss: 0.4313558 Test Loss: 0.5066257
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 139.49720358848572
Epoch: 64, Steps: 88 | Train Loss: 0.3106653 Vali Loss: 0.4311666 Test Loss: 0.5066280
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 137.8425304889679
Epoch: 65, Steps: 88 | Train Loss: 0.3105689 Vali Loss: 0.4313561 Test Loss: 0.5065860
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 147.47914600372314
Epoch: 66, Steps: 88 | Train Loss: 0.3105438 Vali Loss: 0.4319377 Test Loss: 0.5066018
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j720_H8_FITS_custom_ftM_sl180_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.5046451687812805, mae:0.3264346420764923, rse:0.5808899402618408, corr:[0.2568327  0.26297978 0.26435417 0.261929   0.2657366  0.2642836
 0.26630148 0.26736388 0.2660976  0.2671365  0.26599488 0.26556355
 0.26567006 0.26443985 0.26464504 0.26431298 0.26393998 0.26443732
 0.26441717 0.26458597 0.2650251  0.26523194 0.2663009  0.26613218
 0.26636016 0.2661939  0.26522192 0.2656994  0.26561126 0.26515022
 0.26588348 0.26572546 0.2652957  0.26549932 0.26484776 0.2647583
 0.26437956 0.26381326 0.26423952 0.2643081  0.26451594 0.26486215
 0.2646272  0.26463175 0.26473585 0.2648012  0.26533887 0.26505667
 0.26527888 0.26536942 0.26474518 0.26500964 0.26487225 0.2642994
 0.2645997  0.2645175  0.2641788  0.26449376 0.26414862 0.2639508
 0.26381284 0.26346633 0.2638403  0.2638203  0.2637901  0.26396653
 0.26359013 0.26364395 0.2639451  0.26385978 0.26422808 0.26393136
 0.26355696 0.26380265 0.26335314 0.26327077 0.26335844 0.26302916
 0.26321715 0.26333445 0.2630069  0.2632744  0.2633254  0.26317188
 0.26317003 0.26297268 0.26317406 0.26326585 0.263185   0.26324287
 0.26317602 0.26304293 0.26324266 0.2631791  0.26337528 0.2633948
 0.2631054  0.2634864  0.26334837 0.26303065 0.26302612 0.26262674
 0.26243502 0.26258382 0.26247296 0.26256007 0.26264164 0.26269233
 0.26287624 0.26271412 0.26261374 0.2628443  0.26279753 0.2626436
 0.2626291  0.262692   0.2627709  0.26248664 0.26267248 0.263091
 0.26310652 0.26332045 0.26336372 0.2630026  0.26294705 0.26270133
 0.2625499  0.26269916 0.26253325 0.26261187 0.26294592 0.2631496
 0.2637017  0.26398602 0.26349095 0.26334256 0.26318428 0.26272076
 0.26282012 0.26302555 0.26344806 0.26375723 0.26426646 0.26457384
 0.264443   0.2643062  0.26428893 0.26390702 0.26376382 0.2637343
 0.26346418 0.26343495 0.26337948 0.26356727 0.2644118  0.26512682
 0.26621097 0.26711306 0.2664673  0.26591632 0.26544073 0.2651615
 0.2652152  0.26567107 0.26642996 0.26707694 0.26841483 0.2687622
 0.2678568  0.26689258 0.2664347  0.2660448  0.2657938  0.26581052
 0.26595667 0.2659417  0.26605135 0.26610178 0.26638228 0.26662904
 0.26672465 0.26700482 0.26668105 0.26640067 0.26609868 0.26594675
 0.26587868 0.26595336 0.26625845 0.26655447 0.2671404  0.26708418
 0.26666614 0.26617017 0.26590952 0.2657427  0.26561543 0.26560667
 0.26575872 0.26596007 0.26627102 0.2664066  0.26650825 0.2665827
 0.2663747  0.26622874 0.26599595 0.26598242 0.26596203 0.26582164
 0.26577136 0.2656991  0.26558095 0.26575428 0.26600882 0.26591673
 0.26590595 0.26558742 0.26542786 0.26538643 0.26499742 0.2648769
 0.26512703 0.26527485 0.26545927 0.26563683 0.26564348 0.26559183
 0.26545617 0.26551828 0.26549563 0.2652698  0.26519877 0.26505965
 0.26488253 0.26494277 0.26500726 0.2651718  0.26546305 0.26528805
 0.2653618  0.26533833 0.26488486 0.2647915  0.26472512 0.26466084
 0.26487264 0.26485682 0.26493773 0.2652029  0.26511136 0.26511383
 0.26509798 0.26506293 0.2650962  0.2649508  0.26487628 0.2648388
 0.26462117 0.26466733 0.26481706 0.26473334 0.26495463 0.26499176
 0.26502687 0.2652324  0.2650096  0.26480216 0.26446533 0.26401088
 0.2640408  0.26426965 0.26431197 0.26452473 0.2644867  0.26454768
 0.2647465  0.26462397 0.26446408 0.2643896  0.26413676 0.26403052
 0.26411894 0.26416373 0.26419222 0.26406893 0.26436257 0.2646601
 0.26472256 0.26485178 0.2646648  0.26434097 0.2642557  0.2639756
 0.26385292 0.26392695 0.26382375 0.26402983 0.26438004 0.26472464
 0.26525164 0.2655051  0.26519817 0.26493454 0.26454616 0.26427662
 0.26449564 0.2646526  0.26496983 0.26521    0.26586622 0.26615182
 0.26590097 0.26574168 0.2656402  0.26526576 0.2651779  0.26510608
 0.26485738 0.26481038 0.26475888 0.26513153 0.2659356  0.26647425
 0.2675018  0.2682307  0.26760355 0.26713723 0.26657295 0.26624796
 0.2663872  0.26687187 0.26761454 0.26835176 0.269452   0.26931953
 0.2683809  0.26766083 0.2671463  0.26679087 0.26663968 0.2665164
 0.26655996 0.26661918 0.26668966 0.2668324  0.26719597 0.26739514
 0.26744232 0.26754263 0.26718533 0.26699528 0.26670855 0.2665543
 0.26650983 0.26659697 0.26689956 0.2672222  0.26770568 0.2675935
 0.2670908  0.26664338 0.26643294 0.26619148 0.26604548 0.26603043
 0.26621878 0.26654276 0.26676008 0.26679263 0.26700613 0.2671214
 0.2668474  0.26666617 0.2664106  0.26652262 0.2665337  0.26627976
 0.26621002 0.26613697 0.26609305 0.26627335 0.2663277  0.26617357
 0.26624736 0.2661421  0.2661667  0.26608613 0.26581478 0.2658268
 0.26585144 0.26578957 0.26606682 0.26634246 0.2664912  0.26652035
 0.26623926 0.266157   0.2659651  0.26571646 0.26572475 0.2655311
 0.26531577 0.26544303 0.26546484 0.26559883 0.26578403 0.2655903
 0.26557907 0.26542157 0.26513118 0.26529244 0.26521388 0.26508126
 0.26527226 0.2652781  0.26538327 0.26570636 0.2658313  0.26590484
 0.26588166 0.2658717  0.26580536 0.26553676 0.26537484 0.26530516
 0.26513985 0.2651758  0.26517737 0.26509354 0.26529908 0.26540715
 0.26543942 0.26543394 0.26524386 0.2651897  0.26505256 0.26485354
 0.26497725 0.2650271  0.26491833 0.26511315 0.26527584 0.2654112
 0.26545247 0.26536736 0.26520005 0.26505995 0.26483747 0.26465094
 0.2645973  0.26459816 0.2645932  0.26434162 0.26457343 0.2648476
 0.26489374 0.26495865 0.2648938  0.2648077  0.26479238 0.2645718
 0.26447508 0.2645042  0.26438436 0.26461178 0.26493722 0.26524505
 0.26572424 0.26594582 0.26545188 0.2650537  0.26477832 0.26474088
 0.26484793 0.26493815 0.26516733 0.26540306 0.26601654 0.26617137
 0.26598737 0.2658796  0.26570806 0.2655135  0.26548913 0.26523536
 0.2649637  0.26492786 0.2648448  0.26524848 0.26604477 0.26668978
 0.2678495  0.2685378  0.26780337 0.267158   0.26643565 0.26621354
 0.26627314 0.26655605 0.26719993 0.2677929  0.268806   0.26878172
 0.2678817  0.26717314 0.26664147 0.26623797 0.26626    0.26621273
 0.26626414 0.26652002 0.26661265 0.2667121  0.26707187 0.26717678
 0.2673365  0.26751494 0.26700532 0.26674113 0.2663579  0.26595217
 0.2658122  0.26589876 0.26609907 0.26633692 0.26677212 0.26676667
 0.26652429 0.2663508  0.2663574  0.26620632 0.2661222  0.26615068
 0.26630536 0.26656544 0.2667708  0.2668486  0.2669548  0.2668068
 0.26649132 0.26622593 0.2658842  0.26595765 0.26586086 0.2656856
 0.26574868 0.26577097 0.26576293 0.26590317 0.2658984  0.2658587
 0.2659721  0.2657928  0.26599467 0.26607677 0.265914   0.26596865
 0.26601514 0.2660728  0.26628596 0.2663288  0.26636153 0.26619485
 0.26576805 0.2656613  0.26542115 0.2653042  0.26519358 0.26485375
 0.26485938 0.26498356 0.26485175 0.26505083 0.26515248 0.26489174
 0.26504916 0.26484787 0.26470882 0.26500988 0.26493502 0.26493672
 0.26507872 0.26488897 0.26482    0.2648801  0.26485205 0.26478717
 0.26442465 0.2643598  0.26431987 0.26401564 0.264093   0.26403052
 0.26378143 0.26389638 0.26378497 0.26372185 0.26396894 0.26385695
 0.26400653 0.26405573 0.26386356 0.26398483 0.26384792 0.26371112
 0.26385257 0.26365605 0.26354116 0.2636532  0.26348394 0.26347247
 0.2633414  0.26328054 0.26333207 0.26320818 0.2632129  0.26326436
 0.26311946 0.26311952 0.2630828  0.2627651  0.26308298 0.26323384
 0.26333815 0.2634792  0.26321334 0.26328787 0.26338658 0.26309168
 0.26303324 0.2630752  0.26282656 0.26284844 0.26297224 0.26313886
 0.26346397 0.26370105 0.26343805 0.2631864  0.2631299  0.26318648
 0.2631106  0.26320025 0.2635301  0.2637224  0.26438794 0.26447338
 0.26438665 0.2644878  0.26422703 0.26411977 0.26424283 0.2640024
 0.2638011  0.26381323 0.26356697 0.26374787 0.26413077 0.26437834
 0.26519793 0.26594275 0.26550913 0.2648536  0.264328   0.2642031
 0.26412156 0.2643175  0.26495516 0.26557618 0.26682794 0.266818
 0.26623055 0.26616037 0.2655754  0.26509935 0.26525408 0.265067
 0.26511112 0.2654584  0.26530614 0.26513857 0.2652533  0.26538405
 0.26558018 0.26579073 0.2658639  0.26564118 0.26545057 0.2655759
 0.2652705  0.26536247 0.26573738 0.26580256 0.266616   0.2666168
 0.26610738 0.26664698 0.26641673 0.2658843  0.26634502 0.2661281
 0.2658717  0.26606482 0.26602757 0.2659263  0.26574785 0.26566827
 0.265545   0.2654046  0.26647282 0.2663108  0.26665184 0.2666956
 0.26510885 0.2657435  0.26367113 0.2643579  0.26563275 0.2657839 ]
