Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j336_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j336_H5_FITS_custom_ftM_sl90_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11855
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=30, out_features=142, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  470031360.0
params:  4402.0
Trainable parameters:  4402
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 77.29734063148499
Epoch: 1, Steps: 92 | Train Loss: 1.5640656 Vali Loss: 1.3615561 Test Loss: 1.6483665
Validation loss decreased (inf --> 1.361556).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 72.3537335395813
Epoch: 2, Steps: 92 | Train Loss: 0.8939597 Vali Loss: 0.9342988 Test Loss: 1.1313195
Validation loss decreased (1.361556 --> 0.934299).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 75.02494359016418
Epoch: 3, Steps: 92 | Train Loss: 0.6552136 Vali Loss: 0.7500902 Test Loss: 0.9124672
Validation loss decreased (0.934299 --> 0.750090).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 74.5715057849884
Epoch: 4, Steps: 92 | Train Loss: 0.5465885 Vali Loss: 0.6621620 Test Loss: 0.8070377
Validation loss decreased (0.750090 --> 0.662162).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 70.0062403678894
Epoch: 5, Steps: 92 | Train Loss: 0.4913317 Vali Loss: 0.6150157 Test Loss: 0.7518582
Validation loss decreased (0.662162 --> 0.615016).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 77.03668069839478
Epoch: 6, Steps: 92 | Train Loss: 0.4613897 Vali Loss: 0.5880480 Test Loss: 0.7205543
Validation loss decreased (0.615016 --> 0.588048).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 73.89738893508911
Epoch: 7, Steps: 92 | Train Loss: 0.4435176 Vali Loss: 0.5713367 Test Loss: 0.7010359
Validation loss decreased (0.588048 --> 0.571337).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 77.10713291168213
Epoch: 8, Steps: 92 | Train Loss: 0.4319345 Vali Loss: 0.5596280 Test Loss: 0.6879114
Validation loss decreased (0.571337 --> 0.559628).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 77.75298547744751
Epoch: 9, Steps: 92 | Train Loss: 0.4240304 Vali Loss: 0.5513184 Test Loss: 0.6783906
Validation loss decreased (0.559628 --> 0.551318).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 77.07592630386353
Epoch: 10, Steps: 92 | Train Loss: 0.4181732 Vali Loss: 0.5455829 Test Loss: 0.6712717
Validation loss decreased (0.551318 --> 0.545583).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 73.82728314399719
Epoch: 11, Steps: 92 | Train Loss: 0.4137525 Vali Loss: 0.5403427 Test Loss: 0.6655999
Validation loss decreased (0.545583 --> 0.540343).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 78.73663759231567
Epoch: 12, Steps: 92 | Train Loss: 0.4103274 Vali Loss: 0.5366496 Test Loss: 0.6611486
Validation loss decreased (0.540343 --> 0.536650).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 77.62827777862549
Epoch: 13, Steps: 92 | Train Loss: 0.4075094 Vali Loss: 0.5329141 Test Loss: 0.6575209
Validation loss decreased (0.536650 --> 0.532914).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 72.58692812919617
Epoch: 14, Steps: 92 | Train Loss: 0.4051359 Vali Loss: 0.5307980 Test Loss: 0.6545056
Validation loss decreased (0.532914 --> 0.530798).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 79.02743673324585
Epoch: 15, Steps: 92 | Train Loss: 0.4030626 Vali Loss: 0.5283291 Test Loss: 0.6519517
Validation loss decreased (0.530798 --> 0.528329).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 76.73009181022644
Epoch: 16, Steps: 92 | Train Loss: 0.4015727 Vali Loss: 0.5263782 Test Loss: 0.6498415
Validation loss decreased (0.528329 --> 0.526378).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 73.61667370796204
Epoch: 17, Steps: 92 | Train Loss: 0.4001322 Vali Loss: 0.5249712 Test Loss: 0.6479590
Validation loss decreased (0.526378 --> 0.524971).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 80.1117627620697
Epoch: 18, Steps: 92 | Train Loss: 0.3990478 Vali Loss: 0.5233835 Test Loss: 0.6464237
Validation loss decreased (0.524971 --> 0.523383).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 71.2857015132904
Epoch: 19, Steps: 92 | Train Loss: 0.3979508 Vali Loss: 0.5222134 Test Loss: 0.6450081
Validation loss decreased (0.523383 --> 0.522213).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 77.83045387268066
Epoch: 20, Steps: 92 | Train Loss: 0.3971071 Vali Loss: 0.5207307 Test Loss: 0.6437894
Validation loss decreased (0.522213 --> 0.520731).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 77.09579372406006
Epoch: 21, Steps: 92 | Train Loss: 0.3962647 Vali Loss: 0.5199015 Test Loss: 0.6427605
Validation loss decreased (0.520731 --> 0.519902).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 69.85220313072205
Epoch: 22, Steps: 92 | Train Loss: 0.3954959 Vali Loss: 0.5188366 Test Loss: 0.6417490
Validation loss decreased (0.519902 --> 0.518837).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 61.48532581329346
Epoch: 23, Steps: 92 | Train Loss: 0.3947488 Vali Loss: 0.5183625 Test Loss: 0.6409534
Validation loss decreased (0.518837 --> 0.518362).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 61.00293159484863
Epoch: 24, Steps: 92 | Train Loss: 0.3943249 Vali Loss: 0.5179912 Test Loss: 0.6402359
Validation loss decreased (0.518362 --> 0.517991).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 58.703463077545166
Epoch: 25, Steps: 92 | Train Loss: 0.3937525 Vali Loss: 0.5170838 Test Loss: 0.6395584
Validation loss decreased (0.517991 --> 0.517084).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 61.96572685241699
Epoch: 26, Steps: 92 | Train Loss: 0.3934824 Vali Loss: 0.5161159 Test Loss: 0.6389873
Validation loss decreased (0.517084 --> 0.516116).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 62.65924310684204
Epoch: 27, Steps: 92 | Train Loss: 0.3929410 Vali Loss: 0.5160741 Test Loss: 0.6384668
Validation loss decreased (0.516116 --> 0.516074).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 59.33011341094971
Epoch: 28, Steps: 92 | Train Loss: 0.3925245 Vali Loss: 0.5155952 Test Loss: 0.6380407
Validation loss decreased (0.516074 --> 0.515595).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 62.46711802482605
Epoch: 29, Steps: 92 | Train Loss: 0.3923194 Vali Loss: 0.5151985 Test Loss: 0.6376789
Validation loss decreased (0.515595 --> 0.515198).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 61.86361575126648
Epoch: 30, Steps: 92 | Train Loss: 0.3919527 Vali Loss: 0.5143535 Test Loss: 0.6372717
Validation loss decreased (0.515198 --> 0.514354).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 59.94011187553406
Epoch: 31, Steps: 92 | Train Loss: 0.3917163 Vali Loss: 0.5143300 Test Loss: 0.6369360
Validation loss decreased (0.514354 --> 0.514330).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 60.84307527542114
Epoch: 32, Steps: 92 | Train Loss: 0.3915352 Vali Loss: 0.5142246 Test Loss: 0.6366785
Validation loss decreased (0.514330 --> 0.514225).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 60.41881465911865
Epoch: 33, Steps: 92 | Train Loss: 0.3912897 Vali Loss: 0.5137908 Test Loss: 0.6363783
Validation loss decreased (0.514225 --> 0.513791).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 58.75367307662964
Epoch: 34, Steps: 92 | Train Loss: 0.3910082 Vali Loss: 0.5142639 Test Loss: 0.6361416
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 61.493645668029785
Epoch: 35, Steps: 92 | Train Loss: 0.3909624 Vali Loss: 0.5133605 Test Loss: 0.6359417
Validation loss decreased (0.513791 --> 0.513361).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 61.4715850353241
Epoch: 36, Steps: 92 | Train Loss: 0.3906991 Vali Loss: 0.5132849 Test Loss: 0.6357132
Validation loss decreased (0.513361 --> 0.513285).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 54.18446731567383
Epoch: 37, Steps: 92 | Train Loss: 0.3906308 Vali Loss: 0.5128195 Test Loss: 0.6355378
Validation loss decreased (0.513285 --> 0.512819).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 59.26400113105774
Epoch: 38, Steps: 92 | Train Loss: 0.3905489 Vali Loss: 0.5128131 Test Loss: 0.6353946
Validation loss decreased (0.512819 --> 0.512813).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 52.10064125061035
Epoch: 39, Steps: 92 | Train Loss: 0.3904515 Vali Loss: 0.5129105 Test Loss: 0.6352283
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 56.146403551101685
Epoch: 40, Steps: 92 | Train Loss: 0.3901845 Vali Loss: 0.5125970 Test Loss: 0.6351300
Validation loss decreased (0.512813 --> 0.512597).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 57.987876415252686
Epoch: 41, Steps: 92 | Train Loss: 0.3901594 Vali Loss: 0.5126399 Test Loss: 0.6350523
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 51.31847596168518
Epoch: 42, Steps: 92 | Train Loss: 0.3899955 Vali Loss: 0.5125253 Test Loss: 0.6349308
Validation loss decreased (0.512597 --> 0.512525).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 51.83410120010376
Epoch: 43, Steps: 92 | Train Loss: 0.3898976 Vali Loss: 0.5124181 Test Loss: 0.6348465
Validation loss decreased (0.512525 --> 0.512418).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 59.39233636856079
Epoch: 44, Steps: 92 | Train Loss: 0.3898759 Vali Loss: 0.5124157 Test Loss: 0.6347545
Validation loss decreased (0.512418 --> 0.512416).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 60.211158990859985
Epoch: 45, Steps: 92 | Train Loss: 0.3898098 Vali Loss: 0.5118814 Test Loss: 0.6346870
Validation loss decreased (0.512416 --> 0.511881).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 63.885762453079224
Epoch: 46, Steps: 92 | Train Loss: 0.3897727 Vali Loss: 0.5121872 Test Loss: 0.6346106
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 60.15605354309082
Epoch: 47, Steps: 92 | Train Loss: 0.3896265 Vali Loss: 0.5122872 Test Loss: 0.6345567
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 57.13749837875366
Epoch: 48, Steps: 92 | Train Loss: 0.3896957 Vali Loss: 0.5121726 Test Loss: 0.6345180
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 48.53890514373779
Epoch: 49, Steps: 92 | Train Loss: 0.3896180 Vali Loss: 0.5120289 Test Loss: 0.6344742
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 43.47630047798157
Epoch: 50, Steps: 92 | Train Loss: 0.3895188 Vali Loss: 0.5115962 Test Loss: 0.6344385
Validation loss decreased (0.511881 --> 0.511596).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 46.011526584625244
Epoch: 51, Steps: 92 | Train Loss: 0.3894755 Vali Loss: 0.5117788 Test Loss: 0.6343754
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 43.309133529663086
Epoch: 52, Steps: 92 | Train Loss: 0.3895031 Vali Loss: 0.5116591 Test Loss: 0.6343503
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 43.601128339767456
Epoch: 53, Steps: 92 | Train Loss: 0.3894552 Vali Loss: 0.5120449 Test Loss: 0.6343241
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 47.73983597755432
Epoch: 54, Steps: 92 | Train Loss: 0.3893757 Vali Loss: 0.5116371 Test Loss: 0.6342650
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 47.34139823913574
Epoch: 55, Steps: 92 | Train Loss: 0.3893767 Vali Loss: 0.5114793 Test Loss: 0.6342556
Validation loss decreased (0.511596 --> 0.511479).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 46.29851412773132
Epoch: 56, Steps: 92 | Train Loss: 0.3893810 Vali Loss: 0.5117565 Test Loss: 0.6342303
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 50.27823877334595
Epoch: 57, Steps: 92 | Train Loss: 0.3893854 Vali Loss: 0.5117990 Test Loss: 0.6341913
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 46.65425515174866
Epoch: 58, Steps: 92 | Train Loss: 0.3893022 Vali Loss: 0.5111989 Test Loss: 0.6341639
Validation loss decreased (0.511479 --> 0.511199).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 47.83020830154419
Epoch: 59, Steps: 92 | Train Loss: 0.3893394 Vali Loss: 0.5111145 Test Loss: 0.6341501
Validation loss decreased (0.511199 --> 0.511115).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 47.953404903411865
Epoch: 60, Steps: 92 | Train Loss: 0.3892381 Vali Loss: 0.5113966 Test Loss: 0.6341289
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 46.44379210472107
Epoch: 61, Steps: 92 | Train Loss: 0.3893094 Vali Loss: 0.5109675 Test Loss: 0.6341146
Validation loss decreased (0.511115 --> 0.510967).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 43.57873821258545
Epoch: 62, Steps: 92 | Train Loss: 0.3892410 Vali Loss: 0.5114871 Test Loss: 0.6341048
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 48.155577182769775
Epoch: 63, Steps: 92 | Train Loss: 0.3892427 Vali Loss: 0.5112290 Test Loss: 0.6340990
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 42.84206676483154
Epoch: 64, Steps: 92 | Train Loss: 0.3892309 Vali Loss: 0.5117387 Test Loss: 0.6340898
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 49.074787855148315
Epoch: 65, Steps: 92 | Train Loss: 0.3891472 Vali Loss: 0.5116075 Test Loss: 0.6340714
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 46.12253999710083
Epoch: 66, Steps: 92 | Train Loss: 0.3891492 Vali Loss: 0.5115326 Test Loss: 0.6340706
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 46.26999545097351
Epoch: 67, Steps: 92 | Train Loss: 0.3890114 Vali Loss: 0.5110631 Test Loss: 0.6340607
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 49.29414129257202
Epoch: 68, Steps: 92 | Train Loss: 0.3890268 Vali Loss: 0.5113067 Test Loss: 0.6340511
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 44.889506101608276
Epoch: 69, Steps: 92 | Train Loss: 0.3891394 Vali Loss: 0.5111756 Test Loss: 0.6340553
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 47.8374662399292
Epoch: 70, Steps: 92 | Train Loss: 0.3890100 Vali Loss: 0.5111239 Test Loss: 0.6340493
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 47.62427568435669
Epoch: 71, Steps: 92 | Train Loss: 0.3890995 Vali Loss: 0.5112455 Test Loss: 0.6340367
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j336_H5_FITS_custom_ftM_sl90_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.6322439908981323, mae:0.3802177608013153, rse:0.6534940600395203, corr:[0.27362162 0.2744567  0.2782726  0.27541554 0.27477053 0.27971354
 0.28176844 0.28090432 0.28214562 0.28210723 0.27970126 0.28014153
 0.28015766 0.27824068 0.27892336 0.28017157 0.2797431  0.2809081
 0.28347754 0.28443432 0.28442118 0.28608772 0.2860545  0.28437862
 0.28926742 0.2940584  0.29329187 0.29164472 0.29234746 0.29363123
 0.29392505 0.2956032  0.29604572 0.29509956 0.2947507  0.29469112
 0.29368877 0.2928652  0.2932909  0.29335654 0.2932224  0.29431197
 0.29563606 0.29566118 0.29543307 0.296173   0.29522985 0.29328555
 0.2942765  0.29368356 0.29180667 0.2912543  0.29051888 0.28629854
 0.28303757 0.28447026 0.2848222  0.28402123 0.28354216 0.28237376
 0.28085887 0.28017184 0.2800049  0.2797084  0.27968568 0.28020594
 0.2808088  0.2810118  0.28119078 0.2818181  0.28153566 0.27992997
 0.278509   0.2772897  0.2765582  0.27748552 0.27909744 0.27901223
 0.27908114 0.28162554 0.28210056 0.28138417 0.28084147 0.27945837
 0.27809784 0.27740762 0.2768299  0.27638626 0.27619678 0.27636933
 0.27685696 0.2772157  0.27780962 0.2786983  0.27897072 0.27878413
 0.27880183 0.27868336 0.27840075 0.2784295  0.27875602 0.27888235
 0.27933687 0.28070977 0.28075907 0.2797915  0.27938074 0.27873093
 0.2777983  0.2773879  0.27721795 0.2771088  0.27712834 0.27740973
 0.2780684  0.2785476  0.27900162 0.27979523 0.28016454 0.28013298
 0.28039303 0.2804829  0.28038266 0.2802922  0.28016648 0.27992138
 0.27963063 0.27959698 0.27906775 0.27809763 0.277743   0.27741295
 0.27684397 0.27683255 0.2770783  0.2772796  0.2777575  0.27847221
 0.27934426 0.2799175  0.28038138 0.28146127 0.28128406 0.27962926
 0.27910975 0.27899018 0.27868155 0.2786643  0.27878398 0.27883935
 0.27886552 0.27893755 0.27851272 0.2775315  0.2769609  0.27665004
 0.27624795 0.27643237 0.27698448 0.2774758  0.2782569  0.2792192
 0.28069347 0.28220046 0.28286508 0.28383088 0.28355396 0.28086984
 0.28008386 0.2789559  0.27679393 0.2756937  0.2761143  0.27778512
 0.27994522 0.28140202 0.28149235 0.28120127 0.28133786 0.28138268
 0.2808902  0.28062713 0.28100136 0.28130284 0.28163218 0.28254673
 0.28389642 0.28489742 0.2851701  0.28586477 0.28563514 0.28457138
 0.2879833  0.29067227 0.2893271  0.2877506  0.28785864 0.28890082
 0.29002994 0.29201928 0.29268324 0.29255292 0.29293013 0.2929402
 0.29224843 0.29166254 0.29170164 0.29173058 0.29169947 0.29231676
 0.293239   0.29344314 0.29318604 0.29343697 0.29259497 0.29073653
 0.29049277 0.2888738  0.28672087 0.28605375 0.285299   0.28161526
 0.2790816  0.28078938 0.28156897 0.28135607 0.2813253  0.2804635
 0.27925405 0.27867776 0.2784637  0.27814063 0.27802187 0.27853802
 0.279076   0.27906376 0.27918828 0.27949458 0.27883637 0.2770537
 0.27569616 0.27455956 0.2736757  0.27438718 0.27605972 0.27640358
 0.2772404  0.28029728 0.2812373  0.28088892 0.280651   0.27954897
 0.2783128  0.2776376  0.27712214 0.27661458 0.27618888 0.27629688
 0.27666634 0.27666673 0.2769204  0.27757314 0.2775774  0.2771662
 0.27706805 0.27684897 0.27649367 0.27667555 0.27714333 0.27733603
 0.2780276  0.27977514 0.2800608  0.27930436 0.27925554 0.27880988
 0.2778553  0.27751425 0.27743214 0.27704164 0.27681553 0.2772084
 0.2776624  0.27754933 0.27768525 0.27833468 0.27841368 0.27823037
 0.2784047  0.27827778 0.27810907 0.27845362 0.27866662 0.27834278
 0.27824423 0.27863806 0.2782411  0.2774849  0.27761838 0.27734953
 0.27653003 0.27670026 0.27716488 0.27698648 0.27719402 0.2780781
 0.278555   0.27834436 0.27866513 0.27949396 0.27874625 0.2770325
 0.276654   0.27625906 0.2761168  0.27688947 0.2770212  0.2767555
 0.27759382 0.27840927 0.27777338 0.27756602 0.27817157 0.27740812
 0.27682447 0.27800006 0.27828652 0.27748325 0.27813378 0.27868927
 0.27750418 0.2781591  0.27943355 0.27729988 0.27734858 0.27389747]
