Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=138, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j336_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j336_H8_FITS_custom_ftM_sl360_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11585
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=138, out_features=266, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4050213888.0
params:  36974.0
Trainable parameters:  36974
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 186.46181964874268
Epoch: 1, Steps: 90 | Train Loss: 0.9863351 Vali Loss: 0.8783845 Test Loss: 1.0276666
Validation loss decreased (inf --> 0.878385).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 178.35029792785645
Epoch: 2, Steps: 90 | Train Loss: 0.6142789 Vali Loss: 0.6737124 Test Loss: 0.7886867
Validation loss decreased (0.878385 --> 0.673712).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 182.26056838035583
Epoch: 3, Steps: 90 | Train Loss: 0.4818328 Vali Loss: 0.5570695 Test Loss: 0.6547030
Validation loss decreased (0.673712 --> 0.557070).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 172.24506950378418
Epoch: 4, Steps: 90 | Train Loss: 0.4013281 Vali Loss: 0.4837542 Test Loss: 0.5716976
Validation loss decreased (0.557070 --> 0.483754).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 177.5480272769928
Epoch: 5, Steps: 90 | Train Loss: 0.3505589 Vali Loss: 0.4367241 Test Loss: 0.5196982
Validation loss decreased (0.483754 --> 0.436724).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 170.5514416694641
Epoch: 6, Steps: 90 | Train Loss: 0.3183556 Vali Loss: 0.4070340 Test Loss: 0.4874108
Validation loss decreased (0.436724 --> 0.407034).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 174.74169850349426
Epoch: 7, Steps: 90 | Train Loss: 0.2979165 Vali Loss: 0.3879860 Test Loss: 0.4673612
Validation loss decreased (0.407034 --> 0.387986).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 174.68095088005066
Epoch: 8, Steps: 90 | Train Loss: 0.2852032 Vali Loss: 0.3758308 Test Loss: 0.4549761
Validation loss decreased (0.387986 --> 0.375831).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 165.8436894416809
Epoch: 9, Steps: 90 | Train Loss: 0.2772122 Vali Loss: 0.3681785 Test Loss: 0.4477070
Validation loss decreased (0.375831 --> 0.368178).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 166.55922746658325
Epoch: 10, Steps: 90 | Train Loss: 0.2723040 Vali Loss: 0.3632222 Test Loss: 0.4430819
Validation loss decreased (0.368178 --> 0.363222).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 152.12785053253174
Epoch: 11, Steps: 90 | Train Loss: 0.2693690 Vali Loss: 0.3603106 Test Loss: 0.4405964
Validation loss decreased (0.363222 --> 0.360311).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 152.13955283164978
Epoch: 12, Steps: 90 | Train Loss: 0.2673893 Vali Loss: 0.3580874 Test Loss: 0.4391244
Validation loss decreased (0.360311 --> 0.358087).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 151.83048748970032
Epoch: 13, Steps: 90 | Train Loss: 0.2663012 Vali Loss: 0.3567350 Test Loss: 0.4382741
Validation loss decreased (0.358087 --> 0.356735).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 149.81108450889587
Epoch: 14, Steps: 90 | Train Loss: 0.2654795 Vali Loss: 0.3562467 Test Loss: 0.4378105
Validation loss decreased (0.356735 --> 0.356247).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 156.68907070159912
Epoch: 15, Steps: 90 | Train Loss: 0.2650441 Vali Loss: 0.3555377 Test Loss: 0.4374941
Validation loss decreased (0.356247 --> 0.355538).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 146.21275687217712
Epoch: 16, Steps: 90 | Train Loss: 0.2647929 Vali Loss: 0.3554382 Test Loss: 0.4373996
Validation loss decreased (0.355538 --> 0.355438).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 148.18475317955017
Epoch: 17, Steps: 90 | Train Loss: 0.2646085 Vali Loss: 0.3547268 Test Loss: 0.4371993
Validation loss decreased (0.355438 --> 0.354727).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 129.2943410873413
Epoch: 18, Steps: 90 | Train Loss: 0.2645172 Vali Loss: 0.3548872 Test Loss: 0.4371790
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 122.91676235198975
Epoch: 19, Steps: 90 | Train Loss: 0.2643332 Vali Loss: 0.3547223 Test Loss: 0.4371421
Validation loss decreased (0.354727 --> 0.354722).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 130.98595094680786
Epoch: 20, Steps: 90 | Train Loss: 0.2642789 Vali Loss: 0.3543408 Test Loss: 0.4371264
Validation loss decreased (0.354722 --> 0.354341).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 128.7180953025818
Epoch: 21, Steps: 90 | Train Loss: 0.2642423 Vali Loss: 0.3544313 Test Loss: 0.4371360
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 131.84237360954285
Epoch: 22, Steps: 90 | Train Loss: 0.2642259 Vali Loss: 0.3543782 Test Loss: 0.4370294
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 128.44759964942932
Epoch: 23, Steps: 90 | Train Loss: 0.2641706 Vali Loss: 0.3544852 Test Loss: 0.4371037
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 123.86929416656494
Epoch: 24, Steps: 90 | Train Loss: 0.2641334 Vali Loss: 0.3539924 Test Loss: 0.4370233
Validation loss decreased (0.354341 --> 0.353992).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 115.26577544212341
Epoch: 25, Steps: 90 | Train Loss: 0.2640894 Vali Loss: 0.3543628 Test Loss: 0.4369113
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 115.85005497932434
Epoch: 26, Steps: 90 | Train Loss: 0.2641123 Vali Loss: 0.3538079 Test Loss: 0.4369972
Validation loss decreased (0.353992 --> 0.353808).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 113.61957383155823
Epoch: 27, Steps: 90 | Train Loss: 0.2640301 Vali Loss: 0.3538684 Test Loss: 0.4370241
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 122.50688910484314
Epoch: 28, Steps: 90 | Train Loss: 0.2639931 Vali Loss: 0.3540721 Test Loss: 0.4369081
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 121.91770339012146
Epoch: 29, Steps: 90 | Train Loss: 0.2641087 Vali Loss: 0.3542216 Test Loss: 0.4369527
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 111.79340147972107
Epoch: 30, Steps: 90 | Train Loss: 0.2639824 Vali Loss: 0.3543357 Test Loss: 0.4368986
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 110.64822602272034
Epoch: 31, Steps: 90 | Train Loss: 0.2639187 Vali Loss: 0.3539726 Test Loss: 0.4368891
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 106.5699360370636
Epoch: 32, Steps: 90 | Train Loss: 0.2639260 Vali Loss: 0.3539606 Test Loss: 0.4368818
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 99.60919141769409
Epoch: 33, Steps: 90 | Train Loss: 0.2639507 Vali Loss: 0.3539549 Test Loss: 0.4368877
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 97.44170117378235
Epoch: 34, Steps: 90 | Train Loss: 0.2638536 Vali Loss: 0.3541820 Test Loss: 0.4368710
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 88.73678469657898
Epoch: 35, Steps: 90 | Train Loss: 0.2638546 Vali Loss: 0.3539971 Test Loss: 0.4368855
EarlyStopping counter: 9 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 141.2781035900116
Epoch: 36, Steps: 90 | Train Loss: 0.2639153 Vali Loss: 0.3541222 Test Loss: 0.4368639
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j336_H8_FITS_custom_ftM_sl360_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.43450403213500977, mae:0.2912827134132385, rse:0.5417465567588806, corr:[0.27491623 0.2779374  0.2823677  0.27875325 0.2834249  0.28405732
 0.2833583  0.28634906 0.28534207 0.28439617 0.28542027 0.28366262
 0.28336924 0.28353357 0.28143993 0.28178626 0.28230637 0.28134367
 0.28216663 0.2823862  0.2820005  0.28256112 0.28180644 0.28221798
 0.28475687 0.28474343 0.28459105 0.28532445 0.28446543 0.2840603
 0.28455642 0.2834922  0.2828647  0.28311387 0.282378   0.28265867
 0.28316215 0.28213397 0.28227177 0.28277922 0.28231212 0.28293082
 0.2832745  0.28267503 0.2830818  0.28303546 0.2826847  0.2835577
 0.28387642 0.2836391  0.28436878 0.28406096 0.28342804 0.2837736
 0.28335643 0.28289208 0.2831971  0.28270334 0.28238437 0.28286162
 0.28251973 0.28244674 0.28299502 0.28266644 0.28276825 0.28319633
 0.28278512 0.28293425 0.2833652  0.2829271  0.28320536 0.2836949
 0.28332677 0.28362182 0.28377974 0.28304732 0.28299487 0.2827315
 0.28218314 0.28265306 0.28259954 0.28196913 0.28237742 0.2825068
 0.28216898 0.28251612 0.28244936 0.28227907 0.28300828 0.2831481
 0.2829855  0.283318   0.28300965 0.282621   0.2828733  0.2826554
 0.28251076 0.28303853 0.28265435 0.28233376 0.28252935 0.2820649
 0.2820474  0.2823464  0.28186157 0.28200328 0.2825485  0.28228793
 0.28244498 0.28274187 0.28230953 0.28239787 0.2826533  0.28227913
 0.2823559  0.28243813 0.28211868 0.2821766  0.28211272 0.28193384
 0.28235    0.28258315 0.28236777 0.28270072 0.28263214 0.28220215
 0.2823986  0.28207737 0.2815474  0.2819313  0.28207514 0.28212067
 0.282636   0.2825192  0.2823305  0.28274602 0.28282145 0.2828342
 0.28308865 0.2828623  0.2827279  0.2829181  0.2828407  0.28317276
 0.28365138 0.28361323 0.2836873  0.28371468 0.28338975 0.28364447
 0.28382495 0.2833277  0.2833909  0.28362647 0.28341773 0.2837482
 0.28397655 0.28369212 0.28398323 0.28410703 0.28372478 0.28388998
 0.28386256 0.28352323 0.28364825 0.28338134 0.2830469  0.28389215
 0.28484854 0.28455877 0.28458956 0.2843219  0.28413773 0.28454623
 0.28438818 0.28401428 0.2841782  0.2839862  0.2839042  0.28425586
 0.28399733 0.28373823 0.2839251  0.28377977 0.28389475 0.2841378
 0.28373277 0.2837063  0.28372622 0.28319627 0.28333443 0.28382403
 0.2841259  0.28421327 0.28413144 0.28367284 0.28391436 0.28403983
 0.28360796 0.28368872 0.28355515 0.28303805 0.28328437 0.2834135
 0.28316638 0.28343165 0.28333396 0.28302613 0.28350884 0.2835528
 0.28302833 0.28298295 0.28269663 0.28245553 0.2828132  0.2829387
 0.28317884 0.28339216 0.28309315 0.2831259  0.28343764 0.28300726
 0.28285936 0.2830163  0.2825172  0.28248686 0.28283116 0.28256416
 0.28268728 0.28278074 0.28221032 0.28239483 0.28274256 0.28235987
 0.2825457  0.28268433 0.28224087 0.28250626 0.28272003 0.2824198
 0.28284767 0.28299958 0.2825356  0.2827211  0.2826742  0.28230298
 0.28256643 0.2823764  0.28194237 0.28224522 0.282198   0.28205216
 0.2824907  0.2823015  0.28194964 0.28221604 0.28208944 0.2820641
 0.28246647 0.2821763  0.28193286 0.2821018  0.2817147  0.2816755
 0.2819135  0.28161365 0.28170848 0.28192648 0.2815299  0.2818057
 0.28209376 0.281545   0.28166357 0.28178677 0.2812211  0.2814021
 0.28163132 0.28124508 0.28136066 0.28126907 0.28096044 0.2814686
 0.2815809  0.2811991  0.28145558 0.2814386  0.28128597 0.28170478
 0.2812872  0.2809611  0.2814826  0.2814533  0.28143013 0.281913
 0.2815146  0.2811885  0.28136286 0.28077504 0.28077728 0.2812898
 0.2808519  0.28098547 0.28132752 0.2807143  0.2810667  0.28163892
 0.28120393 0.28144208 0.28165895 0.2812702  0.28184304 0.28207776
 0.28148186 0.28210452 0.2825031  0.28232569 0.28274938 0.28225544
 0.28187683 0.28260776 0.2819722  0.28174105 0.2827287  0.2821464
 0.28255215 0.28370494 0.28268194 0.28329393 0.28411397 0.28240028
 0.28298935 0.2820365  0.27923816 0.28149182 0.27814695 0.2857587 ]
