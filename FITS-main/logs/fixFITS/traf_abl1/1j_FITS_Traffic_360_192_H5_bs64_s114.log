Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j192_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j192_H5_FITS_custom_ftM_sl360_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11729
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=90, out_features=138, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1370373120.0
params:  12558.0
Trainable parameters:  12558
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 76.65112590789795
Epoch: 1, Steps: 91 | Train Loss: 0.9783358 Vali Loss: 0.8493549 Test Loss: 0.9863124
Validation loss decreased (inf --> 0.849355).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 70.61509728431702
Epoch: 2, Steps: 91 | Train Loss: 0.5627151 Vali Loss: 0.5837952 Test Loss: 0.6838512
Validation loss decreased (0.849355 --> 0.583795).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 74.05419635772705
Epoch: 3, Steps: 91 | Train Loss: 0.4025657 Vali Loss: 0.4600383 Test Loss: 0.5461250
Validation loss decreased (0.583795 --> 0.460038).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 71.43128681182861
Epoch: 4, Steps: 91 | Train Loss: 0.3260320 Vali Loss: 0.4002329 Test Loss: 0.4821253
Validation loss decreased (0.460038 --> 0.400233).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 69.18352484703064
Epoch: 5, Steps: 91 | Train Loss: 0.2898592 Vali Loss: 0.3720493 Test Loss: 0.4536036
Validation loss decreased (0.400233 --> 0.372049).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 73.16727924346924
Epoch: 6, Steps: 91 | Train Loss: 0.2733712 Vali Loss: 0.3592645 Test Loss: 0.4416447
Validation loss decreased (0.372049 --> 0.359265).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 69.1832025051117
Epoch: 7, Steps: 91 | Train Loss: 0.2660314 Vali Loss: 0.3531647 Test Loss: 0.4367933
Validation loss decreased (0.359265 --> 0.353165).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 71.37590408325195
Epoch: 8, Steps: 91 | Train Loss: 0.2627770 Vali Loss: 0.3508684 Test Loss: 0.4349630
Validation loss decreased (0.353165 --> 0.350868).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 67.10178065299988
Epoch: 9, Steps: 91 | Train Loss: 0.2612666 Vali Loss: 0.3499337 Test Loss: 0.4342325
Validation loss decreased (0.350868 --> 0.349934).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 66.57411170005798
Epoch: 10, Steps: 91 | Train Loss: 0.2606124 Vali Loss: 0.3491526 Test Loss: 0.4339666
Validation loss decreased (0.349934 --> 0.349153).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 71.81477952003479
Epoch: 11, Steps: 91 | Train Loss: 0.2600579 Vali Loss: 0.3486821 Test Loss: 0.4337707
Validation loss decreased (0.349153 --> 0.348682).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 69.74415230751038
Epoch: 12, Steps: 91 | Train Loss: 0.2597606 Vali Loss: 0.3483896 Test Loss: 0.4335613
Validation loss decreased (0.348682 --> 0.348390).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 71.11601328849792
Epoch: 13, Steps: 91 | Train Loss: 0.2596520 Vali Loss: 0.3478340 Test Loss: 0.4334548
Validation loss decreased (0.348390 --> 0.347834).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 70.84608340263367
Epoch: 14, Steps: 91 | Train Loss: 0.2595140 Vali Loss: 0.3478451 Test Loss: 0.4333043
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 69.11122798919678
Epoch: 15, Steps: 91 | Train Loss: 0.2592615 Vali Loss: 0.3479925 Test Loss: 0.4332606
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 70.3789381980896
Epoch: 16, Steps: 91 | Train Loss: 0.2593188 Vali Loss: 0.3475226 Test Loss: 0.4331393
Validation loss decreased (0.347834 --> 0.347523).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 69.02554988861084
Epoch: 17, Steps: 91 | Train Loss: 0.2592951 Vali Loss: 0.3478222 Test Loss: 0.4330139
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 71.7757797241211
Epoch: 18, Steps: 91 | Train Loss: 0.2591294 Vali Loss: 0.3472776 Test Loss: 0.4329185
Validation loss decreased (0.347523 --> 0.347278).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 69.44762659072876
Epoch: 19, Steps: 91 | Train Loss: 0.2591110 Vali Loss: 0.3473279 Test Loss: 0.4328498
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 67.06217074394226
Epoch: 20, Steps: 91 | Train Loss: 0.2589704 Vali Loss: 0.3474216 Test Loss: 0.4328085
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 70.07484865188599
Epoch: 21, Steps: 91 | Train Loss: 0.2590644 Vali Loss: 0.3475877 Test Loss: 0.4327594
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 66.86743521690369
Epoch: 22, Steps: 91 | Train Loss: 0.2589704 Vali Loss: 0.3472719 Test Loss: 0.4326862
Validation loss decreased (0.347278 --> 0.347272).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 70.58727836608887
Epoch: 23, Steps: 91 | Train Loss: 0.2588946 Vali Loss: 0.3474224 Test Loss: 0.4326104
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 71.06289768218994
Epoch: 24, Steps: 91 | Train Loss: 0.2588876 Vali Loss: 0.3470165 Test Loss: 0.4325910
Validation loss decreased (0.347272 --> 0.347016).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 66.10901546478271
Epoch: 25, Steps: 91 | Train Loss: 0.2586978 Vali Loss: 0.3467810 Test Loss: 0.4325690
Validation loss decreased (0.347016 --> 0.346781).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 70.09163999557495
Epoch: 26, Steps: 91 | Train Loss: 0.2588264 Vali Loss: 0.3471079 Test Loss: 0.4325086
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 68.44739985466003
Epoch: 27, Steps: 91 | Train Loss: 0.2588492 Vali Loss: 0.3471948 Test Loss: 0.4324877
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 68.59741187095642
Epoch: 28, Steps: 91 | Train Loss: 0.2586110 Vali Loss: 0.3469511 Test Loss: 0.4324884
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 74.11760306358337
Epoch: 29, Steps: 91 | Train Loss: 0.2586571 Vali Loss: 0.3470058 Test Loss: 0.4324631
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 67.94675946235657
Epoch: 30, Steps: 91 | Train Loss: 0.2586434 Vali Loss: 0.3472069 Test Loss: 0.4324405
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 69.4368040561676
Epoch: 31, Steps: 91 | Train Loss: 0.2585679 Vali Loss: 0.3469152 Test Loss: 0.4323823
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 66.11988711357117
Epoch: 32, Steps: 91 | Train Loss: 0.2585966 Vali Loss: 0.3468893 Test Loss: 0.4323695
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 54.040711879730225
Epoch: 33, Steps: 91 | Train Loss: 0.2585371 Vali Loss: 0.3464226 Test Loss: 0.4323941
Validation loss decreased (0.346781 --> 0.346423).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 56.90153765678406
Epoch: 34, Steps: 91 | Train Loss: 0.2585671 Vali Loss: 0.3463006 Test Loss: 0.4323548
Validation loss decreased (0.346423 --> 0.346301).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 54.769842863082886
Epoch: 35, Steps: 91 | Train Loss: 0.2585649 Vali Loss: 0.3469264 Test Loss: 0.4323264
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 56.42772603034973
Epoch: 36, Steps: 91 | Train Loss: 0.2585117 Vali Loss: 0.3472490 Test Loss: 0.4323164
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 54.71199703216553
Epoch: 37, Steps: 91 | Train Loss: 0.2585840 Vali Loss: 0.3467551 Test Loss: 0.4323072
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 54.39070439338684
Epoch: 38, Steps: 91 | Train Loss: 0.2585501 Vali Loss: 0.3465620 Test Loss: 0.4322977
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 56.49001097679138
Epoch: 39, Steps: 91 | Train Loss: 0.2585186 Vali Loss: 0.3469191 Test Loss: 0.4323017
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 53.95738697052002
Epoch: 40, Steps: 91 | Train Loss: 0.2585125 Vali Loss: 0.3470018 Test Loss: 0.4322591
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 57.6178023815155
Epoch: 41, Steps: 91 | Train Loss: 0.2584678 Vali Loss: 0.3467047 Test Loss: 0.4322867
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 54.50694561004639
Epoch: 42, Steps: 91 | Train Loss: 0.2584290 Vali Loss: 0.3465677 Test Loss: 0.4322914
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 54.25043249130249
Epoch: 43, Steps: 91 | Train Loss: 0.2584892 Vali Loss: 0.3467494 Test Loss: 0.4322771
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 56.33466386795044
Epoch: 44, Steps: 91 | Train Loss: 0.2584920 Vali Loss: 0.3466509 Test Loss: 0.4322288
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j192_H5_FITS_custom_ftM_sl360_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.43136706948280334, mae:0.2972901165485382, rse:0.5420664548873901, corr:[0.28704646 0.28829715 0.28950557 0.29231724 0.29254198 0.29166362
 0.2920945  0.29336247 0.29369688 0.29284993 0.2920743  0.29201016
 0.29203755 0.2915041  0.29080796 0.29068428 0.29108056 0.29134497
 0.29118127 0.29097727 0.29097918 0.2909142  0.29071254 0.29111958
 0.29219022 0.29279584 0.29290643 0.2924987  0.29197848 0.29192212
 0.29222035 0.29226014 0.29181746 0.29133618 0.29126236 0.2913881
 0.29132557 0.2910221  0.2908826  0.29108688 0.2913445  0.29131
 0.29102215 0.29081437 0.29083392 0.29085273 0.29077542 0.29101536
 0.29155615 0.2917502  0.29156035 0.2911537  0.2909098  0.2909983
 0.29112145 0.29092774 0.2905096  0.29025412 0.29027972 0.2903067
 0.2901577  0.28994286 0.28992906 0.29012772 0.29031804 0.29033926
 0.29022816 0.29015875 0.2901888  0.29022518 0.29021338 0.290356
 0.29054967 0.2905781  0.29035047 0.28999543 0.28980145 0.2898509
 0.28990853 0.28974187 0.28946954 0.28939116 0.28952613 0.2895821
 0.28941295 0.28920305 0.2892376  0.28949675 0.28971845 0.28971478
 0.28954795 0.2893929  0.28934765 0.28935882 0.2893664  0.28940874
 0.2894264  0.28941002 0.28922704 0.28900814 0.288978   0.2891366
 0.2892298  0.2890459  0.2887259  0.2885898  0.28869635 0.2887764
 0.28866214 0.2884779  0.28849837 0.2887203  0.28887394 0.2888409
 0.28873777 0.28876463 0.28892943 0.28905573 0.2891608  0.28922975
 0.28917247 0.28910658 0.28902483 0.28895456 0.2890232  0.2891607
 0.28914988 0.28888258 0.28856394 0.28847563 0.28863212 0.28879324
 0.28879714 0.28872725 0.28881493 0.2890743  0.289277   0.28920397
 0.28906402 0.28904042 0.2891633  0.2893742  0.28961542 0.28997168
 0.29016095 0.29018208 0.2901867  0.2902478  0.2904546  0.2907022
 0.29071712 0.29040036 0.28999326 0.28982273 0.28990382 0.28996632
 0.28987387 0.2898161  0.29004344 0.29045817 0.2907486  0.29076385
 0.29067257 0.2907081  0.29088604 0.29100972 0.2910104  0.29119682
 0.29134536 0.29097557 0.29092294 0.29114786 0.29134262 0.29130325
 0.29095027 0.29044285 0.29021037 0.29054683 0.29113153 0.2913116
 0.2910287  0.2909012  0.2913823  0.29193726 0.29171583 0.29094487
 0.2906317  0.29094204 0.2901964  0.2874805  0.28667602 0.29330078]
