Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j96_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j96_H8_FITS_custom_ftM_sl180_ll48_pl96_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 12005
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=74, out_features=113, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  922629632.0
params:  8475.0
Trainable parameters:  8475
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 98.40229225158691
Epoch: 1, Steps: 93 | Train Loss: 1.0224243 Vali Loss: 1.0261096 Test Loss: 1.2175446
Validation loss decreased (inf --> 1.026110).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 91.35039901733398
Epoch: 2, Steps: 93 | Train Loss: 0.6605328 Vali Loss: 0.7805907 Test Loss: 0.9379571
Validation loss decreased (1.026110 --> 0.780591).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 89.36655330657959
Epoch: 3, Steps: 93 | Train Loss: 0.4993351 Vali Loss: 0.6714049 Test Loss: 0.8147869
Validation loss decreased (0.780591 --> 0.671405).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 92.79823613166809
Epoch: 4, Steps: 93 | Train Loss: 0.4110750 Vali Loss: 0.6159726 Test Loss: 0.7486114
Validation loss decreased (0.671405 --> 0.615973).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 78.52535223960876
Epoch: 5, Steps: 93 | Train Loss: 0.3560738 Vali Loss: 0.5787079 Test Loss: 0.7061337
Validation loss decreased (0.615973 --> 0.578708).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 85.57294964790344
Epoch: 6, Steps: 93 | Train Loss: 0.3175742 Vali Loss: 0.5525944 Test Loss: 0.6747930
Validation loss decreased (0.578708 --> 0.552594).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 76.21272373199463
Epoch: 7, Steps: 93 | Train Loss: 0.2885617 Vali Loss: 0.5314429 Test Loss: 0.6497695
Validation loss decreased (0.552594 --> 0.531443).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 83.23669600486755
Epoch: 8, Steps: 93 | Train Loss: 0.2656179 Vali Loss: 0.5144137 Test Loss: 0.6293381
Validation loss decreased (0.531443 --> 0.514414).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 85.656254529953
Epoch: 9, Steps: 93 | Train Loss: 0.2468833 Vali Loss: 0.5019518 Test Loss: 0.6123613
Validation loss decreased (0.514414 --> 0.501952).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 81.73540759086609
Epoch: 10, Steps: 93 | Train Loss: 0.2312947 Vali Loss: 0.4881858 Test Loss: 0.5964839
Validation loss decreased (0.501952 --> 0.488186).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 86.20378422737122
Epoch: 11, Steps: 93 | Train Loss: 0.2180238 Vali Loss: 0.4783723 Test Loss: 0.5831387
Validation loss decreased (0.488186 --> 0.478372).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 87.04464077949524
Epoch: 12, Steps: 93 | Train Loss: 0.2067200 Vali Loss: 0.4684191 Test Loss: 0.5719700
Validation loss decreased (0.478372 --> 0.468419).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 89.84297752380371
Epoch: 13, Steps: 93 | Train Loss: 0.1969972 Vali Loss: 0.4605044 Test Loss: 0.5620837
Validation loss decreased (0.468419 --> 0.460504).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 82.67987656593323
Epoch: 14, Steps: 93 | Train Loss: 0.1885610 Vali Loss: 0.4513212 Test Loss: 0.5528753
Validation loss decreased (0.460504 --> 0.451321).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 87.02676320075989
Epoch: 15, Steps: 93 | Train Loss: 0.1812750 Vali Loss: 0.4472583 Test Loss: 0.5451717
Validation loss decreased (0.451321 --> 0.447258).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 84.56920170783997
Epoch: 16, Steps: 93 | Train Loss: 0.1748427 Vali Loss: 0.4409715 Test Loss: 0.5384859
Validation loss decreased (0.447258 --> 0.440972).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 92.25488662719727
Epoch: 17, Steps: 93 | Train Loss: 0.1692452 Vali Loss: 0.4368227 Test Loss: 0.5323090
Validation loss decreased (0.440972 --> 0.436823).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 89.81095957756042
Epoch: 18, Steps: 93 | Train Loss: 0.1641628 Vali Loss: 0.4324276 Test Loss: 0.5269143
Validation loss decreased (0.436823 --> 0.432428).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 78.66293001174927
Epoch: 19, Steps: 93 | Train Loss: 0.1597960 Vali Loss: 0.4273582 Test Loss: 0.5221447
Validation loss decreased (0.432428 --> 0.427358).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 85.01352787017822
Epoch: 20, Steps: 93 | Train Loss: 0.1558224 Vali Loss: 0.4252894 Test Loss: 0.5177338
Validation loss decreased (0.427358 --> 0.425289).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 80.7472574710846
Epoch: 21, Steps: 93 | Train Loss: 0.1523104 Vali Loss: 0.4215039 Test Loss: 0.5138354
Validation loss decreased (0.425289 --> 0.421504).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 81.05354499816895
Epoch: 22, Steps: 93 | Train Loss: 0.1491854 Vali Loss: 0.4193376 Test Loss: 0.5103497
Validation loss decreased (0.421504 --> 0.419338).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 79.74701690673828
Epoch: 23, Steps: 93 | Train Loss: 0.1463700 Vali Loss: 0.4177977 Test Loss: 0.5070192
Validation loss decreased (0.419338 --> 0.417798).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 85.98101019859314
Epoch: 24, Steps: 93 | Train Loss: 0.1437909 Vali Loss: 0.4127069 Test Loss: 0.5042653
Validation loss decreased (0.417798 --> 0.412707).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 81.01514911651611
Epoch: 25, Steps: 93 | Train Loss: 0.1414912 Vali Loss: 0.4111142 Test Loss: 0.5016738
Validation loss decreased (0.412707 --> 0.411114).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 73.90931630134583
Epoch: 26, Steps: 93 | Train Loss: 0.1393327 Vali Loss: 0.4093375 Test Loss: 0.4994781
Validation loss decreased (0.411114 --> 0.409338).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 82.10053658485413
Epoch: 27, Steps: 93 | Train Loss: 0.1374590 Vali Loss: 0.4088437 Test Loss: 0.4972286
Validation loss decreased (0.409338 --> 0.408844).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 78.65358138084412
Epoch: 28, Steps: 93 | Train Loss: 0.1357543 Vali Loss: 0.4064542 Test Loss: 0.4953879
Validation loss decreased (0.408844 --> 0.406454).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 86.42261505126953
Epoch: 29, Steps: 93 | Train Loss: 0.1341567 Vali Loss: 0.4041384 Test Loss: 0.4934120
Validation loss decreased (0.406454 --> 0.404138).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 87.56179547309875
Epoch: 30, Steps: 93 | Train Loss: 0.1326780 Vali Loss: 0.4045148 Test Loss: 0.4918152
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 85.60745477676392
Epoch: 31, Steps: 93 | Train Loss: 0.1313578 Vali Loss: 0.4043747 Test Loss: 0.4902956
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 79.322181224823
Epoch: 32, Steps: 93 | Train Loss: 0.1301165 Vali Loss: 0.4021658 Test Loss: 0.4889350
Validation loss decreased (0.404138 --> 0.402166).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 82.58249592781067
Epoch: 33, Steps: 93 | Train Loss: 0.1290233 Vali Loss: 0.4016224 Test Loss: 0.4876075
Validation loss decreased (0.402166 --> 0.401622).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 80.78368139266968
Epoch: 34, Steps: 93 | Train Loss: 0.1280406 Vali Loss: 0.4003882 Test Loss: 0.4865024
Validation loss decreased (0.401622 --> 0.400388).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 81.56606316566467
Epoch: 35, Steps: 93 | Train Loss: 0.1270244 Vali Loss: 0.3989576 Test Loss: 0.4854420
Validation loss decreased (0.400388 --> 0.398958).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 80.79745554924011
Epoch: 36, Steps: 93 | Train Loss: 0.1261542 Vali Loss: 0.3980061 Test Loss: 0.4844101
Validation loss decreased (0.398958 --> 0.398006).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 80.2622618675232
Epoch: 37, Steps: 93 | Train Loss: 0.1253087 Vali Loss: 0.3975320 Test Loss: 0.4835248
Validation loss decreased (0.398006 --> 0.397532).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 87.91255402565002
Epoch: 38, Steps: 93 | Train Loss: 0.1245743 Vali Loss: 0.3981970 Test Loss: 0.4826843
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 69.74677324295044
Epoch: 39, Steps: 93 | Train Loss: 0.1238612 Vali Loss: 0.3955876 Test Loss: 0.4818774
Validation loss decreased (0.397532 --> 0.395588).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 72.98407053947449
Epoch: 40, Steps: 93 | Train Loss: 0.1232046 Vali Loss: 0.3947071 Test Loss: 0.4811256
Validation loss decreased (0.395588 --> 0.394707).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 67.46184968948364
Epoch: 41, Steps: 93 | Train Loss: 0.1225658 Vali Loss: 0.3949866 Test Loss: 0.4803631
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 73.92894005775452
Epoch: 42, Steps: 93 | Train Loss: 0.1220132 Vali Loss: 0.3949827 Test Loss: 0.4798229
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 76.55551314353943
Epoch: 43, Steps: 93 | Train Loss: 0.1215118 Vali Loss: 0.3954230 Test Loss: 0.4791869
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 78.15780520439148
Epoch: 44, Steps: 93 | Train Loss: 0.1209795 Vali Loss: 0.3931199 Test Loss: 0.4785238
Validation loss decreased (0.394707 --> 0.393120).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 81.40974926948547
Epoch: 45, Steps: 93 | Train Loss: 0.1205623 Vali Loss: 0.3936777 Test Loss: 0.4780173
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 74.10248923301697
Epoch: 46, Steps: 93 | Train Loss: 0.1201384 Vali Loss: 0.3932334 Test Loss: 0.4775602
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 85.01185822486877
Epoch: 47, Steps: 93 | Train Loss: 0.1196714 Vali Loss: 0.3929094 Test Loss: 0.4770766
Validation loss decreased (0.393120 --> 0.392909).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 81.75604915618896
Epoch: 48, Steps: 93 | Train Loss: 0.1193001 Vali Loss: 0.3930786 Test Loss: 0.4766740
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 87.86650896072388
Epoch: 49, Steps: 93 | Train Loss: 0.1188930 Vali Loss: 0.3916438 Test Loss: 0.4762198
Validation loss decreased (0.392909 --> 0.391644).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 86.51859307289124
Epoch: 50, Steps: 93 | Train Loss: 0.1186098 Vali Loss: 0.3928127 Test Loss: 0.4758362
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 87.34166216850281
Epoch: 51, Steps: 93 | Train Loss: 0.1182744 Vali Loss: 0.3902128 Test Loss: 0.4754660
Validation loss decreased (0.391644 --> 0.390213).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 87.10906052589417
Epoch: 52, Steps: 93 | Train Loss: 0.1180209 Vali Loss: 0.3921176 Test Loss: 0.4751556
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 83.29179167747498
Epoch: 53, Steps: 93 | Train Loss: 0.1177544 Vali Loss: 0.3905174 Test Loss: 0.4748193
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 87.16919898986816
Epoch: 54, Steps: 93 | Train Loss: 0.1174089 Vali Loss: 0.3905702 Test Loss: 0.4745159
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 87.02860045433044
Epoch: 55, Steps: 93 | Train Loss: 0.1172075 Vali Loss: 0.3900589 Test Loss: 0.4742622
Validation loss decreased (0.390213 --> 0.390059).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 92.63414192199707
Epoch: 56, Steps: 93 | Train Loss: 0.1169335 Vali Loss: 0.3889852 Test Loss: 0.4739460
Validation loss decreased (0.390059 --> 0.388985).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 81.91476941108704
Epoch: 57, Steps: 93 | Train Loss: 0.1167449 Vali Loss: 0.3910064 Test Loss: 0.4737134
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 84.7324492931366
Epoch: 58, Steps: 93 | Train Loss: 0.1165604 Vali Loss: 0.3901303 Test Loss: 0.4734235
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 77.318284034729
Epoch: 59, Steps: 93 | Train Loss: 0.1163536 Vali Loss: 0.3898487 Test Loss: 0.4732335
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 80.91212034225464
Epoch: 60, Steps: 93 | Train Loss: 0.1162024 Vali Loss: 0.3897661 Test Loss: 0.4730577
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 77.34714221954346
Epoch: 61, Steps: 93 | Train Loss: 0.1159490 Vali Loss: 0.3892203 Test Loss: 0.4728244
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 83.06518888473511
Epoch: 62, Steps: 93 | Train Loss: 0.1158272 Vali Loss: 0.3887272 Test Loss: 0.4726095
Validation loss decreased (0.388985 --> 0.388727).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 78.75964164733887
Epoch: 63, Steps: 93 | Train Loss: 0.1156225 Vali Loss: 0.3902791 Test Loss: 0.4724498
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 76.61704206466675
Epoch: 64, Steps: 93 | Train Loss: 0.1155552 Vali Loss: 0.3891494 Test Loss: 0.4722596
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 74.04338836669922
Epoch: 65, Steps: 93 | Train Loss: 0.1153808 Vali Loss: 0.3892477 Test Loss: 0.4721227
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 82.73770236968994
Epoch: 66, Steps: 93 | Train Loss: 0.1152172 Vali Loss: 0.3887325 Test Loss: 0.4719691
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 75.96709513664246
Epoch: 67, Steps: 93 | Train Loss: 0.1151181 Vali Loss: 0.3894449 Test Loss: 0.4718115
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 75.3953948020935
Epoch: 68, Steps: 93 | Train Loss: 0.1149918 Vali Loss: 0.3884146 Test Loss: 0.4716654
Validation loss decreased (0.388727 --> 0.388415).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 78.21792840957642
Epoch: 69, Steps: 93 | Train Loss: 0.1148791 Vali Loss: 0.3883399 Test Loss: 0.4715492
Validation loss decreased (0.388415 --> 0.388340).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 74.53355669975281
Epoch: 70, Steps: 93 | Train Loss: 0.1147561 Vali Loss: 0.3881446 Test Loss: 0.4714031
Validation loss decreased (0.388340 --> 0.388145).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 69.50492453575134
Epoch: 71, Steps: 93 | Train Loss: 0.1146335 Vali Loss: 0.3881740 Test Loss: 0.4712995
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 66.39357209205627
Epoch: 72, Steps: 93 | Train Loss: 0.1145392 Vali Loss: 0.3880615 Test Loss: 0.4711970
Validation loss decreased (0.388145 --> 0.388062).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 67.77477836608887
Epoch: 73, Steps: 93 | Train Loss: 0.1144646 Vali Loss: 0.3868169 Test Loss: 0.4710906
Validation loss decreased (0.388062 --> 0.386817).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 65.09122014045715
Epoch: 74, Steps: 93 | Train Loss: 0.1143675 Vali Loss: 0.3876594 Test Loss: 0.4709905
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 69.09574365615845
Epoch: 75, Steps: 93 | Train Loss: 0.1143520 Vali Loss: 0.3879186 Test Loss: 0.4708853
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 68.72844934463501
Epoch: 76, Steps: 93 | Train Loss: 0.1142334 Vali Loss: 0.3883004 Test Loss: 0.4707946
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 61.6042001247406
Epoch: 77, Steps: 93 | Train Loss: 0.1141180 Vali Loss: 0.3879379 Test Loss: 0.4706911
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 63.322330951690674
Epoch: 78, Steps: 93 | Train Loss: 0.1140459 Vali Loss: 0.3871754 Test Loss: 0.4706314
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 50.35980200767517
Epoch: 79, Steps: 93 | Train Loss: 0.1140132 Vali Loss: 0.3876657 Test Loss: 0.4705311
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 50.496546506881714
Epoch: 80, Steps: 93 | Train Loss: 0.1138963 Vali Loss: 0.3869085 Test Loss: 0.4704649
EarlyStopping counter: 7 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 45.11752510070801
Epoch: 81, Steps: 93 | Train Loss: 0.1139126 Vali Loss: 0.3883494 Test Loss: 0.4703991
EarlyStopping counter: 8 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 47.890382528305054
Epoch: 82, Steps: 93 | Train Loss: 0.1138765 Vali Loss: 0.3878082 Test Loss: 0.4703266
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 58.47738552093506
Epoch: 83, Steps: 93 | Train Loss: 0.1137660 Vali Loss: 0.3883265 Test Loss: 0.4702643
EarlyStopping counter: 10 out of 10
Early stopping
train 12005
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=74, out_features=113, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  922629632.0
params:  8475.0
Trainable parameters:  8475
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 63.03275537490845
Epoch: 1, Steps: 93 | Train Loss: 0.2799388 Vali Loss: 0.3753889 Test Loss: 0.4552112
Validation loss decreased (inf --> 0.375389).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 66.64913177490234
Epoch: 2, Steps: 93 | Train Loss: 0.2750425 Vali Loss: 0.3735449 Test Loss: 0.4537305
Validation loss decreased (0.375389 --> 0.373545).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 60.567808628082275
Epoch: 3, Steps: 93 | Train Loss: 0.2743602 Vali Loss: 0.3739597 Test Loss: 0.4528878
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00045125
Epoch: 4 cost time: 69.04831862449646
Epoch: 4, Steps: 93 | Train Loss: 0.2739859 Vali Loss: 0.3727863 Test Loss: 0.4525709
Validation loss decreased (0.373545 --> 0.372786).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 66.67309784889221
Epoch: 5, Steps: 93 | Train Loss: 0.2738603 Vali Loss: 0.3726254 Test Loss: 0.4522457
Validation loss decreased (0.372786 --> 0.372625).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 60.2978994846344
Epoch: 6, Steps: 93 | Train Loss: 0.2739603 Vali Loss: 0.3722487 Test Loss: 0.4522367
Validation loss decreased (0.372625 --> 0.372249).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 72.11705923080444
Epoch: 7, Steps: 93 | Train Loss: 0.2737632 Vali Loss: 0.3722256 Test Loss: 0.4520025
Validation loss decreased (0.372249 --> 0.372226).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 95.84660482406616
Epoch: 8, Steps: 93 | Train Loss: 0.2735794 Vali Loss: 0.3730318 Test Loss: 0.4522160
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 99.41499924659729
Epoch: 9, Steps: 93 | Train Loss: 0.2735419 Vali Loss: 0.3718272 Test Loss: 0.4518833
Validation loss decreased (0.372226 --> 0.371827).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 103.84039759635925
Epoch: 10, Steps: 93 | Train Loss: 0.2735901 Vali Loss: 0.3728983 Test Loss: 0.4520414
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 96.39441132545471
Epoch: 11, Steps: 93 | Train Loss: 0.2735189 Vali Loss: 0.3723448 Test Loss: 0.4518257
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 97.17269945144653
Epoch: 12, Steps: 93 | Train Loss: 0.2734367 Vali Loss: 0.3728942 Test Loss: 0.4518870
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 89.4056179523468
Epoch: 13, Steps: 93 | Train Loss: 0.2734783 Vali Loss: 0.3718316 Test Loss: 0.4519394
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 88.50280928611755
Epoch: 14, Steps: 93 | Train Loss: 0.2734531 Vali Loss: 0.3728550 Test Loss: 0.4517800
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 82.54311442375183
Epoch: 15, Steps: 93 | Train Loss: 0.2734654 Vali Loss: 0.3715076 Test Loss: 0.4517191
Validation loss decreased (0.371827 --> 0.371508).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 93.40587210655212
Epoch: 16, Steps: 93 | Train Loss: 0.2734204 Vali Loss: 0.3722563 Test Loss: 0.4516322
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 93.59054350852966
Epoch: 17, Steps: 93 | Train Loss: 0.2733183 Vali Loss: 0.3725762 Test Loss: 0.4517291
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 94.64827561378479
Epoch: 18, Steps: 93 | Train Loss: 0.2733358 Vali Loss: 0.3745691 Test Loss: 0.4517979
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 87.06806802749634
Epoch: 19, Steps: 93 | Train Loss: 0.2734362 Vali Loss: 0.3723348 Test Loss: 0.4516310
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 83.75549483299255
Epoch: 20, Steps: 93 | Train Loss: 0.2733276 Vali Loss: 0.3715241 Test Loss: 0.4516342
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 88.20369005203247
Epoch: 21, Steps: 93 | Train Loss: 0.2732915 Vali Loss: 0.3715876 Test Loss: 0.4516594
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 87.79012060165405
Epoch: 22, Steps: 93 | Train Loss: 0.2732449 Vali Loss: 0.3735940 Test Loss: 0.4516947
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 92.58338975906372
Epoch: 23, Steps: 93 | Train Loss: 0.2733454 Vali Loss: 0.3713205 Test Loss: 0.4517029
Validation loss decreased (0.371508 --> 0.371321).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 74.26172184944153
Epoch: 24, Steps: 93 | Train Loss: 0.2732555 Vali Loss: 0.3723083 Test Loss: 0.4515920
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 94.50823140144348
Epoch: 25, Steps: 93 | Train Loss: 0.2732885 Vali Loss: 0.3726469 Test Loss: 0.4516565
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 90.93747329711914
Epoch: 26, Steps: 93 | Train Loss: 0.2732187 Vali Loss: 0.3721977 Test Loss: 0.4516608
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 89.36944198608398
Epoch: 27, Steps: 93 | Train Loss: 0.2732869 Vali Loss: 0.3717559 Test Loss: 0.4515167
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 90.98610496520996
Epoch: 28, Steps: 93 | Train Loss: 0.2732570 Vali Loss: 0.3719178 Test Loss: 0.4516342
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 92.31325054168701
Epoch: 29, Steps: 93 | Train Loss: 0.2733521 Vali Loss: 0.3726942 Test Loss: 0.4515287
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 85.56790232658386
Epoch: 30, Steps: 93 | Train Loss: 0.2732244 Vali Loss: 0.3725423 Test Loss: 0.4515346
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 96.32317399978638
Epoch: 31, Steps: 93 | Train Loss: 0.2733356 Vali Loss: 0.3728697 Test Loss: 0.4514825
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 84.39205598831177
Epoch: 32, Steps: 93 | Train Loss: 0.2733479 Vali Loss: 0.3729739 Test Loss: 0.4515290
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 81.91435742378235
Epoch: 33, Steps: 93 | Train Loss: 0.2731242 Vali Loss: 0.3734816 Test Loss: 0.4515543
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j96_H8_FITS_custom_ftM_sl180_ll48_pl96_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.4514695107936859, mae:0.2980884611606598, rse:0.5563750863075256, corr:[0.2753763  0.29131252 0.2915014  0.29059264 0.29044962 0.2898511
 0.29031608 0.2895867  0.28967494 0.28987834 0.2898768  0.2899542
 0.28964123 0.2889335  0.28898364 0.28885782 0.28914505 0.28892168
 0.2888479  0.28911597 0.28939775 0.28960016 0.2906367  0.29058844
 0.28981942 0.28978938 0.28946626 0.28912136 0.28884634 0.28914556
 0.2896599  0.2896588  0.2895568  0.28970996 0.28957152 0.2889855
 0.2883258  0.28792804 0.28829238 0.28848037 0.28880683 0.28918496
 0.28921816 0.28888902 0.2888589  0.2890577  0.289351   0.28912866
 0.28850248 0.28850967 0.28842926 0.2884118  0.28848982 0.2887799
 0.28908116 0.28863648 0.28841    0.28874832 0.28878343 0.2887125
 0.28869382 0.28814793 0.28836548 0.28830943 0.2880645  0.2883206
 0.28827795 0.28806165 0.2881494  0.2883051  0.28832638 0.2882349
 0.28762463 0.287798   0.2878248  0.28784224 0.28808525 0.28786263
 0.2878967  0.2878544  0.28756115 0.28797755 0.28835043 0.28812945
 0.2882988  0.28763708 0.28752655 0.28757685 0.2873207  0.28728417
 0.28702858 0.28685433 0.2863966  0.28637913 0.2866944  0.28785944]
