Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j96_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j96_H8_FITS_custom_ftM_sl180_ll48_pl96_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 12005
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=74, out_features=113, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  922629632.0
params:  8475.0
Trainable parameters:  8475
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 82.23976850509644
Epoch: 1, Steps: 93 | Train Loss: 0.8261189 Vali Loss: 0.6763046 Test Loss: 0.8065332
Validation loss decreased (inf --> 0.676305).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 73.92741703987122
Epoch: 2, Steps: 93 | Train Loss: 0.4467121 Vali Loss: 0.5086131 Test Loss: 0.6088234
Validation loss decreased (0.676305 --> 0.508613).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 80.60290217399597
Epoch: 3, Steps: 93 | Train Loss: 0.3599811 Vali Loss: 0.4473293 Test Loss: 0.5345362
Validation loss decreased (0.508613 --> 0.447329).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 74.78946733474731
Epoch: 4, Steps: 93 | Train Loss: 0.3216360 Vali Loss: 0.4162942 Test Loss: 0.4977291
Validation loss decreased (0.447329 --> 0.416294).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 73.15981411933899
Epoch: 5, Steps: 93 | Train Loss: 0.3017745 Vali Loss: 0.3987387 Test Loss: 0.4785774
Validation loss decreased (0.416294 --> 0.398739).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 66.11866664886475
Epoch: 6, Steps: 93 | Train Loss: 0.2909464 Vali Loss: 0.3896608 Test Loss: 0.4681351
Validation loss decreased (0.398739 --> 0.389661).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 70.20347094535828
Epoch: 7, Steps: 93 | Train Loss: 0.2847551 Vali Loss: 0.3840846 Test Loss: 0.4623211
Validation loss decreased (0.389661 --> 0.384085).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 67.09358263015747
Epoch: 8, Steps: 93 | Train Loss: 0.2809159 Vali Loss: 0.3794868 Test Loss: 0.4588440
Validation loss decreased (0.384085 --> 0.379487).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 74.74129581451416
Epoch: 9, Steps: 93 | Train Loss: 0.2787319 Vali Loss: 0.3787520 Test Loss: 0.4567213
Validation loss decreased (0.379487 --> 0.378752).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 67.24999117851257
Epoch: 10, Steps: 93 | Train Loss: 0.2772777 Vali Loss: 0.3762442 Test Loss: 0.4552940
Validation loss decreased (0.378752 --> 0.376244).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 58.59892010688782
Epoch: 11, Steps: 93 | Train Loss: 0.2762316 Vali Loss: 0.3765346 Test Loss: 0.4544297
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 50.118547439575195
Epoch: 12, Steps: 93 | Train Loss: 0.2756418 Vali Loss: 0.3744379 Test Loss: 0.4538426
Validation loss decreased (0.376244 --> 0.374438).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 58.62071704864502
Epoch: 13, Steps: 93 | Train Loss: 0.2751197 Vali Loss: 0.3741588 Test Loss: 0.4533911
Validation loss decreased (0.374438 --> 0.374159).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 62.57151222229004
Epoch: 14, Steps: 93 | Train Loss: 0.2747517 Vali Loss: 0.3720535 Test Loss: 0.4531970
Validation loss decreased (0.374159 --> 0.372054).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 59.930887937545776
Epoch: 15, Steps: 93 | Train Loss: 0.2745735 Vali Loss: 0.3739212 Test Loss: 0.4529391
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 51.21970558166504
Epoch: 16, Steps: 93 | Train Loss: 0.2744368 Vali Loss: 0.3731350 Test Loss: 0.4528284
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 55.573904275894165
Epoch: 17, Steps: 93 | Train Loss: 0.2744135 Vali Loss: 0.3738540 Test Loss: 0.4526741
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 49.512229681015015
Epoch: 18, Steps: 93 | Train Loss: 0.2740620 Vali Loss: 0.3734123 Test Loss: 0.4526496
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 45.596272468566895
Epoch: 19, Steps: 93 | Train Loss: 0.2741205 Vali Loss: 0.3720311 Test Loss: 0.4525303
Validation loss decreased (0.372054 --> 0.372031).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 88.00131559371948
Epoch: 20, Steps: 93 | Train Loss: 0.2738834 Vali Loss: 0.3732327 Test Loss: 0.4524462
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 82.56193280220032
Epoch: 21, Steps: 93 | Train Loss: 0.2739190 Vali Loss: 0.3731425 Test Loss: 0.4524281
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 82.0738959312439
Epoch: 22, Steps: 93 | Train Loss: 0.2739658 Vali Loss: 0.3735930 Test Loss: 0.4523676
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 82.24773144721985
Epoch: 23, Steps: 93 | Train Loss: 0.2738860 Vali Loss: 0.3748115 Test Loss: 0.4523893
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 83.87971663475037
Epoch: 24, Steps: 93 | Train Loss: 0.2738446 Vali Loss: 0.3714188 Test Loss: 0.4523335
Validation loss decreased (0.372031 --> 0.371419).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 80.3764009475708
Epoch: 25, Steps: 93 | Train Loss: 0.2738018 Vali Loss: 0.3720821 Test Loss: 0.4523353
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 76.22760772705078
Epoch: 26, Steps: 93 | Train Loss: 0.2737107 Vali Loss: 0.3718779 Test Loss: 0.4522355
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 70.43238568305969
Epoch: 27, Steps: 93 | Train Loss: 0.2737054 Vali Loss: 0.3730873 Test Loss: 0.4522140
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 77.83463740348816
Epoch: 28, Steps: 93 | Train Loss: 0.2737041 Vali Loss: 0.3719859 Test Loss: 0.4521678
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 74.61281776428223
Epoch: 29, Steps: 93 | Train Loss: 0.2736378 Vali Loss: 0.3712325 Test Loss: 0.4522625
Validation loss decreased (0.371419 --> 0.371233).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 76.42594599723816
Epoch: 30, Steps: 93 | Train Loss: 0.2735952 Vali Loss: 0.3730006 Test Loss: 0.4521954
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 76.72438383102417
Epoch: 31, Steps: 93 | Train Loss: 0.2735890 Vali Loss: 0.3740443 Test Loss: 0.4521648
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 71.75629210472107
Epoch: 32, Steps: 93 | Train Loss: 0.2735380 Vali Loss: 0.3730560 Test Loss: 0.4521824
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 75.09373593330383
Epoch: 33, Steps: 93 | Train Loss: 0.2736094 Vali Loss: 0.3735385 Test Loss: 0.4521366
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 75.46733045578003
Epoch: 34, Steps: 93 | Train Loss: 0.2736608 Vali Loss: 0.3727770 Test Loss: 0.4521234
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 75.7359139919281
Epoch: 35, Steps: 93 | Train Loss: 0.2735121 Vali Loss: 0.3726706 Test Loss: 0.4521514
EarlyStopping counter: 6 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 76.38424396514893
Epoch: 36, Steps: 93 | Train Loss: 0.2736075 Vali Loss: 0.3722154 Test Loss: 0.4520739
EarlyStopping counter: 7 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 74.90592741966248
Epoch: 37, Steps: 93 | Train Loss: 0.2735049 Vali Loss: 0.3727473 Test Loss: 0.4520753
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 76.59807920455933
Epoch: 38, Steps: 93 | Train Loss: 0.2735508 Vali Loss: 0.3737394 Test Loss: 0.4520739
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 77.24852132797241
Epoch: 39, Steps: 93 | Train Loss: 0.2735265 Vali Loss: 0.3717050 Test Loss: 0.4520009
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j96_H8_FITS_custom_ftM_sl180_ll48_pl96_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.45202380418777466, mae:0.29935529828071594, rse:0.5567165613174438, corr:[0.2785613  0.2903039  0.29022917 0.28817517 0.29101452 0.28965136
 0.29124868 0.29083458 0.29015568 0.29092985 0.29020324 0.28982237
 0.28981397 0.28889826 0.28917256 0.288681   0.28826827 0.2885294
 0.28846586 0.28919044 0.2894264  0.2895917  0.29069588 0.29029804
 0.29010534 0.2901024  0.28919134 0.28954074 0.2896394  0.2894647
 0.28998187 0.28974912 0.28965697 0.2898083  0.28947383 0.28887615
 0.28823388 0.28806984 0.28838348 0.2881897  0.28843796 0.28881544
 0.28866094 0.28900498 0.28907323 0.2891328  0.28985497 0.28925428
 0.28886193 0.28905818 0.28827026 0.28862    0.28880137 0.28860676
 0.28917834 0.28885743 0.28855664 0.2884642  0.2882077  0.28839326
 0.28832322 0.288012   0.2885553  0.2883221  0.28828314 0.288541
 0.28809395 0.28848284 0.28843555 0.288115   0.28880805 0.28835097
 0.2875602  0.28837946 0.28788784 0.28754723 0.28759181 0.28745228
 0.28804582 0.28732625 0.2871577  0.2876918  0.28764424 0.28831774
 0.28821802 0.2876024  0.28844365 0.2876315  0.28784418 0.2877583
 0.28642946 0.28723717 0.28507093 0.28620657 0.28719005 0.2888917 ]
