Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j336_H5_FITS_custom_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4387511040.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 105.61004853248596
Epoch: 1, Steps: 87 | Train Loss: 0.8830347 Vali Loss: 0.7670490 Test Loss: 0.8811067
Validation loss decreased (inf --> 0.767049).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 110.74031186103821
Epoch: 2, Steps: 87 | Train Loss: 0.5032688 Vali Loss: 0.5465044 Test Loss: 0.6316547
Validation loss decreased (0.767049 --> 0.546504).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 110.2321195602417
Epoch: 3, Steps: 87 | Train Loss: 0.3707424 Vali Loss: 0.4428075 Test Loss: 0.5175101
Validation loss decreased (0.546504 --> 0.442807).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 111.71551752090454
Epoch: 4, Steps: 87 | Train Loss: 0.3075282 Vali Loss: 0.3929361 Test Loss: 0.4639447
Validation loss decreased (0.442807 --> 0.392936).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 115.98598527908325
Epoch: 5, Steps: 87 | Train Loss: 0.2780577 Vali Loss: 0.3696687 Test Loss: 0.4400008
Validation loss decreased (0.392936 --> 0.369669).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 114.95035886764526
Epoch: 6, Steps: 87 | Train Loss: 0.2647565 Vali Loss: 0.3589728 Test Loss: 0.4300348
Validation loss decreased (0.369669 --> 0.358973).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 115.75892353057861
Epoch: 7, Steps: 87 | Train Loss: 0.2588943 Vali Loss: 0.3543128 Test Loss: 0.4259920
Validation loss decreased (0.358973 --> 0.354313).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 127.63979983329773
Epoch: 8, Steps: 87 | Train Loss: 0.2563131 Vali Loss: 0.3518174 Test Loss: 0.4242113
Validation loss decreased (0.354313 --> 0.351817).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 131.5086658000946
Epoch: 9, Steps: 87 | Train Loss: 0.2552459 Vali Loss: 0.3503191 Test Loss: 0.4234639
Validation loss decreased (0.351817 --> 0.350319).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 139.15425205230713
Epoch: 10, Steps: 87 | Train Loss: 0.2547378 Vali Loss: 0.3498795 Test Loss: 0.4232655
Validation loss decreased (0.350319 --> 0.349880).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 146.78728222846985
Epoch: 11, Steps: 87 | Train Loss: 0.2543952 Vali Loss: 0.3489736 Test Loss: 0.4231046
Validation loss decreased (0.349880 --> 0.348974).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 137.99429368972778
Epoch: 12, Steps: 87 | Train Loss: 0.2541934 Vali Loss: 0.3486861 Test Loss: 0.4228959
Validation loss decreased (0.348974 --> 0.348686).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 137.17430996894836
Epoch: 13, Steps: 87 | Train Loss: 0.2541224 Vali Loss: 0.3485525 Test Loss: 0.4228858
Validation loss decreased (0.348686 --> 0.348552).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 138.87818121910095
Epoch: 14, Steps: 87 | Train Loss: 0.2540499 Vali Loss: 0.3484323 Test Loss: 0.4227833
Validation loss decreased (0.348552 --> 0.348432).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 142.92602849006653
Epoch: 15, Steps: 87 | Train Loss: 0.2539194 Vali Loss: 0.3484594 Test Loss: 0.4226671
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 143.22135496139526
Epoch: 16, Steps: 87 | Train Loss: 0.2539680 Vali Loss: 0.3485245 Test Loss: 0.4229590
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 142.80974316596985
Epoch: 17, Steps: 87 | Train Loss: 0.2538692 Vali Loss: 0.3478865 Test Loss: 0.4226715
Validation loss decreased (0.348432 --> 0.347887).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 140.82938742637634
Epoch: 18, Steps: 87 | Train Loss: 0.2538661 Vali Loss: 0.3480826 Test Loss: 0.4227725
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 139.3042595386505
Epoch: 19, Steps: 87 | Train Loss: 0.2538670 Vali Loss: 0.3476343 Test Loss: 0.4225160
Validation loss decreased (0.347887 --> 0.347634).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 145.09841442108154
Epoch: 20, Steps: 87 | Train Loss: 0.2538494 Vali Loss: 0.3478041 Test Loss: 0.4226280
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 143.07364296913147
Epoch: 21, Steps: 87 | Train Loss: 0.2537770 Vali Loss: 0.3477989 Test Loss: 0.4227089
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 139.62845087051392
Epoch: 22, Steps: 87 | Train Loss: 0.2536865 Vali Loss: 0.3477430 Test Loss: 0.4226709
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 145.94291758537292
Epoch: 23, Steps: 87 | Train Loss: 0.2537052 Vali Loss: 0.3479068 Test Loss: 0.4224299
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 143.9785258769989
Epoch: 24, Steps: 87 | Train Loss: 0.2536597 Vali Loss: 0.3478363 Test Loss: 0.4223978
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 124.48374056816101
Epoch: 25, Steps: 87 | Train Loss: 0.2537001 Vali Loss: 0.3474491 Test Loss: 0.4225503
Validation loss decreased (0.347634 --> 0.347449).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 112.18453431129456
Epoch: 26, Steps: 87 | Train Loss: 0.2536169 Vali Loss: 0.3475591 Test Loss: 0.4224205
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 111.6907410621643
Epoch: 27, Steps: 87 | Train Loss: 0.2536181 Vali Loss: 0.3474803 Test Loss: 0.4224423
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 102.1519238948822
Epoch: 28, Steps: 87 | Train Loss: 0.2536964 Vali Loss: 0.3476105 Test Loss: 0.4225352
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 92.19808340072632
Epoch: 29, Steps: 87 | Train Loss: 0.2535883 Vali Loss: 0.3478216 Test Loss: 0.4223171
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 89.69946527481079
Epoch: 30, Steps: 87 | Train Loss: 0.2535811 Vali Loss: 0.3476843 Test Loss: 0.4223348
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 90.02705693244934
Epoch: 31, Steps: 87 | Train Loss: 0.2536620 Vali Loss: 0.3474785 Test Loss: 0.4224668
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 85.21436595916748
Epoch: 32, Steps: 87 | Train Loss: 0.2535983 Vali Loss: 0.3476042 Test Loss: 0.4224303
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 87.31199598312378
Epoch: 33, Steps: 87 | Train Loss: 0.2535835 Vali Loss: 0.3477027 Test Loss: 0.4224314
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 90.3049201965332
Epoch: 34, Steps: 87 | Train Loss: 0.2535681 Vali Loss: 0.3479423 Test Loss: 0.4224027
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 88.62167239189148
Epoch: 35, Steps: 87 | Train Loss: 0.2534756 Vali Loss: 0.3472632 Test Loss: 0.4223341
Validation loss decreased (0.347449 --> 0.347263).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 84.35386443138123
Epoch: 36, Steps: 87 | Train Loss: 0.2535132 Vali Loss: 0.3476951 Test Loss: 0.4222547
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 88.00101900100708
Epoch: 37, Steps: 87 | Train Loss: 0.2535880 Vali Loss: 0.3472989 Test Loss: 0.4223472
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 88.91276025772095
Epoch: 38, Steps: 87 | Train Loss: 0.2535537 Vali Loss: 0.3472854 Test Loss: 0.4222385
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 94.58612155914307
Epoch: 39, Steps: 87 | Train Loss: 0.2535163 Vali Loss: 0.3475246 Test Loss: 0.4224118
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 89.68919157981873
Epoch: 40, Steps: 87 | Train Loss: 0.2534597 Vali Loss: 0.3472914 Test Loss: 0.4223442
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 89.15598440170288
Epoch: 41, Steps: 87 | Train Loss: 0.2535070 Vali Loss: 0.3476224 Test Loss: 0.4222832
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 88.23361897468567
Epoch: 42, Steps: 87 | Train Loss: 0.2534583 Vali Loss: 0.3474509 Test Loss: 0.4223233
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 96.85093450546265
Epoch: 43, Steps: 87 | Train Loss: 0.2535055 Vali Loss: 0.3472979 Test Loss: 0.4223166
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 91.34117126464844
Epoch: 44, Steps: 87 | Train Loss: 0.2534849 Vali Loss: 0.3478070 Test Loss: 0.4223043
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 89.26942300796509
Epoch: 45, Steps: 87 | Train Loss: 0.2534354 Vali Loss: 0.3474790 Test Loss: 0.4222256
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j336_H5_FITS_custom_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.42026767134666443, mae:0.2924082577228546, rse:0.532797634601593, corr:[0.28350285 0.28533956 0.28534    0.28782237 0.28926167 0.28891104
 0.28831998 0.28850365 0.28895065 0.28868857 0.28769743 0.28684577
 0.2867338  0.28697395 0.2869183  0.28650245 0.2862853  0.28666005
 0.2873454  0.28777102 0.28766778 0.2872963  0.2872257  0.28805116
 0.28926456 0.28920323 0.28865004 0.28835914 0.28828105 0.28810883
 0.28771824 0.28726408 0.28711158 0.28735304 0.2875823  0.28751013
 0.28729272 0.28721848 0.28744408 0.28782913 0.2881146  0.28817406
 0.28815758 0.2881299  0.28813326 0.28802672 0.28780353 0.28784847
 0.28819555 0.28834006 0.28836375 0.2882041  0.28788698 0.28764868
 0.28763482 0.287724   0.28772172 0.28764006 0.2876123  0.28772968
 0.28794533 0.2880327  0.2879757  0.28794336 0.2879872  0.28802985
 0.28812134 0.28799027 0.2877565  0.28763866 0.28765854 0.2877541
 0.2876302  0.28734878 0.28718582 0.2871322  0.28709212 0.28698784
 0.28684953 0.28679505 0.2869127  0.28713062 0.28726506 0.28723738
 0.28718412 0.28723294 0.28742224 0.2876297  0.2877259  0.28769395
 0.28763995 0.2875106  0.28736746 0.2872544  0.2871824  0.28720808
 0.28716725 0.28715563 0.28709278 0.28684968 0.28659624 0.2865438
 0.28667754 0.2868092  0.28682536 0.2867924  0.286852   0.28701797
 0.287172   0.28717163 0.2870317  0.2869315  0.28699553 0.28717053
 0.28727445 0.28712106 0.2868113  0.28652963 0.28655154 0.28677964
 0.2869371  0.28708178 0.28725624 0.28733253 0.28739443 0.28742394
 0.28737685 0.2873084  0.28731272 0.28739634 0.2874933  0.28756508
 0.2876109  0.28767124 0.2877836  0.28790522 0.28795788 0.28787652
 0.28770354 0.28749496 0.28730315 0.28719303 0.2871677  0.287298
 0.2875251  0.28756446 0.28746906 0.2873711  0.28742033 0.2876232
 0.28782225 0.28788608 0.28779238 0.2876331  0.28751948 0.28753102
 0.28765222 0.28777897 0.2878191  0.287786   0.2877751  0.2877801
 0.28776392 0.28770107 0.2876264  0.2875975  0.28764793 0.2881097
 0.28888994 0.28881776 0.28858647 0.28858593 0.28863445 0.2886059
 0.28849554 0.28839996 0.2883963  0.28844157 0.28840506 0.28828716
 0.28817904 0.28805733 0.28795347 0.2878813  0.28784627 0.28781807
 0.28770685 0.28757265 0.28742456 0.28722405 0.28707936 0.2873806
 0.28806093 0.28827006 0.28825915 0.28820875 0.28810847 0.28803807
 0.2880343  0.2880255  0.28794682 0.28781673 0.28766817 0.28759524
 0.28760678 0.28754884 0.28740248 0.2872613  0.28720868 0.2871929
 0.28705242 0.28675553 0.2864611  0.28635356 0.28647307 0.28684297
 0.2872174  0.2872246  0.28711608 0.28710827 0.28720146 0.28732327
 0.28740498 0.28743428 0.2874473  0.28744906 0.2873602  0.28717715
 0.2870003  0.2868335  0.28670388 0.28662714 0.28658307 0.28650582
 0.28634343 0.28616145 0.28618383 0.28631425 0.28651154 0.28670594
 0.28677914 0.2867336  0.28669426 0.28664976 0.28662196 0.28667536
 0.28680027 0.28688863 0.28683048 0.28661954 0.286367   0.2862071
 0.28616297 0.2860852  0.28589812 0.28570786 0.28566843 0.28580463
 0.2859045  0.28584328 0.28576556 0.28586987 0.2861733  0.28647754
 0.28647837 0.28632706 0.2862717  0.2863141  0.28641278 0.28646842
 0.28638104 0.28622794 0.28615943 0.28618357 0.28615695 0.2859592
 0.28569275 0.28550595 0.2854474  0.2854199  0.28534278 0.28524542
 0.28517413 0.2851653  0.28520733 0.28530973 0.28553942 0.2858272
 0.28596094 0.28607866 0.28623006 0.28629586 0.28636563 0.28649756
 0.28658688 0.28648505 0.28612354 0.2856111  0.28524983 0.28526834
 0.28552496 0.28565046 0.28550196 0.28534576 0.2855371  0.28600597
 0.28634962 0.28631568 0.28612515 0.28614262 0.28645155 0.2867963
 0.28682718 0.28648636 0.28630587 0.2864746  0.28667313 0.28650597
 0.286016   0.28565183 0.28566742 0.28578353 0.28561848 0.28531933
 0.2854561  0.28622448 0.28703862 0.28715974 0.2867245  0.28664365
 0.2873148  0.28772742 0.28632066 0.28342113 0.28290573 0.28833693]
