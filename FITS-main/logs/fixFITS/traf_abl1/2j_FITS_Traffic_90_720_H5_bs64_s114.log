Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j720_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j720_H5_FITS_custom_ftM_sl90_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11471
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=30, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  893721600.0
params:  8370.0
Trainable parameters:  8370
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 104.32315373420715
Epoch: 1, Steps: 89 | Train Loss: 2.6747497 Vali Loss: 2.2938213 Test Loss: 2.8602569
Validation loss decreased (inf --> 2.293821).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 104.72296452522278
Epoch: 2, Steps: 89 | Train Loss: 1.4965254 Vali Loss: 1.5092332 Test Loss: 1.8769073
Validation loss decreased (2.293821 --> 1.509233).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 104.19201803207397
Epoch: 3, Steps: 89 | Train Loss: 1.0158525 Vali Loss: 1.1424460 Test Loss: 1.4116858
Validation loss decreased (1.509233 --> 1.142446).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 105.55539441108704
Epoch: 4, Steps: 89 | Train Loss: 0.7800697 Vali Loss: 0.9472370 Test Loss: 1.1601027
Validation loss decreased (1.142446 --> 0.947237).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 102.78438639640808
Epoch: 5, Steps: 89 | Train Loss: 0.6494105 Vali Loss: 0.8310254 Test Loss: 1.0125023
Validation loss decreased (0.947237 --> 0.831025).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 103.81811428070068
Epoch: 6, Steps: 89 | Train Loss: 0.5708387 Vali Loss: 0.7600730 Test Loss: 0.9208054
Validation loss decreased (0.831025 --> 0.760073).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 104.51753115653992
Epoch: 7, Steps: 89 | Train Loss: 0.5207823 Vali Loss: 0.7127750 Test Loss: 0.8609075
Validation loss decreased (0.760073 --> 0.712775).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 104.89519691467285
Epoch: 8, Steps: 89 | Train Loss: 0.4874211 Vali Loss: 0.6810383 Test Loss: 0.8203798
Validation loss decreased (0.712775 --> 0.681038).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 103.91758322715759
Epoch: 9, Steps: 89 | Train Loss: 0.4644003 Vali Loss: 0.6580632 Test Loss: 0.7917179
Validation loss decreased (0.681038 --> 0.658063).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 106.95074319839478
Epoch: 10, Steps: 89 | Train Loss: 0.4478077 Vali Loss: 0.6424493 Test Loss: 0.7713796
Validation loss decreased (0.658063 --> 0.642449).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 103.31380248069763
Epoch: 11, Steps: 89 | Train Loss: 0.4356203 Vali Loss: 0.6297101 Test Loss: 0.7556141
Validation loss decreased (0.642449 --> 0.629710).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 107.5350272655487
Epoch: 12, Steps: 89 | Train Loss: 0.4261407 Vali Loss: 0.6200197 Test Loss: 0.7436255
Validation loss decreased (0.629710 --> 0.620020).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 106.81637597084045
Epoch: 13, Steps: 89 | Train Loss: 0.4189671 Vali Loss: 0.6127524 Test Loss: 0.7341152
Validation loss decreased (0.620020 --> 0.612752).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 100.87533640861511
Epoch: 14, Steps: 89 | Train Loss: 0.4132445 Vali Loss: 0.6065665 Test Loss: 0.7266912
Validation loss decreased (0.612752 --> 0.606566).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 105.9609625339508
Epoch: 15, Steps: 89 | Train Loss: 0.4085883 Vali Loss: 0.6017244 Test Loss: 0.7203667
Validation loss decreased (0.606566 --> 0.601724).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 107.97145676612854
Epoch: 16, Steps: 89 | Train Loss: 0.4046156 Vali Loss: 0.5980234 Test Loss: 0.7151296
Validation loss decreased (0.601724 --> 0.598023).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 107.68320107460022
Epoch: 17, Steps: 89 | Train Loss: 0.4015417 Vali Loss: 0.5941273 Test Loss: 0.7108228
Validation loss decreased (0.598023 --> 0.594127).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 105.60813522338867
Epoch: 18, Steps: 89 | Train Loss: 0.3988253 Vali Loss: 0.5912754 Test Loss: 0.7070562
Validation loss decreased (0.594127 --> 0.591275).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 102.39324712753296
Epoch: 19, Steps: 89 | Train Loss: 0.3965416 Vali Loss: 0.5883459 Test Loss: 0.7039098
Validation loss decreased (0.591275 --> 0.588346).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 99.25045609474182
Epoch: 20, Steps: 89 | Train Loss: 0.3944671 Vali Loss: 0.5869961 Test Loss: 0.7011309
Validation loss decreased (0.588346 --> 0.586996).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 101.43811678886414
Epoch: 21, Steps: 89 | Train Loss: 0.3929311 Vali Loss: 0.5852975 Test Loss: 0.6988791
Validation loss decreased (0.586996 --> 0.585297).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 103.3547670841217
Epoch: 22, Steps: 89 | Train Loss: 0.3914021 Vali Loss: 0.5829886 Test Loss: 0.6966887
Validation loss decreased (0.585297 --> 0.582989).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 100.32001113891602
Epoch: 23, Steps: 89 | Train Loss: 0.3900900 Vali Loss: 0.5820450 Test Loss: 0.6947904
Validation loss decreased (0.582989 --> 0.582045).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 102.35725450515747
Epoch: 24, Steps: 89 | Train Loss: 0.3888310 Vali Loss: 0.5795908 Test Loss: 0.6932347
Validation loss decreased (0.582045 --> 0.579591).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 102.74313116073608
Epoch: 25, Steps: 89 | Train Loss: 0.3877730 Vali Loss: 0.5792273 Test Loss: 0.6918022
Validation loss decreased (0.579591 --> 0.579227).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 90.8427164554596
Epoch: 26, Steps: 89 | Train Loss: 0.3868925 Vali Loss: 0.5779822 Test Loss: 0.6904453
Validation loss decreased (0.579227 --> 0.577982).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 94.42309975624084
Epoch: 27, Steps: 89 | Train Loss: 0.3860834 Vali Loss: 0.5768046 Test Loss: 0.6893610
Validation loss decreased (0.577982 --> 0.576805).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 93.65359807014465
Epoch: 28, Steps: 89 | Train Loss: 0.3853218 Vali Loss: 0.5768612 Test Loss: 0.6882894
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 91.94464421272278
Epoch: 29, Steps: 89 | Train Loss: 0.3846123 Vali Loss: 0.5753111 Test Loss: 0.6872633
Validation loss decreased (0.576805 --> 0.575311).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 89.03950715065002
Epoch: 30, Steps: 89 | Train Loss: 0.3839652 Vali Loss: 0.5751010 Test Loss: 0.6863728
Validation loss decreased (0.575311 --> 0.575101).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 103.22498536109924
Epoch: 31, Steps: 89 | Train Loss: 0.3833495 Vali Loss: 0.5743624 Test Loss: 0.6855414
Validation loss decreased (0.575101 --> 0.574362).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 102.92897081375122
Epoch: 32, Steps: 89 | Train Loss: 0.3827354 Vali Loss: 0.5739440 Test Loss: 0.6847774
Validation loss decreased (0.574362 --> 0.573944).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 101.00439381599426
Epoch: 33, Steps: 89 | Train Loss: 0.3824450 Vali Loss: 0.5728439 Test Loss: 0.6841840
Validation loss decreased (0.573944 --> 0.572844).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 102.38626837730408
Epoch: 34, Steps: 89 | Train Loss: 0.3818827 Vali Loss: 0.5723054 Test Loss: 0.6835443
Validation loss decreased (0.572844 --> 0.572305).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 102.04693055152893
Epoch: 35, Steps: 89 | Train Loss: 0.3815816 Vali Loss: 0.5725627 Test Loss: 0.6829545
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 105.13995623588562
Epoch: 36, Steps: 89 | Train Loss: 0.3811909 Vali Loss: 0.5712611 Test Loss: 0.6824327
Validation loss decreased (0.572305 --> 0.571261).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 101.73688077926636
Epoch: 37, Steps: 89 | Train Loss: 0.3808939 Vali Loss: 0.5717738 Test Loss: 0.6819153
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 101.79949140548706
Epoch: 38, Steps: 89 | Train Loss: 0.3803863 Vali Loss: 0.5715886 Test Loss: 0.6814897
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 94.91894769668579
Epoch: 39, Steps: 89 | Train Loss: 0.3801619 Vali Loss: 0.5706483 Test Loss: 0.6810654
Validation loss decreased (0.571261 --> 0.570648).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 95.79084587097168
Epoch: 40, Steps: 89 | Train Loss: 0.3798412 Vali Loss: 0.5698543 Test Loss: 0.6806230
Validation loss decreased (0.570648 --> 0.569854).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 97.96501564979553
Epoch: 41, Steps: 89 | Train Loss: 0.3795765 Vali Loss: 0.5700159 Test Loss: 0.6803162
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 96.72074007987976
Epoch: 42, Steps: 89 | Train Loss: 0.3793404 Vali Loss: 0.5701403 Test Loss: 0.6799725
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 93.08131742477417
Epoch: 43, Steps: 89 | Train Loss: 0.3790947 Vali Loss: 0.5692593 Test Loss: 0.6795895
Validation loss decreased (0.569854 --> 0.569259).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 78.16528487205505
Epoch: 44, Steps: 89 | Train Loss: 0.3788126 Vali Loss: 0.5694351 Test Loss: 0.6793233
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 76.86036443710327
Epoch: 45, Steps: 89 | Train Loss: 0.3785760 Vali Loss: 0.5690356 Test Loss: 0.6790560
Validation loss decreased (0.569259 --> 0.569036).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 77.64604663848877
Epoch: 46, Steps: 89 | Train Loss: 0.3784407 Vali Loss: 0.5689718 Test Loss: 0.6787506
Validation loss decreased (0.569036 --> 0.568972).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 78.24536943435669
Epoch: 47, Steps: 89 | Train Loss: 0.3783392 Vali Loss: 0.5685130 Test Loss: 0.6784877
Validation loss decreased (0.568972 --> 0.568513).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 75.39831018447876
Epoch: 48, Steps: 89 | Train Loss: 0.3780779 Vali Loss: 0.5683198 Test Loss: 0.6782464
Validation loss decreased (0.568513 --> 0.568320).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 77.5841817855835
Epoch: 49, Steps: 89 | Train Loss: 0.3778490 Vali Loss: 0.5685407 Test Loss: 0.6780608
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 76.6991126537323
Epoch: 50, Steps: 89 | Train Loss: 0.3777624 Vali Loss: 0.5679926 Test Loss: 0.6778486
Validation loss decreased (0.568320 --> 0.567993).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 77.50983476638794
Epoch: 51, Steps: 89 | Train Loss: 0.3776959 Vali Loss: 0.5678152 Test Loss: 0.6776677
Validation loss decreased (0.567993 --> 0.567815).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 77.35857248306274
Epoch: 52, Steps: 89 | Train Loss: 0.3775063 Vali Loss: 0.5676125 Test Loss: 0.6774596
Validation loss decreased (0.567815 --> 0.567613).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 78.49497890472412
Epoch: 53, Steps: 89 | Train Loss: 0.3774290 Vali Loss: 0.5675629 Test Loss: 0.6772309
Validation loss decreased (0.567613 --> 0.567563).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 78.64889931678772
Epoch: 54, Steps: 89 | Train Loss: 0.3771923 Vali Loss: 0.5672594 Test Loss: 0.6770892
Validation loss decreased (0.567563 --> 0.567259).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 79.24206352233887
Epoch: 55, Steps: 89 | Train Loss: 0.3771251 Vali Loss: 0.5671226 Test Loss: 0.6769394
Validation loss decreased (0.567259 --> 0.567123).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 77.60907411575317
Epoch: 56, Steps: 89 | Train Loss: 0.3771075 Vali Loss: 0.5672755 Test Loss: 0.6768157
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 75.7092137336731
Epoch: 57, Steps: 89 | Train Loss: 0.3769750 Vali Loss: 0.5663608 Test Loss: 0.6766652
Validation loss decreased (0.567123 --> 0.566361).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 91.08716607093811
Epoch: 58, Steps: 89 | Train Loss: 0.3767842 Vali Loss: 0.5665469 Test Loss: 0.6764997
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 85.62661790847778
Epoch: 59, Steps: 89 | Train Loss: 0.3767504 Vali Loss: 0.5660152 Test Loss: 0.6763661
Validation loss decreased (0.566361 --> 0.566015).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 86.53753519058228
Epoch: 60, Steps: 89 | Train Loss: 0.3766822 Vali Loss: 0.5660610 Test Loss: 0.6762463
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 87.61469721794128
Epoch: 61, Steps: 89 | Train Loss: 0.3765382 Vali Loss: 0.5664006 Test Loss: 0.6761274
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 84.18135023117065
Epoch: 62, Steps: 89 | Train Loss: 0.3764972 Vali Loss: 0.5661165 Test Loss: 0.6760368
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 87.4167947769165
Epoch: 63, Steps: 89 | Train Loss: 0.3762892 Vali Loss: 0.5663874 Test Loss: 0.6759058
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 86.28283143043518
Epoch: 64, Steps: 89 | Train Loss: 0.3762583 Vali Loss: 0.5665178 Test Loss: 0.6758369
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 83.83393549919128
Epoch: 65, Steps: 89 | Train Loss: 0.3762365 Vali Loss: 0.5663076 Test Loss: 0.6757278
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 68.20323538780212
Epoch: 66, Steps: 89 | Train Loss: 0.3761702 Vali Loss: 0.5658444 Test Loss: 0.6756355
Validation loss decreased (0.566015 --> 0.565844).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 67.50628709793091
Epoch: 67, Steps: 89 | Train Loss: 0.3760260 Vali Loss: 0.5663961 Test Loss: 0.6755583
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 65.52498960494995
Epoch: 68, Steps: 89 | Train Loss: 0.3760339 Vali Loss: 0.5667893 Test Loss: 0.6754737
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 68.46585607528687
Epoch: 69, Steps: 89 | Train Loss: 0.3759693 Vali Loss: 0.5654519 Test Loss: 0.6753997
Validation loss decreased (0.565844 --> 0.565452).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 68.58664393424988
Epoch: 70, Steps: 89 | Train Loss: 0.3759244 Vali Loss: 0.5659789 Test Loss: 0.6753213
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 68.90201139450073
Epoch: 71, Steps: 89 | Train Loss: 0.3758677 Vali Loss: 0.5655421 Test Loss: 0.6752421
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 69.5245885848999
Epoch: 72, Steps: 89 | Train Loss: 0.3758077 Vali Loss: 0.5664649 Test Loss: 0.6751822
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 68.85391688346863
Epoch: 73, Steps: 89 | Train Loss: 0.3757218 Vali Loss: 0.5659460 Test Loss: 0.6751293
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 70.06392407417297
Epoch: 74, Steps: 89 | Train Loss: 0.3756853 Vali Loss: 0.5659526 Test Loss: 0.6750606
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 70.73963832855225
Epoch: 75, Steps: 89 | Train Loss: 0.3757197 Vali Loss: 0.5652600 Test Loss: 0.6750020
Validation loss decreased (0.565452 --> 0.565260).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 68.73494505882263
Epoch: 76, Steps: 89 | Train Loss: 0.3755892 Vali Loss: 0.5658155 Test Loss: 0.6749586
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 69.80523538589478
Epoch: 77, Steps: 89 | Train Loss: 0.3756184 Vali Loss: 0.5646114 Test Loss: 0.6749110
Validation loss decreased (0.565260 --> 0.564611).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 70.50829744338989
Epoch: 78, Steps: 89 | Train Loss: 0.3756276 Vali Loss: 0.5659264 Test Loss: 0.6748441
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 69.13312721252441
Epoch: 79, Steps: 89 | Train Loss: 0.3754782 Vali Loss: 0.5650559 Test Loss: 0.6748021
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 68.98723983764648
Epoch: 80, Steps: 89 | Train Loss: 0.3754282 Vali Loss: 0.5654852 Test Loss: 0.6747540
EarlyStopping counter: 3 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 67.76193237304688
Epoch: 81, Steps: 89 | Train Loss: 0.3754808 Vali Loss: 0.5655940 Test Loss: 0.6747116
EarlyStopping counter: 4 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 69.08053231239319
Epoch: 82, Steps: 89 | Train Loss: 0.3754953 Vali Loss: 0.5655354 Test Loss: 0.6746760
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 59.47482752799988
Epoch: 83, Steps: 89 | Train Loss: 0.3754253 Vali Loss: 0.5649303 Test Loss: 0.6746429
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 57.51469659805298
Epoch: 84, Steps: 89 | Train Loss: 0.3754074 Vali Loss: 0.5644855 Test Loss: 0.6746007
Validation loss decreased (0.564611 --> 0.564486).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 55.08646011352539
Epoch: 85, Steps: 89 | Train Loss: 0.3754068 Vali Loss: 0.5652951 Test Loss: 0.6745608
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 54.804752826690674
Epoch: 86, Steps: 89 | Train Loss: 0.3753053 Vali Loss: 0.5650148 Test Loss: 0.6745298
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 55.685502767562866
Epoch: 87, Steps: 89 | Train Loss: 0.3752067 Vali Loss: 0.5655347 Test Loss: 0.6745000
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 55.25692939758301
Epoch: 88, Steps: 89 | Train Loss: 0.3753108 Vali Loss: 0.5648014 Test Loss: 0.6744759
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 56.100019454956055
Epoch: 89, Steps: 89 | Train Loss: 0.3752323 Vali Loss: 0.5656452 Test Loss: 0.6744347
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 55.247758626937866
Epoch: 90, Steps: 89 | Train Loss: 0.3751415 Vali Loss: 0.5643857 Test Loss: 0.6744122
Validation loss decreased (0.564486 --> 0.564386).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 54.041889667510986
Epoch: 91, Steps: 89 | Train Loss: 0.3752628 Vali Loss: 0.5647330 Test Loss: 0.6743872
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 57.072625398635864
Epoch: 92, Steps: 89 | Train Loss: 0.3751919 Vali Loss: 0.5651408 Test Loss: 0.6743583
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 56.90618562698364
Epoch: 93, Steps: 89 | Train Loss: 0.3751331 Vali Loss: 0.5653114 Test Loss: 0.6743410
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 57.66668486595154
Epoch: 94, Steps: 89 | Train Loss: 0.3751182 Vali Loss: 0.5652694 Test Loss: 0.6743154
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 57.351696252822876
Epoch: 95, Steps: 89 | Train Loss: 0.3750192 Vali Loss: 0.5652606 Test Loss: 0.6742921
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 57.22498965263367
Epoch: 96, Steps: 89 | Train Loss: 0.3750196 Vali Loss: 0.5647004 Test Loss: 0.6742779
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 58.37501931190491
Epoch: 97, Steps: 89 | Train Loss: 0.3750217 Vali Loss: 0.5654818 Test Loss: 0.6742561
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 58.12350511550903
Epoch: 98, Steps: 89 | Train Loss: 0.3750557 Vali Loss: 0.5650752 Test Loss: 0.6742364
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 56.77159810066223
Epoch: 99, Steps: 89 | Train Loss: 0.3750649 Vali Loss: 0.5646761 Test Loss: 0.6742229
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 58.69870376586914
Epoch: 100, Steps: 89 | Train Loss: 0.3750977 Vali Loss: 0.5648163 Test Loss: 0.6742030
EarlyStopping counter: 10 out of 10
Early stopping
train 11471
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=30, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  893721600.0
params:  8370.0
Trainable parameters:  8370
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 58.650028228759766
Epoch: 1, Steps: 89 | Train Loss: 0.4172643 Vali Loss: 0.5627338 Test Loss: 0.6721824
Validation loss decreased (inf --> 0.562734).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 59.789780139923096
Epoch: 2, Steps: 89 | Train Loss: 0.4156707 Vali Loss: 0.5617479 Test Loss: 0.6712279
Validation loss decreased (0.562734 --> 0.561748).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 60.51645636558533
Epoch: 3, Steps: 89 | Train Loss: 0.4153912 Vali Loss: 0.5614735 Test Loss: 0.6711479
Validation loss decreased (0.561748 --> 0.561473).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 61.654502153396606
Epoch: 4, Steps: 89 | Train Loss: 0.4150338 Vali Loss: 0.5607837 Test Loss: 0.6710858
Validation loss decreased (0.561473 --> 0.560784).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 60.81467413902283
Epoch: 5, Steps: 89 | Train Loss: 0.4150691 Vali Loss: 0.5613708 Test Loss: 0.6708758
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 58.973336696624756
Epoch: 6, Steps: 89 | Train Loss: 0.4150715 Vali Loss: 0.5611773 Test Loss: 0.6708613
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 61.274314880371094
Epoch: 7, Steps: 89 | Train Loss: 0.4150930 Vali Loss: 0.5607829 Test Loss: 0.6709961
Validation loss decreased (0.560784 --> 0.560783).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 58.06882119178772
Epoch: 8, Steps: 89 | Train Loss: 0.4150145 Vali Loss: 0.5617337 Test Loss: 0.6709283
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 59.732224225997925
Epoch: 9, Steps: 89 | Train Loss: 0.4149876 Vali Loss: 0.5614942 Test Loss: 0.6708587
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 59.409998178482056
Epoch: 10, Steps: 89 | Train Loss: 0.4151149 Vali Loss: 0.5614895 Test Loss: 0.6710942
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 59.258347511291504
Epoch: 11, Steps: 89 | Train Loss: 0.4149204 Vali Loss: 0.5622388 Test Loss: 0.6714452
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 55.85084247589111
Epoch: 12, Steps: 89 | Train Loss: 0.4150579 Vali Loss: 0.5605830 Test Loss: 0.6711268
Validation loss decreased (0.560783 --> 0.560583).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 56.60740780830383
Epoch: 13, Steps: 89 | Train Loss: 0.4149407 Vali Loss: 0.5609511 Test Loss: 0.6711186
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 58.544522762298584
Epoch: 14, Steps: 89 | Train Loss: 0.4149706 Vali Loss: 0.5612934 Test Loss: 0.6710260
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 57.629218339920044
Epoch: 15, Steps: 89 | Train Loss: 0.4149369 Vali Loss: 0.5609431 Test Loss: 0.6708872
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 59.33232045173645
Epoch: 16, Steps: 89 | Train Loss: 0.4148451 Vali Loss: 0.5615445 Test Loss: 0.6707571
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 56.37306809425354
Epoch: 17, Steps: 89 | Train Loss: 0.4148710 Vali Loss: 0.5612068 Test Loss: 0.6710279
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 58.483893156051636
Epoch: 18, Steps: 89 | Train Loss: 0.4148801 Vali Loss: 0.5613989 Test Loss: 0.6709526
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 55.27559733390808
Epoch: 19, Steps: 89 | Train Loss: 0.4148443 Vali Loss: 0.5609239 Test Loss: 0.6706786
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 56.94909381866455
Epoch: 20, Steps: 89 | Train Loss: 0.4149521 Vali Loss: 0.5616639 Test Loss: 0.6710673
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 55.306373596191406
Epoch: 21, Steps: 89 | Train Loss: 0.4148591 Vali Loss: 0.5606647 Test Loss: 0.6708186
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 56.9318950176239
Epoch: 22, Steps: 89 | Train Loss: 0.4149118 Vali Loss: 0.5611749 Test Loss: 0.6709780
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j720_H5_FITS_custom_ftM_sl90_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.6699793338775635, mae:0.3975808918476105, rse:0.6693164706230164, corr:[0.25233847 0.26406753 0.26429087 0.26277262 0.26199132 0.26313978
 0.26544735 0.2654607  0.2648307  0.2646334  0.26372048 0.26366737
 0.2635904  0.2633024  0.2638694  0.26445073 0.26466253 0.26497468
 0.26644486 0.26864353 0.26940203 0.26996127 0.26991224 0.26926556
 0.27286038 0.27527946 0.27458027 0.27375004 0.27380705 0.27536517
 0.2770182  0.27849334 0.2784294  0.27811095 0.27827248 0.27781186
 0.2769307  0.27651006 0.2765058  0.27648953 0.27660105 0.27719465
 0.2783906  0.2791941  0.2792068  0.2796612  0.2790368  0.27751777
 0.2774804  0.2761089  0.27464512 0.27442142 0.27375314 0.27054146
 0.26785037 0.26847202 0.26827642 0.26780263 0.26749828 0.26624733
 0.26507574 0.26472586 0.26452884 0.2643     0.26413426 0.26406258
 0.26425573 0.26443598 0.26460958 0.26537916 0.26536143 0.26412445
 0.26269427 0.2612329  0.260268   0.2605343  0.26170415 0.26241192
 0.2632385  0.26559946 0.26624683 0.2659423  0.26516587 0.26366004
 0.2627571  0.26223785 0.2611838  0.26057673 0.2606249  0.26054105
 0.26094773 0.2613047  0.26160455 0.26225102 0.26232278 0.26178378
 0.26145378 0.2614301  0.26171052 0.26199675 0.2620906  0.26228538
 0.26310074 0.26456606 0.2646662  0.2637637  0.26321015 0.26260233
 0.26189107 0.261548   0.26122767 0.2612064  0.2616389  0.2618392
 0.2621213  0.2625664  0.26316714 0.2638645  0.26400313 0.26375395
 0.26375407 0.26370308 0.2637208  0.26386002 0.2639495  0.26396123
 0.26384115 0.26392883 0.26381823 0.26287547 0.26190192 0.2616105
 0.26161757 0.2616263  0.26144853 0.26150367 0.26188487 0.26171982
 0.2624355  0.26332375 0.26395065 0.2650598  0.26518902 0.26380512
 0.263131   0.26279    0.2623077  0.262111   0.26240748 0.2629003
 0.26292747 0.26280382 0.26273155 0.26215124 0.26124284 0.26096335
 0.26117116 0.26131245 0.26130474 0.26171657 0.2624553  0.262897
 0.26439843 0.2665014  0.26715955 0.2675921  0.26750004 0.26547462
 0.2645874  0.26344323 0.2618923  0.26078895 0.26057893 0.2623114
 0.26496688 0.26647952 0.26656693 0.26630595 0.26611063 0.26612675
 0.26634333 0.26650557 0.2665181  0.26666838 0.26701042 0.26725903
 0.26821327 0.2698454  0.27063823 0.27109626 0.27080953 0.2701936
 0.27378762 0.27634332 0.27508673 0.27353725 0.27358693 0.27496582
 0.27624252 0.27776232 0.27802154 0.27778065 0.27793583 0.2776642
 0.27711344 0.27676746 0.2767772  0.27690464 0.27696213 0.27729946
 0.27812338 0.27877882 0.27894738 0.27909452 0.27829945 0.2767989
 0.27624047 0.27439344 0.27252457 0.27212784 0.27187726 0.26933795
 0.26686454 0.2676335  0.26790905 0.26790607 0.2679089  0.26658714
 0.26501197 0.26419663 0.2637635  0.26367092 0.26395988 0.26440957
 0.2647382  0.26467788 0.26475358 0.2651073  0.26467314 0.26295647
 0.26135018 0.26035038 0.25990486 0.26045647 0.26194355 0.26295972
 0.26403937 0.26664683 0.26738402 0.2670328  0.26639524 0.2651622
 0.26444563 0.2640121  0.2630954  0.26235217 0.2619043  0.26174235
 0.26227275 0.26279455 0.26300478 0.26341674 0.26376233 0.2636886
 0.2633826  0.26311246 0.26313466 0.26341614 0.26378837 0.26425263
 0.26479053 0.26572907 0.26563555 0.26514265 0.26495188 0.26426494
 0.2636534  0.26363066 0.26337948 0.2629917  0.26291803 0.26309893
 0.26374033 0.2641687  0.26427802 0.26470292 0.2651505  0.26525182
 0.26513535 0.26492956 0.2649042  0.26487634 0.26473725 0.26482576
 0.26498827 0.2650802  0.26469806 0.26400945 0.26355526 0.26307833
 0.26261926 0.26258337 0.26259503 0.2626402  0.26294184 0.26321015
 0.26421848 0.26502833 0.26551935 0.26659372 0.26672742 0.26512557
 0.26405606 0.26383296 0.26391822 0.26394522 0.2640194  0.26450002
 0.2648394  0.26478145 0.2646289  0.264276   0.2636651  0.26308236
 0.26280463 0.26292905 0.2630315  0.26327303 0.2640383  0.26474705
 0.26613826 0.26763466 0.26816756 0.26881054 0.2683907  0.26589411
 0.2646661  0.2634039  0.26126063 0.25984457 0.259957   0.26190767
 0.26444927 0.26598936 0.26630405 0.26608866 0.26581147 0.26599112
 0.26624548 0.26614767 0.26619697 0.26649806 0.26667103 0.26681507
 0.26786193 0.2694684  0.27012113 0.2704729  0.27018002 0.26949373
 0.27253872 0.27535954 0.27412018 0.27235752 0.27238604 0.2740636
 0.27566677 0.27750075 0.27831468 0.27848217 0.27826452 0.27763668
 0.27738714 0.27729577 0.2768635  0.27653742 0.2767994  0.2776045
 0.2784179  0.27881435 0.278839   0.27880847 0.2779716  0.2764991
 0.27589816 0.27416632 0.27236173 0.27190334 0.271354   0.26831007
 0.26604006 0.26718247 0.2673765  0.2670096  0.26701143 0.26619408
 0.2651859  0.26453575 0.26393685 0.26360422 0.26360604 0.26397645
 0.2644609  0.26458538 0.2646574  0.26502252 0.26466203 0.2631356
 0.2615043  0.26031888 0.25973004 0.26012233 0.2611962  0.261914
 0.26322252 0.26617512 0.2674726  0.26767236 0.2674955  0.26649544
 0.26557547 0.26475385 0.26358938 0.2627203  0.2622554  0.26212552
 0.2626279  0.2631656  0.26353738 0.2639726  0.26414979 0.26403427
 0.26381493 0.26364934 0.26356784 0.2634711  0.26362556 0.26408044
 0.2647192  0.265869   0.2661275  0.26578742 0.26558638 0.26497987
 0.26443392 0.26427928 0.263803   0.26317748 0.2629409  0.26310074
 0.2638688  0.2644705  0.264531   0.2646716  0.26487464 0.26488596
 0.26478422 0.26459312 0.2647403  0.2651808  0.26550508 0.26554725
 0.26522946 0.26501104 0.26462743 0.26399392 0.2636821  0.2634433
 0.2630763  0.2627937  0.26261866 0.26286662 0.26342398 0.26372185
 0.2642878  0.26508564 0.26579186 0.266716   0.2667556  0.26546633
 0.26460484 0.26426768 0.2643293  0.26470774 0.26492068 0.26505217
 0.26508188 0.26498225 0.26465985 0.26408997 0.2635197  0.26315442
 0.26314995 0.26330525 0.26316687 0.26319286 0.26368788 0.26422817
 0.26538745 0.26706314 0.26822618 0.269048   0.268375   0.2658632
 0.26459783 0.2630485  0.26060477 0.2594297  0.26001295 0.26216197
 0.2645856  0.26607874 0.26629376 0.26604208 0.265861   0.26582786
 0.265584   0.26517364 0.2651318  0.26545045 0.2658214  0.26621148
 0.26720473 0.2686763  0.26952627 0.27023873 0.2700544  0.2690947
 0.27162632 0.27400154 0.27257097 0.2712006  0.27190608 0.27390406
 0.27516988 0.27657998 0.27713695 0.27725273 0.27730206 0.2767656
 0.27601588 0.2754777  0.275265   0.2751552  0.2751368  0.27570856
 0.27669373 0.27723473 0.27720124 0.27746293 0.2771467  0.27567938
 0.27481034 0.27330226 0.27167073 0.27115184 0.27073377 0.2681869
 0.26572725 0.26631424 0.26663846 0.26656553 0.26616147 0.2646652
 0.26328707 0.26266316 0.26217455 0.26160726 0.2612585  0.26145822
 0.2619807  0.26221907 0.26216927 0.26224908 0.2620535  0.26100254
 0.2596281  0.25833344 0.25770462 0.25803217 0.25917622 0.26054415
 0.26269767 0.26589176 0.26692662 0.26656994 0.26590088 0.26463366
 0.2637818  0.2633104  0.26241526 0.26153973 0.26104143 0.26091927
 0.26119104 0.2613743  0.26134893 0.26169562 0.2621881  0.26239654
 0.26237836 0.26225355 0.26214218 0.2620054  0.2621043  0.26264194
 0.26361018 0.26498833 0.26540756 0.26498383 0.2641323  0.26294863
 0.26238135 0.26244518 0.26206958 0.2615039  0.26152644 0.26180336
 0.26200807 0.26216638 0.26232484 0.26266682 0.26306513 0.26337418
 0.26366356 0.26371792 0.26362702 0.26349083 0.2634351  0.26368514
 0.26388046 0.26380217 0.26336718 0.26270106 0.26208258 0.26159087
 0.26134783 0.26123998 0.26091242 0.26077706 0.2611866  0.26163495
 0.2620786  0.26278654 0.2637741  0.26496112 0.2649638  0.2636661
 0.2630611  0.26287058 0.26259455 0.2625194  0.26281348 0.26341933
 0.2635163  0.26295117 0.26250708 0.26208654 0.2613398  0.2606799
 0.26046938 0.26069903 0.2610014  0.26139668 0.2619606  0.26224694
 0.2629803  0.26456782 0.2658659  0.26689407 0.26655647 0.26447615
 0.2638344  0.2630927  0.26098162 0.25912854 0.25901142 0.26102588
 0.26349816 0.2645818  0.26480705 0.26498097 0.26460415 0.2642671
 0.2644275  0.26456255 0.26451656 0.264593   0.26500654 0.26531175
 0.26570272 0.26678845 0.26785517 0.2687085  0.26848724 0.26795802
 0.2709271  0.27391168 0.27347836 0.2721005  0.27181694 0.27274773
 0.27398407 0.27568862 0.27658316 0.27701753 0.27697486 0.27634528
 0.27574465 0.2755433  0.2753935  0.27480182 0.27451363 0.2747142
 0.27533203 0.27573544 0.27608496 0.276974   0.27572903 0.2770979 ]
