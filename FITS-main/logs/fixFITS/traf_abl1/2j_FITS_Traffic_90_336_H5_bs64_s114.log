Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j336_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j336_H5_FITS_custom_ftM_sl90_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11855
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=30, out_features=142, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  470031360.0
params:  4402.0
Trainable parameters:  4402
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 67.76983094215393
Epoch: 1, Steps: 92 | Train Loss: 1.6677279 Vali Loss: 1.6143211 Test Loss: 1.9546760
Validation loss decreased (inf --> 1.614321).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 62.10567831993103
Epoch: 2, Steps: 92 | Train Loss: 1.0197223 Vali Loss: 1.1557279 Test Loss: 1.3989731
Validation loss decreased (1.614321 --> 1.155728).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 64.73703479766846
Epoch: 3, Steps: 92 | Train Loss: 0.7435140 Vali Loss: 0.9308531 Test Loss: 1.1291256
Validation loss decreased (1.155728 --> 0.930853).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 66.18422365188599
Epoch: 4, Steps: 92 | Train Loss: 0.6032836 Vali Loss: 0.8084778 Test Loss: 0.9805139
Validation loss decreased (0.930853 --> 0.808478).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 62.855886936187744
Epoch: 5, Steps: 92 | Train Loss: 0.5222880 Vali Loss: 0.7334177 Test Loss: 0.8913062
Validation loss decreased (0.808478 --> 0.733418).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 67.65368223190308
Epoch: 6, Steps: 92 | Train Loss: 0.4718805 Vali Loss: 0.6850755 Test Loss: 0.8340336
Validation loss decreased (0.733418 --> 0.685076).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 66.35857486724854
Epoch: 7, Steps: 92 | Train Loss: 0.4384127 Vali Loss: 0.6526945 Test Loss: 0.7957341
Validation loss decreased (0.685076 --> 0.652695).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 63.90104150772095
Epoch: 8, Steps: 92 | Train Loss: 0.4150377 Vali Loss: 0.6290012 Test Loss: 0.7684986
Validation loss decreased (0.652695 --> 0.629001).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 66.91954970359802
Epoch: 9, Steps: 92 | Train Loss: 0.3981948 Vali Loss: 0.6120512 Test Loss: 0.7484869
Validation loss decreased (0.629001 --> 0.612051).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 69.61615347862244
Epoch: 10, Steps: 92 | Train Loss: 0.3855254 Vali Loss: 0.5992980 Test Loss: 0.7332110
Validation loss decreased (0.612051 --> 0.599298).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 67.07932686805725
Epoch: 11, Steps: 92 | Train Loss: 0.3758414 Vali Loss: 0.5887678 Test Loss: 0.7211114
Validation loss decreased (0.599298 --> 0.588768).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 65.42866897583008
Epoch: 12, Steps: 92 | Train Loss: 0.3682191 Vali Loss: 0.5805253 Test Loss: 0.7114512
Validation loss decreased (0.588768 --> 0.580525).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 67.93379878997803
Epoch: 13, Steps: 92 | Train Loss: 0.3619311 Vali Loss: 0.5729295 Test Loss: 0.7034810
Validation loss decreased (0.580525 --> 0.572930).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 63.63311457633972
Epoch: 14, Steps: 92 | Train Loss: 0.3567403 Vali Loss: 0.5676131 Test Loss: 0.6967258
Validation loss decreased (0.572930 --> 0.567613).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 69.45716428756714
Epoch: 15, Steps: 92 | Train Loss: 0.3523316 Vali Loss: 0.5625191 Test Loss: 0.6910197
Validation loss decreased (0.567613 --> 0.562519).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 65.91865086555481
Epoch: 16, Steps: 92 | Train Loss: 0.3487832 Vali Loss: 0.5581246 Test Loss: 0.6860607
Validation loss decreased (0.562519 --> 0.558125).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 64.55687832832336
Epoch: 17, Steps: 92 | Train Loss: 0.3455832 Vali Loss: 0.5547260 Test Loss: 0.6816772
Validation loss decreased (0.558125 --> 0.554726).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 68.16646766662598
Epoch: 18, Steps: 92 | Train Loss: 0.3429525 Vali Loss: 0.5513172 Test Loss: 0.6779906
Validation loss decreased (0.554726 --> 0.551317).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 65.0536413192749
Epoch: 19, Steps: 92 | Train Loss: 0.3405432 Vali Loss: 0.5485561 Test Loss: 0.6745872
Validation loss decreased (0.551317 --> 0.548556).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 68.68770217895508
Epoch: 20, Steps: 92 | Train Loss: 0.3384988 Vali Loss: 0.5455586 Test Loss: 0.6716142
Validation loss decreased (0.548556 --> 0.545559).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 66.32224297523499
Epoch: 21, Steps: 92 | Train Loss: 0.3366159 Vali Loss: 0.5433040 Test Loss: 0.6689746
Validation loss decreased (0.545559 --> 0.543304).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 63.88990569114685
Epoch: 22, Steps: 92 | Train Loss: 0.3349134 Vali Loss: 0.5410588 Test Loss: 0.6664945
Validation loss decreased (0.543304 --> 0.541059).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 67.59603881835938
Epoch: 23, Steps: 92 | Train Loss: 0.3333478 Vali Loss: 0.5394636 Test Loss: 0.6643547
Validation loss decreased (0.541059 --> 0.539464).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 67.57136106491089
Epoch: 24, Steps: 92 | Train Loss: 0.3321260 Vali Loss: 0.5380447 Test Loss: 0.6624184
Validation loss decreased (0.539464 --> 0.538045).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 66.71737623214722
Epoch: 25, Steps: 92 | Train Loss: 0.3308798 Vali Loss: 0.5361773 Test Loss: 0.6606340
Validation loss decreased (0.538045 --> 0.536177).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 71.3685188293457
Epoch: 26, Steps: 92 | Train Loss: 0.3299150 Vali Loss: 0.5344443 Test Loss: 0.6590710
Validation loss decreased (0.536177 --> 0.534444).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 67.17807412147522
Epoch: 27, Steps: 92 | Train Loss: 0.3288512 Vali Loss: 0.5334467 Test Loss: 0.6575552
Validation loss decreased (0.534444 --> 0.533447).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 67.44832849502563
Epoch: 28, Steps: 92 | Train Loss: 0.3279020 Vali Loss: 0.5322524 Test Loss: 0.6562536
Validation loss decreased (0.533447 --> 0.532252).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 66.66219997406006
Epoch: 29, Steps: 92 | Train Loss: 0.3271738 Vali Loss: 0.5312291 Test Loss: 0.6550825
Validation loss decreased (0.532252 --> 0.531229).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 66.94471573829651
Epoch: 30, Steps: 92 | Train Loss: 0.3263589 Vali Loss: 0.5296876 Test Loss: 0.6538966
Validation loss decreased (0.531229 --> 0.529688).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 61.271974325180054
Epoch: 31, Steps: 92 | Train Loss: 0.3257045 Vali Loss: 0.5290945 Test Loss: 0.6528416
Validation loss decreased (0.529688 --> 0.529094).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 65.71115803718567
Epoch: 32, Steps: 92 | Train Loss: 0.3251116 Vali Loss: 0.5284269 Test Loss: 0.6519460
Validation loss decreased (0.529094 --> 0.528427).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 63.524072885513306
Epoch: 33, Steps: 92 | Train Loss: 0.3245046 Vali Loss: 0.5274950 Test Loss: 0.6510357
Validation loss decreased (0.528427 --> 0.527495).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 63.424909353256226
Epoch: 34, Steps: 92 | Train Loss: 0.3239063 Vali Loss: 0.5273772 Test Loss: 0.6502028
Validation loss decreased (0.527495 --> 0.527377).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 66.12710523605347
Epoch: 35, Steps: 92 | Train Loss: 0.3234937 Vali Loss: 0.5261336 Test Loss: 0.6494626
Validation loss decreased (0.527377 --> 0.526134).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 63.73133182525635
Epoch: 36, Steps: 92 | Train Loss: 0.3229441 Vali Loss: 0.5255654 Test Loss: 0.6487235
Validation loss decreased (0.526134 --> 0.525565).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 63.84223747253418
Epoch: 37, Steps: 92 | Train Loss: 0.3225718 Vali Loss: 0.5246906 Test Loss: 0.6480850
Validation loss decreased (0.525565 --> 0.524691).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 65.02605509757996
Epoch: 38, Steps: 92 | Train Loss: 0.3222143 Vali Loss: 0.5243012 Test Loss: 0.6474702
Validation loss decreased (0.524691 --> 0.524301).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 58.932841539382935
Epoch: 39, Steps: 92 | Train Loss: 0.3218562 Vali Loss: 0.5240299 Test Loss: 0.6468849
Validation loss decreased (0.524301 --> 0.524030).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 58.174906730651855
Epoch: 40, Steps: 92 | Train Loss: 0.3213917 Vali Loss: 0.5234179 Test Loss: 0.6463930
Validation loss decreased (0.524030 --> 0.523418).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 60.310792684555054
Epoch: 41, Steps: 92 | Train Loss: 0.3211191 Vali Loss: 0.5230256 Test Loss: 0.6459262
Validation loss decreased (0.523418 --> 0.523026).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 55.85262489318848
Epoch: 42, Steps: 92 | Train Loss: 0.3207592 Vali Loss: 0.5226178 Test Loss: 0.6454359
Validation loss decreased (0.523026 --> 0.522618).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 60.68077611923218
Epoch: 43, Steps: 92 | Train Loss: 0.3204581 Vali Loss: 0.5223228 Test Loss: 0.6450174
Validation loss decreased (0.522618 --> 0.522323).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 56.777645111083984
Epoch: 44, Steps: 92 | Train Loss: 0.3202409 Vali Loss: 0.5219982 Test Loss: 0.6446106
Validation loss decreased (0.522323 --> 0.521998).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 57.62454295158386
Epoch: 45, Steps: 92 | Train Loss: 0.3199933 Vali Loss: 0.5211939 Test Loss: 0.6442326
Validation loss decreased (0.521998 --> 0.521194).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 56.36022448539734
Epoch: 46, Steps: 92 | Train Loss: 0.3197672 Vali Loss: 0.5212557 Test Loss: 0.6438700
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 64.64759874343872
Epoch: 47, Steps: 92 | Train Loss: 0.3194821 Vali Loss: 0.5211030 Test Loss: 0.6435390
Validation loss decreased (0.521194 --> 0.521103).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 65.25874614715576
Epoch: 48, Steps: 92 | Train Loss: 0.3193710 Vali Loss: 0.5207668 Test Loss: 0.6432367
Validation loss decreased (0.521103 --> 0.520767).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 65.77462935447693
Epoch: 49, Steps: 92 | Train Loss: 0.3191443 Vali Loss: 0.5204458 Test Loss: 0.6429568
Validation loss decreased (0.520767 --> 0.520446).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 64.07820916175842
Epoch: 50, Steps: 92 | Train Loss: 0.3189206 Vali Loss: 0.5197891 Test Loss: 0.6426882
Validation loss decreased (0.520446 --> 0.519789).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 64.49982857704163
Epoch: 51, Steps: 92 | Train Loss: 0.3187372 Vali Loss: 0.5198073 Test Loss: 0.6424038
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 66.60113644599915
Epoch: 52, Steps: 92 | Train Loss: 0.3186285 Vali Loss: 0.5195267 Test Loss: 0.6421743
Validation loss decreased (0.519789 --> 0.519527).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 63.261759996414185
Epoch: 53, Steps: 92 | Train Loss: 0.3184652 Vali Loss: 0.5196865 Test Loss: 0.6419470
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 64.721510887146
Epoch: 54, Steps: 92 | Train Loss: 0.3182826 Vali Loss: 0.5190871 Test Loss: 0.6417070
Validation loss decreased (0.519527 --> 0.519087).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 66.58876729011536
Epoch: 55, Steps: 92 | Train Loss: 0.3181687 Vali Loss: 0.5187681 Test Loss: 0.6415122
Validation loss decreased (0.519087 --> 0.518768).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 63.73378014564514
Epoch: 56, Steps: 92 | Train Loss: 0.3180542 Vali Loss: 0.5189862 Test Loss: 0.6413211
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 65.1912522315979
Epoch: 57, Steps: 92 | Train Loss: 0.3179497 Vali Loss: 0.5188329 Test Loss: 0.6411259
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 66.38386797904968
Epoch: 58, Steps: 92 | Train Loss: 0.3177831 Vali Loss: 0.5181177 Test Loss: 0.6409459
Validation loss decreased (0.518768 --> 0.518118).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 60.60921001434326
Epoch: 59, Steps: 92 | Train Loss: 0.3177197 Vali Loss: 0.5178995 Test Loss: 0.6407898
Validation loss decreased (0.518118 --> 0.517899).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 61.71125555038452
Epoch: 60, Steps: 92 | Train Loss: 0.3175494 Vali Loss: 0.5180703 Test Loss: 0.6406286
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 64.26485681533813
Epoch: 61, Steps: 92 | Train Loss: 0.3175217 Vali Loss: 0.5174606 Test Loss: 0.6404884
Validation loss decreased (0.517899 --> 0.517461).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 60.76277947425842
Epoch: 62, Steps: 92 | Train Loss: 0.3173862 Vali Loss: 0.5179473 Test Loss: 0.6403555
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 61.91049098968506
Epoch: 63, Steps: 92 | Train Loss: 0.3173157 Vali Loss: 0.5175999 Test Loss: 0.6402254
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 62.62457084655762
Epoch: 64, Steps: 92 | Train Loss: 0.3172271 Vali Loss: 0.5179732 Test Loss: 0.6401054
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 59.36874771118164
Epoch: 65, Steps: 92 | Train Loss: 0.3170894 Vali Loss: 0.5177662 Test Loss: 0.6399832
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 56.61427044868469
Epoch: 66, Steps: 92 | Train Loss: 0.3170246 Vali Loss: 0.5176148 Test Loss: 0.6398813
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 49.930253982543945
Epoch: 67, Steps: 92 | Train Loss: 0.3168536 Vali Loss: 0.5170682 Test Loss: 0.6397704
Validation loss decreased (0.517461 --> 0.517068).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 48.74596285820007
Epoch: 68, Steps: 92 | Train Loss: 0.3168016 Vali Loss: 0.5171896 Test Loss: 0.6396734
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 49.80699324607849
Epoch: 69, Steps: 92 | Train Loss: 0.3168428 Vali Loss: 0.5170106 Test Loss: 0.6395885
Validation loss decreased (0.517068 --> 0.517011).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 48.357731342315674
Epoch: 70, Steps: 92 | Train Loss: 0.3166804 Vali Loss: 0.5168704 Test Loss: 0.6395015
Validation loss decreased (0.517011 --> 0.516870).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 47.310455560684204
Epoch: 71, Steps: 92 | Train Loss: 0.3166980 Vali Loss: 0.5169329 Test Loss: 0.6394124
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 50.310240268707275
Epoch: 72, Steps: 92 | Train Loss: 0.3166582 Vali Loss: 0.5170424 Test Loss: 0.6393275
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 46.42224597930908
Epoch: 73, Steps: 92 | Train Loss: 0.3166490 Vali Loss: 0.5168172 Test Loss: 0.6392691
Validation loss decreased (0.516870 --> 0.516817).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 50.00136113166809
Epoch: 74, Steps: 92 | Train Loss: 0.3164385 Vali Loss: 0.5165845 Test Loss: 0.6391881
Validation loss decreased (0.516817 --> 0.516585).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 49.93135118484497
Epoch: 75, Steps: 92 | Train Loss: 0.3163893 Vali Loss: 0.5163647 Test Loss: 0.6391250
Validation loss decreased (0.516585 --> 0.516365).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 46.259430170059204
Epoch: 76, Steps: 92 | Train Loss: 0.3165073 Vali Loss: 0.5167871 Test Loss: 0.6390702
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 48.658751487731934
Epoch: 77, Steps: 92 | Train Loss: 0.3163104 Vali Loss: 0.5166230 Test Loss: 0.6389971
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 48.715728998184204
Epoch: 78, Steps: 92 | Train Loss: 0.3163955 Vali Loss: 0.5163171 Test Loss: 0.6389455
Validation loss decreased (0.516365 --> 0.516317).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 48.907925605773926
Epoch: 79, Steps: 92 | Train Loss: 0.3162701 Vali Loss: 0.5163008 Test Loss: 0.6388928
Validation loss decreased (0.516317 --> 0.516301).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 50.491729497909546
Epoch: 80, Steps: 92 | Train Loss: 0.3162511 Vali Loss: 0.5166804 Test Loss: 0.6388339
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 48.94296932220459
Epoch: 81, Steps: 92 | Train Loss: 0.3162229 Vali Loss: 0.5159870 Test Loss: 0.6387866
Validation loss decreased (0.516301 --> 0.515987).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 48.92726492881775
Epoch: 82, Steps: 92 | Train Loss: 0.3161648 Vali Loss: 0.5161714 Test Loss: 0.6387358
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 51.09364461898804
Epoch: 83, Steps: 92 | Train Loss: 0.3160802 Vali Loss: 0.5158891 Test Loss: 0.6387002
Validation loss decreased (0.515987 --> 0.515889).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 49.2179811000824
Epoch: 84, Steps: 92 | Train Loss: 0.3161151 Vali Loss: 0.5160053 Test Loss: 0.6386603
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 48.26664137840271
Epoch: 85, Steps: 92 | Train Loss: 0.3160281 Vali Loss: 0.5160079 Test Loss: 0.6386188
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 48.97784733772278
Epoch: 86, Steps: 92 | Train Loss: 0.3159976 Vali Loss: 0.5166900 Test Loss: 0.6385841
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 46.77630925178528
Epoch: 87, Steps: 92 | Train Loss: 0.3160586 Vali Loss: 0.5164282 Test Loss: 0.6385534
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 47.60862636566162
Epoch: 88, Steps: 92 | Train Loss: 0.3160254 Vali Loss: 0.5158800 Test Loss: 0.6385165
Validation loss decreased (0.515889 --> 0.515880).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 58.96870565414429
Epoch: 89, Steps: 92 | Train Loss: 0.3158597 Vali Loss: 0.5160873 Test Loss: 0.6384857
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 53.44947171211243
Epoch: 90, Steps: 92 | Train Loss: 0.3160054 Vali Loss: 0.5161124 Test Loss: 0.6384575
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 53.80242657661438
Epoch: 91, Steps: 92 | Train Loss: 0.3160151 Vali Loss: 0.5155158 Test Loss: 0.6384250
Validation loss decreased (0.515880 --> 0.515516).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 55.14906311035156
Epoch: 92, Steps: 92 | Train Loss: 0.3159689 Vali Loss: 0.5158843 Test Loss: 0.6383973
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 52.55217003822327
Epoch: 93, Steps: 92 | Train Loss: 0.3158865 Vali Loss: 0.5159826 Test Loss: 0.6383694
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 55.241305112838745
Epoch: 94, Steps: 92 | Train Loss: 0.3159103 Vali Loss: 0.5162308 Test Loss: 0.6383473
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 55.04135990142822
Epoch: 95, Steps: 92 | Train Loss: 0.3158710 Vali Loss: 0.5163489 Test Loss: 0.6383230
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 52.90387940406799
Epoch: 96, Steps: 92 | Train Loss: 0.3157852 Vali Loss: 0.5155661 Test Loss: 0.6383032
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 56.1065468788147
Epoch: 97, Steps: 92 | Train Loss: 0.3159242 Vali Loss: 0.5159284 Test Loss: 0.6382835
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 53.45773768424988
Epoch: 98, Steps: 92 | Train Loss: 0.3157882 Vali Loss: 0.5156100 Test Loss: 0.6382623
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 52.886348724365234
Epoch: 99, Steps: 92 | Train Loss: 0.3158143 Vali Loss: 0.5160670 Test Loss: 0.6382443
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 53.22956895828247
Epoch: 100, Steps: 92 | Train Loss: 0.3158083 Vali Loss: 0.5158740 Test Loss: 0.6382244
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11855
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=30, out_features=142, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  470031360.0
params:  4402.0
Trainable parameters:  4402
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 41.60974931716919
Epoch: 1, Steps: 92 | Train Loss: 0.3909695 Vali Loss: 0.5119564 Test Loss: 0.6349885
Validation loss decreased (inf --> 0.511956).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 41.753862142562866
Epoch: 2, Steps: 92 | Train Loss: 0.3892917 Vali Loss: 0.5106909 Test Loss: 0.6339500
Validation loss decreased (0.511956 --> 0.510691).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 43.06759262084961
Epoch: 3, Steps: 92 | Train Loss: 0.3890136 Vali Loss: 0.5111952 Test Loss: 0.6343901
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00045125
Epoch: 4 cost time: 41.57039284706116
Epoch: 4, Steps: 92 | Train Loss: 0.3889831 Vali Loss: 0.5112230 Test Loss: 0.6342131
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 40.594886779785156
Epoch: 5, Steps: 92 | Train Loss: 0.3889480 Vali Loss: 0.5112482 Test Loss: 0.6341139
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 43.77148628234863
Epoch: 6, Steps: 92 | Train Loss: 0.3888702 Vali Loss: 0.5109907 Test Loss: 0.6337162
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 41.093488454818726
Epoch: 7, Steps: 92 | Train Loss: 0.3888767 Vali Loss: 0.5109583 Test Loss: 0.6341001
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 42.59940028190613
Epoch: 8, Steps: 92 | Train Loss: 0.3888365 Vali Loss: 0.5109253 Test Loss: 0.6341314
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 42.61983847618103
Epoch: 9, Steps: 92 | Train Loss: 0.3888491 Vali Loss: 0.5106207 Test Loss: 0.6337780
Validation loss decreased (0.510691 --> 0.510621).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 41.70618486404419
Epoch: 10, Steps: 92 | Train Loss: 0.3888287 Vali Loss: 0.5108046 Test Loss: 0.6339025
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 44.52928876876831
Epoch: 11, Steps: 92 | Train Loss: 0.3887880 Vali Loss: 0.5105842 Test Loss: 0.6338004
Validation loss decreased (0.510621 --> 0.510584).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 41.11959886550903
Epoch: 12, Steps: 92 | Train Loss: 0.3887742 Vali Loss: 0.5109534 Test Loss: 0.6339793
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 42.23529314994812
Epoch: 13, Steps: 92 | Train Loss: 0.3887307 Vali Loss: 0.5111414 Test Loss: 0.6337522
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 44.73530411720276
Epoch: 14, Steps: 92 | Train Loss: 0.3886392 Vali Loss: 0.5106123 Test Loss: 0.6338846
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 40.824584007263184
Epoch: 15, Steps: 92 | Train Loss: 0.3887678 Vali Loss: 0.5103020 Test Loss: 0.6338111
Validation loss decreased (0.510584 --> 0.510302).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 44.93206238746643
Epoch: 16, Steps: 92 | Train Loss: 0.3888239 Vali Loss: 0.5105690 Test Loss: 0.6337488
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 43.447680711746216
Epoch: 17, Steps: 92 | Train Loss: 0.3887529 Vali Loss: 0.5102205 Test Loss: 0.6335952
Validation loss decreased (0.510302 --> 0.510221).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 41.38923454284668
Epoch: 18, Steps: 92 | Train Loss: 0.3887888 Vali Loss: 0.5105095 Test Loss: 0.6334811
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 44.50814700126648
Epoch: 19, Steps: 92 | Train Loss: 0.3887696 Vali Loss: 0.5105193 Test Loss: 0.6337906
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 43.35969400405884
Epoch: 20, Steps: 92 | Train Loss: 0.3887695 Vali Loss: 0.5113897 Test Loss: 0.6339144
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 42.683682441711426
Epoch: 21, Steps: 92 | Train Loss: 0.3886571 Vali Loss: 0.5111255 Test Loss: 0.6336353
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 43.85902142524719
Epoch: 22, Steps: 92 | Train Loss: 0.3887278 Vali Loss: 0.5108231 Test Loss: 0.6335410
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 41.39739680290222
Epoch: 23, Steps: 92 | Train Loss: 0.3887440 Vali Loss: 0.5110955 Test Loss: 0.6338533
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 42.76416730880737
Epoch: 24, Steps: 92 | Train Loss: 0.3887806 Vali Loss: 0.5108128 Test Loss: 0.6337647
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 42.94053316116333
Epoch: 25, Steps: 92 | Train Loss: 0.3887963 Vali Loss: 0.5110584 Test Loss: 0.6336960
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 41.957677602767944
Epoch: 26, Steps: 92 | Train Loss: 0.3886934 Vali Loss: 0.5108519 Test Loss: 0.6336362
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 41.990052461624146
Epoch: 27, Steps: 92 | Train Loss: 0.3886138 Vali Loss: 0.5104636 Test Loss: 0.6337115
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j336_H5_FITS_custom_ftM_sl90_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.631759762763977, mae:0.3792135417461395, rse:0.6532437205314636, corr:[0.2674654  0.2800083  0.28026485 0.27857167 0.27823177 0.27941474
 0.28127012 0.2812269  0.28079793 0.28047878 0.2796927  0.27995536
 0.27966553 0.2789142  0.27932635 0.27984414 0.2801361  0.28110072
 0.28289846 0.2845978  0.28500637 0.28589192 0.28626004 0.28604248
 0.2904784  0.293275   0.29207197 0.29056638 0.2905001  0.2921332
 0.2937166  0.29533976 0.29573196 0.29560566 0.2956497  0.29530084
 0.2946472  0.2944846  0.29467413 0.29449156 0.2944525  0.2951028
 0.2962149  0.29651627 0.2957487  0.29562214 0.29505894 0.29390222
 0.29438767 0.29317567 0.29136148 0.2904368  0.2893597  0.2863227
 0.28410143 0.28522012 0.28540704 0.28501582 0.2846034  0.2831998
 0.2818614  0.28142044 0.28106695 0.28066993 0.28075555 0.2811621
 0.2816722  0.28197554 0.2821008  0.28240883 0.2819306  0.28036904
 0.27890557 0.27771685 0.2770724  0.27758622 0.27884632 0.27926508
 0.2798238  0.2822508  0.28274086 0.28207815 0.28145286 0.2801912
 0.27913812 0.27848014 0.27749944 0.27684122 0.27689585 0.2772669
 0.27780482 0.27802527 0.27820894 0.27890947 0.27927142 0.27902153
 0.27885863 0.27873278 0.2784564  0.27832574 0.2787941  0.27961743
 0.28050658 0.28167123 0.28139025 0.28025684 0.27965128 0.2790467
 0.27849934 0.27817214 0.2776061  0.27720234 0.27716616 0.27748322
 0.2783274  0.27884057 0.27900013 0.27962047 0.28001857 0.27993715
 0.28018725 0.28062728 0.28082412 0.28053376 0.2802722  0.28046662
 0.28051454 0.28022382 0.2794198  0.27855566 0.27832994 0.2779861
 0.27737007 0.2770771  0.27700186 0.27733073 0.27787775 0.27825832
 0.27913877 0.27988064 0.27996346 0.28087333 0.28129712 0.28008547
 0.27926275 0.2790228  0.27923667 0.2794057  0.27923796 0.27939004
 0.27976862 0.279817   0.27914324 0.2779474  0.27721295 0.27683464
 0.27648523 0.27648333 0.27669045 0.27748337 0.27882752 0.2796323
 0.2808615  0.28257042 0.28320011 0.28390923 0.28378665 0.2814158
 0.28029224 0.27868125 0.27682605 0.2761332  0.2763605  0.27784103
 0.2798451  0.28085214 0.28083587 0.28090495 0.281357   0.2816618
 0.28152752 0.2815082  0.28182355 0.2819899  0.282132   0.2826667
 0.28398752 0.2854125  0.28566554 0.2858712  0.2855753  0.28492844
 0.2883087  0.29065236 0.28948268 0.2880229  0.28799045 0.28940412
 0.29107076 0.29290178 0.29336464 0.29328343 0.29367465 0.29375982
 0.29337287 0.2930743  0.29305762 0.29304913 0.29317445 0.29348114
 0.29385585 0.2938458  0.29308367 0.29261708 0.29176718 0.2902667
 0.2901406  0.288799   0.2870793  0.28621805 0.2854223  0.28252295
 0.28015447 0.28119728 0.28168014 0.281626   0.28153095 0.2806033
 0.2796787  0.2791371  0.27849633 0.2781213  0.27829623 0.27845934
 0.27849177 0.2785666  0.27871674 0.27898222 0.27869746 0.27712926
 0.27550587 0.27436134 0.27384448 0.27451783 0.27611002 0.27701798
 0.27809876 0.28073284 0.28149936 0.28117362 0.2806403  0.27935776
 0.27827254 0.27752453 0.2766395  0.27617642 0.27626184 0.276521
 0.2767742  0.27683222 0.27687997 0.2772499  0.27743962 0.27725303
 0.2771979  0.27723488 0.27722546 0.27735072 0.27775678 0.27824423
 0.27873573 0.2797769  0.27988574 0.2794411  0.27925292 0.2786841
 0.2780864  0.27776575 0.2773549  0.27724975 0.27750158 0.27760082
 0.2777457  0.27788916 0.27797672 0.2783021  0.27843046 0.2783736
 0.2785964  0.2787569  0.27879763 0.27874196 0.2785768  0.27854797
 0.27867454 0.2788349  0.27844927 0.27783892 0.27784094 0.2777674
 0.2772468  0.27703127 0.2771549  0.2774105  0.2777432  0.27798167
 0.2784231  0.27866095 0.2787136  0.27948904 0.27930257 0.27751243
 0.27673343 0.276825   0.27696374 0.2770371  0.2771594  0.27757487
 0.27818426 0.27872398 0.2785268  0.27799675 0.27824867 0.27827623
 0.27768323 0.2776733  0.27797532 0.27794346 0.27801102 0.2783568
 0.27832818 0.27791256 0.27832642 0.27847248 0.27693707 0.27592814]
