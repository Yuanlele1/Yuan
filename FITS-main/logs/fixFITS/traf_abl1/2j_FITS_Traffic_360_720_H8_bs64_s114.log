Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=138, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j720_H8_FITS_custom_ftM_sl360_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11201
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=138, out_features=414, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6303716352.0
params:  57546.0
Trainable parameters:  57546
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 210.70934557914734
Epoch: 1, Steps: 87 | Train Loss: 1.4025946 Vali Loss: 1.3244042 Test Loss: 1.5737655
Validation loss decreased (inf --> 1.324404).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 209.25620818138123
Epoch: 2, Steps: 87 | Train Loss: 0.9089475 Vali Loss: 1.0741889 Test Loss: 1.2641381
Validation loss decreased (1.324404 --> 1.074189).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 210.82726407051086
Epoch: 3, Steps: 87 | Train Loss: 0.7653248 Vali Loss: 0.9844441 Test Loss: 1.1541259
Validation loss decreased (1.074189 --> 0.984444).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 231.14019298553467
Epoch: 4, Steps: 87 | Train Loss: 0.6959042 Vali Loss: 0.9253028 Test Loss: 1.0817698
Validation loss decreased (0.984444 --> 0.925303).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 224.5850658416748
Epoch: 5, Steps: 87 | Train Loss: 0.6449125 Vali Loss: 0.8773171 Test Loss: 1.0236048
Validation loss decreased (0.925303 --> 0.877317).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 236.6581506729126
Epoch: 6, Steps: 87 | Train Loss: 0.6027332 Vali Loss: 0.8349771 Test Loss: 0.9725716
Validation loss decreased (0.877317 --> 0.834977).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 232.2329568862915
Epoch: 7, Steps: 87 | Train Loss: 0.5666729 Vali Loss: 0.7980933 Test Loss: 0.9295105
Validation loss decreased (0.834977 --> 0.798093).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 219.05097603797913
Epoch: 8, Steps: 87 | Train Loss: 0.5352660 Vali Loss: 0.7658488 Test Loss: 0.8905884
Validation loss decreased (0.798093 --> 0.765849).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 164.27374529838562
Epoch: 9, Steps: 87 | Train Loss: 0.5078662 Vali Loss: 0.7362339 Test Loss: 0.8560469
Validation loss decreased (0.765849 --> 0.736234).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 156.88561391830444
Epoch: 10, Steps: 87 | Train Loss: 0.4836084 Vali Loss: 0.7107824 Test Loss: 0.8253474
Validation loss decreased (0.736234 --> 0.710782).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 208.60871028900146
Epoch: 11, Steps: 87 | Train Loss: 0.4620693 Vali Loss: 0.6877538 Test Loss: 0.7984585
Validation loss decreased (0.710782 --> 0.687754).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 256.2574121952057
Epoch: 12, Steps: 87 | Train Loss: 0.4428129 Vali Loss: 0.6671875 Test Loss: 0.7745138
Validation loss decreased (0.687754 --> 0.667187).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 254.9440462589264
Epoch: 13, Steps: 87 | Train Loss: 0.4255305 Vali Loss: 0.6489764 Test Loss: 0.7527558
Validation loss decreased (0.667187 --> 0.648976).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 314.1881594657898
Epoch: 14, Steps: 87 | Train Loss: 0.4100283 Vali Loss: 0.6316547 Test Loss: 0.7328344
Validation loss decreased (0.648976 --> 0.631655).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 282.63592743873596
Epoch: 15, Steps: 87 | Train Loss: 0.3959741 Vali Loss: 0.6173187 Test Loss: 0.7152925
Validation loss decreased (0.631655 --> 0.617319).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 317.1869578361511
Epoch: 16, Steps: 87 | Train Loss: 0.3832822 Vali Loss: 0.6026236 Test Loss: 0.6992174
Validation loss decreased (0.617319 --> 0.602624).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 339.3514964580536
Epoch: 17, Steps: 87 | Train Loss: 0.3717323 Vali Loss: 0.5897146 Test Loss: 0.6843960
Validation loss decreased (0.602624 --> 0.589715).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 305.5036287307739
Epoch: 18, Steps: 87 | Train Loss: 0.3612578 Vali Loss: 0.5794939 Test Loss: 0.6717171
Validation loss decreased (0.589715 --> 0.579494).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 241.00539445877075
Epoch: 19, Steps: 87 | Train Loss: 0.3516988 Vali Loss: 0.5694344 Test Loss: 0.6595812
Validation loss decreased (0.579494 --> 0.569434).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 231.9799280166626
Epoch: 20, Steps: 87 | Train Loss: 0.3429121 Vali Loss: 0.5597959 Test Loss: 0.6486611
Validation loss decreased (0.569434 --> 0.559796).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 230.38484454154968
Epoch: 21, Steps: 87 | Train Loss: 0.3349093 Vali Loss: 0.5508244 Test Loss: 0.6385432
Validation loss decreased (0.559796 --> 0.550824).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 208.76553392410278
Epoch: 22, Steps: 87 | Train Loss: 0.3275414 Vali Loss: 0.5428085 Test Loss: 0.6293023
Validation loss decreased (0.550824 --> 0.542809).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 201.69255232810974
Epoch: 23, Steps: 87 | Train Loss: 0.3208043 Vali Loss: 0.5360737 Test Loss: 0.6204771
Validation loss decreased (0.542809 --> 0.536074).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 204.58676838874817
Epoch: 24, Steps: 87 | Train Loss: 0.3145926 Vali Loss: 0.5289930 Test Loss: 0.6125910
Validation loss decreased (0.536074 --> 0.528993).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 219.76592659950256
Epoch: 25, Steps: 87 | Train Loss: 0.3088258 Vali Loss: 0.5223687 Test Loss: 0.6055589
Validation loss decreased (0.528993 --> 0.522369).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 201.91376423835754
Epoch: 26, Steps: 87 | Train Loss: 0.3035168 Vali Loss: 0.5165908 Test Loss: 0.5987049
Validation loss decreased (0.522369 --> 0.516591).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 214.08655405044556
Epoch: 27, Steps: 87 | Train Loss: 0.2987262 Vali Loss: 0.5118472 Test Loss: 0.5925524
Validation loss decreased (0.516591 --> 0.511847).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 184.58804202079773
Epoch: 28, Steps: 87 | Train Loss: 0.2940634 Vali Loss: 0.5070561 Test Loss: 0.5871920
Validation loss decreased (0.511847 --> 0.507056).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 238.5843198299408
Epoch: 29, Steps: 87 | Train Loss: 0.2898476 Vali Loss: 0.5019825 Test Loss: 0.5815548
Validation loss decreased (0.507056 --> 0.501982).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 234.68082237243652
Epoch: 30, Steps: 87 | Train Loss: 0.2859888 Vali Loss: 0.4978488 Test Loss: 0.5767338
Validation loss decreased (0.501982 --> 0.497849).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 218.012300491333
Epoch: 31, Steps: 87 | Train Loss: 0.2823230 Vali Loss: 0.4941285 Test Loss: 0.5722740
Validation loss decreased (0.497849 --> 0.494128).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 208.37528228759766
Epoch: 32, Steps: 87 | Train Loss: 0.2789521 Vali Loss: 0.4904944 Test Loss: 0.5680036
Validation loss decreased (0.494128 --> 0.490494).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 200.41287636756897
Epoch: 33, Steps: 87 | Train Loss: 0.2758024 Vali Loss: 0.4866855 Test Loss: 0.5640972
Validation loss decreased (0.490494 --> 0.486686).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 197.85745859146118
Epoch: 34, Steps: 87 | Train Loss: 0.2728245 Vali Loss: 0.4839227 Test Loss: 0.5604333
Validation loss decreased (0.486686 --> 0.483923).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 152.17105078697205
Epoch: 35, Steps: 87 | Train Loss: 0.2700258 Vali Loss: 0.4806154 Test Loss: 0.5570056
Validation loss decreased (0.483923 --> 0.480615).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 235.04727387428284
Epoch: 36, Steps: 87 | Train Loss: 0.2674989 Vali Loss: 0.4776899 Test Loss: 0.5536257
Validation loss decreased (0.480615 --> 0.477690).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 267.1499915122986
Epoch: 37, Steps: 87 | Train Loss: 0.2650457 Vali Loss: 0.4750701 Test Loss: 0.5507975
Validation loss decreased (0.477690 --> 0.475070).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 208.0418667793274
Epoch: 38, Steps: 87 | Train Loss: 0.2628780 Vali Loss: 0.4732751 Test Loss: 0.5478548
Validation loss decreased (0.475070 --> 0.473275).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 197.37412977218628
Epoch: 39, Steps: 87 | Train Loss: 0.2607511 Vali Loss: 0.4702293 Test Loss: 0.5452418
Validation loss decreased (0.473275 --> 0.470229).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 249.65673112869263
Epoch: 40, Steps: 87 | Train Loss: 0.2587008 Vali Loss: 0.4687338 Test Loss: 0.5427287
Validation loss decreased (0.470229 --> 0.468734).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 250.99534010887146
Epoch: 41, Steps: 87 | Train Loss: 0.2568678 Vali Loss: 0.4662791 Test Loss: 0.5404862
Validation loss decreased (0.468734 --> 0.466279).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 204.89808440208435
Epoch: 42, Steps: 87 | Train Loss: 0.2551108 Vali Loss: 0.4642145 Test Loss: 0.5382400
Validation loss decreased (0.466279 --> 0.464215).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 269.49434900283813
Epoch: 43, Steps: 87 | Train Loss: 0.2534838 Vali Loss: 0.4629093 Test Loss: 0.5362718
Validation loss decreased (0.464215 --> 0.462909).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 262.1314334869385
Epoch: 44, Steps: 87 | Train Loss: 0.2519175 Vali Loss: 0.4609690 Test Loss: 0.5342883
Validation loss decreased (0.462909 --> 0.460969).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 266.2512860298157
Epoch: 45, Steps: 87 | Train Loss: 0.2503975 Vali Loss: 0.4600033 Test Loss: 0.5325044
Validation loss decreased (0.460969 --> 0.460003).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 264.8325698375702
Epoch: 46, Steps: 87 | Train Loss: 0.2490240 Vali Loss: 0.4581900 Test Loss: 0.5308130
Validation loss decreased (0.460003 --> 0.458190).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 265.399028301239
Epoch: 47, Steps: 87 | Train Loss: 0.2477803 Vali Loss: 0.4563923 Test Loss: 0.5291452
Validation loss decreased (0.458190 --> 0.456392).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 255.7166826725006
Epoch: 48, Steps: 87 | Train Loss: 0.2465547 Vali Loss: 0.4553719 Test Loss: 0.5276163
Validation loss decreased (0.456392 --> 0.455372).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 264.7916612625122
Epoch: 49, Steps: 87 | Train Loss: 0.2454157 Vali Loss: 0.4538372 Test Loss: 0.5262254
Validation loss decreased (0.455372 --> 0.453837).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 254.01103448867798
Epoch: 50, Steps: 87 | Train Loss: 0.2443088 Vali Loss: 0.4527657 Test Loss: 0.5248485
Validation loss decreased (0.453837 --> 0.452766).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 226.64376711845398
Epoch: 51, Steps: 87 | Train Loss: 0.2432922 Vali Loss: 0.4514928 Test Loss: 0.5235443
Validation loss decreased (0.452766 --> 0.451493).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 219.6036958694458
Epoch: 52, Steps: 87 | Train Loss: 0.2422737 Vali Loss: 0.4503404 Test Loss: 0.5223384
Validation loss decreased (0.451493 --> 0.450340).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 215.60039639472961
Epoch: 53, Steps: 87 | Train Loss: 0.2413476 Vali Loss: 0.4498823 Test Loss: 0.5212122
Validation loss decreased (0.450340 --> 0.449882).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 196.88578009605408
Epoch: 54, Steps: 87 | Train Loss: 0.2405528 Vali Loss: 0.4485949 Test Loss: 0.5201296
Validation loss decreased (0.449882 --> 0.448595).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 195.56805419921875
Epoch: 55, Steps: 87 | Train Loss: 0.2396938 Vali Loss: 0.4480541 Test Loss: 0.5191205
Validation loss decreased (0.448595 --> 0.448054).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 202.051775932312
Epoch: 56, Steps: 87 | Train Loss: 0.2388748 Vali Loss: 0.4469691 Test Loss: 0.5181785
Validation loss decreased (0.448054 --> 0.446969).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 213.07798957824707
Epoch: 57, Steps: 87 | Train Loss: 0.2382227 Vali Loss: 0.4463293 Test Loss: 0.5172816
Validation loss decreased (0.446969 --> 0.446329).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 204.96956729888916
Epoch: 58, Steps: 87 | Train Loss: 0.2374745 Vali Loss: 0.4451640 Test Loss: 0.5163819
Validation loss decreased (0.446329 --> 0.445164).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 187.44514226913452
Epoch: 59, Steps: 87 | Train Loss: 0.2368021 Vali Loss: 0.4446848 Test Loss: 0.5155759
Validation loss decreased (0.445164 --> 0.444685).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 170.69993805885315
Epoch: 60, Steps: 87 | Train Loss: 0.2361778 Vali Loss: 0.4439384 Test Loss: 0.5147837
Validation loss decreased (0.444685 --> 0.443938).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 152.71587133407593
Epoch: 61, Steps: 87 | Train Loss: 0.2356328 Vali Loss: 0.4432145 Test Loss: 0.5140570
Validation loss decreased (0.443938 --> 0.443215).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 166.4507336616516
Epoch: 62, Steps: 87 | Train Loss: 0.2350373 Vali Loss: 0.4432662 Test Loss: 0.5133722
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 170.15934658050537
Epoch: 63, Steps: 87 | Train Loss: 0.2344950 Vali Loss: 0.4423261 Test Loss: 0.5126991
Validation loss decreased (0.443215 --> 0.442326).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 166.97295498847961
Epoch: 64, Steps: 87 | Train Loss: 0.2339990 Vali Loss: 0.4421260 Test Loss: 0.5120760
Validation loss decreased (0.442326 --> 0.442126).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 164.11168503761292
Epoch: 65, Steps: 87 | Train Loss: 0.2335014 Vali Loss: 0.4412124 Test Loss: 0.5114702
Validation loss decreased (0.442126 --> 0.441212).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 155.07168412208557
Epoch: 66, Steps: 87 | Train Loss: 0.2330731 Vali Loss: 0.4407990 Test Loss: 0.5109000
Validation loss decreased (0.441212 --> 0.440799).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 157.94911336898804
Epoch: 67, Steps: 87 | Train Loss: 0.2325782 Vali Loss: 0.4405276 Test Loss: 0.5103757
Validation loss decreased (0.440799 --> 0.440528).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 160.3966760635376
Epoch: 68, Steps: 87 | Train Loss: 0.2321905 Vali Loss: 0.4400405 Test Loss: 0.5098816
Validation loss decreased (0.440528 --> 0.440040).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 154.89641547203064
Epoch: 69, Steps: 87 | Train Loss: 0.2317630 Vali Loss: 0.4392698 Test Loss: 0.5093798
Validation loss decreased (0.440040 --> 0.439270).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 159.1087875366211
Epoch: 70, Steps: 87 | Train Loss: 0.2314175 Vali Loss: 0.4386806 Test Loss: 0.5089154
Validation loss decreased (0.439270 --> 0.438681).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 151.96036171913147
Epoch: 71, Steps: 87 | Train Loss: 0.2311119 Vali Loss: 0.4389477 Test Loss: 0.5084615
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 179.10544395446777
Epoch: 72, Steps: 87 | Train Loss: 0.2307272 Vali Loss: 0.4386470 Test Loss: 0.5080851
Validation loss decreased (0.438681 --> 0.438647).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 177.12840342521667
Epoch: 73, Steps: 87 | Train Loss: 0.2304290 Vali Loss: 0.4378070 Test Loss: 0.5076781
Validation loss decreased (0.438647 --> 0.437807).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 165.01473426818848
Epoch: 74, Steps: 87 | Train Loss: 0.2301148 Vali Loss: 0.4376176 Test Loss: 0.5073075
Validation loss decreased (0.437807 --> 0.437618).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 159.41376328468323
Epoch: 75, Steps: 87 | Train Loss: 0.2298463 Vali Loss: 0.4371931 Test Loss: 0.5069557
Validation loss decreased (0.437618 --> 0.437193).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 169.75106596946716
Epoch: 76, Steps: 87 | Train Loss: 0.2295574 Vali Loss: 0.4367074 Test Loss: 0.5066254
Validation loss decreased (0.437193 --> 0.436707).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 139.64247012138367
Epoch: 77, Steps: 87 | Train Loss: 0.2293247 Vali Loss: 0.4370688 Test Loss: 0.5063040
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 134.94928741455078
Epoch: 78, Steps: 87 | Train Loss: 0.2289985 Vali Loss: 0.4362724 Test Loss: 0.5060068
Validation loss decreased (0.436707 --> 0.436272).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 132.05134773254395
Epoch: 79, Steps: 87 | Train Loss: 0.2288383 Vali Loss: 0.4362257 Test Loss: 0.5057200
Validation loss decreased (0.436272 --> 0.436226).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 107.42561388015747
Epoch: 80, Steps: 87 | Train Loss: 0.2286665 Vali Loss: 0.4366044 Test Loss: 0.5054467
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 196.26908373832703
Epoch: 81, Steps: 87 | Train Loss: 0.2284178 Vali Loss: 0.4357893 Test Loss: 0.5051836
Validation loss decreased (0.436226 --> 0.435789).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 188.6201913356781
Epoch: 82, Steps: 87 | Train Loss: 0.2281961 Vali Loss: 0.4352055 Test Loss: 0.5049476
Validation loss decreased (0.435789 --> 0.435205).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 186.97023725509644
Epoch: 83, Steps: 87 | Train Loss: 0.2279871 Vali Loss: 0.4347516 Test Loss: 0.5047107
Validation loss decreased (0.435205 --> 0.434752).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 184.61067914962769
Epoch: 84, Steps: 87 | Train Loss: 0.2277968 Vali Loss: 0.4357941 Test Loss: 0.5044908
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 167.90338969230652
Epoch: 85, Steps: 87 | Train Loss: 0.2276958 Vali Loss: 0.4354386 Test Loss: 0.5042830
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 173.7371678352356
Epoch: 86, Steps: 87 | Train Loss: 0.2274971 Vali Loss: 0.4351127 Test Loss: 0.5040794
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 216.45034742355347
Epoch: 87, Steps: 87 | Train Loss: 0.2273880 Vali Loss: 0.4345174 Test Loss: 0.5038798
Validation loss decreased (0.434752 --> 0.434517).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 235.28089237213135
Epoch: 88, Steps: 87 | Train Loss: 0.2272195 Vali Loss: 0.4346159 Test Loss: 0.5037103
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 194.3854260444641
Epoch: 89, Steps: 87 | Train Loss: 0.2270100 Vali Loss: 0.4347328 Test Loss: 0.5035398
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 203.67767691612244
Epoch: 90, Steps: 87 | Train Loss: 0.2269089 Vali Loss: 0.4343959 Test Loss: 0.5033705
Validation loss decreased (0.434517 --> 0.434396).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 203.8714017868042
Epoch: 91, Steps: 87 | Train Loss: 0.2267877 Vali Loss: 0.4339191 Test Loss: 0.5032186
Validation loss decreased (0.434396 --> 0.433919).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 173.50180435180664
Epoch: 92, Steps: 87 | Train Loss: 0.2266692 Vali Loss: 0.4339971 Test Loss: 0.5030760
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 170.03339791297913
Epoch: 93, Steps: 87 | Train Loss: 0.2265548 Vali Loss: 0.4333546 Test Loss: 0.5029355
Validation loss decreased (0.433919 --> 0.433355).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 124.22223091125488
Epoch: 94, Steps: 87 | Train Loss: 0.2264650 Vali Loss: 0.4331952 Test Loss: 0.5027989
Validation loss decreased (0.433355 --> 0.433195).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 118.53738403320312
Epoch: 95, Steps: 87 | Train Loss: 0.2263692 Vali Loss: 0.4336969 Test Loss: 0.5026751
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 124.31651854515076
Epoch: 96, Steps: 87 | Train Loss: 0.2262585 Vali Loss: 0.4336901 Test Loss: 0.5025529
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 123.5173692703247
Epoch: 97, Steps: 87 | Train Loss: 0.2261427 Vali Loss: 0.4333011 Test Loss: 0.5024371
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 129.46474313735962
Epoch: 98, Steps: 87 | Train Loss: 0.2260815 Vali Loss: 0.4334015 Test Loss: 0.5023303
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 125.50351762771606
Epoch: 99, Steps: 87 | Train Loss: 0.2259784 Vali Loss: 0.4331660 Test Loss: 0.5022274
Validation loss decreased (0.433195 --> 0.433166).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 119.51810312271118
Epoch: 100, Steps: 87 | Train Loss: 0.2259183 Vali Loss: 0.4336217 Test Loss: 0.5021367
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11201
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=138, out_features=414, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6303716352.0
params:  57546.0
Trainable parameters:  57546
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 109.44043231010437
Epoch: 1, Steps: 87 | Train Loss: 0.3009803 Vali Loss: 0.4062939 Test Loss: 0.4717938
Validation loss decreased (inf --> 0.406294).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 114.34901690483093
Epoch: 2, Steps: 87 | Train Loss: 0.2876666 Vali Loss: 0.3997876 Test Loss: 0.4647798
Validation loss decreased (0.406294 --> 0.399788).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 110.99213337898254
Epoch: 3, Steps: 87 | Train Loss: 0.2846050 Vali Loss: 0.3992319 Test Loss: 0.4645307
Validation loss decreased (0.399788 --> 0.399232).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 110.97067999839783
Epoch: 4, Steps: 87 | Train Loss: 0.2840562 Vali Loss: 0.3995932 Test Loss: 0.4639701
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 122.6660304069519
Epoch: 5, Steps: 87 | Train Loss: 0.2839970 Vali Loss: 0.3994244 Test Loss: 0.4638960
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 102.7746913433075
Epoch: 6, Steps: 87 | Train Loss: 0.2838878 Vali Loss: 0.3996988 Test Loss: 0.4641589
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 102.1460816860199
Epoch: 7, Steps: 87 | Train Loss: 0.2839473 Vali Loss: 0.3988032 Test Loss: 0.4644520
Validation loss decreased (0.399232 --> 0.398803).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 104.20412921905518
Epoch: 8, Steps: 87 | Train Loss: 0.2838127 Vali Loss: 0.3993879 Test Loss: 0.4641708
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 101.89027190208435
Epoch: 9, Steps: 87 | Train Loss: 0.2838478 Vali Loss: 0.3992703 Test Loss: 0.4642600
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 102.61251282691956
Epoch: 10, Steps: 87 | Train Loss: 0.2838158 Vali Loss: 0.3989772 Test Loss: 0.4637139
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 109.22813057899475
Epoch: 11, Steps: 87 | Train Loss: 0.2837859 Vali Loss: 0.3991387 Test Loss: 0.4638575
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 114.77462673187256
Epoch: 12, Steps: 87 | Train Loss: 0.2837639 Vali Loss: 0.3990870 Test Loss: 0.4636666
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 114.63880586624146
Epoch: 13, Steps: 87 | Train Loss: 0.2837145 Vali Loss: 0.3989516 Test Loss: 0.4636717
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 108.5519061088562
Epoch: 14, Steps: 87 | Train Loss: 0.2837421 Vali Loss: 0.3989137 Test Loss: 0.4638068
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 109.79549050331116
Epoch: 15, Steps: 87 | Train Loss: 0.2837240 Vali Loss: 0.3992337 Test Loss: 0.4638169
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 115.25024461746216
Epoch: 16, Steps: 87 | Train Loss: 0.2837569 Vali Loss: 0.3995658 Test Loss: 0.4637804
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 117.32525181770325
Epoch: 17, Steps: 87 | Train Loss: 0.2836662 Vali Loss: 0.3991495 Test Loss: 0.4637667
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j720_H8_FITS_custom_ftM_sl360_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.46374377608299255, mae:0.3075282871723175, rse:0.5568521022796631, corr:[0.25556952 0.26647866 0.26693997 0.26621506 0.26676634 0.2666416
 0.26677462 0.26689297 0.26625893 0.26617184 0.266273   0.2661915
 0.2666336  0.2666588  0.26648927 0.2666586  0.26632062 0.26597828
 0.26613885 0.26619864 0.26616102 0.26582974 0.2654204  0.26628986
 0.26775357 0.26771194 0.2677797  0.26769558 0.2672574  0.26726118
 0.2671208  0.26690567 0.2672159  0.26716802 0.2669799  0.26717857
 0.26689324 0.26661333 0.26718006 0.26737505 0.2670035  0.26696536
 0.2670504  0.2670508  0.26701    0.26678324 0.26662844 0.266937
 0.26737007 0.2672984  0.2673573  0.2675012  0.26741946 0.26717255
 0.26709983 0.26721075 0.26716265 0.26703098 0.2668384  0.26635703
 0.26610866 0.2662741  0.26645666 0.26666048 0.266673   0.26653022
 0.26688516 0.266977   0.2662019  0.2658752  0.26602885 0.26596367
 0.26610953 0.26632962 0.26612008 0.26609632 0.26629156 0.26599753
 0.26560804 0.26556566 0.26570427 0.2660648  0.26628935 0.26627937
 0.26627988 0.2660243  0.26588106 0.26626787 0.26630622 0.265995
 0.26643562 0.26670468 0.26654872 0.26657164 0.2664339  0.26642194
 0.26664495 0.26659882 0.26638463 0.2664461  0.26615655 0.26555726
 0.26526925 0.26529598 0.26570806 0.2661644  0.26594508 0.2656376
 0.2657858  0.26600248 0.26644933 0.26691425 0.26658225 0.2659199
 0.26581934 0.2658402  0.26606488 0.26623392 0.26615465 0.26632518
 0.2663316  0.26613465 0.26610276 0.2660763  0.26577947 0.26567498
 0.26584414 0.26602745 0.26639193 0.26674944 0.266564   0.2658549
 0.26537412 0.26570657 0.2662262  0.26632038 0.2664592  0.2663541
 0.26640627 0.26653814 0.2665941  0.26608425 0.26577327 0.26629305
 0.26642355 0.26619506 0.26631662 0.266644   0.26659942 0.2661397
 0.2657355  0.26605412 0.26653072 0.26668024 0.26705086 0.26744068
 0.26720235 0.26691553 0.2670944  0.26741347 0.26758826 0.26761806
 0.26773956 0.26797956 0.26799527 0.26753634 0.2671036  0.26779133
 0.26908597 0.26874727 0.26800585 0.26784882 0.26804003 0.26799586
 0.26763248 0.26742247 0.26780066 0.26822537 0.26793814 0.26729205
 0.26710668 0.26726618 0.26762792 0.26824215 0.2683918  0.2680521
 0.2680992  0.2682483  0.26779324 0.2674215  0.26731518 0.26713392
 0.26756516 0.26774353 0.2680208  0.26843578 0.2683647  0.26802108
 0.268342   0.26894286 0.26907814 0.26865956 0.26785365 0.2674367
 0.26763335 0.2676502  0.26777232 0.26826978 0.26842743 0.26839364
 0.2685034  0.26839942 0.26806015 0.2676993  0.26741618 0.26765746
 0.2680328  0.26767442 0.2675863  0.26803324 0.26793444 0.2673653
 0.26728538 0.26755428 0.26779744 0.26803368 0.2679247  0.26738194
 0.26702628 0.26708445 0.26720706 0.26709712 0.26712435 0.26757342
 0.2678307  0.26757088 0.26734805 0.26727477 0.26722586 0.26735604
 0.26766282 0.26776198 0.2675757  0.26751998 0.2674533  0.2669782
 0.2667621  0.2672176  0.2675154  0.2674655  0.26750624 0.26747784
 0.26726517 0.2672518  0.26721177 0.267058   0.26741076 0.26792428
 0.26799223 0.26800987 0.26814228 0.26797178 0.2676418  0.26738796
 0.2671805  0.26738968 0.26760888 0.26738116 0.26701954 0.26705074
 0.2673295  0.26744303 0.26759085 0.26815584 0.26816767 0.26717976
 0.2664534  0.2665765  0.26694384 0.2672567  0.26745746 0.26741347
 0.26749432 0.2674304  0.26732224 0.26744995 0.26737025 0.26705778
 0.26689965 0.2670079  0.26708508 0.2675955  0.26782435 0.26748225
 0.26750365 0.26749483 0.26712334 0.2673493  0.26756766 0.2669878
 0.2667975  0.26726052 0.26730227 0.26717982 0.26749125 0.2677575
 0.26805893 0.26785308 0.26740435 0.26748925 0.26799256 0.26838118
 0.26831254 0.2680668  0.2681465  0.26867676 0.26883328 0.26860386
 0.26849714 0.26852873 0.26859912 0.2686499  0.26856956 0.2686697
 0.26895034 0.26892865 0.26893592 0.2693967  0.26959935 0.26932904
 0.26956597 0.26989308 0.26953128 0.26906753 0.26910007 0.26965305
 0.26999724 0.26936406 0.26894733 0.26920816 0.26943958 0.26928142
 0.26907343 0.26921752 0.26955286 0.2694866  0.26925674 0.269269
 0.26921943 0.26913095 0.26918802 0.26911542 0.26912504 0.269387
 0.2694065  0.26916337 0.26895705 0.26900882 0.2692248  0.2692322
 0.26920888 0.26919997 0.26952258 0.26945224 0.26892155 0.2685708
 0.26866752 0.26901597 0.26914644 0.26893243 0.26887524 0.2691512
 0.26935017 0.26937136 0.26934168 0.26935497 0.26947471 0.2694624
 0.26914284 0.2687785  0.26840842 0.26811537 0.2681451  0.2683751
 0.2684946  0.2685004  0.2687653  0.26886898 0.26848587 0.26805562
 0.26805648 0.26837662 0.26866147 0.2686925  0.2686305  0.26885232
 0.26903984 0.2686738  0.26834702 0.26849896 0.26858565 0.2685367
 0.26859492 0.2684563  0.2682058  0.26801565 0.26751587 0.26734245
 0.26785642 0.26802212 0.26791257 0.2681955  0.26790023 0.26702496
 0.26708743 0.26774174 0.26801047 0.26821485 0.26825637 0.26805177
 0.26812622 0.26820073 0.2681882  0.26836273 0.26829913 0.26820534
 0.2685988  0.2687057  0.26859808 0.26863825 0.26819533 0.26780432
 0.267897   0.26786038 0.26774198 0.26794022 0.26773402 0.26737964
 0.2676066  0.26796678 0.2683398  0.2687408  0.268635   0.26829094
 0.26800954 0.26761976 0.26783466 0.2683841  0.26791266 0.26721475
 0.26748684 0.26763415 0.26748657 0.2673799  0.2671623  0.26711035
 0.267105   0.2673055  0.26773512 0.2679604  0.26781458 0.26784658
 0.2679789  0.26799712 0.2681802  0.2683027  0.26820013 0.26809657
 0.26792365 0.26787257 0.26800537 0.26769128 0.2672773  0.26747587
 0.26785704 0.26801014 0.26800203 0.2678232  0.26818427 0.26886296
 0.26849467 0.26803008 0.26872852 0.26945835 0.26927763 0.26854318
 0.26765478 0.2673183  0.26764312 0.26795498 0.26839164 0.26893315
 0.26902226 0.26889548 0.26869896 0.2684437  0.26859677 0.26884416
 0.26871583 0.26896954 0.26938596 0.26892713 0.26845318 0.26898
 0.2695018  0.2687947  0.26844674 0.26876432 0.26894772 0.26857758
 0.26809654 0.26804549 0.26823878 0.26832032 0.26838586 0.26861715
 0.26881918 0.26857585 0.26810497 0.26796612 0.26789013 0.26752183
 0.26735523 0.26747447 0.26739395 0.26740682 0.2675653  0.2675703
 0.26776454 0.26787016 0.26839548 0.26906604 0.26878372 0.2679397
 0.26798746 0.2683227  0.2679611  0.26770353 0.26780528 0.26786575
 0.26797804 0.26792103 0.26776266 0.26786974 0.26777855 0.2674403
 0.26735577 0.26734352 0.2673914  0.2676914  0.26774982 0.26776767
 0.26802447 0.2678614  0.26767766 0.26786134 0.2676871  0.26743475
 0.26767498 0.2676067  0.26733494 0.26752377 0.2675535  0.26752156
 0.26774332 0.26740658 0.2669381  0.2671258  0.2673294  0.2672651
 0.26714626 0.267051   0.2672346  0.26726067 0.2667815  0.26660836
 0.26672032 0.26676306 0.26702368 0.26696748 0.26635224 0.26599738
 0.26586878 0.26580864 0.2663136  0.26702207 0.26752874 0.2678271
 0.2675273  0.26702228 0.26677027 0.26613867 0.2654913  0.2655722
 0.26572505 0.26578024 0.26595053 0.26565704 0.26509106 0.2649532
 0.26502603 0.2652181  0.2653381  0.26509076 0.26495498 0.2650794
 0.2652211  0.26560193 0.26595095 0.266036   0.26623255 0.26621664
 0.2659082  0.26593187 0.2656784  0.26496083 0.26502636 0.26522157
 0.2645393  0.2642173  0.2645809  0.26470292 0.2648026  0.26492783
 0.26498902 0.26537186 0.26538146 0.26518896 0.26533324 0.26530012
 0.26515543 0.26525137 0.26521638 0.26539326 0.26586396 0.26573446
 0.2652721  0.26521832 0.26523414 0.26512113 0.26507077 0.26498556
 0.26494458 0.2649004  0.2647073  0.26483065 0.26529828 0.26569864
 0.2660893  0.26632184 0.26668572 0.2674579  0.26735857 0.26630595
 0.2657991  0.2658346  0.2659826  0.26619127 0.26612744 0.26638886
 0.2670825  0.2669773  0.26641807 0.26627836 0.26618138 0.26640084
 0.26698503 0.2669993  0.2667589  0.26686475 0.26671377 0.26680443
 0.26729923 0.26668048 0.26680472 0.26759535 0.2674913  0.26708815
 0.26694718 0.26670995 0.2669673  0.2672244  0.26716107 0.26745817
 0.26735032 0.26683643 0.2667339  0.26634943 0.2662212  0.26687095
 0.26690742 0.2668637  0.26737627 0.2672595  0.26687622 0.26715317
 0.26754516 0.26755223 0.26783162 0.2675615  0.26741788 0.26752225
 0.26708892 0.26713225 0.26739153 0.26717228 0.26753506 0.2678928
 0.26813862 0.26880264 0.26872137 0.2686769  0.2688647  0.2680617
 0.26835313 0.26801845 0.26647386 0.26756898 0.26560622 0.2688076 ]
