Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6007795200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 185.29384183883667
Epoch: 1, Steps: 84 | Train Loss: 1.2746090 Vali Loss: 1.2859464 Test Loss: 1.5187593
Validation loss decreased (inf --> 1.285946).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 183.49215269088745
Epoch: 2, Steps: 84 | Train Loss: 0.9219673 Vali Loss: 1.1038191 Test Loss: 1.2964489
Validation loss decreased (1.285946 --> 1.103819).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 181.52299857139587
Epoch: 3, Steps: 84 | Train Loss: 0.8041567 Vali Loss: 1.0281272 Test Loss: 1.2046468
Validation loss decreased (1.103819 --> 1.028127).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 177.5953950881958
Epoch: 4, Steps: 84 | Train Loss: 0.7355616 Vali Loss: 0.9700654 Test Loss: 1.1357127
Validation loss decreased (1.028127 --> 0.970065).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 178.51366996765137
Epoch: 5, Steps: 84 | Train Loss: 0.6823426 Vali Loss: 0.9240780 Test Loss: 1.0810505
Validation loss decreased (0.970065 --> 0.924078).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 173.28049278259277
Epoch: 6, Steps: 84 | Train Loss: 0.6376727 Vali Loss: 0.8845679 Test Loss: 1.0349827
Validation loss decreased (0.924078 --> 0.884568).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 176.47594499588013
Epoch: 7, Steps: 84 | Train Loss: 0.5992448 Vali Loss: 0.8469107 Test Loss: 0.9913256
Validation loss decreased (0.884568 --> 0.846911).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 174.26507997512817
Epoch: 8, Steps: 84 | Train Loss: 0.5656480 Vali Loss: 0.8176750 Test Loss: 0.9563208
Validation loss decreased (0.846911 --> 0.817675).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 164.4399552345276
Epoch: 9, Steps: 84 | Train Loss: 0.5359323 Vali Loss: 0.7888219 Test Loss: 0.9219910
Validation loss decreased (0.817675 --> 0.788822).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 169.54289031028748
Epoch: 10, Steps: 84 | Train Loss: 0.5096339 Vali Loss: 0.7627358 Test Loss: 0.8917353
Validation loss decreased (0.788822 --> 0.762736).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 168.30643320083618
Epoch: 11, Steps: 84 | Train Loss: 0.4861744 Vali Loss: 0.7385830 Test Loss: 0.8634793
Validation loss decreased (0.762736 --> 0.738583).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 180.78862619400024
Epoch: 12, Steps: 84 | Train Loss: 0.4650838 Vali Loss: 0.7172461 Test Loss: 0.8381754
Validation loss decreased (0.738583 --> 0.717246).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 163.22502040863037
Epoch: 13, Steps: 84 | Train Loss: 0.4460695 Vali Loss: 0.6988068 Test Loss: 0.8167355
Validation loss decreased (0.717246 --> 0.698807).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 157.93698596954346
Epoch: 14, Steps: 84 | Train Loss: 0.4288850 Vali Loss: 0.6807750 Test Loss: 0.7961821
Validation loss decreased (0.698807 --> 0.680775).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 165.49106550216675
Epoch: 15, Steps: 84 | Train Loss: 0.4133254 Vali Loss: 0.6650171 Test Loss: 0.7768031
Validation loss decreased (0.680775 --> 0.665017).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 155.12771844863892
Epoch: 16, Steps: 84 | Train Loss: 0.3991312 Vali Loss: 0.6502615 Test Loss: 0.7597718
Validation loss decreased (0.665017 --> 0.650262).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 160.08021354675293
Epoch: 17, Steps: 84 | Train Loss: 0.3861984 Vali Loss: 0.6362828 Test Loss: 0.7432514
Validation loss decreased (0.650262 --> 0.636283).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 164.2344422340393
Epoch: 18, Steps: 84 | Train Loss: 0.3743536 Vali Loss: 0.6243873 Test Loss: 0.7290478
Validation loss decreased (0.636283 --> 0.624387).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 157.1218454837799
Epoch: 19, Steps: 84 | Train Loss: 0.3635072 Vali Loss: 0.6120003 Test Loss: 0.7145913
Validation loss decreased (0.624387 --> 0.612000).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 155.44825100898743
Epoch: 20, Steps: 84 | Train Loss: 0.3536075 Vali Loss: 0.6023055 Test Loss: 0.7031063
Validation loss decreased (0.612000 --> 0.602306).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 154.67267656326294
Epoch: 21, Steps: 84 | Train Loss: 0.3443781 Vali Loss: 0.5916513 Test Loss: 0.6911692
Validation loss decreased (0.602306 --> 0.591651).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 156.32918882369995
Epoch: 22, Steps: 84 | Train Loss: 0.3360292 Vali Loss: 0.5829268 Test Loss: 0.6808698
Validation loss decreased (0.591651 --> 0.582927).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 165.07496643066406
Epoch: 23, Steps: 84 | Train Loss: 0.3281435 Vali Loss: 0.5742196 Test Loss: 0.6699666
Validation loss decreased (0.582927 --> 0.574220).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 168.63413095474243
Epoch: 24, Steps: 84 | Train Loss: 0.3210143 Vali Loss: 0.5666243 Test Loss: 0.6616222
Validation loss decreased (0.574220 --> 0.566624).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 176.23884081840515
Epoch: 25, Steps: 84 | Train Loss: 0.3142931 Vali Loss: 0.5590119 Test Loss: 0.6529165
Validation loss decreased (0.566624 --> 0.559012).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 167.16600131988525
Epoch: 26, Steps: 84 | Train Loss: 0.3081423 Vali Loss: 0.5531362 Test Loss: 0.6452806
Validation loss decreased (0.559012 --> 0.553136).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 168.45620346069336
Epoch: 27, Steps: 84 | Train Loss: 0.3024376 Vali Loss: 0.5462213 Test Loss: 0.6378927
Validation loss decreased (0.553136 --> 0.546221).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 171.4934778213501
Epoch: 28, Steps: 84 | Train Loss: 0.2970715 Vali Loss: 0.5410521 Test Loss: 0.6313549
Validation loss decreased (0.546221 --> 0.541052).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 172.58498740196228
Epoch: 29, Steps: 84 | Train Loss: 0.2920780 Vali Loss: 0.5355654 Test Loss: 0.6247799
Validation loss decreased (0.541052 --> 0.535565).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 168.74294447898865
Epoch: 30, Steps: 84 | Train Loss: 0.2874672 Vali Loss: 0.5299084 Test Loss: 0.6186886
Validation loss decreased (0.535565 --> 0.529908).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 169.64218282699585
Epoch: 31, Steps: 84 | Train Loss: 0.2831339 Vali Loss: 0.5259929 Test Loss: 0.6135469
Validation loss decreased (0.529908 --> 0.525993).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 165.99888563156128
Epoch: 32, Steps: 84 | Train Loss: 0.2790515 Vali Loss: 0.5214338 Test Loss: 0.6082296
Validation loss decreased (0.525993 --> 0.521434).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 166.49776434898376
Epoch: 33, Steps: 84 | Train Loss: 0.2753277 Vali Loss: 0.5171602 Test Loss: 0.6032556
Validation loss decreased (0.521434 --> 0.517160).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 169.72583889961243
Epoch: 34, Steps: 84 | Train Loss: 0.2718079 Vali Loss: 0.5129780 Test Loss: 0.5988418
Validation loss decreased (0.517160 --> 0.512978).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 166.71375513076782
Epoch: 35, Steps: 84 | Train Loss: 0.2685112 Vali Loss: 0.5101772 Test Loss: 0.5946317
Validation loss decreased (0.512978 --> 0.510177).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 157.8257303237915
Epoch: 36, Steps: 84 | Train Loss: 0.2654158 Vali Loss: 0.5071388 Test Loss: 0.5907154
Validation loss decreased (0.510177 --> 0.507139).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 155.13649082183838
Epoch: 37, Steps: 84 | Train Loss: 0.2624412 Vali Loss: 0.5036631 Test Loss: 0.5871977
Validation loss decreased (0.507139 --> 0.503663).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 153.4097592830658
Epoch: 38, Steps: 84 | Train Loss: 0.2597550 Vali Loss: 0.5008966 Test Loss: 0.5834873
Validation loss decreased (0.503663 --> 0.500897).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 162.93858671188354
Epoch: 39, Steps: 84 | Train Loss: 0.2571591 Vali Loss: 0.4975352 Test Loss: 0.5802053
Validation loss decreased (0.500897 --> 0.497535).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 160.26444554328918
Epoch: 40, Steps: 84 | Train Loss: 0.2547208 Vali Loss: 0.4948326 Test Loss: 0.5770597
Validation loss decreased (0.497535 --> 0.494833).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 164.10610461235046
Epoch: 41, Steps: 84 | Train Loss: 0.2524576 Vali Loss: 0.4920458 Test Loss: 0.5739796
Validation loss decreased (0.494833 --> 0.492046).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 161.81781458854675
Epoch: 42, Steps: 84 | Train Loss: 0.2503168 Vali Loss: 0.4900777 Test Loss: 0.5713052
Validation loss decreased (0.492046 --> 0.490078).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 159.73558020591736
Epoch: 43, Steps: 84 | Train Loss: 0.2482449 Vali Loss: 0.4878999 Test Loss: 0.5689299
Validation loss decreased (0.490078 --> 0.487900).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 159.64456629753113
Epoch: 44, Steps: 84 | Train Loss: 0.2464362 Vali Loss: 0.4856971 Test Loss: 0.5663469
Validation loss decreased (0.487900 --> 0.485697).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 161.96673464775085
Epoch: 45, Steps: 84 | Train Loss: 0.2446376 Vali Loss: 0.4839187 Test Loss: 0.5640562
Validation loss decreased (0.485697 --> 0.483919).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 154.16854977607727
Epoch: 46, Steps: 84 | Train Loss: 0.2429472 Vali Loss: 0.4821156 Test Loss: 0.5617319
Validation loss decreased (0.483919 --> 0.482116).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 149.23536086082458
Epoch: 47, Steps: 84 | Train Loss: 0.2413177 Vali Loss: 0.4798170 Test Loss: 0.5597301
Validation loss decreased (0.482116 --> 0.479817).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 156.4774613380432
Epoch: 48, Steps: 84 | Train Loss: 0.2398321 Vali Loss: 0.4793212 Test Loss: 0.5579601
Validation loss decreased (0.479817 --> 0.479321).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 156.98860502243042
Epoch: 49, Steps: 84 | Train Loss: 0.2384134 Vali Loss: 0.4770520 Test Loss: 0.5560246
Validation loss decreased (0.479321 --> 0.477052).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 152.01128816604614
Epoch: 50, Steps: 84 | Train Loss: 0.2369967 Vali Loss: 0.4755812 Test Loss: 0.5543256
Validation loss decreased (0.477052 --> 0.475581).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 153.98177790641785
Epoch: 51, Steps: 84 | Train Loss: 0.2357038 Vali Loss: 0.4746322 Test Loss: 0.5526038
Validation loss decreased (0.475581 --> 0.474632).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 156.61365294456482
Epoch: 52, Steps: 84 | Train Loss: 0.2345384 Vali Loss: 0.4729092 Test Loss: 0.5509620
Validation loss decreased (0.474632 --> 0.472909).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 151.13718438148499
Epoch: 53, Steps: 84 | Train Loss: 0.2333310 Vali Loss: 0.4715147 Test Loss: 0.5495946
Validation loss decreased (0.472909 --> 0.471515).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 153.62516975402832
Epoch: 54, Steps: 84 | Train Loss: 0.2323321 Vali Loss: 0.4700982 Test Loss: 0.5482735
Validation loss decreased (0.471515 --> 0.470098).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 154.75438618659973
Epoch: 55, Steps: 84 | Train Loss: 0.2312781 Vali Loss: 0.4692604 Test Loss: 0.5468127
Validation loss decreased (0.470098 --> 0.469260).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 154.36215043067932
Epoch: 56, Steps: 84 | Train Loss: 0.2303331 Vali Loss: 0.4682278 Test Loss: 0.5456629
Validation loss decreased (0.469260 --> 0.468228).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 152.83460211753845
Epoch: 57, Steps: 84 | Train Loss: 0.2294287 Vali Loss: 0.4674267 Test Loss: 0.5444332
Validation loss decreased (0.468228 --> 0.467427).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 146.09579038619995
Epoch: 58, Steps: 84 | Train Loss: 0.2284932 Vali Loss: 0.4663053 Test Loss: 0.5433170
Validation loss decreased (0.467427 --> 0.466305).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 151.65510630607605
Epoch: 59, Steps: 84 | Train Loss: 0.2277222 Vali Loss: 0.4653507 Test Loss: 0.5422826
Validation loss decreased (0.466305 --> 0.465351).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 154.18391847610474
Epoch: 60, Steps: 84 | Train Loss: 0.2269030 Vali Loss: 0.4642544 Test Loss: 0.5412591
Validation loss decreased (0.465351 --> 0.464254).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 152.03351354599
Epoch: 61, Steps: 84 | Train Loss: 0.2261629 Vali Loss: 0.4639670 Test Loss: 0.5403150
Validation loss decreased (0.464254 --> 0.463967).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 149.55131912231445
Epoch: 62, Steps: 84 | Train Loss: 0.2254732 Vali Loss: 0.4625954 Test Loss: 0.5393670
Validation loss decreased (0.463967 --> 0.462595).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 150.8212594985962
Epoch: 63, Steps: 84 | Train Loss: 0.2247917 Vali Loss: 0.4624543 Test Loss: 0.5385303
Validation loss decreased (0.462595 --> 0.462454).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 155.9168381690979
Epoch: 64, Steps: 84 | Train Loss: 0.2241565 Vali Loss: 0.4617453 Test Loss: 0.5377012
Validation loss decreased (0.462454 --> 0.461745).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 162.12448573112488
Epoch: 65, Steps: 84 | Train Loss: 0.2235266 Vali Loss: 0.4609419 Test Loss: 0.5369634
Validation loss decreased (0.461745 --> 0.460942).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 162.01108622550964
Epoch: 66, Steps: 84 | Train Loss: 0.2229238 Vali Loss: 0.4598832 Test Loss: 0.5362020
Validation loss decreased (0.460942 --> 0.459883).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 160.71446681022644
Epoch: 67, Steps: 84 | Train Loss: 0.2224067 Vali Loss: 0.4600701 Test Loss: 0.5355025
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 157.7746241092682
Epoch: 68, Steps: 84 | Train Loss: 0.2219207 Vali Loss: 0.4593940 Test Loss: 0.5348651
Validation loss decreased (0.459883 --> 0.459394).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 158.3554162979126
Epoch: 69, Steps: 84 | Train Loss: 0.2213764 Vali Loss: 0.4586903 Test Loss: 0.5341775
Validation loss decreased (0.459394 --> 0.458690).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 160.78783226013184
Epoch: 70, Steps: 84 | Train Loss: 0.2209336 Vali Loss: 0.4575461 Test Loss: 0.5336205
Validation loss decreased (0.458690 --> 0.457546).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 164.3558473587036
Epoch: 71, Steps: 84 | Train Loss: 0.2205078 Vali Loss: 0.4570087 Test Loss: 0.5330501
Validation loss decreased (0.457546 --> 0.457009).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 157.82175064086914
Epoch: 72, Steps: 84 | Train Loss: 0.2200512 Vali Loss: 0.4569774 Test Loss: 0.5324681
Validation loss decreased (0.457009 --> 0.456977).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 165.63628673553467
Epoch: 73, Steps: 84 | Train Loss: 0.2196514 Vali Loss: 0.4571689 Test Loss: 0.5319906
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 162.2925853729248
Epoch: 74, Steps: 84 | Train Loss: 0.2193097 Vali Loss: 0.4565767 Test Loss: 0.5315260
Validation loss decreased (0.456977 --> 0.456577).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 161.5285906791687
Epoch: 75, Steps: 84 | Train Loss: 0.2189579 Vali Loss: 0.4558881 Test Loss: 0.5310217
Validation loss decreased (0.456577 --> 0.455888).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 161.7802767753601
Epoch: 76, Steps: 84 | Train Loss: 0.2185631 Vali Loss: 0.4561270 Test Loss: 0.5305867
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 167.4440200328827
Epoch: 77, Steps: 84 | Train Loss: 0.2182523 Vali Loss: 0.4550000 Test Loss: 0.5301801
Validation loss decreased (0.455888 --> 0.455000).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 165.60766053199768
Epoch: 78, Steps: 84 | Train Loss: 0.2179267 Vali Loss: 0.4547623 Test Loss: 0.5297912
Validation loss decreased (0.455000 --> 0.454762).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 162.03359150886536
Epoch: 79, Steps: 84 | Train Loss: 0.2176773 Vali Loss: 0.4550398 Test Loss: 0.5294060
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 134.51730751991272
Epoch: 80, Steps: 84 | Train Loss: 0.2173993 Vali Loss: 0.4543790 Test Loss: 0.5290427
Validation loss decreased (0.454762 --> 0.454379).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 120.4468879699707
Epoch: 81, Steps: 84 | Train Loss: 0.2171030 Vali Loss: 0.4536651 Test Loss: 0.5287178
Validation loss decreased (0.454379 --> 0.453665).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 125.20002126693726
Epoch: 82, Steps: 84 | Train Loss: 0.2168595 Vali Loss: 0.4538993 Test Loss: 0.5283663
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 126.24660968780518
Epoch: 83, Steps: 84 | Train Loss: 0.2166313 Vali Loss: 0.4530413 Test Loss: 0.5280384
Validation loss decreased (0.453665 --> 0.453041).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 129.52436113357544
Epoch: 84, Steps: 84 | Train Loss: 0.2164287 Vali Loss: 0.4531978 Test Loss: 0.5277541
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 133.85056495666504
Epoch: 85, Steps: 84 | Train Loss: 0.2161431 Vali Loss: 0.4528064 Test Loss: 0.5274726
Validation loss decreased (0.453041 --> 0.452806).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 135.04067015647888
Epoch: 86, Steps: 84 | Train Loss: 0.2159477 Vali Loss: 0.4525581 Test Loss: 0.5272217
Validation loss decreased (0.452806 --> 0.452558).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 130.68317437171936
Epoch: 87, Steps: 84 | Train Loss: 0.2158197 Vali Loss: 0.4528990 Test Loss: 0.5269737
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 137.83944535255432
Epoch: 88, Steps: 84 | Train Loss: 0.2155293 Vali Loss: 0.4521715 Test Loss: 0.5267462
Validation loss decreased (0.452558 --> 0.452172).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 113.3118348121643
Epoch: 89, Steps: 84 | Train Loss: 0.2154318 Vali Loss: 0.4525715 Test Loss: 0.5265155
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 105.99748754501343
Epoch: 90, Steps: 84 | Train Loss: 0.2152344 Vali Loss: 0.4518103 Test Loss: 0.5262773
Validation loss decreased (0.452172 --> 0.451810).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 107.40768671035767
Epoch: 91, Steps: 84 | Train Loss: 0.2151048 Vali Loss: 0.4516212 Test Loss: 0.5260837
Validation loss decreased (0.451810 --> 0.451621).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 103.92994451522827
Epoch: 92, Steps: 84 | Train Loss: 0.2149323 Vali Loss: 0.4511181 Test Loss: 0.5258794
Validation loss decreased (0.451621 --> 0.451118).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 101.87751197814941
Epoch: 93, Steps: 84 | Train Loss: 0.2147946 Vali Loss: 0.4512421 Test Loss: 0.5257210
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 103.74740767478943
Epoch: 94, Steps: 84 | Train Loss: 0.2146303 Vali Loss: 0.4510171 Test Loss: 0.5255252
Validation loss decreased (0.451118 --> 0.451017).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 99.28052616119385
Epoch: 95, Steps: 84 | Train Loss: 0.2145588 Vali Loss: 0.4512694 Test Loss: 0.5253670
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 94.60980772972107
Epoch: 96, Steps: 84 | Train Loss: 0.2144190 Vali Loss: 0.4511887 Test Loss: 0.5252138
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 94.75192165374756
Epoch: 97, Steps: 84 | Train Loss: 0.2142649 Vali Loss: 0.4510744 Test Loss: 0.5250604
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 94.58970856666565
Epoch: 98, Steps: 84 | Train Loss: 0.2141908 Vali Loss: 0.4508517 Test Loss: 0.5249230
Validation loss decreased (0.451017 --> 0.450852).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 94.82780933380127
Epoch: 99, Steps: 84 | Train Loss: 0.2140866 Vali Loss: 0.4502993 Test Loss: 0.5247834
Validation loss decreased (0.450852 --> 0.450299).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 94.07601499557495
Epoch: 100, Steps: 84 | Train Loss: 0.2139716 Vali Loss: 0.4501926 Test Loss: 0.5246521
Validation loss decreased (0.450299 --> 0.450193).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6007795200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 95.85101819038391
Epoch: 1, Steps: 84 | Train Loss: 0.3057698 Vali Loss: 0.4063083 Test Loss: 0.4730521
Validation loss decreased (inf --> 0.406308).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 94.27342772483826
Epoch: 2, Steps: 84 | Train Loss: 0.2819990 Vali Loss: 0.3943126 Test Loss: 0.4596925
Validation loss decreased (0.406308 --> 0.394313).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 95.35484385490417
Epoch: 3, Steps: 84 | Train Loss: 0.2761880 Vali Loss: 0.3922741 Test Loss: 0.4569375
Validation loss decreased (0.394313 --> 0.392274).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 94.0397539138794
Epoch: 4, Steps: 84 | Train Loss: 0.2750812 Vali Loss: 0.3927706 Test Loss: 0.4565135
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 92.65812635421753
Epoch: 5, Steps: 84 | Train Loss: 0.2749395 Vali Loss: 0.3925618 Test Loss: 0.4569683
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 94.7761743068695
Epoch: 6, Steps: 84 | Train Loss: 0.2749174 Vali Loss: 0.3921760 Test Loss: 0.4569297
Validation loss decreased (0.392274 --> 0.392176).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 94.40215015411377
Epoch: 7, Steps: 84 | Train Loss: 0.2748161 Vali Loss: 0.3922419 Test Loss: 0.4568541
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 86.08364367485046
Epoch: 8, Steps: 84 | Train Loss: 0.2748120 Vali Loss: 0.3921663 Test Loss: 0.4567322
Validation loss decreased (0.392176 --> 0.392166).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 77.83987069129944
Epoch: 9, Steps: 84 | Train Loss: 0.2747628 Vali Loss: 0.3922776 Test Loss: 0.4573413
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 75.26781177520752
Epoch: 10, Steps: 84 | Train Loss: 0.2746833 Vali Loss: 0.3921713 Test Loss: 0.4568418
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 76.03099632263184
Epoch: 11, Steps: 84 | Train Loss: 0.2747215 Vali Loss: 0.3918893 Test Loss: 0.4568420
Validation loss decreased (0.392166 --> 0.391889).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 76.87713289260864
Epoch: 12, Steps: 84 | Train Loss: 0.2746865 Vali Loss: 0.3924011 Test Loss: 0.4569157
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 75.96731424331665
Epoch: 13, Steps: 84 | Train Loss: 0.2746777 Vali Loss: 0.3915796 Test Loss: 0.4564966
Validation loss decreased (0.391889 --> 0.391580).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 76.05904769897461
Epoch: 14, Steps: 84 | Train Loss: 0.2746708 Vali Loss: 0.3921620 Test Loss: 0.4567452
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 75.40693616867065
Epoch: 15, Steps: 84 | Train Loss: 0.2746014 Vali Loss: 0.3921898 Test Loss: 0.4567536
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 75.83651828765869
Epoch: 16, Steps: 84 | Train Loss: 0.2745812 Vali Loss: 0.3917995 Test Loss: 0.4566754
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 74.77054643630981
Epoch: 17, Steps: 84 | Train Loss: 0.2745454 Vali Loss: 0.3915727 Test Loss: 0.4567504
Validation loss decreased (0.391580 --> 0.391573).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 74.15355896949768
Epoch: 18, Steps: 84 | Train Loss: 0.2745089 Vali Loss: 0.3919117 Test Loss: 0.4563754
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 72.58871507644653
Epoch: 19, Steps: 84 | Train Loss: 0.2745348 Vali Loss: 0.3920828 Test Loss: 0.4564802
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 74.84014678001404
Epoch: 20, Steps: 84 | Train Loss: 0.2744952 Vali Loss: 0.3919707 Test Loss: 0.4567996
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 73.69725489616394
Epoch: 21, Steps: 84 | Train Loss: 0.2745277 Vali Loss: 0.3922766 Test Loss: 0.4566587
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 74.05090665817261
Epoch: 22, Steps: 84 | Train Loss: 0.2744437 Vali Loss: 0.3921729 Test Loss: 0.4568714
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 74.13984847068787
Epoch: 23, Steps: 84 | Train Loss: 0.2744195 Vali Loss: 0.3920998 Test Loss: 0.4567011
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 74.50622391700745
Epoch: 24, Steps: 84 | Train Loss: 0.2743984 Vali Loss: 0.3917426 Test Loss: 0.4568558
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 74.30778908729553
Epoch: 25, Steps: 84 | Train Loss: 0.2744367 Vali Loss: 0.3925520 Test Loss: 0.4568775
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 73.05310368537903
Epoch: 26, Steps: 84 | Train Loss: 0.2743626 Vali Loss: 0.3921106 Test Loss: 0.4566508
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 73.17501258850098
Epoch: 27, Steps: 84 | Train Loss: 0.2743952 Vali Loss: 0.3916472 Test Loss: 0.4568188
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4562074840068817, mae:0.3097248375415802, rse:0.5523088574409485, corr:[0.2668955  0.26961538 0.26973355 0.2711933  0.2718118  0.27143973
 0.27120233 0.2715258  0.2718157  0.27156454 0.27105045 0.27082318
 0.27094698 0.27094734 0.27063647 0.27031708 0.27036923 0.27073982
 0.27100888 0.27093723 0.27074838 0.27071875 0.27099353 0.2718057
 0.272762   0.27268016 0.27249613 0.27271992 0.27294147 0.27272984
 0.27213925 0.27157927 0.271366   0.27138096 0.2712711  0.270957
 0.27076766 0.2709029  0.27125698 0.27154297 0.27163988 0.27162933
 0.27158687 0.2714944  0.2712927  0.27097833 0.27073145 0.2709386
 0.27141806 0.2715356  0.2714588  0.27136227 0.27132863 0.2713639
 0.27136722 0.271268   0.2711576  0.2711482  0.27120262 0.27118593
 0.27109364 0.27093884 0.27088633 0.2709793  0.27107793 0.27101618
 0.27078772 0.27060288 0.2706195  0.2707538  0.270748   0.27057505
 0.27023435 0.27001256 0.27009103 0.27028364 0.27043596 0.27053925
 0.27064678 0.2707657  0.27085647 0.27092302 0.27088183 0.27082068
 0.27083123 0.27086315 0.27085698 0.27083176 0.27085295 0.27086246
 0.27095082 0.2708572  0.27063647 0.2704838  0.27045116 0.27051
 0.27035093 0.27009046 0.26991177 0.26987362 0.26998833 0.27014306
 0.2702036  0.27013895 0.27007666 0.2702267  0.2703882  0.27040207
 0.27026287 0.27010095 0.2700769  0.2702194  0.2703452  0.27026463
 0.2702941  0.2702919  0.27027687 0.27018166 0.27009225 0.2699646
 0.26982373 0.26995698 0.27031875 0.27051184 0.27044472 0.27020705
 0.2700204  0.2699742  0.2700843  0.27026892 0.27033615 0.2703332
 0.27032092 0.27034488 0.27042183 0.27054217 0.27060494 0.27030754
 0.27034122 0.27037096 0.27041543 0.27056906 0.27080187 0.27110666
 0.27139235 0.27147073 0.27148196 0.27150077 0.27151886 0.27144262
 0.2712183  0.27092293 0.27071092 0.27078116 0.27094838 0.2710164
 0.27090678 0.2707329  0.270703   0.27088022 0.2711098  0.27122843
 0.27127495 0.2712952  0.2713802  0.27151188 0.27159423 0.27188468
 0.27236736 0.27209064 0.27177075 0.271723   0.2717381  0.27171522
 0.27167332 0.27168205 0.2717332  0.27175385 0.27166772 0.2715427
 0.27152804 0.2716445  0.27185705 0.27204356 0.27209178 0.27197987
 0.27169997 0.2714213  0.27126834 0.2712007  0.27123734 0.2716674
 0.27232742 0.2723697  0.27216387 0.2720346  0.27195853 0.27189347
 0.2718314  0.27181506 0.27191633 0.27211085 0.27220017 0.27209368
 0.27191925 0.27182326 0.2719477  0.27220893 0.2723672  0.27226835
 0.27198994 0.27179092 0.27182537 0.2719592  0.27199525 0.27205735
 0.27218235 0.2721971  0.2721823  0.27209595 0.27188498 0.2716505
 0.27153352 0.2715747  0.2717111  0.27182096 0.27181712 0.27174494
 0.27173534 0.27174595 0.27175564 0.2717946  0.2718701  0.27187786
 0.27167094 0.27127326 0.2709858  0.27090302 0.2710909  0.27138853
 0.2714899  0.2713466  0.27121815 0.27123398 0.27132922 0.27138013
 0.27137703 0.27141008 0.27152926 0.27172726 0.27179956 0.27169773
 0.2715483  0.27147463 0.2715146  0.2716209  0.27172586 0.2717437
 0.27168912 0.27154934 0.27145794 0.27147454 0.2715215  0.27150527
 0.2713053  0.27110434 0.27101564 0.270964   0.27090663 0.2709225
 0.27096823 0.27102926 0.2711192  0.27137536 0.27160755 0.27165323
 0.27147317 0.27117997 0.27101493 0.27115744 0.27152708 0.2718341
 0.27206105 0.27189788 0.27146956 0.27111933 0.27106774 0.2712097
 0.27127314 0.27136278 0.27156684 0.27182946 0.27208617 0.2721984
 0.27209577 0.27190033 0.27185464 0.27203748 0.27223462 0.2722932
 0.27217636 0.27198532 0.2718587  0.27184048 0.27186856 0.27179158
 0.2719343  0.2720786  0.2721721  0.2721534  0.27203083 0.27191195
 0.27187568 0.27184084 0.27189505 0.2720338  0.2722226  0.2723946
 0.27248964 0.2724889  0.27241075 0.2723207  0.2722184  0.2721418
 0.2721111  0.272171   0.27233472 0.27249882 0.27251986 0.2723422
 0.27222556 0.27221534 0.27228716 0.27237374 0.272412   0.27271447
 0.2732792  0.27330232 0.27318403 0.27315232 0.273102   0.27305412
 0.27309224 0.27325997 0.27346084 0.27352154 0.27330557 0.27292958
 0.27268168 0.27276036 0.2731211  0.27343312 0.2734588  0.2732833
 0.27315778 0.27319306 0.2732185  0.27298996 0.27252644 0.2724905
 0.27306956 0.27336904 0.27334988 0.27318552 0.27301642 0.27302337
 0.2732056  0.2733603  0.27334994 0.27322432 0.2730853  0.27303797
 0.2730401  0.27300042 0.27293006 0.27287894 0.2728446  0.27278572
 0.27263924 0.27246794 0.2723955  0.2723786  0.2723076  0.27230522
 0.27237707 0.2724093  0.272513   0.27262536 0.27261922 0.27252796
 0.27252385 0.27270752 0.27297056 0.27307162 0.27288967 0.27261296
 0.2725373  0.2726248  0.2726591  0.27244884 0.2721054  0.27192754
 0.27203533 0.27226445 0.2723533  0.27222377 0.27198192 0.27188075
 0.27187914 0.2718646  0.27181137 0.27175435 0.27176064 0.2718647
 0.2719982  0.2720123  0.2719054  0.2718353  0.27186298 0.2719839
 0.27208382 0.2719243  0.27152115 0.27115774 0.2710987  0.2713059
 0.27156538 0.27157125 0.27146596 0.27153853 0.27172142 0.27187538
 0.27175325 0.2716171  0.27166364 0.27189717 0.27209294 0.27207896
 0.27190143 0.27179384 0.2718926  0.27210265 0.27218404 0.27202958
 0.27180153 0.27170813 0.2717782  0.27182364 0.27162063 0.27125677
 0.27118114 0.2712819  0.271401   0.271331   0.27112475 0.27098423
 0.2709752  0.27124718 0.27166778 0.2719398  0.2720476  0.27205884
 0.2720081  0.27193135 0.27190638 0.27200496 0.27221394 0.27240914
 0.27244848 0.27233353 0.27220175 0.27214935 0.2721448  0.27214694
 0.27218348 0.27221304 0.2722831  0.27234554 0.27229038 0.27214527
 0.27196997 0.27178398 0.27183113 0.27216026 0.27260408 0.27291107
 0.27296206 0.272858   0.27277514 0.2727419  0.2726477  0.27243218
 0.27223906 0.27228212 0.27254555 0.27276084 0.27262738 0.27220747
 0.27189752 0.27200413 0.27243456 0.27275452 0.27270737 0.27267477
 0.27289793 0.27275258 0.27253798 0.2723626  0.27216336 0.27209178
 0.27223688 0.27247617 0.27259246 0.27250326 0.27229115 0.27215672
 0.27214977 0.27219185 0.27229008 0.272415   0.27251503 0.27247828
 0.27222756 0.27189934 0.27165583 0.27153775 0.27154523 0.27183938
 0.27226925 0.27227563 0.27213064 0.27200967 0.27184945 0.27168724
 0.27163422 0.27174053 0.2719135  0.27197358 0.27180663 0.2715468
 0.27139977 0.27147713 0.2717206  0.2719199  0.2718994  0.27165082
 0.27129382 0.2709815  0.27080122 0.27074337 0.27073738 0.27088407
 0.27110365 0.27118096 0.27117926 0.27115208 0.27114666 0.2712469
 0.27145785 0.27167788 0.27178156 0.27170485 0.27145785 0.2711627
 0.27091286 0.27068567 0.27051952 0.27048376 0.2705648  0.27066976
 0.2706696  0.2705766  0.27054387 0.27066597 0.2708278  0.27090544
 0.27074313 0.27043208 0.2702415  0.27022693 0.2703001  0.27035215
 0.27032498 0.27024508 0.27018696 0.27017027 0.27013066 0.2700613
 0.27000427 0.2699027  0.2697014  0.269443   0.26925826 0.26924598
 0.26931578 0.2693078  0.2691723  0.26902732 0.26899537 0.2691409
 0.26927176 0.26933193 0.26929075 0.26915103 0.2691001  0.26920488
 0.26929888 0.26922682 0.26903936 0.2689091  0.26888874 0.26890206
 0.26888058 0.26884246 0.26886198 0.268956   0.2690625  0.2691249
 0.26910642 0.26898232 0.2688699  0.2688279  0.26888576 0.26895985
 0.2689986  0.2692124  0.26954466 0.26969677 0.2696052  0.2693942
 0.26924536 0.26929253 0.26948696 0.2696063  0.26950657 0.2692537
 0.26900268 0.26883724 0.2687604  0.26875725 0.26883584 0.26909295
 0.26942506 0.26960984 0.26955235 0.2693658  0.2693005  0.26953894
 0.26993373 0.27009872 0.27002528 0.2698989  0.26989964 0.26998517
 0.2699604  0.26975244 0.26951665 0.26946756 0.26964664 0.26986304
 0.26992044 0.2698498  0.2698259  0.26990557 0.26998186 0.26992932
 0.26982686 0.269911   0.27017695 0.27036574 0.27027878 0.27031055
 0.270548   0.27032146 0.27010605 0.2701059  0.27020794 0.27041808
 0.2707276  0.27098307 0.27097303 0.2706378  0.2701896  0.27003688
 0.27032045 0.27074862 0.27100194 0.27098504 0.2709576  0.2711087
 0.27129808 0.27131417 0.27118784 0.2711555  0.27144545 0.27211452
 0.27258414 0.27212977 0.27140602 0.2710296  0.27098584 0.27095956
 0.2707721  0.27059096 0.27065557 0.27084795 0.27078333 0.27041614
 0.27024868 0.27068403 0.27155656 0.2720678  0.2718849  0.2715779
 0.27177757 0.2719835  0.2708594  0.26848412 0.26797152 0.27172753]
