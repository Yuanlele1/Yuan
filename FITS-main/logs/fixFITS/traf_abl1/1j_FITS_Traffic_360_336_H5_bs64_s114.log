Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j336_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j336_H5_FITS_custom_ftM_sl360_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11585
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=90, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1727861760.0
params:  15834.0
Trainable parameters:  15834
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 84.83561849594116
Epoch: 1, Steps: 90 | Train Loss: 1.0806372 Vali Loss: 0.9705918 Test Loss: 1.1399370
Validation loss decreased (inf --> 0.970592).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 83.65169215202332
Epoch: 2, Steps: 90 | Train Loss: 0.6787523 Vali Loss: 0.7296661 Test Loss: 0.8578537
Validation loss decreased (0.970592 --> 0.729666).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 83.75602293014526
Epoch: 3, Steps: 90 | Train Loss: 0.5261448 Vali Loss: 0.5983322 Test Loss: 0.7058766
Validation loss decreased (0.729666 --> 0.598332).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 84.77447986602783
Epoch: 4, Steps: 90 | Train Loss: 0.4344967 Vali Loss: 0.5149022 Test Loss: 0.6105266
Validation loss decreased (0.598332 --> 0.514902).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 84.95710229873657
Epoch: 5, Steps: 90 | Train Loss: 0.3758368 Vali Loss: 0.4608787 Test Loss: 0.5497312
Validation loss decreased (0.514902 --> 0.460879).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 82.115389585495
Epoch: 6, Steps: 90 | Train Loss: 0.3378918 Vali Loss: 0.4257531 Test Loss: 0.5107836
Validation loss decreased (0.460879 --> 0.425753).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 84.8154821395874
Epoch: 7, Steps: 90 | Train Loss: 0.3133507 Vali Loss: 0.4026084 Test Loss: 0.4859844
Validation loss decreased (0.425753 --> 0.402608).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 81.7558343410492
Epoch: 8, Steps: 90 | Train Loss: 0.2974976 Vali Loss: 0.3882927 Test Loss: 0.4704185
Validation loss decreased (0.402608 --> 0.388293).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 82.69015336036682
Epoch: 9, Steps: 90 | Train Loss: 0.2873915 Vali Loss: 0.3790216 Test Loss: 0.4605942
Validation loss decreased (0.388293 --> 0.379022).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 81.55088543891907
Epoch: 10, Steps: 90 | Train Loss: 0.2809585 Vali Loss: 0.3728002 Test Loss: 0.4547081
Validation loss decreased (0.379022 --> 0.372800).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 85.83528995513916
Epoch: 11, Steps: 90 | Train Loss: 0.2769285 Vali Loss: 0.3685187 Test Loss: 0.4508126
Validation loss decreased (0.372800 --> 0.368519).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 83.17050909996033
Epoch: 12, Steps: 90 | Train Loss: 0.2742485 Vali Loss: 0.3662751 Test Loss: 0.4486988
Validation loss decreased (0.368519 --> 0.366275).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 82.58052062988281
Epoch: 13, Steps: 90 | Train Loss: 0.2726527 Vali Loss: 0.3645175 Test Loss: 0.4472402
Validation loss decreased (0.366275 --> 0.364517).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 80.46845817565918
Epoch: 14, Steps: 90 | Train Loss: 0.2715626 Vali Loss: 0.3633552 Test Loss: 0.4465312
Validation loss decreased (0.364517 --> 0.363355).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 80.95615196228027
Epoch: 15, Steps: 90 | Train Loss: 0.2708489 Vali Loss: 0.3627914 Test Loss: 0.4459331
Validation loss decreased (0.363355 --> 0.362791).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 81.8072817325592
Epoch: 16, Steps: 90 | Train Loss: 0.2704247 Vali Loss: 0.3620651 Test Loss: 0.4456056
Validation loss decreased (0.362791 --> 0.362065).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 79.29833936691284
Epoch: 17, Steps: 90 | Train Loss: 0.2701662 Vali Loss: 0.3620317 Test Loss: 0.4456149
Validation loss decreased (0.362065 --> 0.362032).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 82.01881670951843
Epoch: 18, Steps: 90 | Train Loss: 0.2699228 Vali Loss: 0.3611321 Test Loss: 0.4454937
Validation loss decreased (0.362032 --> 0.361132).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 84.46080493927002
Epoch: 19, Steps: 90 | Train Loss: 0.2697442 Vali Loss: 0.3609655 Test Loss: 0.4453675
Validation loss decreased (0.361132 --> 0.360966).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 84.0143187046051
Epoch: 20, Steps: 90 | Train Loss: 0.2695893 Vali Loss: 0.3610144 Test Loss: 0.4453350
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 83.32348990440369
Epoch: 21, Steps: 90 | Train Loss: 0.2695548 Vali Loss: 0.3607545 Test Loss: 0.4452701
Validation loss decreased (0.360966 --> 0.360755).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 78.68762826919556
Epoch: 22, Steps: 90 | Train Loss: 0.2695077 Vali Loss: 0.3610569 Test Loss: 0.4452665
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 86.20877599716187
Epoch: 23, Steps: 90 | Train Loss: 0.2693979 Vali Loss: 0.3608640 Test Loss: 0.4451257
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 78.98659706115723
Epoch: 24, Steps: 90 | Train Loss: 0.2693685 Vali Loss: 0.3608274 Test Loss: 0.4451225
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 81.08569955825806
Epoch: 25, Steps: 90 | Train Loss: 0.2693071 Vali Loss: 0.3608607 Test Loss: 0.4451807
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 80.18755173683167
Epoch: 26, Steps: 90 | Train Loss: 0.2692388 Vali Loss: 0.3605242 Test Loss: 0.4451590
Validation loss decreased (0.360755 --> 0.360524).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 75.50597190856934
Epoch: 27, Steps: 90 | Train Loss: 0.2692351 Vali Loss: 0.3605731 Test Loss: 0.4450820
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 65.8154969215393
Epoch: 28, Steps: 90 | Train Loss: 0.2691813 Vali Loss: 0.3605334 Test Loss: 0.4451569
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 64.81140637397766
Epoch: 29, Steps: 90 | Train Loss: 0.2691914 Vali Loss: 0.3604089 Test Loss: 0.4450418
Validation loss decreased (0.360524 --> 0.360409).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 66.03538060188293
Epoch: 30, Steps: 90 | Train Loss: 0.2691749 Vali Loss: 0.3605815 Test Loss: 0.4450730
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 66.81252360343933
Epoch: 31, Steps: 90 | Train Loss: 0.2691574 Vali Loss: 0.3603859 Test Loss: 0.4451005
Validation loss decreased (0.360409 --> 0.360386).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 63.85557317733765
Epoch: 32, Steps: 90 | Train Loss: 0.2692251 Vali Loss: 0.3606918 Test Loss: 0.4451072
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 65.65892052650452
Epoch: 33, Steps: 90 | Train Loss: 0.2691744 Vali Loss: 0.3603517 Test Loss: 0.4450496
Validation loss decreased (0.360386 --> 0.360352).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 67.37543773651123
Epoch: 34, Steps: 90 | Train Loss: 0.2691513 Vali Loss: 0.3609042 Test Loss: 0.4450032
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 63.99210500717163
Epoch: 35, Steps: 90 | Train Loss: 0.2691079 Vali Loss: 0.3603039 Test Loss: 0.4449926
Validation loss decreased (0.360352 --> 0.360304).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 63.41164255142212
Epoch: 36, Steps: 90 | Train Loss: 0.2691539 Vali Loss: 0.3607937 Test Loss: 0.4449571
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 61.67713642120361
Epoch: 37, Steps: 90 | Train Loss: 0.2690640 Vali Loss: 0.3605659 Test Loss: 0.4449189
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 52.88894033432007
Epoch: 38, Steps: 90 | Train Loss: 0.2691472 Vali Loss: 0.3605767 Test Loss: 0.4449145
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 51.17560529708862
Epoch: 39, Steps: 90 | Train Loss: 0.2690941 Vali Loss: 0.3605106 Test Loss: 0.4449376
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 50.75458002090454
Epoch: 40, Steps: 90 | Train Loss: 0.2690959 Vali Loss: 0.3599618 Test Loss: 0.4449129
Validation loss decreased (0.360304 --> 0.359962).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 51.438660621643066
Epoch: 41, Steps: 90 | Train Loss: 0.2689865 Vali Loss: 0.3601637 Test Loss: 0.4448969
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 52.18126654624939
Epoch: 42, Steps: 90 | Train Loss: 0.2689909 Vali Loss: 0.3603441 Test Loss: 0.4448989
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 50.334527254104614
Epoch: 43, Steps: 90 | Train Loss: 0.2690414 Vali Loss: 0.3603729 Test Loss: 0.4448983
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 51.645050048828125
Epoch: 44, Steps: 90 | Train Loss: 0.2690575 Vali Loss: 0.3604770 Test Loss: 0.4448687
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 51.81900691986084
Epoch: 45, Steps: 90 | Train Loss: 0.2689635 Vali Loss: 0.3606139 Test Loss: 0.4448853
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 51.97518515586853
Epoch: 46, Steps: 90 | Train Loss: 0.2689743 Vali Loss: 0.3605095 Test Loss: 0.4448786
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 50.33907866477966
Epoch: 47, Steps: 90 | Train Loss: 0.2689677 Vali Loss: 0.3602279 Test Loss: 0.4448655
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 51.232041120529175
Epoch: 48, Steps: 90 | Train Loss: 0.2689581 Vali Loss: 0.3604074 Test Loss: 0.4448824
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 52.93894958496094
Epoch: 49, Steps: 90 | Train Loss: 0.2689690 Vali Loss: 0.3603823 Test Loss: 0.4448619
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 50.562692165374756
Epoch: 50, Steps: 90 | Train Loss: 0.2689328 Vali Loss: 0.3605579 Test Loss: 0.4448893
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j336_H5_FITS_custom_ftM_sl360_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4424760937690735, mae:0.3023075461387634, rse:0.5466938614845276, corr:[0.2832684  0.27863207 0.28057763 0.28650996 0.28754205 0.28637314
 0.2871446  0.289153   0.2894663  0.28760532 0.2859048  0.2858537
 0.28622898 0.28538945 0.28392348 0.28354803 0.28442177 0.28518206
 0.28514245 0.2850458  0.28538716 0.28562176 0.28545126 0.28579536
 0.28700998 0.2879936  0.28836533 0.28785276 0.28700835 0.286718
 0.28692925 0.2867169  0.2858054  0.28501952 0.2849982  0.28529695
 0.2851547  0.28460053 0.2844804  0.28512335 0.28586012 0.28604218
 0.28591186 0.28598177 0.28631687 0.28637215 0.2860113  0.28596628
 0.2865393  0.28704584 0.28704935 0.2864841  0.28589413 0.28577706
 0.2859405  0.28585145 0.2854446  0.28524083 0.28545332 0.28563982
 0.2854461  0.28510606 0.28518772 0.28574792 0.28620672 0.28613728
 0.28599718 0.28605026 0.28627858 0.28633174 0.2860819  0.2859636
 0.28609636 0.2862112  0.2859958  0.28551412 0.2851785  0.2851687
 0.28522506 0.28503394 0.2847463  0.28477517 0.28513223 0.28535098
 0.28519934 0.28496996 0.28511015 0.28557968 0.28592375 0.28589806
 0.28580573 0.28580284 0.28589022 0.28583762 0.2855808  0.2853959
 0.28541312 0.28551966 0.28537795 0.28504324 0.28482443 0.2848481
 0.28492996 0.28484875 0.28472158 0.28481856 0.2850943  0.2852274
 0.28511542 0.2849894  0.28511    0.28538522 0.28548458 0.28534213
 0.2852508  0.28526273 0.28527558 0.28511512 0.2849659  0.2849978
 0.2851126  0.285187   0.28504065 0.28475645 0.2846545  0.28477612
 0.28487012 0.28477287 0.2846674  0.28479815 0.28507847 0.2852433
 0.28520983 0.2851391  0.2852424  0.28544182 0.28552258 0.28539684
 0.28529945 0.28531867 0.28533897 0.28529283 0.2853049  0.2856544
 0.28604987 0.28622687 0.28614902 0.2859542  0.28595236 0.28618088
 0.28632933 0.28623053 0.28610596 0.28623068 0.2865207  0.286681
 0.2866362  0.28658205 0.28666645 0.28678373 0.28677613 0.2866641
 0.28660586 0.28660226 0.28651005 0.28632137 0.28632933 0.28691664
 0.28756773 0.28732517 0.28679785 0.28635496 0.2862785  0.286546
 0.2867865  0.28677142 0.28668422 0.28677455 0.28696507 0.28700042
 0.28685832 0.2867175  0.28676248 0.2868842  0.2868692  0.28672406
 0.2865491  0.2864631  0.28634062 0.28605705 0.2858118  0.28599826
 0.28647918 0.2864899  0.2861754  0.28590944 0.28593785 0.28615868
 0.28621912 0.28600228 0.28580835 0.28592354 0.28619096 0.2862898
 0.2861597  0.2859903  0.2859844  0.2860435  0.2859747  0.28577656
 0.2855765  0.28547403 0.28539163 0.2851945  0.28494775 0.28497976
 0.28527027 0.2853895  0.28532878 0.28523934 0.2852543  0.28535894
 0.28540593 0.28534928 0.28534296 0.2855189  0.28570446 0.28564325
 0.2853783  0.2851532  0.2851608  0.285298   0.2853553  0.2852816
 0.2851565  0.28504565 0.2850123  0.2848702  0.2847222  0.2847381
 0.28479508 0.2847155  0.28458205 0.28457093 0.2847421  0.28497428
 0.28508872 0.28507286 0.2850939  0.2852414  0.28535038 0.28520915
 0.28489545 0.28467527 0.28467542 0.2847292  0.2846301  0.28444016
 0.28430298 0.28425753 0.284219   0.28412512 0.28405228 0.28409043
 0.28409228 0.2840046  0.28391674 0.2839827  0.28421035 0.284426
 0.28446373 0.2843766  0.2843461  0.2844211  0.28443265 0.2842085
 0.28390127 0.28376445 0.28386378 0.28396067 0.2838408  0.28359
 0.283444   0.2834608  0.28350234 0.2835093  0.28362733 0.2837761
 0.28368178 0.28355423 0.28359118 0.28383303 0.28416073 0.28430858
 0.2841435  0.28385526 0.2837388  0.28381428 0.28382912 0.28361782
 0.28330776 0.28316483 0.283286   0.28347918 0.28356776 0.2835292
 0.28351176 0.28359017 0.28374895 0.28390664 0.28410518 0.2843316
 0.28439888 0.28453225 0.28497303 0.2855091  0.2858406  0.28572157
 0.28517705 0.2846039  0.28439364 0.2845209  0.28463855 0.28453282
 0.28440228 0.28459972 0.28513563 0.2855478  0.2854491  0.28506634
 0.2850137  0.28538844 0.28522214 0.28373238 0.28299287 0.28694636]
