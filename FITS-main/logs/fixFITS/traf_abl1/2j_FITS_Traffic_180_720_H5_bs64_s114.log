Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j720_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j720_H5_FITS_custom_ftM_sl180_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11381
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=50, out_features=250, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1379200000.0
params:  12750.0
Trainable parameters:  12750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 103.2817759513855
Epoch: 1, Steps: 88 | Train Loss: 1.7024086 Vali Loss: 1.5253576 Test Loss: 1.8907444
Validation loss decreased (inf --> 1.525358).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 106.46970629692078
Epoch: 2, Steps: 88 | Train Loss: 0.9759456 Vali Loss: 1.0703573 Test Loss: 1.3235910
Validation loss decreased (1.525358 --> 1.070357).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 106.30862379074097
Epoch: 3, Steps: 88 | Train Loss: 0.7007256 Vali Loss: 0.8508276 Test Loss: 1.0507076
Validation loss decreased (1.070357 --> 0.850828).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 104.27348375320435
Epoch: 4, Steps: 88 | Train Loss: 0.5616803 Vali Loss: 0.7316467 Test Loss: 0.9022135
Validation loss decreased (0.850828 --> 0.731647).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 104.04805850982666
Epoch: 5, Steps: 88 | Train Loss: 0.4830187 Vali Loss: 0.6610895 Test Loss: 0.8147659
Validation loss decreased (0.731647 --> 0.661090).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 103.48876786231995
Epoch: 6, Steps: 88 | Train Loss: 0.4349858 Vali Loss: 0.6155994 Test Loss: 0.7589503
Validation loss decreased (0.661090 --> 0.615599).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 102.46323609352112
Epoch: 7, Steps: 88 | Train Loss: 0.4032865 Vali Loss: 0.5854721 Test Loss: 0.7198141
Validation loss decreased (0.615599 --> 0.585472).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 104.65660047531128
Epoch: 8, Steps: 88 | Train Loss: 0.3809048 Vali Loss: 0.5631089 Test Loss: 0.6914729
Validation loss decreased (0.585472 --> 0.563109).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 104.54749512672424
Epoch: 9, Steps: 88 | Train Loss: 0.3643195 Vali Loss: 0.5465533 Test Loss: 0.6694760
Validation loss decreased (0.563109 --> 0.546553).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 104.23968458175659
Epoch: 10, Steps: 88 | Train Loss: 0.3514640 Vali Loss: 0.5334646 Test Loss: 0.6517925
Validation loss decreased (0.546553 --> 0.533465).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 107.67143082618713
Epoch: 11, Steps: 88 | Train Loss: 0.3410924 Vali Loss: 0.5225432 Test Loss: 0.6375156
Validation loss decreased (0.533465 --> 0.522543).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 106.97559928894043
Epoch: 12, Steps: 88 | Train Loss: 0.3325883 Vali Loss: 0.5128732 Test Loss: 0.6252238
Validation loss decreased (0.522543 --> 0.512873).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 107.42017602920532
Epoch: 13, Steps: 88 | Train Loss: 0.3255556 Vali Loss: 0.5056656 Test Loss: 0.6150353
Validation loss decreased (0.512873 --> 0.505666).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 105.52839398384094
Epoch: 14, Steps: 88 | Train Loss: 0.3194040 Vali Loss: 0.4992636 Test Loss: 0.6060021
Validation loss decreased (0.505666 --> 0.499264).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 109.29839444160461
Epoch: 15, Steps: 88 | Train Loss: 0.3141433 Vali Loss: 0.4936083 Test Loss: 0.5982084
Validation loss decreased (0.499264 --> 0.493608).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 111.24474811553955
Epoch: 16, Steps: 88 | Train Loss: 0.3096400 Vali Loss: 0.4885688 Test Loss: 0.5913271
Validation loss decreased (0.493608 --> 0.488569).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 106.87097072601318
Epoch: 17, Steps: 88 | Train Loss: 0.3056407 Vali Loss: 0.4847953 Test Loss: 0.5854434
Validation loss decreased (0.488569 --> 0.484795).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 107.66374230384827
Epoch: 18, Steps: 88 | Train Loss: 0.3020530 Vali Loss: 0.4812898 Test Loss: 0.5801020
Validation loss decreased (0.484795 --> 0.481290).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 109.40621423721313
Epoch: 19, Steps: 88 | Train Loss: 0.2989803 Vali Loss: 0.4777113 Test Loss: 0.5752994
Validation loss decreased (0.481290 --> 0.477711).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 102.72685384750366
Epoch: 20, Steps: 88 | Train Loss: 0.2960940 Vali Loss: 0.4743820 Test Loss: 0.5708286
Validation loss decreased (0.477711 --> 0.474382).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 110.94328594207764
Epoch: 21, Steps: 88 | Train Loss: 0.2936676 Vali Loss: 0.4717440 Test Loss: 0.5671611
Validation loss decreased (0.474382 --> 0.471744).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 106.02017402648926
Epoch: 22, Steps: 88 | Train Loss: 0.2913807 Vali Loss: 0.4692163 Test Loss: 0.5635251
Validation loss decreased (0.471744 --> 0.469216).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 113.03235340118408
Epoch: 23, Steps: 88 | Train Loss: 0.2892926 Vali Loss: 0.4672928 Test Loss: 0.5605484
Validation loss decreased (0.469216 --> 0.467293).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 109.6864721775055
Epoch: 24, Steps: 88 | Train Loss: 0.2874328 Vali Loss: 0.4652230 Test Loss: 0.5575193
Validation loss decreased (0.467293 --> 0.465223).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 109.43933415412903
Epoch: 25, Steps: 88 | Train Loss: 0.2857998 Vali Loss: 0.4634955 Test Loss: 0.5547929
Validation loss decreased (0.465223 --> 0.463495).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 110.24064111709595
Epoch: 26, Steps: 88 | Train Loss: 0.2841276 Vali Loss: 0.4616997 Test Loss: 0.5524359
Validation loss decreased (0.463495 --> 0.461700).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 112.07000756263733
Epoch: 27, Steps: 88 | Train Loss: 0.2827519 Vali Loss: 0.4603948 Test Loss: 0.5502045
Validation loss decreased (0.461700 --> 0.460395).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 114.14049077033997
Epoch: 28, Steps: 88 | Train Loss: 0.2813941 Vali Loss: 0.4586909 Test Loss: 0.5482143
Validation loss decreased (0.460395 --> 0.458691).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 113.49256873130798
Epoch: 29, Steps: 88 | Train Loss: 0.2801656 Vali Loss: 0.4572971 Test Loss: 0.5463682
Validation loss decreased (0.458691 --> 0.457297).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 109.88293194770813
Epoch: 30, Steps: 88 | Train Loss: 0.2790991 Vali Loss: 0.4560727 Test Loss: 0.5445133
Validation loss decreased (0.457297 --> 0.456073).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 102.7235107421875
Epoch: 31, Steps: 88 | Train Loss: 0.2780425 Vali Loss: 0.4549149 Test Loss: 0.5428366
Validation loss decreased (0.456073 --> 0.454915).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 100.4287040233612
Epoch: 32, Steps: 88 | Train Loss: 0.2770813 Vali Loss: 0.4541787 Test Loss: 0.5414186
Validation loss decreased (0.454915 --> 0.454179).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 109.62284779548645
Epoch: 33, Steps: 88 | Train Loss: 0.2762706 Vali Loss: 0.4538855 Test Loss: 0.5399935
Validation loss decreased (0.454179 --> 0.453886).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 111.0917501449585
Epoch: 34, Steps: 88 | Train Loss: 0.2754368 Vali Loss: 0.4523057 Test Loss: 0.5386838
Validation loss decreased (0.453886 --> 0.452306).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 113.80953788757324
Epoch: 35, Steps: 88 | Train Loss: 0.2746279 Vali Loss: 0.4511195 Test Loss: 0.5374856
Validation loss decreased (0.452306 --> 0.451120).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 105.59782123565674
Epoch: 36, Steps: 88 | Train Loss: 0.2739721 Vali Loss: 0.4509736 Test Loss: 0.5363709
Validation loss decreased (0.451120 --> 0.450974).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 106.1045241355896
Epoch: 37, Steps: 88 | Train Loss: 0.2732284 Vali Loss: 0.4499624 Test Loss: 0.5353240
Validation loss decreased (0.450974 --> 0.449962).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 108.37153220176697
Epoch: 38, Steps: 88 | Train Loss: 0.2727081 Vali Loss: 0.4493577 Test Loss: 0.5343929
Validation loss decreased (0.449962 --> 0.449358).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 109.60821866989136
Epoch: 39, Steps: 88 | Train Loss: 0.2721028 Vali Loss: 0.4486798 Test Loss: 0.5334313
Validation loss decreased (0.449358 --> 0.448680).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 108.45518851280212
Epoch: 40, Steps: 88 | Train Loss: 0.2714935 Vali Loss: 0.4485191 Test Loss: 0.5325643
Validation loss decreased (0.448680 --> 0.448519).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 106.42899346351624
Epoch: 41, Steps: 88 | Train Loss: 0.2710488 Vali Loss: 0.4478173 Test Loss: 0.5317713
Validation loss decreased (0.448519 --> 0.447817).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 108.28088140487671
Epoch: 42, Steps: 88 | Train Loss: 0.2705975 Vali Loss: 0.4471317 Test Loss: 0.5309302
Validation loss decreased (0.447817 --> 0.447132).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 109.25076937675476
Epoch: 43, Steps: 88 | Train Loss: 0.2700898 Vali Loss: 0.4464431 Test Loss: 0.5302125
Validation loss decreased (0.447132 --> 0.446443).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 111.43490028381348
Epoch: 44, Steps: 88 | Train Loss: 0.2697225 Vali Loss: 0.4463708 Test Loss: 0.5295702
Validation loss decreased (0.446443 --> 0.446371).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 109.45360517501831
Epoch: 45, Steps: 88 | Train Loss: 0.2692136 Vali Loss: 0.4459188 Test Loss: 0.5289969
Validation loss decreased (0.446371 --> 0.445919).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 107.98321557044983
Epoch: 46, Steps: 88 | Train Loss: 0.2689699 Vali Loss: 0.4455279 Test Loss: 0.5284001
Validation loss decreased (0.445919 --> 0.445528).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 108.01612401008606
Epoch: 47, Steps: 88 | Train Loss: 0.2686279 Vali Loss: 0.4449344 Test Loss: 0.5278401
Validation loss decreased (0.445528 --> 0.444934).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 105.8654477596283
Epoch: 48, Steps: 88 | Train Loss: 0.2683222 Vali Loss: 0.4449950 Test Loss: 0.5272932
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 99.64528894424438
Epoch: 49, Steps: 88 | Train Loss: 0.2678636 Vali Loss: 0.4445810 Test Loss: 0.5268398
Validation loss decreased (0.444934 --> 0.444581).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 104.63090872764587
Epoch: 50, Steps: 88 | Train Loss: 0.2676088 Vali Loss: 0.4441590 Test Loss: 0.5263849
Validation loss decreased (0.444581 --> 0.444159).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 87.61529207229614
Epoch: 51, Steps: 88 | Train Loss: 0.2674521 Vali Loss: 0.4442939 Test Loss: 0.5259199
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 81.69595503807068
Epoch: 52, Steps: 88 | Train Loss: 0.2670940 Vali Loss: 0.4439176 Test Loss: 0.5254768
Validation loss decreased (0.444159 --> 0.443918).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 83.26719951629639
Epoch: 53, Steps: 88 | Train Loss: 0.2668355 Vali Loss: 0.4431016 Test Loss: 0.5251243
Validation loss decreased (0.443918 --> 0.443102).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 82.27949452400208
Epoch: 54, Steps: 88 | Train Loss: 0.2666519 Vali Loss: 0.4430937 Test Loss: 0.5247601
Validation loss decreased (0.443102 --> 0.443094).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 81.98519968986511
Epoch: 55, Steps: 88 | Train Loss: 0.2664289 Vali Loss: 0.4430202 Test Loss: 0.5243585
Validation loss decreased (0.443094 --> 0.443020).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 80.2339985370636
Epoch: 56, Steps: 88 | Train Loss: 0.2661374 Vali Loss: 0.4430247 Test Loss: 0.5240577
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 80.67479848861694
Epoch: 57, Steps: 88 | Train Loss: 0.2660818 Vali Loss: 0.4422019 Test Loss: 0.5237366
Validation loss decreased (0.443020 --> 0.442202).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 80.48768401145935
Epoch: 58, Steps: 88 | Train Loss: 0.2658626 Vali Loss: 0.4425981 Test Loss: 0.5234605
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 79.41042876243591
Epoch: 59, Steps: 88 | Train Loss: 0.2656961 Vali Loss: 0.4418337 Test Loss: 0.5231554
Validation loss decreased (0.442202 --> 0.441834).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 83.32866740226746
Epoch: 60, Steps: 88 | Train Loss: 0.2655495 Vali Loss: 0.4420594 Test Loss: 0.5228778
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 81.11431050300598
Epoch: 61, Steps: 88 | Train Loss: 0.2653638 Vali Loss: 0.4416783 Test Loss: 0.5226269
Validation loss decreased (0.441834 --> 0.441678).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 82.47456288337708
Epoch: 62, Steps: 88 | Train Loss: 0.2652207 Vali Loss: 0.4417863 Test Loss: 0.5224040
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 83.93012738227844
Epoch: 63, Steps: 88 | Train Loss: 0.2650728 Vali Loss: 0.4416957 Test Loss: 0.5221426
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 88.17017841339111
Epoch: 64, Steps: 88 | Train Loss: 0.2649425 Vali Loss: 0.4411763 Test Loss: 0.5219381
Validation loss decreased (0.441678 --> 0.441176).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 86.3408145904541
Epoch: 65, Steps: 88 | Train Loss: 0.2648389 Vali Loss: 0.4413800 Test Loss: 0.5217405
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 80.49219560623169
Epoch: 66, Steps: 88 | Train Loss: 0.2646276 Vali Loss: 0.4410785 Test Loss: 0.5215433
Validation loss decreased (0.441176 --> 0.441078).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 69.42272543907166
Epoch: 67, Steps: 88 | Train Loss: 0.2645593 Vali Loss: 0.4410644 Test Loss: 0.5213549
Validation loss decreased (0.441078 --> 0.441064).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 84.5441882610321
Epoch: 68, Steps: 88 | Train Loss: 0.2644331 Vali Loss: 0.4403202 Test Loss: 0.5211659
Validation loss decreased (0.441064 --> 0.440320).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 77.89027643203735
Epoch: 69, Steps: 88 | Train Loss: 0.2643816 Vali Loss: 0.4406967 Test Loss: 0.5210013
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 77.67575335502625
Epoch: 70, Steps: 88 | Train Loss: 0.2642346 Vali Loss: 0.4406398 Test Loss: 0.5208562
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 79.17514181137085
Epoch: 71, Steps: 88 | Train Loss: 0.2641096 Vali Loss: 0.4405196 Test Loss: 0.5206957
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 77.8037462234497
Epoch: 72, Steps: 88 | Train Loss: 0.2640020 Vali Loss: 0.4403942 Test Loss: 0.5205593
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 75.21299862861633
Epoch: 73, Steps: 88 | Train Loss: 0.2640279 Vali Loss: 0.4402200 Test Loss: 0.5204355
Validation loss decreased (0.440320 --> 0.440220).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 76.57214093208313
Epoch: 74, Steps: 88 | Train Loss: 0.2638538 Vali Loss: 0.4406384 Test Loss: 0.5202898
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 77.07985067367554
Epoch: 75, Steps: 88 | Train Loss: 0.2638532 Vali Loss: 0.4400522 Test Loss: 0.5201776
Validation loss decreased (0.440220 --> 0.440052).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 72.84764385223389
Epoch: 76, Steps: 88 | Train Loss: 0.2637537 Vali Loss: 0.4405945 Test Loss: 0.5200659
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 70.86604452133179
Epoch: 77, Steps: 88 | Train Loss: 0.2637031 Vali Loss: 0.4402335 Test Loss: 0.5199536
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 72.16160249710083
Epoch: 78, Steps: 88 | Train Loss: 0.2636118 Vali Loss: 0.4403127 Test Loss: 0.5198658
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 74.1512463092804
Epoch: 79, Steps: 88 | Train Loss: 0.2635329 Vali Loss: 0.4398398 Test Loss: 0.5197538
Validation loss decreased (0.440052 --> 0.439840).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 81.36884832382202
Epoch: 80, Steps: 88 | Train Loss: 0.2635280 Vali Loss: 0.4399630 Test Loss: 0.5196691
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 94.60894107818604
Epoch: 81, Steps: 88 | Train Loss: 0.2634792 Vali Loss: 0.4401034 Test Loss: 0.5195827
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 82.93659114837646
Epoch: 82, Steps: 88 | Train Loss: 0.2633405 Vali Loss: 0.4396962 Test Loss: 0.5194883
Validation loss decreased (0.439840 --> 0.439696).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 83.69273018836975
Epoch: 83, Steps: 88 | Train Loss: 0.2633778 Vali Loss: 0.4396213 Test Loss: 0.5194134
Validation loss decreased (0.439696 --> 0.439621).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 80.42922520637512
Epoch: 84, Steps: 88 | Train Loss: 0.2634064 Vali Loss: 0.4400135 Test Loss: 0.5193296
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 82.43070697784424
Epoch: 85, Steps: 88 | Train Loss: 0.2632302 Vali Loss: 0.4394988 Test Loss: 0.5192590
Validation loss decreased (0.439621 --> 0.439499).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 80.21702742576599
Epoch: 86, Steps: 88 | Train Loss: 0.2631792 Vali Loss: 0.4396746 Test Loss: 0.5191888
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 78.87226033210754
Epoch: 87, Steps: 88 | Train Loss: 0.2631302 Vali Loss: 0.4392388 Test Loss: 0.5191172
Validation loss decreased (0.439499 --> 0.439239).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 81.57375001907349
Epoch: 88, Steps: 88 | Train Loss: 0.2631307 Vali Loss: 0.4393839 Test Loss: 0.5190518
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 80.64225339889526
Epoch: 89, Steps: 88 | Train Loss: 0.2630396 Vali Loss: 0.4393720 Test Loss: 0.5190030
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 80.43566799163818
Epoch: 90, Steps: 88 | Train Loss: 0.2630650 Vali Loss: 0.4393416 Test Loss: 0.5189427
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 80.39980006217957
Epoch: 91, Steps: 88 | Train Loss: 0.2630791 Vali Loss: 0.4389052 Test Loss: 0.5188922
Validation loss decreased (0.439239 --> 0.438905).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 84.55132293701172
Epoch: 92, Steps: 88 | Train Loss: 0.2629595 Vali Loss: 0.4395012 Test Loss: 0.5188343
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 79.53742241859436
Epoch: 93, Steps: 88 | Train Loss: 0.2629195 Vali Loss: 0.4389301 Test Loss: 0.5187907
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 78.92100429534912
Epoch: 94, Steps: 88 | Train Loss: 0.2628604 Vali Loss: 0.4395000 Test Loss: 0.5187458
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 82.83581829071045
Epoch: 95, Steps: 88 | Train Loss: 0.2629751 Vali Loss: 0.4391042 Test Loss: 0.5186967
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 79.44987869262695
Epoch: 96, Steps: 88 | Train Loss: 0.2629026 Vali Loss: 0.4388486 Test Loss: 0.5186552
Validation loss decreased (0.438905 --> 0.438849).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 71.28968906402588
Epoch: 97, Steps: 88 | Train Loss: 0.2628281 Vali Loss: 0.4390596 Test Loss: 0.5186151
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 74.04552268981934
Epoch: 98, Steps: 88 | Train Loss: 0.2627488 Vali Loss: 0.4394718 Test Loss: 0.5185778
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 65.06275916099548
Epoch: 99, Steps: 88 | Train Loss: 0.2627934 Vali Loss: 0.4393964 Test Loss: 0.5185401
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 66.98943710327148
Epoch: 100, Steps: 88 | Train Loss: 0.2627877 Vali Loss: 0.4386925 Test Loss: 0.5185037
Validation loss decreased (0.438849 --> 0.438693).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 11381
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=50, out_features=250, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1379200000.0
params:  12750.0
Trainable parameters:  12750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 62.427560567855835
Epoch: 1, Steps: 88 | Train Loss: 0.3156472 Vali Loss: 0.4345998 Test Loss: 0.5114340
Validation loss decreased (inf --> 0.434600).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 64.78733491897583
Epoch: 2, Steps: 88 | Train Loss: 0.3132812 Vali Loss: 0.4336956 Test Loss: 0.5103008
Validation loss decreased (0.434600 --> 0.433696).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 62.883647203445435
Epoch: 3, Steps: 88 | Train Loss: 0.3127955 Vali Loss: 0.4339106 Test Loss: 0.5095938
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00045125
Epoch: 4 cost time: 66.13117384910583
Epoch: 4, Steps: 88 | Train Loss: 0.3126418 Vali Loss: 0.4340573 Test Loss: 0.5095999
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 66.93144798278809
Epoch: 5, Steps: 88 | Train Loss: 0.3126303 Vali Loss: 0.4336122 Test Loss: 0.5097862
Validation loss decreased (0.433696 --> 0.433612).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 67.01819896697998
Epoch: 6, Steps: 88 | Train Loss: 0.3126117 Vali Loss: 0.4335743 Test Loss: 0.5094116
Validation loss decreased (0.433612 --> 0.433574).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 63.807129859924316
Epoch: 7, Steps: 88 | Train Loss: 0.3126522 Vali Loss: 0.4337772 Test Loss: 0.5095392
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 65.08816814422607
Epoch: 8, Steps: 88 | Train Loss: 0.3126099 Vali Loss: 0.4336523 Test Loss: 0.5096459
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 63.593735694885254
Epoch: 9, Steps: 88 | Train Loss: 0.3124945 Vali Loss: 0.4333870 Test Loss: 0.5095271
Validation loss decreased (0.433574 --> 0.433387).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 64.48369812965393
Epoch: 10, Steps: 88 | Train Loss: 0.3125821 Vali Loss: 0.4334471 Test Loss: 0.5092249
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 66.21738028526306
Epoch: 11, Steps: 88 | Train Loss: 0.3125437 Vali Loss: 0.4335945 Test Loss: 0.5091811
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 68.76324939727783
Epoch: 12, Steps: 88 | Train Loss: 0.3124892 Vali Loss: 0.4334952 Test Loss: 0.5095441
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 65.74010515213013
Epoch: 13, Steps: 88 | Train Loss: 0.3125545 Vali Loss: 0.4333101 Test Loss: 0.5091466
Validation loss decreased (0.433387 --> 0.433310).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 64.95566320419312
Epoch: 14, Steps: 88 | Train Loss: 0.3124779 Vali Loss: 0.4336766 Test Loss: 0.5094001
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 63.303189754486084
Epoch: 15, Steps: 88 | Train Loss: 0.3123807 Vali Loss: 0.4333418 Test Loss: 0.5094188
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 66.01659512519836
Epoch: 16, Steps: 88 | Train Loss: 0.3124978 Vali Loss: 0.4332094 Test Loss: 0.5094839
Validation loss decreased (0.433310 --> 0.433209).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 66.1237096786499
Epoch: 17, Steps: 88 | Train Loss: 0.3124863 Vali Loss: 0.4335434 Test Loss: 0.5094318
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 64.78595185279846
Epoch: 18, Steps: 88 | Train Loss: 0.3124676 Vali Loss: 0.4339378 Test Loss: 0.5097391
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 68.73094892501831
Epoch: 19, Steps: 88 | Train Loss: 0.3124322 Vali Loss: 0.4336010 Test Loss: 0.5094602
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 61.67220616340637
Epoch: 20, Steps: 88 | Train Loss: 0.3124258 Vali Loss: 0.4332941 Test Loss: 0.5092785
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 62.76969051361084
Epoch: 21, Steps: 88 | Train Loss: 0.3123858 Vali Loss: 0.4331923 Test Loss: 0.5096748
Validation loss decreased (0.433209 --> 0.433192).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 68.84211659431458
Epoch: 22, Steps: 88 | Train Loss: 0.3123764 Vali Loss: 0.4338135 Test Loss: 0.5092584
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 67.54367852210999
Epoch: 23, Steps: 88 | Train Loss: 0.3124225 Vali Loss: 0.4336683 Test Loss: 0.5093548
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 69.11248779296875
Epoch: 24, Steps: 88 | Train Loss: 0.3124968 Vali Loss: 0.4338267 Test Loss: 0.5093029
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 68.35773372650146
Epoch: 25, Steps: 88 | Train Loss: 0.3124226 Vali Loss: 0.4331073 Test Loss: 0.5092151
Validation loss decreased (0.433192 --> 0.433107).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 69.7248477935791
Epoch: 26, Steps: 88 | Train Loss: 0.3124289 Vali Loss: 0.4334404 Test Loss: 0.5092543
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 73.53413605690002
Epoch: 27, Steps: 88 | Train Loss: 0.3124744 Vali Loss: 0.4335145 Test Loss: 0.5092261
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 71.57762241363525
Epoch: 28, Steps: 88 | Train Loss: 0.3124332 Vali Loss: 0.4338500 Test Loss: 0.5093393
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 68.48958086967468
Epoch: 29, Steps: 88 | Train Loss: 0.3123363 Vali Loss: 0.4336021 Test Loss: 0.5092068
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 74.04767298698425
Epoch: 30, Steps: 88 | Train Loss: 0.3124166 Vali Loss: 0.4335496 Test Loss: 0.5092922
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 70.30909085273743
Epoch: 31, Steps: 88 | Train Loss: 0.3123219 Vali Loss: 0.4339971 Test Loss: 0.5093664
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 70.70956873893738
Epoch: 32, Steps: 88 | Train Loss: 0.3123506 Vali Loss: 0.4335061 Test Loss: 0.5092511
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 71.43740820884705
Epoch: 33, Steps: 88 | Train Loss: 0.3124524 Vali Loss: 0.4332150 Test Loss: 0.5092376
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 72.03961873054504
Epoch: 34, Steps: 88 | Train Loss: 0.3123834 Vali Loss: 0.4335028 Test Loss: 0.5093283
EarlyStopping counter: 9 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 70.13607668876648
Epoch: 35, Steps: 88 | Train Loss: 0.3124553 Vali Loss: 0.4332896 Test Loss: 0.5092350
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j720_H5_FITS_custom_ftM_sl180_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.5071044564247131, mae:0.3293007016181946, rse:0.5823036432266235, corr:[0.26170287 0.26773167 0.2683981  0.26719448 0.2671504  0.26783592
 0.26756245 0.2673824  0.26744407 0.26726076 0.2670018  0.26659855
 0.26641703 0.26689547 0.2669918  0.2667386  0.26683995 0.26695287
 0.26692    0.26701218 0.26721218 0.26781917 0.26837748 0.268017
 0.26740187 0.26663595 0.2664932  0.2664306  0.26659963 0.26710176
 0.2671591  0.2669735  0.2670498  0.26705813 0.26688385 0.26663205
 0.26638773 0.26637185 0.26648608 0.26641312 0.26619884 0.2662067
 0.26624754 0.26616874 0.2662936  0.26660296 0.26656225 0.26610708
 0.26572725 0.26557505 0.26581872 0.26587346 0.26568747 0.2658948
 0.26629454 0.26631522 0.26607308 0.26590952 0.26591504 0.2660077
 0.26596308 0.265711   0.26555815 0.26552936 0.26540238 0.26540345
 0.26552227 0.26537672 0.26513958 0.2652337  0.26533294 0.26522663
 0.2649783  0.26471338 0.26468182 0.26483202 0.26481935 0.26464114
 0.26463336 0.26481542 0.2649561  0.2649307  0.26462564 0.26426104
 0.2641393  0.2641896  0.26417586 0.26419836 0.26430348 0.26423034
 0.26436356 0.26460433 0.26482302 0.2650049  0.2649906  0.2647843
 0.26462713 0.26476744 0.2650612  0.26507634 0.26482928 0.2645762
 0.26441252 0.26432323 0.264276   0.26435414 0.26447275 0.26449892
 0.26431942 0.26415074 0.26406288 0.26416627 0.26440686 0.26445675
 0.26445895 0.26446408 0.26452085 0.2645914  0.26469672 0.26471817
 0.26475063 0.2646528  0.26455593 0.26443866 0.26441106 0.26457188
 0.26468283 0.2646687  0.2646431  0.2645145  0.26432064 0.26449615
 0.26489437 0.2654099  0.2654955  0.2652979  0.26514646 0.26478526
 0.26500133 0.26513478 0.26517582 0.2656009  0.26609567 0.26609433
 0.26597098 0.26563472 0.2654621  0.26542404 0.26536238 0.26547226
 0.26548326 0.26531008 0.26522508 0.26517668 0.2653031  0.2663269
 0.26765352 0.2686374  0.26834205 0.2678928  0.267665   0.26724875
 0.26733136 0.26779166 0.268142   0.26913163 0.27027237 0.2701309
 0.26951304 0.26854026 0.26793522 0.267507   0.26741123 0.26782444
 0.26811302 0.26824    0.26836887 0.26823637 0.2680441  0.267992
 0.26791844 0.2680201  0.26794913 0.26793534 0.2680681  0.26823008
 0.26826173 0.26809585 0.26805952 0.2685067  0.26890948 0.26866272
 0.26814124 0.26736107 0.26706767 0.26714182 0.26723063 0.26752776
 0.26782626 0.26791495 0.26799902 0.26806858 0.26794267 0.26770517
 0.26747018 0.26732048 0.26718616 0.26715645 0.2671065  0.26704627
 0.2670399  0.26690313 0.26670054 0.26670653 0.26672637 0.26656157
 0.2663779  0.2662008  0.26622376 0.26629245 0.26632282 0.26648533
 0.2667356  0.26679966 0.26667103 0.26661646 0.26671728 0.26678413
 0.26666725 0.26634997 0.26599622 0.26584244 0.26590037 0.2660464
 0.26626578 0.26647994 0.26664802 0.26668936 0.2667055  0.26667175
 0.26654854 0.2662547  0.26583204 0.26546276 0.2654774  0.26581237
 0.2660887  0.2661786  0.26615876 0.26620263 0.26630905 0.26627925
 0.26598185 0.2656379  0.26545906 0.26544344 0.26553732 0.26566273
 0.26571408 0.26577315 0.26603514 0.26633918 0.26641372 0.26632947
 0.26627797 0.266328   0.266436   0.26629227 0.2658674  0.26571146
 0.26581526 0.26592222 0.2660367  0.26625776 0.26632616 0.26625326
 0.26607028 0.2658779  0.26565313 0.2656292  0.26569214 0.26558498
 0.26562855 0.26564848 0.26550722 0.26551852 0.265863   0.26618624
 0.26628482 0.26608935 0.2659248  0.26594704 0.26609278 0.2661977
 0.26617345 0.2662055  0.2663621  0.26645637 0.26648855 0.26669106
 0.2669078  0.26707777 0.26668835 0.2661693  0.26609892 0.26613712
 0.26646948 0.2666606  0.26676905 0.2671871  0.267671   0.26755637
 0.2673216  0.26703677 0.26701638 0.267024   0.266937   0.2670195
 0.2669712  0.2668159  0.26692167 0.26709983 0.26738158 0.26814887
 0.26895398 0.26952925 0.2691118  0.26872852 0.26873055 0.26860446
 0.26873493 0.26913184 0.26964417 0.2707251  0.27153075 0.2708561
 0.26969025 0.26846468 0.2679027  0.26763222 0.26752394 0.26786298
 0.26814023 0.26816702 0.2682052  0.2682138  0.26835507 0.26863232
 0.26870131 0.2687145  0.2685311  0.26852745 0.26858914 0.26854914
 0.26847172 0.26829934 0.26826078 0.26864907 0.268839   0.26846322
 0.26814875 0.26780012 0.26763174 0.2675823  0.2675633  0.26772088
 0.26789942 0.26795694 0.26792973 0.26779583 0.26766738 0.26776043
 0.26786485 0.26784623 0.26758236 0.2673774  0.26730844 0.26725996
 0.26718503 0.26704404 0.2669582  0.26702416 0.26694965 0.26671475
 0.26665783 0.26674023 0.26689452 0.26689938 0.2668205  0.26685557
 0.2670256  0.26722714 0.2672888  0.26719734 0.26710472 0.26705387
 0.26695585 0.266791   0.26657495 0.26636016 0.2662688  0.26634482
 0.2664307  0.2664231  0.2663736  0.2664422  0.2665703  0.2666189
 0.26649404 0.2663566  0.26634878 0.26623613 0.2660014  0.26603213
 0.26635382 0.26667655 0.2668144  0.26666835 0.26636058 0.26628178
 0.26650283 0.26674527 0.26679373 0.26666754 0.26637053 0.26607826
 0.26599082 0.2658791  0.26575702 0.26584747 0.26594576 0.266046
 0.26618204 0.26637608 0.26658982 0.2666669  0.26652104 0.2662941
 0.26619878 0.2662305  0.26623818 0.26623732 0.26626644 0.26626182
 0.26616293 0.26615265 0.2661597  0.26617622 0.26625156 0.2661831
 0.26606482 0.26588243 0.2657785  0.26586407 0.2661144  0.26633465
 0.2664546  0.26641008 0.2664006  0.26641983 0.2663738  0.26620817
 0.26595014 0.26581755 0.26589    0.2659783  0.26612458 0.26645926
 0.26684114 0.26724645 0.26701924 0.26644593 0.26623064 0.2663365
 0.26649073 0.2664983  0.2664798  0.26672068 0.26707017 0.26711375
 0.26722562 0.26726758 0.26737604 0.26736432 0.26722598 0.2672401
 0.2669946  0.26646802 0.26629314 0.26651847 0.26719072 0.26832777
 0.2693521  0.2700896  0.26960394 0.26888892 0.26850092 0.26816982
 0.26820543 0.2684921  0.26880825 0.2696429  0.27042386 0.27014542
 0.26955584 0.26873034 0.26818663 0.26780316 0.26756993 0.26776707
 0.26801273 0.26809472 0.26811567 0.26805466 0.26803988 0.2680645
 0.26801145 0.26804203 0.26777852 0.26756677 0.26756507 0.26759744
 0.2676025  0.26753643 0.26753843 0.26790112 0.26826337 0.26825002
 0.26810002 0.26751918 0.26703122 0.26692674 0.26714802 0.26754433
 0.26776105 0.26784393 0.26800025 0.26812154 0.2681258  0.26807207
 0.26786157 0.26758164 0.2672697  0.2671071  0.26711413 0.26709983
 0.26684254 0.26640055 0.26616907 0.26629904 0.26635924 0.26625744
 0.2662893  0.26639032 0.2665508  0.2666178  0.2666482  0.2669112
 0.26727614 0.26732677 0.26706633 0.2668918  0.26690045 0.26684135
 0.26660988 0.2664166  0.2663697  0.26629636 0.26614302 0.26603344
 0.26595598 0.26584616 0.26577678 0.26586267 0.26599294 0.2660765
 0.26607227 0.26604974 0.26617932 0.2662911  0.26621827 0.26616225
 0.2662666  0.26631278 0.26604754 0.26551554 0.2650177  0.26480734
 0.26482606 0.2648511  0.26485974 0.26495028 0.26501402 0.26493815
 0.2648012  0.26474378 0.26484802 0.2650596  0.26508492 0.26491195
 0.2648518  0.2651016  0.26546738 0.26554734 0.2654327  0.2654064
 0.2654009  0.2652757  0.2650744  0.2648775  0.26466325 0.2643854
 0.26410702 0.26408246 0.26422974 0.26437396 0.26441988 0.26432636
 0.26414222 0.2638667  0.26362506 0.2636154  0.26388562 0.26411358
 0.26416942 0.2640664  0.2641583  0.26449198 0.26479787 0.26486245
 0.26461956 0.26427975 0.26400295 0.2637214  0.26363435 0.26402703
 0.26469427 0.26530415 0.26512024 0.26444337 0.26403153 0.26402974
 0.2642146  0.26442137 0.26470888 0.2652695  0.26577055 0.26567748
 0.26543266 0.26506743 0.26501143 0.2651318  0.26514918 0.26533133
 0.26534957 0.2650012  0.26472327 0.2646112  0.26474535 0.26536798
 0.26620358 0.26694983 0.26656055 0.26586595 0.2654686  0.2651057
 0.26514155 0.2655539  0.26615143 0.26745462 0.26863933 0.26854345
 0.2679204  0.26683465 0.26623663 0.26618683 0.26637354 0.2666769
 0.26669046 0.2665145  0.2663775  0.26616755 0.2660265  0.2660826
 0.2662231  0.26652983 0.26661775 0.26657128 0.26646885 0.26630533
 0.26618853 0.2661692  0.26646176 0.26728374 0.26799297 0.26800206
 0.2677066  0.2670214  0.26662603 0.2665215  0.26650435 0.26676592
 0.26705122 0.26717946 0.26728705 0.26725295 0.26704037 0.2669385
 0.26689342 0.2666073  0.26634276 0.26665753 0.26710993 0.26678884
 0.26607275 0.26596    0.266362   0.2662666  0.26543444 0.2661057 ]
