Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j192_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j192_H5_FITS_custom_ftM_sl180_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11909
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=50, out_features=103, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  568230400.0
params:  5253.0
Trainable parameters:  5253
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 62.90024137496948
Epoch: 1, Steps: 93 | Train Loss: 1.0402658 Vali Loss: 0.8682455 Test Loss: 1.0378585
Validation loss decreased (inf --> 0.868246).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 59.32212972640991
Epoch: 2, Steps: 93 | Train Loss: 0.5594534 Vali Loss: 0.6015716 Test Loss: 0.7335219
Validation loss decreased (0.868246 --> 0.601572).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 59.05377125740051
Epoch: 3, Steps: 93 | Train Loss: 0.4260470 Vali Loss: 0.5124117 Test Loss: 0.6296317
Validation loss decreased (0.601572 --> 0.512412).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 59.986870765686035
Epoch: 4, Steps: 93 | Train Loss: 0.3735082 Vali Loss: 0.4685918 Test Loss: 0.5769527
Validation loss decreased (0.512412 --> 0.468592).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 61.61543655395508
Epoch: 5, Steps: 93 | Train Loss: 0.3447873 Vali Loss: 0.4412193 Test Loss: 0.5443582
Validation loss decreased (0.468592 --> 0.441219).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 60.54967474937439
Epoch: 6, Steps: 93 | Train Loss: 0.3265256 Vali Loss: 0.4234523 Test Loss: 0.5228667
Validation loss decreased (0.441219 --> 0.423452).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 62.86121082305908
Epoch: 7, Steps: 93 | Train Loss: 0.3142617 Vali Loss: 0.4111305 Test Loss: 0.5082155
Validation loss decreased (0.423452 --> 0.411131).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 57.057868242263794
Epoch: 8, Steps: 93 | Train Loss: 0.3057651 Vali Loss: 0.4025351 Test Loss: 0.4979668
Validation loss decreased (0.411131 --> 0.402535).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 59.60400176048279
Epoch: 9, Steps: 93 | Train Loss: 0.2997582 Vali Loss: 0.3961782 Test Loss: 0.4907496
Validation loss decreased (0.402535 --> 0.396178).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 59.74095416069031
Epoch: 10, Steps: 93 | Train Loss: 0.2953843 Vali Loss: 0.3908750 Test Loss: 0.4855872
Validation loss decreased (0.396178 --> 0.390875).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 61.92731833457947
Epoch: 11, Steps: 93 | Train Loss: 0.2922104 Vali Loss: 0.3882587 Test Loss: 0.4817303
Validation loss decreased (0.390875 --> 0.388259).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 59.507488489151
Epoch: 12, Steps: 93 | Train Loss: 0.2898159 Vali Loss: 0.3853556 Test Loss: 0.4788496
Validation loss decreased (0.388259 --> 0.385356).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 57.09691500663757
Epoch: 13, Steps: 93 | Train Loss: 0.2880416 Vali Loss: 0.3838780 Test Loss: 0.4766803
Validation loss decreased (0.385356 --> 0.383878).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 62.447980642318726
Epoch: 14, Steps: 93 | Train Loss: 0.2867045 Vali Loss: 0.3822819 Test Loss: 0.4750424
Validation loss decreased (0.383878 --> 0.382282).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 59.58159589767456
Epoch: 15, Steps: 93 | Train Loss: 0.2856622 Vali Loss: 0.3810995 Test Loss: 0.4737799
Validation loss decreased (0.382282 --> 0.381099).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 61.34639286994934
Epoch: 16, Steps: 93 | Train Loss: 0.2848484 Vali Loss: 0.3805181 Test Loss: 0.4727409
Validation loss decreased (0.381099 --> 0.380518).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 55.687942028045654
Epoch: 17, Steps: 93 | Train Loss: 0.2842080 Vali Loss: 0.3795024 Test Loss: 0.4720120
Validation loss decreased (0.380518 --> 0.379502).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 60.38740921020508
Epoch: 18, Steps: 93 | Train Loss: 0.2836742 Vali Loss: 0.3784753 Test Loss: 0.4713362
Validation loss decreased (0.379502 --> 0.378475).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 56.94945812225342
Epoch: 19, Steps: 93 | Train Loss: 0.2832663 Vali Loss: 0.3779554 Test Loss: 0.4708929
Validation loss decreased (0.378475 --> 0.377955).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 61.585020542144775
Epoch: 20, Steps: 93 | Train Loss: 0.2829497 Vali Loss: 0.3782159 Test Loss: 0.4704730
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 56.20743942260742
Epoch: 21, Steps: 93 | Train Loss: 0.2826977 Vali Loss: 0.3775645 Test Loss: 0.4701915
Validation loss decreased (0.377955 --> 0.377565).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 54.62867712974548
Epoch: 22, Steps: 93 | Train Loss: 0.2824896 Vali Loss: 0.3775975 Test Loss: 0.4699154
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 56.45527100563049
Epoch: 23, Steps: 93 | Train Loss: 0.2823016 Vali Loss: 0.3771777 Test Loss: 0.4697361
Validation loss decreased (0.377565 --> 0.377178).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 57.55774140357971
Epoch: 24, Steps: 93 | Train Loss: 0.2821745 Vali Loss: 0.3766546 Test Loss: 0.4695277
Validation loss decreased (0.377178 --> 0.376655).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 58.98647332191467
Epoch: 25, Steps: 93 | Train Loss: 0.2820212 Vali Loss: 0.3771208 Test Loss: 0.4693947
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 54.02606153488159
Epoch: 26, Steps: 93 | Train Loss: 0.2819420 Vali Loss: 0.3764729 Test Loss: 0.4693114
Validation loss decreased (0.376655 --> 0.376473).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 56.6306586265564
Epoch: 27, Steps: 93 | Train Loss: 0.2818677 Vali Loss: 0.3766288 Test Loss: 0.4692196
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 56.221867084503174
Epoch: 28, Steps: 93 | Train Loss: 0.2817735 Vali Loss: 0.3764913 Test Loss: 0.4690517
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 59.87868046760559
Epoch: 29, Steps: 93 | Train Loss: 0.2817276 Vali Loss: 0.3759924 Test Loss: 0.4689703
Validation loss decreased (0.376473 --> 0.375992).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 55.719629764556885
Epoch: 30, Steps: 93 | Train Loss: 0.2816591 Vali Loss: 0.3763862 Test Loss: 0.4689467
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 58.479093074798584
Epoch: 31, Steps: 93 | Train Loss: 0.2816195 Vali Loss: 0.3764639 Test Loss: 0.4688948
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 47.24643039703369
Epoch: 32, Steps: 93 | Train Loss: 0.2815801 Vali Loss: 0.3761004 Test Loss: 0.4688301
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 44.44100379943848
Epoch: 33, Steps: 93 | Train Loss: 0.2815641 Vali Loss: 0.3759983 Test Loss: 0.4688087
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 44.81071972846985
Epoch: 34, Steps: 93 | Train Loss: 0.2815337 Vali Loss: 0.3759230 Test Loss: 0.4687599
Validation loss decreased (0.375992 --> 0.375923).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 45.11600160598755
Epoch: 35, Steps: 93 | Train Loss: 0.2814880 Vali Loss: 0.3760309 Test Loss: 0.4687541
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 44.631465435028076
Epoch: 36, Steps: 93 | Train Loss: 0.2814477 Vali Loss: 0.3761508 Test Loss: 0.4687149
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 43.83311057090759
Epoch: 37, Steps: 93 | Train Loss: 0.2814211 Vali Loss: 0.3762727 Test Loss: 0.4686740
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 46.274025201797485
Epoch: 38, Steps: 93 | Train Loss: 0.2814328 Vali Loss: 0.3765823 Test Loss: 0.4686449
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 44.09444880485535
Epoch: 39, Steps: 93 | Train Loss: 0.2813931 Vali Loss: 0.3764722 Test Loss: 0.4686350
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 47.09381461143494
Epoch: 40, Steps: 93 | Train Loss: 0.2813801 Vali Loss: 0.3760791 Test Loss: 0.4686275
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 42.80424475669861
Epoch: 41, Steps: 93 | Train Loss: 0.2813492 Vali Loss: 0.3762755 Test Loss: 0.4685924
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 45.56055951118469
Epoch: 42, Steps: 93 | Train Loss: 0.2813505 Vali Loss: 0.3763610 Test Loss: 0.4686041
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 43.60653567314148
Epoch: 43, Steps: 93 | Train Loss: 0.2813356 Vali Loss: 0.3760023 Test Loss: 0.4685623
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 44.707934617996216
Epoch: 44, Steps: 93 | Train Loss: 0.2813306 Vali Loss: 0.3764702 Test Loss: 0.4685754
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j192_H5_FITS_custom_ftM_sl180_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4679214358329773, mae:0.30973169207572937, rse:0.5645670294761658, corr:[0.2858959  0.28675193 0.28902918 0.28946924 0.28856924 0.2890026
 0.2900855  0.29036468 0.2897604  0.28941274 0.2895072  0.28906345
 0.28812197 0.2878125  0.28757945 0.28738585 0.28742626 0.28745192
 0.287711   0.28810894 0.2883499  0.28912258 0.2898564  0.2893733
 0.28865764 0.28806922 0.28828642 0.28823504 0.28795192 0.28811476
 0.28840518 0.28860378 0.28865582 0.28847787 0.28829524 0.28794605
 0.287283   0.28687713 0.28702557 0.2873418  0.28746706 0.28752065
 0.2874824  0.28738788 0.28745422 0.28767076 0.28776425 0.28761816
 0.28732398 0.2868697  0.2869439  0.28725234 0.28732035 0.28731307
 0.2872763  0.28729302 0.2874877  0.28750467 0.28717592 0.28686684
 0.28664204 0.28637204 0.28631493 0.28648585 0.2864223  0.2862167
 0.28610146 0.28600937 0.28612074 0.28655282 0.28673035 0.28655052
 0.2863439  0.286162   0.2859644  0.2857948  0.28576267 0.28592986
 0.28611895 0.28603524 0.28583184 0.28584018 0.28591985 0.28586122
 0.28580636 0.28584635 0.2858755  0.2859198  0.28589255 0.28564063
 0.2853225  0.28508347 0.2849852  0.2851362  0.28529084 0.2852687
 0.28526917 0.28544596 0.285567   0.28537968 0.2850686  0.2848464
 0.28477705 0.2847173  0.28444162 0.28416884 0.28431436 0.28467155
 0.28480056 0.28481695 0.28476912 0.28465658 0.2845791  0.28446165
 0.28426236 0.2841604  0.28408906 0.28400964 0.2841849  0.28443953
 0.28463817 0.28477    0.28511286 0.28531647 0.28520346 0.28493997
 0.28442407 0.28401133 0.2841574  0.28438282 0.28451556 0.28497264
 0.2855334  0.28598225 0.2858513  0.28553355 0.285322   0.28520593
 0.28518417 0.28512403 0.28517818 0.28570998 0.28620082 0.28618506
 0.28616112 0.28591463 0.28587708 0.28609243 0.2860757  0.28594682
 0.28559992 0.28499618 0.28476113 0.2849986  0.28552723 0.2866312
 0.28777745 0.28867304 0.2883293  0.28786677 0.2876218  0.28702676
 0.28690413 0.28729892 0.28785074 0.2893025  0.2903321  0.2894453
 0.28857228 0.2881346  0.28798443 0.28734565 0.2869951  0.28740475
 0.28761885 0.28731537 0.28695905 0.2870954  0.28769475 0.2879462
 0.28775856 0.28815648 0.28835213 0.28803477 0.28801578 0.28764623
 0.28669068 0.2865223  0.2868638  0.28686702 0.28682238 0.28798732]
