Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j336_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j336_H5_FITS_custom_ftM_sl180_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11765
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=50, out_features=143, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  788902400.0
params:  7293.0
Trainable parameters:  7293
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 77.05662298202515
Epoch: 1, Steps: 91 | Train Loss: 1.1501740 Vali Loss: 0.9780962 Test Loss: 1.1787093
Validation loss decreased (inf --> 0.978096).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 71.09545731544495
Epoch: 2, Steps: 91 | Train Loss: 0.6295108 Vali Loss: 0.6701488 Test Loss: 0.8161066
Validation loss decreased (0.978096 --> 0.670149).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 71.96965837478638
Epoch: 3, Steps: 91 | Train Loss: 0.4695683 Vali Loss: 0.5569904 Test Loss: 0.6834028
Validation loss decreased (0.670149 --> 0.556990).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 75.78968524932861
Epoch: 4, Steps: 91 | Train Loss: 0.4046941 Vali Loss: 0.5018681 Test Loss: 0.6180360
Validation loss decreased (0.556990 --> 0.501868).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 73.69826650619507
Epoch: 5, Steps: 91 | Train Loss: 0.3700629 Vali Loss: 0.4696196 Test Loss: 0.5783941
Validation loss decreased (0.501868 --> 0.469620).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 76.02805590629578
Epoch: 6, Steps: 91 | Train Loss: 0.3484820 Vali Loss: 0.4476640 Test Loss: 0.5521252
Validation loss decreased (0.469620 --> 0.447664).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 76.88877511024475
Epoch: 7, Steps: 91 | Train Loss: 0.3338210 Vali Loss: 0.4325932 Test Loss: 0.5339695
Validation loss decreased (0.447664 --> 0.432593).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 71.5198745727539
Epoch: 8, Steps: 91 | Train Loss: 0.3234939 Vali Loss: 0.4218658 Test Loss: 0.5212475
Validation loss decreased (0.432593 --> 0.421866).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 77.60658288002014
Epoch: 9, Steps: 91 | Train Loss: 0.3161484 Vali Loss: 0.4145237 Test Loss: 0.5121285
Validation loss decreased (0.421866 --> 0.414524).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 76.35922026634216
Epoch: 10, Steps: 91 | Train Loss: 0.3109339 Vali Loss: 0.4088037 Test Loss: 0.5055327
Validation loss decreased (0.414524 --> 0.408804).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 76.02716445922852
Epoch: 11, Steps: 91 | Train Loss: 0.3069884 Vali Loss: 0.4044726 Test Loss: 0.5006168
Validation loss decreased (0.408804 --> 0.404473).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 72.81888842582703
Epoch: 12, Steps: 91 | Train Loss: 0.3040411 Vali Loss: 0.4013149 Test Loss: 0.4969626
Validation loss decreased (0.404473 --> 0.401315).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 73.90635633468628
Epoch: 13, Steps: 91 | Train Loss: 0.3020049 Vali Loss: 0.3985024 Test Loss: 0.4942613
Validation loss decreased (0.401315 --> 0.398502).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 74.83066034317017
Epoch: 14, Steps: 91 | Train Loss: 0.3001228 Vali Loss: 0.3967465 Test Loss: 0.4921083
Validation loss decreased (0.398502 --> 0.396747).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 70.99800181388855
Epoch: 15, Steps: 91 | Train Loss: 0.2988637 Vali Loss: 0.3957016 Test Loss: 0.4903978
Validation loss decreased (0.396747 --> 0.395702).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 73.0056381225586
Epoch: 16, Steps: 91 | Train Loss: 0.2978676 Vali Loss: 0.3940366 Test Loss: 0.4891775
Validation loss decreased (0.395702 --> 0.394037).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 71.29196166992188
Epoch: 17, Steps: 91 | Train Loss: 0.2970437 Vali Loss: 0.3934512 Test Loss: 0.4881021
Validation loss decreased (0.394037 --> 0.393451).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 66.9320924282074
Epoch: 18, Steps: 91 | Train Loss: 0.2963799 Vali Loss: 0.3919815 Test Loss: 0.4872716
Validation loss decreased (0.393451 --> 0.391982).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 70.33575630187988
Epoch: 19, Steps: 91 | Train Loss: 0.2958312 Vali Loss: 0.3915837 Test Loss: 0.4866313
Validation loss decreased (0.391982 --> 0.391584).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 73.25459814071655
Epoch: 20, Steps: 91 | Train Loss: 0.2953401 Vali Loss: 0.3914775 Test Loss: 0.4861734
Validation loss decreased (0.391584 --> 0.391477).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 68.3373053073883
Epoch: 21, Steps: 91 | Train Loss: 0.2949460 Vali Loss: 0.3909998 Test Loss: 0.4856223
Validation loss decreased (0.391477 --> 0.391000).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 67.24417448043823
Epoch: 22, Steps: 91 | Train Loss: 0.2946977 Vali Loss: 0.3903995 Test Loss: 0.4852868
Validation loss decreased (0.391000 --> 0.390399).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 71.31088662147522
Epoch: 23, Steps: 91 | Train Loss: 0.2944071 Vali Loss: 0.3899315 Test Loss: 0.4849759
Validation loss decreased (0.390399 --> 0.389931).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 67.11251163482666
Epoch: 24, Steps: 91 | Train Loss: 0.2942740 Vali Loss: 0.3899653 Test Loss: 0.4847344
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 64.95011758804321
Epoch: 25, Steps: 91 | Train Loss: 0.2940037 Vali Loss: 0.3895971 Test Loss: 0.4844809
Validation loss decreased (0.389931 --> 0.389597).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 56.10110306739807
Epoch: 26, Steps: 91 | Train Loss: 0.2940086 Vali Loss: 0.3895271 Test Loss: 0.4843574
Validation loss decreased (0.389597 --> 0.389527).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 53.478686571121216
Epoch: 27, Steps: 91 | Train Loss: 0.2937130 Vali Loss: 0.3892164 Test Loss: 0.4841611
Validation loss decreased (0.389527 --> 0.389216).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 54.335800647735596
Epoch: 28, Steps: 91 | Train Loss: 0.2936785 Vali Loss: 0.3892573 Test Loss: 0.4839505
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 58.60652685165405
Epoch: 29, Steps: 91 | Train Loss: 0.2934735 Vali Loss: 0.3893841 Test Loss: 0.4839360
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 54.00909352302551
Epoch: 30, Steps: 91 | Train Loss: 0.2934865 Vali Loss: 0.3892078 Test Loss: 0.4838058
Validation loss decreased (0.389216 --> 0.389208).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 55.34688210487366
Epoch: 31, Steps: 91 | Train Loss: 0.2934292 Vali Loss: 0.3887262 Test Loss: 0.4836749
Validation loss decreased (0.389208 --> 0.388726).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 56.89968490600586
Epoch: 32, Steps: 91 | Train Loss: 0.2932869 Vali Loss: 0.3888641 Test Loss: 0.4836242
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 53.883262634277344
Epoch: 33, Steps: 91 | Train Loss: 0.2933736 Vali Loss: 0.3889623 Test Loss: 0.4835574
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 54.416563749313354
Epoch: 34, Steps: 91 | Train Loss: 0.2932523 Vali Loss: 0.3890536 Test Loss: 0.4835525
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 57.44753932952881
Epoch: 35, Steps: 91 | Train Loss: 0.2932520 Vali Loss: 0.3887977 Test Loss: 0.4834925
EarlyStopping counter: 4 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 46.80805444717407
Epoch: 36, Steps: 91 | Train Loss: 0.2933048 Vali Loss: 0.3886663 Test Loss: 0.4834670
Validation loss decreased (0.388726 --> 0.388666).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 41.55334782600403
Epoch: 37, Steps: 91 | Train Loss: 0.2931647 Vali Loss: 0.3885685 Test Loss: 0.4834231
Validation loss decreased (0.388666 --> 0.388569).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 42.48175764083862
Epoch: 38, Steps: 91 | Train Loss: 0.2931973 Vali Loss: 0.3886765 Test Loss: 0.4833844
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 42.00430965423584
Epoch: 39, Steps: 91 | Train Loss: 0.2931261 Vali Loss: 0.3884577 Test Loss: 0.4833185
Validation loss decreased (0.388569 --> 0.388458).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 41.905638694763184
Epoch: 40, Steps: 91 | Train Loss: 0.2930515 Vali Loss: 0.3884046 Test Loss: 0.4832981
Validation loss decreased (0.388458 --> 0.388405).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 43.16996645927429
Epoch: 41, Steps: 91 | Train Loss: 0.2930811 Vali Loss: 0.3886339 Test Loss: 0.4832735
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 42.35886740684509
Epoch: 42, Steps: 91 | Train Loss: 0.2930805 Vali Loss: 0.3883391 Test Loss: 0.4832551
Validation loss decreased (0.388405 --> 0.388339).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 41.29399299621582
Epoch: 43, Steps: 91 | Train Loss: 0.2929312 Vali Loss: 0.3884964 Test Loss: 0.4832467
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 43.07597255706787
Epoch: 44, Steps: 91 | Train Loss: 0.2929859 Vali Loss: 0.3884567 Test Loss: 0.4831983
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 42.20592951774597
Epoch: 45, Steps: 91 | Train Loss: 0.2928925 Vali Loss: 0.3881253 Test Loss: 0.4831840
Validation loss decreased (0.388339 --> 0.388125).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 40.99856209754944
Epoch: 46, Steps: 91 | Train Loss: 0.2929342 Vali Loss: 0.3880216 Test Loss: 0.4831915
Validation loss decreased (0.388125 --> 0.388022).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 43.87919497489929
Epoch: 47, Steps: 91 | Train Loss: 0.2929329 Vali Loss: 0.3884647 Test Loss: 0.4831549
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 41.81911325454712
Epoch: 48, Steps: 91 | Train Loss: 0.2929051 Vali Loss: 0.3882861 Test Loss: 0.4831539
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 41.77941823005676
Epoch: 49, Steps: 91 | Train Loss: 0.2929283 Vali Loss: 0.3883430 Test Loss: 0.4831610
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 42.83749055862427
Epoch: 50, Steps: 91 | Train Loss: 0.2928922 Vali Loss: 0.3884367 Test Loss: 0.4831282
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 41.83000063896179
Epoch: 51, Steps: 91 | Train Loss: 0.2928060 Vali Loss: 0.3884009 Test Loss: 0.4831262
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 41.78740453720093
Epoch: 52, Steps: 91 | Train Loss: 0.2928653 Vali Loss: 0.3881415 Test Loss: 0.4830956
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 43.1210253238678
Epoch: 53, Steps: 91 | Train Loss: 0.2929182 Vali Loss: 0.3884991 Test Loss: 0.4830791
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 41.62697887420654
Epoch: 54, Steps: 91 | Train Loss: 0.2928593 Vali Loss: 0.3884199 Test Loss: 0.4830900
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 40.454633712768555
Epoch: 55, Steps: 91 | Train Loss: 0.2929165 Vali Loss: 0.3884011 Test Loss: 0.4830788
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 42.485856771469116
Epoch: 56, Steps: 91 | Train Loss: 0.2927360 Vali Loss: 0.3884553 Test Loss: 0.4830851
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j336_H5_FITS_custom_ftM_sl180_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.48126691579818726, mae:0.31544116139411926, rse:0.5701541304588318, corr:[0.28154013 0.28115216 0.28352702 0.28319073 0.28218785 0.28289843
 0.2842287  0.28444964 0.2835805  0.2835319  0.28409725 0.2835448
 0.2824181  0.2824569  0.2825714  0.2822873  0.2821133  0.282092
 0.282412   0.28253576 0.28215083 0.28267977 0.28375107 0.28363723
 0.2830152  0.2824638  0.2826891  0.28255814 0.282313   0.28265443
 0.2831098  0.28326392 0.28311005 0.28289443 0.28289193 0.2827201
 0.2821136  0.28171065 0.28185362 0.28219223 0.2824114  0.2825621
 0.28260234 0.2824189  0.2821141  0.28201184 0.28231266 0.28264892
 0.28255415 0.28202754 0.28217074 0.28266415 0.28276476 0.28265068
 0.2826157  0.28277114 0.2828986  0.28257605 0.28200898 0.2817644
 0.2817267  0.28160083 0.28166014 0.28196305 0.28208536 0.28198493
 0.2819886  0.28194094 0.28203398 0.28221598 0.28205788 0.28180954
 0.28174564 0.28165647 0.28156263 0.2816406  0.28182304 0.28197023
 0.28196228 0.28182036 0.28183332 0.28191975 0.28166044 0.28131667
 0.28145066 0.28179047 0.28181282 0.28173098 0.28180194 0.2819207
 0.28203875 0.28189003 0.28164846 0.2817079  0.28180802 0.28161877
 0.28136176 0.28138608 0.28156194 0.28157675 0.28146994 0.28135237
 0.28131348 0.28128365 0.28108427 0.2808703  0.28091002 0.2809994
 0.2808893  0.28084782 0.28086334 0.28081548 0.2807472  0.28071925
 0.2808172  0.28087544 0.2806878  0.28044748 0.28059283 0.28087428
 0.28102964 0.28098017 0.2810537  0.2810955  0.2810216  0.28094724
 0.28076592 0.28063133 0.28063077 0.28044343 0.2804027  0.2810485
 0.2817847  0.2820707  0.28163943 0.28125322 0.28122613 0.28122765
 0.28120488 0.28121477 0.28131315 0.2817442  0.2821202  0.28200954
 0.28192577 0.28174132 0.28172553 0.28183386 0.28189462 0.28205487
 0.28190863 0.28144312 0.28129703 0.2814095  0.28173915 0.28277647
 0.2839634  0.28477716 0.28422743 0.28347525 0.28306788 0.2826709
 0.2827931  0.28315088 0.28348386 0.2848244  0.28617942 0.2856521
 0.28442928 0.2833023  0.28311443 0.28296322 0.2827287  0.2829602
 0.28321484 0.28332913 0.2834562  0.2834381  0.28347307 0.28365883
 0.28365993 0.28362325 0.28330442 0.28302783 0.282891   0.2828168
 0.28280556 0.2827711  0.28275973 0.28321087 0.28371817 0.2834368
 0.28272817 0.2819247  0.2820053  0.2823822  0.28237763 0.28238806
 0.2824976  0.28263327 0.28282675 0.28281203 0.28255457 0.28232983
 0.282058   0.28175095 0.28163695 0.2818851  0.2820094  0.28190655
 0.2817646  0.28155443 0.28146958 0.28166533 0.28169864 0.28147328
 0.28133187 0.28115967 0.28117746 0.28143036 0.28171164 0.28185692
 0.28182274 0.2816897  0.28166205 0.28176293 0.28170106 0.2814645
 0.2813269  0.281285   0.28116912 0.2811512  0.28130892 0.28136247
 0.28118637 0.2809398  0.28095204 0.28117043 0.28126213 0.2809494
 0.28059813 0.28065312 0.2810002  0.2811892  0.2812762  0.28143904
 0.28154993 0.2814177  0.28103855 0.28073162 0.28076652 0.28087315
 0.28077108 0.28066212 0.280626   0.28050795 0.28042525 0.2805662
 0.28069964 0.28062344 0.2804455  0.2803554  0.28035033 0.2802813
 0.28011912 0.2800851  0.28033692 0.2805284  0.28052405 0.2805008
 0.28044382 0.28040627 0.28037304 0.2801295  0.27985838 0.27985373
 0.2798881  0.27978274 0.27963638 0.27969983 0.27984655 0.27991736
 0.27994677 0.27992243 0.27982786 0.27965522 0.27957964 0.27969512
 0.2798587  0.27975088 0.27971628 0.2799501  0.28009912 0.28007898
 0.2799006  0.2796244  0.2795434  0.27957296 0.27952656 0.27973145
 0.28020748 0.2808737  0.2809417  0.280777   0.28080148 0.28080606
 0.28084823 0.28082818 0.28089902 0.28140065 0.28151548 0.28089494
 0.28083834 0.28118405 0.28126907 0.28105065 0.28100306 0.28111014
 0.280982   0.2805021  0.2799988  0.280034   0.28069264 0.28174877
 0.2829347  0.28435332 0.28453675 0.28411484 0.2840847  0.28299543
 0.28158247 0.28155258 0.28153902 0.2808137  0.28101015 0.28403786]
