Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j96_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j96_H8_FITS_custom_ftM_sl720_ll48_pl96_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=258, out_features=292, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8312272896.0
params:  75628.0
Trainable parameters:  75628
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 205.49932670593262
Epoch: 1, Steps: 89 | Train Loss: 1.0248478 Vali Loss: 1.1293519 Test Loss: 1.2934233
Validation loss decreased (inf --> 1.129352).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 213.10914421081543
Epoch: 2, Steps: 89 | Train Loss: 0.7830093 Vali Loss: 1.0101800 Test Loss: 1.1566880
Validation loss decreased (1.129352 --> 1.010180).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 218.30768203735352
Epoch: 3, Steps: 89 | Train Loss: 0.6850030 Vali Loss: 0.9433191 Test Loss: 1.0830683
Validation loss decreased (1.010180 --> 0.943319).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 214.90277576446533
Epoch: 4, Steps: 89 | Train Loss: 0.6136300 Vali Loss: 0.8927425 Test Loss: 1.0257127
Validation loss decreased (0.943319 --> 0.892742).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 198.90252375602722
Epoch: 5, Steps: 89 | Train Loss: 0.5545672 Vali Loss: 0.8458810 Test Loss: 0.9733766
Validation loss decreased (0.892742 --> 0.845881).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 223.4565076828003
Epoch: 6, Steps: 89 | Train Loss: 0.5044429 Vali Loss: 0.8054217 Test Loss: 0.9275773
Validation loss decreased (0.845881 --> 0.805422).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 205.48826241493225
Epoch: 7, Steps: 89 | Train Loss: 0.4612910 Vali Loss: 0.7665148 Test Loss: 0.8818408
Validation loss decreased (0.805422 --> 0.766515).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 180.5739278793335
Epoch: 8, Steps: 89 | Train Loss: 0.4237344 Vali Loss: 0.7331955 Test Loss: 0.8435709
Validation loss decreased (0.766515 --> 0.733195).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 214.3323142528534
Epoch: 9, Steps: 89 | Train Loss: 0.3909236 Vali Loss: 0.7014698 Test Loss: 0.8098241
Validation loss decreased (0.733195 --> 0.701470).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 214.57660794258118
Epoch: 10, Steps: 89 | Train Loss: 0.3619664 Vali Loss: 0.6776144 Test Loss: 0.7805762
Validation loss decreased (0.701470 --> 0.677614).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 206.72408628463745
Epoch: 11, Steps: 89 | Train Loss: 0.3363813 Vali Loss: 0.6503368 Test Loss: 0.7508686
Validation loss decreased (0.677614 --> 0.650337).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 188.41922640800476
Epoch: 12, Steps: 89 | Train Loss: 0.3136007 Vali Loss: 0.6289421 Test Loss: 0.7244347
Validation loss decreased (0.650337 --> 0.628942).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 209.21001434326172
Epoch: 13, Steps: 89 | Train Loss: 0.2933729 Vali Loss: 0.6062143 Test Loss: 0.6982431
Validation loss decreased (0.628942 --> 0.606214).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 209.29477381706238
Epoch: 14, Steps: 89 | Train Loss: 0.2751598 Vali Loss: 0.5901240 Test Loss: 0.6801873
Validation loss decreased (0.606214 --> 0.590124).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 200.78800296783447
Epoch: 15, Steps: 89 | Train Loss: 0.2588673 Vali Loss: 0.5729584 Test Loss: 0.6617226
Validation loss decreased (0.590124 --> 0.572958).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 205.96133089065552
Epoch: 16, Steps: 89 | Train Loss: 0.2441450 Vali Loss: 0.5582522 Test Loss: 0.6430427
Validation loss decreased (0.572958 --> 0.558252).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 210.0964195728302
Epoch: 17, Steps: 89 | Train Loss: 0.2309019 Vali Loss: 0.5442420 Test Loss: 0.6287313
Validation loss decreased (0.558252 --> 0.544242).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 200.58917951583862
Epoch: 18, Steps: 89 | Train Loss: 0.2188571 Vali Loss: 0.5302173 Test Loss: 0.6120216
Validation loss decreased (0.544242 --> 0.530217).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 216.64684796333313
Epoch: 19, Steps: 89 | Train Loss: 0.2079631 Vali Loss: 0.5198388 Test Loss: 0.5997299
Validation loss decreased (0.530217 --> 0.519839).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 207.87654089927673
Epoch: 20, Steps: 89 | Train Loss: 0.1980196 Vali Loss: 0.5103841 Test Loss: 0.5887049
Validation loss decreased (0.519839 --> 0.510384).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 197.36642503738403
Epoch: 21, Steps: 89 | Train Loss: 0.1889407 Vali Loss: 0.4990906 Test Loss: 0.5764122
Validation loss decreased (0.510384 --> 0.499091).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 218.56932973861694
Epoch: 22, Steps: 89 | Train Loss: 0.1806793 Vali Loss: 0.4912618 Test Loss: 0.5662376
Validation loss decreased (0.499091 --> 0.491262).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 213.71324920654297
Epoch: 23, Steps: 89 | Train Loss: 0.1730710 Vali Loss: 0.4803807 Test Loss: 0.5556194
Validation loss decreased (0.491262 --> 0.480381).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 210.29962277412415
Epoch: 24, Steps: 89 | Train Loss: 0.1661496 Vali Loss: 0.4739412 Test Loss: 0.5481084
Validation loss decreased (0.480381 --> 0.473941).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 225.7785804271698
Epoch: 25, Steps: 89 | Train Loss: 0.1597269 Vali Loss: 0.4672418 Test Loss: 0.5397933
Validation loss decreased (0.473941 --> 0.467242).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 209.92457032203674
Epoch: 26, Steps: 89 | Train Loss: 0.1538440 Vali Loss: 0.4613267 Test Loss: 0.5331780
Validation loss decreased (0.467242 --> 0.461327).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 227.47839999198914
Epoch: 27, Steps: 89 | Train Loss: 0.1484102 Vali Loss: 0.4542999 Test Loss: 0.5259522
Validation loss decreased (0.461327 --> 0.454300).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 227.36394953727722
Epoch: 28, Steps: 89 | Train Loss: 0.1434106 Vali Loss: 0.4489990 Test Loss: 0.5199648
Validation loss decreased (0.454300 --> 0.448999).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 217.12426853179932
Epoch: 29, Steps: 89 | Train Loss: 0.1387563 Vali Loss: 0.4448411 Test Loss: 0.5138113
Validation loss decreased (0.448999 --> 0.444841).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 188.92317056655884
Epoch: 30, Steps: 89 | Train Loss: 0.1344946 Vali Loss: 0.4398225 Test Loss: 0.5087852
Validation loss decreased (0.444841 --> 0.439823).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 170.57487654685974
Epoch: 31, Steps: 89 | Train Loss: 0.1305033 Vali Loss: 0.4344706 Test Loss: 0.5034700
Validation loss decreased (0.439823 --> 0.434471).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 175.91325879096985
Epoch: 32, Steps: 89 | Train Loss: 0.1267891 Vali Loss: 0.4317077 Test Loss: 0.4994383
Validation loss decreased (0.434471 --> 0.431708).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 217.6832857131958
Epoch: 33, Steps: 89 | Train Loss: 0.1233784 Vali Loss: 0.4278289 Test Loss: 0.4955154
Validation loss decreased (0.431708 --> 0.427829).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 190.94439578056335
Epoch: 34, Steps: 89 | Train Loss: 0.1201642 Vali Loss: 0.4232594 Test Loss: 0.4908740
Validation loss decreased (0.427829 --> 0.423259).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 216.14518976211548
Epoch: 35, Steps: 89 | Train Loss: 0.1171729 Vali Loss: 0.4196424 Test Loss: 0.4861127
Validation loss decreased (0.423259 --> 0.419642).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 213.86772656440735
Epoch: 36, Steps: 89 | Train Loss: 0.1143779 Vali Loss: 0.4175009 Test Loss: 0.4832910
Validation loss decreased (0.419642 --> 0.417501).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 211.89542698860168
Epoch: 37, Steps: 89 | Train Loss: 0.1118138 Vali Loss: 0.4137616 Test Loss: 0.4794845
Validation loss decreased (0.417501 --> 0.413762).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 212.38841557502747
Epoch: 38, Steps: 89 | Train Loss: 0.1093638 Vali Loss: 0.4115452 Test Loss: 0.4766811
Validation loss decreased (0.413762 --> 0.411545).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 243.08382105827332
Epoch: 39, Steps: 89 | Train Loss: 0.1070938 Vali Loss: 0.4073295 Test Loss: 0.4734573
Validation loss decreased (0.411545 --> 0.407329).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 246.91195487976074
Epoch: 40, Steps: 89 | Train Loss: 0.1049657 Vali Loss: 0.4071105 Test Loss: 0.4714296
Validation loss decreased (0.407329 --> 0.407110).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 189.92057180404663
Epoch: 41, Steps: 89 | Train Loss: 0.1029703 Vali Loss: 0.4034969 Test Loss: 0.4685777
Validation loss decreased (0.407110 --> 0.403497).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 199.14221692085266
Epoch: 42, Steps: 89 | Train Loss: 0.1010967 Vali Loss: 0.4004438 Test Loss: 0.4659852
Validation loss decreased (0.403497 --> 0.400444).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 215.42973279953003
Epoch: 43, Steps: 89 | Train Loss: 0.0993461 Vali Loss: 0.3995355 Test Loss: 0.4641154
Validation loss decreased (0.400444 --> 0.399536).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 211.96855473518372
Epoch: 44, Steps: 89 | Train Loss: 0.0976955 Vali Loss: 0.3963796 Test Loss: 0.4615725
Validation loss decreased (0.399536 --> 0.396380).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 248.95464444160461
Epoch: 45, Steps: 89 | Train Loss: 0.0961387 Vali Loss: 0.3950942 Test Loss: 0.4593360
Validation loss decreased (0.396380 --> 0.395094).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 214.5605115890503
Epoch: 46, Steps: 89 | Train Loss: 0.0946605 Vali Loss: 0.3937251 Test Loss: 0.4574261
Validation loss decreased (0.395094 --> 0.393725).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 180.5140471458435
Epoch: 47, Steps: 89 | Train Loss: 0.0932871 Vali Loss: 0.3928240 Test Loss: 0.4557751
Validation loss decreased (0.393725 --> 0.392824).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 212.2697319984436
Epoch: 48, Steps: 89 | Train Loss: 0.0919989 Vali Loss: 0.3912520 Test Loss: 0.4538262
Validation loss decreased (0.392824 --> 0.391252).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 209.0384337902069
Epoch: 49, Steps: 89 | Train Loss: 0.0907621 Vali Loss: 0.3896595 Test Loss: 0.4524000
Validation loss decreased (0.391252 --> 0.389660).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 191.72587895393372
Epoch: 50, Steps: 89 | Train Loss: 0.0896235 Vali Loss: 0.3877794 Test Loss: 0.4511688
Validation loss decreased (0.389660 --> 0.387779).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 202.01756048202515
Epoch: 51, Steps: 89 | Train Loss: 0.0885350 Vali Loss: 0.3863656 Test Loss: 0.4495712
Validation loss decreased (0.387779 --> 0.386366).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 199.9265570640564
Epoch: 52, Steps: 89 | Train Loss: 0.0874966 Vali Loss: 0.3847902 Test Loss: 0.4484147
Validation loss decreased (0.386366 --> 0.384790).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 197.2063980102539
Epoch: 53, Steps: 89 | Train Loss: 0.0865278 Vali Loss: 0.3842823 Test Loss: 0.4472216
Validation loss decreased (0.384790 --> 0.384282).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 165.8084704875946
Epoch: 54, Steps: 89 | Train Loss: 0.0856017 Vali Loss: 0.3829150 Test Loss: 0.4461742
Validation loss decreased (0.384282 --> 0.382915).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 171.90237092971802
Epoch: 55, Steps: 89 | Train Loss: 0.0847438 Vali Loss: 0.3840313 Test Loss: 0.4448818
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 191.40786695480347
Epoch: 56, Steps: 89 | Train Loss: 0.0839174 Vali Loss: 0.3805030 Test Loss: 0.4438614
Validation loss decreased (0.382915 --> 0.380503).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 189.08632707595825
Epoch: 57, Steps: 89 | Train Loss: 0.0831408 Vali Loss: 0.3801150 Test Loss: 0.4427579
Validation loss decreased (0.380503 --> 0.380115).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 184.55745792388916
Epoch: 58, Steps: 89 | Train Loss: 0.0824080 Vali Loss: 0.3801584 Test Loss: 0.4418666
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 159.688157081604
Epoch: 59, Steps: 89 | Train Loss: 0.0817126 Vali Loss: 0.3796786 Test Loss: 0.4412126
Validation loss decreased (0.380115 --> 0.379679).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 198.26597905158997
Epoch: 60, Steps: 89 | Train Loss: 0.0810530 Vali Loss: 0.3777524 Test Loss: 0.4401363
Validation loss decreased (0.379679 --> 0.377752).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 190.00003266334534
Epoch: 61, Steps: 89 | Train Loss: 0.0804359 Vali Loss: 0.3769904 Test Loss: 0.4392141
Validation loss decreased (0.377752 --> 0.376990).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 202.74910974502563
Epoch: 62, Steps: 89 | Train Loss: 0.0798258 Vali Loss: 0.3753084 Test Loss: 0.4384956
Validation loss decreased (0.376990 --> 0.375308).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 221.16682124137878
Epoch: 63, Steps: 89 | Train Loss: 0.0792766 Vali Loss: 0.3760359 Test Loss: 0.4379512
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 129.00147914886475
Epoch: 64, Steps: 89 | Train Loss: 0.0787445 Vali Loss: 0.3755933 Test Loss: 0.4370364
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 192.01348209381104
Epoch: 65, Steps: 89 | Train Loss: 0.0782370 Vali Loss: 0.3756752 Test Loss: 0.4364784
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 110.88186264038086
Epoch: 66, Steps: 89 | Train Loss: 0.0777556 Vali Loss: 0.3742557 Test Loss: 0.4359663
Validation loss decreased (0.375308 --> 0.374256).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 193.3364293575287
Epoch: 67, Steps: 89 | Train Loss: 0.0773135 Vali Loss: 0.3744017 Test Loss: 0.4353153
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 153.8850450515747
Epoch: 68, Steps: 89 | Train Loss: 0.0768817 Vali Loss: 0.3722005 Test Loss: 0.4347071
Validation loss decreased (0.374256 --> 0.372201).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 105.11533689498901
Epoch: 69, Steps: 89 | Train Loss: 0.0764698 Vali Loss: 0.3732910 Test Loss: 0.4344143
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 108.28258323669434
Epoch: 70, Steps: 89 | Train Loss: 0.0760718 Vali Loss: 0.3723642 Test Loss: 0.4337697
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 100.7544686794281
Epoch: 71, Steps: 89 | Train Loss: 0.0757139 Vali Loss: 0.3719302 Test Loss: 0.4332810
Validation loss decreased (0.372201 --> 0.371930).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 97.26943755149841
Epoch: 72, Steps: 89 | Train Loss: 0.0753478 Vali Loss: 0.3725569 Test Loss: 0.4328397
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 160.10232973098755
Epoch: 73, Steps: 89 | Train Loss: 0.0750257 Vali Loss: 0.3703805 Test Loss: 0.4324570
Validation loss decreased (0.371930 --> 0.370380).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 114.1952133178711
Epoch: 74, Steps: 89 | Train Loss: 0.0747090 Vali Loss: 0.3712429 Test Loss: 0.4320391
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 143.7887089252472
Epoch: 75, Steps: 89 | Train Loss: 0.0744194 Vali Loss: 0.3690930 Test Loss: 0.4316652
Validation loss decreased (0.370380 --> 0.369093).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 166.56091380119324
Epoch: 76, Steps: 89 | Train Loss: 0.0741313 Vali Loss: 0.3700821 Test Loss: 0.4312605
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 155.59302067756653
Epoch: 77, Steps: 89 | Train Loss: 0.0738655 Vali Loss: 0.3701316 Test Loss: 0.4309683
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 94.60912895202637
Epoch: 78, Steps: 89 | Train Loss: 0.0736001 Vali Loss: 0.3693999 Test Loss: 0.4306057
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 103.72961831092834
Epoch: 79, Steps: 89 | Train Loss: 0.0733635 Vali Loss: 0.3688532 Test Loss: 0.4302515
Validation loss decreased (0.369093 --> 0.368853).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 156.47281193733215
Epoch: 80, Steps: 89 | Train Loss: 0.0731376 Vali Loss: 0.3689547 Test Loss: 0.4299685
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 160.6528081893921
Epoch: 81, Steps: 89 | Train Loss: 0.0729148 Vali Loss: 0.3697718 Test Loss: 0.4296783
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 127.37525200843811
Epoch: 82, Steps: 89 | Train Loss: 0.0726973 Vali Loss: 0.3684415 Test Loss: 0.4293666
Validation loss decreased (0.368853 --> 0.368441).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 94.34579610824585
Epoch: 83, Steps: 89 | Train Loss: 0.0724948 Vali Loss: 0.3681224 Test Loss: 0.4291245
Validation loss decreased (0.368441 --> 0.368122).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 84.20557141304016
Epoch: 84, Steps: 89 | Train Loss: 0.0723105 Vali Loss: 0.3684219 Test Loss: 0.4289204
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 92.58883738517761
Epoch: 85, Steps: 89 | Train Loss: 0.0721457 Vali Loss: 0.3686888 Test Loss: 0.4286964
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 88.97021579742432
Epoch: 86, Steps: 89 | Train Loss: 0.0719766 Vali Loss: 0.3669517 Test Loss: 0.4284552
Validation loss decreased (0.368122 --> 0.366952).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 113.76707601547241
Epoch: 87, Steps: 89 | Train Loss: 0.0717988 Vali Loss: 0.3671090 Test Loss: 0.4282530
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 135.18310737609863
Epoch: 88, Steps: 89 | Train Loss: 0.0716446 Vali Loss: 0.3677603 Test Loss: 0.4280433
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 100.07673931121826
Epoch: 89, Steps: 89 | Train Loss: 0.0715043 Vali Loss: 0.3668668 Test Loss: 0.4278690
Validation loss decreased (0.366952 --> 0.366867).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 142.89620208740234
Epoch: 90, Steps: 89 | Train Loss: 0.0713564 Vali Loss: 0.3654637 Test Loss: 0.4276529
Validation loss decreased (0.366867 --> 0.365464).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 151.5332362651825
Epoch: 91, Steps: 89 | Train Loss: 0.0712327 Vali Loss: 0.3656588 Test Loss: 0.4274960
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 144.58726334571838
Epoch: 92, Steps: 89 | Train Loss: 0.0711070 Vali Loss: 0.3655635 Test Loss: 0.4273646
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 122.48869013786316
Epoch: 93, Steps: 89 | Train Loss: 0.0709927 Vali Loss: 0.3665996 Test Loss: 0.4272090
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 129.61220955848694
Epoch: 94, Steps: 89 | Train Loss: 0.0708835 Vali Loss: 0.3651738 Test Loss: 0.4270810
Validation loss decreased (0.365464 --> 0.365174).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 117.51078867912292
Epoch: 95, Steps: 89 | Train Loss: 0.0707720 Vali Loss: 0.3658146 Test Loss: 0.4269392
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 140.19062161445618
Epoch: 96, Steps: 89 | Train Loss: 0.0706521 Vali Loss: 0.3657193 Test Loss: 0.4268049
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 151.3322412967682
Epoch: 97, Steps: 89 | Train Loss: 0.0705542 Vali Loss: 0.3658544 Test Loss: 0.4266423
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 160.69899988174438
Epoch: 98, Steps: 89 | Train Loss: 0.0704661 Vali Loss: 0.3639493 Test Loss: 0.4265325
Validation loss decreased (0.365174 --> 0.363949).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 154.39773535728455
Epoch: 99, Steps: 89 | Train Loss: 0.0703811 Vali Loss: 0.3657632 Test Loss: 0.4264284
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 153.37228417396545
Epoch: 100, Steps: 89 | Train Loss: 0.0703084 Vali Loss: 0.3654573 Test Loss: 0.4263096
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=258, out_features=292, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8312272896.0
params:  75628.0
Trainable parameters:  75628
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 161.75884222984314
Epoch: 1, Steps: 89 | Train Loss: 0.2373402 Vali Loss: 0.3273039 Test Loss: 0.3930034
Validation loss decreased (inf --> 0.327304).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 150.85226893424988
Epoch: 2, Steps: 89 | Train Loss: 0.2330318 Vali Loss: 0.3274315 Test Loss: 0.3917850
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.000475
Epoch: 3 cost time: 149.24575757980347
Epoch: 3, Steps: 89 | Train Loss: 0.2326432 Vali Loss: 0.3248257 Test Loss: 0.3917241
Validation loss decreased (0.327304 --> 0.324826).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 154.83099555969238
Epoch: 4, Steps: 89 | Train Loss: 0.2323678 Vali Loss: 0.3275431 Test Loss: 0.3915447
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 152.232186794281
Epoch: 5, Steps: 89 | Train Loss: 0.2322628 Vali Loss: 0.3267889 Test Loss: 0.3914189
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 155.3792953491211
Epoch: 6, Steps: 89 | Train Loss: 0.2321534 Vali Loss: 0.3267880 Test Loss: 0.3912144
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 164.06632494926453
Epoch: 7, Steps: 89 | Train Loss: 0.2321213 Vali Loss: 0.3252541 Test Loss: 0.3908457
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 158.1933081150055
Epoch: 8, Steps: 89 | Train Loss: 0.2320147 Vali Loss: 0.3259808 Test Loss: 0.3912917
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 173.0650761127472
Epoch: 9, Steps: 89 | Train Loss: 0.2319127 Vali Loss: 0.3250660 Test Loss: 0.3904335
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 154.58682322502136
Epoch: 10, Steps: 89 | Train Loss: 0.2320382 Vali Loss: 0.3272543 Test Loss: 0.3910280
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 165.5956015586853
Epoch: 11, Steps: 89 | Train Loss: 0.2318974 Vali Loss: 0.3259405 Test Loss: 0.3906943
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 157.67934203147888
Epoch: 12, Steps: 89 | Train Loss: 0.2318202 Vali Loss: 0.3262031 Test Loss: 0.3905297
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 171.6903097629547
Epoch: 13, Steps: 89 | Train Loss: 0.2317165 Vali Loss: 0.3263651 Test Loss: 0.3901676
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j96_H8_FITS_custom_ftM_sl720_ll48_pl96_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.38883042335510254, mae:0.27332764863967896, rse:0.5163373947143555, corr:[0.28276014 0.29277527 0.29396692 0.29331025 0.29270667 0.2927704
 0.29240093 0.29300758 0.29399306 0.2938865  0.29338112 0.29293197
 0.2926179  0.2924003  0.29217517 0.29230413 0.29256693 0.29267436
 0.29308933 0.29377577 0.29394853 0.2930223  0.29154742 0.2912186
 0.29283032 0.293325   0.29320347 0.29292738 0.29254702 0.29253998
 0.2930122  0.2936582  0.2940372  0.29384053 0.29330656 0.29268238
 0.29221007 0.29233876 0.29274476 0.2930543  0.29369798 0.29445735
 0.2947525  0.29450607 0.29372036 0.29295418 0.29270926 0.29263192
 0.2927448  0.2931715  0.29346868 0.29272607 0.29192442 0.29189587
 0.29201996 0.29229733 0.29294837 0.29301754 0.29241812 0.29239392
 0.29273522 0.2924645  0.29228222 0.29249746 0.29279324 0.29346365
 0.29384872 0.29326892 0.29269785 0.2922334  0.2912267  0.2911488
 0.29212826 0.29219007 0.29168013 0.29174012 0.29176602 0.29168573
 0.29194695 0.2920156  0.2921096  0.29253802 0.2921408  0.291018
 0.29076308 0.29113534 0.2915054  0.29191425 0.29158562 0.2912806
 0.29206774 0.2916769  0.29056323 0.29099655 0.29028913 0.29161793]
