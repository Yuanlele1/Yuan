Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6007795200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 130.32775259017944
Epoch: 1, Steps: 84 | Train Loss: 1.0968284 Vali Loss: 1.0028502 Test Loss: 1.1773057
Validation loss decreased (inf --> 1.002850).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 138.22104406356812
Epoch: 2, Steps: 84 | Train Loss: 0.7157260 Vali Loss: 0.7921734 Test Loss: 0.9238138
Validation loss decreased (1.002850 --> 0.792173).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 140.19911909103394
Epoch: 3, Steps: 84 | Train Loss: 0.5710338 Vali Loss: 0.6653908 Test Loss: 0.7734364
Validation loss decreased (0.792173 --> 0.665391).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 142.73297905921936
Epoch: 4, Steps: 84 | Train Loss: 0.4773099 Vali Loss: 0.5791316 Test Loss: 0.6723266
Validation loss decreased (0.665391 --> 0.579132).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 146.56860756874084
Epoch: 5, Steps: 84 | Train Loss: 0.4135288 Vali Loss: 0.5201877 Test Loss: 0.6033170
Validation loss decreased (0.579132 --> 0.520188).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 143.09282398223877
Epoch: 6, Steps: 84 | Train Loss: 0.3697530 Vali Loss: 0.4792625 Test Loss: 0.5558954
Validation loss decreased (0.520188 --> 0.479262).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 157.80840945243835
Epoch: 7, Steps: 84 | Train Loss: 0.3396431 Vali Loss: 0.4513948 Test Loss: 0.5239265
Validation loss decreased (0.479262 --> 0.451395).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 177.41741132736206
Epoch: 8, Steps: 84 | Train Loss: 0.3190126 Vali Loss: 0.4325824 Test Loss: 0.5013326
Validation loss decreased (0.451395 --> 0.432582).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 181.99589157104492
Epoch: 9, Steps: 84 | Train Loss: 0.3049915 Vali Loss: 0.4201723 Test Loss: 0.4865090
Validation loss decreased (0.432582 --> 0.420172).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 177.51406359672546
Epoch: 10, Steps: 84 | Train Loss: 0.2954815 Vali Loss: 0.4117723 Test Loss: 0.4774310
Validation loss decreased (0.420172 --> 0.411772).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 172.8543848991394
Epoch: 11, Steps: 84 | Train Loss: 0.2891209 Vali Loss: 0.4058941 Test Loss: 0.4707763
Validation loss decreased (0.411772 --> 0.405894).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 181.29417061805725
Epoch: 12, Steps: 84 | Train Loss: 0.2847517 Vali Loss: 0.4019967 Test Loss: 0.4662594
Validation loss decreased (0.405894 --> 0.401997).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 179.42362308502197
Epoch: 13, Steps: 84 | Train Loss: 0.2818370 Vali Loss: 0.3990085 Test Loss: 0.4632058
Validation loss decreased (0.401997 --> 0.399009).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 173.9335961341858
Epoch: 14, Steps: 84 | Train Loss: 0.2799064 Vali Loss: 0.3974691 Test Loss: 0.4620546
Validation loss decreased (0.399009 --> 0.397469).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 180.48739910125732
Epoch: 15, Steps: 84 | Train Loss: 0.2785472 Vali Loss: 0.3963264 Test Loss: 0.4601982
Validation loss decreased (0.397469 --> 0.396326).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 179.90271639823914
Epoch: 16, Steps: 84 | Train Loss: 0.2776262 Vali Loss: 0.3955747 Test Loss: 0.4595741
Validation loss decreased (0.396326 --> 0.395575).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 180.86498951911926
Epoch: 17, Steps: 84 | Train Loss: 0.2770350 Vali Loss: 0.3950520 Test Loss: 0.4589937
Validation loss decreased (0.395575 --> 0.395052).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 181.71618175506592
Epoch: 18, Steps: 84 | Train Loss: 0.2765120 Vali Loss: 0.3950104 Test Loss: 0.4585904
Validation loss decreased (0.395052 --> 0.395010).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 177.73150300979614
Epoch: 19, Steps: 84 | Train Loss: 0.2761983 Vali Loss: 0.3946204 Test Loss: 0.4583875
Validation loss decreased (0.395010 --> 0.394620).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 153.8031530380249
Epoch: 20, Steps: 84 | Train Loss: 0.2759825 Vali Loss: 0.3944184 Test Loss: 0.4582904
Validation loss decreased (0.394620 --> 0.394418).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 146.7824251651764
Epoch: 21, Steps: 84 | Train Loss: 0.2757039 Vali Loss: 0.3936161 Test Loss: 0.4579792
Validation loss decreased (0.394418 --> 0.393616).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 141.5329761505127
Epoch: 22, Steps: 84 | Train Loss: 0.2757250 Vali Loss: 0.3935120 Test Loss: 0.4577747
Validation loss decreased (0.393616 --> 0.393512).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 118.06601285934448
Epoch: 23, Steps: 84 | Train Loss: 0.2754166 Vali Loss: 0.3938124 Test Loss: 0.4576145
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 111.89830923080444
Epoch: 24, Steps: 84 | Train Loss: 0.2754924 Vali Loss: 0.3932444 Test Loss: 0.4576264
Validation loss decreased (0.393512 --> 0.393244).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 112.36682391166687
Epoch: 25, Steps: 84 | Train Loss: 0.2753036 Vali Loss: 0.3929619 Test Loss: 0.4574663
Validation loss decreased (0.393244 --> 0.392962).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 113.23655581474304
Epoch: 26, Steps: 84 | Train Loss: 0.2753024 Vali Loss: 0.3934321 Test Loss: 0.4575698
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 115.4438226222992
Epoch: 27, Steps: 84 | Train Loss: 0.2753041 Vali Loss: 0.3929705 Test Loss: 0.4576306
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 113.16189289093018
Epoch: 28, Steps: 84 | Train Loss: 0.2751929 Vali Loss: 0.3931662 Test Loss: 0.4576488
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 111.6707181930542
Epoch: 29, Steps: 84 | Train Loss: 0.2751110 Vali Loss: 0.3932722 Test Loss: 0.4574829
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 112.25574517250061
Epoch: 30, Steps: 84 | Train Loss: 0.2750489 Vali Loss: 0.3924410 Test Loss: 0.4572493
Validation loss decreased (0.392962 --> 0.392441).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 119.48817658424377
Epoch: 31, Steps: 84 | Train Loss: 0.2750508 Vali Loss: 0.3929216 Test Loss: 0.4574229
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 116.14897298812866
Epoch: 32, Steps: 84 | Train Loss: 0.2749294 Vali Loss: 0.3929766 Test Loss: 0.4571858
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 115.93847727775574
Epoch: 33, Steps: 84 | Train Loss: 0.2750057 Vali Loss: 0.3928752 Test Loss: 0.4571147
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 119.07893800735474
Epoch: 34, Steps: 84 | Train Loss: 0.2749452 Vali Loss: 0.3922505 Test Loss: 0.4572463
Validation loss decreased (0.392441 --> 0.392251).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 116.1365110874176
Epoch: 35, Steps: 84 | Train Loss: 0.2749922 Vali Loss: 0.3930122 Test Loss: 0.4571585
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 115.25594902038574
Epoch: 36, Steps: 84 | Train Loss: 0.2749693 Vali Loss: 0.3933074 Test Loss: 0.4571231
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 96.54170560836792
Epoch: 37, Steps: 84 | Train Loss: 0.2748499 Vali Loss: 0.3927837 Test Loss: 0.4572331
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 96.5676577091217
Epoch: 38, Steps: 84 | Train Loss: 0.2749116 Vali Loss: 0.3930323 Test Loss: 0.4570871
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 95.17211723327637
Epoch: 39, Steps: 84 | Train Loss: 0.2748375 Vali Loss: 0.3925730 Test Loss: 0.4573029
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 100.45243072509766
Epoch: 40, Steps: 84 | Train Loss: 0.2747989 Vali Loss: 0.3927465 Test Loss: 0.4572136
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 93.26205158233643
Epoch: 41, Steps: 84 | Train Loss: 0.2747501 Vali Loss: 0.3921775 Test Loss: 0.4571227
Validation loss decreased (0.392251 --> 0.392178).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 95.55355429649353
Epoch: 42, Steps: 84 | Train Loss: 0.2747773 Vali Loss: 0.3925861 Test Loss: 0.4571064
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 99.28200507164001
Epoch: 43, Steps: 84 | Train Loss: 0.2746619 Vali Loss: 0.3924521 Test Loss: 0.4571126
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 101.3470048904419
Epoch: 44, Steps: 84 | Train Loss: 0.2748230 Vali Loss: 0.3925003 Test Loss: 0.4571020
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 102.86989331245422
Epoch: 45, Steps: 84 | Train Loss: 0.2747620 Vali Loss: 0.3924680 Test Loss: 0.4570413
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 100.27867364883423
Epoch: 46, Steps: 84 | Train Loss: 0.2748061 Vali Loss: 0.3925618 Test Loss: 0.4569676
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 96.16235995292664
Epoch: 47, Steps: 84 | Train Loss: 0.2747444 Vali Loss: 0.3920594 Test Loss: 0.4570038
Validation loss decreased (0.392178 --> 0.392059).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 90.06641960144043
Epoch: 48, Steps: 84 | Train Loss: 0.2747677 Vali Loss: 0.3930836 Test Loss: 0.4570520
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 85.13673949241638
Epoch: 49, Steps: 84 | Train Loss: 0.2747733 Vali Loss: 0.3924225 Test Loss: 0.4569364
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 86.75305461883545
Epoch: 50, Steps: 84 | Train Loss: 0.2746810 Vali Loss: 0.3925647 Test Loss: 0.4570763
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 86.00580286979675
Epoch: 51, Steps: 84 | Train Loss: 0.2746091 Vali Loss: 0.3928738 Test Loss: 0.4569925
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 85.62502026557922
Epoch: 52, Steps: 84 | Train Loss: 0.2746731 Vali Loss: 0.3927897 Test Loss: 0.4569260
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 88.20747542381287
Epoch: 53, Steps: 84 | Train Loss: 0.2745634 Vali Loss: 0.3922207 Test Loss: 0.4570052
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 88.29760360717773
Epoch: 54, Steps: 84 | Train Loss: 0.2747046 Vali Loss: 0.3921210 Test Loss: 0.4569523
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 84.1444239616394
Epoch: 55, Steps: 84 | Train Loss: 0.2746626 Vali Loss: 0.3921928 Test Loss: 0.4569066
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 90.73126864433289
Epoch: 56, Steps: 84 | Train Loss: 0.2746893 Vali Loss: 0.3923133 Test Loss: 0.4569773
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 87.66218948364258
Epoch: 57, Steps: 84 | Train Loss: 0.2747252 Vali Loss: 0.3926218 Test Loss: 0.4569554
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j720_H5_FITS_custom_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4564695656299591, mae:0.3103457987308502, rse:0.5524674654006958, corr:[0.26888478 0.2643212  0.26418164 0.2703297  0.2735981  0.27319142
 0.272383   0.27293542 0.27387044 0.27345812 0.2715654  0.26973358
 0.26923478 0.26952246 0.2693865  0.2686732  0.26837602 0.2691125
 0.27022275 0.27072382 0.27058476 0.27050853 0.27105874 0.27239344
 0.27370957 0.27353457 0.27298847 0.27300516 0.2732769  0.27305838
 0.27212307 0.27104    0.27055544 0.2706848  0.2707078  0.270186
 0.26957723 0.2695162  0.2700783  0.2707307  0.27107382 0.27123147
 0.27149177 0.27184257 0.27197543 0.27169254 0.2712893  0.27145192
 0.2721505  0.27252713 0.27239582 0.27183104 0.27116394 0.27076077
 0.27062386 0.2704463  0.2700785  0.26972005 0.26963636 0.26981628
 0.27005836 0.27011877 0.27016482 0.27045915 0.27097774 0.27138326
 0.27140483 0.2712086  0.27115488 0.27135423 0.27152973 0.27149472
 0.27112886 0.27075508 0.27067563 0.2706789  0.27049297 0.27010113
 0.26973963 0.26962805 0.26974425 0.26988724 0.26979008 0.2696017
 0.26964462 0.26998323 0.27040932 0.27068916 0.2708059  0.27084252
 0.27104232 0.2711133  0.2709569  0.27071816 0.27059013 0.27071503
 0.27075967 0.27061033 0.27025595 0.2697918  0.2694676  0.26937458
 0.26937562 0.26929745 0.26917812 0.26926723 0.26947027 0.26966524
 0.2697726  0.2698341  0.27001092 0.27037457 0.2707231  0.27077702
 0.27079856 0.27071446 0.27066645 0.27063128 0.2706267  0.27051803
 0.2702939  0.2702757  0.27046263 0.27048153 0.27028918 0.2700444
 0.26996455 0.2700187  0.27010256 0.27016544 0.27018166 0.27033085
 0.27063963 0.27099296 0.27127832 0.2714829  0.27157524 0.27128726
 0.2712665  0.2711151  0.27085564 0.27068537 0.2706868  0.27086565
 0.2710413  0.27090514 0.2706031  0.27036217 0.27029148 0.2703002
 0.27026233 0.27019545 0.27022454 0.2705108  0.2708571  0.27110276
 0.271236   0.27136883 0.27160427 0.27188817 0.2720576  0.27204612
 0.2719795  0.27191392 0.27190202 0.27189308 0.27180982 0.2719923
 0.27247947 0.2722807  0.2720246  0.2718973  0.27167368 0.27140406
 0.27129042 0.2714337  0.27169284 0.27186406 0.27188814 0.2719197
 0.272103   0.2723447  0.27254397 0.27267876 0.27280316 0.27292085
 0.27285972 0.27261224 0.27228537 0.27200028 0.27191746 0.27226153
 0.2727363  0.27251238 0.272034   0.27175218 0.27171513 0.27181572
 0.27191335 0.27196965 0.2720762  0.27228305 0.27246115 0.27252302
 0.27253315 0.2725614  0.2726872  0.2728303  0.2728462  0.27269903
 0.27248704 0.27234876 0.2722852  0.2721385  0.2718535  0.27171862
 0.27180225 0.27183497 0.2718127  0.27170166 0.27152196 0.27141222
 0.27146548 0.271635   0.27181268 0.27190638 0.27190706 0.27192572
 0.27208537 0.27224797 0.2722827  0.27220738 0.2721378  0.27209875
 0.27196854 0.27167338 0.2713944  0.27119747 0.2712253  0.27139077
 0.27140117 0.27118155 0.2709412  0.27082542 0.27084178 0.2709014
 0.270964   0.2710786  0.2712972  0.27163312 0.27187386 0.2719197
 0.27185756 0.27183205 0.27191618 0.2720421  0.2720721  0.27191326
 0.27168334 0.27147412 0.27138245 0.27133143 0.2712095  0.27104625
 0.27087352 0.27087662 0.27098137 0.27096236 0.27083752 0.2708663
 0.27108103 0.27133837 0.27147013 0.27158013 0.27164614 0.27171835
 0.27180916 0.2718675  0.27187583 0.27185804 0.27179417 0.27161452
 0.271518   0.27127704 0.27094653 0.2707431  0.27080315 0.27099124
 0.2710224  0.27101466 0.27107278 0.27116066 0.27127632 0.27137217
 0.27142653 0.27150553 0.27170926 0.27200451 0.27219474 0.27225113
 0.27224496 0.27227837 0.2723979  0.27254435 0.27259955 0.27241486
 0.2723452  0.27223358 0.27209795 0.27191836 0.27172136 0.2716296
 0.27170122 0.2718024  0.27191707 0.2719796  0.27200827 0.27206722
 0.27220362 0.2723929  0.27256134 0.2727024  0.2728066  0.27289206
 0.27291003 0.27286154 0.27282444 0.27285036 0.27289918 0.27283773
 0.272731   0.27253407 0.272334   0.27225003 0.2722812  0.27261028
 0.27306193 0.2728426  0.2725565  0.2725808  0.27274007 0.2728584
 0.27291027 0.27301267 0.27321774 0.2734178  0.27342236 0.27324063
 0.27307326 0.27308196 0.2732596  0.27338806 0.27333495 0.2731725
 0.27304217 0.27300048 0.27295154 0.27275512 0.27240944 0.2724263
 0.2728847  0.273022   0.27299535 0.27297553 0.27293798 0.27293643
 0.2730302  0.273177   0.27330428 0.27337724 0.273401   0.2734587
 0.2735197  0.27347776 0.27332398 0.2731497  0.27302638 0.27292833
 0.27273268 0.27244756 0.2722343  0.27215827 0.272175   0.27233478
 0.27246848 0.27240154 0.27236173 0.27244404 0.27259183 0.27274224
 0.2728795  0.2730171  0.27317148 0.2732971  0.2732891  0.27316058
 0.2730584  0.2729986  0.2729479  0.27278456 0.27247903 0.27215573
 0.27195507 0.27190793 0.271885   0.27176434 0.2715566  0.27149224
 0.27157706 0.27172896 0.27184016 0.2718788  0.27193496 0.27207938
 0.27224708 0.2723087  0.2723092  0.27238017 0.2724771  0.27252716
 0.27249923 0.2723381  0.27213    0.27200034 0.2719628  0.27189413
 0.27173546 0.27142003 0.27118337 0.27122295 0.27135447 0.2714356
 0.27130297 0.27123538 0.27133656 0.27155596 0.2717279  0.27180436
 0.27188417 0.27209347 0.27238607 0.272587   0.27254695 0.27230373
 0.27206275 0.27195588 0.27194208 0.27187318 0.2716187  0.27128062
 0.27119443 0.27113912 0.27101108 0.27079415 0.2706912  0.27084935
 0.2711083  0.27142537 0.27166617 0.2717297  0.27179685 0.27197677
 0.27219287 0.2723058  0.27229792 0.2723001  0.27241588 0.27256808
 0.27257767 0.27239606 0.27216852 0.2720495  0.2720157  0.27194253
 0.27177262 0.27150574 0.27136707 0.27146715 0.27168688 0.27188498
 0.27197558 0.2719516  0.27203825 0.27226642 0.27249798 0.2726101
 0.27263504 0.2726811  0.27275825 0.272747   0.2725914  0.27239212
 0.27230427 0.2723485  0.2723777  0.27229527 0.27212247 0.27199748
 0.27199146 0.2720238  0.2719926  0.27185535 0.27174842 0.2720535
 0.27266213 0.27272898 0.27263373 0.2726652  0.27278376 0.27293906
 0.27303708 0.27302453 0.2729454  0.27288595 0.27283195 0.27273762
 0.27251244 0.27217072 0.27192187 0.2718461  0.27185732 0.2717721
 0.27150673 0.2712443  0.2711576  0.27120963 0.2713005  0.27156863
 0.271946   0.27198872 0.27204224 0.2722664  0.27248517 0.2725982
 0.27264065 0.27268764 0.27273974 0.272702   0.27248392 0.27220163
 0.27200615 0.2719134  0.27179676 0.27152938 0.27118862 0.27099064
 0.2710075  0.2710758  0.27099192 0.27075568 0.2705757  0.2707928
 0.27125454 0.2715126  0.27158627 0.2716509  0.27182415 0.27207154
 0.27221766 0.27215505 0.27198204 0.27186316 0.27180302 0.27170745
 0.27146652 0.27104604 0.27062595 0.2703786  0.2702689  0.27014604
 0.26992846 0.26976448 0.26985544 0.27017558 0.27045062 0.27054852
 0.27047288 0.27044278 0.27064878 0.27092758 0.27106646 0.27103415
 0.27093798 0.27087045 0.27081248 0.27065077 0.2703071  0.26992178
 0.26970434 0.2696137  0.26945475 0.26914138 0.26883194 0.26875967
 0.26890248 0.2690266  0.26898256 0.268903   0.26900354 0.2693568
 0.2696803  0.26987913 0.2699693  0.27000293 0.27012512 0.2702694
 0.27020878 0.26991072 0.2696171  0.26953778 0.2695445  0.2693511
 0.26889172 0.26842454 0.26825187 0.2683719  0.2685131  0.26847306
 0.26830128 0.2681975  0.26838928 0.2687921  0.26917523 0.26932457
 0.26928118 0.26947534 0.26993296 0.27027324 0.27035654 0.27029288
 0.27024186 0.27022845 0.2700942  0.26970387 0.26921254 0.26892358
 0.26888767 0.26884407 0.26860318 0.26831383 0.26827285 0.26861796
 0.26902005 0.2691144  0.26896974 0.26896352 0.26936847 0.2700869
 0.27067265 0.27078736 0.27071685 0.27080297 0.2710477  0.27109432
 0.27069107 0.27004525 0.26960227 0.26952142 0.26950282 0.26916382
 0.268548   0.26812825 0.26824957 0.26870453 0.26902273 0.2690183
 0.26900512 0.26939797 0.27009583 0.2706438  0.27078542 0.27105218
 0.27169243 0.2719338  0.27201605 0.27197033 0.2716851  0.27134272
 0.27110487 0.2708296  0.27025682 0.2694689  0.26893422 0.26904127
 0.2694855  0.26959226 0.2693105  0.26925355 0.26997375 0.27112532
 0.27183598 0.2717996  0.2715636  0.2717364  0.27233988 0.27300492
 0.273122   0.2724392  0.27194175 0.2719157  0.27159432 0.27038223
 0.2688709  0.26833096 0.26901862 0.2696849  0.26926637 0.26854768
 0.26927036 0.27149993 0.27347052 0.27372387 0.27304828 0.27326846
 0.2745697  0.27454394 0.27028468 0.26342475 0.26356047 0.27075264]
