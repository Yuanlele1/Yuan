Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14688811008.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 309.0726435184479
Epoch: 1, Steps: 84 | Train Loss: 1.2349386 Vali Loss: 1.2358445 Test Loss: 1.4577866
Validation loss decreased (inf --> 1.235844).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 315.3027534484863
Epoch: 2, Steps: 84 | Train Loss: 0.8852041 Vali Loss: 1.0838844 Test Loss: 1.2711059
Validation loss decreased (1.235844 --> 1.083884).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 321.63771772384644
Epoch: 3, Steps: 84 | Train Loss: 0.7813253 Vali Loss: 1.0133507 Test Loss: 1.1853893
Validation loss decreased (1.083884 --> 1.013351).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 277.4581699371338
Epoch: 4, Steps: 84 | Train Loss: 0.7148151 Vali Loss: 0.9596008 Test Loss: 1.1210743
Validation loss decreased (1.013351 --> 0.959601).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 343.54415917396545
Epoch: 5, Steps: 84 | Train Loss: 0.6612529 Vali Loss: 0.9123209 Test Loss: 1.0658907
Validation loss decreased (0.959601 --> 0.912321).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 319.9062752723694
Epoch: 6, Steps: 84 | Train Loss: 0.6160599 Vali Loss: 0.8707351 Test Loss: 1.0168538
Validation loss decreased (0.912321 --> 0.870735).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 328.8686625957489
Epoch: 7, Steps: 84 | Train Loss: 0.5770479 Vali Loss: 0.8358532 Test Loss: 0.9766434
Validation loss decreased (0.870735 --> 0.835853).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 310.4661831855774
Epoch: 8, Steps: 84 | Train Loss: 0.5432950 Vali Loss: 0.8035051 Test Loss: 0.9386218
Validation loss decreased (0.835853 --> 0.803505).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 295.9128246307373
Epoch: 9, Steps: 84 | Train Loss: 0.5134732 Vali Loss: 0.7759553 Test Loss: 0.9060497
Validation loss decreased (0.803505 --> 0.775955).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 317.036016702652
Epoch: 10, Steps: 84 | Train Loss: 0.4870061 Vali Loss: 0.7504182 Test Loss: 0.8757339
Validation loss decreased (0.775955 --> 0.750418).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 304.7651414871216
Epoch: 11, Steps: 84 | Train Loss: 0.4634337 Vali Loss: 0.7258206 Test Loss: 0.8476492
Validation loss decreased (0.750418 --> 0.725821).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 286.1267845630646
Epoch: 12, Steps: 84 | Train Loss: 0.4423372 Vali Loss: 0.7051504 Test Loss: 0.8233103
Validation loss decreased (0.725821 --> 0.705150).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 329.14467096328735
Epoch: 13, Steps: 84 | Train Loss: 0.4233895 Vali Loss: 0.6857107 Test Loss: 0.8009964
Validation loss decreased (0.705150 --> 0.685711).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 321.82129979133606
Epoch: 14, Steps: 84 | Train Loss: 0.4062314 Vali Loss: 0.6685792 Test Loss: 0.7807775
Validation loss decreased (0.685711 --> 0.668579).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 317.08180570602417
Epoch: 15, Steps: 84 | Train Loss: 0.3907135 Vali Loss: 0.6523905 Test Loss: 0.7614567
Validation loss decreased (0.668579 --> 0.652391).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 332.5005600452423
Epoch: 16, Steps: 84 | Train Loss: 0.3765887 Vali Loss: 0.6380541 Test Loss: 0.7446685
Validation loss decreased (0.652391 --> 0.638054).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 335.4735062122345
Epoch: 17, Steps: 84 | Train Loss: 0.3637395 Vali Loss: 0.6240754 Test Loss: 0.7279825
Validation loss decreased (0.638054 --> 0.624075).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 314.0689785480499
Epoch: 18, Steps: 84 | Train Loss: 0.3520268 Vali Loss: 0.6115649 Test Loss: 0.7139695
Validation loss decreased (0.624075 --> 0.611565).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 299.9034514427185
Epoch: 19, Steps: 84 | Train Loss: 0.3412603 Vali Loss: 0.6010361 Test Loss: 0.7011510
Validation loss decreased (0.611565 --> 0.601036).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 294.1331286430359
Epoch: 20, Steps: 84 | Train Loss: 0.3314229 Vali Loss: 0.5900193 Test Loss: 0.6882119
Validation loss decreased (0.601036 --> 0.590019).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 320.23754024505615
Epoch: 21, Steps: 84 | Train Loss: 0.3223797 Vali Loss: 0.5796272 Test Loss: 0.6765059
Validation loss decreased (0.590019 --> 0.579627).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 336.5555171966553
Epoch: 22, Steps: 84 | Train Loss: 0.3140434 Vali Loss: 0.5710337 Test Loss: 0.6662387
Validation loss decreased (0.579627 --> 0.571034).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 316.5055694580078
Epoch: 23, Steps: 84 | Train Loss: 0.3063655 Vali Loss: 0.5630375 Test Loss: 0.6563736
Validation loss decreased (0.571034 --> 0.563037).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 365.5186893939972
Epoch: 24, Steps: 84 | Train Loss: 0.2992446 Vali Loss: 0.5545743 Test Loss: 0.6471282
Validation loss decreased (0.563037 --> 0.554574).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 384.6921133995056
Epoch: 25, Steps: 84 | Train Loss: 0.2927144 Vali Loss: 0.5483317 Test Loss: 0.6390898
Validation loss decreased (0.554574 --> 0.548332).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 318.1937873363495
Epoch: 26, Steps: 84 | Train Loss: 0.2866017 Vali Loss: 0.5424019 Test Loss: 0.6315515
Validation loss decreased (0.548332 --> 0.542402).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 285.9963915348053
Epoch: 27, Steps: 84 | Train Loss: 0.2809111 Vali Loss: 0.5358431 Test Loss: 0.6247734
Validation loss decreased (0.542402 --> 0.535843).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 346.288535118103
Epoch: 28, Steps: 84 | Train Loss: 0.2756966 Vali Loss: 0.5298480 Test Loss: 0.6177874
Validation loss decreased (0.535843 --> 0.529848).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 318.45164704322815
Epoch: 29, Steps: 84 | Train Loss: 0.2707836 Vali Loss: 0.5246554 Test Loss: 0.6112517
Validation loss decreased (0.529848 --> 0.524655).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 265.6814155578613
Epoch: 30, Steps: 84 | Train Loss: 0.2662671 Vali Loss: 0.5198586 Test Loss: 0.6052890
Validation loss decreased (0.524655 --> 0.519859).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 303.537620306015
Epoch: 31, Steps: 84 | Train Loss: 0.2620274 Vali Loss: 0.5147102 Test Loss: 0.6000814
Validation loss decreased (0.519859 --> 0.514710).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 280.29047298431396
Epoch: 32, Steps: 84 | Train Loss: 0.2580738 Vali Loss: 0.5105966 Test Loss: 0.5948565
Validation loss decreased (0.514710 --> 0.510597).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 246.3637137413025
Epoch: 33, Steps: 84 | Train Loss: 0.2543514 Vali Loss: 0.5064741 Test Loss: 0.5901361
Validation loss decreased (0.510597 --> 0.506474).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 248.7596926689148
Epoch: 34, Steps: 84 | Train Loss: 0.2508704 Vali Loss: 0.5032326 Test Loss: 0.5860339
Validation loss decreased (0.506474 --> 0.503233).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 268.8741476535797
Epoch: 35, Steps: 84 | Train Loss: 0.2476812 Vali Loss: 0.4992186 Test Loss: 0.5817885
Validation loss decreased (0.503233 --> 0.499219).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 281.8556115627289
Epoch: 36, Steps: 84 | Train Loss: 0.2446095 Vali Loss: 0.4961508 Test Loss: 0.5778563
Validation loss decreased (0.499219 --> 0.496151).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 281.9856331348419
Epoch: 37, Steps: 84 | Train Loss: 0.2417469 Vali Loss: 0.4929366 Test Loss: 0.5739993
Validation loss decreased (0.496151 --> 0.492937).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 278.7863974571228
Epoch: 38, Steps: 84 | Train Loss: 0.2391159 Vali Loss: 0.4903928 Test Loss: 0.5709532
Validation loss decreased (0.492937 --> 0.490393).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 290.3359763622284
Epoch: 39, Steps: 84 | Train Loss: 0.2366120 Vali Loss: 0.4869471 Test Loss: 0.5674394
Validation loss decreased (0.490393 --> 0.486947).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 325.6892285346985
Epoch: 40, Steps: 84 | Train Loss: 0.2342585 Vali Loss: 0.4841338 Test Loss: 0.5644427
Validation loss decreased (0.486947 --> 0.484134).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 213.9664978981018
Epoch: 41, Steps: 84 | Train Loss: 0.2320076 Vali Loss: 0.4825373 Test Loss: 0.5617490
Validation loss decreased (0.484134 --> 0.482537).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 166.59072303771973
Epoch: 42, Steps: 84 | Train Loss: 0.2299078 Vali Loss: 0.4799818 Test Loss: 0.5590870
Validation loss decreased (0.482537 --> 0.479982).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 160.89648509025574
Epoch: 43, Steps: 84 | Train Loss: 0.2279244 Vali Loss: 0.4777058 Test Loss: 0.5565000
Validation loss decreased (0.479982 --> 0.477706).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 267.72528529167175
Epoch: 44, Steps: 84 | Train Loss: 0.2260846 Vali Loss: 0.4756874 Test Loss: 0.5541620
Validation loss decreased (0.477706 --> 0.475687).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 277.49834752082825
Epoch: 45, Steps: 84 | Train Loss: 0.2242737 Vali Loss: 0.4738647 Test Loss: 0.5519350
Validation loss decreased (0.475687 --> 0.473865).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 231.60384225845337
Epoch: 46, Steps: 84 | Train Loss: 0.2226631 Vali Loss: 0.4720328 Test Loss: 0.5497616
Validation loss decreased (0.473865 --> 0.472033).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 232.6413290500641
Epoch: 47, Steps: 84 | Train Loss: 0.2211353 Vali Loss: 0.4702073 Test Loss: 0.5478246
Validation loss decreased (0.472033 --> 0.470207).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 242.97265577316284
Epoch: 48, Steps: 84 | Train Loss: 0.2196470 Vali Loss: 0.4686756 Test Loss: 0.5459992
Validation loss decreased (0.470207 --> 0.468676).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 224.35213136672974
Epoch: 49, Steps: 84 | Train Loss: 0.2182020 Vali Loss: 0.4671867 Test Loss: 0.5440765
Validation loss decreased (0.468676 --> 0.467187).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 182.15532684326172
Epoch: 50, Steps: 84 | Train Loss: 0.2168747 Vali Loss: 0.4658083 Test Loss: 0.5423445
Validation loss decreased (0.467187 --> 0.465808).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 124.34056639671326
Epoch: 51, Steps: 84 | Train Loss: 0.2157086 Vali Loss: 0.4636492 Test Loss: 0.5406381
Validation loss decreased (0.465808 --> 0.463649).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 200.42727780342102
Epoch: 52, Steps: 84 | Train Loss: 0.2144979 Vali Loss: 0.4632453 Test Loss: 0.5392527
Validation loss decreased (0.463649 --> 0.463245).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 213.55476307868958
Epoch: 53, Steps: 84 | Train Loss: 0.2133857 Vali Loss: 0.4618831 Test Loss: 0.5378190
Validation loss decreased (0.463245 --> 0.461883).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 213.33818745613098
Epoch: 54, Steps: 84 | Train Loss: 0.2123313 Vali Loss: 0.4604185 Test Loss: 0.5364525
Validation loss decreased (0.461883 --> 0.460419).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 225.45426321029663
Epoch: 55, Steps: 84 | Train Loss: 0.2112992 Vali Loss: 0.4598994 Test Loss: 0.5352590
Validation loss decreased (0.460419 --> 0.459899).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 150.40073037147522
Epoch: 56, Steps: 84 | Train Loss: 0.2103765 Vali Loss: 0.4587792 Test Loss: 0.5340058
Validation loss decreased (0.459899 --> 0.458779).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 139.0123152732849
Epoch: 57, Steps: 84 | Train Loss: 0.2094926 Vali Loss: 0.4571545 Test Loss: 0.5328264
Validation loss decreased (0.458779 --> 0.457155).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 193.21222758293152
Epoch: 58, Steps: 84 | Train Loss: 0.2086294 Vali Loss: 0.4567992 Test Loss: 0.5317900
Validation loss decreased (0.457155 --> 0.456799).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 165.79407739639282
Epoch: 59, Steps: 84 | Train Loss: 0.2078509 Vali Loss: 0.4558380 Test Loss: 0.5307122
Validation loss decreased (0.456799 --> 0.455838).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 235.35494017601013
Epoch: 60, Steps: 84 | Train Loss: 0.2070661 Vali Loss: 0.4549850 Test Loss: 0.5297118
Validation loss decreased (0.455838 --> 0.454985).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 229.8192663192749
Epoch: 61, Steps: 84 | Train Loss: 0.2063024 Vali Loss: 0.4540556 Test Loss: 0.5288448
Validation loss decreased (0.454985 --> 0.454056).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 223.10973978042603
Epoch: 62, Steps: 84 | Train Loss: 0.2056406 Vali Loss: 0.4534802 Test Loss: 0.5279285
Validation loss decreased (0.454056 --> 0.453480).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 226.37753534317017
Epoch: 63, Steps: 84 | Train Loss: 0.2050165 Vali Loss: 0.4522672 Test Loss: 0.5271318
Validation loss decreased (0.453480 --> 0.452267).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 236.27066731452942
Epoch: 64, Steps: 84 | Train Loss: 0.2043500 Vali Loss: 0.4518962 Test Loss: 0.5263278
Validation loss decreased (0.452267 --> 0.451896).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 228.8184404373169
Epoch: 65, Steps: 84 | Train Loss: 0.2037873 Vali Loss: 0.4513336 Test Loss: 0.5255775
Validation loss decreased (0.451896 --> 0.451334).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 231.2341070175171
Epoch: 66, Steps: 84 | Train Loss: 0.2032084 Vali Loss: 0.4507869 Test Loss: 0.5248861
Validation loss decreased (0.451334 --> 0.450787).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 245.19804072380066
Epoch: 67, Steps: 84 | Train Loss: 0.2027061 Vali Loss: 0.4501113 Test Loss: 0.5241654
Validation loss decreased (0.450787 --> 0.450111).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 232.44666957855225
Epoch: 68, Steps: 84 | Train Loss: 0.2021543 Vali Loss: 0.4498202 Test Loss: 0.5235190
Validation loss decreased (0.450111 --> 0.449820).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 241.63974595069885
Epoch: 69, Steps: 84 | Train Loss: 0.2016850 Vali Loss: 0.4491026 Test Loss: 0.5229414
Validation loss decreased (0.449820 --> 0.449103).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 243.54417896270752
Epoch: 70, Steps: 84 | Train Loss: 0.2012220 Vali Loss: 0.4481760 Test Loss: 0.5223322
Validation loss decreased (0.449103 --> 0.448176).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 223.72670722007751
Epoch: 71, Steps: 84 | Train Loss: 0.2008009 Vali Loss: 0.4480394 Test Loss: 0.5218206
Validation loss decreased (0.448176 --> 0.448039).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 200.39107084274292
Epoch: 72, Steps: 84 | Train Loss: 0.2004378 Vali Loss: 0.4474688 Test Loss: 0.5212778
Validation loss decreased (0.448039 --> 0.447469).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 208.18078923225403
Epoch: 73, Steps: 84 | Train Loss: 0.2000255 Vali Loss: 0.4472843 Test Loss: 0.5207840
Validation loss decreased (0.447469 --> 0.447284).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 201.32827234268188
Epoch: 74, Steps: 84 | Train Loss: 0.1996400 Vali Loss: 0.4471796 Test Loss: 0.5203100
Validation loss decreased (0.447284 --> 0.447180).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 205.4655683040619
Epoch: 75, Steps: 84 | Train Loss: 0.1992980 Vali Loss: 0.4465840 Test Loss: 0.5198435
Validation loss decreased (0.447180 --> 0.446584).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 204.35383415222168
Epoch: 76, Steps: 84 | Train Loss: 0.1989771 Vali Loss: 0.4457344 Test Loss: 0.5194153
Validation loss decreased (0.446584 --> 0.445734).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 204.70238637924194
Epoch: 77, Steps: 84 | Train Loss: 0.1986445 Vali Loss: 0.4456567 Test Loss: 0.5189890
Validation loss decreased (0.445734 --> 0.445657).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 220.1785023212433
Epoch: 78, Steps: 84 | Train Loss: 0.1983395 Vali Loss: 0.4454799 Test Loss: 0.5186301
Validation loss decreased (0.445657 --> 0.445480).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 218.40069675445557
Epoch: 79, Steps: 84 | Train Loss: 0.1980475 Vali Loss: 0.4449635 Test Loss: 0.5182722
Validation loss decreased (0.445480 --> 0.444963).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 210.15644907951355
Epoch: 80, Steps: 84 | Train Loss: 0.1977985 Vali Loss: 0.4446306 Test Loss: 0.5179220
Validation loss decreased (0.444963 --> 0.444631).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 202.01941919326782
Epoch: 81, Steps: 84 | Train Loss: 0.1975405 Vali Loss: 0.4447811 Test Loss: 0.5175729
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 187.85929441452026
Epoch: 82, Steps: 84 | Train Loss: 0.1972815 Vali Loss: 0.4436143 Test Loss: 0.5172780
Validation loss decreased (0.444631 --> 0.443614).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 186.0609836578369
Epoch: 83, Steps: 84 | Train Loss: 0.1970487 Vali Loss: 0.4432611 Test Loss: 0.5169806
Validation loss decreased (0.443614 --> 0.443261).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 179.43505716323853
Epoch: 84, Steps: 84 | Train Loss: 0.1968324 Vali Loss: 0.4434335 Test Loss: 0.5166823
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 154.83394694328308
Epoch: 85, Steps: 84 | Train Loss: 0.1966319 Vali Loss: 0.4430568 Test Loss: 0.5163999
Validation loss decreased (0.443261 --> 0.443057).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 143.21108102798462
Epoch: 86, Steps: 84 | Train Loss: 0.1963944 Vali Loss: 0.4427463 Test Loss: 0.5161504
Validation loss decreased (0.443057 --> 0.442746).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 139.3710422515869
Epoch: 87, Steps: 84 | Train Loss: 0.1962189 Vali Loss: 0.4429467 Test Loss: 0.5159208
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 123.48364353179932
Epoch: 88, Steps: 84 | Train Loss: 0.1960086 Vali Loss: 0.4428726 Test Loss: 0.5156822
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 128.7272870540619
Epoch: 89, Steps: 84 | Train Loss: 0.1958788 Vali Loss: 0.4423428 Test Loss: 0.5154693
Validation loss decreased (0.442746 --> 0.442343).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 106.51079201698303
Epoch: 90, Steps: 84 | Train Loss: 0.1956978 Vali Loss: 0.4425608 Test Loss: 0.5152583
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 104.75166440010071
Epoch: 91, Steps: 84 | Train Loss: 0.1955585 Vali Loss: 0.4427231 Test Loss: 0.5150691
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 113.55110216140747
Epoch: 92, Steps: 84 | Train Loss: 0.1954350 Vali Loss: 0.4424730 Test Loss: 0.5148817
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 110.49300360679626
Epoch: 93, Steps: 84 | Train Loss: 0.1952449 Vali Loss: 0.4422041 Test Loss: 0.5146908
Validation loss decreased (0.442343 --> 0.442204).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 115.92662692070007
Epoch: 94, Steps: 84 | Train Loss: 0.1951353 Vali Loss: 0.4418503 Test Loss: 0.5145204
Validation loss decreased (0.442204 --> 0.441850).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 138.44915866851807
Epoch: 95, Steps: 84 | Train Loss: 0.1950173 Vali Loss: 0.4415052 Test Loss: 0.5143567
Validation loss decreased (0.441850 --> 0.441505).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 128.8531322479248
Epoch: 96, Steps: 84 | Train Loss: 0.1948949 Vali Loss: 0.4416890 Test Loss: 0.5141986
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 138.4592010974884
Epoch: 97, Steps: 84 | Train Loss: 0.1947646 Vali Loss: 0.4413897 Test Loss: 0.5140517
Validation loss decreased (0.441505 --> 0.441390).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 134.917626619339
Epoch: 98, Steps: 84 | Train Loss: 0.1946781 Vali Loss: 0.4417279 Test Loss: 0.5139169
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 133.43476271629333
Epoch: 99, Steps: 84 | Train Loss: 0.1945662 Vali Loss: 0.4411966 Test Loss: 0.5137843
Validation loss decreased (0.441390 --> 0.441197).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 118.3325891494751
Epoch: 100, Steps: 84 | Train Loss: 0.1944763 Vali Loss: 0.4408261 Test Loss: 0.5136590
Validation loss decreased (0.441197 --> 0.440826).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14688811008.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 188.99554538726807
Epoch: 1, Steps: 84 | Train Loss: 0.2994408 Vali Loss: 0.3986921 Test Loss: 0.4642876
Validation loss decreased (inf --> 0.398692).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 203.2371003627777
Epoch: 2, Steps: 84 | Train Loss: 0.2771744 Vali Loss: 0.3876521 Test Loss: 0.4524530
Validation loss decreased (0.398692 --> 0.387652).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 205.06655883789062
Epoch: 3, Steps: 84 | Train Loss: 0.2718904 Vali Loss: 0.3861736 Test Loss: 0.4506837
Validation loss decreased (0.387652 --> 0.386174).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 226.26569652557373
Epoch: 4, Steps: 84 | Train Loss: 0.2709512 Vali Loss: 0.3860314 Test Loss: 0.4509085
Validation loss decreased (0.386174 --> 0.386031).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 223.78470873832703
Epoch: 5, Steps: 84 | Train Loss: 0.2707952 Vali Loss: 0.3865156 Test Loss: 0.4507572
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 225.41218328475952
Epoch: 6, Steps: 84 | Train Loss: 0.2707677 Vali Loss: 0.3855627 Test Loss: 0.4497533
Validation loss decreased (0.386031 --> 0.385563).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 225.5033209323883
Epoch: 7, Steps: 84 | Train Loss: 0.2706825 Vali Loss: 0.3863906 Test Loss: 0.4509379
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 201.20216536521912
Epoch: 8, Steps: 84 | Train Loss: 0.2705863 Vali Loss: 0.3864319 Test Loss: 0.4502732
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 216.52487206459045
Epoch: 9, Steps: 84 | Train Loss: 0.2706760 Vali Loss: 0.3858525 Test Loss: 0.4512026
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 214.64043641090393
Epoch: 10, Steps: 84 | Train Loss: 0.2705701 Vali Loss: 0.3865428 Test Loss: 0.4505787
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 221.53892731666565
Epoch: 11, Steps: 84 | Train Loss: 0.2705278 Vali Loss: 0.3859585 Test Loss: 0.4509033
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 217.56871461868286
Epoch: 12, Steps: 84 | Train Loss: 0.2705729 Vali Loss: 0.3861977 Test Loss: 0.4505258
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 212.62289309501648
Epoch: 13, Steps: 84 | Train Loss: 0.2704628 Vali Loss: 0.3858615 Test Loss: 0.4500828
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 187.16929054260254
Epoch: 14, Steps: 84 | Train Loss: 0.2704720 Vali Loss: 0.3861009 Test Loss: 0.4502431
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 195.0615439414978
Epoch: 15, Steps: 84 | Train Loss: 0.2704801 Vali Loss: 0.3856924 Test Loss: 0.4497590
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 210.8189902305603
Epoch: 16, Steps: 84 | Train Loss: 0.2704889 Vali Loss: 0.3857322 Test Loss: 0.4498630
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4492195248603821, mae:0.3006259799003601, rse:0.5480625033378601, corr:[0.25930285 0.26517683 0.26788017 0.26575097 0.26724282 0.2685869
 0.26789042 0.26940256 0.27016506 0.26892388 0.26892397 0.26893386
 0.26812568 0.2680521  0.26764974 0.26714113 0.26799127 0.26850602
 0.2682277  0.26861855 0.26860365 0.2678015  0.26779297 0.2681544
 0.26889813 0.26906434 0.26906908 0.26864925 0.26886907 0.2695944
 0.2695373  0.26938036 0.26934254 0.26869965 0.26845625 0.2688786
 0.26860735 0.26817307 0.26830617 0.26817974 0.26835528 0.26920897
 0.26932225 0.2688894  0.26895463 0.2688514  0.26843077 0.2687554
 0.2693014  0.2689957  0.26907912 0.26916268 0.26854312 0.26826897
 0.26851732 0.26829618 0.2679623  0.26800823 0.26783556 0.26764685
 0.26774254 0.26737487 0.26701424 0.267343   0.26767597 0.26785183
 0.2682499  0.26837188 0.2680308  0.26776367 0.26740003 0.26724085
 0.26760975 0.26793242 0.26767913 0.26747504 0.2674003  0.2671912
 0.267455   0.26804793 0.26811075 0.2677989  0.26759535 0.26762632
 0.26790982 0.26804575 0.2676734  0.2673113  0.26729092 0.26705122
 0.26697755 0.26732323 0.26761442 0.26762635 0.267602   0.26739472
 0.26672342 0.26657563 0.26666716 0.26648718 0.26669395 0.26722288
 0.2674896  0.2676051  0.26750067 0.2671052  0.26704478 0.26759815
 0.26793247 0.26791006 0.2680929  0.26831004 0.26830032 0.26804256
 0.26772696 0.26747507 0.2677581  0.26789427 0.26770192 0.26766407
 0.2673089  0.266689   0.26671517 0.26715615 0.26700076 0.2667547
 0.2671698  0.2674475  0.26735988 0.26747954 0.2676683  0.2678355
 0.26806483 0.26822704 0.26843554 0.2689198  0.2693065  0.26884583
 0.2683914  0.2678525  0.2677796  0.26841325 0.268882   0.2685608
 0.2681459  0.26789916 0.26778147 0.2676437  0.26749152 0.2675002
 0.26787424 0.26842123 0.26870415 0.26867115 0.26833358 0.2679701
 0.26799166 0.26822758 0.2682069  0.26826188 0.26859733 0.26855555
 0.26836225 0.26859036 0.26883164 0.2688923  0.26913545 0.26936585
 0.26988629 0.2697372  0.26963955 0.26915404 0.2687662  0.26914805
 0.2695596  0.26966336 0.2699703  0.27014622 0.26978865 0.2696482
 0.2699018  0.26991734 0.2699349  0.2701436  0.27016538 0.2702347
 0.27044985 0.27027833 0.2697402  0.26951426 0.26959008 0.26964882
 0.2700926  0.2701121  0.27011868 0.27000663 0.26985294 0.27001384
 0.27036104 0.2705834  0.27076608 0.27097175 0.2708199  0.2704317
 0.2703911  0.27057955 0.2706373  0.27059737 0.27037367 0.26992464
 0.2696836  0.26971373 0.26968634 0.26964602 0.26933363 0.26868626
 0.26836324 0.26842922 0.26875892 0.2692734  0.26959658 0.26923212
 0.26886395 0.26914254 0.26928675 0.26903456 0.26908356 0.26921305
 0.26895565 0.26865426 0.26850867 0.26849174 0.26885512 0.26913643
 0.26888257 0.26890433 0.2694223  0.26920357 0.26850525 0.26840216
 0.26844665 0.2683462  0.26845568 0.26851177 0.26865146 0.26933244
 0.2697285  0.26913464 0.2685927  0.268773   0.26899928 0.26912054
 0.26905078 0.26861387 0.26869315 0.26954088 0.26985115 0.26939404
 0.26926863 0.26933125 0.26908293 0.2688745  0.26880595 0.26862353
 0.26847267 0.268742   0.26894784 0.2690398  0.26900712 0.2688975
 0.26899743 0.269207   0.2689731  0.26840696 0.26778576 0.26753566
 0.26797387 0.26855007 0.26876536 0.26917934 0.2698431  0.26960805
 0.26883084 0.26861313 0.2688386  0.2686879  0.26845965 0.26849976
 0.26825285 0.26800248 0.2682465  0.2689119  0.26921687 0.26898536
 0.2688927  0.26895383 0.2687618  0.26872778 0.26930475 0.2699611
 0.27002952 0.2697913  0.2697353  0.26994145 0.2704885  0.27080822
 0.2704255  0.26962122 0.26963818 0.26995492 0.26952365 0.26903865
 0.26937115 0.2697415  0.26990864 0.27001244 0.2698552  0.26957455
 0.2695742  0.26971292 0.26983997 0.27003765 0.27020472 0.2704657
 0.270822   0.27085453 0.27062958 0.27065936 0.2708946  0.27087528
 0.270742   0.27046213 0.27019697 0.27033743 0.27051455 0.27048925
 0.27128002 0.27166966 0.27167222 0.2714012  0.2710888  0.27061605
 0.27000636 0.26990178 0.2704465  0.27106112 0.27133417 0.2713999
 0.27153885 0.27169695 0.27158192 0.2713496  0.27129278 0.27106288
 0.27071828 0.27075478 0.27073178 0.2703437  0.2700961  0.27039567
 0.27108142 0.2711466  0.2712355  0.271283   0.27115497 0.270981
 0.27061644 0.27025452 0.27037892 0.2707578  0.2709231  0.27114934
 0.27141756 0.27124515 0.27092102 0.27088588 0.27084416 0.27065882
 0.2705765  0.27067614 0.270923   0.27099773 0.2704505  0.26977137
 0.26958144 0.26956776 0.27       0.27057654 0.27053067 0.2703256
 0.27080327 0.27120084 0.2709381  0.2708713  0.27103573 0.2707272
 0.27033648 0.27014402 0.26992437 0.26997185 0.27016684 0.26989314
 0.2697798  0.27036756 0.27053738 0.27003366 0.26979598 0.2697776
 0.26938832 0.2691887  0.2693989  0.2695976  0.26991856 0.27030995
 0.27019075 0.2697462  0.26967186 0.2699008  0.27005634 0.2701695
 0.27008352 0.2696833  0.2694419  0.26950976 0.2696153  0.26951748
 0.2692584  0.2691659  0.26947302 0.2695666  0.26920828 0.26914012
 0.26909623 0.26898625 0.26909614 0.26932606 0.2693843  0.26979288
 0.27038655 0.2702065  0.26944277 0.2689363  0.26888287 0.26931065
 0.26985976 0.2697896  0.26939973 0.26953968 0.26980993 0.26951727
 0.2692892  0.269303   0.26934785 0.26935098 0.26938817 0.26939818
 0.26929092 0.26927164 0.26926193 0.26957378 0.26992175 0.2698991
 0.27004665 0.27021584 0.2696651  0.26919663 0.26971617 0.270207
 0.2699008  0.26956487 0.2696225  0.2698551  0.27028334 0.27058813
 0.27032682 0.26988393 0.26979956 0.2698119  0.26977423 0.26999155
 0.2706026  0.2711693  0.2713398  0.2708307  0.27021515 0.27013385
 0.2701744  0.27001283 0.27005365 0.27020377 0.27033362 0.27076185
 0.27109835 0.27081236 0.27049154 0.2707174  0.2710517  0.27119955
 0.271276   0.2710647  0.2705235  0.27010718 0.26987875 0.26983696
 0.27068838 0.27128842 0.2716429  0.27165943 0.27124062 0.2704709
 0.26982075 0.26985827 0.27030987 0.27043274 0.2702551  0.27058533
 0.27121302 0.2710772  0.27052525 0.27045488 0.2704524  0.27039203
 0.27092287 0.27128348 0.27068868 0.27009547 0.2697413  0.26930597
 0.26971027 0.2701396  0.270268   0.2702713  0.27030832 0.2702028
 0.27020907 0.2705288  0.27048033 0.27013806 0.270374   0.2707324
 0.27022463 0.2694557  0.26909998 0.268846   0.26898962 0.2695428
 0.2696932  0.2698493  0.27048454 0.27039906 0.26930997 0.26880705
 0.26901722 0.2689252  0.26911333 0.2696346  0.26989037 0.2699386
 0.26983684 0.26957816 0.26960665 0.26988938 0.269962   0.27000013
 0.26991087 0.26919013 0.2685258  0.26864243 0.26881272 0.2687699
 0.2692144  0.26993597 0.2700313  0.26932824 0.26834044 0.2680079
 0.26833943 0.26841775 0.2681378  0.26817    0.2684083  0.2684657
 0.26861432 0.26862133 0.26831287 0.26841456 0.2689823  0.26901567
 0.2681626  0.26714957 0.26686925 0.26740357 0.26777044 0.2674383
 0.26726642 0.26772436 0.2680335  0.2681305  0.2683409  0.2679822
 0.2671441  0.26734486 0.26778513 0.26754645 0.26751977 0.26763436
 0.2672485  0.26724854 0.26759878 0.26709822 0.26670244 0.26739213
 0.26762348 0.26717016 0.26748958 0.26784095 0.26724154 0.2668579
 0.26703233 0.26708904 0.2675052  0.26799268 0.26769412 0.26730698
 0.26735088 0.2673364  0.2670489  0.26708016 0.26696998 0.2668362
 0.26717833 0.26715147 0.2666187  0.26644298 0.26645312 0.26644146
 0.26676682 0.26695988 0.26701012 0.26772475 0.268471   0.2684177
 0.26804596 0.26755238 0.2670224  0.26715776 0.26756427 0.2675662
 0.26794237 0.26858625 0.26854548 0.2681015  0.26795638 0.26778594
 0.26781657 0.26832235 0.268423   0.26822764 0.26846087 0.2682785
 0.26763722 0.2675292  0.26722845 0.2668121  0.26788437 0.26906672
 0.268574   0.26816663 0.26860517 0.26862645 0.26847664 0.26887986
 0.2696571  0.2697643  0.27009445 0.26986295 0.2693383  0.26944825
 0.26933706 0.26901105 0.26939812 0.26936    0.26860857 0.26869032
 0.26898164 0.26868647 0.26901495 0.26944742 0.26924583 0.26960343
 0.27002722 0.2697086  0.26992005 0.27021262 0.26938763 0.2690538
 0.2698994  0.26960552 0.2691512  0.2692878  0.2688265  0.26840228
 0.2687965  0.26877844 0.2688527  0.26971877 0.26970378 0.26956332
 0.27049538 0.27033556 0.27009365 0.27145413 0.27126855 0.27055964
 0.27157733 0.26960346 0.2676332  0.26884532 0.263984   0.27044153]
