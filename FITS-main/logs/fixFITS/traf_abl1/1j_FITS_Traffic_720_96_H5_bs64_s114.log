Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j96_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j96_H5_FITS_custom_ftM_sl720_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=165, out_features=187, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3404417280.0
params:  31042.0
Trainable parameters:  31042
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 91.17068266868591
Epoch: 1, Steps: 89 | Train Loss: 0.5617995 Vali Loss: 0.4115498 Test Loss: 0.4829199
Validation loss decreased (inf --> 0.411550).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 89.58634757995605
Epoch: 2, Steps: 89 | Train Loss: 0.2637743 Vali Loss: 0.3426934 Test Loss: 0.4105693
Validation loss decreased (0.411550 --> 0.342693).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 92.54379320144653
Epoch: 3, Steps: 89 | Train Loss: 0.2423541 Vali Loss: 0.3380122 Test Loss: 0.4051965
Validation loss decreased (0.342693 --> 0.338012).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 93.07241082191467
Epoch: 4, Steps: 89 | Train Loss: 0.2404676 Vali Loss: 0.3367392 Test Loss: 0.4033868
Validation loss decreased (0.338012 --> 0.336739).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 92.30961728096008
Epoch: 5, Steps: 89 | Train Loss: 0.2397790 Vali Loss: 0.3345133 Test Loss: 0.4028394
Validation loss decreased (0.336739 --> 0.334513).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 97.41602635383606
Epoch: 6, Steps: 89 | Train Loss: 0.2392495 Vali Loss: 0.3348810 Test Loss: 0.4020836
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 94.55402207374573
Epoch: 7, Steps: 89 | Train Loss: 0.2389789 Vali Loss: 0.3346834 Test Loss: 0.4016735
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 95.41886138916016
Epoch: 8, Steps: 89 | Train Loss: 0.2386796 Vali Loss: 0.3331281 Test Loss: 0.4013870
Validation loss decreased (0.334513 --> 0.333128).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 95.25502181053162
Epoch: 9, Steps: 89 | Train Loss: 0.2385237 Vali Loss: 0.3341535 Test Loss: 0.4010681
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 105.41738438606262
Epoch: 10, Steps: 89 | Train Loss: 0.2384955 Vali Loss: 0.3338808 Test Loss: 0.4006556
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 113.36938214302063
Epoch: 11, Steps: 89 | Train Loss: 0.2384742 Vali Loss: 0.3335991 Test Loss: 0.4008199
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 121.93274140357971
Epoch: 12, Steps: 89 | Train Loss: 0.2384092 Vali Loss: 0.3344656 Test Loss: 0.4008410
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 112.52396011352539
Epoch: 13, Steps: 89 | Train Loss: 0.2382293 Vali Loss: 0.3343133 Test Loss: 0.4006617
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 120.68051481246948
Epoch: 14, Steps: 89 | Train Loss: 0.2382070 Vali Loss: 0.3339889 Test Loss: 0.4007000
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 115.27846574783325
Epoch: 15, Steps: 89 | Train Loss: 0.2382187 Vali Loss: 0.3330537 Test Loss: 0.4006621
Validation loss decreased (0.333128 --> 0.333054).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 118.13108777999878
Epoch: 16, Steps: 89 | Train Loss: 0.2379399 Vali Loss: 0.3333142 Test Loss: 0.4003997
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 121.09366989135742
Epoch: 17, Steps: 89 | Train Loss: 0.2380400 Vali Loss: 0.3330967 Test Loss: 0.4002605
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 113.85120105743408
Epoch: 18, Steps: 89 | Train Loss: 0.2380010 Vali Loss: 0.3346602 Test Loss: 0.4004844
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 119.19040274620056
Epoch: 19, Steps: 89 | Train Loss: 0.2379250 Vali Loss: 0.3334866 Test Loss: 0.4002941
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 119.5602638721466
Epoch: 20, Steps: 89 | Train Loss: 0.2378546 Vali Loss: 0.3326698 Test Loss: 0.4003040
Validation loss decreased (0.333054 --> 0.332670).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 119.44551634788513
Epoch: 21, Steps: 89 | Train Loss: 0.2378997 Vali Loss: 0.3338505 Test Loss: 0.4001629
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 120.49672389030457
Epoch: 22, Steps: 89 | Train Loss: 0.2377547 Vali Loss: 0.3336416 Test Loss: 0.3999912
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 112.44423937797546
Epoch: 23, Steps: 89 | Train Loss: 0.2378627 Vali Loss: 0.3336796 Test Loss: 0.4004305
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 117.18375015258789
Epoch: 24, Steps: 89 | Train Loss: 0.2377633 Vali Loss: 0.3337590 Test Loss: 0.4003442
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 120.65098214149475
Epoch: 25, Steps: 89 | Train Loss: 0.2377423 Vali Loss: 0.3343526 Test Loss: 0.3999927
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 117.39995455741882
Epoch: 26, Steps: 89 | Train Loss: 0.2377549 Vali Loss: 0.3330817 Test Loss: 0.3999478
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 113.22972083091736
Epoch: 27, Steps: 89 | Train Loss: 0.2375466 Vali Loss: 0.3327623 Test Loss: 0.3998670
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 116.44705271720886
Epoch: 28, Steps: 89 | Train Loss: 0.2377364 Vali Loss: 0.3331950 Test Loss: 0.3998876
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 118.35293340682983
Epoch: 29, Steps: 89 | Train Loss: 0.2376338 Vali Loss: 0.3331738 Test Loss: 0.3998908
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 119.52000212669373
Epoch: 30, Steps: 89 | Train Loss: 0.2376500 Vali Loss: 0.3326988 Test Loss: 0.3996295
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j96_H5_FITS_custom_ftM_sl720_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.39755839109420776, mae:0.2845495343208313, rse:0.5221002697944641, corr:[0.2897256  0.2949473  0.29471618 0.29602227 0.2970767  0.29689783
 0.2964536  0.2967091  0.29742196 0.29762274 0.29688668 0.2957769
 0.29513276 0.29510784 0.29525894 0.29519528 0.29506955 0.2952733
 0.2957731  0.29615906 0.29614004 0.2958545  0.29577234 0.29651076
 0.29757607 0.29724398 0.29652452 0.29631412 0.29648682 0.29655915
 0.296291   0.2958994  0.29579246 0.29602104 0.29619497 0.2959856
 0.29557064 0.2953466  0.29555407 0.29599363 0.2962596  0.296145
 0.29582193 0.29563972 0.29570937 0.29587322 0.29602775 0.29631588
 0.2965525  0.29626712 0.29576498 0.2953967  0.29540747 0.2957884
 0.2962202  0.29634485 0.29613426 0.29588333 0.2958606  0.29604098
 0.29618114 0.29596487 0.29547983 0.29513463 0.29522076 0.29556334
 0.2957098  0.29552385 0.29534253 0.2955119  0.29589164 0.296113
 0.2958684  0.29544666 0.29519475 0.2950344  0.29477137 0.29441437
 0.29424593 0.29450095 0.29498684 0.29522467 0.2949937  0.2945766
 0.29443276 0.2946054  0.2947753  0.29469052 0.2945062  0.29457527
 0.29486403 0.29474583 0.29369754 0.29238364 0.2927463  0.29565936]
