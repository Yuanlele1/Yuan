Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j96_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j96_H5_FITS_custom_ftM_sl90_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 12095
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=30, out_features=62, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  205224960.0
params:  1922.0
Trainable parameters:  1922
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 48.02297806739807
Epoch: 1, Steps: 94 | Train Loss: 1.0004423 Vali Loss: 0.9258968 Test Loss: 1.0710058
Validation loss decreased (inf --> 0.925897).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 44.117151498794556
Epoch: 2, Steps: 94 | Train Loss: 0.6217734 Vali Loss: 0.7238006 Test Loss: 0.8394611
Validation loss decreased (0.925897 --> 0.723801).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 42.052239418029785
Epoch: 3, Steps: 94 | Train Loss: 0.5202837 Vali Loss: 0.6545697 Test Loss: 0.7636282
Validation loss decreased (0.723801 --> 0.654570).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 45.43688130378723
Epoch: 4, Steps: 94 | Train Loss: 0.4834443 Vali Loss: 0.6260010 Test Loss: 0.7317745
Validation loss decreased (0.654570 --> 0.626001).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 45.59464383125305
Epoch: 5, Steps: 94 | Train Loss: 0.4665628 Vali Loss: 0.6117786 Test Loss: 0.7154824
Validation loss decreased (0.626001 --> 0.611779).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 42.83262324333191
Epoch: 6, Steps: 94 | Train Loss: 0.4575236 Vali Loss: 0.6022845 Test Loss: 0.7059965
Validation loss decreased (0.611779 --> 0.602284).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 44.18721055984497
Epoch: 7, Steps: 94 | Train Loss: 0.4519389 Vali Loss: 0.5952938 Test Loss: 0.6997886
Validation loss decreased (0.602284 --> 0.595294).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 43.953556299209595
Epoch: 8, Steps: 94 | Train Loss: 0.4482618 Vali Loss: 0.5905921 Test Loss: 0.6957098
Validation loss decreased (0.595294 --> 0.590592).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 43.18509554862976
Epoch: 9, Steps: 94 | Train Loss: 0.4455334 Vali Loss: 0.5890976 Test Loss: 0.6927718
Validation loss decreased (0.590592 --> 0.589098).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 46.57516050338745
Epoch: 10, Steps: 94 | Train Loss: 0.4436979 Vali Loss: 0.5866921 Test Loss: 0.6905167
Validation loss decreased (0.589098 --> 0.586692).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 41.66445875167847
Epoch: 11, Steps: 94 | Train Loss: 0.4423597 Vali Loss: 0.5835446 Test Loss: 0.6889465
Validation loss decreased (0.586692 --> 0.583545).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 45.48392391204834
Epoch: 12, Steps: 94 | Train Loss: 0.4411234 Vali Loss: 0.5834901 Test Loss: 0.6874835
Validation loss decreased (0.583545 --> 0.583490).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 47.34425663948059
Epoch: 13, Steps: 94 | Train Loss: 0.4404071 Vali Loss: 0.5836787 Test Loss: 0.6865697
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 44.103413343429565
Epoch: 14, Steps: 94 | Train Loss: 0.4395864 Vali Loss: 0.5802858 Test Loss: 0.6856977
Validation loss decreased (0.583490 --> 0.580286).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 45.89817023277283
Epoch: 15, Steps: 94 | Train Loss: 0.4389766 Vali Loss: 0.5794480 Test Loss: 0.6849204
Validation loss decreased (0.580286 --> 0.579448).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 47.148399353027344
Epoch: 16, Steps: 94 | Train Loss: 0.4384828 Vali Loss: 0.5800419 Test Loss: 0.6843563
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 47.5500967502594
Epoch: 17, Steps: 94 | Train Loss: 0.4381835 Vali Loss: 0.5796220 Test Loss: 0.6839322
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 47.45277285575867
Epoch: 18, Steps: 94 | Train Loss: 0.4376542 Vali Loss: 0.5789475 Test Loss: 0.6834340
Validation loss decreased (0.579448 --> 0.578947).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 43.346965074539185
Epoch: 19, Steps: 94 | Train Loss: 0.4373829 Vali Loss: 0.5778199 Test Loss: 0.6830593
Validation loss decreased (0.578947 --> 0.577820).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 48.04905343055725
Epoch: 20, Steps: 94 | Train Loss: 0.4372319 Vali Loss: 0.5798826 Test Loss: 0.6828105
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 46.23286533355713
Epoch: 21, Steps: 94 | Train Loss: 0.4369564 Vali Loss: 0.5802324 Test Loss: 0.6825302
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 44.639750242233276
Epoch: 22, Steps: 94 | Train Loss: 0.4368128 Vali Loss: 0.5810400 Test Loss: 0.6822910
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 48.01425504684448
Epoch: 23, Steps: 94 | Train Loss: 0.4368243 Vali Loss: 0.5783681 Test Loss: 0.6821074
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 44.98468828201294
Epoch: 24, Steps: 94 | Train Loss: 0.4365690 Vali Loss: 0.5774232 Test Loss: 0.6819072
Validation loss decreased (0.577820 --> 0.577423).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 45.75747752189636
Epoch: 25, Steps: 94 | Train Loss: 0.4363606 Vali Loss: 0.5780296 Test Loss: 0.6817945
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 47.80074691772461
Epoch: 26, Steps: 94 | Train Loss: 0.4362886 Vali Loss: 0.5794867 Test Loss: 0.6817164
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 42.48331427574158
Epoch: 27, Steps: 94 | Train Loss: 0.4361350 Vali Loss: 0.5759527 Test Loss: 0.6815121
Validation loss decreased (0.577423 --> 0.575953).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 46.67645287513733
Epoch: 28, Steps: 94 | Train Loss: 0.4360834 Vali Loss: 0.5749440 Test Loss: 0.6814125
Validation loss decreased (0.575953 --> 0.574944).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 45.75207877159119
Epoch: 29, Steps: 94 | Train Loss: 0.4361054 Vali Loss: 0.5762277 Test Loss: 0.6813381
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 44.057162046432495
Epoch: 30, Steps: 94 | Train Loss: 0.4360031 Vali Loss: 0.5782738 Test Loss: 0.6812045
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 48.78983521461487
Epoch: 31, Steps: 94 | Train Loss: 0.4358689 Vali Loss: 0.5771825 Test Loss: 0.6811594
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 45.147693395614624
Epoch: 32, Steps: 94 | Train Loss: 0.4358716 Vali Loss: 0.5772029 Test Loss: 0.6810882
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 43.526747941970825
Epoch: 33, Steps: 94 | Train Loss: 0.4357939 Vali Loss: 0.5758175 Test Loss: 0.6809902
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 46.61798977851868
Epoch: 34, Steps: 94 | Train Loss: 0.4359065 Vali Loss: 0.5781537 Test Loss: 0.6809432
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 47.761496782302856
Epoch: 35, Steps: 94 | Train Loss: 0.4357225 Vali Loss: 0.5794419 Test Loss: 0.6809228
EarlyStopping counter: 7 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 40.48898386955261
Epoch: 36, Steps: 94 | Train Loss: 0.4357612 Vali Loss: 0.5763665 Test Loss: 0.6808800
EarlyStopping counter: 8 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 47.5384955406189
Epoch: 37, Steps: 94 | Train Loss: 0.4356328 Vali Loss: 0.5775563 Test Loss: 0.6808329
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 45.10297155380249
Epoch: 38, Steps: 94 | Train Loss: 0.4357467 Vali Loss: 0.5769625 Test Loss: 0.6808730
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j96_H5_FITS_custom_ftM_sl90_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.6861100792884827, mae:0.4079032242298126, rse:0.6858835220336914, corr:[0.2769092  0.2868335  0.28756738 0.28695917 0.28701878 0.28835338
 0.29038557 0.2903693  0.28941065 0.28945437 0.28869188 0.28768677
 0.28705707 0.28663164 0.2867179  0.28715327 0.2881925  0.28975818
 0.2915207  0.29373935 0.29508153 0.29594484 0.2956297  0.29494953
 0.2996214  0.30308935 0.3029233  0.30227426 0.3019403  0.30240858
 0.30323267 0.30500135 0.3051142  0.30467936 0.30462465 0.30379403
 0.30273455 0.30245587 0.3027045  0.30288437 0.30341738 0.30442187
 0.30555287 0.30650222 0.306672   0.3067052  0.30569565 0.3040252
 0.30385813 0.3019999  0.3003002  0.30003348 0.29861012 0.29404387
 0.29114863 0.2926851  0.29309657 0.29256883 0.2921225  0.29065534
 0.28903934 0.28844956 0.2881631  0.28768104 0.2877904  0.28844136
 0.2890505  0.28980488 0.29022488 0.29008886 0.28936005 0.28798103
 0.28613502 0.28436688 0.28411093 0.2852353  0.28598943 0.28558
 0.2860836  0.28869846 0.2890676  0.28845844 0.28800273 0.28657183
 0.2854691  0.2853144  0.2849266  0.28436688 0.2844676  0.2849994
 0.28478414 0.28461158 0.28540483 0.2845448  0.28388637 0.2864279 ]
