Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j192_H8_FITS_custom_ftM_sl720_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9280140288.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 187.66278409957886
Epoch: 1, Steps: 88 | Train Loss: 0.7715139 Vali Loss: 0.6157503 Test Loss: 0.7064521
Validation loss decreased (inf --> 0.615750).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 246.2876319885254
Epoch: 2, Steps: 88 | Train Loss: 0.3790691 Vali Loss: 0.4114528 Test Loss: 0.4814984
Validation loss decreased (0.615750 --> 0.411453).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 259.0217549800873
Epoch: 3, Steps: 88 | Train Loss: 0.2768482 Vali Loss: 0.3531393 Test Loss: 0.4202903
Validation loss decreased (0.411453 --> 0.353139).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 234.4328384399414
Epoch: 4, Steps: 88 | Train Loss: 0.2491325 Vali Loss: 0.3380909 Test Loss: 0.4063152
Validation loss decreased (0.353139 --> 0.338091).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 234.0344796180725
Epoch: 5, Steps: 88 | Train Loss: 0.2422851 Vali Loss: 0.3335103 Test Loss: 0.4031390
Validation loss decreased (0.338091 --> 0.333510).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 241.92765736579895
Epoch: 6, Steps: 88 | Train Loss: 0.2403887 Vali Loss: 0.3316146 Test Loss: 0.4020912
Validation loss decreased (0.333510 --> 0.331615).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 218.1783881187439
Epoch: 7, Steps: 88 | Train Loss: 0.2396938 Vali Loss: 0.3309626 Test Loss: 0.4017384
Validation loss decreased (0.331615 --> 0.330963).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 237.32941436767578
Epoch: 8, Steps: 88 | Train Loss: 0.2392569 Vali Loss: 0.3308882 Test Loss: 0.4011214
Validation loss decreased (0.330963 --> 0.330888).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 260.663800239563
Epoch: 9, Steps: 88 | Train Loss: 0.2390010 Vali Loss: 0.3298535 Test Loss: 0.4007616
Validation loss decreased (0.330888 --> 0.329853).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 240.18968629837036
Epoch: 10, Steps: 88 | Train Loss: 0.2388836 Vali Loss: 0.3296916 Test Loss: 0.4006208
Validation loss decreased (0.329853 --> 0.329692).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 240.6310374736786
Epoch: 11, Steps: 88 | Train Loss: 0.2386066 Vali Loss: 0.3294879 Test Loss: 0.4006028
Validation loss decreased (0.329692 --> 0.329488).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 243.58214211463928
Epoch: 12, Steps: 88 | Train Loss: 0.2386240 Vali Loss: 0.3291350 Test Loss: 0.4002744
Validation loss decreased (0.329488 --> 0.329135).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 239.64393520355225
Epoch: 13, Steps: 88 | Train Loss: 0.2384606 Vali Loss: 0.3290450 Test Loss: 0.4002324
Validation loss decreased (0.329135 --> 0.329045).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 225.94240999221802
Epoch: 14, Steps: 88 | Train Loss: 0.2384049 Vali Loss: 0.3293464 Test Loss: 0.4001557
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 228.80429887771606
Epoch: 15, Steps: 88 | Train Loss: 0.2384206 Vali Loss: 0.3288948 Test Loss: 0.4000382
Validation loss decreased (0.329045 --> 0.328895).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 224.73620629310608
Epoch: 16, Steps: 88 | Train Loss: 0.2382899 Vali Loss: 0.3280616 Test Loss: 0.3999893
Validation loss decreased (0.328895 --> 0.328062).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 226.457612991333
Epoch: 17, Steps: 88 | Train Loss: 0.2382979 Vali Loss: 0.3286687 Test Loss: 0.3999055
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 226.8338041305542
Epoch: 18, Steps: 88 | Train Loss: 0.2381767 Vali Loss: 0.3288392 Test Loss: 0.3999031
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 222.74354100227356
Epoch: 19, Steps: 88 | Train Loss: 0.2381137 Vali Loss: 0.3286808 Test Loss: 0.3997967
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 230.9625325202942
Epoch: 20, Steps: 88 | Train Loss: 0.2380608 Vali Loss: 0.3287393 Test Loss: 0.3999573
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 205.25996255874634
Epoch: 21, Steps: 88 | Train Loss: 0.2380521 Vali Loss: 0.3285535 Test Loss: 0.3996786
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 180.1721966266632
Epoch: 22, Steps: 88 | Train Loss: 0.2380301 Vali Loss: 0.3284473 Test Loss: 0.3997526
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 203.5695824623108
Epoch: 23, Steps: 88 | Train Loss: 0.2378982 Vali Loss: 0.3288611 Test Loss: 0.3995694
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 182.5868625640869
Epoch: 24, Steps: 88 | Train Loss: 0.2379505 Vali Loss: 0.3287561 Test Loss: 0.3996210
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 182.10968804359436
Epoch: 25, Steps: 88 | Train Loss: 0.2379720 Vali Loss: 0.3288629 Test Loss: 0.3997512
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 200.3900249004364
Epoch: 26, Steps: 88 | Train Loss: 0.2380056 Vali Loss: 0.3285654 Test Loss: 0.3996518
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j192_H8_FITS_custom_ftM_sl720_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.39903247356414795, mae:0.27559152245521545, rse:0.5213544964790344, corr:[0.2806867  0.28534243 0.2895785  0.28772712 0.289246   0.29151282
 0.2905554  0.2915509  0.2925178  0.29135007 0.29165867 0.29191336
 0.29032633 0.28983197 0.28998467 0.28912804 0.289184   0.29002857
 0.28991017 0.28996152 0.290393   0.28996518 0.28953123 0.28973126
 0.290451   0.2905763  0.2910693  0.29097176 0.29076603 0.2910325
 0.2907105  0.29039407 0.2907347  0.2904942  0.290052   0.2902823
 0.28988472 0.289056   0.28935412 0.28980395 0.2895715  0.28965905
 0.289796   0.2893171  0.2891353  0.2892729  0.28893647 0.28912282
 0.29009402 0.29019028 0.29023263 0.29041836 0.289874   0.28957167
 0.29012534 0.29003134 0.28950655 0.28971043 0.2896616  0.28914627
 0.28914124 0.28897947 0.2885095  0.28888577 0.2896605  0.28970715
 0.28945294 0.28931913 0.28896788 0.28865772 0.28867078 0.28900835
 0.28950927 0.28980243 0.28962314 0.28936186 0.28925046 0.28878483
 0.28832054 0.28843814 0.28843826 0.28808004 0.28800544 0.28787687
 0.2875462  0.28777048 0.28827557 0.28838852 0.28865308 0.289036
 0.28888583 0.2885743  0.2884227  0.2881118  0.2878608  0.28795615
 0.28790078 0.28819662 0.2886841  0.28849825 0.288278   0.2885867
 0.28857222 0.2881196  0.2879789  0.2877813  0.2872835  0.28718916
 0.28725427 0.28707504 0.28744736 0.28815177 0.28805974 0.28769118
 0.28770083 0.28752217 0.28716314 0.28700182 0.28691193 0.28691828
 0.28712767 0.28742787 0.28781077 0.2883632  0.2884467  0.2880695
 0.28812364 0.28824392 0.2878923  0.28780922 0.28799903 0.28765622
 0.28743368 0.2877441  0.28765848 0.28752366 0.28811297 0.28858644
 0.28872883 0.28911206 0.28911453 0.28857592 0.28846952 0.2886716
 0.28865018 0.28872794 0.28917292 0.2893764  0.28929564 0.28912878
 0.28873748 0.28823337 0.2878708  0.28780785 0.28827414 0.288818
 0.2887773  0.28877732 0.28912938 0.28901207 0.28881836 0.2892203
 0.28949583 0.28950536 0.2896758  0.28953105 0.2891013  0.28929287
 0.290024   0.2896589  0.2899554  0.29022902 0.28975683 0.28963083
 0.28945383 0.28868464 0.28857356 0.28870064 0.28825656 0.2891361
 0.2909153  0.29089123 0.29077518 0.2916444  0.29087758 0.29036522
 0.29142967 0.28959015 0.2876657  0.2888824  0.28592616 0.29259986]
