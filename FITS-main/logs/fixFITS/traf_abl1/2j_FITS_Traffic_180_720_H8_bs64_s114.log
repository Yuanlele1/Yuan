Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j720_H8_FITS_custom_ftM_sl180_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11381
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=74, out_features=370, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3020999680.0
params:  27750.0
Trainable parameters:  27750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 208.11781406402588
Epoch: 1, Steps: 88 | Train Loss: 1.6700435 Vali Loss: 1.4594191 Test Loss: 1.8122005
Validation loss decreased (inf --> 1.459419).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 201.23943853378296
Epoch: 2, Steps: 88 | Train Loss: 0.9144424 Vali Loss: 1.0054913 Test Loss: 1.2420709
Validation loss decreased (1.459419 --> 1.005491).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 188.82229804992676
Epoch: 3, Steps: 88 | Train Loss: 0.6494223 Vali Loss: 0.8024324 Test Loss: 0.9896857
Validation loss decreased (1.005491 --> 0.802432).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 184.92865538597107
Epoch: 4, Steps: 88 | Train Loss: 0.5245184 Vali Loss: 0.6971971 Test Loss: 0.8603298
Validation loss decreased (0.802432 --> 0.697197).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 185.10487484931946
Epoch: 5, Steps: 88 | Train Loss: 0.4567731 Vali Loss: 0.6375749 Test Loss: 0.7854396
Validation loss decreased (0.697197 --> 0.637575).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 190.9896056652069
Epoch: 6, Steps: 88 | Train Loss: 0.4157285 Vali Loss: 0.5985644 Test Loss: 0.7373300
Validation loss decreased (0.637575 --> 0.598564).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 182.81431770324707
Epoch: 7, Steps: 88 | Train Loss: 0.3881408 Vali Loss: 0.5720991 Test Loss: 0.7028890
Validation loss decreased (0.598564 --> 0.572099).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 189.97581338882446
Epoch: 8, Steps: 88 | Train Loss: 0.3685720 Vali Loss: 0.5530519 Test Loss: 0.6773430
Validation loss decreased (0.572099 --> 0.553052).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 183.27782011032104
Epoch: 9, Steps: 88 | Train Loss: 0.3536344 Vali Loss: 0.5368862 Test Loss: 0.6574718
Validation loss decreased (0.553052 --> 0.536886).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 170.57908868789673
Epoch: 10, Steps: 88 | Train Loss: 0.3417624 Vali Loss: 0.5245188 Test Loss: 0.6411285
Validation loss decreased (0.536886 --> 0.524519).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 180.29608345031738
Epoch: 11, Steps: 88 | Train Loss: 0.3321488 Vali Loss: 0.5146774 Test Loss: 0.6274821
Validation loss decreased (0.524519 --> 0.514677).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 185.16227197647095
Epoch: 12, Steps: 88 | Train Loss: 0.3241644 Vali Loss: 0.5059446 Test Loss: 0.6158982
Validation loss decreased (0.514677 --> 0.505945).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 185.23594975471497
Epoch: 13, Steps: 88 | Train Loss: 0.3174541 Vali Loss: 0.4992011 Test Loss: 0.6060672
Validation loss decreased (0.505945 --> 0.499201).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 184.55864238739014
Epoch: 14, Steps: 88 | Train Loss: 0.3116489 Vali Loss: 0.4925876 Test Loss: 0.5978737
Validation loss decreased (0.499201 --> 0.492588).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 185.52797651290894
Epoch: 15, Steps: 88 | Train Loss: 0.3066286 Vali Loss: 0.4874543 Test Loss: 0.5904369
Validation loss decreased (0.492588 --> 0.487454).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 176.73037910461426
Epoch: 16, Steps: 88 | Train Loss: 0.3022530 Vali Loss: 0.4830019 Test Loss: 0.5837938
Validation loss decreased (0.487454 --> 0.483002).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 176.5302815437317
Epoch: 17, Steps: 88 | Train Loss: 0.2984168 Vali Loss: 0.4791971 Test Loss: 0.5782712
Validation loss decreased (0.483002 --> 0.479197).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 148.96614241600037
Epoch: 18, Steps: 88 | Train Loss: 0.2950341 Vali Loss: 0.4757014 Test Loss: 0.5734062
Validation loss decreased (0.479197 --> 0.475701).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 154.17337656021118
Epoch: 19, Steps: 88 | Train Loss: 0.2920937 Vali Loss: 0.4726796 Test Loss: 0.5688170
Validation loss decreased (0.475701 --> 0.472680).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 166.5284571647644
Epoch: 20, Steps: 88 | Train Loss: 0.2894264 Vali Loss: 0.4693568 Test Loss: 0.5647598
Validation loss decreased (0.472680 --> 0.469357).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 186.48058128356934
Epoch: 21, Steps: 88 | Train Loss: 0.2870079 Vali Loss: 0.4672454 Test Loss: 0.5611880
Validation loss decreased (0.469357 --> 0.467245).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 176.40336966514587
Epoch: 22, Steps: 88 | Train Loss: 0.2847663 Vali Loss: 0.4651548 Test Loss: 0.5578637
Validation loss decreased (0.467245 --> 0.465155).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 181.87769961357117
Epoch: 23, Steps: 88 | Train Loss: 0.2828798 Vali Loss: 0.4627703 Test Loss: 0.5548965
Validation loss decreased (0.465155 --> 0.462770).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 178.90449142456055
Epoch: 24, Steps: 88 | Train Loss: 0.2810909 Vali Loss: 0.4609638 Test Loss: 0.5522181
Validation loss decreased (0.462770 --> 0.460964).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 173.09845972061157
Epoch: 25, Steps: 88 | Train Loss: 0.2794378 Vali Loss: 0.4588456 Test Loss: 0.5497239
Validation loss decreased (0.460964 --> 0.458846).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 179.76491475105286
Epoch: 26, Steps: 88 | Train Loss: 0.2779557 Vali Loss: 0.4581108 Test Loss: 0.5475242
Validation loss decreased (0.458846 --> 0.458111).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 171.24075555801392
Epoch: 27, Steps: 88 | Train Loss: 0.2766258 Vali Loss: 0.4561253 Test Loss: 0.5453702
Validation loss decreased (0.458111 --> 0.456125).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 163.5895733833313
Epoch: 28, Steps: 88 | Train Loss: 0.2753690 Vali Loss: 0.4554313 Test Loss: 0.5435501
Validation loss decreased (0.456125 --> 0.455431).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 128.24437189102173
Epoch: 29, Steps: 88 | Train Loss: 0.2741425 Vali Loss: 0.4543371 Test Loss: 0.5419063
Validation loss decreased (0.455431 --> 0.454337).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 135.35994338989258
Epoch: 30, Steps: 88 | Train Loss: 0.2731231 Vali Loss: 0.4527348 Test Loss: 0.5402799
Validation loss decreased (0.454337 --> 0.452735).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 129.69524574279785
Epoch: 31, Steps: 88 | Train Loss: 0.2722282 Vali Loss: 0.4522495 Test Loss: 0.5387609
Validation loss decreased (0.452735 --> 0.452249).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 141.57446002960205
Epoch: 32, Steps: 88 | Train Loss: 0.2712400 Vali Loss: 0.4507513 Test Loss: 0.5373178
Validation loss decreased (0.452249 --> 0.450751).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 144.63921880722046
Epoch: 33, Steps: 88 | Train Loss: 0.2704331 Vali Loss: 0.4497985 Test Loss: 0.5360997
Validation loss decreased (0.450751 --> 0.449798).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 147.0175805091858
Epoch: 34, Steps: 88 | Train Loss: 0.2696957 Vali Loss: 0.4492229 Test Loss: 0.5348697
Validation loss decreased (0.449798 --> 0.449223).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 128.80890536308289
Epoch: 35, Steps: 88 | Train Loss: 0.2689562 Vali Loss: 0.4482432 Test Loss: 0.5337877
Validation loss decreased (0.449223 --> 0.448243).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 131.23766231536865
Epoch: 36, Steps: 88 | Train Loss: 0.2682685 Vali Loss: 0.4479083 Test Loss: 0.5328127
Validation loss decreased (0.448243 --> 0.447908).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 125.5422728061676
Epoch: 37, Steps: 88 | Train Loss: 0.2676490 Vali Loss: 0.4470339 Test Loss: 0.5318146
Validation loss decreased (0.447908 --> 0.447034).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 106.78151845932007
Epoch: 38, Steps: 88 | Train Loss: 0.2669944 Vali Loss: 0.4466449 Test Loss: 0.5308626
Validation loss decreased (0.447034 --> 0.446645).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 116.94835925102234
Epoch: 39, Steps: 88 | Train Loss: 0.2664642 Vali Loss: 0.4458331 Test Loss: 0.5300273
Validation loss decreased (0.446645 --> 0.445833).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 137.4232943058014
Epoch: 40, Steps: 88 | Train Loss: 0.2659776 Vali Loss: 0.4457242 Test Loss: 0.5292991
Validation loss decreased (0.445833 --> 0.445724).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 118.86898970603943
Epoch: 41, Steps: 88 | Train Loss: 0.2654597 Vali Loss: 0.4452626 Test Loss: 0.5285581
Validation loss decreased (0.445724 --> 0.445263).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 160.46736025810242
Epoch: 42, Steps: 88 | Train Loss: 0.2650313 Vali Loss: 0.4449932 Test Loss: 0.5278735
Validation loss decreased (0.445263 --> 0.444993).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 142.61291360855103
Epoch: 43, Steps: 88 | Train Loss: 0.2645965 Vali Loss: 0.4443116 Test Loss: 0.5272097
Validation loss decreased (0.444993 --> 0.444312).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 193.48471522331238
Epoch: 44, Steps: 88 | Train Loss: 0.2641817 Vali Loss: 0.4439019 Test Loss: 0.5265871
Validation loss decreased (0.444312 --> 0.443902).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 184.06636571884155
Epoch: 45, Steps: 88 | Train Loss: 0.2637963 Vali Loss: 0.4434987 Test Loss: 0.5260474
Validation loss decreased (0.443902 --> 0.443499).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 142.06644105911255
Epoch: 46, Steps: 88 | Train Loss: 0.2635224 Vali Loss: 0.4429086 Test Loss: 0.5254923
Validation loss decreased (0.443499 --> 0.442909).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 124.81350708007812
Epoch: 47, Steps: 88 | Train Loss: 0.2631930 Vali Loss: 0.4428209 Test Loss: 0.5250217
Validation loss decreased (0.442909 --> 0.442821).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 143.4500937461853
Epoch: 48, Steps: 88 | Train Loss: 0.2628220 Vali Loss: 0.4422737 Test Loss: 0.5245386
Validation loss decreased (0.442821 --> 0.442274).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 163.11741995811462
Epoch: 49, Steps: 88 | Train Loss: 0.2625212 Vali Loss: 0.4419977 Test Loss: 0.5241095
Validation loss decreased (0.442274 --> 0.441998).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 167.7399458885193
Epoch: 50, Steps: 88 | Train Loss: 0.2622649 Vali Loss: 0.4415304 Test Loss: 0.5236617
Validation loss decreased (0.441998 --> 0.441530).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 189.66138100624084
Epoch: 51, Steps: 88 | Train Loss: 0.2620092 Vali Loss: 0.4416475 Test Loss: 0.5232772
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 190.98519563674927
Epoch: 52, Steps: 88 | Train Loss: 0.2618259 Vali Loss: 0.4414666 Test Loss: 0.5228903
Validation loss decreased (0.441530 --> 0.441467).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 132.20031714439392
Epoch: 53, Steps: 88 | Train Loss: 0.2615567 Vali Loss: 0.4414608 Test Loss: 0.5225224
Validation loss decreased (0.441467 --> 0.441461).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 152.45301866531372
Epoch: 54, Steps: 88 | Train Loss: 0.2612813 Vali Loss: 0.4408382 Test Loss: 0.5221646
Validation loss decreased (0.441461 --> 0.440838).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 136.01378440856934
Epoch: 55, Steps: 88 | Train Loss: 0.2610711 Vali Loss: 0.4410436 Test Loss: 0.5218395
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 132.97544121742249
Epoch: 56, Steps: 88 | Train Loss: 0.2609145 Vali Loss: 0.4402201 Test Loss: 0.5215592
Validation loss decreased (0.440838 --> 0.440220).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 140.53199791908264
Epoch: 57, Steps: 88 | Train Loss: 0.2607558 Vali Loss: 0.4403716 Test Loss: 0.5212563
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 168.56527996063232
Epoch: 58, Steps: 88 | Train Loss: 0.2605504 Vali Loss: 0.4405199 Test Loss: 0.5209857
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 163.8628237247467
Epoch: 59, Steps: 88 | Train Loss: 0.2603327 Vali Loss: 0.4401475 Test Loss: 0.5207529
Validation loss decreased (0.440220 --> 0.440148).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 156.0816297531128
Epoch: 60, Steps: 88 | Train Loss: 0.2601978 Vali Loss: 0.4401108 Test Loss: 0.5205141
Validation loss decreased (0.440148 --> 0.440111).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 158.91159534454346
Epoch: 61, Steps: 88 | Train Loss: 0.2601367 Vali Loss: 0.4396458 Test Loss: 0.5202745
Validation loss decreased (0.440111 --> 0.439646).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 123.00664472579956
Epoch: 62, Steps: 88 | Train Loss: 0.2598539 Vali Loss: 0.4396720 Test Loss: 0.5200463
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 115.32220339775085
Epoch: 63, Steps: 88 | Train Loss: 0.2597988 Vali Loss: 0.4394657 Test Loss: 0.5198419
Validation loss decreased (0.439646 --> 0.439466).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 111.02441000938416
Epoch: 64, Steps: 88 | Train Loss: 0.2596833 Vali Loss: 0.4391594 Test Loss: 0.5196516
Validation loss decreased (0.439466 --> 0.439159).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 133.08291888237
Epoch: 65, Steps: 88 | Train Loss: 0.2594930 Vali Loss: 0.4392401 Test Loss: 0.5194436
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 99.30135703086853
Epoch: 66, Steps: 88 | Train Loss: 0.2593529 Vali Loss: 0.4397261 Test Loss: 0.5192995
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 125.00442504882812
Epoch: 67, Steps: 88 | Train Loss: 0.2592148 Vali Loss: 0.4394225 Test Loss: 0.5191046
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 109.32134079933167
Epoch: 68, Steps: 88 | Train Loss: 0.2591367 Vali Loss: 0.4389130 Test Loss: 0.5189450
Validation loss decreased (0.439159 --> 0.438913).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 94.7819037437439
Epoch: 69, Steps: 88 | Train Loss: 0.2590987 Vali Loss: 0.4391617 Test Loss: 0.5187944
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 96.13456296920776
Epoch: 70, Steps: 88 | Train Loss: 0.2590886 Vali Loss: 0.4388171 Test Loss: 0.5186594
Validation loss decreased (0.438913 --> 0.438817).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 93.97655630111694
Epoch: 71, Steps: 88 | Train Loss: 0.2589005 Vali Loss: 0.4388763 Test Loss: 0.5185460
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 97.70571160316467
Epoch: 72, Steps: 88 | Train Loss: 0.2588763 Vali Loss: 0.4387195 Test Loss: 0.5184031
Validation loss decreased (0.438817 --> 0.438720).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 97.81818151473999
Epoch: 73, Steps: 88 | Train Loss: 0.2588206 Vali Loss: 0.4382732 Test Loss: 0.5182921
Validation loss decreased (0.438720 --> 0.438273).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 91.28412580490112
Epoch: 74, Steps: 88 | Train Loss: 0.2587245 Vali Loss: 0.4386628 Test Loss: 0.5181640
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 91.75576114654541
Epoch: 75, Steps: 88 | Train Loss: 0.2586031 Vali Loss: 0.4378802 Test Loss: 0.5180517
Validation loss decreased (0.438273 --> 0.437880).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 86.06931591033936
Epoch: 76, Steps: 88 | Train Loss: 0.2585216 Vali Loss: 0.4382652 Test Loss: 0.5179642
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 92.91710138320923
Epoch: 77, Steps: 88 | Train Loss: 0.2584787 Vali Loss: 0.4379598 Test Loss: 0.5178671
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 108.63169693946838
Epoch: 78, Steps: 88 | Train Loss: 0.2584144 Vali Loss: 0.4386397 Test Loss: 0.5177726
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 86.42415809631348
Epoch: 79, Steps: 88 | Train Loss: 0.2584237 Vali Loss: 0.4378405 Test Loss: 0.5176756
Validation loss decreased (0.437880 --> 0.437841).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 101.45017862319946
Epoch: 80, Steps: 88 | Train Loss: 0.2583220 Vali Loss: 0.4377440 Test Loss: 0.5175925
Validation loss decreased (0.437841 --> 0.437744).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 101.55805516242981
Epoch: 81, Steps: 88 | Train Loss: 0.2582911 Vali Loss: 0.4383879 Test Loss: 0.5175027
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 98.48120260238647
Epoch: 82, Steps: 88 | Train Loss: 0.2582762 Vali Loss: 0.4378742 Test Loss: 0.5174305
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 105.31620526313782
Epoch: 83, Steps: 88 | Train Loss: 0.2582179 Vali Loss: 0.4382244 Test Loss: 0.5173619
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 103.39394283294678
Epoch: 84, Steps: 88 | Train Loss: 0.2581663 Vali Loss: 0.4374771 Test Loss: 0.5173075
Validation loss decreased (0.437744 --> 0.437477).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 103.28371691703796
Epoch: 85, Steps: 88 | Train Loss: 0.2580625 Vali Loss: 0.4378440 Test Loss: 0.5172446
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 99.25801587104797
Epoch: 86, Steps: 88 | Train Loss: 0.2580309 Vali Loss: 0.4375234 Test Loss: 0.5171750
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 78.56187605857849
Epoch: 87, Steps: 88 | Train Loss: 0.2580533 Vali Loss: 0.4379138 Test Loss: 0.5171157
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 80.01504802703857
Epoch: 88, Steps: 88 | Train Loss: 0.2580287 Vali Loss: 0.4380166 Test Loss: 0.5170680
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 78.15960597991943
Epoch: 89, Steps: 88 | Train Loss: 0.2578615 Vali Loss: 0.4373730 Test Loss: 0.5170120
Validation loss decreased (0.437477 --> 0.437373).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 68.50169372558594
Epoch: 90, Steps: 88 | Train Loss: 0.2579284 Vali Loss: 0.4377013 Test Loss: 0.5169538
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 67.89113903045654
Epoch: 91, Steps: 88 | Train Loss: 0.2578169 Vali Loss: 0.4373757 Test Loss: 0.5169173
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 68.28185319900513
Epoch: 92, Steps: 88 | Train Loss: 0.2578131 Vali Loss: 0.4379959 Test Loss: 0.5168623
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 67.18766975402832
Epoch: 93, Steps: 88 | Train Loss: 0.2578272 Vali Loss: 0.4374394 Test Loss: 0.5168265
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 68.53158020973206
Epoch: 94, Steps: 88 | Train Loss: 0.2577254 Vali Loss: 0.4376699 Test Loss: 0.5167902
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 58.72930598258972
Epoch: 95, Steps: 88 | Train Loss: 0.2577464 Vali Loss: 0.4374745 Test Loss: 0.5167449
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 57.771528482437134
Epoch: 96, Steps: 88 | Train Loss: 0.2576487 Vali Loss: 0.4374592 Test Loss: 0.5167034
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 57.91053605079651
Epoch: 97, Steps: 88 | Train Loss: 0.2576710 Vali Loss: 0.4372689 Test Loss: 0.5166768
Validation loss decreased (0.437373 --> 0.437269).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 58.180227279663086
Epoch: 98, Steps: 88 | Train Loss: 0.2576871 Vali Loss: 0.4374529 Test Loss: 0.5166355
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 58.627033948898315
Epoch: 99, Steps: 88 | Train Loss: 0.2577075 Vali Loss: 0.4381713 Test Loss: 0.5166070
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 58.27350115776062
Epoch: 100, Steps: 88 | Train Loss: 0.2576222 Vali Loss: 0.4373190 Test Loss: 0.5165758
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11381
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=74, out_features=370, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3020999680.0
params:  27750.0
Trainable parameters:  27750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 59.43753623962402
Epoch: 1, Steps: 88 | Train Loss: 0.3147173 Vali Loss: 0.4332650 Test Loss: 0.5095014
Validation loss decreased (inf --> 0.433265).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 58.40699911117554
Epoch: 2, Steps: 88 | Train Loss: 0.3117410 Vali Loss: 0.4318146 Test Loss: 0.5075405
Validation loss decreased (0.433265 --> 0.431815).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 57.803778409957886
Epoch: 3, Steps: 88 | Train Loss: 0.3109703 Vali Loss: 0.4319133 Test Loss: 0.5075383
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00045125
Epoch: 4 cost time: 58.415900230407715
Epoch: 4, Steps: 88 | Train Loss: 0.3108232 Vali Loss: 0.4309776 Test Loss: 0.5064124
Validation loss decreased (0.431815 --> 0.430978).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 57.81514382362366
Epoch: 5, Steps: 88 | Train Loss: 0.3106426 Vali Loss: 0.4309797 Test Loss: 0.5066305
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 57.34735321998596
Epoch: 6, Steps: 88 | Train Loss: 0.3106929 Vali Loss: 0.4310979 Test Loss: 0.5063028
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 56.80669951438904
Epoch: 7, Steps: 88 | Train Loss: 0.3106315 Vali Loss: 0.4308570 Test Loss: 0.5059173
Validation loss decreased (0.430978 --> 0.430857).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 53.56952667236328
Epoch: 8, Steps: 88 | Train Loss: 0.3106660 Vali Loss: 0.4313756 Test Loss: 0.5063382
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 86.21925067901611
Epoch: 9, Steps: 88 | Train Loss: 0.3106416 Vali Loss: 0.4309812 Test Loss: 0.5067279
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 98.28176522254944
Epoch: 10, Steps: 88 | Train Loss: 0.3105191 Vali Loss: 0.4311687 Test Loss: 0.5065803
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 83.28904581069946
Epoch: 11, Steps: 88 | Train Loss: 0.3104910 Vali Loss: 0.4308203 Test Loss: 0.5060055
Validation loss decreased (0.430857 --> 0.430820).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 85.28703379631042
Epoch: 12, Steps: 88 | Train Loss: 0.3106025 Vali Loss: 0.4312592 Test Loss: 0.5064843
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 88.86781883239746
Epoch: 13, Steps: 88 | Train Loss: 0.3106584 Vali Loss: 0.4315287 Test Loss: 0.5063332
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 113.33180069923401
Epoch: 14, Steps: 88 | Train Loss: 0.3104806 Vali Loss: 0.4311703 Test Loss: 0.5064075
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 110.63832998275757
Epoch: 15, Steps: 88 | Train Loss: 0.3104474 Vali Loss: 0.4310429 Test Loss: 0.5062969
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 107.23546600341797
Epoch: 16, Steps: 88 | Train Loss: 0.3104122 Vali Loss: 0.4311579 Test Loss: 0.5063338
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 107.98519349098206
Epoch: 17, Steps: 88 | Train Loss: 0.3104348 Vali Loss: 0.4311482 Test Loss: 0.5062972
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 112.5789954662323
Epoch: 18, Steps: 88 | Train Loss: 0.3104340 Vali Loss: 0.4308865 Test Loss: 0.5060568
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 110.10958766937256
Epoch: 19, Steps: 88 | Train Loss: 0.3104780 Vali Loss: 0.4311721 Test Loss: 0.5062346
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 111.5868718624115
Epoch: 20, Steps: 88 | Train Loss: 0.3105021 Vali Loss: 0.4310613 Test Loss: 0.5063059
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 114.88087821006775
Epoch: 21, Steps: 88 | Train Loss: 0.3103258 Vali Loss: 0.4311349 Test Loss: 0.5062271
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j720_H8_FITS_custom_ftM_sl180_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.5365654230117798, mae:0.3701927065849304, rse:0.5989797711372375, corr:[0.26340517 0.25219342 0.26346242 0.25849172 0.26357847 0.2638514
 0.2644405  0.26834536 0.26737738 0.2677438  0.26901957 0.26636243
 0.2675124  0.26513174 0.26320824 0.26331565 0.26134673 0.26165724
 0.26178008 0.26252672 0.26447272 0.2647772  0.26564142 0.26700884
 0.2663889  0.26746497 0.2672865  0.26576954 0.26655117 0.26615816
 0.26670378 0.26740274 0.26729128 0.26773387 0.26805782 0.26682672
 0.2664739  0.2655633  0.265114   0.26475927 0.26322436 0.26436707
 0.26480532 0.26462275 0.2659626  0.2668197  0.26702118 0.26812154
 0.26789364 0.26778427 0.26819044 0.2672299  0.2675597  0.26667976
 0.26626763 0.2675297  0.26769835 0.2682599  0.26943564 0.268305
 0.26829362 0.26815554 0.26690885 0.26683968 0.26566312 0.2654288
 0.2656017  0.26572064 0.26688    0.26824257 0.26879838 0.26958364
 0.26941824 0.26858348 0.2682334  0.266625   0.26633137 0.2657469
 0.26528132 0.26651475 0.2666795  0.266886   0.26783976 0.26757658
 0.2677709  0.26733097 0.26639664 0.26650247 0.2655067  0.26545817
 0.26556927 0.26527002 0.26662293 0.26759472 0.26768407 0.26872543
 0.26864326 0.26767552 0.266853   0.26536518 0.2649615  0.26348114
 0.26261958 0.2632993  0.2637852  0.26443622 0.2655442  0.26591575
 0.26517814 0.26521724 0.26430544 0.263481   0.26226255 0.26123026
 0.26218462 0.26238316 0.26304147 0.26448357 0.26518553 0.26550668
 0.2665683  0.26638108 0.26534545 0.26486462 0.26351565 0.26175925
 0.26169106 0.2622744  0.26274723 0.26354876 0.26441953 0.26501122
 0.26469678 0.2652152  0.26442072 0.26281643 0.26271218 0.26126328
 0.26081082 0.26178166 0.2626216  0.26441288 0.26573402 0.26565653
 0.26675087 0.26679397 0.26582214 0.26495022 0.26358414 0.26343903
 0.26286873 0.26224726 0.26338425 0.26489308 0.26612923 0.2669256
 0.26752558 0.268277   0.2673533  0.26559296 0.26427466 0.2638442
 0.26363078 0.264181   0.26563138 0.26732036 0.269448   0.26928136
 0.26977944 0.26946563 0.26744652 0.26686978 0.26575136 0.2650803
 0.26532602 0.26584595 0.26688913 0.26808426 0.26876023 0.26896665
 0.26859668 0.26828334 0.26746604 0.26580197 0.2649736  0.2648604
 0.26457262 0.26521593 0.2671767  0.2682577  0.26928097 0.2696845
 0.2697142  0.26977935 0.26876497 0.26774126 0.26683888 0.26615927
 0.26649296 0.2672613  0.2679086  0.26954195 0.2704812  0.2701852
 0.27068505 0.27031928 0.26961693 0.2683053  0.26722482 0.26676947
 0.26636097 0.26768157 0.26886365 0.26966515 0.270311   0.27084315
 0.27113795 0.27128553 0.27089316 0.26967463 0.26924393 0.26862848
 0.26852238 0.26900652 0.27020496 0.2716766  0.27234718 0.27224153
 0.2724156  0.27266026 0.2717289  0.2706071  0.26881167 0.26805001
 0.26904416 0.269845   0.2705953  0.27130505 0.27221644 0.27264357
 0.2723507  0.2721219  0.27106196 0.26953804 0.26864737 0.26742697
 0.26771334 0.26860446 0.2689017  0.26993558 0.2704298  0.27108544
 0.2717888  0.2716317  0.27044153 0.26880923 0.26796064 0.26717708
 0.26746824 0.2682843  0.26906428 0.27047488 0.27080435 0.27123865
 0.27083114 0.27042112 0.26967564 0.26706764 0.26566228 0.26479658
 0.2642801  0.26497346 0.265921   0.2667291  0.26714936 0.2665948
 0.26678684 0.26692295 0.2658529  0.26469278 0.26244143 0.26247025
 0.26237825 0.26210478 0.26397404 0.26558983 0.26642472 0.2667969
 0.2673687  0.26603675 0.2648856  0.2642233  0.26258808 0.26145288
 0.2609368  0.2613358  0.26235038 0.26383322 0.26507756 0.26510262
 0.26478642 0.26527798 0.2641539  0.2622132  0.2609181  0.26053298
 0.2607406  0.2603273  0.2621071  0.2645525  0.26594254 0.26604337
 0.2662811  0.26647788 0.2654107  0.2638292  0.26181206 0.26064357
 0.26061317 0.2612344  0.26222628 0.26423454 0.26584202 0.26629692
 0.26713538 0.26741284 0.26632798 0.26399148 0.26220953 0.26122257
 0.26063102 0.26233378 0.2644657  0.26651418 0.26907018 0.26959455
 0.2695932  0.26903957 0.267363   0.26519567 0.263349   0.26269364
 0.26278538 0.26364377 0.2647455  0.26632628 0.26729962 0.2672499
 0.2673478  0.267369   0.26636744 0.26426095 0.26232132 0.2619028
 0.26240477 0.26329795 0.26497543 0.26640034 0.26781526 0.2682264
 0.26850858 0.26937285 0.2682275  0.26648036 0.26515257 0.26440695
 0.26507777 0.26577857 0.26676518 0.26831862 0.2685734  0.26919618
 0.26979098 0.26927325 0.2688707  0.26738283 0.2658842  0.26537436
 0.26564336 0.26651347 0.26760504 0.2688896  0.26974875 0.27027962
 0.27027723 0.27092606 0.27055088 0.2695561  0.26834315 0.26679796
 0.26777515 0.26827943 0.2686461  0.26997828 0.2707405  0.27156103
 0.27230546 0.27225253 0.2717279  0.27019498 0.2688571  0.2686491
 0.26776332 0.26891622 0.2704552  0.2709488  0.27202395 0.2723358
 0.2722204  0.27210608 0.2714046  0.26996657 0.26858726 0.26828083
 0.26796922 0.26717335 0.26821497 0.26963165 0.27040923 0.2712302
 0.2715952  0.2719757  0.27088243 0.26915997 0.26778853 0.2679487
 0.26823178 0.2677809  0.2688924  0.27016306 0.2710522  0.27179658
 0.27143258 0.27025774 0.2692565  0.26809597 0.26614124 0.2648634
 0.2642865  0.26410237 0.26507607 0.2662717  0.2671586  0.267286
 0.26771957 0.2674737  0.2654464  0.26515797 0.2642948  0.26261273
 0.26226056 0.26272398 0.2639884  0.26554602 0.2665378  0.26732683
 0.26735246 0.26641485 0.2661087  0.26424837 0.26281205 0.26205334
 0.26057264 0.26086712 0.26210356 0.26381874 0.26544473 0.26563135
 0.2655205  0.2660077  0.26491004 0.2630156  0.26171258 0.2609117
 0.260718   0.26134217 0.26325935 0.2651268  0.2664045  0.26671678
 0.26686013 0.26691723 0.26612207 0.26464775 0.26333636 0.26232156
 0.26145497 0.2616913  0.2626512  0.26470137 0.26606026 0.26673684
 0.26819614 0.26872385 0.26748037 0.2655739  0.26372498 0.2626236
 0.26286098 0.26329014 0.264721   0.26696688 0.26889437 0.26857203
 0.26838014 0.26910093 0.26785558 0.26644942 0.2650893  0.26443264
 0.26454726 0.26437083 0.2648988  0.26677868 0.26790234 0.2677801
 0.2678108  0.26805982 0.26759297 0.2652552  0.2638509  0.26341978
 0.26276574 0.26424572 0.26598355 0.26677743 0.2673848  0.26752907
 0.26841334 0.2694521  0.26895583 0.26813614 0.26693112 0.26593786
 0.26639888 0.26652634 0.2676497  0.2692069  0.26908892 0.2693429
 0.26985416 0.2691388  0.26856917 0.267704   0.26666415 0.26669666
 0.26671657 0.26728076 0.26816827 0.26869643 0.26913327 0.2690398
 0.2693023  0.26990885 0.2695262  0.26890358 0.2683417  0.26779324
 0.26834416 0.26878545 0.26922372 0.27010146 0.2702924  0.2704112
 0.2708645  0.2705811  0.2700447  0.2688307  0.267606   0.26776347
 0.2675951  0.26800707 0.26908374 0.26926497 0.26967844 0.27042422
 0.26998165 0.26984134 0.2694454  0.26835155 0.26795757 0.26734146
 0.26731732 0.26721346 0.26751754 0.26852217 0.26885405 0.26888663
 0.26949346 0.26950502 0.26868185 0.26768968 0.2663481  0.26655686
 0.26649052 0.26632085 0.26737857 0.26813695 0.26858342 0.26897046
 0.26848212 0.26736954 0.26658604 0.26546362 0.26458645 0.26400954
 0.26398754 0.26462507 0.26508066 0.26572105 0.26638106 0.2660829
 0.26614967 0.26672938 0.26588175 0.2649692  0.2640736  0.26349065
 0.2631849  0.2630415  0.2639819  0.26487058 0.26532274 0.2658201
 0.26570624 0.26524147 0.26473036 0.26301342 0.26180896 0.2613122
 0.26076692 0.26108494 0.2617575  0.26252908 0.26357812 0.26390347
 0.26437947 0.2656805  0.26518404 0.2639616  0.2629188  0.26245755
 0.26234362 0.26202208 0.26275897 0.2641529  0.2649119  0.26518556
 0.26564524 0.26531032 0.26514032 0.2643052  0.2633062  0.26267624
 0.2618954  0.26212975 0.2625365  0.26308692 0.26404577 0.26499602
 0.26602453 0.26709113 0.266088   0.26472995 0.26366156 0.26246634
 0.2627942  0.26317012 0.26407242 0.26579055 0.26720345 0.26678282
 0.2669368  0.26658142 0.2656169  0.26516375 0.26401556 0.26374066
 0.2633319  0.26390657 0.2646831  0.26511404 0.26609698 0.26711053
 0.2675139  0.2681479  0.26736522 0.26585114 0.26562053 0.26415402
 0.26433977 0.26527977 0.2651571  0.2662777  0.26701573 0.26580897
 0.2670081  0.2677684  0.26672363 0.2672807  0.26625264 0.2659219
 0.2654812  0.26570407 0.2665746  0.26656613 0.26785037 0.26911414
 0.26834178 0.26923192 0.26869294 0.26727128 0.26858562 0.26524127
 0.26546547 0.2650832  0.26128346 0.2646806  0.26142418 0.2699851 ]
