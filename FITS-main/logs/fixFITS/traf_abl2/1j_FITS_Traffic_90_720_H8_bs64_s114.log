Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j720_H8_FITS_custom_ftM_sl90_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11471
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=42, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1751694336.0
params:  16254.0
Trainable parameters:  16254
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 38.544036865234375
Epoch: 1, Steps: 89 | Train Loss: 2.2426278 Vali Loss: 1.7431977 Test Loss: 2.1769540
Validation loss decreased (inf --> 1.743198).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 39.043102979660034
Epoch: 2, Steps: 89 | Train Loss: 1.0985293 Vali Loss: 1.0818257 Test Loss: 1.3364922
Validation loss decreased (1.743198 --> 1.081826).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 39.23727107048035
Epoch: 3, Steps: 89 | Train Loss: 0.7360998 Vali Loss: 0.8257849 Test Loss: 1.0067550
Validation loss decreased (1.081826 --> 0.825785).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 41.23347616195679
Epoch: 4, Steps: 89 | Train Loss: 0.5853575 Vali Loss: 0.7092919 Test Loss: 0.8570570
Validation loss decreased (0.825785 --> 0.709292).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 40.81559729576111
Epoch: 5, Steps: 89 | Train Loss: 0.5141577 Vali Loss: 0.6516460 Test Loss: 0.7830131
Validation loss decreased (0.709292 --> 0.651646).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 39.948649644851685
Epoch: 6, Steps: 89 | Train Loss: 0.4776712 Vali Loss: 0.6209643 Test Loss: 0.7432591
Validation loss decreased (0.651646 --> 0.620964).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 39.7692334651947
Epoch: 7, Steps: 89 | Train Loss: 0.4574887 Vali Loss: 0.6028538 Test Loss: 0.7205564
Validation loss decreased (0.620964 --> 0.602854).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 41.27110505104065
Epoch: 8, Steps: 89 | Train Loss: 0.4453178 Vali Loss: 0.5922640 Test Loss: 0.7070708
Validation loss decreased (0.602854 --> 0.592264).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 40.377485513687134
Epoch: 9, Steps: 89 | Train Loss: 0.4379314 Vali Loss: 0.5853097 Test Loss: 0.6981577
Validation loss decreased (0.592264 --> 0.585310).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 40.06001663208008
Epoch: 10, Steps: 89 | Train Loss: 0.4329220 Vali Loss: 0.5795833 Test Loss: 0.6919312
Validation loss decreased (0.585310 --> 0.579583).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 40.32719326019287
Epoch: 11, Steps: 89 | Train Loss: 0.4294275 Vali Loss: 0.5757245 Test Loss: 0.6877033
Validation loss decreased (0.579583 --> 0.575724).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 40.15000081062317
Epoch: 12, Steps: 89 | Train Loss: 0.4266783 Vali Loss: 0.5742427 Test Loss: 0.6845796
Validation loss decreased (0.575724 --> 0.574243).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 41.0450336933136
Epoch: 13, Steps: 89 | Train Loss: 0.4248156 Vali Loss: 0.5719016 Test Loss: 0.6820880
Validation loss decreased (0.574243 --> 0.571902).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 40.40705323219299
Epoch: 14, Steps: 89 | Train Loss: 0.4232275 Vali Loss: 0.5701334 Test Loss: 0.6801671
Validation loss decreased (0.571902 --> 0.570133).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 41.31227135658264
Epoch: 15, Steps: 89 | Train Loss: 0.4221559 Vali Loss: 0.5692307 Test Loss: 0.6786074
Validation loss decreased (0.570133 --> 0.569231).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 40.30208444595337
Epoch: 16, Steps: 89 | Train Loss: 0.4211270 Vali Loss: 0.5677890 Test Loss: 0.6774173
Validation loss decreased (0.569231 --> 0.567789).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 39.625428915023804
Epoch: 17, Steps: 89 | Train Loss: 0.4201845 Vali Loss: 0.5669762 Test Loss: 0.6767580
Validation loss decreased (0.567789 --> 0.566976).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 39.75477743148804
Epoch: 18, Steps: 89 | Train Loss: 0.4195509 Vali Loss: 0.5657022 Test Loss: 0.6757168
Validation loss decreased (0.566976 --> 0.565702).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 40.77204513549805
Epoch: 19, Steps: 89 | Train Loss: 0.4189960 Vali Loss: 0.5659696 Test Loss: 0.6749808
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 38.801063537597656
Epoch: 20, Steps: 89 | Train Loss: 0.4185690 Vali Loss: 0.5651366 Test Loss: 0.6743451
Validation loss decreased (0.565702 --> 0.565137).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 37.04994463920593
Epoch: 21, Steps: 89 | Train Loss: 0.4180180 Vali Loss: 0.5641352 Test Loss: 0.6738726
Validation loss decreased (0.565137 --> 0.564135).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 39.12523031234741
Epoch: 22, Steps: 89 | Train Loss: 0.4176615 Vali Loss: 0.5644400 Test Loss: 0.6733503
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 39.94302439689636
Epoch: 23, Steps: 89 | Train Loss: 0.4173119 Vali Loss: 0.5639538 Test Loss: 0.6730323
Validation loss decreased (0.564135 --> 0.563954).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 40.105650663375854
Epoch: 24, Steps: 89 | Train Loss: 0.4169848 Vali Loss: 0.5636529 Test Loss: 0.6724547
Validation loss decreased (0.563954 --> 0.563653).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 39.694315910339355
Epoch: 25, Steps: 89 | Train Loss: 0.4167645 Vali Loss: 0.5627397 Test Loss: 0.6722178
Validation loss decreased (0.563653 --> 0.562740).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 42.43792366981506
Epoch: 26, Steps: 89 | Train Loss: 0.4164699 Vali Loss: 0.5629528 Test Loss: 0.6719118
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 42.0975284576416
Epoch: 27, Steps: 89 | Train Loss: 0.4163561 Vali Loss: 0.5620220 Test Loss: 0.6717009
Validation loss decreased (0.562740 --> 0.562022).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 39.68754029273987
Epoch: 28, Steps: 89 | Train Loss: 0.4161048 Vali Loss: 0.5623168 Test Loss: 0.6714052
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 40.477936029434204
Epoch: 29, Steps: 89 | Train Loss: 0.4157865 Vali Loss: 0.5621722 Test Loss: 0.6712155
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 41.472623109817505
Epoch: 30, Steps: 89 | Train Loss: 0.4158255 Vali Loss: 0.5622552 Test Loss: 0.6709691
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 41.463189125061035
Epoch: 31, Steps: 89 | Train Loss: 0.4155062 Vali Loss: 0.5612990 Test Loss: 0.6708973
Validation loss decreased (0.562022 --> 0.561299).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 41.60626792907715
Epoch: 32, Steps: 89 | Train Loss: 0.4154121 Vali Loss: 0.5621811 Test Loss: 0.6707514
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 42.04513645172119
Epoch: 33, Steps: 89 | Train Loss: 0.4152591 Vali Loss: 0.5616809 Test Loss: 0.6705369
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 40.04952383041382
Epoch: 34, Steps: 89 | Train Loss: 0.4151651 Vali Loss: 0.5612333 Test Loss: 0.6704282
Validation loss decreased (0.561299 --> 0.561233).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 41.01230597496033
Epoch: 35, Steps: 89 | Train Loss: 0.4150145 Vali Loss: 0.5612236 Test Loss: 0.6703565
Validation loss decreased (0.561233 --> 0.561224).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 41.89852046966553
Epoch: 36, Steps: 89 | Train Loss: 0.4150253 Vali Loss: 0.5612198 Test Loss: 0.6701730
Validation loss decreased (0.561224 --> 0.561220).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 42.05798530578613
Epoch: 37, Steps: 89 | Train Loss: 0.4149582 Vali Loss: 0.5615060 Test Loss: 0.6700656
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 41.15061545372009
Epoch: 38, Steps: 89 | Train Loss: 0.4148589 Vali Loss: 0.5610968 Test Loss: 0.6699720
Validation loss decreased (0.561220 --> 0.561097).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 41.70104503631592
Epoch: 39, Steps: 89 | Train Loss: 0.4146100 Vali Loss: 0.5607175 Test Loss: 0.6698771
Validation loss decreased (0.561097 --> 0.560718).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 41.69247484207153
Epoch: 40, Steps: 89 | Train Loss: 0.4146081 Vali Loss: 0.5608999 Test Loss: 0.6697883
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 40.04802870750427
Epoch: 41, Steps: 89 | Train Loss: 0.4145165 Vali Loss: 0.5608327 Test Loss: 0.6698393
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 42.34152865409851
Epoch: 42, Steps: 89 | Train Loss: 0.4144455 Vali Loss: 0.5608618 Test Loss: 0.6697633
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 42.04293990135193
Epoch: 43, Steps: 89 | Train Loss: 0.4144401 Vali Loss: 0.5607637 Test Loss: 0.6696625
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 40.644548416137695
Epoch: 44, Steps: 89 | Train Loss: 0.4141467 Vali Loss: 0.5610639 Test Loss: 0.6695911
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 40.99678874015808
Epoch: 45, Steps: 89 | Train Loss: 0.4142788 Vali Loss: 0.5608126 Test Loss: 0.6695836
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 40.73174262046814
Epoch: 46, Steps: 89 | Train Loss: 0.4143670 Vali Loss: 0.5602905 Test Loss: 0.6695319
Validation loss decreased (0.560718 --> 0.560290).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 40.44521236419678
Epoch: 47, Steps: 89 | Train Loss: 0.4144336 Vali Loss: 0.5605680 Test Loss: 0.6694794
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 40.32591891288757
Epoch: 48, Steps: 89 | Train Loss: 0.4142522 Vali Loss: 0.5608119 Test Loss: 0.6694435
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 40.193312644958496
Epoch: 49, Steps: 89 | Train Loss: 0.4141793 Vali Loss: 0.5608326 Test Loss: 0.6694236
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 41.07538938522339
Epoch: 50, Steps: 89 | Train Loss: 0.4142798 Vali Loss: 0.5604266 Test Loss: 0.6693755
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 39.095128774642944
Epoch: 51, Steps: 89 | Train Loss: 0.4141550 Vali Loss: 0.5605221 Test Loss: 0.6693988
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 39.21360230445862
Epoch: 52, Steps: 89 | Train Loss: 0.4140891 Vali Loss: 0.5603554 Test Loss: 0.6693569
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 37.82802867889404
Epoch: 53, Steps: 89 | Train Loss: 0.4141234 Vali Loss: 0.5605419 Test Loss: 0.6693372
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 38.74595785140991
Epoch: 54, Steps: 89 | Train Loss: 0.4140415 Vali Loss: 0.5607185 Test Loss: 0.6692903
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 38.99013948440552
Epoch: 55, Steps: 89 | Train Loss: 0.4141075 Vali Loss: 0.5600215 Test Loss: 0.6692902
Validation loss decreased (0.560290 --> 0.560022).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 36.759572982788086
Epoch: 56, Steps: 89 | Train Loss: 0.4141253 Vali Loss: 0.5598063 Test Loss: 0.6692711
Validation loss decreased (0.560022 --> 0.559806).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 36.98727464675903
Epoch: 57, Steps: 89 | Train Loss: 0.4140406 Vali Loss: 0.5595942 Test Loss: 0.6692575
Validation loss decreased (0.559806 --> 0.559594).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 38.95664167404175
Epoch: 58, Steps: 89 | Train Loss: 0.4140150 Vali Loss: 0.5595792 Test Loss: 0.6692239
Validation loss decreased (0.559594 --> 0.559579).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 38.403048038482666
Epoch: 59, Steps: 89 | Train Loss: 0.4140516 Vali Loss: 0.5601193 Test Loss: 0.6692116
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 38.25216794013977
Epoch: 60, Steps: 89 | Train Loss: 0.4139960 Vali Loss: 0.5595260 Test Loss: 0.6692058
Validation loss decreased (0.559579 --> 0.559526).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 38.86383581161499
Epoch: 61, Steps: 89 | Train Loss: 0.4139268 Vali Loss: 0.5598749 Test Loss: 0.6692058
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 36.51651954650879
Epoch: 62, Steps: 89 | Train Loss: 0.4139969 Vali Loss: 0.5600165 Test Loss: 0.6691979
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 36.46109747886658
Epoch: 63, Steps: 89 | Train Loss: 0.4140507 Vali Loss: 0.5602869 Test Loss: 0.6691795
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 36.57229781150818
Epoch: 64, Steps: 89 | Train Loss: 0.4137815 Vali Loss: 0.5599128 Test Loss: 0.6691923
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 37.38957238197327
Epoch: 65, Steps: 89 | Train Loss: 0.4138707 Vali Loss: 0.5597591 Test Loss: 0.6691737
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 37.91471457481384
Epoch: 66, Steps: 89 | Train Loss: 0.4138795 Vali Loss: 0.5602271 Test Loss: 0.6691761
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 37.519585371017456
Epoch: 67, Steps: 89 | Train Loss: 0.4138654 Vali Loss: 0.5601394 Test Loss: 0.6691789
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 36.493582010269165
Epoch: 68, Steps: 89 | Train Loss: 0.4137982 Vali Loss: 0.5600443 Test Loss: 0.6691467
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 36.65828847885132
Epoch: 69, Steps: 89 | Train Loss: 0.4138596 Vali Loss: 0.5599259 Test Loss: 0.6691470
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 36.45342302322388
Epoch: 70, Steps: 89 | Train Loss: 0.4138772 Vali Loss: 0.5605136 Test Loss: 0.6691476
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j720_H8_FITS_custom_ftM_sl90_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.6680544018745422, mae:0.3969775140285492, rse:0.6683542728424072, corr:[0.24616425 0.26276562 0.26162902 0.2606208  0.25972483 0.2609499
 0.26390952 0.2646419  0.26472735 0.26445457 0.26346418 0.26383385
 0.26268724 0.2622927  0.26194206 0.26286682 0.26302448 0.26349553
 0.26543596 0.26611328 0.26801494 0.26820484 0.26803574 0.26813766
 0.2714443  0.27533025 0.27459908 0.27251354 0.27325043 0.27493358
 0.27661964 0.2778599  0.27787256 0.277647   0.27724305 0.27723765
 0.27628508 0.275754   0.27601096 0.27589434 0.2763215  0.27678964
 0.2774954  0.27781737 0.2782891  0.27781025 0.27744997 0.27667937
 0.27712494 0.27683714 0.2745791  0.27420738 0.27293822 0.26862705
 0.26682982 0.2665838  0.2672804  0.26671743 0.26576832 0.26536644
 0.26375073 0.26361752 0.26278946 0.26291725 0.26297885 0.2632724
 0.26381502 0.2639491  0.26421115 0.26430932 0.26446804 0.26309237
 0.26154813 0.26013285 0.25906396 0.2602137  0.26102018 0.2615202
 0.26229644 0.26390883 0.2647085  0.2644654  0.26340064 0.26229653
 0.26124436 0.26072294 0.25980145 0.2597241  0.25938496 0.2594995
 0.25982127 0.2603227  0.26077458 0.26128685 0.2617218  0.2615923
 0.2614248  0.26115572 0.26111937 0.26091194 0.26091102 0.26142576
 0.26215485 0.26324767 0.26290655 0.2627326  0.2621261  0.2615408
 0.26087147 0.26031652 0.25993165 0.25995904 0.26012248 0.2603859
 0.2608729  0.26122972 0.2618201  0.2622083  0.26283264 0.2629462
 0.26288784 0.26305094 0.26315662 0.26289517 0.26300544 0.26295295
 0.26302794 0.26289788 0.26211318 0.26170963 0.26127496 0.2606702
 0.2601795  0.26006666 0.26014087 0.26039362 0.26108155 0.2610496
 0.26182848 0.2623513  0.26322362 0.263487   0.26383734 0.26283145
 0.2618751  0.26181942 0.26155317 0.26148066 0.2617717  0.261865
 0.26232517 0.26233402 0.2617385  0.26109856 0.2605994  0.2599806
 0.25971183 0.25982127 0.26021558 0.26069477 0.26157638 0.26226082
 0.26358828 0.2646758  0.26606345 0.26624742 0.26601127 0.2645741
 0.26364952 0.26289824 0.260492   0.25946602 0.25999668 0.26179534
 0.26412907 0.26536977 0.26524305 0.26519638 0.26541495 0.26523867
 0.2648368  0.26476908 0.26498535 0.26535192 0.26598692 0.26659188
 0.26766676 0.2685963  0.2694344  0.26939282 0.2689803  0.2684431
 0.27131099 0.27424565 0.27338278 0.27224234 0.27242938 0.2738451
 0.27507496 0.27622855 0.27651006 0.276608   0.27676454 0.27659562
 0.276055   0.27585888 0.27553555 0.27576998 0.2760255  0.27647802
 0.2771003  0.27731627 0.2774624  0.2772112  0.2767041  0.27528536
 0.2751664  0.27361163 0.27157864 0.27119875 0.27002212 0.26716647
 0.264868   0.26577887 0.26628914 0.2661603  0.26550162 0.264735
 0.2639974  0.26326466 0.26282117 0.2628104  0.26272514 0.26290524
 0.2631519  0.26343834 0.26361802 0.2637592  0.26372787 0.2625767
 0.26096302 0.25949088 0.25898644 0.25961676 0.26125845 0.26172608
 0.26283795 0.26505393 0.26566696 0.26572916 0.26464626 0.26384947
 0.26289815 0.26208022 0.26158375 0.2611522  0.26100895 0.26105613
 0.2612661  0.26170394 0.26214144 0.2624743  0.26295573 0.2628013
 0.26244095 0.26232293 0.26211926 0.26222447 0.26251304 0.26268774
 0.26366445 0.2646676  0.2644624  0.2643632  0.26375934 0.26320332
 0.26253575 0.26203898 0.26155213 0.26154244 0.2617295  0.2617119
 0.26213148 0.26255783 0.26304877 0.26330274 0.26389262 0.2638499
 0.26378378 0.26391545 0.26393098 0.26414266 0.26411757 0.26406932
 0.26415274 0.2639528  0.26321858 0.26293334 0.26255545 0.26201773
 0.26168633 0.2615727  0.26152247 0.26185375 0.26242697 0.26256981
 0.26324126 0.2638423  0.26444837 0.26477715 0.26509535 0.26391906
 0.26308432 0.2628671  0.26263708 0.2628378  0.26290154 0.26318276
 0.26362553 0.2637777  0.26316634 0.26277235 0.26226315 0.26159444
 0.26129362 0.26139873 0.26178142 0.26221463 0.26291186 0.2634073
 0.26456356 0.26573145 0.26709867 0.2674993  0.26696876 0.26507455
 0.26375583 0.26264346 0.26033583 0.25903377 0.259418   0.26135558
 0.26375335 0.26517102 0.26503548 0.26511738 0.26521096 0.26521182
 0.26486757 0.26473984 0.26484635 0.26517287 0.26570842 0.26619732
 0.26732203 0.26799107 0.26895046 0.26915053 0.26844928 0.2679493
 0.2705563  0.27432075 0.27399883 0.27239427 0.27255785 0.2739478
 0.27521035 0.2767425  0.2771429  0.2774689  0.27745792 0.27733088
 0.27688202 0.2764419  0.2764201  0.2765549  0.27673492 0.27712104
 0.27772698 0.2778692  0.27812636 0.27780515 0.27696425 0.2756483
 0.2751918  0.27375013 0.2716535  0.27067253 0.26971647 0.2662196
 0.26483002 0.26607493 0.26685977 0.266976   0.2661232  0.26559743
 0.26460722 0.2639502  0.26354617 0.2633138  0.2634811  0.26337415
 0.2637299  0.26385248 0.2639527  0.26397955 0.2638425  0.26273432
 0.2610221  0.25983068 0.25890785 0.25966638 0.26106867 0.26135722
 0.26313174 0.2652277  0.2664369  0.26667574 0.2656192  0.26484442
 0.26366448 0.2630413  0.26234022 0.2620956  0.26182535 0.26154932
 0.26184037 0.26215592 0.26258942 0.26297548 0.26328608 0.26305425
 0.26278922 0.26261222 0.2623934  0.2626256  0.26276454 0.2630612
 0.26383683 0.26485884 0.26495633 0.2648322  0.26433378 0.26369345
 0.26301068 0.2625615  0.26200482 0.26207373 0.26204574 0.26208866
 0.2624135  0.26264867 0.26291928 0.26329502 0.2637914  0.26381332
 0.263896   0.26388142 0.26407662 0.26432288 0.26430508 0.26436055
 0.26426747 0.2644043  0.2638054  0.2635543  0.26303408 0.26245645
 0.2621252  0.26191136 0.26187623 0.2621335  0.2625527  0.26295176
 0.26360628 0.26412615 0.26480243 0.2652223  0.2653085  0.26431367
 0.2635318  0.26335457 0.26331764 0.26332775 0.26359776 0.2638506
 0.26419008 0.2643869  0.26387158 0.26344535 0.2627737  0.26229253
 0.26193282 0.26185217 0.26221478 0.26264974 0.2633502  0.26396847
 0.2652414  0.2662341  0.2675837  0.2679713  0.2672046  0.26545045
 0.2638984  0.2627606  0.26052424 0.25905588 0.2594067  0.2610817
 0.26340124 0.26473853 0.26490474 0.26490018 0.26497912 0.26490724
 0.26454198 0.26433846 0.2644276  0.26459885 0.26500127 0.2656855
 0.2666943  0.26741162 0.26851666 0.2686257  0.26814067 0.26775974
 0.2702783  0.27393505 0.27324775 0.27172726 0.2718896  0.27312997
 0.27478018 0.2762856  0.27675533 0.27693194 0.27701145 0.2770934
 0.27641448 0.27607092 0.27583167 0.2756548  0.27580997 0.27604297
 0.2766375  0.2767682  0.27689242 0.27669027 0.27606106 0.27467832
 0.27417982 0.2732533  0.27117926 0.27061403 0.26902285 0.2652504
 0.26388493 0.26479653 0.26583582 0.2656457  0.26504415 0.26456723
 0.26340863 0.26281652 0.26194334 0.26184788 0.26146013 0.26142502
 0.26177925 0.261835   0.2619331  0.26180068 0.26173827 0.26050642
 0.25920665 0.2579377  0.2572125  0.2582691  0.25924385 0.2602048
 0.26208612 0.26476377 0.26597586 0.26592413 0.2650483  0.2639899
 0.26304755 0.2622809  0.26138029 0.26114687 0.26049381 0.26049408
 0.26064587 0.26098147 0.2611625  0.26148444 0.26184234 0.26164845
 0.26154536 0.26133403 0.2615268  0.2616741  0.2618409  0.2621374
 0.26276392 0.26405814 0.26399103 0.26397023 0.2634083  0.26273677
 0.26218677 0.26140353 0.26092514 0.2607051  0.26058903 0.26084605
 0.26117694 0.26136926 0.26171634 0.2619524  0.26250055 0.26275295
 0.26257232 0.26282215 0.26316985 0.2632211  0.26354277 0.26326048
 0.26313996 0.26300243 0.26265362 0.26232037 0.2616077  0.26122466
 0.26082164 0.26044986 0.2602925  0.26030034 0.26079544 0.261184
 0.26172718 0.262224   0.26306942 0.26327604 0.26370046 0.26283497
 0.26199383 0.2624059  0.2622323  0.26238823 0.2627685  0.26256096
 0.26295102 0.2630037  0.26270846 0.26202887 0.26156828 0.2609036
 0.26042354 0.26043454 0.26051056 0.26084828 0.2615327  0.26213855
 0.26313964 0.2643316  0.26568753 0.26605007 0.265913   0.26402426
 0.26326704 0.2632131  0.2609401  0.2599718  0.25961939 0.2609111
 0.26286173 0.26452252 0.26459095 0.2645493  0.26471442 0.26432323
 0.26407182 0.26362398 0.26331335 0.26371047 0.2639595  0.26468572
 0.26545396 0.26652315 0.26740715 0.26821926 0.26808172 0.26754925
 0.2713234  0.27505562 0.27530077 0.27323475 0.27172306 0.27307826
 0.27401304 0.27658153 0.27691597 0.27714467 0.27719736 0.2762867
 0.27644867 0.2745363  0.27497733 0.2745761  0.27418822 0.27544355
 0.27446926 0.2750663  0.27479675 0.27652737 0.27246156 0.27329838]
