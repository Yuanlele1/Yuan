Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j96_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j96_H8_FITS_custom_ftM_sl90_ll48_pl96_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 12095
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=42, out_features=86, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  398533632.0
params:  3698.0
Trainable parameters:  3698
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 12.089595556259155
Epoch: 1, Steps: 94 | Train Loss: 0.9983614 Vali Loss: 0.9348301 Test Loss: 1.0760032
Validation loss decreased (inf --> 0.934830).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 11.969049215316772
Epoch: 2, Steps: 94 | Train Loss: 0.6294390 Vali Loss: 0.7256830 Test Loss: 0.8411385
Validation loss decreased (0.934830 --> 0.725683).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 11.295342922210693
Epoch: 3, Steps: 94 | Train Loss: 0.5241461 Vali Loss: 0.6556656 Test Loss: 0.7622607
Validation loss decreased (0.725683 --> 0.655666).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 11.490901231765747
Epoch: 4, Steps: 94 | Train Loss: 0.4843473 Vali Loss: 0.6261455 Test Loss: 0.7291240
Validation loss decreased (0.655666 --> 0.626146).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 11.460907459259033
Epoch: 5, Steps: 94 | Train Loss: 0.4666316 Vali Loss: 0.6109797 Test Loss: 0.7126209
Validation loss decreased (0.626146 --> 0.610980).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 11.68611192703247
Epoch: 6, Steps: 94 | Train Loss: 0.4568935 Vali Loss: 0.6016345 Test Loss: 0.7032527
Validation loss decreased (0.610980 --> 0.601635).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 12.202356100082397
Epoch: 7, Steps: 94 | Train Loss: 0.4512060 Vali Loss: 0.5947717 Test Loss: 0.6971871
Validation loss decreased (0.601635 --> 0.594772).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 12.13155484199524
Epoch: 8, Steps: 94 | Train Loss: 0.4473667 Vali Loss: 0.5897856 Test Loss: 0.6931170
Validation loss decreased (0.594772 --> 0.589786).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 11.652243852615356
Epoch: 9, Steps: 94 | Train Loss: 0.4446906 Vali Loss: 0.5889477 Test Loss: 0.6899697
Validation loss decreased (0.589786 --> 0.588948).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 12.245735168457031
Epoch: 10, Steps: 94 | Train Loss: 0.4426106 Vali Loss: 0.5853343 Test Loss: 0.6877423
Validation loss decreased (0.588948 --> 0.585334).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 11.881255626678467
Epoch: 11, Steps: 94 | Train Loss: 0.4411403 Vali Loss: 0.5858403 Test Loss: 0.6859767
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 12.025029420852661
Epoch: 12, Steps: 94 | Train Loss: 0.4396127 Vali Loss: 0.5830587 Test Loss: 0.6844886
Validation loss decreased (0.585334 --> 0.583059).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 12.394188404083252
Epoch: 13, Steps: 94 | Train Loss: 0.4388392 Vali Loss: 0.5799050 Test Loss: 0.6833730
Validation loss decreased (0.583059 --> 0.579905).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 12.374119520187378
Epoch: 14, Steps: 94 | Train Loss: 0.4380399 Vali Loss: 0.5805802 Test Loss: 0.6823690
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 12.22178316116333
Epoch: 15, Steps: 94 | Train Loss: 0.4373744 Vali Loss: 0.5790521 Test Loss: 0.6816472
Validation loss decreased (0.579905 --> 0.579052).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 12.057737350463867
Epoch: 16, Steps: 94 | Train Loss: 0.4368739 Vali Loss: 0.5792581 Test Loss: 0.6810001
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 12.21270227432251
Epoch: 17, Steps: 94 | Train Loss: 0.4363820 Vali Loss: 0.5788072 Test Loss: 0.6804851
Validation loss decreased (0.579052 --> 0.578807).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 12.66646122932434
Epoch: 18, Steps: 94 | Train Loss: 0.4358888 Vali Loss: 0.5768782 Test Loss: 0.6799936
Validation loss decreased (0.578807 --> 0.576878).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 12.019997596740723
Epoch: 19, Steps: 94 | Train Loss: 0.4355180 Vali Loss: 0.5760056 Test Loss: 0.6796121
Validation loss decreased (0.576878 --> 0.576006).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 12.330985069274902
Epoch: 20, Steps: 94 | Train Loss: 0.4353501 Vali Loss: 0.5767989 Test Loss: 0.6792964
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 13.40153169631958
Epoch: 21, Steps: 94 | Train Loss: 0.4350491 Vali Loss: 0.5755476 Test Loss: 0.6790686
Validation loss decreased (0.576006 --> 0.575548).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 12.750833988189697
Epoch: 22, Steps: 94 | Train Loss: 0.4350172 Vali Loss: 0.5748849 Test Loss: 0.6788626
Validation loss decreased (0.575548 --> 0.574885).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 12.143154621124268
Epoch: 23, Steps: 94 | Train Loss: 0.4348122 Vali Loss: 0.5772862 Test Loss: 0.6787097
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 13.307125091552734
Epoch: 24, Steps: 94 | Train Loss: 0.4346379 Vali Loss: 0.5749388 Test Loss: 0.6785501
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 12.584082841873169
Epoch: 25, Steps: 94 | Train Loss: 0.4344392 Vali Loss: 0.5745758 Test Loss: 0.6784648
Validation loss decreased (0.574885 --> 0.574576).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 13.273532629013062
Epoch: 26, Steps: 94 | Train Loss: 0.4344862 Vali Loss: 0.5743115 Test Loss: 0.6782165
Validation loss decreased (0.574576 --> 0.574311).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 13.198262929916382
Epoch: 27, Steps: 94 | Train Loss: 0.4341877 Vali Loss: 0.5745812 Test Loss: 0.6781730
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 12.541138648986816
Epoch: 28, Steps: 94 | Train Loss: 0.4342637 Vali Loss: 0.5757420 Test Loss: 0.6781360
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 13.29126524925232
Epoch: 29, Steps: 94 | Train Loss: 0.4341096 Vali Loss: 0.5746672 Test Loss: 0.6779749
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 12.207545518875122
Epoch: 30, Steps: 94 | Train Loss: 0.4340041 Vali Loss: 0.5747333 Test Loss: 0.6779579
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 12.493396043777466
Epoch: 31, Steps: 94 | Train Loss: 0.4338315 Vali Loss: 0.5760018 Test Loss: 0.6779068
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 12.853116512298584
Epoch: 32, Steps: 94 | Train Loss: 0.4338609 Vali Loss: 0.5748873 Test Loss: 0.6778162
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 12.269694089889526
Epoch: 33, Steps: 94 | Train Loss: 0.4338928 Vali Loss: 0.5767283 Test Loss: 0.6778060
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 13.476456880569458
Epoch: 34, Steps: 94 | Train Loss: 0.4338703 Vali Loss: 0.5744500 Test Loss: 0.6777131
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 12.659040451049805
Epoch: 35, Steps: 94 | Train Loss: 0.4337545 Vali Loss: 0.5748235 Test Loss: 0.6776810
EarlyStopping counter: 9 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 13.25582766532898
Epoch: 36, Steps: 94 | Train Loss: 0.4337987 Vali Loss: 0.5757888 Test Loss: 0.6776313
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j96_H8_FITS_custom_ftM_sl90_ll48_pl96_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.6828761696815491, mae:0.40544402599334717, rse:0.6842651963233948, corr:[0.27099547 0.28564864 0.28615916 0.28410947 0.28569597 0.28634647
 0.2892709  0.28936616 0.28903955 0.28834486 0.2875548  0.2875409
 0.2858061  0.28660116 0.28534716 0.28705642 0.28683582 0.2887379
 0.2903753  0.29185614 0.29407454 0.2943154  0.29463327 0.2938145
 0.29859346 0.30254975 0.30147067 0.30084455 0.30061004 0.30228668
 0.30291647 0.30435735 0.30471176 0.3041692  0.30423275 0.30317163
 0.30248043 0.302006   0.30140933 0.3020498  0.30176017 0.30298486
 0.30389377 0.3043445  0.30499196 0.30458453 0.30462155 0.3028379
 0.30294335 0.30138794 0.29950795 0.29896855 0.29719713 0.29329294
 0.29025072 0.2912701  0.29180756 0.29123467 0.29012105 0.28886324
 0.2876133  0.2868469  0.28634456 0.28626215 0.2863405  0.28721243
 0.28781658 0.2883785  0.2883962  0.2881405  0.28870815 0.2868662
 0.28501514 0.28341633 0.2830833  0.28367093 0.28518084 0.2849264
 0.28529784 0.28743276 0.28812918 0.28764656 0.28658023 0.28529873
 0.28466246 0.28415602 0.28384808 0.28365266 0.28339493 0.28400245
 0.28418154 0.28481242 0.28336632 0.28437328 0.2829407  0.28602925]
