Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5469963264.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8881687
	speed: 0.9178s/iter; left time: 12023.5972s
Epoch: 1 cost time: 123.9371645450592
Epoch: 1, Steps: 132 | Train Loss: 1.0254582 Vali Loss: 0.7407526 Test Loss: 0.8706830
Validation loss decreased (inf --> 0.740753).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7258812
	speed: 2.2203s/iter; left time: 28794.5500s
Epoch: 2 cost time: 129.2791247367859
Epoch: 2, Steps: 132 | Train Loss: 0.7543619 Vali Loss: 0.6535944 Test Loss: 0.7718810
Validation loss decreased (0.740753 --> 0.653594).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6249641
	speed: 2.1789s/iter; left time: 27970.1643s
Epoch: 3 cost time: 133.79432201385498
Epoch: 3, Steps: 132 | Train Loss: 0.6610382 Vali Loss: 0.5887181 Test Loss: 0.6985050
Validation loss decreased (0.653594 --> 0.588718).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5791083
	speed: 2.3212s/iter; left time: 29490.5690s
Epoch: 4 cost time: 137.82806396484375
Epoch: 4, Steps: 132 | Train Loss: 0.5871813 Vali Loss: 0.5359467 Test Loss: 0.6387386
Validation loss decreased (0.588718 --> 0.535947).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5110307
	speed: 2.1536s/iter; left time: 27077.6213s
Epoch: 5 cost time: 134.9244167804718
Epoch: 5, Steps: 132 | Train Loss: 0.5256124 Vali Loss: 0.4878843 Test Loss: 0.5841046
Validation loss decreased (0.535947 --> 0.487884).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4695165
	speed: 2.3997s/iter; left time: 29854.1946s
Epoch: 6 cost time: 137.77203345298767
Epoch: 6, Steps: 132 | Train Loss: 0.4738088 Vali Loss: 0.4513641 Test Loss: 0.5426873
Validation loss decreased (0.487884 --> 0.451364).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4123043
	speed: 2.2396s/iter; left time: 27567.0333s
Epoch: 7 cost time: 138.83436822891235
Epoch: 7, Steps: 132 | Train Loss: 0.4296723 Vali Loss: 0.4163552 Test Loss: 0.5025160
Validation loss decreased (0.451364 --> 0.416355).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3903944
	speed: 2.5376s/iter; left time: 30900.8800s
Epoch: 8 cost time: 143.68594479560852
Epoch: 8, Steps: 132 | Train Loss: 0.3917566 Vali Loss: 0.3864929 Test Loss: 0.4679616
Validation loss decreased (0.416355 --> 0.386493).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3538339
	speed: 2.1458s/iter; left time: 25846.4161s
Epoch: 9 cost time: 128.13617205619812
Epoch: 9, Steps: 132 | Train Loss: 0.3591270 Vali Loss: 0.3623885 Test Loss: 0.4399734
Validation loss decreased (0.386493 --> 0.362389).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3291632
	speed: 2.0254s/iter; left time: 24128.8675s
Epoch: 10 cost time: 119.44889044761658
Epoch: 10, Steps: 132 | Train Loss: 0.3308020 Vali Loss: 0.3383028 Test Loss: 0.4121932
Validation loss decreased (0.362389 --> 0.338303).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3050695
	speed: 1.9130s/iter; left time: 22536.5401s
Epoch: 11 cost time: 114.97739338874817
Epoch: 11, Steps: 132 | Train Loss: 0.3062846 Vali Loss: 0.3199444 Test Loss: 0.3906647
Validation loss decreased (0.338303 --> 0.319944).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2730496
	speed: 1.9138s/iter; left time: 22294.2634s
Epoch: 12 cost time: 118.67569327354431
Epoch: 12, Steps: 132 | Train Loss: 0.2848707 Vali Loss: 0.3037711 Test Loss: 0.3716141
Validation loss decreased (0.319944 --> 0.303771).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2612461
	speed: 1.8800s/iter; left time: 21651.5781s
Epoch: 13 cost time: 109.78531455993652
Epoch: 13, Steps: 132 | Train Loss: 0.2661019 Vali Loss: 0.2888566 Test Loss: 0.3544261
Validation loss decreased (0.303771 --> 0.288857).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2420403
	speed: 1.8172s/iter; left time: 20689.1464s
Epoch: 14 cost time: 108.5628547668457
Epoch: 14, Steps: 132 | Train Loss: 0.2496354 Vali Loss: 0.2753852 Test Loss: 0.3380744
Validation loss decreased (0.288857 --> 0.275385).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2378599
	speed: 1.9465s/iter; left time: 21904.0314s
Epoch: 15 cost time: 119.64183115959167
Epoch: 15, Steps: 132 | Train Loss: 0.2352632 Vali Loss: 0.2641757 Test Loss: 0.3247435
Validation loss decreased (0.275385 --> 0.264176).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2181494
	speed: 2.0742s/iter; left time: 23067.0316s
Epoch: 16 cost time: 143.1900246143341
Epoch: 16, Steps: 132 | Train Loss: 0.2225193 Vali Loss: 0.2549800 Test Loss: 0.3132818
Validation loss decreased (0.264176 --> 0.254980).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2095164
	speed: 2.4759s/iter; left time: 27207.3380s
Epoch: 17 cost time: 127.64229822158813
Epoch: 17, Steps: 132 | Train Loss: 0.2112654 Vali Loss: 0.2458272 Test Loss: 0.3023930
Validation loss decreased (0.254980 --> 0.245827).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2030916
	speed: 1.9739s/iter; left time: 21430.7667s
Epoch: 18 cost time: 117.15112352371216
Epoch: 18, Steps: 132 | Train Loss: 0.2013856 Vali Loss: 0.2385864 Test Loss: 0.2934521
Validation loss decreased (0.245827 --> 0.238586).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1853322
	speed: 1.9944s/iter; left time: 21389.9832s
Epoch: 19 cost time: 122.89584469795227
Epoch: 19, Steps: 132 | Train Loss: 0.1925538 Vali Loss: 0.2316930 Test Loss: 0.2852114
Validation loss decreased (0.238586 --> 0.231693).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1812026
	speed: 2.0367s/iter; left time: 21574.8737s
Epoch: 20 cost time: 123.8820436000824
Epoch: 20, Steps: 132 | Train Loss: 0.1846930 Vali Loss: 0.2254851 Test Loss: 0.2773499
Validation loss decreased (0.231693 --> 0.225485).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1728708
	speed: 2.2256s/iter; left time: 23281.6869s
Epoch: 21 cost time: 133.3758454322815
Epoch: 21, Steps: 132 | Train Loss: 0.1778003 Vali Loss: 0.2207113 Test Loss: 0.2714752
Validation loss decreased (0.225485 --> 0.220711).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1649991
	speed: 2.1933s/iter; left time: 22654.4712s
Epoch: 22 cost time: 134.539044380188
Epoch: 22, Steps: 132 | Train Loss: 0.1715488 Vali Loss: 0.2154440 Test Loss: 0.2646876
Validation loss decreased (0.220711 --> 0.215444).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1603063
	speed: 2.3662s/iter; left time: 24127.9449s
Epoch: 23 cost time: 149.21158742904663
Epoch: 23, Steps: 132 | Train Loss: 0.1661137 Vali Loss: 0.2114139 Test Loss: 0.2598076
Validation loss decreased (0.215444 --> 0.211414).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1618500
	speed: 2.4136s/iter; left time: 24292.5266s
Epoch: 24 cost time: 150.07721066474915
Epoch: 24, Steps: 132 | Train Loss: 0.1611429 Vali Loss: 0.2076335 Test Loss: 0.2549025
Validation loss decreased (0.211414 --> 0.207634).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1592322
	speed: 2.3568s/iter; left time: 23410.3424s
Epoch: 25 cost time: 134.67961740493774
Epoch: 25, Steps: 132 | Train Loss: 0.1567616 Vali Loss: 0.2044354 Test Loss: 0.2505450
Validation loss decreased (0.207634 --> 0.204435).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1491946
	speed: 2.2497s/iter; left time: 22049.6700s
Epoch: 26 cost time: 138.45990347862244
Epoch: 26, Steps: 132 | Train Loss: 0.1527880 Vali Loss: 0.2014169 Test Loss: 0.2467691
Validation loss decreased (0.204435 --> 0.201417).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1518914
	speed: 2.1904s/iter; left time: 21179.1109s
Epoch: 27 cost time: 127.87476420402527
Epoch: 27, Steps: 132 | Train Loss: 0.1491921 Vali Loss: 0.1989949 Test Loss: 0.2434583
Validation loss decreased (0.201417 --> 0.198995).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1406262
	speed: 2.1998s/iter; left time: 20979.4999s
Epoch: 28 cost time: 132.97047328948975
Epoch: 28, Steps: 132 | Train Loss: 0.1460518 Vali Loss: 0.1966628 Test Loss: 0.2401824
Validation loss decreased (0.198995 --> 0.196663).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1445037
	speed: 2.1998s/iter; left time: 20689.1999s
Epoch: 29 cost time: 136.63705468177795
Epoch: 29, Steps: 132 | Train Loss: 0.1432522 Vali Loss: 0.1945278 Test Loss: 0.2375702
Validation loss decreased (0.196663 --> 0.194528).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1407993
	speed: 2.3068s/iter; left time: 21390.7420s
Epoch: 30 cost time: 142.53615069389343
Epoch: 30, Steps: 132 | Train Loss: 0.1406287 Vali Loss: 0.1925607 Test Loss: 0.2348816
Validation loss decreased (0.194528 --> 0.192561).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1389445
	speed: 2.3574s/iter; left time: 21548.9397s
Epoch: 31 cost time: 137.31641745567322
Epoch: 31, Steps: 132 | Train Loss: 0.1383546 Vali Loss: 0.1911904 Test Loss: 0.2328951
Validation loss decreased (0.192561 --> 0.191190).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1278161
	speed: 2.1111s/iter; left time: 19018.5521s
Epoch: 32 cost time: 124.48114514350891
Epoch: 32, Steps: 132 | Train Loss: 0.1362294 Vali Loss: 0.1900443 Test Loss: 0.2308382
Validation loss decreased (0.191190 --> 0.190044).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1367583
	speed: 2.1648s/iter; left time: 19217.2250s
Epoch: 33 cost time: 128.8449149131775
Epoch: 33, Steps: 132 | Train Loss: 0.1343411 Vali Loss: 0.1883125 Test Loss: 0.2289298
Validation loss decreased (0.190044 --> 0.188312).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1333324
	speed: 2.0962s/iter; left time: 18331.4673s
Epoch: 34 cost time: 130.56388998031616
Epoch: 34, Steps: 132 | Train Loss: 0.1326875 Vali Loss: 0.1871090 Test Loss: 0.2269487
Validation loss decreased (0.188312 --> 0.187109).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1350367
	speed: 2.1348s/iter; left time: 18386.7927s
Epoch: 35 cost time: 128.47037816047668
Epoch: 35, Steps: 132 | Train Loss: 0.1311833 Vali Loss: 0.1859009 Test Loss: 0.2257189
Validation loss decreased (0.187109 --> 0.185901).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1297898
	speed: 2.2593s/iter; left time: 19161.1001s
Epoch: 36 cost time: 136.42293858528137
Epoch: 36, Steps: 132 | Train Loss: 0.1297580 Vali Loss: 0.1853526 Test Loss: 0.2243333
Validation loss decreased (0.185901 --> 0.185353).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1225378
	speed: 3.2655s/iter; left time: 27263.9697s
Epoch: 37 cost time: 247.09686732292175
Epoch: 37, Steps: 132 | Train Loss: 0.1285503 Vali Loss: 0.1842875 Test Loss: 0.2231695
Validation loss decreased (0.185353 --> 0.184288).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1295836
	speed: 3.6824s/iter; left time: 30257.9229s
Epoch: 38 cost time: 170.65302991867065
Epoch: 38, Steps: 132 | Train Loss: 0.1273678 Vali Loss: 0.1835368 Test Loss: 0.2219808
Validation loss decreased (0.184288 --> 0.183537).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1194746
	speed: 2.7349s/iter; left time: 22111.9646s
Epoch: 39 cost time: 150.72833561897278
Epoch: 39, Steps: 132 | Train Loss: 0.1263671 Vali Loss: 0.1827278 Test Loss: 0.2209037
Validation loss decreased (0.183537 --> 0.182728).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1275228
	speed: 2.0603s/iter; left time: 16385.4293s
Epoch: 40 cost time: 120.35551333427429
Epoch: 40, Steps: 132 | Train Loss: 0.1254120 Vali Loss: 0.1822606 Test Loss: 0.2199012
Validation loss decreased (0.182728 --> 0.182261).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1218195
	speed: 2.0795s/iter; left time: 16263.9671s
Epoch: 41 cost time: 128.7357897758484
Epoch: 41, Steps: 132 | Train Loss: 0.1245628 Vali Loss: 0.1818210 Test Loss: 0.2190411
Validation loss decreased (0.182261 --> 0.181821).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1190527
	speed: 2.2490s/iter; left time: 17292.4612s
Epoch: 42 cost time: 142.62314748764038
Epoch: 42, Steps: 132 | Train Loss: 0.1238096 Vali Loss: 0.1809514 Test Loss: 0.2182131
Validation loss decreased (0.181821 --> 0.180951).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1241301
	speed: 2.3167s/iter; left time: 17507.6103s
Epoch: 43 cost time: 140.42633962631226
Epoch: 43, Steps: 132 | Train Loss: 0.1230497 Vali Loss: 0.1804691 Test Loss: 0.2174840
Validation loss decreased (0.180951 --> 0.180469).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1159543
	speed: 2.2841s/iter; left time: 16959.1694s
Epoch: 44 cost time: 138.17221593856812
Epoch: 44, Steps: 132 | Train Loss: 0.1224515 Vali Loss: 0.1804468 Test Loss: 0.2168271
Validation loss decreased (0.180469 --> 0.180447).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1251309
	speed: 2.3059s/iter; left time: 16817.1241s
Epoch: 45 cost time: 135.01597785949707
Epoch: 45, Steps: 132 | Train Loss: 0.1219271 Vali Loss: 0.1796863 Test Loss: 0.2163224
Validation loss decreased (0.180447 --> 0.179686).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1189718
	speed: 1.9560s/iter; left time: 14006.8766s
Epoch: 46 cost time: 117.30037903785706
Epoch: 46, Steps: 132 | Train Loss: 0.1213992 Vali Loss: 0.1796722 Test Loss: 0.2157005
Validation loss decreased (0.179686 --> 0.179672).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1249831
	speed: 1.9301s/iter; left time: 13566.8535s
Epoch: 47 cost time: 119.11342740058899
Epoch: 47, Steps: 132 | Train Loss: 0.1208797 Vali Loss: 0.1792669 Test Loss: 0.2152129
Validation loss decreased (0.179672 --> 0.179267).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1165605
	speed: 2.0512s/iter; left time: 14147.2801s
Epoch: 48 cost time: 125.74791526794434
Epoch: 48, Steps: 132 | Train Loss: 0.1204357 Vali Loss: 0.1791356 Test Loss: 0.2147259
Validation loss decreased (0.179267 --> 0.179136).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1191932
	speed: 2.0956s/iter; left time: 14176.6000s
Epoch: 49 cost time: 136.0371217727661
Epoch: 49, Steps: 132 | Train Loss: 0.1200625 Vali Loss: 0.1792473 Test Loss: 0.2143320
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1150119
	speed: 1.8320s/iter; left time: 12151.3422s
Epoch: 50 cost time: 105.30782318115234
Epoch: 50, Steps: 132 | Train Loss: 0.1196505 Vali Loss: 0.1786514 Test Loss: 0.2139045
Validation loss decreased (0.179136 --> 0.178651).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.1148527
	speed: 1.7801s/iter; left time: 11572.5910s
Epoch: 51 cost time: 107.1935338973999
Epoch: 51, Steps: 132 | Train Loss: 0.1193398 Vali Loss: 0.1786691 Test Loss: 0.2135655
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.1169391
	speed: 1.9571s/iter; left time: 12464.4761s
Epoch: 52 cost time: 121.1048948764801
Epoch: 52, Steps: 132 | Train Loss: 0.1189758 Vali Loss: 0.1784457 Test Loss: 0.2132753
Validation loss decreased (0.178651 --> 0.178446).  Saving model ...
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.1185064
	speed: 1.9584s/iter; left time: 12214.5884s
Epoch: 53 cost time: 112.27505874633789
Epoch: 53, Steps: 132 | Train Loss: 0.1187539 Vali Loss: 0.1782804 Test Loss: 0.2129547
Validation loss decreased (0.178446 --> 0.178280).  Saving model ...
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.1145115
	speed: 1.8918s/iter; left time: 11549.4012s
Epoch: 54 cost time: 119.92172908782959
Epoch: 54, Steps: 132 | Train Loss: 0.1184893 Vali Loss: 0.1782725 Test Loss: 0.2126721
Validation loss decreased (0.178280 --> 0.178272).  Saving model ...
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.1177068
	speed: 2.0194s/iter; left time: 12061.9356s
Epoch: 55 cost time: 123.66758489608765
Epoch: 55, Steps: 132 | Train Loss: 0.1182553 Vali Loss: 0.1780522 Test Loss: 0.2124000
Validation loss decreased (0.178272 --> 0.178052).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.1127899
	speed: 2.0411s/iter; left time: 11921.8059s
Epoch: 56 cost time: 116.75806522369385
Epoch: 56, Steps: 132 | Train Loss: 0.1180734 Vali Loss: 0.1777215 Test Loss: 0.2121644
Validation loss decreased (0.178052 --> 0.177722).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.1142574
	speed: 1.7733s/iter; left time: 10123.6254s
Epoch: 57 cost time: 103.2367308139801
Epoch: 57, Steps: 132 | Train Loss: 0.1178402 Vali Loss: 0.1773985 Test Loss: 0.2119425
Validation loss decreased (0.177722 --> 0.177399).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.1160944
	speed: 1.8483s/iter; left time: 10308.0362s
Epoch: 58 cost time: 114.95361113548279
Epoch: 58, Steps: 132 | Train Loss: 0.1177146 Vali Loss: 0.1779340 Test Loss: 0.2117315
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.1150903
	speed: 1.8296s/iter; left time: 9962.3684s
Epoch: 59 cost time: 112.27408957481384
Epoch: 59, Steps: 132 | Train Loss: 0.1175180 Vali Loss: 0.1776101 Test Loss: 0.2115589
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.1185280
	speed: 1.6780s/iter; left time: 8915.3480s
Epoch: 60 cost time: 98.32410025596619
Epoch: 60, Steps: 132 | Train Loss: 0.1173790 Vali Loss: 0.1776634 Test Loss: 0.2113853
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.1168299
	speed: 2.1174s/iter; left time: 10970.0820s
Epoch: 61 cost time: 136.94731783866882
Epoch: 61, Steps: 132 | Train Loss: 0.1172197 Vali Loss: 0.1776039 Test Loss: 0.2112177
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.1166214
	speed: 2.4987s/iter; left time: 12615.8458s
Epoch: 62 cost time: 150.42620825767517
Epoch: 62, Steps: 132 | Train Loss: 0.1170929 Vali Loss: 0.1777980 Test Loss: 0.2110639
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.1160164
	speed: 2.5382s/iter; left time: 12480.5412s
Epoch: 63 cost time: 158.06890106201172
Epoch: 63, Steps: 132 | Train Loss: 0.1169750 Vali Loss: 0.1773567 Test Loss: 0.2109244
Validation loss decreased (0.177399 --> 0.177357).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.1179429
	speed: 2.4062s/iter; left time: 11513.8595s
Epoch: 64 cost time: 140.6833484172821
Epoch: 64, Steps: 132 | Train Loss: 0.1168444 Vali Loss: 0.1775319 Test Loss: 0.2107995
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.1136822
	speed: 2.5511s/iter; left time: 11870.1028s
Epoch: 65 cost time: 161.09495449066162
Epoch: 65, Steps: 132 | Train Loss: 0.1167842 Vali Loss: 0.1774938 Test Loss: 0.2106916
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.1216366
	speed: 2.5117s/iter; left time: 11355.1874s
Epoch: 66 cost time: 147.45493292808533
Epoch: 66, Steps: 132 | Train Loss: 0.1166749 Vali Loss: 0.1774059 Test Loss: 0.2105816
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.1125267
	speed: 2.5472s/iter; left time: 11179.8048s
Epoch: 67 cost time: 158.8649342060089
Epoch: 67, Steps: 132 | Train Loss: 0.1166012 Vali Loss: 0.1771055 Test Loss: 0.2104917
Validation loss decreased (0.177357 --> 0.177106).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.1175776
	speed: 2.5015s/iter; left time: 10648.9969s
Epoch: 68 cost time: 144.59354376792908
Epoch: 68, Steps: 132 | Train Loss: 0.1165039 Vali Loss: 0.1773385 Test Loss: 0.2104001
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.1170312
	speed: 2.5932s/iter; left time: 10696.8656s
Epoch: 69 cost time: 156.97334027290344
Epoch: 69, Steps: 132 | Train Loss: 0.1164351 Vali Loss: 0.1772796 Test Loss: 0.2102988
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.1254370
	speed: 2.4347s/iter; left time: 9721.5844s
Epoch: 70 cost time: 139.58052015304565
Epoch: 70, Steps: 132 | Train Loss: 0.1163398 Vali Loss: 0.1774166 Test Loss: 0.2102243
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.1166557
	speed: 2.4714s/iter; left time: 9541.9436s
Epoch: 71 cost time: 141.0931215286255
Epoch: 71, Steps: 132 | Train Loss: 0.1163248 Vali Loss: 0.1772490 Test Loss: 0.2101457
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.1181550
	speed: 2.5397s/iter; left time: 9470.4090s
Epoch: 72 cost time: 153.99965119361877
Epoch: 72, Steps: 132 | Train Loss: 0.1162620 Vali Loss: 0.1773031 Test Loss: 0.2100708
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.1186469
	speed: 2.5946s/iter; left time: 9332.6853s
Epoch: 73 cost time: 155.89413356781006
Epoch: 73, Steps: 132 | Train Loss: 0.1162195 Vali Loss: 0.1773272 Test Loss: 0.2100122
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.1065822
	speed: 2.4971s/iter; left time: 8652.4450s
Epoch: 74 cost time: 137.44928669929504
Epoch: 74, Steps: 132 | Train Loss: 0.1161331 Vali Loss: 0.1771243 Test Loss: 0.2099472
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.1175983
	speed: 2.1303s/iter; left time: 7100.2835s
Epoch: 75 cost time: 138.13834238052368
Epoch: 75, Steps: 132 | Train Loss: 0.1161043 Vali Loss: 0.1771531 Test Loss: 0.2098865
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.1178450
	speed: 2.6174s/iter; left time: 8378.4248s
Epoch: 76 cost time: 151.99684262275696
Epoch: 76, Steps: 132 | Train Loss: 0.1160789 Vali Loss: 0.1774634 Test Loss: 0.2098306
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.1126727
	speed: 2.0419s/iter; left time: 6266.4621s
Epoch: 77 cost time: 122.28222632408142
Epoch: 77, Steps: 132 | Train Loss: 0.1160521 Vali Loss: 0.1771757 Test Loss: 0.2097825
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.1119093
	speed: 1.9648s/iter; left time: 5770.7333s
Epoch: 78 cost time: 119.34084606170654
Epoch: 78, Steps: 132 | Train Loss: 0.1159309 Vali Loss: 0.1775081 Test Loss: 0.2097377
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.1201666
	speed: 2.0442s/iter; left time: 5734.0865s
Epoch: 79 cost time: 127.05479788780212
Epoch: 79, Steps: 132 | Train Loss: 0.1159622 Vali Loss: 0.1772275 Test Loss: 0.2096969
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.1142571
	speed: 1.9548s/iter; left time: 5225.0717s
Epoch: 80 cost time: 114.8413507938385
Epoch: 80, Steps: 132 | Train Loss: 0.1159371 Vali Loss: 0.1769500 Test Loss: 0.2096574
Validation loss decreased (0.177106 --> 0.176950).  Saving model ...
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.1129018
	speed: 1.8841s/iter; left time: 4787.6161s
Epoch: 81 cost time: 116.95897650718689
Epoch: 81, Steps: 132 | Train Loss: 0.1158961 Vali Loss: 0.1771645 Test Loss: 0.2096188
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.1179891
	speed: 2.2635s/iter; left time: 5452.8910s
Epoch: 82 cost time: 139.96174430847168
Epoch: 82, Steps: 132 | Train Loss: 0.1158734 Vali Loss: 0.1772606 Test Loss: 0.2095809
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.1182069
	speed: 2.3302s/iter; left time: 5305.8776s
Epoch: 83 cost time: 134.5280101299286
Epoch: 83, Steps: 132 | Train Loss: 0.1158580 Vali Loss: 0.1770215 Test Loss: 0.2095462
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.1153668
	speed: 1.9794s/iter; left time: 4245.8706s
Epoch: 84 cost time: 119.09576511383057
Epoch: 84, Steps: 132 | Train Loss: 0.1158283 Vali Loss: 0.1771743 Test Loss: 0.2095150
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.1114804
	speed: 2.2239s/iter; left time: 4476.6161s
Epoch: 85 cost time: 138.45219492912292
Epoch: 85, Steps: 132 | Train Loss: 0.1157780 Vali Loss: 0.1771435 Test Loss: 0.2094874
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.1141135
	speed: 2.2608s/iter; left time: 4252.5383s
Epoch: 86 cost time: 131.37822246551514
Epoch: 86, Steps: 132 | Train Loss: 0.1157591 Vali Loss: 0.1770810 Test Loss: 0.2094579
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.1132649
	speed: 2.1653s/iter; left time: 3787.0369s
Epoch: 87 cost time: 126.94063830375671
Epoch: 87, Steps: 132 | Train Loss: 0.1157849 Vali Loss: 0.1772919 Test Loss: 0.2094309
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.1199008
	speed: 2.0897s/iter; left time: 3379.0785s
Epoch: 88 cost time: 125.23873543739319
Epoch: 88, Steps: 132 | Train Loss: 0.1157652 Vali Loss: 0.1771326 Test Loss: 0.2094084
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.766650946003179e-06
	iters: 100, epoch: 89 | loss: 0.1102658
	speed: 2.1031s/iter; left time: 3123.1109s
Epoch: 89 cost time: 124.76537609100342
Epoch: 89, Steps: 132 | Train Loss: 0.1157366 Vali Loss: 0.1770824 Test Loss: 0.2093852
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.47831839870302e-06
	iters: 100, epoch: 90 | loss: 0.1131118
	speed: 2.0960s/iter; left time: 2835.8544s
Epoch: 90 cost time: 124.06006264686584
Epoch: 90, Steps: 132 | Train Loss: 0.1157048 Vali Loss: 0.1774520 Test Loss: 0.2093628
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.204402478767869e-06
	iters: 100, epoch: 91 | loss: 0.1149983
	speed: 1.9241s/iter; left time: 2349.2728s
Epoch: 91 cost time: 106.29048991203308
Epoch: 91, Steps: 132 | Train Loss: 0.1156866 Vali Loss: 0.1773852 Test Loss: 0.2093417
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.944182354829475e-06
	iters: 100, epoch: 92 | loss: 0.1135718
	speed: 1.7601s/iter; left time: 1916.7532s
Epoch: 92 cost time: 107.19991254806519
Epoch: 92, Steps: 132 | Train Loss: 0.1156664 Vali Loss: 0.1772616 Test Loss: 0.2093249
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.696973237088e-06
	iters: 100, epoch: 93 | loss: 0.1120832
	speed: 1.7647s/iter; left time: 1688.8550s
Epoch: 93 cost time: 105.68125462532043
Epoch: 93, Steps: 132 | Train Loss: 0.1156537 Vali Loss: 0.1772645 Test Loss: 0.2093055
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.462124575233601e-06
	iters: 100, epoch: 94 | loss: 0.1152657
	speed: 1.7144s/iter; left time: 1414.3564s
Epoch: 94 cost time: 101.62977123260498
Epoch: 94, Steps: 132 | Train Loss: 0.1156065 Vali Loss: 0.1775240 Test Loss: 0.2092887
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.239018346471921e-06
	iters: 100, epoch: 95 | loss: 0.1166311
	speed: 1.6800s/iter; left time: 1164.2702s
Epoch: 95 cost time: 97.50742483139038
Epoch: 95, Steps: 132 | Train Loss: 0.1156622 Vali Loss: 0.1770336 Test Loss: 0.2092728
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.027067429148324e-06
	iters: 100, epoch: 96 | loss: 0.1127055
	speed: 1.5889s/iter; left time: 891.3739s
Epoch: 96 cost time: 94.75296425819397
Epoch: 96, Steps: 132 | Train Loss: 0.1156255 Vali Loss: 0.1773049 Test Loss: 0.2092578
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.825714057690908e-06
	iters: 100, epoch: 97 | loss: 0.1179172
	speed: 1.6261s/iter; left time: 697.6015s
Epoch: 97 cost time: 96.94241714477539
Epoch: 97, Steps: 132 | Train Loss: 0.1156326 Vali Loss: 0.1772820 Test Loss: 0.2092438
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.6344283548063623e-06
	iters: 100, epoch: 98 | loss: 0.1176750
	speed: 1.6411s/iter; left time: 487.4057s
Epoch: 98 cost time: 103.27253937721252
Epoch: 98, Steps: 132 | Train Loss: 0.1155815 Vali Loss: 0.1773315 Test Loss: 0.2092305
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.452706937066044e-06
	iters: 100, epoch: 99 | loss: 0.1148737
	speed: 1.6632s/iter; left time: 274.4335s
Epoch: 99 cost time: 101.6315643787384
Epoch: 99, Steps: 132 | Train Loss: 0.1156100 Vali Loss: 0.1770285 Test Loss: 0.2092185
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.2800715902127414e-06
	iters: 100, epoch: 100 | loss: 0.1222168
	speed: 1.6580s/iter; left time: 54.7126s
Epoch: 100 cost time: 101.92260098457336
Epoch: 100, Steps: 132 | Train Loss: 0.1156101 Vali Loss: 0.1769874 Test Loss: 0.2092052
EarlyStopping counter: 20 out of 20
Early stopping
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5469963264.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2212180
	speed: 0.7277s/iter; left time: 9533.8343s
Epoch: 1 cost time: 95.07090735435486
Epoch: 1, Steps: 132 | Train Loss: 0.2201157 Vali Loss: 0.1767983 Test Loss: 0.2078955
Validation loss decreased (inf --> 0.176798).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2153635
	speed: 1.5825s/iter; left time: 20523.2594s
Epoch: 2 cost time: 94.30191707611084
Epoch: 2, Steps: 132 | Train Loss: 0.2197055 Vali Loss: 0.1767864 Test Loss: 0.2076854
Validation loss decreased (0.176798 --> 0.176786).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2211631
	speed: 1.5612s/iter; left time: 20040.8463s
Epoch: 3 cost time: 93.74657607078552
Epoch: 3, Steps: 132 | Train Loss: 0.2194705 Vali Loss: 0.1766267 Test Loss: 0.2075149
Validation loss decreased (0.176786 --> 0.176627).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2137466
	speed: 1.6300s/iter; left time: 20709.2347s
Epoch: 4 cost time: 96.46081328392029
Epoch: 4, Steps: 132 | Train Loss: 0.2193481 Vali Loss: 0.1764929 Test Loss: 0.2075780
Validation loss decreased (0.176627 --> 0.176493).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2230760
	speed: 1.7028s/iter; left time: 21409.3342s
Epoch: 5 cost time: 97.68677926063538
Epoch: 5, Steps: 132 | Train Loss: 0.2192760 Vali Loss: 0.1769831 Test Loss: 0.2074280
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2012636
	speed: 1.6197s/iter; left time: 20150.4233s
Epoch: 6 cost time: 95.594735622406
Epoch: 6, Steps: 132 | Train Loss: 0.2193211 Vali Loss: 0.1765455 Test Loss: 0.2075120
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2133886
	speed: 1.5531s/iter; left time: 19116.6770s
Epoch: 7 cost time: 89.51701927185059
Epoch: 7, Steps: 132 | Train Loss: 0.2192579 Vali Loss: 0.1767707 Test Loss: 0.2074688
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2087151
	speed: 1.5530s/iter; left time: 18910.7453s
Epoch: 8 cost time: 99.10139775276184
Epoch: 8, Steps: 132 | Train Loss: 0.2191199 Vali Loss: 0.1764892 Test Loss: 0.2074253
Validation loss decreased (0.176493 --> 0.176489).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2260058
	speed: 1.4819s/iter; left time: 17849.4989s
Epoch: 9 cost time: 85.50030732154846
Epoch: 9, Steps: 132 | Train Loss: 0.2191749 Vali Loss: 0.1768030 Test Loss: 0.2074262
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2222418
	speed: 1.4542s/iter; left time: 17323.6348s
Epoch: 10 cost time: 91.10096478462219
Epoch: 10, Steps: 132 | Train Loss: 0.2191561 Vali Loss: 0.1763362 Test Loss: 0.2074788
Validation loss decreased (0.176489 --> 0.176336).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2223820
	speed: 1.5700s/iter; left time: 18495.6426s
Epoch: 11 cost time: 86.88075304031372
Epoch: 11, Steps: 132 | Train Loss: 0.2190292 Vali Loss: 0.1763881 Test Loss: 0.2073816
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2207065
	speed: 1.2974s/iter; left time: 15113.9413s
Epoch: 12 cost time: 75.3030264377594
Epoch: 12, Steps: 132 | Train Loss: 0.2190605 Vali Loss: 0.1766542 Test Loss: 0.2074215
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2278598
	speed: 1.2229s/iter; left time: 14084.2876s
Epoch: 13 cost time: 75.88771796226501
Epoch: 13, Steps: 132 | Train Loss: 0.2190685 Vali Loss: 0.1765650 Test Loss: 0.2074476
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2306735
	speed: 1.2920s/iter; left time: 14709.7939s
Epoch: 14 cost time: 70.62069773674011
Epoch: 14, Steps: 132 | Train Loss: 0.2190765 Vali Loss: 0.1763697 Test Loss: 0.2073434
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2154990
	speed: 1.2370s/iter; left time: 13920.1309s
Epoch: 15 cost time: 74.39276909828186
Epoch: 15, Steps: 132 | Train Loss: 0.2189895 Vali Loss: 0.1762420 Test Loss: 0.2074149
Validation loss decreased (0.176336 --> 0.176242).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2193993
	speed: 1.2506s/iter; left time: 13907.8278s
Epoch: 16 cost time: 72.53632712364197
Epoch: 16, Steps: 132 | Train Loss: 0.2190026 Vali Loss: 0.1763395 Test Loss: 0.2074096
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2186899
	speed: 1.2662s/iter; left time: 13914.0082s
Epoch: 17 cost time: 94.54208588600159
Epoch: 17, Steps: 132 | Train Loss: 0.2189561 Vali Loss: 0.1765272 Test Loss: 0.2074208
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2095942
	speed: 1.7850s/iter; left time: 19379.5185s
Epoch: 18 cost time: 103.07748126983643
Epoch: 18, Steps: 132 | Train Loss: 0.2189625 Vali Loss: 0.1760980 Test Loss: 0.2073927
Validation loss decreased (0.176242 --> 0.176098).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2329861
	speed: 1.6002s/iter; left time: 17162.0032s
Epoch: 19 cost time: 92.84831237792969
Epoch: 19, Steps: 132 | Train Loss: 0.2188756 Vali Loss: 0.1762762 Test Loss: 0.2074108
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2248750
	speed: 1.6444s/iter; left time: 17418.8384s
Epoch: 20 cost time: 100.0445511341095
Epoch: 20, Steps: 132 | Train Loss: 0.2189635 Vali Loss: 0.1761930 Test Loss: 0.2072945
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2174437
	speed: 1.6124s/iter; left time: 16867.4865s
Epoch: 21 cost time: 97.05821323394775
Epoch: 21, Steps: 132 | Train Loss: 0.2188855 Vali Loss: 0.1761656 Test Loss: 0.2074069
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2126926
	speed: 1.6475s/iter; left time: 17016.9366s
Epoch: 22 cost time: 94.20913553237915
Epoch: 22, Steps: 132 | Train Loss: 0.2189417 Vali Loss: 0.1763150 Test Loss: 0.2073665
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2157487
	speed: 1.5460s/iter; left time: 15764.3187s
Epoch: 23 cost time: 92.36301350593567
Epoch: 23, Steps: 132 | Train Loss: 0.2189054 Vali Loss: 0.1765727 Test Loss: 0.2072949
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2162962
	speed: 1.6125s/iter; left time: 16229.7688s
Epoch: 24 cost time: 95.01098346710205
Epoch: 24, Steps: 132 | Train Loss: 0.2189302 Vali Loss: 0.1760762 Test Loss: 0.2073600
Validation loss decreased (0.176098 --> 0.176076).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2227145
	speed: 1.5625s/iter; left time: 15520.2409s
Epoch: 25 cost time: 94.40106463432312
Epoch: 25, Steps: 132 | Train Loss: 0.2189153 Vali Loss: 0.1761205 Test Loss: 0.2073353
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2274855
	speed: 1.5798s/iter; left time: 15483.6431s
Epoch: 26 cost time: 93.22285342216492
Epoch: 26, Steps: 132 | Train Loss: 0.2188173 Vali Loss: 0.1761731 Test Loss: 0.2073414
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2324615
	speed: 1.5781s/iter; left time: 15258.9093s
Epoch: 27 cost time: 96.61912488937378
Epoch: 27, Steps: 132 | Train Loss: 0.2189514 Vali Loss: 0.1760189 Test Loss: 0.2073095
Validation loss decreased (0.176076 --> 0.176019).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2257422
	speed: 1.6559s/iter; left time: 15792.6795s
Epoch: 28 cost time: 99.6564724445343
Epoch: 28, Steps: 132 | Train Loss: 0.2189293 Vali Loss: 0.1761404 Test Loss: 0.2073304
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2136450
	speed: 1.7350s/iter; left time: 16318.1237s
Epoch: 29 cost time: 100.6546483039856
Epoch: 29, Steps: 132 | Train Loss: 0.2189208 Vali Loss: 0.1763418 Test Loss: 0.2073523
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2263360
	speed: 1.7169s/iter; left time: 15921.2769s
Epoch: 30 cost time: 97.08330941200256
Epoch: 30, Steps: 132 | Train Loss: 0.2188216 Vali Loss: 0.1759751 Test Loss: 0.2073021
Validation loss decreased (0.176019 --> 0.175975).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2222859
	speed: 1.5444s/iter; left time: 14117.2381s
Epoch: 31 cost time: 93.97596907615662
Epoch: 31, Steps: 132 | Train Loss: 0.2188827 Vali Loss: 0.1765097 Test Loss: 0.2073181
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2157582
	speed: 1.5473s/iter; left time: 13939.5334s
Epoch: 32 cost time: 92.56005787849426
Epoch: 32, Steps: 132 | Train Loss: 0.2188695 Vali Loss: 0.1763177 Test Loss: 0.2073525
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2216145
	speed: 1.6490s/iter; left time: 14638.4413s
Epoch: 33 cost time: 103.24469208717346
Epoch: 33, Steps: 132 | Train Loss: 0.2188770 Vali Loss: 0.1761071 Test Loss: 0.2073287
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2188974
	speed: 1.5905s/iter; left time: 13908.7223s
Epoch: 34 cost time: 91.92508935928345
Epoch: 34, Steps: 132 | Train Loss: 0.2187905 Vali Loss: 0.1764237 Test Loss: 0.2072964
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2165169
	speed: 1.6501s/iter; left time: 14212.0380s
Epoch: 35 cost time: 94.0643002986908
Epoch: 35, Steps: 132 | Train Loss: 0.2187916 Vali Loss: 0.1761001 Test Loss: 0.2073036
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2327527
	speed: 1.6988s/iter; left time: 14407.5062s
Epoch: 36 cost time: 97.63518047332764
Epoch: 36, Steps: 132 | Train Loss: 0.2189209 Vali Loss: 0.1760758 Test Loss: 0.2072938
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2107474
	speed: 1.5536s/iter; left time: 12970.6446s
Epoch: 37 cost time: 93.68652820587158
Epoch: 37, Steps: 132 | Train Loss: 0.2188539 Vali Loss: 0.1761143 Test Loss: 0.2073151
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2079964
	speed: 1.4958s/iter; left time: 12290.8895s
Epoch: 38 cost time: 86.96768069267273
Epoch: 38, Steps: 132 | Train Loss: 0.2188289 Vali Loss: 0.1760700 Test Loss: 0.2072607
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2160063
	speed: 1.4805s/iter; left time: 11969.9368s
Epoch: 39 cost time: 88.70544028282166
Epoch: 39, Steps: 132 | Train Loss: 0.2187988 Vali Loss: 0.1761888 Test Loss: 0.2072918
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2281120
	speed: 1.4833s/iter; left time: 11797.0234s
Epoch: 40 cost time: 89.29398798942566
Epoch: 40, Steps: 132 | Train Loss: 0.2188359 Vali Loss: 0.1761488 Test Loss: 0.2072998
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2090425
	speed: 1.4743s/iter; left time: 11530.2155s
Epoch: 41 cost time: 85.84981513023376
Epoch: 41, Steps: 132 | Train Loss: 0.2189111 Vali Loss: 0.1761238 Test Loss: 0.2072954
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2196281
	speed: 1.4796s/iter; left time: 11376.6560s
Epoch: 42 cost time: 88.79480624198914
Epoch: 42, Steps: 132 | Train Loss: 0.2187444 Vali Loss: 0.1762865 Test Loss: 0.2073078
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2164636
	speed: 1.4989s/iter; left time: 11327.4895s
Epoch: 43 cost time: 92.38836407661438
Epoch: 43, Steps: 132 | Train Loss: 0.2188635 Vali Loss: 0.1761639 Test Loss: 0.2072904
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2228680
	speed: 1.4592s/iter; left time: 10834.2373s
Epoch: 44 cost time: 87.10060715675354
Epoch: 44, Steps: 132 | Train Loss: 0.2187884 Vali Loss: 0.1761943 Test Loss: 0.2072771
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2321578
	speed: 1.5053s/iter; left time: 10977.9813s
Epoch: 45 cost time: 88.22271990776062
Epoch: 45, Steps: 132 | Train Loss: 0.2188226 Vali Loss: 0.1762135 Test Loss: 0.2073045
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2116150
	speed: 1.4807s/iter; left time: 10603.0400s
Epoch: 46 cost time: 87.06371641159058
Epoch: 46, Steps: 132 | Train Loss: 0.2187690 Vali Loss: 0.1764291 Test Loss: 0.2072991
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2278607
	speed: 1.5292s/iter; left time: 10748.8810s
Epoch: 47 cost time: 94.87067127227783
Epoch: 47, Steps: 132 | Train Loss: 0.2187097 Vali Loss: 0.1762628 Test Loss: 0.2072813
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2190713
	speed: 1.5396s/iter; left time: 10618.7582s
Epoch: 48 cost time: 99.07676720619202
Epoch: 48, Steps: 132 | Train Loss: 0.2188252 Vali Loss: 0.1761217 Test Loss: 0.2072847
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2178765
	speed: 1.4925s/iter; left time: 10096.9336s
Epoch: 49 cost time: 91.4238133430481
Epoch: 49, Steps: 132 | Train Loss: 0.2188137 Vali Loss: 0.1760598 Test Loss: 0.2072812
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2182705
	speed: 1.6446s/iter; left time: 10908.7735s
Epoch: 50 cost time: 100.80626559257507
Epoch: 50, Steps: 132 | Train Loss: 0.2187334 Vali Loss: 0.1760073 Test Loss: 0.2072833
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.20560377836227417, mae:0.2956766188144684, rse:0.4523169994354248, corr:[0.44676962 0.44953775 0.45061234 0.45109126 0.45116895 0.45113212
 0.45120528 0.45107624 0.4509286  0.45090175 0.45074728 0.45072052
 0.45068395 0.45059124 0.45059866 0.4506156  0.45059383 0.4505662
 0.4505114  0.4503815  0.45022917 0.4502366  0.450321   0.45041057
 0.4506118  0.45091385 0.45102236 0.4510407  0.45097438 0.45085388
 0.45074552 0.45059952 0.45044538 0.4503859  0.4502921  0.45020258
 0.4501896  0.45021474 0.45020443 0.450165   0.45014912 0.4501188
 0.4500156  0.4498371  0.4497195  0.44975036 0.44983995 0.449882
 0.45002285 0.4502946  0.4503795  0.45033285 0.45029625 0.45028389
 0.45022932 0.4500979  0.44998085 0.44992802 0.44984573 0.44978023
 0.4497849  0.44978946 0.44981733 0.4498489  0.4497875  0.44974956
 0.44975266 0.4495975  0.4494909  0.44950202 0.44950038 0.44948664
 0.44954345 0.44973046 0.44983828 0.4498     0.44971615 0.4496592
 0.4495614  0.44947165 0.44944203 0.44935662 0.4492462  0.44917986
 0.44914114 0.449194   0.44926482 0.44923714 0.449176   0.44915485
 0.44917113 0.4491068  0.44901806 0.44902527 0.448996   0.44893783
 0.44902426 0.4491304  0.44909874 0.44907492 0.4490148  0.44893366
 0.44888648 0.4488049  0.44870746 0.4486699  0.44860062 0.4485452
 0.4485358  0.44854185 0.44858962 0.44864362 0.44859818 0.4485339
 0.44852126 0.44843665 0.4483522  0.44838184 0.44844472 0.44852173
 0.44876167 0.4489682  0.44897822 0.44889748 0.44884825 0.44885343
 0.44882298 0.4487708  0.44870764 0.44857633 0.4484204  0.44833872
 0.44831324 0.44832405 0.44839698 0.44842967 0.44838688 0.4483811
 0.44845834 0.4484742  0.4484962  0.4485365  0.44847828 0.44843736
 0.44849104 0.44855812 0.4485834  0.44860548 0.4485907  0.44858503
 0.4485304  0.44841126 0.4483572  0.44834742 0.44825485 0.44812557
 0.44806275 0.44804886 0.44808033 0.44814047 0.44817787 0.44815177
 0.44823    0.44827715 0.44818217 0.4481296  0.44807944 0.44791487
 0.44778264 0.4478858  0.44798    0.44799635 0.44794372 0.44786227
 0.44777304 0.44769275 0.44758272 0.4474071  0.44723982 0.44717053
 0.44712043 0.44706827 0.44704333 0.44704846 0.44707793 0.44705656
 0.44693923 0.44672477 0.44657925 0.4465145  0.446458   0.446414
 0.44646814 0.44666803 0.4467354  0.44666472 0.44655392 0.4465286
 0.44650427 0.44639307 0.44631562 0.4462496  0.44610617 0.4460548
 0.44614428 0.4461824  0.4461807  0.44617862 0.4461189  0.4460484
 0.44601524 0.4458144  0.4456054  0.4455549  0.44556972 0.44560963
 0.44568694 0.4458038  0.4458845  0.44589674 0.44578707 0.4457256
 0.44575873 0.44569483 0.44558796 0.44554886 0.44552454 0.4454866
 0.4454947  0.44548213 0.4454454  0.44547266 0.44550097 0.44535404
 0.44521195 0.4451285  0.44502822 0.4449083  0.4449541  0.44508567
 0.4451617  0.445295   0.44544208 0.445464   0.44535196 0.44530863
 0.4452541  0.44506946 0.44500527 0.445014   0.4448862  0.44479617
 0.44482216 0.44482884 0.4448249  0.4448382  0.44479525 0.44472072
 0.44467136 0.44457948 0.44449934 0.44449738 0.44454828 0.44457206
 0.44467327 0.44489792 0.44499537 0.44494465 0.4448949  0.4448557
 0.44476447 0.4446789  0.44456404 0.44444126 0.44439423 0.4443424
 0.4442612  0.44425654 0.44436434 0.4443975  0.4442966  0.44423258
 0.44423983 0.44414303 0.44400752 0.44401553 0.4441356  0.44429973
 0.44452575 0.44465452 0.44470623 0.44469854 0.44462994 0.44463974
 0.4446398  0.4445397  0.4444484  0.44439715 0.4443381  0.4442702
 0.4442171  0.4442286  0.44425964 0.44426128 0.44429138 0.44429126
 0.4442725  0.4442991  0.44434988 0.4443638  0.4443443  0.4443089
 0.44430205 0.44440657 0.44451603 0.44455835 0.44450834 0.4444535
 0.44438908 0.44426692 0.44422457 0.44423124 0.44413248 0.44402403
 0.4440333  0.4440486  0.44404167 0.44405115 0.44405413 0.4440386
 0.44410163 0.44411054 0.44401824 0.44394174 0.44380927 0.44358048
 0.44342655 0.44354275 0.44358927 0.4435065  0.44339412 0.44331715
 0.44318187 0.4430204  0.4429227  0.4428302  0.4426785  0.44258818
 0.44259083 0.4425699  0.44251192 0.44249907 0.4425114  0.44243586
 0.44239116 0.4423831  0.4423349  0.4423135  0.4423337  0.44227126
 0.44221282 0.44234443 0.44239044 0.44233793 0.44226143 0.44218576
 0.44209003 0.44200006 0.4419155  0.44178653 0.44165924 0.44165224
 0.44171396 0.4417123  0.44172385 0.44174856 0.44174868 0.44172415
 0.4416624  0.44150025 0.44141608 0.44141954 0.44138896 0.44135502
 0.44136485 0.4414052  0.44140598 0.4413823  0.4413095  0.441238
 0.44118023 0.44112575 0.44106647 0.44100994 0.44091293 0.44084963
 0.44088078 0.44086733 0.44081    0.44087943 0.44099343 0.44093862
 0.4408484  0.4407834  0.4407184  0.44068432 0.44066477 0.44064155
 0.44069526 0.44087958 0.4410089  0.4410205  0.44097722 0.44096482
 0.44088858 0.44076458 0.44067103 0.44054717 0.44041678 0.44039452
 0.44041118 0.44037345 0.44043475 0.4405379  0.44055563 0.44054392
 0.44053978 0.44045666 0.4404167  0.44045004 0.44044447 0.44040328
 0.4404759  0.4406361  0.44067833 0.44064343 0.44059393 0.44053993
 0.44050604 0.44047385 0.44041067 0.44031256 0.44019648 0.44013584
 0.44016716 0.44017494 0.44015157 0.4401829  0.44020334 0.4401515
 0.44010997 0.4400756  0.4400196  0.43994093 0.43993935 0.44010863
 0.44038612 0.44053227 0.44060448 0.4406515  0.4406248  0.44060656
 0.44053254 0.44037592 0.440272   0.44022644 0.44017658 0.44014212
 0.44018745 0.44024634 0.4402444  0.4402376  0.44027105 0.4402226
 0.44021428 0.44031286 0.44036543 0.44036338 0.44035947 0.44032663
 0.44029966 0.4403868  0.44048014 0.44051674 0.44048163 0.44041744
 0.44034564 0.4403249  0.44034675 0.44025755 0.44010994 0.44008857
 0.4401315  0.4401301  0.44016123 0.44023997 0.4402814  0.4402292
 0.4401891  0.44017404 0.44018042 0.4401398  0.44000694 0.43980217
 0.43963054 0.43968385 0.43974707 0.43974826 0.43964446 0.43954003
 0.43940452 0.43923408 0.43916392 0.4391188  0.43891415 0.4387476
 0.4387691  0.43879336 0.4387562  0.438774   0.4388093  0.4387688
 0.43870243 0.4385984  0.43852985 0.43849596 0.43841788 0.43831193
 0.43826887 0.43837696 0.4384138  0.43835115 0.43826434 0.43820933
 0.4380985  0.43795657 0.43785435 0.43772894 0.43755534 0.4374782
 0.4375214  0.43758163 0.43758985 0.43756902 0.43761593 0.4376709
 0.43761337 0.4374601  0.4373609  0.43728927 0.4372703  0.43727982
 0.43727243 0.4372916  0.43731892 0.4372807  0.43714762 0.43709728
 0.43709862 0.43696406 0.43683058 0.43673941 0.43653613 0.43638927
 0.43641403 0.4363538  0.43621352 0.43615806 0.43613115 0.43597862
 0.4358414  0.4358306  0.43583748 0.43579176 0.43578056 0.43577173
 0.43578205 0.43591213 0.43600443 0.43597308 0.43593863 0.4359275
 0.43588012 0.4358139  0.43572402 0.4355757  0.4354209  0.4353566
 0.43533644 0.4352859  0.4352718  0.43528056 0.4352807  0.4352693
 0.43527374 0.43522835 0.43519092 0.43517977 0.43516353 0.43516052
 0.4352041  0.43528366 0.43536437 0.43537793 0.43524504 0.43514225
 0.4351096  0.4350289  0.43489653 0.43478638 0.43467522 0.43458575
 0.43456924 0.43460792 0.4346155  0.43459377 0.43462026 0.43461472
 0.43455136 0.43452647 0.43451554 0.4344893  0.43452334 0.43465438
 0.43490976 0.43506056 0.43507588 0.4350419  0.43496042 0.43493915
 0.4349353  0.43480057 0.4346671  0.43461108 0.43450484 0.43443152
 0.43446076 0.4344193  0.43434623 0.43436003 0.43441096 0.43440524
 0.4344208  0.43445587 0.43446067 0.43442938 0.43443283 0.43450508
 0.4345209  0.43455932 0.4346375  0.43460009 0.4344729  0.43447617
 0.4344889  0.43434042 0.43418095 0.4341165  0.4340706  0.43398955
 0.43395633 0.43397677 0.43397984 0.43403918 0.43409348 0.43399483
 0.43396065 0.43397713 0.4339654  0.4339787  0.43394932 0.43378794
 0.43361    0.43363973 0.43364033 0.43356574 0.433433   0.4333424
 0.4332338  0.43302783 0.43288386 0.4327364  0.43248683 0.43232197
 0.43226314 0.43228415 0.4323119  0.43223384 0.43226555 0.4323597
 0.43223587 0.4320866  0.43207338 0.43203878 0.43203655 0.4320702
 0.43209067 0.4321456  0.43211997 0.43212032 0.43203643 0.43186316
 0.43181616 0.43172333 0.43153414 0.43134704 0.43109143 0.43107235
 0.42905566 0.4310759  0.42696643 0.42702353 0.43123454 0.42704487
 0.42686653 0.42685118 0.42678222 0.43110573 0.4268773  0.42666712]
