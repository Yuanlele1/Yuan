Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=170, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_360_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_360_j720_H10_FITS_custom_ftM_sl360_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17333
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=170, out_features=510, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3562329600.0
params:  87210.0
Trainable parameters:  87210
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8506287
	speed: 1.4327s/iter; left time: 19199.9809s
Epoch: 1 cost time: 190.28753089904785
Epoch: 1, Steps: 135 | Train Loss: 1.0676097 Vali Loss: 0.6745251 Test Loss: 0.7962894
Validation loss decreased (inf --> 0.674525).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6535346
	speed: 3.2881s/iter; left time: 43619.6670s
Epoch: 2 cost time: 206.22357487678528
Epoch: 2, Steps: 135 | Train Loss: 0.6848323 Vali Loss: 0.5732271 Test Loss: 0.6833872
Validation loss decreased (0.674525 --> 0.573227).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5624064
	speed: 3.4374s/iter; left time: 45136.9571s
Epoch: 3 cost time: 201.38923740386963
Epoch: 3, Steps: 135 | Train Loss: 0.5942870 Vali Loss: 0.5147722 Test Loss: 0.6160387
Validation loss decreased (0.573227 --> 0.514772).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5146186
	speed: 3.0196s/iter; left time: 39243.0034s
Epoch: 4 cost time: 169.99823331832886
Epoch: 4, Steps: 135 | Train Loss: 0.5287211 Vali Loss: 0.4638343 Test Loss: 0.5582868
Validation loss decreased (0.514772 --> 0.463834).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4600256
	speed: 2.7738s/iter; left time: 35674.2790s
Epoch: 5 cost time: 161.00624108314514
Epoch: 5, Steps: 135 | Train Loss: 0.4747236 Vali Loss: 0.4234565 Test Loss: 0.5112894
Validation loss decreased (0.463834 --> 0.423456).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4269470
	speed: 2.7298s/iter; left time: 34739.0135s
Epoch: 6 cost time: 161.18889808654785
Epoch: 6, Steps: 135 | Train Loss: 0.4297533 Vali Loss: 0.3877158 Test Loss: 0.4697751
Validation loss decreased (0.423456 --> 0.387716).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3791710
	speed: 2.7219s/iter; left time: 34271.0291s
Epoch: 7 cost time: 158.8318874835968
Epoch: 7, Steps: 135 | Train Loss: 0.3917064 Vali Loss: 0.3595833 Test Loss: 0.4367434
Validation loss decreased (0.387716 --> 0.359583).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3569057
	speed: 2.8110s/iter; left time: 35014.0284s
Epoch: 8 cost time: 164.79839396476746
Epoch: 8, Steps: 135 | Train Loss: 0.3594389 Vali Loss: 0.3339952 Test Loss: 0.4069032
Validation loss decreased (0.359583 --> 0.333995).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3193454
	speed: 3.0790s/iter; left time: 37936.1236s
Epoch: 9 cost time: 182.79614973068237
Epoch: 9, Steps: 135 | Train Loss: 0.3319333 Vali Loss: 0.3128458 Test Loss: 0.3817343
Validation loss decreased (0.333995 --> 0.312846).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3053477
	speed: 2.8474s/iter; left time: 34698.7184s
Epoch: 10 cost time: 157.9965524673462
Epoch: 10, Steps: 135 | Train Loss: 0.3083223 Vali Loss: 0.2946376 Test Loss: 0.3604343
Validation loss decreased (0.312846 --> 0.294638).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2849998
	speed: 2.2632s/iter; left time: 27273.7713s
Epoch: 11 cost time: 144.49690103530884
Epoch: 11, Steps: 135 | Train Loss: 0.2879716 Vali Loss: 0.2791511 Test Loss: 0.3411976
Validation loss decreased (0.294638 --> 0.279151).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2678567
	speed: 2.8817s/iter; left time: 34337.7749s
Epoch: 12 cost time: 130.84980607032776
Epoch: 12, Steps: 135 | Train Loss: 0.2704381 Vali Loss: 0.2656900 Test Loss: 0.3252621
Validation loss decreased (0.279151 --> 0.265690).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2620332
	speed: 2.2035s/iter; left time: 25958.9043s
Epoch: 13 cost time: 116.265460729599
Epoch: 13, Steps: 135 | Train Loss: 0.2553771 Vali Loss: 0.2540537 Test Loss: 0.3112510
Validation loss decreased (0.265690 --> 0.254054).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2387535
	speed: 2.9264s/iter; left time: 34080.3850s
Epoch: 14 cost time: 242.68220353126526
Epoch: 14, Steps: 135 | Train Loss: 0.2422290 Vali Loss: 0.2441851 Test Loss: 0.2994727
Validation loss decreased (0.254054 --> 0.244185).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2257603
	speed: 3.2547s/iter; left time: 37464.7314s
Epoch: 15 cost time: 145.85183787345886
Epoch: 15, Steps: 135 | Train Loss: 0.2308753 Vali Loss: 0.2358572 Test Loss: 0.2886722
Validation loss decreased (0.244185 --> 0.235857).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2268558
	speed: 2.7281s/iter; left time: 31034.9396s
Epoch: 16 cost time: 134.62779569625854
Epoch: 16, Steps: 135 | Train Loss: 0.2209095 Vali Loss: 0.2283225 Test Loss: 0.2792546
Validation loss decreased (0.235857 --> 0.228322).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2057813
	speed: 2.5960s/iter; left time: 29182.1416s
Epoch: 17 cost time: 165.21816158294678
Epoch: 17, Steps: 135 | Train Loss: 0.2123515 Vali Loss: 0.2218622 Test Loss: 0.2711889
Validation loss decreased (0.228322 --> 0.221862).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2095017
	speed: 2.8022s/iter; left time: 31121.4479s
Epoch: 18 cost time: 188.04651069641113
Epoch: 18, Steps: 135 | Train Loss: 0.2048705 Vali Loss: 0.2158504 Test Loss: 0.2638021
Validation loss decreased (0.221862 --> 0.215850).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1949301
	speed: 2.8430s/iter; left time: 31191.0604s
Epoch: 19 cost time: 146.1783366203308
Epoch: 19, Steps: 135 | Train Loss: 0.1983200 Vali Loss: 0.2113438 Test Loss: 0.2579878
Validation loss decreased (0.215850 --> 0.211344).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1880650
	speed: 2.4594s/iter; left time: 26649.7423s
Epoch: 20 cost time: 142.5620608329773
Epoch: 20, Steps: 135 | Train Loss: 0.1925573 Vali Loss: 0.2074002 Test Loss: 0.2526242
Validation loss decreased (0.211344 --> 0.207400).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1908830
	speed: 2.7516s/iter; left time: 29445.3073s
Epoch: 21 cost time: 155.18520188331604
Epoch: 21, Steps: 135 | Train Loss: 0.1875520 Vali Loss: 0.2036211 Test Loss: 0.2478518
Validation loss decreased (0.207400 --> 0.203621).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1853733
	speed: 2.4708s/iter; left time: 26106.4369s
Epoch: 22 cost time: 145.18598437309265
Epoch: 22, Steps: 135 | Train Loss: 0.1832213 Vali Loss: 0.2003637 Test Loss: 0.2434459
Validation loss decreased (0.203621 --> 0.200364).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1826669
	speed: 2.4018s/iter; left time: 25053.0620s
Epoch: 23 cost time: 135.32616257667542
Epoch: 23, Steps: 135 | Train Loss: 0.1794181 Vali Loss: 0.1978218 Test Loss: 0.2400480
Validation loss decreased (0.200364 --> 0.197822).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1671041
	speed: 2.3551s/iter; left time: 24248.2339s
Epoch: 24 cost time: 141.98561882972717
Epoch: 24, Steps: 135 | Train Loss: 0.1760383 Vali Loss: 0.1951727 Test Loss: 0.2365206
Validation loss decreased (0.197822 --> 0.195173).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1715843
	speed: 2.3067s/iter; left time: 23438.2961s
Epoch: 25 cost time: 138.134126663208
Epoch: 25, Steps: 135 | Train Loss: 0.1730341 Vali Loss: 0.1933848 Test Loss: 0.2338936
Validation loss decreased (0.195173 --> 0.193385).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1709208
	speed: 2.3105s/iter; left time: 23165.4632s
Epoch: 26 cost time: 146.81863975524902
Epoch: 26, Steps: 135 | Train Loss: 0.1704842 Vali Loss: 0.1914220 Test Loss: 0.2312781
Validation loss decreased (0.193385 --> 0.191422).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1606456
	speed: 2.3062s/iter; left time: 22810.8608s
Epoch: 27 cost time: 122.24203872680664
Epoch: 27, Steps: 135 | Train Loss: 0.1681777 Vali Loss: 0.1899615 Test Loss: 0.2289804
Validation loss decreased (0.191422 --> 0.189961).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1752890
	speed: 2.1647s/iter; left time: 21119.0836s
Epoch: 28 cost time: 122.45747923851013
Epoch: 28, Steps: 135 | Train Loss: 0.1661871 Vali Loss: 0.1886267 Test Loss: 0.2271751
Validation loss decreased (0.189961 --> 0.188627).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1568129
	speed: 2.1183s/iter; left time: 20380.2062s
Epoch: 29 cost time: 123.54536318778992
Epoch: 29, Steps: 135 | Train Loss: 0.1643986 Vali Loss: 0.1875894 Test Loss: 0.2252704
Validation loss decreased (0.188627 --> 0.187589).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1642184
	speed: 2.3885s/iter; left time: 22657.5000s
Epoch: 30 cost time: 146.6507670879364
Epoch: 30, Steps: 135 | Train Loss: 0.1628545 Vali Loss: 0.1865044 Test Loss: 0.2238466
Validation loss decreased (0.187589 --> 0.186504).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1656183
	speed: 2.3555s/iter; left time: 22026.4078s
Epoch: 31 cost time: 129.49904799461365
Epoch: 31, Steps: 135 | Train Loss: 0.1615248 Vali Loss: 0.1855724 Test Loss: 0.2224367
Validation loss decreased (0.186504 --> 0.185572).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1629457
	speed: 2.1435s/iter; left time: 19754.3147s
Epoch: 32 cost time: 120.73307085037231
Epoch: 32, Steps: 135 | Train Loss: 0.1602710 Vali Loss: 0.1846978 Test Loss: 0.2212220
Validation loss decreased (0.185572 --> 0.184698).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1580151
	speed: 1.8581s/iter; left time: 16873.0501s
Epoch: 33 cost time: 110.65248131752014
Epoch: 33, Steps: 135 | Train Loss: 0.1591785 Vali Loss: 0.1840363 Test Loss: 0.2202041
Validation loss decreased (0.184698 --> 0.184036).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1538486
	speed: 2.3021s/iter; left time: 20594.3846s
Epoch: 34 cost time: 143.840491771698
Epoch: 34, Steps: 135 | Train Loss: 0.1582946 Vali Loss: 0.1836197 Test Loss: 0.2192696
Validation loss decreased (0.184036 --> 0.183620).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1599847
	speed: 2.0802s/iter; left time: 18328.2344s
Epoch: 35 cost time: 124.04053807258606
Epoch: 35, Steps: 135 | Train Loss: 0.1574598 Vali Loss: 0.1827568 Test Loss: 0.2183452
Validation loss decreased (0.183620 --> 0.182757).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1527142
	speed: 2.1535s/iter; left time: 18683.6266s
Epoch: 36 cost time: 133.37461853027344
Epoch: 36, Steps: 135 | Train Loss: 0.1566888 Vali Loss: 0.1825746 Test Loss: 0.2176093
Validation loss decreased (0.182757 --> 0.182575).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1532196
	speed: 2.0178s/iter; left time: 17234.2253s
Epoch: 37 cost time: 122.19131112098694
Epoch: 37, Steps: 135 | Train Loss: 0.1560914 Vali Loss: 0.1823215 Test Loss: 0.2169311
Validation loss decreased (0.182575 --> 0.182322).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1537982
	speed: 1.8936s/iter; left time: 15917.1920s
Epoch: 38 cost time: 112.90789556503296
Epoch: 38, Steps: 135 | Train Loss: 0.1555136 Vali Loss: 0.1819934 Test Loss: 0.2163410
Validation loss decreased (0.182322 --> 0.181993).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1501655
	speed: 4.0215s/iter; left time: 33261.6322s
Epoch: 39 cost time: 233.41847848892212
Epoch: 39, Steps: 135 | Train Loss: 0.1549442 Vali Loss: 0.1815592 Test Loss: 0.2158126
Validation loss decreased (0.181993 --> 0.181559).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1624173
	speed: 4.5367s/iter; left time: 36910.6546s
Epoch: 40 cost time: 258.93174409866333
Epoch: 40, Steps: 135 | Train Loss: 0.1545532 Vali Loss: 0.1813525 Test Loss: 0.2153048
Validation loss decreased (0.181559 --> 0.181353).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1609531
	speed: 3.8096s/iter; left time: 30480.7353s
Epoch: 41 cost time: 163.5545265674591
Epoch: 41, Steps: 135 | Train Loss: 0.1541780 Vali Loss: 0.1813771 Test Loss: 0.2148992
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1509979
	speed: 2.3057s/iter; left time: 18136.5050s
Epoch: 42 cost time: 131.07998538017273
Epoch: 42, Steps: 135 | Train Loss: 0.1537736 Vali Loss: 0.1810992 Test Loss: 0.2145194
Validation loss decreased (0.181353 --> 0.181099).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1520805
	speed: 2.2458s/iter; left time: 17362.2924s
Epoch: 43 cost time: 131.96838068962097
Epoch: 43, Steps: 135 | Train Loss: 0.1534867 Vali Loss: 0.1810721 Test Loss: 0.2142120
Validation loss decreased (0.181099 --> 0.181072).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1511379
	speed: 2.3017s/iter; left time: 17483.3680s
Epoch: 44 cost time: 135.80781435966492
Epoch: 44, Steps: 135 | Train Loss: 0.1532328 Vali Loss: 0.1806056 Test Loss: 0.2139051
Validation loss decreased (0.181072 --> 0.180606).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1620644
	speed: 2.4980s/iter; left time: 18637.3148s
Epoch: 45 cost time: 144.69235515594482
Epoch: 45, Steps: 135 | Train Loss: 0.1529730 Vali Loss: 0.1805370 Test Loss: 0.2136120
Validation loss decreased (0.180606 --> 0.180537).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1530712
	speed: 2.3214s/iter; left time: 17006.5350s
Epoch: 46 cost time: 134.88623237609863
Epoch: 46, Steps: 135 | Train Loss: 0.1527749 Vali Loss: 0.1807254 Test Loss: 0.2134061
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1545492
	speed: 2.1916s/iter; left time: 15760.0381s
Epoch: 47 cost time: 130.99529242515564
Epoch: 47, Steps: 135 | Train Loss: 0.1525787 Vali Loss: 0.1802800 Test Loss: 0.2131653
Validation loss decreased (0.180537 --> 0.180280).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1506614
	speed: 2.2470s/iter; left time: 15855.1333s
Epoch: 48 cost time: 132.89683198928833
Epoch: 48, Steps: 135 | Train Loss: 0.1524082 Vali Loss: 0.1803777 Test Loss: 0.2129780
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1452848
	speed: 2.2488s/iter; left time: 15563.9587s
Epoch: 49 cost time: 128.8327031135559
Epoch: 49, Steps: 135 | Train Loss: 0.1522932 Vali Loss: 0.1805342 Test Loss: 0.2128211
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1502795
	speed: 2.1807s/iter; left time: 14798.2533s
Epoch: 50 cost time: 131.0487997531891
Epoch: 50, Steps: 135 | Train Loss: 0.1521669 Vali Loss: 0.1800924 Test Loss: 0.2126698
Validation loss decreased (0.180280 --> 0.180092).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.1525514
	speed: 1.8529s/iter; left time: 12323.8711s
Epoch: 51 cost time: 113.52449989318848
Epoch: 51, Steps: 135 | Train Loss: 0.1520283 Vali Loss: 0.1805898 Test Loss: 0.2125269
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.1454254
	speed: 2.3056s/iter; left time: 15023.0077s
Epoch: 52 cost time: 137.37137031555176
Epoch: 52, Steps: 135 | Train Loss: 0.1519515 Vali Loss: 0.1804341 Test Loss: 0.2124074
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.1607827
	speed: 2.4358s/iter; left time: 15543.1156s
Epoch: 53 cost time: 157.77025890350342
Epoch: 53, Steps: 135 | Train Loss: 0.1518293 Vali Loss: 0.1803267 Test Loss: 0.2122985
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.1498132
	speed: 2.2278s/iter; left time: 13914.9072s
Epoch: 54 cost time: 123.52200937271118
Epoch: 54, Steps: 135 | Train Loss: 0.1517931 Vali Loss: 0.1803983 Test Loss: 0.2121846
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.1592625
	speed: 1.9744s/iter; left time: 12065.5858s
Epoch: 55 cost time: 113.59782099723816
Epoch: 55, Steps: 135 | Train Loss: 0.1517043 Vali Loss: 0.1803154 Test Loss: 0.2120943
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.1457594
	speed: 2.2578s/iter; left time: 13492.4390s
Epoch: 56 cost time: 155.8808238506317
Epoch: 56, Steps: 135 | Train Loss: 0.1516291 Vali Loss: 0.1804191 Test Loss: 0.2120129
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.1476266
	speed: 3.0048s/iter; left time: 17551.0206s
Epoch: 57 cost time: 184.89199113845825
Epoch: 57, Steps: 135 | Train Loss: 0.1515981 Vali Loss: 0.1803802 Test Loss: 0.2119448
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.1567157
	speed: 2.7656s/iter; left time: 15780.4566s
Epoch: 58 cost time: 163.2813150882721
Epoch: 58, Steps: 135 | Train Loss: 0.1514949 Vali Loss: 0.1803957 Test Loss: 0.2118804
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.1486262
	speed: 2.8910s/iter; left time: 16105.8081s
Epoch: 59 cost time: 167.87786388397217
Epoch: 59, Steps: 135 | Train Loss: 0.1514850 Vali Loss: 0.1804656 Test Loss: 0.2118219
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.1504737
	speed: 3.0464s/iter; left time: 16560.2316s
Epoch: 60 cost time: 179.0516664981842
Epoch: 60, Steps: 135 | Train Loss: 0.1514468 Vali Loss: 0.1807221 Test Loss: 0.2117769
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.1479982
	speed: 2.9536s/iter; left time: 15656.8253s
Epoch: 61 cost time: 178.28210067749023
Epoch: 61, Steps: 135 | Train Loss: 0.1514259 Vali Loss: 0.1806916 Test Loss: 0.2117343
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.1514918
	speed: 2.8195s/iter; left time: 14565.3331s
Epoch: 62 cost time: 158.70162773132324
Epoch: 62, Steps: 135 | Train Loss: 0.1514117 Vali Loss: 0.1803372 Test Loss: 0.2116868
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.1454242
	speed: 2.7203s/iter; left time: 13686.0325s
Epoch: 63 cost time: 163.14873433113098
Epoch: 63, Steps: 135 | Train Loss: 0.1513475 Vali Loss: 0.1803734 Test Loss: 0.2116514
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.1493616
	speed: 2.6935s/iter; left time: 13187.2506s
Epoch: 64 cost time: 157.1420660018921
Epoch: 64, Steps: 135 | Train Loss: 0.1513582 Vali Loss: 0.1804181 Test Loss: 0.2116150
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.1539767
	speed: 2.7541s/iter; left time: 13112.3667s
Epoch: 65 cost time: 167.60328674316406
Epoch: 65, Steps: 135 | Train Loss: 0.1513082 Vali Loss: 0.1804509 Test Loss: 0.2115865
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.1589595
	speed: 2.9339s/iter; left time: 13572.2288s
Epoch: 66 cost time: 167.4059009552002
Epoch: 66, Steps: 135 | Train Loss: 0.1512988 Vali Loss: 0.1801151 Test Loss: 0.2115542
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.1546387
	speed: 2.6242s/iter; left time: 11785.4576s
Epoch: 67 cost time: 151.26229286193848
Epoch: 67, Steps: 135 | Train Loss: 0.1512624 Vali Loss: 0.1806152 Test Loss: 0.2115365
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.1522118
	speed: 2.8464s/iter; left time: 12398.9516s
Epoch: 68 cost time: 163.9313931465149
Epoch: 68, Steps: 135 | Train Loss: 0.1512903 Vali Loss: 0.1805159 Test Loss: 0.2115093
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.1580899
	speed: 2.7494s/iter; left time: 11605.0687s
Epoch: 69 cost time: 161.44620275497437
Epoch: 69, Steps: 135 | Train Loss: 0.1512175 Vali Loss: 0.1805931 Test Loss: 0.2114893
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.1508732
	speed: 2.5369s/iter; left time: 10365.8736s
Epoch: 70 cost time: 151.09159541130066
Epoch: 70, Steps: 135 | Train Loss: 0.1512313 Vali Loss: 0.1806114 Test Loss: 0.2114729
EarlyStopping counter: 20 out of 20
Early stopping
train 17333
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=170, out_features=510, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3562329600.0
params:  87210.0
Trainable parameters:  87210
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2286952
	speed: 1.0592s/iter; left time: 14194.4283s
Epoch: 1 cost time: 143.1664183139801
Epoch: 1, Steps: 135 | Train Loss: 0.2252841 Vali Loss: 0.1800398 Test Loss: 0.2110183
Validation loss decreased (inf --> 0.180040).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2249556
	speed: 2.5014s/iter; left time: 33183.3864s
Epoch: 2 cost time: 149.78069400787354
Epoch: 2, Steps: 135 | Train Loss: 0.2249158 Vali Loss: 0.1802271 Test Loss: 0.2108197
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2236977
	speed: 2.4412s/iter; left time: 32054.7646s
Epoch: 3 cost time: 138.77283787727356
Epoch: 3, Steps: 135 | Train Loss: 0.2249067 Vali Loss: 0.1800498 Test Loss: 0.2108445
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2322053
	speed: 2.3102s/iter; left time: 30022.7985s
Epoch: 4 cost time: 136.5333285331726
Epoch: 4, Steps: 135 | Train Loss: 0.2249351 Vali Loss: 0.1802392 Test Loss: 0.2108462
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2384892
	speed: 2.5299s/iter; left time: 32536.4275s
Epoch: 5 cost time: 154.3975305557251
Epoch: 5, Steps: 135 | Train Loss: 0.2248664 Vali Loss: 0.1804682 Test Loss: 0.2108194
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2217631
	speed: 2.7396s/iter; left time: 34864.4464s
Epoch: 6 cost time: 158.7262647151947
Epoch: 6, Steps: 135 | Train Loss: 0.2248438 Vali Loss: 0.1795982 Test Loss: 0.2108145
Validation loss decreased (0.180040 --> 0.179598).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2193342
	speed: 2.3498s/iter; left time: 29586.5064s
Epoch: 7 cost time: 137.3115074634552
Epoch: 7, Steps: 135 | Train Loss: 0.2247942 Vali Loss: 0.1799630 Test Loss: 0.2108134
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2262115
	speed: 2.2105s/iter; left time: 27533.7425s
Epoch: 8 cost time: 131.83471941947937
Epoch: 8, Steps: 135 | Train Loss: 0.2247442 Vali Loss: 0.1800161 Test Loss: 0.2107398
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2234066
	speed: 2.1590s/iter; left time: 26600.5825s
Epoch: 9 cost time: 126.3905873298645
Epoch: 9, Steps: 135 | Train Loss: 0.2247692 Vali Loss: 0.1801046 Test Loss: 0.2107994
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2321618
	speed: 2.2184s/iter; left time: 27033.2115s
Epoch: 10 cost time: 126.42824482917786
Epoch: 10, Steps: 135 | Train Loss: 0.2248078 Vali Loss: 0.1797107 Test Loss: 0.2107519
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2286552
	speed: 2.2276s/iter; left time: 26844.7879s
Epoch: 11 cost time: 130.55749583244324
Epoch: 11, Steps: 135 | Train Loss: 0.2247547 Vali Loss: 0.1803012 Test Loss: 0.2107394
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2279980
	speed: 2.3361s/iter; left time: 27836.8452s
Epoch: 12 cost time: 142.00047969818115
Epoch: 12, Steps: 135 | Train Loss: 0.2247397 Vali Loss: 0.1801894 Test Loss: 0.2107734
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2196815
	speed: 2.2529s/iter; left time: 26541.3984s
Epoch: 13 cost time: 132.3277451992035
Epoch: 13, Steps: 135 | Train Loss: 0.2247502 Vali Loss: 0.1798136 Test Loss: 0.2107653
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2223534
	speed: 2.5040s/iter; left time: 29161.1417s
Epoch: 14 cost time: 160.37948346138
Epoch: 14, Steps: 135 | Train Loss: 0.2247204 Vali Loss: 0.1797732 Test Loss: 0.2107297
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2191408
	speed: 2.7091s/iter; left time: 31184.1210s
Epoch: 15 cost time: 162.83138728141785
Epoch: 15, Steps: 135 | Train Loss: 0.2246579 Vali Loss: 0.1800704 Test Loss: 0.2107007
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2214796
	speed: 2.8341s/iter; left time: 32240.6506s
Epoch: 16 cost time: 162.38112831115723
Epoch: 16, Steps: 135 | Train Loss: 0.2247443 Vali Loss: 0.1798184 Test Loss: 0.2107865
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2313765
	speed: 2.7963s/iter; left time: 31432.7087s
Epoch: 17 cost time: 169.22317504882812
Epoch: 17, Steps: 135 | Train Loss: 0.2246994 Vali Loss: 0.1797561 Test Loss: 0.2107734
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2422097
	speed: 2.9923s/iter; left time: 33232.2978s
Epoch: 18 cost time: 169.10848736763
Epoch: 18, Steps: 135 | Train Loss: 0.2247111 Vali Loss: 0.1796611 Test Loss: 0.2107432
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2269420
	speed: 3.0185s/iter; left time: 33116.3242s
Epoch: 19 cost time: 182.55433988571167
Epoch: 19, Steps: 135 | Train Loss: 0.2246147 Vali Loss: 0.1797642 Test Loss: 0.2107311
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2182739
	speed: 2.5328s/iter; left time: 27445.7187s
Epoch: 20 cost time: 154.1322090625763
Epoch: 20, Steps: 135 | Train Loss: 0.2246529 Vali Loss: 0.1798479 Test Loss: 0.2107550
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2293243
	speed: 2.4807s/iter; left time: 26546.1452s
Epoch: 21 cost time: 148.0342733860016
Epoch: 21, Steps: 135 | Train Loss: 0.2246934 Vali Loss: 0.1798103 Test Loss: 0.2107263
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2111878
	speed: 2.4771s/iter; left time: 26173.5069s
Epoch: 22 cost time: 141.30398607254028
Epoch: 22, Steps: 135 | Train Loss: 0.2246005 Vali Loss: 0.1798738 Test Loss: 0.2107156
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2278969
	speed: 2.4734s/iter; left time: 25799.8019s
Epoch: 23 cost time: 142.45877051353455
Epoch: 23, Steps: 135 | Train Loss: 0.2246835 Vali Loss: 0.1799163 Test Loss: 0.2107300
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2292818
	speed: 2.3442s/iter; left time: 24135.6504s
Epoch: 24 cost time: 140.56651902198792
Epoch: 24, Steps: 135 | Train Loss: 0.2245996 Vali Loss: 0.1797121 Test Loss: 0.2107147
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2214938
	speed: 2.2924s/iter; left time: 23293.1839s
Epoch: 25 cost time: 129.81382942199707
Epoch: 25, Steps: 135 | Train Loss: 0.2246561 Vali Loss: 0.1794566 Test Loss: 0.2107159
Validation loss decreased (0.179598 --> 0.179457).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2268916
	speed: 2.2532s/iter; left time: 22590.7889s
Epoch: 26 cost time: 128.08004331588745
Epoch: 26, Steps: 135 | Train Loss: 0.2245680 Vali Loss: 0.1796249 Test Loss: 0.2107339
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2271695
	speed: 2.3353s/iter; left time: 23098.1574s
Epoch: 27 cost time: 140.4933123588562
Epoch: 27, Steps: 135 | Train Loss: 0.2245996 Vali Loss: 0.1799393 Test Loss: 0.2107192
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2283483
	speed: 2.2957s/iter; left time: 22397.2555s
Epoch: 28 cost time: 133.14233589172363
Epoch: 28, Steps: 135 | Train Loss: 0.2245893 Vali Loss: 0.1796470 Test Loss: 0.2107436
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2271668
	speed: 2.1955s/iter; left time: 21122.5577s
Epoch: 29 cost time: 129.49088883399963
Epoch: 29, Steps: 135 | Train Loss: 0.2245817 Vali Loss: 0.1793211 Test Loss: 0.2107044
Validation loss decreased (0.179457 --> 0.179321).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2247143
	speed: 2.2103s/iter; left time: 20966.8024s
Epoch: 30 cost time: 126.24914145469666
Epoch: 30, Steps: 135 | Train Loss: 0.2245499 Vali Loss: 0.1800207 Test Loss: 0.2106971
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2188307
	speed: 1.9886s/iter; left time: 18595.0324s
Epoch: 31 cost time: 110.85000443458557
Epoch: 31, Steps: 135 | Train Loss: 0.2245410 Vali Loss: 0.1796691 Test Loss: 0.2106728
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2265438
	speed: 2.0366s/iter; left time: 18768.9641s
Epoch: 32 cost time: 125.93460512161255
Epoch: 32, Steps: 135 | Train Loss: 0.2245744 Vali Loss: 0.1798854 Test Loss: 0.2107089
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2273719
	speed: 3.8641s/iter; left time: 35090.2632s
Epoch: 33 cost time: 217.28910541534424
Epoch: 33, Steps: 135 | Train Loss: 0.2245234 Vali Loss: 0.1797474 Test Loss: 0.2107158
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2257809
	speed: 2.3588s/iter; left time: 21102.0368s
Epoch: 34 cost time: 97.55646276473999
Epoch: 34, Steps: 135 | Train Loss: 0.2246220 Vali Loss: 0.1796784 Test Loss: 0.2106962
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2197724
	speed: 1.5975s/iter; left time: 14075.7366s
Epoch: 35 cost time: 105.54422783851624
Epoch: 35, Steps: 135 | Train Loss: 0.2245550 Vali Loss: 0.1796258 Test Loss: 0.2107146
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2180997
	speed: 1.8659s/iter; left time: 16188.4430s
Epoch: 36 cost time: 97.8231098651886
Epoch: 36, Steps: 135 | Train Loss: 0.2245562 Vali Loss: 0.1797227 Test Loss: 0.2106994
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2169790
	speed: 1.6125s/iter; left time: 13772.4729s
Epoch: 37 cost time: 98.59476828575134
Epoch: 37, Steps: 135 | Train Loss: 0.2245820 Vali Loss: 0.1794444 Test Loss: 0.2106900
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2324054
	speed: 1.6739s/iter; left time: 14070.9975s
Epoch: 38 cost time: 92.55062365531921
Epoch: 38, Steps: 135 | Train Loss: 0.2245455 Vali Loss: 0.1796925 Test Loss: 0.2107090
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2227987
	speed: 1.5834s/iter; left time: 13096.0960s
Epoch: 39 cost time: 89.94310975074768
Epoch: 39, Steps: 135 | Train Loss: 0.2245513 Vali Loss: 0.1796665 Test Loss: 0.2106918
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2282478
	speed: 1.5445s/iter; left time: 12565.8519s
Epoch: 40 cost time: 91.01202464103699
Epoch: 40, Steps: 135 | Train Loss: 0.2246225 Vali Loss: 0.1797214 Test Loss: 0.2107157
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2257858
	speed: 1.5046s/iter; left time: 12038.4052s
Epoch: 41 cost time: 84.19495034217834
Epoch: 41, Steps: 135 | Train Loss: 0.2245442 Vali Loss: 0.1798760 Test Loss: 0.2107002
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2275762
	speed: 1.4955s/iter; left time: 11763.8054s
Epoch: 42 cost time: 79.9198215007782
Epoch: 42, Steps: 135 | Train Loss: 0.2245139 Vali Loss: 0.1797233 Test Loss: 0.2107071
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2174966
	speed: 1.3652s/iter; left time: 10554.2015s
Epoch: 43 cost time: 79.16791868209839
Epoch: 43, Steps: 135 | Train Loss: 0.2245331 Vali Loss: 0.1795624 Test Loss: 0.2107101
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2228320
	speed: 1.3947s/iter; left time: 10594.2550s
Epoch: 44 cost time: 83.10540843009949
Epoch: 44, Steps: 135 | Train Loss: 0.2245292 Vali Loss: 0.1794309 Test Loss: 0.2106933
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2314907
	speed: 1.3352s/iter; left time: 9961.5559s
Epoch: 45 cost time: 75.23774933815002
Epoch: 45, Steps: 135 | Train Loss: 0.2245871 Vali Loss: 0.1797218 Test Loss: 0.2107131
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2189454
	speed: 1.2099s/iter; left time: 8863.8929s
Epoch: 46 cost time: 70.45959496498108
Epoch: 46, Steps: 135 | Train Loss: 0.2245503 Vali Loss: 0.1793745 Test Loss: 0.2106937
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2210540
	speed: 1.3128s/iter; left time: 9440.5499s
Epoch: 47 cost time: 78.41573119163513
Epoch: 47, Steps: 135 | Train Loss: 0.2245989 Vali Loss: 0.1797082 Test Loss: 0.2107052
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2232015
	speed: 1.3734s/iter; left time: 9691.0448s
Epoch: 48 cost time: 94.55454325675964
Epoch: 48, Steps: 135 | Train Loss: 0.2246035 Vali Loss: 0.1797024 Test Loss: 0.2106933
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2277086
	speed: 1.8854s/iter; left time: 13049.1869s
Epoch: 49 cost time: 108.87488102912903
Epoch: 49, Steps: 135 | Train Loss: 0.2245583 Vali Loss: 0.1798140 Test Loss: 0.2106940
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_360_j720_H10_FITS_custom_ftM_sl360_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.20910432934761047, mae:0.2963256537914276, rse:0.4561512768268585, corr:[0.44593063 0.44883007 0.45014492 0.45041937 0.45044455 0.45050567
 0.4505041  0.45037147 0.4501455  0.4501126  0.44994313 0.44985202
 0.44985807 0.44976828 0.4496598  0.44964308 0.44958973 0.44951594
 0.4494267  0.44924015 0.4492519  0.4491537  0.44916221 0.44932425
 0.4494178  0.44952673 0.44965377 0.44961774 0.44957957 0.4494494
 0.44937012 0.4491748  0.44908762 0.44896173 0.44880447 0.4488199
 0.44881827 0.4487789  0.44877124 0.44868714 0.44865513 0.44860017
 0.44851848 0.4483644  0.4482574  0.44830015 0.44825098 0.44840634
 0.44848654 0.44856972 0.44870874 0.44862926 0.44851324 0.44843996
 0.44839263 0.44836462 0.44823036 0.4481405  0.4480789  0.44793612
 0.44792202 0.44804144 0.44802973 0.44796312 0.44794086 0.44791022
 0.44789234 0.44776076 0.44767275 0.44765118 0.4476844  0.44775024
 0.44773212 0.44778538 0.4478809  0.44777066 0.44772846 0.44768345
 0.44756222 0.4475331  0.44745448 0.4473747  0.4473041  0.44715294
 0.447142   0.4471973  0.4471515  0.44712752 0.4471399  0.44708622
 0.44711256 0.44711375 0.4469911  0.4469644  0.44701868 0.44710526
 0.44707882 0.4471977  0.44728744 0.44722044 0.44714865 0.44713184
 0.44714198 0.4470643  0.446894   0.44676825 0.44674858 0.44669703
 0.44665062 0.4465903  0.44659862 0.4466398  0.44663253 0.446618
 0.4465686  0.446437   0.44641548 0.44646576 0.4465384  0.44666147
 0.44674873 0.4468159  0.44689798 0.4469287  0.44687247 0.4468591
 0.44681865 0.44674337 0.44674945 0.44674224 0.4466116  0.44666222
 0.44668975 0.44660056 0.4465608  0.44656107 0.44662982 0.44658133
 0.4466158  0.44670665 0.44680884 0.44691077 0.44700816 0.4471995
 0.447488   0.447667   0.44780135 0.4477711  0.44777125 0.44770062
 0.4476967  0.4476989  0.44766036 0.44758692 0.44745708 0.4475156
 0.44749254 0.44741207 0.44745106 0.44745967 0.4475225  0.44751427
 0.44755703 0.4475127  0.44748503 0.4474403  0.44738877 0.44728824
 0.44700003 0.44681978 0.4467659  0.44666556 0.4464982  0.4463483
 0.44622573 0.44602004 0.44594505 0.4458261  0.44572276 0.44568667
 0.44567865 0.44567326 0.44560504 0.44563803 0.44565314 0.44563624
 0.44561562 0.44540465 0.44520488 0.44511214 0.44507563 0.4450319
 0.44497934 0.4450441  0.44511482 0.44510216 0.4449858  0.44489977
 0.4447687  0.4446237  0.44454458 0.4444278  0.44431162 0.44425616
 0.44430855 0.44432384 0.44431755 0.44430256 0.44424453 0.44425142
 0.44417608 0.44398493 0.44383612 0.44380462 0.4438157  0.44390467
 0.44394937 0.44394886 0.4440812  0.44403914 0.44390258 0.44390902
 0.4438335  0.44371387 0.44371185 0.44364008 0.44353423 0.44354784
 0.44355816 0.44348466 0.4434533  0.44348666 0.4434924  0.4433959
 0.4433946  0.44323325 0.4430763  0.44315487 0.44317755 0.44323203
 0.443247   0.44327396 0.4433685  0.44330284 0.44324115 0.4431566
 0.44305527 0.4429916  0.44291568 0.4428876  0.4427759  0.44271654
 0.44269675 0.44263786 0.44263148 0.4426477  0.442594   0.4426089
 0.4426416  0.44257227 0.44257817 0.44255602 0.4425875  0.44267783
 0.44266078 0.44272783 0.4427852  0.44268048 0.44268322 0.4426819
 0.44260326 0.44260633 0.442442   0.44227737 0.44225073 0.44220316
 0.4422032  0.4422063  0.4422061  0.44215477 0.44210795 0.44215015
 0.44207934 0.44201177 0.44196415 0.4419972  0.44211102 0.4422234
 0.4423698  0.44246545 0.44257113 0.4426081  0.4425719  0.44256365
 0.44255656 0.44246173 0.44236964 0.4422848  0.4422196  0.4421472
 0.44215223 0.44212767 0.44205344 0.4420997  0.44214565 0.44209248
 0.4421719  0.44225657 0.44230822 0.4424352  0.4425185  0.442726
 0.44303057 0.44322646 0.44333404 0.44338334 0.44341645 0.44339207
 0.44338363 0.44329703 0.44322476 0.44312695 0.44302395 0.44300273
 0.44300216 0.44299686 0.44295043 0.44295615 0.44306183 0.44299972
 0.44296187 0.44287726 0.44279876 0.44266912 0.44255477 0.44241127
 0.44207224 0.4419939  0.44190243 0.44179356 0.4417632  0.441598
 0.4414403  0.44125286 0.44107565 0.44096863 0.4409229  0.4408499
 0.44080615 0.44083607 0.4407838  0.4407424  0.44073522 0.44070038
 0.44070637 0.4406095  0.44047362 0.44043875 0.44046035 0.44046023
 0.44028124 0.4402816  0.4403671  0.44032046 0.4402229  0.44018304
 0.4400462  0.43980578 0.43972746 0.4396476  0.43950924 0.4394688
 0.4394691  0.43946615 0.43949133 0.439467   0.4394427  0.4394156
 0.4393605  0.43921992 0.43913904 0.43917483 0.43912718 0.43919402
 0.43914214 0.43912077 0.4392929  0.43933332 0.43929112 0.43914378
 0.4390729  0.43905944 0.43891665 0.43886364 0.43879786 0.43874094
 0.43880492 0.43879253 0.43876493 0.43876368 0.43867052 0.43866324
 0.4387083  0.43849912 0.4384344  0.43846866 0.4384454  0.43853903
 0.43855372 0.43854582 0.438674   0.43869364 0.438598   0.4385235
 0.43848863 0.43842372 0.43828434 0.4382646  0.438256   0.43818244
 0.4381878  0.43817934 0.43820384 0.43824467 0.43815243 0.43814543
 0.43817225 0.4380647  0.4379831  0.43803835 0.43812025 0.4380981
 0.43813166 0.43815902 0.43820468 0.4382435  0.4381973  0.43822497
 0.43825334 0.4381581  0.43805176 0.4379341  0.43789038 0.43780223
 0.43777868 0.43791652 0.4378728  0.43785015 0.43793488 0.43786734
 0.4377853  0.43772978 0.43767935 0.43770084 0.4377639  0.43792793
 0.43808663 0.438145   0.4381946  0.43827727 0.43829313 0.43827337
 0.4383265  0.4381838  0.438121   0.438083   0.43796444 0.4379966
 0.43801975 0.43797034 0.43796325 0.4380679  0.4380925  0.43802416
 0.43810546 0.43816674 0.43828094 0.43839443 0.43844944 0.43866667
 0.4390356  0.43916813 0.4391929  0.43935058 0.43936375 0.4393176
 0.4393713  0.43924102 0.4391913  0.43923828 0.4391781  0.439052
 0.43902937 0.43907484 0.43909347 0.43917432 0.43920743 0.4391978
 0.43917415 0.43901747 0.43896133 0.43886408 0.43876335 0.43859127
 0.4382404  0.4380757  0.43800756 0.43792987 0.43782076 0.4377548
 0.43764853 0.43750906 0.4373639  0.43723434 0.4372217  0.4370831
 0.43701383 0.43708616 0.4370759  0.43709597 0.43711817 0.437042
 0.4369586  0.43680757 0.43670306 0.4366461  0.43657956 0.4365002
 0.43627006 0.43625218 0.43630838 0.4363182  0.43627247 0.43617254
 0.4362015  0.4359917  0.4358056  0.43573287 0.43557924 0.43559
 0.435605   0.43566868 0.4356454  0.4355988  0.43564695 0.43557656
 0.43546668 0.4352851  0.43519878 0.43519092 0.43520975 0.4352495
 0.435086   0.43505004 0.43514132 0.43514922 0.43513656 0.43495345
 0.4348611  0.43481553 0.4346972  0.4345661  0.43454665 0.43440852
 0.43437576 0.4344269  0.43427426 0.43423107 0.43417364 0.43408135
 0.4339873  0.43380895 0.4337317  0.4337569  0.43377092 0.43376875
 0.43380845 0.43376    0.4337863  0.4338803  0.43381965 0.43378448
 0.43369892 0.43354407 0.43345118 0.4333521  0.4332802  0.4332563
 0.43329203 0.43325004 0.43323278 0.433213   0.43316692 0.43315023
 0.43314254 0.43302783 0.43299228 0.43299806 0.43298954 0.43300512
 0.43297222 0.43300563 0.43308258 0.43310118 0.43308043 0.43306857
 0.4329985  0.4328232  0.43276274 0.43275505 0.43264052 0.43265918
 0.43262106 0.43264267 0.43267366 0.43267852 0.4327011  0.43267965
 0.43266726 0.4325065  0.43250215 0.43255255 0.432619   0.43277088
 0.4328183  0.4328529  0.43291917 0.43296054 0.43304306 0.43299437
 0.43296748 0.43285528 0.43274176 0.4326239  0.4325761  0.43259445
 0.43254653 0.432534   0.43257484 0.43263793 0.43263957 0.43265998
 0.43267223 0.4327596  0.4328934  0.43308958 0.43326652 0.43336725
 0.4335725  0.4337394  0.4337218  0.433789   0.43382585 0.43382403
 0.43379185 0.4335881  0.43351597 0.43348566 0.43349347 0.43345904
 0.4334287  0.43335035 0.43341833 0.4334845  0.43350616 0.43353933
 0.43357006 0.43344104 0.43329504 0.43323696 0.43321928 0.43303466
 0.4325964  0.43236828 0.43225345 0.43215343 0.43197858 0.43185905
 0.43177408 0.43150276 0.4312669  0.43113172 0.43109354 0.43109846
 0.43104964 0.43107122 0.43103072 0.43110174 0.43118247 0.43105093
 0.43112767 0.43093497 0.42661798 0.42655134 0.4265194  0.4265166
 0.426314   0.42621192 0.42622298 0.42616317 0.42603615 0.42593268
 0.42588913 0.42553207 0.425391   0.42526656 0.42519447 0.42522964
 0.42514786 0.42518392 0.42520392 0.4252741  0.4252427  0.4252585
 0.42522737 0.42515886 0.42507446 0.42515826 0.42517164 0.4250244 ]
