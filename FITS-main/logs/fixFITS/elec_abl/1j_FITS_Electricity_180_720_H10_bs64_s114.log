Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_180_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_180_j720_H10_FITS_custom_ftM_sl180_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17513
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=90, out_features=450, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1664064000.0
params:  40950.0
Trainable parameters:  40950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7154255
	speed: 1.0006s/iter; left time: 13509.5428s
Epoch: 1 cost time: 137.50513434410095
Epoch: 1, Steps: 136 | Train Loss: 1.0085312 Vali Loss: 0.4956656 Test Loss: 0.5554684
Validation loss decreased (inf --> 0.495666).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4111424
	speed: 2.4898s/iter; left time: 33276.6150s
Epoch: 2 cost time: 138.92959189414978
Epoch: 2, Steps: 136 | Train Loss: 0.4585685 Vali Loss: 0.3138134 Test Loss: 0.3552801
Validation loss decreased (0.495666 --> 0.313813).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3237326
	speed: 2.6037s/iter; left time: 34444.3008s
Epoch: 3 cost time: 149.47511672973633
Epoch: 3, Steps: 136 | Train Loss: 0.3398423 Vali Loss: 0.2586037 Test Loss: 0.2957534
Validation loss decreased (0.313813 --> 0.258604).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2826223
	speed: 2.5632s/iter; left time: 33559.9484s
Epoch: 4 cost time: 148.0767786502838
Epoch: 4, Steps: 136 | Train Loss: 0.2980941 Vali Loss: 0.2337024 Test Loss: 0.2697597
Validation loss decreased (0.258604 --> 0.233702).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2783292
	speed: 2.6408s/iter; left time: 34216.9641s
Epoch: 5 cost time: 151.05807304382324
Epoch: 5, Steps: 136 | Train Loss: 0.2770677 Vali Loss: 0.2196396 Test Loss: 0.2550432
Validation loss decreased (0.233702 --> 0.219640).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2710728
	speed: 2.8000s/iter; left time: 35898.5564s
Epoch: 6 cost time: 168.35196495056152
Epoch: 6, Steps: 136 | Train Loss: 0.2644976 Vali Loss: 0.2105144 Test Loss: 0.2458600
Validation loss decreased (0.219640 --> 0.210514).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2484134
	speed: 2.8742s/iter; left time: 36459.3050s
Epoch: 7 cost time: 176.6017746925354
Epoch: 7, Steps: 136 | Train Loss: 0.2564086 Vali Loss: 0.2051846 Test Loss: 0.2399046
Validation loss decreased (0.210514 --> 0.205185).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2472275
	speed: 3.0729s/iter; left time: 38561.8896s
Epoch: 8 cost time: 187.85477662086487
Epoch: 8, Steps: 136 | Train Loss: 0.2512537 Vali Loss: 0.2014898 Test Loss: 0.2359352
Validation loss decreased (0.205185 --> 0.201490).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2358473
	speed: 3.1446s/iter; left time: 39034.1312s
Epoch: 9 cost time: 178.6702015399933
Epoch: 9, Steps: 136 | Train Loss: 0.2476938 Vali Loss: 0.1988561 Test Loss: 0.2332852
Validation loss decreased (0.201490 --> 0.198856).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2477682
	speed: 2.9387s/iter; left time: 36078.2092s
Epoch: 10 cost time: 184.5124089717865
Epoch: 10, Steps: 136 | Train Loss: 0.2455461 Vali Loss: 0.1977736 Test Loss: 0.2314179
Validation loss decreased (0.198856 --> 0.197774).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2438746
	speed: 3.0242s/iter; left time: 36716.9616s
Epoch: 11 cost time: 179.1496284008026
Epoch: 11, Steps: 136 | Train Loss: 0.2438670 Vali Loss: 0.1967872 Test Loss: 0.2301303
Validation loss decreased (0.197774 --> 0.196787).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2354609
	speed: 3.0500s/iter; left time: 36615.4889s
Epoch: 12 cost time: 174.75573110580444
Epoch: 12, Steps: 136 | Train Loss: 0.2427627 Vali Loss: 0.1956320 Test Loss: 0.2291916
Validation loss decreased (0.196787 --> 0.195632).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2417051
	speed: 2.8598s/iter; left time: 33942.9103s
Epoch: 13 cost time: 163.2868275642395
Epoch: 13, Steps: 136 | Train Loss: 0.2419080 Vali Loss: 0.1953229 Test Loss: 0.2284477
Validation loss decreased (0.195632 --> 0.195323).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2411858
	speed: 2.9423s/iter; left time: 34521.8827s
Epoch: 14 cost time: 175.1331398487091
Epoch: 14, Steps: 136 | Train Loss: 0.2413826 Vali Loss: 0.1950664 Test Loss: 0.2279063
Validation loss decreased (0.195323 --> 0.195066).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2389962
	speed: 2.9756s/iter; left time: 34507.9076s
Epoch: 15 cost time: 178.31238889694214
Epoch: 15, Steps: 136 | Train Loss: 0.2409247 Vali Loss: 0.1946875 Test Loss: 0.2274864
Validation loss decreased (0.195066 --> 0.194687).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2454510
	speed: 2.9176s/iter; left time: 33438.3542s
Epoch: 16 cost time: 163.7024302482605
Epoch: 16, Steps: 136 | Train Loss: 0.2404470 Vali Loss: 0.1941266 Test Loss: 0.2271177
Validation loss decreased (0.194687 --> 0.194127).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2237274
	speed: 3.1458s/iter; left time: 35626.7130s
Epoch: 17 cost time: 179.59710597991943
Epoch: 17, Steps: 136 | Train Loss: 0.2401502 Vali Loss: 0.1942458 Test Loss: 0.2268299
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2356172
	speed: 2.7967s/iter; left time: 31292.6601s
Epoch: 18 cost time: 160.54036164283752
Epoch: 18, Steps: 136 | Train Loss: 0.2399614 Vali Loss: 0.1939710 Test Loss: 0.2265622
Validation loss decreased (0.194127 --> 0.193971).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2364766
	speed: 2.7497s/iter; left time: 30392.5438s
Epoch: 19 cost time: 167.781085729599
Epoch: 19, Steps: 136 | Train Loss: 0.2396968 Vali Loss: 0.1936789 Test Loss: 0.2263784
Validation loss decreased (0.193971 --> 0.193679).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2375782
	speed: 2.7168s/iter; left time: 29659.5294s
Epoch: 20 cost time: 154.5565493106842
Epoch: 20, Steps: 136 | Train Loss: 0.2395006 Vali Loss: 0.1935557 Test Loss: 0.2261915
Validation loss decreased (0.193679 --> 0.193556).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2530628
	speed: 2.8157s/iter; left time: 30356.1152s
Epoch: 21 cost time: 165.94140577316284
Epoch: 21, Steps: 136 | Train Loss: 0.2392159 Vali Loss: 0.1937610 Test Loss: 0.2260269
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2375933
	speed: 2.9211s/iter; left time: 31095.1565s
Epoch: 22 cost time: 170.11004900932312
Epoch: 22, Steps: 136 | Train Loss: 0.2391394 Vali Loss: 0.1938157 Test Loss: 0.2258801
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2496584
	speed: 2.7628s/iter; left time: 29034.3024s
Epoch: 23 cost time: 164.7799117565155
Epoch: 23, Steps: 136 | Train Loss: 0.2390080 Vali Loss: 0.1934171 Test Loss: 0.2257634
Validation loss decreased (0.193556 --> 0.193417).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2302604
	speed: 2.6678s/iter; left time: 27673.5161s
Epoch: 24 cost time: 147.35824418067932
Epoch: 24, Steps: 136 | Train Loss: 0.2388158 Vali Loss: 0.1934846 Test Loss: 0.2256503
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2417231
	speed: 2.5728s/iter; left time: 26337.2652s
Epoch: 25 cost time: 152.60509419441223
Epoch: 25, Steps: 136 | Train Loss: 0.2387589 Vali Loss: 0.1931683 Test Loss: 0.2255814
Validation loss decreased (0.193417 --> 0.193168).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2403349
	speed: 2.4021s/iter; left time: 24264.1105s
Epoch: 26 cost time: 137.52390313148499
Epoch: 26, Steps: 136 | Train Loss: 0.2385484 Vali Loss: 0.1925956 Test Loss: 0.2254692
Validation loss decreased (0.193168 --> 0.192596).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2262045
	speed: 2.3992s/iter; left time: 23907.7216s
Epoch: 27 cost time: 139.64045453071594
Epoch: 27, Steps: 136 | Train Loss: 0.2386535 Vali Loss: 0.1929059 Test Loss: 0.2254017
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2354640
	speed: 2.1329s/iter; left time: 20964.2274s
Epoch: 28 cost time: 109.98906087875366
Epoch: 28, Steps: 136 | Train Loss: 0.2385630 Vali Loss: 0.1931190 Test Loss: 0.2253217
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2352348
	speed: 1.4626s/iter; left time: 14176.8986s
Epoch: 29 cost time: 88.36426115036011
Epoch: 29, Steps: 136 | Train Loss: 0.2384716 Vali Loss: 0.1925552 Test Loss: 0.2252689
Validation loss decreased (0.192596 --> 0.192555).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2413821
	speed: 1.6841s/iter; left time: 16095.1981s
Epoch: 30 cost time: 90.58932518959045
Epoch: 30, Steps: 136 | Train Loss: 0.2383068 Vali Loss: 0.1927802 Test Loss: 0.2252165
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2290749
	speed: 1.5924s/iter; left time: 15002.2706s
Epoch: 31 cost time: 88.78376364707947
Epoch: 31, Steps: 136 | Train Loss: 0.2383329 Vali Loss: 0.1926506 Test Loss: 0.2251661
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2448754
	speed: 1.9156s/iter; left time: 17786.0397s
Epoch: 32 cost time: 123.31487369537354
Epoch: 32, Steps: 136 | Train Loss: 0.2382410 Vali Loss: 0.1925421 Test Loss: 0.2251357
Validation loss decreased (0.192555 --> 0.192542).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2426999
	speed: 2.0090s/iter; left time: 18380.5559s
Epoch: 33 cost time: 116.09510660171509
Epoch: 33, Steps: 136 | Train Loss: 0.2382108 Vali Loss: 0.1927341 Test Loss: 0.2250816
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2389751
	speed: 2.1182s/iter; left time: 19091.2470s
Epoch: 34 cost time: 121.22799444198608
Epoch: 34, Steps: 136 | Train Loss: 0.2381629 Vali Loss: 0.1926719 Test Loss: 0.2250466
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2317982
	speed: 2.0715s/iter; left time: 18388.3978s
Epoch: 35 cost time: 127.44714379310608
Epoch: 35, Steps: 136 | Train Loss: 0.2380483 Vali Loss: 0.1927104 Test Loss: 0.2250093
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2401061
	speed: 2.0203s/iter; left time: 17659.6805s
Epoch: 36 cost time: 117.45627188682556
Epoch: 36, Steps: 136 | Train Loss: 0.2381223 Vali Loss: 0.1925809 Test Loss: 0.2249815
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2301407
	speed: 2.0576s/iter; left time: 17705.8228s
Epoch: 37 cost time: 117.48449206352234
Epoch: 37, Steps: 136 | Train Loss: 0.2379956 Vali Loss: 0.1925488 Test Loss: 0.2249685
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2451820
	speed: 2.0049s/iter; left time: 16979.2032s
Epoch: 38 cost time: 118.29577994346619
Epoch: 38, Steps: 136 | Train Loss: 0.2379938 Vali Loss: 0.1924129 Test Loss: 0.2249362
Validation loss decreased (0.192542 --> 0.192413).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2371954
	speed: 1.9952s/iter; left time: 16625.8547s
Epoch: 39 cost time: 117.40881109237671
Epoch: 39, Steps: 136 | Train Loss: 0.2379451 Vali Loss: 0.1921607 Test Loss: 0.2249168
Validation loss decreased (0.192413 --> 0.192161).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2414557
	speed: 1.9909s/iter; left time: 16319.0642s
Epoch: 40 cost time: 117.67736792564392
Epoch: 40, Steps: 136 | Train Loss: 0.2380359 Vali Loss: 0.1924080 Test Loss: 0.2249012
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2243568
	speed: 2.0092s/iter; left time: 16195.7730s
Epoch: 41 cost time: 121.41033267974854
Epoch: 41, Steps: 136 | Train Loss: 0.2379834 Vali Loss: 0.1923683 Test Loss: 0.2248844
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2395249
	speed: 2.0333s/iter; left time: 16114.0527s
Epoch: 42 cost time: 119.84751105308533
Epoch: 42, Steps: 136 | Train Loss: 0.2378885 Vali Loss: 0.1923812 Test Loss: 0.2248671
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2508799
	speed: 1.9834s/iter; left time: 15448.7433s
Epoch: 43 cost time: 110.0539128780365
Epoch: 43, Steps: 136 | Train Loss: 0.2378608 Vali Loss: 0.1922541 Test Loss: 0.2248530
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2482346
	speed: 1.9909s/iter; left time: 15236.4840s
Epoch: 44 cost time: 123.30427074432373
Epoch: 44, Steps: 136 | Train Loss: 0.2378135 Vali Loss: 0.1922284 Test Loss: 0.2248465
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2398016
	speed: 2.2312s/iter; left time: 16772.0830s
Epoch: 45 cost time: 136.00988698005676
Epoch: 45, Steps: 136 | Train Loss: 0.2379414 Vali Loss: 0.1921053 Test Loss: 0.2248253
Validation loss decreased (0.192161 --> 0.192105).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2509170
	speed: 2.5054s/iter; left time: 18492.0135s
Epoch: 46 cost time: 144.3333878517151
Epoch: 46, Steps: 136 | Train Loss: 0.2377835 Vali Loss: 0.1922050 Test Loss: 0.2248138
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2423617
	speed: 2.3043s/iter; left time: 16694.3947s
Epoch: 47 cost time: 128.68709182739258
Epoch: 47, Steps: 136 | Train Loss: 0.2378252 Vali Loss: 0.1922479 Test Loss: 0.2248055
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2309066
	speed: 2.2666s/iter; left time: 16113.3117s
Epoch: 48 cost time: 134.86252069473267
Epoch: 48, Steps: 136 | Train Loss: 0.2378841 Vali Loss: 0.1922356 Test Loss: 0.2247952
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2409827
	speed: 2.2931s/iter; left time: 15989.9063s
Epoch: 49 cost time: 129.312513589859
Epoch: 49, Steps: 136 | Train Loss: 0.2377422 Vali Loss: 0.1922692 Test Loss: 0.2247876
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2298008
	speed: 2.2799s/iter; left time: 15587.8707s
Epoch: 50 cost time: 131.22271418571472
Epoch: 50, Steps: 136 | Train Loss: 0.2378038 Vali Loss: 0.1923590 Test Loss: 0.2247795
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2326091
	speed: 1.9850s/iter; left time: 13301.3020s
Epoch: 51 cost time: 109.1128716468811
Epoch: 51, Steps: 136 | Train Loss: 0.2378462 Vali Loss: 0.1921665 Test Loss: 0.2247753
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2322078
	speed: 1.9222s/iter; left time: 12618.9323s
Epoch: 52 cost time: 111.05352091789246
Epoch: 52, Steps: 136 | Train Loss: 0.2377542 Vali Loss: 0.1924036 Test Loss: 0.2247765
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2392142
	speed: 1.9308s/iter; left time: 12413.2414s
Epoch: 53 cost time: 110.1031277179718
Epoch: 53, Steps: 136 | Train Loss: 0.2377389 Vali Loss: 0.1921938 Test Loss: 0.2247611
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2335007
	speed: 2.0468s/iter; left time: 12880.4020s
Epoch: 54 cost time: 116.5892128944397
Epoch: 54, Steps: 136 | Train Loss: 0.2377459 Vali Loss: 0.1919917 Test Loss: 0.2247560
Validation loss decreased (0.192105 --> 0.191992).  Saving model ...
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2313385
	speed: 1.8798s/iter; left time: 11574.0000s
Epoch: 55 cost time: 106.35615134239197
Epoch: 55, Steps: 136 | Train Loss: 0.2377748 Vali Loss: 0.1923168 Test Loss: 0.2247528
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2450913
	speed: 1.9135s/iter; left time: 11521.2009s
Epoch: 56 cost time: 114.50750637054443
Epoch: 56, Steps: 136 | Train Loss: 0.2377932 Vali Loss: 0.1924564 Test Loss: 0.2247507
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2450138
	speed: 1.8209s/iter; left time: 10716.1546s
Epoch: 57 cost time: 105.59732866287231
Epoch: 57, Steps: 136 | Train Loss: 0.2377031 Vali Loss: 0.1922860 Test Loss: 0.2247467
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2419609
	speed: 1.7493s/iter; left time: 10056.5603s
Epoch: 58 cost time: 99.95402121543884
Epoch: 58, Steps: 136 | Train Loss: 0.2376323 Vali Loss: 0.1921392 Test Loss: 0.2247408
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2497061
	speed: 1.9110s/iter; left time: 10726.2723s
Epoch: 59 cost time: 112.56931400299072
Epoch: 59, Steps: 136 | Train Loss: 0.2377646 Vali Loss: 0.1921045 Test Loss: 0.2247367
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2394167
	speed: 1.8215s/iter; left time: 9976.3046s
Epoch: 60 cost time: 98.86312079429626
Epoch: 60, Steps: 136 | Train Loss: 0.2377802 Vali Loss: 0.1921988 Test Loss: 0.2247432
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2387148
	speed: 1.6942s/iter; left time: 9048.5555s
Epoch: 61 cost time: 97.03276085853577
Epoch: 61, Steps: 136 | Train Loss: 0.2376929 Vali Loss: 0.1922060 Test Loss: 0.2247338
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2452373
	speed: 1.7239s/iter; left time: 8972.7792s
Epoch: 62 cost time: 101.40999031066895
Epoch: 62, Steps: 136 | Train Loss: 0.2376735 Vali Loss: 0.1922395 Test Loss: 0.2247355
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2499422
	speed: 1.7417s/iter; left time: 8828.5251s
Epoch: 63 cost time: 101.48625540733337
Epoch: 63, Steps: 136 | Train Loss: 0.2377879 Vali Loss: 0.1922205 Test Loss: 0.2247333
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2361977
	speed: 1.7319s/iter; left time: 8543.5597s
Epoch: 64 cost time: 96.96936249732971
Epoch: 64, Steps: 136 | Train Loss: 0.2377608 Vali Loss: 0.1923075 Test Loss: 0.2247325
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2472294
	speed: 1.7534s/iter; left time: 8410.8390s
Epoch: 65 cost time: 93.21648979187012
Epoch: 65, Steps: 136 | Train Loss: 0.2376812 Vali Loss: 0.1921652 Test Loss: 0.2247298
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2367148
	speed: 1.5682s/iter; left time: 7309.5655s
Epoch: 66 cost time: 91.41627740859985
Epoch: 66, Steps: 136 | Train Loss: 0.2376484 Vali Loss: 0.1919830 Test Loss: 0.2247243
Validation loss decreased (0.191992 --> 0.191983).  Saving model ...
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2333675
	speed: 1.5384s/iter; left time: 6961.1841s
Epoch: 67 cost time: 89.12548041343689
Epoch: 67, Steps: 136 | Train Loss: 0.2378149 Vali Loss: 0.1923965 Test Loss: 0.2247249
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2367429
	speed: 1.5269s/iter; left time: 6701.4632s
Epoch: 68 cost time: 91.71742630004883
Epoch: 68, Steps: 136 | Train Loss: 0.2375832 Vali Loss: 0.1919624 Test Loss: 0.2247232
Validation loss decreased (0.191983 --> 0.191962).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2377195
	speed: 1.6792s/iter; left time: 7141.6380s
Epoch: 69 cost time: 98.37685823440552
Epoch: 69, Steps: 136 | Train Loss: 0.2376661 Vali Loss: 0.1921967 Test Loss: 0.2247188
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2429750
	speed: 1.6810s/iter; left time: 6920.6600s
Epoch: 70 cost time: 96.38845896720886
Epoch: 70, Steps: 136 | Train Loss: 0.2376701 Vali Loss: 0.1919196 Test Loss: 0.2247182
Validation loss decreased (0.191962 --> 0.191920).  Saving model ...
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2401923
	speed: 1.6491s/iter; left time: 6564.9592s
Epoch: 71 cost time: 94.81228709220886
Epoch: 71, Steps: 136 | Train Loss: 0.2376638 Vali Loss: 0.1920743 Test Loss: 0.2247204
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2402482
	speed: 1.6488s/iter; left time: 6339.5579s
Epoch: 72 cost time: 94.19777250289917
Epoch: 72, Steps: 136 | Train Loss: 0.2377323 Vali Loss: 0.1922594 Test Loss: 0.2247186
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2375072
	speed: 1.6576s/iter; left time: 6148.1656s
Epoch: 73 cost time: 93.77587151527405
Epoch: 73, Steps: 136 | Train Loss: 0.2377455 Vali Loss: 0.1919784 Test Loss: 0.2247189
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.2445693
	speed: 1.6405s/iter; left time: 5861.4680s
Epoch: 74 cost time: 98.03545928001404
Epoch: 74, Steps: 136 | Train Loss: 0.2376406 Vali Loss: 0.1922892 Test Loss: 0.2247157
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2461101
	speed: 1.9015s/iter; left time: 6535.3099s
Epoch: 75 cost time: 123.36536860466003
Epoch: 75, Steps: 136 | Train Loss: 0.2376900 Vali Loss: 0.1921301 Test Loss: 0.2247146
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2319593
	speed: 2.2630s/iter; left time: 7470.1819s
Epoch: 76 cost time: 136.17085003852844
Epoch: 76, Steps: 136 | Train Loss: 0.2377695 Vali Loss: 0.1923141 Test Loss: 0.2247139
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2397515
	speed: 2.1985s/iter; left time: 6958.2192s
Epoch: 77 cost time: 130.62380647659302
Epoch: 77, Steps: 136 | Train Loss: 0.2378169 Vali Loss: 0.1919612 Test Loss: 0.2247155
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.2459125
	speed: 2.3211s/iter; left time: 7030.6866s
Epoch: 78 cost time: 142.6469166278839
Epoch: 78, Steps: 136 | Train Loss: 0.2377340 Vali Loss: 0.1922518 Test Loss: 0.2247155
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.2311363
	speed: 2.3390s/iter; left time: 6766.5980s
Epoch: 79 cost time: 136.74984788894653
Epoch: 79, Steps: 136 | Train Loss: 0.2377529 Vali Loss: 0.1919881 Test Loss: 0.2247130
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.2292636
	speed: 2.5260s/iter; left time: 6964.3145s
Epoch: 80 cost time: 156.24253702163696
Epoch: 80, Steps: 136 | Train Loss: 0.2377514 Vali Loss: 0.1921135 Test Loss: 0.2247117
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.2388034
	speed: 2.1079s/iter; left time: 5524.7407s
Epoch: 81 cost time: 116.64323353767395
Epoch: 81, Steps: 136 | Train Loss: 0.2377751 Vali Loss: 0.1920749 Test Loss: 0.2247134
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.2280663
	speed: 2.0349s/iter; left time: 5056.6078s
Epoch: 82 cost time: 113.85950827598572
Epoch: 82, Steps: 136 | Train Loss: 0.2377307 Vali Loss: 0.1918591 Test Loss: 0.2247135
Validation loss decreased (0.191920 --> 0.191859).  Saving model ...
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.2255746
	speed: 1.9872s/iter; left time: 4667.9773s
Epoch: 83 cost time: 114.8664653301239
Epoch: 83, Steps: 136 | Train Loss: 0.2376209 Vali Loss: 0.1923341 Test Loss: 0.2247126
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.2297255
	speed: 1.9412s/iter; left time: 4295.9184s
Epoch: 84 cost time: 114.34022116661072
Epoch: 84, Steps: 136 | Train Loss: 0.2376336 Vali Loss: 0.1920757 Test Loss: 0.2247123
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.2436436
	speed: 1.9869s/iter; left time: 4126.7783s
Epoch: 85 cost time: 114.78999090194702
Epoch: 85, Steps: 136 | Train Loss: 0.2377234 Vali Loss: 0.1918766 Test Loss: 0.2247107
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.2332974
	speed: 1.9814s/iter; left time: 3845.9381s
Epoch: 86 cost time: 116.1613667011261
Epoch: 86, Steps: 136 | Train Loss: 0.2374901 Vali Loss: 0.1920772 Test Loss: 0.2247130
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.2405493
	speed: 1.9616s/iter; left time: 3540.7339s
Epoch: 87 cost time: 120.82181286811829
Epoch: 87, Steps: 136 | Train Loss: 0.2376914 Vali Loss: 0.1922124 Test Loss: 0.2247124
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.2178056
	speed: 2.0983s/iter; left time: 3502.0461s
Epoch: 88 cost time: 121.96048498153687
Epoch: 88, Steps: 136 | Train Loss: 0.2376964 Vali Loss: 0.1923231 Test Loss: 0.2247118
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.766650946003179e-06
	iters: 100, epoch: 89 | loss: 0.2273667
	speed: 2.0923s/iter; left time: 3207.4898s
Epoch: 89 cost time: 120.18600535392761
Epoch: 89, Steps: 136 | Train Loss: 0.2377679 Vali Loss: 0.1920762 Test Loss: 0.2247102
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.47831839870302e-06
	iters: 100, epoch: 90 | loss: 0.2314482
	speed: 2.1249s/iter; left time: 2968.4879s
Epoch: 90 cost time: 124.0215060710907
Epoch: 90, Steps: 136 | Train Loss: 0.2376724 Vali Loss: 0.1922944 Test Loss: 0.2247096
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.204402478767869e-06
	iters: 100, epoch: 91 | loss: 0.2508068
	speed: 1.9745s/iter; left time: 2489.8470s
Epoch: 91 cost time: 112.59210419654846
Epoch: 91, Steps: 136 | Train Loss: 0.2376951 Vali Loss: 0.1922128 Test Loss: 0.2247090
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.944182354829475e-06
	iters: 100, epoch: 92 | loss: 0.2552913
	speed: 1.9630s/iter; left time: 2208.3324s
Epoch: 92 cost time: 111.10539555549622
Epoch: 92, Steps: 136 | Train Loss: 0.2376310 Vali Loss: 0.1922372 Test Loss: 0.2247099
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.696973237088e-06
	iters: 100, epoch: 93 | loss: 0.2263458
	speed: 1.8495s/iter; left time: 1829.1904s
Epoch: 93 cost time: 106.607017993927
Epoch: 93, Steps: 136 | Train Loss: 0.2376053 Vali Loss: 0.1919148 Test Loss: 0.2247100
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.462124575233601e-06
	iters: 100, epoch: 94 | loss: 0.2380220
	speed: 1.8218s/iter; left time: 1553.9547s
Epoch: 94 cost time: 108.90808439254761
Epoch: 94, Steps: 136 | Train Loss: 0.2376286 Vali Loss: 0.1918727 Test Loss: 0.2247095
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.239018346471921e-06
	iters: 100, epoch: 95 | loss: 0.2198604
	speed: 1.9674s/iter; left time: 1410.6494s
Epoch: 95 cost time: 104.81413531303406
Epoch: 95, Steps: 136 | Train Loss: 0.2376939 Vali Loss: 0.1923940 Test Loss: 0.2247088
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.027067429148324e-06
	iters: 100, epoch: 96 | loss: 0.2388342
	speed: 1.7676s/iter; left time: 1026.9996s
Epoch: 96 cost time: 105.55193185806274
Epoch: 96, Steps: 136 | Train Loss: 0.2377289 Vali Loss: 0.1919262 Test Loss: 0.2247083
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.825714057690908e-06
	iters: 100, epoch: 97 | loss: 0.2318511
	speed: 1.8112s/iter; left time: 805.9699s
Epoch: 97 cost time: 103.04180097579956
Epoch: 97, Steps: 136 | Train Loss: 0.2376011 Vali Loss: 0.1916905 Test Loss: 0.2247078
Validation loss decreased (0.191859 --> 0.191691).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
	iters: 100, epoch: 98 | loss: 0.2420422
	speed: 1.7466s/iter; left time: 539.7031s
Epoch: 98 cost time: 92.89525294303894
Epoch: 98, Steps: 136 | Train Loss: 0.2377490 Vali Loss: 0.1923364 Test Loss: 0.2247087
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.452706937066044e-06
	iters: 100, epoch: 99 | loss: 0.2291636
	speed: 1.7710s/iter; left time: 306.3765s
Epoch: 99 cost time: 102.99380612373352
Epoch: 99, Steps: 136 | Train Loss: 0.2376992 Vali Loss: 0.1922247 Test Loss: 0.2247085
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.2800715902127414e-06
	iters: 100, epoch: 100 | loss: 0.2396481
	speed: 1.7113s/iter; left time: 63.3165s
Epoch: 100 cost time: 100.39116334915161
Epoch: 100, Steps: 136 | Train Loss: 0.2376877 Vali Loss: 0.1923344 Test Loss: 0.2247086
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : Electricity_180_j720_H10_FITS_custom_ftM_sl180_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.22312241792678833, mae:0.30653664469718933, rse:0.47119319438934326, corr:[0.44598773 0.4488392  0.44776863 0.44916418 0.44895083 0.44944862
 0.44931293 0.4492561  0.44884622 0.44852918 0.4482735  0.4480061
 0.44780943 0.4476479  0.44761217 0.4475542  0.44751948 0.44745356
 0.44756046 0.44745392 0.44765583 0.44766566 0.44813326 0.4482421
 0.44798973 0.4477132  0.4476605  0.44747108 0.4473196  0.44718322
 0.44700876 0.44683203 0.44661865 0.44652206 0.4463482  0.44623995
 0.4461308  0.44612956 0.44607857 0.4460785  0.44597185 0.4459852
 0.44596335 0.445882   0.4458871  0.44586605 0.4460916  0.44617343
 0.44606087 0.4458961  0.44591072 0.44576263 0.44565454 0.4455665
 0.4455082  0.44543156 0.4453767  0.4453309  0.4452924  0.44523346
 0.44521597 0.44520125 0.44518515 0.44518322 0.44511798 0.4451226
 0.4450655  0.44497094 0.44490272 0.4448794  0.4450132  0.44509983
 0.44494697 0.44482544 0.44484064 0.44470423 0.44463456 0.44454074
 0.44453418 0.44444555 0.44438416 0.44437045 0.4443711  0.4443206
 0.44433144 0.4443048  0.44430426 0.44429275 0.44424632 0.44426432
 0.44421592 0.44414458 0.44405687 0.4440632  0.4441935  0.4442699
 0.4441426  0.44404057 0.44405323 0.44392967 0.443901   0.4438589
 0.44385573 0.44379956 0.4437807  0.44376114 0.44377312 0.4437508
 0.44378546 0.44376776 0.44376013 0.44375503 0.44371626 0.44369632
 0.44359928 0.4434862  0.44337973 0.44336304 0.44346347 0.44356558
 0.44347957 0.44346926 0.44354242 0.44350383 0.44356543 0.44356528
 0.44363314 0.4436255  0.44362485 0.44364783 0.44367057 0.44365996
 0.44377765 0.4437954  0.44375065 0.4437     0.44365877 0.44366333
 0.44365215 0.44369793 0.44372913 0.4438361  0.44406068 0.4442804
 0.44419006 0.44419143 0.4442951  0.44430694 0.4443718  0.44439974
 0.44445956 0.4444838  0.4445568  0.4446386  0.4447386  0.4447645
 0.44505477 0.44518375 0.44510058 0.44505998 0.44506752 0.44518188
 0.4453215  0.4454475  0.44556856 0.44577685 0.446168   0.44604415
 0.4451818  0.44472662 0.44442603 0.44406292 0.44378    0.44354263
 0.44340223 0.4432335  0.44307682 0.44291687 0.44279638 0.44272518
 0.44271925 0.4426279  0.44257778 0.44258434 0.44255292 0.44259673
 0.44260374 0.4425096  0.44242087 0.4424582  0.4425744  0.44254684
 0.44209445 0.441885   0.4417499  0.44155958 0.4414199  0.44127277
 0.44122186 0.44108164 0.44098595 0.4408901  0.44082773 0.44077814
 0.44081318 0.44077006 0.44078705 0.4408164  0.44080722 0.44084108
 0.4408289  0.44074047 0.44059333 0.44060412 0.4406991  0.4407056
 0.4405202  0.44041583 0.44036582 0.4402259  0.4401423  0.44004408
 0.440041   0.43994796 0.4399403  0.4399153  0.4399017  0.439871
 0.4398828  0.43985778 0.4398359  0.43983486 0.43979585 0.4397974
 0.43974966 0.4396335  0.43952656 0.4395151  0.43960643 0.43960148
 0.4394287  0.43936336 0.43935177 0.43923053 0.4391728  0.43911308
 0.43912664 0.43906912 0.43905675 0.43902382 0.43903038 0.43904194
 0.43908614 0.43904653 0.43902636 0.43903658 0.43899998 0.43899903
 0.4389598  0.4388801  0.43880054 0.43879798 0.438889   0.43889213
 0.43877757 0.4387262  0.4386969  0.43865028 0.43861532 0.43859336
 0.43861896 0.43857083 0.43856108 0.4385428  0.43858713 0.4385958
 0.43864363 0.43862104 0.43861845 0.43862253 0.43855685 0.4385387
 0.43844047 0.43831152 0.43818805 0.43819338 0.4382517  0.43832847
 0.43824378 0.43827036 0.43836364 0.4383564  0.43841997 0.43847197
 0.43853724 0.4385459  0.4385589  0.43861717 0.4386882  0.43874
 0.43887302 0.4388826  0.43883845 0.43880013 0.43874416 0.43873546
 0.43871096 0.43870878 0.4387262  0.43883732 0.4389857  0.43916428
 0.43907368 0.4391123  0.43921164 0.43925247 0.43933544 0.43937793
 0.43947497 0.43953556 0.43967247 0.43981564 0.4399784  0.4400995
 0.44041762 0.44052047 0.44042587 0.4404037  0.4403578  0.44042566
 0.4404683  0.44046935 0.44048336 0.4406108  0.44088832 0.44069117
 0.43988276 0.43954033 0.43927944 0.4389634  0.4387259  0.43852213
 0.43838888 0.4381974  0.4380276  0.43783912 0.43767822 0.4376051
 0.43756485 0.43742988 0.4373596  0.43735468 0.43731397 0.43734848
 0.43736172 0.43734896 0.43731666 0.43736207 0.4375035  0.43744695
 0.4370555  0.43687165 0.4367603  0.43660566 0.43648782 0.43638343
 0.436287   0.4361341  0.43601206 0.43588638 0.43579218 0.43569538
 0.43570018 0.43564853 0.43565226 0.43565524 0.43563002 0.4356528
 0.43561733 0.4355677  0.435516   0.43556526 0.4356545  0.43569008
 0.43554106 0.43546912 0.43547413 0.43539518 0.43533427 0.43531364
 0.43530044 0.43519777 0.43514955 0.43510398 0.43505982 0.43501282
 0.43501624 0.43497583 0.43494555 0.43494567 0.43492752 0.43494952
 0.43492162 0.43484056 0.43478113 0.43480778 0.43489733 0.43497956
 0.43480396 0.4347641  0.43479702 0.43472284 0.4347248  0.43470386
 0.43470156 0.43463016 0.43455684 0.43450508 0.4345361  0.43453655
 0.43457007 0.43452248 0.43451828 0.4345351  0.434524   0.4345276
 0.43451256 0.43446147 0.43440768 0.43441093 0.4345212  0.43460944
 0.4344866  0.43446487 0.43448713 0.4344554  0.43446162 0.43444765
 0.43445876 0.43440795 0.43436506 0.43432808 0.4343232  0.43431425
 0.43433833 0.43429843 0.43430677 0.43430936 0.4342671  0.43428385
 0.43419823 0.43411526 0.43404335 0.43406302 0.43414634 0.43427515
 0.43422025 0.43427417 0.43435645 0.43438995 0.43445095 0.4345124
 0.4346058  0.4345931  0.4345895  0.43463394 0.43466052 0.43467793
 0.43478572 0.43476948 0.43473998 0.4347348  0.4346912  0.4346964
 0.434701   0.43475124 0.43480927 0.43493477 0.435131   0.43533763
 0.435269   0.4353435  0.43545684 0.43551558 0.43560097 0.43566144
 0.43575016 0.43577552 0.43585983 0.43596798 0.43609366 0.43619373
 0.43644908 0.4364985  0.4363853  0.4363417  0.43633494 0.43640912
 0.4364846  0.43650508 0.43653327 0.43666077 0.43690026 0.4366823
 0.4358558  0.43540484 0.43509802 0.43476713 0.4345227  0.43430972
 0.43416053 0.43395254 0.43374828 0.4335468  0.43341368 0.4332826
 0.43327153 0.43315658 0.43307918 0.4330926  0.43308973 0.43312964
 0.43315756 0.43313643 0.43309042 0.4331186  0.4332118  0.43307972
 0.4325793  0.4323488  0.4322055  0.43199    0.4318646  0.43174678
 0.4316516  0.43145996 0.43128613 0.4311266  0.43104166 0.43097496
 0.43101358 0.43096673 0.43098313 0.43102878 0.43101996 0.43107527
 0.43107426 0.4310377  0.431008   0.43106472 0.43111575 0.43109003
 0.43088996 0.43070155 0.43060005 0.43041098 0.43029758 0.43021452
 0.4301878  0.4300781  0.4300099  0.42994013 0.42987463 0.4297973
 0.4297787  0.42967674 0.42965847 0.42962727 0.4295603  0.42951632
 0.4294377  0.42939326 0.42935747 0.42937258 0.42944363 0.42946395
 0.42928007 0.42919472 0.4291833  0.42910576 0.4290713  0.42902106
 0.4290411  0.42894167 0.4288851  0.42884222 0.42884725 0.42883292
 0.42888716 0.42883104 0.42885277 0.42884403 0.42881587 0.42884853
 0.42882338 0.42877728 0.42874262 0.42873874 0.42877954 0.42883322
 0.42867312 0.428605   0.42862928 0.4285449  0.4285683  0.42854786
 0.42857143 0.42849842 0.42844254 0.42844167 0.42846724 0.42847228
 0.4285157  0.42848167 0.4284912  0.42853054 0.42846584 0.42847106
 0.42842662 0.4283563  0.42826673 0.42829272 0.42831394 0.4284299
 0.4283852  0.42844588 0.4285316  0.42855406 0.42862025 0.4286863
 0.42874423 0.4287385  0.42875102 0.42877516 0.42885643 0.42892188
 0.42902905 0.42903483 0.4290224  0.42904007 0.4289849  0.42901763
 0.4289918  0.4290889  0.42911065 0.4292333  0.42935297 0.42959732
 0.4294882  0.42957604 0.42967045 0.42971957 0.4297975  0.42986938
 0.42996618 0.42999977 0.430096   0.43028367 0.4304627  0.43063888
 0.43092763 0.43106318 0.43099245 0.4309739  0.4309365  0.43105498
 0.4311058  0.43113467 0.4311228  0.4312307  0.43144798 0.4312281
 0.43037814 0.4299326  0.42961532 0.42928007 0.42907107 0.4288267
 0.42868483 0.42841578 0.42823347 0.42800373 0.42792282 0.42777938
 0.42785046 0.42772377 0.4277997  0.42778498 0.42569277 0.4278216
 0.42571715 0.42355695 0.42342475 0.4234083  0.42346865 0.4233933
 0.42289707 0.42271155 0.4225203  0.42233825 0.42220846 0.42208523
 0.4219235  0.421758   0.4216335  0.42153674 0.421481   0.4214907
 0.421691   0.42166388 0.4219024  0.4216748  0.42175648 0.42144918
 0.42144886 0.42096877 0.42107534 0.42089835 0.42132792 0.42152894]
