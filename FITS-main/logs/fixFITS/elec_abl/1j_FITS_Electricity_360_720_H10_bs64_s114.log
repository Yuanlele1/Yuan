Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=170, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_360_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_360_j720_H10_FITS_custom_ftM_sl360_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17333
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=170, out_features=510, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3562329600.0
params:  87210.0
Trainable parameters:  87210
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6757870
	speed: 0.7085s/iter; left time: 9494.8371s
Epoch: 1 cost time: 98.09407114982605
Epoch: 1, Steps: 135 | Train Loss: 0.9002342 Vali Loss: 0.4867934 Test Loss: 0.5841515
Validation loss decreased (inf --> 0.486793).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4695542
	speed: 1.6346s/iter; left time: 21684.3934s
Epoch: 2 cost time: 93.91340827941895
Epoch: 2, Steps: 135 | Train Loss: 0.5097760 Vali Loss: 0.3512105 Test Loss: 0.4271032
Validation loss decreased (0.486793 --> 0.351211).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3564501
	speed: 1.4049s/iter; left time: 18448.3260s
Epoch: 3 cost time: 73.25286531448364
Epoch: 3, Steps: 135 | Train Loss: 0.3898291 Vali Loss: 0.2764888 Test Loss: 0.3378295
Validation loss decreased (0.351211 --> 0.276489).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3115538
	speed: 1.1316s/iter; left time: 14706.5368s
Epoch: 4 cost time: 63.34827256202698
Epoch: 4, Steps: 135 | Train Loss: 0.3196008 Vali Loss: 0.2320896 Test Loss: 0.2840555
Validation loss decreased (0.276489 --> 0.232090).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2656262
	speed: 1.0949s/iter; left time: 14081.9252s
Epoch: 5 cost time: 61.88776111602783
Epoch: 5, Steps: 135 | Train Loss: 0.2779061 Vali Loss: 0.2073876 Test Loss: 0.2521785
Validation loss decreased (0.232090 --> 0.207388).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2524433
	speed: 1.0463s/iter; left time: 13314.9372s
Epoch: 6 cost time: 61.70749306678772
Epoch: 6, Steps: 135 | Train Loss: 0.2539530 Vali Loss: 0.1937201 Test Loss: 0.2338057
Validation loss decreased (0.207388 --> 0.193720).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2362234
	speed: 1.0716s/iter; left time: 13492.4312s
Epoch: 7 cost time: 63.24555063247681
Epoch: 7, Steps: 135 | Train Loss: 0.2404909 Vali Loss: 0.1868287 Test Loss: 0.2234785
Validation loss decreased (0.193720 --> 0.186829).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2381699
	speed: 1.1874s/iter; left time: 14790.2766s
Epoch: 8 cost time: 81.95415711402893
Epoch: 8, Steps: 135 | Train Loss: 0.2332286 Vali Loss: 0.1832035 Test Loss: 0.2178208
Validation loss decreased (0.186829 --> 0.183203).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2165771
	speed: 1.4886s/iter; left time: 18340.9091s
Epoch: 9 cost time: 88.29555892944336
Epoch: 9, Steps: 135 | Train Loss: 0.2294049 Vali Loss: 0.1817666 Test Loss: 0.2147365
Validation loss decreased (0.183203 --> 0.181767).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2300375
	speed: 1.8620s/iter; left time: 22690.4329s
Epoch: 10 cost time: 108.16469550132751
Epoch: 10, Steps: 135 | Train Loss: 0.2274661 Vali Loss: 0.1806704 Test Loss: 0.2130842
Validation loss decreased (0.181767 --> 0.180670).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2261117
	speed: 1.5864s/iter; left time: 19117.4135s
Epoch: 11 cost time: 91.74665093421936
Epoch: 11, Steps: 135 | Train Loss: 0.2263619 Vali Loss: 0.1809125 Test Loss: 0.2121813
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2269078
	speed: 1.8185s/iter; left time: 21668.8989s
Epoch: 12 cost time: 112.28875303268433
Epoch: 12, Steps: 135 | Train Loss: 0.2258211 Vali Loss: 0.1807052 Test Loss: 0.2117188
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2421173
	speed: 1.8354s/iter; left time: 21622.2825s
Epoch: 13 cost time: 104.54049730300903
Epoch: 13, Steps: 135 | Train Loss: 0.2256139 Vali Loss: 0.1804944 Test Loss: 0.2114164
Validation loss decreased (0.180670 --> 0.180494).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2213910
	speed: 1.8546s/iter; left time: 21598.1460s
Epoch: 14 cost time: 98.31774425506592
Epoch: 14, Steps: 135 | Train Loss: 0.2254046 Vali Loss: 0.1802026 Test Loss: 0.2112589
Validation loss decreased (0.180494 --> 0.180203).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2233633
	speed: 1.6061s/iter; left time: 18487.2812s
Epoch: 15 cost time: 90.91590571403503
Epoch: 15, Steps: 135 | Train Loss: 0.2253197 Vali Loss: 0.1806430 Test Loss: 0.2111265
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2378708
	speed: 1.5098s/iter; left time: 17175.7977s
Epoch: 16 cost time: 87.27037215232849
Epoch: 16, Steps: 135 | Train Loss: 0.2251598 Vali Loss: 0.1806537 Test Loss: 0.2110588
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2174018
	speed: 1.5162s/iter; left time: 17043.6132s
Epoch: 17 cost time: 86.30179572105408
Epoch: 17, Steps: 135 | Train Loss: 0.2251580 Vali Loss: 0.1806884 Test Loss: 0.2110319
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2350431
	speed: 1.4816s/iter; left time: 16454.1637s
Epoch: 18 cost time: 88.65296292304993
Epoch: 18, Steps: 135 | Train Loss: 0.2251427 Vali Loss: 0.1802689 Test Loss: 0.2109779
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2215375
	speed: 1.7654s/iter; left time: 19368.1326s
Epoch: 19 cost time: 109.24357748031616
Epoch: 19, Steps: 135 | Train Loss: 0.2251021 Vali Loss: 0.1803146 Test Loss: 0.2109729
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2203405
	speed: 1.9171s/iter; left time: 20773.3972s
Epoch: 20 cost time: 116.16829299926758
Epoch: 20, Steps: 135 | Train Loss: 0.2250176 Vali Loss: 0.1806022 Test Loss: 0.2109630
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2295305
	speed: 1.9290s/iter; left time: 20642.6700s
Epoch: 21 cost time: 113.72417497634888
Epoch: 21, Steps: 135 | Train Loss: 0.2249737 Vali Loss: 0.1803257 Test Loss: 0.2109389
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2289824
	speed: 1.8274s/iter; left time: 19308.4370s
Epoch: 22 cost time: 95.91384768486023
Epoch: 22, Steps: 135 | Train Loss: 0.2250264 Vali Loss: 0.1804042 Test Loss: 0.2109108
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2305636
	speed: 1.3741s/iter; left time: 14332.8279s
Epoch: 23 cost time: 78.92477464675903
Epoch: 23, Steps: 135 | Train Loss: 0.2250407 Vali Loss: 0.1803357 Test Loss: 0.2109041
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2142305
	speed: 1.3847s/iter; left time: 14257.3592s
Epoch: 24 cost time: 82.72589468955994
Epoch: 24, Steps: 135 | Train Loss: 0.2250054 Vali Loss: 0.1802465 Test Loss: 0.2108896
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2241661
	speed: 1.3898s/iter; left time: 14121.6145s
Epoch: 25 cost time: 86.57230281829834
Epoch: 25, Steps: 135 | Train Loss: 0.2249186 Vali Loss: 0.1803285 Test Loss: 0.2108631
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2253774
	speed: 1.4850s/iter; left time: 14888.2651s
Epoch: 26 cost time: 83.67292428016663
Epoch: 26, Steps: 135 | Train Loss: 0.2249747 Vali Loss: 0.1802166 Test Loss: 0.2108691
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2164938
	speed: 1.3823s/iter; left time: 13672.0768s
Epoch: 27 cost time: 80.84278559684753
Epoch: 27, Steps: 135 | Train Loss: 0.2249114 Vali Loss: 0.1803360 Test Loss: 0.2108587
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2364324
	speed: 1.4224s/iter; left time: 13876.9868s
Epoch: 28 cost time: 86.77355623245239
Epoch: 28, Steps: 135 | Train Loss: 0.2249317 Vali Loss: 0.1802101 Test Loss: 0.2108491
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2147842
	speed: 1.4009s/iter; left time: 13478.1984s
Epoch: 29 cost time: 80.3900990486145
Epoch: 29, Steps: 135 | Train Loss: 0.2248922 Vali Loss: 0.1805015 Test Loss: 0.2108505
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2253128
	speed: 1.3694s/iter; left time: 12989.9771s
Epoch: 30 cost time: 78.9882538318634
Epoch: 30, Steps: 135 | Train Loss: 0.2248975 Vali Loss: 0.1803327 Test Loss: 0.2108421
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2305939
	speed: 1.9295s/iter; left time: 18042.4283s
Epoch: 31 cost time: 169.57204914093018
Epoch: 31, Steps: 135 | Train Loss: 0.2249352 Vali Loss: 0.1803069 Test Loss: 0.2108472
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2285664
	speed: 3.2691s/iter; left time: 30128.4582s
Epoch: 32 cost time: 178.14365696907043
Epoch: 32, Steps: 135 | Train Loss: 0.2248659 Vali Loss: 0.1801547 Test Loss: 0.2108373
Validation loss decreased (0.180203 --> 0.180155).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2227939
	speed: 3.1054s/iter; left time: 28199.9385s
Epoch: 33 cost time: 179.42669892311096
Epoch: 33, Steps: 135 | Train Loss: 0.2248255 Vali Loss: 0.1801234 Test Loss: 0.2108291
Validation loss decreased (0.180155 --> 0.180123).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2186850
	speed: 2.9333s/iter; left time: 26241.3156s
Epoch: 34 cost time: 176.8208658695221
Epoch: 34, Steps: 135 | Train Loss: 0.2248936 Vali Loss: 0.1802456 Test Loss: 0.2108300
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2286340
	speed: 3.2375s/iter; left time: 28525.9393s
Epoch: 35 cost time: 188.63019132614136
Epoch: 35, Steps: 135 | Train Loss: 0.2248756 Vali Loss: 0.1799000 Test Loss: 0.2108189
Validation loss decreased (0.180123 --> 0.179900).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2202089
	speed: 3.0911s/iter; left time: 26818.1187s
Epoch: 36 cost time: 182.07755613327026
Epoch: 36, Steps: 135 | Train Loss: 0.2248173 Vali Loss: 0.1801123 Test Loss: 0.2108168
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2205872
	speed: 3.1558s/iter; left time: 26953.8011s
Epoch: 37 cost time: 185.3665361404419
Epoch: 37, Steps: 135 | Train Loss: 0.2248803 Vali Loss: 0.1802078 Test Loss: 0.2108112
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2222225
	speed: 2.4932s/iter; left time: 20957.5708s
Epoch: 38 cost time: 133.0566349029541
Epoch: 38, Steps: 135 | Train Loss: 0.2248625 Vali Loss: 0.1802277 Test Loss: 0.2108138
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2179457
	speed: 2.5044s/iter; left time: 20714.0728s
Epoch: 39 cost time: 178.84716272354126
Epoch: 39, Steps: 135 | Train Loss: 0.2247666 Vali Loss: 0.1800297 Test Loss: 0.2108033
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2368932
	speed: 3.1910s/iter; left time: 25961.6882s
Epoch: 40 cost time: 189.04482412338257
Epoch: 40, Steps: 135 | Train Loss: 0.2248403 Vali Loss: 0.1800867 Test Loss: 0.2107999
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2343638
	speed: 3.2192s/iter; left time: 25757.1364s
Epoch: 41 cost time: 182.68828082084656
Epoch: 41, Steps: 135 | Train Loss: 0.2248575 Vali Loss: 0.1802950 Test Loss: 0.2108000
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2203266
	speed: 3.2123s/iter; left time: 25268.1326s
Epoch: 42 cost time: 185.14232277870178
Epoch: 42, Steps: 135 | Train Loss: 0.2247719 Vali Loss: 0.1801463 Test Loss: 0.2107992
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2220779
	speed: 2.6194s/iter; left time: 20250.5972s
Epoch: 43 cost time: 120.55010771751404
Epoch: 43, Steps: 135 | Train Loss: 0.2247995 Vali Loss: 0.1802433 Test Loss: 0.2107917
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2217887
	speed: 1.6937s/iter; left time: 12865.4985s
Epoch: 44 cost time: 103.029709815979
Epoch: 44, Steps: 135 | Train Loss: 0.2248187 Vali Loss: 0.1799020 Test Loss: 0.2108015
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2390288
	speed: 2.1193s/iter; left time: 15811.7786s
Epoch: 45 cost time: 123.0397629737854
Epoch: 45, Steps: 135 | Train Loss: 0.2247888 Vali Loss: 0.1799148 Test Loss: 0.2107975
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2251107
	speed: 2.8945s/iter; left time: 21205.0961s
Epoch: 46 cost time: 171.12773537635803
Epoch: 46, Steps: 135 | Train Loss: 0.2248063 Vali Loss: 0.1801850 Test Loss: 0.2107849
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2282486
	speed: 2.6349s/iter; left time: 18947.2734s
Epoch: 47 cost time: 125.75531721115112
Epoch: 47, Steps: 135 | Train Loss: 0.2247841 Vali Loss: 0.1798079 Test Loss: 0.2107899
Validation loss decreased (0.179900 --> 0.179808).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2223943
	speed: 2.2229s/iter; left time: 15684.4855s
Epoch: 48 cost time: 123.1578598022461
Epoch: 48, Steps: 135 | Train Loss: 0.2247733 Vali Loss: 0.1799553 Test Loss: 0.2107890
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2139840
	speed: 2.4250s/iter; left time: 16783.4203s
Epoch: 49 cost time: 167.18293261528015
Epoch: 49, Steps: 135 | Train Loss: 0.2248198 Vali Loss: 0.1801360 Test Loss: 0.2107821
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2218113
	speed: 2.4173s/iter; left time: 16404.1287s
Epoch: 50 cost time: 130.10209560394287
Epoch: 50, Steps: 135 | Train Loss: 0.2248190 Vali Loss: 0.1797146 Test Loss: 0.2107847
Validation loss decreased (0.179808 --> 0.179715).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2258334
	speed: 2.4721s/iter; left time: 16441.8389s
Epoch: 51 cost time: 168.90695190429688
Epoch: 51, Steps: 135 | Train Loss: 0.2247799 Vali Loss: 0.1802512 Test Loss: 0.2107801
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2151746
	speed: 2.7848s/iter; left time: 18146.0752s
Epoch: 52 cost time: 157.36575317382812
Epoch: 52, Steps: 135 | Train Loss: 0.2248157 Vali Loss: 0.1801188 Test Loss: 0.2107816
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2382195
	speed: 2.7710s/iter; left time: 17681.4995s
Epoch: 53 cost time: 155.22173166275024
Epoch: 53, Steps: 135 | Train Loss: 0.2247614 Vali Loss: 0.1800286 Test Loss: 0.2107833
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2219447
	speed: 2.7410s/iter; left time: 17120.0522s
Epoch: 54 cost time: 165.6724956035614
Epoch: 54, Steps: 135 | Train Loss: 0.2248215 Vali Loss: 0.1801104 Test Loss: 0.2107760
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2359315
	speed: 2.6997s/iter; left time: 16498.0908s
Epoch: 55 cost time: 154.57324314117432
Epoch: 55, Steps: 135 | Train Loss: 0.2247901 Vali Loss: 0.1800234 Test Loss: 0.2107763
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2160284
	speed: 2.7664s/iter; left time: 16532.1928s
Epoch: 56 cost time: 165.2947862148285
Epoch: 56, Steps: 135 | Train Loss: 0.2247694 Vali Loss: 0.1801310 Test Loss: 0.2107711
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2193461
	speed: 2.8607s/iter; left time: 16709.3310s
Epoch: 57 cost time: 163.01457810401917
Epoch: 57, Steps: 135 | Train Loss: 0.2248014 Vali Loss: 0.1800835 Test Loss: 0.2107682
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2326709
	speed: 2.7687s/iter; left time: 15797.9377s
Epoch: 58 cost time: 162.04096484184265
Epoch: 58, Steps: 135 | Train Loss: 0.2247167 Vali Loss: 0.1800900 Test Loss: 0.2107669
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2202498
	speed: 2.9032s/iter; left time: 16173.8649s
Epoch: 59 cost time: 167.5452060699463
Epoch: 59, Steps: 135 | Train Loss: 0.2247611 Vali Loss: 0.1801627 Test Loss: 0.2107674
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2236215
	speed: 2.8311s/iter; left time: 15390.0704s
Epoch: 60 cost time: 168.13847708702087
Epoch: 60, Steps: 135 | Train Loss: 0.2247572 Vali Loss: 0.1804106 Test Loss: 0.2107686
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2197311
	speed: 2.8493s/iter; left time: 15103.8861s
Epoch: 61 cost time: 162.44440078735352
Epoch: 61, Steps: 135 | Train Loss: 0.2247735 Vali Loss: 0.1803629 Test Loss: 0.2107718
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2250268
	speed: 2.7507s/iter; left time: 14210.2932s
Epoch: 62 cost time: 160.29889607429504
Epoch: 62, Steps: 135 | Train Loss: 0.2247954 Vali Loss: 0.1800019 Test Loss: 0.2107649
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2158196
	speed: 2.8378s/iter; left time: 14276.8356s
Epoch: 63 cost time: 170.2602550983429
Epoch: 63, Steps: 135 | Train Loss: 0.2247334 Vali Loss: 0.1800207 Test Loss: 0.2107677
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2215045
	speed: 2.7412s/iter; left time: 13420.8148s
Epoch: 64 cost time: 158.21277570724487
Epoch: 64, Steps: 135 | Train Loss: 0.2247816 Vali Loss: 0.1800599 Test Loss: 0.2107635
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2282619
	speed: 2.7040s/iter; left time: 12873.6667s
Epoch: 65 cost time: 156.11574053764343
Epoch: 65, Steps: 135 | Train Loss: 0.2247354 Vali Loss: 0.1800797 Test Loss: 0.2107665
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2361280
	speed: 2.7701s/iter; left time: 12814.5255s
Epoch: 66 cost time: 161.79695892333984
Epoch: 66, Steps: 135 | Train Loss: 0.2247456 Vali Loss: 0.1797315 Test Loss: 0.2107630
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2299239
	speed: 2.7472s/iter; left time: 12337.5050s
Epoch: 67 cost time: 158.42390847206116
Epoch: 67, Steps: 135 | Train Loss: 0.2247135 Vali Loss: 0.1802258 Test Loss: 0.2107666
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2262615
	speed: 2.7200s/iter; left time: 11848.4090s
Epoch: 68 cost time: 160.5360689163208
Epoch: 68, Steps: 135 | Train Loss: 0.2247729 Vali Loss: 0.1801191 Test Loss: 0.2107624
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2351450
	speed: 2.6847s/iter; left time: 11331.9229s
Epoch: 69 cost time: 153.77709794044495
Epoch: 69, Steps: 135 | Train Loss: 0.2246812 Vali Loss: 0.1801879 Test Loss: 0.2107627
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2238569
	speed: 2.5388s/iter; left time: 10373.7099s
Epoch: 70 cost time: 140.47162342071533
Epoch: 70, Steps: 135 | Train Loss: 0.2247160 Vali Loss: 0.1801865 Test Loss: 0.2107638
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_360_j720_H10_FITS_custom_ftM_sl360_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.20919230580329895, mae:0.2966620922088623, rse:0.4562472403049469, corr:[0.44646958 0.44801417 0.4488909  0.45025763 0.4503345  0.45079905
 0.45078826 0.45050967 0.45039213 0.44977257 0.4497148  0.44932628
 0.44928366 0.44925493 0.44910228 0.44929758 0.4491677  0.44938335
 0.44920313 0.44920743 0.44904238 0.44926342 0.44920707 0.44942477
 0.44955108 0.4496368  0.44984698 0.4496782  0.44969735 0.4494201
 0.44928205 0.44902855 0.4487821  0.44877273 0.44852015 0.4485839
 0.44850504 0.4485255  0.4486245  0.44858253 0.44868982 0.44861493
 0.44868013 0.44849437 0.44851375 0.4484967  0.4485148  0.44859985
 0.44861442 0.44875666 0.4486963  0.44866565 0.44844538 0.44837183
 0.44827652 0.44808742 0.4481009  0.44793078 0.44797748 0.4479626
 0.44794735 0.44798502 0.44795895 0.4480655  0.44800475 0.4480461
 0.4479605  0.44790235 0.44780585 0.44777074 0.44781485 0.4477881
 0.44785035 0.44780627 0.44786316 0.44769    0.44758087 0.447489
 0.44737238 0.44736588 0.44723013 0.44722715 0.44718266 0.4472
 0.44723445 0.44715044 0.44724414 0.44722402 0.44727603 0.44720897
 0.44722092 0.447144   0.44701988 0.44711807 0.44706348 0.447162
 0.44712928 0.44719753 0.44716144 0.44710833 0.4470669  0.44689307
 0.44692338 0.4468249  0.44678828 0.44676816 0.44670588 0.4467295
 0.44666293 0.44673553 0.44672945 0.44681343 0.44679973 0.44674113
 0.4467837  0.4466023  0.4465621  0.44653192 0.44667315 0.44670793
 0.4468354  0.44686225 0.44681147 0.44686726 0.44676852 0.44678116
 0.4467343  0.446694   0.4466572  0.4466262  0.44666684 0.44663224
 0.4466718  0.4466625  0.44667116 0.44670737 0.4466473  0.4467027
 0.44665787 0.4467166  0.44679752 0.44691563 0.44699973 0.4471389
 0.44750744 0.44759715 0.44766924 0.44762734 0.44769973 0.44765276
 0.4476501  0.44763422 0.4475548  0.44763783 0.44753402 0.44752577
 0.4475331  0.44747725 0.44755366 0.4475166  0.44756734 0.44751343
 0.4475542  0.44747153 0.4474177  0.44738448 0.44727898 0.44711253
 0.4467541  0.44668192 0.44658324 0.44644824 0.44635573 0.4461988
 0.4461698  0.44599736 0.44596443 0.44590092 0.44584894 0.44584647
 0.44577673 0.4457717  0.44572273 0.4457298  0.44568095 0.4456721
 0.44559142 0.44533914 0.4452     0.44502518 0.44502953 0.4449341
 0.44487926 0.4448963  0.44490182 0.44491026 0.44477728 0.44478267
 0.44468337 0.44463784 0.44457507 0.44452217 0.44450969 0.4444158
 0.44445094 0.4444253  0.44443065 0.44441068 0.44435382 0.4443796
 0.44427416 0.44412535 0.44392812 0.4438536  0.443832   0.4438478
 0.44387627 0.44391894 0.4440465  0.44397354 0.44398865 0.44390342
 0.4438734  0.44386432 0.44377816 0.4438129  0.44370726 0.44372326
 0.4436798  0.44359618 0.44359928 0.44351038 0.44352186 0.4434519
 0.4434416  0.44327772 0.44313276 0.44309196 0.44303906 0.4430749
 0.44306457 0.44317016 0.44318125 0.4432105  0.44316816 0.4430788
 0.44310033 0.4429983  0.44303462 0.4429539  0.44288132 0.44287902
 0.4428172  0.44285986 0.44279417 0.442803   0.44276404 0.44272363
 0.44266486 0.4425727  0.4425486  0.44246295 0.44249842 0.44250092
 0.442564   0.44259906 0.44263092 0.44268396 0.4425997  0.44261605
 0.4425344  0.4425204  0.44246042 0.4423595  0.44239357 0.442299
 0.44230583 0.442258   0.44230005 0.44231462 0.44224393 0.44225112
 0.44213358 0.44206378 0.441958   0.4419906  0.44203594 0.44211516
 0.44224495 0.4423237  0.4424278  0.4424365  0.44248682 0.44243875
 0.4424683  0.4424535  0.44236532 0.44240057 0.4423071  0.44230795
 0.44225422 0.44217193 0.44221142 0.44218042 0.4421921  0.4420841
 0.4421664  0.44220775 0.44226092 0.44238952 0.44249701 0.44263446
 0.44292274 0.4431538  0.44315895 0.4432514  0.4432505  0.44324455
 0.4432941  0.44322062 0.4432327  0.4431454  0.44310492 0.4430982
 0.44302306 0.4430425  0.44299808 0.4429918  0.44295016 0.4429296
 0.44290632 0.44276464 0.44268957 0.44256413 0.44252363 0.44225776
 0.44200125 0.44191986 0.44189963 0.44183555 0.44168153 0.44164407
 0.44150543 0.4414285  0.44130272 0.4411602  0.44110915 0.44097227
 0.44096294 0.44086856 0.44087234 0.4408583  0.44077682 0.440792
 0.4406739  0.44055128 0.44044298 0.44041818 0.44039857 0.44037387
 0.44026995 0.44030443 0.4403525  0.44030574 0.44031066 0.44021013
 0.44016612 0.44004223 0.43991622 0.43987206 0.43971184 0.4396981
 0.43963543 0.43957824 0.43960884 0.43955216 0.43953204 0.43939912
 0.4393761  0.4392763  0.43916884 0.43914387 0.4391087  0.43915108
 0.43913043 0.43922958 0.4393074  0.43935278 0.43930656 0.43928042
 0.4392962  0.43914238 0.43909264 0.43899244 0.43891463 0.43889564
 0.43880138 0.43876034 0.43870035 0.43871486 0.43866652 0.43865258
 0.43858325 0.4384684  0.43839836 0.43835032 0.43843243 0.43837628
 0.43847322 0.4385282  0.43862087 0.43869373 0.4386266  0.4386754
 0.43860996 0.4385749  0.4384574  0.4383732  0.43835822 0.4382562
 0.43826368 0.43818408 0.4382559  0.43822542 0.4381605  0.43818003
 0.43809432 0.43810683 0.43799612 0.4380322  0.43807527 0.43812662
 0.43814987 0.43817696 0.438313   0.43829036 0.43833914 0.43834034
 0.4383256  0.43827274 0.4381402  0.43805993 0.4379518  0.43792537
 0.437848   0.43783134 0.43782893 0.43780357 0.4378143  0.43771303
 0.4377489  0.43761843 0.43760073 0.43762854 0.43772814 0.4378551
 0.4379975  0.43819883 0.43822655 0.43838328 0.43838176 0.43837154
 0.43841857 0.4383109  0.43832812 0.43819496 0.43811002 0.43807235
 0.43795216 0.43799746 0.43792853 0.4379691  0.43793216 0.43794477
 0.43800113 0.43804458 0.4381654  0.4382765  0.43851584 0.43859896
 0.4389826  0.43913317 0.43924463 0.4393328  0.4393134  0.4394082
 0.43933886 0.4393222  0.43927947 0.43921748 0.439159   0.43902934
 0.43903828 0.43896633 0.43903327 0.4390056  0.43896484 0.43900985
 0.43896088 0.43888694 0.4387863  0.43875825 0.43862033 0.43847874
 0.4382165  0.43812197 0.4381479  0.43802375 0.4380079  0.43786922
 0.43776524 0.43761772 0.43740246 0.43734473 0.4371503  0.43705535
 0.4370056  0.4369601  0.43691677 0.43684727 0.4368867  0.43683228
 0.43683693 0.43664154 0.4365379  0.43648723 0.4364774  0.4365013
 0.43633047 0.43636978 0.43637115 0.43641636 0.43633115 0.43628836
 0.43623692 0.43598574 0.43591678 0.43569297 0.43558368 0.43551764
 0.43542498 0.43543738 0.43538544 0.43542483 0.4353148  0.43533462
 0.43526465 0.43511936 0.43506935 0.43502983 0.43516245 0.4351419
 0.4352164  0.43517554 0.43524837 0.4352481  0.43512526 0.43514612
 0.43498534 0.43494192 0.4347779  0.4346146  0.43446463 0.43423006
 0.434234   0.43404254 0.43403366 0.43396312 0.43387857 0.43381983
 0.43372485 0.43369377 0.43353873 0.4336265  0.43366554 0.43372852
 0.43377993 0.4337992  0.43396732 0.43393657 0.43398425 0.43389395
 0.4338539  0.43374553 0.43354782 0.4334581  0.43324015 0.43324482
 0.43309566 0.43305686 0.43307158 0.43301976 0.43305153 0.43294677
 0.43303144 0.43291235 0.43296352 0.4329675  0.43303275 0.43312755
 0.43312943 0.4332653  0.43326372 0.43333292 0.43327403 0.43323374
 0.4332024  0.43299    0.4328778  0.43262577 0.43255684 0.43246514
 0.4323728  0.43238464 0.43232638 0.43246186 0.43237203 0.4324852
 0.43244705 0.4324219  0.4324092  0.43246645 0.432684   0.43271086
 0.433005   0.43307456 0.43320835 0.43324178 0.43320116 0.43316936
 0.43301475 0.4329498  0.43271196 0.4326101  0.43248454 0.4323483
 0.4324356  0.4323178  0.43241632 0.43235487 0.4324589  0.43246886
 0.43254894 0.4327544  0.43279696 0.43313265 0.43325424 0.433516
 0.43375295 0.43392742 0.43399215 0.43394238 0.4339728  0.43377277
 0.43377823 0.4335502  0.43338516 0.4332759  0.4330375  0.43311453
 0.4329547  0.43309987 0.4331042  0.43316832 0.43329138 0.43324164
 0.43344906 0.43329078 0.43337822 0.43328416 0.43336394 0.43318737
 0.43278477 0.43265867 0.4324243  0.43240377 0.43205282 0.43190825
 0.4315716  0.43129775 0.43114173 0.43080547 0.4308195  0.43058357
 0.43077844 0.4307127  0.43082938 0.4310211  0.43099466 0.43118817
 0.43106613 0.43108    0.4308189  0.43088776 0.4308961  0.4307051
 0.4306348  0.43022528 0.4303707  0.42988086 0.42986637 0.429469
 0.42937145 0.4291952  0.42896715 0.42907923 0.42883268 0.4293982
 0.42512205 0.4297384  0.42557016 0.42982543 0.4257523  0.42926633
 0.42531142 0.42390826 0.4245957  0.4231102  0.42412704 0.42572927]
