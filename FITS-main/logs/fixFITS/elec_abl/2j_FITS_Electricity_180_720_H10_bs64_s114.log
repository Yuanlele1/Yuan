Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_180_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_180_j720_H10_FITS_custom_ftM_sl180_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17513
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=90, out_features=450, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1664064000.0
params:  40950.0
Trainable parameters:  40950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8048471
	speed: 1.0007s/iter; left time: 13510.2778s
Epoch: 1 cost time: 137.47246527671814
Epoch: 1, Steps: 136 | Train Loss: 1.1075605 Vali Loss: 0.6250833 Test Loss: 0.6957530
Validation loss decreased (inf --> 0.625083).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4795686
	speed: 2.5217s/iter; left time: 33703.1248s
Epoch: 2 cost time: 150.53611278533936
Epoch: 2, Steps: 136 | Train Loss: 0.5342601 Vali Loss: 0.4162852 Test Loss: 0.4655250
Validation loss decreased (0.625083 --> 0.416285).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3677384
	speed: 2.3359s/iter; left time: 30901.0100s
Epoch: 3 cost time: 134.42907285690308
Epoch: 3, Steps: 136 | Train Loss: 0.3895624 Vali Loss: 0.3383749 Test Loss: 0.3780871
Validation loss decreased (0.416285 --> 0.338375).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3130861
	speed: 2.2417s/iter; left time: 29349.9474s
Epoch: 4 cost time: 125.60662245750427
Epoch: 4, Steps: 136 | Train Loss: 0.3292176 Vali Loss: 0.3009456 Test Loss: 0.3371436
Validation loss decreased (0.338375 --> 0.300946).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2960511
	speed: 1.9788s/iter; left time: 25638.9355s
Epoch: 5 cost time: 106.1921763420105
Epoch: 5, Steps: 136 | Train Loss: 0.2972662 Vali Loss: 0.2784827 Test Loss: 0.3134504
Validation loss decreased (0.300946 --> 0.278483).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2789830
	speed: 1.9274s/iter; left time: 24711.3972s
Epoch: 6 cost time: 110.2790584564209
Epoch: 6, Steps: 136 | Train Loss: 0.2768747 Vali Loss: 0.2625809 Test Loss: 0.2974669
Validation loss decreased (0.278483 --> 0.262581).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2541161
	speed: 1.9583s/iter; left time: 24840.4486s
Epoch: 7 cost time: 107.8543770313263
Epoch: 7, Steps: 136 | Train Loss: 0.2621650 Vali Loss: 0.2510950 Test Loss: 0.2857155
Validation loss decreased (0.262581 --> 0.251095).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2466618
	speed: 2.3995s/iter; left time: 30111.7961s
Epoch: 8 cost time: 146.71015453338623
Epoch: 8, Steps: 136 | Train Loss: 0.2510260 Vali Loss: 0.2416403 Test Loss: 0.2764795
Validation loss decreased (0.251095 --> 0.241640).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2323727
	speed: 2.6155s/iter; left time: 32466.4293s
Epoch: 9 cost time: 148.43012070655823
Epoch: 9, Steps: 136 | Train Loss: 0.2421006 Vali Loss: 0.2338052 Test Loss: 0.2691475
Validation loss decreased (0.241640 --> 0.233805).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2350252
	speed: 2.6080s/iter; left time: 32018.3455s
Epoch: 10 cost time: 147.95451140403748
Epoch: 10, Steps: 136 | Train Loss: 0.2350696 Vali Loss: 0.2281969 Test Loss: 0.2632447
Validation loss decreased (0.233805 --> 0.228197).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2290788
	speed: 2.7011s/iter; left time: 32793.9178s
Epoch: 11 cost time: 157.52239656448364
Epoch: 11, Steps: 136 | Train Loss: 0.2291063 Vali Loss: 0.2232395 Test Loss: 0.2583220
Validation loss decreased (0.228197 --> 0.223239).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2184366
	speed: 2.4886s/iter; left time: 29875.3766s
Epoch: 12 cost time: 147.67502975463867
Epoch: 12, Steps: 136 | Train Loss: 0.2242167 Vali Loss: 0.2186358 Test Loss: 0.2542551
Validation loss decreased (0.223239 --> 0.218636).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2187774
	speed: 2.5838s/iter; left time: 30667.6034s
Epoch: 13 cost time: 149.7979052066803
Epoch: 13, Steps: 136 | Train Loss: 0.2200631 Vali Loss: 0.2153457 Test Loss: 0.2507659
Validation loss decreased (0.218636 --> 0.215346).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2167874
	speed: 2.5807s/iter; left time: 30279.5365s
Epoch: 14 cost time: 149.73749446868896
Epoch: 14, Steps: 136 | Train Loss: 0.2166340 Vali Loss: 0.2125656 Test Loss: 0.2478184
Validation loss decreased (0.215346 --> 0.212566).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2104818
	speed: 2.6388s/iter; left time: 30602.4857s
Epoch: 15 cost time: 148.1619336605072
Epoch: 15, Steps: 136 | Train Loss: 0.2136654 Vali Loss: 0.2099955 Test Loss: 0.2452828
Validation loss decreased (0.212566 --> 0.209996).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2152372
	speed: 2.4885s/iter; left time: 28520.2139s
Epoch: 16 cost time: 147.3620138168335
Epoch: 16, Steps: 136 | Train Loss: 0.2110282 Vali Loss: 0.2076001 Test Loss: 0.2431630
Validation loss decreased (0.209996 --> 0.207600).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1947796
	speed: 2.5473s/iter; left time: 28848.4769s
Epoch: 17 cost time: 149.12478613853455
Epoch: 17, Steps: 136 | Train Loss: 0.2088209 Vali Loss: 0.2060869 Test Loss: 0.2412756
Validation loss decreased (0.207600 --> 0.206087).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2024906
	speed: 2.5944s/iter; left time: 29028.6142s
Epoch: 18 cost time: 147.1726086139679
Epoch: 18, Steps: 136 | Train Loss: 0.2069576 Vali Loss: 0.2043953 Test Loss: 0.2396062
Validation loss decreased (0.206087 --> 0.204395).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2020770
	speed: 2.5439s/iter; left time: 28117.7003s
Epoch: 19 cost time: 147.28254508972168
Epoch: 19, Steps: 136 | Train Loss: 0.2052543 Vali Loss: 0.2029441 Test Loss: 0.2382114
Validation loss decreased (0.204395 --> 0.202944).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2018719
	speed: 2.5974s/iter; left time: 28356.1591s
Epoch: 20 cost time: 154.07111883163452
Epoch: 20, Steps: 136 | Train Loss: 0.2037864 Vali Loss: 0.2017266 Test Loss: 0.2369097
Validation loss decreased (0.202944 --> 0.201727).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2134504
	speed: 2.5303s/iter; left time: 27278.7960s
Epoch: 21 cost time: 144.09249067306519
Epoch: 21, Steps: 136 | Train Loss: 0.2024077 Vali Loss: 0.2010580 Test Loss: 0.2358014
Validation loss decreased (0.201727 --> 0.201058).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1994242
	speed: 2.5591s/iter; left time: 27242.0121s
Epoch: 22 cost time: 148.94592690467834
Epoch: 22, Steps: 136 | Train Loss: 0.2013339 Vali Loss: 0.2002982 Test Loss: 0.2347886
Validation loss decreased (0.201058 --> 0.200298).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2091561
	speed: 2.5614s/iter; left time: 26917.5688s
Epoch: 23 cost time: 142.44739699363708
Epoch: 23, Steps: 136 | Train Loss: 0.2003369 Vali Loss: 0.1992256 Test Loss: 0.2339190
Validation loss decreased (0.200298 --> 0.199226).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1924205
	speed: 2.5382s/iter; left time: 26329.0630s
Epoch: 24 cost time: 150.76391983032227
Epoch: 24, Steps: 136 | Train Loss: 0.1993863 Vali Loss: 0.1987086 Test Loss: 0.2331389
Validation loss decreased (0.199226 --> 0.198709).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2008107
	speed: 2.5181s/iter; left time: 25777.5092s
Epoch: 25 cost time: 141.67543721199036
Epoch: 25, Steps: 136 | Train Loss: 0.1986394 Vali Loss: 0.1978502 Test Loss: 0.2324558
Validation loss decreased (0.198709 --> 0.197850).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1990901
	speed: 2.5144s/iter; left time: 25398.1819s
Epoch: 26 cost time: 141.18364930152893
Epoch: 26, Steps: 136 | Train Loss: 0.1978461 Vali Loss: 0.1968124 Test Loss: 0.2317864
Validation loss decreased (0.197850 --> 0.196812).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1875879
	speed: 2.4948s/iter; left time: 24861.0682s
Epoch: 27 cost time: 143.32486248016357
Epoch: 27, Steps: 136 | Train Loss: 0.1973659 Vali Loss: 0.1967282 Test Loss: 0.2312342
Validation loss decreased (0.196812 --> 0.196728).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1939965
	speed: 2.5237s/iter; left time: 24805.7114s
Epoch: 28 cost time: 143.40003895759583
Epoch: 28, Steps: 136 | Train Loss: 0.1967992 Vali Loss: 0.1965931 Test Loss: 0.2307063
Validation loss decreased (0.196728 --> 0.196593).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1934789
	speed: 2.3591s/iter; left time: 22867.2185s
Epoch: 29 cost time: 133.28013944625854
Epoch: 29, Steps: 136 | Train Loss: 0.1962836 Vali Loss: 0.1957134 Test Loss: 0.2302466
Validation loss decreased (0.196593 --> 0.195713).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1980402
	speed: 2.3600s/iter; left time: 22554.8751s
Epoch: 30 cost time: 132.89559054374695
Epoch: 30, Steps: 136 | Train Loss: 0.1957507 Vali Loss: 0.1956558 Test Loss: 0.2298488
Validation loss decreased (0.195713 --> 0.195656).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1881623
	speed: 2.2393s/iter; left time: 21096.7723s
Epoch: 31 cost time: 124.68736481666565
Epoch: 31, Steps: 136 | Train Loss: 0.1954111 Vali Loss: 0.1952688 Test Loss: 0.2294490
Validation loss decreased (0.195656 --> 0.195269).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2001715
	speed: 2.1609s/iter; left time: 20064.2155s
Epoch: 32 cost time: 125.73129749298096
Epoch: 32, Steps: 136 | Train Loss: 0.1950198 Vali Loss: 0.1949374 Test Loss: 0.2291226
Validation loss decreased (0.195269 --> 0.194937).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1980115
	speed: 2.1737s/iter; left time: 19886.8820s
Epoch: 33 cost time: 125.39194393157959
Epoch: 33, Steps: 136 | Train Loss: 0.1947078 Vali Loss: 0.1949367 Test Loss: 0.2288081
Validation loss decreased (0.194937 --> 0.194937).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1953197
	speed: 2.1528s/iter; left time: 19403.2203s
Epoch: 34 cost time: 123.06410193443298
Epoch: 34, Steps: 136 | Train Loss: 0.1944083 Vali Loss: 0.1947277 Test Loss: 0.2285353
Validation loss decreased (0.194937 --> 0.194728).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1891426
	speed: 2.2085s/iter; left time: 19605.0509s
Epoch: 35 cost time: 127.48614835739136
Epoch: 35, Steps: 136 | Train Loss: 0.1940838 Vali Loss: 0.1945715 Test Loss: 0.2282527
Validation loss decreased (0.194728 --> 0.194572).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1950173
	speed: 2.1911s/iter; left time: 19152.0097s
Epoch: 36 cost time: 128.67148852348328
Epoch: 36, Steps: 136 | Train Loss: 0.1939320 Vali Loss: 0.1943283 Test Loss: 0.2280326
Validation loss decreased (0.194572 --> 0.194328).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1873184
	speed: 2.2390s/iter; left time: 19266.3904s
Epoch: 37 cost time: 125.79878830909729
Epoch: 37, Steps: 136 | Train Loss: 0.1936398 Vali Loss: 0.1941710 Test Loss: 0.2278341
Validation loss decreased (0.194328 --> 0.194171).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1991806
	speed: 2.3057s/iter; left time: 19526.8079s
Epoch: 38 cost time: 133.35622334480286
Epoch: 38, Steps: 136 | Train Loss: 0.1934695 Vali Loss: 0.1939249 Test Loss: 0.2276292
Validation loss decreased (0.194171 --> 0.193925).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1926693
	speed: 2.2618s/iter; left time: 18847.8388s
Epoch: 39 cost time: 127.91967296600342
Epoch: 39, Steps: 136 | Train Loss: 0.1932740 Vali Loss: 0.1935779 Test Loss: 0.2274485
Validation loss decreased (0.193925 --> 0.193578).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1963279
	speed: 2.4034s/iter; left time: 19700.8961s
Epoch: 40 cost time: 142.12229824066162
Epoch: 40, Steps: 136 | Train Loss: 0.1932064 Vali Loss: 0.1937518 Test Loss: 0.2272822
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1819476
	speed: 2.2006s/iter; left time: 17739.4294s
Epoch: 41 cost time: 120.1628143787384
Epoch: 41, Steps: 136 | Train Loss: 0.1930364 Vali Loss: 0.1936406 Test Loss: 0.2271365
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1940130
	speed: 2.0330s/iter; left time: 16111.2866s
Epoch: 42 cost time: 119.00322079658508
Epoch: 42, Steps: 136 | Train Loss: 0.1928440 Vali Loss: 0.1935847 Test Loss: 0.2270029
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2031024
	speed: 2.1317s/iter; left time: 16603.5737s
Epoch: 43 cost time: 122.3838038444519
Epoch: 43, Steps: 136 | Train Loss: 0.1927194 Vali Loss: 0.1934050 Test Loss: 0.2268771
Validation loss decreased (0.193578 --> 0.193405).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2009394
	speed: 2.1208s/iter; left time: 16230.5039s
Epoch: 44 cost time: 116.1606273651123
Epoch: 44, Steps: 136 | Train Loss: 0.1925859 Vali Loss: 0.1933195 Test Loss: 0.2267715
Validation loss decreased (0.193405 --> 0.193319).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1945213
	speed: 2.0043s/iter; left time: 15065.9895s
Epoch: 45 cost time: 113.13248133659363
Epoch: 45, Steps: 136 | Train Loss: 0.1926046 Vali Loss: 0.1931510 Test Loss: 0.2266536
Validation loss decreased (0.193319 --> 0.193151).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2028757
	speed: 2.0133s/iter; left time: 14860.4989s
Epoch: 46 cost time: 116.68695044517517
Epoch: 46, Steps: 136 | Train Loss: 0.1924000 Vali Loss: 0.1932139 Test Loss: 0.2265503
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1956726
	speed: 1.8905s/iter; left time: 13696.7345s
Epoch: 47 cost time: 106.76787948608398
Epoch: 47, Steps: 136 | Train Loss: 0.1923601 Vali Loss: 0.1932214 Test Loss: 0.2264661
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1864964
	speed: 1.8271s/iter; left time: 12989.1413s
Epoch: 48 cost time: 100.96613311767578
Epoch: 48, Steps: 136 | Train Loss: 0.1923452 Vali Loss: 0.1931885 Test Loss: 0.2263892
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1948426
	speed: 1.7331s/iter; left time: 12085.0292s
Epoch: 49 cost time: 96.73369431495667
Epoch: 49, Steps: 136 | Train Loss: 0.1921715 Vali Loss: 0.1932077 Test Loss: 0.2263161
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1857389
	speed: 1.8785s/iter; left time: 12843.1110s
Epoch: 50 cost time: 119.0585606098175
Epoch: 50, Steps: 136 | Train Loss: 0.1921694 Vali Loss: 0.1932359 Test Loss: 0.2262401
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.1877928
	speed: 2.1224s/iter; left time: 14222.1565s
Epoch: 51 cost time: 121.66213274002075
Epoch: 51, Steps: 136 | Train Loss: 0.1921575 Vali Loss: 0.1930248 Test Loss: 0.2261836
Validation loss decreased (0.193151 --> 0.193025).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.1873967
	speed: 1.9073s/iter; left time: 12521.2533s
Epoch: 52 cost time: 104.652419090271
Epoch: 52, Steps: 136 | Train Loss: 0.1920419 Vali Loss: 0.1932646 Test Loss: 0.2261352
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.1931092
	speed: 1.8238s/iter; left time: 11724.8934s
Epoch: 53 cost time: 103.35203766822815
Epoch: 53, Steps: 136 | Train Loss: 0.1919900 Vali Loss: 0.1930426 Test Loss: 0.2260693
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.1882755
	speed: 1.7507s/iter; left time: 11017.4194s
Epoch: 54 cost time: 102.72821569442749
Epoch: 54, Steps: 136 | Train Loss: 0.1919591 Vali Loss: 0.1928216 Test Loss: 0.2260211
Validation loss decreased (0.193025 --> 0.192822).  Saving model ...
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.1866689
	speed: 1.6728s/iter; left time: 10299.4677s
Epoch: 55 cost time: 95.52838134765625
Epoch: 55, Steps: 136 | Train Loss: 0.1919486 Vali Loss: 0.1931352 Test Loss: 0.2259779
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.1976341
	speed: 1.7193s/iter; left time: 10352.1459s
Epoch: 56 cost time: 92.81815242767334
Epoch: 56, Steps: 136 | Train Loss: 0.1919337 Vali Loss: 0.1932568 Test Loss: 0.2259357
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.1975714
	speed: 1.6176s/iter; left time: 9519.7716s
Epoch: 57 cost time: 94.68876934051514
Epoch: 57, Steps: 136 | Train Loss: 0.1918309 Vali Loss: 0.1930925 Test Loss: 0.2258957
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.1951839
	speed: 1.5632s/iter; left time: 8986.8646s
Epoch: 58 cost time: 91.39714169502258
Epoch: 58, Steps: 136 | Train Loss: 0.1917526 Vali Loss: 0.1929311 Test Loss: 0.2258614
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2014460
	speed: 1.6042s/iter; left time: 9004.1266s
Epoch: 59 cost time: 85.85521268844604
Epoch: 59, Steps: 136 | Train Loss: 0.1918389 Vali Loss: 0.1928935 Test Loss: 0.2258275
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.1933367
	speed: 1.5771s/iter; left time: 8637.5172s
Epoch: 60 cost time: 90.8764808177948
Epoch: 60, Steps: 136 | Train Loss: 0.1918280 Vali Loss: 0.1929843 Test Loss: 0.2258039
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.1925419
	speed: 1.5567s/iter; left time: 8314.3528s
Epoch: 61 cost time: 85.44755339622498
Epoch: 61, Steps: 136 | Train Loss: 0.1917410 Vali Loss: 0.1929806 Test Loss: 0.2257705
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.1975861
	speed: 2.1182s/iter; left time: 11025.1483s
Epoch: 62 cost time: 129.49493789672852
Epoch: 62, Steps: 136 | Train Loss: 0.1917085 Vali Loss: 0.1930226 Test Loss: 0.2257491
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2014659
	speed: 2.2059s/iter; left time: 11181.7110s
Epoch: 63 cost time: 126.99360823631287
Epoch: 63, Steps: 136 | Train Loss: 0.1917857 Vali Loss: 0.1930037 Test Loss: 0.2257254
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.1907872
	speed: 2.1611s/iter; left time: 10660.5899s
Epoch: 64 cost time: 118.9113609790802
Epoch: 64, Steps: 136 | Train Loss: 0.1917489 Vali Loss: 0.1930853 Test Loss: 0.2257067
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.1991443
	speed: 2.2864s/iter; left time: 10968.0448s
Epoch: 65 cost time: 131.19907760620117
Epoch: 65, Steps: 136 | Train Loss: 0.1916742 Vali Loss: 0.1929506 Test Loss: 0.2256842
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.1911800
	speed: 2.2350s/iter; left time: 10417.2499s
Epoch: 66 cost time: 128.51483583450317
Epoch: 66, Steps: 136 | Train Loss: 0.1916357 Vali Loss: 0.1927555 Test Loss: 0.2256630
Validation loss decreased (0.192822 --> 0.192755).  Saving model ...
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.1879769
	speed: 2.2256s/iter; left time: 10070.7363s
Epoch: 67 cost time: 126.4838387966156
Epoch: 67, Steps: 136 | Train Loss: 0.1917595 Vali Loss: 0.1931640 Test Loss: 0.2256477
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.1914797
	speed: 2.1947s/iter; left time: 9632.5629s
Epoch: 68 cost time: 127.98581457138062
Epoch: 68, Steps: 136 | Train Loss: 0.1915649 Vali Loss: 0.1927275 Test Loss: 0.2256317
Validation loss decreased (0.192755 --> 0.192728).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.1916898
	speed: 2.4203s/iter; left time: 10293.5790s
Epoch: 69 cost time: 155.16769742965698
Epoch: 69, Steps: 136 | Train Loss: 0.1916208 Vali Loss: 0.1929789 Test Loss: 0.2256151
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.1959905
	speed: 2.6477s/iter; left time: 10900.4032s
Epoch: 70 cost time: 148.0072045326233
Epoch: 70, Steps: 136 | Train Loss: 0.1916157 Vali Loss: 0.1927019 Test Loss: 0.2256040
Validation loss decreased (0.192728 --> 0.192702).  Saving model ...
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.1936271
	speed: 2.5654s/iter; left time: 10212.8245s
Epoch: 71 cost time: 149.28692269325256
Epoch: 71, Steps: 136 | Train Loss: 0.1916056 Vali Loss: 0.1928531 Test Loss: 0.2255945
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.1937577
	speed: 2.4389s/iter; left time: 9377.5927s
Epoch: 72 cost time: 127.98511981964111
Epoch: 72, Steps: 136 | Train Loss: 0.1916523 Vali Loss: 0.1930267 Test Loss: 0.2255786
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.1918484
	speed: 2.2872s/iter; left time: 8483.1978s
Epoch: 73 cost time: 131.14208102226257
Epoch: 73, Steps: 136 | Train Loss: 0.1916608 Vali Loss: 0.1927636 Test Loss: 0.2255727
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.1971693
	speed: 2.1714s/iter; left time: 7758.4590s
Epoch: 74 cost time: 121.70234489440918
Epoch: 74, Steps: 136 | Train Loss: 0.1915692 Vali Loss: 0.1930629 Test Loss: 0.2255591
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.1982576
	speed: 2.3120s/iter; left time: 7946.2873s
Epoch: 75 cost time: 132.57809400558472
Epoch: 75, Steps: 136 | Train Loss: 0.1916016 Vali Loss: 0.1929113 Test Loss: 0.2255493
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.1869457
	speed: 2.1647s/iter; left time: 7145.6541s
Epoch: 76 cost time: 126.08662056922913
Epoch: 76, Steps: 136 | Train Loss: 0.1916601 Vali Loss: 0.1930955 Test Loss: 0.2255423
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.1933358
	speed: 2.1306s/iter; left time: 6743.2305s
Epoch: 77 cost time: 117.00855469703674
Epoch: 77, Steps: 136 | Train Loss: 0.1916967 Vali Loss: 0.1927454 Test Loss: 0.2255360
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.1983583
	speed: 2.0972s/iter; left time: 6352.5525s
Epoch: 78 cost time: 120.41934490203857
Epoch: 78, Steps: 136 | Train Loss: 0.1916252 Vali Loss: 0.1930349 Test Loss: 0.2255291
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.1861794
	speed: 1.9661s/iter; left time: 5687.9562s
Epoch: 79 cost time: 108.63580894470215
Epoch: 79, Steps: 136 | Train Loss: 0.1916388 Vali Loss: 0.1927676 Test Loss: 0.2255214
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.1847711
	speed: 1.9603s/iter; left time: 5404.4857s
Epoch: 80 cost time: 114.21837973594666
Epoch: 80, Steps: 136 | Train Loss: 0.1916322 Vali Loss: 0.1929071 Test Loss: 0.2255152
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.1926261
	speed: 2.0052s/iter; left time: 5255.6087s
Epoch: 81 cost time: 112.80382633209229
Epoch: 81, Steps: 136 | Train Loss: 0.1916502 Vali Loss: 0.1928556 Test Loss: 0.2255102
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.1838800
	speed: 2.0711s/iter; left time: 5146.7054s
Epoch: 82 cost time: 129.875422000885
Epoch: 82, Steps: 136 | Train Loss: 0.1916115 Vali Loss: 0.1926478 Test Loss: 0.2255056
Validation loss decreased (0.192702 --> 0.192648).  Saving model ...
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.1817924
	speed: 1.8279s/iter; left time: 4293.6683s
Epoch: 83 cost time: 109.8594901561737
Epoch: 83, Steps: 136 | Train Loss: 0.1915207 Vali Loss: 0.1931427 Test Loss: 0.2255007
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.1852650
	speed: 1.9979s/iter; left time: 4421.3083s
Epoch: 84 cost time: 108.6572334766388
Epoch: 84, Steps: 136 | Train Loss: 0.1915299 Vali Loss: 0.1928840 Test Loss: 0.2254967
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.1962620
	speed: 1.9069s/iter; left time: 3960.6186s
Epoch: 85 cost time: 106.38002586364746
Epoch: 85, Steps: 136 | Train Loss: 0.1915988 Vali Loss: 0.1926752 Test Loss: 0.2254911
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.1882727
	speed: 1.8983s/iter; left time: 3684.5855s
Epoch: 86 cost time: 98.42343544960022
Epoch: 86, Steps: 136 | Train Loss: 0.1914135 Vali Loss: 0.1928852 Test Loss: 0.2254888
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.1937616
	speed: 1.7580s/iter; left time: 3173.1592s
Epoch: 87 cost time: 98.56640338897705
Epoch: 87, Steps: 136 | Train Loss: 0.1915691 Vali Loss: 0.1930046 Test Loss: 0.2254853
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.1755662
	speed: 1.7061s/iter; left time: 2847.4369s
Epoch: 88 cost time: 98.4594886302948
Epoch: 88, Steps: 136 | Train Loss: 0.1915721 Vali Loss: 0.1931306 Test Loss: 0.2254824
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.766650946003179e-06
	iters: 100, epoch: 89 | loss: 0.1834220
	speed: 1.6801s/iter; left time: 2575.6023s
Epoch: 89 cost time: 99.46682953834534
Epoch: 89, Steps: 136 | Train Loss: 0.1916295 Vali Loss: 0.1928742 Test Loss: 0.2254775
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.47831839870302e-06
	iters: 100, epoch: 90 | loss: 0.1864089
	speed: 1.5483s/iter; left time: 2162.9473s
Epoch: 90 cost time: 90.66003704071045
Epoch: 90, Steps: 136 | Train Loss: 0.1915517 Vali Loss: 0.1931042 Test Loss: 0.2254741
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.204402478767869e-06
	iters: 100, epoch: 91 | loss: 0.2022949
	speed: 1.6252s/iter; left time: 2049.3347s
Epoch: 91 cost time: 103.75886011123657
Epoch: 91, Steps: 136 | Train Loss: 0.1915679 Vali Loss: 0.1930107 Test Loss: 0.2254704
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.944182354829475e-06
	iters: 100, epoch: 92 | loss: 0.2060061
	speed: 2.1440s/iter; left time: 2412.0110s
Epoch: 92 cost time: 121.08271598815918
Epoch: 92, Steps: 136 | Train Loss: 0.1915154 Vali Loss: 0.1930452 Test Loss: 0.2254693
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.696973237088e-06
	iters: 100, epoch: 93 | loss: 0.1822468
	speed: 2.3509s/iter; left time: 2325.0347s
Epoch: 93 cost time: 133.01109218597412
Epoch: 93, Steps: 136 | Train Loss: 0.1914944 Vali Loss: 0.1927239 Test Loss: 0.2254674
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.462124575233601e-06
	iters: 100, epoch: 94 | loss: 0.1915579
	speed: 2.3642s/iter; left time: 2016.6759s
Epoch: 94 cost time: 139.16111779212952
Epoch: 94, Steps: 136 | Train Loss: 0.1915124 Vali Loss: 0.1926783 Test Loss: 0.2254648
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.239018346471921e-06
	iters: 100, epoch: 95 | loss: 0.1774202
	speed: 2.2960s/iter; left time: 1646.2218s
Epoch: 95 cost time: 128.2377212047577
Epoch: 95, Steps: 136 | Train Loss: 0.1915631 Vali Loss: 0.1932050 Test Loss: 0.2254625
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.027067429148324e-06
	iters: 100, epoch: 96 | loss: 0.1925743
	speed: 2.2667s/iter; left time: 1316.9249s
Epoch: 96 cost time: 133.8263804912567
Epoch: 96, Steps: 136 | Train Loss: 0.1915889 Vali Loss: 0.1927381 Test Loss: 0.2254599
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.825714057690908e-06
	iters: 100, epoch: 97 | loss: 0.1868203
	speed: 2.3340s/iter; left time: 1038.6099s
Epoch: 97 cost time: 136.5615029335022
Epoch: 97, Steps: 136 | Train Loss: 0.1914871 Vali Loss: 0.1924908 Test Loss: 0.2254585
Validation loss decreased (0.192648 --> 0.192491).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
	iters: 100, epoch: 98 | loss: 0.1948648
	speed: 2.8065s/iter; left time: 867.2130s
Epoch: 98 cost time: 184.0035457611084
Epoch: 98, Steps: 136 | Train Loss: 0.1916046 Vali Loss: 0.1931484 Test Loss: 0.2254574
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.452706937066044e-06
	iters: 100, epoch: 99 | loss: 0.1847643
	speed: 2.9808s/iter; left time: 515.6861s
Epoch: 99 cost time: 172.14524602890015
Epoch: 99, Steps: 136 | Train Loss: 0.1915632 Vali Loss: 0.1930259 Test Loss: 0.2254557
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.2800715902127414e-06
	iters: 100, epoch: 100 | loss: 0.1930917
	speed: 2.7088s/iter; left time: 100.2246s
Epoch: 100 cost time: 145.91834259033203
Epoch: 100, Steps: 136 | Train Loss: 0.1915549 Vali Loss: 0.1931511 Test Loss: 0.2254542
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1160680107021042e-06
train 17513
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=90, out_features=450, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1664064000.0
params:  40950.0
Trainable parameters:  40950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2293976
	speed: 1.0498s/iter; left time: 14173.2376s
Epoch: 1 cost time: 145.30353951454163
Epoch: 1, Steps: 136 | Train Loss: 0.2383219 Vali Loss: 0.1923359 Test Loss: 0.2249394
Validation loss decreased (inf --> 0.192336).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2411112
	speed: 2.5530s/iter; left time: 34120.2226s
Epoch: 2 cost time: 139.5381269454956
Epoch: 2, Steps: 136 | Train Loss: 0.2379398 Vali Loss: 0.1924837 Test Loss: 0.2248058
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2372853
	speed: 2.0014s/iter; left time: 26476.3271s
Epoch: 3 cost time: 115.45076322555542
Epoch: 3, Steps: 136 | Train Loss: 0.2379400 Vali Loss: 0.1924308 Test Loss: 0.2247910
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2420413
	speed: 1.9349s/iter; left time: 25333.2188s
Epoch: 4 cost time: 105.40069627761841
Epoch: 4, Steps: 136 | Train Loss: 0.2378929 Vali Loss: 0.1927346 Test Loss: 0.2248472
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2302831
	speed: 1.8783s/iter; left time: 24336.8795s
Epoch: 5 cost time: 111.4328715801239
Epoch: 5, Steps: 136 | Train Loss: 0.2376729 Vali Loss: 0.1919588 Test Loss: 0.2247787
Validation loss decreased (0.192336 --> 0.191959).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2351827
	speed: 1.8929s/iter; left time: 24269.3359s
Epoch: 6 cost time: 99.20343327522278
Epoch: 6, Steps: 136 | Train Loss: 0.2378561 Vali Loss: 0.1921287 Test Loss: 0.2247958
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2430906
	speed: 2.1109s/iter; left time: 26777.1632s
Epoch: 7 cost time: 165.87288880348206
Epoch: 7, Steps: 136 | Train Loss: 0.2378801 Vali Loss: 0.1922662 Test Loss: 0.2247501
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2274684
	speed: 3.8496s/iter; left time: 48308.2828s
Epoch: 8 cost time: 217.64482378959656
Epoch: 8, Steps: 136 | Train Loss: 0.2377077 Vali Loss: 0.1921166 Test Loss: 0.2247801
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2407293
	speed: 2.7575s/iter; left time: 34229.4234s
Epoch: 9 cost time: 113.78373575210571
Epoch: 9, Steps: 136 | Train Loss: 0.2377947 Vali Loss: 0.1920311 Test Loss: 0.2247196
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2299603
	speed: 2.1531s/iter; left time: 26433.5266s
Epoch: 10 cost time: 154.6291263103485
Epoch: 10, Steps: 136 | Train Loss: 0.2377334 Vali Loss: 0.1922191 Test Loss: 0.2247434
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2388406
	speed: 3.0576s/iter; left time: 37122.8922s
Epoch: 11 cost time: 141.69503164291382
Epoch: 11, Steps: 136 | Train Loss: 0.2377685 Vali Loss: 0.1921024 Test Loss: 0.2247557
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2387487
	speed: 2.5307s/iter; left time: 30380.9143s
Epoch: 12 cost time: 150.90360355377197
Epoch: 12, Steps: 136 | Train Loss: 0.2377430 Vali Loss: 0.1919793 Test Loss: 0.2247700
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2261364
	speed: 2.3757s/iter; left time: 28197.4128s
Epoch: 13 cost time: 139.211688041687
Epoch: 13, Steps: 136 | Train Loss: 0.2377206 Vali Loss: 0.1921685 Test Loss: 0.2247774
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2434089
	speed: 2.3875s/iter; left time: 28012.4034s
Epoch: 14 cost time: 142.68802785873413
Epoch: 14, Steps: 136 | Train Loss: 0.2376729 Vali Loss: 0.1918324 Test Loss: 0.2247748
Validation loss decreased (0.191959 --> 0.191832).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2295079
	speed: 2.5884s/iter; left time: 30018.0309s
Epoch: 15 cost time: 134.29279923439026
Epoch: 15, Steps: 136 | Train Loss: 0.2375657 Vali Loss: 0.1918882 Test Loss: 0.2247036
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2351356
	speed: 2.5878s/iter; left time: 29658.2981s
Epoch: 16 cost time: 147.78012657165527
Epoch: 16, Steps: 136 | Train Loss: 0.2376935 Vali Loss: 0.1917019 Test Loss: 0.2247660
Validation loss decreased (0.191832 --> 0.191702).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2270568
	speed: 1.9019s/iter; left time: 21539.1221s
Epoch: 17 cost time: 100.11593461036682
Epoch: 17, Steps: 136 | Train Loss: 0.2376760 Vali Loss: 0.1918513 Test Loss: 0.2247307
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2430447
	speed: 1.4124s/iter; left time: 15803.5303s
Epoch: 18 cost time: 78.09241557121277
Epoch: 18, Steps: 136 | Train Loss: 0.2376582 Vali Loss: 0.1920180 Test Loss: 0.2247123
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2404461
	speed: 1.3636s/iter; left time: 15072.1473s
Epoch: 19 cost time: 79.51675820350647
Epoch: 19, Steps: 136 | Train Loss: 0.2376123 Vali Loss: 0.1918595 Test Loss: 0.2247179
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2552176
	speed: 1.3260s/iter; left time: 14475.9684s
Epoch: 20 cost time: 73.35802793502808
Epoch: 20, Steps: 136 | Train Loss: 0.2377531 Vali Loss: 0.1921228 Test Loss: 0.2247424
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2408101
	speed: 1.2577s/iter; left time: 13558.8698s
Epoch: 21 cost time: 72.48518586158752
Epoch: 21, Steps: 136 | Train Loss: 0.2376720 Vali Loss: 0.1916865 Test Loss: 0.2247451
Validation loss decreased (0.191702 --> 0.191687).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2325784
	speed: 1.1781s/iter; left time: 12540.8618s
Epoch: 22 cost time: 68.2167375087738
Epoch: 22, Steps: 136 | Train Loss: 0.2375272 Vali Loss: 0.1920024 Test Loss: 0.2247187
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2387153
	speed: 1.1998s/iter; left time: 12608.7755s
Epoch: 23 cost time: 73.17779302597046
Epoch: 23, Steps: 136 | Train Loss: 0.2376759 Vali Loss: 0.1918662 Test Loss: 0.2246913
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2400034
	speed: 1.3319s/iter; left time: 13815.5113s
Epoch: 24 cost time: 82.12439727783203
Epoch: 24, Steps: 136 | Train Loss: 0.2375276 Vali Loss: 0.1915486 Test Loss: 0.2247100
Validation loss decreased (0.191687 --> 0.191549).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2510195
	speed: 1.1935s/iter; left time: 12218.3095s
Epoch: 25 cost time: 65.6570634841919
Epoch: 25, Steps: 136 | Train Loss: 0.2376345 Vali Loss: 0.1920937 Test Loss: 0.2247227
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2348665
	speed: 1.1994s/iter; left time: 12114.9626s
Epoch: 26 cost time: 67.69062495231628
Epoch: 26, Steps: 136 | Train Loss: 0.2375702 Vali Loss: 0.1918914 Test Loss: 0.2246969
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2454819
	speed: 1.1629s/iter; left time: 11588.5122s
Epoch: 27 cost time: 70.09988045692444
Epoch: 27, Steps: 136 | Train Loss: 0.2375731 Vali Loss: 0.1917803 Test Loss: 0.2247151
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2368762
	speed: 1.2610s/iter; left time: 12394.6206s
Epoch: 28 cost time: 75.48236155509949
Epoch: 28, Steps: 136 | Train Loss: 0.2376605 Vali Loss: 0.1919233 Test Loss: 0.2247163
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2337285
	speed: 1.2531s/iter; left time: 12146.1824s
Epoch: 29 cost time: 70.85864281654358
Epoch: 29, Steps: 136 | Train Loss: 0.2375075 Vali Loss: 0.1914682 Test Loss: 0.2247012
Validation loss decreased (0.191549 --> 0.191468).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2309820
	speed: 1.2329s/iter; left time: 11782.5040s
Epoch: 30 cost time: 70.5807113647461
Epoch: 30, Steps: 136 | Train Loss: 0.2375995 Vali Loss: 0.1918426 Test Loss: 0.2247003
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2349667
	speed: 1.2448s/iter; left time: 11727.5269s
Epoch: 31 cost time: 71.48772954940796
Epoch: 31, Steps: 136 | Train Loss: 0.2375723 Vali Loss: 0.1920178 Test Loss: 0.2247060
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2491785
	speed: 1.2604s/iter; left time: 11702.6790s
Epoch: 32 cost time: 73.02208423614502
Epoch: 32, Steps: 136 | Train Loss: 0.2376163 Vali Loss: 0.1917401 Test Loss: 0.2247123
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2363931
	speed: 1.2618s/iter; left time: 11544.4105s
Epoch: 33 cost time: 73.16542410850525
Epoch: 33, Steps: 136 | Train Loss: 0.2376290 Vali Loss: 0.1917453 Test Loss: 0.2246926
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2370409
	speed: 1.2101s/iter; left time: 10906.2295s
Epoch: 34 cost time: 67.44028615951538
Epoch: 34, Steps: 136 | Train Loss: 0.2375352 Vali Loss: 0.1918405 Test Loss: 0.2247120
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2461217
	speed: 1.2366s/iter; left time: 10977.3179s
Epoch: 35 cost time: 70.4770495891571
Epoch: 35, Steps: 136 | Train Loss: 0.2375765 Vali Loss: 0.1916925 Test Loss: 0.2246791
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2375562
	speed: 1.2258s/iter; left time: 10715.1469s
Epoch: 36 cost time: 71.94152021408081
Epoch: 36, Steps: 136 | Train Loss: 0.2375730 Vali Loss: 0.1919230 Test Loss: 0.2247190
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2377914
	speed: 1.3638s/iter; left time: 11735.9197s
Epoch: 37 cost time: 78.02784371376038
Epoch: 37, Steps: 136 | Train Loss: 0.2376760 Vali Loss: 0.1917884 Test Loss: 0.2246885
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2513877
	speed: 1.3027s/iter; left time: 11032.6782s
Epoch: 38 cost time: 74.34054183959961
Epoch: 38, Steps: 136 | Train Loss: 0.2375841 Vali Loss: 0.1912757 Test Loss: 0.2247136
Validation loss decreased (0.191468 --> 0.191276).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2512910
	speed: 1.2460s/iter; left time: 10383.2789s
Epoch: 39 cost time: 68.85271859169006
Epoch: 39, Steps: 136 | Train Loss: 0.2374889 Vali Loss: 0.1915284 Test Loss: 0.2247163
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2391360
	speed: 1.3326s/iter; left time: 10923.6326s
Epoch: 40 cost time: 70.59882950782776
Epoch: 40, Steps: 136 | Train Loss: 0.2375144 Vali Loss: 0.1918598 Test Loss: 0.2246986
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2371267
	speed: 1.3417s/iter; left time: 10815.4912s
Epoch: 41 cost time: 73.80569744110107
Epoch: 41, Steps: 136 | Train Loss: 0.2375515 Vali Loss: 0.1915394 Test Loss: 0.2246930
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2290947
	speed: 1.5870s/iter; left time: 12576.8449s
Epoch: 42 cost time: 108.36690878868103
Epoch: 42, Steps: 136 | Train Loss: 0.2375979 Vali Loss: 0.1919550 Test Loss: 0.2246847
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2191201
	speed: 1.8494s/iter; left time: 14404.8913s
Epoch: 43 cost time: 98.14057540893555
Epoch: 43, Steps: 136 | Train Loss: 0.2376255 Vali Loss: 0.1918545 Test Loss: 0.2246761
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2319739
	speed: 1.6970s/iter; left time: 12986.8147s
Epoch: 44 cost time: 95.36531949043274
Epoch: 44, Steps: 136 | Train Loss: 0.2374660 Vali Loss: 0.1916278 Test Loss: 0.2247085
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2235741
	speed: 1.7007s/iter; left time: 12784.4783s
Epoch: 45 cost time: 100.16794109344482
Epoch: 45, Steps: 136 | Train Loss: 0.2374274 Vali Loss: 0.1916866 Test Loss: 0.2247063
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2278661
	speed: 1.6199s/iter; left time: 11956.6012s
Epoch: 46 cost time: 97.25615310668945
Epoch: 46, Steps: 136 | Train Loss: 0.2375583 Vali Loss: 0.1917617 Test Loss: 0.2246980
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2368277
	speed: 1.6177s/iter; left time: 11720.5096s
Epoch: 47 cost time: 86.50395345687866
Epoch: 47, Steps: 136 | Train Loss: 0.2375317 Vali Loss: 0.1916943 Test Loss: 0.2247191
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2381285
	speed: 1.2152s/iter; left time: 8638.6689s
Epoch: 48 cost time: 61.16226768493652
Epoch: 48, Steps: 136 | Train Loss: 0.2375747 Vali Loss: 0.1915638 Test Loss: 0.2246859
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2524001
	speed: 0.9901s/iter; left time: 6904.1577s
Epoch: 49 cost time: 55.689836740493774
Epoch: 49, Steps: 136 | Train Loss: 0.2374269 Vali Loss: 0.1918320 Test Loss: 0.2246860
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2361999
	speed: 1.0666s/iter; left time: 7292.5736s
Epoch: 50 cost time: 64.02163505554199
Epoch: 50, Steps: 136 | Train Loss: 0.2375353 Vali Loss: 0.1917168 Test Loss: 0.2247060
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2352931
	speed: 1.2868s/iter; left time: 8622.8757s
Epoch: 51 cost time: 81.46646976470947
Epoch: 51, Steps: 136 | Train Loss: 0.2375228 Vali Loss: 0.1916699 Test Loss: 0.2246880
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2421844
	speed: 1.5472s/iter; left time: 10157.2126s
Epoch: 52 cost time: 87.42715454101562
Epoch: 52, Steps: 136 | Train Loss: 0.2375351 Vali Loss: 0.1915863 Test Loss: 0.2246924
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2421273
	speed: 1.5335s/iter; left time: 9858.7780s
Epoch: 53 cost time: 87.3838050365448
Epoch: 53, Steps: 136 | Train Loss: 0.2375478 Vali Loss: 0.1914318 Test Loss: 0.2246870
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2296257
	speed: 1.4872s/iter; left time: 9358.7268s
Epoch: 54 cost time: 84.16705513000488
Epoch: 54, Steps: 136 | Train Loss: 0.2376431 Vali Loss: 0.1914416 Test Loss: 0.2246886
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2437089
	speed: 1.1823s/iter; left time: 7279.3337s
Epoch: 55 cost time: 64.86008262634277
Epoch: 55, Steps: 136 | Train Loss: 0.2374990 Vali Loss: 0.1915967 Test Loss: 0.2246966
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2353964
	speed: 1.1535s/iter; left time: 6944.9249s
Epoch: 56 cost time: 64.31943917274475
Epoch: 56, Steps: 136 | Train Loss: 0.2375762 Vali Loss: 0.1919425 Test Loss: 0.2246832
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2451109
	speed: 1.1007s/iter; left time: 6477.5139s
Epoch: 57 cost time: 61.992477893829346
Epoch: 57, Steps: 136 | Train Loss: 0.2374611 Vali Loss: 0.1919082 Test Loss: 0.2246918
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2389369
	speed: 1.0576s/iter; left time: 6080.0284s
Epoch: 58 cost time: 59.471298933029175
Epoch: 58, Steps: 136 | Train Loss: 0.2375051 Vali Loss: 0.1915244 Test Loss: 0.2246843
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_180_j720_H10_FITS_custom_ftM_sl180_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.22311407327651978, mae:0.3063657283782959, rse:0.4711843729019165, corr:[0.445291   0.4485954  0.44937992 0.44945315 0.44950542 0.44946486
 0.44938132 0.4492342  0.449034   0.44880328 0.4485504  0.4483601
 0.44811577 0.44799057 0.4478742  0.44773594 0.44764137 0.44760308
 0.44771275 0.44758716 0.44764683 0.44775677 0.4480883  0.44825596
 0.4478482  0.44758055 0.4474989  0.44731876 0.44719172 0.44709477
 0.44690853 0.44672638 0.4466275  0.44647723 0.446291   0.44617838
 0.44605595 0.44596553 0.4459832  0.44598854 0.44593975 0.44595045
 0.4459215  0.44578177 0.44573894 0.4458151  0.44603524 0.446135
 0.44591746 0.4458554  0.44578654 0.44559494 0.44554183 0.4454402
 0.4453535  0.44533405 0.44531393 0.4453136  0.44527137 0.44515595
 0.44510996 0.44507957 0.44506472 0.4450471  0.44495624 0.444991
 0.44488484 0.44478405 0.44478723 0.44483924 0.44497505 0.44506675
 0.4448771  0.44478613 0.4447025  0.44466174 0.4446842  0.44457912
 0.44458407 0.4445605  0.44446254 0.4443299  0.44429073 0.44431922
 0.44428694 0.44426525 0.44416475 0.44415978 0.4441271  0.44407898
 0.44414076 0.44403338 0.44379768 0.44380358 0.44400626 0.44408625
 0.44389525 0.44386086 0.4438372  0.44380328 0.44378912 0.4438083
 0.44376177 0.44371712 0.44377738 0.4437669  0.4436903  0.4436768
 0.4436527  0.44372237 0.44372618 0.44359577 0.4435862  0.44353375
 0.44347903 0.44347972 0.44321612 0.44323638 0.44340006 0.44350204
 0.443394   0.44342315 0.44354138 0.44347778 0.44358656 0.44350776
 0.44362766 0.44358993 0.44358134 0.44369015 0.44362047 0.443637
 0.4436948  0.44376552 0.44375488 0.4437762  0.44367087 0.44364813
 0.44371516 0.4436604  0.44370314 0.4438552  0.44419256 0.44443503
 0.44434017 0.4443591  0.4444285  0.44448635 0.44448218 0.44447675
 0.4445094  0.4445383  0.44463047 0.44474724 0.44479492 0.44483176
 0.4451007  0.44520792 0.44520992 0.445094   0.44508252 0.44524753
 0.4454082  0.4454335  0.4456484  0.4458928  0.4462757  0.44623503
 0.44543052 0.44491416 0.44457135 0.44414085 0.4438732  0.44367176
 0.44348955 0.44332072 0.44311985 0.44301006 0.44284973 0.44280985
 0.4426992  0.44268927 0.44259158 0.4425782  0.4425737  0.44261485
 0.44267604 0.44244283 0.4424545  0.44244662 0.4426019  0.4426557
 0.4421198  0.44193888 0.4417457  0.44156176 0.44143167 0.44132483
 0.4411906  0.44105163 0.44093058 0.4409094  0.44080666 0.44077426
 0.44079557 0.44073895 0.4408158  0.44078937 0.44080842 0.44082654
 0.44081804 0.4407143  0.44064215 0.44069785 0.4408405  0.4408357
 0.4405989  0.44058192 0.44051826 0.44035587 0.44024527 0.44012648
 0.44005325 0.4399957  0.4399878  0.43991554 0.4398506  0.43986872
 0.4398393  0.4397695  0.43981555 0.43977118 0.43970752 0.43967423
 0.4396289  0.43950272 0.43948215 0.43948486 0.43964824 0.4397119
 0.43956962 0.43946844 0.43937534 0.43932948 0.43924767 0.43918887
 0.439142   0.43914747 0.43908814 0.439053   0.4390183  0.43896118
 0.4390325  0.43906555 0.4390637  0.43903387 0.43897513 0.43887988
 0.43888384 0.4387567  0.4387455  0.43870968 0.43886665 0.43891913
 0.438702   0.43874726 0.43872473 0.43866575 0.4386965  0.43870845
 0.4386277  0.43871123 0.43869266 0.43869838 0.4387702  0.43872374
 0.4387998  0.43878534 0.4388205  0.43872237 0.4386928  0.43863508
 0.43851066 0.43835318 0.43812656 0.43820083 0.43831888 0.43839803
 0.43823922 0.4382912  0.43837774 0.4384337  0.43850332 0.43851268
 0.4385749  0.43866077 0.4386822  0.43883735 0.43895766 0.43889782
 0.43898344 0.43903917 0.43902987 0.43894112 0.43879047 0.43868667
 0.4387228  0.43875906 0.43877536 0.4389196  0.43910807 0.43930843
 0.4391077  0.43916216 0.43925506 0.43929514 0.4394017  0.43930802
 0.43947282 0.43951553 0.43967304 0.43989098 0.44005007 0.4401814
 0.4404502  0.44060043 0.44049385 0.44044116 0.44034147 0.44048533
 0.44050843 0.4403687  0.44047397 0.4407028  0.4409399  0.4407346
 0.43995032 0.43949446 0.4391583  0.4389223  0.43867782 0.43846554
 0.4383438  0.4381601  0.4379739  0.43782794 0.43770483 0.43760344
 0.43756557 0.43743232 0.43751967 0.43738723 0.43735224 0.43748888
 0.43745154 0.43740225 0.43740985 0.43749434 0.43769556 0.43766186
 0.43722677 0.43692866 0.43685594 0.43657774 0.43637595 0.43631655
 0.43622297 0.4359157  0.4359186  0.43576896 0.4356609  0.43568358
 0.4356439  0.4357207  0.43572307 0.4357071  0.43569297 0.43577006
 0.43577242 0.43566254 0.43559164 0.4356973  0.43578428 0.4358372
 0.43564624 0.4355966  0.4356075  0.4354571  0.43547773 0.43528095
 0.435187   0.43524635 0.43511188 0.43501025 0.43494457 0.4349637
 0.4348795  0.434884   0.4348962  0.43481684 0.43485102 0.43485138
 0.4348901  0.43486622 0.4348104  0.43483612 0.43497902 0.43509007
 0.43492755 0.43482542 0.4349159  0.4348412  0.43480018 0.43478712
 0.43473533 0.43471515 0.43456015 0.43445334 0.4345116  0.43445304
 0.43445903 0.43448463 0.4345043  0.4344592  0.4344966  0.43445274
 0.43449062 0.43437296 0.4343315  0.43446276 0.4345697  0.43463615
 0.434497   0.43440205 0.43442282 0.43441382 0.43442285 0.43447044
 0.43443596 0.43436888 0.4343668  0.4343897  0.4343575  0.43429884
 0.4342968  0.43436065 0.43438137 0.43435752 0.43433988 0.43433237
 0.43417746 0.4341352  0.43413222 0.43412417 0.43428764 0.43441924
 0.43431586 0.43431404 0.43441626 0.43438318 0.4345887  0.43467125
 0.43456057 0.4346417  0.4346875  0.43471405 0.43480986 0.4347575
 0.43486103 0.43490976 0.43488196 0.4347916  0.43473232 0.4346893
 0.43462574 0.43470824 0.4348011  0.43491808 0.43515334 0.43539298
 0.43528864 0.43530008 0.4355036  0.43552276 0.43562624 0.43559173
 0.43561384 0.43577564 0.43575308 0.4359525  0.43611696 0.4361921
 0.43650675 0.43654308 0.43653858 0.43634883 0.43637663 0.43653643
 0.43654856 0.4365217  0.43651146 0.43674594 0.43693542 0.436649
 0.4359595  0.43545607 0.43508986 0.43480375 0.43454188 0.4343627
 0.43413776 0.43385985 0.43368086 0.4335653  0.43345973 0.43343195
 0.43335506 0.43327755 0.43335003 0.43325657 0.43324825 0.4333641
 0.4333413  0.4332712  0.43322918 0.43337268 0.43341112 0.43331832
 0.43277824 0.43245777 0.43230048 0.43207952 0.43186575 0.43168777
 0.4315759  0.43141863 0.4313065  0.4311548  0.43110004 0.43105543
 0.431077   0.43113545 0.43114036 0.43111467 0.43111873 0.4312088
 0.43116206 0.43106544 0.43110904 0.4311596  0.4311706  0.43125412
 0.43099752 0.4307642  0.43057567 0.43040302 0.43029118 0.43027732
 0.43011883 0.4300702  0.43008274 0.42995352 0.42996114 0.42993155
 0.4299028  0.4299275  0.42993155 0.4298524  0.42975307 0.42957714
 0.42943642 0.4295471  0.42952678 0.42959338 0.429669   0.42966434
 0.42950475 0.42921558 0.4292294  0.4291031  0.42911482 0.42904916
 0.4288712  0.42901388 0.42895105 0.42889068 0.4288668  0.4287551
 0.42884764 0.42890248 0.42879397 0.42879793 0.42877042 0.42877734
 0.42878717 0.42871395 0.42874992 0.42873302 0.42890352 0.42899394
 0.42876044 0.42872557 0.42865092 0.4286497  0.42864725 0.4285781
 0.42846927 0.42850766 0.42846674 0.42849064 0.42848676 0.42631558
 0.42429176 0.42860267 0.42436662 0.42427915 0.42433277 0.42431763
 0.42429093 0.42413855 0.42407984 0.42415717 0.42426628 0.42438662
 0.42428124 0.42434353 0.42435843 0.42431468 0.42440784 0.42445529
 0.42441252 0.42449766 0.42455775 0.42452613 0.42467776 0.42466778
 0.42478955 0.42490715 0.42495868 0.42486298 0.42488125 0.4247786
 0.4246461  0.42475286 0.42484847 0.4249591  0.42519543 0.42539576
 0.4253456  0.4253764  0.42546368 0.425467   0.42558458 0.42556468
 0.42558032 0.42564204 0.42573822 0.42599314 0.42617145 0.42635143
 0.42660823 0.42687088 0.42685747 0.42681634 0.42681056 0.42686784
 0.42679808 0.42676425 0.42693266 0.42710567 0.4273351  0.42704216
 0.42632937 0.42589432 0.42554086 0.42515868 0.42494658 0.42475596
 0.4245352  0.42434657 0.42405322 0.42401072 0.42392594 0.42378455
 0.42378417 0.42377952 0.42361125 0.42352226 0.4236678  0.4236225
 0.4236139  0.42345577 0.4234569  0.42347172 0.4235053  0.42352438
 0.42301467 0.42282492 0.42265928 0.42242104 0.42238376 0.42226887
 0.42208207 0.42196494 0.42176163 0.42174122 0.42165977 0.42158946
 0.42161208 0.42166105 0.42162746 0.4215118  0.42154685 0.42144507
 0.42136553 0.4211501  0.42128882 0.42135912 0.42172766 0.42165527]
