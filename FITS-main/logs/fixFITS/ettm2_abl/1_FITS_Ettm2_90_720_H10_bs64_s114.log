Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=20, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=20, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3225600.0
params:  3780.0
Trainable parameters:  3780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7402223
	speed: 0.0198s/iter; left time: 519.3061s
	iters: 200, epoch: 1 | loss: 0.7538584
	speed: 0.0149s/iter; left time: 387.7885s
Epoch: 1 cost time: 4.3982391357421875
Epoch: 1, Steps: 263 | Train Loss: 0.7545649 Vali Loss: 0.3336360 Test Loss: 0.4653730
Validation loss decreased (inf --> 0.333636).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5050128
	speed: 0.0741s/iter; left time: 1921.9346s
	iters: 200, epoch: 2 | loss: 0.5885188
	speed: 0.0145s/iter; left time: 374.0083s
Epoch: 2 cost time: 4.4357194900512695
Epoch: 2, Steps: 263 | Train Loss: 0.6178222 Vali Loss: 0.2972832 Test Loss: 0.4214925
Validation loss decreased (0.333636 --> 0.297283).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4804904
	speed: 0.0702s/iter; left time: 1802.1078s
	iters: 200, epoch: 3 | loss: 0.5344845
	speed: 0.0143s/iter; left time: 365.6318s
Epoch: 3 cost time: 4.3947978019714355
Epoch: 3, Steps: 263 | Train Loss: 0.5953032 Vali Loss: 0.2908915 Test Loss: 0.4141115
Validation loss decreased (0.297283 --> 0.290891).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3704291
	speed: 0.0728s/iter; left time: 1849.3954s
	iters: 200, epoch: 4 | loss: 0.6363902
	speed: 0.0164s/iter; left time: 415.7926s
Epoch: 4 cost time: 4.711452007293701
Epoch: 4, Steps: 263 | Train Loss: 0.5912957 Vali Loss: 0.2896649 Test Loss: 0.4124425
Validation loss decreased (0.290891 --> 0.289665).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6633882
	speed: 0.0764s/iter; left time: 1920.1348s
	iters: 200, epoch: 5 | loss: 0.5494061
	speed: 0.0164s/iter; left time: 410.5530s
Epoch: 5 cost time: 4.761984586715698
Epoch: 5, Steps: 263 | Train Loss: 0.5907631 Vali Loss: 0.2889861 Test Loss: 0.4117780
Validation loss decreased (0.289665 --> 0.288986).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6129971
	speed: 0.0715s/iter; left time: 1779.5330s
	iters: 200, epoch: 6 | loss: 0.5762400
	speed: 0.0147s/iter; left time: 363.2071s
Epoch: 6 cost time: 4.427206039428711
Epoch: 6, Steps: 263 | Train Loss: 0.5897243 Vali Loss: 0.2890276 Test Loss: 0.4113174
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.7150493
	speed: 0.0729s/iter; left time: 1795.1220s
	iters: 200, epoch: 7 | loss: 0.6680282
	speed: 0.0144s/iter; left time: 351.9661s
Epoch: 7 cost time: 4.455799102783203
Epoch: 7, Steps: 263 | Train Loss: 0.5890398 Vali Loss: 0.2890158 Test Loss: 0.4112925
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.8596091
	speed: 0.0735s/iter; left time: 1790.3115s
	iters: 200, epoch: 8 | loss: 0.6457552
	speed: 0.0148s/iter; left time: 358.1564s
Epoch: 8 cost time: 4.559586524963379
Epoch: 8, Steps: 263 | Train Loss: 0.5880985 Vali Loss: 0.2887060 Test Loss: 0.4111024
Validation loss decreased (0.288986 --> 0.288706).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5994743
	speed: 0.0723s/iter; left time: 1742.7175s
	iters: 200, epoch: 9 | loss: 0.5442153
	speed: 0.0139s/iter; left time: 333.3569s
Epoch: 9 cost time: 4.275851249694824
Epoch: 9, Steps: 263 | Train Loss: 0.5885712 Vali Loss: 0.2890538 Test Loss: 0.4111767
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6371529
	speed: 0.0708s/iter; left time: 1688.4767s
	iters: 200, epoch: 10 | loss: 0.6940966
	speed: 0.0147s/iter; left time: 349.4249s
Epoch: 10 cost time: 4.437487602233887
Epoch: 10, Steps: 263 | Train Loss: 0.5881200 Vali Loss: 0.2889234 Test Loss: 0.4111028
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5297157
	speed: 0.0836s/iter; left time: 1969.4641s
	iters: 200, epoch: 11 | loss: 0.5074497
	speed: 0.0149s/iter; left time: 349.4835s
Epoch: 11 cost time: 4.393547058105469
Epoch: 11, Steps: 263 | Train Loss: 0.5882027 Vali Loss: 0.2893209 Test Loss: 0.4111214
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6328931
	speed: 0.0736s/iter; left time: 1715.2125s
	iters: 200, epoch: 12 | loss: 0.4921747
	speed: 0.0171s/iter; left time: 397.8322s
Epoch: 12 cost time: 4.793476819992065
Epoch: 12, Steps: 263 | Train Loss: 0.5870316 Vali Loss: 0.2890776 Test Loss: 0.4111053
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5460304
	speed: 0.0795s/iter; left time: 1832.7666s
	iters: 200, epoch: 13 | loss: 0.7241815
	speed: 0.0234s/iter; left time: 537.2772s
Epoch: 13 cost time: 5.794697523117065
Epoch: 13, Steps: 263 | Train Loss: 0.5880766 Vali Loss: 0.2892603 Test Loss: 0.4111110
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4131041
	speed: 0.0743s/iter; left time: 1691.5706s
	iters: 200, epoch: 14 | loss: 0.6278952
	speed: 0.0145s/iter; left time: 329.3146s
Epoch: 14 cost time: 4.46247935295105
Epoch: 14, Steps: 263 | Train Loss: 0.5877766 Vali Loss: 0.2893387 Test Loss: 0.4111774
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5691959
	speed: 0.0713s/iter; left time: 1605.5423s
	iters: 200, epoch: 15 | loss: 0.4197191
	speed: 0.0147s/iter; left time: 329.1344s
Epoch: 15 cost time: 4.382527112960815
Epoch: 15, Steps: 263 | Train Loss: 0.5875630 Vali Loss: 0.2895794 Test Loss: 0.4110811
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.6428585
	speed: 0.0717s/iter; left time: 1595.1733s
	iters: 200, epoch: 16 | loss: 0.5734508
	speed: 0.0141s/iter; left time: 311.2958s
Epoch: 16 cost time: 4.376718997955322
Epoch: 16, Steps: 263 | Train Loss: 0.5864645 Vali Loss: 0.2894076 Test Loss: 0.4110850
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.7242573
	speed: 0.0810s/iter; left time: 1782.2581s
	iters: 200, epoch: 17 | loss: 0.4493689
	speed: 0.0143s/iter; left time: 314.0090s
Epoch: 17 cost time: 4.341864347457886
Epoch: 17, Steps: 263 | Train Loss: 0.5869782 Vali Loss: 0.2894530 Test Loss: 0.4110565
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4401332
	speed: 0.0755s/iter; left time: 1641.4477s
	iters: 200, epoch: 18 | loss: 0.6283007
	speed: 0.0162s/iter; left time: 350.5342s
Epoch: 18 cost time: 4.817737817764282
Epoch: 18, Steps: 263 | Train Loss: 0.5868305 Vali Loss: 0.2893949 Test Loss: 0.4111006
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.6229730
	speed: 0.0787s/iter; left time: 1690.0798s
	iters: 200, epoch: 19 | loss: 0.4100893
	speed: 0.0168s/iter; left time: 359.9938s
Epoch: 19 cost time: 4.897902727127075
Epoch: 19, Steps: 263 | Train Loss: 0.5872020 Vali Loss: 0.2893125 Test Loss: 0.4110769
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5487669
	speed: 0.0721s/iter; left time: 1529.1747s
	iters: 200, epoch: 20 | loss: 0.6355795
	speed: 0.0145s/iter; left time: 305.2817s
Epoch: 20 cost time: 4.442694664001465
Epoch: 20, Steps: 263 | Train Loss: 0.5865178 Vali Loss: 0.2895959 Test Loss: 0.4111637
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.6674309
	speed: 0.0785s/iter; left time: 1644.2837s
	iters: 200, epoch: 21 | loss: 0.7642604
	speed: 0.0166s/iter; left time: 346.6314s
Epoch: 21 cost time: 4.9648401737213135
Epoch: 21, Steps: 263 | Train Loss: 0.5870914 Vali Loss: 0.2895305 Test Loss: 0.4111297
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.7076170
	speed: 0.0817s/iter; left time: 1689.5587s
	iters: 200, epoch: 22 | loss: 0.6879652
	speed: 0.0151s/iter; left time: 309.8414s
Epoch: 22 cost time: 4.510473966598511
Epoch: 22, Steps: 263 | Train Loss: 0.5873843 Vali Loss: 0.2892828 Test Loss: 0.4111456
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.6687526
	speed: 0.0709s/iter; left time: 1447.0076s
	iters: 200, epoch: 23 | loss: 0.5033330
	speed: 0.0142s/iter; left time: 288.3186s
Epoch: 23 cost time: 4.297059535980225
Epoch: 23, Steps: 263 | Train Loss: 0.5871343 Vali Loss: 0.2895004 Test Loss: 0.4111464
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5535145
	speed: 0.0708s/iter; left time: 1426.6027s
	iters: 200, epoch: 24 | loss: 0.4790709
	speed: 0.0148s/iter; left time: 296.0910s
Epoch: 24 cost time: 4.4638471603393555
Epoch: 24, Steps: 263 | Train Loss: 0.5870832 Vali Loss: 0.2892752 Test Loss: 0.4112097
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5878068
	speed: 0.0724s/iter; left time: 1440.1223s
	iters: 200, epoch: 25 | loss: 0.5557282
	speed: 0.0155s/iter; left time: 307.6748s
Epoch: 25 cost time: 4.545886278152466
Epoch: 25, Steps: 263 | Train Loss: 0.5868441 Vali Loss: 0.2895247 Test Loss: 0.4111859
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.8511795
	speed: 0.0713s/iter; left time: 1399.1628s
	iters: 200, epoch: 26 | loss: 0.6172483
	speed: 0.0145s/iter; left time: 283.6297s
Epoch: 26 cost time: 4.261091470718384
Epoch: 26, Steps: 263 | Train Loss: 0.5868170 Vali Loss: 0.2893828 Test Loss: 0.4112096
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3451702
	speed: 0.0731s/iter; left time: 1414.8363s
	iters: 200, epoch: 27 | loss: 0.6433029
	speed: 0.0171s/iter; left time: 330.3467s
Epoch: 27 cost time: 4.970864295959473
Epoch: 27, Steps: 263 | Train Loss: 0.5863066 Vali Loss: 0.2894641 Test Loss: 0.4112278
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.6169381
	speed: 0.0838s/iter; left time: 1600.5505s
	iters: 200, epoch: 28 | loss: 0.6110343
	speed: 0.0146s/iter; left time: 276.6706s
Epoch: 28 cost time: 4.333933115005493
Epoch: 28, Steps: 263 | Train Loss: 0.5869351 Vali Loss: 0.2897891 Test Loss: 0.4112099
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.40879321098327637, mae:0.39826205372810364, rse:0.5139208436012268, corr:[0.5450641  0.5485098  0.53950715 0.53719944 0.5380417  0.53546256
 0.5317306  0.5311858  0.53214014 0.5310697  0.528585   0.5274109
 0.5276892  0.5273175  0.52564347 0.5243306  0.52413106 0.5236681
 0.5220572  0.5202884  0.51949584 0.5190912  0.51809704 0.5169265
 0.51659936 0.5169414  0.51677513 0.51577336 0.5148746  0.51472247
 0.51452667 0.51359564 0.5124433  0.51191163 0.51178926 0.51113373
 0.50977325 0.5086778  0.50836915 0.5081751  0.50741553 0.5065107
 0.5062223  0.5064338  0.50628245 0.5053922  0.5043876  0.5037937
 0.50331324 0.50239027 0.50109905 0.50012505 0.49966487 0.49925268
 0.49861085 0.4978911  0.4974796  0.49736637 0.4971723  0.4967911
 0.49647877 0.49646747 0.49660033 0.49657744 0.49630532 0.49607426
 0.4960116  0.4960043  0.49594775 0.49583545 0.49577332 0.49582708
 0.4958991  0.49589166 0.495713   0.4954531  0.49527198 0.49513465
 0.4949409  0.4946066  0.49422267 0.49391246 0.49366808 0.4933456
 0.49292144 0.4925048  0.49217284 0.49188718 0.49153468 0.49117878
 0.49087015 0.4904709  0.48977408 0.48861408 0.48698023 0.48502958
 0.48283473 0.48042154 0.47803047 0.47600645 0.47438422 0.47292867
 0.4714686  0.47015017 0.4690958  0.46802664 0.46668687 0.4652328
 0.46397564 0.46291903 0.4617219  0.46021932 0.45871043 0.45744297
 0.4562678  0.45490423 0.45340663 0.45219457 0.4514283  0.4506607
 0.44960517 0.4483753  0.44731924 0.44653186 0.44570494 0.44457844
 0.44334918 0.44231755 0.44140357 0.44037506 0.43925208 0.43822345
 0.43741915 0.43667445 0.435756   0.43495443 0.4344475  0.43432298
 0.43422428 0.43387187 0.43334574 0.43283907 0.43238643 0.4317211
 0.43076026 0.42971292 0.42867038 0.42773944 0.4267968  0.42598796
 0.4255034  0.4254256  0.42529106 0.4246799  0.42387706 0.4233066
 0.42308813 0.4230967  0.42304108 0.42301756 0.42304552 0.42298335
 0.4228016  0.42241672 0.42224523 0.42238146 0.4226494  0.42283842
 0.42290905 0.422916   0.42280972 0.4225109  0.42218432 0.4220799
 0.4222243  0.4223887  0.42229307 0.4219851  0.42177123 0.42184263
 0.4220003  0.4218873  0.42149597 0.42101732 0.42065296 0.4202738
 0.41969946 0.41897058 0.41823974 0.41738376 0.4160026  0.4137568
 0.41095325 0.4083713  0.40632075 0.4045985  0.40300483 0.40153125
 0.4002798  0.3991785  0.3981002  0.39696586 0.39581808 0.3946535
 0.3933604  0.39189002 0.39029273 0.38880125 0.38749975 0.38620707
 0.38468483 0.38312903 0.38184944 0.380868   0.37984222 0.37855425
 0.3771989  0.37605762 0.3752344  0.37432638 0.37307099 0.37164977
 0.37038264 0.3692209  0.36825138 0.36725947 0.3663965  0.36561242
 0.36469874 0.36354494 0.36229023 0.36137354 0.36087024 0.36060435
 0.36029202 0.3599293  0.35965267 0.35937703 0.3590304  0.35858852
 0.35820103 0.35789806 0.35739613 0.35689917 0.35648853 0.3563436
 0.35664192 0.35714605 0.3574921  0.35746744 0.35733348 0.3572556
 0.35715792 0.35710716 0.3572941  0.35768566 0.35804248 0.35812205
 0.3579557  0.35786444 0.35797    0.35826865 0.35859093 0.3589115
 0.3593225  0.35965142 0.35979277 0.35969475 0.35970354 0.35997465
 0.36030224 0.36044654 0.3604757  0.36072165 0.36125258 0.36170307
 0.3617861  0.36161104 0.36157176 0.36180988 0.3619583  0.3618103
 0.3615809  0.36159924 0.3617049  0.3613421  0.3601745  0.35840937
 0.35666975 0.35537767 0.35447106 0.35387415 0.35360026 0.3536312
 0.35375398 0.353746   0.35371405 0.35375676 0.3536786  0.35327807
 0.35261852 0.35194483 0.35138866 0.35075468 0.34979773 0.34859467
 0.34756836 0.34693038 0.34644306 0.345727   0.34474766 0.34389037
 0.34332573 0.34293514 0.34244823 0.34188455 0.34144935 0.34105954
 0.34036583 0.33936232 0.3385334  0.33825442 0.33827257 0.33797067
 0.33714488 0.33628944 0.33589092 0.3357591  0.33553576 0.33510584
 0.33482715 0.3349277  0.3352136  0.33523363 0.33491802 0.33457786
 0.33436283 0.33420444 0.3341125  0.33423275 0.3345123  0.3347351
 0.33480316 0.33487174 0.33503124 0.335183   0.33515978 0.33503276
 0.33510542 0.335465   0.33592686 0.3361941  0.3363016  0.33649707
 0.33689016 0.3373074  0.33743727 0.3374367  0.3375896  0.33800882
 0.33843878 0.3386721  0.3387405  0.338865   0.3391099  0.3393984
 0.33958262 0.33972257 0.339961   0.34027365 0.34062552 0.34105322
 0.34161568 0.3421807  0.34243527 0.34226355 0.34197262 0.3420017
 0.34238046 0.34260172 0.3421891  0.34118837 0.33995068 0.33860388
 0.3369668  0.33510062 0.33352923 0.33268493 0.3323991  0.33213064
 0.33149868 0.33073533 0.33025578 0.32994983 0.3294505  0.32862422
 0.32774264 0.32703125 0.32626826 0.32513458 0.32365566 0.32220602
 0.32106152 0.32001156 0.3187708  0.3174466  0.31625187 0.3153469
 0.31455368 0.3136863  0.31281996 0.31210688 0.31146532 0.31062016
 0.30959967 0.3086963  0.30813733 0.30775675 0.3070683  0.30607417
 0.30517673 0.304567   0.30406898 0.30342728 0.3027824  0.3025539
 0.3027732  0.30303356 0.30281892 0.30214262 0.3012807  0.30049622
 0.29977548 0.29908133 0.29850656 0.29815066 0.2977847  0.2973902
 0.29714182 0.2973348  0.2977243  0.29781803 0.29751724 0.29729465
 0.297476   0.29781646 0.29780504 0.2973513  0.297005   0.29709992
 0.2973183  0.29707593 0.29645595 0.29615298 0.29658937 0.29738876
 0.2977251  0.29729304 0.29655436 0.29617137 0.29635835 0.29665816
 0.29673505 0.29659626 0.2964615  0.29641956 0.2964546  0.29652634
 0.29657847 0.29646596 0.2961312  0.2956821  0.29524517 0.29484358
 0.29442328 0.29389834 0.29323366 0.29224607 0.29069945 0.28844172
 0.28585857 0.28364348 0.2821099  0.28096625 0.2798314  0.27882317
 0.27828836 0.27825817 0.27827922 0.27793393 0.2773162  0.27680883
 0.2763602  0.2755637  0.2741772  0.27260867 0.2713799  0.27049637
 0.26956087 0.26831448 0.26702797 0.26607206 0.26533154 0.2644316
 0.26327685 0.26225492 0.2615987  0.2609733  0.2600079  0.2587609
 0.25774062 0.25720432 0.25683603 0.25616145 0.25509968 0.25414962
 0.25365558 0.2532429  0.25251102 0.25158322 0.25094113 0.25079265
 0.25077716 0.2506098  0.25021163 0.24971454 0.24914548 0.24843183
 0.24776271 0.24731079 0.24717197 0.24701032 0.24655698 0.24604723
 0.24582942 0.2458682  0.245882   0.24576169 0.24580461 0.24613164
 0.24640453 0.24632744 0.2462414  0.24672925 0.2475375  0.24786752
 0.24734588 0.24663757 0.24664797 0.24735971 0.24779724 0.24762736
 0.24725132 0.24728234 0.24753584 0.24739017 0.24670649 0.2463089
 0.24675716 0.24767812 0.24823007 0.24826631 0.24836801 0.24887708
 0.24933322 0.24923587 0.248794   0.24855298 0.24872859 0.24893127
 0.24885549 0.2485277  0.24795699 0.24682133 0.2447726  0.24207363
 0.23953387 0.23776536 0.23663788 0.23586614 0.23553102 0.23580988
 0.23642364 0.23692751 0.23715383 0.23724821 0.23726724 0.23695718
 0.23619287 0.23526639 0.2346062  0.23415157 0.23338206 0.23202509
 0.23049071 0.22929566 0.22840032 0.22735879 0.22611858 0.22525027
 0.22509363 0.22520709 0.22482869 0.22385238 0.22300877 0.2225394
 0.22188504 0.22064081 0.21920887 0.21833311 0.21814696 0.21789905
 0.21710798 0.21607958 0.21551102 0.21526472 0.21481495 0.2139108
 0.21313506 0.21310085 0.21343769 0.21338008 0.21281311 0.21204823
 0.21147491 0.21099734 0.21049142 0.21008365 0.20995416 0.20971847
 0.20925531 0.20897338 0.20935999 0.210181   0.21051967 0.20978107
 0.20875329 0.20869127 0.20969194 0.21052247 0.21030354 0.20951055
 0.20930877 0.20982178 0.21017593 0.20978345 0.20947038 0.20993625
 0.21075617 0.21096505 0.21056326 0.21049915 0.21108076 0.21142116
 0.21093032 0.21038967 0.21096668 0.212542   0.2136245  0.21345119
 0.21302669 0.21356174 0.21473527 0.21524149 0.2148578  0.21466684
 0.21532106 0.2158472  0.21492428 0.21279055 0.21079834 0.20946884
 0.2079602  0.20569707 0.20355512 0.20276216 0.20312023 0.20332034
 0.20295852 0.2030983  0.20434853 0.20561558 0.20554721 0.20436883
 0.20344765 0.20326945 0.20280914 0.20128565 0.19955821 0.19880795
 0.19871987 0.19777113 0.19552682 0.19353962 0.19313751 0.19339138
 0.19253784 0.19063097 0.18932119 0.18924285 0.18897404 0.18694766
 0.18418792 0.18287164 0.18307245 0.18271357 0.18062893 0.17879094
 0.17874011 0.1790293  0.17704034 0.1738396  0.17247345 0.17344011
 0.1733181  0.17058778 0.16907024 0.17121911 0.1710626  0.1618038 ]
