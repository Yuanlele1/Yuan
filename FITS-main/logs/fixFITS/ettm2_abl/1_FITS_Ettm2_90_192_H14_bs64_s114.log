Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=24, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_192_FITS_ETTm2_ftM_sl90_ll48_pl192_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34279
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=24, out_features=75, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1612800.0
params:  1875.0
Trainable parameters:  1875
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4409398
	speed: 0.0301s/iter; left time: 800.0110s
	iters: 200, epoch: 1 | loss: 0.3288625
	speed: 0.0256s/iter; left time: 679.3605s
Epoch: 1 cost time: 7.053374767303467
Epoch: 1, Steps: 267 | Train Loss: 0.4001234 Vali Loss: 0.1859063 Test Loss: 0.2646968
Validation loss decreased (inf --> 0.185906).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3556998
	speed: 0.1574s/iter; left time: 4144.6424s
	iters: 200, epoch: 2 | loss: 0.2989594
	speed: 0.0262s/iter; left time: 687.9272s
Epoch: 2 cost time: 9.864727020263672
Epoch: 2, Steps: 267 | Train Loss: 0.3513460 Vali Loss: 0.1759541 Test Loss: 0.2528649
Validation loss decreased (0.185906 --> 0.175954).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2173767
	speed: 0.1196s/iter; left time: 3118.7126s
	iters: 200, epoch: 3 | loss: 0.2804362
	speed: 0.0319s/iter; left time: 828.5116s
Epoch: 3 cost time: 8.42930793762207
Epoch: 3, Steps: 267 | Train Loss: 0.3437000 Vali Loss: 0.1736444 Test Loss: 0.2502689
Validation loss decreased (0.175954 --> 0.173644).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2882704
	speed: 0.1879s/iter; left time: 4848.7816s
	iters: 200, epoch: 4 | loss: 0.4640073
	speed: 0.0453s/iter; left time: 1163.5935s
Epoch: 4 cost time: 13.869986295700073
Epoch: 4, Steps: 267 | Train Loss: 0.3414362 Vali Loss: 0.1731778 Test Loss: 0.2491044
Validation loss decreased (0.173644 --> 0.173178).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3677582
	speed: 0.1739s/iter; left time: 4439.9655s
	iters: 200, epoch: 5 | loss: 0.3607400
	speed: 0.0263s/iter; left time: 668.9913s
Epoch: 5 cost time: 10.240150690078735
Epoch: 5, Steps: 267 | Train Loss: 0.3396973 Vali Loss: 0.1728907 Test Loss: 0.2485867
Validation loss decreased (0.173178 --> 0.172891).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2560504
	speed: 0.1927s/iter; left time: 4867.6922s
	iters: 200, epoch: 6 | loss: 0.2772348
	speed: 0.0381s/iter; left time: 957.6936s
Epoch: 6 cost time: 9.08468246459961
Epoch: 6, Steps: 267 | Train Loss: 0.3390827 Vali Loss: 0.1729259 Test Loss: 0.2482446
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3958805
	speed: 0.1366s/iter; left time: 3415.6144s
	iters: 200, epoch: 7 | loss: 0.3195392
	speed: 0.0231s/iter; left time: 576.3260s
Epoch: 7 cost time: 9.042933940887451
Epoch: 7, Steps: 267 | Train Loss: 0.3384981 Vali Loss: 0.1726755 Test Loss: 0.2479395
Validation loss decreased (0.172891 --> 0.172676).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4073476
	speed: 0.1275s/iter; left time: 3152.8144s
	iters: 200, epoch: 8 | loss: 0.2971925
	speed: 0.0311s/iter; left time: 765.7743s
Epoch: 8 cost time: 8.094329595565796
Epoch: 8, Steps: 267 | Train Loss: 0.3376374 Vali Loss: 0.1726007 Test Loss: 0.2478523
Validation loss decreased (0.172676 --> 0.172601).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4046760
	speed: 0.1674s/iter; left time: 4094.6357s
	iters: 200, epoch: 9 | loss: 0.2754012
	speed: 0.0326s/iter; left time: 795.4957s
Epoch: 9 cost time: 10.282313346862793
Epoch: 9, Steps: 267 | Train Loss: 0.3380231 Vali Loss: 0.1727967 Test Loss: 0.2478201
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2731686
	speed: 0.1864s/iter; left time: 4509.4854s
	iters: 200, epoch: 10 | loss: 0.5283371
	speed: 0.0345s/iter; left time: 831.0585s
Epoch: 10 cost time: 9.48790693283081
Epoch: 10, Steps: 267 | Train Loss: 0.3374051 Vali Loss: 0.1726523 Test Loss: 0.2476979
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4425754
	speed: 0.1466s/iter; left time: 3507.5920s
	iters: 200, epoch: 11 | loss: 0.2710417
	speed: 0.0229s/iter; left time: 546.4898s
Epoch: 11 cost time: 6.663337230682373
Epoch: 11, Steps: 267 | Train Loss: 0.3371771 Vali Loss: 0.1728469 Test Loss: 0.2477164
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2184682
	speed: 0.1421s/iter; left time: 3362.6922s
	iters: 200, epoch: 12 | loss: 0.2841770
	speed: 0.0281s/iter; left time: 661.6468s
Epoch: 12 cost time: 7.79990291595459
Epoch: 12, Steps: 267 | Train Loss: 0.3364361 Vali Loss: 0.1726975 Test Loss: 0.2477118
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2096114
	speed: 0.1435s/iter; left time: 3357.0176s
	iters: 200, epoch: 13 | loss: 0.2409794
	speed: 0.0349s/iter; left time: 812.3692s
Epoch: 13 cost time: 9.092191219329834
Epoch: 13, Steps: 267 | Train Loss: 0.3371694 Vali Loss: 0.1729021 Test Loss: 0.2477078
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4785266
	speed: 0.1653s/iter; left time: 3822.9840s
	iters: 200, epoch: 14 | loss: 0.2724476
	speed: 0.0395s/iter; left time: 908.6641s
Epoch: 14 cost time: 10.837880373001099
Epoch: 14, Steps: 267 | Train Loss: 0.3370787 Vali Loss: 0.1728835 Test Loss: 0.2476914
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3603540
	speed: 0.1557s/iter; left time: 3560.2581s
	iters: 200, epoch: 15 | loss: 0.3302360
	speed: 0.0461s/iter; left time: 1050.2947s
Epoch: 15 cost time: 11.152868270874023
Epoch: 15, Steps: 267 | Train Loss: 0.3356835 Vali Loss: 0.1727864 Test Loss: 0.2477264
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2724287
	speed: 0.1274s/iter; left time: 2879.0772s
	iters: 200, epoch: 16 | loss: 0.3890762
	speed: 0.0201s/iter; left time: 452.9247s
Epoch: 16 cost time: 5.818864345550537
Epoch: 16, Steps: 267 | Train Loss: 0.3369147 Vali Loss: 0.1729215 Test Loss: 0.2477965
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3684371
	speed: 0.1223s/iter; left time: 2731.1925s
	iters: 200, epoch: 17 | loss: 0.2923900
	speed: 0.0203s/iter; left time: 451.5474s
Epoch: 17 cost time: 8.908159255981445
Epoch: 17, Steps: 267 | Train Loss: 0.3363210 Vali Loss: 0.1729237 Test Loss: 0.2477359
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1814170
	speed: 0.2034s/iter; left time: 4488.2033s
	iters: 200, epoch: 18 | loss: 0.3225677
	speed: 0.0444s/iter; left time: 974.2717s
Epoch: 18 cost time: 12.403932094573975
Epoch: 18, Steps: 267 | Train Loss: 0.3367923 Vali Loss: 0.1729900 Test Loss: 0.2477416
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2950296
	speed: 0.2468s/iter; left time: 5378.7678s
	iters: 200, epoch: 19 | loss: 0.4003200
	speed: 0.0395s/iter; left time: 856.3140s
Epoch: 19 cost time: 11.084518909454346
Epoch: 19, Steps: 267 | Train Loss: 0.3364306 Vali Loss: 0.1729764 Test Loss: 0.2477244
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2797546
	speed: 0.1689s/iter; left time: 3637.1206s
	iters: 200, epoch: 20 | loss: 0.4232560
	speed: 0.0308s/iter; left time: 659.1634s
Epoch: 20 cost time: 7.17201566696167
Epoch: 20, Steps: 267 | Train Loss: 0.3364118 Vali Loss: 0.1730526 Test Loss: 0.2476888
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3874373
	speed: 0.1305s/iter; left time: 2775.3170s
	iters: 200, epoch: 21 | loss: 0.4064626
	speed: 0.0174s/iter; left time: 368.1488s
Epoch: 21 cost time: 5.770158290863037
Epoch: 21, Steps: 267 | Train Loss: 0.3358649 Vali Loss: 0.1728808 Test Loss: 0.2476660
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2899911
	speed: 0.1530s/iter; left time: 3212.8658s
	iters: 200, epoch: 22 | loss: 0.2868837
	speed: 0.0315s/iter; left time: 659.0927s
Epoch: 22 cost time: 10.488445520401001
Epoch: 22, Steps: 267 | Train Loss: 0.3366614 Vali Loss: 0.1730759 Test Loss: 0.2477129
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2863558
	speed: 0.1750s/iter; left time: 3626.2506s
	iters: 200, epoch: 23 | loss: 0.2584246
	speed: 0.0424s/iter; left time: 875.4353s
Epoch: 23 cost time: 10.01697301864624
Epoch: 23, Steps: 267 | Train Loss: 0.3361593 Vali Loss: 0.1731183 Test Loss: 0.2477282
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4156441
	speed: 0.1610s/iter; left time: 3294.9197s
	iters: 200, epoch: 24 | loss: 0.2918350
	speed: 0.0366s/iter; left time: 744.2117s
Epoch: 24 cost time: 8.714730739593506
Epoch: 24, Steps: 267 | Train Loss: 0.3362959 Vali Loss: 0.1730261 Test Loss: 0.2476916
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3405641
	speed: 0.1496s/iter; left time: 3020.6814s
	iters: 200, epoch: 25 | loss: 0.2952662
	speed: 0.0182s/iter; left time: 364.9273s
Epoch: 25 cost time: 8.263978004455566
Epoch: 25, Steps: 267 | Train Loss: 0.3363389 Vali Loss: 0.1730602 Test Loss: 0.2477423
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3952525
	speed: 0.1351s/iter; left time: 2692.5013s
	iters: 200, epoch: 26 | loss: 0.2963297
	speed: 0.0276s/iter; left time: 546.3702s
Epoch: 26 cost time: 10.145656824111938
Epoch: 26, Steps: 267 | Train Loss: 0.3360642 Vali Loss: 0.1730701 Test Loss: 0.2476884
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3635696
	speed: 0.2052s/iter; left time: 4033.7169s
	iters: 200, epoch: 27 | loss: 0.5068566
	speed: 0.0203s/iter; left time: 397.6441s
Epoch: 27 cost time: 11.537925243377686
Epoch: 27, Steps: 267 | Train Loss: 0.3361847 Vali Loss: 0.1729960 Test Loss: 0.2477306
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4650529
	speed: 0.2108s/iter; left time: 4087.4691s
	iters: 200, epoch: 28 | loss: 0.3690070
	speed: 0.0603s/iter; left time: 1163.1999s
Epoch: 28 cost time: 14.193034172058105
Epoch: 28, Steps: 267 | Train Loss: 0.3363032 Vali Loss: 0.1730386 Test Loss: 0.2477172
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_192_FITS_ETTm2_ftM_sl90_ll48_pl192_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.24867825210094452, mae:0.3066413700580597, rse:0.403658390045166, corr:[0.5682018  0.5677163  0.5588016  0.55954033 0.558665   0.55435544
 0.5535801  0.5545895  0.55296993 0.55119365 0.551641   0.5513739
 0.54949516 0.54893124 0.5495111  0.54842865 0.5464119  0.5459543
 0.5459877  0.54457945 0.5430218  0.5427752  0.5424438  0.541304
 0.54072595 0.5409518  0.54050297 0.53935784 0.53896916 0.5390549
 0.53826255 0.5371347  0.5367692  0.5365283  0.53545815 0.53442174
 0.5342358  0.5339422  0.53289896 0.531966   0.5317218  0.53153265
 0.53098875 0.53065854 0.53065777 0.53023255 0.5294116  0.52880085
 0.52834874 0.5274937  0.5263504  0.52560055 0.5250986  0.52429295
 0.5234017  0.52289104 0.522561   0.5221635  0.52183604 0.5218085
 0.52178484 0.5215506  0.5213751  0.52144223 0.5214102  0.52116853
 0.52097744 0.52090824 0.5208811  0.5207886  0.52077806 0.5209303
 0.5210221  0.5209584  0.5207962  0.5206789  0.52058315 0.5203075
 0.51997846 0.51979756 0.51969033 0.51943123 0.519061   0.5188215
 0.5186762  0.51838875 0.5179578  0.5176553  0.51745415 0.51712257
 0.51669574 0.5163751  0.5160469  0.5153616  0.5141664  0.51265156
 0.5107278  0.5083404  0.50616175 0.5045803  0.50306344 0.5013162
 0.49993968 0.4991257  0.49810797 0.49651545 0.49504107 0.49408984
 0.49308246 0.49172476 0.49041918 0.489338   0.4881998  0.48684326
 0.48554468 0.48441973 0.4831535  0.48179287 0.4807535  0.48000702
 0.4791526  0.47797984 0.47684935 0.47600576 0.47519603 0.47420985
 0.47327343 0.47245225 0.47141823 0.4702072  0.46926028 0.4685714
 0.46776208 0.4667698  0.46592978 0.46551174 0.46499324 0.46448737
 0.46424598 0.46411616 0.46371973 0.4631885  0.46280673 0.46229112
 0.46134746 0.46040425 0.4596395  0.4586907  0.4574441  0.45657173
 0.4562325  0.4558253  0.45501822 0.45451725 0.4544659  0.45382622
 0.4526516  0.4523651  0.45274085 0.4525464  0.4517707  0.45151097
 0.45165843 0.45082957 0.45003593 0.45055053 0.45134988 0.45101678
 0.45051056 0.4512253  0.45189947 0.45140165 0.45100343 0.4515652
 0.45157248 0.45051688 0.45039713 0.45155692 0.45161882 0.4505579
 0.45099184 0.4523214  0.45207    0.4507456  0.4512148  0.4521521
 0.45122713 0.4511111  0.45308855 0.4533709  0.45259178 0.45405334]
