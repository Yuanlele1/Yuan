Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_180_720_FITS_ETTm2_ftM_sl180_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=22, out_features=110, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2168320.0
params:  2530.0
Trainable parameters:  2530
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7288041
	speed: 0.0277s/iter; left time: 722.3071s
	iters: 200, epoch: 1 | loss: 0.4189726
	speed: 0.0176s/iter; left time: 456.5818s
Epoch: 1 cost time: 5.567338943481445
Epoch: 1, Steps: 262 | Train Loss: 0.6884286 Vali Loss: 0.3092545 Test Loss: 0.4287678
Validation loss decreased (inf --> 0.309255).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5606616
	speed: 0.0804s/iter; left time: 2076.8994s
	iters: 200, epoch: 2 | loss: 0.4248101
	speed: 0.0166s/iter; left time: 428.5354s
Epoch: 2 cost time: 4.967998266220093
Epoch: 2, Steps: 262 | Train Loss: 0.5874598 Vali Loss: 0.2882923 Test Loss: 0.4021868
Validation loss decreased (0.309255 --> 0.288292).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5089373
	speed: 0.0884s/iter; left time: 2261.1072s
	iters: 200, epoch: 3 | loss: 0.5571433
	speed: 0.0157s/iter; left time: 399.6037s
Epoch: 3 cost time: 5.061786651611328
Epoch: 3, Steps: 262 | Train Loss: 0.5703289 Vali Loss: 0.2830265 Test Loss: 0.3957647
Validation loss decreased (0.288292 --> 0.283027).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5125293
	speed: 0.0766s/iter; left time: 1938.6101s
	iters: 200, epoch: 4 | loss: 0.7461428
	speed: 0.0154s/iter; left time: 388.0547s
Epoch: 4 cost time: 4.626511096954346
Epoch: 4, Steps: 262 | Train Loss: 0.5650309 Vali Loss: 0.2809949 Test Loss: 0.3930453
Validation loss decreased (0.283027 --> 0.280995).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.9332120
	speed: 0.0735s/iter; left time: 1841.0622s
	iters: 200, epoch: 5 | loss: 0.6620052
	speed: 0.0158s/iter; left time: 394.2524s
Epoch: 5 cost time: 4.6202778816223145
Epoch: 5, Steps: 262 | Train Loss: 0.5625781 Vali Loss: 0.2798265 Test Loss: 0.3914524
Validation loss decreased (0.280995 --> 0.279826).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6633136
	speed: 0.0732s/iter; left time: 1815.3296s
	iters: 200, epoch: 6 | loss: 0.6329058
	speed: 0.0157s/iter; left time: 387.1756s
Epoch: 6 cost time: 4.5579211711883545
Epoch: 6, Steps: 262 | Train Loss: 0.5612645 Vali Loss: 0.2790378 Test Loss: 0.3904791
Validation loss decreased (0.279826 --> 0.279038).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5669699
	speed: 0.0724s/iter; left time: 1776.6395s
	iters: 200, epoch: 7 | loss: 0.7253426
	speed: 0.0152s/iter; left time: 371.4257s
Epoch: 7 cost time: 4.521917343139648
Epoch: 7, Steps: 262 | Train Loss: 0.5603748 Vali Loss: 0.2785021 Test Loss: 0.3897272
Validation loss decreased (0.279038 --> 0.278502).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4289770
	speed: 0.0754s/iter; left time: 1829.9739s
	iters: 200, epoch: 8 | loss: 0.6369647
	speed: 0.0151s/iter; left time: 365.7811s
Epoch: 8 cost time: 4.569195032119751
Epoch: 8, Steps: 262 | Train Loss: 0.5588760 Vali Loss: 0.2784438 Test Loss: 0.3892443
Validation loss decreased (0.278502 --> 0.278444).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4531878
	speed: 0.0738s/iter; left time: 1772.5402s
	iters: 200, epoch: 9 | loss: 0.5356163
	speed: 0.0149s/iter; left time: 355.6691s
Epoch: 9 cost time: 4.394735813140869
Epoch: 9, Steps: 262 | Train Loss: 0.5585698 Vali Loss: 0.2781628 Test Loss: 0.3888220
Validation loss decreased (0.278444 --> 0.278163).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4738716
	speed: 0.0754s/iter; left time: 1790.4691s
	iters: 200, epoch: 10 | loss: 0.4806349
	speed: 0.0159s/iter; left time: 375.0801s
Epoch: 10 cost time: 4.697927951812744
Epoch: 10, Steps: 262 | Train Loss: 0.5581595 Vali Loss: 0.2779757 Test Loss: 0.3884927
Validation loss decreased (0.278163 --> 0.277976).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6435320
	speed: 0.0752s/iter; left time: 1766.2231s
	iters: 200, epoch: 11 | loss: 0.6565160
	speed: 0.0155s/iter; left time: 361.5823s
Epoch: 11 cost time: 4.514615774154663
Epoch: 11, Steps: 262 | Train Loss: 0.5572000 Vali Loss: 0.2778711 Test Loss: 0.3883193
Validation loss decreased (0.277976 --> 0.277871).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.7475668
	speed: 0.0717s/iter; left time: 1665.7973s
	iters: 200, epoch: 12 | loss: 0.6751134
	speed: 0.0153s/iter; left time: 353.1812s
Epoch: 12 cost time: 4.456672191619873
Epoch: 12, Steps: 262 | Train Loss: 0.5576688 Vali Loss: 0.2775196 Test Loss: 0.3881046
Validation loss decreased (0.277871 --> 0.277520).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4942197
	speed: 0.0725s/iter; left time: 1664.6874s
	iters: 200, epoch: 13 | loss: 0.5487453
	speed: 0.0152s/iter; left time: 347.6511s
Epoch: 13 cost time: 4.497310161590576
Epoch: 13, Steps: 262 | Train Loss: 0.5568371 Vali Loss: 0.2774690 Test Loss: 0.3879332
Validation loss decreased (0.277520 --> 0.277469).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5199160
	speed: 0.0729s/iter; left time: 1654.1292s
	iters: 200, epoch: 14 | loss: 0.5761927
	speed: 0.0154s/iter; left time: 347.1186s
Epoch: 14 cost time: 4.6201934814453125
Epoch: 14, Steps: 262 | Train Loss: 0.5569162 Vali Loss: 0.2775077 Test Loss: 0.3878092
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5562397
	speed: 0.0757s/iter; left time: 1698.0242s
	iters: 200, epoch: 15 | loss: 0.5595787
	speed: 0.0159s/iter; left time: 354.7703s
Epoch: 15 cost time: 4.779613494873047
Epoch: 15, Steps: 262 | Train Loss: 0.5563367 Vali Loss: 0.2772036 Test Loss: 0.3876340
Validation loss decreased (0.277469 --> 0.277204).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5106944
	speed: 0.0762s/iter; left time: 1689.7210s
	iters: 200, epoch: 16 | loss: 0.5187325
	speed: 0.0156s/iter; left time: 343.5337s
Epoch: 16 cost time: 4.676191329956055
Epoch: 16, Steps: 262 | Train Loss: 0.5559287 Vali Loss: 0.2773365 Test Loss: 0.3875581
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4586851
	speed: 0.0754s/iter; left time: 1650.9438s
	iters: 200, epoch: 17 | loss: 0.4394003
	speed: 0.0157s/iter; left time: 343.4123s
Epoch: 17 cost time: 4.600873231887817
Epoch: 17, Steps: 262 | Train Loss: 0.5563484 Vali Loss: 0.2770188 Test Loss: 0.3874832
Validation loss decreased (0.277204 --> 0.277019).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.8094271
	speed: 0.0741s/iter; left time: 1603.2434s
	iters: 200, epoch: 18 | loss: 0.4008581
	speed: 0.0151s/iter; left time: 324.6373s
Epoch: 18 cost time: 4.524418592453003
Epoch: 18, Steps: 262 | Train Loss: 0.5559413 Vali Loss: 0.2772041 Test Loss: 0.3873939
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4904653
	speed: 0.0753s/iter; left time: 1609.5041s
	iters: 200, epoch: 19 | loss: 0.4172279
	speed: 0.0153s/iter; left time: 324.8800s
Epoch: 19 cost time: 4.576416254043579
Epoch: 19, Steps: 262 | Train Loss: 0.5548545 Vali Loss: 0.2773077 Test Loss: 0.3873255
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4584226
	speed: 0.0737s/iter; left time: 1556.9031s
	iters: 200, epoch: 20 | loss: 0.5036318
	speed: 0.0151s/iter; left time: 317.9338s
Epoch: 20 cost time: 4.479230642318726
Epoch: 20, Steps: 262 | Train Loss: 0.5556963 Vali Loss: 0.2772257 Test Loss: 0.3872618
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5977667
	speed: 0.0762s/iter; left time: 1588.8561s
	iters: 200, epoch: 21 | loss: 0.4164116
	speed: 0.0154s/iter; left time: 319.4161s
Epoch: 21 cost time: 4.678303956985474
Epoch: 21, Steps: 262 | Train Loss: 0.5551348 Vali Loss: 0.2771968 Test Loss: 0.3872300
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.6692638
	speed: 0.0722s/iter; left time: 1486.4610s
	iters: 200, epoch: 22 | loss: 0.6259912
	speed: 0.0150s/iter; left time: 306.5101s
Epoch: 22 cost time: 4.467941761016846
Epoch: 22, Steps: 262 | Train Loss: 0.5559419 Vali Loss: 0.2771259 Test Loss: 0.3871694
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.6607750
	speed: 0.0739s/iter; left time: 1503.9166s
	iters: 200, epoch: 23 | loss: 0.5312861
	speed: 0.0152s/iter; left time: 308.2063s
Epoch: 23 cost time: 4.524425745010376
Epoch: 23, Steps: 262 | Train Loss: 0.5549558 Vali Loss: 0.2771018 Test Loss: 0.3871481
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4972231
	speed: 0.0724s/iter; left time: 1452.4805s
	iters: 200, epoch: 24 | loss: 0.4921021
	speed: 0.0151s/iter; left time: 301.7125s
Epoch: 24 cost time: 4.521580457687378
Epoch: 24, Steps: 262 | Train Loss: 0.5551265 Vali Loss: 0.2767832 Test Loss: 0.3871048
Validation loss decreased (0.277019 --> 0.276783).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5067787
	speed: 0.0720s/iter; left time: 1427.3724s
	iters: 200, epoch: 25 | loss: 0.5513195
	speed: 0.0154s/iter; left time: 303.4577s
Epoch: 25 cost time: 4.4897847175598145
Epoch: 25, Steps: 262 | Train Loss: 0.5553868 Vali Loss: 0.2770379 Test Loss: 0.3870525
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.9012409
	speed: 0.0747s/iter; left time: 1460.2574s
	iters: 200, epoch: 26 | loss: 0.8279868
	speed: 0.0227s/iter; left time: 440.6420s
Epoch: 26 cost time: 5.4897119998931885
Epoch: 26, Steps: 262 | Train Loss: 0.5556165 Vali Loss: 0.2768925 Test Loss: 0.3870594
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.7285569
	speed: 0.0728s/iter; left time: 1403.8557s
	iters: 200, epoch: 27 | loss: 0.5150371
	speed: 0.0149s/iter; left time: 286.3923s
Epoch: 27 cost time: 4.482985019683838
Epoch: 27, Steps: 262 | Train Loss: 0.5553424 Vali Loss: 0.2772077 Test Loss: 0.3870262
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5363958
	speed: 0.0736s/iter; left time: 1399.4776s
	iters: 200, epoch: 28 | loss: 0.5764707
	speed: 0.0155s/iter; left time: 293.4115s
Epoch: 28 cost time: 4.5794782638549805
Epoch: 28, Steps: 262 | Train Loss: 0.5548266 Vali Loss: 0.2771908 Test Loss: 0.3870052
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.6378205
	speed: 0.0723s/iter; left time: 1356.1910s
	iters: 200, epoch: 29 | loss: 0.5782405
	speed: 0.0152s/iter; left time: 284.3858s
Epoch: 29 cost time: 4.508090496063232
Epoch: 29, Steps: 262 | Train Loss: 0.5545240 Vali Loss: 0.2768663 Test Loss: 0.3869801
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4623466
	speed: 0.0745s/iter; left time: 1378.8935s
	iters: 200, epoch: 30 | loss: 0.4995877
	speed: 0.0165s/iter; left time: 302.7631s
Epoch: 30 cost time: 4.850348949432373
Epoch: 30, Steps: 262 | Train Loss: 0.5551025 Vali Loss: 0.2768850 Test Loss: 0.3869700
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.5541888
	speed: 0.1139s/iter; left time: 2076.9090s
	iters: 200, epoch: 31 | loss: 0.7969885
	speed: 0.0481s/iter; left time: 873.2747s
Epoch: 31 cost time: 10.726362705230713
Epoch: 31, Steps: 262 | Train Loss: 0.5550690 Vali Loss: 0.2771699 Test Loss: 0.3869492
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.5544743
	speed: 0.1748s/iter; left time: 3142.0343s
	iters: 200, epoch: 32 | loss: 0.4493465
	speed: 0.0535s/iter; left time: 956.4052s
Epoch: 32 cost time: 12.001446008682251
Epoch: 32, Steps: 262 | Train Loss: 0.5551573 Vali Loss: 0.2770920 Test Loss: 0.3869421
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.5405067
	speed: 0.1931s/iter; left time: 3421.4925s
	iters: 200, epoch: 33 | loss: 0.4961638
	speed: 0.0510s/iter; left time: 898.5894s
Epoch: 33 cost time: 12.566574811935425
Epoch: 33, Steps: 262 | Train Loss: 0.5557937 Vali Loss: 0.2771003 Test Loss: 0.3869181
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4889253
	speed: 0.1998s/iter; left time: 3486.6682s
	iters: 200, epoch: 34 | loss: 0.6536022
	speed: 0.0596s/iter; left time: 1033.8566s
Epoch: 34 cost time: 14.931281328201294
Epoch: 34, Steps: 262 | Train Loss: 0.5551855 Vali Loss: 0.2768485 Test Loss: 0.3868972
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.6172208
	speed: 0.2057s/iter; left time: 3535.8182s
	iters: 200, epoch: 35 | loss: 0.5403185
	speed: 0.0463s/iter; left time: 792.0026s
Epoch: 35 cost time: 14.514143943786621
Epoch: 35, Steps: 262 | Train Loss: 0.5550269 Vali Loss: 0.2769166 Test Loss: 0.3868881
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.6673875
	speed: 0.2017s/iter; left time: 3414.6344s
	iters: 200, epoch: 36 | loss: 0.6742398
	speed: 0.0541s/iter; left time: 909.8838s
Epoch: 36 cost time: 13.333821773529053
Epoch: 36, Steps: 262 | Train Loss: 0.5556597 Vali Loss: 0.2767830 Test Loss: 0.3868831
Validation loss decreased (0.276783 --> 0.276783).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.5276628
	speed: 0.1982s/iter; left time: 3303.0785s
	iters: 200, epoch: 37 | loss: 0.6109913
	speed: 0.0583s/iter; left time: 965.4169s
Epoch: 37 cost time: 12.313587427139282
Epoch: 37, Steps: 262 | Train Loss: 0.5557023 Vali Loss: 0.2768080 Test Loss: 0.3868736
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.5246718
	speed: 0.1246s/iter; left time: 2043.6207s
	iters: 200, epoch: 38 | loss: 0.4356009
	speed: 0.0155s/iter; left time: 252.5405s
Epoch: 38 cost time: 4.521974563598633
Epoch: 38, Steps: 262 | Train Loss: 0.5547277 Vali Loss: 0.2769536 Test Loss: 0.3868671
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.5293493
	speed: 0.0789s/iter; left time: 1274.4040s
	iters: 200, epoch: 39 | loss: 0.5647016
	speed: 0.0149s/iter; left time: 238.7969s
Epoch: 39 cost time: 4.5912086963653564
Epoch: 39, Steps: 262 | Train Loss: 0.5555647 Vali Loss: 0.2771662 Test Loss: 0.3868615
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.5193141
	speed: 0.0823s/iter; left time: 1306.8067s
	iters: 200, epoch: 40 | loss: 0.5219296
	speed: 0.0172s/iter; left time: 270.8691s
Epoch: 40 cost time: 5.060348272323608
Epoch: 40, Steps: 262 | Train Loss: 0.5548153 Vali Loss: 0.2770037 Test Loss: 0.3868546
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.6460014
	speed: 0.0776s/iter; left time: 1212.1025s
	iters: 200, epoch: 41 | loss: 0.5580283
	speed: 0.0160s/iter; left time: 248.0079s
Epoch: 41 cost time: 4.772508859634399
Epoch: 41, Steps: 262 | Train Loss: 0.5547227 Vali Loss: 0.2768640 Test Loss: 0.3868518
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4823827
	speed: 0.0741s/iter; left time: 1138.2580s
	iters: 200, epoch: 42 | loss: 0.4082594
	speed: 0.0156s/iter; left time: 237.8624s
Epoch: 42 cost time: 4.650087594985962
Epoch: 42, Steps: 262 | Train Loss: 0.5550957 Vali Loss: 0.2770695 Test Loss: 0.3868475
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.5465955
	speed: 0.0730s/iter; left time: 1101.5299s
	iters: 200, epoch: 43 | loss: 0.3310257
	speed: 0.0184s/iter; left time: 276.0006s
Epoch: 43 cost time: 4.854522228240967
Epoch: 43, Steps: 262 | Train Loss: 0.5551513 Vali Loss: 0.2769391 Test Loss: 0.3868422
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.7160147
	speed: 0.0747s/iter; left time: 1108.6967s
	iters: 200, epoch: 44 | loss: 0.6198850
	speed: 0.0151s/iter; left time: 222.0844s
Epoch: 44 cost time: 4.527493953704834
Epoch: 44, Steps: 262 | Train Loss: 0.5555488 Vali Loss: 0.2767938 Test Loss: 0.3868295
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5391912
	speed: 0.0754s/iter; left time: 1099.2094s
	iters: 200, epoch: 45 | loss: 0.5840473
	speed: 0.0155s/iter; left time: 224.7199s
Epoch: 45 cost time: 4.691023826599121
Epoch: 45, Steps: 262 | Train Loss: 0.5550769 Vali Loss: 0.2769785 Test Loss: 0.3868241
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.6034958
	speed: 0.0730s/iter; left time: 1044.1505s
	iters: 200, epoch: 46 | loss: 0.8082197
	speed: 0.0150s/iter; left time: 212.9177s
Epoch: 46 cost time: 4.589648962020874
Epoch: 46, Steps: 262 | Train Loss: 0.5544255 Vali Loss: 0.2770079 Test Loss: 0.3868132
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4964231
	speed: 0.0752s/iter; left time: 1056.9608s
	iters: 200, epoch: 47 | loss: 0.6241945
	speed: 0.0159s/iter; left time: 222.1165s
Epoch: 47 cost time: 4.818441867828369
Epoch: 47, Steps: 262 | Train Loss: 0.5554639 Vali Loss: 0.2768355 Test Loss: 0.3868165
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.6476663
	speed: 0.0713s/iter; left time: 983.5465s
	iters: 200, epoch: 48 | loss: 0.8181380
	speed: 0.0154s/iter; left time: 210.3734s
Epoch: 48 cost time: 4.539444923400879
Epoch: 48, Steps: 262 | Train Loss: 0.5546661 Vali Loss: 0.2764702 Test Loss: 0.3868069
Validation loss decreased (0.276783 --> 0.276470).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4361688
	speed: 0.0756s/iter; left time: 1022.5323s
	iters: 200, epoch: 49 | loss: 0.3975757
	speed: 0.0153s/iter; left time: 205.9349s
Epoch: 49 cost time: 4.485531806945801
Epoch: 49, Steps: 262 | Train Loss: 0.5546350 Vali Loss: 0.2766036 Test Loss: 0.3868034
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.6020190
	speed: 0.0732s/iter; left time: 971.0996s
	iters: 200, epoch: 50 | loss: 0.7247001
	speed: 0.0157s/iter; left time: 206.2400s
Epoch: 50 cost time: 4.574801206588745
Epoch: 50, Steps: 262 | Train Loss: 0.5548380 Vali Loss: 0.2768130 Test Loss: 0.3867988
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4319346
	speed: 0.0723s/iter; left time: 939.4526s
	iters: 200, epoch: 51 | loss: 0.5768952
	speed: 0.0156s/iter; left time: 200.7724s
Epoch: 51 cost time: 4.526881217956543
Epoch: 51, Steps: 262 | Train Loss: 0.5551490 Vali Loss: 0.2769592 Test Loss: 0.3867972
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.5995923
	speed: 0.0755s/iter; left time: 961.2519s
	iters: 200, epoch: 52 | loss: 0.5048264
	speed: 0.0366s/iter; left time: 463.1973s
Epoch: 52 cost time: 10.094607591629028
Epoch: 52, Steps: 262 | Train Loss: 0.5553412 Vali Loss: 0.2767502 Test Loss: 0.3867970
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.6151611
	speed: 0.2130s/iter; left time: 2657.6006s
	iters: 200, epoch: 53 | loss: 0.7918536
	speed: 0.0488s/iter; left time: 603.7045s
Epoch: 53 cost time: 12.34342098236084
Epoch: 53, Steps: 262 | Train Loss: 0.5544914 Vali Loss: 0.2768336 Test Loss: 0.3867910
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.5754490
	speed: 0.1925s/iter; left time: 2351.9706s
	iters: 200, epoch: 54 | loss: 0.3943305
	speed: 0.0485s/iter; left time: 587.5949s
Epoch: 54 cost time: 12.489468574523926
Epoch: 54, Steps: 262 | Train Loss: 0.5549246 Vali Loss: 0.2769702 Test Loss: 0.3867894
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.6344405
	speed: 0.1849s/iter; left time: 2209.7051s
	iters: 200, epoch: 55 | loss: 0.9615376
	speed: 0.0517s/iter; left time: 612.3239s
Epoch: 55 cost time: 11.336977005004883
Epoch: 55, Steps: 262 | Train Loss: 0.5552522 Vali Loss: 0.2766159 Test Loss: 0.3867837
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4676594
	speed: 0.1931s/iter; left time: 2257.7746s
	iters: 200, epoch: 56 | loss: 0.7247961
	speed: 0.0542s/iter; left time: 628.0170s
Epoch: 56 cost time: 12.577080726623535
Epoch: 56, Steps: 262 | Train Loss: 0.5544195 Vali Loss: 0.2768898 Test Loss: 0.3867839
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.5515344
	speed: 0.0751s/iter; left time: 858.8228s
	iters: 200, epoch: 57 | loss: 0.7004298
	speed: 0.0155s/iter; left time: 175.5186s
Epoch: 57 cost time: 4.674847364425659
Epoch: 57, Steps: 262 | Train Loss: 0.5550186 Vali Loss: 0.2767175 Test Loss: 0.3867792
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.5861275
	speed: 0.0733s/iter; left time: 818.9189s
	iters: 200, epoch: 58 | loss: 0.6426213
	speed: 0.0157s/iter; left time: 173.8936s
Epoch: 58 cost time: 4.674445390701294
Epoch: 58, Steps: 262 | Train Loss: 0.5551193 Vali Loss: 0.2769690 Test Loss: 0.3867792
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.5295306
	speed: 0.0765s/iter; left time: 834.7465s
	iters: 200, epoch: 59 | loss: 0.5145293
	speed: 0.0197s/iter; left time: 212.6511s
Epoch: 59 cost time: 5.081558465957642
Epoch: 59, Steps: 262 | Train Loss: 0.5547976 Vali Loss: 0.2767892 Test Loss: 0.3867775
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.5952714
	speed: 0.0755s/iter; left time: 803.2641s
	iters: 200, epoch: 60 | loss: 0.4484316
	speed: 0.0155s/iter; left time: 163.2217s
Epoch: 60 cost time: 4.650029182434082
Epoch: 60, Steps: 262 | Train Loss: 0.5540805 Vali Loss: 0.2765517 Test Loss: 0.3867765
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.5190055
	speed: 0.0750s/iter; left time: 778.4738s
	iters: 200, epoch: 61 | loss: 0.6990767
	speed: 0.0152s/iter; left time: 156.5747s
Epoch: 61 cost time: 4.546456575393677
Epoch: 61, Steps: 262 | Train Loss: 0.5548444 Vali Loss: 0.2769450 Test Loss: 0.3867729
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.6247188
	speed: 0.0757s/iter; left time: 766.1756s
	iters: 200, epoch: 62 | loss: 0.6564491
	speed: 0.0153s/iter; left time: 152.8084s
Epoch: 62 cost time: 4.6053080558776855
Epoch: 62, Steps: 262 | Train Loss: 0.5541311 Vali Loss: 0.2766435 Test Loss: 0.3867739
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.4940546
	speed: 0.0744s/iter; left time: 733.6309s
	iters: 200, epoch: 63 | loss: 0.6020726
	speed: 0.0183s/iter; left time: 178.2748s
Epoch: 63 cost time: 4.89905858039856
Epoch: 63, Steps: 262 | Train Loss: 0.5549920 Vali Loss: 0.2770543 Test Loss: 0.3867702
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.6575171
	speed: 0.0780s/iter; left time: 747.9557s
	iters: 200, epoch: 64 | loss: 0.4857618
	speed: 0.0158s/iter; left time: 149.8051s
Epoch: 64 cost time: 4.655679225921631
Epoch: 64, Steps: 262 | Train Loss: 0.5546153 Vali Loss: 0.2766310 Test Loss: 0.3867696
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.7125650
	speed: 0.0730s/iter; left time: 680.9882s
	iters: 200, epoch: 65 | loss: 0.5036886
	speed: 0.0165s/iter; left time: 152.6812s
Epoch: 65 cost time: 4.568604469299316
Epoch: 65, Steps: 262 | Train Loss: 0.5542499 Vali Loss: 0.2768901 Test Loss: 0.3867687
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.4594263
	speed: 0.0724s/iter; left time: 657.1763s
	iters: 200, epoch: 66 | loss: 0.6073419
	speed: 0.0151s/iter; left time: 135.1983s
Epoch: 66 cost time: 4.507032871246338
Epoch: 66, Steps: 262 | Train Loss: 0.5550369 Vali Loss: 0.2767891 Test Loss: 0.3867666
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.3531812
	speed: 0.0753s/iter; left time: 663.0575s
	iters: 200, epoch: 67 | loss: 0.4363613
	speed: 0.0149s/iter; left time: 129.5058s
Epoch: 67 cost time: 4.552615404129028
Epoch: 67, Steps: 262 | Train Loss: 0.5541548 Vali Loss: 0.2767872 Test Loss: 0.3867642
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.4328045
	speed: 0.0728s/iter; left time: 622.5108s
	iters: 200, epoch: 68 | loss: 0.6069041
	speed: 0.0151s/iter; left time: 127.9286s
Epoch: 68 cost time: 4.625005006790161
Epoch: 68, Steps: 262 | Train Loss: 0.5542759 Vali Loss: 0.2767695 Test Loss: 0.3867623
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_180_720_FITS_ETTm2_ftM_sl180_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3841637969017029, mae:0.3876640200614929, rse:0.4981987178325653, corr:[0.54516137 0.5460153  0.5432695  0.53995425 0.5379191  0.5371471
 0.5370011  0.5366257  0.5357038  0.53446597 0.5333245  0.5326406
 0.5323907  0.53225243 0.5320101  0.5314184  0.53044516 0.5292877
 0.5281898  0.5273331  0.526781   0.52641416 0.52605534 0.5256063
 0.52501863 0.5243395  0.52369934 0.5231684  0.5227191  0.52225506
 0.5217571  0.5212437  0.52070075 0.5201468  0.51959175 0.51906925
 0.5185146  0.5179489  0.5173157  0.5166466  0.51601875 0.51547724
 0.51504385 0.5146669  0.5142408  0.5137097  0.5130285  0.512212
 0.51129335 0.51034147 0.50943756 0.50869924 0.5080856  0.50751495
 0.5069654  0.5064567  0.505953   0.5055352  0.50520486 0.50496733
 0.5047677  0.50460964 0.5044838  0.5042863  0.5040478  0.50382745
 0.50361013 0.50345916 0.50336355 0.503314   0.5032329  0.5030933
 0.50293386 0.502712   0.5024851  0.502216   0.50192136 0.5015621
 0.50117797 0.5007659  0.50032985 0.49989128 0.49949402 0.4991048
 0.49869147 0.49823415 0.4977326  0.4972783  0.49688905 0.49656126
 0.49626622 0.49590215 0.49532318 0.49439287 0.49303356 0.49126562
 0.4892525  0.48733795 0.48563823 0.48412296 0.48272052 0.48136228
 0.4799956  0.47861883 0.47723034 0.47590092 0.4747066  0.4736511
 0.47262493 0.47141883 0.4701331  0.4688029  0.46755153 0.4664375
 0.46541294 0.46448517 0.46358916 0.462638   0.46160692 0.46046013
 0.4592993  0.45824927 0.45736328 0.4565969  0.45584986 0.4550049
 0.45401236 0.45289883 0.4517682  0.45069996 0.44977582 0.4489849
 0.44829258 0.44758758 0.44691852 0.4462959  0.4456921  0.44513175
 0.44458094 0.44402778 0.4434328  0.44277063 0.44203916 0.44118705
 0.44021094 0.43917337 0.43812114 0.43717277 0.43643072 0.4358466
 0.43538105 0.4350488  0.43469274 0.4342709  0.43377388 0.43318594
 0.43258753 0.43206325 0.43159354 0.43121028 0.4309254  0.43069935
 0.4305433  0.43034014 0.43021277 0.43013355 0.43007863 0.43008873
 0.43015307 0.4302728  0.430375   0.43042237 0.4304125  0.4302801
 0.43007487 0.4298235  0.4296068  0.4294238  0.42927995 0.4291503
 0.42894104 0.42862272 0.42829597 0.42799038 0.42776775 0.42755988
 0.42733356 0.42698437 0.4264151  0.42552203 0.42424345 0.42260867
 0.42078283 0.41906494 0.4174955  0.41605    0.41476312 0.4135278
 0.41227165 0.41098472 0.40969488 0.40858954 0.4076605  0.40693873
 0.40631    0.40565696 0.40491167 0.404107   0.40332094 0.4025851
 0.4018601  0.4011111  0.40031657 0.399483   0.3986427  0.39778307
 0.39693743 0.39600766 0.39509407 0.39415684 0.39314175 0.39211756
 0.3910859  0.39012203 0.389328   0.38855094 0.38775066 0.38689792
 0.38594615 0.38493246 0.38394108 0.38309908 0.38244683 0.38199732
 0.3816349  0.38129398 0.3809714  0.38053828 0.38004717 0.37946787
 0.37884656 0.3782062  0.37760887 0.37722018 0.37700754 0.37692893
 0.3769398  0.37699896 0.37712622 0.37729883 0.377536   0.37781253
 0.3780218  0.3781928  0.37826195 0.37819022 0.37801692 0.37787017
 0.37779605 0.37775505 0.37773213 0.37774935 0.37774175 0.37765557
 0.3775576  0.37751484 0.3775387  0.3775854  0.37761572 0.37758875
 0.3774233  0.37714717 0.37679666 0.37651324 0.37635577 0.37633365
 0.37634817 0.3762985  0.37617657 0.3760333  0.37584072 0.375726
 0.37567142 0.3755653  0.37530604 0.37478873 0.37395385 0.3727992
 0.37143287 0.3702017  0.36915404 0.3683547  0.3677138  0.36711925
 0.3665852  0.3660016  0.3653655  0.36471596 0.36417282 0.3637247
 0.3633254  0.36276653 0.3621156  0.36141947 0.36072165 0.36004108
 0.35942125 0.3588213  0.35814098 0.35738102 0.3565695  0.35579476
 0.3550519  0.354383   0.3537097  0.35301402 0.35219628 0.35128513
 0.35035387 0.3494822  0.3487347  0.34811214 0.3475726  0.347024
 0.34642223 0.34575596 0.3450839  0.34447634 0.34397024 0.34361574
 0.34341896 0.34325334 0.34303916 0.34264535 0.34204066 0.34134606
 0.3406095  0.33991313 0.33931956 0.33896014 0.33875084 0.3386172
 0.33849606 0.33838293 0.33826527 0.33820477 0.33816844 0.33820742
 0.33824405 0.33824196 0.33816475 0.33802596 0.3378806  0.33778375
 0.33774665 0.33777753 0.33779642 0.3378037  0.33776373 0.33768928
 0.33763602 0.33764142 0.33770442 0.33781642 0.33792594 0.3380569
 0.33813068 0.33815056 0.33812797 0.33808824 0.33817667 0.33832857
 0.3385263  0.33875987 0.3389693  0.3391116  0.33915454 0.33910105
 0.3389807  0.3387499  0.33835873 0.33772647 0.33687523 0.3357954
 0.3345777  0.33334506 0.33220118 0.3311739  0.33016884 0.32921657
 0.32826847 0.3272892  0.32632703 0.32546526 0.3247088  0.32407147
 0.32348672 0.32282555 0.3220667  0.3212918  0.3205656  0.31988358
 0.31927612 0.31864396 0.31796962 0.31725267 0.31646532 0.31562215
 0.3148237  0.31412116 0.31352988 0.3130072  0.31249437 0.31195354
 0.31136483 0.31073412 0.3100898  0.30951533 0.3089772  0.30854094
 0.30816695 0.30777693 0.30736637 0.30701447 0.30668068 0.30642763
 0.3063307  0.30636698 0.3064111  0.30636173 0.30607533 0.30555433
 0.304789   0.30398512 0.30327138 0.30283886 0.3026675  0.30262578
 0.30263415 0.30260012 0.30249146 0.30226645 0.3019446  0.30166674
 0.30147287 0.30139133 0.30140963 0.30141544 0.3013564  0.30124983
 0.3011222  0.3009527  0.30083087 0.30082712 0.3008572  0.30093867
 0.30099478 0.30101213 0.3009538  0.3008568  0.30077776 0.30071142
 0.30064067 0.30060196 0.300609   0.30064577 0.30062664 0.3005573
 0.300449   0.30026487 0.30003116 0.29973498 0.2994377  0.2991465
 0.29876766 0.2982309  0.29748216 0.29643175 0.29506034 0.2934578
 0.29176447 0.2902301  0.2889352  0.2879171  0.2870872  0.2863101
 0.28551054 0.28465408 0.2838309  0.28319943 0.28284037 0.28266606
 0.2825346  0.28229678 0.28191233 0.2814015  0.28083247 0.2802211
 0.27961853 0.27897817 0.27826068 0.27750227 0.2767155  0.2759498
 0.27517623 0.27441648 0.273695   0.27298158 0.272306   0.27168682
 0.27112147 0.27060282 0.27009818 0.2695576  0.26895782 0.26832858
 0.267703   0.2671169  0.26660922 0.2661659  0.26579884 0.26547968
 0.26515073 0.26482356 0.2644632  0.2640027  0.26351532 0.26294294
 0.26232752 0.26166517 0.26096752 0.26029885 0.2597611  0.25934792
 0.25909376 0.25900525 0.25909102 0.2592251  0.2593558  0.25932312
 0.2591927  0.25902537 0.25881827 0.2586332  0.25847027 0.25831777
 0.25810623 0.25786075 0.25761148 0.25739595 0.25720108 0.25710943
 0.25712085 0.25723022 0.25737914 0.25745985 0.25742182 0.25730687
 0.25708914 0.25691175 0.2568126  0.25680658 0.25688004 0.25696358
 0.2569575  0.2568671  0.2567116  0.25651568 0.2563465  0.25617993
 0.25600246 0.2556737  0.25507978 0.25407383 0.25263885 0.25078675
 0.24876612 0.24697392 0.24551986 0.24443027 0.2436134  0.24292286
 0.24214141 0.24123691 0.24019872 0.2391097  0.23817445 0.23755594
 0.23717406 0.23679651 0.23636998 0.23586132 0.23521453 0.23443513
 0.23355298 0.23264581 0.23182891 0.23110622 0.23052615 0.2300183
 0.2295508  0.22913821 0.22868265 0.22808218 0.22743581 0.2267709
 0.22607717 0.22547759 0.22500497 0.22457255 0.2242004  0.2237587
 0.22327328 0.22270404 0.22215952 0.22177975 0.221651   0.22177629
 0.2219411  0.22204024 0.22181123 0.22135715 0.22077724 0.22015567
 0.21967678 0.21939534 0.2193239  0.21936502 0.21937998 0.2192783
 0.21914013 0.21915619 0.21929531 0.21952228 0.21987931 0.22015943
 0.22035289 0.22034585 0.22011334 0.21979856 0.21958844 0.21955337
 0.21972547 0.22003973 0.22038119 0.2205466  0.2204927  0.22028972
 0.2200443  0.21998698 0.22010389 0.22046113 0.22092372 0.22128381
 0.22151472 0.22154796 0.2214799  0.22155905 0.22186975 0.22233114
 0.22293665 0.22354366 0.22409648 0.22454336 0.22486371 0.2250671
 0.22515598 0.2251312  0.2248863  0.22428249 0.22320002 0.22169523
 0.22000837 0.21845333 0.21717182 0.21617252 0.21539538 0.21477899
 0.21422763 0.21372648 0.21328826 0.21287823 0.21248044 0.21210828
 0.21178634 0.21133727 0.21085665 0.21036407 0.20992593 0.2094086
 0.20873287 0.20784327 0.20676138 0.20558588 0.2044628  0.20340048
 0.20247543 0.20161846 0.20073491 0.19977432 0.19865304 0.19744918
 0.19620088 0.19507872 0.1941074  0.19335    0.19261765 0.19175558
 0.19069216 0.18965983 0.188897   0.1886902  0.18894087 0.18951675
 0.18982644 0.18952243 0.18872318 0.18829904 0.18972608 0.19417179]
