Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=22, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3902976.0
params:  4554.0
Trainable parameters:  4554
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5208837
	speed: 0.0230s/iter; left time: 601.3968s
	iters: 200, epoch: 1 | loss: 0.5382923
	speed: 0.0221s/iter; left time: 575.5559s
Epoch: 1 cost time: 5.433666706085205
Epoch: 1, Steps: 263 | Train Loss: 0.7030202 Vali Loss: 0.3402400 Test Loss: 0.4738375
Validation loss decreased (inf --> 0.340240).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5888926
	speed: 0.0849s/iter; left time: 2202.3520s
	iters: 200, epoch: 2 | loss: 0.5629422
	speed: 0.0166s/iter; left time: 428.4335s
Epoch: 2 cost time: 5.8016357421875
Epoch: 2, Steps: 263 | Train Loss: 0.5579248 Vali Loss: 0.2988611 Test Loss: 0.4238974
Validation loss decreased (0.340240 --> 0.298861).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6530143
	speed: 0.0786s/iter; left time: 2017.6508s
	iters: 200, epoch: 3 | loss: 0.3603731
	speed: 0.0170s/iter; left time: 433.8410s
Epoch: 3 cost time: 4.969509601593018
Epoch: 3, Steps: 263 | Train Loss: 0.5327137 Vali Loss: 0.2913599 Test Loss: 0.4149025
Validation loss decreased (0.298861 --> 0.291360).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4262257
	speed: 0.0788s/iter; left time: 2002.8312s
	iters: 200, epoch: 4 | loss: 0.6719224
	speed: 0.0168s/iter; left time: 426.4907s
Epoch: 4 cost time: 5.5792272090911865
Epoch: 4, Steps: 263 | Train Loss: 0.5277765 Vali Loss: 0.2896816 Test Loss: 0.4126999
Validation loss decreased (0.291360 --> 0.289682).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6142151
	speed: 0.0907s/iter; left time: 2279.9218s
	iters: 200, epoch: 5 | loss: 0.4240442
	speed: 0.0229s/iter; left time: 573.1388s
Epoch: 5 cost time: 5.532733917236328
Epoch: 5, Steps: 263 | Train Loss: 0.5260984 Vali Loss: 0.2890717 Test Loss: 0.4118419
Validation loss decreased (0.289682 --> 0.289072).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5058695
	speed: 0.0851s/iter; left time: 2117.0335s
	iters: 200, epoch: 6 | loss: 0.3823863
	speed: 0.0159s/iter; left time: 394.5601s
Epoch: 6 cost time: 5.04093337059021
Epoch: 6, Steps: 263 | Train Loss: 0.5256311 Vali Loss: 0.2891242 Test Loss: 0.4115233
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6362350
	speed: 0.0788s/iter; left time: 1940.1810s
	iters: 200, epoch: 7 | loss: 0.3791346
	speed: 0.0154s/iter; left time: 377.7740s
Epoch: 7 cost time: 5.022814035415649
Epoch: 7, Steps: 263 | Train Loss: 0.5249801 Vali Loss: 0.2891395 Test Loss: 0.4113480
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4422439
	speed: 0.0789s/iter; left time: 1923.1102s
	iters: 200, epoch: 8 | loss: 0.7652421
	speed: 0.0155s/iter; left time: 377.1468s
Epoch: 8 cost time: 4.8361828327178955
Epoch: 8, Steps: 263 | Train Loss: 0.5244606 Vali Loss: 0.2889692 Test Loss: 0.4111542
Validation loss decreased (0.289072 --> 0.288969).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4396389
	speed: 0.0751s/iter; left time: 1810.4739s
	iters: 200, epoch: 9 | loss: 0.3078373
	speed: 0.0151s/iter; left time: 361.4507s
Epoch: 9 cost time: 4.488987445831299
Epoch: 9, Steps: 263 | Train Loss: 0.5241020 Vali Loss: 0.2889677 Test Loss: 0.4110804
Validation loss decreased (0.288969 --> 0.288968).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4871777
	speed: 0.0739s/iter; left time: 1762.1997s
	iters: 200, epoch: 10 | loss: 0.6365455
	speed: 0.0157s/iter; left time: 373.3545s
Epoch: 10 cost time: 4.749890565872192
Epoch: 10, Steps: 263 | Train Loss: 0.5239579 Vali Loss: 0.2891031 Test Loss: 0.4110915
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4628334
	speed: 0.0793s/iter; left time: 1869.2508s
	iters: 200, epoch: 11 | loss: 0.4734654
	speed: 0.0194s/iter; left time: 454.6586s
Epoch: 11 cost time: 5.341450214385986
Epoch: 11, Steps: 263 | Train Loss: 0.5242047 Vali Loss: 0.2890292 Test Loss: 0.4111813
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5924740
	speed: 0.0804s/iter; left time: 1873.8222s
	iters: 200, epoch: 12 | loss: 0.5103105
	speed: 0.0305s/iter; left time: 706.7082s
Epoch: 12 cost time: 6.194484233856201
Epoch: 12, Steps: 263 | Train Loss: 0.5234313 Vali Loss: 0.2893358 Test Loss: 0.4111342
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5582476
	speed: 0.0842s/iter; left time: 1940.4850s
	iters: 200, epoch: 13 | loss: 0.4307414
	speed: 0.0156s/iter; left time: 357.1336s
Epoch: 13 cost time: 4.707035779953003
Epoch: 13, Steps: 263 | Train Loss: 0.5234918 Vali Loss: 0.2892781 Test Loss: 0.4111474
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5610678
	speed: 0.0805s/iter; left time: 1835.0158s
	iters: 200, epoch: 14 | loss: 0.5824625
	speed: 0.0166s/iter; left time: 377.5939s
Epoch: 14 cost time: 4.873526573181152
Epoch: 14, Steps: 263 | Train Loss: 0.5234234 Vali Loss: 0.2891243 Test Loss: 0.4111489
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.7752599
	speed: 0.0888s/iter; left time: 2000.7659s
	iters: 200, epoch: 15 | loss: 0.5433298
	speed: 0.0153s/iter; left time: 343.2570s
Epoch: 15 cost time: 4.6805126667022705
Epoch: 15, Steps: 263 | Train Loss: 0.5231553 Vali Loss: 0.2894439 Test Loss: 0.4112452
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5770742
	speed: 0.0820s/iter; left time: 1825.0167s
	iters: 200, epoch: 16 | loss: 0.4774829
	speed: 0.0256s/iter; left time: 568.1451s
Epoch: 16 cost time: 5.884151935577393
Epoch: 16, Steps: 263 | Train Loss: 0.5233411 Vali Loss: 0.2891110 Test Loss: 0.4112183
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4651078
	speed: 0.0834s/iter; left time: 1835.3044s
	iters: 200, epoch: 17 | loss: 0.5199212
	speed: 0.0150s/iter; left time: 327.9409s
Epoch: 17 cost time: 4.456397771835327
Epoch: 17, Steps: 263 | Train Loss: 0.5233368 Vali Loss: 0.2895822 Test Loss: 0.4112717
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3983644
	speed: 0.0765s/iter; left time: 1661.8298s
	iters: 200, epoch: 18 | loss: 0.3931780
	speed: 0.0163s/iter; left time: 352.5807s
Epoch: 18 cost time: 4.927612543106079
Epoch: 18, Steps: 263 | Train Loss: 0.5229588 Vali Loss: 0.2897078 Test Loss: 0.4112203
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5086796
	speed: 0.1048s/iter; left time: 2250.6235s
	iters: 200, epoch: 19 | loss: 0.5668174
	speed: 0.0230s/iter; left time: 490.6456s
Epoch: 19 cost time: 8.303386926651001
Epoch: 19, Steps: 263 | Train Loss: 0.5236191 Vali Loss: 0.2896389 Test Loss: 0.4112926
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5339078
	speed: 0.0800s/iter; left time: 1695.3795s
	iters: 200, epoch: 20 | loss: 0.4705287
	speed: 0.0166s/iter; left time: 349.2858s
Epoch: 20 cost time: 4.809763193130493
Epoch: 20, Steps: 263 | Train Loss: 0.5232038 Vali Loss: 0.2896846 Test Loss: 0.4113392
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4886201
	speed: 0.1007s/iter; left time: 2109.1849s
	iters: 200, epoch: 21 | loss: 0.4613877
	speed: 0.0163s/iter; left time: 339.0525s
Epoch: 21 cost time: 4.857777833938599
Epoch: 21, Steps: 263 | Train Loss: 0.5229277 Vali Loss: 0.2894563 Test Loss: 0.4113352
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3523613
	speed: 0.0775s/iter; left time: 1603.3180s
	iters: 200, epoch: 22 | loss: 0.3408720
	speed: 0.0161s/iter; left time: 331.4162s
Epoch: 22 cost time: 4.806546211242676
Epoch: 22, Steps: 263 | Train Loss: 0.5229275 Vali Loss: 0.2896718 Test Loss: 0.4112955
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4979114
	speed: 0.0736s/iter; left time: 1503.1083s
	iters: 200, epoch: 23 | loss: 0.4095380
	speed: 0.0159s/iter; left time: 322.7359s
Epoch: 23 cost time: 5.4648048877716064
Epoch: 23, Steps: 263 | Train Loss: 0.5223927 Vali Loss: 0.2896144 Test Loss: 0.4112835
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5259820
	speed: 0.0850s/iter; left time: 1712.4842s
	iters: 200, epoch: 24 | loss: 0.6620712
	speed: 0.0164s/iter; left time: 329.1556s
Epoch: 24 cost time: 4.8361217975616455
Epoch: 24, Steps: 263 | Train Loss: 0.5227067 Vali Loss: 0.2896228 Test Loss: 0.4113384
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3912350
	speed: 0.0794s/iter; left time: 1579.5421s
	iters: 200, epoch: 25 | loss: 0.6223429
	speed: 0.0180s/iter; left time: 356.3985s
Epoch: 25 cost time: 4.948766231536865
Epoch: 25, Steps: 263 | Train Loss: 0.5228025 Vali Loss: 0.2895170 Test Loss: 0.4113631
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.5125366
	speed: 0.0959s/iter; left time: 1881.7543s
	iters: 200, epoch: 26 | loss: 0.6045457
	speed: 0.0161s/iter; left time: 315.2270s
Epoch: 26 cost time: 5.968386173248291
Epoch: 26, Steps: 263 | Train Loss: 0.5224807 Vali Loss: 0.2894046 Test Loss: 0.4113821
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4062815
	speed: 0.0777s/iter; left time: 1505.1009s
	iters: 200, epoch: 27 | loss: 0.5800372
	speed: 0.0222s/iter; left time: 427.0637s
Epoch: 27 cost time: 5.463557004928589
Epoch: 27, Steps: 263 | Train Loss: 0.5231137 Vali Loss: 0.2895595 Test Loss: 0.4113629
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4538010
	speed: 0.0795s/iter; left time: 1518.1376s
	iters: 200, epoch: 28 | loss: 0.4497744
	speed: 0.0155s/iter; left time: 293.6759s
Epoch: 28 cost time: 5.151584625244141
Epoch: 28, Steps: 263 | Train Loss: 0.5229338 Vali Loss: 0.2898737 Test Loss: 0.4113736
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.5370535
	speed: 0.0773s/iter; left time: 1455.6084s
	iters: 200, epoch: 29 | loss: 0.5504748
	speed: 0.0285s/iter; left time: 534.8422s
Epoch: 29 cost time: 5.982367992401123
Epoch: 29, Steps: 263 | Train Loss: 0.5229154 Vali Loss: 0.2896328 Test Loss: 0.4113782
EarlyStopping counter: 20 out of 20
Early stopping
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=22, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3902976.0
params:  4554.0
Trainable parameters:  4554
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6421250
	speed: 0.0216s/iter; left time: 565.8451s
	iters: 200, epoch: 1 | loss: 0.5695297
	speed: 0.0155s/iter; left time: 403.6216s
Epoch: 1 cost time: 4.674465656280518
Epoch: 1, Steps: 263 | Train Loss: 0.5886280 Vali Loss: 0.2890346 Test Loss: 0.4108806
Validation loss decreased (inf --> 0.289035).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6459471
	speed: 0.0780s/iter; left time: 2022.3294s
	iters: 200, epoch: 2 | loss: 0.4674039
	speed: 0.0177s/iter; left time: 456.3782s
Epoch: 2 cost time: 4.9230265617370605
Epoch: 2, Steps: 263 | Train Loss: 0.5875237 Vali Loss: 0.2888743 Test Loss: 0.4109928
Validation loss decreased (0.289035 --> 0.288874).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6157031
	speed: 0.0821s/iter; left time: 2109.1826s
	iters: 200, epoch: 3 | loss: 0.5101552
	speed: 0.0164s/iter; left time: 419.7888s
Epoch: 3 cost time: 4.915571689605713
Epoch: 3, Steps: 263 | Train Loss: 0.5878276 Vali Loss: 0.2892779 Test Loss: 0.4111142
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.8005605
	speed: 0.0819s/iter; left time: 2081.6830s
	iters: 200, epoch: 4 | loss: 0.5519830
	speed: 0.0167s/iter; left time: 421.6921s
Epoch: 4 cost time: 5.135842561721802
Epoch: 4, Steps: 263 | Train Loss: 0.5868632 Vali Loss: 0.2892093 Test Loss: 0.4110726
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4646212
	speed: 0.0760s/iter; left time: 1912.0293s
	iters: 200, epoch: 5 | loss: 0.5945677
	speed: 0.0165s/iter; left time: 414.5082s
Epoch: 5 cost time: 4.906519651412964
Epoch: 5, Steps: 263 | Train Loss: 0.5866889 Vali Loss: 0.2891135 Test Loss: 0.4110200
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6618296
	speed: 0.0746s/iter; left time: 1857.2132s
	iters: 200, epoch: 6 | loss: 0.7957807
	speed: 0.0148s/iter; left time: 367.1749s
Epoch: 6 cost time: 4.563338041305542
Epoch: 6, Steps: 263 | Train Loss: 0.5872441 Vali Loss: 0.2895846 Test Loss: 0.4111825
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4123835
	speed: 0.0722s/iter; left time: 1778.3370s
	iters: 200, epoch: 7 | loss: 0.5959555
	speed: 0.0148s/iter; left time: 362.2690s
Epoch: 7 cost time: 4.45969033241272
Epoch: 7, Steps: 263 | Train Loss: 0.5873100 Vali Loss: 0.2895027 Test Loss: 0.4111755
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6166638
	speed: 0.0811s/iter; left time: 1976.4842s
	iters: 200, epoch: 8 | loss: 0.7620959
	speed: 0.0167s/iter; left time: 404.1769s
Epoch: 8 cost time: 4.947891473770142
Epoch: 8, Steps: 263 | Train Loss: 0.5871619 Vali Loss: 0.2895346 Test Loss: 0.4111534
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5636862
	speed: 0.0715s/iter; left time: 1724.0846s
	iters: 200, epoch: 9 | loss: 0.7634708
	speed: 0.0154s/iter; left time: 368.8973s
Epoch: 9 cost time: 4.492447376251221
Epoch: 9, Steps: 263 | Train Loss: 0.5866369 Vali Loss: 0.2896236 Test Loss: 0.4111732
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5725634
	speed: 0.0779s/iter; left time: 1857.7276s
	iters: 200, epoch: 10 | loss: 0.5466225
	speed: 0.0156s/iter; left time: 371.3542s
Epoch: 10 cost time: 4.706232309341431
Epoch: 10, Steps: 263 | Train Loss: 0.5871921 Vali Loss: 0.2899396 Test Loss: 0.4112942
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5350782
	speed: 0.0764s/iter; left time: 1799.7513s
	iters: 200, epoch: 11 | loss: 0.4182478
	speed: 0.0173s/iter; left time: 406.2729s
Epoch: 11 cost time: 5.737824440002441
Epoch: 11, Steps: 263 | Train Loss: 0.5867910 Vali Loss: 0.2897419 Test Loss: 0.4112346
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5500268
	speed: 0.0887s/iter; left time: 2066.6414s
	iters: 200, epoch: 12 | loss: 0.4254321
	speed: 0.0153s/iter; left time: 355.0922s
Epoch: 12 cost time: 5.4892897605896
Epoch: 12, Steps: 263 | Train Loss: 0.5863241 Vali Loss: 0.2890089 Test Loss: 0.4112046
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6019410
	speed: 0.0828s/iter; left time: 1909.2249s
	iters: 200, epoch: 13 | loss: 0.4739865
	speed: 0.0148s/iter; left time: 340.6913s
Epoch: 13 cost time: 4.490678548812866
Epoch: 13, Steps: 263 | Train Loss: 0.5869867 Vali Loss: 0.2896801 Test Loss: 0.4111727
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3764492
	speed: 0.0745s/iter; left time: 1697.6561s
	iters: 200, epoch: 14 | loss: 0.6137089
	speed: 0.0164s/iter; left time: 372.9132s
Epoch: 14 cost time: 4.80309796333313
Epoch: 14, Steps: 263 | Train Loss: 0.5865886 Vali Loss: 0.2893948 Test Loss: 0.4112127
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5120634
	speed: 0.0884s/iter; left time: 1990.6613s
	iters: 200, epoch: 15 | loss: 0.9151313
	speed: 0.0162s/iter; left time: 364.1111s
Epoch: 15 cost time: 5.3184428215026855
Epoch: 15, Steps: 263 | Train Loss: 0.5870479 Vali Loss: 0.2896274 Test Loss: 0.4112405
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5422369
	speed: 0.0778s/iter; left time: 1731.8185s
	iters: 200, epoch: 16 | loss: 0.5640268
	speed: 0.0144s/iter; left time: 319.1604s
Epoch: 16 cost time: 4.470759630203247
Epoch: 16, Steps: 263 | Train Loss: 0.5865900 Vali Loss: 0.2898892 Test Loss: 0.4112833
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 1.0094458
	speed: 0.0762s/iter; left time: 1676.7594s
	iters: 200, epoch: 17 | loss: 0.4555503
	speed: 0.0175s/iter; left time: 382.5401s
Epoch: 17 cost time: 5.028656244277954
Epoch: 17, Steps: 263 | Train Loss: 0.5868604 Vali Loss: 0.2896944 Test Loss: 0.4112409
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3403205
	speed: 0.0754s/iter; left time: 1639.3565s
	iters: 200, epoch: 18 | loss: 0.5837999
	speed: 0.0166s/iter; left time: 358.7390s
Epoch: 18 cost time: 4.873532295227051
Epoch: 18, Steps: 263 | Train Loss: 0.5863758 Vali Loss: 0.2899639 Test Loss: 0.4112339
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4856900
	speed: 0.0792s/iter; left time: 1699.6012s
	iters: 200, epoch: 19 | loss: 0.6244404
	speed: 0.0162s/iter; left time: 345.1523s
Epoch: 19 cost time: 4.822151184082031
Epoch: 19, Steps: 263 | Train Loss: 0.5862296 Vali Loss: 0.2899047 Test Loss: 0.4112389
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5587782
	speed: 0.0795s/iter; left time: 1685.8417s
	iters: 200, epoch: 20 | loss: 0.6632764
	speed: 0.0168s/iter; left time: 353.5944s
Epoch: 20 cost time: 4.867504596710205
Epoch: 20, Steps: 263 | Train Loss: 0.5868150 Vali Loss: 0.2897375 Test Loss: 0.4112522
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4259242
	speed: 0.0783s/iter; left time: 1640.5359s
	iters: 200, epoch: 21 | loss: 0.7483234
	speed: 0.0151s/iter; left time: 314.4993s
Epoch: 21 cost time: 4.594212770462036
Epoch: 21, Steps: 263 | Train Loss: 0.5863968 Vali Loss: 0.2898022 Test Loss: 0.4112335
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.7295802
	speed: 0.0806s/iter; left time: 1666.7622s
	iters: 200, epoch: 22 | loss: 0.6117185
	speed: 0.0154s/iter; left time: 317.6881s
Epoch: 22 cost time: 4.693893671035767
Epoch: 22, Steps: 263 | Train Loss: 0.5869758 Vali Loss: 0.2898579 Test Loss: 0.4112925
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.40863877534866333, mae:0.39852216839790344, rse:0.5138237476348877, corr:[0.5493125  0.5476875  0.54072344 0.5389474  0.53855693 0.53596973
 0.5340273  0.5343549  0.5341251  0.53203815 0.5302609  0.5301121
 0.52994865 0.5286838  0.5273423  0.52665687 0.52559346 0.5237902
 0.5224469  0.5219818  0.521181   0.51949006 0.51800674 0.51757073
 0.51731443 0.5165233  0.51593864 0.5159874  0.5158544  0.5150559
 0.5141398  0.5135322  0.5127274  0.5115031  0.51064694 0.51070285
 0.5106475  0.50973594 0.5084668  0.5076678  0.5072145  0.5065881
 0.5058912  0.505697   0.5057474  0.50522804 0.5042287  0.5033466
 0.50277644 0.50196004 0.5007189  0.49962944 0.49901924 0.49856833
 0.4979834  0.49727795 0.4966372  0.49612144 0.4956956  0.4955589
 0.49570292 0.49578226 0.49563605 0.49551257 0.49557808 0.49568257
 0.49555278 0.49528563 0.49519506 0.4953131  0.495474   0.4955793
 0.49566874 0.49579144 0.4957611  0.4955964  0.49544367 0.49530193
 0.4951644  0.49496165 0.494684   0.49442786 0.49427935 0.49411288
 0.49385035 0.4935314  0.49324402 0.493057   0.49284855 0.49253532
 0.49215332 0.49171013 0.4911219  0.4901523  0.48858607 0.48652935
 0.48428202 0.4820682  0.48000637 0.47819316 0.47660404 0.47519812
 0.47392604 0.4727499  0.47151464 0.46995807 0.46824762 0.46682003
 0.46571317 0.46454853 0.46307945 0.46149588 0.4601693  0.45897517
 0.4575839  0.4560265  0.4545772  0.45333016 0.45213243 0.45078912
 0.44962713 0.4487193  0.44774607 0.44666594 0.44571564 0.44488096
 0.4439095  0.4426352  0.4412198  0.44006157 0.43916544 0.43824503
 0.43731225 0.43654668 0.4357547  0.43484995 0.43385157 0.4333859
 0.4334377  0.43327567 0.43249792 0.43164745 0.43136537 0.43122745
 0.43041423 0.42900512 0.42771196 0.42706463 0.4265295  0.42564005
 0.42457855 0.42394808 0.4235483  0.42294523 0.42235187 0.4218333
 0.4212226  0.42073703 0.42075264 0.42142597 0.42184547 0.4213626
 0.4206535  0.42052662 0.4211848  0.4214137  0.42083758 0.4203881
 0.42096007 0.42205456 0.42242634 0.42184988 0.42114988 0.42099178
 0.42134863 0.42179796 0.42199987 0.42189333 0.4216027  0.4214686
 0.42162192 0.4217072  0.42146647 0.42100897 0.42088988 0.42112404
 0.42115825 0.42060402 0.419676   0.4186614  0.41727686 0.415041
 0.4123407  0.41021213 0.4087136  0.4071549  0.40522376 0.403427
 0.40232885 0.4016001  0.40054905 0.3989664  0.3972927  0.39586845
 0.3945938  0.39327145 0.39176115 0.3901419  0.38852513 0.38698784
 0.38545543 0.38403106 0.38270664 0.38137737 0.37994403 0.37850094
 0.37730947 0.3763875  0.3755587  0.37436157 0.37279812 0.37128368
 0.37007788 0.36891958 0.36788267 0.36676845 0.36578846 0.36488208
 0.3639179  0.3628228  0.36160246 0.3605181  0.35961333 0.35890818
 0.35829285 0.35779852 0.3575057  0.3573248  0.3572565  0.3570759
 0.35667056 0.35614666 0.3555977  0.35549843 0.35550198 0.3551588
 0.35464218 0.35433304 0.35445085 0.35460597 0.35459894 0.3545147
 0.3546331  0.3552291  0.35609397 0.35665077 0.35672966 0.35654643
 0.35628366 0.35603347 0.35581294 0.35604775 0.35681984 0.35765922
 0.35816967 0.35836855 0.35877064 0.35922927 0.35940096 0.35920182
 0.35914668 0.35967684 0.36037028 0.3606696  0.3606313  0.3607664
 0.3613063  0.3618389  0.36200735 0.36201367 0.362125   0.36248088
 0.36287636 0.3630485  0.36281025 0.36218703 0.36138815 0.36052215
 0.35944113 0.35803363 0.3566117  0.35581875 0.3557575  0.35602298
 0.3561546  0.35605922 0.35600287 0.35593358 0.35553953 0.35485354
 0.35421938 0.35366905 0.3528663  0.3516623  0.35044378 0.34960705
 0.34900287 0.34816486 0.34706083 0.34604692 0.34523943 0.34447384
 0.34359583 0.34296215 0.3426673  0.34226498 0.3414257  0.34041557
 0.3396149  0.3390134  0.33833468 0.33758235 0.3370081  0.33658683
 0.33595765 0.33513522 0.3346093  0.33448192 0.33439797 0.33387005
 0.33319908 0.33301556 0.33331946 0.3333537  0.33290157 0.33266306
 0.33305407 0.3335732  0.33349377 0.3328611  0.3323498  0.33238697
 0.33269158 0.33282006 0.33266714 0.33264247 0.33306482 0.33376244
 0.33433184 0.33446312 0.3343656  0.33435702 0.33458212 0.33486712
 0.3350368  0.33514243 0.33523628 0.33556786 0.33604062 0.33641028
 0.33662468 0.3370029  0.3376518  0.33823675 0.33833358 0.33813685
 0.33825138 0.33898205 0.33988822 0.340327   0.340341   0.34049585
 0.34110636 0.3418408  0.34226504 0.34243938 0.34270403 0.3431981
 0.34365237 0.34371838 0.34329975 0.3425068  0.3414739  0.3402498
 0.33886978 0.33747932 0.33619976 0.33503702 0.33405268 0.3334311
 0.33307064 0.33268932 0.332122   0.33130822 0.33034813 0.3293934
 0.3285795  0.3279214  0.32722324 0.32621852 0.32476962 0.32311633
 0.32169887 0.32057852 0.31940445 0.3179336  0.3162687  0.31496486
 0.31417388 0.31351784 0.31268197 0.31176296 0.3109739  0.31017512
 0.3092047  0.30815    0.3073083  0.3067305  0.3060121  0.3051983
 0.30470183 0.30456173 0.30431473 0.30356005 0.30257943 0.3019821
 0.30177805 0.30153298 0.30099517 0.3004869  0.30008993 0.29956838
 0.29871503 0.2979068  0.29768935 0.29784834 0.29736897 0.2961867
 0.2952002  0.29526418 0.29583424 0.29587168 0.2952391  0.29473358
 0.29471987 0.2947646  0.29444265 0.29397294 0.29398736 0.29446784
 0.29478154 0.29450876 0.29410648 0.29430765 0.29512203 0.29589748
 0.29610062 0.2960391  0.29643175 0.2973769  0.29816732 0.29807577
 0.29747754 0.29734066 0.29797482 0.2987177  0.29897404 0.29876024
 0.29831123 0.29756984 0.29662064 0.29599527 0.29616803 0.29677013
 0.29700086 0.29644257 0.29525864 0.29371578 0.2919746  0.2901001
 0.2882587  0.28648096 0.28464338 0.28286394 0.28155884 0.28099364
 0.2808649  0.28070927 0.28029978 0.2797348  0.27918488 0.27873585
 0.27824897 0.2775218  0.2763736  0.27489194 0.27336404 0.2720412
 0.27095935 0.2697775  0.2682393  0.26651952 0.26502445 0.26396424
 0.2631105  0.26220337 0.26117736 0.26013714 0.25930262 0.25861496
 0.2578081  0.25670996 0.2555264  0.25463614 0.254055   0.25361708
 0.25314587 0.25251576 0.251922   0.25148395 0.25097242 0.2502542
 0.24957426 0.24938884 0.2495012  0.24928562 0.24848665 0.2475388
 0.24707149 0.2468123  0.24617116 0.24508648 0.24423742 0.24421075
 0.24471551 0.2449942  0.24479191 0.24444655 0.24443844 0.24468814
 0.24476568 0.24458499 0.24464944 0.24538891 0.24634297 0.24681531
 0.24658926 0.24611162 0.24582998 0.24577743 0.24571154 0.24597791
 0.24664794 0.24737589 0.2476266  0.24742481 0.24721783 0.24741851
 0.2477695  0.24811257 0.248631   0.24941884 0.24996114 0.2497334
 0.24898727 0.2486264  0.24900861 0.24950488 0.24972665 0.24997972
 0.25055018 0.2509033  0.25036743 0.24894258 0.24704403 0.24483477
 0.24228631 0.23982778 0.2381361  0.23742434 0.23736058 0.23773302
 0.23848435 0.23928867 0.23954843 0.23911546 0.23850825 0.23809774
 0.23765607 0.23680204 0.23566957 0.2346118  0.23359244 0.23233674
 0.23094028 0.22971386 0.2285991  0.22719575 0.22558315 0.22452624
 0.2244231  0.22461069 0.22403894 0.2226299  0.2213302  0.22049375
 0.21971479 0.21877666 0.21791965 0.21728653 0.21667694 0.2158561
 0.2150174  0.21421298 0.21331501 0.21219881 0.21139583 0.21113856
 0.2110653  0.21069326 0.20999806 0.20952837 0.20960018 0.2097014
 0.20947862 0.20894958 0.20844167 0.2081876  0.2081876  0.2079825
 0.20749539 0.20694122 0.2067047  0.20704585 0.20762561 0.20765336
 0.20720002 0.2072054  0.2081509  0.2091971  0.20931949 0.20855525
 0.20799538 0.20818408 0.20868944 0.2088663  0.20901892 0.20934743
 0.20960952 0.20962167 0.20973037 0.21021658 0.21064197 0.21058157
 0.21058139 0.21141557 0.21278545 0.21375127 0.21401075 0.214245
 0.21488607 0.21541896 0.21547394 0.21563281 0.216581   0.21789318
 0.21865183 0.2186609  0.21823989 0.21720044 0.21500012 0.21195878
 0.20931716 0.20769306 0.20651719 0.2053915  0.20482272 0.20508666
 0.20559244 0.20581873 0.2058705  0.20608194 0.2061772  0.20553556
 0.20426044 0.2030575  0.2021934  0.20132834 0.20026803 0.19912937
 0.19798021 0.1966446  0.19523245 0.19417724 0.19328965 0.19186084
 0.18991835 0.18847223 0.18792495 0.18727827 0.18564859 0.18352245
 0.1821378  0.18136679 0.18011689 0.17834625 0.17685205 0.17605142
 0.17503805 0.1735316  0.17207275 0.1709487  0.16914593 0.16727665
 0.16705672 0.16784205 0.16685636 0.1651741  0.16816074 0.17419311]
