Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=42, out_features=84, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3161088.0
params:  3612.0
Trainable parameters:  3612
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4602085
	speed: 0.0864s/iter; left time: 2220.1157s
	iters: 200, epoch: 1 | loss: 0.4733196
	speed: 0.0776s/iter; left time: 1985.3934s
Epoch: 1 cost time: 20.95537257194519
Epoch: 1, Steps: 258 | Train Loss: 0.5898828 Vali Loss: 0.2829731 Test Loss: 0.3783240
Validation loss decreased (inf --> 0.282973).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5014673
	speed: 0.3275s/iter; left time: 8333.7947s
	iters: 200, epoch: 2 | loss: 0.5090605
	speed: 0.0763s/iter; left time: 1933.6398s
Epoch: 2 cost time: 20.47337818145752
Epoch: 2, Steps: 258 | Train Loss: 0.5246473 Vali Loss: 0.2729151 Test Loss: 0.3671483
Validation loss decreased (0.282973 --> 0.272915).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6010435
	speed: 0.3454s/iter; left time: 8697.8560s
	iters: 200, epoch: 3 | loss: 0.4474578
	speed: 0.0793s/iter; left time: 1990.2281s
Epoch: 3 cost time: 21.343895435333252
Epoch: 3, Steps: 258 | Train Loss: 0.5148741 Vali Loss: 0.2694348 Test Loss: 0.3629517
Validation loss decreased (0.272915 --> 0.269435).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4901797
	speed: 0.3302s/iter; left time: 8230.1032s
	iters: 200, epoch: 4 | loss: 0.4923570
	speed: 0.0773s/iter; left time: 1920.3391s
Epoch: 4 cost time: 20.853721857070923
Epoch: 4, Steps: 258 | Train Loss: 0.5098634 Vali Loss: 0.2675942 Test Loss: 0.3603653
Validation loss decreased (0.269435 --> 0.267594).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5221583
	speed: 0.3332s/iter; left time: 8219.2420s
	iters: 200, epoch: 5 | loss: 0.4602869
	speed: 0.0808s/iter; left time: 1984.0645s
Epoch: 5 cost time: 20.72045636177063
Epoch: 5, Steps: 258 | Train Loss: 0.5069222 Vali Loss: 0.2660815 Test Loss: 0.3587447
Validation loss decreased (0.267594 --> 0.266082).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5500313
	speed: 0.3311s/iter; left time: 8081.2655s
	iters: 200, epoch: 6 | loss: 0.4678247
	speed: 0.0783s/iter; left time: 1902.4075s
Epoch: 6 cost time: 20.875844717025757
Epoch: 6, Steps: 258 | Train Loss: 0.5049936 Vali Loss: 0.2656704 Test Loss: 0.3577928
Validation loss decreased (0.266082 --> 0.265670).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.8479820
	speed: 0.3482s/iter; left time: 8410.6311s
	iters: 200, epoch: 7 | loss: 0.4041609
	speed: 0.0789s/iter; left time: 1898.7351s
Epoch: 7 cost time: 21.593018293380737
Epoch: 7, Steps: 258 | Train Loss: 0.5040354 Vali Loss: 0.2651609 Test Loss: 0.3571881
Validation loss decreased (0.265670 --> 0.265161).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3709380
	speed: 0.3335s/iter; left time: 7968.5855s
	iters: 200, epoch: 8 | loss: 0.4150221
	speed: 0.0780s/iter; left time: 1856.6652s
Epoch: 8 cost time: 20.560362339019775
Epoch: 8, Steps: 258 | Train Loss: 0.5026182 Vali Loss: 0.2651697 Test Loss: 0.3563142
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4234590
	speed: 0.3439s/iter; left time: 8127.9035s
	iters: 200, epoch: 9 | loss: 0.5698938
	speed: 0.0799s/iter; left time: 1879.5388s
Epoch: 9 cost time: 21.246020555496216
Epoch: 9, Steps: 258 | Train Loss: 0.5018202 Vali Loss: 0.2642182 Test Loss: 0.3561316
Validation loss decreased (0.265161 --> 0.264218).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4833384
	speed: 0.3142s/iter; left time: 7344.5208s
	iters: 200, epoch: 10 | loss: 0.4554088
	speed: 0.0761s/iter; left time: 1771.6175s
Epoch: 10 cost time: 20.390742301940918
Epoch: 10, Steps: 258 | Train Loss: 0.5016656 Vali Loss: 0.2641193 Test Loss: 0.3556699
Validation loss decreased (0.264218 --> 0.264119).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4093335
	speed: 0.3396s/iter; left time: 7852.4461s
	iters: 200, epoch: 11 | loss: 0.3884604
	speed: 0.0819s/iter; left time: 1886.0436s
Epoch: 11 cost time: 21.61971426010132
Epoch: 11, Steps: 258 | Train Loss: 0.5014668 Vali Loss: 0.2637277 Test Loss: 0.3556205
Validation loss decreased (0.264119 --> 0.263728).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5286898
	speed: 0.3435s/iter; left time: 7852.6282s
	iters: 200, epoch: 12 | loss: 0.5612014
	speed: 0.0763s/iter; left time: 1735.9571s
Epoch: 12 cost time: 20.6054527759552
Epoch: 12, Steps: 258 | Train Loss: 0.5011283 Vali Loss: 0.2641144 Test Loss: 0.3551988
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6148846
	speed: 0.3172s/iter; left time: 7169.6323s
	iters: 200, epoch: 13 | loss: 0.5533886
	speed: 0.0722s/iter; left time: 1623.9125s
Epoch: 13 cost time: 19.024858713150024
Epoch: 13, Steps: 258 | Train Loss: 0.5003788 Vali Loss: 0.2637807 Test Loss: 0.3552072
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5411181
	speed: 0.3109s/iter; left time: 6947.2391s
	iters: 200, epoch: 14 | loss: 0.5130718
	speed: 0.0757s/iter; left time: 1685.1992s
Epoch: 14 cost time: 19.33358120918274
Epoch: 14, Steps: 258 | Train Loss: 0.5002163 Vali Loss: 0.2636738 Test Loss: 0.3550175
Validation loss decreased (0.263728 --> 0.263674).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4656752
	speed: 0.3145s/iter; left time: 6946.9554s
	iters: 200, epoch: 15 | loss: 0.6952524
	speed: 0.0757s/iter; left time: 1665.5750s
Epoch: 15 cost time: 20.16861915588379
Epoch: 15, Steps: 258 | Train Loss: 0.4998970 Vali Loss: 0.2636590 Test Loss: 0.3549633
Validation loss decreased (0.263674 --> 0.263659).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5530922
	speed: 0.3219s/iter; left time: 7028.1401s
	iters: 200, epoch: 16 | loss: 0.5459868
	speed: 0.0743s/iter; left time: 1615.4558s
Epoch: 16 cost time: 19.382449626922607
Epoch: 16, Steps: 258 | Train Loss: 0.4995619 Vali Loss: 0.2636528 Test Loss: 0.3546887
Validation loss decreased (0.263659 --> 0.263653).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3574296
	speed: 0.3183s/iter; left time: 6866.4840s
	iters: 200, epoch: 17 | loss: 0.4197442
	speed: 0.0709s/iter; left time: 1522.5733s
Epoch: 17 cost time: 19.622610569000244
Epoch: 17, Steps: 258 | Train Loss: 0.5000041 Vali Loss: 0.2634303 Test Loss: 0.3548362
Validation loss decreased (0.263653 --> 0.263430).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3595141
	speed: 0.3111s/iter; left time: 6630.2884s
	iters: 200, epoch: 18 | loss: 0.6390310
	speed: 0.0745s/iter; left time: 1581.1535s
Epoch: 18 cost time: 19.703622579574585
Epoch: 18, Steps: 258 | Train Loss: 0.4994033 Vali Loss: 0.2634141 Test Loss: 0.3546797
Validation loss decreased (0.263430 --> 0.263414).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4307330
	speed: 0.3211s/iter; left time: 6761.2344s
	iters: 200, epoch: 19 | loss: 0.5424648
	speed: 0.0741s/iter; left time: 1553.4239s
Epoch: 19 cost time: 19.786216497421265
Epoch: 19, Steps: 258 | Train Loss: 0.4994099 Vali Loss: 0.2633709 Test Loss: 0.3545964
Validation loss decreased (0.263414 --> 0.263371).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5156396
	speed: 0.3234s/iter; left time: 6726.9849s
	iters: 200, epoch: 20 | loss: 0.4747104
	speed: 0.0734s/iter; left time: 1520.1487s
Epoch: 20 cost time: 19.75078320503235
Epoch: 20, Steps: 258 | Train Loss: 0.4991568 Vali Loss: 0.2635112 Test Loss: 0.3545942
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5555820
	speed: 0.3190s/iter; left time: 6553.4982s
	iters: 200, epoch: 21 | loss: 0.3814228
	speed: 0.0761s/iter; left time: 1555.7115s
Epoch: 21 cost time: 20.113199949264526
Epoch: 21, Steps: 258 | Train Loss: 0.4994926 Vali Loss: 0.2631924 Test Loss: 0.3545556
Validation loss decreased (0.263371 --> 0.263192).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5782801
	speed: 0.3189s/iter; left time: 6467.3138s
	iters: 200, epoch: 22 | loss: 0.4202998
	speed: 0.0772s/iter; left time: 1557.9179s
Epoch: 22 cost time: 20.941210746765137
Epoch: 22, Steps: 258 | Train Loss: 0.4990015 Vali Loss: 0.2632049 Test Loss: 0.3544450
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5044225
	speed: 0.3433s/iter; left time: 6875.0064s
	iters: 200, epoch: 23 | loss: 0.5318047
	speed: 0.0805s/iter; left time: 1604.7745s
Epoch: 23 cost time: 21.37242889404297
Epoch: 23, Steps: 258 | Train Loss: 0.4994952 Vali Loss: 0.2632121 Test Loss: 0.3543707
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5709145
	speed: 0.3378s/iter; left time: 6676.5349s
	iters: 200, epoch: 24 | loss: 0.6641617
	speed: 0.0783s/iter; left time: 1539.9592s
Epoch: 24 cost time: 20.92947816848755
Epoch: 24, Steps: 258 | Train Loss: 0.4993973 Vali Loss: 0.2631971 Test Loss: 0.3543556
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5507820
	speed: 0.3328s/iter; left time: 6492.1988s
	iters: 200, epoch: 25 | loss: 0.3832445
	speed: 0.0783s/iter; left time: 1520.1566s
Epoch: 25 cost time: 21.382355451583862
Epoch: 25, Steps: 258 | Train Loss: 0.4987656 Vali Loss: 0.2629537 Test Loss: 0.3544114
Validation loss decreased (0.263192 --> 0.262954).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3859591
	speed: 0.3369s/iter; left time: 6486.4605s
	iters: 200, epoch: 26 | loss: 0.4533745
	speed: 0.0802s/iter; left time: 1535.5271s
Epoch: 26 cost time: 21.060394287109375
Epoch: 26, Steps: 258 | Train Loss: 0.4986278 Vali Loss: 0.2631254 Test Loss: 0.3544822
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4843583
	speed: 0.3433s/iter; left time: 6520.5061s
	iters: 200, epoch: 27 | loss: 0.4883390
	speed: 0.0792s/iter; left time: 1496.4625s
Epoch: 27 cost time: 21.57121729850769
Epoch: 27, Steps: 258 | Train Loss: 0.4991458 Vali Loss: 0.2631273 Test Loss: 0.3544139
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4799533
	speed: 0.3412s/iter; left time: 6391.8593s
	iters: 200, epoch: 28 | loss: 0.6345103
	speed: 0.0773s/iter; left time: 1441.0790s
Epoch: 28 cost time: 20.70615005493164
Epoch: 28, Steps: 258 | Train Loss: 0.4987598 Vali Loss: 0.2631848 Test Loss: 0.3543812
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.5622708
	speed: 0.3196s/iter; left time: 5904.4110s
	iters: 200, epoch: 29 | loss: 0.4317831
	speed: 0.0779s/iter; left time: 1430.8309s
Epoch: 29 cost time: 20.382634162902832
Epoch: 29, Steps: 258 | Train Loss: 0.4987684 Vali Loss: 0.2632991 Test Loss: 0.3543447
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4347771
	speed: 0.3230s/iter; left time: 5884.2885s
	iters: 200, epoch: 30 | loss: 0.4021660
	speed: 0.0729s/iter; left time: 1321.6979s
Epoch: 30 cost time: 19.408937692642212
Epoch: 30, Steps: 258 | Train Loss: 0.4989255 Vali Loss: 0.2632177 Test Loss: 0.3543240
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3826686
	speed: 0.3145s/iter; left time: 5648.9137s
	iters: 200, epoch: 31 | loss: 0.4507277
	speed: 0.0774s/iter; left time: 1381.9024s
Epoch: 31 cost time: 19.871378660202026
Epoch: 31, Steps: 258 | Train Loss: 0.4987459 Vali Loss: 0.2630705 Test Loss: 0.3542482
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3926317
	speed: 0.3198s/iter; left time: 5661.4660s
	iters: 200, epoch: 32 | loss: 0.5047093
	speed: 0.0759s/iter; left time: 1336.1011s
Epoch: 32 cost time: 20.091078519821167
Epoch: 32, Steps: 258 | Train Loss: 0.4987282 Vali Loss: 0.2630246 Test Loss: 0.3542718
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.5696679
	speed: 0.3142s/iter; left time: 5481.4086s
	iters: 200, epoch: 33 | loss: 0.5510636
	speed: 0.0721s/iter; left time: 1251.2786s
Epoch: 33 cost time: 19.823540687561035
Epoch: 33, Steps: 258 | Train Loss: 0.4987758 Vali Loss: 0.2630344 Test Loss: 0.3543336
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3794306
	speed: 0.3227s/iter; left time: 5545.8922s
	iters: 200, epoch: 34 | loss: 0.5000720
	speed: 0.0756s/iter; left time: 1291.9170s
Epoch: 34 cost time: 20.280725240707397
Epoch: 34, Steps: 258 | Train Loss: 0.4984140 Vali Loss: 0.2629276 Test Loss: 0.3542544
Validation loss decreased (0.262954 --> 0.262928).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4365821
	speed: 0.3199s/iter; left time: 5416.0444s
	iters: 200, epoch: 35 | loss: 0.4253178
	speed: 0.0720s/iter; left time: 1212.4968s
Epoch: 35 cost time: 19.653657913208008
Epoch: 35, Steps: 258 | Train Loss: 0.4983869 Vali Loss: 0.2631364 Test Loss: 0.3542413
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4760132
	speed: 0.3190s/iter; left time: 5318.0450s
	iters: 200, epoch: 36 | loss: 0.4610192
	speed: 0.0721s/iter; left time: 1195.0612s
Epoch: 36 cost time: 19.973514556884766
Epoch: 36, Steps: 258 | Train Loss: 0.4976828 Vali Loss: 0.2624297 Test Loss: 0.3542061
Validation loss decreased (0.262928 --> 0.262430).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.5200391
	speed: 0.2951s/iter; left time: 4842.8809s
	iters: 200, epoch: 37 | loss: 0.5248770
	speed: 0.0725s/iter; left time: 1183.4006s
Epoch: 37 cost time: 19.55570697784424
Epoch: 37, Steps: 258 | Train Loss: 0.4983258 Vali Loss: 0.2630876 Test Loss: 0.3542249
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4241855
	speed: 0.3203s/iter; left time: 5174.5889s
	iters: 200, epoch: 38 | loss: 0.4635789
	speed: 0.0734s/iter; left time: 1178.2225s
Epoch: 38 cost time: 19.965270280838013
Epoch: 38, Steps: 258 | Train Loss: 0.4988309 Vali Loss: 0.2630569 Test Loss: 0.3542168
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.3564410
	speed: 0.3127s/iter; left time: 4971.2390s
	iters: 200, epoch: 39 | loss: 0.4680606
	speed: 0.0704s/iter; left time: 1112.7256s
Epoch: 39 cost time: 19.312673091888428
Epoch: 39, Steps: 258 | Train Loss: 0.4986089 Vali Loss: 0.2627610 Test Loss: 0.3542269
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.5087464
	speed: 0.3026s/iter; left time: 4731.9433s
	iters: 200, epoch: 40 | loss: 0.5215846
	speed: 0.0691s/iter; left time: 1073.9237s
Epoch: 40 cost time: 18.9974684715271
Epoch: 40, Steps: 258 | Train Loss: 0.4983507 Vali Loss: 0.2627705 Test Loss: 0.3542261
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4365725
	speed: 0.3143s/iter; left time: 4834.9570s
	iters: 200, epoch: 41 | loss: 0.4919794
	speed: 0.0737s/iter; left time: 1126.5958s
Epoch: 41 cost time: 19.75562572479248
Epoch: 41, Steps: 258 | Train Loss: 0.4981901 Vali Loss: 0.2630279 Test Loss: 0.3541947
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4218957
	speed: 0.3219s/iter; left time: 4868.5433s
	iters: 200, epoch: 42 | loss: 0.4694403
	speed: 0.0736s/iter; left time: 1105.6727s
Epoch: 42 cost time: 19.297423601150513
Epoch: 42, Steps: 258 | Train Loss: 0.4987287 Vali Loss: 0.2629074 Test Loss: 0.3541873
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.5914029
	speed: 0.2879s/iter; left time: 4279.6820s
	iters: 200, epoch: 43 | loss: 0.3471118
	speed: 0.0674s/iter; left time: 995.4554s
Epoch: 43 cost time: 17.952829837799072
Epoch: 43, Steps: 258 | Train Loss: 0.4988607 Vali Loss: 0.2630123 Test Loss: 0.3541676
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3929170
	speed: 0.2861s/iter; left time: 4179.3443s
	iters: 200, epoch: 44 | loss: 0.4763707
	speed: 0.0630s/iter; left time: 913.8520s
Epoch: 44 cost time: 18.029839038848877
Epoch: 44, Steps: 258 | Train Loss: 0.4981691 Vali Loss: 0.2628738 Test Loss: 0.3541740
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5071659
	speed: 0.2746s/iter; left time: 3939.7074s
	iters: 200, epoch: 45 | loss: 0.4532323
	speed: 0.0670s/iter; left time: 954.0661s
Epoch: 45 cost time: 17.75984501838684
Epoch: 45, Steps: 258 | Train Loss: 0.4986912 Vali Loss: 0.2628251 Test Loss: 0.3541798
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.6295457
	speed: 0.2689s/iter; left time: 3788.4210s
	iters: 200, epoch: 46 | loss: 0.4448755
	speed: 0.0604s/iter; left time: 844.8049s
Epoch: 46 cost time: 16.266847848892212
Epoch: 46, Steps: 258 | Train Loss: 0.4981894 Vali Loss: 0.2627012 Test Loss: 0.3541986
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4952003
	speed: 0.3043s/iter; left time: 4209.1967s
	iters: 200, epoch: 47 | loss: 0.4717154
	speed: 0.0685s/iter; left time: 940.8073s
Epoch: 47 cost time: 18.037808418273926
Epoch: 47, Steps: 258 | Train Loss: 0.4986862 Vali Loss: 0.2628925 Test Loss: 0.3541698
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.8070107
	speed: 0.2431s/iter; left time: 3300.1461s
	iters: 200, epoch: 48 | loss: 0.5809879
	speed: 0.0567s/iter; left time: 763.7781s
Epoch: 48 cost time: 14.861409187316895
Epoch: 48, Steps: 258 | Train Loss: 0.4979308 Vali Loss: 0.2630849 Test Loss: 0.3541678
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4352865
	speed: 0.2314s/iter; left time: 3081.0486s
	iters: 200, epoch: 49 | loss: 0.4698446
	speed: 0.0567s/iter; left time: 749.7133s
Epoch: 49 cost time: 14.979570388793945
Epoch: 49, Steps: 258 | Train Loss: 0.4981418 Vali Loss: 0.2628868 Test Loss: 0.3541721
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.4415473
	speed: 0.2401s/iter; left time: 3135.0728s
	iters: 200, epoch: 50 | loss: 0.3025338
	speed: 0.0546s/iter; left time: 707.4864s
Epoch: 50 cost time: 14.82735013961792
Epoch: 50, Steps: 258 | Train Loss: 0.4986945 Vali Loss: 0.2625125 Test Loss: 0.3541689
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.5364268
	speed: 0.2355s/iter; left time: 3014.6660s
	iters: 200, epoch: 51 | loss: 0.4958016
	speed: 0.0586s/iter; left time: 744.6141s
Epoch: 51 cost time: 15.27162480354309
Epoch: 51, Steps: 258 | Train Loss: 0.4979799 Vali Loss: 0.2629370 Test Loss: 0.3541620
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.3934259
	speed: 0.2457s/iter; left time: 3081.2057s
	iters: 200, epoch: 52 | loss: 0.4219368
	speed: 0.0557s/iter; left time: 693.3675s
Epoch: 52 cost time: 15.127607345581055
Epoch: 52, Steps: 258 | Train Loss: 0.4979594 Vali Loss: 0.2629298 Test Loss: 0.3541639
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.3901166
	speed: 0.2371s/iter; left time: 2912.3920s
	iters: 200, epoch: 53 | loss: 0.5369236
	speed: 0.0546s/iter; left time: 665.5556s
Epoch: 53 cost time: 14.878903865814209
Epoch: 53, Steps: 258 | Train Loss: 0.4984468 Vali Loss: 0.2630747 Test Loss: 0.3541574
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.6720511
	speed: 0.2322s/iter; left time: 2792.2146s
	iters: 200, epoch: 54 | loss: 0.5215874
	speed: 0.0549s/iter; left time: 655.1333s
Epoch: 54 cost time: 14.543956995010376
Epoch: 54, Steps: 258 | Train Loss: 0.4983045 Vali Loss: 0.2629880 Test Loss: 0.3541657
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.4697492
	speed: 0.2338s/iter; left time: 2751.1763s
	iters: 200, epoch: 55 | loss: 0.5224663
	speed: 0.0531s/iter; left time: 619.8943s
Epoch: 55 cost time: 14.385369777679443
Epoch: 55, Steps: 258 | Train Loss: 0.4982795 Vali Loss: 0.2629027 Test Loss: 0.3541578
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.5005400
	speed: 0.2025s/iter; left time: 2330.4327s
	iters: 200, epoch: 56 | loss: 0.4514953
	speed: 0.0546s/iter; left time: 623.4687s
Epoch: 56 cost time: 14.516387224197388
Epoch: 56, Steps: 258 | Train Loss: 0.4980704 Vali Loss: 0.2628863 Test Loss: 0.3541692
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3508615493774414, mae:0.37958526611328125, rse:0.47611549496650696, corr:[0.5286908  0.53226674 0.5352513  0.53691536 0.5372583  0.5367382
 0.53582424 0.5348588  0.53405493 0.53352666 0.53329325 0.5332931
 0.533469   0.5337185  0.53397685 0.5340827  0.5339845  0.5336783
 0.53316617 0.53246343 0.5316674  0.5308279  0.530028   0.5293515
 0.52878517 0.5283355  0.5279792  0.52770084 0.52746946 0.5272539
 0.5270026  0.5267424  0.52643746 0.52601457 0.5254711  0.524839
 0.524138   0.5234287  0.5227218  0.52203965 0.5213963  0.5207576
 0.5201344  0.51951694 0.51891965 0.51832634 0.51775753 0.5172376
 0.5167263  0.5161578  0.5155199  0.5148555  0.5141573  0.5134424
 0.5127431  0.51209295 0.51149863 0.51096517 0.51051044 0.51011884
 0.50981313 0.5095949  0.5094586  0.50936437 0.5092661  0.5091802
 0.5090189  0.5088158  0.5085704  0.508267   0.5078826  0.5074084
 0.50686485 0.5062539  0.50557137 0.504845   0.5041252  0.503435
 0.50283045 0.5023136  0.50183475 0.5013967  0.50099796 0.5006248
 0.50026757 0.4999142  0.4995099  0.4990652  0.49851954 0.4978535
 0.49708197 0.49621782 0.4952267  0.49411264 0.49287823 0.49154902
 0.49018303 0.48888165 0.4876427  0.48645258 0.48530906 0.48419768
 0.48310274 0.48200294 0.48087618 0.47972986 0.47859165 0.47751176
 0.47648376 0.4755109  0.474598   0.47372657 0.4729494  0.4722357
 0.4715424  0.4708639  0.47020262 0.4695205  0.4688198  0.46804187
 0.4672119  0.46630114 0.46535778 0.46437976 0.4633726  0.46233886
 0.46134126 0.46041593 0.45958915 0.45886284 0.45823827 0.45767108
 0.4571438  0.45662376 0.4561094  0.4555607  0.4549237  0.45420632
 0.4534006  0.4525244  0.45159385 0.450656   0.44976875 0.4489166
 0.4481034  0.44728205 0.4465255  0.44582713 0.4452019  0.44461855
 0.4440899  0.44362223 0.44313478 0.4425811  0.44194302 0.44125003
 0.44052923 0.43980402 0.43907124 0.43840873 0.43781328 0.43730864
 0.43689078 0.4365744  0.4363568  0.43622002 0.43610525 0.43595913
 0.43574005 0.4354428  0.43499994 0.43443358 0.43372226 0.43291745
 0.43205902 0.4312032  0.43038616 0.42962217 0.42892206 0.42833683
 0.42784798 0.42744437 0.4271255  0.426845   0.42654303 0.42615026
 0.42564636 0.42496103 0.4240863  0.42298365 0.4216573  0.42010367
 0.41838863 0.41670474 0.4150482  0.41342348 0.41188636 0.41044322
 0.40913364 0.40796027 0.406903   0.40594584 0.40505642 0.40420353
 0.40336886 0.40252277 0.40161723 0.40061626 0.39957064 0.398576
 0.3975867  0.39657786 0.39553282 0.39447582 0.39346865 0.3924881
 0.3915257  0.3905228  0.38951236 0.38849184 0.3874414  0.38635844
 0.385241   0.38410652 0.38302252 0.38193187 0.38082314 0.37975466
 0.3787275  0.37781554 0.3770473  0.3764548  0.37600958 0.37568486
 0.37539434 0.37511012 0.37479666 0.37443045 0.37402812 0.37358305
 0.37310144 0.37253848 0.37188208 0.37124732 0.37060687 0.37003163
 0.36955547 0.36920878 0.3689712  0.368807   0.36873716 0.36873138
 0.36876136 0.36882603 0.3688577  0.36882496 0.36868766 0.3684626
 0.36815184 0.36776873 0.36731058 0.3668159  0.36628303 0.3657851
 0.36533505 0.36491695 0.36454448 0.36422062 0.3639218  0.3636216
 0.36336192 0.36313087 0.3628946  0.36266613 0.36242485 0.36214983
 0.3618734  0.36156482 0.3611979  0.3608324  0.3604433  0.3600576
 0.3596575  0.35924217 0.3587984  0.35828713 0.35770252 0.35696575
 0.35613105 0.355296   0.35448146 0.35366344 0.35283604 0.3519683
 0.35112938 0.35032824 0.34956276 0.3488436  0.34820488 0.34762305
 0.3471402  0.34669515 0.34632567 0.34596595 0.34564114 0.34532538
 0.3450627  0.34478545 0.3444523  0.344045   0.34355074 0.34305382
 0.3425177  0.3420122  0.34153086 0.34108174 0.34070712 0.34044352
 0.34026927 0.34020057 0.34023982 0.34034127 0.34045014 0.34054294
 0.3405901  0.34061396 0.3406145  0.3405346  0.3403966  0.34018373
 0.33996716 0.3397368  0.33952773 0.33939296 0.33930176 0.33929634
 0.3393439  0.33934498 0.33930358 0.3392528  0.3392033  0.3391212
 0.33901808 0.33889392 0.3387609  0.33859146 0.33838475 0.33818972
 0.33801642 0.33789855 0.33782825 0.33778247 0.33775187 0.33769947
 0.33760566 0.33748248 0.337289   0.33708325 0.33679366 0.33644488
 0.3360348  0.33557814 0.33509016 0.33460438 0.33414543 0.33379015
 0.33353704 0.3334332  0.33346203 0.33361942 0.33390126 0.33424655
 0.33461544 0.33499897 0.3353466  0.33560398 0.3357149  0.33569366
 0.3355743  0.33536047 0.3350243  0.33450133 0.3338685  0.3330961
 0.33228076 0.3314893  0.33077914 0.33014423 0.32952297 0.32897502
 0.3284412  0.3279039  0.3273516  0.32677993 0.32619578 0.32556874
 0.32490212 0.32425037 0.323568   0.32285938 0.3222162  0.32162425
 0.3211038  0.3206321  0.32024845 0.319998   0.31979334 0.3196211
 0.31942853 0.31923282 0.31902248 0.3187672  0.31848544 0.31816188
 0.31787288 0.31767115 0.31752443 0.31742737 0.31735018 0.31731373
 0.317359   0.3174992  0.31769833 0.31793216 0.3181792  0.31840006
 0.3185791  0.31867638 0.31870908 0.3186676  0.31857753 0.31844753
 0.3182713  0.31799793 0.31764737 0.31725523 0.31688952 0.31656957
 0.31626427 0.31605527 0.31590977 0.31576604 0.31560424 0.31546107
 0.3152722  0.3150648  0.31482688 0.31453076 0.3141995  0.31382897
 0.31346065 0.31310123 0.3127913  0.312488   0.31219572 0.31193522
 0.3116918  0.31146026 0.31121206 0.31092715 0.3106478  0.31031778
 0.30996656 0.30958927 0.30920297 0.30880722 0.30839655 0.30797657
 0.30757737 0.3072075  0.3068515  0.3064733  0.30610412 0.30570787
 0.30527857 0.30473164 0.30409032 0.30331165 0.30241728 0.30139846
 0.3002667  0.29912502 0.29796755 0.29682678 0.29573795 0.2947221
 0.29376096 0.29289934 0.2921132  0.29146817 0.29091337 0.29037958
 0.28979886 0.2892099  0.28860152 0.2879329  0.28722483 0.28648657
 0.28577545 0.2850362  0.2842547  0.2834993  0.28277022 0.28214976
 0.28159437 0.28107712 0.28060886 0.28017676 0.2798204  0.27949414
 0.2791951  0.27890527 0.27859297 0.27824682 0.27788264 0.27750784
 0.2771079  0.2766492  0.2762289  0.2758384  0.27547196 0.2751511
 0.27487484 0.27467087 0.2745191  0.2744248  0.27440473 0.2744241
 0.27440485 0.27433398 0.27417713 0.27394    0.27367488 0.27340305
 0.27310398 0.27276918 0.27248564 0.27223226 0.27201894 0.27184635
 0.2717167  0.271651   0.27160382 0.27158517 0.2715519  0.27147594
 0.27132413 0.27112928 0.27090052 0.2706471  0.2703371  0.2700059
 0.26961944 0.26920766 0.26881793 0.26851088 0.2682154  0.26798254
 0.26782754 0.26778194 0.26780453 0.26784226 0.26787776 0.26789227
 0.26787168 0.26779658 0.2676657  0.26745918 0.26716954 0.26680607
 0.2663738  0.26585096 0.2651729  0.26434657 0.2634041  0.26231357
 0.26111367 0.25997785 0.25896052 0.25807965 0.25726226 0.25644118
 0.25557414 0.25466064 0.2537205  0.2527788  0.25186908 0.2510333
 0.2502761  0.2495571  0.24891903 0.24834923 0.24784184 0.2474243
 0.2470986  0.24681851 0.24661174 0.24640548 0.2462026  0.24599391
 0.24574359 0.24546482 0.24511898 0.24468991 0.24424991 0.24384336
 0.24346551 0.2431373  0.24291824 0.24274816 0.24265508 0.2426185
 0.24269496 0.2427663  0.24283922 0.2429463  0.24306458 0.24315168
 0.24319857 0.2432544  0.24332567 0.24343814 0.24363522 0.24393383
 0.24428022 0.24459392 0.24480821 0.24497089 0.24510357 0.24522604
 0.24529497 0.24531351 0.2453196  0.24517207 0.24498491 0.24474305
 0.24447824 0.24424173 0.24400589 0.24386488 0.24371533 0.24359126
 0.24344411 0.2433688  0.24334063 0.24328385 0.2432963  0.24324985
 0.24316955 0.2430079  0.24275774 0.24242827 0.24211517 0.2417436
 0.24144189 0.24119297 0.24095967 0.24084638 0.2408851  0.2410373
 0.24125807 0.2415927  0.24194385 0.24227048 0.24256933 0.24276379
 0.24277334 0.24261601 0.24225682 0.24160449 0.24066238 0.23952621
 0.2383098  0.2371303  0.23607305 0.2351605  0.23445329 0.23390546
 0.23344782 0.23300052 0.23267607 0.23237996 0.23209934 0.23178226
 0.23142    0.23102278 0.23059566 0.23012373 0.22967008 0.22916086
 0.22860429 0.22798349 0.22727859 0.22665204 0.22610705 0.22563463
 0.22523522 0.22483066 0.22442968 0.22393414 0.22336581 0.22272539
 0.2221232  0.22147152 0.22080384 0.22014615 0.2195088  0.21893276
 0.2184216  0.21807876 0.21794115 0.21811843 0.21842813 0.21877688
 0.21895704 0.21877488 0.21807    0.21668643 0.21437542 0.21099932]
