Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4544778
	speed: 0.0359s/iter; left time: 923.5436s
	iters: 200, epoch: 1 | loss: 0.7022394
	speed: 0.0284s/iter; left time: 727.9711s
Epoch: 1 cost time: 8.071831226348877
Epoch: 1, Steps: 258 | Train Loss: 0.5723936 Vali Loss: 0.2777416 Test Loss: 0.3739824
Validation loss decreased (inf --> 0.277742).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4349810
	speed: 0.1164s/iter; left time: 2962.5794s
	iters: 200, epoch: 2 | loss: 0.4985230
	speed: 0.0331s/iter; left time: 839.8858s
Epoch: 2 cost time: 8.418288230895996
Epoch: 2, Steps: 258 | Train Loss: 0.5172120 Vali Loss: 0.2686980 Test Loss: 0.3641858
Validation loss decreased (0.277742 --> 0.268698).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5926580
	speed: 0.1182s/iter; left time: 2977.2863s
	iters: 200, epoch: 3 | loss: 0.3564458
	speed: 0.0261s/iter; left time: 655.6644s
Epoch: 3 cost time: 7.345400094985962
Epoch: 3, Steps: 258 | Train Loss: 0.5083822 Vali Loss: 0.2664196 Test Loss: 0.3601144
Validation loss decreased (0.268698 --> 0.266420).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4143704
	speed: 0.1006s/iter; left time: 2507.2529s
	iters: 200, epoch: 4 | loss: 0.6128854
	speed: 0.0258s/iter; left time: 641.6215s
Epoch: 4 cost time: 7.0375282764434814
Epoch: 4, Steps: 258 | Train Loss: 0.5042916 Vali Loss: 0.2644932 Test Loss: 0.3580890
Validation loss decreased (0.266420 --> 0.264493).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4208990
	speed: 0.1016s/iter; left time: 2506.1962s
	iters: 200, epoch: 5 | loss: 0.6471041
	speed: 0.0200s/iter; left time: 490.9056s
Epoch: 5 cost time: 6.1353724002838135
Epoch: 5, Steps: 258 | Train Loss: 0.5019744 Vali Loss: 0.2632241 Test Loss: 0.3566157
Validation loss decreased (0.264493 --> 0.263224).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4641482
	speed: 0.0992s/iter; left time: 2421.4495s
	iters: 200, epoch: 6 | loss: 0.4832663
	speed: 0.0197s/iter; left time: 479.3945s
Epoch: 6 cost time: 6.068034648895264
Epoch: 6, Steps: 258 | Train Loss: 0.4997227 Vali Loss: 0.2625380 Test Loss: 0.3556511
Validation loss decreased (0.263224 --> 0.262538).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5310655
	speed: 0.1013s/iter; left time: 2447.8919s
	iters: 200, epoch: 7 | loss: 0.4512278
	speed: 0.0250s/iter; left time: 602.2697s
Epoch: 7 cost time: 6.628026962280273
Epoch: 7, Steps: 258 | Train Loss: 0.4988393 Vali Loss: 0.2624554 Test Loss: 0.3548595
Validation loss decreased (0.262538 --> 0.262455).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5921586
	speed: 0.0972s/iter; left time: 2322.9998s
	iters: 200, epoch: 8 | loss: 0.4941877
	speed: 0.0227s/iter; left time: 539.2253s
Epoch: 8 cost time: 6.490809917449951
Epoch: 8, Steps: 258 | Train Loss: 0.4979616 Vali Loss: 0.2622595 Test Loss: 0.3542668
Validation loss decreased (0.262455 --> 0.262259).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5366186
	speed: 0.0963s/iter; left time: 2277.3448s
	iters: 200, epoch: 9 | loss: 0.4637832
	speed: 0.0200s/iter; left time: 471.4949s
Epoch: 9 cost time: 5.763864755630493
Epoch: 9, Steps: 258 | Train Loss: 0.4969715 Vali Loss: 0.2615247 Test Loss: 0.3542207
Validation loss decreased (0.262259 --> 0.261525).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5925203
	speed: 0.0897s/iter; left time: 2097.4440s
	iters: 200, epoch: 10 | loss: 0.5905334
	speed: 0.0198s/iter; left time: 459.9110s
Epoch: 10 cost time: 5.867491006851196
Epoch: 10, Steps: 258 | Train Loss: 0.4967525 Vali Loss: 0.2614141 Test Loss: 0.3537720
Validation loss decreased (0.261525 --> 0.261414).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4411637
	speed: 0.1000s/iter; left time: 2311.8809s
	iters: 200, epoch: 11 | loss: 0.6168321
	speed: 0.0222s/iter; left time: 511.2939s
Epoch: 11 cost time: 6.528573751449585
Epoch: 11, Steps: 258 | Train Loss: 0.4954586 Vali Loss: 0.2612070 Test Loss: 0.3536832
Validation loss decreased (0.261414 --> 0.261207).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6347083
	speed: 0.0971s/iter; left time: 2220.5281s
	iters: 200, epoch: 12 | loss: 0.4849268
	speed: 0.0225s/iter; left time: 511.3463s
Epoch: 12 cost time: 6.37360692024231
Epoch: 12, Steps: 258 | Train Loss: 0.4961481 Vali Loss: 0.2607695 Test Loss: 0.3535274
Validation loss decreased (0.261207 --> 0.260769).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4413612
	speed: 0.0876s/iter; left time: 1980.1678s
	iters: 200, epoch: 13 | loss: 0.5066220
	speed: 0.0161s/iter; left time: 362.3651s
Epoch: 13 cost time: 4.936201810836792
Epoch: 13, Steps: 258 | Train Loss: 0.4957576 Vali Loss: 0.2611679 Test Loss: 0.3533006
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6367861
	speed: 0.0673s/iter; left time: 1504.9276s
	iters: 200, epoch: 14 | loss: 0.4812858
	speed: 0.0146s/iter; left time: 324.2933s
Epoch: 14 cost time: 4.701207160949707
Epoch: 14, Steps: 258 | Train Loss: 0.4957763 Vali Loss: 0.2606065 Test Loss: 0.3529517
Validation loss decreased (0.260769 --> 0.260607).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4012551
	speed: 0.0722s/iter; left time: 1593.8273s
	iters: 200, epoch: 15 | loss: 0.6354129
	speed: 0.0139s/iter; left time: 306.0751s
Epoch: 15 cost time: 4.516694784164429
Epoch: 15, Steps: 258 | Train Loss: 0.4955311 Vali Loss: 0.2608686 Test Loss: 0.3529368
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5374254
	speed: 0.0620s/iter; left time: 1353.7593s
	iters: 200, epoch: 16 | loss: 0.4532540
	speed: 0.0159s/iter; left time: 345.8271s
Epoch: 16 cost time: 3.9558072090148926
Epoch: 16, Steps: 258 | Train Loss: 0.4949697 Vali Loss: 0.2605938 Test Loss: 0.3530515
Validation loss decreased (0.260607 --> 0.260594).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5397234
	speed: 0.0663s/iter; left time: 1429.2647s
	iters: 200, epoch: 17 | loss: 0.5074313
	speed: 0.0165s/iter; left time: 353.4567s
Epoch: 17 cost time: 4.573519706726074
Epoch: 17, Steps: 258 | Train Loss: 0.4950247 Vali Loss: 0.2607827 Test Loss: 0.3528188
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6026906
	speed: 0.0666s/iter; left time: 1418.7864s
	iters: 200, epoch: 18 | loss: 0.3719104
	speed: 0.0150s/iter; left time: 317.8303s
Epoch: 18 cost time: 4.6764185428619385
Epoch: 18, Steps: 258 | Train Loss: 0.4949632 Vali Loss: 0.2606691 Test Loss: 0.3528820
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3757881
	speed: 0.0630s/iter; left time: 1326.9849s
	iters: 200, epoch: 19 | loss: 0.4356644
	speed: 0.0167s/iter; left time: 349.6309s
Epoch: 19 cost time: 4.375670909881592
Epoch: 19, Steps: 258 | Train Loss: 0.4951749 Vali Loss: 0.2606961 Test Loss: 0.3527223
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.7398241
	speed: 0.0660s/iter; left time: 1373.1970s
	iters: 200, epoch: 20 | loss: 0.6901695
	speed: 0.0148s/iter; left time: 306.4214s
Epoch: 20 cost time: 4.622880220413208
Epoch: 20, Steps: 258 | Train Loss: 0.4949707 Vali Loss: 0.2606030 Test Loss: 0.3526671
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3369539
	speed: 0.0594s/iter; left time: 1219.8064s
	iters: 200, epoch: 21 | loss: 0.6480064
	speed: 0.0169s/iter; left time: 346.2399s
Epoch: 21 cost time: 3.753196954727173
Epoch: 21, Steps: 258 | Train Loss: 0.4945073 Vali Loss: 0.2604475 Test Loss: 0.3525700
Validation loss decreased (0.260594 --> 0.260448).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4389731
	speed: 0.0509s/iter; left time: 1032.6615s
	iters: 200, epoch: 22 | loss: 0.3643246
	speed: 0.0131s/iter; left time: 263.6968s
Epoch: 22 cost time: 3.5416975021362305
Epoch: 22, Steps: 258 | Train Loss: 0.4946775 Vali Loss: 0.2603291 Test Loss: 0.3526010
Validation loss decreased (0.260448 --> 0.260329).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4354681
	speed: 0.0534s/iter; left time: 1069.4364s
	iters: 200, epoch: 23 | loss: 0.4899466
	speed: 0.0152s/iter; left time: 302.3371s
Epoch: 23 cost time: 4.006048917770386
Epoch: 23, Steps: 258 | Train Loss: 0.4935116 Vali Loss: 0.2603980 Test Loss: 0.3525713
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5260977
	speed: 0.0551s/iter; left time: 1088.7143s
	iters: 200, epoch: 24 | loss: 0.5335602
	speed: 0.0136s/iter; left time: 267.5939s
Epoch: 24 cost time: 3.840395450592041
Epoch: 24, Steps: 258 | Train Loss: 0.4943609 Vali Loss: 0.2603887 Test Loss: 0.3524088
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5058795
	speed: 0.0594s/iter; left time: 1158.7347s
	iters: 200, epoch: 25 | loss: 0.6090977
	speed: 0.0153s/iter; left time: 296.3319s
Epoch: 25 cost time: 4.265801906585693
Epoch: 25, Steps: 258 | Train Loss: 0.4942349 Vali Loss: 0.2602887 Test Loss: 0.3524364
Validation loss decreased (0.260329 --> 0.260289).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4492454
	speed: 0.0587s/iter; left time: 1130.7917s
	iters: 200, epoch: 26 | loss: 0.5044553
	speed: 0.0130s/iter; left time: 249.4939s
Epoch: 26 cost time: 3.806856155395508
Epoch: 26, Steps: 258 | Train Loss: 0.4943406 Vali Loss: 0.2605415 Test Loss: 0.3524809
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5310141
	speed: 0.0568s/iter; left time: 1078.1487s
	iters: 200, epoch: 27 | loss: 0.4816548
	speed: 0.0139s/iter; left time: 263.0956s
Epoch: 27 cost time: 3.8654978275299072
Epoch: 27, Steps: 258 | Train Loss: 0.4937500 Vali Loss: 0.2602461 Test Loss: 0.3524487
Validation loss decreased (0.260289 --> 0.260246).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5185886
	speed: 0.0642s/iter; left time: 1202.1549s
	iters: 200, epoch: 28 | loss: 0.5354443
	speed: 0.0138s/iter; left time: 257.3142s
Epoch: 28 cost time: 4.323025941848755
Epoch: 28, Steps: 258 | Train Loss: 0.4937155 Vali Loss: 0.2601746 Test Loss: 0.3524292
Validation loss decreased (0.260246 --> 0.260175).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.5080081
	speed: 0.0570s/iter; left time: 1053.4831s
	iters: 200, epoch: 29 | loss: 0.4245389
	speed: 0.0132s/iter; left time: 243.1857s
Epoch: 29 cost time: 3.89479660987854
Epoch: 29, Steps: 258 | Train Loss: 0.4944031 Vali Loss: 0.2603012 Test Loss: 0.3524177
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.6667404
	speed: 0.0570s/iter; left time: 1039.3688s
	iters: 200, epoch: 30 | loss: 0.4574393
	speed: 0.0117s/iter; left time: 212.0797s
Epoch: 30 cost time: 3.6145029067993164
Epoch: 30, Steps: 258 | Train Loss: 0.4939747 Vali Loss: 0.2600221 Test Loss: 0.3524194
Validation loss decreased (0.260175 --> 0.260022).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4604883
	speed: 0.0573s/iter; left time: 1029.1014s
	iters: 200, epoch: 31 | loss: 0.4732870
	speed: 0.0139s/iter; left time: 248.2278s
Epoch: 31 cost time: 3.852410316467285
Epoch: 31, Steps: 258 | Train Loss: 0.4935969 Vali Loss: 0.2601946 Test Loss: 0.3524004
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.5314983
	speed: 0.0534s/iter; left time: 944.7508s
	iters: 200, epoch: 32 | loss: 0.5810678
	speed: 0.0144s/iter; left time: 253.8814s
Epoch: 32 cost time: 4.265375137329102
Epoch: 32, Steps: 258 | Train Loss: 0.4931874 Vali Loss: 0.2601509 Test Loss: 0.3522999
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4551722
	speed: 0.0668s/iter; left time: 1164.7370s
	iters: 200, epoch: 33 | loss: 0.4742486
	speed: 0.0149s/iter; left time: 258.8211s
Epoch: 33 cost time: 4.063632011413574
Epoch: 33, Steps: 258 | Train Loss: 0.4930892 Vali Loss: 0.2602394 Test Loss: 0.3522769
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3920030
	speed: 0.0561s/iter; left time: 964.9791s
	iters: 200, epoch: 34 | loss: 0.5601355
	speed: 0.0150s/iter; left time: 256.9659s
Epoch: 34 cost time: 4.092940807342529
Epoch: 34, Steps: 258 | Train Loss: 0.4934773 Vali Loss: 0.2602104 Test Loss: 0.3523345
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5245082
	speed: 0.0593s/iter; left time: 1003.7783s
	iters: 200, epoch: 35 | loss: 0.5592085
	speed: 0.0114s/iter; left time: 191.2563s
Epoch: 35 cost time: 3.6447372436523438
Epoch: 35, Steps: 258 | Train Loss: 0.4932844 Vali Loss: 0.2604312 Test Loss: 0.3523383
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4593197
	speed: 0.0503s/iter; left time: 838.4016s
	iters: 200, epoch: 36 | loss: 0.4842672
	speed: 0.0113s/iter; left time: 187.3205s
Epoch: 36 cost time: 3.510221004486084
Epoch: 36, Steps: 258 | Train Loss: 0.4938116 Vali Loss: 0.2601168 Test Loss: 0.3522565
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3542092
	speed: 0.0484s/iter; left time: 793.8777s
	iters: 200, epoch: 37 | loss: 0.6133042
	speed: 0.0120s/iter; left time: 195.9081s
Epoch: 37 cost time: 3.127009391784668
Epoch: 37, Steps: 258 | Train Loss: 0.4937472 Vali Loss: 0.2599384 Test Loss: 0.3522631
Validation loss decreased (0.260022 --> 0.259938).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4846775
	speed: 0.0462s/iter; left time: 745.9092s
	iters: 200, epoch: 38 | loss: 0.5129995
	speed: 0.0121s/iter; left time: 193.7787s
Epoch: 38 cost time: 3.422003984451294
Epoch: 38, Steps: 258 | Train Loss: 0.4938539 Vali Loss: 0.2601809 Test Loss: 0.3522709
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.5048515
	speed: 0.0493s/iter; left time: 784.0997s
	iters: 200, epoch: 39 | loss: 0.4590266
	speed: 0.0117s/iter; left time: 184.5582s
Epoch: 39 cost time: 3.265307903289795
Epoch: 39, Steps: 258 | Train Loss: 0.4935713 Vali Loss: 0.2597136 Test Loss: 0.3523196
Validation loss decreased (0.259938 --> 0.259714).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4454600
	speed: 0.0427s/iter; left time: 667.9089s
	iters: 200, epoch: 40 | loss: 0.5025565
	speed: 0.0085s/iter; left time: 132.4424s
Epoch: 40 cost time: 2.8710389137268066
Epoch: 40, Steps: 258 | Train Loss: 0.4934657 Vali Loss: 0.2600929 Test Loss: 0.3523543
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.6170201
	speed: 0.0469s/iter; left time: 721.1766s
	iters: 200, epoch: 41 | loss: 0.4472413
	speed: 0.0122s/iter; left time: 186.6138s
Epoch: 41 cost time: 3.2798149585723877
Epoch: 41, Steps: 258 | Train Loss: 0.4932178 Vali Loss: 0.2597907 Test Loss: 0.3522784
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4563319
	speed: 0.0482s/iter; left time: 729.4561s
	iters: 200, epoch: 42 | loss: 0.4517069
	speed: 0.0088s/iter; left time: 131.6270s
Epoch: 42 cost time: 2.9373011589050293
Epoch: 42, Steps: 258 | Train Loss: 0.4940916 Vali Loss: 0.2601701 Test Loss: 0.3522672
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4163353
	speed: 0.0457s/iter; left time: 678.8190s
	iters: 200, epoch: 43 | loss: 0.5680019
	speed: 0.0092s/iter; left time: 136.0498s
Epoch: 43 cost time: 3.0344204902648926
Epoch: 43, Steps: 258 | Train Loss: 0.4933743 Vali Loss: 0.2600936 Test Loss: 0.3522764
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.5364166
	speed: 0.0453s/iter; left time: 661.5456s
	iters: 200, epoch: 44 | loss: 0.6481620
	speed: 0.0116s/iter; left time: 168.6822s
Epoch: 44 cost time: 3.126192569732666
Epoch: 44, Steps: 258 | Train Loss: 0.4933949 Vali Loss: 0.2599618 Test Loss: 0.3522465
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5062079
	speed: 0.0450s/iter; left time: 645.3682s
	iters: 200, epoch: 45 | loss: 0.5197535
	speed: 0.0109s/iter; left time: 154.8451s
Epoch: 45 cost time: 3.0618045330047607
Epoch: 45, Steps: 258 | Train Loss: 0.4930939 Vali Loss: 0.2600175 Test Loss: 0.3522416
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.5717757
	speed: 0.0495s/iter; left time: 697.6698s
	iters: 200, epoch: 46 | loss: 0.4212204
	speed: 0.0097s/iter; left time: 135.9079s
Epoch: 46 cost time: 2.872880458831787
Epoch: 46, Steps: 258 | Train Loss: 0.4935782 Vali Loss: 0.2601147 Test Loss: 0.3522245
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4494974
	speed: 0.0507s/iter; left time: 701.2168s
	iters: 200, epoch: 47 | loss: 0.4069218
	speed: 0.0116s/iter; left time: 159.7128s
Epoch: 47 cost time: 3.368661880493164
Epoch: 47, Steps: 258 | Train Loss: 0.4935118 Vali Loss: 0.2601461 Test Loss: 0.3522356
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4382875
	speed: 0.0494s/iter; left time: 671.2665s
	iters: 200, epoch: 48 | loss: 0.5175716
	speed: 0.0098s/iter; left time: 132.2688s
Epoch: 48 cost time: 3.3169314861297607
Epoch: 48, Steps: 258 | Train Loss: 0.4928470 Vali Loss: 0.2598950 Test Loss: 0.3522065
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4733551
	speed: 0.0452s/iter; left time: 602.3745s
	iters: 200, epoch: 49 | loss: 0.6515802
	speed: 0.0091s/iter; left time: 120.6461s
Epoch: 49 cost time: 2.8792145252227783
Epoch: 49, Steps: 258 | Train Loss: 0.4938956 Vali Loss: 0.2601530 Test Loss: 0.3522008
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.5077739
	speed: 0.0465s/iter; left time: 607.7353s
	iters: 200, epoch: 50 | loss: 0.4308816
	speed: 0.0112s/iter; left time: 145.4237s
Epoch: 50 cost time: 3.0000483989715576
Epoch: 50, Steps: 258 | Train Loss: 0.4937892 Vali Loss: 0.2602675 Test Loss: 0.3521762
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.3937891
	speed: 0.0471s/iter; left time: 603.5016s
	iters: 200, epoch: 51 | loss: 0.5231659
	speed: 0.0107s/iter; left time: 135.9877s
Epoch: 51 cost time: 3.0807578563690186
Epoch: 51, Steps: 258 | Train Loss: 0.4940189 Vali Loss: 0.2602084 Test Loss: 0.3521983
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.5731809
	speed: 0.0489s/iter; left time: 613.6007s
	iters: 200, epoch: 52 | loss: 0.5214300
	speed: 0.0106s/iter; left time: 131.7446s
Epoch: 52 cost time: 3.0071027278900146
Epoch: 52, Steps: 258 | Train Loss: 0.4929254 Vali Loss: 0.2601310 Test Loss: 0.3521765
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.5532489
	speed: 0.0552s/iter; left time: 678.2612s
	iters: 200, epoch: 53 | loss: 0.5475992
	speed: 0.0113s/iter; left time: 137.2846s
Epoch: 53 cost time: 3.307119846343994
Epoch: 53, Steps: 258 | Train Loss: 0.4933226 Vali Loss: 0.2600836 Test Loss: 0.3521757
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.5217904
	speed: 0.0490s/iter; left time: 589.4244s
	iters: 200, epoch: 54 | loss: 0.5041406
	speed: 0.0098s/iter; left time: 116.8842s
Epoch: 54 cost time: 3.1649374961853027
Epoch: 54, Steps: 258 | Train Loss: 0.4934301 Vali Loss: 0.2600968 Test Loss: 0.3521766
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.4592797
	speed: 0.0468s/iter; left time: 550.5526s
	iters: 200, epoch: 55 | loss: 0.4303116
	speed: 0.0115s/iter; left time: 133.8861s
Epoch: 55 cost time: 3.204554796218872
Epoch: 55, Steps: 258 | Train Loss: 0.4931103 Vali Loss: 0.2599649 Test Loss: 0.3522026
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4684763
	speed: 0.0464s/iter; left time: 533.9714s
	iters: 200, epoch: 56 | loss: 0.4418988
	speed: 0.0123s/iter; left time: 140.8416s
Epoch: 56 cost time: 3.5171868801116943
Epoch: 56, Steps: 258 | Train Loss: 0.4930703 Vali Loss: 0.2602232 Test Loss: 0.3521971
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.4039185
	speed: 0.0451s/iter; left time: 507.2396s
	iters: 200, epoch: 57 | loss: 0.5079223
	speed: 0.0148s/iter; left time: 164.6202s
Epoch: 57 cost time: 3.3817434310913086
Epoch: 57, Steps: 258 | Train Loss: 0.4932498 Vali Loss: 0.2599774 Test Loss: 0.3521864
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.3225712
	speed: 0.0475s/iter; left time: 522.7075s
	iters: 200, epoch: 58 | loss: 0.5832924
	speed: 0.0104s/iter; left time: 113.1853s
Epoch: 58 cost time: 3.1563222408294678
Epoch: 58, Steps: 258 | Train Loss: 0.4932380 Vali Loss: 0.2599570 Test Loss: 0.3521836
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.4754395
	speed: 0.0474s/iter; left time: 508.7642s
	iters: 200, epoch: 59 | loss: 0.4325778
	speed: 0.0112s/iter; left time: 118.7158s
Epoch: 59 cost time: 3.14266037940979
Epoch: 59, Steps: 258 | Train Loss: 0.4933710 Vali Loss: 0.2600850 Test Loss: 0.3521844
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3489380478858948, mae:0.37786510586738586, rse:0.47480860352516174, corr:[0.54430395 0.54343104 0.53717065 0.53593755 0.53780234 0.5382559
 0.53669167 0.53537965 0.53557307 0.53662026 0.53697395 0.5362212
 0.5353528  0.53521097 0.535759   0.5360503  0.5354891  0.5344443
 0.53361094 0.5333179  0.5332903  0.5329014  0.53204405 0.531168
 0.5306723  0.53061455 0.5306048  0.5302261  0.5294897  0.5287582
 0.5283193  0.5282339  0.52812445 0.5276256  0.5268345  0.52607423
 0.5255185  0.5251642  0.5247696  0.5242064  0.5235538  0.5229574
 0.522546   0.52221596 0.5217599  0.5210439  0.520167   0.51931095
 0.51858175 0.5179719  0.51737463 0.51671755 0.5159573  0.51517874
 0.5145178  0.51400375 0.5135266  0.5130234  0.51254874 0.5121795
 0.51200885 0.5119648  0.51184845 0.5115463  0.51113915 0.510846
 0.5106907  0.51062775 0.5104536  0.51007664 0.5095902  0.5091695
 0.5088907  0.5086405  0.5082192  0.5076114  0.5069682  0.50644535
 0.50608283 0.5056899  0.50503045 0.504131   0.503196   0.50247717
 0.50207543 0.501816   0.5014285  0.50088936 0.5002836  0.4997823
 0.49944845 0.49912104 0.49857947 0.49774632 0.49664375 0.49535125
 0.49394357 0.4925004  0.49102694 0.48963562 0.48846218 0.4875109
 0.48662108 0.48557708 0.48431048 0.48299572 0.4819052  0.48115146
 0.4805195  0.47977012 0.4788038  0.47768465 0.47664702 0.47577018
 0.4749766  0.47416633 0.4733026  0.4723925  0.4715307  0.4707075
 0.46994117 0.46916685 0.46845195 0.46781716 0.46723986 0.46658763
 0.46572992 0.46461126 0.4633694  0.46226218 0.46148348 0.46093875
 0.46038616 0.4596279  0.45872247 0.45787528 0.45722234 0.4567258
 0.45614696 0.45534292 0.45440105 0.45360157 0.4531246  0.45278543
 0.45228472 0.4514147  0.45040315 0.44953203 0.44896472 0.44852945
 0.44797018 0.44716045 0.44617492 0.44532397 0.4448169  0.44453037
 0.44415486 0.44353497 0.4428074  0.4423357  0.44216526 0.4420646
 0.44167268 0.44091022 0.4400117  0.43935278 0.4390875  0.43904293
 0.43889457 0.4385007  0.4379365  0.43751085 0.43730143 0.43711665
 0.43665683 0.4358428  0.43487704 0.43408597 0.43360913 0.4333246
 0.43288887 0.43217468 0.43136802 0.43069866 0.430252   0.42988455
 0.42941412 0.42871487 0.42787325 0.42699215 0.42608616 0.42500454
 0.42364946 0.42218447 0.42066225 0.41923335 0.41800892 0.4169016
 0.41582823 0.41473338 0.4136286  0.41249755 0.41127777 0.4099487
 0.40858844 0.40735805 0.4063727  0.40558183 0.40484762 0.40403858
 0.40302196 0.401943   0.40101495 0.4002634  0.39951056 0.39848462
 0.3972001  0.39587128 0.3948385  0.394054   0.3932182  0.3920995
 0.39073333 0.3894162  0.38843626 0.38764557 0.38679725 0.38575003
 0.38449675 0.38331875 0.3824136  0.3817625  0.38114893 0.38045886
 0.3797358  0.3792215  0.37901953 0.3789908  0.37890086 0.37854743
 0.37797397 0.3773087  0.37672007 0.3763169  0.37590006 0.37549248
 0.37521777 0.37520728 0.37533787 0.37532562 0.3750464  0.3745304
 0.37408128 0.37405455 0.3743412  0.37458527 0.37437385 0.37369254
 0.37282553 0.37217346 0.37186924 0.37176847 0.37157875 0.37129918
 0.37108177 0.37101978 0.3710559  0.37089455 0.37031886 0.36946133
 0.3688566  0.36882743 0.36914006 0.36929289 0.36884022 0.3678311
 0.3668279  0.36624417 0.36611366 0.36617282 0.36601633 0.3656554
 0.36532614 0.36527333 0.3653702  0.36517754 0.3643607  0.36291137
 0.3613562  0.36027366 0.35961264 0.35896546 0.35805634 0.35694575
 0.35610116 0.355712   0.35553524 0.35516536 0.35445228 0.3535313
 0.35285485 0.3525323  0.35239467 0.35190234 0.35092345 0.34973246
 0.34888294 0.3485744  0.34863243 0.3486248  0.34826568 0.34774226
 0.34726277 0.3470826  0.34706458 0.34694925 0.34667683 0.34641668
 0.34629786 0.34635124 0.34636068 0.34605616 0.34542435 0.34480256
 0.34451777 0.34466693 0.34496823 0.3450166  0.34481156 0.34457037
 0.34467357 0.34507576 0.3454751  0.34555987 0.345191   0.34470177
 0.34440824 0.344354   0.3444443  0.3444594  0.34421772 0.34374934
 0.34329712 0.34301096 0.34285498 0.34273764 0.34268397 0.34282085
 0.34310892 0.34336156 0.34328055 0.3427586  0.3420661  0.3416006
 0.34157985 0.34183943 0.34189263 0.34154284 0.34080902 0.34018958
 0.3400314  0.34028518 0.34054863 0.3404707  0.34002304 0.33959404
 0.33943993 0.3396342  0.33987293 0.33987638 0.33963683 0.3393301
 0.33921784 0.33935738 0.3395335  0.3395142  0.33927193 0.3390442
 0.3390264  0.33913535 0.33909625 0.33870918 0.33816916 0.33764893
 0.33734417 0.3371193  0.33668312 0.3358199  0.33462653 0.33354682
 0.33280772 0.3323583  0.33192772 0.33131614 0.33056346 0.32985544
 0.32937437 0.32912746 0.32882926 0.3283137  0.32766286 0.3269438
 0.32627952 0.32569182 0.32519752 0.3248238  0.32452637 0.3243597
 0.32423455 0.3241007  0.32386103 0.32350662 0.32321092 0.3230398
 0.32301974 0.32302794 0.32288966 0.32261437 0.32231542 0.32214782
 0.32214186 0.32215667 0.32205024 0.3218973  0.3218879  0.32212955
 0.32254878 0.32288373 0.3230016  0.32293537 0.32289588 0.32301873
 0.32322153 0.32320777 0.32286447 0.32230884 0.32181194 0.32151037
 0.32130995 0.3211767  0.32094458 0.32059288 0.32030123 0.32023433
 0.32023707 0.3201597  0.31984285 0.31929782 0.31875315 0.31831768
 0.3180125  0.31772095 0.3173891  0.31699404 0.3166775  0.3165388
 0.31648493 0.31637263 0.31611443 0.31578746 0.315602   0.31550068
 0.3154125  0.31521127 0.31485036 0.31441897 0.314051   0.31382218
 0.3136815  0.3134711  0.31306416 0.31246    0.31182632 0.311251
 0.31081566 0.31045726 0.3101457  0.309706   0.30901676 0.30795735
 0.30654055 0.30505094 0.30369118 0.3026189  0.30181187 0.30108744
 0.30026636 0.29941073 0.2986263  0.29806727 0.29761368 0.29705474
 0.29627508 0.2954218  0.29466617 0.29403788 0.2934848  0.2928184
 0.29193598 0.29085532 0.2898247  0.28913403 0.28871474 0.2884128
 0.28789052 0.28709564 0.28629616 0.28577593 0.28567448 0.28573725
 0.285631   0.28517008 0.28442508 0.28368303 0.28318694 0.2829077
 0.2826321  0.28217727 0.2817325  0.28138196 0.28117722 0.28108013
 0.28093693 0.28067434 0.28027764 0.27989945 0.27971715 0.2797409
 0.27984875 0.27994344 0.2799551  0.27989855 0.2798112  0.27966502
 0.27944276 0.27920648 0.2790982  0.2790731  0.2790289  0.27888474
 0.2786296  0.27835292 0.27807692 0.27782124 0.2774876  0.2770183
 0.2765553  0.27631342 0.27631086 0.27636352 0.27617365 0.27571058
 0.27510533 0.2746944  0.27466297 0.2749121  0.2750263  0.27489224
 0.2745411  0.27424738 0.27409232 0.27402255 0.27393314 0.27377018
 0.2736114  0.27350548 0.27345288 0.2733007  0.2729545  0.27250788
 0.27215725 0.27203596 0.27197292 0.27168807 0.2709164  0.26955134
 0.26789996 0.2665533  0.26563296 0.2648937  0.26398695 0.26285094
 0.26170886 0.26081038 0.26021227 0.25971597 0.25905484 0.25820285
 0.25727996 0.2564713  0.25593382 0.2555191  0.25500533 0.25431687
 0.25349185 0.2526576  0.25204596 0.25164622 0.25147352 0.2514496
 0.25145778 0.25147867 0.2513574  0.2510162  0.250573   0.25014797
 0.24976003 0.24949092 0.24936993 0.24921024 0.24899885 0.24876933
 0.24868254 0.24861257 0.248579   0.24859789 0.24859954 0.24857675
 0.24861874 0.24884613 0.24913736 0.24941365 0.24966305 0.25002107
 0.2505437  0.25114194 0.25160208 0.2517621  0.25153065 0.25106865
 0.2506607  0.25059998 0.25088865 0.251048   0.25105426 0.25087422
 0.2506896  0.25064394 0.25059244 0.25052297 0.25022927 0.24990445
 0.24975093 0.24993256 0.25010392 0.249844   0.24931848 0.24865124
 0.24833016 0.2484102  0.2486493  0.24865794 0.24841604 0.2479988
 0.2479108  0.24818216 0.24844058 0.2484569  0.24815175 0.24769361
 0.24739435 0.24756384 0.24785356 0.2479187  0.24777715 0.24762471
 0.24771121 0.24818008 0.24871331 0.24874158 0.24803281 0.24684386
 0.24561355 0.24463728 0.24385834 0.24301584 0.24208051 0.24110791
 0.2402855  0.23969842 0.23951328 0.23930745 0.2389118  0.23828724
 0.23762985 0.23708586 0.23658459 0.2359573  0.235314   0.23464005
 0.23414731 0.23380916 0.23335427 0.23285317 0.23223156 0.23164944
 0.2313412  0.23116079 0.23091978 0.2302852  0.22941631 0.22860347
 0.22813681 0.22757204 0.22674863 0.22585501 0.22532049 0.22537401
 0.22545114 0.22490427 0.22356632 0.2224394  0.22232944 0.22326513
 0.22372182 0.2224097  0.22020718 0.21985109 0.22169024 0.21969743]
