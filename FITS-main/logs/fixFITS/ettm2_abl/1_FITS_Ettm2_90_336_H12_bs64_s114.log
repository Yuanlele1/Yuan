Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34135
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=22, out_features=104, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2050048.0
params:  2392.0
Trainable parameters:  2392
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5571242
	speed: 0.0438s/iter; left time: 1159.6321s
	iters: 200, epoch: 1 | loss: 0.6696391
	speed: 0.0195s/iter; left time: 514.5060s
Epoch: 1 cost time: 7.720567226409912
Epoch: 1, Steps: 266 | Train Loss: 0.5345845 Vali Loss: 0.2395729 Test Loss: 0.3337820
Validation loss decreased (inf --> 0.239573).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4468310
	speed: 0.1342s/iter; left time: 3520.6206s
	iters: 200, epoch: 2 | loss: 0.5835939
	speed: 0.0190s/iter; left time: 497.2721s
Epoch: 2 cost time: 7.122745990753174
Epoch: 2, Steps: 266 | Train Loss: 0.4595507 Vali Loss: 0.2224764 Test Loss: 0.3135441
Validation loss decreased (0.239573 --> 0.222476).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3327301
	speed: 0.1161s/iter; left time: 3015.0302s
	iters: 200, epoch: 3 | loss: 0.5324528
	speed: 0.0185s/iter; left time: 477.5683s
Epoch: 3 cost time: 5.982151985168457
Epoch: 3, Steps: 266 | Train Loss: 0.4496949 Vali Loss: 0.2195105 Test Loss: 0.3099785
Validation loss decreased (0.222476 --> 0.219511).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4505085
	speed: 0.1103s/iter; left time: 2834.2176s
	iters: 200, epoch: 4 | loss: 0.4853805
	speed: 0.0350s/iter; left time: 895.7165s
Epoch: 4 cost time: 7.788961172103882
Epoch: 4, Steps: 266 | Train Loss: 0.4462558 Vali Loss: 0.2184433 Test Loss: 0.3085636
Validation loss decreased (0.219511 --> 0.218443).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4316012
	speed: 0.0964s/iter; left time: 2451.8641s
	iters: 200, epoch: 5 | loss: 0.6528848
	speed: 0.0189s/iter; left time: 478.6592s
Epoch: 5 cost time: 5.676699638366699
Epoch: 5, Steps: 266 | Train Loss: 0.4446307 Vali Loss: 0.2178205 Test Loss: 0.3079264
Validation loss decreased (0.218443 --> 0.217820).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4840388
	speed: 0.0991s/iter; left time: 2493.6802s
	iters: 200, epoch: 6 | loss: 0.4826840
	speed: 0.0195s/iter; left time: 488.4309s
Epoch: 6 cost time: 5.648056507110596
Epoch: 6, Steps: 266 | Train Loss: 0.4438977 Vali Loss: 0.2179617 Test Loss: 0.3076108
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5267869
	speed: 0.1051s/iter; left time: 2617.1851s
	iters: 200, epoch: 7 | loss: 0.3115757
	speed: 0.0207s/iter; left time: 513.8733s
Epoch: 7 cost time: 6.235145568847656
Epoch: 7, Steps: 266 | Train Loss: 0.4428517 Vali Loss: 0.2176004 Test Loss: 0.3073422
Validation loss decreased (0.217820 --> 0.217600).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5530887
	speed: 0.1159s/iter; left time: 2855.8658s
	iters: 200, epoch: 8 | loss: 0.3337648
	speed: 0.0310s/iter; left time: 760.6749s
Epoch: 8 cost time: 9.23751449584961
Epoch: 8, Steps: 266 | Train Loss: 0.4421465 Vali Loss: 0.2178444 Test Loss: 0.3073315
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4711714
	speed: 0.1244s/iter; left time: 3032.8361s
	iters: 200, epoch: 9 | loss: 0.3354827
	speed: 0.0237s/iter; left time: 576.2133s
Epoch: 9 cost time: 7.211292028427124
Epoch: 9, Steps: 266 | Train Loss: 0.4421457 Vali Loss: 0.2177800 Test Loss: 0.3072856
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4620912
	speed: 0.1047s/iter; left time: 2523.8741s
	iters: 200, epoch: 10 | loss: 0.4281538
	speed: 0.0215s/iter; left time: 516.9025s
Epoch: 10 cost time: 6.176553726196289
Epoch: 10, Steps: 266 | Train Loss: 0.4417912 Vali Loss: 0.2179253 Test Loss: 0.3072383
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4317495
	speed: 0.0983s/iter; left time: 2344.0200s
	iters: 200, epoch: 11 | loss: 0.4376768
	speed: 0.0187s/iter; left time: 444.4788s
Epoch: 11 cost time: 5.759132146835327
Epoch: 11, Steps: 266 | Train Loss: 0.4421663 Vali Loss: 0.2179273 Test Loss: 0.3071979
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5685694
	speed: 0.0929s/iter; left time: 2190.2275s
	iters: 200, epoch: 12 | loss: 0.3475129
	speed: 0.0187s/iter; left time: 438.0756s
Epoch: 12 cost time: 5.446333408355713
Epoch: 12, Steps: 266 | Train Loss: 0.4414491 Vali Loss: 0.2178299 Test Loss: 0.3072250
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4175255
	speed: 0.1462s/iter; left time: 3406.6181s
	iters: 200, epoch: 13 | loss: 0.5144600
	speed: 0.0267s/iter; left time: 620.8312s
Epoch: 13 cost time: 7.0149455070495605
Epoch: 13, Steps: 266 | Train Loss: 0.4414527 Vali Loss: 0.2181273 Test Loss: 0.3072115
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4515486
	speed: 0.1592s/iter; left time: 3669.3692s
	iters: 200, epoch: 14 | loss: 0.2964434
	speed: 0.0209s/iter; left time: 478.7742s
Epoch: 14 cost time: 7.743903636932373
Epoch: 14, Steps: 266 | Train Loss: 0.4411836 Vali Loss: 0.2182422 Test Loss: 0.3072450
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5034363
	speed: 0.1053s/iter; left time: 2398.0495s
	iters: 200, epoch: 15 | loss: 0.4324774
	speed: 0.0219s/iter; left time: 497.0619s
Epoch: 15 cost time: 6.0213072299957275
Epoch: 15, Steps: 266 | Train Loss: 0.4414764 Vali Loss: 0.2180815 Test Loss: 0.3071513
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4043957
	speed: 0.1091s/iter; left time: 2455.1057s
	iters: 200, epoch: 16 | loss: 0.4418360
	speed: 0.0209s/iter; left time: 467.3621s
Epoch: 16 cost time: 6.820461988449097
Epoch: 16, Steps: 266 | Train Loss: 0.4414258 Vali Loss: 0.2184158 Test Loss: 0.3072552
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.6036550
	speed: 0.1145s/iter; left time: 2547.5488s
	iters: 200, epoch: 17 | loss: 0.5597114
	speed: 0.0199s/iter; left time: 440.0368s
Epoch: 17 cost time: 5.857352018356323
Epoch: 17, Steps: 266 | Train Loss: 0.4408192 Vali Loss: 0.2181102 Test Loss: 0.3071862
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3260532
	speed: 0.0972s/iter; left time: 2135.3872s
	iters: 200, epoch: 18 | loss: 0.3611681
	speed: 0.0250s/iter; left time: 546.5600s
Epoch: 18 cost time: 6.218295574188232
Epoch: 18, Steps: 266 | Train Loss: 0.4401640 Vali Loss: 0.2180496 Test Loss: 0.3071835
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4687680
	speed: 0.1121s/iter; left time: 2434.5972s
	iters: 200, epoch: 19 | loss: 0.3808658
	speed: 0.0198s/iter; left time: 427.3396s
Epoch: 19 cost time: 6.112658500671387
Epoch: 19, Steps: 266 | Train Loss: 0.4405239 Vali Loss: 0.2181417 Test Loss: 0.3072080
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5516645
	speed: 0.1588s/iter; left time: 3406.5971s
	iters: 200, epoch: 20 | loss: 0.3354804
	speed: 0.0303s/iter; left time: 645.7914s
Epoch: 20 cost time: 8.208775997161865
Epoch: 20, Steps: 266 | Train Loss: 0.4406136 Vali Loss: 0.2180575 Test Loss: 0.3072138
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5122963
	speed: 0.1191s/iter; left time: 2523.0655s
	iters: 200, epoch: 21 | loss: 0.4074211
	speed: 0.0271s/iter; left time: 571.0596s
Epoch: 21 cost time: 7.513330698013306
Epoch: 21, Steps: 266 | Train Loss: 0.4410404 Vali Loss: 0.2183531 Test Loss: 0.3072363
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4313007
	speed: 0.0940s/iter; left time: 1967.0315s
	iters: 200, epoch: 22 | loss: 0.3997494
	speed: 0.0258s/iter; left time: 538.0176s
Epoch: 22 cost time: 6.68794059753418
Epoch: 22, Steps: 266 | Train Loss: 0.4408587 Vali Loss: 0.2183113 Test Loss: 0.3072561
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5168065
	speed: 0.1023s/iter; left time: 2112.2663s
	iters: 200, epoch: 23 | loss: 0.3472828
	speed: 0.0188s/iter; left time: 385.9834s
Epoch: 23 cost time: 5.808924198150635
Epoch: 23, Steps: 266 | Train Loss: 0.4408018 Vali Loss: 0.2184413 Test Loss: 0.3072490
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3841263
	speed: 0.0922s/iter; left time: 1879.3637s
	iters: 200, epoch: 24 | loss: 0.3944950
	speed: 0.0177s/iter; left time: 358.9044s
Epoch: 24 cost time: 5.354201078414917
Epoch: 24, Steps: 266 | Train Loss: 0.4409326 Vali Loss: 0.2186066 Test Loss: 0.3072702
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3489458
	speed: 0.0949s/iter; left time: 1909.2819s
	iters: 200, epoch: 25 | loss: 0.3249961
	speed: 0.0297s/iter; left time: 593.6281s
Epoch: 25 cost time: 7.193113803863525
Epoch: 25, Steps: 266 | Train Loss: 0.4404398 Vali Loss: 0.2182276 Test Loss: 0.3072388
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.5471380
	speed: 0.1309s/iter; left time: 2598.8451s
	iters: 200, epoch: 26 | loss: 0.5561450
	speed: 0.0234s/iter; left time: 461.8253s
Epoch: 26 cost time: 7.211522817611694
Epoch: 26, Steps: 266 | Train Loss: 0.4409781 Vali Loss: 0.2186564 Test Loss: 0.3072637
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4027989
	speed: 0.0937s/iter; left time: 1835.0938s
	iters: 200, epoch: 27 | loss: 0.3743658
	speed: 0.0169s/iter; left time: 329.8096s
Epoch: 27 cost time: 5.593189477920532
Epoch: 27, Steps: 266 | Train Loss: 0.4402228 Vali Loss: 0.2187054 Test Loss: 0.3072841
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.30874118208885193, mae:0.34317097067832947, rse:0.4488052427768707, corr:[0.5611063  0.5610511  0.5520909  0.5510984  0.5515613  0.54854244
 0.5462958  0.54695517 0.5469599  0.54493266 0.54349583 0.54377073
 0.5436368  0.54225945 0.54111505 0.5408871  0.54027104 0.5388994
 0.5378727  0.53752464 0.53693944 0.53585804 0.53508615 0.5348799
 0.5345169  0.5338209  0.53339535 0.53321415 0.53265923 0.5316909
 0.5308737  0.53039265 0.52980995 0.5290222  0.52839345 0.528063
 0.5275417  0.5267281  0.5259588  0.5254288  0.52496094 0.52452576
 0.52420676 0.52400655 0.52370596 0.5231231  0.5224066  0.5216842
 0.52092016 0.52005917 0.5192135  0.51856    0.51798314 0.51736385
 0.51679766 0.51630133 0.5158605  0.5155188  0.5153188  0.51523906
 0.51510656 0.51489556 0.51474625 0.5147619  0.5147815  0.51469415
 0.51452297 0.5143384  0.51421624 0.51414216 0.5141164  0.51412076
 0.514093   0.51406014 0.5139835  0.51384526 0.513647   0.5133966
 0.5131475  0.5128783  0.5124968  0.5120377  0.5116874  0.5114359
 0.51118684 0.51085234 0.5104749  0.51013845 0.5098161  0.50945
 0.50904965 0.50860727 0.5080764  0.5072025  0.505791   0.5039707
 0.5019151  0.4996976  0.49749172 0.49552813 0.49393657 0.4926578
 0.49147815 0.49034747 0.48923987 0.48798648 0.48663676 0.4853786
 0.48427713 0.48318878 0.48193857 0.48057178 0.4793094  0.47813877
 0.47693262 0.47568357 0.47450012 0.47347546 0.47251138 0.47141844
 0.47034308 0.4694059  0.46854743 0.46760803 0.46655917 0.46556848
 0.46475908 0.46395743 0.46289703 0.46178684 0.46092182 0.46022776
 0.45941284 0.45848882 0.4577048  0.4572974  0.45680758 0.45612732
 0.455555   0.45537832 0.45534143 0.45498058 0.4543496  0.45372924
 0.4530725  0.45211995 0.45087153 0.44986948 0.4492139  0.44861042
 0.4478979  0.44744322 0.4472402  0.44690758 0.446358   0.44579595
 0.4454368  0.4453552  0.44525677 0.44516227 0.44506449 0.44495976
 0.44490504 0.4447097  0.44457516 0.44452178 0.44454646 0.4446403
 0.44474944 0.44481537 0.44481868 0.44479525 0.44480842 0.44479424
 0.4447516  0.44476163 0.44479033 0.4447346  0.44455755 0.44438007
 0.44422233 0.44398752 0.443754   0.44357046 0.44341454 0.4430926
 0.44255808 0.4420193  0.44148815 0.44064167 0.43914944 0.43707883
 0.43495536 0.4330794  0.43124935 0.42939657 0.42783824 0.42671356
 0.42580903 0.4248004  0.42372707 0.42269695 0.42162767 0.42045733
 0.4192879  0.4181763  0.41691446 0.41551536 0.4142046  0.41309607
 0.4119367  0.4107014  0.40957206 0.40861475 0.40764606 0.4065371
 0.4054897  0.4046295  0.40380546 0.4026398  0.40128532 0.4001795
 0.39932862 0.39833003 0.39738217 0.39656398 0.3959641  0.39523628
 0.39420855 0.39313933 0.39219493 0.39139163 0.39061785 0.39012232
 0.3899889  0.38993263 0.3896528  0.38920543 0.38894904 0.38882658
 0.38854194 0.38806486 0.3875625  0.38742653 0.38735583 0.38700533
 0.38674334 0.3868848  0.38723698 0.38720036 0.3868167  0.3865297
 0.3866391  0.38709602 0.3875069  0.38758758 0.3875103  0.38755074
 0.38778204 0.38804913 0.38821658 0.38844532 0.38875598 0.38900757
 0.3892776  0.3895518  0.38982463 0.3898325  0.3897113  0.3898392
 0.39020622 0.39044973 0.39036864 0.39043456 0.39106423 0.39178184
 0.39201972 0.39186826 0.3918475  0.39208958 0.39213955 0.3919222
 0.39186165 0.39210835 0.39220512 0.3917982  0.39098796 0.39002693
 0.3889541  0.3878252  0.38695    0.3865727  0.3864664  0.38645205
 0.38655844 0.38679844 0.38704783 0.38705528 0.38668188 0.38611665
 0.38556883 0.3851282  0.38470116 0.38410664 0.38325492 0.3823335
 0.38162354 0.38107586 0.38042688 0.3795943  0.3787357  0.37802318
 0.37729305 0.3766662  0.37622097 0.37583125 0.37523642 0.3743153
 0.3733823  0.37278038 0.37212634 0.37112674 0.37016377 0.36976734
 0.36950564 0.36865976 0.367395   0.36645365 0.36608055 0.3655805
 0.36489025 0.36457324 0.36421373 0.3633148  0.36247736 0.35895693]
