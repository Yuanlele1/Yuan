Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=14, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=14, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1580544.0
params:  1890.0
Trainable parameters:  1890
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7776318
	speed: 0.0807s/iter; left time: 2115.5737s
	iters: 200, epoch: 1 | loss: 0.5866364
	speed: 0.0750s/iter; left time: 1957.9926s
Epoch: 1 cost time: 20.17751407623291
Epoch: 1, Steps: 263 | Train Loss: 0.7721846 Vali Loss: 0.3430598 Test Loss: 0.4790584
Validation loss decreased (inf --> 0.343060).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5935029
	speed: 0.3238s/iter; left time: 8399.6892s
	iters: 200, epoch: 2 | loss: 0.6389160
	speed: 0.0740s/iter; left time: 1912.7121s
Epoch: 2 cost time: 20.13698697090149
Epoch: 2, Steps: 263 | Train Loss: 0.6276599 Vali Loss: 0.3011907 Test Loss: 0.4271515
Validation loss decreased (0.343060 --> 0.301191).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6447510
	speed: 0.3260s/iter; left time: 8369.0778s
	iters: 200, epoch: 3 | loss: 0.8000606
	speed: 0.0738s/iter; left time: 1887.0191s
Epoch: 3 cost time: 20.43557620048523
Epoch: 3, Steps: 263 | Train Loss: 0.5997726 Vali Loss: 0.2921603 Test Loss: 0.4162778
Validation loss decreased (0.301191 --> 0.292160).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.8567581
	speed: 0.3262s/iter; left time: 8288.5771s
	iters: 200, epoch: 4 | loss: 0.6989725
	speed: 0.0739s/iter; left time: 1871.1759s
Epoch: 4 cost time: 20.250917673110962
Epoch: 4, Steps: 263 | Train Loss: 0.5928959 Vali Loss: 0.2902003 Test Loss: 0.4133317
Validation loss decreased (0.292160 --> 0.290200).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5151061
	speed: 0.3249s/iter; left time: 8170.0941s
	iters: 200, epoch: 5 | loss: 0.6547130
	speed: 0.0738s/iter; left time: 1849.4506s
Epoch: 5 cost time: 20.450921773910522
Epoch: 5, Steps: 263 | Train Loss: 0.5919339 Vali Loss: 0.2897074 Test Loss: 0.4124022
Validation loss decreased (0.290200 --> 0.289707).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5223864
	speed: 0.3296s/iter; left time: 8202.5055s
	iters: 200, epoch: 6 | loss: 0.6037742
	speed: 0.0754s/iter; left time: 1868.6408s
Epoch: 6 cost time: 20.75179648399353
Epoch: 6, Steps: 263 | Train Loss: 0.5896598 Vali Loss: 0.2890676 Test Loss: 0.4117629
Validation loss decreased (0.289707 --> 0.289068).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6602656
	speed: 0.3255s/iter; left time: 8014.8245s
	iters: 200, epoch: 7 | loss: 0.6807027
	speed: 0.0735s/iter; left time: 1801.6827s
Epoch: 7 cost time: 19.867605924606323
Epoch: 7, Steps: 263 | Train Loss: 0.5894094 Vali Loss: 0.2891789 Test Loss: 0.4116241
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3784232
	speed: 0.3253s/iter; left time: 7923.6678s
	iters: 200, epoch: 8 | loss: 0.8083853
	speed: 0.0768s/iter; left time: 1863.8991s
Epoch: 8 cost time: 20.476789236068726
Epoch: 8, Steps: 263 | Train Loss: 0.5892199 Vali Loss: 0.2891936 Test Loss: 0.4114698
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6719561
	speed: 0.3188s/iter; left time: 7681.8943s
	iters: 200, epoch: 9 | loss: 0.4933011
	speed: 0.0739s/iter; left time: 1773.7139s
Epoch: 9 cost time: 19.938761472702026
Epoch: 9, Steps: 263 | Train Loss: 0.5887640 Vali Loss: 0.2893793 Test Loss: 0.4115046
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5609882
	speed: 0.3223s/iter; left time: 7680.7718s
	iters: 200, epoch: 10 | loss: 0.6637101
	speed: 0.0756s/iter; left time: 1795.3835s
Epoch: 10 cost time: 20.60892105102539
Epoch: 10, Steps: 263 | Train Loss: 0.5884718 Vali Loss: 0.2894085 Test Loss: 0.4114107
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5418407
	speed: 0.3284s/iter; left time: 7741.0623s
	iters: 200, epoch: 11 | loss: 1.0221939
	speed: 0.0741s/iter; left time: 1738.6453s
Epoch: 11 cost time: 20.31335735321045
Epoch: 11, Steps: 263 | Train Loss: 0.5882734 Vali Loss: 0.2893611 Test Loss: 0.4112813
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6247002
	speed: 0.3181s/iter; left time: 7414.6424s
	iters: 200, epoch: 12 | loss: 0.5459333
	speed: 0.0732s/iter; left time: 1697.9081s
Epoch: 12 cost time: 20.108625173568726
Epoch: 12, Steps: 263 | Train Loss: 0.5879999 Vali Loss: 0.2894688 Test Loss: 0.4114278
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5433695
	speed: 0.3285s/iter; left time: 7570.1937s
	iters: 200, epoch: 13 | loss: 0.4244522
	speed: 0.0748s/iter; left time: 1716.0746s
Epoch: 13 cost time: 20.48019576072693
Epoch: 13, Steps: 263 | Train Loss: 0.5883595 Vali Loss: 0.2896403 Test Loss: 0.4114005
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.7410546
	speed: 0.3294s/iter; left time: 7503.7454s
	iters: 200, epoch: 14 | loss: 0.5908017
	speed: 0.0744s/iter; left time: 1688.6142s
Epoch: 14 cost time: 20.466079711914062
Epoch: 14, Steps: 263 | Train Loss: 0.5874503 Vali Loss: 0.2894885 Test Loss: 0.4114201
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5678359
	speed: 0.3287s/iter; left time: 7401.0308s
	iters: 200, epoch: 15 | loss: 0.3854921
	speed: 0.0755s/iter; left time: 1692.9580s
Epoch: 15 cost time: 20.50791072845459
Epoch: 15, Steps: 263 | Train Loss: 0.5878570 Vali Loss: 0.2896569 Test Loss: 0.4113687
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.7845715
	speed: 0.3261s/iter; left time: 7257.8975s
	iters: 200, epoch: 16 | loss: 0.5651676
	speed: 0.0749s/iter; left time: 1660.4152s
Epoch: 16 cost time: 20.178664207458496
Epoch: 16, Steps: 263 | Train Loss: 0.5876094 Vali Loss: 0.2897222 Test Loss: 0.4114045
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.6127915
	speed: 0.3280s/iter; left time: 7213.9867s
	iters: 200, epoch: 17 | loss: 0.5578168
	speed: 0.0734s/iter; left time: 1605.9788s
Epoch: 17 cost time: 20.067675590515137
Epoch: 17, Steps: 263 | Train Loss: 0.5885095 Vali Loss: 0.2895370 Test Loss: 0.4113708
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.7243198
	speed: 0.3283s/iter; left time: 7135.0247s
	iters: 200, epoch: 18 | loss: 0.5211990
	speed: 0.0767s/iter; left time: 1659.8994s
Epoch: 18 cost time: 20.62635064125061
Epoch: 18, Steps: 263 | Train Loss: 0.5877713 Vali Loss: 0.2897224 Test Loss: 0.4113873
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5760996
	speed: 0.3297s/iter; left time: 7078.4240s
	iters: 200, epoch: 19 | loss: 0.4691648
	speed: 0.0737s/iter; left time: 1574.2715s
Epoch: 19 cost time: 20.354252576828003
Epoch: 19, Steps: 263 | Train Loss: 0.5872131 Vali Loss: 0.2896098 Test Loss: 0.4113668
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4283980
	speed: 0.3260s/iter; left time: 6911.8319s
	iters: 200, epoch: 20 | loss: 0.5844561
	speed: 0.0758s/iter; left time: 1600.4242s
Epoch: 20 cost time: 20.63788914680481
Epoch: 20, Steps: 263 | Train Loss: 0.5879996 Vali Loss: 0.2897686 Test Loss: 0.4114033
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.7463120
	speed: 0.3267s/iter; left time: 6842.1625s
	iters: 200, epoch: 21 | loss: 0.6507255
	speed: 0.0750s/iter; left time: 1563.9996s
Epoch: 21 cost time: 20.49471426010132
Epoch: 21, Steps: 263 | Train Loss: 0.5877572 Vali Loss: 0.2897935 Test Loss: 0.4114575
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.7579818
	speed: 0.3192s/iter; left time: 6601.4144s
	iters: 200, epoch: 22 | loss: 0.4798062
	speed: 0.0735s/iter; left time: 1512.8198s
Epoch: 22 cost time: 20.185210943222046
Epoch: 22, Steps: 263 | Train Loss: 0.5878826 Vali Loss: 0.2898911 Test Loss: 0.4114619
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.6034616
	speed: 0.3263s/iter; left time: 6661.6662s
	iters: 200, epoch: 23 | loss: 0.6192256
	speed: 0.0751s/iter; left time: 1525.7340s
Epoch: 23 cost time: 20.469586610794067
Epoch: 23, Steps: 263 | Train Loss: 0.5875846 Vali Loss: 0.2896108 Test Loss: 0.4114866
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4928680
	speed: 0.3315s/iter; left time: 6680.1645s
	iters: 200, epoch: 24 | loss: 0.4519342
	speed: 0.0735s/iter; left time: 1473.0730s
Epoch: 24 cost time: 20.16220188140869
Epoch: 24, Steps: 263 | Train Loss: 0.5866421 Vali Loss: 0.2900586 Test Loss: 0.4114759
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4051198
	speed: 0.3270s/iter; left time: 6502.8207s
	iters: 200, epoch: 25 | loss: 0.5671320
	speed: 0.0754s/iter; left time: 1492.3751s
Epoch: 25 cost time: 20.589752435684204
Epoch: 25, Steps: 263 | Train Loss: 0.5871563 Vali Loss: 0.2899275 Test Loss: 0.4114846
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3821959
	speed: 0.3258s/iter; left time: 6395.0736s
	iters: 200, epoch: 26 | loss: 0.8307810
	speed: 0.0756s/iter; left time: 1476.7824s
Epoch: 26 cost time: 20.310508728027344
Epoch: 26, Steps: 263 | Train Loss: 0.5872946 Vali Loss: 0.2896621 Test Loss: 0.4114648
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4094816744327545, mae:0.3986649513244629, rse:0.5143534541130066, corr:[0.538166   0.5446636  0.5411271  0.53443414 0.529847   0.5292346
 0.53058416 0.53098196 0.5296162  0.5275486  0.5262106  0.52620447
 0.5268778  0.5270301  0.52600485 0.524165   0.52240354 0.5214544
 0.5212801  0.52116364 0.52049726 0.5192525  0.517898   0.5170762
 0.5169577  0.51715153 0.5170988  0.5165317  0.51554656 0.51452005
 0.51375425 0.51329416 0.5129315  0.51249856 0.5119355  0.5112736
 0.51049703 0.50962013 0.50867367 0.50780267 0.50717264 0.50688225
 0.50687104 0.50689656 0.50666755 0.506032   0.50511163 0.504085
 0.50314754 0.502388   0.50175387 0.5011578  0.50049984 0.49979666
 0.499203   0.4987662  0.49844828 0.49815726 0.4978131  0.49743357
 0.49710086 0.49688968 0.49686426 0.49694803 0.49696133 0.4968183
 0.4965345  0.4962254  0.49603465 0.49600765 0.4960672  0.496127
 0.4960972  0.49596244 0.49571055 0.49540222 0.4951077  0.4948109
 0.49454203 0.49426705 0.49396238 0.49361956 0.49324298 0.49280414
 0.4922877  0.49172014 0.49116534 0.49072933 0.49041423 0.4901618
 0.48981345 0.48915854 0.4881568  0.48680857 0.48510352 0.48303568
 0.48070636 0.4783627  0.47618124 0.4741878  0.47235686 0.47069323
 0.46916348 0.46772406 0.46647388 0.4654468  0.46452338 0.4635077
 0.4622899  0.46092686 0.4595121  0.45810503 0.45676544 0.45549592
 0.45431232 0.45324764 0.45229152 0.4513746  0.45042863 0.4493497
 0.4482215  0.4471508  0.44619697 0.44534266 0.44452026 0.44363925
 0.4426729  0.44163474 0.44053984 0.43948644 0.43855408 0.43771666
 0.43691325 0.43614197 0.4354142  0.4348905  0.43451834 0.43427944
 0.43404812 0.43372738 0.43328682 0.43274572 0.43214634 0.43146083
 0.43063477 0.4296918  0.42865595 0.42771688 0.4269384  0.42636847
 0.4259303  0.4256504  0.42535606 0.4248637  0.4242791  0.42375237
 0.4234015  0.4232901  0.42328402 0.42331356 0.42321804 0.42290595
 0.42252678 0.42208213 0.42179292 0.42166698 0.42164007 0.42165315
 0.4216805  0.42170992 0.42169714 0.42161494 0.42146587 0.42129153
 0.42113167 0.42105445 0.42104334 0.4210558  0.42101356 0.4208655
 0.4205783  0.42015815 0.4197117  0.41925922 0.4188685  0.41851154
 0.4181235  0.41753933 0.41661677 0.41521946 0.41331062 0.4109342
 0.408367   0.4060457  0.40403438 0.40223792 0.4006024  0.39906627
 0.39761832 0.39623207 0.39505118 0.39416376 0.3934734  0.39272833
 0.39169616 0.39035565 0.3887726  0.38710642 0.38551593 0.38414738
 0.38293347 0.3817995  0.3806756  0.3795522  0.37845844 0.37738
 0.37634167 0.3752921  0.374291   0.3732453  0.3721036  0.37088218
 0.36966005 0.3684509  0.36747032 0.36661857 0.36585918 0.36505347
 0.36411947 0.36308634 0.3620344  0.36119494 0.36067125 0.36046138
 0.3603649  0.360196   0.3598836  0.35945025 0.35905823 0.35875756
 0.35849753 0.3581665  0.3576474  0.3572233  0.3569483  0.35681182
 0.3568844  0.35709047 0.35726294 0.35719088 0.35697037 0.35682765
 0.3569215  0.3573054  0.3578478  0.35827127 0.3583938  0.35820842
 0.35785803 0.3575963  0.35756192 0.35785764 0.3583572  0.35885
 0.3592116  0.3593482  0.3593735  0.35932663 0.3593341  0.35941306
 0.35951224 0.35959086 0.35963893 0.3597208  0.35989046 0.3601295
 0.36038756 0.3605835  0.360652   0.36065105 0.3605731  0.3604847
 0.36032662 0.35997885 0.35930705 0.35825586 0.35690477 0.35539904
 0.35399327 0.35291854 0.3521537  0.35160288 0.35114396 0.35081068
 0.35069636 0.3507354  0.35091075 0.35116863 0.35132542 0.3511795
 0.35069498 0.34995216 0.34912264 0.34834114 0.34767526 0.3470675
 0.3464507  0.3457595  0.34502742 0.34435236 0.34376273 0.34328234
 0.3428194  0.3423365  0.3417733  0.34110713 0.34038126 0.33969197
 0.33909008 0.3385928  0.33819187 0.3378577  0.33754933 0.33721295
 0.33679882 0.33635098 0.33594298 0.33557072 0.3353103  0.33518332
 0.33519623 0.33527508 0.33535102 0.3352969  0.33507407 0.3347575
 0.33441058 0.33408934 0.33390102 0.33395806 0.33419272 0.33446497
 0.3346931  0.33490792 0.33509514 0.33525646 0.33540684 0.33558014
 0.3358043  0.33604103 0.33629465 0.33654392 0.33673733 0.33679706
 0.33673897 0.33668998 0.33668375 0.33683053 0.33711234 0.3374689
 0.33778945 0.3380138  0.33811665 0.33814707 0.33817485 0.3382802
 0.33844155 0.3386619  0.3389415  0.33921555 0.33945218 0.33963865
 0.33978543 0.33991995 0.3400503  0.3402108  0.3403881  0.3405473
 0.34058344 0.34034556 0.33973917 0.33874854 0.33742857 0.33586147
 0.33427566 0.3329046  0.33183444 0.33095604 0.33012384 0.32933056
 0.32857993 0.32786727 0.32733208 0.32702652 0.32686377 0.3266171
 0.3260874  0.32524848 0.32416028 0.32296562 0.3218203  0.3207566
 0.3197155  0.3185963  0.31741863 0.31633317 0.31538385 0.31461072
 0.31393805 0.31325063 0.31246975 0.31161684 0.3107889  0.3100932
 0.30959415 0.30917922 0.30866608 0.3079916  0.3071153  0.30617467
 0.30533656 0.30463144 0.3040422  0.30351993 0.30306262 0.30273587
 0.30256668 0.30254748 0.3025289  0.30236468 0.30190527 0.30118775
 0.30034506 0.2995417  0.29891682 0.29856557 0.29832545 0.29808262
 0.29771817 0.2973237  0.29697308 0.29673657 0.29664087 0.29669338
 0.2968109  0.29693747 0.2970402  0.29706252 0.29698852 0.29680905
 0.2965815  0.29634503 0.29616815 0.29607424 0.29605213 0.29612118
 0.29621738 0.29626036 0.296182   0.29601306 0.2958818  0.2958299
 0.29591918 0.2961186  0.29635563 0.29647034 0.29633594 0.29594773
 0.29543856 0.2949706  0.294675   0.29454544 0.2944632  0.29420272
 0.2935799  0.29248285 0.2909558  0.2891129  0.2870939  0.28500047
 0.28295276 0.28110695 0.27946147 0.278004   0.27670762 0.27563965
 0.27488446 0.2744477  0.27434143 0.27447653 0.27463055 0.27453285
 0.27394557 0.2729349  0.27170855 0.27051798 0.26951033 0.26863396
 0.26777896 0.26675412 0.26554397 0.26433134 0.26333737 0.2626814
 0.26221353 0.26169646 0.26095244 0.2599436  0.25889614 0.2580667
 0.25758448 0.2573055  0.25694343 0.25628182 0.25523174 0.2539942
 0.25296366 0.25232637 0.25205284 0.25190246 0.2516328  0.2511857
 0.25066236 0.2503029  0.2501549  0.25007784 0.24983369 0.24923852
 0.24840254 0.24758431 0.24714711 0.24718125 0.24746957 0.24768724
 0.24759124 0.24715914 0.24663237 0.24624705 0.24617401 0.24637593
 0.2466667  0.2468461  0.24685445 0.24680768 0.24675018 0.24671572
 0.24667186 0.24659146 0.24647646 0.24636859 0.246263   0.24632291
 0.24649954 0.24668975 0.24677199 0.24671718 0.24651074 0.24629888
 0.24621046 0.24638866 0.24678668 0.24722707 0.2475027  0.24751
 0.24726576 0.24695462 0.24680765 0.24684255 0.24697156 0.24692623
 0.24645528 0.24540691 0.24384372 0.2419595  0.23994777 0.23792276
 0.23602255 0.23441255 0.23315978 0.23232931 0.2319354  0.23192869
 0.23212598 0.23241091 0.23270519 0.2329892  0.23324336 0.23332162
 0.2330868  0.23249711 0.2316873  0.23082331 0.23001723 0.22924978
 0.2284108  0.22735718 0.22608747 0.22478135 0.22371756 0.2231337
 0.22300395 0.22308385 0.2230188  0.22253042 0.22169723 0.22074662
 0.21992679 0.21938066 0.21903734 0.21865323 0.21807078 0.21726274
 0.21643436 0.21573766 0.21528076 0.2149255  0.2145869  0.21414636
 0.21363766 0.21323156 0.21301147 0.21292847 0.2128751  0.21257253
 0.21198365 0.21127395 0.2107575  0.21063259 0.21090183 0.21125883
 0.21156117 0.2117814  0.21190862 0.2119406  0.21188368 0.21162145
 0.21114312 0.21058233 0.21020205 0.21023083 0.21067034 0.21116656
 0.21129549 0.21086212 0.21007058 0.20932464 0.20917779 0.20974542
 0.21073024 0.21156427 0.21178715 0.21140572 0.21082126 0.21053022
 0.21082193 0.21155211 0.21227275 0.21264951 0.2125786  0.21227586
 0.21212062 0.21234772 0.21291822 0.21353002 0.21387798 0.21376087
 0.21318926 0.2123196  0.21130401 0.21014154 0.20865747 0.20675363
 0.2046179  0.20268904 0.20136708 0.20078899 0.20077142 0.2009065
 0.20082824 0.20052303 0.20035364 0.20066802 0.20143014 0.20213938
 0.20214075 0.20112208 0.19941324 0.19780064 0.19700012 0.19708055
 0.19742815 0.19713293 0.19565755 0.19332549 0.1911474  0.19005558
 0.19021152 0.19087529 0.19085069 0.18937317 0.1868353  0.18439807
 0.18327633 0.18363735 0.18440616 0.18410768 0.18187086 0.1784528
 0.17554225 0.17476246 0.1759518  0.17730524 0.17630774 0.17220598
 0.16686891 0.16381451 0.16532369 0.16957347 0.17045286 0.16162637]
