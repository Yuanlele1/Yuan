Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=16, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=16, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2064384.0
params:  2448.0
Trainable parameters:  2448
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7609645
	speed: 0.0309s/iter; left time: 810.8373s
	iters: 200, epoch: 1 | loss: 0.5364826
	speed: 0.0265s/iter; left time: 692.6769s
Epoch: 1 cost time: 7.673584938049316
Epoch: 1, Steps: 263 | Train Loss: 0.7059332 Vali Loss: 0.3492057 Test Loss: 0.4847707
Validation loss decreased (inf --> 0.349206).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4099909
	speed: 0.1024s/iter; left time: 2655.3992s
	iters: 200, epoch: 2 | loss: 0.4886910
	speed: 0.0229s/iter; left time: 591.2057s
Epoch: 2 cost time: 7.025148391723633
Epoch: 2, Steps: 263 | Train Loss: 0.5665860 Vali Loss: 0.3032621 Test Loss: 0.4290244
Validation loss decreased (0.349206 --> 0.303262).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4630547
	speed: 0.1084s/iter; left time: 2782.4644s
	iters: 200, epoch: 3 | loss: 0.4519430
	speed: 0.0372s/iter; left time: 952.4838s
Epoch: 3 cost time: 8.931686401367188
Epoch: 3, Steps: 263 | Train Loss: 0.5365432 Vali Loss: 0.2927897 Test Loss: 0.4168033
Validation loss decreased (0.303262 --> 0.292790).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.7661241
	speed: 0.1287s/iter; left time: 3270.2521s
	iters: 200, epoch: 4 | loss: 0.6700599
	speed: 0.0243s/iter; left time: 615.3935s
Epoch: 4 cost time: 8.416999578475952
Epoch: 4, Steps: 263 | Train Loss: 0.5294316 Vali Loss: 0.2906084 Test Loss: 0.4135868
Validation loss decreased (0.292790 --> 0.290608).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3797375
	speed: 0.1022s/iter; left time: 2569.5167s
	iters: 200, epoch: 5 | loss: 0.5885379
	speed: 0.0224s/iter; left time: 561.6289s
Epoch: 5 cost time: 6.750450611114502
Epoch: 5, Steps: 263 | Train Loss: 0.5277738 Vali Loss: 0.2894288 Test Loss: 0.4124146
Validation loss decreased (0.290608 --> 0.289429).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4088809
	speed: 0.0966s/iter; left time: 2403.9691s
	iters: 200, epoch: 6 | loss: 0.4243664
	speed: 0.0232s/iter; left time: 575.4660s
Epoch: 6 cost time: 7.488368988037109
Epoch: 6, Steps: 263 | Train Loss: 0.5262156 Vali Loss: 0.2893919 Test Loss: 0.4118994
Validation loss decreased (0.289429 --> 0.289392).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4361253
	speed: 0.1055s/iter; left time: 2596.7474s
	iters: 200, epoch: 7 | loss: 0.5781552
	speed: 0.0208s/iter; left time: 510.0684s
Epoch: 7 cost time: 6.932248115539551
Epoch: 7, Steps: 263 | Train Loss: 0.5259675 Vali Loss: 0.2892756 Test Loss: 0.4116001
Validation loss decreased (0.289392 --> 0.289276).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4597856
	speed: 0.1054s/iter; left time: 2567.9622s
	iters: 200, epoch: 8 | loss: 0.4510091
	speed: 0.0377s/iter; left time: 913.6735s
Epoch: 8 cost time: 8.894847631454468
Epoch: 8, Steps: 263 | Train Loss: 0.5249341 Vali Loss: 0.2891600 Test Loss: 0.4114261
Validation loss decreased (0.289276 --> 0.289160).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4756130
	speed: 0.1168s/iter; left time: 2815.4827s
	iters: 200, epoch: 9 | loss: 0.5627459
	speed: 0.0234s/iter; left time: 562.5928s
Epoch: 9 cost time: 7.868009090423584
Epoch: 9, Steps: 263 | Train Loss: 0.5250840 Vali Loss: 0.2894116 Test Loss: 0.4113830
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5446342
	speed: 0.1321s/iter; left time: 3149.1854s
	iters: 200, epoch: 10 | loss: 0.6356974
	speed: 0.0210s/iter; left time: 498.4150s
Epoch: 10 cost time: 6.610503673553467
Epoch: 10, Steps: 263 | Train Loss: 0.5245636 Vali Loss: 0.2894363 Test Loss: 0.4114719
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5397306
	speed: 0.0956s/iter; left time: 2254.0502s
	iters: 200, epoch: 11 | loss: 0.5260983
	speed: 0.0206s/iter; left time: 484.4095s
Epoch: 11 cost time: 6.636744022369385
Epoch: 11, Steps: 263 | Train Loss: 0.5244919 Vali Loss: 0.2891820 Test Loss: 0.4114650
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5266871
	speed: 0.1211s/iter; left time: 2823.4891s
	iters: 200, epoch: 12 | loss: 0.5342495
	speed: 0.0273s/iter; left time: 632.5580s
Epoch: 12 cost time: 6.790279865264893
Epoch: 12, Steps: 263 | Train Loss: 0.5241731 Vali Loss: 0.2894214 Test Loss: 0.4113925
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5449855
	speed: 0.1075s/iter; left time: 2478.1948s
	iters: 200, epoch: 13 | loss: 0.6799255
	speed: 0.0183s/iter; left time: 419.6061s
Epoch: 13 cost time: 5.665855407714844
Epoch: 13, Steps: 263 | Train Loss: 0.5245627 Vali Loss: 0.2894512 Test Loss: 0.4115067
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4786054
	speed: 0.0928s/iter; left time: 2114.5892s
	iters: 200, epoch: 14 | loss: 0.5815742
	speed: 0.0215s/iter; left time: 487.6168s
Epoch: 14 cost time: 6.135365962982178
Epoch: 14, Steps: 263 | Train Loss: 0.5236393 Vali Loss: 0.2894428 Test Loss: 0.4115011
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5269423
	speed: 0.1054s/iter; left time: 2373.3494s
	iters: 200, epoch: 15 | loss: 0.5031264
	speed: 0.0228s/iter; left time: 511.5155s
Epoch: 15 cost time: 6.890978097915649
Epoch: 15, Steps: 263 | Train Loss: 0.5240187 Vali Loss: 0.2896742 Test Loss: 0.4114432
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4379518
	speed: 0.1263s/iter; left time: 2811.3563s
	iters: 200, epoch: 16 | loss: 0.4711590
	speed: 0.0284s/iter; left time: 629.7013s
Epoch: 16 cost time: 7.357490062713623
Epoch: 16, Steps: 263 | Train Loss: 0.5240661 Vali Loss: 0.2896204 Test Loss: 0.4114358
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4614666
	speed: 0.0906s/iter; left time: 1993.0644s
	iters: 200, epoch: 17 | loss: 0.6723109
	speed: 0.0210s/iter; left time: 458.8501s
Epoch: 17 cost time: 6.192596435546875
Epoch: 17, Steps: 263 | Train Loss: 0.5239440 Vali Loss: 0.2896639 Test Loss: 0.4114953
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4415397
	speed: 0.0999s/iter; left time: 2171.3802s
	iters: 200, epoch: 18 | loss: 0.3532927
	speed: 0.0193s/iter; left time: 416.4413s
Epoch: 18 cost time: 6.035675525665283
Epoch: 18, Steps: 263 | Train Loss: 0.5234809 Vali Loss: 0.2899249 Test Loss: 0.4115074
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3535492
	speed: 0.1035s/iter; left time: 2221.6116s
	iters: 200, epoch: 19 | loss: 0.6660320
	speed: 0.0283s/iter; left time: 605.3893s
Epoch: 19 cost time: 8.165322303771973
Epoch: 19, Steps: 263 | Train Loss: 0.5240188 Vali Loss: 0.2896908 Test Loss: 0.4114929
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.8657409
	speed: 0.1089s/iter; left time: 2308.4842s
	iters: 200, epoch: 20 | loss: 0.7495978
	speed: 0.0367s/iter; left time: 773.7604s
Epoch: 20 cost time: 7.9129626750946045
Epoch: 20, Steps: 263 | Train Loss: 0.5242383 Vali Loss: 0.2896521 Test Loss: 0.4115116
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4445324
	speed: 0.1483s/iter; left time: 3105.3640s
	iters: 200, epoch: 21 | loss: 0.7346315
	speed: 0.0306s/iter; left time: 636.8186s
Epoch: 21 cost time: 9.159805059432983
Epoch: 21, Steps: 263 | Train Loss: 0.5238326 Vali Loss: 0.2897114 Test Loss: 0.4115728
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5247716
	speed: 0.1280s/iter; left time: 2646.9774s
	iters: 200, epoch: 22 | loss: 0.5578106
	speed: 0.0222s/iter; left time: 457.1339s
Epoch: 22 cost time: 8.656109094619751
Epoch: 22, Steps: 263 | Train Loss: 0.5236503 Vali Loss: 0.2897467 Test Loss: 0.4116066
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4914534
	speed: 0.1239s/iter; left time: 2528.4806s
	iters: 200, epoch: 23 | loss: 0.3907100
	speed: 0.0205s/iter; left time: 416.4668s
Epoch: 23 cost time: 6.581568241119385
Epoch: 23, Steps: 263 | Train Loss: 0.5238175 Vali Loss: 0.2898447 Test Loss: 0.4115859
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3534643
	speed: 0.0974s/iter; left time: 1961.8267s
	iters: 200, epoch: 24 | loss: 0.6977828
	speed: 0.0279s/iter; left time: 558.9519s
Epoch: 24 cost time: 7.445220947265625
Epoch: 24, Steps: 263 | Train Loss: 0.5228409 Vali Loss: 0.2898466 Test Loss: 0.4116057
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4528113
	speed: 0.0996s/iter; left time: 1979.9881s
	iters: 200, epoch: 25 | loss: 0.4754236
	speed: 0.0310s/iter; left time: 613.1215s
Epoch: 25 cost time: 6.867722749710083
Epoch: 25, Steps: 263 | Train Loss: 0.5237289 Vali Loss: 0.2899210 Test Loss: 0.4115772
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.6435865
	speed: 0.1158s/iter; left time: 2272.7937s
	iters: 200, epoch: 26 | loss: 0.7095559
	speed: 0.0189s/iter; left time: 368.6028s
Epoch: 26 cost time: 7.795750856399536
Epoch: 26, Steps: 263 | Train Loss: 0.5233387 Vali Loss: 0.2897649 Test Loss: 0.4116161
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5066354
	speed: 0.1149s/iter; left time: 2224.4171s
	iters: 200, epoch: 27 | loss: 0.3761506
	speed: 0.0195s/iter; left time: 376.0441s
Epoch: 27 cost time: 6.016186952590942
Epoch: 27, Steps: 263 | Train Loss: 0.5232203 Vali Loss: 0.2901235 Test Loss: 0.4116508
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4063010
	speed: 0.1196s/iter; left time: 2284.7190s
	iters: 200, epoch: 28 | loss: 0.3968202
	speed: 0.0322s/iter; left time: 612.2633s
Epoch: 28 cost time: 8.290168285369873
Epoch: 28, Steps: 263 | Train Loss: 0.5233048 Vali Loss: 0.2900685 Test Loss: 0.4116664
EarlyStopping counter: 20 out of 20
Early stopping
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=16, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2064384.0
params:  2448.0
Trainable parameters:  2448
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5721895
	speed: 0.0316s/iter; left time: 828.2010s
	iters: 200, epoch: 1 | loss: 0.5408672
	speed: 0.0211s/iter; left time: 551.9654s
Epoch: 1 cost time: 6.4236602783203125
Epoch: 1, Steps: 263 | Train Loss: 0.5888442 Vali Loss: 0.2891183 Test Loss: 0.4111840
Validation loss decreased (inf --> 0.289118).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4013277
	speed: 0.1016s/iter; left time: 2635.1374s
	iters: 200, epoch: 2 | loss: 0.8373004
	speed: 0.0173s/iter; left time: 446.0194s
Epoch: 2 cost time: 5.938470125198364
Epoch: 2, Steps: 263 | Train Loss: 0.5881071 Vali Loss: 0.2893044 Test Loss: 0.4111976
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.8172650
	speed: 0.1291s/iter; left time: 3315.3849s
	iters: 200, epoch: 3 | loss: 0.6569443
	speed: 0.0192s/iter; left time: 489.8317s
Epoch: 3 cost time: 5.548118352890015
Epoch: 3, Steps: 263 | Train Loss: 0.5874720 Vali Loss: 0.2891202 Test Loss: 0.4112654
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5684285
	speed: 0.1017s/iter; left time: 2585.0657s
	iters: 200, epoch: 4 | loss: 0.7623894
	speed: 0.0254s/iter; left time: 642.3238s
Epoch: 4 cost time: 6.776932001113892
Epoch: 4, Steps: 263 | Train Loss: 0.5871777 Vali Loss: 0.2893455 Test Loss: 0.4113000
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5764478
	speed: 0.0961s/iter; left time: 2415.8376s
	iters: 200, epoch: 5 | loss: 0.5552860
	speed: 0.0250s/iter; left time: 626.7730s
Epoch: 5 cost time: 6.4544219970703125
Epoch: 5, Steps: 263 | Train Loss: 0.5877916 Vali Loss: 0.2893313 Test Loss: 0.4113005
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6444135
	speed: 0.0979s/iter; left time: 2437.3384s
	iters: 200, epoch: 6 | loss: 0.5112779
	speed: 0.0202s/iter; left time: 500.8901s
Epoch: 6 cost time: 5.904282569885254
Epoch: 6, Steps: 263 | Train Loss: 0.5875554 Vali Loss: 0.2893991 Test Loss: 0.4112942
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6096542
	speed: 0.0967s/iter; left time: 2381.9387s
	iters: 200, epoch: 7 | loss: 0.6715479
	speed: 0.0222s/iter; left time: 545.5863s
Epoch: 7 cost time: 6.2918055057525635
Epoch: 7, Steps: 263 | Train Loss: 0.5876802 Vali Loss: 0.2896445 Test Loss: 0.4114034
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6000085
	speed: 0.1007s/iter; left time: 2452.7795s
	iters: 200, epoch: 8 | loss: 0.4709638
	speed: 0.0205s/iter; left time: 497.8656s
Epoch: 8 cost time: 6.378277063369751
Epoch: 8, Steps: 263 | Train Loss: 0.5875686 Vali Loss: 0.2897240 Test Loss: 0.4113906
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5188334
	speed: 0.1367s/iter; left time: 3295.1932s
	iters: 200, epoch: 9 | loss: 0.6132947
	speed: 0.0364s/iter; left time: 874.6410s
Epoch: 9 cost time: 11.075438022613525
Epoch: 9, Steps: 263 | Train Loss: 0.5872586 Vali Loss: 0.2897747 Test Loss: 0.4114556
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5545620
	speed: 0.1330s/iter; left time: 3169.7566s
	iters: 200, epoch: 10 | loss: 0.4786638
	speed: 0.0183s/iter; left time: 435.0456s
Epoch: 10 cost time: 5.871667861938477
Epoch: 10, Steps: 263 | Train Loss: 0.5873665 Vali Loss: 0.2897265 Test Loss: 0.4114024
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6575317
	speed: 0.0909s/iter; left time: 2141.8861s
	iters: 200, epoch: 11 | loss: 0.5307216
	speed: 0.0191s/iter; left time: 448.8708s
Epoch: 11 cost time: 6.548092365264893
Epoch: 11, Steps: 263 | Train Loss: 0.5865274 Vali Loss: 0.2898524 Test Loss: 0.4114169
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4546597
	speed: 0.1065s/iter; left time: 2482.4956s
	iters: 200, epoch: 12 | loss: 0.3488598
	speed: 0.0188s/iter; left time: 436.9936s
Epoch: 12 cost time: 6.433026313781738
Epoch: 12, Steps: 263 | Train Loss: 0.5871345 Vali Loss: 0.2894609 Test Loss: 0.4114474
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4985332
	speed: 0.0955s/iter; left time: 2199.7504s
	iters: 200, epoch: 13 | loss: 0.3687034
	speed: 0.0323s/iter; left time: 740.6424s
Epoch: 13 cost time: 7.240512371063232
Epoch: 13, Steps: 263 | Train Loss: 0.5867778 Vali Loss: 0.2898791 Test Loss: 0.4114361
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6409989
	speed: 0.1113s/iter; left time: 2536.1076s
	iters: 200, epoch: 14 | loss: 0.4773117
	speed: 0.0182s/iter; left time: 412.0809s
Epoch: 14 cost time: 6.8462443351745605
Epoch: 14, Steps: 263 | Train Loss: 0.5867531 Vali Loss: 0.2899900 Test Loss: 0.4115210
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.6510193
	speed: 0.0926s/iter; left time: 2084.5232s
	iters: 200, epoch: 15 | loss: 0.4729833
	speed: 0.0250s/iter; left time: 559.7971s
Epoch: 15 cost time: 6.417587518692017
Epoch: 15, Steps: 263 | Train Loss: 0.5872800 Vali Loss: 0.2894517 Test Loss: 0.4113477
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5233288
	speed: 0.1129s/iter; left time: 2512.1406s
	iters: 200, epoch: 16 | loss: 0.6531503
	speed: 0.0239s/iter; left time: 529.1558s
Epoch: 16 cost time: 7.2541913986206055
Epoch: 16, Steps: 263 | Train Loss: 0.5868642 Vali Loss: 0.2897747 Test Loss: 0.4114367
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.6221004
	speed: 0.1070s/iter; left time: 2352.8223s
	iters: 200, epoch: 17 | loss: 0.5973648
	speed: 0.0241s/iter; left time: 526.6474s
Epoch: 17 cost time: 6.728041887283325
Epoch: 17, Steps: 263 | Train Loss: 0.5871250 Vali Loss: 0.2900216 Test Loss: 0.4114864
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6124564
	speed: 0.1005s/iter; left time: 2184.3171s
	iters: 200, epoch: 18 | loss: 0.6603701
	speed: 0.0288s/iter; left time: 622.0948s
Epoch: 18 cost time: 6.95298433303833
Epoch: 18, Steps: 263 | Train Loss: 0.5871864 Vali Loss: 0.2895957 Test Loss: 0.4114735
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4085844
	speed: 0.0928s/iter; left time: 1992.4148s
	iters: 200, epoch: 19 | loss: 0.8093108
	speed: 0.0236s/iter; left time: 504.9204s
Epoch: 19 cost time: 6.002596855163574
Epoch: 19, Steps: 263 | Train Loss: 0.5873167 Vali Loss: 0.2900147 Test Loss: 0.4114438
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5977464
	speed: 0.0925s/iter; left time: 1961.1361s
	iters: 200, epoch: 20 | loss: 0.6380040
	speed: 0.0229s/iter; left time: 484.2443s
Epoch: 20 cost time: 6.07187819480896
Epoch: 20, Steps: 263 | Train Loss: 0.5871766 Vali Loss: 0.2899102 Test Loss: 0.4114642
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5933732
	speed: 0.1091s/iter; left time: 2285.2936s
	iters: 200, epoch: 21 | loss: 0.5508595
	speed: 0.0204s/iter; left time: 425.7039s
Epoch: 21 cost time: 5.892416000366211
Epoch: 21, Steps: 263 | Train Loss: 0.5869859 Vali Loss: 0.2899177 Test Loss: 0.4114901
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.40884411334991455, mae:0.39847466349601746, rse:0.5139528512954712, corr:[0.54432607 0.54537445 0.5413391  0.5370306  0.5349448  0.534837
 0.53461957 0.5332529  0.531464   0.53020406 0.52977204 0.5295777
 0.5289654  0.52789754 0.52678525 0.5259622  0.5252535  0.5242487
 0.52283204 0.5212487  0.519947   0.5191339  0.518612   0.51812077
 0.5175759  0.5170969  0.5167747  0.5164443  0.5158409  0.5149346
 0.5139268  0.5131072  0.512551   0.5121225  0.51159936 0.51092535
 0.51011527 0.5093217  0.50858915 0.5079064  0.5072918  0.50683814
 0.5065701  0.50641817 0.5061966  0.5056929  0.50493556 0.5039939
 0.50299245 0.5020466  0.5011861  0.500449   0.4997667  0.4991143
 0.49860114 0.49816528 0.49768516 0.49716747 0.49668765 0.49639055
 0.49630633 0.49631447 0.49631548 0.4962366  0.49605766 0.49588507
 0.4957641  0.49570125 0.49570537 0.49573034 0.49576324 0.49582803
 0.49589178 0.49591804 0.49579832 0.49556187 0.49532557 0.49511874
 0.4949897  0.49488276 0.4947248  0.4944581  0.49410623 0.49371645
 0.49336645 0.49307346 0.492808   0.49254766 0.49221525 0.49181208
 0.49136972 0.4908269  0.49012974 0.4891669  0.48776522 0.4858273
 0.48352897 0.48125902 0.47923067 0.47740704 0.47564903 0.47392124
 0.47225633 0.47076356 0.46956638 0.4685665  0.4675375  0.46629882
 0.4648608  0.4633883  0.4620305  0.46070048 0.4593286  0.4578609
 0.4563678  0.45499685 0.4537948  0.4526941  0.4516166  0.45046088
 0.44933987 0.44833487 0.44737816 0.44634795 0.44519895 0.44400766
 0.44293702 0.44203582 0.44113177 0.4401224  0.4390296  0.4379368
 0.43695822 0.4361774  0.4355691  0.43515524 0.4347167  0.43422902
 0.4336799  0.433154   0.43270984 0.43227375 0.43178737 0.43118227
 0.43043962 0.4296368  0.42877156 0.42794013 0.42707825 0.42619678
 0.4253307  0.424741   0.4243632  0.4239777  0.42359293 0.42321184
 0.42288113 0.42272732 0.42268312 0.42277405 0.42284766 0.4227555
 0.4225813  0.4222456  0.4220162  0.42192417 0.42195463 0.4220962
 0.42232937 0.42258856 0.42277464 0.4228117  0.42273584 0.42258897
 0.42241675 0.4222831  0.42218992 0.42213035 0.42206573 0.42196617
 0.42178687 0.42150187 0.42122924 0.42097926 0.42081904 0.42068744
 0.42046323 0.41999155 0.41915748 0.41793254 0.4163875  0.41453886
 0.41243997 0.41026127 0.40805274 0.40595153 0.40419334 0.4027859
 0.40153557 0.40012825 0.39858058 0.39708313 0.39578533 0.39465567
 0.39352074 0.392257   0.39073253 0.38900828 0.38730747 0.3858926
 0.38473073 0.38366848 0.38248676 0.38113424 0.37969542 0.37828267
 0.3769988  0.37582934 0.3748077  0.37377048 0.37261596 0.37137446
 0.3701115  0.3687903  0.3676538  0.36659232 0.3656886  0.36489418
 0.3641224  0.36324486 0.36212313 0.36094084 0.35994455 0.3593839
 0.3591833  0.35907498 0.35882902 0.358348   0.35784236 0.35747504
 0.3573006  0.3571659  0.35677218 0.35633785 0.35593036 0.35566154
 0.3557333  0.35606924 0.35638666 0.3563456  0.35606328 0.3558618
 0.35592535 0.35629067 0.35678905 0.3571521  0.35730076 0.3573314
 0.3573466  0.3573853  0.3573562  0.35737815 0.3575745  0.3580444
 0.35878947 0.35951778 0.36004412 0.3601957  0.3601506  0.36012405
 0.36024573 0.360469   0.36061567 0.36063853 0.36066124 0.3608657
 0.36134923 0.36196753 0.36247012 0.36274678 0.36275578 0.3626723
 0.36260107 0.3624912  0.36216712 0.36148143 0.360423   0.35905638
 0.35754985 0.35614935 0.35496506 0.3541279  0.3536637  0.3536637
 0.35408577 0.3545333  0.3546925  0.35448596 0.35398105 0.35330042
 0.35259655 0.3518831  0.35104457 0.34997714 0.3487845  0.34768763
 0.34693718 0.346557   0.3462991  0.34583676 0.34495524 0.34377444
 0.3425518  0.34165022 0.34111118 0.34072062 0.34026104 0.33966136
 0.33895358 0.3382638  0.3377062  0.3372788  0.3368896  0.33643943
 0.3358634  0.33524162 0.3347144  0.33430427 0.3340757  0.33398753
 0.33399037 0.33399317 0.33399948 0.33398765 0.3340001  0.33404186
 0.33394852 0.333572   0.33301967 0.3326426  0.3326459  0.3330087
 0.3335211  0.33392075 0.33395883 0.3336788  0.3334052  0.3334959
 0.33405888 0.33479953 0.3353807  0.33561435 0.33560345 0.3355754
 0.3357701  0.33630198 0.33689526 0.3373353  0.33750117 0.3374965
 0.33748916 0.33766526 0.3380569  0.33854982 0.33897847 0.3392945
 0.33947653 0.33962744 0.33987135 0.34019688 0.34058827 0.34100026
 0.34137896 0.34165812 0.3418037  0.34186894 0.34191418 0.3419794
 0.34198874 0.34176204 0.3411755  0.34026608 0.33920383 0.3381487
 0.3371642  0.33618593 0.33508372 0.33380896 0.33248973 0.33140776
 0.33068192 0.3302312  0.32992423 0.3295765  0.3291118  0.32849282
 0.32767215 0.32666865 0.3254943  0.32423976 0.32302368 0.3218881
 0.32077208 0.31955397 0.3182209  0.31693715 0.31578043 0.3148804
 0.3141914  0.31354392 0.3127607  0.31177327 0.31069973 0.30969882
 0.30888167 0.30816907 0.30741978 0.3066276  0.30576643 0.30499125
 0.3044289  0.3039972  0.30356085 0.303003   0.30231503 0.30164626
 0.30117363 0.30104798 0.30117458 0.30136892 0.30133954 0.30096382
 0.30027884 0.29942858 0.29860044 0.29794025 0.2972954  0.29665604
 0.29608336 0.295849   0.2959801  0.2962684  0.296402   0.29624665
 0.2958966  0.29567578 0.2958419  0.29631454 0.2968173  0.2970532
 0.29701114 0.2968485  0.2967835  0.2968735  0.2970072  0.2971117
 0.29713148 0.2970779  0.29697967 0.2968817  0.2968649  0.2968619
 0.296905   0.2969992  0.29708868 0.2970377  0.29677874 0.29641762
 0.29620108 0.29627925 0.2966244  0.2969981  0.29712334 0.29683414
 0.2961446  0.29511687 0.29384157 0.29233682 0.2906374  0.2887622
 0.28680277 0.28492993 0.28319824 0.28163248 0.2802286  0.279053
 0.27816516 0.27760363 0.27740708 0.27746207 0.27751446 0.27731627
 0.27665937 0.2755772  0.27421072 0.27277848 0.27143645 0.2701835
 0.26897475 0.26771814 0.26642105 0.26519725 0.26414263 0.26328734
 0.26250315 0.2616776  0.26076785 0.25976166 0.25875494 0.2577707
 0.25680447 0.25587958 0.25509337 0.25454235 0.2540942  0.25361603
 0.2530437  0.25227585 0.2513967  0.2505856  0.24996094 0.24950014
 0.24902618 0.24849299 0.24790543 0.24742801 0.2472265  0.24723367
 0.24722926 0.24687286 0.24613708 0.24518335 0.24428907 0.24375883
 0.24365312 0.24377622 0.2438895  0.24384491 0.24382007 0.24412282
 0.24488643 0.24587396 0.24666025 0.2469571  0.24667293 0.24610782
 0.24568988 0.24567805 0.24594308 0.246163   0.2460566  0.24587856
 0.24586593 0.24619064 0.24674974 0.24730092 0.2475729  0.2476576
 0.24775745 0.24809533 0.24859585 0.24904375 0.24929321 0.24938445
 0.24942544 0.24953373 0.2497232  0.24986337 0.24993093 0.24992535
 0.2498262  0.24944009 0.24854472 0.24705444 0.24507008 0.24285705
 0.24077103 0.23907739 0.23777618 0.23681971 0.23619258 0.23590429
 0.23587674 0.23608693 0.23649056 0.23696202 0.23731358 0.23728181
 0.2367099  0.23561634 0.23424758 0.23291208 0.23179978 0.23086272
 0.22988544 0.22865905 0.2272038  0.22575897 0.22463608 0.22402133
 0.22375278 0.22347036 0.22284883 0.221835   0.22078997 0.21992715
 0.2191757  0.21832727 0.21723196 0.21594533 0.21480072 0.21407375
 0.21382838 0.21371873 0.21343234 0.21280445 0.21210828 0.2115339
 0.21113145 0.21082778 0.21041463 0.20985569 0.20933183 0.20884292
 0.2084313  0.20810181 0.207921   0.20788378 0.20797059 0.2079894
 0.20798235 0.20801891 0.20806886 0.20815092 0.20834701 0.20856266
 0.20870191 0.20867664 0.20849814 0.20828713 0.20820901 0.20824522
 0.20838793 0.20863324 0.20898408 0.20931795 0.20972997 0.21019633
 0.21065871 0.21100606 0.21119522 0.2113094  0.21145135 0.21165904
 0.21193674 0.2122102  0.21243028 0.2127886  0.21348494 0.21454586
 0.2157433  0.21667849 0.21709438 0.21704131 0.21685255 0.2167766
 0.21675931 0.21644838 0.2155191  0.2139679  0.21204585 0.2100517
 0.20818453 0.20647952 0.2049114  0.20359506 0.20280059 0.20263737
 0.20288533 0.20319763 0.20340368 0.20356959 0.20379926 0.20394483
 0.20366369 0.20273787 0.20132923 0.1998966  0.19884695 0.198165
 0.19747934 0.1963919  0.19488321 0.19338451 0.19234186 0.1917401
 0.19113958 0.19007714 0.18843675 0.1865867  0.18512018 0.18406904
 0.18300554 0.18146203 0.17953494 0.17789224 0.17692485 0.17657751
 0.17603768 0.17471704 0.17274274 0.17121932 0.17057218 0.17032672
 0.1693751  0.16730958 0.16537365 0.16559345 0.16764678 0.16751589]
