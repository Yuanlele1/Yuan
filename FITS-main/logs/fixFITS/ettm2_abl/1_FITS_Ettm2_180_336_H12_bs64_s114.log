Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_180_336_FITS_ETTm2_ftM_sl180_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34045
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=34, out_features=97, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2955008.0
params:  3395.0
Trainable parameters:  3395
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4097172
	speed: 0.0454s/iter; left time: 1197.8837s
	iters: 200, epoch: 1 | loss: 0.4763072
	speed: 0.0191s/iter; left time: 502.5831s
Epoch: 1 cost time: 7.991589307785034
Epoch: 1, Steps: 265 | Train Loss: 0.4965493 Vali Loss: 0.2260220 Test Loss: 0.3090023
Validation loss decreased (inf --> 0.226022).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5007074
	speed: 0.1215s/iter; left time: 3174.6187s
	iters: 200, epoch: 2 | loss: 0.5479184
	speed: 0.0272s/iter; left time: 708.7616s
Epoch: 2 cost time: 8.79195523262024
Epoch: 2, Steps: 265 | Train Loss: 0.4410494 Vali Loss: 0.2159334 Test Loss: 0.2974146
Validation loss decreased (0.226022 --> 0.215933).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4206363
	speed: 0.1664s/iter; left time: 4303.6948s
	iters: 200, epoch: 3 | loss: 0.4187434
	speed: 0.0216s/iter; left time: 557.9138s
Epoch: 3 cost time: 7.529923677444458
Epoch: 3, Steps: 265 | Train Loss: 0.4316540 Vali Loss: 0.2126385 Test Loss: 0.2934196
Validation loss decreased (0.215933 --> 0.212638).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3679448
	speed: 0.1421s/iter; left time: 3638.8050s
	iters: 200, epoch: 4 | loss: 0.3459796
	speed: 0.0515s/iter; left time: 1313.0557s
Epoch: 4 cost time: 14.416438102722168
Epoch: 4, Steps: 265 | Train Loss: 0.4277396 Vali Loss: 0.2105819 Test Loss: 0.2912052
Validation loss decreased (0.212638 --> 0.210582).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3996265
	speed: 0.1997s/iter; left time: 5060.0821s
	iters: 200, epoch: 5 | loss: 0.4450577
	speed: 0.0227s/iter; left time: 572.4736s
Epoch: 5 cost time: 9.644293069839478
Epoch: 5, Steps: 265 | Train Loss: 0.4252909 Vali Loss: 0.2097623 Test Loss: 0.2897455
Validation loss decreased (0.210582 --> 0.209762).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3886424
	speed: 0.2120s/iter; left time: 5316.4125s
	iters: 200, epoch: 6 | loss: 0.4525937
	speed: 0.0463s/iter; left time: 1157.3138s
Epoch: 6 cost time: 12.216606378555298
Epoch: 6, Steps: 265 | Train Loss: 0.4233618 Vali Loss: 0.2092168 Test Loss: 0.2887282
Validation loss decreased (0.209762 --> 0.209217).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4225799
	speed: 0.1258s/iter; left time: 3122.1627s
	iters: 200, epoch: 7 | loss: 0.2083648
	speed: 0.0222s/iter; left time: 549.1306s
Epoch: 7 cost time: 6.601582765579224
Epoch: 7, Steps: 265 | Train Loss: 0.4221296 Vali Loss: 0.2087504 Test Loss: 0.2880327
Validation loss decreased (0.209217 --> 0.208750).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4450131
	speed: 0.1081s/iter; left time: 2654.4056s
	iters: 200, epoch: 8 | loss: 0.6961335
	speed: 0.0287s/iter; left time: 702.3387s
Epoch: 8 cost time: 6.907013654708862
Epoch: 8, Steps: 265 | Train Loss: 0.4209534 Vali Loss: 0.2084407 Test Loss: 0.2874682
Validation loss decreased (0.208750 --> 0.208441).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3961387
	speed: 0.1477s/iter; left time: 3585.5175s
	iters: 200, epoch: 9 | loss: 0.3182326
	speed: 0.0349s/iter; left time: 844.2685s
Epoch: 9 cost time: 9.214534044265747
Epoch: 9, Steps: 265 | Train Loss: 0.4205307 Vali Loss: 0.2080761 Test Loss: 0.2870706
Validation loss decreased (0.208441 --> 0.208076).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2464857
	speed: 0.1575s/iter; left time: 3782.1232s
	iters: 200, epoch: 10 | loss: 0.4443813
	speed: 0.0253s/iter; left time: 604.7794s
Epoch: 10 cost time: 7.044840097427368
Epoch: 10, Steps: 265 | Train Loss: 0.4195347 Vali Loss: 0.2079146 Test Loss: 0.2867074
Validation loss decreased (0.208076 --> 0.207915).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3659508
	speed: 0.1043s/iter; left time: 2476.1939s
	iters: 200, epoch: 11 | loss: 0.4060145
	speed: 0.0203s/iter; left time: 479.8651s
Epoch: 11 cost time: 6.272011756896973
Epoch: 11, Steps: 265 | Train Loss: 0.4188013 Vali Loss: 0.2077848 Test Loss: 0.2864983
Validation loss decreased (0.207915 --> 0.207785).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5290475
	speed: 0.1441s/iter; left time: 3383.2072s
	iters: 200, epoch: 12 | loss: 0.3075270
	speed: 0.0255s/iter; left time: 595.3161s
Epoch: 12 cost time: 7.894794940948486
Epoch: 12, Steps: 265 | Train Loss: 0.4181247 Vali Loss: 0.2076958 Test Loss: 0.2861594
Validation loss decreased (0.207785 --> 0.207696).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2719447
	speed: 0.1227s/iter; left time: 2849.2331s
	iters: 200, epoch: 13 | loss: 0.3818792
	speed: 0.0262s/iter; left time: 605.1513s
Epoch: 13 cost time: 8.086855411529541
Epoch: 13, Steps: 265 | Train Loss: 0.4185940 Vali Loss: 0.2075416 Test Loss: 0.2859818
Validation loss decreased (0.207696 --> 0.207542).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3911772
	speed: 0.1187s/iter; left time: 2724.0102s
	iters: 200, epoch: 14 | loss: 0.2884368
	speed: 0.0224s/iter; left time: 512.3703s
Epoch: 14 cost time: 5.990569591522217
Epoch: 14, Steps: 265 | Train Loss: 0.4179142 Vali Loss: 0.2075183 Test Loss: 0.2858496
Validation loss decreased (0.207542 --> 0.207518).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5054560
	speed: 0.1075s/iter; left time: 2438.7063s
	iters: 200, epoch: 15 | loss: 0.3206444
	speed: 0.0280s/iter; left time: 633.2402s
Epoch: 15 cost time: 7.030603408813477
Epoch: 15, Steps: 265 | Train Loss: 0.4183313 Vali Loss: 0.2073790 Test Loss: 0.2857156
Validation loss decreased (0.207518 --> 0.207379).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5078564
	speed: 0.1355s/iter; left time: 3038.7523s
	iters: 200, epoch: 16 | loss: 0.3661240
	speed: 0.0207s/iter; left time: 461.8540s
Epoch: 16 cost time: 6.630721569061279
Epoch: 16, Steps: 265 | Train Loss: 0.4176363 Vali Loss: 0.2074798 Test Loss: 0.2856578
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4262148
	speed: 0.1548s/iter; left time: 3430.3620s
	iters: 200, epoch: 17 | loss: 0.4221264
	speed: 0.0269s/iter; left time: 594.3325s
Epoch: 17 cost time: 9.375644207000732
Epoch: 17, Steps: 265 | Train Loss: 0.4172044 Vali Loss: 0.2073925 Test Loss: 0.2855340
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5664021
	speed: 0.1956s/iter; left time: 4282.2742s
	iters: 200, epoch: 18 | loss: 0.4957262
	speed: 0.0301s/iter; left time: 656.9240s
Epoch: 18 cost time: 9.157122135162354
Epoch: 18, Steps: 265 | Train Loss: 0.4175032 Vali Loss: 0.2070642 Test Loss: 0.2853760
Validation loss decreased (0.207379 --> 0.207064).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3431021
	speed: 0.1357s/iter; left time: 2935.0092s
	iters: 200, epoch: 19 | loss: 0.5014654
	speed: 0.0350s/iter; left time: 754.1915s
Epoch: 19 cost time: 8.412713050842285
Epoch: 19, Steps: 265 | Train Loss: 0.4176705 Vali Loss: 0.2072813 Test Loss: 0.2853319
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3877409
	speed: 0.1373s/iter; left time: 2932.6878s
	iters: 200, epoch: 20 | loss: 0.6039042
	speed: 0.0254s/iter; left time: 539.3959s
Epoch: 20 cost time: 8.607359886169434
Epoch: 20, Steps: 265 | Train Loss: 0.4175323 Vali Loss: 0.2070364 Test Loss: 0.2852574
Validation loss decreased (0.207064 --> 0.207036).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4117520
	speed: 0.1597s/iter; left time: 3370.7099s
	iters: 200, epoch: 21 | loss: 0.5970668
	speed: 0.0379s/iter; left time: 796.1286s
Epoch: 21 cost time: 11.01151990890503
Epoch: 21, Steps: 265 | Train Loss: 0.4172400 Vali Loss: 0.2069220 Test Loss: 0.2852755
Validation loss decreased (0.207036 --> 0.206922).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4735943
	speed: 0.1668s/iter; left time: 3475.7538s
	iters: 200, epoch: 22 | loss: 0.5731028
	speed: 0.0336s/iter; left time: 697.2091s
Epoch: 22 cost time: 7.843900203704834
Epoch: 22, Steps: 265 | Train Loss: 0.4168972 Vali Loss: 0.2073173 Test Loss: 0.2852107
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3418977
	speed: 0.1232s/iter; left time: 2535.0241s
	iters: 200, epoch: 23 | loss: 0.4088240
	speed: 0.0246s/iter; left time: 502.8774s
Epoch: 23 cost time: 6.227795124053955
Epoch: 23, Steps: 265 | Train Loss: 0.4165979 Vali Loss: 0.2072326 Test Loss: 0.2851652
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2773909
	speed: 0.1175s/iter; left time: 2385.4733s
	iters: 200, epoch: 24 | loss: 0.4540658
	speed: 0.0183s/iter; left time: 370.3942s
Epoch: 24 cost time: 6.385376691818237
Epoch: 24, Steps: 265 | Train Loss: 0.4166091 Vali Loss: 0.2071878 Test Loss: 0.2851021
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3509963
	speed: 0.1048s/iter; left time: 2100.4835s
	iters: 200, epoch: 25 | loss: 0.4982739
	speed: 0.0214s/iter; left time: 425.8101s
Epoch: 25 cost time: 7.030033588409424
Epoch: 25, Steps: 265 | Train Loss: 0.4168529 Vali Loss: 0.2070431 Test Loss: 0.2850688
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3707227
	speed: 0.1382s/iter; left time: 2732.6932s
	iters: 200, epoch: 26 | loss: 0.2944145
	speed: 0.0198s/iter; left time: 388.7974s
Epoch: 26 cost time: 8.436148643493652
Epoch: 26, Steps: 265 | Train Loss: 0.4168624 Vali Loss: 0.2071089 Test Loss: 0.2850593
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4363125
	speed: 0.1236s/iter; left time: 2410.9559s
	iters: 200, epoch: 27 | loss: 0.5173829
	speed: 0.0255s/iter; left time: 494.1196s
Epoch: 27 cost time: 6.687022686004639
Epoch: 27, Steps: 265 | Train Loss: 0.4165160 Vali Loss: 0.2071221 Test Loss: 0.2850012
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5559240
	speed: 0.1344s/iter; left time: 2587.3083s
	iters: 200, epoch: 28 | loss: 0.2842181
	speed: 0.0238s/iter; left time: 454.8067s
Epoch: 28 cost time: 7.941579818725586
Epoch: 28, Steps: 265 | Train Loss: 0.4167827 Vali Loss: 0.2072102 Test Loss: 0.2850268
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3809478
	speed: 0.1454s/iter; left time: 2759.7068s
	iters: 200, epoch: 29 | loss: 0.4464193
	speed: 0.0247s/iter; left time: 467.1237s
Epoch: 29 cost time: 8.371323108673096
Epoch: 29, Steps: 265 | Train Loss: 0.4163310 Vali Loss: 0.2070710 Test Loss: 0.2849897
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4313837
	speed: 0.1265s/iter; left time: 2367.0560s
	iters: 200, epoch: 30 | loss: 0.4485638
	speed: 0.0292s/iter; left time: 543.9664s
Epoch: 30 cost time: 7.227397680282593
Epoch: 30, Steps: 265 | Train Loss: 0.4163037 Vali Loss: 0.2071635 Test Loss: 0.2849636
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4123577
	speed: 0.1163s/iter; left time: 2146.5590s
	iters: 200, epoch: 31 | loss: 0.4850780
	speed: 0.0233s/iter; left time: 427.0192s
Epoch: 31 cost time: 6.999779224395752
Epoch: 31, Steps: 265 | Train Loss: 0.4169505 Vali Loss: 0.2071814 Test Loss: 0.2849577
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3655221
	speed: 0.1463s/iter; left time: 2661.3282s
	iters: 200, epoch: 32 | loss: 0.5006697
	speed: 0.0296s/iter; left time: 534.8463s
Epoch: 32 cost time: 8.871509075164795
Epoch: 32, Steps: 265 | Train Loss: 0.4162512 Vali Loss: 0.2070092 Test Loss: 0.2849371
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4756944
	speed: 0.1681s/iter; left time: 3012.1197s
	iters: 200, epoch: 33 | loss: 0.5602169
	speed: 0.0552s/iter; left time: 983.7928s
Epoch: 33 cost time: 13.497702836990356
Epoch: 33, Steps: 265 | Train Loss: 0.4162152 Vali Loss: 0.2071063 Test Loss: 0.2849276
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2893943
	speed: 0.1423s/iter; left time: 2511.6595s
	iters: 200, epoch: 34 | loss: 0.3965274
	speed: 0.0489s/iter; left time: 858.0467s
Epoch: 34 cost time: 12.735675811767578
Epoch: 34, Steps: 265 | Train Loss: 0.4164863 Vali Loss: 0.2069864 Test Loss: 0.2849130
EarlyStopping counter: 13 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4371137
	speed: 0.1541s/iter; left time: 2679.9583s
	iters: 200, epoch: 35 | loss: 0.5199068
	speed: 0.0330s/iter; left time: 570.1140s
Epoch: 35 cost time: 9.455679416656494
Epoch: 35, Steps: 265 | Train Loss: 0.4167380 Vali Loss: 0.2071563 Test Loss: 0.2848968
EarlyStopping counter: 14 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2759139
	speed: 0.1482s/iter; left time: 2538.8429s
	iters: 200, epoch: 36 | loss: 0.4553345
	speed: 0.0299s/iter; left time: 509.6588s
Epoch: 36 cost time: 8.897042989730835
Epoch: 36, Steps: 265 | Train Loss: 0.4160582 Vali Loss: 0.2069972 Test Loss: 0.2848902
EarlyStopping counter: 15 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.5205392
	speed: 0.1740s/iter; left time: 2934.5660s
	iters: 200, epoch: 37 | loss: 0.4168949
	speed: 0.0236s/iter; left time: 395.3334s
Epoch: 37 cost time: 10.412076473236084
Epoch: 37, Steps: 265 | Train Loss: 0.4168125 Vali Loss: 0.2072250 Test Loss: 0.2848808
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2971290
	speed: 0.1580s/iter; left time: 2621.4141s
	iters: 200, epoch: 38 | loss: 0.3425803
	speed: 0.0405s/iter; left time: 667.4407s
Epoch: 38 cost time: 9.11002516746521
Epoch: 38, Steps: 265 | Train Loss: 0.4158513 Vali Loss: 0.2071582 Test Loss: 0.2848684
EarlyStopping counter: 17 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4987486
	speed: 0.1498s/iter; left time: 2447.1062s
	iters: 200, epoch: 39 | loss: 0.6211959
	speed: 0.0251s/iter; left time: 408.1825s
Epoch: 39 cost time: 7.630844593048096
Epoch: 39, Steps: 265 | Train Loss: 0.4166625 Vali Loss: 0.2071246 Test Loss: 0.2848609
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4033796
	speed: 0.1353s/iter; left time: 2173.7956s
	iters: 200, epoch: 40 | loss: 0.3478040
	speed: 0.0258s/iter; left time: 412.2699s
Epoch: 40 cost time: 8.2351713180542
Epoch: 40, Steps: 265 | Train Loss: 0.4162038 Vali Loss: 0.2071093 Test Loss: 0.2848613
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4968332
	speed: 0.1086s/iter; left time: 1715.6416s
	iters: 200, epoch: 41 | loss: 0.3867603
	speed: 0.0363s/iter; left time: 570.0520s
Epoch: 41 cost time: 8.1212317943573
Epoch: 41, Steps: 265 | Train Loss: 0.4162415 Vali Loss: 0.2071150 Test Loss: 0.2848404
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_180_336_FITS_ETTm2_ftM_sl180_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.28652888536453247, mae:0.3317108750343323, rse:0.43235936760902405, corr:[0.55860525 0.56207865 0.5559141  0.55362326 0.55472076 0.5546172
 0.55238533 0.5504514  0.5501966  0.5504453  0.5497405  0.54833645
 0.5475597  0.5478417  0.5481863  0.54757375 0.5461607  0.5450211
 0.5447802  0.5449274  0.5446304  0.54374355 0.5428591  0.5424969
 0.54245484 0.5422277  0.5416667  0.54102755 0.54056066 0.540157
 0.53958184 0.53878653 0.53798133 0.5374714  0.5372111  0.5369058
 0.5362613  0.53541005 0.53461534 0.5340171  0.53357035 0.5331635
 0.5327625  0.53243905 0.53218263 0.5318445  0.5312165  0.53026825
 0.5292142  0.52835846 0.5277718  0.5273388  0.52680874 0.5261145
 0.52546424 0.5250273  0.5246544  0.5242671  0.5237885  0.5233274
 0.52294636 0.5227368  0.5226223  0.5224123  0.5221332  0.52189904
 0.52172524 0.52159417 0.5214623  0.5213459  0.52119714 0.52102375
 0.5208362  0.52058125 0.5203268  0.52005816 0.5197862  0.5194419
 0.5190428  0.51861745 0.5181745  0.5177257  0.5172979  0.51687735
 0.5164774  0.51608926 0.5156835  0.515259   0.5148045  0.5143599
 0.51398927 0.5136419  0.51314735 0.5123557  0.51121306 0.5097126
 0.50793046 0.50610024 0.5043168  0.5026792  0.5012658  0.5000617
 0.49891093 0.497655   0.49625346 0.4948535  0.49363616 0.49262455
 0.49165976 0.49059233 0.4896114  0.48874024 0.4879297  0.48702106
 0.4859088  0.48474002 0.48366582 0.48272997 0.48190174 0.4810335
 0.48011038 0.47916248 0.47823587 0.477336   0.4764793  0.4756399
 0.47475502 0.4738042  0.472838   0.47193563 0.4711431  0.47035342
 0.46950847 0.46858233 0.4677853  0.4672581  0.46686608 0.4664461
 0.4658512  0.4651278  0.46443516 0.46389788 0.46342686 0.4627788
 0.46188655 0.46088564 0.45998532 0.45933464 0.45881832 0.45818228
 0.4574558  0.45686424 0.4563457  0.45584315 0.45531136 0.4547659
 0.4543113  0.45400828 0.45370138 0.45333812 0.45294797 0.4525947
 0.45238733 0.45219624 0.45208362 0.451987   0.45192125 0.45199776
 0.45217913 0.4523629  0.4524047  0.45231685 0.45227164 0.45227438
 0.45231533 0.45225486 0.45208293 0.45187396 0.4517673  0.45177272
 0.4516697  0.45131174 0.4508249  0.4503814  0.45014778 0.45004353
 0.4499451  0.4497359  0.4493328  0.44868687 0.4477228  0.44637793
 0.44473243 0.44307274 0.4415807  0.4403177  0.439203   0.43800366
 0.4366419  0.4352782  0.4340882  0.4331536  0.43233314 0.4316048
 0.43095997 0.43043414 0.4299434  0.42937446 0.4286593  0.42786598
 0.4271649  0.4266331  0.4261604  0.4255569  0.4247606  0.42387757
 0.42308933 0.42231607 0.42159975 0.42078102 0.41980624 0.41886768
 0.4179962  0.41721046 0.41654265 0.41581607 0.415118   0.4144433
 0.41364723 0.41271877 0.41171804 0.41086802 0.41021585 0.40977642
 0.40939188 0.40908417 0.40892035 0.40871143 0.4084151  0.40792805
 0.40730938 0.4066935  0.40623996 0.40602896 0.40586329 0.4056353
 0.40551093 0.40563777 0.4059878  0.4062586  0.40632528 0.40622124
 0.40612388 0.4063413  0.40671915 0.406935   0.40689197 0.40681213
 0.40686    0.40696204 0.4069776  0.40693787 0.40691367 0.40697935
 0.40716118 0.4073177  0.40734842 0.40725994 0.40726867 0.4074357
 0.40751377 0.40739244 0.40702426 0.40670985 0.40659475 0.40662196
 0.40658724 0.4063726  0.40618122 0.4061792  0.40625852 0.40640306
 0.4064909  0.40639618 0.40617958 0.4058378  0.40522432 0.40420672
 0.4028781  0.401739   0.4008295  0.4001084  0.3993705  0.39858118
 0.39797598 0.39749184 0.3969813  0.3963216  0.39564577 0.39505374
 0.39470255 0.39434746 0.39383358 0.39305705 0.39215022 0.39138892
 0.39079872 0.3901339  0.38925803 0.3883361  0.38762328 0.3870853
 0.38632616 0.38538447 0.38447997 0.3839226  0.38336977 0.3823835
 0.38087463 0.3794055  0.37861723 0.37838358 0.3778586  0.37668476
 0.3754462  0.37488994 0.37491393 0.3743759  0.37289622 0.3716697
 0.37221447 0.37346232 0.37308228 0.37101626 0.37038052 0.37372062]
