Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=20, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=20, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3225600.0
params:  3780.0
Trainable parameters:  3780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6927708
	speed: 0.0201s/iter; left time: 526.8139s
	iters: 200, epoch: 1 | loss: 0.6975211
	speed: 0.0156s/iter; left time: 406.0230s
Epoch: 1 cost time: 4.696747779846191
Epoch: 1, Steps: 263 | Train Loss: 0.7043746 Vali Loss: 0.3430115 Test Loss: 0.4767075
Validation loss decreased (inf --> 0.343011).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4614861
	speed: 0.0729s/iter; left time: 1889.7282s
	iters: 200, epoch: 2 | loss: 0.5309034
	speed: 0.0138s/iter; left time: 356.1614s
Epoch: 2 cost time: 4.255573749542236
Epoch: 2, Steps: 263 | Train Loss: 0.5600582 Vali Loss: 0.3002094 Test Loss: 0.4248593
Validation loss decreased (0.343011 --> 0.300209).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4310079
	speed: 0.0715s/iter; left time: 1834.9047s
	iters: 200, epoch: 3 | loss: 0.4776468
	speed: 0.0165s/iter; left time: 422.6400s
Epoch: 3 cost time: 4.707716941833496
Epoch: 3, Steps: 263 | Train Loss: 0.5329140 Vali Loss: 0.2917086 Test Loss: 0.4150321
Validation loss decreased (0.300209 --> 0.291709).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3312365
	speed: 0.0713s/iter; left time: 1811.2476s
	iters: 200, epoch: 4 | loss: 0.5674400
	speed: 0.0166s/iter; left time: 421.0599s
Epoch: 4 cost time: 4.707204341888428
Epoch: 4, Steps: 263 | Train Loss: 0.5276169 Vali Loss: 0.2899281 Test Loss: 0.4127328
Validation loss decreased (0.291709 --> 0.289928).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5924192
	speed: 0.0729s/iter; left time: 1832.1546s
	iters: 200, epoch: 5 | loss: 0.4905359
	speed: 0.0136s/iter; left time: 341.8862s
Epoch: 5 cost time: 4.208493709564209
Epoch: 5, Steps: 263 | Train Loss: 0.5266821 Vali Loss: 0.2890781 Test Loss: 0.4118904
Validation loss decreased (0.289928 --> 0.289078).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5459551
	speed: 0.0716s/iter; left time: 1782.6174s
	iters: 200, epoch: 6 | loss: 0.5132537
	speed: 0.0142s/iter; left time: 352.7422s
Epoch: 6 cost time: 4.320914030075073
Epoch: 6, Steps: 263 | Train Loss: 0.5256045 Vali Loss: 0.2890492 Test Loss: 0.4113508
Validation loss decreased (0.289078 --> 0.289049).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6370295
	speed: 0.0705s/iter; left time: 1736.7289s
	iters: 200, epoch: 7 | loss: 0.5949939
	speed: 0.0139s/iter; left time: 339.8681s
Epoch: 7 cost time: 4.190305233001709
Epoch: 7, Steps: 263 | Train Loss: 0.5249307 Vali Loss: 0.2890275 Test Loss: 0.4112952
Validation loss decreased (0.289049 --> 0.289027).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.7655146
	speed: 0.0692s/iter; left time: 1686.0331s
	iters: 200, epoch: 8 | loss: 0.5756579
	speed: 0.0139s/iter; left time: 337.0046s
Epoch: 8 cost time: 4.343705415725708
Epoch: 8, Steps: 263 | Train Loss: 0.5240623 Vali Loss: 0.2887555 Test Loss: 0.4111423
Validation loss decreased (0.289027 --> 0.288755).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5345229
	speed: 0.0724s/iter; left time: 1744.0175s
	iters: 200, epoch: 9 | loss: 0.4856506
	speed: 0.0140s/iter; left time: 336.1691s
Epoch: 9 cost time: 4.298993110656738
Epoch: 9, Steps: 263 | Train Loss: 0.5244667 Vali Loss: 0.2891310 Test Loss: 0.4112409
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5676516
	speed: 0.0714s/iter; left time: 1700.7460s
	iters: 200, epoch: 10 | loss: 0.6182646
	speed: 0.0145s/iter; left time: 344.9458s
Epoch: 10 cost time: 4.464986562728882
Epoch: 10, Steps: 263 | Train Loss: 0.5240524 Vali Loss: 0.2890196 Test Loss: 0.4111852
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4729236
	speed: 0.0731s/iter; left time: 1722.1862s
	iters: 200, epoch: 11 | loss: 0.4520793
	speed: 0.0146s/iter; left time: 342.7722s
Epoch: 11 cost time: 4.424698114395142
Epoch: 11, Steps: 263 | Train Loss: 0.5241234 Vali Loss: 0.2894455 Test Loss: 0.4112307
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5634854
	speed: 0.0706s/iter; left time: 1646.1746s
	iters: 200, epoch: 12 | loss: 0.4387469
	speed: 0.0152s/iter; left time: 352.1233s
Epoch: 12 cost time: 4.331105470657349
Epoch: 12, Steps: 263 | Train Loss: 0.5230811 Vali Loss: 0.2892296 Test Loss: 0.4112404
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4876484
	speed: 0.0711s/iter; left time: 1639.4686s
	iters: 200, epoch: 13 | loss: 0.6454105
	speed: 0.0135s/iter; left time: 308.9587s
Epoch: 13 cost time: 4.209493398666382
Epoch: 13, Steps: 263 | Train Loss: 0.5240106 Vali Loss: 0.2894332 Test Loss: 0.4112661
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3685984
	speed: 0.0710s/iter; left time: 1617.2735s
	iters: 200, epoch: 14 | loss: 0.5588484
	speed: 0.0147s/iter; left time: 332.8370s
Epoch: 14 cost time: 4.429145574569702
Epoch: 14, Steps: 263 | Train Loss: 0.5237483 Vali Loss: 0.2895428 Test Loss: 0.4113649
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5069758
	speed: 0.0716s/iter; left time: 1612.3264s
	iters: 200, epoch: 15 | loss: 0.3739938
	speed: 0.0147s/iter; left time: 329.9326s
Epoch: 15 cost time: 4.49160099029541
Epoch: 15, Steps: 263 | Train Loss: 0.5235594 Vali Loss: 0.2897789 Test Loss: 0.4112659
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5734246
	speed: 0.0750s/iter; left time: 1670.0427s
	iters: 200, epoch: 16 | loss: 0.5104193
	speed: 0.0167s/iter; left time: 369.0885s
Epoch: 16 cost time: 4.833079099655151
Epoch: 16, Steps: 263 | Train Loss: 0.5225903 Vali Loss: 0.2896207 Test Loss: 0.4112809
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.6449007
	speed: 0.0728s/iter; left time: 1601.4888s
	iters: 200, epoch: 17 | loss: 0.4008572
	speed: 0.0139s/iter; left time: 303.5072s
Epoch: 17 cost time: 4.433291435241699
Epoch: 17, Steps: 263 | Train Loss: 0.5230473 Vali Loss: 0.2896831 Test Loss: 0.4112718
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3935342
	speed: 0.0729s/iter; left time: 1583.6865s
	iters: 200, epoch: 18 | loss: 0.5590248
	speed: 0.0149s/iter; left time: 321.4519s
Epoch: 18 cost time: 4.465407133102417
Epoch: 18, Steps: 263 | Train Loss: 0.5229217 Vali Loss: 0.2896373 Test Loss: 0.4113286
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5553475
	speed: 0.0716s/iter; left time: 1537.0248s
	iters: 200, epoch: 19 | loss: 0.3655774
	speed: 0.0141s/iter; left time: 301.6149s
Epoch: 19 cost time: 4.347745656967163
Epoch: 19, Steps: 263 | Train Loss: 0.5232536 Vali Loss: 0.2895665 Test Loss: 0.4113173
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4884189
	speed: 0.0717s/iter; left time: 1519.6926s
	iters: 200, epoch: 20 | loss: 0.5667942
	speed: 0.0148s/iter; left time: 311.3472s
Epoch: 20 cost time: 4.406763792037964
Epoch: 20, Steps: 263 | Train Loss: 0.5226509 Vali Loss: 0.2898557 Test Loss: 0.4114123
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5943523
	speed: 0.0724s/iter; left time: 1516.7292s
	iters: 200, epoch: 21 | loss: 0.6823440
	speed: 0.0150s/iter; left time: 311.5992s
Epoch: 21 cost time: 4.4544007778167725
Epoch: 21, Steps: 263 | Train Loss: 0.5231638 Vali Loss: 0.2897944 Test Loss: 0.4113812
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.6301345
	speed: 0.0728s/iter; left time: 1505.8644s
	iters: 200, epoch: 22 | loss: 0.6125957
	speed: 0.0145s/iter; left time: 298.7712s
Epoch: 22 cost time: 4.342995882034302
Epoch: 22, Steps: 263 | Train Loss: 0.5234284 Vali Loss: 0.2895601 Test Loss: 0.4114110
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5948564
	speed: 0.0714s/iter; left time: 1457.5030s
	iters: 200, epoch: 23 | loss: 0.4482898
	speed: 0.0149s/iter; left time: 303.3449s
Epoch: 23 cost time: 5.910424709320068
Epoch: 23, Steps: 263 | Train Loss: 0.5232086 Vali Loss: 0.2897851 Test Loss: 0.4114220
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4936261
	speed: 0.0888s/iter; left time: 1789.9430s
	iters: 200, epoch: 24 | loss: 0.4272737
	speed: 0.0166s/iter; left time: 332.3583s
Epoch: 24 cost time: 4.844310998916626
Epoch: 24, Steps: 263 | Train Loss: 0.5231655 Vali Loss: 0.2895508 Test Loss: 0.4114801
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5244230
	speed: 0.0778s/iter; left time: 1548.1848s
	iters: 200, epoch: 25 | loss: 0.4960485
	speed: 0.0162s/iter; left time: 320.5295s
Epoch: 25 cost time: 4.7704551219940186
Epoch: 25, Steps: 263 | Train Loss: 0.5229540 Vali Loss: 0.2897991 Test Loss: 0.4114578
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.7584516
	speed: 0.0720s/iter; left time: 1412.4113s
	iters: 200, epoch: 26 | loss: 0.5502125
	speed: 0.0137s/iter; left time: 267.8578s
Epoch: 26 cost time: 4.2272608280181885
Epoch: 26, Steps: 263 | Train Loss: 0.5229330 Vali Loss: 0.2896646 Test Loss: 0.4114891
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3076748
	speed: 0.0726s/iter; left time: 1405.8361s
	iters: 200, epoch: 27 | loss: 0.5730852
	speed: 0.0306s/iter; left time: 588.8437s
Epoch: 27 cost time: 7.794922590255737
Epoch: 27, Steps: 263 | Train Loss: 0.5224771 Vali Loss: 0.2897525 Test Loss: 0.4115166
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5494640
	speed: 0.0934s/iter; left time: 1784.1317s
	iters: 200, epoch: 28 | loss: 0.5446745
	speed: 0.0145s/iter; left time: 276.2273s
Epoch: 28 cost time: 5.62385892868042
Epoch: 28, Steps: 263 | Train Loss: 0.5230401 Vali Loss: 0.2900768 Test Loss: 0.4114954
EarlyStopping counter: 20 out of 20
Early stopping
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=20, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3225600.0
params:  3780.0
Trainable parameters:  3780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6807930
	speed: 0.0200s/iter; left time: 523.4917s
	iters: 200, epoch: 1 | loss: 0.3158413
	speed: 0.0154s/iter; left time: 402.3557s
Epoch: 1 cost time: 4.483764886856079
Epoch: 1, Steps: 263 | Train Loss: 0.5885778 Vali Loss: 0.2892337 Test Loss: 0.4111110
Validation loss decreased (inf --> 0.289234).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5650181
	speed: 0.0793s/iter; left time: 2056.7984s
	iters: 200, epoch: 2 | loss: 0.5508492
	speed: 0.0145s/iter; left time: 375.3973s
Epoch: 2 cost time: 4.559571027755737
Epoch: 2, Steps: 263 | Train Loss: 0.5873861 Vali Loss: 0.2894518 Test Loss: 0.4111900
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5823774
	speed: 0.0765s/iter; left time: 1964.8087s
	iters: 200, epoch: 3 | loss: 0.7962671
	speed: 0.0142s/iter; left time: 363.0815s
Epoch: 3 cost time: 4.335075616836548
Epoch: 3, Steps: 263 | Train Loss: 0.5881218 Vali Loss: 0.2895627 Test Loss: 0.4111085
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5067116
	speed: 0.0716s/iter; left time: 1820.2481s
	iters: 200, epoch: 4 | loss: 0.5862064
	speed: 0.0146s/iter; left time: 370.3466s
Epoch: 4 cost time: 4.443408012390137
Epoch: 4, Steps: 263 | Train Loss: 0.5878572 Vali Loss: 0.2890368 Test Loss: 0.4110622
Validation loss decreased (0.289234 --> 0.289037).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6843888
	speed: 0.0707s/iter; left time: 1776.8493s
	iters: 200, epoch: 5 | loss: 0.6025354
	speed: 0.0144s/iter; left time: 361.5609s
Epoch: 5 cost time: 4.3503217697143555
Epoch: 5, Steps: 263 | Train Loss: 0.5868858 Vali Loss: 0.2893566 Test Loss: 0.4111320
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6776540
	speed: 0.0716s/iter; left time: 1781.0629s
	iters: 200, epoch: 6 | loss: 0.6850649
	speed: 0.0144s/iter; left time: 357.5477s
Epoch: 6 cost time: 5.522268533706665
Epoch: 6, Steps: 263 | Train Loss: 0.5867272 Vali Loss: 0.2892856 Test Loss: 0.4112802
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4577489
	speed: 0.0825s/iter; left time: 2031.2212s
	iters: 200, epoch: 7 | loss: 0.6352481
	speed: 0.0145s/iter; left time: 356.0691s
Epoch: 7 cost time: 4.379675626754761
Epoch: 7, Steps: 263 | Train Loss: 0.5868189 Vali Loss: 0.2894052 Test Loss: 0.4111701
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6260016
	speed: 0.0797s/iter; left time: 1941.7650s
	iters: 200, epoch: 8 | loss: 0.3342934
	speed: 0.0145s/iter; left time: 352.8674s
Epoch: 8 cost time: 4.397425889968872
Epoch: 8, Steps: 263 | Train Loss: 0.5868833 Vali Loss: 0.2895269 Test Loss: 0.4112576
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6154045
	speed: 0.0735s/iter; left time: 1772.0331s
	iters: 200, epoch: 9 | loss: 0.7316089
	speed: 0.0139s/iter; left time: 333.2538s
Epoch: 9 cost time: 4.2847740650177
Epoch: 9, Steps: 263 | Train Loss: 0.5870664 Vali Loss: 0.2897226 Test Loss: 0.4111758
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4677414
	speed: 0.0703s/iter; left time: 1676.7049s
	iters: 200, epoch: 10 | loss: 0.7096041
	speed: 0.0152s/iter; left time: 360.1633s
Epoch: 10 cost time: 4.533802509307861
Epoch: 10, Steps: 263 | Train Loss: 0.5873754 Vali Loss: 0.2896819 Test Loss: 0.4112050
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4668730
	speed: 0.0726s/iter; left time: 1711.3850s
	iters: 200, epoch: 11 | loss: 0.6115139
	speed: 0.0140s/iter; left time: 328.6621s
Epoch: 11 cost time: 4.358311176300049
Epoch: 11, Steps: 263 | Train Loss: 0.5871753 Vali Loss: 0.2899246 Test Loss: 0.4112072
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6024998
	speed: 0.0728s/iter; left time: 1696.6173s
	iters: 200, epoch: 12 | loss: 0.4816988
	speed: 0.0148s/iter; left time: 342.7091s
Epoch: 12 cost time: 4.41593861579895
Epoch: 12, Steps: 263 | Train Loss: 0.5871409 Vali Loss: 0.2894879 Test Loss: 0.4112447
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5466373
	speed: 0.0716s/iter; left time: 1650.1559s
	iters: 200, epoch: 13 | loss: 0.4500460
	speed: 0.0147s/iter; left time: 336.5417s
Epoch: 13 cost time: 4.4596216678619385
Epoch: 13, Steps: 263 | Train Loss: 0.5871424 Vali Loss: 0.2897059 Test Loss: 0.4113446
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.7050363
	speed: 0.0709s/iter; left time: 1616.1294s
	iters: 200, epoch: 14 | loss: 0.7426464
	speed: 0.0149s/iter; left time: 338.9712s
Epoch: 14 cost time: 4.3635265827178955
Epoch: 14, Steps: 263 | Train Loss: 0.5871164 Vali Loss: 0.2896031 Test Loss: 0.4113064
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.8658628
	speed: 0.0714s/iter; left time: 1607.7081s
	iters: 200, epoch: 15 | loss: 0.4695886
	speed: 0.0146s/iter; left time: 326.7926s
Epoch: 15 cost time: 4.353312730789185
Epoch: 15, Steps: 263 | Train Loss: 0.5864379 Vali Loss: 0.2896705 Test Loss: 0.4112816
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5027642
	speed: 0.0718s/iter; left time: 1597.9356s
	iters: 200, epoch: 16 | loss: 0.6816140
	speed: 0.0148s/iter; left time: 327.0180s
Epoch: 16 cost time: 4.470045804977417
Epoch: 16, Steps: 263 | Train Loss: 0.5865310 Vali Loss: 0.2895471 Test Loss: 0.4113251
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5159272
	speed: 0.0713s/iter; left time: 1569.0390s
	iters: 200, epoch: 17 | loss: 0.5200006
	speed: 0.0148s/iter; left time: 323.5431s
Epoch: 17 cost time: 4.434771299362183
Epoch: 17, Steps: 263 | Train Loss: 0.5872066 Vali Loss: 0.2896453 Test Loss: 0.4112811
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5117278
	speed: 0.0745s/iter; left time: 1618.4067s
	iters: 200, epoch: 18 | loss: 0.6639094
	speed: 0.0148s/iter; left time: 319.7144s
Epoch: 18 cost time: 4.4143781661987305
Epoch: 18, Steps: 263 | Train Loss: 0.5867148 Vali Loss: 0.2898836 Test Loss: 0.4113190
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.6540651
	speed: 0.0734s/iter; left time: 1576.3112s
	iters: 200, epoch: 19 | loss: 0.5407476
	speed: 0.0149s/iter; left time: 318.7590s
Epoch: 19 cost time: 4.4317991733551025
Epoch: 19, Steps: 263 | Train Loss: 0.5859867 Vali Loss: 0.2896755 Test Loss: 0.4112901
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4471183
	speed: 0.0725s/iter; left time: 1537.5706s
	iters: 200, epoch: 20 | loss: 0.5375912
	speed: 0.0147s/iter; left time: 309.2353s
Epoch: 20 cost time: 4.402010679244995
Epoch: 20, Steps: 263 | Train Loss: 0.5867044 Vali Loss: 0.2898025 Test Loss: 0.4113652
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.6795605
	speed: 0.0720s/iter; left time: 1507.9451s
	iters: 200, epoch: 21 | loss: 0.5311166
	speed: 0.0144s/iter; left time: 301.0350s
Epoch: 21 cost time: 4.321128845214844
Epoch: 21, Steps: 263 | Train Loss: 0.5866390 Vali Loss: 0.2898278 Test Loss: 0.4113751
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5511752
	speed: 0.0703s/iter; left time: 1453.3385s
	iters: 200, epoch: 22 | loss: 0.6398002
	speed: 0.0147s/iter; left time: 301.7505s
Epoch: 22 cost time: 4.269305944442749
Epoch: 22, Steps: 263 | Train Loss: 0.5857068 Vali Loss: 0.2900042 Test Loss: 0.4113590
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4493482
	speed: 0.0709s/iter; left time: 1447.9549s
	iters: 200, epoch: 23 | loss: 0.6600788
	speed: 0.0147s/iter; left time: 297.9587s
Epoch: 23 cost time: 4.442693710327148
Epoch: 23, Steps: 263 | Train Loss: 0.5864880 Vali Loss: 0.2901307 Test Loss: 0.4113896
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.7507345
	speed: 0.0732s/iter; left time: 1475.1349s
	iters: 200, epoch: 24 | loss: 0.5489741
	speed: 0.0284s/iter; left time: 568.6984s
Epoch: 24 cost time: 5.884267330169678
Epoch: 24, Steps: 263 | Train Loss: 0.5866059 Vali Loss: 0.2900313 Test Loss: 0.4113924
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4087027609348297, mae:0.398637980222702, rse:0.5138639807701111, corr:[0.5506289  0.549012   0.5421303  0.5398836  0.5400229  0.538167
 0.5353324  0.5340041  0.5336907  0.5324058  0.5303742  0.5292589
 0.5292048  0.52871233 0.52713585 0.5255964  0.5249826  0.5245178
 0.523302   0.5217249  0.52073723 0.52024883 0.51944757 0.51827997
 0.51743096 0.5171667  0.51687294 0.5159902  0.5148325  0.51411283
 0.5136423  0.5128802  0.5118937  0.5112734  0.51107925 0.5106897
 0.50971603 0.5086761  0.50810635 0.5077422  0.5070175  0.5060862
 0.5055432  0.5055764  0.5055567  0.5049882  0.5041612  0.50347894
 0.50295025 0.50213903 0.5008982  0.49966833 0.4988558  0.49831522
 0.497756   0.49706    0.4964758  0.49620867 0.4960509  0.49577597
 0.49534857 0.49504966 0.49505097 0.49516973 0.49520445 0.4952029
 0.4952062  0.49518278 0.4951592  0.49517906 0.4952561  0.4953817
 0.49544254 0.4954691  0.49543983 0.49539745 0.4953472  0.49520883
 0.49503946 0.4948537  0.49466056 0.4944611  0.49426183 0.49404365
 0.49380153 0.49352622 0.49323943 0.49301532 0.49281365 0.49259013
 0.49231175 0.49193385 0.49141547 0.4906335  0.48934773 0.48746234
 0.48515314 0.48276398 0.4805456  0.4786168  0.47694683 0.47549206
 0.47411376 0.47274455 0.4714215  0.47003305 0.46851477 0.46701297
 0.4656539  0.46436653 0.46297333 0.4614088  0.45987478 0.45848235
 0.4571406  0.45574617 0.45432353 0.453008   0.45186895 0.4506289
 0.44928396 0.44799754 0.44692463 0.44603637 0.44512004 0.4440604
 0.44303587 0.44217288 0.44125775 0.4401089  0.43883082 0.43775073
 0.43709147 0.43658286 0.43568707 0.4345103  0.43339393 0.4328953
 0.43285087 0.43265048 0.43201166 0.43128902 0.43095812 0.43082592
 0.43025738 0.42901203 0.42747223 0.42641485 0.4258211  0.42519128
 0.42419592 0.42338324 0.42305958 0.42296785 0.4228374  0.42245415
 0.42195258 0.42166862 0.42147365 0.42131197 0.4211355  0.42105567
 0.42124164 0.4212783  0.42118946 0.42107174 0.42116812 0.42152017
 0.42185485 0.42186716 0.42159522 0.42136177 0.42142284 0.42161712
 0.42167446 0.4216208  0.42167547 0.4219595  0.42228943 0.42245945
 0.4224423  0.4223727  0.422437   0.42244038 0.42224768 0.4218693
 0.42147774 0.42116272 0.42066282 0.41960347 0.41781977 0.41548067
 0.41304645 0.41094112 0.40909442 0.40734485 0.40571174 0.40415806
 0.4026822  0.40128607 0.4000323  0.39882898 0.3975537  0.39617124
 0.39470863 0.39325565 0.39171526 0.3900546  0.38838717 0.38688168
 0.38549662 0.3842038  0.3829123  0.38157701 0.3802262  0.37892315
 0.37772614 0.37654024 0.37538296 0.37407988 0.37265632 0.37133455
 0.37024865 0.36913687 0.36804438 0.3667832  0.36560103 0.36451843
 0.36347258 0.36236933 0.3611461  0.36004066 0.35915014 0.35856792
 0.3582192  0.35807204 0.35805994 0.35793865 0.35766548 0.35727936
 0.3569455  0.35667795 0.35620436 0.35581425 0.35551566 0.35532436
 0.35542178 0.35569096 0.3558659  0.3557097  0.35547435 0.35538393
 0.3554499  0.35574114 0.35628188 0.35680383 0.35703444 0.35693827
 0.3567689  0.35691318 0.35730043 0.35774082 0.35806388 0.35836703
 0.35887447 0.35941306 0.3598159  0.3599261  0.36011103 0.3605957
 0.36113495 0.36133984 0.36116472 0.361103   0.361567   0.36233592
 0.36290544 0.3629935  0.36283028 0.36291993 0.36324313 0.36352283
 0.36353388 0.36335856 0.36317906 0.36292368 0.36234772 0.36122733
 0.35970744 0.35829264 0.35730952 0.3568093  0.35658196 0.3565197
 0.3565473  0.35646033 0.35622114 0.3558376  0.35529712 0.35469782
 0.35415515 0.3535806  0.35282612 0.3518854  0.35093394 0.35006213
 0.34920356 0.34814814 0.34687248 0.34562546 0.34471866 0.34427443
 0.34394908 0.34346455 0.3426762  0.3417466  0.34089947 0.34015083
 0.3393165  0.33838674 0.3375687  0.33698672 0.3364808  0.33581278
 0.33494988 0.3342007  0.33380428 0.33357048 0.33334458 0.33304703
 0.3328316  0.33274212 0.33274913 0.3327019  0.33260512 0.332529
 0.33230564 0.3317967  0.33125263 0.3311698  0.3316166  0.3322033
 0.33251354 0.33251423 0.33238643 0.33236545 0.33253914 0.33287743
 0.3333617  0.33392256 0.3345825  0.33516666 0.3354566  0.33537444
 0.33514035 0.3351363  0.33540523 0.3358733  0.33629978 0.33662817
 0.3369584  0.33742213 0.33793887 0.33835736 0.3386185  0.33891356
 0.33931342 0.33976844 0.3401946  0.34052914 0.34096947 0.3416496
 0.34245247 0.3430596  0.34329608 0.34339947 0.34371287 0.34429446
 0.34477454 0.34472644 0.3440365  0.3430146  0.34201896 0.34100577
 0.3397205  0.33816233 0.33670214 0.33563843 0.33488154 0.3341892
 0.3333946  0.3326121  0.33206457 0.33158728 0.33088893 0.32990238
 0.3288781  0.32802513 0.3271762  0.32607043 0.32462233 0.3230813
 0.32175758 0.32054263 0.3191845  0.31775537 0.31644192 0.315457
 0.3146471  0.3137278  0.31264475 0.31157792 0.31069466 0.30985355
 0.30890274 0.30784392 0.30693024 0.30630314 0.30564028 0.30478236
 0.3038802  0.30313104 0.30255678 0.3019341  0.30114955 0.30048248
 0.30020916 0.30033663 0.30040923 0.30010712 0.29941988 0.2986772
 0.29806882 0.2975568  0.29706818 0.29670694 0.29641932 0.29615477
 0.29569417 0.29511392 0.29454744 0.29417738 0.29413265 0.29446244
 0.2949508  0.2953154  0.2953336  0.29496813 0.2945793  0.29451716
 0.29484344 0.29519808 0.29534498 0.2954526  0.2958921  0.29679543
 0.29769868 0.29808882 0.29796594 0.29779115 0.29794446 0.29814035
 0.29814816 0.29812595 0.2984151  0.29890063 0.2990863  0.29875743
 0.29840046 0.29856366 0.2991206  0.29929835 0.29868677 0.2978366
 0.29763377 0.29803488 0.29802015 0.29662937 0.29393324 0.29088196
 0.2883875  0.2865645  0.28507778 0.28383484 0.2828893  0.2822467
 0.28179303 0.28143823 0.2811139  0.28070706 0.2800788  0.27920455
 0.27815062 0.27706596 0.27595404 0.27478987 0.2735488  0.27223018
 0.27092043 0.26953062 0.2679666  0.26638567 0.26503772 0.26400533
 0.26303843 0.26195988 0.2608613  0.2599334  0.2593214  0.25876054
 0.25790218 0.25675723 0.25569102 0.25494948 0.25427258 0.25342974
 0.25252047 0.25172016 0.2511763  0.2507361  0.25008956 0.24913143
 0.2480809  0.24749987 0.2475163  0.24776803 0.24767804 0.24695815
 0.2459997  0.24522205 0.24483629 0.24453983 0.24412867 0.24385366
 0.24394274 0.24418429 0.24420118 0.24396059 0.24398763 0.24459043
 0.24528122 0.24532087 0.24473035 0.24430482 0.24446161 0.24490513
 0.24509616 0.24511045 0.24541214 0.24601166 0.24620132 0.24589562
 0.24564897 0.24626218 0.24758571 0.24862808 0.24864867 0.24814112
 0.24792497 0.24825104 0.24860427 0.2487413  0.24907313 0.2499471
 0.25092128 0.25132385 0.25110218 0.25077623 0.2508765  0.2512213
 0.25140482 0.25127152 0.25089264 0.25009388 0.24843688 0.24581502
 0.24293989 0.24085848 0.23978572 0.2392166  0.2387744  0.23855844
 0.23872842 0.2392576  0.23978212 0.23988585 0.23948571 0.23873381
 0.23779249 0.23670019 0.23555067 0.2344815  0.2335022  0.23245263
 0.23119624 0.22976606 0.22834338 0.22701366 0.22574377 0.22460727
 0.22378042 0.22336826 0.22312093 0.2226372  0.22189836 0.22097701
 0.2199532  0.21885389 0.21760127 0.21626092 0.21517442 0.21450277
 0.2141415  0.21363616 0.21280774 0.2117421  0.21089686 0.21033227
 0.20984983 0.20941432 0.2091397  0.20922653 0.20965795 0.2097974
 0.20921114 0.20798105 0.20673797 0.20604947 0.20607547 0.20632121
 0.20648453 0.20642482 0.20610186 0.20569995 0.20547017 0.20533182
 0.20525034 0.20538634 0.20592178 0.20669185 0.2072467  0.20714188
 0.20658405 0.20611049 0.20611708 0.20645355 0.20713182 0.20800553
 0.20887226 0.2094109  0.20954585 0.2096763  0.21019247 0.21092685
 0.21141982 0.21154141 0.21171999 0.21249181 0.21363187 0.2145123
 0.2149867  0.21545675 0.21620955 0.21689367 0.21708322 0.21687378
 0.216807   0.21702684 0.21698046 0.2160973  0.21435335 0.21216337
 0.20992997 0.20787385 0.20615098 0.20499647 0.20455977 0.20467111
 0.20498891 0.2053976  0.20587961 0.20622227 0.20610295 0.20543095
 0.20448743 0.2036028  0.20275971 0.20166261 0.20021953 0.19874941
 0.19765356 0.19681346 0.19566189 0.19402832 0.19241388 0.19138265
 0.1908583  0.19015329 0.1887358  0.18693934 0.1855911  0.18461908
 0.18332869 0.1813657  0.17937303 0.17822458 0.17762186 0.17683713
 0.17539422 0.17387113 0.17258433 0.17137901 0.1696191  0.16800222
 0.16743568 0.16735125 0.16646093 0.16576059 0.16859399 0.17446142]
