Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=74, out_features=148, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9812992.0
params:  11100.0
Trainable parameters:  11100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5723025
	speed: 0.0282s/iter; left time: 726.0286s
	iters: 200, epoch: 1 | loss: 0.3835304
	speed: 0.0220s/iter; left time: 562.4318s
Epoch: 1 cost time: 6.21540641784668
Epoch: 1, Steps: 258 | Train Loss: 0.5786951 Vali Loss: 0.2797790 Test Loss: 0.3749194
Validation loss decreased (inf --> 0.279779).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5471435
	speed: 0.0895s/iter; left time: 2276.5872s
	iters: 200, epoch: 2 | loss: 0.5507796
	speed: 0.0212s/iter; left time: 536.6410s
Epoch: 2 cost time: 6.0880303382873535
Epoch: 2, Steps: 258 | Train Loss: 0.5190485 Vali Loss: 0.2701917 Test Loss: 0.3653002
Validation loss decreased (0.279779 --> 0.270192).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5354676
	speed: 0.0912s/iter; left time: 2296.1721s
	iters: 200, epoch: 3 | loss: 0.5294587
	speed: 0.0215s/iter; left time: 539.9323s
Epoch: 3 cost time: 6.174935817718506
Epoch: 3, Steps: 258 | Train Loss: 0.5108751 Vali Loss: 0.2666575 Test Loss: 0.3608850
Validation loss decreased (0.270192 --> 0.266658).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5927936
	speed: 0.0884s/iter; left time: 2204.5163s
	iters: 200, epoch: 4 | loss: 0.4027170
	speed: 0.0205s/iter; left time: 509.7493s
Epoch: 4 cost time: 6.079535722732544
Epoch: 4, Steps: 258 | Train Loss: 0.5058869 Vali Loss: 0.2656313 Test Loss: 0.3589295
Validation loss decreased (0.266658 --> 0.265631).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6847497
	speed: 0.0892s/iter; left time: 2199.4962s
	iters: 200, epoch: 5 | loss: 0.5654822
	speed: 0.0211s/iter; left time: 519.2507s
Epoch: 5 cost time: 5.958488702774048
Epoch: 5, Steps: 258 | Train Loss: 0.5030504 Vali Loss: 0.2649981 Test Loss: 0.3572748
Validation loss decreased (0.265631 --> 0.264998).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5048472
	speed: 0.0896s/iter; left time: 2186.6037s
	iters: 200, epoch: 6 | loss: 0.5734402
	speed: 0.0211s/iter; left time: 512.2187s
Epoch: 6 cost time: 6.111156225204468
Epoch: 6, Steps: 258 | Train Loss: 0.5019877 Vali Loss: 0.2640150 Test Loss: 0.3564015
Validation loss decreased (0.264998 --> 0.264015).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3834324
	speed: 0.0894s/iter; left time: 2160.2030s
	iters: 200, epoch: 7 | loss: 0.5243036
	speed: 0.0206s/iter; left time: 495.1689s
Epoch: 7 cost time: 5.9964940547943115
Epoch: 7, Steps: 258 | Train Loss: 0.5005502 Vali Loss: 0.2638271 Test Loss: 0.3559043
Validation loss decreased (0.264015 --> 0.263827).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4508181
	speed: 0.0905s/iter; left time: 2162.0448s
	iters: 200, epoch: 8 | loss: 0.4622983
	speed: 0.0211s/iter; left time: 501.2756s
Epoch: 8 cost time: 6.046550989151001
Epoch: 8, Steps: 258 | Train Loss: 0.4987968 Vali Loss: 0.2633460 Test Loss: 0.3551974
Validation loss decreased (0.263827 --> 0.263346).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4527164
	speed: 0.0899s/iter; left time: 2124.7853s
	iters: 200, epoch: 9 | loss: 0.6116245
	speed: 0.0204s/iter; left time: 481.2881s
Epoch: 9 cost time: 5.8757102489471436
Epoch: 9, Steps: 258 | Train Loss: 0.4992045 Vali Loss: 0.2631739 Test Loss: 0.3549723
Validation loss decreased (0.263346 --> 0.263174).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4507921
	speed: 0.0929s/iter; left time: 2170.7993s
	iters: 200, epoch: 10 | loss: 0.4948841
	speed: 0.0223s/iter; left time: 518.8218s
Epoch: 10 cost time: 6.310651063919067
Epoch: 10, Steps: 258 | Train Loss: 0.4989162 Vali Loss: 0.2625957 Test Loss: 0.3546655
Validation loss decreased (0.263174 --> 0.262596).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3536406
	speed: 0.0926s/iter; left time: 2139.9194s
	iters: 200, epoch: 11 | loss: 0.5545729
	speed: 0.0216s/iter; left time: 498.3744s
Epoch: 11 cost time: 6.176268100738525
Epoch: 11, Steps: 258 | Train Loss: 0.4980206 Vali Loss: 0.2632019 Test Loss: 0.3542251
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4611574
	speed: 0.0953s/iter; left time: 2178.2053s
	iters: 200, epoch: 12 | loss: 0.4159724
	speed: 0.0204s/iter; left time: 463.9062s
Epoch: 12 cost time: 6.187729120254517
Epoch: 12, Steps: 258 | Train Loss: 0.4977095 Vali Loss: 0.2625732 Test Loss: 0.3540376
Validation loss decreased (0.262596 --> 0.262573).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3668264
	speed: 0.0913s/iter; left time: 2063.3473s
	iters: 200, epoch: 13 | loss: 0.4289390
	speed: 0.0202s/iter; left time: 453.8706s
Epoch: 13 cost time: 5.881615161895752
Epoch: 13, Steps: 258 | Train Loss: 0.4976235 Vali Loss: 0.2626001 Test Loss: 0.3541201
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4526050
	speed: 0.0923s/iter; left time: 2062.0483s
	iters: 200, epoch: 14 | loss: 0.6011019
	speed: 0.0211s/iter; left time: 469.8035s
Epoch: 14 cost time: 6.041003465652466
Epoch: 14, Steps: 258 | Train Loss: 0.4974575 Vali Loss: 0.2621864 Test Loss: 0.3540339
Validation loss decreased (0.262573 --> 0.262186).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4773563
	speed: 0.0917s/iter; left time: 2026.4103s
	iters: 200, epoch: 15 | loss: 0.5615231
	speed: 0.0214s/iter; left time: 470.1357s
Epoch: 15 cost time: 6.126193284988403
Epoch: 15, Steps: 258 | Train Loss: 0.4968314 Vali Loss: 0.2622613 Test Loss: 0.3537068
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4509334
	speed: 0.0926s/iter; left time: 2020.6303s
	iters: 200, epoch: 16 | loss: 0.4341423
	speed: 0.0204s/iter; left time: 442.3154s
Epoch: 16 cost time: 6.097460746765137
Epoch: 16, Steps: 258 | Train Loss: 0.4971250 Vali Loss: 0.2621754 Test Loss: 0.3536456
Validation loss decreased (0.262186 --> 0.262175).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4422752
	speed: 0.0938s/iter; left time: 2023.9855s
	iters: 200, epoch: 17 | loss: 0.3434706
	speed: 0.0219s/iter; left time: 470.9333s
Epoch: 17 cost time: 6.2807464599609375
Epoch: 17, Steps: 258 | Train Loss: 0.4965818 Vali Loss: 0.2621946 Test Loss: 0.3536798
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5141928
	speed: 0.0924s/iter; left time: 1968.8342s
	iters: 200, epoch: 18 | loss: 0.4268408
	speed: 0.0212s/iter; left time: 450.3952s
Epoch: 18 cost time: 6.227245569229126
Epoch: 18, Steps: 258 | Train Loss: 0.4965829 Vali Loss: 0.2621485 Test Loss: 0.3535705
Validation loss decreased (0.262175 --> 0.262148).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5175174
	speed: 0.0932s/iter; left time: 1962.1519s
	iters: 200, epoch: 19 | loss: 0.4608019
	speed: 0.0210s/iter; left time: 440.8765s
Epoch: 19 cost time: 6.091556549072266
Epoch: 19, Steps: 258 | Train Loss: 0.4963526 Vali Loss: 0.2620614 Test Loss: 0.3535795
Validation loss decreased (0.262148 --> 0.262061).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5050087
	speed: 0.0926s/iter; left time: 1925.7096s
	iters: 200, epoch: 20 | loss: 0.3797352
	speed: 0.0218s/iter; left time: 450.3412s
Epoch: 20 cost time: 6.181802749633789
Epoch: 20, Steps: 258 | Train Loss: 0.4967739 Vali Loss: 0.2619655 Test Loss: 0.3535215
Validation loss decreased (0.262061 --> 0.261966).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4227964
	speed: 0.0921s/iter; left time: 1892.3579s
	iters: 200, epoch: 21 | loss: 0.5517148
	speed: 0.0231s/iter; left time: 472.3646s
Epoch: 21 cost time: 6.373477935791016
Epoch: 21, Steps: 258 | Train Loss: 0.4960518 Vali Loss: 0.2618012 Test Loss: 0.3532770
Validation loss decreased (0.261966 --> 0.261801).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5968061
	speed: 0.0945s/iter; left time: 1915.7502s
	iters: 200, epoch: 22 | loss: 0.5032013
	speed: 0.0203s/iter; left time: 409.0815s
Epoch: 22 cost time: 6.129376411437988
Epoch: 22, Steps: 258 | Train Loss: 0.4965758 Vali Loss: 0.2620782 Test Loss: 0.3532269
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5009083
	speed: 0.0962s/iter; left time: 1927.1130s
	iters: 200, epoch: 23 | loss: 0.5117233
	speed: 0.0204s/iter; left time: 406.4559s
Epoch: 23 cost time: 5.915305852890015
Epoch: 23, Steps: 258 | Train Loss: 0.4961131 Vali Loss: 0.2617082 Test Loss: 0.3533283
Validation loss decreased (0.261801 --> 0.261708).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4472099
	speed: 0.0887s/iter; left time: 1753.0428s
	iters: 200, epoch: 24 | loss: 0.5489059
	speed: 0.0207s/iter; left time: 407.0176s
Epoch: 24 cost time: 5.916645765304565
Epoch: 24, Steps: 258 | Train Loss: 0.4960191 Vali Loss: 0.2618588 Test Loss: 0.3532982
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5223233
	speed: 0.0916s/iter; left time: 1787.7964s
	iters: 200, epoch: 25 | loss: 0.3547261
	speed: 0.0207s/iter; left time: 402.4730s
Epoch: 25 cost time: 6.115326881408691
Epoch: 25, Steps: 258 | Train Loss: 0.4963199 Vali Loss: 0.2616819 Test Loss: 0.3532861
Validation loss decreased (0.261708 --> 0.261682).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4201558
	speed: 0.0924s/iter; left time: 1778.8136s
	iters: 200, epoch: 26 | loss: 0.5333872
	speed: 0.0206s/iter; left time: 394.6692s
Epoch: 26 cost time: 6.004263401031494
Epoch: 26, Steps: 258 | Train Loss: 0.4961215 Vali Loss: 0.2614942 Test Loss: 0.3532588
Validation loss decreased (0.261682 --> 0.261494).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4076527
	speed: 0.0907s/iter; left time: 1723.5812s
	iters: 200, epoch: 27 | loss: 0.4754615
	speed: 0.0210s/iter; left time: 395.8884s
Epoch: 27 cost time: 5.9831037521362305
Epoch: 27, Steps: 258 | Train Loss: 0.4959531 Vali Loss: 0.2614111 Test Loss: 0.3532103
Validation loss decreased (0.261494 --> 0.261411).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5276283
	speed: 0.0919s/iter; left time: 1722.1707s
	iters: 200, epoch: 28 | loss: 0.5361345
	speed: 0.0209s/iter; left time: 388.5961s
Epoch: 28 cost time: 6.221817255020142
Epoch: 28, Steps: 258 | Train Loss: 0.4960596 Vali Loss: 0.2617284 Test Loss: 0.3532543
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.6325688
	speed: 0.0929s/iter; left time: 1715.6785s
	iters: 200, epoch: 29 | loss: 0.6081091
	speed: 0.0217s/iter; left time: 399.6262s
Epoch: 29 cost time: 6.213707447052002
Epoch: 29, Steps: 258 | Train Loss: 0.4958934 Vali Loss: 0.2617656 Test Loss: 0.3531759
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4507520
	speed: 0.1249s/iter; left time: 2275.8430s
	iters: 200, epoch: 30 | loss: 0.5833489
	speed: 0.0209s/iter; left time: 378.6845s
Epoch: 30 cost time: 6.1644816398620605
Epoch: 30, Steps: 258 | Train Loss: 0.4953461 Vali Loss: 0.2614861 Test Loss: 0.3531959
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.5624518
	speed: 0.0934s/iter; left time: 1677.6986s
	iters: 200, epoch: 31 | loss: 0.4712661
	speed: 0.0209s/iter; left time: 373.4759s
Epoch: 31 cost time: 6.01492977142334
Epoch: 31, Steps: 258 | Train Loss: 0.4960438 Vali Loss: 0.2618798 Test Loss: 0.3532071
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3611834
	speed: 0.0926s/iter; left time: 1639.1505s
	iters: 200, epoch: 32 | loss: 0.5538737
	speed: 0.0206s/iter; left time: 363.3761s
Epoch: 32 cost time: 6.192151308059692
Epoch: 32, Steps: 258 | Train Loss: 0.4955458 Vali Loss: 0.2618014 Test Loss: 0.3531805
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.6148453
	speed: 0.0904s/iter; left time: 1577.5851s
	iters: 200, epoch: 33 | loss: 0.4459215
	speed: 0.0216s/iter; left time: 374.0380s
Epoch: 33 cost time: 6.100342750549316
Epoch: 33, Steps: 258 | Train Loss: 0.4957224 Vali Loss: 0.2616232 Test Loss: 0.3531103
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.5897445
	speed: 0.1276s/iter; left time: 2193.8074s
	iters: 200, epoch: 34 | loss: 0.3756910
	speed: 0.0207s/iter; left time: 354.4100s
Epoch: 34 cost time: 6.14293646812439
Epoch: 34, Steps: 258 | Train Loss: 0.4963035 Vali Loss: 0.2616914 Test Loss: 0.3530910
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5410853
	speed: 0.0895s/iter; left time: 1515.8577s
	iters: 200, epoch: 35 | loss: 0.6487849
	speed: 0.0206s/iter; left time: 347.1618s
Epoch: 35 cost time: 6.066213130950928
Epoch: 35, Steps: 258 | Train Loss: 0.4957846 Vali Loss: 0.2616616 Test Loss: 0.3531463
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4751059
	speed: 0.0938s/iter; left time: 1563.2895s
	iters: 200, epoch: 36 | loss: 0.5057584
	speed: 0.0212s/iter; left time: 350.5457s
Epoch: 36 cost time: 6.13286280632019
Epoch: 36, Steps: 258 | Train Loss: 0.4956001 Vali Loss: 0.2614307 Test Loss: 0.3530793
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4439720
	speed: 0.0898s/iter; left time: 1474.5018s
	iters: 200, epoch: 37 | loss: 0.5710374
	speed: 0.0212s/iter; left time: 345.0490s
Epoch: 37 cost time: 6.043994665145874
Epoch: 37, Steps: 258 | Train Loss: 0.4951379 Vali Loss: 0.2615381 Test Loss: 0.3531000
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.6722282
	speed: 0.0897s/iter; left time: 1448.4332s
	iters: 200, epoch: 38 | loss: 0.6126087
	speed: 0.0210s/iter; left time: 336.3608s
Epoch: 38 cost time: 6.1318042278289795
Epoch: 38, Steps: 258 | Train Loss: 0.4959731 Vali Loss: 0.2617419 Test Loss: 0.3531233
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4602965
	speed: 0.0985s/iter; left time: 1565.3492s
	iters: 200, epoch: 39 | loss: 0.4866852
	speed: 0.0214s/iter; left time: 337.9182s
Epoch: 39 cost time: 6.964184522628784
Epoch: 39, Steps: 258 | Train Loss: 0.4959396 Vali Loss: 0.2616637 Test Loss: 0.3530890
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3360604
	speed: 0.0894s/iter; left time: 1398.0904s
	iters: 200, epoch: 40 | loss: 0.5884057
	speed: 0.0204s/iter; left time: 317.7420s
Epoch: 40 cost time: 5.952470779418945
Epoch: 40, Steps: 258 | Train Loss: 0.4956260 Vali Loss: 0.2618272 Test Loss: 0.3530701
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3364385
	speed: 0.0921s/iter; left time: 1417.2832s
	iters: 200, epoch: 41 | loss: 0.5765210
	speed: 0.0216s/iter; left time: 330.2246s
Epoch: 41 cost time: 6.190997838973999
Epoch: 41, Steps: 258 | Train Loss: 0.4957661 Vali Loss: 0.2618384 Test Loss: 0.3530789
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.5341411
	speed: 0.0949s/iter; left time: 1435.4611s
	iters: 200, epoch: 42 | loss: 0.3672144
	speed: 0.0205s/iter; left time: 308.1679s
Epoch: 42 cost time: 6.116313695907593
Epoch: 42, Steps: 258 | Train Loss: 0.4952919 Vali Loss: 0.2614543 Test Loss: 0.3530656
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.5708209
	speed: 0.0945s/iter; left time: 1404.1141s
	iters: 200, epoch: 43 | loss: 0.6356099
	speed: 0.0210s/iter; left time: 310.1333s
Epoch: 43 cost time: 6.146070718765259
Epoch: 43, Steps: 258 | Train Loss: 0.4959357 Vali Loss: 0.2614687 Test Loss: 0.3530706
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4325147
	speed: 0.0895s/iter; left time: 1307.2907s
	iters: 200, epoch: 44 | loss: 0.4805425
	speed: 0.0204s/iter; left time: 295.2787s
Epoch: 44 cost time: 5.955587387084961
Epoch: 44, Steps: 258 | Train Loss: 0.4955113 Vali Loss: 0.2615466 Test Loss: 0.3530557
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4924204
	speed: 0.0901s/iter; left time: 1293.2396s
	iters: 200, epoch: 45 | loss: 0.7986757
	speed: 0.0216s/iter; left time: 307.9709s
Epoch: 45 cost time: 6.192352294921875
Epoch: 45, Steps: 258 | Train Loss: 0.4953260 Vali Loss: 0.2617343 Test Loss: 0.3530607
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4582376
	speed: 0.0889s/iter; left time: 1252.2392s
	iters: 200, epoch: 46 | loss: 0.3847058
	speed: 0.0211s/iter; left time: 294.7791s
Epoch: 46 cost time: 6.1594507694244385
Epoch: 46, Steps: 258 | Train Loss: 0.4951026 Vali Loss: 0.2616022 Test Loss: 0.3530578
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3912006
	speed: 0.0925s/iter; left time: 1279.6696s
	iters: 200, epoch: 47 | loss: 0.4008551
	speed: 0.0206s/iter; left time: 283.2242s
Epoch: 47 cost time: 6.091523170471191
Epoch: 47, Steps: 258 | Train Loss: 0.4955433 Vali Loss: 0.2612728 Test Loss: 0.3530246
Validation loss decreased (0.261411 --> 0.261273).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.5533903
	speed: 0.0942s/iter; left time: 1278.1087s
	iters: 200, epoch: 48 | loss: 0.6510856
	speed: 0.0212s/iter; left time: 285.3240s
Epoch: 48 cost time: 6.0880701541900635
Epoch: 48, Steps: 258 | Train Loss: 0.4951764 Vali Loss: 0.2614950 Test Loss: 0.3530332
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.5009772
	speed: 0.0905s/iter; left time: 1205.7826s
	iters: 200, epoch: 49 | loss: 0.5196864
	speed: 0.0209s/iter; left time: 276.4710s
Epoch: 49 cost time: 5.901045322418213
Epoch: 49, Steps: 258 | Train Loss: 0.4953508 Vali Loss: 0.2613834 Test Loss: 0.3530330
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.4066942
	speed: 0.0919s/iter; left time: 1199.9154s
	iters: 200, epoch: 50 | loss: 0.3665077
	speed: 0.0198s/iter; left time: 256.0117s
Epoch: 50 cost time: 5.988343238830566
Epoch: 50, Steps: 258 | Train Loss: 0.4951695 Vali Loss: 0.2615825 Test Loss: 0.3530342
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4436002
	speed: 0.0899s/iter; left time: 1150.6058s
	iters: 200, epoch: 51 | loss: 0.5544715
	speed: 0.0206s/iter; left time: 261.0456s
Epoch: 51 cost time: 5.9938600063323975
Epoch: 51, Steps: 258 | Train Loss: 0.4953777 Vali Loss: 0.2615291 Test Loss: 0.3530254
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4345227
	speed: 0.0925s/iter; left time: 1160.5009s
	iters: 200, epoch: 52 | loss: 0.4408790
	speed: 0.0207s/iter; left time: 257.3719s
Epoch: 52 cost time: 6.213628530502319
Epoch: 52, Steps: 258 | Train Loss: 0.4950009 Vali Loss: 0.2616445 Test Loss: 0.3530325
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4315994
	speed: 0.0944s/iter; left time: 1159.3891s
	iters: 200, epoch: 53 | loss: 0.6220139
	speed: 0.0218s/iter; left time: 265.4983s
Epoch: 53 cost time: 6.240187168121338
Epoch: 53, Steps: 258 | Train Loss: 0.4952094 Vali Loss: 0.2614861 Test Loss: 0.3530180
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.5209768
	speed: 0.0899s/iter; left time: 1080.9110s
	iters: 200, epoch: 54 | loss: 0.3868915
	speed: 0.0207s/iter; left time: 246.5462s
Epoch: 54 cost time: 6.0159547328948975
Epoch: 54, Steps: 258 | Train Loss: 0.4952439 Vali Loss: 0.2615748 Test Loss: 0.3529978
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.3558688
	speed: 0.0921s/iter; left time: 1083.9328s
	iters: 200, epoch: 55 | loss: 0.3918642
	speed: 0.0208s/iter; left time: 242.4263s
Epoch: 55 cost time: 6.015772104263306
Epoch: 55, Steps: 258 | Train Loss: 0.4949966 Vali Loss: 0.2617471 Test Loss: 0.3530129
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4594746
	speed: 0.0919s/iter; left time: 1058.3580s
	iters: 200, epoch: 56 | loss: 0.7493953
	speed: 0.0210s/iter; left time: 239.9299s
Epoch: 56 cost time: 6.210603952407837
Epoch: 56, Steps: 258 | Train Loss: 0.4952567 Vali Loss: 0.2614163 Test Loss: 0.3529982
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.4635536
	speed: 0.0904s/iter; left time: 1017.5193s
	iters: 200, epoch: 57 | loss: 0.5156612
	speed: 0.0213s/iter; left time: 237.7873s
Epoch: 57 cost time: 6.019670248031616
Epoch: 57, Steps: 258 | Train Loss: 0.4949307 Vali Loss: 0.2615224 Test Loss: 0.3530129
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.4699816
	speed: 0.0922s/iter; left time: 1013.8643s
	iters: 200, epoch: 58 | loss: 0.4340669
	speed: 0.0208s/iter; left time: 226.3914s
Epoch: 58 cost time: 6.044511079788208
Epoch: 58, Steps: 258 | Train Loss: 0.4958780 Vali Loss: 0.2613951 Test Loss: 0.3530030
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.5287537
	speed: 0.0912s/iter; left time: 978.8119s
	iters: 200, epoch: 59 | loss: 0.5151774
	speed: 0.0206s/iter; left time: 218.7230s
Epoch: 59 cost time: 6.045137166976929
Epoch: 59, Steps: 258 | Train Loss: 0.4952725 Vali Loss: 0.2614317 Test Loss: 0.3530076
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.4673689
	speed: 0.0888s/iter; left time: 930.4773s
	iters: 200, epoch: 60 | loss: 0.6027269
	speed: 0.0204s/iter; left time: 211.4513s
Epoch: 60 cost time: 5.988609313964844
Epoch: 60, Steps: 258 | Train Loss: 0.4957451 Vali Loss: 0.2616202 Test Loss: 0.3530096
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.6536399
	speed: 0.0902s/iter; left time: 921.5677s
	iters: 200, epoch: 61 | loss: 0.6099765
	speed: 0.0207s/iter; left time: 209.6827s
Epoch: 61 cost time: 6.005913972854614
Epoch: 61, Steps: 258 | Train Loss: 0.4952832 Vali Loss: 0.2614686 Test Loss: 0.3530020
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.3365102
	speed: 0.0879s/iter; left time: 875.7765s
	iters: 200, epoch: 62 | loss: 0.6698690
	speed: 0.0212s/iter; left time: 209.4105s
Epoch: 62 cost time: 5.992374420166016
Epoch: 62, Steps: 258 | Train Loss: 0.4952612 Vali Loss: 0.2616536 Test Loss: 0.3529931
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.3231987
	speed: 0.0938s/iter; left time: 909.9401s
	iters: 200, epoch: 63 | loss: 0.4137807
	speed: 0.0209s/iter; left time: 200.3234s
Epoch: 63 cost time: 6.336876630783081
Epoch: 63, Steps: 258 | Train Loss: 0.4954844 Vali Loss: 0.2615646 Test Loss: 0.3530004
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.3450349
	speed: 0.0911s/iter; left time: 861.0720s
	iters: 200, epoch: 64 | loss: 0.3209601
	speed: 0.0210s/iter; left time: 196.1495s
Epoch: 64 cost time: 6.051681756973267
Epoch: 64, Steps: 258 | Train Loss: 0.4951186 Vali Loss: 0.2616799 Test Loss: 0.3529943
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.5397136
	speed: 0.0942s/iter; left time: 865.9110s
	iters: 200, epoch: 65 | loss: 0.5280194
	speed: 0.0201s/iter; left time: 182.5358s
Epoch: 65 cost time: 5.936608791351318
Epoch: 65, Steps: 258 | Train Loss: 0.4955187 Vali Loss: 0.2614985 Test Loss: 0.3529918
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.4920605
	speed: 0.0908s/iter; left time: 811.0240s
	iters: 200, epoch: 66 | loss: 0.6624519
	speed: 0.0207s/iter; left time: 182.6937s
Epoch: 66 cost time: 6.121193170547485
Epoch: 66, Steps: 258 | Train Loss: 0.4951589 Vali Loss: 0.2615334 Test Loss: 0.3529957
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.5551351
	speed: 0.0884s/iter; left time: 766.4733s
	iters: 200, epoch: 67 | loss: 0.4255582
	speed: 0.0209s/iter; left time: 179.0155s
Epoch: 67 cost time: 6.088109731674194
Epoch: 67, Steps: 258 | Train Loss: 0.4953617 Vali Loss: 0.2613142 Test Loss: 0.3529924
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.34965887665748596, mae:0.3786144256591797, rse:0.4752987325191498, corr:[0.53657633 0.54113686 0.53995985 0.5371964  0.5351145  0.5343784
 0.5347806  0.5356546  0.5362872  0.5363215  0.53584665 0.53517246
 0.5346349  0.53438956 0.5344788  0.5346502  0.5346753  0.5343854
 0.53372234 0.53280544 0.5318841  0.53110695 0.5305982  0.5303781
 0.5303024  0.5302134  0.52996266 0.52950805 0.5288891  0.5282211
 0.5275955  0.5271509  0.52686715 0.5266057  0.5262796  0.5258442
 0.52525556 0.5245747  0.5238449  0.5231489  0.5225412  0.5220121
 0.5215731  0.5211707  0.5207581  0.52024376 0.51961446 0.51890475
 0.51809376 0.5171792  0.5162359  0.5153864  0.51465875 0.5140578
 0.5135718  0.5131746  0.5128102  0.51243824 0.5120638  0.5116859
 0.5113453  0.51108783 0.5109164  0.5107691  0.51058626 0.510378
 0.510057   0.5096955  0.5093276  0.5089708  0.5086302  0.5083048
 0.5079893  0.5076502  0.50723    0.50672394 0.506156   0.50553995
 0.5049354  0.50435936 0.5037607  0.5031574  0.50255924 0.5019644
 0.5013906  0.5008572  0.5003341  0.49986395 0.4994213  0.49899164
 0.49856448 0.4980865  0.49745142 0.49659643 0.49547848 0.49411446
 0.49260414 0.49112844 0.48974377 0.48848265 0.48736    0.48634356
 0.4853776  0.48439947 0.48335695 0.48225307 0.48113716 0.48008853
 0.47910744 0.47819754 0.4773471  0.4765127  0.47572598 0.4749277
 0.4740679  0.47317114 0.47229117 0.47143364 0.47064385 0.46986955
 0.4691356  0.46837604 0.46759284 0.4667408  0.46578735 0.46473095
 0.4636571  0.46263328 0.46171305 0.46090898 0.46021298 0.45955542
 0.45890406 0.4582128  0.45748717 0.45671877 0.45590612 0.45512024
 0.4543999  0.45377898 0.453234   0.4527304  0.45222795 0.45161432
 0.45085135 0.4499011  0.44891956 0.44799364 0.44721502 0.44657993
 0.44608605 0.44568354 0.44523457 0.44466555 0.44398525 0.44327506
 0.442618   0.44206455 0.44159293 0.44123966 0.44090778 0.44054678
 0.44010192 0.4395959  0.4390671  0.43856722 0.43810222 0.43768337
 0.43731928 0.43701512 0.43667093 0.43627903 0.4357953  0.43524754
 0.43466118 0.43408453 0.43353558 0.43301025 0.43246773 0.4319385
 0.43137756 0.43078002 0.43019235 0.42962366 0.42906988 0.42850852
 0.4279554  0.42733833 0.42661646 0.42570475 0.4245664  0.42317963
 0.42162535 0.4201408  0.41871125 0.41733676 0.41606602 0.41487932
 0.41378874 0.4127508  0.4117161  0.41063872 0.4094875  0.40827113
 0.40703    0.4058048  0.40460888 0.40345058 0.40241748 0.40158314
 0.40081927 0.40001667 0.39911896 0.398122   0.39708766 0.39599064
 0.39484563 0.3936239  0.39241764 0.39125672 0.3901504  0.38911617
 0.38815966 0.38729617 0.3865639  0.38582966 0.38502175 0.38415962
 0.3832053  0.38224033 0.38131195 0.38047343 0.37972167 0.3790632
 0.37844673 0.37788993 0.37737465 0.37689108 0.3764606  0.3760523
 0.37565875 0.3752124  0.3747109  0.37430045 0.37393993 0.3736841
 0.37352434 0.373439   0.3733503  0.37319285 0.37301964 0.37283978
 0.37266627 0.3725526  0.37242877 0.37227356 0.37202427 0.37170124
 0.37130153 0.3708498  0.3703626  0.36991066 0.36951172 0.36924762
 0.36910564 0.36899772 0.3688852  0.36871937 0.36844066 0.36801398
 0.36752453 0.36702526 0.3665268  0.3660847  0.36567897 0.36527702
 0.36491263 0.3645262  0.36410528 0.36375338 0.363476   0.36331248
 0.36320823 0.36310115 0.3628967  0.36247626 0.36178234 0.3607355
 0.35946512 0.3582267  0.35714763 0.35627678 0.35564095 0.35516185
 0.35483652 0.35455438 0.35418856 0.3536661  0.35300654 0.35219702
 0.35136023 0.35051176 0.34978423 0.34913185 0.34860873 0.34818527
 0.3478737  0.3475515  0.34717277 0.3467358  0.3462628  0.34589228
 0.3455779  0.34536734 0.34518707 0.34495696 0.34466258 0.34432232
 0.34391183 0.3435057  0.3431932  0.3430011  0.3429244  0.34296924
 0.3430898  0.3432874  0.34351572 0.3436474  0.34368613 0.34359542
 0.34346437 0.34328735 0.3431007  0.34295076 0.34277767 0.3426143
 0.34241584 0.34208554 0.34169883 0.34139067 0.34122807 0.3412009
 0.34131056 0.34150913 0.3417168  0.34183356 0.34180766 0.3416791
 0.3414731  0.3412657  0.34108448 0.34091583 0.3407536  0.3405511
 0.34028575 0.33998984 0.33963832 0.33934268 0.33905    0.33882153
 0.33866486 0.33856228 0.33847225 0.33835667 0.3381596  0.33792892
 0.33763057 0.33734456 0.33709964 0.336959   0.3369751  0.33710155
 0.33730355 0.33755705 0.33779338 0.33793685 0.3379322  0.3378202
 0.33767116 0.3375104  0.33730704 0.33696994 0.3365623  0.33600903
 0.33538696 0.33473668 0.33409652 0.33344296 0.33271945 0.33201128
 0.3312994  0.33060974 0.32996395 0.3293769  0.328858   0.3283541
 0.32784605 0.32736614 0.3268341  0.3262504  0.32571593 0.32520103
 0.3247124  0.3242208  0.32375446 0.3233509  0.32292533 0.32249266
 0.322028   0.32160512 0.32125208 0.32095587 0.32075652 0.3206132
 0.32056543 0.32061046 0.32066202 0.32067463 0.3206009  0.32047504
 0.32038158 0.32036677 0.32040855 0.3204937  0.32060248 0.32070398
 0.32079723 0.32084772 0.3208921  0.32093328 0.32099771 0.32108128
 0.32114545 0.32108584 0.32089657 0.32061622 0.32032493 0.32004282
 0.31973705 0.31951532 0.31933767 0.3191377  0.31890705 0.31870142
 0.3184542  0.31820732 0.3179478  0.3176305  0.3172811  0.3168713
 0.3164456  0.31601447 0.3156302  0.31525606 0.31493056 0.31470385
 0.3145699  0.3145102  0.31446517 0.31437758 0.3142632  0.31402835
 0.31369215 0.31327802 0.31283352 0.31240392 0.31200504 0.31166062
 0.3113846  0.31114885 0.31088814 0.31053957 0.31012684 0.30962473
 0.30905703 0.30838358 0.30767456 0.30689982 0.30608934 0.30521587
 0.30425373 0.30326995 0.30221668 0.30110446 0.29998156 0.298882
 0.29781616 0.29686728 0.2960378  0.29539338 0.2948637  0.2943491
 0.29376552 0.29314741 0.29249036 0.29174748 0.29095635 0.2901436
 0.28935403 0.2885249  0.28764609 0.28681192 0.28603622 0.28543225
 0.2849347  0.2844946  0.2841028  0.28370515 0.28332987 0.2829239
 0.28248826 0.28203386 0.28156468 0.2811176  0.28075483 0.28049204
 0.28028214 0.28003937 0.27984038 0.27961424 0.27931568 0.27896416
 0.27858704 0.27826557 0.2780222  0.27789098 0.2778906  0.2779477
 0.27793494 0.27780628 0.27752358 0.27712706 0.2767255  0.2763857
 0.27611798 0.27593145 0.27589813 0.27594775 0.2760158  0.2760349
 0.27595302 0.27576983 0.27546206 0.2751019  0.2747162  0.2743308
 0.27398556 0.27375457 0.27363762 0.27359647 0.2735246  0.27340785
 0.27317855 0.2728758  0.27255282 0.27228597 0.2719963  0.27175426
 0.2715625  0.2714597  0.27137715 0.27125025 0.27107027 0.27085465
 0.27063233 0.27041915 0.2702498  0.27011645 0.26999846 0.26987275
 0.26969296 0.26939148 0.26885968 0.26808643 0.26711273 0.26592585
 0.26460108 0.2633895  0.2623617  0.26151    0.26071241 0.2598886
 0.25900808 0.25809452 0.2572104  0.256396   0.25566292 0.25503206
 0.25444996 0.25382707 0.25319526 0.25253037 0.25184625 0.2512094
 0.25063276 0.25008097 0.249603   0.24914137 0.24874337 0.24842882
 0.2481802  0.24802965 0.24790058 0.24773315 0.24757569 0.2474287
 0.24721572 0.24694294 0.24668014 0.24638265 0.24613063 0.24597062
 0.2460018  0.24608125 0.24620686 0.24639413 0.2465794  0.24668156
 0.24668734 0.24669454 0.24675277 0.24694508 0.24733165 0.24790417
 0.24851377 0.24897368 0.24916624 0.24915293 0.24901086 0.24884029
 0.2486761  0.24859239 0.24865508 0.24868104 0.2487721  0.2488419
 0.24885824 0.24882786 0.24868184 0.24854843 0.248311   0.24803405
 0.24770635 0.24747843 0.24730316 0.24707429 0.24691299 0.24665605
 0.24637027 0.24603657 0.24571353 0.24543902 0.24532428 0.24523224
 0.24524465 0.24525943 0.24515823 0.24504937 0.24498796 0.24495344
 0.24493289 0.24507144 0.24527799 0.24551767 0.24581851 0.24608853
 0.2462243  0.2462383  0.2460898  0.24565172 0.24490446 0.24393594
 0.24285054 0.24174404 0.24065931 0.23958404 0.23859827 0.23768258
 0.23682809 0.23603497 0.23553157 0.2352122  0.23504458 0.234886
 0.23463967 0.2342663  0.2337721  0.23317674 0.23264253 0.23211
 0.23160739 0.23109384 0.23049434 0.22998857 0.22952825 0.22908701
 0.22869201 0.22829537 0.22795857 0.22755162 0.22707689 0.22650434
 0.22597072 0.22534074 0.22469875 0.22415341 0.22375515 0.22351149
 0.22324234 0.22283813 0.22212164 0.22115946 0.21988249 0.21867283
 0.21796344 0.2180997  0.21901447 0.21997781 0.21944101 0.21506235]
