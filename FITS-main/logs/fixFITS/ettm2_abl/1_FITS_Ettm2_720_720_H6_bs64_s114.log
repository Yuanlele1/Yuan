Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=58, out_features=116, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6028288.0
params:  6844.0
Trainable parameters:  6844
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5858247
	speed: 0.0305s/iter; left time: 784.7270s
	iters: 200, epoch: 1 | loss: 0.4127401
	speed: 0.0212s/iter; left time: 542.2480s
Epoch: 1 cost time: 6.439740896224976
Epoch: 1, Steps: 258 | Train Loss: 0.5873628 Vali Loss: 0.2827148 Test Loss: 0.3772241
Validation loss decreased (inf --> 0.282715).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5300923
	speed: 0.0921s/iter; left time: 2344.3226s
	iters: 200, epoch: 2 | loss: 0.6720407
	speed: 0.0213s/iter; left time: 538.7876s
Epoch: 2 cost time: 6.092255592346191
Epoch: 2, Steps: 258 | Train Loss: 0.5221293 Vali Loss: 0.2721822 Test Loss: 0.3659631
Validation loss decreased (0.282715 --> 0.272182).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4217879
	speed: 0.0909s/iter; left time: 2289.0207s
	iters: 200, epoch: 3 | loss: 0.4274647
	speed: 0.0207s/iter; left time: 518.3874s
Epoch: 3 cost time: 5.964341640472412
Epoch: 3, Steps: 258 | Train Loss: 0.5122908 Vali Loss: 0.2690170 Test Loss: 0.3614881
Validation loss decreased (0.272182 --> 0.269017).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4423734
	speed: 0.0909s/iter; left time: 2264.6880s
	iters: 200, epoch: 4 | loss: 0.4878687
	speed: 0.0209s/iter; left time: 518.1319s
Epoch: 4 cost time: 5.9924585819244385
Epoch: 4, Steps: 258 | Train Loss: 0.5072880 Vali Loss: 0.2670425 Test Loss: 0.3591234
Validation loss decreased (0.269017 --> 0.267042).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4392911
	speed: 0.0884s/iter; left time: 2179.6937s
	iters: 200, epoch: 5 | loss: 0.4308756
	speed: 0.0214s/iter; left time: 524.9925s
Epoch: 5 cost time: 6.0830793380737305
Epoch: 5, Steps: 258 | Train Loss: 0.5046070 Vali Loss: 0.2653105 Test Loss: 0.3579697
Validation loss decreased (0.267042 --> 0.265310).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5504990
	speed: 0.0923s/iter; left time: 2254.2232s
	iters: 200, epoch: 6 | loss: 0.4859658
	speed: 0.0214s/iter; left time: 521.2541s
Epoch: 6 cost time: 6.292764902114868
Epoch: 6, Steps: 258 | Train Loss: 0.5030495 Vali Loss: 0.2651141 Test Loss: 0.3564975
Validation loss decreased (0.265310 --> 0.265114).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3921995
	speed: 0.0913s/iter; left time: 2205.9855s
	iters: 200, epoch: 7 | loss: 0.5406060
	speed: 0.0204s/iter; left time: 491.7024s
Epoch: 7 cost time: 6.203086614608765
Epoch: 7, Steps: 258 | Train Loss: 0.5015565 Vali Loss: 0.2642481 Test Loss: 0.3560246
Validation loss decreased (0.265114 --> 0.264248).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5322501
	speed: 0.0940s/iter; left time: 2245.0083s
	iters: 200, epoch: 8 | loss: 0.6288977
	speed: 0.0216s/iter; left time: 514.9436s
Epoch: 8 cost time: 6.096928358078003
Epoch: 8, Steps: 258 | Train Loss: 0.5008677 Vali Loss: 0.2639375 Test Loss: 0.3555692
Validation loss decreased (0.264248 --> 0.263938).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4510163
	speed: 0.0919s/iter; left time: 2171.8499s
	iters: 200, epoch: 9 | loss: 0.4960947
	speed: 0.0218s/iter; left time: 512.4713s
Epoch: 9 cost time: 6.276542663574219
Epoch: 9, Steps: 258 | Train Loss: 0.4999409 Vali Loss: 0.2635351 Test Loss: 0.3551425
Validation loss decreased (0.263938 --> 0.263535).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4293133
	speed: 0.0935s/iter; left time: 2185.2418s
	iters: 200, epoch: 10 | loss: 0.3402053
	speed: 0.0205s/iter; left time: 477.7828s
Epoch: 10 cost time: 6.028265953063965
Epoch: 10, Steps: 258 | Train Loss: 0.4996634 Vali Loss: 0.2633449 Test Loss: 0.3547379
Validation loss decreased (0.263535 --> 0.263345).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5144936
	speed: 0.0957s/iter; left time: 2213.6992s
	iters: 200, epoch: 11 | loss: 0.4216994
	speed: 0.0222s/iter; left time: 511.9709s
Epoch: 11 cost time: 6.265098333358765
Epoch: 11, Steps: 258 | Train Loss: 0.4998639 Vali Loss: 0.2634860 Test Loss: 0.3545171
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4201303
	speed: 0.0928s/iter; left time: 2121.5421s
	iters: 200, epoch: 12 | loss: 0.4745990
	speed: 0.0204s/iter; left time: 463.9078s
Epoch: 12 cost time: 6.055208683013916
Epoch: 12, Steps: 258 | Train Loss: 0.4986769 Vali Loss: 0.2632599 Test Loss: 0.3544638
Validation loss decreased (0.263345 --> 0.263260).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4954790
	speed: 0.0906s/iter; left time: 2047.6299s
	iters: 200, epoch: 13 | loss: 0.4277168
	speed: 0.0219s/iter; left time: 492.3243s
Epoch: 13 cost time: 6.116800785064697
Epoch: 13, Steps: 258 | Train Loss: 0.4989220 Vali Loss: 0.2632937 Test Loss: 0.3541583
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6042002
	speed: 0.0898s/iter; left time: 2007.4700s
	iters: 200, epoch: 14 | loss: 0.5635893
	speed: 0.0214s/iter; left time: 477.0900s
Epoch: 14 cost time: 6.170480966567993
Epoch: 14, Steps: 258 | Train Loss: 0.4982076 Vali Loss: 0.2629480 Test Loss: 0.3541920
Validation loss decreased (0.263260 --> 0.262948).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4653621
	speed: 0.0903s/iter; left time: 1994.5276s
	iters: 200, epoch: 15 | loss: 0.4589261
	speed: 0.0205s/iter; left time: 450.8387s
Epoch: 15 cost time: 6.130569934844971
Epoch: 15, Steps: 258 | Train Loss: 0.4978374 Vali Loss: 0.2626604 Test Loss: 0.3541270
Validation loss decreased (0.262948 --> 0.262660).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4132890
	speed: 0.0898s/iter; left time: 1961.2841s
	iters: 200, epoch: 16 | loss: 0.4916534
	speed: 0.0205s/iter; left time: 444.4546s
Epoch: 16 cost time: 5.98237943649292
Epoch: 16, Steps: 258 | Train Loss: 0.4983201 Vali Loss: 0.2630385 Test Loss: 0.3537227
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5767421
	speed: 0.0873s/iter; left time: 1882.4637s
	iters: 200, epoch: 17 | loss: 0.3549196
	speed: 0.0212s/iter; left time: 456.1544s
Epoch: 17 cost time: 6.203353404998779
Epoch: 17, Steps: 258 | Train Loss: 0.4978161 Vali Loss: 0.2626760 Test Loss: 0.3538933
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6038935
	speed: 0.0906s/iter; left time: 1931.2045s
	iters: 200, epoch: 18 | loss: 0.4504933
	speed: 0.0211s/iter; left time: 446.8526s
Epoch: 18 cost time: 6.09844446182251
Epoch: 18, Steps: 258 | Train Loss: 0.4973052 Vali Loss: 0.2622978 Test Loss: 0.3538991
Validation loss decreased (0.262660 --> 0.262298).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4622907
	speed: 0.0900s/iter; left time: 1895.0773s
	iters: 200, epoch: 19 | loss: 0.4150719
	speed: 0.0204s/iter; left time: 427.4566s
Epoch: 19 cost time: 5.939598798751831
Epoch: 19, Steps: 258 | Train Loss: 0.4972829 Vali Loss: 0.2625420 Test Loss: 0.3537213
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3597006
	speed: 0.0940s/iter; left time: 1954.8869s
	iters: 200, epoch: 20 | loss: 0.3413760
	speed: 0.0213s/iter; left time: 441.1221s
Epoch: 20 cost time: 6.278917551040649
Epoch: 20, Steps: 258 | Train Loss: 0.4979251 Vali Loss: 0.2627150 Test Loss: 0.3535968
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5645964
	speed: 0.0941s/iter; left time: 1933.8645s
	iters: 200, epoch: 21 | loss: 0.5882059
	speed: 0.0215s/iter; left time: 438.9421s
Epoch: 21 cost time: 6.205726385116577
Epoch: 21, Steps: 258 | Train Loss: 0.4980950 Vali Loss: 0.2625697 Test Loss: 0.3536178
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5308676
	speed: 0.0911s/iter; left time: 1848.7694s
	iters: 200, epoch: 22 | loss: 0.4995556
	speed: 0.0215s/iter; left time: 434.6905s
Epoch: 22 cost time: 6.267530918121338
Epoch: 22, Steps: 258 | Train Loss: 0.4977064 Vali Loss: 0.2628110 Test Loss: 0.3534913
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4261612
	speed: 0.0927s/iter; left time: 1856.4182s
	iters: 200, epoch: 23 | loss: 0.5798346
	speed: 0.0219s/iter; left time: 436.5907s
Epoch: 23 cost time: 6.12137508392334
Epoch: 23, Steps: 258 | Train Loss: 0.4970498 Vali Loss: 0.2623077 Test Loss: 0.3536275
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3941826
	speed: 0.0893s/iter; left time: 1765.9741s
	iters: 200, epoch: 24 | loss: 0.3307531
	speed: 0.0200s/iter; left time: 392.3756s
Epoch: 24 cost time: 5.960628271102905
Epoch: 24, Steps: 258 | Train Loss: 0.4973148 Vali Loss: 0.2625064 Test Loss: 0.3535164
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4708604
	speed: 0.0924s/iter; left time: 1801.8096s
	iters: 200, epoch: 25 | loss: 0.4065584
	speed: 0.0208s/iter; left time: 404.1573s
Epoch: 25 cost time: 5.962144613265991
Epoch: 25, Steps: 258 | Train Loss: 0.4976230 Vali Loss: 0.2626729 Test Loss: 0.3535132
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.5471117
	speed: 0.0901s/iter; left time: 1735.1468s
	iters: 200, epoch: 26 | loss: 0.5581688
	speed: 0.0209s/iter; left time: 399.6209s
Epoch: 26 cost time: 6.0124192237854
Epoch: 26, Steps: 258 | Train Loss: 0.4968390 Vali Loss: 0.2623514 Test Loss: 0.3535180
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5879267
	speed: 0.0892s/iter; left time: 1695.0165s
	iters: 200, epoch: 27 | loss: 0.6442170
	speed: 0.0205s/iter; left time: 387.3178s
Epoch: 27 cost time: 5.910464763641357
Epoch: 27, Steps: 258 | Train Loss: 0.4973806 Vali Loss: 0.2620027 Test Loss: 0.3535412
Validation loss decreased (0.262298 --> 0.262003).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5990611
	speed: 0.0898s/iter; left time: 1682.0527s
	iters: 200, epoch: 28 | loss: 0.5037427
	speed: 0.0215s/iter; left time: 399.7752s
Epoch: 28 cost time: 6.200559377670288
Epoch: 28, Steps: 258 | Train Loss: 0.4973712 Vali Loss: 0.2625267 Test Loss: 0.3534937
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.8166088
	speed: 0.0970s/iter; left time: 1793.1092s
	iters: 200, epoch: 29 | loss: 0.5792223
	speed: 0.0217s/iter; left time: 398.2057s
Epoch: 29 cost time: 6.356790065765381
Epoch: 29, Steps: 258 | Train Loss: 0.4965758 Vali Loss: 0.2624876 Test Loss: 0.3534454
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.5428582
	speed: 0.0947s/iter; left time: 1724.7918s
	iters: 200, epoch: 30 | loss: 0.4239655
	speed: 0.0205s/iter; left time: 371.2631s
Epoch: 30 cost time: 6.0383994579315186
Epoch: 30, Steps: 258 | Train Loss: 0.4971349 Vali Loss: 0.2624423 Test Loss: 0.3533994
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4005176
	speed: 0.0949s/iter; left time: 1704.8386s
	iters: 200, epoch: 31 | loss: 0.4523918
	speed: 0.0208s/iter; left time: 371.8870s
Epoch: 31 cost time: 6.118854999542236
Epoch: 31, Steps: 258 | Train Loss: 0.4966351 Vali Loss: 0.2623063 Test Loss: 0.3533882
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.5030916
	speed: 0.0933s/iter; left time: 1651.8453s
	iters: 200, epoch: 32 | loss: 0.3814252
	speed: 0.0208s/iter; left time: 365.6244s
Epoch: 32 cost time: 6.182941675186157
Epoch: 32, Steps: 258 | Train Loss: 0.4964478 Vali Loss: 0.2624135 Test Loss: 0.3533608
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4438706
	speed: 0.0910s/iter; left time: 1588.2841s
	iters: 200, epoch: 33 | loss: 0.6764761
	speed: 0.0208s/iter; left time: 361.0407s
Epoch: 33 cost time: 6.139731407165527
Epoch: 33, Steps: 258 | Train Loss: 0.4971664 Vali Loss: 0.2624065 Test Loss: 0.3533708
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4330203
	speed: 0.0910s/iter; left time: 1564.1000s
	iters: 200, epoch: 34 | loss: 0.5726250
	speed: 0.0215s/iter; left time: 367.2111s
Epoch: 34 cost time: 6.118960618972778
Epoch: 34, Steps: 258 | Train Loss: 0.4974485 Vali Loss: 0.2624824 Test Loss: 0.3533604
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4484027
	speed: 0.0914s/iter; left time: 1547.8820s
	iters: 200, epoch: 35 | loss: 0.4109158
	speed: 0.0213s/iter; left time: 358.9509s
Epoch: 35 cost time: 5.966044902801514
Epoch: 35, Steps: 258 | Train Loss: 0.4964996 Vali Loss: 0.2622859 Test Loss: 0.3533578
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4621344
	speed: 0.0882s/iter; left time: 1470.1389s
	iters: 200, epoch: 36 | loss: 0.3768031
	speed: 0.0212s/iter; left time: 350.6088s
Epoch: 36 cost time: 6.031994104385376
Epoch: 36, Steps: 258 | Train Loss: 0.4968698 Vali Loss: 0.2623825 Test Loss: 0.3533500
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.5040538
	speed: 0.0886s/iter; left time: 1454.6029s
	iters: 200, epoch: 37 | loss: 0.4503364
	speed: 0.0212s/iter; left time: 345.6281s
Epoch: 37 cost time: 5.958753347396851
Epoch: 37, Steps: 258 | Train Loss: 0.4969285 Vali Loss: 0.2625110 Test Loss: 0.3533492
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4353089
	speed: 0.0935s/iter; left time: 1510.2750s
	iters: 200, epoch: 38 | loss: 0.5849792
	speed: 0.0207s/iter; left time: 333.1146s
Epoch: 38 cost time: 6.098076105117798
Epoch: 38, Steps: 258 | Train Loss: 0.4962936 Vali Loss: 0.2624058 Test Loss: 0.3533185
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4132593
	speed: 0.0905s/iter; left time: 1439.2037s
	iters: 200, epoch: 39 | loss: 0.3965793
	speed: 0.0211s/iter; left time: 332.8506s
Epoch: 39 cost time: 6.051546812057495
Epoch: 39, Steps: 258 | Train Loss: 0.4968947 Vali Loss: 0.2624372 Test Loss: 0.3533502
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4293623
	speed: 0.0919s/iter; left time: 1436.8082s
	iters: 200, epoch: 40 | loss: 0.4288380
	speed: 0.0208s/iter; left time: 322.8310s
Epoch: 40 cost time: 6.140677213668823
Epoch: 40, Steps: 258 | Train Loss: 0.4967715 Vali Loss: 0.2621468 Test Loss: 0.3533151
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4938319
	speed: 0.0893s/iter; left time: 1374.1489s
	iters: 200, epoch: 41 | loss: 0.4810968
	speed: 0.0216s/iter; left time: 329.7386s
Epoch: 41 cost time: 5.973739147186279
Epoch: 41, Steps: 258 | Train Loss: 0.4973133 Vali Loss: 0.2623649 Test Loss: 0.3533343
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4600312
	speed: 0.0913s/iter; left time: 1381.2567s
	iters: 200, epoch: 42 | loss: 0.5668566
	speed: 0.0205s/iter; left time: 308.3106s
Epoch: 42 cost time: 6.045154094696045
Epoch: 42, Steps: 258 | Train Loss: 0.4968404 Vali Loss: 0.2623756 Test Loss: 0.3533057
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4694429
	speed: 0.0913s/iter; left time: 1356.6589s
	iters: 200, epoch: 43 | loss: 0.4946816
	speed: 0.0217s/iter; left time: 320.4135s
Epoch: 43 cost time: 6.295927047729492
Epoch: 43, Steps: 258 | Train Loss: 0.4968396 Vali Loss: 0.2623771 Test Loss: 0.3533064
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3985432
	speed: 0.0905s/iter; left time: 1322.1105s
	iters: 200, epoch: 44 | loss: 0.3245902
	speed: 0.0209s/iter; left time: 303.8832s
Epoch: 44 cost time: 6.0736846923828125
Epoch: 44, Steps: 258 | Train Loss: 0.4967476 Vali Loss: 0.2621814 Test Loss: 0.3532869
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.6131015
	speed: 0.0930s/iter; left time: 1335.0010s
	iters: 200, epoch: 45 | loss: 0.3905120
	speed: 0.0204s/iter; left time: 291.1563s
Epoch: 45 cost time: 6.024872303009033
Epoch: 45, Steps: 258 | Train Loss: 0.4965260 Vali Loss: 0.2623241 Test Loss: 0.3532692
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.3963261
	speed: 0.0903s/iter; left time: 1271.7160s
	iters: 200, epoch: 46 | loss: 0.4726442
	speed: 0.0214s/iter; left time: 299.2119s
Epoch: 46 cost time: 6.096170425415039
Epoch: 46, Steps: 258 | Train Loss: 0.4962911 Vali Loss: 0.2623250 Test Loss: 0.3532928
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4671605
	speed: 0.0913s/iter; left time: 1263.0978s
	iters: 200, epoch: 47 | loss: 0.4377917
	speed: 0.0215s/iter; left time: 295.6009s
Epoch: 47 cost time: 6.275269031524658
Epoch: 47, Steps: 258 | Train Loss: 0.4963194 Vali Loss: 0.2625355 Test Loss: 0.3532752
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_720_720_FITS_ETTm2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.35017260909080505, mae:0.37897607684135437, rse:0.47564780712127686, corr:[0.5304497  0.5364614  0.5392115  0.53891134 0.53718466 0.53542084
 0.5342825  0.5339285  0.53427476 0.53509164 0.5360269  0.5366948
 0.53686875 0.53649104 0.53573966 0.5347723  0.5338224  0.5330636
 0.53255206 0.53225994 0.53214884 0.53208137 0.531962   0.53173083
 0.5313113  0.53072536 0.5300308  0.5293223  0.5286669  0.5281205
 0.5276808  0.5273918  0.5271954  0.52695507 0.5266074  0.5261389
 0.5255319  0.52484566 0.524105   0.5233667  0.5226703  0.52201253
 0.52141786 0.5208804  0.52039677 0.5199195  0.5194315  0.51893103
 0.51836145 0.5176631  0.51684934 0.515998   0.515135   0.5143114
 0.51358676 0.51301014 0.5125834  0.5122843  0.51208466 0.5119141
 0.5117475  0.51155937 0.5113379  0.5110577  0.51070577 0.5103423
 0.5099282  0.509547   0.509229   0.5089704  0.5087368  0.5084923
 0.5082172  0.5078833  0.50745064 0.50692093 0.5063171  0.5056488
 0.50497335 0.5043045  0.50361466 0.50293636 0.5023067  0.50173676
 0.5012369  0.50079805 0.5003654  0.4999454  0.49948078 0.49895334
 0.49836615 0.4977123  0.49694988 0.49606067 0.49501452 0.49381405
 0.49250007 0.49117213 0.48982593 0.4884632  0.4871053  0.48577404
 0.48449    0.48327053 0.4821267  0.48108461 0.4801616  0.47937724
 0.47866538 0.47797015 0.47723934 0.47642162 0.47556388 0.47465396
 0.4736912  0.472725   0.47182003 0.47098085 0.4702328  0.46951544
 0.46882543 0.46809405 0.46731567 0.4664422  0.46543995 0.46429926
 0.4630972  0.46190894 0.46081606 0.45986542 0.4590958  0.4584705
 0.45795572 0.45748296 0.45700976 0.45646304 0.45577642 0.45496908
 0.4540581  0.4531082  0.4521775  0.4513413  0.45066252 0.45009074
 0.44958055 0.4490186  0.4484287  0.44776338 0.44702974 0.44621003
 0.44535697 0.4445328  0.44373533 0.4429972  0.44236028 0.44186947
 0.4415255  0.4412827  0.44104683 0.44080484 0.4404722  0.4400297
 0.43946898 0.438848   0.43823332 0.43769503 0.43725646 0.43692455
 0.43668732 0.4365206  0.43631548 0.43603882 0.43561313 0.43503958
 0.43433014 0.43353626 0.4327114  0.4319092  0.43117774 0.43060115
 0.43017146 0.42987236 0.42967618 0.42949736 0.42923805 0.42879778
 0.42815098 0.42723933 0.4260959  0.42474645 0.4232749  0.42175066
 0.420276   0.41904074 0.41795725 0.4169266  0.4158936  0.4147662
 0.41352352 0.41214758 0.41065508 0.4090969  0.40754515 0.40608937
 0.40481085 0.40374488 0.40285823 0.40207878 0.4013952  0.4008139
 0.40018347 0.39939037 0.39837825 0.397169   0.3958565  0.39447787
 0.39310336 0.3917481  0.3905073  0.38940027 0.38840273 0.3874775
 0.38657856 0.3856821  0.38481697 0.38388428 0.38285896 0.38181388
 0.38076544 0.37981477 0.37900606 0.37836695 0.37785643 0.3774398
 0.37703133 0.3766159  0.37618268 0.37573862 0.37533718 0.3749865
 0.37469885 0.3744146  0.3740988  0.37382874 0.37352812 0.37321892
 0.37290263 0.37259334 0.37227243 0.37193996 0.37166765 0.3714814
 0.37139213 0.371427   0.37151158 0.3715992  0.37161002 0.37152714
 0.37132427 0.37100103 0.370552   0.37003174 0.36946827 0.3689667
 0.36856684 0.36825588 0.36803734 0.36787876 0.36770347 0.36743307
 0.36708048 0.36663225 0.36606953 0.36545816 0.36483252 0.36424503
 0.36379385 0.36347416 0.3632712  0.36321852 0.36323276 0.36326328
 0.36320475 0.3629888  0.3625585  0.36186653 0.36093536 0.35975105
 0.35845694 0.357257   0.35623217 0.35538512 0.35470602 0.35410807
 0.35359567 0.3530943  0.35252768 0.35185248 0.35109457 0.35023943
 0.34938738 0.34853587 0.34780324 0.3471699  0.3467023  0.3463932
 0.34627238 0.34622517 0.34616965 0.34605122 0.34582758 0.34557396
 0.34524578 0.34491146 0.34456053 0.34418765 0.3438205  0.34348413
 0.34313124 0.34277648 0.3424325  0.34208086 0.34170812 0.34135687
 0.34105968 0.3409115  0.34094402 0.3410921  0.3413468  0.3416235
 0.34191197 0.3421157  0.3421981  0.34216624 0.34197465 0.3416938
 0.3413485  0.34091136 0.34047946 0.3401707  0.34003517 0.34003454
 0.34014502 0.34029883 0.34042302 0.34042004 0.34024653 0.33994067
 0.33953616 0.339122   0.33875754 0.33848488 0.33833897 0.33830014
 0.33833858 0.33843586 0.33849642 0.33853167 0.33841956 0.33816165
 0.33776262 0.33726078 0.33672023 0.33622235 0.3358239  0.33561885
 0.3355793  0.33571237 0.33593577 0.33618748 0.33641398 0.33653194
 0.33651277 0.33640143 0.33622882 0.33602795 0.33582717 0.33569407
 0.33568054 0.33575907 0.3358361  0.33575675 0.3355214  0.33502585
 0.33433607 0.3335097  0.3326308  0.33174393 0.3308565  0.33009213
 0.32944942 0.32893962 0.3285474  0.32823634 0.3279626  0.32763064
 0.32718894 0.32665014 0.32594776 0.3251039  0.32425758 0.32344082
 0.3227199  0.32210982 0.32165527 0.32139042 0.32119587 0.32102782
 0.32079864 0.32050955 0.3201478  0.31970343 0.31923887 0.3187762
 0.31842983 0.31827205 0.31825688 0.31835026 0.3184753  0.31860605
 0.3187498  0.3188969  0.3189949  0.3190348  0.3190308  0.31899855
 0.3189836  0.3189911  0.31906712 0.31920525 0.31940195 0.31962088
 0.319795   0.31980962 0.3196477  0.31933233 0.31895283 0.31856003
 0.31816563 0.31789854 0.3177513  0.3176704  0.31762248 0.3176187
 0.31755322 0.31741974 0.31717914 0.3167897  0.31629336 0.3157083
 0.3151271  0.314601   0.3141974  0.3138798  0.31364468 0.31348976
 0.3133649  0.3132264  0.31302348 0.31272748 0.31239468 0.3119882
 0.31157118 0.31117174 0.31082988 0.31055483 0.31032404 0.31012264
 0.30994296 0.30975407 0.30950722 0.30915484 0.30872607 0.30819687
 0.30758962 0.30685502 0.30605334 0.30516556 0.30423537 0.3032534
 0.30221957 0.30121797 0.30021667 0.29921415 0.29822615 0.29725656
 0.29628992 0.2953828  0.29453495 0.29382166 0.29320273 0.29260606
 0.29196084 0.29130024 0.2906159  0.28986275 0.2890787  0.28830165
 0.28759897 0.28693044 0.28628987 0.28574607 0.28528246 0.2849514
 0.28466582 0.2843542  0.28399456 0.28353944 0.28302708 0.28242752
 0.28177273 0.28109944 0.28043133 0.2798135  0.2793047  0.27892336
 0.2786384  0.27837834 0.27820572 0.27805308 0.2778624  0.27761722
 0.27730238 0.27695596 0.27659014 0.27625352 0.27601266 0.27587083
 0.27577153 0.2757148  0.27566147 0.27560118 0.27556252 0.27552786
 0.27543947 0.2752717  0.27510023 0.27490693 0.27471474 0.27453285
 0.27437234 0.27425262 0.27412364 0.2739952  0.27382088 0.27356446
 0.27321592 0.2728432  0.27249163 0.27219465 0.27193385 0.27174905
 0.2715947  0.2714731  0.27139434 0.27136907 0.27127558 0.27113995
 0.27096194 0.2707967  0.27062503 0.2704308  0.27023733 0.27007273
 0.2699439  0.2698444  0.26977137 0.26968047 0.26953298 0.2693059
 0.2689695  0.26848575 0.26778165 0.2668789  0.26584145 0.2646739
 0.26344824 0.26237446 0.26150778 0.2608389  0.26024517 0.2596201
 0.258894   0.25805506 0.25713715 0.25618386 0.2552452  0.2543844
 0.25360516 0.25286257 0.25220066 0.25158677 0.25100836 0.25049612
 0.25004584 0.24960658 0.24921678 0.2488183  0.24844667 0.24811724
 0.24781156 0.24755758 0.24731019 0.24703626 0.24679917 0.24661534
 0.24643381 0.24625094 0.24610746 0.24592333 0.24572407 0.24551761
 0.24538644 0.24523035 0.24509983 0.2450644  0.24512373 0.24523559
 0.24537264 0.24556823 0.2457907  0.24604335 0.24634044 0.24668118
 0.24698904 0.24717447 0.24718992 0.24712682 0.24704885 0.2469999
 0.24694751 0.2469052  0.24690206 0.24678652 0.24669139 0.24660234
 0.24655467 0.24659179 0.24665393 0.24682069 0.24692133 0.24694292
 0.24680193 0.2466002  0.24630499 0.24586236 0.24544561 0.24498264
 0.24458112 0.24425848 0.24405017 0.24397196 0.24409185 0.24425201
 0.2444974  0.24470685 0.24474382 0.24468654 0.24457537 0.24440496
 0.24419543 0.24410196 0.2440945  0.24419087 0.24443357 0.24474223
 0.24499902 0.24516693 0.24514095 0.24475275 0.24396266 0.24286805
 0.24161327 0.24035853 0.23922838 0.23826756 0.23756014 0.2370501
 0.23664808 0.23625043 0.23597352 0.23568705 0.23537432 0.23496632
 0.23445694 0.23387316 0.23325011 0.23260297 0.23205562 0.23154561
 0.23109515 0.23067681 0.23023152 0.22991323 0.229664   0.22942357
 0.22915708 0.22876255 0.22827068 0.22760804 0.22686903 0.22612736
 0.22557719 0.22510883 0.22472154 0.22436954 0.22394896 0.22339323
 0.22260219 0.221682   0.22075976 0.22015102 0.21986315 0.21999429
 0.2203976  0.22080307 0.22088528 0.22025225 0.21830477 0.21447925]
