Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=16, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34135
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=16, out_features=75, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1075200.0
params:  1275.0
Trainable parameters:  1275
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5841325
	speed: 0.0277s/iter; left time: 733.4559s
	iters: 200, epoch: 1 | loss: 0.5004229
	speed: 0.0153s/iter; left time: 403.0218s
Epoch: 1 cost time: 5.404035568237305
Epoch: 1, Steps: 266 | Train Loss: 0.5511640 Vali Loss: 0.2488032 Test Loss: 0.3453156
Validation loss decreased (inf --> 0.248803).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4478994
	speed: 0.0769s/iter; left time: 2017.2566s
	iters: 200, epoch: 2 | loss: 0.3193502
	speed: 0.0153s/iter; left time: 399.6009s
Epoch: 2 cost time: 4.732501029968262
Epoch: 2, Steps: 266 | Train Loss: 0.4670665 Vali Loss: 0.2254107 Test Loss: 0.3170978
Validation loss decreased (0.248803 --> 0.225411).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4170969
	speed: 0.0729s/iter; left time: 1893.1332s
	iters: 200, epoch: 3 | loss: 0.3376223
	speed: 0.0144s/iter; left time: 372.4780s
Epoch: 3 cost time: 4.435932636260986
Epoch: 3, Steps: 266 | Train Loss: 0.4519398 Vali Loss: 0.2203780 Test Loss: 0.3110379
Validation loss decreased (0.225411 --> 0.220378).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5654927
	speed: 0.0706s/iter; left time: 1813.7622s
	iters: 200, epoch: 4 | loss: 0.5260335
	speed: 0.0147s/iter; left time: 376.2141s
Epoch: 4 cost time: 4.442962408065796
Epoch: 4, Steps: 266 | Train Loss: 0.4469040 Vali Loss: 0.2185925 Test Loss: 0.3090420
Validation loss decreased (0.220378 --> 0.218592).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6120355
	speed: 0.0692s/iter; left time: 1759.7832s
	iters: 200, epoch: 5 | loss: 0.3627515
	speed: 0.0148s/iter; left time: 374.9639s
Epoch: 5 cost time: 5.060011625289917
Epoch: 5, Steps: 266 | Train Loss: 0.4454757 Vali Loss: 0.2182608 Test Loss: 0.3083323
Validation loss decreased (0.218592 --> 0.218261).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6417892
	speed: 0.0762s/iter; left time: 1917.9954s
	iters: 200, epoch: 6 | loss: 0.4353330
	speed: 0.0149s/iter; left time: 372.5533s
Epoch: 6 cost time: 4.530189514160156
Epoch: 6, Steps: 266 | Train Loss: 0.4448112 Vali Loss: 0.2180830 Test Loss: 0.3078420
Validation loss decreased (0.218261 --> 0.218083).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5504061
	speed: 0.0701s/iter; left time: 1745.0006s
	iters: 200, epoch: 7 | loss: 0.3324040
	speed: 0.0144s/iter; left time: 358.1621s
Epoch: 7 cost time: 4.563544511795044
Epoch: 7, Steps: 266 | Train Loss: 0.4429709 Vali Loss: 0.2178517 Test Loss: 0.3076861
Validation loss decreased (0.218083 --> 0.217852).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5503484
	speed: 0.0687s/iter; left time: 1692.2457s
	iters: 200, epoch: 8 | loss: 0.3545314
	speed: 0.0159s/iter; left time: 389.1662s
Epoch: 8 cost time: 4.679676532745361
Epoch: 8, Steps: 266 | Train Loss: 0.4430340 Vali Loss: 0.2181130 Test Loss: 0.3075701
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4122292
	speed: 0.0713s/iter; left time: 1737.0927s
	iters: 200, epoch: 9 | loss: 0.5177590
	speed: 0.0148s/iter; left time: 359.8855s
Epoch: 9 cost time: 4.496779680252075
Epoch: 9, Steps: 266 | Train Loss: 0.4424634 Vali Loss: 0.2180548 Test Loss: 0.3074999
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4197681
	speed: 0.0692s/iter; left time: 1668.0320s
	iters: 200, epoch: 10 | loss: 0.2701190
	speed: 0.0155s/iter; left time: 372.6092s
Epoch: 10 cost time: 4.510814428329468
Epoch: 10, Steps: 266 | Train Loss: 0.4420735 Vali Loss: 0.2181208 Test Loss: 0.3074710
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4017581
	speed: 0.0692s/iter; left time: 1650.8330s
	iters: 200, epoch: 11 | loss: 0.5031250
	speed: 0.0151s/iter; left time: 359.0440s
Epoch: 11 cost time: 4.551046133041382
Epoch: 11, Steps: 266 | Train Loss: 0.4415562 Vali Loss: 0.2182929 Test Loss: 0.3074750
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4243197
	speed: 0.0695s/iter; left time: 1638.4450s
	iters: 200, epoch: 12 | loss: 0.4379399
	speed: 0.0194s/iter; left time: 455.2457s
Epoch: 12 cost time: 5.021280288696289
Epoch: 12, Steps: 266 | Train Loss: 0.4422148 Vali Loss: 0.2183690 Test Loss: 0.3075800
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5251098
	speed: 0.0688s/iter; left time: 1604.7775s
	iters: 200, epoch: 13 | loss: 0.5116338
	speed: 0.0149s/iter; left time: 346.5333s
Epoch: 13 cost time: 4.624488592147827
Epoch: 13, Steps: 266 | Train Loss: 0.4420012 Vali Loss: 0.2184800 Test Loss: 0.3074785
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4033009
	speed: 0.0774s/iter; left time: 1782.4260s
	iters: 200, epoch: 14 | loss: 0.2780057
	speed: 0.0149s/iter; left time: 342.0638s
Epoch: 14 cost time: 4.581421613693237
Epoch: 14, Steps: 266 | Train Loss: 0.4421159 Vali Loss: 0.2182388 Test Loss: 0.3074979
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4206020
	speed: 0.0699s/iter; left time: 1592.0274s
	iters: 200, epoch: 15 | loss: 0.2426632
	speed: 0.0144s/iter; left time: 327.0168s
Epoch: 15 cost time: 4.339841365814209
Epoch: 15, Steps: 266 | Train Loss: 0.4415970 Vali Loss: 0.2186098 Test Loss: 0.3074944
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3093705
	speed: 0.0689s/iter; left time: 1552.1293s
	iters: 200, epoch: 16 | loss: 0.4764429
	speed: 0.0149s/iter; left time: 334.6789s
Epoch: 16 cost time: 4.484378337860107
Epoch: 16, Steps: 266 | Train Loss: 0.4410010 Vali Loss: 0.2186352 Test Loss: 0.3075483
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2800858
	speed: 0.0719s/iter; left time: 1599.4421s
	iters: 200, epoch: 17 | loss: 0.4161088
	speed: 0.0146s/iter; left time: 323.9308s
Epoch: 17 cost time: 4.443600177764893
Epoch: 17, Steps: 266 | Train Loss: 0.4411473 Vali Loss: 0.2184902 Test Loss: 0.3075494
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5989301
	speed: 0.0748s/iter; left time: 1643.7900s
	iters: 200, epoch: 18 | loss: 0.5590740
	speed: 0.0168s/iter; left time: 367.2812s
Epoch: 18 cost time: 4.997626543045044
Epoch: 18, Steps: 266 | Train Loss: 0.4414951 Vali Loss: 0.2183488 Test Loss: 0.3075098
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4245260
	speed: 0.0733s/iter; left time: 1592.4157s
	iters: 200, epoch: 19 | loss: 0.4038596
	speed: 0.0156s/iter; left time: 336.3772s
Epoch: 19 cost time: 4.67098331451416
Epoch: 19, Steps: 266 | Train Loss: 0.4418422 Vali Loss: 0.2186366 Test Loss: 0.3074906
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3262247
	speed: 0.0781s/iter; left time: 1674.0368s
	iters: 200, epoch: 20 | loss: 0.5544579
	speed: 0.0150s/iter; left time: 320.2073s
Epoch: 20 cost time: 5.192889928817749
Epoch: 20, Steps: 266 | Train Loss: 0.4414452 Vali Loss: 0.2185767 Test Loss: 0.3075428
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3502404
	speed: 0.0720s/iter; left time: 1525.9187s
	iters: 200, epoch: 21 | loss: 0.4355717
	speed: 0.0156s/iter; left time: 329.6799s
Epoch: 21 cost time: 4.508901119232178
Epoch: 21, Steps: 266 | Train Loss: 0.4410666 Vali Loss: 0.2187641 Test Loss: 0.3075313
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3568351
	speed: 0.0756s/iter; left time: 1580.4683s
	iters: 200, epoch: 22 | loss: 0.4303363
	speed: 0.0160s/iter; left time: 332.2573s
Epoch: 22 cost time: 5.038534879684448
Epoch: 22, Steps: 266 | Train Loss: 0.4407823 Vali Loss: 0.2186386 Test Loss: 0.3075241
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2983332
	speed: 0.1091s/iter; left time: 2253.7296s
	iters: 200, epoch: 23 | loss: 0.6176227
	speed: 0.0155s/iter; left time: 318.8051s
Epoch: 23 cost time: 4.704209327697754
Epoch: 23, Steps: 266 | Train Loss: 0.4414008 Vali Loss: 0.2187819 Test Loss: 0.3075691
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4217546
	speed: 0.0772s/iter; left time: 1573.9381s
	iters: 200, epoch: 24 | loss: 0.4377289
	speed: 0.0210s/iter; left time: 426.0477s
Epoch: 24 cost time: 5.822591066360474
Epoch: 24, Steps: 266 | Train Loss: 0.4412789 Vali Loss: 0.2188201 Test Loss: 0.3076256
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.6187766
	speed: 0.0736s/iter; left time: 1480.6663s
	iters: 200, epoch: 25 | loss: 0.6064554
	speed: 0.0159s/iter; left time: 317.8180s
Epoch: 25 cost time: 4.702565908432007
Epoch: 25, Steps: 266 | Train Loss: 0.4416374 Vali Loss: 0.2188263 Test Loss: 0.3075923
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3617281
	speed: 0.0718s/iter; left time: 1425.6715s
	iters: 200, epoch: 26 | loss: 0.3702596
	speed: 0.0152s/iter; left time: 300.1378s
Epoch: 26 cost time: 4.530712604522705
Epoch: 26, Steps: 266 | Train Loss: 0.4414702 Vali Loss: 0.2187458 Test Loss: 0.3076334
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3898354
	speed: 0.0754s/iter; left time: 1477.2720s
	iters: 200, epoch: 27 | loss: 0.3226383
	speed: 0.0151s/iter; left time: 294.8360s
Epoch: 27 cost time: 5.22023606300354
Epoch: 27, Steps: 266 | Train Loss: 0.4414896 Vali Loss: 0.2186659 Test Loss: 0.3076133
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.30908292531967163, mae:0.3434823453426361, rse:0.4490535855293274, corr:[0.55371803 0.5597838  0.5546789  0.5484507  0.54635876 0.54737633
 0.5479521  0.54661614 0.5445703  0.5434743  0.5437116  0.544213
 0.54387033 0.5426517  0.5413279  0.54063076 0.54040307 0.53999126
 0.5390111  0.5376691  0.5365112  0.535852   0.5354952  0.5350929
 0.5344891  0.5338406  0.53337574 0.533037   0.5325748  0.5318597
 0.53098243 0.5302164  0.5297058  0.52939576 0.5290341  0.52845913
 0.5276287  0.526699   0.52586097 0.5251975  0.52470547 0.52439
 0.5241949  0.5240256  0.5237739  0.5233142  0.52262706 0.52173394
 0.5208072  0.5200201  0.51938426 0.5188292  0.51820844 0.51746786
 0.51675653 0.51619506 0.5158057  0.51554924 0.51533264 0.51513845
 0.51495874 0.5148162  0.51474935 0.51474524 0.5147098  0.51460457
 0.5144506  0.5142766  0.5141521  0.5140912  0.5140769  0.51410085
 0.51410675 0.5140878  0.5139789  0.5137964  0.5135831  0.5133328
 0.5130478  0.51273614 0.51240444 0.5120697  0.5117509  0.51143587
 0.5110975  0.51071    0.51031214 0.50995696 0.50965387 0.5093487
 0.5089384  0.508312   0.50747883 0.5064623  0.5051815  0.5034851
 0.5013354  0.4989605  0.49669886 0.4947927  0.49325493 0.49192095
 0.4905686  0.48914543 0.48785308 0.4867886  0.48587105 0.48488852
 0.4837234  0.48244008 0.48116514 0.47993034 0.47873446 0.47749662
 0.47618446 0.47492158 0.47380713 0.4728311  0.47190952 0.4709127
 0.4698755  0.46885982 0.46790493 0.46699584 0.46609625 0.46518743
 0.46426177 0.46330833 0.46227592 0.46125856 0.46034494 0.45952937
 0.45875743 0.45798102 0.457181   0.4565598  0.45609173 0.45580894
 0.45560217 0.4553426  0.45495892 0.4544     0.45372432 0.4529722
 0.452164   0.4513463  0.4505152  0.44978228 0.44912848 0.4485389
 0.4479563  0.4474792  0.447006   0.4464394  0.44589227 0.44548908
 0.44526905 0.4452645  0.44528812 0.44530028 0.44518638 0.44489726
 0.44458747 0.44427103 0.44417888 0.44428828 0.4444733  0.44464588
 0.4447688  0.4448533  0.44492137 0.44496906 0.44497475 0.44488883
 0.44470575 0.44447026 0.44424182 0.44412288 0.4441195  0.44417194
 0.44412866 0.4438665  0.44344842 0.4429664  0.4425835  0.44233465
 0.44211033 0.44171503 0.44093958 0.4397338  0.43819022 0.43639913
 0.434456   0.43249542 0.43056646 0.42878047 0.42731443 0.4261858
 0.4252264  0.4241748  0.4230184  0.42188284 0.4208685  0.41994014
 0.41896302 0.41783884 0.41647652 0.41499412 0.41359535 0.41246516
 0.4115228  0.41062137 0.4095922  0.40841174 0.40718973 0.40603113
 0.4050002  0.40402362 0.40309367 0.4020814  0.4009686  0.3998511
 0.39883208 0.3978239  0.39695507 0.39607722 0.39526272 0.39452177
 0.39382803 0.3930975  0.39219642 0.3912972  0.39055237 0.39011934
 0.38994497 0.38986316 0.3896927  0.38933036 0.38887653 0.38842508
 0.38803688 0.38769048 0.38728175 0.3870264  0.3869407  0.386979
 0.38720757 0.38749477 0.38763487 0.38744193 0.3870944  0.3868706
 0.38692442 0.38724834 0.38763797 0.38783684 0.38778174 0.387578
 0.3873959  0.3873994  0.38762364 0.38807884 0.38864163 0.38913324
 0.3895015  0.3897005  0.3898934  0.3900993  0.39034027 0.3905193
 0.39050224 0.39036006 0.39021975 0.3903049  0.39072844 0.3913428
 0.3919123  0.3922132  0.39221862 0.3921308  0.3921316  0.39236507
 0.39270964 0.39288655 0.39266124 0.3919717  0.39094397 0.38978952
 0.38872066 0.38786936 0.38719782 0.3867204  0.3864469  0.38639808
 0.38648584 0.3864855  0.386359   0.38617423 0.38596797 0.38573894
 0.38546288 0.385055   0.3844331  0.38361582 0.38271275 0.38186282
 0.38113987 0.38046542 0.3797523  0.37896374 0.37808463 0.37726623
 0.3765631  0.37601805 0.37548223 0.37477547 0.37391254 0.37301457
 0.3721776  0.37144312 0.37076426 0.37011284 0.369459   0.36884734
 0.36822733 0.36755678 0.366822   0.36592755 0.36511508 0.36456275
 0.36434206 0.36427394 0.36402163 0.36294255 0.36020213 0.35451195]
