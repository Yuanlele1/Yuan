Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7748055
	speed: 0.0246s/iter; left time: 644.7207s
	iters: 200, epoch: 1 | loss: 0.4481991
	speed: 0.0248s/iter; left time: 646.1104s
Epoch: 1 cost time: 6.035129070281982
Epoch: 1, Steps: 263 | Train Loss: 0.7660530 Vali Loss: 0.3399530 Test Loss: 0.4734485
Validation loss decreased (inf --> 0.339953).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5458229
	speed: 0.0881s/iter; left time: 2285.8174s
	iters: 200, epoch: 2 | loss: 0.7702087
	speed: 0.0200s/iter; left time: 517.6057s
Epoch: 2 cost time: 5.932236909866333
Epoch: 2, Steps: 263 | Train Loss: 0.6220694 Vali Loss: 0.2991065 Test Loss: 0.4242453
Validation loss decreased (0.339953 --> 0.299107).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5682678
	speed: 0.0867s/iter; left time: 2225.7540s
	iters: 200, epoch: 3 | loss: 0.4728731
	speed: 0.0197s/iter; left time: 503.5315s
Epoch: 3 cost time: 5.617497444152832
Epoch: 3, Steps: 263 | Train Loss: 0.5977712 Vali Loss: 0.2914989 Test Loss: 0.4149544
Validation loss decreased (0.299107 --> 0.291499).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5926932
	speed: 0.0999s/iter; left time: 2538.2938s
	iters: 200, epoch: 4 | loss: 0.5798084
	speed: 0.0171s/iter; left time: 431.9445s
Epoch: 4 cost time: 5.262110233306885
Epoch: 4, Steps: 263 | Train Loss: 0.5925951 Vali Loss: 0.2896439 Test Loss: 0.4128314
Validation loss decreased (0.291499 --> 0.289644).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4714605
	speed: 0.0831s/iter; left time: 2089.0579s
	iters: 200, epoch: 5 | loss: 0.5700869
	speed: 0.0223s/iter; left time: 558.7020s
Epoch: 5 cost time: 5.728116989135742
Epoch: 5, Steps: 263 | Train Loss: 0.5904579 Vali Loss: 0.2893259 Test Loss: 0.4119327
Validation loss decreased (0.289644 --> 0.289326).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6916618
	speed: 0.0864s/iter; left time: 2149.7013s
	iters: 200, epoch: 6 | loss: 0.4866744
	speed: 0.0237s/iter; left time: 587.4000s
Epoch: 6 cost time: 6.244563341140747
Epoch: 6, Steps: 263 | Train Loss: 0.5900594 Vali Loss: 0.2889161 Test Loss: 0.4115490
Validation loss decreased (0.289326 --> 0.288916).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6341287
	speed: 0.0871s/iter; left time: 2145.3240s
	iters: 200, epoch: 7 | loss: 0.4315743
	speed: 0.0197s/iter; left time: 482.7049s
Epoch: 7 cost time: 5.393433570861816
Epoch: 7, Steps: 263 | Train Loss: 0.5879509 Vali Loss: 0.2891215 Test Loss: 0.4114218
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5432451
	speed: 0.0885s/iter; left time: 2155.4995s
	iters: 200, epoch: 8 | loss: 0.7015158
	speed: 0.0248s/iter; left time: 602.6557s
Epoch: 8 cost time: 6.697782039642334
Epoch: 8, Steps: 263 | Train Loss: 0.5886568 Vali Loss: 0.2890912 Test Loss: 0.4112920
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4138756
	speed: 0.0887s/iter; left time: 2137.4207s
	iters: 200, epoch: 9 | loss: 0.7951601
	speed: 0.0174s/iter; left time: 418.3237s
Epoch: 9 cost time: 5.1723692417144775
Epoch: 9, Steps: 263 | Train Loss: 0.5879269 Vali Loss: 0.2890339 Test Loss: 0.4112160
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.7370988
	speed: 0.0879s/iter; left time: 2094.4803s
	iters: 200, epoch: 10 | loss: 0.8525127
	speed: 0.0266s/iter; left time: 631.5114s
Epoch: 10 cost time: 6.326998472213745
Epoch: 10, Steps: 263 | Train Loss: 0.5877521 Vali Loss: 0.2891786 Test Loss: 0.4112670
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4923344
	speed: 0.0909s/iter; left time: 2142.4654s
	iters: 200, epoch: 11 | loss: 0.5199135
	speed: 0.0270s/iter; left time: 634.4121s
Epoch: 11 cost time: 6.699869632720947
Epoch: 11, Steps: 263 | Train Loss: 0.5881834 Vali Loss: 0.2891899 Test Loss: 0.4111847
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.9249596
	speed: 0.0816s/iter; left time: 1901.5065s
	iters: 200, epoch: 12 | loss: 0.4810615
	speed: 0.0173s/iter; left time: 401.0330s
Epoch: 12 cost time: 5.064953565597534
Epoch: 12, Steps: 263 | Train Loss: 0.5878786 Vali Loss: 0.2889366 Test Loss: 0.4111078
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3302572
	speed: 0.0822s/iter; left time: 1895.0805s
	iters: 200, epoch: 13 | loss: 0.9115855
	speed: 0.0188s/iter; left time: 430.9497s
Epoch: 13 cost time: 5.6495678424835205
Epoch: 13, Steps: 263 | Train Loss: 0.5875480 Vali Loss: 0.2892533 Test Loss: 0.4111947
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.7509133
	speed: 0.0858s/iter; left time: 1954.7006s
	iters: 200, epoch: 14 | loss: 0.4851895
	speed: 0.0190s/iter; left time: 430.3950s
Epoch: 14 cost time: 5.073475122451782
Epoch: 14, Steps: 263 | Train Loss: 0.5873428 Vali Loss: 0.2891459 Test Loss: 0.4111922
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.8439010
	speed: 0.0856s/iter; left time: 1927.4120s
	iters: 200, epoch: 15 | loss: 0.6073757
	speed: 0.0161s/iter; left time: 360.3641s
Epoch: 15 cost time: 5.402116537094116
Epoch: 15, Steps: 263 | Train Loss: 0.5876449 Vali Loss: 0.2893885 Test Loss: 0.4111844
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5311363
	speed: 0.0892s/iter; left time: 1986.0313s
	iters: 200, epoch: 16 | loss: 0.8990309
	speed: 0.0167s/iter; left time: 370.5116s
Epoch: 16 cost time: 5.71796989440918
Epoch: 16, Steps: 263 | Train Loss: 0.5876475 Vali Loss: 0.2893270 Test Loss: 0.4111784
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5566288
	speed: 0.0865s/iter; left time: 1902.3603s
	iters: 200, epoch: 17 | loss: 0.5515612
	speed: 0.0183s/iter; left time: 399.5658s
Epoch: 17 cost time: 5.221047639846802
Epoch: 17, Steps: 263 | Train Loss: 0.5871440 Vali Loss: 0.2895318 Test Loss: 0.4111540
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6801430
	speed: 0.0791s/iter; left time: 1718.7079s
	iters: 200, epoch: 18 | loss: 0.7786604
	speed: 0.0170s/iter; left time: 367.5211s
Epoch: 18 cost time: 5.116483688354492
Epoch: 18, Steps: 263 | Train Loss: 0.5869680 Vali Loss: 0.2895732 Test Loss: 0.4111418
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.7689016
	speed: 0.0840s/iter; left time: 1802.5478s
	iters: 200, epoch: 19 | loss: 0.6409618
	speed: 0.0165s/iter; left time: 352.5857s
Epoch: 19 cost time: 5.495750427246094
Epoch: 19, Steps: 263 | Train Loss: 0.5876365 Vali Loss: 0.2894804 Test Loss: 0.4112025
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5165529
	speed: 0.0962s/iter; left time: 2040.5575s
	iters: 200, epoch: 20 | loss: 0.6368751
	speed: 0.0196s/iter; left time: 412.9709s
Epoch: 20 cost time: 5.7867701053619385
Epoch: 20, Steps: 263 | Train Loss: 0.5871712 Vali Loss: 0.2894749 Test Loss: 0.4112125
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4692167
	speed: 0.0860s/iter; left time: 1801.2411s
	iters: 200, epoch: 21 | loss: 0.4808338
	speed: 0.0174s/iter; left time: 361.6946s
Epoch: 21 cost time: 5.234432935714722
Epoch: 21, Steps: 263 | Train Loss: 0.5866893 Vali Loss: 0.2895165 Test Loss: 0.4112355
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.6893061
	speed: 0.0891s/iter; left time: 1841.8142s
	iters: 200, epoch: 22 | loss: 0.8344985
	speed: 0.0162s/iter; left time: 333.4682s
Epoch: 22 cost time: 5.37688136100769
Epoch: 22, Steps: 263 | Train Loss: 0.5869981 Vali Loss: 0.2892527 Test Loss: 0.4112070
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5219234
	speed: 0.0860s/iter; left time: 1756.6919s
	iters: 200, epoch: 23 | loss: 0.7278419
	speed: 0.0160s/iter; left time: 324.0509s
Epoch: 23 cost time: 4.89000940322876
Epoch: 23, Steps: 263 | Train Loss: 0.5872371 Vali Loss: 0.2894766 Test Loss: 0.4112539
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.7313688
	speed: 0.0799s/iter; left time: 1609.4797s
	iters: 200, epoch: 24 | loss: 0.4296901
	speed: 0.0176s/iter; left time: 352.9486s
Epoch: 24 cost time: 4.925473213195801
Epoch: 24, Steps: 263 | Train Loss: 0.5863850 Vali Loss: 0.2893530 Test Loss: 0.4112505
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4333131
	speed: 0.0862s/iter; left time: 1714.7651s
	iters: 200, epoch: 25 | loss: 0.4438262
	speed: 0.0161s/iter; left time: 318.1254s
Epoch: 25 cost time: 5.266310930252075
Epoch: 25, Steps: 263 | Train Loss: 0.5871934 Vali Loss: 0.2896468 Test Loss: 0.4111932
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.8356227
	speed: 0.0844s/iter; left time: 1656.2072s
	iters: 200, epoch: 26 | loss: 0.4662834
	speed: 0.0180s/iter; left time: 351.4960s
Epoch: 26 cost time: 5.199601411819458
Epoch: 26, Steps: 263 | Train Loss: 0.5867377 Vali Loss: 0.2894611 Test Loss: 0.4112447
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4092603623867035, mae:0.39850693941116333, rse:0.5142143964767456, corr:[0.5402952  0.54716015 0.53952295 0.5341979  0.534419   0.5350908
 0.53284603 0.530111   0.52953184 0.5300508  0.5294033  0.527369
 0.5258271  0.525902   0.5262652  0.52519846 0.52302104 0.52141863
 0.5211297  0.52103734 0.51994514 0.5181486  0.516926   0.51685643
 0.5170408  0.51651376 0.5154845  0.5148075  0.5147361  0.5145337
 0.51357126 0.5122308  0.511325   0.51115906 0.51111454 0.51053196
 0.5095175  0.5086157  0.5079813  0.5073278  0.50646603 0.5057358
 0.5055335  0.50574845 0.505791   0.50523454 0.5043204  0.50348485
 0.5028954  0.5022971  0.50147545 0.5006147  0.4999395  0.49948967
 0.4991262  0.49861446 0.49800047 0.49756694 0.497418   0.49739158
 0.49726218 0.4969767  0.49674472 0.49672207 0.4967911  0.49679542
 0.4966341  0.49638388 0.49619922 0.4961178  0.4961378  0.4962194
 0.49622184 0.49609163 0.49580717 0.49550718 0.49532187 0.49513605
 0.49483493 0.49441233 0.49401346 0.49371326 0.49342197 0.492969
 0.49238807 0.49185917 0.4914965  0.49121875 0.4908211  0.4902975
 0.48976952 0.4892209  0.48850977 0.4873587  0.4856332  0.48353067
 0.48132423 0.47909603 0.47677606 0.47451863 0.47268152 0.47144312
 0.4704646  0.46927404 0.46782497 0.4664353  0.4653449  0.464386
 0.46323484 0.46182054 0.4603976  0.45924258 0.45829222 0.45712164
 0.45553398 0.4538617  0.45255703 0.45166928 0.4508524  0.44974577
 0.4485184  0.44753227 0.44686133 0.44618526 0.4451868  0.4439297
 0.44278067 0.44187137 0.44096962 0.43989223 0.43873596 0.437792
 0.43720073 0.43671283 0.43596792 0.4350861  0.43431666 0.43398347
 0.43394992 0.4338529  0.43348196 0.43297154 0.43258184 0.43223163
 0.431583   0.43050897 0.42921284 0.42822626 0.42765582 0.4272641
 0.4266818  0.42599368 0.42534912 0.42484862 0.424552   0.42430118
 0.4240079  0.42381817 0.42378536 0.42393208 0.42398214 0.4237133
 0.42331085 0.4229275  0.4228595  0.42296118 0.42297798 0.4228604
 0.422796   0.422916   0.42308086 0.42303622 0.42271948 0.42229944
 0.42200962 0.42194223 0.4219124  0.42173022 0.42139107 0.42108917
 0.42091513 0.4207133  0.4203268  0.41968653 0.41906342 0.4186738
 0.41843864 0.41795078 0.41689268 0.41529566 0.41344947 0.411462
 0.40923887 0.4067904  0.40429676 0.40214846 0.40057147 0.39931434
 0.3980209  0.39660755 0.39541563 0.3946299  0.39397144 0.39297697
 0.39153895 0.39004233 0.38879326 0.38774347 0.3865824  0.38513908
 0.383552   0.3821858  0.38110474 0.38004628 0.3787609  0.3773209
 0.3761053  0.375268   0.3746717  0.37383237 0.37257874 0.3712052
 0.3700604  0.3690792  0.3681928  0.3671112  0.3660396  0.36519295
 0.3645833  0.3639358  0.36298648 0.36195713 0.3611701  0.36082083
 0.36066934 0.3604085  0.3600228  0.35971674 0.3596708  0.3596376
 0.3593102  0.3586609  0.35789725 0.35760367 0.35768577 0.357718
 0.35756594 0.35733435 0.35723636 0.35721347 0.35721773 0.35721248
 0.35722598 0.3574058  0.3577477  0.35794687 0.35780346 0.3574749
 0.35736263 0.35772192 0.35826755 0.35867092 0.35876238 0.3587131
 0.35890496 0.35933024 0.35974652 0.35975224 0.3594576  0.3592504
 0.35942325 0.35986772 0.36018166 0.3601665  0.35998815 0.3599399
 0.36012262 0.36032966 0.36036345 0.3603343  0.36040226 0.36063868
 0.36079314 0.36054432 0.35978583 0.35868147 0.3574554  0.3561423
 0.35472122 0.35336012 0.35232127 0.35178134 0.35154736 0.35139096
 0.35125312 0.35119575 0.3513839  0.35171577 0.35181266 0.35149708
 0.35092098 0.35031292 0.34975877 0.3491257  0.34830964 0.34738582
 0.3465995  0.3460093  0.3454162  0.34460756 0.34363896 0.34286273
 0.34247127 0.3423857  0.34222597 0.34174174 0.341022   0.34030572
 0.339663   0.3390073  0.3383224  0.33776003 0.33747962 0.33739093
 0.3371714  0.33665767 0.33600664 0.33550137 0.33538455 0.33549976
 0.33554378 0.33537564 0.33520648 0.3351911  0.3352843  0.33528462
 0.33501303 0.33458844 0.33438277 0.33462363 0.3350281  0.3351988
 0.33511263 0.33508536 0.33526883 0.3355338  0.33568886 0.33571655
 0.33580062 0.33603334 0.33639488 0.33670342 0.3368472  0.33685833
 0.3369003  0.33712104 0.3373981  0.33769706 0.33798727 0.33831424
 0.33862904 0.3388432  0.33887324 0.33880293 0.3388032  0.33901924
 0.33932564 0.33956364 0.33968014 0.33971733 0.33981723 0.34000984
 0.340209   0.34034607 0.3404716  0.34070683 0.34100622 0.34117717
 0.34102184 0.34052363 0.3398189  0.33900455 0.33799893 0.33665863
 0.33507144 0.3335709  0.33243343 0.33155593 0.33067635 0.32978833
 0.32907    0.32865638 0.32846463 0.3281595  0.3275566  0.3267621
 0.3260309  0.3254586  0.32479817 0.32380953 0.32252017 0.3212003
 0.3200632  0.31901842 0.31787443 0.31665516 0.31550017 0.31466708
 0.31413198 0.31363946 0.312972   0.31214884 0.31135422 0.3106581
 0.30996794 0.30909956 0.30809635 0.3072606  0.3067252  0.30644763
 0.3062058  0.30575758 0.30512348 0.30446482 0.30393076 0.30356628
 0.30330065 0.3031122  0.30291495 0.3026288  0.3021183  0.30141264
 0.3006026  0.29976624 0.2989664  0.2983119  0.297757   0.29738444
 0.2971956  0.29717982 0.29710475 0.2967972  0.29636496 0.296163
 0.29635116 0.29677257 0.29703528 0.29682526 0.29626647 0.29576263
 0.29570445 0.2960567  0.29651406 0.29677927 0.29681835 0.2968379
 0.296925   0.29697686 0.29682705 0.29649773 0.29621795 0.2960602
 0.29599285 0.29588333 0.29568404 0.29541245 0.29512447 0.2948633
 0.29466    0.29453835 0.29450527 0.2945065  0.2944133  0.2941374
 0.29369968 0.29303128 0.29197556 0.29028925 0.2879982  0.2854199
 0.28309962 0.2813938  0.28014132 0.27893642 0.27761692 0.27647334
 0.27584982 0.27578744 0.2759539  0.27595788 0.2756767  0.2752836
 0.27479544 0.27410406 0.27304402 0.27178952 0.2706842  0.26987693
 0.26917288 0.2681492  0.2666844  0.26515257 0.26405135 0.26351252
 0.26314804 0.26257128 0.26177454 0.26099497 0.26043946 0.25990152
 0.25907922 0.25794265 0.25685635 0.25619918 0.25586227 0.25552693
 0.25503042 0.25438297 0.253777   0.25325266 0.25267497 0.25196716
 0.25124806 0.25083774 0.25076792 0.25076506 0.25053042 0.2499748
 0.24927282 0.24850127 0.24775288 0.247014   0.24636929 0.24608016
 0.24622558 0.24650249 0.24650979 0.24609873 0.24565394 0.24562143
 0.2460119  0.24635698 0.24626766 0.24587798 0.24554226 0.24556066
 0.24583034 0.24604137 0.24604234 0.24601796 0.24613717 0.24651855
 0.246835   0.24686222 0.24670038 0.24666338 0.24680959 0.24700253
 0.247003   0.24688381 0.24686337 0.24704808 0.24727108 0.24731883
 0.24718653 0.24713884 0.24739465 0.24776602 0.24794778 0.24776258
 0.24735239 0.2468071  0.24594842 0.24448608 0.2423587  0.23990463
 0.23763727 0.23586842 0.23444901 0.2332432  0.23247127 0.2324397
 0.23296604 0.23358393 0.2339977  0.23428151 0.23464383 0.23496716
 0.23485129 0.23403263 0.23281808 0.23179439 0.23120733 0.23067026
 0.22966678 0.22818576 0.22681649 0.2260876  0.22590083 0.22570932
 0.22513257 0.22439879 0.22393037 0.22372602 0.22338101 0.22246459
 0.2211836  0.22028032 0.22010995 0.22017561 0.21974179 0.21864353
 0.21758443 0.21718828 0.21735595 0.2172203  0.21637572 0.21515603
 0.21434635 0.21428205 0.21438816 0.21398385 0.21314663 0.2123536
 0.21202272 0.2118872  0.2114551  0.21061246 0.20984846 0.2095139
 0.20972474 0.2101602  0.21048631 0.21076038 0.21118872 0.21148041
 0.21120279 0.21035488 0.2095543  0.20938426 0.20968296 0.20959058
 0.2087509  0.20778283 0.20766681 0.20847198 0.2094771  0.20974517
 0.20932254 0.2090274  0.20939657 0.21007507 0.21032053 0.21000013
 0.20980985 0.21031824 0.21117838 0.21164685 0.21144448 0.21119943
 0.21165732 0.21265864 0.21334298 0.21321519 0.21283937 0.2130535
 0.21378818 0.21393053 0.21253559 0.21000087 0.20765765 0.20634358
 0.20557901 0.20431352 0.20235763 0.2007433  0.20046526 0.20118284
 0.20169628 0.20159489 0.20164806 0.20261277 0.20403871 0.20455196
 0.2034017  0.20141268 0.20013218 0.20005982 0.20016398 0.19919896
 0.19729479 0.19575474 0.19536969 0.1954196  0.19458053 0.19267406
 0.19099954 0.1907618  0.19132794 0.19089569 0.18878803 0.18620692
 0.18498321 0.18503387 0.18439147 0.18186863 0.17860788 0.17723909
 0.17807232 0.17856409 0.17610313 0.17199701 0.1697608  0.1709143
 0.17206123 0.1692629  0.16414036 0.16335693 0.1687332  0.17047258]
