Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_192_FITS_ETTm2_ftM_sl90_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34279
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=18, out_features=56, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  903168.0
params:  1064.0
Trainable parameters:  1064
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4386579
	speed: 0.0190s/iter; left time: 504.6417s
	iters: 200, epoch: 1 | loss: 0.3256700
	speed: 0.0144s/iter; left time: 382.3676s
Epoch: 1 cost time: 4.15913987159729
Epoch: 1, Steps: 267 | Train Loss: 0.4137411 Vali Loss: 0.1887741 Test Loss: 0.2694223
Validation loss decreased (inf --> 0.188774).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4502504
	speed: 0.0727s/iter; left time: 1913.8918s
	iters: 200, epoch: 2 | loss: 0.4171936
	speed: 0.0110s/iter; left time: 289.2783s
Epoch: 2 cost time: 3.5601916313171387
Epoch: 2, Steps: 267 | Train Loss: 0.3560974 Vali Loss: 0.1764831 Test Loss: 0.2539447
Validation loss decreased (0.188774 --> 0.176483).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4068022
	speed: 0.0648s/iter; left time: 1688.2690s
	iters: 200, epoch: 3 | loss: 0.3763549
	speed: 0.0160s/iter; left time: 414.4180s
Epoch: 3 cost time: 3.925455093383789
Epoch: 3, Steps: 267 | Train Loss: 0.3455962 Vali Loss: 0.1739908 Test Loss: 0.2506050
Validation loss decreased (0.176483 --> 0.173991).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4240545
	speed: 0.0650s/iter; left time: 1677.1033s
	iters: 200, epoch: 4 | loss: 0.3257304
	speed: 0.0110s/iter; left time: 283.5916s
Epoch: 4 cost time: 3.7309772968292236
Epoch: 4, Steps: 267 | Train Loss: 0.3418168 Vali Loss: 0.1734742 Test Loss: 0.2493904
Validation loss decreased (0.173991 --> 0.173474).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4897421
	speed: 0.0643s/iter; left time: 1641.8877s
	iters: 200, epoch: 5 | loss: 0.3889573
	speed: 0.0116s/iter; left time: 296.0258s
Epoch: 5 cost time: 3.688969850540161
Epoch: 5, Steps: 267 | Train Loss: 0.3402633 Vali Loss: 0.1730967 Test Loss: 0.2487426
Validation loss decreased (0.173474 --> 0.173097).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3694564
	speed: 0.0657s/iter; left time: 1659.7143s
	iters: 200, epoch: 6 | loss: 0.2471409
	speed: 0.0108s/iter; left time: 270.6424s
Epoch: 6 cost time: 3.8269646167755127
Epoch: 6, Steps: 267 | Train Loss: 0.3398013 Vali Loss: 0.1727570 Test Loss: 0.2485385
Validation loss decreased (0.173097 --> 0.172757).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4786861
	speed: 0.0846s/iter; left time: 2114.0664s
	iters: 200, epoch: 7 | loss: 0.3872646
	speed: 0.0107s/iter; left time: 267.1207s
Epoch: 7 cost time: 5.255761623382568
Epoch: 7, Steps: 267 | Train Loss: 0.3388844 Vali Loss: 0.1729655 Test Loss: 0.2483268
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3128586
	speed: 0.0669s/iter; left time: 1655.3023s
	iters: 200, epoch: 8 | loss: 0.3066845
	speed: 0.0104s/iter; left time: 257.0748s
Epoch: 8 cost time: 3.413172721862793
Epoch: 8, Steps: 267 | Train Loss: 0.3389163 Vali Loss: 0.1729908 Test Loss: 0.2482147
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2447112
	speed: 0.0594s/iter; left time: 1453.6949s
	iters: 200, epoch: 9 | loss: 0.3070574
	speed: 0.0124s/iter; left time: 302.4460s
Epoch: 9 cost time: 3.786257743835449
Epoch: 9, Steps: 267 | Train Loss: 0.3380285 Vali Loss: 0.1732808 Test Loss: 0.2483276
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3789631
	speed: 0.0696s/iter; left time: 1684.5514s
	iters: 200, epoch: 10 | loss: 0.2808767
	speed: 0.0110s/iter; left time: 265.0277s
Epoch: 10 cost time: 3.9854769706726074
Epoch: 10, Steps: 267 | Train Loss: 0.3379291 Vali Loss: 0.1731787 Test Loss: 0.2481742
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2574150
	speed: 0.0704s/iter; left time: 1685.1434s
	iters: 200, epoch: 11 | loss: 0.3080112
	speed: 0.0110s/iter; left time: 262.4136s
Epoch: 11 cost time: 3.6250622272491455
Epoch: 11, Steps: 267 | Train Loss: 0.3376360 Vali Loss: 0.1730977 Test Loss: 0.2481471
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3220713
	speed: 0.0735s/iter; left time: 1740.2502s
	iters: 200, epoch: 12 | loss: 0.3831089
	speed: 0.0115s/iter; left time: 270.9886s
Epoch: 12 cost time: 3.6847076416015625
Epoch: 12, Steps: 267 | Train Loss: 0.3373991 Vali Loss: 0.1731945 Test Loss: 0.2480971
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4111199
	speed: 0.0722s/iter; left time: 1689.5068s
	iters: 200, epoch: 13 | loss: 0.4173866
	speed: 0.0118s/iter; left time: 275.3349s
Epoch: 13 cost time: 4.309550523757935
Epoch: 13, Steps: 267 | Train Loss: 0.3375265 Vali Loss: 0.1730568 Test Loss: 0.2481026
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2886107
	speed: 0.0681s/iter; left time: 1575.4116s
	iters: 200, epoch: 14 | loss: 0.2555617
	speed: 0.0108s/iter; left time: 248.3454s
Epoch: 14 cost time: 3.7441864013671875
Epoch: 14, Steps: 267 | Train Loss: 0.3374283 Vali Loss: 0.1730938 Test Loss: 0.2481446
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3013009
	speed: 0.0668s/iter; left time: 1528.3821s
	iters: 200, epoch: 15 | loss: 0.3405370
	speed: 0.0118s/iter; left time: 268.0246s
Epoch: 15 cost time: 3.9519593715667725
Epoch: 15, Steps: 267 | Train Loss: 0.3372383 Vali Loss: 0.1732387 Test Loss: 0.2480844
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2358696
	speed: 0.0659s/iter; left time: 1490.1533s
	iters: 200, epoch: 16 | loss: 0.3514297
	speed: 0.0114s/iter; left time: 256.4093s
Epoch: 16 cost time: 3.7362914085388184
Epoch: 16, Steps: 267 | Train Loss: 0.3370886 Vali Loss: 0.1733700 Test Loss: 0.2481124
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3320062
	speed: 0.0648s/iter; left time: 1446.7556s
	iters: 200, epoch: 17 | loss: 0.3555019
	speed: 0.0109s/iter; left time: 242.7268s
Epoch: 17 cost time: 3.7465622425079346
Epoch: 17, Steps: 267 | Train Loss: 0.3370454 Vali Loss: 0.1733214 Test Loss: 0.2481085
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2726689
	speed: 0.0613s/iter; left time: 1353.4727s
	iters: 200, epoch: 18 | loss: 0.2652220
	speed: 0.0110s/iter; left time: 241.0900s
Epoch: 18 cost time: 3.6368484497070312
Epoch: 18, Steps: 267 | Train Loss: 0.3365952 Vali Loss: 0.1732626 Test Loss: 0.2481289
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3471264
	speed: 0.0660s/iter; left time: 1438.5944s
	iters: 200, epoch: 19 | loss: 0.3650354
	speed: 0.0183s/iter; left time: 397.3010s
Epoch: 19 cost time: 4.656628847122192
Epoch: 19, Steps: 267 | Train Loss: 0.3363619 Vali Loss: 0.1733353 Test Loss: 0.2481323
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3003617
	speed: 0.0675s/iter; left time: 1453.9989s
	iters: 200, epoch: 20 | loss: 0.5358725
	speed: 0.0183s/iter; left time: 391.7919s
Epoch: 20 cost time: 5.24407958984375
Epoch: 20, Steps: 267 | Train Loss: 0.3371944 Vali Loss: 0.1733465 Test Loss: 0.2481378
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3385803
	speed: 0.0760s/iter; left time: 1615.1766s
	iters: 200, epoch: 21 | loss: 0.2395979
	speed: 0.0109s/iter; left time: 230.9095s
Epoch: 21 cost time: 3.565673589706421
Epoch: 21, Steps: 267 | Train Loss: 0.3370795 Vali Loss: 0.1732178 Test Loss: 0.2481241
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2787556
	speed: 0.0637s/iter; left time: 1337.4618s
	iters: 200, epoch: 22 | loss: 0.2415020
	speed: 0.0159s/iter; left time: 331.8007s
Epoch: 22 cost time: 4.275311231613159
Epoch: 22, Steps: 267 | Train Loss: 0.3370547 Vali Loss: 0.1733669 Test Loss: 0.2481226
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3072739
	speed: 0.0670s/iter; left time: 1388.8531s
	iters: 200, epoch: 23 | loss: 0.3418556
	speed: 0.0109s/iter; left time: 225.7384s
Epoch: 23 cost time: 3.699857711791992
Epoch: 23, Steps: 267 | Train Loss: 0.3367089 Vali Loss: 0.1733967 Test Loss: 0.2481317
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3437805
	speed: 0.0692s/iter; left time: 1415.2523s
	iters: 200, epoch: 24 | loss: 0.2125562
	speed: 0.0111s/iter; left time: 226.4614s
Epoch: 24 cost time: 4.2142333984375
Epoch: 24, Steps: 267 | Train Loss: 0.3367297 Vali Loss: 0.1733507 Test Loss: 0.2481101
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2058561
	speed: 0.0707s/iter; left time: 1427.0394s
	iters: 200, epoch: 25 | loss: 0.2842712
	speed: 0.0111s/iter; left time: 222.8031s
Epoch: 25 cost time: 3.921112298965454
Epoch: 25, Steps: 267 | Train Loss: 0.3367640 Vali Loss: 0.1734890 Test Loss: 0.2481251
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2605526
	speed: 0.0643s/iter; left time: 1280.3023s
	iters: 200, epoch: 26 | loss: 0.2441549
	speed: 0.0121s/iter; left time: 239.6481s
Epoch: 26 cost time: 3.6404147148132324
Epoch: 26, Steps: 267 | Train Loss: 0.3371327 Vali Loss: 0.1734908 Test Loss: 0.2480935
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_192_FITS_ETTm2_ftM_sl90_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.24936555325984955, mae:0.3071210980415344, rse:0.40421584248542786, corr:[0.56028336 0.565337   0.5595071  0.5544635  0.55409294 0.5551554
 0.5542376  0.5521404  0.5511368  0.55162907 0.55201393 0.5510985
 0.5494749  0.54857635 0.5486785  0.5487075  0.54786587 0.5465082
 0.54552364 0.54511136 0.54468673 0.54378027 0.54261667 0.54177815
 0.5414077  0.5411235  0.5406073  0.53990364 0.5392934  0.538861
 0.53833795 0.5375787  0.5367378  0.5361658  0.53588617 0.53558993
 0.53491724 0.5339291  0.5330195  0.532438   0.5321149  0.53184396
 0.531439   0.5309709  0.53058785 0.53022236 0.5297407  0.5290024
 0.52810967 0.5272692  0.526553   0.52595025 0.5253463  0.5246775
 0.52407074 0.523572   0.5231267  0.52273643 0.5224117  0.52221256
 0.5220891  0.5219816  0.5218829  0.52183145 0.5217602  0.52164763
 0.5214849  0.52128804 0.52114916 0.521098   0.5211097  0.5211563
 0.52114224 0.52106744 0.52090245 0.5207075  0.52054876 0.52034336
 0.52007043 0.5197527  0.5194616  0.5192067  0.51896656 0.5186696
 0.51826954 0.51783395 0.51741153 0.5170936  0.51686704 0.5166561
 0.5163425  0.51582783 0.51510465 0.51420766 0.5130364  0.511431
 0.50935704 0.50708747 0.5049816  0.5031768  0.501623   0.50019234
 0.4987689  0.49737602 0.49610677 0.49497527 0.4939047  0.4928112
 0.49163753 0.49040473 0.48915508 0.48788756 0.48668528 0.48552692
 0.48439237 0.48328775 0.4822161  0.4811786  0.48018104 0.4791519
 0.47817224 0.47725528 0.47635692 0.47542724 0.47448885 0.47360748
 0.47275987 0.471845   0.47079158 0.4697227  0.468859   0.4682052
 0.46758184 0.46680936 0.46593326 0.46532318 0.46493015 0.4646883
 0.46434873 0.46389502 0.46350878 0.4632528  0.46298715 0.46243083
 0.4614994  0.46043935 0.45947388 0.4587218  0.4579754  0.45704645
 0.4560564  0.45544773 0.45518562 0.45487478 0.4542889  0.45352966
 0.45285165 0.45261347 0.45265695 0.45277843 0.45260078 0.4520562
 0.45154497 0.45119408 0.45123896 0.45136297 0.45127615 0.4510721
 0.45104194 0.4512731  0.45149806 0.45144328 0.45117748 0.45099357
 0.45112246 0.451399   0.45136255 0.4509749  0.45069897 0.4510712
 0.45181796 0.45203844 0.45151278 0.4507632  0.45086887 0.45184729
 0.45249262 0.45213786 0.45146725 0.45243976 0.4543267  0.4525974 ]
