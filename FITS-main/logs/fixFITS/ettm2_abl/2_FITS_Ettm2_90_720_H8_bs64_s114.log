Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7266694
	speed: 0.0259s/iter; left time: 677.4664s
	iters: 200, epoch: 1 | loss: 0.4198159
	speed: 0.0395s/iter; left time: 1032.2449s
Epoch: 1 cost time: 9.284935474395752
Epoch: 1, Steps: 263 | Train Loss: 0.7146015 Vali Loss: 0.3493512 Test Loss: 0.4849541
Validation loss decreased (inf --> 0.349351).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4976120
	speed: 0.1648s/iter; left time: 4273.9234s
	iters: 200, epoch: 2 | loss: 0.6927804
	speed: 0.0412s/iter; left time: 1065.4749s
Epoch: 2 cost time: 9.951233148574829
Epoch: 2, Steps: 263 | Train Loss: 0.5645857 Vali Loss: 0.3021387 Test Loss: 0.4279816
Validation loss decreased (0.349351 --> 0.302139).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5096024
	speed: 0.1212s/iter; left time: 3110.7996s
	iters: 200, epoch: 3 | loss: 0.4239590
	speed: 0.0205s/iter; left time: 524.0991s
Epoch: 3 cost time: 6.875536203384399
Epoch: 3, Steps: 263 | Train Loss: 0.5356274 Vali Loss: 0.2924560 Test Loss: 0.4163061
Validation loss decreased (0.302139 --> 0.292456).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5296464
	speed: 0.1001s/iter; left time: 2543.7855s
	iters: 200, epoch: 4 | loss: 0.5171439
	speed: 0.0256s/iter; left time: 647.5285s
Epoch: 4 cost time: 7.201746940612793
Epoch: 4, Steps: 263 | Train Loss: 0.5291813 Vali Loss: 0.2899759 Test Loss: 0.4134436
Validation loss decreased (0.292456 --> 0.289976).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4205135
	speed: 0.1198s/iter; left time: 3011.8714s
	iters: 200, epoch: 5 | loss: 0.5088639
	speed: 0.0310s/iter; left time: 777.0648s
Epoch: 5 cost time: 9.910717010498047
Epoch: 5, Steps: 263 | Train Loss: 0.5267043 Vali Loss: 0.2894571 Test Loss: 0.4122695
Validation loss decreased (0.289976 --> 0.289457).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6169187
	speed: 0.2007s/iter; left time: 4994.0243s
	iters: 200, epoch: 6 | loss: 0.4336523
	speed: 0.0314s/iter; left time: 778.6348s
Epoch: 6 cost time: 10.798808813095093
Epoch: 6, Steps: 263 | Train Loss: 0.5261048 Vali Loss: 0.2889470 Test Loss: 0.4117203
Validation loss decreased (0.289457 --> 0.288947).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5651066
	speed: 0.1409s/iter; left time: 3469.3729s
	iters: 200, epoch: 7 | loss: 0.3850451
	speed: 0.0386s/iter; left time: 945.4560s
Epoch: 7 cost time: 9.422632694244385
Epoch: 7, Steps: 263 | Train Loss: 0.5241081 Vali Loss: 0.2891136 Test Loss: 0.4115073
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4847512
	speed: 0.1672s/iter; left time: 4072.8340s
	iters: 200, epoch: 8 | loss: 0.6249006
	speed: 0.0449s/iter; left time: 1090.1037s
Epoch: 8 cost time: 10.690199851989746
Epoch: 8, Steps: 263 | Train Loss: 0.5246706 Vali Loss: 0.2890937 Test Loss: 0.4113509
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3696454
	speed: 0.1017s/iter; left time: 2451.3433s
	iters: 200, epoch: 9 | loss: 0.7083781
	speed: 0.0206s/iter; left time: 494.9901s
Epoch: 9 cost time: 5.806172609329224
Epoch: 9, Steps: 263 | Train Loss: 0.5239885 Vali Loss: 0.2890622 Test Loss: 0.4112810
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6566188
	speed: 0.1039s/iter; left time: 2475.6941s
	iters: 200, epoch: 10 | loss: 0.7598773
	speed: 0.0470s/iter; left time: 1115.5987s
Epoch: 10 cost time: 10.495034217834473
Epoch: 10, Steps: 263 | Train Loss: 0.5238126 Vali Loss: 0.2892265 Test Loss: 0.4113312
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4392447
	speed: 0.1310s/iter; left time: 3087.7175s
	iters: 200, epoch: 11 | loss: 0.4630582
	speed: 0.0247s/iter; left time: 579.9393s
Epoch: 11 cost time: 8.852669477462769
Epoch: 11, Steps: 263 | Train Loss: 0.5241932 Vali Loss: 0.2892580 Test Loss: 0.4112573
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.8242911
	speed: 0.1634s/iter; left time: 3807.7824s
	iters: 200, epoch: 12 | loss: 0.4295005
	speed: 0.0444s/iter; left time: 1031.4532s
Epoch: 12 cost time: 11.651726961135864
Epoch: 12, Steps: 263 | Train Loss: 0.5239270 Vali Loss: 0.2890409 Test Loss: 0.4112156
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2947248
	speed: 0.1596s/iter; left time: 3677.5080s
	iters: 200, epoch: 13 | loss: 0.8111572
	speed: 0.0339s/iter; left time: 777.9646s
Epoch: 13 cost time: 9.834009408950806
Epoch: 13, Steps: 263 | Train Loss: 0.5236313 Vali Loss: 0.2893970 Test Loss: 0.4113328
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6685972
	speed: 0.1338s/iter; left time: 3047.8675s
	iters: 200, epoch: 14 | loss: 0.4322620
	speed: 0.0437s/iter; left time: 992.2596s
Epoch: 14 cost time: 9.914961576461792
Epoch: 14, Steps: 263 | Train Loss: 0.5234578 Vali Loss: 0.2893045 Test Loss: 0.4113423
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.7509245
	speed: 0.1204s/iter; left time: 2711.6322s
	iters: 200, epoch: 15 | loss: 0.5409588
	speed: 0.0351s/iter; left time: 787.7246s
Epoch: 15 cost time: 8.203497409820557
Epoch: 15, Steps: 263 | Train Loss: 0.5237312 Vali Loss: 0.2895630 Test Loss: 0.4113482
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4733973
	speed: 0.1172s/iter; left time: 2608.9109s
	iters: 200, epoch: 16 | loss: 0.8009773
	speed: 0.0393s/iter; left time: 871.6130s
Epoch: 16 cost time: 9.143155097961426
Epoch: 16, Steps: 263 | Train Loss: 0.5237411 Vali Loss: 0.2895281 Test Loss: 0.4113728
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4959477
	speed: 0.1403s/iter; left time: 3086.0669s
	iters: 200, epoch: 17 | loss: 0.4919645
	speed: 0.0745s/iter; left time: 1630.2784s
Epoch: 17 cost time: 14.007556676864624
Epoch: 17, Steps: 263 | Train Loss: 0.5232992 Vali Loss: 0.2897369 Test Loss: 0.4113552
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6060576
	speed: 0.1432s/iter; left time: 3110.7744s
	iters: 200, epoch: 18 | loss: 0.6935117
	speed: 0.0196s/iter; left time: 424.3951s
Epoch: 18 cost time: 7.558629989624023
Epoch: 18, Steps: 263 | Train Loss: 0.5231512 Vali Loss: 0.2897974 Test Loss: 0.4113590
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.6850858
	speed: 0.1729s/iter; left time: 3710.9185s
	iters: 200, epoch: 19 | loss: 0.5712982
	speed: 0.0410s/iter; left time: 875.0627s
Epoch: 19 cost time: 11.283663272857666
Epoch: 19, Steps: 263 | Train Loss: 0.5237505 Vali Loss: 0.2897252 Test Loss: 0.4114392
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4614944
	speed: 0.1469s/iter; left time: 3115.5755s
	iters: 200, epoch: 20 | loss: 0.5668713
	speed: 0.0241s/iter; left time: 508.0729s
Epoch: 20 cost time: 8.60677456855774
Epoch: 20, Steps: 263 | Train Loss: 0.5233398 Vali Loss: 0.2897221 Test Loss: 0.4114566
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4187375
	speed: 0.2738s/iter; left time: 5733.7420s
	iters: 200, epoch: 21 | loss: 0.4288672
	speed: 0.0753s/iter; left time: 1569.4668s
Epoch: 21 cost time: 20.34466576576233
Epoch: 21, Steps: 263 | Train Loss: 0.5229165 Vali Loss: 0.2897721 Test Loss: 0.4114907
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.6139820
	speed: 0.3526s/iter; left time: 7291.4458s
	iters: 200, epoch: 22 | loss: 0.7445405
	speed: 0.0578s/iter; left time: 1188.7704s
Epoch: 22 cost time: 18.47215986251831
Epoch: 22, Steps: 263 | Train Loss: 0.5231953 Vali Loss: 0.2895025 Test Loss: 0.4114533
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4644671
	speed: 0.2067s/iter; left time: 4219.5560s
	iters: 200, epoch: 23 | loss: 0.6486413
	speed: 0.0363s/iter; left time: 737.0215s
Epoch: 23 cost time: 12.235220193862915
Epoch: 23, Steps: 263 | Train Loss: 0.5234113 Vali Loss: 0.2897420 Test Loss: 0.4115195
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.6512032
	speed: 0.2048s/iter; left time: 4128.1188s
	iters: 200, epoch: 24 | loss: 0.3835449
	speed: 0.0267s/iter; left time: 536.2121s
Epoch: 24 cost time: 10.015948057174683
Epoch: 24, Steps: 263 | Train Loss: 0.5226550 Vali Loss: 0.2896170 Test Loss: 0.4115161
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3869149
	speed: 0.1123s/iter; left time: 2233.3703s
	iters: 200, epoch: 25 | loss: 0.3956223
	speed: 0.0316s/iter; left time: 625.7488s
Epoch: 25 cost time: 7.814347743988037
Epoch: 25, Steps: 263 | Train Loss: 0.5233769 Vali Loss: 0.2899188 Test Loss: 0.4114663
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.7451892
	speed: 0.1487s/iter; left time: 2918.3982s
	iters: 200, epoch: 26 | loss: 0.4155352
	speed: 0.0359s/iter; left time: 700.9842s
Epoch: 26 cost time: 11.339101552963257
Epoch: 26, Steps: 263 | Train Loss: 0.5229710 Vali Loss: 0.2897417 Test Loss: 0.4115262
EarlyStopping counter: 20 out of 20
Early stopping
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4495709
	speed: 0.0275s/iter; left time: 720.1361s
	iters: 200, epoch: 1 | loss: 0.5562311
	speed: 0.0468s/iter; left time: 1222.5421s
Epoch: 1 cost time: 9.782373189926147
Epoch: 1, Steps: 263 | Train Loss: 0.5888774 Vali Loss: 0.2888412 Test Loss: 0.4111050
Validation loss decreased (inf --> 0.288841).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5341840
	speed: 0.2048s/iter; left time: 5312.8387s
	iters: 200, epoch: 2 | loss: 0.4323640
	speed: 0.0392s/iter; left time: 1013.3551s
Epoch: 2 cost time: 11.299335956573486
Epoch: 2, Steps: 263 | Train Loss: 0.5885570 Vali Loss: 0.2886801 Test Loss: 0.4110447
Validation loss decreased (0.288841 --> 0.288680).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6791000
	speed: 0.1864s/iter; left time: 4784.9685s
	iters: 200, epoch: 3 | loss: 0.3920760
	speed: 0.0488s/iter; left time: 1248.5908s
Epoch: 3 cost time: 12.821616888046265
Epoch: 3, Steps: 263 | Train Loss: 0.5881667 Vali Loss: 0.2890832 Test Loss: 0.4110969
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3954902
	speed: 0.1490s/iter; left time: 3785.2595s
	iters: 200, epoch: 4 | loss: 0.6821813
	speed: 0.0329s/iter; left time: 831.9220s
Epoch: 4 cost time: 9.095782279968262
Epoch: 4, Steps: 263 | Train Loss: 0.5879779 Vali Loss: 0.2894374 Test Loss: 0.4113267
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5917705
	speed: 0.1324s/iter; left time: 3330.7474s
	iters: 200, epoch: 5 | loss: 0.6935445
	speed: 0.0224s/iter; left time: 560.9373s
Epoch: 5 cost time: 8.957441329956055
Epoch: 5, Steps: 263 | Train Loss: 0.5877703 Vali Loss: 0.2893810 Test Loss: 0.4110416
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5159180
	speed: 0.1354s/iter; left time: 3370.1965s
	iters: 200, epoch: 6 | loss: 0.7456926
	speed: 0.0276s/iter; left time: 684.3424s
Epoch: 6 cost time: 8.009328842163086
Epoch: 6, Steps: 263 | Train Loss: 0.5881912 Vali Loss: 0.2894174 Test Loss: 0.4111243
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5135760
	speed: 0.1701s/iter; left time: 4188.6442s
	iters: 200, epoch: 7 | loss: 0.5255007
	speed: 0.0427s/iter; left time: 1045.9661s
Epoch: 7 cost time: 10.829706907272339
Epoch: 7, Steps: 263 | Train Loss: 0.5875996 Vali Loss: 0.2893872 Test Loss: 0.4112383
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6011384
	speed: 0.1802s/iter; left time: 4388.4896s
	iters: 200, epoch: 8 | loss: 0.5994428
	speed: 0.0375s/iter; left time: 909.9128s
Epoch: 8 cost time: 9.95460557937622
Epoch: 8, Steps: 263 | Train Loss: 0.5876285 Vali Loss: 0.2896912 Test Loss: 0.4112593
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5604604
	speed: 0.1403s/iter; left time: 3380.8606s
	iters: 200, epoch: 9 | loss: 0.6213315
	speed: 0.0266s/iter; left time: 638.9316s
Epoch: 9 cost time: 10.011752367019653
Epoch: 9, Steps: 263 | Train Loss: 0.5872193 Vali Loss: 0.2895429 Test Loss: 0.4113217
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6094154
	speed: 0.1297s/iter; left time: 3091.8450s
	iters: 200, epoch: 10 | loss: 0.6050379
	speed: 0.0322s/iter; left time: 764.5588s
Epoch: 10 cost time: 7.477318048477173
Epoch: 10, Steps: 263 | Train Loss: 0.5869814 Vali Loss: 0.2896558 Test Loss: 0.4112858
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5892714
	speed: 0.1175s/iter; left time: 2768.6392s
	iters: 200, epoch: 11 | loss: 0.5462073
	speed: 0.0305s/iter; left time: 716.6330s
Epoch: 11 cost time: 8.099723100662231
Epoch: 11, Steps: 263 | Train Loss: 0.5870912 Vali Loss: 0.2895485 Test Loss: 0.4112690
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.9782494
	speed: 0.1100s/iter; left time: 2563.5147s
	iters: 200, epoch: 12 | loss: 0.7623575
	speed: 0.0260s/iter; left time: 603.3486s
Epoch: 12 cost time: 7.303520202636719
Epoch: 12, Steps: 263 | Train Loss: 0.5873590 Vali Loss: 0.2891881 Test Loss: 0.4112900
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5895495
	speed: 0.1549s/iter; left time: 3569.7363s
	iters: 200, epoch: 13 | loss: 0.6289236
	speed: 0.0428s/iter; left time: 981.6009s
Epoch: 13 cost time: 9.740378618240356
Epoch: 13, Steps: 263 | Train Loss: 0.5869907 Vali Loss: 0.2895028 Test Loss: 0.4112368
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6448734
	speed: 0.1573s/iter; left time: 3584.6356s
	iters: 200, epoch: 14 | loss: 0.4319512
	speed: 0.0439s/iter; left time: 995.4153s
Epoch: 14 cost time: 11.47974157333374
Epoch: 14, Steps: 263 | Train Loss: 0.5861796 Vali Loss: 0.2895298 Test Loss: 0.4114001
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5756190
	speed: 0.1525s/iter; left time: 3435.2139s
	iters: 200, epoch: 15 | loss: 0.4523394
	speed: 0.0330s/iter; left time: 739.1921s
Epoch: 15 cost time: 10.763592004776001
Epoch: 15, Steps: 263 | Train Loss: 0.5867945 Vali Loss: 0.2896114 Test Loss: 0.4112622
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.6418538
	speed: 0.1480s/iter; left time: 3293.9847s
	iters: 200, epoch: 16 | loss: 0.7440302
	speed: 0.0336s/iter; left time: 743.7723s
Epoch: 16 cost time: 8.701371431350708
Epoch: 16, Steps: 263 | Train Loss: 0.5871075 Vali Loss: 0.2895871 Test Loss: 0.4113598
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5318483
	speed: 0.1194s/iter; left time: 2624.9577s
	iters: 200, epoch: 17 | loss: 0.8073686
	speed: 0.0214s/iter; left time: 468.6647s
Epoch: 17 cost time: 7.070037603378296
Epoch: 17, Steps: 263 | Train Loss: 0.5869020 Vali Loss: 0.2899396 Test Loss: 0.4113661
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4788848
	speed: 0.1158s/iter; left time: 2516.9919s
	iters: 200, epoch: 18 | loss: 0.7662941
	speed: 0.0269s/iter; left time: 582.6484s
Epoch: 18 cost time: 6.859600067138672
Epoch: 18, Steps: 263 | Train Loss: 0.5868183 Vali Loss: 0.2895676 Test Loss: 0.4113669
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3950760
	speed: 0.1313s/iter; left time: 2819.4421s
	iters: 200, epoch: 19 | loss: 0.5692084
	speed: 0.0396s/iter; left time: 846.6645s
Epoch: 19 cost time: 11.528590679168701
Epoch: 19, Steps: 263 | Train Loss: 0.5871087 Vali Loss: 0.2898887 Test Loss: 0.4113481
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5860597
	speed: 0.2026s/iter; left time: 4295.2510s
	iters: 200, epoch: 20 | loss: 0.5352144
	speed: 0.0411s/iter; left time: 868.2959s
Epoch: 20 cost time: 12.461554288864136
Epoch: 20, Steps: 263 | Train Loss: 0.5864627 Vali Loss: 0.2899166 Test Loss: 0.4114169
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5169078
	speed: 0.1812s/iter; left time: 3795.5223s
	iters: 200, epoch: 21 | loss: 0.4969521
	speed: 0.0301s/iter; left time: 626.3955s
Epoch: 21 cost time: 9.949964761734009
Epoch: 21, Steps: 263 | Train Loss: 0.5873028 Vali Loss: 0.2899300 Test Loss: 0.4114024
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5918567
	speed: 0.1473s/iter; left time: 3046.5466s
	iters: 200, epoch: 22 | loss: 0.6158400
	speed: 0.0231s/iter; left time: 475.5638s
Epoch: 22 cost time: 8.298887968063354
Epoch: 22, Steps: 263 | Train Loss: 0.5862573 Vali Loss: 0.2898579 Test Loss: 0.4113705
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.40871357917785645, mae:0.3982914686203003, rse:0.5138707756996155, corr:[0.5464932  0.54803467 0.5419854  0.53730357 0.5363273  0.5362063
 0.53456455 0.5323084  0.53113526 0.5309711  0.53065777 0.5296523
 0.5285387  0.527973   0.5275715  0.5265301  0.5248248  0.5232189
 0.52231896 0.521821   0.521058   0.5198555  0.518642   0.5179127
 0.51756275 0.5171369  0.5164785  0.51577103 0.5152328  0.5148255
 0.51431596 0.5136617  0.51293766 0.5122399  0.5115145  0.5107355
 0.5098809  0.5090613  0.50828314 0.50749326 0.5067198  0.506188
 0.50602746 0.506061   0.50597125 0.50554353 0.5048638  0.5040212
 0.50309855 0.502103   0.5010591  0.50014794 0.49942374 0.49881062
 0.49827334 0.49769878 0.49712712 0.49675995 0.49662462 0.4965798
 0.49641132 0.49608028 0.4958432  0.4958719  0.49603966 0.496148
 0.4960499  0.49580443 0.49561492 0.4956048  0.49572235 0.495878
 0.49595508 0.4959472  0.4958017  0.49557027 0.49534392 0.49508885
 0.49482158 0.4945477  0.49430412 0.4940847  0.4938535  0.49357677
 0.493268   0.4929403  0.49259403 0.49224517 0.49189535 0.4915881
 0.49132925 0.49096993 0.4903313  0.4892625  0.4876685  0.4856351
 0.48336112 0.48105037 0.4788238  0.47680545 0.4750733  0.47359347
 0.47217983 0.47075403 0.46943516 0.46825927 0.46713725 0.46586612
 0.46436027 0.46277386 0.46135783 0.46017733 0.459119   0.45789737
 0.4564086  0.4548746  0.45354515 0.45246685 0.45149198 0.45037192
 0.44917843 0.44809413 0.44717538 0.44629204 0.44531435 0.44424632
 0.44324017 0.4423514  0.44139194 0.44023803 0.4389577  0.4377416
 0.43677765 0.4360757  0.43546367 0.43494487 0.4343755  0.43389884
 0.43352488 0.43318304 0.43280232 0.43236542 0.4319567  0.43152535
 0.43087882 0.42988402 0.4285464  0.4272834  0.42633355 0.42574012
 0.4252613  0.42484027 0.42432964 0.4236904  0.42317432 0.42284685
 0.42261136 0.42244846 0.4222597  0.422149   0.42203575 0.42181537
 0.42164338 0.42146268 0.42152727 0.421763   0.42203403 0.42229927
 0.42259678 0.42288616 0.42299473 0.42279837 0.4224072  0.42207158
 0.42202505 0.42228538 0.4225524  0.4225777  0.42232898 0.42200175
 0.42176136 0.42155382 0.4213038  0.42088276 0.42042366 0.4200613
 0.41978005 0.41936165 0.41854686 0.4172659  0.4156616  0.41385093
 0.41190612 0.40990573 0.40785062 0.4059337  0.40439573 0.40309623
 0.40169027 0.3999447  0.39816952 0.39675808 0.39571047 0.39471614
 0.39350933 0.39208522 0.39051765 0.3889712  0.3875388  0.38622746
 0.38490814 0.38359272 0.38229853 0.38105404 0.37985536 0.37868765
 0.37759092 0.37654325 0.37558624 0.37453678 0.37332836 0.37205496
 0.37081966 0.36952025 0.36836737 0.36726108 0.36636937 0.36564896
 0.3649191  0.36393282 0.36258444 0.36126995 0.36036426 0.36002344
 0.35989505 0.35956755 0.35895526 0.35823786 0.35776356 0.35757542
 0.35752636 0.35742316 0.35706636 0.3567681  0.35648063 0.35614344
 0.3559919  0.35622287 0.35674217 0.35700157 0.3568145  0.3563776
 0.35605568 0.35619318 0.35671684 0.35711956 0.3571314  0.35692573
 0.35690996 0.35736004 0.3579987  0.35847622 0.3585952  0.35850742
 0.3585824  0.35893637 0.35957193 0.36010322 0.36036506 0.3603204
 0.36006492 0.35981995 0.35976967 0.3600077  0.36045673 0.36093622
 0.36133674 0.36159733 0.36175007 0.3619378  0.36211762 0.36226958
 0.36228126 0.36205387 0.36155796 0.36081377 0.3598804  0.35872775
 0.35737926 0.3559908  0.35477617 0.35400704 0.3537125  0.35383314
 0.3542015  0.35443923 0.35434565 0.35396916 0.3534097  0.3528277
 0.35241348 0.35207537 0.3515346  0.35065243 0.34956884 0.34850657
 0.3477023  0.3471347  0.3466116  0.34599185 0.34527984 0.34464008
 0.3440388  0.34341517 0.3426461  0.34178248 0.34102866 0.34050936
 0.34011802 0.3396757  0.33908808 0.33842412 0.33779284 0.33718938
 0.33656764 0.33603615 0.3357357  0.33556327 0.33538508 0.33498707
 0.33441913 0.33397824 0.3339768  0.3343032  0.3346287  0.33460656
 0.33412498 0.33350337 0.3331976  0.33340847 0.3338026  0.33395788
 0.33388948 0.33393437 0.33420035 0.33456522 0.33485052 0.33500454
 0.33513966 0.33527836 0.33546966 0.33569053 0.33593723 0.3361619
 0.3363236  0.33641794 0.3363494  0.3363368  0.33661425 0.33729035
 0.33812132 0.3387348  0.33891207 0.33882838 0.33885342 0.33924654
 0.3398073  0.3402283  0.3404438  0.34060043 0.3409517  0.34151024
 0.34206066 0.3423939  0.3425315  0.3427015  0.34298366 0.3432092
 0.3430968  0.34251744 0.3416156  0.34063998 0.3396735  0.3385955
 0.33727375 0.33581108 0.334476   0.3334273  0.33267218 0.33224118
 0.3319934  0.33165154 0.33108118 0.3302818  0.3293876  0.32853788
 0.32780215 0.32707554 0.3260853  0.32473877 0.32320184 0.32179072
 0.32070044 0.31977358 0.31872743 0.31749833 0.31620467 0.31518817
 0.3145283  0.31399596 0.3132874  0.31232235 0.31131834 0.310478
 0.30981404 0.30905962 0.30806956 0.3070688  0.30628034 0.30583858
 0.30557823 0.30515596 0.30449328 0.30377284 0.3032192  0.3028894
 0.3026294  0.30236787 0.302092   0.3018809  0.30159634 0.30112383
 0.30044386 0.29967228 0.29895136 0.29834917 0.29764575 0.29687288
 0.29621142 0.29598302 0.29610348 0.2962288  0.29611605 0.2959057
 0.29582506 0.2959467  0.29604578 0.2958311  0.29540178 0.29514235
 0.29537535 0.29588583 0.29615068 0.2958297  0.29509994 0.29459497
 0.29470962 0.29530895 0.2959092  0.29620042 0.296277   0.2962534
 0.296215   0.29611984 0.29599157 0.2959589  0.29620612 0.29677078
 0.29745394 0.29789373 0.2978573  0.29739884 0.29680872 0.29638764
 0.29619995 0.2959209  0.2951091  0.29345247 0.29103467 0.28822935
 0.2855798  0.28350678 0.28204024 0.28096184 0.28004977 0.2792955
 0.27874497 0.27841097 0.2782235  0.27802902 0.27765554 0.2770725
 0.27628285 0.27539524 0.27441466 0.27335358 0.27217996 0.27084947
 0.26942202 0.26797956 0.26665592 0.265582   0.2647544  0.26403078
 0.26315725 0.26207733 0.26097605 0.26007757 0.2595165  0.25908116
 0.2584325  0.25739595 0.2561822  0.2552335  0.2546727  0.2543573
 0.25402492 0.25337046 0.25244877 0.2515386  0.2508731  0.25046077
 0.25003648 0.24950027 0.24889773 0.24841261 0.24815616 0.24801405
 0.24780917 0.24731027 0.24665917 0.24605127 0.24558176 0.24527568
 0.24511495 0.24509847 0.24526387 0.24552219 0.24574892 0.24579927
 0.24563305 0.24537416 0.24531189 0.24568179 0.24624942 0.24664776
 0.24661767 0.24623132 0.24581389 0.24572408 0.2460039  0.24667348
 0.24738513 0.24782906 0.24788806 0.2477669  0.24772064 0.24800676
 0.24853277 0.24904928 0.24926727 0.24916333 0.2489889  0.24904983
 0.24935913 0.24970677 0.24988392 0.24977477 0.24954166 0.24932264
 0.2491557  0.24883673 0.24809575 0.24677764 0.24488567 0.24259639
 0.24027257 0.23838751 0.2371646  0.23659015 0.2365332  0.23678869
 0.23700759 0.23705226 0.23704325 0.23708399 0.23709767 0.23682623
 0.23612626 0.23511079 0.2341285  0.2334201  0.23286383 0.23211186
 0.2309821  0.22962941 0.22840488 0.22745788 0.22662288 0.22568384
 0.22463314 0.22372556 0.22312386 0.22267072 0.22218332 0.22150806
 0.22073852 0.22014314 0.2197626  0.21932122 0.21861288 0.21770602
 0.21694732 0.21638845 0.21581838 0.21482354 0.21351968 0.21235341
 0.21179469 0.21188977 0.21213703 0.21207394 0.21171172 0.21114288
 0.21053854 0.20989652 0.20932852 0.20908505 0.2093293  0.20963344
 0.2095404  0.20887895 0.20801783 0.20769353 0.20833331 0.20938022
 0.2099194  0.209632   0.20908521 0.20897166 0.20927933 0.20925474
 0.20856464 0.20778674 0.2078478  0.20877415 0.20986833 0.21028753
 0.21010286 0.21001665 0.21039149 0.21083581 0.21080434 0.21044238
 0.21055919 0.2115411  0.21278058 0.21349427 0.21360609 0.21388957
 0.2149686  0.21637872 0.21706547 0.21657975 0.21567948 0.21543775
 0.215988   0.21636097 0.21558636 0.2137026  0.21153562 0.20969889
 0.20805997 0.20623738 0.20432633 0.20293842 0.20252255 0.20280826
 0.2031007  0.20313281 0.20324911 0.2037804  0.20441335 0.20436166
 0.20325235 0.20161411 0.20036604 0.19983198 0.19945124 0.19848622
 0.19691467 0.19542089 0.19453181 0.19399783 0.19311185 0.19164135
 0.19018596 0.18947448 0.18934202 0.18886322 0.18750505 0.18562481
 0.18417798 0.18340215 0.1825736  0.18112537 0.17937994 0.17848212
 0.1784703  0.1781187  0.17604396 0.17306943 0.17123373 0.17170732
 0.17269284 0.17133798 0.16784784 0.16634995 0.1696037  0.17280677]
