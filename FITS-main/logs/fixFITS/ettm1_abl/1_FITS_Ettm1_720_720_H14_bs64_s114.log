Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4477143
	speed: 0.0340s/iter; left time: 873.3169s
	iters: 200, epoch: 1 | loss: 0.4287235
	speed: 0.0247s/iter; left time: 631.9631s
Epoch: 1 cost time: 7.510750770568848
Epoch: 1, Steps: 258 | Train Loss: 0.5067373 Vali Loss: 1.0141232 Test Loss: 0.4416979
Validation loss decreased (inf --> 1.014123).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3928874
	speed: 0.1472s/iter; left time: 3746.1813s
	iters: 200, epoch: 2 | loss: 0.3963578
	speed: 0.0284s/iter; left time: 719.5428s
Epoch: 2 cost time: 9.171617269515991
Epoch: 2, Steps: 258 | Train Loss: 0.4131293 Vali Loss: 0.9652309 Test Loss: 0.4177590
Validation loss decreased (1.014123 --> 0.965231).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3841108
	speed: 0.1306s/iter; left time: 3289.0358s
	iters: 200, epoch: 3 | loss: 0.3888155
	speed: 0.0304s/iter; left time: 763.2220s
Epoch: 3 cost time: 7.282750844955444
Epoch: 3, Steps: 258 | Train Loss: 0.4026298 Vali Loss: 0.9498407 Test Loss: 0.4145276
Validation loss decreased (0.965231 --> 0.949841).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4062220
	speed: 0.1034s/iter; left time: 2576.3880s
	iters: 200, epoch: 4 | loss: 0.4580542
	speed: 0.0318s/iter; left time: 790.4673s
Epoch: 4 cost time: 8.384599208831787
Epoch: 4, Steps: 258 | Train Loss: 0.3994790 Vali Loss: 0.9434583 Test Loss: 0.4150304
Validation loss decreased (0.949841 --> 0.943458).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4420487
	speed: 0.1218s/iter; left time: 3004.3392s
	iters: 200, epoch: 5 | loss: 0.4116120
	speed: 0.0255s/iter; left time: 625.8571s
Epoch: 5 cost time: 7.5398476123809814
Epoch: 5, Steps: 258 | Train Loss: 0.3983752 Vali Loss: 0.9391553 Test Loss: 0.4151621
Validation loss decreased (0.943458 --> 0.939155).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3986238
	speed: 0.1318s/iter; left time: 3216.7806s
	iters: 200, epoch: 6 | loss: 0.4199855
	speed: 0.0533s/iter; left time: 1295.5026s
Epoch: 6 cost time: 13.888001918792725
Epoch: 6, Steps: 258 | Train Loss: 0.3978785 Vali Loss: 0.9385544 Test Loss: 0.4153945
Validation loss decreased (0.939155 --> 0.938554).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4066201
	speed: 0.1630s/iter; left time: 3936.7534s
	iters: 200, epoch: 7 | loss: 0.4103454
	speed: 0.0293s/iter; left time: 704.5293s
Epoch: 7 cost time: 9.446526050567627
Epoch: 7, Steps: 258 | Train Loss: 0.3974246 Vali Loss: 0.9359883 Test Loss: 0.4161712
Validation loss decreased (0.938554 --> 0.935988).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4382278
	speed: 0.1029s/iter; left time: 2457.9440s
	iters: 200, epoch: 8 | loss: 0.4317838
	speed: 0.0223s/iter; left time: 530.8818s
Epoch: 8 cost time: 6.131108283996582
Epoch: 8, Steps: 258 | Train Loss: 0.3973270 Vali Loss: 0.9353138 Test Loss: 0.4158286
Validation loss decreased (0.935988 --> 0.935314).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3957881
	speed: 0.0907s/iter; left time: 2144.3947s
	iters: 200, epoch: 9 | loss: 0.4034412
	speed: 0.0300s/iter; left time: 705.8509s
Epoch: 9 cost time: 6.997889518737793
Epoch: 9, Steps: 258 | Train Loss: 0.3971203 Vali Loss: 0.9334255 Test Loss: 0.4160736
Validation loss decreased (0.935314 --> 0.933426).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4185200
	speed: 0.1497s/iter; left time: 3500.0759s
	iters: 200, epoch: 10 | loss: 0.3754901
	speed: 0.0248s/iter; left time: 577.2281s
Epoch: 10 cost time: 6.76169490814209
Epoch: 10, Steps: 258 | Train Loss: 0.3968780 Vali Loss: 0.9343039 Test Loss: 0.4162051
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3780868
	speed: 0.0843s/iter; left time: 1948.9282s
	iters: 200, epoch: 11 | loss: 0.3939108
	speed: 0.0184s/iter; left time: 422.8356s
Epoch: 11 cost time: 5.136542081832886
Epoch: 11, Steps: 258 | Train Loss: 0.3967993 Vali Loss: 0.9329076 Test Loss: 0.4160784
Validation loss decreased (0.933426 --> 0.932908).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4012182
	speed: 0.0632s/iter; left time: 1444.1219s
	iters: 200, epoch: 12 | loss: 0.4129649
	speed: 0.0202s/iter; left time: 459.9701s
Epoch: 12 cost time: 4.898904800415039
Epoch: 12, Steps: 258 | Train Loss: 0.3968941 Vali Loss: 0.9324608 Test Loss: 0.4163824
Validation loss decreased (0.932908 --> 0.932461).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3642791
	speed: 0.0717s/iter; left time: 1620.1429s
	iters: 200, epoch: 13 | loss: 0.4125378
	speed: 0.0212s/iter; left time: 477.6940s
Epoch: 13 cost time: 5.317910671234131
Epoch: 13, Steps: 258 | Train Loss: 0.3968457 Vali Loss: 0.9324865 Test Loss: 0.4162943
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3912449
	speed: 0.0710s/iter; left time: 1587.3681s
	iters: 200, epoch: 14 | loss: 0.3794019
	speed: 0.0183s/iter; left time: 406.5647s
Epoch: 14 cost time: 4.55252742767334
Epoch: 14, Steps: 258 | Train Loss: 0.3967264 Vali Loss: 0.9322935 Test Loss: 0.4161350
Validation loss decreased (0.932461 --> 0.932293).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3797838
	speed: 0.0691s/iter; left time: 1526.9794s
	iters: 200, epoch: 15 | loss: 0.4117922
	speed: 0.0223s/iter; left time: 490.6029s
Epoch: 15 cost time: 5.947998762130737
Epoch: 15, Steps: 258 | Train Loss: 0.3967743 Vali Loss: 0.9322793 Test Loss: 0.4164196
Validation loss decreased (0.932293 --> 0.932279).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3737689
	speed: 0.1063s/iter; left time: 2320.2917s
	iters: 200, epoch: 16 | loss: 0.3741005
	speed: 0.0174s/iter; left time: 378.6705s
Epoch: 16 cost time: 5.166274785995483
Epoch: 16, Steps: 258 | Train Loss: 0.3966416 Vali Loss: 0.9312431 Test Loss: 0.4157592
Validation loss decreased (0.932279 --> 0.931243).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3928449
	speed: 0.0597s/iter; left time: 1288.5021s
	iters: 200, epoch: 17 | loss: 0.3642943
	speed: 0.0159s/iter; left time: 341.7471s
Epoch: 17 cost time: 4.390852451324463
Epoch: 17, Steps: 258 | Train Loss: 0.3966053 Vali Loss: 0.9324467 Test Loss: 0.4159833
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4133730
	speed: 0.0716s/iter; left time: 1526.8673s
	iters: 200, epoch: 18 | loss: 0.3939097
	speed: 0.0161s/iter; left time: 340.7835s
Epoch: 18 cost time: 4.25384259223938
Epoch: 18, Steps: 258 | Train Loss: 0.3965566 Vali Loss: 0.9307193 Test Loss: 0.4160227
Validation loss decreased (0.931243 --> 0.930719).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4241287
	speed: 0.0622s/iter; left time: 1309.8231s
	iters: 200, epoch: 19 | loss: 0.3936009
	speed: 0.0172s/iter; left time: 359.8468s
Epoch: 19 cost time: 4.34778618812561
Epoch: 19, Steps: 258 | Train Loss: 0.3965602 Vali Loss: 0.9309354 Test Loss: 0.4161091
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3972679
	speed: 0.0587s/iter; left time: 1220.1792s
	iters: 200, epoch: 20 | loss: 0.3904997
	speed: 0.0165s/iter; left time: 341.7853s
Epoch: 20 cost time: 4.019030332565308
Epoch: 20, Steps: 258 | Train Loss: 0.3964000 Vali Loss: 0.9321612 Test Loss: 0.4159058
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3829969
	speed: 0.0542s/iter; left time: 1112.4451s
	iters: 200, epoch: 21 | loss: 0.3849583
	speed: 0.0097s/iter; left time: 198.5409s
Epoch: 21 cost time: 2.8682103157043457
Epoch: 21, Steps: 258 | Train Loss: 0.3965087 Vali Loss: 0.9312366 Test Loss: 0.4163118
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3900791
	speed: 0.0408s/iter; left time: 827.9783s
	iters: 200, epoch: 22 | loss: 0.4388872
	speed: 0.0092s/iter; left time: 186.6921s
Epoch: 22 cost time: 2.7821319103240967
Epoch: 22, Steps: 258 | Train Loss: 0.3963683 Vali Loss: 0.9305051 Test Loss: 0.4161939
Validation loss decreased (0.930719 --> 0.930505).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4090898
	speed: 0.0493s/iter; left time: 986.6506s
	iters: 200, epoch: 23 | loss: 0.3845296
	speed: 0.0105s/iter; left time: 208.8677s
Epoch: 23 cost time: 3.2627530097961426
Epoch: 23, Steps: 258 | Train Loss: 0.3964171 Vali Loss: 0.9309833 Test Loss: 0.4163080
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4037973
	speed: 0.0466s/iter; left time: 921.6907s
	iters: 200, epoch: 24 | loss: 0.4079799
	speed: 0.0099s/iter; left time: 194.2233s
Epoch: 24 cost time: 2.948460817337036
Epoch: 24, Steps: 258 | Train Loss: 0.3964108 Vali Loss: 0.9316263 Test Loss: 0.4164067
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3954456
	speed: 0.0474s/iter; left time: 924.0186s
	iters: 200, epoch: 25 | loss: 0.4070653
	speed: 0.0099s/iter; left time: 193.0937s
Epoch: 25 cost time: 3.0090229511260986
Epoch: 25, Steps: 258 | Train Loss: 0.3964370 Vali Loss: 0.9304574 Test Loss: 0.4164515
Validation loss decreased (0.930505 --> 0.930457).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3916013
	speed: 0.0469s/iter; left time: 903.2870s
	iters: 200, epoch: 26 | loss: 0.4253387
	speed: 0.0110s/iter; left time: 210.8822s
Epoch: 26 cost time: 2.9685518741607666
Epoch: 26, Steps: 258 | Train Loss: 0.3962236 Vali Loss: 0.9314858 Test Loss: 0.4160408
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3758536
	speed: 0.0830s/iter; left time: 1577.2995s
	iters: 200, epoch: 27 | loss: 0.4052127
	speed: 0.0103s/iter; left time: 195.0807s
Epoch: 27 cost time: 6.434128761291504
Epoch: 27, Steps: 258 | Train Loss: 0.3963693 Vali Loss: 0.9309317 Test Loss: 0.4161362
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4010943
	speed: 0.0473s/iter; left time: 886.4494s
	iters: 200, epoch: 28 | loss: 0.3935373
	speed: 0.0101s/iter; left time: 187.8059s
Epoch: 28 cost time: 2.961341142654419
Epoch: 28, Steps: 258 | Train Loss: 0.3962329 Vali Loss: 0.9312548 Test Loss: 0.4160787
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4224916
	speed: 0.0498s/iter; left time: 920.9457s
	iters: 200, epoch: 29 | loss: 0.3956257
	speed: 0.0122s/iter; left time: 224.3474s
Epoch: 29 cost time: 3.23055100440979
Epoch: 29, Steps: 258 | Train Loss: 0.3962714 Vali Loss: 0.9314520 Test Loss: 0.4163448
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4014650
	speed: 0.0461s/iter; left time: 838.9922s
	iters: 200, epoch: 30 | loss: 0.3889440
	speed: 0.0099s/iter; left time: 179.8586s
Epoch: 30 cost time: 2.97731614112854
Epoch: 30, Steps: 258 | Train Loss: 0.3963841 Vali Loss: 0.9311702 Test Loss: 0.4162447
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4016446
	speed: 0.0471s/iter; left time: 846.4566s
	iters: 200, epoch: 31 | loss: 0.3967610
	speed: 0.0114s/iter; left time: 204.1168s
Epoch: 31 cost time: 3.531937837600708
Epoch: 31, Steps: 258 | Train Loss: 0.3963604 Vali Loss: 0.9308642 Test Loss: 0.4163989
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3904861
	speed: 0.0504s/iter; left time: 892.8267s
	iters: 200, epoch: 32 | loss: 0.3954504
	speed: 0.0694s/iter; left time: 1220.9099s
Epoch: 32 cost time: 12.82896614074707
Epoch: 32, Steps: 258 | Train Loss: 0.3962648 Vali Loss: 0.9295639 Test Loss: 0.4163005
Validation loss decreased (0.930457 --> 0.929564).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3704237
	speed: 0.1329s/iter; left time: 2317.8178s
	iters: 200, epoch: 33 | loss: 0.3957152
	speed: 0.0099s/iter; left time: 171.0677s
Epoch: 33 cost time: 3.0598177909851074
Epoch: 33, Steps: 258 | Train Loss: 0.3962415 Vali Loss: 0.9310322 Test Loss: 0.4162150
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4196925
	speed: 0.0502s/iter; left time: 862.0769s
	iters: 200, epoch: 34 | loss: 0.3757139
	speed: 0.0104s/iter; left time: 177.0425s
Epoch: 34 cost time: 2.969379425048828
Epoch: 34, Steps: 258 | Train Loss: 0.3963743 Vali Loss: 0.9311543 Test Loss: 0.4163463
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4082580
	speed: 0.0498s/iter; left time: 843.2912s
	iters: 200, epoch: 35 | loss: 0.4041365
	speed: 0.0115s/iter; left time: 193.3585s
Epoch: 35 cost time: 3.038181781768799
Epoch: 35, Steps: 258 | Train Loss: 0.3962697 Vali Loss: 0.9303565 Test Loss: 0.4162902
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3941039
	speed: 0.0496s/iter; left time: 826.5536s
	iters: 200, epoch: 36 | loss: 0.4044402
	speed: 0.0101s/iter; left time: 166.6006s
Epoch: 36 cost time: 3.031623363494873
Epoch: 36, Steps: 258 | Train Loss: 0.3961375 Vali Loss: 0.9314566 Test Loss: 0.4163692
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3853141
	speed: 0.0511s/iter; left time: 838.2122s
	iters: 200, epoch: 37 | loss: 0.3650906
	speed: 0.0108s/iter; left time: 176.9103s
Epoch: 37 cost time: 3.124664306640625
Epoch: 37, Steps: 258 | Train Loss: 0.3963369 Vali Loss: 0.9305735 Test Loss: 0.4163113
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3602857
	speed: 0.0887s/iter; left time: 1433.3519s
	iters: 200, epoch: 38 | loss: 0.3957328
	speed: 0.0275s/iter; left time: 441.1817s
Epoch: 38 cost time: 7.747002601623535
Epoch: 38, Steps: 258 | Train Loss: 0.3962822 Vali Loss: 0.9305460 Test Loss: 0.4162286
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.3972861
	speed: 0.1018s/iter; left time: 1618.1978s
	iters: 200, epoch: 39 | loss: 0.3944027
	speed: 0.0276s/iter; left time: 436.1746s
Epoch: 39 cost time: 6.785593032836914
Epoch: 39, Steps: 258 | Train Loss: 0.3962696 Vali Loss: 0.9310983 Test Loss: 0.4161305
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4304809
	speed: 0.1117s/iter; left time: 1746.2779s
	iters: 200, epoch: 40 | loss: 0.3703970
	speed: 0.0288s/iter; left time: 446.8026s
Epoch: 40 cost time: 7.766972780227661
Epoch: 40, Steps: 258 | Train Loss: 0.3961222 Vali Loss: 0.9309270 Test Loss: 0.4162343
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4410628
	speed: 0.1224s/iter; left time: 1882.5923s
	iters: 200, epoch: 41 | loss: 0.3950317
	speed: 0.0282s/iter; left time: 430.7209s
Epoch: 41 cost time: 7.32417893409729
Epoch: 41, Steps: 258 | Train Loss: 0.3962156 Vali Loss: 0.9302255 Test Loss: 0.4163277
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4308936
	speed: 0.1572s/iter; left time: 2377.6546s
	iters: 200, epoch: 42 | loss: 0.3778821
	speed: 0.0186s/iter; left time: 279.4983s
Epoch: 42 cost time: 7.11238956451416
Epoch: 42, Steps: 258 | Train Loss: 0.3960397 Vali Loss: 0.9307780 Test Loss: 0.4162551
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3624757
	speed: 0.0842s/iter; left time: 1252.2152s
	iters: 200, epoch: 43 | loss: 0.4151791
	speed: 0.0179s/iter; left time: 264.7398s
Epoch: 43 cost time: 6.064055681228638
Epoch: 43, Steps: 258 | Train Loss: 0.3961092 Vali Loss: 0.9307898 Test Loss: 0.4161677
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4103840
	speed: 0.0829s/iter; left time: 1211.6134s
	iters: 200, epoch: 44 | loss: 0.3773710
	speed: 0.0203s/iter; left time: 294.7116s
Epoch: 44 cost time: 5.958070278167725
Epoch: 44, Steps: 258 | Train Loss: 0.3961729 Vali Loss: 0.9311799 Test Loss: 0.4164141
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.3883588
	speed: 0.0777s/iter; left time: 1114.5709s
	iters: 200, epoch: 45 | loss: 0.4268656
	speed: 0.0193s/iter; left time: 275.0501s
Epoch: 45 cost time: 5.652441740036011
Epoch: 45, Steps: 258 | Train Loss: 0.3962207 Vali Loss: 0.9312026 Test Loss: 0.4163670
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4238570
	speed: 0.0868s/iter; left time: 1222.8420s
	iters: 200, epoch: 46 | loss: 0.3594434
	speed: 0.0153s/iter; left time: 214.2411s
Epoch: 46 cost time: 5.412306547164917
Epoch: 46, Steps: 258 | Train Loss: 0.3961601 Vali Loss: 0.9305281 Test Loss: 0.4162663
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3995604
	speed: 0.0765s/iter; left time: 1058.7126s
	iters: 200, epoch: 47 | loss: 0.4273967
	speed: 0.0171s/iter; left time: 234.2068s
Epoch: 47 cost time: 5.829781532287598
Epoch: 47, Steps: 258 | Train Loss: 0.3961841 Vali Loss: 0.9307982 Test Loss: 0.4162151
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.3784197
	speed: 0.1047s/iter; left time: 1420.8976s
	iters: 200, epoch: 48 | loss: 0.3915083
	speed: 0.0178s/iter; left time: 239.7801s
Epoch: 48 cost time: 5.3001790046691895
Epoch: 48, Steps: 258 | Train Loss: 0.3962302 Vali Loss: 0.9305947 Test Loss: 0.4162823
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.3641892
	speed: 0.0857s/iter; left time: 1141.0474s
	iters: 200, epoch: 49 | loss: 0.4170124
	speed: 0.0363s/iter; left time: 480.0090s
Epoch: 49 cost time: 7.307162761688232
Epoch: 49, Steps: 258 | Train Loss: 0.3962268 Vali Loss: 0.9302789 Test Loss: 0.4163508
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.3897492
	speed: 0.0854s/iter; left time: 1114.9836s
	iters: 200, epoch: 50 | loss: 0.4367202
	speed: 0.0181s/iter; left time: 233.9840s
Epoch: 50 cost time: 5.460687637329102
Epoch: 50, Steps: 258 | Train Loss: 0.3960843 Vali Loss: 0.9303783 Test Loss: 0.4163012
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.3980195
	speed: 0.1272s/iter; left time: 1628.3354s
	iters: 200, epoch: 51 | loss: 0.4028886
	speed: 0.0150s/iter; left time: 190.5385s
Epoch: 51 cost time: 5.711114406585693
Epoch: 51, Steps: 258 | Train Loss: 0.3962082 Vali Loss: 0.9294788 Test Loss: 0.4162664
Validation loss decreased (0.929564 --> 0.929479).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4115993
	speed: 0.0608s/iter; left time: 762.3732s
	iters: 200, epoch: 52 | loss: 0.3781745
	speed: 0.0123s/iter; left time: 153.2464s
Epoch: 52 cost time: 3.4238953590393066
Epoch: 52, Steps: 258 | Train Loss: 0.3961479 Vali Loss: 0.9299415 Test Loss: 0.4163234
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4124399
	speed: 0.0470s/iter; left time: 577.8803s
	iters: 200, epoch: 53 | loss: 0.3952579
	speed: 0.0127s/iter; left time: 154.6281s
Epoch: 53 cost time: 4.372622013092041
Epoch: 53, Steps: 258 | Train Loss: 0.3960106 Vali Loss: 0.9303172 Test Loss: 0.4162946
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.3899196
	speed: 0.0658s/iter; left time: 791.4746s
	iters: 200, epoch: 54 | loss: 0.3988271
	speed: 0.0120s/iter; left time: 143.1632s
Epoch: 54 cost time: 3.587257146835327
Epoch: 54, Steps: 258 | Train Loss: 0.3961798 Vali Loss: 0.9299610 Test Loss: 0.4162906
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.3697720
	speed: 0.1065s/iter; left time: 1253.3219s
	iters: 200, epoch: 55 | loss: 0.3745194
	speed: 0.0130s/iter; left time: 151.4188s
Epoch: 55 cost time: 4.677119016647339
Epoch: 55, Steps: 258 | Train Loss: 0.3960576 Vali Loss: 0.9299532 Test Loss: 0.4162518
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4013725
	speed: 0.0764s/iter; left time: 879.1875s
	iters: 200, epoch: 56 | loss: 0.4005040
	speed: 0.0127s/iter; left time: 145.0167s
Epoch: 56 cost time: 4.038032293319702
Epoch: 56, Steps: 258 | Train Loss: 0.3961985 Vali Loss: 0.9299493 Test Loss: 0.4162169
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.4051300
	speed: 0.0550s/iter; left time: 618.9540s
	iters: 200, epoch: 57 | loss: 0.3876346
	speed: 0.0140s/iter; left time: 155.6709s
Epoch: 57 cost time: 3.9039621353149414
Epoch: 57, Steps: 258 | Train Loss: 0.3962131 Vali Loss: 0.9309734 Test Loss: 0.4162959
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.3757812
	speed: 0.0575s/iter; left time: 631.9281s
	iters: 200, epoch: 58 | loss: 0.4107864
	speed: 0.0157s/iter; left time: 170.7163s
Epoch: 58 cost time: 3.8680601119995117
Epoch: 58, Steps: 258 | Train Loss: 0.3960617 Vali Loss: 0.9302146 Test Loss: 0.4162660
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.3892443
	speed: 0.0575s/iter; left time: 617.3032s
	iters: 200, epoch: 59 | loss: 0.4007147
	speed: 0.0303s/iter; left time: 322.0198s
Epoch: 59 cost time: 5.6899590492248535
Epoch: 59, Steps: 258 | Train Loss: 0.3961163 Vali Loss: 0.9302713 Test Loss: 0.4162936
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.4124359
	speed: 0.0516s/iter; left time: 540.5387s
	iters: 200, epoch: 60 | loss: 0.3881229
	speed: 0.0136s/iter; left time: 140.8465s
Epoch: 60 cost time: 4.775459289550781
Epoch: 60, Steps: 258 | Train Loss: 0.3961743 Vali Loss: 0.9307683 Test Loss: 0.4162791
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.3857814
	speed: 0.1205s/iter; left time: 1231.1393s
	iters: 200, epoch: 61 | loss: 0.3668333
	speed: 0.0159s/iter; left time: 160.8293s
Epoch: 61 cost time: 4.371380805969238
Epoch: 61, Steps: 258 | Train Loss: 0.3960737 Vali Loss: 0.9302732 Test Loss: 0.4162385
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.3615723
	speed: 0.1183s/iter; left time: 1178.9300s
	iters: 200, epoch: 62 | loss: 0.4143371
	speed: 0.0312s/iter; left time: 307.2769s
Epoch: 62 cost time: 7.039128303527832
Epoch: 62, Steps: 258 | Train Loss: 0.3960193 Vali Loss: 0.9305146 Test Loss: 0.4162380
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.3727719
	speed: 0.1049s/iter; left time: 1018.3137s
	iters: 200, epoch: 63 | loss: 0.3907121
	speed: 0.0136s/iter; left time: 130.8332s
Epoch: 63 cost time: 3.94779109954834
Epoch: 63, Steps: 258 | Train Loss: 0.3960974 Vali Loss: 0.9305502 Test Loss: 0.4162867
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.4105310
	speed: 0.0527s/iter; left time: 498.3213s
	iters: 200, epoch: 64 | loss: 0.3739062
	speed: 0.0199s/iter; left time: 186.3714s
Epoch: 64 cost time: 4.422623872756958
Epoch: 64, Steps: 258 | Train Loss: 0.3961193 Vali Loss: 0.9306819 Test Loss: 0.4162408
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.4105760
	speed: 0.0545s/iter; left time: 500.7912s
	iters: 200, epoch: 65 | loss: 0.3991693
	speed: 0.0156s/iter; left time: 142.0176s
Epoch: 65 cost time: 4.342846155166626
Epoch: 65, Steps: 258 | Train Loss: 0.3960880 Vali Loss: 0.9310240 Test Loss: 0.4162421
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.3915517
	speed: 0.0827s/iter; left time: 738.9699s
	iters: 200, epoch: 66 | loss: 0.3957134
	speed: 0.0128s/iter; left time: 113.1759s
Epoch: 66 cost time: 3.5863139629364014
Epoch: 66, Steps: 258 | Train Loss: 0.3959822 Vali Loss: 0.9309605 Test Loss: 0.4162469
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.3932860
	speed: 0.0575s/iter; left time: 498.7764s
	iters: 200, epoch: 67 | loss: 0.3946303
	speed: 0.0131s/iter; left time: 111.9059s
Epoch: 67 cost time: 3.7474966049194336
Epoch: 67, Steps: 258 | Train Loss: 0.3961783 Vali Loss: 0.9306845 Test Loss: 0.4162431
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.4152387
	speed: 0.0496s/iter; left time: 417.6373s
	iters: 200, epoch: 68 | loss: 0.3755791
	speed: 0.0106s/iter; left time: 88.3502s
Epoch: 68 cost time: 3.1050126552581787
Epoch: 68, Steps: 258 | Train Loss: 0.3961060 Vali Loss: 0.9302186 Test Loss: 0.4162328
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.3970221
	speed: 0.0550s/iter; left time: 448.6712s
	iters: 200, epoch: 69 | loss: 0.3837773
	speed: 0.0113s/iter; left time: 90.9424s
Epoch: 69 cost time: 3.225569725036621
Epoch: 69, Steps: 258 | Train Loss: 0.3961719 Vali Loss: 0.9304118 Test Loss: 0.4162550
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.3955660
	speed: 0.0540s/iter; left time: 426.5278s
	iters: 200, epoch: 70 | loss: 0.3832370
	speed: 0.0119s/iter; left time: 92.5130s
Epoch: 70 cost time: 3.7944421768188477
Epoch: 70, Steps: 258 | Train Loss: 0.3961670 Vali Loss: 0.9308498 Test Loss: 0.4162662
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.4181658
	speed: 0.1320s/iter; left time: 1008.3286s
	iters: 200, epoch: 71 | loss: 0.3877315
	speed: 0.0380s/iter; left time: 286.6711s
Epoch: 71 cost time: 9.56981897354126
Epoch: 71, Steps: 258 | Train Loss: 0.3961170 Vali Loss: 0.9305686 Test Loss: 0.4162610
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4152662456035614, mae:0.4115116596221924, rse:0.6131038665771484, corr:[0.52694374 0.5319178  0.53350776 0.5340515  0.5350803  0.5367823
 0.53824097 0.53894097 0.5393103  0.5397599  0.5404484  0.5410951
 0.54153955 0.5416845  0.54144835 0.540707   0.5395462  0.53829557
 0.53715265 0.53616077 0.5351212  0.53377616 0.53215736 0.53073215
 0.5295066  0.5286048  0.52780265 0.5268566  0.52610314 0.5258798
 0.52637756 0.5274911  0.5285666  0.52920955 0.5291781  0.52908254
 0.5290525  0.5291299  0.5290835  0.5286898  0.52823323 0.5279273
 0.5279313  0.52814454 0.5281508  0.52786505 0.52763975 0.5277012
 0.52791965 0.5279458  0.5276982  0.5271993  0.52676964 0.52653384
 0.52658165 0.52672046 0.52673703 0.52651733 0.52627164 0.5260485
 0.5259546  0.52594656 0.52594876 0.52582437 0.5257681  0.52587765
 0.52603734 0.52611727 0.5261447  0.52614456 0.52623105 0.5263174
 0.52626395 0.5259835  0.52562046 0.5252797  0.52505374 0.524989
 0.52500147 0.52498096 0.5248711  0.5247019  0.5245851  0.52451426
 0.5245532  0.5245854  0.5245714  0.52447796 0.5244157  0.52458304
 0.52497995 0.5254702  0.52589536 0.5261083  0.52609044 0.5258536
 0.52555066 0.5253439  0.525091   0.5248421  0.5245925  0.5243933
 0.5243039  0.5242435  0.5242049  0.5241773  0.5239878  0.52370733
 0.52336717 0.5231226  0.522995   0.5228753  0.5226791  0.52241373
 0.52211934 0.52189916 0.5217826  0.5216701  0.5214773  0.521227
 0.52092975 0.520644   0.52038735 0.5202663  0.52023077 0.52008736
 0.5197821  0.51946604 0.5192495  0.5192277  0.5192663  0.5192347
 0.51914895 0.51904446 0.5189225  0.51887333 0.51895994 0.5189851
 0.51897144 0.5190106  0.5190796  0.5192165  0.5193773  0.5194792
 0.5194932  0.519417   0.5194089  0.51950395 0.51965904 0.51967955
 0.5195063  0.51931375 0.51921284 0.51923007 0.51928324 0.5193648
 0.519388   0.51933026 0.51929295 0.5192267  0.51920956 0.51921475
 0.51924646 0.5193124  0.51944816 0.51963377 0.5198369  0.5200378
 0.520186   0.5202703  0.5204024  0.520574   0.52062315 0.5205189
 0.5203348  0.5201797  0.5201344  0.5201926  0.5202526  0.5202811
 0.52025294 0.520231   0.52023524 0.52035195 0.52055836 0.52079386
 0.5210237  0.5213191  0.52168924 0.5220403  0.5222545  0.5222429
 0.52202034 0.5217004  0.5212752  0.5207526  0.52015966 0.5195346
 0.5188429  0.51808983 0.5172583  0.51642317 0.5156352  0.5149747
 0.51445866 0.5140106  0.5135174  0.512958   0.5123426  0.51172787
 0.51113087 0.51059294 0.5100416  0.50940704 0.508633   0.50792485
 0.5074526  0.50716615 0.50697833 0.50672656 0.5064085  0.50619376
 0.5062739  0.5065238  0.5068434  0.5070903  0.5071302  0.50704336
 0.5070001  0.50712216 0.5073716  0.50755143 0.5075626  0.50744087
 0.5072898  0.50728893 0.507398   0.5077072  0.508035   0.50830907
 0.50842094 0.5083036  0.50811654 0.5080109  0.508041   0.50807935
 0.5080244  0.50788635 0.50770724 0.507602   0.50755805 0.507622
 0.50764436 0.50764924 0.50757176 0.5074462  0.50735635 0.50736284
 0.5074548  0.50758576 0.5076626  0.5076615  0.50763696 0.5077479
 0.5079171  0.5080847  0.50817925 0.5081957  0.5081222  0.50797004
 0.50785744 0.5078241  0.50788754 0.50799954 0.5080657  0.50805134
 0.5079443  0.5079034  0.5079203  0.50805396 0.50819755 0.50826234
 0.50830835 0.50832695 0.5083271  0.5083022  0.5081496  0.50779355
 0.50728625 0.506781   0.5063431  0.5059174  0.50546855 0.50494254
 0.5043466  0.5036785  0.503097   0.5026368  0.5022638  0.50191396
 0.5014798  0.50099003 0.50055355 0.50022775 0.5000087  0.49980167
 0.49963084 0.49937433 0.499044   0.49871868 0.49853754 0.49847156
 0.4984556  0.49845237 0.49835756 0.49820974 0.4980944  0.49809185
 0.49814287 0.49811283 0.49797565 0.49779177 0.4976648  0.4976412
 0.49769035 0.49776244 0.4977149  0.4975495  0.49730983 0.4970908
 0.4970471  0.49712917 0.49722576 0.49730358 0.49732757 0.49739772
 0.4974545  0.49735963 0.49725565 0.49720278 0.4972484  0.49732727
 0.49733096 0.49729002 0.49718845 0.49708128 0.4970159  0.497028
 0.49700832 0.49696803 0.4969097  0.49682185 0.49677947 0.49676853
 0.49673095 0.49668658 0.49663958 0.49660677 0.49663135 0.49667332
 0.4966213  0.49653202 0.49642688 0.49635923 0.4963772  0.49645916
 0.4964689  0.49643737 0.49636385 0.4962284  0.4961394  0.49614498
 0.49623448 0.4963712  0.49649352 0.49663085 0.49679774 0.4970658
 0.49743184 0.49783123 0.49817467 0.49837103 0.49839708 0.49830166
 0.49811372 0.4978498  0.49752143 0.49708983 0.49654934 0.49608892
 0.4957371  0.49548236 0.49524117 0.4949815  0.49461207 0.49418193
 0.49375162 0.49338073 0.49305946 0.4927175  0.4922678  0.49183795
 0.49145824 0.49121863 0.49100664 0.49078554 0.4905167  0.49028262
 0.4901258  0.4900953  0.49014473 0.49022552 0.49030003 0.49030253
 0.49041203 0.49052465 0.49071205 0.4908475  0.4909411  0.49103123
 0.49103472 0.49104097 0.49102628 0.49103668 0.49102885 0.49105534
 0.49104953 0.49103332 0.49098152 0.49103877 0.49114236 0.49134153
 0.49145162 0.4913488  0.49112892 0.49094287 0.49084446 0.49080616
 0.49073786 0.49059984 0.49042764 0.49028015 0.4902315  0.49028873
 0.4903561  0.49039468 0.49035794 0.49026236 0.49016502 0.49008262
 0.49010473 0.49019963 0.4903355  0.49044597 0.4905144  0.49060604
 0.4906171  0.49058697 0.4905362  0.49053413 0.4905081  0.49050692
 0.49048713 0.49049526 0.49046794 0.49045897 0.49048167 0.49050322
 0.49052185 0.49046943 0.49039894 0.4903463  0.490355   0.49041393
 0.49045298 0.49047887 0.4904595  0.49031973 0.49005434 0.48971984
 0.48934072 0.4889165  0.48842198 0.48782474 0.48717076 0.48658365
 0.48614147 0.48569188 0.4851812  0.48459694 0.48389262 0.48313567
 0.48242217 0.48185158 0.48137233 0.48088083 0.48035797 0.47987613
 0.47943947 0.4791147  0.47886187 0.4785165  0.4782429  0.4780415
 0.47797143 0.47798193 0.47798067 0.47800574 0.47804758 0.47816905
 0.47837454 0.47855154 0.4786678  0.47882837 0.47899675 0.47915706
 0.4792589  0.47937664 0.4795282  0.4796305  0.47970024 0.479818
 0.4800075  0.4802553  0.48050395 0.48066795 0.48081747 0.4809798
 0.48106202 0.48099792 0.48080942 0.4806849  0.48070917 0.48081332
 0.48089507 0.48086238 0.48074427 0.48061952 0.48055226 0.4805505
 0.48057738 0.4805598  0.48047465 0.48033145 0.48018545 0.48015857
 0.48023823 0.48037955 0.4805163  0.48054507 0.48049986 0.48052084
 0.48053613 0.48054096 0.48053715 0.4805482  0.48054928 0.48058566
 0.48063636 0.48068872 0.4807041  0.48068205 0.48063782 0.48057732
 0.48049656 0.4805176  0.48051336 0.48049602 0.48045924 0.48045585
 0.4805351  0.4807101  0.48097327 0.4811587  0.4811075  0.4807507
 0.4801871  0.47960326 0.47906902 0.47855094 0.47801208 0.47751743
 0.47702944 0.4765661  0.47612154 0.47570032 0.47530246 0.47484156
 0.47433895 0.47376677 0.47315997 0.47254753 0.47202206 0.47158176
 0.47120243 0.47084066 0.47051287 0.47021818 0.46996284 0.4697733
 0.46971792 0.46971315 0.4697329  0.46974757 0.4698044  0.46988153
 0.47008532 0.47030184 0.47046053 0.47054198 0.470575   0.47068298
 0.47087806 0.4710528  0.47122303 0.47132793 0.47140232 0.47145933
 0.4715627  0.47172555 0.47182792 0.47191486 0.47216284 0.47252494
 0.4727769  0.47286373 0.4727884  0.47264823 0.47262532 0.47272125
 0.47273317 0.47267362 0.47257763 0.472453   0.47242948 0.47241047
 0.47224402 0.47193357 0.47157744 0.4711914  0.4709778  0.4709151
 0.47092915 0.4709079  0.4708285  0.47074363 0.47067115 0.47065765
 0.47062752 0.47048154 0.47034508 0.47030306 0.4703451  0.4704025
 0.47042057 0.47040373 0.47039875 0.47043818 0.47050232 0.470538
 0.47053337 0.47058687 0.47068277 0.47073752 0.4708384  0.47097307
 0.47115946 0.4713857  0.47162443 0.4718021  0.4717736  0.47150084
 0.47107348 0.47068113 0.4703446  0.4699586  0.4694947  0.46902838
 0.46866024 0.46834484 0.46809092 0.4677515  0.46716926 0.46663675
 0.46615472 0.46593684 0.46583566 0.46574947 0.46549097 0.4650303
 0.4645672  0.46424505 0.46414736 0.46404633 0.4639318  0.4638302
 0.46372566 0.4637611  0.46385977 0.46394473 0.46407405 0.46412212
 0.464231   0.46439287 0.4646464  0.46508098 0.4654477  0.46574053
 0.46596736 0.46619803 0.4664762  0.4667836  0.46719432 0.46757746
 0.46801886 0.4684814  0.4688021  0.46922415 0.46933183 0.46637943]
