Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=18, out_features=90, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1451520.0
params:  1710.0
Trainable parameters:  1710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7764542
	speed: 0.1994s/iter; left time: 5205.7429s
	iters: 200, epoch: 1 | loss: 0.6584901
	speed: 0.1878s/iter; left time: 4883.2938s
Epoch: 1 cost time: 51.32288217544556
Epoch: 1, Steps: 262 | Train Loss: 0.7854559 Vali Loss: 1.2308559 Test Loss: 0.6581520
Validation loss decreased (inf --> 1.230856).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5091867
	speed: 0.8435s/iter; left time: 21794.8856s
	iters: 200, epoch: 2 | loss: 0.5039510
	speed: 0.1896s/iter; left time: 4879.0677s
Epoch: 2 cost time: 48.37744879722595
Epoch: 2, Steps: 262 | Train Loss: 0.5128987 Vali Loss: 1.0582224 Test Loss: 0.5007777
Validation loss decreased (1.230856 --> 1.058222).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4707518
	speed: 0.7656s/iter; left time: 19582.5162s
	iters: 200, epoch: 3 | loss: 0.4389608
	speed: 0.1858s/iter; left time: 4732.8582s
Epoch: 3 cost time: 49.61951184272766
Epoch: 3, Steps: 262 | Train Loss: 0.4658280 Vali Loss: 1.0148383 Test Loss: 0.4653181
Validation loss decreased (1.058222 --> 1.014838).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4709506
	speed: 0.8295s/iter; left time: 20999.6371s
	iters: 200, epoch: 4 | loss: 0.4778618
	speed: 0.1842s/iter; left time: 4645.2327s
Epoch: 4 cost time: 49.99730181694031
Epoch: 4, Steps: 262 | Train Loss: 0.4525600 Vali Loss: 0.9970620 Test Loss: 0.4529656
Validation loss decreased (1.014838 --> 0.997062).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4538655
	speed: 0.8137s/iter; left time: 20384.6901s
	iters: 200, epoch: 5 | loss: 0.4894230
	speed: 0.1775s/iter; left time: 4429.3840s
Epoch: 5 cost time: 49.91434192657471
Epoch: 5, Steps: 262 | Train Loss: 0.4474296 Vali Loss: 0.9895613 Test Loss: 0.4483193
Validation loss decreased (0.997062 --> 0.989561).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4043303
	speed: 0.8567s/iter; left time: 21237.6602s
	iters: 200, epoch: 6 | loss: 0.4664550
	speed: 0.1861s/iter; left time: 4594.4374s
Epoch: 6 cost time: 50.70122170448303
Epoch: 6, Steps: 262 | Train Loss: 0.4453571 Vali Loss: 0.9856016 Test Loss: 0.4466291
Validation loss decreased (0.989561 --> 0.985602).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4668702
	speed: 0.8200s/iter; left time: 20114.2717s
	iters: 200, epoch: 7 | loss: 0.4026768
	speed: 0.1873s/iter; left time: 4574.4408s
Epoch: 7 cost time: 50.37800574302673
Epoch: 7, Steps: 262 | Train Loss: 0.4441760 Vali Loss: 0.9837506 Test Loss: 0.4458806
Validation loss decreased (0.985602 --> 0.983751).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4703613
	speed: 0.8244s/iter; left time: 20005.5185s
	iters: 200, epoch: 8 | loss: 0.4148385
	speed: 0.1987s/iter; left time: 4801.2308s
Epoch: 8 cost time: 52.44149470329285
Epoch: 8, Steps: 262 | Train Loss: 0.4438133 Vali Loss: 0.9826635 Test Loss: 0.4457268
Validation loss decreased (0.983751 --> 0.982663).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4445124
	speed: 0.8522s/iter; left time: 20457.0722s
	iters: 200, epoch: 9 | loss: 0.4512879
	speed: 0.1836s/iter; left time: 4390.1366s
Epoch: 9 cost time: 51.164084911346436
Epoch: 9, Steps: 262 | Train Loss: 0.4435367 Vali Loss: 0.9826754 Test Loss: 0.4455127
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4287093
	speed: 0.8196s/iter; left time: 19459.0222s
	iters: 200, epoch: 10 | loss: 0.4601286
	speed: 0.1888s/iter; left time: 4463.0826s
Epoch: 10 cost time: 50.22565174102783
Epoch: 10, Steps: 262 | Train Loss: 0.4436143 Vali Loss: 0.9825136 Test Loss: 0.4456337
Validation loss decreased (0.982663 --> 0.982514).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4707126
	speed: 0.7699s/iter; left time: 18077.2589s
	iters: 200, epoch: 11 | loss: 0.4711830
	speed: 0.1960s/iter; left time: 4582.4304s
Epoch: 11 cost time: 47.394572496414185
Epoch: 11, Steps: 262 | Train Loss: 0.4436597 Vali Loss: 0.9822719 Test Loss: 0.4456382
Validation loss decreased (0.982514 --> 0.982272).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4609106
	speed: 0.8517s/iter; left time: 19774.5416s
	iters: 200, epoch: 12 | loss: 0.4192008
	speed: 0.2000s/iter; left time: 4624.6466s
Epoch: 12 cost time: 53.4657084941864
Epoch: 12, Steps: 262 | Train Loss: 0.4436561 Vali Loss: 0.9818569 Test Loss: 0.4455329
Validation loss decreased (0.982272 --> 0.981857).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4644158
	speed: 0.8316s/iter; left time: 19091.5394s
	iters: 200, epoch: 13 | loss: 0.4387994
	speed: 0.1897s/iter; left time: 4336.9600s
Epoch: 13 cost time: 51.7506058216095
Epoch: 13, Steps: 262 | Train Loss: 0.4435982 Vali Loss: 0.9819825 Test Loss: 0.4457391
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4399607
	speed: 0.8575s/iter; left time: 19461.1535s
	iters: 200, epoch: 14 | loss: 0.4742720
	speed: 0.1935s/iter; left time: 4372.7590s
Epoch: 14 cost time: 52.01114845275879
Epoch: 14, Steps: 262 | Train Loss: 0.4436385 Vali Loss: 0.9813626 Test Loss: 0.4455969
Validation loss decreased (0.981857 --> 0.981363).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4340325
	speed: 0.8129s/iter; left time: 18235.4846s
	iters: 200, epoch: 15 | loss: 0.4481341
	speed: 0.1895s/iter; left time: 4232.5116s
Epoch: 15 cost time: 49.56500315666199
Epoch: 15, Steps: 262 | Train Loss: 0.4434942 Vali Loss: 0.9808816 Test Loss: 0.4457285
Validation loss decreased (0.981363 --> 0.980882).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4214312
	speed: 0.8339s/iter; left time: 18488.8103s
	iters: 200, epoch: 16 | loss: 0.4394695
	speed: 0.1943s/iter; left time: 4288.5390s
Epoch: 16 cost time: 51.77967166900635
Epoch: 16, Steps: 262 | Train Loss: 0.4434444 Vali Loss: 0.9819307 Test Loss: 0.4458518
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4606168
	speed: 0.8636s/iter; left time: 18921.0989s
	iters: 200, epoch: 17 | loss: 0.4744084
	speed: 0.1938s/iter; left time: 4227.4898s
Epoch: 17 cost time: 52.75237154960632
Epoch: 17, Steps: 262 | Train Loss: 0.4434583 Vali Loss: 0.9814823 Test Loss: 0.4457333
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4051260
	speed: 0.8626s/iter; left time: 18673.4703s
	iters: 200, epoch: 18 | loss: 0.4255812
	speed: 0.2081s/iter; left time: 4484.7117s
Epoch: 18 cost time: 53.104493618011475
Epoch: 18, Steps: 262 | Train Loss: 0.4433034 Vali Loss: 0.9807674 Test Loss: 0.4457138
Validation loss decreased (0.980882 --> 0.980767).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4329953
	speed: 0.8472s/iter; left time: 18116.7304s
	iters: 200, epoch: 19 | loss: 0.4336683
	speed: 0.1962s/iter; left time: 4175.7370s
Epoch: 19 cost time: 51.75117540359497
Epoch: 19, Steps: 262 | Train Loss: 0.4435502 Vali Loss: 0.9811894 Test Loss: 0.4457218
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4185596
	speed: 0.8268s/iter; left time: 17463.6202s
	iters: 200, epoch: 20 | loss: 0.4022158
	speed: 0.1948s/iter; left time: 4095.8149s
Epoch: 20 cost time: 50.31433987617493
Epoch: 20, Steps: 262 | Train Loss: 0.4435875 Vali Loss: 0.9815776 Test Loss: 0.4456231
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4567927
	speed: 0.8568s/iter; left time: 17872.7935s
	iters: 200, epoch: 21 | loss: 0.4464152
	speed: 0.1985s/iter; left time: 4120.0488s
Epoch: 21 cost time: 53.173895597457886
Epoch: 21, Steps: 262 | Train Loss: 0.4435424 Vali Loss: 0.9816626 Test Loss: 0.4456775
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4777156
	speed: 0.8159s/iter; left time: 16806.0957s
	iters: 200, epoch: 22 | loss: 0.4878168
	speed: 0.1976s/iter; left time: 4049.6212s
Epoch: 22 cost time: 49.567928075790405
Epoch: 22, Steps: 262 | Train Loss: 0.4434536 Vali Loss: 0.9813760 Test Loss: 0.4457308
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3994277
	speed: 0.8308s/iter; left time: 16895.2979s
	iters: 200, epoch: 23 | loss: 0.4141843
	speed: 0.1853s/iter; left time: 3749.7535s
Epoch: 23 cost time: 50.63235783576965
Epoch: 23, Steps: 262 | Train Loss: 0.4434241 Vali Loss: 0.9817827 Test Loss: 0.4458491
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4733593
	speed: 0.8284s/iter; left time: 16630.3852s
	iters: 200, epoch: 24 | loss: 0.3941443
	speed: 0.1913s/iter; left time: 3821.5220s
Epoch: 24 cost time: 51.59202742576599
Epoch: 24, Steps: 262 | Train Loss: 0.4434495 Vali Loss: 0.9819894 Test Loss: 0.4458279
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4301766
	speed: 0.8444s/iter; left time: 16729.9315s
	iters: 200, epoch: 25 | loss: 0.4637122
	speed: 0.1990s/iter; left time: 3923.4073s
Epoch: 25 cost time: 50.84422731399536
Epoch: 25, Steps: 262 | Train Loss: 0.4434447 Vali Loss: 0.9820554 Test Loss: 0.4457363
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4665740
	speed: 0.8498s/iter; left time: 16614.4836s
	iters: 200, epoch: 26 | loss: 0.4845872
	speed: 0.1966s/iter; left time: 3823.8775s
Epoch: 26 cost time: 52.13456916809082
Epoch: 26, Steps: 262 | Train Loss: 0.4432614 Vali Loss: 0.9818663 Test Loss: 0.4457196
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3981492
	speed: 0.8383s/iter; left time: 16170.2237s
	iters: 200, epoch: 27 | loss: 0.3967199
	speed: 0.1895s/iter; left time: 3635.4086s
Epoch: 27 cost time: 50.39235496520996
Epoch: 27, Steps: 262 | Train Loss: 0.4434471 Vali Loss: 0.9822589 Test Loss: 0.4457608
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4507682
	speed: 0.8438s/iter; left time: 16055.0765s
	iters: 200, epoch: 28 | loss: 0.5082166
	speed: 0.1916s/iter; left time: 3625.8047s
Epoch: 28 cost time: 50.252777099609375
Epoch: 28, Steps: 262 | Train Loss: 0.4432783 Vali Loss: 0.9818098 Test Loss: 0.4456831
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4599634
	speed: 0.8394s/iter; left time: 15751.7709s
	iters: 200, epoch: 29 | loss: 0.4086363
	speed: 0.2011s/iter; left time: 3753.4905s
Epoch: 29 cost time: 53.626776695251465
Epoch: 29, Steps: 262 | Train Loss: 0.4432327 Vali Loss: 0.9819878 Test Loss: 0.4457617
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4308880
	speed: 0.8691s/iter; left time: 16080.1627s
	iters: 200, epoch: 30 | loss: 0.4728270
	speed: 0.2071s/iter; left time: 3812.1628s
Epoch: 30 cost time: 52.8798451423645
Epoch: 30, Steps: 262 | Train Loss: 0.4432548 Vali Loss: 0.9811426 Test Loss: 0.4457485
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4713377
	speed: 0.8396s/iter; left time: 15315.8400s
	iters: 200, epoch: 31 | loss: 0.4643857
	speed: 0.1903s/iter; left time: 3452.7652s
Epoch: 31 cost time: 49.17496705055237
Epoch: 31, Steps: 262 | Train Loss: 0.4434334 Vali Loss: 0.9807094 Test Loss: 0.4457802
Validation loss decreased (0.980767 --> 0.980709).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4418837
	speed: 0.8179s/iter; left time: 14705.2599s
	iters: 200, epoch: 32 | loss: 0.4163618
	speed: 0.1973s/iter; left time: 3527.3489s
Epoch: 32 cost time: 52.45162749290466
Epoch: 32, Steps: 262 | Train Loss: 0.4433221 Vali Loss: 0.9811858 Test Loss: 0.4458713
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4773998
	speed: 0.8615s/iter; left time: 15263.7691s
	iters: 200, epoch: 33 | loss: 0.4457301
	speed: 0.1914s/iter; left time: 3372.0481s
Epoch: 33 cost time: 51.248228549957275
Epoch: 33, Steps: 262 | Train Loss: 0.4433870 Vali Loss: 0.9813101 Test Loss: 0.4458274
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4365423
	speed: 0.8138s/iter; left time: 14205.1404s
	iters: 200, epoch: 34 | loss: 0.4872543
	speed: 0.1706s/iter; left time: 2961.4728s
Epoch: 34 cost time: 47.62048411369324
Epoch: 34, Steps: 262 | Train Loss: 0.4435363 Vali Loss: 0.9810044 Test Loss: 0.4457556
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4396489
	speed: 0.7092s/iter; left time: 12193.2570s
	iters: 200, epoch: 35 | loss: 0.4098575
	speed: 0.1575s/iter; left time: 2692.1472s
Epoch: 35 cost time: 42.96504521369934
Epoch: 35, Steps: 262 | Train Loss: 0.4434390 Vali Loss: 0.9815524 Test Loss: 0.4457685
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4569044
	speed: 0.7189s/iter; left time: 12171.2831s
	iters: 200, epoch: 36 | loss: 0.4202436
	speed: 0.1436s/iter; left time: 2416.1866s
Epoch: 36 cost time: 40.839118242263794
Epoch: 36, Steps: 262 | Train Loss: 0.4433946 Vali Loss: 0.9812284 Test Loss: 0.4457836
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4434668
	speed: 0.7061s/iter; left time: 11769.1492s
	iters: 200, epoch: 37 | loss: 0.4315456
	speed: 0.1656s/iter; left time: 2744.5669s
Epoch: 37 cost time: 45.70673131942749
Epoch: 37, Steps: 262 | Train Loss: 0.4433824 Vali Loss: 0.9822090 Test Loss: 0.4457644
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.5074442
	speed: 0.7195s/iter; left time: 11804.4363s
	iters: 200, epoch: 38 | loss: 0.4269810
	speed: 0.1574s/iter; left time: 2567.2522s
Epoch: 38 cost time: 41.704143047332764
Epoch: 38, Steps: 262 | Train Loss: 0.4432013 Vali Loss: 0.9806364 Test Loss: 0.4457894
Validation loss decreased (0.980709 --> 0.980636).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4243793
	speed: 0.7028s/iter; left time: 11346.4401s
	iters: 200, epoch: 39 | loss: 0.4350975
	speed: 0.1774s/iter; left time: 2846.0752s
Epoch: 39 cost time: 46.02334952354431
Epoch: 39, Steps: 262 | Train Loss: 0.4433104 Vali Loss: 0.9817533 Test Loss: 0.4457961
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4248640
	speed: 0.6874s/iter; left time: 10918.4104s
	iters: 200, epoch: 40 | loss: 0.4606306
	speed: 0.1595s/iter; left time: 2517.4644s
Epoch: 40 cost time: 43.40126395225525
Epoch: 40, Steps: 262 | Train Loss: 0.4434161 Vali Loss: 0.9823598 Test Loss: 0.4457986
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4399631
	speed: 0.7145s/iter; left time: 11160.6546s
	iters: 200, epoch: 41 | loss: 0.4309215
	speed: 0.1804s/iter; left time: 2799.6810s
Epoch: 41 cost time: 45.73109197616577
Epoch: 41, Steps: 262 | Train Loss: 0.4434383 Vali Loss: 0.9810614 Test Loss: 0.4458137
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4366137
	speed: 0.7315s/iter; left time: 11235.0791s
	iters: 200, epoch: 42 | loss: 0.4205275
	speed: 0.1616s/iter; left time: 2465.2614s
Epoch: 42 cost time: 43.78867316246033
Epoch: 42, Steps: 262 | Train Loss: 0.4433471 Vali Loss: 0.9816340 Test Loss: 0.4458209
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4316056
	speed: 0.7017s/iter; left time: 10594.0199s
	iters: 200, epoch: 43 | loss: 0.4041313
	speed: 0.1683s/iter; left time: 2524.7090s
Epoch: 43 cost time: 43.255738735198975
Epoch: 43, Steps: 262 | Train Loss: 0.4432524 Vali Loss: 0.9814879 Test Loss: 0.4458036
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4474058
	speed: 0.7337s/iter; left time: 10883.8536s
	iters: 200, epoch: 44 | loss: 0.4545389
	speed: 0.1781s/iter; left time: 2624.8967s
Epoch: 44 cost time: 45.79729866981506
Epoch: 44, Steps: 262 | Train Loss: 0.4432867 Vali Loss: 0.9812282 Test Loss: 0.4458251
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4686667
	speed: 0.6964s/iter; left time: 10148.5244s
	iters: 200, epoch: 45 | loss: 0.4467924
	speed: 0.1620s/iter; left time: 2344.1207s
Epoch: 45 cost time: 40.0029673576355
Epoch: 45, Steps: 262 | Train Loss: 0.4431314 Vali Loss: 0.9818299 Test Loss: 0.4458323
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4798050
	speed: 0.6933s/iter; left time: 9921.1629s
	iters: 200, epoch: 46 | loss: 0.4262632
	speed: 0.1741s/iter; left time: 2474.0337s
Epoch: 46 cost time: 44.544068336486816
Epoch: 46, Steps: 262 | Train Loss: 0.4433547 Vali Loss: 0.9817099 Test Loss: 0.4458177
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4247322
	speed: 0.7135s/iter; left time: 10023.2791s
	iters: 200, epoch: 47 | loss: 0.4689497
	speed: 0.1785s/iter; left time: 2490.3094s
Epoch: 47 cost time: 46.089768409729004
Epoch: 47, Steps: 262 | Train Loss: 0.4433188 Vali Loss: 0.9823614 Test Loss: 0.4458469
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4002681
	speed: 0.6901s/iter; left time: 9514.0704s
	iters: 200, epoch: 48 | loss: 0.3959398
	speed: 0.1706s/iter; left time: 2334.9467s
Epoch: 48 cost time: 43.68302130699158
Epoch: 48, Steps: 262 | Train Loss: 0.4433720 Vali Loss: 0.9818931 Test Loss: 0.4458371
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4284540
	speed: 0.7017s/iter; left time: 9490.5012s
	iters: 200, epoch: 49 | loss: 0.4332137
	speed: 0.1668s/iter; left time: 2239.8249s
Epoch: 49 cost time: 44.01108479499817
Epoch: 49, Steps: 262 | Train Loss: 0.4431554 Vali Loss: 0.9816681 Test Loss: 0.4458259
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.4187532
	speed: 0.6872s/iter; left time: 9114.3481s
	iters: 200, epoch: 50 | loss: 0.4314593
	speed: 0.1644s/iter; left time: 2164.2534s
Epoch: 50 cost time: 42.45117950439453
Epoch: 50, Steps: 262 | Train Loss: 0.4434833 Vali Loss: 0.9817387 Test Loss: 0.4458260
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4186255
	speed: 0.7133s/iter; left time: 9273.7066s
	iters: 200, epoch: 51 | loss: 0.4787429
	speed: 0.1724s/iter; left time: 2223.5129s
Epoch: 51 cost time: 45.8374400138855
Epoch: 51, Steps: 262 | Train Loss: 0.4434498 Vali Loss: 0.9819862 Test Loss: 0.4458226
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4637500
	speed: 0.6910s/iter; left time: 8802.9432s
	iters: 200, epoch: 52 | loss: 0.4133268
	speed: 0.1661s/iter; left time: 2098.9705s
Epoch: 52 cost time: 42.37961006164551
Epoch: 52, Steps: 262 | Train Loss: 0.4433270 Vali Loss: 0.9812631 Test Loss: 0.4458304
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4684673
	speed: 0.6961s/iter; left time: 8685.1313s
	iters: 200, epoch: 53 | loss: 0.4388893
	speed: 0.1457s/iter; left time: 1803.8561s
Epoch: 53 cost time: 41.80985951423645
Epoch: 53, Steps: 262 | Train Loss: 0.4432558 Vali Loss: 0.9819434 Test Loss: 0.4458348
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4319662
	speed: 0.7307s/iter; left time: 8925.3586s
	iters: 200, epoch: 54 | loss: 0.4094636
	speed: 0.1586s/iter; left time: 1921.5718s
Epoch: 54 cost time: 43.54410696029663
Epoch: 54, Steps: 262 | Train Loss: 0.4432599 Vali Loss: 0.9819594 Test Loss: 0.4458390
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.4583150
	speed: 0.6930s/iter; left time: 8283.5402s
	iters: 200, epoch: 55 | loss: 0.4208832
	speed: 0.1686s/iter; left time: 1998.1487s
Epoch: 55 cost time: 43.982468128204346
Epoch: 55, Steps: 262 | Train Loss: 0.4431843 Vali Loss: 0.9817070 Test Loss: 0.4458338
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4387468
	speed: 0.6780s/iter; left time: 7926.1924s
	iters: 200, epoch: 56 | loss: 0.4270647
	speed: 0.1512s/iter; left time: 1752.6060s
Epoch: 56 cost time: 41.55554986000061
Epoch: 56, Steps: 262 | Train Loss: 0.4433856 Vali Loss: 0.9819651 Test Loss: 0.4458384
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.5355610
	speed: 0.7086s/iter; left time: 8098.3607s
	iters: 200, epoch: 57 | loss: 0.4722468
	speed: 0.1735s/iter; left time: 1965.3688s
Epoch: 57 cost time: 44.064820289611816
Epoch: 57, Steps: 262 | Train Loss: 0.4430590 Vali Loss: 0.9813371 Test Loss: 0.4458283
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.4115916
	speed: 0.6858s/iter; left time: 7657.7912s
	iters: 200, epoch: 58 | loss: 0.4543380
	speed: 0.1598s/iter; left time: 1768.6626s
Epoch: 58 cost time: 41.789408922195435
Epoch: 58, Steps: 262 | Train Loss: 0.4433220 Vali Loss: 0.9799685 Test Loss: 0.4458394
Validation loss decreased (0.980636 --> 0.979968).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.4591061
	speed: 0.6854s/iter; left time: 7474.7822s
	iters: 200, epoch: 59 | loss: 0.4183314
	speed: 0.1568s/iter; left time: 1693.8529s
Epoch: 59 cost time: 42.717118978500366
Epoch: 59, Steps: 262 | Train Loss: 0.4432750 Vali Loss: 0.9812504 Test Loss: 0.4458187
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.4743669
	speed: 0.6010s/iter; left time: 6396.3955s
	iters: 200, epoch: 60 | loss: 0.4265227
	speed: 0.1244s/iter; left time: 1311.1307s
Epoch: 60 cost time: 32.822967767715454
Epoch: 60, Steps: 262 | Train Loss: 0.4433217 Vali Loss: 0.9818309 Test Loss: 0.4458272
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.4448985
	speed: 0.5374s/iter; left time: 5578.3533s
	iters: 200, epoch: 61 | loss: 0.4285485
	speed: 0.1332s/iter; left time: 1369.1199s
Epoch: 61 cost time: 35.50343656539917
Epoch: 61, Steps: 262 | Train Loss: 0.4430819 Vali Loss: 0.9820206 Test Loss: 0.4458279
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.4960627
	speed: 0.5695s/iter; left time: 5763.2220s
	iters: 200, epoch: 62 | loss: 0.4450715
	speed: 0.1200s/iter; left time: 1202.7388s
Epoch: 62 cost time: 33.03121328353882
Epoch: 62, Steps: 262 | Train Loss: 0.4432464 Vali Loss: 0.9818202 Test Loss: 0.4458323
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.4243686
	speed: 0.4801s/iter; left time: 4732.1613s
	iters: 200, epoch: 63 | loss: 0.4515239
	speed: 0.1230s/iter; left time: 1200.1417s
Epoch: 63 cost time: 30.50298523902893
Epoch: 63, Steps: 262 | Train Loss: 0.4431563 Vali Loss: 0.9805818 Test Loss: 0.4458347
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.4606982
	speed: 0.4656s/iter; left time: 4467.4366s
	iters: 200, epoch: 64 | loss: 0.4233542
	speed: 0.0950s/iter; left time: 902.1110s
Epoch: 64 cost time: 27.66029119491577
Epoch: 64, Steps: 262 | Train Loss: 0.4432467 Vali Loss: 0.9816234 Test Loss: 0.4458429
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.4708681
	speed: 0.4981s/iter; left time: 4648.6501s
	iters: 200, epoch: 65 | loss: 0.4275413
	speed: 0.1201s/iter; left time: 1109.2690s
Epoch: 65 cost time: 32.16364097595215
Epoch: 65, Steps: 262 | Train Loss: 0.4430695 Vali Loss: 0.9820026 Test Loss: 0.4458427
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.4551490
	speed: 0.4749s/iter; left time: 4307.6972s
	iters: 200, epoch: 66 | loss: 0.4471840
	speed: 0.1138s/iter; left time: 1021.0364s
Epoch: 66 cost time: 29.34289574623108
Epoch: 66, Steps: 262 | Train Loss: 0.4431460 Vali Loss: 0.9812980 Test Loss: 0.4458393
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.4293609
	speed: 0.4833s/iter; left time: 4257.0224s
	iters: 200, epoch: 67 | loss: 0.4424647
	speed: 0.1240s/iter; left time: 1079.6407s
Epoch: 67 cost time: 31.096572875976562
Epoch: 67, Steps: 262 | Train Loss: 0.4431888 Vali Loss: 0.9822615 Test Loss: 0.4458387
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.4333133
	speed: 0.4607s/iter; left time: 3937.5091s
	iters: 200, epoch: 68 | loss: 0.4604150
	speed: 0.1055s/iter; left time: 891.3552s
Epoch: 68 cost time: 29.45175862312317
Epoch: 68, Steps: 262 | Train Loss: 0.4432126 Vali Loss: 0.9817165 Test Loss: 0.4458405
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.4445795
	speed: 0.4967s/iter; left time: 4115.2515s
	iters: 200, epoch: 69 | loss: 0.4360098
	speed: 0.1031s/iter; left time: 843.7486s
Epoch: 69 cost time: 28.051522970199585
Epoch: 69, Steps: 262 | Train Loss: 0.4433137 Vali Loss: 0.9815268 Test Loss: 0.4458435
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.4668954
	speed: 0.4727s/iter; left time: 3792.8075s
	iters: 200, epoch: 70 | loss: 0.4407810
	speed: 0.1022s/iter; left time: 809.9086s
Epoch: 70 cost time: 28.67934012413025
Epoch: 70, Steps: 262 | Train Loss: 0.4432391 Vali Loss: 0.9825816 Test Loss: 0.4458444
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.4719085
	speed: 0.4315s/iter; left time: 3348.8303s
	iters: 200, epoch: 71 | loss: 0.4831993
	speed: 0.0857s/iter; left time: 656.7022s
Epoch: 71 cost time: 23.738656997680664
Epoch: 71, Steps: 262 | Train Loss: 0.4433543 Vali Loss: 0.9807363 Test Loss: 0.4458456
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.4493533
	speed: 0.3248s/iter; left time: 2435.5383s
	iters: 200, epoch: 72 | loss: 0.4281209
	speed: 0.0826s/iter; left time: 611.0300s
Epoch: 72 cost time: 20.87801766395569
Epoch: 72, Steps: 262 | Train Loss: 0.4432198 Vali Loss: 0.9822425 Test Loss: 0.4458461
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.4156807
	speed: 0.3749s/iter; left time: 2712.9530s
	iters: 200, epoch: 73 | loss: 0.4495753
	speed: 0.0953s/iter; left time: 680.2764s
Epoch: 73 cost time: 24.119855165481567
Epoch: 73, Steps: 262 | Train Loss: 0.4431563 Vali Loss: 0.9816666 Test Loss: 0.4458476
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.4404978
	speed: 0.3661s/iter; left time: 2553.4853s
	iters: 200, epoch: 74 | loss: 0.4465776
	speed: 0.0707s/iter; left time: 486.0017s
Epoch: 74 cost time: 20.473710298538208
Epoch: 74, Steps: 262 | Train Loss: 0.4434563 Vali Loss: 0.9820683 Test Loss: 0.4458469
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.4701010
	speed: 0.3271s/iter; left time: 2195.5093s
	iters: 200, epoch: 75 | loss: 0.4666457
	speed: 0.0781s/iter; left time: 516.7690s
Epoch: 75 cost time: 21.124295711517334
Epoch: 75, Steps: 262 | Train Loss: 0.4432436 Vali Loss: 0.9811017 Test Loss: 0.4458463
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.4198170
	speed: 0.3244s/iter; left time: 2092.8614s
	iters: 200, epoch: 76 | loss: 0.4476180
	speed: 0.0729s/iter; left time: 463.0917s
Epoch: 76 cost time: 18.797062873840332
Epoch: 76, Steps: 262 | Train Loss: 0.4432242 Vali Loss: 0.9813130 Test Loss: 0.4458474
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.3930195
	speed: 0.2940s/iter; left time: 1819.2889s
	iters: 200, epoch: 77 | loss: 0.3967953
	speed: 0.0566s/iter; left time: 344.6126s
Epoch: 77 cost time: 16.852782726287842
Epoch: 77, Steps: 262 | Train Loss: 0.4432160 Vali Loss: 0.9816893 Test Loss: 0.4458461
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.4393622
	speed: 0.2921s/iter; left time: 1731.2898s
	iters: 200, epoch: 78 | loss: 0.4249762
	speed: 0.0620s/iter; left time: 361.0988s
Epoch: 78 cost time: 18.769331216812134
Epoch: 78, Steps: 262 | Train Loss: 0.4432538 Vali Loss: 0.9811197 Test Loss: 0.4458470
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4440270960330963, mae:0.42514973878860474, rse:0.6339799165725708, corr:[0.5329843  0.53346974 0.53248566 0.5305652  0.52839905 0.52687585
 0.52617455 0.5262042  0.5263616  0.5264539  0.52645636 0.5259804
 0.52513987 0.5237559  0.52200586 0.52020943 0.51841396 0.5166304
 0.51484823 0.51302606 0.51111203 0.50900275 0.50672466 0.50441754
 0.50222784 0.50032365 0.4988956  0.49792236 0.49735835 0.49709123
 0.49713612 0.49733284 0.49746498 0.49750403 0.49741063 0.49724278
 0.4969401  0.49651882 0.49610227 0.49565786 0.49526736 0.49491203
 0.49463642 0.49455035 0.49465486 0.4949167  0.4952829  0.4956516
 0.49594215 0.496054   0.49601805 0.49590862 0.49580792 0.495731
 0.49568194 0.49562865 0.4956437  0.49563804 0.49561125 0.49550715
 0.495376   0.49523687 0.4950663  0.49488243 0.49472415 0.49472654
 0.49489596 0.49518207 0.4956127  0.49609828 0.4965664  0.4969062
 0.49705726 0.4970159  0.49691406 0.49677074 0.49669415 0.49669838
 0.4967465  0.49677378 0.49675086 0.4966398  0.4964567  0.49614555
 0.49577257 0.49543253 0.4951972  0.49500823 0.49483103 0.49474612
 0.49472415 0.49468902 0.49454066 0.4942343  0.49378145 0.4930955
 0.49225208 0.49141225 0.4906418  0.49005377 0.48975512 0.48981386
 0.49017859 0.4907769  0.4915732  0.49254507 0.49361327 0.49467468
 0.4955902  0.49621072 0.49648225 0.49662197 0.49662834 0.49655527
 0.4963807  0.49616432 0.4958341  0.49535915 0.49480084 0.49420592
 0.4936791  0.4931487  0.49258426 0.4920631  0.4915664  0.4910764
 0.4906546  0.49034822 0.49015418 0.49004036 0.48993343 0.4897455
 0.48944336 0.4891347  0.4888208  0.48846525 0.48818728 0.48795715
 0.48774454 0.48760802 0.48752746 0.4875028  0.4875317  0.48762628
 0.48775336 0.48789746 0.48794678 0.4879242  0.4879425  0.48784658
 0.4876737  0.4875073  0.4873634  0.48731112 0.48728228 0.48733917
 0.4874523  0.48763058 0.48777458 0.48789436 0.48798183 0.48806834
 0.4881985  0.48840582 0.48866183 0.48902288 0.48942477 0.48980644
 0.4901486  0.49036875 0.49042505 0.4904427  0.49040323 0.49031138
 0.49021098 0.49007785 0.4899401  0.4898073  0.4896112  0.48939964
 0.4891944  0.4890185  0.48886284 0.48874548 0.48867622 0.48866385
 0.48872486 0.48887804 0.48912558 0.4894574  0.4897516  0.48991647
 0.48987484 0.4897058  0.48939994 0.48879474 0.48803097 0.48738182
 0.48692736 0.4866284  0.48641482 0.48647898 0.48655832 0.48646155
 0.4861276  0.4853696  0.48440242 0.4835244  0.4828077  0.482276
 0.48183993 0.481441   0.48096263 0.48029694 0.47944048 0.47849718
 0.47758475 0.47673213 0.47599676 0.4754379  0.47503465 0.4747894
 0.47469947 0.47475582 0.47482902 0.47486898 0.47484672 0.47467437
 0.47431308 0.47390422 0.47347096 0.47304332 0.47270396 0.47250086
 0.47245845 0.4725762  0.47271606 0.47289148 0.4730518  0.47311348
 0.47315726 0.47311243 0.47307274 0.47305018 0.4730621  0.4731089
 0.47319213 0.47322375 0.47323132 0.47322178 0.4731796  0.47310588
 0.47302508 0.47302803 0.47304717 0.4731     0.47313243 0.47315487
 0.47327486 0.47350913 0.47382864 0.4741997  0.47455728 0.47490582
 0.47524107 0.47548184 0.47565228 0.4758068  0.4759748  0.47609526
 0.47620693 0.4762704  0.4763547  0.47644886 0.4765887  0.47675303
 0.4769264  0.47716674 0.47738668 0.47753546 0.47760314 0.47756565
 0.4775003  0.47736266 0.47721088 0.47703737 0.47675806 0.47635382
 0.4757271  0.4750354  0.47443607 0.473873   0.47339725 0.47303638
 0.47291625 0.47295174 0.4732006  0.47363096 0.47411337 0.47460368
 0.474974   0.4750461  0.47495365 0.4748159  0.47469658 0.47457305
 0.4744857  0.47437426 0.47416624 0.47387424 0.4735266  0.4731729
 0.47283915 0.4724799  0.4721528  0.47195277 0.471733   0.47155616
 0.47142446 0.4712466  0.47101864 0.47082427 0.47058672 0.47031856
 0.47001007 0.46967936 0.46931204 0.46894267 0.46856207 0.46821785
 0.4679511  0.4678017  0.4677118  0.46766296 0.46768627 0.46776947
 0.46790496 0.46805316 0.4681521  0.4681872  0.46822473 0.46820864
 0.46814635 0.4680619  0.46804985 0.46812448 0.46821293 0.46831337
 0.4683721  0.4684366  0.4685129  0.46853447 0.46851996 0.46849936
 0.46851283 0.46865225 0.4688489  0.46911106 0.46938595 0.4696472
 0.46980503 0.4699094  0.4699216  0.46989593 0.46986023 0.46981543
 0.4697567  0.46973997 0.46978766 0.4698712  0.4700258  0.4702124
 0.47044155 0.47068912 0.47094238 0.47112942 0.47122896 0.47128722
 0.47134325 0.4714228  0.4716085  0.47177133 0.4718756  0.4718703
 0.47168803 0.4712684  0.47085986 0.4704022  0.46992692 0.46967196
 0.46969613 0.47000054 0.4704173  0.47104132 0.471727   0.47238925
 0.47283638 0.47297007 0.47278166 0.47256255 0.47232231 0.472121
 0.47197238 0.47180396 0.4715689  0.47126335 0.47087348 0.47044852
 0.47005367 0.46967077 0.4693198  0.46902075 0.46877512 0.46856007
 0.46843055 0.46832493 0.46826288 0.46820408 0.4680679  0.46789408
 0.46757957 0.46719941 0.46676683 0.46633548 0.46593466 0.46564707
 0.46544313 0.46538004 0.4654092  0.46551663 0.4656208  0.46576014
 0.46582925 0.46585003 0.46582484 0.46580547 0.46574423 0.4657386
 0.4657153  0.46569574 0.46572378 0.46579656 0.4658774  0.46595973
 0.46602365 0.4660962  0.46613243 0.4661934  0.4662481  0.4662913
 0.46639654 0.46658993 0.46681395 0.46704364 0.46719536 0.4673803
 0.46752298 0.4675852  0.46757412 0.4675622  0.46750885 0.46744257
 0.4674003  0.46739405 0.46739587 0.46744564 0.46752894 0.46760294
 0.46768847 0.46780097 0.46794435 0.4680153  0.46801847 0.4679509
 0.46776608 0.46749088 0.467124   0.46661595 0.46593395 0.46505645
 0.46399266 0.46284735 0.4617322  0.46071205 0.45979145 0.4590808
 0.4586207  0.45821893 0.45803794 0.45799842 0.4579915  0.45790067
 0.45780635 0.45754817 0.4570777  0.45655665 0.45613226 0.45580512
 0.45556247 0.45536998 0.45515707 0.45485872 0.4545126  0.45410952
 0.4537258  0.45336792 0.45304075 0.4528126  0.4526239  0.45249945
 0.45242244 0.4523847  0.45236018 0.45238835 0.4524054  0.452345
 0.4521471  0.45187446 0.45154563 0.4511359  0.4507105  0.45038486
 0.45016134 0.45006192 0.450061   0.45009503 0.45016226 0.45030335
 0.450408   0.4504444  0.45037884 0.45025188 0.45015398 0.45010856
 0.4501252  0.45015466 0.45026115 0.45036703 0.4504366  0.45045143
 0.45044178 0.45034933 0.45025867 0.45017818 0.45013613 0.45016512
 0.45027944 0.45047748 0.45073476 0.45098814 0.45119518 0.45140922
 0.45156944 0.45163256 0.4516267  0.45164537 0.4516749  0.45173115
 0.45184094 0.45200512 0.45215374 0.45228475 0.45236546 0.45243004
 0.452437   0.4525219  0.4526006  0.45265573 0.45265278 0.45257288
 0.45239437 0.45205665 0.45160553 0.4509949  0.4501237  0.44898355
 0.44759944 0.44620842 0.44504854 0.44405237 0.44327822 0.44278398
 0.4425516  0.44254053 0.44273242 0.4430142  0.44336578 0.44368124
 0.44390896 0.443921   0.4437462  0.44355005 0.4433845  0.4432482
 0.4430869  0.44289678 0.4426717  0.44237635 0.44203696 0.44162622
 0.44124866 0.44089323 0.4405532  0.44034168 0.4402205  0.44016337
 0.44016272 0.44022518 0.44028535 0.44031122 0.44029757 0.4402589
 0.44011635 0.43983337 0.43951634 0.43916464 0.43883818 0.4385444
 0.43838045 0.43835086 0.43841955 0.4385409  0.43872184 0.4389502
 0.43915385 0.4392969  0.43935767 0.439321   0.43925047 0.43917805
 0.43908527 0.4389993  0.4389301  0.43887988 0.43888277 0.4388322
 0.4387051  0.43858826 0.43846896 0.43836144 0.43834895 0.43835944
 0.4384316  0.43859673 0.43884715 0.43916717 0.43951014 0.43987998
 0.44021517 0.4404068  0.44049186 0.4405382  0.44054618 0.44055307
 0.44058165 0.4406637  0.44078678 0.44092867 0.44108507 0.441239
 0.44136727 0.44150013 0.44161734 0.4416303  0.4415671  0.4414315
 0.4412256  0.44097766 0.44065058 0.44025713 0.43973914 0.43899998
 0.4380274  0.43701684 0.43610382 0.4353124  0.4347447  0.43439573
 0.4342814  0.43449438 0.4349564  0.4355439  0.43608245 0.43662262
 0.43703967 0.43701807 0.4366881  0.4362371  0.4358175  0.4355135
 0.43535647 0.43525946 0.43514228 0.4349114  0.43451557 0.43407276
 0.43363726 0.43317783 0.43278033 0.43248197 0.4322689  0.43210235
 0.4319635  0.43177965 0.43157524 0.43138263 0.43113863 0.43089762
 0.43067196 0.4304407  0.4302836  0.43016577 0.43006155 0.42994952
 0.42989793 0.4299875  0.43020105 0.43057597 0.4309482  0.4305529 ]
