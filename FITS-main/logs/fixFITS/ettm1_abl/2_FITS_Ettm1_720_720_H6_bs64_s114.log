Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=58, out_features=116, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6028288.0
params:  6844.0
Trainable parameters:  6844
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5112343
	speed: 0.0385s/iter; left time: 989.1947s
	iters: 200, epoch: 1 | loss: 0.3984989
	speed: 0.0326s/iter; left time: 834.3821s
Epoch: 1 cost time: 8.969502449035645
Epoch: 1, Steps: 258 | Train Loss: 0.5153874 Vali Loss: 1.2042749 Test Loss: 0.5981488
Validation loss decreased (inf --> 1.204275).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3397686
	speed: 0.1349s/iter; left time: 3432.1169s
	iters: 200, epoch: 2 | loss: 0.3231258
	speed: 0.0354s/iter; left time: 897.1420s
Epoch: 2 cost time: 9.759241342544556
Epoch: 2, Steps: 258 | Train Loss: 0.3281651 Vali Loss: 1.0667307 Test Loss: 0.5047623
Validation loss decreased (1.204275 --> 1.066731).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2733512
	speed: 0.1500s/iter; left time: 3778.8765s
	iters: 200, epoch: 3 | loss: 0.2634182
	speed: 0.0345s/iter; left time: 866.5032s
Epoch: 3 cost time: 9.987468004226685
Epoch: 3, Steps: 258 | Train Loss: 0.2736253 Vali Loss: 1.0123644 Test Loss: 0.4686003
Validation loss decreased (1.066731 --> 1.012364).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2507081
	speed: 0.1422s/iter; left time: 3545.6332s
	iters: 200, epoch: 4 | loss: 0.2530453
	speed: 0.0344s/iter; left time: 855.0702s
Epoch: 4 cost time: 9.067860841751099
Epoch: 4, Steps: 258 | Train Loss: 0.2495296 Vali Loss: 0.9864415 Test Loss: 0.4506900
Validation loss decreased (1.012364 --> 0.986441).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2312880
	speed: 0.1475s/iter; left time: 3637.9994s
	iters: 200, epoch: 5 | loss: 0.2419301
	speed: 0.0347s/iter; left time: 853.4211s
Epoch: 5 cost time: 9.301709651947021
Epoch: 5, Steps: 258 | Train Loss: 0.2368427 Vali Loss: 0.9705151 Test Loss: 0.4394276
Validation loss decreased (0.986441 --> 0.970515).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2361200
	speed: 0.1363s/iter; left time: 3328.3345s
	iters: 200, epoch: 6 | loss: 0.2369162
	speed: 0.0342s/iter; left time: 831.5107s
Epoch: 6 cost time: 8.483148097991943
Epoch: 6, Steps: 258 | Train Loss: 0.2293141 Vali Loss: 0.9596245 Test Loss: 0.4322241
Validation loss decreased (0.970515 --> 0.959625).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2235581
	speed: 0.1275s/iter; left time: 3080.6753s
	iters: 200, epoch: 7 | loss: 0.2347258
	speed: 0.0330s/iter; left time: 794.1274s
Epoch: 7 cost time: 9.095201015472412
Epoch: 7, Steps: 258 | Train Loss: 0.2244852 Vali Loss: 0.9531026 Test Loss: 0.4266577
Validation loss decreased (0.959625 --> 0.953103).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2120367
	speed: 0.1339s/iter; left time: 3198.8589s
	iters: 200, epoch: 8 | loss: 0.2276776
	speed: 0.0341s/iter; left time: 812.5759s
Epoch: 8 cost time: 9.597057819366455
Epoch: 8, Steps: 258 | Train Loss: 0.2212812 Vali Loss: 0.9489852 Test Loss: 0.4231864
Validation loss decreased (0.953103 --> 0.948985).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2220360
	speed: 0.1509s/iter; left time: 3567.8518s
	iters: 200, epoch: 9 | loss: 0.2125995
	speed: 0.0373s/iter; left time: 878.0236s
Epoch: 9 cost time: 9.99938178062439
Epoch: 9, Steps: 258 | Train Loss: 0.2191128 Vali Loss: 0.9451221 Test Loss: 0.4210234
Validation loss decreased (0.948985 --> 0.945122).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2422827
	speed: 0.1373s/iter; left time: 3210.4056s
	iters: 200, epoch: 10 | loss: 0.2107914
	speed: 0.0373s/iter; left time: 867.7476s
Epoch: 10 cost time: 9.92061185836792
Epoch: 10, Steps: 258 | Train Loss: 0.2174919 Vali Loss: 0.9436732 Test Loss: 0.4199589
Validation loss decreased (0.945122 --> 0.943673).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1986970
	speed: 0.1543s/iter; left time: 3567.6737s
	iters: 200, epoch: 11 | loss: 0.2062368
	speed: 0.0334s/iter; left time: 768.0138s
Epoch: 11 cost time: 9.355057001113892
Epoch: 11, Steps: 258 | Train Loss: 0.2163895 Vali Loss: 0.9420468 Test Loss: 0.4187824
Validation loss decreased (0.943673 --> 0.942047).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2250596
	speed: 0.1435s/iter; left time: 3279.8113s
	iters: 200, epoch: 12 | loss: 0.2098683
	speed: 0.0340s/iter; left time: 774.2481s
Epoch: 12 cost time: 9.243083715438843
Epoch: 12, Steps: 258 | Train Loss: 0.2156904 Vali Loss: 0.9415831 Test Loss: 0.4186355
Validation loss decreased (0.942047 --> 0.941583).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1994009
	speed: 0.1490s/iter; left time: 3367.2862s
	iters: 200, epoch: 13 | loss: 0.2227814
	speed: 0.0332s/iter; left time: 746.8723s
Epoch: 13 cost time: 9.723519325256348
Epoch: 13, Steps: 258 | Train Loss: 0.2151849 Vali Loss: 0.9413142 Test Loss: 0.4186658
Validation loss decreased (0.941583 --> 0.941314).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2211023
	speed: 0.1419s/iter; left time: 3170.1887s
	iters: 200, epoch: 14 | loss: 0.2280347
	speed: 0.0353s/iter; left time: 786.3755s
Epoch: 14 cost time: 9.231950998306274
Epoch: 14, Steps: 258 | Train Loss: 0.2147959 Vali Loss: 0.9407848 Test Loss: 0.4187003
Validation loss decreased (0.941314 --> 0.940785).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2216195
	speed: 0.1491s/iter; left time: 3292.4117s
	iters: 200, epoch: 15 | loss: 0.2304902
	speed: 0.0358s/iter; left time: 786.8320s
Epoch: 15 cost time: 9.155014514923096
Epoch: 15, Steps: 258 | Train Loss: 0.2145426 Vali Loss: 0.9404559 Test Loss: 0.4188097
Validation loss decreased (0.940785 --> 0.940456).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2242312
	speed: 0.1327s/iter; left time: 2896.3114s
	iters: 200, epoch: 16 | loss: 0.2081048
	speed: 0.0347s/iter; left time: 754.9133s
Epoch: 16 cost time: 9.559748649597168
Epoch: 16, Steps: 258 | Train Loss: 0.2143631 Vali Loss: 0.9412828 Test Loss: 0.4192052
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2220610
	speed: 0.1324s/iter; left time: 2856.2457s
	iters: 200, epoch: 17 | loss: 0.2040725
	speed: 0.0347s/iter; left time: 745.7650s
Epoch: 17 cost time: 9.38612675666809
Epoch: 17, Steps: 258 | Train Loss: 0.2142466 Vali Loss: 0.9416930 Test Loss: 0.4194317
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2008487
	speed: 0.1428s/iter; left time: 3044.7181s
	iters: 200, epoch: 18 | loss: 0.2145937
	speed: 0.0345s/iter; left time: 732.7950s
Epoch: 18 cost time: 9.543039321899414
Epoch: 18, Steps: 258 | Train Loss: 0.2141904 Vali Loss: 0.9410715 Test Loss: 0.4196075
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2251024
	speed: 0.1484s/iter; left time: 3123.9027s
	iters: 200, epoch: 19 | loss: 0.2220524
	speed: 0.0338s/iter; left time: 707.4615s
Epoch: 19 cost time: 9.919403314590454
Epoch: 19, Steps: 258 | Train Loss: 0.2140711 Vali Loss: 0.9412377 Test Loss: 0.4198484
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2198097
	speed: 0.1371s/iter; left time: 2852.5449s
	iters: 200, epoch: 20 | loss: 0.2030793
	speed: 0.0310s/iter; left time: 642.3460s
Epoch: 20 cost time: 8.372812271118164
Epoch: 20, Steps: 258 | Train Loss: 0.2140972 Vali Loss: 0.9404684 Test Loss: 0.4201565
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2088460
	speed: 0.1251s/iter; left time: 2569.4271s
	iters: 200, epoch: 21 | loss: 0.2107429
	speed: 0.0297s/iter; left time: 607.1436s
Epoch: 21 cost time: 8.299208402633667
Epoch: 21, Steps: 258 | Train Loss: 0.2140604 Vali Loss: 0.9417319 Test Loss: 0.4206033
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2082382
	speed: 0.1321s/iter; left time: 2680.2804s
	iters: 200, epoch: 22 | loss: 0.2068385
	speed: 0.0312s/iter; left time: 630.1301s
Epoch: 22 cost time: 9.01984715461731
Epoch: 22, Steps: 258 | Train Loss: 0.2139786 Vali Loss: 0.9410014 Test Loss: 0.4202965
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2286566
	speed: 0.1338s/iter; left time: 2678.5437s
	iters: 200, epoch: 23 | loss: 0.2320721
	speed: 0.0322s/iter; left time: 642.1349s
Epoch: 23 cost time: 9.100501775741577
Epoch: 23, Steps: 258 | Train Loss: 0.2140581 Vali Loss: 0.9414086 Test Loss: 0.4204261
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2129334
	speed: 0.1433s/iter; left time: 2833.2257s
	iters: 200, epoch: 24 | loss: 0.1998475
	speed: 0.0377s/iter; left time: 741.9831s
Epoch: 24 cost time: 9.977139472961426
Epoch: 24, Steps: 258 | Train Loss: 0.2140638 Vali Loss: 0.9418236 Test Loss: 0.4207774
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1998424
	speed: 0.1429s/iter; left time: 2787.0836s
	iters: 200, epoch: 25 | loss: 0.2184576
	speed: 0.0338s/iter; left time: 656.1452s
Epoch: 25 cost time: 9.69571590423584
Epoch: 25, Steps: 258 | Train Loss: 0.2139560 Vali Loss: 0.9415193 Test Loss: 0.4207442
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2186147
	speed: 0.1511s/iter; left time: 2908.0048s
	iters: 200, epoch: 26 | loss: 0.2044847
	speed: 0.0367s/iter; left time: 703.1389s
Epoch: 26 cost time: 9.105804920196533
Epoch: 26, Steps: 258 | Train Loss: 0.2139962 Vali Loss: 0.9412245 Test Loss: 0.4209873
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2296446
	speed: 0.1440s/iter; left time: 2735.2093s
	iters: 200, epoch: 27 | loss: 0.2154965
	speed: 0.0369s/iter; left time: 696.6584s
Epoch: 27 cost time: 9.866770267486572
Epoch: 27, Steps: 258 | Train Loss: 0.2139883 Vali Loss: 0.9410278 Test Loss: 0.4205827
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2233099
	speed: 0.1415s/iter; left time: 2651.2947s
	iters: 200, epoch: 28 | loss: 0.1982749
	speed: 0.0326s/iter; left time: 607.8453s
Epoch: 28 cost time: 9.493181228637695
Epoch: 28, Steps: 258 | Train Loss: 0.2140118 Vali Loss: 0.9413641 Test Loss: 0.4206203
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2087844
	speed: 0.1436s/iter; left time: 2652.7261s
	iters: 200, epoch: 29 | loss: 0.2319075
	speed: 0.0298s/iter; left time: 548.4035s
Epoch: 29 cost time: 8.88485598564148
Epoch: 29, Steps: 258 | Train Loss: 0.2139608 Vali Loss: 0.9408748 Test Loss: 0.4210311
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2129996
	speed: 0.1355s/iter; left time: 2468.3221s
	iters: 200, epoch: 30 | loss: 0.2120977
	speed: 0.0356s/iter; left time: 644.4126s
Epoch: 30 cost time: 10.23165512084961
Epoch: 30, Steps: 258 | Train Loss: 0.2139921 Vali Loss: 0.9417951 Test Loss: 0.4207592
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2029880
	speed: 0.1505s/iter; left time: 2702.8331s
	iters: 200, epoch: 31 | loss: 0.2021782
	speed: 0.0334s/iter; left time: 596.8448s
Epoch: 31 cost time: 10.12695050239563
Epoch: 31, Steps: 258 | Train Loss: 0.2139504 Vali Loss: 0.9410725 Test Loss: 0.4208063
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2143639
	speed: 0.1401s/iter; left time: 2479.9417s
	iters: 200, epoch: 32 | loss: 0.2219419
	speed: 0.0321s/iter; left time: 565.3973s
Epoch: 32 cost time: 9.80459713935852
Epoch: 32, Steps: 258 | Train Loss: 0.2139940 Vali Loss: 0.9410440 Test Loss: 0.4206460
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2125464
	speed: 0.1355s/iter; left time: 2363.9811s
	iters: 200, epoch: 33 | loss: 0.1986169
	speed: 0.0329s/iter; left time: 570.4759s
Epoch: 33 cost time: 8.910689353942871
Epoch: 33, Steps: 258 | Train Loss: 0.2140151 Vali Loss: 0.9413323 Test Loss: 0.4209564
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2114177
	speed: 0.1377s/iter; left time: 2365.9039s
	iters: 200, epoch: 34 | loss: 0.2145774
	speed: 0.0335s/iter; left time: 571.7759s
Epoch: 34 cost time: 9.48868203163147
Epoch: 34, Steps: 258 | Train Loss: 0.2140026 Vali Loss: 0.9410159 Test Loss: 0.4208419
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2291603
	speed: 0.1419s/iter; left time: 2402.2817s
	iters: 200, epoch: 35 | loss: 0.2254436
	speed: 0.0392s/iter; left time: 660.1674s
Epoch: 35 cost time: 10.34449291229248
Epoch: 35, Steps: 258 | Train Loss: 0.2139057 Vali Loss: 0.9409728 Test Loss: 0.4207542
EarlyStopping counter: 20 out of 20
Early stopping
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=58, out_features=116, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6028288.0
params:  6844.0
Trainable parameters:  6844
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4350147
	speed: 0.0406s/iter; left time: 1043.6266s
	iters: 200, epoch: 1 | loss: 0.3809627
	speed: 0.0332s/iter; left time: 849.0886s
Epoch: 1 cost time: 9.53536581993103
Epoch: 1, Steps: 258 | Train Loss: 0.4002150 Vali Loss: 0.9365146 Test Loss: 0.4195879
Validation loss decreased (inf --> 0.936515).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3870164
	speed: 0.1502s/iter; left time: 3822.4639s
	iters: 200, epoch: 2 | loss: 0.3859929
	speed: 0.0359s/iter; left time: 908.8714s
Epoch: 2 cost time: 9.806605815887451
Epoch: 2, Steps: 258 | Train Loss: 0.3995987 Vali Loss: 0.9360683 Test Loss: 0.4198739
Validation loss decreased (0.936515 --> 0.936068).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3990439
	speed: 0.1378s/iter; left time: 3470.2407s
	iters: 200, epoch: 3 | loss: 0.4075551
	speed: 0.0292s/iter; left time: 733.7117s
Epoch: 3 cost time: 8.439161539077759
Epoch: 3, Steps: 258 | Train Loss: 0.3991956 Vali Loss: 0.9350600 Test Loss: 0.4190400
Validation loss decreased (0.936068 --> 0.935060).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4068696
	speed: 0.1518s/iter; left time: 3785.0453s
	iters: 200, epoch: 4 | loss: 0.4111651
	speed: 0.0342s/iter; left time: 848.8948s
Epoch: 4 cost time: 10.101791620254517
Epoch: 4, Steps: 258 | Train Loss: 0.3990462 Vali Loss: 0.9345620 Test Loss: 0.4192408
Validation loss decreased (0.935060 --> 0.934562).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4096811
	speed: 0.1586s/iter; left time: 3912.9784s
	iters: 200, epoch: 5 | loss: 0.3949040
	speed: 0.0428s/iter; left time: 1051.3291s
Epoch: 5 cost time: 11.14051866531372
Epoch: 5, Steps: 258 | Train Loss: 0.3989139 Vali Loss: 0.9336671 Test Loss: 0.4200018
Validation loss decreased (0.934562 --> 0.933667).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4331709
	speed: 0.1718s/iter; left time: 4193.6647s
	iters: 200, epoch: 6 | loss: 0.4179759
	speed: 0.0412s/iter; left time: 1000.8853s
Epoch: 6 cost time: 11.637169599533081
Epoch: 6, Steps: 258 | Train Loss: 0.3987732 Vali Loss: 0.9336551 Test Loss: 0.4189196
Validation loss decreased (0.933667 --> 0.933655).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4032171
	speed: 0.1550s/iter; left time: 3743.7918s
	iters: 200, epoch: 7 | loss: 0.4182885
	speed: 0.0319s/iter; left time: 767.3948s
Epoch: 7 cost time: 9.039440393447876
Epoch: 7, Steps: 258 | Train Loss: 0.3987982 Vali Loss: 0.9347328 Test Loss: 0.4197247
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4207658
	speed: 0.1429s/iter; left time: 3415.3513s
	iters: 200, epoch: 8 | loss: 0.4044550
	speed: 0.0295s/iter; left time: 702.6676s
Epoch: 8 cost time: 8.90786099433899
Epoch: 8, Steps: 258 | Train Loss: 0.3987119 Vali Loss: 0.9334813 Test Loss: 0.4193908
Validation loss decreased (0.933655 --> 0.933481).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3849316
	speed: 0.1531s/iter; left time: 3619.0371s
	iters: 200, epoch: 9 | loss: 0.4547382
	speed: 0.0348s/iter; left time: 819.8897s
Epoch: 9 cost time: 10.190458059310913
Epoch: 9, Steps: 258 | Train Loss: 0.3986915 Vali Loss: 0.9337935 Test Loss: 0.4196347
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3854185
	speed: 0.1612s/iter; left time: 3769.1821s
	iters: 200, epoch: 10 | loss: 0.3941481
	speed: 0.0370s/iter; left time: 862.1433s
Epoch: 10 cost time: 9.978827714920044
Epoch: 10, Steps: 258 | Train Loss: 0.3987035 Vali Loss: 0.9330384 Test Loss: 0.4191553
Validation loss decreased (0.933481 --> 0.933038).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3961410
	speed: 0.1506s/iter; left time: 3482.1306s
	iters: 200, epoch: 11 | loss: 0.4281227
	speed: 0.0342s/iter; left time: 787.2093s
Epoch: 11 cost time: 9.587002754211426
Epoch: 11, Steps: 258 | Train Loss: 0.3986153 Vali Loss: 0.9332837 Test Loss: 0.4196267
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4573264
	speed: 0.1489s/iter; left time: 3404.1534s
	iters: 200, epoch: 12 | loss: 0.3937915
	speed: 0.0328s/iter; left time: 745.6068s
Epoch: 12 cost time: 9.04209852218628
Epoch: 12, Steps: 258 | Train Loss: 0.3987228 Vali Loss: 0.9330146 Test Loss: 0.4195022
Validation loss decreased (0.933038 --> 0.933015).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3949927
	speed: 0.1469s/iter; left time: 3319.7477s
	iters: 200, epoch: 13 | loss: 0.3815729
	speed: 0.0326s/iter; left time: 733.1386s
Epoch: 13 cost time: 9.089410781860352
Epoch: 13, Steps: 258 | Train Loss: 0.3985618 Vali Loss: 0.9323328 Test Loss: 0.4192853
Validation loss decreased (0.933015 --> 0.932333).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4037938
	speed: 0.1416s/iter; left time: 3164.4701s
	iters: 200, epoch: 14 | loss: 0.4033810
	speed: 0.0347s/iter; left time: 772.7551s
Epoch: 14 cost time: 9.237558364868164
Epoch: 14, Steps: 258 | Train Loss: 0.3985218 Vali Loss: 0.9329225 Test Loss: 0.4193779
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3842573
	speed: 0.1581s/iter; left time: 3491.5093s
	iters: 200, epoch: 15 | loss: 0.3755397
	speed: 0.0369s/iter; left time: 812.0939s
Epoch: 15 cost time: 11.440702199935913
Epoch: 15, Steps: 258 | Train Loss: 0.3985238 Vali Loss: 0.9332754 Test Loss: 0.4194147
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3528583
	speed: 0.1399s/iter; left time: 3055.0508s
	iters: 200, epoch: 16 | loss: 0.3858506
	speed: 0.0356s/iter; left time: 773.3905s
Epoch: 16 cost time: 9.556160688400269
Epoch: 16, Steps: 258 | Train Loss: 0.3984020 Vali Loss: 0.9331855 Test Loss: 0.4194666
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3599652
	speed: 0.1423s/iter; left time: 3070.8310s
	iters: 200, epoch: 17 | loss: 0.4061286
	speed: 0.0332s/iter; left time: 712.5587s
Epoch: 17 cost time: 9.514312505722046
Epoch: 17, Steps: 258 | Train Loss: 0.3984940 Vali Loss: 0.9332017 Test Loss: 0.4192796
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3994595
	speed: 0.1400s/iter; left time: 2984.3442s
	iters: 200, epoch: 18 | loss: 0.3756510
	speed: 0.0364s/iter; left time: 772.3513s
Epoch: 18 cost time: 9.812638521194458
Epoch: 18, Steps: 258 | Train Loss: 0.3985235 Vali Loss: 0.9339942 Test Loss: 0.4194536
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4059660
	speed: 0.1454s/iter; left time: 3060.7629s
	iters: 200, epoch: 19 | loss: 0.3791178
	speed: 0.0367s/iter; left time: 769.4612s
Epoch: 19 cost time: 9.971335411071777
Epoch: 19, Steps: 258 | Train Loss: 0.3984830 Vali Loss: 0.9332461 Test Loss: 0.4196633
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3786687
	speed: 0.1506s/iter; left time: 3132.7570s
	iters: 200, epoch: 20 | loss: 0.3584940
	speed: 0.0370s/iter; left time: 766.3250s
Epoch: 20 cost time: 10.102754831314087
Epoch: 20, Steps: 258 | Train Loss: 0.3983358 Vali Loss: 0.9334059 Test Loss: 0.4197164
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3900360
	speed: 0.1469s/iter; left time: 3018.4122s
	iters: 200, epoch: 21 | loss: 0.3963803
	speed: 0.0314s/iter; left time: 640.9990s
Epoch: 21 cost time: 9.086692333221436
Epoch: 21, Steps: 258 | Train Loss: 0.3984832 Vali Loss: 0.9336816 Test Loss: 0.4193639
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4076834
	speed: 0.1574s/iter; left time: 3193.2365s
	iters: 200, epoch: 22 | loss: 0.4019363
	speed: 0.0321s/iter; left time: 647.3254s
Epoch: 22 cost time: 9.572319984436035
Epoch: 22, Steps: 258 | Train Loss: 0.3983651 Vali Loss: 0.9325473 Test Loss: 0.4195483
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3694028
	speed: 0.1582s/iter; left time: 3168.9252s
	iters: 200, epoch: 23 | loss: 0.3738062
	speed: 0.0344s/iter; left time: 686.2468s
Epoch: 23 cost time: 10.12407660484314
Epoch: 23, Steps: 258 | Train Loss: 0.3983646 Vali Loss: 0.9325069 Test Loss: 0.4195548
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3843958
	speed: 0.1634s/iter; left time: 3230.2092s
	iters: 200, epoch: 24 | loss: 0.4012525
	speed: 0.0396s/iter; left time: 779.6155s
Epoch: 24 cost time: 11.356948614120483
Epoch: 24, Steps: 258 | Train Loss: 0.3985336 Vali Loss: 0.9334546 Test Loss: 0.4196663
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4050173
	speed: 0.1585s/iter; left time: 3092.0653s
	iters: 200, epoch: 25 | loss: 0.4231881
	speed: 0.0415s/iter; left time: 806.0263s
Epoch: 25 cost time: 11.372707843780518
Epoch: 25, Steps: 258 | Train Loss: 0.3984717 Vali Loss: 0.9333767 Test Loss: 0.4196312
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3779911
	speed: 0.1457s/iter; left time: 2805.7657s
	iters: 200, epoch: 26 | loss: 0.4066051
	speed: 0.0380s/iter; left time: 728.0314s
Epoch: 26 cost time: 9.71948504447937
Epoch: 26, Steps: 258 | Train Loss: 0.3983754 Vali Loss: 0.9323319 Test Loss: 0.4193287
Validation loss decreased (0.932333 --> 0.932332).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4229709
	speed: 0.1542s/iter; left time: 2929.6421s
	iters: 200, epoch: 27 | loss: 0.3885545
	speed: 0.0350s/iter; left time: 660.5700s
Epoch: 27 cost time: 10.203739404678345
Epoch: 27, Steps: 258 | Train Loss: 0.3984152 Vali Loss: 0.9332656 Test Loss: 0.4196905
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4053410
	speed: 0.1500s/iter; left time: 2810.4637s
	iters: 200, epoch: 28 | loss: 0.4003981
	speed: 0.0349s/iter; left time: 649.6905s
Epoch: 28 cost time: 9.585987329483032
Epoch: 28, Steps: 258 | Train Loss: 0.3982813 Vali Loss: 0.9333054 Test Loss: 0.4194922
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3920249
	speed: 0.1419s/iter; left time: 2622.6870s
	iters: 200, epoch: 29 | loss: 0.4307989
	speed: 0.0346s/iter; left time: 636.2290s
Epoch: 29 cost time: 9.686861753463745
Epoch: 29, Steps: 258 | Train Loss: 0.3982149 Vali Loss: 0.9318995 Test Loss: 0.4194066
Validation loss decreased (0.932332 --> 0.931899).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4257307
	speed: 0.1504s/iter; left time: 2740.2100s
	iters: 200, epoch: 30 | loss: 0.4085993
	speed: 0.0329s/iter; left time: 595.4006s
Epoch: 30 cost time: 9.969204187393188
Epoch: 30, Steps: 258 | Train Loss: 0.3982393 Vali Loss: 0.9329504 Test Loss: 0.4195839
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4088145
	speed: 0.1390s/iter; left time: 2497.2252s
	iters: 200, epoch: 31 | loss: 0.4237083
	speed: 0.0305s/iter; left time: 543.9882s
Epoch: 31 cost time: 8.923019170761108
Epoch: 31, Steps: 258 | Train Loss: 0.3982926 Vali Loss: 0.9324109 Test Loss: 0.4195503
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3733365
	speed: 0.1509s/iter; left time: 2670.9742s
	iters: 200, epoch: 32 | loss: 0.3928094
	speed: 0.0357s/iter; left time: 628.7874s
Epoch: 32 cost time: 9.283571004867554
Epoch: 32, Steps: 258 | Train Loss: 0.3984332 Vali Loss: 0.9332185 Test Loss: 0.4196944
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4464207
	speed: 0.1447s/iter; left time: 2524.7972s
	iters: 200, epoch: 33 | loss: 0.4029045
	speed: 0.0352s/iter; left time: 609.6819s
Epoch: 33 cost time: 10.000683546066284
Epoch: 33, Steps: 258 | Train Loss: 0.3983933 Vali Loss: 0.9330307 Test Loss: 0.4195103
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4083607
	speed: 0.1497s/iter; left time: 2573.3696s
	iters: 200, epoch: 34 | loss: 0.4206105
	speed: 0.0356s/iter; left time: 608.2270s
Epoch: 34 cost time: 9.667272329330444
Epoch: 34, Steps: 258 | Train Loss: 0.3983236 Vali Loss: 0.9329954 Test Loss: 0.4194969
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3847288
	speed: 0.1397s/iter; left time: 2364.7941s
	iters: 200, epoch: 35 | loss: 0.4105675
	speed: 0.0384s/iter; left time: 647.0379s
Epoch: 35 cost time: 10.368449687957764
Epoch: 35, Steps: 258 | Train Loss: 0.3982251 Vali Loss: 0.9326796 Test Loss: 0.4195134
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4134747
	speed: 0.1503s/iter; left time: 2505.7552s
	iters: 200, epoch: 36 | loss: 0.4029517
	speed: 0.0385s/iter; left time: 638.3011s
Epoch: 36 cost time: 10.051064729690552
Epoch: 36, Steps: 258 | Train Loss: 0.3983963 Vali Loss: 0.9329159 Test Loss: 0.4193745
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4308638
	speed: 0.1479s/iter; left time: 2427.7251s
	iters: 200, epoch: 37 | loss: 0.4116419
	speed: 0.0330s/iter; left time: 538.6369s
Epoch: 37 cost time: 9.348183393478394
Epoch: 37, Steps: 258 | Train Loss: 0.3983303 Vali Loss: 0.9330295 Test Loss: 0.4194341
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3734043
	speed: 0.1442s/iter; left time: 2330.0499s
	iters: 200, epoch: 38 | loss: 0.4035140
	speed: 0.0359s/iter; left time: 575.7880s
Epoch: 38 cost time: 9.811642408370972
Epoch: 38, Steps: 258 | Train Loss: 0.3982979 Vali Loss: 0.9328923 Test Loss: 0.4193484
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.3884759
	speed: 0.1513s/iter; left time: 2405.9477s
	iters: 200, epoch: 39 | loss: 0.4254290
	speed: 0.0358s/iter; left time: 564.7861s
Epoch: 39 cost time: 10.604400157928467
Epoch: 39, Steps: 258 | Train Loss: 0.3982000 Vali Loss: 0.9326692 Test Loss: 0.4196259
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3918879
	speed: 0.1408s/iter; left time: 2201.9694s
	iters: 200, epoch: 40 | loss: 0.4062334
	speed: 0.0350s/iter; left time: 543.5325s
Epoch: 40 cost time: 9.52515959739685
Epoch: 40, Steps: 258 | Train Loss: 0.3982404 Vali Loss: 0.9325874 Test Loss: 0.4196024
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3856187
	speed: 0.1481s/iter; left time: 2277.7157s
	iters: 200, epoch: 41 | loss: 0.3810547
	speed: 0.0338s/iter; left time: 516.0710s
Epoch: 41 cost time: 9.709207773208618
Epoch: 41, Steps: 258 | Train Loss: 0.3982414 Vali Loss: 0.9321932 Test Loss: 0.4195259
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.3969918
	speed: 0.1432s/iter; left time: 2166.2161s
	iters: 200, epoch: 42 | loss: 0.4269798
	speed: 0.0337s/iter; left time: 506.1941s
Epoch: 42 cost time: 9.456916570663452
Epoch: 42, Steps: 258 | Train Loss: 0.3982588 Vali Loss: 0.9321471 Test Loss: 0.4194995
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3988794
	speed: 0.1460s/iter; left time: 2170.9270s
	iters: 200, epoch: 43 | loss: 0.3934639
	speed: 0.0372s/iter; left time: 548.6380s
Epoch: 43 cost time: 10.50709080696106
Epoch: 43, Steps: 258 | Train Loss: 0.3982879 Vali Loss: 0.9331670 Test Loss: 0.4195284
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3877025
	speed: 0.1564s/iter; left time: 2285.2282s
	iters: 200, epoch: 44 | loss: 0.3883734
	speed: 0.0329s/iter; left time: 476.7033s
Epoch: 44 cost time: 9.719691753387451
Epoch: 44, Steps: 258 | Train Loss: 0.3981634 Vali Loss: 0.9338732 Test Loss: 0.4194888
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.3734027
	speed: 0.1530s/iter; left time: 2196.0747s
	iters: 200, epoch: 45 | loss: 0.3834547
	speed: 0.0358s/iter; left time: 510.4128s
Epoch: 45 cost time: 9.720823287963867
Epoch: 45, Steps: 258 | Train Loss: 0.3982374 Vali Loss: 0.9322305 Test Loss: 0.4195344
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.3786579
	speed: 0.1460s/iter; left time: 2057.5001s
	iters: 200, epoch: 46 | loss: 0.3863645
	speed: 0.0311s/iter; left time: 435.7577s
Epoch: 46 cost time: 9.003916263580322
Epoch: 46, Steps: 258 | Train Loss: 0.3983440 Vali Loss: 0.9329752 Test Loss: 0.4194885
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3999343
	speed: 0.1534s/iter; left time: 2122.2538s
	iters: 200, epoch: 47 | loss: 0.3799160
	speed: 0.0371s/iter; left time: 508.8823s
Epoch: 47 cost time: 11.014715671539307
Epoch: 47, Steps: 258 | Train Loss: 0.3982896 Vali Loss: 0.9329591 Test Loss: 0.4195077
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4018524
	speed: 0.1452s/iter; left time: 1971.5744s
	iters: 200, epoch: 48 | loss: 0.3836660
	speed: 0.0387s/iter; left time: 521.6876s
Epoch: 48 cost time: 10.897160530090332
Epoch: 48, Steps: 258 | Train Loss: 0.3983205 Vali Loss: 0.9325007 Test Loss: 0.4195481
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4055983
	speed: 0.1429s/iter; left time: 1903.1855s
	iters: 200, epoch: 49 | loss: 0.4187922
	speed: 0.0321s/iter; left time: 424.9203s
Epoch: 49 cost time: 9.164257526397705
Epoch: 49, Steps: 258 | Train Loss: 0.3982801 Vali Loss: 0.9326041 Test Loss: 0.4194866
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41843611001968384, mae:0.41422292590141296, rse:0.6154394745826721, corr:[0.52406687 0.5296045  0.5344705  0.53791904 0.53971463 0.5404076
 0.5405081  0.540408   0.5404357  0.5407055  0.5412369  0.541788
 0.5422148  0.54230267 0.5419246  0.54106605 0.5398185  0.5382689
 0.53647774 0.5345542  0.53263927 0.5308189  0.5291635  0.52784055
 0.5267872  0.5260935  0.5257693  0.5256906  0.52586764 0.52623576
 0.5266785  0.52716917 0.52756065 0.5278751  0.52801263 0.5280941
 0.5280192  0.52787745 0.52770406 0.5274316  0.5271407  0.5268322
 0.52656555 0.5264458  0.5264301  0.52645874 0.5265607  0.52667797
 0.52671385 0.5265744  0.5263594  0.5261237  0.5259807  0.5258545
 0.5257737  0.5256988  0.5256025  0.5254387  0.5252323  0.52493495
 0.5246185  0.52434665 0.5241528  0.5240022  0.5239742  0.52408844
 0.524282   0.5245048  0.52473885 0.52490777 0.5250187  0.5250353
 0.52492326 0.5246922  0.5244159  0.52412194 0.5238109  0.5235226
 0.52326465 0.5230478  0.5228548  0.5226618  0.5224684  0.5222446
 0.5220756  0.5219599  0.52195394 0.52203864 0.5222174  0.5225179
 0.52289265 0.5232407  0.52349657 0.52359813 0.5235547  0.5233443
 0.5230591  0.5228405  0.5226192  0.52247983 0.5224022  0.52235883
 0.5223308  0.5222576  0.5221436  0.52204806 0.52191776 0.5217816
 0.5216079  0.5214275  0.52124786 0.52104986 0.52081835 0.5205854
 0.5203555  0.5201391  0.5199435  0.5197541  0.5195608  0.51939684
 0.5192284  0.5190243  0.5187244  0.5183908  0.51807404 0.5177766
 0.5175235  0.51737726 0.51731443 0.51733816 0.51736146 0.5173349
 0.51728326 0.51721126 0.5170837  0.5169337  0.51682943 0.51672053
 0.5166547  0.516693   0.5167884  0.5169586  0.5171908  0.51746035
 0.517712   0.51783043 0.51783746 0.5177465  0.5176358  0.5175219
 0.5174014  0.5173269  0.5173157  0.5173398  0.51737404 0.51746136
 0.5175533  0.5176193  0.51767826 0.5176774  0.5176934  0.5177401
 0.5178245  0.5179343  0.5180608  0.5181905  0.51832736 0.5184909
 0.5186265  0.5186865  0.51873106 0.5187889  0.51883096 0.5188555
 0.5188785  0.5188916  0.51890826 0.5189268  0.5189246  0.51892895
 0.51894414 0.51898974 0.51902807 0.51910806 0.5192373  0.51940405
 0.5195758  0.5197415  0.51988786 0.51997167 0.51992124 0.51974887
 0.5194624  0.5191766  0.51883245 0.5183762  0.517801   0.5171507
 0.5164243  0.51566577 0.5148863  0.51416403 0.51351094 0.51294947
 0.5124753  0.51205415 0.5116337  0.51119906 0.5107016  0.5101058
 0.50938374 0.50856966 0.50771976 0.5069007  0.5061292  0.505499
 0.50509095 0.50488794 0.50486034 0.50490975 0.50495785 0.5049937
 0.5050406  0.50502795 0.504994   0.50497365 0.50496393 0.5049903
 0.50506556 0.5051979  0.50534827 0.5054451  0.50547594 0.50544494
 0.50538373 0.5053672  0.50534457 0.5054212  0.50555867 0.5057891
 0.5060574  0.50622916 0.50628966 0.50626737 0.5062235  0.50613654
 0.5060279  0.5059085  0.50577426 0.5056534  0.50551504 0.50540566
 0.50527287 0.50517756 0.5050907  0.50500464 0.5049394  0.5049085
 0.5049312  0.50502694 0.50516987 0.5053235  0.50545585 0.5056265
 0.5057507  0.5058352  0.50588006 0.5059183  0.50595254 0.50594777
 0.5059265  0.50588226 0.50583893 0.5057991  0.5057552  0.5057262
 0.505687   0.505714   0.5057531  0.5058636  0.5060123  0.50615406
 0.5063068  0.5063919  0.5063665  0.5062528  0.5060229  0.50566196
 0.5051988  0.50473595 0.50430256 0.50384635 0.5033434  0.50279367
 0.5022095  0.50156695 0.50093913 0.50033593 0.49976194 0.49925318
 0.49880552 0.49842387 0.49814692 0.49794132 0.4977668  0.4975584
 0.4973618  0.4971212  0.49684528 0.49656108 0.4963446  0.49620637
 0.49615744 0.49621478 0.49630794 0.49639443 0.49644172 0.49645945
 0.49643248 0.4963326  0.4961713  0.49599147 0.4958139  0.4956486
 0.49550283 0.4954096  0.49533638 0.49527964 0.495192   0.49506244
 0.49496672 0.49490592 0.49486712 0.49486762 0.4948936  0.49501312
 0.49515998 0.49519718 0.49515888 0.4950592  0.49494177 0.49483103
 0.4946995  0.49461296 0.49455443 0.49452066 0.49450168 0.49451083
 0.49448508 0.49445638 0.49441895 0.4943569  0.4943146  0.49429512
 0.49428222 0.49429464 0.49431303 0.4943231  0.49434134 0.49437866
 0.49435547 0.49432158 0.49426505 0.49420106 0.49414226 0.49409297
 0.49399143 0.49388632 0.49379474 0.49368984 0.49361706 0.49360508
 0.49367318 0.49384654 0.4941103  0.49447134 0.49487352 0.49530125
 0.4957191  0.49607107 0.4963419  0.49643755 0.49634314 0.49611053
 0.49577484 0.4953881  0.49502224 0.49466693 0.49424943 0.4938613
 0.49349108 0.49312568 0.49275303 0.49241003 0.49205035 0.4916839
 0.49132293 0.4909785  0.49065632 0.4903554  0.49001047 0.48966968
 0.48930228 0.48896036 0.48860368 0.4882786  0.48801035 0.4878442
 0.48777953 0.48780814 0.48789924 0.48802784 0.48815176 0.48821226
 0.4883012  0.4883417  0.48839256 0.48840305 0.4883991  0.48842692
 0.48843724 0.48846108 0.48845518 0.48842615 0.48833355 0.4882523
 0.4881835  0.48816335 0.48815408 0.48822752 0.48831725 0.48849794
 0.48866725 0.48869723 0.48860785 0.4884498  0.48824993 0.48804918
 0.4878558  0.4876957  0.48759428 0.48754245 0.48753875 0.4875857
 0.48764578 0.4877295  0.48781127 0.4878974  0.48798978 0.48806491
 0.48816538 0.48827165 0.48836783 0.4884249  0.48843113 0.48844534
 0.48839724 0.4883258  0.48824042 0.48819372 0.48813102 0.4880855
 0.48802397 0.4879655  0.48787126 0.48776916 0.4876831  0.48763266
 0.48764992 0.48770967 0.48782024 0.48796076 0.48810932 0.4882377
 0.488279   0.4882388  0.48812464 0.48789325 0.48753923 0.4871033
 0.4866272  0.48617214 0.48576805 0.485367   0.48490107 0.48437467
 0.48382863 0.48319033 0.48250082 0.48181662 0.48114124 0.48049277
 0.47990364 0.47942087 0.47903526 0.47867918 0.47831005 0.4779308
 0.47752625 0.47713664 0.47677782 0.4763813  0.47605067 0.47578847
 0.47563747 0.47559527 0.47562045 0.4757249  0.4758559  0.47602448
 0.47622308 0.47638133 0.47648406 0.47660622 0.4767395  0.47688144
 0.4770029  0.47714078 0.47732478 0.47747627 0.47757572 0.47766834
 0.47777414 0.47791782 0.47808173 0.47821435 0.47835356 0.47854254
 0.4787338  0.47885582 0.47885922 0.47881836 0.47879305 0.47877315
 0.47875556 0.4787193  0.47865587 0.47857326 0.4784564  0.47830567
 0.47813565 0.47795686 0.47780186 0.47769403 0.47763923 0.47767097
 0.47776285 0.47789714 0.47804192 0.47813419 0.47816202 0.4781911
 0.47816646 0.47811937 0.47807965 0.47807837 0.4780974  0.47814155
 0.47818115 0.47821555 0.4782278  0.4782255  0.47821236 0.4781867
 0.4781451  0.47817287 0.4782189  0.4783099  0.47843707 0.4785831
 0.47871664 0.4787699  0.47874507 0.47859702 0.47830376 0.47784266
 0.4772748  0.4767178  0.4761997  0.47569475 0.475148   0.4746065
 0.47403613 0.4734503  0.47284293 0.47223094 0.47164774 0.47108093
 0.47058678 0.4701767  0.46983764 0.46954218 0.46928692 0.4690317
 0.46875998 0.4684551  0.468144   0.4678529  0.46759972 0.46740118
 0.46729404 0.46724996 0.46726793 0.467325   0.46742147 0.46750435
 0.4676221  0.46774223 0.4678661  0.46800575 0.468146   0.46831095
 0.46848816 0.46862513 0.4687522  0.46884578 0.46890166 0.468928
 0.46897182 0.4690806  0.4692102  0.46935847 0.46959606 0.46989372
 0.47015777 0.47035798 0.4704622  0.4704642  0.4704552  0.4704709
 0.4704549  0.47044572 0.47043598 0.47038683 0.47029993 0.47014353
 0.46988073 0.4695569  0.46923596 0.46890697 0.46866548 0.46851328
 0.46846458 0.46849796 0.46860632 0.46876103 0.46891993 0.4690872
 0.4692149  0.4692514  0.46924162 0.46921363 0.469175   0.46912867
 0.46908197 0.46904686 0.46903285 0.46905246 0.4690993  0.46915367
 0.46921435 0.46932405 0.4694697  0.4696109  0.469762   0.46989757
 0.46998817 0.4699981  0.469905   0.46973228 0.46943977 0.46901828
 0.46849993 0.46798992 0.46753165 0.467095   0.4666397  0.46615222
 0.46563998 0.46508545 0.4645656  0.46410036 0.46363285 0.46327606
 0.4629561  0.46270236 0.46246356 0.46225107 0.46202245 0.46174598
 0.46143574 0.46110085 0.46081483 0.46055657 0.4604037  0.4603997
 0.4605178  0.46076414 0.46105033 0.46132722 0.46159965 0.46179494
 0.46195972 0.4620962  0.46223608 0.46245256 0.46268022 0.4629438
 0.46321535 0.46350974 0.46382898 0.46414587 0.46450797 0.46489075
 0.46533006 0.46577066 0.46598426 0.46586895 0.46514884 0.46332973]
