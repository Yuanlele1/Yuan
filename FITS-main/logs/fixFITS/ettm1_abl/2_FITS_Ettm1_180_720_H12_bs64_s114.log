Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=34, out_features=170, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5178880.0
params:  5950.0
Trainable parameters:  5950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6602331
	speed: 0.0476s/iter; left time: 1242.7790s
	iters: 200, epoch: 1 | loss: 0.5201983
	speed: 0.0391s/iter; left time: 1017.3689s
Epoch: 1 cost time: 10.253483295440674
Epoch: 1, Steps: 262 | Train Loss: 0.6733775 Vali Loss: 1.2109109 Test Loss: 0.6368147
Validation loss decreased (inf --> 1.210911).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4042746
	speed: 0.1407s/iter; left time: 3636.2300s
	iters: 200, epoch: 2 | loss: 0.3872975
	speed: 0.0415s/iter; left time: 1068.5410s
Epoch: 2 cost time: 8.536847114562988
Epoch: 2, Steps: 262 | Train Loss: 0.4277501 Vali Loss: 1.0780485 Test Loss: 0.5185915
Validation loss decreased (1.210911 --> 1.078048).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3739855
	speed: 0.1223s/iter; left time: 3127.5194s
	iters: 200, epoch: 3 | loss: 0.3956639
	speed: 0.0262s/iter; left time: 666.8259s
Epoch: 3 cost time: 7.636897325515747
Epoch: 3, Steps: 262 | Train Loss: 0.3886852 Vali Loss: 1.0352781 Test Loss: 0.4829069
Validation loss decreased (1.078048 --> 1.035278).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3473997
	speed: 0.1333s/iter; left time: 3375.6708s
	iters: 200, epoch: 4 | loss: 0.3254822
	speed: 0.0390s/iter; left time: 982.7256s
Epoch: 4 cost time: 9.071524620056152
Epoch: 4, Steps: 262 | Train Loss: 0.3747933 Vali Loss: 1.0127817 Test Loss: 0.4657455
Validation loss decreased (1.035278 --> 1.012782).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3876707
	speed: 0.1387s/iter; left time: 3473.6502s
	iters: 200, epoch: 5 | loss: 0.3902121
	speed: 0.0304s/iter; left time: 758.5178s
Epoch: 5 cost time: 8.078505277633667
Epoch: 5, Steps: 262 | Train Loss: 0.3676203 Vali Loss: 1.0001378 Test Loss: 0.4567756
Validation loss decreased (1.012782 --> 1.000138).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3791521
	speed: 0.1414s/iter; left time: 3506.2532s
	iters: 200, epoch: 6 | loss: 0.3929586
	speed: 0.0518s/iter; left time: 1278.3694s
Epoch: 6 cost time: 12.556585550308228
Epoch: 6, Steps: 262 | Train Loss: 0.3635971 Vali Loss: 0.9927406 Test Loss: 0.4516667
Validation loss decreased (1.000138 --> 0.992741).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3344357
	speed: 0.2350s/iter; left time: 5765.2498s
	iters: 200, epoch: 7 | loss: 0.3618864
	speed: 0.0473s/iter; left time: 1155.0173s
Epoch: 7 cost time: 11.602107048034668
Epoch: 7, Steps: 262 | Train Loss: 0.3611983 Vali Loss: 0.9884652 Test Loss: 0.4488810
Validation loss decreased (0.992741 --> 0.988465).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3483184
	speed: 0.1668s/iter; left time: 4047.3744s
	iters: 200, epoch: 8 | loss: 0.3866329
	speed: 0.0684s/iter; left time: 1651.9263s
Epoch: 8 cost time: 14.515312433242798
Epoch: 8, Steps: 262 | Train Loss: 0.3598426 Vali Loss: 0.9870309 Test Loss: 0.4475484
Validation loss decreased (0.988465 --> 0.987031).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4034468
	speed: 0.1933s/iter; left time: 4640.9298s
	iters: 200, epoch: 9 | loss: 0.3903117
	speed: 0.0259s/iter; left time: 618.1930s
Epoch: 9 cost time: 9.243350982666016
Epoch: 9, Steps: 262 | Train Loss: 0.3590364 Vali Loss: 0.9846301 Test Loss: 0.4469920
Validation loss decreased (0.987031 --> 0.984630).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3443964
	speed: 0.1367s/iter; left time: 3245.5907s
	iters: 200, epoch: 10 | loss: 0.3548070
	speed: 0.0276s/iter; left time: 651.8763s
Epoch: 10 cost time: 7.856249094009399
Epoch: 10, Steps: 262 | Train Loss: 0.3587791 Vali Loss: 0.9838413 Test Loss: 0.4466673
Validation loss decreased (0.984630 --> 0.983841).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3292234
	speed: 0.1236s/iter; left time: 2901.3633s
	iters: 200, epoch: 11 | loss: 0.3696754
	speed: 0.0406s/iter; left time: 949.7091s
Epoch: 11 cost time: 11.788330793380737
Epoch: 11, Steps: 262 | Train Loss: 0.3584566 Vali Loss: 0.9830709 Test Loss: 0.4463882
Validation loss decreased (0.983841 --> 0.983071).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3548320
	speed: 0.1531s/iter; left time: 3554.9829s
	iters: 200, epoch: 12 | loss: 0.3637294
	speed: 0.0250s/iter; left time: 578.0669s
Epoch: 12 cost time: 7.489226818084717
Epoch: 12, Steps: 262 | Train Loss: 0.3584559 Vali Loss: 0.9823588 Test Loss: 0.4462933
Validation loss decreased (0.983071 --> 0.982359).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3625647
	speed: 0.1665s/iter; left time: 3822.3170s
	iters: 200, epoch: 13 | loss: 0.3488128
	speed: 0.0352s/iter; left time: 804.7941s
Epoch: 13 cost time: 9.192948579788208
Epoch: 13, Steps: 262 | Train Loss: 0.3583012 Vali Loss: 0.9826581 Test Loss: 0.4463733
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3688101
	speed: 0.1314s/iter; left time: 2982.6965s
	iters: 200, epoch: 14 | loss: 0.3190006
	speed: 0.0299s/iter; left time: 676.2464s
Epoch: 14 cost time: 7.016762971878052
Epoch: 14, Steps: 262 | Train Loss: 0.3583158 Vali Loss: 0.9830208 Test Loss: 0.4463643
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3277340
	speed: 0.1081s/iter; left time: 2424.8515s
	iters: 200, epoch: 15 | loss: 0.3528843
	speed: 0.0277s/iter; left time: 619.2892s
Epoch: 15 cost time: 7.328843593597412
Epoch: 15, Steps: 262 | Train Loss: 0.3583220 Vali Loss: 0.9826478 Test Loss: 0.4464604
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3409255
	speed: 0.1320s/iter; left time: 2925.5448s
	iters: 200, epoch: 16 | loss: 0.3678481
	speed: 0.0365s/iter; left time: 806.1127s
Epoch: 16 cost time: 10.386087894439697
Epoch: 16, Steps: 262 | Train Loss: 0.3581483 Vali Loss: 0.9824356 Test Loss: 0.4464923
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3616070
	speed: 0.1609s/iter; left time: 3524.5427s
	iters: 200, epoch: 17 | loss: 0.3457006
	speed: 0.0504s/iter; left time: 1098.6464s
Epoch: 17 cost time: 11.464653491973877
Epoch: 17, Steps: 262 | Train Loss: 0.3582404 Vali Loss: 0.9828172 Test Loss: 0.4465649
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3799977
	speed: 0.1675s/iter; left time: 3626.6534s
	iters: 200, epoch: 18 | loss: 0.3352994
	speed: 0.0328s/iter; left time: 706.4732s
Epoch: 18 cost time: 10.110604286193848
Epoch: 18, Steps: 262 | Train Loss: 0.3581876 Vali Loss: 0.9822932 Test Loss: 0.4465012
Validation loss decreased (0.982359 --> 0.982293).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3803465
	speed: 0.1491s/iter; left time: 3187.8325s
	iters: 200, epoch: 19 | loss: 0.3191317
	speed: 0.0282s/iter; left time: 601.1224s
Epoch: 19 cost time: 7.769302606582642
Epoch: 19, Steps: 262 | Train Loss: 0.3582723 Vali Loss: 0.9822785 Test Loss: 0.4464679
Validation loss decreased (0.982293 --> 0.982279).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3654631
	speed: 0.1767s/iter; left time: 3731.5277s
	iters: 200, epoch: 20 | loss: 0.3643380
	speed: 0.0603s/iter; left time: 1267.5642s
Epoch: 20 cost time: 15.227890729904175
Epoch: 20, Steps: 262 | Train Loss: 0.3582161 Vali Loss: 0.9822428 Test Loss: 0.4465034
Validation loss decreased (0.982279 --> 0.982243).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3939065
	speed: 0.2148s/iter; left time: 4479.9654s
	iters: 200, epoch: 21 | loss: 0.3414606
	speed: 0.0558s/iter; left time: 1159.1156s
Epoch: 21 cost time: 14.052302360534668
Epoch: 21, Steps: 262 | Train Loss: 0.3583456 Vali Loss: 0.9817855 Test Loss: 0.4465726
Validation loss decreased (0.982243 --> 0.981785).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3658940
	speed: 0.2449s/iter; left time: 5044.5669s
	iters: 200, epoch: 22 | loss: 0.3835047
	speed: 0.0322s/iter; left time: 659.4338s
Epoch: 22 cost time: 11.131965398788452
Epoch: 22, Steps: 262 | Train Loss: 0.3581390 Vali Loss: 0.9829342 Test Loss: 0.4465720
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3510623
	speed: 0.1611s/iter; left time: 3275.2965s
	iters: 200, epoch: 23 | loss: 0.3592799
	speed: 0.0277s/iter; left time: 560.6982s
Epoch: 23 cost time: 9.066088199615479
Epoch: 23, Steps: 262 | Train Loss: 0.3582320 Vali Loss: 0.9822201 Test Loss: 0.4465589
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3705671
	speed: 0.1289s/iter; left time: 2586.7396s
	iters: 200, epoch: 24 | loss: 0.3856674
	speed: 0.0389s/iter; left time: 777.3259s
Epoch: 24 cost time: 8.860697269439697
Epoch: 24, Steps: 262 | Train Loss: 0.3580763 Vali Loss: 0.9827130 Test Loss: 0.4465320
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3473147
	speed: 0.1752s/iter; left time: 3471.2693s
	iters: 200, epoch: 25 | loss: 0.3532757
	speed: 0.0274s/iter; left time: 539.5691s
Epoch: 25 cost time: 9.517023086547852
Epoch: 25, Steps: 262 | Train Loss: 0.3581056 Vali Loss: 0.9816660 Test Loss: 0.4464708
Validation loss decreased (0.981785 --> 0.981666).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3503599
	speed: 0.1485s/iter; left time: 2903.4705s
	iters: 200, epoch: 26 | loss: 0.3488941
	speed: 0.0410s/iter; left time: 797.0478s
Epoch: 26 cost time: 12.5588960647583
Epoch: 26, Steps: 262 | Train Loss: 0.3582508 Vali Loss: 0.9818588 Test Loss: 0.4465342
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3149794
	speed: 0.1774s/iter; left time: 3421.3811s
	iters: 200, epoch: 27 | loss: 0.3573086
	speed: 0.0316s/iter; left time: 606.9960s
Epoch: 27 cost time: 11.555119514465332
Epoch: 27, Steps: 262 | Train Loss: 0.3581904 Vali Loss: 0.9816496 Test Loss: 0.4465275
Validation loss decreased (0.981666 --> 0.981650).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3679996
	speed: 0.1581s/iter; left time: 3009.0879s
	iters: 200, epoch: 28 | loss: 0.3971452
	speed: 0.0286s/iter; left time: 540.7571s
Epoch: 28 cost time: 9.307775974273682
Epoch: 28, Steps: 262 | Train Loss: 0.3579055 Vali Loss: 0.9824234 Test Loss: 0.4465244
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3583564
	speed: 0.1206s/iter; left time: 2262.5010s
	iters: 200, epoch: 29 | loss: 0.3511432
	speed: 0.0259s/iter; left time: 484.1376s
Epoch: 29 cost time: 9.047191143035889
Epoch: 29, Steps: 262 | Train Loss: 0.3581318 Vali Loss: 0.9813302 Test Loss: 0.4465267
Validation loss decreased (0.981650 --> 0.981330).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3433397
	speed: 0.1807s/iter; left time: 3344.2695s
	iters: 200, epoch: 30 | loss: 0.3289660
	speed: 0.0301s/iter; left time: 553.2642s
Epoch: 30 cost time: 9.839568138122559
Epoch: 30, Steps: 262 | Train Loss: 0.3580000 Vali Loss: 0.9816707 Test Loss: 0.4465710
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3448761
	speed: 0.1688s/iter; left time: 3079.3232s
	iters: 200, epoch: 31 | loss: 0.3790836
	speed: 0.0314s/iter; left time: 569.5602s
Epoch: 31 cost time: 11.221430778503418
Epoch: 31, Steps: 262 | Train Loss: 0.3580595 Vali Loss: 0.9817001 Test Loss: 0.4465976
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3567963
	speed: 0.2286s/iter; left time: 4110.4396s
	iters: 200, epoch: 32 | loss: 0.3348860
	speed: 0.0590s/iter; left time: 1055.2804s
Epoch: 32 cost time: 15.444734811782837
Epoch: 32, Steps: 262 | Train Loss: 0.3582064 Vali Loss: 0.9821578 Test Loss: 0.4465661
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3799312
	speed: 0.1767s/iter; left time: 3131.3846s
	iters: 200, epoch: 33 | loss: 0.3817767
	speed: 0.0326s/iter; left time: 573.6384s
Epoch: 33 cost time: 8.621387004852295
Epoch: 33, Steps: 262 | Train Loss: 0.3581352 Vali Loss: 0.9820879 Test Loss: 0.4465657
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3470311
	speed: 0.1678s/iter; left time: 2928.1352s
	iters: 200, epoch: 34 | loss: 0.3305180
	speed: 0.0512s/iter; left time: 889.0500s
Epoch: 34 cost time: 13.710840463638306
Epoch: 34, Steps: 262 | Train Loss: 0.3581832 Vali Loss: 0.9821412 Test Loss: 0.4465841
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3477851
	speed: 0.1533s/iter; left time: 2636.1652s
	iters: 200, epoch: 35 | loss: 0.3523561
	speed: 0.0347s/iter; left time: 593.0608s
Epoch: 35 cost time: 9.372756719589233
Epoch: 35, Steps: 262 | Train Loss: 0.3582060 Vali Loss: 0.9822558 Test Loss: 0.4465998
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3390012
	speed: 0.1540s/iter; left time: 2607.4357s
	iters: 200, epoch: 36 | loss: 0.3471833
	speed: 0.0394s/iter; left time: 662.7847s
Epoch: 36 cost time: 10.277354717254639
Epoch: 36, Steps: 262 | Train Loss: 0.3581984 Vali Loss: 0.9829646 Test Loss: 0.4465998
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3604806
	speed: 0.1320s/iter; left time: 2200.7139s
	iters: 200, epoch: 37 | loss: 0.3560253
	speed: 0.0327s/iter; left time: 542.3073s
Epoch: 37 cost time: 8.928802251815796
Epoch: 37, Steps: 262 | Train Loss: 0.3580001 Vali Loss: 0.9835328 Test Loss: 0.4465886
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3254630
	speed: 0.1221s/iter; left time: 2004.0127s
	iters: 200, epoch: 38 | loss: 0.3323589
	speed: 0.0320s/iter; left time: 521.2993s
Epoch: 38 cost time: 7.8210625648498535
Epoch: 38, Steps: 262 | Train Loss: 0.3581496 Vali Loss: 0.9830331 Test Loss: 0.4466038
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.3585594
	speed: 0.2006s/iter; left time: 3238.6723s
	iters: 200, epoch: 39 | loss: 0.3471346
	speed: 0.0385s/iter; left time: 618.0069s
Epoch: 39 cost time: 12.81549072265625
Epoch: 39, Steps: 262 | Train Loss: 0.3580451 Vali Loss: 0.9821089 Test Loss: 0.4465894
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3490497
	speed: 0.2031s/iter; left time: 3225.7066s
	iters: 200, epoch: 40 | loss: 0.3529848
	speed: 0.0480s/iter; left time: 757.9483s
Epoch: 40 cost time: 12.632052421569824
Epoch: 40, Steps: 262 | Train Loss: 0.3581207 Vali Loss: 0.9826490 Test Loss: 0.4465884
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3832285
	speed: 0.1783s/iter; left time: 2785.7146s
	iters: 200, epoch: 41 | loss: 0.3181580
	speed: 0.0254s/iter; left time: 394.8566s
Epoch: 41 cost time: 6.819963693618774
Epoch: 41, Steps: 262 | Train Loss: 0.3579613 Vali Loss: 0.9817900 Test Loss: 0.4466185
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.3761612
	speed: 0.1394s/iter; left time: 2140.4994s
	iters: 200, epoch: 42 | loss: 0.3510865
	speed: 0.0230s/iter; left time: 350.8577s
Epoch: 42 cost time: 8.00388216972351
Epoch: 42, Steps: 262 | Train Loss: 0.3581367 Vali Loss: 0.9819636 Test Loss: 0.4466194
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3435801
	speed: 0.1841s/iter; left time: 2779.8884s
	iters: 200, epoch: 43 | loss: 0.3417329
	speed: 0.0453s/iter; left time: 679.2087s
Epoch: 43 cost time: 13.850383281707764
Epoch: 43, Steps: 262 | Train Loss: 0.3580536 Vali Loss: 0.9828189 Test Loss: 0.4466092
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3719133
	speed: 0.2144s/iter; left time: 3180.7684s
	iters: 200, epoch: 44 | loss: 0.3600409
	speed: 0.0601s/iter; left time: 885.3962s
Epoch: 44 cost time: 16.37009286880493
Epoch: 44, Steps: 262 | Train Loss: 0.3581448 Vali Loss: 0.9826714 Test Loss: 0.4466346
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.3419805
	speed: 0.2128s/iter; left time: 3101.0460s
	iters: 200, epoch: 45 | loss: 0.3165527
	speed: 0.0321s/iter; left time: 464.0510s
Epoch: 45 cost time: 8.203995943069458
Epoch: 45, Steps: 262 | Train Loss: 0.3581620 Vali Loss: 0.9830887 Test Loss: 0.4466382
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.3390937
	speed: 0.1331s/iter; left time: 1904.8739s
	iters: 200, epoch: 46 | loss: 0.3527177
	speed: 0.0301s/iter; left time: 428.3585s
Epoch: 46 cost time: 9.59734320640564
Epoch: 46, Steps: 262 | Train Loss: 0.3581359 Vali Loss: 0.9822414 Test Loss: 0.4466284
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4076909
	speed: 0.1734s/iter; left time: 2436.7652s
	iters: 200, epoch: 47 | loss: 0.3641708
	speed: 0.0366s/iter; left time: 510.9799s
Epoch: 47 cost time: 9.667876482009888
Epoch: 47, Steps: 262 | Train Loss: 0.3581426 Vali Loss: 0.9818898 Test Loss: 0.4466501
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.3532498
	speed: 0.1598s/iter; left time: 2203.7713s
	iters: 200, epoch: 48 | loss: 0.3842911
	speed: 0.0361s/iter; left time: 493.6378s
Epoch: 48 cost time: 11.004982233047485
Epoch: 48, Steps: 262 | Train Loss: 0.3581564 Vali Loss: 0.9827413 Test Loss: 0.4466256
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.3418759
	speed: 0.1663s/iter; left time: 2249.8039s
	iters: 200, epoch: 49 | loss: 0.3571123
	speed: 0.0363s/iter; left time: 487.6215s
Epoch: 49 cost time: 11.003917932510376
Epoch: 49, Steps: 262 | Train Loss: 0.3580655 Vali Loss: 0.9835227 Test Loss: 0.4466339
EarlyStopping counter: 20 out of 20
Early stopping
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=34, out_features=170, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5178880.0
params:  5950.0
Trainable parameters:  5950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4123355
	speed: 0.0335s/iter; left time: 874.2919s
	iters: 200, epoch: 1 | loss: 0.4041523
	speed: 0.0549s/iter; left time: 1426.3533s
Epoch: 1 cost time: 10.57772445678711
Epoch: 1, Steps: 262 | Train Loss: 0.4430046 Vali Loss: 0.9806024 Test Loss: 0.4454855
Validation loss decreased (inf --> 0.980602).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4408581
	speed: 0.1149s/iter; left time: 2969.8186s
	iters: 200, epoch: 2 | loss: 0.4352380
	speed: 0.0233s/iter; left time: 600.8070s
Epoch: 2 cost time: 7.501770257949829
Epoch: 2, Steps: 262 | Train Loss: 0.4428982 Vali Loss: 0.9813243 Test Loss: 0.4451196
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4747483
	speed: 0.1696s/iter; left time: 4337.2974s
	iters: 200, epoch: 3 | loss: 0.4870994
	speed: 0.0389s/iter; left time: 990.9271s
Epoch: 3 cost time: 11.603461265563965
Epoch: 3, Steps: 262 | Train Loss: 0.4425629 Vali Loss: 0.9795681 Test Loss: 0.4453436
Validation loss decreased (0.980602 --> 0.979568).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4425125
	speed: 0.1634s/iter; left time: 4135.8674s
	iters: 200, epoch: 4 | loss: 0.4440827
	speed: 0.0444s/iter; left time: 1120.0253s
Epoch: 4 cost time: 12.1267991065979
Epoch: 4, Steps: 262 | Train Loss: 0.4425253 Vali Loss: 0.9802327 Test Loss: 0.4450291
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4568120
	speed: 0.1884s/iter; left time: 4718.7531s
	iters: 200, epoch: 5 | loss: 0.4672713
	speed: 0.0461s/iter; left time: 1150.2663s
Epoch: 5 cost time: 10.908900737762451
Epoch: 5, Steps: 262 | Train Loss: 0.4425825 Vali Loss: 0.9808410 Test Loss: 0.4451150
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4422511
	speed: 0.1356s/iter; left time: 3362.3265s
	iters: 200, epoch: 6 | loss: 0.4492213
	speed: 0.0330s/iter; left time: 814.9362s
Epoch: 6 cost time: 9.073440790176392
Epoch: 6, Steps: 262 | Train Loss: 0.4425735 Vali Loss: 0.9798561 Test Loss: 0.4453242
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4292094
	speed: 0.1312s/iter; left time: 3217.7017s
	iters: 200, epoch: 7 | loss: 0.4313914
	speed: 0.0261s/iter; left time: 638.5335s
Epoch: 7 cost time: 8.71919298171997
Epoch: 7, Steps: 262 | Train Loss: 0.4424720 Vali Loss: 0.9791448 Test Loss: 0.4453301
Validation loss decreased (0.979568 --> 0.979145).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4266089
	speed: 0.2278s/iter; left time: 5528.5714s
	iters: 200, epoch: 8 | loss: 0.4183321
	speed: 0.0495s/iter; left time: 1197.0948s
Epoch: 8 cost time: 16.36309814453125
Epoch: 8, Steps: 262 | Train Loss: 0.4425352 Vali Loss: 0.9796906 Test Loss: 0.4452746
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3716099
	speed: 0.2594s/iter; left time: 6227.4806s
	iters: 200, epoch: 9 | loss: 0.4139325
	speed: 0.0308s/iter; left time: 736.3508s
Epoch: 9 cost time: 10.691108703613281
Epoch: 9, Steps: 262 | Train Loss: 0.4425127 Vali Loss: 0.9798061 Test Loss: 0.4452575
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4546841
	speed: 0.2070s/iter; left time: 4915.2325s
	iters: 200, epoch: 10 | loss: 0.4575719
	speed: 0.0364s/iter; left time: 860.5332s
Epoch: 10 cost time: 10.504485368728638
Epoch: 10, Steps: 262 | Train Loss: 0.4423918 Vali Loss: 0.9799475 Test Loss: 0.4456244
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4834057
	speed: 0.1364s/iter; left time: 3202.7925s
	iters: 200, epoch: 11 | loss: 0.4455273
	speed: 0.0513s/iter; left time: 1199.4039s
Epoch: 11 cost time: 10.85707688331604
Epoch: 11, Steps: 262 | Train Loss: 0.4425877 Vali Loss: 0.9802970 Test Loss: 0.4452250
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4049715
	speed: 0.1258s/iter; left time: 2922.0187s
	iters: 200, epoch: 12 | loss: 0.4244079
	speed: 0.0246s/iter; left time: 568.3343s
Epoch: 12 cost time: 8.023690700531006
Epoch: 12, Steps: 262 | Train Loss: 0.4425088 Vali Loss: 0.9798761 Test Loss: 0.4452220
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4508286
	speed: 0.1556s/iter; left time: 3571.9476s
	iters: 200, epoch: 13 | loss: 0.4496574
	speed: 0.0344s/iter; left time: 785.8713s
Epoch: 13 cost time: 9.825561046600342
Epoch: 13, Steps: 262 | Train Loss: 0.4423921 Vali Loss: 0.9798700 Test Loss: 0.4450202
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4430206
	speed: 0.1486s/iter; left time: 3373.4604s
	iters: 200, epoch: 14 | loss: 0.4322336
	speed: 0.0433s/iter; left time: 977.3537s
Epoch: 14 cost time: 12.14051103591919
Epoch: 14, Steps: 262 | Train Loss: 0.4423514 Vali Loss: 0.9799886 Test Loss: 0.4453554
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4210321
	speed: 0.1789s/iter; left time: 4012.5390s
	iters: 200, epoch: 15 | loss: 0.4618594
	speed: 0.0398s/iter; left time: 889.3438s
Epoch: 15 cost time: 9.915460348129272
Epoch: 15, Steps: 262 | Train Loss: 0.4422997 Vali Loss: 0.9799405 Test Loss: 0.4452066
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4631230
	speed: 0.1703s/iter; left time: 3775.9523s
	iters: 200, epoch: 16 | loss: 0.4624086
	speed: 0.0415s/iter; left time: 916.6058s
Epoch: 16 cost time: 11.757187843322754
Epoch: 16, Steps: 262 | Train Loss: 0.4424123 Vali Loss: 0.9800666 Test Loss: 0.4453944
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4201664
	speed: 0.1479s/iter; left time: 3240.8007s
	iters: 200, epoch: 17 | loss: 0.4467883
	speed: 0.0261s/iter; left time: 569.4435s
Epoch: 17 cost time: 8.481277704238892
Epoch: 17, Steps: 262 | Train Loss: 0.4425153 Vali Loss: 0.9796714 Test Loss: 0.4452403
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4488043
	speed: 0.1428s/iter; left time: 3092.0203s
	iters: 200, epoch: 18 | loss: 0.4497845
	speed: 0.0321s/iter; left time: 691.0278s
Epoch: 18 cost time: 9.814686298370361
Epoch: 18, Steps: 262 | Train Loss: 0.4422500 Vali Loss: 0.9796360 Test Loss: 0.4453226
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4477815
	speed: 0.1921s/iter; left time: 4107.2599s
	iters: 200, epoch: 19 | loss: 0.4589711
	speed: 0.0466s/iter; left time: 992.2353s
Epoch: 19 cost time: 12.368869543075562
Epoch: 19, Steps: 262 | Train Loss: 0.4422220 Vali Loss: 0.9809539 Test Loss: 0.4452737
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4369284
	speed: 0.1964s/iter; left time: 4147.8672s
	iters: 200, epoch: 20 | loss: 0.4662890
	speed: 0.0483s/iter; left time: 1016.0122s
Epoch: 20 cost time: 14.023562908172607
Epoch: 20, Steps: 262 | Train Loss: 0.4423535 Vali Loss: 0.9802390 Test Loss: 0.4451492
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4745190
	speed: 0.2502s/iter; left time: 5219.1194s
	iters: 200, epoch: 21 | loss: 0.4301142
	speed: 0.0422s/iter; left time: 875.9091s
Epoch: 21 cost time: 12.412148714065552
Epoch: 21, Steps: 262 | Train Loss: 0.4421667 Vali Loss: 0.9801285 Test Loss: 0.4453882
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4287302
	speed: 0.1308s/iter; left time: 2693.8838s
	iters: 200, epoch: 22 | loss: 0.3971112
	speed: 0.0389s/iter; left time: 796.4115s
Epoch: 22 cost time: 11.431911945343018
Epoch: 22, Steps: 262 | Train Loss: 0.4423406 Vali Loss: 0.9794484 Test Loss: 0.4452698
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4139026
	speed: 0.1701s/iter; left time: 3459.0571s
	iters: 200, epoch: 23 | loss: 0.4322413
	speed: 0.0411s/iter; left time: 832.3228s
Epoch: 23 cost time: 11.272419929504395
Epoch: 23, Steps: 262 | Train Loss: 0.4424009 Vali Loss: 0.9798291 Test Loss: 0.4452552
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4515765
	speed: 0.1877s/iter; left time: 3768.8107s
	iters: 200, epoch: 24 | loss: 0.4393512
	speed: 0.0405s/iter; left time: 809.5015s
Epoch: 24 cost time: 11.78957724571228
Epoch: 24, Steps: 262 | Train Loss: 0.4421639 Vali Loss: 0.9798419 Test Loss: 0.4452273
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4405308
	speed: 0.1927s/iter; left time: 3818.1258s
	iters: 200, epoch: 25 | loss: 0.4477389
	speed: 0.0592s/iter; left time: 1166.9561s
Epoch: 25 cost time: 13.98650336265564
Epoch: 25, Steps: 262 | Train Loss: 0.4423553 Vali Loss: 0.9809362 Test Loss: 0.4453778
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4348604
	speed: 0.1429s/iter; left time: 2794.0387s
	iters: 200, epoch: 26 | loss: 0.4101586
	speed: 0.0287s/iter; left time: 557.5944s
Epoch: 26 cost time: 8.130500793457031
Epoch: 26, Steps: 262 | Train Loss: 0.4423308 Vali Loss: 0.9798962 Test Loss: 0.4452419
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4403412
	speed: 0.1446s/iter; left time: 2788.9550s
	iters: 200, epoch: 27 | loss: 0.4761236
	speed: 0.0223s/iter; left time: 427.3725s
Epoch: 27 cost time: 7.100297451019287
Epoch: 27, Steps: 262 | Train Loss: 0.4423440 Vali Loss: 0.9806212 Test Loss: 0.4453019
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.44347622990608215, mae:0.4247049391269684, rse:0.6335865259170532, corr:[0.5284702  0.53386605 0.53099835 0.52838784 0.52837086 0.5289423
 0.52823347 0.527085   0.52660817 0.52704895 0.5276697  0.52757686
 0.5269323  0.5256263  0.5237626  0.5217237  0.51974916 0.51786155
 0.5160049  0.51414925 0.512356   0.51052225 0.50837576 0.50589436
 0.50328803 0.5009091  0.49903002 0.4975455  0.49648184 0.49583605
 0.49574542 0.49600628 0.49634907 0.49673685 0.4970248  0.49717292
 0.49694037 0.49635768 0.49587694 0.49553698 0.4953507  0.49506113
 0.4947377  0.49469888 0.4949668  0.49535686 0.49566343 0.4957774
 0.49584016 0.49595493 0.49618286 0.49636084 0.4963461  0.49610895
 0.49590963 0.4959029  0.49618995 0.4963882  0.496361   0.49605167
 0.49575794 0.4956503  0.49565563 0.4956295  0.49545896 0.49539056
 0.49551666 0.49576747 0.49616665 0.49650913 0.49672577 0.4968011
 0.49686834 0.496993   0.49724934 0.49734232 0.4972655  0.49706763
 0.49687386 0.4967338  0.49666858 0.4965912  0.49650472 0.49630213
 0.49607658 0.4959454  0.49590433 0.49578285 0.4955103  0.49534974
 0.49533355 0.4953919  0.49535468 0.49511883 0.49469534 0.49407676
 0.4933637  0.49258274 0.49163988 0.49062955 0.48986346 0.48962122
 0.4899653  0.4907876  0.49192435 0.49314418 0.4942418  0.49507427
 0.4956353  0.49601746 0.49640048 0.49701983 0.49756137 0.49771723
 0.49731526 0.4966708  0.49599734 0.49547482 0.49520242 0.4949595
 0.4946702  0.49414092 0.4934442  0.49287036 0.49239993 0.49194002
 0.49150807 0.49108368 0.49065197 0.4902911  0.49005577 0.48994425
 0.48994377 0.49005687 0.4899657  0.48941168 0.48875454 0.48827362
 0.4881292  0.48831794 0.48847404 0.48834324 0.48801094 0.48779792
 0.4878521  0.48808727 0.48811522 0.48798296 0.4880551  0.48814097
 0.48821902 0.48816592 0.4878381  0.48744056 0.48713145 0.4872463
 0.48765454 0.48808384 0.48817447 0.48809856 0.48808664 0.48832035
 0.48874053 0.48914284 0.48934937 0.48955712 0.48979178 0.49001658
 0.4902083  0.4902421  0.49009654 0.4900986  0.49018103 0.49019247
 0.49013945 0.49001634 0.48995695 0.4900277  0.49006674 0.4900446
 0.48989862 0.48967978 0.48946142 0.48938996 0.48952067 0.48975617
 0.48989967 0.4898502  0.48972186 0.48975554 0.49007505 0.4905179
 0.49071735 0.4905283  0.49000674 0.48924583 0.4885614  0.4881565
 0.48787716 0.48750964 0.48704833 0.486835   0.48663676 0.48636383
 0.48605642 0.48563847 0.4852579  0.484899   0.4843493  0.48360834
 0.48284096 0.48228487 0.48181662 0.48110694 0.48000854 0.47874713
 0.47772053 0.47703966 0.47662053 0.476249   0.4757069  0.4751295
 0.47478512 0.47482035 0.4750012  0.47510043 0.47505248 0.47483003
 0.4745626  0.4745444  0.4746036  0.4744587  0.47402862 0.4734658
 0.47301516 0.47289118 0.47288734 0.47307947 0.47340125 0.47368917
 0.47403833 0.47418517 0.47418487 0.47396654 0.47366667 0.4734845
 0.47354624 0.47370017 0.47391078 0.47411108 0.47422466 0.4742187
 0.47407717 0.47389475 0.47354645 0.4732557  0.4730996  0.47317132
 0.47352204 0.47390878 0.47412482 0.4742489  0.47445366 0.47489703
 0.47548884 0.4757968  0.47578546 0.47571653 0.47585505 0.47610235
 0.47637233 0.4764041  0.47628486 0.47611994 0.47614405 0.47635517
 0.47666416 0.4770542  0.47732776 0.47744387 0.47753528 0.47764456
 0.47782347 0.47786707 0.47775453 0.477504   0.47716677 0.4768096
 0.47627047 0.47555482 0.4747725  0.47397906 0.47341773 0.4731515
 0.47321573 0.47328818 0.47345343 0.47380394 0.47432247 0.47496146
 0.4754583  0.47554934 0.47545344 0.47539663 0.4754129  0.47539175
 0.47529238 0.47499567 0.47446463 0.47387683 0.47342083 0.4731747
 0.47305727 0.4728017  0.47236633 0.4718567  0.4711458  0.4707266
 0.4707951  0.4710334  0.47115916 0.47112963 0.47080114 0.47044128
 0.4702372  0.47017527 0.47000113 0.46959075 0.46896422 0.46841067
 0.46815497 0.46822724 0.46830386 0.46823406 0.46813804 0.46809673
 0.46820074 0.46835995 0.4684207  0.46836826 0.46838313 0.4683436
 0.46823373 0.46812508 0.46817717 0.46838373 0.46861443 0.4688056
 0.4688187  0.46875843 0.46878394 0.46883488 0.4689379  0.46904522
 0.46908998 0.46914783 0.46912274 0.46914348 0.46922016 0.46932024
 0.46927705 0.46917725 0.46902823 0.46899742 0.46912584 0.46928614
 0.469336   0.46939945 0.4695791  0.46985808 0.47028044 0.4706705
 0.47097602 0.4711668  0.47135743 0.47159472 0.47192562 0.47235286
 0.4726948  0.47282076 0.47286174 0.47291595 0.47293705 0.47276783
 0.47231558 0.4716463  0.47110423 0.47064117 0.4701508  0.4697965
 0.46965954 0.46984488 0.47024694 0.4709481  0.47166133 0.47222373
 0.4725194  0.4725951  0.47256586 0.4725985  0.47257322 0.4724694
 0.4723019  0.47209316 0.47189495 0.4716917  0.47135156 0.4708312
 0.47021368 0.46957073 0.46905237 0.468724   0.46855435 0.46844542
 0.4684312  0.46833137 0.46821463 0.46808657 0.46791956 0.46788156
 0.46770868 0.46744937 0.46705517 0.4666247  0.466281   0.46617705
 0.4661224  0.46608546 0.46591002 0.46570596 0.4655619  0.46572176
 0.46598253 0.46624255 0.4663684  0.46636665 0.4661195  0.46588266
 0.46564555 0.4655109  0.46554163 0.4656435  0.46568054 0.4656671
 0.46567392 0.4657893  0.46588713 0.465958   0.46585807 0.46560913
 0.4655417  0.46583748 0.46635935 0.4668827  0.46709564 0.46715954
 0.4670827  0.4669077  0.4667562  0.46675986 0.4667575  0.46672934
 0.4667304  0.46682194 0.46694592 0.46712822 0.46729    0.4673474
 0.46739277 0.46752173 0.46776435 0.467891   0.46790656 0.46789035
 0.46783873 0.46778297 0.4676047  0.4671026  0.4661893  0.46504006
 0.46395195 0.46311572 0.4623875  0.46147174 0.46029803 0.45924613
 0.4586666  0.45839638 0.45838824 0.45827663 0.45798677 0.45769683
 0.457673   0.45772275 0.45768413 0.4576132  0.45748088 0.45719972
 0.45680845 0.45641437 0.45597658 0.4554297  0.4548675  0.45433226
 0.45403644 0.45390987 0.45375144 0.4534861  0.4530624  0.45276007
 0.45270783 0.4528339  0.4528683  0.45277935 0.45254523 0.45233646
 0.45224628 0.4523527  0.4524203  0.45216805 0.4516971  0.45131138
 0.4510866  0.4510377  0.45095116 0.45073992 0.45058674 0.4505918
 0.45067644 0.45077848 0.45076486 0.45064735 0.4505602  0.4505718
 0.45069292 0.45083278 0.45109293 0.45127463 0.45130932 0.4512185
 0.4511253  0.45097247 0.4509279  0.45091522 0.45087147 0.45079193
 0.45071113 0.450718   0.45085827 0.45105982 0.45125103 0.45145702
 0.45154396 0.4514294  0.45126036 0.45129427 0.451461   0.45166522
 0.4518848  0.4521091  0.4522361  0.45231098 0.4523344  0.45239416
 0.45245537 0.45269743 0.45291552 0.45303616 0.45305663 0.45307732
 0.45310423 0.4530249  0.45280337 0.45235676 0.45163217 0.45058358
 0.4491776  0.4476108  0.44614172 0.44483402 0.4438724  0.44337088
 0.44324473 0.44329166 0.44344494 0.44364655 0.4439989  0.4444277
 0.4448045  0.4449393  0.44486883 0.4447309  0.44453788 0.4442481
 0.4438189  0.44337717 0.44304574 0.44275877 0.44247872 0.4420758
 0.44168872 0.4412751  0.4408884  0.4407681  0.4408121  0.4408756
 0.4408388  0.44071484 0.4405331  0.4404357  0.44046816 0.4405559
 0.44041437 0.44001192 0.4396621  0.43939838 0.43924502 0.4390748
 0.43898836 0.43899786 0.43909937 0.43926805 0.43944508 0.43953532
 0.43943614 0.43926242 0.43914112 0.43908727 0.4391488  0.4392165
 0.43911377 0.4388654  0.43858343 0.43837106 0.438356   0.43833214
 0.43825752 0.43828997 0.4383367  0.4383218  0.4383209  0.43820274
 0.43814114 0.4382867  0.438592   0.43893787 0.4391949  0.4394352
 0.43969592 0.43987915 0.44006523 0.440328   0.440551   0.4406939
 0.44074824 0.44080755 0.44088683 0.44095615 0.44103453 0.44112724
 0.44122446 0.44137347 0.44155207 0.44166034 0.44179744 0.44199657
 0.44216952 0.44217876 0.44185823 0.44122076 0.44045168 0.4397403
 0.43907112 0.43832186 0.43738627 0.43637052 0.43563437 0.43532237
 0.4353558  0.43562466 0.43599346 0.43650237 0.43713695 0.43789423
 0.43847296 0.43859488 0.43851158 0.43838894 0.43812644 0.43753776
 0.43668708 0.4358649  0.43541107 0.4352904  0.43512797 0.43468148
 0.43385875 0.4328765  0.4322698  0.43214774 0.43214715 0.43185174
 0.43130887 0.43084866 0.4308366  0.43106386 0.430838   0.4301446
 0.4295048  0.4293859  0.4297531  0.42977744 0.42913738 0.42858455
 0.4290911  0.43019652 0.4302694  0.4293982  0.4301612  0.43374443]
