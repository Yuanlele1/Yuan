Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=66, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=66, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11708928.0
params:  13266.0
Trainable parameters:  13266
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5350568
	speed: 0.0220s/iter; left time: 573.1529s
	iters: 200, epoch: 1 | loss: 0.4664578
	speed: 0.0192s/iter; left time: 496.4280s
Epoch: 1 cost time: 5.0133421421051025
Epoch: 1, Steps: 261 | Train Loss: 0.5733830 Vali Loss: 1.0474523 Test Loss: 0.4647579
Validation loss decreased (inf --> 1.047452).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4636345
	speed: 0.0685s/iter; left time: 1764.3973s
	iters: 200, epoch: 2 | loss: 0.3998970
	speed: 0.0148s/iter; left time: 379.5531s
Epoch: 2 cost time: 4.575714349746704
Epoch: 2, Steps: 261 | Train Loss: 0.4346893 Vali Loss: 0.9901864 Test Loss: 0.4317898
Validation loss decreased (1.047452 --> 0.990186).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4476746
	speed: 0.0725s/iter; left time: 1846.0205s
	iters: 200, epoch: 3 | loss: 0.4842089
	speed: 0.0154s/iter; left time: 390.9198s
Epoch: 3 cost time: 4.513596057891846
Epoch: 3, Steps: 261 | Train Loss: 0.4210560 Vali Loss: 0.9759641 Test Loss: 0.4267730
Validation loss decreased (0.990186 --> 0.975964).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3611655
	speed: 0.0706s/iter; left time: 1780.5163s
	iters: 200, epoch: 4 | loss: 0.4020199
	speed: 0.0153s/iter; left time: 385.5373s
Epoch: 4 cost time: 4.538899898529053
Epoch: 4, Steps: 261 | Train Loss: 0.4169708 Vali Loss: 0.9698067 Test Loss: 0.4257233
Validation loss decreased (0.975964 --> 0.969807).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3816502
	speed: 0.0731s/iter; left time: 1824.8073s
	iters: 200, epoch: 5 | loss: 0.4016531
	speed: 0.0159s/iter; left time: 395.4687s
Epoch: 5 cost time: 4.698029041290283
Epoch: 5, Steps: 261 | Train Loss: 0.4153481 Vali Loss: 0.9662618 Test Loss: 0.4255358
Validation loss decreased (0.969807 --> 0.966262).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3910047
	speed: 0.0713s/iter; left time: 1761.5488s
	iters: 200, epoch: 6 | loss: 0.4277367
	speed: 0.0151s/iter; left time: 370.3576s
Epoch: 6 cost time: 4.4839301109313965
Epoch: 6, Steps: 261 | Train Loss: 0.4146421 Vali Loss: 0.9645315 Test Loss: 0.4255738
Validation loss decreased (0.966262 --> 0.964531).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4105150
	speed: 0.0729s/iter; left time: 1780.1647s
	iters: 200, epoch: 7 | loss: 0.3860808
	speed: 0.0150s/iter; left time: 365.9877s
Epoch: 7 cost time: 4.563778400421143
Epoch: 7, Steps: 261 | Train Loss: 0.4141151 Vali Loss: 0.9634013 Test Loss: 0.4259032
Validation loss decreased (0.964531 --> 0.963401).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4170008
	speed: 0.0695s/iter; left time: 1679.8753s
	iters: 200, epoch: 8 | loss: 0.4242118
	speed: 0.0159s/iter; left time: 381.6518s
Epoch: 8 cost time: 4.909804582595825
Epoch: 8, Steps: 261 | Train Loss: 0.4139435 Vali Loss: 0.9621888 Test Loss: 0.4259017
Validation loss decreased (0.963401 --> 0.962189).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4213168
	speed: 0.0759s/iter; left time: 1814.0485s
	iters: 200, epoch: 9 | loss: 0.4278040
	speed: 0.0161s/iter; left time: 383.9452s
Epoch: 9 cost time: 4.743535757064819
Epoch: 9, Steps: 261 | Train Loss: 0.4138115 Vali Loss: 0.9613360 Test Loss: 0.4264760
Validation loss decreased (0.962189 --> 0.961336).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4199210
	speed: 0.0686s/iter; left time: 1623.2331s
	iters: 200, epoch: 10 | loss: 0.4437100
	speed: 0.0150s/iter; left time: 352.3966s
Epoch: 10 cost time: 4.4893786907196045
Epoch: 10, Steps: 261 | Train Loss: 0.4136524 Vali Loss: 0.9614749 Test Loss: 0.4265167
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3997615
	speed: 0.0695s/iter; left time: 1625.6014s
	iters: 200, epoch: 11 | loss: 0.4277470
	speed: 0.0147s/iter; left time: 342.3946s
Epoch: 11 cost time: 4.586909770965576
Epoch: 11, Steps: 261 | Train Loss: 0.4136893 Vali Loss: 0.9621477 Test Loss: 0.4267073
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4055804
	speed: 0.0673s/iter; left time: 1555.8280s
	iters: 200, epoch: 12 | loss: 0.4385839
	speed: 0.0148s/iter; left time: 341.2254s
Epoch: 12 cost time: 4.392618656158447
Epoch: 12, Steps: 261 | Train Loss: 0.4135191 Vali Loss: 0.9609073 Test Loss: 0.4263964
Validation loss decreased (0.961336 --> 0.960907).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4212970
	speed: 0.0690s/iter; left time: 1577.0330s
	iters: 200, epoch: 13 | loss: 0.4128838
	speed: 0.0149s/iter; left time: 339.4273s
Epoch: 13 cost time: 4.490751028060913
Epoch: 13, Steps: 261 | Train Loss: 0.4135231 Vali Loss: 0.9603667 Test Loss: 0.4264602
Validation loss decreased (0.960907 --> 0.960367).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4300067
	speed: 0.0693s/iter; left time: 1567.6082s
	iters: 200, epoch: 14 | loss: 0.4200814
	speed: 0.0144s/iter; left time: 323.0412s
Epoch: 14 cost time: 4.321579933166504
Epoch: 14, Steps: 261 | Train Loss: 0.4135192 Vali Loss: 0.9602771 Test Loss: 0.4265484
Validation loss decreased (0.960367 --> 0.960277).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4593374
	speed: 0.0714s/iter; left time: 1594.5305s
	iters: 200, epoch: 15 | loss: 0.3959639
	speed: 0.0153s/iter; left time: 339.8374s
Epoch: 15 cost time: 4.468149900436401
Epoch: 15, Steps: 261 | Train Loss: 0.4134682 Vali Loss: 0.9609137 Test Loss: 0.4265325
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3762142
	speed: 0.0689s/iter; left time: 1521.0215s
	iters: 200, epoch: 16 | loss: 0.4419270
	speed: 0.0150s/iter; left time: 329.1750s
Epoch: 16 cost time: 4.444460153579712
Epoch: 16, Steps: 261 | Train Loss: 0.4134827 Vali Loss: 0.9599221 Test Loss: 0.4266866
Validation loss decreased (0.960277 --> 0.959922).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4206626
	speed: 0.0727s/iter; left time: 1586.1111s
	iters: 200, epoch: 17 | loss: 0.4030373
	speed: 0.0153s/iter; left time: 333.2277s
Epoch: 17 cost time: 4.565158128738403
Epoch: 17, Steps: 261 | Train Loss: 0.4134766 Vali Loss: 0.9604071 Test Loss: 0.4268219
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3863653
	speed: 0.0707s/iter; left time: 1525.0035s
	iters: 200, epoch: 18 | loss: 0.4286672
	speed: 0.0160s/iter; left time: 343.5834s
Epoch: 18 cost time: 4.824054718017578
Epoch: 18, Steps: 261 | Train Loss: 0.4133879 Vali Loss: 0.9603825 Test Loss: 0.4266496
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4059595
	speed: 0.0741s/iter; left time: 1578.8159s
	iters: 200, epoch: 19 | loss: 0.4259372
	speed: 0.0149s/iter; left time: 316.7645s
Epoch: 19 cost time: 4.564634323120117
Epoch: 19, Steps: 261 | Train Loss: 0.4133118 Vali Loss: 0.9599518 Test Loss: 0.4266755
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4531075
	speed: 0.0683s/iter; left time: 1437.7869s
	iters: 200, epoch: 20 | loss: 0.3674243
	speed: 0.0147s/iter; left time: 308.5703s
Epoch: 20 cost time: 4.44708776473999
Epoch: 20, Steps: 261 | Train Loss: 0.4133225 Vali Loss: 0.9598057 Test Loss: 0.4267766
Validation loss decreased (0.959922 --> 0.959806).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4378633
	speed: 0.0709s/iter; left time: 1474.0978s
	iters: 200, epoch: 21 | loss: 0.3840960
	speed: 0.0153s/iter; left time: 315.8652s
Epoch: 21 cost time: 4.724200487136841
Epoch: 21, Steps: 261 | Train Loss: 0.4133212 Vali Loss: 0.9602835 Test Loss: 0.4265870
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4158383
	speed: 0.0731s/iter; left time: 1499.5470s
	iters: 200, epoch: 22 | loss: 0.4170689
	speed: 0.0153s/iter; left time: 312.6260s
Epoch: 22 cost time: 4.576767921447754
Epoch: 22, Steps: 261 | Train Loss: 0.4133576 Vali Loss: 0.9611476 Test Loss: 0.4270246
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3844705
	speed: 0.0689s/iter; left time: 1395.2234s
	iters: 200, epoch: 23 | loss: 0.4327304
	speed: 0.0160s/iter; left time: 322.2268s
Epoch: 23 cost time: 4.717006683349609
Epoch: 23, Steps: 261 | Train Loss: 0.4132037 Vali Loss: 0.9601384 Test Loss: 0.4267374
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3720636
	speed: 0.0701s/iter; left time: 1402.0878s
	iters: 200, epoch: 24 | loss: 0.3504445
	speed: 0.0150s/iter; left time: 299.4497s
Epoch: 24 cost time: 4.590856075286865
Epoch: 24, Steps: 261 | Train Loss: 0.4132900 Vali Loss: 0.9608496 Test Loss: 0.4267406
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4342704
	speed: 0.0719s/iter; left time: 1418.2400s
	iters: 200, epoch: 25 | loss: 0.4512751
	speed: 0.0147s/iter; left time: 287.7292s
Epoch: 25 cost time: 4.460488796234131
Epoch: 25, Steps: 261 | Train Loss: 0.4130950 Vali Loss: 0.9599065 Test Loss: 0.4268214
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3981625
	speed: 0.0704s/iter; left time: 1370.9057s
	iters: 200, epoch: 26 | loss: 0.4092616
	speed: 0.0150s/iter; left time: 289.7607s
Epoch: 26 cost time: 4.461509704589844
Epoch: 26, Steps: 261 | Train Loss: 0.4132324 Vali Loss: 0.9600480 Test Loss: 0.4267226
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4179176
	speed: 0.0843s/iter; left time: 1618.8719s
	iters: 200, epoch: 27 | loss: 0.4557210
	speed: 0.0168s/iter; left time: 321.8370s
Epoch: 27 cost time: 5.460202217102051
Epoch: 27, Steps: 261 | Train Loss: 0.4133147 Vali Loss: 0.9603525 Test Loss: 0.4268647
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3516119
	speed: 0.0813s/iter; left time: 1540.3372s
	iters: 200, epoch: 28 | loss: 0.3922170
	speed: 0.0155s/iter; left time: 291.4735s
Epoch: 28 cost time: 4.507112264633179
Epoch: 28, Steps: 261 | Train Loss: 0.4133089 Vali Loss: 0.9598668 Test Loss: 0.4267338
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4082293
	speed: 0.0739s/iter; left time: 1381.8372s
	iters: 200, epoch: 29 | loss: 0.4449469
	speed: 0.0157s/iter; left time: 291.9342s
Epoch: 29 cost time: 4.692082405090332
Epoch: 29, Steps: 261 | Train Loss: 0.4132942 Vali Loss: 0.9597258 Test Loss: 0.4267678
Validation loss decreased (0.959806 --> 0.959726).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3929467
	speed: 0.0718s/iter; left time: 1323.1495s
	iters: 200, epoch: 30 | loss: 0.3924379
	speed: 0.0158s/iter; left time: 288.9323s
Epoch: 30 cost time: 4.755726337432861
Epoch: 30, Steps: 261 | Train Loss: 0.4132498 Vali Loss: 0.9603390 Test Loss: 0.4268367
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3941125
	speed: 0.0695s/iter; left time: 1263.1916s
	iters: 200, epoch: 31 | loss: 0.4068542
	speed: 0.0156s/iter; left time: 282.7324s
Epoch: 31 cost time: 4.541434288024902
Epoch: 31, Steps: 261 | Train Loss: 0.4132699 Vali Loss: 0.9593400 Test Loss: 0.4268788
Validation loss decreased (0.959726 --> 0.959340).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4184027
	speed: 0.0694s/iter; left time: 1243.8071s
	iters: 200, epoch: 32 | loss: 0.4235220
	speed: 0.0147s/iter; left time: 261.1032s
Epoch: 32 cost time: 4.523821830749512
Epoch: 32, Steps: 261 | Train Loss: 0.4132412 Vali Loss: 0.9603769 Test Loss: 0.4269074
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4005381
	speed: 0.0703s/iter; left time: 1240.7128s
	iters: 200, epoch: 33 | loss: 0.3952799
	speed: 0.0156s/iter; left time: 274.2113s
Epoch: 33 cost time: 4.615637540817261
Epoch: 33, Steps: 261 | Train Loss: 0.4131735 Vali Loss: 0.9600619 Test Loss: 0.4268593
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3974108
	speed: 0.0697s/iter; left time: 1212.8123s
	iters: 200, epoch: 34 | loss: 0.3933000
	speed: 0.0151s/iter; left time: 260.6059s
Epoch: 34 cost time: 4.5558435916900635
Epoch: 34, Steps: 261 | Train Loss: 0.4132823 Vali Loss: 0.9597448 Test Loss: 0.4269160
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3995705
	speed: 0.0679s/iter; left time: 1162.9365s
	iters: 200, epoch: 35 | loss: 0.4131147
	speed: 0.0154s/iter; left time: 261.9600s
Epoch: 35 cost time: 4.517600774765015
Epoch: 35, Steps: 261 | Train Loss: 0.4130631 Vali Loss: 0.9603668 Test Loss: 0.4269457
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3942019
	speed: 0.0687s/iter; left time: 1158.0059s
	iters: 200, epoch: 36 | loss: 0.3881986
	speed: 0.0180s/iter; left time: 301.0993s
Epoch: 36 cost time: 4.982550621032715
Epoch: 36, Steps: 261 | Train Loss: 0.4132019 Vali Loss: 0.9595006 Test Loss: 0.4269463
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4500991
	speed: 0.0706s/iter; left time: 1172.2854s
	iters: 200, epoch: 37 | loss: 0.4014719
	speed: 0.0184s/iter; left time: 304.1309s
Epoch: 37 cost time: 5.195718050003052
Epoch: 37, Steps: 261 | Train Loss: 0.4132032 Vali Loss: 0.9597846 Test Loss: 0.4267498
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4051745
	speed: 0.0750s/iter; left time: 1225.5622s
	iters: 200, epoch: 38 | loss: 0.4196186
	speed: 0.0152s/iter; left time: 247.1837s
Epoch: 38 cost time: 4.485708475112915
Epoch: 38, Steps: 261 | Train Loss: 0.4131353 Vali Loss: 0.9602594 Test Loss: 0.4268391
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4525562
	speed: 0.0700s/iter; left time: 1125.9000s
	iters: 200, epoch: 39 | loss: 0.4149546
	speed: 0.0150s/iter; left time: 239.8328s
Epoch: 39 cost time: 4.423665285110474
Epoch: 39, Steps: 261 | Train Loss: 0.4131801 Vali Loss: 0.9601302 Test Loss: 0.4268238
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4139134
	speed: 0.0670s/iter; left time: 1060.7110s
	iters: 200, epoch: 40 | loss: 0.4626955
	speed: 0.0137s/iter; left time: 215.2253s
Epoch: 40 cost time: 4.081762313842773
Epoch: 40, Steps: 261 | Train Loss: 0.4132291 Vali Loss: 0.9590749 Test Loss: 0.4268899
Validation loss decreased (0.959340 --> 0.959075).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4256049
	speed: 0.0653s/iter; left time: 1015.7299s
	iters: 200, epoch: 41 | loss: 0.3901647
	speed: 0.0136s/iter; left time: 210.6773s
Epoch: 41 cost time: 4.102707862854004
Epoch: 41, Steps: 261 | Train Loss: 0.4131652 Vali Loss: 0.9597036 Test Loss: 0.4268573
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4293919
	speed: 0.0657s/iter; left time: 1004.4730s
	iters: 200, epoch: 42 | loss: 0.4095079
	speed: 0.0136s/iter; left time: 206.3070s
Epoch: 42 cost time: 4.179176568984985
Epoch: 42, Steps: 261 | Train Loss: 0.4131905 Vali Loss: 0.9589946 Test Loss: 0.4268643
Validation loss decreased (0.959075 --> 0.958995).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4023165
	speed: 0.0644s/iter; left time: 969.1683s
	iters: 200, epoch: 43 | loss: 0.4293749
	speed: 0.0137s/iter; left time: 205.0094s
Epoch: 43 cost time: 4.124238729476929
Epoch: 43, Steps: 261 | Train Loss: 0.4131524 Vali Loss: 0.9583454 Test Loss: 0.4269013
Validation loss decreased (0.958995 --> 0.958345).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3624162
	speed: 0.0635s/iter; left time: 937.8346s
	iters: 200, epoch: 44 | loss: 0.4046514
	speed: 0.0138s/iter; left time: 202.4677s
Epoch: 44 cost time: 4.131346940994263
Epoch: 44, Steps: 261 | Train Loss: 0.4131755 Vali Loss: 0.9608107 Test Loss: 0.4269035
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4184101
	speed: 0.0653s/iter; left time: 947.3391s
	iters: 200, epoch: 45 | loss: 0.4126255
	speed: 0.0148s/iter; left time: 213.7440s
Epoch: 45 cost time: 4.300816059112549
Epoch: 45, Steps: 261 | Train Loss: 0.4131156 Vali Loss: 0.9600227 Test Loss: 0.4268673
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.3964389
	speed: 0.0656s/iter; left time: 934.8276s
	iters: 200, epoch: 46 | loss: 0.3940744
	speed: 0.0135s/iter; left time: 190.9625s
Epoch: 46 cost time: 4.172330856323242
Epoch: 46, Steps: 261 | Train Loss: 0.4131735 Vali Loss: 0.9594232 Test Loss: 0.4269391
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4251648
	speed: 0.0653s/iter; left time: 913.4023s
	iters: 200, epoch: 47 | loss: 0.4124253
	speed: 0.0136s/iter; left time: 188.6172s
Epoch: 47 cost time: 4.107616662979126
Epoch: 47, Steps: 261 | Train Loss: 0.4131914 Vali Loss: 0.9595859 Test Loss: 0.4269228
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4202091
	speed: 0.0652s/iter; left time: 894.7841s
	iters: 200, epoch: 48 | loss: 0.3989139
	speed: 0.0143s/iter; left time: 194.8832s
Epoch: 48 cost time: 4.434217214584351
Epoch: 48, Steps: 261 | Train Loss: 0.4132141 Vali Loss: 0.9599096 Test Loss: 0.4268838
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4187115
	speed: 0.0695s/iter; left time: 936.1980s
	iters: 200, epoch: 49 | loss: 0.4168066
	speed: 0.0139s/iter; left time: 185.2237s
Epoch: 49 cost time: 4.233652591705322
Epoch: 49, Steps: 261 | Train Loss: 0.4131582 Vali Loss: 0.9595997 Test Loss: 0.4268359
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.4104174
	speed: 0.0634s/iter; left time: 837.2072s
	iters: 200, epoch: 50 | loss: 0.4123978
	speed: 0.0139s/iter; left time: 181.6047s
Epoch: 50 cost time: 4.189847469329834
Epoch: 50, Steps: 261 | Train Loss: 0.4130756 Vali Loss: 0.9606627 Test Loss: 0.4268990
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4164720
	speed: 0.0640s/iter; left time: 828.9162s
	iters: 200, epoch: 51 | loss: 0.4367048
	speed: 0.0151s/iter; left time: 194.6576s
Epoch: 51 cost time: 4.323493003845215
Epoch: 51, Steps: 261 | Train Loss: 0.4131262 Vali Loss: 0.9598035 Test Loss: 0.4268811
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4451566
	speed: 0.0665s/iter; left time: 843.2629s
	iters: 200, epoch: 52 | loss: 0.3998388
	speed: 0.0141s/iter; left time: 177.3265s
Epoch: 52 cost time: 4.279919385910034
Epoch: 52, Steps: 261 | Train Loss: 0.4132136 Vali Loss: 0.9597911 Test Loss: 0.4269173
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4020197
	speed: 0.0722s/iter; left time: 897.5918s
	iters: 200, epoch: 53 | loss: 0.4021636
	speed: 0.0153s/iter; left time: 189.1743s
Epoch: 53 cost time: 4.630312919616699
Epoch: 53, Steps: 261 | Train Loss: 0.4131295 Vali Loss: 0.9591843 Test Loss: 0.4269062
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4074990
	speed: 0.0713s/iter; left time: 868.1292s
	iters: 200, epoch: 54 | loss: 0.4023615
	speed: 0.0153s/iter; left time: 184.8942s
Epoch: 54 cost time: 4.538998365402222
Epoch: 54, Steps: 261 | Train Loss: 0.4131605 Vali Loss: 0.9599875 Test Loss: 0.4268917
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.4023451
	speed: 0.0721s/iter; left time: 857.9588s
	iters: 200, epoch: 55 | loss: 0.4386353
	speed: 0.0150s/iter; left time: 177.6497s
Epoch: 55 cost time: 4.558053493499756
Epoch: 55, Steps: 261 | Train Loss: 0.4131571 Vali Loss: 0.9595326 Test Loss: 0.4269260
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4086184
	speed: 0.0670s/iter; left time: 780.8143s
	iters: 200, epoch: 56 | loss: 0.4246650
	speed: 0.0153s/iter; left time: 176.6820s
Epoch: 56 cost time: 4.500640630722046
Epoch: 56, Steps: 261 | Train Loss: 0.4131359 Vali Loss: 0.9600990 Test Loss: 0.4268838
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.4177625
	speed: 0.0698s/iter; left time: 794.4443s
	iters: 200, epoch: 57 | loss: 0.4186617
	speed: 0.0173s/iter; left time: 195.7458s
Epoch: 57 cost time: 4.910141706466675
Epoch: 57, Steps: 261 | Train Loss: 0.4131715 Vali Loss: 0.9596752 Test Loss: 0.4268920
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.3891805
	speed: 0.0709s/iter; left time: 788.9489s
	iters: 200, epoch: 58 | loss: 0.4765375
	speed: 0.0195s/iter; left time: 215.0029s
Epoch: 58 cost time: 7.031035661697388
Epoch: 58, Steps: 261 | Train Loss: 0.4131526 Vali Loss: 0.9593931 Test Loss: 0.4269018
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.4381299
	speed: 0.1019s/iter; left time: 1106.7894s
	iters: 200, epoch: 59 | loss: 0.4131715
	speed: 0.0198s/iter; left time: 212.6387s
Epoch: 59 cost time: 4.910317897796631
Epoch: 59, Steps: 261 | Train Loss: 0.4130801 Vali Loss: 0.9601486 Test Loss: 0.4268876
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.4075433
	speed: 0.0654s/iter; left time: 693.4508s
	iters: 200, epoch: 60 | loss: 0.4010723
	speed: 0.0135s/iter; left time: 141.7468s
Epoch: 60 cost time: 4.232583522796631
Epoch: 60, Steps: 261 | Train Loss: 0.4131889 Vali Loss: 0.9601961 Test Loss: 0.4269197
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.4394758
	speed: 0.0675s/iter; left time: 698.3946s
	iters: 200, epoch: 61 | loss: 0.4350409
	speed: 0.0143s/iter; left time: 146.1814s
Epoch: 61 cost time: 4.314582824707031
Epoch: 61, Steps: 261 | Train Loss: 0.4132019 Vali Loss: 0.9597610 Test Loss: 0.4269054
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.4119208
	speed: 0.0636s/iter; left time: 641.0520s
	iters: 200, epoch: 62 | loss: 0.3925338
	speed: 0.0154s/iter; left time: 153.4154s
Epoch: 62 cost time: 4.480058431625366
Epoch: 62, Steps: 261 | Train Loss: 0.4129594 Vali Loss: 0.9583877 Test Loss: 0.4269372
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.4538744
	speed: 0.0670s/iter; left time: 657.9431s
	iters: 200, epoch: 63 | loss: 0.4057871
	speed: 0.0136s/iter; left time: 131.7392s
Epoch: 63 cost time: 4.301039934158325
Epoch: 63, Steps: 261 | Train Loss: 0.4131053 Vali Loss: 0.9596475 Test Loss: 0.4269322
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.42657098174095154, mae:0.41564345359802246, rse:0.6213930249214172, corr:[0.5289376  0.53215617 0.5319214  0.53157175 0.5318805  0.53210765
 0.53182703 0.531553   0.5317635  0.5323923  0.53320616 0.5336576
 0.53390473 0.53400147 0.533623   0.53254145 0.53121805 0.5301872
 0.52921474 0.5281218  0.52679265 0.52566236 0.5246915  0.5238755
 0.5229133  0.521609   0.5202949  0.51926845 0.51854897 0.5179701
 0.51780814 0.5181991  0.51886964 0.5194487  0.51964676 0.5196561
 0.51943713 0.5189617  0.51848876 0.5182062  0.5181194  0.51798624
 0.51765287 0.5172873  0.5170166  0.5169277  0.51701444 0.51695794
 0.51664543 0.5163619  0.51621693 0.51615715 0.5161609  0.51607484
 0.5159677  0.51600736 0.51611745 0.5162011  0.5162264  0.5160528
 0.51597446 0.51592314 0.51586354 0.51558113 0.5152445  0.5150842
 0.51515114 0.51539207 0.5159148  0.51639855 0.5166959  0.5167811
 0.5168766  0.5170454  0.51714534 0.51706016 0.5168887  0.5167871
 0.51677996 0.5167605  0.5166219  0.5163529  0.5159981  0.5156142
 0.515397   0.5151956  0.51500857 0.51472455 0.51438785 0.5142649
 0.51441485 0.5146546  0.51480937 0.514825   0.5146484  0.5142712
 0.51379114 0.5132509  0.51261926 0.5119464  0.5114981  0.5114168
 0.5116076  0.51173776 0.5118531  0.5120048  0.51217484 0.51259893
 0.5130352  0.51343757 0.51357025 0.51346993 0.5132296  0.5131394
 0.51316905 0.5132257  0.51315224 0.5129881  0.5128197  0.5128902
 0.5130446  0.51299393 0.5127295  0.5125005  0.5123466  0.51218224
 0.511972   0.5117201  0.51143783 0.51124424 0.51121247 0.5112646
 0.5113131  0.51115316 0.51087266 0.5105433  0.51028955 0.51003796
 0.5098013  0.5095309  0.50925267 0.5091555  0.5092298  0.5093915
 0.5094915  0.50948304 0.5093751  0.5093007  0.50936407 0.5093859
 0.5093582  0.509408   0.5094427  0.5094437  0.50944537 0.50949913
 0.50952625 0.5095017  0.5094076  0.50937235 0.50949013 0.50967336
 0.5098631  0.510005   0.5102463  0.5106152  0.5110337  0.51148385
 0.5118276  0.5120274  0.5121447  0.5122671  0.5123213  0.5123465
 0.5123429  0.5123472  0.5122835  0.5121345  0.5118277  0.5115059
 0.51123536 0.5111497  0.51109093 0.51105356 0.5110201  0.5110278
 0.5110474  0.5110745  0.51105064 0.5109608  0.5108063  0.5105952
 0.5102955  0.5099446  0.50948274 0.5088523  0.50817925 0.5076613
 0.5072135  0.50677544 0.5064811  0.50632745 0.5061879  0.5060218
 0.50581086 0.5054537  0.5051242  0.50478536 0.5044327  0.50405926
 0.5035676  0.5031024  0.50265765 0.5022896  0.5018923  0.5015451
 0.5012603  0.5010046  0.50073004 0.50055236 0.500366   0.5001745
 0.50010943 0.50010437 0.5001297  0.5001312  0.5001348  0.50005066
 0.49995232 0.49988037 0.49974155 0.4994761  0.49920008 0.49895385
 0.49885097 0.49881396 0.4986394  0.4985381  0.49854326 0.49865794
 0.49878597 0.49882385 0.49878412 0.4986511  0.4985015  0.49850464
 0.49863195 0.4987792  0.49886706 0.49888328 0.4987566  0.498662
 0.49854526 0.49851856 0.49847004 0.49839693 0.4982471  0.49825615
 0.49836722 0.4985552  0.49874285 0.49888524 0.49894342 0.49917212
 0.49952692 0.49985754 0.49999177 0.5000819  0.50015485 0.5001752
 0.5002523  0.5002958  0.50022733 0.50010705 0.4999493  0.4998944
 0.49982542 0.49977824 0.49959296 0.49943337 0.49929836 0.4992167
 0.4992434  0.49921322 0.49909118 0.4988228  0.49840677 0.49786383
 0.4971777  0.4965201  0.49596494 0.49533483 0.49478993 0.49438322
 0.4940835  0.4936783  0.49330467 0.49295962 0.4927155  0.4924751
 0.49214953 0.49173084 0.49131304 0.49100727 0.49080086 0.4906739
 0.49061865 0.49052367 0.4904288  0.4903686  0.49036852 0.4903849
 0.49045768 0.49051875 0.49051675 0.490584   0.4906298  0.490622
 0.49054748 0.49041653 0.49024433 0.49008396 0.4899418  0.48978877
 0.4896294  0.48946702 0.48927036 0.4890951  0.48885784 0.4886081
 0.48836082 0.48819643 0.4881073  0.4880491  0.48796415 0.48782185
 0.4876312  0.48750237 0.48749736 0.48749882 0.48755625 0.48762655
 0.48760217 0.48755127 0.48754913 0.48755607 0.4876331  0.48774755
 0.4877504  0.48772097 0.48769042 0.48763835 0.48763222 0.4876773
 0.48769584 0.48771572 0.48783413 0.4880236  0.48822722 0.48841053
 0.4884903  0.4884959  0.4884797  0.48848656 0.48851925 0.48855963
 0.4885633  0.48858213 0.4886151  0.48855922 0.48847234 0.48839965
 0.48840252 0.4884502  0.48855013 0.4886959  0.48883876 0.4889673
 0.48907626 0.48913866 0.4891629  0.48914903 0.48907343 0.4888888
 0.4884995  0.48796862 0.48738334 0.48675928 0.4861828  0.48579994
 0.48550817 0.48517126 0.48492178 0.4848699  0.48491058 0.48495293
 0.48480093 0.48456874 0.4843261  0.4841555  0.484034   0.48393735
 0.48386157 0.4837481  0.4836133  0.4835428  0.48360524 0.4837697
 0.4838784  0.48373985 0.4834962  0.48336247 0.48333514 0.48335752
 0.48338762 0.4833301  0.48325744 0.48312864 0.48299885 0.4828726
 0.48271075 0.48254412 0.4822855  0.48204035 0.48183244 0.48169306
 0.4815439  0.48144102 0.4812553  0.48109376 0.48101184 0.48112386
 0.48113006 0.4809838  0.48083037 0.4807158  0.480608   0.48054877
 0.48048556 0.48044595 0.4804358  0.4804384  0.4804795  0.48056537
 0.48054796 0.48049387 0.48039803 0.4803098  0.48027995 0.48031145
 0.48039892 0.48053735 0.48067564 0.48079553 0.4809078  0.48106655
 0.48113665 0.48119986 0.4812432  0.48137543 0.4815349  0.48163897
 0.48169073 0.48176888 0.4818207  0.48193413 0.4820272  0.4820343
 0.4819478  0.48179817 0.4816666  0.48161578 0.48162577 0.48165342
 0.48160654 0.48151    0.4813285  0.48096788 0.4803666  0.4795573
 0.4786248  0.4777725  0.47704333 0.4763476  0.47565427 0.47498012
 0.47438955 0.4738694  0.47349507 0.47324303 0.47289333 0.47235474
 0.4717853  0.47135285 0.47103155 0.47072682 0.47047824 0.4701736
 0.46989965 0.46985874 0.4700373  0.47023678 0.47045928 0.47071502
 0.47102106 0.47120753 0.471291   0.4713098  0.47127292 0.47125205
 0.47125012 0.4712554  0.47125438 0.47126347 0.4711968  0.47117594
 0.47118944 0.4711936  0.47116095 0.4708962  0.47049072 0.47016692
 0.47001925 0.470012   0.4700104  0.46988237 0.46966097 0.4695087
 0.4694359  0.46939296 0.46931902 0.4691581  0.46894842 0.4688132
 0.46881518 0.46887305 0.46893802 0.46892524 0.46884394 0.46876827
 0.46875688 0.46872315 0.46860403 0.46841606 0.46829283 0.46827945
 0.46832007 0.46838346 0.46848926 0.46857196 0.46858943 0.468689
 0.4687935  0.46895975 0.4691064  0.46923122 0.4693345  0.46937704
 0.4694528  0.46962035 0.46977702 0.46985966 0.4698529  0.46981636
 0.46975288 0.46983173 0.46989897 0.4699517  0.46996772 0.46994475
 0.4699448  0.4698881  0.46977478 0.46950987 0.46905205 0.4683916
 0.46759185 0.46675655 0.46599573 0.4651672  0.46439156 0.4638059
 0.46339977 0.4630147  0.46273044 0.46254417 0.46231586 0.4619666
 0.4615522  0.46110767 0.4607254  0.46042326 0.4601352  0.45992443
 0.4596457  0.45948765 0.4594359  0.4595062  0.45965126 0.45990145
 0.46016863 0.46022502 0.46013287 0.46006447 0.46003327 0.4599652
 0.45987406 0.45982033 0.45975974 0.45964321 0.45944422 0.4592415
 0.4590465  0.4589329  0.45890686 0.45881695 0.45868015 0.45844463
 0.45819515 0.45800707 0.45795408 0.45790428 0.45790535 0.45790893
 0.45780814 0.4576903  0.45760494 0.45753333 0.45755434 0.45761195
 0.45756182 0.45739233 0.45723933 0.45711395 0.45708117 0.45703024
 0.4568918  0.45668063 0.4564307  0.45616046 0.4560279  0.4559592
 0.45594972 0.45602503 0.45612726 0.45630217 0.4564862  0.4566574
 0.45684624 0.4569831  0.45706075 0.45715514 0.4572644  0.4574335
 0.45762524 0.457799   0.45788717 0.45792666 0.45792377 0.4579261
 0.4579128  0.45786163 0.4577843  0.45766467 0.4576407  0.45773283
 0.45790118 0.4580511  0.45804325 0.45787197 0.45756897 0.45711952
 0.45645678 0.45576265 0.4550891  0.45437437 0.45382985 0.4535071
 0.4532891  0.45307404 0.45285654 0.4526615  0.45256227 0.45253742
 0.4523992  0.45209038 0.45170784 0.45134372 0.45116764 0.4509796
 0.45082632 0.45061073 0.45042387 0.4504493  0.45061976 0.45095733
 0.45109886 0.45077273 0.45030466 0.45014966 0.45011264 0.4497419
 0.44899687 0.44821876 0.44780976 0.44782832 0.44779018 0.44747967
 0.44703102 0.44689453 0.4472079  0.4476174  0.4477347  0.44750383
 0.44758096 0.44837677 0.4492234  0.44961822 0.45025152 0.45064536]
