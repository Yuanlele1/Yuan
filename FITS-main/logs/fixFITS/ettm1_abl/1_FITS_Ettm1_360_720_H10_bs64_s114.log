Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=50, out_features=150, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6720000.0
params:  7650.0
Trainable parameters:  7650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5760244
	speed: 0.0354s/iter; left time: 920.0339s
	iters: 200, epoch: 1 | loss: 0.4833201
	speed: 0.0191s/iter; left time: 495.9798s
Epoch: 1 cost time: 6.984394311904907
Epoch: 1, Steps: 261 | Train Loss: 0.5758717 Vali Loss: 1.0557423 Test Loss: 0.4697209
Validation loss decreased (inf --> 1.055742).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4698836
	speed: 0.1037s/iter; left time: 2668.0418s
	iters: 200, epoch: 2 | loss: 0.4231910
	speed: 0.0271s/iter; left time: 693.6900s
Epoch: 2 cost time: 8.575630187988281
Epoch: 2, Steps: 261 | Train Loss: 0.4373171 Vali Loss: 0.9934995 Test Loss: 0.4342951
Validation loss decreased (1.055742 --> 0.993499).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4341005
	speed: 0.1143s/iter; left time: 2912.0545s
	iters: 200, epoch: 3 | loss: 0.3882864
	speed: 0.0170s/iter; left time: 431.0339s
Epoch: 3 cost time: 5.487524509429932
Epoch: 3, Steps: 261 | Train Loss: 0.4226324 Vali Loss: 0.9781581 Test Loss: 0.4280291
Validation loss decreased (0.993499 --> 0.978158).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4197894
	speed: 0.0864s/iter; left time: 2177.9408s
	iters: 200, epoch: 4 | loss: 0.4310182
	speed: 0.0188s/iter; left time: 471.6135s
Epoch: 4 cost time: 5.597810745239258
Epoch: 4, Steps: 261 | Train Loss: 0.4180757 Vali Loss: 0.9699305 Test Loss: 0.4263606
Validation loss decreased (0.978158 --> 0.969930).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3816787
	speed: 0.0884s/iter; left time: 2206.5883s
	iters: 200, epoch: 5 | loss: 0.3940742
	speed: 0.0177s/iter; left time: 439.0684s
Epoch: 5 cost time: 5.129532814025879
Epoch: 5, Steps: 261 | Train Loss: 0.4159791 Vali Loss: 0.9672379 Test Loss: 0.4262579
Validation loss decreased (0.969930 --> 0.967238).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4546335
	speed: 0.0809s/iter; left time: 1997.3540s
	iters: 200, epoch: 6 | loss: 0.4081534
	speed: 0.0170s/iter; left time: 417.0572s
Epoch: 6 cost time: 4.919936418533325
Epoch: 6, Steps: 261 | Train Loss: 0.4150127 Vali Loss: 0.9655039 Test Loss: 0.4261174
Validation loss decreased (0.967238 --> 0.965504).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4197376
	speed: 0.1027s/iter; left time: 2510.0977s
	iters: 200, epoch: 7 | loss: 0.4241143
	speed: 0.0178s/iter; left time: 433.4086s
Epoch: 7 cost time: 5.178857326507568
Epoch: 7, Steps: 261 | Train Loss: 0.4144919 Vali Loss: 0.9648488 Test Loss: 0.4263715
Validation loss decreased (0.965504 --> 0.964849).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3966878
	speed: 0.0924s/iter; left time: 2233.8916s
	iters: 200, epoch: 8 | loss: 0.4011821
	speed: 0.0175s/iter; left time: 421.5043s
Epoch: 8 cost time: 6.0669105052948
Epoch: 8, Steps: 261 | Train Loss: 0.4142869 Vali Loss: 0.9630930 Test Loss: 0.4263477
Validation loss decreased (0.964849 --> 0.963093).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4347909
	speed: 0.0910s/iter; left time: 2177.1515s
	iters: 200, epoch: 9 | loss: 0.4155104
	speed: 0.0245s/iter; left time: 584.2136s
Epoch: 9 cost time: 5.719329595565796
Epoch: 9, Steps: 261 | Train Loss: 0.4141271 Vali Loss: 0.9622697 Test Loss: 0.4263745
Validation loss decreased (0.963093 --> 0.962270).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4074932
	speed: 0.1130s/iter; left time: 2672.3159s
	iters: 200, epoch: 10 | loss: 0.3988855
	speed: 0.0241s/iter; left time: 567.8961s
Epoch: 10 cost time: 8.713045358657837
Epoch: 10, Steps: 261 | Train Loss: 0.4139811 Vali Loss: 0.9609239 Test Loss: 0.4265279
Validation loss decreased (0.962270 --> 0.960924).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4518121
	speed: 0.1041s/iter; left time: 2436.1170s
	iters: 200, epoch: 11 | loss: 0.3848216
	speed: 0.0253s/iter; left time: 588.9935s
Epoch: 11 cost time: 6.197814702987671
Epoch: 11, Steps: 261 | Train Loss: 0.4139501 Vali Loss: 0.9619279 Test Loss: 0.4266857
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3732222
	speed: 0.1050s/iter; left time: 2428.2628s
	iters: 200, epoch: 12 | loss: 0.4464585
	speed: 0.0201s/iter; left time: 463.6930s
Epoch: 12 cost time: 6.925341844558716
Epoch: 12, Steps: 261 | Train Loss: 0.4138310 Vali Loss: 0.9617975 Test Loss: 0.4268047
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3646599
	speed: 0.0932s/iter; left time: 2132.2506s
	iters: 200, epoch: 13 | loss: 0.4348108
	speed: 0.0227s/iter; left time: 516.4259s
Epoch: 13 cost time: 6.203208923339844
Epoch: 13, Steps: 261 | Train Loss: 0.4137221 Vali Loss: 0.9600008 Test Loss: 0.4267209
Validation loss decreased (0.960924 --> 0.960001).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3974737
	speed: 0.1054s/iter; left time: 2382.7630s
	iters: 200, epoch: 14 | loss: 0.4316332
	speed: 0.0194s/iter; left time: 437.6268s
Epoch: 14 cost time: 5.689113140106201
Epoch: 14, Steps: 261 | Train Loss: 0.4138142 Vali Loss: 0.9611470 Test Loss: 0.4267798
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4259460
	speed: 0.0978s/iter; left time: 2186.2846s
	iters: 200, epoch: 15 | loss: 0.3679343
	speed: 0.0171s/iter; left time: 379.5695s
Epoch: 15 cost time: 5.606085300445557
Epoch: 15, Steps: 261 | Train Loss: 0.4138057 Vali Loss: 0.9617735 Test Loss: 0.4266755
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4036942
	speed: 0.0852s/iter; left time: 1881.9265s
	iters: 200, epoch: 16 | loss: 0.4181969
	speed: 0.0187s/iter; left time: 410.7238s
Epoch: 16 cost time: 5.478953123092651
Epoch: 16, Steps: 261 | Train Loss: 0.4137115 Vali Loss: 0.9599552 Test Loss: 0.4269279
Validation loss decreased (0.960001 --> 0.959955).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3988105
	speed: 0.0990s/iter; left time: 2161.2593s
	iters: 200, epoch: 17 | loss: 0.3986639
	speed: 0.0186s/iter; left time: 403.4719s
Epoch: 17 cost time: 5.405702352523804
Epoch: 17, Steps: 261 | Train Loss: 0.4136195 Vali Loss: 0.9611893 Test Loss: 0.4267675
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4448177
	speed: 0.0875s/iter; left time: 1887.8598s
	iters: 200, epoch: 18 | loss: 0.3814767
	speed: 0.0173s/iter; left time: 371.6205s
Epoch: 18 cost time: 5.0410237312316895
Epoch: 18, Steps: 261 | Train Loss: 0.4137495 Vali Loss: 0.9597588 Test Loss: 0.4270492
Validation loss decreased (0.959955 --> 0.959759).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4527857
	speed: 0.0981s/iter; left time: 2090.8743s
	iters: 200, epoch: 19 | loss: 0.4451579
	speed: 0.0267s/iter; left time: 567.0569s
Epoch: 19 cost time: 6.933420181274414
Epoch: 19, Steps: 261 | Train Loss: 0.4136411 Vali Loss: 0.9603149 Test Loss: 0.4268986
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3952915
	speed: 0.1036s/iter; left time: 2180.6431s
	iters: 200, epoch: 20 | loss: 0.4030813
	speed: 0.0186s/iter; left time: 388.7512s
Epoch: 20 cost time: 5.5491838455200195
Epoch: 20, Steps: 261 | Train Loss: 0.4136056 Vali Loss: 0.9610215 Test Loss: 0.4270506
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4291167
	speed: 0.0934s/iter; left time: 1941.6042s
	iters: 200, epoch: 21 | loss: 0.4367649
	speed: 0.0183s/iter; left time: 378.8150s
Epoch: 21 cost time: 5.25568699836731
Epoch: 21, Steps: 261 | Train Loss: 0.4135544 Vali Loss: 0.9602704 Test Loss: 0.4269233
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4083235
	speed: 0.1018s/iter; left time: 2089.6638s
	iters: 200, epoch: 22 | loss: 0.4019968
	speed: 0.0172s/iter; left time: 351.8132s
Epoch: 22 cost time: 5.833554029464722
Epoch: 22, Steps: 261 | Train Loss: 0.4135177 Vali Loss: 0.9607989 Test Loss: 0.4270678
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4525713
	speed: 0.0921s/iter; left time: 1865.7910s
	iters: 200, epoch: 23 | loss: 0.3976304
	speed: 0.0169s/iter; left time: 341.3469s
Epoch: 23 cost time: 5.337947130203247
Epoch: 23, Steps: 261 | Train Loss: 0.4134642 Vali Loss: 0.9600639 Test Loss: 0.4269159
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4364209
	speed: 0.1021s/iter; left time: 2042.2184s
	iters: 200, epoch: 24 | loss: 0.4405566
	speed: 0.0310s/iter; left time: 616.7375s
Epoch: 24 cost time: 7.645536661148071
Epoch: 24, Steps: 261 | Train Loss: 0.4136445 Vali Loss: 0.9613115 Test Loss: 0.4270079
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3779844
	speed: 0.1101s/iter; left time: 2173.7020s
	iters: 200, epoch: 25 | loss: 0.3782007
	speed: 0.0293s/iter; left time: 575.4183s
Epoch: 25 cost time: 6.430132627487183
Epoch: 25, Steps: 261 | Train Loss: 0.4135489 Vali Loss: 0.9608405 Test Loss: 0.4271882
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4378550
	speed: 0.0992s/iter; left time: 1932.3076s
	iters: 200, epoch: 26 | loss: 0.4274470
	speed: 0.0320s/iter; left time: 619.3610s
Epoch: 26 cost time: 7.192468166351318
Epoch: 26, Steps: 261 | Train Loss: 0.4134732 Vali Loss: 0.9596573 Test Loss: 0.4271953
Validation loss decreased (0.959759 --> 0.959657).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4108877
	speed: 0.0870s/iter; left time: 1671.8938s
	iters: 200, epoch: 27 | loss: 0.3903469
	speed: 0.0194s/iter; left time: 370.2715s
Epoch: 27 cost time: 5.398974895477295
Epoch: 27, Steps: 261 | Train Loss: 0.4134729 Vali Loss: 0.9602515 Test Loss: 0.4271384
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4576668
	speed: 0.1141s/iter; left time: 2163.0211s
	iters: 200, epoch: 28 | loss: 0.3997574
	speed: 0.0215s/iter; left time: 405.7593s
Epoch: 28 cost time: 6.138457298278809
Epoch: 28, Steps: 261 | Train Loss: 0.4135349 Vali Loss: 0.9604967 Test Loss: 0.4269643
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4435958
	speed: 0.1006s/iter; left time: 1880.5490s
	iters: 200, epoch: 29 | loss: 0.4058235
	speed: 0.0165s/iter; left time: 306.2238s
Epoch: 29 cost time: 5.1321141719818115
Epoch: 29, Steps: 261 | Train Loss: 0.4134918 Vali Loss: 0.9604367 Test Loss: 0.4271159
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4173198
	speed: 0.1089s/iter; left time: 2007.5125s
	iters: 200, epoch: 30 | loss: 0.3574870
	speed: 0.0185s/iter; left time: 339.2123s
Epoch: 30 cost time: 5.598428249359131
Epoch: 30, Steps: 261 | Train Loss: 0.4135229 Vali Loss: 0.9608846 Test Loss: 0.4271764
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3959040
	speed: 0.1182s/iter; left time: 2148.3836s
	iters: 200, epoch: 31 | loss: 0.4397989
	speed: 0.0387s/iter; left time: 700.1930s
Epoch: 31 cost time: 10.174770832061768
Epoch: 31, Steps: 261 | Train Loss: 0.4133069 Vali Loss: 0.9605021 Test Loss: 0.4270855
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4138705
	speed: 0.0949s/iter; left time: 1699.1524s
	iters: 200, epoch: 32 | loss: 0.4099561
	speed: 0.0173s/iter; left time: 307.6395s
Epoch: 32 cost time: 5.120706558227539
Epoch: 32, Steps: 261 | Train Loss: 0.4134474 Vali Loss: 0.9601045 Test Loss: 0.4271711
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4159327
	speed: 0.0897s/iter; left time: 1583.8638s
	iters: 200, epoch: 33 | loss: 0.3964290
	speed: 0.0222s/iter; left time: 389.7375s
Epoch: 33 cost time: 5.740525960922241
Epoch: 33, Steps: 261 | Train Loss: 0.4134614 Vali Loss: 0.9599517 Test Loss: 0.4269756
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4452239
	speed: 0.1058s/iter; left time: 1839.4497s
	iters: 200, epoch: 34 | loss: 0.4584032
	speed: 0.0201s/iter; left time: 346.6912s
Epoch: 34 cost time: 8.886079788208008
Epoch: 34, Steps: 261 | Train Loss: 0.4135260 Vali Loss: 0.9606500 Test Loss: 0.4270559
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3874694
	speed: 0.1238s/iter; left time: 2120.2730s
	iters: 200, epoch: 35 | loss: 0.4091564
	speed: 0.0179s/iter; left time: 305.2695s
Epoch: 35 cost time: 5.336651802062988
Epoch: 35, Steps: 261 | Train Loss: 0.4133711 Vali Loss: 0.9597400 Test Loss: 0.4270930
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3790975
	speed: 0.0889s/iter; left time: 1499.8782s
	iters: 200, epoch: 36 | loss: 0.4347180
	speed: 0.0251s/iter; left time: 421.4298s
Epoch: 36 cost time: 6.234103441238403
Epoch: 36, Steps: 261 | Train Loss: 0.4133886 Vali Loss: 0.9600523 Test Loss: 0.4272509
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3821489
	speed: 0.1066s/iter; left time: 1770.7427s
	iters: 200, epoch: 37 | loss: 0.3954844
	speed: 0.0259s/iter; left time: 427.0808s
Epoch: 37 cost time: 7.6510093212127686
Epoch: 37, Steps: 261 | Train Loss: 0.4134943 Vali Loss: 0.9598710 Test Loss: 0.4270658
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4278430
	speed: 0.1026s/iter; left time: 1676.0822s
	iters: 200, epoch: 38 | loss: 0.4185857
	speed: 0.0274s/iter; left time: 445.6343s
Epoch: 38 cost time: 6.094488620758057
Epoch: 38, Steps: 261 | Train Loss: 0.4133419 Vali Loss: 0.9605392 Test Loss: 0.4271332
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.3804517
	speed: 0.0963s/iter; left time: 1548.9465s
	iters: 200, epoch: 39 | loss: 0.4123094
	speed: 0.0185s/iter; left time: 295.1083s
Epoch: 39 cost time: 5.423309087753296
Epoch: 39, Steps: 261 | Train Loss: 0.4134207 Vali Loss: 0.9604869 Test Loss: 0.4271102
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4119008
	speed: 0.1049s/iter; left time: 1660.0100s
	iters: 200, epoch: 40 | loss: 0.4150654
	speed: 0.0255s/iter; left time: 400.5710s
Epoch: 40 cost time: 8.493483781814575
Epoch: 40, Steps: 261 | Train Loss: 0.4134000 Vali Loss: 0.9605110 Test Loss: 0.4272466
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4467047
	speed: 0.1041s/iter; left time: 1619.3386s
	iters: 200, epoch: 41 | loss: 0.4246317
	speed: 0.0183s/iter; left time: 282.7479s
Epoch: 41 cost time: 6.268292427062988
Epoch: 41, Steps: 261 | Train Loss: 0.4133730 Vali Loss: 0.9602666 Test Loss: 0.4270998
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4575697
	speed: 0.1074s/iter; left time: 1642.5040s
	iters: 200, epoch: 42 | loss: 0.3858306
	speed: 0.0200s/iter; left time: 303.4519s
Epoch: 42 cost time: 7.461972951889038
Epoch: 42, Steps: 261 | Train Loss: 0.4134203 Vali Loss: 0.9606720 Test Loss: 0.4271175
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4025823
	speed: 0.1029s/iter; left time: 1546.8190s
	iters: 200, epoch: 43 | loss: 0.4160638
	speed: 0.0189s/iter; left time: 281.9873s
Epoch: 43 cost time: 5.3852832317352295
Epoch: 43, Steps: 261 | Train Loss: 0.4133755 Vali Loss: 0.9606352 Test Loss: 0.4272583
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4315127
	speed: 0.0989s/iter; left time: 1461.5944s
	iters: 200, epoch: 44 | loss: 0.4300840
	speed: 0.0211s/iter; left time: 309.0845s
Epoch: 44 cost time: 7.347858905792236
Epoch: 44, Steps: 261 | Train Loss: 0.4133956 Vali Loss: 0.9596943 Test Loss: 0.4271210
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4166220
	speed: 0.0990s/iter; left time: 1437.7362s
	iters: 200, epoch: 45 | loss: 0.4361500
	speed: 0.0174s/iter; left time: 251.2361s
Epoch: 45 cost time: 5.162645578384399
Epoch: 45, Steps: 261 | Train Loss: 0.4133030 Vali Loss: 0.9601804 Test Loss: 0.4271414
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4214727
	speed: 0.0845s/iter; left time: 1204.6817s
	iters: 200, epoch: 46 | loss: 0.4129032
	speed: 0.0177s/iter; left time: 250.3060s
Epoch: 46 cost time: 5.291296482086182
Epoch: 46, Steps: 261 | Train Loss: 0.4133477 Vali Loss: 0.9603871 Test Loss: 0.4271599
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.42686769366264343, mae:0.4157487154006958, rse:0.6216091513633728, corr:[0.5288172  0.53255975 0.53263235 0.53151476 0.5311951  0.53179646
 0.53244954 0.532488   0.53196615 0.5316885  0.53239834 0.5336132
 0.5346111  0.53478146 0.5339322  0.53240275 0.5309919  0.53014195
 0.5294681  0.528602   0.5272692  0.52577364 0.5242831  0.52317125
 0.5223808  0.5215669  0.52064794 0.5197057  0.5188472  0.51811975
 0.517966   0.51854813 0.5194601  0.52021253 0.5204356  0.52027375
 0.5198813  0.51943886 0.51915276 0.5190178  0.51893926 0.5186791
 0.51813173 0.5174816  0.5169289  0.5166646  0.5167474  0.51688844
 0.5168207  0.51657385 0.5161872  0.5157883  0.51560754 0.5156417
 0.51582795 0.5160879  0.51624024 0.51623374 0.51611704 0.515831
 0.5156015  0.5154209  0.51534885 0.5152865  0.5153228  0.515483
 0.5156627  0.51578987 0.51607037 0.5164286  0.5168145  0.5171098
 0.5173216  0.51745766 0.51749057 0.5174154  0.51729584 0.5171744
 0.5170299  0.51683855 0.51662266 0.5164083  0.5161454  0.5157633
 0.5154025  0.51502997 0.5147771  0.51461    0.51450235 0.5145496
 0.5147414  0.5149532  0.5151001  0.51515234 0.51502866 0.5146314
 0.51399875 0.5132208  0.5123882  0.51164186 0.5112168  0.51121616
 0.511545   0.51188874 0.5122132  0.5124794  0.5125898  0.5127642
 0.5129346  0.5132131  0.5134273  0.5134858  0.5133321  0.5131853
 0.5131232  0.51320904 0.5133524  0.51346534 0.5134356  0.5134142
 0.51338756 0.5132781  0.5131276  0.5130798  0.5130437  0.5128812
 0.5125973  0.5122831  0.51201296 0.511874   0.51186806 0.5118775
 0.5118115  0.5115055  0.5110329  0.5104794  0.5100185  0.5097168
 0.5096653  0.5097703  0.5098675  0.5099263  0.50989753 0.5098381
 0.5097865  0.5097767  0.5097607  0.5097399  0.50974554 0.5096706
 0.5095596  0.509555   0.50958365 0.50959396 0.50958514 0.5096012
 0.50960654 0.50959724 0.5095347  0.5094804  0.5095139  0.50962377
 0.50982845 0.510105   0.51049346 0.510916   0.51126146 0.5115568
 0.5117735  0.5119499  0.5121631  0.51243585 0.512639   0.5127329
 0.51268816 0.51256704 0.51239496 0.51223826 0.5120562  0.51190907
 0.5117757  0.5117346  0.5116694  0.51160574 0.511514   0.51140016
 0.51125455 0.5111151  0.5109819  0.510833   0.51058775 0.51023096
 0.50979906 0.50943696 0.5091351  0.50877553 0.50836605 0.50804025
 0.50770986 0.50731754 0.5069718  0.5066996  0.5064377  0.50618565
 0.50591695 0.50549084 0.50502324 0.5045287  0.50411564 0.50383127
 0.50351185 0.50313133 0.5026012  0.5020431  0.50150216 0.5011571
 0.5010113  0.50096124 0.5008542  0.5007545  0.5005707  0.500325
 0.50017655 0.50012714 0.50017273 0.50026244 0.5003707  0.50035214
 0.5002148  0.50002766 0.49980572 0.49956664 0.4993766  0.49918732
 0.49904785 0.4989216  0.49871844 0.4986257  0.49863428 0.49870083
 0.4987368  0.49868244 0.49858928 0.49846447 0.49832705 0.49827746
 0.49830014 0.49836108 0.49844185 0.49853352 0.49852225 0.4984714
 0.49833065 0.49825335 0.49823722 0.49830723 0.49836528 0.498498
 0.49861106 0.49872974 0.4988791  0.49905384 0.49916467 0.4993471
 0.4995524  0.49973834 0.49982867 0.49992988 0.50001365 0.5000244
 0.50008464 0.5001914  0.50028735 0.5003332  0.500239   0.5000936
 0.49987987 0.49973917 0.49957365 0.49948108 0.4993851  0.49927297
 0.49919817 0.49905974 0.49884906 0.4985188  0.49804965 0.4974548
 0.49673086 0.49604443 0.49546805 0.49482033 0.49421236 0.49371356
 0.49337897 0.49311492 0.49300084 0.492898   0.49275228 0.49247444
 0.49208382 0.491654   0.49128178 0.49102885 0.49086058 0.4907512
 0.49070263 0.49060932 0.4904891  0.49038905 0.49034822 0.49033752
 0.49038336 0.49040985 0.49033847 0.49032715 0.49035093 0.49041414
 0.49047732 0.49048126 0.49036857 0.49015072 0.48986974 0.48957354
 0.4893275  0.4891468  0.48898745 0.4888595  0.48867187 0.48845762
 0.4882223  0.48802096 0.4878565  0.48774156 0.48767382 0.48762512
 0.48753548 0.48741218 0.4873012  0.48716792 0.48714137 0.487255
 0.48740637 0.48754394 0.48764446 0.4876388  0.48760504 0.48760745
 0.48760307 0.48765936 0.48774138 0.4877722  0.4877764  0.48779315
 0.4878126  0.4878724  0.48802423 0.4882184  0.48840252 0.48857567
 0.48868167 0.48875707 0.48881388 0.48887652 0.48893058 0.4889556
 0.4889123  0.48884827 0.48879656 0.4886956  0.48859718 0.48852608
 0.48852193 0.48857048 0.48868138 0.48883632 0.48897165 0.48907447
 0.48914772 0.48918328 0.4891865  0.48910984 0.48892    0.4886031
 0.4881087  0.48752043 0.48691538 0.48627093 0.48566437 0.48529148
 0.4851208  0.4850015  0.4849265  0.4848903  0.48478115 0.48461208
 0.48433253 0.4840842  0.48388633 0.4837377  0.4835981  0.48348808
 0.4834643  0.4834797  0.4834613  0.48338953 0.4833125  0.4833115
 0.48338467 0.48338938 0.48336458 0.48340574 0.48345423 0.48345256
 0.48340395 0.48327982 0.48316592 0.48304343 0.48294973 0.4828781
 0.48276886 0.4826328  0.48238903 0.48211983 0.48184857 0.48161814
 0.481408   0.48129672 0.48118252 0.48109165 0.48100612 0.48101938
 0.4809396  0.48076943 0.4806255  0.48053676 0.48046672 0.48044157
 0.48040503 0.48037958 0.4803899  0.4804334  0.4805365  0.48069647
 0.48077264 0.48078105 0.48069373 0.48057088 0.48049295 0.4805146
 0.48063877 0.48083365 0.48100227 0.4810876  0.4810868  0.48109558
 0.48106086 0.4811087  0.48121813 0.48141575 0.48159888 0.48169217
 0.48171833 0.4817685  0.4818214  0.48194832 0.48208672 0.48217693
 0.48218372 0.4820865  0.4819109  0.48173082 0.481596   0.48153862
 0.48149258 0.4814338  0.48127937 0.4809434  0.4803962  0.47968405
 0.4788496  0.47803885 0.47725132 0.47642848 0.47559214 0.4748087
 0.4741384  0.47353545 0.47305453 0.47271404 0.47235504 0.47189158
 0.47139582 0.47094938 0.470547   0.47019538 0.47000775 0.46987918
 0.46976766 0.46973827 0.46978846 0.4698472  0.4700134  0.47032648
 0.4707464  0.47102457 0.47113314 0.4711355  0.47107664 0.4710572
 0.47110632 0.4712131  0.47135478 0.47151697 0.47157878 0.47156516
 0.47145453 0.47129068 0.47118556 0.47105533 0.47091204 0.47078684
 0.47062773 0.4704009  0.47015104 0.46991473 0.46973985 0.4696853
 0.46963164 0.46949673 0.46930292 0.46909913 0.46894258 0.46888232
 0.46891266 0.46896362 0.46903595 0.46909237 0.4691067  0.4690814
 0.4690473  0.46898338 0.46891084 0.4688671  0.46890995 0.46899635
 0.46902698 0.46898782 0.4689544  0.46894407 0.46894863 0.4690574
 0.46913993 0.46923074 0.46929583 0.46937847 0.4694953  0.46958345
 0.46966156 0.4697613  0.4698513  0.46993938 0.47000536 0.4700419
 0.46999392 0.46997344 0.46990386 0.46984577 0.46981648 0.4698112
 0.46984264 0.4698186  0.46971914 0.469466   0.46902242 0.46837634
 0.46757334 0.46669528 0.4658626  0.46495485 0.46410772 0.463487
 0.4631275  0.4628784  0.462707   0.46252    0.46218204 0.4616999
 0.4612106  0.4607786  0.46045685 0.46020782 0.45993495 0.45969644
 0.45938873 0.45919037 0.459127   0.4592318  0.45945254 0.45977908
 0.46010998 0.46021518 0.46012396 0.460025   0.45999423 0.46000248
 0.4600134  0.45998847 0.45985556 0.45960772 0.45932028 0.45912787
 0.45902884 0.45899504 0.45895466 0.45877218 0.45848787 0.45813465
 0.45784697 0.45770428 0.4577321  0.45777613 0.45784855 0.45788932
 0.45780352 0.45765388 0.45749065 0.4573393  0.45730898 0.45740074
 0.4574843  0.45747775 0.45740414 0.45725462 0.457102   0.45692477
 0.45672232 0.45652893 0.45635203 0.45617157 0.4560957  0.45606783
 0.4561039  0.45624134 0.4564289  0.456641   0.45678523 0.45684722
 0.45689884 0.45697048 0.4571117  0.45734233 0.45754972 0.45768443
 0.45773074 0.45775652 0.4578078  0.4579318  0.45804992 0.4580862
 0.45798814 0.4577969  0.4576228  0.45751333 0.45755944 0.45772395
 0.45792654 0.45808536 0.45809683 0.45792654 0.4575644  0.4570049
 0.45626667 0.45559618 0.45506305 0.45452997 0.45404938 0.45361388
 0.45319152 0.45280117 0.45250505 0.4523215  0.45227557 0.45234048
 0.4523803  0.45234087 0.45221248 0.45193186 0.45160022 0.45120585
 0.4510058  0.4509812  0.45105204 0.451167   0.45121998 0.45141405
 0.4516445  0.45164055 0.45141777 0.45118243 0.4508487  0.45028216
 0.44960204 0.4490317  0.44873524 0.44873205 0.44875708 0.4486005
 0.44808447 0.44737884 0.44681174 0.44668612 0.4471221  0.4476742
 0.44780242 0.44748652 0.44729087 0.44790396 0.44970855 0.45071357]
