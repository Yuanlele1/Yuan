Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_336_FITS_ETTm1_ftM_sl90_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34135
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=22, out_features=104, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2050048.0
params:  2392.0
Trainable parameters:  2392
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7589099
	speed: 0.0293s/iter; left time: 775.7497s
	iters: 200, epoch: 1 | loss: 0.5283859
	speed: 0.0188s/iter; left time: 496.3381s
Epoch: 1 cost time: 5.941749811172485
Epoch: 1, Steps: 266 | Train Loss: 0.6863526 Vali Loss: 0.8907495 Test Loss: 0.6172493
Validation loss decreased (inf --> 0.890749).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4384708
	speed: 0.1037s/iter; left time: 2721.0036s
	iters: 200, epoch: 2 | loss: 0.4649695
	speed: 0.0178s/iter; left time: 464.2834s
Epoch: 2 cost time: 5.569571018218994
Epoch: 2, Steps: 266 | Train Loss: 0.4694836 Vali Loss: 0.7595840 Test Loss: 0.4872892
Validation loss decreased (0.890749 --> 0.759584).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4208584
	speed: 0.1054s/iter; left time: 2737.8304s
	iters: 200, epoch: 3 | loss: 0.4492214
	speed: 0.0295s/iter; left time: 764.1254s
Epoch: 3 cost time: 8.466736316680908
Epoch: 3, Steps: 266 | Train Loss: 0.4369308 Vali Loss: 0.7207478 Test Loss: 0.4520901
Validation loss decreased (0.759584 --> 0.720748).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4383092
	speed: 0.1301s/iter; left time: 3343.4710s
	iters: 200, epoch: 4 | loss: 0.4753158
	speed: 0.0230s/iter; left time: 588.9077s
Epoch: 4 cost time: 8.0901620388031
Epoch: 4, Steps: 266 | Train Loss: 0.4282781 Vali Loss: 0.7064285 Test Loss: 0.4389722
Validation loss decreased (0.720748 --> 0.706429).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3814341
	speed: 0.1206s/iter; left time: 3068.3318s
	iters: 200, epoch: 5 | loss: 0.4465067
	speed: 0.0204s/iter; left time: 517.6289s
Epoch: 5 cost time: 6.5069286823272705
Epoch: 5, Steps: 266 | Train Loss: 0.4253054 Vali Loss: 0.6987258 Test Loss: 0.4337710
Validation loss decreased (0.706429 --> 0.698726).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4319879
	speed: 0.1262s/iter; left time: 3177.3282s
	iters: 200, epoch: 6 | loss: 0.4351758
	speed: 0.0332s/iter; left time: 831.1616s
Epoch: 6 cost time: 8.141826868057251
Epoch: 6, Steps: 266 | Train Loss: 0.4243580 Vali Loss: 0.6964905 Test Loss: 0.4318485
Validation loss decreased (0.698726 --> 0.696491).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4384101
	speed: 0.1059s/iter; left time: 2636.9281s
	iters: 200, epoch: 7 | loss: 0.4716399
	speed: 0.0197s/iter; left time: 488.0998s
Epoch: 7 cost time: 6.092078685760498
Epoch: 7, Steps: 266 | Train Loss: 0.4239143 Vali Loss: 0.6958176 Test Loss: 0.4310276
Validation loss decreased (0.696491 --> 0.695818).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4583734
	speed: 0.1011s/iter; left time: 2492.1292s
	iters: 200, epoch: 8 | loss: 0.4481070
	speed: 0.0268s/iter; left time: 657.5807s
Epoch: 8 cost time: 6.714696407318115
Epoch: 8, Steps: 266 | Train Loss: 0.4237676 Vali Loss: 0.6949614 Test Loss: 0.4306450
Validation loss decreased (0.695818 --> 0.694961).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4237411
	speed: 0.0988s/iter; left time: 2407.7382s
	iters: 200, epoch: 9 | loss: 0.4749839
	speed: 0.0201s/iter; left time: 488.0202s
Epoch: 9 cost time: 5.642035722732544
Epoch: 9, Steps: 266 | Train Loss: 0.4236502 Vali Loss: 0.6943538 Test Loss: 0.4307475
Validation loss decreased (0.694961 --> 0.694354).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4510932
	speed: 0.1081s/iter; left time: 2605.3024s
	iters: 200, epoch: 10 | loss: 0.4643117
	speed: 0.0331s/iter; left time: 794.2465s
Epoch: 10 cost time: 9.199584722518921
Epoch: 10, Steps: 266 | Train Loss: 0.4235613 Vali Loss: 0.6949540 Test Loss: 0.4312520
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4381549
	speed: 0.1624s/iter; left time: 3871.5003s
	iters: 200, epoch: 11 | loss: 0.3592119
	speed: 0.0262s/iter; left time: 620.9665s
Epoch: 11 cost time: 9.171566009521484
Epoch: 11, Steps: 266 | Train Loss: 0.4234373 Vali Loss: 0.6949183 Test Loss: 0.4306570
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4216177
	speed: 0.1308s/iter; left time: 3083.4369s
	iters: 200, epoch: 12 | loss: 0.4504345
	speed: 0.0330s/iter; left time: 775.5898s
Epoch: 12 cost time: 10.241844654083252
Epoch: 12, Steps: 266 | Train Loss: 0.4234741 Vali Loss: 0.6946623 Test Loss: 0.4305488
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4239350
	speed: 0.1160s/iter; left time: 2703.2563s
	iters: 200, epoch: 13 | loss: 0.4615850
	speed: 0.0237s/iter; left time: 550.6908s
Epoch: 13 cost time: 6.451813220977783
Epoch: 13, Steps: 266 | Train Loss: 0.4235270 Vali Loss: 0.6942529 Test Loss: 0.4307829
Validation loss decreased (0.694354 --> 0.694253).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4751670
	speed: 0.1258s/iter; left time: 2898.2511s
	iters: 200, epoch: 14 | loss: 0.4349079
	speed: 0.0193s/iter; left time: 443.3578s
Epoch: 14 cost time: 6.394598484039307
Epoch: 14, Steps: 266 | Train Loss: 0.4233476 Vali Loss: 0.6927989 Test Loss: 0.4302958
Validation loss decreased (0.694253 --> 0.692799).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4060023
	speed: 0.1025s/iter; left time: 2335.5309s
	iters: 200, epoch: 15 | loss: 0.3978404
	speed: 0.0191s/iter; left time: 433.7734s
Epoch: 15 cost time: 6.179842710494995
Epoch: 15, Steps: 266 | Train Loss: 0.4234215 Vali Loss: 0.6939968 Test Loss: 0.4306299
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4171189
	speed: 0.1382s/iter; left time: 3110.3786s
	iters: 200, epoch: 16 | loss: 0.3693660
	speed: 0.0289s/iter; left time: 646.6970s
Epoch: 16 cost time: 11.322557926177979
Epoch: 16, Steps: 266 | Train Loss: 0.4235609 Vali Loss: 0.6944680 Test Loss: 0.4305707
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3905172
	speed: 0.1435s/iter; left time: 3191.5228s
	iters: 200, epoch: 17 | loss: 0.4076385
	speed: 0.0243s/iter; left time: 537.6054s
Epoch: 17 cost time: 8.705265998840332
Epoch: 17, Steps: 266 | Train Loss: 0.4234087 Vali Loss: 0.6948960 Test Loss: 0.4311086
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4502181
	speed: 0.1594s/iter; left time: 3503.4587s
	iters: 200, epoch: 18 | loss: 0.4716265
	speed: 0.0195s/iter; left time: 426.5219s
Epoch: 18 cost time: 8.100631952285767
Epoch: 18, Steps: 266 | Train Loss: 0.4232014 Vali Loss: 0.6941881 Test Loss: 0.4308811
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4317451
	speed: 0.1065s/iter; left time: 2311.8521s
	iters: 200, epoch: 19 | loss: 0.4162821
	speed: 0.0206s/iter; left time: 446.0640s
Epoch: 19 cost time: 6.681977272033691
Epoch: 19, Steps: 266 | Train Loss: 0.4233849 Vali Loss: 0.6942498 Test Loss: 0.4308877
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4019523
	speed: 0.1043s/iter; left time: 2237.8941s
	iters: 200, epoch: 20 | loss: 0.4590153
	speed: 0.0199s/iter; left time: 424.4284s
Epoch: 20 cost time: 6.302441358566284
Epoch: 20, Steps: 266 | Train Loss: 0.4233660 Vali Loss: 0.6940894 Test Loss: 0.4309388
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4333590
	speed: 0.1110s/iter; left time: 2350.1184s
	iters: 200, epoch: 21 | loss: 0.4102791
	speed: 0.0292s/iter; left time: 615.8595s
Epoch: 21 cost time: 7.788250207901001
Epoch: 21, Steps: 266 | Train Loss: 0.4233164 Vali Loss: 0.6935568 Test Loss: 0.4309544
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4098771
	speed: 0.1125s/iter; left time: 2353.1854s
	iters: 200, epoch: 22 | loss: 0.4028705
	speed: 0.0186s/iter; left time: 386.4021s
Epoch: 22 cost time: 5.941368103027344
Epoch: 22, Steps: 266 | Train Loss: 0.4232751 Vali Loss: 0.6944580 Test Loss: 0.4309216
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4252554
	speed: 0.0991s/iter; left time: 2046.0470s
	iters: 200, epoch: 23 | loss: 0.4001297
	speed: 0.0181s/iter; left time: 371.3630s
Epoch: 23 cost time: 5.67198371887207
Epoch: 23, Steps: 266 | Train Loss: 0.4234439 Vali Loss: 0.6944953 Test Loss: 0.4309233
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3919896
	speed: 0.1078s/iter; left time: 2197.1647s
	iters: 200, epoch: 24 | loss: 0.4245716
	speed: 0.0203s/iter; left time: 412.4918s
Epoch: 24 cost time: 6.924928426742554
Epoch: 24, Steps: 266 | Train Loss: 0.4230788 Vali Loss: 0.6947147 Test Loss: 0.4307258
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4431953
	speed: 0.1310s/iter; left time: 2636.2193s
	iters: 200, epoch: 25 | loss: 0.4272789
	speed: 0.0245s/iter; left time: 490.9865s
Epoch: 25 cost time: 6.997781991958618
Epoch: 25, Steps: 266 | Train Loss: 0.4233986 Vali Loss: 0.6946942 Test Loss: 0.4309108
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4194005
	speed: 0.1309s/iter; left time: 2597.5705s
	iters: 200, epoch: 26 | loss: 0.3742333
	speed: 0.0210s/iter; left time: 414.1515s
Epoch: 26 cost time: 6.18176531791687
Epoch: 26, Steps: 266 | Train Loss: 0.4232828 Vali Loss: 0.6943731 Test Loss: 0.4309133
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4157575
	speed: 0.0999s/iter; left time: 1956.2535s
	iters: 200, epoch: 27 | loss: 0.4387999
	speed: 0.0190s/iter; left time: 371.0410s
Epoch: 27 cost time: 5.362915277481079
Epoch: 27, Steps: 266 | Train Loss: 0.4232920 Vali Loss: 0.6943634 Test Loss: 0.4309164
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4294450
	speed: 0.0961s/iter; left time: 1857.2586s
	iters: 200, epoch: 28 | loss: 0.4832576
	speed: 0.0181s/iter; left time: 347.2282s
Epoch: 28 cost time: 5.794087171554565
Epoch: 28, Steps: 266 | Train Loss: 0.4233797 Vali Loss: 0.6949596 Test Loss: 0.4309324
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4296034
	speed: 0.1281s/iter; left time: 2440.0780s
	iters: 200, epoch: 29 | loss: 0.4358798
	speed: 0.0350s/iter; left time: 662.8432s
Epoch: 29 cost time: 10.254765510559082
Epoch: 29, Steps: 266 | Train Loss: 0.4232697 Vali Loss: 0.6946533 Test Loss: 0.4310848
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4103947
	speed: 0.1575s/iter; left time: 2959.6980s
	iters: 200, epoch: 30 | loss: 0.4326814
	speed: 0.0458s/iter; left time: 856.3745s
Epoch: 30 cost time: 11.740033626556396
Epoch: 30, Steps: 266 | Train Loss: 0.4231276 Vali Loss: 0.6949648 Test Loss: 0.4311168
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4019659
	speed: 0.1871s/iter; left time: 3465.9969s
	iters: 200, epoch: 31 | loss: 0.4012431
	speed: 0.0297s/iter; left time: 546.5935s
Epoch: 31 cost time: 9.553668737411499
Epoch: 31, Steps: 266 | Train Loss: 0.4232150 Vali Loss: 0.6933941 Test Loss: 0.4308695
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4358614
	speed: 0.1054s/iter; left time: 1924.5113s
	iters: 200, epoch: 32 | loss: 0.4526159
	speed: 0.0298s/iter; left time: 541.6936s
Epoch: 32 cost time: 6.506932020187378
Epoch: 32, Steps: 266 | Train Loss: 0.4232658 Vali Loss: 0.6944209 Test Loss: 0.4308535
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4210051
	speed: 0.1061s/iter; left time: 1909.1579s
	iters: 200, epoch: 33 | loss: 0.4061387
	speed: 0.0295s/iter; left time: 527.4466s
Epoch: 33 cost time: 8.261292695999146
Epoch: 33, Steps: 266 | Train Loss: 0.4232304 Vali Loss: 0.6940695 Test Loss: 0.4309249
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4386746
	speed: 0.1116s/iter; left time: 1978.7006s
	iters: 200, epoch: 34 | loss: 0.3933904
	speed: 0.0262s/iter; left time: 461.3357s
Epoch: 34 cost time: 7.602549076080322
Epoch: 34, Steps: 266 | Train Loss: 0.4230598 Vali Loss: 0.6945555 Test Loss: 0.4309907
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_336_FITS_ETTm1_ftM_sl90_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.43038761615753174, mae:0.41812193393707275, rse:0.6242782473564148, corr:[0.5466433  0.54595524 0.5396552  0.53856987 0.53688496 0.53293335
 0.5301562  0.529089   0.52788746 0.52682453 0.5262847  0.5255503
 0.5241996  0.52262974 0.52060974 0.5173575  0.5135409  0.51061875
 0.5083554  0.50554806 0.5020344  0.49885893 0.49637473 0.49356902
 0.48983222 0.48590624 0.4831284  0.48126036 0.4793305  0.47712252
 0.47602347 0.4765948  0.4770758  0.4769635  0.47708884 0.47793853
 0.4781831  0.47760454 0.47703573 0.47710022 0.4774124  0.47701442
 0.47641155 0.4764056  0.4765568  0.47622427 0.47565937 0.47551554
 0.4758053  0.47596365 0.4758863  0.47590765 0.47638968 0.47678596
 0.47699985 0.47727332 0.47791937 0.47839484 0.47809446 0.47775328
 0.4779328  0.4779742  0.47755912 0.47685304 0.47674587 0.47694933
 0.47667247 0.47614756 0.47635835 0.4773426  0.47803915 0.4782691
 0.4785911  0.4792206  0.48002177 0.4805283  0.48118263 0.48219627
 0.4830877  0.48351657 0.483784   0.48417574 0.48483548 0.4852374
 0.485403   0.48570472 0.48641852 0.48732403 0.4881087  0.48880816
 0.4896217  0.49047047 0.49107623 0.4913214  0.49135458 0.49126986
 0.49072632 0.48963523 0.48842356 0.48755667 0.48712602 0.48690087
 0.48678645 0.4866129  0.48626253 0.4856792  0.48503694 0.48440588
 0.48363712 0.4826974  0.48170626 0.480733   0.4797155  0.47867888
 0.47772142 0.47700658 0.4762277  0.47512022 0.47380632 0.47259155
 0.47157294 0.47029564 0.4687988  0.46781603 0.46736753 0.46684465
 0.46604145 0.4653601  0.46519375 0.46531883 0.4653299  0.46525413
 0.46524248 0.46540555 0.4652825  0.46482876 0.46444637 0.46435976
 0.4642769  0.4640224  0.463814   0.46391705 0.4640589  0.4640082
 0.46386412 0.46400002 0.46435487 0.4645003  0.46466833 0.46492067
 0.46519142 0.46545961 0.46553162 0.46554416 0.4655409  0.46555254
 0.46543843 0.4653965  0.46554586 0.46572456 0.46581262 0.46582064
 0.46599722 0.46634093 0.4666613  0.46694347 0.4674871  0.46837506
 0.46924353 0.46977806 0.47017986 0.47092745 0.4717636  0.47242472
 0.47290248 0.4734376  0.47398838 0.47436455 0.47455096 0.47488222
 0.47541842 0.47604454 0.47654396 0.47700736 0.47771353 0.4788277
 0.48010108 0.48086482 0.48080996 0.4805196  0.48083258 0.4816223
 0.4822193  0.4825562  0.4830778  0.48409292 0.48545    0.48668852
 0.48724046 0.48704857 0.48675692 0.48645744 0.48588175 0.4849715
 0.4839603  0.48295572 0.48164067 0.4800695  0.4785358  0.47730803
 0.47612017 0.4747485  0.47331655 0.47190225 0.47046953 0.4689563
 0.46704653 0.4650826  0.4636037  0.4628142  0.46208578 0.46120548
 0.46058017 0.46065745 0.4608529  0.4607825  0.4608424  0.46070454
 0.4601929  0.4596725  0.45946327 0.45941523 0.45927024 0.45879406
 0.45849723 0.45874995 0.45900118 0.45886806 0.45857608 0.45832214
 0.4584671  0.4585851  0.45866996 0.45897266 0.45922995 0.4594672
 0.4597758  0.46040538 0.4610791  0.4614916  0.46140686 0.4614804
 0.46173823 0.4618802  0.46146357 0.46134758 0.46178254 0.46228394
 0.46239844 0.46234348 0.46248326 0.46302482 0.46353942 0.46379313
 0.46428117 0.46489283 0.46545726 0.46607623 0.467051   0.46824828
 0.46942827 0.47010997 0.47048187 0.47090876 0.47147128 0.4719251
 0.47226158 0.47277132 0.47338226 0.4739878  0.47454077 0.47524342
 0.47608244 0.47670978 0.47695005 0.47688812 0.476395   0.4751307
 0.4729547  0.47038752 0.46829733 0.46706137 0.46629414 0.46573
 0.4655011  0.46558803 0.4654541  0.46475917 0.46398523 0.4634829
 0.46277198 0.4613441  0.45964104 0.4584096  0.45760182 0.45655727
 0.45546997 0.45481655 0.45456058 0.4539895  0.45295095 0.45195457
 0.45118594 0.45028594 0.44920772 0.44850934 0.44805735 0.4474309
 0.44654673 0.44613034 0.4463966  0.44639552 0.4455857  0.4449589
 0.4451428  0.4453675  0.44469965 0.44409594 0.44450966 0.4448807
 0.44449955 0.44439155 0.4453078  0.4456225  0.4455636  0.44673565]
