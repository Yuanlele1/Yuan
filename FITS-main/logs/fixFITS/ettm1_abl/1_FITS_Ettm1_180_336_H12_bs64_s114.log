Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_336_FITS_ETTm1_ftM_sl180_ll48_pl336_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34045
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=34, out_features=97, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2955008.0
params:  3395.0
Trainable parameters:  3395
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5102918
	speed: 0.0276s/iter; left time: 728.7613s
	iters: 200, epoch: 1 | loss: 0.4646356
	speed: 0.0259s/iter; left time: 680.0224s
Epoch: 1 cost time: 6.807986497879028
Epoch: 1, Steps: 265 | Train Loss: 0.5252730 Vali Loss: 0.7733683 Test Loss: 0.4628644
Validation loss decreased (inf --> 0.773368).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3843462
	speed: 0.1376s/iter; left time: 3596.4920s
	iters: 200, epoch: 2 | loss: 0.3951864
	speed: 0.0206s/iter; left time: 537.6179s
Epoch: 2 cost time: 6.866485834121704
Epoch: 2, Steps: 265 | Train Loss: 0.3995375 Vali Loss: 0.7000815 Test Loss: 0.4036241
Validation loss decreased (0.773368 --> 0.700082).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3757981
	speed: 0.0981s/iter; left time: 2538.2310s
	iters: 200, epoch: 3 | loss: 0.3700903
	speed: 0.0237s/iter; left time: 610.3966s
Epoch: 3 cost time: 6.292546272277832
Epoch: 3, Steps: 265 | Train Loss: 0.3816209 Vali Loss: 0.6792516 Test Loss: 0.3895329
Validation loss decreased (0.700082 --> 0.679252).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3747780
	speed: 0.1048s/iter; left time: 2683.5303s
	iters: 200, epoch: 4 | loss: 0.4098461
	speed: 0.0228s/iter; left time: 581.9906s
Epoch: 4 cost time: 6.253887176513672
Epoch: 4, Steps: 265 | Train Loss: 0.3764470 Vali Loss: 0.6717814 Test Loss: 0.3859497
Validation loss decreased (0.679252 --> 0.671781).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3790322
	speed: 0.1106s/iter; left time: 2803.7262s
	iters: 200, epoch: 5 | loss: 0.3763665
	speed: 0.0373s/iter; left time: 940.4806s
Epoch: 5 cost time: 8.616172313690186
Epoch: 5, Steps: 265 | Train Loss: 0.3748982 Vali Loss: 0.6695543 Test Loss: 0.3846331
Validation loss decreased (0.671781 --> 0.669554).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3732454
	speed: 0.1251s/iter; left time: 3136.3068s
	iters: 200, epoch: 6 | loss: 0.3553880
	speed: 0.0272s/iter; left time: 679.5384s
Epoch: 6 cost time: 7.378362417221069
Epoch: 6, Steps: 265 | Train Loss: 0.3746273 Vali Loss: 0.6684082 Test Loss: 0.3843230
Validation loss decreased (0.669554 --> 0.668408).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4033411
	speed: 0.1388s/iter; left time: 3444.4334s
	iters: 200, epoch: 7 | loss: 0.3209753
	speed: 0.0327s/iter; left time: 806.8429s
Epoch: 7 cost time: 8.758048057556152
Epoch: 7, Steps: 265 | Train Loss: 0.3741526 Vali Loss: 0.6675129 Test Loss: 0.3843061
Validation loss decreased (0.668408 --> 0.667513).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3603854
	speed: 0.1131s/iter; left time: 2775.9601s
	iters: 200, epoch: 8 | loss: 0.3736768
	speed: 0.0237s/iter; left time: 579.5034s
Epoch: 8 cost time: 7.953332185745239
Epoch: 8, Steps: 265 | Train Loss: 0.3740794 Vali Loss: 0.6669951 Test Loss: 0.3838835
Validation loss decreased (0.667513 --> 0.666995).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3959064
	speed: 0.1321s/iter; left time: 3208.5839s
	iters: 200, epoch: 9 | loss: 0.3969518
	speed: 0.0357s/iter; left time: 863.7026s
Epoch: 9 cost time: 9.615499258041382
Epoch: 9, Steps: 265 | Train Loss: 0.3738788 Vali Loss: 0.6660419 Test Loss: 0.3838311
Validation loss decreased (0.666995 --> 0.666042).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3813614
	speed: 0.1849s/iter; left time: 4439.9640s
	iters: 200, epoch: 10 | loss: 0.3883395
	speed: 0.0275s/iter; left time: 658.6414s
Epoch: 10 cost time: 7.761247873306274
Epoch: 10, Steps: 265 | Train Loss: 0.3738939 Vali Loss: 0.6661152 Test Loss: 0.3840463
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3541218
	speed: 0.1497s/iter; left time: 3555.1834s
	iters: 200, epoch: 11 | loss: 0.4322498
	speed: 0.0260s/iter; left time: 614.9917s
Epoch: 11 cost time: 10.36570143699646
Epoch: 11, Steps: 265 | Train Loss: 0.3739379 Vali Loss: 0.6664548 Test Loss: 0.3838195
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3780733
	speed: 0.1500s/iter; left time: 3523.9665s
	iters: 200, epoch: 12 | loss: 0.3743992
	speed: 0.0330s/iter; left time: 771.5210s
Epoch: 12 cost time: 8.082559585571289
Epoch: 12, Steps: 265 | Train Loss: 0.3737140 Vali Loss: 0.6656492 Test Loss: 0.3839407
Validation loss decreased (0.666042 --> 0.665649).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3669434
	speed: 0.1099s/iter; left time: 2551.0750s
	iters: 200, epoch: 13 | loss: 0.3425941
	speed: 0.0214s/iter; left time: 494.4792s
Epoch: 13 cost time: 6.084858179092407
Epoch: 13, Steps: 265 | Train Loss: 0.3737376 Vali Loss: 0.6663064 Test Loss: 0.3837853
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3750569
	speed: 0.0987s/iter; left time: 2266.6794s
	iters: 200, epoch: 14 | loss: 0.4531589
	speed: 0.0198s/iter; left time: 451.9221s
Epoch: 14 cost time: 6.05845046043396
Epoch: 14, Steps: 265 | Train Loss: 0.3736482 Vali Loss: 0.6657357 Test Loss: 0.3837901
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3541797
	speed: 0.1028s/iter; left time: 2332.7046s
	iters: 200, epoch: 15 | loss: 0.4040014
	speed: 0.0350s/iter; left time: 791.1819s
Epoch: 15 cost time: 8.296966314315796
Epoch: 15, Steps: 265 | Train Loss: 0.3734741 Vali Loss: 0.6660690 Test Loss: 0.3836571
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3752287
	speed: 0.1269s/iter; left time: 2844.9040s
	iters: 200, epoch: 16 | loss: 0.3765269
	speed: 0.0231s/iter; left time: 515.2208s
Epoch: 16 cost time: 6.966558456420898
Epoch: 16, Steps: 265 | Train Loss: 0.3736704 Vali Loss: 0.6660419 Test Loss: 0.3834633
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3664201
	speed: 0.1365s/iter; left time: 3025.4609s
	iters: 200, epoch: 17 | loss: 0.3557424
	speed: 0.0284s/iter; left time: 625.5066s
Epoch: 17 cost time: 7.570184707641602
Epoch: 17, Steps: 265 | Train Loss: 0.3736862 Vali Loss: 0.6659085 Test Loss: 0.3836723
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3733941
	speed: 0.1140s/iter; left time: 2495.4843s
	iters: 200, epoch: 18 | loss: 0.3564011
	speed: 0.0378s/iter; left time: 823.6003s
Epoch: 18 cost time: 8.603939533233643
Epoch: 18, Steps: 265 | Train Loss: 0.3737170 Vali Loss: 0.6648615 Test Loss: 0.3834639
Validation loss decreased (0.665649 --> 0.664862).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3784973
	speed: 0.1275s/iter; left time: 2758.3502s
	iters: 200, epoch: 19 | loss: 0.3501193
	speed: 0.0209s/iter; left time: 449.9719s
Epoch: 19 cost time: 5.808777332305908
Epoch: 19, Steps: 265 | Train Loss: 0.3733836 Vali Loss: 0.6653516 Test Loss: 0.3836564
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3648418
	speed: 0.1013s/iter; left time: 2163.5644s
	iters: 200, epoch: 20 | loss: 0.3678958
	speed: 0.0208s/iter; left time: 443.2624s
Epoch: 20 cost time: 6.196899890899658
Epoch: 20, Steps: 265 | Train Loss: 0.3735297 Vali Loss: 0.6651880 Test Loss: 0.3836906
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3493951
	speed: 0.1001s/iter; left time: 2112.6090s
	iters: 200, epoch: 21 | loss: 0.3733877
	speed: 0.0307s/iter; left time: 643.8195s
Epoch: 21 cost time: 7.553095579147339
Epoch: 21, Steps: 265 | Train Loss: 0.3736223 Vali Loss: 0.6657738 Test Loss: 0.3837265
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3610125
	speed: 0.1155s/iter; left time: 2405.7317s
	iters: 200, epoch: 22 | loss: 0.3985114
	speed: 0.0280s/iter; left time: 581.4732s
Epoch: 22 cost time: 9.239516973495483
Epoch: 22, Steps: 265 | Train Loss: 0.3733879 Vali Loss: 0.6659291 Test Loss: 0.3836049
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4132631
	speed: 0.1491s/iter; left time: 3067.7672s
	iters: 200, epoch: 23 | loss: 0.4239285
	speed: 0.0326s/iter; left time: 667.2179s
Epoch: 23 cost time: 8.675373077392578
Epoch: 23, Steps: 265 | Train Loss: 0.3734448 Vali Loss: 0.6655744 Test Loss: 0.3835069
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3632224
	speed: 0.1392s/iter; left time: 2826.5952s
	iters: 200, epoch: 24 | loss: 0.3800547
	speed: 0.0366s/iter; left time: 739.1839s
Epoch: 24 cost time: 9.380916833877563
Epoch: 24, Steps: 265 | Train Loss: 0.3736476 Vali Loss: 0.6650017 Test Loss: 0.3837198
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3379162
	speed: 0.1116s/iter; left time: 2236.3766s
	iters: 200, epoch: 25 | loss: 0.3505301
	speed: 0.0178s/iter; left time: 354.6966s
Epoch: 25 cost time: 5.904325723648071
Epoch: 25, Steps: 265 | Train Loss: 0.3732223 Vali Loss: 0.6655560 Test Loss: 0.3835206
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3643852
	speed: 0.1337s/iter; left time: 2644.5444s
	iters: 200, epoch: 26 | loss: 0.3655246
	speed: 0.0476s/iter; left time: 937.2618s
Epoch: 26 cost time: 11.34218454360962
Epoch: 26, Steps: 265 | Train Loss: 0.3734367 Vali Loss: 0.6649769 Test Loss: 0.3836875
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3345257
	speed: 0.1569s/iter; left time: 3060.8485s
	iters: 200, epoch: 27 | loss: 0.3501090
	speed: 0.0250s/iter; left time: 484.6106s
Epoch: 27 cost time: 8.816978216171265
Epoch: 27, Steps: 265 | Train Loss: 0.3733127 Vali Loss: 0.6648853 Test Loss: 0.3836287
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3615791
	speed: 0.1804s/iter; left time: 3471.0866s
	iters: 200, epoch: 28 | loss: 0.4007482
	speed: 0.0363s/iter; left time: 694.8297s
Epoch: 28 cost time: 10.322508573532104
Epoch: 28, Steps: 265 | Train Loss: 0.3734328 Vali Loss: 0.6658572 Test Loss: 0.3837115
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3818447
	speed: 0.1585s/iter; left time: 3008.0668s
	iters: 200, epoch: 29 | loss: 0.3300970
	speed: 0.0267s/iter; left time: 504.0388s
Epoch: 29 cost time: 9.398385763168335
Epoch: 29, Steps: 265 | Train Loss: 0.3735374 Vali Loss: 0.6653411 Test Loss: 0.3836769
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3339370
	speed: 0.1150s/iter; left time: 2152.3759s
	iters: 200, epoch: 30 | loss: 0.4031306
	speed: 0.0228s/iter; left time: 424.5449s
Epoch: 30 cost time: 6.275314092636108
Epoch: 30, Steps: 265 | Train Loss: 0.3735595 Vali Loss: 0.6659855 Test Loss: 0.3836917
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3762716
	speed: 0.1319s/iter; left time: 2434.3464s
	iters: 200, epoch: 31 | loss: 0.3340749
	speed: 0.0220s/iter; left time: 404.5016s
Epoch: 31 cost time: 6.8314855098724365
Epoch: 31, Steps: 265 | Train Loss: 0.3734760 Vali Loss: 0.6656452 Test Loss: 0.3837361
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4064755
	speed: 0.1043s/iter; left time: 1896.9269s
	iters: 200, epoch: 32 | loss: 0.3751111
	speed: 0.0208s/iter; left time: 375.8757s
Epoch: 32 cost time: 6.7833075523376465
Epoch: 32, Steps: 265 | Train Loss: 0.3733197 Vali Loss: 0.6651292 Test Loss: 0.3836384
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3786963
	speed: 0.1199s/iter; left time: 2149.3042s
	iters: 200, epoch: 33 | loss: 0.3527254
	speed: 0.0294s/iter; left time: 523.4619s
Epoch: 33 cost time: 8.120851278305054
Epoch: 33, Steps: 265 | Train Loss: 0.3733818 Vali Loss: 0.6658980 Test Loss: 0.3836378
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3669701
	speed: 0.1337s/iter; left time: 2361.4098s
	iters: 200, epoch: 34 | loss: 0.3632585
	speed: 0.0190s/iter; left time: 333.8397s
Epoch: 34 cost time: 6.994848012924194
Epoch: 34, Steps: 265 | Train Loss: 0.3733446 Vali Loss: 0.6654696 Test Loss: 0.3836288
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3995050
	speed: 0.1458s/iter; left time: 2536.1176s
	iters: 200, epoch: 35 | loss: 0.3718136
	speed: 0.0325s/iter; left time: 561.7369s
Epoch: 35 cost time: 9.524857521057129
Epoch: 35, Steps: 265 | Train Loss: 0.3732807 Vali Loss: 0.6654854 Test Loss: 0.3835990
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3558476
	speed: 0.1203s/iter; left time: 2060.0416s
	iters: 200, epoch: 36 | loss: 0.3417955
	speed: 0.0196s/iter; left time: 334.4525s
Epoch: 36 cost time: 6.053166627883911
Epoch: 36, Steps: 265 | Train Loss: 0.3734025 Vali Loss: 0.6656166 Test Loss: 0.3836196
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3780562
	speed: 0.0995s/iter; left time: 1677.1625s
	iters: 200, epoch: 37 | loss: 0.3400506
	speed: 0.0389s/iter; left time: 652.4986s
Epoch: 37 cost time: 8.009038925170898
Epoch: 37, Steps: 265 | Train Loss: 0.3734782 Vali Loss: 0.6650859 Test Loss: 0.3836424
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4202150
	speed: 0.0992s/iter; left time: 1646.5150s
	iters: 200, epoch: 38 | loss: 0.3920295
	speed: 0.0218s/iter; left time: 360.1318s
Epoch: 38 cost time: 7.289202690124512
Epoch: 38, Steps: 265 | Train Loss: 0.3734666 Vali Loss: 0.6651351 Test Loss: 0.3836885
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_336_FITS_ETTm1_ftM_sl180_ll48_pl336_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3834671676158905, mae:0.3899341821670532, rse:0.5892674326896667, corr:[0.547515   0.5490491  0.5458717  0.54395306 0.54333574 0.5424808
 0.5411871  0.5406484  0.540875   0.541315   0.54168797 0.5418442
 0.5419731  0.54134494 0.5398011  0.5379153  0.53610027 0.5344459
 0.532809   0.5310694  0.5292599  0.5273165  0.5251771  0.5229417
 0.52064764 0.51845145 0.5166189  0.515138   0.5140895  0.51345253
 0.5133524  0.51363903 0.51406986 0.51451993 0.514754   0.5147408
 0.51431775 0.51357937 0.5130181  0.5127319  0.5127353  0.5126821
 0.5124443  0.51220113 0.5120553  0.51206297 0.51222306 0.5123831
 0.512471   0.5124522  0.5124562  0.5125294  0.51262456 0.51261544
 0.5125442  0.5124792  0.5125946  0.5126686  0.51259434 0.5122918
 0.5119927  0.51186806 0.5118789  0.5118548  0.51167154 0.51156163
 0.5116336  0.51186764 0.51225245 0.51257145 0.51272124 0.5127558
 0.5128771  0.5131496  0.51351655 0.5136371  0.51351005 0.5132599
 0.5130784  0.512996   0.512962   0.5128614  0.5127085  0.5124724
 0.51224184 0.5121212  0.5120369  0.51176053 0.5112801  0.5109588
 0.5109412  0.51110226 0.51108295 0.5106914  0.5100018  0.50924736
 0.50859946 0.50793594 0.5070029  0.5058947  0.5052249  0.50541776
 0.5062445  0.50708747 0.50765806 0.5081905  0.50909257 0.5103916
 0.51160425 0.51226246 0.51246315 0.5126773  0.51298136 0.51324207
 0.5131725  0.51281005 0.51224947 0.51168156 0.5112435  0.51079434
 0.51027066 0.50956774 0.5088779  0.5084961  0.50828624 0.5079644
 0.50751185 0.5071303  0.50701666 0.5071359  0.5071772  0.5068763
 0.50631833 0.5059174  0.5057681  0.50561273 0.505395   0.5050248
 0.50466084 0.50457966 0.5046668  0.50468767 0.50453895 0.50438637
 0.5044531  0.50480753 0.50510466 0.5051806  0.5051412  0.50486875
 0.5045981  0.50447786 0.5044467  0.5044701  0.5044046  0.5043977
 0.5044553  0.5045584  0.50458395 0.50464654 0.5047832  0.5049876
 0.5051288  0.5050824  0.5048706  0.50484866 0.50514394 0.5056881
 0.50628334 0.5066339  0.50661856 0.5065018  0.50632143 0.50606716
 0.50579065 0.50551546 0.5053955  0.5055236  0.50574774 0.5059629
 0.50603825 0.505993   0.50590473 0.5058911  0.505987   0.5061477
 0.50632167 0.50646675 0.5065662  0.5066006  0.5065978  0.50660825
 0.5066036  0.50652874 0.5062258  0.50558645 0.50490093 0.50452304
 0.5044483  0.5043789  0.5041354  0.5039282  0.50373966 0.50363344
 0.503621   0.50351954 0.5033755  0.5031551  0.5027367  0.50216275
 0.5015726  0.5010787  0.50053114 0.49972752 0.4986721  0.49758837
 0.49667272 0.49589664 0.49521613 0.49461406 0.4940931  0.49376535
 0.49366522 0.4937723  0.49390453 0.49398792 0.49400225 0.49384794
 0.49355304 0.49337718 0.49329343 0.4931952  0.49301752 0.4927638
 0.49252108 0.49242526 0.4923569  0.4924151  0.49252293 0.49253935
 0.49263513 0.49273935 0.4929257  0.49303702 0.49301887 0.49293146
 0.49292487 0.4930022  0.49317417 0.4933205  0.4933097  0.49317962
 0.49309593 0.49324533 0.493427   0.49354547 0.49350056 0.49340147
 0.4934841  0.49373633 0.4939847  0.49414784 0.49425083 0.49445957
 0.49483058 0.49510255 0.4951965  0.49518684 0.49522695 0.49530637
 0.49545655 0.49553218 0.49556822 0.49559143 0.49572    0.4959477
 0.49622107 0.49650532 0.49664447 0.4966608  0.49670658 0.49682766
 0.49696836 0.4969104  0.49661067 0.4961912  0.49580547 0.49546486
 0.49488154 0.49399474 0.4930138  0.49215716 0.49163565 0.4913287
 0.4911367  0.49090227 0.49087688 0.49120104 0.49170062 0.49207804
 0.49218562 0.4921059  0.49213356 0.4922953  0.4924074  0.4922954
 0.4920123  0.49166116 0.49133933 0.4910822  0.49078146 0.49035102
 0.48985785 0.48941258 0.48917788 0.48909202 0.48872286 0.48824033
 0.48794165 0.48783132 0.48783633 0.48784584 0.48753944 0.4870757
 0.48667547 0.48644066 0.48618278 0.4857833  0.48538005 0.48526806
 0.48542053 0.4854772  0.4850794  0.48481455 0.48535347 0.48582816]
