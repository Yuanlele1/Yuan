Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=42, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4741632.0
params:  5418.0
Trainable parameters:  5418
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5417125
	speed: 0.0227s/iter; left time: 591.3579s
	iters: 200, epoch: 1 | loss: 0.4573222
	speed: 0.0170s/iter; left time: 439.6484s
Epoch: 1 cost time: 5.010785341262817
Epoch: 1, Steps: 261 | Train Loss: 0.5870431 Vali Loss: 1.0596476 Test Loss: 0.4722501
Validation loss decreased (inf --> 1.059648).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4250081
	speed: 0.1016s/iter; left time: 2615.6491s
	iters: 200, epoch: 2 | loss: 0.3982974
	speed: 0.0159s/iter; left time: 406.8692s
Epoch: 2 cost time: 4.749797105789185
Epoch: 2, Steps: 261 | Train Loss: 0.4387463 Vali Loss: 0.9924363 Test Loss: 0.4331320
Validation loss decreased (1.059648 --> 0.992436).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4541915
	speed: 0.0789s/iter; left time: 2011.0253s
	iters: 200, epoch: 3 | loss: 0.4322328
	speed: 0.0158s/iter; left time: 401.6939s
Epoch: 3 cost time: 4.762893915176392
Epoch: 3, Steps: 261 | Train Loss: 0.4229789 Vali Loss: 0.9779416 Test Loss: 0.4276631
Validation loss decreased (0.992436 --> 0.977942).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4419247
	speed: 0.0774s/iter; left time: 1951.1411s
	iters: 200, epoch: 4 | loss: 0.4536683
	speed: 0.0160s/iter; left time: 402.5524s
Epoch: 4 cost time: 4.8716230392456055
Epoch: 4, Steps: 261 | Train Loss: 0.4184414 Vali Loss: 0.9718238 Test Loss: 0.4266050
Validation loss decreased (0.977942 --> 0.971824).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4088643
	speed: 0.0774s/iter; left time: 1932.3120s
	iters: 200, epoch: 5 | loss: 0.3942257
	speed: 0.0156s/iter; left time: 387.3990s
Epoch: 5 cost time: 4.693769454956055
Epoch: 5, Steps: 261 | Train Loss: 0.4164706 Vali Loss: 0.9667835 Test Loss: 0.4264385
Validation loss decreased (0.971824 --> 0.966784).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3855686
	speed: 0.0769s/iter; left time: 1897.9611s
	iters: 200, epoch: 6 | loss: 0.3619510
	speed: 0.0162s/iter; left time: 399.6079s
Epoch: 6 cost time: 4.79450798034668
Epoch: 6, Steps: 261 | Train Loss: 0.4153466 Vali Loss: 0.9660708 Test Loss: 0.4261985
Validation loss decreased (0.966784 --> 0.966071).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4299290
	speed: 0.0762s/iter; left time: 1862.0589s
	iters: 200, epoch: 7 | loss: 0.4341243
	speed: 0.0160s/iter; left time: 389.0279s
Epoch: 7 cost time: 4.848582983016968
Epoch: 7, Steps: 261 | Train Loss: 0.4148908 Vali Loss: 0.9641212 Test Loss: 0.4264835
Validation loss decreased (0.966071 --> 0.964121).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4392464
	speed: 0.0820s/iter; left time: 1981.4890s
	iters: 200, epoch: 8 | loss: 0.4150590
	speed: 0.0176s/iter; left time: 424.2197s
Epoch: 8 cost time: 4.988385200500488
Epoch: 8, Steps: 261 | Train Loss: 0.4144401 Vali Loss: 0.9632189 Test Loss: 0.4265162
Validation loss decreased (0.964121 --> 0.963219).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3858407
	speed: 0.0749s/iter; left time: 1790.9933s
	iters: 200, epoch: 9 | loss: 0.4200022
	speed: 0.0162s/iter; left time: 385.4344s
Epoch: 9 cost time: 4.689101457595825
Epoch: 9, Steps: 261 | Train Loss: 0.4144081 Vali Loss: 0.9631439 Test Loss: 0.4266458
Validation loss decreased (0.963219 --> 0.963144).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4063955
	speed: 0.0772s/iter; left time: 1826.2422s
	iters: 200, epoch: 10 | loss: 0.3728522
	speed: 0.0158s/iter; left time: 372.4185s
Epoch: 10 cost time: 4.670655250549316
Epoch: 10, Steps: 261 | Train Loss: 0.4142808 Vali Loss: 0.9622926 Test Loss: 0.4267810
Validation loss decreased (0.963144 --> 0.962293).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4115168
	speed: 0.0747s/iter; left time: 1746.2537s
	iters: 200, epoch: 11 | loss: 0.4152831
	speed: 0.0158s/iter; left time: 368.5714s
Epoch: 11 cost time: 4.65325927734375
Epoch: 11, Steps: 261 | Train Loss: 0.4142297 Vali Loss: 0.9611799 Test Loss: 0.4270359
Validation loss decreased (0.962293 --> 0.961180).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4157235
	speed: 0.0771s/iter; left time: 1784.1745s
	iters: 200, epoch: 12 | loss: 0.4268558
	speed: 0.0163s/iter; left time: 374.2581s
Epoch: 12 cost time: 4.882640838623047
Epoch: 12, Steps: 261 | Train Loss: 0.4142142 Vali Loss: 0.9610733 Test Loss: 0.4270930
Validation loss decreased (0.961180 --> 0.961073).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3971528
	speed: 0.0761s/iter; left time: 1740.1243s
	iters: 200, epoch: 13 | loss: 0.4181488
	speed: 0.0157s/iter; left time: 357.9785s
Epoch: 13 cost time: 4.595938444137573
Epoch: 13, Steps: 261 | Train Loss: 0.4141484 Vali Loss: 0.9623742 Test Loss: 0.4268101
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4646950
	speed: 0.0775s/iter; left time: 1752.2864s
	iters: 200, epoch: 14 | loss: 0.3803812
	speed: 0.0158s/iter; left time: 356.6908s
Epoch: 14 cost time: 4.794167518615723
Epoch: 14, Steps: 261 | Train Loss: 0.4141023 Vali Loss: 0.9619090 Test Loss: 0.4270309
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4402207
	speed: 0.0786s/iter; left time: 1756.4597s
	iters: 200, epoch: 15 | loss: 0.4307743
	speed: 0.0172s/iter; left time: 382.3252s
Epoch: 15 cost time: 4.975113868713379
Epoch: 15, Steps: 261 | Train Loss: 0.4140515 Vali Loss: 0.9617811 Test Loss: 0.4271559
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4347352
	speed: 0.0868s/iter; left time: 1917.7567s
	iters: 200, epoch: 16 | loss: 0.4702192
	speed: 0.0163s/iter; left time: 358.0177s
Epoch: 16 cost time: 4.933193683624268
Epoch: 16, Steps: 261 | Train Loss: 0.4139366 Vali Loss: 0.9607483 Test Loss: 0.4271256
Validation loss decreased (0.961073 --> 0.960748).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4471862
	speed: 0.0798s/iter; left time: 1740.8415s
	iters: 200, epoch: 17 | loss: 0.4124957
	speed: 0.0164s/iter; left time: 355.3332s
Epoch: 17 cost time: 4.90621018409729
Epoch: 17, Steps: 261 | Train Loss: 0.4139998 Vali Loss: 0.9614060 Test Loss: 0.4270394
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4241502
	speed: 0.0833s/iter; left time: 1795.3357s
	iters: 200, epoch: 18 | loss: 0.4374085
	speed: 0.0165s/iter; left time: 354.9335s
Epoch: 18 cost time: 4.946099281311035
Epoch: 18, Steps: 261 | Train Loss: 0.4139681 Vali Loss: 0.9605021 Test Loss: 0.4273684
Validation loss decreased (0.960748 --> 0.960502).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4013028
	speed: 0.0883s/iter; left time: 1880.0590s
	iters: 200, epoch: 19 | loss: 0.4090557
	speed: 0.0168s/iter; left time: 356.3568s
Epoch: 19 cost time: 4.780611753463745
Epoch: 19, Steps: 261 | Train Loss: 0.4139258 Vali Loss: 0.9610795 Test Loss: 0.4272141
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3733543
	speed: 0.0765s/iter; left time: 1609.1052s
	iters: 200, epoch: 20 | loss: 0.4410717
	speed: 0.0164s/iter; left time: 344.3468s
Epoch: 20 cost time: 4.783248662948608
Epoch: 20, Steps: 261 | Train Loss: 0.4139446 Vali Loss: 0.9610139 Test Loss: 0.4273278
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3985043
	speed: 0.0778s/iter; left time: 1616.4356s
	iters: 200, epoch: 21 | loss: 0.4084993
	speed: 0.0175s/iter; left time: 361.6082s
Epoch: 21 cost time: 5.404149055480957
Epoch: 21, Steps: 261 | Train Loss: 0.4139329 Vali Loss: 0.9610409 Test Loss: 0.4274304
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4199617
	speed: 0.0837s/iter; left time: 1718.5417s
	iters: 200, epoch: 22 | loss: 0.4066217
	speed: 0.0164s/iter; left time: 334.9408s
Epoch: 22 cost time: 4.834609746932983
Epoch: 22, Steps: 261 | Train Loss: 0.4138394 Vali Loss: 0.9607778 Test Loss: 0.4273279
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3668653
	speed: 0.0864s/iter; left time: 1751.3396s
	iters: 200, epoch: 23 | loss: 0.4123819
	speed: 0.0163s/iter; left time: 329.3415s
Epoch: 23 cost time: 5.68640661239624
Epoch: 23, Steps: 261 | Train Loss: 0.4138420 Vali Loss: 0.9603682 Test Loss: 0.4272353
Validation loss decreased (0.960502 --> 0.960368).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4253174
	speed: 0.0772s/iter; left time: 1543.7982s
	iters: 200, epoch: 24 | loss: 0.3555484
	speed: 0.0158s/iter; left time: 313.7970s
Epoch: 24 cost time: 4.703502893447876
Epoch: 24, Steps: 261 | Train Loss: 0.4138674 Vali Loss: 0.9605654 Test Loss: 0.4271967
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3692504
	speed: 0.0774s/iter; left time: 1528.6301s
	iters: 200, epoch: 25 | loss: 0.4042448
	speed: 0.0160s/iter; left time: 314.5870s
Epoch: 25 cost time: 4.8443763256073
Epoch: 25, Steps: 261 | Train Loss: 0.4139443 Vali Loss: 0.9596563 Test Loss: 0.4271741
Validation loss decreased (0.960368 --> 0.959656).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4392197
	speed: 0.0785s/iter; left time: 1529.0913s
	iters: 200, epoch: 26 | loss: 0.4087841
	speed: 0.0156s/iter; left time: 301.4982s
Epoch: 26 cost time: 4.785336017608643
Epoch: 26, Steps: 261 | Train Loss: 0.4137383 Vali Loss: 0.9607152 Test Loss: 0.4272116
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3993415
	speed: 0.0769s/iter; left time: 1477.3676s
	iters: 200, epoch: 27 | loss: 0.4219726
	speed: 0.0160s/iter; left time: 306.5967s
Epoch: 27 cost time: 4.760732412338257
Epoch: 27, Steps: 261 | Train Loss: 0.4138237 Vali Loss: 0.9601958 Test Loss: 0.4274965
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3769947
	speed: 0.0785s/iter; left time: 1487.0300s
	iters: 200, epoch: 28 | loss: 0.4053620
	speed: 0.0164s/iter; left time: 308.7445s
Epoch: 28 cost time: 4.785193681716919
Epoch: 28, Steps: 261 | Train Loss: 0.4138761 Vali Loss: 0.9600663 Test Loss: 0.4271043
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4159922
	speed: 0.0769s/iter; left time: 1437.7774s
	iters: 200, epoch: 29 | loss: 0.4327229
	speed: 0.0159s/iter; left time: 295.6825s
Epoch: 29 cost time: 5.564659833908081
Epoch: 29, Steps: 261 | Train Loss: 0.4137392 Vali Loss: 0.9603921 Test Loss: 0.4274376
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4012469
	speed: 0.0861s/iter; left time: 1587.4865s
	iters: 200, epoch: 30 | loss: 0.3990974
	speed: 0.0165s/iter; left time: 302.9883s
Epoch: 30 cost time: 4.933861494064331
Epoch: 30, Steps: 261 | Train Loss: 0.4137779 Vali Loss: 0.9610164 Test Loss: 0.4272298
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3893084
	speed: 0.0768s/iter; left time: 1396.0689s
	iters: 200, epoch: 31 | loss: 0.4513396
	speed: 0.0151s/iter; left time: 273.5707s
Epoch: 31 cost time: 4.508490562438965
Epoch: 31, Steps: 261 | Train Loss: 0.4138092 Vali Loss: 0.9603438 Test Loss: 0.4273448
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3957071
	speed: 0.0753s/iter; left time: 1349.0485s
	iters: 200, epoch: 32 | loss: 0.4450094
	speed: 0.0154s/iter; left time: 274.4573s
Epoch: 32 cost time: 4.571638107299805
Epoch: 32, Steps: 261 | Train Loss: 0.4137937 Vali Loss: 0.9600791 Test Loss: 0.4272646
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4127375
	speed: 0.0757s/iter; left time: 1336.6281s
	iters: 200, epoch: 33 | loss: 0.4032920
	speed: 0.0155s/iter; left time: 272.2602s
Epoch: 33 cost time: 4.705281019210815
Epoch: 33, Steps: 261 | Train Loss: 0.4137863 Vali Loss: 0.9596334 Test Loss: 0.4273779
Validation loss decreased (0.959656 --> 0.959633).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3980433
	speed: 0.0763s/iter; left time: 1325.8760s
	iters: 200, epoch: 34 | loss: 0.3962884
	speed: 0.0159s/iter; left time: 274.5578s
Epoch: 34 cost time: 4.6948676109313965
Epoch: 34, Steps: 261 | Train Loss: 0.4138536 Vali Loss: 0.9606316 Test Loss: 0.4273627
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4244322
	speed: 0.0771s/iter; left time: 1320.2180s
	iters: 200, epoch: 35 | loss: 0.3882584
	speed: 0.0163s/iter; left time: 277.5289s
Epoch: 35 cost time: 4.822523832321167
Epoch: 35, Steps: 261 | Train Loss: 0.4137203 Vali Loss: 0.9608701 Test Loss: 0.4273122
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4275182
	speed: 0.0855s/iter; left time: 1442.6823s
	iters: 200, epoch: 36 | loss: 0.3953961
	speed: 0.0160s/iter; left time: 267.8494s
Epoch: 36 cost time: 4.77491307258606
Epoch: 36, Steps: 261 | Train Loss: 0.4136430 Vali Loss: 0.9610280 Test Loss: 0.4272206
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4419031
	speed: 0.0775s/iter; left time: 1287.4067s
	iters: 200, epoch: 37 | loss: 0.4279530
	speed: 0.0160s/iter; left time: 263.6642s
Epoch: 37 cost time: 4.82220458984375
Epoch: 37, Steps: 261 | Train Loss: 0.4136872 Vali Loss: 0.9598536 Test Loss: 0.4273375
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4089177
	speed: 0.0776s/iter; left time: 1268.1865s
	iters: 200, epoch: 38 | loss: 0.4341272
	speed: 0.0157s/iter; left time: 255.0821s
Epoch: 38 cost time: 4.701167821884155
Epoch: 38, Steps: 261 | Train Loss: 0.4137484 Vali Loss: 0.9606389 Test Loss: 0.4273517
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4293953
	speed: 0.0765s/iter; left time: 1230.4206s
	iters: 200, epoch: 39 | loss: 0.4140413
	speed: 0.0158s/iter; left time: 251.8981s
Epoch: 39 cost time: 4.673311948776245
Epoch: 39, Steps: 261 | Train Loss: 0.4136277 Vali Loss: 0.9605587 Test Loss: 0.4273786
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4446903
	speed: 0.0757s/iter; left time: 1197.1026s
	iters: 200, epoch: 40 | loss: 0.3933002
	speed: 0.0159s/iter; left time: 249.7020s
Epoch: 40 cost time: 4.705993175506592
Epoch: 40, Steps: 261 | Train Loss: 0.4137602 Vali Loss: 0.9609030 Test Loss: 0.4273658
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4136178
	speed: 0.0741s/iter; left time: 1153.8047s
	iters: 200, epoch: 41 | loss: 0.4211274
	speed: 0.0161s/iter; left time: 248.2576s
Epoch: 41 cost time: 4.696253061294556
Epoch: 41, Steps: 261 | Train Loss: 0.4137904 Vali Loss: 0.9599457 Test Loss: 0.4273307
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4106633
	speed: 0.0751s/iter; left time: 1148.2965s
	iters: 200, epoch: 42 | loss: 0.3943248
	speed: 0.0155s/iter; left time: 235.9058s
Epoch: 42 cost time: 4.619025230407715
Epoch: 42, Steps: 261 | Train Loss: 0.4138098 Vali Loss: 0.9608808 Test Loss: 0.4274483
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4003898
	speed: 0.0787s/iter; left time: 1183.2459s
	iters: 200, epoch: 43 | loss: 0.3795803
	speed: 0.0162s/iter; left time: 242.4621s
Epoch: 43 cost time: 4.7701051235198975
Epoch: 43, Steps: 261 | Train Loss: 0.4137732 Vali Loss: 0.9607741 Test Loss: 0.4273405
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4166042
	speed: 0.0792s/iter; left time: 1169.9800s
	iters: 200, epoch: 44 | loss: 0.3902031
	speed: 0.0167s/iter; left time: 244.4977s
Epoch: 44 cost time: 4.897149562835693
Epoch: 44, Steps: 261 | Train Loss: 0.4137643 Vali Loss: 0.9604047 Test Loss: 0.4273534
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4324940
	speed: 0.0781s/iter; left time: 1134.4693s
	iters: 200, epoch: 45 | loss: 0.4451395
	speed: 0.0168s/iter; left time: 242.7780s
Epoch: 45 cost time: 4.83814549446106
Epoch: 45, Steps: 261 | Train Loss: 0.4137679 Vali Loss: 0.9615620 Test Loss: 0.4273769
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4106900
	speed: 0.0760s/iter; left time: 1083.9086s
	iters: 200, epoch: 46 | loss: 0.4429165
	speed: 0.0166s/iter; left time: 235.1618s
Epoch: 46 cost time: 4.786643743515015
Epoch: 46, Steps: 261 | Train Loss: 0.4136854 Vali Loss: 0.9605473 Test Loss: 0.4273536
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4320985
	speed: 0.0763s/iter; left time: 1067.2689s
	iters: 200, epoch: 47 | loss: 0.4686264
	speed: 0.0157s/iter; left time: 218.6700s
Epoch: 47 cost time: 4.699487924575806
Epoch: 47, Steps: 261 | Train Loss: 0.4136969 Vali Loss: 0.9597576 Test Loss: 0.4273276
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4219514
	speed: 0.0759s/iter; left time: 1042.6432s
	iters: 200, epoch: 48 | loss: 0.3752214
	speed: 0.0160s/iter; left time: 217.6706s
Epoch: 48 cost time: 4.715476989746094
Epoch: 48, Steps: 261 | Train Loss: 0.4136601 Vali Loss: 0.9615268 Test Loss: 0.4273592
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.3994422
	speed: 0.0800s/iter; left time: 1078.0302s
	iters: 200, epoch: 49 | loss: 0.4397615
	speed: 0.0157s/iter; left time: 210.0057s
Epoch: 49 cost time: 4.787720680236816
Epoch: 49, Steps: 261 | Train Loss: 0.4136893 Vali Loss: 0.9604436 Test Loss: 0.4274471
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.3617134
	speed: 0.0772s/iter; left time: 1019.4203s
	iters: 200, epoch: 50 | loss: 0.4032292
	speed: 0.0151s/iter; left time: 198.5900s
Epoch: 50 cost time: 4.608962059020996
Epoch: 50, Steps: 261 | Train Loss: 0.4136250 Vali Loss: 0.9602916 Test Loss: 0.4273660
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4469591
	speed: 0.0758s/iter; left time: 981.4125s
	iters: 200, epoch: 51 | loss: 0.4292681
	speed: 0.0162s/iter; left time: 208.1320s
Epoch: 51 cost time: 4.6522135734558105
Epoch: 51, Steps: 261 | Train Loss: 0.4135928 Vali Loss: 0.9612675 Test Loss: 0.4273987
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4135286
	speed: 0.0786s/iter; left time: 997.4432s
	iters: 200, epoch: 52 | loss: 0.4096514
	speed: 0.0167s/iter; left time: 210.1315s
Epoch: 52 cost time: 4.940905570983887
Epoch: 52, Steps: 261 | Train Loss: 0.4137429 Vali Loss: 0.9595484 Test Loss: 0.4274281
Validation loss decreased (0.959633 --> 0.959548).  Saving model ...
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.3737750
	speed: 0.0791s/iter; left time: 983.6822s
	iters: 200, epoch: 53 | loss: 0.3987698
	speed: 0.0174s/iter; left time: 215.0566s
Epoch: 53 cost time: 4.928601264953613
Epoch: 53, Steps: 261 | Train Loss: 0.4137181 Vali Loss: 0.9607944 Test Loss: 0.4274091
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.3935159
	speed: 0.0803s/iter; left time: 977.6486s
	iters: 200, epoch: 54 | loss: 0.4150957
	speed: 0.0170s/iter; left time: 205.0799s
Epoch: 54 cost time: 4.878844261169434
Epoch: 54, Steps: 261 | Train Loss: 0.4136472 Vali Loss: 0.9611276 Test Loss: 0.4274025
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.3841950
	speed: 0.0770s/iter; left time: 916.2556s
	iters: 200, epoch: 55 | loss: 0.4218853
	speed: 0.0156s/iter; left time: 184.5503s
Epoch: 55 cost time: 4.7619853019714355
Epoch: 55, Steps: 261 | Train Loss: 0.4137092 Vali Loss: 0.9601832 Test Loss: 0.4273833
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4357638
	speed: 0.0774s/iter; left time: 901.1049s
	iters: 200, epoch: 56 | loss: 0.4335603
	speed: 0.0156s/iter; left time: 180.2979s
Epoch: 56 cost time: 4.683790683746338
Epoch: 56, Steps: 261 | Train Loss: 0.4137802 Vali Loss: 0.9601648 Test Loss: 0.4273964
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.4267188
	speed: 0.0763s/iter; left time: 868.8033s
	iters: 200, epoch: 57 | loss: 0.4168107
	speed: 0.0167s/iter; left time: 188.4412s
Epoch: 57 cost time: 4.821299314498901
Epoch: 57, Steps: 261 | Train Loss: 0.4136693 Vali Loss: 0.9609441 Test Loss: 0.4273755
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.4179152
	speed: 0.0785s/iter; left time: 873.4813s
	iters: 200, epoch: 58 | loss: 0.4116212
	speed: 0.0155s/iter; left time: 170.9646s
Epoch: 58 cost time: 4.698091506958008
Epoch: 58, Steps: 261 | Train Loss: 0.4137626 Vali Loss: 0.9603099 Test Loss: 0.4273989
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.3815916
	speed: 0.0823s/iter; left time: 893.8167s
	iters: 200, epoch: 59 | loss: 0.3920799
	speed: 0.0177s/iter; left time: 190.8402s
Epoch: 59 cost time: 5.031117677688599
Epoch: 59, Steps: 261 | Train Loss: 0.4136063 Vali Loss: 0.9601696 Test Loss: 0.4274012
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.4094348
	speed: 0.0868s/iter; left time: 920.7011s
	iters: 200, epoch: 60 | loss: 0.3941427
	speed: 0.0180s/iter; left time: 188.6360s
Epoch: 60 cost time: 5.637702941894531
Epoch: 60, Steps: 261 | Train Loss: 0.4135891 Vali Loss: 0.9610938 Test Loss: 0.4273658
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.3964549
	speed: 0.0858s/iter; left time: 886.8969s
	iters: 200, epoch: 61 | loss: 0.4286249
	speed: 0.0158s/iter; left time: 162.0872s
Epoch: 61 cost time: 4.68500542640686
Epoch: 61, Steps: 261 | Train Loss: 0.4137008 Vali Loss: 0.9611440 Test Loss: 0.4273877
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.3855408
	speed: 0.0758s/iter; left time: 763.5714s
	iters: 200, epoch: 62 | loss: 0.3718818
	speed: 0.0159s/iter; left time: 158.8107s
Epoch: 62 cost time: 4.707191467285156
Epoch: 62, Steps: 261 | Train Loss: 0.4136787 Vali Loss: 0.9606271 Test Loss: 0.4273959
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.4165103
	speed: 0.0752s/iter; left time: 738.5423s
	iters: 200, epoch: 63 | loss: 0.4649456
	speed: 0.0159s/iter; left time: 154.2608s
Epoch: 63 cost time: 4.635752439498901
Epoch: 63, Steps: 261 | Train Loss: 0.4137045 Vali Loss: 0.9594278 Test Loss: 0.4273812
Validation loss decreased (0.959548 --> 0.959428).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.4263181
	speed: 0.0771s/iter; left time: 737.0448s
	iters: 200, epoch: 64 | loss: 0.3920403
	speed: 0.0160s/iter; left time: 151.1316s
Epoch: 64 cost time: 4.771718502044678
Epoch: 64, Steps: 261 | Train Loss: 0.4136462 Vali Loss: 0.9611040 Test Loss: 0.4273841
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.4250122
	speed: 0.0815s/iter; left time: 758.0741s
	iters: 200, epoch: 65 | loss: 0.4050994
	speed: 0.0162s/iter; left time: 148.5825s
Epoch: 65 cost time: 4.906520843505859
Epoch: 65, Steps: 261 | Train Loss: 0.4137223 Vali Loss: 0.9602271 Test Loss: 0.4274014
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.3858403
	speed: 0.0756s/iter; left time: 682.7825s
	iters: 200, epoch: 66 | loss: 0.4117614
	speed: 0.0160s/iter; left time: 142.8458s
Epoch: 66 cost time: 4.645637273788452
Epoch: 66, Steps: 261 | Train Loss: 0.4137116 Vali Loss: 0.9608121 Test Loss: 0.4274019
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.3994612
	speed: 0.0762s/iter; left time: 668.7139s
	iters: 200, epoch: 67 | loss: 0.4056192
	speed: 0.0165s/iter; left time: 143.2013s
Epoch: 67 cost time: 4.934507369995117
Epoch: 67, Steps: 261 | Train Loss: 0.4136427 Vali Loss: 0.9602078 Test Loss: 0.4273948
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.4133340
	speed: 0.0812s/iter; left time: 691.3793s
	iters: 200, epoch: 68 | loss: 0.4371124
	speed: 0.0318s/iter; left time: 267.2471s
Epoch: 68 cost time: 6.498872518539429
Epoch: 68, Steps: 261 | Train Loss: 0.4137572 Vali Loss: 0.9595636 Test Loss: 0.4273944
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.4003384
	speed: 0.0790s/iter; left time: 652.0777s
	iters: 200, epoch: 69 | loss: 0.4083566
	speed: 0.0161s/iter; left time: 131.1273s
Epoch: 69 cost time: 4.745865821838379
Epoch: 69, Steps: 261 | Train Loss: 0.4136572 Vali Loss: 0.9601634 Test Loss: 0.4273961
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.4360791
	speed: 0.0772s/iter; left time: 616.6890s
	iters: 200, epoch: 70 | loss: 0.4291919
	speed: 0.0159s/iter; left time: 125.3379s
Epoch: 70 cost time: 4.801196098327637
Epoch: 70, Steps: 261 | Train Loss: 0.4136634 Vali Loss: 0.9607331 Test Loss: 0.4273858
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.4297371
	speed: 0.1154s/iter; left time: 891.9675s
	iters: 200, epoch: 71 | loss: 0.3933216
	speed: 0.0158s/iter; left time: 120.2947s
Epoch: 71 cost time: 4.619635820388794
Epoch: 71, Steps: 261 | Train Loss: 0.4137417 Vali Loss: 0.9599440 Test Loss: 0.4273848
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.3874972
	speed: 0.0779s/iter; left time: 581.5987s
	iters: 200, epoch: 72 | loss: 0.3928656
	speed: 0.0162s/iter; left time: 119.1234s
Epoch: 72 cost time: 4.810455322265625
Epoch: 72, Steps: 261 | Train Loss: 0.4137300 Vali Loss: 0.9608661 Test Loss: 0.4273913
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.4052823
	speed: 0.0784s/iter; left time: 565.1016s
	iters: 200, epoch: 73 | loss: 0.4014476
	speed: 0.0159s/iter; left time: 112.7674s
Epoch: 73 cost time: 4.7911951541900635
Epoch: 73, Steps: 261 | Train Loss: 0.4136512 Vali Loss: 0.9599816 Test Loss: 0.4273840
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.3674439
	speed: 0.0800s/iter; left time: 555.7877s
	iters: 200, epoch: 74 | loss: 0.3790612
	speed: 0.0164s/iter; left time: 112.2534s
Epoch: 74 cost time: 4.837004661560059
Epoch: 74, Steps: 261 | Train Loss: 0.4137462 Vali Loss: 0.9604213 Test Loss: 0.4273959
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.4070197
	speed: 0.0763s/iter; left time: 510.4235s
	iters: 200, epoch: 75 | loss: 0.4047647
	speed: 0.0158s/iter; left time: 103.8690s
Epoch: 75 cost time: 4.737542152404785
Epoch: 75, Steps: 261 | Train Loss: 0.4135948 Vali Loss: 0.9601634 Test Loss: 0.4273914
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.3948537
	speed: 0.0766s/iter; left time: 492.3937s
	iters: 200, epoch: 76 | loss: 0.4295177
	speed: 0.0159s/iter; left time: 100.8345s
Epoch: 76 cost time: 4.829295635223389
Epoch: 76, Steps: 261 | Train Loss: 0.4136680 Vali Loss: 0.9604061 Test Loss: 0.4273962
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.4038055
	speed: 0.0785s/iter; left time: 484.1218s
	iters: 200, epoch: 77 | loss: 0.4269259
	speed: 0.0160s/iter; left time: 96.8562s
Epoch: 77 cost time: 4.771741628646851
Epoch: 77, Steps: 261 | Train Loss: 0.4137576 Vali Loss: 0.9612314 Test Loss: 0.4273953
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.3966826
	speed: 0.0764s/iter; left time: 451.2457s
	iters: 200, epoch: 78 | loss: 0.3849419
	speed: 0.0164s/iter; left time: 95.2407s
Epoch: 78 cost time: 4.854146957397461
Epoch: 78, Steps: 261 | Train Loss: 0.4136917 Vali Loss: 0.9608576 Test Loss: 0.4273907
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.4047794
	speed: 0.0818s/iter; left time: 461.5392s
	iters: 200, epoch: 79 | loss: 0.4222958
	speed: 0.0158s/iter; left time: 87.7846s
Epoch: 79 cost time: 4.76539421081543
Epoch: 79, Steps: 261 | Train Loss: 0.4135862 Vali Loss: 0.9600522 Test Loss: 0.4273950
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.4391439
	speed: 0.0772s/iter; left time: 415.6694s
	iters: 200, epoch: 80 | loss: 0.3964129
	speed: 0.0161s/iter; left time: 85.2603s
Epoch: 80 cost time: 4.784931659698486
Epoch: 80, Steps: 261 | Train Loss: 0.4136545 Vali Loss: 0.9602197 Test Loss: 0.4274003
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.4027632
	speed: 0.0767s/iter; left time: 392.8660s
	iters: 200, epoch: 81 | loss: 0.4054988
	speed: 0.0161s/iter; left time: 80.6818s
Epoch: 81 cost time: 4.699445962905884
Epoch: 81, Steps: 261 | Train Loss: 0.4136582 Vali Loss: 0.9606744 Test Loss: 0.4273974
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.4294328
	speed: 0.0780s/iter; left time: 378.9100s
	iters: 200, epoch: 82 | loss: 0.4022247
	speed: 0.0157s/iter; left time: 74.4990s
Epoch: 82 cost time: 4.754720687866211
Epoch: 82, Steps: 261 | Train Loss: 0.4136878 Vali Loss: 0.9603778 Test Loss: 0.4273989
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.4057896
	speed: 0.0774s/iter; left time: 355.9238s
	iters: 200, epoch: 83 | loss: 0.4414715
	speed: 0.0155s/iter; left time: 69.7903s
Epoch: 83 cost time: 5.542229652404785
Epoch: 83, Steps: 261 | Train Loss: 0.4136599 Vali Loss: 0.9607704 Test Loss: 0.4274084
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4270544946193695, mae:0.4160441756248474, rse:0.6217451691627502, corr:[0.52829105 0.5322974  0.5332159  0.53236336 0.5313998  0.5312229
 0.5318027  0.53261685 0.5331392  0.5333377  0.5335815  0.53385615
 0.5342308  0.53456396 0.5344374  0.53352255 0.53206867 0.5305484
 0.5291089  0.5278921  0.52679116 0.52588475 0.5249033  0.52383995
 0.5226247  0.52122784 0.51991445 0.51895934 0.5184389  0.5181784
 0.5182834  0.518723   0.5192175  0.51956135 0.51962346 0.5195981
 0.5195278  0.5193639  0.5191151  0.5187682  0.5183844  0.51796806
 0.51755494 0.51727504 0.5171394  0.51713014 0.51723236 0.5172457
 0.51705056 0.5167603  0.5164209  0.51611733 0.5160055  0.5160461
 0.516166   0.51629865 0.5163069  0.5161804  0.5159988  0.51571125
 0.51551396 0.51538765 0.51537144 0.5153618  0.51542443 0.5155848
 0.5157526  0.5158547  0.5160532  0.5162866  0.5165452  0.5167709
 0.51700073 0.5172223  0.5173571  0.51734394 0.5172138  0.51704234
 0.51687485 0.51674104 0.5166524  0.5165729  0.51640195 0.51605475
 0.51566494 0.5152292  0.51489675 0.5146514  0.51448697 0.5144844
 0.51461303 0.51473105 0.51473516 0.51459694 0.5142936  0.51381695
 0.5132679  0.51273096 0.51219136 0.5116592  0.51126605 0.5110961
 0.511164   0.51134163 0.51169896 0.51217324 0.51255107 0.51287895
 0.5130141  0.5131033  0.51311886 0.513095   0.51299953 0.512955
 0.51293886 0.51295215 0.5129172  0.51282996 0.51267785 0.51264524
 0.51270264 0.5126822  0.5125515  0.5124349  0.5122849  0.51203096
 0.51171315 0.5114329  0.51124394 0.51119787 0.5112589  0.5113218
 0.51133287 0.511168   0.5108735  0.5104762  0.510078   0.5097256
 0.50953346 0.50949055 0.509509   0.50957    0.50958866 0.50955635
 0.50946313 0.5093578  0.50924736 0.5091913  0.50923985 0.50926536
 0.5092588  0.5093115  0.50933814 0.50931394 0.50926226 0.50926214
 0.5092993  0.5093792  0.50944906 0.50953054 0.50966746 0.50981945
 0.5099816  0.51012844 0.51033485 0.5106005  0.51089394 0.51126707
 0.5116579  0.5120058  0.5122839  0.5124779  0.5125135  0.5124442
 0.5123288  0.5122544  0.5122174  0.51220155 0.512105   0.51194054
 0.5116928  0.5114829  0.5112603  0.5111022  0.51100075 0.5109415
 0.5108791  0.5108175  0.51075345 0.51069427 0.5105625  0.51033777
 0.5100101  0.5096697  0.50927013 0.5087221  0.5081119  0.50765836
 0.50733495 0.5070617  0.5068606  0.5066727  0.50639933 0.50606847
 0.505727   0.50530803 0.5049263  0.5045342  0.5041788  0.5038798
 0.50352776 0.5031607  0.5027243  0.50229925 0.5018531  0.50148493
 0.5011912  0.50092244 0.5006146  0.50042164 0.5002762  0.500163
 0.50014985 0.5001722  0.5001885  0.5001771  0.50017333 0.50011563
 0.5000282  0.49993864 0.49979556 0.49959052 0.49939716 0.49921054
 0.4991257  0.49912187 0.49906027 0.49902892 0.49897394 0.4988875
 0.49877405 0.49864632 0.49856818 0.498518   0.49845606 0.49842462
 0.49840003 0.4983669  0.49835205 0.49838832 0.49840373 0.49844185
 0.498415   0.49841055 0.49839652 0.49838465 0.49831942 0.4983128
 0.4983206  0.49838808 0.49853802 0.4987633  0.49896902 0.49926233
 0.49958396 0.4998849  0.5000763  0.50023377 0.50031894 0.5002756
 0.5002299  0.5002132  0.5002111  0.500203   0.5001095  0.49999326
 0.49981326 0.49968618 0.49952352 0.49942786 0.49934298 0.49925676
 0.49921107 0.4991037  0.4989218  0.49861744 0.49816406 0.49756888
 0.4968352  0.49614513 0.49559927 0.49503082 0.49451813 0.49408504
 0.4937333  0.49334025 0.49300447 0.4926706  0.49237037 0.4920649
 0.49175426 0.49145243 0.49118572 0.49096924 0.49076685 0.4905822
 0.49046764 0.4903591  0.49028116 0.49026945 0.49033648 0.49042764
 0.4905559  0.49062744 0.49054477 0.49047858 0.49041292 0.49037513
 0.49035865 0.4903378  0.49027348 0.49016252 0.49000394 0.48980173
 0.48958763 0.48937106 0.48913226 0.48892006 0.48869288 0.48849678
 0.4883152  0.48815522 0.48798737 0.4878095  0.48765135 0.48754796
 0.48748827 0.48747957 0.48751333 0.48748267 0.4874427  0.4874198
 0.48737139 0.487329   0.487346   0.4873877  0.48747614 0.48760432
 0.48765773 0.4876794  0.48766    0.48758677 0.4875314  0.4875476
 0.4876087  0.48770946 0.48786476 0.48801944 0.4881519  0.48828894
 0.4883893  0.48847568 0.48853546 0.48857352 0.48858616 0.4885808
 0.48854738 0.48854235 0.4885727  0.48856628 0.48853883 0.48849434
 0.48846325 0.48843563 0.48844957 0.48851955 0.4886156  0.4887384
 0.48888606 0.48902383 0.48911873 0.4890832  0.48889247 0.48857704
 0.4881156  0.4876231  0.48715878 0.48664767 0.48610285 0.48569262
 0.4854141  0.48517022 0.48500007 0.4849298  0.48485434 0.4847713
 0.4845947  0.4844155  0.48422992 0.4840427  0.48383474 0.4836494
 0.48357856 0.483599   0.4836365  0.48363975 0.483615   0.483609
 0.48362678 0.48354632 0.48343217 0.48339844 0.48340994 0.483422
 0.4834287  0.48338482 0.4833274  0.4832159  0.4830781  0.482932
 0.48276514 0.48261607 0.48241785 0.48222357 0.4820124  0.481806
 0.481579   0.48141775 0.48125026 0.48112017 0.48102337 0.48104677
 0.4810178  0.4809102  0.48078716 0.48065314 0.4804943  0.4803738
 0.48028523 0.48026362 0.48031592 0.48040614 0.48052242 0.48065296
 0.48067012 0.48061678 0.48049435 0.48036656 0.4803057  0.48034197
 0.48046955 0.48066923 0.48086566 0.48101565 0.4811099  0.48120555
 0.48121879 0.48124084 0.4812616  0.4813471  0.4814523  0.4815353
 0.48160246 0.48169526 0.48175257 0.48181725 0.4818471  0.48183772
 0.4818069  0.48176113 0.48172268 0.48170733 0.48169893 0.48167542
 0.4815578  0.48134398 0.48100555 0.48051754 0.47989714 0.47919497
 0.47842407 0.4776956  0.4769873  0.47625092 0.47552526 0.47487932
 0.47435203 0.47385687 0.4734124  0.47303584 0.47262537 0.4721365
 0.47164667 0.4712134  0.47080278 0.4703974  0.4701187  0.46990588
 0.46977508 0.4698109  0.46998534 0.47016367 0.47036937 0.47061637
 0.47091112 0.47108233 0.47114557 0.47116596 0.4711664  0.47120672
 0.4712649  0.4713126  0.47134215 0.47138435 0.4713797  0.4713724
 0.47132483 0.47122335 0.47111326 0.4709049  0.47063196 0.47039375
 0.47020575 0.47005206 0.46993187 0.46980497 0.46966407 0.46958548
 0.4694987  0.4693694  0.46922418 0.46908617 0.4689799  0.46893147
 0.4689404  0.46895835 0.46899238 0.46902275 0.46902934 0.4690023
 0.46895233 0.46884313 0.4686965  0.4685578  0.46850765 0.46853733
 0.4685761  0.46859077 0.4686094  0.46862024 0.4686086  0.46868122
 0.46876028 0.46889675 0.4690562  0.4692417  0.4694282  0.46953663
 0.4695981  0.46966875 0.4697446  0.46983463 0.4699126  0.4699638
 0.46993053 0.46993005 0.46988165 0.4698402  0.46981138 0.46977863
 0.46975195 0.4696554  0.4695006  0.46921542 0.46875343 0.46808788
 0.46728203 0.46643    0.46565673 0.46483615 0.46405903 0.4634619
 0.463068   0.4627558  0.46254942 0.46241078 0.46220377 0.46187294
 0.4614663  0.4610088  0.46057093 0.460199   0.45988023 0.4596935
 0.45951054 0.45943782 0.45942217 0.45946044 0.4595395  0.4597393
 0.4600323  0.4601987  0.46019775 0.46016496 0.4601231  0.46004254
 0.45993906 0.45985126 0.45977652 0.45970255 0.45962825 0.45958042
 0.45949894 0.45936492 0.45917806 0.45889795 0.4586178  0.45836276
 0.45818794 0.45808125 0.45803234 0.45792717 0.45784527 0.45780203
 0.45775858 0.45773917 0.45772442 0.45766053 0.45760006 0.4575506
 0.4574644  0.45734563 0.4572474  0.4571483  0.45707858 0.45697573
 0.45681366 0.4566296  0.45645404 0.4562865  0.4562039  0.45613006
 0.4560516  0.45599896 0.45597097 0.456031   0.4561623  0.4563651
 0.45664272 0.45692068 0.4571546  0.45733446 0.45741934 0.45745105
 0.45747173 0.457543   0.45766044 0.45781258 0.45791686 0.45793697
 0.4578596  0.45773533 0.45764637 0.45759436 0.45762914 0.45772165
 0.45783362 0.45792088 0.45789123 0.45770535 0.45734408 0.45679033
 0.45605305 0.45538825 0.45486334 0.4543591  0.45393297 0.4535974
 0.45331234 0.45305687 0.45284018 0.45262957 0.45244652 0.45233363
 0.45225492 0.45219913 0.45213866 0.45196155 0.4516822  0.45122528
 0.4508147  0.45051166 0.4503647  0.45041713 0.45057562 0.45092326
 0.4512366  0.45116633 0.4506864  0.4500698  0.4494391  0.44887137
 0.44850138 0.44836146 0.44834906 0.44831824 0.44804955 0.44758502
 0.4470599  0.44671208 0.44668406 0.4470095  0.4475312  0.44784296
 0.44776115 0.44749606 0.4474935  0.44828176 0.4500271  0.45142344]
