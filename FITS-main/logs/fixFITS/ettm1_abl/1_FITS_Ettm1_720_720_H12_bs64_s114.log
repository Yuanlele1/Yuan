Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=106, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20134912.0
params:  22684.0
Trainable parameters:  22684
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4710283
	speed: 0.0848s/iter; left time: 2178.2470s
	iters: 200, epoch: 1 | loss: 0.4180408
	speed: 0.0629s/iter; left time: 1609.4347s
Epoch: 1 cost time: 19.214763641357422
Epoch: 1, Steps: 258 | Train Loss: 0.5123190 Vali Loss: 1.0186989 Test Loss: 0.4505638
Validation loss decreased (inf --> 1.018699).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3831649
	speed: 0.3448s/iter; left time: 8773.4592s
	iters: 200, epoch: 2 | loss: 0.4190007
	speed: 0.0629s/iter; left time: 1595.0039s
Epoch: 2 cost time: 15.875784397125244
Epoch: 2, Steps: 258 | Train Loss: 0.4155705 Vali Loss: 0.9665689 Test Loss: 0.4207307
Validation loss decreased (1.018699 --> 0.966569).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3833669
	speed: 0.2725s/iter; left time: 6861.8206s
	iters: 200, epoch: 3 | loss: 0.4313729
	speed: 0.0665s/iter; left time: 1668.7163s
Epoch: 3 cost time: 18.00991725921631
Epoch: 3, Steps: 258 | Train Loss: 0.4035045 Vali Loss: 0.9506029 Test Loss: 0.4150716
Validation loss decreased (0.966569 --> 0.950603).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3909288
	speed: 0.2712s/iter; left time: 6761.0224s
	iters: 200, epoch: 4 | loss: 0.3848750
	speed: 0.0553s/iter; left time: 1373.8399s
Epoch: 4 cost time: 16.467752695083618
Epoch: 4, Steps: 258 | Train Loss: 0.3996968 Vali Loss: 0.9431298 Test Loss: 0.4146071
Validation loss decreased (0.950603 --> 0.943130).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3628984
	speed: 0.2902s/iter; left time: 7158.7215s
	iters: 200, epoch: 5 | loss: 0.3737473
	speed: 0.0645s/iter; left time: 1585.5499s
Epoch: 5 cost time: 17.771843433380127
Epoch: 5, Steps: 258 | Train Loss: 0.3984989 Vali Loss: 0.9407132 Test Loss: 0.4149322
Validation loss decreased (0.943130 --> 0.940713).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4311272
	speed: 0.2828s/iter; left time: 6903.5013s
	iters: 200, epoch: 6 | loss: 0.3697801
	speed: 0.0650s/iter; left time: 1580.9513s
Epoch: 6 cost time: 17.581311464309692
Epoch: 6, Steps: 258 | Train Loss: 0.3980554 Vali Loss: 0.9369753 Test Loss: 0.4154727
Validation loss decreased (0.940713 --> 0.936975).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3657526
	speed: 0.2905s/iter; left time: 7017.5854s
	iters: 200, epoch: 7 | loss: 0.4136329
	speed: 0.0661s/iter; left time: 1590.5101s
Epoch: 7 cost time: 17.428531408309937
Epoch: 7, Steps: 258 | Train Loss: 0.3977129 Vali Loss: 0.9364613 Test Loss: 0.4159396
Validation loss decreased (0.936975 --> 0.936461).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4272774
	speed: 0.2862s/iter; left time: 6839.1221s
	iters: 200, epoch: 8 | loss: 0.4154705
	speed: 0.0596s/iter; left time: 1417.2824s
Epoch: 8 cost time: 16.623473167419434
Epoch: 8, Steps: 258 | Train Loss: 0.3974614 Vali Loss: 0.9358403 Test Loss: 0.4161964
Validation loss decreased (0.936461 --> 0.935840).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4054029
	speed: 0.2604s/iter; left time: 6153.9446s
	iters: 200, epoch: 9 | loss: 0.3707682
	speed: 0.0589s/iter; left time: 1386.8796s
Epoch: 9 cost time: 16.275456190109253
Epoch: 9, Steps: 258 | Train Loss: 0.3972299 Vali Loss: 0.9336118 Test Loss: 0.4161788
Validation loss decreased (0.935840 --> 0.933612).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3855235
	speed: 0.2706s/iter; left time: 6325.4814s
	iters: 200, epoch: 10 | loss: 0.3806182
	speed: 0.0666s/iter; left time: 1549.4263s
Epoch: 10 cost time: 16.514981269836426
Epoch: 10, Steps: 258 | Train Loss: 0.3971973 Vali Loss: 0.9345308 Test Loss: 0.4161839
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3897054
	speed: 0.2735s/iter; left time: 6323.8439s
	iters: 200, epoch: 11 | loss: 0.3867984
	speed: 0.0630s/iter; left time: 1449.3199s
Epoch: 11 cost time: 17.493031978607178
Epoch: 11, Steps: 258 | Train Loss: 0.3971945 Vali Loss: 0.9337705 Test Loss: 0.4159133
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3685112
	speed: 0.2738s/iter; left time: 6260.7492s
	iters: 200, epoch: 12 | loss: 0.3945797
	speed: 0.0655s/iter; left time: 1489.9032s
Epoch: 12 cost time: 17.326122045516968
Epoch: 12, Steps: 258 | Train Loss: 0.3970706 Vali Loss: 0.9332223 Test Loss: 0.4163915
Validation loss decreased (0.933612 --> 0.933222).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3818312
	speed: 0.2930s/iter; left time: 6623.7895s
	iters: 200, epoch: 13 | loss: 0.3697174
	speed: 0.0631s/iter; left time: 1420.3649s
Epoch: 13 cost time: 16.914247274398804
Epoch: 13, Steps: 258 | Train Loss: 0.3969306 Vali Loss: 0.9329591 Test Loss: 0.4162549
Validation loss decreased (0.933222 --> 0.932959).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4224203
	speed: 0.2677s/iter; left time: 5981.3753s
	iters: 200, epoch: 14 | loss: 0.3857760
	speed: 0.0676s/iter; left time: 1504.7159s
Epoch: 14 cost time: 19.44369912147522
Epoch: 14, Steps: 258 | Train Loss: 0.3969515 Vali Loss: 0.9324374 Test Loss: 0.4161192
Validation loss decreased (0.932959 --> 0.932437).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3772309
	speed: 0.2621s/iter; left time: 5789.2491s
	iters: 200, epoch: 15 | loss: 0.4073808
	speed: 0.0752s/iter; left time: 1652.8334s
Epoch: 15 cost time: 18.471171140670776
Epoch: 15, Steps: 258 | Train Loss: 0.3968741 Vali Loss: 0.9327228 Test Loss: 0.4163488
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3887330
	speed: 0.2866s/iter; left time: 6256.2262s
	iters: 200, epoch: 16 | loss: 0.4105164
	speed: 0.0531s/iter; left time: 1154.8209s
Epoch: 16 cost time: 15.935221195220947
Epoch: 16, Steps: 258 | Train Loss: 0.3969159 Vali Loss: 0.9322006 Test Loss: 0.4161949
Validation loss decreased (0.932437 --> 0.932201).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4201859
	speed: 0.2666s/iter; left time: 5752.1570s
	iters: 200, epoch: 17 | loss: 0.4029607
	speed: 0.0621s/iter; left time: 1332.8561s
Epoch: 17 cost time: 17.17794442176819
Epoch: 17, Steps: 258 | Train Loss: 0.3968834 Vali Loss: 0.9312817 Test Loss: 0.4162992
Validation loss decreased (0.932201 --> 0.931282).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3905568
	speed: 0.2630s/iter; left time: 5605.3409s
	iters: 200, epoch: 18 | loss: 0.3803021
	speed: 0.0669s/iter; left time: 1418.9196s
Epoch: 18 cost time: 18.40415096282959
Epoch: 18, Steps: 258 | Train Loss: 0.3967055 Vali Loss: 0.9319463 Test Loss: 0.4163100
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3803118
	speed: 0.2770s/iter; left time: 5833.3123s
	iters: 200, epoch: 19 | loss: 0.4068508
	speed: 0.0558s/iter; left time: 1168.4786s
Epoch: 19 cost time: 16.740118980407715
Epoch: 19, Steps: 258 | Train Loss: 0.3967522 Vali Loss: 0.9311118 Test Loss: 0.4162257
Validation loss decreased (0.931282 --> 0.931112).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3946872
	speed: 0.2896s/iter; left time: 6022.7581s
	iters: 200, epoch: 20 | loss: 0.4049915
	speed: 0.0624s/iter; left time: 1292.1051s
Epoch: 20 cost time: 16.928653955459595
Epoch: 20, Steps: 258 | Train Loss: 0.3967301 Vali Loss: 0.9314365 Test Loss: 0.4161829
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4364718
	speed: 0.2646s/iter; left time: 5436.0922s
	iters: 200, epoch: 21 | loss: 0.4279598
	speed: 0.0459s/iter; left time: 938.3352s
Epoch: 21 cost time: 13.516607522964478
Epoch: 21, Steps: 258 | Train Loss: 0.3965844 Vali Loss: 0.9312901 Test Loss: 0.4163567
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3884329
	speed: 0.2669s/iter; left time: 5413.8910s
	iters: 200, epoch: 22 | loss: 0.3592838
	speed: 0.0661s/iter; left time: 1333.6762s
Epoch: 22 cost time: 17.905656576156616
Epoch: 22, Steps: 258 | Train Loss: 0.3965740 Vali Loss: 0.9310113 Test Loss: 0.4163444
Validation loss decreased (0.931112 --> 0.931011).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3871292
	speed: 0.2734s/iter; left time: 5475.6630s
	iters: 200, epoch: 23 | loss: 0.4349445
	speed: 0.0622s/iter; left time: 1240.0577s
Epoch: 23 cost time: 17.23136806488037
Epoch: 23, Steps: 258 | Train Loss: 0.3965737 Vali Loss: 0.9309167 Test Loss: 0.4162300
Validation loss decreased (0.931011 --> 0.930917).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4043674
	speed: 0.2616s/iter; left time: 5170.2006s
	iters: 200, epoch: 24 | loss: 0.3608876
	speed: 0.0658s/iter; left time: 1293.5297s
Epoch: 24 cost time: 17.622594118118286
Epoch: 24, Steps: 258 | Train Loss: 0.3966848 Vali Loss: 0.9315513 Test Loss: 0.4160711
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3637599
	speed: 0.2743s/iter; left time: 5352.0022s
	iters: 200, epoch: 25 | loss: 0.4248860
	speed: 0.0647s/iter; left time: 1256.6597s
Epoch: 25 cost time: 17.42370104789734
Epoch: 25, Steps: 258 | Train Loss: 0.3965856 Vali Loss: 0.9315937 Test Loss: 0.4163273
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4197252
	speed: 0.2635s/iter; left time: 5071.7666s
	iters: 200, epoch: 26 | loss: 0.4021019
	speed: 0.0663s/iter; left time: 1269.7299s
Epoch: 26 cost time: 17.28858208656311
Epoch: 26, Steps: 258 | Train Loss: 0.3966050 Vali Loss: 0.9314815 Test Loss: 0.4162332
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3743633
	speed: 0.2859s/iter; left time: 5429.9162s
	iters: 200, epoch: 27 | loss: 0.4251091
	speed: 0.0598s/iter; left time: 1129.4985s
Epoch: 27 cost time: 17.04094171524048
Epoch: 27, Steps: 258 | Train Loss: 0.3965815 Vali Loss: 0.9312529 Test Loss: 0.4163503
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4286193
	speed: 0.2687s/iter; left time: 5034.4229s
	iters: 200, epoch: 28 | loss: 0.3934279
	speed: 0.0583s/iter; left time: 1086.3280s
Epoch: 28 cost time: 17.12868595123291
Epoch: 28, Steps: 258 | Train Loss: 0.3965299 Vali Loss: 0.9306605 Test Loss: 0.4163318
Validation loss decreased (0.930917 --> 0.930661).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4010249
	speed: 0.2837s/iter; left time: 5241.2290s
	iters: 200, epoch: 29 | loss: 0.4093711
	speed: 0.0604s/iter; left time: 1110.7484s
Epoch: 29 cost time: 16.724676609039307
Epoch: 29, Steps: 258 | Train Loss: 0.3965248 Vali Loss: 0.9304717 Test Loss: 0.4161127
Validation loss decreased (0.930661 --> 0.930472).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3947333
	speed: 0.2857s/iter; left time: 5205.0636s
	iters: 200, epoch: 30 | loss: 0.3771469
	speed: 0.0618s/iter; left time: 1119.9712s
Epoch: 30 cost time: 17.195439100265503
Epoch: 30, Steps: 258 | Train Loss: 0.3964887 Vali Loss: 0.9307019 Test Loss: 0.4163500
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4029861
	speed: 0.2861s/iter; left time: 5139.1808s
	iters: 200, epoch: 31 | loss: 0.4137557
	speed: 0.0509s/iter; left time: 909.0195s
Epoch: 31 cost time: 14.6267991065979
Epoch: 31, Steps: 258 | Train Loss: 0.3964900 Vali Loss: 0.9309241 Test Loss: 0.4162798
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3960171
	speed: 0.2831s/iter; left time: 5010.9149s
	iters: 200, epoch: 32 | loss: 0.3650402
	speed: 0.0733s/iter; left time: 1289.6489s
Epoch: 32 cost time: 18.77515935897827
Epoch: 32, Steps: 258 | Train Loss: 0.3963835 Vali Loss: 0.9314637 Test Loss: 0.4164956
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3924931
	speed: 0.2786s/iter; left time: 4859.8331s
	iters: 200, epoch: 33 | loss: 0.3884928
	speed: 0.0551s/iter; left time: 956.1140s
Epoch: 33 cost time: 16.594002962112427
Epoch: 33, Steps: 258 | Train Loss: 0.3964137 Vali Loss: 0.9306437 Test Loss: 0.4161234
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4121591
	speed: 0.2947s/iter; left time: 5064.9634s
	iters: 200, epoch: 34 | loss: 0.3675674
	speed: 0.0614s/iter; left time: 1049.0863s
Epoch: 34 cost time: 16.481889486312866
Epoch: 34, Steps: 258 | Train Loss: 0.3964117 Vali Loss: 0.9304179 Test Loss: 0.4163667
Validation loss decreased (0.930472 --> 0.930418).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4090155
	speed: 0.2950s/iter; left time: 4993.9283s
	iters: 200, epoch: 35 | loss: 0.3998654
	speed: 0.0690s/iter; left time: 1161.0473s
Epoch: 35 cost time: 17.31999158859253
Epoch: 35, Steps: 258 | Train Loss: 0.3963966 Vali Loss: 0.9308536 Test Loss: 0.4162716
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3728603
	speed: 0.2021s/iter; left time: 3369.6187s
	iters: 200, epoch: 36 | loss: 0.4089862
	speed: 0.0641s/iter; left time: 1061.5762s
Epoch: 36 cost time: 17.423823833465576
Epoch: 36, Steps: 258 | Train Loss: 0.3965072 Vali Loss: 0.9311301 Test Loss: 0.4163950
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4012062
	speed: 0.2783s/iter; left time: 4567.7599s
	iters: 200, epoch: 37 | loss: 0.3919193
	speed: 0.0626s/iter; left time: 1021.9020s
Epoch: 37 cost time: 17.133735418319702
Epoch: 37, Steps: 258 | Train Loss: 0.3965707 Vali Loss: 0.9313769 Test Loss: 0.4164299
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4221898
	speed: 0.2444s/iter; left time: 3948.3917s
	iters: 200, epoch: 38 | loss: 0.4118576
	speed: 0.0622s/iter; left time: 999.1320s
Epoch: 38 cost time: 16.471861600875854
Epoch: 38, Steps: 258 | Train Loss: 0.3964654 Vali Loss: 0.9305584 Test Loss: 0.4163818
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4000628
	speed: 0.2901s/iter; left time: 4611.1059s
	iters: 200, epoch: 39 | loss: 0.3911219
	speed: 0.0685s/iter; left time: 1082.6098s
Epoch: 39 cost time: 18.32801103591919
Epoch: 39, Steps: 258 | Train Loss: 0.3964664 Vali Loss: 0.9298987 Test Loss: 0.4164060
Validation loss decreased (0.930418 --> 0.929899).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3733840
	speed: 0.2941s/iter; left time: 4598.8211s
	iters: 200, epoch: 40 | loss: 0.4162596
	speed: 0.0648s/iter; left time: 1007.2151s
Epoch: 40 cost time: 17.136685609817505
Epoch: 40, Steps: 258 | Train Loss: 0.3964354 Vali Loss: 0.9310469 Test Loss: 0.4163625
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3644415
	speed: 0.2599s/iter; left time: 3997.2060s
	iters: 200, epoch: 41 | loss: 0.3501720
	speed: 0.0585s/iter; left time: 893.2835s
Epoch: 41 cost time: 15.973270654678345
Epoch: 41, Steps: 258 | Train Loss: 0.3963419 Vali Loss: 0.9299744 Test Loss: 0.4165258
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4366225
	speed: 0.2802s/iter; left time: 4237.7463s
	iters: 200, epoch: 42 | loss: 0.4036926
	speed: 0.0611s/iter; left time: 917.8483s
Epoch: 42 cost time: 16.341289043426514
Epoch: 42, Steps: 258 | Train Loss: 0.3964397 Vali Loss: 0.9308629 Test Loss: 0.4163783
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3620074
	speed: 0.2709s/iter; left time: 4027.1397s
	iters: 200, epoch: 43 | loss: 0.4218926
	speed: 0.0630s/iter; left time: 930.1232s
Epoch: 43 cost time: 17.641377925872803
Epoch: 43, Steps: 258 | Train Loss: 0.3963291 Vali Loss: 0.9310601 Test Loss: 0.4163498
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3943151
	speed: 0.2928s/iter; left time: 4277.2110s
	iters: 200, epoch: 44 | loss: 0.4135649
	speed: 0.0654s/iter; left time: 948.3381s
Epoch: 44 cost time: 17.279048442840576
Epoch: 44, Steps: 258 | Train Loss: 0.3963850 Vali Loss: 0.9307315 Test Loss: 0.4162743
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.3994113
	speed: 0.2705s/iter; left time: 3880.7576s
	iters: 200, epoch: 45 | loss: 0.3979498
	speed: 0.0668s/iter; left time: 951.6750s
Epoch: 45 cost time: 18.205852508544922
Epoch: 45, Steps: 258 | Train Loss: 0.3962855 Vali Loss: 0.9300012 Test Loss: 0.4164097
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4009843
	speed: 0.2693s/iter; left time: 3794.1235s
	iters: 200, epoch: 46 | loss: 0.4280630
	speed: 0.0637s/iter; left time: 891.1496s
Epoch: 46 cost time: 16.973731994628906
Epoch: 46, Steps: 258 | Train Loss: 0.3963165 Vali Loss: 0.9308029 Test Loss: 0.4163291
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3962509
	speed: 0.2676s/iter; left time: 3701.4188s
	iters: 200, epoch: 47 | loss: 0.4151333
	speed: 0.0687s/iter; left time: 944.1213s
Epoch: 47 cost time: 17.691955089569092
Epoch: 47, Steps: 258 | Train Loss: 0.3962604 Vali Loss: 0.9304725 Test Loss: 0.4162804
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4020705
	speed: 0.2738s/iter; left time: 3716.8945s
	iters: 200, epoch: 48 | loss: 0.4081221
	speed: 0.0536s/iter; left time: 722.4834s
Epoch: 48 cost time: 15.850466012954712
Epoch: 48, Steps: 258 | Train Loss: 0.3962102 Vali Loss: 0.9311372 Test Loss: 0.4164120
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4019617
	speed: 0.2896s/iter; left time: 3856.5431s
	iters: 200, epoch: 49 | loss: 0.3932706
	speed: 0.0599s/iter; left time: 792.0895s
Epoch: 49 cost time: 16.640437364578247
Epoch: 49, Steps: 258 | Train Loss: 0.3963866 Vali Loss: 0.9312082 Test Loss: 0.4163226
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.3685137
	speed: 0.2615s/iter; left time: 3414.6217s
	iters: 200, epoch: 50 | loss: 0.4402205
	speed: 0.0589s/iter; left time: 763.2051s
Epoch: 50 cost time: 16.025076389312744
Epoch: 50, Steps: 258 | Train Loss: 0.3962957 Vali Loss: 0.9301383 Test Loss: 0.4163049
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4291163
	speed: 0.2931s/iter; left time: 3751.3519s
	iters: 200, epoch: 51 | loss: 0.3862402
	speed: 0.0629s/iter; left time: 798.4853s
Epoch: 51 cost time: 17.130927324295044
Epoch: 51, Steps: 258 | Train Loss: 0.3963696 Vali Loss: 0.9313622 Test Loss: 0.4164655
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4054486
	speed: 0.2728s/iter; left time: 3421.8907s
	iters: 200, epoch: 52 | loss: 0.3750764
	speed: 0.0704s/iter; left time: 875.8657s
Epoch: 52 cost time: 18.532542943954468
Epoch: 52, Steps: 258 | Train Loss: 0.3963835 Vali Loss: 0.9307355 Test Loss: 0.4163100
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4143612
	speed: 0.2627s/iter; left time: 3226.9136s
	iters: 200, epoch: 53 | loss: 0.4179264
	speed: 0.0729s/iter; left time: 888.5207s
Epoch: 53 cost time: 19.30407214164734
Epoch: 53, Steps: 258 | Train Loss: 0.3961644 Vali Loss: 0.9308068 Test Loss: 0.4164160
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4084101
	speed: 0.2973s/iter; left time: 3575.1188s
	iters: 200, epoch: 54 | loss: 0.3548490
	speed: 0.0684s/iter; left time: 815.9295s
Epoch: 54 cost time: 18.263819932937622
Epoch: 54, Steps: 258 | Train Loss: 0.3962717 Vali Loss: 0.9307805 Test Loss: 0.4163487
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.3967390
	speed: 0.2746s/iter; left time: 3231.6287s
	iters: 200, epoch: 55 | loss: 0.4371679
	speed: 0.0566s/iter; left time: 660.4706s
Epoch: 55 cost time: 16.426019430160522
Epoch: 55, Steps: 258 | Train Loss: 0.3962985 Vali Loss: 0.9305144 Test Loss: 0.4163446
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4105912
	speed: 0.2941s/iter; left time: 3384.8597s
	iters: 200, epoch: 56 | loss: 0.4138063
	speed: 0.0627s/iter; left time: 715.2827s
Epoch: 56 cost time: 17.18753695487976
Epoch: 56, Steps: 258 | Train Loss: 0.3963194 Vali Loss: 0.9307603 Test Loss: 0.4163703
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.4071522
	speed: 0.2898s/iter; left time: 3261.1812s
	iters: 200, epoch: 57 | loss: 0.3973339
	speed: 0.0621s/iter; left time: 692.0598s
Epoch: 57 cost time: 17.60161566734314
Epoch: 57, Steps: 258 | Train Loss: 0.3962502 Vali Loss: 0.9314204 Test Loss: 0.4163476
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.3816571
	speed: 0.2851s/iter; left time: 3134.7670s
	iters: 200, epoch: 58 | loss: 0.3638172
	speed: 0.0670s/iter; left time: 729.7700s
Epoch: 58 cost time: 17.797032833099365
Epoch: 58, Steps: 258 | Train Loss: 0.3963271 Vali Loss: 0.9313394 Test Loss: 0.4163783
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.3778709
	speed: 0.2755s/iter; left time: 2958.4252s
	iters: 200, epoch: 59 | loss: 0.4176193
	speed: 0.0740s/iter; left time: 787.3720s
Epoch: 59 cost time: 17.661932706832886
Epoch: 59, Steps: 258 | Train Loss: 0.3962265 Vali Loss: 0.9311895 Test Loss: 0.4163514
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4154016375541687, mae:0.4115815758705139, rse:0.6132038235664368, corr:[0.5284971  0.532085   0.5336285  0.53430957 0.53517824 0.53657085
 0.5379874  0.5390034  0.5396598  0.5402125  0.5409482  0.5416005
 0.54194087 0.541811   0.54124546 0.5404125  0.53950936 0.5385738
 0.5374773  0.5362983  0.53514904 0.5339721  0.53268665 0.5314419
 0.53004616 0.5286941  0.5275559  0.5267455  0.52651143 0.5268027
 0.52739674 0.52811295 0.52863157 0.52899516 0.5291551  0.5294478
 0.5296783  0.5297587  0.52960485 0.52915996 0.52874583 0.5284977
 0.5284842  0.52863497 0.5286342  0.52837217 0.5280548  0.52784115
 0.5277356  0.5276246  0.5275043  0.5272754  0.52701753 0.5267296
 0.5265934  0.52664834 0.5267921  0.52679265 0.5266313  0.5262713
 0.52592254 0.52577597 0.5258588  0.52594656 0.52601296 0.5260371
 0.52600217 0.5259763  0.5260831  0.5262609  0.52648026 0.5266219
 0.52661777 0.5264533  0.52625054 0.52602863 0.5257954  0.5256014
 0.525474   0.52540594 0.5253319  0.5252048  0.5250579  0.52487516
 0.52478486 0.52477074 0.52483195 0.52486634 0.5248621  0.5249255
 0.5251012  0.525391   0.52576107 0.5260963  0.5262886  0.5262093
 0.52590215 0.52554166 0.5251275  0.52483964 0.52467835 0.52456754
 0.52442545 0.52413744 0.52378875 0.52357394 0.523494   0.5235778
 0.52365893 0.5236256  0.52337724 0.52290916 0.52237505 0.5219874
 0.5218252  0.52186614 0.5219591  0.52188694 0.5215697  0.52113914
 0.52074164 0.52049416 0.5203814  0.5204029  0.52041674 0.52022195
 0.5198104  0.5193876  0.51911885 0.51912373 0.5192639  0.5193472
 0.5192852  0.51904833 0.5186739  0.5183687  0.5183386  0.518453
 0.5186377  0.518815   0.5188617  0.5188583  0.51892567 0.51912254
 0.5193909  0.5195498  0.51957464 0.51949173 0.51942754 0.5193936
 0.5193722  0.5193951  0.5194068  0.519369   0.5192751  0.51923645
 0.5192295  0.51920503 0.5191831  0.51910585 0.5191038  0.5192209
 0.51945454 0.5197228  0.51995677 0.5201145  0.5202188  0.52033657
 0.5204567  0.5205342  0.5206277  0.52072614 0.52073574 0.5206548
 0.5205274  0.52040607 0.52034    0.52034575 0.52038306 0.5204557
 0.5205287  0.52060044 0.52062726 0.52068573 0.52080554 0.5209859
 0.5212098  0.5214918  0.5217903  0.52202064 0.522118   0.5220481
 0.5218219  0.52153015 0.5211448  0.520651   0.52006376 0.5194225
 0.5187217  0.5180071  0.51728135 0.5166104  0.5159756  0.5153629
 0.5147487  0.5141144  0.5134592  0.51284355 0.51228154 0.51174825
 0.51116914 0.51055205 0.5098953  0.50922114 0.50851554 0.50791603
 0.50751895 0.5072602  0.50709754 0.5069233  0.5067152  0.5065701
 0.5066129  0.50674367 0.5069308  0.5071037  0.5071458  0.5070746
 0.5069839  0.5069758  0.5070912  0.5072472  0.50738585 0.5074697
 0.50747275 0.50747067 0.50743985 0.5075702  0.50781757 0.50816274
 0.50845695 0.50852937 0.5084167  0.50823593 0.50811195 0.50804776
 0.50803214 0.5080359  0.5080017  0.5079391  0.5078049  0.5076942
 0.5075625  0.5075026  0.5074704  0.5074386  0.50741    0.5073986
 0.5074176  0.50749224 0.50760037 0.50772685 0.5078539  0.50803655
 0.5081597  0.50820416 0.5081781  0.5081568  0.508152   0.50811887
 0.5080765  0.507996   0.5079021  0.5078219  0.50777376 0.50778633
 0.50780904 0.5078953  0.50794667 0.5080192  0.5080814  0.5081391
 0.50827247 0.50842685 0.50853765 0.50855243 0.5083793  0.5079813
 0.5074248  0.5068623  0.5063424  0.5058211  0.5052965  0.5047665
 0.5042462  0.50367737 0.5031247  0.502566   0.50200206 0.5014833
 0.5010143  0.50064546 0.500401   0.50021356 0.5000057  0.49970397
 0.49940646 0.49909565 0.4988082  0.49857074 0.49844158 0.4983578
 0.49828365 0.4982447  0.4981945  0.49815497 0.49814305 0.49818888
 0.4982427  0.49821743 0.49809977 0.4979268  0.49775738 0.4976355
 0.49758312 0.4976208  0.49764517 0.49760863 0.4974515  0.4972016
 0.49701396 0.49694607 0.49698883 0.49713326 0.49728656 0.49745902
 0.49754763 0.49741575 0.49722633 0.49707073 0.4970289  0.49706876
 0.49709147 0.49710232 0.49705637 0.49696982 0.49687317 0.49681848
 0.49675167 0.49671653 0.4967081  0.4966942  0.49670616 0.49672422
 0.4967113  0.4966833  0.4966405  0.49658176 0.49656236 0.4965795
 0.4965478  0.4965114  0.496452   0.49638185 0.4963432  0.49634737
 0.4963137  0.49628285 0.49624076 0.4961251  0.496004   0.49593386
 0.4959668  0.49613607 0.49640018 0.49672693 0.49702203 0.49728447
 0.49752986 0.4977859  0.4980642  0.49831298 0.4984618  0.49847004
 0.49829772 0.4979629  0.49754232 0.49707487 0.49657053 0.49616936
 0.49584594 0.49556494 0.4952706  0.49497172 0.49460173 0.49419513
 0.49378145 0.49339566 0.49304625 0.4927069  0.49230835 0.49194086
 0.49158868 0.49131563 0.49104103 0.49078554 0.4905371  0.49034098
 0.49018607 0.49009338 0.49005884 0.4901006  0.49021292 0.490301
 0.49045867 0.4905479  0.490637   0.49066547 0.49070132 0.49081331
 0.4909048  0.49098784 0.4909726  0.4908817  0.4907227  0.4906254
 0.49060234 0.49067333 0.49075052 0.49088567 0.49099284 0.49115312
 0.49125332 0.49119204 0.49103144 0.49086666 0.4907268  0.49062815
 0.49054313 0.49045077 0.49035132 0.49024364 0.49016225 0.49012825
 0.4901017  0.49009392 0.4900662  0.490002   0.48990417 0.48977602
 0.48972806 0.4897761  0.48990998 0.4900474  0.49013117 0.49019834
 0.49016997 0.49013424 0.49013358 0.49021533 0.49026385 0.49027687
 0.49020836 0.49012613 0.49001956 0.4899772  0.49002367 0.49011725
 0.49023876 0.49029908 0.49030364 0.4902732  0.49025592 0.49028996
 0.4903549  0.49047247 0.49058086 0.4905559  0.49033055 0.489934
 0.48942238 0.48887938 0.4883554  0.4878173  0.487235   0.48665702
 0.48614642 0.48562303 0.4851129  0.48462817 0.4840692  0.48341125
 0.48268762 0.48200783 0.4814013  0.4808401  0.48032713 0.47989458
 0.47950634 0.47919443 0.478939   0.47861275 0.47836217 0.4781738
 0.47809914 0.47810107 0.47810176 0.4781334  0.47816893 0.47825384
 0.47840014 0.47854084 0.47867393 0.47890896 0.47919288 0.47946343
 0.47962597 0.479729   0.4798049  0.4798187  0.47982338 0.47990704
 0.48006263 0.48024756 0.48039183 0.48043904 0.48051146 0.4807084
 0.48096064 0.48113027 0.48110375 0.4809588  0.48078552 0.480644
 0.48060244 0.4806576  0.48077628 0.480881   0.48087394 0.48072207
 0.48048326 0.4802476  0.48011228 0.48009494 0.480162   0.48030058
 0.4804278  0.48052147 0.48058772 0.4805904  0.48054737 0.4805567
 0.48052573 0.48045355 0.4803711  0.48033264 0.4803246  0.48038512
 0.4804855  0.48059818 0.48067164 0.48068994 0.48066556 0.48061535
 0.48055258 0.48059246 0.4806206  0.48064113 0.48064685 0.48068035
 0.480776   0.48092884 0.48112476 0.48122108 0.48108858 0.48066965
 0.48005977 0.47943577 0.47887793 0.47835502 0.4778245  0.47733942
 0.4768542  0.47638214 0.47590214 0.47541144 0.4749219  0.47438163
 0.47383568 0.47327808 0.47272655 0.47218746 0.47170988 0.47126842
 0.47084892 0.47044334 0.4701209  0.46989802 0.46975273 0.46963957
 0.4695825  0.46953613 0.46953833 0.46959692 0.46971992 0.46981505
 0.46994844 0.47007167 0.47020984 0.4703802  0.47055614 0.47075683
 0.47093815 0.47100532 0.47102594 0.47101325 0.47102207 0.47106096
 0.471171   0.4713688  0.47153816 0.47170204 0.47199085 0.47235313
 0.47259787 0.47270557 0.47267398 0.4725724  0.47256812 0.4726884
 0.47277194 0.4728015  0.47274786 0.47256747 0.47237796 0.47217962
 0.47193915 0.47169673 0.47147915 0.47118747 0.47093323 0.47073203
 0.47062466 0.4705988  0.4706361  0.47070858 0.4707567  0.47081214
 0.4708432  0.4707764  0.47069675 0.4706601  0.4706668  0.47068438
 0.4706901  0.47067714 0.4706754  0.47071186 0.47076282 0.470773
 0.4707098  0.4706429  0.47058874 0.47053188 0.47061312 0.47082984
 0.47114033 0.47144938 0.47166649 0.4717568  0.4716562  0.47140107
 0.4711075  0.4709276  0.47079834 0.47053957 0.47006464 0.4694315
 0.46879175 0.46822497 0.46787345 0.4676446  0.46730497 0.4669705
 0.46652344 0.46617362 0.46592537 0.4658393  0.46575505 0.46550164
 0.46507958 0.46456176 0.46417582 0.46392825 0.46389437 0.46400982
 0.4640867  0.46412027 0.46407646 0.4640191  0.46411556 0.46426445
 0.46449214 0.46467292 0.46478614 0.46497858 0.4652183  0.46564594
 0.46620727 0.4667559  0.46710333 0.46717143 0.46723884 0.46749443
 0.468175   0.46910113 0.46976146 0.47018072 0.4700149  0.46725097]
