Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=58, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9042432.0
params:  10266.0
Trainable parameters:  10266
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5478154
	speed: 0.0330s/iter; left time: 858.7109s
	iters: 200, epoch: 1 | loss: 0.4139126
	speed: 0.0310s/iter; left time: 802.7745s
Epoch: 1 cost time: 8.895135402679443
Epoch: 1, Steps: 261 | Train Loss: 0.5433564 Vali Loss: 1.1573466 Test Loss: 0.5527683
Validation loss decreased (inf --> 1.157347).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3456547
	speed: 0.1498s/iter; left time: 3856.3396s
	iters: 200, epoch: 2 | loss: 0.3292384
	speed: 0.0333s/iter; left time: 853.3566s
Epoch: 2 cost time: 9.617649555206299
Epoch: 2, Steps: 261 | Train Loss: 0.3506156 Vali Loss: 1.0488394 Test Loss: 0.4743547
Validation loss decreased (1.157347 --> 1.048839).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3186237
	speed: 0.1259s/iter; left time: 3208.4953s
	iters: 200, epoch: 3 | loss: 0.2954542
	speed: 0.0210s/iter; left time: 532.8890s
Epoch: 3 cost time: 6.198896408081055
Epoch: 3, Steps: 261 | Train Loss: 0.3145940 Vali Loss: 1.0102993 Test Loss: 0.4502877
Validation loss decreased (1.048839 --> 1.010299).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3359813
	speed: 0.1214s/iter; left time: 3060.6585s
	iters: 200, epoch: 4 | loss: 0.3021763
	speed: 0.0227s/iter; left time: 569.9691s
Epoch: 4 cost time: 7.158039808273315
Epoch: 4, Steps: 261 | Train Loss: 0.3009920 Vali Loss: 0.9900553 Test Loss: 0.4404451
Validation loss decreased (1.010299 --> 0.990055).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2939166
	speed: 0.1265s/iter; left time: 3157.0843s
	iters: 200, epoch: 5 | loss: 0.2967530
	speed: 0.0308s/iter; left time: 764.8414s
Epoch: 5 cost time: 8.997377395629883
Epoch: 5, Steps: 261 | Train Loss: 0.2940508 Vali Loss: 0.9813864 Test Loss: 0.4353144
Validation loss decreased (0.990055 --> 0.981386).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2840439
	speed: 0.1900s/iter; left time: 4691.5453s
	iters: 200, epoch: 6 | loss: 0.2870752
	speed: 0.0586s/iter; left time: 1441.0513s
Epoch: 6 cost time: 12.716562509536743
Epoch: 6, Steps: 261 | Train Loss: 0.2900454 Vali Loss: 0.9751589 Test Loss: 0.4323888
Validation loss decreased (0.981386 --> 0.975159).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2702107
	speed: 0.1594s/iter; left time: 3894.4802s
	iters: 200, epoch: 7 | loss: 0.2669966
	speed: 0.0367s/iter; left time: 893.4115s
Epoch: 7 cost time: 11.198239803314209
Epoch: 7, Steps: 261 | Train Loss: 0.2874995 Vali Loss: 0.9719617 Test Loss: 0.4311733
Validation loss decreased (0.975159 --> 0.971962).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2780042
	speed: 0.1613s/iter; left time: 3898.7684s
	iters: 200, epoch: 8 | loss: 0.2925807
	speed: 0.0270s/iter; left time: 650.3381s
Epoch: 8 cost time: 10.450575590133667
Epoch: 8, Steps: 261 | Train Loss: 0.2859533 Vali Loss: 0.9691030 Test Loss: 0.4300918
Validation loss decreased (0.971962 --> 0.969103).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2962699
	speed: 0.1204s/iter; left time: 2878.3597s
	iters: 200, epoch: 9 | loss: 0.2796557
	speed: 0.0258s/iter; left time: 615.3683s
Epoch: 9 cost time: 7.468408823013306
Epoch: 9, Steps: 261 | Train Loss: 0.2848327 Vali Loss: 0.9673898 Test Loss: 0.4298410
Validation loss decreased (0.969103 --> 0.967390).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2796457
	speed: 0.1252s/iter; left time: 2962.2275s
	iters: 200, epoch: 10 | loss: 0.3185675
	speed: 0.0314s/iter; left time: 738.8558s
Epoch: 10 cost time: 7.614323139190674
Epoch: 10, Steps: 261 | Train Loss: 0.2840801 Vali Loss: 0.9661725 Test Loss: 0.4291669
Validation loss decreased (0.967390 --> 0.966173).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2982499
	speed: 0.1330s/iter; left time: 3112.1380s
	iters: 200, epoch: 11 | loss: 0.2979800
	speed: 0.0232s/iter; left time: 541.0661s
Epoch: 11 cost time: 7.684973478317261
Epoch: 11, Steps: 261 | Train Loss: 0.2836322 Vali Loss: 0.9656311 Test Loss: 0.4288929
Validation loss decreased (0.966173 --> 0.965631).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2773348
	speed: 0.1338s/iter; left time: 3094.9496s
	iters: 200, epoch: 12 | loss: 0.2700230
	speed: 0.0321s/iter; left time: 738.3588s
Epoch: 12 cost time: 9.173290252685547
Epoch: 12, Steps: 261 | Train Loss: 0.2832638 Vali Loss: 0.9649661 Test Loss: 0.4289295
Validation loss decreased (0.965631 --> 0.964966).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2877194
	speed: 0.1465s/iter; left time: 3349.3391s
	iters: 200, epoch: 13 | loss: 0.2529321
	speed: 0.0410s/iter; left time: 933.8313s
Epoch: 13 cost time: 9.749807357788086
Epoch: 13, Steps: 261 | Train Loss: 0.2829471 Vali Loss: 0.9649495 Test Loss: 0.4285832
Validation loss decreased (0.964966 --> 0.964949).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2630908
	speed: 0.1250s/iter; left time: 2825.5177s
	iters: 200, epoch: 14 | loss: 0.3098297
	speed: 0.0392s/iter; left time: 882.8692s
Epoch: 14 cost time: 9.065929889678955
Epoch: 14, Steps: 261 | Train Loss: 0.2827860 Vali Loss: 0.9641559 Test Loss: 0.4286418
Validation loss decreased (0.964949 --> 0.964156).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2980556
	speed: 0.1119s/iter; left time: 2501.6917s
	iters: 200, epoch: 15 | loss: 0.2717115
	speed: 0.0255s/iter; left time: 566.5978s
Epoch: 15 cost time: 7.239795923233032
Epoch: 15, Steps: 261 | Train Loss: 0.2826424 Vali Loss: 0.9645438 Test Loss: 0.4282556
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2791538
	speed: 0.1226s/iter; left time: 2707.4344s
	iters: 200, epoch: 16 | loss: 0.2521793
	speed: 0.0299s/iter; left time: 657.4331s
Epoch: 16 cost time: 7.971073627471924
Epoch: 16, Steps: 261 | Train Loss: 0.2825755 Vali Loss: 0.9638118 Test Loss: 0.4283583
Validation loss decreased (0.964156 --> 0.963812).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2756033
	speed: 0.1171s/iter; left time: 2556.6399s
	iters: 200, epoch: 17 | loss: 0.2881671
	speed: 0.0236s/iter; left time: 512.6829s
Epoch: 17 cost time: 7.064811944961548
Epoch: 17, Steps: 261 | Train Loss: 0.2824964 Vali Loss: 0.9639149 Test Loss: 0.4286229
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2905228
	speed: 0.1181s/iter; left time: 2546.9289s
	iters: 200, epoch: 18 | loss: 0.2901858
	speed: 0.0348s/iter; left time: 747.1187s
Epoch: 18 cost time: 9.997236967086792
Epoch: 18, Steps: 261 | Train Loss: 0.2824538 Vali Loss: 0.9639505 Test Loss: 0.4285005
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2882228
	speed: 0.1462s/iter; left time: 3113.9898s
	iters: 200, epoch: 19 | loss: 0.2716658
	speed: 0.0319s/iter; left time: 677.3510s
Epoch: 19 cost time: 8.884083032608032
Epoch: 19, Steps: 261 | Train Loss: 0.2824011 Vali Loss: 0.9638072 Test Loss: 0.4284362
Validation loss decreased (0.963812 --> 0.963807).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2860930
	speed: 0.1715s/iter; left time: 3608.0907s
	iters: 200, epoch: 20 | loss: 0.2844820
	speed: 0.0414s/iter; left time: 866.4862s
Epoch: 20 cost time: 10.487718343734741
Epoch: 20, Steps: 261 | Train Loss: 0.2823757 Vali Loss: 0.9638442 Test Loss: 0.4282881
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2869617
	speed: 0.1730s/iter; left time: 3595.3752s
	iters: 200, epoch: 21 | loss: 0.2972098
	speed: 0.0333s/iter; left time: 689.2380s
Epoch: 21 cost time: 9.723345756530762
Epoch: 21, Steps: 261 | Train Loss: 0.2824401 Vali Loss: 0.9632533 Test Loss: 0.4284615
Validation loss decreased (0.963807 --> 0.963253).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2852695
	speed: 0.1338s/iter; left time: 2745.5845s
	iters: 200, epoch: 22 | loss: 0.2852436
	speed: 0.0261s/iter; left time: 533.9502s
Epoch: 22 cost time: 9.396230459213257
Epoch: 22, Steps: 261 | Train Loss: 0.2823068 Vali Loss: 0.9629732 Test Loss: 0.4283805
Validation loss decreased (0.963253 --> 0.962973).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2846546
	speed: 0.1382s/iter; left time: 2799.1537s
	iters: 200, epoch: 23 | loss: 0.2503220
	speed: 0.0245s/iter; left time: 493.4355s
Epoch: 23 cost time: 7.328829765319824
Epoch: 23, Steps: 261 | Train Loss: 0.2823174 Vali Loss: 0.9642197 Test Loss: 0.4284060
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2789953
	speed: 0.1328s/iter; left time: 2656.0845s
	iters: 200, epoch: 24 | loss: 0.2766431
	speed: 0.0242s/iter; left time: 481.7803s
Epoch: 24 cost time: 6.602134466171265
Epoch: 24, Steps: 261 | Train Loss: 0.2823416 Vali Loss: 0.9631453 Test Loss: 0.4282359
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2851810
	speed: 0.1051s/iter; left time: 2074.1875s
	iters: 200, epoch: 25 | loss: 0.2882183
	speed: 0.0241s/iter; left time: 472.3127s
Epoch: 25 cost time: 7.001292705535889
Epoch: 25, Steps: 261 | Train Loss: 0.2823698 Vali Loss: 0.9630646 Test Loss: 0.4285267
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2763374
	speed: 0.1316s/iter; left time: 2563.4856s
	iters: 200, epoch: 26 | loss: 0.2679335
	speed: 0.0282s/iter; left time: 546.8715s
Epoch: 26 cost time: 8.038869380950928
Epoch: 26, Steps: 261 | Train Loss: 0.2823000 Vali Loss: 0.9635936 Test Loss: 0.4285326
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2744223
	speed: 0.1529s/iter; left time: 2938.7044s
	iters: 200, epoch: 27 | loss: 0.2814814
	speed: 0.0307s/iter; left time: 586.0116s
Epoch: 27 cost time: 9.36699891090393
Epoch: 27, Steps: 261 | Train Loss: 0.2823872 Vali Loss: 0.9645902 Test Loss: 0.4284709
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2785855
	speed: 0.1443s/iter; left time: 2734.2840s
	iters: 200, epoch: 28 | loss: 0.2926791
	speed: 0.0390s/iter; left time: 735.5467s
Epoch: 28 cost time: 11.616994857788086
Epoch: 28, Steps: 261 | Train Loss: 0.2823109 Vali Loss: 0.9628987 Test Loss: 0.4285748
Validation loss decreased (0.962973 --> 0.962899).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2790113
	speed: 0.1417s/iter; left time: 2648.3048s
	iters: 200, epoch: 29 | loss: 0.2760443
	speed: 0.0218s/iter; left time: 405.5542s
Epoch: 29 cost time: 6.990625858306885
Epoch: 29, Steps: 261 | Train Loss: 0.2823358 Vali Loss: 0.9632711 Test Loss: 0.4286589
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2627028
	speed: 0.1128s/iter; left time: 2079.2480s
	iters: 200, epoch: 30 | loss: 0.2934821
	speed: 0.0233s/iter; left time: 427.8677s
Epoch: 30 cost time: 7.221188306808472
Epoch: 30, Steps: 261 | Train Loss: 0.2823025 Vali Loss: 0.9640990 Test Loss: 0.4283973
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2785438
	speed: 0.1311s/iter; left time: 2382.6485s
	iters: 200, epoch: 31 | loss: 0.2715940
	speed: 0.0355s/iter; left time: 641.8435s
Epoch: 31 cost time: 9.611905813217163
Epoch: 31, Steps: 261 | Train Loss: 0.2823056 Vali Loss: 0.9636224 Test Loss: 0.4284232
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2705278
	speed: 0.1536s/iter; left time: 2751.3800s
	iters: 200, epoch: 32 | loss: 0.2860079
	speed: 0.0344s/iter; left time: 611.9451s
Epoch: 32 cost time: 10.161141633987427
Epoch: 32, Steps: 261 | Train Loss: 0.2823329 Vali Loss: 0.9638183 Test Loss: 0.4285625
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2484774
	speed: 0.1610s/iter; left time: 2842.2575s
	iters: 200, epoch: 33 | loss: 0.2673750
	speed: 0.0399s/iter; left time: 700.2641s
Epoch: 33 cost time: 9.900562286376953
Epoch: 33, Steps: 261 | Train Loss: 0.2822580 Vali Loss: 0.9639580 Test Loss: 0.4284366
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3012072
	speed: 0.1241s/iter; left time: 2158.0115s
	iters: 200, epoch: 34 | loss: 0.3005177
	speed: 0.0278s/iter; left time: 480.4060s
Epoch: 34 cost time: 7.157217979431152
Epoch: 34, Steps: 261 | Train Loss: 0.2823111 Vali Loss: 0.9632493 Test Loss: 0.4284810
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2615904
	speed: 0.1149s/iter; left time: 1967.5368s
	iters: 200, epoch: 35 | loss: 0.2649688
	speed: 0.0227s/iter; left time: 385.8693s
Epoch: 35 cost time: 7.348794460296631
Epoch: 35, Steps: 261 | Train Loss: 0.2822042 Vali Loss: 0.9638430 Test Loss: 0.4282939
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2687541
	speed: 0.1480s/iter; left time: 2496.5838s
	iters: 200, epoch: 36 | loss: 0.2636256
	speed: 0.0363s/iter; left time: 608.2615s
Epoch: 36 cost time: 10.961625576019287
Epoch: 36, Steps: 261 | Train Loss: 0.2822996 Vali Loss: 0.9641096 Test Loss: 0.4284667
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2656156
	speed: 0.1739s/iter; left time: 2888.0029s
	iters: 200, epoch: 37 | loss: 0.2786989
	speed: 0.0283s/iter; left time: 466.8274s
Epoch: 37 cost time: 8.047039985656738
Epoch: 37, Steps: 261 | Train Loss: 0.2822928 Vali Loss: 0.9642332 Test Loss: 0.4284083
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2811947
	speed: 0.1785s/iter; left time: 2917.2925s
	iters: 200, epoch: 38 | loss: 0.3087019
	speed: 0.0318s/iter; left time: 517.2432s
Epoch: 38 cost time: 10.255001544952393
Epoch: 38, Steps: 261 | Train Loss: 0.2822636 Vali Loss: 0.9631954 Test Loss: 0.4283629
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2737885
	speed: 0.1241s/iter; left time: 1996.0121s
	iters: 200, epoch: 39 | loss: 0.2875442
	speed: 0.0232s/iter; left time: 370.9563s
Epoch: 39 cost time: 6.93897271156311
Epoch: 39, Steps: 261 | Train Loss: 0.2822833 Vali Loss: 0.9642075 Test Loss: 0.4284422
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2970956
	speed: 0.1153s/iter; left time: 1824.9504s
	iters: 200, epoch: 40 | loss: 0.2882753
	speed: 0.0291s/iter; left time: 457.9162s
Epoch: 40 cost time: 7.417269229888916
Epoch: 40, Steps: 261 | Train Loss: 0.2822943 Vali Loss: 0.9633918 Test Loss: 0.4285007
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2836321
	speed: 0.1322s/iter; left time: 2056.8608s
	iters: 200, epoch: 41 | loss: 0.2845726
	speed: 0.0293s/iter; left time: 453.5448s
Epoch: 41 cost time: 7.813838720321655
Epoch: 41, Steps: 261 | Train Loss: 0.2822146 Vali Loss: 0.9640614 Test Loss: 0.4284342
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2994758
	speed: 0.1333s/iter; left time: 2038.8460s
	iters: 200, epoch: 42 | loss: 0.2544149
	speed: 0.0253s/iter; left time: 384.7635s
Epoch: 42 cost time: 7.4146363735198975
Epoch: 42, Steps: 261 | Train Loss: 0.2822732 Vali Loss: 0.9636704 Test Loss: 0.4284430
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2853092
	speed: 0.1462s/iter; left time: 2199.3036s
	iters: 200, epoch: 43 | loss: 0.2803714
	speed: 0.0318s/iter; left time: 474.6593s
Epoch: 43 cost time: 10.106738567352295
Epoch: 43, Steps: 261 | Train Loss: 0.2822566 Vali Loss: 0.9631169 Test Loss: 0.4284546
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2883165
	speed: 0.1692s/iter; left time: 2500.3220s
	iters: 200, epoch: 44 | loss: 0.2886392
	speed: 0.0338s/iter; left time: 496.2280s
Epoch: 44 cost time: 9.216439485549927
Epoch: 44, Steps: 261 | Train Loss: 0.2822702 Vali Loss: 0.9638610 Test Loss: 0.4285353
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2905440
	speed: 0.1355s/iter; left time: 1967.2052s
	iters: 200, epoch: 45 | loss: 0.3152905
	speed: 0.0249s/iter; left time: 359.1736s
Epoch: 45 cost time: 6.894128799438477
Epoch: 45, Steps: 261 | Train Loss: 0.2822883 Vali Loss: 0.9641897 Test Loss: 0.4285288
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2904758
	speed: 0.1151s/iter; left time: 1641.3413s
	iters: 200, epoch: 46 | loss: 0.2574814
	speed: 0.0265s/iter; left time: 375.1600s
Epoch: 46 cost time: 7.647704124450684
Epoch: 46, Steps: 261 | Train Loss: 0.2822960 Vali Loss: 0.9632769 Test Loss: 0.4284559
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2753003
	speed: 0.1252s/iter; left time: 1751.9414s
	iters: 200, epoch: 47 | loss: 0.2610206
	speed: 0.0270s/iter; left time: 374.9688s
Epoch: 47 cost time: 7.95157790184021
Epoch: 47, Steps: 261 | Train Loss: 0.2822571 Vali Loss: 0.9640853 Test Loss: 0.4284666
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2714328
	speed: 0.1460s/iter; left time: 2005.0808s
	iters: 200, epoch: 48 | loss: 0.2802497
	speed: 0.0340s/iter; left time: 462.9173s
Epoch: 48 cost time: 9.89453649520874
Epoch: 48, Steps: 261 | Train Loss: 0.2822607 Vali Loss: 0.9644904 Test Loss: 0.4284873
EarlyStopping counter: 20 out of 20
Early stopping
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=58, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9042432.0
params:  10266.0
Trainable parameters:  10266
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4285814
	speed: 0.0369s/iter; left time: 959.0273s
	iters: 200, epoch: 1 | loss: 0.4460332
	speed: 0.0357s/iter; left time: 924.1376s
Epoch: 1 cost time: 9.553548812866211
Epoch: 1, Steps: 261 | Train Loss: 0.4144529 Vali Loss: 0.9614167 Test Loss: 0.4274219
Validation loss decreased (inf --> 0.961417).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4260105
	speed: 0.1362s/iter; left time: 3505.1449s
	iters: 200, epoch: 2 | loss: 0.3580141
	speed: 0.0316s/iter; left time: 811.1730s
Epoch: 2 cost time: 7.536748647689819
Epoch: 2, Steps: 261 | Train Loss: 0.4140018 Vali Loss: 0.9612134 Test Loss: 0.4275995
Validation loss decreased (0.961417 --> 0.961213).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3670676
	speed: 0.1529s/iter; left time: 3896.9328s
	iters: 200, epoch: 3 | loss: 0.3953141
	speed: 0.0372s/iter; left time: 944.9169s
Epoch: 3 cost time: 11.218113899230957
Epoch: 3, Steps: 261 | Train Loss: 0.4138519 Vali Loss: 0.9614874 Test Loss: 0.4267724
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4342177
	speed: 0.1502s/iter; left time: 3786.5590s
	iters: 200, epoch: 4 | loss: 0.4064512
	speed: 0.0316s/iter; left time: 792.9340s
Epoch: 4 cost time: 8.36643123626709
Epoch: 4, Steps: 261 | Train Loss: 0.4137233 Vali Loss: 0.9606759 Test Loss: 0.4273018
Validation loss decreased (0.961213 --> 0.960676).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4226350
	speed: 0.1669s/iter; left time: 4165.4203s
	iters: 200, epoch: 5 | loss: 0.3528039
	speed: 0.0332s/iter; left time: 825.0296s
Epoch: 5 cost time: 9.483531951904297
Epoch: 5, Steps: 261 | Train Loss: 0.4136393 Vali Loss: 0.9592596 Test Loss: 0.4271505
Validation loss decreased (0.960676 --> 0.959260).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4323377
	speed: 0.1306s/iter; left time: 3224.8433s
	iters: 200, epoch: 6 | loss: 0.4475047
	speed: 0.0212s/iter; left time: 520.8851s
Epoch: 6 cost time: 7.715898513793945
Epoch: 6, Steps: 261 | Train Loss: 0.4136083 Vali Loss: 0.9606341 Test Loss: 0.4274431
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3893066
	speed: 0.1560s/iter; left time: 3812.6644s
	iters: 200, epoch: 7 | loss: 0.3867274
	speed: 0.0235s/iter; left time: 572.8345s
Epoch: 7 cost time: 7.825367212295532
Epoch: 7, Steps: 261 | Train Loss: 0.4135901 Vali Loss: 0.9606501 Test Loss: 0.4271457
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4004812
	speed: 0.1167s/iter; left time: 2819.9237s
	iters: 200, epoch: 8 | loss: 0.4167315
	speed: 0.0254s/iter; left time: 612.0699s
Epoch: 8 cost time: 7.343804121017456
Epoch: 8, Steps: 261 | Train Loss: 0.4134251 Vali Loss: 0.9597412 Test Loss: 0.4278312
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3934323
	speed: 0.1250s/iter; left time: 2988.1273s
	iters: 200, epoch: 9 | loss: 0.4291850
	speed: 0.0250s/iter; left time: 594.2803s
Epoch: 9 cost time: 7.468620538711548
Epoch: 9, Steps: 261 | Train Loss: 0.4135090 Vali Loss: 0.9604758 Test Loss: 0.4270620
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3956112
	speed: 0.1423s/iter; left time: 3365.8708s
	iters: 200, epoch: 10 | loss: 0.4393564
	speed: 0.0433s/iter; left time: 1019.6298s
Epoch: 10 cost time: 10.482378005981445
Epoch: 10, Steps: 261 | Train Loss: 0.4134581 Vali Loss: 0.9602653 Test Loss: 0.4274448
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4542514
	speed: 0.1532s/iter; left time: 3583.7395s
	iters: 200, epoch: 11 | loss: 0.4049298
	speed: 0.0301s/iter; left time: 701.4769s
Epoch: 11 cost time: 8.635890245437622
Epoch: 11, Steps: 261 | Train Loss: 0.4133361 Vali Loss: 0.9587529 Test Loss: 0.4271559
Validation loss decreased (0.959260 --> 0.958753).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4269917
	speed: 0.1464s/iter; left time: 3386.0075s
	iters: 200, epoch: 12 | loss: 0.4038110
	speed: 0.0383s/iter; left time: 881.4874s
Epoch: 12 cost time: 9.277260541915894
Epoch: 12, Steps: 261 | Train Loss: 0.4134148 Vali Loss: 0.9605191 Test Loss: 0.4271997
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4018134
	speed: 0.1191s/iter; left time: 2723.2893s
	iters: 200, epoch: 13 | loss: 0.4251506
	speed: 0.0297s/iter; left time: 675.1405s
Epoch: 13 cost time: 8.097171306610107
Epoch: 13, Steps: 261 | Train Loss: 0.4133919 Vali Loss: 0.9601842 Test Loss: 0.4271665
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4035568
	speed: 0.1283s/iter; left time: 2900.1114s
	iters: 200, epoch: 14 | loss: 0.4167934
	speed: 0.0330s/iter; left time: 742.1509s
Epoch: 14 cost time: 7.967607736587524
Epoch: 14, Steps: 261 | Train Loss: 0.4133569 Vali Loss: 0.9604461 Test Loss: 0.4273546
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4113644
	speed: 0.1201s/iter; left time: 2684.4196s
	iters: 200, epoch: 15 | loss: 0.4179480
	speed: 0.0235s/iter; left time: 521.8762s
Epoch: 15 cost time: 7.71337890625
Epoch: 15, Steps: 261 | Train Loss: 0.4133783 Vali Loss: 0.9601738 Test Loss: 0.4274341
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3873159
	speed: 0.1485s/iter; left time: 3280.0015s
	iters: 200, epoch: 16 | loss: 0.4325646
	speed: 0.0349s/iter; left time: 768.2598s
Epoch: 16 cost time: 8.97107720375061
Epoch: 16, Steps: 261 | Train Loss: 0.4133066 Vali Loss: 0.9597715 Test Loss: 0.4270639
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4169821
	speed: 0.1455s/iter; left time: 3176.0175s
	iters: 200, epoch: 17 | loss: 0.3900071
	speed: 0.0335s/iter; left time: 727.0275s
Epoch: 17 cost time: 9.006786346435547
Epoch: 17, Steps: 261 | Train Loss: 0.4134618 Vali Loss: 0.9601707 Test Loss: 0.4273849
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3968104
	speed: 0.2032s/iter; left time: 4381.1743s
	iters: 200, epoch: 18 | loss: 0.4237818
	speed: 0.0414s/iter; left time: 889.0332s
Epoch: 18 cost time: 11.192069053649902
Epoch: 18, Steps: 261 | Train Loss: 0.4132456 Vali Loss: 0.9603220 Test Loss: 0.4274203
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3994603
	speed: 0.1262s/iter; left time: 2688.7642s
	iters: 200, epoch: 19 | loss: 0.3968334
	speed: 0.0219s/iter; left time: 464.9504s
Epoch: 19 cost time: 6.812392950057983
Epoch: 19, Steps: 261 | Train Loss: 0.4132803 Vali Loss: 0.9596643 Test Loss: 0.4271185
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4132729
	speed: 0.1200s/iter; left time: 2524.0858s
	iters: 200, epoch: 20 | loss: 0.4020642
	speed: 0.0293s/iter; left time: 614.4784s
Epoch: 20 cost time: 8.603215217590332
Epoch: 20, Steps: 261 | Train Loss: 0.4132864 Vali Loss: 0.9593778 Test Loss: 0.4272758
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4250599
	speed: 0.1489s/iter; left time: 3094.8844s
	iters: 200, epoch: 21 | loss: 0.4324968
	speed: 0.0354s/iter; left time: 732.0425s
Epoch: 21 cost time: 9.707253694534302
Epoch: 21, Steps: 261 | Train Loss: 0.4133023 Vali Loss: 0.9595101 Test Loss: 0.4274381
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4292508
	speed: 0.1271s/iter; left time: 2608.3930s
	iters: 200, epoch: 22 | loss: 0.4294909
	speed: 0.0379s/iter; left time: 773.1982s
Epoch: 22 cost time: 9.346481561660767
Epoch: 22, Steps: 261 | Train Loss: 0.4133609 Vali Loss: 0.9596303 Test Loss: 0.4272392
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4252422
	speed: 0.1368s/iter; left time: 2772.3567s
	iters: 200, epoch: 23 | loss: 0.4485373
	speed: 0.0488s/iter; left time: 983.2713s
Epoch: 23 cost time: 10.201106071472168
Epoch: 23, Steps: 261 | Train Loss: 0.4132797 Vali Loss: 0.9606069 Test Loss: 0.4271942
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3974930
	speed: 0.1197s/iter; left time: 2393.0323s
	iters: 200, epoch: 24 | loss: 0.3992373
	speed: 0.0247s/iter; left time: 492.1749s
Epoch: 24 cost time: 7.066258907318115
Epoch: 24, Steps: 261 | Train Loss: 0.4131984 Vali Loss: 0.9600403 Test Loss: 0.4273226
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3905537
	speed: 0.1229s/iter; left time: 2426.0408s
	iters: 200, epoch: 25 | loss: 0.3838596
	speed: 0.0246s/iter; left time: 483.4588s
Epoch: 25 cost time: 6.703755855560303
Epoch: 25, Steps: 261 | Train Loss: 0.4132688 Vali Loss: 0.9597476 Test Loss: 0.4274492
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3999769
	speed: 0.1219s/iter; left time: 2373.4351s
	iters: 200, epoch: 26 | loss: 0.4067197
	speed: 0.0248s/iter; left time: 481.2322s
Epoch: 26 cost time: 7.392750024795532
Epoch: 26, Steps: 261 | Train Loss: 0.4132539 Vali Loss: 0.9593371 Test Loss: 0.4273830
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3925255
	speed: 0.1358s/iter; left time: 2608.7072s
	iters: 200, epoch: 27 | loss: 0.3967104
	speed: 0.0284s/iter; left time: 543.7628s
Epoch: 27 cost time: 9.365216732025146
Epoch: 27, Steps: 261 | Train Loss: 0.4132999 Vali Loss: 0.9598983 Test Loss: 0.4275287
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4019775
	speed: 0.1453s/iter; left time: 2753.8891s
	iters: 200, epoch: 28 | loss: 0.4011777
	speed: 0.0261s/iter; left time: 491.9242s
Epoch: 28 cost time: 7.674169301986694
Epoch: 28, Steps: 261 | Train Loss: 0.4131679 Vali Loss: 0.9598609 Test Loss: 0.4273176
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3968457
	speed: 0.1356s/iter; left time: 2535.6895s
	iters: 200, epoch: 29 | loss: 0.4249021
	speed: 0.0223s/iter; left time: 413.8860s
Epoch: 29 cost time: 6.936601161956787
Epoch: 29, Steps: 261 | Train Loss: 0.4132044 Vali Loss: 0.9595952 Test Loss: 0.4272487
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3508827
	speed: 0.1189s/iter; left time: 2191.3592s
	iters: 200, epoch: 30 | loss: 0.3879249
	speed: 0.0275s/iter; left time: 504.9598s
Epoch: 30 cost time: 7.48934531211853
Epoch: 30, Steps: 261 | Train Loss: 0.4132511 Vali Loss: 0.9596711 Test Loss: 0.4273615
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3897172
	speed: 0.1273s/iter; left time: 2313.5920s
	iters: 200, epoch: 31 | loss: 0.3963337
	speed: 0.0258s/iter; left time: 465.8228s
Epoch: 31 cost time: 7.8933703899383545
Epoch: 31, Steps: 261 | Train Loss: 0.4132584 Vali Loss: 0.9593017 Test Loss: 0.4272557
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.42681920528411865, mae:0.41588789224624634, rse:0.621573805809021, corr:[0.5257187  0.53421825 0.53505266 0.53335047 0.53293496 0.53403527
 0.5350362  0.5348691  0.53405994 0.53386056 0.53480756 0.5358738
 0.5360555  0.53515923 0.5337585  0.5324514  0.53145874 0.53062457
 0.529542   0.52821237 0.5267154  0.5254357  0.5243042  0.52333003
 0.5223291  0.52111214 0.5199281  0.5190569  0.5185499  0.51823187
 0.5182538  0.5186524  0.5191934  0.5196707  0.5198991  0.52002376
 0.51997954 0.5196619  0.5191797  0.5186469  0.5181591  0.5176905
 0.51729035 0.5171517  0.51728463 0.5175848  0.5178701  0.5177829
 0.5172491  0.51665497 0.51633805 0.5164284  0.5168402  0.51715505
 0.5171181  0.5167866  0.51633936 0.5160639  0.51611793 0.5162395
 0.5163758  0.5163096  0.5160998  0.5157874  0.5156464  0.5157733
 0.51599085 0.51610845 0.5162601  0.51634926 0.5163812  0.51635456
 0.5163859  0.5164834  0.5165439  0.51650757 0.51641285 0.51629597
 0.516123   0.51587576 0.5156082  0.5154247  0.5153594  0.5153364
 0.51539284 0.5153233  0.51516104 0.5148794  0.51454926 0.51438373
 0.5144086  0.5144756  0.51449007 0.5144455  0.5142575  0.51380676
 0.51314133 0.51238245 0.51164645 0.5110801  0.5108982  0.5110849
 0.51141316 0.5115536  0.51166016 0.5118932  0.51222324 0.5127514
 0.5131302  0.5133434  0.5132903  0.5131017  0.51289743 0.5129181
 0.513081   0.5132755  0.5133291  0.51323384 0.51303315 0.51300424
 0.51311433 0.51316166 0.5130936  0.513048   0.5129868  0.51282346
 0.51257306 0.5122885  0.5119915  0.51176596 0.5116715  0.5116756
 0.5117451  0.5116651  0.51144654 0.51109195 0.51072884 0.5104008
 0.51021844 0.5100684  0.50980073 0.5095036  0.50926363 0.50921327
 0.5093211  0.509458   0.5094211  0.5092122  0.50900084 0.50879383
 0.5087039  0.5088247  0.5089268  0.5089089  0.5088589  0.5089572
 0.50919664 0.5094805  0.50961703 0.5095748  0.50942993 0.5092532
 0.5091982  0.5092996  0.50959724 0.50997585 0.5103161  0.5106919
 0.5110845  0.5114862  0.5118689  0.5121823  0.5122419  0.51207477
 0.51177293 0.5115099  0.51133543 0.5112861  0.5112161  0.5111275
 0.5109751  0.5108863  0.5107786  0.5107045  0.5106419  0.51061106
 0.51059335 0.5106298  0.5106608  0.5105947  0.5103316  0.5099395
 0.50955045 0.5093417  0.509184   0.5087961  0.5081545  0.5075348
 0.5070414  0.5067509  0.50674313 0.5068369  0.5067868  0.50657123
 0.50624937 0.50579256 0.5053951  0.5049862  0.5045535  0.504135
 0.5036627  0.5032473  0.50283533 0.5024787  0.50206333 0.5016667
 0.5012806  0.5008737  0.50042546 0.5001316  0.49992785 0.4997879
 0.49977446 0.49977875 0.49978536 0.49981445 0.49994305 0.50004596
 0.5000903  0.50006044 0.49988985 0.49957487 0.49926654 0.4990085
 0.49890184 0.49887457 0.4987028  0.49855262 0.49839783 0.49825335
 0.49812898 0.49805507 0.49808192 0.49810877 0.49805388 0.4980006
 0.49794635 0.49790227 0.4979     0.4979371  0.49787167 0.49780825
 0.49770805 0.49775127 0.49787834 0.49803096 0.49803293 0.49802577
 0.49798775 0.49803492 0.4982065  0.49844298 0.49856964 0.49873912
 0.49895862 0.499239   0.49952874 0.499928   0.5003141  0.5005193
 0.5006504  0.5006995  0.500644   0.5005324  0.500332   0.5001792
 0.5000043  0.49988884 0.49966395 0.4994475  0.4992207  0.49905452
 0.49904215 0.49902904 0.49893573 0.4986604  0.4982128  0.49766806
 0.49704856 0.49651572 0.49608096 0.49547553 0.4948693  0.49437946
 0.49407953 0.49380288 0.493609   0.49336088 0.4930798  0.49273154
 0.49233752 0.49192882 0.49153438 0.4911697  0.4908283  0.49057567
 0.4905112  0.49055308 0.4906766  0.4908193  0.4909183  0.49090266
 0.4908557  0.490761   0.49057853 0.490508   0.49049473 0.49051747
 0.4905181  0.49045962 0.49033222 0.49020037 0.49008426 0.48994732
 0.4897719  0.48953918 0.48922634 0.4889502  0.48868918 0.48853126
 0.48843548 0.48836192 0.48817965 0.4878448  0.48743132 0.48708373
 0.4868899  0.48689654 0.4870287  0.48705223 0.48703882 0.48705027
 0.48705745 0.48712888 0.48727408 0.48734203 0.48736483 0.48737913
 0.4873441  0.48738372 0.48748267 0.4875146  0.48746085 0.48734888
 0.48717356 0.48704135 0.4870867  0.48727098 0.48752096 0.48778293
 0.48794106 0.4879904  0.4879603  0.4879147  0.48790038 0.48793307
 0.48795524 0.48799467 0.48805693 0.48807213 0.4881223  0.48823455
 0.48842016 0.48859304 0.4887415  0.48886797 0.48895088 0.4890183
 0.4891122  0.48920926 0.4892867  0.4892693  0.48913237 0.48887274
 0.4884085  0.4878249  0.4871863  0.48646036 0.48577487 0.48540133
 0.48529917 0.48523828 0.48517704 0.48512885 0.48501024 0.48488268
 0.48467284 0.48451555 0.484375   0.48422205 0.48403174 0.48388457
 0.48389918 0.48402098 0.4841125  0.48407254 0.48392558 0.48378333
 0.48370448 0.4835752  0.48346278 0.48345035 0.48342395 0.48332173
 0.48319003 0.4830276  0.4829562  0.48291758 0.4829136  0.48290202
 0.4828267  0.4827377  0.482534   0.4822887  0.48197973 0.48164934
 0.48130423 0.48110598 0.48094594 0.48085675 0.4807915  0.48083407
 0.48074287 0.48052624 0.4803432  0.48024055 0.48018923 0.4802509
 0.48035452 0.48048183 0.48059344 0.4806262  0.48060027 0.48056728
 0.4804474  0.48035362 0.48028103 0.48024046 0.48025677 0.4803394
 0.4804906  0.48071525 0.4809524  0.48116088 0.48132175 0.4814722
 0.48146877 0.48140436 0.4812948  0.48130217 0.4814286  0.4816204
 0.48183236 0.48202643 0.48206472 0.48203948 0.4819913  0.48197857
 0.48203754 0.48210567 0.48212284 0.48206773 0.4819501  0.48183447
 0.48171175 0.48162407 0.48150733 0.4812372  0.48074377 0.48002735
 0.47913134 0.47823843 0.47740617 0.47662008 0.47592458 0.47532436
 0.4747703  0.474107   0.47341388 0.47283486 0.47235072 0.47194666
 0.47168383 0.4715404  0.47135863 0.4710592  0.47077507 0.47046363
 0.47019708 0.47014076 0.47026923 0.4704009  0.47055054 0.47070768
 0.47085002 0.47079322 0.47062004 0.47048682 0.47048855 0.4706726
 0.47089544 0.4709864  0.47088277 0.47068873 0.47045356 0.47040844
 0.47052908 0.47068262 0.47077808 0.47056744 0.47012943 0.46968555
 0.4693543  0.46915445 0.46901712 0.46886873 0.46875736 0.468809
 0.46895137 0.4690491  0.46901804 0.46882746 0.46854967 0.46832663
 0.46822292 0.4681842  0.4682184  0.46826762 0.46830738 0.46834934
 0.46843755 0.46852526 0.4685645  0.46853018 0.46849895 0.46849477
 0.46848726 0.468505   0.46859184 0.46866924 0.46865058 0.46865535
 0.46862796 0.4687111  0.4688994  0.4691891  0.46948233 0.46960822
 0.46960604 0.46959296 0.46963584 0.4697985  0.47004467 0.4702759
 0.47032917 0.47032478 0.47018644 0.47004372 0.4699774  0.4700105
 0.47014678 0.47021458 0.4701481  0.46981907 0.46918738 0.4683184
 0.4673991  0.46659097 0.4659656  0.4652579  0.4645209  0.46392217
 0.4635287  0.46319818 0.46296173 0.46274173 0.46237427 0.46183443
 0.46124512 0.46067628 0.46021184 0.45983806 0.45946738 0.4591753
 0.45886412 0.45874366 0.45878628 0.45893374 0.45907965 0.45922965
 0.45933977 0.45921686 0.45894805 0.45876715 0.45873392 0.45877257
 0.45882133 0.45885387 0.4588122  0.45872894 0.45866016 0.45867655
 0.45870292 0.45871753 0.4587009  0.45855537 0.4583538  0.45805636
 0.45770395 0.4573389  0.45706844 0.4568386  0.45677233 0.45682865
 0.45684984 0.45681387 0.45671603 0.4565774  0.45656258 0.4566878
 0.4567971  0.45677227 0.4566407  0.45642743 0.45628592 0.456222
 0.45621014 0.45623216 0.45623624 0.45618397 0.45620334 0.45623234
 0.4562632  0.45629314 0.45625556 0.45620912 0.45617676 0.45624575
 0.45649523 0.45681763 0.45709068 0.45725837 0.45728362 0.4572781
 0.45732063 0.45744568 0.45757616 0.45766124 0.45761204 0.45747063
 0.45732203 0.4572501  0.457293   0.45739165 0.45757282 0.45778984
 0.4579983  0.45814246 0.45810467 0.45786053 0.45743492 0.45684752
 0.45610818 0.45543754 0.45483732 0.45414954 0.45349452 0.4529678
 0.45260108 0.45239127 0.45232356 0.45230827 0.45231164 0.45232025
 0.45226255 0.4521125  0.45186377 0.4514709  0.4510681  0.45066658
 0.4505199  0.4505666  0.45067298 0.45077446 0.4507998  0.45095587
 0.4511269  0.45104983 0.45075357 0.4504594  0.45018148 0.44986418
 0.4495517  0.4492508  0.44894686 0.44872013 0.448576   0.44862652
 0.44876897 0.44880286 0.44858843 0.44829682 0.44827098 0.44854805
 0.44897518 0.4492771  0.44943872 0.44994122 0.45111147 0.45126614]
