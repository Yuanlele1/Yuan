Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=14, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_336_FITS_ETTm1_ftM_sl90_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34135
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=14, out_features=66, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  827904.0
params:  990.0
Trainable parameters:  990
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8049779
	speed: 0.1000s/iter; left time: 2649.9697s
	iters: 200, epoch: 1 | loss: 0.5571550
	speed: 0.0812s/iter; left time: 2144.5638s
Epoch: 1 cost time: 22.769294500350952
Epoch: 1, Steps: 266 | Train Loss: 0.7371105 Vali Loss: 0.9612034 Test Loss: 0.6915959
Validation loss decreased (inf --> 0.961203).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5017105
	speed: 0.3490s/iter; left time: 9155.1806s
	iters: 200, epoch: 2 | loss: 0.5599412
	speed: 0.0927s/iter; left time: 2422.4965s
Epoch: 2 cost time: 23.708695650100708
Epoch: 2, Steps: 266 | Train Loss: 0.4907002 Vali Loss: 0.7784759 Test Loss: 0.5068737
Validation loss decreased (0.961203 --> 0.778476).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4602647
	speed: 0.3782s/iter; left time: 9821.2991s
	iters: 200, epoch: 3 | loss: 0.4149303
	speed: 0.0839s/iter; left time: 2170.6054s
Epoch: 3 cost time: 23.870635509490967
Epoch: 3, Steps: 266 | Train Loss: 0.4421462 Vali Loss: 0.7282329 Test Loss: 0.4587505
Validation loss decreased (0.778476 --> 0.728233).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4167335
	speed: 0.3542s/iter; left time: 9103.2551s
	iters: 200, epoch: 4 | loss: 0.4548276
	speed: 0.0782s/iter; left time: 2002.2964s
Epoch: 4 cost time: 21.980567693710327
Epoch: 4, Steps: 266 | Train Loss: 0.4303246 Vali Loss: 0.7099758 Test Loss: 0.4423086
Validation loss decreased (0.728233 --> 0.709976).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4219895
	speed: 0.4128s/iter; left time: 10499.2537s
	iters: 200, epoch: 5 | loss: 0.4045363
	speed: 0.0889s/iter; left time: 2252.9004s
Epoch: 5 cost time: 24.380412578582764
Epoch: 5, Steps: 266 | Train Loss: 0.4266571 Vali Loss: 0.7017563 Test Loss: 0.4360832
Validation loss decreased (0.709976 --> 0.701756).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4182244
	speed: 0.3378s/iter; left time: 8502.2546s
	iters: 200, epoch: 6 | loss: 0.4089193
	speed: 0.0906s/iter; left time: 2270.5100s
Epoch: 6 cost time: 23.11110496520996
Epoch: 6, Steps: 266 | Train Loss: 0.4255783 Vali Loss: 0.6985919 Test Loss: 0.4335136
Validation loss decreased (0.701756 --> 0.698592).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4149335
	speed: 0.3659s/iter; left time: 9113.8915s
	iters: 200, epoch: 7 | loss: 0.4266497
	speed: 0.0844s/iter; left time: 2093.5829s
Epoch: 7 cost time: 23.309916496276855
Epoch: 7, Steps: 266 | Train Loss: 0.4250454 Vali Loss: 0.6979525 Test Loss: 0.4327325
Validation loss decreased (0.698592 --> 0.697953).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3928390
	speed: 0.3891s/iter; left time: 9587.3726s
	iters: 200, epoch: 8 | loss: 0.4946749
	speed: 0.0843s/iter; left time: 2067.9500s
Epoch: 8 cost time: 22.768445014953613
Epoch: 8, Steps: 266 | Train Loss: 0.4248333 Vali Loss: 0.6974846 Test Loss: 0.4323519
Validation loss decreased (0.697953 --> 0.697485).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4108659
	speed: 0.3739s/iter; left time: 9113.1919s
	iters: 200, epoch: 9 | loss: 0.4521239
	speed: 0.0869s/iter; left time: 2108.7212s
Epoch: 9 cost time: 23.451261520385742
Epoch: 9, Steps: 266 | Train Loss: 0.4247110 Vali Loss: 0.6968098 Test Loss: 0.4320236
Validation loss decreased (0.697485 --> 0.696810).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4179073
	speed: 0.4088s/iter; left time: 9856.0132s
	iters: 200, epoch: 10 | loss: 0.4219144
	speed: 0.0843s/iter; left time: 2024.8715s
Epoch: 10 cost time: 24.192046880722046
Epoch: 10, Steps: 266 | Train Loss: 0.4246160 Vali Loss: 0.6963500 Test Loss: 0.4317277
Validation loss decreased (0.696810 --> 0.696350).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4689876
	speed: 0.2335s/iter; left time: 5567.8702s
	iters: 200, epoch: 11 | loss: 0.4587051
	speed: 0.0104s/iter; left time: 247.2607s
Epoch: 11 cost time: 3.374990224838257
Epoch: 11, Steps: 266 | Train Loss: 0.4246013 Vali Loss: 0.6966262 Test Loss: 0.4319484
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4384283
	speed: 0.0454s/iter; left time: 1071.2009s
	iters: 200, epoch: 12 | loss: 0.4164053
	speed: 0.0081s/iter; left time: 190.2849s
Epoch: 12 cost time: 2.7609658241271973
Epoch: 12, Steps: 266 | Train Loss: 0.4246003 Vali Loss: 0.6963248 Test Loss: 0.4320711
Validation loss decreased (0.696350 --> 0.696325).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4388734
	speed: 0.0452s/iter; left time: 1053.5102s
	iters: 200, epoch: 13 | loss: 0.4800518
	speed: 0.0094s/iter; left time: 218.0765s
Epoch: 13 cost time: 2.990544319152832
Epoch: 13, Steps: 266 | Train Loss: 0.4244359 Vali Loss: 0.6957467 Test Loss: 0.4319465
Validation loss decreased (0.696325 --> 0.695747).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4342543
	speed: 0.2430s/iter; left time: 5600.1698s
	iters: 200, epoch: 14 | loss: 0.4706977
	speed: 0.1014s/iter; left time: 2326.4608s
Epoch: 14 cost time: 26.225716590881348
Epoch: 14, Steps: 266 | Train Loss: 0.4244512 Vali Loss: 0.6964659 Test Loss: 0.4321245
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4127806
	speed: 0.0880s/iter; left time: 2004.7849s
	iters: 200, epoch: 15 | loss: 0.3698180
	speed: 0.0090s/iter; left time: 203.5428s
Epoch: 15 cost time: 2.8122966289520264
Epoch: 15, Steps: 266 | Train Loss: 0.4244660 Vali Loss: 0.6960075 Test Loss: 0.4319787
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4334327
	speed: 0.0425s/iter; left time: 957.0883s
	iters: 200, epoch: 16 | loss: 0.4059314
	speed: 0.0085s/iter; left time: 189.7620s
Epoch: 16 cost time: 2.6767051219940186
Epoch: 16, Steps: 266 | Train Loss: 0.4244836 Vali Loss: 0.6967041 Test Loss: 0.4321575
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3828104
	speed: 0.0423s/iter; left time: 941.3254s
	iters: 200, epoch: 17 | loss: 0.4071612
	speed: 0.0090s/iter; left time: 198.9738s
Epoch: 17 cost time: 2.8862006664276123
Epoch: 17, Steps: 266 | Train Loss: 0.4244639 Vali Loss: 0.6962094 Test Loss: 0.4320543
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4416221
	speed: 0.0417s/iter; left time: 917.5992s
	iters: 200, epoch: 18 | loss: 0.4002653
	speed: 0.0080s/iter; left time: 174.7176s
Epoch: 18 cost time: 2.661736488342285
Epoch: 18, Steps: 266 | Train Loss: 0.4244946 Vali Loss: 0.6951531 Test Loss: 0.4319811
Validation loss decreased (0.695747 --> 0.695153).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4641656
	speed: 0.0418s/iter; left time: 906.7797s
	iters: 200, epoch: 19 | loss: 0.4194639
	speed: 0.0081s/iter; left time: 175.1257s
Epoch: 19 cost time: 2.663072347640991
Epoch: 19, Steps: 266 | Train Loss: 0.4243650 Vali Loss: 0.6965802 Test Loss: 0.4324373
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4378245
	speed: 0.0402s/iter; left time: 862.6605s
	iters: 200, epoch: 20 | loss: 0.4133548
	speed: 0.0080s/iter; left time: 170.3976s
Epoch: 20 cost time: 2.5662343502044678
Epoch: 20, Steps: 266 | Train Loss: 0.4243018 Vali Loss: 0.6961526 Test Loss: 0.4321155
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4859826
	speed: 0.0411s/iter; left time: 870.9925s
	iters: 200, epoch: 21 | loss: 0.4122263
	speed: 0.0080s/iter; left time: 168.6006s
Epoch: 21 cost time: 2.632617950439453
Epoch: 21, Steps: 266 | Train Loss: 0.4244392 Vali Loss: 0.6963432 Test Loss: 0.4323554
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4031281
	speed: 0.0415s/iter; left time: 868.0421s
	iters: 200, epoch: 22 | loss: 0.3854693
	speed: 0.0081s/iter; left time: 168.1881s
Epoch: 22 cost time: 2.6732046604156494
Epoch: 22, Steps: 266 | Train Loss: 0.4243707 Vali Loss: 0.6962639 Test Loss: 0.4322295
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4252965
	speed: 0.0412s/iter; left time: 849.7745s
	iters: 200, epoch: 23 | loss: 0.4529011
	speed: 0.0082s/iter; left time: 168.1514s
Epoch: 23 cost time: 2.617058753967285
Epoch: 23, Steps: 266 | Train Loss: 0.4244062 Vali Loss: 0.6965958 Test Loss: 0.4323859
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4438124
	speed: 0.0408s/iter; left time: 832.2916s
	iters: 200, epoch: 24 | loss: 0.3992290
	speed: 0.0084s/iter; left time: 170.3531s
Epoch: 24 cost time: 2.720874547958374
Epoch: 24, Steps: 266 | Train Loss: 0.4243152 Vali Loss: 0.6958635 Test Loss: 0.4320469
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4553018
	speed: 0.0417s/iter; left time: 839.7542s
	iters: 200, epoch: 25 | loss: 0.4954978
	speed: 0.0086s/iter; left time: 172.3554s
Epoch: 25 cost time: 2.7362334728240967
Epoch: 25, Steps: 266 | Train Loss: 0.4242292 Vali Loss: 0.6960145 Test Loss: 0.4321926
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4296347
	speed: 0.0417s/iter; left time: 828.0773s
	iters: 200, epoch: 26 | loss: 0.4127447
	speed: 0.0078s/iter; left time: 154.3039s
Epoch: 26 cost time: 2.5674564838409424
Epoch: 26, Steps: 266 | Train Loss: 0.4243842 Vali Loss: 0.6955850 Test Loss: 0.4321300
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4632949
	speed: 0.0409s/iter; left time: 800.5978s
	iters: 200, epoch: 27 | loss: 0.4121847
	speed: 0.0086s/iter; left time: 167.7121s
Epoch: 27 cost time: 2.6325979232788086
Epoch: 27, Steps: 266 | Train Loss: 0.4243163 Vali Loss: 0.6953791 Test Loss: 0.4322680
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4384788
	speed: 0.0429s/iter; left time: 828.8158s
	iters: 200, epoch: 28 | loss: 0.4512239
	speed: 0.0081s/iter; left time: 155.0968s
Epoch: 28 cost time: 2.7485907077789307
Epoch: 28, Steps: 266 | Train Loss: 0.4243829 Vali Loss: 0.6964891 Test Loss: 0.4321904
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4305353
	speed: 0.0417s/iter; left time: 794.5754s
	iters: 200, epoch: 29 | loss: 0.4420068
	speed: 0.0095s/iter; left time: 180.3566s
Epoch: 29 cost time: 2.779784679412842
Epoch: 29, Steps: 266 | Train Loss: 0.4244065 Vali Loss: 0.6957039 Test Loss: 0.4321542
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4141278
	speed: 0.0420s/iter; left time: 788.6556s
	iters: 200, epoch: 30 | loss: 0.4187434
	speed: 0.0090s/iter; left time: 168.1691s
Epoch: 30 cost time: 2.732752561569214
Epoch: 30, Steps: 266 | Train Loss: 0.4244183 Vali Loss: 0.6958832 Test Loss: 0.4322227
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3799184
	speed: 0.0418s/iter; left time: 774.8402s
	iters: 200, epoch: 31 | loss: 0.4563415
	speed: 0.0087s/iter; left time: 159.6378s
Epoch: 31 cost time: 2.7861080169677734
Epoch: 31, Steps: 266 | Train Loss: 0.4243558 Vali Loss: 0.6966146 Test Loss: 0.4322448
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4063710
	speed: 0.0409s/iter; left time: 746.3152s
	iters: 200, epoch: 32 | loss: 0.4708594
	speed: 0.0083s/iter; left time: 151.1782s
Epoch: 32 cost time: 2.810502767562866
Epoch: 32, Steps: 266 | Train Loss: 0.4243649 Vali Loss: 0.6963478 Test Loss: 0.4322752
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4631712
	speed: 0.0437s/iter; left time: 785.4659s
	iters: 200, epoch: 33 | loss: 0.4411726
	speed: 0.0080s/iter; left time: 143.9703s
Epoch: 33 cost time: 2.6794803142547607
Epoch: 33, Steps: 266 | Train Loss: 0.4242505 Vali Loss: 0.6964493 Test Loss: 0.4322592
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4536731
	speed: 0.0411s/iter; left time: 727.7076s
	iters: 200, epoch: 34 | loss: 0.4591185
	speed: 0.0078s/iter; left time: 137.5467s
Epoch: 34 cost time: 2.6449687480926514
Epoch: 34, Steps: 266 | Train Loss: 0.4243538 Vali Loss: 0.6964601 Test Loss: 0.4323142
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4660171
	speed: 0.0424s/iter; left time: 739.8703s
	iters: 200, epoch: 35 | loss: 0.4279822
	speed: 0.0082s/iter; left time: 142.4502s
Epoch: 35 cost time: 2.666181802749634
Epoch: 35, Steps: 266 | Train Loss: 0.4242329 Vali Loss: 0.6962028 Test Loss: 0.4322394
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4236411
	speed: 0.0411s/iter; left time: 706.5407s
	iters: 200, epoch: 36 | loss: 0.4312668
	speed: 0.0078s/iter; left time: 133.2341s
Epoch: 36 cost time: 2.5764520168304443
Epoch: 36, Steps: 266 | Train Loss: 0.4242356 Vali Loss: 0.6957759 Test Loss: 0.4323587
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4403172
	speed: 0.0423s/iter; left time: 715.6464s
	iters: 200, epoch: 37 | loss: 0.4422668
	speed: 0.0079s/iter; left time: 132.8104s
Epoch: 37 cost time: 2.6782784461975098
Epoch: 37, Steps: 266 | Train Loss: 0.4243141 Vali Loss: 0.6963588 Test Loss: 0.4323053
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4351493
	speed: 0.0410s/iter; left time: 683.4132s
	iters: 200, epoch: 38 | loss: 0.4176862
	speed: 0.0080s/iter; left time: 131.7225s
Epoch: 38 cost time: 2.5765535831451416
Epoch: 38, Steps: 266 | Train Loss: 0.4243841 Vali Loss: 0.6963205 Test Loss: 0.4323682
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_336_FITS_ETTm1_ftM_sl90_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.43208566308021545, mae:0.419333279132843, rse:0.6255085468292236, corr:[0.5463728  0.5464153  0.5422322  0.53788275 0.53570175 0.5349894
 0.5335921  0.53093165 0.5280389  0.5261933  0.52545774 0.52487224
 0.5236234  0.5215726  0.5189364  0.5160261  0.5132345  0.5106497
 0.5080225  0.50512624 0.5019942  0.49869293 0.4953557  0.49204192
 0.48876765 0.48541895 0.48220786 0.479459   0.4774776  0.47606224
 0.47522426 0.4752647  0.47562099 0.4759049  0.4760898  0.476377
 0.47653925 0.4766485  0.47665492 0.47648323 0.47630695 0.47606573
 0.47590125 0.47582263 0.47557425 0.47516558 0.47479805 0.47464058
 0.4748086  0.4752999  0.47597307 0.47649705 0.47684106 0.4769898
 0.47722015 0.4775891  0.47813478 0.47865206 0.4787586  0.4785055
 0.47805822 0.4774612  0.47706518 0.47685626 0.47693205 0.4769804
 0.47666907 0.47616252 0.47597766 0.4764023  0.4771892  0.47811556
 0.4789707  0.47950554 0.4798812  0.48019233 0.48081958 0.4817969
 0.48281643 0.48364934 0.48422024 0.48447394 0.48466423 0.48481226
 0.4850542  0.48543704 0.4859795  0.48659575 0.48723447 0.4879077
 0.48861262 0.4893348  0.49006292 0.49068293 0.49093452 0.49052447
 0.4894968  0.48839572 0.48757818 0.487082   0.48685694 0.4865507
 0.48597226 0.48510715 0.4844199  0.4840375  0.48369467 0.4831711
 0.48239848 0.48147854 0.48058218 0.47982302 0.47917217 0.4785199
 0.47767878 0.4766862  0.47554755 0.47437096 0.47325715 0.47222674
 0.4712487  0.4701073  0.4687682  0.4675168  0.46646935 0.46564215
 0.46508247 0.46477336 0.46465227 0.4646239  0.46462768 0.4646581
 0.46462584 0.46460193 0.46448    0.4642336  0.46393085 0.46369928
 0.46358645 0.46357825 0.46358606 0.46358615 0.46354023 0.46351823
 0.46355337 0.46373594 0.464053   0.46432617 0.4646107  0.4647653
 0.46480167 0.46498656 0.46530876 0.46565822 0.46582156 0.46581614
 0.46564186 0.46541858 0.4652608  0.4652732  0.46554226 0.46589237
 0.4661678  0.46631867 0.46646008 0.46672446 0.46724203 0.4680145
 0.46889627 0.46969116 0.4703064  0.4708849  0.47142726 0.47199285
 0.47253025 0.47304544 0.47352687 0.47394535 0.47422022 0.47446662
 0.47474998 0.47515926 0.47567293 0.47632414 0.47708008 0.47778878
 0.47833556 0.47870928 0.47901043 0.47936374 0.479791   0.48020458
 0.4805106  0.48101234 0.48190683 0.48326087 0.48485819 0.48630577
 0.48706752 0.486869   0.4861591  0.48552588 0.48504055 0.48452842
 0.4837683  0.4826492  0.4812293  0.47980857 0.47851864 0.47746393
 0.4764799  0.4753538  0.473968   0.47225982 0.47030333 0.4684806
 0.46683258 0.46541712 0.464174   0.4631174  0.46213922 0.46131286
 0.4607896  0.46074367 0.46083874 0.4608002  0.46071476 0.4604587
 0.46013388 0.4599897  0.459984   0.45987663 0.45961314 0.45917785
 0.45876226 0.4585457  0.45849922 0.4586786  0.45898688 0.45903385
 0.45891985 0.45871523 0.4586724  0.45899168 0.45944735 0.4599412
 0.46030834 0.46054175 0.4606924  0.46097556 0.46125078 0.46150598
 0.46151733 0.4613808  0.46107963 0.46098545 0.46105143 0.4612217
 0.46154857 0.461968   0.4622167  0.46239933 0.4626806  0.46315706
 0.4639447  0.46472353 0.46536613 0.46596447 0.46664894 0.46734464
 0.4681375  0.4689402  0.46973094 0.47036648 0.47082508 0.47117513
 0.4715787  0.4721482  0.47280875 0.4735378  0.47421587 0.47474524
 0.47509676 0.47528765 0.4753588  0.47527653 0.47488922 0.47390848
 0.4720913  0.46973822 0.46754667 0.46601522 0.46524417 0.46501154
 0.46490985 0.46448383 0.46399212 0.46364704 0.46334803 0.46287405
 0.46203676 0.46081966 0.45947465 0.45827886 0.45743895 0.4568253
 0.45631954 0.45559603 0.45460823 0.45352253 0.4525735  0.4518505
 0.45124725 0.45059234 0.44980493 0.44894803 0.44799197 0.44722113
 0.4467648  0.44652689 0.44631612 0.4459982  0.4455772  0.44526666
 0.44512236 0.44517735 0.4451555  0.44487965 0.44439968 0.44401297
 0.44422308 0.4448943  0.44552588 0.44571906 0.44589683 0.44655395]
