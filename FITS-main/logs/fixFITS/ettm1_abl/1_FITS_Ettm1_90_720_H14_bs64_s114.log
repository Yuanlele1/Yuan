Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=24, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=24, out_features=216, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4644864.0
params:  5400.0
Trainable parameters:  5400
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.9620467
	speed: 0.0532s/iter; left time: 1393.3396s
	iters: 200, epoch: 1 | loss: 0.7740542
	speed: 0.0457s/iter; left time: 1194.0176s
Epoch: 1 cost time: 12.316263437271118
Epoch: 1, Steps: 263 | Train Loss: 0.9422394 Vali Loss: 1.3061982 Test Loss: 0.7599648
Validation loss decreased (inf --> 1.306198).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5634667
	speed: 0.1462s/iter; left time: 3791.9299s
	iters: 200, epoch: 2 | loss: 0.5406925
	speed: 0.0209s/iter; left time: 539.9891s
Epoch: 2 cost time: 6.46695876121521
Epoch: 2, Steps: 263 | Train Loss: 0.5641616 Vali Loss: 1.0921272 Test Loss: 0.5539933
Validation loss decreased (1.306198 --> 1.092127).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4979856
	speed: 0.1095s/iter; left time: 2810.6827s
	iters: 200, epoch: 3 | loss: 0.5498356
	speed: 0.0274s/iter; left time: 699.6858s
Epoch: 3 cost time: 6.938854217529297
Epoch: 3, Steps: 263 | Train Loss: 0.5116314 Vali Loss: 1.0454654 Test Loss: 0.5141108
Validation loss decreased (1.092127 --> 1.045465).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5205096
	speed: 0.1239s/iter; left time: 3147.9270s
	iters: 200, epoch: 4 | loss: 0.4780558
	speed: 0.0432s/iter; left time: 1094.5544s
Epoch: 4 cost time: 10.153300046920776
Epoch: 4, Steps: 263 | Train Loss: 0.4997326 Vali Loss: 1.0272187 Test Loss: 0.5008590
Validation loss decreased (1.045465 --> 1.027219).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5006710
	speed: 0.2216s/iter; left time: 5571.8322s
	iters: 200, epoch: 5 | loss: 0.5180391
	speed: 0.0499s/iter; left time: 1249.7745s
Epoch: 5 cost time: 12.395901203155518
Epoch: 5, Steps: 263 | Train Loss: 0.4957561 Vali Loss: 1.0189066 Test Loss: 0.4957274
Validation loss decreased (1.027219 --> 1.018907).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4696772
	speed: 0.1707s/iter; left time: 4248.4677s
	iters: 200, epoch: 6 | loss: 0.4853095
	speed: 0.0295s/iter; left time: 730.9288s
Epoch: 6 cost time: 10.379132747650146
Epoch: 6, Steps: 263 | Train Loss: 0.4944819 Vali Loss: 1.0160969 Test Loss: 0.4941621
Validation loss decreased (1.018907 --> 1.016097).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5160469
	speed: 0.1561s/iter; left time: 3842.7124s
	iters: 200, epoch: 7 | loss: 0.5150194
	speed: 0.0355s/iter; left time: 871.3164s
Epoch: 7 cost time: 8.624096155166626
Epoch: 7, Steps: 263 | Train Loss: 0.4938552 Vali Loss: 1.0134774 Test Loss: 0.4934282
Validation loss decreased (1.016097 --> 1.013477).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4896744
	speed: 0.1304s/iter; left time: 3176.6319s
	iters: 200, epoch: 8 | loss: 0.5054446
	speed: 0.0482s/iter; left time: 1169.5158s
Epoch: 8 cost time: 10.426120519638062
Epoch: 8, Steps: 263 | Train Loss: 0.4936734 Vali Loss: 1.0125656 Test Loss: 0.4931621
Validation loss decreased (1.013477 --> 1.012566).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4953364
	speed: 0.1200s/iter; left time: 2891.8044s
	iters: 200, epoch: 9 | loss: 0.5071521
	speed: 0.0343s/iter; left time: 822.7201s
Epoch: 9 cost time: 8.748772621154785
Epoch: 9, Steps: 263 | Train Loss: 0.4936146 Vali Loss: 1.0124748 Test Loss: 0.4930461
Validation loss decreased (1.012566 --> 1.012475).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4952004
	speed: 0.1416s/iter; left time: 3375.4786s
	iters: 200, epoch: 10 | loss: 0.4888429
	speed: 0.0246s/iter; left time: 584.4867s
Epoch: 10 cost time: 7.988738298416138
Epoch: 10, Steps: 263 | Train Loss: 0.4932725 Vali Loss: 1.0115283 Test Loss: 0.4930213
Validation loss decreased (1.012475 --> 1.011528).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4811149
	speed: 0.1760s/iter; left time: 4149.5331s
	iters: 200, epoch: 11 | loss: 0.5383947
	speed: 0.0335s/iter; left time: 786.7881s
Epoch: 11 cost time: 10.211403369903564
Epoch: 11, Steps: 263 | Train Loss: 0.4934535 Vali Loss: 1.0119562 Test Loss: 0.4933385
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4555575
	speed: 0.1920s/iter; left time: 4474.1487s
	iters: 200, epoch: 12 | loss: 0.4880427
	speed: 0.0528s/iter; left time: 1225.2905s
Epoch: 12 cost time: 11.538970232009888
Epoch: 12, Steps: 263 | Train Loss: 0.4933601 Vali Loss: 1.0125768 Test Loss: 0.4932461
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5323908
	speed: 0.1156s/iter; left time: 2664.2241s
	iters: 200, epoch: 13 | loss: 0.4421960
	speed: 0.0213s/iter; left time: 488.4199s
Epoch: 13 cost time: 7.366733074188232
Epoch: 13, Steps: 263 | Train Loss: 0.4933566 Vali Loss: 1.0127877 Test Loss: 0.4932653
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4906394
	speed: 0.1169s/iter; left time: 2663.8559s
	iters: 200, epoch: 14 | loss: 0.4735419
	speed: 0.0266s/iter; left time: 603.3349s
Epoch: 14 cost time: 7.731723785400391
Epoch: 14, Steps: 263 | Train Loss: 0.4932775 Vali Loss: 1.0128371 Test Loss: 0.4933786
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4839572
	speed: 0.1223s/iter; left time: 2753.7525s
	iters: 200, epoch: 15 | loss: 0.5206745
	speed: 0.0385s/iter; left time: 864.0244s
Epoch: 15 cost time: 9.54425835609436
Epoch: 15, Steps: 263 | Train Loss: 0.4932589 Vali Loss: 1.0125977 Test Loss: 0.4929498
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4530413
	speed: 0.1655s/iter; left time: 3682.6053s
	iters: 200, epoch: 16 | loss: 0.5507739
	speed: 0.0427s/iter; left time: 946.2527s
Epoch: 16 cost time: 11.329081296920776
Epoch: 16, Steps: 263 | Train Loss: 0.4930868 Vali Loss: 1.0126374 Test Loss: 0.4933664
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4953775
	speed: 0.1599s/iter; left time: 3516.3963s
	iters: 200, epoch: 17 | loss: 0.4796172
	speed: 0.0314s/iter; left time: 687.6225s
Epoch: 17 cost time: 9.869970798492432
Epoch: 17, Steps: 263 | Train Loss: 0.4933029 Vali Loss: 1.0129917 Test Loss: 0.4934726
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5085626
	speed: 0.1636s/iter; left time: 3555.5356s
	iters: 200, epoch: 18 | loss: 0.4937336
	speed: 0.0238s/iter; left time: 514.4666s
Epoch: 18 cost time: 7.8689751625061035
Epoch: 18, Steps: 263 | Train Loss: 0.4931747 Vali Loss: 1.0130128 Test Loss: 0.4932017
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4772068
	speed: 0.1248s/iter; left time: 2679.4249s
	iters: 200, epoch: 19 | loss: 0.5065358
	speed: 0.0244s/iter; left time: 521.8968s
Epoch: 19 cost time: 7.1248462200164795
Epoch: 19, Steps: 263 | Train Loss: 0.4932467 Vali Loss: 1.0127084 Test Loss: 0.4936159
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5129730
	speed: 0.1176s/iter; left time: 2492.6002s
	iters: 200, epoch: 20 | loss: 0.4821006
	speed: 0.0329s/iter; left time: 693.8774s
Epoch: 20 cost time: 7.785327911376953
Epoch: 20, Steps: 263 | Train Loss: 0.4932753 Vali Loss: 1.0126270 Test Loss: 0.4933624
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4751863
	speed: 0.1291s/iter; left time: 2703.6211s
	iters: 200, epoch: 21 | loss: 0.4984942
	speed: 0.0284s/iter; left time: 591.2899s
Epoch: 21 cost time: 7.814826726913452
Epoch: 21, Steps: 263 | Train Loss: 0.4931172 Vali Loss: 1.0120436 Test Loss: 0.4935099
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5093018
	speed: 0.1409s/iter; left time: 2913.3609s
	iters: 200, epoch: 22 | loss: 0.4577706
	speed: 0.0383s/iter; left time: 788.7546s
Epoch: 22 cost time: 9.490467548370361
Epoch: 22, Steps: 263 | Train Loss: 0.4932211 Vali Loss: 1.0124013 Test Loss: 0.4934454
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4848180
	speed: 0.1768s/iter; left time: 3609.0792s
	iters: 200, epoch: 23 | loss: 0.5094388
	speed: 0.0309s/iter; left time: 626.9830s
Epoch: 23 cost time: 8.45093059539795
Epoch: 23, Steps: 263 | Train Loss: 0.4931717 Vali Loss: 1.0131937 Test Loss: 0.4934528
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5342463
	speed: 0.1262s/iter; left time: 2543.6381s
	iters: 200, epoch: 24 | loss: 0.4535665
	speed: 0.0227s/iter; left time: 454.6022s
Epoch: 24 cost time: 7.126336336135864
Epoch: 24, Steps: 263 | Train Loss: 0.4929801 Vali Loss: 1.0133215 Test Loss: 0.4935423
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4508682
	speed: 0.1165s/iter; left time: 2316.5679s
	iters: 200, epoch: 25 | loss: 0.5056741
	speed: 0.0238s/iter; left time: 471.6124s
Epoch: 25 cost time: 6.902991771697998
Epoch: 25, Steps: 263 | Train Loss: 0.4930637 Vali Loss: 1.0127150 Test Loss: 0.4935569
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4859463
	speed: 0.1521s/iter; left time: 2984.2288s
	iters: 200, epoch: 26 | loss: 0.5014232
	speed: 0.0317s/iter; left time: 619.4329s
Epoch: 26 cost time: 8.816089630126953
Epoch: 26, Steps: 263 | Train Loss: 0.4931546 Vali Loss: 1.0131567 Test Loss: 0.4936691
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4556833
	speed: 0.1744s/iter; left time: 3377.1591s
	iters: 200, epoch: 27 | loss: 0.4585960
	speed: 0.0438s/iter; left time: 843.0838s
Epoch: 27 cost time: 12.124535322189331
Epoch: 27, Steps: 263 | Train Loss: 0.4931337 Vali Loss: 1.0138853 Test Loss: 0.4936740
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4976941
	speed: 0.1653s/iter; left time: 3157.6143s
	iters: 200, epoch: 28 | loss: 0.4649161
	speed: 0.0358s/iter; left time: 680.3212s
Epoch: 28 cost time: 9.508679389953613
Epoch: 28, Steps: 263 | Train Loss: 0.4929513 Vali Loss: 1.0122545 Test Loss: 0.4937238
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4942432
	speed: 0.1413s/iter; left time: 2662.1484s
	iters: 200, epoch: 29 | loss: 0.5096789
	speed: 0.0235s/iter; left time: 440.7482s
Epoch: 29 cost time: 7.691009759902954
Epoch: 29, Steps: 263 | Train Loss: 0.4929196 Vali Loss: 1.0125494 Test Loss: 0.4936966
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4540211
	speed: 0.1451s/iter; left time: 2695.6020s
	iters: 200, epoch: 30 | loss: 0.4999990
	speed: 0.0334s/iter; left time: 616.5699s
Epoch: 30 cost time: 9.05345630645752
Epoch: 30, Steps: 263 | Train Loss: 0.4929649 Vali Loss: 1.0125586 Test Loss: 0.4936667
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.49120160937309265, mae:0.4517691731452942, rse:0.6668077707290649, corr:[0.53568536 0.5309495  0.5266286  0.5250997  0.5188331  0.5145616
 0.5147028  0.51224285 0.50812954 0.5077715  0.5085926  0.50684166
 0.5049271  0.50429636 0.50167114 0.49687853 0.4937134  0.49208048
 0.48875856 0.48461252 0.48226538 0.48057738 0.47701272 0.47285306
 0.47042876 0.4679657  0.46400505 0.4606721  0.45983306 0.45902687
 0.457193   0.45691139 0.4580682  0.45856014 0.4580263  0.45833933
 0.45923686 0.4592837  0.45842257 0.45818275 0.45878103 0.45843324
 0.45753765 0.4576163  0.45800677 0.45746452 0.45671612 0.45707485
 0.4578986  0.45782837 0.45740592 0.4577734  0.45873344 0.45900583
 0.45916516 0.45990863 0.46062988 0.46042994 0.45988452 0.46011788
 0.46027246 0.45939207 0.45889616 0.45917216 0.4592578  0.45861185
 0.45828357 0.45871553 0.4590745  0.45916197 0.45972553 0.4608505
 0.4613814  0.46114254 0.46185172 0.46339044 0.46442378 0.46459055
 0.4649471  0.46600002 0.46692047 0.46714038 0.4673318  0.46755588
 0.46788523 0.46843317 0.4692722  0.47001848 0.4704496  0.4711033
 0.47229248 0.4733421  0.47368976 0.47380587 0.4742233  0.47421023
 0.47310355 0.4718961  0.4712698  0.47049937 0.46947894 0.46946505
 0.47015193 0.4699953  0.46915972 0.4687487  0.46856445 0.46789286
 0.4670692  0.46655852 0.46579185 0.4643599  0.46300882 0.4623418
 0.46173054 0.46063823 0.45935237 0.45845243 0.4576281  0.45630747
 0.4549282  0.4539314  0.45302585 0.45185056 0.45066845 0.45020518
 0.45008448 0.4494572  0.4486944  0.4485614  0.44874585 0.44871676
 0.4486404  0.448931   0.44885656 0.4481821  0.4477463  0.44793448
 0.44790566 0.44737968 0.44718593 0.44753513 0.44753966 0.44721428
 0.44735706 0.4479642  0.44804683 0.4474819  0.44758204 0.44817466
 0.44821963 0.447943   0.44814798 0.44881964 0.44894347 0.4485328
 0.4483652  0.44867435 0.44873038 0.4484305  0.44849414 0.4488653
 0.44904363 0.44910103 0.44948795 0.45004526 0.45046365 0.45094
 0.4518282  0.45282105 0.45342132 0.45400217 0.45483825 0.45578533
 0.45642218 0.45687845 0.4572846  0.45755136 0.4577075  0.45809257
 0.45852298 0.4587803  0.4590939  0.45985687 0.46077618 0.46141863
 0.46204942 0.46285042 0.4633032  0.46309334 0.4627761  0.4628172
 0.46309015 0.46350008 0.4644179  0.46586996 0.46711507 0.46756113
 0.46743712 0.46738836 0.46753263 0.46706462 0.46597278 0.46502134
 0.46433598 0.46336356 0.46182    0.46025229 0.45888147 0.45758906
 0.45624408 0.4549174  0.4534816  0.45175254 0.4500689  0.44878757
 0.4471154  0.44502205 0.44334623 0.4426308  0.44184193 0.4405421
 0.43962196 0.43974242 0.43982512 0.43945488 0.43941367 0.43934387
 0.43890342 0.43864387 0.43894443 0.43909204 0.4385274  0.43756884
 0.43722552 0.43733945 0.43695915 0.43650845 0.4367047  0.43686366
 0.43676475 0.4367019  0.43725953 0.43805686 0.43809047 0.437947
 0.43832546 0.4390812  0.43950018 0.43972385 0.43973926 0.43982512
 0.4398893  0.44014823 0.4401352  0.4400646  0.43992138 0.4400655
 0.44057322 0.44099385 0.4410372  0.44144127 0.44220152 0.44259486
 0.44298923 0.4436745  0.44462898 0.4455835  0.44662657 0.4476764
 0.44870484 0.44925404 0.44959408 0.45001036 0.4504819  0.4507838
 0.45121923 0.45213947 0.45301935 0.45353365 0.45403507 0.45495293
 0.4559908  0.45652252 0.45670885 0.45686617 0.45642272 0.45481265
 0.4525139  0.45040464 0.4485783  0.44697475 0.44613257 0.44621235
 0.44626665 0.4459796  0.44594887 0.44593853 0.44532084 0.4442153
 0.44324657 0.4423524  0.4410097  0.4393286  0.43804678 0.43724293
 0.43662256 0.43579677 0.43509462 0.43454325 0.43380097 0.43274942
 0.43166372 0.43065733 0.4296295  0.42876995 0.42818207 0.42793846
 0.42751643 0.4269559  0.42668214 0.42649126 0.4259787  0.42559966
 0.4256388  0.425767   0.425322   0.42479748 0.42478004 0.4247722
 0.4244265  0.4240198  0.4241088  0.42405456 0.42341572 0.42279246
 0.4230709  0.4236084  0.42392695 0.42381018 0.42386004 0.42399555
 0.42415744 0.42463893 0.42510185 0.42516944 0.42506164 0.42534167
 0.42577708 0.42564052 0.42524192 0.4252978  0.4257024  0.42576587
 0.42546806 0.4254246  0.4257563  0.42600197 0.42627937 0.42702305
 0.4278942  0.42828712 0.4285554  0.4293517  0.4306968  0.43174136
 0.4323841  0.4332431  0.43415526 0.4346351  0.43517092 0.43616325
 0.43742192 0.43833366 0.43930784 0.44093147 0.44288364 0.44439256
 0.44529295 0.44606575 0.4468349  0.44724372 0.4466762  0.4457768
 0.44530264 0.44521114 0.44527155 0.44563088 0.44630778 0.44710422
 0.4479612  0.44883046 0.44928744 0.4491033  0.4487488  0.4485001
 0.4479674  0.44682983 0.44543272 0.4442722  0.44314656 0.44189158
 0.44083935 0.4401966  0.43954617 0.43843693 0.43715343 0.43623003
 0.43561178 0.43465763 0.43373832 0.4332195  0.43254432 0.43183562
 0.43167812 0.43156862 0.4311608  0.43061188 0.43042266 0.4305209
 0.43018046 0.42973518 0.42970103 0.42980656 0.42945507 0.42896688
 0.42892367 0.42912304 0.428734   0.4283025  0.42808563 0.42778766
 0.4273275  0.42724967 0.42791605 0.42864957 0.42875513 0.42872855
 0.42899323 0.42930168 0.4293537  0.42962864 0.43032467 0.43090084
 0.43094185 0.43094411 0.43088993 0.43076742 0.4307421  0.4311276
 0.4313808  0.43119138 0.4309039  0.43127337 0.4319044  0.4323228
 0.43267176 0.4331939  0.43372062 0.4341514  0.43478352 0.4357947
 0.43677843 0.4375168  0.43805742 0.4385866  0.43894878 0.4392414
 0.43995917 0.44113287 0.442348   0.443272   0.44419837 0.4453232
 0.44628543 0.44675234 0.44689086 0.44701132 0.44651973 0.4451001
 0.44366863 0.44281366 0.44190982 0.44069162 0.4401671  0.44042113
 0.4403211  0.43988886 0.44014516 0.44029552 0.43946856 0.4381518
 0.43713447 0.43606016 0.43428546 0.43230712 0.4309788  0.4299709
 0.42855197 0.4270019  0.42593893 0.42523712 0.42449957 0.4234539
 0.42238256 0.42116037 0.41986507 0.41896403 0.41849536 0.4179813
 0.4170481  0.41645056 0.416495   0.41655895 0.41622528 0.4161218
 0.41619265 0.41593015 0.4154623  0.41537714 0.4157868  0.41561958
 0.41469127 0.4144014  0.4148773  0.41501445 0.41431284 0.41391695
 0.41414672 0.41420493 0.41383633 0.41382343 0.41430268 0.41446263
 0.4143505  0.4145846  0.41524193 0.4154652  0.4152136  0.41515636
 0.41552576 0.4155012  0.41516826 0.4153727  0.41588905 0.41594207
 0.41555676 0.41556048 0.4160232  0.41626003 0.4161613  0.4163955
 0.41703478 0.4176327  0.41830146 0.41911724 0.41982844 0.42037955
 0.42131266 0.42270395 0.42365238 0.42376554 0.42386878 0.4244942
 0.42515236 0.4257108  0.42657557 0.42800197 0.42931017 0.42997867
 0.43053275 0.43137184 0.43194854 0.43149772 0.43014765 0.4285077
 0.42660686 0.42465508 0.4231647  0.42235863 0.42190206 0.42199823
 0.42266846 0.42343533 0.42384028 0.42382234 0.42338276 0.42252165
 0.421223   0.4196323  0.41792133 0.41620234 0.41459963 0.4133529
 0.4124516  0.41142523 0.41020542 0.4092342  0.40858    0.40760103
 0.4061269  0.40483072 0.40423045 0.4037443  0.40284878 0.40220252
 0.4019879  0.40171996 0.4011691  0.40097708 0.40102667 0.40092734
 0.400661   0.40086436 0.40122962 0.40094784 0.40055406 0.40059817
 0.40090308 0.4009548  0.40091968 0.40119025 0.4011392  0.40047526
 0.40003946 0.40043205 0.40087214 0.40051174 0.40047267 0.40137875
 0.40183556 0.40122074 0.40106356 0.40178928 0.40219745 0.40159002
 0.4012556  0.4018059  0.40198067 0.4011033  0.40064207 0.40117744
 0.40155607 0.40135768 0.40147227 0.40214068 0.40233466 0.40216625
 0.4026299  0.40362394 0.40413633 0.40437979 0.40515807 0.40646824
 0.40737006 0.4080773  0.40904242 0.40984    0.41003826 0.410302
 0.41116482 0.41199148 0.41257292 0.4135988  0.41518703 0.4164035
 0.41680515 0.41704956 0.4174847  0.4171802  0.41557366 0.4135801
 0.41215464 0.4105743  0.4088112  0.40839276 0.40921405 0.4095968
 0.4096285  0.4106931  0.4119018  0.41164118 0.4107164  0.41048443
 0.40998912 0.40782383 0.40531856 0.40419108 0.40331656 0.40108675
 0.39892176 0.39841264 0.3980573  0.3961013  0.3938726  0.39314845
 0.39249167 0.3907162  0.38933086 0.38905773 0.38797894 0.38620806
 0.38585788 0.38577995 0.38402605 0.38248724 0.3833828  0.38408196
 0.3822413  0.3813456  0.3830958  0.3832944  0.38188615 0.38355753
 0.38564947 0.38413095 0.38501897 0.3895301  0.38984928 0.39348873]
