Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_192_FITS_ETTm1_ftM_sl90_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34279
val 11329
test 11329
Model(
  (freq_upsampler): Linear(in_features=18, out_features=56, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  903168.0
params:  1064.0
Trainable parameters:  1064
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5495983
	speed: 0.0176s/iter; left time: 467.7255s
	iters: 200, epoch: 1 | loss: 0.4614773
	speed: 0.0103s/iter; left time: 273.9229s
Epoch: 1 cost time: 3.5110299587249756
Epoch: 1, Steps: 267 | Train Loss: 0.5632599 Vali Loss: 0.6914480 Test Loss: 0.5423598
Validation loss decreased (inf --> 0.691448).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4230223
	speed: 0.0609s/iter; left time: 1602.8896s
	iters: 200, epoch: 2 | loss: 0.4244117
	speed: 0.0101s/iter; left time: 266.2322s
Epoch: 2 cost time: 3.292070150375366
Epoch: 2, Steps: 267 | Train Loss: 0.4091946 Vali Loss: 0.5852004 Test Loss: 0.4381500
Validation loss decreased (0.691448 --> 0.585200).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3776181
	speed: 0.0584s/iter; left time: 1522.1596s
	iters: 200, epoch: 3 | loss: 0.3898008
	speed: 0.0100s/iter; left time: 260.4831s
Epoch: 3 cost time: 3.2533352375030518
Epoch: 3, Steps: 267 | Train Loss: 0.3838124 Vali Loss: 0.5584209 Test Loss: 0.4115281
Validation loss decreased (0.585200 --> 0.558421).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3894073
	speed: 0.0612s/iter; left time: 1579.9512s
	iters: 200, epoch: 4 | loss: 0.4270992
	speed: 0.0102s/iter; left time: 261.3716s
Epoch: 4 cost time: 3.36307692527771
Epoch: 4, Steps: 267 | Train Loss: 0.3779041 Vali Loss: 0.5506366 Test Loss: 0.4038099
Validation loss decreased (0.558421 --> 0.550637).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3861308
	speed: 0.0594s/iter; left time: 1516.4737s
	iters: 200, epoch: 5 | loss: 0.3707704
	speed: 0.0101s/iter; left time: 255.7537s
Epoch: 5 cost time: 3.358018398284912
Epoch: 5, Steps: 267 | Train Loss: 0.3762204 Vali Loss: 0.5476305 Test Loss: 0.4007242
Validation loss decreased (0.550637 --> 0.547630).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4051044
	speed: 0.0593s/iter; left time: 1498.3533s
	iters: 200, epoch: 6 | loss: 0.4029551
	speed: 0.0100s/iter; left time: 252.0813s
Epoch: 6 cost time: 3.24139142036438
Epoch: 6, Steps: 267 | Train Loss: 0.3754109 Vali Loss: 0.5467920 Test Loss: 0.3998862
Validation loss decreased (0.547630 --> 0.546792).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3999757
	speed: 0.0597s/iter; left time: 1493.6393s
	iters: 200, epoch: 7 | loss: 0.3458008
	speed: 0.0102s/iter; left time: 254.5101s
Epoch: 7 cost time: 3.403597593307495
Epoch: 7, Steps: 267 | Train Loss: 0.3752353 Vali Loss: 0.5456499 Test Loss: 0.3994294
Validation loss decreased (0.546792 --> 0.545650).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3680680
	speed: 0.0609s/iter; left time: 1505.1702s
	iters: 200, epoch: 8 | loss: 0.3817366
	speed: 0.0102s/iter; left time: 251.3159s
Epoch: 8 cost time: 3.313138246536255
Epoch: 8, Steps: 267 | Train Loss: 0.3748696 Vali Loss: 0.5456283 Test Loss: 0.3992867
Validation loss decreased (0.545650 --> 0.545628).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3557929
	speed: 0.0808s/iter; left time: 1976.3261s
	iters: 200, epoch: 9 | loss: 0.3885361
	speed: 0.0417s/iter; left time: 1016.8589s
Epoch: 9 cost time: 9.57132601737976
Epoch: 9, Steps: 267 | Train Loss: 0.3749433 Vali Loss: 0.5456348 Test Loss: 0.3991865
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3780938
	speed: 0.0688s/iter; left time: 1665.5794s
	iters: 200, epoch: 10 | loss: 0.3975899
	speed: 0.0100s/iter; left time: 240.9355s
Epoch: 10 cost time: 3.2078018188476562
Epoch: 10, Steps: 267 | Train Loss: 0.3748402 Vali Loss: 0.5452099 Test Loss: 0.3994142
Validation loss decreased (0.545628 --> 0.545210).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4170216
	speed: 0.0595s/iter; left time: 1424.9117s
	iters: 200, epoch: 11 | loss: 0.3804775
	speed: 0.0102s/iter; left time: 243.6937s
Epoch: 11 cost time: 3.2944531440734863
Epoch: 11, Steps: 267 | Train Loss: 0.3747920 Vali Loss: 0.5442143 Test Loss: 0.3994964
Validation loss decreased (0.545210 --> 0.544214).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3583510
	speed: 0.0594s/iter; left time: 1404.6776s
	iters: 200, epoch: 12 | loss: 0.3449253
	speed: 0.0101s/iter; left time: 238.2951s
Epoch: 12 cost time: 3.2608895301818848
Epoch: 12, Steps: 267 | Train Loss: 0.3746657 Vali Loss: 0.5456479 Test Loss: 0.3993231
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3528471
	speed: 0.0591s/iter; left time: 1382.2684s
	iters: 200, epoch: 13 | loss: 0.4194954
	speed: 0.0100s/iter; left time: 232.6739s
Epoch: 13 cost time: 3.272237539291382
Epoch: 13, Steps: 267 | Train Loss: 0.3746330 Vali Loss: 0.5453955 Test Loss: 0.3994749
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3667484
	speed: 0.0586s/iter; left time: 1356.3255s
	iters: 200, epoch: 14 | loss: 0.3674830
	speed: 0.0100s/iter; left time: 231.1107s
Epoch: 14 cost time: 3.2192435264587402
Epoch: 14, Steps: 267 | Train Loss: 0.3746775 Vali Loss: 0.5449585 Test Loss: 0.3989109
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3833336
	speed: 0.0589s/iter; left time: 1345.9194s
	iters: 200, epoch: 15 | loss: 0.3877086
	speed: 0.0099s/iter; left time: 226.2474s
Epoch: 15 cost time: 3.2226579189300537
Epoch: 15, Steps: 267 | Train Loss: 0.3746911 Vali Loss: 0.5444632 Test Loss: 0.3990941
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4031475
	speed: 0.0576s/iter; left time: 1301.8664s
	iters: 200, epoch: 16 | loss: 0.3804396
	speed: 0.0100s/iter; left time: 224.1966s
Epoch: 16 cost time: 3.286489963531494
Epoch: 16, Steps: 267 | Train Loss: 0.3744269 Vali Loss: 0.5452961 Test Loss: 0.3993430
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3956951
	speed: 0.0571s/iter; left time: 1275.7300s
	iters: 200, epoch: 17 | loss: 0.3829667
	speed: 0.0102s/iter; left time: 226.2441s
Epoch: 17 cost time: 3.2467970848083496
Epoch: 17, Steps: 267 | Train Loss: 0.3744318 Vali Loss: 0.5453907 Test Loss: 0.3992585
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3636236
	speed: 0.0626s/iter; left time: 1381.2720s
	iters: 200, epoch: 18 | loss: 0.3760922
	speed: 0.0101s/iter; left time: 222.0335s
Epoch: 18 cost time: 3.3111319541931152
Epoch: 18, Steps: 267 | Train Loss: 0.3746051 Vali Loss: 0.5451437 Test Loss: 0.3992348
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3915137
	speed: 0.0580s/iter; left time: 1264.2010s
	iters: 200, epoch: 19 | loss: 0.3889067
	speed: 0.0101s/iter; left time: 218.4066s
Epoch: 19 cost time: 3.156789779663086
Epoch: 19, Steps: 267 | Train Loss: 0.3744997 Vali Loss: 0.5459244 Test Loss: 0.3993241
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3829003
	speed: 0.0581s/iter; left time: 1249.7951s
	iters: 200, epoch: 20 | loss: 0.3274965
	speed: 0.0100s/iter; left time: 214.4820s
Epoch: 20 cost time: 3.2252726554870605
Epoch: 20, Steps: 267 | Train Loss: 0.3743729 Vali Loss: 0.5460691 Test Loss: 0.3993404
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3548396
	speed: 0.0595s/iter; left time: 1264.1951s
	iters: 200, epoch: 21 | loss: 0.3594575
	speed: 0.0102s/iter; left time: 216.3699s
Epoch: 21 cost time: 3.3167128562927246
Epoch: 21, Steps: 267 | Train Loss: 0.3745565 Vali Loss: 0.5452696 Test Loss: 0.3991517
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3877422
	speed: 0.0576s/iter; left time: 1210.0679s
	iters: 200, epoch: 22 | loss: 0.3772626
	speed: 0.0101s/iter; left time: 211.7086s
Epoch: 22 cost time: 3.248574733734131
Epoch: 22, Steps: 267 | Train Loss: 0.3744781 Vali Loss: 0.5451231 Test Loss: 0.3993565
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3900636
	speed: 0.0592s/iter; left time: 1226.0934s
	iters: 200, epoch: 23 | loss: 0.3801585
	speed: 0.0099s/iter; left time: 203.7419s
Epoch: 23 cost time: 3.277244806289673
Epoch: 23, Steps: 267 | Train Loss: 0.3744301 Vali Loss: 0.5453182 Test Loss: 0.3993901
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3675532
	speed: 0.0583s/iter; left time: 1192.6799s
	iters: 200, epoch: 24 | loss: 0.3561510
	speed: 0.0101s/iter; left time: 205.4076s
Epoch: 24 cost time: 3.268791675567627
Epoch: 24, Steps: 267 | Train Loss: 0.3743638 Vali Loss: 0.5443054 Test Loss: 0.3991475
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3902249
	speed: 0.0575s/iter; left time: 1161.0822s
	iters: 200, epoch: 25 | loss: 0.4016484
	speed: 0.0100s/iter; left time: 200.7773s
Epoch: 25 cost time: 3.301699638366699
Epoch: 25, Steps: 267 | Train Loss: 0.3743130 Vali Loss: 0.5444092 Test Loss: 0.3993819
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3487944
	speed: 0.0560s/iter; left time: 1115.7599s
	iters: 200, epoch: 26 | loss: 0.3907989
	speed: 0.0098s/iter; left time: 194.8141s
Epoch: 26 cost time: 3.2357535362243652
Epoch: 26, Steps: 267 | Train Loss: 0.3741921 Vali Loss: 0.5455671 Test Loss: 0.3993460
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3337182
	speed: 0.0578s/iter; left time: 1135.7923s
	iters: 200, epoch: 27 | loss: 0.3976066
	speed: 0.0099s/iter; left time: 193.7589s
Epoch: 27 cost time: 3.257270574569702
Epoch: 27, Steps: 267 | Train Loss: 0.3743479 Vali Loss: 0.5451953 Test Loss: 0.3993348
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4057884
	speed: 0.0575s/iter; left time: 1114.1998s
	iters: 200, epoch: 28 | loss: 0.4039991
	speed: 0.0100s/iter; left time: 193.2085s
Epoch: 28 cost time: 3.1726326942443848
Epoch: 28, Steps: 267 | Train Loss: 0.3743092 Vali Loss: 0.5451571 Test Loss: 0.3992442
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3835197
	speed: 0.0583s/iter; left time: 1114.1716s
	iters: 200, epoch: 29 | loss: 0.3518384
	speed: 0.0098s/iter; left time: 187.2433s
Epoch: 29 cost time: 3.181518077850342
Epoch: 29, Steps: 267 | Train Loss: 0.3742780 Vali Loss: 0.5457627 Test Loss: 0.3992875
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3561269
	speed: 0.0593s/iter; left time: 1117.9801s
	iters: 200, epoch: 30 | loss: 0.3365139
	speed: 0.0102s/iter; left time: 190.6104s
Epoch: 30 cost time: 3.293865919113159
Epoch: 30, Steps: 267 | Train Loss: 0.3744219 Vali Loss: 0.5453379 Test Loss: 0.3993518
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3939352
	speed: 0.0584s/iter; left time: 1086.5233s
	iters: 200, epoch: 31 | loss: 0.3930557
	speed: 0.0100s/iter; left time: 184.3548s
Epoch: 31 cost time: 3.2386953830718994
Epoch: 31, Steps: 267 | Train Loss: 0.3743603 Vali Loss: 0.5448551 Test Loss: 0.3994367
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_192_FITS_ETTm1_ftM_sl90_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
mse:0.40049150586128235, mae:0.39824870228767395, rse:0.6024183034896851, corr:[0.54795235 0.5513445  0.54450035 0.5398966  0.54018563 0.54042816
 0.53806865 0.5350109  0.5338016  0.5338422  0.5335748  0.532534
 0.53101707 0.5293951  0.52701324 0.52361596 0.51995844 0.5168409
 0.5143059  0.5115841  0.5083626  0.50491756 0.5017471  0.49888492
 0.49581438 0.4921131  0.48838043 0.48562944 0.48419115 0.48315066
 0.48211983 0.48167545 0.48176065 0.4821787  0.48259252 0.48288763
 0.48271984 0.48249853 0.4825159  0.4826438  0.48266795 0.4822066
 0.48162627 0.481397   0.48134986 0.48120788 0.48085055 0.48043993
 0.48035777 0.4808418  0.48168114 0.48222622 0.4824413  0.48255035
 0.4830463  0.48374066 0.4842705  0.484359   0.48406118 0.4839625
 0.48416233 0.4840148  0.4834691  0.4827377  0.4825316  0.48278308
 0.48286414 0.48260275 0.4824883  0.4829323  0.48371428 0.48455748
 0.485209   0.48553362 0.48590705 0.48646685 0.48738796 0.48846558
 0.48936388 0.49003255 0.4905706  0.49092928 0.49131846 0.49165535
 0.4919666  0.49228436 0.4927636  0.49348935 0.49439868 0.49523824
 0.4957943  0.49613857 0.49657938 0.49716654 0.4974948  0.49713862
 0.496188   0.49518046 0.49432495 0.4935266  0.49296176 0.49268663
 0.49246258 0.49190074 0.49112093 0.4904275  0.48992178 0.4894358
 0.4887523  0.48780483 0.48669678 0.48560175 0.48463377 0.48380587
 0.4829562  0.48204342 0.48094267 0.4797665  0.47871175 0.47774437
 0.4766952  0.47532386 0.47391674 0.47306204 0.4726307  0.4721354
 0.4714528  0.470862   0.47065803 0.47076964 0.47090557 0.47094116
 0.4708761  0.47099718 0.47116295 0.47110954 0.47072664 0.4703147
 0.47026056 0.47057262 0.47084844 0.47085026 0.4706453  0.47061026
 0.4708751  0.47135407 0.47178295 0.4719294  0.472088   0.47233072
 0.47266448 0.47311622 0.47344318 0.47361428 0.47368962 0.47385412
 0.4739369  0.47377008 0.47344908 0.4733013  0.47354496 0.47382364
 0.47378314 0.47349426 0.4733859  0.47377002 0.47456735 0.47528937
 0.47556552 0.4755304  0.4757499  0.47660854 0.4775866  0.47821787
 0.47839832 0.47859523 0.47916922 0.47991636 0.48028404 0.48021185
 0.4801827  0.4808718  0.48223186 0.48356974 0.48416778 0.48447683
 0.48595744 0.4889519  0.49127585 0.49163517 0.49327216 0.5000866 ]
