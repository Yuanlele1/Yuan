Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=20, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=20, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3225600.0
params:  3780.0
Trainable parameters:  3780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.9770269
	speed: 0.0230s/iter; left time: 603.2922s
	iters: 200, epoch: 1 | loss: 0.7533393
	speed: 0.0150s/iter; left time: 390.5165s
Epoch: 1 cost time: 4.764926433563232
Epoch: 1, Steps: 263 | Train Loss: 0.9600835 Vali Loss: 1.3422377 Test Loss: 0.8015009
Validation loss decreased (inf --> 1.342238).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5805491
	speed: 0.0746s/iter; left time: 1936.2422s
	iters: 200, epoch: 2 | loss: 0.5242715
	speed: 0.0161s/iter; left time: 414.9437s
Epoch: 2 cost time: 4.717758893966675
Epoch: 2, Steps: 263 | Train Loss: 0.5750575 Vali Loss: 1.1013685 Test Loss: 0.5675774
Validation loss decreased (1.342238 --> 1.101369).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5300528
	speed: 0.0740s/iter; left time: 1900.7810s
	iters: 200, epoch: 3 | loss: 0.4794156
	speed: 0.0153s/iter; left time: 391.1082s
Epoch: 3 cost time: 4.523088455200195
Epoch: 3, Steps: 263 | Train Loss: 0.5144092 Vali Loss: 1.0493562 Test Loss: 0.5193583
Validation loss decreased (1.101369 --> 1.049356).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4928718
	speed: 0.0718s/iter; left time: 1825.6025s
	iters: 200, epoch: 4 | loss: 0.4581712
	speed: 0.0152s/iter; left time: 383.7947s
Epoch: 4 cost time: 4.493107557296753
Epoch: 4, Steps: 263 | Train Loss: 0.5005763 Vali Loss: 1.0284643 Test Loss: 0.5031998
Validation loss decreased (1.049356 --> 1.028464).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4907337
	speed: 0.0747s/iter; left time: 1878.6877s
	iters: 200, epoch: 5 | loss: 0.5155235
	speed: 0.0158s/iter; left time: 395.6809s
Epoch: 5 cost time: 4.551995277404785
Epoch: 5, Steps: 263 | Train Loss: 0.4962335 Vali Loss: 1.0203727 Test Loss: 0.4969235
Validation loss decreased (1.028464 --> 1.020373).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4582711
	speed: 0.0770s/iter; left time: 1916.3412s
	iters: 200, epoch: 6 | loss: 0.4634975
	speed: 0.0170s/iter; left time: 420.3225s
Epoch: 6 cost time: 5.098717212677002
Epoch: 6, Steps: 263 | Train Loss: 0.4946525 Vali Loss: 1.0163500 Test Loss: 0.4946474
Validation loss decreased (1.020373 --> 1.016350).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4648018
	speed: 0.0804s/iter; left time: 1980.2564s
	iters: 200, epoch: 7 | loss: 0.4484401
	speed: 0.0168s/iter; left time: 412.5376s
Epoch: 7 cost time: 4.9673752784729
Epoch: 7, Steps: 263 | Train Loss: 0.4938964 Vali Loss: 1.0141138 Test Loss: 0.4937977
Validation loss decreased (1.016350 --> 1.014114).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5150310
	speed: 0.0737s/iter; left time: 1795.1290s
	iters: 200, epoch: 8 | loss: 0.4882671
	speed: 0.0151s/iter; left time: 365.4380s
Epoch: 8 cost time: 4.368619680404663
Epoch: 8, Steps: 263 | Train Loss: 0.4939113 Vali Loss: 1.0130762 Test Loss: 0.4933434
Validation loss decreased (1.014114 --> 1.013076).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4584569
	speed: 0.0708s/iter; left time: 1706.6732s
	iters: 200, epoch: 9 | loss: 0.5053612
	speed: 0.0147s/iter; left time: 352.3191s
Epoch: 9 cost time: 4.389422416687012
Epoch: 9, Steps: 263 | Train Loss: 0.4938994 Vali Loss: 1.0133836 Test Loss: 0.4931084
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5091050
	speed: 0.0717s/iter; left time: 1709.4196s
	iters: 200, epoch: 10 | loss: 0.5004533
	speed: 0.0147s/iter; left time: 349.8927s
Epoch: 10 cost time: 4.542378664016724
Epoch: 10, Steps: 263 | Train Loss: 0.4937614 Vali Loss: 1.0133076 Test Loss: 0.4934729
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4831791
	speed: 0.0711s/iter; left time: 1674.7419s
	iters: 200, epoch: 11 | loss: 0.5019174
	speed: 0.0150s/iter; left time: 351.3732s
Epoch: 11 cost time: 4.4335479736328125
Epoch: 11, Steps: 263 | Train Loss: 0.4936422 Vali Loss: 1.0133078 Test Loss: 0.4933293
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5100751
	speed: 0.0754s/iter; left time: 1757.7249s
	iters: 200, epoch: 12 | loss: 0.5199805
	speed: 0.0156s/iter; left time: 362.4097s
Epoch: 12 cost time: 4.879116058349609
Epoch: 12, Steps: 263 | Train Loss: 0.4938028 Vali Loss: 1.0124238 Test Loss: 0.4935686
Validation loss decreased (1.013076 --> 1.012424).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4602511
	speed: 0.0720s/iter; left time: 1660.1444s
	iters: 200, epoch: 13 | loss: 0.5030939
	speed: 0.0149s/iter; left time: 342.1114s
Epoch: 13 cost time: 4.413687467575073
Epoch: 13, Steps: 263 | Train Loss: 0.4934924 Vali Loss: 1.0130606 Test Loss: 0.4936395
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4940845
	speed: 0.0726s/iter; left time: 1653.0140s
	iters: 200, epoch: 14 | loss: 0.4534302
	speed: 0.0148s/iter; left time: 335.7148s
Epoch: 14 cost time: 4.494760036468506
Epoch: 14, Steps: 263 | Train Loss: 0.4937122 Vali Loss: 1.0133449 Test Loss: 0.4933826
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5082095
	speed: 0.0758s/iter; left time: 1706.9022s
	iters: 200, epoch: 15 | loss: 0.4621957
	speed: 0.0151s/iter; left time: 339.5050s
Epoch: 15 cost time: 4.523679494857788
Epoch: 15, Steps: 263 | Train Loss: 0.4935744 Vali Loss: 1.0128391 Test Loss: 0.4935924
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4966505
	speed: 0.0764s/iter; left time: 1701.1856s
	iters: 200, epoch: 16 | loss: 0.5217503
	speed: 0.0147s/iter; left time: 325.2338s
Epoch: 16 cost time: 4.519424676895142
Epoch: 16, Steps: 263 | Train Loss: 0.4935495 Vali Loss: 1.0121889 Test Loss: 0.4936701
Validation loss decreased (1.012424 --> 1.012189).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5197663
	speed: 0.0705s/iter; left time: 1550.5798s
	iters: 200, epoch: 17 | loss: 0.4770002
	speed: 0.0148s/iter; left time: 324.4105s
Epoch: 17 cost time: 4.482500076293945
Epoch: 17, Steps: 263 | Train Loss: 0.4934024 Vali Loss: 1.0128838 Test Loss: 0.4937153
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4904267
	speed: 0.0796s/iter; left time: 1729.8789s
	iters: 200, epoch: 18 | loss: 0.5122437
	speed: 0.0166s/iter; left time: 358.1162s
Epoch: 18 cost time: 4.898638963699341
Epoch: 18, Steps: 263 | Train Loss: 0.4934799 Vali Loss: 1.0126910 Test Loss: 0.4938066
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5078441
	speed: 0.0724s/iter; left time: 1554.0117s
	iters: 200, epoch: 19 | loss: 0.4972165
	speed: 0.0152s/iter; left time: 324.3767s
Epoch: 19 cost time: 4.548894643783569
Epoch: 19, Steps: 263 | Train Loss: 0.4935471 Vali Loss: 1.0121032 Test Loss: 0.4935993
Validation loss decreased (1.012189 --> 1.012103).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4894398
	speed: 0.0726s/iter; left time: 1538.7000s
	iters: 200, epoch: 20 | loss: 0.4570804
	speed: 0.0148s/iter; left time: 312.4615s
Epoch: 20 cost time: 4.36326265335083
Epoch: 20, Steps: 263 | Train Loss: 0.4934236 Vali Loss: 1.0124304 Test Loss: 0.4936073
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5325574
	speed: 0.0721s/iter; left time: 1508.9888s
	iters: 200, epoch: 21 | loss: 0.5049531
	speed: 0.0148s/iter; left time: 307.8922s
Epoch: 21 cost time: 4.5354156494140625
Epoch: 21, Steps: 263 | Train Loss: 0.4933793 Vali Loss: 1.0128102 Test Loss: 0.4936022
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4951626
	speed: 0.0724s/iter; left time: 1497.5466s
	iters: 200, epoch: 22 | loss: 0.4644254
	speed: 0.0164s/iter; left time: 338.1420s
Epoch: 22 cost time: 4.847736835479736
Epoch: 22, Steps: 263 | Train Loss: 0.4934248 Vali Loss: 1.0127585 Test Loss: 0.4938086
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4557778
	speed: 0.0753s/iter; left time: 1537.8259s
	iters: 200, epoch: 23 | loss: 0.4908411
	speed: 0.0154s/iter; left time: 312.6126s
Epoch: 23 cost time: 4.637425899505615
Epoch: 23, Steps: 263 | Train Loss: 0.4933947 Vali Loss: 1.0119553 Test Loss: 0.4939353
Validation loss decreased (1.012103 --> 1.011955).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4887595
	speed: 0.0731s/iter; left time: 1472.5877s
	iters: 200, epoch: 24 | loss: 0.5102240
	speed: 0.0148s/iter; left time: 297.4645s
Epoch: 24 cost time: 4.4961183071136475
Epoch: 24, Steps: 263 | Train Loss: 0.4933596 Vali Loss: 1.0124668 Test Loss: 0.4936077
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4517228
	speed: 0.0733s/iter; left time: 1458.2738s
	iters: 200, epoch: 25 | loss: 0.5097287
	speed: 0.0152s/iter; left time: 301.1126s
Epoch: 25 cost time: 4.4962217807769775
Epoch: 25, Steps: 263 | Train Loss: 0.4934702 Vali Loss: 1.0131811 Test Loss: 0.4937837
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4805145
	speed: 0.0709s/iter; left time: 1391.2999s
	iters: 200, epoch: 26 | loss: 0.4846677
	speed: 0.0148s/iter; left time: 289.5211s
Epoch: 26 cost time: 4.453535556793213
Epoch: 26, Steps: 263 | Train Loss: 0.4934100 Vali Loss: 1.0132387 Test Loss: 0.4938969
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4489208
	speed: 0.0727s/iter; left time: 1407.5572s
	iters: 200, epoch: 27 | loss: 0.4667464
	speed: 0.0149s/iter; left time: 286.6843s
Epoch: 27 cost time: 4.4771435260772705
Epoch: 27, Steps: 263 | Train Loss: 0.4934129 Vali Loss: 1.0128076 Test Loss: 0.4938273
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5520524
	speed: 0.0731s/iter; left time: 1395.4806s
	iters: 200, epoch: 28 | loss: 0.4962953
	speed: 0.0154s/iter; left time: 292.3087s
Epoch: 28 cost time: 5.301090240478516
Epoch: 28, Steps: 263 | Train Loss: 0.4932803 Vali Loss: 1.0132231 Test Loss: 0.4938296
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.5106643
	speed: 0.0854s/iter; left time: 1609.2539s
	iters: 200, epoch: 29 | loss: 0.5079039
	speed: 0.0164s/iter; left time: 307.0718s
Epoch: 29 cost time: 4.874869346618652
Epoch: 29, Steps: 263 | Train Loss: 0.4934160 Vali Loss: 1.0127106 Test Loss: 0.4939628
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4470411
	speed: 0.0767s/iter; left time: 1425.2093s
	iters: 200, epoch: 30 | loss: 0.4906208
	speed: 0.0156s/iter; left time: 287.7502s
Epoch: 30 cost time: 4.6966869831085205
Epoch: 30, Steps: 263 | Train Loss: 0.4933417 Vali Loss: 1.0138323 Test Loss: 0.4938978
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4895812
	speed: 0.0753s/iter; left time: 1379.1971s
	iters: 200, epoch: 31 | loss: 0.5032983
	speed: 0.0149s/iter; left time: 271.2035s
Epoch: 31 cost time: 4.380274295806885
Epoch: 31, Steps: 263 | Train Loss: 0.4933045 Vali Loss: 1.0125228 Test Loss: 0.4939235
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.5004159
	speed: 0.0713s/iter; left time: 1286.2335s
	iters: 200, epoch: 32 | loss: 0.4783024
	speed: 0.0149s/iter; left time: 268.3220s
Epoch: 32 cost time: 4.402155876159668
Epoch: 32, Steps: 263 | Train Loss: 0.4932058 Vali Loss: 1.0130709 Test Loss: 0.4939927
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.5096580
	speed: 0.0730s/iter; left time: 1297.9110s
	iters: 200, epoch: 33 | loss: 0.5119174
	speed: 0.0151s/iter; left time: 267.1815s
Epoch: 33 cost time: 4.605634450912476
Epoch: 33, Steps: 263 | Train Loss: 0.4931716 Vali Loss: 1.0128413 Test Loss: 0.4939146
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4844366
	speed: 0.0719s/iter; left time: 1260.4668s
	iters: 200, epoch: 34 | loss: 0.4991835
	speed: 0.0154s/iter; left time: 268.4129s
Epoch: 34 cost time: 4.531671762466431
Epoch: 34, Steps: 263 | Train Loss: 0.4934625 Vali Loss: 1.0132880 Test Loss: 0.4939868
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5066746
	speed: 0.0725s/iter; left time: 1251.5888s
	iters: 200, epoch: 35 | loss: 0.4833966
	speed: 0.0148s/iter; left time: 254.2307s
Epoch: 35 cost time: 4.483097791671753
Epoch: 35, Steps: 263 | Train Loss: 0.4932728 Vali Loss: 1.0118002 Test Loss: 0.4939781
Validation loss decreased (1.011955 --> 1.011800).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4715590
	speed: 0.0715s/iter; left time: 1215.0409s
	iters: 200, epoch: 36 | loss: 0.4644832
	speed: 0.0166s/iter; left time: 280.7377s
Epoch: 36 cost time: 4.805847644805908
Epoch: 36, Steps: 263 | Train Loss: 0.4935128 Vali Loss: 1.0124576 Test Loss: 0.4940246
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4845934
	speed: 0.0780s/iter; left time: 1305.2831s
	iters: 200, epoch: 37 | loss: 0.5078538
	speed: 0.0153s/iter; left time: 253.9903s
Epoch: 37 cost time: 4.590353012084961
Epoch: 37, Steps: 263 | Train Loss: 0.4932043 Vali Loss: 1.0120926 Test Loss: 0.4940307
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.5253417
	speed: 0.0711s/iter; left time: 1170.2162s
	iters: 200, epoch: 38 | loss: 0.4512131
	speed: 0.0152s/iter; left time: 248.6241s
Epoch: 38 cost time: 4.496873140335083
Epoch: 38, Steps: 263 | Train Loss: 0.4933724 Vali Loss: 1.0127288 Test Loss: 0.4940796
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4772034
	speed: 0.0747s/iter; left time: 1210.7705s
	iters: 200, epoch: 39 | loss: 0.5134354
	speed: 0.0146s/iter; left time: 235.7922s
Epoch: 39 cost time: 4.451932668685913
Epoch: 39, Steps: 263 | Train Loss: 0.4932813 Vali Loss: 1.0132214 Test Loss: 0.4940455
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4751554
	speed: 0.0720s/iter; left time: 1147.5780s
	iters: 200, epoch: 40 | loss: 0.5138005
	speed: 0.0145s/iter; left time: 229.8275s
Epoch: 40 cost time: 4.356281757354736
Epoch: 40, Steps: 263 | Train Loss: 0.4932278 Vali Loss: 1.0135261 Test Loss: 0.4940663
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.5452499
	speed: 0.0721s/iter; left time: 1130.3404s
	iters: 200, epoch: 41 | loss: 0.4897895
	speed: 0.0147s/iter; left time: 228.9961s
Epoch: 41 cost time: 4.432160139083862
Epoch: 41, Steps: 263 | Train Loss: 0.4932895 Vali Loss: 1.0128006 Test Loss: 0.4940858
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4862890
	speed: 0.0711s/iter; left time: 1095.9839s
	iters: 200, epoch: 42 | loss: 0.4698707
	speed: 0.0148s/iter; left time: 227.0668s
Epoch: 42 cost time: 4.424699783325195
Epoch: 42, Steps: 263 | Train Loss: 0.4934257 Vali Loss: 1.0128951 Test Loss: 0.4940979
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4918255
	speed: 0.0739s/iter; left time: 1119.3986s
	iters: 200, epoch: 43 | loss: 0.4745269
	speed: 0.0150s/iter; left time: 226.2342s
Epoch: 43 cost time: 4.653446435928345
Epoch: 43, Steps: 263 | Train Loss: 0.4932697 Vali Loss: 1.0128673 Test Loss: 0.4940695
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4924114
	speed: 0.0737s/iter; left time: 1097.9174s
	iters: 200, epoch: 44 | loss: 0.5067440
	speed: 0.0150s/iter; left time: 221.3449s
Epoch: 44 cost time: 4.520319223403931
Epoch: 44, Steps: 263 | Train Loss: 0.4932338 Vali Loss: 1.0132892 Test Loss: 0.4940830
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5083621
	speed: 0.0776s/iter; left time: 1134.4975s
	iters: 200, epoch: 45 | loss: 0.5291995
	speed: 0.0150s/iter; left time: 217.8394s
Epoch: 45 cost time: 4.4266884326934814
Epoch: 45, Steps: 263 | Train Loss: 0.4932782 Vali Loss: 1.0133994 Test Loss: 0.4940476
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.5154960
	speed: 0.0697s/iter; left time: 1001.8962s
	iters: 200, epoch: 46 | loss: 0.5073816
	speed: 0.0158s/iter; left time: 225.3088s
Epoch: 46 cost time: 4.439661026000977
Epoch: 46, Steps: 263 | Train Loss: 0.4933803 Vali Loss: 1.0123507 Test Loss: 0.4940867
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4873503
	speed: 0.0707s/iter; left time: 997.6949s
	iters: 200, epoch: 47 | loss: 0.4887127
	speed: 0.0148s/iter; left time: 207.8233s
Epoch: 47 cost time: 4.428954839706421
Epoch: 47, Steps: 263 | Train Loss: 0.4933353 Vali Loss: 1.0134981 Test Loss: 0.4940973
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4748351
	speed: 0.0730s/iter; left time: 1010.9901s
	iters: 200, epoch: 48 | loss: 0.4651811
	speed: 0.0148s/iter; left time: 203.3196s
Epoch: 48 cost time: 4.533477067947388
Epoch: 48, Steps: 263 | Train Loss: 0.4931438 Vali Loss: 1.0134172 Test Loss: 0.4941203
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4554576
	speed: 0.0732s/iter; left time: 994.0306s
	iters: 200, epoch: 49 | loss: 0.4953324
	speed: 0.0152s/iter; left time: 204.6094s
Epoch: 49 cost time: 4.491489887237549
Epoch: 49, Steps: 263 | Train Loss: 0.4932923 Vali Loss: 1.0126348 Test Loss: 0.4940916
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.4510010
	speed: 0.0720s/iter; left time: 958.1783s
	iters: 200, epoch: 50 | loss: 0.4985547
	speed: 0.0152s/iter; left time: 200.6948s
Epoch: 50 cost time: 4.514692306518555
Epoch: 50, Steps: 263 | Train Loss: 0.4932250 Vali Loss: 1.0134130 Test Loss: 0.4940536
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4923251
	speed: 0.0734s/iter; left time: 958.4426s
	iters: 200, epoch: 51 | loss: 0.4807391
	speed: 0.0158s/iter; left time: 205.0834s
Epoch: 51 cost time: 4.770730018615723
Epoch: 51, Steps: 263 | Train Loss: 0.4933190 Vali Loss: 1.0127581 Test Loss: 0.4940858
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4712917
	speed: 0.0723s/iter; left time: 924.5005s
	iters: 200, epoch: 52 | loss: 0.4725242
	speed: 0.0146s/iter; left time: 185.1359s
Epoch: 52 cost time: 4.371835947036743
Epoch: 52, Steps: 263 | Train Loss: 0.4933689 Vali Loss: 1.0129743 Test Loss: 0.4941145
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.5138265
	speed: 0.0710s/iter; left time: 888.8985s
	iters: 200, epoch: 53 | loss: 0.4442034
	speed: 0.0149s/iter; left time: 185.0392s
Epoch: 53 cost time: 4.457627534866333
Epoch: 53, Steps: 263 | Train Loss: 0.4934115 Vali Loss: 1.0126739 Test Loss: 0.4941236
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4909302
	speed: 0.0720s/iter; left time: 882.3072s
	iters: 200, epoch: 54 | loss: 0.4951666
	speed: 0.0150s/iter; left time: 182.0147s
Epoch: 54 cost time: 4.515645265579224
Epoch: 54, Steps: 263 | Train Loss: 0.4932818 Vali Loss: 1.0129507 Test Loss: 0.4941435
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.4704598
	speed: 0.0737s/iter; left time: 883.8243s
	iters: 200, epoch: 55 | loss: 0.5094100
	speed: 0.0167s/iter; left time: 198.1767s
Epoch: 55 cost time: 4.916850328445435
Epoch: 55, Steps: 263 | Train Loss: 0.4932390 Vali Loss: 1.0124830 Test Loss: 0.4941310
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.49208754301071167, mae:0.45253536105155945, rse:0.66740882396698, corr:[0.53228605 0.5313177  0.5267226  0.5244376  0.52164304 0.5180237
 0.51527435 0.5136247  0.5116289  0.5096008  0.50842345 0.5079971
 0.5070619  0.5049473  0.50204414 0.49890146 0.49559078 0.49224275
 0.48924986 0.4865929  0.48390436 0.48086748 0.47755614 0.4742177
 0.47098508 0.46748424 0.46393412 0.46106488 0.45935276 0.45809972
 0.45689732 0.45638064 0.45653492 0.45729107 0.45804015 0.45837158
 0.4579757  0.45777613 0.4580434  0.45819572 0.4579288  0.45721158
 0.4568603  0.4569674  0.45672268 0.4559942  0.45551094 0.45574734
 0.45637205 0.45680913 0.45699054 0.4571777  0.45778346 0.45839438
 0.45885375 0.45915934 0.4597272  0.46036616 0.46035486 0.4599107
 0.4595995  0.45939943 0.45927733 0.45874026 0.45823538 0.45807898
 0.4580961  0.45806417 0.4581795  0.4586527  0.45925066 0.45992503
 0.46064922 0.4612448  0.46197456 0.46267772 0.4635209  0.46439853
 0.46516022 0.46600866 0.46692127 0.46742883 0.4676032  0.4676133
 0.4680248  0.46879607 0.46955293 0.47010845 0.47074565 0.4717675
 0.47290778 0.47366405 0.4739884  0.4742411  0.47467685 0.47486392
 0.47421062 0.47283664 0.47147146 0.47084284 0.47084188 0.4707605
 0.47036    0.46994045 0.46976772 0.46942303 0.4686274  0.46773016
 0.4671197  0.46664238 0.4658077  0.46455744 0.46334803 0.46253946
 0.46185142 0.46104285 0.459977   0.45888934 0.45785797 0.45664215
 0.4551989  0.45360506 0.45224476 0.45144346 0.45074072 0.4498109
 0.4489278  0.4484272  0.44824672 0.44807577 0.44787204 0.44789752
 0.44806513 0.4482221  0.44792846 0.4473233  0.44692847 0.44696265
 0.44703513 0.4467992  0.4463555  0.44614658 0.44624415 0.44645724
 0.44652775 0.44663948 0.4469692  0.44728324 0.4476563  0.44788787
 0.44806945 0.44852927 0.44896045 0.44910097 0.44895607 0.44897467
 0.44912502 0.44916132 0.44892862 0.44872296 0.44889155 0.44916475
 0.44928485 0.44934142 0.44967985 0.45032722 0.45109197 0.4517615
 0.45238033 0.4530884  0.45386586 0.45470828 0.45541906 0.45613036
 0.45680523 0.45739937 0.45785004 0.4581986  0.4584206  0.45867193
 0.45892707 0.4592646  0.45970073 0.46034503 0.4611909  0.46214077
 0.46309268 0.46383816 0.46413195 0.46397826 0.46379027 0.46403414
 0.4647983  0.4657972  0.46657857 0.46713632 0.46779102 0.46864113
 0.46915165 0.4688432  0.46805632 0.46732074 0.46675342 0.4661408
 0.46511084 0.4636032  0.46188077 0.46039358 0.4590189  0.45754057
 0.4558619  0.45432228 0.45316854 0.4520163  0.45041716 0.44859168
 0.44662392 0.44485638 0.44331077 0.44200835 0.4407751  0.43976533
 0.43918926 0.43906075 0.43886873 0.43863067 0.43885958 0.43903956
 0.43878567 0.43836066 0.43806145 0.4378927  0.4377958  0.43743244
 0.43698004 0.43673176 0.43651572 0.4363186  0.43619078 0.4360201
 0.4362807  0.43668413 0.43696404 0.43716684 0.43727165 0.43768698
 0.43820643 0.4385333  0.43864256 0.4390777  0.43963158 0.44005182
 0.43981874 0.43935356 0.43903556 0.43944994 0.43984953 0.4397217
 0.43955985 0.4399155  0.4404218  0.4409128  0.44140005 0.44200075
 0.4430099  0.44389364 0.44445932 0.4450632  0.44612673 0.44732326
 0.44839913 0.44904873 0.44954565 0.45007882 0.450703   0.45125934
 0.45172516 0.4523051  0.4529577  0.45369518 0.4543648  0.454963
 0.4556543  0.45640406 0.4570164  0.45714194 0.45651454 0.45521104
 0.45340908 0.451302   0.44914633 0.4474002  0.44648156 0.44627148
 0.4461556  0.44576252 0.44545206 0.4452586  0.4448428  0.4440234
 0.4429166  0.44176355 0.44056392 0.43918666 0.43788737 0.436896
 0.43638495 0.4358325  0.4350156  0.43400702 0.43308958 0.43228573
 0.43136543 0.4302677  0.42922702 0.42848316 0.42775017 0.42715147
 0.42664683 0.42619133 0.4258961  0.425787   0.42567956 0.4254905
 0.42513484 0.42494106 0.42486182 0.4247122  0.42431962 0.42376703
 0.42355955 0.42354473 0.42351046 0.42326197 0.42308563 0.4230685
 0.4232074  0.42315772 0.4232976  0.4235076  0.4238738  0.42408362
 0.42412752 0.42437357 0.4247766  0.42515293 0.42523888 0.42511436
 0.42505702 0.42511573 0.42521575 0.42511493 0.42487866 0.42481953
 0.4249951  0.42521387 0.425409   0.42571875 0.42631018 0.42706043
 0.42767864 0.4281092  0.42859378 0.4292877  0.43039572 0.43157238
 0.4325178  0.43343288 0.4343659  0.43507564 0.43567997 0.43627617
 0.43721896 0.43843627 0.43980983 0.44111595 0.44242382 0.44397557
 0.44550872 0.44638982 0.44643864 0.44623908 0.44626033 0.44642448
 0.44625032 0.44580513 0.44575188 0.44641012 0.44743365 0.44839427
 0.44902945 0.44944394 0.4498781  0.45007288 0.44967645 0.44873822
 0.44768667 0.44673046 0.4455744  0.44413075 0.4426478  0.4415585
 0.44087946 0.44026622 0.43955413 0.43869686 0.4376696  0.4365173
 0.43536705 0.43402657 0.4329129  0.43248683 0.4322048  0.4317291
 0.43126196 0.4308895  0.43078122 0.4306061  0.43020332 0.42999294
 0.43006366 0.43020698 0.42994377 0.4294021  0.4290683  0.42917094
 0.42930236 0.4291062  0.42833164 0.42779043 0.42772573 0.4279257
 0.427987   0.4278614  0.42800665 0.428492   0.42885652 0.4290072
 0.42916986 0.42965856 0.430226   0.430567   0.43064693 0.43076712
 0.4310142  0.4312992  0.4311927  0.43094912 0.43092006 0.4312351
 0.43144843 0.43145648 0.43132967 0.43147168 0.43176064 0.43210506
 0.43249702 0.4329885  0.4337357  0.4347159  0.43562472 0.43635008
 0.4371225  0.43814158 0.43898776 0.43940723 0.43956462 0.43993923
 0.44080332 0.44188595 0.4429497  0.44376078 0.4444803  0.44538662
 0.44636026 0.447028   0.44717354 0.44704887 0.44678006 0.44612032
 0.44472453 0.44297275 0.44152987 0.4407568  0.4403664  0.44015998
 0.44002378 0.43999624 0.4401049  0.43991014 0.43922004 0.4381567
 0.43688723 0.4355356  0.43413606 0.43267035 0.4311329  0.42958722
 0.42816538 0.42708018 0.4262189  0.4252019  0.42408404 0.42287752
 0.42182353 0.42078054 0.4196957  0.41868037 0.41781276 0.4172686
 0.41690052 0.4166557  0.4163943  0.41621116 0.41615406 0.41620752
 0.41600057 0.41563946 0.41539118 0.4151559  0.414996   0.41480684
 0.41456607 0.41456646 0.41458157 0.4144738  0.41409144 0.41374823
 0.41358832 0.41365474 0.41372606 0.413674   0.41367328 0.4139112
 0.41438976 0.41462836 0.4146078  0.41456088 0.41467068 0.41476616
 0.41488254 0.4148355  0.41468874 0.41460666 0.41456226 0.41462776
 0.41482243 0.4151429  0.41546392 0.41561678 0.41560486 0.41581875
 0.4163866  0.41704622 0.41771227 0.41840008 0.41925362 0.4202023
 0.42111582 0.42197654 0.4227407  0.42340872 0.4241016  0.42478025
 0.42539492 0.42617792 0.42706126 0.42799473 0.42886004 0.42970031
 0.43061218 0.43135655 0.43151832 0.43081927 0.42953926 0.4282204
 0.42696443 0.42555317 0.42399517 0.42263997 0.42217016 0.4225144
 0.42293334 0.42302844 0.42313358 0.42339855 0.4233764  0.4226293
 0.42117316 0.4194834  0.417909   0.41645914 0.4149654  0.4134786
 0.41235766 0.4116337  0.4109293  0.40988582 0.4086485  0.40758404
 0.4067721  0.40583137 0.40469795 0.4036988  0.40298015 0.40257603
 0.4021445  0.40168178 0.40134925 0.40143672 0.40159798 0.40155566
 0.40119624 0.40103722 0.40129864 0.40151018 0.40151235 0.4012787
 0.40131313 0.40160865 0.401497   0.4009188  0.4005424  0.40087038
 0.40140006 0.40154886 0.4014533  0.4013614  0.40164733 0.40206444
 0.4019921  0.40157714 0.40169024 0.40227756 0.40276915 0.40256914
 0.4020475  0.40187034 0.40205744 0.4018739  0.40129247 0.40083715
 0.40092272 0.4012788  0.40142313 0.4014688  0.40165117 0.40220985
 0.4028747  0.403256   0.4034588  0.4040646  0.40506425 0.40614966
 0.40685916 0.40749753 0.40836397 0.40929514 0.40988177 0.41011727
 0.41047627 0.411356   0.41266948 0.41377482 0.41450983 0.41528323
 0.4162323  0.41691223 0.41682518 0.41602442 0.4149711  0.41381413
 0.41228336 0.41050014 0.40923765 0.40897328 0.40940776 0.40987822
 0.41016632 0.41061577 0.4113153  0.41149834 0.41068164 0.40939188
 0.40843046 0.40769017 0.40633023 0.40405464 0.40178216 0.4003806
 0.39967775 0.39868504 0.39719644 0.39589438 0.39509562 0.39426574
 0.39283684 0.39125878 0.3901528  0.3894031  0.3883501  0.38706636
 0.3863531  0.3863197  0.38626042 0.3853766  0.38435677 0.38483828
 0.38606212 0.3860586  0.3849856  0.38504916 0.3868131  0.3879418
 0.3870654  0.3864968  0.3883595  0.39008203 0.38894194 0.38831806]
