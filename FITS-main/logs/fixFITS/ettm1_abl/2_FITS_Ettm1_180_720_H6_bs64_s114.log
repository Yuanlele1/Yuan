Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=22, out_features=110, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2168320.0
params:  2530.0
Trainable parameters:  2530
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7926486
	speed: 0.0224s/iter; left time: 585.4881s
	iters: 200, epoch: 1 | loss: 0.5376027
	speed: 0.0166s/iter; left time: 431.8880s
Epoch: 1 cost time: 4.88934850692749
Epoch: 1, Steps: 262 | Train Loss: 0.7380329 Vali Loss: 1.2859825 Test Loss: 0.7061424
Validation loss decreased (inf --> 1.285982).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4222164
	speed: 0.0747s/iter; left time: 1929.2171s
	iters: 200, epoch: 2 | loss: 0.4117754
	speed: 0.0161s/iter; left time: 414.8076s
Epoch: 2 cost time: 4.778720855712891
Epoch: 2, Steps: 262 | Train Loss: 0.4526113 Vali Loss: 1.0952383 Test Loss: 0.5353714
Validation loss decreased (1.285982 --> 1.095238).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4088925
	speed: 0.0795s/iter; left time: 2033.4262s
	iters: 200, epoch: 3 | loss: 0.3834653
	speed: 0.0158s/iter; left time: 403.0922s
Epoch: 3 cost time: 4.753908157348633
Epoch: 3, Steps: 262 | Train Loss: 0.3969523 Vali Loss: 1.0414155 Test Loss: 0.4901743
Validation loss decreased (1.095238 --> 1.041415).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3807748
	speed: 0.0777s/iter; left time: 1966.7734s
	iters: 200, epoch: 4 | loss: 0.3883146
	speed: 0.0156s/iter; left time: 394.5653s
Epoch: 4 cost time: 4.703848838806152
Epoch: 4, Steps: 262 | Train Loss: 0.3795568 Vali Loss: 1.0162581 Test Loss: 0.4704911
Validation loss decreased (1.041415 --> 1.016258).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3900544
	speed: 0.0751s/iter; left time: 1882.4541s
	iters: 200, epoch: 5 | loss: 0.3677372
	speed: 0.0158s/iter; left time: 393.9258s
Epoch: 5 cost time: 4.637351989746094
Epoch: 5, Steps: 262 | Train Loss: 0.3711992 Vali Loss: 1.0036466 Test Loss: 0.4600276
Validation loss decreased (1.016258 --> 1.003647).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3794428
	speed: 0.0749s/iter; left time: 1857.8232s
	iters: 200, epoch: 6 | loss: 0.3679472
	speed: 0.0160s/iter; left time: 394.5433s
Epoch: 6 cost time: 4.7118847370147705
Epoch: 6, Steps: 262 | Train Loss: 0.3664895 Vali Loss: 0.9960799 Test Loss: 0.4543695
Validation loss decreased (1.003647 --> 0.996080).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3552873
	speed: 0.0754s/iter; left time: 1848.8940s
	iters: 200, epoch: 7 | loss: 0.3949942
	speed: 0.0161s/iter; left time: 394.4107s
Epoch: 7 cost time: 4.672044992446899
Epoch: 7, Steps: 262 | Train Loss: 0.3638063 Vali Loss: 0.9904700 Test Loss: 0.4509871
Validation loss decreased (0.996080 --> 0.990470).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3785041
	speed: 0.0755s/iter; left time: 1832.3750s
	iters: 200, epoch: 8 | loss: 0.3702850
	speed: 0.0156s/iter; left time: 377.3408s
Epoch: 8 cost time: 4.652585983276367
Epoch: 8, Steps: 262 | Train Loss: 0.3622293 Vali Loss: 0.9876334 Test Loss: 0.4492675
Validation loss decreased (0.990470 --> 0.987633).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3671799
	speed: 0.0760s/iter; left time: 1824.5481s
	iters: 200, epoch: 9 | loss: 0.3609706
	speed: 0.0166s/iter; left time: 396.3390s
Epoch: 9 cost time: 4.856228828430176
Epoch: 9, Steps: 262 | Train Loss: 0.3612150 Vali Loss: 0.9862574 Test Loss: 0.4481466
Validation loss decreased (0.987633 --> 0.986257).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3742685
	speed: 0.0768s/iter; left time: 1823.2268s
	iters: 200, epoch: 10 | loss: 0.3861406
	speed: 0.0157s/iter; left time: 371.4342s
Epoch: 10 cost time: 4.689358949661255
Epoch: 10, Steps: 262 | Train Loss: 0.3604637 Vali Loss: 0.9852427 Test Loss: 0.4476446
Validation loss decreased (0.986257 --> 0.985243).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3789669
	speed: 0.0751s/iter; left time: 1763.9631s
	iters: 200, epoch: 11 | loss: 0.3709071
	speed: 0.0159s/iter; left time: 370.7834s
Epoch: 11 cost time: 4.793138265609741
Epoch: 11, Steps: 262 | Train Loss: 0.3602433 Vali Loss: 0.9831645 Test Loss: 0.4471669
Validation loss decreased (0.985243 --> 0.983164).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3475186
	speed: 0.0762s/iter; left time: 1769.7052s
	iters: 200, epoch: 12 | loss: 0.3506387
	speed: 0.0155s/iter; left time: 359.4160s
Epoch: 12 cost time: 4.683318138122559
Epoch: 12, Steps: 262 | Train Loss: 0.3602081 Vali Loss: 0.9851763 Test Loss: 0.4472504
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3535849
	speed: 0.0899s/iter; left time: 2064.7256s
	iters: 200, epoch: 13 | loss: 0.3668466
	speed: 0.0164s/iter; left time: 373.7201s
Epoch: 13 cost time: 4.827875375747681
Epoch: 13, Steps: 262 | Train Loss: 0.3600689 Vali Loss: 0.9844171 Test Loss: 0.4471355
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3331657
	speed: 0.0805s/iter; left time: 1826.3818s
	iters: 200, epoch: 14 | loss: 0.3607243
	speed: 0.0171s/iter; left time: 387.2265s
Epoch: 14 cost time: 5.027325630187988
Epoch: 14, Steps: 262 | Train Loss: 0.3600549 Vali Loss: 0.9842342 Test Loss: 0.4468379
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3330441
	speed: 0.0803s/iter; left time: 1801.5893s
	iters: 200, epoch: 15 | loss: 0.3388051
	speed: 0.0171s/iter; left time: 381.9793s
Epoch: 15 cost time: 4.989975452423096
Epoch: 15, Steps: 262 | Train Loss: 0.3598634 Vali Loss: 0.9837878 Test Loss: 0.4469496
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3755518
	speed: 0.0796s/iter; left time: 1765.4312s
	iters: 200, epoch: 16 | loss: 0.3530205
	speed: 0.0172s/iter; left time: 379.6246s
Epoch: 16 cost time: 5.058599948883057
Epoch: 16, Steps: 262 | Train Loss: 0.3598374 Vali Loss: 0.9832367 Test Loss: 0.4470640
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3453140
	speed: 0.0790s/iter; left time: 1730.4187s
	iters: 200, epoch: 17 | loss: 0.3444533
	speed: 0.0171s/iter; left time: 373.5607s
Epoch: 17 cost time: 4.919017791748047
Epoch: 17, Steps: 262 | Train Loss: 0.3596332 Vali Loss: 0.9824564 Test Loss: 0.4468272
Validation loss decreased (0.983164 --> 0.982456).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3551394
	speed: 0.0810s/iter; left time: 1752.4570s
	iters: 200, epoch: 18 | loss: 0.3648983
	speed: 0.0159s/iter; left time: 341.9704s
Epoch: 18 cost time: 4.936178684234619
Epoch: 18, Steps: 262 | Train Loss: 0.3598532 Vali Loss: 0.9835290 Test Loss: 0.4469775
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3839526
	speed: 0.0760s/iter; left time: 1625.2734s
	iters: 200, epoch: 19 | loss: 0.3744371
	speed: 0.0161s/iter; left time: 342.4373s
Epoch: 19 cost time: 4.874313116073608
Epoch: 19, Steps: 262 | Train Loss: 0.3598932 Vali Loss: 0.9838322 Test Loss: 0.4470058
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3832458
	speed: 0.0755s/iter; left time: 1594.2401s
	iters: 200, epoch: 20 | loss: 0.3851962
	speed: 0.0158s/iter; left time: 331.6603s
Epoch: 20 cost time: 4.731716632843018
Epoch: 20, Steps: 262 | Train Loss: 0.3599540 Vali Loss: 0.9836800 Test Loss: 0.4469281
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3841128
	speed: 0.0809s/iter; left time: 1688.3927s
	iters: 200, epoch: 21 | loss: 0.3479864
	speed: 0.0156s/iter; left time: 324.6184s
Epoch: 21 cost time: 4.7079644203186035
Epoch: 21, Steps: 262 | Train Loss: 0.3598042 Vali Loss: 0.9830022 Test Loss: 0.4469453
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3384863
	speed: 0.0752s/iter; left time: 1550.0419s
	iters: 200, epoch: 22 | loss: 0.3464956
	speed: 0.0161s/iter; left time: 330.6526s
Epoch: 22 cost time: 4.662010431289673
Epoch: 22, Steps: 262 | Train Loss: 0.3599059 Vali Loss: 0.9837698 Test Loss: 0.4469745
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3242253
	speed: 0.0786s/iter; left time: 1599.4542s
	iters: 200, epoch: 23 | loss: 0.3921937
	speed: 0.0159s/iter; left time: 321.2468s
Epoch: 23 cost time: 4.81703519821167
Epoch: 23, Steps: 262 | Train Loss: 0.3598438 Vali Loss: 0.9839337 Test Loss: 0.4470117
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3393281
	speed: 0.0767s/iter; left time: 1540.3961s
	iters: 200, epoch: 24 | loss: 0.3694725
	speed: 0.0162s/iter; left time: 322.9741s
Epoch: 24 cost time: 4.687419652938843
Epoch: 24, Steps: 262 | Train Loss: 0.3597900 Vali Loss: 0.9843391 Test Loss: 0.4470082
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3626962
	speed: 0.0762s/iter; left time: 1510.1238s
	iters: 200, epoch: 25 | loss: 0.3671914
	speed: 0.0158s/iter; left time: 312.0577s
Epoch: 25 cost time: 4.696852445602417
Epoch: 25, Steps: 262 | Train Loss: 0.3597569 Vali Loss: 0.9836377 Test Loss: 0.4470888
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3658541
	speed: 0.0739s/iter; left time: 1444.6561s
	iters: 200, epoch: 26 | loss: 0.3947834
	speed: 0.0157s/iter; left time: 305.1894s
Epoch: 26 cost time: 4.657666444778442
Epoch: 26, Steps: 262 | Train Loss: 0.3597219 Vali Loss: 0.9837093 Test Loss: 0.4471537
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3636250
	speed: 0.0758s/iter; left time: 1462.3074s
	iters: 200, epoch: 27 | loss: 0.3554075
	speed: 0.0159s/iter; left time: 305.5592s
Epoch: 27 cost time: 4.8795835971832275
Epoch: 27, Steps: 262 | Train Loss: 0.3597698 Vali Loss: 0.9839972 Test Loss: 0.4470882
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3297714
	speed: 0.0823s/iter; left time: 1566.2268s
	iters: 200, epoch: 28 | loss: 0.3813439
	speed: 0.0173s/iter; left time: 328.2960s
Epoch: 28 cost time: 5.1411402225494385
Epoch: 28, Steps: 262 | Train Loss: 0.3597910 Vali Loss: 0.9829800 Test Loss: 0.4470058
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3821965
	speed: 0.0798s/iter; left time: 1497.1707s
	iters: 200, epoch: 29 | loss: 0.3566156
	speed: 0.0157s/iter; left time: 292.9434s
Epoch: 29 cost time: 4.820735931396484
Epoch: 29, Steps: 262 | Train Loss: 0.3596119 Vali Loss: 0.9842011 Test Loss: 0.4470440
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3979001
	speed: 0.0761s/iter; left time: 1408.3711s
	iters: 200, epoch: 30 | loss: 0.3666438
	speed: 0.0158s/iter; left time: 290.2582s
Epoch: 30 cost time: 4.749364137649536
Epoch: 30, Steps: 262 | Train Loss: 0.3598661 Vali Loss: 0.9835346 Test Loss: 0.4470500
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3951200
	speed: 0.0767s/iter; left time: 1399.2463s
	iters: 200, epoch: 31 | loss: 0.3584059
	speed: 0.0337s/iter; left time: 611.8933s
Epoch: 31 cost time: 7.024651765823364
Epoch: 31, Steps: 262 | Train Loss: 0.3597402 Vali Loss: 0.9837618 Test Loss: 0.4470509
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3251158
	speed: 0.1289s/iter; left time: 2317.0767s
	iters: 200, epoch: 32 | loss: 0.3539571
	speed: 0.0160s/iter; left time: 285.8753s
Epoch: 32 cost time: 4.753559827804565
Epoch: 32, Steps: 262 | Train Loss: 0.3598429 Vali Loss: 0.9836038 Test Loss: 0.4470428
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3640101
	speed: 0.0765s/iter; left time: 1356.0582s
	iters: 200, epoch: 33 | loss: 0.3567838
	speed: 0.0204s/iter; left time: 359.1925s
Epoch: 33 cost time: 6.612748861312866
Epoch: 33, Steps: 262 | Train Loss: 0.3598142 Vali Loss: 0.9839115 Test Loss: 0.4470766
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3806899
	speed: 0.1331s/iter; left time: 2323.3809s
	iters: 200, epoch: 34 | loss: 0.3391103
	speed: 0.0160s/iter; left time: 277.8527s
Epoch: 34 cost time: 4.82544732093811
Epoch: 34, Steps: 262 | Train Loss: 0.3596919 Vali Loss: 0.9835045 Test Loss: 0.4471327
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3618569
	speed: 0.0767s/iter; left time: 1317.8466s
	iters: 200, epoch: 35 | loss: 0.3429640
	speed: 0.0161s/iter; left time: 275.7307s
Epoch: 35 cost time: 4.798236608505249
Epoch: 35, Steps: 262 | Train Loss: 0.3596960 Vali Loss: 0.9837891 Test Loss: 0.4471298
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4072684
	speed: 0.0752s/iter; left time: 1273.3517s
	iters: 200, epoch: 36 | loss: 0.3877733
	speed: 0.0162s/iter; left time: 272.2382s
Epoch: 36 cost time: 4.772387742996216
Epoch: 36, Steps: 262 | Train Loss: 0.3597902 Vali Loss: 0.9828689 Test Loss: 0.4471486
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3707524
	speed: 0.0758s/iter; left time: 1263.7182s
	iters: 200, epoch: 37 | loss: 0.3516660
	speed: 0.0158s/iter; left time: 262.0462s
Epoch: 37 cost time: 4.7075653076171875
Epoch: 37, Steps: 262 | Train Loss: 0.3596746 Vali Loss: 0.9833808 Test Loss: 0.4471627
EarlyStopping counter: 20 out of 20
Early stopping
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=22, out_features=110, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2168320.0
params:  2530.0
Trainable parameters:  2530
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4186463
	speed: 0.0623s/iter; left time: 1626.4626s
	iters: 200, epoch: 1 | loss: 0.4284253
	speed: 0.0232s/iter; left time: 603.3728s
Epoch: 1 cost time: 9.594855070114136
Epoch: 1, Steps: 262 | Train Loss: 0.4437545 Vali Loss: 0.9811974 Test Loss: 0.4455644
Validation loss decreased (inf --> 0.981197).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4118389
	speed: 0.0787s/iter; left time: 2032.6319s
	iters: 200, epoch: 2 | loss: 0.4280255
	speed: 0.0163s/iter; left time: 419.9428s
Epoch: 2 cost time: 4.925309181213379
Epoch: 2, Steps: 262 | Train Loss: 0.4433943 Vali Loss: 0.9813208 Test Loss: 0.4453884
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4212726
	speed: 0.0787s/iter; left time: 2013.2629s
	iters: 200, epoch: 3 | loss: 0.5042724
	speed: 0.0169s/iter; left time: 429.3720s
Epoch: 3 cost time: 5.022761106491089
Epoch: 3, Steps: 262 | Train Loss: 0.4432975 Vali Loss: 0.9806912 Test Loss: 0.4454640
Validation loss decreased (0.981197 --> 0.980691).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5092775
	speed: 0.0800s/iter; left time: 2025.1675s
	iters: 200, epoch: 4 | loss: 0.4104660
	speed: 0.0171s/iter; left time: 431.4114s
Epoch: 4 cost time: 5.060311794281006
Epoch: 4, Steps: 262 | Train Loss: 0.4430978 Vali Loss: 0.9815153 Test Loss: 0.4453444
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4674558
	speed: 0.1390s/iter; left time: 3483.5671s
	iters: 200, epoch: 5 | loss: 0.4462835
	speed: 0.0168s/iter; left time: 419.6803s
Epoch: 5 cost time: 8.760765314102173
Epoch: 5, Steps: 262 | Train Loss: 0.4430228 Vali Loss: 0.9804605 Test Loss: 0.4453718
Validation loss decreased (0.980691 --> 0.980460).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4312810
	speed: 0.0791s/iter; left time: 1961.1422s
	iters: 200, epoch: 6 | loss: 0.4080724
	speed: 0.0165s/iter; left time: 408.5179s
Epoch: 6 cost time: 4.884592294692993
Epoch: 6, Steps: 262 | Train Loss: 0.4430847 Vali Loss: 0.9808539 Test Loss: 0.4454478
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4110472
	speed: 0.0750s/iter; left time: 1838.9508s
	iters: 200, epoch: 7 | loss: 0.4070489
	speed: 0.0159s/iter; left time: 388.4300s
Epoch: 7 cost time: 4.7283806800842285
Epoch: 7, Steps: 262 | Train Loss: 0.4429520 Vali Loss: 0.9806739 Test Loss: 0.4458124
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4636368
	speed: 0.0772s/iter; left time: 1874.1151s
	iters: 200, epoch: 8 | loss: 0.4644924
	speed: 0.0159s/iter; left time: 384.0061s
Epoch: 8 cost time: 4.822126626968384
Epoch: 8, Steps: 262 | Train Loss: 0.4428719 Vali Loss: 0.9801645 Test Loss: 0.4453854
Validation loss decreased (0.980460 --> 0.980164).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4230120
	speed: 0.0749s/iter; left time: 1797.0037s
	iters: 200, epoch: 9 | loss: 0.4473236
	speed: 0.0173s/iter; left time: 412.6348s
Epoch: 9 cost time: 5.121027946472168
Epoch: 9, Steps: 262 | Train Loss: 0.4429732 Vali Loss: 0.9813617 Test Loss: 0.4455281
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4381654
	speed: 0.0806s/iter; left time: 1913.9529s
	iters: 200, epoch: 10 | loss: 0.4241557
	speed: 0.0160s/iter; left time: 379.3900s
Epoch: 10 cost time: 5.007477760314941
Epoch: 10, Steps: 262 | Train Loss: 0.4430930 Vali Loss: 0.9802900 Test Loss: 0.4453826
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4650890
	speed: 0.0774s/iter; left time: 1817.4087s
	iters: 200, epoch: 11 | loss: 0.4534303
	speed: 0.0169s/iter; left time: 394.9565s
Epoch: 11 cost time: 5.048135280609131
Epoch: 11, Steps: 262 | Train Loss: 0.4431444 Vali Loss: 0.9810153 Test Loss: 0.4456381
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4122901
	speed: 0.0780s/iter; left time: 1811.7792s
	iters: 200, epoch: 12 | loss: 0.4712772
	speed: 0.0159s/iter; left time: 368.3297s
Epoch: 12 cost time: 4.818498373031616
Epoch: 12, Steps: 262 | Train Loss: 0.4430228 Vali Loss: 0.9810630 Test Loss: 0.4455698
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4300528
	speed: 0.0755s/iter; left time: 1732.9967s
	iters: 200, epoch: 13 | loss: 0.3955756
	speed: 0.0157s/iter; left time: 358.8523s
Epoch: 13 cost time: 4.7308454513549805
Epoch: 13, Steps: 262 | Train Loss: 0.4430828 Vali Loss: 0.9808064 Test Loss: 0.4456370
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4284405
	speed: 0.0776s/iter; left time: 1762.0408s
	iters: 200, epoch: 14 | loss: 0.4651093
	speed: 0.0157s/iter; left time: 355.4547s
Epoch: 14 cost time: 4.661053419113159
Epoch: 14, Steps: 262 | Train Loss: 0.4429741 Vali Loss: 0.9802526 Test Loss: 0.4455808
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4563654
	speed: 0.0740s/iter; left time: 1659.1218s
	iters: 200, epoch: 15 | loss: 0.4424838
	speed: 0.0160s/iter; left time: 358.1434s
Epoch: 15 cost time: 4.704477310180664
Epoch: 15, Steps: 262 | Train Loss: 0.4428870 Vali Loss: 0.9810039 Test Loss: 0.4456742
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4564228
	speed: 0.0769s/iter; left time: 1704.9193s
	iters: 200, epoch: 16 | loss: 0.4821187
	speed: 0.0161s/iter; left time: 355.0929s
Epoch: 16 cost time: 4.783883571624756
Epoch: 16, Steps: 262 | Train Loss: 0.4427634 Vali Loss: 0.9807349 Test Loss: 0.4455800
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4220751
	speed: 0.0750s/iter; left time: 1642.2825s
	iters: 200, epoch: 17 | loss: 0.4902193
	speed: 0.0159s/iter; left time: 347.1251s
Epoch: 17 cost time: 4.685186862945557
Epoch: 17, Steps: 262 | Train Loss: 0.4430223 Vali Loss: 0.9798676 Test Loss: 0.4457497
Validation loss decreased (0.980164 --> 0.979868).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4689361
	speed: 0.0757s/iter; left time: 1637.8554s
	iters: 200, epoch: 18 | loss: 0.4574468
	speed: 0.0161s/iter; left time: 347.2118s
Epoch: 18 cost time: 4.76713228225708
Epoch: 18, Steps: 262 | Train Loss: 0.4428860 Vali Loss: 0.9807020 Test Loss: 0.4456710
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4653614
	speed: 0.0795s/iter; left time: 1699.8455s
	iters: 200, epoch: 19 | loss: 0.4026321
	speed: 0.0403s/iter; left time: 858.2879s
Epoch: 19 cost time: 8.236522197723389
Epoch: 19, Steps: 262 | Train Loss: 0.4430289 Vali Loss: 0.9804714 Test Loss: 0.4457053
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4483995
	speed: 0.0888s/iter; left time: 1875.1347s
	iters: 200, epoch: 20 | loss: 0.4358560
	speed: 0.0164s/iter; left time: 344.9155s
Epoch: 20 cost time: 5.013851165771484
Epoch: 20, Steps: 262 | Train Loss: 0.4429745 Vali Loss: 0.9810045 Test Loss: 0.4456781
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4433408
	speed: 0.0798s/iter; left time: 1665.6777s
	iters: 200, epoch: 21 | loss: 0.4544284
	speed: 0.0167s/iter; left time: 347.3003s
Epoch: 21 cost time: 4.8933424949646
Epoch: 21, Steps: 262 | Train Loss: 0.4427677 Vali Loss: 0.9807132 Test Loss: 0.4455461
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4742215
	speed: 0.0760s/iter; left time: 1565.9954s
	iters: 200, epoch: 22 | loss: 0.4215631
	speed: 0.0164s/iter; left time: 336.0030s
Epoch: 22 cost time: 4.800679922103882
Epoch: 22, Steps: 262 | Train Loss: 0.4429876 Vali Loss: 0.9803049 Test Loss: 0.4457249
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4494472
	speed: 0.0782s/iter; left time: 1591.1986s
	iters: 200, epoch: 23 | loss: 0.4235227
	speed: 0.0171s/iter; left time: 345.7047s
Epoch: 23 cost time: 4.928670644760132
Epoch: 23, Steps: 262 | Train Loss: 0.4427802 Vali Loss: 0.9803190 Test Loss: 0.4456750
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4210109
	speed: 0.0811s/iter; left time: 1627.9840s
	iters: 200, epoch: 24 | loss: 0.4219308
	speed: 0.0172s/iter; left time: 344.2969s
Epoch: 24 cost time: 4.993689060211182
Epoch: 24, Steps: 262 | Train Loss: 0.4429297 Vali Loss: 0.9818127 Test Loss: 0.4456213
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4518608
	speed: 0.0755s/iter; left time: 1495.5559s
	iters: 200, epoch: 25 | loss: 0.4362403
	speed: 0.0158s/iter; left time: 312.2130s
Epoch: 25 cost time: 4.790702819824219
Epoch: 25, Steps: 262 | Train Loss: 0.4430393 Vali Loss: 0.9808942 Test Loss: 0.4456405
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4011782
	speed: 0.0754s/iter; left time: 1474.3141s
	iters: 200, epoch: 26 | loss: 0.4692076
	speed: 0.0159s/iter; left time: 308.4526s
Epoch: 26 cost time: 4.7604148387908936
Epoch: 26, Steps: 262 | Train Loss: 0.4430163 Vali Loss: 0.9815173 Test Loss: 0.4456659
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4071623
	speed: 0.0753s/iter; left time: 1451.9083s
	iters: 200, epoch: 27 | loss: 0.4610162
	speed: 0.0156s/iter; left time: 298.7852s
Epoch: 27 cost time: 4.7253265380859375
Epoch: 27, Steps: 262 | Train Loss: 0.4429284 Vali Loss: 0.9817650 Test Loss: 0.4456683
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4836759
	speed: 0.0768s/iter; left time: 1461.4143s
	iters: 200, epoch: 28 | loss: 0.4343337
	speed: 0.0164s/iter; left time: 311.1218s
Epoch: 28 cost time: 4.953465223312378
Epoch: 28, Steps: 262 | Train Loss: 0.4428596 Vali Loss: 0.9807708 Test Loss: 0.4456878
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4504624
	speed: 0.0823s/iter; left time: 1543.7422s
	iters: 200, epoch: 29 | loss: 0.4277602
	speed: 0.0163s/iter; left time: 303.9667s
Epoch: 29 cost time: 4.9652910232543945
Epoch: 29, Steps: 262 | Train Loss: 0.4429932 Vali Loss: 0.9810433 Test Loss: 0.4457709
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4461541
	speed: 0.0824s/iter; left time: 1525.4277s
	iters: 200, epoch: 30 | loss: 0.4466759
	speed: 0.0183s/iter; left time: 335.9427s
Epoch: 30 cost time: 5.305119276046753
Epoch: 30, Steps: 262 | Train Loss: 0.4429245 Vali Loss: 0.9811160 Test Loss: 0.4457541
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4362414
	speed: 0.0809s/iter; left time: 1476.0805s
	iters: 200, epoch: 31 | loss: 0.4217616
	speed: 0.0184s/iter; left time: 334.2045s
Epoch: 31 cost time: 5.230614900588989
Epoch: 31, Steps: 262 | Train Loss: 0.4428238 Vali Loss: 0.9804109 Test Loss: 0.4456330
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4689392
	speed: 0.0841s/iter; left time: 1512.3149s
	iters: 200, epoch: 32 | loss: 0.4318307
	speed: 0.0178s/iter; left time: 318.2701s
Epoch: 32 cost time: 5.1576316356658936
Epoch: 32, Steps: 262 | Train Loss: 0.4429186 Vali Loss: 0.9807916 Test Loss: 0.4457193
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4301562
	speed: 0.0833s/iter; left time: 1475.0932s
	iters: 200, epoch: 33 | loss: 0.4265252
	speed: 0.0178s/iter; left time: 313.3158s
Epoch: 33 cost time: 5.478567838668823
Epoch: 33, Steps: 262 | Train Loss: 0.4429841 Vali Loss: 0.9800344 Test Loss: 0.4456664
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4237834
	speed: 0.0790s/iter; left time: 1378.5312s
	iters: 200, epoch: 34 | loss: 0.4357417
	speed: 0.0169s/iter; left time: 293.3658s
Epoch: 34 cost time: 5.14412784576416
Epoch: 34, Steps: 262 | Train Loss: 0.4428969 Vali Loss: 0.9812189 Test Loss: 0.4456412
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5190541
	speed: 0.0840s/iter; left time: 1443.5787s
	iters: 200, epoch: 35 | loss: 0.4565587
	speed: 0.0171s/iter; left time: 292.0113s
Epoch: 35 cost time: 5.064025402069092
Epoch: 35, Steps: 262 | Train Loss: 0.4428804 Vali Loss: 0.9811091 Test Loss: 0.4456602
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4692236
	speed: 0.0763s/iter; left time: 1292.5489s
	iters: 200, epoch: 36 | loss: 0.4370732
	speed: 0.0179s/iter; left time: 300.6318s
Epoch: 36 cost time: 5.0612921714782715
Epoch: 36, Steps: 262 | Train Loss: 0.4429755 Vali Loss: 0.9807916 Test Loss: 0.4457229
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4346464
	speed: 0.0800s/iter; left time: 1334.1987s
	iters: 200, epoch: 37 | loss: 0.4336962
	speed: 0.0166s/iter; left time: 274.5477s
Epoch: 37 cost time: 4.942556142807007
Epoch: 37, Steps: 262 | Train Loss: 0.4428499 Vali Loss: 0.9812884 Test Loss: 0.4457105
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4439106285572052, mae:0.4250167906284332, rse:0.6338967084884644, corr:[0.5286434  0.532749   0.5326188  0.5302429  0.5278598  0.52683276
 0.52697355 0.5276588  0.52799714 0.52796406 0.52785695 0.5274876
 0.52689236 0.52556723 0.5235081  0.5212188  0.5190752  0.5172051
 0.515497   0.5137663  0.5119119  0.5098394  0.5075693  0.50522095
 0.50284    0.5005429  0.4985694  0.49704066 0.49609914 0.49576163
 0.49605295 0.49663416 0.49704105 0.4971212  0.49687248 0.4965157
 0.4961382  0.49579597 0.49559814 0.49540314 0.49522576 0.49497253
 0.4947043  0.4945751  0.49462155 0.49482784 0.49514338 0.4954514
 0.49565515 0.49567264 0.49560848 0.49560222 0.49574122 0.49596214
 0.49617815 0.49626806 0.49628535 0.49612913 0.49586833 0.49552372
 0.49525264 0.49513775 0.49513373 0.4951968  0.4952614  0.49539253
 0.49554327 0.495654   0.49583125 0.496072   0.4963962  0.49673125
 0.49700716 0.4971604  0.49725685 0.49721652 0.49712053 0.49699327
 0.49685794 0.49673012 0.4966709  0.49666777 0.49671367 0.49667734
 0.49656394 0.49642387 0.4962898  0.49607006 0.49573675 0.49545878
 0.49526945 0.49516356 0.49507415 0.49492896 0.49465093 0.49406067
 0.4931914  0.492213   0.4912356  0.49043238 0.48998535 0.4900121
 0.49043864 0.4911225  0.49197882 0.49295855 0.49399585 0.49500635
 0.49584833 0.4963634  0.49649635 0.49651125 0.49644107 0.4963622
 0.49623287 0.49609774 0.49584156 0.4954195  0.49491763 0.49438754
 0.4939285  0.49340522 0.4927573  0.49208972 0.49140745 0.4907577
 0.49028128 0.49006736 0.4900595  0.490145   0.49017188 0.49001873
 0.48968017 0.48932764 0.48898646 0.48860705 0.48832294 0.48807025
 0.4878024  0.48760006 0.4874643  0.48742938 0.4875087  0.48769957
 0.4879186  0.48811984 0.48815864 0.48812708 0.48822367 0.48827595
 0.48831204 0.4883731  0.4883956  0.48839042 0.48823732 0.48805377
 0.4878738  0.48779026 0.48773915 0.48777696 0.4878778  0.48804957
 0.48829132 0.48858458 0.48886046 0.48919928 0.489542   0.48984995
 0.49014118 0.49034476 0.4903958  0.49043265 0.4904155  0.4903193
 0.49021122 0.49008372 0.4900013  0.489999   0.4899878  0.48999056
 0.48998016 0.48993763 0.48982307 0.48966375 0.48949963 0.48936558
 0.48929802 0.48932612 0.48946142 0.48970607 0.4899506  0.49010843
 0.49010715 0.49002078 0.4898119  0.48925212 0.48845744 0.48770335
 0.4870707  0.48653543 0.48608288 0.48599398 0.48604044 0.48604307
 0.4859079  0.4853992  0.48469788 0.48405123 0.48348758 0.4830112
 0.48253468 0.48202533 0.48139396 0.48056978 0.47959027 0.4786077
 0.47772285 0.4769123  0.4762002  0.47562617 0.4751591  0.4748316
 0.4746869  0.4747455  0.4748593  0.47495735 0.4749989  0.4748684
 0.47452268 0.47413328 0.4737155  0.47329283 0.47294715 0.47273517
 0.47267222 0.47275633 0.47281155 0.47290477 0.4730196  0.47307497
 0.4731658  0.4731735  0.47318846 0.4731948  0.47321144 0.47325397
 0.47333393 0.47336203 0.47338325 0.47341213 0.47341403 0.473379
 0.4733325  0.47336632 0.47336832 0.47336006 0.47328827 0.4731951
 0.4732555  0.4735165  0.47392386 0.47441623 0.47487724 0.47529638
 0.4756861  0.47594044 0.47610033 0.47624138 0.47640532 0.476505
 0.47659075 0.47661418 0.47667468 0.47676116 0.47692007 0.47711813
 0.4773405  0.47765177 0.47794208 0.4781492  0.47827262 0.4782876
 0.47827727 0.4781828  0.47805583 0.47787464 0.47755054 0.4770618
 0.4763102  0.47546375 0.47469798 0.473971   0.47338268 0.47297478
 0.47290343 0.4730427  0.4734186  0.47395194 0.47447762 0.47494453
 0.4752529  0.47526774 0.4751601  0.4750519  0.47497606 0.47487122
 0.47475496 0.47456768 0.4742596  0.4738812  0.47349548 0.47316104
 0.4728808  0.47256976 0.47227573 0.47210124 0.47186655 0.4716706
 0.47154105 0.4713747  0.47117057 0.47102806 0.4708256  0.47055805
 0.47021106 0.46981987 0.46939415 0.46900377 0.4686564  0.46840563
 0.46826467 0.46823815 0.46819592 0.46809894 0.46799743 0.46791556
 0.46790108 0.46794865 0.46800575 0.46805322 0.4681635  0.46822685
 0.4682137  0.4681487  0.46812358 0.46814844 0.4681585  0.46819296
 0.46820918 0.46827978 0.46840727 0.4684722  0.46846747 0.46840817
 0.46834263 0.4683909  0.4684792  0.46863243 0.46880645 0.46899346
 0.46911293 0.46923453 0.46930844 0.46937713 0.4694462  0.469478
 0.46944162 0.46942008 0.46945125 0.46951026 0.46965575 0.46984476
 0.4701047  0.47041908 0.47077474 0.47106478 0.47125342 0.47137308
 0.47144377 0.47148436 0.47156993 0.4716092  0.47158885 0.47150135
 0.47132227 0.4709915  0.47072658 0.47038922 0.46995    0.4696481
 0.469571   0.46977255 0.47012028 0.47076458 0.47155088 0.47235128
 0.4729174  0.47313255 0.47301054 0.47286704 0.47272375 0.4726319
 0.47257563 0.47245    0.47219387 0.47181305 0.4713119  0.4707712
 0.4702805  0.4698117  0.46941003 0.4691203  0.4689419  0.4688318
 0.46884027 0.46883807 0.4688316  0.46875492 0.46853083 0.46826538
 0.4678762  0.4674799  0.46707675 0.4666904  0.46631366 0.46602336
 0.4657659  0.46563834 0.4656043  0.4656821  0.46579048 0.46597198
 0.4660728  0.46610218 0.46606195 0.4660145  0.46590424 0.46585703
 0.46579012 0.46573612 0.46577242 0.465902   0.4660686  0.46624714
 0.46639922 0.4665362  0.46660075 0.46666643 0.46670294 0.46671528
 0.4668123  0.4670295  0.4672871  0.4675415  0.4676755  0.46781126
 0.46786952 0.46780252 0.46763507 0.46748137 0.46731648 0.4671863
 0.4671359  0.4671701  0.46723306 0.46735573 0.46750614 0.46761534
 0.467706   0.4678071  0.46794668 0.4680117  0.46802813 0.4679931
 0.46783862 0.46758762 0.46723107 0.46672088 0.46602938 0.4651551
 0.46411884 0.463026   0.46198586 0.46104124 0.4601687  0.45947784
 0.4590149  0.45858085 0.45838594 0.45835653 0.45837837 0.45832548
 0.45825583 0.45800385 0.4575415  0.4570489  0.45666382 0.4563721
 0.45614105 0.45593485 0.45568788 0.45534343 0.4549733  0.45456317
 0.45419756 0.45385763 0.45352894 0.45327532 0.45303386 0.4528527
 0.4527298  0.45267192 0.4526414  0.45268196 0.45270103 0.45261663
 0.4523675  0.45204496 0.45167437 0.45121616 0.45074135 0.450377
 0.45010838 0.44996428 0.44990343 0.449877   0.44993246 0.45010883
 0.45027182 0.4503599  0.45030078 0.45013946 0.45000172 0.44993278
 0.4499439  0.44998118 0.4501298  0.45027903 0.4503761  0.45039645
 0.45038703 0.450281   0.45018938 0.45011356 0.45007545 0.45010567
 0.45020404 0.4503596  0.45053318 0.4506524  0.45069948 0.4507767
 0.45086262 0.45091552 0.45096534 0.45109668 0.4512403  0.451366
 0.4514925  0.45163298 0.4517303  0.45182738 0.4519159  0.45203987
 0.45213693 0.45233753 0.45251873 0.4526522  0.4527021  0.45267522
 0.45254508 0.4522385  0.45178863 0.45115268 0.4502689  0.44915405
 0.44783932 0.4465373  0.4454486  0.44446757 0.44364226 0.44304836
 0.4426996  0.44258934 0.44274357 0.44306982 0.4435569  0.44405916
 0.44446844 0.44461572 0.4445078  0.44430155 0.44406116 0.44381228
 0.4435285  0.44324192 0.44298244 0.44270742 0.4424268  0.44207874
 0.44175994 0.44143397 0.44109276 0.44086283 0.44069597 0.44056097
 0.44045222 0.44038638 0.4403065  0.4401939  0.440073   0.43999168
 0.43985617 0.43962273 0.43940222 0.43915835 0.4389251  0.43867195
 0.43850198 0.43842968 0.43843475 0.43850395 0.43866947 0.43892595
 0.43916723 0.43932548 0.43934965 0.43921983 0.43904364 0.43890688
 0.4388211  0.43882343 0.4388939  0.43897215 0.43903658 0.4389218
 0.43861488 0.43829376 0.43802223 0.4378808  0.43798944 0.43822283
 0.4385547  0.43893895 0.43930054 0.4395996  0.43981266 0.44001538
 0.44021448 0.4403342  0.4404251  0.4405312  0.44059336 0.44061524
 0.4406095  0.4406433  0.44073546 0.44089305 0.4411264  0.4414126
 0.44170246 0.4420022  0.4422628  0.44235796 0.44232258 0.4421732
 0.44193947 0.44168803 0.4414223  0.4411468  0.44076946 0.44012547
 0.43913898 0.43798038 0.43683627 0.43583187 0.43516016 0.43485016
 0.43488765 0.43528265 0.43587196 0.43649364 0.4369938  0.43749142
 0.43792838 0.43803993 0.43794858 0.43776232 0.43751803 0.4372059
 0.436827   0.4363631  0.43586847 0.43538874 0.4349472  0.4346377
 0.43437418 0.43396604 0.4333977  0.43271908 0.43204743 0.43152544
 0.43128955 0.4312871  0.43143335 0.43155357 0.43136847 0.4308655
 0.43014848 0.42942297 0.42904285 0.42912495 0.42956427 0.43002376
 0.43021843 0.43011603 0.42996886 0.4304005  0.4318243  0.43357217]
