Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=34, out_features=102, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3107328.0
params:  3570.0
Trainable parameters:  3570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5862696
	speed: 0.0325s/iter; left time: 844.7972s
	iters: 200, epoch: 1 | loss: 0.4544614
	speed: 0.0207s/iter; left time: 536.7088s
Epoch: 1 cost time: 6.635034561157227
Epoch: 1, Steps: 261 | Train Loss: 0.5915589 Vali Loss: 1.0713370 Test Loss: 0.4806158
Validation loss decreased (inf --> 1.071337).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4721367
	speed: 0.1017s/iter; left time: 2618.9693s
	iters: 200, epoch: 2 | loss: 0.4419994
	speed: 0.0201s/iter; left time: 516.0033s
Epoch: 2 cost time: 5.762159585952759
Epoch: 2, Steps: 261 | Train Loss: 0.4421754 Vali Loss: 0.9986445 Test Loss: 0.4381335
Validation loss decreased (1.071337 --> 0.998645).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4188999
	speed: 0.0934s/iter; left time: 2379.2902s
	iters: 200, epoch: 3 | loss: 0.4170870
	speed: 0.0190s/iter; left time: 481.4660s
Epoch: 3 cost time: 5.553827524185181
Epoch: 3, Steps: 261 | Train Loss: 0.4252436 Vali Loss: 0.9812118 Test Loss: 0.4314746
Validation loss decreased (0.998645 --> 0.981212).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4352030
	speed: 0.0832s/iter; left time: 2098.2337s
	iters: 200, epoch: 4 | loss: 0.4147296
	speed: 0.0181s/iter; left time: 454.5222s
Epoch: 4 cost time: 5.199707269668579
Epoch: 4, Steps: 261 | Train Loss: 0.4195780 Vali Loss: 0.9736798 Test Loss: 0.4289400
Validation loss decreased (0.981212 --> 0.973680).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4178094
	speed: 0.0883s/iter; left time: 2204.7153s
	iters: 200, epoch: 5 | loss: 0.4412969
	speed: 0.0181s/iter; left time: 449.4487s
Epoch: 5 cost time: 5.790811777114868
Epoch: 5, Steps: 261 | Train Loss: 0.4172616 Vali Loss: 0.9687963 Test Loss: 0.4281431
Validation loss decreased (0.973680 --> 0.968796).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4221844
	speed: 0.0890s/iter; left time: 2196.7658s
	iters: 200, epoch: 6 | loss: 0.4269318
	speed: 0.0200s/iter; left time: 492.6658s
Epoch: 6 cost time: 5.810199975967407
Epoch: 6, Steps: 261 | Train Loss: 0.4160468 Vali Loss: 0.9665915 Test Loss: 0.4275709
Validation loss decreased (0.968796 --> 0.966592).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4175115
	speed: 0.0998s/iter; left time: 2439.2467s
	iters: 200, epoch: 7 | loss: 0.4421767
	speed: 0.0302s/iter; left time: 735.4421s
Epoch: 7 cost time: 6.69522500038147
Epoch: 7, Steps: 261 | Train Loss: 0.4154382 Vali Loss: 0.9648893 Test Loss: 0.4273095
Validation loss decreased (0.966592 --> 0.964889).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4008625
	speed: 0.0911s/iter; left time: 2201.9412s
	iters: 200, epoch: 8 | loss: 0.4034583
	speed: 0.0186s/iter; left time: 447.5802s
Epoch: 8 cost time: 5.634851455688477
Epoch: 8, Steps: 261 | Train Loss: 0.4150578 Vali Loss: 0.9652588 Test Loss: 0.4272006
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4415117
	speed: 0.0839s/iter; left time: 2006.3767s
	iters: 200, epoch: 9 | loss: 0.4032991
	speed: 0.0182s/iter; left time: 433.5422s
Epoch: 9 cost time: 5.396263360977173
Epoch: 9, Steps: 261 | Train Loss: 0.4149053 Vali Loss: 0.9632577 Test Loss: 0.4275312
Validation loss decreased (0.964889 --> 0.963258).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3987030
	speed: 0.0959s/iter; left time: 2268.6821s
	iters: 200, epoch: 10 | loss: 0.3962573
	speed: 0.0192s/iter; left time: 453.0534s
Epoch: 10 cost time: 5.471977472305298
Epoch: 10, Steps: 261 | Train Loss: 0.4148320 Vali Loss: 0.9625681 Test Loss: 0.4274150
Validation loss decreased (0.963258 --> 0.962568).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4296943
	speed: 0.0909s/iter; left time: 2125.9031s
	iters: 200, epoch: 11 | loss: 0.3976528
	speed: 0.0203s/iter; left time: 473.3900s
Epoch: 11 cost time: 5.808819770812988
Epoch: 11, Steps: 261 | Train Loss: 0.4146603 Vali Loss: 0.9623179 Test Loss: 0.4277428
Validation loss decreased (0.962568 --> 0.962318).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4089784
	speed: 0.0916s/iter; left time: 2119.4156s
	iters: 200, epoch: 12 | loss: 0.3980286
	speed: 0.0198s/iter; left time: 456.2541s
Epoch: 12 cost time: 5.604118585586548
Epoch: 12, Steps: 261 | Train Loss: 0.4146478 Vali Loss: 0.9628823 Test Loss: 0.4276741
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4064431
	speed: 0.0949s/iter; left time: 2169.1268s
	iters: 200, epoch: 13 | loss: 0.3917607
	speed: 0.0291s/iter; left time: 662.3999s
Epoch: 13 cost time: 6.955485105514526
Epoch: 13, Steps: 261 | Train Loss: 0.4146203 Vali Loss: 0.9622294 Test Loss: 0.4278549
Validation loss decreased (0.962318 --> 0.962229).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4057207
	speed: 0.0947s/iter; left time: 2141.9407s
	iters: 200, epoch: 14 | loss: 0.4133539
	speed: 0.0212s/iter; left time: 477.2108s
Epoch: 14 cost time: 5.931237459182739
Epoch: 14, Steps: 261 | Train Loss: 0.4145145 Vali Loss: 0.9626620 Test Loss: 0.4281264
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4027194
	speed: 0.0914s/iter; left time: 2042.0188s
	iters: 200, epoch: 15 | loss: 0.4064925
	speed: 0.0174s/iter; left time: 386.6342s
Epoch: 15 cost time: 5.424493074417114
Epoch: 15, Steps: 261 | Train Loss: 0.4145670 Vali Loss: 0.9631164 Test Loss: 0.4277362
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3994255
	speed: 0.0941s/iter; left time: 2079.3969s
	iters: 200, epoch: 16 | loss: 0.4047399
	speed: 0.0204s/iter; left time: 448.6855s
Epoch: 16 cost time: 5.784576177597046
Epoch: 16, Steps: 261 | Train Loss: 0.4143298 Vali Loss: 0.9621941 Test Loss: 0.4276339
Validation loss decreased (0.962229 --> 0.962194).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4278986
	speed: 0.0967s/iter; left time: 2109.4180s
	iters: 200, epoch: 17 | loss: 0.4338725
	speed: 0.0197s/iter; left time: 428.1182s
Epoch: 17 cost time: 5.742424488067627
Epoch: 17, Steps: 261 | Train Loss: 0.4144218 Vali Loss: 0.9625732 Test Loss: 0.4277633
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4214564
	speed: 0.0988s/iter; left time: 2129.9655s
	iters: 200, epoch: 18 | loss: 0.4153607
	speed: 0.0192s/iter; left time: 413.1222s
Epoch: 18 cost time: 5.695816993713379
Epoch: 18, Steps: 261 | Train Loss: 0.4143395 Vali Loss: 0.9626394 Test Loss: 0.4279027
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4182622
	speed: 0.0885s/iter; left time: 1886.1986s
	iters: 200, epoch: 19 | loss: 0.4009853
	speed: 0.0188s/iter; left time: 398.9841s
Epoch: 19 cost time: 5.523368835449219
Epoch: 19, Steps: 261 | Train Loss: 0.4143347 Vali Loss: 0.9620234 Test Loss: 0.4279016
Validation loss decreased (0.962194 --> 0.962023).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4127074
	speed: 0.0963s/iter; left time: 2026.8132s
	iters: 200, epoch: 20 | loss: 0.4141915
	speed: 0.0180s/iter; left time: 375.9563s
Epoch: 20 cost time: 6.321382522583008
Epoch: 20, Steps: 261 | Train Loss: 0.4143858 Vali Loss: 0.9610606 Test Loss: 0.4279949
Validation loss decreased (0.962023 --> 0.961061).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4046677
	speed: 0.1007s/iter; left time: 2092.2361s
	iters: 200, epoch: 21 | loss: 0.4353693
	speed: 0.0221s/iter; left time: 457.2968s
Epoch: 21 cost time: 5.946528673171997
Epoch: 21, Steps: 261 | Train Loss: 0.4142669 Vali Loss: 0.9623181 Test Loss: 0.4278985
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3805091
	speed: 0.0931s/iter; left time: 1909.4833s
	iters: 200, epoch: 22 | loss: 0.4173941
	speed: 0.0187s/iter; left time: 381.0967s
Epoch: 22 cost time: 5.923455476760864
Epoch: 22, Steps: 261 | Train Loss: 0.4142896 Vali Loss: 0.9611562 Test Loss: 0.4279698
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4214234
	speed: 0.0898s/iter; left time: 1818.5803s
	iters: 200, epoch: 23 | loss: 0.3902746
	speed: 0.0186s/iter; left time: 375.4458s
Epoch: 23 cost time: 5.213505029678345
Epoch: 23, Steps: 261 | Train Loss: 0.4143370 Vali Loss: 0.9616996 Test Loss: 0.4278535
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4063304
	speed: 0.0868s/iter; left time: 1736.2345s
	iters: 200, epoch: 24 | loss: 0.3873209
	speed: 0.0211s/iter; left time: 420.3930s
Epoch: 24 cost time: 5.652388334274292
Epoch: 24, Steps: 261 | Train Loss: 0.4142751 Vali Loss: 0.9620706 Test Loss: 0.4279308
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3993269
	speed: 0.0858s/iter; left time: 1693.8728s
	iters: 200, epoch: 25 | loss: 0.4230451
	speed: 0.0179s/iter; left time: 350.6716s
Epoch: 25 cost time: 5.221067905426025
Epoch: 25, Steps: 261 | Train Loss: 0.4143356 Vali Loss: 0.9608000 Test Loss: 0.4279005
Validation loss decreased (0.961061 --> 0.960800).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3887198
	speed: 0.0942s/iter; left time: 1834.9879s
	iters: 200, epoch: 26 | loss: 0.3915519
	speed: 0.0202s/iter; left time: 390.7350s
Epoch: 26 cost time: 5.902318716049194
Epoch: 26, Steps: 261 | Train Loss: 0.4142662 Vali Loss: 0.9612799 Test Loss: 0.4280457
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4025199
	speed: 0.0910s/iter; left time: 1747.8395s
	iters: 200, epoch: 27 | loss: 0.4268002
	speed: 0.0193s/iter; left time: 368.5055s
Epoch: 27 cost time: 5.799365282058716
Epoch: 27, Steps: 261 | Train Loss: 0.4142341 Vali Loss: 0.9601036 Test Loss: 0.4281142
Validation loss decreased (0.960800 --> 0.960104).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3953803
	speed: 0.1077s/iter; left time: 2041.0686s
	iters: 200, epoch: 28 | loss: 0.4213536
	speed: 0.0185s/iter; left time: 348.9524s
Epoch: 28 cost time: 5.898183822631836
Epoch: 28, Steps: 261 | Train Loss: 0.4142800 Vali Loss: 0.9612439 Test Loss: 0.4280255
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4054159
	speed: 0.0856s/iter; left time: 1599.4817s
	iters: 200, epoch: 29 | loss: 0.4029860
	speed: 0.0185s/iter; left time: 344.7123s
Epoch: 29 cost time: 5.375695705413818
Epoch: 29, Steps: 261 | Train Loss: 0.4141648 Vali Loss: 0.9623593 Test Loss: 0.4279871
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4440903
	speed: 0.0968s/iter; left time: 1783.4651s
	iters: 200, epoch: 30 | loss: 0.4322697
	speed: 0.0181s/iter; left time: 332.6808s
Epoch: 30 cost time: 5.648259878158569
Epoch: 30, Steps: 261 | Train Loss: 0.4142406 Vali Loss: 0.9614630 Test Loss: 0.4281295
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4226391
	speed: 0.0909s/iter; left time: 1652.1049s
	iters: 200, epoch: 31 | loss: 0.3941817
	speed: 0.0205s/iter; left time: 371.1274s
Epoch: 31 cost time: 5.704271554946899
Epoch: 31, Steps: 261 | Train Loss: 0.4142086 Vali Loss: 0.9614953 Test Loss: 0.4280883
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4164395
	speed: 0.1000s/iter; left time: 1790.6185s
	iters: 200, epoch: 32 | loss: 0.3925165
	speed: 0.0229s/iter; left time: 407.1306s
Epoch: 32 cost time: 6.048579216003418
Epoch: 32, Steps: 261 | Train Loss: 0.4142354 Vali Loss: 0.9602989 Test Loss: 0.4279847
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4564465
	speed: 0.0929s/iter; left time: 1638.8120s
	iters: 200, epoch: 33 | loss: 0.3934785
	speed: 0.0185s/iter; left time: 324.0887s
Epoch: 33 cost time: 5.5920350551605225
Epoch: 33, Steps: 261 | Train Loss: 0.4141945 Vali Loss: 0.9616849 Test Loss: 0.4281060
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4330908
	speed: 0.0938s/iter; left time: 1630.2041s
	iters: 200, epoch: 34 | loss: 0.4455361
	speed: 0.0240s/iter; left time: 415.0781s
Epoch: 34 cost time: 6.221532583236694
Epoch: 34, Steps: 261 | Train Loss: 0.4142345 Vali Loss: 0.9617369 Test Loss: 0.4280998
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4127416
	speed: 0.0843s/iter; left time: 1443.6011s
	iters: 200, epoch: 35 | loss: 0.3991756
	speed: 0.0196s/iter; left time: 333.2135s
Epoch: 35 cost time: 5.730298757553101
Epoch: 35, Steps: 261 | Train Loss: 0.4142529 Vali Loss: 0.9616032 Test Loss: 0.4279564
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4019265
	speed: 0.0917s/iter; left time: 1546.7405s
	iters: 200, epoch: 36 | loss: 0.4072127
	speed: 0.0183s/iter; left time: 307.4672s
Epoch: 36 cost time: 5.435000419616699
Epoch: 36, Steps: 261 | Train Loss: 0.4141984 Vali Loss: 0.9606985 Test Loss: 0.4279721
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4592120
	speed: 0.0877s/iter; left time: 1455.6111s
	iters: 200, epoch: 37 | loss: 0.4133103
	speed: 0.0193s/iter; left time: 318.8267s
Epoch: 37 cost time: 5.598987340927124
Epoch: 37, Steps: 261 | Train Loss: 0.4141955 Vali Loss: 0.9612153 Test Loss: 0.4279687
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3992203
	speed: 0.0900s/iter; left time: 1470.2574s
	iters: 200, epoch: 38 | loss: 0.4325091
	speed: 0.0210s/iter; left time: 340.6187s
Epoch: 38 cost time: 5.866032123565674
Epoch: 38, Steps: 261 | Train Loss: 0.4142762 Vali Loss: 0.9612384 Test Loss: 0.4280646
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4224576
	speed: 0.1001s/iter; left time: 1609.3160s
	iters: 200, epoch: 39 | loss: 0.4131294
	speed: 0.0215s/iter; left time: 343.8463s
Epoch: 39 cost time: 5.844023942947388
Epoch: 39, Steps: 261 | Train Loss: 0.4141677 Vali Loss: 0.9604629 Test Loss: 0.4280224
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4122787
	speed: 0.0896s/iter; left time: 1418.1594s
	iters: 200, epoch: 40 | loss: 0.3978994
	speed: 0.0178s/iter; left time: 279.3782s
Epoch: 40 cost time: 5.551728963851929
Epoch: 40, Steps: 261 | Train Loss: 0.4142384 Vali Loss: 0.9611913 Test Loss: 0.4281189
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4304421
	speed: 0.0889s/iter; left time: 1384.0489s
	iters: 200, epoch: 41 | loss: 0.4423523
	speed: 0.0196s/iter; left time: 302.9223s
Epoch: 41 cost time: 5.605266332626343
Epoch: 41, Steps: 261 | Train Loss: 0.4143029 Vali Loss: 0.9616079 Test Loss: 0.4281510
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4000791
	speed: 0.0873s/iter; left time: 1336.2940s
	iters: 200, epoch: 42 | loss: 0.3790261
	speed: 0.0186s/iter; left time: 282.5876s
Epoch: 42 cost time: 5.312201738357544
Epoch: 42, Steps: 261 | Train Loss: 0.4142080 Vali Loss: 0.9613237 Test Loss: 0.4279703
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4130632
	speed: 0.0974s/iter; left time: 1464.5229s
	iters: 200, epoch: 43 | loss: 0.3995379
	speed: 0.0210s/iter; left time: 313.6015s
Epoch: 43 cost time: 5.992999792098999
Epoch: 43, Steps: 261 | Train Loss: 0.4142331 Vali Loss: 0.9611349 Test Loss: 0.4279744
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3982091
	speed: 0.0894s/iter; left time: 1321.8608s
	iters: 200, epoch: 44 | loss: 0.4316564
	speed: 0.0182s/iter; left time: 266.4346s
Epoch: 44 cost time: 5.182382106781006
Epoch: 44, Steps: 261 | Train Loss: 0.4142195 Vali Loss: 0.9615928 Test Loss: 0.4280306
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.3767762
	speed: 0.0928s/iter; left time: 1346.7445s
	iters: 200, epoch: 45 | loss: 0.4151467
	speed: 0.0176s/iter; left time: 253.3489s
Epoch: 45 cost time: 5.871469736099243
Epoch: 45, Steps: 261 | Train Loss: 0.4143377 Vali Loss: 0.9620540 Test Loss: 0.4280350
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4363007
	speed: 0.0876s/iter; left time: 1248.6058s
	iters: 200, epoch: 46 | loss: 0.3787099
	speed: 0.0177s/iter; left time: 251.0826s
Epoch: 46 cost time: 5.375851392745972
Epoch: 46, Steps: 261 | Train Loss: 0.4142674 Vali Loss: 0.9608306 Test Loss: 0.4280229
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3867082
	speed: 0.0892s/iter; left time: 1247.7758s
	iters: 200, epoch: 47 | loss: 0.4060937
	speed: 0.0173s/iter; left time: 240.1487s
Epoch: 47 cost time: 5.548842668533325
Epoch: 47, Steps: 261 | Train Loss: 0.4141298 Vali Loss: 0.9614301 Test Loss: 0.4280427
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.42778655886650085, mae:0.4166116714477539, rse:0.6222777962684631, corr:[0.52640206 0.53065234 0.5327039  0.53288215 0.5321518  0.5315192
 0.5314794  0.5320423  0.53287834 0.5336703  0.5342862  0.534405
 0.5341009  0.53355324 0.53287965 0.5320899  0.5312748  0.53048444
 0.52950656 0.52831364 0.52686363 0.5254186  0.52394825 0.5226319
 0.5215075  0.52051586 0.51974225 0.519257   0.5189845  0.51870716
 0.5185349  0.5185491  0.5186455  0.5187611  0.51880795 0.5188511
 0.5188637  0.5187725  0.5185644  0.518219   0.51779634 0.5173252
 0.5168532  0.5165236  0.5163746  0.5164196  0.5166555  0.5168935
 0.5169407  0.5167841  0.5163979  0.5158582  0.51537836 0.5150557
 0.51495934 0.51510763 0.5153833  0.5156817  0.5159202  0.515928
 0.5157903  0.5154993  0.5151624  0.5148191  0.5146468  0.51473564
 0.515021   0.51538223 0.5158424  0.5162454  0.516534   0.5166688
 0.5167339  0.51679844 0.51685196 0.5168691  0.5168437  0.5167896
 0.51668733 0.516529   0.5163191  0.5160604  0.5157267  0.5152895
 0.51489013 0.51452273 0.5142943  0.5141796  0.51416796 0.5142984
 0.5145353  0.5147199  0.5147311  0.51453114 0.5141165  0.5135231
 0.5128966  0.51236296 0.51193565 0.51161987 0.5114555  0.5114318
 0.51149505 0.51151294 0.5115794  0.51174206 0.5119183  0.5122137
 0.5125141  0.5128777  0.51318455 0.51336724 0.5133341  0.51319927
 0.51299644 0.51281226 0.5126532  0.51255316 0.51248264 0.5125408
 0.5126371  0.5125996  0.51238424 0.5121291  0.51184773 0.5115232
 0.5112229  0.5110272  0.5109464  0.5109802  0.5110744  0.51114166
 0.5111434  0.5109907  0.51072216 0.5103475  0.5099243  0.5094683
 0.50908685 0.5088315  0.5087195  0.5087881  0.5089765  0.50921
 0.509372   0.50940555 0.5092789  0.50907326 0.5089156  0.50879425
 0.5087627  0.50889546 0.50909954 0.50929135 0.50941926 0.5094985
 0.5094963  0.50944394 0.5093497  0.5092979  0.50937825 0.50958544
 0.5099037  0.510271   0.5106715  0.511036   0.5112848  0.51146823
 0.5115789  0.5116522  0.5117588  0.511936   0.5121116  0.5122623
 0.51234853 0.51236576 0.51229745 0.5121612  0.51191735 0.51163274
 0.51133233 0.5111266  0.51096    0.5108807  0.5108709  0.51091063
 0.5109566  0.51098317 0.51095235 0.51085216 0.5106015  0.5102273
 0.5097858  0.50940305 0.5090652  0.50867283 0.50823426 0.5078922
 0.5075723  0.50719756 0.5068335  0.5064933  0.5061475  0.5058525
 0.50562644 0.50536263 0.50509727 0.5047479  0.50433165 0.503873
 0.50331265 0.5027231  0.50211364 0.50158876 0.50114083 0.5008393
 0.500661   0.50052446 0.50032926 0.50018346 0.5000231  0.49984646
 0.49975726 0.49973765 0.4997641  0.4998189  0.4999041  0.49994677
 0.49994445 0.4999127  0.499798   0.49957815 0.4993152  0.49900657
 0.4987597  0.49860144 0.4984927  0.4985127  0.49859628 0.4986782
 0.49867854 0.4985503  0.49835235 0.4981216  0.49790028 0.49778435
 0.49777892 0.4978586  0.4980003  0.49817687 0.49828985 0.49835336
 0.49829295 0.4982036  0.4980938  0.49800715 0.4979287  0.49795341
 0.49803996 0.49819756 0.49841136 0.49864405 0.4987932  0.49896425
 0.4991323  0.49932134 0.4995019  0.49973422 0.4999667  0.50010395
 0.5001798  0.50018716 0.50012124 0.50000775 0.49982563 0.49967438
 0.4995273  0.49947935 0.49939877 0.49934772 0.4992706  0.49915808
 0.4990643  0.49890876 0.49870086 0.49839953 0.4979832  0.497457
 0.49680835 0.49617648 0.49564397 0.49507114 0.4945273  0.49404398
 0.49362454 0.4931723  0.49278155 0.49241647 0.4921044  0.4918084
 0.49153015 0.49126524 0.49102774 0.4908267  0.49062124 0.49041292
 0.490246   0.4900701  0.48992693 0.48986384 0.48989952 0.48998678
 0.4901237  0.4902114  0.4901363  0.49003974 0.48990917 0.4897925
 0.48971638 0.4896813  0.48966905 0.48965916 0.4896256  0.4895418
 0.48941532 0.48925564 0.4890524  0.4888517  0.48861387 0.48838297
 0.48816198 0.48798057 0.48783255 0.48772043 0.48764884 0.48760447
 0.48753595 0.48743924 0.48733065 0.48717466 0.48706463 0.48705718
 0.4871136  0.48722228 0.48737556 0.487513   0.4876188  0.48769137
 0.4876593  0.4875916  0.48751086 0.4874287  0.48741275 0.48749164
 0.48763034 0.48780182 0.48798525 0.48812655 0.48819873 0.4882425
 0.4882437  0.4882649  0.4883134  0.4884109  0.4885307  0.48864043
 0.48869357 0.488693   0.48864982 0.48851892 0.4883593  0.4882132
 0.48813385 0.48812342 0.48819432 0.48833644 0.48850986 0.48869357
 0.4888775  0.48901436 0.48907885 0.48897326 0.48869872 0.48830953
 0.4878046  0.48730022 0.48686022 0.4864182  0.48595974 0.48560885
 0.48533672 0.48504764 0.48478565 0.48460755 0.48446056 0.48437336
 0.48428032 0.48424324 0.48422512 0.48418307 0.48406097 0.48387736
 0.48371616 0.4835952  0.48349464 0.4834151  0.48337469 0.48340145
 0.48347673 0.48345232 0.48334262 0.48324397 0.48314235 0.48303562
 0.48296365 0.48291662 0.48292515 0.48294675 0.4829608  0.4829336
 0.48282766 0.4826602  0.4823769  0.48204875 0.48169252 0.4813643
 0.4810782  0.4809225  0.48085117 0.4808764  0.48094577 0.48108217
 0.48108843 0.4809261  0.4806622  0.48035747 0.4800612  0.47987285
 0.47980535 0.47987142 0.48004007 0.48024282 0.48043936 0.48059997
 0.48061952 0.4805429  0.48038006 0.48021457 0.48011783 0.48013797
 0.48027536 0.48050568 0.4807458  0.48093376 0.4810251  0.48106086
 0.4809711  0.48088503 0.48083913 0.48091868 0.48108545 0.48128638
 0.48148218 0.48165813 0.48174372 0.4817816  0.4817582  0.48169726
 0.4816339  0.48157656 0.48151487 0.4814561  0.48137686 0.48126864
 0.4810759  0.4808136  0.48046282 0.4799836  0.47938013 0.47868884
 0.4779102  0.47715342 0.47641894 0.47568244 0.47498503 0.4743811
 0.47389188 0.47341278 0.47294962 0.47252318 0.4720653  0.471563
 0.47110257 0.4707305  0.4704021  0.47007865 0.46983677 0.4696137
 0.4694331  0.46939203 0.46949917 0.46966556 0.46990687 0.4702166
 0.47057655 0.47082248 0.47094634 0.4709916  0.47097978 0.4709882
 0.4710259  0.47108653 0.4711737  0.47128665 0.4713601  0.47139853
 0.4713618  0.4712353  0.471077   0.47081646 0.47049987 0.47022188
 0.47000235 0.46983624 0.46975353 0.46972048 0.46970218 0.46974418
 0.46974444 0.46964154 0.46945962 0.46924376 0.46906003 0.4689596
 0.4689652  0.4690389  0.46915767 0.46927738 0.469349   0.4693466
 0.46927398 0.46910548 0.46887836 0.46865577 0.46853358 0.46852276
 0.46858338 0.4686887  0.46882963 0.468951   0.46899557 0.46903223
 0.46899462 0.46897852 0.46900725 0.4691164  0.46929014 0.46944827
 0.46956697 0.46964732 0.46966633 0.46964327 0.46959123 0.46953735
 0.4694633  0.46947533 0.46948588 0.46950525 0.46951815 0.46949923
 0.4694544  0.4693175  0.46910697 0.46875292 0.4682197  0.46750915
 0.46670496 0.46591663 0.46525538 0.46460262 0.46399474 0.46351072
 0.46313387 0.46274298 0.46237913 0.46205416 0.46169898 0.4613014
 0.46091494 0.46055004 0.46023184 0.4599665  0.45971426 0.45952946
 0.45931116 0.459174   0.45909268 0.45908263 0.45912477 0.459273
 0.45950067 0.45961916 0.45959234 0.4595362  0.45948955 0.45942706
 0.45937693 0.45937365 0.4594009  0.45942464 0.45942137 0.45939016
 0.45928642 0.45911306 0.4588862  0.45857129 0.45825216 0.45795366
 0.457732   0.45759818 0.45756534 0.4575582  0.45761076 0.457697
 0.4577329  0.4577054  0.4576008  0.45740652 0.45720664 0.45704344
 0.45690557 0.4568035  0.45676586 0.45675835 0.45677722 0.4567489
 0.45663822 0.45647177 0.4562897  0.4561136  0.45603552 0.4560031
 0.45601442 0.45607108 0.45614845 0.4562543  0.45634562 0.45642158
 0.45651633 0.45662683 0.45677227 0.4569628  0.45713797 0.45727307
 0.45733878 0.45734274 0.45728084 0.4571989  0.45709118 0.45700076
 0.4569525  0.45697993 0.45708334 0.4571959  0.4573038  0.4573752
 0.4573889  0.4573385  0.45718077 0.45692036 0.45657176 0.4561199
 0.45554706 0.45503417 0.45461425 0.45417792 0.4537778  0.4534402
 0.45314837 0.45290473 0.4527139  0.45253077 0.45234048 0.45216182
 0.45196983 0.45177937 0.4516167  0.4514403  0.45127472 0.45103598
 0.45086834 0.45075098 0.4506674  0.45063692 0.45061523 0.4507593
 0.45097956 0.4510276  0.45085    0.45059142 0.45021063 0.44963798
 0.44893804 0.44820037 0.44752195 0.4470151  0.44670135 0.44663352
 0.44675475 0.44695058 0.44702902 0.44686005 0.44642508 0.44575018
 0.44511345 0.44495547 0.4456098  0.44718435 0.44928294 0.45046255]
