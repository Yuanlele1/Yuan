Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=42, out_features=84, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3161088.0
params:  3612.0
Trainable parameters:  3612
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5362755
	speed: 0.0239s/iter; left time: 615.2878s
	iters: 200, epoch: 1 | loss: 0.4583243
	speed: 0.0179s/iter; left time: 458.4043s
Epoch: 1 cost time: 5.169410943984985
Epoch: 1, Steps: 258 | Train Loss: 0.5366811 Vali Loss: 1.0258573 Test Loss: 0.4502677
Validation loss decreased (inf --> 1.025857).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4317237
	speed: 0.0723s/iter; left time: 1839.1254s
	iters: 200, epoch: 2 | loss: 0.3904732
	speed: 0.0190s/iter; left time: 482.3635s
Epoch: 2 cost time: 5.236795663833618
Epoch: 2, Steps: 258 | Train Loss: 0.4199534 Vali Loss: 0.9708391 Test Loss: 0.4221126
Validation loss decreased (1.025857 --> 0.970839).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4037308
	speed: 0.0745s/iter; left time: 1876.7992s
	iters: 200, epoch: 3 | loss: 0.3988254
	speed: 0.0196s/iter; left time: 491.7150s
Epoch: 3 cost time: 5.259466886520386
Epoch: 3, Steps: 258 | Train Loss: 0.4082960 Vali Loss: 0.9558738 Test Loss: 0.4184946
Validation loss decreased (0.970839 --> 0.955874).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4052367
	speed: 0.0763s/iter; left time: 1901.2741s
	iters: 200, epoch: 4 | loss: 0.4004729
	speed: 0.0194s/iter; left time: 482.0613s
Epoch: 4 cost time: 5.3352601528167725
Epoch: 4, Steps: 258 | Train Loss: 0.4050169 Vali Loss: 0.9485618 Test Loss: 0.4188136
Validation loss decreased (0.955874 --> 0.948562).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4225718
	speed: 0.0781s/iter; left time: 1926.7216s
	iters: 200, epoch: 5 | loss: 0.3878787
	speed: 0.0188s/iter; left time: 461.8939s
Epoch: 5 cost time: 5.278612375259399
Epoch: 5, Steps: 258 | Train Loss: 0.4035538 Vali Loss: 0.9443333 Test Loss: 0.4193230
Validation loss decreased (0.948562 --> 0.944333).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4002355
	speed: 0.0745s/iter; left time: 1818.8090s
	iters: 200, epoch: 6 | loss: 0.4202291
	speed: 0.0188s/iter; left time: 456.8455s
Epoch: 6 cost time: 5.2780516147613525
Epoch: 6, Steps: 258 | Train Loss: 0.4030440 Vali Loss: 0.9424939 Test Loss: 0.4198435
Validation loss decreased (0.944333 --> 0.942494).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4041594
	speed: 0.0774s/iter; left time: 1868.3643s
	iters: 200, epoch: 7 | loss: 0.4160812
	speed: 0.0187s/iter; left time: 449.0475s
Epoch: 7 cost time: 5.258101463317871
Epoch: 7, Steps: 258 | Train Loss: 0.4026554 Vali Loss: 0.9415044 Test Loss: 0.4202616
Validation loss decreased (0.942494 --> 0.941504).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4098708
	speed: 0.0741s/iter; left time: 1769.9265s
	iters: 200, epoch: 8 | loss: 0.3661383
	speed: 0.0186s/iter; left time: 443.7218s
Epoch: 8 cost time: 5.231041193008423
Epoch: 8, Steps: 258 | Train Loss: 0.4024781 Vali Loss: 0.9400923 Test Loss: 0.4204376
Validation loss decreased (0.941504 --> 0.940092).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4026279
	speed: 0.0763s/iter; left time: 1802.6930s
	iters: 200, epoch: 9 | loss: 0.4111112
	speed: 0.0190s/iter; left time: 446.5162s
Epoch: 9 cost time: 5.232586622238159
Epoch: 9, Steps: 258 | Train Loss: 0.4023395 Vali Loss: 0.9392161 Test Loss: 0.4209772
Validation loss decreased (0.940092 --> 0.939216).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3781186
	speed: 0.0724s/iter; left time: 1693.6867s
	iters: 200, epoch: 10 | loss: 0.4105107
	speed: 0.0159s/iter; left time: 370.6739s
Epoch: 10 cost time: 4.601346969604492
Epoch: 10, Steps: 258 | Train Loss: 0.4021943 Vali Loss: 0.9380310 Test Loss: 0.4210520
Validation loss decreased (0.939216 --> 0.938031).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3688741
	speed: 0.0696s/iter; left time: 1608.5273s
	iters: 200, epoch: 11 | loss: 0.4000103
	speed: 0.0184s/iter; left time: 423.1750s
Epoch: 11 cost time: 5.056609392166138
Epoch: 11, Steps: 258 | Train Loss: 0.4021895 Vali Loss: 0.9385558 Test Loss: 0.4211386
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4303057
	speed: 0.0748s/iter; left time: 1710.2850s
	iters: 200, epoch: 12 | loss: 0.3774603
	speed: 0.0189s/iter; left time: 430.0372s
Epoch: 12 cost time: 5.199019193649292
Epoch: 12, Steps: 258 | Train Loss: 0.4021632 Vali Loss: 0.9372115 Test Loss: 0.4213458
Validation loss decreased (0.938031 --> 0.937211).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4254164
	speed: 0.0752s/iter; left time: 1700.1658s
	iters: 200, epoch: 13 | loss: 0.4099594
	speed: 0.0187s/iter; left time: 419.8204s
Epoch: 13 cost time: 5.27492094039917
Epoch: 13, Steps: 258 | Train Loss: 0.4021243 Vali Loss: 0.9371892 Test Loss: 0.4213457
Validation loss decreased (0.937211 --> 0.937189).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3961262
	speed: 0.0746s/iter; left time: 1667.8925s
	iters: 200, epoch: 14 | loss: 0.3869036
	speed: 0.0193s/iter; left time: 429.6763s
Epoch: 14 cost time: 5.249558210372925
Epoch: 14, Steps: 258 | Train Loss: 0.4020285 Vali Loss: 0.9373490 Test Loss: 0.4214800
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4366309
	speed: 0.0760s/iter; left time: 1677.8801s
	iters: 200, epoch: 15 | loss: 0.4289895
	speed: 0.0188s/iter; left time: 413.8203s
Epoch: 15 cost time: 5.256003379821777
Epoch: 15, Steps: 258 | Train Loss: 0.4019166 Vali Loss: 0.9369889 Test Loss: 0.4218088
Validation loss decreased (0.937189 --> 0.936989).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4274579
	speed: 0.0759s/iter; left time: 1658.0344s
	iters: 200, epoch: 16 | loss: 0.3842452
	speed: 0.0189s/iter; left time: 410.8732s
Epoch: 16 cost time: 5.185856342315674
Epoch: 16, Steps: 258 | Train Loss: 0.4019187 Vali Loss: 0.9380488 Test Loss: 0.4214282
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3661816
	speed: 0.0755s/iter; left time: 1629.7626s
	iters: 200, epoch: 17 | loss: 0.4031179
	speed: 0.0193s/iter; left time: 414.6763s
Epoch: 17 cost time: 5.4242753982543945
Epoch: 17, Steps: 258 | Train Loss: 0.4019611 Vali Loss: 0.9367717 Test Loss: 0.4212292
Validation loss decreased (0.936989 --> 0.936772).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3835826
	speed: 0.0756s/iter; left time: 1611.0195s
	iters: 200, epoch: 18 | loss: 0.3842662
	speed: 0.0189s/iter; left time: 400.1327s
Epoch: 18 cost time: 5.271299839019775
Epoch: 18, Steps: 258 | Train Loss: 0.4018978 Vali Loss: 0.9368876 Test Loss: 0.4215332
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3616205
	speed: 0.0756s/iter; left time: 1592.5986s
	iters: 200, epoch: 19 | loss: 0.4090468
	speed: 0.0185s/iter; left time: 386.6621s
Epoch: 19 cost time: 5.269053936004639
Epoch: 19, Steps: 258 | Train Loss: 0.4018500 Vali Loss: 0.9365215 Test Loss: 0.4215275
Validation loss decreased (0.936772 --> 0.936522).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3731871
	speed: 0.0762s/iter; left time: 1585.7174s
	iters: 200, epoch: 20 | loss: 0.4037002
	speed: 0.0188s/iter; left time: 389.2816s
Epoch: 20 cost time: 5.245343208312988
Epoch: 20, Steps: 258 | Train Loss: 0.4018421 Vali Loss: 0.9373112 Test Loss: 0.4215987
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3858518
	speed: 0.0762s/iter; left time: 1565.5016s
	iters: 200, epoch: 21 | loss: 0.3782533
	speed: 0.0186s/iter; left time: 379.6179s
Epoch: 21 cost time: 5.2958080768585205
Epoch: 21, Steps: 258 | Train Loss: 0.4017189 Vali Loss: 0.9361112 Test Loss: 0.4216657
Validation loss decreased (0.936522 --> 0.936111).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3655845
	speed: 0.0757s/iter; left time: 1535.6061s
	iters: 200, epoch: 22 | loss: 0.4062946
	speed: 0.0185s/iter; left time: 372.6991s
Epoch: 22 cost time: 5.2691240310668945
Epoch: 22, Steps: 258 | Train Loss: 0.4018228 Vali Loss: 0.9362766 Test Loss: 0.4217128
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4029461
	speed: 0.0751s/iter; left time: 1504.5611s
	iters: 200, epoch: 23 | loss: 0.4185390
	speed: 0.0186s/iter; left time: 369.9439s
Epoch: 23 cost time: 5.1886537075042725
Epoch: 23, Steps: 258 | Train Loss: 0.4017637 Vali Loss: 0.9369645 Test Loss: 0.4214889
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3765171
	speed: 0.0757s/iter; left time: 1497.2215s
	iters: 200, epoch: 24 | loss: 0.3839084
	speed: 0.0177s/iter; left time: 347.2931s
Epoch: 24 cost time: 4.998047351837158
Epoch: 24, Steps: 258 | Train Loss: 0.4016775 Vali Loss: 0.9361781 Test Loss: 0.4216234
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4201701
	speed: 0.0726s/iter; left time: 1415.8469s
	iters: 200, epoch: 25 | loss: 0.3983865
	speed: 0.0179s/iter; left time: 347.2154s
Epoch: 25 cost time: 4.860650539398193
Epoch: 25, Steps: 258 | Train Loss: 0.4017535 Vali Loss: 0.9362210 Test Loss: 0.4213107
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4329075
	speed: 0.0741s/iter; left time: 1426.2960s
	iters: 200, epoch: 26 | loss: 0.4395238
	speed: 0.0171s/iter; left time: 327.7339s
Epoch: 26 cost time: 4.918176651000977
Epoch: 26, Steps: 258 | Train Loss: 0.4017932 Vali Loss: 0.9357305 Test Loss: 0.4217928
Validation loss decreased (0.936111 --> 0.935730).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3563192
	speed: 0.0722s/iter; left time: 1370.6798s
	iters: 200, epoch: 27 | loss: 0.3973643
	speed: 0.0174s/iter; left time: 329.2503s
Epoch: 27 cost time: 4.867656707763672
Epoch: 27, Steps: 258 | Train Loss: 0.4018032 Vali Loss: 0.9367341 Test Loss: 0.4215219
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4095689
	speed: 0.0744s/iter; left time: 1393.2147s
	iters: 200, epoch: 28 | loss: 0.3852902
	speed: 0.0187s/iter; left time: 348.2111s
Epoch: 28 cost time: 5.328657627105713
Epoch: 28, Steps: 258 | Train Loss: 0.4017097 Vali Loss: 0.9356982 Test Loss: 0.4215933
Validation loss decreased (0.935730 --> 0.935698).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3936834
	speed: 0.0741s/iter; left time: 1368.7807s
	iters: 200, epoch: 29 | loss: 0.4269167
	speed: 0.0178s/iter; left time: 327.0704s
Epoch: 29 cost time: 4.982401371002197
Epoch: 29, Steps: 258 | Train Loss: 0.4016787 Vali Loss: 0.9366929 Test Loss: 0.4214443
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4594452
	speed: 0.0716s/iter; left time: 1304.6081s
	iters: 200, epoch: 30 | loss: 0.3838844
	speed: 0.0167s/iter; left time: 302.6315s
Epoch: 30 cost time: 4.694692850112915
Epoch: 30, Steps: 258 | Train Loss: 0.4016860 Vali Loss: 0.9356058 Test Loss: 0.4214533
Validation loss decreased (0.935698 --> 0.935606).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4306104
	speed: 0.0728s/iter; left time: 1308.4062s
	iters: 200, epoch: 31 | loss: 0.4146519
	speed: 0.0165s/iter; left time: 295.2059s
Epoch: 31 cost time: 4.679211378097534
Epoch: 31, Steps: 258 | Train Loss: 0.4016062 Vali Loss: 0.9356785 Test Loss: 0.4216839
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3732223
	speed: 0.0736s/iter; left time: 1302.8766s
	iters: 200, epoch: 32 | loss: 0.4414956
	speed: 0.0172s/iter; left time: 302.5400s
Epoch: 32 cost time: 4.880969762802124
Epoch: 32, Steps: 258 | Train Loss: 0.4016565 Vali Loss: 0.9363856 Test Loss: 0.4216898
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3860821
	speed: 0.0734s/iter; left time: 1279.7986s
	iters: 200, epoch: 33 | loss: 0.4023615
	speed: 0.0190s/iter; left time: 330.3285s
Epoch: 33 cost time: 5.2134411334991455
Epoch: 33, Steps: 258 | Train Loss: 0.4016676 Vali Loss: 0.9362490 Test Loss: 0.4216351
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4051264
	speed: 0.0772s/iter; left time: 1326.1856s
	iters: 200, epoch: 34 | loss: 0.4052137
	speed: 0.0189s/iter; left time: 322.9594s
Epoch: 34 cost time: 5.4512927532196045
Epoch: 34, Steps: 258 | Train Loss: 0.4016503 Vali Loss: 0.9361991 Test Loss: 0.4216397
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3890779
	speed: 0.0735s/iter; left time: 1244.2666s
	iters: 200, epoch: 35 | loss: 0.3872640
	speed: 0.0177s/iter; left time: 297.1883s
Epoch: 35 cost time: 5.107587575912476
Epoch: 35, Steps: 258 | Train Loss: 0.4017333 Vali Loss: 0.9349509 Test Loss: 0.4215958
Validation loss decreased (0.935606 --> 0.934951).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4018346
	speed: 0.0748s/iter; left time: 1246.6242s
	iters: 200, epoch: 36 | loss: 0.3985947
	speed: 0.0180s/iter; left time: 299.0439s
Epoch: 36 cost time: 5.0600221157073975
Epoch: 36, Steps: 258 | Train Loss: 0.4015516 Vali Loss: 0.9365021 Test Loss: 0.4216442
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3883538
	speed: 0.0733s/iter; left time: 1203.3540s
	iters: 200, epoch: 37 | loss: 0.3753458
	speed: 0.0192s/iter; left time: 312.7316s
Epoch: 37 cost time: 5.301832675933838
Epoch: 37, Steps: 258 | Train Loss: 0.4015982 Vali Loss: 0.9364074 Test Loss: 0.4217101
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4393133
	speed: 0.0752s/iter; left time: 1214.6439s
	iters: 200, epoch: 38 | loss: 0.3672338
	speed: 0.0190s/iter; left time: 305.1028s
Epoch: 38 cost time: 5.361086368560791
Epoch: 38, Steps: 258 | Train Loss: 0.4016710 Vali Loss: 0.9348765 Test Loss: 0.4215791
Validation loss decreased (0.934951 --> 0.934877).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4099092
	speed: 0.0771s/iter; left time: 1225.7375s
	iters: 200, epoch: 39 | loss: 0.4120643
	speed: 0.0180s/iter; left time: 285.0124s
Epoch: 39 cost time: 5.124147653579712
Epoch: 39, Steps: 258 | Train Loss: 0.4016706 Vali Loss: 0.9359835 Test Loss: 0.4216289
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3966669
	speed: 0.0754s/iter; left time: 1178.7239s
	iters: 200, epoch: 40 | loss: 0.3856184
	speed: 0.0188s/iter; left time: 292.4004s
Epoch: 40 cost time: 5.292172193527222
Epoch: 40, Steps: 258 | Train Loss: 0.4016709 Vali Loss: 0.9358288 Test Loss: 0.4217752
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4030483
	speed: 0.0741s/iter; left time: 1140.4000s
	iters: 200, epoch: 41 | loss: 0.3984187
	speed: 0.0185s/iter; left time: 282.7203s
Epoch: 41 cost time: 5.171613454818726
Epoch: 41, Steps: 258 | Train Loss: 0.4016176 Vali Loss: 0.9350127 Test Loss: 0.4215988
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4086259
	speed: 0.0730s/iter; left time: 1104.6261s
	iters: 200, epoch: 42 | loss: 0.3916018
	speed: 0.0163s/iter; left time: 244.9424s
Epoch: 42 cost time: 4.7183380126953125
Epoch: 42, Steps: 258 | Train Loss: 0.4015188 Vali Loss: 0.9357299 Test Loss: 0.4216967
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3931203
	speed: 0.0683s/iter; left time: 1015.8998s
	iters: 200, epoch: 43 | loss: 0.4127891
	speed: 0.0161s/iter; left time: 237.4383s
Epoch: 43 cost time: 4.646258354187012
Epoch: 43, Steps: 258 | Train Loss: 0.4015210 Vali Loss: 0.9358764 Test Loss: 0.4217028
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3666815
	speed: 0.0692s/iter; left time: 1010.7955s
	iters: 200, epoch: 44 | loss: 0.4150843
	speed: 0.0191s/iter; left time: 276.3802s
Epoch: 44 cost time: 5.097119569778442
Epoch: 44, Steps: 258 | Train Loss: 0.4016576 Vali Loss: 0.9362825 Test Loss: 0.4216704
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4092883
	speed: 0.0778s/iter; left time: 1116.3627s
	iters: 200, epoch: 45 | loss: 0.3961855
	speed: 0.0193s/iter; left time: 274.5740s
Epoch: 45 cost time: 5.51407527923584
Epoch: 45, Steps: 258 | Train Loss: 0.4015952 Vali Loss: 0.9368091 Test Loss: 0.4216004
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4138539
	speed: 0.0775s/iter; left time: 1092.5595s
	iters: 200, epoch: 46 | loss: 0.4017251
	speed: 0.0186s/iter; left time: 259.6043s
Epoch: 46 cost time: 5.075503826141357
Epoch: 46, Steps: 258 | Train Loss: 0.4016303 Vali Loss: 0.9366280 Test Loss: 0.4216862
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4241791
	speed: 0.0680s/iter; left time: 940.6850s
	iters: 200, epoch: 47 | loss: 0.4163104
	speed: 0.0166s/iter; left time: 227.3111s
Epoch: 47 cost time: 4.61937403678894
Epoch: 47, Steps: 258 | Train Loss: 0.4016623 Vali Loss: 0.9354604 Test Loss: 0.4216475
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4419948
	speed: 0.0671s/iter; left time: 911.0912s
	iters: 200, epoch: 48 | loss: 0.3748575
	speed: 0.0161s/iter; left time: 216.3747s
Epoch: 48 cost time: 4.646953105926514
Epoch: 48, Steps: 258 | Train Loss: 0.4015101 Vali Loss: 0.9365063 Test Loss: 0.4217016
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.3998342
	speed: 0.0686s/iter; left time: 913.8477s
	iters: 200, epoch: 49 | loss: 0.4351516
	speed: 0.0161s/iter; left time: 212.4552s
Epoch: 49 cost time: 4.525162935256958
Epoch: 49, Steps: 258 | Train Loss: 0.4016180 Vali Loss: 0.9363045 Test Loss: 0.4215946
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.4054252
	speed: 0.0686s/iter; left time: 895.4088s
	iters: 200, epoch: 50 | loss: 0.4219033
	speed: 0.0166s/iter; left time: 214.5720s
Epoch: 50 cost time: 4.729560852050781
Epoch: 50, Steps: 258 | Train Loss: 0.4015588 Vali Loss: 0.9361048 Test Loss: 0.4216368
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.3773897
	speed: 0.0694s/iter; left time: 888.0441s
	iters: 200, epoch: 51 | loss: 0.4302925
	speed: 0.0165s/iter; left time: 209.8422s
Epoch: 51 cost time: 4.712646484375
Epoch: 51, Steps: 258 | Train Loss: 0.4016365 Vali Loss: 0.9361371 Test Loss: 0.4216339
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4152891
	speed: 0.0684s/iter; left time: 857.9322s
	iters: 200, epoch: 52 | loss: 0.4096088
	speed: 0.0164s/iter; left time: 203.6505s
Epoch: 52 cost time: 4.703800916671753
Epoch: 52, Steps: 258 | Train Loss: 0.4015860 Vali Loss: 0.9363254 Test Loss: 0.4216142
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.3947478
	speed: 0.0694s/iter; left time: 852.3049s
	iters: 200, epoch: 53 | loss: 0.4154983
	speed: 0.0172s/iter; left time: 209.1452s
Epoch: 53 cost time: 4.786705732345581
Epoch: 53, Steps: 258 | Train Loss: 0.4015217 Vali Loss: 0.9358897 Test Loss: 0.4216454
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4248865
	speed: 0.0684s/iter; left time: 822.6192s
	iters: 200, epoch: 54 | loss: 0.4084443
	speed: 0.0166s/iter; left time: 198.4194s
Epoch: 54 cost time: 4.714084148406982
Epoch: 54, Steps: 258 | Train Loss: 0.4014525 Vali Loss: 0.9354883 Test Loss: 0.4217360
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.4047821
	speed: 0.0689s/iter; left time: 810.6398s
	iters: 200, epoch: 55 | loss: 0.4141685
	speed: 0.0163s/iter; left time: 189.9559s
Epoch: 55 cost time: 4.574002742767334
Epoch: 55, Steps: 258 | Train Loss: 0.4015433 Vali Loss: 0.9356065 Test Loss: 0.4216653
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.3881710
	speed: 0.0693s/iter; left time: 797.9009s
	iters: 200, epoch: 56 | loss: 0.3872306
	speed: 0.0169s/iter; left time: 192.5591s
Epoch: 56 cost time: 4.8166491985321045
Epoch: 56, Steps: 258 | Train Loss: 0.4014998 Vali Loss: 0.9355652 Test Loss: 0.4216300
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.3968848
	speed: 0.0698s/iter; left time: 785.0542s
	iters: 200, epoch: 57 | loss: 0.4023332
	speed: 0.0163s/iter; left time: 181.9541s
Epoch: 57 cost time: 4.674141883850098
Epoch: 57, Steps: 258 | Train Loss: 0.4016123 Vali Loss: 0.9365221 Test Loss: 0.4216444
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.3917994
	speed: 0.0696s/iter; left time: 765.1966s
	iters: 200, epoch: 58 | loss: 0.4335353
	speed: 0.0165s/iter; left time: 179.4777s
Epoch: 58 cost time: 4.717071056365967
Epoch: 58, Steps: 258 | Train Loss: 0.4014800 Vali Loss: 0.9352252 Test Loss: 0.4216854
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.42063406109809875, mae:0.4160144031047821, rse:0.617053747177124, corr:[0.5271454  0.52987957 0.5325283  0.5349038  0.53675056 0.5381977
 0.5392828  0.5400175  0.54051733 0.5408548  0.541109   0.54118776
 0.54110193 0.54077524 0.54015386 0.539209   0.53799397 0.5365271
 0.534841   0.5330232  0.5312079  0.52947295 0.52788997 0.52663296
 0.52569103 0.52516454 0.5250619  0.5252651  0.5257315  0.5263508
 0.5269812  0.5275579  0.52795106 0.5281752  0.5281489  0.5279705
 0.5275948  0.5271293  0.52665263 0.5261517  0.52570987 0.52534944
 0.52511156 0.52507865 0.5252122  0.52545404 0.52579457 0.5261501
 0.52639806 0.5264136  0.5262266  0.52585995 0.525397   0.5248315
 0.52425545 0.52372074 0.52328295 0.52296346 0.5227942  0.5227302
 0.52277464 0.52291656 0.5231077  0.52325743 0.52338654 0.52349293
 0.52354246 0.5235398  0.52351713 0.5234623  0.52341026 0.5233568
 0.5232767  0.52316266 0.52305406 0.52294946 0.5228275  0.5227175
 0.522612   0.52253115 0.52246255 0.52240974 0.52237993 0.52235126
 0.5223786  0.52243304 0.5225376  0.52265286 0.5227706  0.52290624
 0.5230417  0.52311796 0.52310824 0.52298594 0.5227911  0.5225098
 0.5222199  0.5220348  0.521894   0.5218649  0.5219119  0.521982
 0.5220435  0.5220406  0.521957   0.52184105 0.5216581  0.5214395
 0.52117616 0.5209056  0.5206436  0.52038205 0.52011496 0.5198517
 0.51958805 0.5193168  0.51903385 0.51873493 0.5184145  0.5181153
 0.517835   0.51756364 0.51727325 0.5170089  0.5167946  0.51662934
 0.5165059  0.5164549  0.51645315 0.5165026  0.5165401  0.5165386
 0.5165162  0.5164721  0.51638573 0.51628166 0.5162009  0.5161067
 0.5160402  0.5160436  0.51609194 0.51621336 0.51640934 0.516664
 0.51693463 0.5171277  0.5172321  0.51724446 0.5172045  0.5171157
 0.51697505 0.5168237  0.51668906 0.51656634 0.51646024 0.5164344
 0.5164409  0.5164721  0.5165378  0.5165939  0.51668555 0.5168105
 0.51696545 0.5171407  0.51733136 0.5175186  0.5177026  0.5178989
 0.51804656 0.5180969  0.51809514 0.5180725  0.518016   0.51793665
 0.5178602  0.5177882  0.51774406 0.5177379  0.51776636 0.5178483
 0.5179845  0.51818097 0.51839554 0.5186419  0.5189053  0.51915973
 0.5193481  0.51944995 0.5194563  0.5193394  0.5190546  0.5186331
 0.51810604 0.517598   0.51709366 0.5165728  0.5160284  0.51549137
 0.51493955 0.5143775  0.5137684  0.51313984 0.5124849  0.5118133
 0.5111341  0.5104538  0.50976694 0.50908834 0.5084223  0.50776213
 0.5070886  0.5064254  0.50579923 0.5052288  0.50469613 0.50424284
 0.5039124  0.50369287 0.50356996 0.5034988  0.5034441  0.50341797
 0.5034456  0.5034748  0.50351894 0.5035844  0.50365007 0.5037204
 0.5037914  0.5038828  0.50396216 0.5040166  0.5040507  0.50407064
 0.50410676 0.5041986  0.50430006 0.50446695 0.504653   0.50487405
 0.5050857  0.5051874  0.5051768  0.50508153 0.5049605  0.504804
 0.5046397  0.50448877 0.5043558  0.5042615  0.5041881  0.5041658
 0.50414115 0.50415003 0.504168   0.5041821  0.50420094 0.50423115
 0.50428015 0.50436425 0.5044736  0.5045851  0.50467074 0.5047793
 0.5048294  0.50483483 0.5047962  0.5047481  0.5047007  0.50464153
 0.5045906  0.5045533  0.5045466  0.5045625  0.50460243 0.50467026
 0.50473154 0.5048357  0.50491536 0.50500894 0.5050787  0.5050907
 0.5050689  0.50494516 0.50470036 0.5043745  0.50395226 0.50343597
 0.5028607  0.50231606 0.50183564 0.5013955  0.50096226 0.50053537
 0.50010324 0.49963173 0.49915892 0.49867553 0.49817225 0.49767905
 0.49720225 0.49675956 0.49639842 0.49611613 0.49589843 0.49570873
 0.4955813  0.4954638  0.49533877 0.4952146  0.4951199  0.49505004
 0.49500334 0.4949897  0.4949725  0.4949332  0.49486884 0.49480233
 0.4947345  0.49464855 0.4945507  0.49445856 0.49436945 0.49426952
 0.49415508 0.4940537  0.4939454  0.49384186 0.49371806 0.49358782
 0.4935129  0.49349922 0.49353138 0.49361122 0.49371886 0.4938856
 0.4940581  0.49412745 0.494114   0.49402952 0.4939142  0.49378628
 0.49364063 0.4935256  0.49344125 0.49338594 0.49335515 0.4933665
 0.49335513 0.4933504  0.49333593 0.49330017 0.493267   0.4932383
 0.49320513 0.49319002 0.4931799  0.49316844 0.49316683 0.49318948
 0.49315542 0.49310648 0.49302527 0.4929394  0.4928617  0.49280158
 0.4927388  0.49269867 0.49269325 0.49270332 0.49275494 0.49285772
 0.4930157  0.49323332 0.49349678 0.49380174 0.49410307 0.49438927
 0.49464592 0.49482927 0.4949419  0.4949101  0.4947281  0.49443725
 0.4940719  0.49368185 0.4933324  0.49301723 0.49266368 0.49234316
 0.49202877 0.4916961  0.49132517 0.4909473  0.49052775 0.49008307
 0.48963165 0.48919764 0.48880553 0.488472   0.48816368 0.4879114
 0.48769477 0.48753697 0.48738798 0.48726654 0.4871666  0.487111
 0.48709127 0.48709834 0.48711643 0.48713902 0.48715135 0.4871265
 0.48713705 0.48714352 0.48718303 0.4872177  0.48725757 0.48733297
 0.4873938  0.4874643  0.4875061  0.4875314  0.48750463 0.48748338
 0.48747241 0.48749775 0.487534   0.48763132 0.48773384 0.4879013
 0.4880611  0.4881125  0.4880658  0.48795503 0.48779625 0.48762083
 0.4874445  0.4872858  0.48717594 0.48710895 0.48708066 0.4871023
 0.48712653 0.4871705  0.48719656 0.4872174  0.48723057 0.48723152
 0.48724535 0.48726946 0.48729584 0.4873148  0.4873111  0.4873369
 0.48731294 0.4872753  0.48722765 0.48721233 0.48719156 0.4871893
 0.48718843 0.4871975  0.4871903  0.48717952 0.48716465 0.4871581
 0.48716944 0.48718986 0.48722583 0.48726717 0.48730478 0.4873217
 0.4872662  0.48714164 0.48695245 0.48665798 0.48624548 0.48574343
 0.4851821  0.48461974 0.48409462 0.48359403 0.48306853 0.48252928
 0.48200175 0.48140967 0.48077366 0.48012304 0.47945246 0.4787723
 0.47811517 0.4775307  0.47704008 0.47661203 0.47623217 0.47591278
 0.47564143 0.4754353  0.47528797 0.4751232  0.4750004  0.4749018
 0.47483888 0.47481024 0.47478938 0.4747967  0.47481272 0.4748614
 0.4749589  0.4750584  0.47515765 0.4752962  0.4754593  0.47562262
 0.47575405 0.4758741  0.47600636 0.47609577 0.47613397 0.47617143
 0.47623175 0.4763395  0.47648668 0.4766379  0.47681803 0.47704378
 0.47727838 0.47744697 0.47750643 0.47750282 0.47748706 0.47746122
 0.47743946 0.47741923 0.47740665 0.47741935 0.4774425  0.4774676
 0.47748575 0.47747687 0.47744423 0.47738978 0.4773214  0.47726154
 0.47721678 0.47719836 0.47720867 0.47722393 0.4772367  0.47729686
 0.47732735 0.4773382  0.4773364  0.47733188 0.47730997 0.47727668
 0.4772256  0.47717136 0.4771139  0.47706702 0.4770334  0.47701785
 0.47700983 0.47705868 0.47711387 0.47718477 0.4772576  0.4773217
 0.4773488  0.47728613 0.4771479  0.47690776 0.47655305 0.47606772
 0.47549957 0.47495097 0.47445256 0.47399113 0.47351348 0.47306278
 0.47259426 0.47210476 0.47157893 0.47101736 0.4704478  0.46985587
 0.46929842 0.4688018  0.4683728  0.46800506 0.46770984 0.46746218
 0.46724918 0.46704432 0.4668579  0.46669647 0.46656284 0.46645746
 0.466403   0.46637806 0.46638557 0.4664195  0.46648946 0.46656913
 0.46669647 0.46684238 0.46700132 0.46716377 0.46731025 0.46744832
 0.46755442 0.46759906 0.4676134  0.46759647 0.4675574  0.46751586
 0.46751955 0.46760947 0.46776152 0.4679642  0.46827075 0.4686462
 0.4689985  0.46928886 0.4694637  0.46950343 0.46946657 0.4693835
 0.46922547 0.4690459  0.46886888 0.46869135 0.46852717 0.46836513
 0.46817687 0.46798575 0.4678194  0.46764854 0.46752056 0.46741936
 0.46735454 0.46731478 0.4673058  0.46732053 0.46733877 0.46738207
 0.46740702 0.46737215 0.4673141  0.46725303 0.4671892  0.46712255
 0.46705744 0.46700168 0.46696562 0.46696356 0.46699613 0.4670484
 0.46711895 0.46723846 0.46739677 0.46755314 0.46771666 0.46785766
 0.46794212 0.4679296  0.46780267 0.4675962  0.46727276 0.46684396
 0.46634242 0.46587503 0.4654996  0.4652059  0.4649548  0.46472842
 0.4645036  0.46424076 0.4639693  0.46368384 0.46332008 0.46297038
 0.462589   0.46222818 0.46188068 0.46158507 0.46132892 0.4610892
 0.4608694  0.46064952 0.46046835 0.46027872 0.4601327  0.46006706
 0.46006748 0.46016052 0.46028927 0.46043873 0.46062237 0.4607851
 0.46095294 0.46111193 0.46127355 0.46148762 0.4616997  0.4619227
 0.46214566 0.46238455 0.46264789 0.46292105 0.4632336  0.46356288
 0.4639397  0.46433103 0.46460962 0.46473482 0.46455696 0.46378863]
