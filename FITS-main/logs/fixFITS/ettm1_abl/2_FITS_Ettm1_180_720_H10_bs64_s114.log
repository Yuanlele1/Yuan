Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=30, out_features=150, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4032000.0
params:  4650.0
Trainable parameters:  4650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7678205
	speed: 0.0209s/iter; left time: 545.1686s
	iters: 200, epoch: 1 | loss: 0.5287985
	speed: 0.0153s/iter; left time: 399.1000s
Epoch: 1 cost time: 4.575605392456055
Epoch: 1, Steps: 262 | Train Loss: 0.7055244 Vali Loss: 1.2373751 Test Loss: 0.6607696
Validation loss decreased (inf --> 1.237375).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4043563
	speed: 0.0725s/iter; left time: 1874.0595s
	iters: 200, epoch: 2 | loss: 0.4238769
	speed: 0.0170s/iter; left time: 436.6761s
Epoch: 2 cost time: 4.876630783081055
Epoch: 2, Steps: 262 | Train Loss: 0.4353999 Vali Loss: 1.0783327 Test Loss: 0.5190402
Validation loss decreased (1.237375 --> 1.078333).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3728998
	speed: 0.0757s/iter; left time: 1936.4554s
	iters: 200, epoch: 3 | loss: 0.3941649
	speed: 0.0152s/iter; left time: 386.6460s
Epoch: 3 cost time: 4.5765135288238525
Epoch: 3, Steps: 262 | Train Loss: 0.3900755 Vali Loss: 1.0344359 Test Loss: 0.4817000
Validation loss decreased (1.078333 --> 1.034436).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3937834
	speed: 0.0732s/iter; left time: 1853.6895s
	iters: 200, epoch: 4 | loss: 0.3542452
	speed: 0.0149s/iter; left time: 375.2450s
Epoch: 4 cost time: 4.49072790145874
Epoch: 4, Steps: 262 | Train Loss: 0.3754911 Vali Loss: 1.0133927 Test Loss: 0.4650358
Validation loss decreased (1.034436 --> 1.013393).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3903283
	speed: 0.0737s/iter; left time: 1847.1636s
	iters: 200, epoch: 5 | loss: 0.3690635
	speed: 0.0159s/iter; left time: 396.5420s
Epoch: 5 cost time: 4.5515830516815186
Epoch: 5, Steps: 262 | Train Loss: 0.3678290 Vali Loss: 1.0007156 Test Loss: 0.4562312
Validation loss decreased (1.013393 --> 1.000716).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3552622
	speed: 0.0738s/iter; left time: 1829.5524s
	iters: 200, epoch: 6 | loss: 0.3851638
	speed: 0.0153s/iter; left time: 378.7698s
Epoch: 6 cost time: 4.561479568481445
Epoch: 6, Steps: 262 | Train Loss: 0.3638480 Vali Loss: 0.9939712 Test Loss: 0.4512073
Validation loss decreased (1.000716 --> 0.993971).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3633799
	speed: 0.0740s/iter; left time: 1815.0910s
	iters: 200, epoch: 7 | loss: 0.3838474
	speed: 0.0157s/iter; left time: 382.4704s
Epoch: 7 cost time: 4.693589687347412
Epoch: 7, Steps: 262 | Train Loss: 0.3613414 Vali Loss: 0.9889057 Test Loss: 0.4486628
Validation loss decreased (0.993971 --> 0.988906).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3481801
	speed: 0.0732s/iter; left time: 1777.3650s
	iters: 200, epoch: 8 | loss: 0.3647273
	speed: 0.0152s/iter; left time: 366.2351s
Epoch: 8 cost time: 4.481470108032227
Epoch: 8, Steps: 262 | Train Loss: 0.3601088 Vali Loss: 0.9862353 Test Loss: 0.4476312
Validation loss decreased (0.988906 --> 0.986235).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3759392
	speed: 0.0722s/iter; left time: 1733.7698s
	iters: 200, epoch: 9 | loss: 0.4005017
	speed: 0.0147s/iter; left time: 351.0203s
Epoch: 9 cost time: 4.332263231277466
Epoch: 9, Steps: 262 | Train Loss: 0.3593491 Vali Loss: 0.9852399 Test Loss: 0.4467922
Validation loss decreased (0.986235 --> 0.985240).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3579812
	speed: 0.0726s/iter; left time: 1723.3005s
	iters: 200, epoch: 10 | loss: 0.3526709
	speed: 0.0150s/iter; left time: 354.1338s
Epoch: 10 cost time: 4.439952373504639
Epoch: 10, Steps: 262 | Train Loss: 0.3590250 Vali Loss: 0.9830898 Test Loss: 0.4465956
Validation loss decreased (0.985240 --> 0.983090).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3295240
	speed: 0.0747s/iter; left time: 1753.8751s
	iters: 200, epoch: 11 | loss: 0.3433116
	speed: 0.0149s/iter; left time: 347.4821s
Epoch: 11 cost time: 4.515854120254517
Epoch: 11, Steps: 262 | Train Loss: 0.3588358 Vali Loss: 0.9824928 Test Loss: 0.4465230
Validation loss decreased (0.983090 --> 0.982493).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3810730
	speed: 0.0725s/iter; left time: 1682.8137s
	iters: 200, epoch: 12 | loss: 0.3761250
	speed: 0.0150s/iter; left time: 347.6358s
Epoch: 12 cost time: 4.49115514755249
Epoch: 12, Steps: 262 | Train Loss: 0.3586875 Vali Loss: 0.9823431 Test Loss: 0.4465612
Validation loss decreased (0.982493 --> 0.982343).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3706927
	speed: 0.0746s/iter; left time: 1712.3669s
	iters: 200, epoch: 13 | loss: 0.3930789
	speed: 0.0148s/iter; left time: 339.2693s
Epoch: 13 cost time: 4.560641288757324
Epoch: 13, Steps: 262 | Train Loss: 0.3587940 Vali Loss: 0.9827983 Test Loss: 0.4465160
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3734750
	speed: 0.0726s/iter; left time: 1648.6253s
	iters: 200, epoch: 14 | loss: 0.3563149
	speed: 0.0147s/iter; left time: 331.9328s
Epoch: 14 cost time: 4.492335081100464
Epoch: 14, Steps: 262 | Train Loss: 0.3587110 Vali Loss: 0.9830660 Test Loss: 0.4465075
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3615991
	speed: 0.0760s/iter; left time: 1704.3354s
	iters: 200, epoch: 15 | loss: 0.3400175
	speed: 0.0156s/iter; left time: 349.2125s
Epoch: 15 cost time: 4.73755145072937
Epoch: 15, Steps: 262 | Train Loss: 0.3586335 Vali Loss: 0.9827353 Test Loss: 0.4465763
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4005736
	speed: 0.0733s/iter; left time: 1625.0583s
	iters: 200, epoch: 16 | loss: 0.3449678
	speed: 0.0153s/iter; left time: 337.9751s
Epoch: 16 cost time: 4.485527753829956
Epoch: 16, Steps: 262 | Train Loss: 0.3586146 Vali Loss: 0.9827083 Test Loss: 0.4466167
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3640372
	speed: 0.0734s/iter; left time: 1607.2760s
	iters: 200, epoch: 17 | loss: 0.3326065
	speed: 0.0151s/iter; left time: 328.7833s
Epoch: 17 cost time: 4.502949476242065
Epoch: 17, Steps: 262 | Train Loss: 0.3585107 Vali Loss: 0.9824039 Test Loss: 0.4466221
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3679800
	speed: 0.0739s/iter; left time: 1599.3982s
	iters: 200, epoch: 18 | loss: 0.3619468
	speed: 0.0155s/iter; left time: 333.1941s
Epoch: 18 cost time: 4.620807647705078
Epoch: 18, Steps: 262 | Train Loss: 0.3585173 Vali Loss: 0.9826922 Test Loss: 0.4468198
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3550290
	speed: 0.0748s/iter; left time: 1600.6443s
	iters: 200, epoch: 19 | loss: 0.3329593
	speed: 0.0163s/iter; left time: 347.5573s
Epoch: 19 cost time: 4.950343370437622
Epoch: 19, Steps: 262 | Train Loss: 0.3584961 Vali Loss: 0.9828620 Test Loss: 0.4465904
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3717732
	speed: 0.0730s/iter; left time: 1542.0709s
	iters: 200, epoch: 20 | loss: 0.3833104
	speed: 0.0153s/iter; left time: 321.7253s
Epoch: 20 cost time: 4.557514667510986
Epoch: 20, Steps: 262 | Train Loss: 0.3585597 Vali Loss: 0.9819655 Test Loss: 0.4466716
Validation loss decreased (0.982343 --> 0.981965).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3784242
	speed: 0.0744s/iter; left time: 1553.0096s
	iters: 200, epoch: 21 | loss: 0.3582393
	speed: 0.0151s/iter; left time: 314.4565s
Epoch: 21 cost time: 4.507889032363892
Epoch: 21, Steps: 262 | Train Loss: 0.3585309 Vali Loss: 0.9830962 Test Loss: 0.4466047
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3861504
	speed: 0.0719s/iter; left time: 1480.7357s
	iters: 200, epoch: 22 | loss: 0.3660358
	speed: 0.0151s/iter; left time: 309.9380s
Epoch: 22 cost time: 4.466611862182617
Epoch: 22, Steps: 262 | Train Loss: 0.3586412 Vali Loss: 0.9828956 Test Loss: 0.4466119
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3594326
	speed: 0.0737s/iter; left time: 1498.8003s
	iters: 200, epoch: 23 | loss: 0.3904965
	speed: 0.0151s/iter; left time: 306.1491s
Epoch: 23 cost time: 4.5331666469573975
Epoch: 23, Steps: 262 | Train Loss: 0.3585814 Vali Loss: 0.9815844 Test Loss: 0.4467414
Validation loss decreased (0.981965 --> 0.981584).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3625291
	speed: 0.0716s/iter; left time: 1438.1583s
	iters: 200, epoch: 24 | loss: 0.3261163
	speed: 0.0155s/iter; left time: 308.9668s
Epoch: 24 cost time: 4.503897666931152
Epoch: 24, Steps: 262 | Train Loss: 0.3586025 Vali Loss: 0.9832455 Test Loss: 0.4467397
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3665176
	speed: 0.0792s/iter; left time: 1568.5587s
	iters: 200, epoch: 25 | loss: 0.3964346
	speed: 0.0161s/iter; left time: 317.7807s
Epoch: 25 cost time: 4.862493515014648
Epoch: 25, Steps: 262 | Train Loss: 0.3585834 Vali Loss: 0.9830922 Test Loss: 0.4467400
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3859163
	speed: 0.0748s/iter; left time: 1463.3740s
	iters: 200, epoch: 26 | loss: 0.3505124
	speed: 0.0150s/iter; left time: 292.6883s
Epoch: 26 cost time: 4.472445011138916
Epoch: 26, Steps: 262 | Train Loss: 0.3584917 Vali Loss: 0.9833825 Test Loss: 0.4467077
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3702648
	speed: 0.0731s/iter; left time: 1409.2678s
	iters: 200, epoch: 27 | loss: 0.3548267
	speed: 0.0154s/iter; left time: 294.5951s
Epoch: 27 cost time: 4.5412445068359375
Epoch: 27, Steps: 262 | Train Loss: 0.3586259 Vali Loss: 0.9828807 Test Loss: 0.4466872
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3363588
	speed: 0.0726s/iter; left time: 1382.2881s
	iters: 200, epoch: 28 | loss: 0.3333555
	speed: 0.0151s/iter; left time: 286.6621s
Epoch: 28 cost time: 4.465006113052368
Epoch: 28, Steps: 262 | Train Loss: 0.3585736 Vali Loss: 0.9826080 Test Loss: 0.4467507
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3491096
	speed: 0.0857s/iter; left time: 1607.5233s
	iters: 200, epoch: 29 | loss: 0.3765231
	speed: 0.0172s/iter; left time: 321.8934s
Epoch: 29 cost time: 5.934689521789551
Epoch: 29, Steps: 262 | Train Loss: 0.3585152 Vali Loss: 0.9817159 Test Loss: 0.4467404
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3886273
	speed: 0.0806s/iter; left time: 1492.0138s
	iters: 200, epoch: 30 | loss: 0.3597049
	speed: 0.0166s/iter; left time: 305.5837s
Epoch: 30 cost time: 4.976306200027466
Epoch: 30, Steps: 262 | Train Loss: 0.3584958 Vali Loss: 0.9822566 Test Loss: 0.4467950
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4013948
	speed: 0.0744s/iter; left time: 1357.2574s
	iters: 200, epoch: 31 | loss: 0.3585699
	speed: 0.0157s/iter; left time: 285.3561s
Epoch: 31 cost time: 4.6348254680633545
Epoch: 31, Steps: 262 | Train Loss: 0.3586096 Vali Loss: 0.9829124 Test Loss: 0.4467411
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3667403
	speed: 0.0822s/iter; left time: 1477.9025s
	iters: 200, epoch: 32 | loss: 0.3380668
	speed: 0.0167s/iter; left time: 297.7171s
Epoch: 32 cost time: 4.907792091369629
Epoch: 32, Steps: 262 | Train Loss: 0.3585491 Vali Loss: 0.9824613 Test Loss: 0.4466868
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3356981
	speed: 0.0723s/iter; left time: 1280.0654s
	iters: 200, epoch: 33 | loss: 0.3788333
	speed: 0.0156s/iter; left time: 274.5351s
Epoch: 33 cost time: 4.505617141723633
Epoch: 33, Steps: 262 | Train Loss: 0.3584653 Vali Loss: 0.9828019 Test Loss: 0.4467946
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3385375
	speed: 0.0744s/iter; left time: 1298.2153s
	iters: 200, epoch: 34 | loss: 0.3937004
	speed: 0.0157s/iter; left time: 273.1686s
Epoch: 34 cost time: 4.598005771636963
Epoch: 34, Steps: 262 | Train Loss: 0.3585266 Vali Loss: 0.9827924 Test Loss: 0.4467680
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3766759
	speed: 0.0751s/iter; left time: 1291.9595s
	iters: 200, epoch: 35 | loss: 0.3504862
	speed: 0.0157s/iter; left time: 268.9241s
Epoch: 35 cost time: 4.785897493362427
Epoch: 35, Steps: 262 | Train Loss: 0.3585297 Vali Loss: 0.9835061 Test Loss: 0.4467635
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3430768
	speed: 0.0774s/iter; left time: 1309.7843s
	iters: 200, epoch: 36 | loss: 0.3584094
	speed: 0.0152s/iter; left time: 255.5996s
Epoch: 36 cost time: 4.570236444473267
Epoch: 36, Steps: 262 | Train Loss: 0.3585591 Vali Loss: 0.9831472 Test Loss: 0.4467572
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3498923
	speed: 0.0741s/iter; left time: 1235.6591s
	iters: 200, epoch: 37 | loss: 0.3547619
	speed: 0.0148s/iter; left time: 245.5631s
Epoch: 37 cost time: 4.4888997077941895
Epoch: 37, Steps: 262 | Train Loss: 0.3585240 Vali Loss: 0.9826709 Test Loss: 0.4467378
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3392243
	speed: 0.0718s/iter; left time: 1178.7964s
	iters: 200, epoch: 38 | loss: 0.3695888
	speed: 0.0148s/iter; left time: 242.1580s
Epoch: 38 cost time: 4.385333299636841
Epoch: 38, Steps: 262 | Train Loss: 0.3586285 Vali Loss: 0.9834983 Test Loss: 0.4467568
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.3247281
	speed: 0.0729s/iter; left time: 1176.3421s
	iters: 200, epoch: 39 | loss: 0.3926461
	speed: 0.0148s/iter; left time: 236.8301s
Epoch: 39 cost time: 4.447996139526367
Epoch: 39, Steps: 262 | Train Loss: 0.3585154 Vali Loss: 0.9837369 Test Loss: 0.4468699
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3436378
	speed: 0.0723s/iter; left time: 1148.3685s
	iters: 200, epoch: 40 | loss: 0.3398687
	speed: 0.0149s/iter; left time: 234.4119s
Epoch: 40 cost time: 4.385666847229004
Epoch: 40, Steps: 262 | Train Loss: 0.3585232 Vali Loss: 0.9834026 Test Loss: 0.4467732
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3948731
	speed: 0.0738s/iter; left time: 1152.9294s
	iters: 200, epoch: 41 | loss: 0.3885760
	speed: 0.0154s/iter; left time: 239.7546s
Epoch: 41 cost time: 4.497966766357422
Epoch: 41, Steps: 262 | Train Loss: 0.3583464 Vali Loss: 0.9824786 Test Loss: 0.4467451
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.3535567
	speed: 0.0742s/iter; left time: 1139.2090s
	iters: 200, epoch: 42 | loss: 0.3838797
	speed: 0.0264s/iter; left time: 402.4240s
Epoch: 42 cost time: 5.763833045959473
Epoch: 42, Steps: 262 | Train Loss: 0.3585742 Vali Loss: 0.9826055 Test Loss: 0.4467317
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3640911
	speed: 0.0753s/iter; left time: 1136.5587s
	iters: 200, epoch: 43 | loss: 0.3739696
	speed: 0.0157s/iter; left time: 235.9319s
Epoch: 43 cost time: 4.478188514709473
Epoch: 43, Steps: 262 | Train Loss: 0.3585360 Vali Loss: 0.9829873 Test Loss: 0.4467550
EarlyStopping counter: 20 out of 20
Early stopping
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=30, out_features=150, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4032000.0
params:  4650.0
Trainable parameters:  4650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4363292
	speed: 0.0211s/iter; left time: 549.9112s
	iters: 200, epoch: 1 | loss: 0.4786765
	speed: 0.0147s/iter; left time: 383.2326s
Epoch: 1 cost time: 4.498812675476074
Epoch: 1, Steps: 262 | Train Loss: 0.4429363 Vali Loss: 0.9810605 Test Loss: 0.4455276
Validation loss decreased (inf --> 0.981061).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4298636
	speed: 0.0718s/iter; left time: 1854.2269s
	iters: 200, epoch: 2 | loss: 0.5061105
	speed: 0.0147s/iter; left time: 377.2045s
Epoch: 2 cost time: 4.367671012878418
Epoch: 2, Steps: 262 | Train Loss: 0.4428300 Vali Loss: 0.9799585 Test Loss: 0.4454719
Validation loss decreased (0.981061 --> 0.979958).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4551409
	speed: 0.0745s/iter; left time: 1906.0516s
	iters: 200, epoch: 3 | loss: 0.4593537
	speed: 0.0161s/iter; left time: 409.9055s
Epoch: 3 cost time: 4.774444580078125
Epoch: 3, Steps: 262 | Train Loss: 0.4428399 Vali Loss: 0.9817788 Test Loss: 0.4453369
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4473850
	speed: 0.0743s/iter; left time: 1881.5765s
	iters: 200, epoch: 4 | loss: 0.4320907
	speed: 0.0151s/iter; left time: 380.6524s
Epoch: 4 cost time: 4.613688230514526
Epoch: 4, Steps: 262 | Train Loss: 0.4425358 Vali Loss: 0.9801747 Test Loss: 0.4452153
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4479980
	speed: 0.0988s/iter; left time: 2475.6907s
	iters: 200, epoch: 5 | loss: 0.4358245
	speed: 0.0296s/iter; left time: 737.4359s
Epoch: 5 cost time: 8.063456773757935
Epoch: 5, Steps: 262 | Train Loss: 0.4427624 Vali Loss: 0.9806593 Test Loss: 0.4453242
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4303899
	speed: 0.0736s/iter; left time: 1823.9853s
	iters: 200, epoch: 6 | loss: 0.4073143
	speed: 0.0153s/iter; left time: 378.8547s
Epoch: 6 cost time: 4.519191026687622
Epoch: 6, Steps: 262 | Train Loss: 0.4425613 Vali Loss: 0.9800793 Test Loss: 0.4454222
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4641723
	speed: 0.0728s/iter; left time: 1785.8862s
	iters: 200, epoch: 7 | loss: 0.4371205
	speed: 0.0148s/iter; left time: 361.2922s
Epoch: 7 cost time: 4.524395942687988
Epoch: 7, Steps: 262 | Train Loss: 0.4426172 Vali Loss: 0.9794282 Test Loss: 0.4450843
Validation loss decreased (0.979958 --> 0.979428).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4364759
	speed: 0.0728s/iter; left time: 1767.0160s
	iters: 200, epoch: 8 | loss: 0.4472413
	speed: 0.0149s/iter; left time: 361.2058s
Epoch: 8 cost time: 4.538020133972168
Epoch: 8, Steps: 262 | Train Loss: 0.4425802 Vali Loss: 0.9794865 Test Loss: 0.4453154
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3973587
	speed: 0.0744s/iter; left time: 1786.2993s
	iters: 200, epoch: 9 | loss: 0.4343508
	speed: 0.0150s/iter; left time: 358.9333s
Epoch: 9 cost time: 4.538744211196899
Epoch: 9, Steps: 262 | Train Loss: 0.4425546 Vali Loss: 0.9796971 Test Loss: 0.4452499
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4643345
	speed: 0.0720s/iter; left time: 1708.4590s
	iters: 200, epoch: 10 | loss: 0.4226227
	speed: 0.0150s/iter; left time: 354.3169s
Epoch: 10 cost time: 4.4114484786987305
Epoch: 10, Steps: 262 | Train Loss: 0.4425098 Vali Loss: 0.9800746 Test Loss: 0.4455033
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4404920
	speed: 0.0793s/iter; left time: 1861.8712s
	iters: 200, epoch: 11 | loss: 0.4007058
	speed: 0.0154s/iter; left time: 360.8399s
Epoch: 11 cost time: 4.790467739105225
Epoch: 11, Steps: 262 | Train Loss: 0.4425604 Vali Loss: 0.9814382 Test Loss: 0.4452859
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4541405
	speed: 0.0715s/iter; left time: 1660.4401s
	iters: 200, epoch: 12 | loss: 0.4336622
	speed: 0.0151s/iter; left time: 348.2396s
Epoch: 12 cost time: 4.482626676559448
Epoch: 12, Steps: 262 | Train Loss: 0.4425085 Vali Loss: 0.9803528 Test Loss: 0.4454229
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4173111
	speed: 0.0750s/iter; left time: 1722.7246s
	iters: 200, epoch: 13 | loss: 0.4047703
	speed: 0.0151s/iter; left time: 346.2810s
Epoch: 13 cost time: 4.570573329925537
Epoch: 13, Steps: 262 | Train Loss: 0.4424443 Vali Loss: 0.9797400 Test Loss: 0.4453830
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4477804
	speed: 0.0740s/iter; left time: 1679.4541s
	iters: 200, epoch: 14 | loss: 0.4354672
	speed: 0.0152s/iter; left time: 344.1310s
Epoch: 14 cost time: 4.518786430358887
Epoch: 14, Steps: 262 | Train Loss: 0.4426383 Vali Loss: 0.9808124 Test Loss: 0.4452427
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4410056
	speed: 0.0723s/iter; left time: 1622.9065s
	iters: 200, epoch: 15 | loss: 0.4447487
	speed: 0.0152s/iter; left time: 338.4597s
Epoch: 15 cost time: 4.451381206512451
Epoch: 15, Steps: 262 | Train Loss: 0.4425640 Vali Loss: 0.9810397 Test Loss: 0.4453472
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4090725
	speed: 0.0716s/iter; left time: 1586.5372s
	iters: 200, epoch: 16 | loss: 0.4519666
	speed: 0.0148s/iter; left time: 325.8549s
Epoch: 16 cost time: 4.409727334976196
Epoch: 16, Steps: 262 | Train Loss: 0.4426037 Vali Loss: 0.9802395 Test Loss: 0.4453553
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4557865
	speed: 0.0720s/iter; left time: 1577.2504s
	iters: 200, epoch: 17 | loss: 0.4141318
	speed: 0.0151s/iter; left time: 328.4405s
Epoch: 17 cost time: 4.520797491073608
Epoch: 17, Steps: 262 | Train Loss: 0.4423854 Vali Loss: 0.9795123 Test Loss: 0.4454222
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4468803
	speed: 0.0727s/iter; left time: 1574.2808s
	iters: 200, epoch: 18 | loss: 0.4483680
	speed: 0.0164s/iter; left time: 352.6758s
Epoch: 18 cost time: 4.769153594970703
Epoch: 18, Steps: 262 | Train Loss: 0.4425570 Vali Loss: 0.9804696 Test Loss: 0.4455223
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4390480
	speed: 0.0735s/iter; left time: 1571.3771s
	iters: 200, epoch: 19 | loss: 0.4325185
	speed: 0.0171s/iter; left time: 363.4336s
Epoch: 19 cost time: 4.729414224624634
Epoch: 19, Steps: 262 | Train Loss: 0.4423445 Vali Loss: 0.9796583 Test Loss: 0.4455167
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4668883
	speed: 0.0772s/iter; left time: 1630.5028s
	iters: 200, epoch: 20 | loss: 0.4847341
	speed: 0.0152s/iter; left time: 320.2306s
Epoch: 20 cost time: 4.6460418701171875
Epoch: 20, Steps: 262 | Train Loss: 0.4424055 Vali Loss: 0.9798833 Test Loss: 0.4452902
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4204611
	speed: 0.0743s/iter; left time: 1550.7178s
	iters: 200, epoch: 21 | loss: 0.4406098
	speed: 0.0151s/iter; left time: 314.5238s
Epoch: 21 cost time: 4.531674146652222
Epoch: 21, Steps: 262 | Train Loss: 0.4424362 Vali Loss: 0.9805200 Test Loss: 0.4454087
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3939741
	speed: 0.0747s/iter; left time: 1538.8680s
	iters: 200, epoch: 22 | loss: 0.4517221
	speed: 0.0168s/iter; left time: 344.2068s
Epoch: 22 cost time: 4.88567590713501
Epoch: 22, Steps: 262 | Train Loss: 0.4425435 Vali Loss: 0.9804658 Test Loss: 0.4453476
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4854274
	speed: 0.0747s/iter; left time: 1520.0400s
	iters: 200, epoch: 23 | loss: 0.4758829
	speed: 0.0147s/iter; left time: 297.3542s
Epoch: 23 cost time: 4.55730676651001
Epoch: 23, Steps: 262 | Train Loss: 0.4424480 Vali Loss: 0.9802067 Test Loss: 0.4454861
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4410656
	speed: 0.0735s/iter; left time: 1476.2868s
	iters: 200, epoch: 24 | loss: 0.4878376
	speed: 0.0155s/iter; left time: 309.9510s
Epoch: 24 cost time: 4.5497822761535645
Epoch: 24, Steps: 262 | Train Loss: 0.4422477 Vali Loss: 0.9798428 Test Loss: 0.4453499
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4713967
	speed: 0.0723s/iter; left time: 1433.2303s
	iters: 200, epoch: 25 | loss: 0.4790359
	speed: 0.0150s/iter; left time: 295.2655s
Epoch: 25 cost time: 4.504781723022461
Epoch: 25, Steps: 262 | Train Loss: 0.4423441 Vali Loss: 0.9796850 Test Loss: 0.4454584
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4107107
	speed: 0.0736s/iter; left time: 1438.8360s
	iters: 200, epoch: 26 | loss: 0.4740370
	speed: 0.0147s/iter; left time: 284.9692s
Epoch: 26 cost time: 4.415363311767578
Epoch: 26, Steps: 262 | Train Loss: 0.4425390 Vali Loss: 0.9803099 Test Loss: 0.4454454
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4567091
	speed: 0.0729s/iter; left time: 1406.1767s
	iters: 200, epoch: 27 | loss: 0.4063509
	speed: 0.0153s/iter; left time: 292.8308s
Epoch: 27 cost time: 4.572948932647705
Epoch: 27, Steps: 262 | Train Loss: 0.4426032 Vali Loss: 0.9797450 Test Loss: 0.4454260
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4432602822780609, mae:0.4244484305381775, rse:0.6334322690963745, corr:[0.5287996  0.5344621  0.53293204 0.5299142  0.5287546  0.5291956
 0.52944    0.5288803  0.52787626 0.5275418  0.52800035 0.5281146
 0.5274072  0.52572423 0.5236015  0.5214697  0.5193918  0.51743233
 0.5156202  0.5138829  0.5121156  0.5101734  0.50801957 0.50573975
 0.5033237  0.50088614 0.4987597  0.4970634  0.49590808 0.49532196
 0.49547422 0.4961649  0.49692094 0.49741846 0.49745917 0.49721733
 0.49677935 0.49631318 0.49609965 0.49599543 0.49599475 0.4959186
 0.49578762 0.49574855 0.49578294 0.4958377  0.49589178 0.49591002
 0.49591509 0.49589947 0.49594736 0.49603003 0.49611297 0.4961356
 0.49616733 0.496233   0.49644747 0.49657807 0.49655658 0.49629232
 0.49594915 0.49568045 0.49554393 0.49555847 0.49563548 0.49581262
 0.49595192 0.49596286 0.49605566 0.4962945  0.4966794  0.49701056
 0.4971198  0.49695614 0.4967669  0.4966314  0.49673888 0.49698737
 0.49716145 0.4971057  0.49690825 0.4967167  0.49667853 0.496662
 0.4966233  0.49658012 0.49653482 0.49635896 0.49599582 0.4956676
 0.49541384 0.49524286 0.4950915  0.4948422  0.49434868 0.49346456
 0.49234417 0.4912707  0.49040407 0.48988062 0.48982298 0.49024025
 0.4909109  0.49156737 0.49214557 0.4927157  0.49339205 0.4942048
 0.49503946 0.49567524 0.4959803  0.49612883 0.4961123  0.49601814
 0.49582732 0.49565008 0.495359   0.49489775 0.49439067 0.49389642
 0.49348757 0.49294546 0.49218675 0.49139118 0.49059305 0.48986328
 0.48939368 0.48925224 0.4893174  0.4894282  0.48944944 0.48932132
 0.48908904 0.48889136 0.4885904  0.48804006 0.4875478  0.48725978
 0.48727316 0.48761752 0.48799738 0.48814395 0.48801768 0.48783755
 0.48782104 0.48806962 0.4882609  0.48822746 0.48805687 0.48761928
 0.48727503 0.48734584 0.48776606 0.48830757 0.48855442 0.48853466
 0.48836157 0.4883231  0.4884377  0.48872077 0.4889516  0.48899493
 0.48886535 0.48869848 0.48860297 0.48875853 0.48903787 0.4893001
 0.48954707 0.48972866 0.48982927 0.49007812 0.4903968  0.49063438
 0.49072772 0.49057648 0.49028492 0.49001428 0.48976701 0.48963594
 0.4895945  0.48961547 0.4896569  0.48972446 0.48980823 0.48987448
 0.48990133 0.48994645 0.49012083 0.4904547  0.49078152 0.49090075
 0.4907375  0.49046862 0.49016264 0.48962212 0.48889214 0.48820767
 0.48767054 0.48724696 0.48686808 0.4866992  0.48647615 0.48613006
 0.4857482  0.4852312  0.48473802 0.4843511  0.48396084 0.4835245
 0.48300356 0.48245397 0.48179272 0.4809174  0.47989225 0.4789265
 0.47815722 0.47749808 0.47687137 0.47622892 0.47551128 0.4748781
 0.4745405  0.4745884  0.47480473 0.4749901  0.47498438 0.47459757
 0.47388357 0.47327998 0.47293848 0.47282717 0.47288522 0.4730017
 0.47303104 0.47295234 0.47262493 0.47234926 0.47227812 0.47237402
 0.4727277  0.47304958 0.47332466 0.47342712 0.47341067 0.4733903
 0.47344515 0.47348303 0.47353458 0.47357786 0.47356024 0.47349483
 0.47345182 0.4735686  0.47367176 0.47376403 0.47371775 0.47355673
 0.47349647 0.4735873  0.4737846  0.47405863 0.47433743 0.47466463
 0.4750823  0.47539192 0.47558296 0.47572267 0.47589225 0.47603902
 0.47626004 0.47646168 0.47668445 0.47684813 0.47700557 0.477159
 0.47736156 0.477739   0.4781519  0.4784619  0.4786001  0.47850248
 0.47829345 0.47798654 0.47774568 0.47762352 0.47751516 0.47725916
 0.47660318 0.47567654 0.47476137 0.4739854  0.4735462  0.4734454
 0.47368205 0.47390315 0.47408915 0.4742869  0.47454283 0.47492906
 0.47531635 0.4754609  0.4754388  0.47534323 0.47524363 0.4751253
 0.4750101  0.47480795 0.47445345 0.47403017 0.4736435  0.47334144
 0.4730719  0.4726945  0.47229448 0.47204784 0.47174403 0.4715652
 0.4715122  0.47135898 0.47106013 0.47080356 0.4705132  0.47029114
 0.47012714 0.46995628 0.4696142  0.4690795  0.46840522 0.4678169
 0.46750423 0.46753657 0.46765572 0.46767932 0.46764106 0.4675932
 0.46766746 0.46785086 0.46800157 0.46802577 0.4680451  0.4680154
 0.46802172 0.46813494 0.4683627  0.46853074 0.46847734 0.4683066
 0.46813467 0.4682073  0.4685623  0.46890226 0.46903473 0.46889585
 0.46861002 0.46848246 0.46854123 0.46881163 0.46913055 0.46936747
 0.46939623 0.46936375 0.4692911  0.4692867  0.46936125 0.4694293
 0.46941093 0.4694098  0.46947172 0.46955463 0.469701   0.46980846
 0.46991295 0.47006336 0.47037196 0.4708136  0.47130647 0.47176135
 0.4720241  0.47205657 0.47202915 0.47201028 0.47200885 0.47194582
 0.47170603 0.47121012 0.4707145  0.47020236 0.4697284  0.46955717
 0.4697059  0.47009024 0.4704713  0.47100738 0.4715977  0.47219768
 0.4726295  0.4728144  0.47277737 0.47275305 0.47271368 0.47268677
 0.47264197 0.4724876  0.47220802 0.47183773 0.47136942 0.47088122
 0.47047278 0.47011265 0.4698359  0.46966466 0.46956122 0.4694428
 0.4693622  0.4691756  0.46896338 0.4687327  0.468454   0.4683018
 0.4681113  0.46794364 0.46769905 0.46735868 0.46695217 0.46663448
 0.4663628  0.4662385  0.4661544  0.46610194 0.46602198 0.46607238
 0.46616685 0.46633762 0.4665114  0.46663967 0.46653637 0.46636868
 0.46613643 0.4659901  0.46606314 0.4663054  0.46656135 0.46674702
 0.4668416  0.46692052 0.46695513 0.4670261  0.4670144  0.46686724
 0.46676433 0.46681586 0.46696594 0.4671625  0.46722972 0.4673098
 0.46731603 0.46720088 0.467053   0.46704444 0.467095   0.46717498
 0.46723792 0.4672492  0.4671631  0.4671223  0.4672034  0.46738464
 0.46764964 0.4679481  0.46826568 0.46844733 0.4685201  0.46850792
 0.468322   0.4679984  0.46757823 0.4670932  0.46652147 0.46578693
 0.46481425 0.46367857 0.46254528 0.46153522 0.4606739  0.46004772
 0.45964405 0.45919922 0.45892224 0.4587589  0.45864204 0.45849437
 0.45837942 0.45814058 0.45778412 0.45746696 0.4572293  0.45698443
 0.45665607 0.45624837 0.45574224 0.45515078 0.45463616 0.45418334
 0.4538444  0.45351958 0.4531505  0.45281547 0.45246947 0.45223907
 0.45215693 0.4522235  0.45233783 0.45252806 0.4526651  0.45268297
 0.45254633 0.45237914 0.45215148 0.45177054 0.45136085 0.45111683
 0.45099974 0.45097354 0.45085108 0.450552   0.45024157 0.45007777
 0.45006442 0.4501801  0.45027682 0.45029905 0.45029518 0.45027316
 0.45025566 0.45021906 0.45031238 0.4503994  0.45043188 0.45042413
 0.4504733  0.45048198 0.45053154 0.45050728 0.4503547  0.45014238
 0.4499975  0.45006287 0.45034808 0.45069456 0.45093605 0.4510926
 0.45112976 0.45107916 0.45109367 0.45133168 0.4516406  0.4518761
 0.4520056  0.45205116 0.45200047 0.45197412 0.45199645 0.45209625
 0.45214465 0.45225736 0.4523248  0.4523853  0.45245442 0.45253482
 0.45252392 0.4522727  0.45181182 0.45118263 0.45044097 0.4495875
 0.44850743 0.44723186 0.44590908 0.44458872 0.44354555 0.44300318
 0.44292545 0.4430729  0.44329658 0.44348466 0.4437617  0.4441233
 0.44448394 0.44461325 0.44446903 0.44420713 0.443942   0.44372192
 0.4434982  0.44329002 0.44312054 0.4429218  0.44269565 0.4423478
 0.44195262 0.44138944 0.44065395 0.4400164  0.43959254 0.43950507
 0.43976963 0.44025806 0.44063732 0.44073722 0.440603   0.44042522
 0.44020557 0.4399184  0.43964878 0.43928233 0.43889636 0.43856648
 0.43851063 0.43868285 0.43887645 0.4389365  0.43889353 0.43889788
 0.43902567 0.43931615 0.43963465 0.43973994 0.43959454 0.43929318
 0.43898124 0.43885398 0.4389443  0.43911016 0.439232   0.43905932
 0.43868354 0.4384693  0.43849704 0.43867588 0.43888664 0.4387914
 0.43849003 0.4382776  0.4383814  0.438837   0.43940997 0.43989047
 0.440109   0.44000632 0.4398507  0.4399098  0.44013274 0.4403943
 0.4405499  0.4406179  0.44066912 0.44078955 0.44102722 0.44129604
 0.44146845 0.44156706 0.44166255 0.44176513 0.44197357 0.44219664
 0.44225207 0.44203806 0.44156566 0.4409805  0.4404179  0.43985233
 0.43912098 0.43819082 0.4371543  0.4362295  0.43577603 0.4358331
 0.4361668  0.4364989  0.436587   0.43650424 0.43647754 0.4368261
 0.437383   0.43764758 0.4375562  0.43725306 0.4369717  0.43682975
 0.43676597 0.43654606 0.4360454  0.4352904  0.4344822  0.43394446
 0.43363035 0.43319643 0.4324881  0.43154266 0.43065003 0.4301566
 0.43022788 0.43057552 0.43087685 0.43094474 0.4307413  0.43062156
 0.43062645 0.43037078 0.42965034 0.42860714 0.42793357 0.42817867
 0.42910483 0.42965636 0.42908373 0.4284791  0.42998877 0.43361598]
