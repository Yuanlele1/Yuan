Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=18, out_features=90, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1451520.0
params:  1710.0
Trainable parameters:  1710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7578844
	speed: 0.0391s/iter; left time: 1021.5598s
	iters: 200, epoch: 1 | loss: 0.6219458
	speed: 0.0422s/iter; left time: 1097.2509s
Epoch: 1 cost time: 10.029370307922363
Epoch: 1, Steps: 262 | Train Loss: 0.7539636 Vali Loss: 1.3226645 Test Loss: 0.7506407
Validation loss decreased (inf --> 1.322664).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4585025
	speed: 0.1693s/iter; left time: 4375.6430s
	iters: 200, epoch: 2 | loss: 0.4403601
	speed: 0.0344s/iter; left time: 885.0665s
Epoch: 2 cost time: 9.757779598236084
Epoch: 2, Steps: 262 | Train Loss: 0.4574005 Vali Loss: 1.1010166 Test Loss: 0.5436332
Validation loss decreased (1.322664 --> 1.101017).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3991400
	speed: 0.1502s/iter; left time: 3841.7403s
	iters: 200, epoch: 3 | loss: 0.3718266
	speed: 0.0439s/iter; left time: 1118.9761s
Epoch: 3 cost time: 12.011060953140259
Epoch: 3, Steps: 262 | Train Loss: 0.3961395 Vali Loss: 1.0420743 Test Loss: 0.4905158
Validation loss decreased (1.101017 --> 1.042074).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3936320
	speed: 0.1608s/iter; left time: 4071.9047s
	iters: 200, epoch: 4 | loss: 0.3978908
	speed: 0.0463s/iter; left time: 1166.3200s
Epoch: 4 cost time: 12.24452018737793
Epoch: 4, Steps: 262 | Train Loss: 0.3785302 Vali Loss: 1.0165492 Test Loss: 0.4693165
Validation loss decreased (1.042074 --> 1.016549).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3756990
	speed: 0.1645s/iter; left time: 4121.6118s
	iters: 200, epoch: 5 | loss: 0.4042705
	speed: 0.0474s/iter; left time: 1182.3309s
Epoch: 5 cost time: 11.601879835128784
Epoch: 5, Steps: 262 | Train Loss: 0.3707467 Vali Loss: 1.0035179 Test Loss: 0.4589234
Validation loss decreased (1.016549 --> 1.003518).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3358697
	speed: 0.1465s/iter; left time: 3631.5759s
	iters: 200, epoch: 6 | loss: 0.3827892
	speed: 0.0328s/iter; left time: 808.6495s
Epoch: 6 cost time: 8.763355493545532
Epoch: 6, Steps: 262 | Train Loss: 0.3665330 Vali Loss: 0.9955503 Test Loss: 0.4534987
Validation loss decreased (1.003518 --> 0.995550).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3820707
	speed: 0.1355s/iter; left time: 3323.4856s
	iters: 200, epoch: 7 | loss: 0.3303125
	speed: 0.0355s/iter; left time: 866.8896s
Epoch: 7 cost time: 9.719484090805054
Epoch: 7, Steps: 262 | Train Loss: 0.3638396 Vali Loss: 0.9909590 Test Loss: 0.4503634
Validation loss decreased (0.995550 --> 0.990959).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3846236
	speed: 0.1665s/iter; left time: 4040.5807s
	iters: 200, epoch: 8 | loss: 0.3392635
	speed: 0.0490s/iter; left time: 1185.2531s
Epoch: 8 cost time: 11.382584810256958
Epoch: 8, Steps: 262 | Train Loss: 0.3624368 Vali Loss: 0.9881245 Test Loss: 0.4488171
Validation loss decreased (0.990959 --> 0.988124).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3624680
	speed: 0.1667s/iter; left time: 4001.8091s
	iters: 200, epoch: 9 | loss: 0.3666887
	speed: 0.0495s/iter; left time: 1183.1842s
Epoch: 9 cost time: 12.505422830581665
Epoch: 9, Steps: 262 | Train Loss: 0.3615588 Vali Loss: 0.9870750 Test Loss: 0.4479130
Validation loss decreased (0.988124 --> 0.987075).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3487214
	speed: 0.1890s/iter; left time: 4486.7893s
	iters: 200, epoch: 10 | loss: 0.3748775
	speed: 0.0352s/iter; left time: 831.9024s
Epoch: 10 cost time: 10.458575248718262
Epoch: 10, Steps: 262 | Train Loss: 0.3612493 Vali Loss: 0.9861930 Test Loss: 0.4476429
Validation loss decreased (0.987075 --> 0.986193).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3825378
	speed: 0.1485s/iter; left time: 3487.9847s
	iters: 200, epoch: 11 | loss: 0.3849870
	speed: 0.0380s/iter; left time: 887.4650s
Epoch: 11 cost time: 9.597787141799927
Epoch: 11, Steps: 262 | Train Loss: 0.3610840 Vali Loss: 0.9855886 Test Loss: 0.4474340
Validation loss decreased (0.986193 --> 0.985589).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3745722
	speed: 0.1468s/iter; left time: 3407.6739s
	iters: 200, epoch: 12 | loss: 0.3408219
	speed: 0.0466s/iter; left time: 1076.7729s
Epoch: 12 cost time: 12.701369524002075
Epoch: 12, Steps: 262 | Train Loss: 0.3609773 Vali Loss: 0.9850019 Test Loss: 0.4472789
Validation loss decreased (0.985589 --> 0.985002).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3772964
	speed: 0.1759s/iter; left time: 4037.6622s
	iters: 200, epoch: 13 | loss: 0.3566232
	speed: 0.0461s/iter; left time: 1053.9283s
Epoch: 13 cost time: 12.764169931411743
Epoch: 13, Steps: 262 | Train Loss: 0.3608804 Vali Loss: 0.9849654 Test Loss: 0.4473591
Validation loss decreased (0.985002 --> 0.984965).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3569332
	speed: 0.1975s/iter; left time: 4481.7841s
	iters: 200, epoch: 14 | loss: 0.3856453
	speed: 0.0384s/iter; left time: 868.5706s
Epoch: 14 cost time: 10.14058518409729
Epoch: 14, Steps: 262 | Train Loss: 0.3608849 Vali Loss: 0.9842576 Test Loss: 0.4472451
Validation loss decreased (0.984965 --> 0.984258).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3533160
	speed: 0.1510s/iter; left time: 3386.6744s
	iters: 200, epoch: 15 | loss: 0.3642506
	speed: 0.0328s/iter; left time: 733.2468s
Epoch: 15 cost time: 9.273799419403076
Epoch: 15, Steps: 262 | Train Loss: 0.3607606 Vali Loss: 0.9837661 Test Loss: 0.4473587
Validation loss decreased (0.984258 --> 0.983766).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3428886
	speed: 0.1422s/iter; left time: 3153.6396s
	iters: 200, epoch: 16 | loss: 0.3585546
	speed: 0.0416s/iter; left time: 918.6278s
Epoch: 16 cost time: 10.751678466796875
Epoch: 16, Steps: 262 | Train Loss: 0.3607151 Vali Loss: 0.9847822 Test Loss: 0.4474615
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3737529
	speed: 0.1810s/iter; left time: 3965.6023s
	iters: 200, epoch: 17 | loss: 0.3852296
	speed: 0.0525s/iter; left time: 1144.6552s
Epoch: 17 cost time: 12.589975595474243
Epoch: 17, Steps: 262 | Train Loss: 0.3607311 Vali Loss: 0.9843412 Test Loss: 0.4473804
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3305089
	speed: 0.1771s/iter; left time: 3834.3355s
	iters: 200, epoch: 18 | loss: 0.3463407
	speed: 0.0403s/iter; left time: 868.6751s
Epoch: 18 cost time: 10.67250943183899
Epoch: 18, Steps: 262 | Train Loss: 0.3606075 Vali Loss: 0.9836426 Test Loss: 0.4473765
Validation loss decreased (0.983766 --> 0.983643).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3519537
	speed: 0.1482s/iter; left time: 3169.1954s
	iters: 200, epoch: 19 | loss: 0.3532301
	speed: 0.0298s/iter; left time: 635.1849s
Epoch: 19 cost time: 9.67156195640564
Epoch: 19, Steps: 262 | Train Loss: 0.3608089 Vali Loss: 0.9840574 Test Loss: 0.4473458
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3408114
	speed: 0.1779s/iter; left time: 3757.2837s
	iters: 200, epoch: 20 | loss: 0.3282055
	speed: 0.0362s/iter; left time: 760.7720s
Epoch: 20 cost time: 11.286773443222046
Epoch: 20, Steps: 262 | Train Loss: 0.3608406 Vali Loss: 0.9844518 Test Loss: 0.4472578
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3711137
	speed: 0.1542s/iter; left time: 3217.2329s
	iters: 200, epoch: 21 | loss: 0.3629501
	speed: 0.0397s/iter; left time: 823.1753s
Epoch: 21 cost time: 11.623605012893677
Epoch: 21, Steps: 262 | Train Loss: 0.3608069 Vali Loss: 0.9845290 Test Loss: 0.4472805
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3885729
	speed: 0.1821s/iter; left time: 3750.1666s
	iters: 200, epoch: 22 | loss: 0.3962894
	speed: 0.0405s/iter; left time: 830.7418s
Epoch: 22 cost time: 11.868061542510986
Epoch: 22, Steps: 262 | Train Loss: 0.3607411 Vali Loss: 0.9842570 Test Loss: 0.4473504
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3256252
	speed: 0.1712s/iter; left time: 3481.3626s
	iters: 200, epoch: 23 | loss: 0.3377113
	speed: 0.0426s/iter; left time: 862.2153s
Epoch: 23 cost time: 10.784639835357666
Epoch: 23, Steps: 262 | Train Loss: 0.3607209 Vali Loss: 0.9846600 Test Loss: 0.4474669
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3848884
	speed: 0.1557s/iter; left time: 3125.9296s
	iters: 200, epoch: 24 | loss: 0.3210875
	speed: 0.0426s/iter; left time: 850.0848s
Epoch: 24 cost time: 11.715128898620605
Epoch: 24, Steps: 262 | Train Loss: 0.3607429 Vali Loss: 0.9848796 Test Loss: 0.4474640
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3501883
	speed: 0.1460s/iter; left time: 2892.5047s
	iters: 200, epoch: 25 | loss: 0.3770275
	speed: 0.0351s/iter; left time: 692.4753s
Epoch: 25 cost time: 10.960323810577393
Epoch: 25, Steps: 262 | Train Loss: 0.3607429 Vali Loss: 0.9849311 Test Loss: 0.4473566
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3797449
	speed: 0.1749s/iter; left time: 3420.3821s
	iters: 200, epoch: 26 | loss: 0.3932960
	speed: 0.0439s/iter; left time: 853.2955s
Epoch: 26 cost time: 12.73661994934082
Epoch: 26, Steps: 262 | Train Loss: 0.3605972 Vali Loss: 0.9847918 Test Loss: 0.4473853
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3247359
	speed: 0.1785s/iter; left time: 3442.8205s
	iters: 200, epoch: 27 | loss: 0.3225265
	speed: 0.0490s/iter; left time: 939.3406s
Epoch: 27 cost time: 13.660242557525635
Epoch: 27, Steps: 262 | Train Loss: 0.3607471 Vali Loss: 0.9851968 Test Loss: 0.4474321
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3651322
	speed: 0.1800s/iter; left time: 3424.8730s
	iters: 200, epoch: 28 | loss: 0.4125216
	speed: 0.0345s/iter; left time: 653.4050s
Epoch: 28 cost time: 12.099966764450073
Epoch: 28, Steps: 262 | Train Loss: 0.3606170 Vali Loss: 0.9847509 Test Loss: 0.4473451
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3741738
	speed: 0.1618s/iter; left time: 3035.2660s
	iters: 200, epoch: 29 | loss: 0.3324177
	speed: 0.0411s/iter; left time: 767.6652s
Epoch: 29 cost time: 10.627309799194336
Epoch: 29, Steps: 262 | Train Loss: 0.3605820 Vali Loss: 0.9848954 Test Loss: 0.4474247
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3511584
	speed: 0.1592s/iter; left time: 2945.2868s
	iters: 200, epoch: 30 | loss: 0.3839872
	speed: 0.0316s/iter; left time: 581.6885s
Epoch: 30 cost time: 10.46989130973816
Epoch: 30, Steps: 262 | Train Loss: 0.3605996 Vali Loss: 0.9840125 Test Loss: 0.4473759
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3843774
	speed: 0.1691s/iter; left time: 3085.4489s
	iters: 200, epoch: 31 | loss: 0.3764912
	speed: 0.0502s/iter; left time: 910.6767s
Epoch: 31 cost time: 11.83630657196045
Epoch: 31, Steps: 262 | Train Loss: 0.3607451 Vali Loss: 0.9836429 Test Loss: 0.4474597
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3596099
	speed: 0.1733s/iter; left time: 3116.0132s
	iters: 200, epoch: 32 | loss: 0.3387848
	speed: 0.0428s/iter; left time: 765.6015s
Epoch: 32 cost time: 12.192274808883667
Epoch: 32, Steps: 262 | Train Loss: 0.3606588 Vali Loss: 0.9840963 Test Loss: 0.4475282
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3882854
	speed: 0.1481s/iter; left time: 2623.2409s
	iters: 200, epoch: 33 | loss: 0.3621342
	speed: 0.0395s/iter; left time: 695.0647s
Epoch: 33 cost time: 12.0716552734375
Epoch: 33, Steps: 262 | Train Loss: 0.3607102 Vali Loss: 0.9842244 Test Loss: 0.4474871
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3542248
	speed: 0.1528s/iter; left time: 2667.4452s
	iters: 200, epoch: 34 | loss: 0.3959546
	speed: 0.0308s/iter; left time: 533.8974s
Epoch: 34 cost time: 9.663904905319214
Epoch: 34, Steps: 262 | Train Loss: 0.3608333 Vali Loss: 0.9839163 Test Loss: 0.4474220
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3574525
	speed: 0.1831s/iter; left time: 3148.0957s
	iters: 200, epoch: 35 | loss: 0.3338671
	speed: 0.0500s/iter; left time: 854.2858s
Epoch: 35 cost time: 13.087067604064941
Epoch: 35, Steps: 262 | Train Loss: 0.3607582 Vali Loss: 0.9844542 Test Loss: 0.4474339
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3709659
	speed: 0.1950s/iter; left time: 3301.3583s
	iters: 200, epoch: 36 | loss: 0.3414744
	speed: 0.0445s/iter; left time: 749.0270s
Epoch: 36 cost time: 12.219395160675049
Epoch: 36, Steps: 262 | Train Loss: 0.3607217 Vali Loss: 0.9841699 Test Loss: 0.4474642
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3609131
	speed: 0.1574s/iter; left time: 2622.9291s
	iters: 200, epoch: 37 | loss: 0.3510799
	speed: 0.0425s/iter; left time: 703.6509s
Epoch: 37 cost time: 11.1909019947052
Epoch: 37, Steps: 262 | Train Loss: 0.3607118 Vali Loss: 0.9851247 Test Loss: 0.4474290
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4115092
	speed: 0.1456s/iter; left time: 2388.3674s
	iters: 200, epoch: 38 | loss: 0.3478960
	speed: 0.0472s/iter; left time: 769.1326s
Epoch: 38 cost time: 11.292861700057983
Epoch: 38, Steps: 262 | Train Loss: 0.3605694 Vali Loss: 0.9835441 Test Loss: 0.4474382
Validation loss decreased (0.983643 --> 0.983544).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.3453331
	speed: 0.1828s/iter; left time: 2951.4538s
	iters: 200, epoch: 39 | loss: 0.3538136
	speed: 0.0497s/iter; left time: 797.2254s
Epoch: 39 cost time: 13.433500051498413
Epoch: 39, Steps: 262 | Train Loss: 0.3606546 Vali Loss: 0.9846547 Test Loss: 0.4474542
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3455998
	speed: 0.1799s/iter; left time: 2857.2337s
	iters: 200, epoch: 40 | loss: 0.3744906
	speed: 0.0557s/iter; left time: 878.3398s
Epoch: 40 cost time: 13.348112344741821
Epoch: 40, Steps: 262 | Train Loss: 0.3607435 Vali Loss: 0.9852768 Test Loss: 0.4474738
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3578607
	speed: 0.1633s/iter; left time: 2550.8190s
	iters: 200, epoch: 41 | loss: 0.3506771
	speed: 0.0311s/iter; left time: 483.3453s
Epoch: 41 cost time: 9.607567548751831
Epoch: 41, Steps: 262 | Train Loss: 0.3607622 Vali Loss: 0.9839739 Test Loss: 0.4474860
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.3556985
	speed: 0.1438s/iter; left time: 2207.9396s
	iters: 200, epoch: 42 | loss: 0.3430876
	speed: 0.0435s/iter; left time: 664.2478s
Epoch: 42 cost time: 11.549450159072876
Epoch: 42, Steps: 262 | Train Loss: 0.3606906 Vali Loss: 0.9845428 Test Loss: 0.4474886
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3509615
	speed: 0.1482s/iter; left time: 2237.9266s
	iters: 200, epoch: 43 | loss: 0.3291509
	speed: 0.0554s/iter; left time: 830.3256s
Epoch: 43 cost time: 12.45742654800415
Epoch: 43, Steps: 262 | Train Loss: 0.3606158 Vali Loss: 0.9844096 Test Loss: 0.4474757
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3632160
	speed: 0.1840s/iter; left time: 2729.1656s
	iters: 200, epoch: 44 | loss: 0.3696545
	speed: 0.0423s/iter; left time: 622.6552s
Epoch: 44 cost time: 13.416250228881836
Epoch: 44, Steps: 262 | Train Loss: 0.3606413 Vali Loss: 0.9841750 Test Loss: 0.4475209
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.3807984
	speed: 0.1922s/iter; left time: 2800.2965s
	iters: 200, epoch: 45 | loss: 0.3631358
	speed: 0.0358s/iter; left time: 518.7919s
Epoch: 45 cost time: 10.386772632598877
Epoch: 45, Steps: 262 | Train Loss: 0.3605181 Vali Loss: 0.9847747 Test Loss: 0.4475123
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.3895537
	speed: 0.1625s/iter; left time: 2325.1697s
	iters: 200, epoch: 46 | loss: 0.3468941
	speed: 0.0474s/iter; left time: 673.4160s
Epoch: 46 cost time: 13.06287693977356
Epoch: 46, Steps: 262 | Train Loss: 0.3606944 Vali Loss: 0.9846365 Test Loss: 0.4474914
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3452934
	speed: 0.1552s/iter; left time: 2180.4330s
	iters: 200, epoch: 47 | loss: 0.3812494
	speed: 0.0406s/iter; left time: 565.9608s
Epoch: 47 cost time: 11.528316974639893
Epoch: 47, Steps: 262 | Train Loss: 0.3606684 Vali Loss: 0.9852885 Test Loss: 0.4475340
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.3256133
	speed: 0.1859s/iter; left time: 2563.0972s
	iters: 200, epoch: 48 | loss: 0.3230572
	speed: 0.0432s/iter; left time: 591.1589s
Epoch: 48 cost time: 11.17712950706482
Epoch: 48, Steps: 262 | Train Loss: 0.3607164 Vali Loss: 0.9848117 Test Loss: 0.4475141
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.3485817
	speed: 0.1503s/iter; left time: 2032.9622s
	iters: 200, epoch: 49 | loss: 0.3531254
	speed: 0.0471s/iter; left time: 632.5762s
Epoch: 49 cost time: 10.721229314804077
Epoch: 49, Steps: 262 | Train Loss: 0.3605390 Vali Loss: 0.9845917 Test Loss: 0.4475005
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.3407211
	speed: 0.1761s/iter; left time: 2335.3768s
	iters: 200, epoch: 50 | loss: 0.3511411
	speed: 0.0385s/iter; left time: 506.1398s
Epoch: 50 cost time: 12.27461314201355
Epoch: 50, Steps: 262 | Train Loss: 0.3608046 Vali Loss: 0.9846399 Test Loss: 0.4474967
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.3405648
	speed: 0.1604s/iter; left time: 2085.0703s
	iters: 200, epoch: 51 | loss: 0.3889384
	speed: 0.0366s/iter; left time: 471.5826s
Epoch: 51 cost time: 10.017460346221924
Epoch: 51, Steps: 262 | Train Loss: 0.3607776 Vali Loss: 0.9848981 Test Loss: 0.4474970
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.3769968
	speed: 0.1560s/iter; left time: 1987.8162s
	iters: 200, epoch: 52 | loss: 0.3361894
	speed: 0.0384s/iter; left time: 484.8950s
Epoch: 52 cost time: 11.333009481430054
Epoch: 52, Steps: 262 | Train Loss: 0.3606775 Vali Loss: 0.9841862 Test Loss: 0.4475026
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.3804530
	speed: 0.1828s/iter; left time: 2280.2777s
	iters: 200, epoch: 53 | loss: 0.3565799
	speed: 0.0467s/iter; left time: 578.1311s
Epoch: 53 cost time: 12.936237573623657
Epoch: 53, Steps: 262 | Train Loss: 0.3606236 Vali Loss: 0.9848593 Test Loss: 0.4475093
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.3513002
	speed: 0.1978s/iter; left time: 2416.6713s
	iters: 200, epoch: 54 | loss: 0.3340651
	speed: 0.0402s/iter; left time: 487.5886s
Epoch: 54 cost time: 11.245832204818726
Epoch: 54, Steps: 262 | Train Loss: 0.3606255 Vali Loss: 0.9848871 Test Loss: 0.4475168
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.3735406
	speed: 0.1477s/iter; left time: 1765.0325s
	iters: 200, epoch: 55 | loss: 0.3426643
	speed: 0.0337s/iter; left time: 399.9228s
Epoch: 55 cost time: 9.743579387664795
Epoch: 55, Steps: 262 | Train Loss: 0.3605682 Vali Loss: 0.9846206 Test Loss: 0.4475085
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.3570332
	speed: 0.1321s/iter; left time: 1544.1885s
	iters: 200, epoch: 56 | loss: 0.3477716
	speed: 0.0326s/iter; left time: 378.3940s
Epoch: 56 cost time: 9.6270432472229
Epoch: 56, Steps: 262 | Train Loss: 0.3607251 Vali Loss: 0.9848850 Test Loss: 0.4475229
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.4342094
	speed: 0.1463s/iter; left time: 1671.7114s
	iters: 200, epoch: 57 | loss: 0.3839642
	speed: 0.0440s/iter; left time: 498.7496s
Epoch: 57 cost time: 10.959625720977783
Epoch: 57, Steps: 262 | Train Loss: 0.3604679 Vali Loss: 0.9842620 Test Loss: 0.4475077
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.3356347
	speed: 0.1652s/iter; left time: 1844.6132s
	iters: 200, epoch: 58 | loss: 0.3698448
	speed: 0.0377s/iter; left time: 417.5233s
Epoch: 58 cost time: 10.582478284835815
Epoch: 58, Steps: 262 | Train Loss: 0.3606792 Vali Loss: 0.9828991 Test Loss: 0.4475285
Validation loss decreased (0.983544 --> 0.982899).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.3733467
	speed: 0.1238s/iter; left time: 1349.9375s
	iters: 200, epoch: 59 | loss: 0.3395092
	speed: 0.0246s/iter; left time: 265.7805s
Epoch: 59 cost time: 8.004297494888306
Epoch: 59, Steps: 262 | Train Loss: 0.3606378 Vali Loss: 0.9841857 Test Loss: 0.4475074
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.3856015
	speed: 0.1216s/iter; left time: 1293.8104s
	iters: 200, epoch: 60 | loss: 0.3472089
	speed: 0.0583s/iter; left time: 614.9144s
Epoch: 60 cost time: 12.241252899169922
Epoch: 60, Steps: 262 | Train Loss: 0.3606790 Vali Loss: 0.9847636 Test Loss: 0.4475067
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.3615038
	speed: 0.1283s/iter; left time: 1331.4811s
	iters: 200, epoch: 61 | loss: 0.3487355
	speed: 0.0267s/iter; left time: 274.2831s
Epoch: 61 cost time: 8.020718812942505
Epoch: 61, Steps: 262 | Train Loss: 0.3604841 Vali Loss: 0.9849524 Test Loss: 0.4475175
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.4026627
	speed: 0.1151s/iter; left time: 1164.2459s
	iters: 200, epoch: 62 | loss: 0.3618773
	speed: 0.0269s/iter; left time: 269.2743s
Epoch: 62 cost time: 7.458680868148804
Epoch: 62, Steps: 262 | Train Loss: 0.3606163 Vali Loss: 0.9847581 Test Loss: 0.4475219
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.3452435
	speed: 0.1291s/iter; left time: 1272.2060s
	iters: 200, epoch: 63 | loss: 0.3673567
	speed: 0.0300s/iter; left time: 292.3153s
Epoch: 63 cost time: 10.607460260391235
Epoch: 63, Steps: 262 | Train Loss: 0.3605489 Vali Loss: 0.9835141 Test Loss: 0.4475192
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.3750358
	speed: 0.1118s/iter; left time: 1073.1535s
	iters: 200, epoch: 64 | loss: 0.3448240
	speed: 0.0401s/iter; left time: 380.2960s
Epoch: 64 cost time: 9.520000457763672
Epoch: 64, Steps: 262 | Train Loss: 0.3606186 Vali Loss: 0.9845660 Test Loss: 0.4475313
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.3829883
	speed: 0.1196s/iter; left time: 1116.1396s
	iters: 200, epoch: 65 | loss: 0.3483856
	speed: 0.0307s/iter; left time: 283.6222s
Epoch: 65 cost time: 8.710432767868042
Epoch: 65, Steps: 262 | Train Loss: 0.3604789 Vali Loss: 0.9849393 Test Loss: 0.4475275
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.3703023
	speed: 0.2351s/iter; left time: 2132.6769s
	iters: 200, epoch: 66 | loss: 0.3629180
	speed: 0.1142s/iter; left time: 1024.7674s
Epoch: 66 cost time: 29.670021295547485
Epoch: 66, Steps: 262 | Train Loss: 0.3605392 Vali Loss: 0.9842130 Test Loss: 0.4475199
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.3494463
	speed: 0.4468s/iter; left time: 3935.4553s
	iters: 200, epoch: 67 | loss: 0.3590514
	speed: 0.0906s/iter; left time: 789.0966s
Epoch: 67 cost time: 25.720109462738037
Epoch: 67, Steps: 262 | Train Loss: 0.3605742 Vali Loss: 0.9851806 Test Loss: 0.4475198
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.3528345
	speed: 0.4146s/iter; left time: 3543.1981s
	iters: 200, epoch: 68 | loss: 0.3748523
	speed: 0.0991s/iter; left time: 837.1835s
Epoch: 68 cost time: 26.8166286945343
Epoch: 68, Steps: 262 | Train Loss: 0.3605912 Vali Loss: 0.9846475 Test Loss: 0.4475230
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.3610000
	speed: 0.4730s/iter; left time: 3918.4157s
	iters: 200, epoch: 69 | loss: 0.3551081
	speed: 0.1105s/iter; left time: 904.3478s
Epoch: 69 cost time: 28.185545682907104
Epoch: 69, Steps: 262 | Train Loss: 0.3606727 Vali Loss: 0.9844423 Test Loss: 0.4475248
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.3803258
	speed: 0.4162s/iter; left time: 3338.8232s
	iters: 200, epoch: 70 | loss: 0.3583620
	speed: 0.1098s/iter; left time: 870.0017s
Epoch: 70 cost time: 28.444210529327393
Epoch: 70, Steps: 262 | Train Loss: 0.3606152 Vali Loss: 0.9855059 Test Loss: 0.4475288
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.3835105
	speed: 0.4536s/iter; left time: 3520.2235s
	iters: 200, epoch: 71 | loss: 0.3921840
	speed: 0.1061s/iter; left time: 812.6545s
Epoch: 71 cost time: 27.90217399597168
Epoch: 71, Steps: 262 | Train Loss: 0.3607052 Vali Loss: 0.9836778 Test Loss: 0.4475317
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.3657700
	speed: 0.4517s/iter; left time: 3387.4365s
	iters: 200, epoch: 72 | loss: 0.3492875
	speed: 0.1027s/iter; left time: 759.6617s
Epoch: 72 cost time: 26.523302793502808
Epoch: 72, Steps: 262 | Train Loss: 0.3605986 Vali Loss: 0.9851704 Test Loss: 0.4475307
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.3384606
	speed: 0.4361s/iter; left time: 3156.4056s
	iters: 200, epoch: 73 | loss: 0.3658481
	speed: 0.1069s/iter; left time: 763.0306s
Epoch: 73 cost time: 28.463372468948364
Epoch: 73, Steps: 262 | Train Loss: 0.3605465 Vali Loss: 0.9846016 Test Loss: 0.4475323
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.3585482
	speed: 0.4688s/iter; left time: 3270.2184s
	iters: 200, epoch: 74 | loss: 0.3634003
	speed: 0.1092s/iter; left time: 750.6829s
Epoch: 74 cost time: 29.1577570438385
Epoch: 74, Steps: 262 | Train Loss: 0.3607889 Vali Loss: 0.9849979 Test Loss: 0.4475310
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.3825845
	speed: 0.4358s/iter; left time: 2925.6742s
	iters: 200, epoch: 75 | loss: 0.3787745
	speed: 0.0992s/iter; left time: 655.8636s
Epoch: 75 cost time: 28.040650606155396
Epoch: 75, Steps: 262 | Train Loss: 0.3606223 Vali Loss: 0.9840233 Test Loss: 0.4475299
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.3421614
	speed: 0.4656s/iter; left time: 3003.6859s
	iters: 200, epoch: 76 | loss: 0.3642242
	speed: 0.1151s/iter; left time: 731.1684s
Epoch: 76 cost time: 28.28239130973816
Epoch: 76, Steps: 262 | Train Loss: 0.3606021 Vali Loss: 0.9842378 Test Loss: 0.4475343
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.3199421
	speed: 0.4281s/iter; left time: 2649.7188s
	iters: 200, epoch: 77 | loss: 0.3239085
	speed: 0.1077s/iter; left time: 655.6635s
Epoch: 77 cost time: 28.39816164970398
Epoch: 77, Steps: 262 | Train Loss: 0.3605977 Vali Loss: 0.9846256 Test Loss: 0.4475348
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.3572482
	speed: 0.4502s/iter; left time: 2668.3107s
	iters: 200, epoch: 78 | loss: 0.3456634
	speed: 0.1029s/iter; left time: 599.6777s
Epoch: 78 cost time: 27.966886281967163
Epoch: 78, Steps: 262 | Train Loss: 0.3606263 Vali Loss: 0.9840551 Test Loss: 0.4475311
EarlyStopping counter: 20 out of 20
Early stopping
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=18, out_features=90, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1451520.0
params:  1710.0
Trainable parameters:  1710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4168859
	speed: 0.1210s/iter; left time: 3158.3023s
	iters: 200, epoch: 1 | loss: 0.4630088
	speed: 0.1033s/iter; left time: 2686.0691s
Epoch: 1 cost time: 27.753686904907227
Epoch: 1, Steps: 262 | Train Loss: 0.4438230 Vali Loss: 0.9828234 Test Loss: 0.4461643
Validation loss decreased (inf --> 0.982823).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4393682
	speed: 0.4217s/iter; left time: 10895.2181s
	iters: 200, epoch: 2 | loss: 0.4687319
	speed: 0.1021s/iter; left time: 2628.9332s
Epoch: 2 cost time: 27.486116409301758
Epoch: 2, Steps: 262 | Train Loss: 0.4435683 Vali Loss: 0.9827720 Test Loss: 0.4460324
Validation loss decreased (0.982823 --> 0.982772).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4183401
	speed: 0.4630s/iter; left time: 11841.1632s
	iters: 200, epoch: 3 | loss: 0.4458597
	speed: 0.1027s/iter; left time: 2617.6944s
Epoch: 3 cost time: 26.98643732070923
Epoch: 3, Steps: 262 | Train Loss: 0.4435457 Vali Loss: 0.9820012 Test Loss: 0.4458600
Validation loss decreased (0.982772 --> 0.982001).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4378726
	speed: 0.4383s/iter; left time: 11095.2501s
	iters: 200, epoch: 4 | loss: 0.4610706
	speed: 0.0979s/iter; left time: 2468.6618s
Epoch: 4 cost time: 26.629275798797607
Epoch: 4, Steps: 262 | Train Loss: 0.4434685 Vali Loss: 0.9813974 Test Loss: 0.4460338
Validation loss decreased (0.982001 --> 0.981397).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4751734
	speed: 0.4352s/iter; left time: 10903.9995s
	iters: 200, epoch: 5 | loss: 0.4402214
	speed: 0.1067s/iter; left time: 2663.4366s
Epoch: 5 cost time: 28.402845859527588
Epoch: 5, Steps: 262 | Train Loss: 0.4435242 Vali Loss: 0.9819165 Test Loss: 0.4459349
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4159723
	speed: 0.4683s/iter; left time: 11608.8956s
	iters: 200, epoch: 6 | loss: 0.4340222
	speed: 0.1089s/iter; left time: 2688.6175s
Epoch: 6 cost time: 29.569341897964478
Epoch: 6, Steps: 262 | Train Loss: 0.4434127 Vali Loss: 0.9821900 Test Loss: 0.4461461
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4218082
	speed: 0.4310s/iter; left time: 10572.5967s
	iters: 200, epoch: 7 | loss: 0.4359112
	speed: 0.1002s/iter; left time: 2448.6509s
Epoch: 7 cost time: 27.040295362472534
Epoch: 7, Steps: 262 | Train Loss: 0.4433942 Vali Loss: 0.9820860 Test Loss: 0.4459141
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4926101
	speed: 0.4698s/iter; left time: 11399.8594s
	iters: 200, epoch: 8 | loss: 0.4825534
	speed: 0.1051s/iter; left time: 2541.0856s
Epoch: 8 cost time: 28.224740982055664
Epoch: 8, Steps: 262 | Train Loss: 0.4434891 Vali Loss: 0.9820049 Test Loss: 0.4459806
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4342122
	speed: 0.4268s/iter; left time: 10245.6965s
	iters: 200, epoch: 9 | loss: 0.4587402
	speed: 0.1020s/iter; left time: 2438.7368s
Epoch: 9 cost time: 26.62856936454773
Epoch: 9, Steps: 262 | Train Loss: 0.4434041 Vali Loss: 0.9817274 Test Loss: 0.4462101
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4125340
	speed: 0.4368s/iter; left time: 10370.5360s
	iters: 200, epoch: 10 | loss: 0.4170154
	speed: 0.1044s/iter; left time: 2468.3934s
Epoch: 10 cost time: 26.49495029449463
Epoch: 10, Steps: 262 | Train Loss: 0.4434600 Vali Loss: 0.9816816 Test Loss: 0.4461319
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5145187
	speed: 0.4674s/iter; left time: 10974.7062s
	iters: 200, epoch: 11 | loss: 0.4626205
	speed: 0.1091s/iter; left time: 2551.0734s
Epoch: 11 cost time: 28.897894859313965
Epoch: 11, Steps: 262 | Train Loss: 0.4433019 Vali Loss: 0.9815147 Test Loss: 0.4461711
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4590575
	speed: 0.4354s/iter; left time: 10108.9264s
	iters: 200, epoch: 12 | loss: 0.4188914
	speed: 0.1129s/iter; left time: 2611.2719s
Epoch: 12 cost time: 28.842750072479248
Epoch: 12, Steps: 262 | Train Loss: 0.4434809 Vali Loss: 0.9815675 Test Loss: 0.4461221
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4253590
	speed: 0.4503s/iter; left time: 10336.7606s
	iters: 200, epoch: 13 | loss: 0.4284225
	speed: 0.1085s/iter; left time: 2479.0311s
Epoch: 13 cost time: 28.921027183532715
Epoch: 13, Steps: 262 | Train Loss: 0.4433620 Vali Loss: 0.9819943 Test Loss: 0.4461103
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4487678
	speed: 0.4450s/iter; left time: 10098.6466s
	iters: 200, epoch: 14 | loss: 0.4140515
	speed: 0.1079s/iter; left time: 2438.2198s
Epoch: 14 cost time: 26.329028606414795
Epoch: 14, Steps: 262 | Train Loss: 0.4433186 Vali Loss: 0.9819446 Test Loss: 0.4461116
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4555269
	speed: 0.4370s/iter; left time: 9803.4710s
	iters: 200, epoch: 15 | loss: 0.4272411
	speed: 0.1079s/iter; left time: 2410.5069s
Epoch: 15 cost time: 28.246991395950317
Epoch: 15, Steps: 262 | Train Loss: 0.4432609 Vali Loss: 0.9820893 Test Loss: 0.4460753
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4600805
	speed: 0.4519s/iter; left time: 10018.9119s
	iters: 200, epoch: 16 | loss: 0.4790179
	speed: 0.1098s/iter; left time: 2423.9606s
Epoch: 16 cost time: 28.751995086669922
Epoch: 16, Steps: 262 | Train Loss: 0.4433202 Vali Loss: 0.9818972 Test Loss: 0.4460407
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4852719
	speed: 0.4509s/iter; left time: 9877.7821s
	iters: 200, epoch: 17 | loss: 0.4387610
	speed: 0.0971s/iter; left time: 2116.6746s
Epoch: 17 cost time: 26.60439920425415
Epoch: 17, Steps: 262 | Train Loss: 0.4432264 Vali Loss: 0.9817994 Test Loss: 0.4462160
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4895651
	speed: 0.4564s/iter; left time: 9879.5867s
	iters: 200, epoch: 18 | loss: 0.4311889
	speed: 0.1078s/iter; left time: 2322.2158s
Epoch: 18 cost time: 29.193153142929077
Epoch: 18, Steps: 262 | Train Loss: 0.4432196 Vali Loss: 0.9816346 Test Loss: 0.4460029
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4355483
	speed: 0.4553s/iter; left time: 9737.6378s
	iters: 200, epoch: 19 | loss: 0.4343871
	speed: 0.0897s/iter; left time: 1909.3804s
Epoch: 19 cost time: 25.534823417663574
Epoch: 19, Steps: 262 | Train Loss: 0.4434827 Vali Loss: 0.9823302 Test Loss: 0.4460797
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4491772
	speed: 0.4448s/iter; left time: 9395.9938s
	iters: 200, epoch: 20 | loss: 0.4666877
	speed: 0.1125s/iter; left time: 2365.7153s
Epoch: 20 cost time: 29.337764978408813
Epoch: 20, Steps: 262 | Train Loss: 0.4432237 Vali Loss: 0.9825211 Test Loss: 0.4459580
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4691222
	speed: 0.4503s/iter; left time: 9393.3255s
	iters: 200, epoch: 21 | loss: 0.4540563
	speed: 0.1045s/iter; left time: 2169.3284s
Epoch: 21 cost time: 28.080886363983154
Epoch: 21, Steps: 262 | Train Loss: 0.4432245 Vali Loss: 0.9817386 Test Loss: 0.4460990
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4931944
	speed: 0.4493s/iter; left time: 9254.3602s
	iters: 200, epoch: 22 | loss: 0.4120844
	speed: 0.1036s/iter; left time: 2123.9590s
Epoch: 22 cost time: 27.387040376663208
Epoch: 22, Steps: 262 | Train Loss: 0.4433447 Vali Loss: 0.9816391 Test Loss: 0.4460826
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4269772
	speed: 0.3946s/iter; left time: 8025.3811s
	iters: 200, epoch: 23 | loss: 0.4486669
	speed: 0.0970s/iter; left time: 1962.8893s
Epoch: 23 cost time: 24.482859134674072
Epoch: 23, Steps: 262 | Train Loss: 0.4433264 Vali Loss: 0.9821763 Test Loss: 0.4461351
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4825239
	speed: 0.3747s/iter; left time: 7522.7114s
	iters: 200, epoch: 24 | loss: 0.4220394
	speed: 0.0767s/iter; left time: 1532.6877s
Epoch: 24 cost time: 22.542430639266968
Epoch: 24, Steps: 262 | Train Loss: 0.4432117 Vali Loss: 0.9824720 Test Loss: 0.4461040
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.44424155354499817, mae:0.42532944679260254, rse:0.634132981300354, corr:[0.52666277 0.53077847 0.5325248  0.5319808  0.52994126 0.52776754
 0.5263285  0.52607656 0.52666116 0.52762955 0.52844733 0.5283621
 0.52736396 0.5254593  0.52307665 0.5207174  0.518521   0.5164916
 0.51457125 0.5126801  0.51075566 0.5086936  0.5064741  0.5041965
 0.50199926 0.5000902  0.49869925 0.4978371  0.49745387 0.49740848
 0.49766195 0.49799734 0.498154   0.49807975 0.4977732  0.49735928
 0.4968667  0.49636218 0.49597895 0.49567625 0.49549866 0.49537405
 0.49528143 0.49526727 0.4953161  0.4954267  0.49559924 0.4957946
 0.495974   0.49605888 0.4960758  0.49608272 0.4961283  0.4961916
 0.4962492  0.49625295 0.4962679  0.4962197  0.49613094 0.49597445
 0.49581409 0.49567434 0.49551058 0.49531683 0.4951096  0.49501237
 0.4950417  0.49517244 0.49547005 0.49588305 0.49636263 0.49679145
 0.49707916 0.4971781  0.49717844 0.49707144 0.49695355 0.49686122
 0.4967965  0.49673754 0.49668035 0.49658835 0.49645886 0.49621856
 0.49592948 0.49567467 0.49552026 0.4953969  0.4952514  0.49513996
 0.4950199  0.4948225  0.4944815  0.49401072 0.493476   0.4928272
 0.4921311  0.49148765 0.49089321 0.4904018  0.49011442 0.4901339
 0.49045876 0.49106315 0.4919422  0.4930551  0.49426395 0.49538544
 0.49622446 0.49662936 0.49657732 0.49634275 0.49597937 0.49558824
 0.49518475 0.49485567 0.49453136 0.49414757 0.49372008 0.4932439
 0.49278063 0.49223033 0.4915676  0.49090937 0.49028772 0.48973826
 0.48935223 0.48917732 0.48917228 0.4892459  0.4892697  0.48911366
 0.4887441  0.48830172 0.48783755 0.4873618  0.48702952 0.48682132
 0.4866934  0.48667827 0.48671994 0.4868021  0.48691234 0.48706847
 0.48725525 0.48747382 0.48761415 0.4877005  0.48784706 0.48788664
 0.4878515  0.48782867 0.48782176 0.48789468 0.48796183 0.48808202
 0.48820868 0.4883512  0.4884043  0.48838416 0.48829037 0.48817024
 0.4880923  0.4881207  0.48825228 0.4885621  0.48898688 0.48945552
 0.48992804 0.49030304 0.4905145  0.4906685  0.49074322 0.49073657
 0.49071026 0.4906603  0.49063456 0.49065167 0.49063006 0.4905938
 0.4905275  0.49042538 0.49025744 0.49005017 0.4898416  0.4896772
 0.4896132  0.48968595 0.48988888 0.49019372 0.4904435  0.49052837
 0.49037924 0.49010026 0.4897114  0.48905787 0.48827738 0.48762676
 0.48716795 0.48683348 0.4865456  0.48651648 0.48649928 0.4863376
 0.4859854  0.48525047 0.4843271  0.48349112 0.4828009  0.48227698
 0.48182574 0.4813964  0.48086828 0.48013052 0.4791815  0.47813788
 0.47713757 0.4762173  0.47544456 0.47487825 0.4744876  0.47426307
 0.47419593 0.47427568 0.47437084 0.47443566 0.47445455 0.47433499
 0.47403228 0.47369114 0.47332087 0.47293943 0.4726233  0.47242764
 0.4723869  0.4725141  0.4726707  0.4728844  0.47310016 0.47322822
 0.47334132 0.47335857 0.47337472 0.47340262 0.4734632  0.47356158
 0.4736955  0.4737694  0.47380617 0.4738059  0.473745   0.4736164
 0.4734431  0.47331583 0.47316965 0.47303644 0.4728831  0.47273937
 0.4727446  0.47292644 0.4732514  0.4736745  0.4741056  0.4745273
 0.4749309  0.47521606 0.47541317 0.47558624 0.4757777  0.47592807
 0.47607914 0.47618312 0.47630033 0.47640657 0.4765311  0.47664315
 0.47673362 0.47688076 0.4770158  0.47710884 0.4771673  0.47716948
 0.47719106 0.4771747  0.47717434 0.47716126 0.4770208  0.47670123
 0.4760757  0.4753025  0.47457    0.47386026 0.4732739  0.47286722
 0.47278127 0.47291285 0.47329685 0.47385654 0.47441658 0.4749042
 0.47518423 0.4751072  0.47486004 0.47461528 0.47445658 0.47436035
 0.47433826 0.47429952 0.47414592 0.4738796  0.47352824 0.47315195
 0.4727899  0.47240007 0.47205654 0.47186857 0.4716769  0.4715446
 0.4714575  0.47130865 0.471086   0.4708804  0.47062594 0.4703481
 0.47005585 0.4697758  0.469492   0.46922478 0.46894678 0.4686826
 0.46845186 0.46828604 0.46813133 0.4679845  0.46790394 0.46790034
 0.46798137 0.4681092  0.46821934 0.46828756 0.4683758  0.46841526
 0.4684051  0.4683665  0.4683878  0.46848288 0.46857488 0.46866697
 0.4687047  0.46874332 0.4687924  0.46877864 0.46871954 0.4686425
 0.46859252 0.46867058 0.46881002 0.46902892 0.46927527 0.46952066
 0.46967447 0.46978578 0.469806   0.46978673 0.46975672 0.4697153
 0.46965933 0.46964964 0.46970862 0.46979606 0.46994314 0.47009763
 0.4702746  0.47045833 0.47065362 0.47080913 0.47092262 0.47104582
 0.47120634 0.47140345 0.47170436 0.47195715 0.4721335  0.47219726
 0.47208855 0.4717377  0.47138548 0.47094238 0.47043204 0.4701062
 0.47003928 0.47025812 0.47061896 0.47124958 0.4720087  0.4727886
 0.47334433 0.47352526 0.47329187 0.47295794 0.4725484  0.4721533
 0.47180367 0.47143543 0.47101185 0.47054923 0.47005403 0.4695934
 0.46923777 0.46895042 0.46873027 0.46857366 0.46846187 0.4683544
 0.46830046 0.46823207 0.4681771  0.46810436 0.46794185 0.46774456
 0.46741    0.46702746 0.4666109  0.46621826 0.4658755  0.4656647
 0.4655472  0.4655765  0.4656901  0.4658642  0.46599823 0.46612358
 0.46613035 0.46605736 0.4659258  0.46581206 0.46568754 0.46566528
 0.46568233 0.46575847 0.46592918 0.4661623  0.46638602 0.46655968
 0.46664307 0.46665823 0.46657142 0.4664727  0.46635887 0.46625128
 0.4662517  0.46639606 0.4666134  0.46686602 0.4670466  0.46725592
 0.467412   0.46747157 0.46745333 0.46744755 0.46742362 0.46741375
 0.4674488  0.4675231  0.46758875 0.4676706  0.46774638 0.4677683
 0.4677721  0.46779224 0.46786082 0.46788844 0.46790153 0.46790683
 0.4678558  0.46776548 0.46761617 0.4673167  0.46678808 0.46597746
 0.46488208 0.46363318 0.46238717 0.46125537 0.46026325 0.45952183
 0.45905143 0.45862615 0.45838988 0.45825475 0.45812598 0.45792273
 0.45775232 0.4574644  0.45699745 0.4564993  0.456107   0.4558159
 0.45561373 0.4554655  0.4552822  0.4549802  0.4545869  0.4540957
 0.4536094  0.45316654 0.4527993  0.4525933  0.45248392 0.4524773
 0.4525262  0.45259234 0.45262626 0.45265806 0.45262867 0.452493
 0.45222008 0.45190483 0.45158732 0.45123836 0.450909   0.45069063
 0.4505496  0.45048586 0.4504546  0.45039785 0.45033866 0.45034686
 0.4503455  0.45032126 0.45024657 0.45016086 0.45014334 0.45019734
 0.450311   0.45041782 0.45057425 0.45069766 0.450759   0.45074618
 0.45070475 0.45058075 0.45045817 0.45033622 0.45022923 0.45016566
 0.4501575  0.45021197 0.45031917 0.45043278 0.45053428 0.45069456
 0.45086446 0.450996   0.45109984 0.45124573 0.4513921  0.45152718
 0.45166424 0.45180368 0.45188254 0.4519188  0.4518978  0.45186877
 0.45180964 0.45186254 0.45194942 0.45205447 0.45213938 0.4521809
 0.45214412 0.45195782 0.4516504  0.45116097 0.45037535 0.44927666
 0.44788727 0.44643795 0.44517523 0.44405395 0.44316527 0.44261292
 0.44241604 0.4425488  0.4429789  0.4435418  0.44414404 0.44460437
 0.44483608 0.44472933 0.44435802 0.4439439  0.4435852  0.44330665
 0.443059   0.44284502 0.44265765 0.44244143 0.44219792 0.441863
 0.4415161  0.44112492 0.4406789  0.44031084 0.44001037 0.43978477
 0.43965945 0.4396578  0.4397175  0.43978697 0.43982786 0.43982777
 0.43967742 0.4393412  0.43894935 0.43852943 0.43817288 0.43790862
 0.43784705 0.43798053 0.43824026 0.43853518 0.43883753 0.4391033
 0.4392515  0.43925968 0.43913382 0.4388992  0.43866214 0.43848354
 0.4383591  0.4383085  0.43832088 0.43836942 0.4384583  0.43845493
 0.43833068 0.4381759  0.43799216 0.43780994 0.43772787 0.437682
 0.43771303 0.4378472  0.4380771  0.4383859  0.43872905 0.4391179
 0.43950137 0.43978077 0.44000143 0.44022334 0.44042453 0.4406147
 0.44078732 0.44095924 0.4411075  0.44121513 0.44129017 0.44133875
 0.44136164 0.4414141  0.44149563 0.44153807 0.44157782 0.4416134
 0.44161284 0.44155407 0.44134578 0.44097042 0.44037578 0.43951824
 0.43846402 0.4374789  0.4367181  0.4361796  0.4358886  0.4357594
 0.43574992 0.4359296  0.4362571  0.43667352 0.43706852 0.4375289
 0.43793005 0.43793297 0.43764293 0.4372262  0.43680573 0.43643352
 0.43610874 0.43573695 0.43527615 0.43470696 0.43406084 0.4335017
 0.43307617 0.43270648 0.43240905 0.43215016 0.43185484 0.431459
 0.43097597 0.43041775 0.4299114  0.42955384 0.42930514 0.42919803
 0.42915943 0.4290211  0.42869723 0.42806542 0.42717293 0.42622244
 0.4255902  0.4256502  0.4265808  0.4283904  0.43043125 0.43113697]
