Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=14, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=14, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1580544.0
params:  1890.0
Trainable parameters:  1890
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.1003389
	speed: 0.0168s/iter; left time: 440.0118s
	iters: 200, epoch: 1 | loss: 0.7706132
	speed: 0.0133s/iter; left time: 347.1715s
Epoch: 1 cost time: 3.8058242797851562
Epoch: 1, Steps: 263 | Train Loss: 1.0259022 Vali Loss: 1.4373976 Test Loss: 0.8968046
Validation loss decreased (inf --> 1.437398).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6230876
	speed: 0.0547s/iter; left time: 1419.3351s
	iters: 200, epoch: 2 | loss: 0.6018832
	speed: 0.0132s/iter; left time: 342.0233s
Epoch: 2 cost time: 3.721741199493408
Epoch: 2, Steps: 263 | Train Loss: 0.6099167 Vali Loss: 1.1359599 Test Loss: 0.6056825
Validation loss decreased (1.437398 --> 1.135960).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5503795
	speed: 0.0580s/iter; left time: 1490.1891s
	iters: 200, epoch: 3 | loss: 0.5142732
	speed: 0.0129s/iter; left time: 329.7404s
Epoch: 3 cost time: 3.8563449382781982
Epoch: 3, Steps: 263 | Train Loss: 0.5275397 Vali Loss: 1.0651076 Test Loss: 0.5378318
Validation loss decreased (1.135960 --> 1.065108).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4957210
	speed: 0.0572s/iter; left time: 1454.4065s
	iters: 200, epoch: 4 | loss: 0.4919893
	speed: 0.0139s/iter; left time: 352.5418s
Epoch: 4 cost time: 3.9859426021575928
Epoch: 4, Steps: 263 | Train Loss: 0.5067112 Vali Loss: 1.0371790 Test Loss: 0.5140244
Validation loss decreased (1.065108 --> 1.037179).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5154269
	speed: 0.0578s/iter; left time: 1454.3144s
	iters: 200, epoch: 5 | loss: 0.5201436
	speed: 0.0141s/iter; left time: 352.6673s
Epoch: 5 cost time: 4.072507858276367
Epoch: 5, Steps: 263 | Train Loss: 0.4994663 Vali Loss: 1.0243067 Test Loss: 0.5033991
Validation loss decreased (1.037179 --> 1.024307).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5072920
	speed: 0.0572s/iter; left time: 1423.7914s
	iters: 200, epoch: 6 | loss: 0.4516603
	speed: 0.0131s/iter; left time: 325.9175s
Epoch: 6 cost time: 3.900294780731201
Epoch: 6, Steps: 263 | Train Loss: 0.4964873 Vali Loss: 1.0195893 Test Loss: 0.4985555
Validation loss decreased (1.024307 --> 1.019589).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4682457
	speed: 0.0572s/iter; left time: 1408.3229s
	iters: 200, epoch: 7 | loss: 0.4639920
	speed: 0.0135s/iter; left time: 330.8051s
Epoch: 7 cost time: 3.893441915512085
Epoch: 7, Steps: 263 | Train Loss: 0.4952015 Vali Loss: 1.0157673 Test Loss: 0.4961666
Validation loss decreased (1.019589 --> 1.015767).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5308912
	speed: 0.0562s/iter; left time: 1368.6481s
	iters: 200, epoch: 8 | loss: 0.4815321
	speed: 0.0133s/iter; left time: 323.7604s
Epoch: 8 cost time: 3.978292942047119
Epoch: 8, Steps: 263 | Train Loss: 0.4947451 Vali Loss: 1.0155147 Test Loss: 0.4950649
Validation loss decreased (1.015767 --> 1.015515).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5209088
	speed: 0.0575s/iter; left time: 1385.2265s
	iters: 200, epoch: 9 | loss: 0.5373725
	speed: 0.0138s/iter; left time: 330.8797s
Epoch: 9 cost time: 3.960596799850464
Epoch: 9, Steps: 263 | Train Loss: 0.4945512 Vali Loss: 1.0136994 Test Loss: 0.4945736
Validation loss decreased (1.015515 --> 1.013699).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5114060
	speed: 0.0555s/iter; left time: 1322.6644s
	iters: 200, epoch: 10 | loss: 0.5547506
	speed: 0.0116s/iter; left time: 276.1268s
Epoch: 10 cost time: 3.4843928813934326
Epoch: 10, Steps: 263 | Train Loss: 0.4944058 Vali Loss: 1.0137137 Test Loss: 0.4942511
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5219009
	speed: 0.0517s/iter; left time: 1219.3263s
	iters: 200, epoch: 11 | loss: 0.4777082
	speed: 0.0125s/iter; left time: 292.4426s
Epoch: 11 cost time: 3.573676109313965
Epoch: 11, Steps: 263 | Train Loss: 0.4944536 Vali Loss: 1.0140201 Test Loss: 0.4944469
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5038347
	speed: 0.0543s/iter; left time: 1266.6397s
	iters: 200, epoch: 12 | loss: 0.4908155
	speed: 0.0118s/iter; left time: 274.2575s
Epoch: 12 cost time: 3.6380345821380615
Epoch: 12, Steps: 263 | Train Loss: 0.4944480 Vali Loss: 1.0130787 Test Loss: 0.4943197
Validation loss decreased (1.013699 --> 1.013079).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5458283
	speed: 0.0570s/iter; left time: 1314.5910s
	iters: 200, epoch: 13 | loss: 0.4480129
	speed: 0.0122s/iter; left time: 279.0898s
Epoch: 13 cost time: 3.567005157470703
Epoch: 13, Steps: 263 | Train Loss: 0.4942906 Vali Loss: 1.0140218 Test Loss: 0.4942715
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5481893
	speed: 0.0541s/iter; left time: 1231.5515s
	iters: 200, epoch: 14 | loss: 0.4824675
	speed: 0.0124s/iter; left time: 282.1489s
Epoch: 14 cost time: 3.6384565830230713
Epoch: 14, Steps: 263 | Train Loss: 0.4943342 Vali Loss: 1.0140440 Test Loss: 0.4943670
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4432364
	speed: 0.0547s/iter; left time: 1232.2930s
	iters: 200, epoch: 15 | loss: 0.5336954
	speed: 0.0123s/iter; left time: 276.1775s
Epoch: 15 cost time: 3.689786434173584
Epoch: 15, Steps: 263 | Train Loss: 0.4943199 Vali Loss: 1.0139735 Test Loss: 0.4945689
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5711460
	speed: 0.0548s/iter; left time: 1219.0399s
	iters: 200, epoch: 16 | loss: 0.4640412
	speed: 0.0121s/iter; left time: 267.0535s
Epoch: 16 cost time: 3.590130090713501
Epoch: 16, Steps: 263 | Train Loss: 0.4942876 Vali Loss: 1.0131289 Test Loss: 0.4944608
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4784939
	speed: 0.0538s/iter; left time: 1184.2200s
	iters: 200, epoch: 17 | loss: 0.4900738
	speed: 0.0121s/iter; left time: 264.1517s
Epoch: 17 cost time: 3.5061140060424805
Epoch: 17, Steps: 263 | Train Loss: 0.4942606 Vali Loss: 1.0138294 Test Loss: 0.4944457
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4726702
	speed: 0.0533s/iter; left time: 1158.1706s
	iters: 200, epoch: 18 | loss: 0.5443451
	speed: 0.0135s/iter; left time: 291.7076s
Epoch: 18 cost time: 3.8881335258483887
Epoch: 18, Steps: 263 | Train Loss: 0.4941509 Vali Loss: 1.0146730 Test Loss: 0.4945962
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4755872
	speed: 0.0568s/iter; left time: 1219.0840s
	iters: 200, epoch: 19 | loss: 0.4771186
	speed: 0.0141s/iter; left time: 300.9115s
Epoch: 19 cost time: 4.067018985748291
Epoch: 19, Steps: 263 | Train Loss: 0.4942042 Vali Loss: 1.0130610 Test Loss: 0.4945729
Validation loss decreased (1.013079 --> 1.013061).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5174168
	speed: 0.0576s/iter; left time: 1220.2994s
	iters: 200, epoch: 20 | loss: 0.5164943
	speed: 0.0131s/iter; left time: 276.3727s
Epoch: 20 cost time: 3.8337647914886475
Epoch: 20, Steps: 263 | Train Loss: 0.4941973 Vali Loss: 1.0138588 Test Loss: 0.4946966
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5043344
	speed: 0.0569s/iter; left time: 1190.9240s
	iters: 200, epoch: 21 | loss: 0.5150785
	speed: 0.0135s/iter; left time: 281.8995s
Epoch: 21 cost time: 3.921478509902954
Epoch: 21, Steps: 263 | Train Loss: 0.4942098 Vali Loss: 1.0142748 Test Loss: 0.4946695
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5318336
	speed: 0.0569s/iter; left time: 1176.6963s
	iters: 200, epoch: 22 | loss: 0.4973507
	speed: 0.0142s/iter; left time: 293.0774s
Epoch: 22 cost time: 4.081506013870239
Epoch: 22, Steps: 263 | Train Loss: 0.4941142 Vali Loss: 1.0135329 Test Loss: 0.4947331
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5069736
	speed: 0.0572s/iter; left time: 1168.4944s
	iters: 200, epoch: 23 | loss: 0.4960177
	speed: 0.0137s/iter; left time: 278.7042s
Epoch: 23 cost time: 4.052157163619995
Epoch: 23, Steps: 263 | Train Loss: 0.4940534 Vali Loss: 1.0133837 Test Loss: 0.4946204
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4967156
	speed: 0.0586s/iter; left time: 1181.7592s
	iters: 200, epoch: 24 | loss: 0.4973280
	speed: 0.0136s/iter; left time: 272.0261s
Epoch: 24 cost time: 4.052022933959961
Epoch: 24, Steps: 263 | Train Loss: 0.4940638 Vali Loss: 1.0134920 Test Loss: 0.4945739
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5361829
	speed: 0.0569s/iter; left time: 1130.8107s
	iters: 200, epoch: 25 | loss: 0.4859758
	speed: 0.0130s/iter; left time: 258.0867s
Epoch: 25 cost time: 3.8670897483825684
Epoch: 25, Steps: 263 | Train Loss: 0.4943238 Vali Loss: 1.0131930 Test Loss: 0.4947127
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4381017
	speed: 0.0583s/iter; left time: 1143.7972s
	iters: 200, epoch: 26 | loss: 0.4611147
	speed: 0.0127s/iter; left time: 248.1444s
Epoch: 26 cost time: 3.778700828552246
Epoch: 26, Steps: 263 | Train Loss: 0.4940498 Vali Loss: 1.0136633 Test Loss: 0.4948045
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4564025
	speed: 0.0565s/iter; left time: 1094.8907s
	iters: 200, epoch: 27 | loss: 0.4637533
	speed: 0.0130s/iter; left time: 251.1459s
Epoch: 27 cost time: 3.9072771072387695
Epoch: 27, Steps: 263 | Train Loss: 0.4940057 Vali Loss: 1.0138762 Test Loss: 0.4947940
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4948486
	speed: 0.0568s/iter; left time: 1084.8230s
	iters: 200, epoch: 28 | loss: 0.4963609
	speed: 0.0130s/iter; left time: 246.9906s
Epoch: 28 cost time: 3.8032009601593018
Epoch: 28, Steps: 263 | Train Loss: 0.4938436 Vali Loss: 1.0139945 Test Loss: 0.4947386
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4978145
	speed: 0.0564s/iter; left time: 1061.7576s
	iters: 200, epoch: 29 | loss: 0.4991558
	speed: 0.0129s/iter; left time: 241.3040s
Epoch: 29 cost time: 3.78023362159729
Epoch: 29, Steps: 263 | Train Loss: 0.4940146 Vali Loss: 1.0133659 Test Loss: 0.4947079
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4929417
	speed: 0.0571s/iter; left time: 1060.6057s
	iters: 200, epoch: 30 | loss: 0.5003807
	speed: 0.0139s/iter; left time: 256.5987s
Epoch: 30 cost time: 3.9326436519622803
Epoch: 30, Steps: 263 | Train Loss: 0.4940417 Vali Loss: 1.0135280 Test Loss: 0.4948216
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4674217
	speed: 0.0563s/iter; left time: 1030.9508s
	iters: 200, epoch: 31 | loss: 0.5072666
	speed: 0.0130s/iter; left time: 237.3497s
Epoch: 31 cost time: 3.842421054840088
Epoch: 31, Steps: 263 | Train Loss: 0.4939525 Vali Loss: 1.0132074 Test Loss: 0.4948199
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4861847
	speed: 0.0578s/iter; left time: 1043.2674s
	iters: 200, epoch: 32 | loss: 0.5066671
	speed: 0.0133s/iter; left time: 239.0472s
Epoch: 32 cost time: 3.9733896255493164
Epoch: 32, Steps: 263 | Train Loss: 0.4937918 Vali Loss: 1.0141325 Test Loss: 0.4948158
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.5160061
	speed: 0.0548s/iter; left time: 974.5722s
	iters: 200, epoch: 33 | loss: 0.4836181
	speed: 0.0137s/iter; left time: 242.9261s
Epoch: 33 cost time: 3.8358569145202637
Epoch: 33, Steps: 263 | Train Loss: 0.4940117 Vali Loss: 1.0148666 Test Loss: 0.4948605
EarlyStopping counter: 14 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4863816
	speed: 0.0597s/iter; left time: 1046.5833s
	iters: 200, epoch: 34 | loss: 0.4902799
	speed: 0.0142s/iter; left time: 247.9431s
Epoch: 34 cost time: 4.156623363494873
Epoch: 34, Steps: 263 | Train Loss: 0.4940069 Vali Loss: 1.0138605 Test Loss: 0.4949157
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5461689
	speed: 0.0567s/iter; left time: 977.8311s
	iters: 200, epoch: 35 | loss: 0.5111874
	speed: 0.0144s/iter; left time: 246.5199s
Epoch: 35 cost time: 3.9624264240264893
Epoch: 35, Steps: 263 | Train Loss: 0.4941200 Vali Loss: 1.0140086 Test Loss: 0.4948419
EarlyStopping counter: 16 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.5505542
	speed: 0.0559s/iter; left time: 950.5038s
	iters: 200, epoch: 36 | loss: 0.5457880
	speed: 0.0134s/iter; left time: 226.2390s
Epoch: 36 cost time: 3.788745164871216
Epoch: 36, Steps: 263 | Train Loss: 0.4939824 Vali Loss: 1.0140321 Test Loss: 0.4948190
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.5058751
	speed: 0.0560s/iter; left time: 936.8331s
	iters: 200, epoch: 37 | loss: 0.5131339
	speed: 0.0133s/iter; left time: 221.9271s
Epoch: 37 cost time: 3.8911499977111816
Epoch: 37, Steps: 263 | Train Loss: 0.4940606 Vali Loss: 1.0143030 Test Loss: 0.4948887
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4667466
	speed: 0.0558s/iter; left time: 919.0774s
	iters: 200, epoch: 38 | loss: 0.5209107
	speed: 0.0143s/iter; left time: 234.6468s
Epoch: 38 cost time: 3.9311931133270264
Epoch: 38, Steps: 263 | Train Loss: 0.4939734 Vali Loss: 1.0145711 Test Loss: 0.4949590
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4596470
	speed: 0.0553s/iter; left time: 896.2321s
	iters: 200, epoch: 39 | loss: 0.5037075
	speed: 0.0132s/iter; left time: 212.9569s
Epoch: 39 cost time: 3.854515552520752
Epoch: 39, Steps: 263 | Train Loss: 0.4940783 Vali Loss: 1.0139635 Test Loss: 0.4949204
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.49270978569984436, mae:0.4527982175350189, rse:0.6678306460380554, corr:[0.5306574  0.5318243  0.52698404 0.52157193 0.519166   0.518913
 0.51772285 0.51469594 0.51114386 0.50880677 0.5080655  0.50780725
 0.50678843 0.5045961  0.5014724  0.49800056 0.49492612 0.492402
 0.48994753 0.48706082 0.4836855  0.4800452  0.47648516 0.473176
 0.470057   0.4668258  0.4635806  0.46069068 0.4586062  0.45721757
 0.45651862 0.45672125 0.4570938  0.45715034 0.456946   0.4568963
 0.45692992 0.4571618  0.45741183 0.45739713 0.45718315 0.4567252
 0.4563629  0.45626688 0.45621163 0.45612028 0.45601112 0.45591503
 0.45597696 0.45633212 0.45695347 0.45754182 0.45802793 0.45827642
 0.45851305 0.4588083  0.45928282 0.45979255 0.45992094 0.45970446
 0.45931292 0.45874903 0.45838538 0.45821136 0.4583693  0.4585355
 0.45835182 0.4579375  0.45783812 0.45833704 0.45919287 0.46016848
 0.4610712  0.461654   0.4621285  0.46256694 0.4633472  0.4644625
 0.46556795 0.46642947 0.46697852 0.46718782 0.46737468 0.4675721
 0.4679917  0.46863896 0.4694394  0.4702004  0.4708318  0.47142515
 0.47206202 0.47277963 0.47357127 0.47424975 0.47457308 0.47424188
 0.47334296 0.47238582 0.4716033  0.4710656  0.4707514  0.47042486
 0.4699491  0.46920565 0.4685218  0.4680371  0.4675788  0.46699774
 0.4662503  0.4653954  0.46453387 0.46376044 0.4630758  0.46239558
 0.4615295  0.46050256 0.45930052 0.45805195 0.45689282 0.45586142
 0.4549226  0.45379493 0.45243078 0.45118284 0.4502     0.4494697
 0.44898617 0.44868296 0.44848254 0.44829956 0.44810528 0.4479553
 0.44778094 0.44769737 0.44759056 0.44738674 0.44711387 0.44687018
 0.44667593 0.44652438 0.44639152 0.446329   0.44631925 0.44640282
 0.4465362  0.4467548  0.44703278 0.44721997 0.4474695  0.44767588
 0.4478316  0.4481718  0.44859946 0.44895107 0.44902068 0.4489105
 0.44872487 0.448643   0.44869578 0.44885984 0.44911262 0.4492518
 0.44921124 0.44910356 0.44918162 0.44960546 0.4504398  0.45152745
 0.45259434 0.4534209  0.45401186 0.4546479  0.45536745 0.45616758
 0.45687652 0.4574557  0.4579087  0.45823154 0.4583515  0.45844626
 0.45863137 0.45903128 0.45960474 0.46032092 0.46105984 0.4616199
 0.46190214 0.4620054  0.46215567 0.46253806 0.4631039  0.46367228
 0.4641305  0.46477565 0.46570915 0.46685097 0.46792275 0.46870756
 0.46893215 0.4684426  0.4675848  0.4668295  0.4661304  0.46531886
 0.4642398  0.46288866 0.46133578 0.45982537 0.45839775 0.45712724
 0.45585307 0.45442566 0.45283198 0.45108768 0.44927832 0.4477266
 0.44627982 0.44483525 0.44326332 0.44171366 0.44027635 0.43918493
 0.43864527 0.4387457  0.43895802 0.4389353  0.43882015 0.4385198
 0.4381861  0.4381109  0.43820643 0.43815958 0.4378991  0.4374165
 0.43693483 0.43662575 0.43641078 0.4363968  0.43656042 0.4365357
 0.43646082 0.43637225 0.43646145 0.43690428 0.43741432 0.43793067
 0.43833414 0.438674   0.43899986 0.43949237 0.43991953 0.4402589
 0.4402823  0.4401254  0.4397857  0.43970072 0.43974987 0.43982837
 0.44004938 0.44043082 0.4406983  0.44097102 0.44142067 0.4420956
 0.44313347 0.44415152 0.4450172  0.44584867 0.44676575 0.44760138
 0.44843408 0.44917682 0.44990203 0.45051306 0.45099658 0.451375
 0.45174763 0.452237   0.45278108 0.45345742 0.45420873 0.4549264
 0.4555675  0.45604083 0.45634305 0.45640415 0.45604175 0.45500305
 0.4531542  0.45089176 0.44880772 0.44722497 0.4461727  0.44556457
 0.44523585 0.44481146 0.44445223 0.4441602  0.44373417 0.44305706
 0.44211087 0.44098228 0.43984416 0.43876976 0.4378672  0.43703857
 0.43634886 0.4355553  0.4346499  0.43372333 0.43286395 0.4320513
 0.4311783  0.43019003 0.42919254 0.42834485 0.42752832 0.4269051
 0.426489   0.42620623 0.42597938 0.42574522 0.4254662  0.42524445
 0.42503184 0.4249344  0.42484018 0.42466858 0.42434835 0.42389256
 0.42358974 0.42339286 0.42329657 0.4231919  0.4231232  0.4230388
 0.42300427 0.4229015  0.4229949  0.42315423 0.42346862 0.4238171
 0.42414528 0.4245609  0.42497545 0.42535728 0.4256055  0.42567736
 0.4255798  0.42536825 0.4252179  0.42514855 0.42515692 0.4252615
 0.4254128  0.42558023 0.42580444 0.42613006 0.42659786 0.4271956
 0.42781842 0.4284074  0.4289888  0.42958125 0.43035924 0.43126956
 0.43222746 0.4332689  0.43427345 0.4350568  0.43574676 0.43636435
 0.4370988  0.4380047  0.43923223 0.4406804  0.44209766 0.44330522
 0.44416258 0.4446984  0.4451081  0.44554737 0.44595903 0.4460809
 0.44580448 0.4453728  0.4452564  0.4456635  0.44650316 0.44761512
 0.44854614 0.44883734 0.44871554 0.44846487 0.4481751  0.4477592
 0.44711933 0.4461951  0.44497648 0.44368842 0.44252357 0.4416709
 0.44105986 0.4404311  0.4396242  0.43853283 0.43713316 0.4356885
 0.4345654  0.43367863 0.4330526  0.43280402 0.43257934 0.4321787
 0.431697   0.4311674  0.43079916 0.43052974 0.4302314  0.42996708
 0.4297107  0.42956564 0.4294617  0.42938927 0.42932227 0.4292315
 0.42906573 0.42890647 0.4286176  0.42847583 0.4284592  0.42847455
 0.4283887  0.4281711  0.42805427 0.428216   0.4285702  0.42904776
 0.42948422 0.429874   0.43018472 0.43047243 0.43071392 0.43087053
 0.4308862  0.43089786 0.43082651 0.4307798  0.4307792  0.43092048
 0.431143   0.43149284 0.4318213  0.43213707 0.43237945 0.43268275
 0.43308938 0.43350977 0.43397316 0.43458652 0.43527964 0.43596599
 0.43666208 0.43745455 0.43817556 0.43878978 0.43930414 0.4397863
 0.44034877 0.44103426 0.4419492  0.4429674  0.4439683  0.44484717
 0.44544616 0.44576702 0.44586864 0.44583628 0.4455602  0.44488364
 0.44364417 0.4422324  0.44095424 0.44018683 0.43985888 0.43990934
 0.439813   0.4391814  0.43831125 0.43754825 0.43691975 0.4363312
 0.43560424 0.43459174 0.43327108 0.43176737 0.4302666  0.42888317
 0.42762715 0.42653233 0.4255309  0.42443    0.423328   0.42222717
 0.42123836 0.42030582 0.41944274 0.41871378 0.41805458 0.41749933
 0.41701078 0.41672355 0.41661325 0.41659045 0.41649646 0.41633785
 0.41607603 0.4158981  0.41589323 0.41582605 0.4156574  0.41538784
 0.41503942 0.41479376 0.41459316 0.414518   0.41452777 0.41462442
 0.41459295 0.41438907 0.41411516 0.41396433 0.41406673 0.4143741
 0.4147733  0.41501182 0.4151303  0.41522652 0.41537476 0.4155302
 0.4158173  0.41604382 0.4160135  0.4157009  0.41523784 0.4149632
 0.41502196 0.41534907 0.4157534  0.41605052 0.41618168 0.41635057
 0.41669032 0.41718128 0.41789836 0.41879305 0.41973835 0.4205658
 0.42129153 0.42207158 0.42290244 0.42367607 0.42436653 0.4249598
 0.42551288 0.42622122 0.42705297 0.42803797 0.4290971  0.43002936
 0.43065226 0.43086067 0.43074286 0.43038076 0.42978275 0.42878532
 0.42719302 0.4252361  0.4235806  0.42230704 0.42177027 0.42191845
 0.42235804 0.42251533 0.42236972 0.42212644 0.42180744 0.4212922
 0.42043024 0.41916588 0.41754612 0.41589016 0.4144948  0.41334614
 0.41237974 0.41147402 0.410526   0.40943655 0.40826356 0.40711883
 0.40612975 0.40526554 0.40451163 0.4038383  0.40308347 0.40231952
 0.4016765  0.40139335 0.40141422 0.40160972 0.401708   0.40165487
 0.40137544 0.40107706 0.40096536 0.40102485 0.40125942 0.40134874
 0.4012119  0.4009328  0.40060827 0.40052703 0.4008216  0.40134868
 0.40167904 0.40164202 0.4014188  0.40108684 0.40096822 0.40123084
 0.40162134 0.40182826 0.40182334 0.40156046 0.4012848  0.401111
 0.40113413 0.40127093 0.40129855 0.4009837  0.40056694 0.40032223
 0.4004002  0.40075672 0.4012085  0.40160847 0.40178782 0.4019336
 0.4022655  0.4028486  0.40360984 0.40446955 0.4051461  0.40563318
 0.40602633 0.40667793 0.40767467 0.40884012 0.40987718 0.4105927
 0.41097903 0.41122812 0.41170824 0.41254526 0.41365638 0.41477504
 0.41552576 0.41567877 0.41524836 0.4144425  0.41349795 0.4124796
 0.41114923 0.4095714  0.40809017 0.40704882 0.4069451  0.40786675
 0.40917552 0.40997437 0.41010222 0.4098303  0.4094759  0.40912932
 0.4086639  0.4078029  0.40642187 0.4046529  0.4028243  0.40122828
 0.40015534 0.3995202  0.39895946 0.3980847  0.39673784 0.3951508
 0.39368537 0.39272192 0.39225572 0.39197364 0.3914527  0.3904618
 0.3891776  0.38800436 0.38748017 0.38746753 0.3874056  0.38710696
 0.38645834 0.3857558  0.38547862 0.38576224 0.3862833  0.38648918
 0.3860647  0.38536555 0.3849355  0.38533428 0.3870324  0.38877928]
