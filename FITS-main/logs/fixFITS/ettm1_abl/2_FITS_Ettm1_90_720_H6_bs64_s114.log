Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=16, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=16, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2064384.0
params:  2448.0
Trainable parameters:  2448
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.0261446
	speed: 0.0198s/iter; left time: 518.3610s
	iters: 200, epoch: 1 | loss: 0.7346631
	speed: 0.0141s/iter; left time: 368.1673s
Epoch: 1 cost time: 4.224526405334473
Epoch: 1, Steps: 263 | Train Loss: 0.9705869 Vali Loss: 1.4754945 Test Loss: 0.9529816
Validation loss decreased (inf --> 1.475495).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5804486
	speed: 0.0604s/iter; left time: 1567.8055s
	iters: 200, epoch: 2 | loss: 0.4601840
	speed: 0.0116s/iter; left time: 300.8525s
Epoch: 2 cost time: 3.7007808685302734
Epoch: 2, Steps: 263 | Train Loss: 0.5672605 Vali Loss: 1.1563650 Test Loss: 0.6370240
Validation loss decreased (1.475495 --> 1.156365).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4639364
	speed: 0.0571s/iter; left time: 1467.1314s
	iters: 200, epoch: 3 | loss: 0.4586291
	speed: 0.0119s/iter; left time: 305.0867s
Epoch: 3 cost time: 3.6615946292877197
Epoch: 3, Steps: 263 | Train Loss: 0.4817208 Vali Loss: 1.0801046 Test Loss: 0.5612563
Validation loss decreased (1.156365 --> 1.080105).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4466118
	speed: 0.0597s/iter; left time: 1517.1276s
	iters: 200, epoch: 4 | loss: 0.4835833
	speed: 0.0122s/iter; left time: 309.4976s
Epoch: 4 cost time: 3.7520968914031982
Epoch: 4, Steps: 263 | Train Loss: 0.4602027 Vali Loss: 1.0522738 Test Loss: 0.5342810
Validation loss decreased (1.080105 --> 1.052274).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4088259
	speed: 0.0577s/iter; left time: 1450.5273s
	iters: 200, epoch: 5 | loss: 0.4588929
	speed: 0.0118s/iter; left time: 294.4338s
Epoch: 5 cost time: 3.6682958602905273
Epoch: 5, Steps: 263 | Train Loss: 0.4520794 Vali Loss: 1.0380780 Test Loss: 0.5200933
Validation loss decreased (1.052274 --> 1.038078).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4611806
	speed: 0.0581s/iter; left time: 1446.4645s
	iters: 200, epoch: 6 | loss: 0.4508547
	speed: 0.0115s/iter; left time: 284.8571s
Epoch: 6 cost time: 3.6785731315612793
Epoch: 6, Steps: 263 | Train Loss: 0.4479990 Vali Loss: 1.0304525 Test Loss: 0.5120670
Validation loss decreased (1.038078 --> 1.030452).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4556926
	speed: 0.0587s/iter; left time: 1445.8426s
	iters: 200, epoch: 7 | loss: 0.4181339
	speed: 0.0120s/iter; left time: 294.3976s
Epoch: 7 cost time: 3.676772356033325
Epoch: 7, Steps: 263 | Train Loss: 0.4458276 Vali Loss: 1.0245230 Test Loss: 0.5065814
Validation loss decreased (1.030452 --> 1.024523).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4791842
	speed: 0.0586s/iter; left time: 1427.3353s
	iters: 200, epoch: 8 | loss: 0.4857751
	speed: 0.0116s/iter; left time: 282.1189s
Epoch: 8 cost time: 3.6910853385925293
Epoch: 8, Steps: 263 | Train Loss: 0.4443821 Vali Loss: 1.0222552 Test Loss: 0.5031325
Validation loss decreased (1.024523 --> 1.022255).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4541831
	speed: 0.0569s/iter; left time: 1370.2113s
	iters: 200, epoch: 9 | loss: 0.4429306
	speed: 0.0120s/iter; left time: 287.5566s
Epoch: 9 cost time: 3.617177724838257
Epoch: 9, Steps: 263 | Train Loss: 0.4435139 Vali Loss: 1.0190735 Test Loss: 0.5009307
Validation loss decreased (1.022255 --> 1.019073).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4311997
	speed: 0.0592s/iter; left time: 1410.4745s
	iters: 200, epoch: 10 | loss: 0.4423213
	speed: 0.0134s/iter; left time: 319.1487s
Epoch: 10 cost time: 3.9894022941589355
Epoch: 10, Steps: 263 | Train Loss: 0.4429420 Vali Loss: 1.0179732 Test Loss: 0.4991639
Validation loss decreased (1.019073 --> 1.017973).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4812248
	speed: 0.0624s/iter; left time: 1470.1832s
	iters: 200, epoch: 11 | loss: 0.4546847
	speed: 0.0122s/iter; left time: 287.1955s
Epoch: 11 cost time: 3.6896214485168457
Epoch: 11, Steps: 263 | Train Loss: 0.4426321 Vali Loss: 1.0172594 Test Loss: 0.4979545
Validation loss decreased (1.017973 --> 1.017259).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4455329
	speed: 0.0579s/iter; left time: 1349.3513s
	iters: 200, epoch: 12 | loss: 0.4301878
	speed: 0.0120s/iter; left time: 279.3192s
Epoch: 12 cost time: 3.649434804916382
Epoch: 12, Steps: 263 | Train Loss: 0.4422286 Vali Loss: 1.0163802 Test Loss: 0.4973915
Validation loss decreased (1.017259 --> 1.016380).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4325471
	speed: 0.0597s/iter; left time: 1376.1498s
	iters: 200, epoch: 13 | loss: 0.4636716
	speed: 0.0116s/iter; left time: 265.2380s
Epoch: 13 cost time: 3.6766679286956787
Epoch: 13, Steps: 263 | Train Loss: 0.4421360 Vali Loss: 1.0158842 Test Loss: 0.4967549
Validation loss decreased (1.016380 --> 1.015884).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4144003
	speed: 0.0599s/iter; left time: 1363.7299s
	iters: 200, epoch: 14 | loss: 0.4576837
	speed: 0.0118s/iter; left time: 267.4533s
Epoch: 14 cost time: 3.698904514312744
Epoch: 14, Steps: 263 | Train Loss: 0.4421053 Vali Loss: 1.0166748 Test Loss: 0.4964210
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4470181
	speed: 0.0582s/iter; left time: 1311.3108s
	iters: 200, epoch: 15 | loss: 0.4452912
	speed: 0.0121s/iter; left time: 271.6715s
Epoch: 15 cost time: 3.669485330581665
Epoch: 15, Steps: 263 | Train Loss: 0.4420696 Vali Loss: 1.0165249 Test Loss: 0.4962879
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4810822
	speed: 0.0580s/iter; left time: 1291.0637s
	iters: 200, epoch: 16 | loss: 0.4339566
	speed: 0.0119s/iter; left time: 263.6560s
Epoch: 16 cost time: 3.65728497505188
Epoch: 16, Steps: 263 | Train Loss: 0.4420467 Vali Loss: 1.0159987 Test Loss: 0.4962122
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4229851
	speed: 0.0582s/iter; left time: 1280.8860s
	iters: 200, epoch: 17 | loss: 0.4374423
	speed: 0.0120s/iter; left time: 262.9374s
Epoch: 17 cost time: 3.7190089225769043
Epoch: 17, Steps: 263 | Train Loss: 0.4420488 Vali Loss: 1.0158596 Test Loss: 0.4960158
Validation loss decreased (1.015884 --> 1.015860).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4630772
	speed: 0.0582s/iter; left time: 1265.0699s
	iters: 200, epoch: 18 | loss: 0.4162508
	speed: 0.0125s/iter; left time: 270.4153s
Epoch: 18 cost time: 3.8261826038360596
Epoch: 18, Steps: 263 | Train Loss: 0.4420577 Vali Loss: 1.0158224 Test Loss: 0.4959985
Validation loss decreased (1.015860 --> 1.015822).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4373378
	speed: 0.0649s/iter; left time: 1394.1092s
	iters: 200, epoch: 19 | loss: 0.4664730
	speed: 0.0124s/iter; left time: 264.9585s
Epoch: 19 cost time: 3.8794548511505127
Epoch: 19, Steps: 263 | Train Loss: 0.4420616 Vali Loss: 1.0160382 Test Loss: 0.4958995
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3921457
	speed: 0.0594s/iter; left time: 1258.5721s
	iters: 200, epoch: 20 | loss: 0.4618300
	speed: 0.0124s/iter; left time: 261.9795s
Epoch: 20 cost time: 3.7988219261169434
Epoch: 20, Steps: 263 | Train Loss: 0.4419707 Vali Loss: 1.0161761 Test Loss: 0.4958990
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4244930
	speed: 0.0612s/iter; left time: 1281.0901s
	iters: 200, epoch: 21 | loss: 0.4683171
	speed: 0.0121s/iter; left time: 251.3742s
Epoch: 21 cost time: 3.83613657951355
Epoch: 21, Steps: 263 | Train Loss: 0.4420178 Vali Loss: 1.0160228 Test Loss: 0.4959953
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4507236
	speed: 0.0616s/iter; left time: 1273.1593s
	iters: 200, epoch: 22 | loss: 0.4097562
	speed: 0.0134s/iter; left time: 275.8757s
Epoch: 22 cost time: 4.107872247695923
Epoch: 22, Steps: 263 | Train Loss: 0.4419249 Vali Loss: 1.0161197 Test Loss: 0.4960437
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4482950
	speed: 0.0631s/iter; left time: 1288.3159s
	iters: 200, epoch: 23 | loss: 0.4277537
	speed: 0.0144s/iter; left time: 292.1128s
Epoch: 23 cost time: 4.257253170013428
Epoch: 23, Steps: 263 | Train Loss: 0.4420931 Vali Loss: 1.0161527 Test Loss: 0.4959489
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5040487
	speed: 0.0632s/iter; left time: 1272.9962s
	iters: 200, epoch: 24 | loss: 0.4372821
	speed: 0.0114s/iter; left time: 228.1391s
Epoch: 24 cost time: 3.6197776794433594
Epoch: 24, Steps: 263 | Train Loss: 0.4420570 Vali Loss: 1.0163100 Test Loss: 0.4960161
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4473126
	speed: 0.0576s/iter; left time: 1145.6490s
	iters: 200, epoch: 25 | loss: 0.4210949
	speed: 0.0135s/iter; left time: 266.5534s
Epoch: 25 cost time: 3.9620959758758545
Epoch: 25, Steps: 263 | Train Loss: 0.4420877 Vali Loss: 1.0159477 Test Loss: 0.4961236
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4007340
	speed: 0.0627s/iter; left time: 1230.6031s
	iters: 200, epoch: 26 | loss: 0.4214699
	speed: 0.0121s/iter; left time: 235.7247s
Epoch: 26 cost time: 3.7353992462158203
Epoch: 26, Steps: 263 | Train Loss: 0.4419831 Vali Loss: 1.0168501 Test Loss: 0.4959789
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4677490
	speed: 0.0584s/iter; left time: 1131.1421s
	iters: 200, epoch: 27 | loss: 0.4278847
	speed: 0.0121s/iter; left time: 233.5768s
Epoch: 27 cost time: 3.6860005855560303
Epoch: 27, Steps: 263 | Train Loss: 0.4419056 Vali Loss: 1.0167353 Test Loss: 0.4959823
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4087694
	speed: 0.0628s/iter; left time: 1198.9757s
	iters: 200, epoch: 28 | loss: 0.4601789
	speed: 0.0133s/iter; left time: 253.0522s
Epoch: 28 cost time: 4.30844783782959
Epoch: 28, Steps: 263 | Train Loss: 0.4420468 Vali Loss: 1.0169677 Test Loss: 0.4959956
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4490255
	speed: 0.0625s/iter; left time: 1177.0150s
	iters: 200, epoch: 29 | loss: 0.4527008
	speed: 0.0122s/iter; left time: 229.4465s
Epoch: 29 cost time: 3.913381576538086
Epoch: 29, Steps: 263 | Train Loss: 0.4420457 Vali Loss: 1.0167179 Test Loss: 0.4960597
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4953847
	speed: 0.0605s/iter; left time: 1124.4090s
	iters: 200, epoch: 30 | loss: 0.4186179
	speed: 0.0126s/iter; left time: 232.7863s
Epoch: 30 cost time: 3.897221326828003
Epoch: 30, Steps: 263 | Train Loss: 0.4418959 Vali Loss: 1.0159991 Test Loss: 0.4960331
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4452853
	speed: 0.0607s/iter; left time: 1110.6664s
	iters: 200, epoch: 31 | loss: 0.4048256
	speed: 0.0124s/iter; left time: 226.5208s
Epoch: 31 cost time: 3.81809663772583
Epoch: 31, Steps: 263 | Train Loss: 0.4419434 Vali Loss: 1.0152315 Test Loss: 0.4959907
Validation loss decreased (1.015822 --> 1.015231).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4775233
	speed: 0.0608s/iter; left time: 1097.0620s
	iters: 200, epoch: 32 | loss: 0.3846736
	speed: 0.0123s/iter; left time: 221.1080s
Epoch: 32 cost time: 3.8503236770629883
Epoch: 32, Steps: 263 | Train Loss: 0.4419473 Vali Loss: 1.0160179 Test Loss: 0.4961330
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4434611
	speed: 0.0593s/iter; left time: 1055.1719s
	iters: 200, epoch: 33 | loss: 0.3965184
	speed: 0.0138s/iter; left time: 244.8221s
Epoch: 33 cost time: 4.071716070175171
Epoch: 33, Steps: 263 | Train Loss: 0.4420482 Vali Loss: 1.0159901 Test Loss: 0.4960768
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4490863
	speed: 0.0668s/iter; left time: 1170.5370s
	iters: 200, epoch: 34 | loss: 0.3985570
	speed: 0.0143s/iter; left time: 249.1947s
Epoch: 34 cost time: 4.455785751342773
Epoch: 34, Steps: 263 | Train Loss: 0.4419308 Vali Loss: 1.0160310 Test Loss: 0.4961490
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4547553
	speed: 0.0662s/iter; left time: 1142.3237s
	iters: 200, epoch: 35 | loss: 0.4846912
	speed: 0.0146s/iter; left time: 250.3646s
Epoch: 35 cost time: 4.452222585678101
Epoch: 35, Steps: 263 | Train Loss: 0.4419998 Vali Loss: 1.0163611 Test Loss: 0.4961343
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4647804
	speed: 0.0605s/iter; left time: 1027.7702s
	iters: 200, epoch: 36 | loss: 0.4623101
	speed: 0.0126s/iter; left time: 212.8315s
Epoch: 36 cost time: 3.8237075805664062
Epoch: 36, Steps: 263 | Train Loss: 0.4419965 Vali Loss: 1.0167478 Test Loss: 0.4960907
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.5070333
	speed: 0.0622s/iter; left time: 1040.7616s
	iters: 200, epoch: 37 | loss: 0.4441797
	speed: 0.0123s/iter; left time: 204.1314s
Epoch: 37 cost time: 3.8869383335113525
Epoch: 37, Steps: 263 | Train Loss: 0.4419181 Vali Loss: 1.0155221 Test Loss: 0.4961411
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4381540
	speed: 0.0594s/iter; left time: 979.0501s
	iters: 200, epoch: 38 | loss: 0.4655218
	speed: 0.0122s/iter; left time: 199.7589s
Epoch: 38 cost time: 3.80578875541687
Epoch: 38, Steps: 263 | Train Loss: 0.4420071 Vali Loss: 1.0158007 Test Loss: 0.4961852
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4691020
	speed: 0.0602s/iter; left time: 975.7908s
	iters: 200, epoch: 39 | loss: 0.4533358
	speed: 0.0125s/iter; left time: 201.0068s
Epoch: 39 cost time: 3.872025489807129
Epoch: 39, Steps: 263 | Train Loss: 0.4419184 Vali Loss: 1.0171740 Test Loss: 0.4961433
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4303387
	speed: 0.0601s/iter; left time: 957.9819s
	iters: 200, epoch: 40 | loss: 0.4850358
	speed: 0.0121s/iter; left time: 192.4933s
Epoch: 40 cost time: 3.741291046142578
Epoch: 40, Steps: 263 | Train Loss: 0.4419176 Vali Loss: 1.0168517 Test Loss: 0.4962470
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4259903
	speed: 0.0618s/iter; left time: 968.8956s
	iters: 200, epoch: 41 | loss: 0.4573327
	speed: 0.0132s/iter; left time: 205.6937s
Epoch: 41 cost time: 3.9237804412841797
Epoch: 41, Steps: 263 | Train Loss: 0.4419667 Vali Loss: 1.0163745 Test Loss: 0.4961569
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.3995126
	speed: 0.0594s/iter; left time: 916.2501s
	iters: 200, epoch: 42 | loss: 0.3887521
	speed: 0.0119s/iter; left time: 183.0031s
Epoch: 42 cost time: 3.8298633098602295
Epoch: 42, Steps: 263 | Train Loss: 0.4418402 Vali Loss: 1.0158409 Test Loss: 0.4962257
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4678296
	speed: 0.0606s/iter; left time: 918.4739s
	iters: 200, epoch: 43 | loss: 0.4299316
	speed: 0.0121s/iter; left time: 182.7119s
Epoch: 43 cost time: 3.7670235633850098
Epoch: 43, Steps: 263 | Train Loss: 0.4418966 Vali Loss: 1.0165210 Test Loss: 0.4962174
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4516292
	speed: 0.0786s/iter; left time: 1170.6009s
	iters: 200, epoch: 44 | loss: 0.4329992
	speed: 0.0446s/iter; left time: 660.0003s
Epoch: 44 cost time: 9.125267028808594
Epoch: 44, Steps: 263 | Train Loss: 0.4420132 Vali Loss: 1.0167276 Test Loss: 0.4962353
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4686455
	speed: 0.1412s/iter; left time: 2065.5074s
	iters: 200, epoch: 45 | loss: 0.4723471
	speed: 0.0514s/iter; left time: 747.0264s
Epoch: 45 cost time: 10.12688684463501
Epoch: 45, Steps: 263 | Train Loss: 0.4419185 Vali Loss: 1.0165577 Test Loss: 0.4962172
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.3956006
	speed: 0.1295s/iter; left time: 1860.4951s
	iters: 200, epoch: 46 | loss: 0.4998603
	speed: 0.0341s/iter; left time: 485.8487s
Epoch: 46 cost time: 9.328437328338623
Epoch: 46, Steps: 263 | Train Loss: 0.4418844 Vali Loss: 1.0153040 Test Loss: 0.4962733
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4582988
	speed: 0.1484s/iter; left time: 2092.8252s
	iters: 200, epoch: 47 | loss: 0.4312367
	speed: 0.0388s/iter; left time: 543.7942s
Epoch: 47 cost time: 9.70682954788208
Epoch: 47, Steps: 263 | Train Loss: 0.4419272 Vali Loss: 1.0160218 Test Loss: 0.4962896
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4636769
	speed: 0.1370s/iter; left time: 1895.9043s
	iters: 200, epoch: 48 | loss: 0.4609103
	speed: 0.0299s/iter; left time: 410.3143s
Epoch: 48 cost time: 9.450809717178345
Epoch: 48, Steps: 263 | Train Loss: 0.4419733 Vali Loss: 1.0156751 Test Loss: 0.4962533
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4202757
	speed: 0.1542s/iter; left time: 2093.8054s
	iters: 200, epoch: 49 | loss: 0.4256338
	speed: 0.0305s/iter; left time: 411.4560s
Epoch: 49 cost time: 9.998035669326782
Epoch: 49, Steps: 263 | Train Loss: 0.4419703 Vali Loss: 1.0161750 Test Loss: 0.4962188
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.4489692
	speed: 0.1562s/iter; left time: 2079.8867s
	iters: 200, epoch: 50 | loss: 0.4373425
	speed: 0.0316s/iter; left time: 417.3190s
Epoch: 50 cost time: 10.017327547073364
Epoch: 50, Steps: 263 | Train Loss: 0.4418940 Vali Loss: 1.0156473 Test Loss: 0.4962773
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4379977
	speed: 0.1553s/iter; left time: 2026.3592s
	iters: 200, epoch: 51 | loss: 0.4207129
	speed: 0.0337s/iter; left time: 436.2267s
Epoch: 51 cost time: 9.78876781463623
Epoch: 51, Steps: 263 | Train Loss: 0.4419401 Vali Loss: 1.0162385 Test Loss: 0.4962372
EarlyStopping counter: 20 out of 20
Early stopping
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=16, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2064384.0
params:  2448.0
Trainable parameters:  2448
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5295704
	speed: 0.0235s/iter; left time: 615.5762s
	iters: 200, epoch: 1 | loss: 0.4422908
	speed: 0.0533s/iter; left time: 1391.8872s
Epoch: 1 cost time: 10.197332382202148
Epoch: 1, Steps: 263 | Train Loss: 0.4942663 Vali Loss: 1.0133559 Test Loss: 0.4944192
Validation loss decreased (inf --> 1.013356).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4960688
	speed: 0.1349s/iter; left time: 3499.8469s
	iters: 200, epoch: 2 | loss: 0.5048975
	speed: 0.0529s/iter; left time: 1366.6172s
Epoch: 2 cost time: 9.384084463119507
Epoch: 2, Steps: 263 | Train Loss: 0.4940875 Vali Loss: 1.0137323 Test Loss: 0.4946896
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4977370
	speed: 0.0629s/iter; left time: 1616.1225s
	iters: 200, epoch: 3 | loss: 0.4775910
	speed: 0.0131s/iter; left time: 335.4335s
Epoch: 3 cost time: 3.9973559379577637
Epoch: 3, Steps: 263 | Train Loss: 0.4938771 Vali Loss: 1.0145353 Test Loss: 0.4947790
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5452446
	speed: 0.0601s/iter; left time: 1526.0601s
	iters: 200, epoch: 4 | loss: 0.5133439
	speed: 0.0124s/iter; left time: 313.5686s
Epoch: 4 cost time: 3.7667644023895264
Epoch: 4, Steps: 263 | Train Loss: 0.4939986 Vali Loss: 1.0139414 Test Loss: 0.4949366
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5128046
	speed: 0.0601s/iter; left time: 1511.1846s
	iters: 200, epoch: 5 | loss: 0.5027057
	speed: 0.0128s/iter; left time: 319.6893s
Epoch: 5 cost time: 3.917962074279785
Epoch: 5, Steps: 263 | Train Loss: 0.4938325 Vali Loss: 1.0130949 Test Loss: 0.4944085
Validation loss decreased (1.013356 --> 1.013095).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4735340
	speed: 0.0629s/iter; left time: 1566.0581s
	iters: 200, epoch: 6 | loss: 0.4699887
	speed: 0.0126s/iter; left time: 311.3922s
Epoch: 6 cost time: 3.927995204925537
Epoch: 6, Steps: 263 | Train Loss: 0.4938668 Vali Loss: 1.0145941 Test Loss: 0.4948473
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5298691
	speed: 0.0591s/iter; left time: 1454.2381s
	iters: 200, epoch: 7 | loss: 0.5126675
	speed: 0.0130s/iter; left time: 318.0255s
Epoch: 7 cost time: 3.852581262588501
Epoch: 7, Steps: 263 | Train Loss: 0.4938147 Vali Loss: 1.0135365 Test Loss: 0.4950443
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5428191
	speed: 0.0622s/iter; left time: 1514.4518s
	iters: 200, epoch: 8 | loss: 0.4882434
	speed: 0.0123s/iter; left time: 297.5944s
Epoch: 8 cost time: 4.037101984024048
Epoch: 8, Steps: 263 | Train Loss: 0.4939169 Vali Loss: 1.0136895 Test Loss: 0.4946187
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4872220
	speed: 0.0597s/iter; left time: 1438.5534s
	iters: 200, epoch: 9 | loss: 0.4689447
	speed: 0.0132s/iter; left time: 316.3227s
Epoch: 9 cost time: 3.8375720977783203
Epoch: 9, Steps: 263 | Train Loss: 0.4939141 Vali Loss: 1.0134302 Test Loss: 0.4946626
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5210202
	speed: 0.0603s/iter; left time: 1436.4404s
	iters: 200, epoch: 10 | loss: 0.5196564
	speed: 0.0121s/iter; left time: 287.7567s
Epoch: 10 cost time: 3.810389757156372
Epoch: 10, Steps: 263 | Train Loss: 0.4935935 Vali Loss: 1.0131391 Test Loss: 0.4951609
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4441485
	speed: 0.0634s/iter; left time: 1493.7255s
	iters: 200, epoch: 11 | loss: 0.5523065
	speed: 0.0124s/iter; left time: 291.9238s
Epoch: 11 cost time: 3.848489284515381
Epoch: 11, Steps: 263 | Train Loss: 0.4937019 Vali Loss: 1.0136687 Test Loss: 0.4948280
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5094646
	speed: 0.0575s/iter; left time: 1339.8509s
	iters: 200, epoch: 12 | loss: 0.5034267
	speed: 0.0122s/iter; left time: 283.6682s
Epoch: 12 cost time: 3.707042694091797
Epoch: 12, Steps: 263 | Train Loss: 0.4936905 Vali Loss: 1.0141516 Test Loss: 0.4949630
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5585665
	speed: 0.0624s/iter; left time: 1437.5064s
	iters: 200, epoch: 13 | loss: 0.4497241
	speed: 0.0122s/iter; left time: 279.4060s
Epoch: 13 cost time: 3.881087303161621
Epoch: 13, Steps: 263 | Train Loss: 0.4937359 Vali Loss: 1.0137228 Test Loss: 0.4950545
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5053343
	speed: 0.0596s/iter; left time: 1358.0895s
	iters: 200, epoch: 14 | loss: 0.4616029
	speed: 0.0124s/iter; left time: 280.6367s
Epoch: 14 cost time: 3.7766852378845215
Epoch: 14, Steps: 263 | Train Loss: 0.4935837 Vali Loss: 1.0137850 Test Loss: 0.4951541
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4830152
	speed: 0.0613s/iter; left time: 1379.6147s
	iters: 200, epoch: 15 | loss: 0.5627945
	speed: 0.0124s/iter; left time: 277.6311s
Epoch: 15 cost time: 3.875519275665283
Epoch: 15, Steps: 263 | Train Loss: 0.4937863 Vali Loss: 1.0149972 Test Loss: 0.4950771
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4977773
	speed: 0.0613s/iter; left time: 1365.3021s
	iters: 200, epoch: 16 | loss: 0.4845358
	speed: 0.0120s/iter; left time: 266.0538s
Epoch: 16 cost time: 3.7523696422576904
Epoch: 16, Steps: 263 | Train Loss: 0.4936992 Vali Loss: 1.0142390 Test Loss: 0.4949163
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4850066
	speed: 0.0614s/iter; left time: 1350.7478s
	iters: 200, epoch: 17 | loss: 0.4921596
	speed: 0.0123s/iter; left time: 269.2050s
Epoch: 17 cost time: 3.8528947830200195
Epoch: 17, Steps: 263 | Train Loss: 0.4936424 Vali Loss: 1.0144525 Test Loss: 0.4954053
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4937607
	speed: 0.0604s/iter; left time: 1311.6780s
	iters: 200, epoch: 18 | loss: 0.4467305
	speed: 0.0125s/iter; left time: 270.6186s
Epoch: 18 cost time: 3.8399181365966797
Epoch: 18, Steps: 263 | Train Loss: 0.4937734 Vali Loss: 1.0138124 Test Loss: 0.4950519
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4975395
	speed: 0.0612s/iter; left time: 1313.3274s
	iters: 200, epoch: 19 | loss: 0.4865428
	speed: 0.0122s/iter; left time: 261.3440s
Epoch: 19 cost time: 3.8032896518707275
Epoch: 19, Steps: 263 | Train Loss: 0.4938475 Vali Loss: 1.0152986 Test Loss: 0.4955720
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4457697
	speed: 0.0757s/iter; left time: 1605.3776s
	iters: 200, epoch: 20 | loss: 0.4364429
	speed: 0.0243s/iter; left time: 512.2534s
Epoch: 20 cost time: 9.034544944763184
Epoch: 20, Steps: 263 | Train Loss: 0.4935140 Vali Loss: 1.0139937 Test Loss: 0.4952061
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5307326
	speed: 0.1602s/iter; left time: 3354.0930s
	iters: 200, epoch: 21 | loss: 0.4812682
	speed: 0.0317s/iter; left time: 660.3375s
Epoch: 21 cost time: 9.876879453659058
Epoch: 21, Steps: 263 | Train Loss: 0.4936972 Vali Loss: 1.0146015 Test Loss: 0.4951551
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4870365
	speed: 0.1399s/iter; left time: 2893.5966s
	iters: 200, epoch: 22 | loss: 0.4958348
	speed: 0.0245s/iter; left time: 503.5114s
Epoch: 22 cost time: 8.899355411529541
Epoch: 22, Steps: 263 | Train Loss: 0.4937158 Vali Loss: 1.0147964 Test Loss: 0.4952187
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4584114
	speed: 0.1346s/iter; left time: 2747.2770s
	iters: 200, epoch: 23 | loss: 0.5102714
	speed: 0.0332s/iter; left time: 674.0220s
Epoch: 23 cost time: 8.624324798583984
Epoch: 23, Steps: 263 | Train Loss: 0.4937224 Vali Loss: 1.0133111 Test Loss: 0.4950450
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4797893
	speed: 0.1209s/iter; left time: 2436.7800s
	iters: 200, epoch: 24 | loss: 0.4798366
	speed: 0.0455s/iter; left time: 912.9486s
Epoch: 24 cost time: 8.117688655853271
Epoch: 24, Steps: 263 | Train Loss: 0.4937472 Vali Loss: 1.0140040 Test Loss: 0.4952728
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4938048
	speed: 0.1354s/iter; left time: 2692.0869s
	iters: 200, epoch: 25 | loss: 0.5265663
	speed: 0.0433s/iter; left time: 857.6067s
Epoch: 25 cost time: 10.392736673355103
Epoch: 25, Steps: 263 | Train Loss: 0.4936801 Vali Loss: 1.0140800 Test Loss: 0.4952506
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.49247342348098755, mae:0.4525509774684906, rse:0.667670488357544, corr:[0.5285439  0.53186727 0.5287368  0.5241778  0.52140605 0.52010477
 0.5183523  0.51570565 0.51312304 0.5115303  0.5106202  0.50967157
 0.5083399  0.5064112  0.5036627  0.5001425  0.49637538 0.49299258
 0.49020508 0.48769042 0.4850543  0.48198232 0.47838482 0.47441706
 0.47052914 0.4669684  0.46394992 0.46134385 0.45901316 0.4567895
 0.45523196 0.45513815 0.45576414 0.45625523 0.45635954 0.45643938
 0.45641908 0.4566093  0.45696217 0.457158   0.45712793 0.45667538
 0.45617375 0.4558989  0.45569456 0.4555978  0.45567253 0.45586005
 0.45614296 0.4565314  0.4569831  0.45723778 0.45742637 0.45760962
 0.45811832 0.45887083 0.4597207  0.46029612 0.46021873 0.45986265
 0.45963907 0.45943886 0.4593475  0.45909995 0.4589472  0.45883188
 0.45854992 0.4582491  0.4583709  0.4589976  0.45972282 0.46039236
 0.46106252 0.4616696  0.46240577 0.46308863 0.4639467  0.46498838
 0.46594656 0.46670097 0.46725926 0.46757883 0.46791202 0.4681093
 0.46833044 0.46868572 0.46937206 0.47039303 0.4715548  0.47264004
 0.4734933  0.47417173 0.47490844 0.47568774 0.47619468 0.4759086
 0.47484088 0.47359976 0.4726085  0.4720859  0.47201702 0.47197136
 0.47151923 0.47041607 0.469191   0.46833014 0.46786192 0.46752337
 0.4669881  0.4661178  0.46502098 0.46395934 0.4630866  0.46237123
 0.46156985 0.4607149  0.45978    0.45880497 0.4577525  0.4565234
 0.4551788  0.45365334 0.45212397 0.45102215 0.45027587 0.44958732
 0.44887927 0.44823587 0.44785145 0.44781747 0.44805685 0.4483655
 0.44836757 0.44815502 0.44779286 0.44746494 0.44730216 0.44731453
 0.44731903 0.44721508 0.4470422  0.44701815 0.44718432 0.4474804
 0.44767204 0.4477346  0.4477811  0.4478267  0.44816563 0.44857135
 0.44882712 0.44907585 0.44927722 0.4494364  0.44948962 0.4495814
 0.44963136 0.44959155 0.44941488 0.44924864 0.44934532 0.44960734
 0.44986472 0.4500047  0.45012918 0.45037445 0.45092037 0.4517809
 0.45277888 0.4536645  0.45432848 0.45499477 0.45571104 0.456522
 0.45722684 0.4577547  0.4581271  0.45843104 0.45862845 0.4588519
 0.4590798  0.4593136  0.45950243 0.45979664 0.4603399  0.4610666
 0.4617826  0.46232143 0.4626536  0.46287104 0.46310514 0.4635021
 0.46418816 0.4653543  0.46668968 0.46780968 0.46855348 0.46907786
 0.46928772 0.46892703 0.46819267 0.46743202 0.46655703 0.4654971
 0.46424377 0.46291068 0.46153674 0.4602756  0.45898917 0.45767966
 0.45623598 0.45465463 0.4530233  0.45129693 0.4493881  0.4475828
 0.44580677 0.44416773 0.442664   0.44138998 0.44022173 0.43926966
 0.43873802 0.43875965 0.43882397 0.4386526  0.43850097 0.4381707
 0.43772006 0.43743283 0.4373131  0.4371705  0.43706062 0.4368954
 0.4366996  0.4364203  0.4358861  0.43551695 0.43566662 0.43600982
 0.43652073 0.43685994 0.43702242 0.43724036 0.43736884 0.4376184
 0.43794808 0.43831554 0.43861717 0.43900543 0.43924507 0.43944094
 0.43941996 0.4393443  0.43907705 0.43903604 0.4390243  0.43898222
 0.4391848  0.4397074  0.4401841  0.4406837  0.44128203 0.44194987
 0.4428454  0.44362843 0.4443033  0.44515535 0.44634494 0.44751942
 0.44856605 0.4492715  0.4497919  0.45020103 0.450625   0.45108756
 0.45162344 0.45225647 0.452849   0.45353767 0.45433375 0.455162
 0.4559331  0.456496   0.45683196 0.45690334 0.4565871  0.4556617
 0.4539666  0.45182025 0.4496934  0.4478765  0.4465625  0.44589305
 0.44572783 0.44550377 0.44518244 0.4447168  0.44402203 0.44313425
 0.44215047 0.4411591  0.44019815 0.43921465 0.43832374 0.4374893
 0.43685442 0.43615314 0.43531927 0.43436554 0.43337458 0.43236175
 0.43128818 0.43017447 0.42916328 0.42838186 0.42762142 0.42708498
 0.42679656 0.42665857 0.42657074 0.42646366 0.42632514 0.42629436
 0.4262756  0.42632484 0.42620704 0.42579773 0.425126   0.42440552
 0.42411998 0.4241587  0.42431945 0.4242855  0.42405295 0.42363697
 0.423287   0.4229853  0.42310625 0.42341793 0.423913   0.42433757
 0.4246155  0.42491454 0.42516348 0.4253485  0.42541587 0.42538372
 0.42534173 0.42533487 0.42547777 0.42561397 0.42561305 0.42552558
 0.4254265  0.42542228 0.42564052 0.4260557  0.42657828 0.42713892
 0.4276833  0.42824703 0.4289092  0.42963335 0.4305678  0.4315372
 0.432381   0.43322754 0.43407625 0.4348101  0.4355827  0.43632066
 0.4371481  0.43804047 0.43924168 0.44079646 0.4424692  0.4439477
 0.44490862 0.44528756 0.4453729  0.4455508  0.44591004 0.44623768
 0.44634974 0.44633892 0.44649208 0.44688976 0.4474752  0.448294
 0.44909388 0.44948372 0.44961154 0.4495829  0.44930485 0.44861957
 0.4475184  0.44617465 0.4447785  0.4435824  0.44260815 0.44182086
 0.44105175 0.44016042 0.4392773  0.43847167 0.43766063 0.4368197
 0.43600002 0.4349152  0.4337427  0.43296656 0.4324371  0.43197578
 0.4315673  0.43109125 0.4307836  0.43066227 0.43066108 0.4308244
 0.4309088  0.43077305 0.43026084 0.42956004 0.42897338 0.42872536
 0.4287377  0.4288698  0.4286626  0.42838648 0.42812908 0.42799887
 0.42796406 0.42793316 0.4280511  0.4283591  0.42865375 0.42895693
 0.42931202 0.42987314 0.43056315 0.4312161  0.43156046 0.4314863
 0.43113008 0.43094584 0.43096337 0.4311704  0.43129435 0.43127143
 0.43112648 0.43122828 0.4316522  0.43238708 0.433043   0.43345973
 0.43362132 0.43365198 0.4339274  0.43473834 0.43589255 0.43698356
 0.43779898 0.4384221  0.43884817 0.43928894 0.4398378  0.44046974
 0.441132   0.44175833 0.44254047 0.4434952  0.44466156 0.44595617
 0.447029   0.44765735 0.44781363 0.44763482 0.44720104 0.4465183
 0.44542348 0.44413725 0.4427662  0.44168267 0.4410136  0.44090885
 0.44086936 0.4403665  0.43958545 0.43880525 0.43806762 0.4373232
 0.4364651  0.43542987 0.4342337  0.4329511  0.43161586 0.43022624
 0.42879617 0.4274538  0.4262255  0.42497584 0.42387548 0.42285162
 0.42191872 0.4208659  0.419694   0.41862804 0.4178026  0.41736072
 0.4171453  0.4171294  0.41711894 0.41701975 0.41677982 0.41655707
 0.4163198  0.4161919  0.41614196 0.41582537 0.4153738  0.415032
 0.41492552 0.41512224 0.4152311  0.41514528 0.4148351  0.414495
 0.41408026 0.41365138 0.41329685 0.41314432 0.41327354 0.4136394
 0.41416067 0.41455585 0.41485617 0.4151178  0.4153944  0.41563746
 0.41602182 0.41632637 0.41630682 0.415927   0.41536516 0.41504145
 0.41510558 0.4154501  0.41584995 0.4161459  0.41632283 0.41661707
 0.4170899  0.41760057 0.41818443 0.41888    0.41972438 0.42060852
 0.42148125 0.4223801  0.42320505 0.42382607 0.4243006  0.42472446
 0.425197   0.425882   0.42663214 0.4274991  0.42853698 0.42970547
 0.43085444 0.43171018 0.43205288 0.43168053 0.4305904  0.42898294
 0.4271125  0.42530388 0.4239118  0.42274258 0.4221523  0.42227817
 0.42282888 0.42319584 0.42323443 0.42307132 0.4226835  0.4219494
 0.42082956 0.4194151  0.41778094 0.41614068 0.41462776 0.41317835
 0.41188043 0.41081268 0.40995532 0.4091044  0.40813273 0.40701503
 0.40589774 0.40482298 0.4039091  0.40323833 0.4026742  0.4022922
 0.4020672  0.40203062 0.4019766  0.40189117 0.40176418 0.40180412
 0.40192294 0.4020639  0.40206945 0.4017382  0.40129006 0.400814
 0.40062132 0.4007714  0.40092707 0.40095633 0.40091336 0.40095085
 0.40105596 0.4013228  0.40180197 0.4020588  0.40203097 0.40188536
 0.40170908 0.40164757 0.40189758 0.4021684  0.40226814 0.4019837
 0.40148777 0.4010793  0.40093157 0.40089586 0.40099743 0.4011247
 0.40114847 0.40105686 0.40099254 0.40119258 0.40157664 0.40218684
 0.40294796 0.40366122 0.4042171  0.40477526 0.40534538 0.4060578
 0.40680635 0.40762278 0.4083402  0.40884826 0.409212   0.4096579
 0.41034594 0.4112576  0.41234657 0.41339466 0.41429567 0.41509917
 0.41579622 0.41629943 0.4164057  0.4159272  0.41489214 0.41357633
 0.4122403  0.41120481 0.41049385 0.40984195 0.40943256 0.40955967
 0.4101406  0.4107412  0.41118008 0.41130942 0.41105524 0.410468
 0.40966693 0.40865844 0.40741456 0.4058713  0.4041137  0.40246642
 0.40141642 0.40085125 0.40025073 0.39910892 0.39730936 0.3953709
 0.39388287 0.39304772 0.39241642 0.39148656 0.3901444  0.38877276
 0.38789394 0.387518   0.38742724 0.3869952  0.3859998  0.3852832
 0.38542753 0.38612333 0.38640442 0.38567355 0.38455364 0.38439888
 0.3856734  0.38705137 0.3868399  0.38578272 0.3877628  0.39447364]
