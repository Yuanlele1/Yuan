Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=26, out_features=78, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1817088.0
params:  2106.0
Trainable parameters:  2106
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6022768
	speed: 0.2302s/iter; left time: 5985.2876s
	iters: 200, epoch: 1 | loss: 0.4478025
	speed: 0.2259s/iter; left time: 5851.7418s
Epoch: 1 cost time: 59.43896007537842
Epoch: 1, Steps: 261 | Train Loss: 0.5660925 Vali Loss: 1.2038424 Test Loss: 0.5885417
Validation loss decreased (inf --> 1.203842).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3751060
	speed: 0.9812s/iter; left time: 25256.8606s
	iters: 200, epoch: 2 | loss: 0.3526454
	speed: 0.2247s/iter; left time: 5761.3353s
Epoch: 2 cost time: 60.282482624053955
Epoch: 2, Steps: 261 | Train Loss: 0.3629817 Vali Loss: 1.0600553 Test Loss: 0.4814591
Validation loss decreased (1.203842 --> 1.060055).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3146028
	speed: 0.9314s/iter; left time: 23731.8805s
	iters: 200, epoch: 3 | loss: 0.2986758
	speed: 0.2171s/iter; left time: 5508.5788s
Epoch: 3 cost time: 59.68994116783142
Epoch: 3, Steps: 261 | Train Loss: 0.3199452 Vali Loss: 1.0138487 Test Loss: 0.4529273
Validation loss decreased (1.060055 --> 1.013849).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3305216
	speed: 0.9410s/iter; left time: 23730.4795s
	iters: 200, epoch: 4 | loss: 0.2976826
	speed: 0.2101s/iter; left time: 5277.5791s
Epoch: 4 cost time: 57.415730476379395
Epoch: 4, Steps: 261 | Train Loss: 0.3060221 Vali Loss: 0.9930581 Test Loss: 0.4424284
Validation loss decreased (1.013849 --> 0.993058).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3175921
	speed: 0.9347s/iter; left time: 23327.7264s
	iters: 200, epoch: 5 | loss: 0.2989942
	speed: 0.2121s/iter; left time: 5272.7564s
Epoch: 5 cost time: 54.95641875267029
Epoch: 5, Steps: 261 | Train Loss: 0.2998670 Vali Loss: 0.9834796 Test Loss: 0.4380735
Validation loss decreased (0.993058 --> 0.983480).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3028138
	speed: 0.9481s/iter; left time: 23413.5159s
	iters: 200, epoch: 6 | loss: 0.2905408
	speed: 0.2275s/iter; left time: 5594.5221s
Epoch: 6 cost time: 59.310341119766235
Epoch: 6, Steps: 261 | Train Loss: 0.2963499 Vali Loss: 0.9788617 Test Loss: 0.4355591
Validation loss decreased (0.983480 --> 0.978862).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2928006
	speed: 0.9542s/iter; left time: 23316.0797s
	iters: 200, epoch: 7 | loss: 0.2942559
	speed: 0.2150s/iter; left time: 5232.6966s
Epoch: 7 cost time: 56.558717012405396
Epoch: 7, Steps: 261 | Train Loss: 0.2942086 Vali Loss: 0.9745712 Test Loss: 0.4341401
Validation loss decreased (0.978862 --> 0.974571).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3079734
	speed: 0.9245s/iter; left time: 22348.7311s
	iters: 200, epoch: 8 | loss: 0.2834117
	speed: 0.2167s/iter; left time: 5216.5952s
Epoch: 8 cost time: 55.99091339111328
Epoch: 8, Steps: 261 | Train Loss: 0.2928827 Vali Loss: 0.9739574 Test Loss: 0.4333950
Validation loss decreased (0.974571 --> 0.973957).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3047321
	speed: 0.9353s/iter; left time: 22366.6804s
	iters: 200, epoch: 9 | loss: 0.3030587
	speed: 0.2028s/iter; left time: 4829.6440s
Epoch: 9 cost time: 55.61919331550598
Epoch: 9, Steps: 261 | Train Loss: 0.2919511 Vali Loss: 0.9707144 Test Loss: 0.4331168
Validation loss decreased (0.973957 --> 0.970714).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3075028
	speed: 0.8957s/iter; left time: 21186.2130s
	iters: 200, epoch: 10 | loss: 0.2952780
	speed: 0.2069s/iter; left time: 4872.2848s
Epoch: 10 cost time: 53.60718250274658
Epoch: 10, Steps: 261 | Train Loss: 0.2912619 Vali Loss: 0.9702649 Test Loss: 0.4326703
Validation loss decreased (0.970714 --> 0.970265).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2768549
	speed: 0.9178s/iter; left time: 21469.1939s
	iters: 200, epoch: 11 | loss: 0.2835115
	speed: 0.2148s/iter; left time: 5003.4735s
Epoch: 11 cost time: 57.5497031211853
Epoch: 11, Steps: 261 | Train Loss: 0.2908074 Vali Loss: 0.9694356 Test Loss: 0.4324352
Validation loss decreased (0.970265 --> 0.969436).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2680261
	speed: 0.9107s/iter; left time: 21065.0016s
	iters: 200, epoch: 12 | loss: 0.2951083
	speed: 0.2080s/iter; left time: 4789.4646s
Epoch: 12 cost time: 54.562432289123535
Epoch: 12, Steps: 261 | Train Loss: 0.2904113 Vali Loss: 0.9683467 Test Loss: 0.4322539
Validation loss decreased (0.969436 --> 0.968347).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3131813
	speed: 0.9019s/iter; left time: 20624.4565s
	iters: 200, epoch: 13 | loss: 0.2944760
	speed: 0.2134s/iter; left time: 4859.0202s
Epoch: 13 cost time: 55.621737003326416
Epoch: 13, Steps: 261 | Train Loss: 0.2902295 Vali Loss: 0.9683470 Test Loss: 0.4321049
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3027082
	speed: 0.8902s/iter; left time: 20125.6758s
	iters: 200, epoch: 14 | loss: 0.2748029
	speed: 0.2034s/iter; left time: 4577.4199s
Epoch: 14 cost time: 52.943525314331055
Epoch: 14, Steps: 261 | Train Loss: 0.2900264 Vali Loss: 0.9681741 Test Loss: 0.4322426
Validation loss decreased (0.968347 --> 0.968174).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3217006
	speed: 0.8715s/iter; left time: 19474.5850s
	iters: 200, epoch: 15 | loss: 0.2636377
	speed: 0.2186s/iter; left time: 4863.2704s
Epoch: 15 cost time: 56.629323959350586
Epoch: 15, Steps: 261 | Train Loss: 0.2898811 Vali Loss: 0.9688156 Test Loss: 0.4320074
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2967497
	speed: 0.8934s/iter; left time: 19732.4353s
	iters: 200, epoch: 16 | loss: 0.2887329
	speed: 0.1951s/iter; left time: 4289.7574s
Epoch: 16 cost time: 54.454846143722534
Epoch: 16, Steps: 261 | Train Loss: 0.2898972 Vali Loss: 0.9681088 Test Loss: 0.4318840
Validation loss decreased (0.968174 --> 0.968109).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3085777
	speed: 0.8502s/iter; left time: 18555.9354s
	iters: 200, epoch: 17 | loss: 0.3165489
	speed: 0.2015s/iter; left time: 4376.7500s
Epoch: 17 cost time: 52.28032183647156
Epoch: 17, Steps: 261 | Train Loss: 0.2898485 Vali Loss: 0.9686776 Test Loss: 0.4319055
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2996274
	speed: 0.8960s/iter; left time: 19322.2953s
	iters: 200, epoch: 18 | loss: 0.2838502
	speed: 0.2020s/iter; left time: 4336.0514s
Epoch: 18 cost time: 53.32868766784668
Epoch: 18, Steps: 261 | Train Loss: 0.2896692 Vali Loss: 0.9683900 Test Loss: 0.4319111
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3129663
	speed: 0.8543s/iter; left time: 18198.2717s
	iters: 200, epoch: 19 | loss: 0.2881625
	speed: 0.1853s/iter; left time: 3929.5923s
Epoch: 19 cost time: 51.14502263069153
Epoch: 19, Steps: 261 | Train Loss: 0.2897118 Vali Loss: 0.9677505 Test Loss: 0.4321216
Validation loss decreased (0.968109 --> 0.967751).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2932719
	speed: 0.8690s/iter; left time: 18285.8863s
	iters: 200, epoch: 20 | loss: 0.2828526
	speed: 0.2053s/iter; left time: 4299.0244s
Epoch: 20 cost time: 52.65063524246216
Epoch: 20, Steps: 261 | Train Loss: 0.2896833 Vali Loss: 0.9676802 Test Loss: 0.4318153
Validation loss decreased (0.967751 --> 0.967680).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2836592
	speed: 0.8559s/iter; left time: 17785.6239s
	iters: 200, epoch: 21 | loss: 0.2940870
	speed: 0.1997s/iter; left time: 4130.8314s
Epoch: 21 cost time: 51.16637444496155
Epoch: 21, Steps: 261 | Train Loss: 0.2896502 Vali Loss: 0.9678163 Test Loss: 0.4318049
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2928768
	speed: 0.8381s/iter; left time: 17197.1864s
	iters: 200, epoch: 22 | loss: 0.2847122
	speed: 0.1950s/iter; left time: 3982.6591s
Epoch: 22 cost time: 51.30451202392578
Epoch: 22, Steps: 261 | Train Loss: 0.2896887 Vali Loss: 0.9673589 Test Loss: 0.4319062
Validation loss decreased (0.967680 --> 0.967359).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2952353
	speed: 0.8132s/iter; left time: 16474.0774s
	iters: 200, epoch: 23 | loss: 0.2950801
	speed: 0.2101s/iter; left time: 4236.1712s
Epoch: 23 cost time: 53.72596716880798
Epoch: 23, Steps: 261 | Train Loss: 0.2897235 Vali Loss: 0.9676363 Test Loss: 0.4320230
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3089761
	speed: 0.8460s/iter; left time: 16918.7956s
	iters: 200, epoch: 24 | loss: 0.2789085
	speed: 0.1984s/iter; left time: 3947.8481s
Epoch: 24 cost time: 52.880038261413574
Epoch: 24, Steps: 261 | Train Loss: 0.2896113 Vali Loss: 0.9680618 Test Loss: 0.4321688
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3070445
	speed: 0.8728s/iter; left time: 17225.4701s
	iters: 200, epoch: 25 | loss: 0.2683378
	speed: 0.2040s/iter; left time: 4005.1559s
Epoch: 25 cost time: 53.33348631858826
Epoch: 25, Steps: 261 | Train Loss: 0.2896458 Vali Loss: 0.9685563 Test Loss: 0.4320088
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2770346
	speed: 0.8841s/iter; left time: 17219.1814s
	iters: 200, epoch: 26 | loss: 0.2928099
	speed: 0.1978s/iter; left time: 3831.7792s
Epoch: 26 cost time: 51.317328214645386
Epoch: 26, Steps: 261 | Train Loss: 0.2896174 Vali Loss: 0.9673886 Test Loss: 0.4319748
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2939558
	speed: 0.8610s/iter; left time: 16545.0077s
	iters: 200, epoch: 27 | loss: 0.2707150
	speed: 0.2116s/iter; left time: 4045.4592s
Epoch: 27 cost time: 54.52101802825928
Epoch: 27, Steps: 261 | Train Loss: 0.2895982 Vali Loss: 0.9681137 Test Loss: 0.4320627
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2981608
	speed: 0.9021s/iter; left time: 17098.5424s
	iters: 200, epoch: 28 | loss: 0.2708216
	speed: 0.2046s/iter; left time: 3857.5924s
Epoch: 28 cost time: 54.38699674606323
Epoch: 28, Steps: 261 | Train Loss: 0.2896685 Vali Loss: 0.9685614 Test Loss: 0.4319745
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2655034
	speed: 0.8717s/iter; left time: 16293.8707s
	iters: 200, epoch: 29 | loss: 0.2997466
	speed: 0.1997s/iter; left time: 3712.9491s
Epoch: 29 cost time: 53.37244367599487
Epoch: 29, Steps: 261 | Train Loss: 0.2896108 Vali Loss: 0.9673238 Test Loss: 0.4321188
Validation loss decreased (0.967359 --> 0.967324).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2894996
	speed: 0.8518s/iter; left time: 15699.7068s
	iters: 200, epoch: 30 | loss: 0.2914459
	speed: 0.1888s/iter; left time: 3461.2976s
Epoch: 30 cost time: 50.979915380477905
Epoch: 30, Steps: 261 | Train Loss: 0.2895984 Vali Loss: 0.9678876 Test Loss: 0.4319686
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2914137
	speed: 0.8123s/iter; left time: 14761.1305s
	iters: 200, epoch: 31 | loss: 0.2894031
	speed: 0.1895s/iter; left time: 3423.6538s
Epoch: 31 cost time: 48.865182638168335
Epoch: 31, Steps: 261 | Train Loss: 0.2896684 Vali Loss: 0.9677474 Test Loss: 0.4319932
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2983248
	speed: 0.8171s/iter; left time: 14634.3779s
	iters: 200, epoch: 32 | loss: 0.2644773
	speed: 0.1866s/iter; left time: 3323.7893s
Epoch: 32 cost time: 48.67368721961975
Epoch: 32, Steps: 261 | Train Loss: 0.2896114 Vali Loss: 0.9684115 Test Loss: 0.4320974
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2942711
	speed: 0.8083s/iter; left time: 14265.0327s
	iters: 200, epoch: 33 | loss: 0.3099117
	speed: 0.1830s/iter; left time: 3211.3788s
Epoch: 33 cost time: 50.043893814086914
Epoch: 33, Steps: 261 | Train Loss: 0.2896783 Vali Loss: 0.9678041 Test Loss: 0.4320931
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2754278
	speed: 0.8265s/iter; left time: 14371.2525s
	iters: 200, epoch: 34 | loss: 0.2709355
	speed: 0.1862s/iter; left time: 3219.0165s
Epoch: 34 cost time: 48.98472476005554
Epoch: 34, Steps: 261 | Train Loss: 0.2896468 Vali Loss: 0.9676887 Test Loss: 0.4321076
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2879749
	speed: 0.8133s/iter; left time: 13929.4351s
	iters: 200, epoch: 35 | loss: 0.2652625
	speed: 0.2275s/iter; left time: 3873.7962s
Epoch: 35 cost time: 57.98066020011902
Epoch: 35, Steps: 261 | Train Loss: 0.2896296 Vali Loss: 0.9680741 Test Loss: 0.4321580
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2846065
	speed: 0.9558s/iter; left time: 16119.8719s
	iters: 200, epoch: 36 | loss: 0.2714957
	speed: 0.2191s/iter; left time: 3672.6095s
Epoch: 36 cost time: 58.54948091506958
Epoch: 36, Steps: 261 | Train Loss: 0.2896072 Vali Loss: 0.9674076 Test Loss: 0.4320814
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2730653
	speed: 0.9296s/iter; left time: 15436.7666s
	iters: 200, epoch: 37 | loss: 0.2893775
	speed: 0.2217s/iter; left time: 3659.1198s
Epoch: 37 cost time: 58.753634214401245
Epoch: 37, Steps: 261 | Train Loss: 0.2896794 Vali Loss: 0.9676036 Test Loss: 0.4320402
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3136550
	speed: 0.9769s/iter; left time: 15966.4958s
	iters: 200, epoch: 38 | loss: 0.3146681
	speed: 0.2208s/iter; left time: 3586.2250s
Epoch: 38 cost time: 58.03615164756775
Epoch: 38, Steps: 261 | Train Loss: 0.2895850 Vali Loss: 0.9664466 Test Loss: 0.4320774
Validation loss decreased (0.967324 --> 0.966447).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.3118908
	speed: 0.9443s/iter; left time: 15186.5213s
	iters: 200, epoch: 39 | loss: 0.2722120
	speed: 0.2188s/iter; left time: 3496.3644s
Epoch: 39 cost time: 58.67537021636963
Epoch: 39, Steps: 261 | Train Loss: 0.2896205 Vali Loss: 0.9679122 Test Loss: 0.4320367
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3260452
	speed: 0.9566s/iter; left time: 15134.9335s
	iters: 200, epoch: 40 | loss: 0.2628606
	speed: 0.2257s/iter; left time: 3548.1202s
Epoch: 40 cost time: 57.42959928512573
Epoch: 40, Steps: 261 | Train Loss: 0.2896191 Vali Loss: 0.9677183 Test Loss: 0.4320691
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2856025
	speed: 0.9737s/iter; left time: 15151.1871s
	iters: 200, epoch: 41 | loss: 0.2966059
	speed: 0.2114s/iter; left time: 3268.5906s
Epoch: 41 cost time: 57.14899396896362
Epoch: 41, Steps: 261 | Train Loss: 0.2896177 Vali Loss: 0.9680417 Test Loss: 0.4320950
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.3096160
	speed: 0.9711s/iter; left time: 14857.5911s
	iters: 200, epoch: 42 | loss: 0.2987080
	speed: 0.2336s/iter; left time: 3550.7594s
Epoch: 42 cost time: 59.9091317653656
Epoch: 42, Steps: 261 | Train Loss: 0.2896293 Vali Loss: 0.9680108 Test Loss: 0.4320757
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2990826
	speed: 0.9705s/iter; left time: 14595.7808s
	iters: 200, epoch: 43 | loss: 0.2806385
	speed: 0.2241s/iter; left time: 3347.2992s
Epoch: 43 cost time: 60.34495687484741
Epoch: 43, Steps: 261 | Train Loss: 0.2895900 Vali Loss: 0.9674426 Test Loss: 0.4320176
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2750858
	speed: 0.9518s/iter; left time: 14065.5812s
	iters: 200, epoch: 44 | loss: 0.3291776
	speed: 0.2222s/iter; left time: 3262.1312s
Epoch: 44 cost time: 56.7441987991333
Epoch: 44, Steps: 261 | Train Loss: 0.2895280 Vali Loss: 0.9684216 Test Loss: 0.4321260
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2646165
	speed: 0.9553s/iter; left time: 13867.6666s
	iters: 200, epoch: 45 | loss: 0.2918817
	speed: 0.2201s/iter; left time: 3173.3771s
Epoch: 45 cost time: 58.13258194923401
Epoch: 45, Steps: 261 | Train Loss: 0.2895586 Vali Loss: 0.9680618 Test Loss: 0.4321365
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2766388
	speed: 0.9547s/iter; left time: 13609.6143s
	iters: 200, epoch: 46 | loss: 0.3144784
	speed: 0.2265s/iter; left time: 3206.5384s
Epoch: 46 cost time: 58.738216161727905
Epoch: 46, Steps: 261 | Train Loss: 0.2896017 Vali Loss: 0.9682204 Test Loss: 0.4321397
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3060569
	speed: 0.9545s/iter; left time: 13358.0448s
	iters: 200, epoch: 47 | loss: 0.2750370
	speed: 0.2247s/iter; left time: 3122.8753s
Epoch: 47 cost time: 59.72155475616455
Epoch: 47, Steps: 261 | Train Loss: 0.2896223 Vali Loss: 0.9679652 Test Loss: 0.4320856
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2837948
	speed: 0.9869s/iter; left time: 13554.1356s
	iters: 200, epoch: 48 | loss: 0.3067778
	speed: 0.2167s/iter; left time: 2954.8265s
Epoch: 48 cost time: 58.88600707054138
Epoch: 48, Steps: 261 | Train Loss: 0.2896073 Vali Loss: 0.9676141 Test Loss: 0.4320473
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.3127733
	speed: 0.9804s/iter; left time: 13208.2621s
	iters: 200, epoch: 49 | loss: 0.2881932
	speed: 0.2270s/iter; left time: 3035.4638s
Epoch: 49 cost time: 60.160524129867554
Epoch: 49, Steps: 261 | Train Loss: 0.2896326 Vali Loss: 0.9673569 Test Loss: 0.4321246
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2898764
	speed: 0.9206s/iter; left time: 12162.5642s
	iters: 200, epoch: 50 | loss: 0.2774586
	speed: 0.2300s/iter; left time: 3016.3197s
Epoch: 50 cost time: 59.54801821708679
Epoch: 50, Steps: 261 | Train Loss: 0.2895199 Vali Loss: 0.9673162 Test Loss: 0.4320908
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2739155
	speed: 0.9543s/iter; left time: 12359.6039s
	iters: 200, epoch: 51 | loss: 0.2863296
	speed: 0.2345s/iter; left time: 3013.8126s
Epoch: 51 cost time: 60.870214223861694
Epoch: 51, Steps: 261 | Train Loss: 0.2895566 Vali Loss: 0.9678898 Test Loss: 0.4321171
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2930053
	speed: 0.9345s/iter; left time: 11858.6885s
	iters: 200, epoch: 52 | loss: 0.2884620
	speed: 0.2036s/iter; left time: 2562.9067s
Epoch: 52 cost time: 56.15821099281311
Epoch: 52, Steps: 261 | Train Loss: 0.2895931 Vali Loss: 0.9679532 Test Loss: 0.4320839
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2986626
	speed: 0.8766s/iter; left time: 10895.2159s
	iters: 200, epoch: 53 | loss: 0.2986277
	speed: 0.1878s/iter; left time: 2315.7031s
Epoch: 53 cost time: 50.556663036346436
Epoch: 53, Steps: 261 | Train Loss: 0.2895242 Vali Loss: 0.9682447 Test Loss: 0.4321252
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2878993
	speed: 0.8762s/iter; left time: 10661.3744s
	iters: 200, epoch: 54 | loss: 0.2750838
	speed: 0.2124s/iter; left time: 2563.3789s
Epoch: 54 cost time: 55.394984006881714
Epoch: 54, Steps: 261 | Train Loss: 0.2895933 Vali Loss: 0.9674615 Test Loss: 0.4320995
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2896765
	speed: 0.8712s/iter; left time: 10373.4620s
	iters: 200, epoch: 55 | loss: 0.2885283
	speed: 0.1915s/iter; left time: 2261.3243s
Epoch: 55 cost time: 53.18244028091431
Epoch: 55, Steps: 261 | Train Loss: 0.2895860 Vali Loss: 0.9676628 Test Loss: 0.4320973
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2921419
	speed: 0.8877s/iter; left time: 10338.6544s
	iters: 200, epoch: 56 | loss: 0.3045503
	speed: 0.1992s/iter; left time: 2300.1970s
Epoch: 56 cost time: 53.76958918571472
Epoch: 56, Steps: 261 | Train Loss: 0.2895450 Vali Loss: 0.9688666 Test Loss: 0.4320902
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2922758
	speed: 0.8496s/iter; left time: 9672.6724s
	iters: 200, epoch: 57 | loss: 0.3109859
	speed: 0.1927s/iter; left time: 2174.4456s
Epoch: 57 cost time: 51.950589179992676
Epoch: 57, Steps: 261 | Train Loss: 0.2896597 Vali Loss: 0.9679325 Test Loss: 0.4321250
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2564599
	speed: 0.8657s/iter; left time: 9630.5948s
	iters: 200, epoch: 58 | loss: 0.2837279
	speed: 0.2028s/iter; left time: 2235.7332s
Epoch: 58 cost time: 52.95777726173401
Epoch: 58, Steps: 261 | Train Loss: 0.2895867 Vali Loss: 0.9679624 Test Loss: 0.4321592
EarlyStopping counter: 20 out of 20
Early stopping
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=26, out_features=78, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1817088.0
params:  2106.0
Trainable parameters:  2106
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3881746
	speed: 0.2105s/iter; left time: 5472.8718s
	iters: 200, epoch: 1 | loss: 0.4105038
	speed: 0.1935s/iter; left time: 5011.3654s
Epoch: 1 cost time: 53.741286277770996
Epoch: 1, Steps: 261 | Train Loss: 0.4181588 Vali Loss: 0.9656316 Test Loss: 0.4309590
Validation loss decreased (inf --> 0.965632).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4473813
	speed: 0.9130s/iter; left time: 23499.8204s
	iters: 200, epoch: 2 | loss: 0.4200610
	speed: 0.2012s/iter; left time: 5159.3140s
Epoch: 2 cost time: 52.95531749725342
Epoch: 2, Steps: 261 | Train Loss: 0.4176494 Vali Loss: 0.9639203 Test Loss: 0.4313246
Validation loss decreased (0.965632 --> 0.963920).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4255277
	speed: 0.8641s/iter; left time: 22016.8857s
	iters: 200, epoch: 3 | loss: 0.4001080
	speed: 0.1690s/iter; left time: 4289.1696s
Epoch: 3 cost time: 46.97839021682739
Epoch: 3, Steps: 261 | Train Loss: 0.4176039 Vali Loss: 0.9644908 Test Loss: 0.4308942
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4294806
	speed: 0.7065s/iter; left time: 17817.6538s
	iters: 200, epoch: 4 | loss: 0.3943278
	speed: 0.1506s/iter; left time: 3782.7356s
Epoch: 4 cost time: 42.20442533493042
Epoch: 4, Steps: 261 | Train Loss: 0.4173572 Vali Loss: 0.9638495 Test Loss: 0.4303396
Validation loss decreased (0.963920 --> 0.963849).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4187116
	speed: 0.6625s/iter; left time: 16533.5224s
	iters: 200, epoch: 5 | loss: 0.4136978
	speed: 0.1440s/iter; left time: 3578.4531s
Epoch: 5 cost time: 37.88138246536255
Epoch: 5, Steps: 261 | Train Loss: 0.4173713 Vali Loss: 0.9641418 Test Loss: 0.4308837
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4118404
	speed: 0.6499s/iter; left time: 16049.7806s
	iters: 200, epoch: 6 | loss: 0.3897469
	speed: 0.1476s/iter; left time: 3629.8260s
Epoch: 6 cost time: 40.31884503364563
Epoch: 6, Steps: 261 | Train Loss: 0.4172390 Vali Loss: 0.9632429 Test Loss: 0.4308567
Validation loss decreased (0.963849 --> 0.963243).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4497130
	speed: 0.6861s/iter; left time: 16764.7204s
	iters: 200, epoch: 7 | loss: 0.4145263
	speed: 0.1609s/iter; left time: 3915.8883s
Epoch: 7 cost time: 42.98319339752197
Epoch: 7, Steps: 261 | Train Loss: 0.4173211 Vali Loss: 0.9647534 Test Loss: 0.4311129
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4050929
	speed: 0.6383s/iter; left time: 15429.8393s
	iters: 200, epoch: 8 | loss: 0.4397929
	speed: 0.1514s/iter; left time: 3643.6845s
Epoch: 8 cost time: 39.605958223342896
Epoch: 8, Steps: 261 | Train Loss: 0.4172444 Vali Loss: 0.9633650 Test Loss: 0.4307690
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4178498
	speed: 0.6800s/iter; left time: 16259.6755s
	iters: 200, epoch: 9 | loss: 0.4244286
	speed: 0.1483s/iter; left time: 3530.6657s
Epoch: 9 cost time: 40.60756230354309
Epoch: 9, Steps: 261 | Train Loss: 0.4172863 Vali Loss: 0.9639896 Test Loss: 0.4309817
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4181577
	speed: 0.6518s/iter; left time: 15416.6678s
	iters: 200, epoch: 10 | loss: 0.4479756
	speed: 0.1496s/iter; left time: 3523.8075s
Epoch: 10 cost time: 39.53954005241394
Epoch: 10, Steps: 261 | Train Loss: 0.4171764 Vali Loss: 0.9648361 Test Loss: 0.4309906
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4301720
	speed: 0.6289s/iter; left time: 14711.5555s
	iters: 200, epoch: 11 | loss: 0.4648389
	speed: 0.1553s/iter; left time: 3618.0491s
Epoch: 11 cost time: 40.7642080783844
Epoch: 11, Steps: 261 | Train Loss: 0.4172393 Vali Loss: 0.9635091 Test Loss: 0.4306427
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4217992
	speed: 0.6882s/iter; left time: 15917.8570s
	iters: 200, epoch: 12 | loss: 0.4238978
	speed: 0.1449s/iter; left time: 3337.3718s
Epoch: 12 cost time: 39.13991022109985
Epoch: 12, Steps: 261 | Train Loss: 0.4171821 Vali Loss: 0.9633698 Test Loss: 0.4309827
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4204643
	speed: 0.6148s/iter; left time: 14059.7664s
	iters: 200, epoch: 13 | loss: 0.4075163
	speed: 0.1325s/iter; left time: 3017.1581s
Epoch: 13 cost time: 37.110474586486816
Epoch: 13, Steps: 261 | Train Loss: 0.4171827 Vali Loss: 0.9645194 Test Loss: 0.4309297
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4047611
	speed: 0.6542s/iter; left time: 14791.1994s
	iters: 200, epoch: 14 | loss: 0.3998981
	speed: 0.1562s/iter; left time: 3515.2809s
Epoch: 14 cost time: 40.70500445365906
Epoch: 14, Steps: 261 | Train Loss: 0.4172021 Vali Loss: 0.9631695 Test Loss: 0.4310248
Validation loss decreased (0.963243 --> 0.963169).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3959237
	speed: 0.5963s/iter; left time: 13325.2548s
	iters: 200, epoch: 15 | loss: 0.3937099
	speed: 0.1505s/iter; left time: 3347.9762s
Epoch: 15 cost time: 39.69131422042847
Epoch: 15, Steps: 261 | Train Loss: 0.4171227 Vali Loss: 0.9624367 Test Loss: 0.4305118
Validation loss decreased (0.963169 --> 0.962437).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4406548
	speed: 0.6666s/iter; left time: 14723.6246s
	iters: 200, epoch: 16 | loss: 0.4134310
	speed: 0.1614s/iter; left time: 3549.5599s
Epoch: 16 cost time: 41.155704736709595
Epoch: 16, Steps: 261 | Train Loss: 0.4169865 Vali Loss: 0.9640473 Test Loss: 0.4309064
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4206672
	speed: 0.6275s/iter; left time: 13695.8207s
	iters: 200, epoch: 17 | loss: 0.4414052
	speed: 0.1476s/iter; left time: 3206.4275s
Epoch: 17 cost time: 39.40844917297363
Epoch: 17, Steps: 261 | Train Loss: 0.4171181 Vali Loss: 0.9631701 Test Loss: 0.4307539
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4166476
	speed: 0.6260s/iter; left time: 13499.8128s
	iters: 200, epoch: 18 | loss: 0.3826568
	speed: 0.1466s/iter; left time: 3147.4976s
Epoch: 18 cost time: 40.009647369384766
Epoch: 18, Steps: 261 | Train Loss: 0.4170134 Vali Loss: 0.9644642 Test Loss: 0.4309338
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4187440
	speed: 0.6646s/iter; left time: 14158.2179s
	iters: 200, epoch: 19 | loss: 0.4077219
	speed: 0.1437s/iter; left time: 3046.8248s
Epoch: 19 cost time: 40.27289390563965
Epoch: 19, Steps: 261 | Train Loss: 0.4170175 Vali Loss: 0.9633459 Test Loss: 0.4307946
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3840766
	speed: 0.6453s/iter; left time: 13577.9135s
	iters: 200, epoch: 20 | loss: 0.4098378
	speed: 0.1600s/iter; left time: 3351.7584s
Epoch: 20 cost time: 41.67047667503357
Epoch: 20, Steps: 261 | Train Loss: 0.4170649 Vali Loss: 0.9626169 Test Loss: 0.4309447
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4260925
	speed: 0.6883s/iter; left time: 14304.4567s
	iters: 200, epoch: 21 | loss: 0.4206558
	speed: 0.1565s/iter; left time: 3236.6688s
Epoch: 21 cost time: 42.11348366737366
Epoch: 21, Steps: 261 | Train Loss: 0.4170264 Vali Loss: 0.9642711 Test Loss: 0.4306574
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4227843
	speed: 0.6400s/iter; left time: 13132.4896s
	iters: 200, epoch: 22 | loss: 0.4364416
	speed: 0.1503s/iter; left time: 3070.0820s
Epoch: 22 cost time: 39.8653609752655
Epoch: 22, Steps: 261 | Train Loss: 0.4171145 Vali Loss: 0.9634913 Test Loss: 0.4310287
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4570982
	speed: 0.6928s/iter; left time: 14036.4458s
	iters: 200, epoch: 23 | loss: 0.4414922
	speed: 0.1579s/iter; left time: 3183.4746s
Epoch: 23 cost time: 41.95502424240112
Epoch: 23, Steps: 261 | Train Loss: 0.4171172 Vali Loss: 0.9636037 Test Loss: 0.4309713
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4055398
	speed: 0.6717s/iter; left time: 13432.5968s
	iters: 200, epoch: 24 | loss: 0.4199799
	speed: 0.1536s/iter; left time: 3055.6031s
Epoch: 24 cost time: 40.38130331039429
Epoch: 24, Steps: 261 | Train Loss: 0.4169809 Vali Loss: 0.9631377 Test Loss: 0.4306491
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3891347
	speed: 0.6859s/iter; left time: 13537.5450s
	iters: 200, epoch: 25 | loss: 0.4405861
	speed: 0.1725s/iter; left time: 3387.3917s
Epoch: 25 cost time: 44.35580372810364
Epoch: 25, Steps: 261 | Train Loss: 0.4169748 Vali Loss: 0.9637240 Test Loss: 0.4308441
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4288118
	speed: 0.6594s/iter; left time: 12841.6234s
	iters: 200, epoch: 26 | loss: 0.4414209
	speed: 0.1560s/iter; left time: 3023.1377s
Epoch: 26 cost time: 40.42662811279297
Epoch: 26, Steps: 261 | Train Loss: 0.4170734 Vali Loss: 0.9644931 Test Loss: 0.4306808
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3957332
	speed: 0.6748s/iter; left time: 12966.3516s
	iters: 200, epoch: 27 | loss: 0.3916180
	speed: 0.1563s/iter; left time: 2988.1813s
Epoch: 27 cost time: 42.600282430648804
Epoch: 27, Steps: 261 | Train Loss: 0.4170908 Vali Loss: 0.9642008 Test Loss: 0.4308844
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4164961
	speed: 0.6647s/iter; left time: 12598.7211s
	iters: 200, epoch: 28 | loss: 0.4149325
	speed: 0.1450s/iter; left time: 2733.2529s
Epoch: 28 cost time: 39.661916971206665
Epoch: 28, Steps: 261 | Train Loss: 0.4171478 Vali Loss: 0.9632060 Test Loss: 0.4308649
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3937525
	speed: 0.6760s/iter; left time: 12635.6204s
	iters: 200, epoch: 29 | loss: 0.4258198
	speed: 0.1559s/iter; left time: 2898.8971s
Epoch: 29 cost time: 42.71095609664917
Epoch: 29, Steps: 261 | Train Loss: 0.4170322 Vali Loss: 0.9629186 Test Loss: 0.4307972
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4142898
	speed: 0.6890s/iter; left time: 12700.0997s
	iters: 200, epoch: 30 | loss: 0.4127711
	speed: 0.1460s/iter; left time: 2677.3782s
Epoch: 30 cost time: 41.63218021392822
Epoch: 30, Steps: 261 | Train Loss: 0.4170907 Vali Loss: 0.9631650 Test Loss: 0.4307260
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4173373
	speed: 0.6756s/iter; left time: 12276.8217s
	iters: 200, epoch: 31 | loss: 0.4328725
	speed: 0.1586s/iter; left time: 2866.8629s
Epoch: 31 cost time: 41.918619871139526
Epoch: 31, Steps: 261 | Train Loss: 0.4170055 Vali Loss: 0.9633492 Test Loss: 0.4307882
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4124014
	speed: 0.6697s/iter; left time: 11993.7423s
	iters: 200, epoch: 32 | loss: 0.4313840
	speed: 0.1589s/iter; left time: 2830.3938s
Epoch: 32 cost time: 42.33867526054382
Epoch: 32, Steps: 261 | Train Loss: 0.4169327 Vali Loss: 0.9632192 Test Loss: 0.4307472
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4375656
	speed: 0.6801s/iter; left time: 12002.5498s
	iters: 200, epoch: 33 | loss: 0.3806411
	speed: 0.1585s/iter; left time: 2781.3621s
Epoch: 33 cost time: 41.61297821998596
Epoch: 33, Steps: 261 | Train Loss: 0.4169986 Vali Loss: 0.9638579 Test Loss: 0.4308110
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4175896
	speed: 0.6891s/iter; left time: 11982.2953s
	iters: 200, epoch: 34 | loss: 0.4515064
	speed: 0.1576s/iter; left time: 2724.9636s
Epoch: 34 cost time: 42.13257622718811
Epoch: 34, Steps: 261 | Train Loss: 0.4169869 Vali Loss: 0.9634533 Test Loss: 0.4308515
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4454350
	speed: 0.6355s/iter; left time: 10884.0093s
	iters: 200, epoch: 35 | loss: 0.4236114
	speed: 0.1460s/iter; left time: 2486.5069s
Epoch: 35 cost time: 39.995113134384155
Epoch: 35, Steps: 261 | Train Loss: 0.4169313 Vali Loss: 0.9639662 Test Loss: 0.4309476
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.43021368980407715, mae:0.41903170943260193, rse:0.6240406036376953, corr:[0.5263315  0.52981514 0.53250813 0.53408813 0.53459513 0.53441375
 0.5339349  0.5334546  0.5331707  0.5332098  0.5336298  0.53408384
 0.5343347  0.5341919  0.53358024 0.5324772  0.5310772  0.5295745
 0.52796686 0.5263201  0.52466446 0.5232297  0.52199584 0.5210867
 0.52042943 0.5198477  0.5193466  0.51895607 0.5186266  0.51822215
 0.5178398  0.51758033 0.5174126  0.5173356  0.51730317 0.51735777
 0.51745003 0.5174987  0.5174686  0.5173065  0.51702785 0.51662236
 0.5161064  0.5155912  0.51513267 0.5147836  0.51463956 0.5146533
 0.5147585  0.5149407  0.5151059  0.51518154 0.5151705  0.51503605
 0.5147907  0.5144897  0.51414996 0.51384026 0.513637   0.51351506
 0.5135546  0.5137024  0.5139297  0.5141358  0.51433825 0.51452583
 0.51463455 0.5146242  0.51462364 0.5146132  0.5146217  0.514643
 0.5147011  0.5148034  0.51491255 0.51498187 0.5149857  0.51493067
 0.5148032  0.5145901  0.51430416 0.51396596 0.5135644  0.51309717
 0.51268077 0.51229966 0.51204205 0.5118766  0.51178056 0.51178586
 0.51187253 0.51191545 0.5118162  0.51155275 0.5111265  0.5105489
 0.50994915 0.5094509  0.5090713  0.508832   0.5087826  0.5089292
 0.50923556 0.5095629  0.50994945 0.5103796  0.5107269  0.5110559
 0.5112666  0.5114626  0.5115944  0.51165825 0.51161367 0.5115527
 0.5114841  0.51145166 0.51143014 0.51143754 0.51143914 0.5115264
 0.5116401  0.5116208  0.51144433 0.5112411  0.511021   0.5107578
 0.51049036 0.5102765  0.5101226  0.51003987 0.5099973  0.50994325
 0.5098626  0.5096938  0.509473   0.50918674 0.50885624 0.5084681
 0.50808114 0.50771767 0.5073893  0.5071579  0.50703174 0.5070108
 0.5070584  0.5071394  0.50719684 0.5072185  0.5072369  0.50718284
 0.5070681  0.5069733  0.50688845 0.50681967 0.5068033  0.5069024
 0.5070858  0.50735325 0.507644   0.50793713 0.5082453  0.50851643
 0.5087365  0.5088935  0.5090543  0.5092224  0.50938815 0.5096057
 0.5098179  0.5100032  0.51017326 0.5103377  0.5104346  0.5104615
 0.51040685 0.5103043  0.5101575  0.50999093 0.5097649  0.50951767
 0.5092458  0.5090182  0.5087829  0.5085971  0.5084716  0.50842214
 0.5084247  0.50847244 0.5085206  0.5085474  0.5084329  0.5081694
 0.50777537 0.5073847  0.5069936  0.5065164  0.50598294 0.5055513
 0.5051829  0.5048162  0.5045121  0.50426877 0.5040378  0.503838
 0.50366724 0.50342596 0.5031566  0.5028005  0.5023868  0.5019388
 0.5014042  0.5008516  0.50029016 0.49983236 0.4994582  0.4992416
 0.4991363  0.49906054 0.49889717 0.49875918 0.49858317 0.4983726
 0.49822605 0.49813056 0.49810374 0.49813938 0.49825588 0.49838257
 0.498493   0.49859068 0.4986039  0.49851108 0.49834478 0.4980738
 0.49778402 0.49749076 0.497158   0.49687812 0.49663097 0.4964266
 0.49625337 0.49610248 0.49601507 0.49597618 0.49595723 0.49597454
 0.49599934 0.49600473 0.49598742 0.49596804 0.49591452 0.49587038
 0.49579656 0.49577048 0.49576923 0.4957944  0.49579722 0.49583575
 0.4958746  0.49594867 0.49608368 0.49627918 0.49646363 0.49672607
 0.4969987  0.49726877 0.4974877  0.49770164 0.49787664 0.49793524
 0.49792847 0.4978617  0.49774238 0.4975875  0.4973838  0.49721614
 0.4970623  0.49700573 0.49694332 0.49693248 0.4969077  0.4968522
 0.49680713 0.49667686 0.4964569  0.49611723 0.49563372 0.49501333
 0.4942484  0.49349895 0.49287012 0.4922412  0.49167785 0.49121332
 0.4908298  0.49043068 0.49008808 0.48976725 0.48949656 0.48924163
 0.4890043  0.48878178 0.48860776 0.48848772 0.48838052 0.488283
 0.48821655 0.48812443 0.48804522 0.48803076 0.48810062 0.48822176
 0.4883852  0.48849264 0.48841926 0.48829833 0.48810396 0.48788658
 0.48767355 0.4874817  0.4873147  0.48718616 0.48708197 0.48697996
 0.48687682 0.48676714 0.4866211  0.48646617 0.4862579  0.48603874
 0.48580626 0.48558834 0.48537317 0.48517367 0.48500842 0.48489225
 0.4847998  0.48474693 0.484732   0.48470134 0.48469126 0.48472384
 0.48473775 0.48474953 0.48477486 0.4847772  0.48479584 0.4848517
 0.4848813  0.48493734 0.48500362 0.48504856 0.48510516 0.48518035
 0.48524728 0.4853073  0.48538238 0.4854517  0.48550925 0.48559466
 0.48565176 0.48571002 0.4857598  0.48581356 0.48585773 0.48588622
 0.48587435 0.4858486  0.48582682 0.48576725 0.485708   0.48566732
 0.4856724  0.48571524 0.48580736 0.48594818 0.48610142 0.4862575
 0.48641    0.48652095 0.48656878 0.48644537 0.48615572 0.4857434
 0.48520756 0.48467645 0.4842246  0.48378408 0.48332977 0.48298052
 0.4827192  0.48244685 0.48219725 0.4820193  0.48186043 0.48173976
 0.4815964  0.48150295 0.48143378 0.48136878 0.48125494 0.48110443
 0.48097932 0.48089546 0.48084757 0.4808365  0.48087993 0.4810012
 0.4811777  0.481246   0.4812185  0.48116848 0.4810799  0.48094204
 0.48080352 0.48065776 0.48055723 0.48048198 0.4804217  0.48035744
 0.48025274 0.48012796 0.47992986 0.47970557 0.47944942 0.4791918
 0.47892863 0.47872978 0.47854617 0.47840977 0.4782988  0.47827747
 0.47823066 0.4781446  0.47805774 0.47797462 0.4778792  0.4778089
 0.47774664 0.47771007 0.47771242 0.47773683 0.47780213 0.47791678
 0.4780037  0.47808906 0.47813815 0.47815773 0.47816128 0.47816148
 0.4781608  0.4781748  0.47818488 0.4782046  0.47824278 0.4783571
 0.4784564  0.47859836 0.47876194 0.4789815  0.4792064  0.4793966
 0.47953892 0.47965094 0.47969303 0.47971228 0.4797003  0.47967684
 0.47965655 0.47964007 0.4796163  0.4795847  0.47952366 0.47942802
 0.4792374  0.478967   0.4786079  0.4781249  0.47752443 0.4768403
 0.4760724  0.47533464 0.47463343 0.47393024 0.47324616 0.47261927
 0.4720617  0.47148407 0.47090372 0.47036627 0.4698327  0.46930218
 0.46885702 0.46854216 0.46831322 0.46811712 0.46799254 0.46785805
 0.46772155 0.46766362 0.4676915  0.46775904 0.4679016  0.4681306
 0.46843618 0.4686469  0.46874624 0.4687658  0.46870238 0.46862474
 0.46854585 0.46846732 0.46841922 0.46842074 0.4684301  0.468453
 0.4684542  0.4684078  0.46835    0.46820155 0.4679742  0.4677399
 0.46750635 0.46727377 0.46705845 0.4668445  0.46662593 0.46648154
 0.4663608  0.46624047 0.46614784 0.46608067 0.46605167 0.46606424
 0.46611315 0.46615428 0.46618646 0.466207   0.46621105 0.46620882
 0.46622154 0.46621695 0.46619663 0.46617195 0.46618074 0.46620727
 0.46620697 0.46617132 0.4661375  0.46610418 0.46606544 0.4661016
 0.46613985 0.46623674 0.46638155 0.4665864  0.46683258 0.46704614
 0.46721885 0.46735984 0.4674538  0.4675172  0.4675492  0.46756813
 0.46755877 0.4675939  0.4676085  0.4676195  0.46760562 0.46753848
 0.46741778 0.46717128 0.4668255  0.46631926 0.4656333  0.46477503
 0.46383673 0.4629486  0.46224186 0.46159416 0.46103495 0.46061856
 0.4603156  0.46000057 0.45970476 0.45942992 0.45912075 0.45876497
 0.45840368 0.45804918 0.45771572 0.45742092 0.4571325  0.45689058
 0.45662555 0.45644832 0.45633724 0.45631436 0.45637003 0.45655334
 0.4568341  0.45701426 0.4570272  0.45695728 0.4568392  0.45665154
 0.4564408  0.45626608 0.45614132 0.45606267 0.45602268 0.45602506
 0.45601708 0.45598602 0.45593208 0.45581663 0.45567146 0.45550135
 0.4553373  0.4551997  0.45509    0.45495945 0.4548569  0.4547915
 0.45473376 0.45469692 0.4546703  0.45463184 0.4545949  0.45455465
 0.45446724 0.45433006 0.4541827  0.4540282  0.45388824 0.45375144
 0.4536152  0.4535128  0.45345464 0.45343563 0.45350093 0.45358476
 0.45366642 0.45373785 0.4537788  0.4538143  0.4538322  0.45386446
 0.45393118 0.45403072 0.45418242 0.4543942  0.45461887 0.45483223
 0.45500723 0.45513675 0.45520025 0.4552186  0.45518443 0.45512226
 0.45505047 0.45500723 0.45500523 0.45501533 0.4550352  0.455037
 0.454988   0.4548648  0.45461458 0.4542373  0.4537497  0.453148
 0.45243198 0.45180136 0.45131165 0.45085675 0.45045677 0.4501011
 0.4497456  0.44938272 0.4490278  0.44868886 0.44838428 0.44815832
 0.4479962  0.44788414 0.44782847 0.44776043 0.4476826  0.44752532
 0.44741705 0.44734716 0.44731265 0.4473208  0.44734016 0.44749397
 0.44770938 0.44778106 0.44765437 0.44746676 0.44720927 0.44685954
 0.44649318 0.44617298 0.44591808 0.44571963 0.44552055 0.44532114
 0.44513056 0.4449862  0.44490215 0.4448963  0.4449944  0.4451348
 0.4453263  0.44561604 0.44594255 0.44617134 0.44598728 0.44476953]
