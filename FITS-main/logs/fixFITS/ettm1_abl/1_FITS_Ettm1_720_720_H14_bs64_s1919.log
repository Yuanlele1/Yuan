Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=1919, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4602542
	speed: 0.0221s/iter; left time: 568.1046s
	iters: 200, epoch: 1 | loss: 0.4640631
	speed: 0.0167s/iter; left time: 427.6045s
Epoch: 1 cost time: 4.805849075317383
Epoch: 1, Steps: 258 | Train Loss: 0.5025067 Vali Loss: 1.0061276 Test Loss: 0.4443125
Validation loss decreased (inf --> 1.006128).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4122151
	speed: 0.0688s/iter; left time: 1749.8411s
	iters: 200, epoch: 2 | loss: 0.4525040
	speed: 0.0169s/iter; left time: 427.4636s
Epoch: 2 cost time: 4.781243085861206
Epoch: 2, Steps: 258 | Train Loss: 0.4127329 Vali Loss: 0.9619054 Test Loss: 0.4188654
Validation loss decreased (1.006128 --> 0.961905).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4129052
	speed: 0.0699s/iter; left time: 1761.0757s
	iters: 200, epoch: 3 | loss: 0.3883758
	speed: 0.0170s/iter; left time: 425.6449s
Epoch: 3 cost time: 4.76470947265625
Epoch: 3, Steps: 258 | Train Loss: 0.4020852 Vali Loss: 0.9465268 Test Loss: 0.4147727
Validation loss decreased (0.961905 --> 0.946527).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3858245
	speed: 0.0689s/iter; left time: 1718.1746s
	iters: 200, epoch: 4 | loss: 0.3974395
	speed: 0.0173s/iter; left time: 429.0494s
Epoch: 4 cost time: 4.772158145904541
Epoch: 4, Steps: 258 | Train Loss: 0.3991911 Vali Loss: 0.9414886 Test Loss: 0.4146756
Validation loss decreased (0.946527 --> 0.941489).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4102562
	speed: 0.0690s/iter; left time: 1702.9396s
	iters: 200, epoch: 5 | loss: 0.4304048
	speed: 0.0170s/iter; left time: 416.8625s
Epoch: 5 cost time: 4.752701044082642
Epoch: 5, Steps: 258 | Train Loss: 0.3980331 Vali Loss: 0.9389655 Test Loss: 0.4152530
Validation loss decreased (0.941489 --> 0.938965).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3753613
	speed: 0.0701s/iter; left time: 1710.5631s
	iters: 200, epoch: 6 | loss: 0.4147762
	speed: 0.0165s/iter; left time: 400.8005s
Epoch: 6 cost time: 4.765763521194458
Epoch: 6, Steps: 258 | Train Loss: 0.3978239 Vali Loss: 0.9370382 Test Loss: 0.4156452
Validation loss decreased (0.938965 --> 0.937038).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4140219
	speed: 0.0699s/iter; left time: 1687.7134s
	iters: 200, epoch: 7 | loss: 0.3755326
	speed: 0.0165s/iter; left time: 396.4666s
Epoch: 7 cost time: 4.764625549316406
Epoch: 7, Steps: 258 | Train Loss: 0.3974595 Vali Loss: 0.9350947 Test Loss: 0.4159712
Validation loss decreased (0.937038 --> 0.935095).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4212998
	speed: 0.0696s/iter; left time: 1662.1684s
	iters: 200, epoch: 8 | loss: 0.4218146
	speed: 0.0167s/iter; left time: 396.9210s
Epoch: 8 cost time: 4.7183287143707275
Epoch: 8, Steps: 258 | Train Loss: 0.3973577 Vali Loss: 0.9347213 Test Loss: 0.4158058
Validation loss decreased (0.935095 --> 0.934721).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3926979
	speed: 0.0703s/iter; left time: 1662.1703s
	iters: 200, epoch: 9 | loss: 0.3986612
	speed: 0.0170s/iter; left time: 400.1155s
Epoch: 9 cost time: 4.890104532241821
Epoch: 9, Steps: 258 | Train Loss: 0.3970294 Vali Loss: 0.9334939 Test Loss: 0.4159157
Validation loss decreased (0.934721 --> 0.933494).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4127474
	speed: 0.0701s/iter; left time: 1638.1301s
	iters: 200, epoch: 10 | loss: 0.4233306
	speed: 0.0168s/iter; left time: 390.2297s
Epoch: 10 cost time: 4.746904134750366
Epoch: 10, Steps: 258 | Train Loss: 0.3970084 Vali Loss: 0.9337901 Test Loss: 0.4161246
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3884363
	speed: 0.0705s/iter; left time: 1629.7597s
	iters: 200, epoch: 11 | loss: 0.3875858
	speed: 0.0167s/iter; left time: 384.9349s
Epoch: 11 cost time: 4.7088587284088135
Epoch: 11, Steps: 258 | Train Loss: 0.3968974 Vali Loss: 0.9324639 Test Loss: 0.4157047
Validation loss decreased (0.933494 --> 0.932464).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4078477
	speed: 0.0704s/iter; left time: 1609.4373s
	iters: 200, epoch: 12 | loss: 0.4198991
	speed: 0.0167s/iter; left time: 379.0820s
Epoch: 12 cost time: 4.824730157852173
Epoch: 12, Steps: 258 | Train Loss: 0.3967550 Vali Loss: 0.9329531 Test Loss: 0.4160424
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3821144
	speed: 0.0704s/iter; left time: 1591.9679s
	iters: 200, epoch: 13 | loss: 0.3941148
	speed: 0.0168s/iter; left time: 378.0467s
Epoch: 13 cost time: 4.801920175552368
Epoch: 13, Steps: 258 | Train Loss: 0.3967893 Vali Loss: 0.9325051 Test Loss: 0.4160841
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3612601
	speed: 0.0707s/iter; left time: 1579.6814s
	iters: 200, epoch: 14 | loss: 0.3902140
	speed: 0.0170s/iter; left time: 378.9223s
Epoch: 14 cost time: 4.781304836273193
Epoch: 14, Steps: 258 | Train Loss: 0.3967706 Vali Loss: 0.9325235 Test Loss: 0.4163438
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4033030
	speed: 0.0709s/iter; left time: 1565.0380s
	iters: 200, epoch: 15 | loss: 0.4189094
	speed: 0.0169s/iter; left time: 372.6850s
Epoch: 15 cost time: 4.867129325866699
Epoch: 15, Steps: 258 | Train Loss: 0.3966764 Vali Loss: 0.9314769 Test Loss: 0.4162986
Validation loss decreased (0.932464 --> 0.931477).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3780318
	speed: 0.0710s/iter; left time: 1549.1082s
	iters: 200, epoch: 16 | loss: 0.3666559
	speed: 0.0169s/iter; left time: 367.4010s
Epoch: 16 cost time: 4.793561935424805
Epoch: 16, Steps: 258 | Train Loss: 0.3965490 Vali Loss: 0.9316252 Test Loss: 0.4162564
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3763765
	speed: 0.0701s/iter; left time: 1513.0509s
	iters: 200, epoch: 17 | loss: 0.4032270
	speed: 0.0169s/iter; left time: 362.3481s
Epoch: 17 cost time: 4.864763498306274
Epoch: 17, Steps: 258 | Train Loss: 0.3965164 Vali Loss: 0.9309777 Test Loss: 0.4162215
Validation loss decreased (0.931477 --> 0.930978).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4265296
	speed: 0.0696s/iter; left time: 1483.1686s
	iters: 200, epoch: 18 | loss: 0.3996300
	speed: 0.0166s/iter; left time: 352.5701s
Epoch: 18 cost time: 4.7538557052612305
Epoch: 18, Steps: 258 | Train Loss: 0.3965839 Vali Loss: 0.9308352 Test Loss: 0.4163145
Validation loss decreased (0.930978 --> 0.930835).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3963105
	speed: 0.0692s/iter; left time: 1456.3724s
	iters: 200, epoch: 19 | loss: 0.3853300
	speed: 0.0169s/iter; left time: 353.6625s
Epoch: 19 cost time: 4.789527416229248
Epoch: 19, Steps: 258 | Train Loss: 0.3964961 Vali Loss: 0.9316276 Test Loss: 0.4160945
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4014186
	speed: 0.0703s/iter; left time: 1463.1246s
	iters: 200, epoch: 20 | loss: 0.4000459
	speed: 0.0165s/iter; left time: 342.3686s
Epoch: 20 cost time: 4.814316272735596
Epoch: 20, Steps: 258 | Train Loss: 0.3965966 Vali Loss: 0.9311869 Test Loss: 0.4163427
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4125532
	speed: 0.0699s/iter; left time: 1435.2621s
	iters: 200, epoch: 21 | loss: 0.3863945
	speed: 0.0164s/iter; left time: 334.4513s
Epoch: 21 cost time: 4.734422922134399
Epoch: 21, Steps: 258 | Train Loss: 0.3965978 Vali Loss: 0.9315478 Test Loss: 0.4158963
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4081834
	speed: 0.0697s/iter; left time: 1414.0297s
	iters: 200, epoch: 22 | loss: 0.3777140
	speed: 0.0167s/iter; left time: 336.7093s
Epoch: 22 cost time: 4.793087959289551
Epoch: 22, Steps: 258 | Train Loss: 0.3964908 Vali Loss: 0.9317134 Test Loss: 0.4162943
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3989965
	speed: 0.0705s/iter; left time: 1412.2162s
	iters: 200, epoch: 23 | loss: 0.3741107
	speed: 0.0166s/iter; left time: 330.4988s
Epoch: 23 cost time: 4.829454183578491
Epoch: 23, Steps: 258 | Train Loss: 0.3965201 Vali Loss: 0.9316401 Test Loss: 0.4163475
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4007470
	speed: 0.0692s/iter; left time: 1367.3056s
	iters: 200, epoch: 24 | loss: 0.4205778
	speed: 0.0168s/iter; left time: 329.4262s
Epoch: 24 cost time: 4.82680606842041
Epoch: 24, Steps: 258 | Train Loss: 0.3963796 Vali Loss: 0.9309304 Test Loss: 0.4162804
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3962411
	speed: 0.0697s/iter; left time: 1359.4467s
	iters: 200, epoch: 25 | loss: 0.3779438
	speed: 0.0164s/iter; left time: 319.2179s
Epoch: 25 cost time: 4.784128427505493
Epoch: 25, Steps: 258 | Train Loss: 0.3963367 Vali Loss: 0.9301300 Test Loss: 0.4161040
Validation loss decreased (0.930835 --> 0.930130).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4150677
	speed: 0.0695s/iter; left time: 1338.8610s
	iters: 200, epoch: 26 | loss: 0.4203307
	speed: 0.0169s/iter; left time: 324.2155s
Epoch: 26 cost time: 4.755673170089722
Epoch: 26, Steps: 258 | Train Loss: 0.3964209 Vali Loss: 0.9310713 Test Loss: 0.4161777
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4022098
	speed: 0.0696s/iter; left time: 1321.7269s
	iters: 200, epoch: 27 | loss: 0.3922333
	speed: 0.0166s/iter; left time: 314.5534s
Epoch: 27 cost time: 4.764607906341553
Epoch: 27, Steps: 258 | Train Loss: 0.3964011 Vali Loss: 0.9306099 Test Loss: 0.4160829
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4034977
	speed: 0.0704s/iter; left time: 1318.9464s
	iters: 200, epoch: 28 | loss: 0.4067354
	speed: 0.0168s/iter; left time: 312.3102s
Epoch: 28 cost time: 4.717103719711304
Epoch: 28, Steps: 258 | Train Loss: 0.3963225 Vali Loss: 0.9309777 Test Loss: 0.4161803
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3749178
	speed: 0.0701s/iter; left time: 1295.1007s
	iters: 200, epoch: 29 | loss: 0.3694463
	speed: 0.0169s/iter; left time: 309.8641s
Epoch: 29 cost time: 4.817904233932495
Epoch: 29, Steps: 258 | Train Loss: 0.3962890 Vali Loss: 0.9304917 Test Loss: 0.4162407
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4030504
	speed: 0.0692s/iter; left time: 1260.8784s
	iters: 200, epoch: 30 | loss: 0.4162708
	speed: 0.0170s/iter; left time: 307.2174s
Epoch: 30 cost time: 4.735795021057129
Epoch: 30, Steps: 258 | Train Loss: 0.3962717 Vali Loss: 0.9304067 Test Loss: 0.4163250
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3679391
	speed: 0.0703s/iter; left time: 1262.7600s
	iters: 200, epoch: 31 | loss: 0.4184503
	speed: 0.0170s/iter; left time: 303.2103s
Epoch: 31 cost time: 4.766834735870361
Epoch: 31, Steps: 258 | Train Loss: 0.3961780 Vali Loss: 0.9309032 Test Loss: 0.4161014
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3833238
	speed: 0.0699s/iter; left time: 1238.1559s
	iters: 200, epoch: 32 | loss: 0.3968065
	speed: 0.0169s/iter; left time: 297.1196s
Epoch: 32 cost time: 4.799363851547241
Epoch: 32, Steps: 258 | Train Loss: 0.3963650 Vali Loss: 0.9306843 Test Loss: 0.4163048
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3951080
	speed: 0.0689s/iter; left time: 1202.0529s
	iters: 200, epoch: 33 | loss: 0.4056104
	speed: 0.0169s/iter; left time: 293.8829s
Epoch: 33 cost time: 4.771912336349487
Epoch: 33, Steps: 258 | Train Loss: 0.3963448 Vali Loss: 0.9306632 Test Loss: 0.4161139
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3963893
	speed: 0.0719s/iter; left time: 1236.2854s
	iters: 200, epoch: 34 | loss: 0.4154358
	speed: 0.0167s/iter; left time: 285.0803s
Epoch: 34 cost time: 4.812316656112671
Epoch: 34, Steps: 258 | Train Loss: 0.3963630 Vali Loss: 0.9311171 Test Loss: 0.4163966
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3840444
	speed: 0.0696s/iter; left time: 1177.4455s
	iters: 200, epoch: 35 | loss: 0.4090478
	speed: 0.0167s/iter; left time: 280.7778s
Epoch: 35 cost time: 4.813207149505615
Epoch: 35, Steps: 258 | Train Loss: 0.3962969 Vali Loss: 0.9310587 Test Loss: 0.4161675
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4102217
	speed: 0.0699s/iter; left time: 1164.9827s
	iters: 200, epoch: 36 | loss: 0.3915822
	speed: 0.0169s/iter; left time: 279.4691s
Epoch: 36 cost time: 4.820565223693848
Epoch: 36, Steps: 258 | Train Loss: 0.3962547 Vali Loss: 0.9307572 Test Loss: 0.4163189
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3968482
	speed: 0.0694s/iter; left time: 1139.2770s
	iters: 200, epoch: 37 | loss: 0.3798578
	speed: 0.0169s/iter; left time: 275.9145s
Epoch: 37 cost time: 4.736780405044556
Epoch: 37, Steps: 258 | Train Loss: 0.3961610 Vali Loss: 0.9301850 Test Loss: 0.4162019
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3649402
	speed: 0.0701s/iter; left time: 1132.8727s
	iters: 200, epoch: 38 | loss: 0.4268223
	speed: 0.0166s/iter; left time: 266.9326s
Epoch: 38 cost time: 4.8481292724609375
Epoch: 38, Steps: 258 | Train Loss: 0.3962706 Vali Loss: 0.9307536 Test Loss: 0.4163586
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4015591
	speed: 0.0707s/iter; left time: 1124.5195s
	iters: 200, epoch: 39 | loss: 0.3916437
	speed: 0.0171s/iter; left time: 270.8333s
Epoch: 39 cost time: 4.9092230796813965
Epoch: 39, Steps: 258 | Train Loss: 0.3963152 Vali Loss: 0.9312402 Test Loss: 0.4162698
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4187048
	speed: 0.0713s/iter; left time: 1114.3922s
	iters: 200, epoch: 40 | loss: 0.3992157
	speed: 0.0170s/iter; left time: 264.1000s
Epoch: 40 cost time: 4.787719964981079
Epoch: 40, Steps: 258 | Train Loss: 0.3962619 Vali Loss: 0.9306846 Test Loss: 0.4159437
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3841321
	speed: 0.0701s/iter; left time: 1077.6973s
	iters: 200, epoch: 41 | loss: 0.3919136
	speed: 0.0171s/iter; left time: 260.9205s
Epoch: 41 cost time: 4.842027187347412
Epoch: 41, Steps: 258 | Train Loss: 0.3962242 Vali Loss: 0.9305086 Test Loss: 0.4163014
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4147538
	speed: 0.0697s/iter; left time: 1054.3624s
	iters: 200, epoch: 42 | loss: 0.3799115
	speed: 0.0169s/iter; left time: 253.5294s
Epoch: 42 cost time: 4.851330280303955
Epoch: 42, Steps: 258 | Train Loss: 0.3962529 Vali Loss: 0.9309716 Test Loss: 0.4162866
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3691075
	speed: 0.0706s/iter; left time: 1049.5278s
	iters: 200, epoch: 43 | loss: 0.3977515
	speed: 0.0167s/iter; left time: 247.1128s
Epoch: 43 cost time: 4.795261383056641
Epoch: 43, Steps: 258 | Train Loss: 0.3961818 Vali Loss: 0.9307416 Test Loss: 0.4161598
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3839167
	speed: 0.0715s/iter; left time: 1044.7528s
	iters: 200, epoch: 44 | loss: 0.4103054
	speed: 0.0166s/iter; left time: 241.3585s
Epoch: 44 cost time: 4.842393159866333
Epoch: 44, Steps: 258 | Train Loss: 0.3961369 Vali Loss: 0.9314305 Test Loss: 0.4163083
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4037757
	speed: 0.0690s/iter; left time: 990.6454s
	iters: 200, epoch: 45 | loss: 0.3488165
	speed: 0.0167s/iter; left time: 238.0840s
Epoch: 45 cost time: 4.76802659034729
Epoch: 45, Steps: 258 | Train Loss: 0.3960817 Vali Loss: 0.9304960 Test Loss: 0.4163764
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4150996804237366, mae:0.41149136424064636, rse:0.6129809021949768, corr:[0.52480435 0.5313526  0.53358746 0.53396523 0.53489935 0.53682923
 0.5385974  0.53928244 0.53934085 0.5396016  0.54046726 0.5415279
 0.54226613 0.5423226  0.54165673 0.54042417 0.53897226 0.5376806
 0.5366534  0.53582066 0.53491503 0.5336624  0.5321103  0.53073984
 0.5296042  0.528815   0.5281044  0.5271771  0.5263474  0.52601874
 0.52647066 0.52761847 0.52875483 0.52942467 0.52939326 0.52933156
 0.5294357  0.529673   0.5296427  0.5290131  0.528132   0.52746195
 0.52738494 0.5278106  0.5281138  0.5279419  0.5275399  0.5272779
 0.5272803  0.52732664 0.52726936 0.5270001  0.526741   0.52660424
 0.52671254 0.5268841  0.5269055  0.5266657  0.5263761  0.5261364
 0.5260743  0.5261061  0.5260967  0.5259256  0.52585775 0.5260527
 0.5263885  0.5266409  0.52673554 0.5266685  0.5266016  0.52656543
 0.52650476 0.52632713 0.52608377 0.5257972  0.5255321  0.52537763
 0.5253154  0.5252771  0.5251873  0.52502584 0.52486086 0.52470183
 0.5246729  0.5247215  0.52482533 0.52489084 0.52491677 0.5250057
 0.5251625  0.5253667  0.5256147  0.52585185 0.5260263  0.52602524
 0.5258729  0.52568525 0.52539593 0.5251605  0.52500683 0.52491766
 0.5248553  0.52472234 0.52457404 0.524515   0.52445084 0.52440655
 0.5242837  0.5240919  0.52381736 0.5234515  0.52304584 0.52268857
 0.52239484 0.52217215 0.5219684  0.52170694 0.521416   0.5212338
 0.5211956  0.5212688  0.5213138  0.5213213  0.52123    0.5209362
 0.5205049  0.5201665  0.52000755 0.52001363 0.519937   0.51962763
 0.5191766  0.5188015  0.5186681  0.5188387  0.51916933 0.51925707
 0.5190485  0.5187489  0.51859593 0.5188406  0.5194177  0.5200188
 0.5203582  0.52031213 0.5200959  0.51992357 0.5199323  0.52000487
 0.5200243  0.5200383  0.52005905 0.5200789  0.52004284 0.5199879
 0.5198619  0.5196512  0.5194597  0.5192605  0.5191754  0.5192384
 0.5194725  0.5198437  0.5202748  0.52064276 0.5208658  0.5209689
 0.52098614 0.52097845 0.52105355 0.5211538  0.5211004  0.5208944
 0.5206683  0.5205688  0.5206402  0.52077943 0.52078736 0.52060276
 0.52030045 0.52010864 0.5201531  0.5204702  0.5208858  0.52117044
 0.5212303  0.52123034 0.52134895 0.52159363 0.5218505  0.5219424
 0.521773   0.5213843  0.52080494 0.52013314 0.5194831  0.5189152
 0.5183498  0.5177248  0.51700836 0.516283   0.5155893  0.51496303
 0.5143893  0.51380646 0.5131855  0.5126163  0.51215476 0.51179814
 0.5114347  0.5110028  0.5104372  0.5097777  0.50908977 0.508583
 0.5083127  0.5081027  0.5078278  0.5074029  0.50694335 0.50668335
 0.5067879  0.50705904 0.5073426  0.5074938  0.50741696 0.50721973
 0.5070704  0.50708103 0.50723016 0.5073585  0.5074148  0.50742173
 0.5074148  0.50747585 0.5075294  0.50770813 0.5079489  0.50825715
 0.5085089  0.5085243  0.5083348  0.50807005 0.5078689  0.50776374
 0.50774926 0.50779843 0.5078121  0.50777256 0.5076372  0.50752133
 0.50740755 0.5073945  0.5074083  0.50742036 0.5074515  0.50753075
 0.5076535  0.5077865  0.50785613 0.507835   0.50775033 0.50772756
 0.50770897 0.50768304 0.50765556 0.5077008  0.50781864 0.5079512
 0.50810426 0.5082319  0.5083289  0.5083805  0.50835174 0.5082641
 0.50812864 0.5080927  0.5081148  0.5082252  0.50831056 0.5083034
 0.5082894  0.5082829  0.5083053  0.50831646 0.50816077 0.50773007
 0.50708336 0.50643134 0.50591904 0.5055347  0.50520724 0.5048237
 0.5043354  0.50373226 0.50318784 0.50275844 0.50241715 0.50209576
 0.5016824  0.50120455 0.50076604 0.50043523 0.500223   0.50005007
 0.49994075 0.4997532  0.4994479  0.49904352 0.49865627 0.4983228
 0.49809954 0.49804878 0.49807075 0.49812612 0.49819666 0.49830166
 0.49837345 0.49830922 0.49811357 0.49787074 0.49770385 0.49766907
 0.49770853 0.49772707 0.4975728  0.49728537 0.49697244 0.49677435
 0.49680227 0.49691567 0.49692962 0.4968349  0.49672788 0.49683326
 0.49707773 0.497187   0.4971682  0.49703822 0.49690405 0.496834
 0.4968082  0.49684405 0.4968622  0.49685892 0.49685234 0.49688616
 0.49687067 0.49682072 0.4967361  0.4966323  0.49662086 0.49670923
 0.49679852 0.49681938 0.4967066  0.4964905  0.49631658 0.49627605
 0.49632746 0.49648592 0.49664313 0.49672058 0.49671417 0.49663684
 0.49643928 0.49621916 0.49601066 0.4957963  0.49566957 0.49566427
 0.49576876 0.49595004 0.4961355  0.49633366 0.4965498  0.49686745
 0.49730724 0.49780494 0.4982382  0.49848667 0.49850836 0.49835533
 0.4980795  0.49773183 0.49734905 0.4969058  0.49639064 0.4959815
 0.49570075 0.49553046 0.49537647 0.4951795  0.4948351  0.49438807
 0.4939213  0.49351987 0.4931783  0.49280795 0.4923095  0.49181455
 0.49139738 0.4911923  0.4910868  0.49098253 0.4907501  0.490422
 0.49006742 0.48983058 0.48976865 0.48988482 0.49009937 0.4902318
 0.49036226 0.4903903  0.4904588  0.49053875 0.49066693 0.49082628
 0.49085808 0.4908197  0.49072212 0.490678   0.49068946 0.49078497
 0.49083894 0.49083138 0.49075416 0.49079704 0.49093696 0.49120146
 0.49136057 0.49127156 0.4910459  0.49085543 0.49076667 0.49074543
 0.49068028 0.49050173 0.49025244 0.49003166 0.48994645 0.49001747
 0.49013573 0.49024326 0.4902768  0.49025962 0.4902585  0.49028295
 0.49037483 0.4904602  0.49049196 0.4904479  0.490379   0.49041086
 0.4904662  0.49055058 0.49060458 0.49062866 0.49053186 0.4904073
 0.49028546 0.49024037 0.4901896  0.49014464 0.49010262 0.49007294
 0.49012125 0.49020153 0.4902987  0.49035174 0.49033114 0.4902473
 0.49013335 0.49010524 0.4901437  0.49011222 0.489913   0.4895395
 0.4890457  0.4885253  0.48803902 0.48755023 0.48702827 0.48651764
 0.48607722 0.4856032  0.4851081  0.48461956 0.48406187 0.48341894
 0.48271152 0.48202738 0.48137206 0.48071116 0.48008403 0.47958645
 0.47920093 0.47894984 0.47876045 0.4784476  0.47816494 0.4779256
 0.47780365 0.4777591  0.47768897 0.47762612 0.47756872 0.47761747
 0.4778197  0.47809058 0.478364   0.4786789  0.47892302 0.4790507
 0.47905627 0.4790979  0.47926426 0.47947535 0.47967324 0.4798424
 0.47994745 0.4799993  0.480017   0.4799999  0.48006734 0.4802576
 0.48045596 0.4805418  0.4804777  0.48040012 0.48036578 0.4803143
 0.48020133 0.48001948 0.47986656 0.47983447 0.4799456  0.48014545
 0.4803408  0.4804298  0.4803761  0.48017555 0.47989154 0.47968623
 0.47962496 0.47971806 0.47988042 0.47995868 0.47994477 0.47997862
 0.4800106  0.4800708  0.4801789  0.48032218 0.48041016 0.48043427
 0.48036236 0.4802398  0.4801159  0.48005393 0.4800665  0.48010597
 0.4801251  0.48022774 0.480308   0.48038763 0.48044208 0.48047584
 0.4804981  0.48053518 0.48065013 0.4807632  0.48074928 0.480517
 0.4801133  0.47965324 0.47916466 0.47863007 0.47804737 0.4775143
 0.47699988 0.47651517 0.47604343 0.4755916  0.47517535 0.4747281
 0.4742764  0.4737641  0.47319248 0.47256145 0.47197834 0.47148037
 0.47108454 0.4707562  0.47050175 0.47027233 0.4700229  0.4697578
 0.46957755 0.4694427  0.4693579  0.46930918 0.4693323  0.46940905
 0.46963874 0.46991506 0.47014332 0.47027212 0.470304   0.47037548
 0.47055107 0.4707424  0.47095668 0.4711048  0.4711976  0.47126457
 0.4714171  0.47170874 0.47197792 0.47218046 0.47242373 0.47267467
 0.47279748 0.47282225 0.4727988  0.47280598 0.47296202 0.47320223
 0.47329554 0.4732451  0.47308967 0.4728536  0.47269386 0.47255683
 0.47234398 0.47207305 0.4717953  0.47145545 0.47121271 0.47106773
 0.4710158  0.47100776 0.4710196  0.47104335 0.47104236 0.47104034
 0.47096992 0.47077608 0.47063196 0.4706207  0.47069058 0.47073644
 0.47069895 0.47059748 0.47050175 0.47046375 0.47049242 0.47056586
 0.47067633 0.4708868  0.4711141  0.47122103 0.47127283 0.47128814
 0.47133476 0.4714334  0.47154483 0.4715942  0.47146028 0.47117132
 0.47088867 0.47079954 0.47079918 0.47062805 0.4701687  0.46949205
 0.46879143 0.46818176 0.46780142 0.46754155 0.46718913 0.46694803
 0.46670687 0.46661273 0.4665137  0.46636263 0.4660424  0.4655722
 0.46515894 0.4649118  0.46487236 0.46477413 0.4646109  0.4644156
 0.46418247 0.4640445  0.46395963 0.46392027 0.46402147 0.4640855
 0.4641691  0.464212   0.46428153 0.46454096 0.4647879  0.4650067
 0.46516544 0.46530333 0.46550414 0.46581542 0.4663641  0.46696013
 0.4675536  0.46789673 0.46775818 0.4677523  0.467862   0.46515766]
