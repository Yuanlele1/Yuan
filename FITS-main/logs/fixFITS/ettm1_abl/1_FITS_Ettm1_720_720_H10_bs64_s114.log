Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=90, out_features=180, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14515200.0
params:  16380.0
Trainable parameters:  16380
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5380737
	speed: 0.0426s/iter; left time: 1093.9733s
	iters: 200, epoch: 1 | loss: 0.4451547
	speed: 0.0342s/iter; left time: 876.1031s
Epoch: 1 cost time: 9.555935621261597
Epoch: 1, Steps: 258 | Train Loss: 0.5148773 Vali Loss: 1.0234624 Test Loss: 0.4453326
Validation loss decreased (inf --> 1.023462).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4203031
	speed: 0.1445s/iter; left time: 3676.1929s
	iters: 200, epoch: 2 | loss: 0.4166641
	speed: 0.0277s/iter; left time: 702.6754s
Epoch: 2 cost time: 8.548922777175903
Epoch: 2, Steps: 258 | Train Loss: 0.4148431 Vali Loss: 0.9673131 Test Loss: 0.4188263
Validation loss decreased (1.023462 --> 0.967313).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3918861
	speed: 0.1562s/iter; left time: 3934.1454s
	iters: 200, epoch: 3 | loss: 0.4086730
	speed: 0.0444s/iter; left time: 1114.2724s
Epoch: 3 cost time: 12.83094072341919
Epoch: 3, Steps: 258 | Train Loss: 0.4032265 Vali Loss: 0.9506522 Test Loss: 0.4144337
Validation loss decreased (0.967313 --> 0.950652).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4184019
	speed: 0.1958s/iter; left time: 4879.6616s
	iters: 200, epoch: 4 | loss: 0.4219819
	speed: 0.0452s/iter; left time: 1122.5302s
Epoch: 4 cost time: 10.815732955932617
Epoch: 4, Steps: 258 | Train Loss: 0.3999426 Vali Loss: 0.9433703 Test Loss: 0.4143131
Validation loss decreased (0.950652 --> 0.943370).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4151027
	speed: 0.1693s/iter; left time: 4177.6025s
	iters: 200, epoch: 5 | loss: 0.4395168
	speed: 0.0370s/iter; left time: 908.2497s
Epoch: 5 cost time: 10.878414630889893
Epoch: 5, Steps: 258 | Train Loss: 0.3987872 Vali Loss: 0.9396282 Test Loss: 0.4149977
Validation loss decreased (0.943370 --> 0.939628).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3846772
	speed: 0.1782s/iter; left time: 4350.9623s
	iters: 200, epoch: 6 | loss: 0.4128349
	speed: 0.0409s/iter; left time: 995.4974s
Epoch: 6 cost time: 10.142426490783691
Epoch: 6, Steps: 258 | Train Loss: 0.3981571 Vali Loss: 0.9370371 Test Loss: 0.4160465
Validation loss decreased (0.939628 --> 0.937037).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3683546
	speed: 0.1423s/iter; left time: 3435.9401s
	iters: 200, epoch: 7 | loss: 0.3822961
	speed: 0.0330s/iter; left time: 792.6534s
Epoch: 7 cost time: 8.533884525299072
Epoch: 7, Steps: 258 | Train Loss: 0.3980315 Vali Loss: 0.9366630 Test Loss: 0.4159429
Validation loss decreased (0.937037 --> 0.936663).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3831772
	speed: 0.1515s/iter; left time: 3619.1707s
	iters: 200, epoch: 8 | loss: 0.3745782
	speed: 0.0292s/iter; left time: 694.1324s
Epoch: 8 cost time: 10.042103052139282
Epoch: 8, Steps: 258 | Train Loss: 0.3976055 Vali Loss: 0.9349158 Test Loss: 0.4162522
Validation loss decreased (0.936663 --> 0.934916).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3889568
	speed: 0.1621s/iter; left time: 3830.4775s
	iters: 200, epoch: 9 | loss: 0.3767785
	speed: 0.0448s/iter; left time: 1053.3403s
Epoch: 9 cost time: 11.601394176483154
Epoch: 9, Steps: 258 | Train Loss: 0.3976061 Vali Loss: 0.9344810 Test Loss: 0.4165640
Validation loss decreased (0.934916 --> 0.934481).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3813459
	speed: 0.1916s/iter; left time: 4480.3668s
	iters: 200, epoch: 10 | loss: 0.3997947
	speed: 0.0404s/iter; left time: 939.9796s
Epoch: 10 cost time: 10.857032060623169
Epoch: 10, Steps: 258 | Train Loss: 0.3976103 Vali Loss: 0.9338166 Test Loss: 0.4165705
Validation loss decreased (0.934481 --> 0.933817).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4338440
	speed: 0.1692s/iter; left time: 3912.3328s
	iters: 200, epoch: 11 | loss: 0.3951672
	speed: 0.0369s/iter; left time: 848.5740s
Epoch: 11 cost time: 9.850196123123169
Epoch: 11, Steps: 258 | Train Loss: 0.3972906 Vali Loss: 0.9326274 Test Loss: 0.4165479
Validation loss decreased (0.933817 --> 0.932627).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3683916
	speed: 0.1804s/iter; left time: 4124.9164s
	iters: 200, epoch: 12 | loss: 0.3785320
	speed: 0.0392s/iter; left time: 892.8752s
Epoch: 12 cost time: 12.084739446640015
Epoch: 12, Steps: 258 | Train Loss: 0.3973798 Vali Loss: 0.9329585 Test Loss: 0.4165279
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3949504
	speed: 0.1548s/iter; left time: 3499.7098s
	iters: 200, epoch: 13 | loss: 0.3904267
	speed: 0.0406s/iter; left time: 913.8040s
Epoch: 13 cost time: 10.927368402481079
Epoch: 13, Steps: 258 | Train Loss: 0.3973301 Vali Loss: 0.9327163 Test Loss: 0.4166679
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4007659
	speed: 0.2115s/iter; left time: 4726.6878s
	iters: 200, epoch: 14 | loss: 0.4193838
	speed: 0.0541s/iter; left time: 1203.6944s
Epoch: 14 cost time: 13.893897294998169
Epoch: 14, Steps: 258 | Train Loss: 0.3972012 Vali Loss: 0.9333272 Test Loss: 0.4165662
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3931941
	speed: 0.1824s/iter; left time: 4029.5232s
	iters: 200, epoch: 15 | loss: 0.4085623
	speed: 0.0517s/iter; left time: 1137.0870s
Epoch: 15 cost time: 12.875762939453125
Epoch: 15, Steps: 258 | Train Loss: 0.3973355 Vali Loss: 0.9330313 Test Loss: 0.4166060
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3997578
	speed: 0.1701s/iter; left time: 3712.5760s
	iters: 200, epoch: 16 | loss: 0.4084832
	speed: 0.0624s/iter; left time: 1356.2665s
Epoch: 16 cost time: 12.602855443954468
Epoch: 16, Steps: 258 | Train Loss: 0.3971909 Vali Loss: 0.9326909 Test Loss: 0.4168607
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4297665
	speed: 0.1859s/iter; left time: 4009.4276s
	iters: 200, epoch: 17 | loss: 0.3778645
	speed: 0.0360s/iter; left time: 772.6736s
Epoch: 17 cost time: 10.620879650115967
Epoch: 17, Steps: 258 | Train Loss: 0.3971197 Vali Loss: 0.9325053 Test Loss: 0.4168235
Validation loss decreased (0.932627 --> 0.932505).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3875786
	speed: 0.2137s/iter; left time: 4554.0287s
	iters: 200, epoch: 18 | loss: 0.3879512
	speed: 0.0309s/iter; left time: 655.4208s
Epoch: 18 cost time: 10.705549716949463
Epoch: 18, Steps: 258 | Train Loss: 0.3971013 Vali Loss: 0.9319797 Test Loss: 0.4165545
Validation loss decreased (0.932505 --> 0.931980).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4363644
	speed: 0.1962s/iter; left time: 4131.0285s
	iters: 200, epoch: 19 | loss: 0.4119386
	speed: 0.0364s/iter; left time: 762.5015s
Epoch: 19 cost time: 10.963606357574463
Epoch: 19, Steps: 258 | Train Loss: 0.3971418 Vali Loss: 0.9313840 Test Loss: 0.4169949
Validation loss decreased (0.931980 --> 0.931384).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3745954
	speed: 0.1576s/iter; left time: 3278.7678s
	iters: 200, epoch: 20 | loss: 0.4412822
	speed: 0.0315s/iter; left time: 652.6691s
Epoch: 20 cost time: 8.86842393875122
Epoch: 20, Steps: 258 | Train Loss: 0.3970597 Vali Loss: 0.9319588 Test Loss: 0.4167261
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3934514
	speed: 0.1921s/iter; left time: 3946.5246s
	iters: 200, epoch: 21 | loss: 0.4145792
	speed: 0.0361s/iter; left time: 737.4293s
Epoch: 21 cost time: 10.324848175048828
Epoch: 21, Steps: 258 | Train Loss: 0.3970006 Vali Loss: 0.9324315 Test Loss: 0.4163570
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3786283
	speed: 0.1528s/iter; left time: 3099.4923s
	iters: 200, epoch: 22 | loss: 0.4126050
	speed: 0.0280s/iter; left time: 565.2830s
Epoch: 22 cost time: 9.303472757339478
Epoch: 22, Steps: 258 | Train Loss: 0.3968705 Vali Loss: 0.9316585 Test Loss: 0.4166073
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4071244
	speed: 0.1664s/iter; left time: 3331.1922s
	iters: 200, epoch: 23 | loss: 0.3793335
	speed: 0.0406s/iter; left time: 808.5656s
Epoch: 23 cost time: 13.940826177597046
Epoch: 23, Steps: 258 | Train Loss: 0.3969057 Vali Loss: 0.9318971 Test Loss: 0.4165962
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4025299
	speed: 0.1682s/iter; left time: 3325.6048s
	iters: 200, epoch: 24 | loss: 0.4171311
	speed: 0.0360s/iter; left time: 707.5507s
Epoch: 24 cost time: 11.107253551483154
Epoch: 24, Steps: 258 | Train Loss: 0.3968258 Vali Loss: 0.9315999 Test Loss: 0.4164592
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3889744
	speed: 0.1868s/iter; left time: 3644.7076s
	iters: 200, epoch: 25 | loss: 0.3953338
	speed: 0.0514s/iter; left time: 997.2356s
Epoch: 25 cost time: 12.306621551513672
Epoch: 25, Steps: 258 | Train Loss: 0.3969337 Vali Loss: 0.9307503 Test Loss: 0.4166153
Validation loss decreased (0.931384 --> 0.930750).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3769456
	speed: 0.1876s/iter; left time: 3611.8428s
	iters: 200, epoch: 26 | loss: 0.4276463
	speed: 0.0436s/iter; left time: 835.0991s
Epoch: 26 cost time: 10.493313074111938
Epoch: 26, Steps: 258 | Train Loss: 0.3969860 Vali Loss: 0.9313157 Test Loss: 0.4166451
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4028708
	speed: 0.1832s/iter; left time: 3480.0235s
	iters: 200, epoch: 27 | loss: 0.3896253
	speed: 0.0440s/iter; left time: 831.7058s
Epoch: 27 cost time: 12.486612796783447
Epoch: 27, Steps: 258 | Train Loss: 0.3968694 Vali Loss: 0.9314903 Test Loss: 0.4167831
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3966313
	speed: 0.1623s/iter; left time: 3041.3750s
	iters: 200, epoch: 28 | loss: 0.3961258
	speed: 0.0365s/iter; left time: 680.8490s
Epoch: 28 cost time: 10.48203182220459
Epoch: 28, Steps: 258 | Train Loss: 0.3969428 Vali Loss: 0.9317592 Test Loss: 0.4167593
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3890909
	speed: 0.1817s/iter; left time: 3357.2613s
	iters: 200, epoch: 29 | loss: 0.4096504
	speed: 0.0361s/iter; left time: 663.3023s
Epoch: 29 cost time: 10.409664869308472
Epoch: 29, Steps: 258 | Train Loss: 0.3968831 Vali Loss: 0.9318889 Test Loss: 0.4166326
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3864618
	speed: 0.1585s/iter; left time: 2888.0521s
	iters: 200, epoch: 30 | loss: 0.4038312
	speed: 0.0296s/iter; left time: 536.9500s
Epoch: 30 cost time: 10.6933913230896
Epoch: 30, Steps: 258 | Train Loss: 0.3969517 Vali Loss: 0.9313390 Test Loss: 0.4167258
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3704386
	speed: 0.1731s/iter; left time: 3109.9141s
	iters: 200, epoch: 31 | loss: 0.3639318
	speed: 0.0422s/iter; left time: 753.8393s
Epoch: 31 cost time: 11.44881558418274
Epoch: 31, Steps: 258 | Train Loss: 0.3969618 Vali Loss: 0.9311149 Test Loss: 0.4167839
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4115629
	speed: 0.1806s/iter; left time: 3197.2717s
	iters: 200, epoch: 32 | loss: 0.3868067
	speed: 0.0363s/iter; left time: 638.3353s
Epoch: 32 cost time: 10.779731750488281
Epoch: 32, Steps: 258 | Train Loss: 0.3968429 Vali Loss: 0.9315829 Test Loss: 0.4165289
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3759868
	speed: 0.2034s/iter; left time: 3548.9988s
	iters: 200, epoch: 33 | loss: 0.3684606
	speed: 0.0433s/iter; left time: 750.9481s
Epoch: 33 cost time: 12.03889799118042
Epoch: 33, Steps: 258 | Train Loss: 0.3968073 Vali Loss: 0.9307554 Test Loss: 0.4169438
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4081673
	speed: 0.1827s/iter; left time: 3140.8242s
	iters: 200, epoch: 34 | loss: 0.4011825
	speed: 0.0324s/iter; left time: 554.2273s
Epoch: 34 cost time: 9.724705696105957
Epoch: 34, Steps: 258 | Train Loss: 0.3968023 Vali Loss: 0.9315326 Test Loss: 0.4166855
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3912049
	speed: 0.2317s/iter; left time: 3922.9911s
	iters: 200, epoch: 35 | loss: 0.4024184
	speed: 0.0341s/iter; left time: 574.4575s
Epoch: 35 cost time: 12.01738715171814
Epoch: 35, Steps: 258 | Train Loss: 0.3968723 Vali Loss: 0.9302094 Test Loss: 0.4167104
Validation loss decreased (0.930750 --> 0.930209).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3854464
	speed: 0.1802s/iter; left time: 3004.3160s
	iters: 200, epoch: 36 | loss: 0.4151088
	speed: 0.0372s/iter; left time: 616.2801s
Epoch: 36 cost time: 11.275222539901733
Epoch: 36, Steps: 258 | Train Loss: 0.3968199 Vali Loss: 0.9314065 Test Loss: 0.4164784
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3881203
	speed: 0.1556s/iter; left time: 2554.4737s
	iters: 200, epoch: 37 | loss: 0.3941204
	speed: 0.0324s/iter; left time: 528.5008s
Epoch: 37 cost time: 9.552228212356567
Epoch: 37, Steps: 258 | Train Loss: 0.3967468 Vali Loss: 0.9301116 Test Loss: 0.4166538
Validation loss decreased (0.930209 --> 0.930112).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3867854
	speed: 0.1896s/iter; left time: 3063.3135s
	iters: 200, epoch: 38 | loss: 0.4154511
	speed: 0.0345s/iter; left time: 554.0134s
Epoch: 38 cost time: 11.0860915184021
Epoch: 38, Steps: 258 | Train Loss: 0.3969222 Vali Loss: 0.9302720 Test Loss: 0.4166584
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4307095
	speed: 0.1816s/iter; left time: 2886.3169s
	iters: 200, epoch: 39 | loss: 0.3862099
	speed: 0.0309s/iter; left time: 487.7743s
Epoch: 39 cost time: 9.62725019454956
Epoch: 39, Steps: 258 | Train Loss: 0.3967524 Vali Loss: 0.9316546 Test Loss: 0.4166382
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3672839
	speed: 0.1571s/iter; left time: 2457.4881s
	iters: 200, epoch: 40 | loss: 0.4156166
	speed: 0.0326s/iter; left time: 506.4211s
Epoch: 40 cost time: 9.241042137145996
Epoch: 40, Steps: 258 | Train Loss: 0.3968591 Vali Loss: 0.9307655 Test Loss: 0.4166023
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3760718
	speed: 0.1708s/iter; left time: 2627.3691s
	iters: 200, epoch: 41 | loss: 0.3911959
	speed: 0.0409s/iter; left time: 625.6096s
Epoch: 41 cost time: 10.156813621520996
Epoch: 41, Steps: 258 | Train Loss: 0.3968185 Vali Loss: 0.9297236 Test Loss: 0.4167031
Validation loss decreased (0.930112 --> 0.929724).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.3734455
	speed: 0.1687s/iter; left time: 2551.5934s
	iters: 200, epoch: 42 | loss: 0.3881136
	speed: 0.0400s/iter; left time: 601.0406s
Epoch: 42 cost time: 11.438143730163574
Epoch: 42, Steps: 258 | Train Loss: 0.3966988 Vali Loss: 0.9311202 Test Loss: 0.4165868
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3890790
	speed: 0.1500s/iter; left time: 2229.5005s
	iters: 200, epoch: 43 | loss: 0.3930103
	speed: 0.0404s/iter; left time: 595.9778s
Epoch: 43 cost time: 9.690796375274658
Epoch: 43, Steps: 258 | Train Loss: 0.3967110 Vali Loss: 0.9302757 Test Loss: 0.4167085
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3801069
	speed: 0.1406s/iter; left time: 2053.5468s
	iters: 200, epoch: 44 | loss: 0.3925424
	speed: 0.0335s/iter; left time: 486.5556s
Epoch: 44 cost time: 9.205264329910278
Epoch: 44, Steps: 258 | Train Loss: 0.3967370 Vali Loss: 0.9308597 Test Loss: 0.4167223
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.3817891
	speed: 0.1290s/iter; left time: 1850.9031s
	iters: 200, epoch: 45 | loss: 0.3924003
	speed: 0.0402s/iter; left time: 572.7796s
Epoch: 45 cost time: 10.173243761062622
Epoch: 45, Steps: 258 | Train Loss: 0.3967349 Vali Loss: 0.9306602 Test Loss: 0.4166610
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4133798
	speed: 0.1850s/iter; left time: 2606.5406s
	iters: 200, epoch: 46 | loss: 0.3990381
	speed: 0.0372s/iter; left time: 519.9215s
Epoch: 46 cost time: 11.946539878845215
Epoch: 46, Steps: 258 | Train Loss: 0.3968257 Vali Loss: 0.9309272 Test Loss: 0.4167257
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3737266
	speed: 0.1634s/iter; left time: 2260.5452s
	iters: 200, epoch: 47 | loss: 0.4065558
	speed: 0.0255s/iter; left time: 350.5888s
Epoch: 47 cost time: 8.912482023239136
Epoch: 47, Steps: 258 | Train Loss: 0.3967280 Vali Loss: 0.9314530 Test Loss: 0.4167648
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4055081
	speed: 0.1620s/iter; left time: 2199.4740s
	iters: 200, epoch: 48 | loss: 0.4291562
	speed: 0.0515s/iter; left time: 693.4899s
Epoch: 48 cost time: 11.391986846923828
Epoch: 48, Steps: 258 | Train Loss: 0.3967581 Vali Loss: 0.9313391 Test Loss: 0.4166296
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.3855006
	speed: 0.1548s/iter; left time: 2061.7164s
	iters: 200, epoch: 49 | loss: 0.3971777
	speed: 0.0297s/iter; left time: 392.2466s
Epoch: 49 cost time: 8.915707111358643
Epoch: 49, Steps: 258 | Train Loss: 0.3966691 Vali Loss: 0.9308763 Test Loss: 0.4167411
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.4262079
	speed: 0.1679s/iter; left time: 2192.8934s
	iters: 200, epoch: 50 | loss: 0.3708057
	speed: 0.0303s/iter; left time: 393.0340s
Epoch: 50 cost time: 9.350686073303223
Epoch: 50, Steps: 258 | Train Loss: 0.3966430 Vali Loss: 0.9310189 Test Loss: 0.4167404
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.3890825
	speed: 0.1894s/iter; left time: 2424.9797s
	iters: 200, epoch: 51 | loss: 0.3944887
	speed: 0.0500s/iter; left time: 635.0615s
Epoch: 51 cost time: 12.400347232818604
Epoch: 51, Steps: 258 | Train Loss: 0.3966687 Vali Loss: 0.9305991 Test Loss: 0.4166456
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.3828523
	speed: 0.1497s/iter; left time: 1877.9069s
	iters: 200, epoch: 52 | loss: 0.3980829
	speed: 0.0403s/iter; left time: 500.9790s
Epoch: 52 cost time: 11.143247127532959
Epoch: 52, Steps: 258 | Train Loss: 0.3966958 Vali Loss: 0.9315426 Test Loss: 0.4166262
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.3931071
	speed: 0.1936s/iter; left time: 2378.9711s
	iters: 200, epoch: 53 | loss: 0.4243706
	speed: 0.0462s/iter; left time: 562.3467s
Epoch: 53 cost time: 12.432664632797241
Epoch: 53, Steps: 258 | Train Loss: 0.3967246 Vali Loss: 0.9306504 Test Loss: 0.4167146
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4167571
	speed: 0.1700s/iter; left time: 2044.8646s
	iters: 200, epoch: 54 | loss: 0.4185459
	speed: 0.0307s/iter; left time: 366.7411s
Epoch: 54 cost time: 9.307970762252808
Epoch: 54, Steps: 258 | Train Loss: 0.3969157 Vali Loss: 0.9308435 Test Loss: 0.4167081
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.3618567
	speed: 0.1497s/iter; left time: 1762.2547s
	iters: 200, epoch: 55 | loss: 0.4123047
	speed: 0.0324s/iter; left time: 377.5009s
Epoch: 55 cost time: 9.311152219772339
Epoch: 55, Steps: 258 | Train Loss: 0.3965555 Vali Loss: 0.9311432 Test Loss: 0.4166528
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4200569
	speed: 0.1737s/iter; left time: 1999.2446s
	iters: 200, epoch: 56 | loss: 0.3830524
	speed: 0.0393s/iter; left time: 448.5232s
Epoch: 56 cost time: 12.344303607940674
Epoch: 56, Steps: 258 | Train Loss: 0.3966892 Vali Loss: 0.9311568 Test Loss: 0.4166262
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.3643493
	speed: 0.1957s/iter; left time: 2201.9481s
	iters: 200, epoch: 57 | loss: 0.3596491
	speed: 0.0521s/iter; left time: 580.9423s
Epoch: 57 cost time: 13.941642045974731
Epoch: 57, Steps: 258 | Train Loss: 0.3967483 Vali Loss: 0.9312209 Test Loss: 0.4166346
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.3848446
	speed: 0.1991s/iter; left time: 2189.3194s
	iters: 200, epoch: 58 | loss: 0.3897407
	speed: 0.0437s/iter; left time: 476.5380s
Epoch: 58 cost time: 12.08719778060913
Epoch: 58, Steps: 258 | Train Loss: 0.3966209 Vali Loss: 0.9311962 Test Loss: 0.4166201
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.3901037
	speed: 0.1715s/iter; left time: 1841.8924s
	iters: 200, epoch: 59 | loss: 0.3936892
	speed: 0.0429s/iter; left time: 456.3696s
Epoch: 59 cost time: 12.324820280075073
Epoch: 59, Steps: 258 | Train Loss: 0.3967315 Vali Loss: 0.9312141 Test Loss: 0.4166643
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.3828428
	speed: 0.1540s/iter; left time: 1614.1364s
	iters: 200, epoch: 60 | loss: 0.4217995
	speed: 0.0283s/iter; left time: 294.0975s
Epoch: 60 cost time: 10.322647333145142
Epoch: 60, Steps: 258 | Train Loss: 0.3966310 Vali Loss: 0.9302828 Test Loss: 0.4166310
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.3955786
	speed: 0.1621s/iter; left time: 1656.5802s
	iters: 200, epoch: 61 | loss: 0.4014554
	speed: 0.0346s/iter; left time: 350.2298s
Epoch: 61 cost time: 10.590888977050781
Epoch: 61, Steps: 258 | Train Loss: 0.3966542 Vali Loss: 0.9315920 Test Loss: 0.4166450
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4157015085220337, mae:0.41179800033569336, rse:0.6134251356124878, corr:[0.52649784 0.5318635  0.5352234  0.5363742  0.5364931  0.53684884
 0.5376911  0.53880054 0.53992057 0.54076385 0.5413279  0.54158103
 0.54168904 0.5416076  0.5412002  0.5404091  0.53937095 0.5382374
 0.5370456  0.5358505  0.5346808  0.5334546  0.5321023  0.5307843
 0.5293986  0.5281881  0.527295   0.52671325 0.5265682  0.52682155
 0.527346   0.52806985 0.5287572  0.5293842  0.5297722  0.5300452
 0.53002584 0.5297886  0.52940786 0.52888906 0.5284689  0.5282003
 0.5281202  0.5282177  0.52826786 0.52814215 0.527951   0.52777535
 0.52764857 0.5275765  0.52765095 0.5277655  0.52784336 0.5276621
 0.52725524 0.5267401  0.5262961  0.52599895 0.525927   0.52590734
 0.52587265 0.52579826 0.52568513 0.5254978  0.52540773 0.5255041
 0.5257223  0.5259613  0.52617645 0.5262868  0.5263462  0.5263497
 0.52630424 0.52621907 0.52614576 0.5260199  0.5257917  0.5255
 0.525212   0.52500147 0.524903   0.5249004  0.52495325 0.5249385
 0.52486324 0.52467704 0.52444905 0.5242304  0.52412903 0.5242828
 0.52467996 0.525207   0.5257349  0.5261163  0.52629185 0.52620107
 0.52592903 0.52562696 0.5252393  0.524909   0.5246786  0.5245654
 0.5245599  0.52455515 0.5245291  0.5245108  0.52438146 0.524184
 0.5239033  0.52362883 0.5233959  0.523185   0.52298135 0.52277225
 0.52252066 0.5222318  0.52192223 0.5216004  0.52130246 0.52110255
 0.5209613  0.5208035  0.520529   0.52021444 0.5199219  0.5196577
 0.5194568  0.5193936  0.5194214  0.5195018  0.51949865 0.5193625
 0.51916665 0.5189679  0.5187581  0.5186135  0.5186143  0.5186396
 0.5187009  0.5188205  0.518924   0.51904076 0.51920354 0.5194172
 0.5196349  0.5197394  0.51974076 0.5196529  0.5195545  0.5194475
 0.51933724 0.519312   0.51934093 0.51937455 0.5193574  0.51936096
 0.51936054 0.5193442  0.51937634 0.51940876 0.5195228  0.5197018
 0.51991    0.52008384 0.52019876 0.52025723 0.52031404 0.52042985
 0.5205743  0.5206896  0.5208193  0.520951   0.5209978  0.5209485
 0.5208423  0.5207284  0.5206617  0.5206546  0.5206687  0.52069485
 0.52070016 0.5206955  0.52065367 0.52067083 0.5207909  0.52101254
 0.5213029  0.52164197 0.5219696  0.52221006 0.52229744 0.5222232
 0.52200115 0.52172136 0.52132976 0.52079856 0.5201584  0.5194716
 0.5187359  0.5179756  0.51717895 0.51640195 0.51565623 0.5149781
 0.5143864  0.51387405 0.51340604 0.51297545 0.5125289  0.512013
 0.5113814  0.5106815  0.50996745 0.50929105 0.50863165 0.5080748
 0.5076699  0.5073484  0.5071054  0.5068881  0.5067058  0.5066265
 0.5067356  0.50690293 0.50708705 0.5072479  0.50731057 0.5072946
 0.50725776 0.5072555  0.50730526 0.5073451  0.50737566 0.50740784
 0.50745016 0.50755006 0.5076184  0.5077655  0.5079318  0.5081377
 0.50831264 0.50833535 0.50823784 0.5080979  0.5079936  0.5079056
 0.5078308  0.5077688  0.5077002  0.5076517  0.50758696 0.50757813
 0.5075592  0.50759095 0.5076202  0.5076218  0.50760144 0.50758135
 0.507578   0.5076157  0.50765926 0.50768495 0.50767875 0.5077302
 0.5077696  0.50780785 0.50784487 0.5079111  0.5079773  0.50798166
 0.5079452  0.5078634  0.5077768  0.50771433 0.50768286 0.5076975
 0.50771004 0.5077824  0.50783724 0.5079269  0.50801396 0.5080735
 0.50815773 0.5082142  0.50821227 0.5081482  0.50796986 0.50763345
 0.50716347 0.50666076 0.5061573  0.5056213  0.50507104 0.50454193
 0.50406563 0.50359803 0.503176   0.502738   0.5022369  0.50170106
 0.50113493 0.50061285 0.5002143  0.49994117 0.49975127 0.49956357
 0.4994232  0.49924567 0.49902698 0.49879414 0.4986356  0.4985379
 0.49848688 0.49849054 0.49846324 0.4983874  0.498274   0.49819306
 0.49816194 0.49814695 0.49814388 0.4981408  0.49810797 0.49802393
 0.49788442 0.49773836 0.4975695  0.49741492 0.4972603  0.49711615
 0.49707088 0.49710903 0.49718136 0.49728367 0.49737173 0.49749687
 0.49757895 0.49747974 0.49730656 0.49711832 0.49699008 0.49694383
 0.49693292 0.4969754  0.49702427 0.4970705  0.49710482 0.49716118
 0.49716672 0.49716145 0.49713597 0.49705985 0.4969723  0.49687782
 0.49677187 0.49670008 0.49666834 0.4966763  0.49675137 0.49687275
 0.4969324  0.49694845 0.49688518 0.49675792 0.49660814 0.4964715
 0.49631745 0.49623224 0.4962288  0.49624056 0.49626902 0.49629614
 0.49631786 0.4963732  0.4964748  0.4966653  0.49692005 0.4972375
 0.49759158 0.49793464 0.49822688 0.49840435 0.49844503 0.49838534
 0.49822745 0.4979789  0.49767503 0.49728262 0.4967663  0.4962699
 0.49582037 0.49545187 0.49515522 0.49492985 0.49467307 0.49435487
 0.49395484 0.49350193 0.49305338 0.49265036 0.4922711  0.49197927
 0.49172124 0.49150306 0.491217   0.4908896  0.49054193 0.49025902
 0.49006367 0.48998526 0.49000925 0.4901238  0.49030218 0.49044886
 0.490642   0.49074548 0.49080688 0.49075037 0.49064657 0.49058977
 0.49055162 0.4905927  0.49064735 0.49069414 0.4906734  0.49064836
 0.49061042 0.4906054  0.49061123 0.49071988 0.49086398 0.49109992
 0.49128795 0.4913092  0.4912044  0.49105862 0.4908993  0.490758
 0.4906333  0.49053317 0.4904615  0.4904074  0.49037415 0.4903667
 0.49033496 0.490301   0.49023426 0.49014506 0.49005774 0.48997232
 0.48996603 0.49003547 0.490163   0.49029344 0.4903904  0.49049234
 0.4904986  0.49045762 0.49039087 0.49036986 0.49034    0.49034435
 0.4903475  0.49037746 0.49037376 0.49036053 0.4903396  0.4903073
 0.49029124 0.49026427 0.4902608  0.49028784 0.4903532  0.4904418
 0.49050474 0.49055743 0.49057505 0.49048412 0.4902471  0.4898844
 0.48942694 0.48893023 0.4884299  0.48789364 0.48729557 0.48668236
 0.48611948 0.48553535 0.48495403 0.48440245 0.48381907 0.48319274
 0.48254862 0.48196235 0.4814402  0.48093444 0.4804326  0.4799705
 0.4795428  0.47920462 0.4789553  0.47866157 0.47842973 0.4782204
 0.47807333 0.4779721  0.47788185 0.47787043 0.47794098 0.47813377
 0.47842965 0.47870854 0.47889826 0.47905922 0.47916317 0.47922215
 0.47923803 0.4793169  0.4794951  0.47967955 0.47983402 0.47998413
 0.48013484 0.48029622 0.48046193 0.4805824  0.48072132 0.48091784
 0.4810944  0.48117092 0.48110935 0.48102337 0.48098627 0.48099303
 0.48103148 0.48106456 0.48107    0.48104253 0.48096824 0.48084703
 0.48069084 0.4805062  0.48032612 0.4801616  0.48002982 0.47999144
 0.48002952 0.4801302  0.48025355 0.48031223 0.48028898 0.48028308
 0.48025188 0.48024166 0.4802839  0.48038897 0.4804855  0.48055035
 0.48054478 0.48049682 0.48044676 0.48044237 0.4804929  0.48056617
 0.48060662 0.48069113 0.48072788 0.4807633  0.48081726 0.48091397
 0.48104468 0.48116434 0.48126167 0.4812646  0.48111615 0.4807733
 0.48028773 0.47976542 0.47923422 0.47866604 0.47805297 0.4775014
 0.4770103  0.47660005 0.47621927 0.47580954 0.4753411  0.47475678
 0.47412267 0.47347942 0.47288418 0.47235438 0.47193533 0.4715848
 0.47125795 0.4709027  0.4705575  0.47025755 0.47003436 0.4699076
 0.46990672 0.46995544 0.47002918 0.47009376 0.4701556  0.4701806
 0.4702665  0.47038278 0.4705277  0.4706834  0.47081652 0.47095358
 0.47108638 0.47115412 0.47120944 0.47124213 0.47127402 0.47130927
 0.4714072  0.47160593 0.4718063  0.47199854 0.47226122 0.4725474
 0.47271842 0.4727788  0.472738   0.47263792 0.47262606 0.4727226
 0.4728022  0.47285777 0.47284758 0.47270456 0.4725052  0.47225687
 0.47196755 0.47172436 0.47158164 0.4714401  0.47132602 0.47117493
 0.4709823  0.4707679  0.47059292 0.4705032  0.4704883  0.47057322
 0.47068885 0.47072718 0.4707281  0.47070956 0.47067118 0.470616
 0.47056434 0.47053874 0.47055864 0.47061062 0.47064844 0.47062308
 0.4705372  0.47051328 0.4705924  0.47073507 0.47098795 0.47127873
 0.4715391  0.47170863 0.471769   0.47174948 0.4716101  0.4713512
 0.4710214  0.4707354  0.4704793  0.47015402 0.46970886 0.46917543
 0.46863833 0.46812725 0.46775776 0.46748805 0.46714297 0.46682099
 0.46636787 0.46590936 0.46545863 0.46516412 0.46502566 0.46496257
 0.464914   0.46477607 0.4645744  0.46426994 0.46401986 0.46394458
 0.46401042 0.46421304 0.46438494 0.46441564 0.46434802 0.46415955
 0.4640561  0.46411014 0.46439433 0.46493754 0.4654968  0.4659804
 0.4662868  0.46644866 0.46655023 0.46667713 0.46700507 0.4675249
 0.46827292 0.4690737  0.46955475 0.4696703  0.46895322 0.466108  ]
