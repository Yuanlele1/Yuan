Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=22, out_features=110, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2168320.0
params:  2530.0
Trainable parameters:  2530
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7844242
	speed: 0.0249s/iter; left time: 648.6302s
	iters: 200, epoch: 1 | loss: 0.5493096
	speed: 0.0185s/iter; left time: 481.4757s
Epoch: 1 cost time: 5.468258380889893
Epoch: 1, Steps: 262 | Train Loss: 0.7443408 Vali Loss: 1.1853905 Test Loss: 0.6086597
Validation loss decreased (inf --> 1.185390).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4581635
	speed: 0.0767s/iter; left time: 1980.9716s
	iters: 200, epoch: 2 | loss: 0.4658144
	speed: 0.0170s/iter; left time: 437.6094s
Epoch: 2 cost time: 4.768773317337036
Epoch: 2, Steps: 262 | Train Loss: 0.4986716 Vali Loss: 1.0448016 Test Loss: 0.4868897
Validation loss decreased (1.185390 --> 1.044802).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4728623
	speed: 0.0792s/iter; left time: 2025.5851s
	iters: 200, epoch: 3 | loss: 0.4508368
	speed: 0.0183s/iter; left time: 465.0579s
Epoch: 3 cost time: 5.982051134109497
Epoch: 3, Steps: 262 | Train Loss: 0.4609787 Vali Loss: 1.0075405 Test Loss: 0.4588830
Validation loss decreased (1.044802 --> 1.007540).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4502695
	speed: 0.0937s/iter; left time: 2370.9678s
	iters: 200, epoch: 4 | loss: 0.4621572
	speed: 0.0178s/iter; left time: 449.9111s
Epoch: 4 cost time: 5.514158010482788
Epoch: 4, Steps: 262 | Train Loss: 0.4499821 Vali Loss: 0.9923949 Test Loss: 0.4495375
Validation loss decreased (1.007540 --> 0.992395).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4666523
	speed: 0.0821s/iter; left time: 2056.8710s
	iters: 200, epoch: 5 | loss: 0.4438740
	speed: 0.0170s/iter; left time: 424.2412s
Epoch: 5 cost time: 6.217241048812866
Epoch: 5, Steps: 262 | Train Loss: 0.4459232 Vali Loss: 0.9870387 Test Loss: 0.4462796
Validation loss decreased (0.992395 --> 0.987039).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4598393
	speed: 0.0919s/iter; left time: 2279.0228s
	iters: 200, epoch: 6 | loss: 0.4470760
	speed: 0.0186s/iter; left time: 459.6997s
Epoch: 6 cost time: 6.0454816818237305
Epoch: 6, Steps: 262 | Train Loss: 0.4443608 Vali Loss: 0.9845654 Test Loss: 0.4454707
Validation loss decreased (0.987039 --> 0.984565).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4325385
	speed: 0.0913s/iter; left time: 2238.9396s
	iters: 200, epoch: 7 | loss: 0.4851261
	speed: 0.0165s/iter; left time: 402.1522s
Epoch: 7 cost time: 5.117990255355835
Epoch: 7, Steps: 262 | Train Loss: 0.4438388 Vali Loss: 0.9823228 Test Loss: 0.4452062
Validation loss decreased (0.984565 --> 0.982323).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4645359
	speed: 0.0765s/iter; left time: 1855.9043s
	iters: 200, epoch: 8 | loss: 0.4528144
	speed: 0.0167s/iter; left time: 404.2974s
Epoch: 8 cost time: 4.869121313095093
Epoch: 8, Steps: 262 | Train Loss: 0.4436301 Vali Loss: 0.9815748 Test Loss: 0.4451289
Validation loss decreased (0.982323 --> 0.981575).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4516748
	speed: 0.0824s/iter; left time: 1977.4938s
	iters: 200, epoch: 9 | loss: 0.4437567
	speed: 0.0162s/iter; left time: 386.6710s
Epoch: 9 cost time: 4.9727888107299805
Epoch: 9, Steps: 262 | Train Loss: 0.4434068 Vali Loss: 0.9816035 Test Loss: 0.4451635
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4607589
	speed: 0.0796s/iter; left time: 1889.9295s
	iters: 200, epoch: 10 | loss: 0.4747403
	speed: 0.0173s/iter; left time: 408.7931s
Epoch: 10 cost time: 5.0793561935424805
Epoch: 10, Steps: 262 | Train Loss: 0.4430797 Vali Loss: 0.9813546 Test Loss: 0.4452461
Validation loss decreased (0.981575 --> 0.981355).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4661114
	speed: 0.0840s/iter; left time: 1971.4734s
	iters: 200, epoch: 11 | loss: 0.4558992
	speed: 0.0157s/iter; left time: 366.1568s
Epoch: 11 cost time: 4.7406229972839355
Epoch: 11, Steps: 262 | Train Loss: 0.4431551 Vali Loss: 0.9797090 Test Loss: 0.4450250
Validation loss decreased (0.981355 --> 0.979709).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4272020
	speed: 0.0856s/iter; left time: 1987.0343s
	iters: 200, epoch: 12 | loss: 0.4311718
	speed: 0.0203s/iter; left time: 469.6649s
Epoch: 12 cost time: 5.966798305511475
Epoch: 12, Steps: 262 | Train Loss: 0.4433043 Vali Loss: 0.9820977 Test Loss: 0.4453455
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4351906
	speed: 0.0883s/iter; left time: 2027.4096s
	iters: 200, epoch: 13 | loss: 0.4517792
	speed: 0.0165s/iter; left time: 376.3755s
Epoch: 13 cost time: 5.084747791290283
Epoch: 13, Steps: 262 | Train Loss: 0.4432380 Vali Loss: 0.9814873 Test Loss: 0.4453272
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4096690
	speed: 0.0742s/iter; left time: 1684.7488s
	iters: 200, epoch: 14 | loss: 0.4448461
	speed: 0.0164s/iter; left time: 369.9541s
Epoch: 14 cost time: 4.82419753074646
Epoch: 14, Steps: 262 | Train Loss: 0.4432862 Vali Loss: 0.9813951 Test Loss: 0.4451169
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4092955
	speed: 0.0798s/iter; left time: 1789.4721s
	iters: 200, epoch: 15 | loss: 0.4165106
	speed: 0.0167s/iter; left time: 373.8221s
Epoch: 15 cost time: 5.08062219619751
Epoch: 15, Steps: 262 | Train Loss: 0.4430811 Vali Loss: 0.9810165 Test Loss: 0.4452652
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4624540
	speed: 0.0810s/iter; left time: 1796.2236s
	iters: 200, epoch: 16 | loss: 0.4348474
	speed: 0.0195s/iter; left time: 430.5338s
Epoch: 16 cost time: 5.588103771209717
Epoch: 16, Steps: 262 | Train Loss: 0.4430654 Vali Loss: 0.9804578 Test Loss: 0.4454117
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4252724
	speed: 0.0856s/iter; left time: 1876.3449s
	iters: 200, epoch: 17 | loss: 0.4235348
	speed: 0.0167s/iter; left time: 364.2447s
Epoch: 17 cost time: 4.902801036834717
Epoch: 17, Steps: 262 | Train Loss: 0.4428170 Vali Loss: 0.9797067 Test Loss: 0.4451447
Validation loss decreased (0.979709 --> 0.979707).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4363998
	speed: 0.0772s/iter; left time: 1670.9718s
	iters: 200, epoch: 18 | loss: 0.4504704
	speed: 0.0169s/iter; left time: 364.5225s
Epoch: 18 cost time: 5.009488344192505
Epoch: 18, Steps: 262 | Train Loss: 0.4430960 Vali Loss: 0.9807830 Test Loss: 0.4453273
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4740696
	speed: 0.0808s/iter; left time: 1727.7735s
	iters: 200, epoch: 19 | loss: 0.4608648
	speed: 0.0185s/iter; left time: 393.4094s
Epoch: 19 cost time: 5.449214696884155
Epoch: 19, Steps: 262 | Train Loss: 0.4431493 Vali Loss: 0.9810901 Test Loss: 0.4453784
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4725707
	speed: 0.0776s/iter; left time: 1638.9254s
	iters: 200, epoch: 20 | loss: 0.4752032
	speed: 0.0159s/iter; left time: 335.0409s
Epoch: 20 cost time: 4.83228325843811
Epoch: 20, Steps: 262 | Train Loss: 0.4432182 Vali Loss: 0.9809457 Test Loss: 0.4452878
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4729936
	speed: 0.0802s/iter; left time: 1673.7509s
	iters: 200, epoch: 21 | loss: 0.4290751
	speed: 0.0169s/iter; left time: 351.6739s
Epoch: 21 cost time: 5.033428430557251
Epoch: 21, Steps: 262 | Train Loss: 0.4430314 Vali Loss: 0.9802371 Test Loss: 0.4453008
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4164787
	speed: 0.0813s/iter; left time: 1674.9851s
	iters: 200, epoch: 22 | loss: 0.4257482
	speed: 0.0173s/iter; left time: 354.2782s
Epoch: 22 cost time: 4.942286729812622
Epoch: 22, Steps: 262 | Train Loss: 0.4431516 Vali Loss: 0.9809961 Test Loss: 0.4453214
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3988994
	speed: 0.0845s/iter; left time: 1717.9540s
	iters: 200, epoch: 23 | loss: 0.4830090
	speed: 0.0174s/iter; left time: 352.7809s
Epoch: 23 cost time: 4.999811172485352
Epoch: 23, Steps: 262 | Train Loss: 0.4430759 Vali Loss: 0.9811054 Test Loss: 0.4453170
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4181437
	speed: 0.0913s/iter; left time: 1832.2916s
	iters: 200, epoch: 24 | loss: 0.4551962
	speed: 0.0177s/iter; left time: 354.1116s
Epoch: 24 cost time: 5.870790243148804
Epoch: 24, Steps: 262 | Train Loss: 0.4430081 Vali Loss: 0.9815809 Test Loss: 0.4453725
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4471103
	speed: 0.0936s/iter; left time: 1853.5611s
	iters: 200, epoch: 25 | loss: 0.4529286
	speed: 0.0158s/iter; left time: 311.4751s
Epoch: 25 cost time: 4.971611738204956
Epoch: 25, Steps: 262 | Train Loss: 0.4429566 Vali Loss: 0.9808559 Test Loss: 0.4454453
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4506133
	speed: 0.0761s/iter; left time: 1487.3357s
	iters: 200, epoch: 26 | loss: 0.4873813
	speed: 0.0157s/iter; left time: 306.2196s
Epoch: 26 cost time: 4.868319988250732
Epoch: 26, Steps: 262 | Train Loss: 0.4429129 Vali Loss: 0.9809242 Test Loss: 0.4455054
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4479138
	speed: 0.0769s/iter; left time: 1484.1201s
	iters: 200, epoch: 27 | loss: 0.4385528
	speed: 0.0165s/iter; left time: 317.2484s
Epoch: 27 cost time: 4.921785593032837
Epoch: 27, Steps: 262 | Train Loss: 0.4429699 Vali Loss: 0.9811627 Test Loss: 0.4454021
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4051061
	speed: 0.0808s/iter; left time: 1537.3791s
	iters: 200, epoch: 28 | loss: 0.4698697
	speed: 0.0163s/iter; left time: 308.6546s
Epoch: 28 cost time: 5.111720085144043
Epoch: 28, Steps: 262 | Train Loss: 0.4429889 Vali Loss: 0.9801982 Test Loss: 0.4453525
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4716682
	speed: 0.0834s/iter; left time: 1565.7614s
	iters: 200, epoch: 29 | loss: 0.4396861
	speed: 0.0161s/iter; left time: 300.8264s
Epoch: 29 cost time: 4.965299844741821
Epoch: 29, Steps: 262 | Train Loss: 0.4427669 Vali Loss: 0.9814180 Test Loss: 0.4453852
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4913304
	speed: 0.0811s/iter; left time: 1500.2000s
	iters: 200, epoch: 30 | loss: 0.4527493
	speed: 0.0187s/iter; left time: 343.3778s
Epoch: 30 cost time: 5.430739641189575
Epoch: 30, Steps: 262 | Train Loss: 0.4430868 Vali Loss: 0.9807255 Test Loss: 0.4453821
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4873576
	speed: 0.0838s/iter; left time: 1528.6419s
	iters: 200, epoch: 31 | loss: 0.4400202
	speed: 0.0173s/iter; left time: 314.2246s
Epoch: 31 cost time: 5.024909496307373
Epoch: 31, Steps: 262 | Train Loss: 0.4429215 Vali Loss: 0.9809643 Test Loss: 0.4453892
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3995146
	speed: 0.0847s/iter; left time: 1523.4979s
	iters: 200, epoch: 32 | loss: 0.4365059
	speed: 0.0172s/iter; left time: 306.6739s
Epoch: 32 cost time: 5.012264251708984
Epoch: 32, Steps: 262 | Train Loss: 0.4430502 Vali Loss: 0.9808256 Test Loss: 0.4453993
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4484153
	speed: 0.0751s/iter; left time: 1329.7145s
	iters: 200, epoch: 33 | loss: 0.4392754
	speed: 0.0169s/iter; left time: 297.3611s
Epoch: 33 cost time: 4.834463596343994
Epoch: 33, Steps: 262 | Train Loss: 0.4430151 Vali Loss: 0.9811011 Test Loss: 0.4454094
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4693452
	speed: 0.0775s/iter; left time: 1352.6651s
	iters: 200, epoch: 34 | loss: 0.4168975
	speed: 0.0171s/iter; left time: 297.0851s
Epoch: 34 cost time: 4.997989654541016
Epoch: 34, Steps: 262 | Train Loss: 0.4428594 Vali Loss: 0.9806806 Test Loss: 0.4454511
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4456458
	speed: 0.0862s/iter; left time: 1481.4186s
	iters: 200, epoch: 35 | loss: 0.4218310
	speed: 0.0174s/iter; left time: 298.0737s
Epoch: 35 cost time: 5.485201835632324
Epoch: 35, Steps: 262 | Train Loss: 0.4428634 Vali Loss: 0.9809723 Test Loss: 0.4454493
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.5029032
	speed: 0.0788s/iter; left time: 1333.8898s
	iters: 200, epoch: 36 | loss: 0.4776047
	speed: 0.0172s/iter; left time: 289.8109s
Epoch: 36 cost time: 5.055165529251099
Epoch: 36, Steps: 262 | Train Loss: 0.4429774 Vali Loss: 0.9800662 Test Loss: 0.4454877
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4571290
	speed: 0.0803s/iter; left time: 1338.3878s
	iters: 200, epoch: 37 | loss: 0.4323539
	speed: 0.0170s/iter; left time: 281.0372s
Epoch: 37 cost time: 5.09287428855896
Epoch: 37, Steps: 262 | Train Loss: 0.4428317 Vali Loss: 0.9805604 Test Loss: 0.4454992
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.44335827231407166, mae:0.4245544672012329, rse:0.6335022449493408, corr:[0.5319729  0.53363293 0.5322611  0.5294193  0.52700925 0.52610654
 0.5261906  0.52657795 0.526628   0.526554   0.52668184 0.5266928
 0.52646196 0.5253838  0.52339363 0.52101994 0.51876926 0.51689655
 0.51534826 0.5139274  0.5124655  0.5107454  0.50866914 0.5063143
 0.50378513 0.50128853 0.49915063 0.4975516  0.49664703 0.49640405
 0.49677187 0.49737144 0.49773356 0.49770385 0.49731117 0.49683434
 0.4964381  0.4961909  0.49613512 0.49604148 0.49586052 0.4954843
 0.4950171  0.49469948 0.4946552  0.49490649 0.49536654 0.49582693
 0.49609324 0.49602747 0.49573553 0.49543694 0.49533117 0.49544463
 0.49571064 0.4959659  0.49617335 0.4961417  0.49588022 0.4954206
 0.49497905 0.49471447 0.49465343 0.49477518 0.49499717 0.49531394
 0.4956018  0.49575987 0.49589866 0.49605212 0.4962864  0.49657163
 0.49686423 0.49709308 0.49728966 0.49732563 0.49725428 0.49709773
 0.49690196 0.4967156  0.4966175  0.4965957  0.49662223 0.496545
 0.4963665  0.4961503  0.49596167 0.49574423 0.49548346 0.49532646
 0.49527228 0.4952698  0.49522486 0.4950687  0.49475184 0.4941266
 0.49325538 0.49232927 0.49147373 0.49084207 0.4905508  0.4906437
 0.491014   0.4915414  0.49220154 0.49302408 0.4940043  0.49507698
 0.49606863 0.49675283 0.49700347 0.49701598 0.49681276 0.49650913
 0.49615002 0.4958733  0.49562263 0.4953411  0.49503043 0.49463865
 0.4941898  0.49355346 0.4927207  0.4918835  0.49113756 0.49056408
 0.490252   0.4901877  0.49022213 0.49020746 0.49002022 0.48962545
 0.48911902 0.48874545 0.48855594 0.48845    0.48845428 0.48841044
 0.48822832 0.48799628 0.48777533 0.4876685  0.4877363  0.48797002
 0.4882363  0.488417   0.48833057 0.488067   0.48787975 0.48770213
 0.48763856 0.4877621  0.4879824  0.48823446 0.4883178  0.48827422
 0.48811984 0.48798016 0.48786196 0.48787773 0.4880331  0.48831388
 0.48867092 0.48904085 0.48933047 0.48962185 0.48990163 0.49017298
 0.4904708  0.49071184 0.49079785 0.49081203 0.49068213 0.49038914
 0.4900335  0.4896662  0.48939323 0.48926857 0.48919505 0.48916525
 0.48912874 0.48905718 0.48892665 0.48878294 0.48867255 0.4886211
 0.4886297  0.48870093 0.4888389  0.48905393 0.4892648  0.48940086
 0.4893873  0.48929363 0.48908198 0.48856434 0.48788047 0.48729753
 0.48686948 0.4865272  0.48620695 0.48613515 0.4860873  0.48593798
 0.48567355 0.48512575 0.4844662  0.48388508 0.48335665 0.48286796
 0.4823521  0.4817959  0.48112845 0.48028612 0.479312   0.4783445
 0.47748163 0.47671115 0.47605738 0.47555467 0.47515264 0.47485706
 0.4746853  0.47465503 0.47464904 0.47463116 0.47459093 0.47442934
 0.47410437 0.4737639  0.47340629 0.4730427  0.4727481  0.47257146
 0.47252727 0.47261527 0.47267374 0.47276837 0.47289062 0.4729729
 0.47311237 0.47319242 0.47327247 0.47330782 0.47329578 0.47325692
 0.4732334  0.4731853  0.47319317 0.47328827 0.47342527 0.47355986
 0.47366777 0.47380012 0.4738326  0.47379646 0.4736715  0.47352663
 0.4735491  0.47379276 0.47419542 0.47467673 0.47510478 0.47545666
 0.4757497  0.4759096  0.47600424 0.4761266  0.47631952 0.4764826
 0.476632   0.4766931  0.4767394  0.47676265 0.47682333 0.47691625
 0.4770493  0.47729793 0.47755656 0.47774497 0.47783893 0.47780207
 0.47771212 0.47752193 0.4773063  0.4770763  0.47676402 0.476348
 0.4757059  0.47496086 0.47425038 0.47353503 0.4729297  0.4725131
 0.4724597  0.47266138 0.47312555 0.47374144 0.47430086 0.47473598
 0.47496662 0.4749134  0.47479102 0.4747406  0.47478288 0.47483334
 0.47486505 0.47477835 0.47449562 0.47406882 0.47358152 0.47313452
 0.47277385 0.4724468  0.47219732 0.4720853  0.47190407 0.47171178
 0.47152868 0.47128692 0.47103366 0.47090557 0.47080252 0.47069433
 0.47050288 0.4701938  0.4697304  0.46918663 0.4686236  0.4681748
 0.46793246 0.46793082 0.468025   0.468112   0.46816477 0.46814707
 0.4680906  0.4680234  0.46795464 0.4679114  0.46797687 0.468031
 0.46801943 0.46793926 0.46786872 0.46782672 0.46777228 0.46776378
 0.46778128 0.46789786 0.46810657 0.46827647 0.46837473 0.46840096
 0.46840486 0.46852213 0.46871316 0.46900836 0.4693504  0.46969298
 0.46991098 0.47003058 0.46999604 0.46987405 0.4697227  0.469575
 0.46945503 0.46945536 0.46958995 0.46978247 0.4700413  0.47030646
 0.4706003  0.47092772 0.47129366 0.47161165 0.47183368 0.47196674
 0.4720183  0.47201854 0.4720627  0.47209704 0.47208977 0.4719879
 0.4717206  0.47121885 0.47073036 0.47019768 0.46965083 0.46935394
 0.46938396 0.46974993 0.4702582  0.4709817  0.47173467 0.47240344
 0.47279802 0.47285795 0.47263122 0.47242472 0.4722675  0.47220945
 0.472226   0.4721943  0.4720221  0.47168994 0.47119117 0.47062418
 0.47010437 0.46964428 0.46929786 0.469091   0.46898928 0.46892393
 0.4689207  0.46887052 0.46880266 0.46868736 0.46848318 0.4682902
 0.4680154  0.46773058 0.4673965  0.4670093  0.46655568 0.46612456
 0.46571434 0.46546283 0.46537647 0.4654809  0.46568123 0.46597618
 0.46617883 0.46626577 0.4662316  0.466156   0.46602044 0.4659705
 0.46593463 0.4659216  0.46596244 0.46602175 0.46603784 0.4660086
 0.4659484  0.4659244  0.46592104 0.466018   0.46615902 0.4662992
 0.46648028 0.4666999  0.46688178 0.46701434 0.46704188 0.46712688
 0.46721452 0.46725193 0.46722454 0.46718577 0.4670667  0.46690226
 0.4667618  0.46669528 0.46670008 0.4668296  0.46704593 0.46726412
 0.46746308 0.46763346 0.46778646 0.46782443 0.46778297 0.46768096
 0.467474   0.46719062 0.46682897 0.46634647 0.46569765 0.46484792
 0.46378687 0.46262607 0.4615175  0.46056047 0.45976505 0.45921734
 0.45891437 0.45861262 0.45846435 0.45838717 0.4583081  0.45815945
 0.4580379  0.45779023 0.4573655  0.45689097 0.4564757  0.4561339
 0.45587212 0.45566797 0.45544088 0.45511633 0.4547317  0.45427936
 0.4538468  0.45345178 0.45311087 0.45290247 0.4527553  0.4526832
 0.45264557 0.45262295 0.45257667 0.4525591  0.45251343 0.45239767
 0.45218554 0.45197713 0.4517877  0.45154914 0.45127955 0.45105052
 0.45082024 0.45061362 0.45042604 0.45025447 0.45018184 0.45028177
 0.45043018 0.45054573 0.45052493 0.45037752 0.45020962 0.4500776
 0.45002642 0.45003563 0.450179   0.450339   0.45043263 0.45041156
 0.4503116  0.45009035 0.44989368 0.44977108 0.4497768  0.44994375
 0.4502396  0.4506003  0.45093167 0.45113158 0.4511699  0.45117128
 0.4511471  0.45109797 0.45107543 0.45116264 0.45128313 0.45138252
 0.4514609  0.4515244  0.45153782 0.4515562  0.4515903  0.45168903
 0.4517945  0.45199686 0.4521647  0.45225176 0.452232   0.4521267
 0.45193636 0.45161662 0.45120037 0.45064062 0.44985723 0.44882926
 0.44754294 0.4461764  0.44494173 0.44380027 0.4428786  0.44230068
 0.44207317 0.4421352  0.44243494 0.44282767 0.44327867 0.44367135
 0.44396517 0.44406146 0.4439999  0.44392094 0.4438469  0.44375408
 0.44358206 0.4433478  0.44308162 0.44276294 0.4424171  0.44200373
 0.44161844 0.44123995 0.44087276 0.44064796 0.44052967 0.44049108
 0.4405202  0.44062525 0.44072595 0.44077852 0.44077793 0.44074282
 0.44057807 0.44026518 0.43994915 0.4396511  0.43943462 0.4392725
 0.43921623 0.4392215  0.43922085 0.43918216 0.43916532 0.43922555
 0.4393352  0.4394713  0.43957153 0.43954885 0.43942207 0.43921506
 0.4389397  0.4386953  0.4385607  0.4385611  0.43870765 0.43880084
 0.43873608 0.4385769  0.43831047 0.43801013 0.43785504 0.43782848
 0.4380059  0.43839347 0.43889976 0.43940794 0.43979144 0.44004413
 0.44016102 0.44011554 0.4400372  0.44004387 0.44011438 0.4402428
 0.44038367 0.44053158 0.44064796 0.4407322  0.44083285 0.44099066
 0.4412213  0.44156212 0.44194567 0.44217676 0.44220692 0.4420102
 0.4416352  0.44120228 0.4407785  0.4404145  0.44004425 0.43951052
 0.43871197 0.43776298 0.43676096 0.43576962 0.43498844 0.43451506
 0.43442053 0.43476826 0.4353754  0.43600738 0.4364355  0.43673664
 0.43688986 0.43673337 0.43650848 0.43638644 0.43637076 0.43633312
 0.43613365 0.43565905 0.4349662  0.43420717 0.4335599  0.4332562
 0.43325153 0.43328667 0.43320808 0.43289843 0.43235016 0.43168974
 0.43115246 0.43084025 0.4308369  0.43107796 0.43127257 0.43124774
 0.43086436 0.43014467 0.42945397 0.42910305 0.42928782 0.42990986
 0.43066305 0.43118674 0.43127313 0.43127653 0.43164995 0.4322693 ]
