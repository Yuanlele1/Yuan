Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=30, out_features=150, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4032000.0
params:  4650.0
Trainable parameters:  4650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7744572
	speed: 0.0234s/iter; left time: 609.7278s
	iters: 200, epoch: 1 | loss: 0.5578415
	speed: 0.0173s/iter; left time: 449.3924s
Epoch: 1 cost time: 5.0996410846710205
Epoch: 1, Steps: 262 | Train Loss: 0.7276101 Vali Loss: 1.1654837 Test Loss: 0.5890157
Validation loss decreased (inf --> 1.165484).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4527979
	speed: 0.0760s/iter; left time: 1963.4026s
	iters: 200, epoch: 2 | loss: 0.4863860
	speed: 0.0166s/iter; left time: 428.2263s
Epoch: 2 cost time: 4.660494089126587
Epoch: 2, Steps: 262 | Train Loss: 0.4905018 Vali Loss: 1.0348979 Test Loss: 0.4792742
Validation loss decreased (1.165484 --> 1.034898).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4349381
	speed: 0.0744s/iter; left time: 1903.4549s
	iters: 200, epoch: 3 | loss: 0.4684213
	speed: 0.0155s/iter; left time: 394.8260s
Epoch: 3 cost time: 4.657291889190674
Epoch: 3, Steps: 262 | Train Loss: 0.4579122 Vali Loss: 1.0038012 Test Loss: 0.4559436
Validation loss decreased (1.034898 --> 1.003801).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4721230
	speed: 0.0821s/iter; left time: 2078.8536s
	iters: 200, epoch: 4 | loss: 0.4247841
	speed: 0.0167s/iter; left time: 419.9452s
Epoch: 4 cost time: 5.318889379501343
Epoch: 4, Steps: 262 | Train Loss: 0.4490041 Vali Loss: 0.9922717 Test Loss: 0.4485545
Validation loss decreased (1.003801 --> 0.992272).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4723926
	speed: 0.0733s/iter; left time: 1835.9532s
	iters: 200, epoch: 5 | loss: 0.4463063
	speed: 0.0163s/iter; left time: 407.2205s
Epoch: 5 cost time: 4.724809408187866
Epoch: 5, Steps: 262 | Train Loss: 0.4451911 Vali Loss: 0.9865289 Test Loss: 0.4461411
Validation loss decreased (0.992272 --> 0.986529).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4332569
	speed: 0.0808s/iter; left time: 2003.3689s
	iters: 200, epoch: 6 | loss: 0.4710572
	speed: 0.0164s/iter; left time: 404.6361s
Epoch: 6 cost time: 4.838648796081543
Epoch: 6, Steps: 262 | Train Loss: 0.4440131 Vali Loss: 0.9844742 Test Loss: 0.4451455
Validation loss decreased (0.986529 --> 0.984474).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4452206
	speed: 0.0799s/iter; left time: 1959.5706s
	iters: 200, epoch: 7 | loss: 0.4714409
	speed: 0.0174s/iter; left time: 424.5748s
Epoch: 7 cost time: 5.017693996429443
Epoch: 7, Steps: 262 | Train Loss: 0.4432654 Vali Loss: 0.9823300 Test Loss: 0.4448586
Validation loss decreased (0.984474 --> 0.982330).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4295429
	speed: 0.0783s/iter; left time: 1901.3140s
	iters: 200, epoch: 8 | loss: 0.4481747
	speed: 0.0154s/iter; left time: 371.8173s
Epoch: 8 cost time: 4.6293089389801025
Epoch: 8, Steps: 262 | Train Loss: 0.4431209 Vali Loss: 0.9814520 Test Loss: 0.4451070
Validation loss decreased (0.982330 --> 0.981452).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4635096
	speed: 0.0747s/iter; left time: 1793.5407s
	iters: 200, epoch: 9 | loss: 0.4937105
	speed: 0.0153s/iter; left time: 366.4697s
Epoch: 9 cost time: 4.5923943519592285
Epoch: 9, Steps: 262 | Train Loss: 0.4429391 Vali Loss: 0.9815117 Test Loss: 0.4449708
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4420355
	speed: 0.0736s/iter; left time: 1746.6961s
	iters: 200, epoch: 10 | loss: 0.4356425
	speed: 0.0153s/iter; left time: 362.8235s
Epoch: 10 cost time: 4.464174270629883
Epoch: 10, Steps: 262 | Train Loss: 0.4429342 Vali Loss: 0.9800090 Test Loss: 0.4449916
Validation loss decreased (0.981452 --> 0.980009).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4066131
	speed: 0.0755s/iter; left time: 1773.8421s
	iters: 200, epoch: 11 | loss: 0.4233617
	speed: 0.0152s/iter; left time: 355.4249s
Epoch: 11 cost time: 4.534298419952393
Epoch: 11, Steps: 262 | Train Loss: 0.4428796 Vali Loss: 0.9797063 Test Loss: 0.4450045
Validation loss decreased (0.980009 --> 0.979706).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4710769
	speed: 0.0729s/iter; left time: 1693.7865s
	iters: 200, epoch: 12 | loss: 0.4645581
	speed: 0.0149s/iter; left time: 343.6341s
Epoch: 12 cost time: 4.4560463428497314
Epoch: 12, Steps: 262 | Train Loss: 0.4427667 Vali Loss: 0.9796932 Test Loss: 0.4450416
Validation loss decreased (0.979706 --> 0.979693).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4570794
	speed: 0.0737s/iter; left time: 1690.9764s
	iters: 200, epoch: 13 | loss: 0.4854058
	speed: 0.0154s/iter; left time: 352.9983s
Epoch: 13 cost time: 4.612234115600586
Epoch: 13, Steps: 262 | Train Loss: 0.4429211 Vali Loss: 0.9801731 Test Loss: 0.4449511
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4601541
	speed: 0.0744s/iter; left time: 1688.1576s
	iters: 200, epoch: 14 | loss: 0.4390862
	speed: 0.0152s/iter; left time: 342.9356s
Epoch: 14 cost time: 4.5751800537109375
Epoch: 14, Steps: 262 | Train Loss: 0.4428184 Vali Loss: 0.9804750 Test Loss: 0.4449414
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4467745
	speed: 0.0757s/iter; left time: 1697.8470s
	iters: 200, epoch: 15 | loss: 0.4186855
	speed: 0.0156s/iter; left time: 347.5695s
Epoch: 15 cost time: 4.601562261581421
Epoch: 15, Steps: 262 | Train Loss: 0.4427173 Vali Loss: 0.9801866 Test Loss: 0.4450409
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4944765
	speed: 0.0747s/iter; left time: 1655.4209s
	iters: 200, epoch: 16 | loss: 0.4246535
	speed: 0.0152s/iter; left time: 336.4963s
Epoch: 16 cost time: 4.517182111740112
Epoch: 16, Steps: 262 | Train Loss: 0.4426842 Vali Loss: 0.9801393 Test Loss: 0.4450497
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4485917
	speed: 0.0731s/iter; left time: 1602.0472s
	iters: 200, epoch: 17 | loss: 0.4107298
	speed: 0.0148s/iter; left time: 321.9481s
Epoch: 17 cost time: 4.393982172012329
Epoch: 17, Steps: 262 | Train Loss: 0.4425448 Vali Loss: 0.9798414 Test Loss: 0.4450355
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4543389
	speed: 0.0774s/iter; left time: 1674.5326s
	iters: 200, epoch: 18 | loss: 0.4466302
	speed: 0.0174s/iter; left time: 374.4377s
Epoch: 18 cost time: 5.080446720123291
Epoch: 18, Steps: 262 | Train Loss: 0.4425416 Vali Loss: 0.9800557 Test Loss: 0.4451892
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4386197
	speed: 0.0764s/iter; left time: 1632.8398s
	iters: 200, epoch: 19 | loss: 0.4102576
	speed: 0.0151s/iter; left time: 322.0655s
Epoch: 19 cost time: 4.579280138015747
Epoch: 19, Steps: 262 | Train Loss: 0.4425084 Vali Loss: 0.9802284 Test Loss: 0.4449887
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4592801
	speed: 0.0724s/iter; left time: 1529.8210s
	iters: 200, epoch: 20 | loss: 0.4730523
	speed: 0.0150s/iter; left time: 314.8794s
Epoch: 20 cost time: 4.5487141609191895
Epoch: 20, Steps: 262 | Train Loss: 0.4425812 Vali Loss: 0.9793817 Test Loss: 0.4450781
Validation loss decreased (0.979693 --> 0.979382).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4672485
	speed: 0.0747s/iter; left time: 1558.2971s
	iters: 200, epoch: 21 | loss: 0.4425661
	speed: 0.0149s/iter; left time: 309.6713s
Epoch: 21 cost time: 4.489145278930664
Epoch: 21, Steps: 262 | Train Loss: 0.4425414 Vali Loss: 0.9804864 Test Loss: 0.4450396
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4761053
	speed: 0.0811s/iter; left time: 1671.2397s
	iters: 200, epoch: 22 | loss: 0.4512496
	speed: 0.0157s/iter; left time: 321.9908s
Epoch: 22 cost time: 4.875293731689453
Epoch: 22, Steps: 262 | Train Loss: 0.4426728 Vali Loss: 0.9802678 Test Loss: 0.4450177
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4438611
	speed: 0.0809s/iter; left time: 1644.8639s
	iters: 200, epoch: 23 | loss: 0.4811775
	speed: 0.0152s/iter; left time: 307.0531s
Epoch: 23 cost time: 4.624661207199097
Epoch: 23, Steps: 262 | Train Loss: 0.4425959 Vali Loss: 0.9789494 Test Loss: 0.4451356
Validation loss decreased (0.979382 --> 0.978949).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4477522
	speed: 0.0758s/iter; left time: 1522.2162s
	iters: 200, epoch: 24 | loss: 0.4023502
	speed: 0.0150s/iter; left time: 300.1971s
Epoch: 24 cost time: 4.430451154708862
Epoch: 24, Steps: 262 | Train Loss: 0.4426212 Vali Loss: 0.9805572 Test Loss: 0.4451124
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4513943
	speed: 0.0729s/iter; left time: 1443.7157s
	iters: 200, epoch: 25 | loss: 0.4894088
	speed: 0.0151s/iter; left time: 297.0000s
Epoch: 25 cost time: 4.50704026222229
Epoch: 25, Steps: 262 | Train Loss: 0.4425893 Vali Loss: 0.9804466 Test Loss: 0.4451620
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4766447
	speed: 0.0740s/iter; left time: 1447.1903s
	iters: 200, epoch: 26 | loss: 0.4326409
	speed: 0.0149s/iter; left time: 289.7493s
Epoch: 26 cost time: 4.51850962638855
Epoch: 26, Steps: 262 | Train Loss: 0.4424748 Vali Loss: 0.9807141 Test Loss: 0.4450984
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4567482
	speed: 0.0733s/iter; left time: 1414.4044s
	iters: 200, epoch: 27 | loss: 0.4380678
	speed: 0.0153s/iter; left time: 293.2288s
Epoch: 27 cost time: 5.4645304679870605
Epoch: 27, Steps: 262 | Train Loss: 0.4426353 Vali Loss: 0.9802004 Test Loss: 0.4450741
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4149964
	speed: 0.0841s/iter; left time: 1600.2761s
	iters: 200, epoch: 28 | loss: 0.4109514
	speed: 0.0153s/iter; left time: 289.5151s
Epoch: 28 cost time: 4.623410940170288
Epoch: 28, Steps: 262 | Train Loss: 0.4425685 Vali Loss: 0.9799255 Test Loss: 0.4451261
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4316966
	speed: 0.0744s/iter; left time: 1396.9216s
	iters: 200, epoch: 29 | loss: 0.4649488
	speed: 0.0152s/iter; left time: 283.2260s
Epoch: 29 cost time: 4.596027851104736
Epoch: 29, Steps: 262 | Train Loss: 0.4424945 Vali Loss: 0.9790266 Test Loss: 0.4451175
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4799630
	speed: 0.0733s/iter; left time: 1355.4249s
	iters: 200, epoch: 30 | loss: 0.4439555
	speed: 0.0421s/iter; left time: 774.6163s
Epoch: 30 cost time: 8.924418926239014
Epoch: 30, Steps: 262 | Train Loss: 0.4424679 Vali Loss: 0.9795471 Test Loss: 0.4451553
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4970530
	speed: 0.0900s/iter; left time: 1642.1766s
	iters: 200, epoch: 31 | loss: 0.4423446
	speed: 0.0151s/iter; left time: 273.0235s
Epoch: 31 cost time: 4.411837577819824
Epoch: 31, Steps: 262 | Train Loss: 0.4426080 Vali Loss: 0.9802489 Test Loss: 0.4451230
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4519963
	speed: 0.0709s/iter; left time: 1275.4198s
	iters: 200, epoch: 32 | loss: 0.4170620
	speed: 0.0151s/iter; left time: 270.4937s
Epoch: 32 cost time: 4.401947021484375
Epoch: 32, Steps: 262 | Train Loss: 0.4425273 Vali Loss: 0.9797814 Test Loss: 0.4450764
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4147080
	speed: 0.0877s/iter; left time: 1553.6702s
	iters: 200, epoch: 33 | loss: 0.4687707
	speed: 0.0149s/iter; left time: 262.9645s
Epoch: 33 cost time: 5.874128341674805
Epoch: 33, Steps: 262 | Train Loss: 0.4424226 Vali Loss: 0.9801242 Test Loss: 0.4451815
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4178525
	speed: 0.0747s/iter; left time: 1303.6587s
	iters: 200, epoch: 34 | loss: 0.4861229
	speed: 0.0156s/iter; left time: 270.3705s
Epoch: 34 cost time: 4.546497344970703
Epoch: 34, Steps: 262 | Train Loss: 0.4424973 Vali Loss: 0.9801253 Test Loss: 0.4451790
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4657906
	speed: 0.0740s/iter; left time: 1272.8514s
	iters: 200, epoch: 35 | loss: 0.4323879
	speed: 0.0153s/iter; left time: 260.7028s
Epoch: 35 cost time: 4.604741334915161
Epoch: 35, Steps: 262 | Train Loss: 0.4425007 Vali Loss: 0.9808348 Test Loss: 0.4451714
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4223640
	speed: 0.0749s/iter; left time: 1267.5829s
	iters: 200, epoch: 36 | loss: 0.4426117
	speed: 0.0159s/iter; left time: 267.6109s
Epoch: 36 cost time: 4.661036491394043
Epoch: 36, Steps: 262 | Train Loss: 0.4425356 Vali Loss: 0.9804714 Test Loss: 0.4451489
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4320580
	speed: 0.0737s/iter; left time: 1228.5527s
	iters: 200, epoch: 37 | loss: 0.4386396
	speed: 0.0154s/iter; left time: 254.3800s
Epoch: 37 cost time: 4.609829425811768
Epoch: 37, Steps: 262 | Train Loss: 0.4424902 Vali Loss: 0.9799737 Test Loss: 0.4451197
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4180445
	speed: 0.0760s/iter; left time: 1246.8092s
	iters: 200, epoch: 38 | loss: 0.4558837
	speed: 0.0152s/iter; left time: 247.5753s
Epoch: 38 cost time: 4.532080411911011
Epoch: 38, Steps: 262 | Train Loss: 0.4426208 Vali Loss: 0.9807779 Test Loss: 0.4451218
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4006670
	speed: 0.0725s/iter; left time: 1171.1859s
	iters: 200, epoch: 39 | loss: 0.4862030
	speed: 0.0148s/iter; left time: 238.0436s
Epoch: 39 cost time: 4.475421190261841
Epoch: 39, Steps: 262 | Train Loss: 0.4424800 Vali Loss: 0.9810011 Test Loss: 0.4452311
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4237280
	speed: 0.0728s/iter; left time: 1156.8292s
	iters: 200, epoch: 40 | loss: 0.4197413
	speed: 0.0147s/iter; left time: 232.3069s
Epoch: 40 cost time: 4.46253514289856
Epoch: 40, Steps: 262 | Train Loss: 0.4424901 Vali Loss: 0.9806893 Test Loss: 0.4451643
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4878004
	speed: 0.0718s/iter; left time: 1121.7360s
	iters: 200, epoch: 41 | loss: 0.4801070
	speed: 0.0151s/iter; left time: 233.6633s
Epoch: 41 cost time: 4.439984560012817
Epoch: 41, Steps: 262 | Train Loss: 0.4422702 Vali Loss: 0.9797788 Test Loss: 0.4451421
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4369153
	speed: 0.0726s/iter; left time: 1114.5979s
	iters: 200, epoch: 42 | loss: 0.4741890
	speed: 0.0147s/iter; left time: 223.8681s
Epoch: 42 cost time: 4.399438858032227
Epoch: 42, Steps: 262 | Train Loss: 0.4425515 Vali Loss: 0.9799149 Test Loss: 0.4451251
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4500391
	speed: 0.0762s/iter; left time: 1151.1406s
	iters: 200, epoch: 43 | loss: 0.4607647
	speed: 0.0155s/iter; left time: 232.6579s
Epoch: 43 cost time: 4.66322922706604
Epoch: 43, Steps: 262 | Train Loss: 0.4425029 Vali Loss: 0.9802877 Test Loss: 0.4451361
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.44331902265548706, mae:0.42457863688468933, rse:0.6334742307662964, corr:[0.5316729  0.53404886 0.5313159  0.5287238  0.5279341  0.52773243
 0.52693486 0.52598214 0.5254196  0.52574736 0.52644753 0.5264508
 0.52582026 0.52456695 0.523009   0.52127284 0.5193544  0.5174703
 0.515783   0.5142409  0.5126614  0.51080006 0.5086074  0.5062446
 0.5038376  0.501593   0.49972168 0.4980911  0.49674287 0.4958285
 0.49570242 0.4962693  0.49698892 0.49740908 0.49731287 0.4969623
 0.4964938  0.4960525  0.4958404  0.49565428 0.49551505 0.49529704
 0.49505278 0.49493966 0.49492082 0.4949537  0.49505755 0.49519432
 0.4953277  0.49534982 0.49529994 0.49523333 0.4952438  0.49529856
 0.49537116 0.49538943 0.4954577  0.49543878 0.49540064 0.49528202
 0.4951495  0.49499634 0.49477777 0.49453655 0.49433172 0.4943761
 0.4945983  0.4948383  0.4951574  0.49551174 0.49591336 0.49627966
 0.49657074 0.49675962 0.4969714  0.49709377 0.4971902  0.497216
 0.49715155 0.49702266 0.49694002 0.49691856 0.4969443  0.49683535
 0.4966159  0.49641117 0.49629793 0.4961712  0.49597362 0.4959055
 0.49593443 0.49593553 0.49573305 0.49526572 0.49460962 0.4938073
 0.49299854 0.4922427  0.49141723 0.49054107 0.4898832  0.4897753
 0.4902666  0.4911066  0.49200958 0.4928313  0.49365866 0.494645
 0.49576542 0.4967323  0.49727243 0.49745768 0.497275   0.49688944
 0.49644622 0.49617976 0.49597403 0.49566582 0.49523675 0.49467418
 0.49412715 0.49354094 0.49290806 0.4923389  0.4917232  0.49101534
 0.49038672 0.48998445 0.48980463 0.48974735 0.48968086 0.4895128
 0.48924917 0.4890063  0.48866448 0.48809174 0.487566   0.48721197
 0.48711753 0.48733705 0.48761776 0.4877173  0.48757717 0.48736
 0.48725563 0.4873895  0.48752868 0.48758078 0.4876351  0.4874639
 0.48725075 0.48721287 0.48734182 0.48758852 0.48770267 0.48776454
 0.48779082 0.48786122 0.4878628  0.48785526 0.48782873 0.48786154
 0.4880371  0.48836017 0.48870242 0.48908496 0.48939297 0.48961127
 0.48985973 0.49010494 0.49023262 0.49034336 0.49033764 0.4901971
 0.4900461  0.4898884  0.48976135 0.48966795 0.48950225 0.48936412
 0.48930827 0.4893524  0.4894079  0.489434   0.48942572 0.4894034
 0.48938283 0.48940146 0.48950925 0.48974025 0.49002215 0.4902154
 0.49016652 0.48991048 0.4894916  0.4888553  0.48817688 0.4876632
 0.48726276 0.48683825 0.48640117 0.48631006 0.48639593 0.48648638
 0.48640257 0.48587942 0.48516956 0.48456165 0.48404047 0.48350954
 0.48286062 0.48216447 0.48143896 0.48062083 0.47967684 0.47866637
 0.47768512 0.4767332  0.47587055 0.47516388 0.4745873  0.47421184
 0.47410727 0.4742567  0.47441736 0.47445777 0.47437155 0.47412035
 0.4737557  0.47352037 0.4733422  0.47310126 0.47282282 0.47259253
 0.472467   0.4724865  0.4724251  0.47240886 0.4724808  0.47259855
 0.47288296 0.47310284 0.47325647 0.47322428 0.47306457 0.47290748
 0.47286078 0.47285846 0.472944   0.473085   0.4731965  0.47321928
 0.4731625  0.47315755 0.4730887  0.4730327  0.4729353  0.4728564
 0.4729882  0.47331816 0.47372472 0.47411487 0.47440422 0.474677
 0.47503898 0.4753687  0.47567168 0.4759585  0.47621727 0.47632372
 0.47637776 0.4763721  0.47645605 0.47660175 0.4768134  0.4769843
 0.47708735 0.47725064 0.47745186 0.47768614 0.4779598  0.47818053
 0.47834346 0.4783138  0.47815722 0.47793323 0.47763515 0.4772482
 0.47662523 0.47586557 0.47511333 0.47435188 0.4737363  0.47335774
 0.4733729  0.47355565 0.47389117 0.47433436 0.4748109  0.4753132
 0.4756948  0.47574472 0.47558978 0.47537798 0.4752024  0.47505432
 0.47494563 0.47478145 0.47448117 0.47410318 0.47372463 0.4733964
 0.47308937 0.47268778 0.47226596 0.4719625  0.47160333 0.4713796
 0.47133073 0.47126597 0.4711062  0.47093645 0.47063044 0.4702792
 0.46994585 0.46966818 0.46936572 0.46902016 0.46861398 0.46826354
 0.46807766 0.46810955 0.46816692 0.46814606 0.46809176 0.4680292
 0.46804637 0.46812657 0.4681782  0.4681699  0.46822143 0.46823442
 0.46820155 0.46815586 0.46817264 0.46821472 0.46820018 0.46818468
 0.46814921 0.46820313 0.4683679  0.46847525 0.46851122 0.46851206
 0.46855247 0.468752   0.46896845 0.4691647  0.46926013 0.46929118
 0.46927345 0.4693713  0.46950457 0.46962848 0.46966067 0.46955836
 0.46937895 0.46933627 0.46949947 0.46975443 0.4700399  0.4702311
 0.47038636 0.47056967 0.47084865 0.47114924 0.47141424 0.47166014
 0.4718518  0.4719546  0.47201476 0.47203127 0.47201258 0.47196093
 0.47181252 0.47146392 0.47109172 0.47063056 0.47014475 0.46993312
 0.47003874 0.4703759  0.47069183 0.47114986 0.47171137 0.47237164
 0.47293448 0.4732125  0.4731657  0.47303444 0.47286034 0.47270113
 0.4725361  0.4722932  0.47196475 0.47157696 0.47109893 0.47057414
 0.4700767  0.46958888 0.4691588  0.4688333  0.46860647 0.46845222
 0.46844754 0.4684379  0.46839482 0.46823126 0.46789795 0.46760213
 0.46728706 0.46708223 0.46690935 0.46671164 0.46645045 0.46621272
 0.46595705 0.46581995 0.4657532  0.4657708  0.4657886  0.46589205
 0.46593112 0.46592626 0.46585956 0.46579066 0.46563292 0.46555185
 0.46545714 0.46539056 0.46543464 0.46558312 0.4657598  0.4659269
 0.46603656 0.46611685 0.46612462 0.4661674  0.46618822 0.4661506
 0.46618098 0.46632394 0.46651417 0.46674368 0.46691057 0.46718627
 0.4674514  0.4675724  0.46753162 0.46745333 0.46732417 0.46723205
 0.46723273 0.46731722 0.46739024 0.46748188 0.4675739  0.46763065
 0.4677056  0.46783617 0.46802837 0.46811515 0.46810785 0.46805155
 0.46791512 0.46773678 0.46747455 0.46703684 0.46633014 0.465351
 0.46416956 0.46295375 0.4618449  0.46088126 0.46002927 0.4593844
 0.45898944 0.45860904 0.45841795 0.45832798 0.4582719  0.4581949
 0.4581596  0.45800364 0.4577219  0.4574643  0.4572748  0.4570599
 0.45673206 0.45629305 0.45577982 0.4552553  0.45484596 0.45447966
 0.45413554 0.4537472  0.4533444  0.4530722  0.45289236 0.45284313
 0.45285428 0.45285106 0.4527438  0.45263118 0.4525006  0.4523795
 0.45226398 0.45222077 0.45213553 0.4518414  0.45140287 0.45101076
 0.45071614 0.45059124 0.45053458 0.45046335 0.4504501  0.45053747
 0.45064762 0.45074332 0.4507268  0.4506306  0.45057166 0.45055968
 0.45056126 0.45049736 0.45049372 0.45047414 0.45046172 0.4504978
 0.45062274 0.450689   0.45076072 0.4507895  0.4507952  0.4508393
 0.45094565 0.45111126 0.45128787 0.45137417 0.45135286 0.45138457
 0.45147786 0.4515711  0.4516406  0.45173866 0.45178196 0.45178637
 0.45184734 0.45200533 0.45216143 0.45230654 0.4524015  0.45249042
 0.45251852 0.45263064 0.45273066 0.45279998 0.45281368 0.45279667
 0.45271587 0.4524872  0.4521007  0.45150077 0.4506427  0.44951499
 0.44813374 0.44670317 0.44541958 0.44425014 0.44332117 0.44278422
 0.4426499  0.44276708 0.44300762 0.4432239  0.44349033 0.44383076
 0.44424555 0.44456318 0.44470397 0.44471404 0.4445917  0.44431785
 0.44387394 0.4433901  0.4430064  0.44270557 0.44246298 0.44215855
 0.44185787 0.44148722 0.44104612 0.4407157  0.44047126 0.44033468
 0.4403365  0.44048923 0.44064036 0.4407215  0.44075054 0.44077736
 0.44066676 0.44034454 0.4399492  0.43948925 0.4391009  0.43881813
 0.43875027 0.4388181  0.4388861  0.43889704 0.43892115 0.4390356
 0.4391831  0.43930838 0.4393325  0.43922424 0.43913323 0.43913332
 0.43915194 0.43914923 0.4390961  0.439003   0.43897855 0.43890166
 0.4387564  0.43866885 0.43858397 0.43848848 0.4385021  0.43850803
 0.43857592 0.43874428 0.4389582  0.43916225 0.43929708 0.43944326
 0.43962622 0.4397514  0.43986246 0.44000912 0.44010174 0.44013682
 0.44013903 0.4401974  0.4403236  0.4404839  0.44066706 0.44084877
 0.44101402 0.4412101  0.44141042 0.44148767 0.4414789  0.44141716
 0.44131804 0.44118276 0.44093597 0.4405115  0.43990016 0.4391153
 0.43818882 0.4372468  0.43631083 0.43532747 0.43441984 0.43374076
 0.4334867  0.43379706 0.43446916 0.43524992 0.4359381  0.4366056
 0.4371499  0.43731853 0.43728465 0.437187   0.43700826 0.43666553
 0.4361977  0.435708   0.43532282 0.4350045  0.43462363 0.43421805
 0.433772   0.43325713 0.43283743 0.43255064 0.43231532 0.43203035
 0.4317123  0.43137658 0.43120635 0.43126413 0.43127713 0.4311201
 0.4306727  0.4299668  0.42940584 0.42921    0.4293517  0.4295228
 0.42957678 0.4295775  0.4297307  0.43043518 0.43138853 0.43115315]
