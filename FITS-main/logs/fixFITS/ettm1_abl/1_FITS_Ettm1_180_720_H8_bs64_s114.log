Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=26, out_features=130, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3028480.0
params:  3510.0
Trainable parameters:  3510
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7557209
	speed: 0.0212s/iter; left time: 553.2558s
	iters: 200, epoch: 1 | loss: 0.5618466
	speed: 0.0151s/iter; left time: 392.1758s
Epoch: 1 cost time: 4.573447227478027
Epoch: 1, Steps: 262 | Train Loss: 0.7398453 Vali Loss: 1.1802082 Test Loss: 0.6001492
Validation loss decreased (inf --> 1.180208).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5394268
	speed: 0.0760s/iter; left time: 1964.3484s
	iters: 200, epoch: 2 | loss: 0.4812361
	speed: 0.0150s/iter; left time: 385.6011s
Epoch: 2 cost time: 4.46821403503418
Epoch: 2, Steps: 262 | Train Loss: 0.4958652 Vali Loss: 1.0436131 Test Loss: 0.4832165
Validation loss decreased (1.180208 --> 1.043613).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4355836
	speed: 0.0750s/iter; left time: 1917.6745s
	iters: 200, epoch: 3 | loss: 0.5004398
	speed: 0.0151s/iter; left time: 383.8216s
Epoch: 3 cost time: 4.644155025482178
Epoch: 3, Steps: 262 | Train Loss: 0.4604757 Vali Loss: 1.0081387 Test Loss: 0.4574026
Validation loss decreased (1.043613 --> 1.008139).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4802654
	speed: 0.0742s/iter; left time: 1877.4787s
	iters: 200, epoch: 4 | loss: 0.4591860
	speed: 0.0150s/iter; left time: 377.6747s
Epoch: 4 cost time: 4.596925973892212
Epoch: 4, Steps: 262 | Train Loss: 0.4498567 Vali Loss: 0.9926615 Test Loss: 0.4485507
Validation loss decreased (1.008139 --> 0.992662).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4094101
	speed: 0.0727s/iter; left time: 1820.2051s
	iters: 200, epoch: 5 | loss: 0.4207155
	speed: 0.0151s/iter; left time: 377.0739s
Epoch: 5 cost time: 4.4517621994018555
Epoch: 5, Steps: 262 | Train Loss: 0.4455069 Vali Loss: 0.9867536 Test Loss: 0.4456939
Validation loss decreased (0.992662 --> 0.986754).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4366954
	speed: 0.0726s/iter; left time: 1800.0478s
	iters: 200, epoch: 6 | loss: 0.4395893
	speed: 0.0152s/iter; left time: 376.2351s
Epoch: 6 cost time: 4.493691921234131
Epoch: 6, Steps: 262 | Train Loss: 0.4440038 Vali Loss: 0.9832591 Test Loss: 0.4449572
Validation loss decreased (0.986754 --> 0.983259).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4352504
	speed: 0.0711s/iter; left time: 1743.3447s
	iters: 200, epoch: 7 | loss: 0.4636186
	speed: 0.0151s/iter; left time: 369.8372s
Epoch: 7 cost time: 4.460651874542236
Epoch: 7, Steps: 262 | Train Loss: 0.4433534 Vali Loss: 0.9829448 Test Loss: 0.4447918
Validation loss decreased (0.983259 --> 0.982945).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4314997
	speed: 0.0726s/iter; left time: 1761.9820s
	iters: 200, epoch: 8 | loss: 0.4770206
	speed: 0.0149s/iter; left time: 360.2018s
Epoch: 8 cost time: 4.485771894454956
Epoch: 8, Steps: 262 | Train Loss: 0.4431350 Vali Loss: 0.9817051 Test Loss: 0.4447131
Validation loss decreased (0.982945 --> 0.981705).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4269367
	speed: 0.0790s/iter; left time: 1896.5521s
	iters: 200, epoch: 9 | loss: 0.4721995
	speed: 0.0165s/iter; left time: 394.4645s
Epoch: 9 cost time: 4.878465414047241
Epoch: 9, Steps: 262 | Train Loss: 0.4429909 Vali Loss: 0.9804103 Test Loss: 0.4447775
Validation loss decreased (0.981705 --> 0.980410).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4543061
	speed: 0.0755s/iter; left time: 1792.1204s
	iters: 200, epoch: 10 | loss: 0.4556538
	speed: 0.0150s/iter; left time: 354.7225s
Epoch: 10 cost time: 4.508725643157959
Epoch: 10, Steps: 262 | Train Loss: 0.4428131 Vali Loss: 0.9807557 Test Loss: 0.4449629
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4202146
	speed: 0.0773s/iter; left time: 1815.1950s
	iters: 200, epoch: 11 | loss: 0.4377533
	speed: 0.0161s/iter; left time: 376.8939s
Epoch: 11 cost time: 4.752385854721069
Epoch: 11, Steps: 262 | Train Loss: 0.4428372 Vali Loss: 0.9815637 Test Loss: 0.4450610
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4697512
	speed: 0.0719s/iter; left time: 1669.7747s
	iters: 200, epoch: 12 | loss: 0.4220233
	speed: 0.0151s/iter; left time: 350.0903s
Epoch: 12 cost time: 4.406733512878418
Epoch: 12, Steps: 262 | Train Loss: 0.4429577 Vali Loss: 0.9814534 Test Loss: 0.4451068
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4169887
	speed: 0.0719s/iter; left time: 1651.7472s
	iters: 200, epoch: 13 | loss: 0.4118578
	speed: 0.0150s/iter; left time: 343.2396s
Epoch: 13 cost time: 4.46161150932312
Epoch: 13, Steps: 262 | Train Loss: 0.4429225 Vali Loss: 0.9801387 Test Loss: 0.4449894
Validation loss decreased (0.980410 --> 0.980139).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3965516
	speed: 0.0727s/iter; left time: 1650.0125s
	iters: 200, epoch: 14 | loss: 0.4649310
	speed: 0.0151s/iter; left time: 341.7099s
Epoch: 14 cost time: 4.486831426620483
Epoch: 14, Steps: 262 | Train Loss: 0.4428140 Vali Loss: 0.9804963 Test Loss: 0.4453073
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4326117
	speed: 0.0899s/iter; left time: 2015.6594s
	iters: 200, epoch: 15 | loss: 0.4173352
	speed: 0.0149s/iter; left time: 332.6400s
Epoch: 15 cost time: 4.842325687408447
Epoch: 15, Steps: 262 | Train Loss: 0.4429111 Vali Loss: 0.9809288 Test Loss: 0.4450115
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4778157
	speed: 0.0785s/iter; left time: 1740.2942s
	iters: 200, epoch: 16 | loss: 0.4351395
	speed: 0.0152s/iter; left time: 334.4103s
Epoch: 16 cost time: 4.556499481201172
Epoch: 16, Steps: 262 | Train Loss: 0.4429075 Vali Loss: 0.9803784 Test Loss: 0.4450078
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4778301
	speed: 0.0717s/iter; left time: 1571.8271s
	iters: 200, epoch: 17 | loss: 0.4415817
	speed: 0.0153s/iter; left time: 333.1110s
Epoch: 17 cost time: 4.538931846618652
Epoch: 17, Steps: 262 | Train Loss: 0.4429141 Vali Loss: 0.9800406 Test Loss: 0.4450847
Validation loss decreased (0.980139 --> 0.980041).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4818128
	speed: 0.0726s/iter; left time: 1571.5556s
	iters: 200, epoch: 18 | loss: 0.4434210
	speed: 0.0149s/iter; left time: 320.0182s
Epoch: 18 cost time: 4.37475323677063
Epoch: 18, Steps: 262 | Train Loss: 0.4427411 Vali Loss: 0.9798338 Test Loss: 0.4451010
Validation loss decreased (0.980041 --> 0.979834).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4583653
	speed: 0.0727s/iter; left time: 1554.0404s
	iters: 200, epoch: 19 | loss: 0.4342832
	speed: 0.0151s/iter; left time: 321.6953s
Epoch: 19 cost time: 4.449002265930176
Epoch: 19, Steps: 262 | Train Loss: 0.4426638 Vali Loss: 0.9803579 Test Loss: 0.4451355
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4354604
	speed: 0.0742s/iter; left time: 1568.0284s
	iters: 200, epoch: 20 | loss: 0.4324485
	speed: 0.0150s/iter; left time: 314.5099s
Epoch: 20 cost time: 4.574801921844482
Epoch: 20, Steps: 262 | Train Loss: 0.4427014 Vali Loss: 0.9802712 Test Loss: 0.4450437
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4910952
	speed: 0.0715s/iter; left time: 1492.0413s
	iters: 200, epoch: 21 | loss: 0.4143647
	speed: 0.0151s/iter; left time: 314.3292s
Epoch: 21 cost time: 4.470880508422852
Epoch: 21, Steps: 262 | Train Loss: 0.4427303 Vali Loss: 0.9800805 Test Loss: 0.4451942
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4450190
	speed: 0.0954s/iter; left time: 1965.0655s
	iters: 200, epoch: 22 | loss: 0.4368302
	speed: 0.0194s/iter; left time: 398.1431s
Epoch: 22 cost time: 5.2896952629089355
Epoch: 22, Steps: 262 | Train Loss: 0.4427120 Vali Loss: 0.9810808 Test Loss: 0.4451887
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4559530
	speed: 0.0760s/iter; left time: 1545.8018s
	iters: 200, epoch: 23 | loss: 0.4068642
	speed: 0.0167s/iter; left time: 338.1317s
Epoch: 23 cost time: 4.80951189994812
Epoch: 23, Steps: 262 | Train Loss: 0.4428203 Vali Loss: 0.9806617 Test Loss: 0.4450694
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4625660
	speed: 0.0752s/iter; left time: 1509.3968s
	iters: 200, epoch: 24 | loss: 0.4329713
	speed: 0.0151s/iter; left time: 301.9614s
Epoch: 24 cost time: 4.537394285202026
Epoch: 24, Steps: 262 | Train Loss: 0.4428527 Vali Loss: 0.9799281 Test Loss: 0.4451770
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4388055
	speed: 0.0714s/iter; left time: 1415.4997s
	iters: 200, epoch: 25 | loss: 0.4669803
	speed: 0.0151s/iter; left time: 297.6386s
Epoch: 25 cost time: 4.431688547134399
Epoch: 25, Steps: 262 | Train Loss: 0.4427535 Vali Loss: 0.9806435 Test Loss: 0.4452036
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3848273
	speed: 0.0741s/iter; left time: 1447.7881s
	iters: 200, epoch: 26 | loss: 0.4423372
	speed: 0.0149s/iter; left time: 289.0826s
Epoch: 26 cost time: 4.465743541717529
Epoch: 26, Steps: 262 | Train Loss: 0.4426753 Vali Loss: 0.9802212 Test Loss: 0.4452150
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4642054
	speed: 0.0740s/iter; left time: 1426.6577s
	iters: 200, epoch: 27 | loss: 0.3779331
	speed: 0.0412s/iter; left time: 790.3616s
Epoch: 27 cost time: 9.144855976104736
Epoch: 27, Steps: 262 | Train Loss: 0.4427595 Vali Loss: 0.9804424 Test Loss: 0.4452453
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4424953
	speed: 0.1063s/iter; left time: 2022.2362s
	iters: 200, epoch: 28 | loss: 0.4738629
	speed: 0.0437s/iter; left time: 826.2838s
Epoch: 28 cost time: 10.356452465057373
Epoch: 28, Steps: 262 | Train Loss: 0.4425868 Vali Loss: 0.9803720 Test Loss: 0.4452165
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4095268
	speed: 0.0873s/iter; left time: 1638.4036s
	iters: 200, epoch: 29 | loss: 0.4999733
	speed: 0.0158s/iter; left time: 294.1191s
Epoch: 29 cost time: 4.5370118618011475
Epoch: 29, Steps: 262 | Train Loss: 0.4427592 Vali Loss: 0.9810022 Test Loss: 0.4451672
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4382593
	speed: 0.0719s/iter; left time: 1330.4107s
	iters: 200, epoch: 30 | loss: 0.4614376
	speed: 0.0155s/iter; left time: 285.0931s
Epoch: 30 cost time: 4.574261426925659
Epoch: 30, Steps: 262 | Train Loss: 0.4426815 Vali Loss: 0.9799113 Test Loss: 0.4452685
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4812556
	speed: 0.0737s/iter; left time: 1343.7950s
	iters: 200, epoch: 31 | loss: 0.4605317
	speed: 0.0154s/iter; left time: 279.8072s
Epoch: 31 cost time: 4.571339130401611
Epoch: 31, Steps: 262 | Train Loss: 0.4425949 Vali Loss: 0.9811832 Test Loss: 0.4452134
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4619182
	speed: 0.0738s/iter; left time: 1326.1875s
	iters: 200, epoch: 32 | loss: 0.4497750
	speed: 0.0148s/iter; left time: 264.7484s
Epoch: 32 cost time: 4.489299297332764
Epoch: 32, Steps: 262 | Train Loss: 0.4425028 Vali Loss: 0.9807654 Test Loss: 0.4452201
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4115866
	speed: 0.0723s/iter; left time: 1281.3891s
	iters: 200, epoch: 33 | loss: 0.4415287
	speed: 0.0150s/iter; left time: 264.0387s
Epoch: 33 cost time: 4.563967943191528
Epoch: 33, Steps: 262 | Train Loss: 0.4426391 Vali Loss: 0.9803576 Test Loss: 0.4451786
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4034931
	speed: 0.0727s/iter; left time: 1268.6001s
	iters: 200, epoch: 34 | loss: 0.4174147
	speed: 0.0150s/iter; left time: 260.4583s
Epoch: 34 cost time: 4.521727561950684
Epoch: 34, Steps: 262 | Train Loss: 0.4427031 Vali Loss: 0.9805859 Test Loss: 0.4452357
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4778476
	speed: 0.0729s/iter; left time: 1252.6409s
	iters: 200, epoch: 35 | loss: 0.4262495
	speed: 0.0151s/iter; left time: 257.7296s
Epoch: 35 cost time: 4.540930509567261
Epoch: 35, Steps: 262 | Train Loss: 0.4425761 Vali Loss: 0.9811051 Test Loss: 0.4452641
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4405730
	speed: 0.0722s/iter; left time: 1221.6795s
	iters: 200, epoch: 36 | loss: 0.4293416
	speed: 0.0152s/iter; left time: 255.3941s
Epoch: 36 cost time: 4.433895111083984
Epoch: 36, Steps: 262 | Train Loss: 0.4427140 Vali Loss: 0.9806545 Test Loss: 0.4452151
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4507650
	speed: 0.0726s/iter; left time: 1210.8000s
	iters: 200, epoch: 37 | loss: 0.4515144
	speed: 0.0152s/iter; left time: 251.6050s
Epoch: 37 cost time: 4.516658306121826
Epoch: 37, Steps: 262 | Train Loss: 0.4427690 Vali Loss: 0.9805136 Test Loss: 0.4451975
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4482226
	speed: 0.0784s/iter; left time: 1287.0631s
	iters: 200, epoch: 38 | loss: 0.4225347
	speed: 0.0152s/iter; left time: 247.1636s
Epoch: 38 cost time: 4.660785913467407
Epoch: 38, Steps: 262 | Train Loss: 0.4426660 Vali Loss: 0.9802716 Test Loss: 0.4451998
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.44329479336738586, mae:0.42444199323654175, rse:0.6334568858146667, corr:[0.52884746 0.5329891  0.53247315 0.52986634 0.5276108  0.5266124
 0.5264815  0.52686334 0.52711064 0.52734876 0.5276838  0.5274767
 0.52658474 0.5249102  0.52302605 0.52140796 0.51990336 0.5182318
 0.5162855  0.51416785 0.51206964 0.5100588  0.50809383 0.5060497
 0.50371027 0.501142   0.49877506 0.49693507 0.495868   0.4955881
 0.49605852 0.49680704 0.4972861  0.49731997 0.49695078 0.49648914
 0.49607217 0.4957397  0.49555796 0.49532112 0.49506006 0.49472284
 0.49444023 0.49440375 0.49458987 0.4948992  0.49523702 0.49549612
 0.495648   0.49567282 0.49571627 0.49586    0.4961002  0.49630424
 0.49639702 0.49633422 0.4962644  0.49613008 0.49599737 0.4958194
 0.49567756 0.4955913  0.49549598 0.49539056 0.4952907  0.49535006
 0.49553204 0.49574238 0.49604502 0.49640056 0.4968045  0.49716422
 0.497406   0.4974831  0.49751437 0.49744666 0.49737853 0.49733013
 0.49731418 0.49730882 0.4973276  0.49731204 0.49722663 0.49695033
 0.49658123 0.4962905  0.49618784 0.49615607 0.49604955 0.49592403
 0.4957376  0.49549258 0.49519956 0.49487814 0.4945026  0.4938927
 0.49303222 0.49203104 0.49097964 0.49007183 0.4895328  0.48951358
 0.48993143 0.49061602 0.49149597 0.49255884 0.4937615  0.49496418
 0.4959913  0.4966941  0.4970391  0.4972101  0.49715242 0.49689755
 0.496477   0.49605983 0.49562582 0.49514532 0.49466237 0.49415156
 0.49364936 0.49301353 0.49224663 0.4915495  0.49095425 0.4904538
 0.49011675 0.4899619  0.48992786 0.4899438  0.48992124 0.4897705
 0.48946646 0.48912293 0.4886997  0.48813525 0.48763403 0.4872355
 0.4869871  0.4870069  0.48721102 0.48747915 0.48769206 0.4878134
 0.4878351  0.48782924 0.48773646 0.48766205 0.48775774 0.48774365
 0.48762366 0.4875106  0.48746246 0.4875777  0.4877239  0.48791918
 0.48806974 0.4881747  0.48815277 0.48813665 0.48820433 0.48842737
 0.48878974 0.48918587 0.48946068 0.48967037 0.48981512 0.48994982
 0.49017662 0.49043334 0.4905946  0.4907265  0.49071643 0.49052638
 0.49026942 0.4900084  0.48984468 0.48979288 0.48968974 0.48951715
 0.48927966 0.48906767 0.48896226 0.489039   0.48924354 0.4894397
 0.4894999  0.48943907 0.48940775 0.48957095 0.4899052  0.49022183
 0.49026042 0.48997036 0.4893787  0.48847675 0.48754364 0.48689115
 0.486532   0.48630217 0.48606616 0.48604134 0.48601672 0.4858964
 0.4856677  0.4851739  0.48460937 0.48417148 0.48379272 0.48336208
 0.48276028 0.48202    0.4811849  0.48028266 0.47937945 0.47855163
 0.4777653  0.47693026 0.4761148  0.47546783 0.4750183  0.47477362
 0.47469077 0.4746927  0.4746164  0.4744713  0.47433037 0.47414765
 0.47387904 0.47366095 0.47342232 0.47312438 0.47283942 0.472665
 0.47264066 0.47277912 0.47286922 0.47298113 0.47309712 0.4731601
 0.47329214 0.4733567  0.47342974 0.473462   0.47347942 0.47350734
 0.47355834 0.47355    0.47353873 0.47354406 0.47354648 0.47353342
 0.473516   0.4735783  0.4736012  0.47361937 0.4735808  0.4735211
 0.4736066  0.47384924 0.47417662 0.4745358  0.47485194 0.47516513
 0.47553784 0.47585955 0.47613934 0.4763969  0.476623   0.47669914
 0.47670004 0.4766256  0.476635   0.47672552 0.4769064  0.4770806
 0.47720546 0.47737905 0.4775603  0.4777429  0.47793102 0.47804096
 0.4780715  0.47790608 0.4776008  0.47721353 0.47675598 0.47627223
 0.47566444 0.47501156 0.4743996  0.4737586  0.47318792 0.4727771
 0.47272676 0.47290933 0.47334146 0.47392386 0.4744871  0.4749814
 0.47530505 0.47533152 0.47522783 0.4751242  0.47506538 0.4749969
 0.4749132  0.47474298 0.4744508  0.47411722 0.4738141  0.47356102
 0.47329578 0.47290808 0.47247657 0.47215658 0.47180504 0.47156033
 0.47143245 0.47128227 0.47109705 0.47098264 0.4708251  0.4706345
 0.47039318 0.47011927 0.4697831  0.46942016 0.4690292  0.4686752
 0.46841335 0.46829137 0.4682025  0.46811488 0.46808374 0.46809605
 0.46815985 0.46822602 0.4682288  0.46817356 0.4681974  0.46822548
 0.4682429  0.46824256 0.46826464 0.46827635 0.46822152 0.4681808
 0.46816793 0.46827364 0.46847358 0.46859577 0.468598   0.4685044
 0.4684051  0.46846882 0.46863005 0.46889007 0.46914926 0.46934366
 0.4693868  0.46940374 0.46940318 0.46945983 0.46956348 0.46962428
 0.46956423 0.4694748  0.46945655 0.46953952 0.46977642 0.47006202
 0.4703416  0.4705643  0.47076294 0.47093353 0.4711255  0.47139606
 0.47169673 0.47194603 0.47212282 0.4721627  0.47208095 0.4719147
 0.4716564  0.47124013 0.47083807 0.47035423 0.469807   0.46948248
 0.46948603 0.4698302  0.47030494 0.4709786  0.47166434 0.4722756
 0.47265306 0.47273895 0.472572   0.47242662 0.47233206 0.4723354
 0.4723944  0.47237644 0.47219756 0.47186804 0.47139743 0.47086415
 0.47033206 0.46977124 0.46923956 0.46881187 0.46851966 0.46833965
 0.4683169  0.468304   0.46829924 0.468245   0.4680762  0.46791282
 0.4676475  0.46738884 0.46710718 0.46680534 0.46647406 0.46619546
 0.46593353 0.46580756 0.46577516 0.4658513  0.46593782 0.4660774
 0.4661257  0.46610877 0.46604463 0.4659966  0.46589297 0.46586126
 0.4658073  0.465756   0.46577057 0.4658487  0.46594796 0.4660663
 0.46617788 0.46628025 0.4662815  0.46625456 0.466175   0.46607143
 0.46609882 0.46630523 0.46658954 0.46687442 0.46702364 0.46718264
 0.46729273 0.46731976 0.46730068 0.4673145  0.46725744 0.4671173
 0.46692958 0.46676502 0.4666624  0.46674272 0.46697876 0.46724066
 0.46745276 0.4675828  0.46766594 0.46765494 0.4676468  0.46769038
 0.4677011  0.46764067 0.4674179  0.46694022 0.46616322 0.46513012
 0.46393332 0.46272755 0.46162474 0.46064296 0.4597367  0.45900416
 0.45851946 0.45811427 0.45801002 0.45811927 0.45828676 0.4583276
 0.45824423 0.45789897 0.45736703 0.45690456 0.45664066 0.4565154
 0.45642343 0.456264   0.45595613 0.45547542 0.4549464  0.45440012
 0.45393473 0.4535389  0.45322192 0.45306253 0.45298156 0.4529715
 0.45296845 0.45293972 0.45285714 0.452793   0.45270362 0.45255646
 0.45231715 0.4520945  0.45188972 0.45162868 0.45133966 0.45110267
 0.45087713 0.45070547 0.45057032 0.45046577 0.45048058 0.45064303
 0.45080832 0.4508857  0.45079234 0.45058173 0.45041186 0.45034426
 0.4503815  0.45043144 0.4505447  0.45059317 0.45054927 0.4504503
 0.45039988 0.4503321  0.4503132  0.45027614 0.4502073  0.45015708
 0.4501774  0.4503207  0.45058286 0.45087254 0.45110503 0.4513091
 0.4514104  0.45137393 0.45130014 0.4513534  0.45148402 0.45162344
 0.45173115 0.4517962  0.45178127 0.4517854  0.45184407 0.4519945
 0.45214435 0.45237017 0.45254508 0.45264822 0.45266888 0.4526319
 0.45252597 0.45229274 0.4519472  0.45142496 0.4506439  0.4495971
 0.4483161  0.4470132  0.44587138 0.44480765 0.4438866  0.44322333
 0.44286475 0.4428056  0.44303882 0.4434324  0.44394225 0.44441405
 0.44474664 0.4448017  0.4446277  0.44441864 0.4442529  0.44413322
 0.44397703 0.44375584 0.4434681  0.44307512 0.4426312  0.4421179
 0.44166878 0.44125116 0.44086346 0.44063842 0.44053003 0.44050997
 0.4405685  0.4407093  0.44083533 0.4408988  0.44088775 0.44081306
 0.440572   0.44016203 0.43977442 0.43943822 0.4392261  0.4390814
 0.4390442  0.4390468  0.43901926 0.4389575  0.43894598 0.43903637
 0.43914893 0.43921906 0.43917337 0.43897498 0.43876588 0.43865967
 0.4386574  0.43873918 0.4388221  0.43880498 0.43870866 0.43842974
 0.43806255 0.43784964 0.43780056 0.43787867 0.4381011  0.4382497
 0.438335   0.4384379  0.43862548 0.4389465  0.43934998 0.4398034
 0.44018453 0.4403243  0.4403127  0.44031942 0.44037753 0.44050604
 0.4406482  0.44078094 0.4408755  0.44095126 0.44109076 0.4413262
 0.4416363  0.44201636 0.44239047 0.44259575 0.4426526  0.44257343
 0.44240284 0.44222236 0.44203058 0.44177663 0.44132817 0.44055182
 0.43946964 0.43834755 0.43737352 0.43657428 0.43601432 0.43564388
 0.43547717 0.43564105 0.4360913  0.43673396 0.43738776 0.43807146
 0.43860713 0.43867728 0.4384203  0.43801546 0.4376285  0.43735412
 0.43718973 0.43697852 0.4365957  0.43596354 0.4351278  0.4343765
 0.43384737 0.43348205 0.43324977 0.4329957  0.43256614 0.43193668
 0.4312929  0.43077776 0.43056208 0.4305968  0.43054375 0.43028626
 0.42981058 0.4292086  0.428816   0.42871258 0.4287976  0.42883763
 0.42883274 0.42900363 0.42951313 0.43060213 0.43212405 0.43321738]
