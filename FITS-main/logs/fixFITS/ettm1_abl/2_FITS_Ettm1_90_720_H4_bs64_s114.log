Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=14, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=14, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1580544.0
params:  1890.0
Trainable parameters:  1890
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.1169270
	speed: 0.0184s/iter; left time: 481.7521s
	iters: 200, epoch: 1 | loss: 0.7639319
	speed: 0.0136s/iter; left time: 355.3447s
Epoch: 1 cost time: 4.059827089309692
Epoch: 1, Steps: 263 | Train Loss: 1.0280794 Vali Loss: 1.5307298 Test Loss: 0.9976569
Validation loss decreased (inf --> 1.530730).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6113571
	speed: 0.0594s/iter; left time: 1539.9897s
	iters: 200, epoch: 2 | loss: 0.5702708
	speed: 0.0138s/iter; left time: 356.8199s
Epoch: 2 cost time: 4.0373804569244385
Epoch: 2, Steps: 263 | Train Loss: 0.5911983 Vali Loss: 1.1752748 Test Loss: 0.6544336
Validation loss decreased (1.530730 --> 1.175275).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5109316
	speed: 0.0590s/iter; left time: 1515.9519s
	iters: 200, epoch: 3 | loss: 0.4745320
	speed: 0.0140s/iter; left time: 357.4821s
Epoch: 3 cost time: 4.038349866867065
Epoch: 3, Steps: 263 | Train Loss: 0.4896360 Vali Loss: 1.0881324 Test Loss: 0.5699914
Validation loss decreased (1.175275 --> 1.088132).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4543480
	speed: 0.0610s/iter; left time: 1550.4348s
	iters: 200, epoch: 4 | loss: 0.4489523
	speed: 0.0140s/iter; left time: 353.7329s
Epoch: 4 cost time: 4.206695795059204
Epoch: 4, Steps: 263 | Train Loss: 0.4639144 Vali Loss: 1.0560845 Test Loss: 0.5405979
Validation loss decreased (1.088132 --> 1.056085).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4674901
	speed: 0.0612s/iter; left time: 1539.9651s
	iters: 200, epoch: 5 | loss: 0.4739192
	speed: 0.0145s/iter; left time: 362.6542s
Epoch: 5 cost time: 4.187147378921509
Epoch: 5, Steps: 263 | Train Loss: 0.4547723 Vali Loss: 1.0403979 Test Loss: 0.5256482
Validation loss decreased (1.056085 --> 1.040398).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4602614
	speed: 0.0603s/iter; left time: 1500.6561s
	iters: 200, epoch: 6 | loss: 0.4097704
	speed: 0.0148s/iter; left time: 365.9725s
Epoch: 6 cost time: 4.24023175239563
Epoch: 6, Steps: 263 | Train Loss: 0.4501562 Vali Loss: 1.0327587 Test Loss: 0.5164289
Validation loss decreased (1.040398 --> 1.032759).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4249988
	speed: 0.0611s/iter; left time: 1505.6077s
	iters: 200, epoch: 7 | loss: 0.4203532
	speed: 0.0137s/iter; left time: 335.6667s
Epoch: 7 cost time: 4.141739845275879
Epoch: 7, Steps: 263 | Train Loss: 0.4473882 Vali Loss: 1.0263773 Test Loss: 0.5103026
Validation loss decreased (1.032759 --> 1.026377).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4779293
	speed: 0.0609s/iter; left time: 1483.2623s
	iters: 200, epoch: 8 | loss: 0.4339801
	speed: 0.0148s/iter; left time: 358.2879s
Epoch: 8 cost time: 4.244297027587891
Epoch: 8, Steps: 263 | Train Loss: 0.4457164 Vali Loss: 1.0238838 Test Loss: 0.5059476
Validation loss decreased (1.026377 --> 1.023884).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4687386
	speed: 0.0608s/iter; left time: 1465.3976s
	iters: 200, epoch: 9 | loss: 0.4818902
	speed: 0.0142s/iter; left time: 341.3159s
Epoch: 9 cost time: 4.1836183071136475
Epoch: 9, Steps: 263 | Train Loss: 0.4446125 Vali Loss: 1.0204259 Test Loss: 0.5029841
Validation loss decreased (1.023884 --> 1.020426).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4582572
	speed: 0.0613s/iter; left time: 1461.9676s
	iters: 200, epoch: 10 | loss: 0.4976743
	speed: 0.0139s/iter; left time: 330.7832s
Epoch: 10 cost time: 4.211771726608276
Epoch: 10, Steps: 263 | Train Loss: 0.4438311 Vali Loss: 1.0192248 Test Loss: 0.5007917
Validation loss decreased (1.020426 --> 1.019225).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4677702
	speed: 0.0603s/iter; left time: 1420.4585s
	iters: 200, epoch: 11 | loss: 0.4286207
	speed: 0.0149s/iter; left time: 349.4175s
Epoch: 11 cost time: 4.110572099685669
Epoch: 11, Steps: 263 | Train Loss: 0.4434365 Vali Loss: 1.0186015 Test Loss: 0.4995681
Validation loss decreased (1.019225 --> 1.018602).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4514158
	speed: 0.0601s/iter; left time: 1399.9102s
	iters: 200, epoch: 12 | loss: 0.4396044
	speed: 0.0144s/iter; left time: 334.9040s
Epoch: 12 cost time: 4.089967727661133
Epoch: 12, Steps: 263 | Train Loss: 0.4431544 Vali Loss: 1.0169424 Test Loss: 0.4983156
Validation loss decreased (1.018602 --> 1.016942).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4878924
	speed: 0.0606s/iter; left time: 1397.2752s
	iters: 200, epoch: 13 | loss: 0.4017409
	speed: 0.0146s/iter; left time: 335.0679s
Epoch: 13 cost time: 4.190451145172119
Epoch: 13, Steps: 263 | Train Loss: 0.4428393 Vali Loss: 1.0174910 Test Loss: 0.4975571
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4909311
	speed: 0.0608s/iter; left time: 1384.1904s
	iters: 200, epoch: 14 | loss: 0.4321049
	speed: 0.0141s/iter; left time: 319.8958s
Epoch: 14 cost time: 4.133284091949463
Epoch: 14, Steps: 263 | Train Loss: 0.4427749 Vali Loss: 1.0172285 Test Loss: 0.4971268
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3971680
	speed: 0.0613s/iter; left time: 1381.2383s
	iters: 200, epoch: 15 | loss: 0.4773462
	speed: 0.0142s/iter; left time: 317.3768s
Epoch: 15 cost time: 4.215773582458496
Epoch: 15, Steps: 263 | Train Loss: 0.4427077 Vali Loss: 1.0169153 Test Loss: 0.4969578
Validation loss decreased (1.016942 --> 1.016915).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5105746
	speed: 0.0610s/iter; left time: 1357.5533s
	iters: 200, epoch: 16 | loss: 0.4159035
	speed: 0.0136s/iter; left time: 301.3859s
Epoch: 16 cost time: 4.139185428619385
Epoch: 16, Steps: 263 | Train Loss: 0.4426496 Vali Loss: 1.0158930 Test Loss: 0.4965530
Validation loss decreased (1.016915 --> 1.015893).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4288188
	speed: 0.0599s/iter; left time: 1316.8212s
	iters: 200, epoch: 17 | loss: 0.4392658
	speed: 0.0141s/iter; left time: 309.4818s
Epoch: 17 cost time: 4.12094521522522
Epoch: 17, Steps: 263 | Train Loss: 0.4426158 Vali Loss: 1.0165113 Test Loss: 0.4963574
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4234065
	speed: 0.0587s/iter; left time: 1274.5619s
	iters: 200, epoch: 18 | loss: 0.4863606
	speed: 0.0142s/iter; left time: 307.2875s
Epoch: 18 cost time: 4.058651924133301
Epoch: 18, Steps: 263 | Train Loss: 0.4425170 Vali Loss: 1.0172408 Test Loss: 0.4963438
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4250533
	speed: 0.0588s/iter; left time: 1262.8811s
	iters: 200, epoch: 19 | loss: 0.4277598
	speed: 0.0139s/iter; left time: 296.0705s
Epoch: 19 cost time: 4.086669921875
Epoch: 19, Steps: 263 | Train Loss: 0.4425620 Vali Loss: 1.0156034 Test Loss: 0.4962584
Validation loss decreased (1.015893 --> 1.015603).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4631215
	speed: 0.0601s/iter; left time: 1274.9454s
	iters: 200, epoch: 20 | loss: 0.4620730
	speed: 0.0140s/iter; left time: 294.6422s
Epoch: 20 cost time: 4.044746398925781
Epoch: 20, Steps: 263 | Train Loss: 0.4425584 Vali Loss: 1.0163598 Test Loss: 0.4963138
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4510787
	speed: 0.0601s/iter; left time: 1258.1682s
	iters: 200, epoch: 21 | loss: 0.4616819
	speed: 0.0137s/iter; left time: 286.0905s
Epoch: 21 cost time: 4.137222528457642
Epoch: 21, Steps: 263 | Train Loss: 0.4425743 Vali Loss: 1.0167881 Test Loss: 0.4962823
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4752260
	speed: 0.0579s/iter; left time: 1196.8353s
	iters: 200, epoch: 22 | loss: 0.4452600
	speed: 0.0141s/iter; left time: 289.4392s
Epoch: 22 cost time: 4.00033164024353
Epoch: 22, Steps: 263 | Train Loss: 0.4424918 Vali Loss: 1.0160096 Test Loss: 0.4963195
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4529586
	speed: 0.0602s/iter; left time: 1228.9588s
	iters: 200, epoch: 23 | loss: 0.4439026
	speed: 0.0141s/iter; left time: 287.4501s
Epoch: 23 cost time: 4.140554189682007
Epoch: 23, Steps: 263 | Train Loss: 0.4424434 Vali Loss: 1.0159171 Test Loss: 0.4962348
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4437611
	speed: 0.0588s/iter; left time: 1185.6237s
	iters: 200, epoch: 24 | loss: 0.4458958
	speed: 0.0138s/iter; left time: 277.5684s
Epoch: 24 cost time: 4.0891783237457275
Epoch: 24, Steps: 263 | Train Loss: 0.4424521 Vali Loss: 1.0159990 Test Loss: 0.4961503
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4798701
	speed: 0.0597s/iter; left time: 1186.6579s
	iters: 200, epoch: 25 | loss: 0.4359686
	speed: 0.0133s/iter; left time: 263.4747s
Epoch: 25 cost time: 3.931173801422119
Epoch: 25, Steps: 263 | Train Loss: 0.4426860 Vali Loss: 1.0156419 Test Loss: 0.4962458
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3922629
	speed: 0.0592s/iter; left time: 1161.3737s
	iters: 200, epoch: 26 | loss: 0.4137440
	speed: 0.0140s/iter; left time: 273.5570s
Epoch: 26 cost time: 4.03062891960144
Epoch: 26, Steps: 263 | Train Loss: 0.4424470 Vali Loss: 1.0161705 Test Loss: 0.4963915
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4095746
	speed: 0.0594s/iter; left time: 1149.8679s
	iters: 200, epoch: 27 | loss: 0.4155762
	speed: 0.0140s/iter; left time: 269.3707s
Epoch: 27 cost time: 4.0735204219818115
Epoch: 27, Steps: 263 | Train Loss: 0.4424075 Vali Loss: 1.0163766 Test Loss: 0.4963581
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4425640
	speed: 0.0603s/iter; left time: 1151.8415s
	iters: 200, epoch: 28 | loss: 0.4443971
	speed: 0.0144s/iter; left time: 273.7816s
Epoch: 28 cost time: 4.173278570175171
Epoch: 28, Steps: 263 | Train Loss: 0.4422693 Vali Loss: 1.0164961 Test Loss: 0.4962962
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4453994
	speed: 0.0582s/iter; left time: 1097.0683s
	iters: 200, epoch: 29 | loss: 0.4469329
	speed: 0.0147s/iter; left time: 274.9367s
Epoch: 29 cost time: 4.075374364852905
Epoch: 29, Steps: 263 | Train Loss: 0.4424215 Vali Loss: 1.0158557 Test Loss: 0.4962876
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4417741
	speed: 0.0590s/iter; left time: 1095.9231s
	iters: 200, epoch: 30 | loss: 0.4473160
	speed: 0.0138s/iter; left time: 255.5962s
Epoch: 30 cost time: 4.000149726867676
Epoch: 30, Steps: 263 | Train Loss: 0.4424494 Vali Loss: 1.0160525 Test Loss: 0.4964183
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4188685
	speed: 0.0586s/iter; left time: 1073.6630s
	iters: 200, epoch: 31 | loss: 0.4545838
	speed: 0.0141s/iter; left time: 256.1488s
Epoch: 31 cost time: 4.058988571166992
Epoch: 31, Steps: 263 | Train Loss: 0.4423710 Vali Loss: 1.0157114 Test Loss: 0.4963939
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4353130
	speed: 0.0592s/iter; left time: 1068.6967s
	iters: 200, epoch: 32 | loss: 0.4535509
	speed: 0.0138s/iter; left time: 247.5596s
Epoch: 32 cost time: 4.123282194137573
Epoch: 32, Steps: 263 | Train Loss: 0.4422297 Vali Loss: 1.0165957 Test Loss: 0.4963553
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4622149
	speed: 0.0604s/iter; left time: 1074.4193s
	iters: 200, epoch: 33 | loss: 0.4327059
	speed: 0.0136s/iter; left time: 240.5653s
Epoch: 33 cost time: 4.013403654098511
Epoch: 33, Steps: 263 | Train Loss: 0.4424252 Vali Loss: 1.0173746 Test Loss: 0.4964408
EarlyStopping counter: 14 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4361270
	speed: 0.0597s/iter; left time: 1046.5124s
	iters: 200, epoch: 34 | loss: 0.4392720
	speed: 0.0143s/iter; left time: 249.6241s
Epoch: 34 cost time: 4.137846231460571
Epoch: 34, Steps: 263 | Train Loss: 0.4424197 Vali Loss: 1.0163594 Test Loss: 0.4964965
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4882453
	speed: 0.0612s/iter; left time: 1055.5142s
	iters: 200, epoch: 35 | loss: 0.4581671
	speed: 0.0143s/iter; left time: 246.0836s
Epoch: 35 cost time: 4.3104567527771
Epoch: 35, Steps: 263 | Train Loss: 0.4425262 Vali Loss: 1.0165052 Test Loss: 0.4964149
EarlyStopping counter: 16 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4929791
	speed: 0.0581s/iter; left time: 986.7562s
	iters: 200, epoch: 36 | loss: 0.4885789
	speed: 0.0141s/iter; left time: 239.0459s
Epoch: 36 cost time: 4.088602781295776
Epoch: 36, Steps: 263 | Train Loss: 0.4424028 Vali Loss: 1.0165186 Test Loss: 0.4963844
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4531318
	speed: 0.0604s/iter; left time: 1010.5495s
	iters: 200, epoch: 37 | loss: 0.4596972
	speed: 0.0140s/iter; left time: 232.6580s
Epoch: 37 cost time: 4.100521087646484
Epoch: 37, Steps: 263 | Train Loss: 0.4424730 Vali Loss: 1.0168194 Test Loss: 0.4964778
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4185272
	speed: 0.0593s/iter; left time: 975.9198s
	iters: 200, epoch: 38 | loss: 0.4662327
	speed: 0.0141s/iter; left time: 230.0977s
Epoch: 38 cost time: 4.138883829116821
Epoch: 38, Steps: 263 | Train Loss: 0.4423940 Vali Loss: 1.0171053 Test Loss: 0.4965531
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4125016
	speed: 0.0590s/iter; left time: 955.5653s
	iters: 200, epoch: 39 | loss: 0.4517868
	speed: 0.0140s/iter; left time: 225.2241s
Epoch: 39 cost time: 3.9774270057678223
Epoch: 39, Steps: 263 | Train Loss: 0.4424918 Vali Loss: 1.0164678 Test Loss: 0.4964955
EarlyStopping counter: 20 out of 20
Early stopping
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=14, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1580544.0
params:  1890.0
Trainable parameters:  1890
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4701392
	speed: 0.0190s/iter; left time: 497.1311s
	iters: 200, epoch: 1 | loss: 0.5072130
	speed: 0.0144s/iter; left time: 375.6545s
Epoch: 1 cost time: 4.268626689910889
Epoch: 1, Steps: 263 | Train Loss: 0.4943725 Vali Loss: 1.0125945 Test Loss: 0.4946372
Validation loss decreased (inf --> 1.012594).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5472155
	speed: 0.0593s/iter; left time: 1537.4691s
	iters: 200, epoch: 2 | loss: 0.5249954
	speed: 0.0138s/iter; left time: 357.0204s
Epoch: 2 cost time: 4.043368577957153
Epoch: 2, Steps: 263 | Train Loss: 0.4942646 Vali Loss: 1.0138464 Test Loss: 0.4948564
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5015299
	speed: 0.0594s/iter; left time: 1525.5212s
	iters: 200, epoch: 3 | loss: 0.4420886
	speed: 0.0136s/iter; left time: 346.8197s
Epoch: 3 cost time: 4.006728410720825
Epoch: 3, Steps: 263 | Train Loss: 0.4941760 Vali Loss: 1.0137410 Test Loss: 0.4945694
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4972102
	speed: 0.0603s/iter; left time: 1532.2883s
	iters: 200, epoch: 4 | loss: 0.4720115
	speed: 0.0143s/iter; left time: 361.6581s
Epoch: 4 cost time: 4.164973020553589
Epoch: 4, Steps: 263 | Train Loss: 0.4941403 Vali Loss: 1.0142103 Test Loss: 0.4946061
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5120323
	speed: 0.0598s/iter; left time: 1504.9651s
	iters: 200, epoch: 5 | loss: 0.4641261
	speed: 0.0144s/iter; left time: 359.8245s
Epoch: 5 cost time: 4.132272005081177
Epoch: 5, Steps: 263 | Train Loss: 0.4942798 Vali Loss: 1.0145087 Test Loss: 0.4950404
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4814085
	speed: 0.0620s/iter; left time: 1543.2655s
	iters: 200, epoch: 6 | loss: 0.4851333
	speed: 0.0151s/iter; left time: 374.8294s
Epoch: 6 cost time: 4.343127727508545
Epoch: 6, Steps: 263 | Train Loss: 0.4940856 Vali Loss: 1.0127850 Test Loss: 0.4946116
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5058016
	speed: 0.0618s/iter; left time: 1521.2949s
	iters: 200, epoch: 7 | loss: 0.4860496
	speed: 0.0146s/iter; left time: 358.7827s
Epoch: 7 cost time: 4.248997449874878
Epoch: 7, Steps: 263 | Train Loss: 0.4940395 Vali Loss: 1.0139893 Test Loss: 0.4949705
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4822611
	speed: 0.0608s/iter; left time: 1480.3894s
	iters: 200, epoch: 8 | loss: 0.4565973
	speed: 0.0141s/iter; left time: 342.3257s
Epoch: 8 cost time: 4.17212986946106
Epoch: 8, Steps: 263 | Train Loss: 0.4942049 Vali Loss: 1.0146050 Test Loss: 0.4951010
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4710030
	speed: 0.0591s/iter; left time: 1424.0755s
	iters: 200, epoch: 9 | loss: 0.4617944
	speed: 0.0144s/iter; left time: 344.8419s
Epoch: 9 cost time: 4.194580316543579
Epoch: 9, Steps: 263 | Train Loss: 0.4941020 Vali Loss: 1.0132204 Test Loss: 0.4946908
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4988064
	speed: 0.0618s/iter; left time: 1473.3293s
	iters: 200, epoch: 10 | loss: 0.4741756
	speed: 0.0140s/iter; left time: 332.3328s
Epoch: 10 cost time: 4.146857500076294
Epoch: 10, Steps: 263 | Train Loss: 0.4941685 Vali Loss: 1.0146947 Test Loss: 0.4949830
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5148515
	speed: 0.0605s/iter; left time: 1425.1616s
	iters: 200, epoch: 11 | loss: 0.4591713
	speed: 0.0147s/iter; left time: 344.5286s
Epoch: 11 cost time: 4.221211194992065
Epoch: 11, Steps: 263 | Train Loss: 0.4941077 Vali Loss: 1.0150373 Test Loss: 0.4953237
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4716789
	speed: 0.0607s/iter; left time: 1415.6518s
	iters: 200, epoch: 12 | loss: 0.4791707
	speed: 0.0142s/iter; left time: 328.6520s
Epoch: 12 cost time: 4.1384735107421875
Epoch: 12, Steps: 263 | Train Loss: 0.4939398 Vali Loss: 1.0140758 Test Loss: 0.4950458
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5331445
	speed: 0.0583s/iter; left time: 1343.2570s
	iters: 200, epoch: 13 | loss: 0.4987211
	speed: 0.0147s/iter; left time: 337.7815s
Epoch: 13 cost time: 4.137167453765869
Epoch: 13, Steps: 263 | Train Loss: 0.4940972 Vali Loss: 1.0152504 Test Loss: 0.4951736
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4846473
	speed: 0.0583s/iter; left time: 1327.5690s
	iters: 200, epoch: 14 | loss: 0.4704978
	speed: 0.0141s/iter; left time: 319.0144s
Epoch: 14 cost time: 4.104868412017822
Epoch: 14, Steps: 263 | Train Loss: 0.4940209 Vali Loss: 1.0134428 Test Loss: 0.4953265
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4813743
	speed: 0.0615s/iter; left time: 1385.4311s
	iters: 200, epoch: 15 | loss: 0.5041396
	speed: 0.0140s/iter; left time: 313.0692s
Epoch: 15 cost time: 4.156426191329956
Epoch: 15, Steps: 263 | Train Loss: 0.4939835 Vali Loss: 1.0154772 Test Loss: 0.4954993
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4816488
	speed: 0.0593s/iter; left time: 1319.2025s
	iters: 200, epoch: 16 | loss: 0.4690819
	speed: 0.0147s/iter; left time: 325.7318s
Epoch: 16 cost time: 4.133381128311157
Epoch: 16, Steps: 263 | Train Loss: 0.4940336 Vali Loss: 1.0145949 Test Loss: 0.4952435
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5096813
	speed: 0.0587s/iter; left time: 1291.4036s
	iters: 200, epoch: 17 | loss: 0.4951607
	speed: 0.0146s/iter; left time: 319.5742s
Epoch: 17 cost time: 4.141868829727173
Epoch: 17, Steps: 263 | Train Loss: 0.4941812 Vali Loss: 1.0148479 Test Loss: 0.4951322
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5058242
	speed: 0.0599s/iter; left time: 1301.8264s
	iters: 200, epoch: 18 | loss: 0.5524391
	speed: 0.0139s/iter; left time: 301.6218s
Epoch: 18 cost time: 4.17014479637146
Epoch: 18, Steps: 263 | Train Loss: 0.4940191 Vali Loss: 1.0135909 Test Loss: 0.4951462
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4745374
	speed: 0.0598s/iter; left time: 1284.0644s
	iters: 200, epoch: 19 | loss: 0.5388198
	speed: 0.0147s/iter; left time: 313.4189s
Epoch: 19 cost time: 4.199930429458618
Epoch: 19, Steps: 263 | Train Loss: 0.4940808 Vali Loss: 1.0145465 Test Loss: 0.4954840
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4855203
	speed: 0.0591s/iter; left time: 1252.9052s
	iters: 200, epoch: 20 | loss: 0.4886663
	speed: 0.0142s/iter; left time: 298.6664s
Epoch: 20 cost time: 4.072116374969482
Epoch: 20, Steps: 263 | Train Loss: 0.4940512 Vali Loss: 1.0144069 Test Loss: 0.4953126
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5033506
	speed: 0.0584s/iter; left time: 1223.8493s
	iters: 200, epoch: 21 | loss: 0.5845638
	speed: 0.0138s/iter; left time: 287.4002s
Epoch: 21 cost time: 4.200555801391602
Epoch: 21, Steps: 263 | Train Loss: 0.4939935 Vali Loss: 1.0137541 Test Loss: 0.4952551
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4927436411380768, mae:0.4529174268245697, rse:0.6678536534309387, corr:[0.5280072  0.52977896 0.527567   0.5236129  0.52024066 0.51830953
 0.5167997  0.5147429  0.5123449  0.5104616  0.5093429  0.508528
 0.5072681  0.50508416 0.5019756  0.49832693 0.4948255  0.49187654
 0.48931798 0.486682   0.4837223  0.48034668 0.47661066 0.4727254
 0.4690105  0.4655533  0.4625498  0.46014833 0.45841205 0.45703357
 0.45607668 0.45592526 0.45602226 0.45598277 0.4558665  0.45600486
 0.45623443 0.45657778 0.45682338 0.45673236 0.45641115 0.45586482
 0.45548594 0.45547858 0.45557734 0.45560175 0.45547533 0.45518836
 0.45493293 0.45498598 0.4554709  0.45617107 0.45698783 0.45766193
 0.458227   0.4586666  0.4591726  0.4597791  0.4602245  0.460501
 0.46054262 0.46012583 0.45955172 0.45898905 0.45885843 0.45904082
 0.4591467  0.45909092 0.45917735 0.4595916  0.46019113 0.46094507
 0.46183035 0.4625884  0.4632424  0.46368974 0.46423617 0.46501595
 0.46589378 0.4667356  0.4674197  0.46779448 0.4680503  0.4681665
 0.46838796 0.4688104  0.46947744 0.47028708 0.47112894 0.471954
 0.47267646 0.4732541  0.47375864 0.47421455 0.47460306 0.4746402
 0.4741767  0.47346067 0.47270176 0.47215167 0.4719466  0.47179076
 0.47134167 0.47034967 0.46927035 0.46850225 0.46800628 0.4675703
 0.4669778  0.46613535 0.46510828 0.46408194 0.4632082  0.4625247
 0.46184665 0.46109983 0.46011385 0.45889843 0.45758522 0.45636347
 0.45539516 0.45447764 0.45344278 0.4523886  0.45124865 0.45002344
 0.44895872 0.44828272 0.44806117 0.44814503 0.4483101  0.44840503
 0.44826633 0.44806433 0.44784227 0.4476543  0.44752532 0.4474399
 0.44729495 0.44706005 0.44677734 0.44659144 0.44654593 0.44667235
 0.44686174 0.4470757  0.4472661  0.44734043 0.4475456  0.44785467
 0.44825253 0.44887483 0.44951814 0.44995594 0.44998878 0.44978422
 0.44950375 0.44933838 0.4493049  0.44939125 0.4496301  0.44986644
 0.45002398 0.45015353 0.45041078 0.45087045 0.45157978 0.4524473
 0.45329204 0.4539603  0.4544479  0.45495278 0.45543784 0.45593467
 0.45639807 0.45688227 0.45738646 0.45782846 0.45804736 0.45817715
 0.45834845 0.45872793 0.45927703 0.45992622 0.46053168 0.46090338
 0.46100566 0.46101537 0.46121073 0.46174404 0.46248007 0.46316275
 0.4636781  0.46438524 0.46544763 0.46682498 0.46822143 0.46932513
 0.46975562 0.4692818  0.46827808 0.46733674 0.4665043  0.46565056
 0.46457204 0.4631933  0.46154323 0.4599174  0.45843878 0.45725802
 0.45620662 0.4550347  0.45358643 0.4517648  0.44964102 0.44767383
 0.44590008 0.4443687  0.442973   0.44175982 0.44064838 0.43975103
 0.4392691  0.43935618 0.43952262 0.43941677 0.43915775 0.4386326
 0.4380488  0.43782747 0.43797016 0.4381367  0.4381193  0.43771335
 0.43704128 0.43636635 0.43580547 0.43566325 0.43599802 0.4363335
 0.43662658 0.4367286  0.43677324 0.43701392 0.4373182  0.4377529
 0.43818486 0.43854746 0.43877903 0.43905455 0.4392278  0.43942058
 0.43949637 0.43955576 0.4394192  0.43935838 0.43919808 0.43891883
 0.4388123  0.43904594 0.43936774 0.43982628 0.44047478 0.4412327
 0.44222084 0.44309115 0.4438251  0.44463354 0.4456383  0.44661975
 0.44760925 0.4484899  0.449314   0.44996673 0.45043266 0.45074683
 0.45104456 0.45148376 0.4520179  0.4527288  0.45353422 0.45430022
 0.45495424 0.45538843 0.455593   0.4555017  0.4549737  0.45382574
 0.45199627 0.44991705 0.4481391  0.44691065 0.44619146 0.44583818
 0.44565332 0.44525468 0.444848   0.44447893 0.44396755 0.4432046
 0.44218004 0.44100127 0.43985575 0.43880117 0.43792292 0.43709484
 0.43639246 0.43559015 0.43468696 0.43376637 0.43292347 0.43213284
 0.4312979  0.4304043  0.4295871  0.4289657  0.42828396 0.42761454
 0.42696634 0.42636833 0.42588764 0.42555967 0.4253449  0.4252792
 0.4252155  0.4252115  0.425147   0.4249915  0.42471004 0.42430297
 0.42401424 0.42374972 0.42352328 0.42329657 0.4232081  0.42324564
 0.42343038 0.42352653 0.4236923  0.42374682 0.4238336  0.42394492
 0.42413652 0.42454562 0.42500708 0.42538634 0.4255245  0.42541388
 0.42517546 0.4249585  0.42496067 0.42511058 0.42526913 0.42536572
 0.4253528  0.42528808 0.42533785 0.42561725 0.42614725 0.42682177
 0.4274656  0.42801684 0.42855296 0.42912728 0.4299229  0.43087542
 0.43188196 0.43296826 0.4339844  0.43472174 0.43533337 0.43587366
 0.4365845  0.43753093 0.43885005 0.4403823  0.44183317 0.44303107
 0.44385767 0.4443477  0.4446939  0.44504824 0.44544077 0.44571596
 0.4457783  0.44572967 0.44585204 0.4462746  0.44701284 0.44811022
 0.4492053  0.4497458  0.4498117  0.4496102  0.44922668 0.44860375
 0.44773805 0.44669676 0.44552752 0.4444163  0.44342616 0.44260356
 0.44178873 0.4407876  0.43964604 0.43847656 0.4373364  0.4363645
 0.43565464 0.43486407 0.43394247 0.43313453 0.43231928 0.43150568
 0.43091094 0.43057483 0.43061733 0.43081173 0.43087128 0.43079805
 0.43058458 0.43042055 0.4302979  0.4302427  0.4302168  0.4301325
 0.42985806 0.42943627 0.42875034 0.4281856  0.4278465  0.42772156
 0.4276954  0.42769802 0.42789793 0.42836902 0.42890066 0.42934924
 0.42958006 0.42969534 0.4298017  0.43006837 0.4304893  0.4309303
 0.43117344 0.43121818 0.43092582 0.43044707 0.429938   0.42967215
 0.4297166  0.43014956 0.4307261  0.43131748 0.43173274 0.4320581
 0.432365   0.43262854 0.43296528 0.43353543 0.43425196 0.43498787
 0.43576014 0.43667972 0.43759096 0.4384232  0.43909746 0.4395916
 0.43999755 0.44042346 0.44109884 0.4419801  0.44296256 0.4438978
 0.44456184 0.44492435 0.44504702 0.44500345 0.4446881  0.44397816
 0.44276905 0.4414792  0.44036776 0.43977106 0.4396229  0.4399459
 0.44024172 0.44002235 0.43941706 0.43868345 0.4378847  0.4370352
 0.43605834 0.4348482  0.43336365 0.43170097 0.4300407  0.42855766
 0.42732614 0.42638522 0.4256012  0.42468035 0.42365026 0.4224846
 0.4213261  0.42015842 0.41904652 0.41810247 0.41727224 0.41657427
 0.41594613 0.415544   0.41535977 0.4153323  0.4153052  0.41528115
 0.4152071  0.4152831  0.4155831  0.41579524 0.41583782 0.41566834
 0.41528597 0.41488525 0.41443545 0.4140858  0.41387278 0.41386026
 0.41383076 0.4137014  0.4135279  0.4134585  0.41360697 0.41392016
 0.41429046 0.41444328 0.41440532 0.41425475 0.41408315 0.41389686
 0.4139236  0.41402003 0.41399264 0.41376346 0.413395   0.41320446
 0.41334137 0.4137556  0.4142758  0.41471276 0.41497865 0.41523662
 0.41557837 0.41595897 0.41650626 0.4172494  0.41811305 0.41893762
 0.41975084 0.42072472 0.42183262 0.4228784  0.42374298 0.42432892
 0.42468715 0.42511618 0.4257154  0.4266572  0.42787588 0.42906344
 0.42987394 0.43010235 0.42986453 0.42937145 0.4287464  0.42782843
 0.42636168 0.42455444 0.4230805  0.42199665 0.4216964  0.42215514
 0.42295226 0.4233831  0.423306   0.42295927 0.422496   0.42190987
 0.42111695 0.42003512 0.4186073  0.4170448  0.41560417 0.41431296
 0.41319278 0.4122146  0.41132757 0.41040015 0.40941414 0.4084124
 0.40750852 0.4067118  0.40605906 0.40552786 0.40487087 0.4040296
 0.4030536  0.40226898 0.40184137 0.40187603 0.40216678 0.40252867
 0.40260494 0.4023765  0.4019935  0.40160155 0.4014767  0.40146482
 0.40145224 0.4013058  0.4008869  0.4004309  0.40021288 0.40032265
 0.4004829  0.4005219  0.40048495 0.40024328 0.4000277  0.400069
 0.40024894 0.4003864  0.40049314 0.4004257  0.4003173  0.4002259
 0.4002686  0.40042967 0.4005127  0.40019885 0.39963782 0.39912474
 0.39895576 0.39926085 0.3999203  0.4007017  0.40121317 0.4014625
 0.40160328 0.40178102 0.4021308  0.4028269  0.4036703  0.40451777
 0.40520805 0.4059274  0.40676695 0.4077158  0.4086517  0.40943718
 0.40999815 0.41042614 0.41106445 0.41206795 0.41337767 0.41468695
 0.41553417 0.41565317 0.41510162 0.4142118  0.41334295 0.41261497
 0.41178697 0.41081703 0.40985727 0.40908405 0.40901977 0.4099239
 0.41133243 0.41240844 0.4128242  0.41252562 0.4117209  0.41076106
 0.4099075  0.40912238 0.40818506 0.4068214  0.4049894  0.40290737
 0.40113667 0.39999843 0.39945823 0.3990701  0.39826232 0.39683113
 0.39494145 0.39312702 0.3918715  0.39126486 0.39078838 0.3897189
 0.38782993 0.3855936  0.38418785 0.38404948 0.38445637 0.3844811
 0.38332906 0.38126707 0.37965092 0.37980822 0.3816346  0.38333747
 0.38287652 0.38001552 0.377279   0.378931   0.38578254 0.3908306 ]
