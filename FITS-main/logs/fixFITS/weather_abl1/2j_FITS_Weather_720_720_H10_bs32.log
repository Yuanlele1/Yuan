Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13171200.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7202252
	speed: 0.0732s/iter; left time: 4041.7941s
	iters: 200, epoch: 1 | loss: 0.5931120
	speed: 0.0498s/iter; left time: 2741.8504s
	iters: 300, epoch: 1 | loss: 0.5447056
	speed: 0.0457s/iter; left time: 2513.9722s
	iters: 400, epoch: 1 | loss: 0.4701969
	speed: 0.0501s/iter; left time: 2752.0031s
	iters: 500, epoch: 1 | loss: 0.4712469
	speed: 0.0483s/iter; left time: 2646.3885s
Epoch: 1 cost time: 28.975035667419434
Epoch: 1, Steps: 553 | Train Loss: 0.6188832 Vali Loss: 0.6841549 Test Loss: 0.3354676
Validation loss decreased (inf --> 0.684155).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5330241
	speed: 0.1903s/iter; left time: 10400.8490s
	iters: 200, epoch: 2 | loss: 0.4240173
	speed: 0.0537s/iter; left time: 2930.1025s
	iters: 300, epoch: 2 | loss: 0.4502455
	speed: 0.0494s/iter; left time: 2687.3785s
	iters: 400, epoch: 2 | loss: 0.4106563
	speed: 0.0492s/iter; left time: 2672.7240s
	iters: 500, epoch: 2 | loss: 0.4453436
	speed: 0.0484s/iter; left time: 2626.4595s
Epoch: 2 cost time: 27.764416456222534
Epoch: 2, Steps: 553 | Train Loss: 0.4293247 Vali Loss: 0.6300154 Test Loss: 0.3213295
Validation loss decreased (0.684155 --> 0.630015).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3722277
	speed: 0.1758s/iter; left time: 9508.2758s
	iters: 200, epoch: 3 | loss: 0.3702281
	speed: 0.0524s/iter; left time: 2827.3559s
	iters: 300, epoch: 3 | loss: 0.3536744
	speed: 0.0458s/iter; left time: 2470.7916s
	iters: 400, epoch: 3 | loss: 0.3943011
	speed: 0.0483s/iter; left time: 2598.7748s
	iters: 500, epoch: 3 | loss: 0.4013908
	speed: 0.0414s/iter; left time: 2222.1043s
Epoch: 3 cost time: 25.458580017089844
Epoch: 3, Steps: 553 | Train Loss: 0.3743442 Vali Loss: 0.6074721 Test Loss: 0.3154045
Validation loss decreased (0.630015 --> 0.607472).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2762245
	speed: 0.2345s/iter; left time: 12553.1200s
	iters: 200, epoch: 4 | loss: 0.3922020
	speed: 0.0646s/iter; left time: 3455.0110s
	iters: 300, epoch: 4 | loss: 0.3419172
	speed: 0.0534s/iter; left time: 2846.5482s
	iters: 400, epoch: 4 | loss: 0.3661138
	speed: 0.0340s/iter; left time: 1810.6678s
	iters: 500, epoch: 4 | loss: 0.2491679
	speed: 0.0391s/iter; left time: 2078.1183s
Epoch: 4 cost time: 28.845929861068726
Epoch: 4, Steps: 553 | Train Loss: 0.3509354 Vali Loss: 0.5990517 Test Loss: 0.3130730
Validation loss decreased (0.607472 --> 0.599052).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2937637
	speed: 0.1844s/iter; left time: 9769.5937s
	iters: 200, epoch: 5 | loss: 0.4270345
	speed: 0.0373s/iter; left time: 1970.3185s
	iters: 300, epoch: 5 | loss: 0.3291247
	speed: 0.0427s/iter; left time: 2255.2732s
	iters: 400, epoch: 5 | loss: 0.2904537
	speed: 0.0361s/iter; left time: 1899.4858s
	iters: 500, epoch: 5 | loss: 0.3530932
	speed: 0.0344s/iter; left time: 1807.9084s
Epoch: 5 cost time: 20.93462562561035
Epoch: 5, Steps: 553 | Train Loss: 0.3406932 Vali Loss: 0.5970029 Test Loss: 0.3119583
Validation loss decreased (0.599052 --> 0.597003).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3283975
	speed: 0.1432s/iter; left time: 7508.7715s
	iters: 200, epoch: 6 | loss: 0.3146659
	speed: 0.0424s/iter; left time: 2216.8790s
	iters: 300, epoch: 6 | loss: 0.3002512
	speed: 0.0519s/iter; left time: 2710.0261s
	iters: 400, epoch: 6 | loss: 0.3331681
	speed: 0.0598s/iter; left time: 3115.8107s
	iters: 500, epoch: 6 | loss: 0.3336187
	speed: 0.0479s/iter; left time: 2493.1938s
Epoch: 6 cost time: 28.165913105010986
Epoch: 6, Steps: 553 | Train Loss: 0.3364299 Vali Loss: 0.5967128 Test Loss: 0.3114644
Validation loss decreased (0.597003 --> 0.596713).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2986621
	speed: 0.2394s/iter; left time: 12422.9736s
	iters: 200, epoch: 7 | loss: 0.3445279
	speed: 0.0510s/iter; left time: 2643.0847s
	iters: 300, epoch: 7 | loss: 0.2910188
	speed: 0.0344s/iter; left time: 1777.5405s
	iters: 400, epoch: 7 | loss: 0.3354794
	speed: 0.0411s/iter; left time: 2117.7810s
	iters: 500, epoch: 7 | loss: 0.3329073
	speed: 0.0441s/iter; left time: 2270.0742s
Epoch: 7 cost time: 24.888266563415527
Epoch: 7, Steps: 553 | Train Loss: 0.3348029 Vali Loss: 0.5981883 Test Loss: 0.3114714
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3098997
	speed: 0.1834s/iter; left time: 9415.4930s
	iters: 200, epoch: 8 | loss: 0.3922050
	speed: 0.0581s/iter; left time: 2978.9563s
	iters: 300, epoch: 8 | loss: 0.3074561
	speed: 0.0543s/iter; left time: 2777.0869s
	iters: 400, epoch: 8 | loss: 0.2968425
	speed: 0.0469s/iter; left time: 2392.3103s
	iters: 500, epoch: 8 | loss: 0.3337803
	speed: 0.0604s/iter; left time: 3077.0132s
Epoch: 8 cost time: 30.61393165588379
Epoch: 8, Steps: 553 | Train Loss: 0.3344999 Vali Loss: 0.5992913 Test Loss: 0.3112700
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3724919
	speed: 0.1501s/iter; left time: 7623.5405s
	iters: 200, epoch: 9 | loss: 0.3443364
	speed: 0.0543s/iter; left time: 2752.0576s
	iters: 300, epoch: 9 | loss: 0.3545004
	speed: 0.0476s/iter; left time: 2407.8894s
	iters: 400, epoch: 9 | loss: 0.2519748
	speed: 0.0381s/iter; left time: 1925.0096s
	iters: 500, epoch: 9 | loss: 0.2783205
	speed: 0.0344s/iter; left time: 1734.3994s
Epoch: 9 cost time: 23.406606197357178
Epoch: 9, Steps: 553 | Train Loss: 0.3343014 Vali Loss: 0.5988134 Test Loss: 0.3112076
EarlyStopping counter: 3 out of 3
Early stopping
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13171200.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6273547
	speed: 0.0412s/iter; left time: 2272.7704s
	iters: 200, epoch: 1 | loss: 0.4728915
	speed: 0.0452s/iter; left time: 2489.6926s
	iters: 300, epoch: 1 | loss: 0.5305561
	speed: 0.0377s/iter; left time: 2072.1865s
	iters: 400, epoch: 1 | loss: 0.5131491
	speed: 0.0436s/iter; left time: 2391.7136s
	iters: 500, epoch: 1 | loss: 0.4802476
	speed: 0.0804s/iter; left time: 4408.1069s
Epoch: 1 cost time: 27.126569032669067
Epoch: 1, Steps: 553 | Train Loss: 0.5637794 Vali Loss: 0.5974502 Test Loss: 0.3104081
Validation loss decreased (inf --> 0.597450).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4879446
	speed: 0.2484s/iter; left time: 13572.4025s
	iters: 200, epoch: 2 | loss: 0.5826015
	speed: 0.0719s/iter; left time: 3919.5198s
	iters: 300, epoch: 2 | loss: 0.5232917
	speed: 0.0697s/iter; left time: 3792.8576s
	iters: 400, epoch: 2 | loss: 0.5583926
	speed: 0.0411s/iter; left time: 2234.0868s
	iters: 500, epoch: 2 | loss: 0.5032068
	speed: 0.0360s/iter; left time: 1953.5638s
Epoch: 2 cost time: 31.987619400024414
Epoch: 2, Steps: 553 | Train Loss: 0.5624277 Vali Loss: 0.5966671 Test Loss: 0.3099993
Validation loss decreased (0.597450 --> 0.596667).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4347653
	speed: 0.1662s/iter; left time: 8988.8861s
	iters: 200, epoch: 3 | loss: 0.5899773
	speed: 0.0567s/iter; left time: 3061.0989s
	iters: 300, epoch: 3 | loss: 0.5813743
	speed: 0.0378s/iter; left time: 2039.1652s
	iters: 400, epoch: 3 | loss: 0.6282897
	speed: 0.0433s/iter; left time: 2327.9986s
	iters: 500, epoch: 3 | loss: 0.4874299
	speed: 0.0581s/iter; left time: 3118.6391s
Epoch: 3 cost time: 26.109662532806396
Epoch: 3, Steps: 553 | Train Loss: 0.5620434 Vali Loss: 0.5944119 Test Loss: 0.3097011
Validation loss decreased (0.596667 --> 0.594412).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5646625
	speed: 0.2052s/iter; left time: 10988.4711s
	iters: 200, epoch: 4 | loss: 0.5289684
	speed: 0.0509s/iter; left time: 2722.2960s
	iters: 300, epoch: 4 | loss: 0.4748955
	speed: 0.0435s/iter; left time: 2322.6393s
	iters: 400, epoch: 4 | loss: 0.5177541
	speed: 0.0451s/iter; left time: 2403.4715s
	iters: 500, epoch: 4 | loss: 0.4748061
	speed: 0.0427s/iter; left time: 2267.1206s
Epoch: 4 cost time: 26.455912351608276
Epoch: 4, Steps: 553 | Train Loss: 0.5615304 Vali Loss: 0.5952521 Test Loss: 0.3095611
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5594353
	speed: 0.1406s/iter; left time: 7450.3615s
	iters: 200, epoch: 5 | loss: 0.5337292
	speed: 0.0350s/iter; left time: 1851.0139s
	iters: 300, epoch: 5 | loss: 0.5224032
	speed: 0.0413s/iter; left time: 2180.1917s
	iters: 400, epoch: 5 | loss: 0.5951176
	speed: 0.0590s/iter; left time: 3110.2018s
	iters: 500, epoch: 5 | loss: 0.4914561
	speed: 0.0635s/iter; left time: 3338.0656s
Epoch: 5 cost time: 26.277936220169067
Epoch: 5, Steps: 553 | Train Loss: 0.5611635 Vali Loss: 0.5942202 Test Loss: 0.3088672
Validation loss decreased (0.594412 --> 0.594220).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6256269
	speed: 0.2781s/iter; left time: 14581.3797s
	iters: 200, epoch: 6 | loss: 0.5356037
	speed: 0.0818s/iter; left time: 4278.6021s
	iters: 300, epoch: 6 | loss: 0.4653606
	speed: 0.0769s/iter; left time: 4017.1780s
	iters: 400, epoch: 6 | loss: 0.5137933
	speed: 0.0409s/iter; left time: 2132.6998s
	iters: 500, epoch: 6 | loss: 0.6666307
	speed: 0.0360s/iter; left time: 1875.6001s
Epoch: 6 cost time: 34.83608627319336
Epoch: 6, Steps: 553 | Train Loss: 0.5610208 Vali Loss: 0.5940511 Test Loss: 0.3086952
Validation loss decreased (0.594220 --> 0.594051).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5181037
	speed: 0.1693s/iter; left time: 8782.8061s
	iters: 200, epoch: 7 | loss: 0.4389466
	speed: 0.0386s/iter; left time: 1999.3956s
	iters: 300, epoch: 7 | loss: 0.6345263
	speed: 0.0333s/iter; left time: 1723.4202s
	iters: 400, epoch: 7 | loss: 0.6347131
	speed: 0.0452s/iter; left time: 2332.7239s
	iters: 500, epoch: 7 | loss: 0.5339280
	speed: 0.0324s/iter; left time: 1667.2980s
Epoch: 7 cost time: 21.032497882843018
Epoch: 7, Steps: 553 | Train Loss: 0.5606775 Vali Loss: 0.5932879 Test Loss: 0.3086995
Validation loss decreased (0.594051 --> 0.593288).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6546559
	speed: 0.1774s/iter; left time: 9103.7833s
	iters: 200, epoch: 8 | loss: 0.5420367
	speed: 0.0477s/iter; left time: 2442.1217s
	iters: 300, epoch: 8 | loss: 0.4927912
	speed: 0.0454s/iter; left time: 2321.2906s
	iters: 400, epoch: 8 | loss: 0.7580402
	speed: 0.0486s/iter; left time: 2480.6756s
	iters: 500, epoch: 8 | loss: 0.5524758
	speed: 0.0533s/iter; left time: 2713.4962s
Epoch: 8 cost time: 26.73402452468872
Epoch: 8, Steps: 553 | Train Loss: 0.5605840 Vali Loss: 0.5937430 Test Loss: 0.3085510
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5873172
	speed: 0.2451s/iter; left time: 12447.6689s
	iters: 200, epoch: 9 | loss: 0.5335498
	speed: 0.0469s/iter; left time: 2378.4238s
	iters: 300, epoch: 9 | loss: 0.5414394
	speed: 0.0464s/iter; left time: 2345.2569s
	iters: 400, epoch: 9 | loss: 0.4867936
	speed: 0.0481s/iter; left time: 2425.8736s
	iters: 500, epoch: 9 | loss: 0.5091698
	speed: 0.0495s/iter; left time: 2494.3751s
Epoch: 9 cost time: 25.81793737411499
Epoch: 9, Steps: 553 | Train Loss: 0.5603644 Vali Loss: 0.5940610 Test Loss: 0.3082516
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5066352
	speed: 0.1674s/iter; left time: 8408.8885s
	iters: 200, epoch: 10 | loss: 0.4596452
	speed: 0.0485s/iter; left time: 2432.6967s
	iters: 300, epoch: 10 | loss: 0.5189863
	speed: 0.0676s/iter; left time: 3381.6194s
	iters: 400, epoch: 10 | loss: 0.5611688
	speed: 0.0603s/iter; left time: 3009.5292s
	iters: 500, epoch: 10 | loss: 0.4498945
	speed: 0.0485s/iter; left time: 2418.6649s
Epoch: 10 cost time: 30.959023475646973
Epoch: 10, Steps: 553 | Train Loss: 0.5602675 Vali Loss: 0.5933012 Test Loss: 0.3081611
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3079966604709625, mae:0.33013805747032166, rse:0.7303063273429871, corr:[0.46968055 0.472566   0.47260454 0.47148758 0.47013542 0.4691622
 0.46878353 0.468805   0.46886104 0.4685788  0.46781003 0.46673068
 0.4654962  0.464383   0.4635926  0.4631381  0.462962   0.46278945
 0.46242973 0.4617264  0.460719   0.459529   0.45834953 0.4571555
 0.45599943 0.45485246 0.45368582 0.45247585 0.4513155  0.45031074
 0.44963706 0.44933143 0.449392   0.44958183 0.44974995 0.4496907
 0.4494467  0.44895953 0.44826868 0.44746816 0.44669902 0.4461075
 0.44567767 0.4453193  0.44495282 0.4444827  0.44390044 0.44322273
 0.44234717 0.4414771  0.44075897 0.44018936 0.439812   0.4395395
 0.43926525 0.43887237 0.4383599  0.43771124 0.4370244  0.43633878
 0.43576595 0.435352   0.4351369  0.43504986 0.43501043 0.43495318
 0.4347837  0.43449053 0.4341108  0.43368986 0.433322   0.43295115
 0.432699   0.4325408  0.432405   0.43225056 0.43205553 0.43184343
 0.43158343 0.4312636  0.43095413 0.4306503  0.43037754 0.43018898
 0.43010628 0.43008575 0.43008742 0.4299706  0.42982042 0.4296069
 0.42939827 0.42923275 0.42906106 0.42891365 0.42878607 0.42864162
 0.42846504 0.42824936 0.42798907 0.42773357 0.42749712 0.42730474
 0.42718834 0.4271237  0.42706648 0.42697063 0.42677853 0.4265171
 0.42613193 0.42569405 0.4252125  0.42468673 0.4241528  0.42372015
 0.42339274 0.42315295 0.42303458 0.42303047 0.4230777  0.42306685
 0.42302367 0.42293623 0.42276156 0.42252204 0.42220503 0.42182705
 0.42139685 0.42092228 0.42047283 0.4200795  0.4197481  0.4195069
 0.41937315 0.41930804 0.41928932 0.41923255 0.41910952 0.4189112
 0.4185345  0.4181238  0.4177403  0.41743308 0.41715097 0.41687816
 0.41660652 0.4163288  0.41604826 0.41576618 0.41541535 0.41493267
 0.41435328 0.4137922  0.41326162 0.4127602  0.41236222 0.41200313
 0.4116809  0.4112185  0.41066518 0.41006246 0.40949044 0.40899494
 0.40859058 0.40830302 0.40806204 0.40784067 0.40752766 0.40709746
 0.4065572  0.40593317 0.40524417 0.40456173 0.40395644 0.40349224
 0.4031187  0.4028227  0.40253484 0.40222877 0.4018534  0.4013731
 0.40085724 0.40029338 0.39971283 0.3991518  0.398674   0.39826903
 0.39791727 0.39753926 0.3971272  0.39665854 0.39612335 0.3955573
 0.39504534 0.39453143 0.3940932  0.39371708 0.39337838 0.3930584
 0.39269644 0.39228374 0.3918632  0.3913726  0.3908363  0.3902598
 0.38975078 0.3893287  0.38899788 0.38874292 0.38845396 0.38813564
 0.3877299  0.3872348  0.386659   0.38606933 0.38549957 0.38496426
 0.38445732 0.3840959  0.3837702  0.38341725 0.38303608 0.3826681
 0.38229123 0.38189697 0.38152543 0.3811932  0.38094023 0.38082427
 0.38079405 0.3808178  0.3808171  0.38068777 0.38036746 0.37981987
 0.37909415 0.37829453 0.37746578 0.3767166  0.37606677 0.37559554
 0.37535018 0.37527078 0.3751887  0.37509108 0.37493753 0.37466627
 0.37425423 0.37378496 0.37328276 0.3728226  0.37243494 0.37215772
 0.3719507  0.37172797 0.37151203 0.37115642 0.3707161  0.37022674
 0.36979792 0.36945975 0.36910364 0.36879325 0.36848685 0.36826897
 0.3680916  0.36790878 0.36769593 0.36745983 0.3671968  0.36686736
 0.36649588 0.36614603 0.36576638 0.36532456 0.36489645 0.36447948
 0.36405462 0.36355925 0.36305735 0.36254492 0.36202216 0.36155754
 0.36119398 0.36084926 0.36055613 0.3602764  0.35996738 0.35957336
 0.35912654 0.3586146  0.35800594 0.35727516 0.35655135 0.35587868
 0.35523906 0.35468006 0.35419467 0.35376543 0.35329646 0.3528258
 0.35220066 0.3514693  0.35061875 0.3497142  0.34879583 0.34791556
 0.34716105 0.34655264 0.3460912  0.34569088 0.34536955 0.3450274
 0.34463105 0.34421092 0.34372193 0.3431562  0.34255594 0.34191215
 0.34127325 0.3406238  0.33998695 0.33940357 0.33890143 0.33840618
 0.33795086 0.33753833 0.33710656 0.33670765 0.33637908 0.3360641
 0.3357845  0.33550426 0.3352186  0.33497334 0.33470726 0.33445194
 0.3341695  0.33387116 0.33359095 0.33328643 0.33294562 0.33261347
 0.33228004 0.33186612 0.33139554 0.33089367 0.33042002 0.32999453
 0.3296948  0.3295017  0.3294531  0.32948983 0.3295735  0.32962894
 0.32960296 0.32946625 0.3292215  0.3288265  0.32832342 0.32778838
 0.32724372 0.32677853 0.32649425 0.3263235  0.32626    0.32625517
 0.32623076 0.3261611  0.32603338 0.32579806 0.32549828 0.3251708
 0.32486713 0.3245926  0.3243758  0.32422355 0.3240958  0.32393223
 0.32369736 0.32339916 0.32299647 0.32252264 0.3220436  0.32151622
 0.32100528 0.3205358  0.32015753 0.31984168 0.31960425 0.3194345
 0.3192778  0.31921175 0.3191948  0.3192339  0.31930736 0.31938624
 0.31942242 0.31938466 0.31921858 0.3189143  0.3184841  0.31793758
 0.3173914  0.31684542 0.31639153 0.3160393  0.3157867  0.31563926
 0.3155918  0.31559548 0.3156014  0.31552714 0.31541592 0.31528988
 0.31514797 0.31498602 0.31484324 0.31474915 0.3146286  0.31449413
 0.31428787 0.31404287 0.31368378 0.31326285 0.31274202 0.31216326
 0.31157154 0.31106213 0.31060746 0.31018716 0.3098191  0.30956137
 0.30934286 0.30913275 0.3089313  0.3086911  0.30836138 0.3079335
 0.30738202 0.30674103 0.30606493 0.30534542 0.30460522 0.30374444
 0.3029082  0.30213806 0.30145347 0.3008983  0.300357   0.29981163
 0.29924026 0.29859427 0.29789656 0.29717064 0.29649252 0.2958585
 0.29535574 0.29498416 0.2947087  0.29445064 0.29412964 0.293769
 0.29333276 0.29285285 0.29238528 0.29195428 0.29158914 0.29128754
 0.29108027 0.29093236 0.2907767  0.29059973 0.29041752 0.2902011
 0.2899504  0.2896692  0.2893448  0.2889886  0.28861526 0.28820053
 0.2878172  0.28747165 0.28716302 0.2869302  0.2868015  0.28680173
 0.2868486  0.28693196 0.28702354 0.2870613  0.2870128  0.2868317
 0.2865536  0.28622207 0.28585118 0.28549984 0.28519717 0.28499654
 0.28490523 0.2849144  0.28496608 0.28499356 0.285002   0.28495622
 0.2848091  0.28457803 0.28432167 0.283972   0.28361365 0.2832417
 0.28291965 0.2826417  0.28242305 0.28222647 0.28206742 0.28194845
 0.2818553  0.2817686  0.28164005 0.28146464 0.28120783 0.280837
 0.28038022 0.27985716 0.2793041  0.278831   0.27845103 0.2782111
 0.27807778 0.27802196 0.27797303 0.27788374 0.27773747 0.27745554
 0.277062   0.27658707 0.27606577 0.27556276 0.27510652 0.2746975
 0.2743257  0.27399626 0.27362382 0.27320683 0.2727392  0.2722039
 0.2716576  0.27118784 0.2708611  0.27068025 0.27065766 0.27069363
 0.2707232  0.2706717  0.27050158 0.2701443  0.269653   0.26903665
 0.26838854 0.2677361  0.2671632  0.26672766 0.2663757  0.26608753
 0.26584843 0.26563287 0.26539844 0.2651351  0.26484606 0.2645813
 0.2643342  0.26410893 0.26388916 0.26360783 0.26320124 0.26259026
 0.26202282 0.26117343 0.2601451  0.2594341  0.25877917 0.25825098
 0.25784373 0.25754082 0.2572737  0.2570412  0.25674787 0.25642666
 0.25604948 0.25562617 0.2551533  0.25467905 0.25420147 0.253759
 0.2533061  0.25284025 0.25238234 0.25194296 0.25153708 0.2511769
 0.25088656 0.25063086 0.25040522 0.25016925 0.24990328 0.24960637
 0.24919955 0.2487319  0.2481499  0.24748784 0.24682972 0.24621089
 0.2456888  0.24532804 0.24507785 0.24490923 0.24478854 0.24457693
 0.24428396 0.24394138 0.24352151 0.2430578  0.24260756 0.24224047
 0.24190865 0.24164134 0.24141185 0.24118473 0.24098663 0.24073415
 0.24045381 0.24013032 0.23983024 0.23955216 0.23932154 0.23917404
 0.23910059 0.2390679  0.23902929 0.23894246 0.23882772 0.2386771
 0.23854274 0.2384308  0.238348   0.23824859 0.23817825 0.23811609
 0.23806876 0.23796079 0.2378554  0.23767035 0.237516   0.23734836
 0.23722543 0.23710261 0.23696402 0.23677835 0.23656248 0.23634045
 0.23615868 0.23602793 0.23597349 0.23599496 0.2361176  0.23630317
 0.23647746 0.23656097 0.23644526 0.23612568 0.2356291  0.23497434
 0.23431161 0.2337403  0.23328972 0.23300508 0.23286247 0.23283309
 0.23281392 0.23274738 0.23260315 0.23232985 0.23199001 0.23159549
 0.23119622 0.23085229 0.23063755 0.23052226 0.23045033 0.23035127
 0.23015127 0.22977856 0.2293245  0.22886966 0.22847408 0.22818497
 0.22806637 0.2280693  0.2281029  0.22805522 0.22785382 0.2274297
 0.22688894 0.22634768 0.22590597 0.22568603 0.22572887 0.22599255
 0.226283   0.22637609 0.22611053 0.22545552 0.2243613  0.2231446
 0.22209068 0.22171976 0.22199447 0.22266918 0.22297913 0.22163126]
