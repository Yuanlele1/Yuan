Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=514, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Weather_720_j720_H_FITS_custom_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=82, out_features=164, bias=True)
    (1): Linear(in_features=82, out_features=164, bias=True)
    (2): Linear(in_features=82, out_features=164, bias=True)
    (3): Linear(in_features=82, out_features=164, bias=True)
    (4): Linear(in_features=82, out_features=164, bias=True)
    (5): Linear(in_features=82, out_features=164, bias=True)
    (6): Linear(in_features=82, out_features=164, bias=True)
    (7): Linear(in_features=82, out_features=164, bias=True)
    (8): Linear(in_features=82, out_features=164, bias=True)
    (9): Linear(in_features=82, out_features=164, bias=True)
    (10): Linear(in_features=82, out_features=164, bias=True)
    (11): Linear(in_features=82, out_features=164, bias=True)
    (12): Linear(in_features=82, out_features=164, bias=True)
    (13): Linear(in_features=82, out_features=164, bias=True)
    (14): Linear(in_features=82, out_features=164, bias=True)
    (15): Linear(in_features=82, out_features=164, bias=True)
    (16): Linear(in_features=82, out_features=164, bias=True)
    (17): Linear(in_features=82, out_features=164, bias=True)
    (18): Linear(in_features=82, out_features=164, bias=True)
    (19): Linear(in_features=82, out_features=164, bias=True)
    (20): Linear(in_features=82, out_features=164, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  18074112.0
params:  285852.0
Trainable parameters:  285852
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8708802
	speed: 0.0411s/iter; left time: 2269.1618s
	iters: 200, epoch: 1 | loss: 0.8737884
	speed: 0.0389s/iter; left time: 2144.1016s
	iters: 300, epoch: 1 | loss: 0.7521138
	speed: 0.0412s/iter; left time: 2266.7537s
	iters: 400, epoch: 1 | loss: 0.6392940
	speed: 0.0409s/iter; left time: 2244.4172s
	iters: 500, epoch: 1 | loss: 0.7067240
	speed: 0.0444s/iter; left time: 2435.3068s
Epoch: 1 cost time: 23.06031823158264
Epoch: 1, Steps: 553 | Train Loss: 0.6844506 Vali Loss: 0.6178776 Test Loss: 0.3212352
Validation loss decreased (inf --> 0.617878).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5430632
	speed: 0.1481s/iter; left time: 8091.3433s
	iters: 200, epoch: 2 | loss: 0.5706004
	speed: 0.0374s/iter; left time: 2037.5593s
	iters: 300, epoch: 2 | loss: 0.4619328
	speed: 0.0394s/iter; left time: 2144.2805s
	iters: 400, epoch: 2 | loss: 0.5242103
	speed: 0.0378s/iter; left time: 2052.1506s
	iters: 500, epoch: 2 | loss: 0.7692135
	speed: 0.0373s/iter; left time: 2025.8318s
Epoch: 2 cost time: 21.35950469970703
Epoch: 2, Steps: 553 | Train Loss: 0.5750957 Vali Loss: 0.6050775 Test Loss: 0.3151825
Validation loss decreased (0.617878 --> 0.605078).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4999720
	speed: 0.1471s/iter; left time: 7955.8627s
	iters: 200, epoch: 3 | loss: 0.8393793
	speed: 0.0376s/iter; left time: 2031.2170s
	iters: 300, epoch: 3 | loss: 0.4704739
	speed: 0.0382s/iter; left time: 2061.0370s
	iters: 400, epoch: 3 | loss: 0.6808656
	speed: 0.0350s/iter; left time: 1882.9086s
	iters: 500, epoch: 3 | loss: 0.6284756
	speed: 0.0372s/iter; left time: 1996.0372s
Epoch: 3 cost time: 20.90471649169922
Epoch: 3, Steps: 553 | Train Loss: 0.5665131 Vali Loss: 0.6024553 Test Loss: 0.3129127
Validation loss decreased (0.605078 --> 0.602455).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6675315
	speed: 0.1666s/iter; left time: 8920.4312s
	iters: 200, epoch: 4 | loss: 0.4720511
	speed: 0.0375s/iter; left time: 2006.0338s
	iters: 300, epoch: 4 | loss: 0.5657929
	speed: 0.0417s/iter; left time: 2225.8223s
	iters: 400, epoch: 4 | loss: 0.4558062
	speed: 0.0396s/iter; left time: 2108.8717s
	iters: 500, epoch: 4 | loss: 0.5744274
	speed: 0.0401s/iter; left time: 2130.2845s
Epoch: 4 cost time: 22.282268524169922
Epoch: 4, Steps: 553 | Train Loss: 0.5646126 Vali Loss: 0.5994390 Test Loss: 0.3113919
Validation loss decreased (0.602455 --> 0.599439).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5798728
	speed: 0.1862s/iter; left time: 9865.2987s
	iters: 200, epoch: 5 | loss: 0.5595216
	speed: 0.0441s/iter; left time: 2333.4089s
	iters: 300, epoch: 5 | loss: 0.5459728
	speed: 0.0384s/iter; left time: 2029.2266s
	iters: 400, epoch: 5 | loss: 0.6475694
	speed: 0.0385s/iter; left time: 2028.8620s
	iters: 500, epoch: 5 | loss: 0.5592666
	speed: 0.0377s/iter; left time: 1983.3539s
Epoch: 5 cost time: 23.771790742874146
Epoch: 5, Steps: 553 | Train Loss: 0.5634041 Vali Loss: 0.5984979 Test Loss: 0.3108795
Validation loss decreased (0.599439 --> 0.598498).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4772206
	speed: 0.1634s/iter; left time: 8565.7522s
	iters: 200, epoch: 6 | loss: 0.6779094
	speed: 0.0401s/iter; left time: 2097.8750s
	iters: 300, epoch: 6 | loss: 0.5020378
	speed: 0.0375s/iter; left time: 1961.3295s
	iters: 400, epoch: 6 | loss: 0.4347300
	speed: 0.0394s/iter; left time: 2053.1290s
	iters: 500, epoch: 6 | loss: 0.4939930
	speed: 0.0412s/iter; left time: 2146.4061s
Epoch: 6 cost time: 22.374095916748047
Epoch: 6, Steps: 553 | Train Loss: 0.5627956 Vali Loss: 0.5975018 Test Loss: 0.3105806
Validation loss decreased (0.598498 --> 0.597502).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.7128680
	speed: 0.1634s/iter; left time: 8476.2209s
	iters: 200, epoch: 7 | loss: 0.5724028
	speed: 0.0422s/iter; left time: 2185.0173s
	iters: 300, epoch: 7 | loss: 0.6855244
	speed: 0.0347s/iter; left time: 1793.8366s
	iters: 400, epoch: 7 | loss: 0.4872151
	speed: 0.0369s/iter; left time: 1901.3879s
	iters: 500, epoch: 7 | loss: 0.5312822
	speed: 0.0418s/iter; left time: 2152.4323s
Epoch: 7 cost time: 22.26920747756958
Epoch: 7, Steps: 553 | Train Loss: 0.5622809 Vali Loss: 0.5970941 Test Loss: 0.3101168
Validation loss decreased (0.597502 --> 0.597094).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4616530
	speed: 0.1698s/iter; left time: 8715.1366s
	iters: 200, epoch: 8 | loss: 0.4805776
	speed: 0.0363s/iter; left time: 1857.2971s
	iters: 300, epoch: 8 | loss: 0.4718802
	speed: 0.0466s/iter; left time: 2380.8375s
	iters: 400, epoch: 8 | loss: 0.5191073
	speed: 0.0371s/iter; left time: 1892.5774s
	iters: 500, epoch: 8 | loss: 0.5716708
	speed: 0.0429s/iter; left time: 2185.5564s
Epoch: 8 cost time: 23.249054193496704
Epoch: 8, Steps: 553 | Train Loss: 0.5617669 Vali Loss: 0.5959775 Test Loss: 0.3097423
Validation loss decreased (0.597094 --> 0.595977).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4352643
	speed: 0.1720s/iter; left time: 8735.9081s
	iters: 200, epoch: 9 | loss: 0.4983698
	speed: 0.0361s/iter; left time: 1829.6852s
	iters: 300, epoch: 9 | loss: 0.5317420
	speed: 0.0375s/iter; left time: 1898.1639s
	iters: 400, epoch: 9 | loss: 0.4961521
	speed: 0.0383s/iter; left time: 1935.4124s
	iters: 500, epoch: 9 | loss: 0.5581729
	speed: 0.0361s/iter; left time: 1819.1340s
Epoch: 9 cost time: 21.331633806228638
Epoch: 9, Steps: 553 | Train Loss: 0.5615715 Vali Loss: 0.5953027 Test Loss: 0.3097981
Validation loss decreased (0.595977 --> 0.595303).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4587120
	speed: 0.1672s/iter; left time: 8399.2265s
	iters: 200, epoch: 10 | loss: 0.6270586
	speed: 0.0430s/iter; left time: 2157.1529s
	iters: 300, epoch: 10 | loss: 0.5268700
	speed: 0.0390s/iter; left time: 1949.9919s
	iters: 400, epoch: 10 | loss: 0.5763843
	speed: 0.0376s/iter; left time: 1879.1965s
	iters: 500, epoch: 10 | loss: 0.7789593
	speed: 0.0372s/iter; left time: 1852.5278s
Epoch: 10 cost time: 22.39054012298584
Epoch: 10, Steps: 553 | Train Loss: 0.5611798 Vali Loss: 0.5958565 Test Loss: 0.3092874
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4575287
	speed: 0.1631s/iter; left time: 8099.7194s
	iters: 200, epoch: 11 | loss: 0.5979425
	speed: 0.0377s/iter; left time: 1868.0621s
	iters: 300, epoch: 11 | loss: 0.5747430
	speed: 0.0360s/iter; left time: 1780.6949s
	iters: 400, epoch: 11 | loss: 0.7376105
	speed: 0.0369s/iter; left time: 1821.2080s
	iters: 500, epoch: 11 | loss: 0.4938466
	speed: 0.0394s/iter; left time: 1940.0351s
Epoch: 11 cost time: 21.172497034072876
Epoch: 11, Steps: 553 | Train Loss: 0.5608482 Vali Loss: 0.5941392 Test Loss: 0.3090058
Validation loss decreased (0.595303 --> 0.594139).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6372343
	speed: 0.1602s/iter; left time: 7869.2639s
	iters: 200, epoch: 12 | loss: 0.6003517
	speed: 0.0380s/iter; left time: 1863.0090s
	iters: 300, epoch: 12 | loss: 0.4899770
	speed: 0.0384s/iter; left time: 1880.4161s
	iters: 400, epoch: 12 | loss: 0.6845762
	speed: 0.0392s/iter; left time: 1912.4909s
	iters: 500, epoch: 12 | loss: 0.4746011
	speed: 0.0440s/iter; left time: 2146.0230s
Epoch: 12 cost time: 22.810636281967163
Epoch: 12, Steps: 553 | Train Loss: 0.5607531 Vali Loss: 0.5940058 Test Loss: 0.3090088
Validation loss decreased (0.594139 --> 0.594006).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5897401
	speed: 0.1735s/iter; left time: 8427.2663s
	iters: 200, epoch: 13 | loss: 0.5669501
	speed: 0.0417s/iter; left time: 2018.9882s
	iters: 300, epoch: 13 | loss: 0.6639640
	speed: 0.0394s/iter; left time: 1903.5535s
	iters: 400, epoch: 13 | loss: 0.4770876
	speed: 0.0405s/iter; left time: 1956.5686s
	iters: 500, epoch: 13 | loss: 0.4251690
	speed: 0.0373s/iter; left time: 1798.5334s
Epoch: 13 cost time: 22.705744981765747
Epoch: 13, Steps: 553 | Train Loss: 0.5607128 Vali Loss: 0.5939344 Test Loss: 0.3089833
Validation loss decreased (0.594006 --> 0.593934).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6671559
	speed: 0.1651s/iter; left time: 7925.9997s
	iters: 200, epoch: 14 | loss: 0.4750605
	speed: 0.0424s/iter; left time: 2030.3219s
	iters: 300, epoch: 14 | loss: 0.4688644
	speed: 0.0394s/iter; left time: 1883.7386s
	iters: 400, epoch: 14 | loss: 0.5166901
	speed: 0.0364s/iter; left time: 1737.3141s
	iters: 500, epoch: 14 | loss: 0.5945931
	speed: 0.0417s/iter; left time: 1984.1525s
Epoch: 14 cost time: 22.224916219711304
Epoch: 14, Steps: 553 | Train Loss: 0.5602132 Vali Loss: 0.5943512 Test Loss: 0.3087803
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5779403
	speed: 0.1605s/iter; left time: 7615.3756s
	iters: 200, epoch: 15 | loss: 0.5374553
	speed: 0.0380s/iter; left time: 1800.2386s
	iters: 300, epoch: 15 | loss: 0.5320840
	speed: 0.0420s/iter; left time: 1986.9455s
	iters: 400, epoch: 15 | loss: 0.5634210
	speed: 0.0412s/iter; left time: 1943.6830s
	iters: 500, epoch: 15 | loss: 0.4523407
	speed: 0.0421s/iter; left time: 1982.0960s
Epoch: 15 cost time: 22.600568294525146
Epoch: 15, Steps: 553 | Train Loss: 0.5603908 Vali Loss: 0.5939873 Test Loss: 0.3087033
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.6598204
	speed: 0.1605s/iter; left time: 7530.3075s
	iters: 200, epoch: 16 | loss: 0.6013665
	speed: 0.0390s/iter; left time: 1827.4384s
	iters: 300, epoch: 16 | loss: 0.5618353
	speed: 0.0460s/iter; left time: 2148.1626s
	iters: 400, epoch: 16 | loss: 0.6322871
	speed: 0.0419s/iter; left time: 1951.2659s
	iters: 500, epoch: 16 | loss: 0.5260845
	speed: 0.0441s/iter; left time: 2050.5379s
Epoch: 16 cost time: 23.874178171157837
Epoch: 16, Steps: 553 | Train Loss: 0.5602581 Vali Loss: 0.5940843 Test Loss: 0.3083889
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H_FITS_custom_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.308277428150177, mae:0.3303303122520447, rse:0.7306391000747681, corr:[0.47313762 0.47348174 0.47203827 0.4708947  0.47033516 0.47003028
 0.4695391  0.46866065 0.4675596  0.466548   0.46588874 0.4657116
 0.46576664 0.4657906  0.4655181  0.46477067 0.4637225  0.46259445
 0.46170288 0.46105888 0.46052054 0.45987096 0.45901763 0.45781526
 0.45637706 0.45487532 0.45354655 0.45253143 0.45187637 0.4514038
 0.45095786 0.4503924  0.44979328 0.44922718 0.44888133 0.44871795
 0.4487454  0.44871712 0.44845548 0.44788432 0.44707286 0.44626975
 0.44564116 0.44526327 0.4450843  0.4449269  0.4446649  0.4441775
 0.44327658 0.44214466 0.44101116 0.44002575 0.4393949  0.43911391
 0.4390581  0.43900114 0.43879324 0.43832427 0.43769673 0.43701252
 0.43646634 0.4361292  0.43601763 0.4359908  0.43589285 0.43562415
 0.43512076 0.43447196 0.43382466 0.43329498 0.43297333 0.43271804
 0.43255523 0.43237507 0.432066   0.43162772 0.43114054 0.43075317
 0.4305031  0.43036932 0.43037277 0.4304084  0.4303646  0.4302201
 0.43001044 0.4297858  0.42963114 0.42949167 0.4294939  0.42956242
 0.4296558  0.42970526 0.42958    0.42928398 0.42886543 0.428375
 0.42790493 0.42753208 0.42728153 0.42718452 0.42717865 0.42717633
 0.42712083 0.42695618 0.42665565 0.4262479  0.42576545 0.42532042
 0.42490804 0.42459983 0.4243786  0.42417744 0.42395672 0.4237609
 0.42355704 0.42331615 0.42310566 0.42295858 0.42285904 0.4227387
 0.42262104 0.4224998  0.4223358  0.42215377 0.4219363  0.4216912
 0.4214001  0.42104533 0.42066628 0.4202753  0.41987246 0.41949356
 0.41918087 0.41894844 0.41882977 0.41878286 0.418768   0.41874197
 0.41854227 0.41824257 0.41788018 0.41753173 0.41719386 0.41691786
 0.41674456 0.41666594 0.41664642 0.41661078 0.41642082 0.4159817
 0.41533783 0.4146176  0.41387436 0.413142   0.41254684 0.4120939
 0.41182983 0.4115821  0.4113279  0.41102752 0.41068304 0.41032627
 0.4099564  0.4096126  0.40923965 0.40883383 0.40831316 0.40767455
 0.40695053 0.40615913 0.4053424  0.40459052 0.40397087 0.4035331
 0.4031864  0.40288246 0.40256727 0.40223297 0.4018738  0.40147468
 0.40109575 0.40070522 0.4002928  0.3998482  0.39940643 0.39894557
 0.39845893 0.39790386 0.39733353 0.39678308 0.3962704  0.39582038
 0.39544567 0.39504263 0.39463758 0.39419767 0.39371356 0.39323357
 0.39277595 0.39240187 0.39216056 0.39193407 0.3916647  0.39126822
 0.39074898 0.39008576 0.38931885 0.38852495 0.38773358 0.38709098
 0.38660148 0.3862707  0.38603774 0.38585344 0.38564494 0.38534036
 0.38490897 0.38449278 0.3840374  0.3835444  0.38308635 0.38273928
 0.3824659  0.3822105  0.38196182 0.3816742  0.38134405 0.381002
 0.38060156 0.38015044 0.37964597 0.37908164 0.37848964 0.3779136
 0.3774145  0.37705496 0.3767731  0.37654597 0.37626913 0.3759333
 0.37558523 0.37523174 0.37482992 0.3745114  0.37433186 0.37426937
 0.37424368 0.37420443 0.37403578 0.3736767  0.3731114  0.37239537
 0.3715641  0.37068602 0.36996257 0.36936533 0.3689916  0.3687995
 0.368775   0.36882162 0.36877862 0.36872858 0.36867255 0.36865386
 0.36865112 0.36863545 0.36853373 0.36834896 0.36810076 0.3677214
 0.3672916  0.36692545 0.36662787 0.3663344  0.36610642 0.36589953
 0.36565936 0.36529598 0.36487252 0.36439005 0.3638491  0.36331034
 0.36280146 0.3622332  0.36164886 0.3610434  0.36042702 0.359784
 0.35916442 0.3585827  0.35800898 0.3574042  0.35688344 0.3564302
 0.35592258 0.355346   0.35469013 0.35399592 0.35326126 0.3526266
 0.35197428 0.3513865  0.35083038 0.3502974  0.34973398 0.34911785
 0.34847784 0.34784684 0.34726295 0.34670252 0.346235   0.3458013
 0.34540632 0.3450711  0.34472016 0.34430274 0.3438365  0.34330902
 0.34275046 0.34214184 0.3415071  0.34090865 0.34039897 0.33991477
 0.3394761  0.33906546 0.33860227 0.33812568 0.33768257 0.33723265
 0.33683112 0.33646747 0.3361573  0.33594593 0.33574456 0.33553648
 0.33524737 0.33487242 0.33441478 0.33388808 0.33333436 0.33284965
 0.33245558 0.33211032 0.33182245 0.33156902 0.33135504 0.33111376
 0.33085227 0.33050507 0.330096   0.32960826 0.32911485 0.32860905
 0.32811776 0.3276865  0.32735693 0.32708564 0.32688755 0.32675934
 0.32664543 0.32654223 0.32645196 0.32632166 0.32616332 0.3259851
 0.32575858 0.32549918 0.3252158  0.3248379  0.32440016 0.32392386
 0.32347745 0.32308245 0.32280764 0.32268074 0.32267168 0.32271877
 0.32276478 0.32278326 0.32270595 0.32253432 0.3223087  0.32198295
 0.32162628 0.32128853 0.32102203 0.32080325 0.32060662 0.32040253
 0.32011828 0.3198295  0.31950918 0.31919223 0.3188935  0.3186283
 0.31839788 0.3181904  0.3179954  0.31778926 0.31759304 0.3173775
 0.3172037  0.3170209  0.31686938 0.31670862 0.3165196  0.31632417
 0.3161431  0.3159585  0.31579202 0.31559595 0.31544498 0.3153634
 0.31532383 0.31527627 0.31521285 0.31513172 0.3149395  0.31466734
 0.31430298 0.3139388  0.31355911 0.31323758 0.3129185  0.31259072
 0.31222895 0.3118838  0.31150013 0.31104803 0.3105774  0.31017226
 0.30980295 0.3094634  0.30915812 0.30883533 0.30842045 0.3078778
 0.307206   0.3064661  0.30575413 0.30510008 0.30452484 0.30394596
 0.30341494 0.30288768 0.30229208 0.30166674 0.3009918  0.30031824
 0.29970035 0.29910827 0.2985617  0.29804054 0.29754055 0.2969631
 0.2963046  0.2955716  0.29479247 0.29400393 0.2932688  0.29269403
 0.29226905 0.29199213 0.2918422  0.29174706 0.29164544 0.2914791
 0.29126206 0.2909826  0.2906205  0.290214   0.2898332  0.2894776
 0.289158   0.28887692 0.28860825 0.28835893 0.28813863 0.28791004
 0.2877185  0.28753567 0.2873301  0.2871083  0.2868876  0.2866999
 0.28649208 0.2862912  0.28611994 0.28596222 0.2858091  0.28562984
 0.28544888 0.28527394 0.28505647 0.28478852 0.2844647  0.2841584
 0.28390667 0.28375113 0.2836792  0.28367123 0.2837502  0.28386238
 0.28392655 0.28389776 0.2837839  0.2834749  0.2830561  0.28254732
 0.28206158 0.2816549  0.28138337 0.28121158 0.28111097 0.28104624
 0.28095475 0.2807949  0.2805199  0.28015298 0.27971646 0.27923867
 0.27879685 0.27843228 0.27813748 0.2779663  0.27784613 0.277767
 0.27766314 0.2775007  0.2772466  0.27689257 0.27647343 0.2759648
 0.2754308  0.27493024 0.2744753  0.27410644 0.2738034  0.27352569
 0.2732495  0.27298683 0.27269268 0.27240726 0.27216482 0.2719527
 0.27180165 0.2717293  0.27170432 0.27163008 0.27145645 0.27110192
 0.27057424 0.26992068 0.26923493 0.26855728 0.2679954  0.26754153
 0.26721323 0.26692927 0.26665556 0.2663896  0.2660707  0.2657245
 0.26540232 0.2651391  0.26490924 0.26466653 0.2643581  0.26397702
 0.2635117  0.26300654 0.2625315  0.26210973 0.26173306 0.2613222
 0.26113307 0.2606836  0.26001415 0.25943908 0.25868112 0.25787058
 0.25713688 0.25661278 0.25634265 0.25633907 0.25643954 0.25652638
 0.25641283 0.2559967  0.2552535  0.2542891  0.25322887 0.25223693
 0.2514102  0.2507871  0.25035974 0.25004804 0.24977699 0.24950184
 0.24923761 0.24896601 0.24870357 0.24842721 0.24811977 0.2477696
 0.24727018 0.24664937 0.24587314 0.24501924 0.24424471 0.2436296
 0.24323009 0.2430788  0.24304877 0.24304217 0.2429653  0.24267527
 0.24223287 0.24174942 0.24127547 0.24090488 0.24069898 0.24067333
 0.24069735 0.2407261  0.2406985  0.24060099 0.24050859 0.2403822
 0.24027471 0.24015279 0.24002567 0.23981375 0.23947535 0.23903473
 0.2385475  0.2380959  0.23774526 0.23753119 0.23750046 0.23760846
 0.23783174 0.23807876 0.2382628  0.23829035 0.2382291  0.23811308
 0.2380164  0.23787923 0.23774484 0.23749423 0.237197   0.23679452
 0.23637679 0.23597795 0.23567991 0.23554689 0.23561281 0.23584482
 0.23614696 0.23635906 0.23636083 0.23609792 0.23565371 0.23514934
 0.23470765 0.2344358  0.23431624 0.23432201 0.2343457  0.23421426
 0.23389816 0.23339067 0.23272842 0.23205678 0.2315119  0.23121059
 0.23113513 0.23121569 0.23132917 0.23130555 0.23112175 0.23076142
 0.23030029 0.22984512 0.22950482 0.22925925 0.22905074 0.22879662
 0.22845636 0.22800612 0.22759096 0.22729746 0.22712967 0.22701767
 0.22687179 0.22656177 0.22604601 0.2253987  0.22481124 0.22444756
 0.22450869 0.22495604 0.22550407 0.22580276 0.22557399 0.22475544
 0.22352727 0.2222998  0.22158407 0.22167622 0.22229862 0.22302358
 0.22323257 0.22297679 0.22247174 0.22244993 0.22328298 0.22425883]
