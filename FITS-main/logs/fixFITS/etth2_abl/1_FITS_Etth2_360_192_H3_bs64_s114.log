Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_192_FITS_ETTh2_ftM_sl360_ll48_pl192_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=58, out_features=88, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4573184.0
params:  5192.0
Trainable parameters:  5192
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.501228094100952
Epoch: 1, Steps: 63 | Train Loss: 0.7097082 Vali Loss: 0.3764561 Test Loss: 0.4252840
Validation loss decreased (inf --> 0.376456).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.05492639541626
Epoch: 2, Steps: 63 | Train Loss: 0.5989587 Vali Loss: 0.3367444 Test Loss: 0.3930410
Validation loss decreased (0.376456 --> 0.336744).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.4285831451416016
Epoch: 3, Steps: 63 | Train Loss: 0.5637959 Vali Loss: 0.3195928 Test Loss: 0.3814158
Validation loss decreased (0.336744 --> 0.319593).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.614701986312866
Epoch: 4, Steps: 63 | Train Loss: 0.5497421 Vali Loss: 0.3104626 Test Loss: 0.3757051
Validation loss decreased (0.319593 --> 0.310463).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.8175058364868164
Epoch: 5, Steps: 63 | Train Loss: 0.5397061 Vali Loss: 0.3049549 Test Loss: 0.3718161
Validation loss decreased (0.310463 --> 0.304955).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.5487802028656006
Epoch: 6, Steps: 63 | Train Loss: 0.5355912 Vali Loss: 0.3015312 Test Loss: 0.3690511
Validation loss decreased (0.304955 --> 0.301531).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.4784464836120605
Epoch: 7, Steps: 63 | Train Loss: 0.5314417 Vali Loss: 0.2986881 Test Loss: 0.3673254
Validation loss decreased (0.301531 --> 0.298688).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.1191086769104004
Epoch: 8, Steps: 63 | Train Loss: 0.5278042 Vali Loss: 0.2967213 Test Loss: 0.3658956
Validation loss decreased (0.298688 --> 0.296721).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.526298999786377
Epoch: 9, Steps: 63 | Train Loss: 0.5262039 Vali Loss: 0.2953017 Test Loss: 0.3646426
Validation loss decreased (0.296721 --> 0.295302).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.5800797939300537
Epoch: 10, Steps: 63 | Train Loss: 0.5244336 Vali Loss: 0.2937687 Test Loss: 0.3637811
Validation loss decreased (0.295302 --> 0.293769).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.153730869293213
Epoch: 11, Steps: 63 | Train Loss: 0.5209440 Vali Loss: 0.2927518 Test Loss: 0.3628784
Validation loss decreased (0.293769 --> 0.292752).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.642000913619995
Epoch: 12, Steps: 63 | Train Loss: 0.5204515 Vali Loss: 0.2913834 Test Loss: 0.3623830
Validation loss decreased (0.292752 --> 0.291383).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.4164605140686035
Epoch: 13, Steps: 63 | Train Loss: 0.5197221 Vali Loss: 0.2910669 Test Loss: 0.3617854
Validation loss decreased (0.291383 --> 0.291067).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.589081048965454
Epoch: 14, Steps: 63 | Train Loss: 0.5183945 Vali Loss: 0.2904111 Test Loss: 0.3612839
Validation loss decreased (0.291067 --> 0.290411).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.719877243041992
Epoch: 15, Steps: 63 | Train Loss: 0.5178522 Vali Loss: 0.2898599 Test Loss: 0.3608362
Validation loss decreased (0.290411 --> 0.289860).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.206976890563965
Epoch: 16, Steps: 63 | Train Loss: 0.5163075 Vali Loss: 0.2892383 Test Loss: 0.3604172
Validation loss decreased (0.289860 --> 0.289238).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.317171096801758
Epoch: 17, Steps: 63 | Train Loss: 0.5170688 Vali Loss: 0.2887276 Test Loss: 0.3601159
Validation loss decreased (0.289238 --> 0.288728).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.8095357418060303
Epoch: 18, Steps: 63 | Train Loss: 0.5164548 Vali Loss: 0.2880873 Test Loss: 0.3599391
Validation loss decreased (0.288728 --> 0.288087).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.24161696434021
Epoch: 19, Steps: 63 | Train Loss: 0.5142563 Vali Loss: 0.2878602 Test Loss: 0.3595578
Validation loss decreased (0.288087 --> 0.287860).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.4387173652648926
Epoch: 20, Steps: 63 | Train Loss: 0.5143837 Vali Loss: 0.2873943 Test Loss: 0.3594324
Validation loss decreased (0.287860 --> 0.287394).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.8266875743865967
Epoch: 21, Steps: 63 | Train Loss: 0.5150180 Vali Loss: 0.2871932 Test Loss: 0.3591411
Validation loss decreased (0.287394 --> 0.287193).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.329617738723755
Epoch: 22, Steps: 63 | Train Loss: 0.5139248 Vali Loss: 0.2869360 Test Loss: 0.3589584
Validation loss decreased (0.287193 --> 0.286936).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.6799678802490234
Epoch: 23, Steps: 63 | Train Loss: 0.5128923 Vali Loss: 0.2866267 Test Loss: 0.3587939
Validation loss decreased (0.286936 --> 0.286627).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.787644863128662
Epoch: 24, Steps: 63 | Train Loss: 0.5130565 Vali Loss: 0.2861744 Test Loss: 0.3587093
Validation loss decreased (0.286627 --> 0.286174).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.204556703567505
Epoch: 25, Steps: 63 | Train Loss: 0.5134317 Vali Loss: 0.2860991 Test Loss: 0.3585712
Validation loss decreased (0.286174 --> 0.286099).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.421782970428467
Epoch: 26, Steps: 63 | Train Loss: 0.5130123 Vali Loss: 0.2857627 Test Loss: 0.3583833
Validation loss decreased (0.286099 --> 0.285763).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.367006778717041
Epoch: 27, Steps: 63 | Train Loss: 0.5132871 Vali Loss: 0.2856262 Test Loss: 0.3583433
Validation loss decreased (0.285763 --> 0.285626).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.5034549236297607
Epoch: 28, Steps: 63 | Train Loss: 0.5116238 Vali Loss: 0.2854399 Test Loss: 0.3583021
Validation loss decreased (0.285626 --> 0.285440).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.4218735694885254
Epoch: 29, Steps: 63 | Train Loss: 0.5125727 Vali Loss: 0.2853875 Test Loss: 0.3581471
Validation loss decreased (0.285440 --> 0.285388).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.5132148265838623
Epoch: 30, Steps: 63 | Train Loss: 0.5123668 Vali Loss: 0.2852047 Test Loss: 0.3580883
Validation loss decreased (0.285388 --> 0.285205).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.0560362339019775
Epoch: 31, Steps: 63 | Train Loss: 0.5121858 Vali Loss: 0.2849987 Test Loss: 0.3579957
Validation loss decreased (0.285205 --> 0.284999).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.4178194999694824
Epoch: 32, Steps: 63 | Train Loss: 0.5103066 Vali Loss: 0.2849096 Test Loss: 0.3579287
Validation loss decreased (0.284999 --> 0.284910).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.397564172744751
Epoch: 33, Steps: 63 | Train Loss: 0.5116934 Vali Loss: 0.2845733 Test Loss: 0.3579623
Validation loss decreased (0.284910 --> 0.284573).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.283083200454712
Epoch: 34, Steps: 63 | Train Loss: 0.5117846 Vali Loss: 0.2846186 Test Loss: 0.3578267
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.455129861831665
Epoch: 35, Steps: 63 | Train Loss: 0.5118310 Vali Loss: 0.2845957 Test Loss: 0.3578187
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.2972047328948975
Epoch: 36, Steps: 63 | Train Loss: 0.5115479 Vali Loss: 0.2843879 Test Loss: 0.3577147
Validation loss decreased (0.284573 --> 0.284388).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.987966775894165
Epoch: 37, Steps: 63 | Train Loss: 0.5110812 Vali Loss: 0.2843276 Test Loss: 0.3576493
Validation loss decreased (0.284388 --> 0.284328).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.4524612426757812
Epoch: 38, Steps: 63 | Train Loss: 0.5114718 Vali Loss: 0.2842804 Test Loss: 0.3576869
Validation loss decreased (0.284328 --> 0.284280).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.257274866104126
Epoch: 39, Steps: 63 | Train Loss: 0.5109436 Vali Loss: 0.2841653 Test Loss: 0.3576074
Validation loss decreased (0.284280 --> 0.284165).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.0586190223693848
Epoch: 40, Steps: 63 | Train Loss: 0.5097594 Vali Loss: 0.2841274 Test Loss: 0.3575470
Validation loss decreased (0.284165 --> 0.284127).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.670135259628296
Epoch: 41, Steps: 63 | Train Loss: 0.5109663 Vali Loss: 0.2841381 Test Loss: 0.3574932
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.443772077560425
Epoch: 42, Steps: 63 | Train Loss: 0.5101490 Vali Loss: 0.2840869 Test Loss: 0.3574617
Validation loss decreased (0.284127 --> 0.284087).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0865652561187744
Epoch: 43, Steps: 63 | Train Loss: 0.5094183 Vali Loss: 0.2839949 Test Loss: 0.3574444
Validation loss decreased (0.284087 --> 0.283995).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.486440896987915
Epoch: 44, Steps: 63 | Train Loss: 0.5109833 Vali Loss: 0.2838221 Test Loss: 0.3574241
Validation loss decreased (0.283995 --> 0.283822).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.18119215965271
Epoch: 45, Steps: 63 | Train Loss: 0.5106854 Vali Loss: 0.2838567 Test Loss: 0.3573996
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8822448253631592
Epoch: 46, Steps: 63 | Train Loss: 0.5107951 Vali Loss: 0.2837861 Test Loss: 0.3573898
Validation loss decreased (0.283822 --> 0.283786).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.185661554336548
Epoch: 47, Steps: 63 | Train Loss: 0.5106522 Vali Loss: 0.2837595 Test Loss: 0.3573463
Validation loss decreased (0.283786 --> 0.283760).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9700074195861816
Epoch: 48, Steps: 63 | Train Loss: 0.5099264 Vali Loss: 0.2837102 Test Loss: 0.3573232
Validation loss decreased (0.283760 --> 0.283710).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.2177982330322266
Epoch: 49, Steps: 63 | Train Loss: 0.5102404 Vali Loss: 0.2836132 Test Loss: 0.3573553
Validation loss decreased (0.283710 --> 0.283613).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.3552968502044678
Epoch: 50, Steps: 63 | Train Loss: 0.5098363 Vali Loss: 0.2836161 Test Loss: 0.3572791
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.3191990852355957
Epoch: 51, Steps: 63 | Train Loss: 0.5106635 Vali Loss: 0.2835765 Test Loss: 0.3572802
Validation loss decreased (0.283613 --> 0.283577).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.7264184951782227
Epoch: 52, Steps: 63 | Train Loss: 0.5099968 Vali Loss: 0.2835518 Test Loss: 0.3572362
Validation loss decreased (0.283577 --> 0.283552).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.1574461460113525
Epoch: 53, Steps: 63 | Train Loss: 0.5106294 Vali Loss: 0.2834575 Test Loss: 0.3572599
Validation loss decreased (0.283552 --> 0.283458).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.546804428100586
Epoch: 54, Steps: 63 | Train Loss: 0.5106713 Vali Loss: 0.2834084 Test Loss: 0.3572068
Validation loss decreased (0.283458 --> 0.283408).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.8825433254241943
Epoch: 55, Steps: 63 | Train Loss: 0.5099706 Vali Loss: 0.2833984 Test Loss: 0.3571960
Validation loss decreased (0.283408 --> 0.283398).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.5580618381500244
Epoch: 56, Steps: 63 | Train Loss: 0.5102497 Vali Loss: 0.2833768 Test Loss: 0.3571738
Validation loss decreased (0.283398 --> 0.283377).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.3732423782348633
Epoch: 57, Steps: 63 | Train Loss: 0.5095046 Vali Loss: 0.2833757 Test Loss: 0.3571724
Validation loss decreased (0.283377 --> 0.283376).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.9997684955596924
Epoch: 58, Steps: 63 | Train Loss: 0.5089130 Vali Loss: 0.2833286 Test Loss: 0.3571573
Validation loss decreased (0.283376 --> 0.283329).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.945667028427124
Epoch: 59, Steps: 63 | Train Loss: 0.5099654 Vali Loss: 0.2833546 Test Loss: 0.3571230
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.3792407512664795
Epoch: 60, Steps: 63 | Train Loss: 0.5101525 Vali Loss: 0.2829832 Test Loss: 0.3571512
Validation loss decreased (0.283329 --> 0.282983).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.83402419090271
Epoch: 61, Steps: 63 | Train Loss: 0.5096799 Vali Loss: 0.2830776 Test Loss: 0.3571326
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.631272792816162
Epoch: 62, Steps: 63 | Train Loss: 0.5097510 Vali Loss: 0.2832625 Test Loss: 0.3571160
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.2609097957611084
Epoch: 63, Steps: 63 | Train Loss: 0.5101749 Vali Loss: 0.2832304 Test Loss: 0.3571108
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.0304088592529297
Epoch: 64, Steps: 63 | Train Loss: 0.5100805 Vali Loss: 0.2832255 Test Loss: 0.3570974
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.044829845428467
Epoch: 65, Steps: 63 | Train Loss: 0.5089890 Vali Loss: 0.2831768 Test Loss: 0.3570864
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.839268207550049
Epoch: 66, Steps: 63 | Train Loss: 0.5094301 Vali Loss: 0.2831834 Test Loss: 0.3570911
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.55210280418396
Epoch: 67, Steps: 63 | Train Loss: 0.5099789 Vali Loss: 0.2831686 Test Loss: 0.3570789
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.7317492961883545
Epoch: 68, Steps: 63 | Train Loss: 0.5099334 Vali Loss: 0.2831285 Test Loss: 0.3570746
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.5685791969299316
Epoch: 69, Steps: 63 | Train Loss: 0.5093017 Vali Loss: 0.2831279 Test Loss: 0.3570713
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.3126461505889893
Epoch: 70, Steps: 63 | Train Loss: 0.5097800 Vali Loss: 0.2830426 Test Loss: 0.3570541
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.7643988132476807
Epoch: 71, Steps: 63 | Train Loss: 0.5089429 Vali Loss: 0.2830901 Test Loss: 0.3570493
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.3005142211914062
Epoch: 72, Steps: 63 | Train Loss: 0.5090123 Vali Loss: 0.2830297 Test Loss: 0.3570422
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.4017927646636963
Epoch: 73, Steps: 63 | Train Loss: 0.5090887 Vali Loss: 0.2830903 Test Loss: 0.3570414
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.9334259033203125
Epoch: 74, Steps: 63 | Train Loss: 0.5095743 Vali Loss: 0.2829832 Test Loss: 0.3570279
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.0693490505218506
Epoch: 75, Steps: 63 | Train Loss: 0.5096285 Vali Loss: 0.2830654 Test Loss: 0.3570343
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.1847774982452393
Epoch: 76, Steps: 63 | Train Loss: 0.5101119 Vali Loss: 0.2830504 Test Loss: 0.3570219
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.180133104324341
Epoch: 77, Steps: 63 | Train Loss: 0.5093442 Vali Loss: 0.2826760 Test Loss: 0.3570202
Validation loss decreased (0.282983 --> 0.282676).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.6558213233947754
Epoch: 78, Steps: 63 | Train Loss: 0.5095120 Vali Loss: 0.2830174 Test Loss: 0.3570150
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.970104694366455
Epoch: 79, Steps: 63 | Train Loss: 0.5093823 Vali Loss: 0.2829223 Test Loss: 0.3570047
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.2066118717193604
Epoch: 80, Steps: 63 | Train Loss: 0.5097049 Vali Loss: 0.2829663 Test Loss: 0.3570077
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.547266960144043
Epoch: 81, Steps: 63 | Train Loss: 0.5095737 Vali Loss: 0.2830082 Test Loss: 0.3570024
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.39988374710083
Epoch: 82, Steps: 63 | Train Loss: 0.5090316 Vali Loss: 0.2830293 Test Loss: 0.3569966
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 3.1439146995544434
Epoch: 83, Steps: 63 | Train Loss: 0.5093618 Vali Loss: 0.2829208 Test Loss: 0.3569973
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.545900344848633
Epoch: 84, Steps: 63 | Train Loss: 0.5096072 Vali Loss: 0.2829543 Test Loss: 0.3569948
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.2646100521087646
Epoch: 85, Steps: 63 | Train Loss: 0.5091715 Vali Loss: 0.2829728 Test Loss: 0.3569905
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.0540969371795654
Epoch: 86, Steps: 63 | Train Loss: 0.5089209 Vali Loss: 0.2829954 Test Loss: 0.3569878
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.0295615196228027
Epoch: 87, Steps: 63 | Train Loss: 0.5090238 Vali Loss: 0.2825806 Test Loss: 0.3569864
Validation loss decreased (0.282676 --> 0.282581).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.1515350341796875
Epoch: 88, Steps: 63 | Train Loss: 0.5096046 Vali Loss: 0.2829592 Test Loss: 0.3569799
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.221101760864258
Epoch: 89, Steps: 63 | Train Loss: 0.5096772 Vali Loss: 0.2826926 Test Loss: 0.3569821
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.0194454193115234
Epoch: 90, Steps: 63 | Train Loss: 0.5098104 Vali Loss: 0.2829553 Test Loss: 0.3569772
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.513413906097412
Epoch: 91, Steps: 63 | Train Loss: 0.5098600 Vali Loss: 0.2828529 Test Loss: 0.3569751
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.781386137008667
Epoch: 92, Steps: 63 | Train Loss: 0.5097311 Vali Loss: 0.2829586 Test Loss: 0.3569737
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.535207509994507
Epoch: 93, Steps: 63 | Train Loss: 0.5088745 Vali Loss: 0.2829659 Test Loss: 0.3569747
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.8709042072296143
Epoch: 94, Steps: 63 | Train Loss: 0.5096924 Vali Loss: 0.2829529 Test Loss: 0.3569679
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.0477890968322754
Epoch: 95, Steps: 63 | Train Loss: 0.5084093 Vali Loss: 0.2829534 Test Loss: 0.3569687
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.2137463092803955
Epoch: 96, Steps: 63 | Train Loss: 0.5072265 Vali Loss: 0.2829010 Test Loss: 0.3569643
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.6105527877807617
Epoch: 97, Steps: 63 | Train Loss: 0.5085192 Vali Loss: 0.2828858 Test Loss: 0.3569659
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.136702060699463
Epoch: 98, Steps: 63 | Train Loss: 0.5087598 Vali Loss: 0.2829335 Test Loss: 0.3569672
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.903343915939331
Epoch: 99, Steps: 63 | Train Loss: 0.5076288 Vali Loss: 0.2829458 Test Loss: 0.3569634
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.3154654502868652
Epoch: 100, Steps: 63 | Train Loss: 0.5097209 Vali Loss: 0.2828553 Test Loss: 0.3569625
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_360_192_FITS_ETTh2_ftM_sl360_ll48_pl192_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3373911380767822, mae:0.37786751985549927, rse:0.46580880880355835, corr:[0.2640277  0.26870728 0.26877558 0.26597765 0.26431498 0.26387206
 0.2635673  0.26252362 0.26079175 0.2589821  0.25745717 0.25596625
 0.25461507 0.25359422 0.25294235 0.25259113 0.25211334 0.25115502
 0.24974288 0.2482866  0.24696136 0.24566235 0.2440004  0.24160953
 0.2389235  0.23667495 0.23525771 0.23424762 0.23305555 0.23131317
 0.2292412  0.22721769 0.22579882 0.22488962 0.22405416 0.22265111
 0.22065601 0.21887833 0.21803965 0.21788232 0.2177907  0.21712285
 0.21568103 0.21405013 0.21281648 0.21198022 0.21103872 0.20941418
 0.20711395 0.20483366 0.20322129 0.20219587 0.20118442 0.19965036
 0.1973649  0.19484249 0.19305496 0.1920175  0.19164859 0.19137737
 0.19090633 0.19006371 0.1893996  0.18912962 0.18904863 0.18892685
 0.18825603 0.1870989  0.18602054 0.18539491 0.18517202 0.18492146
 0.18404178 0.18260919 0.18116893 0.18023589 0.18007357 0.18011723
 0.17962734 0.17843944 0.17741306 0.17682354 0.17698216 0.17744252
 0.17770192 0.17725189 0.17634426 0.17555499 0.17520131 0.17536947
 0.17551614 0.17516957 0.17458111 0.17394885 0.17360088 0.17342691
 0.17306021 0.17202778 0.17052372 0.16899054 0.16803196 0.16779178
 0.16796741 0.16768377 0.16692564 0.16601534 0.16542622 0.16567661
 0.16607818 0.16614996 0.16539963 0.1643645  0.16335148 0.16300713
 0.16326497 0.16336603 0.16288488 0.16146524 0.15962099 0.15804122
 0.15719864 0.15655325 0.15578286 0.15464614 0.15319964 0.15207638
 0.1517168  0.15177505 0.15150853 0.15047185 0.14919417 0.14828226
 0.14827567 0.1487438  0.14895849 0.14846925 0.14734933 0.14638656
 0.14596105 0.14642374 0.14710315 0.14683945 0.14529622 0.14324056
 0.14153075 0.14052281 0.13994022 0.13928531 0.1383123  0.13709605
 0.13652906 0.1364382  0.13683146 0.13700639 0.13638261 0.1352533
 0.13456623 0.1350808  0.1359836  0.13678226 0.1364997  0.13538755
 0.13448112 0.1345503  0.13560571 0.13647792 0.13624501 0.13442189
 0.13230799 0.1310298  0.13077109 0.13081376 0.13011767 0.12813887
 0.12571415 0.12405615 0.1236351  0.12373255 0.12324071 0.1214125
 0.11947656 0.11873759 0.11940371 0.12047125 0.11971056 0.11648417
 0.11327586 0.11228858 0.11400162 0.11591575 0.11348184 0.10428007]
