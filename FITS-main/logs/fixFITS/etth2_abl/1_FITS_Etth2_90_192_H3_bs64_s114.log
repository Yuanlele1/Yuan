Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_192_FITS_ETTh2_ftM_sl90_ll48_pl192_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8359
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=22, out_features=68, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1340416.0
params:  1564.0
Trainable parameters:  1564
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.809931993484497
Epoch: 1, Steps: 65 | Train Loss: 0.7556391 Vali Loss: 0.3713671 Test Loss: 0.5216971
Validation loss decreased (inf --> 0.371367).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3788630962371826
Epoch: 2, Steps: 65 | Train Loss: 0.6669122 Vali Loss: 0.3369441 Test Loss: 0.4733586
Validation loss decreased (0.371367 --> 0.336944).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.3593344688415527
Epoch: 3, Steps: 65 | Train Loss: 0.6217268 Vali Loss: 0.3182815 Test Loss: 0.4473439
Validation loss decreased (0.336944 --> 0.318281).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4512343406677246
Epoch: 4, Steps: 65 | Train Loss: 0.5979501 Vali Loss: 0.3070844 Test Loss: 0.4320281
Validation loss decreased (0.318281 --> 0.307084).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.9141952991485596
Epoch: 5, Steps: 65 | Train Loss: 0.5835982 Vali Loss: 0.2999523 Test Loss: 0.4225399
Validation loss decreased (0.307084 --> 0.299952).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.7215759754180908
Epoch: 6, Steps: 65 | Train Loss: 0.5745925 Vali Loss: 0.2951561 Test Loss: 0.4164902
Validation loss decreased (0.299952 --> 0.295156).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.6846308708190918
Epoch: 7, Steps: 65 | Train Loss: 0.5671373 Vali Loss: 0.2918444 Test Loss: 0.4123917
Validation loss decreased (0.295156 --> 0.291844).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8794751167297363
Epoch: 8, Steps: 65 | Train Loss: 0.5638921 Vali Loss: 0.2892458 Test Loss: 0.4093544
Validation loss decreased (0.291844 --> 0.289246).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.5338361263275146
Epoch: 9, Steps: 65 | Train Loss: 0.5617413 Vali Loss: 0.2875134 Test Loss: 0.4072922
Validation loss decreased (0.289246 --> 0.287513).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6371123790740967
Epoch: 10, Steps: 65 | Train Loss: 0.5550088 Vali Loss: 0.2861435 Test Loss: 0.4056200
Validation loss decreased (0.287513 --> 0.286144).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.6754026412963867
Epoch: 11, Steps: 65 | Train Loss: 0.5571592 Vali Loss: 0.2848578 Test Loss: 0.4043717
Validation loss decreased (0.286144 --> 0.284858).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.996335506439209
Epoch: 12, Steps: 65 | Train Loss: 0.5557761 Vali Loss: 0.2838352 Test Loss: 0.4033194
Validation loss decreased (0.284858 --> 0.283835).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9761042594909668
Epoch: 13, Steps: 65 | Train Loss: 0.5523591 Vali Loss: 0.2830378 Test Loss: 0.4025189
Validation loss decreased (0.283835 --> 0.283038).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.553903579711914
Epoch: 14, Steps: 65 | Train Loss: 0.5530182 Vali Loss: 0.2825186 Test Loss: 0.4018221
Validation loss decreased (0.283038 --> 0.282519).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.8679931163787842
Epoch: 15, Steps: 65 | Train Loss: 0.5513937 Vali Loss: 0.2818965 Test Loss: 0.4012797
Validation loss decreased (0.282519 --> 0.281896).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.3231971263885498
Epoch: 16, Steps: 65 | Train Loss: 0.5511008 Vali Loss: 0.2814011 Test Loss: 0.4007312
Validation loss decreased (0.281896 --> 0.281401).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.595705509185791
Epoch: 17, Steps: 65 | Train Loss: 0.5499205 Vali Loss: 0.2809453 Test Loss: 0.4002746
Validation loss decreased (0.281401 --> 0.280945).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.96533203125
Epoch: 18, Steps: 65 | Train Loss: 0.5491338 Vali Loss: 0.2805989 Test Loss: 0.3999593
Validation loss decreased (0.280945 --> 0.280599).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.8340661525726318
Epoch: 19, Steps: 65 | Train Loss: 0.5472468 Vali Loss: 0.2802577 Test Loss: 0.3996525
Validation loss decreased (0.280599 --> 0.280258).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8583364486694336
Epoch: 20, Steps: 65 | Train Loss: 0.5470791 Vali Loss: 0.2798007 Test Loss: 0.3993478
Validation loss decreased (0.280258 --> 0.279801).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.408038854598999
Epoch: 21, Steps: 65 | Train Loss: 0.5469582 Vali Loss: 0.2795446 Test Loss: 0.3991495
Validation loss decreased (0.279801 --> 0.279545).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.7642605304718018
Epoch: 22, Steps: 65 | Train Loss: 0.5476008 Vali Loss: 0.2793823 Test Loss: 0.3989194
Validation loss decreased (0.279545 --> 0.279382).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.80161714553833
Epoch: 23, Steps: 65 | Train Loss: 0.5465514 Vali Loss: 0.2791370 Test Loss: 0.3987291
Validation loss decreased (0.279382 --> 0.279137).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.7303950786590576
Epoch: 24, Steps: 65 | Train Loss: 0.5459460 Vali Loss: 0.2789080 Test Loss: 0.3986231
Validation loss decreased (0.279137 --> 0.278908).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9106051921844482
Epoch: 25, Steps: 65 | Train Loss: 0.5446566 Vali Loss: 0.2786160 Test Loss: 0.3984515
Validation loss decreased (0.278908 --> 0.278616).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5249006748199463
Epoch: 26, Steps: 65 | Train Loss: 0.5439126 Vali Loss: 0.2785988 Test Loss: 0.3983145
Validation loss decreased (0.278616 --> 0.278599).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8101253509521484
Epoch: 27, Steps: 65 | Train Loss: 0.5453237 Vali Loss: 0.2783917 Test Loss: 0.3981948
Validation loss decreased (0.278599 --> 0.278392).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.8229520320892334
Epoch: 28, Steps: 65 | Train Loss: 0.5450160 Vali Loss: 0.2783334 Test Loss: 0.3981092
Validation loss decreased (0.278392 --> 0.278333).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6994340419769287
Epoch: 29, Steps: 65 | Train Loss: 0.5446851 Vali Loss: 0.2781713 Test Loss: 0.3980223
Validation loss decreased (0.278333 --> 0.278171).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4789979457855225
Epoch: 30, Steps: 65 | Train Loss: 0.5442272 Vali Loss: 0.2779428 Test Loss: 0.3979541
Validation loss decreased (0.278171 --> 0.277943).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.736912488937378
Epoch: 31, Steps: 65 | Train Loss: 0.5447964 Vali Loss: 0.2779562 Test Loss: 0.3978587
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.7213826179504395
Epoch: 32, Steps: 65 | Train Loss: 0.5432522 Vali Loss: 0.2778458 Test Loss: 0.3977984
Validation loss decreased (0.277943 --> 0.277846).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.6343870162963867
Epoch: 33, Steps: 65 | Train Loss: 0.5435227 Vali Loss: 0.2776745 Test Loss: 0.3977612
Validation loss decreased (0.277846 --> 0.277675).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.794468879699707
Epoch: 34, Steps: 65 | Train Loss: 0.5436612 Vali Loss: 0.2776555 Test Loss: 0.3976758
Validation loss decreased (0.277675 --> 0.277656).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.5864222049713135
Epoch: 35, Steps: 65 | Train Loss: 0.5447288 Vali Loss: 0.2775462 Test Loss: 0.3976299
Validation loss decreased (0.277656 --> 0.277546).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.4393198490142822
Epoch: 36, Steps: 65 | Train Loss: 0.5439813 Vali Loss: 0.2775134 Test Loss: 0.3975931
Validation loss decreased (0.277546 --> 0.277513).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.7416527271270752
Epoch: 37, Steps: 65 | Train Loss: 0.5422001 Vali Loss: 0.2774346 Test Loss: 0.3975632
Validation loss decreased (0.277513 --> 0.277435).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.1407995223999023
Epoch: 38, Steps: 65 | Train Loss: 0.5435773 Vali Loss: 0.2772506 Test Loss: 0.3975224
Validation loss decreased (0.277435 --> 0.277251).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.6160540580749512
Epoch: 39, Steps: 65 | Train Loss: 0.5439728 Vali Loss: 0.2772758 Test Loss: 0.3974763
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.6201114654541016
Epoch: 40, Steps: 65 | Train Loss: 0.5430057 Vali Loss: 0.2772021 Test Loss: 0.3974402
Validation loss decreased (0.277251 --> 0.277202).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.798902988433838
Epoch: 41, Steps: 65 | Train Loss: 0.5425869 Vali Loss: 0.2770982 Test Loss: 0.3974258
Validation loss decreased (0.277202 --> 0.277098).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.7476961612701416
Epoch: 42, Steps: 65 | Train Loss: 0.5422016 Vali Loss: 0.2770841 Test Loss: 0.3973869
Validation loss decreased (0.277098 --> 0.277084).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.5168907642364502
Epoch: 43, Steps: 65 | Train Loss: 0.5432673 Vali Loss: 0.2770459 Test Loss: 0.3973720
Validation loss decreased (0.277084 --> 0.277046).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.885103702545166
Epoch: 44, Steps: 65 | Train Loss: 0.5424090 Vali Loss: 0.2770115 Test Loss: 0.3973500
Validation loss decreased (0.277046 --> 0.277012).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7162835597991943
Epoch: 45, Steps: 65 | Train Loss: 0.5392662 Vali Loss: 0.2769730 Test Loss: 0.3973331
Validation loss decreased (0.277012 --> 0.276973).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.9707159996032715
Epoch: 46, Steps: 65 | Train Loss: 0.5418625 Vali Loss: 0.2769533 Test Loss: 0.3973070
Validation loss decreased (0.276973 --> 0.276953).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.748201608657837
Epoch: 47, Steps: 65 | Train Loss: 0.5418642 Vali Loss: 0.2768572 Test Loss: 0.3972918
Validation loss decreased (0.276953 --> 0.276857).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.1870691776275635
Epoch: 48, Steps: 65 | Train Loss: 0.5405259 Vali Loss: 0.2768484 Test Loss: 0.3972857
Validation loss decreased (0.276857 --> 0.276848).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.1565325260162354
Epoch: 49, Steps: 65 | Train Loss: 0.5430239 Vali Loss: 0.2768369 Test Loss: 0.3972652
Validation loss decreased (0.276848 --> 0.276837).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.462261438369751
Epoch: 50, Steps: 65 | Train Loss: 0.5421465 Vali Loss: 0.2767296 Test Loss: 0.3972524
Validation loss decreased (0.276837 --> 0.276730).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.5384924411773682
Epoch: 51, Steps: 65 | Train Loss: 0.5434325 Vali Loss: 0.2767547 Test Loss: 0.3972433
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.589569091796875
Epoch: 52, Steps: 65 | Train Loss: 0.5416248 Vali Loss: 0.2767235 Test Loss: 0.3972262
Validation loss decreased (0.276730 --> 0.276724).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.7673332691192627
Epoch: 53, Steps: 65 | Train Loss: 0.5417377 Vali Loss: 0.2766777 Test Loss: 0.3972102
Validation loss decreased (0.276724 --> 0.276678).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5898101329803467
Epoch: 54, Steps: 65 | Train Loss: 0.5422859 Vali Loss: 0.2766362 Test Loss: 0.3972063
Validation loss decreased (0.276678 --> 0.276636).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.6236979961395264
Epoch: 55, Steps: 65 | Train Loss: 0.5403841 Vali Loss: 0.2765949 Test Loss: 0.3971933
Validation loss decreased (0.276636 --> 0.276595).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.450066089630127
Epoch: 56, Steps: 65 | Train Loss: 0.5413925 Vali Loss: 0.2766590 Test Loss: 0.3971784
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.5770862102508545
Epoch: 57, Steps: 65 | Train Loss: 0.5417736 Vali Loss: 0.2766491 Test Loss: 0.3971767
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.617901086807251
Epoch: 58, Steps: 65 | Train Loss: 0.5430519 Vali Loss: 0.2765028 Test Loss: 0.3971622
Validation loss decreased (0.276595 --> 0.276503).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.6167943477630615
Epoch: 59, Steps: 65 | Train Loss: 0.5412787 Vali Loss: 0.2765784 Test Loss: 0.3971594
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9367263317108154
Epoch: 60, Steps: 65 | Train Loss: 0.5427005 Vali Loss: 0.2762343 Test Loss: 0.3971532
Validation loss decreased (0.276503 --> 0.276234).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.679290533065796
Epoch: 61, Steps: 65 | Train Loss: 0.5408643 Vali Loss: 0.2765492 Test Loss: 0.3971481
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6941485404968262
Epoch: 62, Steps: 65 | Train Loss: 0.5421676 Vali Loss: 0.2761908 Test Loss: 0.3971388
Validation loss decreased (0.276234 --> 0.276191).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.5286366939544678
Epoch: 63, Steps: 65 | Train Loss: 0.5417231 Vali Loss: 0.2763739 Test Loss: 0.3971272
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.5987162590026855
Epoch: 64, Steps: 65 | Train Loss: 0.5417288 Vali Loss: 0.2764858 Test Loss: 0.3971265
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.5800135135650635
Epoch: 65, Steps: 65 | Train Loss: 0.5422975 Vali Loss: 0.2764867 Test Loss: 0.3971213
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.795720100402832
Epoch: 66, Steps: 65 | Train Loss: 0.5421013 Vali Loss: 0.2764392 Test Loss: 0.3971153
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.5942635536193848
Epoch: 67, Steps: 65 | Train Loss: 0.5414591 Vali Loss: 0.2764629 Test Loss: 0.3971106
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.4491820335388184
Epoch: 68, Steps: 65 | Train Loss: 0.5426553 Vali Loss: 0.2764721 Test Loss: 0.3971054
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.8372228145599365
Epoch: 69, Steps: 65 | Train Loss: 0.5412783 Vali Loss: 0.2764199 Test Loss: 0.3971037
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.650923728942871
Epoch: 70, Steps: 65 | Train Loss: 0.5417285 Vali Loss: 0.2764427 Test Loss: 0.3970999
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.81632661819458
Epoch: 71, Steps: 65 | Train Loss: 0.5424963 Vali Loss: 0.2763358 Test Loss: 0.3970960
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.743546724319458
Epoch: 72, Steps: 65 | Train Loss: 0.5400251 Vali Loss: 0.2764035 Test Loss: 0.3970959
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.728269100189209
Epoch: 73, Steps: 65 | Train Loss: 0.5385418 Vali Loss: 0.2763738 Test Loss: 0.3970911
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.0529568195343018
Epoch: 74, Steps: 65 | Train Loss: 0.5414574 Vali Loss: 0.2763398 Test Loss: 0.3970871
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.611520767211914
Epoch: 75, Steps: 65 | Train Loss: 0.5420918 Vali Loss: 0.2763807 Test Loss: 0.3970836
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.900742530822754
Epoch: 76, Steps: 65 | Train Loss: 0.5408579 Vali Loss: 0.2762602 Test Loss: 0.3970782
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.769026756286621
Epoch: 77, Steps: 65 | Train Loss: 0.5425690 Vali Loss: 0.2763779 Test Loss: 0.3970778
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.615349292755127
Epoch: 78, Steps: 65 | Train Loss: 0.5409485 Vali Loss: 0.2761091 Test Loss: 0.3970756
Validation loss decreased (0.276191 --> 0.276109).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.506162405014038
Epoch: 79, Steps: 65 | Train Loss: 0.5406120 Vali Loss: 0.2762948 Test Loss: 0.3970754
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.6621205806732178
Epoch: 80, Steps: 65 | Train Loss: 0.5409886 Vali Loss: 0.2763709 Test Loss: 0.3970698
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.460465908050537
Epoch: 81, Steps: 65 | Train Loss: 0.5423098 Vali Loss: 0.2763675 Test Loss: 0.3970715
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.4497730731964111
Epoch: 82, Steps: 65 | Train Loss: 0.5411167 Vali Loss: 0.2763245 Test Loss: 0.3970682
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.8762390613555908
Epoch: 83, Steps: 65 | Train Loss: 0.5419913 Vali Loss: 0.2762490 Test Loss: 0.3970647
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.7594091892242432
Epoch: 84, Steps: 65 | Train Loss: 0.5408249 Vali Loss: 0.2761935 Test Loss: 0.3970639
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.9394700527191162
Epoch: 85, Steps: 65 | Train Loss: 0.5424998 Vali Loss: 0.2763428 Test Loss: 0.3970643
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.0899760723114014
Epoch: 86, Steps: 65 | Train Loss: 0.5422792 Vali Loss: 0.2762922 Test Loss: 0.3970618
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.7038304805755615
Epoch: 87, Steps: 65 | Train Loss: 0.5422233 Vali Loss: 0.2762878 Test Loss: 0.3970602
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.7218501567840576
Epoch: 88, Steps: 65 | Train Loss: 0.5397114 Vali Loss: 0.2763085 Test Loss: 0.3970574
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.5811083316802979
Epoch: 89, Steps: 65 | Train Loss: 0.5403333 Vali Loss: 0.2759486 Test Loss: 0.3970565
Validation loss decreased (0.276109 --> 0.275949).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.109313488006592
Epoch: 90, Steps: 65 | Train Loss: 0.5399583 Vali Loss: 0.2762769 Test Loss: 0.3970558
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.6553950309753418
Epoch: 91, Steps: 65 | Train Loss: 0.5411813 Vali Loss: 0.2762718 Test Loss: 0.3970541
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.6009535789489746
Epoch: 92, Steps: 65 | Train Loss: 0.5408772 Vali Loss: 0.2763147 Test Loss: 0.3970535
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.5677454471588135
Epoch: 93, Steps: 65 | Train Loss: 0.5399999 Vali Loss: 0.2759589 Test Loss: 0.3970518
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.5493888854980469
Epoch: 94, Steps: 65 | Train Loss: 0.5413752 Vali Loss: 0.2762553 Test Loss: 0.3970504
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.577216386795044
Epoch: 95, Steps: 65 | Train Loss: 0.5407567 Vali Loss: 0.2759061 Test Loss: 0.3970495
Validation loss decreased (0.275949 --> 0.275906).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.6355071067810059
Epoch: 96, Steps: 65 | Train Loss: 0.5420099 Vali Loss: 0.2762764 Test Loss: 0.3970489
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.4081604480743408
Epoch: 97, Steps: 65 | Train Loss: 0.5426517 Vali Loss: 0.2762790 Test Loss: 0.3970473
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.3081834316253662
Epoch: 98, Steps: 65 | Train Loss: 0.5406552 Vali Loss: 0.2761465 Test Loss: 0.3970476
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.7174694538116455
Epoch: 99, Steps: 65 | Train Loss: 0.5404124 Vali Loss: 0.2759221 Test Loss: 0.3970464
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.2105162143707275
Epoch: 100, Steps: 65 | Train Loss: 0.5418494 Vali Loss: 0.2762817 Test Loss: 0.3970459
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_90_192_FITS_ETTh2_ftM_sl90_ll48_pl192_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.37907981872558594, mae:0.39170464873313904, rse:0.4937489628791809, corr:[0.2664512  0.27124813 0.26767772 0.26665264 0.26687577 0.2653639
 0.26361218 0.2630864  0.26271322 0.26137942 0.25979358 0.25847033
 0.2570535  0.2556216  0.25452557 0.25399905 0.25344875 0.2526189
 0.25158927 0.25049815 0.24941182 0.24824303 0.24678066 0.24457285
 0.2414126  0.23887262 0.23674469 0.23489788 0.2331465  0.23169757
 0.23074323 0.22924942 0.22769766 0.22652301 0.22583568 0.22464524
 0.22293882 0.22169788 0.22089334 0.22006613 0.21924634 0.21876733
 0.21848308 0.21757716 0.21633346 0.21517351 0.2139362  0.21167803
 0.208      0.20499699 0.20272489 0.20065378 0.19842198 0.1963412
 0.19493882 0.19295008 0.19135152 0.19008963 0.18967153 0.18892932
 0.18773961 0.18718415 0.18725586 0.18728682 0.18672475 0.18591373
 0.18537432 0.18479615 0.18411025 0.18322378 0.18207222 0.18067034
 0.17821832 0.1762102  0.17460437 0.17358024 0.1726132  0.17177492
 0.17141372 0.17064829 0.17046897 0.17024562 0.16998094 0.1696667
 0.16931283 0.16908497 0.1688374  0.16883926 0.16883816 0.16843818
 0.16762862 0.16683272 0.166777   0.1664895  0.16527301 0.16355395
 0.1614276  0.15965311 0.15749137 0.15564026 0.15462838 0.15407462
 0.15391248 0.15294252 0.15275842 0.15297027 0.15299512 0.15249476
 0.15186049 0.15176219 0.15135837 0.1507313  0.15035735 0.15026903
 0.14999515 0.14896747 0.14815249 0.14747144 0.14582616 0.14313696
 0.14030586 0.13868168 0.13700464 0.13530795 0.1338009  0.13314284
 0.13307056 0.13228776 0.13184005 0.1318686  0.13196363 0.13128343
 0.13049546 0.13039316 0.13031809 0.12962525 0.1287677  0.12852508
 0.12842923 0.12762567 0.12676385 0.1263676  0.12534134 0.12279723
 0.11906462 0.11689715 0.11544577 0.11367279 0.11175007 0.11073672
 0.11102986 0.11061852 0.11023192 0.11001094 0.11022405 0.10996114
 0.10944213 0.10955127 0.10991689 0.10997137 0.10944727 0.10904119
 0.10914408 0.108859   0.10847104 0.10824005 0.10787226 0.10611714
 0.10261247 0.10057013 0.09986265 0.09916671 0.0976546  0.09649239
 0.09686638 0.09679116 0.09658424 0.09636996 0.0967471  0.09641831
 0.09556496 0.09540871 0.0957415  0.0959178  0.09491213 0.09417426
 0.09485646 0.09465212 0.09293809 0.09253229 0.09613829 0.09810737]
