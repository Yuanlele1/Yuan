Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=58, out_features=166, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8626688.0
params:  9794.0
Trainable parameters:  9794
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.6208391189575195
Epoch: 1, Steps: 63 | Train Loss: 0.8438045 Vali Loss: 0.4544218 Test Loss: 0.4754280
Validation loss decreased (inf --> 0.454422).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9887089729309082
Epoch: 2, Steps: 63 | Train Loss: 0.7152276 Vali Loss: 0.4136097 Test Loss: 0.4311937
Validation loss decreased (0.454422 --> 0.413610).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.7170894145965576
Epoch: 3, Steps: 63 | Train Loss: 0.6713617 Vali Loss: 0.3969046 Test Loss: 0.4139376
Validation loss decreased (0.413610 --> 0.396905).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3203675746917725
Epoch: 4, Steps: 63 | Train Loss: 0.6534161 Vali Loss: 0.3884184 Test Loss: 0.4065833
Validation loss decreased (0.396905 --> 0.388418).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.5552773475646973
Epoch: 5, Steps: 63 | Train Loss: 0.6434832 Vali Loss: 0.3827229 Test Loss: 0.4029713
Validation loss decreased (0.388418 --> 0.382723).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.8727617263793945
Epoch: 6, Steps: 63 | Train Loss: 0.6405963 Vali Loss: 0.3781486 Test Loss: 0.4009370
Validation loss decreased (0.382723 --> 0.378149).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.8611781597137451
Epoch: 7, Steps: 63 | Train Loss: 0.6364207 Vali Loss: 0.3765849 Test Loss: 0.3996745
Validation loss decreased (0.378149 --> 0.376585).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.770369291305542
Epoch: 8, Steps: 63 | Train Loss: 0.6336191 Vali Loss: 0.3771711 Test Loss: 0.3988105
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.44267201423645
Epoch: 9, Steps: 63 | Train Loss: 0.6315497 Vali Loss: 0.3720580 Test Loss: 0.3981943
Validation loss decreased (0.376585 --> 0.372058).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.533151626586914
Epoch: 10, Steps: 63 | Train Loss: 0.6301193 Vali Loss: 0.3746920 Test Loss: 0.3976475
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.0897469520568848
Epoch: 11, Steps: 63 | Train Loss: 0.6292072 Vali Loss: 0.3708934 Test Loss: 0.3970681
Validation loss decreased (0.372058 --> 0.370893).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.7179133892059326
Epoch: 12, Steps: 63 | Train Loss: 0.6287031 Vali Loss: 0.3697099 Test Loss: 0.3969153
Validation loss decreased (0.370893 --> 0.369710).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.912111520767212
Epoch: 13, Steps: 63 | Train Loss: 0.6261144 Vali Loss: 0.3716230 Test Loss: 0.3963563
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.2221145629882812
Epoch: 14, Steps: 63 | Train Loss: 0.6266299 Vali Loss: 0.3701458 Test Loss: 0.3961135
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.082475423812866
Epoch: 15, Steps: 63 | Train Loss: 0.6266803 Vali Loss: 0.3705312 Test Loss: 0.3959668
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7487237453460693
Epoch: 16, Steps: 63 | Train Loss: 0.6246859 Vali Loss: 0.3676809 Test Loss: 0.3957160
Validation loss decreased (0.369710 --> 0.367681).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.6788504123687744
Epoch: 17, Steps: 63 | Train Loss: 0.6267957 Vali Loss: 0.3694197 Test Loss: 0.3954989
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.2815189361572266
Epoch: 18, Steps: 63 | Train Loss: 0.6240112 Vali Loss: 0.3683802 Test Loss: 0.3953484
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9396748542785645
Epoch: 19, Steps: 63 | Train Loss: 0.6244987 Vali Loss: 0.3679679 Test Loss: 0.3951939
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8689651489257812
Epoch: 20, Steps: 63 | Train Loss: 0.6250233 Vali Loss: 0.3689858 Test Loss: 0.3950740
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.6595792770385742
Epoch: 21, Steps: 63 | Train Loss: 0.6245675 Vali Loss: 0.3698469 Test Loss: 0.3949688
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.3208770751953125
Epoch: 22, Steps: 63 | Train Loss: 0.6247492 Vali Loss: 0.3665747 Test Loss: 0.3947805
Validation loss decreased (0.367681 --> 0.366575).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.801736831665039
Epoch: 23, Steps: 63 | Train Loss: 0.6221479 Vali Loss: 0.3640817 Test Loss: 0.3947130
Validation loss decreased (0.366575 --> 0.364082).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.997612476348877
Epoch: 24, Steps: 63 | Train Loss: 0.6230962 Vali Loss: 0.3683253 Test Loss: 0.3945977
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.696505069732666
Epoch: 25, Steps: 63 | Train Loss: 0.6235694 Vali Loss: 0.3651511 Test Loss: 0.3945488
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.317509412765503
Epoch: 26, Steps: 63 | Train Loss: 0.6222001 Vali Loss: 0.3640738 Test Loss: 0.3944369
Validation loss decreased (0.364082 --> 0.364074).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8799817562103271
Epoch: 27, Steps: 63 | Train Loss: 0.6223576 Vali Loss: 0.3683951 Test Loss: 0.3943520
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.0543956756591797
Epoch: 28, Steps: 63 | Train Loss: 0.6223311 Vali Loss: 0.3651825 Test Loss: 0.3943414
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.9260780811309814
Epoch: 29, Steps: 63 | Train Loss: 0.6234360 Vali Loss: 0.3675227 Test Loss: 0.3942192
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9966583251953125
Epoch: 30, Steps: 63 | Train Loss: 0.6222242 Vali Loss: 0.3689474 Test Loss: 0.3941587
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.8444335460662842
Epoch: 31, Steps: 63 | Train Loss: 0.6205622 Vali Loss: 0.3652400 Test Loss: 0.3941312
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.7484681606292725
Epoch: 32, Steps: 63 | Train Loss: 0.6209589 Vali Loss: 0.3653511 Test Loss: 0.3941032
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.6281626224517822
Epoch: 33, Steps: 63 | Train Loss: 0.6218087 Vali Loss: 0.3670438 Test Loss: 0.3939850
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.9065208435058594
Epoch: 34, Steps: 63 | Train Loss: 0.6202951 Vali Loss: 0.3675666 Test Loss: 0.3940082
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.760681390762329
Epoch: 35, Steps: 63 | Train Loss: 0.6206544 Vali Loss: 0.3672855 Test Loss: 0.3939580
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.6891133785247803
Epoch: 36, Steps: 63 | Train Loss: 0.6194843 Vali Loss: 0.3678659 Test Loss: 0.3938614
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8502120971679688
Epoch: 37, Steps: 63 | Train Loss: 0.6207435 Vali Loss: 0.3644215 Test Loss: 0.3938348
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.1200876235961914
Epoch: 38, Steps: 63 | Train Loss: 0.6211560 Vali Loss: 0.3663563 Test Loss: 0.3938307
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7459962368011475
Epoch: 39, Steps: 63 | Train Loss: 0.6206829 Vali Loss: 0.3649271 Test Loss: 0.3937835
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.5549023151397705
Epoch: 40, Steps: 63 | Train Loss: 0.6187583 Vali Loss: 0.3669915 Test Loss: 0.3937373
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9178681373596191
Epoch: 41, Steps: 63 | Train Loss: 0.6204474 Vali Loss: 0.3660137 Test Loss: 0.3937324
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.268552541732788
Epoch: 42, Steps: 63 | Train Loss: 0.6197065 Vali Loss: 0.3664804 Test Loss: 0.3936972
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.917356014251709
Epoch: 43, Steps: 63 | Train Loss: 0.6195575 Vali Loss: 0.3692643 Test Loss: 0.3936904
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.53108811378479
Epoch: 44, Steps: 63 | Train Loss: 0.6194759 Vali Loss: 0.3662632 Test Loss: 0.3936563
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5548248291015625
Epoch: 45, Steps: 63 | Train Loss: 0.6189916 Vali Loss: 0.3654548 Test Loss: 0.3936252
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.0448176860809326
Epoch: 46, Steps: 63 | Train Loss: 0.6206764 Vali Loss: 0.3633853 Test Loss: 0.3936063
Validation loss decreased (0.364074 --> 0.363385).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.910008430480957
Epoch: 47, Steps: 63 | Train Loss: 0.6203186 Vali Loss: 0.3647156 Test Loss: 0.3936031
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.0947134494781494
Epoch: 48, Steps: 63 | Train Loss: 0.6188640 Vali Loss: 0.3663470 Test Loss: 0.3935642
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.7117745876312256
Epoch: 49, Steps: 63 | Train Loss: 0.6197355 Vali Loss: 0.3657777 Test Loss: 0.3935390
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.9597265720367432
Epoch: 50, Steps: 63 | Train Loss: 0.6181434 Vali Loss: 0.3658022 Test Loss: 0.3935395
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9820530414581299
Epoch: 51, Steps: 63 | Train Loss: 0.6193539 Vali Loss: 0.3662029 Test Loss: 0.3935170
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.0447943210601807
Epoch: 52, Steps: 63 | Train Loss: 0.6206544 Vali Loss: 0.3647182 Test Loss: 0.3935038
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.8577404022216797
Epoch: 53, Steps: 63 | Train Loss: 0.6189635 Vali Loss: 0.3656771 Test Loss: 0.3934947
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.6616902351379395
Epoch: 54, Steps: 63 | Train Loss: 0.6200326 Vali Loss: 0.3657160 Test Loss: 0.3934860
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.5601894855499268
Epoch: 55, Steps: 63 | Train Loss: 0.6210971 Vali Loss: 0.3634886 Test Loss: 0.3934616
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7792606353759766
Epoch: 56, Steps: 63 | Train Loss: 0.6195167 Vali Loss: 0.3648631 Test Loss: 0.3934379
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.9838786125183105
Epoch: 57, Steps: 63 | Train Loss: 0.6193227 Vali Loss: 0.3685636 Test Loss: 0.3934425
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.8169331550598145
Epoch: 58, Steps: 63 | Train Loss: 0.6200595 Vali Loss: 0.3639024 Test Loss: 0.3934378
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.8377065658569336
Epoch: 59, Steps: 63 | Train Loss: 0.6198126 Vali Loss: 0.3675137 Test Loss: 0.3934197
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.6446051597595215
Epoch: 60, Steps: 63 | Train Loss: 0.6181111 Vali Loss: 0.3637370 Test Loss: 0.3934242
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.8301408290863037
Epoch: 61, Steps: 63 | Train Loss: 0.6197472 Vali Loss: 0.3653378 Test Loss: 0.3934049
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.016312837600708
Epoch: 62, Steps: 63 | Train Loss: 0.6191805 Vali Loss: 0.3674637 Test Loss: 0.3934132
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.371910572052002
Epoch: 63, Steps: 63 | Train Loss: 0.6190471 Vali Loss: 0.3651704 Test Loss: 0.3933910
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.998830795288086
Epoch: 64, Steps: 63 | Train Loss: 0.6202874 Vali Loss: 0.3654400 Test Loss: 0.3933915
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.7208473682403564
Epoch: 65, Steps: 63 | Train Loss: 0.6181917 Vali Loss: 0.3675078 Test Loss: 0.3933781
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.0383548736572266
Epoch: 66, Steps: 63 | Train Loss: 0.6204094 Vali Loss: 0.3647697 Test Loss: 0.3933866
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3889429569244385, mae:0.4129883646965027, rse:0.4986346960067749, corr:[0.262585   0.26590505 0.26380238 0.26502123 0.26256058 0.2610547
 0.2609813  0.2591718  0.25788307 0.25724635 0.25574818 0.254276
 0.25315553 0.25187483 0.25112596 0.25043324 0.24950337 0.2488103
 0.24814273 0.24701266 0.24599443 0.24522309 0.24396794 0.2421996
 0.24006012 0.23819332 0.2368109  0.23553723 0.23382933 0.23217714
 0.23070996 0.22902012 0.22720012 0.22587521 0.22484216 0.22348315
 0.22207257 0.22123483 0.22032288 0.21895091 0.21812998 0.21757603
 0.2166015  0.21554051 0.21447198 0.21273316 0.21104304 0.20918514
 0.20615831 0.20350622 0.20187968 0.19983615 0.1974582  0.1960412
 0.19404513 0.1915141  0.1903396  0.18916528 0.18755077 0.18672787
 0.1862945  0.18507035 0.18481204 0.1848615  0.1841477  0.18345864
 0.18290609 0.18183069 0.1812497  0.18116835 0.18008399 0.17859372
 0.17694011 0.17521755 0.17368783 0.17274716 0.17148058 0.17048292
 0.170045   0.16915531 0.16839473 0.16838573 0.16841064 0.16786313
 0.16777979 0.16764955 0.16715316 0.16675363 0.16643384 0.16566865
 0.16543981 0.16542768 0.16508882 0.16487774 0.16488633 0.16399714
 0.16244677 0.16147038 0.1601447  0.15899995 0.15841098 0.15771934
 0.15700532 0.1568923  0.15694417 0.15642855 0.15665196 0.15692702
 0.15665065 0.15614639 0.15600601 0.15534207 0.15476266 0.15469944
 0.15431915 0.1536997  0.15346079 0.15286356 0.15192129 0.15078528
 0.1487906  0.14691557 0.14582808 0.14481945 0.14317565 0.14201447
 0.14120688 0.13998704 0.13907918 0.13859624 0.13792485 0.13725816
 0.13716683 0.13680579 0.13653596 0.13634957 0.13579576 0.13527553
 0.13520476 0.1347564  0.13433632 0.13420267 0.13353409 0.13201585
 0.12953268 0.12752236 0.12588432 0.12484089 0.12342907 0.12232888
 0.12210575 0.1215345  0.12077757 0.12072865 0.12124181 0.12112172
 0.12143041 0.1216564  0.12160565 0.12150533 0.12171541 0.12143495
 0.12109574 0.12115241 0.12090813 0.12071345 0.12069888 0.1196504
 0.11750685 0.11638053 0.11537804 0.11426386 0.11315469 0.11216654
 0.11143927 0.11077735 0.11064707 0.11035573 0.11049973 0.11073099
 0.11092041 0.11088252 0.11122096 0.11139788 0.11124605 0.11120129
 0.11124211 0.11102598 0.11120108 0.11151679 0.1115777  0.11149222
 0.11060654 0.10988533 0.10953837 0.10944447 0.10878561 0.10864701
 0.10877641 0.1083953  0.1086252  0.10920332 0.10952388 0.10945986
 0.1099998  0.1096862  0.1091322  0.1094249  0.10958382 0.10926505
 0.1094903  0.10992061 0.10987544 0.10998832 0.11019167 0.10952974
 0.10841395 0.10747679 0.10641375 0.10591454 0.10534959 0.10486509
 0.10489157 0.10560002 0.10571838 0.10592956 0.10647352 0.10618269
 0.10613113 0.10629386 0.10609957 0.10600404 0.10671826 0.10700537
 0.1070988  0.10783131 0.10841387 0.10873023 0.10908744 0.10872365
 0.1073525  0.10671256 0.10590027 0.10523119 0.10500847 0.10476811
 0.10416094 0.10462874 0.10551845 0.1056281  0.10652464 0.10758517
 0.10831804 0.10880189 0.10922188 0.10943311 0.11006488 0.11088411
 0.11081506 0.1110597  0.11187795 0.1122397  0.11281107 0.11342093
 0.11272025 0.11203905 0.11200572 0.11188566 0.11127993 0.11177149
 0.1119206  0.11175337 0.11262868 0.11305659 0.11311729 0.11376511
 0.11469816 0.1142527  0.11444676 0.11486211 0.11475975 0.11511261
 0.11574876 0.11543337 0.11545082 0.11621492 0.11636893 0.11674707
 0.11680714 0.11617686 0.11537782 0.11546731 0.11460168 0.11391044
 0.11397801 0.11341332 0.1137535  0.11503728 0.11574332 0.11587352
 0.11745556 0.11784494 0.11776327 0.11889552 0.11942177 0.11916845
 0.1200825  0.12083337 0.12030191 0.12092321 0.12194015 0.12122843
 0.12020773 0.11986674 0.11831774 0.11786878 0.11748881 0.11614117
 0.11550581 0.11671834 0.11662116 0.11689705 0.11865054 0.11841464
 0.11956175 0.12097061 0.12008555 0.11986748 0.12196349 0.12037925
 0.11830246 0.11925457 0.11743546 0.11521906 0.1164774  0.10941506]
