Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  48787200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.060650110244751
Epoch: 1, Steps: 56 | Train Loss: 1.0551540 Vali Loss: 0.7918763 Test Loss: 0.4415547
Validation loss decreased (inf --> 0.791876).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.2060532569885254
Epoch: 2, Steps: 56 | Train Loss: 0.9054587 Vali Loss: 0.7455804 Test Loss: 0.4133241
Validation loss decreased (0.791876 --> 0.745580).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.012248992919922
Epoch: 3, Steps: 56 | Train Loss: 0.8686557 Vali Loss: 0.7231034 Test Loss: 0.4031166
Validation loss decreased (0.745580 --> 0.723103).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0246694087982178
Epoch: 4, Steps: 56 | Train Loss: 0.8534732 Vali Loss: 0.7054152 Test Loss: 0.3972613
Validation loss decreased (0.723103 --> 0.705415).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.0766825675964355
Epoch: 5, Steps: 56 | Train Loss: 0.8419383 Vali Loss: 0.6950375 Test Loss: 0.3934010
Validation loss decreased (0.705415 --> 0.695037).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.980823040008545
Epoch: 6, Steps: 56 | Train Loss: 0.8358857 Vali Loss: 0.6843969 Test Loss: 0.3904257
Validation loss decreased (0.695037 --> 0.684397).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.0077009201049805
Epoch: 7, Steps: 56 | Train Loss: 0.8290450 Vali Loss: 0.6796778 Test Loss: 0.3882840
Validation loss decreased (0.684397 --> 0.679678).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.9374058246612549
Epoch: 8, Steps: 56 | Train Loss: 0.8259484 Vali Loss: 0.6747906 Test Loss: 0.3866242
Validation loss decreased (0.679678 --> 0.674791).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.2703444957733154
Epoch: 9, Steps: 56 | Train Loss: 0.8211648 Vali Loss: 0.6754885 Test Loss: 0.3852408
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.094224452972412
Epoch: 10, Steps: 56 | Train Loss: 0.8209770 Vali Loss: 0.6666740 Test Loss: 0.3842058
Validation loss decreased (0.674791 --> 0.666674).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.102893352508545
Epoch: 11, Steps: 56 | Train Loss: 0.8164843 Vali Loss: 0.6692547 Test Loss: 0.3833493
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1043524742126465
Epoch: 12, Steps: 56 | Train Loss: 0.8166548 Vali Loss: 0.6650660 Test Loss: 0.3826995
Validation loss decreased (0.666674 --> 0.665066).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9432621002197266
Epoch: 13, Steps: 56 | Train Loss: 0.8146820 Vali Loss: 0.6650295 Test Loss: 0.3821484
Validation loss decreased (0.665066 --> 0.665030).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.0914223194122314
Epoch: 14, Steps: 56 | Train Loss: 0.8131191 Vali Loss: 0.6624900 Test Loss: 0.3816789
Validation loss decreased (0.665030 --> 0.662490).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.9848461151123047
Epoch: 15, Steps: 56 | Train Loss: 0.8109816 Vali Loss: 0.6634430 Test Loss: 0.3813270
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.9435973167419434
Epoch: 16, Steps: 56 | Train Loss: 0.8096477 Vali Loss: 0.6625537 Test Loss: 0.3810438
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.0448100566864014
Epoch: 17, Steps: 56 | Train Loss: 0.8083892 Vali Loss: 0.6574574 Test Loss: 0.3808063
Validation loss decreased (0.662490 --> 0.657457).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8867278099060059
Epoch: 18, Steps: 56 | Train Loss: 0.8100369 Vali Loss: 0.6602112 Test Loss: 0.3806401
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.960749626159668
Epoch: 19, Steps: 56 | Train Loss: 0.8081589 Vali Loss: 0.6551602 Test Loss: 0.3804457
Validation loss decreased (0.657457 --> 0.655160).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8336541652679443
Epoch: 20, Steps: 56 | Train Loss: 0.8073493 Vali Loss: 0.6562374 Test Loss: 0.3803288
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0434415340423584
Epoch: 21, Steps: 56 | Train Loss: 0.8066845 Vali Loss: 0.6613529 Test Loss: 0.3801789
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.167025327682495
Epoch: 22, Steps: 56 | Train Loss: 0.8065946 Vali Loss: 0.6548565 Test Loss: 0.3800380
Validation loss decreased (0.655160 --> 0.654857).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8829612731933594
Epoch: 23, Steps: 56 | Train Loss: 0.8063705 Vali Loss: 0.6578085 Test Loss: 0.3799955
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.21197247505188
Epoch: 24, Steps: 56 | Train Loss: 0.8067520 Vali Loss: 0.6550415 Test Loss: 0.3799407
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.2206485271453857
Epoch: 25, Steps: 56 | Train Loss: 0.8056459 Vali Loss: 0.6534903 Test Loss: 0.3798576
Validation loss decreased (0.654857 --> 0.653490).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.024874687194824
Epoch: 26, Steps: 56 | Train Loss: 0.8066669 Vali Loss: 0.6521914 Test Loss: 0.3798858
Validation loss decreased (0.653490 --> 0.652191).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.781864881515503
Epoch: 27, Steps: 56 | Train Loss: 0.8054986 Vali Loss: 0.6557181 Test Loss: 0.3798001
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.146289348602295
Epoch: 28, Steps: 56 | Train Loss: 0.8054406 Vali Loss: 0.6533259 Test Loss: 0.3798075
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7761483192443848
Epoch: 29, Steps: 56 | Train Loss: 0.8051028 Vali Loss: 0.6522822 Test Loss: 0.3797558
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9933362007141113
Epoch: 30, Steps: 56 | Train Loss: 0.8059348 Vali Loss: 0.6558875 Test Loss: 0.3796962
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.9945228099822998
Epoch: 31, Steps: 56 | Train Loss: 0.8043616 Vali Loss: 0.6507583 Test Loss: 0.3797124
Validation loss decreased (0.652191 --> 0.650758).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.8145394325256348
Epoch: 32, Steps: 56 | Train Loss: 0.8044014 Vali Loss: 0.6509778 Test Loss: 0.3797014
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.8504908084869385
Epoch: 33, Steps: 56 | Train Loss: 0.8057224 Vali Loss: 0.6497247 Test Loss: 0.3796794
Validation loss decreased (0.650758 --> 0.649725).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.0092270374298096
Epoch: 34, Steps: 56 | Train Loss: 0.8052467 Vali Loss: 0.6536324 Test Loss: 0.3796559
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.0971269607543945
Epoch: 35, Steps: 56 | Train Loss: 0.8026184 Vali Loss: 0.6552425 Test Loss: 0.3796521
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.0413429737091064
Epoch: 36, Steps: 56 | Train Loss: 0.8045538 Vali Loss: 0.6482399 Test Loss: 0.3796522
Validation loss decreased (0.649725 --> 0.648240).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.7867419719696045
Epoch: 37, Steps: 56 | Train Loss: 0.8042248 Vali Loss: 0.6521594 Test Loss: 0.3796586
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.867386817932129
Epoch: 38, Steps: 56 | Train Loss: 0.8028221 Vali Loss: 0.6513751 Test Loss: 0.3796360
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.1293959617614746
Epoch: 39, Steps: 56 | Train Loss: 0.8016287 Vali Loss: 0.6492895 Test Loss: 0.3796425
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.0065412521362305
Epoch: 40, Steps: 56 | Train Loss: 0.8034457 Vali Loss: 0.6533870 Test Loss: 0.3796486
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.10518217086792
Epoch: 41, Steps: 56 | Train Loss: 0.8031882 Vali Loss: 0.6510625 Test Loss: 0.3796258
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.9691367149353027
Epoch: 42, Steps: 56 | Train Loss: 0.8027875 Vali Loss: 0.6499199 Test Loss: 0.3796245
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.1037466526031494
Epoch: 43, Steps: 56 | Train Loss: 0.8031011 Vali Loss: 0.6484094 Test Loss: 0.3796237
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.9961447715759277
Epoch: 44, Steps: 56 | Train Loss: 0.8029856 Vali Loss: 0.6496369 Test Loss: 0.3796187
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.1016783714294434
Epoch: 45, Steps: 56 | Train Loss: 0.8038045 Vali Loss: 0.6553869 Test Loss: 0.3796174
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.2115392684936523
Epoch: 46, Steps: 56 | Train Loss: 0.8005470 Vali Loss: 0.6476311 Test Loss: 0.3796161
Validation loss decreased (0.648240 --> 0.647631).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.8577823638916016
Epoch: 47, Steps: 56 | Train Loss: 0.8024462 Vali Loss: 0.6482192 Test Loss: 0.3796188
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.2810122966766357
Epoch: 48, Steps: 56 | Train Loss: 0.8030550 Vali Loss: 0.6530952 Test Loss: 0.3796198
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9099352359771729
Epoch: 49, Steps: 56 | Train Loss: 0.8017318 Vali Loss: 0.6496485 Test Loss: 0.3796161
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.2387006282806396
Epoch: 50, Steps: 56 | Train Loss: 0.8022375 Vali Loss: 0.6475567 Test Loss: 0.3796279
Validation loss decreased (0.647631 --> 0.647557).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.954404592514038
Epoch: 51, Steps: 56 | Train Loss: 0.8026401 Vali Loss: 0.6522212 Test Loss: 0.3796119
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.072948694229126
Epoch: 52, Steps: 56 | Train Loss: 0.8022358 Vali Loss: 0.6537066 Test Loss: 0.3796214
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.1942782402038574
Epoch: 53, Steps: 56 | Train Loss: 0.8022102 Vali Loss: 0.6491072 Test Loss: 0.3796170
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.8752686977386475
Epoch: 54, Steps: 56 | Train Loss: 0.8036204 Vali Loss: 0.6397753 Test Loss: 0.3796148
Validation loss decreased (0.647557 --> 0.639775).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.011396646499634
Epoch: 55, Steps: 56 | Train Loss: 0.8031667 Vali Loss: 0.6475708 Test Loss: 0.3796118
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.007351875305176
Epoch: 56, Steps: 56 | Train Loss: 0.8013309 Vali Loss: 0.6496279 Test Loss: 0.3796151
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.2227954864501953
Epoch: 57, Steps: 56 | Train Loss: 0.8036478 Vali Loss: 0.6491978 Test Loss: 0.3796190
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.144970178604126
Epoch: 58, Steps: 56 | Train Loss: 0.8008421 Vali Loss: 0.6473696 Test Loss: 0.3796122
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.0548713207244873
Epoch: 59, Steps: 56 | Train Loss: 0.8031225 Vali Loss: 0.6485922 Test Loss: 0.3796275
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.939680576324463
Epoch: 60, Steps: 56 | Train Loss: 0.8024488 Vali Loss: 0.6524664 Test Loss: 0.3796322
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.062640428543091
Epoch: 61, Steps: 56 | Train Loss: 0.8014286 Vali Loss: 0.6466558 Test Loss: 0.3796261
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.0123093128204346
Epoch: 62, Steps: 56 | Train Loss: 0.8014603 Vali Loss: 0.6518217 Test Loss: 0.3796194
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.1665279865264893
Epoch: 63, Steps: 56 | Train Loss: 0.8004375 Vali Loss: 0.6492507 Test Loss: 0.3796315
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.1208038330078125
Epoch: 64, Steps: 56 | Train Loss: 0.8021694 Vali Loss: 0.6479223 Test Loss: 0.3796251
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.091071605682373
Epoch: 65, Steps: 56 | Train Loss: 0.8016950 Vali Loss: 0.6498920 Test Loss: 0.3796269
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.969275951385498
Epoch: 66, Steps: 56 | Train Loss: 0.8013528 Vali Loss: 0.6480716 Test Loss: 0.3796258
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.0839109420776367
Epoch: 67, Steps: 56 | Train Loss: 0.8023845 Vali Loss: 0.6470973 Test Loss: 0.3796224
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.7861168384552002
Epoch: 68, Steps: 56 | Train Loss: 0.8020913 Vali Loss: 0.6481708 Test Loss: 0.3796287
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.8758389949798584
Epoch: 69, Steps: 56 | Train Loss: 0.8036680 Vali Loss: 0.6486848 Test Loss: 0.3796337
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9244849681854248
Epoch: 70, Steps: 56 | Train Loss: 0.8012439 Vali Loss: 0.6500838 Test Loss: 0.3796251
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.976280927658081
Epoch: 71, Steps: 56 | Train Loss: 0.8016047 Vali Loss: 0.6460228 Test Loss: 0.3796332
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.773949384689331
Epoch: 72, Steps: 56 | Train Loss: 0.8010091 Vali Loss: 0.6528344 Test Loss: 0.3796293
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.9505243301391602
Epoch: 73, Steps: 56 | Train Loss: 0.8014606 Vali Loss: 0.6494355 Test Loss: 0.3796310
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.0776662826538086
Epoch: 74, Steps: 56 | Train Loss: 0.8027141 Vali Loss: 0.6482394 Test Loss: 0.3796346
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37822672724723816, mae:0.4233512878417969, rse:0.491566002368927, corr:[ 0.21526441  0.2205239   0.2179691   0.21742797  0.21847606  0.21801989
  0.21635301  0.21550876  0.21511917  0.21392971  0.21219198  0.21084261
  0.20999455  0.20904586  0.20797063  0.20724252  0.2067756   0.20594314
  0.20471059  0.20377108  0.20318298  0.20248567  0.20114969  0.1997665
  0.19863847  0.19773912  0.1965894   0.19544923  0.19474195  0.19429317
  0.19375238  0.19286266  0.1919313   0.19092737  0.18999708  0.18925524
  0.18860717  0.18787235  0.18692483  0.18617262  0.18577304  0.18541972
  0.18478023  0.18405843  0.18341236  0.18291408  0.18202586  0.1804549
  0.17885599  0.1776405   0.17671548  0.17598589  0.17538393  0.17463106
  0.17375137  0.173092    0.17263775  0.17212358  0.17148532  0.17103347
  0.17091955  0.17093211  0.1710145   0.17107722  0.17100587  0.1711167
  0.17127222  0.17134807  0.17129122  0.1711631   0.17091486  0.1705913
  0.17027713  0.16989094  0.1693741   0.16878434  0.16856642  0.16871977
  0.16875774  0.16852108  0.16816238  0.16789684  0.16785106  0.16799095
  0.1680975   0.168046    0.16801174  0.16816773  0.1683818   0.16841748
  0.1682632   0.16825268  0.16856861  0.16883846  0.16888566  0.1687501
  0.16867822  0.1686232   0.16835815  0.16800909  0.16785523  0.16773707
  0.16756845  0.16725849  0.16709521  0.16708653  0.16716003  0.16707925
  0.16686888  0.16674058  0.16670145  0.16647714  0.16612728  0.1660602
  0.16622072  0.16626394  0.16590163  0.1653213   0.16490604  0.16442478
  0.163589    0.1626749   0.16191785  0.16135487  0.16081986  0.16047041
  0.16017717  0.15981084  0.15927562  0.15851773  0.15796015  0.15757182
  0.15713902  0.15640585  0.15572564  0.1554247   0.1551568   0.15470396
  0.15396592  0.15350346  0.15359826  0.15376587  0.15337047  0.15228425
  0.15090543  0.14994586  0.14935958  0.14884192  0.14839743  0.14806718
  0.1477277   0.14733253  0.14698088  0.14657028  0.14602609  0.14533721
  0.1448058   0.14454228  0.1443348   0.143917    0.14342794  0.14346752
  0.1439132   0.14424512  0.14434542  0.14438115  0.14447215  0.14417912
  0.14343041  0.1425393   0.14211157  0.14217645  0.14228044  0.14194718
  0.14114325  0.14026937  0.13957779  0.13899231  0.13831785  0.1378368
  0.13758957  0.13738485  0.1372172   0.1371318   0.13730727  0.13761048
  0.13771386  0.13787802  0.13840409  0.13915507  0.13966756  0.13965409
  0.1395498   0.13978465  0.14003031  0.14000347  0.1399508   0.140116
  0.14048852  0.14041352  0.13988112  0.13940948  0.13945946  0.13953678
  0.13940974  0.13926254  0.13923913  0.13938205  0.13968603  0.14018188
  0.14076017  0.14119697  0.14127764  0.14117914  0.1412587   0.1413877
  0.14113288  0.14035653  0.13964987  0.1396012   0.13985613  0.13995713
  0.13966246  0.13951911  0.13956513  0.13963814  0.13966727  0.1396796
  0.13980033  0.13986458  0.13999566  0.14042196  0.14107496  0.14169367
  0.14214951  0.14264154  0.14333662  0.14399926  0.1443647   0.14453295
  0.14489257  0.14541353  0.14586195  0.14603928  0.1460011   0.14600454
  0.14617677  0.14625168  0.14644125  0.14691901  0.1476481   0.14830916
  0.14881201  0.14940481  0.15013453  0.15093118  0.15152995  0.15217699
  0.15297355  0.15368141  0.15402056  0.15437683  0.15512711  0.15599129
  0.1565857   0.15677752  0.15695426  0.1572767   0.15760571  0.15793097
  0.15825482  0.15850921  0.1586781   0.15885125  0.15925679  0.15985893
  0.16035846  0.16083542  0.16128372  0.16185755  0.16249746  0.16297975
  0.16321583  0.16345902  0.16377948  0.16424505  0.16482912  0.16528016
  0.1654877   0.16538075  0.16515346  0.16512226  0.16515522  0.16514637
  0.16508704  0.16508313  0.16516946  0.16537486  0.16558123  0.1657651
  0.16593528  0.16635141  0.16681646  0.16707368  0.16723955  0.16768162
  0.16813755  0.16846637  0.16855855  0.16872498  0.16914973  0.16942945
  0.16934054  0.16891591  0.16862388  0.16846243  0.16830686  0.1681345
  0.16799963  0.16790795  0.16767734  0.16750309  0.16744706  0.1675197
  0.16746725  0.16738047  0.16777271  0.16848016  0.16917937  0.16946086
  0.16934758  0.16950002  0.17001466  0.17069465  0.17125235  0.17146513
  0.17144799  0.1714204   0.17145075  0.1714006   0.17152476  0.17166461
  0.17172565  0.17165306  0.17165537  0.17182387  0.17178318  0.17180663
  0.17200038  0.17251638  0.17316657  0.17365347  0.1741065   0.1748854
  0.17573322  0.17628017  0.17632398  0.17636955  0.17675461  0.17733186
  0.17773801  0.17783894  0.17778797  0.17774189  0.1778098   0.17807506
  0.17867282  0.17935216  0.17956975  0.17949916  0.17923002  0.17909352
  0.17904963  0.17912023  0.17935702  0.1797354   0.18000804  0.17989312
  0.17957102  0.17941539  0.1794425   0.17939578  0.1793512   0.17951012
  0.17965344  0.17952627  0.17934255  0.17945364  0.17990114  0.18038058
  0.18066835  0.18092747  0.1812736   0.18158261  0.18175153  0.18177849
  0.1819151   0.18180723  0.18164273  0.18143937  0.18112272  0.18089548
  0.18079181  0.18072224  0.18058981  0.18050504  0.18037674  0.18018815
  0.1798192   0.1795959   0.17948589  0.17936102  0.17923588  0.17912655
  0.17910175  0.1791566   0.17912641  0.17896451  0.17877404  0.17858335
  0.17810223  0.1774591   0.1768107   0.17612249  0.17521465  0.17409551
  0.17312768  0.17231603  0.17164095  0.17100133  0.17031042  0.16977146
  0.16923955  0.1687319   0.16842696  0.16802351  0.1674821   0.16688968
  0.16640407  0.16614187  0.16569963  0.16512525  0.16441481  0.16392308
  0.1635206   0.16326669  0.16294847  0.16280606  0.16271396  0.16245367
  0.1619275   0.16146505  0.1610858   0.16100049  0.16108084  0.16110045
  0.16101503  0.16072208  0.16029857  0.16005251  0.1598942   0.15967964
  0.1593867   0.15910783  0.15872519  0.15859161  0.15849338  0.15800545
  0.15741014  0.15690446  0.15671237  0.1564774   0.15605561  0.15561573
  0.15549277  0.15544306  0.1552232   0.15506479  0.15492548  0.15457879
  0.15387248  0.1531183   0.1526326   0.15208003  0.15141377  0.15065667
  0.15002379  0.14939557  0.14867547  0.14777173  0.1470733   0.14660232
  0.14612095  0.14542156  0.14462243  0.14398564  0.14349103  0.14317794
  0.14275765  0.1421928   0.14181519  0.14156651  0.14115584  0.14027545
  0.13918586  0.13827974  0.13744912  0.13657552  0.13596827  0.1357466
  0.13525216  0.13443673  0.13365754  0.1332325   0.13292795  0.13207386
  0.13088326  0.12998018  0.1297208   0.12951387  0.12902969  0.128487
  0.12794256  0.1275209   0.12708254  0.12659387  0.12583373  0.12475154
  0.12339052  0.12223373  0.1214631   0.12044536  0.11912383  0.11771006
  0.11667674  0.11599617  0.11523148  0.11414642  0.11300153  0.11209976
  0.11131278  0.11026986  0.10912469  0.10819379  0.10750968  0.10698766
  0.10609328  0.10523053  0.10478324  0.10442049  0.1038653   0.10267113
  0.10120863  0.09970079  0.09844285  0.09751592  0.09688959  0.09625443
  0.09526568  0.09388224  0.09310225  0.09286104  0.0928303   0.09230141
  0.09149684  0.09099931  0.09086841  0.09040453  0.08962581  0.08882369
  0.08830129  0.08794691  0.08727136  0.08625849  0.08510441  0.08399786
  0.08274778  0.08154995  0.08038892  0.07939633  0.07850518  0.07747386
  0.07639658  0.07549164  0.0747034   0.07384539  0.07284325  0.0720808
  0.07150607  0.07089549  0.0700492   0.06921315  0.06898343  0.06911815
  0.0688002   0.06805681  0.06730335  0.06695309  0.06654052  0.065676
  0.06430526  0.06287556  0.0614271   0.06035648  0.05931842  0.05825978
  0.05681285  0.05518638  0.05386271  0.05325893  0.05301712  0.05214486
  0.05086982  0.0500394   0.05010226  0.05040681  0.05012102  0.04970086
  0.049644    0.05024931  0.05080159  0.05077138  0.04995047  0.04920285
  0.04857719  0.04777914  0.04670308  0.04556391  0.04466252  0.04426496
  0.04400025  0.04352285  0.04283833  0.04221368  0.04168833  0.04157964
  0.04143544  0.04070071  0.03989907  0.03967895  0.03970474  0.03978569
  0.03927946  0.03816354  0.03765034  0.03823663  0.03856326  0.03777149
  0.03612165  0.03473011  0.03409662  0.0336455   0.03319021  0.03229615
  0.03102657  0.02983887  0.02923962  0.02945646  0.02995245  0.02983353
  0.0288084   0.02809552  0.02868792  0.02952582  0.02954075  0.02890435
  0.02888645  0.0298477   0.03041304  0.03014694  0.02954968  0.02936575
  0.02928567  0.02874067  0.02765273  0.02623696  0.02546348  0.02482664
  0.02449318  0.02377786  0.02341673  0.02331336  0.02322488  0.0232234
  0.02333165  0.02252287  0.02156178  0.02142516  0.02242498  0.0235708
  0.02347247  0.02186712  0.0210977   0.02162838  0.02180139  0.01982696
  0.01658618  0.01397797  0.01310075  0.01251495  0.01136117  0.00934536
  0.00765484  0.00659916  0.0066233   0.00694291  0.00607329  0.00431219
  0.00330667  0.00376889  0.005043    0.00417322  0.00192539  0.00221599
  0.00469401  0.00350419 -0.00358587 -0.00789596 -0.00123937  0.00428771]
