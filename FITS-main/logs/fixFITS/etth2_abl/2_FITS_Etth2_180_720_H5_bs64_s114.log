Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=50, out_features=250, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11200000.0
params:  12750.0
Trainable parameters:  12750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.6299872398376465
Epoch: 1, Steps: 60 | Train Loss: 1.0946821 Vali Loss: 0.8130345 Test Loss: 0.5929508
Validation loss decreased (inf --> 0.813035).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.4394590854644775
Epoch: 2, Steps: 60 | Train Loss: 0.9018493 Vali Loss: 0.7382503 Test Loss: 0.5193116
Validation loss decreased (0.813035 --> 0.738250).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.3445639610290527
Epoch: 3, Steps: 60 | Train Loss: 0.8061231 Vali Loss: 0.7042829 Test Loss: 0.4794525
Validation loss decreased (0.738250 --> 0.704283).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.0157859325408936
Epoch: 4, Steps: 60 | Train Loss: 0.7523641 Vali Loss: 0.6778680 Test Loss: 0.4554470
Validation loss decreased (0.704283 --> 0.677868).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.428985834121704
Epoch: 5, Steps: 60 | Train Loss: 0.7206783 Vali Loss: 0.6704544 Test Loss: 0.4414538
Validation loss decreased (0.677868 --> 0.670454).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2085628509521484
Epoch: 6, Steps: 60 | Train Loss: 0.7014664 Vali Loss: 0.6627434 Test Loss: 0.4324363
Validation loss decreased (0.670454 --> 0.662743).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.0353972911834717
Epoch: 7, Steps: 60 | Train Loss: 0.6898498 Vali Loss: 0.6564844 Test Loss: 0.4266711
Validation loss decreased (0.662743 --> 0.656484).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.911314010620117
Epoch: 8, Steps: 60 | Train Loss: 0.6807616 Vali Loss: 0.6524521 Test Loss: 0.4227622
Validation loss decreased (0.656484 --> 0.652452).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.431644916534424
Epoch: 9, Steps: 60 | Train Loss: 0.6778803 Vali Loss: 0.6485084 Test Loss: 0.4200616
Validation loss decreased (0.652452 --> 0.648508).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.351960897445679
Epoch: 10, Steps: 60 | Train Loss: 0.6736749 Vali Loss: 0.6389776 Test Loss: 0.4180730
Validation loss decreased (0.648508 --> 0.638978).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.081219911575317
Epoch: 11, Steps: 60 | Train Loss: 0.6689928 Vali Loss: 0.6425555 Test Loss: 0.4165309
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.709143877029419
Epoch: 12, Steps: 60 | Train Loss: 0.6686497 Vali Loss: 0.6401094 Test Loss: 0.4152640
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.152338027954102
Epoch: 13, Steps: 60 | Train Loss: 0.6671443 Vali Loss: 0.6389858 Test Loss: 0.4143131
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.679776668548584
Epoch: 14, Steps: 60 | Train Loss: 0.6669818 Vali Loss: 0.6418893 Test Loss: 0.4134830
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.573594570159912
Epoch: 15, Steps: 60 | Train Loss: 0.6644660 Vali Loss: 0.6427951 Test Loss: 0.4128033
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.8539364337921143
Epoch: 16, Steps: 60 | Train Loss: 0.6626563 Vali Loss: 0.6338424 Test Loss: 0.4122172
Validation loss decreased (0.638978 --> 0.633842).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.84450626373291
Epoch: 17, Steps: 60 | Train Loss: 0.6626561 Vali Loss: 0.6369927 Test Loss: 0.4117260
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.499469041824341
Epoch: 18, Steps: 60 | Train Loss: 0.6624593 Vali Loss: 0.6371191 Test Loss: 0.4112650
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.066168785095215
Epoch: 19, Steps: 60 | Train Loss: 0.6608781 Vali Loss: 0.6349134 Test Loss: 0.4108953
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.694441795349121
Epoch: 20, Steps: 60 | Train Loss: 0.6600062 Vali Loss: 0.6357691 Test Loss: 0.4105609
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.6440415382385254
Epoch: 21, Steps: 60 | Train Loss: 0.6605052 Vali Loss: 0.6306666 Test Loss: 0.4102725
Validation loss decreased (0.633842 --> 0.630667).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.019328594207764
Epoch: 22, Steps: 60 | Train Loss: 0.6597672 Vali Loss: 0.6373051 Test Loss: 0.4099772
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.985321521759033
Epoch: 23, Steps: 60 | Train Loss: 0.6610869 Vali Loss: 0.6330177 Test Loss: 0.4097363
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.8992230892181396
Epoch: 24, Steps: 60 | Train Loss: 0.6591101 Vali Loss: 0.6318949 Test Loss: 0.4095329
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.0770959854125977
Epoch: 25, Steps: 60 | Train Loss: 0.6585996 Vali Loss: 0.6355213 Test Loss: 0.4093306
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.207456588745117
Epoch: 26, Steps: 60 | Train Loss: 0.6595643 Vali Loss: 0.6353784 Test Loss: 0.4091393
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.074230909347534
Epoch: 27, Steps: 60 | Train Loss: 0.6573028 Vali Loss: 0.6288913 Test Loss: 0.4089855
Validation loss decreased (0.630667 --> 0.628891).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.4851577281951904
Epoch: 28, Steps: 60 | Train Loss: 0.6569136 Vali Loss: 0.6327082 Test Loss: 0.4088500
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.3819005489349365
Epoch: 29, Steps: 60 | Train Loss: 0.6577515 Vali Loss: 0.6319265 Test Loss: 0.4086985
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.2893970012664795
Epoch: 30, Steps: 60 | Train Loss: 0.6570974 Vali Loss: 0.6325815 Test Loss: 0.4085637
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.9787237644195557
Epoch: 31, Steps: 60 | Train Loss: 0.6552550 Vali Loss: 0.6346972 Test Loss: 0.4084187
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.8066365718841553
Epoch: 32, Steps: 60 | Train Loss: 0.6565532 Vali Loss: 0.6328117 Test Loss: 0.4083360
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.691558361053467
Epoch: 33, Steps: 60 | Train Loss: 0.6566166 Vali Loss: 0.6295244 Test Loss: 0.4082223
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.438498020172119
Epoch: 34, Steps: 60 | Train Loss: 0.6570199 Vali Loss: 0.6324805 Test Loss: 0.4081254
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.4562160968780518
Epoch: 35, Steps: 60 | Train Loss: 0.6560294 Vali Loss: 0.6333836 Test Loss: 0.4080534
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.426173686981201
Epoch: 36, Steps: 60 | Train Loss: 0.6576170 Vali Loss: 0.6304079 Test Loss: 0.4079545
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.8783516883850098
Epoch: 37, Steps: 60 | Train Loss: 0.6568074 Vali Loss: 0.6302412 Test Loss: 0.4078984
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.4805209636688232
Epoch: 38, Steps: 60 | Train Loss: 0.6551844 Vali Loss: 0.6327452 Test Loss: 0.4078182
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.2924611568450928
Epoch: 39, Steps: 60 | Train Loss: 0.6551146 Vali Loss: 0.6298475 Test Loss: 0.4077541
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.3407745361328125
Epoch: 40, Steps: 60 | Train Loss: 0.6566716 Vali Loss: 0.6305682 Test Loss: 0.4077029
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.999973773956299
Epoch: 41, Steps: 60 | Train Loss: 0.6548576 Vali Loss: 0.6285357 Test Loss: 0.4076450
Validation loss decreased (0.628891 --> 0.628536).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.0282278060913086
Epoch: 42, Steps: 60 | Train Loss: 0.6556513 Vali Loss: 0.6292599 Test Loss: 0.4075870
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.1765148639678955
Epoch: 43, Steps: 60 | Train Loss: 0.6574472 Vali Loss: 0.6274607 Test Loss: 0.4075366
Validation loss decreased (0.628536 --> 0.627461).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.3301846981048584
Epoch: 44, Steps: 60 | Train Loss: 0.6556709 Vali Loss: 0.6337178 Test Loss: 0.4074853
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.746138095855713
Epoch: 45, Steps: 60 | Train Loss: 0.6545809 Vali Loss: 0.6341319 Test Loss: 0.4074428
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.884650707244873
Epoch: 46, Steps: 60 | Train Loss: 0.6565180 Vali Loss: 0.6328267 Test Loss: 0.4074079
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.4126172065734863
Epoch: 47, Steps: 60 | Train Loss: 0.6543041 Vali Loss: 0.6292048 Test Loss: 0.4073621
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.02851939201355
Epoch: 48, Steps: 60 | Train Loss: 0.6559957 Vali Loss: 0.6307654 Test Loss: 0.4073375
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.2535452842712402
Epoch: 49, Steps: 60 | Train Loss: 0.6564170 Vali Loss: 0.6324670 Test Loss: 0.4072917
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.746552228927612
Epoch: 50, Steps: 60 | Train Loss: 0.6553205 Vali Loss: 0.6320739 Test Loss: 0.4072607
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.613196611404419
Epoch: 51, Steps: 60 | Train Loss: 0.6544838 Vali Loss: 0.6269444 Test Loss: 0.4072287
Validation loss decreased (0.627461 --> 0.626944).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.7197134494781494
Epoch: 52, Steps: 60 | Train Loss: 0.6540912 Vali Loss: 0.6297851 Test Loss: 0.4071932
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.201985836029053
Epoch: 53, Steps: 60 | Train Loss: 0.6562416 Vali Loss: 0.6302256 Test Loss: 0.4071701
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.9309394359588623
Epoch: 54, Steps: 60 | Train Loss: 0.6555006 Vali Loss: 0.6290653 Test Loss: 0.4071440
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.343352317810059
Epoch: 55, Steps: 60 | Train Loss: 0.6548736 Vali Loss: 0.6282868 Test Loss: 0.4071156
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.066340684890747
Epoch: 56, Steps: 60 | Train Loss: 0.6559220 Vali Loss: 0.6291628 Test Loss: 0.4070963
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.499640941619873
Epoch: 57, Steps: 60 | Train Loss: 0.6566674 Vali Loss: 0.6297345 Test Loss: 0.4070673
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.7100751399993896
Epoch: 58, Steps: 60 | Train Loss: 0.6544417 Vali Loss: 0.6290215 Test Loss: 0.4070489
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.0454564094543457
Epoch: 59, Steps: 60 | Train Loss: 0.6553082 Vali Loss: 0.6298981 Test Loss: 0.4070287
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.496530771255493
Epoch: 60, Steps: 60 | Train Loss: 0.6552760 Vali Loss: 0.6285536 Test Loss: 0.4070073
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.537900924682617
Epoch: 61, Steps: 60 | Train Loss: 0.6543570 Vali Loss: 0.6303307 Test Loss: 0.4069912
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.52400541305542
Epoch: 62, Steps: 60 | Train Loss: 0.6548144 Vali Loss: 0.6327607 Test Loss: 0.4069743
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.8027889728546143
Epoch: 63, Steps: 60 | Train Loss: 0.6549720 Vali Loss: 0.6256757 Test Loss: 0.4069552
Validation loss decreased (0.626944 --> 0.625676).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.991407871246338
Epoch: 64, Steps: 60 | Train Loss: 0.6549309 Vali Loss: 0.6289665 Test Loss: 0.4069401
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.629577159881592
Epoch: 65, Steps: 60 | Train Loss: 0.6539783 Vali Loss: 0.6290171 Test Loss: 0.4069241
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.3940658569335938
Epoch: 66, Steps: 60 | Train Loss: 0.6556713 Vali Loss: 0.6259882 Test Loss: 0.4069112
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 4.179709196090698
Epoch: 67, Steps: 60 | Train Loss: 0.6549114 Vali Loss: 0.6318251 Test Loss: 0.4068955
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.559983968734741
Epoch: 68, Steps: 60 | Train Loss: 0.6555063 Vali Loss: 0.6330622 Test Loss: 0.4068869
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.2904529571533203
Epoch: 69, Steps: 60 | Train Loss: 0.6564885 Vali Loss: 0.6291251 Test Loss: 0.4068737
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.7197020053863525
Epoch: 70, Steps: 60 | Train Loss: 0.6544884 Vali Loss: 0.6285247 Test Loss: 0.4068621
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.2825663089752197
Epoch: 71, Steps: 60 | Train Loss: 0.6537462 Vali Loss: 0.6279177 Test Loss: 0.4068497
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.204643487930298
Epoch: 72, Steps: 60 | Train Loss: 0.6554973 Vali Loss: 0.6268349 Test Loss: 0.4068376
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.841921091079712
Epoch: 73, Steps: 60 | Train Loss: 0.6557909 Vali Loss: 0.6274808 Test Loss: 0.4068287
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.4446582794189453
Epoch: 74, Steps: 60 | Train Loss: 0.6533673 Vali Loss: 0.6291261 Test Loss: 0.4068206
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.360386371612549
Epoch: 75, Steps: 60 | Train Loss: 0.6553654 Vali Loss: 0.6295577 Test Loss: 0.4068129
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.0614280700683594
Epoch: 76, Steps: 60 | Train Loss: 0.6534424 Vali Loss: 0.6300037 Test Loss: 0.4068026
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.302530527114868
Epoch: 77, Steps: 60 | Train Loss: 0.6553409 Vali Loss: 0.6319972 Test Loss: 0.4067942
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.9611947536468506
Epoch: 78, Steps: 60 | Train Loss: 0.6549700 Vali Loss: 0.6221676 Test Loss: 0.4067872
Validation loss decreased (0.625676 --> 0.622168).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.378610610961914
Epoch: 79, Steps: 60 | Train Loss: 0.6542622 Vali Loss: 0.6301376 Test Loss: 0.4067785
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.3751888275146484
Epoch: 80, Steps: 60 | Train Loss: 0.6556979 Vali Loss: 0.6251762 Test Loss: 0.4067729
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.357475519180298
Epoch: 81, Steps: 60 | Train Loss: 0.6544334 Vali Loss: 0.6302996 Test Loss: 0.4067676
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.415034770965576
Epoch: 82, Steps: 60 | Train Loss: 0.6541268 Vali Loss: 0.6230842 Test Loss: 0.4067603
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 3.7307932376861572
Epoch: 83, Steps: 60 | Train Loss: 0.6536622 Vali Loss: 0.6275480 Test Loss: 0.4067534
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 4.0067315101623535
Epoch: 84, Steps: 60 | Train Loss: 0.6540377 Vali Loss: 0.6279447 Test Loss: 0.4067473
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 4.439114570617676
Epoch: 85, Steps: 60 | Train Loss: 0.6529938 Vali Loss: 0.6322037 Test Loss: 0.4067434
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 3.8708364963531494
Epoch: 86, Steps: 60 | Train Loss: 0.6546793 Vali Loss: 0.6283218 Test Loss: 0.4067378
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 4.0726563930511475
Epoch: 87, Steps: 60 | Train Loss: 0.6546504 Vali Loss: 0.6272646 Test Loss: 0.4067329
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 4.3629021644592285
Epoch: 88, Steps: 60 | Train Loss: 0.6557980 Vali Loss: 0.6294778 Test Loss: 0.4067276
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 3.682669162750244
Epoch: 89, Steps: 60 | Train Loss: 0.6540016 Vali Loss: 0.6309613 Test Loss: 0.4067247
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.592054843902588
Epoch: 90, Steps: 60 | Train Loss: 0.6552786 Vali Loss: 0.6271440 Test Loss: 0.4067183
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 3.5522191524505615
Epoch: 91, Steps: 60 | Train Loss: 0.6548429 Vali Loss: 0.6267966 Test Loss: 0.4067156
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.9743809700012207
Epoch: 92, Steps: 60 | Train Loss: 0.6555700 Vali Loss: 0.6331943 Test Loss: 0.4067115
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.634979248046875
Epoch: 93, Steps: 60 | Train Loss: 0.6547163 Vali Loss: 0.6279305 Test Loss: 0.4067074
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 3.011960029602051
Epoch: 94, Steps: 60 | Train Loss: 0.6544044 Vali Loss: 0.6272625 Test Loss: 0.4067041
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.644972085952759
Epoch: 95, Steps: 60 | Train Loss: 0.6530344 Vali Loss: 0.6282935 Test Loss: 0.4067007
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.498307943344116
Epoch: 96, Steps: 60 | Train Loss: 0.6547116 Vali Loss: 0.6246054 Test Loss: 0.4066975
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 3.194565773010254
Epoch: 97, Steps: 60 | Train Loss: 0.6559030 Vali Loss: 0.6280544 Test Loss: 0.4066947
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 3.557605504989624
Epoch: 98, Steps: 60 | Train Loss: 0.6546200 Vali Loss: 0.6325876 Test Loss: 0.4066921
EarlyStopping counter: 20 out of 20
Early stopping
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=50, out_features=250, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11200000.0
params:  12750.0
Trainable parameters:  12750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.087078809738159
Epoch: 1, Steps: 60 | Train Loss: 0.8146968 Vali Loss: 0.6260689 Test Loss: 0.4059783
Validation loss decreased (inf --> 0.626069).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.3516459465026855
Epoch: 2, Steps: 60 | Train Loss: 0.8137667 Vali Loss: 0.6302221 Test Loss: 0.4055367
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.554584980010986
Epoch: 3, Steps: 60 | Train Loss: 0.8114324 Vali Loss: 0.6232626 Test Loss: 0.4051975
Validation loss decreased (0.626069 --> 0.623263).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.7356643676757812
Epoch: 4, Steps: 60 | Train Loss: 0.8069609 Vali Loss: 0.6260620 Test Loss: 0.4049931
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.5174920558929443
Epoch: 5, Steps: 60 | Train Loss: 0.8103390 Vali Loss: 0.6260870 Test Loss: 0.4048805
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.9157724380493164
Epoch: 6, Steps: 60 | Train Loss: 0.8113870 Vali Loss: 0.6269352 Test Loss: 0.4047966
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.514371395111084
Epoch: 7, Steps: 60 | Train Loss: 0.8102831 Vali Loss: 0.6264539 Test Loss: 0.4047620
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.113576412200928
Epoch: 8, Steps: 60 | Train Loss: 0.8111192 Vali Loss: 0.6243823 Test Loss: 0.4047005
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.0809085369110107
Epoch: 9, Steps: 60 | Train Loss: 0.8108943 Vali Loss: 0.6256956 Test Loss: 0.4046577
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.7257893085479736
Epoch: 10, Steps: 60 | Train Loss: 0.8099435 Vali Loss: 0.6263292 Test Loss: 0.4045802
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.900217056274414
Epoch: 11, Steps: 60 | Train Loss: 0.8101353 Vali Loss: 0.6235325 Test Loss: 0.4045448
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.485771417617798
Epoch: 12, Steps: 60 | Train Loss: 0.8100173 Vali Loss: 0.6235837 Test Loss: 0.4045679
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.8938851356506348
Epoch: 13, Steps: 60 | Train Loss: 0.8093231 Vali Loss: 0.6208611 Test Loss: 0.4045162
Validation loss decreased (0.623263 --> 0.620861).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.558417558670044
Epoch: 14, Steps: 60 | Train Loss: 0.8100191 Vali Loss: 0.6243164 Test Loss: 0.4045023
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.519355058670044
Epoch: 15, Steps: 60 | Train Loss: 0.8085359 Vali Loss: 0.6252056 Test Loss: 0.4044588
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9400665760040283
Epoch: 16, Steps: 60 | Train Loss: 0.8107176 Vali Loss: 0.6238753 Test Loss: 0.4044797
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.540649175643921
Epoch: 17, Steps: 60 | Train Loss: 0.8099531 Vali Loss: 0.6230104 Test Loss: 0.4044293
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.210355758666992
Epoch: 18, Steps: 60 | Train Loss: 0.8075310 Vali Loss: 0.6234257 Test Loss: 0.4044150
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.966073751449585
Epoch: 19, Steps: 60 | Train Loss: 0.8100147 Vali Loss: 0.6215776 Test Loss: 0.4044155
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.9628658294677734
Epoch: 20, Steps: 60 | Train Loss: 0.8080289 Vali Loss: 0.6254095 Test Loss: 0.4043916
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.5455451011657715
Epoch: 21, Steps: 60 | Train Loss: 0.8091855 Vali Loss: 0.6240313 Test Loss: 0.4043719
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.152942180633545
Epoch: 22, Steps: 60 | Train Loss: 0.8076297 Vali Loss: 0.6237903 Test Loss: 0.4043647
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.441025733947754
Epoch: 23, Steps: 60 | Train Loss: 0.8088140 Vali Loss: 0.6282954 Test Loss: 0.4043491
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.749232292175293
Epoch: 24, Steps: 60 | Train Loss: 0.8087061 Vali Loss: 0.6234781 Test Loss: 0.4043521
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.662489175796509
Epoch: 25, Steps: 60 | Train Loss: 0.8081299 Vali Loss: 0.6237977 Test Loss: 0.4043350
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.188612699508667
Epoch: 26, Steps: 60 | Train Loss: 0.8080315 Vali Loss: 0.6214436 Test Loss: 0.4043465
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.893028497695923
Epoch: 27, Steps: 60 | Train Loss: 0.8079769 Vali Loss: 0.6217868 Test Loss: 0.4043310
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.950146436691284
Epoch: 28, Steps: 60 | Train Loss: 0.8062029 Vali Loss: 0.6227225 Test Loss: 0.4043007
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.463795185089111
Epoch: 29, Steps: 60 | Train Loss: 0.8077860 Vali Loss: 0.6236235 Test Loss: 0.4043186
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.366638422012329
Epoch: 30, Steps: 60 | Train Loss: 0.8086521 Vali Loss: 0.6216865 Test Loss: 0.4042958
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.808602809906006
Epoch: 31, Steps: 60 | Train Loss: 0.8088288 Vali Loss: 0.6216191 Test Loss: 0.4043020
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.755086898803711
Epoch: 32, Steps: 60 | Train Loss: 0.8067940 Vali Loss: 0.6247346 Test Loss: 0.4042858
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.330823659896851
Epoch: 33, Steps: 60 | Train Loss: 0.8083727 Vali Loss: 0.6241665 Test Loss: 0.4042935
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.402755469083786, mae:0.4296022355556488, rse:0.5072551369667053, corr:[ 2.21120253e-01  2.22486809e-01  2.20462218e-01  2.20283449e-01
  2.19271824e-01  2.17460334e-01  2.16945544e-01  2.16489270e-01
  2.15117753e-01  2.13783666e-01  2.13165388e-01  2.12117270e-01
  2.10849628e-01  2.09914908e-01  2.09226564e-01  2.08480984e-01
  2.08002672e-01  2.07597971e-01  2.06963167e-01  2.06008121e-01
  2.05102742e-01  2.04498038e-01  2.03799829e-01  2.02450812e-01
  2.00302407e-01  1.98512718e-01  1.97245762e-01  1.96198687e-01
  1.95008039e-01  1.94065541e-01  1.93137303e-01  1.91886231e-01
  1.90465420e-01  1.89390719e-01  1.88391849e-01  1.87204644e-01
  1.86364442e-01  1.85571074e-01  1.84622318e-01  1.83451891e-01
  1.82919249e-01  1.82732105e-01  1.82372183e-01  1.81588396e-01
  1.80905417e-01  1.80380434e-01  1.79457277e-01  1.77487805e-01
  1.74877122e-01  1.72970936e-01  1.71376437e-01  1.70058101e-01
  1.68675110e-01  1.67667538e-01  1.66672021e-01  1.65760547e-01
  1.65259659e-01  1.64918482e-01  1.64806098e-01  1.64328873e-01
  1.63946480e-01  1.63442820e-01  1.63751096e-01  1.63960963e-01
  1.63875788e-01  1.63335681e-01  1.62932396e-01  1.62920713e-01
  1.63271442e-01  1.63327828e-01  1.62557021e-01  1.61659583e-01
  1.60685912e-01  1.60224020e-01  1.59668788e-01  1.58939257e-01
  1.58328086e-01  1.58382267e-01  1.58463001e-01  1.58306256e-01
  1.58374265e-01  1.58448488e-01  1.58582926e-01  1.58468381e-01
  1.58613354e-01  1.58773333e-01  1.59105957e-01  1.58878416e-01
  1.58485100e-01  1.58307627e-01  1.58476606e-01  1.58370733e-01
  1.58439755e-01  1.58762008e-01  1.58775762e-01  1.58204049e-01
  1.57067731e-01  1.56659082e-01  1.56315252e-01  1.56036064e-01
  1.55583352e-01  1.55044407e-01  1.54790848e-01  1.54813915e-01
  1.55110016e-01  1.55123770e-01  1.55240148e-01  1.55364439e-01
  1.55901045e-01  1.56100154e-01  1.56075135e-01  1.55762866e-01
  1.55644700e-01  1.55578852e-01  1.55391306e-01  1.54825822e-01
  1.54265985e-01  1.54006913e-01  1.53726950e-01  1.52864143e-01
  1.51138663e-01  1.49801970e-01  1.48928732e-01  1.48347408e-01
  1.47480339e-01  1.46738291e-01  1.46324828e-01  1.45884410e-01
  1.45439923e-01  1.44955590e-01  1.44575030e-01  1.43923894e-01
  1.43540055e-01  1.43436298e-01  1.43423408e-01  1.43104851e-01
  1.42758012e-01  1.42368942e-01  1.42075405e-01  1.41633436e-01
  1.41356513e-01  1.41138405e-01  1.40550405e-01  1.39564633e-01
  1.37493283e-01  1.35914385e-01  1.34384722e-01  1.33500233e-01
  1.33083403e-01  1.32906795e-01  1.32564664e-01  1.31927922e-01
  1.31763443e-01  1.32043988e-01  1.32413343e-01  1.32086918e-01
  1.31889388e-01  1.31598994e-01  1.31444409e-01  1.31214231e-01
  1.30808815e-01  1.30321249e-01  1.30314007e-01  1.30575478e-01
  1.30557254e-01  1.30358011e-01  1.29920632e-01  1.28992036e-01
  1.27179846e-01  1.26171276e-01  1.25460610e-01  1.24988914e-01
  1.24521106e-01  1.24198347e-01  1.24014840e-01  1.23581566e-01
  1.23238139e-01  1.22951746e-01  1.23386763e-01  1.23585820e-01
  1.23497874e-01  1.22940987e-01  1.22743540e-01  1.22790307e-01
  1.22718453e-01  1.22371919e-01  1.22408457e-01  1.22458830e-01
  1.22503519e-01  1.22852236e-01  1.23230428e-01  1.23165108e-01
  1.22419447e-01  1.22289576e-01  1.22510076e-01  1.22909240e-01
  1.22980088e-01  1.23040333e-01  1.23330429e-01  1.23285949e-01
  1.23280875e-01  1.23341188e-01  1.24033488e-01  1.24117278e-01
  1.23876661e-01  1.23536080e-01  1.23728767e-01  1.24007851e-01
  1.24077201e-01  1.24203041e-01  1.24413371e-01  1.24643721e-01
  1.24877162e-01  1.25022784e-01  1.24929413e-01  1.24331616e-01
  1.23494096e-01  1.23138651e-01  1.22852683e-01  1.22586966e-01
  1.22203670e-01  1.22259140e-01  1.22202061e-01  1.22265764e-01
  1.22792996e-01  1.23566113e-01  1.24125190e-01  1.24043331e-01
  1.24132924e-01  1.24204569e-01  1.24483250e-01  1.24898784e-01
  1.25402555e-01  1.25540286e-01  1.25480920e-01  1.25601649e-01
  1.26186967e-01  1.26960605e-01  1.27379924e-01  1.27476513e-01
  1.27055943e-01  1.26981258e-01  1.26728266e-01  1.26531824e-01
  1.26209185e-01  1.26156822e-01  1.26218170e-01  1.26742497e-01
  1.27645552e-01  1.28184274e-01  1.28927469e-01  1.29415706e-01
  1.30344212e-01  1.31142065e-01  1.31793335e-01  1.32064328e-01
  1.32106781e-01  1.32311985e-01  1.32801041e-01  1.33382320e-01
  1.34025052e-01  1.34693354e-01  1.35425791e-01  1.35931730e-01
  1.35686114e-01  1.35448962e-01  1.35203466e-01  1.35834172e-01
  1.36431292e-01  1.37070179e-01  1.37355343e-01  1.37958214e-01
  1.39274746e-01  1.40262663e-01  1.40764236e-01  1.40666828e-01
  1.41364232e-01  1.42348409e-01  1.43191159e-01  1.43153474e-01
  1.42939121e-01  1.43351510e-01  1.44267246e-01  1.44924134e-01
  1.45302206e-01  1.45867094e-01  1.46458298e-01  1.46749601e-01
  1.46488950e-01  1.46375611e-01  1.46087706e-01  1.46287635e-01
  1.46726504e-01  1.47301510e-01  1.47294462e-01  1.47150189e-01
  1.48382798e-01  1.49946570e-01  1.50877476e-01  1.50913298e-01
  1.51479930e-01  1.52300313e-01  1.53392524e-01  1.54053599e-01
  1.54068500e-01  1.53936520e-01  1.54163450e-01  1.54639930e-01
  1.54808789e-01  1.54800355e-01  1.55095100e-01  1.55244902e-01
  1.54591277e-01  1.54076770e-01  1.54287025e-01  1.54871300e-01
  1.54690221e-01  1.54513925e-01  1.54497430e-01  1.54844761e-01
  1.54952079e-01  1.55027851e-01  1.55945495e-01  1.57043695e-01
  1.57880887e-01  1.57762229e-01  1.58036098e-01  1.58468410e-01
  1.58789203e-01  1.59178838e-01  1.59905717e-01  1.60375088e-01
  1.60433710e-01  1.60648078e-01  1.60974398e-01  1.61007091e-01
  1.60650060e-01  1.61039174e-01  1.61481380e-01  1.61683798e-01
  1.61756068e-01  1.61991701e-01  1.62338614e-01  1.62475944e-01
  1.62907004e-01  1.63601890e-01  1.64218023e-01  1.64250225e-01
  1.64308742e-01  1.64578840e-01  1.65325180e-01  1.65925398e-01
  1.66231185e-01  1.66544199e-01  1.67023882e-01  1.67373180e-01
  1.67613372e-01  1.68467119e-01  1.69349879e-01  1.69305831e-01
  1.68761119e-01  1.68936267e-01  1.69377342e-01  1.69318289e-01
  1.69121906e-01  1.69657916e-01  1.70480609e-01  1.70988977e-01
  1.71367198e-01  1.72189638e-01  1.73127070e-01  1.73732236e-01
  1.74295023e-01  1.74353227e-01  1.74515828e-01  1.75077096e-01
  1.75899863e-01  1.76194429e-01  1.75912097e-01  1.75833315e-01
  1.76190883e-01  1.76888898e-01  1.77297279e-01  1.77511632e-01
  1.77292451e-01  1.76992983e-01  1.76552564e-01  1.76690638e-01
  1.76953942e-01  1.77267134e-01  1.77789718e-01  1.78469628e-01
  1.79176465e-01  1.79602772e-01  1.80385485e-01  1.80808216e-01
  1.81083351e-01  1.80908069e-01  1.81032151e-01  1.81125775e-01
  1.80866674e-01  1.80578798e-01  1.80610284e-01  1.80660546e-01
  1.80351064e-01  1.79994479e-01  1.79997310e-01  1.80130288e-01
  1.79781437e-01  1.79410815e-01  1.79128364e-01  1.79211915e-01
  1.79234877e-01  1.79185882e-01  1.79076225e-01  1.79161474e-01
  1.79272398e-01  1.79144308e-01  1.79015055e-01  1.78842857e-01
  1.78593040e-01  1.77916199e-01  1.77257434e-01  1.76563159e-01
  1.75859198e-01  1.75098091e-01  1.74600050e-01  1.74383640e-01
  1.74227238e-01  1.74021319e-01  1.73795834e-01  1.73477963e-01
  1.72782093e-01  1.72133192e-01  1.71718493e-01  1.71439946e-01
  1.70685217e-01  1.69641346e-01  1.68998465e-01  1.68781787e-01
  1.68485716e-01  1.67620674e-01  1.66996762e-01  1.66670829e-01
  1.66548699e-01  1.66023612e-01  1.65628985e-01  1.65423661e-01
  1.65003330e-01  1.64392874e-01  1.64224863e-01  1.64610237e-01
  1.64676473e-01  1.64311975e-01  1.63860932e-01  1.63808882e-01
  1.63906991e-01  1.63883373e-01  1.63039133e-01  1.62363261e-01
  1.62125885e-01  1.61905870e-01  1.61075532e-01  1.60359055e-01
  1.60221159e-01  1.60324261e-01  1.60155669e-01  1.59482241e-01
  1.59141853e-01  1.58792928e-01  1.58392325e-01  1.57824337e-01
  1.57386839e-01  1.57007173e-01  1.56578377e-01  1.56022787e-01
  1.55499279e-01  1.55270472e-01  1.54921085e-01  1.54011607e-01
  1.52563423e-01  1.51475877e-01  1.50830641e-01  1.50201157e-01
  1.49194717e-01  1.48122415e-01  1.47045955e-01  1.45947859e-01
  1.45119995e-01  1.44431785e-01  1.44037798e-01  1.43242210e-01
  1.42186552e-01  1.40988618e-01  1.40169695e-01  1.39686823e-01
  1.39235526e-01  1.38612449e-01  1.37895986e-01  1.37539387e-01
  1.37946218e-01  1.38481870e-01  1.37933850e-01  1.36430874e-01
  1.35010824e-01  1.34106487e-01  1.32423848e-01  1.30437374e-01
  1.29608884e-01  1.29790694e-01  1.29263297e-01  1.28320783e-01
  1.27813816e-01  1.27522796e-01  1.26770869e-01  1.25284493e-01
  1.24262556e-01  1.23686127e-01  1.23267695e-01  1.22733973e-01
  1.22502111e-01  1.22333527e-01  1.21611856e-01  1.20648012e-01
  1.20378643e-01  1.21126242e-01  1.21181533e-01  1.19574532e-01
  1.17059879e-01  1.15363263e-01  1.13991410e-01  1.12255417e-01
  1.10032588e-01  1.07716568e-01  1.05566114e-01  1.04095899e-01
  1.03715688e-01  1.03303112e-01  1.02080792e-01  1.00261606e-01
  9.91078466e-02  9.83852670e-02  9.78590548e-02  9.69334319e-02
  9.59573165e-02  9.52317342e-02  9.41050053e-02  9.23152342e-02
  9.07802284e-02  9.06190202e-02  9.06689242e-02  8.87791589e-02
  8.52839053e-02  8.30476284e-02  8.21752176e-02  8.13352764e-02
  8.01245794e-02  7.95884281e-02  7.91134089e-02  7.76047483e-02
  7.63676688e-02  7.61323273e-02  7.60792866e-02  7.47254342e-02
  7.32370764e-02  7.24519789e-02  7.20418170e-02  7.11828172e-02
  7.03253672e-02  6.94490373e-02  6.83334619e-02  6.76958039e-02
  6.83596358e-02  6.93148896e-02  6.85549900e-02  6.61313161e-02
  6.38609976e-02  6.27929792e-02  6.11419566e-02  5.86425476e-02
  5.67327999e-02  5.58928177e-02  5.49054481e-02  5.32504767e-02
  5.18348366e-02  5.10606691e-02  5.05671017e-02  4.98482250e-02
  4.91566211e-02  4.83335741e-02  4.75834422e-02  4.66086343e-02
  4.58096676e-02  4.56045382e-02  4.56403978e-02  4.51015048e-02
  4.41470668e-02  4.40199859e-02  4.40756977e-02  4.25270163e-02
  3.94934267e-02  3.77626494e-02  3.75769995e-02  3.70537862e-02
  3.44296768e-02  3.15290503e-02  2.98267044e-02  2.93146074e-02
  2.86340564e-02  2.73803435e-02  2.63591763e-02  2.59168819e-02
  2.62589529e-02  2.59464253e-02  2.53692809e-02  2.51108203e-02
  2.51922868e-02  2.49209665e-02  2.45648138e-02  2.44164038e-02
  2.46100854e-02  2.50539053e-02  2.46709678e-02  2.30715144e-02
  2.05218922e-02  1.92883667e-02  1.85751691e-02  1.74748227e-02
  1.61056519e-02  1.52993668e-02  1.46702211e-02  1.38502903e-02
  1.38816107e-02  1.45566389e-02  1.49675310e-02  1.43432068e-02
  1.35779083e-02  1.29293799e-02  1.28894420e-02  1.27847753e-02
  1.18613848e-02  1.04525266e-02  9.43370257e-03  9.14420746e-03
  9.65891406e-03  1.01509215e-02  1.01725757e-02  9.46972705e-03
  7.64563680e-03  5.56427287e-03  3.64366523e-03  2.64898571e-03
  1.72113418e-03  6.66972541e-04 -2.52941682e-04 -3.31483549e-04
  1.48699619e-04 -6.86318526e-05 -5.68948453e-04 -1.53020304e-03
 -1.92137028e-03 -2.08733953e-03 -1.92241662e-03 -2.12298264e-03
 -2.48435698e-03 -2.66535603e-03 -2.55563459e-03 -2.99025117e-03
 -3.44018661e-03 -2.28113984e-03 -1.11704960e-03 -2.20641005e-03
 -4.90454491e-03 -6.49010204e-03 -7.54100224e-03 -9.06842854e-03
 -1.06174275e-02 -1.13289934e-02 -1.18865091e-02 -1.24308961e-02
 -1.23193003e-02 -1.20071899e-02 -1.18459761e-02 -1.19748879e-02
 -1.13909421e-02 -1.14135994e-02 -1.15175648e-02 -1.17586134e-02
 -1.17304549e-02 -1.16499495e-02 -1.15937339e-02 -1.17843943e-02
 -1.22531448e-02 -1.17220897e-02 -1.07176527e-02 -1.10145845e-02
 -1.41023248e-02 -1.68317389e-02 -1.76021568e-02 -1.76789276e-02
 -1.91208180e-02 -2.01749615e-02 -2.01869030e-02 -2.01652776e-02
 -2.05824822e-02 -2.09453031e-02 -2.03924198e-02 -2.02585068e-02
 -2.04687715e-02 -2.16358751e-02 -2.15697214e-02 -2.15266105e-02
 -2.21064296e-02 -2.31524147e-02 -2.40602735e-02 -2.42761746e-02
 -2.50063483e-02 -2.67889798e-02 -2.84046754e-02 -2.48268917e-02]
