Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=26, out_features=74, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1723904.0
params:  1998.0
Trainable parameters:  1998
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.139357805252075
Epoch: 1, Steps: 63 | Train Loss: 0.8684542 Vali Loss: 0.4718742 Test Loss: 0.5022541
Validation loss decreased (inf --> 0.471874).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.4681167602539062
Epoch: 2, Steps: 63 | Train Loss: 0.7542387 Vali Loss: 0.4313104 Test Loss: 0.4546568
Validation loss decreased (0.471874 --> 0.431310).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.2712290287017822
Epoch: 3, Steps: 63 | Train Loss: 0.7022014 Vali Loss: 0.4085411 Test Loss: 0.4309872
Validation loss decreased (0.431310 --> 0.408541).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9313395023345947
Epoch: 4, Steps: 63 | Train Loss: 0.6723938 Vali Loss: 0.3967379 Test Loss: 0.4184185
Validation loss decreased (0.408541 --> 0.396738).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.0438435077667236
Epoch: 5, Steps: 63 | Train Loss: 0.6566953 Vali Loss: 0.3905712 Test Loss: 0.4113199
Validation loss decreased (0.396738 --> 0.390571).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.5632576942443848
Epoch: 6, Steps: 63 | Train Loss: 0.6504131 Vali Loss: 0.3855226 Test Loss: 0.4070477
Validation loss decreased (0.390571 --> 0.385523).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.861476182937622
Epoch: 7, Steps: 63 | Train Loss: 0.6445607 Vali Loss: 0.3804865 Test Loss: 0.4044685
Validation loss decreased (0.385523 --> 0.380486).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8602533340454102
Epoch: 8, Steps: 63 | Train Loss: 0.6397540 Vali Loss: 0.3801208 Test Loss: 0.4028548
Validation loss decreased (0.380486 --> 0.380121).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.9845938682556152
Epoch: 9, Steps: 63 | Train Loss: 0.6355254 Vali Loss: 0.3783389 Test Loss: 0.4015941
Validation loss decreased (0.380121 --> 0.378339).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.224174737930298
Epoch: 10, Steps: 63 | Train Loss: 0.6352007 Vali Loss: 0.3764367 Test Loss: 0.4005275
Validation loss decreased (0.378339 --> 0.376437).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.7431795597076416
Epoch: 11, Steps: 63 | Train Loss: 0.6337441 Vali Loss: 0.3762768 Test Loss: 0.3998320
Validation loss decreased (0.376437 --> 0.376277).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9347350597381592
Epoch: 12, Steps: 63 | Train Loss: 0.6312255 Vali Loss: 0.3749560 Test Loss: 0.3993127
Validation loss decreased (0.376277 --> 0.374956).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.2385900020599365
Epoch: 13, Steps: 63 | Train Loss: 0.6278073 Vali Loss: 0.3746069 Test Loss: 0.3988773
Validation loss decreased (0.374956 --> 0.374607).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.4734811782836914
Epoch: 14, Steps: 63 | Train Loss: 0.6276957 Vali Loss: 0.3725817 Test Loss: 0.3983918
Validation loss decreased (0.374607 --> 0.372582).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.154864549636841
Epoch: 15, Steps: 63 | Train Loss: 0.6274583 Vali Loss: 0.3759687 Test Loss: 0.3981039
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.879455089569092
Epoch: 16, Steps: 63 | Train Loss: 0.6302586 Vali Loss: 0.3745763 Test Loss: 0.3977717
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.3577420711517334
Epoch: 17, Steps: 63 | Train Loss: 0.6282871 Vali Loss: 0.3738635 Test Loss: 0.3975078
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.5659196376800537
Epoch: 18, Steps: 63 | Train Loss: 0.6283268 Vali Loss: 0.3713323 Test Loss: 0.3973859
Validation loss decreased (0.372582 --> 0.371332).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.2835307121276855
Epoch: 19, Steps: 63 | Train Loss: 0.6289046 Vali Loss: 0.3728411 Test Loss: 0.3970860
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9252042770385742
Epoch: 20, Steps: 63 | Train Loss: 0.6253194 Vali Loss: 0.3726758 Test Loss: 0.3969277
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.12522554397583
Epoch: 21, Steps: 63 | Train Loss: 0.6278270 Vali Loss: 0.3679834 Test Loss: 0.3967436
Validation loss decreased (0.371332 --> 0.367983).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.794848918914795
Epoch: 22, Steps: 63 | Train Loss: 0.6268136 Vali Loss: 0.3710094 Test Loss: 0.3966780
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.119917869567871
Epoch: 23, Steps: 63 | Train Loss: 0.6264767 Vali Loss: 0.3689524 Test Loss: 0.3965087
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.408656358718872
Epoch: 24, Steps: 63 | Train Loss: 0.6265517 Vali Loss: 0.3695925 Test Loss: 0.3964105
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8433449268341064
Epoch: 25, Steps: 63 | Train Loss: 0.6263492 Vali Loss: 0.3703391 Test Loss: 0.3962933
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7163095474243164
Epoch: 26, Steps: 63 | Train Loss: 0.6262200 Vali Loss: 0.3702115 Test Loss: 0.3962043
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.8401687145233154
Epoch: 27, Steps: 63 | Train Loss: 0.6245020 Vali Loss: 0.3726526 Test Loss: 0.3960240
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.985572338104248
Epoch: 28, Steps: 63 | Train Loss: 0.6263773 Vali Loss: 0.3731251 Test Loss: 0.3960535
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7348754405975342
Epoch: 29, Steps: 63 | Train Loss: 0.6238615 Vali Loss: 0.3701268 Test Loss: 0.3959695
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.438288450241089
Epoch: 30, Steps: 63 | Train Loss: 0.6254633 Vali Loss: 0.3688823 Test Loss: 0.3959076
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.316530466079712
Epoch: 31, Steps: 63 | Train Loss: 0.6253777 Vali Loss: 0.3660111 Test Loss: 0.3958244
Validation loss decreased (0.367983 --> 0.366011).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.4001240730285645
Epoch: 32, Steps: 63 | Train Loss: 0.6246829 Vali Loss: 0.3691149 Test Loss: 0.3957322
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.4729294776916504
Epoch: 33, Steps: 63 | Train Loss: 0.6244528 Vali Loss: 0.3692710 Test Loss: 0.3957122
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.985193967819214
Epoch: 34, Steps: 63 | Train Loss: 0.6241147 Vali Loss: 0.3704881 Test Loss: 0.3956273
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.761740207672119
Epoch: 35, Steps: 63 | Train Loss: 0.6236285 Vali Loss: 0.3707391 Test Loss: 0.3955981
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.0430781841278076
Epoch: 36, Steps: 63 | Train Loss: 0.6244347 Vali Loss: 0.3674721 Test Loss: 0.3955459
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.1689746379852295
Epoch: 37, Steps: 63 | Train Loss: 0.6244433 Vali Loss: 0.3714732 Test Loss: 0.3954853
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.4672467708587646
Epoch: 38, Steps: 63 | Train Loss: 0.6231052 Vali Loss: 0.3679927 Test Loss: 0.3954584
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.8385295867919922
Epoch: 39, Steps: 63 | Train Loss: 0.6221729 Vali Loss: 0.3691519 Test Loss: 0.3954315
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.305647850036621
Epoch: 40, Steps: 63 | Train Loss: 0.6249622 Vali Loss: 0.3684321 Test Loss: 0.3954075
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.123969078063965
Epoch: 41, Steps: 63 | Train Loss: 0.6255838 Vali Loss: 0.3690652 Test Loss: 0.3953815
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.8872814178466797
Epoch: 42, Steps: 63 | Train Loss: 0.6226664 Vali Loss: 0.3698433 Test Loss: 0.3953409
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.86922025680542
Epoch: 43, Steps: 63 | Train Loss: 0.6245861 Vali Loss: 0.3706362 Test Loss: 0.3953120
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.2916259765625
Epoch: 44, Steps: 63 | Train Loss: 0.6246361 Vali Loss: 0.3699265 Test Loss: 0.3952917
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.315247058868408
Epoch: 45, Steps: 63 | Train Loss: 0.6237426 Vali Loss: 0.3679048 Test Loss: 0.3952447
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.3733558654785156
Epoch: 46, Steps: 63 | Train Loss: 0.6250156 Vali Loss: 0.3700961 Test Loss: 0.3952435
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.3715009689331055
Epoch: 47, Steps: 63 | Train Loss: 0.6232370 Vali Loss: 0.3666109 Test Loss: 0.3952060
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.621210813522339
Epoch: 48, Steps: 63 | Train Loss: 0.6242359 Vali Loss: 0.3681008 Test Loss: 0.3952107
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.055701971054077
Epoch: 49, Steps: 63 | Train Loss: 0.6231371 Vali Loss: 0.3687604 Test Loss: 0.3951785
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.832892894744873
Epoch: 50, Steps: 63 | Train Loss: 0.6233805 Vali Loss: 0.3697353 Test Loss: 0.3951795
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.998070478439331
Epoch: 51, Steps: 63 | Train Loss: 0.6216001 Vali Loss: 0.3698836 Test Loss: 0.3951637
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3911674916744232, mae:0.4145926833152771, rse:0.5000585913658142, corr:[0.2606603  0.26491237 0.26510975 0.26262495 0.26035556 0.25949308
 0.25943568 0.25898573 0.25761828 0.25551072 0.25357273 0.25221655
 0.25161755 0.25105414 0.25026312 0.24915753 0.2479663  0.24692675
 0.24626516 0.24575672 0.24503957 0.24384226 0.24208277 0.24008058
 0.23803373 0.23636301 0.23489954 0.23353113 0.23203854 0.23051412
 0.22892623 0.22726488 0.22555101 0.22393255 0.22252014 0.22129679
 0.22020994 0.21908736 0.21800411 0.21683033 0.21570529 0.2146271
 0.2137253  0.21288498 0.21183248 0.21023358 0.20788714 0.20502596
 0.20203462 0.1996988  0.19790576 0.19636367 0.19457553 0.19259845
 0.19047548 0.18852612 0.18716922 0.18618585 0.18546833 0.18455057
 0.18351023 0.18230219 0.1814067  0.18081202 0.18056707 0.18047683
 0.18011715 0.17936479 0.17828175 0.17725632 0.1761707  0.17514598
 0.17384614 0.17244074 0.17089395 0.1694203  0.16826196 0.16760282
 0.16715781 0.16657665 0.1660258  0.16542067 0.16493215 0.16459516
 0.16457714 0.1645038  0.16432214 0.16374807 0.16293107 0.16207372
 0.16161458 0.16156836 0.1618963  0.16214243 0.1618092  0.16093186
 0.15948279 0.15806973 0.15685247 0.15606205 0.15533811 0.15458128
 0.15392627 0.15334359 0.1531799  0.15317993 0.1534687  0.15356578
 0.15346909 0.15277024 0.15187263 0.1510466  0.15056917 0.15053673
 0.15071787 0.15072367 0.15028462 0.14927492 0.14784837 0.14631483
 0.1447386  0.14336608 0.14204516 0.14095479 0.13984506 0.13878284
 0.13781843 0.13691935 0.13604502 0.13514839 0.13442492 0.13379917
 0.13339181 0.13289352 0.13241361 0.13187782 0.13140742 0.13097146
 0.13064718 0.13043451 0.1302987  0.12991843 0.12895897 0.12751228
 0.1255131  0.12377342 0.12211408 0.12083626 0.1198394  0.1191067
 0.11856643 0.11795577 0.11752984 0.11731482 0.11743859 0.11748352
 0.11781911 0.11792313 0.11774622 0.11719743 0.11663717 0.11618615
 0.11610407 0.11641461 0.11682191 0.11691714 0.11633095 0.11512392
 0.11336733 0.11196508 0.11065511 0.10969484 0.10887964 0.10804848
 0.10736132 0.10666028 0.10629433 0.1059847  0.10618255 0.10651269
 0.10698789 0.10714961 0.10703225 0.10678507 0.10663033 0.10662587
 0.10686231 0.10713539 0.10733239 0.10743815 0.10730508 0.10704353
 0.1066691  0.1065978  0.10637094 0.1061206  0.10557273 0.10501759
 0.10481557 0.10502251 0.10560545 0.10598414 0.10628837 0.10607529
 0.10590062 0.10573169 0.10583364 0.10602269 0.10606242 0.10603653
 0.10597771 0.10618214 0.10658106 0.10693366 0.10701609 0.10661729
 0.10562032 0.10429301 0.10296843 0.10216592 0.1016541  0.1017278
 0.10190459 0.10219165 0.10228905 0.10224608 0.10240503 0.1025805
 0.10287447 0.10281599 0.1026985  0.10262564 0.10285927 0.10341895
 0.10417551 0.10498145 0.10548431 0.10557286 0.10513135 0.10452002
 0.10362626 0.10313773 0.10261387 0.10222252 0.10160276 0.10098939
 0.10065684 0.10107164 0.10206737 0.10295708 0.10383645 0.10420076
 0.10459564 0.10504247 0.1056039  0.10625047 0.10675392 0.107209
 0.1073157  0.10746995 0.10800093 0.1086865  0.10929041 0.10961335
 0.1094016  0.10894579 0.10827351 0.10800956 0.10795656 0.10852487
 0.10922816 0.10962077 0.10971227 0.10946625 0.10957006 0.10985591
 0.11071617 0.1112723  0.1116197  0.11152659 0.11144312 0.11166593
 0.11229976 0.11295912 0.11340493 0.11346129 0.11303529 0.1127466
 0.11234242 0.11233824 0.11228085 0.11231306 0.11173643 0.11093041
 0.11025888 0.11006349 0.11111668 0.11239954 0.1136923  0.11424638
 0.11445972 0.11432969 0.11476895 0.11582662 0.11689943 0.1175351
 0.11744784 0.11708292 0.11688638 0.11716249 0.11786939 0.11854815
 0.11836069 0.11754966 0.11598131 0.11460349 0.11368591 0.1138621
 0.11434533 0.11493254 0.11503919 0.1145007  0.11450125 0.1154348
 0.1176877  0.11898115 0.11903401 0.11789756 0.11680941 0.11653093
 0.11742631 0.11849238 0.11847971 0.1162767  0.1116358  0.10647798]
