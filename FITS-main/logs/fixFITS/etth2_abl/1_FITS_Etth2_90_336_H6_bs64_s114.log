Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=34, out_features=160, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4874240.0
params:  5600.0
Trainable parameters:  5600
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.949432134628296
Epoch: 1, Steps: 64 | Train Loss: 0.8981756 Vali Loss: 0.4686984 Test Loss: 0.5458292
Validation loss decreased (inf --> 0.468698).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.0625858306884766
Epoch: 2, Steps: 64 | Train Loss: 0.7756238 Vali Loss: 0.4250394 Test Loss: 0.4895767
Validation loss decreased (0.468698 --> 0.425039).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.259233474731445
Epoch: 3, Steps: 64 | Train Loss: 0.7237043 Vali Loss: 0.4035964 Test Loss: 0.4627100
Validation loss decreased (0.425039 --> 0.403596).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.111431360244751
Epoch: 4, Steps: 64 | Train Loss: 0.6964955 Vali Loss: 0.3906229 Test Loss: 0.4486822
Validation loss decreased (0.403596 --> 0.390623).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.097195625305176
Epoch: 5, Steps: 64 | Train Loss: 0.6834176 Vali Loss: 0.3788055 Test Loss: 0.4409260
Validation loss decreased (0.390623 --> 0.378805).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.9290101528167725
Epoch: 6, Steps: 64 | Train Loss: 0.6740187 Vali Loss: 0.3787158 Test Loss: 0.4361492
Validation loss decreased (0.378805 --> 0.378716).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.404783010482788
Epoch: 7, Steps: 64 | Train Loss: 0.6713505 Vali Loss: 0.3742101 Test Loss: 0.4332630
Validation loss decreased (0.378716 --> 0.374210).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.7625796794891357
Epoch: 8, Steps: 64 | Train Loss: 0.6671300 Vali Loss: 0.3729028 Test Loss: 0.4312176
Validation loss decreased (0.374210 --> 0.372903).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.286653995513916
Epoch: 9, Steps: 64 | Train Loss: 0.6650368 Vali Loss: 0.3718773 Test Loss: 0.4299914
Validation loss decreased (0.372903 --> 0.371877).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.302518844604492
Epoch: 10, Steps: 64 | Train Loss: 0.6626612 Vali Loss: 0.3730062 Test Loss: 0.4289704
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.48671555519104
Epoch: 11, Steps: 64 | Train Loss: 0.6606722 Vali Loss: 0.3741376 Test Loss: 0.4281909
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.399909973144531
Epoch: 12, Steps: 64 | Train Loss: 0.6610334 Vali Loss: 0.3750137 Test Loss: 0.4275746
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.034396171569824
Epoch: 13, Steps: 64 | Train Loss: 0.6597977 Vali Loss: 0.3711207 Test Loss: 0.4271278
Validation loss decreased (0.371877 --> 0.371121).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.80932879447937
Epoch: 14, Steps: 64 | Train Loss: 0.6573946 Vali Loss: 0.3702241 Test Loss: 0.4266997
Validation loss decreased (0.371121 --> 0.370224).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.7070014476776123
Epoch: 15, Steps: 64 | Train Loss: 0.6585350 Vali Loss: 0.3680646 Test Loss: 0.4263661
Validation loss decreased (0.370224 --> 0.368065).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.1185035705566406
Epoch: 16, Steps: 64 | Train Loss: 0.6573698 Vali Loss: 0.3691771 Test Loss: 0.4260669
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.090751886367798
Epoch: 17, Steps: 64 | Train Loss: 0.6574605 Vali Loss: 0.3677648 Test Loss: 0.4257352
Validation loss decreased (0.368065 --> 0.367765).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.911073684692383
Epoch: 18, Steps: 64 | Train Loss: 0.6560086 Vali Loss: 0.3673799 Test Loss: 0.4255762
Validation loss decreased (0.367765 --> 0.367380).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.5431816577911377
Epoch: 19, Steps: 64 | Train Loss: 0.6561348 Vali Loss: 0.3690247 Test Loss: 0.4253595
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.341386079788208
Epoch: 20, Steps: 64 | Train Loss: 0.6550525 Vali Loss: 0.3674114 Test Loss: 0.4251503
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.435831308364868
Epoch: 21, Steps: 64 | Train Loss: 0.6554672 Vali Loss: 0.3693413 Test Loss: 0.4249541
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.6561248302459717
Epoch: 22, Steps: 64 | Train Loss: 0.6525998 Vali Loss: 0.3694061 Test Loss: 0.4248315
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.9617919921875
Epoch: 23, Steps: 64 | Train Loss: 0.6532045 Vali Loss: 0.3690011 Test Loss: 0.4246963
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.082599878311157
Epoch: 24, Steps: 64 | Train Loss: 0.6546526 Vali Loss: 0.3680526 Test Loss: 0.4245237
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.920870304107666
Epoch: 25, Steps: 64 | Train Loss: 0.6542163 Vali Loss: 0.3696075 Test Loss: 0.4244091
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.9784159660339355
Epoch: 26, Steps: 64 | Train Loss: 0.6535786 Vali Loss: 0.3657474 Test Loss: 0.4243263
Validation loss decreased (0.367380 --> 0.365747).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.6339144706726074
Epoch: 27, Steps: 64 | Train Loss: 0.6535740 Vali Loss: 0.3664918 Test Loss: 0.4242299
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.989182472229004
Epoch: 28, Steps: 64 | Train Loss: 0.6537142 Vali Loss: 0.3654211 Test Loss: 0.4241358
Validation loss decreased (0.365747 --> 0.365421).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.7463550567626953
Epoch: 29, Steps: 64 | Train Loss: 0.6536662 Vali Loss: 0.3661523 Test Loss: 0.4240800
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.934354543685913
Epoch: 30, Steps: 64 | Train Loss: 0.6527834 Vali Loss: 0.3655080 Test Loss: 0.4239696
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.927389144897461
Epoch: 31, Steps: 64 | Train Loss: 0.6511116 Vali Loss: 0.3690435 Test Loss: 0.4239118
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.4420175552368164
Epoch: 32, Steps: 64 | Train Loss: 0.6518969 Vali Loss: 0.3651745 Test Loss: 0.4238213
Validation loss decreased (0.365421 --> 0.365175).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.7047343254089355
Epoch: 33, Steps: 64 | Train Loss: 0.6521770 Vali Loss: 0.3643929 Test Loss: 0.4237682
Validation loss decreased (0.365175 --> 0.364393).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.1271371841430664
Epoch: 34, Steps: 64 | Train Loss: 0.6520914 Vali Loss: 0.3630498 Test Loss: 0.4237091
Validation loss decreased (0.364393 --> 0.363050).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.8825793266296387
Epoch: 35, Steps: 64 | Train Loss: 0.6503925 Vali Loss: 0.3664005 Test Loss: 0.4236402
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.9517335891723633
Epoch: 36, Steps: 64 | Train Loss: 0.6515565 Vali Loss: 0.3649004 Test Loss: 0.4236265
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.991004467010498
Epoch: 37, Steps: 64 | Train Loss: 0.6518237 Vali Loss: 0.3655681 Test Loss: 0.4235750
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 5.118957281112671
Epoch: 38, Steps: 64 | Train Loss: 0.6514789 Vali Loss: 0.3647145 Test Loss: 0.4235259
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.626568078994751
Epoch: 39, Steps: 64 | Train Loss: 0.6515270 Vali Loss: 0.3675916 Test Loss: 0.4234993
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.773538589477539
Epoch: 40, Steps: 64 | Train Loss: 0.6522514 Vali Loss: 0.3666984 Test Loss: 0.4234357
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.0710744857788086
Epoch: 41, Steps: 64 | Train Loss: 0.6500113 Vali Loss: 0.3674855 Test Loss: 0.4233894
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.3164827823638916
Epoch: 42, Steps: 64 | Train Loss: 0.6512270 Vali Loss: 0.3671443 Test Loss: 0.4233739
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.8770508766174316
Epoch: 43, Steps: 64 | Train Loss: 0.6514391 Vali Loss: 0.3645884 Test Loss: 0.4233584
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.3258373737335205
Epoch: 44, Steps: 64 | Train Loss: 0.6521240 Vali Loss: 0.3644437 Test Loss: 0.4233140
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.3487002849578857
Epoch: 45, Steps: 64 | Train Loss: 0.6517535 Vali Loss: 0.3674982 Test Loss: 0.4232914
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.861865520477295
Epoch: 46, Steps: 64 | Train Loss: 0.6506188 Vali Loss: 0.3657481 Test Loss: 0.4232605
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.5781819820404053
Epoch: 47, Steps: 64 | Train Loss: 0.6515343 Vali Loss: 0.3632171 Test Loss: 0.4232358
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.390218734741211
Epoch: 48, Steps: 64 | Train Loss: 0.6513970 Vali Loss: 0.3671987 Test Loss: 0.4232001
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.2899608612060547
Epoch: 49, Steps: 64 | Train Loss: 0.6489197 Vali Loss: 0.3648927 Test Loss: 0.4231832
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.1965560913085938
Epoch: 50, Steps: 64 | Train Loss: 0.6511718 Vali Loss: 0.3668789 Test Loss: 0.4231645
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.415605068206787
Epoch: 51, Steps: 64 | Train Loss: 0.6511172 Vali Loss: 0.3649625 Test Loss: 0.4231544
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.318493366241455
Epoch: 52, Steps: 64 | Train Loss: 0.6504542 Vali Loss: 0.3649333 Test Loss: 0.4231284
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 5.461015701293945
Epoch: 53, Steps: 64 | Train Loss: 0.6503632 Vali Loss: 0.3626586 Test Loss: 0.4231062
Validation loss decreased (0.363050 --> 0.362659).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 4.5940258502960205
Epoch: 54, Steps: 64 | Train Loss: 0.6495141 Vali Loss: 0.3653634 Test Loss: 0.4230868
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.2726216316223145
Epoch: 55, Steps: 64 | Train Loss: 0.6508174 Vali Loss: 0.3659969 Test Loss: 0.4230780
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 4.008016347885132
Epoch: 56, Steps: 64 | Train Loss: 0.6514080 Vali Loss: 0.3661832 Test Loss: 0.4230618
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.495379686355591
Epoch: 57, Steps: 64 | Train Loss: 0.6509341 Vali Loss: 0.3655058 Test Loss: 0.4230515
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 4.493756055831909
Epoch: 58, Steps: 64 | Train Loss: 0.6513933 Vali Loss: 0.3664292 Test Loss: 0.4230321
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.4060142040252686
Epoch: 59, Steps: 64 | Train Loss: 0.6516011 Vali Loss: 0.3642181 Test Loss: 0.4230123
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.594090223312378
Epoch: 60, Steps: 64 | Train Loss: 0.6505061 Vali Loss: 0.3660799 Test Loss: 0.4230045
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.4154412746429443
Epoch: 61, Steps: 64 | Train Loss: 0.6485982 Vali Loss: 0.3654045 Test Loss: 0.4229991
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.589277744293213
Epoch: 62, Steps: 64 | Train Loss: 0.6510020 Vali Loss: 0.3646940 Test Loss: 0.4229848
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.821718454360962
Epoch: 63, Steps: 64 | Train Loss: 0.6501705 Vali Loss: 0.3666451 Test Loss: 0.4229718
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.4188954830169678
Epoch: 64, Steps: 64 | Train Loss: 0.6497661 Vali Loss: 0.3648358 Test Loss: 0.4229604
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.6457369327545166
Epoch: 65, Steps: 64 | Train Loss: 0.6507768 Vali Loss: 0.3645469 Test Loss: 0.4229500
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.9658474922180176
Epoch: 66, Steps: 64 | Train Loss: 0.6509087 Vali Loss: 0.3662309 Test Loss: 0.4229444
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.646151065826416
Epoch: 67, Steps: 64 | Train Loss: 0.6500307 Vali Loss: 0.3664136 Test Loss: 0.4229383
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 3.117171049118042
Epoch: 68, Steps: 64 | Train Loss: 0.6493447 Vali Loss: 0.3660039 Test Loss: 0.4229294
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.5866739749908447
Epoch: 69, Steps: 64 | Train Loss: 0.6508507 Vali Loss: 0.3666094 Test Loss: 0.4229190
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.684154033660889
Epoch: 70, Steps: 64 | Train Loss: 0.6483279 Vali Loss: 0.3668264 Test Loss: 0.4229175
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 4.905053377151489
Epoch: 71, Steps: 64 | Train Loss: 0.6497536 Vali Loss: 0.3665848 Test Loss: 0.4229077
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.345961093902588
Epoch: 72, Steps: 64 | Train Loss: 0.6499491 Vali Loss: 0.3670554 Test Loss: 0.4229015
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 4.070245742797852
Epoch: 73, Steps: 64 | Train Loss: 0.6493156 Vali Loss: 0.3639509 Test Loss: 0.4228958
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4183589518070221, mae:0.4259384274482727, rse:0.5171470642089844, corr:[0.2639354  0.264528   0.26358774 0.2633432  0.26049668 0.26019162
 0.2580755  0.25738823 0.2563808  0.25468317 0.2543494  0.25248688
 0.250923   0.24998339 0.2485654  0.24805032 0.2472382  0.24632876
 0.2455151  0.2441335  0.24332772 0.24230671 0.24099727 0.23935612
 0.23622908 0.23390132 0.2320935  0.23037852 0.22946939 0.2281862
 0.2271369  0.226651   0.22539742 0.2244299  0.22392887 0.22273412
 0.2218866  0.22107488 0.22019573 0.21955521 0.2187546  0.21808147
 0.2174571  0.21674055 0.21577205 0.21435869 0.21339568 0.21110804
 0.20727718 0.20451611 0.20190507 0.19984876 0.19805476 0.1960108
 0.19455516 0.19278078 0.19194564 0.19071853 0.18992074 0.18912897
 0.18802935 0.18765554 0.18743429 0.18715054 0.18687561 0.18625599
 0.18549335 0.1849143  0.18403684 0.18338308 0.18256818 0.1804318
 0.17770644 0.17628004 0.17452495 0.17322758 0.1722059  0.17153308
 0.17157952 0.17085467 0.17060299 0.17030363 0.1699815  0.1697123
 0.16906431 0.16877712 0.16880916 0.16856037 0.16827253 0.16785666
 0.16730365 0.16683277 0.16638026 0.16597496 0.16552117 0.16391633
 0.16169126 0.15987416 0.15783493 0.15649144 0.15539034 0.15442793
 0.15457335 0.15428635 0.1544102  0.15416397 0.15402876 0.1540545
 0.15354896 0.15308739 0.1525532  0.15224417 0.15200223 0.15131839
 0.15068956 0.15029183 0.14947595 0.14834549 0.14696652 0.14430967
 0.14143446 0.1394571  0.13744298 0.13623393 0.1351243  0.13393915
 0.13350041 0.1332704  0.13293432 0.1324932  0.13230248 0.13194224
 0.13139786 0.13099302 0.13061166 0.1301879  0.12975286 0.12909052
 0.12847234 0.12804316 0.12738629 0.12666321 0.12559943 0.12282203
 0.11929377 0.11684403 0.11453514 0.11293869 0.11149827 0.11050575
 0.11037517 0.10982848 0.10968304 0.10959601 0.1096507  0.10954038
 0.10910407 0.10904428 0.10920848 0.10922185 0.10908791 0.10884583
 0.10851911 0.10812178 0.1077037  0.10737675 0.10687341 0.1044506
 0.10114534 0.09940761 0.09770577 0.09659864 0.0956234  0.09484724
 0.09529673 0.0954392  0.09543566 0.09557248 0.09595418 0.09579215
 0.09578943 0.09586801 0.0957781  0.09623888 0.09627431 0.09617621
 0.09626702 0.09618881 0.09610388 0.09606668 0.09598944 0.09493512
 0.09294413 0.09219579 0.09118881 0.09053683 0.09064779 0.09048463
 0.09142099 0.09245125 0.09296418 0.09353066 0.09426779 0.09429489
 0.0944033  0.09432153 0.09419342 0.09458774 0.09434037 0.09424061
 0.09434555 0.09422361 0.09408409 0.09387541 0.0936399  0.09195254
 0.0892674  0.08760587 0.0858831  0.08493984 0.08454789 0.08424211
 0.0854135  0.0870218  0.08762194 0.08830193 0.08903987 0.08862374
 0.08848132 0.08830129 0.0881301  0.08875581 0.08885469 0.08882426
 0.0891986  0.08933657 0.08944736 0.08973531 0.0893644  0.08784104
 0.08551241 0.08412672 0.0826427  0.08191994 0.08219024 0.08256478
 0.08418231 0.08580308 0.08657923 0.08756815 0.0886452  0.0887754
 0.08910013 0.08953414 0.08979556 0.09083801 0.09100775 0.09151052
 0.09182016 0.09161789 0.09154375 0.09181688 0.09198941 0.09125388
 0.08977882 0.08906946 0.08878405 0.08872195 0.08902611 0.08973559
 0.09105945 0.09234109 0.09282137 0.09338669 0.09431908 0.09438264
 0.09438121 0.09404399 0.09403784 0.09454701 0.09441342 0.09418437
 0.09432194 0.0939914  0.09368979 0.09389564 0.09422394 0.0935819
 0.09244193 0.09173734 0.09080204 0.09030809 0.09028089 0.09002031
 0.09036889 0.09109417 0.09131762 0.09229612 0.09341696 0.09354204
 0.09392619 0.093555   0.09355405 0.09481574 0.09437428 0.09450501
 0.09521934 0.09485088 0.09532498 0.0956218  0.09603911 0.09564036
 0.09393411 0.09384245 0.09337953 0.09322814 0.09386993 0.09366773
 0.09489885 0.09633408 0.09639356 0.09787209 0.09865541 0.09874351
 0.09997185 0.09836625 0.09849343 0.09937957 0.09765916 0.09874666
 0.0986564  0.09716187 0.0990479  0.09940208 0.10123559 0.1003125 ]
