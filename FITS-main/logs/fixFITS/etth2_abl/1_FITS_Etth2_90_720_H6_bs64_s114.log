Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=34, out_features=306, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9321984.0
params:  10710.0
Trainable parameters:  10710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.1284878253936768
Epoch: 1, Steps: 61 | Train Loss: 1.3218986 Vali Loss: 0.8328428 Test Loss: 0.6608977
Validation loss decreased (inf --> 0.832843).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.214465618133545
Epoch: 2, Steps: 61 | Train Loss: 1.1049707 Vali Loss: 0.7516900 Test Loss: 0.5612318
Validation loss decreased (0.832843 --> 0.751690).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.8816308975219727
Epoch: 3, Steps: 61 | Train Loss: 1.0011820 Vali Loss: 0.7059556 Test Loss: 0.5080044
Validation loss decreased (0.751690 --> 0.705956).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.6469991207122803
Epoch: 4, Steps: 61 | Train Loss: 0.9468399 Vali Loss: 0.6785049 Test Loss: 0.4774734
Validation loss decreased (0.705956 --> 0.678505).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.274599313735962
Epoch: 5, Steps: 61 | Train Loss: 0.9155331 Vali Loss: 0.6623700 Test Loss: 0.4593632
Validation loss decreased (0.678505 --> 0.662370).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.04953670501709
Epoch: 6, Steps: 61 | Train Loss: 0.8958361 Vali Loss: 0.6595418 Test Loss: 0.4479997
Validation loss decreased (0.662370 --> 0.659542).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.9085144996643066
Epoch: 7, Steps: 61 | Train Loss: 0.8842124 Vali Loss: 0.6510941 Test Loss: 0.4408772
Validation loss decreased (0.659542 --> 0.651094).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.8704681396484375
Epoch: 8, Steps: 61 | Train Loss: 0.8766971 Vali Loss: 0.6412482 Test Loss: 0.4361554
Validation loss decreased (0.651094 --> 0.641248).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.016664743423462
Epoch: 9, Steps: 61 | Train Loss: 0.8716551 Vali Loss: 0.6372329 Test Loss: 0.4329767
Validation loss decreased (0.641248 --> 0.637233).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.8712387084960938
Epoch: 10, Steps: 61 | Train Loss: 0.8683141 Vali Loss: 0.6396137 Test Loss: 0.4307328
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.3906803131103516
Epoch: 11, Steps: 61 | Train Loss: 0.8646431 Vali Loss: 0.6382023 Test Loss: 0.4290877
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.8625569343566895
Epoch: 12, Steps: 61 | Train Loss: 0.8634773 Vali Loss: 0.6377963 Test Loss: 0.4278968
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.577932596206665
Epoch: 13, Steps: 61 | Train Loss: 0.8613542 Vali Loss: 0.6334435 Test Loss: 0.4269821
Validation loss decreased (0.637233 --> 0.633444).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.3112101554870605
Epoch: 14, Steps: 61 | Train Loss: 0.8603388 Vali Loss: 0.6358654 Test Loss: 0.4262612
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.444243669509888
Epoch: 15, Steps: 61 | Train Loss: 0.8597564 Vali Loss: 0.6284194 Test Loss: 0.4256575
Validation loss decreased (0.633444 --> 0.628419).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.842510223388672
Epoch: 16, Steps: 61 | Train Loss: 0.8583672 Vali Loss: 0.6339217 Test Loss: 0.4251610
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.511116027832031
Epoch: 17, Steps: 61 | Train Loss: 0.8575121 Vali Loss: 0.6302783 Test Loss: 0.4247398
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.4825973510742188
Epoch: 18, Steps: 61 | Train Loss: 0.8571689 Vali Loss: 0.6325261 Test Loss: 0.4244147
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.116425037384033
Epoch: 19, Steps: 61 | Train Loss: 0.8567049 Vali Loss: 0.6348091 Test Loss: 0.4240735
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.875427484512329
Epoch: 20, Steps: 61 | Train Loss: 0.8573088 Vali Loss: 0.6304194 Test Loss: 0.4238251
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.4806723594665527
Epoch: 21, Steps: 61 | Train Loss: 0.8558748 Vali Loss: 0.6296167 Test Loss: 0.4236004
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.2803001403808594
Epoch: 22, Steps: 61 | Train Loss: 0.8539423 Vali Loss: 0.6280419 Test Loss: 0.4233936
Validation loss decreased (0.628419 --> 0.628042).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.5955801010131836
Epoch: 23, Steps: 61 | Train Loss: 0.8545636 Vali Loss: 0.6295485 Test Loss: 0.4231962
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.9665791988372803
Epoch: 24, Steps: 61 | Train Loss: 0.8544865 Vali Loss: 0.6303246 Test Loss: 0.4230349
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.052904844284058
Epoch: 25, Steps: 61 | Train Loss: 0.8536541 Vali Loss: 0.6314678 Test Loss: 0.4228662
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.164354085922241
Epoch: 26, Steps: 61 | Train Loss: 0.8537457 Vali Loss: 0.6301929 Test Loss: 0.4227258
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.783355951309204
Epoch: 27, Steps: 61 | Train Loss: 0.8542144 Vali Loss: 0.6290717 Test Loss: 0.4225981
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.838942766189575
Epoch: 28, Steps: 61 | Train Loss: 0.8527785 Vali Loss: 0.6331934 Test Loss: 0.4224819
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.444577217102051
Epoch: 29, Steps: 61 | Train Loss: 0.8533777 Vali Loss: 0.6302971 Test Loss: 0.4223886
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.9361376762390137
Epoch: 30, Steps: 61 | Train Loss: 0.8538058 Vali Loss: 0.6259799 Test Loss: 0.4222801
Validation loss decreased (0.628042 --> 0.625980).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.789006233215332
Epoch: 31, Steps: 61 | Train Loss: 0.8532530 Vali Loss: 0.6293308 Test Loss: 0.4221938
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.052151441574097
Epoch: 32, Steps: 61 | Train Loss: 0.8535476 Vali Loss: 0.6275684 Test Loss: 0.4221131
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.5611343383789062
Epoch: 33, Steps: 61 | Train Loss: 0.8533754 Vali Loss: 0.6311138 Test Loss: 0.4220335
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.1025612354278564
Epoch: 34, Steps: 61 | Train Loss: 0.8534929 Vali Loss: 0.6306818 Test Loss: 0.4219591
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.8365023136138916
Epoch: 35, Steps: 61 | Train Loss: 0.8531367 Vali Loss: 0.6288724 Test Loss: 0.4218960
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.155773401260376
Epoch: 36, Steps: 61 | Train Loss: 0.8513772 Vali Loss: 0.6334262 Test Loss: 0.4218428
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.8125271797180176
Epoch: 37, Steps: 61 | Train Loss: 0.8517196 Vali Loss: 0.6291540 Test Loss: 0.4217789
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.333297252655029
Epoch: 38, Steps: 61 | Train Loss: 0.8529689 Vali Loss: 0.6291287 Test Loss: 0.4217274
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.215155124664307
Epoch: 39, Steps: 61 | Train Loss: 0.8514588 Vali Loss: 0.6302803 Test Loss: 0.4216762
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.54598331451416
Epoch: 40, Steps: 61 | Train Loss: 0.8518597 Vali Loss: 0.6266637 Test Loss: 0.4216264
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.414609432220459
Epoch: 41, Steps: 61 | Train Loss: 0.8503341 Vali Loss: 0.6318867 Test Loss: 0.4215885
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.1160175800323486
Epoch: 42, Steps: 61 | Train Loss: 0.8526365 Vali Loss: 0.6303148 Test Loss: 0.4215429
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.158458948135376
Epoch: 43, Steps: 61 | Train Loss: 0.8504795 Vali Loss: 0.6293300 Test Loss: 0.4215076
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.1078758239746094
Epoch: 44, Steps: 61 | Train Loss: 0.8510799 Vali Loss: 0.6287757 Test Loss: 0.4214712
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.5804495811462402
Epoch: 45, Steps: 61 | Train Loss: 0.8507061 Vali Loss: 0.6273719 Test Loss: 0.4214410
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.361156702041626
Epoch: 46, Steps: 61 | Train Loss: 0.8513520 Vali Loss: 0.6296494 Test Loss: 0.4214117
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.863611936569214
Epoch: 47, Steps: 61 | Train Loss: 0.8512089 Vali Loss: 0.6267370 Test Loss: 0.4213758
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 5.100215196609497
Epoch: 48, Steps: 61 | Train Loss: 0.8517797 Vali Loss: 0.6250131 Test Loss: 0.4213493
Validation loss decreased (0.625980 --> 0.625013).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.6827962398529053
Epoch: 49, Steps: 61 | Train Loss: 0.8519213 Vali Loss: 0.6244106 Test Loss: 0.4213244
Validation loss decreased (0.625013 --> 0.624411).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.9747753143310547
Epoch: 50, Steps: 61 | Train Loss: 0.8509098 Vali Loss: 0.6283958 Test Loss: 0.4212968
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.116544246673584
Epoch: 51, Steps: 61 | Train Loss: 0.8506021 Vali Loss: 0.6286310 Test Loss: 0.4212745
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.5154783725738525
Epoch: 52, Steps: 61 | Train Loss: 0.8494445 Vali Loss: 0.6274452 Test Loss: 0.4212523
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 5.353215456008911
Epoch: 53, Steps: 61 | Train Loss: 0.8505875 Vali Loss: 0.6286474 Test Loss: 0.4212313
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.6416118144989014
Epoch: 54, Steps: 61 | Train Loss: 0.8496337 Vali Loss: 0.6251833 Test Loss: 0.4212124
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.9525163173675537
Epoch: 55, Steps: 61 | Train Loss: 0.8507992 Vali Loss: 0.6247465 Test Loss: 0.4211967
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.679567813873291
Epoch: 56, Steps: 61 | Train Loss: 0.8519568 Vali Loss: 0.6279100 Test Loss: 0.4211765
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.615903377532959
Epoch: 57, Steps: 61 | Train Loss: 0.8509686 Vali Loss: 0.6281992 Test Loss: 0.4211619
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.077347755432129
Epoch: 58, Steps: 61 | Train Loss: 0.8510488 Vali Loss: 0.6238446 Test Loss: 0.4211439
Validation loss decreased (0.624411 --> 0.623845).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.0095789432525635
Epoch: 59, Steps: 61 | Train Loss: 0.8509683 Vali Loss: 0.6280701 Test Loss: 0.4211279
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.0159614086151123
Epoch: 60, Steps: 61 | Train Loss: 0.8504529 Vali Loss: 0.6246538 Test Loss: 0.4211164
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 4.118140935897827
Epoch: 61, Steps: 61 | Train Loss: 0.8501535 Vali Loss: 0.6237167 Test Loss: 0.4211025
Validation loss decreased (0.623845 --> 0.623717).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.176438570022583
Epoch: 62, Steps: 61 | Train Loss: 0.8497693 Vali Loss: 0.6256881 Test Loss: 0.4210916
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.6113877296447754
Epoch: 63, Steps: 61 | Train Loss: 0.8513102 Vali Loss: 0.6255300 Test Loss: 0.4210791
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.750640630722046
Epoch: 64, Steps: 61 | Train Loss: 0.8514090 Vali Loss: 0.6280904 Test Loss: 0.4210680
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.330798625946045
Epoch: 65, Steps: 61 | Train Loss: 0.8492815 Vali Loss: 0.6282020 Test Loss: 0.4210579
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.7924423217773438
Epoch: 66, Steps: 61 | Train Loss: 0.8501254 Vali Loss: 0.6264881 Test Loss: 0.4210503
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 4.72865104675293
Epoch: 67, Steps: 61 | Train Loss: 0.8505214 Vali Loss: 0.6284841 Test Loss: 0.4210413
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.5124247074127197
Epoch: 68, Steps: 61 | Train Loss: 0.8501467 Vali Loss: 0.6271375 Test Loss: 0.4210306
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.120217800140381
Epoch: 69, Steps: 61 | Train Loss: 0.8515222 Vali Loss: 0.6304906 Test Loss: 0.4210227
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 3.352538585662842
Epoch: 70, Steps: 61 | Train Loss: 0.8495890 Vali Loss: 0.6234118 Test Loss: 0.4210135
Validation loss decreased (0.623717 --> 0.623412).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 4.186771392822266
Epoch: 71, Steps: 61 | Train Loss: 0.8513158 Vali Loss: 0.6242830 Test Loss: 0.4210069
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 4.164088249206543
Epoch: 72, Steps: 61 | Train Loss: 0.8512510 Vali Loss: 0.6255529 Test Loss: 0.4210000
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.215996265411377
Epoch: 73, Steps: 61 | Train Loss: 0.8499361 Vali Loss: 0.6269095 Test Loss: 0.4209918
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.9245574474334717
Epoch: 74, Steps: 61 | Train Loss: 0.8483197 Vali Loss: 0.6294406 Test Loss: 0.4209848
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.605119466781616
Epoch: 75, Steps: 61 | Train Loss: 0.8502615 Vali Loss: 0.6255251 Test Loss: 0.4209800
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 4.174068450927734
Epoch: 76, Steps: 61 | Train Loss: 0.8503619 Vali Loss: 0.6290462 Test Loss: 0.4209740
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 4.808881044387817
Epoch: 77, Steps: 61 | Train Loss: 0.8502985 Vali Loss: 0.6252604 Test Loss: 0.4209664
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.7600691318511963
Epoch: 78, Steps: 61 | Train Loss: 0.8509413 Vali Loss: 0.6241111 Test Loss: 0.4209612
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 3.458028554916382
Epoch: 79, Steps: 61 | Train Loss: 0.8510147 Vali Loss: 0.6258115 Test Loss: 0.4209567
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.6557042598724365
Epoch: 80, Steps: 61 | Train Loss: 0.8498236 Vali Loss: 0.6259242 Test Loss: 0.4209523
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.079782247543335
Epoch: 81, Steps: 61 | Train Loss: 0.8501458 Vali Loss: 0.6239309 Test Loss: 0.4209472
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.3499512672424316
Epoch: 82, Steps: 61 | Train Loss: 0.8512634 Vali Loss: 0.6275562 Test Loss: 0.4209431
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.480318307876587
Epoch: 83, Steps: 61 | Train Loss: 0.8515931 Vali Loss: 0.6240053 Test Loss: 0.4209384
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 4.360638380050659
Epoch: 84, Steps: 61 | Train Loss: 0.8504037 Vali Loss: 0.6299938 Test Loss: 0.4209351
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.958491086959839
Epoch: 85, Steps: 61 | Train Loss: 0.8499114 Vali Loss: 0.6244454 Test Loss: 0.4209314
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 5.679741382598877
Epoch: 86, Steps: 61 | Train Loss: 0.8503374 Vali Loss: 0.6269978 Test Loss: 0.4209276
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.4847404956817627
Epoch: 87, Steps: 61 | Train Loss: 0.8509962 Vali Loss: 0.6298089 Test Loss: 0.4209239
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 3.642467498779297
Epoch: 88, Steps: 61 | Train Loss: 0.8507717 Vali Loss: 0.6255208 Test Loss: 0.4209202
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 5.772756099700928
Epoch: 89, Steps: 61 | Train Loss: 0.8510785 Vali Loss: 0.6287322 Test Loss: 0.4209180
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.80975079536438
Epoch: 90, Steps: 61 | Train Loss: 0.8497190 Vali Loss: 0.6259925 Test Loss: 0.4209156
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.41937607526779175, mae:0.4369020164012909, rse:0.517615795135498, corr:[ 2.20599011e-01  2.21085310e-01  2.19884977e-01  2.18859121e-01
  2.17013702e-01  2.16394246e-01  2.14836150e-01  2.14506552e-01
  2.13325858e-01  2.11971566e-01  2.11417601e-01  2.09519312e-01
  2.08489031e-01  2.07650661e-01  2.06656933e-01  2.06424594e-01
  2.05462828e-01  2.04904824e-01  2.04243898e-01  2.03012884e-01
  2.02380359e-01  2.01634645e-01  2.00671896e-01  1.98609069e-01
  1.95810407e-01  1.94164619e-01  1.92163423e-01  1.91095218e-01
  1.90731332e-01  1.89584613e-01  1.89355522e-01  1.88976899e-01
  1.87901109e-01  1.87249586e-01  1.86553627e-01  1.85674474e-01
  1.85073331e-01  1.84272423e-01  1.83778048e-01  1.83338940e-01
  1.82645187e-01  1.82430506e-01  1.82165816e-01  1.81749701e-01
  1.81414619e-01  1.80641443e-01  1.79975569e-01  1.77957088e-01
  1.74884006e-01  1.72784880e-01  1.70683444e-01  1.69522077e-01
  1.68532312e-01  1.67758793e-01  1.67704344e-01  1.66765019e-01
  1.66773766e-01  1.66517824e-01  1.66242540e-01  1.65962413e-01
  1.65513933e-01  1.65441871e-01  1.65480837e-01  1.65735587e-01
  1.65910140e-01  1.65852338e-01  1.65814415e-01  1.65562168e-01
  1.65387318e-01  1.65107071e-01  1.64450437e-01  1.63328111e-01
  1.61482885e-01  1.60534710e-01  1.59548551e-01  1.59001485e-01
  1.58535957e-01  1.58525974e-01  1.59101099e-01  1.58779338e-01
  1.58808738e-01  1.58610508e-01  1.58433020e-01  1.58389419e-01
  1.58122972e-01  1.58069909e-01  1.58225387e-01  1.58302903e-01
  1.58142954e-01  1.58055216e-01  1.57967344e-01  1.57471240e-01
  1.57261401e-01  1.56941473e-01  1.56364694e-01  1.55557990e-01
  1.53924912e-01  1.52702123e-01  1.51602566e-01  1.50812984e-01
  1.50072157e-01  1.49575874e-01  1.49663329e-01  1.49295345e-01
  1.49629205e-01  1.49418488e-01  1.49462193e-01  1.49485335e-01
  1.49175763e-01  1.49206281e-01  1.48901433e-01  1.48759708e-01
  1.48627579e-01  1.48122847e-01  1.47883430e-01  1.47591174e-01
  1.47211775e-01  1.46553174e-01  1.45491093e-01  1.43811390e-01
  1.41585588e-01  1.40228599e-01  1.38811156e-01  1.37993857e-01
  1.37199625e-01  1.36631399e-01  1.36470169e-01  1.36265337e-01
  1.36411145e-01  1.36094257e-01  1.35982111e-01  1.35649845e-01
  1.34973764e-01  1.34596333e-01  1.34268284e-01  1.33816704e-01
  1.33402199e-01  1.32869899e-01  1.32249221e-01  1.31735623e-01
  1.31434262e-01  1.30752653e-01  1.29731342e-01  1.27960593e-01
  1.24842696e-01  1.22863844e-01  1.21214531e-01  1.20215386e-01
  1.19620770e-01  1.19132400e-01  1.18925989e-01  1.18599631e-01
  1.18984245e-01  1.18687116e-01  1.18605547e-01  1.18529834e-01
  1.17889233e-01  1.17820278e-01  1.17893703e-01  1.17671840e-01
  1.17593706e-01  1.17525272e-01  1.17150329e-01  1.16943516e-01
  1.17143638e-01  1.16508707e-01  1.15691625e-01  1.14077039e-01
  1.11251988e-01  1.09882459e-01  1.08644761e-01  1.07867278e-01
  1.07626103e-01  1.07781343e-01  1.08330168e-01  1.08586662e-01
  1.08926006e-01  1.08600788e-01  1.08929850e-01  1.08876415e-01
  1.08369760e-01  1.08287811e-01  1.08367786e-01  1.08469866e-01
  1.08392388e-01  1.08473219e-01  1.08635731e-01  1.08685702e-01
  1.08776569e-01  1.08786158e-01  1.08576715e-01  1.07968338e-01
  1.06463514e-01  1.06107026e-01  1.05882749e-01  1.06010452e-01
  1.06480666e-01  1.07056685e-01  1.08422764e-01  1.09641925e-01
  1.10460691e-01  1.10528842e-01  1.11013427e-01  1.11199170e-01
  1.11068062e-01  1.11188330e-01  1.11383468e-01  1.11567684e-01
  1.11661069e-01  1.11997396e-01  1.12286344e-01  1.12342939e-01
  1.12348907e-01  1.12079553e-01  1.11799896e-01  1.10567242e-01
  1.08727306e-01  1.07723132e-01  1.06591925e-01  1.06447153e-01
  1.06532149e-01  1.07137814e-01  1.08621083e-01  1.10150985e-01
  1.11078769e-01  1.11490019e-01  1.11997172e-01  1.12081856e-01
  1.12293787e-01  1.12570509e-01  1.12853169e-01  1.13450378e-01
  1.13923572e-01  1.14263363e-01  1.14554748e-01  1.14762478e-01
  1.15152158e-01  1.15386426e-01  1.15111165e-01  1.14609703e-01
  1.13605261e-01  1.13116421e-01  1.12367004e-01  1.12335734e-01
  1.12806596e-01  1.13647260e-01  1.15288220e-01  1.16245173e-01
  1.17271334e-01  1.17797561e-01  1.18275538e-01  1.18552223e-01
  1.18829355e-01  1.19207256e-01  1.19297035e-01  1.19605616e-01
  1.19680151e-01  1.20186992e-01  1.20477699e-01  1.20545365e-01
  1.20732203e-01  1.20764434e-01  1.20651722e-01  1.20258451e-01
  1.19188383e-01  1.18897453e-01  1.18841887e-01  1.19195923e-01
  1.19638294e-01  1.20787486e-01  1.22516841e-01  1.23733006e-01
  1.24436803e-01  1.24531597e-01  1.24905907e-01  1.24832712e-01
  1.24769084e-01  1.24772653e-01  1.24946833e-01  1.25139832e-01
  1.25347361e-01  1.25412568e-01  1.25320002e-01  1.25089556e-01
  1.25013411e-01  1.25076458e-01  1.25063121e-01  1.24521345e-01
  1.23712786e-01  1.23342827e-01  1.22907110e-01  1.22711182e-01
  1.22838125e-01  1.23200856e-01  1.23653702e-01  1.24094799e-01
  1.24535151e-01  1.25020534e-01  1.25363678e-01  1.25285402e-01
  1.25346750e-01  1.25443593e-01  1.25400469e-01  1.25770763e-01
  1.25909656e-01  1.25913322e-01  1.25964060e-01  1.25774667e-01
  1.25884473e-01  1.25818819e-01  1.25408575e-01  1.24826953e-01
  1.23715334e-01  1.23179473e-01  1.22897424e-01  1.23024203e-01
  1.23170361e-01  1.23453416e-01  1.24369912e-01  1.25332475e-01
  1.26080096e-01  1.26415253e-01  1.26842216e-01  1.26936078e-01
  1.27394766e-01  1.27735227e-01  1.27811894e-01  1.28300250e-01
  1.28602624e-01  1.28561705e-01  1.28744707e-01  1.28878802e-01
  1.29180282e-01  1.29482746e-01  1.29411161e-01  1.29295096e-01
  1.28467992e-01  1.28156796e-01  1.27797216e-01  1.28232136e-01
  1.29206061e-01  1.30126312e-01  1.32106036e-01  1.33402914e-01
  1.33919567e-01  1.34396106e-01  1.34933814e-01  1.35155544e-01
  1.35488376e-01  1.35684520e-01  1.35788515e-01  1.36259064e-01
  1.36599809e-01  1.36723027e-01  1.37057498e-01  1.37362182e-01
  1.37776405e-01  1.38369724e-01  1.38701454e-01  1.38534844e-01
  1.37795076e-01  1.37773037e-01  1.37903675e-01  1.38299659e-01
  1.39380649e-01  1.40827090e-01  1.43190384e-01  1.45315081e-01
  1.46368325e-01  1.47269309e-01  1.48213834e-01  1.48758620e-01
  1.49511978e-01  1.50141194e-01  1.50796145e-01  1.51538223e-01
  1.51889026e-01  1.52682707e-01  1.53369829e-01  1.53565690e-01
  1.54249221e-01  1.54890701e-01  1.55091256e-01  1.55269057e-01
  1.55303210e-01  1.55544207e-01  1.55858889e-01  1.56579360e-01
  1.57669261e-01  1.59005478e-01  1.61584258e-01  1.63311362e-01
  1.64034531e-01  1.64870158e-01  1.65657997e-01  1.66045621e-01
  1.66553706e-01  1.66637138e-01  1.66627705e-01  1.67047650e-01
  1.67134106e-01  1.67093024e-01  1.67257160e-01  1.67325124e-01
  1.67505458e-01  1.67529777e-01  1.67456508e-01  1.67181462e-01
  1.66489393e-01  1.66574895e-01  1.66545585e-01  1.66868165e-01
  1.67379752e-01  1.67847574e-01  1.69236124e-01  1.70184419e-01
  1.70461267e-01  1.70887560e-01  1.71181828e-01  1.71149075e-01
  1.71003714e-01  1.70913592e-01  1.70939639e-01  1.70838967e-01
  1.70614466e-01  1.70573637e-01  1.70444086e-01  1.70301303e-01
  1.70547009e-01  1.70691967e-01  1.70575842e-01  1.70186192e-01
  1.69541553e-01  1.69170320e-01  1.68941632e-01  1.68852955e-01
  1.68811172e-01  1.69024393e-01  1.70157060e-01  1.70566112e-01
  1.70513645e-01  1.70540527e-01  1.70663372e-01  1.70485422e-01
  1.70358926e-01  1.70360595e-01  1.70230150e-01  1.70277342e-01
  1.70161590e-01  1.70032099e-01  1.69942722e-01  1.69908270e-01
  1.69857606e-01  1.69712692e-01  1.69654474e-01  1.69377491e-01
  1.68796778e-01  1.68386951e-01  1.67707160e-01  1.67546168e-01
  1.67512760e-01  1.67457789e-01  1.67918175e-01  1.67963251e-01
  1.67790085e-01  1.67871073e-01  1.67956322e-01  1.67700320e-01
  1.67409226e-01  1.67119846e-01  1.66823283e-01  1.66660249e-01
  1.66395470e-01  1.66116565e-01  1.65857270e-01  1.65513515e-01
  1.65137216e-01  1.64917082e-01  1.64660782e-01  1.63632587e-01
  1.61837026e-01  1.60636649e-01  1.59867719e-01  1.59021765e-01
  1.58233598e-01  1.57566145e-01  1.57318965e-01  1.56957075e-01
  1.56583473e-01  1.55944899e-01  1.55649245e-01  1.55240849e-01
  1.54537871e-01  1.54174194e-01  1.53789908e-01  1.53495207e-01
  1.53373510e-01  1.53077334e-01  1.52520746e-01  1.52003393e-01
  1.51652083e-01  1.51259199e-01  1.50759637e-01  1.49253696e-01
  1.46907941e-01  1.45137653e-01  1.43694639e-01  1.42327681e-01
  1.41300544e-01  1.40888795e-01  1.40609875e-01  1.40347213e-01
  1.40130892e-01  1.39841601e-01  1.39727280e-01  1.39277309e-01
  1.38352290e-01  1.37884945e-01  1.37860730e-01  1.37523785e-01
  1.37235090e-01  1.36731997e-01  1.35775626e-01  1.35202065e-01
  1.34719297e-01  1.34189025e-01  1.33611202e-01  1.31866083e-01
  1.28851503e-01  1.26479864e-01  1.24469191e-01  1.22357793e-01
  1.20619133e-01  1.19103849e-01  1.18184105e-01  1.17512412e-01
  1.17457069e-01  1.17020711e-01  1.16841964e-01  1.16603553e-01
  1.15685269e-01  1.15142345e-01  1.14652880e-01  1.13836728e-01
  1.12974383e-01  1.12293012e-01  1.11332595e-01  1.10544883e-01
  1.09720103e-01  1.08646259e-01  1.07652202e-01  1.05398163e-01
  1.01888359e-01  9.98108834e-02  9.76323187e-02  9.57241580e-02
  9.44944918e-02  9.38350856e-02  9.34792906e-02  9.27896798e-02
  9.26614851e-02  9.23557803e-02  9.23630595e-02  9.22472179e-02
  9.13522542e-02  9.05983970e-02  9.01682898e-02  8.93150568e-02
  8.87025595e-02  8.82003084e-02  8.72052014e-02  8.62203613e-02
  8.51891041e-02  8.39994922e-02  8.31074789e-02  8.08125287e-02
  7.75492042e-02  7.54234716e-02  7.35659376e-02  7.19534084e-02
  7.07108825e-02  6.96616843e-02  6.91888332e-02  6.89010844e-02
  6.88372254e-02  6.86204061e-02  6.85412139e-02  6.82157576e-02
  6.73041940e-02  6.65909052e-02  6.63808063e-02  6.59072623e-02
  6.54144362e-02  6.51995242e-02  6.46831766e-02  6.41320944e-02
  6.34062067e-02  6.24835566e-02  6.17854632e-02  5.96469194e-02
  5.66680990e-02  5.47658391e-02  5.28086685e-02  5.15055917e-02
  5.04042506e-02  4.93821092e-02  4.88618016e-02  4.87522222e-02
  4.86340001e-02  4.82367799e-02  4.86869104e-02  4.88778353e-02
  4.82789539e-02  4.78702448e-02  4.77514975e-02  4.74111177e-02
  4.71153520e-02  4.69489656e-02  4.61965390e-02  4.58679572e-02
  4.55383807e-02  4.47115786e-02  4.43608277e-02  4.25602682e-02
  3.93527560e-02  3.77336331e-02  3.56865637e-02  3.42114978e-02
  3.31260860e-02  3.21100578e-02  3.17642316e-02  3.13784406e-02
  3.12344432e-02  3.09987403e-02  3.11543476e-02  3.10649555e-02
  3.02597079e-02  2.94411015e-02  2.89260689e-02  2.82216761e-02
  2.73985453e-02  2.65480373e-02  2.54339091e-02  2.47551389e-02
  2.39593312e-02  2.33725142e-02  2.36302726e-02  2.13498753e-02
  1.82519965e-02  1.66646522e-02  1.44160772e-02  1.29235955e-02
  1.17614307e-02  1.04370974e-02  9.68384556e-03  9.22558364e-03
  8.92942958e-03  8.85605905e-03  9.17316694e-03  8.87892861e-03
  8.08837451e-03  7.35864323e-03  7.17984047e-03  7.05993082e-03
  6.68425672e-03  6.10059733e-03  5.27784321e-03  4.76778578e-03
  4.13943967e-03  4.07008827e-03  4.59587341e-03  2.57713045e-03
 -1.31081179e-04 -1.79800577e-03 -4.16288385e-03 -5.70590654e-03
 -7.59310834e-03 -8.99814256e-03 -8.89448449e-03 -9.12719965e-03
 -8.95938836e-03 -8.69555585e-03 -8.08408763e-03 -8.08518007e-03
 -9.04796924e-03 -9.80427302e-03 -1.02020158e-02 -1.07539771e-02
 -1.07854437e-02 -1.11118611e-02 -1.16697690e-02 -1.20058162e-02
 -1.28780045e-02 -1.25136096e-02 -1.17597384e-02 -1.39624318e-02
 -1.66803245e-02 -1.85501557e-02 -2.08324101e-02 -2.13139411e-02
 -2.29672492e-02 -2.35839989e-02 -2.27625631e-02 -2.32790876e-02
 -2.28911247e-02 -2.27857474e-02 -2.21235994e-02 -2.20450796e-02
 -2.30879318e-02 -2.43285857e-02 -2.46917363e-02 -2.53961980e-02
 -2.61298288e-02 -2.67553106e-02 -2.59101335e-02 -2.43305415e-02
 -2.51311418e-02 -2.11235881e-02 -1.95345413e-02 -1.95356738e-02]
