Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_192_FITS_ETTh2_ftM_sl180_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8269
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=58, out_features=119, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6184192.0
params:  7021.0
Trainable parameters:  7021
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.4437901973724365
Epoch: 1, Steps: 64 | Train Loss: 0.6977593 Vali Loss: 0.3413999 Test Loss: 0.4677583
Validation loss decreased (inf --> 0.341400).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5068037509918213
Epoch: 2, Steps: 64 | Train Loss: 0.5948076 Vali Loss: 0.3096235 Test Loss: 0.4277953
Validation loss decreased (0.341400 --> 0.309623).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9010074138641357
Epoch: 3, Steps: 64 | Train Loss: 0.5605814 Vali Loss: 0.2965283 Test Loss: 0.4127783
Validation loss decreased (0.309623 --> 0.296528).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.166020393371582
Epoch: 4, Steps: 64 | Train Loss: 0.5487174 Vali Loss: 0.2900545 Test Loss: 0.4055196
Validation loss decreased (0.296528 --> 0.290054).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8703970909118652
Epoch: 5, Steps: 64 | Train Loss: 0.5415270 Vali Loss: 0.2858148 Test Loss: 0.4012184
Validation loss decreased (0.290054 --> 0.285815).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.6007745265960693
Epoch: 6, Steps: 64 | Train Loss: 0.5343198 Vali Loss: 0.2833853 Test Loss: 0.3980751
Validation loss decreased (0.285815 --> 0.283385).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.351240873336792
Epoch: 7, Steps: 64 | Train Loss: 0.5318645 Vali Loss: 0.2814575 Test Loss: 0.3958748
Validation loss decreased (0.283385 --> 0.281458).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.3334722518920898
Epoch: 8, Steps: 64 | Train Loss: 0.5293150 Vali Loss: 0.2801506 Test Loss: 0.3940570
Validation loss decreased (0.281458 --> 0.280151).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.6811766624450684
Epoch: 9, Steps: 64 | Train Loss: 0.5271257 Vali Loss: 0.2791027 Test Loss: 0.3925888
Validation loss decreased (0.280151 --> 0.279103).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.564500093460083
Epoch: 10, Steps: 64 | Train Loss: 0.5246090 Vali Loss: 0.2779830 Test Loss: 0.3914090
Validation loss decreased (0.279103 --> 0.277983).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7639999389648438
Epoch: 11, Steps: 64 | Train Loss: 0.5251841 Vali Loss: 0.2776376 Test Loss: 0.3903737
Validation loss decreased (0.277983 --> 0.277638).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4242892265319824
Epoch: 12, Steps: 64 | Train Loss: 0.5260095 Vali Loss: 0.2769954 Test Loss: 0.3896157
Validation loss decreased (0.277638 --> 0.276995).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.2533841133117676
Epoch: 13, Steps: 64 | Train Loss: 0.5232657 Vali Loss: 0.2764530 Test Loss: 0.3888159
Validation loss decreased (0.276995 --> 0.276453).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.2889604568481445
Epoch: 14, Steps: 64 | Train Loss: 0.5203369 Vali Loss: 0.2759983 Test Loss: 0.3882126
Validation loss decreased (0.276453 --> 0.275998).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.2993793487548828
Epoch: 15, Steps: 64 | Train Loss: 0.5235533 Vali Loss: 0.2755020 Test Loss: 0.3876148
Validation loss decreased (0.275998 --> 0.275502).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.3103008270263672
Epoch: 16, Steps: 64 | Train Loss: 0.5192244 Vali Loss: 0.2753688 Test Loss: 0.3870917
Validation loss decreased (0.275502 --> 0.275369).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.8042545318603516
Epoch: 17, Steps: 64 | Train Loss: 0.5219194 Vali Loss: 0.2751063 Test Loss: 0.3866044
Validation loss decreased (0.275369 --> 0.275106).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.5470705032348633
Epoch: 18, Steps: 64 | Train Loss: 0.5211169 Vali Loss: 0.2748873 Test Loss: 0.3862523
Validation loss decreased (0.275106 --> 0.274887).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.3898749351501465
Epoch: 19, Steps: 64 | Train Loss: 0.5194617 Vali Loss: 0.2746601 Test Loss: 0.3857951
Validation loss decreased (0.274887 --> 0.274660).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3663809299468994
Epoch: 20, Steps: 64 | Train Loss: 0.5194944 Vali Loss: 0.2745082 Test Loss: 0.3854976
Validation loss decreased (0.274660 --> 0.274508).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.1141376495361328
Epoch: 21, Steps: 64 | Train Loss: 0.5190598 Vali Loss: 0.2744002 Test Loss: 0.3851338
Validation loss decreased (0.274508 --> 0.274400).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.89894700050354
Epoch: 22, Steps: 64 | Train Loss: 0.5180784 Vali Loss: 0.2741679 Test Loss: 0.3849143
Validation loss decreased (0.274400 --> 0.274168).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5398268699645996
Epoch: 23, Steps: 64 | Train Loss: 0.5174917 Vali Loss: 0.2739920 Test Loss: 0.3846602
Validation loss decreased (0.274168 --> 0.273992).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4500317573547363
Epoch: 24, Steps: 64 | Train Loss: 0.5180189 Vali Loss: 0.2738495 Test Loss: 0.3844627
Validation loss decreased (0.273992 --> 0.273849).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.2694344520568848
Epoch: 25, Steps: 64 | Train Loss: 0.5171139 Vali Loss: 0.2738022 Test Loss: 0.3842698
Validation loss decreased (0.273849 --> 0.273802).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4331045150756836
Epoch: 26, Steps: 64 | Train Loss: 0.5192417 Vali Loss: 0.2735360 Test Loss: 0.3840839
Validation loss decreased (0.273802 --> 0.273536).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4342665672302246
Epoch: 27, Steps: 64 | Train Loss: 0.5182897 Vali Loss: 0.2735608 Test Loss: 0.3838468
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5594086647033691
Epoch: 28, Steps: 64 | Train Loss: 0.5150762 Vali Loss: 0.2734775 Test Loss: 0.3836607
Validation loss decreased (0.273536 --> 0.273477).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.9041738510131836
Epoch: 29, Steps: 64 | Train Loss: 0.5165515 Vali Loss: 0.2733698 Test Loss: 0.3835378
Validation loss decreased (0.273477 --> 0.273370).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5652711391448975
Epoch: 30, Steps: 64 | Train Loss: 0.5164957 Vali Loss: 0.2733315 Test Loss: 0.3833734
Validation loss decreased (0.273370 --> 0.273332).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.551828145980835
Epoch: 31, Steps: 64 | Train Loss: 0.5175150 Vali Loss: 0.2731727 Test Loss: 0.3832383
Validation loss decreased (0.273332 --> 0.273173).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.9036428928375244
Epoch: 32, Steps: 64 | Train Loss: 0.5178953 Vali Loss: 0.2731561 Test Loss: 0.3831342
Validation loss decreased (0.273173 --> 0.273156).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4121994972229004
Epoch: 33, Steps: 64 | Train Loss: 0.5153650 Vali Loss: 0.2731511 Test Loss: 0.3830152
Validation loss decreased (0.273156 --> 0.273151).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5542335510253906
Epoch: 34, Steps: 64 | Train Loss: 0.5171256 Vali Loss: 0.2731199 Test Loss: 0.3828746
Validation loss decreased (0.273151 --> 0.273120).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.418773889541626
Epoch: 35, Steps: 64 | Train Loss: 0.5156661 Vali Loss: 0.2730591 Test Loss: 0.3827799
Validation loss decreased (0.273120 --> 0.273059).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.5929124355316162
Epoch: 36, Steps: 64 | Train Loss: 0.5132664 Vali Loss: 0.2730077 Test Loss: 0.3826939
Validation loss decreased (0.273059 --> 0.273008).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6115474700927734
Epoch: 37, Steps: 64 | Train Loss: 0.5155736 Vali Loss: 0.2728929 Test Loss: 0.3826083
Validation loss decreased (0.273008 --> 0.272893).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7867698669433594
Epoch: 38, Steps: 64 | Train Loss: 0.5153213 Vali Loss: 0.2728949 Test Loss: 0.3825355
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.2925918102264404
Epoch: 39, Steps: 64 | Train Loss: 0.5144878 Vali Loss: 0.2728012 Test Loss: 0.3824461
Validation loss decreased (0.272893 --> 0.272801).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7406377792358398
Epoch: 40, Steps: 64 | Train Loss: 0.5160002 Vali Loss: 0.2727637 Test Loss: 0.3823571
Validation loss decreased (0.272801 --> 0.272764).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.5949923992156982
Epoch: 41, Steps: 64 | Train Loss: 0.5164815 Vali Loss: 0.2723324 Test Loss: 0.3822762
Validation loss decreased (0.272764 --> 0.272332).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.1562018394470215
Epoch: 42, Steps: 64 | Train Loss: 0.5158053 Vali Loss: 0.2724394 Test Loss: 0.3822239
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.4089019298553467
Epoch: 43, Steps: 64 | Train Loss: 0.5165213 Vali Loss: 0.2726538 Test Loss: 0.3821794
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.6233789920806885
Epoch: 44, Steps: 64 | Train Loss: 0.5149948 Vali Loss: 0.2726349 Test Loss: 0.3821122
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.3480844497680664
Epoch: 45, Steps: 64 | Train Loss: 0.5166416 Vali Loss: 0.2725804 Test Loss: 0.3820595
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.4545342922210693
Epoch: 46, Steps: 64 | Train Loss: 0.5141439 Vali Loss: 0.2725385 Test Loss: 0.3820283
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.4467658996582031
Epoch: 47, Steps: 64 | Train Loss: 0.5137359 Vali Loss: 0.2722986 Test Loss: 0.3819495
Validation loss decreased (0.272332 --> 0.272299).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.2918283939361572
Epoch: 48, Steps: 64 | Train Loss: 0.5131135 Vali Loss: 0.2724280 Test Loss: 0.3819275
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9800546169281006
Epoch: 49, Steps: 64 | Train Loss: 0.5152987 Vali Loss: 0.2725418 Test Loss: 0.3818628
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.4750711917877197
Epoch: 50, Steps: 64 | Train Loss: 0.5159742 Vali Loss: 0.2724828 Test Loss: 0.3818132
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3475139141082764
Epoch: 51, Steps: 64 | Train Loss: 0.5141945 Vali Loss: 0.2724642 Test Loss: 0.3817774
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3080272674560547
Epoch: 52, Steps: 64 | Train Loss: 0.5139689 Vali Loss: 0.2724685 Test Loss: 0.3817703
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.7142493724822998
Epoch: 53, Steps: 64 | Train Loss: 0.5149312 Vali Loss: 0.2724320 Test Loss: 0.3817177
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.600229024887085
Epoch: 54, Steps: 64 | Train Loss: 0.5158749 Vali Loss: 0.2722598 Test Loss: 0.3816920
Validation loss decreased (0.272299 --> 0.272260).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.3251540660858154
Epoch: 55, Steps: 64 | Train Loss: 0.5157589 Vali Loss: 0.2724453 Test Loss: 0.3816545
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.454561710357666
Epoch: 56, Steps: 64 | Train Loss: 0.5141321 Vali Loss: 0.2724465 Test Loss: 0.3816230
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.4591517448425293
Epoch: 57, Steps: 64 | Train Loss: 0.5160582 Vali Loss: 0.2723653 Test Loss: 0.3816027
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.1720523834228516
Epoch: 58, Steps: 64 | Train Loss: 0.5124411 Vali Loss: 0.2724009 Test Loss: 0.3815824
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.5953974723815918
Epoch: 59, Steps: 64 | Train Loss: 0.5135171 Vali Loss: 0.2723728 Test Loss: 0.3815475
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.463794231414795
Epoch: 60, Steps: 64 | Train Loss: 0.5130635 Vali Loss: 0.2723588 Test Loss: 0.3815257
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.4637670516967773
Epoch: 61, Steps: 64 | Train Loss: 0.5146810 Vali Loss: 0.2723048 Test Loss: 0.3815104
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.8733413219451904
Epoch: 62, Steps: 64 | Train Loss: 0.5132538 Vali Loss: 0.2723170 Test Loss: 0.3814939
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.3771588802337646
Epoch: 63, Steps: 64 | Train Loss: 0.5145714 Vali Loss: 0.2723205 Test Loss: 0.3814708
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.2904798984527588
Epoch: 64, Steps: 64 | Train Loss: 0.5152428 Vali Loss: 0.2723307 Test Loss: 0.3814631
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.663262128829956
Epoch: 65, Steps: 64 | Train Loss: 0.5133506 Vali Loss: 0.2723099 Test Loss: 0.3814445
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.226074457168579
Epoch: 66, Steps: 64 | Train Loss: 0.5134685 Vali Loss: 0.2723075 Test Loss: 0.3814256
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.473912000656128
Epoch: 67, Steps: 64 | Train Loss: 0.5157912 Vali Loss: 0.2722507 Test Loss: 0.3814172
Validation loss decreased (0.272260 --> 0.272251).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.3703010082244873
Epoch: 68, Steps: 64 | Train Loss: 0.5160392 Vali Loss: 0.2721749 Test Loss: 0.3814010
Validation loss decreased (0.272251 --> 0.272175).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3875141143798828
Epoch: 69, Steps: 64 | Train Loss: 0.5152406 Vali Loss: 0.2719183 Test Loss: 0.3813841
Validation loss decreased (0.272175 --> 0.271918).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.3833816051483154
Epoch: 70, Steps: 64 | Train Loss: 0.5146357 Vali Loss: 0.2722804 Test Loss: 0.3813761
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.768754005432129
Epoch: 71, Steps: 64 | Train Loss: 0.5153471 Vali Loss: 0.2722196 Test Loss: 0.3813618
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.988603115081787
Epoch: 72, Steps: 64 | Train Loss: 0.5137800 Vali Loss: 0.2722494 Test Loss: 0.3813545
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.7367196083068848
Epoch: 73, Steps: 64 | Train Loss: 0.5153189 Vali Loss: 0.2722025 Test Loss: 0.3813399
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.5886874198913574
Epoch: 74, Steps: 64 | Train Loss: 0.5129642 Vali Loss: 0.2722477 Test Loss: 0.3813255
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.622446060180664
Epoch: 75, Steps: 64 | Train Loss: 0.5119650 Vali Loss: 0.2722651 Test Loss: 0.3813149
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.5537984371185303
Epoch: 76, Steps: 64 | Train Loss: 0.5147928 Vali Loss: 0.2722349 Test Loss: 0.3813078
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.5845611095428467
Epoch: 77, Steps: 64 | Train Loss: 0.5162421 Vali Loss: 0.2722273 Test Loss: 0.3812992
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.6016654968261719
Epoch: 78, Steps: 64 | Train Loss: 0.5158974 Vali Loss: 0.2721178 Test Loss: 0.3812932
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.8573293685913086
Epoch: 79, Steps: 64 | Train Loss: 0.5130247 Vali Loss: 0.2721235 Test Loss: 0.3812844
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.407820701599121
Epoch: 80, Steps: 64 | Train Loss: 0.5137604 Vali Loss: 0.2721584 Test Loss: 0.3812783
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.8721609115600586
Epoch: 81, Steps: 64 | Train Loss: 0.5139543 Vali Loss: 0.2721852 Test Loss: 0.3812714
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.646549940109253
Epoch: 82, Steps: 64 | Train Loss: 0.5127921 Vali Loss: 0.2721062 Test Loss: 0.3812640
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.3991544246673584
Epoch: 83, Steps: 64 | Train Loss: 0.5142692 Vali Loss: 0.2721582 Test Loss: 0.3812559
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.3385884761810303
Epoch: 84, Steps: 64 | Train Loss: 0.5149069 Vali Loss: 0.2722126 Test Loss: 0.3812516
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.7459635734558105
Epoch: 85, Steps: 64 | Train Loss: 0.5144957 Vali Loss: 0.2722000 Test Loss: 0.3812430
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.0565569400787354
Epoch: 86, Steps: 64 | Train Loss: 0.5145727 Vali Loss: 0.2721620 Test Loss: 0.3812396
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.7527718544006348
Epoch: 87, Steps: 64 | Train Loss: 0.5142863 Vali Loss: 0.2721898 Test Loss: 0.3812346
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.5682964324951172
Epoch: 88, Steps: 64 | Train Loss: 0.5150335 Vali Loss: 0.2721935 Test Loss: 0.3812289
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.8253593444824219
Epoch: 89, Steps: 64 | Train Loss: 0.5156797 Vali Loss: 0.2722103 Test Loss: 0.3812246
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_192_FITS_ETTh2_ftM_sl180_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.36004894971847534, mae:0.3872576951980591, rse:0.4811955690383911, corr:[0.26541328 0.2693669  0.26746136 0.2689861  0.26725408 0.2659791
 0.26584676 0.26453358 0.26342922 0.26250613 0.26112682 0.25965425
 0.25809443 0.2568839  0.25618812 0.25544137 0.25478777 0.25417653
 0.25324056 0.25210443 0.2508571  0.24967879 0.24849069 0.24637838
 0.24350236 0.24124965 0.23941728 0.23754455 0.2358106  0.23397338
 0.23192886 0.23021367 0.22846578 0.22633117 0.22498019 0.22361928
 0.22155172 0.22028409 0.21964213 0.21815236 0.21732311 0.2171443
 0.21603847 0.2147024  0.21400288 0.21262573 0.21097    0.2091391
 0.20610103 0.20357779 0.20207058 0.19999354 0.19761707 0.19621995
 0.19406894 0.19137295 0.19014746 0.18894175 0.1873941  0.18638116
 0.18572932 0.18455829 0.18463758 0.18448335 0.18347189 0.18299691
 0.18274362 0.18157884 0.18098204 0.18089138 0.17973772 0.17828317
 0.17654546 0.1748112  0.17349042 0.17250973 0.1708975  0.16996546
 0.1698024  0.168836   0.16795726 0.16796313 0.1680051  0.16741666
 0.1674635  0.16752535 0.16717587 0.166933   0.16665636 0.16577846
 0.16550411 0.16545132 0.16498323 0.16473825 0.16480283 0.16383636
 0.16199799 0.16099897 0.15960264 0.15824685 0.15746307 0.15673466
 0.15592077 0.15551591 0.15538508 0.15490662 0.15501752 0.15505774
 0.15479437 0.15442117 0.15425828 0.15361078 0.15317737 0.1530136
 0.15265363 0.15218475 0.1519045  0.1513313  0.15040986 0.14909707
 0.14698596 0.1451509  0.1437777  0.14251623 0.14106992 0.13988394
 0.13889223 0.1379134  0.1372271  0.13657317 0.13596302 0.13537423
 0.13507679 0.13461491 0.13462794 0.13441384 0.13382295 0.13347776
 0.13338293 0.13284932 0.13258074 0.13259755 0.13182892 0.13048531
 0.12799558 0.12593089 0.12429754 0.12308509 0.12164338 0.12084672
 0.12073065 0.12001955 0.11949583 0.11965269 0.1201629  0.12011686
 0.12075754 0.12099609 0.1210617  0.12138486 0.12153845 0.12098569
 0.12097992 0.12119196 0.12059785 0.12063016 0.12096028 0.11976327
 0.11728178 0.11617433 0.11502706 0.11403098 0.11284205 0.11154412
 0.11086816 0.11011686 0.10940247 0.10887385 0.10949129 0.10941496
 0.10922846 0.1092855  0.10950866 0.10924223 0.10921366 0.10830789
 0.10750353 0.1072262  0.10693194 0.10630595 0.10671768 0.10663451]
