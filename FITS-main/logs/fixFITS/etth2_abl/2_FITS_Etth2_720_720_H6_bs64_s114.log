Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.373224973678589
Epoch: 1, Steps: 56 | Train Loss: 0.8439728 Vali Loss: 0.8233936 Test Loss: 0.4685059
Validation loss decreased (inf --> 0.823394).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8393850326538086
Epoch: 2, Steps: 56 | Train Loss: 0.6805049 Vali Loss: 0.7694822 Test Loss: 0.4303380
Validation loss decreased (0.823394 --> 0.769482).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.970905065536499
Epoch: 3, Steps: 56 | Train Loss: 0.6127637 Vali Loss: 0.7426921 Test Loss: 0.4157301
Validation loss decreased (0.769482 --> 0.742692).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.4577460289001465
Epoch: 4, Steps: 56 | Train Loss: 0.5776770 Vali Loss: 0.7294474 Test Loss: 0.4091251
Validation loss decreased (0.742692 --> 0.729447).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.2927350997924805
Epoch: 5, Steps: 56 | Train Loss: 0.5549880 Vali Loss: 0.7190614 Test Loss: 0.4054162
Validation loss decreased (0.729447 --> 0.719061).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.215470552444458
Epoch: 6, Steps: 56 | Train Loss: 0.5381637 Vali Loss: 0.7112554 Test Loss: 0.4029370
Validation loss decreased (0.719061 --> 0.711255).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.393284797668457
Epoch: 7, Steps: 56 | Train Loss: 0.5241843 Vali Loss: 0.7065097 Test Loss: 0.4010228
Validation loss decreased (0.711255 --> 0.706510).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.806612968444824
Epoch: 8, Steps: 56 | Train Loss: 0.5128724 Vali Loss: 0.6988652 Test Loss: 0.3995027
Validation loss decreased (0.706510 --> 0.698865).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.1396400928497314
Epoch: 9, Steps: 56 | Train Loss: 0.5035322 Vali Loss: 0.6985557 Test Loss: 0.3981951
Validation loss decreased (0.698865 --> 0.698556).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.0177366733551025
Epoch: 10, Steps: 56 | Train Loss: 0.4956075 Vali Loss: 0.6957971 Test Loss: 0.3969595
Validation loss decreased (0.698556 --> 0.695797).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.4189674854278564
Epoch: 11, Steps: 56 | Train Loss: 0.4883749 Vali Loss: 0.6897939 Test Loss: 0.3958244
Validation loss decreased (0.695797 --> 0.689794).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.2598164081573486
Epoch: 12, Steps: 56 | Train Loss: 0.4822478 Vali Loss: 0.6901047 Test Loss: 0.3948789
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.0637969970703125
Epoch: 13, Steps: 56 | Train Loss: 0.4766132 Vali Loss: 0.6871610 Test Loss: 0.3939822
Validation loss decreased (0.689794 --> 0.687161).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.263890266418457
Epoch: 14, Steps: 56 | Train Loss: 0.4701826 Vali Loss: 0.6823565 Test Loss: 0.3931614
Validation loss decreased (0.687161 --> 0.682356).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.1176085472106934
Epoch: 15, Steps: 56 | Train Loss: 0.4671918 Vali Loss: 0.6847184 Test Loss: 0.3923631
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.9376060962677002
Epoch: 16, Steps: 56 | Train Loss: 0.4634589 Vali Loss: 0.6801493 Test Loss: 0.3917192
Validation loss decreased (0.682356 --> 0.680149).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.328758478164673
Epoch: 17, Steps: 56 | Train Loss: 0.4599186 Vali Loss: 0.6837078 Test Loss: 0.3910541
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8920187950134277
Epoch: 18, Steps: 56 | Train Loss: 0.4564750 Vali Loss: 0.6809015 Test Loss: 0.3904948
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0910894870758057
Epoch: 19, Steps: 56 | Train Loss: 0.4536076 Vali Loss: 0.6784077 Test Loss: 0.3899239
Validation loss decreased (0.680149 --> 0.678408).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.1113462448120117
Epoch: 20, Steps: 56 | Train Loss: 0.4510100 Vali Loss: 0.6778777 Test Loss: 0.3894759
Validation loss decreased (0.678408 --> 0.677878).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.85971999168396
Epoch: 21, Steps: 56 | Train Loss: 0.4487929 Vali Loss: 0.6743657 Test Loss: 0.3890871
Validation loss decreased (0.677878 --> 0.674366).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.9716997146606445
Epoch: 22, Steps: 56 | Train Loss: 0.4469336 Vali Loss: 0.6716019 Test Loss: 0.3886207
Validation loss decreased (0.674366 --> 0.671602).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.081615447998047
Epoch: 23, Steps: 56 | Train Loss: 0.4456039 Vali Loss: 0.6772313 Test Loss: 0.3882784
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.150172472000122
Epoch: 24, Steps: 56 | Train Loss: 0.4427195 Vali Loss: 0.6729291 Test Loss: 0.3879431
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.132728338241577
Epoch: 25, Steps: 56 | Train Loss: 0.4416257 Vali Loss: 0.6720151 Test Loss: 0.3876270
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.1874356269836426
Epoch: 26, Steps: 56 | Train Loss: 0.4408229 Vali Loss: 0.6683784 Test Loss: 0.3873186
Validation loss decreased (0.671602 --> 0.668378).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.065279006958008
Epoch: 27, Steps: 56 | Train Loss: 0.4389553 Vali Loss: 0.6674062 Test Loss: 0.3870178
Validation loss decreased (0.668378 --> 0.667406).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.9404716491699219
Epoch: 28, Steps: 56 | Train Loss: 0.4371364 Vali Loss: 0.6699924 Test Loss: 0.3867828
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7893733978271484
Epoch: 29, Steps: 56 | Train Loss: 0.4366953 Vali Loss: 0.6670974 Test Loss: 0.3865628
Validation loss decreased (0.667406 --> 0.667097).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.8849477767944336
Epoch: 30, Steps: 56 | Train Loss: 0.4357535 Vali Loss: 0.6714122 Test Loss: 0.3863318
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.8950037956237793
Epoch: 31, Steps: 56 | Train Loss: 0.4348363 Vali Loss: 0.6664007 Test Loss: 0.3861407
Validation loss decreased (0.667097 --> 0.666401).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.6876320838928223
Epoch: 32, Steps: 56 | Train Loss: 0.4337881 Vali Loss: 0.6654494 Test Loss: 0.3859609
Validation loss decreased (0.666401 --> 0.665449).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5721769332885742
Epoch: 33, Steps: 56 | Train Loss: 0.4321573 Vali Loss: 0.6673758 Test Loss: 0.3858234
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7074038982391357
Epoch: 34, Steps: 56 | Train Loss: 0.4332850 Vali Loss: 0.6702859 Test Loss: 0.3856205
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.4565300941467285
Epoch: 35, Steps: 56 | Train Loss: 0.4315990 Vali Loss: 0.6650375 Test Loss: 0.3854860
Validation loss decreased (0.665449 --> 0.665038).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.5493733882904053
Epoch: 36, Steps: 56 | Train Loss: 0.4308859 Vali Loss: 0.6663355 Test Loss: 0.3853460
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9581921100616455
Epoch: 37, Steps: 56 | Train Loss: 0.4294571 Vali Loss: 0.6644074 Test Loss: 0.3852424
Validation loss decreased (0.665038 --> 0.664407).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7563040256500244
Epoch: 38, Steps: 56 | Train Loss: 0.4290413 Vali Loss: 0.6656259 Test Loss: 0.3851165
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.667515754699707
Epoch: 39, Steps: 56 | Train Loss: 0.4294511 Vali Loss: 0.6648089 Test Loss: 0.3850167
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.6126747131347656
Epoch: 40, Steps: 56 | Train Loss: 0.4287369 Vali Loss: 0.6650574 Test Loss: 0.3849272
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6907000541687012
Epoch: 41, Steps: 56 | Train Loss: 0.4281862 Vali Loss: 0.6619664 Test Loss: 0.3848412
Validation loss decreased (0.664407 --> 0.661966).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.7366108894348145
Epoch: 42, Steps: 56 | Train Loss: 0.4277639 Vali Loss: 0.6638950 Test Loss: 0.3847304
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.6454241275787354
Epoch: 43, Steps: 56 | Train Loss: 0.4278521 Vali Loss: 0.6644393 Test Loss: 0.3846691
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.074005603790283
Epoch: 44, Steps: 56 | Train Loss: 0.4264848 Vali Loss: 0.6627429 Test Loss: 0.3845846
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7588567733764648
Epoch: 45, Steps: 56 | Train Loss: 0.4265414 Vali Loss: 0.6638303 Test Loss: 0.3845018
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.7328393459320068
Epoch: 46, Steps: 56 | Train Loss: 0.4260136 Vali Loss: 0.6624082 Test Loss: 0.3844405
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.8160827159881592
Epoch: 47, Steps: 56 | Train Loss: 0.4262236 Vali Loss: 0.6634518 Test Loss: 0.3843906
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.8460993766784668
Epoch: 48, Steps: 56 | Train Loss: 0.4248585 Vali Loss: 0.6632980 Test Loss: 0.3843341
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.946054458618164
Epoch: 49, Steps: 56 | Train Loss: 0.4257370 Vali Loss: 0.6636126 Test Loss: 0.3842770
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.234835386276245
Epoch: 50, Steps: 56 | Train Loss: 0.4252169 Vali Loss: 0.6619071 Test Loss: 0.3842262
Validation loss decreased (0.661966 --> 0.661907).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.7775044441223145
Epoch: 51, Steps: 56 | Train Loss: 0.4245512 Vali Loss: 0.6615332 Test Loss: 0.3841873
Validation loss decreased (0.661907 --> 0.661533).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6441774368286133
Epoch: 52, Steps: 56 | Train Loss: 0.4246621 Vali Loss: 0.6608607 Test Loss: 0.3841454
Validation loss decreased (0.661533 --> 0.660861).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.668994665145874
Epoch: 53, Steps: 56 | Train Loss: 0.4240512 Vali Loss: 0.6611607 Test Loss: 0.3841024
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.6887388229370117
Epoch: 54, Steps: 56 | Train Loss: 0.4245150 Vali Loss: 0.6600907 Test Loss: 0.3840728
Validation loss decreased (0.660861 --> 0.660091).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.6790385246276855
Epoch: 55, Steps: 56 | Train Loss: 0.4237620 Vali Loss: 0.6620877 Test Loss: 0.3840480
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.488464117050171
Epoch: 56, Steps: 56 | Train Loss: 0.4234167 Vali Loss: 0.6584278 Test Loss: 0.3840055
Validation loss decreased (0.660091 --> 0.658428).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.8324041366577148
Epoch: 57, Steps: 56 | Train Loss: 0.4236206 Vali Loss: 0.6631469 Test Loss: 0.3839790
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.7750287055969238
Epoch: 58, Steps: 56 | Train Loss: 0.4239603 Vali Loss: 0.6620724 Test Loss: 0.3839493
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.7072699069976807
Epoch: 59, Steps: 56 | Train Loss: 0.4238379 Vali Loss: 0.6607913 Test Loss: 0.3839187
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.705540418624878
Epoch: 60, Steps: 56 | Train Loss: 0.4228715 Vali Loss: 0.6600606 Test Loss: 0.3838953
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.7867741584777832
Epoch: 61, Steps: 56 | Train Loss: 0.4231030 Vali Loss: 0.6606708 Test Loss: 0.3838770
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6970574855804443
Epoch: 62, Steps: 56 | Train Loss: 0.4227432 Vali Loss: 0.6626701 Test Loss: 0.3838497
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.5870580673217773
Epoch: 63, Steps: 56 | Train Loss: 0.4231243 Vali Loss: 0.6595631 Test Loss: 0.3838252
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.6723151206970215
Epoch: 64, Steps: 56 | Train Loss: 0.4228886 Vali Loss: 0.6587713 Test Loss: 0.3838118
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.9140915870666504
Epoch: 65, Steps: 56 | Train Loss: 0.4228382 Vali Loss: 0.6596197 Test Loss: 0.3837857
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.687279224395752
Epoch: 66, Steps: 56 | Train Loss: 0.4216990 Vali Loss: 0.6595456 Test Loss: 0.3837734
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.7246484756469727
Epoch: 67, Steps: 56 | Train Loss: 0.4230434 Vali Loss: 0.6578751 Test Loss: 0.3837582
Validation loss decreased (0.658428 --> 0.657875).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.9573609828948975
Epoch: 68, Steps: 56 | Train Loss: 0.4227744 Vali Loss: 0.6539507 Test Loss: 0.3837426
Validation loss decreased (0.657875 --> 0.653951).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.8214972019195557
Epoch: 69, Steps: 56 | Train Loss: 0.4222314 Vali Loss: 0.6581643 Test Loss: 0.3837323
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5705745220184326
Epoch: 70, Steps: 56 | Train Loss: 0.4223341 Vali Loss: 0.6599539 Test Loss: 0.3837189
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.6742515563964844
Epoch: 71, Steps: 56 | Train Loss: 0.4221194 Vali Loss: 0.6593859 Test Loss: 0.3837077
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.5383377075195312
Epoch: 72, Steps: 56 | Train Loss: 0.4221127 Vali Loss: 0.6599090 Test Loss: 0.3836969
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.8483457565307617
Epoch: 73, Steps: 56 | Train Loss: 0.4213914 Vali Loss: 0.6600239 Test Loss: 0.3836854
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.6835057735443115
Epoch: 74, Steps: 56 | Train Loss: 0.4215880 Vali Loss: 0.6557190 Test Loss: 0.3836745
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.631105661392212
Epoch: 75, Steps: 56 | Train Loss: 0.4224902 Vali Loss: 0.6592098 Test Loss: 0.3836660
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.6476080417633057
Epoch: 76, Steps: 56 | Train Loss: 0.4220413 Vali Loss: 0.6576328 Test Loss: 0.3836560
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.7539260387420654
Epoch: 77, Steps: 56 | Train Loss: 0.4218213 Vali Loss: 0.6539791 Test Loss: 0.3836486
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.8796236515045166
Epoch: 78, Steps: 56 | Train Loss: 0.4215222 Vali Loss: 0.6581523 Test Loss: 0.3836386
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.511958122253418
Epoch: 79, Steps: 56 | Train Loss: 0.4209403 Vali Loss: 0.6589505 Test Loss: 0.3836333
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.6822683811187744
Epoch: 80, Steps: 56 | Train Loss: 0.4213028 Vali Loss: 0.6583515 Test Loss: 0.3836242
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.6164875030517578
Epoch: 81, Steps: 56 | Train Loss: 0.4211192 Vali Loss: 0.6602857 Test Loss: 0.3836201
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.6980364322662354
Epoch: 82, Steps: 56 | Train Loss: 0.4213491 Vali Loss: 0.6568317 Test Loss: 0.3836147
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.5978608131408691
Epoch: 83, Steps: 56 | Train Loss: 0.4213817 Vali Loss: 0.6541865 Test Loss: 0.3836064
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.6830174922943115
Epoch: 84, Steps: 56 | Train Loss: 0.4211073 Vali Loss: 0.6571932 Test Loss: 0.3836012
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.8440256118774414
Epoch: 85, Steps: 56 | Train Loss: 0.4217914 Vali Loss: 0.6604970 Test Loss: 0.3835988
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.7804019451141357
Epoch: 86, Steps: 56 | Train Loss: 0.4217343 Vali Loss: 0.6604402 Test Loss: 0.3835914
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.7857184410095215
Epoch: 87, Steps: 56 | Train Loss: 0.4211677 Vali Loss: 0.6593868 Test Loss: 0.3835877
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.610126256942749
Epoch: 88, Steps: 56 | Train Loss: 0.4214904 Vali Loss: 0.6556528 Test Loss: 0.3835846
EarlyStopping counter: 20 out of 20
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8298168182373047
Epoch: 1, Steps: 56 | Train Loss: 0.8088925 Vali Loss: 0.6509938 Test Loss: 0.3824771
Validation loss decreased (inf --> 0.650994).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6865861415863037
Epoch: 2, Steps: 56 | Train Loss: 0.8053803 Vali Loss: 0.6506689 Test Loss: 0.3815247
Validation loss decreased (0.650994 --> 0.650669).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.6035172939300537
Epoch: 3, Steps: 56 | Train Loss: 0.8023467 Vali Loss: 0.6472830 Test Loss: 0.3807666
Validation loss decreased (0.650669 --> 0.647283).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.5480074882507324
Epoch: 4, Steps: 56 | Train Loss: 0.7994920 Vali Loss: 0.6477393 Test Loss: 0.3802778
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7675156593322754
Epoch: 5, Steps: 56 | Train Loss: 0.7998116 Vali Loss: 0.6447964 Test Loss: 0.3799632
Validation loss decreased (0.647283 --> 0.644796).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.7284772396087646
Epoch: 6, Steps: 56 | Train Loss: 0.8002589 Vali Loss: 0.6447048 Test Loss: 0.3797329
Validation loss decreased (0.644796 --> 0.644705).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.6659305095672607
Epoch: 7, Steps: 56 | Train Loss: 0.8001645 Vali Loss: 0.6454287 Test Loss: 0.3795598
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.7445344924926758
Epoch: 8, Steps: 56 | Train Loss: 0.7984321 Vali Loss: 0.6454093 Test Loss: 0.3794883
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.6158392429351807
Epoch: 9, Steps: 56 | Train Loss: 0.7976969 Vali Loss: 0.6435743 Test Loss: 0.3794492
Validation loss decreased (0.644705 --> 0.643574).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.7001323699951172
Epoch: 10, Steps: 56 | Train Loss: 0.7982147 Vali Loss: 0.6385977 Test Loss: 0.3795296
Validation loss decreased (0.643574 --> 0.638598).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9379427433013916
Epoch: 11, Steps: 56 | Train Loss: 0.7977909 Vali Loss: 0.6453375 Test Loss: 0.3794938
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.7260324954986572
Epoch: 12, Steps: 56 | Train Loss: 0.7970597 Vali Loss: 0.6438152 Test Loss: 0.3794224
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.7091236114501953
Epoch: 13, Steps: 56 | Train Loss: 0.7976869 Vali Loss: 0.6433914 Test Loss: 0.3795255
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.795353651046753
Epoch: 14, Steps: 56 | Train Loss: 0.7967733 Vali Loss: 0.6421864 Test Loss: 0.3794000
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.8661365509033203
Epoch: 15, Steps: 56 | Train Loss: 0.7980450 Vali Loss: 0.6444708 Test Loss: 0.3794714
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7155117988586426
Epoch: 16, Steps: 56 | Train Loss: 0.7971579 Vali Loss: 0.6436059 Test Loss: 0.3794557
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.6565461158752441
Epoch: 17, Steps: 56 | Train Loss: 0.7975538 Vali Loss: 0.6445626 Test Loss: 0.3794241
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7608885765075684
Epoch: 18, Steps: 56 | Train Loss: 0.7961306 Vali Loss: 0.6425211 Test Loss: 0.3794361
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6976993083953857
Epoch: 19, Steps: 56 | Train Loss: 0.7975148 Vali Loss: 0.6425389 Test Loss: 0.3794565
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8661472797393799
Epoch: 20, Steps: 56 | Train Loss: 0.7977962 Vali Loss: 0.6419368 Test Loss: 0.3794186
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.7321434020996094
Epoch: 21, Steps: 56 | Train Loss: 0.7977581 Vali Loss: 0.6440874 Test Loss: 0.3795228
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.6656675338745117
Epoch: 22, Steps: 56 | Train Loss: 0.7973657 Vali Loss: 0.6399490 Test Loss: 0.3794196
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8019344806671143
Epoch: 23, Steps: 56 | Train Loss: 0.7982163 Vali Loss: 0.6379973 Test Loss: 0.3794258
Validation loss decreased (0.638598 --> 0.637997).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.7706327438354492
Epoch: 24, Steps: 56 | Train Loss: 0.7975080 Vali Loss: 0.6416953 Test Loss: 0.3794721
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.6711845397949219
Epoch: 25, Steps: 56 | Train Loss: 0.7963105 Vali Loss: 0.6415955 Test Loss: 0.3794664
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8326361179351807
Epoch: 26, Steps: 56 | Train Loss: 0.7968167 Vali Loss: 0.6410859 Test Loss: 0.3794670
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6664035320281982
Epoch: 27, Steps: 56 | Train Loss: 0.7963585 Vali Loss: 0.6334621 Test Loss: 0.3795102
Validation loss decreased (0.637997 --> 0.633462).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6800634860992432
Epoch: 28, Steps: 56 | Train Loss: 0.7963689 Vali Loss: 0.6400421 Test Loss: 0.3795018
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.859098196029663
Epoch: 29, Steps: 56 | Train Loss: 0.7971842 Vali Loss: 0.6395616 Test Loss: 0.3794878
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5425302982330322
Epoch: 30, Steps: 56 | Train Loss: 0.7966403 Vali Loss: 0.6398233 Test Loss: 0.3794673
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.677907943725586
Epoch: 31, Steps: 56 | Train Loss: 0.7969130 Vali Loss: 0.6400812 Test Loss: 0.3794946
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.8804912567138672
Epoch: 32, Steps: 56 | Train Loss: 0.7962143 Vali Loss: 0.6391849 Test Loss: 0.3794717
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.6912648677825928
Epoch: 33, Steps: 56 | Train Loss: 0.7967762 Vali Loss: 0.6411343 Test Loss: 0.3794897
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7454748153686523
Epoch: 34, Steps: 56 | Train Loss: 0.7965096 Vali Loss: 0.6365901 Test Loss: 0.3794833
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.7040643692016602
Epoch: 35, Steps: 56 | Train Loss: 0.7943644 Vali Loss: 0.6395361 Test Loss: 0.3795036
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.648665189743042
Epoch: 36, Steps: 56 | Train Loss: 0.7947887 Vali Loss: 0.6389837 Test Loss: 0.3794876
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9232444763183594
Epoch: 37, Steps: 56 | Train Loss: 0.7968916 Vali Loss: 0.6402596 Test Loss: 0.3794782
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.623866081237793
Epoch: 38, Steps: 56 | Train Loss: 0.7962415 Vali Loss: 0.6340435 Test Loss: 0.3794728
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.619459629058838
Epoch: 39, Steps: 56 | Train Loss: 0.7946405 Vali Loss: 0.6410074 Test Loss: 0.3795015
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.841698408126831
Epoch: 40, Steps: 56 | Train Loss: 0.7960585 Vali Loss: 0.6382250 Test Loss: 0.3794900
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.8178961277008057
Epoch: 41, Steps: 56 | Train Loss: 0.7951790 Vali Loss: 0.6401306 Test Loss: 0.3795064
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.710202932357788
Epoch: 42, Steps: 56 | Train Loss: 0.7949948 Vali Loss: 0.6430486 Test Loss: 0.3794802
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.5799264907836914
Epoch: 43, Steps: 56 | Train Loss: 0.7957632 Vali Loss: 0.6411889 Test Loss: 0.3794962
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.6692075729370117
Epoch: 44, Steps: 56 | Train Loss: 0.7951815 Vali Loss: 0.6405694 Test Loss: 0.3794930
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7628238201141357
Epoch: 45, Steps: 56 | Train Loss: 0.7964020 Vali Loss: 0.6377424 Test Loss: 0.3795185
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.5227644443511963
Epoch: 46, Steps: 56 | Train Loss: 0.7962572 Vali Loss: 0.6404513 Test Loss: 0.3794901
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.547682285308838
Epoch: 47, Steps: 56 | Train Loss: 0.7950892 Vali Loss: 0.6375613 Test Loss: 0.3794974
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3781009316444397, mae:0.4227309823036194, rse:0.49148425459861755, corr:[0.21972163 0.22162393 0.21986268 0.2199733  0.21951944 0.2181019
 0.21770145 0.21745464 0.21608941 0.2145405  0.21378186 0.21264009
 0.2109297  0.20973474 0.20916706 0.2084853  0.20762198 0.20707855
 0.20648344 0.2054133  0.20422912 0.20338453 0.20248395 0.20125581
 0.19984266 0.19856592 0.19739431 0.19629598 0.19560598 0.1951094
 0.19454868 0.19370264 0.19281934 0.19194002 0.1909163  0.18987475
 0.18906362 0.18851934 0.18779571 0.18692379 0.18633442 0.18596976
 0.18547471 0.18474117 0.184013   0.18348907 0.18300213 0.18167992
 0.17990121 0.17833765 0.17728946 0.17645349 0.17577584 0.17520832
 0.17467117 0.17420088 0.17340787 0.17252603 0.17198984 0.17167583
 0.17131962 0.17125171 0.17171645 0.1720614  0.17166494 0.17173868
 0.17217194 0.17206028 0.1716372  0.17150623 0.1714543  0.17105703
 0.17074843 0.17060246 0.17015684 0.16930035 0.16916868 0.16960834
 0.16954105 0.16897532 0.1688728  0.16899388 0.16874042 0.16862762
 0.16898118 0.16936097 0.16933994 0.16933557 0.16947673 0.16949797
 0.16922289 0.16896002 0.16916737 0.16918796 0.16913824 0.16926458
 0.16939187 0.16923757 0.16903532 0.16898328 0.1688255  0.16817422
 0.16790244 0.16803142 0.16813625 0.16776368 0.16761152 0.16795419
 0.16809656 0.1679266  0.16771977 0.16757469 0.16734105 0.16742481
 0.16757607 0.16721134 0.16655576 0.16604885 0.16587207 0.16523783
 0.16427676 0.16363336 0.16328447 0.16254902 0.16157694 0.16115175
 0.16108227 0.16058393 0.15975305 0.15885076 0.15818377 0.1576075
 0.15733817 0.15695876 0.15637292 0.15606093 0.15593907 0.15570208
 0.15498225 0.154232   0.15417334 0.15419988 0.15382113 0.15287104
 0.15147695 0.15041943 0.14973265 0.14924626 0.14884126 0.14857846
 0.14834175 0.14781412 0.147378   0.14709708 0.14677194 0.14602172
 0.14507332 0.14442275 0.14409444 0.14381815 0.14351721 0.14381349
 0.14416833 0.14393817 0.14386247 0.14434609 0.14482585 0.14429244
 0.1433619  0.14276767 0.14246057 0.14201704 0.14170636 0.1414998
 0.1409612  0.13989854 0.13907923 0.13876908 0.13819216 0.13738665
 0.13688095 0.13670091 0.13660537 0.13655226 0.13669176 0.1372299
 0.13752633 0.13759767 0.13778417 0.13821417 0.13902502 0.1395356
 0.13938637 0.1390431  0.13898954 0.13941994 0.13979718 0.13978973
 0.13994263 0.13984376 0.13943985 0.13911171 0.13947205 0.1396176
 0.13898882 0.13842028 0.13842334 0.13882166 0.13923737 0.1399235
 0.14063242 0.14072102 0.1405068  0.14063805 0.14119999 0.14134796
 0.14097412 0.14041999 0.13982882 0.13947047 0.13923214 0.1393904
 0.13922769 0.13911939 0.13931188 0.13986717 0.14032197 0.1403201
 0.14029442 0.14030094 0.14048146 0.1411017  0.14191532 0.14257424
 0.14305876 0.14359504 0.14414088 0.14434251 0.14467801 0.14563464
 0.14673506 0.14698294 0.1467155  0.14690658 0.147348   0.14743575
 0.14740857 0.147814   0.14898916 0.14981025 0.14984307 0.14968957
 0.15002786 0.1509024  0.15179804 0.15282038 0.15367514 0.15431882
 0.15469807 0.15531325 0.15622947 0.15689895 0.15725848 0.15765086
 0.15864272 0.15951896 0.1596965  0.1595584  0.15962066 0.15998313
 0.16018772 0.16047348 0.16117245 0.16174604 0.16207941 0.16242105
 0.16300355 0.163664   0.16403075 0.1644469  0.16499814 0.16524045
 0.16502272 0.16500044 0.16557328 0.1664741  0.16708873 0.16710664
 0.16710173 0.16730389 0.16744742 0.16752735 0.16753116 0.16746998
 0.16714054 0.16670248 0.16672955 0.16728418 0.16771123 0.16761145
 0.16763121 0.16831364 0.16893297 0.16901074 0.16899233 0.16948481
 0.16975304 0.16982818 0.16979335 0.16994505 0.17034158 0.1708406
 0.17096362 0.17016329 0.16942102 0.16952576 0.17005722 0.169756
 0.16873737 0.16838245 0.16885282 0.16947939 0.16952248 0.16932708
 0.16897741 0.16880123 0.16943681 0.17032436 0.17074001 0.17079584
 0.17118253 0.1720225  0.17238864 0.17253679 0.17336881 0.17413713
 0.17381449 0.17316923 0.17339253 0.17395242 0.1741667  0.17388411
 0.17375007 0.17380258 0.17411953 0.17462815 0.17481956 0.1749244
 0.17513497 0.17570753 0.17608923 0.17614749 0.17648563 0.17749448
 0.17846757 0.17915754 0.17968044 0.18013059 0.18028153 0.18047619
 0.18113303 0.1816661  0.1814623  0.18092231 0.18088964 0.18116175
 0.18159194 0.18241806 0.1831274  0.18328421 0.18297073 0.18304235
 0.18331271 0.1832368  0.1831343  0.18364275 0.18428965 0.18412916
 0.18356018 0.18352877 0.18374287 0.18352708 0.18337862 0.18395612
 0.18451421 0.18435489 0.18385914 0.1836584  0.18389235 0.1843804
 0.18496446 0.18543406 0.18558714 0.18562964 0.18593279 0.18632385
 0.18661019 0.1863625  0.18587291 0.18544269 0.18517682 0.18517855
 0.18514225 0.18486142 0.18452121 0.1845985  0.1847224  0.18441755
 0.18393403 0.18377832 0.18373722 0.18347076 0.1833008  0.18319288
 0.18294683 0.18290009 0.18320876 0.18332423 0.18278825 0.18230192
 0.1819533  0.18149325 0.18071817 0.18003774 0.17939712 0.17826277
 0.17690083 0.17586605 0.17552035 0.17510973 0.17416698 0.17348786
 0.17323695 0.17297514 0.17260532 0.17204578 0.17155214 0.17101008
 0.17044546 0.170292   0.1701617  0.16985042 0.16893673 0.16808635
 0.16748592 0.16723715 0.16690898 0.16660891 0.1664236  0.16616684
 0.16562799 0.1650537  0.16490522 0.16539402 0.16565946 0.16510984
 0.16450551 0.16440734 0.16431719 0.1638867  0.1634357  0.16324899
 0.16308351 0.16278881 0.16238071 0.1622075  0.16167127 0.16069116
 0.16036357 0.16027263 0.15972736 0.15883408 0.1585179  0.15861134
 0.15837234 0.15781392 0.1574904  0.15756804 0.15732393 0.15667923
 0.15613927 0.15590103 0.15556225 0.1547761  0.15410906 0.15352905
 0.15270776 0.15156105 0.15059559 0.14992434 0.14938638 0.14858882
 0.14765215 0.14711057 0.1469165  0.14658628 0.14578587 0.1450989
 0.14465803 0.14419065 0.14387266 0.14373952 0.14338537 0.14229392
 0.1408839  0.13992588 0.13945699 0.1390583  0.13848783 0.13765788
 0.13666205 0.13626274 0.13618693 0.13551666 0.1343794  0.13350728
 0.1330729  0.13243449 0.13172789 0.13130112 0.13101894 0.13053524
 0.12974596 0.12911408 0.12839256 0.12743632 0.1264604  0.125549
 0.12416547 0.12283932 0.12229105 0.12156828 0.12017936 0.11875277
 0.11821762 0.1176729  0.11618321 0.11458842 0.11377928 0.11308517
 0.11186678 0.11097165 0.11071299 0.10973748 0.10797709 0.10723916
 0.10706349 0.10618094 0.10492726 0.1044988  0.10460626 0.10334609
 0.10127738 0.10000277 0.09933794 0.09802397 0.09664518 0.09629704
 0.09618977 0.09499094 0.09365118 0.0930606  0.09323297 0.09318898
 0.09269045 0.09191527 0.09106181 0.09023999 0.09001184 0.08986392
 0.08921096 0.08832901 0.08751973 0.08666246 0.08531378 0.08374146
 0.08229143 0.08135755 0.08057108 0.07978886 0.07906639 0.07816189
 0.07696128 0.0755548  0.07425632 0.07330273 0.07238496 0.07163709
 0.07114281 0.07092925 0.0703316  0.0690058  0.0680848  0.0681831
 0.06852179 0.06849062 0.0678623  0.06685116 0.06549828 0.06433187
 0.06353533 0.06268182 0.06128804 0.06030646 0.05957942 0.05853706
 0.05677289 0.05529332 0.05436705 0.05328503 0.05192243 0.05106217
 0.05125303 0.0512864  0.05063554 0.05003295 0.04989108 0.05006021
 0.04990918 0.04984747 0.04987142 0.0500312  0.04990605 0.04953701
 0.04843767 0.04709861 0.04640912 0.04592021 0.04499501 0.04441097
 0.04442432 0.04398834 0.0426505  0.04188405 0.04236474 0.0428444
 0.04167396 0.03986094 0.03962    0.04012452 0.03921913 0.03793127
 0.03767296 0.03789949 0.03776142 0.03733727 0.03658072 0.03589675
 0.03516816 0.03445057 0.03385771 0.03325891 0.03288456 0.03208382
 0.03086742 0.02993648 0.02969494 0.02998282 0.03025744 0.03012999
 0.02939671 0.02886097 0.02920685 0.02959114 0.02950583 0.02914895
 0.02913962 0.02929957 0.0291003  0.02948026 0.0303188  0.03032992
 0.02906823 0.02825473 0.02862057 0.02823533 0.0268015  0.02533237
 0.0253018  0.02498913 0.02439745 0.0241071  0.0240768  0.02362919
 0.02278741 0.02181236 0.02165684 0.02158693 0.02130669 0.02152094
 0.02216798 0.02165019 0.02039983 0.01923947 0.01895672 0.01868678
 0.0174105  0.01495799 0.01302128 0.01221356 0.01201459 0.01080179
 0.0092385  0.00804767 0.00757236 0.00680858 0.00537685 0.00494795
 0.00511776 0.00427239 0.00383036 0.00433732 0.0047721  0.00445009
 0.00446871 0.00537753 0.00461177 0.00232951 0.00326627 0.00911512]
