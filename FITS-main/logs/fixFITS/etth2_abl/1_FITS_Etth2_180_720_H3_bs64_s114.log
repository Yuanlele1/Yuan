Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=34, out_features=170, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5178880.0
params:  5950.0
Trainable parameters:  5950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7502408027648926
Epoch: 1, Steps: 60 | Train Loss: 1.1686322 Vali Loss: 0.7870816 Test Loss: 0.5636885
Validation loss decreased (inf --> 0.787082).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.752619981765747
Epoch: 2, Steps: 60 | Train Loss: 0.9985982 Vali Loss: 0.7210132 Test Loss: 0.4958864
Validation loss decreased (0.787082 --> 0.721013).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.1973400115966797
Epoch: 3, Steps: 60 | Train Loss: 0.9238558 Vali Loss: 0.6905112 Test Loss: 0.4618706
Validation loss decreased (0.721013 --> 0.690511).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.040905237197876
Epoch: 4, Steps: 60 | Train Loss: 0.8834043 Vali Loss: 0.6736575 Test Loss: 0.4431330
Validation loss decreased (0.690511 --> 0.673658).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.2406208515167236
Epoch: 5, Steps: 60 | Train Loss: 0.8594047 Vali Loss: 0.6599200 Test Loss: 0.4319810
Validation loss decreased (0.673658 --> 0.659920).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.300069808959961
Epoch: 6, Steps: 60 | Train Loss: 0.8496646 Vali Loss: 0.6498883 Test Loss: 0.4251499
Validation loss decreased (0.659920 --> 0.649888).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.112410306930542
Epoch: 7, Steps: 60 | Train Loss: 0.8394371 Vali Loss: 0.6525142 Test Loss: 0.4206949
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8747303485870361
Epoch: 8, Steps: 60 | Train Loss: 0.8351332 Vali Loss: 0.6444593 Test Loss: 0.4176782
Validation loss decreased (0.649888 --> 0.644459).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.9338395595550537
Epoch: 9, Steps: 60 | Train Loss: 0.8315693 Vali Loss: 0.6425731 Test Loss: 0.4156125
Validation loss decreased (0.644459 --> 0.642573).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.374690294265747
Epoch: 10, Steps: 60 | Train Loss: 0.8283145 Vali Loss: 0.6430082 Test Loss: 0.4140319
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.2507212162017822
Epoch: 11, Steps: 60 | Train Loss: 0.8267030 Vali Loss: 0.6409497 Test Loss: 0.4128215
Validation loss decreased (0.642573 --> 0.640950).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.0814757347106934
Epoch: 12, Steps: 60 | Train Loss: 0.8254559 Vali Loss: 0.6353127 Test Loss: 0.4119365
Validation loss decreased (0.640950 --> 0.635313).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.7782084941864014
Epoch: 13, Steps: 60 | Train Loss: 0.8238153 Vali Loss: 0.6379291 Test Loss: 0.4112100
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.992260456085205
Epoch: 14, Steps: 60 | Train Loss: 0.8197735 Vali Loss: 0.6341947 Test Loss: 0.4105897
Validation loss decreased (0.635313 --> 0.634195).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.866534948348999
Epoch: 15, Steps: 60 | Train Loss: 0.8199166 Vali Loss: 0.6361901 Test Loss: 0.4101539
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.065208673477173
Epoch: 16, Steps: 60 | Train Loss: 0.8205891 Vali Loss: 0.6349269 Test Loss: 0.4097357
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.176224708557129
Epoch: 17, Steps: 60 | Train Loss: 0.8188063 Vali Loss: 0.6343226 Test Loss: 0.4093474
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.0579886436462402
Epoch: 18, Steps: 60 | Train Loss: 0.8199483 Vali Loss: 0.6340089 Test Loss: 0.4090596
Validation loss decreased (0.634195 --> 0.634009).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9539031982421875
Epoch: 19, Steps: 60 | Train Loss: 0.8182487 Vali Loss: 0.6330626 Test Loss: 0.4087701
Validation loss decreased (0.634009 --> 0.633063).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8148539066314697
Epoch: 20, Steps: 60 | Train Loss: 0.8193844 Vali Loss: 0.6324902 Test Loss: 0.4085348
Validation loss decreased (0.633063 --> 0.632490).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.5882134437561035
Epoch: 21, Steps: 60 | Train Loss: 0.8159369 Vali Loss: 0.6324259 Test Loss: 0.4083353
Validation loss decreased (0.632490 --> 0.632426).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.1167023181915283
Epoch: 22, Steps: 60 | Train Loss: 0.8163145 Vali Loss: 0.6349455 Test Loss: 0.4081545
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.7854678630828857
Epoch: 23, Steps: 60 | Train Loss: 0.8165372 Vali Loss: 0.6302207 Test Loss: 0.4079731
Validation loss decreased (0.632426 --> 0.630221).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.2098209857940674
Epoch: 24, Steps: 60 | Train Loss: 0.8189092 Vali Loss: 0.6342655 Test Loss: 0.4078264
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9873580932617188
Epoch: 25, Steps: 60 | Train Loss: 0.8160770 Vali Loss: 0.6311023 Test Loss: 0.4077002
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8164522647857666
Epoch: 26, Steps: 60 | Train Loss: 0.8174111 Vali Loss: 0.6292219 Test Loss: 0.4075811
Validation loss decreased (0.630221 --> 0.629222).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.9178237915039062
Epoch: 27, Steps: 60 | Train Loss: 0.8187355 Vali Loss: 0.6315647 Test Loss: 0.4074644
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.409104347229004
Epoch: 28, Steps: 60 | Train Loss: 0.8172640 Vali Loss: 0.6333764 Test Loss: 0.4073679
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.9859566688537598
Epoch: 29, Steps: 60 | Train Loss: 0.8158505 Vali Loss: 0.6328781 Test Loss: 0.4072769
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.4693896770477295
Epoch: 30, Steps: 60 | Train Loss: 0.8163836 Vali Loss: 0.6295172 Test Loss: 0.4072109
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.7091732025146484
Epoch: 31, Steps: 60 | Train Loss: 0.8170972 Vali Loss: 0.6308873 Test Loss: 0.4071263
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.0084168910980225
Epoch: 32, Steps: 60 | Train Loss: 0.8163494 Vali Loss: 0.6318390 Test Loss: 0.4070586
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.073103189468384
Epoch: 33, Steps: 60 | Train Loss: 0.8169448 Vali Loss: 0.6278711 Test Loss: 0.4069965
Validation loss decreased (0.629222 --> 0.627871).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.168147563934326
Epoch: 34, Steps: 60 | Train Loss: 0.8171081 Vali Loss: 0.6286801 Test Loss: 0.4069620
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.515373706817627
Epoch: 35, Steps: 60 | Train Loss: 0.8135574 Vali Loss: 0.6296712 Test Loss: 0.4068713
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.4784140586853027
Epoch: 36, Steps: 60 | Train Loss: 0.8149781 Vali Loss: 0.6293811 Test Loss: 0.4068478
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.486490249633789
Epoch: 37, Steps: 60 | Train Loss: 0.8147388 Vali Loss: 0.6275423 Test Loss: 0.4068052
Validation loss decreased (0.627871 --> 0.627542).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.2591497898101807
Epoch: 38, Steps: 60 | Train Loss: 0.8143821 Vali Loss: 0.6310132 Test Loss: 0.4067645
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.2924396991729736
Epoch: 39, Steps: 60 | Train Loss: 0.8131755 Vali Loss: 0.6297892 Test Loss: 0.4067343
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.07899808883667
Epoch: 40, Steps: 60 | Train Loss: 0.8153814 Vali Loss: 0.6285015 Test Loss: 0.4066940
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.1779825687408447
Epoch: 41, Steps: 60 | Train Loss: 0.8149357 Vali Loss: 0.6282193 Test Loss: 0.4066489
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.9805197715759277
Epoch: 42, Steps: 60 | Train Loss: 0.8136943 Vali Loss: 0.6277665 Test Loss: 0.4066350
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.7694687843322754
Epoch: 43, Steps: 60 | Train Loss: 0.8149505 Vali Loss: 0.6306983 Test Loss: 0.4065929
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.0345990657806396
Epoch: 44, Steps: 60 | Train Loss: 0.8133717 Vali Loss: 0.6285833 Test Loss: 0.4065739
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.3346383571624756
Epoch: 45, Steps: 60 | Train Loss: 0.8146421 Vali Loss: 0.6284281 Test Loss: 0.4065461
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8531148433685303
Epoch: 46, Steps: 60 | Train Loss: 0.8157384 Vali Loss: 0.6305878 Test Loss: 0.4065154
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.9837243556976318
Epoch: 47, Steps: 60 | Train Loss: 0.8129221 Vali Loss: 0.6288937 Test Loss: 0.4064909
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9254868030548096
Epoch: 48, Steps: 60 | Train Loss: 0.8138946 Vali Loss: 0.6274253 Test Loss: 0.4064716
Validation loss decreased (0.627542 --> 0.627425).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9778125286102295
Epoch: 49, Steps: 60 | Train Loss: 0.8126382 Vali Loss: 0.6288881 Test Loss: 0.4064532
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.007570266723633
Epoch: 50, Steps: 60 | Train Loss: 0.8150676 Vali Loss: 0.6277122 Test Loss: 0.4064351
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.5137271881103516
Epoch: 51, Steps: 60 | Train Loss: 0.8130799 Vali Loss: 0.6293944 Test Loss: 0.4064127
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6767144203186035
Epoch: 52, Steps: 60 | Train Loss: 0.8138250 Vali Loss: 0.6290259 Test Loss: 0.4064020
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 5.470220327377319
Epoch: 53, Steps: 60 | Train Loss: 0.8138137 Vali Loss: 0.6266550 Test Loss: 0.4063835
Validation loss decreased (0.627425 --> 0.626655).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.7723639011383057
Epoch: 54, Steps: 60 | Train Loss: 0.8139767 Vali Loss: 0.6278439 Test Loss: 0.4063686
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.463081121444702
Epoch: 55, Steps: 60 | Train Loss: 0.8145906 Vali Loss: 0.6338143 Test Loss: 0.4063563
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.8337101936340332
Epoch: 56, Steps: 60 | Train Loss: 0.8120337 Vali Loss: 0.6263183 Test Loss: 0.4063453
Validation loss decreased (0.626655 --> 0.626318).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.0219695568084717
Epoch: 57, Steps: 60 | Train Loss: 0.8142947 Vali Loss: 0.6286576 Test Loss: 0.4063325
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.12662410736084
Epoch: 58, Steps: 60 | Train Loss: 0.8139915 Vali Loss: 0.6303279 Test Loss: 0.4063270
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.0450940132141113
Epoch: 59, Steps: 60 | Train Loss: 0.8129071 Vali Loss: 0.6276017 Test Loss: 0.4063175
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9189646244049072
Epoch: 60, Steps: 60 | Train Loss: 0.8140567 Vali Loss: 0.6274397 Test Loss: 0.4063057
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.8865172863006592
Epoch: 61, Steps: 60 | Train Loss: 0.8108227 Vali Loss: 0.6314884 Test Loss: 0.4062962
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.9358420372009277
Epoch: 62, Steps: 60 | Train Loss: 0.8151055 Vali Loss: 0.6283183 Test Loss: 0.4062842
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.996316909790039
Epoch: 63, Steps: 60 | Train Loss: 0.8142542 Vali Loss: 0.6285836 Test Loss: 0.4062795
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.0944631099700928
Epoch: 64, Steps: 60 | Train Loss: 0.8143038 Vali Loss: 0.6289004 Test Loss: 0.4062717
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.1040313243865967
Epoch: 65, Steps: 60 | Train Loss: 0.8106590 Vali Loss: 0.6292028 Test Loss: 0.4062615
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.766589641571045
Epoch: 66, Steps: 60 | Train Loss: 0.8138152 Vali Loss: 0.6254092 Test Loss: 0.4062532
Validation loss decreased (0.626318 --> 0.625409).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.1605472564697266
Epoch: 67, Steps: 60 | Train Loss: 0.8123318 Vali Loss: 0.6279751 Test Loss: 0.4062473
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.1867363452911377
Epoch: 68, Steps: 60 | Train Loss: 0.8118078 Vali Loss: 0.6242554 Test Loss: 0.4062424
Validation loss decreased (0.625409 --> 0.624255).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.077029228210449
Epoch: 69, Steps: 60 | Train Loss: 0.8141300 Vali Loss: 0.6300051 Test Loss: 0.4062357
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.889857292175293
Epoch: 70, Steps: 60 | Train Loss: 0.8123927 Vali Loss: 0.6242241 Test Loss: 0.4062304
Validation loss decreased (0.624255 --> 0.624224).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.8459413051605225
Epoch: 71, Steps: 60 | Train Loss: 0.8143591 Vali Loss: 0.6281418 Test Loss: 0.4062245
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.9390451908111572
Epoch: 72, Steps: 60 | Train Loss: 0.8152516 Vali Loss: 0.6307799 Test Loss: 0.4062185
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.1334540843963623
Epoch: 73, Steps: 60 | Train Loss: 0.8135800 Vali Loss: 0.6287986 Test Loss: 0.4062138
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.7432966232299805
Epoch: 74, Steps: 60 | Train Loss: 0.8135311 Vali Loss: 0.6263391 Test Loss: 0.4062099
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.1438679695129395
Epoch: 75, Steps: 60 | Train Loss: 0.8125545 Vali Loss: 0.6261957 Test Loss: 0.4062064
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.309441328048706
Epoch: 76, Steps: 60 | Train Loss: 0.8137285 Vali Loss: 0.6288759 Test Loss: 0.4062011
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.0729293823242188
Epoch: 77, Steps: 60 | Train Loss: 0.8112444 Vali Loss: 0.6289201 Test Loss: 0.4061965
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.7908046245574951
Epoch: 78, Steps: 60 | Train Loss: 0.8147691 Vali Loss: 0.6259181 Test Loss: 0.4061937
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.18325138092041
Epoch: 79, Steps: 60 | Train Loss: 0.8140670 Vali Loss: 0.6308728 Test Loss: 0.4061902
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.266409397125244
Epoch: 80, Steps: 60 | Train Loss: 0.8116514 Vali Loss: 0.6300920 Test Loss: 0.4061866
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.9981942176818848
Epoch: 81, Steps: 60 | Train Loss: 0.8137687 Vali Loss: 0.6250696 Test Loss: 0.4061835
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.019960403442383
Epoch: 82, Steps: 60 | Train Loss: 0.8141720 Vali Loss: 0.6289804 Test Loss: 0.4061797
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.7848708629608154
Epoch: 83, Steps: 60 | Train Loss: 0.8143380 Vali Loss: 0.6313256 Test Loss: 0.4061774
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.1117985248565674
Epoch: 84, Steps: 60 | Train Loss: 0.8141048 Vali Loss: 0.6316081 Test Loss: 0.4061750
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.31756329536438
Epoch: 85, Steps: 60 | Train Loss: 0.8121014 Vali Loss: 0.6347233 Test Loss: 0.4061713
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.0464277267456055
Epoch: 86, Steps: 60 | Train Loss: 0.8138021 Vali Loss: 0.6284735 Test Loss: 0.4061693
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.280642509460449
Epoch: 87, Steps: 60 | Train Loss: 0.8135181 Vali Loss: 0.6246503 Test Loss: 0.4061675
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.5774807929992676
Epoch: 88, Steps: 60 | Train Loss: 0.8130808 Vali Loss: 0.6281093 Test Loss: 0.4061639
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.8634860515594482
Epoch: 89, Steps: 60 | Train Loss: 0.8135734 Vali Loss: 0.6258535 Test Loss: 0.4061623
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.0798850059509277
Epoch: 90, Steps: 60 | Train Loss: 0.8138271 Vali Loss: 0.6256907 Test Loss: 0.4061601
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4044046401977539, mae:0.43065571784973145, rse:0.5082926154136658, corr:[ 2.17204601e-01  2.21610650e-01  2.20755577e-01  2.18792781e-01
  2.18130946e-01  2.17853218e-01  2.16915846e-01  2.15321764e-01
  2.14278877e-01  2.13597372e-01  2.12735683e-01  2.11112678e-01
  2.09498331e-01  2.08506271e-01  2.08192527e-01  2.07920715e-01
  2.07208440e-01  2.06072420e-01  2.05133915e-01  2.04716951e-01
  2.04462796e-01  2.03846067e-01  2.02628642e-01  2.00969398e-01
  1.99201465e-01  1.97790995e-01  1.96564034e-01  1.95509911e-01
  1.94373354e-01  1.93231240e-01  1.92009315e-01  1.90723225e-01
  1.89399987e-01  1.88253298e-01  1.87370092e-01  1.86405376e-01
  1.85307771e-01  1.84083134e-01  1.83243304e-01  1.82625592e-01
  1.82136133e-01  1.81535602e-01  1.80971101e-01  1.80543154e-01
  1.80127889e-01  1.79399520e-01  1.78097039e-01  1.76120237e-01
  1.73799708e-01  1.71857223e-01  1.70160741e-01  1.68935508e-01
  1.67898610e-01  1.67192861e-01  1.66433617e-01  1.65469572e-01
  1.64639160e-01  1.64086893e-01  1.64049119e-01  1.63861781e-01
  1.63590908e-01  1.62875399e-01  1.62528574e-01  1.62518054e-01
  1.62807822e-01  1.62873238e-01  1.62599966e-01  1.62250042e-01
  1.62234947e-01  1.62463203e-01  1.62235543e-01  1.61365598e-01
  1.59935281e-01  1.58911079e-01  1.58420771e-01  1.58138439e-01
  1.57709703e-01  1.57343164e-01  1.57044575e-01  1.56876996e-01
  1.56994104e-01  1.57012016e-01  1.56956121e-01  1.56749114e-01
  1.56770870e-01  1.56785831e-01  1.56891674e-01  1.56810150e-01
  1.56601459e-01  1.56328991e-01  1.56230450e-01  1.56228170e-01
  1.56363577e-01  1.56369299e-01  1.56104833e-01  1.55661136e-01
  1.54832140e-01  1.54122978e-01  1.53324336e-01  1.52917311e-01
  1.52764559e-01  1.52579293e-01  1.52407125e-01  1.52173623e-01
  1.52054876e-01  1.51991338e-01  1.52303532e-01  1.52574569e-01
  1.52727902e-01  1.52341694e-01  1.51914924e-01  1.51700735e-01
  1.51805714e-01  1.51864395e-01  1.51644915e-01  1.51199043e-01
  1.50833592e-01  1.50580466e-01  1.50197297e-01  1.49330914e-01
  1.47860840e-01  1.46544859e-01  1.45541653e-01  1.45061940e-01
  1.44554332e-01  1.43791050e-01  1.42900646e-01  1.42238945e-01
  1.41985029e-01  1.41782895e-01  1.41591564e-01  1.41070843e-01
  1.40439734e-01  1.39851183e-01  1.39606699e-01  1.39575273e-01
  1.39429748e-01  1.38880998e-01  1.38197139e-01  1.37850955e-01
  1.37986049e-01  1.38106406e-01  1.37507841e-01  1.36269286e-01
  1.34317979e-01  1.32943943e-01  1.32005319e-01  1.31637469e-01
  1.31343678e-01  1.30904719e-01  1.30359337e-01  1.29707009e-01
  1.29324868e-01  1.29224837e-01  1.29457638e-01  1.29473239e-01
  1.29473493e-01  1.29027829e-01  1.28419429e-01  1.28047571e-01
  1.28133252e-01  1.28242955e-01  1.28104612e-01  1.27834886e-01
  1.27725065e-01  1.27807558e-01  1.27593979e-01  1.26838326e-01
  1.25344709e-01  1.24212272e-01  1.23405099e-01  1.23180836e-01
  1.23127304e-01  1.22805446e-01  1.22162998e-01  1.21522889e-01
  1.21336989e-01  1.21250980e-01  1.21403046e-01  1.21340789e-01
  1.21189833e-01  1.20854512e-01  1.20753564e-01  1.20831974e-01
  1.20961137e-01  1.20833956e-01  1.20705038e-01  1.20767966e-01
  1.21089697e-01  1.21563546e-01  1.21726930e-01  1.21540420e-01
  1.21064149e-01  1.20960720e-01  1.21057034e-01  1.21331416e-01
  1.21406332e-01  1.21359691e-01  1.21566482e-01  1.21808842e-01
  1.22067094e-01  1.22080676e-01  1.22316711e-01  1.22430496e-01
  1.22639827e-01  1.22497737e-01  1.22171469e-01  1.21937774e-01
  1.22091904e-01  1.22556254e-01  1.22925870e-01  1.23073317e-01
  1.23023294e-01  1.22972973e-01  1.23039328e-01  1.22940369e-01
  1.22401662e-01  1.21631555e-01  1.20889105e-01  1.20633170e-01
  1.20604239e-01  1.20799102e-01  1.20857932e-01  1.20970525e-01
  1.21259384e-01  1.21672772e-01  1.22203767e-01  1.22420207e-01
  1.22546688e-01  1.22531451e-01  1.22761257e-01  1.23153061e-01
  1.23578556e-01  1.23846330e-01  1.24102913e-01  1.24537364e-01
  1.25159606e-01  1.25748828e-01  1.25946045e-01  1.25800803e-01
  1.25346929e-01  1.25251323e-01  1.25253782e-01  1.25374556e-01
  1.25264570e-01  1.25158057e-01  1.25172377e-01  1.25609383e-01
  1.26350492e-01  1.26933441e-01  1.27789214e-01  1.28460377e-01
  1.29166320e-01  1.29452810e-01  1.29570901e-01  1.29826501e-01
  1.30316138e-01  1.30867064e-01  1.31240711e-01  1.31546214e-01
  1.32015795e-01  1.32673696e-01  1.33354470e-01  1.33837268e-01
  1.33814678e-01  1.33800656e-01  1.33786544e-01  1.34272784e-01
  1.34707376e-01  1.35344833e-01  1.35866508e-01  1.36315852e-01
  1.36966571e-01  1.37639090e-01  1.38488382e-01  1.39028788e-01
  1.39602646e-01  1.39984950e-01  1.40402094e-01  1.40715554e-01
  1.41052648e-01  1.41434550e-01  1.41870156e-01  1.42388061e-01
  1.42918006e-01  1.43393680e-01  1.43904388e-01  1.44385993e-01
  1.44642010e-01  1.44818410e-01  1.44659951e-01  1.44672483e-01
  1.44612297e-01  1.44829318e-01  1.45250529e-01  1.45617217e-01
  1.46194652e-01  1.46738663e-01  1.47583559e-01  1.48542881e-01
  1.49473086e-01  1.49708763e-01  1.49866611e-01  1.50368497e-01
  1.51225477e-01  1.52029127e-01  1.52308509e-01  1.52220845e-01
  1.52134731e-01  1.52413145e-01  1.53049603e-01  1.53568000e-01
  1.53498054e-01  1.53182477e-01  1.52870744e-01  1.52920559e-01
  1.53007194e-01  1.53270826e-01  1.53250292e-01  1.53508991e-01
  1.54040009e-01  1.54548168e-01  1.55074179e-01  1.55519426e-01
  1.56269670e-01  1.56812847e-01  1.57481015e-01  1.57945111e-01
  1.58192754e-01  1.58338174e-01  1.58712789e-01  1.59411624e-01
  1.60129473e-01  1.60537690e-01  1.60608992e-01  1.60636574e-01
  1.60616472e-01  1.60867900e-01  1.61032259e-01  1.61170915e-01
  1.61306649e-01  1.61617443e-01  1.62236288e-01  1.62711725e-01
  1.62922069e-01  1.62993565e-01  1.63444400e-01  1.64199352e-01
  1.65110990e-01  1.65465072e-01  1.65538162e-01  1.65754423e-01
  1.66312993e-01  1.66864112e-01  1.67177290e-01  1.67406216e-01
  1.67857662e-01  1.68637559e-01  1.69474542e-01  1.69787839e-01
  1.69487476e-01  1.68988898e-01  1.68679506e-01  1.68812543e-01
  1.69270515e-01  1.69869795e-01  1.70490503e-01  1.71156913e-01
  1.71838686e-01  1.72639504e-01  1.73376232e-01  1.73816770e-01
  1.74267665e-01  1.74515545e-01  1.74872622e-01  1.75231740e-01
  1.75509036e-01  1.75662339e-01  1.75805300e-01  1.76102251e-01
  1.76423341e-01  1.76755726e-01  1.77036688e-01  1.77368268e-01
  1.77390873e-01  1.77239135e-01  1.76770926e-01  1.76674321e-01
  1.76783934e-01  1.77236229e-01  1.77932411e-01  1.78601891e-01
  1.79140627e-01  1.79394722e-01  1.80009723e-01  1.80491507e-01
  1.80988207e-01  1.80890009e-01  1.80662215e-01  1.80433199e-01
  1.80227235e-01  1.80064499e-01  1.79931939e-01  1.79868594e-01
  1.79874972e-01  1.79912806e-01  1.79906398e-01  1.79841623e-01
  1.79503068e-01  1.79245204e-01  1.78983048e-01  1.78912938e-01
  1.78836167e-01  1.78842410e-01  1.78870484e-01  1.78989515e-01
  1.79002494e-01  1.78735554e-01  1.78403914e-01  1.77995086e-01
  1.77705035e-01  1.77290589e-01  1.76933378e-01  1.76369339e-01
  1.75599396e-01  1.74739733e-01  1.74079388e-01  1.73786029e-01
  1.73750356e-01  1.73758194e-01  1.73532367e-01  1.73040345e-01
  1.72309011e-01  1.71719983e-01  1.71202794e-01  1.70752451e-01
  1.70129865e-01  1.69410601e-01  1.68826327e-01  1.68417558e-01
  1.68117687e-01  1.67596683e-01  1.67104959e-01  1.66493475e-01
  1.66074157e-01  1.65573463e-01  1.65081263e-01  1.64642841e-01
  1.64242372e-01  1.63969263e-01  1.63838238e-01  1.63915291e-01
  1.63901970e-01  1.63809016e-01  1.63677290e-01  1.63562745e-01
  1.63209051e-01  1.62853375e-01  1.62228063e-01  1.61826685e-01
  1.61488220e-01  1.61153167e-01  1.60593435e-01  1.60013080e-01
  1.59320414e-01  1.58688307e-01  1.58457354e-01  1.58089876e-01
  1.57524779e-01  1.56501457e-01  1.55674696e-01  1.55184254e-01
  1.54944092e-01  1.54626340e-01  1.54222310e-01  1.53858617e-01
  1.53759331e-01  1.53857440e-01  1.53618634e-01  1.52812943e-01
  1.51428849e-01  1.50194570e-01  1.49229303e-01  1.48325250e-01
  1.47170380e-01  1.45976260e-01  1.44950420e-01  1.44200176e-01
  1.43689901e-01  1.42859325e-01  1.41991854e-01  1.40995786e-01
  1.40236959e-01  1.39503151e-01  1.38811290e-01  1.38052881e-01
  1.37298986e-01  1.36795864e-01  1.36551425e-01  1.36447504e-01
  1.36331886e-01  1.36077777e-01  1.35596380e-01  1.34742260e-01
  1.33206949e-01  1.31505042e-01  1.29870713e-01  1.28632128e-01
  1.27763212e-01  1.27176926e-01  1.26334220e-01  1.25539437e-01
  1.24915294e-01  1.24485530e-01  1.24266528e-01  1.23530746e-01
  1.22541331e-01  1.21277466e-01  1.20445453e-01  1.20077886e-01
  1.19891830e-01  1.19429827e-01  1.18677512e-01  1.18151031e-01
  1.17941998e-01  1.17891759e-01  1.17210589e-01  1.15664080e-01
  1.13463819e-01  1.11693673e-01  1.10322595e-01  1.08886115e-01
  1.06803365e-01  1.04481131e-01  1.02815695e-01  1.02009535e-01
  1.01467207e-01  1.00353569e-01  9.89061743e-02  9.73400921e-02
  9.61793959e-02  9.50764567e-02  9.41626132e-02  9.31607038e-02
  9.21867564e-02  9.14692506e-02  9.09664258e-02  9.05211046e-02
  8.98912475e-02  8.90042409e-02  8.78023282e-02  8.62566158e-02
  8.42652097e-02  8.23686123e-02  8.06389824e-02  7.94356316e-02
  7.84126595e-02  7.73089528e-02  7.59908706e-02  7.46106803e-02
  7.38664716e-02  7.35273361e-02  7.34279826e-02  7.26881027e-02
  7.14440346e-02  6.99591115e-02  6.90194964e-02  6.86340854e-02
  6.83954135e-02  6.77154437e-02  6.67452738e-02  6.61424026e-02
  6.61928281e-02  6.62956163e-02  6.55260757e-02  6.36803210e-02
  6.13174662e-02  5.95979281e-02  5.83316870e-02  5.70076071e-02
  5.52312918e-02  5.33390790e-02  5.19079566e-02  5.09945042e-02
  5.02289236e-02  4.91730049e-02  4.80238050e-02  4.70482707e-02
  4.64705937e-02  4.58284989e-02  4.51316200e-02  4.43578623e-02
  4.37148139e-02  4.32837009e-02  4.29723151e-02  4.27169800e-02
  4.24625687e-02  4.22306880e-02  4.16356884e-02  4.03261855e-02
  3.81386541e-02  3.58731039e-02  3.38146202e-02  3.24592330e-02
  3.09884232e-02  2.92326994e-02  2.70786900e-02  2.54089348e-02
  2.45893896e-02  2.43618432e-02  2.42677703e-02  2.35375240e-02
  2.28307843e-02  2.22269762e-02  2.21344400e-02  2.20564734e-02
  2.17933338e-02  2.14021057e-02  2.13216189e-02  2.16380525e-02
  2.21264120e-02  2.23835483e-02  2.17497088e-02  2.06369422e-02
  1.91373546e-02  1.79325845e-02  1.62643399e-02  1.41901840e-02
  1.22673912e-02  1.13625238e-02  1.14415679e-02  1.15445107e-02
  1.13142282e-02  1.07359374e-02  1.05243931e-02  1.06723290e-02
  1.09865759e-02  1.05897691e-02  9.68823675e-03  8.85361806e-03
  8.31550080e-03  8.17360543e-03  8.13690666e-03  7.84800481e-03
  7.73724401e-03  7.89698120e-03  7.94652943e-03  7.08470447e-03
  4.95574158e-03  2.84241815e-03  1.32125849e-03  7.90127320e-04
  2.11004517e-04 -9.47115768e-04 -2.63395836e-03 -3.64422938e-03
 -3.28379474e-03 -2.61425017e-03 -2.15191697e-03 -2.94164126e-03
 -3.98588972e-03 -4.75486461e-03 -4.63551376e-03 -4.30140318e-03
 -4.34836512e-03 -5.08744875e-03 -5.67268906e-03 -5.36214281e-03
 -4.38569207e-03 -3.49375024e-03 -3.72740324e-03 -4.89290757e-03
 -6.44535571e-03 -7.74994958e-03 -9.45674535e-03 -1.15397340e-02
 -1.35010965e-02 -1.44887837e-02 -1.45799993e-02 -1.44811524e-02
 -1.45612461e-02 -1.47210555e-02 -1.43892560e-02 -1.39979906e-02
 -1.33758187e-02 -1.36708720e-02 -1.41977333e-02 -1.47523517e-02
 -1.46273552e-02 -1.39585305e-02 -1.33898864e-02 -1.32816629e-02
 -1.33500351e-02 -1.29001252e-02 -1.28250886e-02 -1.37266619e-02
 -1.59595087e-02 -1.78599264e-02 -1.90370753e-02 -1.97195150e-02
 -2.12595817e-02 -2.30836403e-02 -2.41838116e-02 -2.39810366e-02
 -2.31428649e-02 -2.30125543e-02 -2.33324300e-02 -2.39637829e-02
 -2.38604471e-02 -2.42341161e-02 -2.41594519e-02 -2.45040897e-02
 -2.50762999e-02 -2.53402460e-02 -2.48980559e-02 -2.41105855e-02
 -2.40837615e-02 -2.41434854e-02 -2.48269103e-02 -2.48763841e-02]
