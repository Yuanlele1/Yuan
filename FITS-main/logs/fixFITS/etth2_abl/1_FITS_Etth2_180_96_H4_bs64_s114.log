Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_96_FITS_ETTh2_ftM_sl180_ll48_pl96_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8365
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=42, out_features=64, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2408448.0
params:  2752.0
Trainable parameters:  2752
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8155066967010498
Epoch: 1, Steps: 65 | Train Loss: 0.5636609 Vali Loss: 0.2611419 Test Loss: 0.3567938
Validation loss decreased (inf --> 0.261142).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9058916568756104
Epoch: 2, Steps: 65 | Train Loss: 0.4792963 Vali Loss: 0.2360058 Test Loss: 0.3219217
Validation loss decreased (0.261142 --> 0.236006).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.2425572872161865
Epoch: 3, Steps: 65 | Train Loss: 0.4508105 Vali Loss: 0.2258709 Test Loss: 0.3090899
Validation loss decreased (0.236006 --> 0.225871).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.845123529434204
Epoch: 4, Steps: 65 | Train Loss: 0.4385272 Vali Loss: 0.2201183 Test Loss: 0.3032648
Validation loss decreased (0.225871 --> 0.220118).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.341783285140991
Epoch: 5, Steps: 65 | Train Loss: 0.4317305 Vali Loss: 0.2192452 Test Loss: 0.3003724
Validation loss decreased (0.220118 --> 0.219245).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.76663875579834
Epoch: 6, Steps: 65 | Train Loss: 0.4267628 Vali Loss: 0.2178043 Test Loss: 0.2984717
Validation loss decreased (0.219245 --> 0.217804).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.162407636642456
Epoch: 7, Steps: 65 | Train Loss: 0.4244371 Vali Loss: 0.2160615 Test Loss: 0.2969945
Validation loss decreased (0.217804 --> 0.216062).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.0799639225006104
Epoch: 8, Steps: 65 | Train Loss: 0.4228730 Vali Loss: 0.2149498 Test Loss: 0.2958948
Validation loss decreased (0.216062 --> 0.214950).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.7029972076416016
Epoch: 9, Steps: 65 | Train Loss: 0.4202380 Vali Loss: 0.2130222 Test Loss: 0.2950946
Validation loss decreased (0.214950 --> 0.213022).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.2092816829681396
Epoch: 10, Steps: 65 | Train Loss: 0.4198344 Vali Loss: 0.2129442 Test Loss: 0.2944181
Validation loss decreased (0.213022 --> 0.212944).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.8872182369232178
Epoch: 11, Steps: 65 | Train Loss: 0.4185790 Vali Loss: 0.2129903 Test Loss: 0.2937430
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.433130979537964
Epoch: 12, Steps: 65 | Train Loss: 0.4157191 Vali Loss: 0.2115568 Test Loss: 0.2932806
Validation loss decreased (0.212944 --> 0.211557).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.14469051361084
Epoch: 13, Steps: 65 | Train Loss: 0.4162710 Vali Loss: 0.2119462 Test Loss: 0.2928652
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.8190035820007324
Epoch: 14, Steps: 65 | Train Loss: 0.4156540 Vali Loss: 0.2108877 Test Loss: 0.2924990
Validation loss decreased (0.211557 --> 0.210888).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.983372688293457
Epoch: 15, Steps: 65 | Train Loss: 0.4134453 Vali Loss: 0.2118728 Test Loss: 0.2921105
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5359647274017334
Epoch: 16, Steps: 65 | Train Loss: 0.4129348 Vali Loss: 0.2106033 Test Loss: 0.2917728
Validation loss decreased (0.210888 --> 0.210603).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.1740784645080566
Epoch: 17, Steps: 65 | Train Loss: 0.4130792 Vali Loss: 0.2109952 Test Loss: 0.2915065
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.0644400119781494
Epoch: 18, Steps: 65 | Train Loss: 0.4117115 Vali Loss: 0.2111099 Test Loss: 0.2912637
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.7309861183166504
Epoch: 19, Steps: 65 | Train Loss: 0.4120297 Vali Loss: 0.2105416 Test Loss: 0.2910579
Validation loss decreased (0.210603 --> 0.210542).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.131330728530884
Epoch: 20, Steps: 65 | Train Loss: 0.4126322 Vali Loss: 0.2103452 Test Loss: 0.2908036
Validation loss decreased (0.210542 --> 0.210345).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.513871908187866
Epoch: 21, Steps: 65 | Train Loss: 0.4126953 Vali Loss: 0.2099534 Test Loss: 0.2906618
Validation loss decreased (0.210345 --> 0.209953).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.159503221511841
Epoch: 22, Steps: 65 | Train Loss: 0.4116258 Vali Loss: 0.2088184 Test Loss: 0.2904339
Validation loss decreased (0.209953 --> 0.208818).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.826831579208374
Epoch: 23, Steps: 65 | Train Loss: 0.4104971 Vali Loss: 0.2097667 Test Loss: 0.2903170
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.911876916885376
Epoch: 24, Steps: 65 | Train Loss: 0.4118369 Vali Loss: 0.2098849 Test Loss: 0.2902274
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.8323640823364258
Epoch: 25, Steps: 65 | Train Loss: 0.4108336 Vali Loss: 0.2093290 Test Loss: 0.2900459
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8453032970428467
Epoch: 26, Steps: 65 | Train Loss: 0.4109510 Vali Loss: 0.2101794 Test Loss: 0.2899528
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.965404748916626
Epoch: 27, Steps: 65 | Train Loss: 0.4108805 Vali Loss: 0.2102488 Test Loss: 0.2898788
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.271782875061035
Epoch: 28, Steps: 65 | Train Loss: 0.4093365 Vali Loss: 0.2095594 Test Loss: 0.2897644
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.9967870712280273
Epoch: 29, Steps: 65 | Train Loss: 0.4084642 Vali Loss: 0.2093775 Test Loss: 0.2896962
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.891707420349121
Epoch: 30, Steps: 65 | Train Loss: 0.4097240 Vali Loss: 0.2093782 Test Loss: 0.2895956
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.2444822788238525
Epoch: 31, Steps: 65 | Train Loss: 0.4085296 Vali Loss: 0.2090985 Test Loss: 0.2894798
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.2658748626708984
Epoch: 32, Steps: 65 | Train Loss: 0.4086928 Vali Loss: 0.2092117 Test Loss: 0.2894229
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.202709913253784
Epoch: 33, Steps: 65 | Train Loss: 0.4085747 Vali Loss: 0.2096457 Test Loss: 0.2893601
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6281287670135498
Epoch: 34, Steps: 65 | Train Loss: 0.4078734 Vali Loss: 0.2083854 Test Loss: 0.2892902
Validation loss decreased (0.208818 --> 0.208385).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.0915868282318115
Epoch: 35, Steps: 65 | Train Loss: 0.4093243 Vali Loss: 0.2080027 Test Loss: 0.2892648
Validation loss decreased (0.208385 --> 0.208003).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.1045517921447754
Epoch: 36, Steps: 65 | Train Loss: 0.4085512 Vali Loss: 0.2083678 Test Loss: 0.2891799
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.3850300312042236
Epoch: 37, Steps: 65 | Train Loss: 0.4084496 Vali Loss: 0.2083137 Test Loss: 0.2891064
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6539766788482666
Epoch: 38, Steps: 65 | Train Loss: 0.4080275 Vali Loss: 0.2086540 Test Loss: 0.2891116
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.8932311534881592
Epoch: 39, Steps: 65 | Train Loss: 0.4077300 Vali Loss: 0.2086844 Test Loss: 0.2890564
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.083141326904297
Epoch: 40, Steps: 65 | Train Loss: 0.4089587 Vali Loss: 0.2088104 Test Loss: 0.2889984
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9144797325134277
Epoch: 41, Steps: 65 | Train Loss: 0.4088040 Vali Loss: 0.2081066 Test Loss: 0.2889662
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.370060682296753
Epoch: 42, Steps: 65 | Train Loss: 0.4087688 Vali Loss: 0.2075938 Test Loss: 0.2889346
Validation loss decreased (0.208003 --> 0.207594).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.1800384521484375
Epoch: 43, Steps: 65 | Train Loss: 0.4073434 Vali Loss: 0.2091095 Test Loss: 0.2888855
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.6715095043182373
Epoch: 44, Steps: 65 | Train Loss: 0.4082224 Vali Loss: 0.2072173 Test Loss: 0.2888551
Validation loss decreased (0.207594 --> 0.207217).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.174248695373535
Epoch: 45, Steps: 65 | Train Loss: 0.4068386 Vali Loss: 0.2072809 Test Loss: 0.2888241
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.0152747631073
Epoch: 46, Steps: 65 | Train Loss: 0.4085728 Vali Loss: 0.2091261 Test Loss: 0.2887812
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.748905897140503
Epoch: 47, Steps: 65 | Train Loss: 0.4083451 Vali Loss: 0.2081313 Test Loss: 0.2887681
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.752288818359375
Epoch: 48, Steps: 65 | Train Loss: 0.4082767 Vali Loss: 0.2089465 Test Loss: 0.2887318
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.9031991958618164
Epoch: 49, Steps: 65 | Train Loss: 0.4090564 Vali Loss: 0.2095337 Test Loss: 0.2887256
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.724752426147461
Epoch: 50, Steps: 65 | Train Loss: 0.4085774 Vali Loss: 0.2074969 Test Loss: 0.2887079
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.6770975589752197
Epoch: 51, Steps: 65 | Train Loss: 0.4087277 Vali Loss: 0.2085755 Test Loss: 0.2886814
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.7113080024719238
Epoch: 52, Steps: 65 | Train Loss: 0.4062894 Vali Loss: 0.2080899 Test Loss: 0.2886534
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.9390172958374023
Epoch: 53, Steps: 65 | Train Loss: 0.4073567 Vali Loss: 0.2081777 Test Loss: 0.2886254
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5341885089874268
Epoch: 54, Steps: 65 | Train Loss: 0.4081997 Vali Loss: 0.2086287 Test Loss: 0.2886064
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.558255910873413
Epoch: 55, Steps: 65 | Train Loss: 0.4071884 Vali Loss: 0.2086855 Test Loss: 0.2886055
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.021064281463623
Epoch: 56, Steps: 65 | Train Loss: 0.4076066 Vali Loss: 0.2090950 Test Loss: 0.2885731
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.280320167541504
Epoch: 57, Steps: 65 | Train Loss: 0.4084306 Vali Loss: 0.2081076 Test Loss: 0.2885575
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.2563207149505615
Epoch: 58, Steps: 65 | Train Loss: 0.4078742 Vali Loss: 0.2084585 Test Loss: 0.2885470
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.131331205368042
Epoch: 59, Steps: 65 | Train Loss: 0.4069115 Vali Loss: 0.2087269 Test Loss: 0.2885431
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.7540044784545898
Epoch: 60, Steps: 65 | Train Loss: 0.4064853 Vali Loss: 0.2090497 Test Loss: 0.2885138
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.476308822631836
Epoch: 61, Steps: 65 | Train Loss: 0.4069248 Vali Loss: 0.2079101 Test Loss: 0.2885120
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.929515838623047
Epoch: 62, Steps: 65 | Train Loss: 0.4078415 Vali Loss: 0.2085265 Test Loss: 0.2885008
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.9878833293914795
Epoch: 63, Steps: 65 | Train Loss: 0.4062783 Vali Loss: 0.2083837 Test Loss: 0.2884921
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.3201539516448975
Epoch: 64, Steps: 65 | Train Loss: 0.4073946 Vali Loss: 0.2086033 Test Loss: 0.2884808
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_96_FITS_ETTh2_ftM_sl180_ll48_pl96_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.2885409891605377, mae:0.34109461307525635, rse:0.4328974783420563, corr:[0.27321035 0.2781891  0.2760095  0.27462962 0.27464157 0.27374598
 0.27201435 0.27080625 0.27005994 0.26889756 0.26744875 0.2658901
 0.26443258 0.26312098 0.26230907 0.26185486 0.2613189  0.26045996
 0.25947052 0.258415   0.25719547 0.25589493 0.25450704 0.2525186
 0.24968202 0.2469761  0.24514525 0.24398316 0.24239126 0.24032229
 0.23844253 0.23692425 0.23516513 0.23311953 0.23149689 0.23046665
 0.22931474 0.22744417 0.22597753 0.22546718 0.22542262 0.22461237
 0.22317566 0.22215089 0.22179046 0.22077595 0.21867177 0.21637803
 0.21417093 0.21200351 0.20937459 0.20732962 0.20621249 0.20506908
 0.20262632 0.19992526 0.1986678  0.19796321 0.1967143  0.19495852
 0.19435121 0.1942933  0.19429846 0.19352739 0.19310626 0.1932173
 0.19271536 0.19121382 0.19040315 0.19068252 0.1901141  0.18808931
 0.18537252 0.18426736 0.18374364 0.18220915 0.18013355 0.17967725
 0.17995304 0.17914362 0.17801568 0.17785503 0.17899373 0.17954381
 0.17905262 0.17854093 0.17989185 0.18093376 0.18015148 0.17869356
 0.17927198 0.18004902 0.1791136  0.17814656 0.1808758  0.18362932]
