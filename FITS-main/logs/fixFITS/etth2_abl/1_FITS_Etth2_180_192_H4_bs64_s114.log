Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_192_FITS_ETTh2_ftM_sl180_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8269
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=42, out_features=86, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3236352.0
params:  3698.0
Trainable parameters:  3698
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8869891166687012
Epoch: 1, Steps: 64 | Train Loss: 0.6922872 Vali Loss: 0.3379784 Test Loss: 0.4663411
Validation loss decreased (inf --> 0.337978).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.1017019748687744
Epoch: 2, Steps: 64 | Train Loss: 0.5989719 Vali Loss: 0.3096774 Test Loss: 0.4294689
Validation loss decreased (0.337978 --> 0.309677).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.230645179748535
Epoch: 3, Steps: 64 | Train Loss: 0.5649594 Vali Loss: 0.2975902 Test Loss: 0.4144912
Validation loss decreased (0.309677 --> 0.297590).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6379294395446777
Epoch: 4, Steps: 64 | Train Loss: 0.5486836 Vali Loss: 0.2912431 Test Loss: 0.4068978
Validation loss decreased (0.297590 --> 0.291243).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.1933906078338623
Epoch: 5, Steps: 64 | Train Loss: 0.5426025 Vali Loss: 0.2873679 Test Loss: 0.4025118
Validation loss decreased (0.291243 --> 0.287368).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.190654993057251
Epoch: 6, Steps: 64 | Train Loss: 0.5385228 Vali Loss: 0.2848993 Test Loss: 0.3995340
Validation loss decreased (0.287368 --> 0.284899).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.7288694381713867
Epoch: 7, Steps: 64 | Train Loss: 0.5343054 Vali Loss: 0.2830024 Test Loss: 0.3972366
Validation loss decreased (0.284899 --> 0.283002).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.1751015186309814
Epoch: 8, Steps: 64 | Train Loss: 0.5345526 Vali Loss: 0.2815367 Test Loss: 0.3955345
Validation loss decreased (0.283002 --> 0.281537).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.558217763900757
Epoch: 9, Steps: 64 | Train Loss: 0.5304877 Vali Loss: 0.2803655 Test Loss: 0.3941162
Validation loss decreased (0.281537 --> 0.280365).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.0890824794769287
Epoch: 10, Steps: 64 | Train Loss: 0.5295728 Vali Loss: 0.2796029 Test Loss: 0.3928975
Validation loss decreased (0.280365 --> 0.279603).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.871434211730957
Epoch: 11, Steps: 64 | Train Loss: 0.5290186 Vali Loss: 0.2788640 Test Loss: 0.3919192
Validation loss decreased (0.279603 --> 0.278864).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.8893606662750244
Epoch: 12, Steps: 64 | Train Loss: 0.5252311 Vali Loss: 0.2778320 Test Loss: 0.3909886
Validation loss decreased (0.278864 --> 0.277832).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.8033428192138672
Epoch: 13, Steps: 64 | Train Loss: 0.5250874 Vali Loss: 0.2778437 Test Loss: 0.3902396
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.8639500141143799
Epoch: 14, Steps: 64 | Train Loss: 0.5261908 Vali Loss: 0.2772520 Test Loss: 0.3895863
Validation loss decreased (0.277832 --> 0.277252).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.4538156986236572
Epoch: 15, Steps: 64 | Train Loss: 0.5250288 Vali Loss: 0.2768989 Test Loss: 0.3889731
Validation loss decreased (0.277252 --> 0.276899).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.822105884552002
Epoch: 16, Steps: 64 | Train Loss: 0.5223446 Vali Loss: 0.2765918 Test Loss: 0.3884278
Validation loss decreased (0.276899 --> 0.276592).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.278282642364502
Epoch: 17, Steps: 64 | Train Loss: 0.5221861 Vali Loss: 0.2762441 Test Loss: 0.3879534
Validation loss decreased (0.276592 --> 0.276244).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.2416534423828125
Epoch: 18, Steps: 64 | Train Loss: 0.5227918 Vali Loss: 0.2756492 Test Loss: 0.3874980
Validation loss decreased (0.276244 --> 0.275649).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.5952324867248535
Epoch: 19, Steps: 64 | Train Loss: 0.5219898 Vali Loss: 0.2756785 Test Loss: 0.3870749
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.110267162322998
Epoch: 20, Steps: 64 | Train Loss: 0.5213973 Vali Loss: 0.2756318 Test Loss: 0.3867353
Validation loss decreased (0.275649 --> 0.275632).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.4790782928466797
Epoch: 21, Steps: 64 | Train Loss: 0.5190227 Vali Loss: 0.2753096 Test Loss: 0.3863831
Validation loss decreased (0.275632 --> 0.275310).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.173571825027466
Epoch: 22, Steps: 64 | Train Loss: 0.5208116 Vali Loss: 0.2752113 Test Loss: 0.3860648
Validation loss decreased (0.275310 --> 0.275211).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.3078389167785645
Epoch: 23, Steps: 64 | Train Loss: 0.5222010 Vali Loss: 0.2750884 Test Loss: 0.3858092
Validation loss decreased (0.275211 --> 0.275088).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.013524055480957
Epoch: 24, Steps: 64 | Train Loss: 0.5191129 Vali Loss: 0.2748896 Test Loss: 0.3855908
Validation loss decreased (0.275088 --> 0.274890).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.0198113918304443
Epoch: 25, Steps: 64 | Train Loss: 0.5208813 Vali Loss: 0.2747630 Test Loss: 0.3853712
Validation loss decreased (0.274890 --> 0.274763).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.806941032409668
Epoch: 26, Steps: 64 | Train Loss: 0.5183953 Vali Loss: 0.2746260 Test Loss: 0.3851811
Validation loss decreased (0.274763 --> 0.274626).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1852550506591797
Epoch: 27, Steps: 64 | Train Loss: 0.5200155 Vali Loss: 0.2745680 Test Loss: 0.3849698
Validation loss decreased (0.274626 --> 0.274568).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.2474613189697266
Epoch: 28, Steps: 64 | Train Loss: 0.5196614 Vali Loss: 0.2744536 Test Loss: 0.3848106
Validation loss decreased (0.274568 --> 0.274454).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.103482484817505
Epoch: 29, Steps: 64 | Train Loss: 0.5198198 Vali Loss: 0.2743462 Test Loss: 0.3845807
Validation loss decreased (0.274454 --> 0.274346).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.333746910095215
Epoch: 30, Steps: 64 | Train Loss: 0.5189546 Vali Loss: 0.2741719 Test Loss: 0.3844613
Validation loss decreased (0.274346 --> 0.274172).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.927267551422119
Epoch: 31, Steps: 64 | Train Loss: 0.5192173 Vali Loss: 0.2739613 Test Loss: 0.3843180
Validation loss decreased (0.274172 --> 0.273961).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.649707794189453
Epoch: 32, Steps: 64 | Train Loss: 0.5194136 Vali Loss: 0.2739958 Test Loss: 0.3842013
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.1038684844970703
Epoch: 33, Steps: 64 | Train Loss: 0.5186298 Vali Loss: 0.2739321 Test Loss: 0.3840767
Validation loss decreased (0.273961 --> 0.273932).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.3273251056671143
Epoch: 34, Steps: 64 | Train Loss: 0.5194594 Vali Loss: 0.2739959 Test Loss: 0.3839533
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.3719403743743896
Epoch: 35, Steps: 64 | Train Loss: 0.5168309 Vali Loss: 0.2739708 Test Loss: 0.3838438
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.868037462234497
Epoch: 36, Steps: 64 | Train Loss: 0.5185620 Vali Loss: 0.2738678 Test Loss: 0.3837596
Validation loss decreased (0.273932 --> 0.273868).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.0758628845214844
Epoch: 37, Steps: 64 | Train Loss: 0.5175928 Vali Loss: 0.2737559 Test Loss: 0.3836541
Validation loss decreased (0.273868 --> 0.273756).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.4003520011901855
Epoch: 38, Steps: 64 | Train Loss: 0.5161747 Vali Loss: 0.2737091 Test Loss: 0.3836061
Validation loss decreased (0.273756 --> 0.273709).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.9577815532684326
Epoch: 39, Steps: 64 | Train Loss: 0.5155233 Vali Loss: 0.2737147 Test Loss: 0.3834965
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.015956401824951
Epoch: 40, Steps: 64 | Train Loss: 0.5175816 Vali Loss: 0.2736909 Test Loss: 0.3834223
Validation loss decreased (0.273709 --> 0.273691).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.952043056488037
Epoch: 41, Steps: 64 | Train Loss: 0.5174567 Vali Loss: 0.2736720 Test Loss: 0.3833463
Validation loss decreased (0.273691 --> 0.273672).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.5455150604248047
Epoch: 42, Steps: 64 | Train Loss: 0.5177790 Vali Loss: 0.2736428 Test Loss: 0.3832845
Validation loss decreased (0.273672 --> 0.273643).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.8638358116149902
Epoch: 43, Steps: 64 | Train Loss: 0.5161203 Vali Loss: 0.2736006 Test Loss: 0.3832318
Validation loss decreased (0.273643 --> 0.273601).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.9691104888916016
Epoch: 44, Steps: 64 | Train Loss: 0.5177841 Vali Loss: 0.2735718 Test Loss: 0.3831729
Validation loss decreased (0.273601 --> 0.273572).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9667069911956787
Epoch: 45, Steps: 64 | Train Loss: 0.5184310 Vali Loss: 0.2735216 Test Loss: 0.3831097
Validation loss decreased (0.273572 --> 0.273522).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.993703842163086
Epoch: 46, Steps: 64 | Train Loss: 0.5178966 Vali Loss: 0.2734632 Test Loss: 0.3830691
Validation loss decreased (0.273522 --> 0.273463).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.9622516632080078
Epoch: 47, Steps: 64 | Train Loss: 0.5180259 Vali Loss: 0.2731620 Test Loss: 0.3830127
Validation loss decreased (0.273463 --> 0.273162).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.3765883445739746
Epoch: 48, Steps: 64 | Train Loss: 0.5190090 Vali Loss: 0.2734597 Test Loss: 0.3829582
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.124751567840576
Epoch: 49, Steps: 64 | Train Loss: 0.5185125 Vali Loss: 0.2734389 Test Loss: 0.3829029
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.8728227615356445
Epoch: 50, Steps: 64 | Train Loss: 0.5175873 Vali Loss: 0.2734089 Test Loss: 0.3828649
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.8535475730895996
Epoch: 51, Steps: 64 | Train Loss: 0.5157295 Vali Loss: 0.2733633 Test Loss: 0.3828203
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.7036638259887695
Epoch: 52, Steps: 64 | Train Loss: 0.5178468 Vali Loss: 0.2733738 Test Loss: 0.3827807
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.098782777786255
Epoch: 53, Steps: 64 | Train Loss: 0.5178044 Vali Loss: 0.2732929 Test Loss: 0.3827451
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.140124797821045
Epoch: 54, Steps: 64 | Train Loss: 0.5149922 Vali Loss: 0.2733308 Test Loss: 0.3827168
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.1525356769561768
Epoch: 55, Steps: 64 | Train Loss: 0.5170026 Vali Loss: 0.2733282 Test Loss: 0.3826753
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9842207431793213
Epoch: 56, Steps: 64 | Train Loss: 0.5173647 Vali Loss: 0.2733036 Test Loss: 0.3826537
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.6353213787078857
Epoch: 57, Steps: 64 | Train Loss: 0.5167270 Vali Loss: 0.2732576 Test Loss: 0.3826212
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.29557728767395
Epoch: 58, Steps: 64 | Train Loss: 0.5183476 Vali Loss: 0.2732522 Test Loss: 0.3826030
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.2123560905456543
Epoch: 59, Steps: 64 | Train Loss: 0.5187022 Vali Loss: 0.2732313 Test Loss: 0.3825726
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.172779083251953
Epoch: 60, Steps: 64 | Train Loss: 0.5172704 Vali Loss: 0.2732129 Test Loss: 0.3825361
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.3383748531341553
Epoch: 61, Steps: 64 | Train Loss: 0.5164697 Vali Loss: 0.2732303 Test Loss: 0.3825233
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.7061755657196045
Epoch: 62, Steps: 64 | Train Loss: 0.5181749 Vali Loss: 0.2731890 Test Loss: 0.3824943
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.0573058128356934
Epoch: 63, Steps: 64 | Train Loss: 0.5168376 Vali Loss: 0.2730988 Test Loss: 0.3824753
Validation loss decreased (0.273162 --> 0.273099).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.3080637454986572
Epoch: 64, Steps: 64 | Train Loss: 0.5184201 Vali Loss: 0.2732162 Test Loss: 0.3824575
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.0776262283325195
Epoch: 65, Steps: 64 | Train Loss: 0.5162900 Vali Loss: 0.2731972 Test Loss: 0.3824416
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.05026912689209
Epoch: 66, Steps: 64 | Train Loss: 0.5171737 Vali Loss: 0.2731822 Test Loss: 0.3824215
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.9641976356506348
Epoch: 67, Steps: 64 | Train Loss: 0.5173199 Vali Loss: 0.2731770 Test Loss: 0.3824118
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.056913137435913
Epoch: 68, Steps: 64 | Train Loss: 0.5169325 Vali Loss: 0.2731452 Test Loss: 0.3823919
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.93888258934021
Epoch: 69, Steps: 64 | Train Loss: 0.5160385 Vali Loss: 0.2731119 Test Loss: 0.3823793
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.980764150619507
Epoch: 70, Steps: 64 | Train Loss: 0.5179408 Vali Loss: 0.2731640 Test Loss: 0.3823611
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.1434950828552246
Epoch: 71, Steps: 64 | Train Loss: 0.5168542 Vali Loss: 0.2731366 Test Loss: 0.3823502
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.9564533233642578
Epoch: 72, Steps: 64 | Train Loss: 0.5154961 Vali Loss: 0.2731210 Test Loss: 0.3823356
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.6427314281463623
Epoch: 73, Steps: 64 | Train Loss: 0.5149816 Vali Loss: 0.2731393 Test Loss: 0.3823227
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.241560697555542
Epoch: 74, Steps: 64 | Train Loss: 0.5156878 Vali Loss: 0.2731178 Test Loss: 0.3823122
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.1073484420776367
Epoch: 75, Steps: 64 | Train Loss: 0.5178056 Vali Loss: 0.2727076 Test Loss: 0.3822952
Validation loss decreased (0.273099 --> 0.272708).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.0781023502349854
Epoch: 76, Steps: 64 | Train Loss: 0.5155193 Vali Loss: 0.2730823 Test Loss: 0.3822950
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.570545196533203
Epoch: 77, Steps: 64 | Train Loss: 0.5164438 Vali Loss: 0.2729780 Test Loss: 0.3822840
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.449793577194214
Epoch: 78, Steps: 64 | Train Loss: 0.5159386 Vali Loss: 0.2730554 Test Loss: 0.3822712
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.0426762104034424
Epoch: 79, Steps: 64 | Train Loss: 0.5171950 Vali Loss: 0.2730885 Test Loss: 0.3822648
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 3.0684001445770264
Epoch: 80, Steps: 64 | Train Loss: 0.5157889 Vali Loss: 0.2730987 Test Loss: 0.3822568
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.7517740726470947
Epoch: 81, Steps: 64 | Train Loss: 0.5174873 Vali Loss: 0.2730828 Test Loss: 0.3822480
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.4410054683685303
Epoch: 82, Steps: 64 | Train Loss: 0.5168901 Vali Loss: 0.2730753 Test Loss: 0.3822413
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.8509774208068848
Epoch: 83, Steps: 64 | Train Loss: 0.5165023 Vali Loss: 0.2730892 Test Loss: 0.3822345
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.9322474002838135
Epoch: 84, Steps: 64 | Train Loss: 0.5178208 Vali Loss: 0.2729640 Test Loss: 0.3822314
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.201120615005493
Epoch: 85, Steps: 64 | Train Loss: 0.5162280 Vali Loss: 0.2730414 Test Loss: 0.3822233
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.387432098388672
Epoch: 86, Steps: 64 | Train Loss: 0.5153707 Vali Loss: 0.2729796 Test Loss: 0.3822162
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.14243221282959
Epoch: 87, Steps: 64 | Train Loss: 0.5152054 Vali Loss: 0.2729063 Test Loss: 0.3822144
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.179643154144287
Epoch: 88, Steps: 64 | Train Loss: 0.5173236 Vali Loss: 0.2730488 Test Loss: 0.3822077
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.222730875015259
Epoch: 89, Steps: 64 | Train Loss: 0.5163392 Vali Loss: 0.2730394 Test Loss: 0.3822027
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.3861546516418457
Epoch: 90, Steps: 64 | Train Loss: 0.5165465 Vali Loss: 0.2730555 Test Loss: 0.3821962
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.2538557052612305
Epoch: 91, Steps: 64 | Train Loss: 0.5154782 Vali Loss: 0.2730122 Test Loss: 0.3821917
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.454267740249634
Epoch: 92, Steps: 64 | Train Loss: 0.5177751 Vali Loss: 0.2730457 Test Loss: 0.3821883
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.7933728694915771
Epoch: 93, Steps: 64 | Train Loss: 0.5152032 Vali Loss: 0.2729961 Test Loss: 0.3821833
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.9904212951660156
Epoch: 94, Steps: 64 | Train Loss: 0.5180325 Vali Loss: 0.2730358 Test Loss: 0.3821798
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.7686071395874023
Epoch: 95, Steps: 64 | Train Loss: 0.5163331 Vali Loss: 0.2730379 Test Loss: 0.3821760
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_192_FITS_ETTh2_ftM_sl180_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.36052390933036804, mae:0.3876441419124603, rse:0.4815128743648529, corr:[0.2647935  0.2703525  0.26788253 0.26697534 0.26737803 0.26640362
 0.2646401  0.26380607 0.26339266 0.262194   0.2606045  0.25916559
 0.25795496 0.2567539  0.255806   0.2552216  0.25467274 0.2538003
 0.2526125  0.2513477  0.25005797 0.24880014 0.24744113 0.2454025
 0.24266246 0.24008174 0.23824613 0.23682466 0.23495202 0.23279491
 0.23100598 0.22943601 0.22772594 0.22567937 0.22407871 0.22290005
 0.22154361 0.21971695 0.21831869 0.21769725 0.21734841 0.21631654
 0.21480115 0.21363376 0.21295023 0.21174853 0.20973141 0.20747042
 0.20512223 0.2028013  0.2002622  0.1982161  0.19669832 0.19502857
 0.1925053  0.19006392 0.1888943  0.18809417 0.18692638 0.185334
 0.18460621 0.18422061 0.18385555 0.18296552 0.182382   0.18222906
 0.18173154 0.18060382 0.17980686 0.17958178 0.17885992 0.17728856
 0.17507507 0.1736359  0.17260799 0.17127177 0.16976796 0.16903767
 0.1687104  0.16800787 0.16731726 0.16688511 0.16706388 0.16709255
 0.16667835 0.16601686 0.16601282 0.16622373 0.16589467 0.16491821
 0.16427095 0.16419016 0.16432622 0.16391687 0.16309099 0.16246755
 0.16141993 0.16000652 0.15829839 0.15727884 0.15662119 0.15577865
 0.15480742 0.15419589 0.15436941 0.15442693 0.15421182 0.15385458
 0.15396877 0.1538456  0.15332864 0.15263955 0.15241377 0.15241215
 0.15214829 0.15134785 0.1506817  0.15039897 0.1496471  0.14790191
 0.1455679  0.14384633 0.14270073 0.14163214 0.14008181 0.13860814
 0.13770215 0.13691719 0.13603902 0.13530318 0.13502595 0.13461876
 0.13410904 0.13338646 0.13324791 0.133388   0.13324414 0.13244927
 0.13176726 0.13170847 0.13190268 0.13162501 0.13040982 0.12889323
 0.12677509 0.12498281 0.12297826 0.12147134 0.12048364 0.119775
 0.1191729  0.11846521 0.11827446 0.11834596 0.11872007 0.11883887
 0.11947113 0.11969955 0.11980538 0.11997676 0.12039468 0.12040906
 0.11988443 0.11939728 0.11937091 0.11980459 0.11969277 0.11839513
 0.11591864 0.11457506 0.11369276 0.11307037 0.11203759 0.11057878
 0.10949364 0.10878173 0.10869627 0.10845912 0.10843931 0.10811274
 0.10836738 0.1086212  0.10880465 0.10854772 0.10810067 0.10745631
 0.10719426 0.10613082 0.10485231 0.10472223 0.10561534 0.10427085]
