Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=58, out_features=290, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  15070720.0
params:  17110.0
Trainable parameters:  17110
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9274590015411377
Epoch: 1, Steps: 60 | Train Loss: 1.1708852 Vali Loss: 0.7708993 Test Loss: 0.5542312
Validation loss decreased (inf --> 0.770899).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9912102222442627
Epoch: 2, Steps: 60 | Train Loss: 0.9751175 Vali Loss: 0.7077053 Test Loss: 0.4808074
Validation loss decreased (0.770899 --> 0.707705).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8770885467529297
Epoch: 3, Steps: 60 | Train Loss: 0.8958987 Vali Loss: 0.6718819 Test Loss: 0.4468039
Validation loss decreased (0.707705 --> 0.671882).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.7218811511993408
Epoch: 4, Steps: 60 | Train Loss: 0.8607764 Vali Loss: 0.6571028 Test Loss: 0.4301531
Validation loss decreased (0.671882 --> 0.657103).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7314002513885498
Epoch: 5, Steps: 60 | Train Loss: 0.8442219 Vali Loss: 0.6505159 Test Loss: 0.4215237
Validation loss decreased (0.657103 --> 0.650516).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.5434348583221436
Epoch: 6, Steps: 60 | Train Loss: 0.8325771 Vali Loss: 0.6433984 Test Loss: 0.4166117
Validation loss decreased (0.650516 --> 0.643398).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.2351574897766113
Epoch: 7, Steps: 60 | Train Loss: 0.8298370 Vali Loss: 0.6429211 Test Loss: 0.4137387
Validation loss decreased (0.643398 --> 0.642921).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.1123764514923096
Epoch: 8, Steps: 60 | Train Loss: 0.8259003 Vali Loss: 0.6400903 Test Loss: 0.4119349
Validation loss decreased (0.642921 --> 0.640090).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.05258846282959
Epoch: 9, Steps: 60 | Train Loss: 0.8216787 Vali Loss: 0.6360180 Test Loss: 0.4106749
Validation loss decreased (0.640090 --> 0.636018).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.036165952682495
Epoch: 10, Steps: 60 | Train Loss: 0.8218155 Vali Loss: 0.6361016 Test Loss: 0.4098403
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.1741089820861816
Epoch: 11, Steps: 60 | Train Loss: 0.8205980 Vali Loss: 0.6360523 Test Loss: 0.4091898
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9695889949798584
Epoch: 12, Steps: 60 | Train Loss: 0.8187390 Vali Loss: 0.6316342 Test Loss: 0.4086548
Validation loss decreased (0.636018 --> 0.631634).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.1862924098968506
Epoch: 13, Steps: 60 | Train Loss: 0.8182552 Vali Loss: 0.6318755 Test Loss: 0.4082530
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.9816806316375732
Epoch: 14, Steps: 60 | Train Loss: 0.8177969 Vali Loss: 0.6353381 Test Loss: 0.4079233
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.9557793140411377
Epoch: 15, Steps: 60 | Train Loss: 0.8161473 Vali Loss: 0.6287912 Test Loss: 0.4076442
Validation loss decreased (0.631634 --> 0.628791).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.9634220600128174
Epoch: 16, Steps: 60 | Train Loss: 0.8173663 Vali Loss: 0.6311231 Test Loss: 0.4074238
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.950723648071289
Epoch: 17, Steps: 60 | Train Loss: 0.8178670 Vali Loss: 0.6337098 Test Loss: 0.4072132
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.319887638092041
Epoch: 18, Steps: 60 | Train Loss: 0.8169572 Vali Loss: 0.6347926 Test Loss: 0.4070246
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0550074577331543
Epoch: 19, Steps: 60 | Train Loss: 0.8164200 Vali Loss: 0.6325008 Test Loss: 0.4068537
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8869335651397705
Epoch: 20, Steps: 60 | Train Loss: 0.8169987 Vali Loss: 0.6315262 Test Loss: 0.4066981
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.914538860321045
Epoch: 21, Steps: 60 | Train Loss: 0.8151876 Vali Loss: 0.6294296 Test Loss: 0.4066375
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.504328489303589
Epoch: 22, Steps: 60 | Train Loss: 0.8142951 Vali Loss: 0.6303294 Test Loss: 0.4064921
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.3243002891540527
Epoch: 23, Steps: 60 | Train Loss: 0.8155911 Vali Loss: 0.6301176 Test Loss: 0.4063861
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.2137532234191895
Epoch: 24, Steps: 60 | Train Loss: 0.8134153 Vali Loss: 0.6280015 Test Loss: 0.4063132
Validation loss decreased (0.628791 --> 0.628001).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.2926223278045654
Epoch: 25, Steps: 60 | Train Loss: 0.8123651 Vali Loss: 0.6309786 Test Loss: 0.4062340
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8274877071380615
Epoch: 26, Steps: 60 | Train Loss: 0.8131247 Vali Loss: 0.6286285 Test Loss: 0.4061801
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.02546763420105
Epoch: 27, Steps: 60 | Train Loss: 0.8135186 Vali Loss: 0.6280559 Test Loss: 0.4061183
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.144803285598755
Epoch: 28, Steps: 60 | Train Loss: 0.8130953 Vali Loss: 0.6316955 Test Loss: 0.4060290
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7808897495269775
Epoch: 29, Steps: 60 | Train Loss: 0.8143557 Vali Loss: 0.6285919 Test Loss: 0.4059802
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.6616010665893555
Epoch: 30, Steps: 60 | Train Loss: 0.8108818 Vali Loss: 0.6314608 Test Loss: 0.4059497
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.341646671295166
Epoch: 31, Steps: 60 | Train Loss: 0.8131779 Vali Loss: 0.6286246 Test Loss: 0.4058880
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.916430950164795
Epoch: 32, Steps: 60 | Train Loss: 0.8124563 Vali Loss: 0.6309260 Test Loss: 0.4058591
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.3867111206054688
Epoch: 33, Steps: 60 | Train Loss: 0.8124377 Vali Loss: 0.6287688 Test Loss: 0.4058260
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7975399494171143
Epoch: 34, Steps: 60 | Train Loss: 0.8120754 Vali Loss: 0.6280661 Test Loss: 0.4057935
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.7551937103271484
Epoch: 35, Steps: 60 | Train Loss: 0.8126658 Vali Loss: 0.6315476 Test Loss: 0.4057443
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.79970383644104
Epoch: 36, Steps: 60 | Train Loss: 0.8106307 Vali Loss: 0.6281757 Test Loss: 0.4057293
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.836775302886963
Epoch: 37, Steps: 60 | Train Loss: 0.8109799 Vali Loss: 0.6309448 Test Loss: 0.4057143
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.8441412448883057
Epoch: 38, Steps: 60 | Train Loss: 0.8132379 Vali Loss: 0.6264543 Test Loss: 0.4056684
Validation loss decreased (0.628001 --> 0.626454).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.9876413345336914
Epoch: 39, Steps: 60 | Train Loss: 0.8104238 Vali Loss: 0.6284540 Test Loss: 0.4056470
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.4607436656951904
Epoch: 40, Steps: 60 | Train Loss: 0.8113920 Vali Loss: 0.6267306 Test Loss: 0.4056284
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.0551583766937256
Epoch: 41, Steps: 60 | Train Loss: 0.8129666 Vali Loss: 0.6310163 Test Loss: 0.4056089
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.8060200214385986
Epoch: 42, Steps: 60 | Train Loss: 0.8125803 Vali Loss: 0.6281340 Test Loss: 0.4055870
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0337581634521484
Epoch: 43, Steps: 60 | Train Loss: 0.8116205 Vali Loss: 0.6264088 Test Loss: 0.4055670
Validation loss decreased (0.626454 --> 0.626409).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.940821647644043
Epoch: 44, Steps: 60 | Train Loss: 0.8126311 Vali Loss: 0.6273743 Test Loss: 0.4055546
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.747342824935913
Epoch: 45, Steps: 60 | Train Loss: 0.8122151 Vali Loss: 0.6294324 Test Loss: 0.4055296
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.090675115585327
Epoch: 46, Steps: 60 | Train Loss: 0.8119532 Vali Loss: 0.6249102 Test Loss: 0.4055119
Validation loss decreased (0.626409 --> 0.624910).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.799668550491333
Epoch: 47, Steps: 60 | Train Loss: 0.8104736 Vali Loss: 0.6267388 Test Loss: 0.4055063
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9243392944335938
Epoch: 48, Steps: 60 | Train Loss: 0.8135733 Vali Loss: 0.6271127 Test Loss: 0.4054941
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.6128473281860352
Epoch: 49, Steps: 60 | Train Loss: 0.8140070 Vali Loss: 0.6311505 Test Loss: 0.4054773
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.699246883392334
Epoch: 50, Steps: 60 | Train Loss: 0.8120692 Vali Loss: 0.6238241 Test Loss: 0.4054699
Validation loss decreased (0.624910 --> 0.623824).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.7065024375915527
Epoch: 51, Steps: 60 | Train Loss: 0.8109708 Vali Loss: 0.6255056 Test Loss: 0.4054566
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.882537603378296
Epoch: 52, Steps: 60 | Train Loss: 0.8107335 Vali Loss: 0.6277857 Test Loss: 0.4054469
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.69608473777771
Epoch: 53, Steps: 60 | Train Loss: 0.8115382 Vali Loss: 0.6320817 Test Loss: 0.4054380
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.5567784309387207
Epoch: 54, Steps: 60 | Train Loss: 0.8118849 Vali Loss: 0.6246700 Test Loss: 0.4054352
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.1799938678741455
Epoch: 55, Steps: 60 | Train Loss: 0.8106820 Vali Loss: 0.6269650 Test Loss: 0.4054148
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9817819595336914
Epoch: 56, Steps: 60 | Train Loss: 0.8126062 Vali Loss: 0.6306639 Test Loss: 0.4054093
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.6063897609710693
Epoch: 57, Steps: 60 | Train Loss: 0.8102775 Vali Loss: 0.6287764 Test Loss: 0.4054024
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.8365135192871094
Epoch: 58, Steps: 60 | Train Loss: 0.8109424 Vali Loss: 0.6254514 Test Loss: 0.4053967
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.5916600227355957
Epoch: 59, Steps: 60 | Train Loss: 0.8113312 Vali Loss: 0.6282312 Test Loss: 0.4053909
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.008023262023926
Epoch: 60, Steps: 60 | Train Loss: 0.8119316 Vali Loss: 0.6284127 Test Loss: 0.4053833
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.1788110733032227
Epoch: 61, Steps: 60 | Train Loss: 0.8110184 Vali Loss: 0.6258335 Test Loss: 0.4053793
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6279823780059814
Epoch: 62, Steps: 60 | Train Loss: 0.8108278 Vali Loss: 0.6291603 Test Loss: 0.4053695
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.6867821216583252
Epoch: 63, Steps: 60 | Train Loss: 0.8118497 Vali Loss: 0.6259352 Test Loss: 0.4053632
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.6900112628936768
Epoch: 64, Steps: 60 | Train Loss: 0.8119547 Vali Loss: 0.6281437 Test Loss: 0.4053575
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.6000654697418213
Epoch: 65, Steps: 60 | Train Loss: 0.8116082 Vali Loss: 0.6265015 Test Loss: 0.4053505
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.6417131423950195
Epoch: 66, Steps: 60 | Train Loss: 0.8110320 Vali Loss: 0.6278731 Test Loss: 0.4053493
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.729970932006836
Epoch: 67, Steps: 60 | Train Loss: 0.8122162 Vali Loss: 0.6277772 Test Loss: 0.4053451
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.6895618438720703
Epoch: 68, Steps: 60 | Train Loss: 0.8115209 Vali Loss: 0.6272451 Test Loss: 0.4053358
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.19807505607605
Epoch: 69, Steps: 60 | Train Loss: 0.8111288 Vali Loss: 0.6252359 Test Loss: 0.4053341
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9366538524627686
Epoch: 70, Steps: 60 | Train Loss: 0.8099779 Vali Loss: 0.6240165 Test Loss: 0.4053298
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.40364348888397217, mae:0.4301720857620239, rse:0.5078140497207642, corr:[ 0.21911152  0.22108443  0.21941458  0.2203938   0.21823421  0.21711315
  0.21720117  0.21556538  0.21474901  0.21410105  0.21247531  0.21154957
  0.2107397   0.20920941  0.20866857  0.20851907  0.20769507  0.20720032
  0.20686868  0.20575906  0.20486179  0.20446093  0.20345825  0.20199953
  0.20032208  0.19852076  0.19718558  0.19641775  0.19514875  0.19384763
  0.19300982  0.19175708  0.19023858  0.18937424  0.18848062  0.18719935
  0.18625763  0.18549342  0.18448476  0.18369937  0.18334794  0.18267128
  0.1820127   0.18167284  0.18088315  0.17990248  0.17945287  0.17761739
  0.17451374  0.172765    0.17153673  0.16982648  0.16871883  0.1682648
  0.16693506  0.16588487  0.16565569  0.16504492  0.16475418  0.16469213
  0.16410491  0.16334683  0.16378695  0.16385561  0.16361484  0.16371396
  0.16351455  0.16306178  0.16339292  0.1634643   0.16272603  0.16214366
  0.16105255  0.15997769  0.15958405  0.15910877  0.15815741  0.1581162
  0.15824746  0.15775919  0.15773919  0.15804256  0.15795226  0.15764369
  0.1578067   0.15772972  0.15766503  0.15775263  0.15765347  0.15741895
  0.1574843   0.15721367  0.15704474  0.15732689  0.15725204  0.15653737
  0.15578032  0.15522908  0.15432034  0.15406145  0.15387458  0.15318209
  0.1528386   0.1528994   0.15296826  0.15298486  0.153257    0.15313356
  0.15323374  0.15339997  0.15315458  0.15261337  0.15265992  0.15264842
  0.1522176   0.1519505   0.15189965  0.15164538  0.15126902  0.15029901
  0.148756    0.14774539  0.14686356  0.14606452  0.1453627   0.14471084
  0.14395325  0.14340192  0.1431882   0.14288118  0.14276001  0.14238793
  0.14166375  0.14112775  0.14117585  0.14092593  0.14050722  0.14035636
  0.13989404  0.13917254  0.13912053  0.13906302  0.13851976  0.13782205
  0.1359129   0.13414016  0.1331526   0.13281003  0.1321316   0.13167208
  0.13145164  0.13085495  0.1306509   0.13082224  0.13078146  0.13040227
  0.13047034  0.13004468  0.12971826  0.12996902  0.1298412   0.12928158
  0.12938331  0.12938997  0.1289479   0.12910795  0.12912382  0.12789795
  0.12634888  0.1256914   0.12493063  0.12454247  0.12434103  0.12383662
  0.12344589  0.12314105  0.12276396  0.12247451  0.12291055  0.12292687
  0.12275551  0.12259116  0.12245219  0.12235422  0.12256674  0.12242851
  0.12218634  0.12234571  0.12258933  0.12269127  0.12310556  0.12333445
  0.12272183  0.12248725  0.12258176  0.12255368  0.12264338  0.1230869
  0.1231619   0.12311983  0.12376895  0.1239208   0.12393626  0.12409814
  0.12401765  0.12351296  0.12362933  0.12380483  0.12361705  0.12394521
  0.12439685  0.12433077  0.1243618   0.1247889   0.12499639  0.12459014
  0.12379064  0.12287611  0.12231421  0.12246828  0.12227646  0.12209604
  0.12235909  0.12269361  0.12286133  0.12334956  0.12383483  0.12388628
  0.12423779  0.12423503  0.12422504  0.12476514  0.1252845   0.1253974
  0.12583919  0.12635012  0.12660493  0.127118    0.12770517  0.12757628
  0.12707764  0.12714447  0.12686408  0.1267208   0.12687308  0.12707277
  0.12695679  0.12728973  0.12803164  0.12868299  0.12974265  0.13037124
  0.13115275  0.1315859   0.1317441   0.132102    0.13256052  0.13268787
  0.1329214   0.13349512  0.13392366  0.13436319  0.13519849  0.13564794
  0.13532682  0.13534409  0.13538377  0.13589813  0.13641456  0.13711776
  0.13743347  0.13801165  0.13881966  0.13926943  0.14009485  0.14070664
  0.14138731  0.14200096  0.14238371  0.1423942   0.1429987   0.14374132
  0.14378522  0.14389099  0.14439349  0.14471067  0.14525224  0.14587212
  0.1459193   0.14590724  0.1459224   0.14595011  0.14592187  0.14649302
  0.14679757  0.14695041  0.14804213  0.14881907  0.14919998  0.1499441
  0.15075497  0.15093155  0.15168847  0.15233369  0.15245175  0.15304916
  0.15362266  0.153509    0.15373607  0.15435958  0.1546058   0.15475169
  0.15480964  0.15430012  0.1538951   0.15434295  0.15429282  0.1541514
  0.15444529  0.15494467  0.1551818   0.15574147  0.15633395  0.1567302
  0.15790421  0.15838914  0.15839842  0.15903601  0.15967447  0.15953664
  0.15996565  0.1608225   0.16093366  0.16110796  0.16195537  0.16229662
  0.16203997  0.16208772  0.16202566  0.16229755  0.16281554  0.16297807
  0.16323808  0.16376477  0.16400854  0.16414008  0.16473001  0.16511364
  0.16557452  0.16627926  0.16650902  0.16653988  0.16726339  0.16768523
  0.16786218  0.16869538  0.16933343  0.16941904  0.16998789  0.17041506
  0.17007518  0.17020825  0.17063695  0.17011507  0.16980083  0.17071791
  0.17127891  0.17165104  0.17269383  0.17344767  0.17371015  0.17439961
  0.17493722  0.17489782  0.17557248  0.17614107  0.176165    0.17653555
  0.17691752  0.17683382  0.17724565  0.17805374  0.17813227  0.17815804
  0.1783312   0.17804126  0.17761318  0.17805128  0.1780621   0.17797711
  0.1786501   0.17924076  0.17960748  0.18020931  0.18093921  0.18097703
  0.18129812  0.18134713  0.18126555  0.1812826   0.18123901  0.18091825
  0.18078077  0.18086652  0.18070568  0.1805578   0.18073839  0.1806168
  0.18005706  0.17994799  0.17985006  0.17964481  0.1795104   0.17954496
  0.17940386  0.17942353  0.17954819  0.17932184  0.17907561  0.17882538
  0.17844206  0.17791636  0.17755686  0.1768722   0.1760867   0.1755862
  0.17517118  0.174741    0.17460611  0.17461635  0.1744426   0.17396684
  0.17318797  0.17261478  0.17225893  0.17169563  0.17080852  0.17026018
  0.16988225  0.16928041  0.16890697  0.16844364  0.16784313  0.16721667
  0.1669424   0.16637379  0.16599315  0.16581413  0.16535214  0.16503224
  0.16514896  0.1649797   0.16457638  0.16484371  0.16500513  0.16464373
  0.16428038  0.16409439  0.16344783  0.16311449  0.1625493   0.16188966
  0.16163956  0.16142431  0.16059251  0.16003561  0.15988524  0.15908279
  0.1584848   0.15790077  0.15712005  0.15658541  0.15646529  0.1560485
  0.15580559  0.15565124  0.15488867  0.15449533  0.15476526  0.1540766
  0.15258162  0.1518496   0.15107259  0.14981392  0.1486786   0.14763771
  0.14659296  0.14591894  0.14504588  0.1437889   0.14337182  0.14274882
  0.14157653  0.1407182   0.14034525  0.13959935  0.13895787  0.13871565
  0.138294    0.13801718  0.13798934  0.13762794  0.13732152  0.13668595
  0.13465971  0.13284723  0.13183533  0.13055915  0.12934607  0.12883294
  0.12776339  0.1268508   0.12660785  0.12602164  0.12525564  0.12456948
  0.12360766  0.1223511   0.12201586  0.12176771  0.12109499  0.12077284
  0.12057159  0.11972704  0.11907372  0.1191551   0.11862467  0.11749275
  0.11578047  0.11376192  0.11219391  0.11098829  0.10860346  0.10639974
  0.10542145  0.10404558  0.10255068  0.10176814  0.10080412  0.09925214
  0.09825247  0.09688234  0.09565531  0.09522504  0.09455052  0.09322191
  0.09265086  0.09233502  0.09117764  0.09066916  0.09043385  0.08853706
  0.0862404   0.08488592  0.08320541  0.08194365  0.08079047  0.07903321
  0.07768307  0.07692323  0.07587469  0.07496822  0.07492795  0.07398449
  0.07266075  0.07207797  0.07170796  0.07070071  0.07010596  0.06958583
  0.06886338  0.06865896  0.06839621  0.06763649  0.06721841  0.06614392
  0.06354947  0.06171814  0.06045059  0.05868362  0.05702437  0.05558643
  0.05395451  0.05274506  0.05173919  0.05052628  0.04994614  0.04920267
  0.04786079  0.04687072  0.04662761  0.04590722  0.04523598  0.04517566
  0.0449908   0.04451964  0.04408181  0.04363534  0.04314789  0.04226494
  0.04006757  0.03800732  0.03657904  0.03477862  0.03250811  0.0313806
  0.02971304  0.02777515  0.02686039  0.02600136  0.02507259  0.02472295
  0.02447133  0.02333269  0.02318964  0.0233539   0.02302074  0.02325072
  0.02397874  0.0236795   0.02344565  0.02391356  0.023432    0.02273576
  0.02148456  0.01956894  0.01790145  0.01707396  0.01501748  0.01352738
  0.01371416  0.01281867  0.01198762  0.01265094  0.01273705  0.01188226
  0.0123239   0.0120621   0.01097085  0.01100789  0.01117571  0.01015007
  0.00986896  0.01010555  0.00953734  0.00966749  0.01046357  0.00890406
  0.00638943  0.00568869  0.00405759  0.00229876  0.00160589  0.00052596
 -0.00081673 -0.00105705 -0.00161686 -0.00231268 -0.00141397 -0.00188945
 -0.00301592 -0.00316463 -0.00279587 -0.00325676 -0.00305412 -0.00283499
 -0.00325794 -0.0032609  -0.00298603 -0.00277553 -0.00223529 -0.00262175
 -0.00527703 -0.00682264 -0.00751187 -0.00950069 -0.01147502 -0.01224674
 -0.01341523 -0.01423005 -0.01399344 -0.01388145 -0.01329438 -0.01279169
 -0.0126876  -0.01360681 -0.01323307 -0.01286689 -0.01305847 -0.01244014
 -0.01148146 -0.01204042 -0.01241057 -0.01126252 -0.01120428 -0.01177368
 -0.01380495 -0.01623727 -0.01754172 -0.01828002 -0.02017937 -0.02074773
 -0.02107389 -0.02193455 -0.02213713 -0.02241831 -0.02206366 -0.02161942
 -0.02149476 -0.02292675 -0.0224347  -0.02174213 -0.02206063 -0.0224457
 -0.02152777 -0.02184078 -0.02348431 -0.02304145 -0.02577722 -0.02553422]
