Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_192_FITS_ETTh2_ftM_sl360_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=106, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  15386112.0
params:  17334.0
Trainable parameters:  17334
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.6964356899261475
Epoch: 1, Steps: 63 | Train Loss: 0.6973006 Vali Loss: 0.3666715 Test Loss: 0.4162087
Validation loss decreased (inf --> 0.366672).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.7868626117706299
Epoch: 2, Steps: 63 | Train Loss: 0.5844858 Vali Loss: 0.3296532 Test Loss: 0.3884917
Validation loss decreased (0.366672 --> 0.329653).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.755002737045288
Epoch: 3, Steps: 63 | Train Loss: 0.5548041 Vali Loss: 0.3142746 Test Loss: 0.3781123
Validation loss decreased (0.329653 --> 0.314275).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.8285353183746338
Epoch: 4, Steps: 63 | Train Loss: 0.5420073 Vali Loss: 0.3055981 Test Loss: 0.3722190
Validation loss decreased (0.314275 --> 0.305598).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.003507375717163
Epoch: 5, Steps: 63 | Train Loss: 0.5339614 Vali Loss: 0.2999297 Test Loss: 0.3684855
Validation loss decreased (0.305598 --> 0.299930).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.9118835926055908
Epoch: 6, Steps: 63 | Train Loss: 0.5289301 Vali Loss: 0.2968632 Test Loss: 0.3655375
Validation loss decreased (0.299930 --> 0.296863).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4815537929534912
Epoch: 7, Steps: 63 | Train Loss: 0.5243726 Vali Loss: 0.2940905 Test Loss: 0.3636529
Validation loss decreased (0.296863 --> 0.294091).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8481125831604004
Epoch: 8, Steps: 63 | Train Loss: 0.5210837 Vali Loss: 0.2920848 Test Loss: 0.3623773
Validation loss decreased (0.294091 --> 0.292085).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.8444442749023438
Epoch: 9, Steps: 63 | Train Loss: 0.5201615 Vali Loss: 0.2908653 Test Loss: 0.3612365
Validation loss decreased (0.292085 --> 0.290865).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.4739437103271484
Epoch: 10, Steps: 63 | Train Loss: 0.5166629 Vali Loss: 0.2895471 Test Loss: 0.3603867
Validation loss decreased (0.290865 --> 0.289547).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7983653545379639
Epoch: 11, Steps: 63 | Train Loss: 0.5157727 Vali Loss: 0.2885669 Test Loss: 0.3596734
Validation loss decreased (0.289547 --> 0.288567).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4209082126617432
Epoch: 12, Steps: 63 | Train Loss: 0.5147108 Vali Loss: 0.2877531 Test Loss: 0.3590204
Validation loss decreased (0.288567 --> 0.287753).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.7175402641296387
Epoch: 13, Steps: 63 | Train Loss: 0.5145556 Vali Loss: 0.2870568 Test Loss: 0.3585616
Validation loss decreased (0.287753 --> 0.287057).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.5330963134765625
Epoch: 14, Steps: 63 | Train Loss: 0.5137013 Vali Loss: 0.2864103 Test Loss: 0.3580601
Validation loss decreased (0.287057 --> 0.286410).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.9755799770355225
Epoch: 15, Steps: 63 | Train Loss: 0.5123374 Vali Loss: 0.2858801 Test Loss: 0.3577430
Validation loss decreased (0.286410 --> 0.285880).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5722944736480713
Epoch: 16, Steps: 63 | Train Loss: 0.5111536 Vali Loss: 0.2852960 Test Loss: 0.3575198
Validation loss decreased (0.285880 --> 0.285296).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4803433418273926
Epoch: 17, Steps: 63 | Train Loss: 0.5111632 Vali Loss: 0.2848937 Test Loss: 0.3572264
Validation loss decreased (0.285296 --> 0.284894).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4662988185882568
Epoch: 18, Steps: 63 | Train Loss: 0.5108879 Vali Loss: 0.2845030 Test Loss: 0.3569366
Validation loss decreased (0.284894 --> 0.284503).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5543451309204102
Epoch: 19, Steps: 63 | Train Loss: 0.5106089 Vali Loss: 0.2842291 Test Loss: 0.3567488
Validation loss decreased (0.284503 --> 0.284229).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.7965812683105469
Epoch: 20, Steps: 63 | Train Loss: 0.5086561 Vali Loss: 0.2838527 Test Loss: 0.3565114
Validation loss decreased (0.284229 --> 0.283853).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.5601727962493896
Epoch: 21, Steps: 63 | Train Loss: 0.5100696 Vali Loss: 0.2834711 Test Loss: 0.3563757
Validation loss decreased (0.283853 --> 0.283471).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.4242157936096191
Epoch: 22, Steps: 63 | Train Loss: 0.5098792 Vali Loss: 0.2833259 Test Loss: 0.3562060
Validation loss decreased (0.283471 --> 0.283326).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8733148574829102
Epoch: 23, Steps: 63 | Train Loss: 0.5089891 Vali Loss: 0.2831429 Test Loss: 0.3560981
Validation loss decreased (0.283326 --> 0.283143).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6901021003723145
Epoch: 24, Steps: 63 | Train Loss: 0.5086178 Vali Loss: 0.2828962 Test Loss: 0.3559460
Validation loss decreased (0.283143 --> 0.282896).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.8524441719055176
Epoch: 25, Steps: 63 | Train Loss: 0.5089097 Vali Loss: 0.2825921 Test Loss: 0.3558870
Validation loss decreased (0.282896 --> 0.282592).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7293286323547363
Epoch: 26, Steps: 63 | Train Loss: 0.5079285 Vali Loss: 0.2824976 Test Loss: 0.3557836
Validation loss decreased (0.282592 --> 0.282498).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1521592140197754
Epoch: 27, Steps: 63 | Train Loss: 0.5080553 Vali Loss: 0.2822888 Test Loss: 0.3557061
Validation loss decreased (0.282498 --> 0.282289).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.9668800830841064
Epoch: 28, Steps: 63 | Train Loss: 0.5081356 Vali Loss: 0.2820622 Test Loss: 0.3556465
Validation loss decreased (0.282289 --> 0.282062).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6958599090576172
Epoch: 29, Steps: 63 | Train Loss: 0.5078858 Vali Loss: 0.2820094 Test Loss: 0.3555175
Validation loss decreased (0.282062 --> 0.282009).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.8200111389160156
Epoch: 30, Steps: 63 | Train Loss: 0.5069821 Vali Loss: 0.2818119 Test Loss: 0.3554540
Validation loss decreased (0.282009 --> 0.281812).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6441314220428467
Epoch: 31, Steps: 63 | Train Loss: 0.5077113 Vali Loss: 0.2817044 Test Loss: 0.3553942
Validation loss decreased (0.281812 --> 0.281704).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.554556131362915
Epoch: 32, Steps: 63 | Train Loss: 0.5077216 Vali Loss: 0.2816330 Test Loss: 0.3553477
Validation loss decreased (0.281704 --> 0.281633).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.1209447383880615
Epoch: 33, Steps: 63 | Train Loss: 0.5072929 Vali Loss: 0.2814783 Test Loss: 0.3552876
Validation loss decreased (0.281633 --> 0.281478).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3779847621917725
Epoch: 34, Steps: 63 | Train Loss: 0.5070321 Vali Loss: 0.2814755 Test Loss: 0.3551055
Validation loss decreased (0.281478 --> 0.281476).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.4698326587677002
Epoch: 35, Steps: 63 | Train Loss: 0.5066748 Vali Loss: 0.2813903 Test Loss: 0.3551454
Validation loss decreased (0.281476 --> 0.281390).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.690802812576294
Epoch: 36, Steps: 63 | Train Loss: 0.5067077 Vali Loss: 0.2812378 Test Loss: 0.3550768
Validation loss decreased (0.281390 --> 0.281238).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8128912448883057
Epoch: 37, Steps: 63 | Train Loss: 0.5070376 Vali Loss: 0.2811308 Test Loss: 0.3550477
Validation loss decreased (0.281238 --> 0.281131).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.5469024181365967
Epoch: 38, Steps: 63 | Train Loss: 0.5059422 Vali Loss: 0.2810614 Test Loss: 0.3550503
Validation loss decreased (0.281131 --> 0.281061).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.6390674114227295
Epoch: 39, Steps: 63 | Train Loss: 0.5058579 Vali Loss: 0.2810093 Test Loss: 0.3550417
Validation loss decreased (0.281061 --> 0.281009).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.297635555267334
Epoch: 40, Steps: 63 | Train Loss: 0.5052977 Vali Loss: 0.2809780 Test Loss: 0.3549604
Validation loss decreased (0.281009 --> 0.280978).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.5293841361999512
Epoch: 41, Steps: 63 | Train Loss: 0.5049042 Vali Loss: 0.2808707 Test Loss: 0.3549558
Validation loss decreased (0.280978 --> 0.280871).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.6005394458770752
Epoch: 42, Steps: 63 | Train Loss: 0.5065014 Vali Loss: 0.2808175 Test Loss: 0.3548955
Validation loss decreased (0.280871 --> 0.280818).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.15645694732666
Epoch: 43, Steps: 63 | Train Loss: 0.5067757 Vali Loss: 0.2808281 Test Loss: 0.3549125
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.9727280139923096
Epoch: 44, Steps: 63 | Train Loss: 0.5057356 Vali Loss: 0.2807712 Test Loss: 0.3548268
Validation loss decreased (0.280818 --> 0.280771).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.4344258308410645
Epoch: 45, Steps: 63 | Train Loss: 0.5062209 Vali Loss: 0.2806871 Test Loss: 0.3547960
Validation loss decreased (0.280771 --> 0.280687).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.3039891719818115
Epoch: 46, Steps: 63 | Train Loss: 0.5063470 Vali Loss: 0.2806808 Test Loss: 0.3547490
Validation loss decreased (0.280687 --> 0.280681).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.0740244388580322
Epoch: 47, Steps: 63 | Train Loss: 0.5060349 Vali Loss: 0.2805904 Test Loss: 0.3547976
Validation loss decreased (0.280681 --> 0.280590).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.424961805343628
Epoch: 48, Steps: 63 | Train Loss: 0.5047300 Vali Loss: 0.2805727 Test Loss: 0.3547628
Validation loss decreased (0.280590 --> 0.280573).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.0635485649108887
Epoch: 49, Steps: 63 | Train Loss: 0.5051878 Vali Loss: 0.2805596 Test Loss: 0.3547342
Validation loss decreased (0.280573 --> 0.280560).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.459441900253296
Epoch: 50, Steps: 63 | Train Loss: 0.5045814 Vali Loss: 0.2804171 Test Loss: 0.3547622
Validation loss decreased (0.280560 --> 0.280417).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.4643206596374512
Epoch: 51, Steps: 63 | Train Loss: 0.5055675 Vali Loss: 0.2804631 Test Loss: 0.3547155
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6243345737457275
Epoch: 52, Steps: 63 | Train Loss: 0.5059012 Vali Loss: 0.2803709 Test Loss: 0.3547147
Validation loss decreased (0.280417 --> 0.280371).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.609574794769287
Epoch: 53, Steps: 63 | Train Loss: 0.5038975 Vali Loss: 0.2802769 Test Loss: 0.3546900
Validation loss decreased (0.280371 --> 0.280277).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.7904198169708252
Epoch: 54, Steps: 63 | Train Loss: 0.5061477 Vali Loss: 0.2803866 Test Loss: 0.3546329
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.9125921726226807
Epoch: 55, Steps: 63 | Train Loss: 0.5043061 Vali Loss: 0.2802617 Test Loss: 0.3546344
Validation loss decreased (0.280277 --> 0.280262).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9549415111541748
Epoch: 56, Steps: 63 | Train Loss: 0.5057901 Vali Loss: 0.2802711 Test Loss: 0.3546288
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.7053749561309814
Epoch: 57, Steps: 63 | Train Loss: 0.5058492 Vali Loss: 0.2802727 Test Loss: 0.3546219
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.0742993354797363
Epoch: 58, Steps: 63 | Train Loss: 0.5056619 Vali Loss: 0.2802245 Test Loss: 0.3545999
Validation loss decreased (0.280262 --> 0.280224).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.239309787750244
Epoch: 59, Steps: 63 | Train Loss: 0.5057437 Vali Loss: 0.2801824 Test Loss: 0.3546015
Validation loss decreased (0.280224 --> 0.280182).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.025116443634033
Epoch: 60, Steps: 63 | Train Loss: 0.5055501 Vali Loss: 0.2801974 Test Loss: 0.3545964
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.9816443920135498
Epoch: 61, Steps: 63 | Train Loss: 0.5045697 Vali Loss: 0.2801453 Test Loss: 0.3545789
Validation loss decreased (0.280182 --> 0.280145).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.9162251949310303
Epoch: 62, Steps: 63 | Train Loss: 0.5053804 Vali Loss: 0.2802092 Test Loss: 0.3545666
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.67539644241333
Epoch: 63, Steps: 63 | Train Loss: 0.5056370 Vali Loss: 0.2800501 Test Loss: 0.3545703
Validation loss decreased (0.280145 --> 0.280050).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.712052345275879
Epoch: 64, Steps: 63 | Train Loss: 0.5059168 Vali Loss: 0.2801256 Test Loss: 0.3545583
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.710864543914795
Epoch: 65, Steps: 63 | Train Loss: 0.5057626 Vali Loss: 0.2801324 Test Loss: 0.3545494
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.9447436332702637
Epoch: 66, Steps: 63 | Train Loss: 0.5051424 Vali Loss: 0.2801221 Test Loss: 0.3545514
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.5221178531646729
Epoch: 67, Steps: 63 | Train Loss: 0.5050589 Vali Loss: 0.2801285 Test Loss: 0.3545296
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.9581208229064941
Epoch: 68, Steps: 63 | Train Loss: 0.5047806 Vali Loss: 0.2801037 Test Loss: 0.3545250
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.8812158107757568
Epoch: 69, Steps: 63 | Train Loss: 0.5045034 Vali Loss: 0.2800480 Test Loss: 0.3545081
Validation loss decreased (0.280050 --> 0.280048).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5723867416381836
Epoch: 70, Steps: 63 | Train Loss: 0.5055530 Vali Loss: 0.2800825 Test Loss: 0.3545167
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.8320536613464355
Epoch: 71, Steps: 63 | Train Loss: 0.5052774 Vali Loss: 0.2800458 Test Loss: 0.3544996
Validation loss decreased (0.280048 --> 0.280046).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.2578465938568115
Epoch: 72, Steps: 63 | Train Loss: 0.5056984 Vali Loss: 0.2799863 Test Loss: 0.3544951
Validation loss decreased (0.280046 --> 0.279986).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.9276208877563477
Epoch: 73, Steps: 63 | Train Loss: 0.5050834 Vali Loss: 0.2800600 Test Loss: 0.3544942
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.1054089069366455
Epoch: 74, Steps: 63 | Train Loss: 0.5052826 Vali Loss: 0.2800417 Test Loss: 0.3544891
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.8886535167694092
Epoch: 75, Steps: 63 | Train Loss: 0.5052678 Vali Loss: 0.2800419 Test Loss: 0.3544837
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.0933735370635986
Epoch: 76, Steps: 63 | Train Loss: 0.5044976 Vali Loss: 0.2800343 Test Loss: 0.3544793
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.463897466659546
Epoch: 77, Steps: 63 | Train Loss: 0.5047377 Vali Loss: 0.2800248 Test Loss: 0.3544699
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.6995091438293457
Epoch: 78, Steps: 63 | Train Loss: 0.5053351 Vali Loss: 0.2798766 Test Loss: 0.3544731
Validation loss decreased (0.279986 --> 0.279877).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.042584180831909
Epoch: 79, Steps: 63 | Train Loss: 0.5054140 Vali Loss: 0.2799214 Test Loss: 0.3544647
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.7817425727844238
Epoch: 80, Steps: 63 | Train Loss: 0.5051069 Vali Loss: 0.2799774 Test Loss: 0.3544658
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.9765517711639404
Epoch: 81, Steps: 63 | Train Loss: 0.5051615 Vali Loss: 0.2800083 Test Loss: 0.3544630
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.120147705078125
Epoch: 82, Steps: 63 | Train Loss: 0.5041270 Vali Loss: 0.2799458 Test Loss: 0.3544604
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.7793395519256592
Epoch: 83, Steps: 63 | Train Loss: 0.5042266 Vali Loss: 0.2799174 Test Loss: 0.3544579
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.5174436569213867
Epoch: 84, Steps: 63 | Train Loss: 0.5048238 Vali Loss: 0.2799166 Test Loss: 0.3544514
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.5243730545043945
Epoch: 85, Steps: 63 | Train Loss: 0.5050338 Vali Loss: 0.2799085 Test Loss: 0.3544451
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 3.9464492797851562
Epoch: 86, Steps: 63 | Train Loss: 0.5056609 Vali Loss: 0.2799648 Test Loss: 0.3544499
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.2683942317962646
Epoch: 87, Steps: 63 | Train Loss: 0.5039195 Vali Loss: 0.2799757 Test Loss: 0.3544453
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 3.644740104675293
Epoch: 88, Steps: 63 | Train Loss: 0.5045220 Vali Loss: 0.2798601 Test Loss: 0.3544431
Validation loss decreased (0.279877 --> 0.279860).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 3.1537556648254395
Epoch: 89, Steps: 63 | Train Loss: 0.5046407 Vali Loss: 0.2799603 Test Loss: 0.3544396
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.163245916366577
Epoch: 90, Steps: 63 | Train Loss: 0.5038839 Vali Loss: 0.2799576 Test Loss: 0.3544384
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 4.2305567264556885
Epoch: 91, Steps: 63 | Train Loss: 0.5052290 Vali Loss: 0.2799404 Test Loss: 0.3544351
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 3.023352861404419
Epoch: 92, Steps: 63 | Train Loss: 0.5049671 Vali Loss: 0.2798470 Test Loss: 0.3544333
Validation loss decreased (0.279860 --> 0.279847).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.111618995666504
Epoch: 93, Steps: 63 | Train Loss: 0.5035933 Vali Loss: 0.2798636 Test Loss: 0.3544287
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.6074538230895996
Epoch: 94, Steps: 63 | Train Loss: 0.5046635 Vali Loss: 0.2799150 Test Loss: 0.3544320
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.5018978118896484
Epoch: 95, Steps: 63 | Train Loss: 0.5054626 Vali Loss: 0.2796005 Test Loss: 0.3544250
Validation loss decreased (0.279847 --> 0.279601).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 3.428183078765869
Epoch: 96, Steps: 63 | Train Loss: 0.5054819 Vali Loss: 0.2799194 Test Loss: 0.3544210
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.0868330001831055
Epoch: 97, Steps: 63 | Train Loss: 0.5051790 Vali Loss: 0.2798830 Test Loss: 0.3544202
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.3447484970092773
Epoch: 98, Steps: 63 | Train Loss: 0.5049548 Vali Loss: 0.2799246 Test Loss: 0.3544233
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.087986469268799
Epoch: 99, Steps: 63 | Train Loss: 0.5053655 Vali Loss: 0.2799271 Test Loss: 0.3544240
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.7820942401885986
Epoch: 100, Steps: 63 | Train Loss: 0.5049974 Vali Loss: 0.2798626 Test Loss: 0.3544211
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_360_192_FITS_ETTh2_ftM_sl360_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3355261981487274, mae:0.37668177485466003, rse:0.4645196199417114, corr:[0.26506326 0.270023   0.26681757 0.26818871 0.26673907 0.26471788
 0.26463017 0.26349238 0.26165816 0.26065004 0.25942272 0.25768209
 0.2562618  0.25519595 0.2545381  0.25407806 0.25339845 0.25263688
 0.2518316  0.25055715 0.24891222 0.24771357 0.24612948 0.24370569
 0.24139099 0.23945896 0.23739612 0.23587215 0.2347883  0.23318945
 0.23164968 0.23030336 0.22839677 0.22645631 0.22567683 0.2247838
 0.22262822 0.22143896 0.22136158 0.2202795  0.21895437 0.21888627
 0.21852487 0.21700618 0.21588819 0.2153174  0.21379349 0.21173145
 0.21025173 0.2085581  0.20656604 0.20533672 0.20416546 0.20206377
 0.1999775  0.19853391 0.1969025  0.19514237 0.19447629 0.19370311
 0.19262323 0.19242494 0.19288659 0.19233342 0.19148156 0.19136418
 0.19077814 0.18968107 0.18938677 0.18921629 0.1881473  0.18704377
 0.18640468 0.18561159 0.18475302 0.18408997 0.18317628 0.18217061
 0.18188466 0.18153188 0.18066932 0.1802128  0.18073443 0.18040688
 0.17955981 0.17976761 0.18015909 0.1793787  0.17871743 0.17881517
 0.17815566 0.17732139 0.17775957 0.177895   0.17710048 0.1765487
 0.17611392 0.17466351 0.17343874 0.17299406 0.17212522 0.17081426
 0.17050765 0.17031386 0.16955331 0.16923128 0.16965948 0.16964708
 0.1687836  0.16875236 0.16851874 0.16758236 0.16699322 0.1672967
 0.1668127  0.1658486  0.16588885 0.16531911 0.1636155  0.16235068
 0.16148657 0.15967155 0.15851593 0.1583709  0.15721151 0.1554761
 0.15515296 0.15498857 0.15361701 0.15283827 0.15326174 0.15260448
 0.15150827 0.15156782 0.15149216 0.15052006 0.15034696 0.15094438
 0.1501275  0.14939241 0.15000683 0.14994863 0.14865297 0.14762314
 0.14649688 0.14451538 0.14326282 0.14304756 0.14214103 0.14098291
 0.14132012 0.14091124 0.13982676 0.13996446 0.14042151 0.13946219
 0.13872051 0.13978064 0.13988243 0.1393982  0.13980466 0.1402639
 0.13945433 0.13912211 0.14001113 0.13967288 0.13882595 0.13853055
 0.13755277 0.13530451 0.13441236 0.13447855 0.13293515 0.13112769
 0.13072006 0.12926361 0.12715134 0.12699604 0.12671974 0.12438229
 0.12383524 0.12453898 0.12277938 0.12191688 0.12297012 0.12038542
 0.11676786 0.11790773 0.11748587 0.11210137 0.11448228 0.11387994]
