Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=0, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.6347522735595703
Epoch: 1, Steps: 56 | Train Loss: 1.0331568 Vali Loss: 0.7748775 Test Loss: 0.4328141
Validation loss decreased (inf --> 0.774877).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6774799823760986
Epoch: 2, Steps: 56 | Train Loss: 0.8884475 Vali Loss: 0.7307091 Test Loss: 0.4072667
Validation loss decreased (0.774877 --> 0.730709).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.999307632446289
Epoch: 3, Steps: 56 | Train Loss: 0.8588923 Vali Loss: 0.7097679 Test Loss: 0.3979217
Validation loss decreased (0.730709 --> 0.709768).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6185426712036133
Epoch: 4, Steps: 56 | Train Loss: 0.8410635 Vali Loss: 0.6908643 Test Loss: 0.3926774
Validation loss decreased (0.709768 --> 0.690864).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5900750160217285
Epoch: 5, Steps: 56 | Train Loss: 0.8343386 Vali Loss: 0.6850649 Test Loss: 0.3894660
Validation loss decreased (0.690864 --> 0.685065).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.5300555229187012
Epoch: 6, Steps: 56 | Train Loss: 0.8254466 Vali Loss: 0.6785234 Test Loss: 0.3871916
Validation loss decreased (0.685065 --> 0.678523).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.56434965133667
Epoch: 7, Steps: 56 | Train Loss: 0.8212871 Vali Loss: 0.6724102 Test Loss: 0.3855529
Validation loss decreased (0.678523 --> 0.672410).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5905683040618896
Epoch: 8, Steps: 56 | Train Loss: 0.8165132 Vali Loss: 0.6742715 Test Loss: 0.3842265
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.5771608352661133
Epoch: 9, Steps: 56 | Train Loss: 0.8167433 Vali Loss: 0.6711994 Test Loss: 0.3832189
Validation loss decreased (0.672410 --> 0.671199).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6549742221832275
Epoch: 10, Steps: 56 | Train Loss: 0.8155754 Vali Loss: 0.6616267 Test Loss: 0.3824700
Validation loss decreased (0.671199 --> 0.661627).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7625579833984375
Epoch: 11, Steps: 56 | Train Loss: 0.8125997 Vali Loss: 0.6635643 Test Loss: 0.3817176
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.527287244796753
Epoch: 12, Steps: 56 | Train Loss: 0.8123132 Vali Loss: 0.6589248 Test Loss: 0.3813034
Validation loss decreased (0.661627 --> 0.658925).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5335369110107422
Epoch: 13, Steps: 56 | Train Loss: 0.8100234 Vali Loss: 0.6585692 Test Loss: 0.3808861
Validation loss decreased (0.658925 --> 0.658569).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.4787399768829346
Epoch: 14, Steps: 56 | Train Loss: 0.8100791 Vali Loss: 0.6595479 Test Loss: 0.3805936
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5859739780426025
Epoch: 15, Steps: 56 | Train Loss: 0.8081755 Vali Loss: 0.6617031 Test Loss: 0.3803404
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.8402278423309326
Epoch: 16, Steps: 56 | Train Loss: 0.8084634 Vali Loss: 0.6613108 Test Loss: 0.3801054
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.5042407512664795
Epoch: 17, Steps: 56 | Train Loss: 0.8059700 Vali Loss: 0.6556362 Test Loss: 0.3800066
Validation loss decreased (0.658569 --> 0.655636).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.5468735694885254
Epoch: 18, Steps: 56 | Train Loss: 0.8086050 Vali Loss: 0.6610117 Test Loss: 0.3798684
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5472588539123535
Epoch: 19, Steps: 56 | Train Loss: 0.8053694 Vali Loss: 0.6566812 Test Loss: 0.3797522
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5020999908447266
Epoch: 20, Steps: 56 | Train Loss: 0.8049372 Vali Loss: 0.6519191 Test Loss: 0.3795989
Validation loss decreased (0.655636 --> 0.651919).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.5282018184661865
Epoch: 21, Steps: 56 | Train Loss: 0.8044776 Vali Loss: 0.6526778 Test Loss: 0.3795679
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5890657901763916
Epoch: 22, Steps: 56 | Train Loss: 0.8056642 Vali Loss: 0.6572899 Test Loss: 0.3794677
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5766990184783936
Epoch: 23, Steps: 56 | Train Loss: 0.8046343 Vali Loss: 0.6554429 Test Loss: 0.3794290
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6432466506958008
Epoch: 24, Steps: 56 | Train Loss: 0.8054786 Vali Loss: 0.6500307 Test Loss: 0.3793657
Validation loss decreased (0.651919 --> 0.650031).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.5082998275756836
Epoch: 25, Steps: 56 | Train Loss: 0.8039008 Vali Loss: 0.6551402 Test Loss: 0.3793485
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4695498943328857
Epoch: 26, Steps: 56 | Train Loss: 0.8030696 Vali Loss: 0.6526194 Test Loss: 0.3793415
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5217692852020264
Epoch: 27, Steps: 56 | Train Loss: 0.8033355 Vali Loss: 0.6551165 Test Loss: 0.3792701
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.636516809463501
Epoch: 28, Steps: 56 | Train Loss: 0.8016170 Vali Loss: 0.6519887 Test Loss: 0.3792506
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6816885471343994
Epoch: 29, Steps: 56 | Train Loss: 0.8033918 Vali Loss: 0.6502787 Test Loss: 0.3792447
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.6309139728546143
Epoch: 30, Steps: 56 | Train Loss: 0.8032803 Vali Loss: 0.6555206 Test Loss: 0.3792713
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6426477432250977
Epoch: 31, Steps: 56 | Train Loss: 0.8023263 Vali Loss: 0.6528831 Test Loss: 0.3792326
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.657651662826538
Epoch: 32, Steps: 56 | Train Loss: 0.8023592 Vali Loss: 0.6507346 Test Loss: 0.3792153
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5862324237823486
Epoch: 33, Steps: 56 | Train Loss: 0.8023890 Vali Loss: 0.6500291 Test Loss: 0.3792022
Validation loss decreased (0.650031 --> 0.650029).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.683645248413086
Epoch: 34, Steps: 56 | Train Loss: 0.8011775 Vali Loss: 0.6493723 Test Loss: 0.3792340
Validation loss decreased (0.650029 --> 0.649372).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.7178919315338135
Epoch: 35, Steps: 56 | Train Loss: 0.8024521 Vali Loss: 0.6493289 Test Loss: 0.3791998
Validation loss decreased (0.649372 --> 0.649329).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.781898021697998
Epoch: 36, Steps: 56 | Train Loss: 0.8009057 Vali Loss: 0.6488898 Test Loss: 0.3791669
Validation loss decreased (0.649329 --> 0.648890).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.7116038799285889
Epoch: 37, Steps: 56 | Train Loss: 0.8013379 Vali Loss: 0.6501414 Test Loss: 0.3791960
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7199220657348633
Epoch: 38, Steps: 56 | Train Loss: 0.8019687 Vali Loss: 0.6471287 Test Loss: 0.3791528
Validation loss decreased (0.648890 --> 0.647129).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.6248235702514648
Epoch: 39, Steps: 56 | Train Loss: 0.8025321 Vali Loss: 0.6504494 Test Loss: 0.3791538
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7344207763671875
Epoch: 40, Steps: 56 | Train Loss: 0.8012764 Vali Loss: 0.6502191 Test Loss: 0.3791533
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9438207149505615
Epoch: 41, Steps: 56 | Train Loss: 0.8018457 Vali Loss: 0.6497823 Test Loss: 0.3791654
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.551896095275879
Epoch: 42, Steps: 56 | Train Loss: 0.8004606 Vali Loss: 0.6520256 Test Loss: 0.3791649
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.6091272830963135
Epoch: 43, Steps: 56 | Train Loss: 0.8002111 Vali Loss: 0.6509728 Test Loss: 0.3791636
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.6663906574249268
Epoch: 44, Steps: 56 | Train Loss: 0.8018798 Vali Loss: 0.6492013 Test Loss: 0.3791640
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7821669578552246
Epoch: 45, Steps: 56 | Train Loss: 0.8008152 Vali Loss: 0.6488144 Test Loss: 0.3791537
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6101958751678467
Epoch: 46, Steps: 56 | Train Loss: 0.8012429 Vali Loss: 0.6492853 Test Loss: 0.3791487
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.746917724609375
Epoch: 47, Steps: 56 | Train Loss: 0.8009155 Vali Loss: 0.6513662 Test Loss: 0.3791569
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.7000656127929688
Epoch: 48, Steps: 56 | Train Loss: 0.8007770 Vali Loss: 0.6463929 Test Loss: 0.3791609
Validation loss decreased (0.647129 --> 0.646393).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.6825511455535889
Epoch: 49, Steps: 56 | Train Loss: 0.8004586 Vali Loss: 0.6483701 Test Loss: 0.3791451
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6340622901916504
Epoch: 50, Steps: 56 | Train Loss: 0.8004212 Vali Loss: 0.6505597 Test Loss: 0.3791415
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6146535873413086
Epoch: 51, Steps: 56 | Train Loss: 0.7998222 Vali Loss: 0.6485522 Test Loss: 0.3791422
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.7133071422576904
Epoch: 52, Steps: 56 | Train Loss: 0.8002395 Vali Loss: 0.6412333 Test Loss: 0.3791397
Validation loss decreased (0.646393 --> 0.641233).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6341273784637451
Epoch: 53, Steps: 56 | Train Loss: 0.8016693 Vali Loss: 0.6469864 Test Loss: 0.3791483
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9703378677368164
Epoch: 54, Steps: 56 | Train Loss: 0.7996633 Vali Loss: 0.6427307 Test Loss: 0.3791360
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.5660760402679443
Epoch: 55, Steps: 56 | Train Loss: 0.8003715 Vali Loss: 0.6500514 Test Loss: 0.3791405
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.6717405319213867
Epoch: 56, Steps: 56 | Train Loss: 0.7982019 Vali Loss: 0.6491259 Test Loss: 0.3791412
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.5897555351257324
Epoch: 57, Steps: 56 | Train Loss: 0.8007038 Vali Loss: 0.6419793 Test Loss: 0.3791492
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.5354313850402832
Epoch: 58, Steps: 56 | Train Loss: 0.7997855 Vali Loss: 0.6505275 Test Loss: 0.3791369
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.5719642639160156
Epoch: 59, Steps: 56 | Train Loss: 0.8008983 Vali Loss: 0.6461636 Test Loss: 0.3791476
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.5174074172973633
Epoch: 60, Steps: 56 | Train Loss: 0.7999520 Vali Loss: 0.6515055 Test Loss: 0.3791483
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6849513053894043
Epoch: 61, Steps: 56 | Train Loss: 0.8002999 Vali Loss: 0.6487496 Test Loss: 0.3791453
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.530360460281372
Epoch: 62, Steps: 56 | Train Loss: 0.8001605 Vali Loss: 0.6480608 Test Loss: 0.3791508
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.5785605907440186
Epoch: 63, Steps: 56 | Train Loss: 0.8000150 Vali Loss: 0.6467812 Test Loss: 0.3791450
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.904254674911499
Epoch: 64, Steps: 56 | Train Loss: 0.7983229 Vali Loss: 0.6461266 Test Loss: 0.3791397
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.528731107711792
Epoch: 65, Steps: 56 | Train Loss: 0.8012285 Vali Loss: 0.6466285 Test Loss: 0.3791475
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.4877204895019531
Epoch: 66, Steps: 56 | Train Loss: 0.8008396 Vali Loss: 0.6489087 Test Loss: 0.3791476
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.6901209354400635
Epoch: 67, Steps: 56 | Train Loss: 0.8008652 Vali Loss: 0.6440712 Test Loss: 0.3791440
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.4946043491363525
Epoch: 68, Steps: 56 | Train Loss: 0.8009389 Vali Loss: 0.6482433 Test Loss: 0.3791477
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.6058905124664307
Epoch: 69, Steps: 56 | Train Loss: 0.8010682 Vali Loss: 0.6480654 Test Loss: 0.3791432
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5854673385620117
Epoch: 70, Steps: 56 | Train Loss: 0.7996588 Vali Loss: 0.6464711 Test Loss: 0.3791435
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.216466188430786
Epoch: 71, Steps: 56 | Train Loss: 0.8006454 Vali Loss: 0.6460576 Test Loss: 0.3791503
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.008352518081665
Epoch: 72, Steps: 56 | Train Loss: 0.7993612 Vali Loss: 0.6500570 Test Loss: 0.3791500
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3777378499507904, mae:0.4229428768157959, rse:0.4912481904029846, corr:[ 0.21605587  0.22041078  0.21804185  0.21912006  0.2190958   0.21726187
  0.21706302  0.2171277   0.21538666  0.21380703  0.21324785  0.21212877
  0.21044938  0.20942478  0.20882036  0.20790066  0.20717256  0.2067622
  0.20590541  0.20456266  0.20365106  0.20321138  0.20213267  0.20066011
  0.19923833  0.19817251  0.19706713  0.19609262  0.19545129  0.19484618
  0.1941274   0.19322607  0.19244012  0.1915356   0.19063486  0.18972805
  0.18892078  0.1883189   0.18759759  0.1867601   0.18604054  0.18563634
  0.18532573  0.18477264  0.18392004  0.18327649  0.18249576  0.18101399
  0.17940027  0.17819187  0.1773053   0.17655985  0.1758411   0.1750616
  0.17443414  0.17392999  0.17317387  0.17244655  0.17213906  0.17195587
  0.17148203  0.1712261   0.17161834  0.17191634  0.1717209   0.17185293
  0.17214642  0.1720229   0.17170565  0.17171744  0.17165487  0.17107901
  0.17054322  0.17032947  0.17000616  0.16939905  0.16927223  0.16950743
  0.1692732   0.16885021  0.16887592  0.16906469  0.16886857  0.16868605
  0.16897266  0.16934322  0.16933325  0.16924205  0.16945887  0.16959491
  0.16929615  0.16917568  0.16951326  0.16961153  0.16938049  0.16928168
  0.16944453  0.16939947  0.16910368  0.16895215  0.16885178  0.16832876
  0.16794112  0.16802168  0.16813381  0.16778497  0.16770802  0.16808812
  0.16812441  0.16762872  0.167414    0.16750866  0.1672603   0.16686562
  0.16684784  0.16697162  0.1665854   0.1659437   0.16565338  0.16518214
  0.16416796  0.16341874  0.16316968  0.16263303  0.16172579  0.16124853
  0.1610538   0.16051693  0.15991122  0.15957573  0.15926577  0.15852055
  0.15783472  0.15742028  0.15696107  0.15637253  0.15598248  0.15586928
  0.15525612  0.15449902  0.15440623  0.15465222  0.1543987   0.1534607
  0.15225893  0.151247    0.15028901  0.14966029  0.14947179  0.14926553
  0.1487838   0.14836979  0.14816092  0.14766707  0.14702539  0.14648506
  0.14600885  0.14556777  0.14532112  0.14508522  0.14464146  0.14463386
  0.14507727  0.14529166  0.14521347  0.14539205  0.14577681  0.14530487
  0.14420384  0.14355896  0.14368537  0.14354695  0.14289427  0.14242959
  0.14219335  0.14149931  0.1405677   0.14010595  0.13974588  0.1391556
  0.13863616  0.13861279  0.13873826  0.13853066  0.13846533  0.138881
  0.13918243  0.13937445  0.13970116  0.14012544  0.14059387  0.1409575
  0.1411623   0.14109455  0.14094795  0.14123783  0.14162481  0.14147389
  0.1413077   0.1413564   0.14133239  0.14101492  0.1410518   0.14125077
  0.1410505   0.14067216  0.14076728  0.14126857  0.14146122  0.1416209
  0.14211811  0.1425329   0.1424585   0.14247687  0.14287116  0.14281313
  0.14211284  0.14160995  0.14164123  0.1414942   0.14095712  0.14092666
  0.14114504  0.14118391  0.14109136  0.14126615  0.14133494  0.14104743
  0.14113069  0.14157386  0.14176857  0.14188479  0.14254545  0.14338851
  0.14368941  0.14395185  0.14480792  0.14553595  0.14558774  0.14573587
  0.14652707  0.14701295  0.14688171  0.14694755  0.14735137  0.14745224
  0.14731199  0.14758994  0.14840953  0.14886059  0.14901674  0.14955296
  0.1503624   0.15102676  0.15160258  0.15244143  0.15320723  0.153835
  0.15441413  0.15494683  0.15531917  0.15595904  0.15697214  0.15766592
  0.15787567  0.15807399  0.15874763  0.1593243   0.15933555  0.15925445
  0.15949437  0.1599578   0.16039358  0.16078219  0.16130334  0.1617667
  0.16196184  0.16233425  0.1630581   0.16390477  0.1644367   0.16467579
  0.16493419  0.1652986   0.1655628   0.16594085  0.16661759  0.16717263
  0.16726586  0.166958    0.16670673  0.16676922  0.16685621  0.16684344
  0.16680475  0.16679838  0.1668984   0.16726917  0.16770746  0.16785699
  0.16781114  0.16809076  0.16856891  0.16886692  0.16917835  0.16985501
  0.1702995   0.17026149  0.17002384  0.17023748  0.17081304  0.17107442
  0.17094436  0.17068298  0.17053188  0.17024618  0.16993414  0.16978651
  0.16976666  0.16971986  0.16949251  0.16928874  0.16910802  0.16917372
  0.16934255  0.16951108  0.16999242  0.1705988   0.1711114   0.171227
  0.17116025  0.17155622  0.17217222  0.17266111  0.17306596  0.17330082
  0.17327802  0.17316349  0.173185    0.17333509  0.17360668  0.17368467
  0.1737304   0.1738353   0.17395109  0.17391896  0.17369266  0.17378895
  0.17406006  0.17453904  0.17512167  0.17565706  0.1761851   0.17680243
  0.17728528  0.17760113  0.17779016  0.17818452  0.17863859  0.17904133
  0.17943308  0.17975597  0.17979862  0.17960148  0.1795982   0.17993651
  0.18044269  0.18081456  0.18089958  0.18110584  0.18117547  0.18098007
  0.18061297  0.1805583   0.18085136  0.1811191   0.18109815  0.18085146
  0.1807446   0.18084364  0.18097234  0.18092096  0.18085031  0.18097775
  0.1811646   0.18125162  0.18122478  0.18124925  0.18147731  0.18180291
  0.18214187  0.18256679  0.18299069  0.18317033  0.18320733  0.18336831
  0.18363322  0.1832996   0.1829054   0.18289307  0.18277775  0.18232308
  0.18202211  0.18211414  0.18202178  0.1816834   0.18147512  0.18148562
  0.18118717  0.18083166  0.18078974  0.18086034  0.18065897  0.18025038
  0.18013294  0.1802974   0.18032418  0.18006693  0.17975965  0.17952333
  0.17905658  0.17860733  0.17808276  0.17726085  0.17624822  0.17525484
  0.17423776  0.17308532  0.17246127  0.17220104  0.1714769   0.17062815
  0.17023692  0.17008318  0.16959812  0.16884214  0.1685123   0.1682319
  0.16758272  0.16722748  0.16708142  0.16659066  0.16558103  0.16515756
  0.16506632  0.16462919  0.16397776  0.16397162  0.16398254  0.16327669
  0.16263813  0.16275123  0.16265196  0.16212498  0.16195749  0.16221686
  0.16217957  0.16173239  0.16156831  0.16159368  0.16112606  0.16059566
  0.16064562  0.16069923  0.16015267  0.15991609  0.15988542  0.15920424
  0.15841445  0.1581517   0.15819173  0.15765755  0.15697691  0.15670256
  0.15654704  0.15610133  0.15580826  0.15596999  0.15578042  0.15510538
  0.1545445   0.15417016  0.15355286  0.15276736  0.15229961  0.151673
  0.15085082  0.15031706  0.14994153  0.1490059   0.14813039  0.14788824
  0.14750163  0.14635916  0.14541465  0.14527313  0.14488974  0.144071
  0.14366582  0.14365369  0.14319669  0.14239848  0.14211279  0.1417784
  0.14049308  0.13902998  0.13835266  0.13793409  0.13710894  0.13639405
  0.13596438  0.13544938  0.13467798  0.13420486  0.13391593  0.13290472
  0.1316372   0.13108283  0.13093954  0.13021421  0.12946786  0.1293741
  0.12894098  0.12787896  0.12715632  0.12709355  0.12654471  0.12523378
  0.12391586  0.1230213   0.12208208  0.12080336  0.11975687  0.11864812
  0.11741877  0.11646401  0.11580176  0.11491299  0.11384256  0.11299726
  0.11200663  0.11062188  0.10968423  0.10929449  0.10856425  0.10747048
  0.1064975   0.10605936  0.10550427  0.10450807  0.1037814   0.10287448
  0.10151411  0.10008637  0.09910309  0.09817995  0.09722693  0.09661613
  0.09602602  0.09480357  0.09389252  0.09367781  0.0936371   0.09285234
  0.0920598   0.0919662   0.09177329  0.09073186  0.09002171  0.08984833
  0.08916003  0.08792174  0.08712207  0.08674677  0.08570182  0.08410455
  0.08278586  0.08192672  0.08083414  0.07980201  0.0790916   0.07802964
  0.07659473  0.07549699  0.0747223   0.07376297  0.07277239  0.07229809
  0.07157373  0.07037106  0.0695864   0.06950681  0.06938157  0.06870757
  0.06819142  0.06822222  0.06775691  0.06673542  0.06589092  0.06527685
  0.0640808   0.06256545  0.06130553  0.06053754  0.05942333  0.05827543
  0.05706298  0.05562903  0.05420017  0.05347441  0.05315764  0.05226174
  0.05128934  0.05079143  0.05058867  0.05032196  0.05018644  0.0504174
  0.05020509  0.0499432   0.05016225  0.05064752  0.05027521  0.04952805
  0.04879325  0.04800991  0.04716201  0.04630127  0.04536197  0.0446301
  0.04427861  0.04409518  0.04354217  0.0429153   0.04283129  0.04318764
  0.04257429  0.04120911  0.04094683  0.04130664  0.04042271  0.0393756
  0.03933639  0.0393087   0.03869462  0.03851818  0.03864887  0.03810746
  0.03657566  0.03542578  0.03506385  0.03435287  0.03353842  0.03284215
  0.0320307   0.03105827  0.03065838  0.03097105  0.03090988  0.03026723
  0.02989995  0.0300734   0.03010961  0.03000036  0.03072407  0.0312373
  0.03057547  0.03008106  0.03056032  0.03116317  0.03075064  0.03021438
  0.03000697  0.02958773  0.02891958  0.02813954  0.02750277  0.02636543
  0.02599576  0.02568979  0.02521811  0.02479575  0.02514401  0.02527207
  0.02409854  0.02268006  0.02329966  0.02398355  0.02306952  0.02288835
  0.02438762  0.02409488  0.02212217  0.02146694  0.0223505   0.02135339
  0.01854982  0.01654599  0.01584643  0.01453274  0.01331563  0.01245031
  0.01155703  0.01035225  0.00989918  0.00910422  0.00704339  0.00668462
  0.00783153  0.00626547  0.00403556  0.00508389  0.00658895  0.00364826
  0.00140138  0.00437211  0.00304556 -0.00397593 -0.00115391  0.00915224]
