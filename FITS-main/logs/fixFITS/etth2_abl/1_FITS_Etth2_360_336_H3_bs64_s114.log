Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=58, out_features=112, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5820416.0
params:  6608.0
Trainable parameters:  6608
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.039402484893799
Epoch: 1, Steps: 62 | Train Loss: 0.8204373 Vali Loss: 0.4812361 Test Loss: 0.4143812
Validation loss decreased (inf --> 0.481236).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.1092112064361572
Epoch: 2, Steps: 62 | Train Loss: 0.7037022 Vali Loss: 0.4380691 Test Loss: 0.3846520
Validation loss decreased (0.481236 --> 0.438069).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.4792087078094482
Epoch: 3, Steps: 62 | Train Loss: 0.6650365 Vali Loss: 0.4209264 Test Loss: 0.3763405
Validation loss decreased (0.438069 --> 0.420926).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.1585159301757812
Epoch: 4, Steps: 62 | Train Loss: 0.6492768 Vali Loss: 0.4101011 Test Loss: 0.3732087
Validation loss decreased (0.420926 --> 0.410101).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.8614206314086914
Epoch: 5, Steps: 62 | Train Loss: 0.6400654 Vali Loss: 0.4017883 Test Loss: 0.3716214
Validation loss decreased (0.410101 --> 0.401788).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.4710781574249268
Epoch: 6, Steps: 62 | Train Loss: 0.6344931 Vali Loss: 0.4017571 Test Loss: 0.3703921
Validation loss decreased (0.401788 --> 0.401757).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.813403606414795
Epoch: 7, Steps: 62 | Train Loss: 0.6307276 Vali Loss: 0.3976991 Test Loss: 0.3693410
Validation loss decreased (0.401757 --> 0.397699).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.585742950439453
Epoch: 8, Steps: 62 | Train Loss: 0.6275032 Vali Loss: 0.3952119 Test Loss: 0.3685183
Validation loss decreased (0.397699 --> 0.395212).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.790200710296631
Epoch: 9, Steps: 62 | Train Loss: 0.6247880 Vali Loss: 0.3921340 Test Loss: 0.3678311
Validation loss decreased (0.395212 --> 0.392134).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.071690797805786
Epoch: 10, Steps: 62 | Train Loss: 0.6229877 Vali Loss: 0.3904596 Test Loss: 0.3672542
Validation loss decreased (0.392134 --> 0.390460).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.50541615486145
Epoch: 11, Steps: 62 | Train Loss: 0.6211462 Vali Loss: 0.3859512 Test Loss: 0.3668684
Validation loss decreased (0.390460 --> 0.385951).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1426639556884766
Epoch: 12, Steps: 62 | Train Loss: 0.6197879 Vali Loss: 0.3860094 Test Loss: 0.3664213
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.1520814895629883
Epoch: 13, Steps: 62 | Train Loss: 0.6193078 Vali Loss: 0.3870838 Test Loss: 0.3661412
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.1291439533233643
Epoch: 14, Steps: 62 | Train Loss: 0.6181919 Vali Loss: 0.3833567 Test Loss: 0.3659300
Validation loss decreased (0.385951 --> 0.383357).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.8843226432800293
Epoch: 15, Steps: 62 | Train Loss: 0.6169103 Vali Loss: 0.3836092 Test Loss: 0.3656894
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5390026569366455
Epoch: 16, Steps: 62 | Train Loss: 0.6169056 Vali Loss: 0.3821719 Test Loss: 0.3655967
Validation loss decreased (0.383357 --> 0.382172).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.61435866355896
Epoch: 17, Steps: 62 | Train Loss: 0.6154172 Vali Loss: 0.3818152 Test Loss: 0.3653838
Validation loss decreased (0.382172 --> 0.381815).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.134585380554199
Epoch: 18, Steps: 62 | Train Loss: 0.6156893 Vali Loss: 0.3834063 Test Loss: 0.3652727
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.676833152770996
Epoch: 19, Steps: 62 | Train Loss: 0.6149773 Vali Loss: 0.3832534 Test Loss: 0.3651411
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.373562812805176
Epoch: 20, Steps: 62 | Train Loss: 0.6149180 Vali Loss: 0.3833205 Test Loss: 0.3650119
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.344515800476074
Epoch: 21, Steps: 62 | Train Loss: 0.6145072 Vali Loss: 0.3827339 Test Loss: 0.3649624
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.1167166233062744
Epoch: 22, Steps: 62 | Train Loss: 0.6140029 Vali Loss: 0.3790496 Test Loss: 0.3648590
Validation loss decreased (0.381815 --> 0.379050).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.411048173904419
Epoch: 23, Steps: 62 | Train Loss: 0.6139779 Vali Loss: 0.3823793 Test Loss: 0.3648156
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.102726936340332
Epoch: 24, Steps: 62 | Train Loss: 0.6131975 Vali Loss: 0.3805301 Test Loss: 0.3647097
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.5725204944610596
Epoch: 25, Steps: 62 | Train Loss: 0.6130315 Vali Loss: 0.3810833 Test Loss: 0.3646040
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.570582389831543
Epoch: 26, Steps: 62 | Train Loss: 0.6122345 Vali Loss: 0.3803251 Test Loss: 0.3647591
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.3347768783569336
Epoch: 27, Steps: 62 | Train Loss: 0.6129540 Vali Loss: 0.3790966 Test Loss: 0.3646154
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.331510066986084
Epoch: 28, Steps: 62 | Train Loss: 0.6127012 Vali Loss: 0.3785735 Test Loss: 0.3645244
Validation loss decreased (0.379050 --> 0.378573).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.8906676769256592
Epoch: 29, Steps: 62 | Train Loss: 0.6117125 Vali Loss: 0.3803835 Test Loss: 0.3645830
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.144627094268799
Epoch: 30, Steps: 62 | Train Loss: 0.6118926 Vali Loss: 0.3811634 Test Loss: 0.3645426
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.126976251602173
Epoch: 31, Steps: 62 | Train Loss: 0.6123769 Vali Loss: 0.3786783 Test Loss: 0.3645373
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.2478368282318115
Epoch: 32, Steps: 62 | Train Loss: 0.6113990 Vali Loss: 0.3773479 Test Loss: 0.3645037
Validation loss decreased (0.378573 --> 0.377348).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.9281444549560547
Epoch: 33, Steps: 62 | Train Loss: 0.6120542 Vali Loss: 0.3774652 Test Loss: 0.3644475
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.098621129989624
Epoch: 34, Steps: 62 | Train Loss: 0.6111512 Vali Loss: 0.3808728 Test Loss: 0.3643914
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.2789664268493652
Epoch: 35, Steps: 62 | Train Loss: 0.6118821 Vali Loss: 0.3804895 Test Loss: 0.3643988
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.201972723007202
Epoch: 36, Steps: 62 | Train Loss: 0.6111891 Vali Loss: 0.3771590 Test Loss: 0.3643370
Validation loss decreased (0.377348 --> 0.377159).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.399174928665161
Epoch: 37, Steps: 62 | Train Loss: 0.6115536 Vali Loss: 0.3764822 Test Loss: 0.3643463
Validation loss decreased (0.377159 --> 0.376482).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7059805393218994
Epoch: 38, Steps: 62 | Train Loss: 0.6115465 Vali Loss: 0.3804359 Test Loss: 0.3643653
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.8663127422332764
Epoch: 39, Steps: 62 | Train Loss: 0.6115490 Vali Loss: 0.3773717 Test Loss: 0.3643204
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.8570942878723145
Epoch: 40, Steps: 62 | Train Loss: 0.6109721 Vali Loss: 0.3784012 Test Loss: 0.3642817
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.009232521057129
Epoch: 41, Steps: 62 | Train Loss: 0.6113729 Vali Loss: 0.3807099 Test Loss: 0.3642899
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.0609617233276367
Epoch: 42, Steps: 62 | Train Loss: 0.6113629 Vali Loss: 0.3823868 Test Loss: 0.3643331
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9150993824005127
Epoch: 43, Steps: 62 | Train Loss: 0.6112936 Vali Loss: 0.3786668 Test Loss: 0.3642728
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.1776669025421143
Epoch: 44, Steps: 62 | Train Loss: 0.6111800 Vali Loss: 0.3749133 Test Loss: 0.3642649
Validation loss decreased (0.376482 --> 0.374913).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.082287311553955
Epoch: 45, Steps: 62 | Train Loss: 0.6103970 Vali Loss: 0.3775837 Test Loss: 0.3642541
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.1184604167938232
Epoch: 46, Steps: 62 | Train Loss: 0.6108387 Vali Loss: 0.3787372 Test Loss: 0.3642581
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.037717580795288
Epoch: 47, Steps: 62 | Train Loss: 0.6107915 Vali Loss: 0.3786746 Test Loss: 0.3642687
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.397043228149414
Epoch: 48, Steps: 62 | Train Loss: 0.6109423 Vali Loss: 0.3782236 Test Loss: 0.3642405
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.3690133094787598
Epoch: 49, Steps: 62 | Train Loss: 0.6108658 Vali Loss: 0.3774374 Test Loss: 0.3642343
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.0058562755584717
Epoch: 50, Steps: 62 | Train Loss: 0.6102796 Vali Loss: 0.3778739 Test Loss: 0.3642432
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.6132824420928955
Epoch: 51, Steps: 62 | Train Loss: 0.6106550 Vali Loss: 0.3768276 Test Loss: 0.3642240
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.3061678409576416
Epoch: 52, Steps: 62 | Train Loss: 0.6106756 Vali Loss: 0.3775844 Test Loss: 0.3642197
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.2432754039764404
Epoch: 53, Steps: 62 | Train Loss: 0.6099912 Vali Loss: 0.3769363 Test Loss: 0.3642043
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.874030351638794
Epoch: 54, Steps: 62 | Train Loss: 0.6105871 Vali Loss: 0.3789179 Test Loss: 0.3642036
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.6378328800201416
Epoch: 55, Steps: 62 | Train Loss: 0.6099215 Vali Loss: 0.3782367 Test Loss: 0.3642198
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.935730218887329
Epoch: 56, Steps: 62 | Train Loss: 0.6098431 Vali Loss: 0.3796330 Test Loss: 0.3641987
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.8445584774017334
Epoch: 57, Steps: 62 | Train Loss: 0.6105208 Vali Loss: 0.3777926 Test Loss: 0.3642042
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.8167924880981445
Epoch: 58, Steps: 62 | Train Loss: 0.6106235 Vali Loss: 0.3773109 Test Loss: 0.3641913
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.39654803276062
Epoch: 59, Steps: 62 | Train Loss: 0.6096684 Vali Loss: 0.3786163 Test Loss: 0.3642005
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.7348973751068115
Epoch: 60, Steps: 62 | Train Loss: 0.6101713 Vali Loss: 0.3774904 Test Loss: 0.3641779
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.879120111465454
Epoch: 61, Steps: 62 | Train Loss: 0.6098930 Vali Loss: 0.3793792 Test Loss: 0.3641759
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.3516478538513184
Epoch: 62, Steps: 62 | Train Loss: 0.6098429 Vali Loss: 0.3773048 Test Loss: 0.3641684
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.2249464988708496
Epoch: 63, Steps: 62 | Train Loss: 0.6100851 Vali Loss: 0.3786510 Test Loss: 0.3641679
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.9085206985473633
Epoch: 64, Steps: 62 | Train Loss: 0.6105936 Vali Loss: 0.3774340 Test Loss: 0.3641681
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3598018288612366, mae:0.39787620306015015, rse:0.4795912206172943, corr:[0.25917733 0.26445007 0.26441127 0.26123762 0.25948912 0.25935745
 0.2593957  0.25845975 0.2565304  0.2545306  0.2531619  0.2521728
 0.2512935  0.250083   0.24882136 0.24782823 0.24713407 0.24643807
 0.24555425 0.24442932 0.24317259 0.24193317 0.24051496 0.23891164
 0.23711315 0.23550695 0.23421091 0.23320378 0.23236431 0.23141527
 0.2301416  0.22859827 0.22708759 0.22591612 0.22501022 0.22406462
 0.22277896 0.22133009 0.22005261 0.2190573  0.2182565  0.21728475
 0.21594729 0.21441667 0.21284257 0.21141046 0.20992154 0.20812576
 0.20603631 0.20395596 0.2021535  0.20069833 0.1994021  0.1979866
 0.19622466 0.19424637 0.19254708 0.19126704 0.19058612 0.19025172
 0.18984967 0.18920949 0.18849826 0.18793583 0.18755886 0.18738905
 0.18697622 0.18627177 0.18528946 0.18446994 0.183878   0.18341187
 0.18279059 0.18182734 0.18069175 0.1796999  0.17917272 0.17893583
 0.17868806 0.17822269 0.17765994 0.17706162 0.17672452 0.17665257
 0.17676531 0.17667419 0.1763073  0.17574507 0.17523257 0.17501505
 0.17508173 0.17513533 0.1751425  0.17493625 0.17454107 0.17399354
 0.17338024 0.17255192 0.1716568  0.17074308 0.17005906 0.169778
 0.169905   0.1698661  0.16965017 0.16918027 0.1687934  0.16870731
 0.16872716 0.16860944 0.16813217 0.16742878 0.1665568  0.1659692
 0.16573063 0.16559856 0.1652937  0.1643944  0.16312663 0.16183874
 0.16085024 0.16017602 0.15963379 0.15895548 0.15809464 0.15738861
 0.15690617 0.15673003 0.15649962 0.15590931 0.15512896 0.15444878
 0.15416257 0.15414566 0.15410368 0.15371515 0.15293676 0.15213028
 0.15155359 0.15153694 0.15174384 0.15142527 0.15032168 0.14868759
 0.14710604 0.1460027  0.14541316 0.14512895 0.14472528 0.14403132
 0.14332545 0.14285117 0.14278142 0.14282346 0.14260657 0.14200607
 0.14138184 0.14114974 0.1412253  0.14145699 0.14144684 0.14105758
 0.14060313 0.1403798  0.14051443 0.14066783 0.14046918 0.139619
 0.13848472 0.1374     0.13656983 0.13608304 0.13559176 0.134796
 0.13381983 0.13284676 0.13229318 0.13204272 0.13195242 0.13158531
 0.1310687  0.13055836 0.13028015 0.1303942  0.13060004 0.13068832
 0.13076378 0.13090847 0.1312262  0.13180879 0.13226624 0.1320981
 0.13143805 0.13054483 0.12990732 0.1298091  0.1299685  0.12994242
 0.12954833 0.12881078 0.12809214 0.12774658 0.12793161 0.12812828
 0.12815282 0.12782721 0.12737471 0.1272972  0.12749341 0.12785529
 0.12808105 0.1281632  0.12805614 0.12773918 0.1274683  0.12704568
 0.12651537 0.12558457 0.12460368 0.12392446 0.12361313 0.12380672
 0.12403508 0.12407345 0.12356774 0.12262617 0.1218917  0.12151703
 0.12146875 0.1213882  0.1210875  0.12082318 0.12069581 0.12097944
 0.12153403 0.12228422 0.12269668 0.1225663  0.12211982 0.12163016
 0.12109741 0.12062546 0.12011479 0.11942453 0.11876546 0.11808111
 0.11774276 0.11779551 0.11784499 0.11746531 0.11707243 0.11701451
 0.11747975 0.11845256 0.11934517 0.12020162 0.12067664 0.1210631
 0.12131439 0.12176862 0.12258761 0.12320582 0.12361708 0.12356348
 0.12333123 0.12296408 0.12270674 0.12255909 0.1223293  0.12212782
 0.12198576 0.12213692 0.1223508  0.12232499 0.12245002 0.12218877
 0.12205015 0.1221939  0.12249684 0.12282107 0.12299965 0.12297683
 0.12262367 0.12238545 0.12236454 0.12259343 0.12292403 0.12318807
 0.12261614 0.12139111 0.12023236 0.11956072 0.11894124 0.11830366
 0.11751211 0.11668403 0.11624644 0.11570495 0.11527766 0.11485621
 0.11432744 0.11399614 0.11401116 0.1146356  0.11492727 0.11510493
 0.1146041  0.11414026 0.11420424 0.11444321 0.11510564 0.1151146
 0.11407217 0.11196287 0.10982753 0.10859579 0.10813063 0.10803064
 0.1076217  0.1069612  0.106084   0.10570969 0.10630637 0.10701202
 0.10685461 0.10582986 0.10519566 0.10578459 0.10693613 0.10729567
 0.10594992 0.10418257 0.1039701  0.10644072 0.1107518  0.10964105]
