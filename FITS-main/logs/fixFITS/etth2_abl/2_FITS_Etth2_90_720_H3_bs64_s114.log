Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=22, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3902976.0
params:  4554.0
Trainable parameters:  4554
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.837873697280884
Epoch: 1, Steps: 61 | Train Loss: 1.3030213 Vali Loss: 0.8614721 Test Loss: 0.7035931
Validation loss decreased (inf --> 0.861472).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8801064491271973
Epoch: 2, Steps: 61 | Train Loss: 1.1038893 Vali Loss: 0.7887772 Test Loss: 0.6092519
Validation loss decreased (0.861472 --> 0.788777).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.242065906524658
Epoch: 3, Steps: 61 | Train Loss: 0.9899217 Vali Loss: 0.7395021 Test Loss: 0.5517418
Validation loss decreased (0.788777 --> 0.739502).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.885007381439209
Epoch: 4, Steps: 61 | Train Loss: 0.9192260 Vali Loss: 0.7068648 Test Loss: 0.5138668
Validation loss decreased (0.739502 --> 0.706865).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.930391788482666
Epoch: 5, Steps: 61 | Train Loss: 0.8745712 Vali Loss: 0.6858394 Test Loss: 0.4886216
Validation loss decreased (0.706865 --> 0.685839).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.8465499877929688
Epoch: 6, Steps: 61 | Train Loss: 0.8432149 Vali Loss: 0.6744337 Test Loss: 0.4711885
Validation loss decreased (0.685839 --> 0.674434).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.2993950843811035
Epoch: 7, Steps: 61 | Train Loss: 0.8233798 Vali Loss: 0.6616884 Test Loss: 0.4589225
Validation loss decreased (0.674434 --> 0.661688).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.1123762130737305
Epoch: 8, Steps: 61 | Train Loss: 0.8084186 Vali Loss: 0.6577801 Test Loss: 0.4502158
Validation loss decreased (0.661688 --> 0.657780).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7105844020843506
Epoch: 9, Steps: 61 | Train Loss: 0.7975365 Vali Loss: 0.6534224 Test Loss: 0.4438568
Validation loss decreased (0.657780 --> 0.653422).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.7810561656951904
Epoch: 10, Steps: 61 | Train Loss: 0.7893823 Vali Loss: 0.6491026 Test Loss: 0.4392596
Validation loss decreased (0.653422 --> 0.649103).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.447706699371338
Epoch: 11, Steps: 61 | Train Loss: 0.7835808 Vali Loss: 0.6385143 Test Loss: 0.4358332
Validation loss decreased (0.649103 --> 0.638514).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.128265380859375
Epoch: 12, Steps: 61 | Train Loss: 0.7802136 Vali Loss: 0.6417438 Test Loss: 0.4332319
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.3383615016937256
Epoch: 13, Steps: 61 | Train Loss: 0.7774911 Vali Loss: 0.6355425 Test Loss: 0.4312961
Validation loss decreased (0.638514 --> 0.635543).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.217658281326294
Epoch: 14, Steps: 61 | Train Loss: 0.7748489 Vali Loss: 0.6402752 Test Loss: 0.4297569
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.318617820739746
Epoch: 15, Steps: 61 | Train Loss: 0.7731818 Vali Loss: 0.6370476 Test Loss: 0.4285727
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.2115590572357178
Epoch: 16, Steps: 61 | Train Loss: 0.7706044 Vali Loss: 0.6354992 Test Loss: 0.4276058
Validation loss decreased (0.635543 --> 0.635499).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.168659210205078
Epoch: 17, Steps: 61 | Train Loss: 0.7694051 Vali Loss: 0.6388351 Test Loss: 0.4268587
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.0756142139434814
Epoch: 18, Steps: 61 | Train Loss: 0.7690207 Vali Loss: 0.6356670 Test Loss: 0.4262401
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.437014579772949
Epoch: 19, Steps: 61 | Train Loss: 0.7677234 Vali Loss: 0.6330240 Test Loss: 0.4257421
Validation loss decreased (0.635499 --> 0.633024).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.7072539329528809
Epoch: 20, Steps: 61 | Train Loss: 0.7664629 Vali Loss: 0.6331866 Test Loss: 0.4253190
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.1437342166900635
Epoch: 21, Steps: 61 | Train Loss: 0.7662417 Vali Loss: 0.6329356 Test Loss: 0.4249626
Validation loss decreased (0.633024 --> 0.632936).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.8917884826660156
Epoch: 22, Steps: 61 | Train Loss: 0.7665255 Vali Loss: 0.6346495 Test Loss: 0.4246469
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6959750652313232
Epoch: 23, Steps: 61 | Train Loss: 0.7660440 Vali Loss: 0.6245517 Test Loss: 0.4243897
Validation loss decreased (0.632936 --> 0.624552).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.083153247833252
Epoch: 24, Steps: 61 | Train Loss: 0.7653565 Vali Loss: 0.6303798 Test Loss: 0.4241452
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.0228917598724365
Epoch: 25, Steps: 61 | Train Loss: 0.7660760 Vali Loss: 0.6283776 Test Loss: 0.4239814
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.143535614013672
Epoch: 26, Steps: 61 | Train Loss: 0.7641257 Vali Loss: 0.6346632 Test Loss: 0.4237994
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.9372761249542236
Epoch: 27, Steps: 61 | Train Loss: 0.7637747 Vali Loss: 0.6269592 Test Loss: 0.4236359
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.557358980178833
Epoch: 28, Steps: 61 | Train Loss: 0.7637708 Vali Loss: 0.6337738 Test Loss: 0.4235037
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.366248369216919
Epoch: 29, Steps: 61 | Train Loss: 0.7634349 Vali Loss: 0.6278319 Test Loss: 0.4233744
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.3679914474487305
Epoch: 30, Steps: 61 | Train Loss: 0.7647858 Vali Loss: 0.6353110 Test Loss: 0.4232658
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.316655397415161
Epoch: 31, Steps: 61 | Train Loss: 0.7644362 Vali Loss: 0.6265255 Test Loss: 0.4231485
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.1639795303344727
Epoch: 32, Steps: 61 | Train Loss: 0.7644489 Vali Loss: 0.6329274 Test Loss: 0.4230736
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.4697625637054443
Epoch: 33, Steps: 61 | Train Loss: 0.7633105 Vali Loss: 0.6295867 Test Loss: 0.4229753
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9807236194610596
Epoch: 34, Steps: 61 | Train Loss: 0.7643843 Vali Loss: 0.6288139 Test Loss: 0.4229048
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.7807421684265137
Epoch: 35, Steps: 61 | Train Loss: 0.7633819 Vali Loss: 0.6317723 Test Loss: 0.4228394
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.1406965255737305
Epoch: 36, Steps: 61 | Train Loss: 0.7634968 Vali Loss: 0.6324128 Test Loss: 0.4227686
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.296722888946533
Epoch: 37, Steps: 61 | Train Loss: 0.7625236 Vali Loss: 0.6305494 Test Loss: 0.4227039
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9416933059692383
Epoch: 38, Steps: 61 | Train Loss: 0.7631463 Vali Loss: 0.6226206 Test Loss: 0.4226501
Validation loss decreased (0.624552 --> 0.622621).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.4729816913604736
Epoch: 39, Steps: 61 | Train Loss: 0.7624208 Vali Loss: 0.6309853 Test Loss: 0.4226030
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.2726290225982666
Epoch: 40, Steps: 61 | Train Loss: 0.7633533 Vali Loss: 0.6233985 Test Loss: 0.4225502
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.228358268737793
Epoch: 41, Steps: 61 | Train Loss: 0.7626816 Vali Loss: 0.6351109 Test Loss: 0.4225098
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.5585591793060303
Epoch: 42, Steps: 61 | Train Loss: 0.7629708 Vali Loss: 0.6319683 Test Loss: 0.4224665
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.2511277198791504
Epoch: 43, Steps: 61 | Train Loss: 0.7620354 Vali Loss: 0.6306781 Test Loss: 0.4224244
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.311389446258545
Epoch: 44, Steps: 61 | Train Loss: 0.7630954 Vali Loss: 0.6292729 Test Loss: 0.4223848
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.895991325378418
Epoch: 45, Steps: 61 | Train Loss: 0.7613241 Vali Loss: 0.6273636 Test Loss: 0.4223531
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.924142837524414
Epoch: 46, Steps: 61 | Train Loss: 0.7626577 Vali Loss: 0.6298416 Test Loss: 0.4223174
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.228863000869751
Epoch: 47, Steps: 61 | Train Loss: 0.7618834 Vali Loss: 0.6254901 Test Loss: 0.4222884
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.0657241344451904
Epoch: 48, Steps: 61 | Train Loss: 0.7627632 Vali Loss: 0.6280916 Test Loss: 0.4222634
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.2663655281066895
Epoch: 49, Steps: 61 | Train Loss: 0.7613245 Vali Loss: 0.6276430 Test Loss: 0.4222351
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.977473258972168
Epoch: 50, Steps: 61 | Train Loss: 0.7617365 Vali Loss: 0.6247374 Test Loss: 0.4222066
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.7962698936462402
Epoch: 51, Steps: 61 | Train Loss: 0.7615632 Vali Loss: 0.6293759 Test Loss: 0.4221857
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.31545090675354
Epoch: 52, Steps: 61 | Train Loss: 0.7607354 Vali Loss: 0.6330075 Test Loss: 0.4221665
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.148653984069824
Epoch: 53, Steps: 61 | Train Loss: 0.7605138 Vali Loss: 0.6263180 Test Loss: 0.4221406
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.760955333709717
Epoch: 54, Steps: 61 | Train Loss: 0.7617936 Vali Loss: 0.6301896 Test Loss: 0.4221165
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.8456487655639648
Epoch: 55, Steps: 61 | Train Loss: 0.7611828 Vali Loss: 0.6278670 Test Loss: 0.4221030
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.144193172454834
Epoch: 56, Steps: 61 | Train Loss: 0.7614277 Vali Loss: 0.6308452 Test Loss: 0.4220855
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.0703108310699463
Epoch: 57, Steps: 61 | Train Loss: 0.7609650 Vali Loss: 0.6279273 Test Loss: 0.4220691
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.7960083484649658
Epoch: 58, Steps: 61 | Train Loss: 0.7606768 Vali Loss: 0.6302979 Test Loss: 0.4220524
EarlyStopping counter: 20 out of 20
Early stopping
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=22, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3902976.0
params:  4554.0
Trainable parameters:  4554
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.1590020656585693
Epoch: 1, Steps: 61 | Train Loss: 0.8545879 Vali Loss: 0.6274070 Test Loss: 0.4220135
Validation loss decreased (inf --> 0.627407).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.190157651901245
Epoch: 2, Steps: 61 | Train Loss: 0.8538263 Vali Loss: 0.6296840 Test Loss: 0.4216340
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.942298412322998
Epoch: 3, Steps: 61 | Train Loss: 0.8500197 Vali Loss: 0.6274072 Test Loss: 0.4213600
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.96144700050354
Epoch: 4, Steps: 61 | Train Loss: 0.8521583 Vali Loss: 0.6325086 Test Loss: 0.4211846
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.5284364223480225
Epoch: 5, Steps: 61 | Train Loss: 0.8506082 Vali Loss: 0.6320350 Test Loss: 0.4210023
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 6.367334604263306
Epoch: 6, Steps: 61 | Train Loss: 0.8510414 Vali Loss: 0.6291894 Test Loss: 0.4209027
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.62619686126709
Epoch: 7, Steps: 61 | Train Loss: 0.8502603 Vali Loss: 0.6230896 Test Loss: 0.4207631
Validation loss decreased (0.627407 --> 0.623090).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 6.355855941772461
Epoch: 8, Steps: 61 | Train Loss: 0.8488508 Vali Loss: 0.6297266 Test Loss: 0.4207098
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.385668039321899
Epoch: 9, Steps: 61 | Train Loss: 0.8497108 Vali Loss: 0.6236225 Test Loss: 0.4206169
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 5.723297119140625
Epoch: 10, Steps: 61 | Train Loss: 0.8496182 Vali Loss: 0.6227486 Test Loss: 0.4205318
Validation loss decreased (0.623090 --> 0.622749).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.6694111824035645
Epoch: 11, Steps: 61 | Train Loss: 0.8501735 Vali Loss: 0.6225348 Test Loss: 0.4204494
Validation loss decreased (0.622749 --> 0.622535).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.108170032501221
Epoch: 12, Steps: 61 | Train Loss: 0.8494655 Vali Loss: 0.6257114 Test Loss: 0.4204065
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.661067008972168
Epoch: 13, Steps: 61 | Train Loss: 0.8476198 Vali Loss: 0.6211205 Test Loss: 0.4203734
Validation loss decreased (0.622535 --> 0.621121).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 5.9128077030181885
Epoch: 14, Steps: 61 | Train Loss: 0.8486355 Vali Loss: 0.6240563 Test Loss: 0.4203254
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.413096189498901
Epoch: 15, Steps: 61 | Train Loss: 0.8480187 Vali Loss: 0.6250888 Test Loss: 0.4202740
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.8026793003082275
Epoch: 16, Steps: 61 | Train Loss: 0.8471654 Vali Loss: 0.6228663 Test Loss: 0.4202341
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.9037158489227295
Epoch: 17, Steps: 61 | Train Loss: 0.8468902 Vali Loss: 0.6249459 Test Loss: 0.4202321
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.821664810180664
Epoch: 18, Steps: 61 | Train Loss: 0.8477947 Vali Loss: 0.6232450 Test Loss: 0.4201891
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.554916143417358
Epoch: 19, Steps: 61 | Train Loss: 0.8479715 Vali Loss: 0.6234278 Test Loss: 0.4201444
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.0219337940216064
Epoch: 20, Steps: 61 | Train Loss: 0.8473197 Vali Loss: 0.6254539 Test Loss: 0.4201373
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 5.209888219833374
Epoch: 21, Steps: 61 | Train Loss: 0.8483115 Vali Loss: 0.6202495 Test Loss: 0.4200991
Validation loss decreased (0.621121 --> 0.620250).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.290916919708252
Epoch: 22, Steps: 61 | Train Loss: 0.8469501 Vali Loss: 0.6275651 Test Loss: 0.4200879
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.101525068283081
Epoch: 23, Steps: 61 | Train Loss: 0.8480725 Vali Loss: 0.6207059 Test Loss: 0.4200544
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.3325862884521484
Epoch: 24, Steps: 61 | Train Loss: 0.8470789 Vali Loss: 0.6246979 Test Loss: 0.4200287
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.1651172637939453
Epoch: 25, Steps: 61 | Train Loss: 0.8484019 Vali Loss: 0.6245161 Test Loss: 0.4200391
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.701754093170166
Epoch: 26, Steps: 61 | Train Loss: 0.8472566 Vali Loss: 0.6241597 Test Loss: 0.4200085
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8077809810638428
Epoch: 27, Steps: 61 | Train Loss: 0.8479847 Vali Loss: 0.6220458 Test Loss: 0.4199842
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.844843864440918
Epoch: 28, Steps: 61 | Train Loss: 0.8464370 Vali Loss: 0.6233172 Test Loss: 0.4199570
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.049360513687134
Epoch: 29, Steps: 61 | Train Loss: 0.8475546 Vali Loss: 0.6220048 Test Loss: 0.4199667
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9268534183502197
Epoch: 30, Steps: 61 | Train Loss: 0.8477377 Vali Loss: 0.6184421 Test Loss: 0.4199423
Validation loss decreased (0.620250 --> 0.618442).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.9056828022003174
Epoch: 31, Steps: 61 | Train Loss: 0.8473933 Vali Loss: 0.6240208 Test Loss: 0.4199493
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.9102509021759033
Epoch: 32, Steps: 61 | Train Loss: 0.8471790 Vali Loss: 0.6209651 Test Loss: 0.4199440
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.005458354949951
Epoch: 33, Steps: 61 | Train Loss: 0.8466089 Vali Loss: 0.6258794 Test Loss: 0.4199305
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9684948921203613
Epoch: 34, Steps: 61 | Train Loss: 0.8461583 Vali Loss: 0.6235191 Test Loss: 0.4199186
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.2920761108398438
Epoch: 35, Steps: 61 | Train Loss: 0.8464447 Vali Loss: 0.6206799 Test Loss: 0.4199141
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.4685921669006348
Epoch: 36, Steps: 61 | Train Loss: 0.8465858 Vali Loss: 0.6170146 Test Loss: 0.4198988
Validation loss decreased (0.618442 --> 0.617015).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.0477235317230225
Epoch: 37, Steps: 61 | Train Loss: 0.8466707 Vali Loss: 0.6258146 Test Loss: 0.4199029
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.387578010559082
Epoch: 38, Steps: 61 | Train Loss: 0.8468799 Vali Loss: 0.6219339 Test Loss: 0.4198912
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.6339869499206543
Epoch: 39, Steps: 61 | Train Loss: 0.8468878 Vali Loss: 0.6165791 Test Loss: 0.4198837
Validation loss decreased (0.617015 --> 0.616579).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.9883747100830078
Epoch: 40, Steps: 61 | Train Loss: 0.8466518 Vali Loss: 0.6270564 Test Loss: 0.4198853
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9627182483673096
Epoch: 41, Steps: 61 | Train Loss: 0.8442606 Vali Loss: 0.6216514 Test Loss: 0.4198763
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.8961782455444336
Epoch: 42, Steps: 61 | Train Loss: 0.8468449 Vali Loss: 0.6253273 Test Loss: 0.4198708
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.7465581893920898
Epoch: 43, Steps: 61 | Train Loss: 0.8460898 Vali Loss: 0.6255342 Test Loss: 0.4198740
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.5031087398529053
Epoch: 44, Steps: 61 | Train Loss: 0.8453844 Vali Loss: 0.6237888 Test Loss: 0.4198655
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9495677947998047
Epoch: 45, Steps: 61 | Train Loss: 0.8468630 Vali Loss: 0.6236446 Test Loss: 0.4198593
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.342369318008423
Epoch: 46, Steps: 61 | Train Loss: 0.8462713 Vali Loss: 0.6212857 Test Loss: 0.4198520
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.9804396629333496
Epoch: 47, Steps: 61 | Train Loss: 0.8445691 Vali Loss: 0.6255058 Test Loss: 0.4198466
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.1076736450195312
Epoch: 48, Steps: 61 | Train Loss: 0.8459280 Vali Loss: 0.6243377 Test Loss: 0.4198492
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.047045946121216
Epoch: 49, Steps: 61 | Train Loss: 0.8447866 Vali Loss: 0.6260694 Test Loss: 0.4198378
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.9894096851348877
Epoch: 50, Steps: 61 | Train Loss: 0.8452523 Vali Loss: 0.6237029 Test Loss: 0.4198352
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.956653118133545
Epoch: 51, Steps: 61 | Train Loss: 0.8464008 Vali Loss: 0.6254592 Test Loss: 0.4198350
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.263598918914795
Epoch: 52, Steps: 61 | Train Loss: 0.8461786 Vali Loss: 0.6222742 Test Loss: 0.4198302
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.122671365737915
Epoch: 53, Steps: 61 | Train Loss: 0.8464288 Vali Loss: 0.6191990 Test Loss: 0.4198346
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.893336296081543
Epoch: 54, Steps: 61 | Train Loss: 0.8454126 Vali Loss: 0.6218207 Test Loss: 0.4198301
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.9149582386016846
Epoch: 55, Steps: 61 | Train Loss: 0.8449488 Vali Loss: 0.6215774 Test Loss: 0.4198278
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.065722942352295
Epoch: 56, Steps: 61 | Train Loss: 0.8460636 Vali Loss: 0.6207224 Test Loss: 0.4198236
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.83469557762146
Epoch: 57, Steps: 61 | Train Loss: 0.8468460 Vali Loss: 0.6214204 Test Loss: 0.4198238
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.857694149017334
Epoch: 58, Steps: 61 | Train Loss: 0.8472155 Vali Loss: 0.6211637 Test Loss: 0.4198243
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.7017278671264648
Epoch: 59, Steps: 61 | Train Loss: 0.8469448 Vali Loss: 0.6239411 Test Loss: 0.4198218
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.41828978061676025, mae:0.436567097902298, rse:0.5169450044631958, corr:[ 0.21901841  0.2209346   0.21884155  0.21824613  0.21749648  0.2160214
  0.21550092  0.2149022   0.21398333  0.21261387  0.21178836  0.21081033
  0.20914084  0.20788665  0.20730478  0.20703033  0.20639041  0.20576635
  0.20536081  0.20460676  0.20361023  0.2027766   0.20199987  0.20044498
  0.19769588  0.19579911  0.1944848   0.1936057   0.1927908   0.19204636
  0.19184998  0.19092485  0.18981975  0.18890782  0.18841608  0.18750377
  0.18617214  0.18535553  0.18480287  0.18413511  0.1834811   0.18311018
  0.18307386  0.18244638  0.18181978  0.18139863  0.18069099  0.17875919
  0.17547289  0.17324644  0.17141016  0.17016657  0.16926882  0.16865034
  0.16842191  0.16706415  0.16622403  0.16567114  0.16562669  0.16511884
  0.16418275  0.16417117  0.164422    0.1645394   0.16476046  0.16516623
  0.16539314  0.16482668  0.16434038  0.16443942  0.16421849  0.16313076
  0.16108738  0.16044207  0.15997408  0.15929766  0.15878057  0.1591026
  0.15972203  0.15888646  0.15829353  0.15829682  0.15849641  0.15812129
  0.15745056  0.15764189  0.15810917  0.15805334  0.15774527  0.15779494
  0.1580172   0.15753299  0.15705279  0.15696463  0.15697618  0.15632483
  0.15443143  0.15341806  0.1527516   0.15215127  0.15127754  0.15053217
  0.15093455  0.15045881  0.15013328  0.14996742  0.15044989  0.15055427
  0.14983559  0.14944783  0.1493572   0.14937617  0.14908491  0.14867742
  0.14867271  0.1483954   0.14800303  0.14740622  0.14646704  0.14519817
  0.14305224  0.1416443   0.14034338  0.1396459   0.13883813  0.13822691
  0.13814391  0.13749166  0.13720724  0.13688241  0.1368792   0.13643791
  0.13560635  0.13501547  0.13452683  0.13409133  0.13372159  0.13325606
  0.13276263  0.13199288  0.13161176  0.13138056  0.13046098  0.12883735
  0.12595864  0.12396193  0.12245554  0.12174455  0.1213111   0.12068848
  0.12055421  0.11991514  0.11987957  0.11982986  0.11977531  0.1191622
  0.11836926  0.11815038  0.11789589  0.11769009  0.11762305  0.11766302
  0.11768414  0.1172012   0.11721984  0.11728285  0.11677429  0.11534028
  0.11283578  0.11156525  0.11058293  0.10998869  0.10976546  0.10998662
  0.11052729  0.11012375  0.11003699  0.10992282  0.11011935  0.10952352
  0.10871804  0.10847922  0.10844323  0.10854927  0.10838791  0.10851128
  0.10899647  0.1090744   0.10898032  0.10933154  0.10956572  0.10931786
  0.108006    0.10768812  0.10764409  0.10782378  0.1082569   0.10902314
  0.11072204  0.11132591  0.11170732  0.11185226  0.11228183  0.1120936
  0.1115426   0.11140426  0.1114919   0.11162574  0.11165296  0.11205269
  0.11251824  0.11238825  0.11229996  0.11222482  0.11221226  0.11141539
  0.10957936  0.10854524  0.10769217  0.10748079  0.10735605  0.10807648
  0.1096599   0.11068267  0.11116822  0.11130209  0.11171094  0.11165079
  0.11131502  0.11124837  0.11156972  0.11210554  0.11254168  0.11285532
  0.11324193  0.11352645  0.1137946   0.11415425  0.11427752  0.11407645
  0.11287521  0.11224046  0.11170945  0.11182662  0.11229803  0.11302765
  0.11450465  0.11498521  0.11558173  0.11590964  0.11647934  0.11662195
  0.11647303  0.11680441  0.11689085  0.1170743   0.11723723  0.11783523
  0.11824829  0.11828508  0.11840243  0.11878007  0.11906417  0.11886329
  0.11771642  0.11764643  0.11793572  0.11838811  0.11879599  0.12005087
  0.12191764  0.12265947  0.12333513  0.12362683  0.12400611  0.12356872
  0.12318089  0.12318875  0.12359549  0.12385612  0.12395082  0.12414865
  0.12460907  0.12472024  0.12482294  0.1251307   0.1253655   0.1252274
  0.124477    0.12414598  0.12376449  0.12355437  0.12365481  0.1239576
  0.12452509  0.12454551  0.12506923  0.1255648   0.12584868  0.12553072
  0.12513745  0.12534563  0.1256649   0.12587492  0.12588118  0.12626234
  0.12673782  0.12649722  0.1263987   0.12647413  0.12649071  0.12621854
  0.12497668  0.12460577  0.12461952  0.12470683  0.12462737  0.12505479
  0.12612288  0.1263785   0.12673973  0.1271144   0.12760222  0.1274824
  0.12725708  0.12741041  0.12771462  0.12788935  0.12800843  0.12840046
  0.12893638  0.12904476  0.12905513  0.12947954  0.12994374  0.1301286
  0.12904477  0.12878604  0.12897025  0.12916404  0.12978226  0.13086706
  0.13272938  0.13338755  0.1336203   0.13396126  0.1346865   0.13481805
  0.13429128  0.13419671  0.13475761  0.13507043  0.13501315  0.13528816
  0.1362225   0.13676932  0.13679023  0.1371622   0.13775145  0.13805196
  0.13751446  0.1374871   0.13766968  0.13804917  0.13882563  0.14043342
  0.14297754  0.14479762  0.14601853  0.14668883  0.14747265  0.14810845
  0.14844048  0.14891662  0.14957306  0.15027037  0.15078585  0.15160325
  0.15241893  0.15301476  0.15347089  0.15407197  0.15468858  0.15518753
  0.15512452  0.15544133  0.15597568  0.156569    0.15754837  0.15908337
  0.1615286   0.16294596  0.16393392  0.16463329  0.165418    0.16576691
  0.1657651   0.16596149  0.16606607  0.166121    0.16611113  0.16612455
  0.16643357  0.16658494  0.16671258  0.16683666  0.16676244  0.16669855
  0.16591266  0.1658911   0.16600296  0.16614532  0.16647913  0.16726053
  0.16862734  0.16926599  0.16970111  0.1700763   0.17035227  0.17018148
  0.16978253  0.16965875  0.16963896  0.16952555  0.16932064  0.16927381
  0.16939254  0.16933475  0.16931036  0.16954827  0.16964367  0.16945757
  0.16890499  0.16873498  0.16862407  0.1684832   0.16843748  0.16865845
  0.1696868   0.16981104  0.16981801  0.16982365  0.16990069  0.16971344
  0.16932058  0.16922964  0.169139    0.16907991  0.16898683  0.16903819
  0.16913772  0.16906485  0.168974    0.16903958  0.1691183   0.16898115
  0.1682732   0.16785675  0.16731676  0.1671617   0.1671744   0.16725521
  0.16760297  0.16740747  0.16730541  0.16726622  0.16729018  0.16679111
  0.16618595  0.1659702   0.16573916  0.16552614  0.16523685  0.16496396
  0.16477834  0.16447473  0.16424376  0.1642263   0.16393776  0.16319887
  0.16170189  0.16068317  0.15990172  0.15901801  0.15828109  0.15763909
  0.15743028  0.15664215  0.1560499   0.15555473  0.15504006  0.15426758
  0.15350138  0.1531595   0.15284951  0.15251486  0.15221441  0.15195149
  0.1517072   0.15131615  0.15117256  0.15111674  0.15044478  0.14894615
  0.14681292  0.14525475  0.14388093  0.14264327  0.14174587  0.14124312
  0.14089118  0.14028788  0.14011148  0.13999394  0.13966493  0.13875917
  0.13788542  0.13742103  0.13710485  0.13670383  0.13632868  0.13592376
  0.13522398  0.13440496  0.13395533  0.13400804  0.13332751  0.13151029
  0.12869467  0.1265346   0.12464248  0.12257244  0.1208503   0.11945855
  0.11869697  0.11751815  0.11703285  0.11687733  0.11665162  0.11581084
  0.11472543  0.11418576  0.11351576  0.11263336  0.1117115   0.11113274
  0.11044808  0.10974885  0.10917979  0.10849427  0.10707613  0.10473047
  0.10156404  0.09966023  0.09766388  0.09576841  0.09439193  0.09382593
  0.09364698  0.09246483  0.09204447  0.09193315  0.09184756  0.09109414
  0.09000395  0.08944252  0.08894239  0.0881153   0.08752618  0.08729795
  0.08679514  0.08568437  0.08463509  0.08387828  0.08251842  0.08015
  0.07706197  0.07521933  0.07377225  0.0720512   0.07063193  0.06980301
  0.06958301  0.06878386  0.06838737  0.0683397   0.06813497  0.0672579
  0.06610861  0.06558081  0.06528031  0.06479343  0.06433689  0.06427746
  0.06437485  0.06398993  0.06325848  0.06260035  0.06155209  0.05956404
  0.05656921  0.05467262  0.05323613  0.05192095  0.05053988  0.0496112
  0.04931517  0.04852115  0.04809109  0.04799832  0.0481192   0.04757158
  0.04672578  0.04638026  0.0461928   0.04593707  0.04552552  0.04530049
  0.04519589  0.04481473  0.04432839  0.04407421  0.0431911   0.04144908
  0.03844542  0.03626047  0.03456225  0.03320321  0.03185948  0.03089548
  0.03062988  0.02964763  0.02921653  0.02916112  0.0290012   0.02826975
  0.02722383  0.02652219  0.0259948   0.0253526   0.02436749  0.02346909
  0.02291754  0.02222032  0.02183603  0.02155328  0.02087568  0.01909626
  0.01618322  0.01385675  0.01212698  0.0109597   0.00968454  0.00838406
  0.00771605  0.00696742  0.00668562  0.00662464  0.00676181  0.00616393
  0.00512892  0.00460496  0.00463317  0.00489262  0.00472301  0.00408379
  0.00366005  0.00344069  0.0034216   0.00350066  0.00301211  0.00163225
 -0.00095884 -0.00345522 -0.00554185 -0.00695179 -0.00820723 -0.00950938
 -0.00982314 -0.00991224 -0.00936266 -0.00916573 -0.00921216 -0.00957852
 -0.0100683  -0.01073875 -0.01137605 -0.01139438 -0.01093787 -0.01088514
 -0.01123544 -0.01166638 -0.01133423 -0.0110689  -0.01236927 -0.01419049
 -0.01669953 -0.01870252 -0.02127162 -0.02268362 -0.02262896 -0.02271906
 -0.02285935 -0.02334814 -0.02243664 -0.02176044 -0.02169876 -0.02285526
 -0.02359107 -0.02381465 -0.02425909 -0.02489493 -0.02534936 -0.02539645
 -0.02535271 -0.02503813 -0.02429642 -0.02478558 -0.02558073 -0.02130979]
