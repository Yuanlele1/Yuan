Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.550151824951172
Epoch: 1, Steps: 56 | Train Loss: 1.0507083 Vali Loss: 0.7889829 Test Loss: 0.4428128
Validation loss decreased (inf --> 0.788983).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.3766751289367676
Epoch: 2, Steps: 56 | Train Loss: 0.9032679 Vali Loss: 0.7398914 Test Loss: 0.4121087
Validation loss decreased (0.788983 --> 0.739891).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.502453327178955
Epoch: 3, Steps: 56 | Train Loss: 0.8651483 Vali Loss: 0.7130888 Test Loss: 0.4013231
Validation loss decreased (0.739891 --> 0.713089).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.606855630874634
Epoch: 4, Steps: 56 | Train Loss: 0.8496848 Vali Loss: 0.6962374 Test Loss: 0.3954251
Validation loss decreased (0.713089 --> 0.696237).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.4289231300354004
Epoch: 5, Steps: 56 | Train Loss: 0.8396135 Vali Loss: 0.6912364 Test Loss: 0.3915667
Validation loss decreased (0.696237 --> 0.691236).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.557281970977783
Epoch: 6, Steps: 56 | Train Loss: 0.8316635 Vali Loss: 0.6833876 Test Loss: 0.3888880
Validation loss decreased (0.691236 --> 0.683388).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.5187976360321045
Epoch: 7, Steps: 56 | Train Loss: 0.8274220 Vali Loss: 0.6753527 Test Loss: 0.3868196
Validation loss decreased (0.683388 --> 0.675353).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.378479480743408
Epoch: 8, Steps: 56 | Train Loss: 0.8242779 Vali Loss: 0.6795006 Test Loss: 0.3853408
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.5458106994628906
Epoch: 9, Steps: 56 | Train Loss: 0.8207898 Vali Loss: 0.6712816 Test Loss: 0.3841309
Validation loss decreased (0.675353 --> 0.671282).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.322617769241333
Epoch: 10, Steps: 56 | Train Loss: 0.8189109 Vali Loss: 0.6722821 Test Loss: 0.3832198
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.4979307651519775
Epoch: 11, Steps: 56 | Train Loss: 0.8149001 Vali Loss: 0.6636753 Test Loss: 0.3825016
Validation loss decreased (0.671282 --> 0.663675).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.3209731578826904
Epoch: 12, Steps: 56 | Train Loss: 0.8142675 Vali Loss: 0.6645970 Test Loss: 0.3820488
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.4552648067474365
Epoch: 13, Steps: 56 | Train Loss: 0.8145917 Vali Loss: 0.6617532 Test Loss: 0.3815167
Validation loss decreased (0.663675 --> 0.661753).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.73152756690979
Epoch: 14, Steps: 56 | Train Loss: 0.8137840 Vali Loss: 0.6621445 Test Loss: 0.3811731
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.3785204887390137
Epoch: 15, Steps: 56 | Train Loss: 0.8106997 Vali Loss: 0.6604937 Test Loss: 0.3808829
Validation loss decreased (0.661753 --> 0.660494).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.433969736099243
Epoch: 16, Steps: 56 | Train Loss: 0.8100075 Vali Loss: 0.6571509 Test Loss: 0.3806454
Validation loss decreased (0.660494 --> 0.657151).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.3951706886291504
Epoch: 17, Steps: 56 | Train Loss: 0.8086444 Vali Loss: 0.6626654 Test Loss: 0.3804498
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.9334821701049805
Epoch: 18, Steps: 56 | Train Loss: 0.8105663 Vali Loss: 0.6580794 Test Loss: 0.3803009
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.221816062927246
Epoch: 19, Steps: 56 | Train Loss: 0.8103359 Vali Loss: 0.6537624 Test Loss: 0.3801441
Validation loss decreased (0.657151 --> 0.653762).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.59559965133667
Epoch: 20, Steps: 56 | Train Loss: 0.8085436 Vali Loss: 0.6565582 Test Loss: 0.3800673
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.4681766033172607
Epoch: 21, Steps: 56 | Train Loss: 0.8086355 Vali Loss: 0.6539604 Test Loss: 0.3799886
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.382652759552002
Epoch: 22, Steps: 56 | Train Loss: 0.8083783 Vali Loss: 0.6564468 Test Loss: 0.3798692
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.392399549484253
Epoch: 23, Steps: 56 | Train Loss: 0.8072195 Vali Loss: 0.6548283 Test Loss: 0.3798112
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.0288422107696533
Epoch: 24, Steps: 56 | Train Loss: 0.8080435 Vali Loss: 0.6547967 Test Loss: 0.3798091
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.390662670135498
Epoch: 25, Steps: 56 | Train Loss: 0.8064944 Vali Loss: 0.6560757 Test Loss: 0.3797294
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.4165127277374268
Epoch: 26, Steps: 56 | Train Loss: 0.8062622 Vali Loss: 0.6543555 Test Loss: 0.3796875
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.396146297454834
Epoch: 27, Steps: 56 | Train Loss: 0.8061520 Vali Loss: 0.6544548 Test Loss: 0.3797086
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.349700689315796
Epoch: 28, Steps: 56 | Train Loss: 0.8043738 Vali Loss: 0.6552758 Test Loss: 0.3796809
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.433065414428711
Epoch: 29, Steps: 56 | Train Loss: 0.8046527 Vali Loss: 0.6525682 Test Loss: 0.3796276
Validation loss decreased (0.653762 --> 0.652568).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.7651398181915283
Epoch: 30, Steps: 56 | Train Loss: 0.8061169 Vali Loss: 0.6533605 Test Loss: 0.3796305
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.216508388519287
Epoch: 31, Steps: 56 | Train Loss: 0.8045547 Vali Loss: 0.6506802 Test Loss: 0.3796393
Validation loss decreased (0.652568 --> 0.650680).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.328688383102417
Epoch: 32, Steps: 56 | Train Loss: 0.8037927 Vali Loss: 0.6528777 Test Loss: 0.3796158
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.344935894012451
Epoch: 33, Steps: 56 | Train Loss: 0.8051273 Vali Loss: 0.6524485 Test Loss: 0.3796057
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.465027093887329
Epoch: 34, Steps: 56 | Train Loss: 0.8047621 Vali Loss: 0.6486119 Test Loss: 0.3795883
Validation loss decreased (0.650680 --> 0.648612).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.9918692111968994
Epoch: 35, Steps: 56 | Train Loss: 0.8053320 Vali Loss: 0.6504763 Test Loss: 0.3795830
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.470250129699707
Epoch: 36, Steps: 56 | Train Loss: 0.8035083 Vali Loss: 0.6537775 Test Loss: 0.3795924
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.2015035152435303
Epoch: 37, Steps: 56 | Train Loss: 0.8052027 Vali Loss: 0.6519792 Test Loss: 0.3795950
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.4286205768585205
Epoch: 38, Steps: 56 | Train Loss: 0.8046025 Vali Loss: 0.6492628 Test Loss: 0.3795663
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.5365793704986572
Epoch: 39, Steps: 56 | Train Loss: 0.8046583 Vali Loss: 0.6496036 Test Loss: 0.3795851
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.499403238296509
Epoch: 40, Steps: 56 | Train Loss: 0.8029675 Vali Loss: 0.6509922 Test Loss: 0.3795746
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.5279412269592285
Epoch: 41, Steps: 56 | Train Loss: 0.8025107 Vali Loss: 0.6524621 Test Loss: 0.3795910
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.2549238204956055
Epoch: 42, Steps: 56 | Train Loss: 0.8037394 Vali Loss: 0.6554714 Test Loss: 0.3795789
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.4895925521850586
Epoch: 43, Steps: 56 | Train Loss: 0.8044455 Vali Loss: 0.6517789 Test Loss: 0.3795893
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.3478777408599854
Epoch: 44, Steps: 56 | Train Loss: 0.8027120 Vali Loss: 0.6518670 Test Loss: 0.3795869
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.4640932083129883
Epoch: 45, Steps: 56 | Train Loss: 0.8026818 Vali Loss: 0.6475923 Test Loss: 0.3795776
Validation loss decreased (0.648612 --> 0.647592).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.1653892993927
Epoch: 46, Steps: 56 | Train Loss: 0.8040676 Vali Loss: 0.6519084 Test Loss: 0.3795793
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.160064697265625
Epoch: 47, Steps: 56 | Train Loss: 0.8012759 Vali Loss: 0.6464657 Test Loss: 0.3795879
Validation loss decreased (0.647592 --> 0.646466).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.594453811645508
Epoch: 48, Steps: 56 | Train Loss: 0.8007568 Vali Loss: 0.6490858 Test Loss: 0.3795956
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.7009775638580322
Epoch: 49, Steps: 56 | Train Loss: 0.8045948 Vali Loss: 0.6518506 Test Loss: 0.3795941
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.509471893310547
Epoch: 50, Steps: 56 | Train Loss: 0.8027300 Vali Loss: 0.6462060 Test Loss: 0.3795866
Validation loss decreased (0.646466 --> 0.646206).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.725102663040161
Epoch: 51, Steps: 56 | Train Loss: 0.8028384 Vali Loss: 0.6496154 Test Loss: 0.3795964
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.8061771392822266
Epoch: 52, Steps: 56 | Train Loss: 0.8034002 Vali Loss: 0.6533570 Test Loss: 0.3796018
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.5827713012695312
Epoch: 53, Steps: 56 | Train Loss: 0.8031378 Vali Loss: 0.6479749 Test Loss: 0.3796004
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.712364435195923
Epoch: 54, Steps: 56 | Train Loss: 0.8015535 Vali Loss: 0.6502286 Test Loss: 0.3795986
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.684354543685913
Epoch: 55, Steps: 56 | Train Loss: 0.8023724 Vali Loss: 0.6490066 Test Loss: 0.3795940
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.3614296913146973
Epoch: 56, Steps: 56 | Train Loss: 0.8024701 Vali Loss: 0.6509161 Test Loss: 0.3796043
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.6359469890594482
Epoch: 57, Steps: 56 | Train Loss: 0.8037578 Vali Loss: 0.6460218 Test Loss: 0.3796016
Validation loss decreased (0.646206 --> 0.646022).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.375425338745117
Epoch: 58, Steps: 56 | Train Loss: 0.8026427 Vali Loss: 0.6499755 Test Loss: 0.3796035
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.74458646774292
Epoch: 59, Steps: 56 | Train Loss: 0.8027756 Vali Loss: 0.6484950 Test Loss: 0.3796119
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.3658902645111084
Epoch: 60, Steps: 56 | Train Loss: 0.8037063 Vali Loss: 0.6502348 Test Loss: 0.3796132
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.6487433910369873
Epoch: 61, Steps: 56 | Train Loss: 0.8022542 Vali Loss: 0.6452261 Test Loss: 0.3796090
Validation loss decreased (0.646022 --> 0.645226).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.192887306213379
Epoch: 62, Steps: 56 | Train Loss: 0.8020120 Vali Loss: 0.6502089 Test Loss: 0.3796170
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.640941619873047
Epoch: 63, Steps: 56 | Train Loss: 0.8031251 Vali Loss: 0.6459225 Test Loss: 0.3796131
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.733991861343384
Epoch: 64, Steps: 56 | Train Loss: 0.8026207 Vali Loss: 0.6489265 Test Loss: 0.3796176
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.296583652496338
Epoch: 65, Steps: 56 | Train Loss: 0.8031479 Vali Loss: 0.6456575 Test Loss: 0.3796182
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.4373624324798584
Epoch: 66, Steps: 56 | Train Loss: 0.8026840 Vali Loss: 0.6496099 Test Loss: 0.3796251
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.3542330265045166
Epoch: 67, Steps: 56 | Train Loss: 0.8022796 Vali Loss: 0.6485667 Test Loss: 0.3796208
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.565912961959839
Epoch: 68, Steps: 56 | Train Loss: 0.8017474 Vali Loss: 0.6490836 Test Loss: 0.3796270
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.4136240482330322
Epoch: 69, Steps: 56 | Train Loss: 0.8024376 Vali Loss: 0.6491137 Test Loss: 0.3796282
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.4505763053894043
Epoch: 70, Steps: 56 | Train Loss: 0.8028662 Vali Loss: 0.6511123 Test Loss: 0.3796246
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.826704502105713
Epoch: 71, Steps: 56 | Train Loss: 0.8022665 Vali Loss: 0.6490220 Test Loss: 0.3796244
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.6470022201538086
Epoch: 72, Steps: 56 | Train Loss: 0.8014118 Vali Loss: 0.6514786 Test Loss: 0.3796245
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.7172293663024902
Epoch: 73, Steps: 56 | Train Loss: 0.8027457 Vali Loss: 0.6503167 Test Loss: 0.3796272
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.190031051635742
Epoch: 74, Steps: 56 | Train Loss: 0.8013259 Vali Loss: 0.6474413 Test Loss: 0.3796272
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.2785418033599854
Epoch: 75, Steps: 56 | Train Loss: 0.8029013 Vali Loss: 0.6488824 Test Loss: 0.3796274
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.55131196975708
Epoch: 76, Steps: 56 | Train Loss: 0.8021276 Vali Loss: 0.6499150 Test Loss: 0.3796218
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.4880940914154053
Epoch: 77, Steps: 56 | Train Loss: 0.8011484 Vali Loss: 0.6465215 Test Loss: 0.3796249
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.557293653488159
Epoch: 78, Steps: 56 | Train Loss: 0.8038983 Vali Loss: 0.6476145 Test Loss: 0.3796262
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.2293429374694824
Epoch: 79, Steps: 56 | Train Loss: 0.8009003 Vali Loss: 0.6489439 Test Loss: 0.3796280
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.301539659500122
Epoch: 80, Steps: 56 | Train Loss: 0.8032547 Vali Loss: 0.6511611 Test Loss: 0.3796274
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.2939963340759277
Epoch: 81, Steps: 56 | Train Loss: 0.8009750 Vali Loss: 0.6468455 Test Loss: 0.3796303
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3782094120979309, mae:0.4232763648033142, rse:0.49155473709106445, corr:[ 0.21514395  0.22039     0.21926045  0.21733172  0.21716878  0.21784459
  0.21746966  0.21575226  0.21409896  0.21301992  0.21232116  0.21129912
  0.2099873   0.2088386   0.20804256  0.20749423  0.20682028  0.20576616
  0.20479004  0.20413844  0.20354395  0.20270382  0.20134896  0.19986315
  0.19860882  0.19768842  0.19676611  0.19574614  0.19480255  0.19395801
  0.19322109  0.19248499  0.19175114  0.19092676  0.19008707  0.18913692
  0.18811712  0.18731637  0.18683168  0.1865231   0.1860983   0.18532348
  0.18436864  0.18369327  0.1832446   0.18280832  0.18190366  0.18028295
  0.1785367   0.17720334  0.17635812  0.17574754  0.17515904  0.17444298
  0.17356947  0.17278218  0.1721895   0.17176655  0.17144962  0.171205
  0.17102109  0.17081727  0.17078938  0.17100233  0.17117758  0.17138055
  0.17140831  0.17128953  0.17108437  0.17096911  0.17084236  0.17055424
  0.17011195  0.16961837  0.16925752  0.16895773  0.16875553  0.16855325
  0.16827883  0.16814627  0.16822755  0.16835515  0.16831216  0.1681445
  0.16804896  0.16815338  0.16841131  0.16865754  0.16867246  0.16849989
  0.1682872   0.16822924  0.16846713  0.16873989  0.16887432  0.16874288
  0.1685016   0.16830763  0.16810484  0.16786924  0.16768128  0.16741607
  0.16724224  0.16711366  0.16707523  0.16700603  0.16699497  0.16701071
  0.16691972  0.16670352  0.16647148  0.16626705  0.16613542  0.16617313
  0.1662306   0.16616797  0.16584526  0.16524336  0.16465275  0.16406797
  0.16342868  0.16277906  0.16208164  0.16140074  0.16074511  0.16025715
  0.15982057  0.15943387  0.1591099   0.15869185  0.15827619  0.15766062
  0.15690017  0.15612945  0.15565488  0.15552695  0.1552455   0.15475735
  0.1540808   0.15362482  0.1535966   0.15365997  0.15330878  0.15226732
  0.15074751  0.14957395  0.14903736  0.1488547   0.14855257  0.1479888
  0.14736448  0.14693986  0.14680566  0.14664099  0.14619511  0.14545913
  0.14480199  0.14447273  0.14430848  0.1440004   0.14350574  0.14329976
  0.14355902  0.14408611  0.14455158  0.14455947  0.14415175  0.14347595
  0.142977    0.14268096  0.14245161  0.14208831  0.14164108  0.14117943
  0.14066634  0.14008981  0.13948078  0.1388747   0.13819395  0.13764156
  0.13727713  0.13709524  0.13713323  0.13722238  0.13735454  0.13749264
  0.13752455  0.13762598  0.13797592  0.13866846  0.1395353   0.13996278
  0.1397865   0.13947809  0.13945974  0.13987748  0.14040121  0.14049038
  0.14024995  0.13984816  0.13963361  0.13964163  0.1397423   0.13948046
  0.13905367  0.13890532  0.13910413  0.13949819  0.13990258  0.14016332
  0.14034005  0.1406267   0.14101128  0.14126153  0.14120354  0.14086431
  0.14050314  0.14019738  0.13989244  0.1396299   0.13935363  0.13936183
  0.13949153  0.1397719   0.13984703  0.13969561  0.13955544  0.1394955
  0.13962343  0.13979335  0.14007898  0.1405886   0.14126825  0.1419585
  0.14247856  0.14280745  0.14316487  0.14371122  0.14440873  0.14499494
  0.14530039  0.14534844  0.14544924  0.1458311   0.14623782  0.14638096
  0.14642556  0.14650992  0.14687848  0.14749977  0.14809081  0.14854431
  0.14893113  0.14950003  0.15025426  0.15119776  0.15194929  0.15250337
  0.15303284  0.15368485  0.15431911  0.15488954  0.15544106  0.155926
  0.15650018  0.15703957  0.15746897  0.15770027  0.15771113  0.15778226
  0.15805662  0.15846848  0.15892583  0.1593806   0.15980294  0.16017123
  0.16037151  0.1607485   0.16132541  0.1621031   0.16293085  0.16350162
  0.16361171  0.16351931  0.16355504  0.16399468  0.16480106  0.16542628
  0.16560727  0.16545099  0.16518854  0.16514288  0.16514105  0.1651134
  0.1651006   0.16518724  0.16537498  0.16565175  0.16589929  0.1661345
  0.16625026  0.16656952  0.1671066   0.1676249   0.16790561  0.16807768
  0.16812068  0.16835131  0.16872033  0.16908701  0.16935658  0.1693714
  0.16935477  0.16923971  0.16912502  0.16884416  0.16848816  0.16820055
  0.16803832  0.16799562  0.16789225  0.167832    0.1677744   0.16775128
  0.16767749  0.16768512  0.16816974  0.16880275  0.169334    0.16955468
  0.16951792  0.16969523  0.17016993  0.17086765  0.17156751  0.17188083
  0.1717291   0.17149019  0.1715362   0.17176883  0.17209129  0.17221595
  0.1722293   0.17219284  0.17217052  0.17223713  0.17217684  0.17233618
  0.17256983  0.1729407   0.17349336  0.17414458  0.1747733   0.17535368
  0.17579865  0.17623933  0.17661177  0.176984    0.17734598  0.17772388
  0.1779923   0.17805505  0.17803624  0.17809153  0.1783571   0.17882656
  0.17937702  0.17980938  0.1799016   0.17996491  0.17988004  0.17976475
  0.17959924  0.17954929  0.17971452  0.18001531  0.18024795  0.18019609
  0.17988193  0.17968631  0.17979574  0.18001974  0.18011454  0.18003806
  0.17985031  0.17974472  0.17990987  0.18028021  0.18064329  0.18090278
  0.1811615   0.18151689  0.18184932  0.1820227   0.18212077  0.18219487
  0.18241684  0.18238859  0.1822142   0.18187407  0.18132631  0.18089868
  0.18073568  0.18077001  0.18077943  0.1806803   0.18048555  0.18038331
  0.18020365  0.17996092  0.17960325  0.17928709  0.1792132   0.1792801
  0.1792784   0.17918254  0.17904224  0.17896117  0.1789578   0.17892282
  0.17848854  0.17775594  0.17694189  0.17617066  0.1753273   0.17422107
  0.17312847  0.17217274  0.17155185  0.17118809  0.17066753  0.16998117
  0.16918409  0.16859105  0.16840602  0.16822113  0.16774242  0.1670174
  0.16635051  0.16606796  0.16580285  0.16546763  0.16483615  0.16416349
  0.16362432  0.1635129   0.16342899  0.16317937  0.16262539  0.16204222
  0.16168793  0.16167627  0.16149603  0.1611827   0.16092886  0.16089723
  0.1610227   0.16090843  0.16045122  0.1600742   0.15985397  0.15967119
  0.15939082  0.15906183  0.15858251  0.15835243  0.15828605  0.15794817
  0.1573461   0.15655947  0.15612383  0.15606043  0.1561308   0.15589105
  0.15536489  0.15475295  0.1544185   0.1545368   0.15468474  0.15443635
  0.15370736  0.1528823   0.1523319   0.1517601   0.15109627  0.15032122
  0.14958681  0.14891069  0.14836068  0.14771411  0.14696786  0.146093
  0.14535868  0.14493325  0.14469093  0.14429006  0.14345986  0.14270417
  0.14231361  0.14219241  0.14205663  0.1416012   0.14092337  0.1400442
  0.13908274  0.1381931   0.13734609  0.13647051  0.13577127  0.13542007
  0.13500305  0.13438958  0.13359764  0.13287939  0.13246039  0.13197695
  0.13119926  0.13011017  0.12920055  0.12871765  0.12862878  0.12858823
  0.12801689  0.12708671  0.12625597  0.12584443  0.12547289  0.12461697
  0.12308334  0.12148442  0.12038334  0.11944911  0.11849511  0.11733789
  0.11619005  0.11528882  0.11461936  0.11393601  0.11299376  0.11173338
  0.11048226  0.10950609  0.10889796  0.10823952  0.1071226   0.10594407
  0.10497548  0.10461177  0.10459693  0.10414306  0.10315385  0.10172357
  0.10042915  0.09935216  0.09841923  0.0974318   0.09641927  0.09551522
  0.09468307  0.09360789  0.09271623  0.09203395  0.09188826  0.09194565
  0.09181429  0.09116548  0.09019032  0.08924963  0.08892112  0.0889001
  0.08850995  0.08751632  0.08624272  0.08525569  0.08448952  0.08354561
  0.08206099  0.08050095  0.07918461  0.07830152  0.07764166  0.07675263
  0.07552518  0.07426493  0.07336494  0.07283539  0.07225299  0.07146859
  0.07041657  0.06960554  0.06929305  0.0691658   0.06883947  0.06819443
  0.06746945  0.06720854  0.06722096  0.066992    0.06597133  0.0644264
  0.06295612  0.06200648  0.06112932  0.06018923  0.05882866  0.05743403
  0.05606915  0.05479549  0.05355386  0.05266636  0.05229845  0.05192124
  0.05135867  0.05052117  0.04971237  0.04929541  0.04941695  0.05014033
  0.05065257  0.05057091  0.04999595  0.04966174  0.04957655  0.04964776
  0.04908371  0.04765422  0.04596496  0.04482011  0.04440447  0.04434197
  0.04388648  0.04297977  0.04229213  0.0422857   0.04238157  0.04202605
  0.04099175  0.03984909  0.03955202  0.04001549  0.03992158  0.03907791
  0.0379246   0.03716801  0.03742017  0.03821226  0.03809924  0.03675925
  0.03497329  0.03385049  0.03359517  0.03333292  0.0326398   0.0313339
  0.02994107  0.02905229  0.02877033  0.02883636  0.028991    0.02907793
  0.02885051  0.02841554  0.02814551  0.02803112  0.02836033  0.02900936
  0.02956833  0.02962759  0.02898325  0.02850495  0.02858492  0.02895864
  0.02884187  0.02797691  0.02673235  0.02532018  0.02451385  0.02384583
  0.02343857  0.02271232  0.02246492  0.02270629  0.02274735  0.02203547
  0.02105717  0.02015868  0.02041065  0.02132594  0.02177825  0.02134611
  0.02065082  0.02013512  0.0204621   0.02076796  0.02005048  0.01787337
  0.01518783  0.01309054  0.0123048   0.01185159  0.01116331  0.00965767
  0.00797064  0.00621827  0.00532385  0.00563495  0.00627846  0.00658996
  0.00566432  0.00315819  0.00145785  0.0019147   0.00438505  0.00646602
  0.0047036  -0.00026417 -0.00361699 -0.00083292  0.00389711 -0.00096646]
