Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=103, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7825
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=103, out_features=116, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  10705408.0
params:  12064.0
Trainable parameters:  12064
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.6618998050689697
Epoch: 1, Steps: 61 | Train Loss: 0.6181310 Vali Loss: 0.2982132 Test Loss: 0.3110385
Validation loss decreased (inf --> 0.298213).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.7031588554382324
Epoch: 2, Steps: 61 | Train Loss: 0.4855803 Vali Loss: 0.2640617 Test Loss: 0.2898528
Validation loss decreased (0.298213 --> 0.264062).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.664879560470581
Epoch: 3, Steps: 61 | Train Loss: 0.4587764 Vali Loss: 0.2512414 Test Loss: 0.2844198
Validation loss decreased (0.264062 --> 0.251241).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.478299140930176
Epoch: 4, Steps: 61 | Train Loss: 0.4458018 Vali Loss: 0.2425596 Test Loss: 0.2820580
Validation loss decreased (0.251241 --> 0.242560).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.3020482063293457
Epoch: 5, Steps: 61 | Train Loss: 0.4373064 Vali Loss: 0.2379199 Test Loss: 0.2802685
Validation loss decreased (0.242560 --> 0.237920).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.0023210048675537
Epoch: 6, Steps: 61 | Train Loss: 0.4317769 Vali Loss: 0.2347335 Test Loss: 0.2790659
Validation loss decreased (0.237920 --> 0.234733).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.319462537765503
Epoch: 7, Steps: 61 | Train Loss: 0.4279186 Vali Loss: 0.2321071 Test Loss: 0.2784810
Validation loss decreased (0.234733 --> 0.232107).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.0051472187042236
Epoch: 8, Steps: 61 | Train Loss: 0.4246421 Vali Loss: 0.2298291 Test Loss: 0.2779473
Validation loss decreased (0.232107 --> 0.229829).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.8869729042053223
Epoch: 9, Steps: 61 | Train Loss: 0.4215250 Vali Loss: 0.2275249 Test Loss: 0.2775213
Validation loss decreased (0.229829 --> 0.227525).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.9260103702545166
Epoch: 10, Steps: 61 | Train Loss: 0.4200078 Vali Loss: 0.2258033 Test Loss: 0.2772267
Validation loss decreased (0.227525 --> 0.225803).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.3067140579223633
Epoch: 11, Steps: 61 | Train Loss: 0.4185287 Vali Loss: 0.2251415 Test Loss: 0.2769732
Validation loss decreased (0.225803 --> 0.225141).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.3889284133911133
Epoch: 12, Steps: 61 | Train Loss: 0.4173242 Vali Loss: 0.2245688 Test Loss: 0.2765400
Validation loss decreased (0.225141 --> 0.224569).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.3556227684020996
Epoch: 13, Steps: 61 | Train Loss: 0.4157373 Vali Loss: 0.2240207 Test Loss: 0.2762092
Validation loss decreased (0.224569 --> 0.224021).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.3149988651275635
Epoch: 14, Steps: 61 | Train Loss: 0.4146423 Vali Loss: 0.2232788 Test Loss: 0.2761976
Validation loss decreased (0.224021 --> 0.223279).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.137868642807007
Epoch: 15, Steps: 61 | Train Loss: 0.4143390 Vali Loss: 0.2221393 Test Loss: 0.2760651
Validation loss decreased (0.223279 --> 0.222139).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.3807079792022705
Epoch: 16, Steps: 61 | Train Loss: 0.4133184 Vali Loss: 0.2214622 Test Loss: 0.2759348
Validation loss decreased (0.222139 --> 0.221462).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.4295032024383545
Epoch: 17, Steps: 61 | Train Loss: 0.4131855 Vali Loss: 0.2211591 Test Loss: 0.2758385
Validation loss decreased (0.221462 --> 0.221159).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.300239324569702
Epoch: 18, Steps: 61 | Train Loss: 0.4119627 Vali Loss: 0.2206032 Test Loss: 0.2757540
Validation loss decreased (0.221159 --> 0.220603).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.0290234088897705
Epoch: 19, Steps: 61 | Train Loss: 0.4119963 Vali Loss: 0.2208657 Test Loss: 0.2756368
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.742922067642212
Epoch: 20, Steps: 61 | Train Loss: 0.4115103 Vali Loss: 0.2204770 Test Loss: 0.2754115
Validation loss decreased (0.220603 --> 0.220477).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.8067283630371094
Epoch: 21, Steps: 61 | Train Loss: 0.4110341 Vali Loss: 0.2202941 Test Loss: 0.2753711
Validation loss decreased (0.220477 --> 0.220294).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.0995867252349854
Epoch: 22, Steps: 61 | Train Loss: 0.4108474 Vali Loss: 0.2190563 Test Loss: 0.2754558
Validation loss decreased (0.220294 --> 0.219056).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.6423392295837402
Epoch: 23, Steps: 61 | Train Loss: 0.4103900 Vali Loss: 0.2190078 Test Loss: 0.2752786
Validation loss decreased (0.219056 --> 0.219008).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.313920497894287
Epoch: 24, Steps: 61 | Train Loss: 0.4102491 Vali Loss: 0.2192931 Test Loss: 0.2751677
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.99286150932312
Epoch: 25, Steps: 61 | Train Loss: 0.4097830 Vali Loss: 0.2190210 Test Loss: 0.2751818
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.44362735748291
Epoch: 26, Steps: 61 | Train Loss: 0.4097830 Vali Loss: 0.2185788 Test Loss: 0.2751681
Validation loss decreased (0.219008 --> 0.218579).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.4645349979400635
Epoch: 27, Steps: 61 | Train Loss: 0.4092356 Vali Loss: 0.2190360 Test Loss: 0.2750273
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.254862070083618
Epoch: 28, Steps: 61 | Train Loss: 0.4090369 Vali Loss: 0.2192769 Test Loss: 0.2748774
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.292536973953247
Epoch: 29, Steps: 61 | Train Loss: 0.4088059 Vali Loss: 0.2189706 Test Loss: 0.2748857
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.867108106613159
Epoch: 30, Steps: 61 | Train Loss: 0.4084912 Vali Loss: 0.2182970 Test Loss: 0.2748860
Validation loss decreased (0.218579 --> 0.218297).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.346363067626953
Epoch: 31, Steps: 61 | Train Loss: 0.4070759 Vali Loss: 0.2184344 Test Loss: 0.2748878
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.1673688888549805
Epoch: 32, Steps: 61 | Train Loss: 0.4084962 Vali Loss: 0.2186909 Test Loss: 0.2748874
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.2665061950683594
Epoch: 33, Steps: 61 | Train Loss: 0.4079864 Vali Loss: 0.2190335 Test Loss: 0.2747279
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.332990884780884
Epoch: 34, Steps: 61 | Train Loss: 0.4080269 Vali Loss: 0.2190443 Test Loss: 0.2747770
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.8610825538635254
Epoch: 35, Steps: 61 | Train Loss: 0.4074152 Vali Loss: 0.2174705 Test Loss: 0.2747091
Validation loss decreased (0.218297 --> 0.217471).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.5189671516418457
Epoch: 36, Steps: 61 | Train Loss: 0.4079972 Vali Loss: 0.2171464 Test Loss: 0.2746898
Validation loss decreased (0.217471 --> 0.217146).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8001775741577148
Epoch: 37, Steps: 61 | Train Loss: 0.4077057 Vali Loss: 0.2184026 Test Loss: 0.2745929
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6972906589508057
Epoch: 38, Steps: 61 | Train Loss: 0.4072837 Vali Loss: 0.2171649 Test Loss: 0.2746371
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.3032734394073486
Epoch: 39, Steps: 61 | Train Loss: 0.4073009 Vali Loss: 0.2172190 Test Loss: 0.2745825
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.5768916606903076
Epoch: 40, Steps: 61 | Train Loss: 0.4073536 Vali Loss: 0.2172972 Test Loss: 0.2745712
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.6250126361846924
Epoch: 41, Steps: 61 | Train Loss: 0.4067142 Vali Loss: 0.2180604 Test Loss: 0.2745680
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.0916643142700195
Epoch: 42, Steps: 61 | Train Loss: 0.4065997 Vali Loss: 0.2172171 Test Loss: 0.2745502
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.417006492614746
Epoch: 43, Steps: 61 | Train Loss: 0.4067455 Vali Loss: 0.2177132 Test Loss: 0.2744982
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.355173349380493
Epoch: 44, Steps: 61 | Train Loss: 0.4071864 Vali Loss: 0.2180657 Test Loss: 0.2744846
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.2676920890808105
Epoch: 45, Steps: 61 | Train Loss: 0.4066061 Vali Loss: 0.2170911 Test Loss: 0.2745517
Validation loss decreased (0.217146 --> 0.217091).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.064295530319214
Epoch: 46, Steps: 61 | Train Loss: 0.4069165 Vali Loss: 0.2174321 Test Loss: 0.2744908
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.251776933670044
Epoch: 47, Steps: 61 | Train Loss: 0.4066935 Vali Loss: 0.2179789 Test Loss: 0.2744965
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.295855760574341
Epoch: 48, Steps: 61 | Train Loss: 0.4064895 Vali Loss: 0.2171594 Test Loss: 0.2745042
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.8428618907928467
Epoch: 49, Steps: 61 | Train Loss: 0.4061597 Vali Loss: 0.2176073 Test Loss: 0.2744788
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.038515090942383
Epoch: 50, Steps: 61 | Train Loss: 0.4060085 Vali Loss: 0.2172180 Test Loss: 0.2744575
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.2247331142425537
Epoch: 51, Steps: 61 | Train Loss: 0.4065689 Vali Loss: 0.2170913 Test Loss: 0.2744834
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.8207449913024902
Epoch: 52, Steps: 61 | Train Loss: 0.4065350 Vali Loss: 0.2170716 Test Loss: 0.2744880
Validation loss decreased (0.217091 --> 0.217072).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.3733582496643066
Epoch: 53, Steps: 61 | Train Loss: 0.4056189 Vali Loss: 0.2170298 Test Loss: 0.2744753
Validation loss decreased (0.217072 --> 0.217030).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.640105724334717
Epoch: 54, Steps: 61 | Train Loss: 0.4064251 Vali Loss: 0.2172778 Test Loss: 0.2744362
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.4110724925994873
Epoch: 55, Steps: 61 | Train Loss: 0.4055078 Vali Loss: 0.2162249 Test Loss: 0.2744588
Validation loss decreased (0.217030 --> 0.216225).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.566554069519043
Epoch: 56, Steps: 61 | Train Loss: 0.4049140 Vali Loss: 0.2176173 Test Loss: 0.2744155
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.8719165325164795
Epoch: 57, Steps: 61 | Train Loss: 0.4062444 Vali Loss: 0.2171041 Test Loss: 0.2744490
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.4163076877593994
Epoch: 58, Steps: 61 | Train Loss: 0.4064338 Vali Loss: 0.2175392 Test Loss: 0.2744553
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.6721596717834473
Epoch: 59, Steps: 61 | Train Loss: 0.4061598 Vali Loss: 0.2183964 Test Loss: 0.2744136
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.0594379901885986
Epoch: 60, Steps: 61 | Train Loss: 0.4063765 Vali Loss: 0.2170965 Test Loss: 0.2744209
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.3623883724212646
Epoch: 61, Steps: 61 | Train Loss: 0.4060757 Vali Loss: 0.2153179 Test Loss: 0.2744262
Validation loss decreased (0.216225 --> 0.215318).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.005709409713745
Epoch: 62, Steps: 61 | Train Loss: 0.4054106 Vali Loss: 0.2169204 Test Loss: 0.2744020
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.86458683013916
Epoch: 63, Steps: 61 | Train Loss: 0.4057721 Vali Loss: 0.2180191 Test Loss: 0.2744015
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.9481661319732666
Epoch: 64, Steps: 61 | Train Loss: 0.4060185 Vali Loss: 0.2174745 Test Loss: 0.2743972
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.8547708988189697
Epoch: 65, Steps: 61 | Train Loss: 0.4063366 Vali Loss: 0.2176085 Test Loss: 0.2743868
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.907160758972168
Epoch: 66, Steps: 61 | Train Loss: 0.4042183 Vali Loss: 0.2173418 Test Loss: 0.2744012
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.3111038208007812
Epoch: 67, Steps: 61 | Train Loss: 0.4048062 Vali Loss: 0.2173293 Test Loss: 0.2743787
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.1586484909057617
Epoch: 68, Steps: 61 | Train Loss: 0.4062649 Vali Loss: 0.2161401 Test Loss: 0.2743797
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.2599470615386963
Epoch: 69, Steps: 61 | Train Loss: 0.4056577 Vali Loss: 0.2164702 Test Loss: 0.2743788
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.8433377742767334
Epoch: 70, Steps: 61 | Train Loss: 0.4041168 Vali Loss: 0.2166788 Test Loss: 0.2743714
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.0166876316070557
Epoch: 71, Steps: 61 | Train Loss: 0.4060166 Vali Loss: 0.2170428 Test Loss: 0.2743759
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.507394552230835
Epoch: 72, Steps: 61 | Train Loss: 0.4050294 Vali Loss: 0.2162509 Test Loss: 0.2743642
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.906863212585449
Epoch: 73, Steps: 61 | Train Loss: 0.4055291 Vali Loss: 0.2173349 Test Loss: 0.2743694
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.086329698562622
Epoch: 74, Steps: 61 | Train Loss: 0.4050984 Vali Loss: 0.2169001 Test Loss: 0.2743695
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.8955392837524414
Epoch: 75, Steps: 61 | Train Loss: 0.4060724 Vali Loss: 0.2168725 Test Loss: 0.2743633
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.8772153854370117
Epoch: 76, Steps: 61 | Train Loss: 0.4056891 Vali Loss: 0.2167673 Test Loss: 0.2743628
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.3002898693084717
Epoch: 77, Steps: 61 | Train Loss: 0.4059893 Vali Loss: 0.2173028 Test Loss: 0.2743481
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.8327746391296387
Epoch: 78, Steps: 61 | Train Loss: 0.4057189 Vali Loss: 0.2175166 Test Loss: 0.2743658
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 3.0213441848754883
Epoch: 79, Steps: 61 | Train Loss: 0.4062142 Vali Loss: 0.2174934 Test Loss: 0.2743590
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.83671236038208
Epoch: 80, Steps: 61 | Train Loss: 0.4056050 Vali Loss: 0.2175970 Test Loss: 0.2743494
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.4742212295532227
Epoch: 81, Steps: 61 | Train Loss: 0.4060053 Vali Loss: 0.2162257 Test Loss: 0.2743478
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_96_FITS_ETTh2_ftM_sl720_ll48_pl96_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.2739724814891815, mae:0.33854997158050537, rse:0.42182737588882446, corr:[0.2708669  0.27418658 0.27493352 0.27362904 0.27232265 0.27173522
 0.2716125  0.27131054 0.27043948 0.26894876 0.26731655 0.26572672
 0.26449034 0.2637861  0.26334617 0.26290166 0.26214856 0.26118076
 0.26010057 0.25909123 0.25815707 0.2570509  0.25556317 0.25364158
 0.25160098 0.24977782 0.24843854 0.24746272 0.2465227  0.24530502
 0.2437297  0.24188882 0.24005519 0.23872568 0.23790823 0.23719896
 0.23632523 0.23526424 0.23427461 0.23330663 0.23255634 0.23197457
 0.23133844 0.23043619 0.2293847  0.22816157 0.22684482 0.2253076
 0.2235107  0.22177297 0.2204544  0.21908945 0.21774247 0.21630631
 0.21442144 0.21248081 0.21070237 0.20907642 0.20763105 0.20656674
 0.20600818 0.20569822 0.20564266 0.20573501 0.20522639 0.20452923
 0.20374253 0.20315471 0.20289081 0.20279664 0.20233068 0.2014069
 0.20008375 0.19866855 0.19766887 0.19672747 0.19619104 0.19575389
 0.19512789 0.19383423 0.19300784 0.19294588 0.1930968  0.19320318
 0.19291157 0.1923914  0.19161789 0.19114763 0.19083406 0.19112661
 0.19122127 0.19022839 0.18890946 0.18811093 0.18898824 0.19077948]
