Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=30, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  8370.0
Trainable parameters:  8370
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0024259090423584
Epoch: 1, Steps: 61 | Train Loss: 1.2969268 Vali Loss: 0.8670523 Test Loss: 0.6999016
Validation loss decreased (inf --> 0.867052).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.264955520629883
Epoch: 2, Steps: 61 | Train Loss: 1.0807645 Vali Loss: 0.7866519 Test Loss: 0.5999281
Validation loss decreased (0.867052 --> 0.786652).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9831385612487793
Epoch: 3, Steps: 61 | Train Loss: 0.9619291 Vali Loss: 0.7339147 Test Loss: 0.5409741
Validation loss decreased (0.786652 --> 0.733915).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.2379140853881836
Epoch: 4, Steps: 61 | Train Loss: 0.8965703 Vali Loss: 0.7054551 Test Loss: 0.5038338
Validation loss decreased (0.733915 --> 0.705455).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.6430654525756836
Epoch: 5, Steps: 61 | Train Loss: 0.8525439 Vali Loss: 0.6849377 Test Loss: 0.4799998
Validation loss decreased (0.705455 --> 0.684938).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.470651149749756
Epoch: 6, Steps: 61 | Train Loss: 0.8273643 Vali Loss: 0.6688406 Test Loss: 0.4641128
Validation loss decreased (0.684938 --> 0.668841).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1563429832458496
Epoch: 7, Steps: 61 | Train Loss: 0.8090813 Vali Loss: 0.6609858 Test Loss: 0.4533281
Validation loss decreased (0.668841 --> 0.660986).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.0936522483825684
Epoch: 8, Steps: 61 | Train Loss: 0.7971716 Vali Loss: 0.6556010 Test Loss: 0.4459183
Validation loss decreased (0.660986 --> 0.655601).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.894106864929199
Epoch: 9, Steps: 61 | Train Loss: 0.7879036 Vali Loss: 0.6457095 Test Loss: 0.4406697
Validation loss decreased (0.655601 --> 0.645709).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.0262062549591064
Epoch: 10, Steps: 61 | Train Loss: 0.7835118 Vali Loss: 0.6481342 Test Loss: 0.4368790
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.486142635345459
Epoch: 11, Steps: 61 | Train Loss: 0.7781710 Vali Loss: 0.6441070 Test Loss: 0.4341283
Validation loss decreased (0.645709 --> 0.644107).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.67341947555542
Epoch: 12, Steps: 61 | Train Loss: 0.7755711 Vali Loss: 0.6456793 Test Loss: 0.4320720
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.242476224899292
Epoch: 13, Steps: 61 | Train Loss: 0.7731316 Vali Loss: 0.6421109 Test Loss: 0.4304861
Validation loss decreased (0.644107 --> 0.642111).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.698573112487793
Epoch: 14, Steps: 61 | Train Loss: 0.7722138 Vali Loss: 0.6402976 Test Loss: 0.4292323
Validation loss decreased (0.642111 --> 0.640298).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.577460289001465
Epoch: 15, Steps: 61 | Train Loss: 0.7702109 Vali Loss: 0.6401727 Test Loss: 0.4282551
Validation loss decreased (0.640298 --> 0.640173).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.81345272064209
Epoch: 16, Steps: 61 | Train Loss: 0.7685403 Vali Loss: 0.6353523 Test Loss: 0.4274558
Validation loss decreased (0.640173 --> 0.635352).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.6914753913879395
Epoch: 17, Steps: 61 | Train Loss: 0.7662985 Vali Loss: 0.6366059 Test Loss: 0.4267818
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.5681629180908203
Epoch: 18, Steps: 61 | Train Loss: 0.7665388 Vali Loss: 0.6346045 Test Loss: 0.4262506
Validation loss decreased (0.635352 --> 0.634604).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.706062078475952
Epoch: 19, Steps: 61 | Train Loss: 0.7654365 Vali Loss: 0.6338224 Test Loss: 0.4257852
Validation loss decreased (0.634604 --> 0.633822).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.633861541748047
Epoch: 20, Steps: 61 | Train Loss: 0.7654933 Vali Loss: 0.6345392 Test Loss: 0.4253856
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.2092840671539307
Epoch: 21, Steps: 61 | Train Loss: 0.7652500 Vali Loss: 0.6364304 Test Loss: 0.4250329
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.653404712677002
Epoch: 22, Steps: 61 | Train Loss: 0.7634697 Vali Loss: 0.6324218 Test Loss: 0.4247364
Validation loss decreased (0.633822 --> 0.632422).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.246188163757324
Epoch: 23, Steps: 61 | Train Loss: 0.7646631 Vali Loss: 0.6324270 Test Loss: 0.4244852
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.24973464012146
Epoch: 24, Steps: 61 | Train Loss: 0.7641503 Vali Loss: 0.6329662 Test Loss: 0.4242456
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.49121880531311
Epoch: 25, Steps: 61 | Train Loss: 0.7620865 Vali Loss: 0.6338444 Test Loss: 0.4240143
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.795189619064331
Epoch: 26, Steps: 61 | Train Loss: 0.7621167 Vali Loss: 0.6315114 Test Loss: 0.4238421
Validation loss decreased (0.632422 --> 0.631511).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.3644676208496094
Epoch: 27, Steps: 61 | Train Loss: 0.7625682 Vali Loss: 0.6351208 Test Loss: 0.4236481
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.472865581512451
Epoch: 28, Steps: 61 | Train Loss: 0.7629509 Vali Loss: 0.6295226 Test Loss: 0.4235155
Validation loss decreased (0.631511 --> 0.629523).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.1833853721618652
Epoch: 29, Steps: 61 | Train Loss: 0.7623096 Vali Loss: 0.6284682 Test Loss: 0.4233733
Validation loss decreased (0.629523 --> 0.628468).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.5462470054626465
Epoch: 30, Steps: 61 | Train Loss: 0.7628275 Vali Loss: 0.6317759 Test Loss: 0.4232361
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.4034318923950195
Epoch: 31, Steps: 61 | Train Loss: 0.7618303 Vali Loss: 0.6299564 Test Loss: 0.4231184
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.0109894275665283
Epoch: 32, Steps: 61 | Train Loss: 0.7623572 Vali Loss: 0.6270177 Test Loss: 0.4230044
Validation loss decreased (0.628468 --> 0.627018).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.405102252960205
Epoch: 33, Steps: 61 | Train Loss: 0.7615972 Vali Loss: 0.6281826 Test Loss: 0.4229201
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.6849279403686523
Epoch: 34, Steps: 61 | Train Loss: 0.7602813 Vali Loss: 0.6317015 Test Loss: 0.4228144
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.475506544113159
Epoch: 35, Steps: 61 | Train Loss: 0.7618515 Vali Loss: 0.6291715 Test Loss: 0.4227356
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.361582040786743
Epoch: 36, Steps: 61 | Train Loss: 0.7604443 Vali Loss: 0.6296065 Test Loss: 0.4226584
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.144549131393433
Epoch: 37, Steps: 61 | Train Loss: 0.7600784 Vali Loss: 0.6357446 Test Loss: 0.4225810
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.038691282272339
Epoch: 38, Steps: 61 | Train Loss: 0.7606974 Vali Loss: 0.6331617 Test Loss: 0.4225111
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.334441900253296
Epoch: 39, Steps: 61 | Train Loss: 0.7607698 Vali Loss: 0.6330248 Test Loss: 0.4224473
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.791790723800659
Epoch: 40, Steps: 61 | Train Loss: 0.7602506 Vali Loss: 0.6267906 Test Loss: 0.4223851
Validation loss decreased (0.627018 --> 0.626791).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.152606248855591
Epoch: 41, Steps: 61 | Train Loss: 0.7603009 Vali Loss: 0.6280723 Test Loss: 0.4223398
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.3689162731170654
Epoch: 42, Steps: 61 | Train Loss: 0.7590301 Vali Loss: 0.6270540 Test Loss: 0.4222837
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.4361448287963867
Epoch: 43, Steps: 61 | Train Loss: 0.7582762 Vali Loss: 0.6291038 Test Loss: 0.4222341
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.1762683391571045
Epoch: 44, Steps: 61 | Train Loss: 0.7597732 Vali Loss: 0.6280971 Test Loss: 0.4221877
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.2931442260742188
Epoch: 45, Steps: 61 | Train Loss: 0.7593763 Vali Loss: 0.6270223 Test Loss: 0.4221474
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.635820150375366
Epoch: 46, Steps: 61 | Train Loss: 0.7611051 Vali Loss: 0.6228959 Test Loss: 0.4221037
Validation loss decreased (0.626791 --> 0.622896).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.949649095535278
Epoch: 47, Steps: 61 | Train Loss: 0.7601782 Vali Loss: 0.6276946 Test Loss: 0.4220645
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.9668924808502197
Epoch: 48, Steps: 61 | Train Loss: 0.7603336 Vali Loss: 0.6275797 Test Loss: 0.4220331
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.3489139080047607
Epoch: 49, Steps: 61 | Train Loss: 0.7581692 Vali Loss: 0.6287128 Test Loss: 0.4220001
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.429875612258911
Epoch: 50, Steps: 61 | Train Loss: 0.7589056 Vali Loss: 0.6279084 Test Loss: 0.4219741
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.650312662124634
Epoch: 51, Steps: 61 | Train Loss: 0.7607991 Vali Loss: 0.6292874 Test Loss: 0.4219349
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.7337446212768555
Epoch: 52, Steps: 61 | Train Loss: 0.7595174 Vali Loss: 0.6290679 Test Loss: 0.4219075
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.4696335792541504
Epoch: 53, Steps: 61 | Train Loss: 0.7597575 Vali Loss: 0.6285518 Test Loss: 0.4218854
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.932211399078369
Epoch: 54, Steps: 61 | Train Loss: 0.7595676 Vali Loss: 0.6280254 Test Loss: 0.4218576
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.76253342628479
Epoch: 55, Steps: 61 | Train Loss: 0.7605063 Vali Loss: 0.6301578 Test Loss: 0.4218337
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.191967010498047
Epoch: 56, Steps: 61 | Train Loss: 0.7607466 Vali Loss: 0.6285214 Test Loss: 0.4218162
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.9169023036956787
Epoch: 57, Steps: 61 | Train Loss: 0.7589994 Vali Loss: 0.6305496 Test Loss: 0.4217940
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.665497064590454
Epoch: 58, Steps: 61 | Train Loss: 0.7587744 Vali Loss: 0.6285840 Test Loss: 0.4217726
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.400043249130249
Epoch: 59, Steps: 61 | Train Loss: 0.7602573 Vali Loss: 0.6311495 Test Loss: 0.4217558
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.7857861518859863
Epoch: 60, Steps: 61 | Train Loss: 0.7586281 Vali Loss: 0.6293794 Test Loss: 0.4217368
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.8671820163726807
Epoch: 61, Steps: 61 | Train Loss: 0.7599698 Vali Loss: 0.6287665 Test Loss: 0.4217185
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.1789519786834717
Epoch: 62, Steps: 61 | Train Loss: 0.7597042 Vali Loss: 0.6289508 Test Loss: 0.4217044
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 5.688553810119629
Epoch: 63, Steps: 61 | Train Loss: 0.7603456 Vali Loss: 0.6273452 Test Loss: 0.4216857
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 4.424795150756836
Epoch: 64, Steps: 61 | Train Loss: 0.7595274 Vali Loss: 0.6295643 Test Loss: 0.4216756
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 5.427525281906128
Epoch: 65, Steps: 61 | Train Loss: 0.7597644 Vali Loss: 0.6271118 Test Loss: 0.4216610
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.693654775619507
Epoch: 66, Steps: 61 | Train Loss: 0.7588991 Vali Loss: 0.6289194 Test Loss: 0.4216491
EarlyStopping counter: 20 out of 20
Early stopping
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=30, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  8370.0
Trainable parameters:  8370
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.325814962387085
Epoch: 1, Steps: 61 | Train Loss: 0.8527982 Vali Loss: 0.6311377 Test Loss: 0.4214360
Validation loss decreased (inf --> 0.631138).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.2985196113586426
Epoch: 2, Steps: 61 | Train Loss: 0.8516058 Vali Loss: 0.6301256 Test Loss: 0.4210323
Validation loss decreased (0.631138 --> 0.630126).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.8993418216705322
Epoch: 3, Steps: 61 | Train Loss: 0.8502848 Vali Loss: 0.6252388 Test Loss: 0.4207510
Validation loss decreased (0.630126 --> 0.625239).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.9490153789520264
Epoch: 4, Steps: 61 | Train Loss: 0.8510071 Vali Loss: 0.6286981 Test Loss: 0.4205517
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.6606271266937256
Epoch: 5, Steps: 61 | Train Loss: 0.8496910 Vali Loss: 0.6247417 Test Loss: 0.4203782
Validation loss decreased (0.625239 --> 0.624742).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.8723299503326416
Epoch: 6, Steps: 61 | Train Loss: 0.8493648 Vali Loss: 0.6254584 Test Loss: 0.4202612
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.3092117309570312
Epoch: 7, Steps: 61 | Train Loss: 0.8489289 Vali Loss: 0.6251720 Test Loss: 0.4201199
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.5252010822296143
Epoch: 8, Steps: 61 | Train Loss: 0.8484603 Vali Loss: 0.6264204 Test Loss: 0.4200440
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.0934395790100098
Epoch: 9, Steps: 61 | Train Loss: 0.8482051 Vali Loss: 0.6288335 Test Loss: 0.4199525
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.14027738571167
Epoch: 10, Steps: 61 | Train Loss: 0.8471335 Vali Loss: 0.6249077 Test Loss: 0.4199014
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.2287538051605225
Epoch: 11, Steps: 61 | Train Loss: 0.8470202 Vali Loss: 0.6267852 Test Loss: 0.4198513
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.0022053718566895
Epoch: 12, Steps: 61 | Train Loss: 0.8479800 Vali Loss: 0.6222200 Test Loss: 0.4197887
Validation loss decreased (0.624742 --> 0.622220).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.413463354110718
Epoch: 13, Steps: 61 | Train Loss: 0.8474793 Vali Loss: 0.6257266 Test Loss: 0.4197455
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.5604820251464844
Epoch: 14, Steps: 61 | Train Loss: 0.8466135 Vali Loss: 0.6229894 Test Loss: 0.4197257
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.9755196571350098
Epoch: 15, Steps: 61 | Train Loss: 0.8467094 Vali Loss: 0.6241408 Test Loss: 0.4196567
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.795297622680664
Epoch: 16, Steps: 61 | Train Loss: 0.8474056 Vali Loss: 0.6244453 Test Loss: 0.4195867
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.8065688610076904
Epoch: 17, Steps: 61 | Train Loss: 0.8464426 Vali Loss: 0.6223068 Test Loss: 0.4195993
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.4861130714416504
Epoch: 18, Steps: 61 | Train Loss: 0.8465638 Vali Loss: 0.6257873 Test Loss: 0.4195641
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.3486685752868652
Epoch: 19, Steps: 61 | Train Loss: 0.8451702 Vali Loss: 0.6252640 Test Loss: 0.4195323
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.204258680343628
Epoch: 20, Steps: 61 | Train Loss: 0.8455436 Vali Loss: 0.6230645 Test Loss: 0.4195129
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.068248271942139
Epoch: 21, Steps: 61 | Train Loss: 0.8451419 Vali Loss: 0.6207569 Test Loss: 0.4195116
Validation loss decreased (0.622220 --> 0.620757).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.964036226272583
Epoch: 22, Steps: 61 | Train Loss: 0.8462434 Vali Loss: 0.6266049 Test Loss: 0.4194962
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.0175466537475586
Epoch: 23, Steps: 61 | Train Loss: 0.8452347 Vali Loss: 0.6226627 Test Loss: 0.4194555
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.7771575450897217
Epoch: 24, Steps: 61 | Train Loss: 0.8455936 Vali Loss: 0.6292857 Test Loss: 0.4194542
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.4341375827789307
Epoch: 25, Steps: 61 | Train Loss: 0.8457808 Vali Loss: 0.6193474 Test Loss: 0.4194257
Validation loss decreased (0.620757 --> 0.619347).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.6072192192077637
Epoch: 26, Steps: 61 | Train Loss: 0.8466805 Vali Loss: 0.6254107 Test Loss: 0.4194063
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.8504135608673096
Epoch: 27, Steps: 61 | Train Loss: 0.8455226 Vali Loss: 0.6231152 Test Loss: 0.4193931
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.615563154220581
Epoch: 28, Steps: 61 | Train Loss: 0.8452709 Vali Loss: 0.6230640 Test Loss: 0.4193969
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.370150327682495
Epoch: 29, Steps: 61 | Train Loss: 0.8446468 Vali Loss: 0.6225369 Test Loss: 0.4193943
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.144052743911743
Epoch: 30, Steps: 61 | Train Loss: 0.8462668 Vali Loss: 0.6216161 Test Loss: 0.4193731
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 5.219601631164551
Epoch: 31, Steps: 61 | Train Loss: 0.8451701 Vali Loss: 0.6206450 Test Loss: 0.4193647
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.7649571895599365
Epoch: 32, Steps: 61 | Train Loss: 0.8455413 Vali Loss: 0.6237030 Test Loss: 0.4193621
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 5.065053462982178
Epoch: 33, Steps: 61 | Train Loss: 0.8457203 Vali Loss: 0.6222164 Test Loss: 0.4193347
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.5867955684661865
Epoch: 34, Steps: 61 | Train Loss: 0.8457513 Vali Loss: 0.6254742 Test Loss: 0.4193523
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.567444562911987
Epoch: 35, Steps: 61 | Train Loss: 0.8458192 Vali Loss: 0.6207671 Test Loss: 0.4193422
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.4742164611816406
Epoch: 36, Steps: 61 | Train Loss: 0.8453020 Vali Loss: 0.6235006 Test Loss: 0.4193304
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.2783069610595703
Epoch: 37, Steps: 61 | Train Loss: 0.8457706 Vali Loss: 0.6211805 Test Loss: 0.4193316
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.354391098022461
Epoch: 38, Steps: 61 | Train Loss: 0.8450123 Vali Loss: 0.6233469 Test Loss: 0.4193200
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.4276938438415527
Epoch: 39, Steps: 61 | Train Loss: 0.8454879 Vali Loss: 0.6150157 Test Loss: 0.4193144
Validation loss decreased (0.619347 --> 0.615016).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.482772588729858
Epoch: 40, Steps: 61 | Train Loss: 0.8459569 Vali Loss: 0.6178539 Test Loss: 0.4193038
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.8036704063415527
Epoch: 41, Steps: 61 | Train Loss: 0.8444623 Vali Loss: 0.6217403 Test Loss: 0.4193078
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.8168349266052246
Epoch: 42, Steps: 61 | Train Loss: 0.8456983 Vali Loss: 0.6243449 Test Loss: 0.4193021
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.315561056137085
Epoch: 43, Steps: 61 | Train Loss: 0.8449717 Vali Loss: 0.6251311 Test Loss: 0.4192973
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.7454464435577393
Epoch: 44, Steps: 61 | Train Loss: 0.8448704 Vali Loss: 0.6233376 Test Loss: 0.4192976
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.889944553375244
Epoch: 45, Steps: 61 | Train Loss: 0.8435583 Vali Loss: 0.6196649 Test Loss: 0.4192914
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.698307752609253
Epoch: 46, Steps: 61 | Train Loss: 0.8442618 Vali Loss: 0.6229733 Test Loss: 0.4192884
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.382783651351929
Epoch: 47, Steps: 61 | Train Loss: 0.8454788 Vali Loss: 0.6235453 Test Loss: 0.4192885
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.640046834945679
Epoch: 48, Steps: 61 | Train Loss: 0.8441961 Vali Loss: 0.6247437 Test Loss: 0.4192805
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.876676321029663
Epoch: 49, Steps: 61 | Train Loss: 0.8443149 Vali Loss: 0.6212114 Test Loss: 0.4192791
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.5116117000579834
Epoch: 50, Steps: 61 | Train Loss: 0.8449696 Vali Loss: 0.6212165 Test Loss: 0.4192755
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.7016985416412354
Epoch: 51, Steps: 61 | Train Loss: 0.8458275 Vali Loss: 0.6213614 Test Loss: 0.4192749
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.452533483505249
Epoch: 52, Steps: 61 | Train Loss: 0.8450447 Vali Loss: 0.6231341 Test Loss: 0.4192717
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.8470184803009033
Epoch: 53, Steps: 61 | Train Loss: 0.8459033 Vali Loss: 0.6210150 Test Loss: 0.4192726
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 4.407982587814331
Epoch: 54, Steps: 61 | Train Loss: 0.8442071 Vali Loss: 0.6180555 Test Loss: 0.4192660
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.8249714374542236
Epoch: 55, Steps: 61 | Train Loss: 0.8437685 Vali Loss: 0.6216257 Test Loss: 0.4192686
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.9353082180023193
Epoch: 56, Steps: 61 | Train Loss: 0.8426712 Vali Loss: 0.6252661 Test Loss: 0.4192657
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.1877989768981934
Epoch: 57, Steps: 61 | Train Loss: 0.8437804 Vali Loss: 0.6194936 Test Loss: 0.4192621
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 4.628100633621216
Epoch: 58, Steps: 61 | Train Loss: 0.8454817 Vali Loss: 0.6219593 Test Loss: 0.4192598
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.152735471725464
Epoch: 59, Steps: 61 | Train Loss: 0.8446944 Vali Loss: 0.6225943 Test Loss: 0.4192617
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.417725145816803, mae:0.436204731464386, rse:0.5165960192680359, corr:[ 2.20819950e-01  2.20338285e-01  2.19333142e-01  2.19271526e-01
  2.17218563e-01  2.16599539e-01  2.16067046e-01  2.14845821e-01
  2.14405343e-01  2.13044405e-01  2.12087497e-01  2.11260006e-01
  2.09474295e-01  2.08402693e-01  2.07803652e-01  2.07017794e-01
  2.06573293e-01  2.06401706e-01  2.05633998e-01  2.04717308e-01
  2.03973666e-01  2.03231394e-01  2.02631697e-01  2.01027229e-01
  1.98204130e-01  1.96401015e-01  1.94733709e-01  1.93695977e-01
  1.93230122e-01  1.92687541e-01  1.92342564e-01  1.91519216e-01
  1.90498918e-01  1.89290285e-01  1.88684687e-01  1.87879518e-01
  1.86735779e-01  1.85962126e-01  1.85162768e-01  1.84483469e-01
  1.84081361e-01  1.83613181e-01  1.83345526e-01  1.82903841e-01
  1.82389542e-01  1.81705311e-01  1.81232527e-01  1.79582402e-01
  1.76263154e-01  1.74025565e-01  1.71987116e-01  1.70656502e-01
  1.69759005e-01  1.69137225e-01  1.68971121e-01  1.67672694e-01
  1.67228863e-01  1.66580901e-01  1.66243568e-01  1.65779054e-01
  1.65026143e-01  1.64885923e-01  1.64849222e-01  1.65114939e-01
  1.65479764e-01  1.65610448e-01  1.65612221e-01  1.65304288e-01
  1.65182382e-01  1.65024236e-01  1.64666131e-01  1.63989082e-01
  1.61996588e-01  1.61022186e-01  1.60361931e-01  1.59906730e-01
  1.59474075e-01  1.59632787e-01  1.60216019e-01  1.59557134e-01
  1.59290850e-01  1.59013957e-01  1.58895209e-01  1.58823952e-01
  1.58380106e-01  1.58295453e-01  1.58550024e-01  1.58595517e-01
  1.58342049e-01  1.58437699e-01  1.58501446e-01  1.57786995e-01
  1.57613441e-01  1.57722503e-01  1.57474816e-01  1.56796291e-01
  1.55232474e-01  1.54197603e-01  1.53137878e-01  1.52483076e-01
  1.51780844e-01  1.51185170e-01  1.51453450e-01  1.50918171e-01
  1.51078865e-01  1.50984541e-01  1.50904328e-01  1.50815189e-01
  1.50524691e-01  1.50195763e-01  1.49740070e-01  1.49828911e-01
  1.49635673e-01  1.49173796e-01  1.49149254e-01  1.48689479e-01
  1.48374557e-01  1.48163423e-01  1.47219747e-01  1.45709142e-01
  1.43701077e-01  1.42426357e-01  1.41039401e-01  1.40406743e-01
  1.39540419e-01  1.38888568e-01  1.38903365e-01  1.38299912e-01
  1.38078168e-01  1.37802631e-01  1.37628928e-01  1.37004673e-01
  1.36301979e-01  1.35701358e-01  1.35031030e-01  1.34694695e-01
  1.34376034e-01  1.33846000e-01  1.33387282e-01  1.32586479e-01
  1.32162452e-01  1.31994769e-01  1.31255046e-01  1.29760787e-01
  1.26873106e-01  1.24835193e-01  1.23235196e-01  1.22531727e-01
  1.21894270e-01  1.21233530e-01  1.21380433e-01  1.20786563e-01
  1.20590977e-01  1.20487981e-01  1.20623901e-01  1.20141730e-01
  1.19357042e-01  1.18998736e-01  1.18570037e-01  1.18482076e-01
  1.18279308e-01  1.18037581e-01  1.18208051e-01  1.18007809e-01
  1.17990628e-01  1.17835365e-01  1.17424019e-01  1.16155334e-01
  1.13678806e-01  1.12363540e-01  1.11182079e-01  1.10716783e-01
  1.10513732e-01  1.10527858e-01  1.11215942e-01  1.11093439e-01
  1.11021183e-01  1.10566631e-01  1.10675186e-01  1.10241555e-01
  1.09662533e-01  1.09282739e-01  1.08956136e-01  1.09168060e-01
  1.09108493e-01  1.09129742e-01  1.09595776e-01  1.09825201e-01
  1.09879807e-01  1.10178038e-01  1.10318296e-01  1.10139355e-01
  1.08879678e-01  1.08570412e-01  1.08384274e-01  1.08743370e-01
  1.09266527e-01  1.09697461e-01  1.11316711e-01  1.12170085e-01
  1.12449661e-01  1.12366028e-01  1.13064617e-01  1.13038272e-01
  1.12440400e-01  1.12338856e-01  1.12296246e-01  1.12304322e-01
  1.12354830e-01  1.12726942e-01  1.13033608e-01  1.13052137e-01
  1.13221310e-01  1.13032766e-01  1.12842426e-01  1.12086162e-01
  1.10537648e-01  1.09545760e-01  1.08258687e-01  1.08078524e-01
  1.08274274e-01  1.08875036e-01  1.10156618e-01  1.11306168e-01
  1.12128168e-01  1.12373270e-01  1.12622850e-01  1.12313382e-01
  1.12151094e-01  1.12305097e-01  1.12419210e-01  1.12740785e-01
  1.13299578e-01  1.13716856e-01  1.14013880e-01  1.14271082e-01
  1.14612989e-01  1.15017772e-01  1.15186825e-01  1.15013018e-01
  1.13936134e-01  1.13477826e-01  1.12783745e-01  1.12689868e-01
  1.13046244e-01  1.13860093e-01  1.15468986e-01  1.16004303e-01
  1.16775952e-01  1.17124148e-01  1.17611289e-01  1.17646270e-01
  1.17442928e-01  1.17834829e-01  1.17806651e-01  1.17990367e-01
  1.18260637e-01  1.18948571e-01  1.19243667e-01  1.19262509e-01
  1.19544648e-01  1.19947895e-01  1.20144196e-01  1.20034948e-01
  1.19062424e-01  1.18879706e-01  1.18936516e-01  1.19584672e-01
  1.20201424e-01  1.21383943e-01  1.23307548e-01  1.24275275e-01
  1.24872871e-01  1.24828763e-01  1.25199273e-01  1.24901168e-01
  1.24583788e-01  1.24442890e-01  1.24719009e-01  1.24986753e-01
  1.25150204e-01  1.25338331e-01  1.25712231e-01  1.25801310e-01
  1.25991300e-01  1.26327440e-01  1.26554623e-01  1.26479894e-01
  1.25879526e-01  1.25576630e-01  1.25114501e-01  1.24952085e-01
  1.25065729e-01  1.25323817e-01  1.25796720e-01  1.25895336e-01
  1.26370102e-01  1.26589864e-01  1.26881734e-01  1.26776442e-01
  1.26452237e-01  1.26477122e-01  1.26535982e-01  1.26775876e-01
  1.26916364e-01  1.27276823e-01  1.27636731e-01  1.27494052e-01
  1.27678663e-01  1.27756983e-01  1.27781957e-01  1.27736390e-01
  1.26537979e-01  1.25866592e-01  1.25571549e-01  1.25872031e-01
  1.25997290e-01  1.26213059e-01  1.27095714e-01  1.27440542e-01
  1.28002360e-01  1.28247425e-01  1.28625318e-01  1.28702953e-01
  1.28582537e-01  1.28448695e-01  1.28620207e-01  1.29033983e-01
  1.29210874e-01  1.29342735e-01  1.29722998e-01  1.30054772e-01
  1.30459085e-01  1.30905375e-01  1.31083384e-01  1.31278127e-01
  1.30493671e-01  1.30218521e-01  1.29838496e-01  1.30011812e-01
  1.31096736e-01  1.32049933e-01  1.33585244e-01  1.34474814e-01
  1.35011688e-01  1.35016575e-01  1.35383263e-01  1.35859430e-01
  1.35790914e-01  1.35408700e-01  1.35438636e-01  1.35899380e-01
  1.36285827e-01  1.36477888e-01  1.37081057e-01  1.37638047e-01
  1.37935087e-01  1.38671413e-01  1.39285177e-01  1.39209837e-01
  1.38545275e-01  1.38698325e-01  1.38785884e-01  1.39049813e-01
  1.39995471e-01  1.41467199e-01  1.43696263e-01  1.45595744e-01
  1.46837845e-01  1.47344112e-01  1.48132041e-01  1.48847893e-01
  1.49304211e-01  1.49872884e-01  1.50331691e-01  1.50860876e-01
  1.51588574e-01  1.52524084e-01  1.53073475e-01  1.53629825e-01
  1.54271260e-01  1.54905066e-01  1.55500412e-01  1.55900061e-01
  1.55875310e-01  1.56321436e-01  1.56719878e-01  1.57158300e-01
  1.58287853e-01  1.59894869e-01  1.62093103e-01  1.63527235e-01
  1.64533809e-01  1.65054142e-01  1.65835440e-01  1.66297108e-01
  1.66352496e-01  1.66511282e-01  1.66580647e-01  1.66758731e-01
  1.66872129e-01  1.66872263e-01  1.67079136e-01  1.67212889e-01
  1.67356938e-01  1.67464808e-01  1.67535275e-01  1.67556703e-01
  1.66738123e-01  1.66646704e-01  1.66695520e-01  1.66843429e-01
  1.67248547e-01  1.68039069e-01  1.69316158e-01  1.70000777e-01
  1.70336440e-01  1.70533538e-01  1.70927465e-01  1.70959488e-01
  1.70544222e-01  1.70297354e-01  1.70270726e-01  1.70192316e-01
  1.69980466e-01  1.69908151e-01  1.69883415e-01  1.69836536e-01
  1.70002699e-01  1.70196399e-01  1.70247495e-01  1.70196638e-01
  1.69704229e-01  1.69350475e-01  1.69192255e-01  1.69117123e-01
  1.69036463e-01  1.69301659e-01  1.70319289e-01  1.70433253e-01
  1.70508236e-01  1.70442984e-01  1.70359358e-01  1.70276657e-01
  1.70106173e-01  1.69905260e-01  1.69637486e-01  1.69726133e-01
  1.69685692e-01  1.69584200e-01  1.69654772e-01  1.69659212e-01
  1.69661209e-01  1.69730216e-01  1.69745803e-01  1.69640914e-01
  1.69146985e-01  1.68710947e-01  1.68015927e-01  1.67853981e-01
  1.67843565e-01  1.67859614e-01  1.68179512e-01  1.68110952e-01
  1.68057546e-01  1.67955875e-01  1.67899027e-01  1.67420924e-01
  1.67003050e-01  1.66677147e-01  1.66157648e-01  1.66063979e-01
  1.65992081e-01  1.65667742e-01  1.65399760e-01  1.65141165e-01
  1.64885044e-01  1.64895386e-01  1.64723814e-01  1.63903922e-01
  1.62409857e-01  1.61481962e-01  1.60711616e-01  1.59747183e-01
  1.58900768e-01  1.58344775e-01  1.58171982e-01  1.57329544e-01
  1.56693652e-01  1.56208813e-01  1.55673891e-01  1.54927239e-01
  1.54226124e-01  1.53807700e-01  1.53375924e-01  1.53092518e-01
  1.52717665e-01  1.52341843e-01  1.52245253e-01  1.51944131e-01
  1.51553601e-01  1.51455104e-01  1.51200697e-01  1.49912000e-01
  1.47722781e-01  1.46093160e-01  1.44733250e-01  1.43551379e-01
  1.42565832e-01  1.41996622e-01  1.41716138e-01  1.41192839e-01
  1.40750423e-01  1.40453398e-01  1.40255690e-01  1.39656320e-01
  1.38849124e-01  1.38060227e-01  1.37670875e-01  1.37571737e-01
  1.37253135e-01  1.36814281e-01  1.36375532e-01  1.35641843e-01
  1.34832233e-01  1.34641990e-01  1.34095147e-01  1.32481351e-01
  1.29935384e-01  1.27681479e-01  1.25478148e-01  1.23601742e-01
  1.21999197e-01  1.20188139e-01  1.19321905e-01  1.18660040e-01
  1.18477024e-01  1.17919773e-01  1.17378473e-01  1.16757654e-01
  1.15848280e-01  1.15040690e-01  1.14188775e-01  1.13500498e-01
  1.12566113e-01  1.11987077e-01  1.11495003e-01  1.10627837e-01
  1.09754100e-01  1.09301992e-01  1.08363032e-01  1.06126077e-01
  1.02915257e-01  1.00894772e-01  9.85892341e-02  9.66834053e-02
  9.53537896e-02  9.47076604e-02  9.46850479e-02  9.37458947e-02
  9.32073295e-02  9.28187817e-02  9.25958231e-02  9.19827446e-02
  9.12159234e-02  9.05174315e-02  8.96979719e-02  8.90485942e-02
  8.84877220e-02  8.78914595e-02  8.74797478e-02  8.66980851e-02
  8.55714679e-02  8.47858116e-02  8.37221295e-02  8.14966485e-02
  7.85311237e-02  7.65349865e-02  7.47112334e-02  7.31561333e-02
  7.18776435e-02  7.07323551e-02  7.05780983e-02  7.02780932e-02
  6.99459761e-02  6.94774315e-02  6.89984784e-02  6.82489201e-02
  6.73708096e-02  6.66516125e-02  6.61433861e-02  6.59262836e-02
  6.55197799e-02  6.51132166e-02  6.51595667e-02  6.48997426e-02
  6.42427206e-02  6.37169331e-02  6.26520365e-02  6.05921075e-02
  5.79873547e-02  5.61690032e-02  5.42639904e-02  5.29105142e-02
  5.18578961e-02  5.09327464e-02  5.04611991e-02  4.98821251e-02
  4.95709255e-02  4.91311476e-02  4.89695743e-02  4.85827848e-02
  4.79847789e-02  4.73217778e-02  4.68601733e-02  4.69550937e-02
  4.66373749e-02  4.61471938e-02  4.61500175e-02  4.60001417e-02
  4.55111749e-02  4.52956855e-02  4.43975590e-02  4.26601507e-02
  3.97987776e-02  3.75818014e-02  3.57517414e-02  3.44412252e-02
  3.29446383e-02  3.18181925e-02  3.16528045e-02  3.09152044e-02
  3.05148885e-02  3.02082431e-02  2.98533998e-02  2.92247385e-02
  2.83810757e-02  2.73769833e-02  2.65719369e-02  2.62514856e-02
  2.53108442e-02  2.41802372e-02  2.38607600e-02  2.32130196e-02
  2.24819817e-02  2.25532129e-02  2.23081727e-02  2.00961344e-02
  1.71805955e-02  1.52229434e-02  1.30813234e-02  1.14866598e-02
  1.03104021e-02  9.26500373e-03  8.74067936e-03  8.08979012e-03
  7.85103347e-03  7.60458503e-03  7.31158257e-03  6.73814444e-03
  6.24752557e-03  5.46661066e-03  5.02313068e-03  5.51781524e-03
  5.07610850e-03  4.17186180e-03  4.54425812e-03  4.38899966e-03
  3.65734356e-03  4.36327234e-03  4.54982743e-03  2.57694512e-03
 -9.49123933e-05 -2.30811513e-03 -4.56710020e-03 -6.01057056e-03
 -7.72283226e-03 -9.25025064e-03 -8.97797104e-03 -8.98316503e-03
 -8.81117955e-03 -8.36402364e-03 -8.16903543e-03 -8.97434540e-03
 -9.47421696e-03 -9.82910115e-03 -1.05080493e-02 -1.04125803e-02
 -1.02904672e-02 -1.06552392e-02 -1.02119548e-02 -1.03365239e-02
 -1.09938513e-02 -1.05264010e-02 -1.09027373e-02 -1.31132482e-02
 -1.61059871e-02 -1.81248859e-02 -2.06591357e-02 -2.14488786e-02
 -2.20521484e-02 -2.30931509e-02 -2.21957732e-02 -2.18761861e-02
 -2.18131077e-02 -2.17967145e-02 -2.11046096e-02 -2.17854157e-02
 -2.27059685e-02 -2.35686172e-02 -2.40990445e-02 -2.42339913e-02
 -2.50986256e-02 -2.51949709e-02 -2.38927975e-02 -2.46916264e-02
 -2.53165029e-02 -2.35474017e-02 -2.59813499e-02 -2.27446221e-02]
