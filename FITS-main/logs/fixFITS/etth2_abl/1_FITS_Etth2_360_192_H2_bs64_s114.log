Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_192_FITS_ETTh2_ftM_sl360_ll48_pl192_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=42, out_features=64, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2408448.0
params:  2752.0
Trainable parameters:  2752
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8220293521881104
Epoch: 1, Steps: 63 | Train Loss: 0.6951833 Vali Loss: 0.3700850 Test Loss: 0.4224428
Validation loss decreased (inf --> 0.370085).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.2328484058380127
Epoch: 2, Steps: 63 | Train Loss: 0.5938790 Vali Loss: 0.3328593 Test Loss: 0.3903153
Validation loss decreased (0.370085 --> 0.332859).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0662636756896973
Epoch: 3, Steps: 63 | Train Loss: 0.5603717 Vali Loss: 0.3160439 Test Loss: 0.3782369
Validation loss decreased (0.332859 --> 0.316044).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.5395376682281494
Epoch: 4, Steps: 63 | Train Loss: 0.5448102 Vali Loss: 0.3073328 Test Loss: 0.3719380
Validation loss decreased (0.316044 --> 0.307333).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8845653533935547
Epoch: 5, Steps: 63 | Train Loss: 0.5353903 Vali Loss: 0.3024767 Test Loss: 0.3684199
Validation loss decreased (0.307333 --> 0.302477).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.0566182136535645
Epoch: 6, Steps: 63 | Train Loss: 0.5314771 Vali Loss: 0.2992651 Test Loss: 0.3659362
Validation loss decreased (0.302477 --> 0.299265).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.292057752609253
Epoch: 7, Steps: 63 | Train Loss: 0.5283022 Vali Loss: 0.2968709 Test Loss: 0.3644515
Validation loss decreased (0.299265 --> 0.296871).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.2320899963378906
Epoch: 8, Steps: 63 | Train Loss: 0.5261085 Vali Loss: 0.2950087 Test Loss: 0.3633186
Validation loss decreased (0.296871 --> 0.295009).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.9737088680267334
Epoch: 9, Steps: 63 | Train Loss: 0.5243367 Vali Loss: 0.2938570 Test Loss: 0.3624168
Validation loss decreased (0.295009 --> 0.293857).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.072938919067383
Epoch: 10, Steps: 63 | Train Loss: 0.5215857 Vali Loss: 0.2926406 Test Loss: 0.3618203
Validation loss decreased (0.293857 --> 0.292641).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.8867716789245605
Epoch: 11, Steps: 63 | Train Loss: 0.5217358 Vali Loss: 0.2917143 Test Loss: 0.3612026
Validation loss decreased (0.292641 --> 0.291714).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.177109479904175
Epoch: 12, Steps: 63 | Train Loss: 0.5200871 Vali Loss: 0.2910624 Test Loss: 0.3606509
Validation loss decreased (0.291714 --> 0.291062).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9240005016326904
Epoch: 13, Steps: 63 | Train Loss: 0.5197854 Vali Loss: 0.2903752 Test Loss: 0.3603467
Validation loss decreased (0.291062 --> 0.290375).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.9326319694519043
Epoch: 14, Steps: 63 | Train Loss: 0.5185669 Vali Loss: 0.2899537 Test Loss: 0.3599147
Validation loss decreased (0.290375 --> 0.289954).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.1060290336608887
Epoch: 15, Steps: 63 | Train Loss: 0.5188781 Vali Loss: 0.2892220 Test Loss: 0.3595166
Validation loss decreased (0.289954 --> 0.289222).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.09582781791687
Epoch: 16, Steps: 63 | Train Loss: 0.5176220 Vali Loss: 0.2885886 Test Loss: 0.3593683
Validation loss decreased (0.289222 --> 0.288589).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.441164016723633
Epoch: 17, Steps: 63 | Train Loss: 0.5164579 Vali Loss: 0.2884963 Test Loss: 0.3592777
Validation loss decreased (0.288589 --> 0.288496).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.3804404735565186
Epoch: 18, Steps: 63 | Train Loss: 0.5171037 Vali Loss: 0.2882765 Test Loss: 0.3589673
Validation loss decreased (0.288496 --> 0.288276).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.181412935256958
Epoch: 19, Steps: 63 | Train Loss: 0.5158085 Vali Loss: 0.2876782 Test Loss: 0.3586375
Validation loss decreased (0.288276 --> 0.287678).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.1541056632995605
Epoch: 20, Steps: 63 | Train Loss: 0.5162939 Vali Loss: 0.2875781 Test Loss: 0.3586489
Validation loss decreased (0.287678 --> 0.287578).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0075061321258545
Epoch: 21, Steps: 63 | Train Loss: 0.5142970 Vali Loss: 0.2874036 Test Loss: 0.3584297
Validation loss decreased (0.287578 --> 0.287404).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.185861349105835
Epoch: 22, Steps: 63 | Train Loss: 0.5138792 Vali Loss: 0.2871449 Test Loss: 0.3583416
Validation loss decreased (0.287404 --> 0.287145).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9516024589538574
Epoch: 23, Steps: 63 | Train Loss: 0.5142483 Vali Loss: 0.2869409 Test Loss: 0.3581483
Validation loss decreased (0.287145 --> 0.286941).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.9980719089508057
Epoch: 24, Steps: 63 | Train Loss: 0.5139549 Vali Loss: 0.2867102 Test Loss: 0.3580428
Validation loss decreased (0.286941 --> 0.286710).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9054951667785645
Epoch: 25, Steps: 63 | Train Loss: 0.5147628 Vali Loss: 0.2865994 Test Loss: 0.3579391
Validation loss decreased (0.286710 --> 0.286599).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.134669780731201
Epoch: 26, Steps: 63 | Train Loss: 0.5134123 Vali Loss: 0.2864435 Test Loss: 0.3577689
Validation loss decreased (0.286599 --> 0.286443).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.153310775756836
Epoch: 27, Steps: 63 | Train Loss: 0.5131334 Vali Loss: 0.2862575 Test Loss: 0.3577678
Validation loss decreased (0.286443 --> 0.286257).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.022982835769653
Epoch: 28, Steps: 63 | Train Loss: 0.5134249 Vali Loss: 0.2862631 Test Loss: 0.3576125
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 5.844522953033447
Epoch: 29, Steps: 63 | Train Loss: 0.5141514 Vali Loss: 0.2859140 Test Loss: 0.3575640
Validation loss decreased (0.286257 --> 0.285914).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 5.473047494888306
Epoch: 30, Steps: 63 | Train Loss: 0.5135698 Vali Loss: 0.2859346 Test Loss: 0.3574777
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 6.226696252822876
Epoch: 31, Steps: 63 | Train Loss: 0.5130278 Vali Loss: 0.2857811 Test Loss: 0.3574227
Validation loss decreased (0.285914 --> 0.285781).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 6.812716960906982
Epoch: 32, Steps: 63 | Train Loss: 0.5135471 Vali Loss: 0.2856054 Test Loss: 0.3573949
Validation loss decreased (0.285781 --> 0.285605).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 6.4424309730529785
Epoch: 33, Steps: 63 | Train Loss: 0.5124592 Vali Loss: 0.2855747 Test Loss: 0.3573975
Validation loss decreased (0.285605 --> 0.285575).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 6.120120048522949
Epoch: 34, Steps: 63 | Train Loss: 0.5128908 Vali Loss: 0.2855038 Test Loss: 0.3572524
Validation loss decreased (0.285575 --> 0.285504).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.448777914047241
Epoch: 35, Steps: 63 | Train Loss: 0.5123722 Vali Loss: 0.2853946 Test Loss: 0.3572007
Validation loss decreased (0.285504 --> 0.285395).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 6.043193101882935
Epoch: 36, Steps: 63 | Train Loss: 0.5132618 Vali Loss: 0.2849854 Test Loss: 0.3572016
Validation loss decreased (0.285395 --> 0.284985).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 7.028861045837402
Epoch: 37, Steps: 63 | Train Loss: 0.5127500 Vali Loss: 0.2852345 Test Loss: 0.3571211
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 5.770964860916138
Epoch: 38, Steps: 63 | Train Loss: 0.5114603 Vali Loss: 0.2851409 Test Loss: 0.3571228
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 5.100651264190674
Epoch: 39, Steps: 63 | Train Loss: 0.5120239 Vali Loss: 0.2847735 Test Loss: 0.3570884
Validation loss decreased (0.284985 --> 0.284773).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 5.111029148101807
Epoch: 40, Steps: 63 | Train Loss: 0.5121257 Vali Loss: 0.2850259 Test Loss: 0.3570457
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 7.616691827774048
Epoch: 41, Steps: 63 | Train Loss: 0.5128418 Vali Loss: 0.2850419 Test Loss: 0.3569511
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 6.207988262176514
Epoch: 42, Steps: 63 | Train Loss: 0.5122127 Vali Loss: 0.2848879 Test Loss: 0.3569837
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.2716970443725586
Epoch: 43, Steps: 63 | Train Loss: 0.5120998 Vali Loss: 0.2849170 Test Loss: 0.3569355
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.086505651473999
Epoch: 44, Steps: 63 | Train Loss: 0.5119619 Vali Loss: 0.2848901 Test Loss: 0.3568994
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.4304497241973877
Epoch: 45, Steps: 63 | Train Loss: 0.5118184 Vali Loss: 0.2848473 Test Loss: 0.3568535
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.0046651363372803
Epoch: 46, Steps: 63 | Train Loss: 0.5119929 Vali Loss: 0.2847636 Test Loss: 0.3568682
Validation loss decreased (0.284773 --> 0.284764).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.141951560974121
Epoch: 47, Steps: 63 | Train Loss: 0.5111777 Vali Loss: 0.2847033 Test Loss: 0.3568530
Validation loss decreased (0.284764 --> 0.284703).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.06620717048645
Epoch: 48, Steps: 63 | Train Loss: 0.5113510 Vali Loss: 0.2847436 Test Loss: 0.3567963
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9698479175567627
Epoch: 49, Steps: 63 | Train Loss: 0.5116692 Vali Loss: 0.2845920 Test Loss: 0.3568233
Validation loss decreased (0.284703 --> 0.284592).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.9433050155639648
Epoch: 50, Steps: 63 | Train Loss: 0.5110204 Vali Loss: 0.2844813 Test Loss: 0.3567760
Validation loss decreased (0.284592 --> 0.284481).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9453959465026855
Epoch: 51, Steps: 63 | Train Loss: 0.5120232 Vali Loss: 0.2846299 Test Loss: 0.3567555
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.0170600414276123
Epoch: 52, Steps: 63 | Train Loss: 0.5123567 Vali Loss: 0.2845278 Test Loss: 0.3567738
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.6553988456726074
Epoch: 53, Steps: 63 | Train Loss: 0.5116684 Vali Loss: 0.2842464 Test Loss: 0.3567334
Validation loss decreased (0.284481 --> 0.284246).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9660048484802246
Epoch: 54, Steps: 63 | Train Loss: 0.5114975 Vali Loss: 0.2845079 Test Loss: 0.3567392
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.173137903213501
Epoch: 55, Steps: 63 | Train Loss: 0.5120360 Vali Loss: 0.2844862 Test Loss: 0.3566945
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.6654644012451172
Epoch: 56, Steps: 63 | Train Loss: 0.5111651 Vali Loss: 0.2844307 Test Loss: 0.3566820
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.025969982147217
Epoch: 57, Steps: 63 | Train Loss: 0.5118184 Vali Loss: 0.2844453 Test Loss: 0.3567036
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.2671773433685303
Epoch: 58, Steps: 63 | Train Loss: 0.5116666 Vali Loss: 0.2843953 Test Loss: 0.3566748
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.0282843112945557
Epoch: 59, Steps: 63 | Train Loss: 0.5112682 Vali Loss: 0.2843622 Test Loss: 0.3566562
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.2595553398132324
Epoch: 60, Steps: 63 | Train Loss: 0.5086975 Vali Loss: 0.2843211 Test Loss: 0.3566628
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.8122410774230957
Epoch: 61, Steps: 63 | Train Loss: 0.5112000 Vali Loss: 0.2843274 Test Loss: 0.3566670
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.8931536674499512
Epoch: 62, Steps: 63 | Train Loss: 0.5116738 Vali Loss: 0.2843458 Test Loss: 0.3566494
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.894275426864624
Epoch: 63, Steps: 63 | Train Loss: 0.5111894 Vali Loss: 0.2843334 Test Loss: 0.3566402
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.9434211254119873
Epoch: 64, Steps: 63 | Train Loss: 0.5106460 Vali Loss: 0.2842904 Test Loss: 0.3566422
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.8338725566864014
Epoch: 65, Steps: 63 | Train Loss: 0.5116386 Vali Loss: 0.2842552 Test Loss: 0.3566448
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.0118372440338135
Epoch: 66, Steps: 63 | Train Loss: 0.5109554 Vali Loss: 0.2842303 Test Loss: 0.3566133
Validation loss decreased (0.284246 --> 0.284230).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.0770676136016846
Epoch: 67, Steps: 63 | Train Loss: 0.5107203 Vali Loss: 0.2841989 Test Loss: 0.3566084
Validation loss decreased (0.284230 --> 0.284199).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.0687713623046875
Epoch: 68, Steps: 63 | Train Loss: 0.5102453 Vali Loss: 0.2842324 Test Loss: 0.3566165
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.9553377628326416
Epoch: 69, Steps: 63 | Train Loss: 0.5113780 Vali Loss: 0.2839493 Test Loss: 0.3566124
Validation loss decreased (0.284199 --> 0.283949).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.8250479698181152
Epoch: 70, Steps: 63 | Train Loss: 0.5108913 Vali Loss: 0.2842417 Test Loss: 0.3566043
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.9482057094573975
Epoch: 71, Steps: 63 | Train Loss: 0.5115852 Vali Loss: 0.2840674 Test Loss: 0.3566087
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.8667685985565186
Epoch: 72, Steps: 63 | Train Loss: 0.5108165 Vali Loss: 0.2841931 Test Loss: 0.3565979
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.1348743438720703
Epoch: 73, Steps: 63 | Train Loss: 0.5116904 Vali Loss: 0.2841699 Test Loss: 0.3565918
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.181816339492798
Epoch: 74, Steps: 63 | Train Loss: 0.5115966 Vali Loss: 0.2841562 Test Loss: 0.3565951
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.966691017150879
Epoch: 75, Steps: 63 | Train Loss: 0.5112511 Vali Loss: 0.2840912 Test Loss: 0.3565869
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.8353931903839111
Epoch: 76, Steps: 63 | Train Loss: 0.5099906 Vali Loss: 0.2841724 Test Loss: 0.3565749
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.7141919136047363
Epoch: 77, Steps: 63 | Train Loss: 0.5115116 Vali Loss: 0.2840087 Test Loss: 0.3565823
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.090669631958008
Epoch: 78, Steps: 63 | Train Loss: 0.5116208 Vali Loss: 0.2841114 Test Loss: 0.3565740
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.1578190326690674
Epoch: 79, Steps: 63 | Train Loss: 0.5116738 Vali Loss: 0.2837818 Test Loss: 0.3565627
Validation loss decreased (0.283949 --> 0.283782).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.4386675357818604
Epoch: 80, Steps: 63 | Train Loss: 0.5105839 Vali Loss: 0.2841369 Test Loss: 0.3565632
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.208873748779297
Epoch: 81, Steps: 63 | Train Loss: 0.5114021 Vali Loss: 0.2840010 Test Loss: 0.3565594
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.6913657188415527
Epoch: 82, Steps: 63 | Train Loss: 0.5117312 Vali Loss: 0.2841440 Test Loss: 0.3565586
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 5.492264747619629
Epoch: 83, Steps: 63 | Train Loss: 0.5111343 Vali Loss: 0.2841112 Test Loss: 0.3565545
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 5.944532632827759
Epoch: 84, Steps: 63 | Train Loss: 0.5118030 Vali Loss: 0.2840343 Test Loss: 0.3565517
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 5.504555702209473
Epoch: 85, Steps: 63 | Train Loss: 0.5113906 Vali Loss: 0.2840593 Test Loss: 0.3565511
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 4.70482325553894
Epoch: 86, Steps: 63 | Train Loss: 0.5104208 Vali Loss: 0.2840695 Test Loss: 0.3565453
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 6.4534032344818115
Epoch: 87, Steps: 63 | Train Loss: 0.5106820 Vali Loss: 0.2840574 Test Loss: 0.3565430
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 5.575458765029907
Epoch: 88, Steps: 63 | Train Loss: 0.5108469 Vali Loss: 0.2840801 Test Loss: 0.3565428
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 6.059575319290161
Epoch: 89, Steps: 63 | Train Loss: 0.5107894 Vali Loss: 0.2840700 Test Loss: 0.3565418
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 4.294885873794556
Epoch: 90, Steps: 63 | Train Loss: 0.5102536 Vali Loss: 0.2840803 Test Loss: 0.3565421
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 5.533648729324341
Epoch: 91, Steps: 63 | Train Loss: 0.5114772 Vali Loss: 0.2840815 Test Loss: 0.3565362
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 5.117345571517944
Epoch: 92, Steps: 63 | Train Loss: 0.5111072 Vali Loss: 0.2840792 Test Loss: 0.3565407
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 6.325967311859131
Epoch: 93, Steps: 63 | Train Loss: 0.5113780 Vali Loss: 0.2840507 Test Loss: 0.3565368
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 3.5624959468841553
Epoch: 94, Steps: 63 | Train Loss: 0.5090513 Vali Loss: 0.2836756 Test Loss: 0.3565347
Validation loss decreased (0.283782 --> 0.283676).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 6.36180305480957
Epoch: 95, Steps: 63 | Train Loss: 0.5110808 Vali Loss: 0.2840800 Test Loss: 0.3565329
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 5.51995325088501
Epoch: 96, Steps: 63 | Train Loss: 0.5110024 Vali Loss: 0.2840628 Test Loss: 0.3565311
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 4.6556501388549805
Epoch: 97, Steps: 63 | Train Loss: 0.5109217 Vali Loss: 0.2840318 Test Loss: 0.3565326
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.58184552192688
Epoch: 98, Steps: 63 | Train Loss: 0.5107215 Vali Loss: 0.2840268 Test Loss: 0.3565330
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.9858241081237793
Epoch: 99, Steps: 63 | Train Loss: 0.5105598 Vali Loss: 0.2840855 Test Loss: 0.3565287
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.299269199371338
Epoch: 100, Steps: 63 | Train Loss: 0.5106300 Vali Loss: 0.2839676 Test Loss: 0.3565297
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_360_192_FITS_ETTh2_ftM_sl360_ll48_pl192_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.33776727318763733, mae:0.37778240442276, rse:0.4660683870315552, corr:[0.26408538 0.26685265 0.26807782 0.26716098 0.2650808  0.26325497
 0.26213625 0.26134154 0.26055056 0.25935698 0.2578482  0.25615838
 0.25474826 0.25377074 0.25311604 0.2526694  0.2520968  0.25129145
 0.2501588  0.24873865 0.24714065 0.24537434 0.24350198 0.24143164
 0.23935011 0.23733154 0.23557377 0.23394786 0.23238277 0.23086302
 0.22946827 0.22794016 0.22631036 0.22459228 0.22302471 0.22168104
 0.22054002 0.21955943 0.21883443 0.2181304  0.21733247 0.21635312
 0.21511605 0.21372765 0.21230754 0.21097071 0.20963159 0.20817095
 0.20654546 0.20487706 0.20324403 0.20161001 0.1999068  0.19816251
 0.1963309  0.19454162 0.19306323 0.19169709 0.19063182 0.18992862
 0.18968119 0.18951963 0.18939961 0.18915945 0.18867733 0.18806706
 0.18726169 0.18630402 0.1853305  0.18442334 0.18363558 0.18296443
 0.18225172 0.18150441 0.18066408 0.17970793 0.17892013 0.17834054
 0.17784874 0.17736483 0.17713696 0.17683323 0.1765244  0.1761434
 0.17585368 0.17559876 0.17543846 0.1753468  0.17517301 0.17500697
 0.17472559 0.17427541 0.1738479  0.17330477 0.17269672 0.17204812
 0.17139944 0.1706387  0.16983935 0.16890608 0.16792502 0.16704406
 0.16646793 0.1660274  0.1658614  0.16584757 0.16574235 0.16565135
 0.16521688 0.16460924 0.16389373 0.16348143 0.1631395  0.16292463
 0.16270252 0.16218556 0.16147594 0.16037335 0.15897955 0.15742964
 0.1560474  0.1547046  0.15365389 0.15292197 0.1521824  0.15137587
 0.15060267 0.1498813  0.14921522 0.14856757 0.14813147 0.14777227
 0.14752789 0.14714944 0.1466304  0.14613281 0.14571002 0.14551568
 0.14527883 0.14514147 0.14500703 0.14447358 0.14338802 0.14198157
 0.1404516  0.13889419 0.13744813 0.13633516 0.13568835 0.13525556
 0.13520263 0.13500386 0.1348863  0.13483015 0.13474223 0.13450806
 0.13424608 0.13424987 0.13407315 0.13408288 0.13417631 0.13435899
 0.13460806 0.13473558 0.1347067  0.13424583 0.13352874 0.1323795
 0.1312293  0.1302106  0.12922102 0.12820928 0.12710632 0.1257328
 0.12419512 0.122673   0.12146089 0.1207237  0.1204796  0.12029963
 0.12008227 0.11952628 0.11834168 0.1168213  0.11503565 0.11338865
 0.11273824 0.11284537 0.11313241 0.11254571 0.10987712 0.10418443]
