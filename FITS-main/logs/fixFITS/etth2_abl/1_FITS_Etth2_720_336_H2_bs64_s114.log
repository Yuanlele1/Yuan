Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=72, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=72, out_features=105, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6773760.0
params:  7665.0
Trainable parameters:  7665
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.7896673679351807
Epoch: 1, Steps: 59 | Train Loss: 0.8612733 Vali Loss: 0.5360797 Test Loss: 0.4057252
Validation loss decreased (inf --> 0.536080).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.64479923248291
Epoch: 2, Steps: 59 | Train Loss: 0.7258017 Vali Loss: 0.4736357 Test Loss: 0.3787817
Validation loss decreased (0.536080 --> 0.473636).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.205018997192383
Epoch: 3, Steps: 59 | Train Loss: 0.6852146 Vali Loss: 0.4474451 Test Loss: 0.3709079
Validation loss decreased (0.473636 --> 0.447445).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.319460868835449
Epoch: 4, Steps: 59 | Train Loss: 0.6662843 Vali Loss: 0.4321879 Test Loss: 0.3675461
Validation loss decreased (0.447445 --> 0.432188).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.3226935863494873
Epoch: 5, Steps: 59 | Train Loss: 0.6533744 Vali Loss: 0.4242907 Test Loss: 0.3654917
Validation loss decreased (0.432188 --> 0.424291).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.611768960952759
Epoch: 6, Steps: 59 | Train Loss: 0.6486317 Vali Loss: 0.4146698 Test Loss: 0.3640842
Validation loss decreased (0.424291 --> 0.414670).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.2425742149353027
Epoch: 7, Steps: 59 | Train Loss: 0.6451703 Vali Loss: 0.4091509 Test Loss: 0.3632224
Validation loss decreased (0.414670 --> 0.409151).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.9421887397766113
Epoch: 8, Steps: 59 | Train Loss: 0.6406907 Vali Loss: 0.4063697 Test Loss: 0.3625803
Validation loss decreased (0.409151 --> 0.406370).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.4739553928375244
Epoch: 9, Steps: 59 | Train Loss: 0.6378486 Vali Loss: 0.4046295 Test Loss: 0.3622189
Validation loss decreased (0.406370 --> 0.404629).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.667191982269287
Epoch: 10, Steps: 59 | Train Loss: 0.6331757 Vali Loss: 0.3998492 Test Loss: 0.3617960
Validation loss decreased (0.404629 --> 0.399849).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.2306973934173584
Epoch: 11, Steps: 59 | Train Loss: 0.6338069 Vali Loss: 0.4012630 Test Loss: 0.3616143
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.0773215293884277
Epoch: 12, Steps: 59 | Train Loss: 0.6317524 Vali Loss: 0.3969967 Test Loss: 0.3613608
Validation loss decreased (0.399849 --> 0.396997).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.033414125442505
Epoch: 13, Steps: 59 | Train Loss: 0.6319956 Vali Loss: 0.3963378 Test Loss: 0.3613804
Validation loss decreased (0.396997 --> 0.396338).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.7980237007141113
Epoch: 14, Steps: 59 | Train Loss: 0.6297658 Vali Loss: 0.3973091 Test Loss: 0.3611744
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.062509059906006
Epoch: 15, Steps: 59 | Train Loss: 0.6297431 Vali Loss: 0.3973807 Test Loss: 0.3612259
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.8035898208618164
Epoch: 16, Steps: 59 | Train Loss: 0.6272333 Vali Loss: 0.3949209 Test Loss: 0.3611631
Validation loss decreased (0.396338 --> 0.394921).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.082613945007324
Epoch: 17, Steps: 59 | Train Loss: 0.6279339 Vali Loss: 0.3938733 Test Loss: 0.3609931
Validation loss decreased (0.394921 --> 0.393873).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.3769359588623047
Epoch: 18, Steps: 59 | Train Loss: 0.6261177 Vali Loss: 0.3934051 Test Loss: 0.3608196
Validation loss decreased (0.393873 --> 0.393405).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.6180038452148438
Epoch: 19, Steps: 59 | Train Loss: 0.6260684 Vali Loss: 0.3945720 Test Loss: 0.3608224
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.5633552074432373
Epoch: 20, Steps: 59 | Train Loss: 0.6250048 Vali Loss: 0.3933527 Test Loss: 0.3607968
Validation loss decreased (0.393405 --> 0.393353).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.3895559310913086
Epoch: 21, Steps: 59 | Train Loss: 0.6242065 Vali Loss: 0.3927603 Test Loss: 0.3607997
Validation loss decreased (0.393353 --> 0.392760).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0484399795532227
Epoch: 22, Steps: 59 | Train Loss: 0.6234272 Vali Loss: 0.3923503 Test Loss: 0.3608691
Validation loss decreased (0.392760 --> 0.392350).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.856841564178467
Epoch: 23, Steps: 59 | Train Loss: 0.6246959 Vali Loss: 0.3905302 Test Loss: 0.3606920
Validation loss decreased (0.392350 --> 0.390530).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.6839101314544678
Epoch: 24, Steps: 59 | Train Loss: 0.6222404 Vali Loss: 0.3891228 Test Loss: 0.3607568
Validation loss decreased (0.390530 --> 0.389123).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.4046523571014404
Epoch: 25, Steps: 59 | Train Loss: 0.6229109 Vali Loss: 0.3871319 Test Loss: 0.3606935
Validation loss decreased (0.389123 --> 0.387132).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.7431819438934326
Epoch: 26, Steps: 59 | Train Loss: 0.6220735 Vali Loss: 0.3882812 Test Loss: 0.3606949
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.564793109893799
Epoch: 27, Steps: 59 | Train Loss: 0.6230627 Vali Loss: 0.3882571 Test Loss: 0.3607532
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.7459168434143066
Epoch: 28, Steps: 59 | Train Loss: 0.6229377 Vali Loss: 0.3888283 Test Loss: 0.3606945
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.5086660385131836
Epoch: 29, Steps: 59 | Train Loss: 0.6230362 Vali Loss: 0.3885623 Test Loss: 0.3606804
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.5018258094787598
Epoch: 30, Steps: 59 | Train Loss: 0.6222751 Vali Loss: 0.3888914 Test Loss: 0.3606229
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.380073070526123
Epoch: 31, Steps: 59 | Train Loss: 0.6227692 Vali Loss: 0.3867534 Test Loss: 0.3605865
Validation loss decreased (0.387132 --> 0.386753).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.2873146533966064
Epoch: 32, Steps: 59 | Train Loss: 0.6223681 Vali Loss: 0.3865428 Test Loss: 0.3605988
Validation loss decreased (0.386753 --> 0.386543).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.340961217880249
Epoch: 33, Steps: 59 | Train Loss: 0.6223158 Vali Loss: 0.3888102 Test Loss: 0.3606602
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.479158639907837
Epoch: 34, Steps: 59 | Train Loss: 0.6212950 Vali Loss: 0.3873073 Test Loss: 0.3605834
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.8381643295288086
Epoch: 35, Steps: 59 | Train Loss: 0.6217213 Vali Loss: 0.3838902 Test Loss: 0.3606200
Validation loss decreased (0.386543 --> 0.383890).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.4780068397521973
Epoch: 36, Steps: 59 | Train Loss: 0.6210695 Vali Loss: 0.3867551 Test Loss: 0.3606239
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.133258581161499
Epoch: 37, Steps: 59 | Train Loss: 0.6204689 Vali Loss: 0.3868804 Test Loss: 0.3605552
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.1731576919555664
Epoch: 38, Steps: 59 | Train Loss: 0.6214709 Vali Loss: 0.3869178 Test Loss: 0.3605772
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.722534656524658
Epoch: 39, Steps: 59 | Train Loss: 0.6214425 Vali Loss: 0.3843466 Test Loss: 0.3605638
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.9629123210906982
Epoch: 40, Steps: 59 | Train Loss: 0.6209469 Vali Loss: 0.3869342 Test Loss: 0.3605682
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.2268385887145996
Epoch: 41, Steps: 59 | Train Loss: 0.6206770 Vali Loss: 0.3887463 Test Loss: 0.3605438
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.3591129779815674
Epoch: 42, Steps: 59 | Train Loss: 0.6211632 Vali Loss: 0.3889164 Test Loss: 0.3605814
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.866644859313965
Epoch: 43, Steps: 59 | Train Loss: 0.6194070 Vali Loss: 0.3890778 Test Loss: 0.3605924
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.7910544872283936
Epoch: 44, Steps: 59 | Train Loss: 0.6185970 Vali Loss: 0.3868548 Test Loss: 0.3605506
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.5017027854919434
Epoch: 45, Steps: 59 | Train Loss: 0.6195475 Vali Loss: 0.3867490 Test Loss: 0.3606020
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.29895281791687
Epoch: 46, Steps: 59 | Train Loss: 0.6212989 Vali Loss: 0.3855485 Test Loss: 0.3605825
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.3525381088256836
Epoch: 47, Steps: 59 | Train Loss: 0.6194145 Vali Loss: 0.3861188 Test Loss: 0.3605937
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.9607012271881104
Epoch: 48, Steps: 59 | Train Loss: 0.6200940 Vali Loss: 0.3864080 Test Loss: 0.3605453
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.612534999847412
Epoch: 49, Steps: 59 | Train Loss: 0.6198696 Vali Loss: 0.3854826 Test Loss: 0.3605787
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3974971771240234
Epoch: 50, Steps: 59 | Train Loss: 0.6210115 Vali Loss: 0.3857716 Test Loss: 0.3605577
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.4357237815856934
Epoch: 51, Steps: 59 | Train Loss: 0.6209307 Vali Loss: 0.3864685 Test Loss: 0.3605618
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.8442673683166504
Epoch: 52, Steps: 59 | Train Loss: 0.6190311 Vali Loss: 0.3864890 Test Loss: 0.3605484
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.4872071743011475
Epoch: 53, Steps: 59 | Train Loss: 0.6205170 Vali Loss: 0.3864140 Test Loss: 0.3605231
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.7447891235351562
Epoch: 54, Steps: 59 | Train Loss: 0.6195728 Vali Loss: 0.3848663 Test Loss: 0.3605581
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.094827890396118
Epoch: 55, Steps: 59 | Train Loss: 0.6190529 Vali Loss: 0.3855878 Test Loss: 0.3605209
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35639017820358276, mae:0.39704814553260803, rse:0.47731202840805054, corr:[0.2562227  0.26023772 0.2629418  0.26326236 0.2615209  0.25932178
 0.2577155  0.25682467 0.2563689  0.25592557 0.2553388  0.25445232
 0.25338605 0.2520549  0.25076428 0.24971472 0.24893223 0.24826793
 0.24767955 0.24698123 0.24603519 0.2447629  0.24317233 0.24162793
 0.24031436 0.2393821  0.23866104 0.23794265 0.23709239 0.23605129
 0.23488124 0.2336982  0.23261105 0.23167065 0.23089977 0.23031892
 0.2298328  0.22937375 0.2288683  0.22821182 0.22731519 0.22614177
 0.22485073 0.2235829  0.22231029 0.22111751 0.21990564 0.2185998
 0.21720988 0.21578519 0.2142579  0.21254894 0.21076995 0.20899507
 0.20719726 0.20564578 0.20438343 0.2033516  0.20256051 0.20194261
 0.20144145 0.20100948 0.20054547 0.20009378 0.1995559  0.1990971
 0.19863892 0.19823548 0.19774164 0.19721898 0.1964757  0.19553447
 0.19446965 0.19336648 0.19234772 0.19138318 0.19059312 0.1899676
 0.1895458  0.18926378 0.18910125 0.18892835 0.18862025 0.18813916
 0.1875804  0.18698266 0.18640706 0.18591161 0.18553096 0.18531783
 0.18531175 0.18527865 0.18525805 0.18509792 0.18468928 0.18407406
 0.18336041 0.18263131 0.18208043 0.18164068 0.1812807  0.18089344
 0.18060388 0.18024121 0.17990096 0.17946652 0.17908488 0.1786799
 0.17815481 0.17755483 0.17694198 0.17642078 0.17585813 0.17534807
 0.17479669 0.17418271 0.17352916 0.1726703  0.1716952  0.17053933
 0.16927776 0.16802263 0.16689317 0.16599655 0.16525388 0.1647273
 0.16425276 0.16389047 0.16353662 0.1630286  0.16234218 0.16140147
 0.16037801 0.15932478 0.15846488 0.15790837 0.15761665 0.15750599
 0.15736787 0.15700513 0.15625519 0.15503597 0.1534664  0.15175831
 0.15016434 0.14893615 0.14811239 0.14752892 0.14695051 0.14626507
 0.14541434 0.14438626 0.14339447 0.14252785 0.14187157 0.14129162
 0.14094995 0.14081171 0.14075412 0.14056155 0.14006941 0.13937882
 0.13863978 0.1379411  0.13738443 0.13688631 0.1364385  0.1358761
 0.13520373 0.13431366 0.13315266 0.13169198 0.1300667  0.12836014
 0.12695749 0.1258805  0.12529594 0.12512945 0.12501673 0.12480109
 0.12438759 0.12373764 0.12296213 0.12227437 0.12165202 0.12137774
 0.1215388  0.12198582 0.12256286 0.12296552 0.12308894 0.12260713
 0.12171508 0.12066315 0.11961134 0.11886254 0.11845202 0.11830409
 0.11833847 0.11831373 0.11813566 0.1176183  0.11681306 0.11581465
 0.11492203 0.11448937 0.11453991 0.11504157 0.1156835  0.11629678
 0.11657933 0.1166067  0.11632965 0.11576372 0.11503074 0.11415888
 0.11335143 0.11274031 0.11239304 0.11234906 0.11230016 0.11232472
 0.11202959 0.1115951  0.11094537 0.11022214 0.10968557 0.10935019
 0.10921592 0.10926162 0.10931196 0.10944451 0.10950002 0.10961565
 0.1097763  0.1101533  0.11064631 0.11120299 0.11165566 0.11178843
 0.11141922 0.11062202 0.10959777 0.10880259 0.1082681  0.10796433
 0.10807931 0.10846917 0.10905182 0.10962339 0.11008352 0.11042706
 0.11062394 0.11096951 0.11126335 0.11194425 0.11262755 0.11361726
 0.11439976 0.11510107 0.11582192 0.11617198 0.11644521 0.11650973
 0.11654861 0.1164694  0.11659698 0.11700814 0.11740066 0.11763141
 0.11755358 0.11713967 0.11659834 0.11581037 0.1153848  0.11522787
 0.11547399 0.11592811 0.11647947 0.11696626 0.11720213 0.1170902
 0.11666176 0.11618374 0.11595286 0.11597536 0.11639535 0.11703047
 0.11747933 0.11744034 0.116757   0.11569997 0.11430748 0.11265626
 0.1113176  0.11062518 0.11070294 0.11115988 0.11185356 0.11245215
 0.11251327 0.1124595  0.11232667 0.11244942 0.11245987 0.11301538
 0.11376916 0.11485269 0.11585654 0.11644717 0.11641899 0.11559974
 0.11443941 0.11308204 0.11215527 0.11175179 0.11182515 0.11212932
 0.11247575 0.11265258 0.11217865 0.11150488 0.1110201  0.11144384
 0.1123723  0.1137367  0.1156277  0.11745921 0.11855912 0.11854286
 0.11751602 0.11620801 0.11579549 0.11673995 0.11964586 0.12315107]
