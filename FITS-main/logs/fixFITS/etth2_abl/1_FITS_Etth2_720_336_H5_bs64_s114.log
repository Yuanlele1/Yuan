Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  35629440.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7430429458618164
Epoch: 1, Steps: 59 | Train Loss: 0.8550550 Vali Loss: 0.5226097 Test Loss: 0.3953645
Validation loss decreased (inf --> 0.522610).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.932563304901123
Epoch: 2, Steps: 59 | Train Loss: 0.7102910 Vali Loss: 0.4661221 Test Loss: 0.3747040
Validation loss decreased (0.522610 --> 0.466122).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.9602911472320557
Epoch: 3, Steps: 59 | Train Loss: 0.6756354 Vali Loss: 0.4381624 Test Loss: 0.3686550
Validation loss decreased (0.466122 --> 0.438162).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0143582820892334
Epoch: 4, Steps: 59 | Train Loss: 0.6594466 Vali Loss: 0.4259675 Test Loss: 0.3656755
Validation loss decreased (0.438162 --> 0.425968).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.104564905166626
Epoch: 5, Steps: 59 | Train Loss: 0.6508301 Vali Loss: 0.4174989 Test Loss: 0.3637617
Validation loss decreased (0.425968 --> 0.417499).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.817286729812622
Epoch: 6, Steps: 59 | Train Loss: 0.6432020 Vali Loss: 0.4124092 Test Loss: 0.3628269
Validation loss decreased (0.417499 --> 0.412409).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9190797805786133
Epoch: 7, Steps: 59 | Train Loss: 0.6389182 Vali Loss: 0.4075470 Test Loss: 0.3622109
Validation loss decreased (0.412409 --> 0.407547).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.6413888931274414
Epoch: 8, Steps: 59 | Train Loss: 0.6368037 Vali Loss: 0.4044042 Test Loss: 0.3613988
Validation loss decreased (0.407547 --> 0.404404).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.881159782409668
Epoch: 9, Steps: 59 | Train Loss: 0.6335835 Vali Loss: 0.4025218 Test Loss: 0.3608659
Validation loss decreased (0.404404 --> 0.402522).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.731865406036377
Epoch: 10, Steps: 59 | Train Loss: 0.6311182 Vali Loss: 0.4013847 Test Loss: 0.3607027
Validation loss decreased (0.402522 --> 0.401385).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7388174533843994
Epoch: 11, Steps: 59 | Train Loss: 0.6297234 Vali Loss: 0.3962263 Test Loss: 0.3602685
Validation loss decreased (0.401385 --> 0.396226).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.020982265472412
Epoch: 12, Steps: 59 | Train Loss: 0.6288027 Vali Loss: 0.3965916 Test Loss: 0.3601384
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6088790893554688
Epoch: 13, Steps: 59 | Train Loss: 0.6272803 Vali Loss: 0.3967232 Test Loss: 0.3600143
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.8237147331237793
Epoch: 14, Steps: 59 | Train Loss: 0.6257259 Vali Loss: 0.3952210 Test Loss: 0.3598258
Validation loss decreased (0.396226 --> 0.395221).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.8942577838897705
Epoch: 15, Steps: 59 | Train Loss: 0.6236907 Vali Loss: 0.3943089 Test Loss: 0.3599837
Validation loss decreased (0.395221 --> 0.394309).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7545146942138672
Epoch: 16, Steps: 59 | Train Loss: 0.6233022 Vali Loss: 0.3906426 Test Loss: 0.3596779
Validation loss decreased (0.394309 --> 0.390643).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.7331178188323975
Epoch: 17, Steps: 59 | Train Loss: 0.6207870 Vali Loss: 0.3917469 Test Loss: 0.3597150
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8150746822357178
Epoch: 18, Steps: 59 | Train Loss: 0.6236103 Vali Loss: 0.3921944 Test Loss: 0.3596752
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9674780368804932
Epoch: 19, Steps: 59 | Train Loss: 0.6214478 Vali Loss: 0.3899033 Test Loss: 0.3595518
Validation loss decreased (0.390643 --> 0.389903).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.7915422916412354
Epoch: 20, Steps: 59 | Train Loss: 0.6200013 Vali Loss: 0.3882526 Test Loss: 0.3595422
Validation loss decreased (0.389903 --> 0.388253).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.7679016590118408
Epoch: 21, Steps: 59 | Train Loss: 0.6213293 Vali Loss: 0.3887574 Test Loss: 0.3594255
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.8021039962768555
Epoch: 22, Steps: 59 | Train Loss: 0.6203684 Vali Loss: 0.3873411 Test Loss: 0.3593754
Validation loss decreased (0.388253 --> 0.387341).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6247601509094238
Epoch: 23, Steps: 59 | Train Loss: 0.6210719 Vali Loss: 0.3869786 Test Loss: 0.3592744
Validation loss decreased (0.387341 --> 0.386979).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8168373107910156
Epoch: 24, Steps: 59 | Train Loss: 0.6201193 Vali Loss: 0.3876141 Test Loss: 0.3592996
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9047884941101074
Epoch: 25, Steps: 59 | Train Loss: 0.6195290 Vali Loss: 0.3868602 Test Loss: 0.3592896
Validation loss decreased (0.386979 --> 0.386860).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.785749912261963
Epoch: 26, Steps: 59 | Train Loss: 0.6184791 Vali Loss: 0.3855682 Test Loss: 0.3593429
Validation loss decreased (0.386860 --> 0.385568).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.695131540298462
Epoch: 27, Steps: 59 | Train Loss: 0.6186435 Vali Loss: 0.3872643 Test Loss: 0.3593284
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.8852076530456543
Epoch: 28, Steps: 59 | Train Loss: 0.6189638 Vali Loss: 0.3876514 Test Loss: 0.3592314
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.9786922931671143
Epoch: 29, Steps: 59 | Train Loss: 0.6188754 Vali Loss: 0.3853487 Test Loss: 0.3591636
Validation loss decreased (0.385568 --> 0.385349).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9358623027801514
Epoch: 30, Steps: 59 | Train Loss: 0.6168589 Vali Loss: 0.3872956 Test Loss: 0.3592184
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.4244892597198486
Epoch: 31, Steps: 59 | Train Loss: 0.6166000 Vali Loss: 0.3865538 Test Loss: 0.3591042
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.924401044845581
Epoch: 32, Steps: 59 | Train Loss: 0.6172973 Vali Loss: 0.3850054 Test Loss: 0.3591438
Validation loss decreased (0.385349 --> 0.385005).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.979151725769043
Epoch: 33, Steps: 59 | Train Loss: 0.6153629 Vali Loss: 0.3843718 Test Loss: 0.3591201
Validation loss decreased (0.385005 --> 0.384372).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7923061847686768
Epoch: 34, Steps: 59 | Train Loss: 0.6167113 Vali Loss: 0.3831224 Test Loss: 0.3591432
Validation loss decreased (0.384372 --> 0.383122).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.742532730102539
Epoch: 35, Steps: 59 | Train Loss: 0.6174585 Vali Loss: 0.3853998 Test Loss: 0.3591174
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.8795688152313232
Epoch: 36, Steps: 59 | Train Loss: 0.6172344 Vali Loss: 0.3836102 Test Loss: 0.3590327
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.772261619567871
Epoch: 37, Steps: 59 | Train Loss: 0.6181364 Vali Loss: 0.3833114 Test Loss: 0.3590754
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7640724182128906
Epoch: 38, Steps: 59 | Train Loss: 0.6166916 Vali Loss: 0.3828021 Test Loss: 0.3590943
Validation loss decreased (0.383122 --> 0.382802).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.9021251201629639
Epoch: 39, Steps: 59 | Train Loss: 0.6164779 Vali Loss: 0.3862176 Test Loss: 0.3590763
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.016636848449707
Epoch: 40, Steps: 59 | Train Loss: 0.6151159 Vali Loss: 0.3861567 Test Loss: 0.3591480
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.7808043956756592
Epoch: 41, Steps: 59 | Train Loss: 0.6152625 Vali Loss: 0.3834708 Test Loss: 0.3590404
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.5182671546936035
Epoch: 42, Steps: 59 | Train Loss: 0.6154841 Vali Loss: 0.3852217 Test Loss: 0.3590572
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9139189720153809
Epoch: 43, Steps: 59 | Train Loss: 0.6150400 Vali Loss: 0.3851518 Test Loss: 0.3590914
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.967588186264038
Epoch: 44, Steps: 59 | Train Loss: 0.6166323 Vali Loss: 0.3828037 Test Loss: 0.3590572
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7983319759368896
Epoch: 45, Steps: 59 | Train Loss: 0.6161091 Vali Loss: 0.3827373 Test Loss: 0.3590686
Validation loss decreased (0.382802 --> 0.382737).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8930301666259766
Epoch: 46, Steps: 59 | Train Loss: 0.6149457 Vali Loss: 0.3794316 Test Loss: 0.3590363
Validation loss decreased (0.382737 --> 0.379432).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.9407188892364502
Epoch: 47, Steps: 59 | Train Loss: 0.6160201 Vali Loss: 0.3833886 Test Loss: 0.3590660
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.8771686553955078
Epoch: 48, Steps: 59 | Train Loss: 0.6144167 Vali Loss: 0.3826799 Test Loss: 0.3590522
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.8848345279693604
Epoch: 49, Steps: 59 | Train Loss: 0.6146614 Vali Loss: 0.3843707 Test Loss: 0.3590528
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.8497967720031738
Epoch: 50, Steps: 59 | Train Loss: 0.6164445 Vali Loss: 0.3803636 Test Loss: 0.3590589
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9793109893798828
Epoch: 51, Steps: 59 | Train Loss: 0.6148127 Vali Loss: 0.3836714 Test Loss: 0.3590440
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.7757961750030518
Epoch: 52, Steps: 59 | Train Loss: 0.6152726 Vali Loss: 0.3829786 Test Loss: 0.3590634
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6018569469451904
Epoch: 53, Steps: 59 | Train Loss: 0.6160010 Vali Loss: 0.3819629 Test Loss: 0.3590561
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.7808783054351807
Epoch: 54, Steps: 59 | Train Loss: 0.6146834 Vali Loss: 0.3820756 Test Loss: 0.3590291
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.8702521324157715
Epoch: 55, Steps: 59 | Train Loss: 0.6146418 Vali Loss: 0.3821923 Test Loss: 0.3590520
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.8828675746917725
Epoch: 56, Steps: 59 | Train Loss: 0.6155399 Vali Loss: 0.3818383 Test Loss: 0.3590187
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.9264423847198486
Epoch: 57, Steps: 59 | Train Loss: 0.6160240 Vali Loss: 0.3830140 Test Loss: 0.3590509
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.83943772315979
Epoch: 58, Steps: 59 | Train Loss: 0.6152225 Vali Loss: 0.3827070 Test Loss: 0.3590446
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.8906266689300537
Epoch: 59, Steps: 59 | Train Loss: 0.6137812 Vali Loss: 0.3819250 Test Loss: 0.3590573
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.7045173645019531
Epoch: 60, Steps: 59 | Train Loss: 0.6141042 Vali Loss: 0.3806084 Test Loss: 0.3590384
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.5985808372497559
Epoch: 61, Steps: 59 | Train Loss: 0.6153288 Vali Loss: 0.3837991 Test Loss: 0.3590287
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.8961684703826904
Epoch: 62, Steps: 59 | Train Loss: 0.6152522 Vali Loss: 0.3811727 Test Loss: 0.3590177
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.9689826965332031
Epoch: 63, Steps: 59 | Train Loss: 0.6153420 Vali Loss: 0.3809398 Test Loss: 0.3590228
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.8718481063842773
Epoch: 64, Steps: 59 | Train Loss: 0.6145582 Vali Loss: 0.3826569 Test Loss: 0.3590255
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.8725485801696777
Epoch: 65, Steps: 59 | Train Loss: 0.6145916 Vali Loss: 0.3828473 Test Loss: 0.3590427
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.7745630741119385
Epoch: 66, Steps: 59 | Train Loss: 0.6130687 Vali Loss: 0.3810050 Test Loss: 0.3590378
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3548046350479126, mae:0.3961946368217468, rse:0.4762491285800934, corr:[0.25821835 0.26418078 0.26094785 0.26107207 0.262386   0.26130483
 0.25954542 0.25953147 0.25957698 0.25802982 0.25602728 0.25511318
 0.2547826  0.2536983  0.2524101  0.25185516 0.2516859  0.25086272
 0.2494958  0.24848479 0.2478838  0.24707066 0.24553668 0.24404064
 0.2428572  0.24178024 0.24038683 0.23920701 0.23859577 0.23806135
 0.2370891  0.23588848 0.23508959 0.23436107 0.23337433 0.23242095
 0.23186047 0.23137294 0.23039888 0.22931746 0.22870694 0.2283418
 0.22761607 0.22654971 0.22556339 0.22488098 0.2237318  0.22179341
 0.2198778  0.21856159 0.21743281 0.21605207 0.2145012  0.2128094
 0.21107191 0.20959608 0.20814054 0.20655346 0.20510091 0.20408405
 0.20359948 0.20341185 0.20337503 0.20324782 0.20269306 0.20211023
 0.20153804 0.20120206 0.20093653 0.20054129 0.19984342 0.19908209
 0.1983041  0.19728787 0.19604991 0.19489554 0.19424102 0.19373417
 0.19302839 0.19225483 0.19190551 0.19156873 0.19098888 0.19042514
 0.19023268 0.19019693 0.18990415 0.18931362 0.18873844 0.18839094
 0.18817504 0.18790151 0.18787168 0.18795024 0.18785043 0.18745315
 0.18701585 0.1866779  0.18630221 0.1855018  0.18464279 0.18415743
 0.18407498 0.18363859 0.18297485 0.18247953 0.18252715 0.1824659
 0.18169226 0.18086083 0.18050615 0.18053696 0.1801251  0.179384
 0.17871532 0.17834674 0.1779392  0.17699718 0.17598192 0.17515272
 0.17432575 0.17310373 0.17172685 0.17077136 0.17029476 0.16992463
 0.16910832 0.16817279 0.1675055  0.16686706 0.16598932 0.1650405
 0.16456093 0.16424549 0.16363065 0.16274269 0.16212627 0.16196348
 0.16159223 0.1607719  0.1600778  0.159783   0.15927705 0.15781076
 0.15575711 0.15428425 0.1534386  0.1524323  0.15107976 0.15009077
 0.14972551 0.14929639 0.1484712  0.14745308 0.1468162  0.14647633
 0.14602122 0.14535631 0.14496613 0.14485376 0.14434832 0.14365482
 0.14324515 0.14331993 0.14338663 0.14291823 0.14214359 0.14133576
 0.14056638 0.13929349 0.13775127 0.13658762 0.13600884 0.13509205
 0.13371156 0.13221805 0.13153681 0.13132426 0.13045606 0.12919696
 0.1283474  0.12818156 0.12805517 0.12760423 0.12696198 0.12689987
 0.12694906 0.12650967 0.12618896 0.12656698 0.1274142  0.1272746
 0.12620558 0.12540033 0.12529348 0.12531218 0.12465709 0.1237055
 0.12324552 0.12300064 0.12248479 0.1217005  0.12130558 0.12126658
 0.12096644 0.12047555 0.12022125 0.12067507 0.12108359 0.12091441
 0.12047736 0.12058934 0.12107834 0.12107792 0.12055685 0.11992621
 0.11935915 0.11836832 0.11699654 0.116306   0.11646902 0.11684872
 0.11622518 0.11542639 0.11519334 0.11542092 0.11523812 0.11442667
 0.11372884 0.11363072 0.11346003 0.11322891 0.11326185 0.11405844
 0.1148636  0.11504583 0.11496983 0.11543578 0.11611268 0.11591871
 0.11475036 0.11375892 0.11372603 0.11407396 0.11357904 0.11261666
 0.11249713 0.11284021 0.11282206 0.11240667 0.1124484  0.11320494
 0.11363695 0.11393669 0.11451067 0.11599579 0.11699326 0.11736958
 0.11743216 0.11808293 0.11895696 0.11907864 0.11924688 0.11987432
 0.12078401 0.12077209 0.12020876 0.12016143 0.12071395 0.12098403
 0.12024958 0.11942912 0.11979434 0.1204315  0.12061308 0.11970912
 0.11915196 0.11958134 0.12020861 0.12036526 0.1204805  0.12098964
 0.1212612  0.1208033  0.11998851 0.11999459 0.12089282 0.12132901
 0.120684   0.1199021  0.11969642 0.11958055 0.11864708 0.11731176
 0.11690672 0.11678355 0.11598124 0.11490361 0.11501204 0.11559184
 0.11503421 0.11430738 0.11481774 0.1166754  0.11726511 0.11693782
 0.11657822 0.11756407 0.11847157 0.11821397 0.11762719 0.11781663
 0.1182168  0.11694343 0.11538389 0.11523259 0.11585481 0.11501899
 0.11315393 0.11313637 0.11485839 0.1161304  0.115335   0.11526306
 0.11688294 0.11832456 0.11807292 0.11821335 0.12071706 0.12283381
 0.12136679 0.11966951 0.12276518 0.12637863 0.1237945  0.11699495]
