Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=22, out_features=104, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2050048.0
params:  2392.0
Trainable parameters:  2392
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.5775697231292725
Epoch: 1, Steps: 64 | Train Loss: 0.9503739 Vali Loss: 0.4937519 Test Loss: 0.5793096
Validation loss decreased (inf --> 0.493752).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.4967947006225586
Epoch: 2, Steps: 64 | Train Loss: 0.8245743 Vali Loss: 0.4473077 Test Loss: 0.5199682
Validation loss decreased (0.493752 --> 0.447308).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8021812438964844
Epoch: 3, Steps: 64 | Train Loss: 0.7632596 Vali Loss: 0.4256017 Test Loss: 0.4877828
Validation loss decreased (0.447308 --> 0.425602).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.324096918106079
Epoch: 4, Steps: 64 | Train Loss: 0.7291800 Vali Loss: 0.4067295 Test Loss: 0.4687069
Validation loss decreased (0.425602 --> 0.406730).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.6986963748931885
Epoch: 5, Steps: 64 | Train Loss: 0.7077751 Vali Loss: 0.4007200 Test Loss: 0.4566721
Validation loss decreased (0.406730 --> 0.400720).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.1830027103424072
Epoch: 6, Steps: 64 | Train Loss: 0.6946146 Vali Loss: 0.3916884 Test Loss: 0.4488206
Validation loss decreased (0.400720 --> 0.391688).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.8900928497314453
Epoch: 7, Steps: 64 | Train Loss: 0.6856258 Vali Loss: 0.3863950 Test Loss: 0.4433549
Validation loss decreased (0.391688 --> 0.386395).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.929459810256958
Epoch: 8, Steps: 64 | Train Loss: 0.6781651 Vali Loss: 0.3838291 Test Loss: 0.4395506
Validation loss decreased (0.386395 --> 0.383829).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.21429181098938
Epoch: 9, Steps: 64 | Train Loss: 0.6747309 Vali Loss: 0.3799984 Test Loss: 0.4367287
Validation loss decreased (0.383829 --> 0.379998).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.139915943145752
Epoch: 10, Steps: 64 | Train Loss: 0.6714860 Vali Loss: 0.3794452 Test Loss: 0.4347550
Validation loss decreased (0.379998 --> 0.379445).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9718818664550781
Epoch: 11, Steps: 64 | Train Loss: 0.6694354 Vali Loss: 0.3767043 Test Loss: 0.4330904
Validation loss decreased (0.379445 --> 0.376704).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.019376277923584
Epoch: 12, Steps: 64 | Train Loss: 0.6667488 Vali Loss: 0.3754337 Test Loss: 0.4317552
Validation loss decreased (0.376704 --> 0.375434).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9933221340179443
Epoch: 13, Steps: 64 | Train Loss: 0.6649463 Vali Loss: 0.3756685 Test Loss: 0.4307795
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.853644847869873
Epoch: 14, Steps: 64 | Train Loss: 0.6642416 Vali Loss: 0.3735931 Test Loss: 0.4299579
Validation loss decreased (0.375434 --> 0.373593).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.705502986907959
Epoch: 15, Steps: 64 | Train Loss: 0.6637697 Vali Loss: 0.3749869 Test Loss: 0.4292653
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.031895637512207
Epoch: 16, Steps: 64 | Train Loss: 0.6624556 Vali Loss: 0.3736390 Test Loss: 0.4286362
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.912111759185791
Epoch: 17, Steps: 64 | Train Loss: 0.6617699 Vali Loss: 0.3714319 Test Loss: 0.4281309
Validation loss decreased (0.373593 --> 0.371432).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8620762825012207
Epoch: 18, Steps: 64 | Train Loss: 0.6608019 Vali Loss: 0.3732289 Test Loss: 0.4276449
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.1938083171844482
Epoch: 19, Steps: 64 | Train Loss: 0.6595746 Vali Loss: 0.3709682 Test Loss: 0.4273154
Validation loss decreased (0.371432 --> 0.370968).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9654319286346436
Epoch: 20, Steps: 64 | Train Loss: 0.6597129 Vali Loss: 0.3705394 Test Loss: 0.4269775
Validation loss decreased (0.370968 --> 0.370539).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.180429697036743
Epoch: 21, Steps: 64 | Train Loss: 0.6578774 Vali Loss: 0.3695821 Test Loss: 0.4266192
Validation loss decreased (0.370539 --> 0.369582).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.8299963474273682
Epoch: 22, Steps: 64 | Train Loss: 0.6584246 Vali Loss: 0.3728012 Test Loss: 0.4263918
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.0921378135681152
Epoch: 23, Steps: 64 | Train Loss: 0.6571087 Vali Loss: 0.3676884 Test Loss: 0.4261385
Validation loss decreased (0.369582 --> 0.367688).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8977949619293213
Epoch: 24, Steps: 64 | Train Loss: 0.6568004 Vali Loss: 0.3701674 Test Loss: 0.4259472
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.342827320098877
Epoch: 25, Steps: 64 | Train Loss: 0.6571841 Vali Loss: 0.3683355 Test Loss: 0.4257424
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.0662503242492676
Epoch: 26, Steps: 64 | Train Loss: 0.6558393 Vali Loss: 0.3687794 Test Loss: 0.4256023
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1104655265808105
Epoch: 27, Steps: 64 | Train Loss: 0.6557316 Vali Loss: 0.3702420 Test Loss: 0.4253958
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.8297014236450195
Epoch: 28, Steps: 64 | Train Loss: 0.6564118 Vali Loss: 0.3677417 Test Loss: 0.4252726
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7189128398895264
Epoch: 29, Steps: 64 | Train Loss: 0.6555083 Vali Loss: 0.3679937 Test Loss: 0.4251149
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9480407238006592
Epoch: 30, Steps: 64 | Train Loss: 0.6557960 Vali Loss: 0.3675635 Test Loss: 0.4249876
Validation loss decreased (0.367688 --> 0.367563).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.9816131591796875
Epoch: 31, Steps: 64 | Train Loss: 0.6552900 Vali Loss: 0.3710390 Test Loss: 0.4248912
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.4860103130340576
Epoch: 32, Steps: 64 | Train Loss: 0.6550138 Vali Loss: 0.3688653 Test Loss: 0.4248199
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.1654036045074463
Epoch: 33, Steps: 64 | Train Loss: 0.6555503 Vali Loss: 0.3686932 Test Loss: 0.4247005
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.087858200073242
Epoch: 34, Steps: 64 | Train Loss: 0.6540344 Vali Loss: 0.3682153 Test Loss: 0.4245947
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.7552084922790527
Epoch: 35, Steps: 64 | Train Loss: 0.6535100 Vali Loss: 0.3676626 Test Loss: 0.4245177
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.960742473602295
Epoch: 36, Steps: 64 | Train Loss: 0.6546232 Vali Loss: 0.3684712 Test Loss: 0.4244491
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6148335933685303
Epoch: 37, Steps: 64 | Train Loss: 0.6544785 Vali Loss: 0.3667765 Test Loss: 0.4243895
Validation loss decreased (0.367563 --> 0.366776).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.6754693984985352
Epoch: 38, Steps: 64 | Train Loss: 0.6544948 Vali Loss: 0.3659289 Test Loss: 0.4242930
Validation loss decreased (0.366776 --> 0.365929).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.0175657272338867
Epoch: 39, Steps: 64 | Train Loss: 0.6527571 Vali Loss: 0.3656769 Test Loss: 0.4242323
Validation loss decreased (0.365929 --> 0.365677).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7936217784881592
Epoch: 40, Steps: 64 | Train Loss: 0.6544412 Vali Loss: 0.3681112 Test Loss: 0.4241799
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9895851612091064
Epoch: 41, Steps: 64 | Train Loss: 0.6527942 Vali Loss: 0.3678623 Test Loss: 0.4241298
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.7925128936767578
Epoch: 42, Steps: 64 | Train Loss: 0.6530559 Vali Loss: 0.3695334 Test Loss: 0.4240839
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.7911043167114258
Epoch: 43, Steps: 64 | Train Loss: 0.6528188 Vali Loss: 0.3635527 Test Loss: 0.4240116
Validation loss decreased (0.365677 --> 0.363553).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.23738169670105
Epoch: 44, Steps: 64 | Train Loss: 0.6534672 Vali Loss: 0.3645595 Test Loss: 0.4239759
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.0604007244110107
Epoch: 45, Steps: 64 | Train Loss: 0.6532229 Vali Loss: 0.3650915 Test Loss: 0.4239278
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.5821828842163086
Epoch: 46, Steps: 64 | Train Loss: 0.6535372 Vali Loss: 0.3691194 Test Loss: 0.4238988
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.7515342235565186
Epoch: 47, Steps: 64 | Train Loss: 0.6531906 Vali Loss: 0.3660752 Test Loss: 0.4238654
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.8461658954620361
Epoch: 48, Steps: 64 | Train Loss: 0.6536409 Vali Loss: 0.3684908 Test Loss: 0.4238268
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.8429417610168457
Epoch: 49, Steps: 64 | Train Loss: 0.6528138 Vali Loss: 0.3677963 Test Loss: 0.4238049
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.753065586090088
Epoch: 50, Steps: 64 | Train Loss: 0.6536076 Vali Loss: 0.3697760 Test Loss: 0.4237657
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9871420860290527
Epoch: 51, Steps: 64 | Train Loss: 0.6513071 Vali Loss: 0.3655497 Test Loss: 0.4237393
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.8939471244812012
Epoch: 52, Steps: 64 | Train Loss: 0.6512981 Vali Loss: 0.3686360 Test Loss: 0.4237127
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.52756667137146
Epoch: 53, Steps: 64 | Train Loss: 0.6532580 Vali Loss: 0.3665641 Test Loss: 0.4236902
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9805562496185303
Epoch: 54, Steps: 64 | Train Loss: 0.6524755 Vali Loss: 0.3647160 Test Loss: 0.4236631
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.5685570240020752
Epoch: 55, Steps: 64 | Train Loss: 0.6537520 Vali Loss: 0.3642136 Test Loss: 0.4236407
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7259633541107178
Epoch: 56, Steps: 64 | Train Loss: 0.6510183 Vali Loss: 0.3647430 Test Loss: 0.4236203
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.6759614944458008
Epoch: 57, Steps: 64 | Train Loss: 0.6536256 Vali Loss: 0.3655336 Test Loss: 0.4236018
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.805161952972412
Epoch: 58, Steps: 64 | Train Loss: 0.6532990 Vali Loss: 0.3665244 Test Loss: 0.4235780
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.093904972076416
Epoch: 59, Steps: 64 | Train Loss: 0.6520079 Vali Loss: 0.3669730 Test Loss: 0.4235630
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.073798894882202
Epoch: 60, Steps: 64 | Train Loss: 0.6529583 Vali Loss: 0.3660848 Test Loss: 0.4235443
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.7938637733459473
Epoch: 61, Steps: 64 | Train Loss: 0.6523469 Vali Loss: 0.3646081 Test Loss: 0.4235314
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.7627668380737305
Epoch: 62, Steps: 64 | Train Loss: 0.6521971 Vali Loss: 0.3665617 Test Loss: 0.4235173
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.986072063446045
Epoch: 63, Steps: 64 | Train Loss: 0.6524565 Vali Loss: 0.3660525 Test Loss: 0.4234992
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4192640483379364, mae:0.4266328513622284, rse:0.5177061557769775, corr:[0.26220152 0.2660921  0.26276308 0.2609203  0.26053095 0.25912982
 0.25710103 0.25598863 0.25564238 0.25471497 0.2531297  0.25164673
 0.25068694 0.24962558 0.2484057  0.24745145 0.24669205 0.24577905
 0.24464832 0.2434694  0.24239044 0.24115953 0.23956731 0.23759216
 0.23493306 0.23250349 0.23024474 0.22891933 0.22806422 0.22690408
 0.22572793 0.22470349 0.22417909 0.22353755 0.22261456 0.22162813
 0.22085388 0.22020403 0.21913372 0.21808638 0.21759135 0.21729833
 0.21657819 0.21528584 0.21424654 0.2134134  0.21184444 0.20907606
 0.2055133  0.2028877  0.20037176 0.19799098 0.19623037 0.19487756
 0.19366544 0.191591   0.1902003  0.18944177 0.18909438 0.18821737
 0.18700942 0.18664838 0.18660684 0.18633027 0.18571247 0.18519793
 0.18462713 0.18368033 0.18266137 0.18193099 0.18079287 0.17884065
 0.17597254 0.17417608 0.17277052 0.17153157 0.170587   0.17020361
 0.17002119 0.1692232  0.16898142 0.16894926 0.16870627 0.16816169
 0.16772747 0.167645   0.16749392 0.16723582 0.16698387 0.16672777
 0.16627304 0.16542199 0.16492689 0.16467877 0.16395473 0.1622701
 0.15968913 0.15788645 0.15653798 0.15512283 0.15378892 0.15312947
 0.15345614 0.15283068 0.15248236 0.15249954 0.1528381  0.15264632
 0.1519539  0.15164326 0.15143488 0.15114191 0.15055944 0.1499845
 0.14954367 0.1487246  0.14781412 0.14660196 0.14489686 0.14260942
 0.13952187 0.13702957 0.1350952  0.13394083 0.1326877  0.13181128
 0.13171244 0.13128835 0.13102609 0.13081053 0.13069063 0.13023463
 0.12963872 0.12925473 0.12892532 0.12846541 0.12793463 0.12739871
 0.12675296 0.12585634 0.12522624 0.12444024 0.12279546 0.12021969
 0.11675764 0.11401403 0.11164549 0.10997222 0.10870581 0.10782365
 0.10769636 0.10714726 0.10697893 0.10699379 0.10717276 0.10702772
 0.10675662 0.106768   0.10678404 0.10676108 0.10674085 0.1066198
 0.10629167 0.10559716 0.10530088 0.10502449 0.10394076 0.1016744
 0.0986271  0.09679663 0.09516238 0.09373564 0.09267127 0.09229028
 0.09278771 0.09270625 0.09280741 0.09292297 0.09325388 0.09321241
 0.09301659 0.09318849 0.09341346 0.09364584 0.09364881 0.09371183
 0.09387599 0.09371439 0.09355997 0.09356596 0.09323512 0.09227882
 0.09062921 0.08969812 0.08861695 0.08766637 0.08756297 0.08811757
 0.08933855 0.089824   0.09048745 0.09118344 0.09183224 0.09188329
 0.09170825 0.09182891 0.09196748 0.09208823 0.09211838 0.09235087
 0.09243485 0.09225113 0.09217137 0.09200812 0.09115645 0.08908208
 0.08644097 0.08494653 0.08351758 0.08207373 0.08100946 0.08156759
 0.08343367 0.08474347 0.0855493  0.08599    0.08649548 0.08645426
 0.08617225 0.08618312 0.08639264 0.08656435 0.08671193 0.08708588
 0.08761622 0.08799794 0.08797661 0.08775713 0.08708948 0.08559866
 0.0829222  0.08133973 0.08029386 0.07961644 0.07934117 0.0800129
 0.08203179 0.08339661 0.08461105 0.0854386  0.08606996 0.08634207
 0.08657147 0.0873391  0.08775706 0.0882351  0.08883486 0.08976629
 0.09003194 0.08979865 0.08976875 0.0900587  0.08988088 0.08869468
 0.08679868 0.08623758 0.0864839  0.08647641 0.08603964 0.08663314
 0.08859049 0.08988135 0.0907884  0.09123756 0.09159635 0.09155281
 0.09169319 0.09201164 0.09224241 0.09215918 0.09213594 0.09240504
 0.09268524 0.09240755 0.09219905 0.09220082 0.09209616 0.09143524
 0.08989416 0.08883418 0.08826981 0.0879734  0.08763744 0.0873981
 0.08814221 0.08884704 0.09001364 0.09099241 0.09145769 0.09129671
 0.09130805 0.09213168 0.09285627 0.09300471 0.09289452 0.09349357
 0.0943251  0.09437171 0.09440645 0.09449725 0.09449912 0.09382799
 0.09192728 0.09088725 0.09069787 0.09141462 0.09168349 0.0916727
 0.09267084 0.09386345 0.09569741 0.09708256 0.09772757 0.09745606
 0.09734133 0.09779736 0.09830225 0.09827665 0.09804536 0.09832066
 0.09879202 0.09807546 0.09724627 0.09767686 0.09892267 0.09675664]
