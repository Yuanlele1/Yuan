Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9638712406158447
Epoch: 1, Steps: 61 | Train Loss: 1.3963485 Vali Loss: 0.8724798 Test Loss: 0.7028117
Validation loss decreased (inf --> 0.872480).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.270463466644287
Epoch: 2, Steps: 61 | Train Loss: 1.1925958 Vali Loss: 0.7893801 Test Loss: 0.6038076
Validation loss decreased (0.872480 --> 0.789380).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.4201817512512207
Epoch: 3, Steps: 61 | Train Loss: 1.0749430 Vali Loss: 0.7353644 Test Loss: 0.5454613
Validation loss decreased (0.789380 --> 0.735364).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0653791427612305
Epoch: 4, Steps: 61 | Train Loss: 1.0071247 Vali Loss: 0.7023554 Test Loss: 0.5088624
Validation loss decreased (0.735364 --> 0.702355).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.3585784435272217
Epoch: 5, Steps: 61 | Train Loss: 0.9617771 Vali Loss: 0.6859084 Test Loss: 0.4843919
Validation loss decreased (0.702355 --> 0.685908).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.1919939517974854
Epoch: 6, Steps: 61 | Train Loss: 0.9333881 Vali Loss: 0.6731825 Test Loss: 0.4678084
Validation loss decreased (0.685908 --> 0.673183).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.173609972000122
Epoch: 7, Steps: 61 | Train Loss: 0.9136669 Vali Loss: 0.6592304 Test Loss: 0.4562991
Validation loss decreased (0.673183 --> 0.659230).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.5922420024871826
Epoch: 8, Steps: 61 | Train Loss: 0.8987581 Vali Loss: 0.6570286 Test Loss: 0.4481466
Validation loss decreased (0.659230 --> 0.657029).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.895979642868042
Epoch: 9, Steps: 61 | Train Loss: 0.8877847 Vali Loss: 0.6521268 Test Loss: 0.4421685
Validation loss decreased (0.657029 --> 0.652127).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.5005810260772705
Epoch: 10, Steps: 61 | Train Loss: 0.8810549 Vali Loss: 0.6431831 Test Loss: 0.4377869
Validation loss decreased (0.652127 --> 0.643183).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9000914096832275
Epoch: 11, Steps: 61 | Train Loss: 0.8764367 Vali Loss: 0.6404083 Test Loss: 0.4345679
Validation loss decreased (0.643183 --> 0.640408).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.2301313877105713
Epoch: 12, Steps: 61 | Train Loss: 0.8725912 Vali Loss: 0.6371405 Test Loss: 0.4321262
Validation loss decreased (0.640408 --> 0.637141).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9591009616851807
Epoch: 13, Steps: 61 | Train Loss: 0.8681176 Vali Loss: 0.6404030 Test Loss: 0.4302422
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.5027945041656494
Epoch: 14, Steps: 61 | Train Loss: 0.8657180 Vali Loss: 0.6368510 Test Loss: 0.4288151
Validation loss decreased (0.637141 --> 0.636851).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.4627768993377686
Epoch: 15, Steps: 61 | Train Loss: 0.8633749 Vali Loss: 0.6362093 Test Loss: 0.4276840
Validation loss decreased (0.636851 --> 0.636209).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.4332895278930664
Epoch: 16, Steps: 61 | Train Loss: 0.8630253 Vali Loss: 0.6317087 Test Loss: 0.4267708
Validation loss decreased (0.636209 --> 0.631709).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.042750597000122
Epoch: 17, Steps: 61 | Train Loss: 0.8600759 Vali Loss: 0.6341199 Test Loss: 0.4260330
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.294537305831909
Epoch: 18, Steps: 61 | Train Loss: 0.8607064 Vali Loss: 0.6378185 Test Loss: 0.4254578
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.272897720336914
Epoch: 19, Steps: 61 | Train Loss: 0.8597911 Vali Loss: 0.6348036 Test Loss: 0.4249834
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.013077735900879
Epoch: 20, Steps: 61 | Train Loss: 0.8581423 Vali Loss: 0.6333866 Test Loss: 0.4245772
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.202341079711914
Epoch: 21, Steps: 61 | Train Loss: 0.8582370 Vali Loss: 0.6312543 Test Loss: 0.4242523
Validation loss decreased (0.631709 --> 0.631254).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0149149894714355
Epoch: 22, Steps: 61 | Train Loss: 0.8579955 Vali Loss: 0.6273513 Test Loss: 0.4239729
Validation loss decreased (0.631254 --> 0.627351).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.4644033908843994
Epoch: 23, Steps: 61 | Train Loss: 0.8560941 Vali Loss: 0.6324540 Test Loss: 0.4237459
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.809281826019287
Epoch: 24, Steps: 61 | Train Loss: 0.8567184 Vali Loss: 0.6310208 Test Loss: 0.4235508
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.016218423843384
Epoch: 25, Steps: 61 | Train Loss: 0.8550063 Vali Loss: 0.6287965 Test Loss: 0.4233698
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.9293675422668457
Epoch: 26, Steps: 61 | Train Loss: 0.8560248 Vali Loss: 0.6313411 Test Loss: 0.4232146
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.4727799892425537
Epoch: 27, Steps: 61 | Train Loss: 0.8562935 Vali Loss: 0.6346803 Test Loss: 0.4230890
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.264296531677246
Epoch: 28, Steps: 61 | Train Loss: 0.8541595 Vali Loss: 0.6273949 Test Loss: 0.4229548
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.094691753387451
Epoch: 29, Steps: 61 | Train Loss: 0.8536727 Vali Loss: 0.6277946 Test Loss: 0.4228544
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.74290132522583
Epoch: 30, Steps: 61 | Train Loss: 0.8539192 Vali Loss: 0.6319253 Test Loss: 0.4227544
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.545506000518799
Epoch: 31, Steps: 61 | Train Loss: 0.8542971 Vali Loss: 0.6283003 Test Loss: 0.4226809
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.0776710510253906
Epoch: 32, Steps: 61 | Train Loss: 0.8539744 Vali Loss: 0.6298193 Test Loss: 0.4225927
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.359060764312744
Epoch: 33, Steps: 61 | Train Loss: 0.8544802 Vali Loss: 0.6236199 Test Loss: 0.4225322
Validation loss decreased (0.627351 --> 0.623620).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9356598854064941
Epoch: 34, Steps: 61 | Train Loss: 0.8553725 Vali Loss: 0.6312464 Test Loss: 0.4224660
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.149447202682495
Epoch: 35, Steps: 61 | Train Loss: 0.8539637 Vali Loss: 0.6325535 Test Loss: 0.4224127
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.980137825012207
Epoch: 36, Steps: 61 | Train Loss: 0.8545397 Vali Loss: 0.6281033 Test Loss: 0.4223548
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.903475046157837
Epoch: 37, Steps: 61 | Train Loss: 0.8534929 Vali Loss: 0.6303797 Test Loss: 0.4223087
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.14422345161438
Epoch: 38, Steps: 61 | Train Loss: 0.8544082 Vali Loss: 0.6278973 Test Loss: 0.4222592
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.633084297180176
Epoch: 39, Steps: 61 | Train Loss: 0.8531973 Vali Loss: 0.6252574 Test Loss: 0.4222165
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.1175878047943115
Epoch: 40, Steps: 61 | Train Loss: 0.8540618 Vali Loss: 0.6286504 Test Loss: 0.4221754
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.920424222946167
Epoch: 41, Steps: 61 | Train Loss: 0.8536685 Vali Loss: 0.6288226 Test Loss: 0.4221402
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.270139694213867
Epoch: 42, Steps: 61 | Train Loss: 0.8535260 Vali Loss: 0.6295244 Test Loss: 0.4221052
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.7185165882110596
Epoch: 43, Steps: 61 | Train Loss: 0.8522857 Vali Loss: 0.6275446 Test Loss: 0.4220745
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.6040101051330566
Epoch: 44, Steps: 61 | Train Loss: 0.8532179 Vali Loss: 0.6285271 Test Loss: 0.4220429
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.5358588695526123
Epoch: 45, Steps: 61 | Train Loss: 0.8541666 Vali Loss: 0.6308479 Test Loss: 0.4220190
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.5014560222625732
Epoch: 46, Steps: 61 | Train Loss: 0.8531672 Vali Loss: 0.6323479 Test Loss: 0.4219948
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.148081064224243
Epoch: 47, Steps: 61 | Train Loss: 0.8540909 Vali Loss: 0.6288247 Test Loss: 0.4219723
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9989640712738037
Epoch: 48, Steps: 61 | Train Loss: 0.8533223 Vali Loss: 0.6266031 Test Loss: 0.4219463
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.09196400642395
Epoch: 49, Steps: 61 | Train Loss: 0.8519996 Vali Loss: 0.6318741 Test Loss: 0.4219280
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.143608808517456
Epoch: 50, Steps: 61 | Train Loss: 0.8516196 Vali Loss: 0.6238342 Test Loss: 0.4219029
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.2357707023620605
Epoch: 51, Steps: 61 | Train Loss: 0.8522323 Vali Loss: 0.6276907 Test Loss: 0.4218888
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.0330753326416016
Epoch: 52, Steps: 61 | Train Loss: 0.8525034 Vali Loss: 0.6271001 Test Loss: 0.4218715
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.578202724456787
Epoch: 53, Steps: 61 | Train Loss: 0.8514708 Vali Loss: 0.6266052 Test Loss: 0.4218534
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4208877682685852, mae:0.4377484619617462, rse:0.5185479521751404, corr:[ 0.21522592  0.22109863  0.21858175  0.2158774   0.21522324  0.21490502
  0.21342193  0.21174434  0.21141002  0.21122068  0.20995884  0.20758025
  0.20584264  0.20557573  0.20564304  0.20504133  0.20380579  0.2028145
  0.20237406  0.20202863  0.20127892  0.20005445  0.19852908  0.19669247
  0.19448316  0.19266176  0.19097172  0.18956497  0.18851034  0.18777803
  0.18746862  0.18689601  0.18641937  0.1857419   0.18483801  0.18370908
  0.18288055  0.18272081  0.18268852  0.18231936  0.18166071  0.18103832
  0.18062702  0.18007687  0.17954624  0.17899553  0.17802799  0.17603374
  0.17293285  0.17053953  0.16887328  0.16820309  0.1677675   0.16701551
  0.16621     0.16484158  0.16450419  0.16486005  0.16540514  0.16505899
  0.16408794  0.16392455  0.16453311  0.1652334   0.16552272  0.16532387
  0.16495427  0.16446376  0.16414303  0.16401678  0.16356975  0.16246027
  0.16050649  0.15916191  0.15819976  0.15772715  0.15760112  0.1577473
  0.15788345  0.1571732   0.15683722  0.1571673   0.15781197  0.1580229
  0.15766557  0.15745135  0.15759635  0.1577923   0.15781958  0.15751137
  0.15706222  0.15653995  0.15637878  0.1562771   0.15575276  0.15454231
  0.15252441  0.15108038  0.14987814  0.14902537  0.14839616  0.14797546
  0.14829098  0.14798813  0.14771582  0.14757681  0.1479709   0.14834611
  0.14814451  0.14766712  0.14719157  0.14705515  0.14722188  0.14731805
  0.14717901  0.14658079  0.14588735  0.14500755  0.14373098  0.14200203
  0.13986443  0.13850485  0.13731179  0.13641213  0.13551095  0.13492256
  0.1347319   0.13443474  0.13438146  0.13424931  0.13417047  0.1338325
  0.13326025  0.13277903  0.1323659   0.13199645  0.13172203  0.13129868
  0.13072813  0.13012187  0.12974456  0.12935618  0.12821643  0.12616086
  0.12316179  0.12095132  0.11936561  0.1184307   0.11779637  0.11725552
  0.11716925  0.11702112  0.11723115  0.11742003  0.11751921  0.11718048
  0.11671987  0.11665423  0.11671159  0.11672021  0.11663314  0.11645193
  0.11622519  0.11586595  0.11569052  0.1152963   0.11429247  0.11255743
  0.11000787  0.10834654  0.10720786  0.10663861  0.1063011   0.1062146
  0.10656242  0.10661154  0.10702925  0.10745255  0.10800893  0.10804339
  0.10775499  0.10760213  0.10755187  0.10769507  0.10772526  0.10775401
  0.10777869  0.10757581  0.10734089  0.10744923  0.10743285  0.10698172
  0.10568623  0.10477677  0.10425895  0.10441025  0.10503319  0.10565275
  0.10660873  0.10700263  0.10761765  0.10840097  0.10950635  0.1102253
  0.11040558  0.11025964  0.11016523  0.11039284  0.11077815  0.11117438
  0.11120629  0.11086874  0.11057687  0.11030088  0.10999738  0.1091725
  0.1077185   0.10672808  0.10567154  0.10501541  0.10479864  0.10564172
  0.1072238   0.1083242   0.10909682  0.10975858  0.1106358   0.11126348
  0.11167901  0.11194547  0.11217262  0.11250963  0.11313435  0.11389401
  0.11444227  0.11445435  0.11417589  0.11406105  0.11395194  0.11363342
  0.11251863  0.11185241  0.11128961  0.11125527  0.11164021  0.11258614
  0.11439446  0.11513083  0.1155524   0.11587819  0.11678351  0.11756993
  0.11798009  0.11836248  0.11851874  0.11874425  0.11898846  0.11954493
  0.11995643  0.1200334   0.119903    0.11964674  0.11930338  0.11887436
  0.11795042  0.1176796   0.1175978   0.11771278  0.11794122  0.11895392
  0.12072734  0.12178636  0.12255973  0.12284487  0.1233089   0.12352663
  0.12384466  0.12413251  0.12435832  0.12431704  0.12412492  0.12401177
  0.12414385  0.12415093  0.12398883  0.12359852  0.12293822  0.12205011
  0.12102284  0.12074549  0.12062264  0.12053072  0.12051974  0.12065218
  0.12122884  0.12171348  0.12256653  0.12332772  0.12384655  0.12390215
  0.12370957  0.12378073  0.1242476   0.12492026  0.12531233  0.12547871
  0.12533294  0.12482122  0.12441446  0.12407928  0.12368929  0.12304749
  0.12171977  0.12107597  0.12110036  0.1215075   0.12182946  0.12228398
  0.12314843  0.12369169  0.12440692  0.1251285   0.12591444  0.12630263
  0.12648016  0.12673329  0.1271267   0.12750965  0.1277247   0.12782286
  0.12783684  0.1276434   0.12743507  0.12747031  0.12759343  0.12762228
  0.12661551  0.12606058  0.12591599  0.12619828  0.12719414  0.12863055
  0.13065234  0.13174354  0.1323493   0.13279796  0.13357902  0.1343696
  0.13479334  0.13503878  0.1352396   0.13532461  0.13544072  0.13567251
  0.1360874   0.1362357   0.13611709  0.13617691  0.13634169  0.1364371
  0.13621795  0.13651156  0.13688157  0.13732868  0.13818036  0.1398029
  0.14211188  0.14403196  0.14559971  0.14670667  0.14760618  0.14825872
  0.1487594   0.1493553   0.15003096  0.15068221  0.151252    0.1518887
  0.1524334   0.1528621   0.15320228  0.15348542  0.15373453  0.15388
  0.15373197  0.1541172   0.15485084  0.15562686  0.15666533  0.15821935
  0.16065563  0.16221218  0.16332929  0.16410775  0.16482313  0.16523893
  0.165545    0.16600557  0.16643873  0.16667074  0.16663435  0.1664887
  0.16652393  0.16649774  0.16650046  0.16652279  0.16642435  0.16622552
  0.16538404  0.16519623  0.16537587  0.16582529  0.16647924  0.16743043
  0.16872971  0.16938671  0.16963245  0.16982017  0.17012542  0.17029996
  0.17019     0.170034    0.17002405  0.17015548  0.17029895  0.17029734
  0.17013921  0.16984367  0.16956314  0.16945736  0.1692872   0.16889726
  0.16826299  0.16796115  0.16784552  0.16784321  0.16793205  0.16820131
  0.1691623   0.16951692  0.16966404  0.16961896  0.16963723  0.16971898
  0.16980377  0.16987713  0.16985525  0.16982126  0.16966449  0.16951412
  0.16933386  0.16920096  0.16908903  0.1690216   0.16894566  0.16865376
  0.1679356   0.16735166  0.16672035  0.16647315  0.16652028  0.1667477
  0.16716316  0.16716442  0.16695294  0.16668233  0.16656044  0.16646734
  0.1664305   0.16640581  0.16629416  0.16611189  0.16583475  0.16550316
  0.16521238  0.16487007  0.16457754  0.16414411  0.16342929  0.16238773
  0.16101854  0.15989175  0.15886146  0.15771325  0.15675837  0.1561506
  0.15608671  0.15581055  0.15546408  0.154845    0.15415512  0.15352824
  0.15327767  0.15331423  0.15324906  0.15294784  0.15240775  0.15182817
  0.15130952  0.15100743  0.15079744  0.15025006  0.14906846  0.14721441
  0.14504838  0.14335962  0.1421469   0.14106756  0.13994421  0.13898931
  0.13855648  0.13854894  0.13891692  0.13906644  0.13867612  0.13785222
  0.13713185  0.13683382  0.13671975  0.1365184   0.13605335  0.13525581
  0.13435225  0.13367592  0.13318741  0.13263401  0.13132787  0.12916961
  0.12642817  0.12439756  0.12269224  0.12070709  0.11874153  0.11713299
  0.11645996  0.115793    0.11563417  0.1156819   0.11570326  0.11520194
  0.11440039  0.11392196  0.11352589  0.11303345  0.11224026  0.11134557
  0.11025406  0.10913789  0.10807294  0.10698275  0.10547442  0.10321057
  0.10002334  0.09770247  0.09557796  0.09398142  0.09296234  0.09251302
  0.09223951  0.09107545  0.09031088  0.08993605  0.09021406  0.09040398
  0.09014791  0.08970904  0.0890764   0.08833461  0.08774488  0.08721235
  0.08641575  0.08514544  0.08370043  0.08218599  0.08032683  0.07803892
  0.07528979  0.07334045  0.07157041  0.06965306  0.06809121  0.06727998
  0.0674137   0.06714143  0.06682876  0.06642644  0.06607299  0.06575064
  0.0654931   0.06542598  0.06516445  0.06462175  0.06398102  0.06349593
  0.06317874  0.06270394  0.06198101  0.06093143  0.0591787   0.05672473
  0.05384934  0.05201607  0.05040197  0.04880755  0.04730252  0.04648445
  0.04659268  0.04656712  0.0465623   0.046416    0.04638991  0.04621741
  0.0462604   0.04679696  0.04718328  0.04691029  0.04596503  0.04507472
  0.0447236   0.04465798  0.04450649  0.04399709  0.04232301  0.03979626
  0.03670621  0.03490724  0.03361484  0.03227676  0.03058189  0.02911632
  0.02874705  0.02861114  0.02933529  0.03020399  0.03052118  0.02984155
  0.02858913  0.02783678  0.02782913  0.02796671  0.02716284  0.02545763
  0.02381327  0.02287588  0.02290561  0.02275219  0.02145752  0.01878056
  0.01561441  0.01354831  0.01216104  0.01104019  0.00962103  0.00806243
  0.0070726   0.00657804  0.00698065  0.00761214  0.00810974  0.00779968
  0.00688258  0.00616407  0.00591746  0.00620831  0.00645537  0.00595682
  0.00471221  0.00323705  0.00252629  0.0027087   0.00241337  0.00070935
 -0.00226708 -0.00496614 -0.00719006 -0.00881232 -0.00982147 -0.01042739
 -0.0106941  -0.011382   -0.01134855 -0.0105302  -0.0093315  -0.00881801
 -0.0092174  -0.01017225 -0.01115106 -0.01164541 -0.01135422 -0.0107895
 -0.01088333 -0.01232841 -0.01385295 -0.01427061 -0.01446286 -0.01549563
 -0.01833704 -0.02075233 -0.02322568 -0.02535826 -0.02624005 -0.0255027
 -0.02375528 -0.02352823 -0.02403755 -0.02467424 -0.02394569 -0.02262131
 -0.02133224 -0.02153765 -0.02396196 -0.02719583 -0.0282876  -0.02589431
 -0.02319702 -0.02455498 -0.02898052 -0.02986472 -0.0231012  -0.01538033]
