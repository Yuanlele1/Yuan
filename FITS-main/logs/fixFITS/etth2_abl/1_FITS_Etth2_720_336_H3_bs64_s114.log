Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=103, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=103, out_features=151, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13935488.0
params:  15704.0
Trainable parameters:  15704
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.760291576385498
Epoch: 1, Steps: 59 | Train Loss: 0.8495475 Vali Loss: 0.5252915 Test Loss: 0.3968578
Validation loss decreased (inf --> 0.525292).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.809370756149292
Epoch: 2, Steps: 59 | Train Loss: 0.7133888 Vali Loss: 0.4678719 Test Loss: 0.3741307
Validation loss decreased (0.525292 --> 0.467872).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.269979476928711
Epoch: 3, Steps: 59 | Train Loss: 0.6764499 Vali Loss: 0.4423497 Test Loss: 0.3674757
Validation loss decreased (0.467872 --> 0.442350).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.1751320362091064
Epoch: 4, Steps: 59 | Train Loss: 0.6631515 Vali Loss: 0.4293186 Test Loss: 0.3645938
Validation loss decreased (0.442350 --> 0.429319).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.6053144931793213
Epoch: 5, Steps: 59 | Train Loss: 0.6518338 Vali Loss: 0.4201545 Test Loss: 0.3630237
Validation loss decreased (0.429319 --> 0.420155).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.706218957901001
Epoch: 6, Steps: 59 | Train Loss: 0.6467910 Vali Loss: 0.4149869 Test Loss: 0.3624316
Validation loss decreased (0.420155 --> 0.414987).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.3624143600463867
Epoch: 7, Steps: 59 | Train Loss: 0.6404794 Vali Loss: 0.4120255 Test Loss: 0.3618358
Validation loss decreased (0.414987 --> 0.412025).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.1603572368621826
Epoch: 8, Steps: 59 | Train Loss: 0.6385967 Vali Loss: 0.4048362 Test Loss: 0.3613078
Validation loss decreased (0.412025 --> 0.404836).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.780456781387329
Epoch: 9, Steps: 59 | Train Loss: 0.6351797 Vali Loss: 0.4026781 Test Loss: 0.3613287
Validation loss decreased (0.404836 --> 0.402678).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.4499335289001465
Epoch: 10, Steps: 59 | Train Loss: 0.6343469 Vali Loss: 0.4022603 Test Loss: 0.3611965
Validation loss decreased (0.402678 --> 0.402260).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.440181016921997
Epoch: 11, Steps: 59 | Train Loss: 0.6319000 Vali Loss: 0.3997825 Test Loss: 0.3608004
Validation loss decreased (0.402260 --> 0.399782).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1842429637908936
Epoch: 12, Steps: 59 | Train Loss: 0.6316296 Vali Loss: 0.4000056 Test Loss: 0.3608225
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.2320196628570557
Epoch: 13, Steps: 59 | Train Loss: 0.6285665 Vali Loss: 0.3988550 Test Loss: 0.3606898
Validation loss decreased (0.399782 --> 0.398855).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.8806302547454834
Epoch: 14, Steps: 59 | Train Loss: 0.6293786 Vali Loss: 0.3952096 Test Loss: 0.3606597
Validation loss decreased (0.398855 --> 0.395210).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.534083366394043
Epoch: 15, Steps: 59 | Train Loss: 0.6283340 Vali Loss: 0.3974584 Test Loss: 0.3605417
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.636781930923462
Epoch: 16, Steps: 59 | Train Loss: 0.6267949 Vali Loss: 0.3938870 Test Loss: 0.3605306
Validation loss decreased (0.395210 --> 0.393887).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.5495433807373047
Epoch: 17, Steps: 59 | Train Loss: 0.6264969 Vali Loss: 0.3936789 Test Loss: 0.3604759
Validation loss decreased (0.393887 --> 0.393679).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.3654427528381348
Epoch: 18, Steps: 59 | Train Loss: 0.6249468 Vali Loss: 0.3928139 Test Loss: 0.3604257
Validation loss decreased (0.393679 --> 0.392814).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.3798818588256836
Epoch: 19, Steps: 59 | Train Loss: 0.6233135 Vali Loss: 0.3916168 Test Loss: 0.3603388
Validation loss decreased (0.392814 --> 0.391617).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.3971076011657715
Epoch: 20, Steps: 59 | Train Loss: 0.6243688 Vali Loss: 0.3913603 Test Loss: 0.3602695
Validation loss decreased (0.391617 --> 0.391360).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.660038471221924
Epoch: 21, Steps: 59 | Train Loss: 0.6232376 Vali Loss: 0.3892917 Test Loss: 0.3601597
Validation loss decreased (0.391360 --> 0.389292).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.927518844604492
Epoch: 22, Steps: 59 | Train Loss: 0.6220288 Vali Loss: 0.3902464 Test Loss: 0.3602298
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.0231075286865234
Epoch: 23, Steps: 59 | Train Loss: 0.6224770 Vali Loss: 0.3883432 Test Loss: 0.3603193
Validation loss decreased (0.389292 --> 0.388343).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.5947766304016113
Epoch: 24, Steps: 59 | Train Loss: 0.6226881 Vali Loss: 0.3899319 Test Loss: 0.3600745
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.4137258529663086
Epoch: 25, Steps: 59 | Train Loss: 0.6217648 Vali Loss: 0.3893267 Test Loss: 0.3602310
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.7062888145446777
Epoch: 26, Steps: 59 | Train Loss: 0.6213589 Vali Loss: 0.3895360 Test Loss: 0.3601751
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.988629102706909
Epoch: 27, Steps: 59 | Train Loss: 0.6199851 Vali Loss: 0.3859662 Test Loss: 0.3601069
Validation loss decreased (0.388343 --> 0.385966).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.6406967639923096
Epoch: 28, Steps: 59 | Train Loss: 0.6202519 Vali Loss: 0.3892636 Test Loss: 0.3601817
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.063887596130371
Epoch: 29, Steps: 59 | Train Loss: 0.6215039 Vali Loss: 0.3894686 Test Loss: 0.3601585
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.7473790645599365
Epoch: 30, Steps: 59 | Train Loss: 0.6214308 Vali Loss: 0.3890390 Test Loss: 0.3600705
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.7572271823883057
Epoch: 31, Steps: 59 | Train Loss: 0.6196047 Vali Loss: 0.3869212 Test Loss: 0.3601564
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.256061553955078
Epoch: 32, Steps: 59 | Train Loss: 0.6200647 Vali Loss: 0.3875196 Test Loss: 0.3600786
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.1137337684631348
Epoch: 33, Steps: 59 | Train Loss: 0.6192390 Vali Loss: 0.3883724 Test Loss: 0.3600911
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.5115160942077637
Epoch: 34, Steps: 59 | Train Loss: 0.6199766 Vali Loss: 0.3839111 Test Loss: 0.3600760
Validation loss decreased (0.385966 --> 0.383911).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.414308547973633
Epoch: 35, Steps: 59 | Train Loss: 0.6189331 Vali Loss: 0.3874368 Test Loss: 0.3600697
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.711280107498169
Epoch: 36, Steps: 59 | Train Loss: 0.6181109 Vali Loss: 0.3893532 Test Loss: 0.3600694
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.289876937866211
Epoch: 37, Steps: 59 | Train Loss: 0.6182767 Vali Loss: 0.3893152 Test Loss: 0.3600367
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.5173661708831787
Epoch: 38, Steps: 59 | Train Loss: 0.6186462 Vali Loss: 0.3858685 Test Loss: 0.3600135
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.1287317276000977
Epoch: 39, Steps: 59 | Train Loss: 0.6189793 Vali Loss: 0.3853874 Test Loss: 0.3600629
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.8281478881835938
Epoch: 40, Steps: 59 | Train Loss: 0.6181463 Vali Loss: 0.3872123 Test Loss: 0.3600240
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.4900965690612793
Epoch: 41, Steps: 59 | Train Loss: 0.6196921 Vali Loss: 0.3836228 Test Loss: 0.3600141
Validation loss decreased (0.383911 --> 0.383623).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.5172810554504395
Epoch: 42, Steps: 59 | Train Loss: 0.6190943 Vali Loss: 0.3868289 Test Loss: 0.3600157
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.7777388095855713
Epoch: 43, Steps: 59 | Train Loss: 0.6190309 Vali Loss: 0.3842774 Test Loss: 0.3600357
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.448328733444214
Epoch: 44, Steps: 59 | Train Loss: 0.6183711 Vali Loss: 0.3850566 Test Loss: 0.3600311
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.751370906829834
Epoch: 45, Steps: 59 | Train Loss: 0.6185105 Vali Loss: 0.3858099 Test Loss: 0.3600290
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.895113468170166
Epoch: 46, Steps: 59 | Train Loss: 0.6160966 Vali Loss: 0.3869485 Test Loss: 0.3600291
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.0545997619628906
Epoch: 47, Steps: 59 | Train Loss: 0.6184085 Vali Loss: 0.3836164 Test Loss: 0.3600130
Validation loss decreased (0.383623 --> 0.383616).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.9053194522857666
Epoch: 48, Steps: 59 | Train Loss: 0.6179548 Vali Loss: 0.3840825 Test Loss: 0.3600344
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.7264795303344727
Epoch: 49, Steps: 59 | Train Loss: 0.6183007 Vali Loss: 0.3834483 Test Loss: 0.3600221
Validation loss decreased (0.383616 --> 0.383448).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.9364800453186035
Epoch: 50, Steps: 59 | Train Loss: 0.6174118 Vali Loss: 0.3848110 Test Loss: 0.3600452
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.590155601501465
Epoch: 51, Steps: 59 | Train Loss: 0.6175201 Vali Loss: 0.3811402 Test Loss: 0.3600142
Validation loss decreased (0.383448 --> 0.381140).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.0057220458984375
Epoch: 52, Steps: 59 | Train Loss: 0.6185374 Vali Loss: 0.3840279 Test Loss: 0.3600316
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.322401523590088
Epoch: 53, Steps: 59 | Train Loss: 0.6183138 Vali Loss: 0.3844750 Test Loss: 0.3600422
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.4250102043151855
Epoch: 54, Steps: 59 | Train Loss: 0.6178635 Vali Loss: 0.3838534 Test Loss: 0.3600550
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.9986133575439453
Epoch: 55, Steps: 59 | Train Loss: 0.6168488 Vali Loss: 0.3839965 Test Loss: 0.3600390
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.4687421321868896
Epoch: 56, Steps: 59 | Train Loss: 0.6179363 Vali Loss: 0.3837850 Test Loss: 0.3600305
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.281949043273926
Epoch: 57, Steps: 59 | Train Loss: 0.6167035 Vali Loss: 0.3829682 Test Loss: 0.3600476
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.5403060913085938
Epoch: 58, Steps: 59 | Train Loss: 0.6182374 Vali Loss: 0.3839223 Test Loss: 0.3600349
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.006277084350586
Epoch: 59, Steps: 59 | Train Loss: 0.6164658 Vali Loss: 0.3827528 Test Loss: 0.3600455
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.717400550842285
Epoch: 60, Steps: 59 | Train Loss: 0.6163109 Vali Loss: 0.3822607 Test Loss: 0.3600316
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.2084426879882812
Epoch: 61, Steps: 59 | Train Loss: 0.6169313 Vali Loss: 0.3865504 Test Loss: 0.3600329
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.6355321407318115
Epoch: 62, Steps: 59 | Train Loss: 0.6167626 Vali Loss: 0.3834198 Test Loss: 0.3600358
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.7046265602111816
Epoch: 63, Steps: 59 | Train Loss: 0.6169036 Vali Loss: 0.3807853 Test Loss: 0.3600410
Validation loss decreased (0.381140 --> 0.380785).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.842421770095825
Epoch: 64, Steps: 59 | Train Loss: 0.6177529 Vali Loss: 0.3827811 Test Loss: 0.3600279
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.543846845626831
Epoch: 65, Steps: 59 | Train Loss: 0.6173392 Vali Loss: 0.3866792 Test Loss: 0.3600485
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.160912275314331
Epoch: 66, Steps: 59 | Train Loss: 0.6174748 Vali Loss: 0.3835046 Test Loss: 0.3600355
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.8783562183380127
Epoch: 67, Steps: 59 | Train Loss: 0.6164299 Vali Loss: 0.3819632 Test Loss: 0.3600432
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.5624840259552
Epoch: 68, Steps: 59 | Train Loss: 0.6184754 Vali Loss: 0.3849266 Test Loss: 0.3600484
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.8321175575256348
Epoch: 69, Steps: 59 | Train Loss: 0.6155376 Vali Loss: 0.3845851 Test Loss: 0.3600366
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.12129545211792
Epoch: 70, Steps: 59 | Train Loss: 0.6167892 Vali Loss: 0.3846852 Test Loss: 0.3600385
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.859877586364746
Epoch: 71, Steps: 59 | Train Loss: 0.6163371 Vali Loss: 0.3826958 Test Loss: 0.3600384
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.4374425411224365
Epoch: 72, Steps: 59 | Train Loss: 0.6166972 Vali Loss: 0.3843341 Test Loss: 0.3600377
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.1911873817443848
Epoch: 73, Steps: 59 | Train Loss: 0.6181880 Vali Loss: 0.3848579 Test Loss: 0.3600430
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.322888135910034
Epoch: 74, Steps: 59 | Train Loss: 0.6170076 Vali Loss: 0.3847953 Test Loss: 0.3600485
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.1625378131866455
Epoch: 75, Steps: 59 | Train Loss: 0.6166691 Vali Loss: 0.3829582 Test Loss: 0.3600388
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.9470994472503662
Epoch: 76, Steps: 59 | Train Loss: 0.6167917 Vali Loss: 0.3853841 Test Loss: 0.3600419
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.9875547885894775
Epoch: 77, Steps: 59 | Train Loss: 0.6167184 Vali Loss: 0.3847546 Test Loss: 0.3600414
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.572455883026123
Epoch: 78, Steps: 59 | Train Loss: 0.6176410 Vali Loss: 0.3854177 Test Loss: 0.3600356
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.6280875205993652
Epoch: 79, Steps: 59 | Train Loss: 0.6179964 Vali Loss: 0.3844783 Test Loss: 0.3600372
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.443427324295044
Epoch: 80, Steps: 59 | Train Loss: 0.6179899 Vali Loss: 0.3849243 Test Loss: 0.3600417
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.6009154319763184
Epoch: 81, Steps: 59 | Train Loss: 0.6180418 Vali Loss: 0.3855468 Test Loss: 0.3600411
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.9294183254241943
Epoch: 82, Steps: 59 | Train Loss: 0.6162621 Vali Loss: 0.3825691 Test Loss: 0.3600437
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.4830126762390137
Epoch: 83, Steps: 59 | Train Loss: 0.6160007 Vali Loss: 0.3829183 Test Loss: 0.3600428
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3558245301246643, mae:0.39663004875183105, rse:0.476933091878891, corr:[0.25714067 0.2623943  0.2629981  0.26066643 0.25931078 0.25949034
 0.26010227 0.25990725 0.25843495 0.2563114  0.25469664 0.25387135
 0.25365046 0.2532561  0.2523734  0.25104317 0.24973679 0.24880534
 0.24834187 0.24797061 0.24723361 0.24590036 0.24404986 0.24232186
 0.24105862 0.24033712 0.23971944 0.23884687 0.23765323 0.23635745
 0.23525015 0.23443897 0.23385309 0.23322958 0.23233831 0.23127164
 0.23030117 0.22965449 0.22927621 0.22887355 0.2281717  0.22705998
 0.22582194 0.2247858  0.22390798 0.22306088 0.22188847 0.22021465
 0.21830364 0.21656585 0.21510342 0.21377502 0.21252559 0.21109094
 0.20922205 0.20728834 0.20560703 0.20426954 0.20331594 0.20264202
 0.20210828 0.20161963 0.20118713 0.20086455 0.2004335  0.20011112
 0.19972518 0.199331   0.19877805 0.19817945 0.1973946  0.19650619
 0.19558471 0.19468023 0.1938302  0.19291247 0.19207317 0.19129843
 0.19062772 0.19000967 0.1896163  0.18929653 0.18891928 0.18848623
 0.18813044 0.18782459 0.18750384 0.18711202 0.18662256 0.1861791
 0.18591757 0.18573594 0.18577999 0.18587175 0.18578525 0.18540314
 0.1847233  0.18384409 0.18309088 0.18249072 0.18208058 0.18177007
 0.18160847 0.18132108 0.18101253 0.18059921 0.18028265 0.17998248
 0.17948294 0.1788454  0.17810123 0.17748652 0.17694306 0.17663053
 0.17639343 0.17607397 0.17554407 0.1745141  0.17320284 0.17178585
 0.17057523 0.16972698 0.16920106 0.16875766 0.1680454  0.16709252
 0.1659556  0.16501644 0.16439931 0.1639673  0.16355881 0.16291544
 0.16212335 0.16125235 0.16056164 0.16013029 0.15979978 0.15943108
 0.15885848 0.15809764 0.15727644 0.15643468 0.15556788 0.15455702
 0.15326543 0.15178849 0.15026082 0.14888616 0.14782967 0.14720796
 0.14684975 0.1463761  0.14572889 0.14483918 0.14387862 0.14299767
 0.14255972 0.14249885 0.14247592 0.1421228  0.14126678 0.1403997
 0.13989927 0.13989283 0.14014383 0.14011362 0.13948326 0.1381671
 0.13663645 0.13527997 0.13437873 0.1337534  0.13308348 0.13191155
 0.1304297  0.12874556 0.12740499 0.12668487 0.12637495 0.12622675
 0.12591866 0.1253814  0.12477437 0.12437452 0.12403387 0.12395424
 0.12400668 0.12393343 0.12376931 0.12355985 0.12361336 0.12365118
 0.12363377 0.1234227  0.12277777 0.12190119 0.12097015 0.12022427
 0.11989725 0.11983564 0.11984184 0.11953338 0.1188748  0.11800532
 0.11742364 0.11749772 0.11796246 0.11848059 0.11856377 0.11826596
 0.11779716 0.11768471 0.11802374 0.11845866 0.1185034  0.1176916
 0.1162294  0.11466597 0.11359378 0.11341781 0.11371149 0.11418752
 0.11407431 0.11353227 0.11265535 0.11187486 0.11154396 0.1115032
 0.11143641 0.11118972 0.1107251  0.11053233 0.11069085 0.11137831
 0.11224414 0.11295245 0.11314873 0.11289298 0.11245322 0.11208398
 0.11182776 0.11159468 0.11121701 0.11079415 0.11014153 0.10936881
 0.10907301 0.10930901 0.10992412 0.11042738 0.11053567 0.1103784
 0.11018956 0.11064479 0.11164277 0.11327337 0.11456495 0.11544082
 0.11553404 0.11542368 0.11573609 0.11634466 0.11745856 0.11832098
 0.11859258 0.11800835 0.11730169 0.11713    0.11745915 0.11808211
 0.1185032  0.1184092  0.11797142 0.11714841 0.11690571 0.11709353
 0.11773573 0.11830719 0.1185117  0.11836878 0.11812402 0.11802195
 0.11814276 0.11851196 0.11870589 0.11845133 0.11800791 0.11771362
 0.11767157 0.11778951 0.1176731  0.11711228 0.11586686 0.11407005
 0.11272459 0.11244478 0.11322232 0.1139962  0.11427423 0.11360513
 0.11218962 0.11141688 0.11189086 0.11363105 0.11500404 0.11577395
 0.11526994 0.11441565 0.11396692 0.11453048 0.11577303 0.11656997
 0.11633842 0.11470482 0.11281791 0.11150935 0.11139039 0.11217552
 0.11303955 0.11311597 0.11182112 0.1106576  0.11089727 0.11307316
 0.11553085 0.11692439 0.11701524 0.11628738 0.11584716 0.11674333
 0.11881895 0.1207375  0.12096896 0.11917637 0.11863734 0.12177847]
