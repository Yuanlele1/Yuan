Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=42, out_features=81, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3048192.0
params:  3483.0
Trainable parameters:  3483
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9663958549499512
Epoch: 1, Steps: 62 | Train Loss: 0.8138101 Vali Loss: 0.4721232 Test Loss: 0.4278501
Validation loss decreased (inf --> 0.472123).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9772794246673584
Epoch: 2, Steps: 62 | Train Loss: 0.7050969 Vali Loss: 0.4332700 Test Loss: 0.3956493
Validation loss decreased (0.472123 --> 0.433270).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.072834014892578
Epoch: 3, Steps: 62 | Train Loss: 0.6663733 Vali Loss: 0.4145392 Test Loss: 0.3832942
Validation loss decreased (0.433270 --> 0.414539).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.016524076461792
Epoch: 4, Steps: 62 | Train Loss: 0.6490597 Vali Loss: 0.4041601 Test Loss: 0.3780891
Validation loss decreased (0.414539 --> 0.404160).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8667702674865723
Epoch: 5, Steps: 62 | Train Loss: 0.6398794 Vali Loss: 0.4019022 Test Loss: 0.3750533
Validation loss decreased (0.404160 --> 0.401902).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.481292486190796
Epoch: 6, Steps: 62 | Train Loss: 0.6352035 Vali Loss: 0.3979272 Test Loss: 0.3729302
Validation loss decreased (0.401902 --> 0.397927).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.280607223510742
Epoch: 7, Steps: 62 | Train Loss: 0.6307770 Vali Loss: 0.3922089 Test Loss: 0.3713836
Validation loss decreased (0.397927 --> 0.392209).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.361269474029541
Epoch: 8, Steps: 62 | Train Loss: 0.6285173 Vali Loss: 0.3931231 Test Loss: 0.3703741
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.2130844593048096
Epoch: 9, Steps: 62 | Train Loss: 0.6257578 Vali Loss: 0.3910766 Test Loss: 0.3694530
Validation loss decreased (0.392209 --> 0.391077).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.847221851348877
Epoch: 10, Steps: 62 | Train Loss: 0.6246463 Vali Loss: 0.3898991 Test Loss: 0.3687374
Validation loss decreased (0.391077 --> 0.389899).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.3645355701446533
Epoch: 11, Steps: 62 | Train Loss: 0.6231311 Vali Loss: 0.3868971 Test Loss: 0.3682460
Validation loss decreased (0.389899 --> 0.386897).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.391737222671509
Epoch: 12, Steps: 62 | Train Loss: 0.6214646 Vali Loss: 0.3853735 Test Loss: 0.3677186
Validation loss decreased (0.386897 --> 0.385373).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.1202778816223145
Epoch: 13, Steps: 62 | Train Loss: 0.6209594 Vali Loss: 0.3857813 Test Loss: 0.3673314
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.4171531200408936
Epoch: 14, Steps: 62 | Train Loss: 0.6199078 Vali Loss: 0.3857229 Test Loss: 0.3669791
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.723545789718628
Epoch: 15, Steps: 62 | Train Loss: 0.6179523 Vali Loss: 0.3857694 Test Loss: 0.3668549
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.960075616836548
Epoch: 16, Steps: 62 | Train Loss: 0.6187269 Vali Loss: 0.3863017 Test Loss: 0.3665585
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.180577039718628
Epoch: 17, Steps: 62 | Train Loss: 0.6179902 Vali Loss: 0.3831034 Test Loss: 0.3662921
Validation loss decreased (0.385373 --> 0.383103).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.347343921661377
Epoch: 18, Steps: 62 | Train Loss: 0.6173289 Vali Loss: 0.3843264 Test Loss: 0.3662864
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.7411093711853027
Epoch: 19, Steps: 62 | Train Loss: 0.6173486 Vali Loss: 0.3780718 Test Loss: 0.3660444
Validation loss decreased (0.383103 --> 0.378072).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.209441900253296
Epoch: 20, Steps: 62 | Train Loss: 0.6164043 Vali Loss: 0.3821851 Test Loss: 0.3659516
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.7808613777160645
Epoch: 21, Steps: 62 | Train Loss: 0.6165883 Vali Loss: 0.3812566 Test Loss: 0.3658860
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.9211418628692627
Epoch: 22, Steps: 62 | Train Loss: 0.6164097 Vali Loss: 0.3811144 Test Loss: 0.3657756
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.4083807468414307
Epoch: 23, Steps: 62 | Train Loss: 0.6162235 Vali Loss: 0.3809273 Test Loss: 0.3657075
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.1675901412963867
Epoch: 24, Steps: 62 | Train Loss: 0.6145437 Vali Loss: 0.3846326 Test Loss: 0.3655269
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9555480480194092
Epoch: 25, Steps: 62 | Train Loss: 0.6154393 Vali Loss: 0.3810132 Test Loss: 0.3654940
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.469137191772461
Epoch: 26, Steps: 62 | Train Loss: 0.6147840 Vali Loss: 0.3815656 Test Loss: 0.3653791
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.2526307106018066
Epoch: 27, Steps: 62 | Train Loss: 0.6150395 Vali Loss: 0.3778183 Test Loss: 0.3654242
Validation loss decreased (0.378072 --> 0.377818).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.7093830108642578
Epoch: 28, Steps: 62 | Train Loss: 0.6147112 Vali Loss: 0.3834315 Test Loss: 0.3654078
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.088505983352661
Epoch: 29, Steps: 62 | Train Loss: 0.6148600 Vali Loss: 0.3816002 Test Loss: 0.3653160
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.2445321083068848
Epoch: 30, Steps: 62 | Train Loss: 0.6145881 Vali Loss: 0.3822730 Test Loss: 0.3652088
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.4056179523468018
Epoch: 31, Steps: 62 | Train Loss: 0.6140069 Vali Loss: 0.3804744 Test Loss: 0.3651696
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.511550188064575
Epoch: 32, Steps: 62 | Train Loss: 0.6139917 Vali Loss: 0.3809288 Test Loss: 0.3652212
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.7884631156921387
Epoch: 33, Steps: 62 | Train Loss: 0.6140072 Vali Loss: 0.3765844 Test Loss: 0.3651242
Validation loss decreased (0.377818 --> 0.376584).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9081306457519531
Epoch: 34, Steps: 62 | Train Loss: 0.6139823 Vali Loss: 0.3796417 Test Loss: 0.3651199
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.481322765350342
Epoch: 35, Steps: 62 | Train Loss: 0.6141267 Vali Loss: 0.3804421 Test Loss: 0.3651254
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.931950330734253
Epoch: 36, Steps: 62 | Train Loss: 0.6131783 Vali Loss: 0.3797225 Test Loss: 0.3650535
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.4858360290527344
Epoch: 37, Steps: 62 | Train Loss: 0.6136304 Vali Loss: 0.3772207 Test Loss: 0.3650962
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.338912010192871
Epoch: 38, Steps: 62 | Train Loss: 0.6135762 Vali Loss: 0.3793710 Test Loss: 0.3650078
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.0068957805633545
Epoch: 39, Steps: 62 | Train Loss: 0.6133355 Vali Loss: 0.3781730 Test Loss: 0.3649849
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.755314588546753
Epoch: 40, Steps: 62 | Train Loss: 0.6135150 Vali Loss: 0.3810426 Test Loss: 0.3649615
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.7322618961334229
Epoch: 41, Steps: 62 | Train Loss: 0.6136719 Vali Loss: 0.3781059 Test Loss: 0.3649370
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.8156564235687256
Epoch: 42, Steps: 62 | Train Loss: 0.6133818 Vali Loss: 0.3791676 Test Loss: 0.3649444
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.608916997909546
Epoch: 43, Steps: 62 | Train Loss: 0.6129010 Vali Loss: 0.3794732 Test Loss: 0.3649615
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.0714704990386963
Epoch: 44, Steps: 62 | Train Loss: 0.6132436 Vali Loss: 0.3791585 Test Loss: 0.3649321
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.36944580078125
Epoch: 45, Steps: 62 | Train Loss: 0.6131496 Vali Loss: 0.3792809 Test Loss: 0.3648615
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.378074884414673
Epoch: 46, Steps: 62 | Train Loss: 0.6123933 Vali Loss: 0.3812957 Test Loss: 0.3649011
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.1898117065429688
Epoch: 47, Steps: 62 | Train Loss: 0.6125279 Vali Loss: 0.3779888 Test Loss: 0.3648598
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.686450719833374
Epoch: 48, Steps: 62 | Train Loss: 0.6131413 Vali Loss: 0.3800634 Test Loss: 0.3648658
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.1490180492401123
Epoch: 49, Steps: 62 | Train Loss: 0.6124788 Vali Loss: 0.3772565 Test Loss: 0.3648344
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.557616949081421
Epoch: 50, Steps: 62 | Train Loss: 0.6121004 Vali Loss: 0.3795302 Test Loss: 0.3648362
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9402060508728027
Epoch: 51, Steps: 62 | Train Loss: 0.6130023 Vali Loss: 0.3804741 Test Loss: 0.3648103
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.8251471519470215
Epoch: 52, Steps: 62 | Train Loss: 0.6130742 Vali Loss: 0.3766522 Test Loss: 0.3648319
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.2250864505767822
Epoch: 53, Steps: 62 | Train Loss: 0.6125592 Vali Loss: 0.3764784 Test Loss: 0.3647899
Validation loss decreased (0.376584 --> 0.376478).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.4955928325653076
Epoch: 54, Steps: 62 | Train Loss: 0.6129656 Vali Loss: 0.3799867 Test Loss: 0.3647820
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.253342866897583
Epoch: 55, Steps: 62 | Train Loss: 0.6123998 Vali Loss: 0.3791825 Test Loss: 0.3647989
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9131495952606201
Epoch: 56, Steps: 62 | Train Loss: 0.6120956 Vali Loss: 0.3795982 Test Loss: 0.3647943
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.036011219024658
Epoch: 57, Steps: 62 | Train Loss: 0.6126392 Vali Loss: 0.3780822 Test Loss: 0.3647911
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.5958352088928223
Epoch: 58, Steps: 62 | Train Loss: 0.6124045 Vali Loss: 0.3764158 Test Loss: 0.3647782
Validation loss decreased (0.376478 --> 0.376416).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.3080294132232666
Epoch: 59, Steps: 62 | Train Loss: 0.6127117 Vali Loss: 0.3786800 Test Loss: 0.3647830
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.182934045791626
Epoch: 60, Steps: 62 | Train Loss: 0.6125675 Vali Loss: 0.3769739 Test Loss: 0.3647622
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.9145572185516357
Epoch: 61, Steps: 62 | Train Loss: 0.6127120 Vali Loss: 0.3791154 Test Loss: 0.3647680
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.1301748752593994
Epoch: 62, Steps: 62 | Train Loss: 0.6128409 Vali Loss: 0.3789853 Test Loss: 0.3647577
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.9322290420532227
Epoch: 63, Steps: 62 | Train Loss: 0.6124018 Vali Loss: 0.3794512 Test Loss: 0.3647623
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.2814650535583496
Epoch: 64, Steps: 62 | Train Loss: 0.6120912 Vali Loss: 0.3763874 Test Loss: 0.3647564
Validation loss decreased (0.376416 --> 0.376387).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.770646333694458
Epoch: 65, Steps: 62 | Train Loss: 0.6120469 Vali Loss: 0.3802158 Test Loss: 0.3647440
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.9746513366699219
Epoch: 66, Steps: 62 | Train Loss: 0.6126771 Vali Loss: 0.3775635 Test Loss: 0.3647386
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.847226858139038
Epoch: 67, Steps: 62 | Train Loss: 0.6124961 Vali Loss: 0.3776497 Test Loss: 0.3647495
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.8290507793426514
Epoch: 68, Steps: 62 | Train Loss: 0.6121981 Vali Loss: 0.3782077 Test Loss: 0.3647346
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.2225465774536133
Epoch: 69, Steps: 62 | Train Loss: 0.6125669 Vali Loss: 0.3811269 Test Loss: 0.3647320
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.0196428298950195
Epoch: 70, Steps: 62 | Train Loss: 0.6126065 Vali Loss: 0.3789198 Test Loss: 0.3647283
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.749082326889038
Epoch: 71, Steps: 62 | Train Loss: 0.6118651 Vali Loss: 0.3786024 Test Loss: 0.3647304
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.4433822631835938
Epoch: 72, Steps: 62 | Train Loss: 0.6117554 Vali Loss: 0.3771894 Test Loss: 0.3647217
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.246218204498291
Epoch: 73, Steps: 62 | Train Loss: 0.6121077 Vali Loss: 0.3782006 Test Loss: 0.3647319
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.264124870300293
Epoch: 74, Steps: 62 | Train Loss: 0.6122325 Vali Loss: 0.3807639 Test Loss: 0.3647182
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.104236364364624
Epoch: 75, Steps: 62 | Train Loss: 0.6123533 Vali Loss: 0.3781110 Test Loss: 0.3647243
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.1910853385925293
Epoch: 76, Steps: 62 | Train Loss: 0.6119128 Vali Loss: 0.3785601 Test Loss: 0.3647084
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.0147411823272705
Epoch: 77, Steps: 62 | Train Loss: 0.6114266 Vali Loss: 0.3787532 Test Loss: 0.3647144
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.8010294437408447
Epoch: 78, Steps: 62 | Train Loss: 0.6124649 Vali Loss: 0.3775477 Test Loss: 0.3647068
EarlyStopping counter: 14 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.324016809463501
Epoch: 79, Steps: 62 | Train Loss: 0.6120974 Vali Loss: 0.3789966 Test Loss: 0.3647054
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.0119006633758545
Epoch: 80, Steps: 62 | Train Loss: 0.6122050 Vali Loss: 0.3788164 Test Loss: 0.3647112
EarlyStopping counter: 16 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.8561031818389893
Epoch: 81, Steps: 62 | Train Loss: 0.6123414 Vali Loss: 0.3811369 Test Loss: 0.3647064
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.7216010093688965
Epoch: 82, Steps: 62 | Train Loss: 0.6123665 Vali Loss: 0.3807091 Test Loss: 0.3647025
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.2318432331085205
Epoch: 83, Steps: 62 | Train Loss: 0.6124523 Vali Loss: 0.3786479 Test Loss: 0.3647036
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.4569931030273438
Epoch: 84, Steps: 62 | Train Loss: 0.6124754 Vali Loss: 0.3781847 Test Loss: 0.3647034
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3603002727031708, mae:0.39804261922836304, rse:0.4799233078956604, corr:[0.25859314 0.26267183 0.26442906 0.2630528  0.26035368 0.2583573
 0.25749525 0.25715607 0.25673586 0.25563473 0.25389344 0.25181422
 0.25015268 0.24899326 0.24836868 0.24800472 0.24746187 0.2465365
 0.24523    0.24367926 0.24217327 0.24082173 0.23954938 0.23833156
 0.23699994 0.23558217 0.23409179 0.23256281 0.23115411 0.22999845
 0.22909082 0.22827856 0.22730102 0.22607687 0.22462855 0.22316656
 0.22180265 0.220669   0.2198665  0.2191908  0.2183691  0.21713898
 0.21549211 0.21360517 0.21165515 0.20992874 0.20838214 0.20689723
 0.20537098 0.20377156 0.20200968 0.20010288 0.1981335  0.19627471
 0.19462566 0.19325352 0.1922117  0.19121805 0.19030893 0.18952678
 0.18892938 0.18849696 0.18819635 0.18793085 0.18754694 0.18706888
 0.18632552 0.18542263 0.1843533  0.18340652 0.18260585 0.18198398
 0.18149692 0.18100537 0.18039724 0.17959101 0.17873523 0.1778961
 0.17721267 0.17677815 0.17668787 0.17671888 0.17674324 0.1765545
 0.1762269  0.1757209  0.17522725 0.17483167 0.17458351 0.17452125
 0.17459314 0.17455524 0.17446375 0.1741555  0.17363225 0.17297298
 0.17237727 0.17180014 0.17135824 0.17089625 0.17034204 0.16970187
 0.16916126 0.16861206 0.16833356 0.16824766 0.16835566 0.16849574
 0.16833979 0.16778101 0.16694105 0.16614154 0.1653843  0.16489644
 0.16463657 0.16443017 0.16420244 0.16356914 0.16255789 0.16120969
 0.15976161 0.15842777 0.15745956 0.15696293 0.15669952 0.15653832
 0.1561119  0.15549026 0.15469016 0.15380903 0.15313996 0.15275465
 0.15264289 0.15255342 0.15235949 0.15198772 0.15145144 0.15091352
 0.15033226 0.14994079 0.14972955 0.1493955  0.14877388 0.14784116
 0.14666799 0.14532442 0.14395243 0.14285624 0.14218864 0.14189158
 0.14190349 0.14195749 0.14192474 0.1416622  0.14116773 0.1404658
 0.13986808 0.13963804 0.13964503 0.13984743 0.14006568 0.14006351
 0.13986288 0.13947724 0.13905516 0.13860518 0.1382321  0.13779691
 0.13735732 0.13676183 0.13581179 0.13461111 0.13326342 0.13196687
 0.13102476 0.13044581 0.13031784 0.13032942 0.13032691 0.13000704
 0.1295506  0.12900807 0.12851466 0.12832822 0.1284343  0.12877226
 0.12930876 0.12982695 0.13011663 0.13023181 0.13010553 0.12966256
 0.1291958  0.12871379 0.1282797  0.12793794 0.12756668 0.12711923
 0.1267394  0.12645939 0.12631044 0.1262638  0.12639849 0.12643708
 0.12644845 0.12633552 0.12610677 0.12598579 0.12586984 0.12593074
 0.12614973 0.12662107 0.12709056 0.12712052 0.12672356 0.12576069
 0.12465879 0.12351017 0.12270919 0.12240911 0.12229823 0.12235495
 0.12219671 0.12189703 0.12133384 0.12054005 0.11995202 0.11955766
 0.11942667 0.11940293 0.11938583 0.11947136 0.11954713 0.11970671
 0.11987783 0.1203329  0.12080075 0.12101572 0.12086269 0.12027594
 0.11921614 0.11803491 0.11706731 0.11641789 0.1162254  0.1160684
 0.11590284 0.11571189 0.1154112  0.11489899 0.11459908 0.11468218
 0.1152401  0.11622878 0.11716833 0.11818368 0.11894773 0.11965548
 0.1200061  0.12030779 0.12089714 0.12142663 0.12197055 0.12217829
 0.12210357 0.12166831 0.12112816 0.12069015 0.12036497 0.12030313
 0.12038313 0.12064505 0.12085527 0.12081675 0.12097541 0.12088091
 0.12084983 0.12088236 0.12091406 0.12097608 0.12107727 0.12127145
 0.12136266 0.12146036 0.12149133 0.12141801 0.12136551 0.12141385
 0.12091293 0.11978744 0.11848665 0.11746924 0.11652374 0.11583289
 0.11527755 0.11480731 0.11457733 0.11414858 0.11380897 0.11362984
 0.11345449 0.11331902 0.11309065 0.11302397 0.11270463 0.11275279
 0.11279424 0.11311911 0.11363538 0.11366218 0.11362111 0.1130785
 0.11208314 0.11059084 0.10904898 0.10787172 0.10703637 0.10665948
 0.10653333 0.10658377 0.10633589 0.10590886 0.10582229 0.10608317
 0.10645227 0.10660729 0.10666365 0.10647308 0.10580015 0.10486623
 0.10415701 0.10462683 0.1065713  0.10898703 0.11074843 0.10873785]
