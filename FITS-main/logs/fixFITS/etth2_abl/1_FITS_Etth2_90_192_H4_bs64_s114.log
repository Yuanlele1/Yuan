Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_192_FITS_ETTh2_ftM_sl90_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8359
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=26, out_features=81, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1886976.0
params:  2187.0
Trainable parameters:  2187
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.0360047817230225
Epoch: 1, Steps: 65 | Train Loss: 0.7458713 Vali Loss: 0.3630230 Test Loss: 0.5106021
Validation loss decreased (inf --> 0.363023).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.2948551177978516
Epoch: 2, Steps: 65 | Train Loss: 0.6536668 Vali Loss: 0.3281181 Test Loss: 0.4617231
Validation loss decreased (0.363023 --> 0.328118).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.4937162399291992
Epoch: 3, Steps: 65 | Train Loss: 0.6111289 Vali Loss: 0.3098839 Test Loss: 0.4370438
Validation loss decreased (0.328118 --> 0.309884).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4078850746154785
Epoch: 4, Steps: 65 | Train Loss: 0.5880652 Vali Loss: 0.3002589 Test Loss: 0.4236484
Validation loss decreased (0.309884 --> 0.300259).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5258522033691406
Epoch: 5, Steps: 65 | Train Loss: 0.5726743 Vali Loss: 0.2943549 Test Loss: 0.4158598
Validation loss decreased (0.300259 --> 0.294355).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.5116937160491943
Epoch: 6, Steps: 65 | Train Loss: 0.5673576 Vali Loss: 0.2905605 Test Loss: 0.4110580
Validation loss decreased (0.294355 --> 0.290560).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.7189373970031738
Epoch: 7, Steps: 65 | Train Loss: 0.5623151 Vali Loss: 0.2879793 Test Loss: 0.4078965
Validation loss decreased (0.290560 --> 0.287979).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5161304473876953
Epoch: 8, Steps: 65 | Train Loss: 0.5597677 Vali Loss: 0.2860767 Test Loss: 0.4056900
Validation loss decreased (0.287979 --> 0.286077).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4799845218658447
Epoch: 9, Steps: 65 | Train Loss: 0.5571974 Vali Loss: 0.2847012 Test Loss: 0.4041575
Validation loss decreased (0.286077 --> 0.284701).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.744699239730835
Epoch: 10, Steps: 65 | Train Loss: 0.5546892 Vali Loss: 0.2835961 Test Loss: 0.4029550
Validation loss decreased (0.284701 --> 0.283596).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.044203519821167
Epoch: 11, Steps: 65 | Train Loss: 0.5517758 Vali Loss: 0.2827026 Test Loss: 0.4019704
Validation loss decreased (0.283596 --> 0.282703).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.58555006980896
Epoch: 12, Steps: 65 | Train Loss: 0.5512858 Vali Loss: 0.2819527 Test Loss: 0.4012520
Validation loss decreased (0.282703 --> 0.281953).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.3147053718566895
Epoch: 13, Steps: 65 | Train Loss: 0.5501723 Vali Loss: 0.2810547 Test Loss: 0.4006735
Validation loss decreased (0.281953 --> 0.281055).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.1590518951416016
Epoch: 14, Steps: 65 | Train Loss: 0.5489365 Vali Loss: 0.2808548 Test Loss: 0.4001867
Validation loss decreased (0.281055 --> 0.280855).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4375534057617188
Epoch: 15, Steps: 65 | Train Loss: 0.5488632 Vali Loss: 0.2803940 Test Loss: 0.3997546
Validation loss decreased (0.280855 --> 0.280394).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4189717769622803
Epoch: 16, Steps: 65 | Train Loss: 0.5467760 Vali Loss: 0.2800042 Test Loss: 0.3994283
Validation loss decreased (0.280394 --> 0.280004).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.1812002658843994
Epoch: 17, Steps: 65 | Train Loss: 0.5472662 Vali Loss: 0.2796662 Test Loss: 0.3991176
Validation loss decreased (0.280004 --> 0.279666).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6306414604187012
Epoch: 18, Steps: 65 | Train Loss: 0.5458749 Vali Loss: 0.2792306 Test Loss: 0.3988933
Validation loss decreased (0.279666 --> 0.279231).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.3336763381958008
Epoch: 19, Steps: 65 | Train Loss: 0.5471244 Vali Loss: 0.2790382 Test Loss: 0.3986516
Validation loss decreased (0.279231 --> 0.279038).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8292579650878906
Epoch: 20, Steps: 65 | Train Loss: 0.5441073 Vali Loss: 0.2787791 Test Loss: 0.3984235
Validation loss decreased (0.279038 --> 0.278779).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.5159649848937988
Epoch: 21, Steps: 65 | Train Loss: 0.5446677 Vali Loss: 0.2785762 Test Loss: 0.3982483
Validation loss decreased (0.278779 --> 0.278576).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.3852605819702148
Epoch: 22, Steps: 65 | Train Loss: 0.5456250 Vali Loss: 0.2779557 Test Loss: 0.3981460
Validation loss decreased (0.278576 --> 0.277956).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3148136138916016
Epoch: 23, Steps: 65 | Train Loss: 0.5445830 Vali Loss: 0.2782149 Test Loss: 0.3979979
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4570870399475098
Epoch: 24, Steps: 65 | Train Loss: 0.5435680 Vali Loss: 0.2779534 Test Loss: 0.3978986
Validation loss decreased (0.277956 --> 0.277953).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.6973464488983154
Epoch: 25, Steps: 65 | Train Loss: 0.5449395 Vali Loss: 0.2778733 Test Loss: 0.3977890
Validation loss decreased (0.277953 --> 0.277873).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.3839075565338135
Epoch: 26, Steps: 65 | Train Loss: 0.5444977 Vali Loss: 0.2776898 Test Loss: 0.3976717
Validation loss decreased (0.277873 --> 0.277690).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4080841541290283
Epoch: 27, Steps: 65 | Train Loss: 0.5433630 Vali Loss: 0.2775449 Test Loss: 0.3975818
Validation loss decreased (0.277690 --> 0.277545).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.540574073791504
Epoch: 28, Steps: 65 | Train Loss: 0.5435954 Vali Loss: 0.2774607 Test Loss: 0.3975417
Validation loss decreased (0.277545 --> 0.277461).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.290370225906372
Epoch: 29, Steps: 65 | Train Loss: 0.5427831 Vali Loss: 0.2772907 Test Loss: 0.3974661
Validation loss decreased (0.277461 --> 0.277291).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5101802349090576
Epoch: 30, Steps: 65 | Train Loss: 0.5437371 Vali Loss: 0.2772129 Test Loss: 0.3974220
Validation loss decreased (0.277291 --> 0.277213).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.4108552932739258
Epoch: 31, Steps: 65 | Train Loss: 0.5431649 Vali Loss: 0.2771623 Test Loss: 0.3973469
Validation loss decreased (0.277213 --> 0.277162).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.8869121074676514
Epoch: 32, Steps: 65 | Train Loss: 0.5430177 Vali Loss: 0.2770800 Test Loss: 0.3973232
Validation loss decreased (0.277162 --> 0.277080).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.6227128505706787
Epoch: 33, Steps: 65 | Train Loss: 0.5421006 Vali Loss: 0.2770036 Test Loss: 0.3972647
Validation loss decreased (0.277080 --> 0.277004).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6238903999328613
Epoch: 34, Steps: 65 | Train Loss: 0.5431356 Vali Loss: 0.2768912 Test Loss: 0.3972297
Validation loss decreased (0.277004 --> 0.276891).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.6165990829467773
Epoch: 35, Steps: 65 | Train Loss: 0.5426852 Vali Loss: 0.2767907 Test Loss: 0.3972122
Validation loss decreased (0.276891 --> 0.276791).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.2913658618927002
Epoch: 36, Steps: 65 | Train Loss: 0.5421509 Vali Loss: 0.2767416 Test Loss: 0.3971696
Validation loss decreased (0.276791 --> 0.276742).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5042846202850342
Epoch: 37, Steps: 65 | Train Loss: 0.5418513 Vali Loss: 0.2767083 Test Loss: 0.3971428
Validation loss decreased (0.276742 --> 0.276708).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4991240501403809
Epoch: 38, Steps: 65 | Train Loss: 0.5422560 Vali Loss: 0.2766572 Test Loss: 0.3970895
Validation loss decreased (0.276708 --> 0.276657).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.4885082244873047
Epoch: 39, Steps: 65 | Train Loss: 0.5414971 Vali Loss: 0.2766019 Test Loss: 0.3970759
Validation loss decreased (0.276657 --> 0.276602).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.8920025825500488
Epoch: 40, Steps: 65 | Train Loss: 0.5410689 Vali Loss: 0.2764864 Test Loss: 0.3970658
Validation loss decreased (0.276602 --> 0.276486).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.4796879291534424
Epoch: 41, Steps: 65 | Train Loss: 0.5421366 Vali Loss: 0.2765075 Test Loss: 0.3970414
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.7281789779663086
Epoch: 42, Steps: 65 | Train Loss: 0.5423990 Vali Loss: 0.2764348 Test Loss: 0.3970259
Validation loss decreased (0.276486 --> 0.276435).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.4116089344024658
Epoch: 43, Steps: 65 | Train Loss: 0.5420059 Vali Loss: 0.2764041 Test Loss: 0.3970082
Validation loss decreased (0.276435 --> 0.276404).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.321491003036499
Epoch: 44, Steps: 65 | Train Loss: 0.5414035 Vali Loss: 0.2759898 Test Loss: 0.3969792
Validation loss decreased (0.276404 --> 0.275990).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.6036159992218018
Epoch: 45, Steps: 65 | Train Loss: 0.5373227 Vali Loss: 0.2763358 Test Loss: 0.3969563
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.5998308658599854
Epoch: 46, Steps: 65 | Train Loss: 0.5411006 Vali Loss: 0.2763111 Test Loss: 0.3969422
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.6886284351348877
Epoch: 47, Steps: 65 | Train Loss: 0.5407733 Vali Loss: 0.2762743 Test Loss: 0.3969382
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.4055516719818115
Epoch: 48, Steps: 65 | Train Loss: 0.5413920 Vali Loss: 0.2761824 Test Loss: 0.3969215
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.429396629333496
Epoch: 49, Steps: 65 | Train Loss: 0.5400520 Vali Loss: 0.2762205 Test Loss: 0.3969048
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.3946211338043213
Epoch: 50, Steps: 65 | Train Loss: 0.5414170 Vali Loss: 0.2761866 Test Loss: 0.3969008
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.1852986812591553
Epoch: 51, Steps: 65 | Train Loss: 0.5414331 Vali Loss: 0.2758417 Test Loss: 0.3968950
Validation loss decreased (0.275990 --> 0.275842).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6872715950012207
Epoch: 52, Steps: 65 | Train Loss: 0.5410806 Vali Loss: 0.2761144 Test Loss: 0.3968918
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.861858606338501
Epoch: 53, Steps: 65 | Train Loss: 0.5398718 Vali Loss: 0.2760364 Test Loss: 0.3968769
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.2084925174713135
Epoch: 54, Steps: 65 | Train Loss: 0.5406192 Vali Loss: 0.2760679 Test Loss: 0.3968763
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.524770975112915
Epoch: 55, Steps: 65 | Train Loss: 0.5390489 Vali Loss: 0.2759902 Test Loss: 0.3968653
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.1736304759979248
Epoch: 56, Steps: 65 | Train Loss: 0.5402752 Vali Loss: 0.2756248 Test Loss: 0.3968600
Validation loss decreased (0.275842 --> 0.275625).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.537942886352539
Epoch: 57, Steps: 65 | Train Loss: 0.5400978 Vali Loss: 0.2760322 Test Loss: 0.3968515
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.215024471282959
Epoch: 58, Steps: 65 | Train Loss: 0.5406815 Vali Loss: 0.2760257 Test Loss: 0.3968524
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.448331594467163
Epoch: 59, Steps: 65 | Train Loss: 0.5412376 Vali Loss: 0.2759350 Test Loss: 0.3968417
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.3886215686798096
Epoch: 60, Steps: 65 | Train Loss: 0.5408949 Vali Loss: 0.2758449 Test Loss: 0.3968474
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.5159201622009277
Epoch: 61, Steps: 65 | Train Loss: 0.5412795 Vali Loss: 0.2759598 Test Loss: 0.3968364
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.428722620010376
Epoch: 62, Steps: 65 | Train Loss: 0.5400258 Vali Loss: 0.2756393 Test Loss: 0.3968325
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.3258750438690186
Epoch: 63, Steps: 65 | Train Loss: 0.5405435 Vali Loss: 0.2759071 Test Loss: 0.3968279
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.3006508350372314
Epoch: 64, Steps: 65 | Train Loss: 0.5403394 Vali Loss: 0.2758548 Test Loss: 0.3968216
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.424924373626709
Epoch: 65, Steps: 65 | Train Loss: 0.5404434 Vali Loss: 0.2759125 Test Loss: 0.3968180
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.332348346710205
Epoch: 66, Steps: 65 | Train Loss: 0.5412213 Vali Loss: 0.2759202 Test Loss: 0.3968183
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.5263135433197021
Epoch: 67, Steps: 65 | Train Loss: 0.5383974 Vali Loss: 0.2759100 Test Loss: 0.3968165
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.3344097137451172
Epoch: 68, Steps: 65 | Train Loss: 0.5408757 Vali Loss: 0.2758828 Test Loss: 0.3968120
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3149456977844238
Epoch: 69, Steps: 65 | Train Loss: 0.5401250 Vali Loss: 0.2758891 Test Loss: 0.3968084
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.4606554508209229
Epoch: 70, Steps: 65 | Train Loss: 0.5406027 Vali Loss: 0.2758636 Test Loss: 0.3968067
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.2062454223632812
Epoch: 71, Steps: 65 | Train Loss: 0.5412010 Vali Loss: 0.2757933 Test Loss: 0.3968064
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.6073720455169678
Epoch: 72, Steps: 65 | Train Loss: 0.5388294 Vali Loss: 0.2758292 Test Loss: 0.3968004
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.4165451526641846
Epoch: 73, Steps: 65 | Train Loss: 0.5395867 Vali Loss: 0.2758385 Test Loss: 0.3967982
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.4818377494812012
Epoch: 74, Steps: 65 | Train Loss: 0.5397837 Vali Loss: 0.2757118 Test Loss: 0.3967993
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.8091638088226318
Epoch: 75, Steps: 65 | Train Loss: 0.5409648 Vali Loss: 0.2758425 Test Loss: 0.3967962
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.4100537300109863
Epoch: 76, Steps: 65 | Train Loss: 0.5404558 Vali Loss: 0.2758196 Test Loss: 0.3967939
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_192_FITS_ETTh2_ftM_sl90_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3788157105445862, mae:0.3914967477321625, rse:0.49357691407203674, corr:[0.26687118 0.27108234 0.26706263 0.2675092  0.26683092 0.26447037
 0.26414704 0.26377955 0.26217625 0.26129544 0.26064813 0.25858423
 0.25669047 0.2559125  0.25491744 0.25385132 0.25349534 0.25307158
 0.25191128 0.25070283 0.24973264 0.24859205 0.24724908 0.24519657
 0.24195056 0.23918456 0.23704156 0.23517542 0.23331918 0.2320982
 0.23127812 0.22962047 0.22836123 0.22744952 0.22631282 0.22485714
 0.22366424 0.2225794  0.22133306 0.22050124 0.21999572 0.21944681
 0.21896341 0.21809965 0.21683538 0.21564738 0.21447767 0.21204318
 0.2082829  0.20564961 0.20325845 0.20067266 0.19869229 0.1973023
 0.19560821 0.19303569 0.19214965 0.1915342  0.19051443 0.18921341
 0.18845022 0.18807557 0.18763654 0.18758737 0.18737243 0.18664037
 0.18608151 0.18548165 0.18459232 0.18372989 0.18303518 0.1814722
 0.17840406 0.17672785 0.17564602 0.17397152 0.17248122 0.17252508
 0.17276065 0.17132725 0.17076397 0.17108917 0.17096396 0.17015797
 0.16980729 0.16994175 0.16961753 0.16939709 0.16934189 0.16879885
 0.1680264  0.16756889 0.16731003 0.16650335 0.16575666 0.16478023
 0.16225088 0.1599074  0.15826225 0.15674382 0.1549697  0.15403916
 0.15443693 0.15370682 0.15320429 0.15346879 0.15377268 0.15316294
 0.15250123 0.15257609 0.15199585 0.15122087 0.1510145  0.15074809
 0.15009877 0.14940225 0.14903405 0.14791486 0.14612162 0.14423327
 0.14147031 0.13892296 0.13726585 0.13636899 0.13467622 0.13330407
 0.13338412 0.13312456 0.13252114 0.13233146 0.13276201 0.13221605
 0.13126321 0.13114344 0.13099888 0.13019952 0.12962045 0.1294731
 0.12886755 0.12799734 0.12780966 0.12723635 0.12551075 0.12344556
 0.12041476 0.1176188  0.11552754 0.11428906 0.11258605 0.11100021
 0.11113284 0.11095098 0.11049183 0.11018097 0.11094908 0.11105932
 0.11028706 0.11034583 0.11069748 0.11044601 0.10987569 0.10978813
 0.1096782  0.10906976 0.10923825 0.10916451 0.10821243 0.10676299
 0.1040948  0.10167344 0.10024689 0.09986367 0.09835181 0.09657045
 0.09705143 0.09745827 0.09700517 0.09630042 0.09712901 0.09738351
 0.0964723  0.0962704  0.09650599 0.09647177 0.09559005 0.09483756
 0.09448631 0.09443799 0.09494535 0.09416587 0.09546461 0.09962826]
