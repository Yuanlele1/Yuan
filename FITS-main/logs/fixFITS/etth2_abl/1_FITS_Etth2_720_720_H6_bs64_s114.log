Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.5975265502929688
Epoch: 1, Steps: 56 | Train Loss: 1.0296828 Vali Loss: 0.7782894 Test Loss: 0.4328576
Validation loss decreased (inf --> 0.778289).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6427905559539795
Epoch: 2, Steps: 56 | Train Loss: 0.8919047 Vali Loss: 0.7351655 Test Loss: 0.4079538
Validation loss decreased (0.778289 --> 0.735166).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8699259757995605
Epoch: 3, Steps: 56 | Train Loss: 0.8614753 Vali Loss: 0.7121031 Test Loss: 0.3987614
Validation loss decreased (0.735166 --> 0.712103).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.8734910488128662
Epoch: 4, Steps: 56 | Train Loss: 0.8467456 Vali Loss: 0.6996101 Test Loss: 0.3936287
Validation loss decreased (0.712103 --> 0.699610).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8371832370758057
Epoch: 5, Steps: 56 | Train Loss: 0.8371202 Vali Loss: 0.6896808 Test Loss: 0.3902935
Validation loss decreased (0.699610 --> 0.689681).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.8628602027893066
Epoch: 6, Steps: 56 | Train Loss: 0.8307197 Vali Loss: 0.6818236 Test Loss: 0.3878268
Validation loss decreased (0.689681 --> 0.681824).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.8588740825653076
Epoch: 7, Steps: 56 | Train Loss: 0.8252515 Vali Loss: 0.6773739 Test Loss: 0.3859622
Validation loss decreased (0.681824 --> 0.677374).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.522444725036621
Epoch: 8, Steps: 56 | Train Loss: 0.8212751 Vali Loss: 0.6703005 Test Loss: 0.3845450
Validation loss decreased (0.677374 --> 0.670300).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7349669933319092
Epoch: 9, Steps: 56 | Train Loss: 0.8190138 Vali Loss: 0.6705328 Test Loss: 0.3834407
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6256802082061768
Epoch: 10, Steps: 56 | Train Loss: 0.8172098 Vali Loss: 0.6685493 Test Loss: 0.3825228
Validation loss decreased (0.670300 --> 0.668549).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7772035598754883
Epoch: 11, Steps: 56 | Train Loss: 0.8150936 Vali Loss: 0.6630638 Test Loss: 0.3818029
Validation loss decreased (0.668549 --> 0.663064).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.6967642307281494
Epoch: 12, Steps: 56 | Train Loss: 0.8137941 Vali Loss: 0.6641574 Test Loss: 0.3812602
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6422338485717773
Epoch: 13, Steps: 56 | Train Loss: 0.8120270 Vali Loss: 0.6618186 Test Loss: 0.3808401
Validation loss decreased (0.663064 --> 0.661819).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6193535327911377
Epoch: 14, Steps: 56 | Train Loss: 0.8080105 Vali Loss: 0.6578904 Test Loss: 0.3804918
Validation loss decreased (0.661819 --> 0.657890).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.8048529624938965
Epoch: 15, Steps: 56 | Train Loss: 0.8095738 Vali Loss: 0.6608676 Test Loss: 0.3801260
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.5919947624206543
Epoch: 16, Steps: 56 | Train Loss: 0.8089327 Vali Loss: 0.6569720 Test Loss: 0.3799200
Validation loss decreased (0.657890 --> 0.656972).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.5789268016815186
Epoch: 17, Steps: 56 | Train Loss: 0.8079794 Vali Loss: 0.6612662 Test Loss: 0.3797158
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.5054407119750977
Epoch: 18, Steps: 56 | Train Loss: 0.8066197 Vali Loss: 0.6591545 Test Loss: 0.3795724
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.1202998161315918
Epoch: 19, Steps: 56 | Train Loss: 0.8058878 Vali Loss: 0.6571280 Test Loss: 0.3793824
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 0.9730074405670166
Epoch: 20, Steps: 56 | Train Loss: 0.8053018 Vali Loss: 0.6570429 Test Loss: 0.3793433
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.352297306060791
Epoch: 21, Steps: 56 | Train Loss: 0.8049449 Vali Loss: 0.6541011 Test Loss: 0.3792940
Validation loss decreased (0.656972 --> 0.654101).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.4044842720031738
Epoch: 22, Steps: 56 | Train Loss: 0.8051059 Vali Loss: 0.6518250 Test Loss: 0.3791571
Validation loss decreased (0.654101 --> 0.651825).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.398205041885376
Epoch: 23, Steps: 56 | Train Loss: 0.8057325 Vali Loss: 0.6581587 Test Loss: 0.3791381
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.077986240386963
Epoch: 24, Steps: 56 | Train Loss: 0.8031044 Vali Loss: 0.6540762 Test Loss: 0.3791026
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.2548449039459229
Epoch: 25, Steps: 56 | Train Loss: 0.8037912 Vali Loss: 0.6538709 Test Loss: 0.3790759
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.311095952987671
Epoch: 26, Steps: 56 | Train Loss: 0.8047908 Vali Loss: 0.6503717 Test Loss: 0.3790117
Validation loss decreased (0.651825 --> 0.650372).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.447073221206665
Epoch: 27, Steps: 56 | Train Loss: 0.8034106 Vali Loss: 0.6499550 Test Loss: 0.3789561
Validation loss decreased (0.650372 --> 0.649955).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.3132152557373047
Epoch: 28, Steps: 56 | Train Loss: 0.8019224 Vali Loss: 0.6529448 Test Loss: 0.3789587
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.3435826301574707
Epoch: 29, Steps: 56 | Train Loss: 0.8030689 Vali Loss: 0.6503915 Test Loss: 0.3789275
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 0.9843082427978516
Epoch: 30, Steps: 56 | Train Loss: 0.8030321 Vali Loss: 0.6550598 Test Loss: 0.3789017
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.1277375221252441
Epoch: 31, Steps: 56 | Train Loss: 0.8030465 Vali Loss: 0.6503919 Test Loss: 0.3789032
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 0.9905610084533691
Epoch: 32, Steps: 56 | Train Loss: 0.8024937 Vali Loss: 0.6498709 Test Loss: 0.3788801
Validation loss decreased (0.649955 --> 0.649871).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 0.8595192432403564
Epoch: 33, Steps: 56 | Train Loss: 0.8006397 Vali Loss: 0.6521025 Test Loss: 0.3789068
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 0.9125711917877197
Epoch: 34, Steps: 56 | Train Loss: 0.8042260 Vali Loss: 0.6551771 Test Loss: 0.3788461
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.0987448692321777
Epoch: 35, Steps: 56 | Train Loss: 0.8021568 Vali Loss: 0.6502277 Test Loss: 0.3788581
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.0545411109924316
Epoch: 36, Steps: 56 | Train Loss: 0.8018966 Vali Loss: 0.6517080 Test Loss: 0.3788511
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.3165223598480225
Epoch: 37, Steps: 56 | Train Loss: 0.8001616 Vali Loss: 0.6499991 Test Loss: 0.3788591
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.0520164966583252
Epoch: 38, Steps: 56 | Train Loss: 0.8003597 Vali Loss: 0.6513860 Test Loss: 0.3788554
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3454039096832275
Epoch: 39, Steps: 56 | Train Loss: 0.8021521 Vali Loss: 0.6508169 Test Loss: 0.3788476
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.1995110511779785
Epoch: 40, Steps: 56 | Train Loss: 0.8015907 Vali Loss: 0.6513077 Test Loss: 0.3788564
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.011284351348877
Epoch: 41, Steps: 56 | Train Loss: 0.8013046 Vali Loss: 0.6482984 Test Loss: 0.3788516
Validation loss decreased (0.649871 --> 0.648298).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.2420439720153809
Epoch: 42, Steps: 56 | Train Loss: 0.8012291 Vali Loss: 0.6504409 Test Loss: 0.3788399
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.1642372608184814
Epoch: 43, Steps: 56 | Train Loss: 0.8021716 Vali Loss: 0.6511836 Test Loss: 0.3788596
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.0556347370147705
Epoch: 44, Steps: 56 | Train Loss: 0.8000871 Vali Loss: 0.6495898 Test Loss: 0.3788396
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.1066336631774902
Epoch: 45, Steps: 56 | Train Loss: 0.8008738 Vali Loss: 0.6509891 Test Loss: 0.3788234
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.2143642902374268
Epoch: 46, Steps: 56 | Train Loss: 0.8003804 Vali Loss: 0.6495994 Test Loss: 0.3788369
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.2430739402770996
Epoch: 47, Steps: 56 | Train Loss: 0.8013809 Vali Loss: 0.6507664 Test Loss: 0.3788454
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.2541124820709229
Epoch: 48, Steps: 56 | Train Loss: 0.7991946 Vali Loss: 0.6508453 Test Loss: 0.3788434
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.1805338859558105
Epoch: 49, Steps: 56 | Train Loss: 0.8014902 Vali Loss: 0.6512642 Test Loss: 0.3788391
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.147855520248413
Epoch: 50, Steps: 56 | Train Loss: 0.8008897 Vali Loss: 0.6497530 Test Loss: 0.3788477
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.0521268844604492
Epoch: 51, Steps: 56 | Train Loss: 0.7999740 Vali Loss: 0.6493080 Test Loss: 0.3788431
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.019052505493164
Epoch: 52, Steps: 56 | Train Loss: 0.8006768 Vali Loss: 0.6488643 Test Loss: 0.3788445
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.0891540050506592
Epoch: 53, Steps: 56 | Train Loss: 0.7998791 Vali Loss: 0.6492013 Test Loss: 0.3788505
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.322627067565918
Epoch: 54, Steps: 56 | Train Loss: 0.8011699 Vali Loss: 0.6482239 Test Loss: 0.3788575
Validation loss decreased (0.648298 --> 0.648224).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.1645634174346924
Epoch: 55, Steps: 56 | Train Loss: 0.7999982 Vali Loss: 0.6503306 Test Loss: 0.3788624
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.0674686431884766
Epoch: 56, Steps: 56 | Train Loss: 0.7996556 Vali Loss: 0.6466702 Test Loss: 0.3788536
Validation loss decreased (0.648224 --> 0.646670).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.2727911472320557
Epoch: 57, Steps: 56 | Train Loss: 0.8004037 Vali Loss: 0.6516294 Test Loss: 0.3788561
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.0225255489349365
Epoch: 58, Steps: 56 | Train Loss: 0.8013327 Vali Loss: 0.6506001 Test Loss: 0.3788536
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.0939602851867676
Epoch: 59, Steps: 56 | Train Loss: 0.8013947 Vali Loss: 0.6495337 Test Loss: 0.3788543
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.2372636795043945
Epoch: 60, Steps: 56 | Train Loss: 0.7997400 Vali Loss: 0.6486156 Test Loss: 0.3788536
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.1967971324920654
Epoch: 61, Steps: 56 | Train Loss: 0.8004474 Vali Loss: 0.6494380 Test Loss: 0.3788641
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.3702986240386963
Epoch: 62, Steps: 56 | Train Loss: 0.7999792 Vali Loss: 0.6514500 Test Loss: 0.3788574
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 0.9908528327941895
Epoch: 63, Steps: 56 | Train Loss: 0.8009763 Vali Loss: 0.6484919 Test Loss: 0.3788537
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.0449938774108887
Epoch: 64, Steps: 56 | Train Loss: 0.8007388 Vali Loss: 0.6477776 Test Loss: 0.3788632
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 0.777092695236206
Epoch: 65, Steps: 56 | Train Loss: 0.8008147 Vali Loss: 0.6485993 Test Loss: 0.3788536
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 0.9129848480224609
Epoch: 66, Steps: 56 | Train Loss: 0.7987435 Vali Loss: 0.6485630 Test Loss: 0.3788599
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.0917091369628906
Epoch: 67, Steps: 56 | Train Loss: 0.8016100 Vali Loss: 0.6469772 Test Loss: 0.3788638
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.0222868919372559
Epoch: 68, Steps: 56 | Train Loss: 0.8012379 Vali Loss: 0.6430742 Test Loss: 0.3788593
Validation loss decreased (0.646670 --> 0.643074).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 0.9879043102264404
Epoch: 69, Steps: 56 | Train Loss: 0.8003322 Vali Loss: 0.6473809 Test Loss: 0.3788629
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 0.9999330043792725
Epoch: 70, Steps: 56 | Train Loss: 0.8007116 Vali Loss: 0.6492413 Test Loss: 0.3788656
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.0914721488952637
Epoch: 71, Steps: 56 | Train Loss: 0.8003988 Vali Loss: 0.6486114 Test Loss: 0.3788693
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 0.9502439498901367
Epoch: 72, Steps: 56 | Train Loss: 0.8005617 Vali Loss: 0.6492308 Test Loss: 0.3788659
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.1063528060913086
Epoch: 73, Steps: 56 | Train Loss: 0.7992263 Vali Loss: 0.6494164 Test Loss: 0.3788665
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 0.9907932281494141
Epoch: 74, Steps: 56 | Train Loss: 0.7997541 Vali Loss: 0.6450957 Test Loss: 0.3788660
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.1028883457183838
Epoch: 75, Steps: 56 | Train Loss: 0.8017019 Vali Loss: 0.6486480 Test Loss: 0.3788664
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.0453674793243408
Epoch: 76, Steps: 56 | Train Loss: 0.8009149 Vali Loss: 0.6470554 Test Loss: 0.3788668
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.0225450992584229
Epoch: 77, Steps: 56 | Train Loss: 0.8006192 Vali Loss: 0.6434807 Test Loss: 0.3788693
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.0890963077545166
Epoch: 78, Steps: 56 | Train Loss: 0.8001279 Vali Loss: 0.6477252 Test Loss: 0.3788661
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.0925843715667725
Epoch: 79, Steps: 56 | Train Loss: 0.7990604 Vali Loss: 0.6485555 Test Loss: 0.3788690
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.3011116981506348
Epoch: 80, Steps: 56 | Train Loss: 0.7998757 Vali Loss: 0.6478970 Test Loss: 0.3788675
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.248056173324585
Epoch: 81, Steps: 56 | Train Loss: 0.7996084 Vali Loss: 0.6498471 Test Loss: 0.3788697
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.1923456192016602
Epoch: 82, Steps: 56 | Train Loss: 0.8001367 Vali Loss: 0.6464943 Test Loss: 0.3788717
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.004157304763794
Epoch: 83, Steps: 56 | Train Loss: 0.8002715 Vali Loss: 0.6437916 Test Loss: 0.3788678
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.2445805072784424
Epoch: 84, Steps: 56 | Train Loss: 0.7998017 Vali Loss: 0.6467977 Test Loss: 0.3788711
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.3073718547821045
Epoch: 85, Steps: 56 | Train Loss: 0.8012714 Vali Loss: 0.6502637 Test Loss: 0.3788726
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.417189121246338
Epoch: 86, Steps: 56 | Train Loss: 0.8012102 Vali Loss: 0.6501604 Test Loss: 0.3788711
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.3319313526153564
Epoch: 87, Steps: 56 | Train Loss: 0.8001664 Vali Loss: 0.6491371 Test Loss: 0.3788709
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 0.9556782245635986
Epoch: 88, Steps: 56 | Train Loss: 0.8008817 Vali Loss: 0.6453513 Test Loss: 0.3788735
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37746289372444153, mae:0.42276471853256226, rse:0.49106940627098083, corr:[ 0.21560162  0.22025122  0.21789905  0.21914169  0.21894166  0.21701932
  0.21699303  0.21709976  0.21530782  0.21377023  0.21323214  0.21197784
  0.21026532  0.20928968  0.20866846  0.20771149  0.20703556  0.20661028
  0.20571698  0.2045033   0.20375259  0.20324127  0.20201237  0.20061994
  0.19938512  0.19831832  0.19711864  0.19617009  0.19559953  0.19495437
  0.1941811   0.19334391  0.1926363   0.19173263  0.1907434   0.1897647
  0.18897264  0.18845712  0.1877657   0.18684994  0.18607423  0.1857131
  0.18541987  0.18471704  0.18374358  0.18320222  0.18263316  0.18120761
  0.17950946  0.17827395  0.17743039  0.17665298  0.17589374  0.17524122
  0.17484583  0.17444333  0.1735729   0.17265165  0.17226778  0.17208573
  0.17161335  0.17136225  0.17180906  0.17219533  0.17203195  0.17211017
  0.17236832  0.1722978   0.1720378   0.17200442  0.17190453  0.17145462
  0.17111495  0.17089628  0.17036943  0.16964893  0.16960046  0.16986935
  0.16957048  0.16913755  0.16917494  0.16923687  0.16888638  0.16873641
  0.16910432  0.16937123  0.16922146  0.16920125  0.16954489  0.16965447
  0.16929913  0.16920117  0.1695688   0.16967626  0.16957657  0.1696524
  0.16981904  0.16965038  0.16932052  0.16917932  0.16899732  0.16842408
  0.1681696   0.168317    0.16825512  0.16776027  0.16776371  0.16818652
  0.16806822  0.16750666  0.16744228  0.16764057  0.1673152   0.16692728
  0.1670717   0.16720985  0.16666324  0.16600588  0.16595782  0.16566677
  0.16457868  0.16362701  0.1632457   0.16268536  0.16179746  0.1613743
  0.16129903  0.16085778  0.1601758   0.15961362  0.15920433  0.15862617
  0.15813352  0.15764958  0.15701075  0.15646833  0.15626097  0.15618207
  0.1555579   0.15496972  0.15507433  0.15520501  0.15462637  0.15352811
  0.15240757  0.151502    0.15055764  0.14993906  0.14984374  0.14975587
  0.14925206  0.1486583   0.1483756   0.14803544  0.14748992  0.14681429
  0.1462494   0.14597854  0.14585929  0.14546461  0.14482602  0.14486292
  0.14543232  0.14564283  0.14553751  0.14575286  0.14616539  0.14569388
  0.1445994   0.14393497  0.14399122  0.1438736   0.14338368  0.14301154
  0.14271088  0.14193495  0.1409572   0.1404126   0.14005366  0.13966505
  0.13929667  0.13908488  0.13894738  0.1388235   0.13897525  0.13933037
  0.1394101   0.13957734  0.1400196   0.14042516  0.1407287   0.14099675
  0.14123362  0.1412178   0.1410855   0.14140068  0.14186403  0.14179151
  0.14162901  0.14161931  0.14157347  0.1412485   0.14116293  0.14119361
  0.14100721  0.1407877   0.14088944  0.14126626  0.14150599  0.14187013
  0.14244777  0.14278506  0.14268905  0.14276747  0.1431817   0.1431588
  0.14254043  0.14199968  0.14182274  0.1415828   0.14118563  0.14125709
  0.14136977  0.14127173  0.1411474   0.14139785  0.14167145  0.14161634
  0.14170538  0.14193147  0.14206502  0.14239539  0.14319238  0.14395979
  0.14426097  0.14463869  0.14543845  0.1459545   0.14598235  0.14632231
  0.14722906  0.14767686  0.1475422   0.14770886  0.14821373  0.14835024
  0.14810464  0.14816345  0.14882132  0.14936316  0.14964604  0.15014851
  0.15088938  0.15160877  0.15222794  0.15299998  0.15374431  0.15446177
  0.15511522  0.15561381  0.15591651  0.15647803  0.15737103  0.15802988
  0.15841027  0.15875992  0.15924016  0.15944105  0.15933901  0.15946303
  0.15984115  0.16015504  0.16037178  0.16073948  0.1613528   0.16183877
  0.16202772  0.16245109  0.16319998  0.16396594  0.1645064   0.16493374
  0.16530025  0.16549467  0.1655087   0.16578087  0.16639175  0.16685888
  0.16703838  0.16698226  0.16680723  0.16669561  0.16669507  0.16678238
  0.16671337  0.16647817  0.16654119  0.16712627  0.16760601  0.16752551
  0.16740684  0.16788942  0.16850309  0.16875975  0.1690526   0.16972293
  0.1700452   0.17000745  0.17006788  0.17051573  0.17091441  0.17090295
  0.17078575  0.17065303  0.17046796  0.17012715  0.16988124  0.16970792
  0.16951013  0.16945325  0.1694861   0.16949102  0.16928218  0.16926104
  0.16938028  0.16942272  0.16971451  0.17024888  0.17085505  0.17110212
  0.17109035  0.17143914  0.17193803  0.17233719  0.17275366  0.17307708
  0.17312837  0.17302991  0.17301267  0.17313007  0.17347234  0.17364545
  0.17361532  0.17349544  0.17354135  0.1737051   0.17366055  0.17375505
  0.17399065  0.17446832  0.17495707  0.17536628  0.1759318   0.17668687
  0.17724827  0.17763357  0.17796299  0.17839597  0.1786914   0.17892106
  0.17922993  0.17944254  0.17936143  0.1792283   0.17938611  0.17973888
  0.18023054  0.18079555  0.18104486  0.18103622  0.18083099  0.18078588
  0.18075086  0.18066369  0.1807353   0.18105929  0.18124968  0.1809811
  0.18071787  0.18076774  0.18085401  0.1806646   0.18058896  0.18090875
  0.18116333  0.18100813  0.18075222  0.18080385  0.18117853  0.18155842
  0.18183523  0.1821501   0.18251088  0.18277438  0.18298306  0.18317427
  0.18331937  0.18294607  0.18262726  0.1826098   0.18240584  0.18191822
  0.18162675  0.18171783  0.18169639  0.18149897  0.18133122  0.1812594
  0.18094072  0.18062048  0.18046649  0.18033132  0.18014093  0.17997462
  0.17997882  0.18002115  0.17991449  0.17970838  0.17956468  0.17947
  0.17909943  0.1786513   0.1780494   0.17717895  0.1762006   0.17526662
  0.17429066  0.17315412  0.17248449  0.17212072  0.17131485  0.17043045
  0.17003521  0.16991773  0.1694914   0.16877422  0.16848636  0.16825546
  0.16756448  0.16707164  0.1668739   0.16652617  0.16563322  0.16511256
  0.16488662  0.16446428  0.16387983  0.16389059  0.16395397  0.16334097
  0.1626614   0.1626303   0.16253044  0.1621497   0.1620508   0.16223897
  0.16211274  0.16158366  0.16134344  0.16139926  0.16106477  0.1605573
  0.16046347  0.16039509  0.15980378  0.15951991  0.15949103  0.15890951
  0.15818048  0.15781379  0.15777215  0.1573951   0.15699384  0.15688568
  0.15670125  0.15612732  0.1557857   0.1560818   0.15607513  0.15538475
  0.15464486  0.15418005  0.1535664   0.15272215  0.15222757  0.15174434
  0.15098624  0.15023768  0.14969258  0.14887139  0.1480992   0.14772026
  0.14727816  0.14634658  0.14552732  0.1453142   0.14493866  0.14423098
  0.14373218  0.14351751  0.14310056  0.14244698  0.14206293  0.14155383
  0.14040616  0.1392159   0.13853444  0.1379649   0.13719559  0.1366637
  0.13627487  0.13568349  0.1348648   0.1343671   0.1340647   0.13311182
  0.13193405  0.13139568  0.1312304   0.13055182  0.12989672  0.1298834
  0.1295214   0.12846932  0.12761864  0.12739848  0.12678324  0.12547138
  0.12417964  0.12338981  0.12257812  0.12128115  0.12012805  0.11903825
  0.11791056  0.11694153  0.11614364  0.11510207  0.11392567  0.11313078
  0.11242235  0.11131536  0.11034033  0.10973527  0.1089597   0.10795118
  0.10695282  0.1064333   0.105915    0.10497794  0.10423736  0.10339913
  0.10216837  0.10067733  0.09950569  0.09861175  0.09782366  0.09710722
  0.09625231  0.09503119  0.09425354  0.09391514  0.09366041  0.0930282
  0.09257147  0.09254163  0.09221558  0.09113905  0.09045278  0.09030932
  0.08981279  0.08881327  0.08797738  0.08741865  0.08638409  0.08487301
  0.08341201  0.08236171  0.08128604  0.08025217  0.07940557  0.0784562
  0.07745229  0.07651453  0.0754796   0.07445318  0.07381456  0.07358601
  0.07280915  0.0716432   0.07101633  0.07084625  0.07050277  0.06992561
  0.06966498  0.06966443  0.06901971  0.06806645  0.06740023  0.06675304
  0.06543289  0.06392837  0.06264468  0.06168259  0.06046004  0.05940802
  0.05824428  0.05669013  0.05519376  0.05449483  0.05408833  0.0530656
  0.05223934  0.05207841  0.05199413  0.0516092   0.05152907  0.0520665
  0.05201823  0.05157702  0.051605    0.0520907   0.05167595  0.05069147
  0.04979591  0.04909416  0.04834415  0.04746925  0.04660058  0.04602626
  0.04561328  0.04514986  0.04443499  0.04383186  0.04366248  0.04382899
  0.04325347  0.0421279   0.04195649  0.04231977  0.0416595   0.0408617
  0.04064991  0.04014692  0.03925367  0.03911632  0.03934174  0.03886935
  0.03743451  0.03632595  0.03588983  0.03511424  0.03431835  0.03361828
  0.03276231  0.03179893  0.03141158  0.03159539  0.03137333  0.03080842
  0.03064267  0.03081835  0.03068052  0.03053387  0.03135421  0.03188871
  0.03117383  0.03071083  0.0313151   0.03191543  0.03127434  0.03050033
  0.03031995  0.03003676  0.02925147  0.02824089  0.02768751  0.02676315
  0.02618603  0.02543892  0.02493754  0.02482719  0.02516668  0.02506881
  0.02404491  0.02306077  0.02378275  0.02425679  0.02330672  0.02323509
  0.02461511  0.02402386  0.02189592  0.02126592  0.02216541  0.0209337
  0.01771638  0.01552792  0.01496715  0.01353259  0.01173764  0.01050365
  0.00976458  0.00859523  0.00788951  0.00735585  0.00612712  0.00607214
  0.0067452   0.00497516  0.00321129  0.00458277  0.00609013  0.00311834
  0.00061822  0.00340233  0.00259395 -0.00459431 -0.00253326  0.01113376]
