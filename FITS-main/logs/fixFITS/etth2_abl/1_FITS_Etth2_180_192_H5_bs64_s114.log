Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_192_FITS_ETTh2_ftM_sl180_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8269
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=50, out_features=103, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4614400.0
params:  5253.0
Trainable parameters:  5253
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0674984455108643
Epoch: 1, Steps: 64 | Train Loss: 0.7055051 Vali Loss: 0.3455602 Test Loss: 0.4716496
Validation loss decreased (inf --> 0.345560).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.7186436653137207
Epoch: 2, Steps: 64 | Train Loss: 0.6016359 Vali Loss: 0.3121789 Test Loss: 0.4287204
Validation loss decreased (0.345560 --> 0.312179).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.377930164337158
Epoch: 3, Steps: 64 | Train Loss: 0.5632618 Vali Loss: 0.2980037 Test Loss: 0.4120520
Validation loss decreased (0.312179 --> 0.298004).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.834657907485962
Epoch: 4, Steps: 64 | Train Loss: 0.5498762 Vali Loss: 0.2909580 Test Loss: 0.4041617
Validation loss decreased (0.298004 --> 0.290958).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.0047829151153564
Epoch: 5, Steps: 64 | Train Loss: 0.5418185 Vali Loss: 0.2865580 Test Loss: 0.3997813
Validation loss decreased (0.290958 --> 0.286558).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.127922058105469
Epoch: 6, Steps: 64 | Train Loss: 0.5366709 Vali Loss: 0.2839153 Test Loss: 0.3967646
Validation loss decreased (0.286558 --> 0.283915).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.715700626373291
Epoch: 7, Steps: 64 | Train Loss: 0.5343194 Vali Loss: 0.2820533 Test Loss: 0.3947497
Validation loss decreased (0.283915 --> 0.282053).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.397092819213867
Epoch: 8, Steps: 64 | Train Loss: 0.5307980 Vali Loss: 0.2806329 Test Loss: 0.3929706
Validation loss decreased (0.282053 --> 0.280633).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9969184398651123
Epoch: 9, Steps: 64 | Train Loss: 0.5300605 Vali Loss: 0.2795754 Test Loss: 0.3918025
Validation loss decreased (0.280633 --> 0.279575).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.762151002883911
Epoch: 10, Steps: 64 | Train Loss: 0.5276171 Vali Loss: 0.2787017 Test Loss: 0.3905856
Validation loss decreased (0.279575 --> 0.278702).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.340445041656494
Epoch: 11, Steps: 64 | Train Loss: 0.5256478 Vali Loss: 0.2780091 Test Loss: 0.3897384
Validation loss decreased (0.278702 --> 0.278009).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.2507266998291016
Epoch: 12, Steps: 64 | Train Loss: 0.5239984 Vali Loss: 0.2773038 Test Loss: 0.3889933
Validation loss decreased (0.278009 --> 0.277304).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.7647581100463867
Epoch: 13, Steps: 64 | Train Loss: 0.5224874 Vali Loss: 0.2768260 Test Loss: 0.3883085
Validation loss decreased (0.277304 --> 0.276826).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.9864392280578613
Epoch: 14, Steps: 64 | Train Loss: 0.5221646 Vali Loss: 0.2764871 Test Loss: 0.3876690
Validation loss decreased (0.276826 --> 0.276487).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.264296054840088
Epoch: 15, Steps: 64 | Train Loss: 0.5238566 Vali Loss: 0.2761081 Test Loss: 0.3871640
Validation loss decreased (0.276487 --> 0.276108).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.793362617492676
Epoch: 16, Steps: 64 | Train Loss: 0.5216459 Vali Loss: 0.2755031 Test Loss: 0.3866237
Validation loss decreased (0.276108 --> 0.275503).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.5573177337646484
Epoch: 17, Steps: 64 | Train Loss: 0.5207452 Vali Loss: 0.2755755 Test Loss: 0.3862034
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.8205971717834473
Epoch: 18, Steps: 64 | Train Loss: 0.5206477 Vali Loss: 0.2752551 Test Loss: 0.3858872
Validation loss decreased (0.275503 --> 0.275255).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.3701140880584717
Epoch: 19, Steps: 64 | Train Loss: 0.5191746 Vali Loss: 0.2750014 Test Loss: 0.3855853
Validation loss decreased (0.275255 --> 0.275001).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.059377908706665
Epoch: 20, Steps: 64 | Train Loss: 0.5182898 Vali Loss: 0.2747091 Test Loss: 0.3852843
Validation loss decreased (0.275001 --> 0.274709).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.4400477409362793
Epoch: 21, Steps: 64 | Train Loss: 0.5210470 Vali Loss: 0.2746159 Test Loss: 0.3849951
Validation loss decreased (0.274709 --> 0.274616).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.6580302715301514
Epoch: 22, Steps: 64 | Train Loss: 0.5192550 Vali Loss: 0.2745230 Test Loss: 0.3846939
Validation loss decreased (0.274616 --> 0.274523).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.372504949569702
Epoch: 23, Steps: 64 | Train Loss: 0.5190079 Vali Loss: 0.2743889 Test Loss: 0.3844465
Validation loss decreased (0.274523 --> 0.274389).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.0221824645996094
Epoch: 24, Steps: 64 | Train Loss: 0.5179709 Vali Loss: 0.2741938 Test Loss: 0.3842137
Validation loss decreased (0.274389 --> 0.274194).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.4178178310394287
Epoch: 25, Steps: 64 | Train Loss: 0.5187554 Vali Loss: 0.2740957 Test Loss: 0.3839971
Validation loss decreased (0.274194 --> 0.274096).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.7023050785064697
Epoch: 26, Steps: 64 | Train Loss: 0.5160126 Vali Loss: 0.2740041 Test Loss: 0.3837890
Validation loss decreased (0.274096 --> 0.274004).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.916330099105835
Epoch: 27, Steps: 64 | Train Loss: 0.5180410 Vali Loss: 0.2738645 Test Loss: 0.3836258
Validation loss decreased (0.274004 --> 0.273865).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.068197011947632
Epoch: 28, Steps: 64 | Train Loss: 0.5185837 Vali Loss: 0.2737764 Test Loss: 0.3835133
Validation loss decreased (0.273865 --> 0.273776).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.601259469985962
Epoch: 29, Steps: 64 | Train Loss: 0.5177803 Vali Loss: 0.2736869 Test Loss: 0.3833756
Validation loss decreased (0.273776 --> 0.273687).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.3000810146331787
Epoch: 30, Steps: 64 | Train Loss: 0.5176583 Vali Loss: 0.2735715 Test Loss: 0.3831933
Validation loss decreased (0.273687 --> 0.273571).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.484269618988037
Epoch: 31, Steps: 64 | Train Loss: 0.5167893 Vali Loss: 0.2736160 Test Loss: 0.3830265
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.4072582721710205
Epoch: 32, Steps: 64 | Train Loss: 0.5163531 Vali Loss: 0.2735242 Test Loss: 0.3829331
Validation loss decreased (0.273571 --> 0.273524).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.392637252807617
Epoch: 33, Steps: 64 | Train Loss: 0.5183191 Vali Loss: 0.2732399 Test Loss: 0.3828129
Validation loss decreased (0.273524 --> 0.273240).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.6706583499908447
Epoch: 34, Steps: 64 | Train Loss: 0.5173634 Vali Loss: 0.2732059 Test Loss: 0.3827297
Validation loss decreased (0.273240 --> 0.273206).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.690507650375366
Epoch: 35, Steps: 64 | Train Loss: 0.5169270 Vali Loss: 0.2732912 Test Loss: 0.3826507
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.661574125289917
Epoch: 36, Steps: 64 | Train Loss: 0.5176689 Vali Loss: 0.2731782 Test Loss: 0.3825473
Validation loss decreased (0.273206 --> 0.273178).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.7199456691741943
Epoch: 37, Steps: 64 | Train Loss: 0.5167462 Vali Loss: 0.2731763 Test Loss: 0.3824646
Validation loss decreased (0.273178 --> 0.273176).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.149949073791504
Epoch: 38, Steps: 64 | Train Loss: 0.5156724 Vali Loss: 0.2730626 Test Loss: 0.3824084
Validation loss decreased (0.273176 --> 0.273063).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.885582447052002
Epoch: 39, Steps: 64 | Train Loss: 0.5185153 Vali Loss: 0.2731028 Test Loss: 0.3823171
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.3669021129608154
Epoch: 40, Steps: 64 | Train Loss: 0.5163347 Vali Loss: 0.2730532 Test Loss: 0.3822362
Validation loss decreased (0.273063 --> 0.273053).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.3142387866973877
Epoch: 41, Steps: 64 | Train Loss: 0.5155702 Vali Loss: 0.2730804 Test Loss: 0.3821693
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.9958090782165527
Epoch: 42, Steps: 64 | Train Loss: 0.5152879 Vali Loss: 0.2729160 Test Loss: 0.3821073
Validation loss decreased (0.273053 --> 0.272916).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.902952194213867
Epoch: 43, Steps: 64 | Train Loss: 0.5153391 Vali Loss: 0.2729637 Test Loss: 0.3820560
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.39919376373291
Epoch: 44, Steps: 64 | Train Loss: 0.5147170 Vali Loss: 0.2728458 Test Loss: 0.3819883
Validation loss decreased (0.272916 --> 0.272846).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.156860828399658
Epoch: 45, Steps: 64 | Train Loss: 0.5166308 Vali Loss: 0.2728933 Test Loss: 0.3819483
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.523599147796631
Epoch: 46, Steps: 64 | Train Loss: 0.5172311 Vali Loss: 0.2728835 Test Loss: 0.3819030
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 6.762306451797485
Epoch: 47, Steps: 64 | Train Loss: 0.5148445 Vali Loss: 0.2728509 Test Loss: 0.3818604
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.0587074756622314
Epoch: 48, Steps: 64 | Train Loss: 0.5168293 Vali Loss: 0.2727972 Test Loss: 0.3818178
Validation loss decreased (0.272846 --> 0.272797).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.7965645790100098
Epoch: 49, Steps: 64 | Train Loss: 0.5168037 Vali Loss: 0.2728169 Test Loss: 0.3817582
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.209893226623535
Epoch: 50, Steps: 64 | Train Loss: 0.5146613 Vali Loss: 0.2727981 Test Loss: 0.3817296
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.1440744400024414
Epoch: 51, Steps: 64 | Train Loss: 0.5162830 Vali Loss: 0.2726764 Test Loss: 0.3816909
Validation loss decreased (0.272797 --> 0.272676).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.2696237564086914
Epoch: 52, Steps: 64 | Train Loss: 0.5160380 Vali Loss: 0.2727274 Test Loss: 0.3816529
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.9571235179901123
Epoch: 53, Steps: 64 | Train Loss: 0.5172050 Vali Loss: 0.2727329 Test Loss: 0.3816278
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.1980812549591064
Epoch: 54, Steps: 64 | Train Loss: 0.5133407 Vali Loss: 0.2727258 Test Loss: 0.3816009
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.5854127407073975
Epoch: 55, Steps: 64 | Train Loss: 0.5152811 Vali Loss: 0.2726716 Test Loss: 0.3815666
Validation loss decreased (0.272676 --> 0.272672).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.1592743396759033
Epoch: 56, Steps: 64 | Train Loss: 0.5148224 Vali Loss: 0.2726652 Test Loss: 0.3815374
Validation loss decreased (0.272672 --> 0.272665).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.89508318901062
Epoch: 57, Steps: 64 | Train Loss: 0.5146404 Vali Loss: 0.2725779 Test Loss: 0.3815041
Validation loss decreased (0.272665 --> 0.272578).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.983793258666992
Epoch: 58, Steps: 64 | Train Loss: 0.5137043 Vali Loss: 0.2725092 Test Loss: 0.3814871
Validation loss decreased (0.272578 --> 0.272509).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.6656107902526855
Epoch: 59, Steps: 64 | Train Loss: 0.5155046 Vali Loss: 0.2726471 Test Loss: 0.3814664
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 4.282243728637695
Epoch: 60, Steps: 64 | Train Loss: 0.5165036 Vali Loss: 0.2726169 Test Loss: 0.3814478
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.685732126235962
Epoch: 61, Steps: 64 | Train Loss: 0.5157812 Vali Loss: 0.2725866 Test Loss: 0.3814263
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.6472411155700684
Epoch: 62, Steps: 64 | Train Loss: 0.5135159 Vali Loss: 0.2725754 Test Loss: 0.3813972
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.157923698425293
Epoch: 63, Steps: 64 | Train Loss: 0.5161221 Vali Loss: 0.2721798 Test Loss: 0.3813944
Validation loss decreased (0.272509 --> 0.272180).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.8098866939544678
Epoch: 64, Steps: 64 | Train Loss: 0.5135270 Vali Loss: 0.2725363 Test Loss: 0.3813739
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.5837275981903076
Epoch: 65, Steps: 64 | Train Loss: 0.5165048 Vali Loss: 0.2725680 Test Loss: 0.3813572
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.584657669067383
Epoch: 66, Steps: 64 | Train Loss: 0.5159091 Vali Loss: 0.2725520 Test Loss: 0.3813451
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.2004199028015137
Epoch: 67, Steps: 64 | Train Loss: 0.5150760 Vali Loss: 0.2725511 Test Loss: 0.3813308
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.100513458251953
Epoch: 68, Steps: 64 | Train Loss: 0.5146160 Vali Loss: 0.2725693 Test Loss: 0.3813145
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.195918560028076
Epoch: 69, Steps: 64 | Train Loss: 0.5138594 Vali Loss: 0.2725278 Test Loss: 0.3812955
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.455975770950317
Epoch: 70, Steps: 64 | Train Loss: 0.5146906 Vali Loss: 0.2725526 Test Loss: 0.3812928
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.6161131858825684
Epoch: 71, Steps: 64 | Train Loss: 0.5146821 Vali Loss: 0.2723699 Test Loss: 0.3812773
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.9986002445220947
Epoch: 72, Steps: 64 | Train Loss: 0.5153577 Vali Loss: 0.2724410 Test Loss: 0.3812657
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.004305362701416
Epoch: 73, Steps: 64 | Train Loss: 0.5159942 Vali Loss: 0.2724929 Test Loss: 0.3812581
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.100391387939453
Epoch: 74, Steps: 64 | Train Loss: 0.5155836 Vali Loss: 0.2724750 Test Loss: 0.3812455
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.496004343032837
Epoch: 75, Steps: 64 | Train Loss: 0.5138923 Vali Loss: 0.2724696 Test Loss: 0.3812368
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 4.203720569610596
Epoch: 76, Steps: 64 | Train Loss: 0.5144095 Vali Loss: 0.2725199 Test Loss: 0.3812260
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.9633891582489014
Epoch: 77, Steps: 64 | Train Loss: 0.5151515 Vali Loss: 0.2725070 Test Loss: 0.3812178
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.6806766986846924
Epoch: 78, Steps: 64 | Train Loss: 0.5117656 Vali Loss: 0.2725043 Test Loss: 0.3812097
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 3.344975709915161
Epoch: 79, Steps: 64 | Train Loss: 0.5150436 Vali Loss: 0.2724578 Test Loss: 0.3812014
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.0437004566192627
Epoch: 80, Steps: 64 | Train Loss: 0.5153040 Vali Loss: 0.2724273 Test Loss: 0.3811911
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.619664430618286
Epoch: 81, Steps: 64 | Train Loss: 0.5128475 Vali Loss: 0.2724262 Test Loss: 0.3811876
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.274228096008301
Epoch: 82, Steps: 64 | Train Loss: 0.5144739 Vali Loss: 0.2724603 Test Loss: 0.3811807
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.519808530807495
Epoch: 83, Steps: 64 | Train Loss: 0.5149657 Vali Loss: 0.2724659 Test Loss: 0.3811747
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_192_FITS_ETTh2_ftM_sl180_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3602341413497925, mae:0.38736841082572937, rse:0.4813193082809448, corr:[0.26504055 0.27040192 0.26744166 0.26799175 0.26803097 0.26604754
 0.26517004 0.26478457 0.26349548 0.26209298 0.26110783 0.2597577
 0.25820997 0.25687337 0.25598973 0.2555912  0.25507733 0.25413808
 0.25309956 0.25203875 0.25066027 0.24938074 0.24829456 0.24618852
 0.24302612 0.24068975 0.2393514  0.23745333 0.23501493 0.23358808
 0.23226227 0.22968912 0.22746612 0.2265831  0.22536275 0.22309256
 0.22155933 0.22083358 0.219556   0.21805654 0.2177116  0.21728018
 0.21576309 0.21436189 0.21382402 0.21271877 0.21067029 0.20854814
 0.20612478 0.20342451 0.20107469 0.1996225  0.1977472  0.1953116
 0.19319645 0.19151565 0.18978345 0.1881543  0.18747865 0.18651018
 0.18531229 0.18456456 0.18484811 0.18434235 0.1834025  0.18314016
 0.18277113 0.18141381 0.18058614 0.18075435 0.18002743 0.17809373
 0.17608288 0.17506832 0.1736693  0.17191957 0.17085484 0.170374
 0.1693163  0.16828625 0.16827081 0.16804896 0.16754636 0.16733898
 0.16739479 0.1669089  0.16671629 0.16687994 0.16644809 0.16541706
 0.16518565 0.16529724 0.1648431  0.16421682 0.16424523 0.16384248
 0.16180015 0.16014133 0.15940343 0.15855    0.15703648 0.15614823
 0.15593322 0.15526718 0.15473834 0.15480688 0.1551244  0.1547745
 0.15454224 0.15441565 0.15406275 0.15333143 0.15303148 0.15289491
 0.15234852 0.1515919  0.15137625 0.15115231 0.15006125 0.14843856
 0.14662714 0.14484929 0.1432474  0.14220694 0.14105634 0.13961291
 0.13843264 0.13759033 0.13683744 0.13609733 0.13573791 0.13535033
 0.13492818 0.13434702 0.13439164 0.13440782 0.13390757 0.13327193
 0.13304673 0.13257787 0.13195956 0.13202412 0.13185048 0.13021514
 0.12719849 0.12554885 0.12422743 0.12254577 0.1210423  0.12055716
 0.12038001 0.11947922 0.11896759 0.11926243 0.11992888 0.11998594
 0.12049203 0.12070654 0.12088496 0.1211023  0.12128428 0.12099951
 0.12073781 0.12078611 0.12059613 0.12044645 0.12058014 0.11997104
 0.11742619 0.11555421 0.11465254 0.11412523 0.11272189 0.11108095
 0.11046766 0.10993896 0.10923945 0.10862409 0.1093459  0.1098558
 0.10970877 0.10909261 0.10959295 0.11012098 0.10944623 0.10810781
 0.10787284 0.10740766 0.10664025 0.10643159 0.10707341 0.10634901]
