Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_96_FITS_ETTh2_ftM_sl360_ll48_pl96_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8185
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=42, out_features=53, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1994496.0
params:  2279.0
Trainable parameters:  2279
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.646984577178955
Epoch: 1, Steps: 63 | Train Loss: 0.6117118 Vali Loss: 0.2982894 Test Loss: 0.3624015
Validation loss decreased (inf --> 0.298289).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6446831226348877
Epoch: 2, Steps: 63 | Train Loss: 0.4947134 Vali Loss: 0.2592502 Test Loss: 0.3237455
Validation loss decreased (0.298289 --> 0.259250).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.2513601779937744
Epoch: 3, Steps: 63 | Train Loss: 0.4633878 Vali Loss: 0.2434997 Test Loss: 0.3090348
Validation loss decreased (0.259250 --> 0.243500).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.986692190170288
Epoch: 4, Steps: 63 | Train Loss: 0.4477613 Vali Loss: 0.2370897 Test Loss: 0.3017949
Validation loss decreased (0.243500 --> 0.237090).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8754374980926514
Epoch: 5, Steps: 63 | Train Loss: 0.4378668 Vali Loss: 0.2336856 Test Loss: 0.2974334
Validation loss decreased (0.237090 --> 0.233686).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.7582883834838867
Epoch: 6, Steps: 63 | Train Loss: 0.4338529 Vali Loss: 0.2304248 Test Loss: 0.2943512
Validation loss decreased (0.233686 --> 0.230425).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9694643020629883
Epoch: 7, Steps: 63 | Train Loss: 0.4300617 Vali Loss: 0.2281102 Test Loss: 0.2918466
Validation loss decreased (0.230425 --> 0.228110).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.507657527923584
Epoch: 8, Steps: 63 | Train Loss: 0.4276174 Vali Loss: 0.2263486 Test Loss: 0.2900303
Validation loss decreased (0.228110 --> 0.226349).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.8546123504638672
Epoch: 9, Steps: 63 | Train Loss: 0.4237885 Vali Loss: 0.2262577 Test Loss: 0.2886926
Validation loss decreased (0.226349 --> 0.226258).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.696178674697876
Epoch: 10, Steps: 63 | Train Loss: 0.4212166 Vali Loss: 0.2238519 Test Loss: 0.2874515
Validation loss decreased (0.226258 --> 0.223852).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7807202339172363
Epoch: 11, Steps: 63 | Train Loss: 0.4175211 Vali Loss: 0.2235129 Test Loss: 0.2866216
Validation loss decreased (0.223852 --> 0.223513).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9983885288238525
Epoch: 12, Steps: 63 | Train Loss: 0.4187062 Vali Loss: 0.2226127 Test Loss: 0.2856275
Validation loss decreased (0.223513 --> 0.222613).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9426589012145996
Epoch: 13, Steps: 63 | Train Loss: 0.4165559 Vali Loss: 0.2214157 Test Loss: 0.2850417
Validation loss decreased (0.222613 --> 0.221416).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.8726119995117188
Epoch: 14, Steps: 63 | Train Loss: 0.4119616 Vali Loss: 0.2217066 Test Loss: 0.2844212
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.03721284866333
Epoch: 15, Steps: 63 | Train Loss: 0.4137331 Vali Loss: 0.2208620 Test Loss: 0.2838908
Validation loss decreased (0.221416 --> 0.220862).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4437825679779053
Epoch: 16, Steps: 63 | Train Loss: 0.4147913 Vali Loss: 0.2212013 Test Loss: 0.2834404
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.8752059936523438
Epoch: 17, Steps: 63 | Train Loss: 0.4116853 Vali Loss: 0.2196240 Test Loss: 0.2831804
Validation loss decreased (0.220862 --> 0.219624).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6959078311920166
Epoch: 18, Steps: 63 | Train Loss: 0.4099566 Vali Loss: 0.2195613 Test Loss: 0.2827666
Validation loss decreased (0.219624 --> 0.219561).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.132754325866699
Epoch: 19, Steps: 63 | Train Loss: 0.4132381 Vali Loss: 0.2194689 Test Loss: 0.2825258
Validation loss decreased (0.219561 --> 0.219469).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.269970417022705
Epoch: 20, Steps: 63 | Train Loss: 0.4089826 Vali Loss: 0.2200811 Test Loss: 0.2821441
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.476451873779297
Epoch: 21, Steps: 63 | Train Loss: 0.4120647 Vali Loss: 0.2190540 Test Loss: 0.2818606
Validation loss decreased (0.219469 --> 0.219054).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.2005670070648193
Epoch: 22, Steps: 63 | Train Loss: 0.4102463 Vali Loss: 0.2192280 Test Loss: 0.2816617
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9964826107025146
Epoch: 23, Steps: 63 | Train Loss: 0.4126363 Vali Loss: 0.2190145 Test Loss: 0.2813515
Validation loss decreased (0.219054 --> 0.219015).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.0745456218719482
Epoch: 24, Steps: 63 | Train Loss: 0.4115859 Vali Loss: 0.2185418 Test Loss: 0.2811873
Validation loss decreased (0.219015 --> 0.218542).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.3875696659088135
Epoch: 25, Steps: 63 | Train Loss: 0.4112643 Vali Loss: 0.2186966 Test Loss: 0.2810761
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.6920311450958252
Epoch: 26, Steps: 63 | Train Loss: 0.4123587 Vali Loss: 0.2195186 Test Loss: 0.2809318
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.200059413909912
Epoch: 27, Steps: 63 | Train Loss: 0.4100134 Vali Loss: 0.2178953 Test Loss: 0.2807299
Validation loss decreased (0.218542 --> 0.217895).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.3065264225006104
Epoch: 28, Steps: 63 | Train Loss: 0.4096630 Vali Loss: 0.2191463 Test Loss: 0.2806138
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.9207732677459717
Epoch: 29, Steps: 63 | Train Loss: 0.4096284 Vali Loss: 0.2187172 Test Loss: 0.2804929
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.0365521907806396
Epoch: 30, Steps: 63 | Train Loss: 0.4101642 Vali Loss: 0.2180060 Test Loss: 0.2803859
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.1559267044067383
Epoch: 31, Steps: 63 | Train Loss: 0.4088143 Vali Loss: 0.2185016 Test Loss: 0.2803502
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.902841329574585
Epoch: 32, Steps: 63 | Train Loss: 0.4077769 Vali Loss: 0.2176206 Test Loss: 0.2803174
Validation loss decreased (0.217895 --> 0.217621).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.8401556015014648
Epoch: 33, Steps: 63 | Train Loss: 0.4053630 Vali Loss: 0.2179178 Test Loss: 0.2802037
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.2394018173217773
Epoch: 34, Steps: 63 | Train Loss: 0.4079012 Vali Loss: 0.2169575 Test Loss: 0.2801691
Validation loss decreased (0.217621 --> 0.216958).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.0687615871429443
Epoch: 35, Steps: 63 | Train Loss: 0.4098834 Vali Loss: 0.2168508 Test Loss: 0.2800413
Validation loss decreased (0.216958 --> 0.216851).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.9710004329681396
Epoch: 36, Steps: 63 | Train Loss: 0.4081252 Vali Loss: 0.2180555 Test Loss: 0.2799802
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6675477027893066
Epoch: 37, Steps: 63 | Train Loss: 0.4097668 Vali Loss: 0.2173969 Test Loss: 0.2799421
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7598700523376465
Epoch: 38, Steps: 63 | Train Loss: 0.4075353 Vali Loss: 0.2173289 Test Loss: 0.2798376
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7809100151062012
Epoch: 39, Steps: 63 | Train Loss: 0.4076805 Vali Loss: 0.2165010 Test Loss: 0.2798221
Validation loss decreased (0.216851 --> 0.216501).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.000054121017456
Epoch: 40, Steps: 63 | Train Loss: 0.4086827 Vali Loss: 0.2168131 Test Loss: 0.2797886
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.733893871307373
Epoch: 41, Steps: 63 | Train Loss: 0.4056706 Vali Loss: 0.2175862 Test Loss: 0.2797133
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.982104778289795
Epoch: 42, Steps: 63 | Train Loss: 0.4079687 Vali Loss: 0.2169263 Test Loss: 0.2796727
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.8544249534606934
Epoch: 43, Steps: 63 | Train Loss: 0.4062865 Vali Loss: 0.2175906 Test Loss: 0.2796293
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.303678512573242
Epoch: 44, Steps: 63 | Train Loss: 0.4083208 Vali Loss: 0.2166518 Test Loss: 0.2795974
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.805988073348999
Epoch: 45, Steps: 63 | Train Loss: 0.4060576 Vali Loss: 0.2160818 Test Loss: 0.2795455
Validation loss decreased (0.216501 --> 0.216082).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.4969704151153564
Epoch: 46, Steps: 63 | Train Loss: 0.4074284 Vali Loss: 0.2157900 Test Loss: 0.2795202
Validation loss decreased (0.216082 --> 0.215790).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.7595205307006836
Epoch: 47, Steps: 63 | Train Loss: 0.4082234 Vali Loss: 0.2165934 Test Loss: 0.2794931
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9447574615478516
Epoch: 48, Steps: 63 | Train Loss: 0.4092907 Vali Loss: 0.2172245 Test Loss: 0.2794289
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.8829913139343262
Epoch: 49, Steps: 63 | Train Loss: 0.4057707 Vali Loss: 0.2163634 Test Loss: 0.2794257
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.8656840324401855
Epoch: 50, Steps: 63 | Train Loss: 0.4085764 Vali Loss: 0.2174495 Test Loss: 0.2793830
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.8610310554504395
Epoch: 51, Steps: 63 | Train Loss: 0.4080542 Vali Loss: 0.2171480 Test Loss: 0.2793860
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.7389702796936035
Epoch: 52, Steps: 63 | Train Loss: 0.4056577 Vali Loss: 0.2172066 Test Loss: 0.2793232
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.9056997299194336
Epoch: 53, Steps: 63 | Train Loss: 0.4056466 Vali Loss: 0.2167961 Test Loss: 0.2793044
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.7119858264923096
Epoch: 54, Steps: 63 | Train Loss: 0.4064242 Vali Loss: 0.2166209 Test Loss: 0.2793176
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.716254711151123
Epoch: 55, Steps: 63 | Train Loss: 0.4072734 Vali Loss: 0.2163186 Test Loss: 0.2792787
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.6912460327148438
Epoch: 56, Steps: 63 | Train Loss: 0.4055006 Vali Loss: 0.2173069 Test Loss: 0.2792711
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.8090229034423828
Epoch: 57, Steps: 63 | Train Loss: 0.4086522 Vali Loss: 0.2165720 Test Loss: 0.2792391
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.8183350563049316
Epoch: 58, Steps: 63 | Train Loss: 0.4073905 Vali Loss: 0.2157935 Test Loss: 0.2792072
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.1829416751861572
Epoch: 59, Steps: 63 | Train Loss: 0.4073028 Vali Loss: 0.2168912 Test Loss: 0.2792311
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.145496129989624
Epoch: 60, Steps: 63 | Train Loss: 0.4067959 Vali Loss: 0.2165539 Test Loss: 0.2791887
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.8685669898986816
Epoch: 61, Steps: 63 | Train Loss: 0.4077588 Vali Loss: 0.2172300 Test Loss: 0.2791803
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.8902289867401123
Epoch: 62, Steps: 63 | Train Loss: 0.4065041 Vali Loss: 0.2177678 Test Loss: 0.2791705
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.7415761947631836
Epoch: 63, Steps: 63 | Train Loss: 0.4040189 Vali Loss: 0.2166103 Test Loss: 0.2791739
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.0265135765075684
Epoch: 64, Steps: 63 | Train Loss: 0.4049298 Vali Loss: 0.2169711 Test Loss: 0.2791654
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.253298044204712
Epoch: 65, Steps: 63 | Train Loss: 0.4058287 Vali Loss: 0.2180542 Test Loss: 0.2791577
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.9586334228515625
Epoch: 66, Steps: 63 | Train Loss: 0.4062879 Vali Loss: 0.2166327 Test Loss: 0.2791311
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_96_FITS_ETTh2_ftM_sl360_ll48_pl96_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.27914103865623474, mae:0.3408929407596588, rse:0.42578771710395813, corr:[0.27243957 0.27445653 0.2753019  0.27441552 0.27235308 0.2705315
 0.2694008  0.2684892  0.26745957 0.2660175  0.26433584 0.26256946
 0.26119658 0.2603669  0.25987047 0.2595313  0.2589567  0.25818178
 0.2571387  0.25590125 0.25455263 0.25307056 0.25147268 0.2495753
 0.24751203 0.24549358 0.24381804 0.24235646 0.24101701 0.23973565
 0.2384977  0.23713602 0.23551443 0.2338251  0.23230243 0.23107426
 0.23012191 0.2293086  0.22874399 0.22814192 0.22740777 0.22652644
 0.22544967 0.22422819 0.22309802 0.22203094 0.220952   0.2196642
 0.21805385 0.21632761 0.21480738 0.21351232 0.21229964 0.2110442
 0.2095226  0.20779803 0.20623091 0.20472151 0.20346527 0.20282091
 0.20279028 0.20298913 0.20323557 0.20328878 0.2030035  0.20251329
 0.20185688 0.20102797 0.20024318 0.19950855 0.19894288 0.19852796
 0.19811538 0.19758306 0.19688526 0.19597645 0.1952752  0.19484097
 0.19444768 0.19394243 0.19387425 0.19382717 0.1937527  0.19361693
 0.19349407 0.19328937 0.19303598 0.19282901 0.19244976 0.19228521
 0.1920908  0.19144243 0.19061534 0.18892044 0.18592815 0.18183933]
