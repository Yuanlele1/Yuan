Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=810, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=196, out_features=392, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  68841472.0
params:  77224.0
Trainable parameters:  77224
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.718674898147583
Epoch: 1, Steps: 56 | Train Loss: 1.0310494 Vali Loss: 0.7703722 Test Loss: 0.4316318
Validation loss decreased (inf --> 0.770372).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6292433738708496
Epoch: 2, Steps: 56 | Train Loss: 0.8903587 Vali Loss: 0.7331094 Test Loss: 0.4064182
Validation loss decreased (0.770372 --> 0.733109).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.6793525218963623
Epoch: 3, Steps: 56 | Train Loss: 0.8577920 Vali Loss: 0.7113388 Test Loss: 0.3976990
Validation loss decreased (0.733109 --> 0.711339).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6269445419311523
Epoch: 4, Steps: 56 | Train Loss: 0.8432235 Vali Loss: 0.6954653 Test Loss: 0.3926129
Validation loss decreased (0.711339 --> 0.695465).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.468130588531494
Epoch: 5, Steps: 56 | Train Loss: 0.8345778 Vali Loss: 0.6874396 Test Loss: 0.3893852
Validation loss decreased (0.695465 --> 0.687440).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.69405198097229
Epoch: 6, Steps: 56 | Train Loss: 0.8295513 Vali Loss: 0.6823663 Test Loss: 0.3871785
Validation loss decreased (0.687440 --> 0.682366).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5958878993988037
Epoch: 7, Steps: 56 | Train Loss: 0.8232840 Vali Loss: 0.6781017 Test Loss: 0.3853323
Validation loss decreased (0.682366 --> 0.678102).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.6775939464569092
Epoch: 8, Steps: 56 | Train Loss: 0.8209935 Vali Loss: 0.6724364 Test Loss: 0.3840689
Validation loss decreased (0.678102 --> 0.672436).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.6012706756591797
Epoch: 9, Steps: 56 | Train Loss: 0.8184494 Vali Loss: 0.6658657 Test Loss: 0.3830213
Validation loss decreased (0.672436 --> 0.665866).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.527137279510498
Epoch: 10, Steps: 56 | Train Loss: 0.8164524 Vali Loss: 0.6656059 Test Loss: 0.3822636
Validation loss decreased (0.665866 --> 0.665606).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.660893440246582
Epoch: 11, Steps: 56 | Train Loss: 0.8120115 Vali Loss: 0.6663313 Test Loss: 0.3815821
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.7344424724578857
Epoch: 12, Steps: 56 | Train Loss: 0.8117893 Vali Loss: 0.6654145 Test Loss: 0.3810769
Validation loss decreased (0.665606 --> 0.665414).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5920095443725586
Epoch: 13, Steps: 56 | Train Loss: 0.8098362 Vali Loss: 0.6607938 Test Loss: 0.3806731
Validation loss decreased (0.665414 --> 0.660794).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6080238819122314
Epoch: 14, Steps: 56 | Train Loss: 0.8069813 Vali Loss: 0.6589100 Test Loss: 0.3803164
Validation loss decreased (0.660794 --> 0.658910).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6696977615356445
Epoch: 15, Steps: 56 | Train Loss: 0.8091327 Vali Loss: 0.6571648 Test Loss: 0.3800697
Validation loss decreased (0.658910 --> 0.657165).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7441749572753906
Epoch: 16, Steps: 56 | Train Loss: 0.8081857 Vali Loss: 0.6570250 Test Loss: 0.3798587
Validation loss decreased (0.657165 --> 0.657025).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.5834746360778809
Epoch: 17, Steps: 56 | Train Loss: 0.8064926 Vali Loss: 0.6585179 Test Loss: 0.3796845
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6990272998809814
Epoch: 18, Steps: 56 | Train Loss: 0.8070419 Vali Loss: 0.6578296 Test Loss: 0.3795737
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6015939712524414
Epoch: 19, Steps: 56 | Train Loss: 0.8046963 Vali Loss: 0.6537547 Test Loss: 0.3794549
Validation loss decreased (0.657025 --> 0.653755).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.789766788482666
Epoch: 20, Steps: 56 | Train Loss: 0.8058778 Vali Loss: 0.6555068 Test Loss: 0.3793779
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.9247491359710693
Epoch: 21, Steps: 56 | Train Loss: 0.8049833 Vali Loss: 0.6541405 Test Loss: 0.3793474
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.7783677577972412
Epoch: 22, Steps: 56 | Train Loss: 0.8039960 Vali Loss: 0.6543022 Test Loss: 0.3792605
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6290459632873535
Epoch: 23, Steps: 56 | Train Loss: 0.8034714 Vali Loss: 0.6510657 Test Loss: 0.3791894
Validation loss decreased (0.653755 --> 0.651066).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.7047300338745117
Epoch: 24, Steps: 56 | Train Loss: 0.8036547 Vali Loss: 0.6534566 Test Loss: 0.3791692
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.541259765625
Epoch: 25, Steps: 56 | Train Loss: 0.8035016 Vali Loss: 0.6568110 Test Loss: 0.3791390
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.6594362258911133
Epoch: 26, Steps: 56 | Train Loss: 0.8031170 Vali Loss: 0.6532444 Test Loss: 0.3791055
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6938753128051758
Epoch: 27, Steps: 56 | Train Loss: 0.8043856 Vali Loss: 0.6512683 Test Loss: 0.3790710
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6224117279052734
Epoch: 28, Steps: 56 | Train Loss: 0.8040564 Vali Loss: 0.6538122 Test Loss: 0.3790462
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.8144378662109375
Epoch: 29, Steps: 56 | Train Loss: 0.8022621 Vali Loss: 0.6505747 Test Loss: 0.3790258
Validation loss decreased (0.651066 --> 0.650575).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4626569747924805
Epoch: 30, Steps: 56 | Train Loss: 0.8036263 Vali Loss: 0.6508210 Test Loss: 0.3790416
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.7399883270263672
Epoch: 31, Steps: 56 | Train Loss: 0.8028764 Vali Loss: 0.6514195 Test Loss: 0.3790181
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.653357982635498
Epoch: 32, Steps: 56 | Train Loss: 0.8038542 Vali Loss: 0.6512774 Test Loss: 0.3789969
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5763306617736816
Epoch: 33, Steps: 56 | Train Loss: 0.8012703 Vali Loss: 0.6491100 Test Loss: 0.3789833
Validation loss decreased (0.650575 --> 0.649110).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6139659881591797
Epoch: 34, Steps: 56 | Train Loss: 0.8017188 Vali Loss: 0.6538435 Test Loss: 0.3790066
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.6911542415618896
Epoch: 35, Steps: 56 | Train Loss: 0.8029026 Vali Loss: 0.6490250 Test Loss: 0.3789709
Validation loss decreased (0.649110 --> 0.649025).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.606048583984375
Epoch: 36, Steps: 56 | Train Loss: 0.8017048 Vali Loss: 0.6477919 Test Loss: 0.3789722
Validation loss decreased (0.649025 --> 0.647792).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5635769367218018
Epoch: 37, Steps: 56 | Train Loss: 0.8009861 Vali Loss: 0.6510292 Test Loss: 0.3789696
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.6450893878936768
Epoch: 38, Steps: 56 | Train Loss: 0.8030309 Vali Loss: 0.6489372 Test Loss: 0.3789542
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7252757549285889
Epoch: 39, Steps: 56 | Train Loss: 0.8016654 Vali Loss: 0.6506827 Test Loss: 0.3789507
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.559278964996338
Epoch: 40, Steps: 56 | Train Loss: 0.8028855 Vali Loss: 0.6494442 Test Loss: 0.3789656
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.536257028579712
Epoch: 41, Steps: 56 | Train Loss: 0.8008466 Vali Loss: 0.6488285 Test Loss: 0.3789517
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.6033802032470703
Epoch: 42, Steps: 56 | Train Loss: 0.8019441 Vali Loss: 0.6486206 Test Loss: 0.3789470
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.6077277660369873
Epoch: 43, Steps: 56 | Train Loss: 0.8002717 Vali Loss: 0.6469694 Test Loss: 0.3789485
Validation loss decreased (0.647792 --> 0.646969).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.5364563465118408
Epoch: 44, Steps: 56 | Train Loss: 0.8004487 Vali Loss: 0.6496800 Test Loss: 0.3789466
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.624600887298584
Epoch: 45, Steps: 56 | Train Loss: 0.8024460 Vali Loss: 0.6496576 Test Loss: 0.3789462
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.946631669998169
Epoch: 46, Steps: 56 | Train Loss: 0.8021862 Vali Loss: 0.6522883 Test Loss: 0.3789437
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.90476655960083
Epoch: 47, Steps: 56 | Train Loss: 0.8003745 Vali Loss: 0.6504278 Test Loss: 0.3789409
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.5703296661376953
Epoch: 48, Steps: 56 | Train Loss: 0.7994828 Vali Loss: 0.6515145 Test Loss: 0.3789517
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.6518583297729492
Epoch: 49, Steps: 56 | Train Loss: 0.8006605 Vali Loss: 0.6504681 Test Loss: 0.3789544
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6614222526550293
Epoch: 50, Steps: 56 | Train Loss: 0.7999468 Vali Loss: 0.6534576 Test Loss: 0.3789434
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9785478115081787
Epoch: 51, Steps: 56 | Train Loss: 0.8007715 Vali Loss: 0.6500233 Test Loss: 0.3789533
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6960804462432861
Epoch: 52, Steps: 56 | Train Loss: 0.8009214 Vali Loss: 0.6519208 Test Loss: 0.3789605
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.721553087234497
Epoch: 53, Steps: 56 | Train Loss: 0.8003022 Vali Loss: 0.6505636 Test Loss: 0.3789521
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.7486631870269775
Epoch: 54, Steps: 56 | Train Loss: 0.8007840 Vali Loss: 0.6500850 Test Loss: 0.3789568
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.6070525646209717
Epoch: 55, Steps: 56 | Train Loss: 0.8012071 Vali Loss: 0.6508609 Test Loss: 0.3789582
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.565901517868042
Epoch: 56, Steps: 56 | Train Loss: 0.8014292 Vali Loss: 0.6496866 Test Loss: 0.3789550
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.7570955753326416
Epoch: 57, Steps: 56 | Train Loss: 0.8004627 Vali Loss: 0.6481228 Test Loss: 0.3789601
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.6925177574157715
Epoch: 58, Steps: 56 | Train Loss: 0.7994131 Vali Loss: 0.6472658 Test Loss: 0.3789570
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.703209400177002
Epoch: 59, Steps: 56 | Train Loss: 0.8010204 Vali Loss: 0.6492408 Test Loss: 0.3789584
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.7426738739013672
Epoch: 60, Steps: 56 | Train Loss: 0.7998652 Vali Loss: 0.6509515 Test Loss: 0.3789603
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.5818800926208496
Epoch: 61, Steps: 56 | Train Loss: 0.7990402 Vali Loss: 0.6499654 Test Loss: 0.3789592
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.5007905960083008
Epoch: 62, Steps: 56 | Train Loss: 0.7990621 Vali Loss: 0.6456745 Test Loss: 0.3789594
Validation loss decreased (0.646969 --> 0.645674).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.5680444240570068
Epoch: 63, Steps: 56 | Train Loss: 0.8007684 Vali Loss: 0.6486241 Test Loss: 0.3789624
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.7030525207519531
Epoch: 64, Steps: 56 | Train Loss: 0.8003282 Vali Loss: 0.6478584 Test Loss: 0.3789619
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.6211795806884766
Epoch: 65, Steps: 56 | Train Loss: 0.8006922 Vali Loss: 0.6449981 Test Loss: 0.3789738
Validation loss decreased (0.645674 --> 0.644998).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.119030714035034
Epoch: 66, Steps: 56 | Train Loss: 0.8003888 Vali Loss: 0.6449180 Test Loss: 0.3789624
Validation loss decreased (0.644998 --> 0.644918).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.5606696605682373
Epoch: 67, Steps: 56 | Train Loss: 0.7990448 Vali Loss: 0.6503621 Test Loss: 0.3789679
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.6164350509643555
Epoch: 68, Steps: 56 | Train Loss: 0.7983527 Vali Loss: 0.6464090 Test Loss: 0.3789644
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.5581700801849365
Epoch: 69, Steps: 56 | Train Loss: 0.7970176 Vali Loss: 0.6468235 Test Loss: 0.3789632
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5652234554290771
Epoch: 70, Steps: 56 | Train Loss: 0.8002196 Vali Loss: 0.6456509 Test Loss: 0.3789723
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.5521759986877441
Epoch: 71, Steps: 56 | Train Loss: 0.8003798 Vali Loss: 0.6499500 Test Loss: 0.3789685
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.5700101852416992
Epoch: 72, Steps: 56 | Train Loss: 0.7981894 Vali Loss: 0.6470722 Test Loss: 0.3789698
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.5157461166381836
Epoch: 73, Steps: 56 | Train Loss: 0.7987561 Vali Loss: 0.6509427 Test Loss: 0.3789731
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.020540952682495
Epoch: 74, Steps: 56 | Train Loss: 0.7988225 Vali Loss: 0.6444490 Test Loss: 0.3789734
Validation loss decreased (0.644918 --> 0.644449).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.8492517471313477
Epoch: 75, Steps: 56 | Train Loss: 0.7975507 Vali Loss: 0.6460943 Test Loss: 0.3789716
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.5506691932678223
Epoch: 76, Steps: 56 | Train Loss: 0.8002031 Vali Loss: 0.6474023 Test Loss: 0.3789743
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.6745460033416748
Epoch: 77, Steps: 56 | Train Loss: 0.7993524 Vali Loss: 0.6480550 Test Loss: 0.3789728
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.559190273284912
Epoch: 78, Steps: 56 | Train Loss: 0.7997570 Vali Loss: 0.6494563 Test Loss: 0.3789742
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.6375339031219482
Epoch: 79, Steps: 56 | Train Loss: 0.8002273 Vali Loss: 0.6476194 Test Loss: 0.3789716
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.685401201248169
Epoch: 80, Steps: 56 | Train Loss: 0.8002243 Vali Loss: 0.6445724 Test Loss: 0.3789778
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.7490417957305908
Epoch: 81, Steps: 56 | Train Loss: 0.7998025 Vali Loss: 0.6482767 Test Loss: 0.3789776
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.6173179149627686
Epoch: 82, Steps: 56 | Train Loss: 0.7990750 Vali Loss: 0.6486795 Test Loss: 0.3789768
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.6437132358551025
Epoch: 83, Steps: 56 | Train Loss: 0.8003748 Vali Loss: 0.6484941 Test Loss: 0.3789756
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.809159278869629
Epoch: 84, Steps: 56 | Train Loss: 0.7988557 Vali Loss: 0.6489946 Test Loss: 0.3789776
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.883643627166748
Epoch: 85, Steps: 56 | Train Loss: 0.8010145 Vali Loss: 0.6455988 Test Loss: 0.3789801
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.535182237625122
Epoch: 86, Steps: 56 | Train Loss: 0.7998278 Vali Loss: 0.6440208 Test Loss: 0.3789779
Validation loss decreased (0.644449 --> 0.644021).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.5632660388946533
Epoch: 87, Steps: 56 | Train Loss: 0.8008043 Vali Loss: 0.6453982 Test Loss: 0.3789794
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.6065685749053955
Epoch: 88, Steps: 56 | Train Loss: 0.8003574 Vali Loss: 0.6432210 Test Loss: 0.3789786
Validation loss decreased (0.644021 --> 0.643221).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.6304798126220703
Epoch: 89, Steps: 56 | Train Loss: 0.8004668 Vali Loss: 0.6465768 Test Loss: 0.3789780
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.8280484676361084
Epoch: 90, Steps: 56 | Train Loss: 0.8003814 Vali Loss: 0.6476803 Test Loss: 0.3789795
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.6017262935638428
Epoch: 91, Steps: 56 | Train Loss: 0.8002184 Vali Loss: 0.6478794 Test Loss: 0.3789786
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.549100637435913
Epoch: 92, Steps: 56 | Train Loss: 0.7997458 Vali Loss: 0.6442223 Test Loss: 0.3789783
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.0529279708862305
Epoch: 93, Steps: 56 | Train Loss: 0.8003172 Vali Loss: 0.6491164 Test Loss: 0.3789780
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.5289292335510254
Epoch: 94, Steps: 56 | Train Loss: 0.7998617 Vali Loss: 0.6496338 Test Loss: 0.3789802
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.569993495941162
Epoch: 95, Steps: 56 | Train Loss: 0.8010188 Vali Loss: 0.6498327 Test Loss: 0.3789790
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.8287265300750732
Epoch: 96, Steps: 56 | Train Loss: 0.8012800 Vali Loss: 0.6497809 Test Loss: 0.3789798
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.7738909721374512
Epoch: 97, Steps: 56 | Train Loss: 0.7999908 Vali Loss: 0.6452796 Test Loss: 0.3789803
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.5944042205810547
Epoch: 98, Steps: 56 | Train Loss: 0.8005282 Vali Loss: 0.6442226 Test Loss: 0.3789802
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.400710344314575
Epoch: 99, Steps: 56 | Train Loss: 0.8008835 Vali Loss: 0.6480202 Test Loss: 0.3789805
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.5981204509735107
Epoch: 100, Steps: 56 | Train Loss: 0.8003303 Vali Loss: 0.6477622 Test Loss: 0.3789796
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3775757849216461, mae:0.42277562618255615, rse:0.49114280939102173, corr:[ 0.21588571  0.22017714  0.21776976  0.21895191  0.21896698  0.21716459
  0.21703097  0.21712177  0.21538207  0.21383199  0.21334246  0.21226308
  0.21058527  0.20952421  0.20894808  0.20813017  0.20744541  0.20697641
  0.20609365  0.20481831  0.20391037  0.20334585  0.2022185   0.20081356
  0.19941397  0.19824201  0.1971081   0.19622934  0.19568014  0.19506598
  0.1942658   0.19330327  0.19247998  0.1915502   0.19060284  0.18971804
  0.18899053  0.18839633  0.18762448  0.18683757  0.18625331  0.18583402
  0.185349    0.18470073  0.18392743  0.18331915  0.18253091  0.18106054
  0.17945038  0.17818882  0.17729473  0.17662586  0.1759583   0.17514044
  0.17447412  0.17402264  0.17334935  0.17262986  0.1722874   0.17206302
  0.1715616   0.17128481  0.17168073  0.172065    0.17196484  0.1720689
  0.17220946  0.1720131   0.17184049  0.17200537  0.17193827  0.171294
  0.17081267  0.17070056  0.17039335  0.16977014  0.1696629   0.16984037
  0.1694689   0.16902646  0.16918756  0.16942693  0.1690978   0.16875061
  0.16892971  0.16924374  0.16925567  0.16923945  0.16944857  0.16952835
  0.16924493  0.16913867  0.16949555  0.16970545  0.1696494   0.16953804
  0.1694904   0.169371    0.16923885  0.16916905  0.16897954  0.16841097
  0.16804296  0.16798198  0.16794057  0.16768852  0.16773982  0.16795789
  0.16772662  0.16724765  0.16721666  0.16741963  0.16722545  0.16690747
  0.16685125  0.16684453  0.16654193  0.16612741  0.16589947  0.16532506
  0.16426258  0.1634546   0.16305026  0.16246258  0.1616529   0.16111867
  0.16072513  0.16018333  0.15977575  0.15944295  0.15895642  0.15823655
  0.15775286  0.15734392  0.1567861   0.15633626  0.15610258  0.15582731
  0.15504181  0.15451266  0.15482317  0.15509774  0.1545572   0.15341409
  0.15219668  0.15125927  0.15045205  0.1499862   0.14980422  0.14944498
  0.14880165  0.14833283  0.148253    0.1479748   0.1473747   0.14664261
  0.14599687  0.14561884  0.14553842  0.14537138  0.14487556  0.14474794
  0.14504544  0.14525448  0.14540334  0.1457696   0.14608741  0.14548035
  0.14438207  0.14376493  0.1438107   0.14367937  0.14319351  0.14275792
  0.14235254  0.14159253  0.1408137   0.1404594   0.14001073  0.1393694
  0.13890386  0.13883142  0.13883977  0.1386714   0.13866292  0.13887697
  0.13889924  0.13909064  0.13966371  0.14021939  0.14058888  0.14081514
  0.1410314   0.14107512  0.14100064  0.14137208  0.14187387  0.14169723
  0.14128193  0.14118725  0.14137675  0.14126466  0.14108619  0.14095563
  0.14079577  0.1406753   0.14079154  0.14121836  0.14150554  0.14165626
  0.14193313  0.14232245  0.14257498  0.1428299   0.14314689  0.1430266
  0.14238074  0.14181747  0.14173956  0.14170372  0.1413131   0.14115247
  0.14118361  0.1413031   0.14131929  0.14142887  0.14151545  0.14139585
  0.14143156  0.14165173  0.14192934  0.1423864   0.14308515  0.1436771
  0.14393462  0.14437082  0.14531153  0.14602193  0.14608915  0.14617875
  0.14681955  0.14730072  0.14730874  0.1474887   0.14799592  0.1482006
  0.1479017   0.14783685  0.14860211  0.1493928   0.14962626  0.1498657
  0.15055485  0.15144049  0.15214753  0.15295292  0.15371817  0.15429421
  0.15477633  0.15547189  0.15619585  0.15682666  0.15746604  0.15805541
  0.15855925  0.15883246  0.15917532  0.15954068  0.15967305  0.15971623
  0.1598817   0.16022722  0.16055962  0.1608701   0.16137165  0.16186586
  0.16206336  0.162475    0.16328308  0.1640989   0.16453159  0.16485071
  0.16530673  0.16567867  0.1658138   0.16617782  0.16684179  0.16722213
  0.16723771  0.16715124  0.16705574  0.16697682  0.16695169  0.16699283
  0.16682811  0.16653995  0.1667489   0.16754447  0.16801786  0.16777973
  0.16757457  0.1680482   0.16865535  0.16900086  0.16940904  0.16998747
  0.17014456  0.17017762  0.17042306  0.17078958  0.17097974  0.17101969
  0.17110798  0.17102148  0.17086826  0.1706796   0.17036472  0.16983429
  0.16952239  0.1697817   0.1699605   0.16968714  0.16929105  0.16944383
  0.16965389  0.16954783  0.1698736   0.17064764  0.17126249  0.17128415
  0.17121753  0.17171434  0.1722439   0.17256105  0.1730141   0.17340776
  0.17342597  0.17334676  0.17351022  0.17370817  0.17385064  0.17378086
  0.17373809  0.17375055  0.17385246  0.17394838  0.17382969  0.17394558
  0.1742233   0.17468198  0.17514516  0.17562027  0.1762504   0.17697889
  0.17747837  0.17785427  0.17817491  0.1785939   0.17896362  0.17930087
  0.17959519  0.17977585  0.17986444  0.17989124  0.17992878  0.18003713
  0.18050958  0.18117686  0.18136913  0.18129691  0.18116803  0.18111391
  0.18091053  0.18085538  0.1812101   0.18165204  0.18169577  0.18137172
  0.18123592  0.18134716  0.18140796  0.18130179  0.18128629  0.18148042
  0.18162107  0.18159351  0.18149312  0.18149847  0.18175513  0.18213058
  0.182457    0.182786    0.18311937  0.18329306  0.1833917   0.18362634
  0.18392934  0.18360493  0.1831965   0.18318827  0.18309392  0.18263246
  0.18233429  0.18253599  0.1825674   0.18220544  0.18192361  0.18196042
  0.1816927   0.18122192  0.1810679   0.1811772   0.18102375  0.18055801
  0.18043958  0.18070768  0.18073869  0.18039732  0.1801641   0.1801166
  0.17965634  0.17900403  0.17840548  0.17771427  0.17676744  0.17574427
  0.17478094  0.17370988  0.17305033  0.17276165  0.17208216  0.1711376
  0.17054218  0.17038816  0.17006898  0.16931744  0.16887729  0.16863993
  0.16803442  0.16750114  0.1671998   0.16682717  0.16594598  0.1654472
  0.16535528  0.16510484  0.16453317  0.16440883  0.16437817  0.1637299
  0.16300207  0.1629591   0.16290264  0.16251887  0.16238645  0.16259772
  0.16246846  0.16185099  0.16161145  0.16182055  0.16150287  0.16081648
  0.16067915  0.16078915  0.16026452  0.15987821  0.15988918  0.1594679
  0.15867333  0.15812154  0.1581218   0.1578441   0.15728302  0.15698838
  0.15690422  0.15644127  0.15593968  0.15608849  0.15618989  0.15561233
  0.1548291   0.15440126  0.15395066  0.15311064  0.15246806  0.15195172
  0.15122981  0.15038912  0.14976533  0.14902648  0.1482727   0.14776543
  0.1473011   0.14652231  0.14577125  0.1455191   0.14519836  0.14453815
  0.14391984  0.14363195  0.14335898  0.14281066  0.14234743  0.14182281
  0.14074327  0.13944465  0.1385563   0.13798806  0.13730502  0.13667612
  0.1361775   0.13569452  0.1349833   0.13442762  0.13411859  0.13325043
  0.13196626  0.13124326  0.13121627  0.13080521  0.13000119  0.12957361
  0.1291443   0.12834848  0.1276215   0.1273738   0.12683167  0.12560134
  0.12420212  0.12328497  0.12248467  0.12119191  0.11991808  0.1187182
  0.1175917   0.116694    0.1160183   0.11508767  0.11391813  0.11308601
  0.1123732   0.11121818  0.11009484  0.10941783  0.1087042   0.10770403
  0.1066597   0.10625216  0.10594724  0.10503684  0.10414813  0.10324486
  0.10205477  0.1005562   0.09934066  0.09836975  0.09740923  0.09654234
  0.0957391   0.09463328  0.09385908  0.09358607  0.09351244  0.09284976
  0.09209304  0.09196858  0.09189811  0.0909813   0.09019774  0.09002778
  0.0895846   0.08846761  0.087537    0.08719157  0.08633916  0.08462895
  0.08295725  0.08204906  0.08113633  0.07999136  0.0790131   0.07805703
  0.07696258  0.07591845  0.07499819  0.07409608  0.07329293  0.07289004
  0.07222669  0.07122123  0.07057487  0.07039446  0.0700723   0.06933206
  0.06886457  0.06895843  0.06853535  0.06753009  0.06667916  0.06602269
  0.06476937  0.06326298  0.06203541  0.06110179  0.05965063  0.05835042
  0.05725926  0.05586837  0.05431546  0.0536028   0.05340229  0.05247025
  0.05153869  0.05144325  0.05152205  0.05095388  0.05052661  0.05105824
  0.05118905  0.05075153  0.05083562  0.05161925  0.05127403  0.04996502
  0.04893532  0.0484597   0.0477321   0.04656862  0.04556727  0.04502153
  0.04449387  0.04396434  0.04347229  0.04305065  0.04285785  0.04318028
  0.04294255  0.04180363  0.04128867  0.04152682  0.04089472  0.03998923
  0.0398369   0.03979227  0.03921201  0.03896559  0.03917906  0.03891297
  0.03732113  0.03575558  0.03521613  0.03458486  0.03357059  0.03251301
  0.03174416  0.03111827  0.0308998   0.03122726  0.03122107  0.03068122
  0.03041442  0.03067256  0.03055533  0.03009165  0.03070105  0.03145755
  0.03091564  0.03033078  0.03105097  0.03202339  0.03138013  0.03025009
  0.02999889  0.02986501  0.02887415  0.02748845  0.02688864  0.02614064
  0.02565853  0.02511445  0.0248948   0.02481353  0.02508969  0.02516201
  0.02425419  0.02310786  0.02375277  0.02440613  0.02342475  0.02312993
  0.02466173  0.02449318  0.02241307  0.02160741  0.02257921  0.02137845
  0.0178898   0.01558608  0.01521552  0.01376738  0.01178867  0.01076447
  0.01050683  0.0095469   0.00890078  0.00837708  0.00683916  0.00641054
  0.0070918   0.0053723   0.00326164  0.00434553  0.00585667  0.00293379
  0.00077732  0.00413979  0.00315968 -0.0044822  -0.00201773  0.00989344]
