Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=514, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.4921424388885498
Epoch: 1, Steps: 59 | Train Loss: 0.8172585 Vali Loss: 0.4975292 Test Loss: 0.3891209
Validation loss decreased (inf --> 0.497529).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.4743261337280273
Epoch: 2, Steps: 59 | Train Loss: 0.6899477 Vali Loss: 0.4496931 Test Loss: 0.3710821
Validation loss decreased (0.497529 --> 0.449693).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.4589378833770752
Epoch: 3, Steps: 59 | Train Loss: 0.6625723 Vali Loss: 0.4265208 Test Loss: 0.3655877
Validation loss decreased (0.449693 --> 0.426521).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4848124980926514
Epoch: 4, Steps: 59 | Train Loss: 0.6494126 Vali Loss: 0.4169551 Test Loss: 0.3630332
Validation loss decreased (0.426521 --> 0.416955).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.40665602684021
Epoch: 5, Steps: 59 | Train Loss: 0.6421883 Vali Loss: 0.4096039 Test Loss: 0.3616081
Validation loss decreased (0.416955 --> 0.409604).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.3774950504302979
Epoch: 6, Steps: 59 | Train Loss: 0.6356143 Vali Loss: 0.4028952 Test Loss: 0.3605163
Validation loss decreased (0.409604 --> 0.402895).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.3020923137664795
Epoch: 7, Steps: 59 | Train Loss: 0.6334836 Vali Loss: 0.4011931 Test Loss: 0.3600860
Validation loss decreased (0.402895 --> 0.401193).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.3417344093322754
Epoch: 8, Steps: 59 | Train Loss: 0.6304932 Vali Loss: 0.3982520 Test Loss: 0.3594538
Validation loss decreased (0.401193 --> 0.398252).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4582266807556152
Epoch: 9, Steps: 59 | Train Loss: 0.6274924 Vali Loss: 0.3968304 Test Loss: 0.3592389
Validation loss decreased (0.398252 --> 0.396830).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.3824350833892822
Epoch: 10, Steps: 59 | Train Loss: 0.6253235 Vali Loss: 0.3946095 Test Loss: 0.3590462
Validation loss decreased (0.396830 --> 0.394609).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.4391710758209229
Epoch: 11, Steps: 59 | Train Loss: 0.6232851 Vali Loss: 0.3925845 Test Loss: 0.3590080
Validation loss decreased (0.394609 --> 0.392585).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.3867521286010742
Epoch: 12, Steps: 59 | Train Loss: 0.6231515 Vali Loss: 0.3910457 Test Loss: 0.3586942
Validation loss decreased (0.392585 --> 0.391046).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6474130153656006
Epoch: 13, Steps: 59 | Train Loss: 0.6202715 Vali Loss: 0.3909162 Test Loss: 0.3586817
Validation loss decreased (0.391046 --> 0.390916).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.135352611541748
Epoch: 14, Steps: 59 | Train Loss: 0.6208780 Vali Loss: 0.3889827 Test Loss: 0.3587090
Validation loss decreased (0.390916 --> 0.388983).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.3414978981018066
Epoch: 15, Steps: 59 | Train Loss: 0.6194811 Vali Loss: 0.3895582 Test Loss: 0.3585915
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.374567985534668
Epoch: 16, Steps: 59 | Train Loss: 0.6198445 Vali Loss: 0.3886057 Test Loss: 0.3586327
Validation loss decreased (0.388983 --> 0.388606).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3457298278808594
Epoch: 17, Steps: 59 | Train Loss: 0.6174459 Vali Loss: 0.3877929 Test Loss: 0.3584442
Validation loss decreased (0.388606 --> 0.387793).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4437451362609863
Epoch: 18, Steps: 59 | Train Loss: 0.6175809 Vali Loss: 0.3875047 Test Loss: 0.3584276
Validation loss decreased (0.387793 --> 0.387505).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.3499484062194824
Epoch: 19, Steps: 59 | Train Loss: 0.6181478 Vali Loss: 0.3894996 Test Loss: 0.3584477
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.434570550918579
Epoch: 20, Steps: 59 | Train Loss: 0.6182949 Vali Loss: 0.3865877 Test Loss: 0.3584791
Validation loss decreased (0.387505 --> 0.386588).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.3677196502685547
Epoch: 21, Steps: 59 | Train Loss: 0.6163795 Vali Loss: 0.3846288 Test Loss: 0.3583007
Validation loss decreased (0.386588 --> 0.384629).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.3440039157867432
Epoch: 22, Steps: 59 | Train Loss: 0.6160499 Vali Loss: 0.3856453 Test Loss: 0.3582476
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3756883144378662
Epoch: 23, Steps: 59 | Train Loss: 0.6166821 Vali Loss: 0.3825004 Test Loss: 0.3582896
Validation loss decreased (0.384629 --> 0.382500).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4353225231170654
Epoch: 24, Steps: 59 | Train Loss: 0.6160990 Vali Loss: 0.3834825 Test Loss: 0.3582724
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.4036872386932373
Epoch: 25, Steps: 59 | Train Loss: 0.6160909 Vali Loss: 0.3834794 Test Loss: 0.3582112
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4541962146759033
Epoch: 26, Steps: 59 | Train Loss: 0.6150570 Vali Loss: 0.3846619 Test Loss: 0.3582179
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.992617607116699
Epoch: 27, Steps: 59 | Train Loss: 0.6161521 Vali Loss: 0.3830060 Test Loss: 0.3581740
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.4278202056884766
Epoch: 28, Steps: 59 | Train Loss: 0.6152438 Vali Loss: 0.3826632 Test Loss: 0.3582193
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.3862121105194092
Epoch: 29, Steps: 59 | Train Loss: 0.6149111 Vali Loss: 0.3843140 Test Loss: 0.3582446
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.3430252075195312
Epoch: 30, Steps: 59 | Train Loss: 0.6145764 Vali Loss: 0.3854945 Test Loss: 0.3582096
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.4905281066894531
Epoch: 31, Steps: 59 | Train Loss: 0.6142251 Vali Loss: 0.3828935 Test Loss: 0.3582366
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.5402758121490479
Epoch: 32, Steps: 59 | Train Loss: 0.6148999 Vali Loss: 0.3827063 Test Loss: 0.3581710
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4566295146942139
Epoch: 33, Steps: 59 | Train Loss: 0.6134882 Vali Loss: 0.3830559 Test Loss: 0.3582164
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.4111428260803223
Epoch: 34, Steps: 59 | Train Loss: 0.6135240 Vali Loss: 0.3831953 Test Loss: 0.3581981
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.5300977230072021
Epoch: 35, Steps: 59 | Train Loss: 0.6139251 Vali Loss: 0.3825122 Test Loss: 0.3581813
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.519578218460083
Epoch: 36, Steps: 59 | Train Loss: 0.6138488 Vali Loss: 0.3814484 Test Loss: 0.3582155
Validation loss decreased (0.382500 --> 0.381448).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6727027893066406
Epoch: 37, Steps: 59 | Train Loss: 0.6136657 Vali Loss: 0.3814785 Test Loss: 0.3581611
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4741475582122803
Epoch: 38, Steps: 59 | Train Loss: 0.6130182 Vali Loss: 0.3801765 Test Loss: 0.3581645
Validation loss decreased (0.381448 --> 0.380176).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.3786935806274414
Epoch: 39, Steps: 59 | Train Loss: 0.6143229 Vali Loss: 0.3828907 Test Loss: 0.3582120
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.326195001602173
Epoch: 40, Steps: 59 | Train Loss: 0.6129993 Vali Loss: 0.3828314 Test Loss: 0.3582145
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.472792387008667
Epoch: 41, Steps: 59 | Train Loss: 0.6130542 Vali Loss: 0.3832829 Test Loss: 0.3581490
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.606917142868042
Epoch: 42, Steps: 59 | Train Loss: 0.6124830 Vali Loss: 0.3809832 Test Loss: 0.3581797
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.5587756633758545
Epoch: 43, Steps: 59 | Train Loss: 0.6136810 Vali Loss: 0.3810264 Test Loss: 0.3581965
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.515866994857788
Epoch: 44, Steps: 59 | Train Loss: 0.6130959 Vali Loss: 0.3817675 Test Loss: 0.3581360
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5245487689971924
Epoch: 45, Steps: 59 | Train Loss: 0.6115123 Vali Loss: 0.3812131 Test Loss: 0.3581478
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.4890797138214111
Epoch: 46, Steps: 59 | Train Loss: 0.6127707 Vali Loss: 0.3818653 Test Loss: 0.3581387
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.4404182434082031
Epoch: 47, Steps: 59 | Train Loss: 0.6132783 Vali Loss: 0.3794096 Test Loss: 0.3581184
Validation loss decreased (0.380176 --> 0.379410).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.4705843925476074
Epoch: 48, Steps: 59 | Train Loss: 0.6125029 Vali Loss: 0.3798950 Test Loss: 0.3581131
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.6587786674499512
Epoch: 49, Steps: 59 | Train Loss: 0.6121171 Vali Loss: 0.3827824 Test Loss: 0.3581554
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6391596794128418
Epoch: 50, Steps: 59 | Train Loss: 0.6134306 Vali Loss: 0.3824281 Test Loss: 0.3581584
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.1950466632843018
Epoch: 51, Steps: 59 | Train Loss: 0.6125592 Vali Loss: 0.3816784 Test Loss: 0.3581411
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.4785306453704834
Epoch: 52, Steps: 59 | Train Loss: 0.6125669 Vali Loss: 0.3809785 Test Loss: 0.3581246
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.439913034439087
Epoch: 53, Steps: 59 | Train Loss: 0.6117371 Vali Loss: 0.3797112 Test Loss: 0.3581235
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5061182975769043
Epoch: 54, Steps: 59 | Train Loss: 0.6127020 Vali Loss: 0.3811090 Test Loss: 0.3581368
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.6109046936035156
Epoch: 55, Steps: 59 | Train Loss: 0.6131004 Vali Loss: 0.3796530 Test Loss: 0.3581309
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.464301586151123
Epoch: 56, Steps: 59 | Train Loss: 0.6117203 Vali Loss: 0.3820636 Test Loss: 0.3581623
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.4506313800811768
Epoch: 57, Steps: 59 | Train Loss: 0.6103589 Vali Loss: 0.3796805 Test Loss: 0.3581320
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.4772179126739502
Epoch: 58, Steps: 59 | Train Loss: 0.6113441 Vali Loss: 0.3794855 Test Loss: 0.3581227
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.4764723777770996
Epoch: 59, Steps: 59 | Train Loss: 0.6123816 Vali Loss: 0.3813274 Test Loss: 0.3581433
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.4696505069732666
Epoch: 60, Steps: 59 | Train Loss: 0.6109069 Vali Loss: 0.3796687 Test Loss: 0.3581704
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.4184417724609375
Epoch: 61, Steps: 59 | Train Loss: 0.6124213 Vali Loss: 0.3803630 Test Loss: 0.3581778
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.7367780208587646
Epoch: 62, Steps: 59 | Train Loss: 0.6114370 Vali Loss: 0.3806339 Test Loss: 0.3581418
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.4317903518676758
Epoch: 63, Steps: 59 | Train Loss: 0.6117742 Vali Loss: 0.3804436 Test Loss: 0.3581564
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.568108081817627
Epoch: 64, Steps: 59 | Train Loss: 0.6112173 Vali Loss: 0.3816108 Test Loss: 0.3581570
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.387725591659546
Epoch: 65, Steps: 59 | Train Loss: 0.6115606 Vali Loss: 0.3814102 Test Loss: 0.3581642
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.530099630355835
Epoch: 66, Steps: 59 | Train Loss: 0.6114746 Vali Loss: 0.3829644 Test Loss: 0.3581539
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.410557508468628
Epoch: 67, Steps: 59 | Train Loss: 0.6110196 Vali Loss: 0.3780295 Test Loss: 0.3581510
Validation loss decreased (0.379410 --> 0.378030).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.4661974906921387
Epoch: 68, Steps: 59 | Train Loss: 0.6127500 Vali Loss: 0.3819164 Test Loss: 0.3581769
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.40256929397583
Epoch: 69, Steps: 59 | Train Loss: 0.6121940 Vali Loss: 0.3803790 Test Loss: 0.3581692
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.3472232818603516
Epoch: 70, Steps: 59 | Train Loss: 0.6125150 Vali Loss: 0.3781362 Test Loss: 0.3581521
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.3933806419372559
Epoch: 71, Steps: 59 | Train Loss: 0.6112915 Vali Loss: 0.3773315 Test Loss: 0.3581655
Validation loss decreased (0.378030 --> 0.377331).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.4790055751800537
Epoch: 72, Steps: 59 | Train Loss: 0.6129394 Vali Loss: 0.3779234 Test Loss: 0.3581566
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.5026185512542725
Epoch: 73, Steps: 59 | Train Loss: 0.6127388 Vali Loss: 0.3797244 Test Loss: 0.3581496
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.5327465534210205
Epoch: 74, Steps: 59 | Train Loss: 0.6120124 Vali Loss: 0.3800454 Test Loss: 0.3581579
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.944667100906372
Epoch: 75, Steps: 59 | Train Loss: 0.6119000 Vali Loss: 0.3781191 Test Loss: 0.3581591
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.5553038120269775
Epoch: 76, Steps: 59 | Train Loss: 0.6116134 Vali Loss: 0.3794145 Test Loss: 0.3581611
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.596501111984253
Epoch: 77, Steps: 59 | Train Loss: 0.6128270 Vali Loss: 0.3806169 Test Loss: 0.3581492
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.5217788219451904
Epoch: 78, Steps: 59 | Train Loss: 0.6115748 Vali Loss: 0.3791334 Test Loss: 0.3581527
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.465773105621338
Epoch: 79, Steps: 59 | Train Loss: 0.6128414 Vali Loss: 0.3791654 Test Loss: 0.3581561
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.681396961212158
Epoch: 80, Steps: 59 | Train Loss: 0.6121003 Vali Loss: 0.3798138 Test Loss: 0.3581527
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.6353468894958496
Epoch: 81, Steps: 59 | Train Loss: 0.6116730 Vali Loss: 0.3803633 Test Loss: 0.3581571
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.4201922416687012
Epoch: 82, Steps: 59 | Train Loss: 0.6113948 Vali Loss: 0.3789365 Test Loss: 0.3581575
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.42755126953125
Epoch: 83, Steps: 59 | Train Loss: 0.6103320 Vali Loss: 0.3791875 Test Loss: 0.3581502
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.7669713497161865
Epoch: 84, Steps: 59 | Train Loss: 0.6128951 Vali Loss: 0.3792870 Test Loss: 0.3581515
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.371476411819458
Epoch: 85, Steps: 59 | Train Loss: 0.6122038 Vali Loss: 0.3789262 Test Loss: 0.3581540
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.5688979625701904
Epoch: 86, Steps: 59 | Train Loss: 0.6106321 Vali Loss: 0.3804127 Test Loss: 0.3581541
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.1252212524414062
Epoch: 87, Steps: 59 | Train Loss: 0.6119590 Vali Loss: 0.3791601 Test Loss: 0.3581560
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.610367774963379
Epoch: 88, Steps: 59 | Train Loss: 0.6126606 Vali Loss: 0.3822234 Test Loss: 0.3581518
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.64863920211792
Epoch: 89, Steps: 59 | Train Loss: 0.6124714 Vali Loss: 0.3794591 Test Loss: 0.3581572
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.4974799156188965
Epoch: 90, Steps: 59 | Train Loss: 0.6107235 Vali Loss: 0.3809187 Test Loss: 0.3581570
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.4163787364959717
Epoch: 91, Steps: 59 | Train Loss: 0.6110790 Vali Loss: 0.3797163 Test Loss: 0.3581586
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35393160581588745, mae:0.3954601585865021, rse:0.47566282749176025, corr:[0.2590105  0.2632344  0.26138625 0.26308438 0.26216984 0.26052058
 0.2609888  0.2607103  0.25890493 0.25801685 0.25749663 0.25598657
 0.25471586 0.2539462  0.2530447  0.25217405 0.25187013 0.25134835
 0.25023174 0.24913631 0.24832858 0.24743935 0.24617136 0.24496347
 0.24352963 0.24201557 0.24080984 0.23993407 0.23896365 0.23796111
 0.23723687 0.23639128 0.23543839 0.23462169 0.23385209 0.23283401
 0.23201944 0.23161167 0.23077866 0.22957218 0.22888224 0.22861873
 0.22791491 0.2268083  0.22593433 0.22527477 0.22395422 0.22209592
 0.22062774 0.21929783 0.21755406 0.21609126 0.2150943  0.21348988
 0.2114039  0.20997314 0.20868008 0.20687029 0.20538834 0.2046943
 0.20407908 0.20349611 0.20355289 0.20373626 0.20316881 0.20263591
 0.20230818 0.20185556 0.2011794  0.20076126 0.20035404 0.19950268
 0.19841695 0.19764452 0.19683765 0.19550647 0.19457519 0.19429994
 0.19379304 0.19282395 0.19243066 0.19227357 0.191682   0.1910758
 0.19103715 0.19103903 0.19052143 0.1900242  0.18987869 0.18957812
 0.18896078 0.18869627 0.18899287 0.18882494 0.1883226  0.1882222
 0.18815692 0.18728796 0.18631037 0.18600331 0.18576643 0.18477237
 0.18409023 0.18413438 0.18407671 0.18323345 0.18289538 0.18320675
 0.18279888 0.18179035 0.18126142 0.18126558 0.1806599  0.17996767
 0.17975932 0.17946121 0.17856444 0.17754734 0.17701979 0.17606048
 0.17453074 0.1734234  0.17289549 0.17193647 0.17074634 0.1702951
 0.17003635 0.1691446  0.16818021 0.16762033 0.1668782  0.1657225
 0.1651564  0.16503233 0.164577   0.1639147  0.16360912 0.16327569
 0.16228604 0.1614866  0.16147472 0.16119695 0.16006337 0.158687
 0.15739158 0.1559362  0.15443438 0.15351866 0.15285587 0.15192677
 0.15106472 0.15055344 0.15012167 0.14920507 0.14836828 0.14781968
 0.14716583 0.14660235 0.14666107 0.14666687 0.14584398 0.14524582
 0.14507318 0.14457954 0.14395389 0.14404184 0.14429305 0.14323615
 0.14151855 0.14040408 0.13981427 0.13868693 0.13744323 0.1365046
 0.1356815  0.13422805 0.13314569 0.13273531 0.13189408 0.13087332
 0.1303115  0.13014999 0.12984279 0.12961923 0.12935454 0.12904763
 0.1285273  0.12836039 0.1287462  0.12887575 0.1289592  0.12892291
 0.12857175 0.12783061 0.12718464 0.12704347 0.12652694 0.1255007
 0.1249648  0.12482762 0.12443197 0.12377916 0.12367243 0.12364502
 0.12297049 0.12256943 0.12278622 0.1230562  0.12281626 0.122893
 0.12331849 0.12338807 0.12316182 0.12320145 0.12313786 0.12223455
 0.12128495 0.1208036  0.11996621 0.11884833 0.11843406 0.1190026
 0.11878566 0.11827996 0.11835165 0.11853863 0.11775512 0.11670256
 0.116489   0.11652379 0.11591133 0.11598182 0.11680181 0.11730889
 0.11711654 0.11739991 0.11812739 0.11833513 0.11810102 0.11810096
 0.11786649 0.11696054 0.11637515 0.11664137 0.11640755 0.11547778
 0.11527617 0.11556825 0.11566676 0.11563148 0.11588017 0.11612993
 0.11602944 0.11687844 0.1180877  0.11911702 0.11952446 0.12052092
 0.12126526 0.12135465 0.12162539 0.12228738 0.12296252 0.12295495
 0.12335539 0.12378129 0.12363931 0.12333761 0.12357596 0.12373008
 0.12297128 0.122634   0.12352747 0.12367576 0.1232429  0.12304068
 0.12346372 0.12335318 0.12310468 0.12382563 0.12451793 0.1240729
 0.12371634 0.12433909 0.12418775 0.12318704 0.12328471 0.12419733
 0.12393747 0.1228867  0.12249853 0.12222058 0.12101126 0.12002804
 0.12022353 0.11959806 0.11818878 0.11815362 0.11918938 0.11834756
 0.11651465 0.11711626 0.11852739 0.11866067 0.11833876 0.11974267
 0.11992223 0.11909147 0.11943801 0.12061559 0.12026209 0.11941744
 0.11997994 0.11944558 0.11755983 0.11698733 0.11768361 0.11642057
 0.11473177 0.11637191 0.117966   0.11695821 0.11626143 0.11867423
 0.11942261 0.11803187 0.11915164 0.12153963 0.12106586 0.1205094
 0.12228497 0.1220272  0.12088967 0.12387113 0.1253102  0.11636073]
