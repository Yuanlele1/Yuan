Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=26, out_features=130, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3028480.0
params:  3510.0
Trainable parameters:  3510
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.8913559913635254
Epoch: 1, Steps: 60 | Train Loss: 1.1944622 Vali Loss: 0.7900378 Test Loss: 0.5763187
Validation loss decreased (inf --> 0.790038).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.681826591491699
Epoch: 2, Steps: 60 | Train Loss: 1.0295929 Vali Loss: 0.7281371 Test Loss: 0.5075088
Validation loss decreased (0.790038 --> 0.728137).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.3786396980285645
Epoch: 3, Steps: 60 | Train Loss: 0.9494068 Vali Loss: 0.6960816 Test Loss: 0.4700024
Validation loss decreased (0.728137 --> 0.696082).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.8942339420318604
Epoch: 4, Steps: 60 | Train Loss: 0.8996875 Vali Loss: 0.6746773 Test Loss: 0.4484112
Validation loss decreased (0.696082 --> 0.674677).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.018482208251953
Epoch: 5, Steps: 60 | Train Loss: 0.8706376 Vali Loss: 0.6620778 Test Loss: 0.4352454
Validation loss decreased (0.674677 --> 0.662078).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.6822431087493896
Epoch: 6, Steps: 60 | Train Loss: 0.8541685 Vali Loss: 0.6561387 Test Loss: 0.4270594
Validation loss decreased (0.662078 --> 0.656139).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.1953611373901367
Epoch: 7, Steps: 60 | Train Loss: 0.8437726 Vali Loss: 0.6518855 Test Loss: 0.4217625
Validation loss decreased (0.656139 --> 0.651886).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.286611557006836
Epoch: 8, Steps: 60 | Train Loss: 0.8366053 Vali Loss: 0.6437452 Test Loss: 0.4183295
Validation loss decreased (0.651886 --> 0.643745).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.7845704555511475
Epoch: 9, Steps: 60 | Train Loss: 0.8339519 Vali Loss: 0.6441162 Test Loss: 0.4159135
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.3112120628356934
Epoch: 10, Steps: 60 | Train Loss: 0.8262976 Vali Loss: 0.6399578 Test Loss: 0.4142597
Validation loss decreased (0.643745 --> 0.639958).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7792868614196777
Epoch: 11, Steps: 60 | Train Loss: 0.8259894 Vali Loss: 0.6399314 Test Loss: 0.4130349
Validation loss decreased (0.639958 --> 0.639931).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.809415578842163
Epoch: 12, Steps: 60 | Train Loss: 0.8242400 Vali Loss: 0.6417366 Test Loss: 0.4121423
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.986046075820923
Epoch: 13, Steps: 60 | Train Loss: 0.8197458 Vali Loss: 0.6355676 Test Loss: 0.4114635
Validation loss decreased (0.639931 --> 0.635568).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.618013381958008
Epoch: 14, Steps: 60 | Train Loss: 0.8233231 Vali Loss: 0.6420158 Test Loss: 0.4109069
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.0424044132232666
Epoch: 15, Steps: 60 | Train Loss: 0.8201430 Vali Loss: 0.6366241 Test Loss: 0.4105202
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9483397006988525
Epoch: 16, Steps: 60 | Train Loss: 0.8216145 Vali Loss: 0.6404815 Test Loss: 0.4101186
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.123671054840088
Epoch: 17, Steps: 60 | Train Loss: 0.8205655 Vali Loss: 0.6347972 Test Loss: 0.4098192
Validation loss decreased (0.635568 --> 0.634797).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.2210781574249268
Epoch: 18, Steps: 60 | Train Loss: 0.8219914 Vali Loss: 0.6339505 Test Loss: 0.4095779
Validation loss decreased (0.634797 --> 0.633950).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.6130502223968506
Epoch: 19, Steps: 60 | Train Loss: 0.8217889 Vali Loss: 0.6348849 Test Loss: 0.4093378
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.035597562789917
Epoch: 20, Steps: 60 | Train Loss: 0.8184702 Vali Loss: 0.6339893 Test Loss: 0.4091672
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.1187944412231445
Epoch: 21, Steps: 60 | Train Loss: 0.8202308 Vali Loss: 0.6350721 Test Loss: 0.4089966
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.1144328117370605
Epoch: 22, Steps: 60 | Train Loss: 0.8192103 Vali Loss: 0.6299718 Test Loss: 0.4088486
Validation loss decreased (0.633950 --> 0.629972).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.0100200176239014
Epoch: 23, Steps: 60 | Train Loss: 0.8173911 Vali Loss: 0.6336927 Test Loss: 0.4087306
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.645503282546997
Epoch: 24, Steps: 60 | Train Loss: 0.8159194 Vali Loss: 0.6333789 Test Loss: 0.4086129
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.061901807785034
Epoch: 25, Steps: 60 | Train Loss: 0.8188532 Vali Loss: 0.6308702 Test Loss: 0.4085278
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.964411735534668
Epoch: 26, Steps: 60 | Train Loss: 0.8164608 Vali Loss: 0.6278241 Test Loss: 0.4084119
Validation loss decreased (0.629972 --> 0.627824).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.2429020404815674
Epoch: 27, Steps: 60 | Train Loss: 0.8164975 Vali Loss: 0.6291705 Test Loss: 0.4083329
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.1210110187530518
Epoch: 28, Steps: 60 | Train Loss: 0.8155179 Vali Loss: 0.6298515 Test Loss: 0.4082514
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.4270005226135254
Epoch: 29, Steps: 60 | Train Loss: 0.8159326 Vali Loss: 0.6284814 Test Loss: 0.4081782
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.2975175380706787
Epoch: 30, Steps: 60 | Train Loss: 0.8159787 Vali Loss: 0.6301988 Test Loss: 0.4081303
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.887542724609375
Epoch: 31, Steps: 60 | Train Loss: 0.8152238 Vali Loss: 0.6335897 Test Loss: 0.4080598
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.9925825595855713
Epoch: 32, Steps: 60 | Train Loss: 0.8165183 Vali Loss: 0.6317807 Test Loss: 0.4080087
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.454868793487549
Epoch: 33, Steps: 60 | Train Loss: 0.8150377 Vali Loss: 0.6318054 Test Loss: 0.4079597
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.8135745525360107
Epoch: 34, Steps: 60 | Train Loss: 0.8159396 Vali Loss: 0.6301092 Test Loss: 0.4078996
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.0187795162200928
Epoch: 35, Steps: 60 | Train Loss: 0.8157581 Vali Loss: 0.6287112 Test Loss: 0.4078805
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.9781627655029297
Epoch: 36, Steps: 60 | Train Loss: 0.8147497 Vali Loss: 0.6340353 Test Loss: 0.4078311
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.669578790664673
Epoch: 37, Steps: 60 | Train Loss: 0.8161378 Vali Loss: 0.6280676 Test Loss: 0.4078018
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.0400495529174805
Epoch: 38, Steps: 60 | Train Loss: 0.8161851 Vali Loss: 0.6297764 Test Loss: 0.4077654
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.6179628372192383
Epoch: 39, Steps: 60 | Train Loss: 0.8155398 Vali Loss: 0.6319820 Test Loss: 0.4077384
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.6359987258911133
Epoch: 40, Steps: 60 | Train Loss: 0.8146015 Vali Loss: 0.6331229 Test Loss: 0.4076954
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.692598342895508
Epoch: 41, Steps: 60 | Train Loss: 0.8150803 Vali Loss: 0.6355911 Test Loss: 0.4076711
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.104954481124878
Epoch: 42, Steps: 60 | Train Loss: 0.8155708 Vali Loss: 0.6304299 Test Loss: 0.4076517
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.672318696975708
Epoch: 43, Steps: 60 | Train Loss: 0.8140444 Vali Loss: 0.6296813 Test Loss: 0.4076208
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.4382381439208984
Epoch: 44, Steps: 60 | Train Loss: 0.8154442 Vali Loss: 0.6272406 Test Loss: 0.4076095
Validation loss decreased (0.627824 --> 0.627241).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.2673656940460205
Epoch: 45, Steps: 60 | Train Loss: 0.8152478 Vali Loss: 0.6311457 Test Loss: 0.4075830
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8473632335662842
Epoch: 46, Steps: 60 | Train Loss: 0.8149171 Vali Loss: 0.6311445 Test Loss: 0.4075586
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.1815710067749023
Epoch: 47, Steps: 60 | Train Loss: 0.8153516 Vali Loss: 0.6267583 Test Loss: 0.4075420
Validation loss decreased (0.627241 --> 0.626758).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.2660937309265137
Epoch: 48, Steps: 60 | Train Loss: 0.8155587 Vali Loss: 0.6298434 Test Loss: 0.4075276
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.510633945465088
Epoch: 49, Steps: 60 | Train Loss: 0.8159633 Vali Loss: 0.6279758 Test Loss: 0.4075126
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.023977756500244
Epoch: 50, Steps: 60 | Train Loss: 0.8148551 Vali Loss: 0.6286507 Test Loss: 0.4074979
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.9713563919067383
Epoch: 51, Steps: 60 | Train Loss: 0.8124411 Vali Loss: 0.6289834 Test Loss: 0.4074795
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.2007384300231934
Epoch: 52, Steps: 60 | Train Loss: 0.8166676 Vali Loss: 0.6302372 Test Loss: 0.4074693
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.844733238220215
Epoch: 53, Steps: 60 | Train Loss: 0.8146865 Vali Loss: 0.6273652 Test Loss: 0.4074558
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.463974952697754
Epoch: 54, Steps: 60 | Train Loss: 0.8168568 Vali Loss: 0.6274791 Test Loss: 0.4074408
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.025951385498047
Epoch: 55, Steps: 60 | Train Loss: 0.8119573 Vali Loss: 0.6298777 Test Loss: 0.4074299
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.447671413421631
Epoch: 56, Steps: 60 | Train Loss: 0.8134523 Vali Loss: 0.6317818 Test Loss: 0.4074154
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.217193365097046
Epoch: 57, Steps: 60 | Train Loss: 0.8160623 Vali Loss: 0.6298276 Test Loss: 0.4074062
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.957484245300293
Epoch: 58, Steps: 60 | Train Loss: 0.8148782 Vali Loss: 0.6281068 Test Loss: 0.4073991
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.592972993850708
Epoch: 59, Steps: 60 | Train Loss: 0.8155766 Vali Loss: 0.6308442 Test Loss: 0.4073927
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.440295457839966
Epoch: 60, Steps: 60 | Train Loss: 0.8150369 Vali Loss: 0.6292852 Test Loss: 0.4073853
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.05733585357666
Epoch: 61, Steps: 60 | Train Loss: 0.8150518 Vali Loss: 0.6273464 Test Loss: 0.4073737
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.847745656967163
Epoch: 62, Steps: 60 | Train Loss: 0.8149629 Vali Loss: 0.6279912 Test Loss: 0.4073711
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.3935720920562744
Epoch: 63, Steps: 60 | Train Loss: 0.8125240 Vali Loss: 0.6341259 Test Loss: 0.4073637
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.2290260791778564
Epoch: 64, Steps: 60 | Train Loss: 0.8148960 Vali Loss: 0.6315016 Test Loss: 0.4073527
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.046149730682373
Epoch: 65, Steps: 60 | Train Loss: 0.8147259 Vali Loss: 0.6291326 Test Loss: 0.4073475
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.7087907791137695
Epoch: 66, Steps: 60 | Train Loss: 0.8135885 Vali Loss: 0.6273077 Test Loss: 0.4073425
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.381561756134033
Epoch: 67, Steps: 60 | Train Loss: 0.8150928 Vali Loss: 0.6280450 Test Loss: 0.4073370
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4057105481624603, mae:0.4315127730369568, rse:0.5091126561164856, corr:[ 0.21804911  0.2205364   0.2203481   0.21843031  0.21667963  0.21584786
  0.21561876  0.21519502  0.2142048   0.21251827  0.21089447  0.20964773
  0.20900129  0.2085104   0.20781429  0.20682226  0.20581096  0.20503119
  0.20465136  0.2044927   0.20412011  0.20318386  0.20163186  0.19970453
  0.19782673  0.1964572   0.1954506   0.19461216  0.19357379  0.19229756
  0.19084054  0.18938722  0.18802527  0.18690069  0.18606527  0.18523088
  0.184293    0.18312433  0.18204021  0.18105432  0.18037735  0.17991222
  0.17957368  0.17917803  0.17848389  0.17739773  0.17590007  0.17398588
  0.1719347   0.17021358  0.16859129  0.16716473  0.1656993   0.16454948
  0.16375032  0.16320837  0.16285542  0.16240864  0.16202505  0.16148949
  0.16119885  0.16087642  0.16079532  0.16062702  0.16049504  0.16034134
  0.16023526  0.16021028  0.16020708  0.16015558  0.15983468  0.15922567
  0.15824175  0.15725127  0.15636604  0.15572473  0.15539406  0.15544984
  0.15550365  0.15534338  0.15515548  0.15495706  0.15496267  0.15501347
  0.15517236  0.15513021  0.15498047  0.15461709  0.15431531  0.15417245
  0.15430024  0.15456802  0.15485951  0.15490349  0.15456364  0.15395263
  0.15299886  0.15221675  0.15142258  0.15089996  0.15051134  0.15005304
  0.14977455  0.14975703  0.1500667   0.1503661   0.15069729  0.15076536
  0.15071747  0.15036671  0.15004805  0.14980224  0.14974698  0.14975779
  0.1497009   0.14947039  0.14901468  0.14834751  0.14759964  0.14680825
  0.14583018  0.14484987  0.14369017  0.14270617  0.14185433  0.14121571
  0.14070785  0.14029303  0.13991049  0.13937241  0.13899796  0.13865587
  0.13838597  0.13798675  0.13750705  0.13696925  0.13652825  0.1361495
  0.13591164  0.13588828  0.13595162  0.13583699  0.13512069  0.13399276
  0.13230918  0.1309601   0.12985434  0.12922822  0.12879364  0.1283813
  0.12798652  0.12757118  0.12735465  0.12727056  0.12736776  0.12722376
  0.12713778  0.12684439  0.12636548  0.12583348  0.12557924  0.12557928
  0.12576208  0.12597643  0.1260135   0.12574077  0.12502219  0.12418377
  0.12312535  0.12241623  0.12168602  0.12111686  0.12062255  0.12012991
  0.11967289  0.11937506  0.11938765  0.11932005  0.11940813  0.11938336
  0.119358    0.11913003  0.11887344  0.11859509  0.11845573  0.11842496
  0.11861911  0.11892943  0.11925104  0.11958559  0.11965682  0.1195532
  0.11918339  0.11893797  0.1187387   0.11879148  0.11893913  0.1191347
  0.11951336  0.11981161  0.12004203  0.12005796  0.12022957  0.12022149
  0.1203583   0.12041038  0.12042632  0.12034015  0.12028608  0.1204282
  0.1206949   0.12107674  0.12138972  0.12146427  0.12129447  0.12089368
  0.12028381  0.11966933  0.11912256  0.11887952  0.11868645  0.11872438
  0.11884471  0.11918923  0.11968269  0.12016631  0.12070678  0.12100419
  0.12125265  0.1212673   0.12133794  0.12149848  0.12182966  0.12220101
  0.12260373  0.12300857  0.12340657  0.12378114  0.1239688   0.12412787
  0.12398285  0.12389155  0.12361351  0.12340441  0.12318578  0.12318108
  0.12336766  0.12391967  0.12470812  0.12534402  0.12617691  0.12676822
  0.12745394  0.12799671  0.12842812  0.12870425  0.12889981  0.12913859
  0.1294299   0.12984681  0.1303622   0.13080351  0.13110057  0.13137878
  0.13146351  0.13170245  0.13187061  0.13233665  0.13268882  0.1333123
  0.13396505  0.13455282  0.1351647   0.13565147  0.13631663  0.13687098
  0.13759547  0.13816418  0.13871254  0.13906008  0.13938688  0.1397156
  0.14002843  0.14035852  0.14070465  0.14109698  0.14152244  0.14198545
  0.14218582  0.14224295  0.14201024  0.14207451  0.14217934  0.1425416
  0.14309211  0.1436429   0.14443067  0.14507252  0.14570054  0.14631137
  0.14709006  0.14772461  0.14837447  0.14888017  0.14918956  0.14943436
  0.14968732  0.15006037  0.15048064  0.15081841  0.1510667   0.15120964
  0.15104288  0.15083796  0.15060434  0.15058018  0.1506139   0.15097621
  0.15125905  0.15174684  0.1523027   0.15275358  0.15330231  0.15386277
  0.15465803  0.1550574   0.15541898  0.15566291  0.15598702  0.15637971
  0.15681337  0.157241    0.15767188  0.15808424  0.15844867  0.15884516
  0.15902385  0.15920286  0.15926388  0.1594504   0.1596941   0.15993898
  0.16035253  0.16081819  0.16131115  0.16171573  0.16220102  0.16270007
  0.16339336  0.16396108  0.1644299   0.16471344  0.1648595   0.16492887
  0.16516386  0.16565533  0.1663304   0.16701399  0.16755362  0.16772388
  0.16763653  0.16746822  0.16738157  0.1675444   0.16797413  0.16863073
  0.16938742  0.17011009  0.17062318  0.17114307  0.17171267  0.17227677
  0.172973    0.17342645  0.17379065  0.17400354  0.17415074  0.17424658
  0.17435083  0.17458902  0.1748694   0.17519207  0.17540081  0.17567529
  0.1757636   0.17582771  0.17572427  0.17588022  0.17603363  0.17630035
  0.17678949  0.17748792  0.17832486  0.17892954  0.17959401  0.17983013
  0.17998335  0.1797578   0.17956442  0.17943071  0.17931776  0.17925845
  0.17924371  0.17922147  0.17914744  0.17899278  0.17874835  0.17858732
  0.17834847  0.17823072  0.178075    0.17801015  0.1779465   0.17797956
  0.17802739  0.1781145   0.17811565  0.1779011   0.17763548  0.17724681
  0.17684701  0.1762686   0.17572567  0.175088    0.1744019   0.17370749
  0.17314783  0.17281526  0.17261234  0.17249724  0.17231141  0.1720142
  0.17145956  0.17080493  0.17001894  0.16933222  0.16876493  0.16834253
  0.16804656  0.16763341  0.16717012  0.16656086  0.16608095  0.16555524
  0.16508302  0.16451547  0.16395876  0.1634854   0.1631122   0.16290386
  0.16281363  0.16289414  0.16291308  0.16287057  0.16278622  0.16269258
  0.1624277   0.16217199  0.16165826  0.16116047  0.16053697  0.15988451
  0.15924063  0.158803    0.15832648  0.1577539   0.1573383   0.15680425
  0.15626034  0.15544908  0.15473199  0.15407692  0.1535759   0.153213
  0.15305232  0.1529325   0.15278672  0.15257284  0.15208885  0.15135768
  0.150215    0.14897254  0.14774725  0.14663106  0.14562027  0.1448104
  0.14403066  0.1431229   0.14221293  0.14114635  0.14032109  0.13945897
  0.13871267  0.13785028  0.13704528  0.13630027  0.13562046  0.13512665
  0.13481665  0.13472547  0.13469873  0.13448492  0.13389689  0.13287015
  0.13133205  0.12985216  0.12852858  0.12740417  0.12638363  0.1255849
  0.12479411  0.12426436  0.12381186  0.12319002  0.12251351  0.12147813
  0.1205616   0.11963563  0.11898226  0.11842605  0.11798059  0.11761322
  0.1173472   0.11715323  0.11674476  0.11615957  0.11514833  0.1138228
  0.11205559  0.11031577  0.10853638  0.10678753  0.1049222   0.10308018
  0.10145826  0.10009774  0.09889664  0.09764848  0.09664156  0.09560715
  0.09460654  0.09331146  0.09214032  0.09103928  0.09014336  0.08956092
  0.08917456  0.08883784  0.08827997  0.08733465  0.08592964  0.08422945
  0.08232729  0.08072039  0.07924001  0.07799807  0.07668869  0.07538278
  0.07415298  0.07300412  0.07224796  0.07157455  0.0711657   0.07050768
  0.06976344  0.06874599  0.06773481  0.06673732  0.06605269  0.06557462
  0.0652208   0.06490752  0.06449933  0.06388463  0.06287915  0.06154117
  0.05979358  0.05810976  0.05641174  0.0548401   0.05339232  0.05204095
  0.05072053  0.04935124  0.04805183  0.04687652  0.04600296  0.04527823
  0.04463907  0.04373669  0.0428265   0.0419341   0.04134882  0.04112652
  0.04115298  0.04120561  0.0409303   0.04024304  0.03897898  0.03737845
  0.03553739  0.03388191  0.03209249  0.0304517   0.02857879  0.02683252
  0.02511062  0.02376671  0.02280385  0.02216845  0.02185331  0.0212117
  0.0206244   0.01995557  0.01967955  0.01960033  0.01973948  0.02000709
  0.02036604  0.02056061  0.0204935   0.02029747  0.01959309  0.01859481
  0.01698952  0.01551495  0.0139355   0.01249468  0.01128624  0.01052707
  0.01015268  0.00978437  0.0095275   0.00919979  0.00898088  0.0086847
  0.00849714  0.00808054  0.00758553  0.00707876  0.006462    0.00600393
  0.00590651  0.00598166  0.00620158  0.00615767  0.00554271  0.00436038
  0.00272575  0.00144787  0.00028552 -0.00058437 -0.00153661 -0.0024908
 -0.00367719 -0.00473728 -0.00524821 -0.00558168 -0.00540813 -0.00563209
 -0.00585662 -0.00626166 -0.00645602 -0.00663196 -0.00660993 -0.00657835
 -0.00642202 -0.00623721 -0.00623302 -0.00621917 -0.00652239 -0.00714982
 -0.00832003 -0.00969789 -0.01148922 -0.01334199 -0.01501979 -0.01605177
 -0.01642238 -0.01637066 -0.01614668 -0.01606577 -0.01596656 -0.01618858
 -0.01600875 -0.0160506  -0.01584113 -0.01590713 -0.01606644 -0.01618657
 -0.01605165 -0.01557717 -0.01503627 -0.01445758 -0.01473052 -0.01568353
 -0.01735925 -0.01879683 -0.0200003  -0.02079793 -0.02188634 -0.02303476
 -0.02424232 -0.02522809 -0.02569227 -0.02590345 -0.02552631 -0.02525641
 -0.02473679 -0.02503772 -0.0252813  -0.02576028 -0.0259656  -0.02573921
 -0.02548475 -0.02548232 -0.02558153 -0.02486883 -0.02519697 -0.02723246]
