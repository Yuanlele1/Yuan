Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_192_FITS_ETTh2_ftM_sl90_ll48_pl192_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8359
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=18, out_features=56, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  903168.0
params:  1064.0
Trainable parameters:  1064
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.482248067855835
Epoch: 1, Steps: 65 | Train Loss: 0.7708386 Vali Loss: 0.3721347 Test Loss: 0.5225337
Validation loss decreased (inf --> 0.372135).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5164823532104492
Epoch: 2, Steps: 65 | Train Loss: 0.6752946 Vali Loss: 0.3356499 Test Loss: 0.4728152
Validation loss decreased (0.372135 --> 0.335650).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.9166743755340576
Epoch: 3, Steps: 65 | Train Loss: 0.6237371 Vali Loss: 0.3163170 Test Loss: 0.4464142
Validation loss decreased (0.335650 --> 0.316317).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6726632118225098
Epoch: 4, Steps: 65 | Train Loss: 0.6024271 Vali Loss: 0.3052175 Test Loss: 0.4314237
Validation loss decreased (0.316317 --> 0.305218).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.975127935409546
Epoch: 5, Steps: 65 | Train Loss: 0.5861857 Vali Loss: 0.2983046 Test Loss: 0.4220627
Validation loss decreased (0.305218 --> 0.298305).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.710853099822998
Epoch: 6, Steps: 65 | Train Loss: 0.5768732 Vali Loss: 0.2938688 Test Loss: 0.4161122
Validation loss decreased (0.298305 --> 0.293869).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.3351664543151855
Epoch: 7, Steps: 65 | Train Loss: 0.5676737 Vali Loss: 0.2906621 Test Loss: 0.4119450
Validation loss decreased (0.293869 --> 0.290662).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.604846715927124
Epoch: 8, Steps: 65 | Train Loss: 0.5638882 Vali Loss: 0.2883909 Test Loss: 0.4090587
Validation loss decreased (0.290662 --> 0.288391).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.0174953937530518
Epoch: 9, Steps: 65 | Train Loss: 0.5603131 Vali Loss: 0.2866723 Test Loss: 0.4068941
Validation loss decreased (0.288391 --> 0.286672).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.008305072784424
Epoch: 10, Steps: 65 | Train Loss: 0.5590813 Vali Loss: 0.2853268 Test Loss: 0.4052841
Validation loss decreased (0.286672 --> 0.285327).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.0915098190307617
Epoch: 11, Steps: 65 | Train Loss: 0.5560754 Vali Loss: 0.2842794 Test Loss: 0.4040643
Validation loss decreased (0.285327 --> 0.284279).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.713953971862793
Epoch: 12, Steps: 65 | Train Loss: 0.5541807 Vali Loss: 0.2833590 Test Loss: 0.4029616
Validation loss decreased (0.284279 --> 0.283359).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.3397231101989746
Epoch: 13, Steps: 65 | Train Loss: 0.5530123 Vali Loss: 0.2826380 Test Loss: 0.4021614
Validation loss decreased (0.283359 --> 0.282638).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6902148723602295
Epoch: 14, Steps: 65 | Train Loss: 0.5535561 Vali Loss: 0.2820338 Test Loss: 0.4015503
Validation loss decreased (0.282638 --> 0.282034).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.97029709815979
Epoch: 15, Steps: 65 | Train Loss: 0.5525665 Vali Loss: 0.2814802 Test Loss: 0.4010242
Validation loss decreased (0.282034 --> 0.281480).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.944042444229126
Epoch: 16, Steps: 65 | Train Loss: 0.5494287 Vali Loss: 0.2810627 Test Loss: 0.4006056
Validation loss decreased (0.281480 --> 0.281063).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.704685926437378
Epoch: 17, Steps: 65 | Train Loss: 0.5493843 Vali Loss: 0.2806901 Test Loss: 0.4002542
Validation loss decreased (0.281063 --> 0.280690).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.734541416168213
Epoch: 18, Steps: 65 | Train Loss: 0.5495481 Vali Loss: 0.2803325 Test Loss: 0.3999618
Validation loss decreased (0.280690 --> 0.280333).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.937898874282837
Epoch: 19, Steps: 65 | Train Loss: 0.5493699 Vali Loss: 0.2800476 Test Loss: 0.3997204
Validation loss decreased (0.280333 --> 0.280048).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.576653242111206
Epoch: 20, Steps: 65 | Train Loss: 0.5473862 Vali Loss: 0.2797697 Test Loss: 0.3994706
Validation loss decreased (0.280048 --> 0.279770).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.6862094402313232
Epoch: 21, Steps: 65 | Train Loss: 0.5483297 Vali Loss: 0.2795052 Test Loss: 0.3992882
Validation loss decreased (0.279770 --> 0.279505).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.6076068878173828
Epoch: 22, Steps: 65 | Train Loss: 0.5457560 Vali Loss: 0.2792703 Test Loss: 0.3991127
Validation loss decreased (0.279505 --> 0.279270).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.5476322174072266
Epoch: 23, Steps: 65 | Train Loss: 0.5472356 Vali Loss: 0.2791045 Test Loss: 0.3989722
Validation loss decreased (0.279270 --> 0.279105).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.0075066089630127
Epoch: 24, Steps: 65 | Train Loss: 0.5449252 Vali Loss: 0.2785669 Test Loss: 0.3987923
Validation loss decreased (0.279105 --> 0.278567).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9192638397216797
Epoch: 25, Steps: 65 | Train Loss: 0.5455645 Vali Loss: 0.2787433 Test Loss: 0.3987266
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.859248399734497
Epoch: 26, Steps: 65 | Train Loss: 0.5467578 Vali Loss: 0.2785628 Test Loss: 0.3986219
Validation loss decreased (0.278567 --> 0.278563).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1411023139953613
Epoch: 27, Steps: 65 | Train Loss: 0.5461676 Vali Loss: 0.2783933 Test Loss: 0.3985264
Validation loss decreased (0.278563 --> 0.278393).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.0640366077423096
Epoch: 28, Steps: 65 | Train Loss: 0.5454436 Vali Loss: 0.2782652 Test Loss: 0.3984425
Validation loss decreased (0.278393 --> 0.278265).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.098825454711914
Epoch: 29, Steps: 65 | Train Loss: 0.5451286 Vali Loss: 0.2781851 Test Loss: 0.3983645
Validation loss decreased (0.278265 --> 0.278185).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.54044771194458
Epoch: 30, Steps: 65 | Train Loss: 0.5450804 Vali Loss: 0.2780688 Test Loss: 0.3982899
Validation loss decreased (0.278185 --> 0.278069).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.0917105674743652
Epoch: 31, Steps: 65 | Train Loss: 0.5437794 Vali Loss: 0.2777078 Test Loss: 0.3982427
Validation loss decreased (0.278069 --> 0.277708).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.996415376663208
Epoch: 32, Steps: 65 | Train Loss: 0.5429869 Vali Loss: 0.2778621 Test Loss: 0.3981792
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4840271472930908
Epoch: 33, Steps: 65 | Train Loss: 0.5450194 Vali Loss: 0.2778350 Test Loss: 0.3981326
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7212855815887451
Epoch: 34, Steps: 65 | Train Loss: 0.5443934 Vali Loss: 0.2776858 Test Loss: 0.3980770
Validation loss decreased (0.277708 --> 0.277686).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.701017141342163
Epoch: 35, Steps: 65 | Train Loss: 0.5446370 Vali Loss: 0.2776856 Test Loss: 0.3980565
Validation loss decreased (0.277686 --> 0.277686).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.723808765411377
Epoch: 36, Steps: 65 | Train Loss: 0.5442256 Vali Loss: 0.2775651 Test Loss: 0.3980050
Validation loss decreased (0.277686 --> 0.277565).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.0708565711975098
Epoch: 37, Steps: 65 | Train Loss: 0.5448886 Vali Loss: 0.2774138 Test Loss: 0.3979564
Validation loss decreased (0.277565 --> 0.277414).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.2870688438415527
Epoch: 38, Steps: 65 | Train Loss: 0.5441974 Vali Loss: 0.2773981 Test Loss: 0.3979328
Validation loss decreased (0.277414 --> 0.277398).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.1044886112213135
Epoch: 39, Steps: 65 | Train Loss: 0.5429213 Vali Loss: 0.2773937 Test Loss: 0.3978988
Validation loss decreased (0.277398 --> 0.277394).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7397427558898926
Epoch: 40, Steps: 65 | Train Loss: 0.5444785 Vali Loss: 0.2773654 Test Loss: 0.3978687
Validation loss decreased (0.277394 --> 0.277365).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.263998031616211
Epoch: 41, Steps: 65 | Train Loss: 0.5440404 Vali Loss: 0.2772667 Test Loss: 0.3978464
Validation loss decreased (0.277365 --> 0.277267).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.9360172748565674
Epoch: 42, Steps: 65 | Train Loss: 0.5435403 Vali Loss: 0.2769216 Test Loss: 0.3978306
Validation loss decreased (0.277267 --> 0.276922).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.8774874210357666
Epoch: 43, Steps: 65 | Train Loss: 0.5388245 Vali Loss: 0.2771057 Test Loss: 0.3978048
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.8989334106445312
Epoch: 44, Steps: 65 | Train Loss: 0.5431320 Vali Loss: 0.2770809 Test Loss: 0.3977846
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.7845566272735596
Epoch: 45, Steps: 65 | Train Loss: 0.5428962 Vali Loss: 0.2771126 Test Loss: 0.3977607
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.439723491668701
Epoch: 46, Steps: 65 | Train Loss: 0.5434751 Vali Loss: 0.2770872 Test Loss: 0.3977494
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.883021354675293
Epoch: 47, Steps: 65 | Train Loss: 0.5427044 Vali Loss: 0.2770566 Test Loss: 0.3977231
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.313416004180908
Epoch: 48, Steps: 65 | Train Loss: 0.5439701 Vali Loss: 0.2769744 Test Loss: 0.3977165
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.810054063796997
Epoch: 49, Steps: 65 | Train Loss: 0.5440083 Vali Loss: 0.2769841 Test Loss: 0.3977007
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.837369680404663
Epoch: 50, Steps: 65 | Train Loss: 0.5410401 Vali Loss: 0.2769085 Test Loss: 0.3976916
Validation loss decreased (0.276922 --> 0.276908).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9706144332885742
Epoch: 51, Steps: 65 | Train Loss: 0.5429244 Vali Loss: 0.2769268 Test Loss: 0.3976750
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.2001852989196777
Epoch: 52, Steps: 65 | Train Loss: 0.5431793 Vali Loss: 0.2768190 Test Loss: 0.3976683
Validation loss decreased (0.276908 --> 0.276819).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.917252540588379
Epoch: 53, Steps: 65 | Train Loss: 0.5420200 Vali Loss: 0.2767766 Test Loss: 0.3976541
Validation loss decreased (0.276819 --> 0.276777).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.8086910247802734
Epoch: 54, Steps: 65 | Train Loss: 0.5421865 Vali Loss: 0.2767993 Test Loss: 0.3976488
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.305971145629883
Epoch: 55, Steps: 65 | Train Loss: 0.5424408 Vali Loss: 0.2768154 Test Loss: 0.3976301
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.2033586502075195
Epoch: 56, Steps: 65 | Train Loss: 0.5423913 Vali Loss: 0.2767209 Test Loss: 0.3976234
Validation loss decreased (0.276777 --> 0.276721).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.9539141654968262
Epoch: 57, Steps: 65 | Train Loss: 0.5410579 Vali Loss: 0.2767718 Test Loss: 0.3976244
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.4682223796844482
Epoch: 58, Steps: 65 | Train Loss: 0.5421226 Vali Loss: 0.2766408 Test Loss: 0.3976091
Validation loss decreased (0.276721 --> 0.276641).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.575495481491089
Epoch: 59, Steps: 65 | Train Loss: 0.5432056 Vali Loss: 0.2766853 Test Loss: 0.3976052
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.2937707901000977
Epoch: 60, Steps: 65 | Train Loss: 0.5391695 Vali Loss: 0.2767419 Test Loss: 0.3975949
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.8364663124084473
Epoch: 61, Steps: 65 | Train Loss: 0.5430255 Vali Loss: 0.2765993 Test Loss: 0.3975908
Validation loss decreased (0.276641 --> 0.276599).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.1122748851776123
Epoch: 62, Steps: 65 | Train Loss: 0.5432272 Vali Loss: 0.2765505 Test Loss: 0.3975855
Validation loss decreased (0.276599 --> 0.276550).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.3548965454101562
Epoch: 63, Steps: 65 | Train Loss: 0.5411756 Vali Loss: 0.2764977 Test Loss: 0.3975787
Validation loss decreased (0.276550 --> 0.276498).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.3163793087005615
Epoch: 64, Steps: 65 | Train Loss: 0.5430253 Vali Loss: 0.2766686 Test Loss: 0.3975755
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.157522201538086
Epoch: 65, Steps: 65 | Train Loss: 0.5423195 Vali Loss: 0.2766096 Test Loss: 0.3975741
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.6133606433868408
Epoch: 66, Steps: 65 | Train Loss: 0.5428336 Vali Loss: 0.2766349 Test Loss: 0.3975659
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.442732810974121
Epoch: 67, Steps: 65 | Train Loss: 0.5424726 Vali Loss: 0.2766123 Test Loss: 0.3975579
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.0093467235565186
Epoch: 68, Steps: 65 | Train Loss: 0.5422650 Vali Loss: 0.2766219 Test Loss: 0.3975553
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.7205901145935059
Epoch: 69, Steps: 65 | Train Loss: 0.5410080 Vali Loss: 0.2766286 Test Loss: 0.3975517
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9017813205718994
Epoch: 70, Steps: 65 | Train Loss: 0.5413443 Vali Loss: 0.2764925 Test Loss: 0.3975465
Validation loss decreased (0.276498 --> 0.276492).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.2303950786590576
Epoch: 71, Steps: 65 | Train Loss: 0.5416878 Vali Loss: 0.2764827 Test Loss: 0.3975458
Validation loss decreased (0.276492 --> 0.276483).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.1418278217315674
Epoch: 72, Steps: 65 | Train Loss: 0.5426867 Vali Loss: 0.2765609 Test Loss: 0.3975395
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.7667632102966309
Epoch: 73, Steps: 65 | Train Loss: 0.5420820 Vali Loss: 0.2765497 Test Loss: 0.3975362
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.22562575340271
Epoch: 74, Steps: 65 | Train Loss: 0.5418587 Vali Loss: 0.2764857 Test Loss: 0.3975376
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.9127476215362549
Epoch: 75, Steps: 65 | Train Loss: 0.5407882 Vali Loss: 0.2763919 Test Loss: 0.3975334
Validation loss decreased (0.276483 --> 0.276392).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.074690580368042
Epoch: 76, Steps: 65 | Train Loss: 0.5404104 Vali Loss: 0.2765473 Test Loss: 0.3975293
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.4095468521118164
Epoch: 77, Steps: 65 | Train Loss: 0.5420760 Vali Loss: 0.2764934 Test Loss: 0.3975258
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.056459665298462
Epoch: 78, Steps: 65 | Train Loss: 0.5419834 Vali Loss: 0.2765570 Test Loss: 0.3975245
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.178680896759033
Epoch: 79, Steps: 65 | Train Loss: 0.5419065 Vali Loss: 0.2765316 Test Loss: 0.3975232
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.6265590190887451
Epoch: 80, Steps: 65 | Train Loss: 0.5416269 Vali Loss: 0.2764136 Test Loss: 0.3975222
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.67268967628479
Epoch: 81, Steps: 65 | Train Loss: 0.5423857 Vali Loss: 0.2765347 Test Loss: 0.3975187
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.9365253448486328
Epoch: 82, Steps: 65 | Train Loss: 0.5410346 Vali Loss: 0.2765282 Test Loss: 0.3975166
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.7583060264587402
Epoch: 83, Steps: 65 | Train Loss: 0.5414261 Vali Loss: 0.2764420 Test Loss: 0.3975172
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.330686092376709
Epoch: 84, Steps: 65 | Train Loss: 0.5417206 Vali Loss: 0.2765191 Test Loss: 0.3975134
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.921522855758667
Epoch: 85, Steps: 65 | Train Loss: 0.5429268 Vali Loss: 0.2764821 Test Loss: 0.3975120
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.8322837352752686
Epoch: 86, Steps: 65 | Train Loss: 0.5423179 Vali Loss: 0.2765153 Test Loss: 0.3975117
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.9192230701446533
Epoch: 87, Steps: 65 | Train Loss: 0.5406617 Vali Loss: 0.2765075 Test Loss: 0.3975106
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.5710461139678955
Epoch: 88, Steps: 65 | Train Loss: 0.5421363 Vali Loss: 0.2763909 Test Loss: 0.3975096
Validation loss decreased (0.276392 --> 0.276391).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 3.2458648681640625
Epoch: 89, Steps: 65 | Train Loss: 0.5426575 Vali Loss: 0.2764862 Test Loss: 0.3975076
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.8359334468841553
Epoch: 90, Steps: 65 | Train Loss: 0.5417919 Vali Loss: 0.2764636 Test Loss: 0.3975059
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 3.0874266624450684
Epoch: 91, Steps: 65 | Train Loss: 0.5414632 Vali Loss: 0.2764130 Test Loss: 0.3975059
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 3.00364089012146
Epoch: 92, Steps: 65 | Train Loss: 0.5411248 Vali Loss: 0.2764532 Test Loss: 0.3975036
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.524137258529663
Epoch: 93, Steps: 65 | Train Loss: 0.5423068 Vali Loss: 0.2764539 Test Loss: 0.3975035
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.940460205078125
Epoch: 94, Steps: 65 | Train Loss: 0.5415148 Vali Loss: 0.2764615 Test Loss: 0.3975025
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.7918694019317627
Epoch: 95, Steps: 65 | Train Loss: 0.5384556 Vali Loss: 0.2764641 Test Loss: 0.3975019
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.2857189178466797
Epoch: 96, Steps: 65 | Train Loss: 0.5411376 Vali Loss: 0.2764843 Test Loss: 0.3975005
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.328993797302246
Epoch: 97, Steps: 65 | Train Loss: 0.5411792 Vali Loss: 0.2764236 Test Loss: 0.3974999
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.1120970249176025
Epoch: 98, Steps: 65 | Train Loss: 0.5398901 Vali Loss: 0.2764448 Test Loss: 0.3974999
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.5688121318817139
Epoch: 99, Steps: 65 | Train Loss: 0.5424418 Vali Loss: 0.2764624 Test Loss: 0.3974989
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.4522547721862793
Epoch: 100, Steps: 65 | Train Loss: 0.5416313 Vali Loss: 0.2763855 Test Loss: 0.3974983
Validation loss decreased (0.276391 --> 0.276385).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_90_192_FITS_ETTh2_ftM_sl90_ll48_pl192_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.37940120697021484, mae:0.3919896185398102, rse:0.4939582049846649, corr:[0.26729122 0.2705446  0.26837084 0.26593322 0.26531628 0.26519063
 0.26426935 0.26275647 0.26186228 0.2612805  0.26018938 0.25825316
 0.25624952 0.2550377  0.2543806  0.2538257  0.25314727 0.2524452
 0.2516683  0.25058    0.24929783 0.24789403 0.24633041 0.24431232
 0.24152409 0.23892789 0.23648931 0.23457228 0.23314108 0.23187801
 0.23073272 0.22907205 0.22769755 0.22659257 0.22575334 0.2245242
 0.2229983  0.22177267 0.2208135  0.22002903 0.2195051  0.2191516
 0.2186737  0.21765745 0.21656363 0.21544738 0.2138643  0.21139352
 0.2078786  0.20505632 0.20273502 0.2007649  0.19885503 0.19683628
 0.19497843 0.19264217 0.19123454 0.19023025 0.18952246 0.18837948
 0.187288   0.18710056 0.18720035 0.18695642 0.18636125 0.18586114
 0.18551603 0.18485647 0.18410842 0.18329938 0.18206735 0.18036778
 0.17785093 0.17611387 0.17465478 0.17344938 0.17252056 0.17204563
 0.17183559 0.17075887 0.17015205 0.17001802 0.17012516 0.16986895
 0.16927303 0.16902599 0.16905956 0.16906343 0.1686907  0.16812123
 0.16760407 0.16702497 0.16673437 0.16630815 0.16530935 0.16365768
 0.16120222 0.15925659 0.1575262  0.15599121 0.15467344 0.15375848
 0.15375328 0.15320088 0.15284547 0.15265842 0.15275891 0.15267996
 0.15204258 0.15148224 0.15103103 0.15080526 0.15055771 0.15012778
 0.14967611 0.14896958 0.14831144 0.14740798 0.14570017 0.143215
 0.14033854 0.13830788 0.1366633  0.13543104 0.13410969 0.13307762
 0.13272826 0.13224965 0.1320507  0.13183224 0.1316677  0.13121553
 0.13066705 0.13029349 0.12996846 0.1295716  0.1290904  0.12858948
 0.12802748 0.12731007 0.12683347 0.12635505 0.12495134 0.1224455
 0.11904775 0.11668765 0.11488725 0.11335231 0.11194086 0.11086685
 0.11068162 0.1101798  0.11010331 0.11007487 0.11022457 0.11009058
 0.10995443 0.1101619  0.11023005 0.11008261 0.10973646 0.10940415
 0.10917685 0.10870454 0.10852452 0.1084491  0.10783885 0.10595609
 0.10270143 0.10061852 0.09950932 0.09887099 0.09797584 0.09698931
 0.09680503 0.09637456 0.09644087 0.09651883 0.09667982 0.09611719
 0.09540477 0.09536741 0.09531434 0.0950651  0.0942143  0.09363551
 0.09364593 0.09327597 0.09343132 0.095185   0.09732261 0.0962989 ]
