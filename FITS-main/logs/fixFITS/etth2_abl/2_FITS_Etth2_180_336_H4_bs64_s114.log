Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=42, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4515840.0
params:  5160.0
Trainable parameters:  5160
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.5096287727355957
Epoch: 1, Steps: 63 | Train Loss: 0.7386287 Vali Loss: 0.4872997 Test Loss: 0.5262190
Validation loss decreased (inf --> 0.487300).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.282315969467163
Epoch: 2, Steps: 63 | Train Loss: 0.6152836 Vali Loss: 0.4515270 Test Loss: 0.4786720
Validation loss decreased (0.487300 --> 0.451527).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.5118212699890137
Epoch: 3, Steps: 63 | Train Loss: 0.5464884 Vali Loss: 0.4263864 Test Loss: 0.4521348
Validation loss decreased (0.451527 --> 0.426386).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.1295719146728516
Epoch: 4, Steps: 63 | Train Loss: 0.5054586 Vali Loss: 0.4107073 Test Loss: 0.4362970
Validation loss decreased (0.426386 --> 0.410707).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.4698286056518555
Epoch: 5, Steps: 63 | Train Loss: 0.4807944 Vali Loss: 0.4023376 Test Loss: 0.4261801
Validation loss decreased (0.410707 --> 0.402338).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.9680070877075195
Epoch: 6, Steps: 63 | Train Loss: 0.4637979 Vali Loss: 0.3961219 Test Loss: 0.4197764
Validation loss decreased (0.402338 --> 0.396122).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.7746188640594482
Epoch: 7, Steps: 63 | Train Loss: 0.4539614 Vali Loss: 0.3935041 Test Loss: 0.4152398
Validation loss decreased (0.396122 --> 0.393504).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.265308141708374
Epoch: 8, Steps: 63 | Train Loss: 0.4460802 Vali Loss: 0.3880160 Test Loss: 0.4120887
Validation loss decreased (0.393504 --> 0.388016).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.870065927505493
Epoch: 9, Steps: 63 | Train Loss: 0.4395238 Vali Loss: 0.3838359 Test Loss: 0.4097963
Validation loss decreased (0.388016 --> 0.383836).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.4315340518951416
Epoch: 10, Steps: 63 | Train Loss: 0.4355406 Vali Loss: 0.3839524 Test Loss: 0.4079967
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.843304395675659
Epoch: 11, Steps: 63 | Train Loss: 0.4324864 Vali Loss: 0.3815785 Test Loss: 0.4065562
Validation loss decreased (0.383836 --> 0.381579).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.076546669006348
Epoch: 12, Steps: 63 | Train Loss: 0.4301641 Vali Loss: 0.3812795 Test Loss: 0.4054942
Validation loss decreased (0.381579 --> 0.381280).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.5462210178375244
Epoch: 13, Steps: 63 | Train Loss: 0.4281566 Vali Loss: 0.3796678 Test Loss: 0.4045488
Validation loss decreased (0.381280 --> 0.379668).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.0303609371185303
Epoch: 14, Steps: 63 | Train Loss: 0.4263057 Vali Loss: 0.3789173 Test Loss: 0.4037304
Validation loss decreased (0.379668 --> 0.378917).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.1758129596710205
Epoch: 15, Steps: 63 | Train Loss: 0.4264318 Vali Loss: 0.3779964 Test Loss: 0.4029823
Validation loss decreased (0.378917 --> 0.377996).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.6943554878234863
Epoch: 16, Steps: 63 | Train Loss: 0.4243543 Vali Loss: 0.3774155 Test Loss: 0.4022894
Validation loss decreased (0.377996 --> 0.377415).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.5792293548583984
Epoch: 17, Steps: 63 | Train Loss: 0.4234970 Vali Loss: 0.3768104 Test Loss: 0.4017935
Validation loss decreased (0.377415 --> 0.376810).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.519270420074463
Epoch: 18, Steps: 63 | Train Loss: 0.4223574 Vali Loss: 0.3743049 Test Loss: 0.4012421
Validation loss decreased (0.376810 --> 0.374305).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.2052345275878906
Epoch: 19, Steps: 63 | Train Loss: 0.4227378 Vali Loss: 0.3767225 Test Loss: 0.4008889
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.2168259620666504
Epoch: 20, Steps: 63 | Train Loss: 0.4205159 Vali Loss: 0.3730713 Test Loss: 0.4004391
Validation loss decreased (0.374305 --> 0.373071).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.6242234706878662
Epoch: 21, Steps: 63 | Train Loss: 0.4200199 Vali Loss: 0.3726342 Test Loss: 0.4001506
Validation loss decreased (0.373071 --> 0.372634).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0412280559539795
Epoch: 22, Steps: 63 | Train Loss: 0.4197607 Vali Loss: 0.3719231 Test Loss: 0.3997660
Validation loss decreased (0.372634 --> 0.371923).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.94623064994812
Epoch: 23, Steps: 63 | Train Loss: 0.4194073 Vali Loss: 0.3741890 Test Loss: 0.3995369
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.70819354057312
Epoch: 24, Steps: 63 | Train Loss: 0.4197352 Vali Loss: 0.3701025 Test Loss: 0.3991774
Validation loss decreased (0.371923 --> 0.370102).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.3369243144989014
Epoch: 25, Steps: 63 | Train Loss: 0.4185131 Vali Loss: 0.3707238 Test Loss: 0.3989497
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8175890445709229
Epoch: 26, Steps: 63 | Train Loss: 0.4180069 Vali Loss: 0.3714338 Test Loss: 0.3987172
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.618670701980591
Epoch: 27, Steps: 63 | Train Loss: 0.4188209 Vali Loss: 0.3700523 Test Loss: 0.3985303
Validation loss decreased (0.370102 --> 0.370052).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.20001220703125
Epoch: 28, Steps: 63 | Train Loss: 0.4182004 Vali Loss: 0.3721230 Test Loss: 0.3983315
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.475672721862793
Epoch: 29, Steps: 63 | Train Loss: 0.4173583 Vali Loss: 0.3718636 Test Loss: 0.3982000
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.4316015243530273
Epoch: 30, Steps: 63 | Train Loss: 0.4168890 Vali Loss: 0.3704103 Test Loss: 0.3980702
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.747030019760132
Epoch: 31, Steps: 63 | Train Loss: 0.4176588 Vali Loss: 0.3707200 Test Loss: 0.3978898
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.8782286643981934
Epoch: 32, Steps: 63 | Train Loss: 0.4171325 Vali Loss: 0.3701842 Test Loss: 0.3977717
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.6785295009613037
Epoch: 33, Steps: 63 | Train Loss: 0.4162562 Vali Loss: 0.3685984 Test Loss: 0.3975908
Validation loss decreased (0.370052 --> 0.368598).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.3526687622070312
Epoch: 34, Steps: 63 | Train Loss: 0.4158065 Vali Loss: 0.3699431 Test Loss: 0.3975085
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.860487937927246
Epoch: 35, Steps: 63 | Train Loss: 0.4163017 Vali Loss: 0.3678503 Test Loss: 0.3973979
Validation loss decreased (0.368598 --> 0.367850).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.564554214477539
Epoch: 36, Steps: 63 | Train Loss: 0.4167089 Vali Loss: 0.3723991 Test Loss: 0.3973206
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.52856183052063
Epoch: 37, Steps: 63 | Train Loss: 0.4172073 Vali Loss: 0.3695371 Test Loss: 0.3972169
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6392946243286133
Epoch: 38, Steps: 63 | Train Loss: 0.4162280 Vali Loss: 0.3663357 Test Loss: 0.3971387
Validation loss decreased (0.367850 --> 0.366336).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.39154314994812
Epoch: 39, Steps: 63 | Train Loss: 0.4163353 Vali Loss: 0.3731427 Test Loss: 0.3970627
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.2317514419555664
Epoch: 40, Steps: 63 | Train Loss: 0.4153568 Vali Loss: 0.3698452 Test Loss: 0.3969748
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.381094217300415
Epoch: 41, Steps: 63 | Train Loss: 0.4162222 Vali Loss: 0.3705189 Test Loss: 0.3969078
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.32065486907959
Epoch: 42, Steps: 63 | Train Loss: 0.4155009 Vali Loss: 0.3719181 Test Loss: 0.3968135
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.4898436069488525
Epoch: 43, Steps: 63 | Train Loss: 0.4149336 Vali Loss: 0.3699076 Test Loss: 0.3968137
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.511350393295288
Epoch: 44, Steps: 63 | Train Loss: 0.4152674 Vali Loss: 0.3713956 Test Loss: 0.3967326
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.204336404800415
Epoch: 45, Steps: 63 | Train Loss: 0.4151710 Vali Loss: 0.3694398 Test Loss: 0.3966847
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.122223138809204
Epoch: 46, Steps: 63 | Train Loss: 0.4155314 Vali Loss: 0.3677463 Test Loss: 0.3966164
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.512206792831421
Epoch: 47, Steps: 63 | Train Loss: 0.4132337 Vali Loss: 0.3714901 Test Loss: 0.3965893
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.803537607192993
Epoch: 48, Steps: 63 | Train Loss: 0.4151954 Vali Loss: 0.3694568 Test Loss: 0.3965127
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.439636707305908
Epoch: 49, Steps: 63 | Train Loss: 0.4154472 Vali Loss: 0.3688338 Test Loss: 0.3964670
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.0917727947235107
Epoch: 50, Steps: 63 | Train Loss: 0.4149473 Vali Loss: 0.3702695 Test Loss: 0.3964322
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.1490659713745117
Epoch: 51, Steps: 63 | Train Loss: 0.4151947 Vali Loss: 0.3673998 Test Loss: 0.3963880
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.6531713008880615
Epoch: 52, Steps: 63 | Train Loss: 0.4151464 Vali Loss: 0.3684766 Test Loss: 0.3963655
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.2027297019958496
Epoch: 53, Steps: 63 | Train Loss: 0.4155269 Vali Loss: 0.3687088 Test Loss: 0.3963393
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.200448751449585
Epoch: 54, Steps: 63 | Train Loss: 0.4157126 Vali Loss: 0.3675379 Test Loss: 0.3962983
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.5051584243774414
Epoch: 55, Steps: 63 | Train Loss: 0.4140613 Vali Loss: 0.3708516 Test Loss: 0.3962635
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.2039542198181152
Epoch: 56, Steps: 63 | Train Loss: 0.4138541 Vali Loss: 0.3666129 Test Loss: 0.3962204
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.9548368453979492
Epoch: 57, Steps: 63 | Train Loss: 0.4155597 Vali Loss: 0.3670705 Test Loss: 0.3962076
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.008697509765625
Epoch: 58, Steps: 63 | Train Loss: 0.4158583 Vali Loss: 0.3690706 Test Loss: 0.3961694
EarlyStopping counter: 20 out of 20
Early stopping
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=42, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4515840.0
params:  5160.0
Trainable parameters:  5160
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7325668334960938
Epoch: 1, Steps: 63 | Train Loss: 0.6230328 Vali Loss: 0.3655659 Test Loss: 0.3955640
Validation loss decreased (inf --> 0.365566).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.187183380126953
Epoch: 2, Steps: 63 | Train Loss: 0.6237382 Vali Loss: 0.3676064 Test Loss: 0.3947023
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.5568807125091553
Epoch: 3, Steps: 63 | Train Loss: 0.6221761 Vali Loss: 0.3661529 Test Loss: 0.3941535
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.9044878482818604
Epoch: 4, Steps: 63 | Train Loss: 0.6214705 Vali Loss: 0.3667822 Test Loss: 0.3938355
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.0929503440856934
Epoch: 5, Steps: 63 | Train Loss: 0.6196635 Vali Loss: 0.3655482 Test Loss: 0.3934436
Validation loss decreased (0.365566 --> 0.365548).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.940634250640869
Epoch: 6, Steps: 63 | Train Loss: 0.6191713 Vali Loss: 0.3659094 Test Loss: 0.3931183
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.2586960792541504
Epoch: 7, Steps: 63 | Train Loss: 0.6212258 Vali Loss: 0.3659489 Test Loss: 0.3931038
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.389965534210205
Epoch: 8, Steps: 63 | Train Loss: 0.6190442 Vali Loss: 0.3656106 Test Loss: 0.3929048
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.1417806148529053
Epoch: 9, Steps: 63 | Train Loss: 0.6196785 Vali Loss: 0.3648063 Test Loss: 0.3928585
Validation loss decreased (0.365548 --> 0.364806).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.9694197177886963
Epoch: 10, Steps: 63 | Train Loss: 0.6179199 Vali Loss: 0.3656588 Test Loss: 0.3928303
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.3166446685791016
Epoch: 11, Steps: 63 | Train Loss: 0.6195856 Vali Loss: 0.3668073 Test Loss: 0.3926108
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.2516825199127197
Epoch: 12, Steps: 63 | Train Loss: 0.6185320 Vali Loss: 0.3681613 Test Loss: 0.3926612
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.218433141708374
Epoch: 13, Steps: 63 | Train Loss: 0.6179039 Vali Loss: 0.3653777 Test Loss: 0.3926528
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.1167850494384766
Epoch: 14, Steps: 63 | Train Loss: 0.6188058 Vali Loss: 0.3651799 Test Loss: 0.3926495
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.324573278427124
Epoch: 15, Steps: 63 | Train Loss: 0.6189557 Vali Loss: 0.3647473 Test Loss: 0.3926261
Validation loss decreased (0.364806 --> 0.364747).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.2079169750213623
Epoch: 16, Steps: 63 | Train Loss: 0.6187448 Vali Loss: 0.3629436 Test Loss: 0.3926986
Validation loss decreased (0.364747 --> 0.362944).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.066774845123291
Epoch: 17, Steps: 63 | Train Loss: 0.6175908 Vali Loss: 0.3630253 Test Loss: 0.3924807
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.52820086479187
Epoch: 18, Steps: 63 | Train Loss: 0.6151325 Vali Loss: 0.3659501 Test Loss: 0.3925477
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.278806447982788
Epoch: 19, Steps: 63 | Train Loss: 0.6181583 Vali Loss: 0.3648787 Test Loss: 0.3924707
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.4374232292175293
Epoch: 20, Steps: 63 | Train Loss: 0.6171543 Vali Loss: 0.3622341 Test Loss: 0.3923626
Validation loss decreased (0.362944 --> 0.362234).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.298138380050659
Epoch: 21, Steps: 63 | Train Loss: 0.6178766 Vali Loss: 0.3640263 Test Loss: 0.3924221
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.30131459236145
Epoch: 22, Steps: 63 | Train Loss: 0.6173140 Vali Loss: 0.3640474 Test Loss: 0.3923705
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.3661041259765625
Epoch: 23, Steps: 63 | Train Loss: 0.6169934 Vali Loss: 0.3611167 Test Loss: 0.3924651
Validation loss decreased (0.362234 --> 0.361117).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.4645543098449707
Epoch: 24, Steps: 63 | Train Loss: 0.6175681 Vali Loss: 0.3613983 Test Loss: 0.3924984
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.191256284713745
Epoch: 25, Steps: 63 | Train Loss: 0.6181934 Vali Loss: 0.3618794 Test Loss: 0.3924181
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.1205241680145264
Epoch: 26, Steps: 63 | Train Loss: 0.6179625 Vali Loss: 0.3653201 Test Loss: 0.3924075
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.2146503925323486
Epoch: 27, Steps: 63 | Train Loss: 0.6172346 Vali Loss: 0.3647839 Test Loss: 0.3923644
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.840557336807251
Epoch: 28, Steps: 63 | Train Loss: 0.6165485 Vali Loss: 0.3632410 Test Loss: 0.3923532
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.0766468048095703
Epoch: 29, Steps: 63 | Train Loss: 0.6189087 Vali Loss: 0.3635345 Test Loss: 0.3923519
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.4318974018096924
Epoch: 30, Steps: 63 | Train Loss: 0.6175508 Vali Loss: 0.3618626 Test Loss: 0.3923119
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.7804949283599854
Epoch: 31, Steps: 63 | Train Loss: 0.6165966 Vali Loss: 0.3661807 Test Loss: 0.3923494
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.547738552093506
Epoch: 32, Steps: 63 | Train Loss: 0.6175080 Vali Loss: 0.3634677 Test Loss: 0.3923137
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.7190749645233154
Epoch: 33, Steps: 63 | Train Loss: 0.6182516 Vali Loss: 0.3635098 Test Loss: 0.3923063
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.0033488273620605
Epoch: 34, Steps: 63 | Train Loss: 0.6173598 Vali Loss: 0.3634018 Test Loss: 0.3922746
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.457510471343994
Epoch: 35, Steps: 63 | Train Loss: 0.6169709 Vali Loss: 0.3661830 Test Loss: 0.3922812
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.2198660373687744
Epoch: 36, Steps: 63 | Train Loss: 0.6183936 Vali Loss: 0.3631409 Test Loss: 0.3922451
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.943133592605591
Epoch: 37, Steps: 63 | Train Loss: 0.6184206 Vali Loss: 0.3636420 Test Loss: 0.3922921
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.347757339477539
Epoch: 38, Steps: 63 | Train Loss: 0.6175089 Vali Loss: 0.3632446 Test Loss: 0.3922783
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.016935348510742
Epoch: 39, Steps: 63 | Train Loss: 0.6168773 Vali Loss: 0.3631012 Test Loss: 0.3922811
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.724367380142212
Epoch: 40, Steps: 63 | Train Loss: 0.6162198 Vali Loss: 0.3648141 Test Loss: 0.3922793
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.252937078475952
Epoch: 41, Steps: 63 | Train Loss: 0.6165666 Vali Loss: 0.3622815 Test Loss: 0.3922643
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.4466142654418945
Epoch: 42, Steps: 63 | Train Loss: 0.6164057 Vali Loss: 0.3640859 Test Loss: 0.3922762
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.4627647399902344
Epoch: 43, Steps: 63 | Train Loss: 0.6160932 Vali Loss: 0.3650713 Test Loss: 0.3922848
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.38781458139419556, mae:0.41204291582107544, rse:0.49791085720062256, corr:[0.2636203  0.26664618 0.2646231  0.26356417 0.2631709  0.26180133
 0.26004353 0.2589451  0.2583489  0.25699586 0.25548196 0.25410172
 0.25312936 0.25184095 0.25065088 0.24992757 0.2494998  0.24867599
 0.24745768 0.24637577 0.24562733 0.2448935  0.24369393 0.24195205
 0.23968393 0.23776874 0.23612064 0.23477466 0.23336795 0.23195176
 0.23040166 0.22873184 0.22712721 0.22579855 0.22471602 0.2235761
 0.2224617  0.22126164 0.22027536 0.21921627 0.21840963 0.21765548
 0.21684255 0.21580322 0.21470195 0.21337792 0.21166047 0.2094513
 0.20654918 0.20389546 0.20169833 0.20003748 0.1981943  0.19606736
 0.19372845 0.19181758 0.19065829 0.18929416 0.1879016  0.18668123
 0.18623917 0.18567592 0.18519022 0.18456036 0.18418245 0.18370588
 0.18261953 0.18130933 0.18076815 0.180823   0.18002476 0.17847665
 0.17644674 0.17497018 0.17353551 0.1720734  0.17102534 0.17074582
 0.17030783 0.169278   0.1687141  0.16864866 0.16878335 0.16852342
 0.16827235 0.16798536 0.1680677  0.16788706 0.16745976 0.16683367
 0.16647714 0.16612676 0.16606805 0.16611037 0.16592029 0.16553852
 0.16435951 0.16302016 0.16176139 0.16108698 0.1604462  0.15974553
 0.15932979 0.15913214 0.15925722 0.15913868 0.15927185 0.15946724
 0.15973288 0.15921058 0.15840976 0.15775785 0.15744133 0.15710321
 0.15651025 0.1556983  0.1552096  0.15483566 0.15391845 0.15237933
 0.15038896 0.14872417 0.14717245 0.14588958 0.14458093 0.14354056
 0.14270942 0.14183128 0.1410202  0.14042746 0.14012322 0.13963866
 0.13918374 0.1386238  0.13831404 0.13793066 0.13737458 0.13654135
 0.13586998 0.135532   0.13552001 0.13520235 0.13401201 0.13223661
 0.12973563 0.12784828 0.12603974 0.12466916 0.12351698 0.12268073
 0.12220262 0.12169945 0.12152128 0.12155442 0.12189107 0.12200233
 0.12245589 0.12244005 0.12241639 0.12251022 0.12266671 0.12224029
 0.12136515 0.12084152 0.1209942  0.12142888 0.12111458 0.11969549
 0.11746819 0.11614149 0.11503021 0.11401616 0.11274772 0.11139859
 0.11060473 0.11004092 0.10996404 0.1098572  0.11014411 0.11021931
 0.11027057 0.11009009 0.11007547 0.11019407 0.11017957 0.1097892
 0.10951723 0.10938273 0.10959421 0.11004902 0.11026674 0.11008537
 0.10926595 0.1087285  0.10819532 0.10820512 0.10811264 0.1079296
 0.10789207 0.10807143 0.1084947  0.10851397 0.10874766 0.10876752
 0.10899817 0.1088387  0.10859425 0.1084826  0.10859296 0.10878396
 0.10874841 0.10872798 0.1089578  0.10923926 0.1093109  0.10883567
 0.10773461 0.1066755  0.1058248  0.10537256 0.1048831  0.10508634
 0.105454   0.10586616 0.10587324 0.10574616 0.10578917 0.10559782
 0.10546005 0.10526324 0.10557414 0.10591893 0.10604079 0.10599543
 0.10616776 0.10666473 0.1072326  0.10768951 0.10781763 0.107696
 0.10669953 0.10563865 0.10451794 0.10417592 0.10396083 0.10365248
 0.10333391 0.10367056 0.10434448 0.10454135 0.10487128 0.10532241
 0.10656754 0.10773117 0.10827492 0.10836891 0.10846598 0.10893611
 0.10927546 0.10957783 0.11013284 0.1107766  0.11133145 0.11151039
 0.11106025 0.11059472 0.11013617 0.11021115 0.11006398 0.11017466
 0.11030348 0.11057726 0.11119451 0.11157173 0.11204603 0.1123409
 0.11319429 0.11347162 0.11344573 0.11316682 0.11343876 0.11398097
 0.11423349 0.11391798 0.1137587  0.11418111 0.11461688 0.11515298
 0.11478659 0.11441084 0.11385889 0.1137748  0.11321379 0.11259035
 0.11214536 0.11196026 0.11279891 0.11349141 0.11443213 0.11516213
 0.11609244 0.11633142 0.1166726  0.11717413 0.11766455 0.11824337
 0.11862391 0.11894182 0.11910781 0.1192878  0.11947187 0.11925433
 0.11810633 0.11726274 0.11652581 0.11628249 0.11534148 0.11445078
 0.11361787 0.11407843 0.11515363 0.11546895 0.11580801 0.11659541
 0.11845385 0.11853239 0.11781483 0.11704595 0.11696797 0.11676665
 0.11626981 0.11485366 0.11289553 0.1121012  0.11294097 0.11242849]
