Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_96_FITS_ETTh2_ftM_sl90_ll48_pl96_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8455
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=34, out_features=70, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2132480.0
params:  2450.0
Trainable parameters:  2450
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.197139024734497
Epoch: 1, Steps: 66 | Train Loss: 0.5901848 Vali Loss: 0.2825502 Test Loss: 0.3739341
Validation loss decreased (inf --> 0.282550).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.2650482654571533
Epoch: 2, Steps: 66 | Train Loss: 0.5123313 Vali Loss: 0.2546414 Test Loss: 0.3376162
Validation loss decreased (0.282550 --> 0.254641).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.507375478744507
Epoch: 3, Steps: 66 | Train Loss: 0.4802297 Vali Loss: 0.2416479 Test Loss: 0.3217810
Validation loss decreased (0.254641 --> 0.241648).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.6386799812316895
Epoch: 4, Steps: 66 | Train Loss: 0.4655699 Vali Loss: 0.2351157 Test Loss: 0.3138008
Validation loss decreased (0.241648 --> 0.235116).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.7481017112731934
Epoch: 5, Steps: 66 | Train Loss: 0.4566121 Vali Loss: 0.2311258 Test Loss: 0.3091347
Validation loss decreased (0.235116 --> 0.231126).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.170074939727783
Epoch: 6, Steps: 66 | Train Loss: 0.4513108 Vali Loss: 0.2279449 Test Loss: 0.3061496
Validation loss decreased (0.231126 --> 0.227945).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.844972848892212
Epoch: 7, Steps: 66 | Train Loss: 0.4469290 Vali Loss: 0.2256480 Test Loss: 0.3040057
Validation loss decreased (0.227945 --> 0.225648).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.945652961730957
Epoch: 8, Steps: 66 | Train Loss: 0.4441024 Vali Loss: 0.2230165 Test Loss: 0.3023517
Validation loss decreased (0.225648 --> 0.223017).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.044870138168335
Epoch: 9, Steps: 66 | Train Loss: 0.4417462 Vali Loss: 0.2227797 Test Loss: 0.3010532
Validation loss decreased (0.223017 --> 0.222780).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.934600591659546
Epoch: 10, Steps: 66 | Train Loss: 0.4395562 Vali Loss: 0.2213945 Test Loss: 0.2999991
Validation loss decreased (0.222780 --> 0.221395).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.5637900829315186
Epoch: 11, Steps: 66 | Train Loss: 0.4380095 Vali Loss: 0.2202784 Test Loss: 0.2990423
Validation loss decreased (0.221395 --> 0.220278).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.337693929672241
Epoch: 12, Steps: 66 | Train Loss: 0.4364140 Vali Loss: 0.2198377 Test Loss: 0.2982398
Validation loss decreased (0.220278 --> 0.219838).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.2133772373199463
Epoch: 13, Steps: 66 | Train Loss: 0.4351933 Vali Loss: 0.2195836 Test Loss: 0.2975567
Validation loss decreased (0.219838 --> 0.219584).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.5184199810028076
Epoch: 14, Steps: 66 | Train Loss: 0.4342376 Vali Loss: 0.2186171 Test Loss: 0.2970182
Validation loss decreased (0.219584 --> 0.218617).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.724440574645996
Epoch: 15, Steps: 66 | Train Loss: 0.4332136 Vali Loss: 0.2183693 Test Loss: 0.2964663
Validation loss decreased (0.218617 --> 0.218369).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.1997182369232178
Epoch: 16, Steps: 66 | Train Loss: 0.4322822 Vali Loss: 0.2173564 Test Loss: 0.2960203
Validation loss decreased (0.218369 --> 0.217356).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.380671977996826
Epoch: 17, Steps: 66 | Train Loss: 0.4315116 Vali Loss: 0.2183122 Test Loss: 0.2956002
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.8573358058929443
Epoch: 18, Steps: 66 | Train Loss: 0.4308615 Vali Loss: 0.2172635 Test Loss: 0.2952751
Validation loss decreased (0.217356 --> 0.217264).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.580824613571167
Epoch: 19, Steps: 66 | Train Loss: 0.4300302 Vali Loss: 0.2163037 Test Loss: 0.2949450
Validation loss decreased (0.217264 --> 0.216304).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.0691959857940674
Epoch: 20, Steps: 66 | Train Loss: 0.4292964 Vali Loss: 0.2168373 Test Loss: 0.2946777
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.434563636779785
Epoch: 21, Steps: 66 | Train Loss: 0.4290722 Vali Loss: 0.2161973 Test Loss: 0.2944247
Validation loss decreased (0.216304 --> 0.216197).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.8516666889190674
Epoch: 22, Steps: 66 | Train Loss: 0.4285339 Vali Loss: 0.2160954 Test Loss: 0.2942253
Validation loss decreased (0.216197 --> 0.216095).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.009693145751953
Epoch: 23, Steps: 66 | Train Loss: 0.4276063 Vali Loss: 0.2168801 Test Loss: 0.2939831
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.043689489364624
Epoch: 24, Steps: 66 | Train Loss: 0.4277371 Vali Loss: 0.2148555 Test Loss: 0.2938154
Validation loss decreased (0.216095 --> 0.214855).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2042295932769775
Epoch: 25, Steps: 66 | Train Loss: 0.4273162 Vali Loss: 0.2157367 Test Loss: 0.2936437
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.262490272521973
Epoch: 26, Steps: 66 | Train Loss: 0.4271665 Vali Loss: 0.2152585 Test Loss: 0.2935024
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.726430654525757
Epoch: 27, Steps: 66 | Train Loss: 0.4268904 Vali Loss: 0.2151168 Test Loss: 0.2933601
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.307908296585083
Epoch: 28, Steps: 66 | Train Loss: 0.4264921 Vali Loss: 0.2148964 Test Loss: 0.2932320
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.3136937618255615
Epoch: 29, Steps: 66 | Train Loss: 0.4264154 Vali Loss: 0.2143496 Test Loss: 0.2930999
Validation loss decreased (0.214855 --> 0.214350).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.944852590560913
Epoch: 30, Steps: 66 | Train Loss: 0.4260985 Vali Loss: 0.2142141 Test Loss: 0.2930299
Validation loss decreased (0.214350 --> 0.214214).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.9034035205841064
Epoch: 31, Steps: 66 | Train Loss: 0.4258395 Vali Loss: 0.2155323 Test Loss: 0.2929171
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.913374900817871
Epoch: 32, Steps: 66 | Train Loss: 0.4252023 Vali Loss: 0.2138868 Test Loss: 0.2928149
Validation loss decreased (0.214214 --> 0.213887).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.927253246307373
Epoch: 33, Steps: 66 | Train Loss: 0.4254523 Vali Loss: 0.2151502 Test Loss: 0.2927569
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.7189719676971436
Epoch: 34, Steps: 66 | Train Loss: 0.4252520 Vali Loss: 0.2141438 Test Loss: 0.2926735
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.5609400272369385
Epoch: 35, Steps: 66 | Train Loss: 0.4252048 Vali Loss: 0.2140666 Test Loss: 0.2925938
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.3356218338012695
Epoch: 36, Steps: 66 | Train Loss: 0.4249896 Vali Loss: 0.2148402 Test Loss: 0.2925483
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.042978525161743
Epoch: 37, Steps: 66 | Train Loss: 0.4248716 Vali Loss: 0.2143912 Test Loss: 0.2924910
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.682258367538452
Epoch: 38, Steps: 66 | Train Loss: 0.4248014 Vali Loss: 0.2138755 Test Loss: 0.2924430
Validation loss decreased (0.213887 --> 0.213876).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.2743802070617676
Epoch: 39, Steps: 66 | Train Loss: 0.4246117 Vali Loss: 0.2144785 Test Loss: 0.2923932
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.4034483432769775
Epoch: 40, Steps: 66 | Train Loss: 0.4244025 Vali Loss: 0.2139394 Test Loss: 0.2923576
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9639525413513184
Epoch: 41, Steps: 66 | Train Loss: 0.4244486 Vali Loss: 0.2142201 Test Loss: 0.2922996
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.419071912765503
Epoch: 42, Steps: 66 | Train Loss: 0.4243484 Vali Loss: 0.2146969 Test Loss: 0.2922588
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.758990526199341
Epoch: 43, Steps: 66 | Train Loss: 0.4241407 Vali Loss: 0.2140636 Test Loss: 0.2922247
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.176705360412598
Epoch: 44, Steps: 66 | Train Loss: 0.4239958 Vali Loss: 0.2132344 Test Loss: 0.2921977
Validation loss decreased (0.213876 --> 0.213234).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.1974096298217773
Epoch: 45, Steps: 66 | Train Loss: 0.4241083 Vali Loss: 0.2140244 Test Loss: 0.2921591
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.8980605602264404
Epoch: 46, Steps: 66 | Train Loss: 0.4240704 Vali Loss: 0.2133045 Test Loss: 0.2921246
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.7274389266967773
Epoch: 47, Steps: 66 | Train Loss: 0.4238521 Vali Loss: 0.2141177 Test Loss: 0.2921022
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.2781553268432617
Epoch: 48, Steps: 66 | Train Loss: 0.4236412 Vali Loss: 0.2134505 Test Loss: 0.2920685
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.38417911529541
Epoch: 49, Steps: 66 | Train Loss: 0.4234690 Vali Loss: 0.2139924 Test Loss: 0.2920581
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.3499159812927246
Epoch: 50, Steps: 66 | Train Loss: 0.4238164 Vali Loss: 0.2135940 Test Loss: 0.2920236
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.909064054489136
Epoch: 51, Steps: 66 | Train Loss: 0.4236591 Vali Loss: 0.2127612 Test Loss: 0.2920012
Validation loss decreased (0.213234 --> 0.212761).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.7285208702087402
Epoch: 52, Steps: 66 | Train Loss: 0.4235843 Vali Loss: 0.2127148 Test Loss: 0.2919882
Validation loss decreased (0.212761 --> 0.212715).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 5.738842010498047
Epoch: 53, Steps: 66 | Train Loss: 0.4236324 Vali Loss: 0.2135273 Test Loss: 0.2919690
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 4.06303334236145
Epoch: 54, Steps: 66 | Train Loss: 0.4234361 Vali Loss: 0.2137620 Test Loss: 0.2919461
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.4392364025115967
Epoch: 55, Steps: 66 | Train Loss: 0.4229782 Vali Loss: 0.2136913 Test Loss: 0.2919291
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.85506272315979
Epoch: 56, Steps: 66 | Train Loss: 0.4233315 Vali Loss: 0.2139257 Test Loss: 0.2919107
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.7218706607818604
Epoch: 57, Steps: 66 | Train Loss: 0.4233940 Vali Loss: 0.2149504 Test Loss: 0.2918956
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.3045759201049805
Epoch: 58, Steps: 66 | Train Loss: 0.4233825 Vali Loss: 0.2137689 Test Loss: 0.2918894
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.7218995094299316
Epoch: 59, Steps: 66 | Train Loss: 0.4233901 Vali Loss: 0.2138760 Test Loss: 0.2918748
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.2556254863739014
Epoch: 60, Steps: 66 | Train Loss: 0.4233316 Vali Loss: 0.2140246 Test Loss: 0.2918594
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.6228530406951904
Epoch: 61, Steps: 66 | Train Loss: 0.4232630 Vali Loss: 0.2137360 Test Loss: 0.2918513
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.8123133182525635
Epoch: 62, Steps: 66 | Train Loss: 0.4227858 Vali Loss: 0.2133031 Test Loss: 0.2918414
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 4.137033939361572
Epoch: 63, Steps: 66 | Train Loss: 0.4231664 Vali Loss: 0.2134579 Test Loss: 0.2918266
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.716840982437134
Epoch: 64, Steps: 66 | Train Loss: 0.4232425 Vali Loss: 0.2130599 Test Loss: 0.2918148
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.7380149364471436
Epoch: 65, Steps: 66 | Train Loss: 0.4232224 Vali Loss: 0.2140475 Test Loss: 0.2918057
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.692556619644165
Epoch: 66, Steps: 66 | Train Loss: 0.4220811 Vali Loss: 0.2138244 Test Loss: 0.2917939
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.0248775482177734
Epoch: 67, Steps: 66 | Train Loss: 0.4230192 Vali Loss: 0.2136881 Test Loss: 0.2917942
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 4.2200658321380615
Epoch: 68, Steps: 66 | Train Loss: 0.4230306 Vali Loss: 0.2129464 Test Loss: 0.2917805
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.037461757659912
Epoch: 69, Steps: 66 | Train Loss: 0.4219322 Vali Loss: 0.2146339 Test Loss: 0.2917750
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.406914472579956
Epoch: 70, Steps: 66 | Train Loss: 0.4230208 Vali Loss: 0.2130170 Test Loss: 0.2917656
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.8975138664245605
Epoch: 71, Steps: 66 | Train Loss: 0.4230283 Vali Loss: 0.2134565 Test Loss: 0.2917604
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.0427193641662598
Epoch: 72, Steps: 66 | Train Loss: 0.4229717 Vali Loss: 0.2129451 Test Loss: 0.2917524
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_96_FITS_ETTh2_ftM_sl90_ll48_pl96_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.29251712560653687, mae:0.3401781916618347, rse:0.435869961977005, corr:[0.2758423  0.277476   0.2756552  0.27592728 0.27316117 0.2730807
 0.27131772 0.27084494 0.26985258 0.26850796 0.26805347 0.26596743
 0.26434347 0.2632188  0.26194808 0.26138532 0.26056254 0.26011977
 0.2593341  0.25797656 0.25721422 0.2562576  0.25525755 0.25325426
 0.25006542 0.24769565 0.2451048  0.24339938 0.2421425  0.2404757
 0.23967682 0.23849882 0.23714826 0.23599142 0.23496032 0.2339781
 0.23230882 0.23099771 0.23026162 0.22895442 0.22851217 0.22822921
 0.22738229 0.22703995 0.22613645 0.22506504 0.22459817 0.22230381
 0.21906312 0.2162465  0.21342427 0.2119209  0.20991114 0.20849068
 0.20729692 0.20484693 0.20448099 0.2029363  0.20179941 0.20136261
 0.19966678 0.19950134 0.19944288 0.1988467  0.19922481 0.19826272
 0.19755295 0.19733877 0.19602567 0.19594766 0.19531646 0.19332239
 0.19143473 0.18913476 0.1876959  0.18705463 0.18516342 0.18529429
 0.18507886 0.18371883 0.18413608 0.18298654 0.1833716  0.18357727
 0.18207052 0.1826492  0.18202601 0.18227717 0.18213512 0.1801138
 0.18109058 0.17838743 0.17803471 0.17917506 0.17634855 0.17962544]
