Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=18, out_features=85, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1370880.0
params:  1615.0
Trainable parameters:  1615
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.6653823852539062
Epoch: 1, Steps: 64 | Train Loss: 0.8546770 Vali Loss: 0.5135115 Test Loss: 0.6072692
Validation loss decreased (inf --> 0.513512).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9106736183166504
Epoch: 2, Steps: 64 | Train Loss: 0.7412352 Vali Loss: 0.4673768 Test Loss: 0.5495629
Validation loss decreased (0.513512 --> 0.467377).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.83353590965271
Epoch: 3, Steps: 64 | Train Loss: 0.6709344 Vali Loss: 0.4420217 Test Loss: 0.5131580
Validation loss decreased (0.467377 --> 0.442022).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.1701152324676514
Epoch: 4, Steps: 64 | Train Loss: 0.6291633 Vali Loss: 0.4247867 Test Loss: 0.4889367
Validation loss decreased (0.442022 --> 0.424787).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.260810613632202
Epoch: 5, Steps: 64 | Train Loss: 0.6000189 Vali Loss: 0.4111449 Test Loss: 0.4723849
Validation loss decreased (0.424787 --> 0.411145).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.1625988483428955
Epoch: 6, Steps: 64 | Train Loss: 0.5813869 Vali Loss: 0.4023260 Test Loss: 0.4610004
Validation loss decreased (0.411145 --> 0.402326).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.739004373550415
Epoch: 7, Steps: 64 | Train Loss: 0.5673709 Vali Loss: 0.3924090 Test Loss: 0.4527744
Validation loss decreased (0.402326 --> 0.392409).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.4387764930725098
Epoch: 8, Steps: 64 | Train Loss: 0.5569905 Vali Loss: 0.3881657 Test Loss: 0.4468253
Validation loss decreased (0.392409 --> 0.388166).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.81961989402771
Epoch: 9, Steps: 64 | Train Loss: 0.5510966 Vali Loss: 0.3881451 Test Loss: 0.4424178
Validation loss decreased (0.388166 --> 0.388145).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.9356417655944824
Epoch: 10, Steps: 64 | Train Loss: 0.5460050 Vali Loss: 0.3835508 Test Loss: 0.4391330
Validation loss decreased (0.388145 --> 0.383551).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9178216457366943
Epoch: 11, Steps: 64 | Train Loss: 0.5417640 Vali Loss: 0.3823967 Test Loss: 0.4365778
Validation loss decreased (0.383551 --> 0.382397).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.920783519744873
Epoch: 12, Steps: 64 | Train Loss: 0.5379885 Vali Loss: 0.3805255 Test Loss: 0.4345658
Validation loss decreased (0.382397 --> 0.380525).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9988293647766113
Epoch: 13, Steps: 64 | Train Loss: 0.5352530 Vali Loss: 0.3763708 Test Loss: 0.4329908
Validation loss decreased (0.380525 --> 0.376371).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.879465103149414
Epoch: 14, Steps: 64 | Train Loss: 0.5342392 Vali Loss: 0.3763947 Test Loss: 0.4317445
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.086052656173706
Epoch: 15, Steps: 64 | Train Loss: 0.5323967 Vali Loss: 0.3759866 Test Loss: 0.4307570
Validation loss decreased (0.376371 --> 0.375987).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.818152904510498
Epoch: 16, Steps: 64 | Train Loss: 0.5307335 Vali Loss: 0.3767619 Test Loss: 0.4298894
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.398110866546631
Epoch: 17, Steps: 64 | Train Loss: 0.5297800 Vali Loss: 0.3700767 Test Loss: 0.4292569
Validation loss decreased (0.375987 --> 0.370077).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7598705291748047
Epoch: 18, Steps: 64 | Train Loss: 0.5291520 Vali Loss: 0.3728639 Test Loss: 0.4286570
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.2506582736968994
Epoch: 19, Steps: 64 | Train Loss: 0.5275701 Vali Loss: 0.3725542 Test Loss: 0.4281409
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.0575790405273438
Epoch: 20, Steps: 64 | Train Loss: 0.5273765 Vali Loss: 0.3713060 Test Loss: 0.4277333
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.9736557006835938
Epoch: 21, Steps: 64 | Train Loss: 0.5279114 Vali Loss: 0.3745895 Test Loss: 0.4273794
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.303276300430298
Epoch: 22, Steps: 64 | Train Loss: 0.5270986 Vali Loss: 0.3708843 Test Loss: 0.4270775
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.2681167125701904
Epoch: 23, Steps: 64 | Train Loss: 0.5270303 Vali Loss: 0.3696420 Test Loss: 0.4267735
Validation loss decreased (0.370077 --> 0.369642).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.890838861465454
Epoch: 24, Steps: 64 | Train Loss: 0.5263002 Vali Loss: 0.3714207 Test Loss: 0.4265469
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.8014039993286133
Epoch: 25, Steps: 64 | Train Loss: 0.5262883 Vali Loss: 0.3711413 Test Loss: 0.4263448
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7869360446929932
Epoch: 26, Steps: 64 | Train Loss: 0.5261314 Vali Loss: 0.3705524 Test Loss: 0.4261390
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.2829370498657227
Epoch: 27, Steps: 64 | Train Loss: 0.5250229 Vali Loss: 0.3680525 Test Loss: 0.4259865
Validation loss decreased (0.369642 --> 0.368053).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.252870559692383
Epoch: 28, Steps: 64 | Train Loss: 0.5256058 Vali Loss: 0.3686222 Test Loss: 0.4258202
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.9891014099121094
Epoch: 29, Steps: 64 | Train Loss: 0.5249883 Vali Loss: 0.3679082 Test Loss: 0.4256719
Validation loss decreased (0.368053 --> 0.367908).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.4470927715301514
Epoch: 30, Steps: 64 | Train Loss: 0.5240886 Vali Loss: 0.3678321 Test Loss: 0.4255605
Validation loss decreased (0.367908 --> 0.367832).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.5501317977905273
Epoch: 31, Steps: 64 | Train Loss: 0.5243261 Vali Loss: 0.3714478 Test Loss: 0.4254276
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.39617919921875
Epoch: 32, Steps: 64 | Train Loss: 0.5226689 Vali Loss: 0.3696731 Test Loss: 0.4253424
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.239487886428833
Epoch: 33, Steps: 64 | Train Loss: 0.5244323 Vali Loss: 0.3691308 Test Loss: 0.4252428
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.4096474647521973
Epoch: 34, Steps: 64 | Train Loss: 0.5238892 Vali Loss: 0.3661567 Test Loss: 0.4251456
Validation loss decreased (0.367832 --> 0.366157).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.475820541381836
Epoch: 35, Steps: 64 | Train Loss: 0.5229062 Vali Loss: 0.3696161 Test Loss: 0.4250700
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.8843872547149658
Epoch: 36, Steps: 64 | Train Loss: 0.5241642 Vali Loss: 0.3693200 Test Loss: 0.4249993
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.894974946975708
Epoch: 37, Steps: 64 | Train Loss: 0.5229596 Vali Loss: 0.3698486 Test Loss: 0.4249243
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9842126369476318
Epoch: 38, Steps: 64 | Train Loss: 0.5232694 Vali Loss: 0.3701273 Test Loss: 0.4248714
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.9255504608154297
Epoch: 39, Steps: 64 | Train Loss: 0.5233355 Vali Loss: 0.3650402 Test Loss: 0.4248161
Validation loss decreased (0.366157 --> 0.365040).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.383098840713501
Epoch: 40, Steps: 64 | Train Loss: 0.5230638 Vali Loss: 0.3652100 Test Loss: 0.4247434
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.1857972145080566
Epoch: 41, Steps: 64 | Train Loss: 0.5235952 Vali Loss: 0.3709457 Test Loss: 0.4246985
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.1301989555358887
Epoch: 42, Steps: 64 | Train Loss: 0.5224305 Vali Loss: 0.3676958 Test Loss: 0.4246473
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.2439754009246826
Epoch: 43, Steps: 64 | Train Loss: 0.5234021 Vali Loss: 0.3682481 Test Loss: 0.4246123
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.9807922840118408
Epoch: 44, Steps: 64 | Train Loss: 0.5218335 Vali Loss: 0.3670555 Test Loss: 0.4245589
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.439086437225342
Epoch: 45, Steps: 64 | Train Loss: 0.5224110 Vali Loss: 0.3675081 Test Loss: 0.4245227
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.907423257827759
Epoch: 46, Steps: 64 | Train Loss: 0.5231385 Vali Loss: 0.3682154 Test Loss: 0.4244863
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.11173939704895
Epoch: 47, Steps: 64 | Train Loss: 0.5220913 Vali Loss: 0.3669546 Test Loss: 0.4244540
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.401799440383911
Epoch: 48, Steps: 64 | Train Loss: 0.5225149 Vali Loss: 0.3696604 Test Loss: 0.4244223
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.0133912563323975
Epoch: 49, Steps: 64 | Train Loss: 0.5226450 Vali Loss: 0.3684904 Test Loss: 0.4243933
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.548454999923706
Epoch: 50, Steps: 64 | Train Loss: 0.5218728 Vali Loss: 0.3667345 Test Loss: 0.4243697
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.107105016708374
Epoch: 51, Steps: 64 | Train Loss: 0.5226020 Vali Loss: 0.3685816 Test Loss: 0.4243417
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.8796100616455078
Epoch: 52, Steps: 64 | Train Loss: 0.5223653 Vali Loss: 0.3683246 Test Loss: 0.4243129
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.0937366485595703
Epoch: 53, Steps: 64 | Train Loss: 0.5211098 Vali Loss: 0.3687518 Test Loss: 0.4242867
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.1054229736328125
Epoch: 54, Steps: 64 | Train Loss: 0.5224553 Vali Loss: 0.3674955 Test Loss: 0.4242599
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.8373379707336426
Epoch: 55, Steps: 64 | Train Loss: 0.5220883 Vali Loss: 0.3665617 Test Loss: 0.4242416
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.0402915477752686
Epoch: 56, Steps: 64 | Train Loss: 0.5226367 Vali Loss: 0.3701909 Test Loss: 0.4242243
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.366438865661621
Epoch: 57, Steps: 64 | Train Loss: 0.5224977 Vali Loss: 0.3684351 Test Loss: 0.4242102
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.9331562519073486
Epoch: 58, Steps: 64 | Train Loss: 0.5228485 Vali Loss: 0.3671795 Test Loss: 0.4241797
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.1339504718780518
Epoch: 59, Steps: 64 | Train Loss: 0.5216828 Vali Loss: 0.3667169 Test Loss: 0.4241672
EarlyStopping counter: 20 out of 20
Early stopping
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=18, out_features=85, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1370880.0
params:  1615.0
Trainable parameters:  1615
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.050820827484131
Epoch: 1, Steps: 64 | Train Loss: 0.6535794 Vali Loss: 0.3678048 Test Loss: 0.4239408
Validation loss decreased (inf --> 0.367805).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9810287952423096
Epoch: 2, Steps: 64 | Train Loss: 0.6521953 Vali Loss: 0.3670792 Test Loss: 0.4235473
Validation loss decreased (0.367805 --> 0.367079).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0921237468719482
Epoch: 3, Steps: 64 | Train Loss: 0.6516430 Vali Loss: 0.3649847 Test Loss: 0.4231316
Validation loss decreased (0.367079 --> 0.364985).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9465816020965576
Epoch: 4, Steps: 64 | Train Loss: 0.6517394 Vali Loss: 0.3672096 Test Loss: 0.4229008
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.099637985229492
Epoch: 5, Steps: 64 | Train Loss: 0.6514423 Vali Loss: 0.3665045 Test Loss: 0.4227765
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.9877548217773438
Epoch: 6, Steps: 64 | Train Loss: 0.6519761 Vali Loss: 0.3631298 Test Loss: 0.4225467
Validation loss decreased (0.364985 --> 0.363130).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.1632232666015625
Epoch: 7, Steps: 64 | Train Loss: 0.6507983 Vali Loss: 0.3677565 Test Loss: 0.4223067
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.9592585563659668
Epoch: 8, Steps: 64 | Train Loss: 0.6501620 Vali Loss: 0.3659493 Test Loss: 0.4222604
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.443262815475464
Epoch: 9, Steps: 64 | Train Loss: 0.6502294 Vali Loss: 0.3665700 Test Loss: 0.4221889
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2389535903930664
Epoch: 10, Steps: 64 | Train Loss: 0.6503767 Vali Loss: 0.3635461 Test Loss: 0.4220619
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.3334479331970215
Epoch: 11, Steps: 64 | Train Loss: 0.6500843 Vali Loss: 0.3659006 Test Loss: 0.4219709
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.0937583446502686
Epoch: 12, Steps: 64 | Train Loss: 0.6480575 Vali Loss: 0.3635587 Test Loss: 0.4219439
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.7464702129364014
Epoch: 13, Steps: 64 | Train Loss: 0.6490721 Vali Loss: 0.3604829 Test Loss: 0.4218286
Validation loss decreased (0.363130 --> 0.360483).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.6158976554870605
Epoch: 14, Steps: 64 | Train Loss: 0.6485078 Vali Loss: 0.3643679 Test Loss: 0.4217690
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.7174410820007324
Epoch: 15, Steps: 64 | Train Loss: 0.6487794 Vali Loss: 0.3641053 Test Loss: 0.4217993
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.755659818649292
Epoch: 16, Steps: 64 | Train Loss: 0.6475517 Vali Loss: 0.3636077 Test Loss: 0.4216967
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.105194330215454
Epoch: 17, Steps: 64 | Train Loss: 0.6483802 Vali Loss: 0.3637043 Test Loss: 0.4216713
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.837792158126831
Epoch: 18, Steps: 64 | Train Loss: 0.6473150 Vali Loss: 0.3620535 Test Loss: 0.4216022
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9615044593811035
Epoch: 19, Steps: 64 | Train Loss: 0.6485624 Vali Loss: 0.3623147 Test Loss: 0.4215891
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.0880162715911865
Epoch: 20, Steps: 64 | Train Loss: 0.6482626 Vali Loss: 0.3632204 Test Loss: 0.4215478
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.9652771949768066
Epoch: 21, Steps: 64 | Train Loss: 0.6468212 Vali Loss: 0.3637541 Test Loss: 0.4215349
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.6092886924743652
Epoch: 22, Steps: 64 | Train Loss: 0.6482048 Vali Loss: 0.3649958 Test Loss: 0.4215213
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.457289218902588
Epoch: 23, Steps: 64 | Train Loss: 0.6479912 Vali Loss: 0.3639529 Test Loss: 0.4214923
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8591649532318115
Epoch: 24, Steps: 64 | Train Loss: 0.6479260 Vali Loss: 0.3624005 Test Loss: 0.4214862
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.07419753074646
Epoch: 25, Steps: 64 | Train Loss: 0.6476644 Vali Loss: 0.3639972 Test Loss: 0.4214561
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.488372564315796
Epoch: 26, Steps: 64 | Train Loss: 0.6472521 Vali Loss: 0.3625275 Test Loss: 0.4214439
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.0552446842193604
Epoch: 27, Steps: 64 | Train Loss: 0.6478819 Vali Loss: 0.3652129 Test Loss: 0.4214251
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.5252981185913086
Epoch: 28, Steps: 64 | Train Loss: 0.6477943 Vali Loss: 0.3644629 Test Loss: 0.4213894
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.414194107055664
Epoch: 29, Steps: 64 | Train Loss: 0.6470607 Vali Loss: 0.3638260 Test Loss: 0.4213987
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.186859369277954
Epoch: 30, Steps: 64 | Train Loss: 0.6473642 Vali Loss: 0.3622549 Test Loss: 0.4213666
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.915222644805908
Epoch: 31, Steps: 64 | Train Loss: 0.6476922 Vali Loss: 0.3654010 Test Loss: 0.4213602
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.2337591648101807
Epoch: 32, Steps: 64 | Train Loss: 0.6462784 Vali Loss: 0.3623444 Test Loss: 0.4213653
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5781631469726562
Epoch: 33, Steps: 64 | Train Loss: 0.6460302 Vali Loss: 0.3627135 Test Loss: 0.4213393
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.41709551215171814, mae:0.42544296383857727, rse:0.5163655877113342, corr:[0.2633657  0.26614812 0.2640132  0.26187792 0.26116928 0.26055852
 0.2591322  0.25714996 0.2561864  0.2556683  0.25469658 0.2527717
 0.25088662 0.24977802 0.24917501 0.24853292 0.2476418  0.24675474
 0.24597567 0.24499771 0.24389161 0.24269731 0.241353   0.23966329
 0.23711985 0.23492473 0.2329396  0.23155345 0.23053183 0.229489
 0.22848241 0.2270566  0.2260222  0.22533438 0.22475524 0.2236346
 0.22213815 0.22106978 0.22027837 0.21952578 0.21886952 0.21830139
 0.21777585 0.21687634 0.21593067 0.2148731  0.21331944 0.21094175
 0.20756441 0.20486358 0.20245998 0.20037477 0.19858046 0.19684051
 0.19538447 0.19311428 0.19147159 0.19023992 0.1895882  0.18868311
 0.1874966  0.18701236 0.18680923 0.18654539 0.18613656 0.18585154
 0.18541697 0.18450648 0.18360567 0.18304874 0.18212585 0.18043004
 0.17760164 0.1757228  0.17433298 0.17331706 0.17253897 0.17218257
 0.17206724 0.17090076 0.17013921 0.16982967 0.1697142  0.16923122
 0.1685564  0.16833875 0.16833375 0.16814767 0.1676446  0.16716729
 0.16690816 0.1663574  0.16597281 0.16563791 0.16502357 0.1638674
 0.16173443 0.15989456 0.15819243 0.15674406 0.15570058 0.15521146
 0.15568855 0.15515344 0.15467839 0.15435928 0.15461731 0.15464677
 0.15395279 0.15332353 0.15265407 0.1522167  0.1517747  0.15134305
 0.15099053 0.15029778 0.14972374 0.14888136 0.14722405 0.14479272
 0.14175531 0.13966751 0.1379615  0.1367855  0.13549156 0.13447545
 0.13410658 0.13337007 0.13293657 0.13252829 0.13232711 0.13189411
 0.1312676  0.13088638 0.13061218 0.13022745 0.12964156 0.12894446
 0.12826452 0.12748152 0.12716441 0.12678204 0.12530394 0.12255166
 0.11870322 0.11594779 0.11390114 0.11257769 0.11159945 0.11083644
 0.11057352 0.10968335 0.10922442 0.10904484 0.10912898 0.10883311
 0.10817525 0.10780246 0.10764857 0.10777041 0.10793892 0.10791323
 0.10768854 0.10710343 0.1069143  0.10686302 0.106222   0.10429697
 0.10107133 0.09893755 0.097477   0.09652179 0.09570593 0.09527044
 0.09572395 0.095566   0.09555467 0.09544168 0.09553693 0.0952877
 0.09485777 0.0948639  0.09497491 0.09521117 0.09524871 0.0953695
 0.09566094 0.09565813 0.0957077  0.0959366  0.0958235  0.09498372
 0.09325297 0.09233529 0.0917206  0.09125391 0.09106244 0.09121235
 0.09231059 0.09285209 0.0932026  0.09318525 0.09332807 0.09340781
 0.0933843  0.0934276  0.0932529  0.0931946  0.09324046 0.09353816
 0.09370575 0.09357835 0.0935624  0.09347174 0.0929511  0.09145115
 0.08907388 0.08741418 0.08610072 0.08522715 0.0844809  0.08463608
 0.08588234 0.08682923 0.08744039 0.08751676 0.08748434 0.08704752
 0.08670281 0.08683125 0.08712759 0.0872989  0.08736542 0.08756232
 0.0879083  0.08825917 0.08856719 0.08881918 0.08850038 0.08735556
 0.08498115 0.08337022 0.08206336 0.08134026 0.08113959 0.08162949
 0.08331215 0.08436262 0.08524406 0.08591721 0.08661713 0.08684378
 0.08674923 0.08736359 0.08802782 0.08872618 0.08909804 0.08969384
 0.09005599 0.09022993 0.09050196 0.09088483 0.09085885 0.09007009
 0.08860552 0.08813429 0.08806425 0.08799265 0.08792429 0.08865631
 0.09027345 0.0911789  0.0919209  0.09235461 0.09267967 0.09228317
 0.09176697 0.09169412 0.09215935 0.09261377 0.09280795 0.09290037
 0.09308664 0.09303713 0.09320414 0.09353912 0.09354965 0.09296579
 0.09156755 0.0907649  0.09014896 0.08956397 0.08923081 0.08925732
 0.09009451 0.09047545 0.09105216 0.09145074 0.09185043 0.09196833
 0.09188432 0.09231635 0.09295019 0.0935686  0.09387304 0.09437525
 0.09494696 0.09513736 0.09564042 0.09611016 0.09616469 0.0954677
 0.09382677 0.09336586 0.0932216  0.09310196 0.09288778 0.09347801
 0.09509599 0.09578175 0.09617024 0.09649665 0.09733684 0.09785298
 0.09776117 0.09729468 0.09706817 0.09731735 0.09775122 0.09817299
 0.09835014 0.09777936 0.09764519 0.09834157 0.09905217 0.09850138]
