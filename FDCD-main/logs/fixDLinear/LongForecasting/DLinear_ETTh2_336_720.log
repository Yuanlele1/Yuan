Args in experiment:
Namespace(activation='gelu', batch_size=32, c_out=7, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, individual=False, is_training=1, itr=1, label_len=48, learning_rate=0.005, loss='mse', lradj='type1', model='DLinear', model_id='ETTh2_336_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seq_len=336, target='OT', test_flop=False, train_epochs=10, train_only=False, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_336_720_DLinear_ETTh2_ftM_sl336_ll48_pl720_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2161
test 2161
	iters: 100, epoch: 1 | loss: 0.6969482
	speed: 0.1025s/iter; left time: 232.7213s
	iters: 200, epoch: 1 | loss: 0.7787927
	speed: 0.1006s/iter; left time: 218.3700s
Epoch: 1 cost time: 23.97173023223877
Epoch: 1, Steps: 237 | Train Loss: 0.7812779 Vali Loss: 0.7557095 Test Loss: 0.9777321
Validation loss decreased (inf --> 0.755709).  Saving model ...
Updating learning rate to 0.005
	iters: 100, epoch: 2 | loss: 1.0428197
	speed: 0.4398s/iter; left time: 894.5006s
	iters: 200, epoch: 2 | loss: 0.5747623
	speed: 0.1138s/iter; left time: 220.0433s
Epoch: 2 cost time: 28.263911724090576
Epoch: 2, Steps: 237 | Train Loss: 0.7699355 Vali Loss: 0.7313419 Test Loss: 0.8179224
Validation loss decreased (0.755709 --> 0.731342).  Saving model ...
Updating learning rate to 0.0025
	iters: 100, epoch: 3 | loss: 0.9554482
	speed: 0.5118s/iter; left time: 919.7439s
	iters: 200, epoch: 3 | loss: 0.4233493
	speed: 0.1079s/iter; left time: 183.1387s
Epoch: 3 cost time: 25.972930431365967
Epoch: 3, Steps: 237 | Train Loss: 0.7312878 Vali Loss: 0.6609223 Test Loss: 0.6388544
Validation loss decreased (0.731342 --> 0.660922).  Saving model ...
Updating learning rate to 0.00125
	iters: 100, epoch: 4 | loss: 0.9444215
	speed: 0.6163s/iter; left time: 961.4936s
	iters: 200, epoch: 4 | loss: 0.6862503
	speed: 0.1334s/iter; left time: 194.7841s
Epoch: 4 cost time: 33.57402849197388
Epoch: 4, Steps: 237 | Train Loss: 0.7148993 Vali Loss: 0.6947998 Test Loss: 0.8522854
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000625
	iters: 100, epoch: 5 | loss: 0.7731199
	speed: 0.6206s/iter; left time: 821.0805s
	iters: 200, epoch: 5 | loss: 0.8484520
	speed: 0.1398s/iter; left time: 171.0290s
Epoch: 5 cost time: 34.26941990852356
Epoch: 5, Steps: 237 | Train Loss: 0.7039179 Vali Loss: 0.6395999 Test Loss: 0.5898596
Validation loss decreased (0.660922 --> 0.639600).  Saving model ...
Updating learning rate to 0.0003125
	iters: 100, epoch: 6 | loss: 0.7153298
	speed: 0.6327s/iter; left time: 687.1342s
	iters: 200, epoch: 6 | loss: 0.4554686
	speed: 0.1330s/iter; left time: 131.1539s
Epoch: 6 cost time: 32.90220618247986
Epoch: 6, Steps: 237 | Train Loss: 0.6995527 Vali Loss: 0.6519437 Test Loss: 0.6850982
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00015625
	iters: 100, epoch: 7 | loss: 0.9886315
	speed: 0.5980s/iter; left time: 507.6822s
	iters: 200, epoch: 7 | loss: 0.7111598
	speed: 0.1218s/iter; left time: 91.2482s
Epoch: 7 cost time: 30.1124849319458
Epoch: 7, Steps: 237 | Train Loss: 0.6965137 Vali Loss: 0.6696826 Test Loss: 0.7488707
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-05
	iters: 100, epoch: 8 | loss: 0.5664958
	speed: 0.5148s/iter; left time: 315.0345s
	iters: 200, epoch: 8 | loss: 0.7336869
	speed: 0.1224s/iter; left time: 62.6706s
Epoch: 8 cost time: 30.114073753356934
Epoch: 8, Steps: 237 | Train Loss: 0.6949909 Vali Loss: 0.6685381 Test Loss: 0.7404550
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : ETTh2_336_720_DLinear_ETTh2_ftM_sl336_ll48_pl720_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.5885617733001709, mae:0.545235276222229
