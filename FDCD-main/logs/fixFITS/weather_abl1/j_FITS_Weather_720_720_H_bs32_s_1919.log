Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=1919, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Weather_720_j720_H_FITS_custom_ftM_sl720_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=82, out_features=164, bias=True)
    (1): Linear(in_features=82, out_features=164, bias=True)
    (2): Linear(in_features=82, out_features=164, bias=True)
    (3): Linear(in_features=82, out_features=164, bias=True)
    (4): Linear(in_features=82, out_features=164, bias=True)
    (5): Linear(in_features=82, out_features=164, bias=True)
    (6): Linear(in_features=82, out_features=164, bias=True)
    (7): Linear(in_features=82, out_features=164, bias=True)
    (8): Linear(in_features=82, out_features=164, bias=True)
    (9): Linear(in_features=82, out_features=164, bias=True)
    (10): Linear(in_features=82, out_features=164, bias=True)
    (11): Linear(in_features=82, out_features=164, bias=True)
    (12): Linear(in_features=82, out_features=164, bias=True)
    (13): Linear(in_features=82, out_features=164, bias=True)
    (14): Linear(in_features=82, out_features=164, bias=True)
    (15): Linear(in_features=82, out_features=164, bias=True)
    (16): Linear(in_features=82, out_features=164, bias=True)
    (17): Linear(in_features=82, out_features=164, bias=True)
    (18): Linear(in_features=82, out_features=164, bias=True)
    (19): Linear(in_features=82, out_features=164, bias=True)
    (20): Linear(in_features=82, out_features=164, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  18074112.0
params:  285852.0
Trainable parameters:  285852
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7929016
	speed: 0.0378s/iter; left time: 2086.0979s
	iters: 200, epoch: 1 | loss: 0.5987812
	speed: 0.0324s/iter; left time: 1785.7245s
	iters: 300, epoch: 1 | loss: 0.5333351
	speed: 0.0432s/iter; left time: 2374.7271s
	iters: 400, epoch: 1 | loss: 0.5564163
	speed: 0.0315s/iter; left time: 1731.8750s
	iters: 500, epoch: 1 | loss: 0.7265424
	speed: 0.0368s/iter; left time: 2013.9817s
Epoch: 1 cost time: 19.905638694763184
Epoch: 1, Steps: 553 | Train Loss: 0.6822700 Vali Loss: 0.6163172 Test Loss: 0.3216056
Validation loss decreased (inf --> 0.616317).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5996636
	speed: 0.1525s/iter; left time: 8336.0467s
	iters: 200, epoch: 2 | loss: 0.5034668
	speed: 0.0413s/iter; left time: 2251.3229s
	iters: 300, epoch: 2 | loss: 0.4710311
	speed: 0.0407s/iter; left time: 2214.0235s
	iters: 400, epoch: 2 | loss: 0.5457538
	speed: 0.0376s/iter; left time: 2042.8609s
	iters: 500, epoch: 2 | loss: 0.4828560
	speed: 0.0413s/iter; left time: 2238.1976s
Epoch: 2 cost time: 22.75325345993042
Epoch: 2, Steps: 553 | Train Loss: 0.5747596 Vali Loss: 0.6042340 Test Loss: 0.3151419
Validation loss decreased (0.616317 --> 0.604234).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5851486
	speed: 0.1742s/iter; left time: 9420.6663s
	iters: 200, epoch: 3 | loss: 0.5059248
	speed: 0.0433s/iter; left time: 2335.6051s
	iters: 300, epoch: 3 | loss: 0.3878565
	speed: 0.0395s/iter; left time: 2127.4686s
	iters: 400, epoch: 3 | loss: 0.7052892
	speed: 0.0353s/iter; left time: 1900.1760s
	iters: 500, epoch: 3 | loss: 0.4987090
	speed: 0.0377s/iter; left time: 2026.3557s
Epoch: 3 cost time: 22.111820459365845
Epoch: 3, Steps: 553 | Train Loss: 0.5664848 Vali Loss: 0.6025286 Test Loss: 0.3126986
Validation loss decreased (0.604234 --> 0.602529).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5654526
	speed: 0.1472s/iter; left time: 7881.8056s
	iters: 200, epoch: 4 | loss: 0.6325636
	speed: 0.0379s/iter; left time: 2023.6240s
	iters: 300, epoch: 4 | loss: 0.5573246
	speed: 0.0382s/iter; left time: 2039.6205s
	iters: 400, epoch: 4 | loss: 0.7047988
	speed: 0.0328s/iter; left time: 1746.5414s
	iters: 500, epoch: 4 | loss: 0.5010082
	speed: 0.0357s/iter; left time: 1899.1393s
Epoch: 4 cost time: 20.24158263206482
Epoch: 4, Steps: 553 | Train Loss: 0.5645820 Vali Loss: 0.5996109 Test Loss: 0.3117281
Validation loss decreased (0.602529 --> 0.599611).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4596055
	speed: 0.1611s/iter; left time: 8535.4080s
	iters: 200, epoch: 5 | loss: 0.5809196
	speed: 0.0375s/iter; left time: 1982.6913s
	iters: 300, epoch: 5 | loss: 0.4753587
	speed: 0.0387s/iter; left time: 2045.0478s
	iters: 400, epoch: 5 | loss: 0.5584398
	speed: 0.0413s/iter; left time: 2177.4915s
	iters: 500, epoch: 5 | loss: 0.6424786
	speed: 0.0439s/iter; left time: 2310.8205s
Epoch: 5 cost time: 22.453696250915527
Epoch: 5, Steps: 553 | Train Loss: 0.5634122 Vali Loss: 0.5989606 Test Loss: 0.3110402
Validation loss decreased (0.599611 --> 0.598961).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5537629
	speed: 0.1668s/iter; left time: 8747.1813s
	iters: 200, epoch: 6 | loss: 0.5450324
	speed: 0.0400s/iter; left time: 2093.5085s
	iters: 300, epoch: 6 | loss: 0.4734167
	speed: 0.0382s/iter; left time: 1996.5640s
	iters: 400, epoch: 6 | loss: 0.5409981
	speed: 0.0446s/iter; left time: 2324.5714s
	iters: 500, epoch: 6 | loss: 0.6313863
	speed: 0.0357s/iter; left time: 1858.2134s
Epoch: 6 cost time: 22.728780508041382
Epoch: 6, Steps: 553 | Train Loss: 0.5628697 Vali Loss: 0.5973507 Test Loss: 0.3102983
Validation loss decreased (0.598961 --> 0.597351).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4219749
	speed: 0.1740s/iter; left time: 9028.8489s
	iters: 200, epoch: 7 | loss: 0.6638070
	speed: 0.0424s/iter; left time: 2193.0435s
	iters: 300, epoch: 7 | loss: 0.6698473
	speed: 0.0419s/iter; left time: 2163.9394s
	iters: 400, epoch: 7 | loss: 0.7141546
	speed: 0.0415s/iter; left time: 2139.1313s
	iters: 500, epoch: 7 | loss: 0.5623380
	speed: 0.0414s/iter; left time: 2132.1612s
Epoch: 7 cost time: 24.1496160030365
Epoch: 7, Steps: 553 | Train Loss: 0.5622913 Vali Loss: 0.5967025 Test Loss: 0.3100087
Validation loss decreased (0.597351 --> 0.596703).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4891162
	speed: 0.1672s/iter; left time: 8580.4617s
	iters: 200, epoch: 8 | loss: 0.5283029
	speed: 0.0435s/iter; left time: 2226.0324s
	iters: 300, epoch: 8 | loss: 0.5359764
	speed: 0.0454s/iter; left time: 2321.2031s
	iters: 400, epoch: 8 | loss: 0.5835101
	speed: 0.0380s/iter; left time: 1938.2185s
	iters: 500, epoch: 8 | loss: 0.4479564
	speed: 0.0543s/iter; left time: 2764.6878s
Epoch: 8 cost time: 24.553635597229004
Epoch: 8, Steps: 553 | Train Loss: 0.5618181 Vali Loss: 0.5964186 Test Loss: 0.3097495
Validation loss decreased (0.596703 --> 0.596419).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5167162
	speed: 0.1661s/iter; left time: 8433.1875s
	iters: 200, epoch: 9 | loss: 0.5410661
	speed: 0.0411s/iter; left time: 2083.9214s
	iters: 300, epoch: 9 | loss: 0.5715798
	speed: 0.0397s/iter; left time: 2007.6830s
	iters: 400, epoch: 9 | loss: 0.5192923
	speed: 0.0344s/iter; left time: 1734.0687s
	iters: 500, epoch: 9 | loss: 0.5755938
	speed: 0.0380s/iter; left time: 1912.9358s
Epoch: 9 cost time: 21.986865520477295
Epoch: 9, Steps: 553 | Train Loss: 0.5615334 Vali Loss: 0.5961782 Test Loss: 0.3094277
Validation loss decreased (0.596419 --> 0.596178).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5969681
	speed: 0.1694s/iter; left time: 8506.4552s
	iters: 200, epoch: 10 | loss: 0.5129364
	speed: 0.0456s/iter; left time: 2283.3356s
	iters: 300, epoch: 10 | loss: 0.6859421
	speed: 0.0403s/iter; left time: 2017.2165s
	iters: 400, epoch: 10 | loss: 0.5946530
	speed: 0.0408s/iter; left time: 2035.9698s
	iters: 500, epoch: 10 | loss: 0.5719440
	speed: 0.0401s/iter; left time: 1996.9019s
Epoch: 10 cost time: 23.83477520942688
Epoch: 10, Steps: 553 | Train Loss: 0.5611767 Vali Loss: 0.5955079 Test Loss: 0.3091958
Validation loss decreased (0.596178 --> 0.595508).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5677429
	speed: 0.1789s/iter; left time: 8888.4567s
	iters: 200, epoch: 11 | loss: 0.4758641
	speed: 0.0376s/iter; left time: 1862.2574s
	iters: 300, epoch: 11 | loss: 0.5448963
	speed: 0.0393s/iter; left time: 1945.2516s
	iters: 400, epoch: 11 | loss: 0.5192056
	speed: 0.0418s/iter; left time: 2062.9889s
	iters: 500, epoch: 11 | loss: 0.5960631
	speed: 0.0352s/iter; left time: 1735.9411s
Epoch: 11 cost time: 23.058708429336548
Epoch: 11, Steps: 553 | Train Loss: 0.5609354 Vali Loss: 0.5940979 Test Loss: 0.3093790
Validation loss decreased (0.595508 --> 0.594098).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5393068
	speed: 0.1621s/iter; left time: 7964.3292s
	iters: 200, epoch: 12 | loss: 0.5068936
	speed: 0.0391s/iter; left time: 1917.9560s
	iters: 300, epoch: 12 | loss: 0.5262567
	speed: 0.0416s/iter; left time: 2036.6685s
	iters: 400, epoch: 12 | loss: 0.7292054
	speed: 0.0386s/iter; left time: 1884.1488s
	iters: 500, epoch: 12 | loss: 0.6605906
	speed: 0.0391s/iter; left time: 1905.9703s
Epoch: 12 cost time: 22.410284757614136
Epoch: 12, Steps: 553 | Train Loss: 0.5607726 Vali Loss: 0.5941178 Test Loss: 0.3092260
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6068569
	speed: 0.1655s/iter; left time: 8036.6992s
	iters: 200, epoch: 13 | loss: 0.5652117
	speed: 0.0389s/iter; left time: 1883.8421s
	iters: 300, epoch: 13 | loss: 0.5908566
	speed: 0.0404s/iter; left time: 1953.8089s
	iters: 400, epoch: 13 | loss: 0.4921702
	speed: 0.0476s/iter; left time: 2298.2453s
	iters: 500, epoch: 13 | loss: 0.5670151
	speed: 0.0425s/iter; left time: 2044.9612s
Epoch: 13 cost time: 24.174143314361572
Epoch: 13, Steps: 553 | Train Loss: 0.5605394 Vali Loss: 0.5944939 Test Loss: 0.3090974
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5628259
	speed: 0.1686s/iter; left time: 8096.6181s
	iters: 200, epoch: 14 | loss: 0.5013112
	speed: 0.0386s/iter; left time: 1849.1867s
	iters: 300, epoch: 14 | loss: 0.5215933
	speed: 0.0407s/iter; left time: 1944.0239s
	iters: 400, epoch: 14 | loss: 0.5726531
	speed: 0.0406s/iter; left time: 1936.6853s
	iters: 500, epoch: 14 | loss: 0.5168672
	speed: 0.0411s/iter; left time: 1955.1554s
Epoch: 14 cost time: 22.54652428627014
Epoch: 14, Steps: 553 | Train Loss: 0.5602513 Vali Loss: 0.5941522 Test Loss: 0.3088188
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H_FITS_custom_ftM_sl720_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.3086538016796112, mae:0.3305584490299225, rse:0.7310850024223328, corr:[0.4732453  0.47522724 0.4746493  0.4736264  0.4726756  0.4718292
 0.4709288  0.46982497 0.4685784  0.46733433 0.46630377 0.4656841
 0.46532208 0.46498924 0.4644661  0.46367025 0.4628197  0.4620675
 0.46162498 0.46140045 0.46117324 0.46064195 0.45965263 0.45810443
 0.45628315 0.454602   0.45343158 0.45288295 0.45282206 0.4528765
 0.45276    0.4522468  0.4513853  0.45028073 0.44923508 0.44838706
 0.44789618 0.4476052  0.4473344  0.446934   0.4463623  0.44572634
 0.44510123 0.44454536 0.44407824 0.44363803 0.44322369 0.44280207
 0.44218367 0.44147867 0.44078737 0.44011232 0.43959364 0.43924597
 0.43902174 0.43879387 0.43849254 0.43805996 0.43758124 0.43710917
 0.43674245 0.4364889  0.43633282 0.43615913 0.4358803  0.43546054
 0.43489033 0.43424174 0.43363696 0.43315437 0.43286967 0.43264022
 0.4325069  0.43236735 0.4321209  0.4317746  0.43140957 0.4311435
 0.43096423 0.4308381  0.43078044 0.43066886 0.43040785 0.43005833
 0.42970908 0.42940676 0.4292095  0.42903632 0.42898545 0.42897585
 0.428978   0.42894328 0.42877075 0.42848688 0.42814654 0.42779028
 0.42747486 0.42723373 0.4270614  0.42697683 0.42693302 0.42688048
 0.42679155 0.42661825 0.42634222 0.4259852  0.42557558 0.42520258
 0.4248464  0.4245478  0.4242742  0.42397907 0.42367026 0.4234321
 0.42325428 0.42310217 0.42299998 0.42293823 0.42286143 0.42269042
 0.42246756 0.42220572 0.42190078 0.42160532 0.4213392  0.42113093
 0.42095375 0.4207595  0.4205433  0.42026907 0.41990584 0.41948798
 0.4190811  0.41873366 0.4185009  0.41834113 0.41821715 0.41807547
 0.4177529  0.41733232 0.4168756  0.4164915  0.41620386 0.41605365
 0.41602916 0.41605753 0.4160486  0.41591457 0.415538   0.4148719
 0.4140632  0.41330507 0.41263884 0.41205353 0.41161224 0.41128042
 0.4111029  0.41089582 0.41066608 0.4103777  0.4100468  0.4096969
 0.40932143 0.40896448 0.40857565 0.40817192 0.40768912 0.40715444
 0.4066149  0.40610152 0.40562144 0.4052063  0.40485314 0.4045507
 0.40418753 0.40372992 0.40315187 0.40250432 0.40184048 0.4012155
 0.40073356 0.40035865 0.40004936 0.39974764 0.3994388  0.3990635
 0.39861318 0.39806125 0.39747497 0.39690363 0.3963713  0.3959031
 0.39551824 0.39511123 0.39470378 0.39425865 0.3937529  0.39321342
 0.39264113 0.39208364 0.39159516 0.39109936 0.39057902 0.38999763
 0.38943127 0.38888323 0.38836923 0.38793463 0.3875307  0.3872132
 0.38693205 0.38665852 0.38634625 0.38599563 0.38559058 0.38511333
 0.38457495 0.38415653 0.38381582 0.38353464 0.38334274 0.38323662
 0.3831154  0.382886   0.3825286  0.38203257 0.3814604  0.3809448
 0.38053432 0.38028568 0.3801714  0.38008076 0.37991053 0.37956172
 0.37902507 0.37836128 0.3775943  0.376841   0.37614554 0.37563342
 0.37536764 0.3752872  0.37516826 0.3749735  0.37468597 0.37429446
 0.37384695 0.37345675 0.37315798 0.37296817 0.37283936 0.37272322
 0.37250102 0.37210137 0.37159687 0.37097943 0.3704274  0.37006503
 0.36998588 0.37011695 0.37022325 0.37024528 0.37005267 0.36967427
 0.36913952 0.36852804 0.36794612 0.36751956 0.3673223  0.36724627
 0.3672229  0.36720538 0.3670541  0.36670318 0.3662636  0.36576933
 0.365254   0.36468396 0.36415312 0.36365363 0.36315218 0.36268532
 0.3622639  0.36178595 0.3612979  0.36079374 0.3602662  0.3596772
 0.35907146 0.35844406 0.35779524 0.35712627 0.3565441  0.3560506
 0.3555612  0.35508487 0.35460076 0.35410467 0.35353857 0.3529885
 0.35233545 0.3516715  0.3510114  0.35042107 0.3499105  0.34946522
 0.34908083 0.34872177 0.34836474 0.34794196 0.3475159  0.34700957
 0.3464283  0.34582478 0.34518227 0.34451893 0.3438908  0.34328082
 0.34269592 0.34209552 0.34148195 0.34089386 0.34035712 0.33980364
 0.33927193 0.33878082 0.33831996 0.33795524 0.3377242  0.3375222
 0.33729893 0.33693475 0.33636844 0.33564276 0.3347458  0.33380687
 0.33292192 0.33221844 0.33177936 0.33157432 0.33152428 0.3315568
 0.33156016 0.33140728 0.33110544 0.3307038  0.33029726 0.3299169
 0.3296359  0.32940745 0.32921806 0.32898688 0.32871395 0.32838058
 0.3280157  0.3276805  0.32745987 0.32734385 0.32732725 0.3274474
 0.32754478 0.3275876  0.32759428 0.32746574 0.3272288  0.32690814
 0.32651335 0.32610455 0.3257402  0.32538432 0.32507256 0.3248001
 0.32455805 0.32428443 0.32396343 0.3235851  0.32314676 0.32268035
 0.32227373 0.32203034 0.32195008 0.32201374 0.32216507 0.32223266
 0.3221579  0.32189104 0.3214657  0.32089984 0.3202839  0.3197178
 0.31922978 0.3189513  0.31883723 0.31885287 0.3189294  0.319004
 0.31902787 0.31897965 0.31884632 0.3186411  0.318397   0.31812456
 0.31789833 0.3176965  0.31756985 0.31749845 0.3174598  0.31745744
 0.31745625 0.31736848 0.31715256 0.31673798 0.3162169  0.31568518
 0.31521693 0.314853   0.3146426  0.31457323 0.3144858  0.3143267
 0.3140125  0.3136045  0.31310257 0.31264126 0.31224686 0.31197602
 0.3118158  0.31176403 0.31166422 0.31140065 0.31097123 0.31045854
 0.30988663 0.30932233 0.30884427 0.3084462  0.30806422 0.307634
 0.3070961  0.30645388 0.30575526 0.30500454 0.30421558 0.3032883
 0.30233315 0.30137926 0.30044454 0.29965204 0.29893625 0.2983529
 0.2979308  0.29760966 0.29737118 0.2971518  0.29693514 0.29661947
 0.29621333 0.29571053 0.29513705 0.2945188  0.29388705 0.29331532
 0.29278055 0.29230306 0.29191703 0.29162106 0.29141787 0.29128128
 0.2912218  0.29117876 0.29106614 0.29084554 0.29052916 0.29010746
 0.28961918 0.2891268  0.28866842 0.28829762 0.28804392 0.2878578
 0.28772706 0.28755406 0.28725976 0.2868583  0.28642312 0.28606805
 0.2857894  0.2856351  0.28561231 0.28566283 0.2857183  0.28570357
 0.2856262  0.2855087  0.28534922 0.28517306 0.28497246 0.2847779
 0.28457117 0.28434515 0.28406155 0.28372347 0.28340486 0.28313848
 0.28292403 0.2827692  0.2826828  0.28253677 0.28236696 0.28213722
 0.28189346 0.28163016 0.2813597  0.2810447  0.28070322 0.2803517
 0.28000215 0.27967513 0.2793752  0.27913848 0.2789464  0.2787448
 0.27850237 0.2781647  0.277692   0.27717325 0.27663508 0.2761947
 0.27590138 0.2757878  0.27583167 0.27594072 0.27602023 0.2758997
 0.275534   0.27495867 0.2742692  0.27365524 0.2732452  0.27308366
 0.27315173 0.2733813  0.273583   0.2736416  0.27347237 0.2730373
 0.27243868 0.27183545 0.27135417 0.27102116 0.27082705 0.2706564
 0.27043307 0.27010763 0.26970217 0.26921186 0.2687589  0.2683491
 0.2680149  0.26768354 0.26730487 0.26683712 0.26621675 0.26549643
 0.26479718 0.2642472  0.26392248 0.26382014 0.26387307 0.2639775
 0.26398417 0.26380393 0.26341313 0.2628195  0.26206642 0.2611948
 0.2606603  0.2600931  0.25964692 0.25954556 0.25937986 0.25910065
 0.25864837 0.2580391  0.25729895 0.2565366  0.25576782 0.25507733
 0.2544535  0.25388032 0.25332817 0.25280666 0.25228974 0.2517856
 0.2512766  0.2507656  0.25028104 0.24983427 0.24942577 0.24904084
 0.24868359 0.24831004 0.2479198  0.24749258 0.24703807 0.24661936
 0.24619608 0.24584883 0.24553196 0.24525544 0.2450412  0.24483676
 0.24460827 0.24435459 0.24401169 0.24358791 0.243135   0.24260367
 0.24211633 0.24176492 0.24151516 0.2413533  0.24125682 0.2412019
 0.24107784 0.2408889  0.24062622 0.24031284 0.24003789 0.23976383
 0.23953928 0.23933949 0.23920633 0.23912859 0.23912998 0.23922327
 0.23939702 0.23961155 0.23980464 0.23990183 0.23989566 0.23973861
 0.2394648  0.239085   0.23862849 0.23809311 0.237599   0.23719355
 0.23695396 0.23682502 0.23687063 0.23698168 0.23720062 0.2374241
 0.23764974 0.23780002 0.23783061 0.23772052 0.23747356 0.23713069
 0.23675601 0.23637289 0.23602515 0.23572652 0.2355084  0.23535252
 0.23520717 0.23503359 0.23477252 0.23446126 0.23413233 0.23377371
 0.23346286 0.23321377 0.23297113 0.23272754 0.2324501  0.23213552
 0.23173995 0.23126666 0.23074289 0.23017904 0.22969465 0.22934307
 0.22917752 0.22921136 0.22941872 0.22964437 0.22974508 0.22962673
 0.22927493 0.22872882 0.228202   0.22785322 0.22772434 0.22774987
 0.22782503 0.22776084 0.22740611 0.22671294 0.22581777 0.2249309
 0.22433737 0.22413021 0.2241675  0.22421066 0.22403015 0.2235641
 0.22288223 0.2222688  0.2220667  0.22246258 0.22320293 0.22396384
 0.22427621 0.22420175 0.22392082 0.22400095 0.22456257 0.22447151]
