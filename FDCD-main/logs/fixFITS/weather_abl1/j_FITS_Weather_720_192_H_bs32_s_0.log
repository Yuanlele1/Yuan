Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=82, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j192_H', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=3, pred_len=192, root_path='./dataset/', seed=0, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Weather_720_j192_H_FITS_custom_ftM_sl720_ll48_pl192_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35976
val 5079
test 10348
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=82, out_features=103, bias=True)
    (1): Linear(in_features=82, out_features=103, bias=True)
    (2): Linear(in_features=82, out_features=103, bias=True)
    (3): Linear(in_features=82, out_features=103, bias=True)
    (4): Linear(in_features=82, out_features=103, bias=True)
    (5): Linear(in_features=82, out_features=103, bias=True)
    (6): Linear(in_features=82, out_features=103, bias=True)
    (7): Linear(in_features=82, out_features=103, bias=True)
    (8): Linear(in_features=82, out_features=103, bias=True)
    (9): Linear(in_features=82, out_features=103, bias=True)
    (10): Linear(in_features=82, out_features=103, bias=True)
    (11): Linear(in_features=82, out_features=103, bias=True)
    (12): Linear(in_features=82, out_features=103, bias=True)
    (13): Linear(in_features=82, out_features=103, bias=True)
    (14): Linear(in_features=82, out_features=103, bias=True)
    (15): Linear(in_features=82, out_features=103, bias=True)
    (16): Linear(in_features=82, out_features=103, bias=True)
    (17): Linear(in_features=82, out_features=103, bias=True)
    (18): Linear(in_features=82, out_features=103, bias=True)
    (19): Linear(in_features=82, out_features=103, bias=True)
    (20): Linear(in_features=82, out_features=103, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11351424.0
params:  179529.0
Trainable parameters:  179529
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4861712
	speed: 0.0394s/iter; left time: 2208.7530s
	iters: 200, epoch: 1 | loss: 0.6763697
	speed: 0.0335s/iter; left time: 1873.7029s
	iters: 300, epoch: 1 | loss: 0.3992760
	speed: 0.0286s/iter; left time: 1596.5201s
	iters: 400, epoch: 1 | loss: 0.3396952
	speed: 0.0365s/iter; left time: 2039.1357s
	iters: 500, epoch: 1 | loss: 0.3680902
	speed: 0.0295s/iter; left time: 1644.9519s
Epoch: 1 cost time: 19.082821130752563
Epoch: 1, Steps: 562 | Train Loss: 0.5220183 Vali Loss: 0.4609751 Test Loss: 0.2034075
Validation loss decreased (inf --> 0.460975).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.8219625
	speed: 0.1450s/iter; left time: 8054.5765s
	iters: 200, epoch: 2 | loss: 0.3970357
	speed: 0.0328s/iter; left time: 1817.9321s
	iters: 300, epoch: 2 | loss: 0.3261343
	speed: 0.0343s/iter; left time: 1898.2908s
	iters: 400, epoch: 2 | loss: 0.5112540
	speed: 0.0460s/iter; left time: 2538.5384s
	iters: 500, epoch: 2 | loss: 0.6158341
	speed: 0.0329s/iter; left time: 1815.4420s
Epoch: 2 cost time: 20.807210445404053
Epoch: 2, Steps: 562 | Train Loss: 0.4556527 Vali Loss: 0.4479584 Test Loss: 0.1952037
Validation loss decreased (0.460975 --> 0.447958).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6142768
	speed: 0.1563s/iter; left time: 8593.9446s
	iters: 200, epoch: 3 | loss: 0.5921926
	speed: 0.0320s/iter; left time: 1758.3954s
	iters: 300, epoch: 3 | loss: 0.5818869
	speed: 0.0333s/iter; left time: 1826.3189s
	iters: 400, epoch: 3 | loss: 0.9115137
	speed: 0.0361s/iter; left time: 1971.6109s
	iters: 500, epoch: 3 | loss: 0.6064447
	speed: 0.0453s/iter; left time: 2471.2120s
Epoch: 3 cost time: 21.303759574890137
Epoch: 3, Steps: 562 | Train Loss: 0.4502072 Vali Loss: 0.4431852 Test Loss: 0.1921525
Validation loss decreased (0.447958 --> 0.443185).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3434040
	speed: 0.1444s/iter; left time: 7857.5006s
	iters: 200, epoch: 4 | loss: 0.3427519
	speed: 0.0396s/iter; left time: 2150.6993s
	iters: 300, epoch: 4 | loss: 0.5167974
	speed: 0.0361s/iter; left time: 1957.6451s
	iters: 400, epoch: 4 | loss: 0.3748485
	speed: 0.0336s/iter; left time: 1819.2187s
	iters: 500, epoch: 4 | loss: 0.3531919
	speed: 0.0384s/iter; left time: 2073.5912s
Epoch: 4 cost time: 20.80011534690857
Epoch: 4, Steps: 562 | Train Loss: 0.4477879 Vali Loss: 0.4405122 Test Loss: 0.1900310
Validation loss decreased (0.443185 --> 0.440512).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5996000
	speed: 0.1398s/iter; left time: 7527.6596s
	iters: 200, epoch: 5 | loss: 0.4084125
	speed: 0.0282s/iter; left time: 1516.1247s
	iters: 300, epoch: 5 | loss: 0.3687910
	speed: 0.0314s/iter; left time: 1686.8776s
	iters: 400, epoch: 5 | loss: 0.3574340
	speed: 0.0299s/iter; left time: 1602.8165s
	iters: 500, epoch: 5 | loss: 0.3745347
	speed: 0.0268s/iter; left time: 1432.0144s
Epoch: 5 cost time: 17.454188585281372
Epoch: 5, Steps: 562 | Train Loss: 0.4462973 Vali Loss: 0.4383204 Test Loss: 0.1890913
Validation loss decreased (0.440512 --> 0.438320).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2984236
	speed: 0.1450s/iter; left time: 7728.0571s
	iters: 200, epoch: 6 | loss: 0.4161127
	speed: 0.0334s/iter; left time: 1776.3512s
	iters: 300, epoch: 6 | loss: 0.3419916
	speed: 0.0351s/iter; left time: 1865.3228s
	iters: 400, epoch: 6 | loss: 0.6584252
	speed: 0.0297s/iter; left time: 1571.6617s
	iters: 500, epoch: 6 | loss: 0.3166182
	speed: 0.0337s/iter; left time: 1783.1806s
Epoch: 6 cost time: 18.949573755264282
Epoch: 6, Steps: 562 | Train Loss: 0.4454085 Vali Loss: 0.4374493 Test Loss: 0.1882939
Validation loss decreased (0.438320 --> 0.437449).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.7544321
	speed: 0.1452s/iter; left time: 7657.7709s
	iters: 200, epoch: 7 | loss: 0.6154146
	speed: 0.0450s/iter; left time: 2368.7951s
	iters: 300, epoch: 7 | loss: 0.6592278
	speed: 0.0336s/iter; left time: 1766.4622s
	iters: 400, epoch: 7 | loss: 0.3729704
	speed: 0.0289s/iter; left time: 1513.0340s
	iters: 500, epoch: 7 | loss: 0.2981434
	speed: 0.0284s/iter; left time: 1488.4233s
Epoch: 7 cost time: 19.70089292526245
Epoch: 7, Steps: 562 | Train Loss: 0.4446327 Vali Loss: 0.4372039 Test Loss: 0.1881915
Validation loss decreased (0.437449 --> 0.437204).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3110721
	speed: 0.1371s/iter; left time: 7152.9135s
	iters: 200, epoch: 8 | loss: 0.8263150
	speed: 0.0310s/iter; left time: 1616.6200s
	iters: 300, epoch: 8 | loss: 0.5855774
	speed: 0.0313s/iter; left time: 1624.9977s
	iters: 400, epoch: 8 | loss: 0.3632073
	speed: 0.0313s/iter; left time: 1622.7183s
	iters: 500, epoch: 8 | loss: 0.3861113
	speed: 0.0327s/iter; left time: 1692.6392s
Epoch: 8 cost time: 18.391220808029175
Epoch: 8, Steps: 562 | Train Loss: 0.4441437 Vali Loss: 0.4367096 Test Loss: 0.1878110
Validation loss decreased (0.437204 --> 0.436710).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.8569334
	speed: 0.1543s/iter; left time: 7961.3498s
	iters: 200, epoch: 9 | loss: 0.3351116
	speed: 0.0374s/iter; left time: 1924.6538s
	iters: 300, epoch: 9 | loss: 0.3179178
	speed: 0.0313s/iter; left time: 1608.4492s
	iters: 400, epoch: 9 | loss: 0.6292984
	speed: 0.0329s/iter; left time: 1685.4416s
	iters: 500, epoch: 9 | loss: 0.3657190
	speed: 0.0287s/iter; left time: 1471.3174s
Epoch: 9 cost time: 18.86112666130066
Epoch: 9, Steps: 562 | Train Loss: 0.4437317 Vali Loss: 0.4368215 Test Loss: 0.1874843
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3490560
	speed: 0.1451s/iter; left time: 7408.5119s
	iters: 200, epoch: 10 | loss: 0.3256857
	speed: 0.0361s/iter; left time: 1841.3476s
	iters: 300, epoch: 10 | loss: 0.3079785
	speed: 0.0304s/iter; left time: 1546.3469s
	iters: 400, epoch: 10 | loss: 0.3362392
	speed: 0.0317s/iter; left time: 1607.8442s
	iters: 500, epoch: 10 | loss: 0.3647972
	speed: 0.0349s/iter; left time: 1766.6030s
Epoch: 10 cost time: 19.60843801498413
Epoch: 10, Steps: 562 | Train Loss: 0.4433008 Vali Loss: 0.4347814 Test Loss: 0.1869137
Validation loss decreased (0.436710 --> 0.434781).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5731465
	speed: 0.1307s/iter; left time: 6598.7411s
	iters: 200, epoch: 11 | loss: 0.3422089
	speed: 0.0309s/iter; left time: 1554.3806s
	iters: 300, epoch: 11 | loss: 0.5851117
	speed: 0.0288s/iter; left time: 1448.0736s
	iters: 400, epoch: 11 | loss: 0.6035827
	speed: 0.0289s/iter; left time: 1451.9589s
	iters: 500, epoch: 11 | loss: 0.6067411
	speed: 0.0312s/iter; left time: 1564.0585s
Epoch: 11 cost time: 16.947075843811035
Epoch: 11, Steps: 562 | Train Loss: 0.4426634 Vali Loss: 0.4366946 Test Loss: 0.1872162
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3151270
	speed: 0.1368s/iter; left time: 6830.4366s
	iters: 200, epoch: 12 | loss: 0.3161198
	speed: 0.0385s/iter; left time: 1919.4226s
	iters: 300, epoch: 12 | loss: 0.3702909
	speed: 0.0436s/iter; left time: 2167.0334s
	iters: 400, epoch: 12 | loss: 0.6066186
	speed: 0.0366s/iter; left time: 1814.3421s
	iters: 500, epoch: 12 | loss: 0.5681528
	speed: 0.0319s/iter; left time: 1581.3808s
Epoch: 12 cost time: 21.272501707077026
Epoch: 12, Steps: 562 | Train Loss: 0.4427362 Vali Loss: 0.4358232 Test Loss: 0.1867031
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3415895
	speed: 0.1409s/iter; left time: 6956.2084s
	iters: 200, epoch: 13 | loss: 0.4060372
	speed: 0.0344s/iter; left time: 1696.1782s
	iters: 300, epoch: 13 | loss: 0.4044327
	speed: 0.0350s/iter; left time: 1722.5904s
	iters: 400, epoch: 13 | loss: 0.3358561
	speed: 0.0328s/iter; left time: 1608.8361s
	iters: 500, epoch: 13 | loss: 0.4688832
	speed: 0.0366s/iter; left time: 1790.5145s
Epoch: 13 cost time: 20.452698469161987
Epoch: 13, Steps: 562 | Train Loss: 0.4423877 Vali Loss: 0.4358244 Test Loss: 0.1862825
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j192_H_FITS_custom_ftM_sl720_ll48_pl192_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10348
mse:0.18717436492443085, mae:0.2386006861925125, rse:0.5694968700408936, corr:[0.4768861  0.47801527 0.47708696 0.47602522 0.47541776 0.47518158
 0.47501945 0.47463596 0.47394904 0.47311595 0.472429   0.47208425
 0.47196394 0.4718251  0.4714469  0.4706435  0.46956402 0.4683107
 0.467237   0.46641997 0.46586594 0.46536455 0.4646992  0.46365157
 0.46230212 0.4609157  0.45979172 0.45909095 0.45886382 0.4588912
 0.4589329  0.45871207 0.45815986 0.45727682 0.45630988 0.45539102
 0.45471737 0.45421058 0.45378825 0.45337725 0.45295906 0.45252752
 0.4520673  0.45158222 0.45105442 0.45042917 0.44976714 0.44905353
 0.44819292 0.44731796 0.44658515 0.44596282 0.44557643 0.44532168
 0.44511592 0.44481292 0.4443584  0.4437782  0.4431734  0.44266227
 0.4423424  0.4421943  0.44214812 0.44207847 0.44184956 0.44141927
 0.44079125 0.44007796 0.43942913 0.43896064 0.4387493  0.43863314
 0.43860656 0.43854567 0.43831825 0.43793818 0.4374645  0.437044
 0.43670845 0.43647093 0.43636668 0.43629703 0.43616492 0.435987
 0.43580252 0.43565226 0.43558332 0.43552262 0.435547   0.4355372
 0.43546444 0.43526214 0.434828   0.43424857 0.4336548  0.43315697
 0.43287584 0.43286356 0.43304226 0.43331683 0.4335273  0.4335368
 0.43328083 0.43276036 0.43206522 0.43133366 0.4306789  0.4302468
 0.4300138  0.4299685  0.42997542 0.4298849  0.42962688 0.42931497
 0.428996   0.42871985 0.42860258 0.4286711  0.42883062 0.42892233
 0.428898   0.4286716  0.42821953 0.4276232  0.42697722 0.42641017
 0.42599437 0.4257413  0.42565808 0.42569214 0.425736   0.4257265
 0.42565703 0.42551485 0.42537257 0.42518494 0.42498097 0.4247682
 0.42444015 0.42405075 0.42360735 0.4231693  0.42274576 0.422349
 0.42202148 0.42176783 0.42160055 0.42148063 0.42129606 0.4209434
 0.42044026 0.41988894 0.41929314 0.4186581  0.41811615 0.41764057
 0.41734368 0.4170357  0.41673505 0.41642988 0.4161306  0.4158693
 0.41563678 0.4154759  0.41529945 0.41510656 0.4147907  0.41434905
 0.41375747 0.4130165  0.41215107 0.41126266 0.41049883 0.4099936
 0.40969148 0.4095628  0.409445   0.40923575 0.40881    0.40817618
 0.4074902  0.40684488 0.40636188 0.40604258 0.40573633 0.40521315
 0.4043204  0.4031668  0.40219435 0.40191358 0.4025242  0.40283915]
