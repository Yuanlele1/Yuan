Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j96_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=3, pred_len=96, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Weather_720_j96_H8_FITS_custom_ftM_sl720_ll48_pl96_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36072
val 5175
test 10444
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=58, out_features=65, bias=True)
    (1): Linear(in_features=58, out_features=65, bias=True)
    (2): Linear(in_features=58, out_features=65, bias=True)
    (3): Linear(in_features=58, out_features=65, bias=True)
    (4): Linear(in_features=58, out_features=65, bias=True)
    (5): Linear(in_features=58, out_features=65, bias=True)
    (6): Linear(in_features=58, out_features=65, bias=True)
    (7): Linear(in_features=58, out_features=65, bias=True)
    (8): Linear(in_features=58, out_features=65, bias=True)
    (9): Linear(in_features=58, out_features=65, bias=True)
    (10): Linear(in_features=58, out_features=65, bias=True)
    (11): Linear(in_features=58, out_features=65, bias=True)
    (12): Linear(in_features=58, out_features=65, bias=True)
    (13): Linear(in_features=58, out_features=65, bias=True)
    (14): Linear(in_features=58, out_features=65, bias=True)
    (15): Linear(in_features=58, out_features=65, bias=True)
    (16): Linear(in_features=58, out_features=65, bias=True)
    (17): Linear(in_features=58, out_features=65, bias=True)
    (18): Linear(in_features=58, out_features=65, bias=True)
    (19): Linear(in_features=58, out_features=65, bias=True)
    (20): Linear(in_features=58, out_features=65, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5066880.0
params:  80535.0
Trainable parameters:  80535
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5603976
	speed: 0.0273s/iter; left time: 1534.4622s
	iters: 200, epoch: 1 | loss: 0.3259351
	speed: 0.0223s/iter; left time: 1252.3599s
	iters: 300, epoch: 1 | loss: 0.3032131
	speed: 0.0242s/iter; left time: 1353.8885s
	iters: 400, epoch: 1 | loss: 0.7162461
	speed: 0.0229s/iter; left time: 1280.4627s
	iters: 500, epoch: 1 | loss: 0.3700106
	speed: 0.0258s/iter; left time: 1441.7373s
Epoch: 1 cost time: 13.770353078842163
Epoch: 1, Steps: 563 | Train Loss: 0.4640228 Vali Loss: 0.4014206 Test Loss: 0.1575090
Validation loss decreased (inf --> 0.401421).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3878435
	speed: 0.1688s/iter; left time: 9390.1091s
	iters: 200, epoch: 2 | loss: 0.4118259
	speed: 0.0240s/iter; left time: 1333.4455s
	iters: 300, epoch: 2 | loss: 0.3206607
	speed: 0.0243s/iter; left time: 1346.0108s
	iters: 400, epoch: 2 | loss: 0.2600796
	speed: 0.0239s/iter; left time: 1324.4306s
	iters: 500, epoch: 2 | loss: 0.3241949
	speed: 0.0251s/iter; left time: 1385.0966s
Epoch: 2 cost time: 14.231466054916382
Epoch: 2, Steps: 563 | Train Loss: 0.4016537 Vali Loss: 0.3831217 Test Loss: 0.1498346
Validation loss decreased (0.401421 --> 0.383122).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3704164
	speed: 0.0904s/iter; left time: 4977.7499s
	iters: 200, epoch: 3 | loss: 0.3417140
	speed: 0.0230s/iter; left time: 1262.1438s
	iters: 300, epoch: 3 | loss: 0.3097650
	speed: 0.0246s/iter; left time: 1349.1497s
	iters: 400, epoch: 3 | loss: 0.2752778
	speed: 0.0230s/iter; left time: 1261.1270s
	iters: 500, epoch: 3 | loss: 0.2968860
	speed: 0.0230s/iter; left time: 1256.5006s
Epoch: 3 cost time: 13.31509280204773
Epoch: 3, Steps: 563 | Train Loss: 0.3962850 Vali Loss: 0.3833103 Test Loss: 0.1480902
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3750460
	speed: 0.0860s/iter; left time: 4688.4426s
	iters: 200, epoch: 4 | loss: 0.2347548
	speed: 0.0254s/iter; left time: 1380.5366s
	iters: 300, epoch: 4 | loss: 0.3168519
	speed: 0.0259s/iter; left time: 1409.1523s
	iters: 400, epoch: 4 | loss: 0.3142547
	speed: 0.0244s/iter; left time: 1320.4943s
	iters: 500, epoch: 4 | loss: 0.2771583
	speed: 0.0234s/iter; left time: 1267.5486s
Epoch: 4 cost time: 13.991578340530396
Epoch: 4, Steps: 563 | Train Loss: 0.3939689 Vali Loss: 0.3835412 Test Loss: 0.1462728
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6984752
	speed: 0.0871s/iter; left time: 4697.0341s
	iters: 200, epoch: 5 | loss: 0.2667542
	speed: 0.0225s/iter; left time: 1209.7078s
	iters: 300, epoch: 5 | loss: 0.2193323
	speed: 0.0219s/iter; left time: 1176.2994s
	iters: 400, epoch: 5 | loss: 0.3495878
	speed: 0.0228s/iter; left time: 1224.5011s
	iters: 500, epoch: 5 | loss: 0.2535589
	speed: 0.0247s/iter; left time: 1324.4911s
Epoch: 5 cost time: 13.092668056488037
Epoch: 5, Steps: 563 | Train Loss: 0.3926634 Vali Loss: 0.3842072 Test Loss: 0.1458932
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j96_H8_FITS_custom_ftM_sl720_ll48_pl96_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10444
mse:0.1503206044435501, mae:0.2067979872226715, rse:0.510930061340332, corr:[0.4709356  0.47604522 0.47870767 0.47909537 0.47774118 0.4755426
 0.4734685  0.47210437 0.4715744  0.47163734 0.47191605 0.47211936
 0.4719919  0.47142398 0.4704954  0.46936867 0.46831146 0.46742073
 0.4668425  0.46648327 0.46627948 0.4660799  0.46576083 0.46513447
 0.46413994 0.4628078  0.46125555 0.45961905 0.45824632 0.45733088
 0.45689833 0.4568388  0.4570049  0.45714545 0.4571566  0.45690298
 0.45644248 0.4557618  0.45493376 0.4540673  0.45332056 0.45281088
 0.45252922 0.45236358 0.4522227  0.45199326 0.45166484 0.4512045
 0.45047447 0.44957897 0.44861197 0.4475763  0.4466449  0.44585434
 0.4452249  0.44472954 0.4443869  0.44411796 0.44389752 0.44366676
 0.4433722  0.4429423  0.4423913  0.44174293 0.44103575 0.44036505
 0.43977103 0.4393112  0.43902096 0.43887076 0.43884677 0.43878216
 0.4386895  0.43850645 0.43816534 0.4376949  0.43714073 0.43663114
 0.4361909  0.43583283 0.43564898 0.43556607 0.43544155 0.435208
 0.43486246 0.4343394  0.43368012 0.4328157  0.43201745 0.4314194
 0.43125063 0.43149677 0.4318456  0.4319343  0.43135875 0.42935753]
