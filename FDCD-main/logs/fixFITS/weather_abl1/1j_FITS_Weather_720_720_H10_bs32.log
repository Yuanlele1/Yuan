Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=144, batch_size=64, c_out=7, checkpoints='./checkpoints/', cut_freq=70, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='weather.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=21, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=True, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Weather_720_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=3, pred_len=720, root_path='./dataset/', seed=2021, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35448
val 4551
test 9820
Model(
  (freq_upsampler): ModuleList(
    (0): Linear(in_features=70, out_features=140, bias=True)
    (1): Linear(in_features=70, out_features=140, bias=True)
    (2): Linear(in_features=70, out_features=140, bias=True)
    (3): Linear(in_features=70, out_features=140, bias=True)
    (4): Linear(in_features=70, out_features=140, bias=True)
    (5): Linear(in_features=70, out_features=140, bias=True)
    (6): Linear(in_features=70, out_features=140, bias=True)
    (7): Linear(in_features=70, out_features=140, bias=True)
    (8): Linear(in_features=70, out_features=140, bias=True)
    (9): Linear(in_features=70, out_features=140, bias=True)
    (10): Linear(in_features=70, out_features=140, bias=True)
    (11): Linear(in_features=70, out_features=140, bias=True)
    (12): Linear(in_features=70, out_features=140, bias=True)
    (13): Linear(in_features=70, out_features=140, bias=True)
    (14): Linear(in_features=70, out_features=140, bias=True)
    (15): Linear(in_features=70, out_features=140, bias=True)
    (16): Linear(in_features=70, out_features=140, bias=True)
    (17): Linear(in_features=70, out_features=140, bias=True)
    (18): Linear(in_features=70, out_features=140, bias=True)
    (19): Linear(in_features=70, out_features=140, bias=True)
    (20): Linear(in_features=70, out_features=140, bias=True)
  )
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13171200.0
params:  208740.0
Trainable parameters:  208740
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8044301
	speed: 0.0411s/iter; left time: 2269.2888s
	iters: 200, epoch: 1 | loss: 0.7097409
	speed: 0.0490s/iter; left time: 2697.2851s
	iters: 300, epoch: 1 | loss: 0.6344911
	speed: 0.0429s/iter; left time: 2357.5178s
	iters: 400, epoch: 1 | loss: 0.5885920
	speed: 0.0466s/iter; left time: 2560.7552s
	iters: 500, epoch: 1 | loss: 0.5474780
	speed: 0.0389s/iter; left time: 2131.0275s
Epoch: 1 cost time: 23.974366188049316
Epoch: 1, Steps: 553 | Train Loss: 0.6879347 Vali Loss: 0.6174577 Test Loss: 0.3217325
Validation loss decreased (inf --> 0.617458).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7144024
	speed: 0.2035s/iter; left time: 11118.9903s
	iters: 200, epoch: 2 | loss: 0.4825799
	speed: 0.0446s/iter; left time: 2434.8946s
	iters: 300, epoch: 2 | loss: 0.6907299
	speed: 0.0469s/iter; left time: 2554.1236s
	iters: 400, epoch: 2 | loss: 0.5705237
	speed: 0.0486s/iter; left time: 2643.4958s
	iters: 500, epoch: 2 | loss: 0.6459236
	speed: 0.0447s/iter; left time: 2424.5890s
Epoch: 2 cost time: 27.365700483322144
Epoch: 2, Steps: 553 | Train Loss: 0.5758130 Vali Loss: 0.6045932 Test Loss: 0.3155768
Validation loss decreased (0.617458 --> 0.604593).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5830278
	speed: 0.1526s/iter; left time: 8257.4852s
	iters: 200, epoch: 3 | loss: 0.5900827
	speed: 0.0490s/iter; left time: 2643.1687s
	iters: 300, epoch: 3 | loss: 0.5336768
	speed: 0.0383s/iter; left time: 2062.5212s
	iters: 400, epoch: 3 | loss: 0.6252362
	speed: 0.0416s/iter; left time: 2235.7521s
	iters: 500, epoch: 3 | loss: 0.6322173
	speed: 0.0371s/iter; left time: 1993.0077s
Epoch: 3 cost time: 23.083972930908203
Epoch: 3, Steps: 553 | Train Loss: 0.5668810 Vali Loss: 0.6024517 Test Loss: 0.3135783
Validation loss decreased (0.604593 --> 0.602452).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4327554
	speed: 0.1842s/iter; left time: 9859.9196s
	iters: 200, epoch: 4 | loss: 0.6177879
	speed: 0.0419s/iter; left time: 2240.4598s
	iters: 300, epoch: 4 | loss: 0.5313635
	speed: 0.0324s/iter; left time: 1728.8604s
	iters: 400, epoch: 4 | loss: 0.5840181
	speed: 0.0417s/iter; left time: 2219.0004s
	iters: 500, epoch: 4 | loss: 0.4279472
	speed: 0.0490s/iter; left time: 2602.7402s
Epoch: 4 cost time: 23.70414924621582
Epoch: 4, Steps: 553 | Train Loss: 0.5650051 Vali Loss: 0.6000813 Test Loss: 0.3122967
Validation loss decreased (0.602452 --> 0.600081).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4602176
	speed: 0.1679s/iter; left time: 8897.1253s
	iters: 200, epoch: 5 | loss: 0.6833277
	speed: 0.0349s/iter; left time: 1846.4355s
	iters: 300, epoch: 5 | loss: 0.5580639
	speed: 0.0386s/iter; left time: 2035.9128s
	iters: 400, epoch: 5 | loss: 0.4908301
	speed: 0.0369s/iter; left time: 1946.1450s
	iters: 500, epoch: 5 | loss: 0.5710062
	speed: 0.0442s/iter; left time: 2323.2504s
Epoch: 5 cost time: 22.371126174926758
Epoch: 5, Steps: 553 | Train Loss: 0.5638004 Vali Loss: 0.5982038 Test Loss: 0.3111158
Validation loss decreased (0.600081 --> 0.598204).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5599698
	speed: 0.1901s/iter; left time: 9970.5297s
	iters: 200, epoch: 6 | loss: 0.4992825
	speed: 0.0550s/iter; left time: 2877.3255s
	iters: 300, epoch: 6 | loss: 0.5027092
	speed: 0.0383s/iter; left time: 1999.5321s
	iters: 400, epoch: 6 | loss: 0.5714244
	speed: 0.0489s/iter; left time: 2550.8793s
	iters: 500, epoch: 6 | loss: 0.5680634
	speed: 0.0340s/iter; left time: 1769.9916s
Epoch: 6 cost time: 25.15489625930786
Epoch: 6, Steps: 553 | Train Loss: 0.5628517 Vali Loss: 0.5963978 Test Loss: 0.3103568
Validation loss decreased (0.598204 --> 0.596398).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5104360
	speed: 0.1720s/iter; left time: 8925.4150s
	iters: 200, epoch: 7 | loss: 0.5852817
	speed: 0.0535s/iter; left time: 2770.0155s
	iters: 300, epoch: 7 | loss: 0.4535050
	speed: 0.0613s/iter; left time: 3168.5623s
	iters: 400, epoch: 7 | loss: 0.5617047
	speed: 0.0335s/iter; left time: 1728.4738s
	iters: 500, epoch: 7 | loss: 0.4837149
	speed: 0.0401s/iter; left time: 2065.7011s
Epoch: 7 cost time: 25.60103178024292
Epoch: 7, Steps: 553 | Train Loss: 0.5621622 Vali Loss: 0.5965281 Test Loss: 0.3101958
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5606779
	speed: 0.1634s/iter; left time: 8389.3652s
	iters: 200, epoch: 8 | loss: 0.6840612
	speed: 0.0576s/iter; left time: 2951.7887s
	iters: 300, epoch: 8 | loss: 0.4844505
	speed: 0.0363s/iter; left time: 1853.7695s
	iters: 400, epoch: 8 | loss: 0.4923600
	speed: 0.0408s/iter; left time: 2080.5388s
	iters: 500, epoch: 8 | loss: 0.6034741
	speed: 0.0492s/iter; left time: 2505.2792s
Epoch: 8 cost time: 26.260480165481567
Epoch: 8, Steps: 553 | Train Loss: 0.5621249 Vali Loss: 0.5968724 Test Loss: 0.3097864
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6019118
	speed: 0.2092s/iter; left time: 10623.3769s
	iters: 200, epoch: 9 | loss: 0.5688910
	speed: 0.0430s/iter; left time: 2177.3204s
	iters: 300, epoch: 9 | loss: 0.5684128
	speed: 0.0527s/iter; left time: 2663.6352s
	iters: 400, epoch: 9 | loss: 0.4392715
	speed: 0.0489s/iter; left time: 2466.2781s
	iters: 500, epoch: 9 | loss: 0.4993702
	speed: 0.0534s/iter; left time: 2690.7008s
Epoch: 9 cost time: 27.755828857421875
Epoch: 9, Steps: 553 | Train Loss: 0.5617026 Vali Loss: 0.5958898 Test Loss: 0.3096546
Validation loss decreased (0.596398 --> 0.595890).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6756970
	speed: 0.1578s/iter; left time: 7926.1964s
	iters: 200, epoch: 10 | loss: 0.5458773
	speed: 0.0521s/iter; left time: 2609.2858s
	iters: 300, epoch: 10 | loss: 0.4955173
	speed: 0.0351s/iter; left time: 1755.2313s
	iters: 400, epoch: 10 | loss: 0.5104192
	speed: 0.0507s/iter; left time: 2532.1548s
	iters: 500, epoch: 10 | loss: 0.7263004
	speed: 0.0419s/iter; left time: 2089.7919s
Epoch: 10 cost time: 25.26246666908264
Epoch: 10, Steps: 553 | Train Loss: 0.5614040 Vali Loss: 0.5947301 Test Loss: 0.3093076
Validation loss decreased (0.595890 --> 0.594730).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5377880
	speed: 0.1948s/iter; left time: 9675.9962s
	iters: 200, epoch: 11 | loss: 0.6465798
	speed: 0.0357s/iter; left time: 1772.0462s
	iters: 300, epoch: 11 | loss: 0.5264456
	speed: 0.0454s/iter; left time: 2247.2411s
	iters: 400, epoch: 11 | loss: 0.4882153
	speed: 0.0447s/iter; left time: 2208.8320s
	iters: 500, epoch: 11 | loss: 0.5043219
	speed: 0.0448s/iter; left time: 2206.5224s
Epoch: 11 cost time: 25.251875400543213
Epoch: 11, Steps: 553 | Train Loss: 0.5611133 Vali Loss: 0.5952423 Test Loss: 0.3091213
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6231477
	speed: 0.2208s/iter; left time: 10844.7733s
	iters: 200, epoch: 12 | loss: 0.6139098
	speed: 0.0368s/iter; left time: 1805.8029s
	iters: 300, epoch: 12 | loss: 0.7363285
	speed: 0.0482s/iter; left time: 2355.8259s
	iters: 400, epoch: 12 | loss: 0.6628449
	speed: 0.0411s/iter; left time: 2005.3356s
	iters: 500, epoch: 12 | loss: 0.6555755
	speed: 0.0318s/iter; left time: 1549.3255s
Epoch: 12 cost time: 22.942395210266113
Epoch: 12, Steps: 553 | Train Loss: 0.5610648 Vali Loss: 0.5944011 Test Loss: 0.3090658
Validation loss decreased (0.594730 --> 0.594401).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.7193362
	speed: 0.1512s/iter; left time: 7344.7964s
	iters: 200, epoch: 13 | loss: 0.4743749
	speed: 0.0397s/iter; left time: 1923.3453s
	iters: 300, epoch: 13 | loss: 0.6139078
	speed: 0.0467s/iter; left time: 2256.8742s
	iters: 400, epoch: 13 | loss: 0.5138223
	speed: 0.0370s/iter; left time: 1786.7526s
	iters: 500, epoch: 13 | loss: 0.7062979
	speed: 0.0372s/iter; left time: 1793.7160s
Epoch: 13 cost time: 23.302221059799194
Epoch: 13, Steps: 553 | Train Loss: 0.5607170 Vali Loss: 0.5941437 Test Loss: 0.3089672
Validation loss decreased (0.594401 --> 0.594144).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5342926
	speed: 0.1656s/iter; left time: 7951.1775s
	iters: 200, epoch: 14 | loss: 0.5230553
	speed: 0.0478s/iter; left time: 2290.9739s
	iters: 300, epoch: 14 | loss: 0.6274839
	speed: 0.0594s/iter; left time: 2838.0483s
	iters: 400, epoch: 14 | loss: 0.5518452
	speed: 0.0493s/iter; left time: 2354.0373s
	iters: 500, epoch: 14 | loss: 0.6572073
	speed: 0.0436s/iter; left time: 2074.3274s
Epoch: 14 cost time: 26.538517951965332
Epoch: 14, Steps: 553 | Train Loss: 0.5606153 Vali Loss: 0.5941853 Test Loss: 0.3088325
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4958985
	speed: 0.1640s/iter; left time: 7784.7992s
	iters: 200, epoch: 15 | loss: 0.7153580
	speed: 0.0362s/iter; left time: 1713.0244s
	iters: 300, epoch: 15 | loss: 0.5916940
	speed: 0.0343s/iter; left time: 1622.3620s
	iters: 400, epoch: 15 | loss: 0.5050627
	speed: 0.0393s/iter; left time: 1855.2906s
	iters: 500, epoch: 15 | loss: 0.5487574
	speed: 0.0456s/iter; left time: 2145.1464s
Epoch: 15 cost time: 22.101344347000122
Epoch: 15, Steps: 553 | Train Loss: 0.5603227 Vali Loss: 0.5945138 Test Loss: 0.3088368
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4305752
	speed: 0.1838s/iter; left time: 8619.4272s
	iters: 200, epoch: 16 | loss: 0.5841796
	speed: 0.0467s/iter; left time: 2186.6824s
	iters: 300, epoch: 16 | loss: 0.5773864
	speed: 0.0526s/iter; left time: 2455.9000s
	iters: 400, epoch: 16 | loss: 0.5737438
	speed: 0.0385s/iter; left time: 1792.2836s
	iters: 500, epoch: 16 | loss: 0.5095451
	speed: 0.0359s/iter; left time: 1667.4493s
Epoch: 16 cost time: 23.940482139587402
Epoch: 16, Steps: 553 | Train Loss: 0.5603230 Vali Loss: 0.5934553 Test Loss: 0.3085814
Validation loss decreased (0.594144 --> 0.593455).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5431986
	speed: 0.1571s/iter; left time: 7282.8382s
	iters: 200, epoch: 17 | loss: 0.4734311
	speed: 0.0359s/iter; left time: 1661.9256s
	iters: 300, epoch: 17 | loss: 0.5020053
	speed: 0.0503s/iter; left time: 2320.7398s
	iters: 400, epoch: 17 | loss: 0.5528862
	speed: 0.0455s/iter; left time: 2095.3283s
	iters: 500, epoch: 17 | loss: 0.6454817
	speed: 0.0364s/iter; left time: 1672.4304s
Epoch: 17 cost time: 23.02144694328308
Epoch: 17, Steps: 553 | Train Loss: 0.5602321 Vali Loss: 0.5935406 Test Loss: 0.3086581
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5114928
	speed: 0.1613s/iter; left time: 7386.2495s
	iters: 200, epoch: 18 | loss: 0.7335405
	speed: 0.0389s/iter; left time: 1778.9508s
	iters: 300, epoch: 18 | loss: 0.4923753
	speed: 0.0435s/iter; left time: 1984.4193s
	iters: 400, epoch: 18 | loss: 0.5703895
	speed: 0.0495s/iter; left time: 2252.5166s
	iters: 500, epoch: 18 | loss: 0.5243578
	speed: 0.0586s/iter; left time: 2662.4363s
Epoch: 18 cost time: 26.562089443206787
Epoch: 18, Steps: 553 | Train Loss: 0.5596100 Vali Loss: 0.5936954 Test Loss: 0.3086908
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4821431
	speed: 0.1565s/iter; left time: 7081.5051s
	iters: 200, epoch: 19 | loss: 0.5060256
	speed: 0.0532s/iter; left time: 2402.0665s
	iters: 300, epoch: 19 | loss: 0.4515913
	speed: 0.0408s/iter; left time: 1838.3836s
	iters: 400, epoch: 19 | loss: 0.4385141
	speed: 0.0321s/iter; left time: 1442.6353s
	iters: 500, epoch: 19 | loss: 0.7441760
	speed: 0.0412s/iter; left time: 1848.6423s
Epoch: 19 cost time: 22.66018271446228
Epoch: 19, Steps: 553 | Train Loss: 0.5598855 Vali Loss: 0.5934761 Test Loss: 0.3085765
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : Weather_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 9820
mse:0.30787578225135803, mae:0.32993122935295105, rse:0.7301629781723022, corr:[0.4739543  0.4747084  0.47406736 0.47330943 0.4726155  0.4719627
 0.4712698  0.47042206 0.46935526 0.46810198 0.46685585 0.46590388
 0.4652728  0.4649284  0.4647198  0.46439716 0.46386242 0.46300352
 0.46191823 0.46072233 0.45961455 0.4587311  0.45818588 0.45782042
 0.45752963 0.45713344 0.45650896 0.45561418 0.4545489  0.45342094
 0.45242146 0.4516151  0.45105463 0.45060676 0.45020422 0.44968814
 0.44910336 0.44839472 0.44761944 0.44688582 0.4463199  0.44600815
 0.44585058 0.44566548 0.44532266 0.44475263 0.44400406 0.44315228
 0.442139   0.44118145 0.44043398 0.43987587 0.43953398 0.4393067
 0.43908027 0.43873665 0.4382559  0.43762252 0.43693522 0.43624046
 0.43564954 0.43520853 0.4349532  0.434819   0.43474272 0.43467584
 0.43453878 0.4343247  0.43406215 0.43378216 0.43355682 0.43330455
 0.4331257  0.4329885  0.43282595 0.43262398 0.4323941  0.43217608
 0.43194202 0.43166447 0.43139857 0.4311161  0.4307981  0.43049797
 0.43026292 0.43008196 0.42994353 0.42973578 0.42953128 0.42928913
 0.4290586  0.42886576 0.42866924 0.4285036  0.428377   0.42824996
 0.4281103  0.42794377 0.42773718 0.42753547 0.42734453 0.42717475
 0.42703742 0.4269224  0.4267952  0.4266292  0.42640254 0.42614833
 0.42582896 0.42549857 0.42515406 0.42476016 0.42432982 0.42394957
 0.42361757 0.42332235 0.42311195 0.42299548 0.42293343 0.42284915
 0.42276043 0.4226535  0.42248407 0.42227104 0.42199543 0.4216758
 0.42133594 0.4209716  0.42063513 0.42033    0.42002562 0.41973367
 0.41947064 0.41922817 0.4190283  0.4188365  0.41865775 0.4185045
 0.4182763  0.4180787  0.4179295  0.41783845 0.41771606 0.4175293
 0.41727605 0.41696343 0.41662544 0.4162853  0.41588175 0.41534865
 0.41470683 0.41403463 0.41333163 0.4125976  0.4119356  0.41133076
 0.41087994 0.41046244 0.41014507 0.40992683 0.40981722 0.40975434
 0.40964988 0.40947098 0.40914547 0.40870342 0.40811932 0.40746036
 0.406815   0.40621853 0.40568072 0.405215   0.4048078  0.404435
 0.40402117 0.4035495  0.40300825 0.4024449  0.4018913  0.4013541
 0.40089935 0.40048346 0.4000709  0.3996397  0.399208   0.39874756
 0.39825478 0.39769244 0.39710346 0.39652532 0.3959622  0.39543867
 0.39499965 0.39454642 0.39414468 0.39375475 0.39336005 0.3929741
 0.3925776  0.39220327 0.39190868 0.39162213 0.39132977 0.3909713
 0.39058107 0.39013734 0.38964126 0.38911995 0.38853067 0.38795123
 0.38738653 0.38686663 0.38638863 0.3859719  0.3855862  0.38518408
 0.38472834 0.38432992 0.38391924 0.38348407 0.3830907  0.3828217
 0.3826356  0.38246918 0.3822918  0.38205576 0.38174635 0.38140482
 0.38100713 0.38058308 0.3801342  0.3796447  0.37912852 0.3785898
 0.3780853  0.3776405  0.3771939  0.37676737 0.37631947 0.37590805
 0.37559196 0.3753765  0.37514734 0.37493902 0.37474215 0.37450102
 0.37417075 0.37379703 0.37337297 0.3729404  0.37251914 0.37214935
 0.37179878 0.37143746 0.37111846 0.37073234 0.37032768 0.36991084
 0.36954042 0.36920658 0.3688407  0.36853087 0.36824757 0.36806902
 0.36796194 0.3678841  0.3677924  0.36766964 0.36750835 0.36725482
 0.36694214 0.36664784 0.36634272 0.3659983  0.3656997  0.3654454
 0.36520648 0.36490497 0.36457634 0.36418608 0.36370948 0.36320448
 0.36271584 0.36217746 0.3616491  0.36112335 0.36059454 0.36002848
 0.35946184 0.35889032 0.3582965  0.35765475 0.35706523 0.35654435
 0.35601783 0.35549617 0.3549562  0.35438982 0.35373607 0.3530753
 0.35229158 0.35146964 0.35063568 0.34985942 0.349155   0.34851542
 0.3479424  0.34740224 0.34686598 0.34627455 0.34569305 0.34508705
 0.3444874  0.34396893 0.34350044 0.34306243 0.34266198 0.34224468
 0.3417945  0.34125465 0.3406276  0.3399581  0.3393122  0.3386748
 0.3381169  0.33765858 0.33723414 0.33687598 0.33659294 0.33629778
 0.33600077 0.33566827 0.3353159  0.3350171  0.33472973 0.33449554
 0.3342607  0.33400887 0.3337388  0.33338824 0.33294207 0.33246478
 0.3319871  0.33149406 0.3310422  0.33066273 0.3303829  0.3301632
 0.3300016  0.3298333  0.3296404  0.32937315 0.32906592 0.32870895
 0.32833007 0.3279793  0.3277039  0.32746655 0.3272773  0.32713836
 0.3269873  0.32681796 0.32664204 0.32639605 0.32609883 0.32577044
 0.3254133  0.32507095 0.32477748 0.32448897 0.32422808 0.3239847
 0.32376465 0.32353693 0.32331184 0.32310542 0.32290003 0.32267386
 0.3224218  0.32216805 0.32187644 0.32156524 0.32127002 0.32092193
 0.32056278 0.32020923 0.31990838 0.31964287 0.31942543 0.3192511
 0.31905842 0.31891602 0.31877562 0.31865034 0.31853455 0.31842706
 0.31831798 0.31820625 0.31805936 0.31787005 0.31764492 0.31736758
 0.31710672 0.31682295 0.3165655  0.31631425 0.31606632 0.31585774
 0.31570578 0.3155771  0.31545275 0.31526166 0.3150508  0.31484634
 0.31465492 0.31447852 0.31435865 0.31432468 0.3142931  0.31426805
 0.31417063 0.31404495 0.3138058  0.31350508 0.31310958 0.31264082
 0.3121418  0.31168693 0.31123772 0.31076    0.31026772 0.30982405
 0.30939102 0.30898532 0.30865246 0.30838755 0.30815583 0.30792972
 0.30764577 0.30727804 0.3068195  0.30621308 0.30546612 0.30447245
 0.3033864  0.30227444 0.3012054  0.30031383 0.29958263 0.2990361
 0.29864302 0.2982899  0.29793024 0.29751205 0.29705194 0.29650912
 0.2959443  0.29539925 0.2949056  0.29445967 0.29403633 0.29367772
 0.29333058 0.29299167 0.2926738  0.29236615 0.2920684  0.29176885
 0.29149657 0.2912423  0.29097146 0.29069248 0.29044613 0.29021773
 0.2900195  0.28986645 0.28975102 0.28968555 0.28966558 0.2896387
 0.2896222  0.2895665  0.28941965 0.28917593 0.28885084 0.28849342
 0.28807566 0.2876623  0.2873071  0.28702438 0.28682253 0.28665754
 0.2865307  0.28641868 0.28625903 0.2860411  0.2857539  0.28544566
 0.28514752 0.2848918  0.2846694  0.2844606  0.2842905  0.28412881
 0.28392884 0.28368813 0.2834408  0.2831123  0.2827858  0.2824506
 0.28216788 0.28193995 0.28177506 0.28161025 0.28144065 0.28126994
 0.28108242 0.2808854  0.2806674  0.2804615  0.28026673 0.28006083
 0.27986607 0.27965918 0.27940768 0.27914327 0.27882302 0.27846816
 0.27806157 0.2776265  0.2771896  0.2767755  0.27644363 0.27615178
 0.27591202 0.27570963 0.27550638 0.2752939  0.27505088 0.27475104
 0.27439374 0.27402243 0.27360234 0.27318627 0.2728016  0.27243635
 0.27211675 0.271867   0.27168086 0.27149966 0.2713102  0.27104086
 0.27068597 0.2702469  0.26976317 0.269223   0.26870754 0.2682109
 0.2677781  0.2673865  0.26704392 0.2667467  0.26641107 0.26601633
 0.265573   0.26510394 0.26462954 0.2641769  0.26377296 0.26347083
 0.26325503 0.2631124  0.26300785 0.26286495 0.2626075  0.26215392
 0.26173267 0.26102152 0.26015064 0.25952435 0.25887123 0.25824136
 0.25760514 0.2569542  0.25626087 0.25557312 0.2548781  0.25425607
 0.25373828 0.25334767 0.25308353 0.25295532 0.25290853 0.25290775
 0.2528637  0.25272027 0.25246504 0.25209734 0.251639   0.25111628
 0.25057656 0.25000843 0.24943669 0.2488571  0.24828237 0.24774425
 0.24719542 0.24669875 0.24621236 0.2457507  0.24536026 0.24503385
 0.24477224 0.24458933 0.2444119  0.24421225 0.24398054 0.24362355
 0.24319847 0.24277176 0.24233052 0.24190475 0.24153604 0.24126437
 0.24100469 0.24076034 0.24049222 0.24017935 0.23988728 0.23957138
 0.23929591 0.2390757  0.23898016 0.23899473 0.23911594 0.2393305
 0.23959202 0.23982058 0.23993781 0.23988178 0.23968795 0.23936635
 0.23900323 0.2386521  0.23836294 0.23812741 0.23801874 0.23801726
 0.2381236  0.23822042 0.2383163  0.23827995 0.23816842 0.23792815
 0.23762234 0.237257   0.23688494 0.23655279 0.23631814 0.23622523
 0.23628984 0.2364592  0.23667493 0.23686136 0.2369951  0.2370335
 0.2369399  0.23671241 0.23633377 0.2358705  0.2353758  0.23484573
 0.23437087 0.2339681  0.23359759 0.23325828 0.23291606 0.23258741
 0.23226541 0.23198259 0.23178688 0.23165059 0.23160213 0.23157308
 0.23151676 0.23140466 0.23125532 0.23103824 0.230757   0.23043153
 0.23008369 0.2297018  0.22938204 0.2291523  0.22899078 0.22886918
 0.22879648 0.22873749 0.22866973 0.22857277 0.22843985 0.22820693
 0.22790359 0.22753185 0.22708529 0.22661617 0.22619174 0.22590292
 0.22577825 0.22581808 0.2259906  0.2262333  0.22631036 0.22618671
 0.22578833 0.22543308 0.2251844  0.22521912 0.22535095 0.22488557]
