Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=14, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=14, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1580544.0
params:  1890.0
Trainable parameters:  1890
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7391329
	speed: 0.0812s/iter; left time: 2126.9808s
	iters: 200, epoch: 1 | loss: 0.5483886
	speed: 0.0777s/iter; left time: 2026.8511s
Epoch: 1 cost time: 20.776592016220093
Epoch: 1, Steps: 263 | Train Loss: 0.7248204 Vali Loss: 0.3543110 Test Loss: 0.4934793
Validation loss decreased (inf --> 0.354311).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5433636
	speed: 0.3296s/iter; left time: 8550.2766s
	iters: 200, epoch: 2 | loss: 0.5805002
	speed: 0.0737s/iter; left time: 1905.4000s
Epoch: 2 cost time: 20.42325758934021
Epoch: 2, Steps: 263 | Train Loss: 0.5737696 Vali Loss: 0.3053977 Test Loss: 0.4325565
Validation loss decreased (0.354311 --> 0.305398).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5785682
	speed: 0.3291s/iter; left time: 8448.8978s
	iters: 200, epoch: 3 | loss: 0.7150033
	speed: 0.0749s/iter; left time: 1915.7686s
Epoch: 3 cost time: 20.65643882751465
Epoch: 3, Steps: 263 | Train Loss: 0.5388762 Vali Loss: 0.2934997 Test Loss: 0.4181474
Validation loss decreased (0.305398 --> 0.293500).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.7644726
	speed: 0.3364s/iter; left time: 8548.1700s
	iters: 200, epoch: 4 | loss: 0.6237868
	speed: 0.0753s/iter; left time: 1906.6055s
Epoch: 4 cost time: 20.859124183654785
Epoch: 4, Steps: 263 | Train Loss: 0.5299796 Vali Loss: 0.2907548 Test Loss: 0.4142228
Validation loss decreased (0.293500 --> 0.290755).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4593906
	speed: 0.3313s/iter; left time: 8331.6803s
	iters: 200, epoch: 5 | loss: 0.5841751
	speed: 0.0773s/iter; left time: 1935.0364s
Epoch: 5 cost time: 20.654409408569336
Epoch: 5, Steps: 263 | Train Loss: 0.5284117 Vali Loss: 0.2900223 Test Loss: 0.4129667
Validation loss decreased (0.290755 --> 0.290022).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4663292
	speed: 0.3271s/iter; left time: 8140.7357s
	iters: 200, epoch: 6 | loss: 0.5380960
	speed: 0.0750s/iter; left time: 1858.9850s
Epoch: 6 cost time: 20.2534761428833
Epoch: 6, Steps: 263 | Train Loss: 0.5261509 Vali Loss: 0.2892984 Test Loss: 0.4121720
Validation loss decreased (0.290022 --> 0.289298).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5890601
	speed: 0.3303s/iter; left time: 8132.4526s
	iters: 200, epoch: 7 | loss: 0.6072078
	speed: 0.0767s/iter; left time: 1880.0236s
Epoch: 7 cost time: 20.665558338165283
Epoch: 7, Steps: 263 | Train Loss: 0.5258129 Vali Loss: 0.2893550 Test Loss: 0.4119259
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3385124
	speed: 0.3338s/iter; left time: 8131.0075s
	iters: 200, epoch: 8 | loss: 0.7187994
	speed: 0.0779s/iter; left time: 1888.9910s
Epoch: 8 cost time: 21.02192449569702
Epoch: 8, Steps: 263 | Train Loss: 0.5255786 Vali Loss: 0.2893429 Test Loss: 0.4117029
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5984580
	speed: 0.3320s/iter; left time: 7999.5084s
	iters: 200, epoch: 9 | loss: 0.4405429
	speed: 0.0780s/iter; left time: 1872.3336s
Epoch: 9 cost time: 20.94045662879944
Epoch: 9, Steps: 263 | Train Loss: 0.5251355 Vali Loss: 0.2895390 Test Loss: 0.4117180
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5012961
	speed: 0.3296s/iter; left time: 7855.5782s
	iters: 200, epoch: 10 | loss: 0.5916488
	speed: 0.0747s/iter; left time: 1773.3238s
Epoch: 10 cost time: 20.26871943473816
Epoch: 10, Steps: 263 | Train Loss: 0.5248568 Vali Loss: 0.2895529 Test Loss: 0.4115832
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4835331
	speed: 0.3319s/iter; left time: 7824.2268s
	iters: 200, epoch: 11 | loss: 0.9106068
	speed: 0.0763s/iter; left time: 1789.8320s
Epoch: 11 cost time: 20.5549955368042
Epoch: 11, Steps: 263 | Train Loss: 0.5246723 Vali Loss: 0.2895420 Test Loss: 0.4114766
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5575310
	speed: 0.3318s/iter; left time: 7734.6220s
	iters: 200, epoch: 12 | loss: 0.4871854
	speed: 0.0750s/iter; left time: 1741.0023s
Epoch: 12 cost time: 20.656683444976807
Epoch: 12, Steps: 263 | Train Loss: 0.5244221 Vali Loss: 0.2896422 Test Loss: 0.4116057
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4855975
	speed: 0.3324s/iter; left time: 7660.2482s
	iters: 200, epoch: 13 | loss: 0.3790626
	speed: 0.0744s/iter; left time: 1707.7458s
Epoch: 13 cost time: 20.50520372390747
Epoch: 13, Steps: 263 | Train Loss: 0.5247404 Vali Loss: 0.2898281 Test Loss: 0.4115874
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6605954
	speed: 0.3307s/iter; left time: 7534.0816s
	iters: 200, epoch: 14 | loss: 0.5284082
	speed: 0.0767s/iter; left time: 1738.7220s
Epoch: 14 cost time: 20.86711883544922
Epoch: 14, Steps: 263 | Train Loss: 0.5239337 Vali Loss: 0.2897007 Test Loss: 0.4116293
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5065311
	speed: 0.3267s/iter; left time: 7357.8430s
	iters: 200, epoch: 15 | loss: 0.3444395
	speed: 0.0749s/iter; left time: 1679.1494s
Epoch: 15 cost time: 20.49180769920349
Epoch: 15, Steps: 263 | Train Loss: 0.5242942 Vali Loss: 0.2898796 Test Loss: 0.4115847
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.6993728
	speed: 0.3290s/iter; left time: 7321.7152s
	iters: 200, epoch: 16 | loss: 0.5036563
	speed: 0.0765s/iter; left time: 1694.1191s
Epoch: 16 cost time: 20.71754503250122
Epoch: 16, Steps: 263 | Train Loss: 0.5240740 Vali Loss: 0.2899434 Test Loss: 0.4116170
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5465385
	speed: 0.3297s/iter; left time: 7251.6057s
	iters: 200, epoch: 17 | loss: 0.4977997
	speed: 0.0746s/iter; left time: 1633.5095s
Epoch: 17 cost time: 20.364932775497437
Epoch: 17, Steps: 263 | Train Loss: 0.5248766 Vali Loss: 0.2897703 Test Loss: 0.4116007
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6452215
	speed: 0.3288s/iter; left time: 7144.2998s
	iters: 200, epoch: 18 | loss: 0.4648504
	speed: 0.0774s/iter; left time: 1674.4465s
Epoch: 18 cost time: 20.7402241230011
Epoch: 18, Steps: 263 | Train Loss: 0.5242221 Vali Loss: 0.2899693 Test Loss: 0.4116271
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5134627
	speed: 0.3278s/iter; left time: 7037.5950s
	iters: 200, epoch: 19 | loss: 0.4189568
	speed: 0.0734s/iter; left time: 1568.6775s
Epoch: 19 cost time: 20.207777976989746
Epoch: 19, Steps: 263 | Train Loss: 0.5237283 Vali Loss: 0.2898631 Test Loss: 0.4116161
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3824144
	speed: 0.3293s/iter; left time: 6983.5117s
	iters: 200, epoch: 20 | loss: 0.5210980
	speed: 0.0735s/iter; left time: 1551.0854s
Epoch: 20 cost time: 20.414212942123413
Epoch: 20, Steps: 263 | Train Loss: 0.5244291 Vali Loss: 0.2900243 Test Loss: 0.4116544
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.6655465
	speed: 0.3276s/iter; left time: 6860.5689s
	iters: 200, epoch: 21 | loss: 0.5799336
	speed: 0.0747s/iter; left time: 1556.5846s
Epoch: 21 cost time: 20.54550051689148
Epoch: 21, Steps: 263 | Train Loss: 0.5242127 Vali Loss: 0.2900603 Test Loss: 0.4117205
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.6751928
	speed: 0.3447s/iter; left time: 7127.1293s
	iters: 200, epoch: 22 | loss: 0.4289943
	speed: 0.0786s/iter; left time: 1616.4026s
Epoch: 22 cost time: 21.77477264404297
Epoch: 22, Steps: 263 | Train Loss: 0.5243283 Vali Loss: 0.2901679 Test Loss: 0.4117364
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5384098
	speed: 0.3585s/iter; left time: 7318.5213s
	iters: 200, epoch: 23 | loss: 0.5519578
	speed: 0.0796s/iter; left time: 1616.4449s
Epoch: 23 cost time: 21.970278024673462
Epoch: 23, Steps: 263 | Train Loss: 0.5240645 Vali Loss: 0.2898847 Test Loss: 0.4117595
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4398905
	speed: 0.3429s/iter; left time: 6910.0670s
	iters: 200, epoch: 24 | loss: 0.4047775
	speed: 0.0789s/iter; left time: 1581.5105s
Epoch: 24 cost time: 21.60639715194702
Epoch: 24, Steps: 263 | Train Loss: 0.5232277 Vali Loss: 0.2903337 Test Loss: 0.4117497
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3619780
	speed: 0.3424s/iter; left time: 6810.7132s
	iters: 200, epoch: 25 | loss: 0.5058085
	speed: 0.0811s/iter; left time: 1605.5090s
Epoch: 25 cost time: 21.763784408569336
Epoch: 25, Steps: 263 | Train Loss: 0.5236873 Vali Loss: 0.2902054 Test Loss: 0.4117607
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3415008
	speed: 0.3467s/iter; left time: 6803.5756s
	iters: 200, epoch: 26 | loss: 0.7400998
	speed: 0.0806s/iter; left time: 1573.0759s
Epoch: 26 cost time: 21.630491733551025
Epoch: 26, Steps: 263 | Train Loss: 0.5238107 Vali Loss: 0.2899442 Test Loss: 0.4117477
EarlyStopping counter: 20 out of 20
Early stopping
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=14, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1580544.0
params:  1890.0
Trainable parameters:  1890
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5393384
	speed: 0.0853s/iter; left time: 2235.4453s
	iters: 200, epoch: 1 | loss: 0.5784267
	speed: 0.0804s/iter; left time: 2097.9874s
Epoch: 1 cost time: 21.472527742385864
Epoch: 1, Steps: 263 | Train Loss: 0.5896709 Vali Loss: 0.2891395 Test Loss: 0.4114360
Validation loss decreased (inf --> 0.289140).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6218918
	speed: 0.3589s/iter; left time: 9308.2308s
	iters: 200, epoch: 2 | loss: 0.6645035
	speed: 0.0817s/iter; left time: 2110.5505s
Epoch: 2 cost time: 22.197094440460205
Epoch: 2, Steps: 263 | Train Loss: 0.5886849 Vali Loss: 0.2892472 Test Loss: 0.4113216
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6917176
	speed: 0.3530s/iter; left time: 9062.3832s
	iters: 200, epoch: 3 | loss: 0.5684048
	speed: 0.0793s/iter; left time: 2026.9983s
Epoch: 3 cost time: 21.744875192642212
Epoch: 3, Steps: 263 | Train Loss: 0.5886120 Vali Loss: 0.2892630 Test Loss: 0.4113828
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.7201790
	speed: 0.3479s/iter; left time: 8841.0644s
	iters: 200, epoch: 4 | loss: 0.6886587
	speed: 0.0800s/iter; left time: 2025.2213s
Epoch: 4 cost time: 21.62589979171753
Epoch: 4, Steps: 263 | Train Loss: 0.5886960 Vali Loss: 0.2896764 Test Loss: 0.4113959
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6772292
	speed: 0.3479s/iter; left time: 8750.4378s
	iters: 200, epoch: 5 | loss: 0.5412102
	speed: 0.0815s/iter; left time: 2042.5672s
Epoch: 5 cost time: 22.10996651649475
Epoch: 5, Steps: 263 | Train Loss: 0.5873032 Vali Loss: 0.2895310 Test Loss: 0.4114583
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6044475
	speed: 0.3555s/iter; left time: 8846.5116s
	iters: 200, epoch: 6 | loss: 0.6723447
	speed: 0.0825s/iter; left time: 2045.7731s
Epoch: 6 cost time: 22.14209294319153
Epoch: 6, Steps: 263 | Train Loss: 0.5877434 Vali Loss: 0.2899048 Test Loss: 0.4114830
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6602030
	speed: 0.3442s/iter; left time: 8475.5447s
	iters: 200, epoch: 7 | loss: 0.5799412
	speed: 0.0777s/iter; left time: 1904.2248s
Epoch: 7 cost time: 21.426684856414795
Epoch: 7, Steps: 263 | Train Loss: 0.5875220 Vali Loss: 0.2895581 Test Loss: 0.4114824
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5471024
	speed: 0.3485s/iter; left time: 8489.1267s
	iters: 200, epoch: 8 | loss: 0.5709791
	speed: 0.0774s/iter; left time: 1878.1116s
Epoch: 8 cost time: 21.44904589653015
Epoch: 8, Steps: 263 | Train Loss: 0.5876982 Vali Loss: 0.2896385 Test Loss: 0.4114606
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4734559
	speed: 0.3424s/iter; left time: 8251.2833s
	iters: 200, epoch: 9 | loss: 0.5012482
	speed: 0.0793s/iter; left time: 1903.3914s
Epoch: 9 cost time: 21.66167426109314
Epoch: 9, Steps: 263 | Train Loss: 0.5875343 Vali Loss: 0.2898227 Test Loss: 0.4114311
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.8061210
	speed: 0.3549s/iter; left time: 8458.4348s
	iters: 200, epoch: 10 | loss: 0.7357992
	speed: 0.0799s/iter; left time: 1896.9024s
Epoch: 10 cost time: 21.883838176727295
Epoch: 10, Steps: 263 | Train Loss: 0.5876474 Vali Loss: 0.2897422 Test Loss: 0.4114785
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5977822
	speed: 0.3579s/iter; left time: 8435.0699s
	iters: 200, epoch: 11 | loss: 0.6296905
	speed: 0.0792s/iter; left time: 1858.0836s
Epoch: 11 cost time: 22.073973178863525
Epoch: 11, Steps: 263 | Train Loss: 0.5872030 Vali Loss: 0.2897375 Test Loss: 0.4114552
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4986815
	speed: 0.3497s/iter; left time: 8150.4306s
	iters: 200, epoch: 12 | loss: 0.6041678
	speed: 0.0802s/iter; left time: 1861.3425s
Epoch: 12 cost time: 22.182714462280273
Epoch: 12, Steps: 263 | Train Loss: 0.5876916 Vali Loss: 0.2902216 Test Loss: 0.4116491
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4995974
	speed: 0.3589s/iter; left time: 8271.8734s
	iters: 200, epoch: 13 | loss: 0.6364160
	speed: 0.0779s/iter; left time: 1786.7613s
Epoch: 13 cost time: 21.376728534698486
Epoch: 13, Steps: 263 | Train Loss: 0.5872829 Vali Loss: 0.2899509 Test Loss: 0.4115800
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.7228280
	speed: 0.3395s/iter; left time: 7734.7756s
	iters: 200, epoch: 14 | loss: 0.4799141
	speed: 0.0802s/iter; left time: 1819.0794s
Epoch: 14 cost time: 21.464695692062378
Epoch: 14, Steps: 263 | Train Loss: 0.5873868 Vali Loss: 0.2897625 Test Loss: 0.4114893
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5779909
	speed: 0.3509s/iter; left time: 7901.1178s
	iters: 200, epoch: 15 | loss: 0.5330668
	speed: 0.0812s/iter; left time: 1820.7101s
Epoch: 15 cost time: 22.09153652191162
Epoch: 15, Steps: 263 | Train Loss: 0.5877258 Vali Loss: 0.2896084 Test Loss: 0.4115572
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5631731
	speed: 0.3525s/iter; left time: 7845.3097s
	iters: 200, epoch: 16 | loss: 0.4141416
	speed: 0.0797s/iter; left time: 1766.8044s
Epoch: 16 cost time: 22.051273345947266
Epoch: 16, Steps: 263 | Train Loss: 0.5875367 Vali Loss: 0.2900178 Test Loss: 0.4115382
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5912105
	speed: 0.3587s/iter; left time: 7889.8363s
	iters: 200, epoch: 17 | loss: 0.5583547
	speed: 0.0839s/iter; left time: 1836.6215s
Epoch: 17 cost time: 22.183205127716064
Epoch: 17, Steps: 263 | Train Loss: 0.5867571 Vali Loss: 0.2899754 Test Loss: 0.4115545
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.7126593
	speed: 0.3475s/iter; left time: 7550.4665s
	iters: 200, epoch: 18 | loss: 0.5089378
	speed: 0.0773s/iter; left time: 1671.6915s
Epoch: 18 cost time: 21.575560569763184
Epoch: 18, Steps: 263 | Train Loss: 0.5861319 Vali Loss: 0.2899411 Test Loss: 0.4115688
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.6255638
	speed: 0.3540s/iter; left time: 7598.9331s
	iters: 200, epoch: 19 | loss: 0.6039303
	speed: 0.0769s/iter; left time: 1642.9888s
Epoch: 19 cost time: 21.627158403396606
Epoch: 19, Steps: 263 | Train Loss: 0.5866686 Vali Loss: 0.2898979 Test Loss: 0.4115755
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4221749
	speed: 0.3513s/iter; left time: 7449.4121s
	iters: 200, epoch: 20 | loss: 0.5817588
	speed: 0.0793s/iter; left time: 1673.5850s
Epoch: 20 cost time: 21.851926565170288
Epoch: 20, Steps: 263 | Train Loss: 0.5871276 Vali Loss: 0.2901173 Test Loss: 0.4116569
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.6227216
	speed: 0.3594s/iter; left time: 7525.9399s
	iters: 200, epoch: 21 | loss: 0.6851684
	speed: 0.0790s/iter; left time: 1646.4345s
Epoch: 21 cost time: 21.874709844589233
Epoch: 21, Steps: 263 | Train Loss: 0.5870489 Vali Loss: 0.2901382 Test Loss: 0.4116503
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4091232419013977, mae:0.3985259532928467, rse:0.5141282677650452, corr:[0.54258275 0.5447795  0.54179204 0.53726155 0.53393596 0.53284997
 0.5329974  0.53276485 0.5316793  0.53027856 0.5292611  0.52881175
 0.5285966  0.52808315 0.5269922  0.5255494  0.52426046 0.5234847
 0.5230896  0.5225899  0.5216691  0.5203716  0.519024   0.5180765
 0.5176483  0.5174656  0.517168   0.51657236 0.51574165 0.51492214
 0.51424295 0.5136719  0.51305753 0.5123313  0.51153845 0.51080805
 0.5101593  0.50954664 0.5088528  0.50809896 0.5074186  0.5069611
 0.506755   0.5066658  0.5064694  0.5059404  0.5050959  0.50405663
 0.50300974 0.5021063  0.5013767  0.50078297 0.5001649  0.49943325
 0.4987156  0.49809238 0.49759912 0.49724224 0.49694052 0.49667093
 0.49642536 0.49620795 0.49610537 0.49612984 0.49616465 0.49614203
 0.49603218 0.49589956 0.4958377  0.49587095 0.49596316 0.49607173
 0.49610683 0.49604425 0.4958245  0.49550363 0.49519947 0.4949228
 0.49470487 0.49449763 0.4942566  0.4939596  0.49364322 0.49331346
 0.4929799  0.49263653 0.49228126 0.49195766 0.49163032 0.49126002
 0.49079022 0.49010637 0.48916346 0.48795512 0.48646396 0.48465696
 0.48254535 0.48032114 0.4781725  0.47620097 0.47438514 0.4727195
 0.47115764 0.46963713 0.4682266  0.46701416 0.4659408  0.4648465
 0.46359733 0.46222052 0.4608095  0.4593982  0.4580264  0.4566696
 0.45530477 0.45396858 0.45271075 0.45155987 0.45052496 0.44950718
 0.44852844 0.44759423 0.44668102 0.44576088 0.4448356  0.443897
 0.44297022 0.44204742 0.44103122 0.43994617 0.4388626  0.43783528
 0.43690547 0.4360991  0.43538043 0.434833   0.43436146 0.4339837
 0.43366107 0.43337244 0.43307555 0.43266314 0.43206823 0.4312273
 0.43018058 0.4291044  0.42811725 0.4273712  0.42678195 0.4262412
 0.4256209  0.42505682 0.42453527 0.4239867  0.42352504 0.42318633
 0.42294025 0.42278492 0.42258984 0.4224055  0.4222188  0.42201722
 0.4219345  0.42186865 0.42194477 0.42208895 0.42221424 0.42228526
 0.4223141  0.4223139  0.4222713  0.42220134 0.4221515  0.4221699
 0.422246   0.42234713 0.4223757  0.42226374 0.42197734 0.42157614
 0.4211478  0.42078105 0.4205794  0.4204769  0.4204241  0.42030242
 0.4200071  0.41941416 0.4184394  0.41701284 0.41513225 0.41284215
 0.4103778  0.40813315 0.40617004 0.40440395 0.40277866 0.40121302
 0.39967608 0.39810386 0.39664027 0.39541522 0.39441076 0.39344612
 0.392325   0.39103007 0.38961375 0.38818207 0.38683495 0.38564152
 0.38446644 0.3832052  0.38180915 0.38036734 0.37901244 0.37781253
 0.37680498 0.3758752  0.37497792 0.37392193 0.37264293 0.3712263
 0.36984035 0.36856776 0.36764228 0.3668994  0.36624804 0.36550322
 0.36458522 0.36354488 0.36246082 0.3615426  0.36087272 0.36045355
 0.36012584 0.35975805 0.35932148 0.3588384  0.35844454 0.3581492
 0.35787952 0.35751998 0.3569644  0.35654223 0.3562997  0.35618448
 0.3562573  0.35644245 0.3565921  0.35650197 0.35625622 0.35602885
 0.3559235  0.3560155  0.35628235 0.35655716 0.35675287 0.35687545
 0.35699442 0.3572134  0.35751438 0.35792854 0.35838616 0.35879225
 0.35914508 0.3593877  0.35959688 0.3597246  0.35983264 0.35992545
 0.36000723 0.3601159  0.3602792  0.3605358  0.36086306 0.36116934
 0.36141223 0.36156306 0.3616343  0.36173594 0.361841   0.3619585
 0.36197513 0.36173642 0.36115053 0.36018288 0.35888904 0.35735175
 0.3557973  0.35451567 0.35355586 0.35289377 0.35242176 0.35214442
 0.35208744 0.35211524 0.35220522 0.35235876 0.35247305 0.35238788
 0.35203776 0.35142598 0.35063422 0.349758   0.34893146 0.34820455
 0.34757158 0.3469456  0.34626836 0.34554413 0.34477305 0.34403604
 0.34333792 0.34273124 0.34217364 0.34158242 0.34093207 0.34025928
 0.33959934 0.3389954  0.33847222 0.33801368 0.3375736  0.33708653
 0.33651766 0.3359454  0.33548298 0.33513087 0.33493045 0.33483073
 0.33477664 0.3346761  0.3345138  0.33424008 0.3339004  0.3336194
 0.33343908 0.3333537  0.33339316 0.33358395 0.33380172 0.33389276
 0.33383715 0.3337634  0.33375323 0.3338503  0.3340817  0.33444527
 0.33491284 0.3353889  0.3358655  0.33632243 0.33670062 0.3369094
 0.33693734 0.3368842  0.33674067 0.33665305 0.33669078 0.33690342
 0.33725882 0.33771655 0.33818156 0.33857608 0.33886993 0.33910513
 0.33929682 0.33952105 0.33984503 0.34021658 0.34055975 0.3408112
 0.34097576 0.34109372 0.3412032  0.34135428 0.3415382  0.34171396
 0.341762   0.34154454 0.340998   0.34011385 0.33892855 0.3374847
 0.33596867 0.33460242 0.33351105 0.33263698 0.3318563  0.33113855
 0.33042887 0.3296648  0.3289566  0.32840818 0.32804817 0.32773846
 0.32725805 0.32649356 0.32540596 0.32409874 0.32275134 0.32149258
 0.32034576 0.31920466 0.31800202 0.3168114  0.3156375  0.31459478
 0.31371462 0.31293705 0.31214666 0.31123766 0.31021816 0.3092064
 0.30841154 0.30789328 0.30755457 0.3072453  0.30673212 0.30596405
 0.30505356 0.3041246  0.30333063 0.30275163 0.30237362 0.30215275
 0.3020093  0.30194017 0.30189615 0.30184612 0.30165052 0.30121133
 0.30047587 0.29949263 0.29846063 0.29767892 0.29720473 0.29702163
 0.29691166 0.29675946 0.29644734 0.29600427 0.2955848  0.29538682
 0.29544616 0.29568398 0.29592648 0.29596436 0.29572713 0.2952765
 0.29483366 0.29458284 0.29462206 0.29486725 0.29514584 0.29536533
 0.2954526  0.29539895 0.29523227 0.29506135 0.2950344  0.29513752
 0.29538313 0.29574436 0.29619625 0.2966105  0.29680616 0.2966501
 0.29612985 0.29534876 0.29452443 0.29387861 0.29356453 0.2935087
 0.2934508  0.29304442 0.29210272 0.29060534 0.28870922 0.2866204
 0.28459343 0.28286088 0.28141126 0.280164   0.27901354 0.27795818
 0.2770184  0.27622533 0.27571553 0.27556133 0.27565464 0.27574906
 0.2755184  0.2748284  0.27366316 0.27219546 0.27068135 0.26932162
 0.26824197 0.2673308  0.26642123 0.26542348 0.26434997 0.26333562
 0.26246166 0.26177835 0.26122847 0.2606343  0.25989145 0.2589652
 0.25795767 0.25700855 0.25623247 0.2556462  0.25505784 0.25431612
 0.2534403  0.25249588 0.25168762 0.2511948  0.25105235 0.251129
 0.25116482 0.2510249  0.25064135 0.25008953 0.24950027 0.24891904
 0.24835172 0.24768619 0.24698806 0.24635541 0.24592714 0.24582972
 0.246037   0.24631923 0.24643649 0.24619465 0.24569105 0.2452331
 0.24516468 0.24559185 0.24630487 0.24692658 0.24702558 0.24651845
 0.24567564 0.24503835 0.24502216 0.24562465 0.24636398 0.24689923
 0.24697646 0.24671265 0.24645755 0.24657056 0.24703744 0.24761012
 0.24793744 0.24793203 0.2477209  0.24757127 0.24767746 0.24801086
 0.24833359 0.24843645 0.24832815 0.24811448 0.24803898 0.2481454
 0.24824095 0.24794683 0.2469867  0.24532104 0.24312298 0.24067098
 0.23832902 0.23639756 0.23489945 0.23380601 0.23314862 0.23299342
 0.23326692 0.23383266 0.2344659  0.23495908 0.23517673 0.2350218
 0.23451018 0.2337514  0.23289964 0.23201792 0.23109065 0.23005816
 0.228918   0.22770761 0.2265355  0.2254966  0.22464156 0.22399719
 0.22349924 0.22306906 0.22259752 0.2219603  0.22118585 0.2202927
 0.2193084  0.21833342 0.21746475 0.21673952 0.21618067 0.21570848
 0.21527661 0.21476392 0.21418531 0.21354629 0.21302503 0.21267627
 0.21248199 0.21236342 0.21212381 0.21159472 0.21079534 0.20973939
 0.20868465 0.20793718 0.20772037 0.20798175 0.20848058 0.2088017
 0.20881145 0.20858133 0.2082497  0.20800334 0.20800695 0.20817286
 0.2083656  0.20847814 0.20851117 0.20857492 0.20880157 0.20914674
 0.20951337 0.20980619 0.2100179  0.21011783 0.2102637  0.21046422
 0.21070105 0.21088338 0.21091379 0.21082668 0.21075164 0.21082996
 0.2112108  0.21192335 0.21288814 0.21401079 0.21507205 0.21582712
 0.21615016 0.21606427 0.21578386 0.21556309 0.21558584 0.21575063
 0.21573682 0.21521047 0.21403643 0.21231979 0.21025652 0.20808715
 0.20601821 0.20416538 0.20253995 0.20121957 0.20042005 0.20030645
 0.20078328 0.20153233 0.20221265 0.20260282 0.20265634 0.20242284
 0.20195493 0.2012557  0.2003246  0.19920541 0.19801974 0.19692625
 0.19606607 0.19539331 0.19467615 0.19369704 0.19239804 0.19090244
 0.1894515  0.1882774  0.18736489 0.18646076 0.185354   0.18392026
 0.18236387 0.18100119 0.18003051 0.17939869 0.1787132  0.17772546
 0.17630045 0.17477418 0.17347787 0.17261808 0.17171775 0.1703075
 0.16830516 0.16632047 0.16536604 0.16590723 0.16658564 0.16449282]
