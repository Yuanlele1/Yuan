Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_180_720_FITS_ETTm2_ftM_sl180_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=34, out_features=170, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5178880.0
params:  5950.0
Trainable parameters:  5950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6967798
	speed: 0.0285s/iter; left time: 742.7860s
	iters: 200, epoch: 1 | loss: 0.5659743
	speed: 0.0292s/iter; left time: 758.0104s
Epoch: 1 cost time: 9.458194732666016
Epoch: 1, Steps: 262 | Train Loss: 0.6590501 Vali Loss: 0.2993146 Test Loss: 0.4154724
Validation loss decreased (inf --> 0.299315).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7450328
	speed: 0.1716s/iter; left time: 4433.3654s
	iters: 200, epoch: 2 | loss: 0.4329509
	speed: 0.0307s/iter; left time: 791.0971s
Epoch: 2 cost time: 11.578386545181274
Epoch: 2, Steps: 262 | Train Loss: 0.5787874 Vali Loss: 0.2853531 Test Loss: 0.3987368
Validation loss decreased (0.299315 --> 0.285353).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5573225
	speed: 0.1362s/iter; left time: 3484.2615s
	iters: 200, epoch: 3 | loss: 0.4461413
	speed: 0.0226s/iter; left time: 574.6884s
Epoch: 3 cost time: 7.823877811431885
Epoch: 3, Steps: 262 | Train Loss: 0.5679606 Vali Loss: 0.2815954 Test Loss: 0.3943158
Validation loss decreased (0.285353 --> 0.281595).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5997046
	speed: 0.1411s/iter; left time: 3572.0118s
	iters: 200, epoch: 4 | loss: 0.4230095
	speed: 0.0225s/iter; left time: 566.7065s
Epoch: 4 cost time: 7.436543941497803
Epoch: 4, Steps: 262 | Train Loss: 0.5642714 Vali Loss: 0.2799388 Test Loss: 0.3921869
Validation loss decreased (0.281595 --> 0.279939).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.7843366
	speed: 0.1078s/iter; left time: 2701.9087s
	iters: 200, epoch: 5 | loss: 0.6165162
	speed: 0.0430s/iter; left time: 1072.9845s
Epoch: 5 cost time: 9.303346157073975
Epoch: 5, Steps: 262 | Train Loss: 0.5616549 Vali Loss: 0.2789597 Test Loss: 0.3908776
Validation loss decreased (0.279939 --> 0.278960).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4141566
	speed: 0.1507s/iter; left time: 3735.7352s
	iters: 200, epoch: 6 | loss: 0.5547054
	speed: 0.0270s/iter; left time: 666.4227s
Epoch: 6 cost time: 7.469170808792114
Epoch: 6, Steps: 262 | Train Loss: 0.5610128 Vali Loss: 0.2783253 Test Loss: 0.3900942
Validation loss decreased (0.278960 --> 0.278325).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6074452
	speed: 0.1601s/iter; left time: 3927.5003s
	iters: 200, epoch: 7 | loss: 0.6454990
	speed: 0.0273s/iter; left time: 668.0388s
Epoch: 7 cost time: 7.890094041824341
Epoch: 7, Steps: 262 | Train Loss: 0.5596844 Vali Loss: 0.2780270 Test Loss: 0.3894398
Validation loss decreased (0.278325 --> 0.278027).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4509515
	speed: 0.1882s/iter; left time: 4565.9323s
	iters: 200, epoch: 8 | loss: 0.7356528
	speed: 0.0427s/iter; left time: 1032.0317s
Epoch: 8 cost time: 11.667299747467041
Epoch: 8, Steps: 262 | Train Loss: 0.5579722 Vali Loss: 0.2778862 Test Loss: 0.3890188
Validation loss decreased (0.278027 --> 0.277886).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4300404
	speed: 0.1503s/iter; left time: 3607.9930s
	iters: 200, epoch: 9 | loss: 0.4321373
	speed: 0.0369s/iter; left time: 883.1942s
Epoch: 9 cost time: 10.626592874526978
Epoch: 9, Steps: 262 | Train Loss: 0.5577697 Vali Loss: 0.2772714 Test Loss: 0.3885318
Validation loss decreased (0.277886 --> 0.277271).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4902381
	speed: 0.1383s/iter; left time: 3283.2889s
	iters: 200, epoch: 10 | loss: 0.5321550
	speed: 0.0302s/iter; left time: 713.9814s
Epoch: 10 cost time: 7.8018341064453125
Epoch: 10, Steps: 262 | Train Loss: 0.5575784 Vali Loss: 0.2773935 Test Loss: 0.3882983
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6260600
	speed: 0.1676s/iter; left time: 3935.2146s
	iters: 200, epoch: 11 | loss: 0.7521265
	speed: 0.0359s/iter; left time: 839.9644s
Epoch: 11 cost time: 12.785422086715698
Epoch: 11, Steps: 262 | Train Loss: 0.5574811 Vali Loss: 0.2770613 Test Loss: 0.3880320
Validation loss decreased (0.277271 --> 0.277061).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4846827
	speed: 0.1946s/iter; left time: 4517.3242s
	iters: 200, epoch: 12 | loss: 0.4633945
	speed: 0.0488s/iter; left time: 1127.2715s
Epoch: 12 cost time: 12.828318119049072
Epoch: 12, Steps: 262 | Train Loss: 0.5567048 Vali Loss: 0.2770692 Test Loss: 0.3878249
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6399590
	speed: 0.1827s/iter; left time: 4194.6184s
	iters: 200, epoch: 13 | loss: 0.3006952
	speed: 0.0219s/iter; left time: 500.1401s
Epoch: 13 cost time: 6.608452796936035
Epoch: 13, Steps: 262 | Train Loss: 0.5565911 Vali Loss: 0.2771391 Test Loss: 0.3876815
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6871318
	speed: 0.1299s/iter; left time: 2947.1382s
	iters: 200, epoch: 14 | loss: 0.7769588
	speed: 0.0258s/iter; left time: 583.8193s
Epoch: 14 cost time: 7.197327136993408
Epoch: 14, Steps: 262 | Train Loss: 0.5562675 Vali Loss: 0.2770607 Test Loss: 0.3875072
Validation loss decreased (0.277061 --> 0.277061).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4673728
	speed: 0.1352s/iter; left time: 3033.1103s
	iters: 200, epoch: 15 | loss: 0.5367944
	speed: 0.0334s/iter; left time: 746.5719s
Epoch: 15 cost time: 9.632768630981445
Epoch: 15, Steps: 262 | Train Loss: 0.5558628 Vali Loss: 0.2769732 Test Loss: 0.3874164
Validation loss decreased (0.277061 --> 0.276973).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5498776
	speed: 0.1704s/iter; left time: 3777.6510s
	iters: 200, epoch: 16 | loss: 0.4777365
	speed: 0.0461s/iter; left time: 1017.4840s
Epoch: 16 cost time: 11.604088306427002
Epoch: 16, Steps: 262 | Train Loss: 0.5553847 Vali Loss: 0.2766564 Test Loss: 0.3873300
Validation loss decreased (0.276973 --> 0.276656).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5331854
	speed: 0.1556s/iter; left time: 3409.3501s
	iters: 200, epoch: 17 | loss: 0.4437656
	speed: 0.0437s/iter; left time: 952.6146s
Epoch: 17 cost time: 11.062755584716797
Epoch: 17, Steps: 262 | Train Loss: 0.5555519 Vali Loss: 0.2765933 Test Loss: 0.3872022
Validation loss decreased (0.276656 --> 0.276593).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3784743
	speed: 0.1261s/iter; left time: 2729.0196s
	iters: 200, epoch: 18 | loss: 0.5624677
	speed: 0.0228s/iter; left time: 490.4212s
Epoch: 18 cost time: 8.004106760025024
Epoch: 18, Steps: 262 | Train Loss: 0.5557730 Vali Loss: 0.2766237 Test Loss: 0.3871652
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.6430790
	speed: 0.1387s/iter; left time: 2965.1961s
	iters: 200, epoch: 19 | loss: 0.6368164
	speed: 0.0378s/iter; left time: 804.3979s
Epoch: 19 cost time: 8.730383157730103
Epoch: 19, Steps: 262 | Train Loss: 0.5554045 Vali Loss: 0.2766659 Test Loss: 0.3870904
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5156837
	speed: 0.1580s/iter; left time: 3338.1940s
	iters: 200, epoch: 20 | loss: 0.4775803
	speed: 0.0326s/iter; left time: 685.9488s
Epoch: 20 cost time: 11.867692947387695
Epoch: 20, Steps: 262 | Train Loss: 0.5554719 Vali Loss: 0.2766723 Test Loss: 0.3869872
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3764346
	speed: 0.1248s/iter; left time: 2602.5945s
	iters: 200, epoch: 21 | loss: 0.5044218
	speed: 0.0380s/iter; left time: 788.0142s
Epoch: 21 cost time: 9.1810462474823
Epoch: 21, Steps: 262 | Train Loss: 0.5550047 Vali Loss: 0.2766200 Test Loss: 0.3869780
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4773316
	speed: 0.1574s/iter; left time: 3242.6329s
	iters: 200, epoch: 22 | loss: 0.6033143
	speed: 0.0377s/iter; left time: 773.4910s
Epoch: 22 cost time: 10.103727102279663
Epoch: 22, Steps: 262 | Train Loss: 0.5555566 Vali Loss: 0.2767613 Test Loss: 0.3869274
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5506246
	speed: 0.2075s/iter; left time: 4220.6831s
	iters: 200, epoch: 23 | loss: 0.6200449
	speed: 0.0421s/iter; left time: 851.6307s
Epoch: 23 cost time: 12.253773212432861
Epoch: 23, Steps: 262 | Train Loss: 0.5545473 Vali Loss: 0.2764126 Test Loss: 0.3868683
Validation loss decreased (0.276593 --> 0.276413).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.6182033
	speed: 0.1297s/iter; left time: 2603.5719s
	iters: 200, epoch: 24 | loss: 0.4043064
	speed: 0.0242s/iter; left time: 482.4032s
Epoch: 24 cost time: 7.426586866378784
Epoch: 24, Steps: 262 | Train Loss: 0.5550505 Vali Loss: 0.2766238 Test Loss: 0.3868071
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5319722
	speed: 0.1349s/iter; left time: 2673.7102s
	iters: 200, epoch: 25 | loss: 0.4097159
	speed: 0.0223s/iter; left time: 439.7148s
Epoch: 25 cost time: 6.600679636001587
Epoch: 25, Steps: 262 | Train Loss: 0.5547865 Vali Loss: 0.2763999 Test Loss: 0.3867955
Validation loss decreased (0.276413 --> 0.276400).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4100715
	speed: 0.1393s/iter; left time: 2722.7023s
	iters: 200, epoch: 26 | loss: 0.5940568
	speed: 0.0350s/iter; left time: 680.1242s
Epoch: 26 cost time: 9.235710620880127
Epoch: 26, Steps: 262 | Train Loss: 0.5544034 Vali Loss: 0.2763414 Test Loss: 0.3867671
Validation loss decreased (0.276400 --> 0.276341).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.6627690
	speed: 0.1476s/iter; left time: 2846.6981s
	iters: 200, epoch: 27 | loss: 0.5135154
	speed: 0.0388s/iter; left time: 745.0866s
Epoch: 27 cost time: 10.536926984786987
Epoch: 27, Steps: 262 | Train Loss: 0.5542306 Vali Loss: 0.2765222 Test Loss: 0.3867755
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5170795
	speed: 0.1503s/iter; left time: 2858.8882s
	iters: 200, epoch: 28 | loss: 0.3586415
	speed: 0.0261s/iter; left time: 494.8819s
Epoch: 28 cost time: 9.136303424835205
Epoch: 28, Steps: 262 | Train Loss: 0.5538560 Vali Loss: 0.2768605 Test Loss: 0.3867239
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.6597881
	speed: 0.1429s/iter; left time: 2681.0281s
	iters: 200, epoch: 29 | loss: 0.8834863
	speed: 0.0320s/iter; left time: 597.5936s
Epoch: 29 cost time: 8.776742219924927
Epoch: 29, Steps: 262 | Train Loss: 0.5548682 Vali Loss: 0.2762450 Test Loss: 0.3867359
Validation loss decreased (0.276341 --> 0.276245).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.6763868
	speed: 0.1286s/iter; left time: 2378.6562s
	iters: 200, epoch: 30 | loss: 0.4388235
	speed: 0.0447s/iter; left time: 823.2595s
Epoch: 30 cost time: 10.788087606430054
Epoch: 30, Steps: 262 | Train Loss: 0.5545443 Vali Loss: 0.2765111 Test Loss: 0.3866969
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.5078290
	speed: 0.1471s/iter; left time: 2683.7355s
	iters: 200, epoch: 31 | loss: 0.6265297
	speed: 0.0302s/iter; left time: 547.1180s
Epoch: 31 cost time: 9.525022029876709
Epoch: 31, Steps: 262 | Train Loss: 0.5543105 Vali Loss: 0.2765553 Test Loss: 0.3866887
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3342503
	speed: 0.1874s/iter; left time: 3370.0194s
	iters: 200, epoch: 32 | loss: 0.5462127
	speed: 0.0575s/iter; left time: 1027.2924s
Epoch: 32 cost time: 14.865221977233887
Epoch: 32, Steps: 262 | Train Loss: 0.5543980 Vali Loss: 0.2763443 Test Loss: 0.3866797
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3823143
	speed: 0.1530s/iter; left time: 2711.1233s
	iters: 200, epoch: 33 | loss: 0.3925309
	speed: 0.0290s/iter; left time: 511.1749s
Epoch: 33 cost time: 7.988693952560425
Epoch: 33, Steps: 262 | Train Loss: 0.5548048 Vali Loss: 0.2763594 Test Loss: 0.3866738
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4453323
	speed: 0.1479s/iter; left time: 2582.1283s
	iters: 200, epoch: 34 | loss: 0.4392676
	speed: 0.0234s/iter; left time: 405.5018s
Epoch: 34 cost time: 7.90701699256897
Epoch: 34, Steps: 262 | Train Loss: 0.5547161 Vali Loss: 0.2764276 Test Loss: 0.3866529
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5977006
	speed: 0.1752s/iter; left time: 3012.1854s
	iters: 200, epoch: 35 | loss: 0.4755200
	speed: 0.0437s/iter; left time: 747.3866s
Epoch: 35 cost time: 12.6222083568573
Epoch: 35, Steps: 262 | Train Loss: 0.5538642 Vali Loss: 0.2764565 Test Loss: 0.3866174
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.5433261
	speed: 0.1812s/iter; left time: 3067.8384s
	iters: 200, epoch: 36 | loss: 0.6882129
	speed: 0.0416s/iter; left time: 700.3801s
Epoch: 36 cost time: 11.615467548370361
Epoch: 36, Steps: 262 | Train Loss: 0.5542462 Vali Loss: 0.2761931 Test Loss: 0.3866200
Validation loss decreased (0.276245 --> 0.276193).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.7766008
	speed: 0.1841s/iter; left time: 3068.2887s
	iters: 200, epoch: 37 | loss: 0.6603667
	speed: 0.0230s/iter; left time: 381.0248s
Epoch: 37 cost time: 6.5430896282196045
Epoch: 37, Steps: 262 | Train Loss: 0.5542412 Vali Loss: 0.2761977 Test Loss: 0.3866081
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.5669317
	speed: 0.1241s/iter; left time: 2035.8837s
	iters: 200, epoch: 38 | loss: 0.6665672
	speed: 0.0280s/iter; left time: 456.3399s
Epoch: 38 cost time: 7.459667205810547
Epoch: 38, Steps: 262 | Train Loss: 0.5541373 Vali Loss: 0.2764820 Test Loss: 0.3865989
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4839113
	speed: 0.1389s/iter; left time: 2242.6374s
	iters: 200, epoch: 39 | loss: 0.7121791
	speed: 0.0243s/iter; left time: 390.1665s
Epoch: 39 cost time: 7.978299856185913
Epoch: 39, Steps: 262 | Train Loss: 0.5541116 Vali Loss: 0.2762946 Test Loss: 0.3865807
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.5237885
	speed: 0.1490s/iter; left time: 2365.8305s
	iters: 200, epoch: 40 | loss: 0.5626721
	speed: 0.0516s/iter; left time: 814.1686s
Epoch: 40 cost time: 11.68514633178711
Epoch: 40, Steps: 262 | Train Loss: 0.5544518 Vali Loss: 0.2765605 Test Loss: 0.3865810
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3924484
	speed: 0.1918s/iter; left time: 2996.1626s
	iters: 200, epoch: 41 | loss: 0.4834454
	speed: 0.0328s/iter; left time: 508.9479s
Epoch: 41 cost time: 10.6303071975708
Epoch: 41, Steps: 262 | Train Loss: 0.5542890 Vali Loss: 0.2765748 Test Loss: 0.3865742
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.6427619
	speed: 0.1294s/iter; left time: 1986.9100s
	iters: 200, epoch: 42 | loss: 0.5614535
	speed: 0.0261s/iter; left time: 398.1359s
Epoch: 42 cost time: 7.909475088119507
Epoch: 42, Steps: 262 | Train Loss: 0.5543129 Vali Loss: 0.2767035 Test Loss: 0.3865753
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4993809
	speed: 0.1261s/iter; left time: 1903.1364s
	iters: 200, epoch: 43 | loss: 0.4707828
	speed: 0.0252s/iter; left time: 377.4372s
Epoch: 43 cost time: 6.80694580078125
Epoch: 43, Steps: 262 | Train Loss: 0.5537778 Vali Loss: 0.2766222 Test Loss: 0.3865654
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.5938465
	speed: 0.1258s/iter; left time: 1865.5454s
	iters: 200, epoch: 44 | loss: 0.6779325
	speed: 0.0383s/iter; left time: 564.1170s
Epoch: 44 cost time: 8.591217041015625
Epoch: 44, Steps: 262 | Train Loss: 0.5537695 Vali Loss: 0.2762570 Test Loss: 0.3865614
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4873922
	speed: 0.1385s/iter; left time: 2018.2714s
	iters: 200, epoch: 45 | loss: 0.5411680
	speed: 0.0423s/iter; left time: 612.2958s
Epoch: 45 cost time: 10.584288358688354
Epoch: 45, Steps: 262 | Train Loss: 0.5543021 Vali Loss: 0.2760544 Test Loss: 0.3865556
Validation loss decreased (0.276193 --> 0.276054).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.3440828
	speed: 0.2028s/iter; left time: 2902.3111s
	iters: 200, epoch: 46 | loss: 0.3674943
	speed: 0.0558s/iter; left time: 792.9739s
Epoch: 46 cost time: 15.064897537231445
Epoch: 46, Steps: 262 | Train Loss: 0.5544710 Vali Loss: 0.2765529 Test Loss: 0.3865476
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4929961
	speed: 0.1426s/iter; left time: 2003.3878s
	iters: 200, epoch: 47 | loss: 0.4228479
	speed: 0.0284s/iter; left time: 396.4273s
Epoch: 47 cost time: 9.131084442138672
Epoch: 47, Steps: 262 | Train Loss: 0.5545977 Vali Loss: 0.2763113 Test Loss: 0.3865390
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4632637
	speed: 0.1487s/iter; left time: 2050.4404s
	iters: 200, epoch: 48 | loss: 0.5813128
	speed: 0.0228s/iter; left time: 312.1827s
Epoch: 48 cost time: 8.420027017593384
Epoch: 48, Steps: 262 | Train Loss: 0.5539089 Vali Loss: 0.2762769 Test Loss: 0.3865292
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.8202187
	speed: 0.1540s/iter; left time: 2083.4469s
	iters: 200, epoch: 49 | loss: 0.4000617
	speed: 0.0253s/iter; left time: 340.3113s
Epoch: 49 cost time: 7.280846357345581
Epoch: 49, Steps: 262 | Train Loss: 0.5544756 Vali Loss: 0.2761304 Test Loss: 0.3865296
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.5514525
	speed: 0.1591s/iter; left time: 2110.4101s
	iters: 200, epoch: 50 | loss: 0.6269557
	speed: 0.0289s/iter; left time: 380.0479s
Epoch: 50 cost time: 10.319189071655273
Epoch: 50, Steps: 262 | Train Loss: 0.5542956 Vali Loss: 0.2763263 Test Loss: 0.3865337
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.5037032
	speed: 0.1833s/iter; left time: 2383.4023s
	iters: 200, epoch: 51 | loss: 0.6165220
	speed: 0.0211s/iter; left time: 271.5777s
Epoch: 51 cost time: 7.922771215438843
Epoch: 51, Steps: 262 | Train Loss: 0.5542451 Vali Loss: 0.2764065 Test Loss: 0.3865228
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.6410693
	speed: 0.1094s/iter; left time: 1393.4980s
	iters: 200, epoch: 52 | loss: 0.5019500
	speed: 0.0211s/iter; left time: 266.2634s
Epoch: 52 cost time: 6.294977903366089
Epoch: 52, Steps: 262 | Train Loss: 0.5546007 Vali Loss: 0.2764535 Test Loss: 0.3865189
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4796647
	speed: 0.1154s/iter; left time: 1440.2063s
	iters: 200, epoch: 53 | loss: 0.7326536
	speed: 0.0226s/iter; left time: 280.0970s
Epoch: 53 cost time: 8.271871328353882
Epoch: 53, Steps: 262 | Train Loss: 0.5537295 Vali Loss: 0.2760937 Test Loss: 0.3865188
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4370372
	speed: 0.1479s/iter; left time: 1806.1514s
	iters: 200, epoch: 54 | loss: 0.4395331
	speed: 0.0293s/iter; left time: 354.5164s
Epoch: 54 cost time: 7.172208786010742
Epoch: 54, Steps: 262 | Train Loss: 0.5533531 Vali Loss: 0.2763094 Test Loss: 0.3865162
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.6162691
	speed: 0.1311s/iter; left time: 1566.9046s
	iters: 200, epoch: 55 | loss: 0.5665185
	speed: 0.0213s/iter; left time: 252.4563s
Epoch: 55 cost time: 7.333464622497559
Epoch: 55, Steps: 262 | Train Loss: 0.5535437 Vali Loss: 0.2762821 Test Loss: 0.3865083
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4319561
	speed: 0.1423s/iter; left time: 1663.4245s
	iters: 200, epoch: 56 | loss: 0.5381632
	speed: 0.0501s/iter; left time: 581.1756s
Epoch: 56 cost time: 10.417882680892944
Epoch: 56, Steps: 262 | Train Loss: 0.5543468 Vali Loss: 0.2764021 Test Loss: 0.3865103
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.5975509
	speed: 0.1510s/iter; left time: 1725.4861s
	iters: 200, epoch: 57 | loss: 0.4728493
	speed: 0.0306s/iter; left time: 346.8583s
Epoch: 57 cost time: 12.016279220581055
Epoch: 57, Steps: 262 | Train Loss: 0.5536195 Vali Loss: 0.2765116 Test Loss: 0.3865072
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.5509739
	speed: 0.1606s/iter; left time: 1793.1019s
	iters: 200, epoch: 58 | loss: 0.3954071
	speed: 0.0220s/iter; left time: 243.2302s
Epoch: 58 cost time: 7.824899673461914
Epoch: 58, Steps: 262 | Train Loss: 0.5534752 Vali Loss: 0.2764065 Test Loss: 0.3865051
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.5458431
	speed: 0.1096s/iter; left time: 1194.8681s
	iters: 200, epoch: 59 | loss: 0.5465406
	speed: 0.0238s/iter; left time: 257.3206s
Epoch: 59 cost time: 7.148101329803467
Epoch: 59, Steps: 262 | Train Loss: 0.5536588 Vali Loss: 0.2763273 Test Loss: 0.3864992
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.5625070
	speed: 0.1426s/iter; left time: 1517.1791s
	iters: 200, epoch: 60 | loss: 0.6942332
	speed: 0.0254s/iter; left time: 267.7804s
Epoch: 60 cost time: 7.311050653457642
Epoch: 60, Steps: 262 | Train Loss: 0.5537648 Vali Loss: 0.2762479 Test Loss: 0.3864974
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.4996993
	speed: 0.1740s/iter; left time: 1806.5996s
	iters: 200, epoch: 61 | loss: 0.4979900
	speed: 0.0371s/iter; left time: 381.4962s
Epoch: 61 cost time: 10.645845890045166
Epoch: 61, Steps: 262 | Train Loss: 0.5545456 Vali Loss: 0.2761207 Test Loss: 0.3864968
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.5286548
	speed: 0.1754s/iter; left time: 1774.9929s
	iters: 200, epoch: 62 | loss: 0.5775116
	speed: 0.0312s/iter; left time: 312.7476s
Epoch: 62 cost time: 10.802951097488403
Epoch: 62, Steps: 262 | Train Loss: 0.5530621 Vali Loss: 0.2763060 Test Loss: 0.3864923
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.4593417
	speed: 0.1564s/iter; left time: 1541.6775s
	iters: 200, epoch: 63 | loss: 0.7854471
	speed: 0.0275s/iter; left time: 267.8994s
Epoch: 63 cost time: 8.480337858200073
Epoch: 63, Steps: 262 | Train Loss: 0.5535295 Vali Loss: 0.2765682 Test Loss: 0.3864902
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.7422976
	speed: 0.1260s/iter; left time: 1208.7660s
	iters: 200, epoch: 64 | loss: 0.5499613
	speed: 0.0228s/iter; left time: 216.5549s
Epoch: 64 cost time: 7.515753507614136
Epoch: 64, Steps: 262 | Train Loss: 0.5536548 Vali Loss: 0.2762226 Test Loss: 0.3864886
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.6795518
	speed: 0.1375s/iter; left time: 1283.5195s
	iters: 200, epoch: 65 | loss: 0.6400425
	speed: 0.0288s/iter; left time: 266.3589s
Epoch: 65 cost time: 8.358536958694458
Epoch: 65, Steps: 262 | Train Loss: 0.5542769 Vali Loss: 0.2763863 Test Loss: 0.3864907
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_180_720_FITS_ETTm2_ftM_sl180_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3839208483695984, mae:0.38740360736846924, rse:0.49804115295410156, corr:[0.5491427  0.5459556  0.5414613  0.5406583  0.5405411  0.5387087
 0.5366053  0.53579044 0.5357813  0.535261   0.5339033  0.53271854
 0.5323366  0.5322748  0.53185874 0.5309397  0.53006625 0.5295817
 0.5291303  0.52829236 0.52717966 0.52629834 0.52593464 0.5258048
 0.5254199  0.52472687 0.52415323 0.5239183  0.52369845 0.5230266
 0.5219652  0.52104497 0.52061397 0.5205408  0.52036554 0.5198397
 0.51903296 0.5183284  0.51775706 0.51714504 0.51640785 0.5156896
 0.5152645  0.5151415  0.51500267 0.51455724 0.51373905 0.51277125
 0.51187074 0.5110852  0.51031125 0.50953287 0.50876576 0.5081069
 0.5076415  0.50727415 0.50675493 0.50618905 0.50571865 0.50545007
 0.5052498  0.505035   0.5047739  0.50445646 0.50424844 0.50418144
 0.50406134 0.5038382  0.5035647  0.50339454 0.5033111  0.5032278
 0.5030891  0.50282425 0.50257164 0.5023303  0.5020956  0.5017539
 0.5013309  0.50086576 0.50039756 0.49995857 0.4995656  0.49915922
 0.49873585 0.49831036 0.4978751  0.4974676  0.49705437 0.4966456
 0.4962768  0.49589327 0.49538177 0.494624   0.4935403  0.49203587
 0.4901019  0.4880384  0.4860626  0.48432136 0.4828712  0.4816278
 0.48041144 0.47911063 0.4776981  0.47631294 0.47507617 0.4739752
 0.47288993 0.4716891  0.47058257 0.469595   0.46868354 0.4676849
 0.46647215 0.46520323 0.46403033 0.46300045 0.46208322 0.46112642
 0.46012503 0.45914218 0.4582084  0.45731303 0.45642436 0.45548242
 0.45449847 0.453493   0.45250666 0.451538   0.45062187 0.4497332
 0.4488919  0.4480372  0.44730064 0.44668403 0.44609225 0.44554344
 0.44501612 0.4444997  0.44391868 0.4432435  0.44252315 0.44175598
 0.4409336  0.4400249  0.43897867 0.4379263  0.43705985 0.43638155
 0.43584368 0.43542263 0.43489859 0.43430933 0.43379056 0.43335104
 0.4329423  0.43247962 0.43188754 0.43134448 0.43103063 0.43087795
 0.4307357  0.43037352 0.4300821  0.4299987  0.43011194 0.43029165
 0.4303149  0.4301769  0.43002236 0.43006232 0.4302945  0.4304053
 0.43026468 0.42989692 0.429557   0.42937735 0.42932606 0.4292354
 0.4289547  0.42855266 0.42826343 0.4281072  0.4280261  0.4278649
 0.4276316  0.4273389  0.42692873 0.4262046  0.42498636 0.42326605
 0.42132533 0.41959438 0.41808927 0.41669124 0.41537088 0.4140888
 0.4128817  0.41175422 0.41060153 0.40944734 0.4082815  0.40735722
 0.40673655 0.4062631  0.4056796  0.40489796 0.4040176  0.40318266
 0.40245613 0.40178865 0.4010635  0.40021527 0.39930117 0.39835933
 0.39747977 0.39657402 0.39574733 0.39487252 0.39385644 0.39279222
 0.39169064 0.3906898  0.38992056 0.3892047  0.3884817  0.38768736
 0.3867506  0.38570556 0.38463932 0.38370067 0.38293308 0.3823813
 0.38193285 0.3815703  0.38130766 0.38094273 0.38054496 0.3800552
 0.37948722 0.37881556 0.37810722 0.3776166  0.37735277 0.37729895
 0.3773656  0.37745234 0.37758112 0.37769762 0.37784556 0.37801334
 0.3781076  0.37824053 0.37835142 0.37836733 0.3783022  0.37824482
 0.37815613 0.37798357 0.37781093 0.3777954  0.37788177 0.37788695
 0.37778053 0.37761986 0.3775015  0.37745768 0.37747768 0.37748078
 0.37735277 0.3771642  0.37694862 0.37679705 0.37666816 0.37655842
 0.37647602 0.37644407 0.37647194 0.37648126 0.37629917 0.3760855
 0.37596557 0.3759264  0.37585017 0.3755153  0.3747353  0.37347043
 0.37195343 0.3706953  0.36973122 0.36900288 0.36832815 0.36763757
 0.36702946 0.36641476 0.36575988 0.3650802  0.36451283 0.36409828
 0.36378112 0.36330333 0.36271644 0.36208412 0.36144215 0.36076042
 0.3600667  0.35933948 0.35853603 0.35773066 0.35696006 0.35625315
 0.35554898 0.35488027 0.35417992 0.35345653 0.35260227 0.3516898
 0.35081393 0.35003033 0.3493191  0.34860888 0.3479071  0.3472393
 0.34664893 0.34607416 0.34544915 0.34476435 0.34411496 0.34369105
 0.343541   0.34340888 0.34311393 0.34259072 0.34200495 0.34156898
 0.3411409  0.34053797 0.3397643  0.3391891  0.3389302  0.33891642
 0.33889362 0.33871868 0.33844322 0.3383386  0.3384097  0.33854163
 0.33846027 0.33817658 0.33791575 0.33788234 0.3380116  0.33805418
 0.33786327 0.33763874 0.3375943  0.33779946 0.3379475  0.33777237
 0.3373836  0.3371509  0.3373145  0.33774802 0.33804142 0.33805412
 0.3378774  0.33782312 0.33798346 0.33815452 0.33824968 0.33820224
 0.3382461  0.33854622 0.33893424 0.33911896 0.33899912 0.33877888
 0.33873957 0.33883166 0.3387693  0.33827102 0.33732566 0.33608234
 0.33477944 0.33356345 0.33245063 0.3314269  0.33047962 0.3296956
 0.32892874 0.3279848  0.32686237 0.32577506 0.32491344 0.32435375
 0.32390743 0.32333866 0.32271117 0.32219195 0.32173488 0.32112488
 0.32030132 0.3193106  0.31839594 0.31767476 0.31701672 0.31627733
 0.3155073  0.3148181  0.31425434 0.31370625 0.3130417  0.31226817
 0.31152895 0.3109521  0.31051683 0.3101201  0.30958194 0.309026
 0.3085532  0.3081462  0.30777505 0.30747575 0.3071548  0.30692577
 0.3068691  0.30686793 0.30673817 0.3065106  0.30621943 0.3059184
 0.30538186 0.30459493 0.3036638  0.30303282 0.3028378  0.30284682
 0.30280852 0.30256474 0.3022592  0.30204508 0.3019498  0.3019312
 0.30179468 0.30155054 0.3013658  0.3012499  0.30115697 0.30100697
 0.30079553 0.30057177 0.30051965 0.30067885 0.3007805  0.30080342
 0.30076525 0.3007925  0.30083933 0.3008313  0.30077508 0.30073023
 0.30079082 0.30096868 0.3010997  0.30105326 0.30082476 0.30065888
 0.30067304 0.30066004 0.30041507 0.29989713 0.2994411  0.29932705
 0.2994148  0.29930615 0.29864368 0.29735523 0.29571965 0.29407135
 0.2925607  0.29123783 0.290006   0.2888891  0.28792408 0.2870892
 0.28630194 0.28543696 0.2845224  0.2837397  0.28326795 0.283125
 0.2831563  0.2831636  0.2830612  0.28282434 0.2823658  0.28160763
 0.28066102 0.27968383 0.27880883 0.2780856  0.27737805 0.27658173
 0.27567142 0.2748007  0.27408674 0.27349007 0.27294225 0.27236792
 0.27172884 0.27104568 0.2703316  0.26960653 0.26893023 0.26836076
 0.2678593  0.26734194 0.26676884 0.26616654 0.26567432 0.26534012
 0.26507273 0.26483077 0.26454255 0.2641658  0.2638107  0.26330286
 0.26259857 0.26173964 0.26095435 0.26048252 0.26035795 0.26026022
 0.26001504 0.25969452 0.25952783 0.25951335 0.2595675  0.25941935
 0.25918782 0.2590572  0.25903034 0.25899097 0.2587415  0.25829962
 0.25789872 0.2578477  0.25809097 0.25829986 0.2581706  0.25785512
 0.25763276 0.2576685  0.2578021  0.25775316 0.25753182 0.2574057
 0.25742787 0.2575404  0.25747517 0.2571861  0.25693178 0.2569648
 0.2572108  0.25742322 0.25737154 0.2570857  0.25682306 0.25667632
 0.25659344 0.25635332 0.25587693 0.25510746 0.25394464 0.2521771
 0.24993277 0.24780366 0.24620526 0.24525908 0.2446709  0.24402648
 0.24304809 0.24188736 0.2407529  0.2397523  0.23894286 0.23836009
 0.23791139 0.2375142  0.2372413  0.23694804 0.2363475  0.23534688
 0.23413205 0.23302725 0.23220943 0.2315704  0.23101693 0.23043002
 0.22987178 0.22944778 0.22902817 0.22844662 0.22780834 0.22713575
 0.22640716 0.22575182 0.2251805  0.224628   0.22420688 0.22382559
 0.22345811 0.22294787 0.22237352 0.22191876 0.22170715 0.22170724
 0.22165287 0.22156289 0.22131391 0.22119115 0.22117037 0.22099744
 0.22063176 0.22018579 0.21993631 0.21993618 0.21996798 0.21983951
 0.21972108 0.21995752 0.22041474 0.22078905 0.22094308 0.2207667
 0.22069141 0.22089243 0.22117096 0.22120418 0.22089075 0.2204312
 0.22030425 0.22063226 0.2210962  0.22121395 0.22101665 0.22090757
 0.22108032 0.22143695 0.22147615 0.22128549 0.22120409 0.221465
 0.22198518 0.22222975 0.22201934 0.2218642  0.22223903 0.2230383
 0.22384395 0.22420318 0.22425891 0.2244859  0.22510587 0.22581933
 0.2261427  0.22596492 0.22550991 0.22496593 0.22412723 0.22266479
 0.22069685 0.2188998  0.21778195 0.21723428 0.21675259 0.21598715
 0.21498108 0.21412851 0.21364824 0.21336526 0.21302424 0.21255188
 0.21212232 0.2117489  0.21153797 0.21128492 0.2108048  0.21000884
 0.2090826  0.20819187 0.20735049 0.20643556 0.20541263 0.20427871
 0.20320901 0.2022208  0.20127432 0.20035689 0.19932455 0.1981629
 0.1968459  0.19558528 0.19446656 0.1936745  0.19299123 0.19229081
 0.19155811 0.19108175 0.19081908 0.19047016 0.18977465 0.18938997
 0.18972673 0.1906526  0.19103746 0.19048293 0.18990824 0.19258296]
