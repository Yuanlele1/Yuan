Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=24, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34135
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=24, out_features=113, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2429952.0
params:  2825.0
Trainable parameters:  2825
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5899996
	speed: 0.0424s/iter; left time: 1122.6806s
	iters: 200, epoch: 1 | loss: 0.4646278
	speed: 0.0227s/iter; left time: 598.2343s
Epoch: 1 cost time: 7.999687910079956
Epoch: 1, Steps: 266 | Train Loss: 0.5251738 Vali Loss: 0.2364624 Test Loss: 0.3304332
Validation loss decreased (inf --> 0.236462).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6475621
	speed: 0.1831s/iter; left time: 4802.5084s
	iters: 200, epoch: 2 | loss: 0.4977419
	speed: 0.0585s/iter; left time: 1527.7406s
Epoch: 2 cost time: 15.106406688690186
Epoch: 2, Steps: 266 | Train Loss: 0.4582696 Vali Loss: 0.2216457 Test Loss: 0.3129175
Validation loss decreased (0.236462 --> 0.221646).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4752019
	speed: 0.2206s/iter; left time: 5728.8901s
	iters: 200, epoch: 3 | loss: 0.3129058
	speed: 0.0368s/iter; left time: 952.5511s
Epoch: 3 cost time: 10.875847816467285
Epoch: 3, Steps: 266 | Train Loss: 0.4489672 Vali Loss: 0.2190223 Test Loss: 0.3095338
Validation loss decreased (0.221646 --> 0.219022).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4420345
	speed: 0.2111s/iter; left time: 5426.2556s
	iters: 200, epoch: 4 | loss: 0.4478766
	speed: 0.0225s/iter; left time: 575.4649s
Epoch: 4 cost time: 8.940208673477173
Epoch: 4, Steps: 266 | Train Loss: 0.4457847 Vali Loss: 0.2179284 Test Loss: 0.3083285
Validation loss decreased (0.219022 --> 0.217928).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4020296
	speed: 0.1116s/iter; left time: 2839.7635s
	iters: 200, epoch: 5 | loss: 0.4210472
	speed: 0.0355s/iter; left time: 900.0826s
Epoch: 5 cost time: 8.682877540588379
Epoch: 5, Steps: 266 | Train Loss: 0.4443777 Vali Loss: 0.2179179 Test Loss: 0.3078383
Validation loss decreased (0.217928 --> 0.217918).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4617751
	speed: 0.1335s/iter; left time: 3359.9683s
	iters: 200, epoch: 6 | loss: 0.2721764
	speed: 0.0361s/iter; left time: 904.4262s
Epoch: 6 cost time: 9.709100246429443
Epoch: 6, Steps: 266 | Train Loss: 0.4434897 Vali Loss: 0.2178555 Test Loss: 0.3074473
Validation loss decreased (0.217918 --> 0.217856).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3963462
	speed: 0.2084s/iter; left time: 5191.0943s
	iters: 200, epoch: 7 | loss: 0.4797224
	speed: 0.0318s/iter; left time: 788.6338s
Epoch: 7 cost time: 12.4038405418396
Epoch: 7, Steps: 266 | Train Loss: 0.4433046 Vali Loss: 0.2178214 Test Loss: 0.3073618
Validation loss decreased (0.217856 --> 0.217821).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3843211
	speed: 0.1729s/iter; left time: 4260.1069s
	iters: 200, epoch: 8 | loss: 0.3466480
	speed: 0.0520s/iter; left time: 1276.2632s
Epoch: 8 cost time: 14.42800259590149
Epoch: 8, Steps: 266 | Train Loss: 0.4419645 Vali Loss: 0.2178489 Test Loss: 0.3072560
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3932965
	speed: 0.1642s/iter; left time: 4002.9389s
	iters: 200, epoch: 9 | loss: 0.3646575
	speed: 0.0315s/iter; left time: 763.6946s
Epoch: 9 cost time: 9.797412157058716
Epoch: 9, Steps: 266 | Train Loss: 0.4419105 Vali Loss: 0.2180179 Test Loss: 0.3072009
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5434040
	speed: 0.1965s/iter; left time: 4736.7093s
	iters: 200, epoch: 10 | loss: 0.3592453
	speed: 0.0321s/iter; left time: 770.1627s
Epoch: 10 cost time: 9.252108097076416
Epoch: 10, Steps: 266 | Train Loss: 0.4420371 Vali Loss: 0.2179397 Test Loss: 0.3071775
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4618045
	speed: 0.1414s/iter; left time: 3370.9064s
	iters: 200, epoch: 11 | loss: 0.3182051
	speed: 0.0570s/iter; left time: 1352.2490s
Epoch: 11 cost time: 13.964493036270142
Epoch: 11, Steps: 266 | Train Loss: 0.4418737 Vali Loss: 0.2178990 Test Loss: 0.3071624
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5038537
	speed: 0.1892s/iter; left time: 4460.4357s
	iters: 200, epoch: 12 | loss: 0.5050764
	speed: 0.0402s/iter; left time: 942.7607s
Epoch: 12 cost time: 11.71799349784851
Epoch: 12, Steps: 266 | Train Loss: 0.4416051 Vali Loss: 0.2181268 Test Loss: 0.3071067
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4693224
	speed: 0.1595s/iter; left time: 3718.7031s
	iters: 200, epoch: 13 | loss: 0.3331521
	speed: 0.0286s/iter; left time: 664.8227s
Epoch: 13 cost time: 8.528512954711914
Epoch: 13, Steps: 266 | Train Loss: 0.4409877 Vali Loss: 0.2178561 Test Loss: 0.3070603
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3865434
	speed: 0.1550s/iter; left time: 3571.1958s
	iters: 200, epoch: 14 | loss: 0.5659534
	speed: 0.0213s/iter; left time: 489.4545s
Epoch: 14 cost time: 8.057910680770874
Epoch: 14, Steps: 266 | Train Loss: 0.4407897 Vali Loss: 0.2181333 Test Loss: 0.3070752
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4813554
	speed: 0.2051s/iter; left time: 4671.5764s
	iters: 200, epoch: 15 | loss: 0.4603831
	speed: 0.0390s/iter; left time: 885.2030s
Epoch: 15 cost time: 15.406935453414917
Epoch: 15, Steps: 266 | Train Loss: 0.4412369 Vali Loss: 0.2181587 Test Loss: 0.3070817
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3706941
	speed: 0.2284s/iter; left time: 5141.3616s
	iters: 200, epoch: 16 | loss: 0.4823271
	speed: 0.0343s/iter; left time: 768.5989s
Epoch: 16 cost time: 10.260850667953491
Epoch: 16, Steps: 266 | Train Loss: 0.4411702 Vali Loss: 0.2181387 Test Loss: 0.3070923
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3873395
	speed: 0.1438s/iter; left time: 3199.7819s
	iters: 200, epoch: 17 | loss: 0.5925527
	speed: 0.0300s/iter; left time: 665.3449s
Epoch: 17 cost time: 9.850282669067383
Epoch: 17, Steps: 266 | Train Loss: 0.4403270 Vali Loss: 0.2180693 Test Loss: 0.3071082
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4932055
	speed: 0.1398s/iter; left time: 3072.0994s
	iters: 200, epoch: 18 | loss: 0.4236283
	speed: 0.0412s/iter; left time: 901.8751s
Epoch: 18 cost time: 10.359375715255737
Epoch: 18, Steps: 266 | Train Loss: 0.4408781 Vali Loss: 0.2183171 Test Loss: 0.3071503
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2473659
	speed: 0.1547s/iter; left time: 3359.1313s
	iters: 200, epoch: 19 | loss: 0.3051110
	speed: 0.0287s/iter; left time: 619.3592s
Epoch: 19 cost time: 9.550516366958618
Epoch: 19, Steps: 266 | Train Loss: 0.4405345 Vali Loss: 0.2181360 Test Loss: 0.3070836
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3908239
	speed: 0.1745s/iter; left time: 3741.9419s
	iters: 200, epoch: 20 | loss: 0.5318393
	speed: 0.0362s/iter; left time: 772.6040s
Epoch: 20 cost time: 12.651424884796143
Epoch: 20, Steps: 266 | Train Loss: 0.4404582 Vali Loss: 0.2181339 Test Loss: 0.3071175
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3625110
	speed: 0.2050s/iter; left time: 4341.6912s
	iters: 200, epoch: 21 | loss: 0.3325794
	speed: 0.0273s/iter; left time: 576.5034s
Epoch: 21 cost time: 9.878291130065918
Epoch: 21, Steps: 266 | Train Loss: 0.4403959 Vali Loss: 0.2183337 Test Loss: 0.3071217
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5134541
	speed: 0.1487s/iter; left time: 3110.8973s
	iters: 200, epoch: 22 | loss: 0.3931468
	speed: 0.0341s/iter; left time: 709.5718s
Epoch: 22 cost time: 9.678376197814941
Epoch: 22, Steps: 266 | Train Loss: 0.4407116 Vali Loss: 0.2184966 Test Loss: 0.3071262
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3793632
	speed: 0.1308s/iter; left time: 2700.0451s
	iters: 200, epoch: 23 | loss: 0.3633380
	speed: 0.0315s/iter; left time: 648.0077s
Epoch: 23 cost time: 7.253792762756348
Epoch: 23, Steps: 266 | Train Loss: 0.4406504 Vali Loss: 0.2185577 Test Loss: 0.3071823
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4035957
	speed: 0.1852s/iter; left time: 3775.2808s
	iters: 200, epoch: 24 | loss: 0.5098391
	speed: 0.0400s/iter; left time: 810.6319s
Epoch: 24 cost time: 11.258364915847778
Epoch: 24, Steps: 266 | Train Loss: 0.4406915 Vali Loss: 0.2186463 Test Loss: 0.3071669
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3262637
	speed: 0.1767s/iter; left time: 3555.1063s
	iters: 200, epoch: 25 | loss: 0.5488118
	speed: 0.0291s/iter; left time: 582.1805s
Epoch: 25 cost time: 11.005961894989014
Epoch: 25, Steps: 266 | Train Loss: 0.4406337 Vali Loss: 0.2184520 Test Loss: 0.3071979
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4993182
	speed: 0.1821s/iter; left time: 3615.6123s
	iters: 200, epoch: 26 | loss: 0.3323632
	speed: 0.0521s/iter; left time: 1029.2116s
Epoch: 26 cost time: 12.541458129882812
Epoch: 26, Steps: 266 | Train Loss: 0.4402897 Vali Loss: 0.2185306 Test Loss: 0.3071807
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3057500
	speed: 0.1212s/iter; left time: 2373.3374s
	iters: 200, epoch: 27 | loss: 0.2975033
	speed: 0.0237s/iter; left time: 461.3291s
Epoch: 27 cost time: 7.850250005722046
Epoch: 27, Steps: 266 | Train Loss: 0.4407160 Vali Loss: 0.2185445 Test Loss: 0.3071971
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.30875909328460693, mae:0.3432677090167999, rse:0.44881826639175415, corr:[0.5618261  0.5611928  0.5515137  0.55271983 0.55179095 0.5471036
 0.5468493  0.5485672  0.546815   0.5445401  0.545147   0.5452627
 0.54315907 0.54198086 0.54240894 0.541528   0.53947824 0.5388427
 0.5389547  0.53760487 0.5358609  0.53559893 0.53553796 0.53439164
 0.53342104 0.5336771  0.53372484 0.5326559  0.5316797  0.5315125
 0.53112876 0.53027713 0.52979195 0.52966255 0.5290729  0.5281445
 0.52755    0.5271242  0.5263207  0.5254183  0.5250216  0.52493536
 0.5245682  0.52403146 0.5237286  0.5234081  0.52274287 0.5219183
 0.5212514  0.52052927 0.5195522  0.5186927  0.5181502  0.51766324
 0.51705444 0.51647663 0.51612335 0.5158914  0.5155359  0.5152036
 0.5150725  0.51503474 0.5149063  0.5147552  0.51470786 0.5147024
 0.51454365 0.5142739  0.5141464  0.5141491  0.5140826  0.5140039
 0.5140247  0.5140845  0.5139033  0.5135766  0.51334786 0.51318187
 0.5129413  0.51266426 0.51245385 0.51219547 0.5117601  0.51130563
 0.511048   0.5108328  0.5104365  0.5100014  0.50976235 0.50955546
 0.5090808  0.5084345  0.50791436 0.5072381  0.5058922  0.5039219
 0.5016882  0.49936724 0.49711642 0.49527568 0.4938686  0.49252596
 0.49122742 0.4902644  0.48936847 0.48797178 0.48634356 0.48514342
 0.4843092  0.48327202 0.48187202 0.4805651  0.47957277 0.47846374
 0.47705686 0.47570977 0.4746228  0.4736149  0.47257715 0.47152323
 0.4705248  0.46946093 0.46841624 0.46759662 0.46682334 0.46579164
 0.46467987 0.46378103 0.46287477 0.46178508 0.46077994 0.4601048
 0.45946983 0.4585145  0.45751888 0.45708385 0.45677623 0.45624194
 0.45572272 0.45553276 0.45531464 0.45469877 0.45412737 0.45386767
 0.45336127 0.4522275  0.45101082 0.45036507 0.44979587 0.4488534
 0.44795328 0.44771117 0.44759372 0.4470342  0.4464492  0.4461695
 0.44589373 0.44555658 0.44532657 0.44537348 0.4452914  0.44497532
 0.4448489  0.44479743 0.44476998 0.4447504  0.444829   0.44486243
 0.44470263 0.44471005 0.44506904 0.445315   0.44509917 0.44480917
 0.44484997 0.44492927 0.44469255 0.44444567 0.4444648  0.44441527
 0.44395038 0.44347227 0.44347018 0.44349936 0.44304016 0.44236967
 0.4420653  0.44195065 0.4413729  0.440256   0.43890345 0.4372947
 0.43534845 0.43331474 0.43140996 0.42955363 0.42783526 0.42656803
 0.42570958 0.4247757  0.42375463 0.4228991  0.42204034 0.42075434
 0.41925636 0.4181113  0.41715252 0.41580436 0.41418102 0.41300285
 0.41223225 0.41122293 0.40981802 0.40860903 0.40777147 0.40674433
 0.40543017 0.40437344 0.40374622 0.40283436 0.40147862 0.40033478
 0.3995371  0.39837456 0.39704475 0.3960482  0.3955987  0.3949415
 0.3938636  0.39300898 0.39239857 0.39152554 0.3904055  0.3898626
 0.38985473 0.38956073 0.38900757 0.3888857  0.3891114  0.3888449
 0.3882086  0.3878966  0.38759643 0.38706553 0.3866282  0.38674098
 0.38702977 0.38677713 0.38646206 0.38666564 0.3868729  0.38647395
 0.38620463 0.38679156 0.3873388  0.3870379  0.3867647  0.38722473
 0.38761896 0.38744476 0.38744235 0.38798362 0.38818878 0.3878253
 0.38806736 0.3890008  0.38952136 0.38932335 0.38954455 0.39026654
 0.390361   0.38993064 0.39008415 0.39083067 0.391169   0.3910163
 0.39130715 0.39194435 0.3919735  0.3915397  0.3916172  0.39223796
 0.39254    0.39244106 0.39253253 0.39259014 0.39181772 0.39045843
 0.3893759  0.388478   0.38724047 0.38627544 0.38636023 0.3869192
 0.38698843 0.3869293  0.38730988 0.38740966 0.38661194 0.38584462
 0.3858775  0.38583827 0.38492492 0.38387835 0.38343865 0.38289934
 0.38170084 0.3807886  0.38067812 0.38014865 0.37864166 0.37764493
 0.3776082  0.3771645  0.3756733  0.3747812  0.3750604  0.37451437
 0.37247455 0.37126413 0.37157038 0.3708923  0.3687383  0.367956
 0.3685557  0.3672768  0.36475855 0.36473295 0.3658183  0.36328405
 0.36035094 0.36237144 0.3630772  0.35725105 0.3581057  0.36722413]
