Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34135
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=18, out_features=85, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1370880.0
params:  1615.0
Trainable parameters:  1615
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5121871
	speed: 0.0199s/iter; left time: 527.4059s
	iters: 200, epoch: 1 | loss: 0.4738515
	speed: 0.0175s/iter; left time: 462.7105s
Epoch: 1 cost time: 5.38809061050415
Epoch: 1, Steps: 266 | Train Loss: 0.5448013 Vali Loss: 0.2456367 Test Loss: 0.3406304
Validation loss decreased (inf --> 0.245637).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3712463
	speed: 0.0836s/iter; left time: 2192.8431s
	iters: 200, epoch: 2 | loss: 0.4583638
	speed: 0.0151s/iter; left time: 393.3988s
Epoch: 2 cost time: 4.597743511199951
Epoch: 2, Steps: 266 | Train Loss: 0.4632266 Vali Loss: 0.2238842 Test Loss: 0.3150325
Validation loss decreased (0.245637 --> 0.223884).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6815572
	speed: 0.0720s/iter; left time: 1870.5106s
	iters: 200, epoch: 3 | loss: 0.2655193
	speed: 0.0165s/iter; left time: 427.8417s
Epoch: 3 cost time: 4.807597398757935
Epoch: 3, Steps: 266 | Train Loss: 0.4499856 Vali Loss: 0.2196722 Test Loss: 0.3100959
Validation loss decreased (0.223884 --> 0.219672).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3922274
	speed: 0.0714s/iter; left time: 1836.3678s
	iters: 200, epoch: 4 | loss: 0.5575731
	speed: 0.0140s/iter; left time: 358.6762s
Epoch: 4 cost time: 4.462080717086792
Epoch: 4, Steps: 266 | Train Loss: 0.4462508 Vali Loss: 0.2184743 Test Loss: 0.3086191
Validation loss decreased (0.219672 --> 0.218474).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5529752
	speed: 0.0728s/iter; left time: 1851.9887s
	iters: 200, epoch: 5 | loss: 0.4357716
	speed: 0.0204s/iter; left time: 516.0819s
Epoch: 5 cost time: 5.196730375289917
Epoch: 5, Steps: 266 | Train Loss: 0.4451583 Vali Loss: 0.2182643 Test Loss: 0.3080954
Validation loss decreased (0.218474 --> 0.218264).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5004041
	speed: 0.0901s/iter; left time: 2269.0083s
	iters: 200, epoch: 6 | loss: 0.5291235
	speed: 0.0151s/iter; left time: 377.5689s
Epoch: 6 cost time: 5.641190767288208
Epoch: 6, Steps: 266 | Train Loss: 0.4440383 Vali Loss: 0.2180545 Test Loss: 0.3078336
Validation loss decreased (0.218264 --> 0.218054).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3626836
	speed: 0.0849s/iter; left time: 2113.8597s
	iters: 200, epoch: 7 | loss: 0.3220413
	speed: 0.0137s/iter; left time: 338.8148s
Epoch: 7 cost time: 5.547008514404297
Epoch: 7, Steps: 266 | Train Loss: 0.4430772 Vali Loss: 0.2181033 Test Loss: 0.3076439
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5905232
	speed: 0.0708s/iter; left time: 1745.0878s
	iters: 200, epoch: 8 | loss: 0.3000351
	speed: 0.0159s/iter; left time: 390.8407s
Epoch: 8 cost time: 4.5828492641448975
Epoch: 8, Steps: 266 | Train Loss: 0.4428381 Vali Loss: 0.2181833 Test Loss: 0.3075023
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4686871
	speed: 0.0813s/iter; left time: 1980.6880s
	iters: 200, epoch: 9 | loss: 0.3024588
	speed: 0.0162s/iter; left time: 393.7067s
Epoch: 9 cost time: 5.225060224533081
Epoch: 9, Steps: 266 | Train Loss: 0.4427387 Vali Loss: 0.2180851 Test Loss: 0.3074173
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4011296
	speed: 0.0746s/iter; left time: 1797.5836s
	iters: 200, epoch: 10 | loss: 0.4081407
	speed: 0.0176s/iter; left time: 423.3945s
Epoch: 10 cost time: 5.154058218002319
Epoch: 10, Steps: 266 | Train Loss: 0.4424628 Vali Loss: 0.2181755 Test Loss: 0.3074134
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2346607
	speed: 0.0712s/iter; left time: 1696.8688s
	iters: 200, epoch: 11 | loss: 0.5438476
	speed: 0.0152s/iter; left time: 359.7058s
Epoch: 11 cost time: 4.5077173709869385
Epoch: 11, Steps: 266 | Train Loss: 0.4421038 Vali Loss: 0.2184069 Test Loss: 0.3074174
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6835303
	speed: 0.0719s/iter; left time: 1695.3965s
	iters: 200, epoch: 12 | loss: 0.5705317
	speed: 0.0143s/iter; left time: 336.1980s
Epoch: 12 cost time: 4.440984487533569
Epoch: 12, Steps: 266 | Train Loss: 0.4416086 Vali Loss: 0.2181768 Test Loss: 0.3074630
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5616976
	speed: 0.0784s/iter; left time: 1827.6868s
	iters: 200, epoch: 13 | loss: 0.5233406
	speed: 0.0141s/iter; left time: 327.0166s
Epoch: 13 cost time: 4.547836065292358
Epoch: 13, Steps: 266 | Train Loss: 0.4416722 Vali Loss: 0.2182611 Test Loss: 0.3074459
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4622654
	speed: 0.0727s/iter; left time: 1675.9117s
	iters: 200, epoch: 14 | loss: 0.3869581
	speed: 0.0240s/iter; left time: 549.7305s
Epoch: 14 cost time: 5.43171238899231
Epoch: 14, Steps: 266 | Train Loss: 0.4417341 Vali Loss: 0.2182169 Test Loss: 0.3074406
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.6051055
	speed: 0.0841s/iter; left time: 1916.1876s
	iters: 200, epoch: 15 | loss: 0.3849008
	speed: 0.0152s/iter; left time: 345.4919s
Epoch: 15 cost time: 5.698957204818726
Epoch: 15, Steps: 266 | Train Loss: 0.4416413 Vali Loss: 0.2182004 Test Loss: 0.3073816
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4219953
	speed: 0.0799s/iter; left time: 1797.5048s
	iters: 200, epoch: 16 | loss: 0.6750182
	speed: 0.0148s/iter; left time: 331.8068s
Epoch: 16 cost time: 4.6742236614227295
Epoch: 16, Steps: 266 | Train Loss: 0.4411225 Vali Loss: 0.2184067 Test Loss: 0.3074016
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3178162
	speed: 0.0729s/iter; left time: 1621.2000s
	iters: 200, epoch: 17 | loss: 0.3537272
	speed: 0.0210s/iter; left time: 465.0725s
Epoch: 17 cost time: 4.973148345947266
Epoch: 17, Steps: 266 | Train Loss: 0.4406952 Vali Loss: 0.2184237 Test Loss: 0.3074106
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5447403
	speed: 0.0809s/iter; left time: 1777.7069s
	iters: 200, epoch: 18 | loss: 0.4291527
	speed: 0.0154s/iter; left time: 336.3577s
Epoch: 18 cost time: 4.547031402587891
Epoch: 18, Steps: 266 | Train Loss: 0.4412911 Vali Loss: 0.2184855 Test Loss: 0.3075058
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5566348
	speed: 0.0699s/iter; left time: 1518.8052s
	iters: 200, epoch: 19 | loss: 0.4188500
	speed: 0.0141s/iter; left time: 305.5992s
Epoch: 19 cost time: 4.270414352416992
Epoch: 19, Steps: 266 | Train Loss: 0.4403435 Vali Loss: 0.2185752 Test Loss: 0.3074215
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4738367
	speed: 0.0861s/iter; left time: 1847.0875s
	iters: 200, epoch: 20 | loss: 0.4390959
	speed: 0.0151s/iter; left time: 322.9979s
Epoch: 20 cost time: 5.8268749713897705
Epoch: 20, Steps: 266 | Train Loss: 0.4413484 Vali Loss: 0.2184235 Test Loss: 0.3075030
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3032930
	speed: 0.0709s/iter; left time: 1501.2334s
	iters: 200, epoch: 21 | loss: 0.4839379
	speed: 0.0147s/iter; left time: 309.0313s
Epoch: 21 cost time: 4.466655731201172
Epoch: 21, Steps: 266 | Train Loss: 0.4414655 Vali Loss: 0.2185759 Test Loss: 0.3075027
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5579560
	speed: 0.0744s/iter; left time: 1555.3270s
	iters: 200, epoch: 22 | loss: 0.3667107
	speed: 0.0158s/iter; left time: 329.6850s
Epoch: 22 cost time: 4.923144340515137
Epoch: 22, Steps: 266 | Train Loss: 0.4412721 Vali Loss: 0.2185637 Test Loss: 0.3074699
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5770558
	speed: 0.0780s/iter; left time: 1611.2215s
	iters: 200, epoch: 23 | loss: 0.5041251
	speed: 0.0145s/iter; left time: 298.0641s
Epoch: 23 cost time: 4.594228506088257
Epoch: 23, Steps: 266 | Train Loss: 0.4410146 Vali Loss: 0.2186778 Test Loss: 0.3074450
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4504893
	speed: 0.0878s/iter; left time: 1790.3616s
	iters: 200, epoch: 24 | loss: 0.4551863
	speed: 0.0221s/iter; left time: 447.4723s
Epoch: 24 cost time: 6.765095949172974
Epoch: 24, Steps: 266 | Train Loss: 0.4404799 Vali Loss: 0.2185541 Test Loss: 0.3074887
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.6125575
	speed: 0.0766s/iter; left time: 1541.4088s
	iters: 200, epoch: 25 | loss: 0.5256175
	speed: 0.0138s/iter; left time: 275.5817s
Epoch: 25 cost time: 4.515016317367554
Epoch: 25, Steps: 266 | Train Loss: 0.4410402 Vali Loss: 0.2186443 Test Loss: 0.3074594
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3527568
	speed: 0.0745s/iter; left time: 1479.4889s
	iters: 200, epoch: 26 | loss: 0.2655202
	speed: 0.0149s/iter; left time: 294.1818s
Epoch: 26 cost time: 4.937525033950806
Epoch: 26, Steps: 266 | Train Loss: 0.4406770 Vali Loss: 0.2186712 Test Loss: 0.3074846
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.3092341721057892, mae:0.34363746643066406, rse:0.44916340708732605, corr:[0.55634516 0.56019497 0.55371666 0.5481875  0.547807   0.5488665
 0.54747504 0.5446363  0.5433786  0.5442272  0.54495025 0.5439398
 0.5420608  0.5412209  0.54161406 0.5416286  0.5402407  0.5382436
 0.5370849  0.5369457  0.53668827 0.5356106  0.5343036  0.5337744
 0.5339822  0.5339614  0.5331884  0.5320773  0.53142965 0.5313387
 0.5311109  0.5303546  0.5293898  0.5288407  0.5286924  0.528356
 0.52739716 0.5261199  0.5252175  0.5248975  0.52476466 0.52445567
 0.5239425  0.52353    0.5233562  0.5231353  0.5226001  0.5217226
 0.520819   0.52012765 0.519519   0.5188398  0.5180334  0.5172762
 0.5168241  0.5165653  0.51625496 0.51584756 0.5154827  0.5153511
 0.5153882  0.51539844 0.51527435 0.51507163 0.514891   0.51480556
 0.5147559  0.5146344  0.5144485  0.5142701  0.5141748  0.51418114
 0.51417595 0.5140826  0.5138379  0.51356053 0.5133768  0.5132018
 0.5129408  0.512565   0.5121767  0.5118928  0.5116947  0.5114333
 0.5110035  0.5104524  0.5099869  0.5097426  0.50960714 0.50933814
 0.50880563 0.5080435  0.50719297 0.5062045  0.5048168  0.50288457
 0.50061685 0.49840313 0.49638873 0.4944461  0.49255118 0.49095938
 0.48985195 0.4890382  0.48815513 0.48697016 0.4856467  0.484434
 0.48341998 0.48245123 0.4812873  0.47989658 0.47852048 0.47733888
 0.47628117 0.475199   0.474042   0.47290838 0.47190735 0.4709647
 0.47001225 0.46899113 0.46793967 0.46697882 0.46615255 0.46535948
 0.46446836 0.46342954 0.46230632 0.46131328 0.46051416 0.45975578
 0.45887545 0.45792913 0.45709127 0.45661455 0.4562692  0.45589262
 0.45540717 0.4549462  0.45463622 0.4543474  0.45385355 0.45304576
 0.4520695  0.45118213 0.45044315 0.44983324 0.44917125 0.4484625
 0.44782093 0.44741595 0.44703227 0.44645038 0.44585806 0.44553813
 0.44549328 0.44555774 0.4454485  0.44529894 0.4452368  0.44524866
 0.44523257 0.44490084 0.44450137 0.44431093 0.4444533  0.44475704
 0.44490135 0.4447888  0.4446161  0.44465217 0.44485706 0.44486475
 0.44446215 0.44386685 0.44352838 0.44367108 0.4439831  0.44404215
 0.4437115  0.44323516 0.44300056 0.44298112 0.44291937 0.44259316
 0.44205004 0.44145215 0.44072258 0.43959898 0.43791416 0.43579504
 0.43365017 0.4317892  0.43011475 0.4284516  0.42690814 0.42570376
 0.42487216 0.4241286  0.42322698 0.42210963 0.42092812 0.41982546
 0.41880107 0.41774586 0.41647816 0.41504404 0.41362375 0.41238445
 0.4112609  0.41019657 0.4091469  0.40811473 0.40711802 0.40609932
 0.40506285 0.40402746 0.40314204 0.40231594 0.40138575 0.40031216
 0.3991638  0.39798313 0.39708844 0.39635444 0.3956749  0.39485073
 0.39384463 0.39281914 0.39189684 0.39123234 0.390722   0.39029992
 0.38995877 0.38975894 0.38967255 0.38950497 0.38916567 0.3887255
 0.38838112 0.38816765 0.38784707 0.38747653 0.3870925  0.386858
 0.38702998 0.3873958  0.38749877 0.3870888  0.38655418 0.3864157
 0.3867643  0.3872432  0.3874637  0.38739493 0.38732615 0.3874591
 0.38767046 0.3877295  0.38762295 0.3876899  0.38807943 0.38855678
 0.38887104 0.38892066 0.3889869  0.38916412 0.38939    0.38951057
 0.38950732 0.3896505  0.39002985 0.39049366 0.39080322 0.3909382
 0.39118296 0.39167976 0.3922344  0.39250797 0.39232793 0.39201865
 0.39196873 0.39213726 0.3920531  0.3913204  0.39004698 0.3887209
 0.38770992 0.38697818 0.38625145 0.3855572  0.38521618 0.38544822
 0.3859253  0.38606453 0.38582066 0.38557506 0.38556224 0.38564506
 0.38544852 0.38477364 0.3838379  0.38303062 0.38245806 0.3818698
 0.3810387  0.38004705 0.37925592 0.3788145  0.37841186 0.37772676
 0.37672308 0.37587819 0.37546757 0.3752777  0.37483883 0.3738974
 0.37272102 0.37186745 0.37143221 0.3709778  0.37006125 0.3689397
 0.36817926 0.36796147 0.36768714 0.36654225 0.36495584 0.36405444
 0.36424154 0.36428124 0.36296314 0.36110434 0.36100498 0.3613434 ]
