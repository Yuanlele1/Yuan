Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=14, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34135
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=14, out_features=66, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  827904.0
params:  990.0
Trainable parameters:  990
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6840233
	speed: 0.0817s/iter; left time: 2164.3098s
	iters: 200, epoch: 1 | loss: 0.6204049
	speed: 0.0720s/iter; left time: 1899.7083s
Epoch: 1 cost time: 20.25118613243103
Epoch: 1, Steps: 266 | Train Loss: 0.5471004 Vali Loss: 0.2485695 Test Loss: 0.3445561
Validation loss decreased (inf --> 0.248570).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4530887
	speed: 0.3231s/iter; left time: 8475.3214s
	iters: 200, epoch: 2 | loss: 0.5218892
	speed: 0.0707s/iter; left time: 1847.6440s
Epoch: 2 cost time: 19.585869073867798
Epoch: 2, Steps: 266 | Train Loss: 0.4669855 Vali Loss: 0.2249171 Test Loss: 0.3160889
Validation loss decreased (0.248570 --> 0.224917).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5964909
	speed: 0.3219s/iter; left time: 8360.4266s
	iters: 200, epoch: 3 | loss: 0.3096432
	speed: 0.0711s/iter; left time: 1839.8649s
Epoch: 3 cost time: 19.96573519706726
Epoch: 3, Steps: 266 | Train Loss: 0.4504074 Vali Loss: 0.2200039 Test Loss: 0.3105097
Validation loss decreased (0.224917 --> 0.220004).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4794637
	speed: 0.3241s/iter; left time: 8330.4309s
	iters: 200, epoch: 4 | loss: 0.5758106
	speed: 0.0704s/iter; left time: 1801.4213s
Epoch: 4 cost time: 19.629502296447754
Epoch: 4, Steps: 266 | Train Loss: 0.4464449 Vali Loss: 0.2190333 Test Loss: 0.3090937
Validation loss decreased (0.220004 --> 0.219033).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4281070
	speed: 0.3240s/iter; left time: 8242.2249s
	iters: 200, epoch: 5 | loss: 0.2437775
	speed: 0.0719s/iter; left time: 1821.1180s
Epoch: 5 cost time: 20.23233199119568
Epoch: 5, Steps: 266 | Train Loss: 0.4457105 Vali Loss: 0.2186075 Test Loss: 0.3085276
Validation loss decreased (0.219033 --> 0.218607).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2785906
	speed: 0.3280s/iter; left time: 8255.5857s
	iters: 200, epoch: 6 | loss: 0.6021658
	speed: 0.0726s/iter; left time: 1819.4195s
Epoch: 6 cost time: 19.846010446548462
Epoch: 6, Steps: 266 | Train Loss: 0.4437954 Vali Loss: 0.2182892 Test Loss: 0.3081250
Validation loss decreased (0.218607 --> 0.218289).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5302461
	speed: 0.3192s/iter; left time: 7950.5113s
	iters: 200, epoch: 7 | loss: 0.3119760
	speed: 0.0710s/iter; left time: 1761.7465s
Epoch: 7 cost time: 19.53635573387146
Epoch: 7, Steps: 266 | Train Loss: 0.4438222 Vali Loss: 0.2185053 Test Loss: 0.3079975
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5879166
	speed: 0.3203s/iter; left time: 7891.9524s
	iters: 200, epoch: 8 | loss: 0.4486938
	speed: 0.0721s/iter; left time: 1769.0046s
Epoch: 8 cost time: 19.942288875579834
Epoch: 8, Steps: 266 | Train Loss: 0.4428008 Vali Loss: 0.2184582 Test Loss: 0.3078192
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3068593
	speed: 0.3223s/iter; left time: 7854.8541s
	iters: 200, epoch: 9 | loss: 0.3859488
	speed: 0.0744s/iter; left time: 1806.2018s
Epoch: 9 cost time: 19.951791763305664
Epoch: 9, Steps: 266 | Train Loss: 0.4434354 Vali Loss: 0.2183955 Test Loss: 0.3078755
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5214269
	speed: 0.3259s/iter; left time: 7856.0956s
	iters: 200, epoch: 10 | loss: 0.3701605
	speed: 0.0723s/iter; left time: 1736.8091s
Epoch: 10 cost time: 20.112797737121582
Epoch: 10, Steps: 266 | Train Loss: 0.4431587 Vali Loss: 0.2185585 Test Loss: 0.3077204
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3120944
	speed: 0.3253s/iter; left time: 7754.7637s
	iters: 200, epoch: 11 | loss: 0.4287607
	speed: 0.0719s/iter; left time: 1706.3448s
Epoch: 11 cost time: 20.093717336654663
Epoch: 11, Steps: 266 | Train Loss: 0.4425483 Vali Loss: 0.2186031 Test Loss: 0.3077816
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4778015
	speed: 0.3286s/iter; left time: 7747.3989s
	iters: 200, epoch: 12 | loss: 0.5435407
	speed: 0.0713s/iter; left time: 1674.3078s
Epoch: 12 cost time: 20.087368726730347
Epoch: 12, Steps: 266 | Train Loss: 0.4426991 Vali Loss: 0.2184160 Test Loss: 0.3077786
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3325134
	speed: 0.3223s/iter; left time: 7513.2432s
	iters: 200, epoch: 13 | loss: 0.3299517
	speed: 0.0716s/iter; left time: 1662.6863s
Epoch: 13 cost time: 19.87805962562561
Epoch: 13, Steps: 266 | Train Loss: 0.4417661 Vali Loss: 0.2185347 Test Loss: 0.3077126
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5501438
	speed: 0.3255s/iter; left time: 7499.9179s
	iters: 200, epoch: 14 | loss: 0.4783060
	speed: 0.0706s/iter; left time: 1620.5569s
Epoch: 14 cost time: 19.610626459121704
Epoch: 14, Steps: 266 | Train Loss: 0.4423502 Vali Loss: 0.2185209 Test Loss: 0.3077415
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4523565
	speed: 0.3272s/iter; left time: 7453.3117s
	iters: 200, epoch: 15 | loss: 0.4445646
	speed: 0.0713s/iter; left time: 1617.5327s
Epoch: 15 cost time: 19.955933094024658
Epoch: 15, Steps: 266 | Train Loss: 0.4424732 Vali Loss: 0.2185416 Test Loss: 0.3077687
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4799171
	speed: 0.3271s/iter; left time: 7362.9623s
	iters: 200, epoch: 16 | loss: 0.7537358
	speed: 0.0705s/iter; left time: 1579.7808s
Epoch: 16 cost time: 20.12441611289978
Epoch: 16, Steps: 266 | Train Loss: 0.4419291 Vali Loss: 0.2186058 Test Loss: 0.3077074
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3889071
	speed: 0.3267s/iter; left time: 7266.4458s
	iters: 200, epoch: 17 | loss: 0.4984671
	speed: 0.0717s/iter; left time: 1588.6727s
Epoch: 17 cost time: 20.06009340286255
Epoch: 17, Steps: 266 | Train Loss: 0.4414419 Vali Loss: 0.2187936 Test Loss: 0.3077370
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3888606
	speed: 0.3252s/iter; left time: 7148.4159s
	iters: 200, epoch: 18 | loss: 0.5527030
	speed: 0.0738s/iter; left time: 1614.8909s
Epoch: 18 cost time: 20.09469437599182
Epoch: 18, Steps: 266 | Train Loss: 0.4419802 Vali Loss: 0.2188634 Test Loss: 0.3077730
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3039212
	speed: 0.3196s/iter; left time: 6938.7378s
	iters: 200, epoch: 19 | loss: 0.4500259
	speed: 0.0712s/iter; left time: 1538.2490s
Epoch: 19 cost time: 19.536696672439575
Epoch: 19, Steps: 266 | Train Loss: 0.4413322 Vali Loss: 0.2187999 Test Loss: 0.3077123
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3976824
	speed: 0.3245s/iter; left time: 6959.0110s
	iters: 200, epoch: 20 | loss: 0.3417277
	speed: 0.0735s/iter; left time: 1568.4197s
Epoch: 20 cost time: 19.974746227264404
Epoch: 20, Steps: 266 | Train Loss: 0.4415735 Vali Loss: 0.2188353 Test Loss: 0.3077281
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3517821
	speed: 0.3287s/iter; left time: 6961.9102s
	iters: 200, epoch: 21 | loss: 0.3873171
	speed: 0.0708s/iter; left time: 1493.0786s
Epoch: 21 cost time: 19.730782985687256
Epoch: 21, Steps: 266 | Train Loss: 0.4417688 Vali Loss: 0.2189243 Test Loss: 0.3077701
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4300968
	speed: 0.3208s/iter; left time: 6709.0583s
	iters: 200, epoch: 22 | loss: 0.5373778
	speed: 0.0715s/iter; left time: 1487.4351s
Epoch: 22 cost time: 19.56046986579895
Epoch: 22, Steps: 266 | Train Loss: 0.4410733 Vali Loss: 0.2189264 Test Loss: 0.3077742
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4630060
	speed: 0.3188s/iter; left time: 6583.4658s
	iters: 200, epoch: 23 | loss: 0.8030865
	speed: 0.0699s/iter; left time: 1436.7057s
Epoch: 23 cost time: 19.436540842056274
Epoch: 23, Steps: 266 | Train Loss: 0.4417954 Vali Loss: 0.2188945 Test Loss: 0.3077976
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5040346
	speed: 0.3121s/iter; left time: 6362.1467s
	iters: 200, epoch: 24 | loss: 0.4273443
	speed: 0.0699s/iter; left time: 1417.7380s
Epoch: 24 cost time: 19.4868106842041
Epoch: 24, Steps: 266 | Train Loss: 0.4411927 Vali Loss: 0.2191287 Test Loss: 0.3078099
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4521724
	speed: 0.3168s/iter; left time: 6372.3475s
	iters: 200, epoch: 25 | loss: 0.3437697
	speed: 0.0705s/iter; left time: 1411.7507s
Epoch: 25 cost time: 19.427513360977173
Epoch: 25, Steps: 266 | Train Loss: 0.4417032 Vali Loss: 0.2187322 Test Loss: 0.3077869
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3293767
	speed: 0.3193s/iter; left time: 6338.4200s
	iters: 200, epoch: 26 | loss: 0.5667734
	speed: 0.0718s/iter; left time: 1418.4217s
Epoch: 26 cost time: 19.936612844467163
Epoch: 26, Steps: 266 | Train Loss: 0.4418345 Vali Loss: 0.2189758 Test Loss: 0.3077625
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.30952250957489014, mae:0.3438154458999634, rse:0.449372798204422, corr:[0.55374664 0.5591067  0.5561386  0.55080956 0.5473844  0.54685116
 0.5475008  0.54729456 0.5458534  0.54404354 0.5428676  0.54261804
 0.5428211  0.5427161  0.5418517  0.5404568  0.53905517 0.5380775
 0.53752565 0.53703606 0.5362983  0.53529906 0.534276   0.5335415
 0.5331765  0.53299534 0.5327846  0.53239584 0.5318304  0.53123075
 0.5306604  0.5301226  0.52955765 0.52895886 0.5283452  0.52777004
 0.5271774  0.5265052  0.52575517 0.52498335 0.5243211  0.5238872
 0.52366185 0.52349913 0.5232309  0.5227322  0.5220436  0.521224
 0.52040046 0.5196736  0.51904064 0.51849884 0.51796246 0.5173929
 0.5168693  0.5164428  0.5161062  0.5158414  0.51560044 0.5153805
 0.51517123 0.5150021  0.51491094 0.51489216 0.51486844 0.5147849
 0.51463914 0.5144506  0.5143052  0.51423836 0.51423585 0.5142689
 0.5142493  0.5141573  0.5139355  0.5136353  0.51333386 0.51304144
 0.51275146 0.51244015 0.5121018  0.5117458  0.51140195 0.511069
 0.51073015 0.510358   0.50996846 0.50959355 0.50924784 0.50892586
 0.5085465  0.50796914 0.5071459  0.50603443 0.5045632  0.50268006
 0.5004888  0.49827018 0.49622712 0.49441174 0.49276453 0.4912458
 0.48977938 0.48831293 0.4869628  0.48581228 0.48481807 0.48382473
 0.48273823 0.4815695  0.48035747 0.47911045 0.47784495 0.4765623
 0.47529513 0.47411826 0.4730643  0.47211394 0.47121924 0.47029522
 0.46935123 0.46841    0.4674805  0.4665639  0.46563953 0.4646976
 0.4637485  0.46279988 0.4618162  0.46084917 0.4599527  0.45911792
 0.4583124  0.45754656 0.45680317 0.45621997 0.45576018 0.45544574
 0.45520967 0.45496124 0.4546371  0.45417887 0.45359656 0.45290527
 0.4521268  0.45130724 0.45046481 0.44970417 0.4490177  0.44840997
 0.44786808 0.4474937  0.4471871  0.44677833 0.44626743 0.4457178
 0.4452053  0.44486293 0.44466087 0.44462833 0.44466186 0.44462916
 0.44456536 0.44439825 0.4443003  0.4442974  0.44435167 0.44444132
 0.44454345 0.44463995 0.44469368 0.44468784 0.44464403 0.44457743
 0.44451854 0.44447455 0.44441712 0.4443332  0.44421208 0.44407532
 0.44391334 0.44369864 0.4434656  0.44316417 0.44280753 0.44239292
 0.44191623 0.44133708 0.44053108 0.43939114 0.43785468 0.4359323
 0.43381155 0.43180254 0.43001696 0.42842707 0.42700472 0.425695
 0.42444125 0.42315927 0.42193905 0.4208936  0.42004526 0.41923422
 0.41829368 0.41719702 0.41595054 0.41463184 0.4133349  0.41215137
 0.41101104 0.40987632 0.40874225 0.40765995 0.4066745  0.40575564
 0.40486792 0.40391245 0.40290523 0.40180457 0.40063456 0.39950085
 0.39850548 0.3975879  0.39683756 0.39607525 0.39529297 0.39445135
 0.39355257 0.39264065 0.39169645 0.39085636 0.39018497 0.38974342
 0.38949192 0.38936535 0.38927802 0.38912874 0.38889155 0.3885406
 0.38812995 0.38772747 0.387347   0.3871934  0.38718945 0.3871543
 0.38710234 0.38704363 0.38696247 0.3867847  0.38659698 0.3864919
 0.3865058  0.3866778  0.38696805 0.3872532  0.38744327 0.38748667
 0.38738775 0.38723183 0.38713467 0.38726562 0.38765994 0.38817832
 0.38868672 0.3890152  0.38920665 0.38931185 0.38947234 0.38977072
 0.39012817 0.39049563 0.3907675  0.39094368 0.39109167 0.39125475
 0.39147064 0.39167935 0.39179653 0.391807   0.3917254  0.3916863
 0.3917429  0.39183012 0.3917884  0.3914265  0.39059806 0.38930714
 0.387846   0.38667426 0.38600025 0.38576138 0.3856951  0.38560477
 0.38543937 0.38518757 0.38502887 0.3851068  0.38531974 0.38539314
 0.38512516 0.38450253 0.3836678  0.38286242 0.38224804 0.38178983
 0.38128424 0.38050327 0.37945133 0.3783813  0.37756294 0.37718192
 0.37706402 0.37688875 0.37630358 0.37523013 0.37401575 0.37313995
 0.3728122  0.3728086  0.37255585 0.37161493 0.37005562 0.36851463
 0.36764374 0.36764097 0.36790594 0.36724296 0.36513808 0.3623955
 0.36079463 0.36158752 0.36398005 0.3643911  0.35856456 0.34510508]
