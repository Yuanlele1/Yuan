Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_360_720_FITS_ETTm2_ftM_sl360_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=26, out_features=78, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1817088.0
params:  2106.0
Trainable parameters:  2106
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5097967
	speed: 0.0882s/iter; left time: 2293.5761s
	iters: 200, epoch: 1 | loss: 0.5234054
	speed: 0.0794s/iter; left time: 2057.4404s
Epoch: 1 cost time: 21.711812496185303
Epoch: 1, Steps: 261 | Train Loss: 0.6303164 Vali Loss: 0.2913535 Test Loss: 0.3977658
Validation loss decreased (inf --> 0.291354).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5371317
	speed: 0.3648s/iter; left time: 9391.0673s
	iters: 200, epoch: 2 | loss: 0.4745391
	speed: 0.0948s/iter; left time: 2430.3443s
Epoch: 2 cost time: 25.05173397064209
Epoch: 2, Steps: 261 | Train Loss: 0.5494666 Vali Loss: 0.2774726 Test Loss: 0.3806635
Validation loss decreased (0.291354 --> 0.277473).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3802005
	speed: 0.4309s/iter; left time: 10979.0870s
	iters: 200, epoch: 3 | loss: 0.5562842
	speed: 0.1161s/iter; left time: 2947.3778s
Epoch: 3 cost time: 27.26995062828064
Epoch: 3, Steps: 261 | Train Loss: 0.5373556 Vali Loss: 0.2733280 Test Loss: 0.3762419
Validation loss decreased (0.277473 --> 0.273328).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5322804
	speed: 0.4329s/iter; left time: 10915.7606s
	iters: 200, epoch: 4 | loss: 0.7275094
	speed: 0.1567s/iter; left time: 3935.4265s
Epoch: 4 cost time: 33.853020906448364
Epoch: 4, Steps: 261 | Train Loss: 0.5323861 Vali Loss: 0.2709661 Test Loss: 0.3741293
Validation loss decreased (0.273328 --> 0.270966).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4041141
	speed: 0.3773s/iter; left time: 9415.4248s
	iters: 200, epoch: 5 | loss: 0.3934600
	speed: 0.0928s/iter; left time: 2307.0135s
Epoch: 5 cost time: 23.706541776657104
Epoch: 5, Steps: 261 | Train Loss: 0.5290140 Vali Loss: 0.2701080 Test Loss: 0.3728479
Validation loss decreased (0.270966 --> 0.270108).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4900392
	speed: 0.3764s/iter; left time: 9296.3156s
	iters: 200, epoch: 6 | loss: 0.8325885
	speed: 0.1004s/iter; left time: 2470.0939s
Epoch: 6 cost time: 25.610780239105225
Epoch: 6, Steps: 261 | Train Loss: 0.5272526 Vali Loss: 0.2690894 Test Loss: 0.3719755
Validation loss decreased (0.270108 --> 0.269089).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5983598
	speed: 0.4033s/iter; left time: 9855.7437s
	iters: 200, epoch: 7 | loss: 0.3953296
	speed: 0.0956s/iter; left time: 2325.3150s
Epoch: 7 cost time: 26.42566418647766
Epoch: 7, Steps: 261 | Train Loss: 0.5251284 Vali Loss: 0.2683136 Test Loss: 0.3713899
Validation loss decreased (0.269089 --> 0.268314).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6124877
	speed: 0.3825s/iter; left time: 9247.6509s
	iters: 200, epoch: 8 | loss: 0.7826173
	speed: 0.0847s/iter; left time: 2038.1438s
Epoch: 8 cost time: 22.62362241744995
Epoch: 8, Steps: 261 | Train Loss: 0.5245308 Vali Loss: 0.2673794 Test Loss: 0.3710733
Validation loss decreased (0.268314 --> 0.267379).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6202704
	speed: 0.3796s/iter; left time: 9078.4191s
	iters: 200, epoch: 9 | loss: 0.4840163
	speed: 0.0988s/iter; left time: 2353.6848s
Epoch: 9 cost time: 26.13919734954834
Epoch: 9, Steps: 261 | Train Loss: 0.5234847 Vali Loss: 0.2674130 Test Loss: 0.3707485
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4141269
	speed: 0.3820s/iter; left time: 9035.6731s
	iters: 200, epoch: 10 | loss: 0.6402360
	speed: 0.1116s/iter; left time: 2628.3507s
Epoch: 10 cost time: 27.971479654312134
Epoch: 10, Steps: 261 | Train Loss: 0.5227725 Vali Loss: 0.2669103 Test Loss: 0.3705837
Validation loss decreased (0.267379 --> 0.266910).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3757890
	speed: 0.3994s/iter; left time: 9341.4922s
	iters: 200, epoch: 11 | loss: 0.4001680
	speed: 0.0873s/iter; left time: 2033.2793s
Epoch: 11 cost time: 22.605348587036133
Epoch: 11, Steps: 261 | Train Loss: 0.5221182 Vali Loss: 0.2666967 Test Loss: 0.3704466
Validation loss decreased (0.266910 --> 0.266697).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6602656
	speed: 0.4171s/iter; left time: 9647.9405s
	iters: 200, epoch: 12 | loss: 0.6820045
	speed: 0.0857s/iter; left time: 1974.4858s
Epoch: 12 cost time: 23.61987018585205
Epoch: 12, Steps: 261 | Train Loss: 0.5221986 Vali Loss: 0.2666063 Test Loss: 0.3703255
Validation loss decreased (0.266697 --> 0.266606).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5231403
	speed: 0.3981s/iter; left time: 9104.5359s
	iters: 200, epoch: 13 | loss: 0.6066662
	speed: 0.1045s/iter; left time: 2380.0137s
Epoch: 13 cost time: 27.005908727645874
Epoch: 13, Steps: 261 | Train Loss: 0.5216710 Vali Loss: 0.2663165 Test Loss: 0.3701555
Validation loss decreased (0.266606 --> 0.266316).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5671861
	speed: 0.4455s/iter; left time: 10072.9464s
	iters: 200, epoch: 14 | loss: 0.5117195
	speed: 0.0877s/iter; left time: 1973.0019s
Epoch: 14 cost time: 24.633373975753784
Epoch: 14, Steps: 261 | Train Loss: 0.5216932 Vali Loss: 0.2660005 Test Loss: 0.3701015
Validation loss decreased (0.266316 --> 0.266000).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4142859
	speed: 0.4009s/iter; left time: 8957.9612s
	iters: 200, epoch: 15 | loss: 0.6135445
	speed: 0.0946s/iter; left time: 2104.7942s
Epoch: 15 cost time: 25.845536470413208
Epoch: 15, Steps: 261 | Train Loss: 0.5214839 Vali Loss: 0.2658740 Test Loss: 0.3700266
Validation loss decreased (0.266000 --> 0.265874).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4942832
	speed: 0.3767s/iter; left time: 8319.3660s
	iters: 200, epoch: 16 | loss: 0.4025808
	speed: 0.0828s/iter; left time: 1819.3707s
Epoch: 16 cost time: 22.436641693115234
Epoch: 16, Steps: 261 | Train Loss: 0.5212098 Vali Loss: 0.2661236 Test Loss: 0.3699820
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4198227
	speed: 0.4145s/iter; left time: 9047.1061s
	iters: 200, epoch: 17 | loss: 0.4142217
	speed: 0.0795s/iter; left time: 1727.1986s
Epoch: 17 cost time: 22.044995546340942
Epoch: 17, Steps: 261 | Train Loss: 0.5208029 Vali Loss: 0.2661648 Test Loss: 0.3699273
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4821365
	speed: 0.3357s/iter; left time: 7239.9241s
	iters: 200, epoch: 18 | loss: 0.5144206
	speed: 0.0979s/iter; left time: 2101.4457s
Epoch: 18 cost time: 23.93253445625305
Epoch: 18, Steps: 261 | Train Loss: 0.5212052 Vali Loss: 0.2660481 Test Loss: 0.3699327
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4470085
	speed: 0.3640s/iter; left time: 7754.5700s
	iters: 200, epoch: 19 | loss: 0.5914214
	speed: 0.0837s/iter; left time: 1775.6064s
Epoch: 19 cost time: 21.273626565933228
Epoch: 19, Steps: 261 | Train Loss: 0.5208529 Vali Loss: 0.2657097 Test Loss: 0.3698902
Validation loss decreased (0.265874 --> 0.265710).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4990009
	speed: 0.3654s/iter; left time: 7688.7875s
	iters: 200, epoch: 20 | loss: 0.6250735
	speed: 0.1074s/iter; left time: 2248.8201s
Epoch: 20 cost time: 27.109906435012817
Epoch: 20, Steps: 261 | Train Loss: 0.5206079 Vali Loss: 0.2656652 Test Loss: 0.3698121
Validation loss decreased (0.265710 --> 0.265665).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4127856
	speed: 0.3835s/iter; left time: 7969.6945s
	iters: 200, epoch: 21 | loss: 0.5051626
	speed: 0.0800s/iter; left time: 1654.4902s
Epoch: 21 cost time: 22.10875701904297
Epoch: 21, Steps: 261 | Train Loss: 0.5202403 Vali Loss: 0.2658060 Test Loss: 0.3698435
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4100797
	speed: 0.3705s/iter; left time: 7603.3996s
	iters: 200, epoch: 22 | loss: 0.3622161
	speed: 0.0860s/iter; left time: 1755.6584s
Epoch: 22 cost time: 23.29249930381775
Epoch: 22, Steps: 261 | Train Loss: 0.5207108 Vali Loss: 0.2657090 Test Loss: 0.3698148
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5486162
	speed: 0.3931s/iter; left time: 7964.3655s
	iters: 200, epoch: 23 | loss: 0.4291904
	speed: 0.0851s/iter; left time: 1715.4535s
Epoch: 23 cost time: 23.995079278945923
Epoch: 23, Steps: 261 | Train Loss: 0.5200459 Vali Loss: 0.2657040 Test Loss: 0.3697829
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5684394
	speed: 0.4079s/iter; left time: 8156.8439s
	iters: 200, epoch: 24 | loss: 0.6028322
	speed: 0.0837s/iter; left time: 1664.7300s
Epoch: 24 cost time: 23.45875358581543
Epoch: 24, Steps: 261 | Train Loss: 0.5205429 Vali Loss: 0.2655870 Test Loss: 0.3697324
Validation loss decreased (0.265665 --> 0.265587).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5597554
	speed: 0.3598s/iter; left time: 7101.5868s
	iters: 200, epoch: 25 | loss: 0.4880514
	speed: 0.0926s/iter; left time: 1818.3694s
Epoch: 25 cost time: 24.394601583480835
Epoch: 25, Steps: 261 | Train Loss: 0.5204787 Vali Loss: 0.2654026 Test Loss: 0.3697379
Validation loss decreased (0.265587 --> 0.265403).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.7091457
	speed: 0.4093s/iter; left time: 7971.5300s
	iters: 200, epoch: 26 | loss: 0.7314036
	speed: 0.1076s/iter; left time: 2084.4413s
Epoch: 26 cost time: 28.737037897109985
Epoch: 26, Steps: 261 | Train Loss: 0.5203925 Vali Loss: 0.2654532 Test Loss: 0.3697391
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5504236
	speed: 0.4085s/iter; left time: 7848.5952s
	iters: 200, epoch: 27 | loss: 0.4757040
	speed: 0.0838s/iter; left time: 1602.0016s
Epoch: 27 cost time: 22.247122526168823
Epoch: 27, Steps: 261 | Train Loss: 0.5197343 Vali Loss: 0.2659705 Test Loss: 0.3697166
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5049485
	speed: 0.3720s/iter; left time: 7051.0412s
	iters: 200, epoch: 28 | loss: 0.7004539
	speed: 0.0966s/iter; left time: 1821.1141s
Epoch: 28 cost time: 25.110801935195923
Epoch: 28, Steps: 261 | Train Loss: 0.5202748 Vali Loss: 0.2657487 Test Loss: 0.3697079
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.6766427
	speed: 0.3565s/iter; left time: 6664.9063s
	iters: 200, epoch: 29 | loss: 0.6169283
	speed: 0.1004s/iter; left time: 1867.4040s
Epoch: 29 cost time: 24.009454488754272
Epoch: 29, Steps: 261 | Train Loss: 0.5201686 Vali Loss: 0.2654338 Test Loss: 0.3697281
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.5790930
	speed: 0.3658s/iter; left time: 6741.7363s
	iters: 200, epoch: 30 | loss: 0.5128265
	speed: 0.0950s/iter; left time: 1740.7797s
Epoch: 30 cost time: 23.14517593383789
Epoch: 30, Steps: 261 | Train Loss: 0.5202061 Vali Loss: 0.2656689 Test Loss: 0.3697113
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4959310
	speed: 0.3752s/iter; left time: 6816.9106s
	iters: 200, epoch: 31 | loss: 0.6433479
	speed: 0.1045s/iter; left time: 1888.9296s
Epoch: 31 cost time: 27.8547306060791
Epoch: 31, Steps: 261 | Train Loss: 0.5198861 Vali Loss: 0.2655738 Test Loss: 0.3697136
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4640173
	speed: 0.4230s/iter; left time: 7576.6235s
	iters: 200, epoch: 32 | loss: 0.4187796
	speed: 0.1045s/iter; left time: 1860.3962s
Epoch: 32 cost time: 25.776235342025757
Epoch: 32, Steps: 261 | Train Loss: 0.5203401 Vali Loss: 0.2651552 Test Loss: 0.3697087
Validation loss decreased (0.265403 --> 0.265155).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3624116
	speed: 0.3590s/iter; left time: 6336.4706s
	iters: 200, epoch: 33 | loss: 0.4018085
	speed: 0.0841s/iter; left time: 1476.6056s
Epoch: 33 cost time: 23.1985867023468
Epoch: 33, Steps: 261 | Train Loss: 0.5199109 Vali Loss: 0.2654636 Test Loss: 0.3697001
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4117583
	speed: 0.4072s/iter; left time: 7080.4752s
	iters: 200, epoch: 34 | loss: 0.6092821
	speed: 0.0887s/iter; left time: 1533.6440s
Epoch: 34 cost time: 23.857893466949463
Epoch: 34, Steps: 261 | Train Loss: 0.5199735 Vali Loss: 0.2656278 Test Loss: 0.3697111
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5167072
	speed: 0.4150s/iter; left time: 7106.9847s
	iters: 200, epoch: 35 | loss: 0.5592922
	speed: 0.1033s/iter; left time: 1759.6571s
Epoch: 35 cost time: 27.059025526046753
Epoch: 35, Steps: 261 | Train Loss: 0.5197741 Vali Loss: 0.2653631 Test Loss: 0.3696938
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3995886
	speed: 0.3961s/iter; left time: 6681.0398s
	iters: 200, epoch: 36 | loss: 0.5925614
	speed: 0.0972s/iter; left time: 1629.9128s
Epoch: 36 cost time: 25.55408501625061
Epoch: 36, Steps: 261 | Train Loss: 0.5201757 Vali Loss: 0.2656277 Test Loss: 0.3696843
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.6834157
	speed: 0.4132s/iter; left time: 6860.4380s
	iters: 200, epoch: 37 | loss: 0.4799203
	speed: 0.0971s/iter; left time: 1602.3745s
Epoch: 37 cost time: 25.010274648666382
Epoch: 37, Steps: 261 | Train Loss: 0.5199505 Vali Loss: 0.2654492 Test Loss: 0.3696893
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3881538
	speed: 0.3785s/iter; left time: 6185.6892s
	iters: 200, epoch: 38 | loss: 0.4548427
	speed: 0.0973s/iter; left time: 1579.7641s
Epoch: 38 cost time: 24.5197594165802
Epoch: 38, Steps: 261 | Train Loss: 0.5196435 Vali Loss: 0.2657138 Test Loss: 0.3696881
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.6506816
	speed: 0.3612s/iter; left time: 5808.5152s
	iters: 200, epoch: 39 | loss: 0.5139263
	speed: 0.0843s/iter; left time: 1346.7417s
Epoch: 39 cost time: 22.761406183242798
Epoch: 39, Steps: 261 | Train Loss: 0.5196974 Vali Loss: 0.2653257 Test Loss: 0.3696877
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4134091
	speed: 0.3933s/iter; left time: 6222.1442s
	iters: 200, epoch: 40 | loss: 0.4989927
	speed: 0.0991s/iter; left time: 1557.5433s
Epoch: 40 cost time: 27.458213806152344
Epoch: 40, Steps: 261 | Train Loss: 0.5201195 Vali Loss: 0.2653658 Test Loss: 0.3696754
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4148840
	speed: 0.3901s/iter; left time: 6070.4634s
	iters: 200, epoch: 41 | loss: 0.4065536
	speed: 0.0762s/iter; left time: 1178.6755s
Epoch: 41 cost time: 22.288689374923706
Epoch: 41, Steps: 261 | Train Loss: 0.5190728 Vali Loss: 0.2653863 Test Loss: 0.3696775
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.5220801
	speed: 0.3560s/iter; left time: 5446.3682s
	iters: 200, epoch: 42 | loss: 0.3287596
	speed: 0.0870s/iter; left time: 1322.0826s
Epoch: 42 cost time: 21.93889617919922
Epoch: 42, Steps: 261 | Train Loss: 0.5199533 Vali Loss: 0.2656546 Test Loss: 0.3696733
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.5443546
	speed: 0.3463s/iter; left time: 5208.5693s
	iters: 200, epoch: 43 | loss: 0.4843870
	speed: 0.0873s/iter; left time: 1304.4659s
Epoch: 43 cost time: 23.342811346054077
Epoch: 43, Steps: 261 | Train Loss: 0.5197151 Vali Loss: 0.2653100 Test Loss: 0.3696716
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.6092350
	speed: 0.3520s/iter; left time: 5202.4930s
	iters: 200, epoch: 44 | loss: 0.5183812
	speed: 0.0897s/iter; left time: 1316.7214s
Epoch: 44 cost time: 21.779948949813843
Epoch: 44, Steps: 261 | Train Loss: 0.5196132 Vali Loss: 0.2653305 Test Loss: 0.3696679
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5005068
	speed: 0.3750s/iter; left time: 5444.5867s
	iters: 200, epoch: 45 | loss: 0.4510933
	speed: 0.1047s/iter; left time: 1508.7653s
Epoch: 45 cost time: 25.500296115875244
Epoch: 45, Steps: 261 | Train Loss: 0.5198292 Vali Loss: 0.2653471 Test Loss: 0.3696639
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4572264
	speed: 0.4442s/iter; left time: 6332.6361s
	iters: 200, epoch: 46 | loss: 0.4199608
	speed: 0.1134s/iter; left time: 1605.4390s
Epoch: 46 cost time: 32.26689624786377
Epoch: 46, Steps: 261 | Train Loss: 0.5196078 Vali Loss: 0.2655403 Test Loss: 0.3696636
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3126559
	speed: 0.5032s/iter; left time: 7042.9012s
	iters: 200, epoch: 47 | loss: 0.4334617
	speed: 0.1186s/iter; left time: 1648.5167s
Epoch: 47 cost time: 31.97679901123047
Epoch: 47, Steps: 261 | Train Loss: 0.5193706 Vali Loss: 0.2655304 Test Loss: 0.3696659
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.6541279
	speed: 0.4514s/iter; left time: 6200.1985s
	iters: 200, epoch: 48 | loss: 0.4754805
	speed: 0.0989s/iter; left time: 1349.0266s
Epoch: 48 cost time: 26.470685243606567
Epoch: 48, Steps: 261 | Train Loss: 0.5196409 Vali Loss: 0.2653501 Test Loss: 0.3696671
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4406798
	speed: 0.4978s/iter; left time: 6706.8766s
	iters: 200, epoch: 49 | loss: 0.5514065
	speed: 0.1078s/iter; left time: 1442.0924s
Epoch: 49 cost time: 29.487873792648315
Epoch: 49, Steps: 261 | Train Loss: 0.5198654 Vali Loss: 0.2653538 Test Loss: 0.3696611
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.5806018
	speed: 0.4754s/iter; left time: 6280.7255s
	iters: 200, epoch: 50 | loss: 0.5154197
	speed: 0.1075s/iter; left time: 1408.9915s
Epoch: 50 cost time: 31.54029607772827
Epoch: 50, Steps: 261 | Train Loss: 0.5196593 Vali Loss: 0.2654500 Test Loss: 0.3696612
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.5592345
	speed: 0.4566s/iter; left time: 5913.9715s
	iters: 200, epoch: 51 | loss: 0.4842308
	speed: 0.1020s/iter; left time: 1310.5303s
Epoch: 51 cost time: 27.216540336608887
Epoch: 51, Steps: 261 | Train Loss: 0.5196974 Vali Loss: 0.2653361 Test Loss: 0.3696630
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.5749084
	speed: 0.4971s/iter; left time: 6308.1737s
	iters: 200, epoch: 52 | loss: 0.5716723
	speed: 0.1106s/iter; left time: 1392.0119s
Epoch: 52 cost time: 28.59844136238098
Epoch: 52, Steps: 261 | Train Loss: 0.5195346 Vali Loss: 0.2652129 Test Loss: 0.3696601
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_360_720_FITS_ETTm2_ftM_sl360_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3669785261154175, mae:0.38301384449005127, rse:0.48692795634269714, corr:[0.53168404 0.5364838  0.53915536 0.5396747  0.53883207 0.53739357
 0.53605694 0.535105   0.5346377  0.5346481  0.5349566  0.5354058
 0.53579867 0.53593135 0.535726   0.5351398  0.53430504 0.53336513
 0.5324621  0.5316895  0.5311487  0.5308948  0.53083444 0.5309214
 0.5310029  0.53092474 0.5306299  0.5301357  0.5295211  0.52883554
 0.5281525  0.52756274 0.52706736 0.52663887 0.5262406  0.52584285
 0.5254226  0.52498347 0.52453876 0.52409023 0.5236169  0.5231168
 0.5225554  0.5218966  0.52113575 0.52025354 0.51932824 0.51842433
 0.5175643  0.516764   0.5160021  0.51531976 0.5146688  0.514016
 0.5133689  0.51275617 0.51219505 0.5117151  0.51134527 0.51106524
 0.51086754 0.5107035  0.51054883 0.5103524  0.5100918  0.5098022
 0.50947917 0.5091692  0.5089081  0.50871754 0.5085713  0.50843257
 0.5082678  0.50805944 0.50777704 0.5073843  0.5069122  0.506365
 0.50579107 0.50519466 0.50461966 0.5040457  0.5034849  0.5029128
 0.50236195 0.501855   0.5013756  0.50093424 0.5004923  0.50000703
 0.49943057 0.49871355 0.49777335 0.4965948  0.495209   0.49366912
 0.49204394 0.49047884 0.4890321  0.4876893  0.4864126  0.48521334
 0.4840731  0.48293936 0.4817733  0.4806249  0.47954974 0.47858864
 0.47774142 0.47694978 0.47619253 0.47540915 0.47459474 0.4737371
 0.47286418 0.47202647 0.471269   0.47060108 0.4700483  0.4695398
 0.4689957  0.46833238 0.4675812  0.46674165 0.46582562 0.46486536
 0.4639359  0.4630552  0.46222252 0.46144825 0.4607529  0.46011934
 0.45952687 0.45895877 0.45841277 0.4578903  0.4573242  0.45672625
 0.45605192 0.45528337 0.4544167  0.45345744 0.45245484 0.4514297
 0.4504417  0.4495407  0.4487849  0.44816592 0.44762874 0.44709376
 0.4465349  0.4459426  0.44526726 0.4445322  0.44379303 0.44311687
 0.44252563 0.44202274 0.4415594  0.44117886 0.44083282 0.4405015
 0.44021252 0.43997028 0.43981934 0.43975407 0.4396876  0.4396117
 0.4394461  0.4391724  0.438775   0.43826902 0.4376582  0.4370135
 0.43638995 0.4358469  0.43538624 0.43498504 0.43462187 0.43428195
 0.43393    0.4335506  0.4331742  0.43278232 0.4323598  0.4318753
 0.43128335 0.43051013 0.42951408 0.42826274 0.42684263 0.4252584
 0.42359555 0.42204383 0.42058802 0.41917562 0.41780752 0.4164292
 0.41503286 0.41358486 0.41207972 0.41057712 0.40915018 0.40786827
 0.40675083 0.405798   0.4049236  0.4040744  0.40320832 0.40235168
 0.40146562 0.40052706 0.39957663 0.3986909  0.39784488 0.3970172
 0.39615357 0.39505428 0.39377964 0.3924027  0.3909974  0.38964507
 0.3884669  0.38747588 0.38669947 0.38600612 0.38530484 0.3845177
 0.38365325 0.382777   0.38192293 0.3811264  0.38040936 0.37978676
 0.37918493 0.3785488  0.37788543 0.37716883 0.37643352 0.3757638
 0.37519863 0.3747669  0.37446368 0.37436125 0.37429923 0.37421364
 0.37408957 0.37388095 0.37357768 0.37322655 0.37290478 0.37271124
 0.37261173 0.37265766 0.37275153 0.3728274  0.37281585 0.37267584
 0.37246636 0.3722289  0.37202716 0.3718692  0.37168688 0.37149048
 0.37128425 0.37106514 0.37081409 0.3704848  0.3701292  0.36981004
 0.3695408  0.36930823 0.3691059  0.3689645  0.3688169  0.36864585
 0.3683922  0.3680567  0.3676839  0.36733904 0.3670337  0.36679485
 0.36655292 0.36623892 0.36577857 0.3651028  0.36415538 0.36297846
 0.36169133 0.360487   0.35941294 0.35846347 0.3576425  0.35693413
 0.35626596 0.35560313 0.35495335 0.35426578 0.3536034  0.35303447
 0.3525644  0.35210586 0.3516632  0.3512149  0.35072365 0.35018164
 0.3497211  0.3493481  0.34908134 0.3489986  0.3490697  0.34922966
 0.3493278  0.34927866 0.34899512 0.3484965  0.34784642 0.34711403
 0.34641823 0.3458147  0.3453109  0.34493002 0.34466702 0.3444965
 0.34436512 0.34430188 0.34428394 0.34425986 0.3441694  0.3440338
 0.34383312 0.3435397  0.34315655 0.34265572 0.3420548  0.34147915
 0.34097403 0.34059435 0.34037724 0.34034055 0.34042236 0.34054378
 0.34060976 0.34061712 0.34055048 0.34038877 0.34019065 0.3399979
 0.33985442 0.339762   0.3396619  0.33954233 0.33940968 0.33923557
 0.33902243 0.33884114 0.33868584 0.3385682  0.33847964 0.33835804
 0.33819264 0.33799547 0.33771935 0.33738607 0.33700916 0.3366843
 0.3364247  0.3362884  0.33625892 0.33629325 0.33637232 0.33640316
 0.33635876 0.33626735 0.33616355 0.33604652 0.3359239  0.3358164
 0.33570057 0.33552742 0.33522648 0.33468905 0.3339659  0.33305213
 0.3320042  0.33093363 0.32991454 0.3289689  0.32804984 0.32718706
 0.32633004 0.32543758 0.32455367 0.3236738  0.32282576 0.32203835
 0.32135698 0.32081333 0.320265   0.31966755 0.31900507 0.3182587
 0.31752667 0.31685472 0.31634971 0.3160686  0.31597885 0.3160265
 0.31610003 0.31606913 0.31588778 0.31557107 0.31513882 0.31467545
 0.31425494 0.31394064 0.31371403 0.313542   0.3133847  0.31324026
 0.3131164  0.3130585  0.31305707 0.31309786 0.31317836 0.3132615
 0.3132838  0.31322873 0.31307098 0.3127754  0.31238642 0.31194806
 0.3115005  0.31110442 0.3107725  0.3105456  0.3103837  0.31023663
 0.31001753 0.309731   0.30930707 0.3088212  0.30830058 0.30783024
 0.3074356  0.30715853 0.30700603 0.3069052  0.30681896 0.30671206
 0.3066515  0.30663767 0.30663863 0.30662838 0.30656615 0.30641738
 0.30616826 0.3058176  0.3053448  0.30480236 0.30428007 0.30379257
 0.3034172  0.303173   0.30307814 0.30306503 0.30300516 0.30287924
 0.30262002 0.3022157  0.3016422  0.30094224 0.3001853  0.29939175
 0.2985583  0.29764384 0.29665694 0.29558676 0.29443386 0.29319566
 0.29195544 0.29078755 0.28971598 0.28875574 0.28785735 0.2869763
 0.28608322 0.28517884 0.28428164 0.2834541  0.28268397 0.2819474
 0.2812563  0.28062043 0.28001302 0.27938676 0.27873555 0.27803776
 0.27734196 0.27667287 0.27606302 0.27555805 0.27517176 0.2748925
 0.27464578 0.2742982  0.27385542 0.27332604 0.2728056  0.27229297
 0.27183914 0.27146447 0.2711497  0.2708621  0.27058715 0.2702864
 0.26999512 0.26970768 0.26945236 0.26925582 0.26910737 0.2690075
 0.26889023 0.26873407 0.2684746  0.2680959  0.26763454 0.26710907
 0.2665566  0.26606426 0.26564106 0.26530644 0.26506367 0.26490304
 0.26476756 0.26461354 0.26447996 0.26433176 0.2641818  0.26406956
 0.26396123 0.2638801  0.26379058 0.26369157 0.26353726 0.26334986
 0.2631378  0.26295933 0.26282954 0.26274937 0.2626479  0.2625379
 0.26236704 0.2621149  0.26180527 0.2614295  0.2610435  0.2606978
 0.26043484 0.26028743 0.26025313 0.26028097 0.26027203 0.26018584
 0.2599853  0.25965306 0.25922668 0.25868604 0.25811487 0.25753212
 0.2569278  0.25624344 0.25539628 0.2543529  0.25314823 0.2517762
 0.25030038 0.24887688 0.24756719 0.24639425 0.24532771 0.24435346
 0.24334775 0.24234769 0.241335   0.24032848 0.23939697 0.23859128
 0.23784913 0.23712036 0.23641743 0.23572724 0.23497769 0.23415567
 0.23332721 0.23255478 0.23190843 0.23140092 0.23106243 0.2308281
 0.23061086 0.23031433 0.22989003 0.22935067 0.22875227 0.22816266
 0.22763763 0.22721101 0.22686277 0.22654216 0.22623478 0.2259021
 0.2255865  0.2252617  0.2249608  0.22473963 0.22460543 0.22458254
 0.22457287 0.2245294  0.22436383 0.2240742  0.2237109  0.22332501
 0.22296704 0.22263558 0.22237615 0.2222369  0.22218725 0.22211772
 0.2220139  0.22190408 0.22179669 0.22164679 0.2215248  0.22143087
 0.22149841 0.22166225 0.22183298 0.22199567 0.22205968 0.22203623
 0.22187519 0.22168234 0.22148916 0.22128555 0.22118227 0.22103277
 0.2208464  0.22061983 0.22034113 0.22008112 0.2198912  0.21978481
 0.21988286 0.2202033  0.22065629 0.22122057 0.22178815 0.22221835
 0.2225182  0.2227228  0.2228737  0.22297744 0.22311307 0.22326002
 0.22335143 0.22330594 0.22305736 0.22245418 0.22145627 0.22017516
 0.2187299  0.2173106  0.21602114 0.214894   0.21395244 0.2132402
 0.21262008 0.212053   0.21157084 0.21112731 0.21066143 0.21019977
 0.20979767 0.20937425 0.20891963 0.2083968  0.20788442 0.20729625
 0.20676534 0.20633148 0.20604421 0.20606035 0.2063175  0.20667174
 0.20697597 0.20697208 0.20659357 0.20587032 0.20495754 0.20396757
 0.20315179 0.20252427 0.20213169 0.20197283 0.20187454 0.20179398
 0.20153652 0.20113505 0.200637   0.20008492 0.19948477 0.19891183
 0.19842093 0.19805829 0.19784327 0.19800901 0.19841653 0.19898982]
