Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=16, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=16, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2064384.0
params:  2448.0
Trainable parameters:  2448
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8236187
	speed: 0.0237s/iter; left time: 621.3043s
	iters: 200, epoch: 1 | loss: 0.5783688
	speed: 0.0208s/iter; left time: 544.0324s
Epoch: 1 cost time: 5.518681526184082
Epoch: 1, Steps: 263 | Train Loss: 0.7597064 Vali Loss: 0.3397934 Test Loss: 0.4729145
Validation loss decreased (inf --> 0.339793).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4445918
	speed: 0.0820s/iter; left time: 2127.0334s
	iters: 200, epoch: 2 | loss: 0.5394433
	speed: 0.0222s/iter; left time: 572.6212s
Epoch: 2 cost time: 5.631514549255371
Epoch: 2, Steps: 263 | Train Loss: 0.6231446 Vali Loss: 0.2993942 Test Loss: 0.4240867
Validation loss decreased (0.339793 --> 0.299394).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5155248
	speed: 0.0851s/iter; left time: 2184.3468s
	iters: 200, epoch: 3 | loss: 0.5043334
	speed: 0.0179s/iter; left time: 457.4445s
Epoch: 3 cost time: 5.149283170700073
Epoch: 3, Steps: 263 | Train Loss: 0.5978856 Vali Loss: 0.2915149 Test Loss: 0.4150555
Validation loss decreased (0.299394 --> 0.291515).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.8569229
	speed: 0.0824s/iter; left time: 2094.4889s
	iters: 200, epoch: 4 | loss: 0.7507378
	speed: 0.0171s/iter; left time: 432.6075s
Epoch: 4 cost time: 4.977175951004028
Epoch: 4, Steps: 263 | Train Loss: 0.5925518 Vali Loss: 0.2901260 Test Loss: 0.4128444
Validation loss decreased (0.291515 --> 0.290126).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4246837
	speed: 0.0795s/iter; left time: 1999.0972s
	iters: 200, epoch: 5 | loss: 0.6595586
	speed: 0.0188s/iter; left time: 471.4393s
Epoch: 5 cost time: 5.391942501068115
Epoch: 5, Steps: 263 | Train Loss: 0.5914672 Vali Loss: 0.2891959 Test Loss: 0.4120166
Validation loss decreased (0.290126 --> 0.289196).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4577740
	speed: 0.0864s/iter; left time: 2149.1877s
	iters: 200, epoch: 6 | loss: 0.4753395
	speed: 0.0176s/iter; left time: 435.9023s
Epoch: 6 cost time: 5.115410089492798
Epoch: 6, Steps: 263 | Train Loss: 0.5899812 Vali Loss: 0.2892608 Test Loss: 0.4116580
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4892687
	speed: 0.0866s/iter; left time: 2131.6540s
	iters: 200, epoch: 7 | loss: 0.6482310
	speed: 0.0274s/iter; left time: 672.3277s
Epoch: 7 cost time: 6.391785383224487
Epoch: 7, Steps: 263 | Train Loss: 0.5898148 Vali Loss: 0.2891690 Test Loss: 0.4114216
Validation loss decreased (0.289196 --> 0.289169).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5155311
	speed: 0.0819s/iter; left time: 1996.0224s
	iters: 200, epoch: 8 | loss: 0.5056878
	speed: 0.0171s/iter; left time: 415.4711s
Epoch: 8 cost time: 5.071579456329346
Epoch: 8, Steps: 263 | Train Loss: 0.5887072 Vali Loss: 0.2890648 Test Loss: 0.4112897
Validation loss decreased (0.289169 --> 0.289065).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5327432
	speed: 0.0779s/iter; left time: 1877.3500s
	iters: 200, epoch: 9 | loss: 0.6318664
	speed: 0.0166s/iter; left time: 397.1856s
Epoch: 9 cost time: 4.99666690826416
Epoch: 9, Steps: 263 | Train Loss: 0.5889058 Vali Loss: 0.2893102 Test Loss: 0.4112581
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.6110206
	speed: 0.0815s/iter; left time: 1942.6699s
	iters: 200, epoch: 10 | loss: 0.7137337
	speed: 0.0169s/iter; left time: 401.6290s
Epoch: 10 cost time: 5.079585075378418
Epoch: 10, Steps: 263 | Train Loss: 0.5883367 Vali Loss: 0.2893102 Test Loss: 0.4113367
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6065807
	speed: 0.0791s/iter; left time: 1864.6300s
	iters: 200, epoch: 11 | loss: 0.5903071
	speed: 0.0170s/iter; left time: 399.4478s
Epoch: 11 cost time: 5.143455266952515
Epoch: 11, Steps: 263 | Train Loss: 0.5882639 Vali Loss: 0.2890397 Test Loss: 0.4113219
Validation loss decreased (0.289065 --> 0.289040).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5901855
	speed: 0.0831s/iter; left time: 1936.6194s
	iters: 200, epoch: 12 | loss: 0.5992625
	speed: 0.0168s/iter; left time: 390.9519s
Epoch: 12 cost time: 5.085076570510864
Epoch: 12, Steps: 263 | Train Loss: 0.5879106 Vali Loss: 0.2892534 Test Loss: 0.4112301
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6111997
	speed: 0.0782s/iter; left time: 1801.2515s
	iters: 200, epoch: 13 | loss: 0.7617051
	speed: 0.0177s/iter; left time: 405.0499s
Epoch: 13 cost time: 5.163971900939941
Epoch: 13, Steps: 263 | Train Loss: 0.5883522 Vali Loss: 0.2892804 Test Loss: 0.4113365
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5366023
	speed: 0.0875s/iter; left time: 1993.2154s
	iters: 200, epoch: 14 | loss: 0.6519916
	speed: 0.0162s/iter; left time: 367.5233s
Epoch: 14 cost time: 4.920947551727295
Epoch: 14, Steps: 263 | Train Loss: 0.5873158 Vali Loss: 0.2892646 Test Loss: 0.4113303
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5908073
	speed: 0.0843s/iter; left time: 1898.5843s
	iters: 200, epoch: 15 | loss: 0.5652685
	speed: 0.0189s/iter; left time: 423.7301s
Epoch: 15 cost time: 5.405833959579468
Epoch: 15, Steps: 263 | Train Loss: 0.5877439 Vali Loss: 0.2894700 Test Loss: 0.4112447
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4906704
	speed: 0.0768s/iter; left time: 1709.1435s
	iters: 200, epoch: 16 | loss: 0.5280629
	speed: 0.0172s/iter; left time: 380.9393s
Epoch: 16 cost time: 5.154186964035034
Epoch: 16, Steps: 263 | Train Loss: 0.5877943 Vali Loss: 0.2894081 Test Loss: 0.4112292
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5165802
	speed: 0.0824s/iter; left time: 1812.8856s
	iters: 200, epoch: 17 | loss: 0.7535307
	speed: 0.0190s/iter; left time: 416.1267s
Epoch: 17 cost time: 5.443849325180054
Epoch: 17, Steps: 263 | Train Loss: 0.5876584 Vali Loss: 0.2894419 Test Loss: 0.4112767
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4947190
	speed: 0.0839s/iter; left time: 1823.6481s
	iters: 200, epoch: 18 | loss: 0.3965304
	speed: 0.0171s/iter; left time: 369.8828s
Epoch: 18 cost time: 5.1198601722717285
Epoch: 18, Steps: 263 | Train Loss: 0.5871362 Vali Loss: 0.2896873 Test Loss: 0.4112746
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3964165
	speed: 0.0831s/iter; left time: 1783.7425s
	iters: 200, epoch: 19 | loss: 0.7482254
	speed: 0.0174s/iter; left time: 372.3730s
Epoch: 19 cost time: 5.471879243850708
Epoch: 19, Steps: 263 | Train Loss: 0.5877395 Vali Loss: 0.2894635 Test Loss: 0.4112664
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.9724150
	speed: 0.0816s/iter; left time: 1730.8394s
	iters: 200, epoch: 20 | loss: 0.8425948
	speed: 0.0175s/iter; left time: 368.6523s
Epoch: 20 cost time: 5.249456167221069
Epoch: 20, Steps: 263 | Train Loss: 0.5879853 Vali Loss: 0.2894191 Test Loss: 0.4112799
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4989328
	speed: 0.0820s/iter; left time: 1717.3569s
	iters: 200, epoch: 21 | loss: 0.8248223
	speed: 0.0234s/iter; left time: 488.5919s
Epoch: 21 cost time: 5.827653884887695
Epoch: 21, Steps: 263 | Train Loss: 0.5875280 Vali Loss: 0.2894661 Test Loss: 0.4113303
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5878780
	speed: 0.0786s/iter; left time: 1626.3129s
	iters: 200, epoch: 22 | loss: 0.6258303
	speed: 0.0174s/iter; left time: 357.8723s
Epoch: 22 cost time: 5.166014194488525
Epoch: 22, Steps: 263 | Train Loss: 0.5873197 Vali Loss: 0.2894892 Test Loss: 0.4113497
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5514362
	speed: 0.0992s/iter; left time: 2026.1696s
	iters: 200, epoch: 23 | loss: 0.4378369
	speed: 0.0176s/iter; left time: 356.8119s
Epoch: 23 cost time: 5.415281534194946
Epoch: 23, Steps: 263 | Train Loss: 0.5875076 Vali Loss: 0.2895846 Test Loss: 0.4113233
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3963871
	speed: 0.0799s/iter; left time: 1610.6813s
	iters: 200, epoch: 24 | loss: 0.7821691
	speed: 0.0179s/iter; left time: 358.7906s
Epoch: 24 cost time: 5.111467599868774
Epoch: 24, Steps: 263 | Train Loss: 0.5864121 Vali Loss: 0.2895822 Test Loss: 0.4113369
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5078322
	speed: 0.0789s/iter; left time: 1568.4845s
	iters: 200, epoch: 25 | loss: 0.5332891
	speed: 0.0169s/iter; left time: 333.4537s
Epoch: 25 cost time: 4.965461492538452
Epoch: 25, Steps: 263 | Train Loss: 0.5874074 Vali Loss: 0.2896577 Test Loss: 0.4113086
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.7226606
	speed: 0.0793s/iter; left time: 1556.8130s
	iters: 200, epoch: 26 | loss: 0.7955989
	speed: 0.0165s/iter; left time: 322.0983s
Epoch: 26 cost time: 4.901888370513916
Epoch: 26, Steps: 263 | Train Loss: 0.5869614 Vali Loss: 0.2895025 Test Loss: 0.4113504
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5677558
	speed: 0.0858s/iter; left time: 1661.2633s
	iters: 200, epoch: 27 | loss: 0.4213566
	speed: 0.0185s/iter; left time: 355.7824s
Epoch: 27 cost time: 5.092628240585327
Epoch: 27, Steps: 263 | Train Loss: 0.5868329 Vali Loss: 0.2898551 Test Loss: 0.4113766
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4555528
	speed: 0.0775s/iter; left time: 1480.2055s
	iters: 200, epoch: 28 | loss: 0.4444427
	speed: 0.0169s/iter; left time: 320.3243s
Epoch: 28 cost time: 5.304414749145508
Epoch: 28, Steps: 263 | Train Loss: 0.5869260 Vali Loss: 0.2897919 Test Loss: 0.4113837
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.6832191
	speed: 0.0790s/iter; left time: 1487.2068s
	iters: 200, epoch: 29 | loss: 0.5105026
	speed: 0.0165s/iter; left time: 308.6077s
Epoch: 29 cost time: 5.019747018814087
Epoch: 29, Steps: 263 | Train Loss: 0.5876154 Vali Loss: 0.2898709 Test Loss: 0.4113567
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.5507070
	speed: 0.0796s/iter; left time: 1477.7316s
	iters: 200, epoch: 30 | loss: 0.7052602
	speed: 0.0167s/iter; left time: 308.6400s
Epoch: 30 cost time: 4.902324199676514
Epoch: 30, Steps: 263 | Train Loss: 0.5869366 Vali Loss: 0.2896354 Test Loss: 0.4113774
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.5780444
	speed: 0.0809s/iter; left time: 1480.8383s
	iters: 200, epoch: 31 | loss: 0.6157086
	speed: 0.0180s/iter; left time: 327.4549s
Epoch: 31 cost time: 5.192448616027832
Epoch: 31, Steps: 263 | Train Loss: 0.5868324 Vali Loss: 0.2898117 Test Loss: 0.4113836
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.408976674079895, mae:0.39869236946105957, rse:0.5140361785888672, corr:[0.5471296  0.54810876 0.5423484  0.53798664 0.53684634 0.5364695
 0.53481007 0.53224367 0.5304034  0.52983683 0.5296688  0.5288016
 0.5271072  0.5254798  0.5247092  0.52455956 0.52412874 0.52289945
 0.5212227  0.5198504  0.51916313 0.51882666 0.51827186 0.51737195
 0.5164401  0.5158568  0.51566166 0.51543945 0.51478577 0.51374406
 0.5126969  0.51203096 0.51171875 0.5114739  0.51100576 0.5102551
 0.5093097  0.5083814  0.50754887 0.50683737 0.5062687  0.5059127
 0.5057547  0.50563574 0.5053476  0.5047558  0.50394344 0.5030311
 0.5021584  0.5013387  0.50049937 0.4996499  0.49882433 0.49809134
 0.49757785 0.49718538 0.49676746 0.49631804 0.49592358 0.4957333
 0.49575996 0.4958738  0.49596465 0.49598244 0.49590418 0.4958175
 0.4957291  0.49564537 0.4955832  0.4955476  0.49556494 0.49565274
 0.4957604  0.49584806 0.49580902 0.4956577  0.4954616  0.49522406
 0.49499297 0.4947747  0.494569   0.49434042 0.49408603 0.49377626
 0.49342653 0.4930637  0.49271655 0.4924183  0.49211913 0.49178362
 0.49137363 0.49081025 0.49004787 0.48898342 0.4874631  0.48544946
 0.48311433 0.4807958  0.47870788 0.47686213 0.47515255 0.4735372
 0.47199154 0.47053668 0.4692348  0.46803492 0.46680063 0.46543548
 0.46399125 0.4626375  0.46142027 0.46018252 0.4588169  0.45730886
 0.45581654 0.4545498  0.45351866 0.4525485  0.45145908 0.4501702
 0.44890356 0.44785827 0.4470121  0.44614717 0.44509703 0.44385582
 0.4426259  0.44156528 0.44062805 0.43972567 0.43876898 0.43772635
 0.43668613 0.43577033 0.43499812 0.43444148 0.43391562 0.4334374
 0.43302917 0.4327255  0.43249506 0.432171   0.4316427  0.43087527
 0.42994997 0.42903206 0.42813286 0.42732745 0.42652503 0.42575145
 0.42504737 0.4246084  0.4242841  0.42381313 0.42324236 0.42271885
 0.42241362 0.42244783 0.42262545 0.42284694 0.42292884 0.42280242
 0.42264384 0.42240924 0.42228112 0.4222188  0.42218432 0.4222161
 0.4223791  0.4226411  0.4228588  0.42289683 0.4227589  0.4225359
 0.4223588  0.4223419  0.42245322 0.42262208 0.42277065 0.42287615
 0.42288303 0.42271122 0.4223746  0.4218442  0.42126593 0.42080206
 0.4205218  0.42027763 0.419762   0.4186796  0.4169275  0.4146282
 0.41217998 0.41002724 0.40814263 0.40631732 0.40448424 0.40266958
 0.40099034 0.39948496 0.39821216 0.3971104  0.3960373  0.39486942
 0.39357474 0.39223936 0.39085987 0.38942012 0.3879227  0.38646632
 0.3850381  0.38368654 0.38237885 0.38108966 0.37980247 0.3785086
 0.37726402 0.37606224 0.37498668 0.37387958 0.37264168 0.37129924
 0.3699787  0.36870086 0.3676894  0.3667422  0.3658182  0.36482567
 0.36375865 0.36263773 0.3614594  0.3604046  0.3595873  0.35913852
 0.35895228 0.35883158 0.35860088 0.35814604 0.35757485 0.35699263
 0.35650706 0.356141   0.355784   0.35568935 0.3557421  0.35578886
 0.35585654 0.3558907  0.35580814 0.35548    0.35513127 0.35503376
 0.3552467  0.35569784 0.35620403 0.35650933 0.35657826 0.3565391
 0.35653585 0.35667017 0.35689095 0.3572474  0.35769066 0.35814333
 0.35858527 0.35890135 0.35915756 0.35932255 0.3595222  0.35975283
 0.35994166 0.360049   0.3600877  0.36019543 0.36047834 0.3609109
 0.36139247 0.36176905 0.36196142 0.36207107 0.3621494  0.3623181
 0.36251757 0.36258334 0.36231983 0.36161765 0.3605283  0.35915503
 0.35769126 0.35638276 0.35531643 0.35455063 0.35406023 0.35390216
 0.35404444 0.35419455 0.35419032 0.35401607 0.3536741  0.3532309
 0.35278878 0.35231775 0.35169718 0.35083094 0.3497929  0.34873462
 0.34782305 0.34706587 0.34635842 0.34561023 0.3447944  0.3440733
 0.3434856  0.3430458  0.342568   0.34187472 0.3409517  0.33995658
 0.33906    0.3383618  0.33783367 0.33737463 0.3369163  0.3364562
 0.33598635 0.3355325  0.33508494 0.33453193 0.33399022 0.33361647
 0.33355305 0.33372313 0.33395866 0.3340142  0.33383217 0.33356673
 0.33332318 0.33310008 0.33290333 0.33282036 0.33286765 0.33303863
 0.33333224 0.33370876 0.3339657  0.3339847  0.33385807 0.33382776
 0.33409432 0.33458585 0.33514357 0.3355709  0.3357938  0.3358495
 0.3359061  0.33612102 0.3363813  0.33664995 0.33686712 0.33708972
 0.33736426 0.33772004 0.33808497 0.33838665 0.338609   0.33884105
 0.33911312 0.3394641  0.33990833 0.3403618  0.34081122 0.3412668
 0.3417431  0.34220055 0.3425724  0.34284788 0.3430206  0.34314662
 0.34320763 0.3430932  0.34266979 0.34188503 0.3407921  0.33948871
 0.33813655 0.33685663 0.33568263 0.33457625 0.33353564 0.3326606
 0.3319546  0.33131054 0.33067185 0.32999504 0.32931793 0.32865503
 0.32796782 0.3272047  0.3262448  0.3250501  0.3236947  0.32231876
 0.32103068 0.31981108 0.3186156  0.31745493 0.31627563 0.31519717
 0.31428853 0.31352308 0.3127753  0.3119058  0.31091958 0.30991057
 0.3090536  0.30835775 0.30771697 0.3070671  0.3062932  0.3055026
 0.30483413 0.3042409  0.3036158  0.30286112 0.30204025 0.30138063
 0.30106553 0.30113423 0.3013193  0.3013151  0.30089527 0.30016777
 0.29935873 0.2986271  0.29801676 0.29755497 0.29710665 0.2967407
 0.29649875 0.29643306 0.2963316  0.29598078 0.29538795 0.29485393
 0.29465085 0.29487637 0.295329   0.2956374  0.29563358 0.29540512
 0.29523382 0.29525393 0.29539934 0.2954782  0.29538673 0.29529163
 0.2953611  0.29562655 0.29593682 0.29612917 0.29621384 0.29619533
 0.296209   0.29631227 0.29649362 0.29669783 0.29694313 0.29727852
 0.29768267 0.29793248 0.29779202 0.2971852  0.29633147 0.2956017
 0.2952248  0.2950092  0.29452774 0.29338    0.29151654 0.2891869
 0.28684425 0.28488827 0.28332525 0.2819761  0.28070414 0.2795851
 0.278724   0.27815294 0.2778632  0.27772257 0.2775426  0.27720532
 0.2766103  0.2757761  0.27471346 0.27350786 0.27227524 0.27106714
 0.26991978 0.2687298  0.26739842 0.26595473 0.2645535  0.26341054
 0.2625564  0.26188183 0.26119247 0.26030287 0.25928685 0.25830892
 0.257503   0.2568387  0.256177   0.25540414 0.25441667 0.25337872
 0.25256297 0.25197616 0.25148615 0.25093064 0.25027323 0.24966125
 0.2492067  0.2490043  0.24887662 0.24863368 0.24820094 0.24759418
 0.24693684 0.2462093  0.24552538 0.24495056 0.24458358 0.244556
 0.24483538 0.24515176 0.24522813 0.24491636 0.24444126 0.2441984
 0.24440257 0.24488573 0.24530517 0.24548511 0.2453897  0.245257
 0.24531032 0.24556854 0.24581446 0.24585564 0.24562329 0.24551861
 0.24572033 0.24622554 0.24682829 0.24733393 0.24756753 0.24759835
 0.24747999 0.24736357 0.24732463 0.24746965 0.24789272 0.24856006
 0.24920042 0.24954152 0.24950613 0.24921696 0.24905561 0.24921308
 0.2495225  0.24948686 0.24869429 0.24709514 0.24498709 0.24277696
 0.24078031 0.23911056 0.23768727 0.2365978  0.23607193 0.23622224
 0.23673889 0.23719086 0.23727883 0.2370267  0.23669185 0.23644271
 0.2362505  0.23586126 0.23505875 0.23387688 0.23259579 0.23147814
 0.23053902 0.22954668 0.22830327 0.22688654 0.22567382 0.22504085
 0.22491933 0.22485025 0.22429469 0.22306192 0.22160847 0.22040676
 0.21956317 0.21883334 0.21786907 0.21660021 0.21542871 0.21476857
 0.21471593 0.21475057 0.2143491  0.21330705 0.21218099 0.21153936
 0.2115613  0.2118873  0.2118516  0.2112137  0.21040066 0.20979536
 0.20957051 0.2094241  0.20901254 0.20826104 0.20755123 0.20720288
 0.20745115 0.20797786 0.2081014  0.20754655 0.2067727  0.20636357
 0.20658477 0.20710132 0.20738779 0.20727208 0.20710188 0.20719439
 0.20760554 0.20797466 0.20797397 0.20753343 0.20724605 0.20747524
 0.20812    0.20865186 0.20867379 0.2083902  0.20834707 0.20883046
 0.20958506 0.21001506 0.20980495 0.20944925 0.20968506 0.21081546
 0.21239346 0.2135706  0.21389377 0.21364543 0.21359567 0.21412912
 0.21488646 0.21508443 0.21419416 0.21240482 0.21032879 0.20843542
 0.20678441 0.20521076 0.20361781 0.2022723  0.20171604 0.20216535
 0.20308381 0.20365079 0.20359585 0.20329708 0.20327057 0.20358013
 0.20367923 0.20292409 0.20127837 0.1994636  0.19843286 0.19838253
 0.19856054 0.19792205 0.19606394 0.19374502 0.19220164 0.19190495
 0.19208635 0.19156374 0.18979122 0.18744509 0.18594238 0.18564531
 0.18554738 0.18417616 0.18127832 0.17846805 0.17744237 0.1784105
 0.17922659 0.17777337 0.17392924 0.17059709 0.17017432 0.17208669
 0.17271133 0.16938713 0.16449125 0.16409677 0.1695619  0.16995527]
