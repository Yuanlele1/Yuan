Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=24, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=24, out_features=216, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4644864.0
params:  5400.0
Trainable parameters:  5400
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8840048
	speed: 0.0413s/iter; left time: 1081.5588s
	iters: 200, epoch: 1 | loss: 0.5497004
	speed: 0.0317s/iter; left time: 826.9507s
Epoch: 1 cost time: 9.064156293869019
Epoch: 1, Steps: 263 | Train Loss: 0.6947581 Vali Loss: 0.3356865 Test Loss: 0.4666410
Validation loss decreased (inf --> 0.335687).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7304715
	speed: 0.1768s/iter; left time: 4587.1222s
	iters: 200, epoch: 2 | loss: 0.5536198
	speed: 0.0248s/iter; left time: 641.2917s
Epoch: 2 cost time: 9.44709300994873
Epoch: 2, Steps: 263 | Train Loss: 0.5533588 Vali Loss: 0.2976709 Test Loss: 0.4214225
Validation loss decreased (0.335687 --> 0.297671).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3034260
	speed: 0.1556s/iter; left time: 3995.5786s
	iters: 200, epoch: 3 | loss: 0.6730427
	speed: 0.0217s/iter; left time: 556.0218s
Epoch: 3 cost time: 7.973278284072876
Epoch: 3, Steps: 263 | Train Loss: 0.5315792 Vali Loss: 0.2913482 Test Loss: 0.4140215
Validation loss decreased (0.297671 --> 0.291348).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3911056
	speed: 0.1284s/iter; left time: 3261.6397s
	iters: 200, epoch: 4 | loss: 0.4714521
	speed: 0.0207s/iter; left time: 523.1328s
Epoch: 4 cost time: 7.035106897354126
Epoch: 4, Steps: 263 | Train Loss: 0.5275410 Vali Loss: 0.2894620 Test Loss: 0.4122389
Validation loss decreased (0.291348 --> 0.289462).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4768692
	speed: 0.1197s/iter; left time: 3010.5712s
	iters: 200, epoch: 5 | loss: 0.3829828
	speed: 0.0386s/iter; left time: 967.6357s
Epoch: 5 cost time: 9.181674003601074
Epoch: 5, Steps: 263 | Train Loss: 0.5259524 Vali Loss: 0.2890956 Test Loss: 0.4115873
Validation loss decreased (0.289462 --> 0.289096).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5864221
	speed: 0.1824s/iter; left time: 4539.7152s
	iters: 200, epoch: 6 | loss: 0.4347614
	speed: 0.0262s/iter; left time: 648.2377s
Epoch: 6 cost time: 9.559953212738037
Epoch: 6, Steps: 263 | Train Loss: 0.5250322 Vali Loss: 0.2892752 Test Loss: 0.4113431
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4565302
	speed: 0.1449s/iter; left time: 3568.7683s
	iters: 200, epoch: 7 | loss: 0.5252625
	speed: 0.0309s/iter; left time: 758.3306s
Epoch: 7 cost time: 8.10853624343872
Epoch: 7, Steps: 263 | Train Loss: 0.5249034 Vali Loss: 0.2887475 Test Loss: 0.4110738
Validation loss decreased (0.289096 --> 0.288747).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.6999110
	speed: 0.1209s/iter; left time: 2945.8246s
	iters: 200, epoch: 8 | loss: 0.4567921
	speed: 0.0225s/iter; left time: 544.8451s
Epoch: 8 cost time: 6.950465440750122
Epoch: 8, Steps: 263 | Train Loss: 0.5239633 Vali Loss: 0.2890438 Test Loss: 0.4110022
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.7547138
	speed: 0.1222s/iter; left time: 2945.1238s
	iters: 200, epoch: 9 | loss: 0.4993145
	speed: 0.0377s/iter; left time: 905.7368s
Epoch: 9 cost time: 10.102286100387573
Epoch: 9, Steps: 263 | Train Loss: 0.5240577 Vali Loss: 0.2888806 Test Loss: 0.4110783
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5329480
	speed: 0.1833s/iter; left time: 4367.7638s
	iters: 200, epoch: 10 | loss: 0.6148915
	speed: 0.0384s/iter; left time: 910.9393s
Epoch: 10 cost time: 10.126349687576294
Epoch: 10, Steps: 263 | Train Loss: 0.5237573 Vali Loss: 0.2891455 Test Loss: 0.4110662
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5956994
	speed: 0.1595s/iter; left time: 3758.5668s
	iters: 200, epoch: 11 | loss: 0.5537400
	speed: 0.0347s/iter; left time: 815.1084s
Epoch: 11 cost time: 10.744547843933105
Epoch: 11, Steps: 263 | Train Loss: 0.5234779 Vali Loss: 0.2892039 Test Loss: 0.4110816
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4677142
	speed: 0.1229s/iter; left time: 2864.3987s
	iters: 200, epoch: 12 | loss: 0.3659775
	speed: 0.0225s/iter; left time: 522.4127s
Epoch: 12 cost time: 8.37413740158081
Epoch: 12, Steps: 263 | Train Loss: 0.5235846 Vali Loss: 0.2890796 Test Loss: 0.4110816
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4310298
	speed: 0.1262s/iter; left time: 2907.3911s
	iters: 200, epoch: 13 | loss: 0.7959943
	speed: 0.0445s/iter; left time: 1021.2107s
Epoch: 13 cost time: 8.698640823364258
Epoch: 13, Steps: 263 | Train Loss: 0.5227839 Vali Loss: 0.2892775 Test Loss: 0.4110866
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3155697
	speed: 0.1414s/iter; left time: 3222.2128s
	iters: 200, epoch: 14 | loss: 0.5635887
	speed: 0.0237s/iter; left time: 536.5093s
Epoch: 14 cost time: 7.09446382522583
Epoch: 14, Steps: 263 | Train Loss: 0.5233964 Vali Loss: 0.2891716 Test Loss: 0.4111547
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3731589
	speed: 0.1360s/iter; left time: 3061.7443s
	iters: 200, epoch: 15 | loss: 0.8344027
	speed: 0.0311s/iter; left time: 696.2622s
Epoch: 15 cost time: 10.04225492477417
Epoch: 15, Steps: 263 | Train Loss: 0.5224285 Vali Loss: 0.2893873 Test Loss: 0.4111964
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5117871
	speed: 0.1372s/iter; left time: 3052.7905s
	iters: 200, epoch: 16 | loss: 0.3596812
	speed: 0.0341s/iter; left time: 754.5144s
Epoch: 16 cost time: 8.539960384368896
Epoch: 16, Steps: 263 | Train Loss: 0.5228201 Vali Loss: 0.2893773 Test Loss: 0.4112687
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5736500
	speed: 0.1288s/iter; left time: 2831.9449s
	iters: 200, epoch: 17 | loss: 0.6885821
	speed: 0.0261s/iter; left time: 571.4610s
Epoch: 17 cost time: 7.697061538696289
Epoch: 17, Steps: 263 | Train Loss: 0.5229307 Vali Loss: 0.2894520 Test Loss: 0.4111697
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5419895
	speed: 0.1441s/iter; left time: 3130.6122s
	iters: 200, epoch: 18 | loss: 0.5457056
	speed: 0.0217s/iter; left time: 468.7723s
Epoch: 18 cost time: 7.261711359024048
Epoch: 18, Steps: 263 | Train Loss: 0.5229787 Vali Loss: 0.2893001 Test Loss: 0.4111981
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5935813
	speed: 0.1379s/iter; left time: 2959.2519s
	iters: 200, epoch: 19 | loss: 0.5978810
	speed: 0.0287s/iter; left time: 612.4033s
Epoch: 19 cost time: 8.612075090408325
Epoch: 19, Steps: 263 | Train Loss: 0.5229363 Vali Loss: 0.2895301 Test Loss: 0.4112013
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4550189
	speed: 0.1460s/iter; left time: 3096.2365s
	iters: 200, epoch: 20 | loss: 0.4423001
	speed: 0.0224s/iter; left time: 471.8565s
Epoch: 20 cost time: 6.84437370300293
Epoch: 20, Steps: 263 | Train Loss: 0.5225257 Vali Loss: 0.2894640 Test Loss: 0.4112597
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.6459481
	speed: 0.1144s/iter; left time: 2396.4558s
	iters: 200, epoch: 21 | loss: 0.3807380
	speed: 0.0291s/iter; left time: 605.4914s
Epoch: 21 cost time: 7.307928562164307
Epoch: 21, Steps: 263 | Train Loss: 0.5229679 Vali Loss: 0.2893109 Test Loss: 0.4112054
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5232848
	speed: 0.1484s/iter; left time: 3068.9552s
	iters: 200, epoch: 22 | loss: 0.5573190
	speed: 0.0240s/iter; left time: 493.3965s
Epoch: 22 cost time: 8.029737949371338
Epoch: 22, Steps: 263 | Train Loss: 0.5231855 Vali Loss: 0.2896191 Test Loss: 0.4112344
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4735135
	speed: 0.1491s/iter; left time: 3044.4967s
	iters: 200, epoch: 23 | loss: 0.5334560
	speed: 0.0328s/iter; left time: 665.3829s
Epoch: 23 cost time: 10.857025623321533
Epoch: 23, Steps: 263 | Train Loss: 0.5228958 Vali Loss: 0.2894213 Test Loss: 0.4113002
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4305958
	speed: 0.1299s/iter; left time: 2616.9221s
	iters: 200, epoch: 24 | loss: 0.4950628
	speed: 0.0332s/iter; left time: 664.7822s
Epoch: 24 cost time: 8.05760908126831
Epoch: 24, Steps: 263 | Train Loss: 0.5218994 Vali Loss: 0.2896909 Test Loss: 0.4112978
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4613041
	speed: 0.1291s/iter; left time: 2568.5040s
	iters: 200, epoch: 25 | loss: 0.4664811
	speed: 0.0393s/iter; left time: 777.6171s
Epoch: 25 cost time: 8.525191068649292
Epoch: 25, Steps: 263 | Train Loss: 0.5227228 Vali Loss: 0.2896096 Test Loss: 0.4113264
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3756242
	speed: 0.1311s/iter; left time: 2573.7514s
	iters: 200, epoch: 26 | loss: 0.5865362
	speed: 0.0357s/iter; left time: 696.1105s
Epoch: 26 cost time: 10.049583911895752
Epoch: 26, Steps: 263 | Train Loss: 0.5230617 Vali Loss: 0.2893512 Test Loss: 0.4113085
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5086236
	speed: 0.1408s/iter; left time: 2725.8070s
	iters: 200, epoch: 27 | loss: 0.5221729
	speed: 0.0461s/iter; left time: 888.6727s
Epoch: 27 cost time: 10.786956071853638
Epoch: 27, Steps: 263 | Train Loss: 0.5227408 Vali Loss: 0.2896251 Test Loss: 0.4113610
EarlyStopping counter: 20 out of 20
Early stopping
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=24, out_features=216, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4644864.0
params:  5400.0
Trainable parameters:  5400
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6628054
	speed: 0.0297s/iter; left time: 778.1179s
	iters: 200, epoch: 1 | loss: 0.3433622
	speed: 0.0258s/iter; left time: 673.2112s
Epoch: 1 cost time: 8.232771635055542
Epoch: 1, Steps: 263 | Train Loss: 0.5882540 Vali Loss: 0.2885618 Test Loss: 0.4108010
Validation loss decreased (inf --> 0.288562).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.7023375
	speed: 0.1278s/iter; left time: 3314.1264s
	iters: 200, epoch: 2 | loss: 0.4649015
	speed: 0.0237s/iter; left time: 611.9655s
Epoch: 2 cost time: 7.648777961730957
Epoch: 2, Steps: 263 | Train Loss: 0.5875300 Vali Loss: 0.2888844 Test Loss: 0.4109277
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5970656
	speed: 0.1136s/iter; left time: 2915.4482s
	iters: 200, epoch: 3 | loss: 0.4821430
	speed: 0.0268s/iter; left time: 684.5607s
Epoch: 3 cost time: 7.5538694858551025
Epoch: 3, Steps: 263 | Train Loss: 0.5882098 Vali Loss: 0.2892363 Test Loss: 0.4109980
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.6816250
	speed: 0.1442s/iter; left time: 3663.9606s
	iters: 200, epoch: 4 | loss: 0.6909261
	speed: 0.0341s/iter; left time: 862.1105s
Epoch: 4 cost time: 8.833681106567383
Epoch: 4, Steps: 263 | Train Loss: 0.5874067 Vali Loss: 0.2891680 Test Loss: 0.4109127
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5420505
	speed: 0.1741s/iter; left time: 4377.7627s
	iters: 200, epoch: 5 | loss: 0.4135284
	speed: 0.0290s/iter; left time: 726.7243s
Epoch: 5 cost time: 10.84044885635376
Epoch: 5, Steps: 263 | Train Loss: 0.5870782 Vali Loss: 0.2891818 Test Loss: 0.4109211
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5491192
	speed: 0.1612s/iter; left time: 4011.2917s
	iters: 200, epoch: 6 | loss: 0.6490227
	speed: 0.0286s/iter; left time: 707.6905s
Epoch: 6 cost time: 8.714780569076538
Epoch: 6, Steps: 263 | Train Loss: 0.5875104 Vali Loss: 0.2893818 Test Loss: 0.4110478
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.6982426
	speed: 0.1369s/iter; left time: 3370.8290s
	iters: 200, epoch: 7 | loss: 0.5777705
	speed: 0.0249s/iter; left time: 611.3544s
Epoch: 7 cost time: 7.8045618534088135
Epoch: 7, Steps: 263 | Train Loss: 0.5872341 Vali Loss: 0.2895593 Test Loss: 0.4110658
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.9437473
	speed: 0.1145s/iter; left time: 2788.3186s
	iters: 200, epoch: 8 | loss: 0.4036917
	speed: 0.0197s/iter; left time: 477.3186s
Epoch: 8 cost time: 5.762353897094727
Epoch: 8, Steps: 263 | Train Loss: 0.5869142 Vali Loss: 0.2891022 Test Loss: 0.4110261
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.6214610
	speed: 0.1280s/iter; left time: 3085.5138s
	iters: 200, epoch: 9 | loss: 0.6080407
	speed: 0.0307s/iter; left time: 736.7241s
Epoch: 9 cost time: 9.702507495880127
Epoch: 9, Steps: 263 | Train Loss: 0.5874802 Vali Loss: 0.2892154 Test Loss: 0.4110589
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5931780
	speed: 0.1677s/iter; left time: 3997.5560s
	iters: 200, epoch: 10 | loss: 0.6068678
	speed: 0.0345s/iter; left time: 818.9208s
Epoch: 10 cost time: 11.373079538345337
Epoch: 10, Steps: 263 | Train Loss: 0.5868530 Vali Loss: 0.2894222 Test Loss: 0.4111215
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5906746
	speed: 0.1633s/iter; left time: 3849.9253s
	iters: 200, epoch: 11 | loss: 0.4960696
	speed: 0.0319s/iter; left time: 749.7867s
Epoch: 11 cost time: 7.610858678817749
Epoch: 11, Steps: 263 | Train Loss: 0.5870678 Vali Loss: 0.2896152 Test Loss: 0.4110928
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.7131285
	speed: 0.1338s/iter; left time: 3118.8844s
	iters: 200, epoch: 12 | loss: 0.4289902
	speed: 0.0309s/iter; left time: 716.1441s
Epoch: 12 cost time: 9.0101957321167
Epoch: 12, Steps: 263 | Train Loss: 0.5871834 Vali Loss: 0.2896246 Test Loss: 0.4111151
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.7479498
	speed: 0.1412s/iter; left time: 3253.4881s
	iters: 200, epoch: 13 | loss: 0.5082046
	speed: 0.0366s/iter; left time: 840.6377s
Epoch: 13 cost time: 10.50347375869751
Epoch: 13, Steps: 263 | Train Loss: 0.5867823 Vali Loss: 0.2897645 Test Loss: 0.4111635
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5024926
	speed: 0.1662s/iter; left time: 3786.4412s
	iters: 200, epoch: 14 | loss: 0.5748641
	speed: 0.0372s/iter; left time: 844.4004s
Epoch: 14 cost time: 10.211897134780884
Epoch: 14, Steps: 263 | Train Loss: 0.5866853 Vali Loss: 0.2896596 Test Loss: 0.4111304
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5633050
	speed: 0.1570s/iter; left time: 3535.7962s
	iters: 200, epoch: 15 | loss: 0.4260828
	speed: 0.0210s/iter; left time: 470.7369s
Epoch: 15 cost time: 8.775026321411133
Epoch: 15, Steps: 263 | Train Loss: 0.5870746 Vali Loss: 0.2897071 Test Loss: 0.4111233
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5632713
	speed: 0.1225s/iter; left time: 2726.5190s
	iters: 200, epoch: 16 | loss: 0.6410632
	speed: 0.0225s/iter; left time: 498.7319s
Epoch: 16 cost time: 7.086658954620361
Epoch: 16, Steps: 263 | Train Loss: 0.5868889 Vali Loss: 0.2896521 Test Loss: 0.4111914
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.6428716
	speed: 0.1180s/iter; left time: 2594.9134s
	iters: 200, epoch: 17 | loss: 0.5460482
	speed: 0.0229s/iter; left time: 500.6738s
Epoch: 17 cost time: 7.371910572052002
Epoch: 17, Steps: 263 | Train Loss: 0.5863172 Vali Loss: 0.2896498 Test Loss: 0.4111224
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5449799
	speed: 0.1339s/iter; left time: 2910.2901s
	iters: 200, epoch: 18 | loss: 0.6742263
	speed: 0.0355s/iter; left time: 766.8518s
Epoch: 18 cost time: 8.97111463546753
Epoch: 18, Steps: 263 | Train Loss: 0.5867108 Vali Loss: 0.2895014 Test Loss: 0.4111663
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5322930
	speed: 0.1522s/iter; left time: 3267.9247s
	iters: 200, epoch: 19 | loss: 0.5097210
	speed: 0.0331s/iter; left time: 708.2408s
Epoch: 19 cost time: 9.08041524887085
Epoch: 19, Steps: 263 | Train Loss: 0.5868762 Vali Loss: 0.2896948 Test Loss: 0.4111860
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5916920
	speed: 0.1198s/iter; left time: 2541.1853s
	iters: 200, epoch: 20 | loss: 0.8784426
	speed: 0.0320s/iter; left time: 676.3378s
Epoch: 20 cost time: 8.560297012329102
Epoch: 20, Steps: 263 | Train Loss: 0.5866533 Vali Loss: 0.2897374 Test Loss: 0.4111686
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4483168
	speed: 0.1267s/iter; left time: 2653.9379s
	iters: 200, epoch: 21 | loss: 0.4429234
	speed: 0.0286s/iter; left time: 596.7866s
Epoch: 21 cost time: 7.13775634765625
Epoch: 21, Steps: 263 | Train Loss: 0.5866989 Vali Loss: 0.2895598 Test Loss: 0.4111740
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.408478707075119, mae:0.39801833033561707, rse:0.5137231349945068, corr:[0.54837674 0.54681736 0.53952664 0.5379422  0.53780174 0.535485
 0.53379357 0.5336261  0.53278035 0.53135407 0.53086793 0.5304715
 0.5290473  0.5277017  0.52727455 0.52671075 0.5254391  0.52434933
 0.52365786 0.52257115 0.52105534 0.5198611  0.5192003  0.51877576
 0.5182954  0.51763004 0.5168563  0.5163434  0.51624817 0.5158201
 0.51451695 0.5130767  0.51248413 0.5124478  0.5120953  0.51149493
 0.51082355 0.5100141  0.5091649  0.5086493  0.50831807 0.5077655
 0.5071228  0.50690055 0.5068592  0.50627685 0.5052961  0.5045373
 0.50396526 0.5030675  0.5018154  0.50073904 0.49996653 0.49926317
 0.4987518  0.4983915  0.49794498 0.49741766 0.49705943 0.49694732
 0.49673864 0.49637336 0.49629456 0.49652728 0.4965524  0.49622366
 0.49596313 0.49591807 0.49596182 0.49586177 0.49575892 0.4958642
 0.49594095 0.49583122 0.49553812 0.49534187 0.49533457 0.4951962
 0.4949289  0.49469134 0.49448404 0.4940961  0.49356133 0.4931747
 0.49298885 0.49281943 0.49249315 0.49210224 0.49179724 0.4915182
 0.49105248 0.4903776  0.48973316 0.4890543  0.48777857 0.48572287
 0.48337963 0.48121867 0.47915462 0.47700438 0.47499514 0.47357795
 0.47271731 0.47189236 0.4706799  0.46907243 0.46751273 0.46624383
 0.46500754 0.46363702 0.4622052  0.46083778 0.45957422 0.45829067
 0.45696878 0.45564026 0.4542382  0.4527979  0.4514976  0.4503716
 0.44956157 0.44885993 0.44794056 0.44680855 0.44570854 0.44475377
 0.44386527 0.44282368 0.4415698  0.44045216 0.43958095 0.43875113
 0.43788448 0.43696418 0.43595415 0.43518046 0.43470165 0.43458068
 0.4344442  0.43404895 0.43352386 0.43291438 0.4321283  0.43121397
 0.4304781  0.42983267 0.4287382  0.42742786 0.42648876 0.42625064
 0.42612353 0.4256859  0.42488676 0.42408144 0.42365366 0.42344818
 0.42318445 0.42284474 0.422381   0.42213255 0.42216602 0.42220867
 0.4221268  0.4217684  0.42165688 0.42179558 0.4219002  0.42192653
 0.42207775 0.42234337 0.42245784 0.42240363 0.4224663  0.422729
 0.42293823 0.42283228 0.42240995 0.42197043 0.42176607 0.42176676
 0.4216997  0.42141873 0.42119402 0.42107078 0.42085135 0.4202596
 0.41948503 0.41897532 0.41862208 0.41782957 0.41623393 0.41397944
 0.41143474 0.40892112 0.40671894 0.4050394  0.4037223  0.40240204
 0.4011988  0.4004138  0.3997666  0.39858213 0.3968784  0.39530367
 0.39408642 0.3929536  0.39164448 0.39027253 0.38886875 0.38730383
 0.38563958 0.3842504  0.38300762 0.38157406 0.37996495 0.37859863
 0.37763923 0.3766705  0.3754725  0.37410727 0.37289062 0.3718784
 0.37085316 0.36951843 0.3682056  0.36706    0.36642    0.3660201
 0.36529112 0.36402354 0.3626606  0.36179543 0.3612179  0.3606499
 0.36020914 0.36015502 0.3601843  0.35971168 0.35894814 0.3583996
 0.35814995 0.35777912 0.35702598 0.35654005 0.3563738  0.35631427
 0.3564149  0.35654888 0.35645357 0.35603335 0.3558497  0.35616887
 0.35661933 0.35698694 0.35734275 0.35751528 0.3573316  0.3570049
 0.3569889  0.35741472 0.35775596 0.35784066 0.35787037 0.3580993
 0.35869992 0.3594302  0.36010212 0.3603643  0.3603822  0.36054543
 0.36092624 0.36112118 0.36086905 0.36065665 0.3610629  0.36174664
 0.3620106  0.36181748 0.36169654 0.36191285 0.3620598  0.36203957
 0.3620891  0.36222395 0.3621182  0.3615493  0.36055037 0.3591502
 0.35750186 0.35607857 0.35516396 0.35461712 0.35424167 0.35415608
 0.35439724 0.3547153  0.3549374  0.35496143 0.35465166 0.35401672
 0.3532786  0.35263094 0.35203022 0.35130686 0.35047397 0.34968942
 0.34901005 0.3481997  0.34707075 0.3457343  0.34460014 0.34408745
 0.3438968  0.3435424  0.3428632  0.3421706  0.3416079  0.3408758
 0.33986288 0.3391033  0.33887064 0.33860767 0.33786282 0.33718783
 0.33710125 0.33709314 0.33639187 0.3353199  0.33502886 0.3353982
 0.33548456 0.33518538 0.33523762 0.33544806 0.33504602 0.3342612
 0.3339392  0.33400726 0.33375767 0.33335632 0.3335086  0.3340291
 0.3340537  0.3337131  0.33388022 0.33459607 0.33499885 0.3349868
 0.3353672  0.33616054 0.3365441  0.33624226 0.3359794  0.33623055
 0.33662784 0.33678356 0.33682734 0.33710894 0.33738863 0.33752498
 0.3377874  0.3383941  0.3390081  0.33934718 0.3395589  0.3398712
 0.34011778 0.34023422 0.3404271  0.3407508  0.34107453 0.34136254
 0.3418127  0.34240547 0.34278846 0.3427938  0.34261897 0.34250277
 0.34244174 0.3423763  0.3422289  0.3417382  0.34065104 0.33915076
 0.33769295 0.33653414 0.33547217 0.3343447  0.33328983 0.33249778
 0.33189705 0.3314171  0.3309967  0.3305245  0.32992607 0.32910317
 0.32813808 0.32732663 0.3267051  0.32588065 0.3245101  0.32293484
 0.32176673 0.3209031  0.3197478  0.31823438 0.31683412 0.31590027
 0.3150634  0.31408036 0.31324518 0.31268984 0.3120239  0.3109876
 0.30997345 0.3092395  0.30852228 0.30754942 0.3064002  0.30552518
 0.3050482  0.3047606  0.30450615 0.3040368  0.30322367 0.30243292
 0.30216026 0.30238694 0.30243525 0.30207402 0.30163097 0.30140927
 0.30094036 0.2997424  0.29824036 0.29747772 0.2976451  0.29800645
 0.29769257 0.29685047 0.2959754  0.2954109  0.295103   0.29500562
 0.29520205 0.29565185 0.29598737 0.29593098 0.29570055 0.29541832
 0.29511613 0.2951094  0.2958551  0.29686064 0.29699403 0.2961568
 0.29551545 0.29594612 0.29678816 0.29717752 0.29736906 0.29766127
 0.2977151  0.2972644  0.29687205 0.29689592 0.29685357 0.29645014
 0.29635096 0.2969457  0.29748386 0.29726517 0.29665783 0.29627553
 0.29592314 0.29516414 0.29423398 0.29327244 0.29181015 0.28944626
 0.28679445 0.28464583 0.28292158 0.28135222 0.28027764 0.27994877
 0.27977014 0.27923214 0.27868283 0.27855986 0.27852282 0.27804995
 0.27711508 0.2761938  0.2753938  0.2744977  0.27334735 0.2720187
 0.27065963 0.2692954  0.26802823 0.2669095  0.26583144 0.26476
 0.26369852 0.26266906 0.26166987 0.2607809  0.26009426 0.25926334
 0.2580092  0.25672597 0.25599992 0.25571236 0.255167   0.25430816
 0.25361368 0.25315064 0.252745   0.2523287  0.2519504  0.2515512
 0.25105992 0.25074154 0.25048935 0.24995385 0.24917606 0.24869817
 0.24879786 0.24867742 0.24765795 0.24611019 0.24521932 0.24545918
 0.24599423 0.24590534 0.24526584 0.24469753 0.24467547 0.2450381
 0.24531436 0.24526729 0.24509996 0.24515384 0.24537553 0.2454777
 0.24524638 0.2449727  0.24534711 0.24654023 0.24754839 0.24766445
 0.2470792  0.24677154 0.24711522 0.24759997 0.24781978 0.24821036
 0.24882182 0.24897993 0.24827701 0.24759433 0.24796931 0.24885692
 0.24880014 0.24787842 0.24782948 0.24924847 0.2504822  0.24997796
 0.2486125  0.24816874 0.24849872 0.24791673 0.2457939  0.2432275
 0.24113625 0.2393233  0.23757775 0.23624526 0.23563866 0.2358095
 0.2365396  0.23736313 0.23785704 0.23792417 0.2377509  0.23737195
 0.23666765 0.2357864  0.23508133 0.23448394 0.23362531 0.2323832
 0.23088716 0.22924577 0.22779064 0.22694097 0.22643432 0.22565368
 0.22463393 0.2240624  0.22378734 0.22292525 0.22154775 0.22064973
 0.22054605 0.22022016 0.21889788 0.21742384 0.21689224 0.21685778
 0.21651623 0.21595119 0.21559341 0.21496059 0.21391071 0.21304575
 0.21275097 0.21243803 0.21159923 0.21089548 0.21095242 0.21107736
 0.21087205 0.2105341  0.20993711 0.2088244  0.20796125 0.2079285
 0.20814922 0.20764695 0.20701343 0.20747541 0.20836116 0.20812652
 0.20752597 0.20813943 0.20915358 0.20885989 0.20807107 0.2085058
 0.20961292 0.20965219 0.20910366 0.20938131 0.21023583 0.21035339
 0.2102029  0.21096222 0.21218027 0.21280813 0.21300912 0.2132288
 0.2131145  0.21256903 0.2124476  0.21319668 0.21408007 0.21475065
 0.21560758 0.21638876 0.21644789 0.21614593 0.21631247 0.2166063
 0.21627054 0.2156055  0.215009   0.21366464 0.21108139 0.20854078
 0.20720917 0.20635247 0.2050583  0.20399933 0.20393194 0.20402774
 0.20353487 0.20344356 0.20436877 0.20512576 0.20479737 0.2042327
 0.20418817 0.20380847 0.20249955 0.20127837 0.20073362 0.1998932
 0.19837251 0.19725452 0.1968027  0.19575359 0.19386871 0.19260772
 0.19229433 0.19142802 0.1896513  0.18841274 0.18785496 0.18629162
 0.1841325  0.18328817 0.18298796 0.1809873  0.17831926 0.17803736
 0.17870407 0.17703006 0.17431842 0.17425783 0.17472583 0.1725825
 0.17097774 0.17271982 0.17247003 0.16818088 0.16956665 0.17700456]
