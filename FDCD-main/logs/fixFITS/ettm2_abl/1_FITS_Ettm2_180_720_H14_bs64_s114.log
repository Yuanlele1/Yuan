Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=38, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_180_720_FITS_ETTm2_ftM_sl180_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=38, out_features=190, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6469120.0
params:  7410.0
Trainable parameters:  7410
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.1059771
	speed: 0.0179s/iter; left time: 467.4406s
	iters: 200, epoch: 1 | loss: 0.8922869
	speed: 0.0122s/iter; left time: 317.5700s
Epoch: 1 cost time: 3.739959716796875
Epoch: 1, Steps: 262 | Train Loss: 0.6668933 Vali Loss: 0.3008622 Test Loss: 0.4181775
Validation loss decreased (inf --> 0.300862).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6405199
	speed: 0.0593s/iter; left time: 1531.1178s
	iters: 200, epoch: 2 | loss: 0.5125064
	speed: 0.0117s/iter; left time: 300.4656s
Epoch: 2 cost time: 3.6764590740203857
Epoch: 2, Steps: 262 | Train Loss: 0.5802277 Vali Loss: 0.2860126 Test Loss: 0.4001970
Validation loss decreased (0.300862 --> 0.286013).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4051103
	speed: 0.0590s/iter; left time: 1508.4419s
	iters: 200, epoch: 3 | loss: 0.4442534
	speed: 0.0120s/iter; left time: 305.8998s
Epoch: 3 cost time: 3.682029962539673
Epoch: 3, Steps: 262 | Train Loss: 0.5690950 Vali Loss: 0.2821471 Test Loss: 0.3951790
Validation loss decreased (0.286013 --> 0.282147).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4827196
	speed: 0.0609s/iter; left time: 1541.4674s
	iters: 200, epoch: 4 | loss: 0.5055945
	speed: 0.0138s/iter; left time: 347.9899s
Epoch: 4 cost time: 4.276142358779907
Epoch: 4, Steps: 262 | Train Loss: 0.5639202 Vali Loss: 0.2804215 Test Loss: 0.3926485
Validation loss decreased (0.282147 --> 0.280421).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4364776
	speed: 0.0700s/iter; left time: 1753.6158s
	iters: 200, epoch: 5 | loss: 0.4652387
	speed: 0.0175s/iter; left time: 436.6639s
Epoch: 5 cost time: 4.784716844558716
Epoch: 5, Steps: 262 | Train Loss: 0.5624725 Vali Loss: 0.2794179 Test Loss: 0.3912132
Validation loss decreased (0.280421 --> 0.279418).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4807181
	speed: 0.0717s/iter; left time: 1777.6335s
	iters: 200, epoch: 6 | loss: 0.6095536
	speed: 0.0162s/iter; left time: 399.8769s
Epoch: 6 cost time: 5.051643371582031
Epoch: 6, Steps: 262 | Train Loss: 0.5605084 Vali Loss: 0.2785198 Test Loss: 0.3902681
Validation loss decreased (0.279418 --> 0.278520).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5626453
	speed: 0.0640s/iter; left time: 1569.3922s
	iters: 200, epoch: 7 | loss: 0.5634428
	speed: 0.0120s/iter; left time: 293.7598s
Epoch: 7 cost time: 3.729783535003662
Epoch: 7, Steps: 262 | Train Loss: 0.5593983 Vali Loss: 0.2778345 Test Loss: 0.3895334
Validation loss decreased (0.278520 --> 0.277834).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5826299
	speed: 0.0608s/iter; left time: 1475.7434s
	iters: 200, epoch: 8 | loss: 0.4815475
	speed: 0.0118s/iter; left time: 285.3380s
Epoch: 8 cost time: 3.6548614501953125
Epoch: 8, Steps: 262 | Train Loss: 0.5583507 Vali Loss: 0.2777363 Test Loss: 0.3889983
Validation loss decreased (0.277834 --> 0.277736).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5110320
	speed: 0.0583s/iter; left time: 1400.1229s
	iters: 200, epoch: 9 | loss: 0.4954879
	speed: 0.0120s/iter; left time: 287.4952s
Epoch: 9 cost time: 3.730743408203125
Epoch: 9, Steps: 262 | Train Loss: 0.5575513 Vali Loss: 0.2776771 Test Loss: 0.3886471
Validation loss decreased (0.277736 --> 0.277677).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4984874
	speed: 0.0612s/iter; left time: 1453.8911s
	iters: 200, epoch: 10 | loss: 0.5009320
	speed: 0.0197s/iter; left time: 466.8858s
Epoch: 10 cost time: 5.115988492965698
Epoch: 10, Steps: 262 | Train Loss: 0.5576908 Vali Loss: 0.2775118 Test Loss: 0.3883107
Validation loss decreased (0.277677 --> 0.277512).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4981897
	speed: 0.0769s/iter; left time: 1804.7920s
	iters: 200, epoch: 11 | loss: 0.5347807
	speed: 0.0136s/iter; left time: 317.2770s
Epoch: 11 cost time: 5.018044471740723
Epoch: 11, Steps: 262 | Train Loss: 0.5564557 Vali Loss: 0.2770980 Test Loss: 0.3880448
Validation loss decreased (0.277512 --> 0.277098).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4724377
	speed: 0.0685s/iter; left time: 1591.0428s
	iters: 200, epoch: 12 | loss: 0.4737965
	speed: 0.0142s/iter; left time: 328.1920s
Epoch: 12 cost time: 4.302501678466797
Epoch: 12, Steps: 262 | Train Loss: 0.5562802 Vali Loss: 0.2768498 Test Loss: 0.3878207
Validation loss decreased (0.277098 --> 0.276850).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5848062
	speed: 0.1106s/iter; left time: 2539.5768s
	iters: 200, epoch: 13 | loss: 0.6846290
	speed: 0.0176s/iter; left time: 402.1611s
Epoch: 13 cost time: 4.926378488540649
Epoch: 13, Steps: 262 | Train Loss: 0.5564601 Vali Loss: 0.2770874 Test Loss: 0.3876988
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.8644229
	speed: 0.0622s/iter; left time: 1412.5516s
	iters: 200, epoch: 14 | loss: 0.6420082
	speed: 0.0121s/iter; left time: 272.3644s
Epoch: 14 cost time: 3.605400323867798
Epoch: 14, Steps: 262 | Train Loss: 0.5559317 Vali Loss: 0.2768754 Test Loss: 0.3875105
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4563411
	speed: 0.0582s/iter; left time: 1306.5993s
	iters: 200, epoch: 15 | loss: 0.4683028
	speed: 0.0122s/iter; left time: 271.9460s
Epoch: 15 cost time: 3.690795660018921
Epoch: 15, Steps: 262 | Train Loss: 0.5552379 Vali Loss: 0.2764373 Test Loss: 0.3873527
Validation loss decreased (0.276850 --> 0.276437).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.6133929
	speed: 0.0592s/iter; left time: 1312.1346s
	iters: 200, epoch: 16 | loss: 0.5351394
	speed: 0.0128s/iter; left time: 281.9363s
Epoch: 16 cost time: 3.841001272201538
Epoch: 16, Steps: 262 | Train Loss: 0.5559608 Vali Loss: 0.2769140 Test Loss: 0.3872854
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4919099
	speed: 0.0649s/iter; left time: 1422.9181s
	iters: 200, epoch: 17 | loss: 0.6124122
	speed: 0.0207s/iter; left time: 451.8897s
Epoch: 17 cost time: 5.072991609573364
Epoch: 17, Steps: 262 | Train Loss: 0.5554614 Vali Loss: 0.2768303 Test Loss: 0.3871605
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.7424473
	speed: 0.0685s/iter; left time: 1483.0152s
	iters: 200, epoch: 18 | loss: 0.5553967
	speed: 0.0210s/iter; left time: 452.3706s
Epoch: 18 cost time: 4.94899845123291
Epoch: 18, Steps: 262 | Train Loss: 0.5547813 Vali Loss: 0.2766833 Test Loss: 0.3870843
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5675484
	speed: 0.0660s/iter; left time: 1411.4536s
	iters: 200, epoch: 19 | loss: 0.4233010
	speed: 0.0169s/iter; left time: 360.0314s
Epoch: 19 cost time: 5.169316530227661
Epoch: 19, Steps: 262 | Train Loss: 0.5530085 Vali Loss: 0.2765383 Test Loss: 0.3870484
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5135099
	speed: 0.0927s/iter; left time: 1958.2108s
	iters: 200, epoch: 20 | loss: 0.6300534
	speed: 0.0287s/iter; left time: 602.4833s
Epoch: 20 cost time: 7.55229640007019
Epoch: 20, Steps: 262 | Train Loss: 0.5548301 Vali Loss: 0.2767398 Test Loss: 0.3869725
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4706616
	speed: 0.1091s/iter; left time: 2275.1816s
	iters: 200, epoch: 21 | loss: 0.6634673
	speed: 0.0121s/iter; left time: 250.6270s
Epoch: 21 cost time: 3.8752403259277344
Epoch: 21, Steps: 262 | Train Loss: 0.5551244 Vali Loss: 0.2768111 Test Loss: 0.3869266
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5870814
	speed: 0.0606s/iter; left time: 1248.0605s
	iters: 200, epoch: 22 | loss: 0.9018546
	speed: 0.0122s/iter; left time: 249.5358s
Epoch: 22 cost time: 3.7331652641296387
Epoch: 22, Steps: 262 | Train Loss: 0.5539300 Vali Loss: 0.2765351 Test Loss: 0.3868790
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.6132054
	speed: 0.0688s/iter; left time: 1399.1895s
	iters: 200, epoch: 23 | loss: 0.5730564
	speed: 0.0201s/iter; left time: 406.7799s
Epoch: 23 cost time: 5.507425785064697
Epoch: 23, Steps: 262 | Train Loss: 0.5551675 Vali Loss: 0.2766287 Test Loss: 0.3868462
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.6142842
	speed: 0.0667s/iter; left time: 1338.9013s
	iters: 200, epoch: 24 | loss: 0.6071602
	speed: 0.0150s/iter; left time: 299.5365s
Epoch: 24 cost time: 4.396631717681885
Epoch: 24, Steps: 262 | Train Loss: 0.5546149 Vali Loss: 0.2764790 Test Loss: 0.3867975
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4799452
	speed: 0.0623s/iter; left time: 1233.7472s
	iters: 200, epoch: 25 | loss: 0.6547240
	speed: 0.0187s/iter; left time: 369.4457s
Epoch: 25 cost time: 4.50124454498291
Epoch: 25, Steps: 262 | Train Loss: 0.5538027 Vali Loss: 0.2764345 Test Loss: 0.3867724
Validation loss decreased (0.276437 --> 0.276434).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3403943
	speed: 0.0633s/iter; left time: 1238.5084s
	iters: 200, epoch: 26 | loss: 0.7540634
	speed: 0.0121s/iter; left time: 235.8174s
Epoch: 26 cost time: 4.279230356216431
Epoch: 26, Steps: 262 | Train Loss: 0.5545794 Vali Loss: 0.2764948 Test Loss: 0.3867533
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5752220
	speed: 0.0862s/iter; left time: 1662.1902s
	iters: 200, epoch: 27 | loss: 0.5261227
	speed: 0.0147s/iter; left time: 282.8668s
Epoch: 27 cost time: 4.313819646835327
Epoch: 27, Steps: 262 | Train Loss: 0.5537966 Vali Loss: 0.2765036 Test Loss: 0.3867356
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3484612
	speed: 0.0638s/iter; left time: 1213.9713s
	iters: 200, epoch: 28 | loss: 0.4497058
	speed: 0.0116s/iter; left time: 219.4551s
Epoch: 28 cost time: 3.653841018676758
Epoch: 28, Steps: 262 | Train Loss: 0.5541569 Vali Loss: 0.2763517 Test Loss: 0.3867096
Validation loss decreased (0.276434 --> 0.276352).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4908157
	speed: 0.0584s/iter; left time: 1095.4807s
	iters: 200, epoch: 29 | loss: 0.3831386
	speed: 0.0117s/iter; left time: 218.5878s
Epoch: 29 cost time: 3.694986343383789
Epoch: 29, Steps: 262 | Train Loss: 0.5540312 Vali Loss: 0.2763029 Test Loss: 0.3866796
Validation loss decreased (0.276352 --> 0.276303).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.6568717
	speed: 0.0583s/iter; left time: 1079.0332s
	iters: 200, epoch: 30 | loss: 0.5587239
	speed: 0.0118s/iter; left time: 216.5477s
Epoch: 30 cost time: 3.650510549545288
Epoch: 30, Steps: 262 | Train Loss: 0.5544708 Vali Loss: 0.2765038 Test Loss: 0.3866352
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4980408
	speed: 0.0575s/iter; left time: 1048.0404s
	iters: 200, epoch: 31 | loss: 0.4494653
	speed: 0.0124s/iter; left time: 225.4196s
Epoch: 31 cost time: 3.7410266399383545
Epoch: 31, Steps: 262 | Train Loss: 0.5542761 Vali Loss: 0.2762305 Test Loss: 0.3866353
Validation loss decreased (0.276303 --> 0.276231).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4349619
	speed: 0.0587s/iter; left time: 1055.0984s
	iters: 200, epoch: 32 | loss: 0.5636219
	speed: 0.0123s/iter; left time: 220.5913s
Epoch: 32 cost time: 3.8094098567962646
Epoch: 32, Steps: 262 | Train Loss: 0.5541133 Vali Loss: 0.2766109 Test Loss: 0.3866324
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.5915881
	speed: 0.0793s/iter; left time: 1404.9955s
	iters: 200, epoch: 33 | loss: 0.3922053
	speed: 0.0141s/iter; left time: 249.0446s
Epoch: 33 cost time: 4.1279730796813965
Epoch: 33, Steps: 262 | Train Loss: 0.5544170 Vali Loss: 0.2763732 Test Loss: 0.3866083
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.5625731
	speed: 0.0831s/iter; left time: 1450.9031s
	iters: 200, epoch: 34 | loss: 0.7666917
	speed: 0.0143s/iter; left time: 248.4959s
Epoch: 34 cost time: 4.221908330917358
Epoch: 34, Steps: 262 | Train Loss: 0.5540531 Vali Loss: 0.2765585 Test Loss: 0.3865851
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5875913
	speed: 0.0581s/iter; left time: 998.5434s
	iters: 200, epoch: 35 | loss: 0.3142933
	speed: 0.0117s/iter; left time: 199.8052s
Epoch: 35 cost time: 3.4909210205078125
Epoch: 35, Steps: 262 | Train Loss: 0.5537291 Vali Loss: 0.2764846 Test Loss: 0.3865831
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4170332
	speed: 0.0616s/iter; left time: 1043.3158s
	iters: 200, epoch: 36 | loss: 0.6182867
	speed: 0.0141s/iter; left time: 238.0776s
Epoch: 36 cost time: 4.257028102874756
Epoch: 36, Steps: 262 | Train Loss: 0.5547188 Vali Loss: 0.2761777 Test Loss: 0.3865650
Validation loss decreased (0.276231 --> 0.276178).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.7148726
	speed: 0.0588s/iter; left time: 980.3521s
	iters: 200, epoch: 37 | loss: 0.5782250
	speed: 0.0118s/iter; left time: 195.7624s
Epoch: 37 cost time: 3.6012325286865234
Epoch: 37, Steps: 262 | Train Loss: 0.5541797 Vali Loss: 0.2765852 Test Loss: 0.3865652
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.5382702
	speed: 0.0633s/iter; left time: 1038.9127s
	iters: 200, epoch: 38 | loss: 0.5274479
	speed: 0.0152s/iter; left time: 247.1848s
Epoch: 38 cost time: 4.48111629486084
Epoch: 38, Steps: 262 | Train Loss: 0.5543695 Vali Loss: 0.2766127 Test Loss: 0.3865561
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.5455800
	speed: 0.0725s/iter; left time: 1170.9044s
	iters: 200, epoch: 39 | loss: 0.5725116
	speed: 0.0286s/iter; left time: 458.6100s
Epoch: 39 cost time: 6.2158544063568115
Epoch: 39, Steps: 262 | Train Loss: 0.5542496 Vali Loss: 0.2765414 Test Loss: 0.3865417
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.5353875
	speed: 0.0739s/iter; left time: 1173.6863s
	iters: 200, epoch: 40 | loss: 0.5195354
	speed: 0.0127s/iter; left time: 200.8735s
Epoch: 40 cost time: 4.194701433181763
Epoch: 40, Steps: 262 | Train Loss: 0.5547636 Vali Loss: 0.2761959 Test Loss: 0.3865400
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.5994319
	speed: 0.0786s/iter; left time: 1227.7887s
	iters: 200, epoch: 41 | loss: 0.4247701
	speed: 0.0227s/iter; left time: 352.0903s
Epoch: 41 cost time: 5.457084894180298
Epoch: 41, Steps: 262 | Train Loss: 0.5546888 Vali Loss: 0.2764614 Test Loss: 0.3865270
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.3618949
	speed: 0.0777s/iter; left time: 1192.6670s
	iters: 200, epoch: 42 | loss: 0.4464981
	speed: 0.0317s/iter; left time: 483.4662s
Epoch: 42 cost time: 6.35390043258667
Epoch: 42, Steps: 262 | Train Loss: 0.5540209 Vali Loss: 0.2764222 Test Loss: 0.3865210
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3907541
	speed: 0.0709s/iter; left time: 1070.3390s
	iters: 200, epoch: 43 | loss: 0.5435502
	speed: 0.0155s/iter; left time: 232.8700s
Epoch: 43 cost time: 4.590831995010376
Epoch: 43, Steps: 262 | Train Loss: 0.5539414 Vali Loss: 0.2763739 Test Loss: 0.3865081
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.5514196
	speed: 0.0684s/iter; left time: 1015.1420s
	iters: 200, epoch: 44 | loss: 0.5887827
	speed: 0.0124s/iter; left time: 183.3221s
Epoch: 44 cost time: 3.82625675201416
Epoch: 44, Steps: 262 | Train Loss: 0.5543870 Vali Loss: 0.2763720 Test Loss: 0.3865122
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5983505
	speed: 0.0670s/iter; left time: 976.8985s
	iters: 200, epoch: 45 | loss: 0.5201080
	speed: 0.0239s/iter; left time: 345.3459s
Epoch: 45 cost time: 5.511613845825195
Epoch: 45, Steps: 262 | Train Loss: 0.5539960 Vali Loss: 0.2763958 Test Loss: 0.3865089
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4636949
	speed: 0.0834s/iter; left time: 1193.0229s
	iters: 200, epoch: 46 | loss: 0.7073354
	speed: 0.0156s/iter; left time: 221.7042s
Epoch: 46 cost time: 4.687227964401245
Epoch: 46, Steps: 262 | Train Loss: 0.5541977 Vali Loss: 0.2761990 Test Loss: 0.3865049
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.5398337
	speed: 0.0825s/iter; left time: 1158.9714s
	iters: 200, epoch: 47 | loss: 0.4854005
	speed: 0.0178s/iter; left time: 248.2581s
Epoch: 47 cost time: 4.955719709396362
Epoch: 47, Steps: 262 | Train Loss: 0.5534925 Vali Loss: 0.2763222 Test Loss: 0.3864964
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.5087147
	speed: 0.0833s/iter; left time: 1148.8574s
	iters: 200, epoch: 48 | loss: 0.7348291
	speed: 0.0269s/iter; left time: 367.8941s
Epoch: 48 cost time: 7.003463506698608
Epoch: 48, Steps: 262 | Train Loss: 0.5541001 Vali Loss: 0.2764924 Test Loss: 0.3864857
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4325874
	speed: 0.0829s/iter; left time: 1121.4947s
	iters: 200, epoch: 49 | loss: 0.3075029
	speed: 0.0249s/iter; left time: 333.7342s
Epoch: 49 cost time: 6.102338075637817
Epoch: 49, Steps: 262 | Train Loss: 0.5532455 Vali Loss: 0.2762825 Test Loss: 0.3864883
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.3891897
	speed: 0.0729s/iter; left time: 967.3639s
	iters: 200, epoch: 50 | loss: 0.5456056
	speed: 0.0142s/iter; left time: 186.9371s
Epoch: 50 cost time: 4.397167205810547
Epoch: 50, Steps: 262 | Train Loss: 0.5543098 Vali Loss: 0.2764844 Test Loss: 0.3864808
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.3783974
	speed: 0.0714s/iter; left time: 928.7250s
	iters: 200, epoch: 51 | loss: 0.5807499
	speed: 0.0148s/iter; left time: 190.5210s
Epoch: 51 cost time: 4.9199957847595215
Epoch: 51, Steps: 262 | Train Loss: 0.5543593 Vali Loss: 0.2764254 Test Loss: 0.3864804
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.5049450
	speed: 0.1010s/iter; left time: 1286.1073s
	iters: 200, epoch: 52 | loss: 0.5405548
	speed: 0.0140s/iter; left time: 176.8570s
Epoch: 52 cost time: 5.094653129577637
Epoch: 52, Steps: 262 | Train Loss: 0.5537551 Vali Loss: 0.2762916 Test Loss: 0.3864766
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4311985
	speed: 0.0651s/iter; left time: 812.0523s
	iters: 200, epoch: 53 | loss: 0.5815012
	speed: 0.0123s/iter; left time: 152.3879s
Epoch: 53 cost time: 3.826436758041382
Epoch: 53, Steps: 262 | Train Loss: 0.5541233 Vali Loss: 0.2761886 Test Loss: 0.3864741
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.5604969
	speed: 0.0780s/iter; left time: 953.0165s
	iters: 200, epoch: 54 | loss: 0.5198368
	speed: 0.0224s/iter; left time: 271.7760s
Epoch: 54 cost time: 5.622548818588257
Epoch: 54, Steps: 262 | Train Loss: 0.5536294 Vali Loss: 0.2764265 Test Loss: 0.3864709
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.4972671
	speed: 0.0756s/iter; left time: 903.2713s
	iters: 200, epoch: 55 | loss: 0.7132947
	speed: 0.0159s/iter; left time: 188.1597s
Epoch: 55 cost time: 5.1209492683410645
Epoch: 55, Steps: 262 | Train Loss: 0.5538327 Vali Loss: 0.2766202 Test Loss: 0.3864689
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.5157995
	speed: 0.0690s/iter; left time: 806.4051s
	iters: 200, epoch: 56 | loss: 0.3993347
	speed: 0.0149s/iter; left time: 172.9062s
Epoch: 56 cost time: 4.408545255661011
Epoch: 56, Steps: 262 | Train Loss: 0.5543022 Vali Loss: 0.2762383 Test Loss: 0.3864656
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_180_720_FITS_ETTm2_ftM_sl180_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3839258849620819, mae:0.3873733878135681, rse:0.4980444014072418, corr:[0.5489925  0.5447895  0.54031765 0.540647   0.5401842  0.5376019
 0.53628075 0.53652495 0.53611207 0.5343835  0.53304833 0.53311193
 0.5333591  0.5326065  0.531455   0.5309815  0.5309997  0.5304682
 0.52915037 0.52795905 0.5275019  0.5272118  0.5264413  0.52548635
 0.5250374  0.5251056  0.5250259  0.52433866 0.5234115  0.5227908
 0.52245915 0.52200353 0.5212745  0.52066356 0.52039576 0.5201595
 0.5194826  0.518512   0.51763254 0.51700616 0.51645845 0.5158384
 0.5153491  0.51515317 0.5150004  0.5145333  0.51363045 0.51263094
 0.5118328  0.5111678  0.51038736 0.5094832  0.5086512  0.50807583
 0.5077172  0.50734556 0.50677466 0.50625724 0.50588924 0.50561255
 0.50525415 0.5049234  0.50477374 0.504683   0.5045553  0.50432134
 0.50398976 0.5037603  0.5036601  0.5036195  0.50349027 0.50329435
 0.5031235  0.50292164 0.50271076 0.50242466 0.50211525 0.5017576
 0.5013888  0.5009827  0.5005295  0.50007236 0.49967867 0.49930367
 0.49888122 0.49842265 0.49793407 0.49749023 0.4970691  0.49667087
 0.49632344 0.4959537  0.4954383  0.494674   0.4935803  0.49208194
 0.49017856 0.48815188 0.48621845 0.48451427 0.4830546  0.48175597
 0.48050463 0.47921443 0.47780827 0.47636354 0.47502312 0.47387668
 0.4728245  0.47167933 0.47060725 0.46965715 0.46879417 0.46783912
 0.46664762 0.46534827 0.46408805 0.4629685  0.4620122  0.46107662
 0.46013698 0.45916197 0.4581442  0.45714822 0.45622906 0.4553809
 0.45453548 0.45358393 0.45255613 0.45154244 0.45065    0.4498273
 0.44902208 0.4481534  0.4474029  0.4467999  0.4462112  0.4456305
 0.4450542  0.44450712 0.44391146 0.4432146  0.4424758  0.44172418
 0.44093403 0.4400126  0.43892083 0.4378841  0.43713492 0.43656856
 0.43601593 0.43548542 0.43490934 0.43440214 0.43397948 0.4334748
 0.4328897  0.43240264 0.43207484 0.43186986 0.431599   0.43115264
 0.43072093 0.43040687 0.43041673 0.43050256 0.43045616 0.43035606
 0.43030658 0.43035945 0.4304059  0.43042886 0.43048853 0.43049625
 0.43041536 0.43017894 0.4298983  0.4296693  0.429513   0.42931974
 0.42898098 0.42860237 0.42840216 0.4282851  0.42808163 0.4276708
 0.4272397  0.4269476  0.4266701  0.4260313  0.42477357 0.42298687
 0.42108157 0.41945955 0.418006   0.41653687 0.4150933  0.41370463
 0.41239318 0.4111214  0.40984774 0.40872046 0.40778476 0.40714172
 0.40661445 0.40601858 0.40531743 0.4045777  0.40382862 0.40305233
 0.40225062 0.40148374 0.40074614 0.3999902  0.39919582 0.39832306
 0.39744052 0.39650574 0.3956711  0.3947843  0.39372545 0.39260828
 0.3914957  0.39056018 0.3898914  0.38918415 0.38836253 0.38746133
 0.38653052 0.3856451  0.3847556  0.38384783 0.38296866 0.3823463
 0.381982   0.3817678  0.38154563 0.38105053 0.3804663  0.37989235
 0.37938383 0.3788431  0.37824985 0.37785026 0.37764838 0.37758094
 0.37758762 0.37763187 0.3777325  0.3777739  0.37778845 0.37783194
 0.37795702 0.3782943  0.378601   0.3786173  0.3783536  0.37812603
 0.37808436 0.37809208 0.37803122 0.37798122 0.3780053  0.37805247
 0.3780746  0.37799466 0.37782177 0.37768805 0.37771675 0.37778705
 0.37765235 0.37732777 0.37694064 0.37671927 0.37662014 0.37654898
 0.37645352 0.3763769  0.37637824 0.37637827 0.37618038 0.37594593
 0.3758038  0.37571436 0.37555107 0.3751455  0.37440166 0.37329775
 0.37194064 0.37067285 0.36955315 0.3686931  0.36802813 0.3674374
 0.36688682 0.36624274 0.3655355  0.3648439  0.36428833 0.36384174
 0.3634745  0.3630422  0.36258888 0.3620737  0.36147606 0.36079276
 0.36008087 0.35933262 0.358508   0.3577064  0.3569782  0.35630137
 0.35556853 0.3548289  0.354079   0.35338604 0.35262352 0.3517948
 0.35093424 0.35006648 0.34922153 0.348444   0.34783947 0.34741062
 0.34703502 0.34649006 0.34577194 0.34507877 0.34459767 0.34434432
 0.34413543 0.34374964 0.3433269  0.342992   0.3427053  0.34228733
 0.34152973 0.34060508 0.33981654 0.3394126  0.3391404  0.33886805
 0.33868822 0.33870807 0.33877185 0.33874476 0.338545   0.33842066
 0.3384445  0.33854184 0.3385626  0.33851773 0.33852276 0.33859077
 0.33857116 0.33842593 0.33818474 0.33800587 0.33786464 0.33771718
 0.3376607  0.33780798 0.33808953 0.33828858 0.33823073 0.3381017
 0.338064   0.33817938 0.33829185 0.33830178 0.3384471  0.33871344
 0.33896267 0.33900276 0.33882025 0.33867118 0.33874047 0.33891344
 0.3389623  0.3388006  0.33859718 0.33839294 0.33790264 0.3367661
 0.33508605 0.3334066  0.3321442  0.33120593 0.33022538 0.32919258
 0.32825032 0.32747138 0.32671648 0.3258094  0.3247615  0.32388663
 0.32334825 0.3229704  0.32257864 0.3221201  0.3215779  0.32090625
 0.32012114 0.3191653  0.31815246 0.3172055  0.31636426 0.31567106
 0.31520033 0.31486994 0.3144773  0.31386778 0.31310666 0.31239352
 0.31179753 0.31123596 0.31064495 0.31012762 0.30968317 0.3093034
 0.30878752 0.30810112 0.30756524 0.30747092 0.30749094 0.30730125
 0.30691448 0.30663463 0.3066594  0.30684334 0.30671066 0.30618808
 0.3054552  0.3048761  0.3043453  0.30380148 0.3032419  0.30287164
 0.30287284 0.30297557 0.3028912  0.3025775  0.30226475 0.3021605
 0.30201626 0.30166948 0.30131337 0.30118063 0.30130604 0.3014059
 0.30124658 0.30088794 0.30067492 0.30074474 0.30081746 0.3009055
 0.30107757 0.3013624  0.30150402 0.30134755 0.3010625  0.30088165
 0.30081072 0.30067042 0.30036783 0.30011335 0.30009258 0.3002495
 0.30026722 0.29996303 0.29959735 0.29940373 0.29935148 0.2991784
 0.29875186 0.29826602 0.2978433  0.29721797 0.29604203 0.29429594
 0.29241166 0.2909111  0.28980118 0.28885832 0.2878928  0.2869096
 0.28596947 0.28504086 0.2841497  0.2834486  0.2830926  0.2830396
 0.28306365 0.2829499  0.2826829  0.2823807  0.2820168  0.28147689
 0.28076443 0.2798929  0.27892733 0.27798182 0.27709126 0.27629358
 0.2755387  0.27481598 0.2741164  0.27344254 0.2728702  0.2723637
 0.27176085 0.27101365 0.27023742 0.26959673 0.26911312 0.26863894
 0.26804432 0.26741835 0.26693466 0.2665899  0.26629636 0.26598227
 0.26567227 0.2654542  0.2651732  0.2646462  0.26406512 0.26353642
 0.26312634 0.26258823 0.26178744 0.26096675 0.26049253 0.26026455
 0.260049   0.25976458 0.2596307  0.2597025  0.25981054 0.2595241
 0.25899008 0.25866684 0.25877425 0.25907227 0.25905785 0.25863725
 0.25814706 0.2579561  0.25794545 0.2578538  0.25766718 0.2576934
 0.25783083 0.2576817  0.2571144  0.25660214 0.2567757  0.25751704
 0.25790673 0.25755003 0.2568413  0.25655577 0.25687557 0.25722462
 0.2570931  0.25671023 0.2565481  0.2566438  0.25666866 0.25637215
 0.25599447 0.2557379  0.25547728 0.25479114 0.25348005 0.25166973
 0.24976192 0.24810548 0.24662222 0.24530618 0.24425672 0.24349223
 0.24275403 0.24184462 0.24068063 0.23944667 0.2385042  0.23802686
 0.23773807 0.2373022  0.23677494 0.23626904 0.23578753 0.23524027
 0.23449594 0.2335671  0.23264797 0.23189178 0.2313653  0.23084152
 0.23020904 0.22961304 0.22912355 0.22864977 0.2281193  0.22735937
 0.22646292 0.22586788 0.22561598 0.2253107  0.22479558 0.22416055
 0.22384576 0.22380926 0.2237044  0.22327043 0.22272518 0.22252719
 0.22261927 0.22269842 0.22226317 0.2216827  0.22135623 0.22124934
 0.22110264 0.2207344  0.2204241  0.22039302 0.22041331 0.22011636
 0.21970887 0.21981066 0.22039919 0.22094046 0.22102301 0.22059263
 0.22040261 0.22070251 0.22100402 0.22082768 0.22038034 0.22018307
 0.22045358 0.2207306  0.22064625 0.22031827 0.22025752 0.2204913
 0.22049652 0.2202367  0.22006138 0.22052842 0.2212695  0.22152919
 0.22131501 0.22114012 0.22140756 0.22194807 0.2222492  0.22229475
 0.22268802 0.22347945 0.22418584 0.22442693 0.22448696 0.22488146
 0.2255202  0.22581707 0.22538058 0.22449502 0.22354078 0.22242373
 0.22091495 0.21924011 0.21787283 0.21701366 0.21641296 0.2157082
 0.2147743  0.2139093  0.21338381 0.2131492  0.21297197 0.21271372
 0.21250409 0.2123553  0.2123322  0.21218388 0.21176586 0.21102138
 0.21019128 0.20938547 0.20843036 0.20719364 0.20588373 0.20483647
 0.20420414 0.20352443 0.20232779 0.20079635 0.19951671 0.19882742
 0.19814566 0.19684765 0.19501604 0.19367416 0.19316308 0.19280185
 0.191714   0.1904396  0.18988585 0.19006483 0.18948466 0.1881947
 0.18764925 0.18904302 0.1901825  0.18868633 0.1876784  0.19603623]
