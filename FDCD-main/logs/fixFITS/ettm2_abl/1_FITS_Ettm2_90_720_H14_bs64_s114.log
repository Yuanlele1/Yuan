Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=24, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=24, out_features=216, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4644864.0
params:  5400.0
Trainable parameters:  5400
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.9365311
	speed: 0.0497s/iter; left time: 1302.9939s
	iters: 200, epoch: 1 | loss: 0.5927804
	speed: 0.0415s/iter; left time: 1083.4878s
Epoch: 1 cost time: 11.096679925918579
Epoch: 1, Steps: 263 | Train Loss: 0.7418334 Vali Loss: 0.3268798 Test Loss: 0.4563088
Validation loss decreased (inf --> 0.326880).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.8095720
	speed: 0.1559s/iter; left time: 4043.2835s
	iters: 200, epoch: 2 | loss: 0.6169353
	speed: 0.0352s/iter; left time: 910.3716s
Epoch: 2 cost time: 11.331861019134521
Epoch: 2, Steps: 263 | Train Loss: 0.6124007 Vali Loss: 0.2955586 Test Loss: 0.4192329
Validation loss decreased (0.326880 --> 0.295559).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3386007
	speed: 0.2133s/iter; left time: 5476.6156s
	iters: 200, epoch: 3 | loss: 0.7535290
	speed: 0.0384s/iter; left time: 981.5306s
Epoch: 3 cost time: 11.909790515899658
Epoch: 3, Steps: 263 | Train Loss: 0.5949299 Vali Loss: 0.2907769 Test Loss: 0.4135755
Validation loss decreased (0.295559 --> 0.290777).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4389852
	speed: 0.2004s/iter; left time: 5092.2397s
	iters: 200, epoch: 4 | loss: 0.5283582
	speed: 0.0470s/iter; left time: 1188.7841s
Epoch: 4 cost time: 11.249076128005981
Epoch: 4, Steps: 263 | Train Loss: 0.5916475 Vali Loss: 0.2892456 Test Loss: 0.4121298
Validation loss decreased (0.290777 --> 0.289246).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5351083
	speed: 0.1554s/iter; left time: 3908.4730s
	iters: 200, epoch: 5 | loss: 0.4290839
	speed: 0.0342s/iter; left time: 857.4103s
Epoch: 5 cost time: 11.08400559425354
Epoch: 5, Steps: 263 | Train Loss: 0.5901969 Vali Loss: 0.2889915 Test Loss: 0.4115628
Validation loss decreased (0.289246 --> 0.288991).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6584128
	speed: 0.1658s/iter; left time: 4126.8664s
	iters: 200, epoch: 6 | loss: 0.4872565
	speed: 0.0288s/iter; left time: 715.0005s
Epoch: 6 cost time: 9.668912172317505
Epoch: 6, Steps: 263 | Train Loss: 0.5892995 Vali Loss: 0.2892159 Test Loss: 0.4113452
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5116838
	speed: 0.1901s/iter; left time: 4680.8092s
	iters: 200, epoch: 7 | loss: 0.5898566
	speed: 0.0474s/iter; left time: 1162.9918s
Epoch: 7 cost time: 13.136923789978027
Epoch: 7, Steps: 263 | Train Loss: 0.5892301 Vali Loss: 0.2887093 Test Loss: 0.4110658
Validation loss decreased (0.288991 --> 0.288709).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.7862787
	speed: 0.2397s/iter; left time: 5838.4306s
	iters: 200, epoch: 8 | loss: 0.5131537
	speed: 0.0389s/iter; left time: 944.1866s
Epoch: 8 cost time: 12.513257503509521
Epoch: 8, Steps: 263 | Train Loss: 0.5882167 Vali Loss: 0.2889962 Test Loss: 0.4109718
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.8485084
	speed: 0.1475s/iter; left time: 3553.2824s
	iters: 200, epoch: 9 | loss: 0.5599840
	speed: 0.0302s/iter; left time: 724.5016s
Epoch: 9 cost time: 8.89410138130188
Epoch: 9, Steps: 263 | Train Loss: 0.5883483 Vali Loss: 0.2888253 Test Loss: 0.4110369
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5988727
	speed: 0.1414s/iter; left time: 3370.0609s
	iters: 200, epoch: 10 | loss: 0.6902227
	speed: 0.0536s/iter; left time: 1273.2162s
Epoch: 10 cost time: 13.550225973129272
Epoch: 10, Steps: 263 | Train Loss: 0.5880237 Vali Loss: 0.2890523 Test Loss: 0.4109853
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6688725
	speed: 0.2141s/iter; left time: 5045.5619s
	iters: 200, epoch: 11 | loss: 0.6215011
	speed: 0.0617s/iter; left time: 1447.5407s
Epoch: 11 cost time: 16.244924783706665
Epoch: 11, Steps: 263 | Train Loss: 0.5877143 Vali Loss: 0.2890961 Test Loss: 0.4109748
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5247346
	speed: 0.2510s/iter; left time: 5851.1337s
	iters: 200, epoch: 12 | loss: 0.4107499
	speed: 0.0597s/iter; left time: 1385.0260s
Epoch: 12 cost time: 15.587055206298828
Epoch: 12, Steps: 263 | Train Loss: 0.5878334 Vali Loss: 0.2889293 Test Loss: 0.4109401
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4831191
	speed: 0.1545s/iter; left time: 3560.0813s
	iters: 200, epoch: 13 | loss: 0.8944579
	speed: 0.0260s/iter; left time: 596.8236s
Epoch: 13 cost time: 8.198611736297607
Epoch: 13, Steps: 263 | Train Loss: 0.5869334 Vali Loss: 0.2891128 Test Loss: 0.4109290
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3539070
	speed: 0.1556s/iter; left time: 3544.5623s
	iters: 200, epoch: 14 | loss: 0.6332179
	speed: 0.0294s/iter; left time: 667.9655s
Epoch: 14 cost time: 8.983153581619263
Epoch: 14, Steps: 263 | Train Loss: 0.5876169 Vali Loss: 0.2889901 Test Loss: 0.4109783
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4187776
	speed: 0.2005s/iter; left time: 4515.6541s
	iters: 200, epoch: 15 | loss: 0.9378285
	speed: 0.0355s/iter; left time: 795.1705s
Epoch: 15 cost time: 10.918473243713379
Epoch: 15, Steps: 263 | Train Loss: 0.5865249 Vali Loss: 0.2891905 Test Loss: 0.4110037
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5748998
	speed: 0.1797s/iter; left time: 3999.1248s
	iters: 200, epoch: 16 | loss: 0.4033232
	speed: 0.0515s/iter; left time: 1140.0185s
Epoch: 16 cost time: 12.348598718643188
Epoch: 16, Steps: 263 | Train Loss: 0.5869585 Vali Loss: 0.2891602 Test Loss: 0.4110515
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.6443048
	speed: 0.1625s/iter; left time: 3574.6499s
	iters: 200, epoch: 17 | loss: 0.7738332
	speed: 0.0342s/iter; left time: 749.1125s
Epoch: 17 cost time: 9.067731618881226
Epoch: 17, Steps: 263 | Train Loss: 0.5870797 Vali Loss: 0.2892286 Test Loss: 0.4109478
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.6083022
	speed: 0.1553s/iter; left time: 3373.6546s
	iters: 200, epoch: 18 | loss: 0.6121701
	speed: 0.0430s/iter; left time: 929.3861s
Epoch: 18 cost time: 11.20452094078064
Epoch: 18, Steps: 263 | Train Loss: 0.5871300 Vali Loss: 0.2890671 Test Loss: 0.4109657
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.6671784
	speed: 0.1928s/iter; left time: 4138.1550s
	iters: 200, epoch: 19 | loss: 0.6714089
	speed: 0.0562s/iter; left time: 1200.4215s
Epoch: 19 cost time: 14.517037391662598
Epoch: 19, Steps: 263 | Train Loss: 0.5870789 Vali Loss: 0.2892941 Test Loss: 0.4109665
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5101689
	speed: 0.2096s/iter; left time: 4443.5590s
	iters: 200, epoch: 20 | loss: 0.4964030
	speed: 0.0388s/iter; left time: 818.6768s
Epoch: 20 cost time: 10.664670467376709
Epoch: 20, Steps: 263 | Train Loss: 0.5866135 Vali Loss: 0.2892217 Test Loss: 0.4110153
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.7256282
	speed: 0.1538s/iter; left time: 3219.9437s
	iters: 200, epoch: 21 | loss: 0.4271531
	speed: 0.0312s/iter; left time: 650.2097s
Epoch: 21 cost time: 8.738887071609497
Epoch: 21, Steps: 263 | Train Loss: 0.5871072 Vali Loss: 0.2890626 Test Loss: 0.4109536
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5868118
	speed: 0.1678s/iter; left time: 3469.7070s
	iters: 200, epoch: 22 | loss: 0.6263990
	speed: 0.0390s/iter; left time: 802.0420s
Epoch: 22 cost time: 9.748530864715576
Epoch: 22, Steps: 263 | Train Loss: 0.5873494 Vali Loss: 0.2893734 Test Loss: 0.4109845
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5321478
	speed: 0.2252s/iter; left time: 4597.7007s
	iters: 200, epoch: 23 | loss: 0.5992337
	speed: 0.0379s/iter; left time: 770.3094s
Epoch: 23 cost time: 13.68705439567566
Epoch: 23, Steps: 263 | Train Loss: 0.5870247 Vali Loss: 0.2891639 Test Loss: 0.4110384
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4827654
	speed: 0.2376s/iter; left time: 4788.5565s
	iters: 200, epoch: 24 | loss: 0.5562676
	speed: 0.0310s/iter; left time: 620.8811s
Epoch: 24 cost time: 10.198271989822388
Epoch: 24, Steps: 263 | Train Loss: 0.5858997 Vali Loss: 0.2894289 Test Loss: 0.4110295
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5179043
	speed: 0.1534s/iter; left time: 3050.8416s
	iters: 200, epoch: 25 | loss: 0.5233684
	speed: 0.0395s/iter; left time: 780.9720s
Epoch: 25 cost time: 10.26536250114441
Epoch: 25, Steps: 263 | Train Loss: 0.5868252 Vali Loss: 0.2893547 Test Loss: 0.4110642
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4211526
	speed: 0.1882s/iter; left time: 3693.5321s
	iters: 200, epoch: 26 | loss: 0.6583486
	speed: 0.0466s/iter; left time: 909.7457s
Epoch: 26 cost time: 13.878860712051392
Epoch: 26, Steps: 263 | Train Loss: 0.5872062 Vali Loss: 0.2890976 Test Loss: 0.4110462
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5704504
	speed: 0.1973s/iter; left time: 3820.4243s
	iters: 200, epoch: 27 | loss: 0.5864772
	speed: 0.0417s/iter; left time: 803.4932s
Epoch: 27 cost time: 11.686554193496704
Epoch: 27, Steps: 263 | Train Loss: 0.5868432 Vali Loss: 0.2893637 Test Loss: 0.4110908
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4087654650211334, mae:0.3981781601905823, rse:0.5139033794403076, corr:[0.54704803 0.54685557 0.53758    0.53830767 0.53690284 0.5324923
 0.5322273  0.5329438  0.5308365  0.5292992  0.52979124 0.5290275
 0.5271353  0.5269303  0.5269587  0.52501243 0.52311295 0.52302235
 0.5224636  0.5204821  0.5194428  0.5196527  0.51884174 0.51727444
 0.5168304  0.5169658  0.516098   0.51510704 0.5151574  0.5149259
 0.51358056 0.51267606 0.51278234 0.51231205 0.51096284 0.5102152
 0.51007307 0.50919133 0.5079098  0.5075634  0.50764126 0.50704765
 0.5062593  0.50624394 0.50626767 0.5053992  0.50441456 0.5040683
 0.5036192  0.50247765 0.50134283 0.5007919  0.5001909  0.49919307
 0.49851188 0.4983212  0.49803874 0.4974953  0.49715236 0.49713215
 0.49699908 0.49668968 0.4966153  0.496768   0.49672326 0.4964533
 0.49624377 0.49614552 0.49606597 0.4959585  0.49592766 0.4960081
 0.4959898  0.49581188 0.49556342 0.49541503 0.4952567  0.49488893
 0.49453864 0.4943637  0.49412087 0.49363863 0.49319574 0.49294543
 0.4926723  0.4922871  0.49197215 0.49175486 0.49133605 0.49075457
 0.49030963 0.4898806  0.48912773 0.4879684  0.4865111  0.484667
 0.48223212 0.47961146 0.47745302 0.4756016  0.4737366  0.4722188
 0.47126892 0.47039366 0.46917918 0.46783528 0.46658733 0.46522564
 0.46375987 0.46259746 0.46155697 0.4601483  0.45857078 0.45736074
 0.4563839  0.45503938 0.45352563 0.45254397 0.45188537 0.45076656
 0.44940388 0.44839916 0.44764864 0.44666508 0.44551444 0.4445306
 0.44361836 0.4424789  0.44127262 0.4403601  0.43956506 0.4385886
 0.43768257 0.43702114 0.4362536  0.43539667 0.4347326  0.4345492
 0.43444782 0.4341761  0.4339233  0.43364874 0.43305963 0.4322088
 0.43143764 0.4306186  0.4293217  0.4280297  0.4273306  0.42702872
 0.42635584 0.42557773 0.42518046 0.42495948 0.42448142 0.42378807
 0.4234539  0.42358467 0.4235017  0.42323345 0.42310354 0.42300954
 0.42275852 0.42242283 0.42260337 0.42297435 0.42297617 0.42281544
 0.42295936 0.42317066 0.42292288 0.42244047 0.4222539  0.4222116
 0.42197853 0.42178842 0.4218783  0.42195737 0.42179233 0.42169946
 0.42177927 0.4215069  0.42070594 0.41992682 0.41963223 0.41935968
 0.41874546 0.41812366 0.41761848 0.41663995 0.41484532 0.41269165
 0.4106237  0.40846193 0.40607262 0.40395728 0.4023806  0.4009957
 0.3997381  0.39880195 0.39797422 0.3968143  0.39542967 0.39415485
 0.39293426 0.3915638  0.39015952 0.38893148 0.38766328 0.38612965
 0.38454738 0.3832894  0.38209265 0.38068134 0.37933016 0.37832797
 0.37746978 0.3763925  0.37532452 0.37437096 0.37332574 0.37211826
 0.37100604 0.37001005 0.36907843 0.36796346 0.36709857 0.36645076
 0.3655492  0.36426672 0.36300975 0.36216426 0.36147866 0.3608576
 0.36043864 0.36021647 0.35998702 0.3596936  0.35954502 0.35930187
 0.358729   0.35815775 0.3578805  0.3579547  0.35771725 0.35720146
 0.35703906 0.35712567 0.35700136 0.35671535 0.35677496 0.35689902
 0.3566672  0.35660115 0.35718882 0.35775337 0.3577248  0.3575823
 0.35778403 0.35792354 0.35762873 0.35767752 0.35842016 0.3590411
 0.35906655 0.35901123 0.35945785 0.35973006 0.35952297 0.35940704
 0.35969883 0.3599114  0.35984    0.3600254  0.36057478 0.36090073
 0.3608524  0.36086348 0.36098212 0.3608825  0.36064446 0.3607915
 0.36115012 0.3610672  0.3604329  0.35962012 0.3586312  0.35722408
 0.35566106 0.3544309  0.3535319  0.35279375 0.3524633  0.35283634
 0.35345513 0.35364494 0.3535644  0.3535199  0.35318387 0.3524491
 0.35181788 0.35155022 0.35118377 0.3503526  0.34939215 0.34854636
 0.34757942 0.3464936  0.34573492 0.3452788  0.34456328 0.34361103
 0.34297487 0.34278613 0.34246117 0.3419081  0.34153277 0.34114504
 0.34025088 0.33925742 0.3389746  0.339063   0.33861548 0.33780935
 0.33743876 0.33732155 0.33668768 0.33578843 0.3356418  0.33597448
 0.33584842 0.3354042  0.33543926 0.3356243  0.3352367  0.33470795
 0.33469543 0.3347758  0.3343366  0.33387804 0.33401468 0.33438858
 0.3344288  0.33441317 0.33467484 0.3349494  0.3350239  0.33528468
 0.33586577 0.33616215 0.33611006 0.33626652 0.33675018 0.33688757
 0.3365571  0.336563   0.33711118 0.33759505 0.33765045 0.33782488
 0.33835295 0.33873114 0.33869314 0.33876944 0.33916563 0.3394332
 0.33934563 0.33941928 0.3399031  0.34023842 0.34020537 0.34025237
 0.340647   0.34105313 0.3412334  0.3413697  0.3414751  0.3414056
 0.34132043 0.34138983 0.3412598  0.34044006 0.33908218 0.3377338
 0.33640388 0.33491233 0.33353096 0.33259326 0.33190334 0.33119926
 0.33065957 0.3305219  0.33038777 0.32979876 0.32900676 0.32831877
 0.32759073 0.3266869  0.325801   0.32499805 0.32396278 0.3226259
 0.3213454  0.32020262 0.31897414 0.31774485 0.31672513 0.3158781
 0.31486353 0.31384525 0.31325367 0.3128402  0.31199571 0.31084156
 0.31013396 0.30984968 0.30929428 0.3083953  0.3076744  0.30737233
 0.30701554 0.30629557 0.3054662  0.30457497 0.30354908 0.30283785
 0.30278364 0.30291742 0.30256793 0.30209228 0.3020414  0.3021538
 0.30177167 0.30098662 0.30022734 0.29940802 0.29830542 0.29761687
 0.29753807 0.2972829  0.2964346  0.29600573 0.29657423 0.2970677
 0.2966313  0.29616758 0.2965085  0.29688084 0.2966455  0.29635438
 0.29646027 0.29637364 0.2959491  0.29592264 0.29632753 0.29633847
 0.29580608 0.29559186 0.29592445 0.29615554 0.29615375 0.296207
 0.2962226  0.2959425  0.29578623 0.29606688 0.2962532  0.29594508
 0.29570004 0.2958673  0.29590273 0.29555437 0.29535097 0.29532307
 0.29472515 0.29349974 0.29258168 0.29191223 0.29030862 0.28739554
 0.28448233 0.28248304 0.28092623 0.27943373 0.27843857 0.2780784
 0.27778202 0.27757213 0.2778339  0.27820158 0.27791354 0.27711886
 0.276356   0.27558145 0.27428898 0.27282023 0.27190685 0.271302
 0.27021596 0.2687042  0.26757687 0.26688212 0.26583445 0.26433182
 0.26309925 0.26244214 0.2618138  0.26091212 0.26016095 0.2596338
 0.25891057 0.25789404 0.25700733 0.2564351  0.25584063 0.25523806
 0.2548253  0.25428444 0.25334686 0.2522899  0.25156033 0.2511072
 0.2505679  0.25013208 0.24996914 0.24997087 0.24997227 0.24994442
 0.24979661 0.24907406 0.24791802 0.24690321 0.24624535 0.24565677
 0.2451903  0.24530527 0.2457881  0.2457556  0.24519604 0.24492355
 0.24511766 0.24519657 0.24516805 0.24563687 0.24633183 0.2464766
 0.24601759 0.24558458 0.24542157 0.24545516 0.245804   0.2466319
 0.2470206  0.24655496 0.24617322 0.24665019 0.24710292 0.2468259
 0.24648055 0.24689762 0.24742414 0.24733686 0.24719316 0.24764036
 0.2482001  0.24842647 0.24858606 0.2485623  0.24794362 0.24717388
 0.2472562  0.24778225 0.24733189 0.24564041 0.24364623 0.24166662
 0.23932542 0.23717679 0.2362163  0.23606862 0.23572169 0.23548767
 0.2361246  0.23703416 0.2371733  0.23682071 0.23681256 0.23681061
 0.23613617 0.23521669 0.234741   0.23413919 0.23273203 0.23120813
 0.23039463 0.22968282 0.22841318 0.22732452 0.22705759 0.22667933
 0.22538848 0.22419809 0.22409604 0.22422397 0.22362897 0.22258991
 0.22179279 0.22112104 0.220259   0.21947896 0.21889597 0.21799968
 0.21692081 0.21627808 0.21602541 0.21539001 0.21474276 0.21468504
 0.2147557  0.21416964 0.21336699 0.21331008 0.21347901 0.2126097
 0.21134694 0.2108496  0.2106435  0.20981413 0.20919904 0.20948273
 0.20980299 0.20937006 0.20925649 0.21021436 0.2108177  0.20987004
 0.20900294 0.2097168  0.21062715 0.21025711 0.2097159  0.20983166
 0.20949279 0.2082996  0.208094   0.2092642  0.2098273  0.20888166
 0.208403   0.20928094 0.20978403 0.20924722 0.20922534 0.21020757
 0.21099283 0.21145397 0.21244939 0.21345583 0.2133489  0.2130327
 0.21386522 0.21482927 0.21459778 0.21411489 0.2145149  0.21471822
 0.2139265  0.21345106 0.21379304 0.21319887 0.21091098 0.20859583
 0.20709777 0.20527445 0.20315324 0.20229639 0.20253633 0.2022291
 0.20172565 0.202722   0.20442516 0.20462258 0.203665   0.20346291
 0.20369495 0.20271815 0.20108831 0.20048939 0.20036481 0.19914746
 0.19758274 0.19707733 0.19667572 0.1951392  0.19350518 0.1928956
 0.19212352 0.19043759 0.18929318 0.18917087 0.18817629 0.18573958
 0.18433386 0.18424764 0.1827298  0.17988579 0.17878275 0.17943266
 0.17820035 0.17561015 0.17483371 0.1750269  0.17278612 0.17062551
 0.17143041 0.17168133 0.16965495 0.1698787  0.1713003  0.1709823 ]
