Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=20, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34135
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=20, out_features=94, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1684480.0
params:  1974.0
Trainable parameters:  1974
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5372143
	speed: 0.0214s/iter; left time: 566.7578s
	iters: 200, epoch: 1 | loss: 0.7631723
	speed: 0.0131s/iter; left time: 346.0181s
Epoch: 1 cost time: 4.250135183334351
Epoch: 1, Steps: 266 | Train Loss: 0.5354888 Vali Loss: 0.2408723 Test Loss: 0.3352213
Validation loss decreased (inf --> 0.240872).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6478007
	speed: 0.0667s/iter; left time: 1750.5121s
	iters: 200, epoch: 2 | loss: 0.7072445
	speed: 0.0133s/iter; left time: 346.5998s
Epoch: 2 cost time: 4.02522873878479
Epoch: 2, Steps: 266 | Train Loss: 0.4601426 Vali Loss: 0.2228792 Test Loss: 0.3138337
Validation loss decreased (0.240872 --> 0.222879).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3606311
	speed: 0.0675s/iter; left time: 1753.7010s
	iters: 200, epoch: 3 | loss: 0.4515998
	speed: 0.0129s/iter; left time: 334.7793s
Epoch: 3 cost time: 4.126457691192627
Epoch: 3, Steps: 266 | Train Loss: 0.4484390 Vali Loss: 0.2194820 Test Loss: 0.3097840
Validation loss decreased (0.222879 --> 0.219482).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5198030
	speed: 0.0645s/iter; left time: 1657.6899s
	iters: 200, epoch: 4 | loss: 0.5225455
	speed: 0.0130s/iter; left time: 333.4989s
Epoch: 4 cost time: 3.822906017303467
Epoch: 4, Steps: 266 | Train Loss: 0.4456557 Vali Loss: 0.2184019 Test Loss: 0.3086076
Validation loss decreased (0.219482 --> 0.218402).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4606904
	speed: 0.0641s/iter; left time: 1629.9895s
	iters: 200, epoch: 5 | loss: 0.2964235
	speed: 0.0129s/iter; left time: 327.5508s
Epoch: 5 cost time: 3.8800976276397705
Epoch: 5, Steps: 266 | Train Loss: 0.4445419 Vali Loss: 0.2180533 Test Loss: 0.3078938
Validation loss decreased (0.218402 --> 0.218053).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5527142
	speed: 0.0662s/iter; left time: 1665.6683s
	iters: 200, epoch: 6 | loss: 0.4214658
	speed: 0.0124s/iter; left time: 310.0576s
Epoch: 6 cost time: 3.9093706607818604
Epoch: 6, Steps: 266 | Train Loss: 0.4439837 Vali Loss: 0.2180754 Test Loss: 0.3076888
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3478665
	speed: 0.0693s/iter; left time: 1725.0122s
	iters: 200, epoch: 7 | loss: 0.4231794
	speed: 0.0141s/iter; left time: 349.8410s
Epoch: 7 cost time: 4.341277122497559
Epoch: 7, Steps: 266 | Train Loss: 0.4430819 Vali Loss: 0.2181921 Test Loss: 0.3076300
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3734404
	speed: 0.0653s/iter; left time: 1609.4945s
	iters: 200, epoch: 8 | loss: 0.5464318
	speed: 0.0125s/iter; left time: 307.7510s
Epoch: 8 cost time: 4.050890207290649
Epoch: 8, Steps: 266 | Train Loss: 0.4425083 Vali Loss: 0.2179737 Test Loss: 0.3075014
Validation loss decreased (0.218053 --> 0.217974).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2958243
	speed: 0.0670s/iter; left time: 1633.1115s
	iters: 200, epoch: 9 | loss: 0.5523585
	speed: 0.0135s/iter; left time: 327.7175s
Epoch: 9 cost time: 4.206176996231079
Epoch: 9, Steps: 266 | Train Loss: 0.4422970 Vali Loss: 0.2179088 Test Loss: 0.3073774
Validation loss decreased (0.217974 --> 0.217909).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4912447
	speed: 0.0682s/iter; left time: 1643.4386s
	iters: 200, epoch: 10 | loss: 0.5133740
	speed: 0.0129s/iter; left time: 308.8182s
Epoch: 10 cost time: 4.062781095504761
Epoch: 10, Steps: 266 | Train Loss: 0.4418754 Vali Loss: 0.2181538 Test Loss: 0.3073580
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3573488
	speed: 0.0645s/iter; left time: 1537.7434s
	iters: 200, epoch: 11 | loss: 0.4903682
	speed: 0.0126s/iter; left time: 299.2809s
Epoch: 11 cost time: 3.827848434448242
Epoch: 11, Steps: 266 | Train Loss: 0.4419007 Vali Loss: 0.2180954 Test Loss: 0.3072885
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.7156017
	speed: 0.0649s/iter; left time: 1529.9023s
	iters: 200, epoch: 12 | loss: 0.4256301
	speed: 0.0129s/iter; left time: 303.1452s
Epoch: 12 cost time: 4.000892639160156
Epoch: 12, Steps: 266 | Train Loss: 0.4413741 Vali Loss: 0.2180430 Test Loss: 0.3072739
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5911588
	speed: 0.0641s/iter; left time: 1494.9585s
	iters: 200, epoch: 13 | loss: 0.4980907
	speed: 0.0128s/iter; left time: 297.4405s
Epoch: 13 cost time: 4.007844686508179
Epoch: 13, Steps: 266 | Train Loss: 0.4416882 Vali Loss: 0.2180861 Test Loss: 0.3073353
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.5619252
	speed: 0.0671s/iter; left time: 1546.3218s
	iters: 200, epoch: 14 | loss: 0.3221345
	speed: 0.0124s/iter; left time: 285.5419s
Epoch: 14 cost time: 3.9736316204071045
Epoch: 14, Steps: 266 | Train Loss: 0.4416774 Vali Loss: 0.2182695 Test Loss: 0.3072648
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5901377
	speed: 0.0659s/iter; left time: 1501.5426s
	iters: 200, epoch: 15 | loss: 0.2934241
	speed: 0.0127s/iter; left time: 288.4918s
Epoch: 15 cost time: 4.049420595169067
Epoch: 15, Steps: 266 | Train Loss: 0.4415819 Vali Loss: 0.2180623 Test Loss: 0.3072938
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4796253
	speed: 0.0648s/iter; left time: 1459.3707s
	iters: 200, epoch: 16 | loss: 0.3847973
	speed: 0.0124s/iter; left time: 277.2117s
Epoch: 16 cost time: 3.8743038177490234
Epoch: 16, Steps: 266 | Train Loss: 0.4416843 Vali Loss: 0.2183210 Test Loss: 0.3073573
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5203527
	speed: 0.0650s/iter; left time: 1444.8907s
	iters: 200, epoch: 17 | loss: 0.3146109
	speed: 0.0122s/iter; left time: 269.5939s
Epoch: 17 cost time: 3.7871768474578857
Epoch: 17, Steps: 266 | Train Loss: 0.4413545 Vali Loss: 0.2182067 Test Loss: 0.3073376
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3626123
	speed: 0.0618s/iter; left time: 1357.6674s
	iters: 200, epoch: 18 | loss: 0.5380525
	speed: 0.0121s/iter; left time: 265.3020s
Epoch: 18 cost time: 3.7319819927215576
Epoch: 18, Steps: 266 | Train Loss: 0.4408741 Vali Loss: 0.2183437 Test Loss: 0.3073641
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4038921
	speed: 0.0616s/iter; left time: 1338.1302s
	iters: 200, epoch: 19 | loss: 0.3724109
	speed: 0.0121s/iter; left time: 261.8420s
Epoch: 19 cost time: 3.6979424953460693
Epoch: 19, Steps: 266 | Train Loss: 0.4410224 Vali Loss: 0.2185223 Test Loss: 0.3073708
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3664275
	speed: 0.0637s/iter; left time: 1366.2834s
	iters: 200, epoch: 20 | loss: 0.5513461
	speed: 0.0125s/iter; left time: 266.1747s
Epoch: 20 cost time: 3.9628593921661377
Epoch: 20, Steps: 266 | Train Loss: 0.4408395 Vali Loss: 0.2185812 Test Loss: 0.3073822
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3368418
	speed: 0.0656s/iter; left time: 1388.5978s
	iters: 200, epoch: 21 | loss: 0.6901297
	speed: 0.0126s/iter; left time: 266.6241s
Epoch: 21 cost time: 4.089299440383911
Epoch: 21, Steps: 266 | Train Loss: 0.4406069 Vali Loss: 0.2185845 Test Loss: 0.3073665
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5329152
	speed: 0.0641s/iter; left time: 1339.9738s
	iters: 200, epoch: 22 | loss: 0.3512572
	speed: 0.0127s/iter; left time: 264.0660s
Epoch: 22 cost time: 3.8394699096679688
Epoch: 22, Steps: 266 | Train Loss: 0.4404937 Vali Loss: 0.2184790 Test Loss: 0.3073769
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5212103
	speed: 0.0651s/iter; left time: 1344.0682s
	iters: 200, epoch: 23 | loss: 0.4569187
	speed: 0.0129s/iter; left time: 265.1888s
Epoch: 23 cost time: 3.935655355453491
Epoch: 23, Steps: 266 | Train Loss: 0.4406940 Vali Loss: 0.2185767 Test Loss: 0.3073975
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4828233
	speed: 0.0639s/iter; left time: 1303.4398s
	iters: 200, epoch: 24 | loss: 0.4792374
	speed: 0.0130s/iter; left time: 263.2640s
Epoch: 24 cost time: 3.809812307357788
Epoch: 24, Steps: 266 | Train Loss: 0.4408517 Vali Loss: 0.2186749 Test Loss: 0.3074000
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4114308
	speed: 0.0642s/iter; left time: 1291.0363s
	iters: 200, epoch: 25 | loss: 0.3759421
	speed: 0.0123s/iter; left time: 245.2871s
Epoch: 25 cost time: 3.9345543384552
Epoch: 25, Steps: 266 | Train Loss: 0.4407005 Vali Loss: 0.2184867 Test Loss: 0.3074181
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.6449611
	speed: 0.0648s/iter; left time: 1285.5481s
	iters: 200, epoch: 26 | loss: 0.3240542
	speed: 0.0127s/iter; left time: 250.2365s
Epoch: 26 cost time: 3.960029125213623
Epoch: 26, Steps: 266 | Train Loss: 0.4408770 Vali Loss: 0.2185488 Test Loss: 0.3074044
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2676976
	speed: 0.0642s/iter; left time: 1257.6108s
	iters: 200, epoch: 27 | loss: 0.3295582
	speed: 0.0120s/iter; left time: 233.9470s
Epoch: 27 cost time: 3.7073943614959717
Epoch: 27, Steps: 266 | Train Loss: 0.4407812 Vali Loss: 0.2187138 Test Loss: 0.3074309
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3971758
	speed: 0.0631s/iter; left time: 1219.1861s
	iters: 200, epoch: 28 | loss: 0.3478553
	speed: 0.0115s/iter; left time: 221.7659s
Epoch: 28 cost time: 3.7518837451934814
Epoch: 28, Steps: 266 | Train Loss: 0.4410719 Vali Loss: 0.2186190 Test Loss: 0.3074122
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4387489
	speed: 0.0644s/iter; left time: 1227.1226s
	iters: 200, epoch: 29 | loss: 0.5244180
	speed: 0.0126s/iter; left time: 239.1188s
Epoch: 29 cost time: 3.9291672706604004
Epoch: 29, Steps: 266 | Train Loss: 0.4412462 Vali Loss: 0.2187652 Test Loss: 0.3074220
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_336_FITS_ETTm2_ftM_sl90_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.30876755714416504, mae:0.3434358537197113, rse:0.4488244354724884, corr:[0.5610084  0.5624776  0.5545016  0.5515591  0.5524622  0.55098176
 0.5474313  0.5459699  0.5469076  0.54687065 0.54499286 0.5434674
 0.54368657 0.54395443 0.54252213 0.5403987  0.5396224  0.53999937
 0.5396799  0.53819096 0.5368047  0.5363708  0.53608173 0.53511196
 0.53389394 0.53342855 0.53358644 0.5332862  0.53222036 0.53117627
 0.5306862  0.5302949  0.5295196  0.5286581  0.5282505  0.5281399
 0.52760327 0.52655476 0.5255676  0.5250525  0.52474797 0.5243657
 0.52390665 0.5235607  0.5233182  0.5228455  0.52207816 0.5212435
 0.5205907  0.51996744 0.5191424  0.5182721  0.51758415 0.517101
 0.5166952  0.516206   0.51567173 0.51530546 0.5150995  0.5149161
 0.51466846 0.51448816 0.5145148  0.51463777 0.51461047 0.5144228
 0.5142343  0.5141232  0.514134   0.51418054 0.51418024 0.51414967
 0.5141198  0.5141604  0.5141445  0.5139971  0.5137457  0.5134804
 0.51330507 0.51315236 0.51288426 0.5124842  0.51213205 0.51190835
 0.5117292  0.51142746 0.5110086  0.51065314 0.5104133  0.51014555
 0.509706   0.5091171  0.50850195 0.5077147  0.5063889  0.5044727
 0.50237554 0.50044096 0.49856326 0.4965554  0.49460784 0.49306995
 0.49191326 0.49079248 0.48956463 0.4882711  0.4870467  0.48586404
 0.48459655 0.48322427 0.48187053 0.48065555 0.47953868 0.4783057
 0.47692633 0.47559482 0.47444645 0.4733997  0.4723064  0.47107592
 0.46995932 0.46905577 0.4681885  0.46722502 0.46619177 0.46521598
 0.46432912 0.4633932  0.4622757  0.46114504 0.4602175  0.45944548
 0.45865718 0.45780748 0.45700604 0.45652527 0.45612586 0.45569384
 0.45524114 0.4549283  0.45477587 0.45450243 0.45393407 0.45314914
 0.4523283  0.45154548 0.45062476 0.4496643  0.4487969  0.44812405
 0.4475516  0.44703373 0.44645056 0.44584003 0.44545162 0.4452703
 0.44504553 0.44484246 0.4447094  0.44472504 0.4446027  0.44422564
 0.44395    0.44389892 0.44416577 0.44438374 0.4443861  0.44441277
 0.44467974 0.44506586 0.44526973 0.4452663  0.44528598 0.445405
 0.4454587  0.44529387 0.44500834 0.44486606 0.44489205 0.44490406
 0.44475037 0.44451264 0.44438568 0.44422892 0.44385898 0.44332328
 0.44293582 0.44280192 0.44251776 0.44155604 0.43980682 0.43771118
 0.43575877 0.43401507 0.43222126 0.43040714 0.42889583 0.4277434
 0.42666307 0.42548016 0.42439312 0.4234803  0.42256102 0.42141914
 0.42011324 0.41886005 0.41760576 0.41622034 0.41471514 0.4133407
 0.4121881  0.4112154  0.41017526 0.40895686 0.4077285  0.40661734
 0.40562052 0.40456817 0.40349066 0.4023892  0.4013349  0.40032676
 0.399246   0.39794675 0.39683995 0.39598697 0.3954034  0.39476928
 0.39385173 0.39279833 0.3918278  0.3911155  0.39044598 0.3897485
 0.38920543 0.38900512 0.38898528 0.3887554  0.3882949  0.3878474
 0.38756603 0.38722572 0.38655567 0.3860793  0.3860877  0.38634375
 0.38646373 0.38624626 0.3860058  0.38602385 0.38628802 0.38635185
 0.3861193  0.3861471  0.38673732 0.38735777 0.38740197 0.3870177
 0.3868999  0.3873672  0.38787636 0.38799593 0.38797358 0.3883231
 0.38905907 0.3894636  0.3893589  0.38920715 0.38965476 0.39042076
 0.3907166  0.3905239  0.3905066  0.3911408  0.39194885 0.39218208
 0.39192325 0.3918708  0.39238158 0.39301887 0.39315638 0.39293146
 0.3928937  0.3931845  0.39331082 0.39284027 0.39192504 0.390991
 0.39014065 0.38919818 0.38817522 0.38750282 0.38739765 0.38763285
 0.38775417 0.38762173 0.38755342 0.38759223 0.387371   0.3867483
 0.38609707 0.38574982 0.38546637 0.38474151 0.38354057 0.3823535
 0.38163242 0.3811372  0.38036528 0.37927756 0.3783146  0.37785134
 0.3773993  0.37653127 0.37538898 0.3746221  0.37439632 0.37393504
 0.37262484 0.37105837 0.3703129  0.37029174 0.36973152 0.36809307
 0.36646768 0.36603752 0.36612538 0.36501792 0.36308977 0.36242118
 0.36343378 0.36334223 0.3604881  0.35841104 0.36110038 0.36230966]
