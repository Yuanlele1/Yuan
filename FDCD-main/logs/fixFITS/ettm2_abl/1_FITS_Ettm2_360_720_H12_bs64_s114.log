Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_360_720_FITS_ETTm2_ftM_sl360_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=58, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9042432.0
params:  10266.0
Trainable parameters:  10266
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7340782
	speed: 0.0509s/iter; left time: 1322.3229s
	iters: 200, epoch: 1 | loss: 0.6460869
	speed: 0.0643s/iter; left time: 1666.4776s
Epoch: 1 cost time: 15.321121454238892
Epoch: 1, Steps: 261 | Train Loss: 0.6147333 Vali Loss: 0.2848507 Test Loss: 0.3901074
Validation loss decreased (inf --> 0.284851).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5560145
	speed: 0.1930s/iter; left time: 4966.8270s
	iters: 200, epoch: 2 | loss: 0.5658402
	speed: 0.0275s/iter; left time: 706.0509s
Epoch: 2 cost time: 7.869268417358398
Epoch: 2, Steps: 261 | Train Loss: 0.5427799 Vali Loss: 0.2742684 Test Loss: 0.3779150
Validation loss decreased (0.284851 --> 0.274268).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.6548393
	speed: 0.1239s/iter; left time: 3156.4655s
	iters: 200, epoch: 3 | loss: 0.4609481
	speed: 0.0309s/iter; left time: 783.2450s
Epoch: 3 cost time: 8.021777153015137
Epoch: 3, Steps: 261 | Train Loss: 0.5326605 Vali Loss: 0.2711875 Test Loss: 0.3743630
Validation loss decreased (0.274268 --> 0.271188).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4204353
	speed: 0.1390s/iter; left time: 3505.7314s
	iters: 200, epoch: 4 | loss: 0.5780347
	speed: 0.0354s/iter; left time: 888.6405s
Epoch: 4 cost time: 9.481377124786377
Epoch: 4, Steps: 261 | Train Loss: 0.5286861 Vali Loss: 0.2688881 Test Loss: 0.3726560
Validation loss decreased (0.271188 --> 0.268888).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4617278
	speed: 0.2191s/iter; left time: 5466.9629s
	iters: 200, epoch: 5 | loss: 0.5096602
	speed: 0.0430s/iter; left time: 1068.7009s
Epoch: 5 cost time: 12.836262226104736
Epoch: 5, Steps: 261 | Train Loss: 0.5260487 Vali Loss: 0.2676178 Test Loss: 0.3715082
Validation loss decreased (0.268888 --> 0.267618).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.6850119
	speed: 0.1427s/iter; left time: 3523.6719s
	iters: 200, epoch: 6 | loss: 0.6194912
	speed: 0.0422s/iter; left time: 1038.2377s
Epoch: 6 cost time: 9.30824327468872
Epoch: 6, Steps: 261 | Train Loss: 0.5238344 Vali Loss: 0.2666773 Test Loss: 0.3707777
Validation loss decreased (0.267618 --> 0.266677).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3970858
	speed: 0.1502s/iter; left time: 3671.0025s
	iters: 200, epoch: 7 | loss: 0.5580759
	speed: 0.0269s/iter; left time: 654.9426s
Epoch: 7 cost time: 7.55709171295166
Epoch: 7, Steps: 261 | Train Loss: 0.5222777 Vali Loss: 0.2662385 Test Loss: 0.3703292
Validation loss decreased (0.266677 --> 0.266238).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5659106
	speed: 0.1228s/iter; left time: 2967.6560s
	iters: 200, epoch: 8 | loss: 0.3845296
	speed: 0.0235s/iter; left time: 566.7430s
Epoch: 8 cost time: 7.541488409042358
Epoch: 8, Steps: 261 | Train Loss: 0.5215918 Vali Loss: 0.2655632 Test Loss: 0.3699831
Validation loss decreased (0.266238 --> 0.265563).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4794304
	speed: 0.1705s/iter; left time: 4076.6012s
	iters: 200, epoch: 9 | loss: 0.4313967
	speed: 0.0413s/iter; left time: 983.8951s
Epoch: 9 cost time: 12.261260271072388
Epoch: 9, Steps: 261 | Train Loss: 0.5208913 Vali Loss: 0.2655834 Test Loss: 0.3696195
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5410460
	speed: 0.2051s/iter; left time: 4850.4666s
	iters: 200, epoch: 10 | loss: 0.3156110
	speed: 0.0417s/iter; left time: 982.7989s
Epoch: 10 cost time: 12.931149005889893
Epoch: 10, Steps: 261 | Train Loss: 0.5196087 Vali Loss: 0.2653300 Test Loss: 0.3694074
Validation loss decreased (0.265563 --> 0.265330).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5378851
	speed: 0.1712s/iter; left time: 4003.8875s
	iters: 200, epoch: 11 | loss: 0.6089410
	speed: 0.0381s/iter; left time: 886.6389s
Epoch: 11 cost time: 8.074567556381226
Epoch: 11, Steps: 261 | Train Loss: 0.5194585 Vali Loss: 0.2650078 Test Loss: 0.3692375
Validation loss decreased (0.265330 --> 0.265008).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4306518
	speed: 0.1557s/iter; left time: 3601.3611s
	iters: 200, epoch: 12 | loss: 0.5524315
	speed: 0.0248s/iter; left time: 570.6190s
Epoch: 12 cost time: 8.296181917190552
Epoch: 12, Steps: 261 | Train Loss: 0.5193565 Vali Loss: 0.2648202 Test Loss: 0.3691285
Validation loss decreased (0.265008 --> 0.264820).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3507363
	speed: 0.1912s/iter; left time: 4372.7738s
	iters: 200, epoch: 13 | loss: 0.5784461
	speed: 0.0460s/iter; left time: 1047.8628s
Epoch: 13 cost time: 12.83162784576416
Epoch: 13, Steps: 261 | Train Loss: 0.5193957 Vali Loss: 0.2645907 Test Loss: 0.3690097
Validation loss decreased (0.264820 --> 0.264591).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4328030
	speed: 0.1596s/iter; left time: 3608.9814s
	iters: 200, epoch: 14 | loss: 0.3995596
	speed: 0.0359s/iter; left time: 808.1347s
Epoch: 14 cost time: 11.796935796737671
Epoch: 14, Steps: 261 | Train Loss: 0.5184364 Vali Loss: 0.2644562 Test Loss: 0.3689166
Validation loss decreased (0.264591 --> 0.264456).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.7850592
	speed: 0.1999s/iter; left time: 4466.6058s
	iters: 200, epoch: 15 | loss: 0.4567387
	speed: 0.0385s/iter; left time: 857.0793s
Epoch: 15 cost time: 12.616395711898804
Epoch: 15, Steps: 261 | Train Loss: 0.5185673 Vali Loss: 0.2642488 Test Loss: 0.3688883
Validation loss decreased (0.264456 --> 0.264249).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5566521
	speed: 0.1980s/iter; left time: 4374.0224s
	iters: 200, epoch: 16 | loss: 0.4024526
	speed: 0.0491s/iter; left time: 1080.3696s
Epoch: 16 cost time: 13.336494207382202
Epoch: 16, Steps: 261 | Train Loss: 0.5179256 Vali Loss: 0.2643430 Test Loss: 0.3688529
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4785770
	speed: 0.1555s/iter; left time: 3393.4104s
	iters: 200, epoch: 17 | loss: 0.5453307
	speed: 0.0487s/iter; left time: 1058.6323s
Epoch: 17 cost time: 10.645339965820312
Epoch: 17, Steps: 261 | Train Loss: 0.5176574 Vali Loss: 0.2638944 Test Loss: 0.3688064
Validation loss decreased (0.264249 --> 0.263894).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4482662
	speed: 0.1622s/iter; left time: 3497.3245s
	iters: 200, epoch: 18 | loss: 0.6363896
	speed: 0.0567s/iter; left time: 1216.3694s
Epoch: 18 cost time: 13.596507549285889
Epoch: 18, Steps: 261 | Train Loss: 0.5178688 Vali Loss: 0.2640745 Test Loss: 0.3687567
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.6614159
	speed: 0.1833s/iter; left time: 3905.6928s
	iters: 200, epoch: 19 | loss: 0.5096282
	speed: 0.0428s/iter; left time: 908.4518s
Epoch: 19 cost time: 11.691988229751587
Epoch: 19, Steps: 261 | Train Loss: 0.5178745 Vali Loss: 0.2642468 Test Loss: 0.3687362
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5818000
	speed: 0.2009s/iter; left time: 4227.2135s
	iters: 200, epoch: 20 | loss: 0.4649132
	speed: 0.0249s/iter; left time: 521.1645s
Epoch: 20 cost time: 8.890528917312622
Epoch: 20, Steps: 261 | Train Loss: 0.5175513 Vali Loss: 0.2640282 Test Loss: 0.3686788
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4009106
	speed: 0.1494s/iter; left time: 3104.2551s
	iters: 200, epoch: 21 | loss: 0.4957277
	speed: 0.0348s/iter; left time: 720.0436s
Epoch: 21 cost time: 9.657557964324951
Epoch: 21, Steps: 261 | Train Loss: 0.5176344 Vali Loss: 0.2639428 Test Loss: 0.3686632
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5878094
	speed: 0.1744s/iter; left time: 3577.9630s
	iters: 200, epoch: 22 | loss: 0.5018248
	speed: 0.0453s/iter; left time: 924.2284s
Epoch: 22 cost time: 11.134033918380737
Epoch: 22, Steps: 261 | Train Loss: 0.5174040 Vali Loss: 0.2639249 Test Loss: 0.3686250
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3502092
	speed: 0.1793s/iter; left time: 3632.9608s
	iters: 200, epoch: 23 | loss: 0.4604261
	speed: 0.0555s/iter; left time: 1117.9571s
Epoch: 23 cost time: 11.703266382217407
Epoch: 23, Steps: 261 | Train Loss: 0.5173651 Vali Loss: 0.2636831 Test Loss: 0.3686502
Validation loss decreased (0.263894 --> 0.263683).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3843412
	speed: 0.1495s/iter; left time: 2989.2643s
	iters: 200, epoch: 24 | loss: 0.5516218
	speed: 0.0320s/iter; left time: 636.3278s
Epoch: 24 cost time: 9.944201707839966
Epoch: 24, Steps: 261 | Train Loss: 0.5170717 Vali Loss: 0.2640337 Test Loss: 0.3686301
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5943502
	speed: 0.2164s/iter; left time: 4270.1209s
	iters: 200, epoch: 25 | loss: 0.4492719
	speed: 0.0655s/iter; left time: 1286.6416s
Epoch: 25 cost time: 15.565800666809082
Epoch: 25, Steps: 261 | Train Loss: 0.5170132 Vali Loss: 0.2637543 Test Loss: 0.3686149
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4827230
	speed: 0.2212s/iter; left time: 4308.3441s
	iters: 200, epoch: 26 | loss: 0.5023953
	speed: 0.0475s/iter; left time: 920.7065s
Epoch: 26 cost time: 12.721786975860596
Epoch: 26, Steps: 261 | Train Loss: 0.5167611 Vali Loss: 0.2636740 Test Loss: 0.3686069
Validation loss decreased (0.263683 --> 0.263674).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5539216
	speed: 0.1684s/iter; left time: 3235.6773s
	iters: 200, epoch: 27 | loss: 0.4336553
	speed: 0.0363s/iter; left time: 694.5263s
Epoch: 27 cost time: 9.383773803710938
Epoch: 27, Steps: 261 | Train Loss: 0.5172524 Vali Loss: 0.2637943 Test Loss: 0.3686064
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.7380884
	speed: 0.1242s/iter; left time: 2353.1785s
	iters: 200, epoch: 28 | loss: 0.4847969
	speed: 0.0297s/iter; left time: 559.8984s
Epoch: 28 cost time: 7.4743897914886475
Epoch: 28, Steps: 261 | Train Loss: 0.5167976 Vali Loss: 0.2637337 Test Loss: 0.3685918
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.7173260
	speed: 0.1722s/iter; left time: 3219.6287s
	iters: 200, epoch: 29 | loss: 0.3041465
	speed: 0.0359s/iter; left time: 667.5339s
Epoch: 29 cost time: 12.651113271713257
Epoch: 29, Steps: 261 | Train Loss: 0.5171896 Vali Loss: 0.2634203 Test Loss: 0.3685858
Validation loss decreased (0.263674 --> 0.263420).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.6135205
	speed: 0.2071s/iter; left time: 3818.0144s
	iters: 200, epoch: 30 | loss: 0.3347667
	speed: 0.0377s/iter; left time: 691.9837s
Epoch: 30 cost time: 12.445018529891968
Epoch: 30, Steps: 261 | Train Loss: 0.5168475 Vali Loss: 0.2637314 Test Loss: 0.3685796
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.5192276
	speed: 0.2017s/iter; left time: 3665.5314s
	iters: 200, epoch: 31 | loss: 0.5489969
	speed: 0.0366s/iter; left time: 661.0509s
Epoch: 31 cost time: 9.844795942306519
Epoch: 31, Steps: 261 | Train Loss: 0.5174103 Vali Loss: 0.2636976 Test Loss: 0.3685593
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3427834
	speed: 0.1337s/iter; left time: 2393.9351s
	iters: 200, epoch: 32 | loss: 0.6438240
	speed: 0.0264s/iter; left time: 469.7258s
Epoch: 32 cost time: 7.758469820022583
Epoch: 32, Steps: 261 | Train Loss: 0.5170549 Vali Loss: 0.2636454 Test Loss: 0.3685604
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.6504673
	speed: 0.1348s/iter; left time: 2379.7265s
	iters: 200, epoch: 33 | loss: 0.6007513
	speed: 0.0271s/iter; left time: 474.9169s
Epoch: 33 cost time: 7.383814096450806
Epoch: 33, Steps: 261 | Train Loss: 0.5170000 Vali Loss: 0.2639081 Test Loss: 0.3685372
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.6786898
	speed: 0.1847s/iter; left time: 3211.9448s
	iters: 200, epoch: 34 | loss: 0.5198227
	speed: 0.0486s/iter; left time: 840.8926s
Epoch: 34 cost time: 12.869520425796509
Epoch: 34, Steps: 261 | Train Loss: 0.5163652 Vali Loss: 0.2637374 Test Loss: 0.3685379
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4222779
	speed: 0.1791s/iter; left time: 3066.7729s
	iters: 200, epoch: 35 | loss: 0.6604726
	speed: 0.0350s/iter; left time: 595.4820s
Epoch: 35 cost time: 8.587442398071289
Epoch: 35, Steps: 261 | Train Loss: 0.5167837 Vali Loss: 0.2634965 Test Loss: 0.3685254
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4551817
	speed: 0.1395s/iter; left time: 2352.1140s
	iters: 200, epoch: 36 | loss: 0.5438107
	speed: 0.0381s/iter; left time: 639.0667s
Epoch: 36 cost time: 10.655649662017822
Epoch: 36, Steps: 261 | Train Loss: 0.5167038 Vali Loss: 0.2634935 Test Loss: 0.3685252
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.6196842
	speed: 0.2975s/iter; left time: 4940.4422s
	iters: 200, epoch: 37 | loss: 0.4660472
	speed: 0.0337s/iter; left time: 555.8308s
Epoch: 37 cost time: 13.543938398361206
Epoch: 37, Steps: 261 | Train Loss: 0.5168579 Vali Loss: 0.2633832 Test Loss: 0.3685202
Validation loss decreased (0.263420 --> 0.263383).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4186278
	speed: 0.2108s/iter; left time: 3445.4541s
	iters: 200, epoch: 38 | loss: 0.4507322
	speed: 0.0523s/iter; left time: 849.2668s
Epoch: 38 cost time: 11.47629165649414
Epoch: 38, Steps: 261 | Train Loss: 0.5168228 Vali Loss: 0.2634345 Test Loss: 0.3685252
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4829944
	speed: 0.1555s/iter; left time: 2500.3798s
	iters: 200, epoch: 39 | loss: 0.5739028
	speed: 0.0338s/iter; left time: 540.4631s
Epoch: 39 cost time: 10.857378482818604
Epoch: 39, Steps: 261 | Train Loss: 0.5168585 Vali Loss: 0.2634958 Test Loss: 0.3685281
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4978516
	speed: 0.1761s/iter; left time: 2786.5816s
	iters: 200, epoch: 40 | loss: 0.4702690
	speed: 0.0279s/iter; left time: 438.7602s
Epoch: 40 cost time: 8.818550825119019
Epoch: 40, Steps: 261 | Train Loss: 0.5170214 Vali Loss: 0.2636144 Test Loss: 0.3685394
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4606727
	speed: 0.1726s/iter; left time: 2685.9230s
	iters: 200, epoch: 41 | loss: 0.5645136
	speed: 0.0354s/iter; left time: 547.5825s
Epoch: 41 cost time: 10.303492307662964
Epoch: 41, Steps: 261 | Train Loss: 0.5165988 Vali Loss: 0.2634968 Test Loss: 0.3685264
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.5030186
	speed: 0.1482s/iter; left time: 2266.8511s
	iters: 200, epoch: 42 | loss: 0.5781205
	speed: 0.0240s/iter; left time: 364.6633s
Epoch: 42 cost time: 7.042316436767578
Epoch: 42, Steps: 261 | Train Loss: 0.5166952 Vali Loss: 0.2636045 Test Loss: 0.3685259
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.6009554
	speed: 0.1276s/iter; left time: 1919.4932s
	iters: 200, epoch: 43 | loss: 0.4896104
	speed: 0.0426s/iter; left time: 636.2097s
Epoch: 43 cost time: 9.971765995025635
Epoch: 43, Steps: 261 | Train Loss: 0.5162509 Vali Loss: 0.2638458 Test Loss: 0.3685138
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.5916526
	speed: 0.1351s/iter; left time: 1996.9665s
	iters: 200, epoch: 44 | loss: 0.6046649
	speed: 0.0348s/iter; left time: 510.6342s
Epoch: 44 cost time: 8.550514936447144
Epoch: 44, Steps: 261 | Train Loss: 0.5167117 Vali Loss: 0.2636401 Test Loss: 0.3685114
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4338740
	speed: 0.1497s/iter; left time: 2173.5935s
	iters: 200, epoch: 45 | loss: 0.4114148
	speed: 0.0267s/iter; left time: 385.6092s
Epoch: 45 cost time: 8.265062093734741
Epoch: 45, Steps: 261 | Train Loss: 0.5168034 Vali Loss: 0.2636821 Test Loss: 0.3685040
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.7359891
	speed: 0.1771s/iter; left time: 2524.3829s
	iters: 200, epoch: 46 | loss: 0.5058503
	speed: 0.0312s/iter; left time: 442.2460s
Epoch: 46 cost time: 10.907251119613647
Epoch: 46, Steps: 261 | Train Loss: 0.5163917 Vali Loss: 0.2635746 Test Loss: 0.3685044
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.5934336
	speed: 0.1707s/iter; left time: 2388.8968s
	iters: 200, epoch: 47 | loss: 0.6791971
	speed: 0.0260s/iter; left time: 360.6164s
Epoch: 47 cost time: 6.859510183334351
Epoch: 47, Steps: 261 | Train Loss: 0.5169251 Vali Loss: 0.2638614 Test Loss: 0.3685051
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4914928
	speed: 0.1145s/iter; left time: 1573.1379s
	iters: 200, epoch: 48 | loss: 0.4804699
	speed: 0.0348s/iter; left time: 473.8756s
Epoch: 48 cost time: 9.419112205505371
Epoch: 48, Steps: 261 | Train Loss: 0.5161834 Vali Loss: 0.2636282 Test Loss: 0.3685055
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.5665833
	speed: 0.1502s/iter; left time: 2024.0305s
	iters: 200, epoch: 49 | loss: 0.4586428
	speed: 0.0305s/iter; left time: 407.3333s
Epoch: 49 cost time: 10.099359035491943
Epoch: 49, Steps: 261 | Train Loss: 0.5169672 Vali Loss: 0.2636207 Test Loss: 0.3684989
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.6789425
	speed: 0.2383s/iter; left time: 3149.0762s
	iters: 200, epoch: 50 | loss: 0.4585165
	speed: 0.0400s/iter; left time: 524.2388s
Epoch: 50 cost time: 11.65157437324524
Epoch: 50, Steps: 261 | Train Loss: 0.5162678 Vali Loss: 0.2635283 Test Loss: 0.3685009
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.6070929
	speed: 0.1986s/iter; left time: 2572.0995s
	iters: 200, epoch: 51 | loss: 0.5293883
	speed: 0.0470s/iter; left time: 604.3196s
Epoch: 51 cost time: 12.036986351013184
Epoch: 51, Steps: 261 | Train Loss: 0.5160015 Vali Loss: 0.2631387 Test Loss: 0.3685030
Validation loss decreased (0.263383 --> 0.263139).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4888287
	speed: 0.1683s/iter; left time: 2136.3386s
	iters: 200, epoch: 52 | loss: 0.3688978
	speed: 0.0237s/iter; left time: 298.8900s
Epoch: 52 cost time: 8.905121088027954
Epoch: 52, Steps: 261 | Train Loss: 0.5164442 Vali Loss: 0.2636111 Test Loss: 0.3684993
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4167862
	speed: 0.1502s/iter; left time: 1866.9043s
	iters: 200, epoch: 53 | loss: 0.6729538
	speed: 0.0388s/iter; left time: 478.6062s
Epoch: 53 cost time: 9.463212013244629
Epoch: 53, Steps: 261 | Train Loss: 0.5160327 Vali Loss: 0.2635353 Test Loss: 0.3684934
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4561667
	speed: 0.1528s/iter; left time: 1859.2376s
	iters: 200, epoch: 54 | loss: 0.4854425
	speed: 0.0331s/iter; left time: 399.9554s
Epoch: 54 cost time: 9.101373195648193
Epoch: 54, Steps: 261 | Train Loss: 0.5168196 Vali Loss: 0.2634056 Test Loss: 0.3685019
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.5459849
	speed: 0.1970s/iter; left time: 2345.8615s
	iters: 200, epoch: 55 | loss: 0.4308616
	speed: 0.0414s/iter; left time: 488.4952s
Epoch: 55 cost time: 11.205507040023804
Epoch: 55, Steps: 261 | Train Loss: 0.5160434 Vali Loss: 0.2635630 Test Loss: 0.3685008
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.5046825
	speed: 0.1861s/iter; left time: 2167.0998s
	iters: 200, epoch: 56 | loss: 0.6192260
	speed: 0.0352s/iter; left time: 405.9935s
Epoch: 56 cost time: 11.558723211288452
Epoch: 56, Steps: 261 | Train Loss: 0.5168520 Vali Loss: 0.2635900 Test Loss: 0.3685004
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.5193422
	speed: 0.1769s/iter; left time: 2014.4148s
	iters: 200, epoch: 57 | loss: 0.4765970
	speed: 0.0351s/iter; left time: 395.7101s
Epoch: 57 cost time: 8.996125221252441
Epoch: 57, Steps: 261 | Train Loss: 0.5167452 Vali Loss: 0.2637518 Test Loss: 0.3685007
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.4915441
	speed: 0.1625s/iter; left time: 1807.9878s
	iters: 200, epoch: 58 | loss: 0.6698213
	speed: 0.0377s/iter; left time: 416.0287s
Epoch: 58 cost time: 10.193963766098022
Epoch: 58, Steps: 261 | Train Loss: 0.5166356 Vali Loss: 0.2636163 Test Loss: 0.3685001
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.3839610
	speed: 0.1367s/iter; left time: 1484.6018s
	iters: 200, epoch: 59 | loss: 0.5459922
	speed: 0.0344s/iter; left time: 370.6301s
Epoch: 59 cost time: 8.361923217773438
Epoch: 59, Steps: 261 | Train Loss: 0.5164103 Vali Loss: 0.2634357 Test Loss: 0.3684962
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.5742713
	speed: 0.1675s/iter; left time: 1775.9362s
	iters: 200, epoch: 60 | loss: 0.5027586
	speed: 0.0388s/iter; left time: 407.6555s
Epoch: 60 cost time: 10.511596441268921
Epoch: 60, Steps: 261 | Train Loss: 0.5168913 Vali Loss: 0.2632875 Test Loss: 0.3685035
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.6673461
	speed: 0.2253s/iter; left time: 2329.6944s
	iters: 200, epoch: 61 | loss: 0.3967565
	speed: 0.0624s/iter; left time: 638.6575s
Epoch: 61 cost time: 15.906059503555298
Epoch: 61, Steps: 261 | Train Loss: 0.5162520 Vali Loss: 0.2636389 Test Loss: 0.3684976
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.4972570
	speed: 0.2081s/iter; left time: 2097.6964s
	iters: 200, epoch: 62 | loss: 0.3874644
	speed: 0.0393s/iter; left time: 392.2143s
Epoch: 62 cost time: 11.095284223556519
Epoch: 62, Steps: 261 | Train Loss: 0.5162658 Vali Loss: 0.2634903 Test Loss: 0.3684983
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.4763506
	speed: 0.1655s/iter; left time: 1625.2203s
	iters: 200, epoch: 63 | loss: 0.5478186
	speed: 0.0305s/iter; left time: 296.0517s
Epoch: 63 cost time: 8.845451354980469
Epoch: 63, Steps: 261 | Train Loss: 0.5167207 Vali Loss: 0.2635602 Test Loss: 0.3684992
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.6294124
	speed: 0.1444s/iter; left time: 1380.3342s
	iters: 200, epoch: 64 | loss: 0.4999491
	speed: 0.0305s/iter; left time: 288.4711s
Epoch: 64 cost time: 8.202097177505493
Epoch: 64, Steps: 261 | Train Loss: 0.5167290 Vali Loss: 0.2635158 Test Loss: 0.3684979
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.4933605
	speed: 0.1476s/iter; left time: 1371.8072s
	iters: 200, epoch: 65 | loss: 0.6540114
	speed: 0.0324s/iter; left time: 298.0564s
Epoch: 65 cost time: 9.56457781791687
Epoch: 65, Steps: 261 | Train Loss: 0.5164916 Vali Loss: 0.2636769 Test Loss: 0.3684984
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.6083370
	speed: 0.2054s/iter; left time: 1856.3174s
	iters: 200, epoch: 66 | loss: 0.4397798
	speed: 0.0380s/iter; left time: 339.5810s
Epoch: 66 cost time: 13.364444971084595
Epoch: 66, Steps: 261 | Train Loss: 0.5164122 Vali Loss: 0.2633606 Test Loss: 0.3684976
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.6013265
	speed: 0.1932s/iter; left time: 1695.6802s
	iters: 200, epoch: 67 | loss: 0.5451235
	speed: 0.0403s/iter; left time: 349.3953s
Epoch: 67 cost time: 11.467171430587769
Epoch: 67, Steps: 261 | Train Loss: 0.5166884 Vali Loss: 0.2636680 Test Loss: 0.3684989
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.4228313
	speed: 0.1509s/iter; left time: 1285.1867s
	iters: 200, epoch: 68 | loss: 0.3683181
	speed: 0.0307s/iter; left time: 258.1336s
Epoch: 68 cost time: 9.270243406295776
Epoch: 68, Steps: 261 | Train Loss: 0.5168483 Vali Loss: 0.2633632 Test Loss: 0.3684967
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.5275969
	speed: 0.1385s/iter; left time: 1142.7332s
	iters: 200, epoch: 69 | loss: 0.6252866
	speed: 0.0326s/iter; left time: 265.5642s
Epoch: 69 cost time: 8.843342065811157
Epoch: 69, Steps: 261 | Train Loss: 0.5166640 Vali Loss: 0.2635976 Test Loss: 0.3685004
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.6519334
	speed: 0.1984s/iter; left time: 1585.5483s
	iters: 200, epoch: 70 | loss: 0.3975681
	speed: 0.0503s/iter; left time: 396.6593s
Epoch: 70 cost time: 13.901262283325195
Epoch: 70, Steps: 261 | Train Loss: 0.5163428 Vali Loss: 0.2637608 Test Loss: 0.3684978
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.4538240
	speed: 0.1858s/iter; left time: 1436.5768s
	iters: 200, epoch: 71 | loss: 0.5084921
	speed: 0.0343s/iter; left time: 261.4228s
Epoch: 71 cost time: 10.206540822982788
Epoch: 71, Steps: 261 | Train Loss: 0.5167344 Vali Loss: 0.2637547 Test Loss: 0.3684985
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_360_720_FITS_ETTm2_ftM_sl360_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3658098876476288, mae:0.3819781541824341, rse:0.4861520826816559, corr:[0.543969   0.54463136 0.53952336 0.53744066 0.5384298  0.5393915
 0.5386341  0.5369415  0.53589356 0.5360142  0.5365442  0.53656566
 0.5358568  0.53499705 0.5346352  0.53476816 0.534914   0.5345182
 0.5335739  0.5325628  0.5319733  0.5318741  0.53185904 0.53162557
 0.53110445 0.5305249  0.53018606 0.530094   0.52998316 0.52957
 0.52890646 0.52830845 0.527959   0.5278199  0.5276428  0.52717644
 0.52637047 0.52545136 0.524705   0.52422446 0.5238962  0.5235635
 0.52307945 0.5224463  0.5217768  0.5211172  0.5204977  0.51983243
 0.51903    0.5181425  0.51725644 0.5165337  0.51590246 0.5152342
 0.5144979  0.5137548  0.51307243 0.51254    0.5121722  0.51187253
 0.511556   0.5112018  0.51088136 0.5106131  0.5104107  0.510268
 0.5100741  0.5098182  0.50955737 0.5093592  0.50920653 0.50905114
 0.508841   0.50857925 0.5082579  0.5078753  0.5074852  0.50706285
 0.50661916 0.50612706 0.5056306  0.5050952  0.5045571  0.50398415
 0.5034357  0.50293434 0.5024307  0.5019527  0.5014752  0.5009816
 0.50047094 0.49992374 0.49926665 0.49844146 0.4973611  0.49595943
 0.4942762  0.49255705 0.4909769  0.48956594 0.48827845 0.48709345
 0.4859463  0.4847456  0.48346478 0.48222384 0.4811031  0.4801206
 0.47923455 0.4783726  0.47754192 0.4767161  0.47592628 0.4751613
 0.47441092 0.47367027 0.47293377 0.47218212 0.47147673 0.4708024
 0.47016424 0.46952137 0.46891233 0.4682966  0.46765223 0.46698
 0.46628508 0.46550632 0.46461993 0.4636912  0.46283272 0.4620798
 0.46140692 0.46076292 0.4600867  0.4593935  0.45865068 0.45797047
 0.45734492 0.4567377  0.45607576 0.45531565 0.45451495 0.45370156
 0.45291907 0.45214272 0.4513576  0.4505259  0.44965398 0.44880342
 0.44810116 0.44755486 0.44699287 0.44631255 0.44554752 0.44483492
 0.44427916 0.44390875 0.44359943 0.44330013 0.44288152 0.4423624
 0.44188014 0.44151706 0.44132453 0.44119567 0.44094637 0.44064304
 0.44031528 0.4400623  0.4398731  0.43965098 0.43927854 0.43875372
 0.4381493  0.4375826  0.43709645 0.43668282 0.43631738 0.43595633
 0.4355378  0.43503836 0.43452471 0.4340152  0.43356097 0.43318674
 0.4328671  0.43249965 0.43195426 0.4310798  0.42983568 0.4281914
 0.4263412  0.42462853 0.42306095 0.42154518 0.4200733  0.4185708
 0.41704476 0.41550437 0.41400596 0.41262084 0.4113813  0.41025764
 0.40917197 0.40808046 0.4069278  0.40579036 0.40473568 0.40383714
 0.40299273 0.40211678 0.40122607 0.40037912 0.3995363  0.3987147
 0.3978832  0.39686003 0.39585894 0.39494038 0.39402813 0.3930288
 0.3919901  0.390968   0.39013493 0.38936827 0.3885853  0.38764656
 0.38656098 0.38543326 0.38436252 0.38344172 0.3826934  0.3820924
 0.38148966 0.38083395 0.38022768 0.37969342 0.37925842 0.37886778
 0.3783566  0.37766632 0.3768864  0.37634894 0.37598023 0.37580326
 0.37579346 0.37577015 0.37560797 0.37531832 0.3750197  0.3748649
 0.3747642  0.37478736 0.37476832 0.37467486 0.3744802  0.37420723
 0.37397808 0.37381288 0.37375852 0.37377146 0.37370172 0.37355825
 0.37335685 0.37309414 0.37278667 0.37240395 0.37209445 0.37194812
 0.37185305 0.37165195 0.3712733  0.37085956 0.37043652 0.37011805
 0.36983436 0.36951667 0.36912006 0.36866325 0.3682078  0.36791998
 0.36780834 0.36780903 0.36775458 0.36741936 0.36658928 0.36526918
 0.36366904 0.36221397 0.36103973 0.36012888 0.3594355  0.358872
 0.35828903 0.3576032  0.35682446 0.35593912 0.35514605 0.35459724
 0.35426256 0.35393512 0.35354593 0.35303858 0.35243517 0.35186908
 0.35154572 0.35138446 0.35126412 0.35118395 0.35111994 0.3510937
 0.35103267 0.3509185  0.35068968 0.35036922 0.34997657 0.34952116
 0.34906694 0.34863853 0.3482177  0.3478387  0.3475228  0.34725964
 0.34698525 0.34672418 0.3464506  0.34611228 0.3457466  0.34553853
 0.34552437 0.34558678 0.3455655  0.34527975 0.34473872 0.344197
 0.34373644 0.3433939  0.34311572 0.34285763 0.34253177 0.34218523
 0.34190333 0.3418361  0.3418972  0.3419102  0.3418334  0.34169394
 0.34164366 0.34175175 0.34192604 0.3420484  0.3420396  0.3418386
 0.3415319  0.34132427 0.34122917 0.3411744  0.34103248 0.34068617
 0.34024528 0.3399085  0.33971187 0.33965254 0.3395744  0.339435
 0.33914477 0.33882144 0.3385724  0.33843476 0.338442   0.33840957
 0.33829492 0.3381321  0.33799097 0.3378864  0.3378246  0.33781272
 0.3377938  0.33771855 0.33753115 0.3371319  0.3365272  0.33564034
 0.33450186 0.33329716 0.3321636  0.33115497 0.3302389  0.3294391
 0.32865468 0.3277944  0.32690647 0.32599673 0.32512105 0.32432377
 0.32364768 0.3231001  0.3225253  0.32195288 0.3214117  0.3208605
 0.3203281  0.3197547  0.31921238 0.31880236 0.3186057  0.31863433
 0.31871113 0.3186166  0.31833    0.3179689  0.31762448 0.3173543
 0.3171484  0.31699198 0.31682587 0.31662259 0.31639862 0.3161976
 0.31601983 0.31588712 0.31575125 0.31561366 0.31552312 0.31549
 0.3154608  0.31542763 0.31534326 0.31516355 0.31494752 0.31469217
 0.3143539  0.31392983 0.31341457 0.31296524 0.31261963 0.3123801
 0.31213805 0.31188825 0.31148985 0.31108987 0.31075582 0.31058705
 0.31052622 0.31048414 0.3103703  0.31008023 0.30969524 0.30934545
 0.3092076  0.30921596 0.3091737  0.30899042 0.30866778 0.30832282
 0.3081075  0.30803618 0.30792806 0.30768096 0.3073052  0.30681914
 0.30639538 0.30611444 0.30595887 0.30576113 0.3053388  0.3047798
 0.30418903 0.30370173 0.3032967  0.30290607 0.3024796  0.30197573
 0.3013924  0.30073276 0.30000263 0.29911968 0.29798013 0.2965494
 0.29501042 0.2936012  0.2924197  0.291467   0.29062182 0.28979632
 0.2889416  0.28805614 0.28715077 0.286293   0.28550944 0.2848356
 0.28431654 0.28388867 0.28339595 0.2827094  0.2818691  0.28100935
 0.28034183 0.27992016 0.2796252  0.2792937  0.27884388 0.27837005
 0.27798617 0.27768385 0.27748474 0.27729702 0.2770415  0.27660474
 0.2760625  0.27555215 0.27514374 0.2747919  0.27442545 0.2739838
 0.27355832 0.27319697 0.27293518 0.27272725 0.27243787 0.27203968
 0.27154568 0.27112454 0.27083632 0.2706704  0.2705235  0.27021027
 0.26967838 0.2690567  0.26850432 0.26818097 0.26809648 0.26812917
 0.26808533 0.2678975  0.26768437 0.26750365 0.26740903 0.2674141
 0.26737258 0.2672607  0.26707137 0.2668707  0.2666675  0.2664854
 0.2663083  0.26615626 0.26602322 0.2659383  0.26583835 0.26578045
 0.26568985 0.26550364 0.2652007  0.26476496 0.2643217  0.26397386
 0.26374328 0.26360416 0.26347077 0.263275   0.26297125 0.26263714
 0.2623929  0.2621956  0.2619839  0.2616344  0.2612391  0.26087573
 0.2605896  0.26032305 0.2599105  0.2591701  0.25798368 0.25629553
 0.2543102  0.25246277 0.25097844 0.24985997 0.24896193 0.24813008
 0.2471323  0.24601565 0.24484552 0.24374996 0.24284707 0.24214418
 0.24144194 0.24064107 0.2398039  0.23902053 0.23825704 0.23749131
 0.2367293  0.23596843 0.23527555 0.2347511  0.23453267 0.23453778
 0.23454636 0.23434298 0.23389596 0.23336574 0.23290873 0.23254476
 0.23217943 0.2317352  0.23118615 0.23060575 0.23015624 0.22983967
 0.22960557 0.2292281  0.22867982 0.2281294  0.22774047 0.22765507
 0.22769876 0.22772206 0.22752915 0.22719479 0.2268869  0.22671445
 0.2266389  0.22647879 0.22620395 0.22590761 0.2256711  0.22549815
 0.22549778 0.22565576 0.2257643  0.2256172  0.22533345 0.22505909
 0.22511347 0.22532667 0.22545865 0.22541776 0.22517778 0.22489484
 0.22464596 0.22456206 0.22455168 0.22442943 0.22431879 0.22405712
 0.22382703 0.22369693 0.22360742 0.22353023 0.22341388 0.22325328
 0.22326885 0.22350183 0.22378919 0.22409067 0.22430526 0.22438271
 0.22454289 0.22491786 0.22543454 0.2258612  0.22621997 0.2265282
 0.22679782 0.22699064 0.22698058 0.22644976 0.22524919 0.2236156
 0.22191614 0.22052836 0.2194486  0.21845286 0.21742706 0.21652539
 0.21573506 0.21514949 0.2147821  0.214391   0.21381108 0.21317962
 0.2127428  0.21243212 0.21215297 0.21172625 0.21127057 0.21075296
 0.21046789 0.2103681  0.21023782 0.21006162 0.20975782 0.20945634
 0.20940039 0.2094378  0.2094011  0.20908377 0.20846558 0.2076237
 0.20696954 0.2064806  0.20604844 0.2055137  0.2046853  0.20393178
 0.20341277 0.20334825 0.20345765 0.20328905 0.20264544 0.20200856
 0.20202054 0.20268117 0.20311092 0.20278825 0.20163046 0.20217617]
