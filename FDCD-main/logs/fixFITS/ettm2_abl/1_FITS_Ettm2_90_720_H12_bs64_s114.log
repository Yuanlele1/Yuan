Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=22, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3902976.0
params:  4554.0
Trainable parameters:  4554
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5476914
	speed: 0.0434s/iter; left time: 1137.4164s
	iters: 200, epoch: 1 | loss: 0.5776674
	speed: 0.0267s/iter; left time: 696.1074s
Epoch: 1 cost time: 8.650297403335571
Epoch: 1, Steps: 263 | Train Loss: 0.7493235 Vali Loss: 0.3293732 Test Loss: 0.4602430
Validation loss decreased (inf --> 0.329373).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6488334
	speed: 0.1065s/iter; left time: 2762.0064s
	iters: 200, epoch: 2 | loss: 0.6254137
	speed: 0.0205s/iter; left time: 529.2803s
Epoch: 2 cost time: 6.931322336196899
Epoch: 2, Steps: 263 | Train Loss: 0.6146101 Vali Loss: 0.2959874 Test Loss: 0.4202321
Validation loss decreased (0.329373 --> 0.295987).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.7293620
	speed: 0.1227s/iter; left time: 3151.0003s
	iters: 200, epoch: 3 | loss: 0.4025346
	speed: 0.0202s/iter; left time: 516.8367s
Epoch: 3 cost time: 6.511738538742065
Epoch: 3, Steps: 263 | Train Loss: 0.5951029 Vali Loss: 0.2906006 Test Loss: 0.4138008
Validation loss decreased (0.295987 --> 0.290601).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4772209
	speed: 0.1197s/iter; left time: 3041.9470s
	iters: 200, epoch: 4 | loss: 0.7539147
	speed: 0.0267s/iter; left time: 676.1687s
Epoch: 4 cost time: 9.3816978931427
Epoch: 4, Steps: 263 | Train Loss: 0.5914681 Vali Loss: 0.2894517 Test Loss: 0.4122826
Validation loss decreased (0.290601 --> 0.289452).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.6896869
	speed: 0.1431s/iter; left time: 3599.7850s
	iters: 200, epoch: 5 | loss: 0.4752008
	speed: 0.0394s/iter; left time: 985.7847s
Epoch: 5 cost time: 8.932702779769897
Epoch: 5, Steps: 263 | Train Loss: 0.5901163 Vali Loss: 0.2889944 Test Loss: 0.4116572
Validation loss decreased (0.289452 --> 0.288994).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5691186
	speed: 0.1168s/iter; left time: 2907.3342s
	iters: 200, epoch: 6 | loss: 0.4294614
	speed: 0.0203s/iter; left time: 502.9200s
Epoch: 6 cost time: 6.243961572647095
Epoch: 6, Steps: 263 | Train Loss: 0.5897984 Vali Loss: 0.2890877 Test Loss: 0.4114250
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.7128131
	speed: 0.1181s/iter; left time: 2907.1384s
	iters: 200, epoch: 7 | loss: 0.4256421
	speed: 0.0222s/iter; left time: 545.3024s
Epoch: 7 cost time: 6.819629192352295
Epoch: 7, Steps: 263 | Train Loss: 0.5891676 Vali Loss: 0.2891018 Test Loss: 0.4112841
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4954608
	speed: 0.1201s/iter; left time: 2924.4925s
	iters: 200, epoch: 8 | loss: 0.8596171
	speed: 0.0219s/iter; left time: 532.0283s
Epoch: 8 cost time: 6.1865150928497314
Epoch: 8, Steps: 263 | Train Loss: 0.5886383 Vali Loss: 0.2889258 Test Loss: 0.4110945
Validation loss decreased (0.288994 --> 0.288926).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4944752
	speed: 0.1084s/iter; left time: 2611.0505s
	iters: 200, epoch: 9 | loss: 0.3443226
	speed: 0.0211s/iter; left time: 505.9795s
Epoch: 9 cost time: 6.336354732513428
Epoch: 9, Steps: 263 | Train Loss: 0.5882710 Vali Loss: 0.2889037 Test Loss: 0.4110179
Validation loss decreased (0.288926 --> 0.288904).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.5468885
	speed: 0.1129s/iter; left time: 2690.9024s
	iters: 200, epoch: 10 | loss: 0.7144715
	speed: 0.0345s/iter; left time: 818.2072s
Epoch: 10 cost time: 8.470845699310303
Epoch: 10, Steps: 263 | Train Loss: 0.5881254 Vali Loss: 0.2890018 Test Loss: 0.4109951
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5189735
	speed: 0.1359s/iter; left time: 3203.6620s
	iters: 200, epoch: 11 | loss: 0.5318010
	speed: 0.0406s/iter; left time: 952.3401s
Epoch: 11 cost time: 9.031898498535156
Epoch: 11, Steps: 263 | Train Loss: 0.5884142 Vali Loss: 0.2889180 Test Loss: 0.4110785
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.6659226
	speed: 0.1222s/iter; left time: 2848.2392s
	iters: 200, epoch: 12 | loss: 0.5732667
	speed: 0.0301s/iter; left time: 699.3545s
Epoch: 12 cost time: 7.864944219589233
Epoch: 12, Steps: 263 | Train Loss: 0.5875527 Vali Loss: 0.2891884 Test Loss: 0.4110016
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6271878
	speed: 0.1161s/iter; left time: 2675.6843s
	iters: 200, epoch: 13 | loss: 0.4829741
	speed: 0.0282s/iter; left time: 647.2477s
Epoch: 13 cost time: 7.998746156692505
Epoch: 13, Steps: 263 | Train Loss: 0.5876233 Vali Loss: 0.2891171 Test Loss: 0.4109983
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.6305303
	speed: 0.1200s/iter; left time: 2734.8592s
	iters: 200, epoch: 14 | loss: 0.6534137
	speed: 0.0361s/iter; left time: 818.2021s
Epoch: 14 cost time: 8.474276065826416
Epoch: 14, Steps: 263 | Train Loss: 0.5875463 Vali Loss: 0.2889419 Test Loss: 0.4109774
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.8710093
	speed: 0.1224s/iter; left time: 2757.1834s
	iters: 200, epoch: 15 | loss: 0.6094579
	speed: 0.0226s/iter; left time: 507.6651s
Epoch: 15 cost time: 7.61282205581665
Epoch: 15, Steps: 263 | Train Loss: 0.5872358 Vali Loss: 0.2892385 Test Loss: 0.4110523
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.6466739
	speed: 0.1174s/iter; left time: 2612.7107s
	iters: 200, epoch: 16 | loss: 0.5357632
	speed: 0.0298s/iter; left time: 660.8670s
Epoch: 16 cost time: 7.313801288604736
Epoch: 16, Steps: 263 | Train Loss: 0.5874461 Vali Loss: 0.2889113 Test Loss: 0.4110271
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5216664
	speed: 0.1325s/iter; left time: 2914.1722s
	iters: 200, epoch: 17 | loss: 0.5845256
	speed: 0.0234s/iter; left time: 513.3324s
Epoch: 17 cost time: 6.7880823612213135
Epoch: 17, Steps: 263 | Train Loss: 0.5874394 Vali Loss: 0.2893557 Test Loss: 0.4110544
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4472032
	speed: 0.1160s/iter; left time: 2520.5421s
	iters: 200, epoch: 18 | loss: 0.4413017
	speed: 0.0409s/iter; left time: 883.9865s
Epoch: 18 cost time: 9.346952676773071
Epoch: 18, Steps: 263 | Train Loss: 0.5870080 Vali Loss: 0.2894818 Test Loss: 0.4110008
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5711635
	speed: 0.1204s/iter; left time: 2584.8597s
	iters: 200, epoch: 19 | loss: 0.6365045
	speed: 0.0206s/iter; left time: 440.0553s
Epoch: 19 cost time: 7.661927700042725
Epoch: 19, Steps: 263 | Train Loss: 0.5877474 Vali Loss: 0.2894024 Test Loss: 0.4110629
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5993189
	speed: 0.1144s/iter; left time: 2425.4878s
	iters: 200, epoch: 20 | loss: 0.5280117
	speed: 0.0211s/iter; left time: 445.6669s
Epoch: 20 cost time: 6.932909727096558
Epoch: 20, Steps: 263 | Train Loss: 0.5872765 Vali Loss: 0.2894286 Test Loss: 0.4110892
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5479047
	speed: 0.1464s/iter; left time: 3065.0494s
	iters: 200, epoch: 21 | loss: 0.5178722
	speed: 0.0306s/iter; left time: 637.5493s
Epoch: 21 cost time: 9.407764911651611
Epoch: 21, Steps: 263 | Train Loss: 0.5869612 Vali Loss: 0.2892019 Test Loss: 0.4110850
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3954686
	speed: 0.1132s/iter; left time: 2340.4697s
	iters: 200, epoch: 22 | loss: 0.3827430
	speed: 0.0261s/iter; left time: 537.7268s
Epoch: 22 cost time: 7.159682273864746
Epoch: 22, Steps: 263 | Train Loss: 0.5869606 Vali Loss: 0.2894208 Test Loss: 0.4110484
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5588205
	speed: 0.1175s/iter; left time: 2398.2594s
	iters: 200, epoch: 23 | loss: 0.4589342
	speed: 0.0217s/iter; left time: 440.6671s
Epoch: 23 cost time: 7.068669557571411
Epoch: 23, Steps: 263 | Train Loss: 0.5863540 Vali Loss: 0.2893513 Test Loss: 0.4110239
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5905970
	speed: 0.1476s/iter; left time: 2975.4081s
	iters: 200, epoch: 24 | loss: 0.7430333
	speed: 0.0280s/iter; left time: 561.5990s
Epoch: 24 cost time: 7.678964138031006
Epoch: 24, Steps: 263 | Train Loss: 0.5867078 Vali Loss: 0.2893577 Test Loss: 0.4110754
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4384777
	speed: 0.1091s/iter; left time: 2169.9214s
	iters: 200, epoch: 25 | loss: 0.6985416
	speed: 0.0280s/iter; left time: 554.6679s
Epoch: 25 cost time: 8.120400428771973
Epoch: 25, Steps: 263 | Train Loss: 0.5868129 Vali Loss: 0.2892519 Test Loss: 0.4110966
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.5740612
	speed: 0.1310s/iter; left time: 2570.6579s
	iters: 200, epoch: 26 | loss: 0.6789992
	speed: 0.0302s/iter; left time: 590.2794s
Epoch: 26 cost time: 7.672969579696655
Epoch: 26, Steps: 263 | Train Loss: 0.5864484 Vali Loss: 0.2891362 Test Loss: 0.4111122
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4553354
	speed: 0.1059s/iter; left time: 2049.6471s
	iters: 200, epoch: 27 | loss: 0.6508232
	speed: 0.0209s/iter; left time: 402.3451s
Epoch: 27 cost time: 6.242075204849243
Epoch: 27, Steps: 263 | Train Loss: 0.5871606 Vali Loss: 0.2892925 Test Loss: 0.4110928
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5089855
	speed: 0.1054s/iter; left time: 2014.0564s
	iters: 200, epoch: 28 | loss: 0.5048594
	speed: 0.0225s/iter; left time: 427.0928s
Epoch: 28 cost time: 6.46176552772522
Epoch: 28, Steps: 263 | Train Loss: 0.5869564 Vali Loss: 0.2896055 Test Loss: 0.4111026
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.6031836
	speed: 0.1296s/iter; left time: 2440.7257s
	iters: 200, epoch: 29 | loss: 0.6177109
	speed: 0.0237s/iter; left time: 444.2067s
Epoch: 29 cost time: 6.311219215393066
Epoch: 29, Steps: 263 | Train Loss: 0.5869341 Vali Loss: 0.2893626 Test Loss: 0.4111036
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_90_720_FITS_ETTm2_ftM_sl90_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.40869760513305664, mae:0.3982327878475189, rse:0.5138607621192932, corr:[0.5478126  0.5476345  0.538243   0.5384439  0.5385006  0.5341458
 0.5320687  0.5333671  0.53276956 0.5299258  0.5288294  0.5294253
 0.5283847  0.52601093 0.52519363 0.5255245  0.52436155 0.52215916
 0.521312   0.5214632  0.5204048  0.5184204  0.5176148  0.5180114
 0.51767486 0.51646763 0.5159872  0.51629555 0.51590025 0.5146082
 0.5136072  0.51326764 0.51271605 0.51175207 0.5111169  0.5109173
 0.51028955 0.5091611  0.5083163  0.50794077 0.5074356  0.50669634
 0.5062923  0.5063578  0.5061694  0.50533086 0.5043937  0.50378525
 0.5031317  0.50205666 0.5009562  0.5003629  0.49994504 0.49924886
 0.49844798 0.4979441  0.49768886 0.49734232 0.49680552 0.49646842
 0.4964188  0.4963552  0.49621123 0.49618593 0.49627066 0.49628425
 0.49612698 0.495957   0.4959561  0.49606273 0.49609515 0.49605832
 0.496029   0.49608493 0.49605975 0.4959096  0.49572203 0.49553657
 0.49535403 0.49503902 0.4945563  0.4941198  0.49388155 0.49362406
 0.49320132 0.4927262  0.49242926 0.49229515 0.4920084  0.49150553
 0.49104676 0.49066836 0.49010336 0.48903555 0.48748568 0.48568854
 0.48359257 0.48116717 0.4788396  0.47697628 0.47540107 0.47385505
 0.4724529  0.47134757 0.47022682 0.4687326  0.46713504 0.46585608
 0.46477282 0.46346626 0.46188772 0.46046    0.45942527 0.45831767
 0.45678812 0.4551282  0.45375583 0.45269397 0.45163402 0.4504129
 0.44929826 0.44832405 0.44728488 0.44614738 0.4451593  0.4443887
 0.44365242 0.44265383 0.4413751  0.44020957 0.4393246  0.43847466
 0.43753827 0.4366657  0.4359035  0.43526408 0.4345798  0.43416828
 0.43409568 0.43397728 0.43346217 0.4327362  0.43220457 0.43178165
 0.431119   0.4301974  0.42911577 0.42806953 0.42699036 0.42605257
 0.42545763 0.42526448 0.42495394 0.42430785 0.42382976 0.42364112
 0.42344004 0.42324182 0.42314628 0.42324626 0.42318004 0.42283022
 0.42263308 0.42258266 0.42253336 0.4222647  0.42206013 0.42223063
 0.42258847 0.42276853 0.42279658 0.42289415 0.42296764 0.42269325
 0.422214   0.4220261  0.4221762  0.42228583 0.4221915  0.42211178
 0.42210162 0.42189634 0.42147478 0.42106935 0.42086664 0.42060077
 0.42005914 0.41941214 0.41881114 0.4179231  0.4162835  0.41392356
 0.41147885 0.40936747 0.40733162 0.4052726  0.4035331  0.40219736
 0.4009829  0.39969015 0.39852488 0.39750087 0.39633006 0.39487436
 0.39339295 0.39210498 0.3907385  0.3890888  0.38738707 0.38604498
 0.38497394 0.38382137 0.38239375 0.38088503 0.37954733 0.37840796
 0.3773641  0.37629497 0.37528846 0.37419707 0.3729721  0.37170416
 0.3704983  0.36928293 0.36830115 0.3673427  0.36650097 0.36565238
 0.36461216 0.36335826 0.36204657 0.36108947 0.36050254 0.36012614
 0.3597697  0.35952199 0.35939166 0.35906622 0.35853663 0.35809147
 0.35795838 0.3578429  0.3572753  0.35676342 0.35656607 0.35656977
 0.35662133 0.35661623 0.35659802 0.35638985 0.35613066 0.3561118
 0.35646275 0.35694042 0.35711658 0.35698405 0.35712314 0.35771772
 0.3581568  0.35809088 0.35791522 0.35830867 0.35900933 0.3593487
 0.359404   0.35958746 0.35997203 0.35993525 0.35956532 0.3595064
 0.3598889  0.3601295  0.35997763 0.3600296  0.36068413 0.36137232
 0.36152172 0.36135754 0.36147514 0.36189872 0.36200708 0.3617651
 0.36169508 0.36197123 0.36200222 0.36121523 0.35981667 0.35840487
 0.35715476 0.3558722  0.35466552 0.35400873 0.3540269  0.35433155
 0.35449716 0.35451046 0.35454354 0.3544242  0.3538778  0.3530459
 0.35230532 0.3517189  0.35109288 0.3503851  0.349729   0.34905782
 0.3481965  0.34716374 0.3462493  0.3455903  0.3449508  0.34424353
 0.34356824 0.34308937 0.34261906 0.34198284 0.34129024 0.34069195
 0.34005138 0.3392423  0.33842495 0.33786017 0.33745328 0.33694178
 0.33636194 0.33602032 0.33591998 0.33562925 0.33514065 0.3348107
 0.3348762  0.3349784  0.33474657 0.33430412 0.33411106 0.33425862
 0.3342495  0.33386698 0.3335511  0.33362868 0.3337484  0.33362937
 0.3336123  0.33405375 0.3346273  0.33484092 0.3347597  0.33483818
 0.3352606  0.3356905  0.33598757 0.33624905 0.3365678  0.33680126
 0.3368938  0.33701214 0.33713007 0.33724174 0.33732677 0.33751473
 0.3378467  0.33820105 0.33844355 0.33866084 0.33891177 0.33909747
 0.33912182 0.33928967 0.33986363 0.34050104 0.3408198  0.34091693
 0.3412084  0.3417081  0.34198523 0.34194687 0.342006   0.3424104
 0.3427721  0.3426492  0.34213206 0.34153682 0.34075305 0.33940464
 0.33758742 0.33600757 0.33499792 0.3341942  0.33324313 0.33232585
 0.3316986  0.3313137  0.33088702 0.33027148 0.32962397 0.3290117
 0.32828206 0.32728404 0.32606423 0.3248374  0.32370335 0.3225934
 0.32143492 0.32021278 0.31902713 0.31794283 0.31676713 0.31553447
 0.31439734 0.31352985 0.31292355 0.3123765  0.3117215  0.31091356
 0.31004643 0.30911228 0.30816233 0.30740327 0.30680606 0.30627054
 0.30556896 0.30458757 0.30362356 0.3030431  0.3028467  0.30269724
 0.30233347 0.3019976  0.30189747 0.3018783  0.30145997 0.30067843
 0.29995623 0.29949152 0.29903468 0.29837894 0.2975544  0.2970091
 0.29683486 0.29684722 0.29670528 0.29637292 0.2960592  0.29601997
 0.2962608  0.296572   0.29665333 0.29640546 0.29621506 0.29630992
 0.2964056  0.29607892 0.29559442 0.29554296 0.29590443 0.29615155
 0.29604673 0.2960014  0.2962906  0.2965544  0.2964793  0.29617664
 0.2960314  0.29600367 0.29593214 0.29597813 0.2963281  0.2966658
 0.2966351  0.2964565  0.2965903  0.29686216 0.29666322 0.29596734
 0.29540387 0.29509953 0.29440185 0.29278585 0.29064807 0.28854078
 0.28649807 0.28433838 0.2823488  0.28100857 0.28027922 0.2797128
 0.2790664  0.27863094 0.2785206  0.27837706 0.2779214  0.2773151
 0.27661106 0.27568445 0.27442652 0.27318782 0.27229673 0.27156726
 0.27060062 0.26928034 0.26793987 0.2668313  0.26579762 0.26467744
 0.2635346  0.2625644  0.26167142 0.2606019  0.2595329  0.25879335
 0.2583785  0.25788495 0.2570849  0.25617525 0.25529268 0.25446224
 0.25370848 0.25307176 0.25262088 0.25210232 0.25129786 0.2506052
 0.25054988 0.25100476 0.25096378 0.25006714 0.2490727  0.24864185
 0.24843888 0.2476933  0.24681711 0.2465076  0.246517   0.2461172
 0.24540073 0.24514927 0.2455097  0.24567899 0.24538061 0.24522047
 0.24550663 0.24572982 0.24568246 0.24595645 0.24661459 0.24689187
 0.24636681 0.24582778 0.24602354 0.24645671 0.24629518 0.24616472
 0.24677566 0.24764474 0.24770553 0.24716829 0.2470815  0.24764413
 0.2478478  0.24752973 0.24758627 0.24833421 0.24884088 0.24856582
 0.24828693 0.24877606 0.24944301 0.24928877 0.2486353  0.24844363
 0.24877694 0.24881513 0.2482029  0.24721058 0.24576539 0.24345277
 0.24053746 0.23817514 0.2369691  0.23646371 0.23609488 0.23604892
 0.23665547 0.2375543  0.2379568  0.23772909 0.23738693 0.23711848
 0.2365507  0.23546977 0.23435268 0.23362201 0.23293339 0.23174274
 0.23021361 0.22891286 0.22793242 0.22685835 0.22566424 0.22485018
 0.22449782 0.22408397 0.2232197  0.22221607 0.22161758 0.2211848
 0.22046243 0.21960989 0.21894975 0.21835184 0.21754564 0.21664838
 0.2161258  0.21587048 0.2154308  0.2145258  0.21371749 0.2133059
 0.21306701 0.21272661 0.21226723 0.21184213 0.21143535 0.2107399
 0.21006612 0.20977785 0.20969318 0.20934033 0.20888822 0.20861667
 0.20868257 0.2088415  0.20904668 0.20952955 0.21004504 0.20978302
 0.20898458 0.20888956 0.20980115 0.2104207  0.21002886 0.20960128
 0.21015494 0.21064451 0.2098042  0.20841786 0.20846757 0.2095074
 0.2095508  0.20851745 0.20843823 0.20989834 0.21081495 0.20995814
 0.20903222 0.20979309 0.21127294 0.21193801 0.21214008 0.21299307
 0.21414806 0.21454935 0.2144755  0.21479158 0.21518037 0.2147784
 0.21408966 0.21420516 0.21463625 0.21395446 0.21205732 0.21022165
 0.20886257 0.20714487 0.20492795 0.20337886 0.20321764 0.20368722
 0.20384863 0.20402618 0.20478696 0.20553379 0.2053519  0.20443943
 0.20365885 0.20313562 0.20223285 0.20091009 0.19995604 0.19957297
 0.19894835 0.19747257 0.19570073 0.1945892  0.1939322  0.19286409
 0.19147608 0.190612   0.19009353 0.1889595  0.18733469 0.18610342
 0.18545137 0.18427642 0.18227217 0.1809364  0.18072252 0.18034221
 0.17858598 0.17696872 0.17667118 0.176313   0.17360681 0.17071971
 0.17087659 0.17189606 0.16902366 0.16464612 0.16555917 0.16784903]
