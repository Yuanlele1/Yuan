Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_360_720_FITS_ETTm2_ftM_sl360_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=50, out_features=150, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6720000.0
params:  7650.0
Trainable parameters:  7650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5631552
	speed: 0.0236s/iter; left time: 612.7411s
	iters: 200, epoch: 1 | loss: 0.5076924
	speed: 0.0170s/iter; left time: 440.3544s
Epoch: 1 cost time: 5.029515743255615
Epoch: 1, Steps: 261 | Train Loss: 0.6145921 Vali Loss: 0.2858823 Test Loss: 0.3907972
Validation loss decreased (inf --> 0.285882).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5785295
	speed: 0.0759s/iter; left time: 1952.8833s
	iters: 200, epoch: 2 | loss: 0.5031456
	speed: 0.0159s/iter; left time: 408.4149s
Epoch: 2 cost time: 4.724047422409058
Epoch: 2, Steps: 261 | Train Loss: 0.5443075 Vali Loss: 0.2748616 Test Loss: 0.3783787
Validation loss decreased (0.285882 --> 0.274862).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5228583
	speed: 0.0881s/iter; left time: 2245.9471s
	iters: 200, epoch: 3 | loss: 0.5925009
	speed: 0.0173s/iter; left time: 438.9163s
Epoch: 3 cost time: 5.847776889801025
Epoch: 3, Steps: 261 | Train Loss: 0.5338396 Vali Loss: 0.2709995 Test Loss: 0.3747406
Validation loss decreased (0.274862 --> 0.271000).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4754868
	speed: 0.0770s/iter; left time: 1941.1191s
	iters: 200, epoch: 4 | loss: 0.5630119
	speed: 0.0161s/iter; left time: 403.5672s
Epoch: 4 cost time: 4.758899211883545
Epoch: 4, Steps: 261 | Train Loss: 0.5288345 Vali Loss: 0.2692133 Test Loss: 0.3727054
Validation loss decreased (0.271000 --> 0.269213).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4741272
	speed: 0.0764s/iter; left time: 1907.4972s
	iters: 200, epoch: 5 | loss: 0.5654950
	speed: 0.0153s/iter; left time: 380.1174s
Epoch: 5 cost time: 4.6025612354278564
Epoch: 5, Steps: 261 | Train Loss: 0.5262737 Vali Loss: 0.2678312 Test Loss: 0.3717658
Validation loss decreased (0.269213 --> 0.267831).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5370425
	speed: 0.0745s/iter; left time: 1840.1343s
	iters: 200, epoch: 6 | loss: 0.5741760
	speed: 0.0155s/iter; left time: 380.3386s
Epoch: 6 cost time: 4.534180641174316
Epoch: 6, Steps: 261 | Train Loss: 0.5239648 Vali Loss: 0.2671385 Test Loss: 0.3709326
Validation loss decreased (0.267831 --> 0.267139).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3501469
	speed: 0.0821s/iter; left time: 2005.0850s
	iters: 200, epoch: 7 | loss: 0.4273642
	speed: 0.0167s/iter; left time: 405.5127s
Epoch: 7 cost time: 5.088251113891602
Epoch: 7, Steps: 261 | Train Loss: 0.5226176 Vali Loss: 0.2664785 Test Loss: 0.3703187
Validation loss decreased (0.267139 --> 0.266478).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4208260
	speed: 0.0782s/iter; left time: 1891.3567s
	iters: 200, epoch: 8 | loss: 0.4245561
	speed: 0.0161s/iter; left time: 386.4711s
Epoch: 8 cost time: 4.692585706710815
Epoch: 8, Steps: 261 | Train Loss: 0.5215059 Vali Loss: 0.2661539 Test Loss: 0.3700428
Validation loss decreased (0.266478 --> 0.266154).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4797530
	speed: 0.0777s/iter; left time: 1858.0689s
	iters: 200, epoch: 9 | loss: 0.5443267
	speed: 0.0174s/iter; left time: 413.3643s
Epoch: 9 cost time: 4.895673036575317
Epoch: 9, Steps: 261 | Train Loss: 0.5212083 Vali Loss: 0.2656698 Test Loss: 0.3697499
Validation loss decreased (0.266154 --> 0.265670).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4720081
	speed: 0.0844s/iter; left time: 1996.1017s
	iters: 200, epoch: 10 | loss: 0.6859127
	speed: 0.0168s/iter; left time: 394.7282s
Epoch: 10 cost time: 4.987957954406738
Epoch: 10, Steps: 261 | Train Loss: 0.5206432 Vali Loss: 0.2652442 Test Loss: 0.3695166
Validation loss decreased (0.265670 --> 0.265244).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.6079412
	speed: 0.0790s/iter; left time: 1848.7725s
	iters: 200, epoch: 11 | loss: 0.4355617
	speed: 0.0170s/iter; left time: 396.2292s
Epoch: 11 cost time: 4.9077417850494385
Epoch: 11, Steps: 261 | Train Loss: 0.5198359 Vali Loss: 0.2649713 Test Loss: 0.3693286
Validation loss decreased (0.265244 --> 0.264971).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4962848
	speed: 0.0826s/iter; left time: 1909.9173s
	iters: 200, epoch: 12 | loss: 0.5783324
	speed: 0.0167s/iter; left time: 385.6509s
Epoch: 12 cost time: 5.0199854373931885
Epoch: 12, Steps: 261 | Train Loss: 0.5197316 Vali Loss: 0.2648283 Test Loss: 0.3692034
Validation loss decreased (0.264971 --> 0.264828).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.6082750
	speed: 0.0835s/iter; left time: 1908.7829s
	iters: 200, epoch: 13 | loss: 0.5156195
	speed: 0.0406s/iter; left time: 925.2156s
Epoch: 13 cost time: 7.81470799446106
Epoch: 13, Steps: 261 | Train Loss: 0.5194118 Vali Loss: 0.2649090 Test Loss: 0.3691182
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4037747
	speed: 0.0948s/iter; left time: 2143.2565s
	iters: 200, epoch: 14 | loss: 0.6621741
	speed: 0.0157s/iter; left time: 353.9569s
Epoch: 14 cost time: 4.731748104095459
Epoch: 14, Steps: 261 | Train Loss: 0.5189762 Vali Loss: 0.2646406 Test Loss: 0.3689894
Validation loss decreased (0.264828 --> 0.264641).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4246449
	speed: 0.0763s/iter; left time: 1704.3826s
	iters: 200, epoch: 15 | loss: 0.5610623
	speed: 0.0164s/iter; left time: 364.8919s
Epoch: 15 cost time: 4.604426145553589
Epoch: 15, Steps: 261 | Train Loss: 0.5185229 Vali Loss: 0.2644854 Test Loss: 0.3690262
Validation loss decreased (0.264641 --> 0.264485).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5656204
	speed: 0.0750s/iter; left time: 1656.9631s
	iters: 200, epoch: 16 | loss: 0.4644372
	speed: 0.0157s/iter; left time: 344.5428s
Epoch: 16 cost time: 4.5881805419921875
Epoch: 16, Steps: 261 | Train Loss: 0.5181859 Vali Loss: 0.2644935 Test Loss: 0.3688840
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4530625
	speed: 0.0763s/iter; left time: 1664.7966s
	iters: 200, epoch: 17 | loss: 0.4356143
	speed: 0.0166s/iter; left time: 359.6231s
Epoch: 17 cost time: 4.759040355682373
Epoch: 17, Steps: 261 | Train Loss: 0.5183785 Vali Loss: 0.2642596 Test Loss: 0.3688742
Validation loss decreased (0.264485 --> 0.264260).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5380928
	speed: 0.0825s/iter; left time: 1778.6268s
	iters: 200, epoch: 18 | loss: 0.4727922
	speed: 0.0194s/iter; left time: 415.8205s
Epoch: 18 cost time: 6.568043231964111
Epoch: 18, Steps: 261 | Train Loss: 0.5183090 Vali Loss: 0.2644475 Test Loss: 0.3688131
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4739676
	speed: 0.0918s/iter; left time: 1955.4929s
	iters: 200, epoch: 19 | loss: 0.4099222
	speed: 0.0168s/iter; left time: 356.1762s
Epoch: 19 cost time: 4.823754549026489
Epoch: 19, Steps: 261 | Train Loss: 0.5178111 Vali Loss: 0.2641328 Test Loss: 0.3687643
Validation loss decreased (0.264260 --> 0.264133).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4325509
	speed: 0.0776s/iter; left time: 1632.1056s
	iters: 200, epoch: 20 | loss: 0.6164371
	speed: 0.0166s/iter; left time: 346.7375s
Epoch: 20 cost time: 4.78260612487793
Epoch: 20, Steps: 261 | Train Loss: 0.5176247 Vali Loss: 0.2639110 Test Loss: 0.3687868
Validation loss decreased (0.264133 --> 0.263911).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4780172
	speed: 0.0821s/iter; left time: 1706.9969s
	iters: 200, epoch: 21 | loss: 0.4607429
	speed: 0.0171s/iter; left time: 352.8892s
Epoch: 21 cost time: 5.091233491897583
Epoch: 21, Steps: 261 | Train Loss: 0.5176318 Vali Loss: 0.2638955 Test Loss: 0.3687428
Validation loss decreased (0.263911 --> 0.263896).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3906371
	speed: 0.0787s/iter; left time: 1615.1969s
	iters: 200, epoch: 22 | loss: 0.3693981
	speed: 0.0163s/iter; left time: 332.0008s
Epoch: 22 cost time: 4.855194807052612
Epoch: 22, Steps: 261 | Train Loss: 0.5177404 Vali Loss: 0.2639848 Test Loss: 0.3687035
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5221952
	speed: 0.0760s/iter; left time: 1539.6545s
	iters: 200, epoch: 23 | loss: 0.4624143
	speed: 0.0155s/iter; left time: 312.8634s
Epoch: 23 cost time: 4.6534950733184814
Epoch: 23, Steps: 261 | Train Loss: 0.5177691 Vali Loss: 0.2639620 Test Loss: 0.3687460
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3803249
	speed: 0.0759s/iter; left time: 1518.5079s
	iters: 200, epoch: 24 | loss: 0.5410979
	speed: 0.0164s/iter; left time: 326.9808s
Epoch: 24 cost time: 4.837530136108398
Epoch: 24, Steps: 261 | Train Loss: 0.5178215 Vali Loss: 0.2638137 Test Loss: 0.3686911
Validation loss decreased (0.263896 --> 0.263814).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4606976
	speed: 0.0832s/iter; left time: 1642.0193s
	iters: 200, epoch: 25 | loss: 0.4500440
	speed: 0.0169s/iter; left time: 331.7320s
Epoch: 25 cost time: 5.170510530471802
Epoch: 25, Steps: 261 | Train Loss: 0.5175160 Vali Loss: 0.2639014 Test Loss: 0.3686470
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.5248225
	speed: 0.1024s/iter; left time: 1994.9675s
	iters: 200, epoch: 26 | loss: 0.6685431
	speed: 0.0164s/iter; left time: 316.9437s
Epoch: 26 cost time: 5.912620306015015
Epoch: 26, Steps: 261 | Train Loss: 0.5175315 Vali Loss: 0.2641129 Test Loss: 0.3686543
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4636002
	speed: 0.0893s/iter; left time: 1716.1714s
	iters: 200, epoch: 27 | loss: 0.5121095
	speed: 0.0158s/iter; left time: 301.9941s
Epoch: 27 cost time: 4.64750599861145
Epoch: 27, Steps: 261 | Train Loss: 0.5171787 Vali Loss: 0.2637209 Test Loss: 0.3686498
Validation loss decreased (0.263814 --> 0.263721).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4801383
	speed: 0.0833s/iter; left time: 1578.0577s
	iters: 200, epoch: 28 | loss: 0.4309757
	speed: 0.0203s/iter; left time: 382.4526s
Epoch: 28 cost time: 5.368157148361206
Epoch: 28, Steps: 261 | Train Loss: 0.5173387 Vali Loss: 0.2637791 Test Loss: 0.3686437
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.6218415
	speed: 0.0799s/iter; left time: 1494.3586s
	iters: 200, epoch: 29 | loss: 0.4861006
	speed: 0.0165s/iter; left time: 307.4035s
Epoch: 29 cost time: 4.839223623275757
Epoch: 29, Steps: 261 | Train Loss: 0.5176225 Vali Loss: 0.2637287 Test Loss: 0.3686426
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4344128
	speed: 0.0807s/iter; left time: 1486.8352s
	iters: 200, epoch: 30 | loss: 0.4325041
	speed: 0.0161s/iter; left time: 295.7167s
Epoch: 30 cost time: 4.826480388641357
Epoch: 30, Steps: 261 | Train Loss: 0.5173118 Vali Loss: 0.2638539 Test Loss: 0.3686476
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.5166082
	speed: 0.0777s/iter; left time: 1412.4982s
	iters: 200, epoch: 31 | loss: 0.5271448
	speed: 0.0186s/iter; left time: 336.8300s
Epoch: 31 cost time: 5.009379625320435
Epoch: 31, Steps: 261 | Train Loss: 0.5169054 Vali Loss: 0.2636792 Test Loss: 0.3686208
Validation loss decreased (0.263721 --> 0.263679).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.5698895
	speed: 0.0765s/iter; left time: 1369.7978s
	iters: 200, epoch: 32 | loss: 0.5103931
	speed: 0.0167s/iter; left time: 297.4264s
Epoch: 32 cost time: 4.893743515014648
Epoch: 32, Steps: 261 | Train Loss: 0.5171556 Vali Loss: 0.2638658 Test Loss: 0.3686104
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.5431218
	speed: 0.0818s/iter; left time: 1443.8946s
	iters: 200, epoch: 33 | loss: 0.4202595
	speed: 0.0158s/iter; left time: 276.8404s
Epoch: 33 cost time: 4.701435089111328
Epoch: 33, Steps: 261 | Train Loss: 0.5167303 Vali Loss: 0.2637181 Test Loss: 0.3686167
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4191246
	speed: 0.0764s/iter; left time: 1328.1594s
	iters: 200, epoch: 34 | loss: 0.6116536
	speed: 0.0155s/iter; left time: 268.2244s
Epoch: 34 cost time: 4.738914251327515
Epoch: 34, Steps: 261 | Train Loss: 0.5167047 Vali Loss: 0.2637087 Test Loss: 0.3686030
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5531593
	speed: 0.0787s/iter; left time: 1347.0508s
	iters: 200, epoch: 35 | loss: 0.3750510
	speed: 0.0170s/iter; left time: 289.5945s
Epoch: 35 cost time: 5.000491619110107
Epoch: 35, Steps: 261 | Train Loss: 0.5168966 Vali Loss: 0.2638351 Test Loss: 0.3685997
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3902903
	speed: 0.0826s/iter; left time: 1392.9214s
	iters: 200, epoch: 36 | loss: 0.3574378
	speed: 0.0166s/iter; left time: 278.5886s
Epoch: 36 cost time: 4.976495742797852
Epoch: 36, Steps: 261 | Train Loss: 0.5169008 Vali Loss: 0.2634807 Test Loss: 0.3686040
Validation loss decreased (0.263679 --> 0.263481).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4640175
	speed: 0.0793s/iter; left time: 1316.1240s
	iters: 200, epoch: 37 | loss: 0.5047236
	speed: 0.0155s/iter; left time: 255.8002s
Epoch: 37 cost time: 4.6028056144714355
Epoch: 37, Steps: 261 | Train Loss: 0.5174969 Vali Loss: 0.2635364 Test Loss: 0.3686054
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.6692902
	speed: 0.0792s/iter; left time: 1294.6213s
	iters: 200, epoch: 38 | loss: 0.4827397
	speed: 0.0155s/iter; left time: 251.4822s
Epoch: 38 cost time: 4.627710342407227
Epoch: 38, Steps: 261 | Train Loss: 0.5167203 Vali Loss: 0.2639135 Test Loss: 0.3685980
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.7896888
	speed: 0.0794s/iter; left time: 1277.0028s
	iters: 200, epoch: 39 | loss: 0.6567990
	speed: 0.0163s/iter; left time: 259.7711s
Epoch: 39 cost time: 4.852613210678101
Epoch: 39, Steps: 261 | Train Loss: 0.5167636 Vali Loss: 0.2637485 Test Loss: 0.3685909
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3681312
	speed: 0.0753s/iter; left time: 1191.9062s
	iters: 200, epoch: 40 | loss: 0.5254820
	speed: 0.0159s/iter; left time: 249.2209s
Epoch: 40 cost time: 4.662330865859985
Epoch: 40, Steps: 261 | Train Loss: 0.5170414 Vali Loss: 0.2635705 Test Loss: 0.3685896
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.5416179
	speed: 0.0787s/iter; left time: 1224.9398s
	iters: 200, epoch: 41 | loss: 0.4553153
	speed: 0.0166s/iter; left time: 256.8084s
Epoch: 41 cost time: 4.909965991973877
Epoch: 41, Steps: 261 | Train Loss: 0.5161531 Vali Loss: 0.2639621 Test Loss: 0.3685809
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.5249640
	speed: 0.0863s/iter; left time: 1319.9122s
	iters: 200, epoch: 42 | loss: 0.4094537
	speed: 0.0156s/iter; left time: 236.8334s
Epoch: 42 cost time: 4.649672269821167
Epoch: 42, Steps: 261 | Train Loss: 0.5169216 Vali Loss: 0.2636705 Test Loss: 0.3685819
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4634482
	speed: 0.0760s/iter; left time: 1142.3369s
	iters: 200, epoch: 43 | loss: 0.5340552
	speed: 0.0156s/iter; left time: 232.7805s
Epoch: 43 cost time: 4.664283275604248
Epoch: 43, Steps: 261 | Train Loss: 0.5168210 Vali Loss: 0.2636611 Test Loss: 0.3685791
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4910095
	speed: 0.0792s/iter; left time: 1170.4189s
	iters: 200, epoch: 44 | loss: 0.5015582
	speed: 0.0164s/iter; left time: 241.2635s
Epoch: 44 cost time: 5.2547993659973145
Epoch: 44, Steps: 261 | Train Loss: 0.5170754 Vali Loss: 0.2637645 Test Loss: 0.3685826
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.6533200
	speed: 0.0761s/iter; left time: 1105.1140s
	iters: 200, epoch: 45 | loss: 0.5045589
	speed: 0.0166s/iter; left time: 239.4770s
Epoch: 45 cost time: 4.7933878898620605
Epoch: 45, Steps: 261 | Train Loss: 0.5172650 Vali Loss: 0.2635397 Test Loss: 0.3685811
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.6863611
	speed: 0.0815s/iter; left time: 1161.7963s
	iters: 200, epoch: 46 | loss: 0.3815928
	speed: 0.0167s/iter; left time: 236.3067s
Epoch: 46 cost time: 4.942558288574219
Epoch: 46, Steps: 261 | Train Loss: 0.5167438 Vali Loss: 0.2637897 Test Loss: 0.3685760
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.5749442
	speed: 0.0816s/iter; left time: 1142.1207s
	iters: 200, epoch: 47 | loss: 0.3372927
	speed: 0.0168s/iter; left time: 234.0541s
Epoch: 47 cost time: 4.95789909362793
Epoch: 47, Steps: 261 | Train Loss: 0.5167986 Vali Loss: 0.2636335 Test Loss: 0.3685801
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.5740631
	speed: 0.0846s/iter; left time: 1162.2763s
	iters: 200, epoch: 48 | loss: 0.5666126
	speed: 0.0167s/iter; left time: 227.3134s
Epoch: 48 cost time: 5.13768196105957
Epoch: 48, Steps: 261 | Train Loss: 0.5166854 Vali Loss: 0.2636992 Test Loss: 0.3685777
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4832276
	speed: 0.0842s/iter; left time: 1135.0231s
	iters: 200, epoch: 49 | loss: 0.5164369
	speed: 0.0164s/iter; left time: 219.6371s
Epoch: 49 cost time: 4.829653263092041
Epoch: 49, Steps: 261 | Train Loss: 0.5165718 Vali Loss: 0.2636824 Test Loss: 0.3685714
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.8293442
	speed: 0.0749s/iter; left time: 989.6294s
	iters: 200, epoch: 50 | loss: 0.5726820
	speed: 0.0156s/iter; left time: 203.9573s
Epoch: 50 cost time: 4.651541471481323
Epoch: 50, Steps: 261 | Train Loss: 0.5170676 Vali Loss: 0.2636792 Test Loss: 0.3685688
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4063720
	speed: 0.0771s/iter; left time: 998.9137s
	iters: 200, epoch: 51 | loss: 0.4677575
	speed: 0.0156s/iter; left time: 200.9436s
Epoch: 51 cost time: 4.656474590301514
Epoch: 51, Steps: 261 | Train Loss: 0.5165881 Vali Loss: 0.2637266 Test Loss: 0.3685683
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.5457178
	speed: 0.0763s/iter; left time: 968.7707s
	iters: 200, epoch: 52 | loss: 0.4850714
	speed: 0.0161s/iter; left time: 202.8053s
Epoch: 52 cost time: 4.7137415409088135
Epoch: 52, Steps: 261 | Train Loss: 0.5167675 Vali Loss: 0.2637499 Test Loss: 0.3685716
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.5982144
	speed: 0.0971s/iter; left time: 1207.4650s
	iters: 200, epoch: 53 | loss: 0.3278351
	speed: 0.0166s/iter; left time: 204.0472s
Epoch: 53 cost time: 5.037118196487427
Epoch: 53, Steps: 261 | Train Loss: 0.5168590 Vali Loss: 0.2635220 Test Loss: 0.3685677
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.5051312
	speed: 0.0808s/iter; left time: 983.7718s
	iters: 200, epoch: 54 | loss: 0.5172719
	speed: 0.0165s/iter; left time: 199.3277s
Epoch: 54 cost time: 4.947889804840088
Epoch: 54, Steps: 261 | Train Loss: 0.5168286 Vali Loss: 0.2637176 Test Loss: 0.3685716
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.4788399
	speed: 0.0842s/iter; left time: 1002.8009s
	iters: 200, epoch: 55 | loss: 0.5512142
	speed: 0.0165s/iter; left time: 194.8267s
Epoch: 55 cost time: 4.879766941070557
Epoch: 55, Steps: 261 | Train Loss: 0.5168191 Vali Loss: 0.2638043 Test Loss: 0.3685725
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4273283
	speed: 0.0761s/iter; left time: 886.7038s
	iters: 200, epoch: 56 | loss: 0.4972066
	speed: 0.0229s/iter; left time: 264.4717s
Epoch: 56 cost time: 5.376835823059082
Epoch: 56, Steps: 261 | Train Loss: 0.5170205 Vali Loss: 0.2634918 Test Loss: 0.3685714
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_360_720_FITS_ETTm2_ftM_sl360_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.36590880155563354, mae:0.3820221424102783, rse:0.48621776700019836, corr:[0.539924   0.54373354 0.5399255  0.5369231  0.53671646 0.5379255
 0.5387077  0.5380555  0.5366267  0.5356134  0.5354915  0.5360692
 0.53663963 0.536569   0.5358532  0.53489727 0.53422505 0.5339615
 0.5338905  0.53365624 0.53312457 0.5324513  0.5318643  0.53162587
 0.53166276 0.5316951  0.53148496 0.53093594 0.5301739  0.5294132
 0.5288738  0.5286339  0.5285351  0.528333   0.5278601  0.52712315
 0.52625865 0.52545166 0.524823   0.5243654  0.5239815  0.52359104
 0.5231199  0.5225447  0.52190584 0.5212066  0.520507   0.51980394
 0.51904225 0.51822525 0.51735586 0.51654994 0.5157939  0.5150716
 0.5144003  0.51379204 0.5132266  0.5127185  0.5122837  0.5118985
 0.5115661  0.51127225 0.5110247  0.51078355 0.5105431  0.51033574
 0.5101093  0.5098688  0.50962865 0.5094069  0.50918937 0.5089715
 0.5087407  0.5084972  0.50819844 0.50780606 0.50735873 0.5068611
 0.5063642  0.50585085 0.50534964 0.5048169  0.5042734  0.50369775
 0.50315267 0.5026696  0.5022008  0.50175595 0.50130105 0.5008108
 0.5002786  0.4996874  0.49898273 0.49811664 0.49702635 0.49565887
 0.49404544 0.49238548 0.49081835 0.48938373 0.488078   0.48690158
 0.48579654 0.4846628  0.48343542 0.48220325 0.4810554  0.48004425
 0.4791507  0.47830492 0.47750384 0.47670218 0.47590178 0.4750813
 0.4742522  0.47345543 0.47271428 0.47202095 0.4714054  0.4707952
 0.4701429  0.46940437 0.46866462 0.46794945 0.4672797  0.4666372
 0.46599272 0.46526888 0.46443412 0.46352434 0.4626461  0.46186605
 0.46121055 0.46064922 0.46008974 0.45948064 0.45873985 0.45795155
 0.4571619  0.45644382 0.45580333 0.45516837 0.45446658 0.45361307
 0.45265102 0.45169926 0.45092043 0.4503253  0.4497962  0.44918635
 0.448469   0.4476902  0.4468695  0.4461153  0.44552436 0.44509226
 0.4446921  0.4442155  0.44359183 0.44294974 0.4423493  0.44187397
 0.44157222 0.44137144 0.4412196  0.44102195 0.44069102 0.440351
 0.44003862 0.4397973  0.43958122 0.4393134  0.43892097 0.4384273
 0.43791497 0.43749517 0.43718046 0.43691647 0.43662348 0.43623087
 0.43569124 0.43505618 0.4344848  0.43404666 0.4337441  0.43349382
 0.4331843  0.4326761  0.4319056  0.4308554  0.42961094 0.42814562
 0.4265156  0.42491183 0.4232855  0.42158693 0.41989955 0.41826266
 0.41674417 0.41533145 0.41398865 0.41270813 0.41148674 0.41031146
 0.4091376  0.4079584  0.40672812 0.40551397 0.40440142 0.40351084
 0.40277717 0.4020807  0.40133533 0.40050617 0.3995253  0.39845428
 0.39736468 0.39618838 0.3951723  0.39438114 0.39370105 0.3929557
 0.3920925  0.39109263 0.39010313 0.38909382 0.3881054  0.38709947
 0.38610044 0.38513252 0.38418457 0.3832907  0.38251647 0.38192415
 0.38143006 0.3809436  0.38044432 0.3798665  0.3792463  0.3786621
 0.37810275 0.37755924 0.37702328 0.37665126 0.3762972  0.37601954
 0.37592387 0.3759662  0.3760277  0.37599155 0.37580475 0.3755395
 0.37517917 0.37492937 0.37476256 0.37467033 0.37454978 0.3743145
 0.37403622 0.3737807  0.37366766 0.37370548 0.37375262 0.373751
 0.37363932 0.37337986 0.3729975  0.37250262 0.37205553 0.37176535
 0.3715796  0.37139812 0.37116337 0.37095982 0.370743   0.3705558
 0.37031895 0.36998326 0.36954424 0.36905518 0.36857447 0.36823538
 0.36803806 0.36793914 0.3678085  0.3674478  0.3666544  0.36542284
 0.36394036 0.36258224 0.36145762 0.36052203 0.35969555 0.35889518
 0.3580471  0.35719454 0.35645518 0.35581324 0.3553175  0.35491905
 0.35447636 0.35383072 0.35308573 0.35239086 0.35186902 0.35157493
 0.35153297 0.35151365 0.3513515  0.35110146 0.35083988 0.3506653
 0.3505324  0.3503986  0.35015613 0.34979522 0.34934783 0.3488714
 0.34847453 0.34818912 0.3479601  0.34773174 0.3474402  0.34706047
 0.34661552 0.34626836 0.34608302 0.34597906 0.3458208  0.34558693
 0.3452675  0.3449047  0.34461123 0.34439883 0.34422192 0.34407145
 0.34377533 0.34328434 0.34268528 0.3421916  0.34192348 0.34192523
 0.34207842 0.3422735  0.34232444 0.34212902 0.34179816 0.34149185
 0.34137714 0.3414672  0.34160864 0.34167102 0.34160057 0.34137756
 0.3410861  0.34090164 0.34082636 0.34081027 0.34075525 0.34054843
 0.34024614 0.33998236 0.33977604 0.33965233 0.3395006  0.33928096
 0.3388969  0.33844906 0.3380695  0.33785754 0.33789283 0.33799478
 0.33802348 0.33788574 0.33760613 0.3372864  0.33709368 0.33715847
 0.33741134 0.33764783 0.3376118  0.33708715 0.33614656 0.33493048
 0.33369327 0.33267918 0.33190593 0.33120888 0.33037773 0.3294141
 0.3283605  0.32732642 0.32647625 0.32578745 0.32517004 0.3245161
 0.32380074 0.32308865 0.32234    0.32164377 0.32103947 0.32046166
 0.31991428 0.3193317  0.31878102 0.31835854 0.31812137 0.31807923
 0.3180845  0.31795794 0.3176938  0.3174048  0.31717312 0.31703267
 0.31692955 0.31678388 0.3164934  0.31604218 0.31553048 0.31514478
 0.3150097  0.31514144 0.31535426 0.31545383 0.3153587  0.3151084
 0.31482342 0.31469393 0.31475788 0.31487864 0.3149054  0.31469432
 0.3142222  0.3136318  0.31307402 0.3127348  0.31257546 0.31247807
 0.3122737  0.31198502 0.3115855  0.311272   0.3110895  0.3110313
 0.3109469  0.3107301  0.31035495 0.30983135 0.30931824 0.30895883
 0.30889264 0.30901092 0.30907443 0.30893776 0.30856353 0.30806085
 0.30761752 0.30734423 0.3071629  0.30699536 0.30677035 0.30639717
 0.30597994 0.30565435 0.30553463 0.30553326 0.30541167 0.30505875
 0.30438784 0.3035     0.302556   0.30177584 0.3012951  0.3010388
 0.30080584 0.30038196 0.29968017 0.2987008  0.2975066  0.29616728
 0.29483074 0.2936077  0.292502   0.29149732 0.2905313  0.28958404
 0.28865406 0.28775418 0.28688866 0.28609854 0.28536054 0.2846597
 0.2840236  0.28345776 0.28291216 0.28232086 0.28167698 0.28097528
 0.28029114 0.27965465 0.27906564 0.27853194 0.27804917 0.2776467
 0.2773085  0.27696306 0.27668813 0.2765117  0.27642658 0.27625456
 0.27590978 0.27539974 0.2748273  0.2743259  0.27398857 0.2737625
 0.27358505 0.27330473 0.272874   0.27234986 0.27183306 0.27146703
 0.2712541  0.27117464 0.2710936  0.2709236  0.27065432 0.2702827
 0.2698568  0.26944822 0.26903585 0.2686198  0.26821923 0.26789957
 0.2676965  0.26763794 0.26775485 0.26790535 0.26796877 0.2679188
 0.26772887 0.26753187 0.26741442 0.26740927 0.2673926  0.2672441
 0.266882   0.26638192 0.26587    0.2655092  0.26532656 0.26536503
 0.2654741  0.265496   0.26534292 0.2649825  0.26452988 0.26409724
 0.26374575 0.2635171  0.26339227 0.26331696 0.26319918 0.26303226
 0.26283643 0.26253784 0.26211822 0.26154003 0.2609725  0.26053256
 0.2602812  0.2601363  0.2598748  0.25923696 0.2580479  0.25624934
 0.25407657 0.25202572 0.25040194 0.24924006 0.24835245 0.24751925
 0.24646868 0.24523221 0.24389726 0.24264067 0.2416393  0.24094088
 0.24037679 0.23982148 0.23926371 0.23870844 0.23807809 0.23735361
 0.23657179 0.23576567 0.23500702 0.23437974 0.23401922 0.23391435
 0.23394397 0.23390105 0.23362426 0.23309204 0.23241183 0.23175184
 0.23126438 0.23101486 0.23092031 0.23080154 0.23054957 0.23009133
 0.22954725 0.22898507 0.22853719 0.22827047 0.22809921 0.22797391
 0.22774875 0.22748259 0.22722912 0.22711857 0.22715475 0.2271799
 0.22700605 0.22651272 0.22587503 0.22539596 0.22525099 0.2253671
 0.22563127 0.22586252 0.22585325 0.22550124 0.2250523  0.22474748
 0.22489606 0.22529285 0.22560056 0.22561315 0.2252495  0.22474054
 0.2243411  0.2243541  0.2247204  0.2250978  0.22532296 0.22505575
 0.22448105 0.22393137 0.2236851  0.22383814 0.22414011 0.22420213
 0.22397242 0.22359385 0.22332567 0.22353944 0.22422037 0.22497927
 0.22549479 0.22557949 0.22531812 0.22498623 0.22506693 0.22566423
 0.22647797 0.2270397  0.22698276 0.22608419 0.22450092 0.2227475
 0.22122589 0.22010928 0.21917322 0.21810627 0.21683498 0.21561933
 0.21461293 0.21401808 0.2138486  0.21378084 0.21347676 0.21289499
 0.212236   0.2116219  0.2112292  0.21100184 0.21090616 0.21062833
 0.21024486 0.20979993 0.20942806 0.20939504 0.20960018 0.20983686
 0.20996425 0.2097413  0.20927581 0.20880151 0.20849074 0.208164
 0.20775601 0.2070561  0.20617938 0.20541851 0.20487782 0.20469421
 0.20446996 0.2041045  0.20359257 0.20303522 0.20266144 0.20277452
 0.20311828 0.20312795 0.20247102 0.20152235 0.20107362 0.20268114]
