Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTm2', data_path='ETTm2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm2_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm2_360_720_FITS_ETTm2_ftM_sl360_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=34, out_features=102, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3107328.0
params:  3570.0
Trainable parameters:  3570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.8805900
	speed: 0.0326s/iter; left time: 848.6499s
	iters: 200, epoch: 1 | loss: 0.4807805
	speed: 0.0195s/iter; left time: 506.1685s
Epoch: 1 cost time: 6.5546324253082275
Epoch: 1, Steps: 261 | Train Loss: 0.6211315 Vali Loss: 0.2902656 Test Loss: 0.3954414
Validation loss decreased (inf --> 0.290266).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4540565
	speed: 0.1016s/iter; left time: 2615.0884s
	iters: 200, epoch: 2 | loss: 0.3490008
	speed: 0.0186s/iter; left time: 476.1018s
Epoch: 2 cost time: 5.426662921905518
Epoch: 2, Steps: 261 | Train Loss: 0.5471863 Vali Loss: 0.2774511 Test Loss: 0.3805429
Validation loss decreased (0.290266 --> 0.277451).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4839634
	speed: 0.0966s/iter; left time: 2460.5048s
	iters: 200, epoch: 3 | loss: 0.4714165
	speed: 0.0193s/iter; left time: 490.2811s
Epoch: 3 cost time: 6.294370412826538
Epoch: 3, Steps: 261 | Train Loss: 0.5362827 Vali Loss: 0.2731757 Test Loss: 0.3761563
Validation loss decreased (0.277451 --> 0.273176).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5481344
	speed: 0.1119s/iter; left time: 2822.8040s
	iters: 200, epoch: 4 | loss: 0.7147005
	speed: 0.0272s/iter; left time: 684.0689s
Epoch: 4 cost time: 8.354753732681274
Epoch: 4, Steps: 261 | Train Loss: 0.5313689 Vali Loss: 0.2709404 Test Loss: 0.3738566
Validation loss decreased (0.273176 --> 0.270940).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.7204218
	speed: 0.1252s/iter; left time: 3124.8787s
	iters: 200, epoch: 5 | loss: 0.5167405
	speed: 0.0245s/iter; left time: 608.0883s
Epoch: 5 cost time: 7.547105073928833
Epoch: 5, Steps: 261 | Train Loss: 0.5283653 Vali Loss: 0.2690322 Test Loss: 0.3725673
Validation loss decreased (0.270940 --> 0.269032).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4265527
	speed: 0.1073s/iter; left time: 2649.7598s
	iters: 200, epoch: 6 | loss: 0.6158193
	speed: 0.0278s/iter; left time: 682.8352s
Epoch: 6 cost time: 7.625975608825684
Epoch: 6, Steps: 261 | Train Loss: 0.5264482 Vali Loss: 0.2685498 Test Loss: 0.3718390
Validation loss decreased (0.269032 --> 0.268550).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5986120
	speed: 0.0947s/iter; left time: 2313.6827s
	iters: 200, epoch: 7 | loss: 0.4521229
	speed: 0.0252s/iter; left time: 613.8936s
Epoch: 7 cost time: 6.150989532470703
Epoch: 7, Steps: 261 | Train Loss: 0.5244232 Vali Loss: 0.2678370 Test Loss: 0.3712340
Validation loss decreased (0.268550 --> 0.267837).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5035337
	speed: 0.1092s/iter; left time: 2640.1443s
	iters: 200, epoch: 8 | loss: 0.4246975
	speed: 0.0257s/iter; left time: 618.3138s
Epoch: 8 cost time: 6.990704774856567
Epoch: 8, Steps: 261 | Train Loss: 0.5224975 Vali Loss: 0.2673199 Test Loss: 0.3707676
Validation loss decreased (0.267837 --> 0.267320).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4945974
	speed: 0.1190s/iter; left time: 2845.6743s
	iters: 200, epoch: 9 | loss: 0.4187219
	speed: 0.0249s/iter; left time: 593.7517s
Epoch: 9 cost time: 6.278239965438843
Epoch: 9, Steps: 261 | Train Loss: 0.5230327 Vali Loss: 0.2668318 Test Loss: 0.3705863
Validation loss decreased (0.267320 --> 0.266832).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4068331
	speed: 0.0994s/iter; left time: 2350.0340s
	iters: 200, epoch: 10 | loss: 0.7664652
	speed: 0.0247s/iter; left time: 580.5856s
Epoch: 10 cost time: 7.727300643920898
Epoch: 10, Steps: 261 | Train Loss: 0.5216356 Vali Loss: 0.2666414 Test Loss: 0.3703348
Validation loss decreased (0.266832 --> 0.266641).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5049469
	speed: 0.1288s/iter; left time: 3012.6635s
	iters: 200, epoch: 11 | loss: 0.4231271
	speed: 0.0326s/iter; left time: 759.8488s
Epoch: 11 cost time: 7.979618072509766
Epoch: 11, Steps: 261 | Train Loss: 0.5220075 Vali Loss: 0.2662464 Test Loss: 0.3701219
Validation loss decreased (0.266641 --> 0.266246).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4495887
	speed: 0.1107s/iter; left time: 2559.6083s
	iters: 200, epoch: 12 | loss: 0.6790163
	speed: 0.0216s/iter; left time: 497.6414s
Epoch: 12 cost time: 7.383874416351318
Epoch: 12, Steps: 261 | Train Loss: 0.5210016 Vali Loss: 0.2657726 Test Loss: 0.3699935
Validation loss decreased (0.266246 --> 0.265773).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3756146
	speed: 0.1131s/iter; left time: 2586.3323s
	iters: 200, epoch: 13 | loss: 0.5859706
	speed: 0.0201s/iter; left time: 457.8429s
Epoch: 13 cost time: 7.106086492538452
Epoch: 13, Steps: 261 | Train Loss: 0.5214128 Vali Loss: 0.2662554 Test Loss: 0.3699194
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4827150
	speed: 0.1067s/iter; left time: 2411.3591s
	iters: 200, epoch: 14 | loss: 0.4540328
	speed: 0.0275s/iter; left time: 619.2694s
Epoch: 14 cost time: 6.488312482833862
Epoch: 14, Steps: 261 | Train Loss: 0.5210281 Vali Loss: 0.2660311 Test Loss: 0.3698287
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.6308022
	speed: 0.1269s/iter; left time: 2836.1031s
	iters: 200, epoch: 15 | loss: 0.6673588
	speed: 0.0326s/iter; left time: 725.0650s
Epoch: 15 cost time: 8.884310483932495
Epoch: 15, Steps: 261 | Train Loss: 0.5205152 Vali Loss: 0.2656049 Test Loss: 0.3697430
Validation loss decreased (0.265773 --> 0.265605).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3940434
	speed: 0.1306s/iter; left time: 2884.8215s
	iters: 200, epoch: 16 | loss: 0.7076522
	speed: 0.0309s/iter; left time: 678.5959s
Epoch: 16 cost time: 7.955672979354858
Epoch: 16, Steps: 261 | Train Loss: 0.5203534 Vali Loss: 0.2655656 Test Loss: 0.3697054
Validation loss decreased (0.265605 --> 0.265566).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5165705
	speed: 0.1326s/iter; left time: 2894.7093s
	iters: 200, epoch: 17 | loss: 0.4285605
	speed: 0.0257s/iter; left time: 558.2641s
Epoch: 17 cost time: 6.583231449127197
Epoch: 17, Steps: 261 | Train Loss: 0.5197354 Vali Loss: 0.2653746 Test Loss: 0.3696379
Validation loss decreased (0.265566 --> 0.265375).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5336299
	speed: 0.0975s/iter; left time: 2101.5847s
	iters: 200, epoch: 18 | loss: 0.4967465
	speed: 0.0175s/iter; left time: 376.0545s
Epoch: 18 cost time: 5.610271215438843
Epoch: 18, Steps: 261 | Train Loss: 0.5198862 Vali Loss: 0.2654746 Test Loss: 0.3696249
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.6207060
	speed: 0.0863s/iter; left time: 1838.3283s
	iters: 200, epoch: 19 | loss: 0.7480972
	speed: 0.0175s/iter; left time: 370.6644s
Epoch: 19 cost time: 5.068763494491577
Epoch: 19, Steps: 261 | Train Loss: 0.5189074 Vali Loss: 0.2652849 Test Loss: 0.3695928
Validation loss decreased (0.265375 --> 0.265285).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.6334392
	speed: 0.0884s/iter; left time: 1859.4789s
	iters: 200, epoch: 20 | loss: 0.4677081
	speed: 0.0175s/iter; left time: 365.8958s
Epoch: 20 cost time: 5.943487167358398
Epoch: 20, Steps: 261 | Train Loss: 0.5194666 Vali Loss: 0.2653013 Test Loss: 0.3695475
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4069607
	speed: 0.0946s/iter; left time: 1966.2069s
	iters: 200, epoch: 21 | loss: 0.4511859
	speed: 0.0278s/iter; left time: 574.6523s
Epoch: 21 cost time: 7.2682905197143555
Epoch: 21, Steps: 261 | Train Loss: 0.5192416 Vali Loss: 0.2651757 Test Loss: 0.3695282
Validation loss decreased (0.265285 --> 0.265176).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4487333
	speed: 0.1131s/iter; left time: 2321.3714s
	iters: 200, epoch: 22 | loss: 0.5915855
	speed: 0.0216s/iter; left time: 440.3584s
Epoch: 22 cost time: 6.206056833267212
Epoch: 22, Steps: 261 | Train Loss: 0.5192494 Vali Loss: 0.2650821 Test Loss: 0.3695092
Validation loss decreased (0.265176 --> 0.265082).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5185569
	speed: 0.0829s/iter; left time: 1680.0713s
	iters: 200, epoch: 23 | loss: 0.4625365
	speed: 0.0232s/iter; left time: 467.8524s
Epoch: 23 cost time: 7.45857048034668
Epoch: 23, Steps: 261 | Train Loss: 0.5190371 Vali Loss: 0.2650352 Test Loss: 0.3694997
Validation loss decreased (0.265082 --> 0.265035).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.6062656
	speed: 0.1165s/iter; left time: 2330.4416s
	iters: 200, epoch: 24 | loss: 0.4489197
	speed: 0.0167s/iter; left time: 331.7186s
Epoch: 24 cost time: 5.5632452964782715
Epoch: 24, Steps: 261 | Train Loss: 0.5192056 Vali Loss: 0.2653474 Test Loss: 0.3694927
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4674452
	speed: 0.0853s/iter; left time: 1683.3438s
	iters: 200, epoch: 25 | loss: 0.4886055
	speed: 0.0163s/iter; left time: 320.2634s
Epoch: 25 cost time: 6.236770391464233
Epoch: 25, Steps: 261 | Train Loss: 0.5193320 Vali Loss: 0.2651185 Test Loss: 0.3694646
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.5888943
	speed: 0.0954s/iter; left time: 1857.4420s
	iters: 200, epoch: 26 | loss: 0.4600502
	speed: 0.0179s/iter; left time: 347.3842s
Epoch: 26 cost time: 5.150821685791016
Epoch: 26, Steps: 261 | Train Loss: 0.5193577 Vali Loss: 0.2650568 Test Loss: 0.3694611
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4676515
	speed: 0.0907s/iter; left time: 1743.7599s
	iters: 200, epoch: 27 | loss: 0.5421414
	speed: 0.0163s/iter; left time: 311.5383s
Epoch: 27 cost time: 4.947176218032837
Epoch: 27, Steps: 261 | Train Loss: 0.5189056 Vali Loss: 0.2648843 Test Loss: 0.3694617
Validation loss decreased (0.265035 --> 0.264884).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3702997
	speed: 0.0880s/iter; left time: 1668.4645s
	iters: 200, epoch: 28 | loss: 0.4517220
	speed: 0.0170s/iter; left time: 320.3342s
Epoch: 28 cost time: 4.901377439498901
Epoch: 28, Steps: 261 | Train Loss: 0.5190995 Vali Loss: 0.2651522 Test Loss: 0.3694704
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3968218
	speed: 0.1102s/iter; left time: 2059.6895s
	iters: 200, epoch: 29 | loss: 0.5082239
	speed: 0.0175s/iter; left time: 324.9185s
Epoch: 29 cost time: 5.914517164230347
Epoch: 29, Steps: 261 | Train Loss: 0.5187549 Vali Loss: 0.2650452 Test Loss: 0.3694345
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.5219150
	speed: 0.0830s/iter; left time: 1530.7077s
	iters: 200, epoch: 30 | loss: 0.5553172
	speed: 0.0208s/iter; left time: 380.7636s
Epoch: 30 cost time: 5.566134214401245
Epoch: 30, Steps: 261 | Train Loss: 0.5192822 Vali Loss: 0.2650669 Test Loss: 0.3694303
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.6251499
	speed: 0.0810s/iter; left time: 1472.4316s
	iters: 200, epoch: 31 | loss: 0.3764971
	speed: 0.0165s/iter; left time: 298.6642s
Epoch: 31 cost time: 4.928466558456421
Epoch: 31, Steps: 261 | Train Loss: 0.5187033 Vali Loss: 0.2648507 Test Loss: 0.3694181
Validation loss decreased (0.264884 --> 0.264851).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.5288050
	speed: 0.0841s/iter; left time: 1506.1664s
	iters: 200, epoch: 32 | loss: 0.4703110
	speed: 0.0329s/iter; left time: 585.4791s
Epoch: 32 cost time: 7.148536682128906
Epoch: 32, Steps: 261 | Train Loss: 0.5186198 Vali Loss: 0.2648555 Test Loss: 0.3694254
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4690289
	speed: 0.0980s/iter; left time: 1730.3917s
	iters: 200, epoch: 33 | loss: 0.5772485
	speed: 0.0161s/iter; left time: 283.1856s
Epoch: 33 cost time: 5.425070762634277
Epoch: 33, Steps: 261 | Train Loss: 0.5184710 Vali Loss: 0.2649059 Test Loss: 0.3694212
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.5703223
	speed: 0.0901s/iter; left time: 1566.9150s
	iters: 200, epoch: 34 | loss: 0.4730956
	speed: 0.0175s/iter; left time: 302.0579s
Epoch: 34 cost time: 6.194740295410156
Epoch: 34, Steps: 261 | Train Loss: 0.5189741 Vali Loss: 0.2651144 Test Loss: 0.3694164
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.7238770
	speed: 0.0965s/iter; left time: 1653.5183s
	iters: 200, epoch: 35 | loss: 0.6626833
	speed: 0.0177s/iter; left time: 300.9782s
Epoch: 35 cost time: 5.111621856689453
Epoch: 35, Steps: 261 | Train Loss: 0.5188655 Vali Loss: 0.2647087 Test Loss: 0.3694093
Validation loss decreased (0.264851 --> 0.264709).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.6241855
	speed: 0.0901s/iter; left time: 1520.4121s
	iters: 200, epoch: 36 | loss: 0.4460877
	speed: 0.0212s/iter; left time: 354.6426s
Epoch: 36 cost time: 6.3633434772491455
Epoch: 36, Steps: 261 | Train Loss: 0.5182172 Vali Loss: 0.2649372 Test Loss: 0.3693995
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3539267
	speed: 0.0872s/iter; left time: 1448.2230s
	iters: 200, epoch: 37 | loss: 0.3589697
	speed: 0.0208s/iter; left time: 343.5622s
Epoch: 37 cost time: 5.461513996124268
Epoch: 37, Steps: 261 | Train Loss: 0.5183469 Vali Loss: 0.2650502 Test Loss: 0.3694012
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4930534
	speed: 0.0951s/iter; left time: 1554.9482s
	iters: 200, epoch: 38 | loss: 0.4180127
	speed: 0.0197s/iter; left time: 319.2371s
Epoch: 38 cost time: 5.94077730178833
Epoch: 38, Steps: 261 | Train Loss: 0.5184148 Vali Loss: 0.2647363 Test Loss: 0.3693953
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4846977
	speed: 0.0862s/iter; left time: 1385.9368s
	iters: 200, epoch: 39 | loss: 0.5005166
	speed: 0.0249s/iter; left time: 398.4977s
Epoch: 39 cost time: 7.568646669387817
Epoch: 39, Steps: 261 | Train Loss: 0.5190338 Vali Loss: 0.2651240 Test Loss: 0.3694021
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.5301585
	speed: 0.1001s/iter; left time: 1583.7929s
	iters: 200, epoch: 40 | loss: 0.5826266
	speed: 0.0171s/iter; left time: 269.6030s
Epoch: 40 cost time: 5.168409109115601
Epoch: 40, Steps: 261 | Train Loss: 0.5185622 Vali Loss: 0.2648131 Test Loss: 0.3693836
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.6729787
	speed: 0.1072s/iter; left time: 1667.4400s
	iters: 200, epoch: 41 | loss: 0.3687254
	speed: 0.0229s/iter; left time: 353.3928s
Epoch: 41 cost time: 7.79243803024292
Epoch: 41, Steps: 261 | Train Loss: 0.5190156 Vali Loss: 0.2650575 Test Loss: 0.3693888
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.5200593
	speed: 0.1081s/iter; left time: 1653.3990s
	iters: 200, epoch: 42 | loss: 0.6273937
	speed: 0.0169s/iter; left time: 256.4928s
Epoch: 42 cost time: 6.346070766448975
Epoch: 42, Steps: 261 | Train Loss: 0.5189683 Vali Loss: 0.2648143 Test Loss: 0.3693862
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.5037148
	speed: 0.0780s/iter; left time: 1173.5408s
	iters: 200, epoch: 43 | loss: 0.5100901
	speed: 0.0167s/iter; left time: 248.8901s
Epoch: 43 cost time: 4.944449186325073
Epoch: 43, Steps: 261 | Train Loss: 0.5185792 Vali Loss: 0.2648095 Test Loss: 0.3693788
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4985601
	speed: 0.0889s/iter; left time: 1314.3283s
	iters: 200, epoch: 44 | loss: 0.6152999
	speed: 0.0299s/iter; left time: 438.2180s
Epoch: 44 cost time: 6.9607994556427
Epoch: 44, Steps: 261 | Train Loss: 0.5184758 Vali Loss: 0.2651269 Test Loss: 0.3693829
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5637051
	speed: 0.0911s/iter; left time: 1322.0860s
	iters: 200, epoch: 45 | loss: 0.5560303
	speed: 0.0167s/iter; left time: 240.4159s
Epoch: 45 cost time: 5.038790702819824
Epoch: 45, Steps: 261 | Train Loss: 0.5190191 Vali Loss: 0.2649070 Test Loss: 0.3693919
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.5012307
	speed: 0.0843s/iter; left time: 1202.1244s
	iters: 200, epoch: 46 | loss: 0.3512595
	speed: 0.0179s/iter; left time: 253.0348s
Epoch: 46 cost time: 4.920875549316406
Epoch: 46, Steps: 261 | Train Loss: 0.5186853 Vali Loss: 0.2648730 Test Loss: 0.3693837
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4890465
	speed: 0.0832s/iter; left time: 1163.9100s
	iters: 200, epoch: 47 | loss: 0.4693530
	speed: 0.0350s/iter; left time: 486.3943s
Epoch: 47 cost time: 7.172679424285889
Epoch: 47, Steps: 261 | Train Loss: 0.5183329 Vali Loss: 0.2646261 Test Loss: 0.3693842
Validation loss decreased (0.264709 --> 0.264626).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4942744
	speed: 0.1065s/iter; left time: 1463.1305s
	iters: 200, epoch: 48 | loss: 0.5118441
	speed: 0.0411s/iter; left time: 560.7260s
Epoch: 48 cost time: 10.195494651794434
Epoch: 48, Steps: 261 | Train Loss: 0.5177276 Vali Loss: 0.2643954 Test Loss: 0.3693706
Validation loss decreased (0.264626 --> 0.264395).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4352087
	speed: 0.0997s/iter; left time: 1342.6206s
	iters: 200, epoch: 49 | loss: 0.4051187
	speed: 0.0175s/iter; left time: 234.6458s
Epoch: 49 cost time: 5.43096661567688
Epoch: 49, Steps: 261 | Train Loss: 0.5187755 Vali Loss: 0.2649467 Test Loss: 0.3693703
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.6082703
	speed: 0.0934s/iter; left time: 1233.9023s
	iters: 200, epoch: 50 | loss: 0.3412459
	speed: 0.0166s/iter; left time: 217.0409s
Epoch: 50 cost time: 6.059654235839844
Epoch: 50, Steps: 261 | Train Loss: 0.5188183 Vali Loss: 0.2647130 Test Loss: 0.3693711
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4497868
	speed: 0.1102s/iter; left time: 1427.4546s
	iters: 200, epoch: 51 | loss: 0.5156817
	speed: 0.0345s/iter; left time: 442.9948s
Epoch: 51 cost time: 7.739182710647583
Epoch: 51, Steps: 261 | Train Loss: 0.5190526 Vali Loss: 0.2647001 Test Loss: 0.3693744
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4329718
	speed: 0.1128s/iter; left time: 1431.3266s
	iters: 200, epoch: 52 | loss: 0.4014157
	speed: 0.0458s/iter; left time: 576.1693s
Epoch: 52 cost time: 10.46021318435669
Epoch: 52, Steps: 261 | Train Loss: 0.5187497 Vali Loss: 0.2647440 Test Loss: 0.3693763
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.5788922
	speed: 0.1004s/iter; left time: 1247.7358s
	iters: 200, epoch: 53 | loss: 0.4974115
	speed: 0.0175s/iter; left time: 215.9052s
Epoch: 53 cost time: 5.175713300704956
Epoch: 53, Steps: 261 | Train Loss: 0.5186673 Vali Loss: 0.2649157 Test Loss: 0.3693712
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.7664616
	speed: 0.0878s/iter; left time: 1068.0713s
	iters: 200, epoch: 54 | loss: 0.6501791
	speed: 0.0229s/iter; left time: 276.8788s
Epoch: 54 cost time: 6.039019346237183
Epoch: 54, Steps: 261 | Train Loss: 0.5182777 Vali Loss: 0.2649998 Test Loss: 0.3693713
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.6494541
	speed: 0.0768s/iter; left time: 914.0225s
	iters: 200, epoch: 55 | loss: 0.4889663
	speed: 0.0169s/iter; left time: 200.0200s
Epoch: 55 cost time: 4.90474271774292
Epoch: 55, Steps: 261 | Train Loss: 0.5183859 Vali Loss: 0.2648118 Test Loss: 0.3693706
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.5350367
	speed: 0.0810s/iter; left time: 943.4777s
	iters: 200, epoch: 56 | loss: 0.4631190
	speed: 0.0211s/iter; left time: 243.2704s
Epoch: 56 cost time: 5.396969795227051
Epoch: 56, Steps: 261 | Train Loss: 0.5186387 Vali Loss: 0.2644049 Test Loss: 0.3693713
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.5203583
	speed: 0.0804s/iter; left time: 915.3086s
	iters: 200, epoch: 57 | loss: 0.5557635
	speed: 0.0172s/iter; left time: 193.8814s
Epoch: 57 cost time: 5.06883978843689
Epoch: 57, Steps: 261 | Train Loss: 0.5186505 Vali Loss: 0.2649519 Test Loss: 0.3693694
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.4239985
	speed: 0.0875s/iter; left time: 973.1440s
	iters: 200, epoch: 58 | loss: 0.4794500
	speed: 0.0163s/iter; left time: 179.7244s
Epoch: 58 cost time: 5.490381956100464
Epoch: 58, Steps: 261 | Train Loss: 0.5186429 Vali Loss: 0.2647280 Test Loss: 0.3693705
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.5318368
	speed: 0.0951s/iter; left time: 1033.2432s
	iters: 200, epoch: 59 | loss: 0.5272585
	speed: 0.0186s/iter; left time: 200.3372s
Epoch: 59 cost time: 6.314425230026245
Epoch: 59, Steps: 261 | Train Loss: 0.5186305 Vali Loss: 0.2647978 Test Loss: 0.3693708
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.7048529
	speed: 0.1074s/iter; left time: 1138.2686s
	iters: 200, epoch: 60 | loss: 0.4945620
	speed: 0.0193s/iter; left time: 202.7974s
Epoch: 60 cost time: 5.824755668640137
Epoch: 60, Steps: 261 | Train Loss: 0.5183586 Vali Loss: 0.2647754 Test Loss: 0.3693694
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.5686666
	speed: 0.0888s/iter; left time: 917.9205s
	iters: 200, epoch: 61 | loss: 0.7173383
	speed: 0.0169s/iter; left time: 172.9117s
Epoch: 61 cost time: 5.019986867904663
Epoch: 61, Steps: 261 | Train Loss: 0.5179557 Vali Loss: 0.2648999 Test Loss: 0.3693695
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.5560046
	speed: 0.0813s/iter; left time: 819.1379s
	iters: 200, epoch: 62 | loss: 0.3369946
	speed: 0.0247s/iter; left time: 246.3829s
Epoch: 62 cost time: 6.518938302993774
Epoch: 62, Steps: 261 | Train Loss: 0.5184463 Vali Loss: 0.2650077 Test Loss: 0.3693689
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.5903190
	speed: 0.0879s/iter; left time: 863.5020s
	iters: 200, epoch: 63 | loss: 0.5911928
	speed: 0.0197s/iter; left time: 191.6922s
Epoch: 63 cost time: 5.394630193710327
Epoch: 63, Steps: 261 | Train Loss: 0.5189292 Vali Loss: 0.2648748 Test Loss: 0.3693675
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.5618520
	speed: 0.0888s/iter; left time: 848.2829s
	iters: 200, epoch: 64 | loss: 0.5083865
	speed: 0.0242s/iter; left time: 228.7090s
Epoch: 64 cost time: 6.032492160797119
Epoch: 64, Steps: 261 | Train Loss: 0.5179170 Vali Loss: 0.2649444 Test Loss: 0.3693673
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.5173242
	speed: 0.0862s/iter; left time: 801.6202s
	iters: 200, epoch: 65 | loss: 0.6674703
	speed: 0.0163s/iter; left time: 150.2548s
Epoch: 65 cost time: 4.796002626419067
Epoch: 65, Steps: 261 | Train Loss: 0.5183522 Vali Loss: 0.2649759 Test Loss: 0.3693684
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.4963058
	speed: 0.0845s/iter; left time: 763.9100s
	iters: 200, epoch: 66 | loss: 0.5483850
	speed: 0.0169s/iter; left time: 150.5741s
Epoch: 66 cost time: 5.609037399291992
Epoch: 66, Steps: 261 | Train Loss: 0.5184310 Vali Loss: 0.2649076 Test Loss: 0.3693687
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.5408700
	speed: 0.0851s/iter; left time: 746.3888s
	iters: 200, epoch: 67 | loss: 0.5452684
	speed: 0.0271s/iter; left time: 235.0019s
Epoch: 67 cost time: 6.4069836139678955
Epoch: 67, Steps: 261 | Train Loss: 0.5179421 Vali Loss: 0.2648777 Test Loss: 0.3693630
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.5413467
	speed: 0.0941s/iter; left time: 800.9955s
	iters: 200, epoch: 68 | loss: 0.5815721
	speed: 0.0191s/iter; left time: 160.5063s
Epoch: 68 cost time: 6.566258430480957
Epoch: 68, Steps: 261 | Train Loss: 0.5185841 Vali Loss: 0.2648769 Test Loss: 0.3693649
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm2_360_720_FITS_ETTm2_ftM_sl360_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.3666454255580902, mae:0.38276606798171997, rse:0.48670694231987, corr:[0.53569824 0.5405507  0.54115605 0.53944093 0.53743476 0.5360306
 0.5355569  0.5357695  0.53626263 0.53665704 0.5366268  0.5362024
 0.53556573 0.53489697 0.5343688  0.5339813  0.53373736 0.53354204
 0.533288   0.53287864 0.53234285 0.53178525 0.5312407  0.5308245
 0.5305087  0.5302148  0.5299189  0.5295985  0.52925014 0.52883124
 0.5283513  0.52787566 0.52741116 0.5269495  0.5264786  0.52599484
 0.52549046 0.5249801  0.5244849  0.5239922  0.5234716  0.5229253
 0.5223336  0.5216836  0.52098984 0.5202251  0.51945096 0.5187078
 0.5179749  0.5172483  0.5164827  0.51574063 0.5149867  0.5142103
 0.5134511  0.5127712  0.51219636 0.51175416 0.51145214 0.5112302
 0.5110418  0.5108219  0.51055783 0.5102321  0.509873   0.5095608
 0.5093012  0.5091292  0.5090488  0.5090345  0.5090041  0.5088887
 0.5086495  0.50829935 0.5078632  0.5073626  0.5068756  0.5064121
 0.5059989  0.50558716 0.505172   0.50468516 0.5041234  0.5034596
 0.50277525 0.50215167 0.501603   0.50117147 0.5008144  0.5004576
 0.50002223 0.4994227  0.49855813 0.49740207 0.49599272 0.49439928
 0.49271685 0.49112946 0.4897109  0.48842773 0.4872167  0.48606318
 0.48492727 0.48374793 0.48249003 0.48123232 0.48005697 0.47902668
 0.47814825 0.4773559  0.47662285 0.47587055 0.47508356 0.47424152
 0.4733751  0.47253883 0.471773   0.47108087 0.47049794 0.46993795
 0.469336   0.4686201  0.46785334 0.46705198 0.46623674 0.46542224
 0.46464816 0.46388465 0.4630941  0.46227667 0.46147254 0.46069437
 0.4599494  0.45925325 0.45860752 0.45802143 0.45741016 0.45678687
 0.4561004  0.45534736 0.45454    0.45368674 0.4528335  0.45196834
 0.45111826 0.45030177 0.4495602  0.44887918 0.44820416 0.44748116
 0.44673803 0.4460264  0.44533518 0.44470084 0.4441584  0.44373026
 0.44336966 0.4430131  0.4425608  0.4420526  0.44146857 0.44085968
 0.44033512 0.4399573  0.43978882 0.43979907 0.4398354  0.4398481
 0.43971682 0.43942264 0.4389791  0.43845263 0.4379018  0.4374221
 0.4370487  0.43678218 0.4365461  0.43625712 0.4358515  0.43532473
 0.4346872  0.4340002  0.4334048  0.43294892 0.43263876 0.4324056
 0.4321389  0.43167475 0.43089843 0.42972556 0.4282366  0.42646235
 0.42456263 0.4228192  0.42126042 0.41983652 0.41852787 0.41722837
 0.4158864  0.41443682 0.41286767 0.41124937 0.40968472 0.40828013
 0.40708318 0.4061071  0.40524116 0.40441403 0.4035491  0.40266
 0.40170252 0.40066406 0.3996156  0.39866832 0.39781284 0.39703545
 0.3962625  0.39526048 0.39409816 0.39284793 0.39158383 0.39037716
 0.38933513 0.38845092 0.38773414 0.3870241  0.38622558 0.3852541
 0.38415045 0.38302988 0.38196993 0.38103697 0.38026512 0.37966728
 0.37915164 0.37864834 0.37815028 0.3775962  0.37698513 0.37637547
 0.37578583 0.37524867 0.37477896 0.37450314 0.37427032 0.3740468
 0.37386587 0.37370032 0.37353724 0.37338936 0.37328142 0.3732626
 0.37323302 0.37323177 0.37315002 0.37297234 0.3726953  0.37235987
 0.37209246 0.37195164 0.3719749  0.37210208 0.37217823 0.37215328
 0.37201452 0.3717697  0.37144583 0.3710477  0.37069023 0.37046152
 0.37034765 0.37027934 0.37018153 0.37004423 0.36976665 0.3693494
 0.36877048 0.36811438 0.3675183  0.36711296 0.366918   0.3669115
 0.3669352  0.36682406 0.36644194 0.36569476 0.3645407  0.36309382
 0.36156672 0.36025918 0.3592513  0.35850197 0.3579418  0.35746062
 0.35691044 0.35622424 0.35542667 0.35451716 0.35364908 0.3529729
 0.35253733 0.35224116 0.3520431  0.35183954 0.3515147  0.35101524
 0.35046747 0.34990168 0.3494016  0.34912512 0.3490977  0.34927207
 0.34945506 0.34950888 0.3492981  0.34883475 0.34819493 0.34748396
 0.3468424  0.34633744 0.3459796  0.345769   0.34566006 0.34557104
 0.34540096 0.34516755 0.34487772 0.34451926 0.34409985 0.34373277
 0.34346625 0.34329993 0.34321514 0.34311002 0.3428976  0.34260798
 0.3422042  0.34171867 0.3412208  0.34080926 0.34050244 0.3403136
 0.34022075 0.3402616  0.3404005  0.3405518  0.34068984 0.34076998
 0.34078822 0.34072474 0.3405438  0.34029594 0.3400656  0.33987877
 0.3397571  0.33974904 0.33979252 0.33982855 0.33979    0.33957916
 0.339203   0.33873165 0.3381867  0.3376765  0.3372653  0.33705574
 0.33700678 0.33709326 0.3372144  0.3372641  0.33721805 0.33700442
 0.3366851  0.33639225 0.33624423 0.33626926 0.33643833 0.33668983
 0.3368871  0.33688986 0.33656684 0.33580282 0.3347177  0.3334078
 0.33204836 0.33085254 0.32991982 0.32922506 0.32862198 0.32803383
 0.3273219  0.32640415 0.32534856 0.3242161  0.3231256  0.3221963
 0.32152766 0.321146   0.3208362  0.32047814 0.31998327 0.31928885
 0.31849137 0.3176678  0.31699374 0.31659263 0.31648546 0.31662866
 0.31686458 0.3169939  0.31692028 0.31663537 0.31616127 0.31560624
 0.31508008 0.31468615 0.31443393 0.31429657 0.31421852 0.3141573
 0.31408015 0.31401    0.3139233  0.3138179  0.31372005 0.31364474
 0.31358168 0.31355828 0.3135563  0.31350535 0.31338513 0.31316188
 0.3128194  0.31239694 0.3119198  0.31149033 0.3111227  0.31083262
 0.31057322 0.31037736 0.31015065 0.30993178 0.3096847  0.30943498
 0.309163   0.3088907  0.30863407 0.30834916 0.30805087 0.30775237
 0.3075627  0.30748776 0.30747196 0.3074635  0.30738932 0.30719644
 0.30688494 0.3064863  0.30601436 0.30556262 0.30522516 0.30497068
 0.30480024 0.30466452 0.3045326  0.30431896 0.30391163 0.3033642
 0.30270848 0.3020377  0.30140677 0.30087936 0.30048314 0.30014482
 0.29975158 0.29916638 0.2983371  0.29723477 0.29589182 0.29437983
 0.2928748  0.29152784 0.29038563 0.28944987 0.28862002 0.28779635
 0.28691489 0.28597292 0.28500775 0.28411815 0.2833297  0.282637
 0.28205386 0.2815607  0.2810844  0.28053418 0.2798775  0.27909434
 0.27826077 0.27744752 0.27673802 0.2762222  0.27593148 0.27583665
 0.27581018 0.27565336 0.275336   0.27486023 0.27434084 0.27380776
 0.27334422 0.27299178 0.2727364  0.27253574 0.27234262 0.27207014
 0.27172127 0.27127257 0.2707688  0.2702795  0.26984072 0.26951095
 0.26926652 0.2691042  0.2689521  0.26875356 0.2684841  0.2681
 0.26761135 0.26710245 0.26660702 0.26618677 0.26589045 0.26573673
 0.2656763  0.26564977 0.26566666 0.2656499  0.26557145 0.26545563
 0.26526156 0.26503962 0.2648013  0.2645895  0.26438877 0.2642286
 0.26409453 0.26401123 0.26395717 0.2639033  0.26375914 0.26355883
 0.2632904  0.26299182 0.26272902 0.26250648 0.26234913 0.26222897
 0.2620937  0.2619127  0.2616624  0.26132765 0.26088148 0.2603844
 0.25992933 0.25957188 0.25936508 0.25923824 0.25916335 0.25901982
 0.2586832  0.25803348 0.25699294 0.2555902  0.25397614 0.25226587
 0.25062323 0.24925311 0.24816957 0.24728529 0.24645412 0.24558094
 0.24450958 0.24330872 0.24203412 0.24079172 0.23971923 0.23890261
 0.23825105 0.23766072 0.23708817 0.23647764 0.2357313  0.23485914
 0.23397319 0.23319314 0.23262388 0.23228616 0.23218668 0.23220758
 0.23219615 0.2319999  0.23154682 0.23088199 0.23012908 0.22943677
 0.22891526 0.22861123 0.22847334 0.22838801 0.22826694 0.22800323
 0.22761263 0.22707418 0.22648102 0.2259782  0.22564514 0.2255562
 0.22561367 0.22574224 0.22579996 0.22571659 0.22548063 0.22510621
 0.22464544 0.22413574 0.22370431 0.22346923 0.22342542 0.22345544
 0.22352137 0.22360666 0.22366232 0.22358699 0.22343715 0.22322331
 0.22313659 0.22314017 0.22317229 0.22323054 0.22322123 0.22313394
 0.22289346 0.22259891 0.22229141 0.22197948 0.221828   0.22168843
 0.2215917  0.22152898 0.2214599  0.22140668 0.22135271 0.22124462
 0.22117107 0.22115915 0.22117819 0.22131246 0.22154866 0.22180115
 0.2221184  0.22252665 0.22300355 0.22345454 0.22388226 0.22420794
 0.2243416  0.22422649 0.22384664 0.22309831 0.22198163 0.22063603
 0.21918666 0.21781537 0.2165931  0.21551652 0.21459506 0.21389188
 0.21328172 0.21275821 0.21236846 0.2120406  0.21167192 0.21125789
 0.21085611 0.21039373 0.20990703 0.20940341 0.20901804 0.20866205
 0.2084321  0.20827962 0.20816056 0.20818408 0.20828456 0.2083566
 0.20832232 0.20798782 0.20739505 0.20667875 0.20603663 0.20550784
 0.20517787 0.2048671  0.20448565 0.20399751 0.203287   0.20248857
 0.20161296 0.20092916 0.2005768  0.20050679 0.20050722 0.20044824
 0.20021303 0.19972932 0.19904357 0.19876692 0.19931231 0.20062396]
