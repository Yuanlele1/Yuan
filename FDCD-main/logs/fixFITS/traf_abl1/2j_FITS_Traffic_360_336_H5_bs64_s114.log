Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j336_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j336_H5_FITS_custom_ftM_sl360_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11585
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=90, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1727861760.0
params:  15834.0
Trainable parameters:  15834
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 95.58515858650208
Epoch: 1, Steps: 90 | Train Loss: 1.2556735 Vali Loss: 1.3329333 Test Loss: 1.5748233
Validation loss decreased (inf --> 1.332933).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 88.4439857006073
Epoch: 2, Steps: 90 | Train Loss: 0.9274091 Vali Loss: 1.1277865 Test Loss: 1.3276646
Validation loss decreased (1.332933 --> 1.127787).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 89.970144033432
Epoch: 3, Steps: 90 | Train Loss: 0.7951344 Vali Loss: 1.0325946 Test Loss: 1.2135332
Validation loss decreased (1.127787 --> 1.032595).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 93.18283104896545
Epoch: 4, Steps: 90 | Train Loss: 0.7201203 Vali Loss: 0.9687313 Test Loss: 1.1377096
Validation loss decreased (1.032595 --> 0.968731).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 91.97254395484924
Epoch: 5, Steps: 90 | Train Loss: 0.6640578 Vali Loss: 0.9152327 Test Loss: 1.0745906
Validation loss decreased (0.968731 --> 0.915233).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 91.89692950248718
Epoch: 6, Steps: 90 | Train Loss: 0.6171877 Vali Loss: 0.8677091 Test Loss: 1.0191622
Validation loss decreased (0.915233 --> 0.867709).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 88.91169214248657
Epoch: 7, Steps: 90 | Train Loss: 0.5766401 Vali Loss: 0.8277727 Test Loss: 0.9724729
Validation loss decreased (0.867709 --> 0.827773).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 90.84571552276611
Epoch: 8, Steps: 90 | Train Loss: 0.5412432 Vali Loss: 0.7919424 Test Loss: 0.9297029
Validation loss decreased (0.827773 --> 0.791942).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 90.92380023002625
Epoch: 9, Steps: 90 | Train Loss: 0.5099418 Vali Loss: 0.7601966 Test Loss: 0.8920848
Validation loss decreased (0.791942 --> 0.760197).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 89.43432855606079
Epoch: 10, Steps: 90 | Train Loss: 0.4822508 Vali Loss: 0.7315810 Test Loss: 0.8586758
Validation loss decreased (0.760197 --> 0.731581).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 91.80797529220581
Epoch: 11, Steps: 90 | Train Loss: 0.4575084 Vali Loss: 0.7036911 Test Loss: 0.8268983
Validation loss decreased (0.731581 --> 0.703691).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 88.28175330162048
Epoch: 12, Steps: 90 | Train Loss: 0.4353415 Vali Loss: 0.6804978 Test Loss: 0.7993015
Validation loss decreased (0.703691 --> 0.680498).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 91.312824010849
Epoch: 13, Steps: 90 | Train Loss: 0.4154826 Vali Loss: 0.6591088 Test Loss: 0.7748206
Validation loss decreased (0.680498 --> 0.659109).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 90.08416199684143
Epoch: 14, Steps: 90 | Train Loss: 0.3975498 Vali Loss: 0.6396477 Test Loss: 0.7522153
Validation loss decreased (0.659109 --> 0.639648).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 93.01869893074036
Epoch: 15, Steps: 90 | Train Loss: 0.3813043 Vali Loss: 0.6226507 Test Loss: 0.7324014
Validation loss decreased (0.639648 --> 0.622651).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 93.47370862960815
Epoch: 16, Steps: 90 | Train Loss: 0.3666680 Vali Loss: 0.6056257 Test Loss: 0.7127426
Validation loss decreased (0.622651 --> 0.605626).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 89.64466834068298
Epoch: 17, Steps: 90 | Train Loss: 0.3533761 Vali Loss: 0.5913424 Test Loss: 0.6961486
Validation loss decreased (0.605626 --> 0.591342).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 87.09107613563538
Epoch: 18, Steps: 90 | Train Loss: 0.3411898 Vali Loss: 0.5777776 Test Loss: 0.6808918
Validation loss decreased (0.591342 --> 0.577778).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 90.62771272659302
Epoch: 19, Steps: 90 | Train Loss: 0.3300987 Vali Loss: 0.5652551 Test Loss: 0.6665608
Validation loss decreased (0.577778 --> 0.565255).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 96.93125700950623
Epoch: 20, Steps: 90 | Train Loss: 0.3199783 Vali Loss: 0.5549240 Test Loss: 0.6546164
Validation loss decreased (0.565255 --> 0.554924).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 87.11899852752686
Epoch: 21, Steps: 90 | Train Loss: 0.3106989 Vali Loss: 0.5435046 Test Loss: 0.6419180
Validation loss decreased (0.554924 --> 0.543505).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 89.49092435836792
Epoch: 22, Steps: 90 | Train Loss: 0.3021870 Vali Loss: 0.5352731 Test Loss: 0.6320714
Validation loss decreased (0.543505 --> 0.535273).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 96.38536405563354
Epoch: 23, Steps: 90 | Train Loss: 0.2943513 Vali Loss: 0.5260248 Test Loss: 0.6217302
Validation loss decreased (0.535273 --> 0.526025).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 90.06397223472595
Epoch: 24, Steps: 90 | Train Loss: 0.2871250 Vali Loss: 0.5182124 Test Loss: 0.6125708
Validation loss decreased (0.526025 --> 0.518212).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 90.03726577758789
Epoch: 25, Steps: 90 | Train Loss: 0.2804853 Vali Loss: 0.5107028 Test Loss: 0.6040916
Validation loss decreased (0.518212 --> 0.510703).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 88.24375486373901
Epoch: 26, Steps: 90 | Train Loss: 0.2743229 Vali Loss: 0.5038044 Test Loss: 0.5964291
Validation loss decreased (0.510703 --> 0.503804).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 92.7919135093689
Epoch: 27, Steps: 90 | Train Loss: 0.2686569 Vali Loss: 0.4976332 Test Loss: 0.5892451
Validation loss decreased (0.503804 --> 0.497633).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 93.98835039138794
Epoch: 28, Steps: 90 | Train Loss: 0.2633690 Vali Loss: 0.4914750 Test Loss: 0.5825723
Validation loss decreased (0.497633 --> 0.491475).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 93.03947019577026
Epoch: 29, Steps: 90 | Train Loss: 0.2585232 Vali Loss: 0.4859131 Test Loss: 0.5761362
Validation loss decreased (0.491475 --> 0.485913).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 94.11488103866577
Epoch: 30, Steps: 90 | Train Loss: 0.2539707 Vali Loss: 0.4814845 Test Loss: 0.5710980
Validation loss decreased (0.485913 --> 0.481485).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 94.15137672424316
Epoch: 31, Steps: 90 | Train Loss: 0.2497625 Vali Loss: 0.4760842 Test Loss: 0.5653194
Validation loss decreased (0.481485 --> 0.476084).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 92.94395518302917
Epoch: 32, Steps: 90 | Train Loss: 0.2458608 Vali Loss: 0.4719062 Test Loss: 0.5603058
Validation loss decreased (0.476084 --> 0.471906).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 94.3766815662384
Epoch: 33, Steps: 90 | Train Loss: 0.2422068 Vali Loss: 0.4679873 Test Loss: 0.5559900
Validation loss decreased (0.471906 --> 0.467987).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 94.83957862854004
Epoch: 34, Steps: 90 | Train Loss: 0.2387741 Vali Loss: 0.4644819 Test Loss: 0.5515488
Validation loss decreased (0.467987 --> 0.464482).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 93.20418620109558
Epoch: 35, Steps: 90 | Train Loss: 0.2355880 Vali Loss: 0.4600386 Test Loss: 0.5473791
Validation loss decreased (0.464482 --> 0.460039).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 90.54842042922974
Epoch: 36, Steps: 90 | Train Loss: 0.2326386 Vali Loss: 0.4572558 Test Loss: 0.5436083
Validation loss decreased (0.460039 --> 0.457256).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 94.5615062713623
Epoch: 37, Steps: 90 | Train Loss: 0.2298149 Vali Loss: 0.4538110 Test Loss: 0.5400278
Validation loss decreased (0.457256 --> 0.453811).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 94.45327305793762
Epoch: 38, Steps: 90 | Train Loss: 0.2272296 Vali Loss: 0.4508647 Test Loss: 0.5368321
Validation loss decreased (0.453811 --> 0.450865).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 87.53706884384155
Epoch: 39, Steps: 90 | Train Loss: 0.2247562 Vali Loss: 0.4482864 Test Loss: 0.5339263
Validation loss decreased (0.450865 --> 0.448286).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 91.37585091590881
Epoch: 40, Steps: 90 | Train Loss: 0.2224858 Vali Loss: 0.4450260 Test Loss: 0.5308532
Validation loss decreased (0.448286 --> 0.445026).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 89.33555126190186
Epoch: 41, Steps: 90 | Train Loss: 0.2202876 Vali Loss: 0.4429775 Test Loss: 0.5284210
Validation loss decreased (0.445026 --> 0.442978).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 87.6444845199585
Epoch: 42, Steps: 90 | Train Loss: 0.2182741 Vali Loss: 0.4407313 Test Loss: 0.5257713
Validation loss decreased (0.442978 --> 0.440731).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 85.85807251930237
Epoch: 43, Steps: 90 | Train Loss: 0.2163894 Vali Loss: 0.4387163 Test Loss: 0.5234704
Validation loss decreased (0.440731 --> 0.438716).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 85.3384153842926
Epoch: 44, Steps: 90 | Train Loss: 0.2145993 Vali Loss: 0.4368381 Test Loss: 0.5211091
Validation loss decreased (0.438716 --> 0.436838).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 87.2710165977478
Epoch: 45, Steps: 90 | Train Loss: 0.2128723 Vali Loss: 0.4349407 Test Loss: 0.5190626
Validation loss decreased (0.436838 --> 0.434941).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 86.42168354988098
Epoch: 46, Steps: 90 | Train Loss: 0.2113207 Vali Loss: 0.4330021 Test Loss: 0.5170097
Validation loss decreased (0.434941 --> 0.433002).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 86.8094093799591
Epoch: 47, Steps: 90 | Train Loss: 0.2098123 Vali Loss: 0.4311742 Test Loss: 0.5152552
Validation loss decreased (0.433002 --> 0.431174).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 90.84706830978394
Epoch: 48, Steps: 90 | Train Loss: 0.2083998 Vali Loss: 0.4296068 Test Loss: 0.5133661
Validation loss decreased (0.431174 --> 0.429607).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 85.3638482093811
Epoch: 49, Steps: 90 | Train Loss: 0.2070585 Vali Loss: 0.4282310 Test Loss: 0.5118324
Validation loss decreased (0.429607 --> 0.428231).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 86.65884923934937
Epoch: 50, Steps: 90 | Train Loss: 0.2057915 Vali Loss: 0.4269245 Test Loss: 0.5102190
Validation loss decreased (0.428231 --> 0.426924).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 83.37033581733704
Epoch: 51, Steps: 90 | Train Loss: 0.2046399 Vali Loss: 0.4250065 Test Loss: 0.5086386
Validation loss decreased (0.426924 --> 0.425006).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 89.60484027862549
Epoch: 52, Steps: 90 | Train Loss: 0.2035215 Vali Loss: 0.4237944 Test Loss: 0.5072445
Validation loss decreased (0.425006 --> 0.423794).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 85.77871060371399
Epoch: 53, Steps: 90 | Train Loss: 0.2024446 Vali Loss: 0.4226136 Test Loss: 0.5059487
Validation loss decreased (0.423794 --> 0.422614).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 82.88416171073914
Epoch: 54, Steps: 90 | Train Loss: 0.2014468 Vali Loss: 0.4216757 Test Loss: 0.5047381
Validation loss decreased (0.422614 --> 0.421676).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 85.41318154335022
Epoch: 55, Steps: 90 | Train Loss: 0.2004795 Vali Loss: 0.4205094 Test Loss: 0.5035505
Validation loss decreased (0.421676 --> 0.420509).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 92.86624908447266
Epoch: 56, Steps: 90 | Train Loss: 0.1995762 Vali Loss: 0.4192067 Test Loss: 0.5024005
Validation loss decreased (0.420509 --> 0.419207).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 90.86258101463318
Epoch: 57, Steps: 90 | Train Loss: 0.1987272 Vali Loss: 0.4182524 Test Loss: 0.5014262
Validation loss decreased (0.419207 --> 0.418252).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 86.70490765571594
Epoch: 58, Steps: 90 | Train Loss: 0.1979478 Vali Loss: 0.4178303 Test Loss: 0.5003780
Validation loss decreased (0.418252 --> 0.417830).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 90.05140542984009
Epoch: 59, Steps: 90 | Train Loss: 0.1971981 Vali Loss: 0.4166640 Test Loss: 0.4994099
Validation loss decreased (0.417830 --> 0.416664).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 91.19628620147705
Epoch: 60, Steps: 90 | Train Loss: 0.1964153 Vali Loss: 0.4158544 Test Loss: 0.4985146
Validation loss decreased (0.416664 --> 0.415854).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 89.10132217407227
Epoch: 61, Steps: 90 | Train Loss: 0.1957619 Vali Loss: 0.4154449 Test Loss: 0.4976621
Validation loss decreased (0.415854 --> 0.415445).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 89.96606206893921
Epoch: 62, Steps: 90 | Train Loss: 0.1951085 Vali Loss: 0.4142491 Test Loss: 0.4968251
Validation loss decreased (0.415445 --> 0.414249).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 91.73182559013367
Epoch: 63, Steps: 90 | Train Loss: 0.1945263 Vali Loss: 0.4137193 Test Loss: 0.4961252
Validation loss decreased (0.414249 --> 0.413719).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 89.3468406200409
Epoch: 64, Steps: 90 | Train Loss: 0.1939388 Vali Loss: 0.4129908 Test Loss: 0.4954087
Validation loss decreased (0.413719 --> 0.412991).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 87.42396974563599
Epoch: 65, Steps: 90 | Train Loss: 0.1934034 Vali Loss: 0.4125231 Test Loss: 0.4946915
Validation loss decreased (0.412991 --> 0.412523).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 91.01497220993042
Epoch: 66, Steps: 90 | Train Loss: 0.1928706 Vali Loss: 0.4118311 Test Loss: 0.4940873
Validation loss decreased (0.412523 --> 0.411831).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 92.88730049133301
Epoch: 67, Steps: 90 | Train Loss: 0.1923829 Vali Loss: 0.4110321 Test Loss: 0.4934450
Validation loss decreased (0.411831 --> 0.411032).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 92.93004775047302
Epoch: 68, Steps: 90 | Train Loss: 0.1919015 Vali Loss: 0.4106540 Test Loss: 0.4928866
Validation loss decreased (0.411032 --> 0.410654).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 89.6276068687439
Epoch: 69, Steps: 90 | Train Loss: 0.1914709 Vali Loss: 0.4098702 Test Loss: 0.4922893
Validation loss decreased (0.410654 --> 0.409870).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 88.47933959960938
Epoch: 70, Steps: 90 | Train Loss: 0.1909852 Vali Loss: 0.4097099 Test Loss: 0.4917715
Validation loss decreased (0.409870 --> 0.409710).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 88.88925504684448
Epoch: 71, Steps: 90 | Train Loss: 0.1906104 Vali Loss: 0.4090326 Test Loss: 0.4913010
Validation loss decreased (0.409710 --> 0.409033).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 85.26238703727722
Epoch: 72, Steps: 90 | Train Loss: 0.1902326 Vali Loss: 0.4089438 Test Loss: 0.4907940
Validation loss decreased (0.409033 --> 0.408944).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 86.220285654068
Epoch: 73, Steps: 90 | Train Loss: 0.1898706 Vali Loss: 0.4082796 Test Loss: 0.4903612
Validation loss decreased (0.408944 --> 0.408280).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 85.7751772403717
Epoch: 74, Steps: 90 | Train Loss: 0.1895290 Vali Loss: 0.4078221 Test Loss: 0.4899403
Validation loss decreased (0.408280 --> 0.407822).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 91.12156939506531
Epoch: 75, Steps: 90 | Train Loss: 0.1891841 Vali Loss: 0.4075615 Test Loss: 0.4895180
Validation loss decreased (0.407822 --> 0.407562).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 90.79660177230835
Epoch: 76, Steps: 90 | Train Loss: 0.1888637 Vali Loss: 0.4069328 Test Loss: 0.4891414
Validation loss decreased (0.407562 --> 0.406933).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 88.71475648880005
Epoch: 77, Steps: 90 | Train Loss: 0.1885632 Vali Loss: 0.4067219 Test Loss: 0.4887708
Validation loss decreased (0.406933 --> 0.406722).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 92.17371988296509
Epoch: 78, Steps: 90 | Train Loss: 0.1883177 Vali Loss: 0.4067586 Test Loss: 0.4884188
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 93.76486468315125
Epoch: 79, Steps: 90 | Train Loss: 0.1880506 Vali Loss: 0.4064382 Test Loss: 0.4880960
Validation loss decreased (0.406722 --> 0.406438).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 91.12533283233643
Epoch: 80, Steps: 90 | Train Loss: 0.1877890 Vali Loss: 0.4057925 Test Loss: 0.4878021
Validation loss decreased (0.406438 --> 0.405793).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 89.49689888954163
Epoch: 81, Steps: 90 | Train Loss: 0.1875200 Vali Loss: 0.4054928 Test Loss: 0.4874805
Validation loss decreased (0.405793 --> 0.405493).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 94.33824396133423
Epoch: 82, Steps: 90 | Train Loss: 0.1872879 Vali Loss: 0.4052711 Test Loss: 0.4872019
Validation loss decreased (0.405493 --> 0.405271).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 90.24793314933777
Epoch: 83, Steps: 90 | Train Loss: 0.1870722 Vali Loss: 0.4047734 Test Loss: 0.4869455
Validation loss decreased (0.405271 --> 0.404773).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 88.11602020263672
Epoch: 84, Steps: 90 | Train Loss: 0.1868626 Vali Loss: 0.4049720 Test Loss: 0.4866807
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 91.26748085021973
Epoch: 85, Steps: 90 | Train Loss: 0.1866885 Vali Loss: 0.4045716 Test Loss: 0.4864447
Validation loss decreased (0.404773 --> 0.404572).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 92.33849167823792
Epoch: 86, Steps: 90 | Train Loss: 0.1864946 Vali Loss: 0.4040682 Test Loss: 0.4862124
Validation loss decreased (0.404572 --> 0.404068).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 91.25416684150696
Epoch: 87, Steps: 90 | Train Loss: 0.1863087 Vali Loss: 0.4039389 Test Loss: 0.4859864
Validation loss decreased (0.404068 --> 0.403939).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 87.1229248046875
Epoch: 88, Steps: 90 | Train Loss: 0.1861691 Vali Loss: 0.4041500 Test Loss: 0.4857848
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 90.35517477989197
Epoch: 89, Steps: 90 | Train Loss: 0.1859871 Vali Loss: 0.4040468 Test Loss: 0.4855863
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 89.4382209777832
Epoch: 90, Steps: 90 | Train Loss: 0.1858404 Vali Loss: 0.4037858 Test Loss: 0.4853926
Validation loss decreased (0.403939 --> 0.403786).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 87.69915175437927
Epoch: 91, Steps: 90 | Train Loss: 0.1856658 Vali Loss: 0.4033544 Test Loss: 0.4852243
Validation loss decreased (0.403786 --> 0.403354).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 85.77521467208862
Epoch: 92, Steps: 90 | Train Loss: 0.1855552 Vali Loss: 0.4033714 Test Loss: 0.4850583
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 79.0728235244751
Epoch: 93, Steps: 90 | Train Loss: 0.1854137 Vali Loss: 0.4029382 Test Loss: 0.4848983
Validation loss decreased (0.403354 --> 0.402938).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 72.24646496772766
Epoch: 94, Steps: 90 | Train Loss: 0.1853111 Vali Loss: 0.4027417 Test Loss: 0.4847448
Validation loss decreased (0.402938 --> 0.402742).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 73.16702365875244
Epoch: 95, Steps: 90 | Train Loss: 0.1851835 Vali Loss: 0.4028981 Test Loss: 0.4846003
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 71.83769392967224
Epoch: 96, Steps: 90 | Train Loss: 0.1850663 Vali Loss: 0.4026803 Test Loss: 0.4844662
Validation loss decreased (0.402742 --> 0.402680).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 74.3993730545044
Epoch: 97, Steps: 90 | Train Loss: 0.1849726 Vali Loss: 0.4025958 Test Loss: 0.4843341
Validation loss decreased (0.402680 --> 0.402596).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 73.15111923217773
Epoch: 98, Steps: 90 | Train Loss: 0.1848850 Vali Loss: 0.4027183 Test Loss: 0.4842182
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 69.04002451896667
Epoch: 99, Steps: 90 | Train Loss: 0.1847426 Vali Loss: 0.4023868 Test Loss: 0.4840952
Validation loss decreased (0.402596 --> 0.402387).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 74.7708466053009
Epoch: 100, Steps: 90 | Train Loss: 0.1846570 Vali Loss: 0.4021298 Test Loss: 0.4839804
Validation loss decreased (0.402387 --> 0.402130).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 11585
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=90, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1727861760.0
params:  15834.0
Trainable parameters:  15834
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 72.37336039543152
Epoch: 1, Steps: 90 | Train Loss: 0.2842701 Vali Loss: 0.3664999 Test Loss: 0.4485879
Validation loss decreased (inf --> 0.366500).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 70.79647994041443
Epoch: 2, Steps: 90 | Train Loss: 0.2707705 Vali Loss: 0.3608977 Test Loss: 0.4447708
Validation loss decreased (0.366500 --> 0.360898).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 70.08159828186035
Epoch: 3, Steps: 90 | Train Loss: 0.2692767 Vali Loss: 0.3606553 Test Loss: 0.4449137
Validation loss decreased (0.360898 --> 0.360655).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 71.79880785942078
Epoch: 4, Steps: 90 | Train Loss: 0.2691210 Vali Loss: 0.3605599 Test Loss: 0.4449697
Validation loss decreased (0.360655 --> 0.360560).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 74.36986756324768
Epoch: 5, Steps: 90 | Train Loss: 0.2690106 Vali Loss: 0.3602536 Test Loss: 0.4447842
Validation loss decreased (0.360560 --> 0.360254).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 69.96495294570923
Epoch: 6, Steps: 90 | Train Loss: 0.2690403 Vali Loss: 0.3602493 Test Loss: 0.4448322
Validation loss decreased (0.360254 --> 0.360249).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 71.25956606864929
Epoch: 7, Steps: 90 | Train Loss: 0.2690478 Vali Loss: 0.3601432 Test Loss: 0.4448806
Validation loss decreased (0.360249 --> 0.360143).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 72.4997251033783
Epoch: 8, Steps: 90 | Train Loss: 0.2689978 Vali Loss: 0.3603576 Test Loss: 0.4450027
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 72.87521004676819
Epoch: 9, Steps: 90 | Train Loss: 0.2689414 Vali Loss: 0.3603398 Test Loss: 0.4449054
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 71.48106455802917
Epoch: 10, Steps: 90 | Train Loss: 0.2689890 Vali Loss: 0.3599323 Test Loss: 0.4447274
Validation loss decreased (0.360143 --> 0.359932).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 61.68860459327698
Epoch: 11, Steps: 90 | Train Loss: 0.2689087 Vali Loss: 0.3599389 Test Loss: 0.4447718
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 58.05681920051575
Epoch: 12, Steps: 90 | Train Loss: 0.2688732 Vali Loss: 0.3601494 Test Loss: 0.4444767
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 55.63539528846741
Epoch: 13, Steps: 90 | Train Loss: 0.2688906 Vali Loss: 0.3602548 Test Loss: 0.4447823
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 55.21026372909546
Epoch: 14, Steps: 90 | Train Loss: 0.2688533 Vali Loss: 0.3601213 Test Loss: 0.4445871
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 58.432719230651855
Epoch: 15, Steps: 90 | Train Loss: 0.2688635 Vali Loss: 0.3596253 Test Loss: 0.4445673
Validation loss decreased (0.359932 --> 0.359625).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 56.58456540107727
Epoch: 16, Steps: 90 | Train Loss: 0.2688915 Vali Loss: 0.3602006 Test Loss: 0.4445254
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 54.158123254776
Epoch: 17, Steps: 90 | Train Loss: 0.2688067 Vali Loss: 0.3597609 Test Loss: 0.4444083
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 57.444050312042236
Epoch: 18, Steps: 90 | Train Loss: 0.2687515 Vali Loss: 0.3600660 Test Loss: 0.4445108
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 58.75660729408264
Epoch: 19, Steps: 90 | Train Loss: 0.2687995 Vali Loss: 0.3597901 Test Loss: 0.4447422
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 54.97513818740845
Epoch: 20, Steps: 90 | Train Loss: 0.2688203 Vali Loss: 0.3600232 Test Loss: 0.4445748
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 57.63786244392395
Epoch: 21, Steps: 90 | Train Loss: 0.2688225 Vali Loss: 0.3601541 Test Loss: 0.4445129
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 58.65193247795105
Epoch: 22, Steps: 90 | Train Loss: 0.2688164 Vali Loss: 0.3600646 Test Loss: 0.4446011
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 57.03646278381348
Epoch: 23, Steps: 90 | Train Loss: 0.2687505 Vali Loss: 0.3601216 Test Loss: 0.4445198
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 53.7330117225647
Epoch: 24, Steps: 90 | Train Loss: 0.2687195 Vali Loss: 0.3601241 Test Loss: 0.4444869
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 56.63497972488403
Epoch: 25, Steps: 90 | Train Loss: 0.2687277 Vali Loss: 0.3602053 Test Loss: 0.4444196
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j336_H5_FITS_custom_ftM_sl360_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.44212809205055237, mae:0.30153951048851013, rse:0.546478807926178, corr:[0.27931926 0.28545466 0.286202   0.28657934 0.28622892 0.28593478
 0.28627434 0.28670356 0.2866088  0.28624713 0.28622627 0.28646323
 0.2864914  0.28620747 0.28599793 0.2860764  0.28621867 0.2861922
 0.2859981  0.2858294  0.28570747 0.2855172  0.2854382  0.28604332
 0.28698197 0.28713804 0.28691414 0.286589   0.2863138  0.28624573
 0.2863108  0.28623423 0.28601503 0.28588188 0.28588083 0.28592536
 0.28603068 0.28618115 0.2863249  0.28635055 0.28627262 0.2862515
 0.28635874 0.28636083 0.28617173 0.2858533  0.28567284 0.28600118
 0.28658852 0.28679165 0.286693   0.286392   0.2859837  0.2856158
 0.28544834 0.28555635 0.2858465  0.2861463  0.286257   0.28617924
 0.2862123  0.28652197 0.2869658  0.28721657 0.28709537 0.28670764
 0.28643358 0.28608912 0.28562906 0.28521517 0.28505704 0.2852802
 0.28550175 0.28544316 0.28518805 0.28494048 0.28480545 0.28471884
 0.2846364  0.28459772 0.28469682 0.28495273 0.285232   0.2853698
 0.28540426 0.28544304 0.2855452  0.2856508  0.2857022  0.28571823
 0.28577137 0.28564194 0.28536555 0.28510645 0.28498855 0.28500167
 0.2849418  0.28486374 0.2848217  0.28489047 0.28502536 0.28510022
 0.28507966 0.28503144 0.28507257 0.285211   0.28528157 0.28514698
 0.2849116  0.28476888 0.28482667 0.285007   0.28514433 0.28519395
 0.28521773 0.28507906 0.28483254 0.28465354 0.2847937  0.28507352
 0.2851089  0.2849632  0.28484145 0.28490648 0.285174   0.28540057
 0.2853768  0.2851268  0.28487605 0.28482962 0.28497818 0.28517514
 0.28527012 0.28520557 0.28512517 0.2851582  0.28529322 0.28535563
 0.2853152  0.28519595 0.28512433 0.2852547  0.28558648 0.28604984
 0.28628397 0.28635344 0.28647688 0.28664625 0.2867987  0.28686267
 0.2867949  0.28664327 0.28651738 0.28650495 0.28660646 0.2867653
 0.2869064  0.28694507 0.28685486 0.28670248 0.28659025 0.28652996
 0.2864522  0.2862843  0.28607714 0.28598782 0.28616887 0.28672108
 0.28713843 0.28681174 0.28657657 0.2866244  0.28684413 0.2870971
 0.287245   0.28724244 0.2871648  0.2870971  0.28704923 0.2869824
 0.2868716  0.28666255 0.28645587 0.2864065  0.28658637 0.28689533
 0.28702933 0.28690407 0.28661364 0.2863365  0.28626078 0.2865304
 0.28689525 0.28681    0.28661057 0.2864824  0.2863776  0.28627363
 0.28620228 0.28618947 0.2861933  0.2861127  0.28590992 0.28577736
 0.2859211  0.28623646 0.28645936 0.2863892  0.286113   0.28586984
 0.28573588 0.28565526 0.2855599  0.28542584 0.2852764  0.2852563
 0.28530186 0.28522873 0.285219   0.28527233 0.2852308  0.2850705
 0.28494188 0.28497067 0.28509516 0.28516126 0.28509665 0.28503913
 0.2851934  0.28548634 0.28568494 0.28562546 0.28538758 0.2851781
 0.2850797  0.2850422  0.28508088 0.28500473 0.28487462 0.28484252
 0.2848593  0.28484738 0.28486153 0.2849084  0.28494295 0.2849314
 0.2848745  0.28482395 0.28482994 0.28486124 0.28482676 0.2846996
 0.2845783  0.28453717 0.28457853 0.28465542 0.28471506 0.28470612
 0.28449932 0.28412196 0.28382483 0.28379533 0.28394797 0.2840724
 0.28403994 0.28401583 0.28412136 0.28434086 0.28458458 0.28479108
 0.284911   0.28488857 0.28471273 0.28447774 0.2843082  0.28422445
 0.2842174  0.2842002  0.28415143 0.28410748 0.28412563 0.2841923
 0.28418437 0.28398144 0.28362605 0.28331926 0.28324342 0.2832176
 0.28300306 0.2829463  0.28314793 0.28340945 0.28357315 0.28358784
 0.28348902 0.2833491  0.28326592 0.2833198  0.28349516 0.28365633
 0.28363943 0.28345278 0.28333852 0.28345874 0.28370345 0.28376085
 0.2835777  0.2833999  0.28347194 0.28366718 0.28380737 0.28387547
 0.28391019 0.28422832 0.2847352  0.2849636  0.2849247  0.28489193
 0.28489378 0.2847382  0.2844182  0.2842558  0.2845078  0.2849574
 0.2851764  0.2851197  0.2851498  0.28540778 0.2855655  0.2853076
 0.2848753  0.2847784  0.28488255 0.28446624 0.28376678 0.28502455]
