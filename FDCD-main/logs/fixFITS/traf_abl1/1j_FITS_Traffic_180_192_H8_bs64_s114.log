Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j192_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j192_H8_FITS_custom_ftM_sl180_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11909
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=74, out_features=152, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1241059328.0
params:  11400.0
Trainable parameters:  11400
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 83.49051666259766
Epoch: 1, Steps: 93 | Train Loss: 0.8905512 Vali Loss: 0.7316654 Test Loss: 0.8860003
Validation loss decreased (inf --> 0.731665).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 76.33813953399658
Epoch: 2, Steps: 93 | Train Loss: 0.4796879 Vali Loss: 0.5378987 Test Loss: 0.6610402
Validation loss decreased (0.731665 --> 0.537899).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 75.564120054245
Epoch: 3, Steps: 93 | Train Loss: 0.3856134 Vali Loss: 0.4716614 Test Loss: 0.5809214
Validation loss decreased (0.537899 --> 0.471661).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 83.79934287071228
Epoch: 4, Steps: 93 | Train Loss: 0.3447393 Vali Loss: 0.4355961 Test Loss: 0.5371237
Validation loss decreased (0.471661 --> 0.435596).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 81.8289852142334
Epoch: 5, Steps: 93 | Train Loss: 0.3211351 Vali Loss: 0.4140365 Test Loss: 0.5107019
Validation loss decreased (0.435596 --> 0.414037).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 79.31336045265198
Epoch: 6, Steps: 93 | Train Loss: 0.3065593 Vali Loss: 0.4003601 Test Loss: 0.4944107
Validation loss decreased (0.414037 --> 0.400360).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 81.56706833839417
Epoch: 7, Steps: 93 | Train Loss: 0.2973813 Vali Loss: 0.3915104 Test Loss: 0.4842577
Validation loss decreased (0.400360 --> 0.391510).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 87.61387491226196
Epoch: 8, Steps: 93 | Train Loss: 0.2914562 Vali Loss: 0.3863132 Test Loss: 0.4777207
Validation loss decreased (0.391510 --> 0.386313).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 79.67378449440002
Epoch: 9, Steps: 93 | Train Loss: 0.2875864 Vali Loss: 0.3823386 Test Loss: 0.4736045
Validation loss decreased (0.386313 --> 0.382339).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 71.46749258041382
Epoch: 10, Steps: 93 | Train Loss: 0.2850354 Vali Loss: 0.3798202 Test Loss: 0.4709852
Validation loss decreased (0.382339 --> 0.379820).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 72.0345070362091
Epoch: 11, Steps: 93 | Train Loss: 0.2832990 Vali Loss: 0.3783856 Test Loss: 0.4690941
Validation loss decreased (0.379820 --> 0.378386).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 71.49288702011108
Epoch: 12, Steps: 93 | Train Loss: 0.2821240 Vali Loss: 0.3770460 Test Loss: 0.4679167
Validation loss decreased (0.378386 --> 0.377046).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 63.943655490875244
Epoch: 13, Steps: 93 | Train Loss: 0.2812857 Vali Loss: 0.3764001 Test Loss: 0.4670229
Validation loss decreased (0.377046 --> 0.376400).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 69.92140293121338
Epoch: 14, Steps: 93 | Train Loss: 0.2806790 Vali Loss: 0.3757476 Test Loss: 0.4664968
Validation loss decreased (0.376400 --> 0.375748).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 71.9885687828064
Epoch: 15, Steps: 93 | Train Loss: 0.2802774 Vali Loss: 0.3748333 Test Loss: 0.4660418
Validation loss decreased (0.375748 --> 0.374833).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 87.91143774986267
Epoch: 16, Steps: 93 | Train Loss: 0.2799167 Vali Loss: 0.3746807 Test Loss: 0.4657333
Validation loss decreased (0.374833 --> 0.374681).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 96.62870001792908
Epoch: 17, Steps: 93 | Train Loss: 0.2796759 Vali Loss: 0.3739056 Test Loss: 0.4654824
Validation loss decreased (0.374681 --> 0.373906).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 102.9613904953003
Epoch: 18, Steps: 93 | Train Loss: 0.2795062 Vali Loss: 0.3739863 Test Loss: 0.4653044
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 99.00029945373535
Epoch: 19, Steps: 93 | Train Loss: 0.2793514 Vali Loss: 0.3742056 Test Loss: 0.4651377
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 96.58732318878174
Epoch: 20, Steps: 93 | Train Loss: 0.2792284 Vali Loss: 0.3739591 Test Loss: 0.4650402
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 88.04210424423218
Epoch: 21, Steps: 93 | Train Loss: 0.2791143 Vali Loss: 0.3738073 Test Loss: 0.4649309
Validation loss decreased (0.373906 --> 0.373807).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 84.7363805770874
Epoch: 22, Steps: 93 | Train Loss: 0.2790426 Vali Loss: 0.3736573 Test Loss: 0.4648352
Validation loss decreased (0.373807 --> 0.373657).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 88.89509391784668
Epoch: 23, Steps: 93 | Train Loss: 0.2789596 Vali Loss: 0.3737387 Test Loss: 0.4647450
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 88.9436547756195
Epoch: 24, Steps: 93 | Train Loss: 0.2788903 Vali Loss: 0.3741078 Test Loss: 0.4646251
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 91.6155173778534
Epoch: 25, Steps: 93 | Train Loss: 0.2788263 Vali Loss: 0.3734372 Test Loss: 0.4645987
Validation loss decreased (0.373657 --> 0.373437).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 84.967360496521
Epoch: 26, Steps: 93 | Train Loss: 0.2787840 Vali Loss: 0.3736173 Test Loss: 0.4645214
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 88.1603364944458
Epoch: 27, Steps: 93 | Train Loss: 0.2787398 Vali Loss: 0.3731670 Test Loss: 0.4645038
Validation loss decreased (0.373437 --> 0.373167).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 90.38722467422485
Epoch: 28, Steps: 93 | Train Loss: 0.2786911 Vali Loss: 0.3734536 Test Loss: 0.4643970
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 90.3759171962738
Epoch: 29, Steps: 93 | Train Loss: 0.2786620 Vali Loss: 0.3734540 Test Loss: 0.4643763
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 89.9950909614563
Epoch: 30, Steps: 93 | Train Loss: 0.2786331 Vali Loss: 0.3732578 Test Loss: 0.4643440
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 90.2208104133606
Epoch: 31, Steps: 93 | Train Loss: 0.2786078 Vali Loss: 0.3733815 Test Loss: 0.4643147
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 92.13430953025818
Epoch: 32, Steps: 93 | Train Loss: 0.2785420 Vali Loss: 0.3732690 Test Loss: 0.4642984
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 83.13180327415466
Epoch: 33, Steps: 93 | Train Loss: 0.2785412 Vali Loss: 0.3735944 Test Loss: 0.4642281
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 72.17091298103333
Epoch: 34, Steps: 93 | Train Loss: 0.2785129 Vali Loss: 0.3730905 Test Loss: 0.4642213
Validation loss decreased (0.373167 --> 0.373091).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 106.44684886932373
Epoch: 35, Steps: 93 | Train Loss: 0.2784747 Vali Loss: 0.3731629 Test Loss: 0.4642024
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 115.36591410636902
Epoch: 36, Steps: 93 | Train Loss: 0.2784416 Vali Loss: 0.3730733 Test Loss: 0.4641825
Validation loss decreased (0.373091 --> 0.373073).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 114.21436619758606
Epoch: 37, Steps: 93 | Train Loss: 0.2784783 Vali Loss: 0.3732122 Test Loss: 0.4641601
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 125.10855293273926
Epoch: 38, Steps: 93 | Train Loss: 0.2784213 Vali Loss: 0.3729420 Test Loss: 0.4641272
Validation loss decreased (0.373073 --> 0.372942).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 118.73011183738708
Epoch: 39, Steps: 93 | Train Loss: 0.2784172 Vali Loss: 0.3732976 Test Loss: 0.4641223
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 103.29117774963379
Epoch: 40, Steps: 93 | Train Loss: 0.2784127 Vali Loss: 0.3732758 Test Loss: 0.4640694
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 105.2884521484375
Epoch: 41, Steps: 93 | Train Loss: 0.2783666 Vali Loss: 0.3730297 Test Loss: 0.4640681
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 121.94156527519226
Epoch: 42, Steps: 93 | Train Loss: 0.2784033 Vali Loss: 0.3731387 Test Loss: 0.4640525
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 117.09385585784912
Epoch: 43, Steps: 93 | Train Loss: 0.2783604 Vali Loss: 0.3733171 Test Loss: 0.4640095
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 126.09613585472107
Epoch: 44, Steps: 93 | Train Loss: 0.2783354 Vali Loss: 0.3734693 Test Loss: 0.4640314
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 99.77241063117981
Epoch: 45, Steps: 93 | Train Loss: 0.2783400 Vali Loss: 0.3732443 Test Loss: 0.4640338
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 96.86612892150879
Epoch: 46, Steps: 93 | Train Loss: 0.2783374 Vali Loss: 0.3726058 Test Loss: 0.4639702
Validation loss decreased (0.372942 --> 0.372606).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 98.28566360473633
Epoch: 47, Steps: 93 | Train Loss: 0.2783057 Vali Loss: 0.3729397 Test Loss: 0.4639764
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 106.7898280620575
Epoch: 48, Steps: 93 | Train Loss: 0.2782984 Vali Loss: 0.3728634 Test Loss: 0.4639614
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 108.22940516471863
Epoch: 49, Steps: 93 | Train Loss: 0.2782960 Vali Loss: 0.3740605 Test Loss: 0.4639333
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 112.94712901115417
Epoch: 50, Steps: 93 | Train Loss: 0.2782699 Vali Loss: 0.3729923 Test Loss: 0.4639561
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 90.0423309803009
Epoch: 51, Steps: 93 | Train Loss: 0.2782700 Vali Loss: 0.3729399 Test Loss: 0.4639433
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 111.4039466381073
Epoch: 52, Steps: 93 | Train Loss: 0.2782558 Vali Loss: 0.3729231 Test Loss: 0.4639264
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 115.60855340957642
Epoch: 53, Steps: 93 | Train Loss: 0.2782488 Vali Loss: 0.3731193 Test Loss: 0.4639254
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 110.74446058273315
Epoch: 54, Steps: 93 | Train Loss: 0.2782397 Vali Loss: 0.3735365 Test Loss: 0.4639123
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 94.58374762535095
Epoch: 55, Steps: 93 | Train Loss: 0.2782704 Vali Loss: 0.3730482 Test Loss: 0.4639220
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 77.3640296459198
Epoch: 56, Steps: 93 | Train Loss: 0.2782475 Vali Loss: 0.3731470 Test Loss: 0.4639060
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j192_H8_FITS_custom_ftM_sl180_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.46311938762664795, mae:0.30308687686920166, rse:0.5616626143455505, corr:[0.27595738 0.288146   0.2873781  0.28607553 0.28787917 0.2863524
 0.2882698  0.28764457 0.28766677 0.28817752 0.28759864 0.28772837
 0.28742063 0.2866017  0.28660914 0.286042   0.28597906 0.28602713
 0.28536737 0.28586426 0.28609866 0.28664428 0.28765753 0.28729513
 0.28725263 0.28705162 0.2863497  0.28644294 0.28655753 0.286582
 0.287022   0.28672612 0.28673866 0.28674585 0.28649813 0.28634474
 0.2859266  0.28535303 0.2853614  0.28562877 0.28555527 0.28559425
 0.28538668 0.28553748 0.28559595 0.28570935 0.28613007 0.2859144
 0.28580266 0.28569338 0.28553355 0.2856798  0.28571078 0.28547987
 0.285709   0.28559548 0.2855059  0.28546774 0.2852529  0.28523034
 0.28513956 0.28479713 0.2846093  0.28474352 0.28451195 0.28448623
 0.28453052 0.28454828 0.28459293 0.28466794 0.2849843  0.28496465
 0.2846818  0.28440803 0.28419378 0.2841236  0.28422612 0.28418463
 0.28426987 0.28417644 0.28399882 0.28409597 0.28394723 0.2838656
 0.28390914 0.28393215 0.2839957  0.28414252 0.28408134 0.2838623
 0.2836935  0.28368852 0.28386593 0.28387046 0.28387007 0.28384703
 0.28387398 0.28388512 0.2837942  0.28376088 0.28375715 0.28360876
 0.28340918 0.28321132 0.28292763 0.2829832  0.2829704  0.28298947
 0.28322554 0.28310576 0.28295302 0.28313175 0.28316838 0.2832756
 0.2832829  0.28315228 0.2831538  0.2829012  0.283029   0.28330183
 0.2834074  0.28349236 0.28351307 0.28339854 0.28348187 0.28346533
 0.28298923 0.28278273 0.28266916 0.28272495 0.28302377 0.2832888
 0.28390768 0.2842869  0.2839298  0.28389487 0.28384972 0.28372988
 0.28376788 0.28375372 0.28392532 0.28424188 0.28483063 0.2848172
 0.2845085  0.28432292 0.28416663 0.28430888 0.28455797 0.28449813
 0.2839599  0.28367475 0.28392693 0.28426453 0.28487882 0.28553244
 0.28667393 0.28743365 0.2868636  0.28648558 0.28606516 0.2859186
 0.2857497  0.28579992 0.28632888 0.28713143 0.2883529  0.28809464
 0.28702885 0.286416   0.2859368  0.2855427  0.28567564 0.28573674
 0.28601503 0.28597227 0.28614208 0.28614178 0.2864146  0.28677288
 0.28674757 0.28678665 0.28660637 0.286115   0.2859124  0.2857029
 0.2843468  0.28472137 0.28401306 0.28456578 0.28653812 0.28672168]
