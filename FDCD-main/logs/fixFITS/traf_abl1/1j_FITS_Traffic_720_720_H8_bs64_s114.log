Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10841
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14688811008.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 283.72342467308044
Epoch: 1, Steps: 84 | Train Loss: 1.0610943 Vali Loss: 0.9756333 Test Loss: 1.1439925
Validation loss decreased (inf --> 0.975633).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 261.0932786464691
Epoch: 2, Steps: 84 | Train Loss: 0.6967174 Vali Loss: 0.7735915 Test Loss: 0.9012290
Validation loss decreased (0.975633 --> 0.773592).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 277.192994594574
Epoch: 3, Steps: 84 | Train Loss: 0.5550577 Vali Loss: 0.6470969 Test Loss: 0.7511761
Validation loss decreased (0.773592 --> 0.647097).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 290.1422452926636
Epoch: 4, Steps: 84 | Train Loss: 0.4629562 Vali Loss: 0.5628924 Test Loss: 0.6524122
Validation loss decreased (0.647097 --> 0.562892).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 283.6615381240845
Epoch: 5, Steps: 84 | Train Loss: 0.4009368 Vali Loss: 0.5056076 Test Loss: 0.5861242
Validation loss decreased (0.562892 --> 0.505608).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 283.1335618495941
Epoch: 6, Steps: 84 | Train Loss: 0.3587719 Vali Loss: 0.4671417 Test Loss: 0.5414873
Validation loss decreased (0.505608 --> 0.467142).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 263.13906359672546
Epoch: 7, Steps: 84 | Train Loss: 0.3300709 Vali Loss: 0.4402201 Test Loss: 0.5106707
Validation loss decreased (0.467142 --> 0.440220).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 283.1083183288574
Epoch: 8, Steps: 84 | Train Loss: 0.3108880 Vali Loss: 0.4224573 Test Loss: 0.4902346
Validation loss decreased (0.440220 --> 0.422457).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 270.15226793289185
Epoch: 9, Steps: 84 | Train Loss: 0.2978618 Vali Loss: 0.4112458 Test Loss: 0.4771841
Validation loss decreased (0.422457 --> 0.411246).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 272.53140783309937
Epoch: 10, Steps: 84 | Train Loss: 0.2890381 Vali Loss: 0.4038023 Test Loss: 0.4683968
Validation loss decreased (0.411246 --> 0.403802).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 268.5982630252838
Epoch: 11, Steps: 84 | Train Loss: 0.2832821 Vali Loss: 0.3980021 Test Loss: 0.4623715
Validation loss decreased (0.403802 --> 0.398002).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 267.1464648246765
Epoch: 12, Steps: 84 | Train Loss: 0.2793525 Vali Loss: 0.3946096 Test Loss: 0.4586096
Validation loss decreased (0.398002 --> 0.394610).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 266.2267003059387
Epoch: 13, Steps: 84 | Train Loss: 0.2767071 Vali Loss: 0.3919190 Test Loss: 0.4556629
Validation loss decreased (0.394610 --> 0.391919).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 277.8949990272522
Epoch: 14, Steps: 84 | Train Loss: 0.2749326 Vali Loss: 0.3907284 Test Loss: 0.4545245
Validation loss decreased (0.391919 --> 0.390728).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 261.63411235809326
Epoch: 15, Steps: 84 | Train Loss: 0.2738226 Vali Loss: 0.3894662 Test Loss: 0.4528831
Validation loss decreased (0.390728 --> 0.389466).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 281.1164174079895
Epoch: 16, Steps: 84 | Train Loss: 0.2729591 Vali Loss: 0.3891093 Test Loss: 0.4527359
Validation loss decreased (0.389466 --> 0.389109).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 251.48647022247314
Epoch: 17, Steps: 84 | Train Loss: 0.2723567 Vali Loss: 0.3887199 Test Loss: 0.4520841
Validation loss decreased (0.389109 --> 0.388720).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 217.786687374115
Epoch: 18, Steps: 84 | Train Loss: 0.2720021 Vali Loss: 0.3877088 Test Loss: 0.4514708
Validation loss decreased (0.388720 --> 0.387709).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 239.42164492607117
Epoch: 19, Steps: 84 | Train Loss: 0.2716911 Vali Loss: 0.3884680 Test Loss: 0.4518762
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 218.395920753479
Epoch: 20, Steps: 84 | Train Loss: 0.2715290 Vali Loss: 0.3875313 Test Loss: 0.4508855
Validation loss decreased (0.387709 --> 0.387531).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 195.12057161331177
Epoch: 21, Steps: 84 | Train Loss: 0.2713294 Vali Loss: 0.3870361 Test Loss: 0.4508163
Validation loss decreased (0.387531 --> 0.387036).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 261.34031081199646
Epoch: 22, Steps: 84 | Train Loss: 0.2712474 Vali Loss: 0.3873690 Test Loss: 0.4508661
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 233.83131742477417
Epoch: 23, Steps: 84 | Train Loss: 0.2711129 Vali Loss: 0.3873408 Test Loss: 0.4505698
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 232.88160014152527
Epoch: 24, Steps: 84 | Train Loss: 0.2709766 Vali Loss: 0.3867410 Test Loss: 0.4505921
Validation loss decreased (0.387036 --> 0.386741).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 208.25620102882385
Epoch: 25, Steps: 84 | Train Loss: 0.2709793 Vali Loss: 0.3871503 Test Loss: 0.4504489
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 179.28109622001648
Epoch: 26, Steps: 84 | Train Loss: 0.2708687 Vali Loss: 0.3878495 Test Loss: 0.4507918
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 172.26918745040894
Epoch: 27, Steps: 84 | Train Loss: 0.2708060 Vali Loss: 0.3869242 Test Loss: 0.4507081
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 168.8159945011139
Epoch: 28, Steps: 84 | Train Loss: 0.2708051 Vali Loss: 0.3867871 Test Loss: 0.4506507
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 175.4292130470276
Epoch: 29, Steps: 84 | Train Loss: 0.2707305 Vali Loss: 0.3870720 Test Loss: 0.4504628
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 169.11970806121826
Epoch: 30, Steps: 84 | Train Loss: 0.2707605 Vali Loss: 0.3871046 Test Loss: 0.4503596
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 174.7166223526001
Epoch: 31, Steps: 84 | Train Loss: 0.2707368 Vali Loss: 0.3864122 Test Loss: 0.4504060
Validation loss decreased (0.386741 --> 0.386412).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 173.56358981132507
Epoch: 32, Steps: 84 | Train Loss: 0.2706864 Vali Loss: 0.3868748 Test Loss: 0.4503754
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 160.63874459266663
Epoch: 33, Steps: 84 | Train Loss: 0.2706316 Vali Loss: 0.3867635 Test Loss: 0.4505277
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 141.08942461013794
Epoch: 34, Steps: 84 | Train Loss: 0.2705513 Vali Loss: 0.3869524 Test Loss: 0.4505124
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 146.19438457489014
Epoch: 35, Steps: 84 | Train Loss: 0.2706576 Vali Loss: 0.3864931 Test Loss: 0.4502305
EarlyStopping counter: 4 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 138.45402359962463
Epoch: 36, Steps: 84 | Train Loss: 0.2705568 Vali Loss: 0.3866872 Test Loss: 0.4503324
EarlyStopping counter: 5 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 133.15719199180603
Epoch: 37, Steps: 84 | Train Loss: 0.2704973 Vali Loss: 0.3866549 Test Loss: 0.4503470
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 135.6973283290863
Epoch: 38, Steps: 84 | Train Loss: 0.2705565 Vali Loss: 0.3867304 Test Loss: 0.4503897
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 128.706937789917
Epoch: 39, Steps: 84 | Train Loss: 0.2705753 Vali Loss: 0.3863458 Test Loss: 0.4503176
Validation loss decreased (0.386412 --> 0.386346).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 118.2589201927185
Epoch: 40, Steps: 84 | Train Loss: 0.2705705 Vali Loss: 0.3859052 Test Loss: 0.4503099
Validation loss decreased (0.386346 --> 0.385905).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 132.73634123802185
Epoch: 41, Steps: 84 | Train Loss: 0.2705257 Vali Loss: 0.3865556 Test Loss: 0.4502023
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 124.79031300544739
Epoch: 42, Steps: 84 | Train Loss: 0.2704659 Vali Loss: 0.3863694 Test Loss: 0.4503178
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 121.40509605407715
Epoch: 43, Steps: 84 | Train Loss: 0.2704422 Vali Loss: 0.3863401 Test Loss: 0.4503008
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 124.63395142555237
Epoch: 44, Steps: 84 | Train Loss: 0.2704704 Vali Loss: 0.3861490 Test Loss: 0.4501924
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 135.06133008003235
Epoch: 45, Steps: 84 | Train Loss: 0.2703465 Vali Loss: 0.3863402 Test Loss: 0.4503556
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 135.39190030097961
Epoch: 46, Steps: 84 | Train Loss: 0.2704002 Vali Loss: 0.3863139 Test Loss: 0.4502747
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 128.33937764167786
Epoch: 47, Steps: 84 | Train Loss: 0.2704635 Vali Loss: 0.3862031 Test Loss: 0.4502715
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 266.7279853820801
Epoch: 48, Steps: 84 | Train Loss: 0.2704144 Vali Loss: 0.3860944 Test Loss: 0.4502055
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 298.05383372306824
Epoch: 49, Steps: 84 | Train Loss: 0.2703341 Vali Loss: 0.3864156 Test Loss: 0.4502209
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 252.02826809883118
Epoch: 50, Steps: 84 | Train Loss: 0.2702870 Vali Loss: 0.3862870 Test Loss: 0.4501457
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4497775137424469, mae:0.3009794354438782, rse:0.5484027862548828, corr:[0.26136866 0.25989887 0.26650408 0.26379445 0.26775283 0.27052432
 0.26921135 0.2709423  0.27137515 0.269134   0.2692977  0.2688328
 0.26656866 0.26677138 0.2668726  0.2655757  0.26651523 0.2674656
 0.26682436 0.26752996 0.26831272 0.26787668 0.268291   0.26902768
 0.26981455 0.27061453 0.27125472 0.27037352 0.2699885  0.27026233
 0.26929018 0.26861653 0.2687795  0.26785755 0.26722968 0.26779693
 0.26743737 0.2670497  0.26783276 0.26785123 0.26771465 0.2687537
 0.2690496  0.26859567 0.26903802 0.26922798 0.26873484 0.26917258
 0.26977432 0.26921216 0.2692585  0.26949647 0.26873335 0.26819262
 0.26806375 0.2673987  0.26724255 0.267691   0.26728433 0.26699454
 0.26762828 0.26762426 0.26742652 0.26801646 0.26823983 0.26811278
 0.26853904 0.26867524 0.26840216 0.268601   0.26851982 0.26814985
 0.26831803 0.26834634 0.26784712 0.267845   0.267829   0.26708725
 0.2668508  0.2671151  0.26680294 0.2667512  0.26715067 0.2670008
 0.26697376 0.26757333 0.2676892  0.26763928 0.26828963 0.26850575
 0.26827037 0.26834255 0.2682662  0.2678759  0.26792052 0.2679438
 0.26730952 0.26735142 0.26758516 0.26703548 0.26678726 0.2669395
 0.26666245 0.26658684 0.2668527  0.26678827 0.26685205 0.26727784
 0.26718685 0.2670921  0.26765844 0.26791364 0.2678132  0.26807702
 0.26830822 0.26794118 0.26789373 0.26782483 0.26742128 0.2674902
 0.26773593 0.26769105 0.26780283 0.26787594 0.26736736 0.2671425
 0.26746947 0.26724598 0.2670144  0.26741412 0.2674429  0.26736277
 0.26795563 0.26840064 0.26841795 0.2687868  0.26905438 0.2685487
 0.2687815  0.26889575 0.26831755 0.2680236  0.26808023 0.2677578
 0.2677208  0.2679888  0.26789653 0.2677264  0.267847   0.26775986
 0.26773846 0.26805142 0.2679522  0.26775965 0.26812118 0.2684418
 0.26845786 0.2687619  0.26899073 0.2688476  0.2689846  0.26919174
 0.26905072 0.26899606 0.26907888 0.26887977 0.26878172 0.26906982
 0.26974732 0.26965567 0.26976362 0.26942092 0.26891133 0.26908517
 0.26915452 0.26889086 0.26907665 0.26928884 0.26919132 0.26947936
 0.26986727 0.26983088 0.26996025 0.27018964 0.27010754 0.2702493
 0.27048814 0.27020445 0.2698537  0.26979065 0.269508   0.26940444
 0.27013648 0.27014393 0.26981008 0.26978567 0.26972076 0.26961717
 0.2697563  0.2696529  0.2694632  0.26986295 0.270077   0.2697551
 0.26996297 0.27036506 0.27013034 0.27003714 0.27033055 0.27020147
 0.2700012  0.27008852 0.2698259  0.26948223 0.26945895 0.2693318
 0.2692754  0.26943204 0.26940444 0.26908565 0.2691101  0.26915568
 0.26896563 0.26916292 0.26945814 0.2693135  0.26932728 0.26950058
 0.26934513 0.26936162 0.26967034 0.26952592 0.26933774 0.26958925
 0.26958174 0.26930875 0.26928675 0.2690113  0.26869643 0.26889405
 0.2688569  0.26849234 0.26859465 0.2686111  0.2682075  0.26824424
 0.26856756 0.26854664 0.2687538  0.26931244 0.269408   0.26937926
 0.26961163 0.26956895 0.26941895 0.26950467 0.26941288 0.26933137
 0.2696632  0.2696122  0.26921526 0.26926315 0.2692254  0.26874322
 0.26853007 0.2686926  0.2685813  0.26864925 0.26878586 0.26852518
 0.26847407 0.26881865 0.2688839  0.2690738  0.26949787 0.2695318
 0.26941517 0.26961547 0.26963082 0.26941836 0.26945838 0.26930282
 0.2690747  0.2690966  0.26902884 0.2686369  0.2686082  0.26874563
 0.26843375 0.26859793 0.26899073 0.2688527  0.26883343 0.26917648
 0.26911035 0.26896736 0.26927105 0.26942566 0.26950738 0.26998043
 0.27014083 0.269855   0.26989362 0.2700364  0.26991665 0.2698035
 0.2698731  0.26957104 0.26946992 0.26960787 0.2694737  0.26939252
 0.2696405  0.26959038 0.26956394 0.26975548 0.26964542 0.2694981
 0.26981947 0.2700608  0.27008882 0.27037472 0.27052328 0.27037245
 0.27040097 0.2704105  0.2702349  0.27037856 0.27056232 0.27019253
 0.27001762 0.27007377 0.26981327 0.2696882  0.26983383 0.2698903
 0.27060413 0.27103522 0.27089894 0.2704878  0.27055153 0.27070618
 0.27063206 0.27076477 0.2708933  0.2709149  0.27121443 0.2713396
 0.27103308 0.27104908 0.27123356 0.27096972 0.27089626 0.2711498
 0.2709757  0.27068055 0.27066815 0.27041018 0.26995113 0.27008542
 0.27066565 0.2707213  0.27098238 0.2710834  0.27089316 0.2711065
 0.2712605  0.27094233 0.27096587 0.27121165 0.2709553  0.27085093
 0.27111813 0.2710163  0.27086586 0.27104008 0.2709222  0.27071252
 0.27085856 0.2707435  0.27036467 0.27031407 0.2701161  0.26987648
 0.27024546 0.27045327 0.27017695 0.27010328 0.2702811  0.27027363
 0.27046102 0.270754   0.2706466  0.27064487 0.2708914  0.27074173
 0.2705974  0.27079535 0.27068156 0.2704223  0.27058494 0.27052206
 0.270079   0.26992202 0.26971006 0.2693135  0.26934803 0.26962328
 0.2696246  0.26977822 0.26993704 0.26974767 0.2697879  0.27004
 0.26999846 0.27007568 0.27046224 0.27059713 0.27057198 0.27067083
 0.2704818  0.27017707 0.27021357 0.2700371  0.26963678 0.26964712
 0.26971045 0.26939023 0.26936454 0.26945674 0.26911002 0.2690978
 0.26932526 0.26931813 0.26932675 0.2695284  0.26946387 0.26951045
 0.26980814 0.26981023 0.26988232 0.27038231 0.27052042 0.27017885
 0.27001637 0.26989186 0.26973864 0.26992428 0.26989374 0.2693774
 0.26934692 0.26937783 0.26898697 0.26885876 0.26906115 0.26892564
 0.26888275 0.26941556 0.26955205 0.26951638 0.26974365 0.2696866
 0.26965937 0.27010748 0.2702845  0.2701966  0.2705031  0.27059743
 0.2702235  0.27021196 0.27037033 0.27013776 0.26996824 0.26986387
 0.2695426  0.26948044 0.26967666 0.26954418 0.2695833  0.27002174
 0.2701357  0.27007484 0.2704671  0.27060345 0.2704675  0.27071142
 0.27079406 0.27056563 0.270775   0.27090457 0.2705321  0.27046824
 0.27051133 0.27010897 0.27009875 0.2705315  0.27035117 0.26997042
 0.27004942 0.27001646 0.26995254 0.27021843 0.27015978 0.27005586
 0.27085957 0.27092925 0.27059802 0.2707238  0.27092442 0.2708332
 0.2709621  0.2711335  0.27095738 0.27097774 0.27101323 0.27056676
 0.27033824 0.2704129  0.27004498 0.26974398 0.26989487 0.26966652
 0.26934454 0.26955333 0.26955694 0.26938382 0.26960716 0.26986438
 0.2702644  0.27061012 0.27081612 0.27064767 0.2706641  0.27073592
 0.27049622 0.27055988 0.2707976  0.27054852 0.27036402 0.2705053
 0.27020952 0.26982543 0.26982263 0.2695657  0.269359   0.26961085
 0.26936594 0.26879498 0.26889718 0.26903233 0.26874876 0.2690063
 0.2694164  0.26927155 0.26950958 0.26985836 0.26974568 0.2699031
 0.27019697 0.26992518 0.2697979  0.27001697 0.26968884 0.2693252
 0.2694852  0.26921177 0.26871678 0.26886597 0.268841   0.2684616
 0.2686052  0.26862946 0.2681696  0.26828533 0.26850426 0.26821387
 0.26833275 0.2688256  0.2688174  0.2688704  0.2691399  0.26891035
 0.26879665 0.2689365  0.2684727  0.2681424  0.26848438 0.26827717
 0.2676994  0.2677865  0.26772013 0.2671951  0.26722428 0.26731932
 0.2669933  0.26717246 0.2674782  0.26729748 0.26753762 0.2679854
 0.2675876  0.26747566 0.26797092 0.2679903  0.2680187  0.2683532
 0.26813126 0.26786804 0.26808417 0.26785794 0.26752537 0.26771295
 0.2673593  0.2667008  0.26688814 0.2669343  0.2665198  0.26692402
 0.2672984  0.26677418 0.26685265 0.2674482  0.26742396 0.26751354
 0.26784027 0.267819   0.26797757 0.26852515 0.26831937 0.26802105
 0.26840457 0.2681641  0.26747537 0.26751325 0.2674151  0.26698023
 0.2672421  0.2673207  0.26688302 0.26722294 0.26749504 0.26697737
 0.26714694 0.26768252 0.26744968 0.2676974  0.26849642 0.26840878
 0.2683506  0.26888305 0.26898736 0.26894417 0.26927733 0.2689657
 0.26845032 0.268686   0.26830223 0.26734862 0.2675553  0.2676743
 0.26687858 0.26701832 0.2674426  0.26697513 0.26722872 0.2678423
 0.2675758  0.2679005  0.2686568  0.26845548 0.26856545 0.2695462
 0.27012265 0.2699615  0.27035373 0.2700489  0.26954433 0.26973101
 0.2690287  0.26796862 0.2681626  0.2678377  0.26696807 0.26769114
 0.26802272 0.26720986 0.26808402 0.26914608 0.26886904 0.2696962
 0.27057743 0.26990896 0.2703345  0.27121276 0.27009025 0.26979867
 0.27123517 0.2708461  0.27035147 0.27063212 0.26943105 0.26862285
 0.26888803 0.26713037 0.26634887 0.26820767 0.26772773 0.26755965
 0.2703108  0.27040517 0.27033502 0.2731588  0.27279577 0.27147824
 0.27324998 0.2694824  0.26470727 0.26739967 0.2585251  0.27352816]
