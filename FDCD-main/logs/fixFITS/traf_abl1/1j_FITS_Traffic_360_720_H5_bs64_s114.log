Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j720_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j720_H5_FITS_custom_ftM_sl360_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11201
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=90, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2681164800.0
params:  24570.0
Trainable parameters:  24570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 118.61740326881409
Epoch: 1, Steps: 87 | Train Loss: 1.2725459 Vali Loss: 1.1058643 Test Loss: 1.3056477
Validation loss decreased (inf --> 1.105864).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 114.3929226398468
Epoch: 2, Steps: 87 | Train Loss: 0.7669072 Vali Loss: 0.8380101 Test Loss: 0.9750965
Validation loss decreased (1.105864 --> 0.838010).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 115.45461654663086
Epoch: 3, Steps: 87 | Train Loss: 0.6094295 Vali Loss: 0.7136515 Test Loss: 0.8251158
Validation loss decreased (0.838010 --> 0.713652).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 117.5074565410614
Epoch: 4, Steps: 87 | Train Loss: 0.5203357 Vali Loss: 0.6306745 Test Loss: 0.7267716
Validation loss decreased (0.713652 --> 0.630674).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 110.71188807487488
Epoch: 5, Steps: 87 | Train Loss: 0.4583626 Vali Loss: 0.5715912 Test Loss: 0.6568535
Validation loss decreased (0.630674 --> 0.571591).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 115.47934079170227
Epoch: 6, Steps: 87 | Train Loss: 0.4132356 Vali Loss: 0.5276700 Test Loss: 0.6060614
Validation loss decreased (0.571591 --> 0.527670).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 110.41798782348633
Epoch: 7, Steps: 87 | Train Loss: 0.3801577 Vali Loss: 0.4947668 Test Loss: 0.5689383
Validation loss decreased (0.527670 --> 0.494767).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 109.59792518615723
Epoch: 8, Steps: 87 | Train Loss: 0.3556726 Vali Loss: 0.4711612 Test Loss: 0.5418820
Validation loss decreased (0.494767 --> 0.471161).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 113.88730645179749
Epoch: 9, Steps: 87 | Train Loss: 0.3376474 Vali Loss: 0.4535817 Test Loss: 0.5217680
Validation loss decreased (0.471161 --> 0.453582).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 111.76771354675293
Epoch: 10, Steps: 87 | Train Loss: 0.3244451 Vali Loss: 0.4406699 Test Loss: 0.5073909
Validation loss decreased (0.453582 --> 0.440670).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 112.443692445755
Epoch: 11, Steps: 87 | Train Loss: 0.3147054 Vali Loss: 0.4313803 Test Loss: 0.4969009
Validation loss decreased (0.440670 --> 0.431380).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 111.7472813129425
Epoch: 12, Steps: 87 | Train Loss: 0.3076319 Vali Loss: 0.4240517 Test Loss: 0.4893564
Validation loss decreased (0.431380 --> 0.424052).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 108.92674994468689
Epoch: 13, Steps: 87 | Train Loss: 0.3023081 Vali Loss: 0.4189888 Test Loss: 0.4838826
Validation loss decreased (0.424052 --> 0.418989).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 110.93212342262268
Epoch: 14, Steps: 87 | Train Loss: 0.2984266 Vali Loss: 0.4156907 Test Loss: 0.4798568
Validation loss decreased (0.418989 --> 0.415691).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 109.24769473075867
Epoch: 15, Steps: 87 | Train Loss: 0.2956320 Vali Loss: 0.4127642 Test Loss: 0.4769048
Validation loss decreased (0.415691 --> 0.412764).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 111.89947724342346
Epoch: 16, Steps: 87 | Train Loss: 0.2935213 Vali Loss: 0.4109494 Test Loss: 0.4751512
Validation loss decreased (0.412764 --> 0.410949).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 112.10681319236755
Epoch: 17, Steps: 87 | Train Loss: 0.2920432 Vali Loss: 0.4084600 Test Loss: 0.4735475
Validation loss decreased (0.410949 --> 0.408460).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 108.9851303100586
Epoch: 18, Steps: 87 | Train Loss: 0.2909426 Vali Loss: 0.4080132 Test Loss: 0.4724114
Validation loss decreased (0.408460 --> 0.408013).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 114.05413818359375
Epoch: 19, Steps: 87 | Train Loss: 0.2901782 Vali Loss: 0.4073370 Test Loss: 0.4716921
Validation loss decreased (0.408013 --> 0.407337).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 99.52360916137695
Epoch: 20, Steps: 87 | Train Loss: 0.2895780 Vali Loss: 0.4065019 Test Loss: 0.4713345
Validation loss decreased (0.407337 --> 0.406502).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 91.09449768066406
Epoch: 21, Steps: 87 | Train Loss: 0.2891449 Vali Loss: 0.4063379 Test Loss: 0.4708303
Validation loss decreased (0.406502 --> 0.406338).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 87.90856528282166
Epoch: 22, Steps: 87 | Train Loss: 0.2887826 Vali Loss: 0.4058362 Test Loss: 0.4706356
Validation loss decreased (0.406338 --> 0.405836).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 90.15880727767944
Epoch: 23, Steps: 87 | Train Loss: 0.2885283 Vali Loss: 0.4057440 Test Loss: 0.4704384
Validation loss decreased (0.405836 --> 0.405744).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 90.56887555122375
Epoch: 24, Steps: 87 | Train Loss: 0.2883325 Vali Loss: 0.4058846 Test Loss: 0.4703552
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 89.57875990867615
Epoch: 25, Steps: 87 | Train Loss: 0.2882576 Vali Loss: 0.4054910 Test Loss: 0.4703220
Validation loss decreased (0.405744 --> 0.405491).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 90.22667074203491
Epoch: 26, Steps: 87 | Train Loss: 0.2881577 Vali Loss: 0.4053912 Test Loss: 0.4701833
Validation loss decreased (0.405491 --> 0.405391).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 88.75447940826416
Epoch: 27, Steps: 87 | Train Loss: 0.2879937 Vali Loss: 0.4051833 Test Loss: 0.4701592
Validation loss decreased (0.405391 --> 0.405183).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 72.85912871360779
Epoch: 28, Steps: 87 | Train Loss: 0.2879524 Vali Loss: 0.4052366 Test Loss: 0.4700860
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 70.24356317520142
Epoch: 29, Steps: 87 | Train Loss: 0.2879433 Vali Loss: 0.4050444 Test Loss: 0.4701118
Validation loss decreased (0.405183 --> 0.405044).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 70.46325755119324
Epoch: 30, Steps: 87 | Train Loss: 0.2878900 Vali Loss: 0.4052864 Test Loss: 0.4702236
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 71.2445797920227
Epoch: 31, Steps: 87 | Train Loss: 0.2879124 Vali Loss: 0.4050070 Test Loss: 0.4700718
Validation loss decreased (0.405044 --> 0.405007).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 70.12705159187317
Epoch: 32, Steps: 87 | Train Loss: 0.2878583 Vali Loss: 0.4046342 Test Loss: 0.4701108
Validation loss decreased (0.405007 --> 0.404634).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 71.43053388595581
Epoch: 33, Steps: 87 | Train Loss: 0.2879172 Vali Loss: 0.4045460 Test Loss: 0.4700718
Validation loss decreased (0.404634 --> 0.404546).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 70.4763412475586
Epoch: 34, Steps: 87 | Train Loss: 0.2878246 Vali Loss: 0.4053790 Test Loss: 0.4702223
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 70.50454902648926
Epoch: 35, Steps: 87 | Train Loss: 0.2877825 Vali Loss: 0.4048877 Test Loss: 0.4701192
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 71.75887179374695
Epoch: 36, Steps: 87 | Train Loss: 0.2877655 Vali Loss: 0.4050786 Test Loss: 0.4701191
EarlyStopping counter: 3 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 69.04672694206238
Epoch: 37, Steps: 87 | Train Loss: 0.2877386 Vali Loss: 0.4050914 Test Loss: 0.4701143
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 58.804584980010986
Epoch: 38, Steps: 87 | Train Loss: 0.2877632 Vali Loss: 0.4047501 Test Loss: 0.4701623
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 59.4019193649292
Epoch: 39, Steps: 87 | Train Loss: 0.2877379 Vali Loss: 0.4047022 Test Loss: 0.4701689
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 58.400376081466675
Epoch: 40, Steps: 87 | Train Loss: 0.2876867 Vali Loss: 0.4050210 Test Loss: 0.4701917
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 58.91086483001709
Epoch: 41, Steps: 87 | Train Loss: 0.2876983 Vali Loss: 0.4049233 Test Loss: 0.4701495
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 58.642828702926636
Epoch: 42, Steps: 87 | Train Loss: 0.2876687 Vali Loss: 0.4046525 Test Loss: 0.4701926
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 60.176844120025635
Epoch: 43, Steps: 87 | Train Loss: 0.2876722 Vali Loss: 0.4044882 Test Loss: 0.4701376
Validation loss decreased (0.404546 --> 0.404488).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 56.924724102020264
Epoch: 44, Steps: 87 | Train Loss: 0.2876877 Vali Loss: 0.4053333 Test Loss: 0.4701807
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 58.81786751747131
Epoch: 45, Steps: 87 | Train Loss: 0.2876842 Vali Loss: 0.4050556 Test Loss: 0.4701659
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 59.90884208679199
Epoch: 46, Steps: 87 | Train Loss: 0.2876927 Vali Loss: 0.4048915 Test Loss: 0.4701751
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 61.8589813709259
Epoch: 47, Steps: 87 | Train Loss: 0.2877070 Vali Loss: 0.4053581 Test Loss: 0.4701734
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 62.011188983917236
Epoch: 48, Steps: 87 | Train Loss: 0.2876662 Vali Loss: 0.4046640 Test Loss: 0.4701321
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 62.56100416183472
Epoch: 49, Steps: 87 | Train Loss: 0.2876947 Vali Loss: 0.4044332 Test Loss: 0.4701531
Validation loss decreased (0.404488 --> 0.404433).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 63.353113412857056
Epoch: 50, Steps: 87 | Train Loss: 0.2876180 Vali Loss: 0.4045794 Test Loss: 0.4701265
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 62.126534938812256
Epoch: 51, Steps: 87 | Train Loss: 0.2876699 Vali Loss: 0.4048278 Test Loss: 0.4701118
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 63.494369983673096
Epoch: 52, Steps: 87 | Train Loss: 0.2876950 Vali Loss: 0.4047679 Test Loss: 0.4701074
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 63.33522891998291
Epoch: 53, Steps: 87 | Train Loss: 0.2876064 Vali Loss: 0.4046596 Test Loss: 0.4701405
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 63.25160837173462
Epoch: 54, Steps: 87 | Train Loss: 0.2876420 Vali Loss: 0.4046904 Test Loss: 0.4701421
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 64.49325251579285
Epoch: 55, Steps: 87 | Train Loss: 0.2876284 Vali Loss: 0.4045412 Test Loss: 0.4701364
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 62.90534424781799
Epoch: 56, Steps: 87 | Train Loss: 0.2876188 Vali Loss: 0.4052419 Test Loss: 0.4701451
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 63.28527212142944
Epoch: 57, Steps: 87 | Train Loss: 0.2876100 Vali Loss: 0.4042610 Test Loss: 0.4701503
Validation loss decreased (0.404433 --> 0.404261).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 63.34936761856079
Epoch: 58, Steps: 87 | Train Loss: 0.2876033 Vali Loss: 0.4050176 Test Loss: 0.4701436
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 62.080174684524536
Epoch: 59, Steps: 87 | Train Loss: 0.2876559 Vali Loss: 0.4045029 Test Loss: 0.4701160
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 63.80917406082153
Epoch: 60, Steps: 87 | Train Loss: 0.2876374 Vali Loss: 0.4044989 Test Loss: 0.4701415
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 61.03974103927612
Epoch: 61, Steps: 87 | Train Loss: 0.2876182 Vali Loss: 0.4051109 Test Loss: 0.4701180
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 64.18285250663757
Epoch: 62, Steps: 87 | Train Loss: 0.2876328 Vali Loss: 0.4047346 Test Loss: 0.4701045
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 63.9506676197052
Epoch: 63, Steps: 87 | Train Loss: 0.2876416 Vali Loss: 0.4049379 Test Loss: 0.4701167
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 62.37943410873413
Epoch: 64, Steps: 87 | Train Loss: 0.2876366 Vali Loss: 0.4048859 Test Loss: 0.4701066
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 63.78635120391846
Epoch: 65, Steps: 87 | Train Loss: 0.2875707 Vali Loss: 0.4042782 Test Loss: 0.4701147
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 61.830506324768066
Epoch: 66, Steps: 87 | Train Loss: 0.2876189 Vali Loss: 0.4048479 Test Loss: 0.4701102
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 63.49577760696411
Epoch: 67, Steps: 87 | Train Loss: 0.2876198 Vali Loss: 0.4047285 Test Loss: 0.4700856
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j720_H5_FITS_custom_ftM_sl360_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.469442218542099, mae:0.31602752208709717, rse:0.56026291847229, corr:[0.26344556 0.267466   0.26811892 0.27005157 0.27072355 0.27023518
 0.26991394 0.2701245  0.2701846  0.26969922 0.26907012 0.26871383
 0.26867086 0.26869106 0.26869154 0.26878035 0.2690556  0.2695109
 0.2699373  0.27008858 0.26988748 0.26946422 0.26928473 0.26991835
 0.2708168  0.2709612  0.27085185 0.27066633 0.27033722 0.26998577
 0.26976502 0.26963744 0.26944718 0.26918954 0.26905215 0.26909322
 0.2692379  0.26927587 0.26922607 0.2692649  0.2695001  0.2697927
 0.26988474 0.26973394 0.2695176  0.26933098 0.2692297  0.2694316
 0.26975402 0.2698073  0.26979092 0.26967373 0.26939514 0.26908162
 0.26891482 0.26889774 0.26887783 0.26879433 0.2687482  0.26878452
 0.26887682 0.26886973 0.26881918 0.26888272 0.2691151  0.2693335
 0.26930168 0.26908293 0.26891282 0.26888826 0.26889506 0.26895195
 0.26893923 0.26887968 0.2688285  0.26868698 0.2684248  0.26819673
 0.2681786  0.26829797 0.2683468  0.26831934 0.26829618 0.26837188
 0.26848093 0.268489   0.2684517  0.26854685 0.26880184 0.26890442
 0.2688567  0.26860422 0.26843753 0.26848164 0.26853356 0.26845363
 0.26824075 0.26816443 0.268201   0.26819247 0.26805785 0.26792252
 0.2679662  0.2681211  0.26820087 0.26824734 0.26830074 0.2684119
 0.26851144 0.26848394 0.26842463 0.26849794 0.26861283 0.2685506
 0.26846483 0.2682753  0.2681527  0.2681459  0.2682518  0.26827395
 0.26817396 0.26823038 0.26838875 0.26838875 0.26820645 0.2680502
 0.2681257  0.2682999  0.26838103 0.2683647  0.26835135 0.26851118
 0.26876292 0.26887485 0.2688405  0.26885208 0.26897264 0.26877925
 0.26877797 0.2685999  0.26841387 0.26846078 0.26872218 0.2690771
 0.26923752 0.2693641  0.26950458 0.26948228 0.26931694 0.26921293
 0.26931378 0.26953116 0.26963568 0.2696598  0.2696754  0.2697919
 0.2699637  0.27004114 0.2700232  0.27004412 0.2701718  0.27029377
 0.27029082 0.27009273 0.2698672  0.26982918 0.27009475 0.27068508
 0.27111185 0.2708767  0.2706665  0.27049667 0.2703065  0.27018565
 0.27020425 0.27031398 0.27039212 0.27040768 0.2704349  0.27050966
 0.2705948  0.2705874  0.2705277  0.27055505 0.2707159  0.27087992
 0.27080527 0.27053368 0.2702775  0.27013633 0.2701402  0.27037215
 0.2707074  0.2707332  0.27067912 0.27060896 0.27048466 0.27039906
 0.2704338  0.27052984 0.27057835 0.27056065 0.270531   0.27053666
 0.27055645 0.2704912  0.2704133  0.27043146 0.27054083 0.27058554
 0.27041325 0.27012095 0.26992273 0.26985148 0.26980993 0.26986846
 0.2699942  0.27001217 0.2700136  0.2699905  0.26991248 0.2698652
 0.26992637 0.27003852 0.2700913  0.2700697  0.27003855 0.27001807
 0.26999608 0.26990026 0.26980093 0.26981747 0.269936   0.2699731
 0.26978347 0.2694928  0.26942053 0.26947558 0.2695467  0.26957884
 0.2695267  0.2694586  0.26946747 0.26947466 0.26940125 0.26933756
 0.26941782 0.2695942  0.26969627 0.2697274  0.26972923 0.2697508
 0.2697614  0.26970658 0.26963183 0.2696531  0.26977783 0.26982734
 0.26971436 0.26948032 0.2693639  0.26942423 0.26949438 0.26944754
 0.26928037 0.26919928 0.26925024 0.2693051  0.2692501  0.26925012
 0.26936552 0.2695114  0.26952192 0.26951438 0.26952067 0.26956728
 0.26960093 0.26953772 0.26946616 0.26950276 0.26956743 0.2694675
 0.26935053 0.269155   0.2690319  0.26907206 0.26921505 0.26924267
 0.269079   0.2690876  0.26924548 0.26929542 0.2691988  0.26915365
 0.269297   0.2695215  0.26962143 0.2695875  0.26954353 0.26963922
 0.26978394 0.26980332 0.2697373  0.2697537  0.26988566 0.26982996
 0.26975507 0.26953784 0.26944    0.26957074 0.26992005 0.27030173
 0.2704508  0.2705915  0.270799   0.27086413 0.27080938 0.27078164
 0.2708618  0.270995   0.2710497  0.27102366 0.27101845 0.27111918
 0.2712253  0.27122435 0.271163   0.27115405 0.27123347 0.2712504
 0.27121228 0.27109194 0.27099934 0.27103886 0.27122763 0.27156827
 0.27172345 0.27148622 0.27135834 0.27133226 0.27128074 0.2712332
 0.27124944 0.2713238  0.27138326 0.27139777 0.27141935 0.27147454
 0.27149376 0.2714062  0.2712964  0.2712941  0.27142432 0.27154183
 0.27147242 0.27126333 0.27110544 0.27105588 0.27100715 0.27105007
 0.27124268 0.27125785 0.27126455 0.27128315 0.27122965 0.2711909
 0.27124277 0.27132523 0.2713399  0.27129444 0.27126938 0.27132455
 0.27135596 0.27126953 0.2711382  0.27110612 0.27117562 0.2712119
 0.27105433 0.27078545 0.2706263  0.27058706 0.27054265 0.2705702
 0.2706729  0.27071556 0.27076304 0.27078128 0.27074233 0.27074948
 0.27087122 0.2709978  0.2709901  0.27086917 0.27077135 0.27076763
 0.27077428 0.27063704 0.2704495  0.27038732 0.270475   0.27051255
 0.27031398 0.2700028  0.269848   0.26992124 0.27001634 0.27005044
 0.27000037 0.2699419  0.26996    0.26999396 0.26996985 0.26997826
 0.27010873 0.27024725 0.2702721  0.27021074 0.27015808 0.2701712
 0.2701997  0.27012068 0.27001414 0.2700133  0.2700952  0.27011135
 0.2700134  0.26980165 0.26970896 0.26979816 0.26985365 0.26982394
 0.2697249  0.2697426  0.26982135 0.2698557  0.2698141  0.2698324
 0.2699885  0.2701509  0.27016032 0.2700808  0.27006245 0.2701149
 0.2701085  0.2699531  0.2697872  0.26976135 0.26973057 0.2695433
 0.26936257 0.26914856 0.26907745 0.26912326 0.26920465 0.2692521
 0.26922938 0.26936454 0.26960707 0.26971018 0.26966238 0.26966006
 0.2698241  0.26999602 0.26996136 0.26977068 0.26967198 0.26978642
 0.26993647 0.26989016 0.26971632 0.26963556 0.26969075 0.26973858
 0.26966196 0.2694969  0.26948485 0.26967123 0.26990366 0.27010775
 0.27023664 0.27040774 0.2706739  0.2707552  0.27070424 0.27070448
 0.2708198  0.27091303 0.27084237 0.27066383 0.27057755 0.27064404
 0.2706989  0.27059686 0.27041262 0.2703829  0.2705199  0.27065915
 0.27063376 0.27048305 0.2704024  0.27045396 0.270563   0.27076337
 0.27087718 0.27061298 0.2705633  0.2706071  0.27057064 0.2705448
 0.27059245 0.2706222  0.27052495 0.27035964 0.2702759  0.27031985
 0.27031603 0.27013218 0.2699302  0.26990408 0.27004084 0.27011356
 0.2699573  0.26974165 0.2696777  0.26973164 0.2697386  0.26977977
 0.26994103 0.2699753  0.2700584  0.27016202 0.27018723 0.27022597
 0.270327   0.27038193 0.27028015 0.27009842 0.26999056 0.26998866
 0.26989502 0.26962563 0.26937386 0.2693558  0.26949134 0.26949784
 0.26925653 0.26898825 0.26893792 0.2690298  0.26900062 0.2689465
 0.2690124  0.2691591  0.2693643  0.26946834 0.26944193 0.26945752
 0.2695809  0.26964495 0.26951218 0.2692908  0.26917025 0.26916665
 0.26906312 0.268723   0.268408   0.26838723 0.26855677 0.26858246
 0.2683288  0.26806083 0.26807258 0.2682554  0.2683057  0.2682339
 0.26821658 0.26835835 0.26857156 0.26866245 0.26863942 0.26865828
 0.26871687 0.26862383 0.26832762 0.26804623 0.2679722  0.2680006
 0.2678711  0.26750332 0.2671891  0.26720876 0.26742402 0.26745707
 0.26719224 0.266949   0.26701444 0.26723853 0.26727235 0.2671193
 0.26703247 0.26718977 0.26741722 0.26745573 0.26740178 0.26749027
 0.2676724  0.26763245 0.26727903 0.26694506 0.26692194 0.2670566
 0.26697    0.26658335 0.26627907 0.26636514 0.2666616  0.2667518
 0.2665523  0.26637208 0.26652846 0.26682636 0.2669711  0.26690626
 0.2668698  0.26714057 0.26747566 0.26748213 0.2672944  0.26729748
 0.26749453 0.26747912 0.26707286 0.26664108 0.266592   0.2668028
 0.26678818 0.26638553 0.26606783 0.26624835 0.26663437 0.2667531
 0.26651838 0.2663573  0.26662263 0.2670537  0.26731366 0.26757756
 0.26796606 0.26844805 0.26883316 0.2687675  0.26854828 0.2685724
 0.26874542 0.26861292 0.26806962 0.26759598 0.26761025 0.2678296
 0.26768887 0.26720163 0.26699367 0.26738113 0.26793617 0.26807517
 0.2678419  0.26781818 0.26820418 0.2686136  0.268712   0.26898402
 0.2694543  0.2695336  0.26956913 0.26933944 0.26908997 0.269157
 0.26925033 0.26882333 0.26799023 0.26756665 0.26793    0.2684027
 0.2682183  0.26768294 0.26779178 0.26863798 0.26927933 0.26914874
 0.26880658 0.26907408 0.26977652 0.27000058 0.26953268 0.26934704
 0.26990458 0.27035215 0.2703354  0.26995787 0.2697318  0.269745
 0.26930714 0.26811188 0.26720306 0.267686   0.26882327 0.2689917
 0.26826498 0.26841733 0.2701805  0.271624   0.27107415 0.26974764
 0.26996034 0.27107212 0.26930207 0.26404324 0.26338446 0.26990294]
