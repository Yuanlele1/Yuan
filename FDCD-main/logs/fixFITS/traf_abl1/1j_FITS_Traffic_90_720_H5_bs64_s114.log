Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j720_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j720_H5_FITS_custom_ftM_sl90_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11471
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=30, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  893721600.0
params:  8370.0
Trainable parameters:  8370
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 119.67018818855286
Epoch: 1, Steps: 89 | Train Loss: 2.5037340 Vali Loss: 2.0415430 Test Loss: 2.5457895
Validation loss decreased (inf --> 2.041543).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 116.74083924293518
Epoch: 2, Steps: 89 | Train Loss: 1.3302123 Vali Loss: 1.3020192 Test Loss: 1.6144500
Validation loss decreased (2.041543 --> 1.302019).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 115.06424951553345
Epoch: 3, Steps: 89 | Train Loss: 0.9024271 Vali Loss: 0.9821097 Test Loss: 1.2087157
Validation loss decreased (1.302019 --> 0.982110).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 117.1557252407074
Epoch: 4, Steps: 89 | Train Loss: 0.7040789 Vali Loss: 0.8220658 Test Loss: 1.0026269
Validation loss decreased (0.982110 --> 0.822066).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 121.04006934165955
Epoch: 5, Steps: 89 | Train Loss: 0.5996204 Vali Loss: 0.7322335 Test Loss: 0.8890741
Validation loss decreased (0.822066 --> 0.732233).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 119.64549803733826
Epoch: 6, Steps: 89 | Train Loss: 0.5402803 Vali Loss: 0.6810707 Test Loss: 0.8227043
Validation loss decreased (0.732233 --> 0.681071).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 120.11927270889282
Epoch: 7, Steps: 89 | Train Loss: 0.5046400 Vali Loss: 0.6489881 Test Loss: 0.7818169
Validation loss decreased (0.681071 --> 0.648988).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 119.32500767707825
Epoch: 8, Steps: 89 | Train Loss: 0.4823547 Vali Loss: 0.6288836 Test Loss: 0.7557341
Validation loss decreased (0.648988 --> 0.628884).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 122.84169578552246
Epoch: 9, Steps: 89 | Train Loss: 0.4677667 Vali Loss: 0.6146933 Test Loss: 0.7380501
Validation loss decreased (0.628884 --> 0.614693).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 118.6425449848175
Epoch: 10, Steps: 89 | Train Loss: 0.4576768 Vali Loss: 0.6057752 Test Loss: 0.7260365
Validation loss decreased (0.614693 --> 0.605775).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 120.5689435005188
Epoch: 11, Steps: 89 | Train Loss: 0.4505187 Vali Loss: 0.5984424 Test Loss: 0.7169333
Validation loss decreased (0.605775 --> 0.598442).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 121.96814751625061
Epoch: 12, Steps: 89 | Train Loss: 0.4450331 Vali Loss: 0.5928582 Test Loss: 0.7101070
Validation loss decreased (0.598442 --> 0.592858).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 121.7740547657013
Epoch: 13, Steps: 89 | Train Loss: 0.4410152 Vali Loss: 0.5889627 Test Loss: 0.7048103
Validation loss decreased (0.592858 --> 0.588963).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 121.21504235267639
Epoch: 14, Steps: 89 | Train Loss: 0.4378046 Vali Loss: 0.5854886 Test Loss: 0.7007986
Validation loss decreased (0.588963 --> 0.585489).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 98.55858254432678
Epoch: 15, Steps: 89 | Train Loss: 0.4352080 Vali Loss: 0.5829851 Test Loss: 0.6973159
Validation loss decreased (0.585489 --> 0.582985).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 94.74495315551758
Epoch: 16, Steps: 89 | Train Loss: 0.4328745 Vali Loss: 0.5810460 Test Loss: 0.6943614
Validation loss decreased (0.582985 --> 0.581046).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 96.0847384929657
Epoch: 17, Steps: 89 | Train Loss: 0.4312289 Vali Loss: 0.5786888 Test Loss: 0.6919802
Validation loss decreased (0.581046 --> 0.578689).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 99.61705708503723
Epoch: 18, Steps: 89 | Train Loss: 0.4296647 Vali Loss: 0.5771745 Test Loss: 0.6899065
Validation loss decreased (0.578689 --> 0.577175).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 94.35352611541748
Epoch: 19, Steps: 89 | Train Loss: 0.4283886 Vali Loss: 0.5753070 Test Loss: 0.6881136
Validation loss decreased (0.577175 --> 0.575307).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 96.1228883266449
Epoch: 20, Steps: 89 | Train Loss: 0.4271164 Vali Loss: 0.5749478 Test Loss: 0.6865593
Validation loss decreased (0.575307 --> 0.574948).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 94.7419810295105
Epoch: 21, Steps: 89 | Train Loss: 0.4263246 Vali Loss: 0.5739651 Test Loss: 0.6853468
Validation loss decreased (0.574948 --> 0.573965).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 98.34233832359314
Epoch: 22, Steps: 89 | Train Loss: 0.4254059 Vali Loss: 0.5723937 Test Loss: 0.6840614
Validation loss decreased (0.573965 --> 0.572394).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 98.73991894721985
Epoch: 23, Steps: 89 | Train Loss: 0.4246283 Vali Loss: 0.5720582 Test Loss: 0.6829600
Validation loss decreased (0.572394 --> 0.572058).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 89.27688980102539
Epoch: 24, Steps: 89 | Train Loss: 0.4238180 Vali Loss: 0.5701693 Test Loss: 0.6821104
Validation loss decreased (0.572058 --> 0.570169).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 88.50140643119812
Epoch: 25, Steps: 89 | Train Loss: 0.4231897 Vali Loss: 0.5702478 Test Loss: 0.6812810
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 89.27447724342346
Epoch: 26, Steps: 89 | Train Loss: 0.4226601 Vali Loss: 0.5694910 Test Loss: 0.6805162
Validation loss decreased (0.570169 --> 0.569491).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 90.27500605583191
Epoch: 27, Steps: 89 | Train Loss: 0.4221868 Vali Loss: 0.5686892 Test Loss: 0.6798813
Validation loss decreased (0.569491 --> 0.568689).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 82.34549188613892
Epoch: 28, Steps: 89 | Train Loss: 0.4217075 Vali Loss: 0.5691391 Test Loss: 0.6793004
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 95.36595153808594
Epoch: 29, Steps: 89 | Train Loss: 0.4212594 Vali Loss: 0.5678651 Test Loss: 0.6786544
Validation loss decreased (0.568689 --> 0.567865).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 96.86889934539795
Epoch: 30, Steps: 89 | Train Loss: 0.4208505 Vali Loss: 0.5679125 Test Loss: 0.6781375
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 95.85368537902832
Epoch: 31, Steps: 89 | Train Loss: 0.4204405 Vali Loss: 0.5673640 Test Loss: 0.6776241
Validation loss decreased (0.567865 --> 0.567364).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 75.49457693099976
Epoch: 32, Steps: 89 | Train Loss: 0.4200191 Vali Loss: 0.5671919 Test Loss: 0.6771865
Validation loss decreased (0.567364 --> 0.567192).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 72.76285219192505
Epoch: 33, Steps: 89 | Train Loss: 0.4199254 Vali Loss: 0.5663456 Test Loss: 0.6768199
Validation loss decreased (0.567192 --> 0.566346).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 70.7457344532013
Epoch: 34, Steps: 89 | Train Loss: 0.4195283 Vali Loss: 0.5660019 Test Loss: 0.6764459
Validation loss decreased (0.566346 --> 0.566002).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 75.91443610191345
Epoch: 35, Steps: 89 | Train Loss: 0.4193855 Vali Loss: 0.5664468 Test Loss: 0.6761063
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 77.36568641662598
Epoch: 36, Steps: 89 | Train Loss: 0.4191341 Vali Loss: 0.5653136 Test Loss: 0.6757982
Validation loss decreased (0.566002 --> 0.565314).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 79.04242086410522
Epoch: 37, Steps: 89 | Train Loss: 0.4189794 Vali Loss: 0.5659404 Test Loss: 0.6754797
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 78.19713711738586
Epoch: 38, Steps: 89 | Train Loss: 0.4185684 Vali Loss: 0.5658908 Test Loss: 0.6752352
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 76.93868708610535
Epoch: 39, Steps: 89 | Train Loss: 0.4184742 Vali Loss: 0.5650848 Test Loss: 0.6749942
Validation loss decreased (0.565314 --> 0.565085).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 71.30473971366882
Epoch: 40, Steps: 89 | Train Loss: 0.4182636 Vali Loss: 0.5644532 Test Loss: 0.6747220
Validation loss decreased (0.565085 --> 0.564453).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 75.43536972999573
Epoch: 41, Steps: 89 | Train Loss: 0.4180943 Vali Loss: 0.5647446 Test Loss: 0.6745713
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 77.68501615524292
Epoch: 42, Steps: 89 | Train Loss: 0.4179586 Vali Loss: 0.5649665 Test Loss: 0.6743661
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 75.42243361473083
Epoch: 43, Steps: 89 | Train Loss: 0.4178058 Vali Loss: 0.5642042 Test Loss: 0.6741288
Validation loss decreased (0.564453 --> 0.564204).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 76.19851994514465
Epoch: 44, Steps: 89 | Train Loss: 0.4175906 Vali Loss: 0.5644630 Test Loss: 0.6740032
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 76.21439981460571
Epoch: 45, Steps: 89 | Train Loss: 0.4174391 Vali Loss: 0.5641746 Test Loss: 0.6738384
Validation loss decreased (0.564204 --> 0.564175).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 75.37543821334839
Epoch: 46, Steps: 89 | Train Loss: 0.4173953 Vali Loss: 0.5641550 Test Loss: 0.6736634
Validation loss decreased (0.564175 --> 0.564155).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 62.67048931121826
Epoch: 47, Steps: 89 | Train Loss: 0.4173698 Vali Loss: 0.5637944 Test Loss: 0.6735093
Validation loss decreased (0.564155 --> 0.563794).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 63.2447235584259
Epoch: 48, Steps: 89 | Train Loss: 0.4171605 Vali Loss: 0.5636512 Test Loss: 0.6733738
Validation loss decreased (0.563794 --> 0.563651).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 59.67183542251587
Epoch: 49, Steps: 89 | Train Loss: 0.4169936 Vali Loss: 0.5639994 Test Loss: 0.6732973
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 62.301278591156006
Epoch: 50, Steps: 89 | Train Loss: 0.4169777 Vali Loss: 0.5635285 Test Loss: 0.6731763
Validation loss decreased (0.563651 --> 0.563529).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 62.06340718269348
Epoch: 51, Steps: 89 | Train Loss: 0.4169903 Vali Loss: 0.5634411 Test Loss: 0.6730886
Validation loss decreased (0.563529 --> 0.563441).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 61.58371567726135
Epoch: 52, Steps: 89 | Train Loss: 0.4168414 Vali Loss: 0.5632899 Test Loss: 0.6729734
Validation loss decreased (0.563441 --> 0.563290).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 60.19083547592163
Epoch: 53, Steps: 89 | Train Loss: 0.4168325 Vali Loss: 0.5633118 Test Loss: 0.6728289
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 62.59583282470703
Epoch: 54, Steps: 89 | Train Loss: 0.4166305 Vali Loss: 0.5630989 Test Loss: 0.6727694
Validation loss decreased (0.563290 --> 0.563099).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 59.29577016830444
Epoch: 55, Steps: 89 | Train Loss: 0.4166304 Vali Loss: 0.5629574 Test Loss: 0.6726914
Validation loss decreased (0.563099 --> 0.562957).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 61.03345990180969
Epoch: 56, Steps: 89 | Train Loss: 0.4166678 Vali Loss: 0.5632033 Test Loss: 0.6726478
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 59.49976992607117
Epoch: 57, Steps: 89 | Train Loss: 0.4165807 Vali Loss: 0.5623389 Test Loss: 0.6725636
Validation loss decreased (0.562957 --> 0.562339).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 59.07680892944336
Epoch: 58, Steps: 89 | Train Loss: 0.4164205 Vali Loss: 0.5625553 Test Loss: 0.6724634
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 56.64277672767639
Epoch: 59, Steps: 89 | Train Loss: 0.4164453 Vali Loss: 0.5620435 Test Loss: 0.6724009
Validation loss decreased (0.562339 --> 0.562044).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 56.57153940200806
Epoch: 60, Steps: 89 | Train Loss: 0.4164125 Vali Loss: 0.5621747 Test Loss: 0.6723477
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 58.00295901298523
Epoch: 61, Steps: 89 | Train Loss: 0.4163044 Vali Loss: 0.5625529 Test Loss: 0.6722844
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 58.37605166435242
Epoch: 62, Steps: 89 | Train Loss: 0.4163093 Vali Loss: 0.5623387 Test Loss: 0.6722493
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 59.98675870895386
Epoch: 63, Steps: 89 | Train Loss: 0.4161169 Vali Loss: 0.5626355 Test Loss: 0.6721706
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 56.7434663772583
Epoch: 64, Steps: 89 | Train Loss: 0.4161256 Vali Loss: 0.5628430 Test Loss: 0.6721621
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 59.265153646469116
Epoch: 65, Steps: 89 | Train Loss: 0.4161415 Vali Loss: 0.5626494 Test Loss: 0.6721010
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 55.186288356781006
Epoch: 66, Steps: 89 | Train Loss: 0.4161107 Vali Loss: 0.5622307 Test Loss: 0.6720617
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 57.85839605331421
Epoch: 67, Steps: 89 | Train Loss: 0.4159903 Vali Loss: 0.5628030 Test Loss: 0.6720273
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 55.935924768447876
Epoch: 68, Steps: 89 | Train Loss: 0.4160314 Vali Loss: 0.5632762 Test Loss: 0.6719892
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 57.57498025894165
Epoch: 69, Steps: 89 | Train Loss: 0.4159992 Vali Loss: 0.5619364 Test Loss: 0.6719562
Validation loss decreased (0.562044 --> 0.561936).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 55.71886610984802
Epoch: 70, Steps: 89 | Train Loss: 0.4159799 Vali Loss: 0.5624966 Test Loss: 0.6719188
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 57.85023760795593
Epoch: 71, Steps: 89 | Train Loss: 0.4159483 Vali Loss: 0.5620674 Test Loss: 0.6718824
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 56.90925717353821
Epoch: 72, Steps: 89 | Train Loss: 0.4159097 Vali Loss: 0.5630467 Test Loss: 0.6718598
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 57.90056657791138
Epoch: 73, Steps: 89 | Train Loss: 0.4158407 Vali Loss: 0.5625767 Test Loss: 0.6718441
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 54.947190046310425
Epoch: 74, Steps: 89 | Train Loss: 0.4158315 Vali Loss: 0.5625666 Test Loss: 0.6718129
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 54.5403208732605
Epoch: 75, Steps: 89 | Train Loss: 0.4158921 Vali Loss: 0.5619315 Test Loss: 0.6717860
Validation loss decreased (0.561936 --> 0.561931).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 55.58297872543335
Epoch: 76, Steps: 89 | Train Loss: 0.4157718 Vali Loss: 0.5624863 Test Loss: 0.6717758
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 55.55541706085205
Epoch: 77, Steps: 89 | Train Loss: 0.4158232 Vali Loss: 0.5613093 Test Loss: 0.6717591
Validation loss decreased (0.561931 --> 0.561309).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 52.298044204711914
Epoch: 78, Steps: 89 | Train Loss: 0.4158706 Vali Loss: 0.5626466 Test Loss: 0.6717216
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 56.11475467681885
Epoch: 79, Steps: 89 | Train Loss: 0.4157169 Vali Loss: 0.5617616 Test Loss: 0.6717071
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 53.8639600276947
Epoch: 80, Steps: 89 | Train Loss: 0.4156847 Vali Loss: 0.5622711 Test Loss: 0.6716874
EarlyStopping counter: 3 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 54.01225757598877
Epoch: 81, Steps: 89 | Train Loss: 0.4157635 Vali Loss: 0.5623672 Test Loss: 0.6716685
EarlyStopping counter: 4 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 53.050928592681885
Epoch: 82, Steps: 89 | Train Loss: 0.4158083 Vali Loss: 0.5623324 Test Loss: 0.6716580
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 53.81960391998291
Epoch: 83, Steps: 89 | Train Loss: 0.4157425 Vali Loss: 0.5617189 Test Loss: 0.6716496
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 53.36178970336914
Epoch: 84, Steps: 89 | Train Loss: 0.4157390 Vali Loss: 0.5612997 Test Loss: 0.6716298
Validation loss decreased (0.561309 --> 0.561300).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 54.717193603515625
Epoch: 85, Steps: 89 | Train Loss: 0.4157596 Vali Loss: 0.5621165 Test Loss: 0.6716092
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 55.933327436447144
Epoch: 86, Steps: 89 | Train Loss: 0.4156523 Vali Loss: 0.5618809 Test Loss: 0.6715998
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 56.84045696258545
Epoch: 87, Steps: 89 | Train Loss: 0.4155638 Vali Loss: 0.5624208 Test Loss: 0.6715905
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 54.65848636627197
Epoch: 88, Steps: 89 | Train Loss: 0.4156943 Vali Loss: 0.5616995 Test Loss: 0.6715850
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 55.041035175323486
Epoch: 89, Steps: 89 | Train Loss: 0.4156154 Vali Loss: 0.5625540 Test Loss: 0.6715606
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 54.41575360298157
Epoch: 90, Steps: 89 | Train Loss: 0.4155276 Vali Loss: 0.5612679 Test Loss: 0.6715558
Validation loss decreased (0.561300 --> 0.561268).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 55.43513536453247
Epoch: 91, Steps: 89 | Train Loss: 0.4156834 Vali Loss: 0.5616838 Test Loss: 0.6715473
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 55.21512579917908
Epoch: 92, Steps: 89 | Train Loss: 0.4156131 Vali Loss: 0.5620816 Test Loss: 0.6715334
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 52.99335241317749
Epoch: 93, Steps: 89 | Train Loss: 0.4155572 Vali Loss: 0.5622940 Test Loss: 0.6715310
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 52.327415227890015
Epoch: 94, Steps: 89 | Train Loss: 0.4155507 Vali Loss: 0.5622336 Test Loss: 0.6715187
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 53.53848123550415
Epoch: 95, Steps: 89 | Train Loss: 0.4154458 Vali Loss: 0.5622126 Test Loss: 0.6715087
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 53.530211210250854
Epoch: 96, Steps: 89 | Train Loss: 0.4154620 Vali Loss: 0.5616969 Test Loss: 0.6715087
EarlyStopping counter: 6 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 52.563133001327515
Epoch: 97, Steps: 89 | Train Loss: 0.4154750 Vali Loss: 0.5624680 Test Loss: 0.6714979
EarlyStopping counter: 7 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 54.03046250343323
Epoch: 98, Steps: 89 | Train Loss: 0.4155180 Vali Loss: 0.5620692 Test Loss: 0.6714897
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 52.60997462272644
Epoch: 99, Steps: 89 | Train Loss: 0.4155382 Vali Loss: 0.5616659 Test Loss: 0.6714876
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 52.8568594455719
Epoch: 100, Steps: 89 | Train Loss: 0.4155833 Vali Loss: 0.5618244 Test Loss: 0.6714783
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j720_H5_FITS_custom_ftM_sl90_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.67038494348526, mae:0.3994769752025604, rse:0.6695190072059631, corr:[0.25820795 0.2576363  0.26171753 0.2619458  0.25922096 0.26307854
 0.2666021  0.26462615 0.26496083 0.2667003  0.2639212  0.26294473
 0.26458403 0.26296848 0.2617813  0.26370138 0.26416802 0.2636879
 0.26598245 0.26827872 0.2680993  0.26911834 0.26981026 0.2686638
 0.27221194 0.27654332 0.27627563 0.27394736 0.27410424 0.2764229
 0.27707624 0.27826497 0.2790082  0.27824423 0.27736446 0.27751833
 0.27714923 0.275651   0.27550128 0.27639174 0.27635813 0.27671126
 0.27843437 0.27911815 0.27840745 0.27886078 0.2784834  0.27673408
 0.27748534 0.27748618 0.27607074 0.27506617 0.27414978 0.27032053
 0.26674724 0.26757926 0.2678184  0.26689172 0.26621532 0.26536673
 0.26419917 0.26311874 0.26266792 0.2627946  0.26294664 0.26326102
 0.26412734 0.26474303 0.26457214 0.26490143 0.2648708  0.26333255
 0.2617891  0.2609331  0.26037216 0.2608021  0.26214877 0.26231915
 0.26252076 0.26493043 0.26553074 0.26483372 0.26414415 0.2630547
 0.26196474 0.26095292 0.26017794 0.26014084 0.26010913 0.2597942
 0.260403   0.26102018 0.26127118 0.26191235 0.2623582  0.2621493
 0.26202527 0.26212263 0.26197284 0.26174584 0.26200777 0.26236776
 0.26279926 0.26403922 0.26424226 0.26339722 0.2628971  0.26248246
 0.2617501  0.26098943 0.2605922  0.26081052 0.26097828 0.260967
 0.26170892 0.26234618 0.2625342  0.2631578  0.26366997 0.26364264
 0.2638686  0.26414338 0.26410866 0.26385966 0.26384443 0.2639431
 0.26368302 0.26353666 0.2632372  0.2623635  0.26177737 0.2616248
 0.26121405 0.2607774  0.2608055  0.26135334 0.26186293 0.26178536
 0.26278684 0.26362783 0.26388007 0.26476094 0.26480877 0.26334128
 0.26275826 0.26276273 0.26257145 0.26239473 0.26251873 0.26289216
 0.26298025 0.26294318 0.26265213 0.26183012 0.2611133  0.26090968
 0.26062056 0.2604679  0.2608771  0.26164505 0.26240584 0.2630536
 0.2645842  0.266165   0.26664528 0.26748466 0.26736224 0.26494408
 0.2642613  0.26344398 0.26147598 0.26020047 0.26062453 0.26254416
 0.26455417 0.2658417  0.2662675  0.26606715 0.26581264 0.26585373
 0.2655292  0.26498505 0.26520595 0.26584718 0.2662501  0.26689902
 0.26839218 0.2696133  0.2697704  0.27027985 0.27009785 0.2691079
 0.27232513 0.27528563 0.2743985  0.27280968 0.2729176  0.27418533
 0.27507767 0.27661338 0.27722964 0.27702758 0.2770511  0.27693006
 0.27627093 0.2755109  0.2755178  0.2758955  0.27602828 0.27651894
 0.27756122 0.27795592 0.27759442 0.27768588 0.27697483 0.27534497
 0.27519518 0.2740252  0.27226228 0.27145824 0.27073583 0.26758596
 0.26492527 0.26598284 0.26648712 0.26612994 0.26578772 0.2648841
 0.26382425 0.26302287 0.26260316 0.26262352 0.2627225  0.26289457
 0.26339945 0.2637644  0.2638615  0.26401553 0.26379678 0.26249334
 0.26108733 0.2600623  0.2594814  0.25997347 0.26140988 0.26204407
 0.26277873 0.26539284 0.26622626 0.2659109  0.26543072 0.26428002
 0.26319283 0.2623697  0.2616826  0.2615143  0.2614083  0.26126906
 0.26168808 0.26213205 0.26237652 0.2628918  0.2632876  0.26317286
 0.2629725  0.26288453 0.2627509  0.26265237 0.2628028  0.26316044
 0.26371455 0.26505157 0.26534155 0.2647153  0.2642776  0.26371813
 0.262991   0.26234385 0.2619409  0.26205295 0.2621977  0.26218858
 0.2627499  0.26323965 0.26339793 0.26392    0.2644176  0.26446795
 0.2645383  0.2646622  0.26468053 0.26455462 0.2645392  0.26466352
 0.26455408 0.2644998  0.26423985 0.26356727 0.26306054 0.26272917
 0.2623241  0.2620259  0.26202524 0.2624983  0.26305073 0.26319954
 0.26409915 0.26486182 0.26511866 0.2658421  0.26580977 0.26432207
 0.26356885 0.26345566 0.26332477 0.26319182 0.26326483 0.26364863
 0.26393357 0.26405594 0.26389802 0.26323435 0.26253325 0.26214477
 0.26183957 0.26173425 0.262022   0.26269934 0.26348826 0.2639623
 0.2653186  0.26692057 0.2675137  0.26819754 0.26784018 0.26530045
 0.26407215 0.2629381  0.2608526  0.25949252 0.25976253 0.26166943
 0.2638744  0.26527733 0.26571468 0.2656501  0.2653841  0.26524207
 0.26496994 0.2645095  0.26462206 0.26523164 0.26567274 0.2661801
 0.26753616 0.2688414  0.2691054  0.26955965 0.26932266 0.26842663
 0.27152222 0.27508533 0.27436656 0.27269045 0.2726206  0.2739534
 0.27513844 0.27690685 0.27775717 0.27775204 0.2777625  0.27754724
 0.27694768 0.276256   0.27610028 0.27641657 0.27663448 0.2770118
 0.27790585 0.27840734 0.27817512 0.2781601  0.2774095  0.27578324
 0.27533484 0.27409178 0.27218392 0.271021   0.27001214 0.26687104
 0.2647231  0.26625112 0.26717752 0.26708087 0.26677784 0.26587555
 0.26483342 0.26405582 0.26363426 0.2635824  0.2635619  0.26367742
 0.2641395  0.2644506  0.264328   0.26437303 0.26414636 0.26281682
 0.2612454  0.26009026 0.259423   0.25980416 0.2610431  0.26169223
 0.2627924  0.26567644 0.26689202 0.26679516 0.26633936 0.26523554
 0.2642109  0.2633763  0.262646   0.2624192  0.26220715 0.26191872
 0.26225597 0.26264876 0.26283118 0.26331863 0.2636984  0.2635435
 0.26318875 0.26301602 0.262915   0.2628105  0.2630066  0.263329
 0.26384816 0.2652244  0.26572928 0.2652642  0.2648385  0.26423597
 0.2635163  0.2628897  0.2624444  0.26246613 0.2625697  0.26248035
 0.26288515 0.26326987 0.2633961  0.26392084 0.26443192 0.26442716
 0.26438713 0.2645108  0.26462123 0.26456556 0.26458576 0.2647514
 0.26465264 0.26462206 0.26447675 0.26385778 0.2632806  0.262915
 0.26255813 0.26222044 0.2621496  0.26257378 0.26308495 0.2634265
 0.264252   0.2649618  0.26521158 0.26595378 0.26604477 0.26466483
 0.26387453 0.26378384 0.26372507 0.26364085 0.26371336 0.26411334
 0.2643966  0.26450074 0.264404   0.26381204 0.26301724 0.26250416
 0.26224005 0.26211983 0.26229444 0.26296732 0.2637494  0.2643739
 0.26572806 0.2672914  0.2679292  0.26858845 0.26816338 0.265568
 0.26413274 0.26283213 0.2606965  0.25929925 0.25942373 0.26124388
 0.26340735 0.2648557  0.26534495 0.26531214 0.26498467 0.2648199
 0.26466307 0.26415843 0.2640796  0.26466763 0.2651529  0.26560447
 0.26686606 0.26821962 0.26857507 0.2690782  0.26907298 0.26843578
 0.2712679  0.27418986 0.2733536  0.2718879  0.27184835 0.2733174
 0.27470133 0.27656928 0.27748293 0.27750313 0.27745155 0.2772479
 0.27672887 0.27597138 0.27570012 0.27593312 0.27602455 0.27621663
 0.27696007 0.27744383 0.27717227 0.2772038  0.2766541  0.27508372
 0.27455246 0.27349567 0.27181143 0.27080554 0.26966122 0.26627988
 0.26386392 0.2652907  0.26630506 0.26621574 0.265792   0.26490244
 0.26395145 0.26307812 0.26250428 0.26231325 0.26208666 0.2619572
 0.26224744 0.26246318 0.26230893 0.26238018 0.26225743 0.26101443
 0.25950307 0.25837407 0.25778693 0.2582045  0.2595108  0.26031548
 0.26181248 0.265129   0.26659217 0.26642564 0.26584718 0.2647589
 0.26379654 0.26291052 0.26209986 0.2617898  0.2614207  0.26097485
 0.26109666 0.2613827  0.26152086 0.2620079  0.26242468 0.26221073
 0.26186684 0.2618363  0.2618431  0.2617819  0.26205885 0.26243842
 0.26298618 0.26432076 0.26484424 0.26435405 0.2638384  0.26326597
 0.26262033 0.26196042 0.26145768 0.26141098 0.26139566 0.26127517
 0.26159325 0.26190472 0.2619835  0.26262045 0.2633053  0.26319325
 0.26299176 0.26322466 0.26347774 0.26347283 0.2635604  0.26374894
 0.26358604 0.26341915 0.2632944  0.26265296 0.26184216 0.26144502
 0.26121157 0.26080033 0.2606686  0.26111445 0.26148614 0.2616869
 0.26239488 0.26302168 0.26325554 0.26416895 0.26446286 0.26309463
 0.26231456 0.2624808  0.26263416 0.26257014 0.26267684 0.26303083
 0.26310545 0.26314142 0.2632412  0.2625271  0.26144537 0.26114586
 0.2610789  0.2607134  0.26085806 0.26166445 0.2622673  0.26267046
 0.26397386 0.2654461  0.26601383 0.26698592 0.26688588 0.26432714
 0.2630794  0.2626033  0.2611103  0.2595217  0.25948352 0.26111645
 0.2629765  0.2644348  0.2653167  0.2650441  0.26436326 0.26466662
 0.26471135 0.26377282 0.26389852 0.26476598 0.26466393 0.2647323
 0.26627418 0.26756415 0.26784894 0.26900533 0.26922324 0.26817194
 0.27121043 0.27521628 0.27479026 0.27284974 0.27235726 0.27297884
 0.27432904 0.27717122 0.27810833 0.27708727 0.2773926  0.27844396
 0.27693814 0.27511117 0.27590388 0.27504918 0.27254066 0.27307233
 0.27479616 0.27482894 0.27580103 0.2761316  0.27232298 0.27547836]
