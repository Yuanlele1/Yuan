Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j96_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j96_H5_FITS_custom_ftM_sl360_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11825
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1132047360.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 65.31527280807495
Epoch: 1, Steps: 92 | Train Loss: 0.7301364 Vali Loss: 0.5832148 Test Loss: 0.6806592
Validation loss decreased (inf --> 0.583215).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 65.01558017730713
Epoch: 2, Steps: 92 | Train Loss: 0.3685211 Vali Loss: 0.4049809 Test Loss: 0.4804888
Validation loss decreased (0.583215 --> 0.404981).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 63.45781087875366
Epoch: 3, Steps: 92 | Train Loss: 0.2817574 Vali Loss: 0.3597725 Test Loss: 0.4337344
Validation loss decreased (0.404981 --> 0.359772).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 63.60377073287964
Epoch: 4, Steps: 92 | Train Loss: 0.2607422 Vali Loss: 0.3495165 Test Loss: 0.4248222
Validation loss decreased (0.359772 --> 0.349516).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 63.38745093345642
Epoch: 5, Steps: 92 | Train Loss: 0.2560245 Vali Loss: 0.3466999 Test Loss: 0.4235055
Validation loss decreased (0.349516 --> 0.346700).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 58.889322996139526
Epoch: 6, Steps: 92 | Train Loss: 0.2549437 Vali Loss: 0.3451505 Test Loss: 0.4232458
Validation loss decreased (0.346700 --> 0.345150).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 63.34142589569092
Epoch: 7, Steps: 92 | Train Loss: 0.2545246 Vali Loss: 0.3461919 Test Loss: 0.4229615
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 63.813902616500854
Epoch: 8, Steps: 92 | Train Loss: 0.2542033 Vali Loss: 0.3444699 Test Loss: 0.4227420
Validation loss decreased (0.345150 --> 0.344470).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 60.33903741836548
Epoch: 9, Steps: 92 | Train Loss: 0.2540887 Vali Loss: 0.3449305 Test Loss: 0.4225262
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 60.449405670166016
Epoch: 10, Steps: 92 | Train Loss: 0.2539683 Vali Loss: 0.3451848 Test Loss: 0.4223627
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 64.22626495361328
Epoch: 11, Steps: 92 | Train Loss: 0.2536748 Vali Loss: 0.3450626 Test Loss: 0.4222427
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 59.84041953086853
Epoch: 12, Steps: 92 | Train Loss: 0.2536870 Vali Loss: 0.3450748 Test Loss: 0.4221162
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 62.40719747543335
Epoch: 13, Steps: 92 | Train Loss: 0.2537186 Vali Loss: 0.3445292 Test Loss: 0.4219771
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 60.5922417640686
Epoch: 14, Steps: 92 | Train Loss: 0.2535721 Vali Loss: 0.3442709 Test Loss: 0.4220118
Validation loss decreased (0.344470 --> 0.344271).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 64.37246990203857
Epoch: 15, Steps: 92 | Train Loss: 0.2535543 Vali Loss: 0.3439122 Test Loss: 0.4219804
Validation loss decreased (0.344271 --> 0.343912).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 57.17184662818909
Epoch: 16, Steps: 92 | Train Loss: 0.2534970 Vali Loss: 0.3444291 Test Loss: 0.4217518
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 60.597031116485596
Epoch: 17, Steps: 92 | Train Loss: 0.2534146 Vali Loss: 0.3444311 Test Loss: 0.4218376
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 60.03103709220886
Epoch: 18, Steps: 92 | Train Loss: 0.2534008 Vali Loss: 0.3438170 Test Loss: 0.4218061
Validation loss decreased (0.343912 --> 0.343817).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 60.763238191604614
Epoch: 19, Steps: 92 | Train Loss: 0.2533512 Vali Loss: 0.3449705 Test Loss: 0.4217271
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 59.92270755767822
Epoch: 20, Steps: 92 | Train Loss: 0.2533737 Vali Loss: 0.3448886 Test Loss: 0.4215773
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 64.27677154541016
Epoch: 21, Steps: 92 | Train Loss: 0.2533496 Vali Loss: 0.3451736 Test Loss: 0.4215270
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 62.109081745147705
Epoch: 22, Steps: 92 | Train Loss: 0.2532827 Vali Loss: 0.3442302 Test Loss: 0.4215954
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 60.67463231086731
Epoch: 23, Steps: 92 | Train Loss: 0.2532698 Vali Loss: 0.3447413 Test Loss: 0.4214506
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 60.85517430305481
Epoch: 24, Steps: 92 | Train Loss: 0.2532330 Vali Loss: 0.3443964 Test Loss: 0.4214419
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 60.21279239654541
Epoch: 25, Steps: 92 | Train Loss: 0.2533131 Vali Loss: 0.3438031 Test Loss: 0.4214235
Validation loss decreased (0.343817 --> 0.343803).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 61.15547180175781
Epoch: 26, Steps: 92 | Train Loss: 0.2531951 Vali Loss: 0.3441812 Test Loss: 0.4214286
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 61.76713585853577
Epoch: 27, Steps: 92 | Train Loss: 0.2532375 Vali Loss: 0.3429866 Test Loss: 0.4213561
Validation loss decreased (0.343803 --> 0.342987).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 64.4686815738678
Epoch: 28, Steps: 92 | Train Loss: 0.2531742 Vali Loss: 0.3445697 Test Loss: 0.4213638
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 58.295982837677
Epoch: 29, Steps: 92 | Train Loss: 0.2531261 Vali Loss: 0.3439867 Test Loss: 0.4213006
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 62.09801173210144
Epoch: 30, Steps: 92 | Train Loss: 0.2531864 Vali Loss: 0.3441412 Test Loss: 0.4213690
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 60.320068359375
Epoch: 31, Steps: 92 | Train Loss: 0.2531057 Vali Loss: 0.3438011 Test Loss: 0.4213229
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 61.59003949165344
Epoch: 32, Steps: 92 | Train Loss: 0.2530629 Vali Loss: 0.3437704 Test Loss: 0.4213203
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 58.21977138519287
Epoch: 33, Steps: 92 | Train Loss: 0.2530733 Vali Loss: 0.3439016 Test Loss: 0.4213260
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 63.24569392204285
Epoch: 34, Steps: 92 | Train Loss: 0.2531580 Vali Loss: 0.3440796 Test Loss: 0.4212929
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 60.423569202423096
Epoch: 35, Steps: 92 | Train Loss: 0.2530676 Vali Loss: 0.3443173 Test Loss: 0.4212970
EarlyStopping counter: 8 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 59.24533700942993
Epoch: 36, Steps: 92 | Train Loss: 0.2530335 Vali Loss: 0.3437530 Test Loss: 0.4212589
EarlyStopping counter: 9 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 59.8503794670105
Epoch: 37, Steps: 92 | Train Loss: 0.2530821 Vali Loss: 0.3439840 Test Loss: 0.4212706
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j96_H5_FITS_custom_ftM_sl360_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.419902503490448, mae:0.29336121678352356, rse:0.5365716218948364, corr:[0.2882985  0.29264402 0.29372746 0.29512545 0.29509935 0.29462072
 0.29488087 0.29544958 0.29546815 0.29504287 0.29486328 0.2949509
 0.2948233  0.2943596  0.29401752 0.29405686 0.29419422 0.29415944
 0.29399425 0.2939081  0.2938487  0.29364198 0.29356283 0.2943201
 0.2955386  0.29595834 0.29585177 0.29547772 0.29506493 0.29485825
 0.29482868 0.2947706  0.29461852 0.29450977 0.29452315 0.2944312
 0.29413372 0.29378182 0.29366946 0.2937813  0.29385504 0.29373896
 0.2935841  0.2936539  0.29396486 0.29426384 0.2944066  0.29460543
 0.29478592 0.2946449  0.2944502  0.29433692 0.29430893 0.29430306
 0.29426548 0.29415387 0.2939557  0.29372317 0.2935419  0.29341793
 0.29339564 0.29346126 0.29361027 0.29377255 0.29388282 0.29394245
 0.29398078 0.29402125 0.29404232 0.29399988 0.29385647 0.29371676
 0.29352582 0.29338318 0.2934359  0.2936062  0.2936484  0.29344556
 0.29313514 0.29286432 0.29267618 0.2926356  0.29281092 0.29310682
 0.2933785  0.293603   0.29389337 0.29412395 0.29394478 0.2933567
 0.29288158 0.2927269  0.29211468 0.29050705 0.28995833 0.2941876 ]
