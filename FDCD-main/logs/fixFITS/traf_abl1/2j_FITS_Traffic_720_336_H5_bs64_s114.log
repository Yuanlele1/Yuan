Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j336_H5_FITS_custom_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4387511040.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 150.43819570541382
Epoch: 1, Steps: 87 | Train Loss: 1.1081832 Vali Loss: 1.1830826 Test Loss: 1.3732862
Validation loss decreased (inf --> 1.183083).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 146.40579843521118
Epoch: 2, Steps: 87 | Train Loss: 0.8277516 Vali Loss: 1.0307078 Test Loss: 1.1914116
Validation loss decreased (1.183083 --> 1.030708).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 138.4289267063141
Epoch: 3, Steps: 87 | Train Loss: 0.7208235 Vali Loss: 0.9562986 Test Loss: 1.1047215
Validation loss decreased (1.030708 --> 0.956299).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 140.03975009918213
Epoch: 4, Steps: 87 | Train Loss: 0.6521669 Vali Loss: 0.9013519 Test Loss: 1.0412264
Validation loss decreased (0.956299 --> 0.901352).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 138.51298332214355
Epoch: 5, Steps: 87 | Train Loss: 0.5976702 Vali Loss: 0.8549337 Test Loss: 0.9876903
Validation loss decreased (0.901352 --> 0.854934).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 144.61052107810974
Epoch: 6, Steps: 87 | Train Loss: 0.5518760 Vali Loss: 0.8140030 Test Loss: 0.9410774
Validation loss decreased (0.854934 --> 0.814003).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 138.94227004051208
Epoch: 7, Steps: 87 | Train Loss: 0.5126414 Vali Loss: 0.7785726 Test Loss: 0.9002786
Validation loss decreased (0.814003 --> 0.778573).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 138.48930287361145
Epoch: 8, Steps: 87 | Train Loss: 0.4785171 Vali Loss: 0.7448906 Test Loss: 0.8621485
Validation loss decreased (0.778573 --> 0.744891).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 143.8888816833496
Epoch: 9, Steps: 87 | Train Loss: 0.4486767 Vali Loss: 0.7164158 Test Loss: 0.8300264
Validation loss decreased (0.744891 --> 0.716416).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 139.59431266784668
Epoch: 10, Steps: 87 | Train Loss: 0.4223798 Vali Loss: 0.6916252 Test Loss: 0.8014719
Validation loss decreased (0.716416 --> 0.691625).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 131.70716285705566
Epoch: 11, Steps: 87 | Train Loss: 0.3989637 Vali Loss: 0.6667538 Test Loss: 0.7735394
Validation loss decreased (0.691625 --> 0.666754).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 128.7272174358368
Epoch: 12, Steps: 87 | Train Loss: 0.3781493 Vali Loss: 0.6458584 Test Loss: 0.7491235
Validation loss decreased (0.666754 --> 0.645858).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 127.13232612609863
Epoch: 13, Steps: 87 | Train Loss: 0.3595505 Vali Loss: 0.6261529 Test Loss: 0.7269900
Validation loss decreased (0.645858 --> 0.626153).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 128.91126132011414
Epoch: 14, Steps: 87 | Train Loss: 0.3428373 Vali Loss: 0.6091918 Test Loss: 0.7075467
Validation loss decreased (0.626153 --> 0.609192).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 140.64414262771606
Epoch: 15, Steps: 87 | Train Loss: 0.3277069 Vali Loss: 0.5931814 Test Loss: 0.6889093
Validation loss decreased (0.609192 --> 0.593181).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 128.74130582809448
Epoch: 16, Steps: 87 | Train Loss: 0.3141209 Vali Loss: 0.5794718 Test Loss: 0.6733946
Validation loss decreased (0.593181 --> 0.579472).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 125.80855202674866
Epoch: 17, Steps: 87 | Train Loss: 0.3017547 Vali Loss: 0.5650389 Test Loss: 0.6572025
Validation loss decreased (0.579472 --> 0.565039).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 125.55455207824707
Epoch: 18, Steps: 87 | Train Loss: 0.2905123 Vali Loss: 0.5546870 Test Loss: 0.6453250
Validation loss decreased (0.565039 --> 0.554687).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 128.91215538978577
Epoch: 19, Steps: 87 | Train Loss: 0.2802925 Vali Loss: 0.5431457 Test Loss: 0.6326927
Validation loss decreased (0.554687 --> 0.543146).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 126.69709157943726
Epoch: 20, Steps: 87 | Train Loss: 0.2709503 Vali Loss: 0.5331382 Test Loss: 0.6210189
Validation loss decreased (0.543146 --> 0.533138).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 130.7177379131317
Epoch: 21, Steps: 87 | Train Loss: 0.2623478 Vali Loss: 0.5240268 Test Loss: 0.6109095
Validation loss decreased (0.533138 --> 0.524027).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 127.55956768989563
Epoch: 22, Steps: 87 | Train Loss: 0.2544827 Vali Loss: 0.5152626 Test Loss: 0.6005129
Validation loss decreased (0.524027 --> 0.515263).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 127.96234488487244
Epoch: 23, Steps: 87 | Train Loss: 0.2472724 Vali Loss: 0.5079952 Test Loss: 0.5918656
Validation loss decreased (0.515263 --> 0.507995).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 126.4539725780487
Epoch: 24, Steps: 87 | Train Loss: 0.2406281 Vali Loss: 0.5007631 Test Loss: 0.5838441
Validation loss decreased (0.507995 --> 0.500763).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 123.87086534500122
Epoch: 25, Steps: 87 | Train Loss: 0.2345323 Vali Loss: 0.4933459 Test Loss: 0.5760930
Validation loss decreased (0.500763 --> 0.493346).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 119.46684646606445
Epoch: 26, Steps: 87 | Train Loss: 0.2288595 Vali Loss: 0.4868611 Test Loss: 0.5683185
Validation loss decreased (0.493346 --> 0.486861).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 123.03483176231384
Epoch: 27, Steps: 87 | Train Loss: 0.2236428 Vali Loss: 0.4809123 Test Loss: 0.5619490
Validation loss decreased (0.486861 --> 0.480912).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 128.21853399276733
Epoch: 28, Steps: 87 | Train Loss: 0.2188137 Vali Loss: 0.4759595 Test Loss: 0.5562148
Validation loss decreased (0.480912 --> 0.475960).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 133.04023385047913
Epoch: 29, Steps: 87 | Train Loss: 0.2143144 Vali Loss: 0.4714242 Test Loss: 0.5505001
Validation loss decreased (0.475960 --> 0.471424).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 132.07105803489685
Epoch: 30, Steps: 87 | Train Loss: 0.2101265 Vali Loss: 0.4669546 Test Loss: 0.5456221
Validation loss decreased (0.471424 --> 0.466955).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 133.83140182495117
Epoch: 31, Steps: 87 | Train Loss: 0.2062687 Vali Loss: 0.4625537 Test Loss: 0.5412039
Validation loss decreased (0.466955 --> 0.462554).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 134.0461802482605
Epoch: 32, Steps: 87 | Train Loss: 0.2026422 Vali Loss: 0.4582868 Test Loss: 0.5362409
Validation loss decreased (0.462554 --> 0.458287).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 134.92168807983398
Epoch: 33, Steps: 87 | Train Loss: 0.1992589 Vali Loss: 0.4543375 Test Loss: 0.5318431
Validation loss decreased (0.458287 --> 0.454337).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 137.36654806137085
Epoch: 34, Steps: 87 | Train Loss: 0.1960978 Vali Loss: 0.4513272 Test Loss: 0.5279655
Validation loss decreased (0.454337 --> 0.451327).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 136.14923334121704
Epoch: 35, Steps: 87 | Train Loss: 0.1931369 Vali Loss: 0.4472103 Test Loss: 0.5242693
Validation loss decreased (0.451327 --> 0.447210).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 132.23483610153198
Epoch: 36, Steps: 87 | Train Loss: 0.1904165 Vali Loss: 0.4445880 Test Loss: 0.5207905
Validation loss decreased (0.447210 --> 0.444588).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 132.51997685432434
Epoch: 37, Steps: 87 | Train Loss: 0.1878670 Vali Loss: 0.4412280 Test Loss: 0.5177917
Validation loss decreased (0.444588 --> 0.441228).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 133.91726779937744
Epoch: 38, Steps: 87 | Train Loss: 0.1854557 Vali Loss: 0.4383762 Test Loss: 0.5144594
Validation loss decreased (0.441228 --> 0.438376).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 129.9258828163147
Epoch: 39, Steps: 87 | Train Loss: 0.1831859 Vali Loss: 0.4360410 Test Loss: 0.5115680
Validation loss decreased (0.438376 --> 0.436041).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 134.62559032440186
Epoch: 40, Steps: 87 | Train Loss: 0.1810359 Vali Loss: 0.4337087 Test Loss: 0.5091486
Validation loss decreased (0.436041 --> 0.433709).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 131.87500882148743
Epoch: 41, Steps: 87 | Train Loss: 0.1790720 Vali Loss: 0.4320005 Test Loss: 0.5067083
Validation loss decreased (0.433709 --> 0.432001).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 131.93256187438965
Epoch: 42, Steps: 87 | Train Loss: 0.1771754 Vali Loss: 0.4294138 Test Loss: 0.5041988
Validation loss decreased (0.432001 --> 0.429414).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 134.42162108421326
Epoch: 43, Steps: 87 | Train Loss: 0.1754215 Vali Loss: 0.4270649 Test Loss: 0.5018713
Validation loss decreased (0.429414 --> 0.427065).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 134.80559253692627
Epoch: 44, Steps: 87 | Train Loss: 0.1737835 Vali Loss: 0.4259751 Test Loss: 0.5000613
Validation loss decreased (0.427065 --> 0.425975).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 122.29534792900085
Epoch: 45, Steps: 87 | Train Loss: 0.1721814 Vali Loss: 0.4239028 Test Loss: 0.4979289
Validation loss decreased (0.425975 --> 0.423903).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 125.6400887966156
Epoch: 46, Steps: 87 | Train Loss: 0.1707135 Vali Loss: 0.4221913 Test Loss: 0.4962589
Validation loss decreased (0.423903 --> 0.422191).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 128.92942547798157
Epoch: 47, Steps: 87 | Train Loss: 0.1693286 Vali Loss: 0.4203987 Test Loss: 0.4941444
Validation loss decreased (0.422191 --> 0.420399).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 126.60614585876465
Epoch: 48, Steps: 87 | Train Loss: 0.1679872 Vali Loss: 0.4192024 Test Loss: 0.4926164
Validation loss decreased (0.420399 --> 0.419202).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 130.91028118133545
Epoch: 49, Steps: 87 | Train Loss: 0.1667777 Vali Loss: 0.4175529 Test Loss: 0.4911398
Validation loss decreased (0.419202 --> 0.417553).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 130.89560294151306
Epoch: 50, Steps: 87 | Train Loss: 0.1656145 Vali Loss: 0.4162190 Test Loss: 0.4897269
Validation loss decreased (0.417553 --> 0.416219).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 126.17247676849365
Epoch: 51, Steps: 87 | Train Loss: 0.1645302 Vali Loss: 0.4147506 Test Loss: 0.4883254
Validation loss decreased (0.416219 --> 0.414751).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 126.25135779380798
Epoch: 52, Steps: 87 | Train Loss: 0.1634537 Vali Loss: 0.4130055 Test Loss: 0.4869687
Validation loss decreased (0.414751 --> 0.413005).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 124.58273458480835
Epoch: 53, Steps: 87 | Train Loss: 0.1624811 Vali Loss: 0.4123141 Test Loss: 0.4858118
Validation loss decreased (0.413005 --> 0.412314).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 125.72313714027405
Epoch: 54, Steps: 87 | Train Loss: 0.1615335 Vali Loss: 0.4114538 Test Loss: 0.4846186
Validation loss decreased (0.412314 --> 0.411454).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 128.2164556980133
Epoch: 55, Steps: 87 | Train Loss: 0.1606313 Vali Loss: 0.4102161 Test Loss: 0.4835381
Validation loss decreased (0.411454 --> 0.410216).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 122.07909560203552
Epoch: 56, Steps: 87 | Train Loss: 0.1598330 Vali Loss: 0.4091702 Test Loss: 0.4823912
Validation loss decreased (0.410216 --> 0.409170).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 126.75658988952637
Epoch: 57, Steps: 87 | Train Loss: 0.1590344 Vali Loss: 0.4088519 Test Loss: 0.4813863
Validation loss decreased (0.409170 --> 0.408852).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 129.68743658065796
Epoch: 58, Steps: 87 | Train Loss: 0.1582841 Vali Loss: 0.4073971 Test Loss: 0.4804806
Validation loss decreased (0.408852 --> 0.407397).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 119.98192048072815
Epoch: 59, Steps: 87 | Train Loss: 0.1575587 Vali Loss: 0.4067998 Test Loss: 0.4795501
Validation loss decreased (0.407397 --> 0.406800).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 116.75214123725891
Epoch: 60, Steps: 87 | Train Loss: 0.1568878 Vali Loss: 0.4061604 Test Loss: 0.4788318
Validation loss decreased (0.406800 --> 0.406160).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 123.18605852127075
Epoch: 61, Steps: 87 | Train Loss: 0.1562369 Vali Loss: 0.4053777 Test Loss: 0.4779562
Validation loss decreased (0.406160 --> 0.405378).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 122.3820264339447
Epoch: 62, Steps: 87 | Train Loss: 0.1556378 Vali Loss: 0.4046059 Test Loss: 0.4772191
Validation loss decreased (0.405378 --> 0.404606).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 123.13302850723267
Epoch: 63, Steps: 87 | Train Loss: 0.1550697 Vali Loss: 0.4039708 Test Loss: 0.4764047
Validation loss decreased (0.404606 --> 0.403971).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 120.17889261245728
Epoch: 64, Steps: 87 | Train Loss: 0.1545258 Vali Loss: 0.4030710 Test Loss: 0.4758207
Validation loss decreased (0.403971 --> 0.403071).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 119.67994403839111
Epoch: 65, Steps: 87 | Train Loss: 0.1539869 Vali Loss: 0.4026578 Test Loss: 0.4751112
Validation loss decreased (0.403071 --> 0.402658).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 121.80572938919067
Epoch: 66, Steps: 87 | Train Loss: 0.1535046 Vali Loss: 0.4018239 Test Loss: 0.4745110
Validation loss decreased (0.402658 --> 0.401824).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 120.07882785797119
Epoch: 67, Steps: 87 | Train Loss: 0.1530331 Vali Loss: 0.4020895 Test Loss: 0.4739503
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 121.92493629455566
Epoch: 68, Steps: 87 | Train Loss: 0.1525944 Vali Loss: 0.4009398 Test Loss: 0.4733723
Validation loss decreased (0.401824 --> 0.400940).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 117.45077276229858
Epoch: 69, Steps: 87 | Train Loss: 0.1521929 Vali Loss: 0.4003188 Test Loss: 0.4727826
Validation loss decreased (0.400940 --> 0.400319).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 115.87796258926392
Epoch: 70, Steps: 87 | Train Loss: 0.1517823 Vali Loss: 0.4004375 Test Loss: 0.4723059
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 124.2822482585907
Epoch: 71, Steps: 87 | Train Loss: 0.1513899 Vali Loss: 0.3993215 Test Loss: 0.4717872
Validation loss decreased (0.400319 --> 0.399322).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 123.78787326812744
Epoch: 72, Steps: 87 | Train Loss: 0.1510387 Vali Loss: 0.3991807 Test Loss: 0.4713851
Validation loss decreased (0.399322 --> 0.399181).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 118.5465931892395
Epoch: 73, Steps: 87 | Train Loss: 0.1507095 Vali Loss: 0.3986932 Test Loss: 0.4709577
Validation loss decreased (0.399181 --> 0.398693).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 114.18493294715881
Epoch: 74, Steps: 87 | Train Loss: 0.1503875 Vali Loss: 0.3983402 Test Loss: 0.4705689
Validation loss decreased (0.398693 --> 0.398340).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 119.01001024246216
Epoch: 75, Steps: 87 | Train Loss: 0.1500877 Vali Loss: 0.3982675 Test Loss: 0.4701611
Validation loss decreased (0.398340 --> 0.398267).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 118.44806790351868
Epoch: 76, Steps: 87 | Train Loss: 0.1497626 Vali Loss: 0.3977726 Test Loss: 0.4697753
Validation loss decreased (0.398267 --> 0.397773).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 123.19986486434937
Epoch: 77, Steps: 87 | Train Loss: 0.1494927 Vali Loss: 0.3973902 Test Loss: 0.4694621
Validation loss decreased (0.397773 --> 0.397390).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 118.43188166618347
Epoch: 78, Steps: 87 | Train Loss: 0.1492103 Vali Loss: 0.3972192 Test Loss: 0.4691186
Validation loss decreased (0.397390 --> 0.397219).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 118.51412582397461
Epoch: 79, Steps: 87 | Train Loss: 0.1489653 Vali Loss: 0.3965999 Test Loss: 0.4688050
Validation loss decreased (0.397219 --> 0.396600).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 128.5332796573639
Epoch: 80, Steps: 87 | Train Loss: 0.1487221 Vali Loss: 0.3965617 Test Loss: 0.4684962
Validation loss decreased (0.396600 --> 0.396562).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 129.51732230186462
Epoch: 81, Steps: 87 | Train Loss: 0.1485040 Vali Loss: 0.3960527 Test Loss: 0.4682296
Validation loss decreased (0.396562 --> 0.396053).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 128.1504340171814
Epoch: 82, Steps: 87 | Train Loss: 0.1482776 Vali Loss: 0.3962488 Test Loss: 0.4679658
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 123.61949110031128
Epoch: 83, Steps: 87 | Train Loss: 0.1480889 Vali Loss: 0.3959096 Test Loss: 0.4676959
Validation loss decreased (0.396053 --> 0.395910).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 127.03810238838196
Epoch: 84, Steps: 87 | Train Loss: 0.1478731 Vali Loss: 0.3959025 Test Loss: 0.4674504
Validation loss decreased (0.395910 --> 0.395903).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 122.57842445373535
Epoch: 85, Steps: 87 | Train Loss: 0.1477258 Vali Loss: 0.3953698 Test Loss: 0.4672190
Validation loss decreased (0.395903 --> 0.395370).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 129.5407223701477
Epoch: 86, Steps: 87 | Train Loss: 0.1475326 Vali Loss: 0.3953820 Test Loss: 0.4670023
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 129.04020309448242
Epoch: 87, Steps: 87 | Train Loss: 0.1473751 Vali Loss: 0.3949144 Test Loss: 0.4667945
Validation loss decreased (0.395370 --> 0.394914).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 126.99403262138367
Epoch: 88, Steps: 87 | Train Loss: 0.1471937 Vali Loss: 0.3945355 Test Loss: 0.4665915
Validation loss decreased (0.394914 --> 0.394535).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 121.71330881118774
Epoch: 89, Steps: 87 | Train Loss: 0.1470492 Vali Loss: 0.3948445 Test Loss: 0.4663905
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 126.036954164505
Epoch: 90, Steps: 87 | Train Loss: 0.1469058 Vali Loss: 0.3944440 Test Loss: 0.4662100
Validation loss decreased (0.394535 --> 0.394444).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 127.50876355171204
Epoch: 91, Steps: 87 | Train Loss: 0.1467540 Vali Loss: 0.3941466 Test Loss: 0.4660486
Validation loss decreased (0.394444 --> 0.394147).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 128.66243982315063
Epoch: 92, Steps: 87 | Train Loss: 0.1466413 Vali Loss: 0.3940277 Test Loss: 0.4658724
Validation loss decreased (0.394147 --> 0.394028).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 133.12396597862244
Epoch: 93, Steps: 87 | Train Loss: 0.1465282 Vali Loss: 0.3938976 Test Loss: 0.4657206
Validation loss decreased (0.394028 --> 0.393898).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 123.22088313102722
Epoch: 94, Steps: 87 | Train Loss: 0.1464183 Vali Loss: 0.3936581 Test Loss: 0.4655840
Validation loss decreased (0.393898 --> 0.393658).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 129.9146671295166
Epoch: 95, Steps: 87 | Train Loss: 0.1462797 Vali Loss: 0.3936396 Test Loss: 0.4654309
Validation loss decreased (0.393658 --> 0.393640).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 134.3483922481537
Epoch: 96, Steps: 87 | Train Loss: 0.1461655 Vali Loss: 0.3935748 Test Loss: 0.4653112
Validation loss decreased (0.393640 --> 0.393575).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 128.23014307022095
Epoch: 97, Steps: 87 | Train Loss: 0.1460780 Vali Loss: 0.3931748 Test Loss: 0.4651773
Validation loss decreased (0.393575 --> 0.393175).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 127.74217557907104
Epoch: 98, Steps: 87 | Train Loss: 0.1459797 Vali Loss: 0.3931637 Test Loss: 0.4650725
Validation loss decreased (0.393175 --> 0.393164).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 126.90344858169556
Epoch: 99, Steps: 87 | Train Loss: 0.1459086 Vali Loss: 0.3930988 Test Loss: 0.4649478
Validation loss decreased (0.393164 --> 0.393099).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 126.36486196517944
Epoch: 100, Steps: 87 | Train Loss: 0.1458022 Vali Loss: 0.3930935 Test Loss: 0.4648310
Validation loss decreased (0.393099 --> 0.393093).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4387511040.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 108.58016061782837
Epoch: 1, Steps: 87 | Train Loss: 0.2649332 Vali Loss: 0.3507743 Test Loss: 0.4237024
Validation loss decreased (inf --> 0.350774).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 97.98377847671509
Epoch: 2, Steps: 87 | Train Loss: 0.2545359 Vali Loss: 0.3488051 Test Loss: 0.4233158
Validation loss decreased (0.350774 --> 0.348805).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 97.75919270515442
Epoch: 3, Steps: 87 | Train Loss: 0.2541636 Vali Loss: 0.3484266 Test Loss: 0.4230824
Validation loss decreased (0.348805 --> 0.348427).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 97.68255805969238
Epoch: 4, Steps: 87 | Train Loss: 0.2539704 Vali Loss: 0.3477860 Test Loss: 0.4232659
Validation loss decreased (0.348427 --> 0.347786).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 100.0756528377533
Epoch: 5, Steps: 87 | Train Loss: 0.2539819 Vali Loss: 0.3474992 Test Loss: 0.4229164
Validation loss decreased (0.347786 --> 0.347499).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 107.35533547401428
Epoch: 6, Steps: 87 | Train Loss: 0.2539809 Vali Loss: 0.3476546 Test Loss: 0.4232013
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 105.03547883033752
Epoch: 7, Steps: 87 | Train Loss: 0.2539019 Vali Loss: 0.3473686 Test Loss: 0.4224300
Validation loss decreased (0.347499 --> 0.347369).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 106.9153048992157
Epoch: 8, Steps: 87 | Train Loss: 0.2538259 Vali Loss: 0.3476104 Test Loss: 0.4226836
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 104.44006395339966
Epoch: 9, Steps: 87 | Train Loss: 0.2537907 Vali Loss: 0.3479718 Test Loss: 0.4230953
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 105.00390720367432
Epoch: 10, Steps: 87 | Train Loss: 0.2538503 Vali Loss: 0.3472809 Test Loss: 0.4223818
Validation loss decreased (0.347369 --> 0.347281).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 109.31036520004272
Epoch: 11, Steps: 87 | Train Loss: 0.2537472 Vali Loss: 0.3476554 Test Loss: 0.4223427
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 96.43917870521545
Epoch: 12, Steps: 87 | Train Loss: 0.2537042 Vali Loss: 0.3474141 Test Loss: 0.4224328
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 81.1825520992279
Epoch: 13, Steps: 87 | Train Loss: 0.2537329 Vali Loss: 0.3475505 Test Loss: 0.4224925
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 81.91551566123962
Epoch: 14, Steps: 87 | Train Loss: 0.2537455 Vali Loss: 0.3472182 Test Loss: 0.4227886
Validation loss decreased (0.347281 --> 0.347218).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 84.35402226448059
Epoch: 15, Steps: 87 | Train Loss: 0.2537022 Vali Loss: 0.3471918 Test Loss: 0.4223888
Validation loss decreased (0.347218 --> 0.347192).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 83.51345562934875
Epoch: 16, Steps: 87 | Train Loss: 0.2537050 Vali Loss: 0.3475283 Test Loss: 0.4220961
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 79.69485187530518
Epoch: 17, Steps: 87 | Train Loss: 0.2536430 Vali Loss: 0.3474171 Test Loss: 0.4226913
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 80.20694613456726
Epoch: 18, Steps: 87 | Train Loss: 0.2536103 Vali Loss: 0.3474098 Test Loss: 0.4221442
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 80.01735734939575
Epoch: 19, Steps: 87 | Train Loss: 0.2534838 Vali Loss: 0.3478462 Test Loss: 0.4224924
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 79.3943178653717
Epoch: 20, Steps: 87 | Train Loss: 0.2535772 Vali Loss: 0.3475639 Test Loss: 0.4221642
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 76.78882956504822
Epoch: 21, Steps: 87 | Train Loss: 0.2535993 Vali Loss: 0.3474604 Test Loss: 0.4223313
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 73.12882852554321
Epoch: 22, Steps: 87 | Train Loss: 0.2535106 Vali Loss: 0.3474378 Test Loss: 0.4224856
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 75.55367851257324
Epoch: 23, Steps: 87 | Train Loss: 0.2535071 Vali Loss: 0.3473285 Test Loss: 0.4223445
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 74.74794292449951
Epoch: 24, Steps: 87 | Train Loss: 0.2534722 Vali Loss: 0.3470840 Test Loss: 0.4222082
Validation loss decreased (0.347192 --> 0.347084).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 75.54478168487549
Epoch: 25, Steps: 87 | Train Loss: 0.2535054 Vali Loss: 0.3472804 Test Loss: 0.4220260
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 72.43386721611023
Epoch: 26, Steps: 87 | Train Loss: 0.2533900 Vali Loss: 0.3470404 Test Loss: 0.4223119
Validation loss decreased (0.347084 --> 0.347040).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 71.8351583480835
Epoch: 27, Steps: 87 | Train Loss: 0.2534840 Vali Loss: 0.3473561 Test Loss: 0.4222426
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 76.15634727478027
Epoch: 28, Steps: 87 | Train Loss: 0.2533836 Vali Loss: 0.3471845 Test Loss: 0.4220367
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 75.50193405151367
Epoch: 29, Steps: 87 | Train Loss: 0.2534164 Vali Loss: 0.3474865 Test Loss: 0.4224002
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 74.62387323379517
Epoch: 30, Steps: 87 | Train Loss: 0.2534740 Vali Loss: 0.3474529 Test Loss: 0.4224235
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 72.75162863731384
Epoch: 31, Steps: 87 | Train Loss: 0.2534989 Vali Loss: 0.3474426 Test Loss: 0.4221908
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 75.48961615562439
Epoch: 32, Steps: 87 | Train Loss: 0.2534338 Vali Loss: 0.3473736 Test Loss: 0.4221075
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 74.16247701644897
Epoch: 33, Steps: 87 | Train Loss: 0.2533374 Vali Loss: 0.3472427 Test Loss: 0.4222000
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 74.27429842948914
Epoch: 34, Steps: 87 | Train Loss: 0.2533828 Vali Loss: 0.3471412 Test Loss: 0.4223155
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 74.09560346603394
Epoch: 35, Steps: 87 | Train Loss: 0.2533788 Vali Loss: 0.3472589 Test Loss: 0.4224266
EarlyStopping counter: 9 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 74.21917605400085
Epoch: 36, Steps: 87 | Train Loss: 0.2534090 Vali Loss: 0.3470875 Test Loss: 0.4221713
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j336_H5_FITS_custom_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4202391803264618, mae:0.29217877984046936, rse:0.5327795743942261, corr:[0.2816173  0.28799316 0.2882113  0.2883708  0.2882395  0.28779343
 0.28754738 0.28769362 0.2879481  0.2879985  0.28788164 0.2878005
 0.2878004  0.28773192 0.28759447 0.28750822 0.28753594 0.28756982
 0.2874808  0.2873266  0.28721812 0.28715485 0.2871709  0.2877023
 0.28863248 0.28869945 0.28859484 0.28864756 0.2885598  0.28821072
 0.28779972 0.28752953 0.2875264  0.28771803 0.2878363  0.28782043
 0.2878133  0.28784084 0.28786612 0.2878605  0.28786594 0.28793597
 0.288131   0.28829497 0.2883362  0.28817734 0.28793645 0.28801385
 0.28841713 0.28863615 0.28871226 0.2885935  0.28825048 0.28782284
 0.28748953 0.2873445  0.2873907  0.28758505 0.28777885 0.2878467
 0.28784263 0.28783157 0.28790924 0.2880232  0.28794247 0.2876231
 0.28745508 0.28742835 0.28754616 0.28765604 0.28757295 0.2874004
 0.2871638  0.28705725 0.28717512 0.2872688  0.2872434  0.28720087
 0.28725475 0.28738907 0.28749374 0.28746372 0.28729472 0.28711158
 0.28707936 0.28717858 0.2872917  0.28728673 0.28716865 0.2870643
 0.28711414 0.28719187 0.28724027 0.28719416 0.286998   0.28674853
 0.28644758 0.2864043  0.28665027 0.28690353 0.28702447 0.28704083
 0.28701928 0.28699508 0.28697866 0.28695086 0.28689587 0.28681487
 0.28675297 0.28672668 0.2867325  0.2867648  0.28678894 0.286786
 0.28677002 0.2867185  0.28669855 0.28668866 0.28678164 0.28688395
 0.2868697  0.28694323 0.2871557  0.2872708  0.28729895 0.28725758
 0.287187   0.28717977 0.28730083 0.28750807 0.2876902  0.2877515
 0.28765157 0.2874739  0.28738123 0.2874471  0.28759745 0.28767318
 0.28763947 0.2875617  0.28752857 0.28757426 0.28760412 0.287625
 0.28765485 0.2875968  0.28759953 0.28771445 0.2878817  0.28795105
 0.28780589 0.2875248  0.28729898 0.28727087 0.2874142  0.2876095
 0.28774402 0.28777564 0.28773254 0.28767568 0.2876607  0.2876411
 0.287605   0.28756434 0.28751183 0.28741488 0.28731525 0.28766176
 0.28847897 0.28862914 0.2886497  0.2887891  0.28885737 0.28878322
 0.2885907  0.28835672 0.2881608  0.28804207 0.2879844  0.28803834
 0.28823105 0.28839493 0.28842673 0.28829166 0.28805426 0.28781575
 0.28760746 0.28753906 0.28758737 0.28760958 0.28758594 0.2878161
 0.28828773 0.2883852  0.28848448 0.28867996 0.28870556 0.28844935
 0.28804758 0.28775296 0.2877269  0.28789312 0.28797606 0.28785723
 0.28762457 0.2873616  0.28716397 0.28702578 0.2869036  0.28678805
 0.28668225 0.2866336  0.28666624 0.28673062 0.2867694  0.2869292
 0.28719065 0.2873084  0.28741634 0.28749564 0.2873983  0.2870964
 0.2867196  0.28646576 0.286452   0.28660032 0.2866737  0.28657597
 0.28643385 0.28632835 0.28632265 0.28640914 0.28653428 0.286613
 0.28656885 0.28643575 0.28643084 0.28649047 0.28661034 0.28672525
 0.28671542 0.28661212 0.2866055  0.28672087 0.28688723 0.28701368
 0.28704295 0.28699902 0.28695562 0.28694233 0.28691515 0.2868302
 0.2867143  0.28658617 0.28647345 0.28637162 0.28625032 0.28611404
 0.28593227 0.28575072 0.28567284 0.28571972 0.2858292  0.28590134
 0.28576544 0.28558734 0.28551427 0.28552142 0.28568417 0.2859882
 0.28622147 0.28623804 0.28609914 0.28595904 0.28589445 0.28584525
 0.2857888  0.28574097 0.2857486  0.28579897 0.28584453 0.2858563
 0.28582054 0.28577816 0.2857565  0.28577152 0.28586078 0.28595734
 0.28592107 0.28599083 0.28620806 0.2862986  0.28621924 0.28604504
 0.28586528 0.28579718 0.28587887 0.28599536 0.2860365  0.28600445
 0.28593156 0.28582388 0.28567675 0.2855141  0.28543562 0.2855074
 0.2857379  0.28602856 0.2862581  0.28634894 0.28638282 0.28653213
 0.2867683  0.28682196 0.28670412 0.28649282 0.28629568 0.28615013
 0.28602636 0.28593183 0.2859254  0.28604418 0.28624886 0.28646535
 0.286653   0.28681746 0.2869238  0.28683    0.28649917 0.28617075
 0.2861925  0.28657007 0.28673923 0.28616348 0.28536204 0.28652093]
