Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j336_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j336_H8_FITS_custom_ftM_sl180_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11765
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=74, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1730951168.0
params:  15900.0
Trainable parameters:  15900
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 145.58722043037415
Epoch: 1, Steps: 91 | Train Loss: 1.2587695 Vali Loss: 1.2002301 Test Loss: 1.4548311
Validation loss decreased (inf --> 1.200230).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 145.66661429405212
Epoch: 2, Steps: 91 | Train Loss: 0.7476810 Vali Loss: 0.8871308 Test Loss: 1.0824124
Validation loss decreased (1.200230 --> 0.887131).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 129.5237193107605
Epoch: 3, Steps: 91 | Train Loss: 0.5555562 Vali Loss: 0.7397719 Test Loss: 0.9097272
Validation loss decreased (0.887131 --> 0.739772).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 127.30286574363708
Epoch: 4, Steps: 91 | Train Loss: 0.4602777 Vali Loss: 0.6585424 Test Loss: 0.8147804
Validation loss decreased (0.739772 --> 0.658542).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 125.70262408256531
Epoch: 5, Steps: 91 | Train Loss: 0.4053921 Vali Loss: 0.6087442 Test Loss: 0.7560067
Validation loss decreased (0.658542 --> 0.608744).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 121.86325335502625
Epoch: 6, Steps: 91 | Train Loss: 0.3697251 Vali Loss: 0.5741160 Test Loss: 0.7148290
Validation loss decreased (0.608744 --> 0.574116).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 134.33303499221802
Epoch: 7, Steps: 91 | Train Loss: 0.3441228 Vali Loss: 0.5483833 Test Loss: 0.6840842
Validation loss decreased (0.574116 --> 0.548383).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 128.1642894744873
Epoch: 8, Steps: 91 | Train Loss: 0.3245719 Vali Loss: 0.5287346 Test Loss: 0.6591684
Validation loss decreased (0.548383 --> 0.528735).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 132.85370230674744
Epoch: 9, Steps: 91 | Train Loss: 0.3089418 Vali Loss: 0.5126538 Test Loss: 0.6390007
Validation loss decreased (0.528735 --> 0.512654).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 120.76956033706665
Epoch: 10, Steps: 91 | Train Loss: 0.2961408 Vali Loss: 0.4985362 Test Loss: 0.6215509
Validation loss decreased (0.512654 --> 0.498536).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 130.3938591480255
Epoch: 11, Steps: 91 | Train Loss: 0.2853329 Vali Loss: 0.4869365 Test Loss: 0.6066927
Validation loss decreased (0.498536 --> 0.486936).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 125.82364630699158
Epoch: 12, Steps: 91 | Train Loss: 0.2761729 Vali Loss: 0.4771789 Test Loss: 0.5941095
Validation loss decreased (0.486936 --> 0.477179).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 122.07215094566345
Epoch: 13, Steps: 91 | Train Loss: 0.2682265 Vali Loss: 0.4684644 Test Loss: 0.5835472
Validation loss decreased (0.477179 --> 0.468464).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 127.33742022514343
Epoch: 14, Steps: 91 | Train Loss: 0.2613946 Vali Loss: 0.4609815 Test Loss: 0.5737761
Validation loss decreased (0.468464 --> 0.460981).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 126.21432518959045
Epoch: 15, Steps: 91 | Train Loss: 0.2554300 Vali Loss: 0.4541169 Test Loss: 0.5652655
Validation loss decreased (0.460981 --> 0.454117).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 128.19045543670654
Epoch: 16, Steps: 91 | Train Loss: 0.2501514 Vali Loss: 0.4484487 Test Loss: 0.5579184
Validation loss decreased (0.454117 --> 0.448449).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 119.88379669189453
Epoch: 17, Steps: 91 | Train Loss: 0.2455218 Vali Loss: 0.4437464 Test Loss: 0.5514783
Validation loss decreased (0.448449 --> 0.443746).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 124.13599681854248
Epoch: 18, Steps: 91 | Train Loss: 0.2414143 Vali Loss: 0.4391998 Test Loss: 0.5458183
Validation loss decreased (0.443746 --> 0.439200).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 120.24195265769958
Epoch: 19, Steps: 91 | Train Loss: 0.2377389 Vali Loss: 0.4348881 Test Loss: 0.5404733
Validation loss decreased (0.439200 --> 0.434888).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 120.74269795417786
Epoch: 20, Steps: 91 | Train Loss: 0.2344788 Vali Loss: 0.4315011 Test Loss: 0.5357818
Validation loss decreased (0.434888 --> 0.431501).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 125.79121732711792
Epoch: 21, Steps: 91 | Train Loss: 0.2315304 Vali Loss: 0.4280880 Test Loss: 0.5318356
Validation loss decreased (0.431501 --> 0.428088).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 123.16110396385193
Epoch: 22, Steps: 91 | Train Loss: 0.2288843 Vali Loss: 0.4253363 Test Loss: 0.5278674
Validation loss decreased (0.428088 --> 0.425336).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 127.30169486999512
Epoch: 23, Steps: 91 | Train Loss: 0.2265160 Vali Loss: 0.4227000 Test Loss: 0.5243698
Validation loss decreased (0.425336 --> 0.422700).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 124.96016597747803
Epoch: 24, Steps: 91 | Train Loss: 0.2243185 Vali Loss: 0.4196751 Test Loss: 0.5212609
Validation loss decreased (0.422700 --> 0.419675).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 111.98148941993713
Epoch: 25, Steps: 91 | Train Loss: 0.2223588 Vali Loss: 0.4175841 Test Loss: 0.5184937
Validation loss decreased (0.419675 --> 0.417584).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 97.19729971885681
Epoch: 26, Steps: 91 | Train Loss: 0.2204690 Vali Loss: 0.4159668 Test Loss: 0.5160220
Validation loss decreased (0.417584 --> 0.415967).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 100.83966493606567
Epoch: 27, Steps: 91 | Train Loss: 0.2188984 Vali Loss: 0.4139046 Test Loss: 0.5136642
Validation loss decreased (0.415967 --> 0.413905).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 107.50446486473083
Epoch: 28, Steps: 91 | Train Loss: 0.2174339 Vali Loss: 0.4122948 Test Loss: 0.5114357
Validation loss decreased (0.413905 --> 0.412295).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 111.84021472930908
Epoch: 29, Steps: 91 | Train Loss: 0.2160224 Vali Loss: 0.4106971 Test Loss: 0.5095014
Validation loss decreased (0.412295 --> 0.410697).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 123.86805295944214
Epoch: 30, Steps: 91 | Train Loss: 0.2148019 Vali Loss: 0.4091369 Test Loss: 0.5076888
Validation loss decreased (0.410697 --> 0.409137).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 122.49416923522949
Epoch: 31, Steps: 91 | Train Loss: 0.2136431 Vali Loss: 0.4081106 Test Loss: 0.5060573
Validation loss decreased (0.409137 --> 0.408111).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 126.06178975105286
Epoch: 32, Steps: 91 | Train Loss: 0.2124862 Vali Loss: 0.4065959 Test Loss: 0.5045404
Validation loss decreased (0.408111 --> 0.406596).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 123.98425507545471
Epoch: 33, Steps: 91 | Train Loss: 0.2115650 Vali Loss: 0.4059092 Test Loss: 0.5032202
Validation loss decreased (0.406596 --> 0.405909).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 111.25900793075562
Epoch: 34, Steps: 91 | Train Loss: 0.2106107 Vali Loss: 0.4046517 Test Loss: 0.5018817
Validation loss decreased (0.405909 --> 0.404652).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 126.19220018386841
Epoch: 35, Steps: 91 | Train Loss: 0.2097354 Vali Loss: 0.4040213 Test Loss: 0.5007005
Validation loss decreased (0.404652 --> 0.404021).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 124.96652793884277
Epoch: 36, Steps: 91 | Train Loss: 0.2089329 Vali Loss: 0.4028853 Test Loss: 0.4995805
Validation loss decreased (0.404021 --> 0.402885).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 124.20666265487671
Epoch: 37, Steps: 91 | Train Loss: 0.2082762 Vali Loss: 0.4025220 Test Loss: 0.4985693
Validation loss decreased (0.402885 --> 0.402522).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 121.31899189949036
Epoch: 38, Steps: 91 | Train Loss: 0.2075807 Vali Loss: 0.4013341 Test Loss: 0.4976186
Validation loss decreased (0.402522 --> 0.401334).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 119.84072685241699
Epoch: 39, Steps: 91 | Train Loss: 0.2070249 Vali Loss: 0.4007595 Test Loss: 0.4967512
Validation loss decreased (0.401334 --> 0.400759).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 110.19434785842896
Epoch: 40, Steps: 91 | Train Loss: 0.2064303 Vali Loss: 0.4000714 Test Loss: 0.4959367
Validation loss decreased (0.400759 --> 0.400071).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 103.07784724235535
Epoch: 41, Steps: 91 | Train Loss: 0.2058526 Vali Loss: 0.3993556 Test Loss: 0.4951237
Validation loss decreased (0.400071 --> 0.399356).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 120.19682216644287
Epoch: 42, Steps: 91 | Train Loss: 0.2053871 Vali Loss: 0.3991696 Test Loss: 0.4944623
Validation loss decreased (0.399356 --> 0.399170).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 114.46836948394775
Epoch: 43, Steps: 91 | Train Loss: 0.2048463 Vali Loss: 0.3985305 Test Loss: 0.4938161
Validation loss decreased (0.399170 --> 0.398530).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 113.6470627784729
Epoch: 44, Steps: 91 | Train Loss: 0.2044223 Vali Loss: 0.3977632 Test Loss: 0.4931300
Validation loss decreased (0.398530 --> 0.397763).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 92.27674007415771
Epoch: 45, Steps: 91 | Train Loss: 0.2039745 Vali Loss: 0.3973594 Test Loss: 0.4926028
Validation loss decreased (0.397763 --> 0.397359).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 91.8219051361084
Epoch: 46, Steps: 91 | Train Loss: 0.2036117 Vali Loss: 0.3972519 Test Loss: 0.4920903
Validation loss decreased (0.397359 --> 0.397252).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 102.24628162384033
Epoch: 47, Steps: 91 | Train Loss: 0.2032990 Vali Loss: 0.3965133 Test Loss: 0.4915804
Validation loss decreased (0.397252 --> 0.396513).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 98.80634427070618
Epoch: 48, Steps: 91 | Train Loss: 0.2028777 Vali Loss: 0.3960354 Test Loss: 0.4911153
Validation loss decreased (0.396513 --> 0.396035).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 98.64401769638062
Epoch: 49, Steps: 91 | Train Loss: 0.2025343 Vali Loss: 0.3958681 Test Loss: 0.4906634
Validation loss decreased (0.396035 --> 0.395868).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 98.34230852127075
Epoch: 50, Steps: 91 | Train Loss: 0.2022326 Vali Loss: 0.3956861 Test Loss: 0.4902539
Validation loss decreased (0.395868 --> 0.395686).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 98.4256522655487
Epoch: 51, Steps: 91 | Train Loss: 0.2019966 Vali Loss: 0.3952966 Test Loss: 0.4898512
Validation loss decreased (0.395686 --> 0.395297).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 90.29582166671753
Epoch: 52, Steps: 91 | Train Loss: 0.2017395 Vali Loss: 0.3947260 Test Loss: 0.4895028
Validation loss decreased (0.395297 --> 0.394726).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 84.30882048606873
Epoch: 53, Steps: 91 | Train Loss: 0.2015081 Vali Loss: 0.3945055 Test Loss: 0.4891683
Validation loss decreased (0.394726 --> 0.394505).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 91.43024563789368
Epoch: 54, Steps: 91 | Train Loss: 0.2012535 Vali Loss: 0.3940638 Test Loss: 0.4888669
Validation loss decreased (0.394505 --> 0.394064).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 84.9289813041687
Epoch: 55, Steps: 91 | Train Loss: 0.2010112 Vali Loss: 0.3944056 Test Loss: 0.4885446
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 69.28306889533997
Epoch: 56, Steps: 91 | Train Loss: 0.2009090 Vali Loss: 0.3938470 Test Loss: 0.4882686
Validation loss decreased (0.394064 --> 0.393847).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 90.22731113433838
Epoch: 57, Steps: 91 | Train Loss: 0.2006063 Vali Loss: 0.3938613 Test Loss: 0.4880284
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 69.96925139427185
Epoch: 58, Steps: 91 | Train Loss: 0.2004190 Vali Loss: 0.3934540 Test Loss: 0.4877603
Validation loss decreased (0.393847 --> 0.393454).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 71.09813117980957
Epoch: 59, Steps: 91 | Train Loss: 0.2003147 Vali Loss: 0.3934946 Test Loss: 0.4875388
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 91.07026314735413
Epoch: 60, Steps: 91 | Train Loss: 0.2001386 Vali Loss: 0.3935357 Test Loss: 0.4873026
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 96.11669540405273
Epoch: 61, Steps: 91 | Train Loss: 0.1998751 Vali Loss: 0.3931609 Test Loss: 0.4871146
Validation loss decreased (0.393454 --> 0.393161).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 111.09390306472778
Epoch: 62, Steps: 91 | Train Loss: 0.1998282 Vali Loss: 0.3927186 Test Loss: 0.4869041
Validation loss decreased (0.393161 --> 0.392719).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 86.05683636665344
Epoch: 63, Steps: 91 | Train Loss: 0.1996854 Vali Loss: 0.3928554 Test Loss: 0.4867212
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 86.71033501625061
Epoch: 64, Steps: 91 | Train Loss: 0.1995279 Vali Loss: 0.3921063 Test Loss: 0.4865407
Validation loss decreased (0.392719 --> 0.392106).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 130.38083958625793
Epoch: 65, Steps: 91 | Train Loss: 0.1993891 Vali Loss: 0.3921051 Test Loss: 0.4863851
Validation loss decreased (0.392106 --> 0.392105).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 92.03456282615662
Epoch: 66, Steps: 91 | Train Loss: 0.1993140 Vali Loss: 0.3925960 Test Loss: 0.4862288
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 77.75752568244934
Epoch: 67, Steps: 91 | Train Loss: 0.1991609 Vali Loss: 0.3920916 Test Loss: 0.4860966
Validation loss decreased (0.392105 --> 0.392092).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 80.93765759468079
Epoch: 68, Steps: 91 | Train Loss: 0.1990459 Vali Loss: 0.3920112 Test Loss: 0.4859509
Validation loss decreased (0.392092 --> 0.392011).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 83.94479823112488
Epoch: 69, Steps: 91 | Train Loss: 0.1989911 Vali Loss: 0.3918318 Test Loss: 0.4858229
Validation loss decreased (0.392011 --> 0.391832).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 77.78148460388184
Epoch: 70, Steps: 91 | Train Loss: 0.1988759 Vali Loss: 0.3921674 Test Loss: 0.4856974
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 88.43164610862732
Epoch: 71, Steps: 91 | Train Loss: 0.1988401 Vali Loss: 0.3918664 Test Loss: 0.4855813
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 85.64970517158508
Epoch: 72, Steps: 91 | Train Loss: 0.1987155 Vali Loss: 0.3916705 Test Loss: 0.4854643
Validation loss decreased (0.391832 --> 0.391671).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 84.57795667648315
Epoch: 73, Steps: 91 | Train Loss: 0.1986212 Vali Loss: 0.3917635 Test Loss: 0.4853663
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 80.70950984954834
Epoch: 74, Steps: 91 | Train Loss: 0.1986026 Vali Loss: 0.3914403 Test Loss: 0.4852671
Validation loss decreased (0.391671 --> 0.391440).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 89.1330235004425
Epoch: 75, Steps: 91 | Train Loss: 0.1984954 Vali Loss: 0.3912604 Test Loss: 0.4851770
Validation loss decreased (0.391440 --> 0.391260).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 91.64380979537964
Epoch: 76, Steps: 91 | Train Loss: 0.1983975 Vali Loss: 0.3915305 Test Loss: 0.4850884
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 86.86273002624512
Epoch: 77, Steps: 91 | Train Loss: 0.1983255 Vali Loss: 0.3911473 Test Loss: 0.4849934
Validation loss decreased (0.391260 --> 0.391147).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 93.2143087387085
Epoch: 78, Steps: 91 | Train Loss: 0.1983071 Vali Loss: 0.3911098 Test Loss: 0.4849236
Validation loss decreased (0.391147 --> 0.391110).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 120.17985320091248
Epoch: 79, Steps: 91 | Train Loss: 0.1982217 Vali Loss: 0.3908556 Test Loss: 0.4848408
Validation loss decreased (0.391110 --> 0.390856).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 131.693665266037
Epoch: 80, Steps: 91 | Train Loss: 0.1981781 Vali Loss: 0.3910514 Test Loss: 0.4847805
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 132.9915828704834
Epoch: 81, Steps: 91 | Train Loss: 0.1981108 Vali Loss: 0.3911510 Test Loss: 0.4847149
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 133.0014772415161
Epoch: 82, Steps: 91 | Train Loss: 0.1980927 Vali Loss: 0.3909726 Test Loss: 0.4846516
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 85.26276969909668
Epoch: 83, Steps: 91 | Train Loss: 0.1980421 Vali Loss: 0.3905807 Test Loss: 0.4845840
Validation loss decreased (0.390856 --> 0.390581).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 83.89963817596436
Epoch: 84, Steps: 91 | Train Loss: 0.1980233 Vali Loss: 0.3905949 Test Loss: 0.4845314
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 72.52247428894043
Epoch: 85, Steps: 91 | Train Loss: 0.1980031 Vali Loss: 0.3908334 Test Loss: 0.4844801
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 74.35390758514404
Epoch: 86, Steps: 91 | Train Loss: 0.1978724 Vali Loss: 0.3906847 Test Loss: 0.4844284
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 80.88600492477417
Epoch: 87, Steps: 91 | Train Loss: 0.1979033 Vali Loss: 0.3903823 Test Loss: 0.4843793
Validation loss decreased (0.390581 --> 0.390382).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 83.75081968307495
Epoch: 88, Steps: 91 | Train Loss: 0.1978592 Vali Loss: 0.3901506 Test Loss: 0.4843336
Validation loss decreased (0.390382 --> 0.390151).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 90.51811623573303
Epoch: 89, Steps: 91 | Train Loss: 0.1977626 Vali Loss: 0.3905222 Test Loss: 0.4842914
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 116.19668650627136
Epoch: 90, Steps: 91 | Train Loss: 0.1977719 Vali Loss: 0.3905035 Test Loss: 0.4842498
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 103.64870429039001
Epoch: 91, Steps: 91 | Train Loss: 0.1977527 Vali Loss: 0.3902618 Test Loss: 0.4842060
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 106.93480563163757
Epoch: 92, Steps: 91 | Train Loss: 0.1976955 Vali Loss: 0.3906465 Test Loss: 0.4841749
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 84.48228216171265
Epoch: 93, Steps: 91 | Train Loss: 0.1976525 Vali Loss: 0.3904252 Test Loss: 0.4841415
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 85.91467237472534
Epoch: 94, Steps: 91 | Train Loss: 0.1976242 Vali Loss: 0.3905582 Test Loss: 0.4841064
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 87.3180410861969
Epoch: 95, Steps: 91 | Train Loss: 0.1976387 Vali Loss: 0.3904905 Test Loss: 0.4840709
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 87.78512811660767
Epoch: 96, Steps: 91 | Train Loss: 0.1975868 Vali Loss: 0.3904531 Test Loss: 0.4840403
EarlyStopping counter: 8 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 93.32475185394287
Epoch: 97, Steps: 91 | Train Loss: 0.1976425 Vali Loss: 0.3904621 Test Loss: 0.4840126
EarlyStopping counter: 9 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 84.90372157096863
Epoch: 98, Steps: 91 | Train Loss: 0.1975432 Vali Loss: 0.3904302 Test Loss: 0.4839855
EarlyStopping counter: 10 out of 10
Early stopping
train 11765
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=74, out_features=212, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1730951168.0
params:  15900.0
Trainable parameters:  15900
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 58.960973262786865
Epoch: 1, Steps: 91 | Train Loss: 0.2923778 Vali Loss: 0.3864774 Test Loss: 0.4790031
Validation loss decreased (inf --> 0.386477).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 58.79577374458313
Epoch: 2, Steps: 91 | Train Loss: 0.2905403 Vali Loss: 0.3858019 Test Loss: 0.4792464
Validation loss decreased (0.386477 --> 0.385802).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 60.0077645778656
Epoch: 3, Steps: 91 | Train Loss: 0.2904278 Vali Loss: 0.3852628 Test Loss: 0.4789149
Validation loss decreased (0.385802 --> 0.385263).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 61.12600874900818
Epoch: 4, Steps: 91 | Train Loss: 0.2902965 Vali Loss: 0.3857460 Test Loss: 0.4788724
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 58.258052825927734
Epoch: 5, Steps: 91 | Train Loss: 0.2901448 Vali Loss: 0.3856381 Test Loss: 0.4792641
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 60.62668228149414
Epoch: 6, Steps: 91 | Train Loss: 0.2902678 Vali Loss: 0.3854668 Test Loss: 0.4790674
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 60.087029695510864
Epoch: 7, Steps: 91 | Train Loss: 0.2903146 Vali Loss: 0.3858072 Test Loss: 0.4788288
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 59.72034001350403
Epoch: 8, Steps: 91 | Train Loss: 0.2900941 Vali Loss: 0.3855864 Test Loss: 0.4789929
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 61.550936460494995
Epoch: 9, Steps: 91 | Train Loss: 0.2901491 Vali Loss: 0.3855014 Test Loss: 0.4789385
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 61.38240599632263
Epoch: 10, Steps: 91 | Train Loss: 0.2901132 Vali Loss: 0.3856987 Test Loss: 0.4789371
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 56.405938148498535
Epoch: 11, Steps: 91 | Train Loss: 0.2901130 Vali Loss: 0.3857182 Test Loss: 0.4787188
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 52.21401619911194
Epoch: 12, Steps: 91 | Train Loss: 0.2901048 Vali Loss: 0.3852422 Test Loss: 0.4786796
Validation loss decreased (0.385263 --> 0.385242).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 58.288026332855225
Epoch: 13, Steps: 91 | Train Loss: 0.2900818 Vali Loss: 0.3853527 Test Loss: 0.4787940
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 56.92137122154236
Epoch: 14, Steps: 91 | Train Loss: 0.2901865 Vali Loss: 0.3859318 Test Loss: 0.4786928
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 52.234769344329834
Epoch: 15, Steps: 91 | Train Loss: 0.2900557 Vali Loss: 0.3854158 Test Loss: 0.4786031
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 57.88011121749878
Epoch: 16, Steps: 91 | Train Loss: 0.2901583 Vali Loss: 0.3854704 Test Loss: 0.4787331
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 63.88801383972168
Epoch: 17, Steps: 91 | Train Loss: 0.2900375 Vali Loss: 0.3852785 Test Loss: 0.4787298
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 59.94241714477539
Epoch: 18, Steps: 91 | Train Loss: 0.2900568 Vali Loss: 0.3852226 Test Loss: 0.4785795
Validation loss decreased (0.385242 --> 0.385223).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 54.31286549568176
Epoch: 19, Steps: 91 | Train Loss: 0.2899766 Vali Loss: 0.3854897 Test Loss: 0.4785459
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 52.439385652542114
Epoch: 20, Steps: 91 | Train Loss: 0.2900466 Vali Loss: 0.3853281 Test Loss: 0.4784879
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 54.88113284111023
Epoch: 21, Steps: 91 | Train Loss: 0.2899894 Vali Loss: 0.3854103 Test Loss: 0.4787219
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 59.59435677528381
Epoch: 22, Steps: 91 | Train Loss: 0.2900906 Vali Loss: 0.3856364 Test Loss: 0.4787031
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 53.37000107765198
Epoch: 23, Steps: 91 | Train Loss: 0.2899822 Vali Loss: 0.3853783 Test Loss: 0.4787450
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 52.84112739562988
Epoch: 24, Steps: 91 | Train Loss: 0.2900258 Vali Loss: 0.3851999 Test Loss: 0.4786039
Validation loss decreased (0.385223 --> 0.385200).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 69.33551502227783
Epoch: 25, Steps: 91 | Train Loss: 0.2900252 Vali Loss: 0.3851693 Test Loss: 0.4785126
Validation loss decreased (0.385200 --> 0.385169).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 72.3419828414917
Epoch: 26, Steps: 91 | Train Loss: 0.2899464 Vali Loss: 0.3849027 Test Loss: 0.4786300
Validation loss decreased (0.385169 --> 0.384903).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 70.89315152168274
Epoch: 27, Steps: 91 | Train Loss: 0.2899356 Vali Loss: 0.3853367 Test Loss: 0.4786584
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 70.70001578330994
Epoch: 28, Steps: 91 | Train Loss: 0.2897864 Vali Loss: 0.3852535 Test Loss: 0.4785282
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 71.8950834274292
Epoch: 29, Steps: 91 | Train Loss: 0.2899072 Vali Loss: 0.3851360 Test Loss: 0.4784708
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 72.25440073013306
Epoch: 30, Steps: 91 | Train Loss: 0.2900419 Vali Loss: 0.3852750 Test Loss: 0.4784913
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 71.79394507408142
Epoch: 31, Steps: 91 | Train Loss: 0.2898949 Vali Loss: 0.3852725 Test Loss: 0.4784871
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 69.61561894416809
Epoch: 32, Steps: 91 | Train Loss: 0.2899426 Vali Loss: 0.3852106 Test Loss: 0.4785521
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 69.62100863456726
Epoch: 33, Steps: 91 | Train Loss: 0.2899907 Vali Loss: 0.3851092 Test Loss: 0.4785348
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 56.00169348716736
Epoch: 34, Steps: 91 | Train Loss: 0.2899176 Vali Loss: 0.3852590 Test Loss: 0.4784972
EarlyStopping counter: 8 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 56.92179203033447
Epoch: 35, Steps: 91 | Train Loss: 0.2899343 Vali Loss: 0.3851199 Test Loss: 0.4785740
EarlyStopping counter: 9 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 55.18232011795044
Epoch: 36, Steps: 91 | Train Loss: 0.2899224 Vali Loss: 0.3854900 Test Loss: 0.4785085
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j336_H8_FITS_custom_ftM_sl180_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.476672500371933, mae:0.3084878623485565, rse:0.5674260854721069, corr:[0.2679244  0.2826713  0.2826239  0.28207317 0.28250787 0.2819481
 0.28216583 0.28151113 0.28143832 0.28144613 0.28131026 0.28127044
 0.28143144 0.28130314 0.28128058 0.28097662 0.28094402 0.280598
 0.28059378 0.28055108 0.28069004 0.28097963 0.2818979  0.28205988
 0.28170943 0.28135684 0.28106087 0.2811709  0.28071278 0.28062856
 0.2808579  0.28107563 0.28091002 0.2807089  0.28096533 0.28109306
 0.28083017 0.28056565 0.28065816 0.28083146 0.28107852 0.2809254
 0.28102908 0.28118727 0.28101975 0.2807237  0.28091013 0.2810273
 0.280723   0.28029224 0.28013727 0.2805094  0.28066465 0.2810617
 0.281049   0.2808466  0.28112668 0.2811615  0.28120178 0.2812656
 0.2809752  0.28042212 0.28040832 0.28025958 0.28036124 0.2805344
 0.28036812 0.28016797 0.2802725  0.28043675 0.28058356 0.28066865
 0.2800107  0.27963015 0.27935302 0.27931687 0.27956268 0.2795219
 0.27959573 0.27981034 0.2799972  0.279976   0.2803273  0.28027457
 0.28006983 0.27995008 0.2797695  0.2796741  0.27958044 0.27957833
 0.27956057 0.2797009  0.27978823 0.27997395 0.280194   0.28048587
 0.2803863  0.2797742  0.2793084  0.27947482 0.27980563 0.27989775
 0.28007364 0.2797296  0.27958745 0.27977192 0.2797524  0.27947125
 0.2794817  0.2797235  0.27970427 0.2797157  0.27962515 0.27980688
 0.27972835 0.27943105 0.2794127  0.27958146 0.2797167  0.2797799
 0.2796998  0.2792207  0.27917066 0.27915716 0.2788985  0.2788718
 0.2792296  0.2792652  0.2792418  0.27963307 0.27985254 0.27968147
 0.27988386 0.28022724 0.2799252  0.27977577 0.27945417 0.27953807
 0.27955887 0.27959365 0.27992854 0.28017014 0.2807438  0.28080568
 0.28062907 0.28049752 0.28038788 0.28033867 0.28056267 0.2803875
 0.28006348 0.27980286 0.27966368 0.28001282 0.2809031  0.28183725
 0.28268465 0.28332353 0.2829296  0.28258923 0.2820784  0.28187063
 0.28158778 0.28146014 0.2822097  0.28304225 0.28465208 0.2847533
 0.28343162 0.28237677 0.28226006 0.28206754 0.28196535 0.28215647
 0.2821546  0.28184506 0.2816428  0.28184354 0.28210634 0.2822763
 0.28214413 0.2820807  0.28180307 0.28162807 0.2813583  0.28136846
 0.28136238 0.28107002 0.2811235  0.28136948 0.28221422 0.28224003
 0.28172138 0.28118134 0.28106818 0.2811974  0.2812065  0.2811434
 0.28117695 0.28136107 0.28112942 0.28110445 0.2811047  0.28085843
 0.2806532  0.2806996  0.28067312 0.28070652 0.28068423 0.2803501
 0.280132   0.2800679  0.28022215 0.27999628 0.28008536 0.2803236
 0.28012568 0.27980515 0.2796158  0.27969858 0.28011158 0.2805776
 0.28023505 0.27991948 0.2803739  0.28075644 0.28072113 0.28038698
 0.2803204  0.28034103 0.28013283 0.2799367  0.28010026 0.27993044
 0.2793872  0.27958462 0.28024644 0.28021675 0.28009382 0.28027317
 0.27996838 0.27944967 0.2794506  0.2798735  0.27982357 0.2795075
 0.2794636  0.27962512 0.27993962 0.28019223 0.28025433 0.27988532
 0.27967787 0.27939594 0.27907053 0.27900717 0.27907717 0.2791063
 0.27891758 0.27887702 0.2788065  0.27901313 0.27934548 0.27947214
 0.27915362 0.27896565 0.2792656  0.27957126 0.2795095  0.2791517
 0.27929208 0.27948052 0.27938318 0.27897787 0.27883548 0.2787767
 0.27864882 0.27878782 0.27889386 0.27884853 0.2785167  0.27855223
 0.27857998 0.27855158 0.2786902  0.27861035 0.27848092 0.27876425
 0.27892002 0.2786308  0.27879027 0.27895626 0.27879584 0.2784834
 0.27856654 0.27862173 0.27856869 0.27871335 0.27883697 0.2789033
 0.27923143 0.27964368 0.27954784 0.27944422 0.2790778  0.27906385
 0.27894658 0.27887928 0.27905342 0.27927494 0.27983025 0.27962768
 0.27932104 0.27953133 0.27988026 0.27995735 0.2798049  0.27940154
 0.27952942 0.27936968 0.27918628 0.27959454 0.27978393 0.28018656
 0.28174666 0.28237116 0.28173342 0.28177238 0.28095156 0.28107077
 0.28100082 0.28095055 0.28120962 0.28137568 0.2817621  0.28080755]
