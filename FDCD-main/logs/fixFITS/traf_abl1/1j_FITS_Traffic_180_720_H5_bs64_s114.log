Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j720_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j720_H5_FITS_custom_ftM_sl180_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11381
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=50, out_features=250, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1379200000.0
params:  12750.0
Trainable parameters:  12750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 108.98159527778625
Epoch: 1, Steps: 88 | Train Loss: 1.5825266 Vali Loss: 1.2921307 Test Loss: 1.6002731
Validation loss decreased (inf --> 1.292131).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 110.57193851470947
Epoch: 2, Steps: 88 | Train Loss: 0.8445258 Vali Loss: 0.8524197 Test Loss: 1.0473987
Validation loss decreased (1.292131 --> 0.852420).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 110.13242197036743
Epoch: 3, Steps: 88 | Train Loss: 0.5906134 Vali Loss: 0.6673034 Test Loss: 0.8153391
Validation loss decreased (0.852420 --> 0.667303).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 109.77082514762878
Epoch: 4, Steps: 88 | Train Loss: 0.4783373 Vali Loss: 0.5814387 Test Loss: 0.7059072
Validation loss decreased (0.667303 --> 0.581439).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 109.80500221252441
Epoch: 5, Steps: 88 | Train Loss: 0.4225503 Vali Loss: 0.5359723 Test Loss: 0.6474681
Validation loss decreased (0.581439 --> 0.535972).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 110.67775821685791
Epoch: 6, Steps: 88 | Train Loss: 0.3915461 Vali Loss: 0.5086287 Test Loss: 0.6123041
Validation loss decreased (0.535972 --> 0.508629).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 111.93275713920593
Epoch: 7, Steps: 88 | Train Loss: 0.3721594 Vali Loss: 0.4913713 Test Loss: 0.5885619
Validation loss decreased (0.508629 --> 0.491371).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 108.43173861503601
Epoch: 8, Steps: 88 | Train Loss: 0.3590118 Vali Loss: 0.4788031 Test Loss: 0.5719901
Validation loss decreased (0.491371 --> 0.478803).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 110.25470876693726
Epoch: 9, Steps: 88 | Train Loss: 0.3496125 Vali Loss: 0.4701136 Test Loss: 0.5598422
Validation loss decreased (0.478803 --> 0.470114).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 108.91072416305542
Epoch: 10, Steps: 88 | Train Loss: 0.3426669 Vali Loss: 0.4636066 Test Loss: 0.5506869
Validation loss decreased (0.470114 --> 0.463607).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 103.88018822669983
Epoch: 11, Steps: 88 | Train Loss: 0.3373529 Vali Loss: 0.4583474 Test Loss: 0.5436180
Validation loss decreased (0.463607 --> 0.458347).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 103.6102888584137
Epoch: 12, Steps: 88 | Train Loss: 0.3332052 Vali Loss: 0.4535197 Test Loss: 0.5381016
Validation loss decreased (0.458347 --> 0.453520).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 101.57845854759216
Epoch: 13, Steps: 88 | Train Loss: 0.3300846 Vali Loss: 0.4507132 Test Loss: 0.5337848
Validation loss decreased (0.453520 --> 0.450713).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 103.15645909309387
Epoch: 14, Steps: 88 | Train Loss: 0.3274051 Vali Loss: 0.4482259 Test Loss: 0.5302836
Validation loss decreased (0.450713 --> 0.448226).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 102.78308510780334
Epoch: 15, Steps: 88 | Train Loss: 0.3253044 Vali Loss: 0.4459786 Test Loss: 0.5272501
Validation loss decreased (0.448226 --> 0.445979).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 104.43772482872009
Epoch: 16, Steps: 88 | Train Loss: 0.3236065 Vali Loss: 0.4440927 Test Loss: 0.5249546
Validation loss decreased (0.445979 --> 0.444093).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 102.44094777107239
Epoch: 17, Steps: 88 | Train Loss: 0.3221721 Vali Loss: 0.4431927 Test Loss: 0.5229394
Validation loss decreased (0.444093 --> 0.443193).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 83.88175678253174
Epoch: 18, Steps: 88 | Train Loss: 0.3208677 Vali Loss: 0.4422323 Test Loss: 0.5213431
Validation loss decreased (0.443193 --> 0.442232).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 83.30675554275513
Epoch: 19, Steps: 88 | Train Loss: 0.3199372 Vali Loss: 0.4409418 Test Loss: 0.5198439
Validation loss decreased (0.442232 --> 0.440942).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 81.40828847885132
Epoch: 20, Steps: 88 | Train Loss: 0.3189683 Vali Loss: 0.4397763 Test Loss: 0.5185719
Validation loss decreased (0.440942 --> 0.439776).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 80.51053953170776
Epoch: 21, Steps: 88 | Train Loss: 0.3183386 Vali Loss: 0.4389926 Test Loss: 0.5175925
Validation loss decreased (0.439776 --> 0.438993).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 82.7095718383789
Epoch: 22, Steps: 88 | Train Loss: 0.3176860 Vali Loss: 0.4383876 Test Loss: 0.5166705
Validation loss decreased (0.438993 --> 0.438388).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 81.39683032035828
Epoch: 23, Steps: 88 | Train Loss: 0.3171118 Vali Loss: 0.4381081 Test Loss: 0.5159812
Validation loss decreased (0.438388 --> 0.438108).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 81.47099924087524
Epoch: 24, Steps: 88 | Train Loss: 0.3166486 Vali Loss: 0.4375340 Test Loss: 0.5152776
Validation loss decreased (0.438108 --> 0.437534).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 66.40800404548645
Epoch: 25, Steps: 88 | Train Loss: 0.3162985 Vali Loss: 0.4372151 Test Loss: 0.5146526
Validation loss decreased (0.437534 --> 0.437215).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 64.72184658050537
Epoch: 26, Steps: 88 | Train Loss: 0.3157866 Vali Loss: 0.4366438 Test Loss: 0.5140887
Validation loss decreased (0.437215 --> 0.436644).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 62.93495273590088
Epoch: 27, Steps: 88 | Train Loss: 0.3155215 Vali Loss: 0.4365703 Test Loss: 0.5136289
Validation loss decreased (0.436644 --> 0.436570).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 63.810373306274414
Epoch: 28, Steps: 88 | Train Loss: 0.3151524 Vali Loss: 0.4359244 Test Loss: 0.5132329
Validation loss decreased (0.436570 --> 0.435924).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 64.43176174163818
Epoch: 29, Steps: 88 | Train Loss: 0.3148811 Vali Loss: 0.4355805 Test Loss: 0.5129135
Validation loss decreased (0.435924 --> 0.435580).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 64.14434504508972
Epoch: 30, Steps: 88 | Train Loss: 0.3147086 Vali Loss: 0.4354198 Test Loss: 0.5125762
Validation loss decreased (0.435580 --> 0.435420).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 64.10431146621704
Epoch: 31, Steps: 88 | Train Loss: 0.3144961 Vali Loss: 0.4350650 Test Loss: 0.5122514
Validation loss decreased (0.435420 --> 0.435065).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 62.96018981933594
Epoch: 32, Steps: 88 | Train Loss: 0.3142744 Vali Loss: 0.4351764 Test Loss: 0.5120068
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 63.22284698486328
Epoch: 33, Steps: 88 | Train Loss: 0.3142007 Vali Loss: 0.4356788 Test Loss: 0.5117463
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 63.45102334022522
Epoch: 34, Steps: 88 | Train Loss: 0.3140531 Vali Loss: 0.4348256 Test Loss: 0.5115670
Validation loss decreased (0.435065 --> 0.434826).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 62.47292709350586
Epoch: 35, Steps: 88 | Train Loss: 0.3138503 Vali Loss: 0.4343373 Test Loss: 0.5113471
Validation loss decreased (0.434826 --> 0.434337).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 64.0720374584198
Epoch: 36, Steps: 88 | Train Loss: 0.3138049 Vali Loss: 0.4348147 Test Loss: 0.5111867
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 62.66764307022095
Epoch: 37, Steps: 88 | Train Loss: 0.3135975 Vali Loss: 0.4344636 Test Loss: 0.5110850
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 63.288227796554565
Epoch: 38, Steps: 88 | Train Loss: 0.3136166 Vali Loss: 0.4343601 Test Loss: 0.5109801
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 54.325703620910645
Epoch: 39, Steps: 88 | Train Loss: 0.3135204 Vali Loss: 0.4342231 Test Loss: 0.5107999
Validation loss decreased (0.434337 --> 0.434223).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 50.30400109291077
Epoch: 40, Steps: 88 | Train Loss: 0.3133310 Vali Loss: 0.4345756 Test Loss: 0.5106856
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 50.447319984436035
Epoch: 41, Steps: 88 | Train Loss: 0.3133510 Vali Loss: 0.4343537 Test Loss: 0.5106231
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 50.756763219833374
Epoch: 42, Steps: 88 | Train Loss: 0.3133015 Vali Loss: 0.4341262 Test Loss: 0.5104396
Validation loss decreased (0.434223 --> 0.434126).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 50.09049034118652
Epoch: 43, Steps: 88 | Train Loss: 0.3131557 Vali Loss: 0.4338032 Test Loss: 0.5103379
Validation loss decreased (0.434126 --> 0.433803).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 50.09458065032959
Epoch: 44, Steps: 88 | Train Loss: 0.3131671 Vali Loss: 0.4341722 Test Loss: 0.5103066
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 50.44181966781616
Epoch: 45, Steps: 88 | Train Loss: 0.3129695 Vali Loss: 0.4341251 Test Loss: 0.5102943
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 50.669254302978516
Epoch: 46, Steps: 88 | Train Loss: 0.3130874 Vali Loss: 0.4340717 Test Loss: 0.5102295
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 49.740559577941895
Epoch: 47, Steps: 88 | Train Loss: 0.3130641 Vali Loss: 0.4337519 Test Loss: 0.5101437
Validation loss decreased (0.433803 --> 0.433752).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 51.13757276535034
Epoch: 48, Steps: 88 | Train Loss: 0.3130584 Vali Loss: 0.4341459 Test Loss: 0.5101011
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 51.130778312683105
Epoch: 49, Steps: 88 | Train Loss: 0.3128264 Vali Loss: 0.4340220 Test Loss: 0.5100871
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 49.547159910202026
Epoch: 50, Steps: 88 | Train Loss: 0.3128343 Vali Loss: 0.4339322 Test Loss: 0.5100442
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 50.56471347808838
Epoch: 51, Steps: 88 | Train Loss: 0.3129519 Vali Loss: 0.4342626 Test Loss: 0.5099973
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 50.06303930282593
Epoch: 52, Steps: 88 | Train Loss: 0.3128033 Vali Loss: 0.4341519 Test Loss: 0.5099290
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 50.2501060962677
Epoch: 53, Steps: 88 | Train Loss: 0.3127659 Vali Loss: 0.4335877 Test Loss: 0.5099248
Validation loss decreased (0.433752 --> 0.433588).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 50.44706678390503
Epoch: 54, Steps: 88 | Train Loss: 0.3128042 Vali Loss: 0.4338013 Test Loss: 0.5098984
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 50.112555503845215
Epoch: 55, Steps: 88 | Train Loss: 0.3127703 Vali Loss: 0.4339429 Test Loss: 0.5098362
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 48.66825556755066
Epoch: 56, Steps: 88 | Train Loss: 0.3126498 Vali Loss: 0.4341366 Test Loss: 0.5098410
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 51.35046339035034
Epoch: 57, Steps: 88 | Train Loss: 0.3128019 Vali Loss: 0.4334867 Test Loss: 0.5098099
Validation loss decreased (0.433588 --> 0.433487).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 50.608495473861694
Epoch: 58, Steps: 88 | Train Loss: 0.3127329 Vali Loss: 0.4340858 Test Loss: 0.5098150
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 49.85439443588257
Epoch: 59, Steps: 88 | Train Loss: 0.3127371 Vali Loss: 0.4334897 Test Loss: 0.5097610
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 51.134984254837036
Epoch: 60, Steps: 88 | Train Loss: 0.3127532 Vali Loss: 0.4338825 Test Loss: 0.5097387
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 49.9617874622345
Epoch: 61, Steps: 88 | Train Loss: 0.3126893 Vali Loss: 0.4336326 Test Loss: 0.5097218
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 48.867148876190186
Epoch: 62, Steps: 88 | Train Loss: 0.3126829 Vali Loss: 0.4338787 Test Loss: 0.5097306
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 47.978769063949585
Epoch: 63, Steps: 88 | Train Loss: 0.3126665 Vali Loss: 0.4339433 Test Loss: 0.5096807
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 48.62264323234558
Epoch: 64, Steps: 88 | Train Loss: 0.3126618 Vali Loss: 0.4335561 Test Loss: 0.5096868
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 49.01644206047058
Epoch: 65, Steps: 88 | Train Loss: 0.3126911 Vali Loss: 0.4339032 Test Loss: 0.5096751
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 48.83246302604675
Epoch: 66, Steps: 88 | Train Loss: 0.3125681 Vali Loss: 0.4337127 Test Loss: 0.5096535
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 50.1930570602417
Epoch: 67, Steps: 88 | Train Loss: 0.3126107 Vali Loss: 0.4338081 Test Loss: 0.5096443
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j720_H5_FITS_custom_ftM_sl180_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.5077415704727173, mae:0.330452561378479, rse:0.5826693177223206, corr:[0.26464126 0.26313677 0.26527792 0.2662184  0.26608676 0.26699257
 0.26810098 0.26852468 0.2684288  0.26827246 0.26803818 0.2673733
 0.26667732 0.2665473  0.26613888 0.26561186 0.2655309  0.26568663
 0.2658245  0.265931   0.2663717  0.2674751  0.26817766 0.26780522
 0.26759684 0.26728323 0.26714045 0.26680747 0.26669866 0.26714036
 0.267302   0.2670179  0.26691076 0.26689702 0.26653048 0.26589075
 0.26551455 0.26557228 0.2656187  0.26556996 0.2655596  0.2657697
 0.266045   0.26606867 0.26605603 0.26642746 0.26676002 0.26656023
 0.26620826 0.2660441  0.26625952 0.26622757 0.2658284  0.2656763
 0.26588452 0.26603964 0.26594138 0.26577467 0.26566157 0.26543784
 0.2651013  0.2649598  0.26514733 0.2653338  0.26532128 0.26531276
 0.26534775 0.2653331  0.26537678 0.2655963  0.26569512 0.2655768
 0.26529223 0.26498133 0.26482582 0.26477215 0.26475596 0.26471585
 0.26460308 0.26458254 0.2647512  0.26490712 0.2648575  0.26477945
 0.26472405 0.26464257 0.26462302 0.26466593 0.2646313  0.26448888
 0.26463738 0.26474154 0.264708   0.26472571 0.26475987 0.2647826
 0.26472273 0.26461014 0.26456293 0.26444104 0.2641891  0.26401582
 0.26402608 0.26405504 0.2640019  0.26399675 0.26400045 0.26405212
 0.26410925 0.26411855 0.26399222 0.26399332 0.26414183 0.26409838
 0.26404053 0.26398546 0.26399544 0.26412684 0.26432496 0.2644005
 0.2645479  0.26465276 0.26467136 0.26456875 0.26444915 0.2643404
 0.26414263 0.2639662  0.26395246 0.2640047  0.2641064  0.26454028
 0.26502305 0.2653811  0.26515612 0.26479942 0.2647046  0.2643463
 0.26448917 0.2646555  0.26477572 0.26526102 0.2657896  0.26567498
 0.2654174  0.26526633 0.265391   0.26533952 0.26508242 0.2650867
 0.26501423 0.26475433 0.26467603 0.26475108 0.26509792 0.26627663
 0.26755363 0.2683209  0.26784304 0.26723757 0.26692575 0.26660454
 0.26680285 0.26730782 0.26779246 0.2689551  0.27008182 0.26972333
 0.26881036 0.2676277  0.26715434 0.2670007  0.26692453 0.26711282
 0.26711702 0.26707566 0.26738554 0.26760307 0.26759678 0.2677337
 0.2679771  0.26818675 0.26782608 0.26744276 0.26730067 0.26727661
 0.26732665 0.2672931  0.26736858 0.26798716 0.2684769  0.26805064
 0.26742908 0.26694542 0.26689637 0.2668085  0.2666152  0.2667717
 0.26704574 0.2671596  0.26729098 0.2674851  0.26758823 0.26754555
 0.2673615  0.26719338 0.2670191  0.2669305  0.2668274  0.2667021
 0.26665795 0.26663992 0.2666925  0.26693335 0.26707268 0.26691532
 0.26667556 0.26649868 0.2666195  0.26670805 0.26656595 0.26639432
 0.26636592 0.26652262 0.2667836  0.2669446  0.26693067 0.26685917
 0.2668301  0.2668181  0.2667924  0.26669475 0.26652977 0.26638463
 0.26627627 0.2661564  0.2661922  0.26636502 0.26658043 0.26664117
 0.26650932 0.26628157 0.2661637  0.26608086 0.26598355 0.26597017
 0.2660472  0.26620197 0.2664369  0.26670045 0.26682714 0.26683852
 0.26678628 0.26663232 0.26638022 0.26620245 0.26610425 0.26598912
 0.26588845 0.26582837 0.26585135 0.2659352  0.2660056  0.26609978
 0.26622322 0.26624677 0.2661579  0.26603302 0.26585957 0.26572496
 0.2656459  0.26570958 0.26583058 0.26594687 0.2659147  0.26584393
 0.26578712 0.26579702 0.26565817 0.26546484 0.26542017 0.26542643
 0.2654886  0.2653494  0.2651151  0.2651303  0.26542437 0.26564547
 0.26578012 0.26584944 0.26586744 0.26566398 0.26540643 0.26531264
 0.26520756 0.26510742 0.26521033 0.26541004 0.2655815  0.26587853
 0.26620114 0.26655254 0.26649016 0.26622564 0.26604578 0.265748
 0.26584202 0.2660001  0.26614678 0.2665247  0.2669581  0.26691732
 0.26683295 0.26667634 0.26670805 0.26671255 0.26647508 0.26629692
 0.26611748 0.2659897  0.26609883 0.26621306 0.26651284 0.26750666
 0.26865155 0.26941168 0.26890892 0.2682536  0.2679707  0.2677151
 0.2678758  0.26828116 0.26873893 0.26988885 0.2708691  0.27026019
 0.2691358  0.2681322  0.26786184 0.26773325 0.26754063 0.26765937
 0.26773062 0.26773316 0.26800337 0.26828736 0.26843253 0.2685016
 0.26846844 0.26856598 0.26834163 0.2680603  0.26786733 0.26772258
 0.26772875 0.2678137  0.2680111  0.26853684 0.2688376  0.2684377
 0.26785466 0.26719677 0.26701495 0.26704398 0.26691863 0.26695573
 0.26717383 0.26741    0.26767164 0.26785204 0.2678988  0.26791403
 0.26778015 0.26758027 0.26740244 0.26737908 0.26733288 0.26725006
 0.26720113 0.26709208 0.26706356 0.2672736  0.26735714 0.2672042
 0.26714483 0.26710367 0.26710728 0.26703513 0.26690474 0.26688623
 0.26699337 0.26716048 0.2673605  0.26754475 0.2675931  0.26752427
 0.26749167 0.26748723 0.2673523  0.26712385 0.26689377 0.2666599
 0.26645195 0.26636782 0.26641864 0.26663065 0.266863   0.26696554
 0.26687363 0.26672655 0.26664966 0.26656353 0.2664366  0.26638457
 0.26644275 0.26659808 0.26680487 0.2669489  0.26698035 0.26696882
 0.2669371  0.26690972 0.26680252 0.2665878  0.26637885 0.2663064
 0.26630703 0.2661517  0.26603535 0.2661825  0.26633692 0.2664821
 0.26657614 0.2665874  0.26652983 0.26637706 0.2662158  0.2661614
 0.2661578  0.26613316 0.26613548 0.26621005 0.26628205 0.26632187
 0.26630592 0.26631278 0.2661978  0.2659936  0.26581475 0.26558623
 0.2655303  0.26550785 0.26540253 0.265316   0.26546308 0.26578945
 0.26614782 0.26627    0.26622644 0.26603913 0.2657893  0.2656312
 0.2655665  0.26558474 0.26562694 0.26560637 0.26577082 0.26625475
 0.2667063  0.26703212 0.26669744 0.2661771  0.26599306 0.26595291
 0.26597062 0.26599404 0.2661098  0.26650378 0.2668886  0.26684526
 0.2668855  0.2669237  0.2669115  0.26670307 0.26642275 0.26637173
 0.26626197 0.26607713 0.2661088  0.26617408 0.26648173 0.26745698
 0.26864067 0.26954466 0.26901177 0.26809457 0.26762125 0.2673319
 0.2674535  0.26789075 0.26834634 0.26926103 0.27008548 0.26971734
 0.26889944 0.26792777 0.26760244 0.26748356 0.26731107 0.26748237
 0.26766    0.26773703 0.26793948 0.26806042 0.26811087 0.26823965
 0.26833317 0.2683934  0.2679905  0.26755282 0.26730508 0.2671396
 0.26703396 0.2669537  0.2670755  0.26758942 0.26792395 0.26772144
 0.26756144 0.2672852  0.2671507  0.2671405  0.26716185 0.26743957
 0.2677525  0.2678574  0.26793152 0.26806605 0.2681144  0.26803154
 0.2677675  0.26741627 0.267023   0.26690945 0.2669315  0.26679924
 0.26666245 0.26660258 0.26656654 0.2666855  0.26677912 0.2667033
 0.26672402 0.26689813 0.2671388  0.26712513 0.26693743 0.26688647
 0.26704225 0.26731288 0.26750252 0.26748568 0.26732376 0.26709577
 0.26682872 0.26664773 0.2664974  0.26621047 0.26595548 0.26588657
 0.26580575 0.26568916 0.2657592  0.26600817 0.2661521  0.26615164
 0.26607072 0.26598465 0.26603234 0.26604643 0.26597083 0.26596862
 0.265944   0.2657837  0.26569685 0.2657841  0.26583597 0.26568726
 0.26537153 0.26508656 0.26503766 0.26502132 0.2647287  0.26443222
 0.26446864 0.26460722 0.26457956 0.2645602  0.2647046  0.26499128
 0.2651887  0.2651256  0.265037   0.26504946 0.26496676 0.26478058
 0.26472434 0.26477548 0.26473993 0.26464298 0.26454386 0.26440495
 0.26423365 0.26418322 0.26411122 0.26407725 0.26413357 0.26401404
 0.2637348  0.26358977 0.26370975 0.26395136 0.2641896  0.26434797
 0.2645711  0.2647114  0.2646951  0.26460668 0.2645827  0.26450855
 0.26421717 0.2640003  0.2640155  0.26394123 0.26380238 0.26391786
 0.26421508 0.2646209  0.26454958 0.26414946 0.26393196 0.2639534
 0.26403064 0.26403826 0.2641675  0.26478365 0.26550522 0.26563066
 0.26556477 0.26533514 0.26522845 0.26503316 0.26479104 0.26491615
 0.26491824 0.2645463  0.26429728 0.2642611  0.26444453 0.26500225
 0.26581016 0.26668343 0.26653793 0.26585025 0.26528528 0.26496947
 0.2651495  0.26554915 0.26604724 0.2671686  0.26809445 0.2678394
 0.2673722  0.26650032 0.26591158 0.26565647 0.26554334 0.26585117
 0.26612872 0.26608083 0.26599932 0.26600373 0.266151   0.26635626
 0.26645255 0.26670477 0.26675177 0.2667527  0.26658982 0.2663479
 0.2664152  0.26655468 0.26670238 0.2672368  0.26759624 0.26729012
 0.26710448 0.26705265 0.26690078 0.26653194 0.2663646  0.26671717
 0.26714277 0.2671401  0.2668398  0.26670462 0.26676422 0.26672268
 0.26652735 0.26651278 0.26657864 0.26681173 0.2670933  0.26657054
 0.26587364 0.2659699  0.26571104 0.26438296 0.2643797  0.26840508]
