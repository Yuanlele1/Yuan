Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j192_H5_FITS_custom_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3804936960.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 139.33152437210083
Epoch: 1, Steps: 88 | Train Loss: 1.0941230 Vali Loss: 1.1615063 Test Loss: 1.3376925
Validation loss decreased (inf --> 1.161506).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 128.63375878334045
Epoch: 2, Steps: 88 | Train Loss: 0.8299339 Vali Loss: 1.0184569 Test Loss: 1.1693546
Validation loss decreased (1.161506 --> 1.018457).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 122.5847373008728
Epoch: 3, Steps: 88 | Train Loss: 0.7228389 Vali Loss: 0.9469885 Test Loss: 1.0869209
Validation loss decreased (1.018457 --> 0.946989).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 122.25714755058289
Epoch: 4, Steps: 88 | Train Loss: 0.6523189 Vali Loss: 0.8966615 Test Loss: 1.0285939
Validation loss decreased (0.946989 --> 0.896661).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 124.4191951751709
Epoch: 5, Steps: 88 | Train Loss: 0.5961937 Vali Loss: 0.8531993 Test Loss: 0.9799102
Validation loss decreased (0.896661 --> 0.853199).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 124.03689789772034
Epoch: 6, Steps: 88 | Train Loss: 0.5490794 Vali Loss: 0.8168195 Test Loss: 0.9383513
Validation loss decreased (0.853199 --> 0.816819).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 124.70296478271484
Epoch: 7, Steps: 88 | Train Loss: 0.5086244 Vali Loss: 0.7783626 Test Loss: 0.8947350
Validation loss decreased (0.816819 --> 0.778363).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 124.39329099655151
Epoch: 8, Steps: 88 | Train Loss: 0.4734746 Vali Loss: 0.7476101 Test Loss: 0.8588685
Validation loss decreased (0.778363 --> 0.747610).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 127.65103507041931
Epoch: 9, Steps: 88 | Train Loss: 0.4425923 Vali Loss: 0.7218261 Test Loss: 0.8309244
Validation loss decreased (0.747610 --> 0.721826).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 125.05958199501038
Epoch: 10, Steps: 88 | Train Loss: 0.4152791 Vali Loss: 0.6945994 Test Loss: 0.7992996
Validation loss decreased (0.721826 --> 0.694599).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 120.49424529075623
Epoch: 11, Steps: 88 | Train Loss: 0.3909517 Vali Loss: 0.6700193 Test Loss: 0.7708813
Validation loss decreased (0.694599 --> 0.670019).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 116.78774118423462
Epoch: 12, Steps: 88 | Train Loss: 0.3692355 Vali Loss: 0.6492231 Test Loss: 0.7471839
Validation loss decreased (0.670019 --> 0.649223).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 114.43709325790405
Epoch: 13, Steps: 88 | Train Loss: 0.3497688 Vali Loss: 0.6308222 Test Loss: 0.7268700
Validation loss decreased (0.649223 --> 0.630822).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 119.62891411781311
Epoch: 14, Steps: 88 | Train Loss: 0.3322185 Vali Loss: 0.6138872 Test Loss: 0.7078493
Validation loss decreased (0.630822 --> 0.613887).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 114.38053750991821
Epoch: 15, Steps: 88 | Train Loss: 0.3163622 Vali Loss: 0.5984154 Test Loss: 0.6896769
Validation loss decreased (0.613887 --> 0.598415).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 118.2278823852539
Epoch: 16, Steps: 88 | Train Loss: 0.3020373 Vali Loss: 0.5823988 Test Loss: 0.6711619
Validation loss decreased (0.598415 --> 0.582399).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 124.30257892608643
Epoch: 17, Steps: 88 | Train Loss: 0.2889597 Vali Loss: 0.5694180 Test Loss: 0.6571528
Validation loss decreased (0.582399 --> 0.569418).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 112.1442220211029
Epoch: 18, Steps: 88 | Train Loss: 0.2771057 Vali Loss: 0.5573534 Test Loss: 0.6434571
Validation loss decreased (0.569418 --> 0.557353).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 111.9980583190918
Epoch: 19, Steps: 88 | Train Loss: 0.2662752 Vali Loss: 0.5449229 Test Loss: 0.6292662
Validation loss decreased (0.557353 --> 0.544923).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 114.57791781425476
Epoch: 20, Steps: 88 | Train Loss: 0.2563977 Vali Loss: 0.5345418 Test Loss: 0.6177558
Validation loss decreased (0.544923 --> 0.534542).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 106.88913202285767
Epoch: 21, Steps: 88 | Train Loss: 0.2473143 Vali Loss: 0.5249778 Test Loss: 0.6068070
Validation loss decreased (0.534542 --> 0.524978).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 112.49055457115173
Epoch: 22, Steps: 88 | Train Loss: 0.2390044 Vali Loss: 0.5149688 Test Loss: 0.5959235
Validation loss decreased (0.524978 --> 0.514969).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 112.51982712745667
Epoch: 23, Steps: 88 | Train Loss: 0.2313786 Vali Loss: 0.5066311 Test Loss: 0.5867332
Validation loss decreased (0.514969 --> 0.506631).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 119.31327700614929
Epoch: 24, Steps: 88 | Train Loss: 0.2243256 Vali Loss: 0.4993761 Test Loss: 0.5778231
Validation loss decreased (0.506631 --> 0.499376).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 114.10274863243103
Epoch: 25, Steps: 88 | Train Loss: 0.2178569 Vali Loss: 0.4938776 Test Loss: 0.5717350
Validation loss decreased (0.499376 --> 0.493878).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 117.09865546226501
Epoch: 26, Steps: 88 | Train Loss: 0.2118488 Vali Loss: 0.4862336 Test Loss: 0.5636339
Validation loss decreased (0.493878 --> 0.486234).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 105.21028280258179
Epoch: 27, Steps: 88 | Train Loss: 0.2062874 Vali Loss: 0.4800652 Test Loss: 0.5568319
Validation loss decreased (0.486234 --> 0.480065).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 106.73644351959229
Epoch: 28, Steps: 88 | Train Loss: 0.2011274 Vali Loss: 0.4742670 Test Loss: 0.5503778
Validation loss decreased (0.480065 --> 0.474267).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 114.58472037315369
Epoch: 29, Steps: 88 | Train Loss: 0.1963710 Vali Loss: 0.4694609 Test Loss: 0.5447987
Validation loss decreased (0.474267 --> 0.469461).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 110.58533477783203
Epoch: 30, Steps: 88 | Train Loss: 0.1919339 Vali Loss: 0.4637239 Test Loss: 0.5387692
Validation loss decreased (0.469461 --> 0.463724).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 108.09605431556702
Epoch: 31, Steps: 88 | Train Loss: 0.1878084 Vali Loss: 0.4590826 Test Loss: 0.5336016
Validation loss decreased (0.463724 --> 0.459083).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 113.24882078170776
Epoch: 32, Steps: 88 | Train Loss: 0.1839801 Vali Loss: 0.4551221 Test Loss: 0.5294551
Validation loss decreased (0.459083 --> 0.455122).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 118.90834498405457
Epoch: 33, Steps: 88 | Train Loss: 0.1803765 Vali Loss: 0.4504108 Test Loss: 0.5245662
Validation loss decreased (0.455122 --> 0.450411).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 115.76998448371887
Epoch: 34, Steps: 88 | Train Loss: 0.1770775 Vali Loss: 0.4468524 Test Loss: 0.5205446
Validation loss decreased (0.450411 --> 0.446852).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 117.9329161643982
Epoch: 35, Steps: 88 | Train Loss: 0.1739380 Vali Loss: 0.4436239 Test Loss: 0.5169072
Validation loss decreased (0.446852 --> 0.443624).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 125.6497266292572
Epoch: 36, Steps: 88 | Train Loss: 0.1710055 Vali Loss: 0.4403260 Test Loss: 0.5130566
Validation loss decreased (0.443624 --> 0.440326).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 117.32861304283142
Epoch: 37, Steps: 88 | Train Loss: 0.1682609 Vali Loss: 0.4370510 Test Loss: 0.5097237
Validation loss decreased (0.440326 --> 0.437051).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 118.43382573127747
Epoch: 38, Steps: 88 | Train Loss: 0.1657185 Vali Loss: 0.4343953 Test Loss: 0.5065517
Validation loss decreased (0.437051 --> 0.434395).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 122.83836531639099
Epoch: 39, Steps: 88 | Train Loss: 0.1633195 Vali Loss: 0.4316381 Test Loss: 0.5036306
Validation loss decreased (0.434395 --> 0.431638).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 122.03222942352295
Epoch: 40, Steps: 88 | Train Loss: 0.1610831 Vali Loss: 0.4284327 Test Loss: 0.5004947
Validation loss decreased (0.431638 --> 0.428433).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 119.17623901367188
Epoch: 41, Steps: 88 | Train Loss: 0.1590029 Vali Loss: 0.4269228 Test Loss: 0.4984699
Validation loss decreased (0.428433 --> 0.426923).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 124.78713250160217
Epoch: 42, Steps: 88 | Train Loss: 0.1569835 Vali Loss: 0.4242916 Test Loss: 0.4956005
Validation loss decreased (0.426923 --> 0.424292).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 122.12992668151855
Epoch: 43, Steps: 88 | Train Loss: 0.1550966 Vali Loss: 0.4218661 Test Loss: 0.4930989
Validation loss decreased (0.424292 --> 0.421866).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 117.24825882911682
Epoch: 44, Steps: 88 | Train Loss: 0.1533732 Vali Loss: 0.4200474 Test Loss: 0.4911655
Validation loss decreased (0.421866 --> 0.420047).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 116.36044907569885
Epoch: 45, Steps: 88 | Train Loss: 0.1516863 Vali Loss: 0.4180142 Test Loss: 0.4889400
Validation loss decreased (0.420047 --> 0.418014).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 123.30170774459839
Epoch: 46, Steps: 88 | Train Loss: 0.1501600 Vali Loss: 0.4159782 Test Loss: 0.4868909
Validation loss decreased (0.418014 --> 0.415978).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 114.61798286437988
Epoch: 47, Steps: 88 | Train Loss: 0.1486891 Vali Loss: 0.4150439 Test Loss: 0.4849704
Validation loss decreased (0.415978 --> 0.415044).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 119.16253304481506
Epoch: 48, Steps: 88 | Train Loss: 0.1472731 Vali Loss: 0.4126930 Test Loss: 0.4832573
Validation loss decreased (0.415044 --> 0.412693).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 121.32014489173889
Epoch: 49, Steps: 88 | Train Loss: 0.1459834 Vali Loss: 0.4120546 Test Loss: 0.4817802
Validation loss decreased (0.412693 --> 0.412055).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 119.27042150497437
Epoch: 50, Steps: 88 | Train Loss: 0.1447558 Vali Loss: 0.4095716 Test Loss: 0.4800228
Validation loss decreased (0.412055 --> 0.409572).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 111.36109280586243
Epoch: 51, Steps: 88 | Train Loss: 0.1435787 Vali Loss: 0.4080725 Test Loss: 0.4786490
Validation loss decreased (0.409572 --> 0.408073).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 113.15065026283264
Epoch: 52, Steps: 88 | Train Loss: 0.1424673 Vali Loss: 0.4073999 Test Loss: 0.4774672
Validation loss decreased (0.408073 --> 0.407400).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 112.44514727592468
Epoch: 53, Steps: 88 | Train Loss: 0.1414094 Vali Loss: 0.4054648 Test Loss: 0.4761284
Validation loss decreased (0.407400 --> 0.405465).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 112.33087301254272
Epoch: 54, Steps: 88 | Train Loss: 0.1404379 Vali Loss: 0.4048554 Test Loss: 0.4750617
Validation loss decreased (0.405465 --> 0.404855).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 121.03223180770874
Epoch: 55, Steps: 88 | Train Loss: 0.1395167 Vali Loss: 0.4032684 Test Loss: 0.4736606
Validation loss decreased (0.404855 --> 0.403268).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 117.75667309761047
Epoch: 56, Steps: 88 | Train Loss: 0.1386097 Vali Loss: 0.4021972 Test Loss: 0.4725680
Validation loss decreased (0.403268 --> 0.402197).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 116.9029791355133
Epoch: 57, Steps: 88 | Train Loss: 0.1377993 Vali Loss: 0.4016641 Test Loss: 0.4716461
Validation loss decreased (0.402197 --> 0.401664).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 111.09499478340149
Epoch: 58, Steps: 88 | Train Loss: 0.1369663 Vali Loss: 0.4008361 Test Loss: 0.4703746
Validation loss decreased (0.401664 --> 0.400836).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 111.0834538936615
Epoch: 59, Steps: 88 | Train Loss: 0.1362364 Vali Loss: 0.3997035 Test Loss: 0.4696487
Validation loss decreased (0.400836 --> 0.399704).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 114.59485602378845
Epoch: 60, Steps: 88 | Train Loss: 0.1355369 Vali Loss: 0.3993387 Test Loss: 0.4687259
Validation loss decreased (0.399704 --> 0.399339).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 110.35002565383911
Epoch: 61, Steps: 88 | Train Loss: 0.1348697 Vali Loss: 0.3982865 Test Loss: 0.4679137
Validation loss decreased (0.399339 --> 0.398286).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 117.81310319900513
Epoch: 62, Steps: 88 | Train Loss: 0.1342234 Vali Loss: 0.3974526 Test Loss: 0.4671850
Validation loss decreased (0.398286 --> 0.397453).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 115.37704062461853
Epoch: 63, Steps: 88 | Train Loss: 0.1336003 Vali Loss: 0.3968221 Test Loss: 0.4664007
Validation loss decreased (0.397453 --> 0.396822).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 111.24305391311646
Epoch: 64, Steps: 88 | Train Loss: 0.1330477 Vali Loss: 0.3963830 Test Loss: 0.4656726
Validation loss decreased (0.396822 --> 0.396383).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 112.17027306556702
Epoch: 65, Steps: 88 | Train Loss: 0.1324557 Vali Loss: 0.3956254 Test Loss: 0.4650754
Validation loss decreased (0.396383 --> 0.395625).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 114.11534452438354
Epoch: 66, Steps: 88 | Train Loss: 0.1319766 Vali Loss: 0.3949604 Test Loss: 0.4643662
Validation loss decreased (0.395625 --> 0.394960).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 107.83670020103455
Epoch: 67, Steps: 88 | Train Loss: 0.1314694 Vali Loss: 0.3940980 Test Loss: 0.4636739
Validation loss decreased (0.394960 --> 0.394098).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 107.76890516281128
Epoch: 68, Steps: 88 | Train Loss: 0.1310146 Vali Loss: 0.3939388 Test Loss: 0.4632190
Validation loss decreased (0.394098 --> 0.393939).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 110.65258622169495
Epoch: 69, Steps: 88 | Train Loss: 0.1305554 Vali Loss: 0.3930597 Test Loss: 0.4626422
Validation loss decreased (0.393939 --> 0.393060).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 106.45601844787598
Epoch: 70, Steps: 88 | Train Loss: 0.1301615 Vali Loss: 0.3931122 Test Loss: 0.4621358
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 109.05794978141785
Epoch: 71, Steps: 88 | Train Loss: 0.1297345 Vali Loss: 0.3922294 Test Loss: 0.4616335
Validation loss decreased (0.393060 --> 0.392229).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 114.9929940700531
Epoch: 72, Steps: 88 | Train Loss: 0.1293706 Vali Loss: 0.3914646 Test Loss: 0.4610600
Validation loss decreased (0.392229 --> 0.391465).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 109.03031873703003
Epoch: 73, Steps: 88 | Train Loss: 0.1290120 Vali Loss: 0.3913737 Test Loss: 0.4607784
Validation loss decreased (0.391465 --> 0.391374).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 107.76097393035889
Epoch: 74, Steps: 88 | Train Loss: 0.1286701 Vali Loss: 0.3910995 Test Loss: 0.4602866
Validation loss decreased (0.391374 --> 0.391100).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 108.75856018066406
Epoch: 75, Steps: 88 | Train Loss: 0.1283419 Vali Loss: 0.3905606 Test Loss: 0.4598438
Validation loss decreased (0.391100 --> 0.390561).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 108.12065720558167
Epoch: 76, Steps: 88 | Train Loss: 0.1280142 Vali Loss: 0.3901014 Test Loss: 0.4595396
Validation loss decreased (0.390561 --> 0.390101).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 103.64210891723633
Epoch: 77, Steps: 88 | Train Loss: 0.1277546 Vali Loss: 0.3901705 Test Loss: 0.4591053
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 107.72341346740723
Epoch: 78, Steps: 88 | Train Loss: 0.1274548 Vali Loss: 0.3893721 Test Loss: 0.4587690
Validation loss decreased (0.390101 --> 0.389372).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 105.75849270820618
Epoch: 79, Steps: 88 | Train Loss: 0.1271845 Vali Loss: 0.3887250 Test Loss: 0.4584245
Validation loss decreased (0.389372 --> 0.388725).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 106.13764786720276
Epoch: 80, Steps: 88 | Train Loss: 0.1269365 Vali Loss: 0.3887878 Test Loss: 0.4581211
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 111.27684020996094
Epoch: 81, Steps: 88 | Train Loss: 0.1266852 Vali Loss: 0.3882158 Test Loss: 0.4577951
Validation loss decreased (0.388725 --> 0.388216).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 103.04515242576599
Epoch: 82, Steps: 88 | Train Loss: 0.1264657 Vali Loss: 0.3883925 Test Loss: 0.4575308
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 103.79245853424072
Epoch: 83, Steps: 88 | Train Loss: 0.1262431 Vali Loss: 0.3882136 Test Loss: 0.4572777
Validation loss decreased (0.388216 --> 0.388214).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 106.60518264770508
Epoch: 84, Steps: 88 | Train Loss: 0.1260531 Vali Loss: 0.3880553 Test Loss: 0.4570363
Validation loss decreased (0.388214 --> 0.388055).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 106.68861532211304
Epoch: 85, Steps: 88 | Train Loss: 0.1258556 Vali Loss: 0.3874522 Test Loss: 0.4567655
Validation loss decreased (0.388055 --> 0.387452).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 105.52377200126648
Epoch: 86, Steps: 88 | Train Loss: 0.1256878 Vali Loss: 0.3878181 Test Loss: 0.4565397
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 105.94382071495056
Epoch: 87, Steps: 88 | Train Loss: 0.1255069 Vali Loss: 0.3875184 Test Loss: 0.4563126
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 110.08120346069336
Epoch: 88, Steps: 88 | Train Loss: 0.1253130 Vali Loss: 0.3868727 Test Loss: 0.4561059
Validation loss decreased (0.387452 --> 0.386873).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 107.49047446250916
Epoch: 89, Steps: 88 | Train Loss: 0.1251838 Vali Loss: 0.3869615 Test Loss: 0.4559345
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 103.41518378257751
Epoch: 90, Steps: 88 | Train Loss: 0.1250072 Vali Loss: 0.3862489 Test Loss: 0.4557546
Validation loss decreased (0.386873 --> 0.386249).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 112.7517659664154
Epoch: 91, Steps: 88 | Train Loss: 0.1248612 Vali Loss: 0.3866094 Test Loss: 0.4555542
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 114.00398969650269
Epoch: 92, Steps: 88 | Train Loss: 0.1247521 Vali Loss: 0.3859055 Test Loss: 0.4553922
Validation loss decreased (0.386249 --> 0.385905).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 109.56930303573608
Epoch: 93, Steps: 88 | Train Loss: 0.1246254 Vali Loss: 0.3862120 Test Loss: 0.4552425
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 112.18190026283264
Epoch: 94, Steps: 88 | Train Loss: 0.1244877 Vali Loss: 0.3854903 Test Loss: 0.4550812
Validation loss decreased (0.385905 --> 0.385490).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 112.02228307723999
Epoch: 95, Steps: 88 | Train Loss: 0.1243583 Vali Loss: 0.3861843 Test Loss: 0.4549675
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 112.24731135368347
Epoch: 96, Steps: 88 | Train Loss: 0.1242480 Vali Loss: 0.3853962 Test Loss: 0.4548107
Validation loss decreased (0.385490 --> 0.385396).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 116.63495635986328
Epoch: 97, Steps: 88 | Train Loss: 0.1241339 Vali Loss: 0.3850790 Test Loss: 0.4546675
Validation loss decreased (0.385396 --> 0.385079).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 116.11678075790405
Epoch: 98, Steps: 88 | Train Loss: 0.1240337 Vali Loss: 0.3855080 Test Loss: 0.4545481
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 113.2792866230011
Epoch: 99, Steps: 88 | Train Loss: 0.1239803 Vali Loss: 0.3852428 Test Loss: 0.4544154
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 109.8342604637146
Epoch: 100, Steps: 88 | Train Loss: 0.1238743 Vali Loss: 0.3853075 Test Loss: 0.4543214
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3804936960.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 114.15331745147705
Epoch: 1, Steps: 88 | Train Loss: 0.2531382 Vali Loss: 0.3372726 Test Loss: 0.4105926
Validation loss decreased (inf --> 0.337273).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 114.35145092010498
Epoch: 2, Steps: 88 | Train Loss: 0.2447234 Vali Loss: 0.3368296 Test Loss: 0.4096879
Validation loss decreased (0.337273 --> 0.336830).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 110.84217882156372
Epoch: 3, Steps: 88 | Train Loss: 0.2445516 Vali Loss: 0.3361715 Test Loss: 0.4096135
Validation loss decreased (0.336830 --> 0.336172).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 116.02397298812866
Epoch: 4, Steps: 88 | Train Loss: 0.2443466 Vali Loss: 0.3360938 Test Loss: 0.4092769
Validation loss decreased (0.336172 --> 0.336094).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 118.13280129432678
Epoch: 5, Steps: 88 | Train Loss: 0.2442612 Vali Loss: 0.3363161 Test Loss: 0.4094429
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 112.66107058525085
Epoch: 6, Steps: 88 | Train Loss: 0.2441418 Vali Loss: 0.3366487 Test Loss: 0.4095920
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 118.75796699523926
Epoch: 7, Steps: 88 | Train Loss: 0.2441466 Vali Loss: 0.3358405 Test Loss: 0.4095406
Validation loss decreased (0.336094 --> 0.335840).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 117.32302832603455
Epoch: 8, Steps: 88 | Train Loss: 0.2440872 Vali Loss: 0.3361393 Test Loss: 0.4094262
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 118.08598494529724
Epoch: 9, Steps: 88 | Train Loss: 0.2440230 Vali Loss: 0.3363651 Test Loss: 0.4092920
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 111.89875411987305
Epoch: 10, Steps: 88 | Train Loss: 0.2440208 Vali Loss: 0.3362122 Test Loss: 0.4091776
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 119.04052114486694
Epoch: 11, Steps: 88 | Train Loss: 0.2440100 Vali Loss: 0.3361055 Test Loss: 0.4090053
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 120.42161440849304
Epoch: 12, Steps: 88 | Train Loss: 0.2439815 Vali Loss: 0.3361579 Test Loss: 0.4090036
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 113.93600749969482
Epoch: 13, Steps: 88 | Train Loss: 0.2438410 Vali Loss: 0.3355504 Test Loss: 0.4090302
Validation loss decreased (0.335840 --> 0.335550).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 110.01781225204468
Epoch: 14, Steps: 88 | Train Loss: 0.2438855 Vali Loss: 0.3356913 Test Loss: 0.4090813
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 99.14469289779663
Epoch: 15, Steps: 88 | Train Loss: 0.2437967 Vali Loss: 0.3356197 Test Loss: 0.4089512
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 87.58319568634033
Epoch: 16, Steps: 88 | Train Loss: 0.2437682 Vali Loss: 0.3361975 Test Loss: 0.4087134
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 83.87228512763977
Epoch: 17, Steps: 88 | Train Loss: 0.2438932 Vali Loss: 0.3350583 Test Loss: 0.4087749
Validation loss decreased (0.335550 --> 0.335058).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 88.84580492973328
Epoch: 18, Steps: 88 | Train Loss: 0.2438718 Vali Loss: 0.3359918 Test Loss: 0.4087520
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 89.40572190284729
Epoch: 19, Steps: 88 | Train Loss: 0.2438541 Vali Loss: 0.3357526 Test Loss: 0.4088933
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 90.20355796813965
Epoch: 20, Steps: 88 | Train Loss: 0.2438451 Vali Loss: 0.3360679 Test Loss: 0.4088091
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 94.7642879486084
Epoch: 21, Steps: 88 | Train Loss: 0.2437892 Vali Loss: 0.3360512 Test Loss: 0.4088285
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 93.19583201408386
Epoch: 22, Steps: 88 | Train Loss: 0.2437849 Vali Loss: 0.3357391 Test Loss: 0.4088646
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 94.23706984519958
Epoch: 23, Steps: 88 | Train Loss: 0.2437808 Vali Loss: 0.3358352 Test Loss: 0.4089017
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 98.1274151802063
Epoch: 24, Steps: 88 | Train Loss: 0.2437250 Vali Loss: 0.3357826 Test Loss: 0.4085054
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 90.52364253997803
Epoch: 25, Steps: 88 | Train Loss: 0.2437013 Vali Loss: 0.3352224 Test Loss: 0.4088178
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 90.67181205749512
Epoch: 26, Steps: 88 | Train Loss: 0.2436236 Vali Loss: 0.3357508 Test Loss: 0.4086021
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 99.33689618110657
Epoch: 27, Steps: 88 | Train Loss: 0.2436503 Vali Loss: 0.3358315 Test Loss: 0.4087046
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j192_H5_FITS_custom_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4078502357006073, mae:0.2873346209526062, rse:0.5270834565162659, corr:[0.28746668 0.29404968 0.2939068  0.29384634 0.29389817 0.2938896
 0.29401103 0.2941686  0.2940684  0.2936786  0.29334554 0.29333237
 0.29347652 0.29342    0.29318264 0.2930387  0.29311836 0.29323855
 0.2932011  0.29307228 0.2930015  0.292974   0.29297942 0.29344338
 0.29424697 0.29415184 0.29390904 0.29388323 0.29385993 0.293777
 0.29375184 0.29381195 0.2938388  0.2936817  0.29335076 0.2931068
 0.29318616 0.2934029  0.29347822 0.2933633  0.29327548 0.29335326
 0.29344192 0.29326293 0.29271936 0.29204616 0.29168844 0.2920474
 0.2928159  0.2931741  0.29317406 0.29301947 0.29287916 0.2928199
 0.29277793 0.29268277 0.29260114 0.29262275 0.29268196 0.2926571
 0.29256234 0.29242536 0.29236236 0.29243007 0.29259005 0.2927241
 0.29266948 0.29238987 0.29201472 0.29175916 0.29168403 0.2917426
 0.2916514  0.2914314  0.29129586 0.2912819  0.29137295 0.2914685
 0.29144174 0.291321   0.29127857 0.29140112 0.29158428 0.29170477
 0.29176253 0.29179582 0.29188693 0.2920297  0.29212096 0.29199508
 0.29157817 0.29102492 0.29067945 0.29071882 0.29091465 0.29093882
 0.29050744 0.29002807 0.28993443 0.29015985 0.29039845 0.29043698
 0.29029822 0.29019183 0.29029596 0.29055855 0.2907812  0.2908372
 0.29079443 0.29079577 0.2909462  0.2911706  0.2912666  0.2911347
 0.29081956 0.29052454 0.29041716 0.29039472 0.29039976 0.2903018
 0.29009295 0.29011256 0.2903892  0.29059458 0.29069442 0.29072645
 0.29072398 0.2907104  0.29071638 0.29076344 0.29087237 0.2910013
 0.2910733  0.2910985  0.2911926  0.29143268 0.2917265  0.29186133
 0.29179648 0.2916419  0.29155186 0.2915759  0.29157794 0.2915543
 0.2915755  0.29151016 0.29143447 0.29134184 0.2912256  0.29112822
 0.29111373 0.29124138 0.29146066 0.29163238 0.29164484 0.2915427
 0.29143843 0.29140723 0.29149428 0.2917272  0.29211575 0.29247758
 0.29260483 0.2924871  0.29230055 0.29216954 0.29209536 0.29238167
 0.29292554 0.29262254 0.29226944 0.2922449  0.2923094  0.29233223
 0.29228798 0.29221147 0.29215264 0.29213598 0.29209897 0.292017
 0.29199418 0.29211998 0.29242912 0.2927065  0.29271233 0.29253605
 0.29241347 0.29243496 0.29220462 0.29147094 0.29106528 0.29253322]
