Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j96_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j96_H5_FITS_custom_ftM_sl180_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 12005
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=50, out_features=76, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  419276800.0
params:  3876.0
Trainable parameters:  3876
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 52.11806893348694
Epoch: 1, Steps: 93 | Train Loss: 0.7698762 Vali Loss: 0.6585862 Test Loss: 0.7857326
Validation loss decreased (inf --> 0.658586).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 48.7681679725647
Epoch: 2, Steps: 93 | Train Loss: 0.4312634 Vali Loss: 0.5006406 Test Loss: 0.5982222
Validation loss decreased (0.658586 --> 0.500641).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 48.307740926742554
Epoch: 3, Steps: 93 | Train Loss: 0.3515264 Vali Loss: 0.4426257 Test Loss: 0.5299144
Validation loss decreased (0.500641 --> 0.442626).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 49.33361482620239
Epoch: 4, Steps: 93 | Train Loss: 0.3179268 Vali Loss: 0.4149909 Test Loss: 0.4974855
Validation loss decreased (0.442626 --> 0.414991).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 48.09837484359741
Epoch: 5, Steps: 93 | Train Loss: 0.3010513 Vali Loss: 0.4000932 Test Loss: 0.4807937
Validation loss decreased (0.414991 --> 0.400093).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 51.03157019615173
Epoch: 6, Steps: 93 | Train Loss: 0.2918491 Vali Loss: 0.3911126 Test Loss: 0.4717756
Validation loss decreased (0.400093 --> 0.391113).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 48.954381227493286
Epoch: 7, Steps: 93 | Train Loss: 0.2864360 Vali Loss: 0.3856468 Test Loss: 0.4666173
Validation loss decreased (0.391113 --> 0.385647).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 52.361286878585815
Epoch: 8, Steps: 93 | Train Loss: 0.2833295 Vali Loss: 0.3822934 Test Loss: 0.4633974
Validation loss decreased (0.385647 --> 0.382293).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 53.013169050216675
Epoch: 9, Steps: 93 | Train Loss: 0.2813818 Vali Loss: 0.3806831 Test Loss: 0.4615126
Validation loss decreased (0.382293 --> 0.380683).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 49.52524280548096
Epoch: 10, Steps: 93 | Train Loss: 0.2800587 Vali Loss: 0.3796212 Test Loss: 0.4602682
Validation loss decreased (0.380683 --> 0.379621).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 51.92711329460144
Epoch: 11, Steps: 93 | Train Loss: 0.2792084 Vali Loss: 0.3781798 Test Loss: 0.4594923
Validation loss decreased (0.379621 --> 0.378180).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 48.993346214294434
Epoch: 12, Steps: 93 | Train Loss: 0.2785116 Vali Loss: 0.3790458 Test Loss: 0.4589200
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 48.31213855743408
Epoch: 13, Steps: 93 | Train Loss: 0.2782783 Vali Loss: 0.3765226 Test Loss: 0.4585089
Validation loss decreased (0.378180 --> 0.376523).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 50.96360921859741
Epoch: 14, Steps: 93 | Train Loss: 0.2779965 Vali Loss: 0.3779644 Test Loss: 0.4583209
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 52.67613959312439
Epoch: 15, Steps: 93 | Train Loss: 0.2777868 Vali Loss: 0.3763162 Test Loss: 0.4582038
Validation loss decreased (0.376523 --> 0.376316).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 49.618037700653076
Epoch: 16, Steps: 93 | Train Loss: 0.2775412 Vali Loss: 0.3783142 Test Loss: 0.4579794
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 51.66246438026428
Epoch: 17, Steps: 93 | Train Loss: 0.2774402 Vali Loss: 0.3771885 Test Loss: 0.4578681
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 47.49866843223572
Epoch: 18, Steps: 93 | Train Loss: 0.2774180 Vali Loss: 0.3753717 Test Loss: 0.4577581
Validation loss decreased (0.376316 --> 0.375372).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 47.13658022880554
Epoch: 19, Steps: 93 | Train Loss: 0.2774328 Vali Loss: 0.3759439 Test Loss: 0.4577202
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 49.78764796257019
Epoch: 20, Steps: 93 | Train Loss: 0.2772001 Vali Loss: 0.3765210 Test Loss: 0.4576565
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 46.56078028678894
Epoch: 21, Steps: 93 | Train Loss: 0.2771254 Vali Loss: 0.3761171 Test Loss: 0.4576376
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 50.244040727615356
Epoch: 22, Steps: 93 | Train Loss: 0.2769929 Vali Loss: 0.3764048 Test Loss: 0.4575536
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 47.757548093795776
Epoch: 23, Steps: 93 | Train Loss: 0.2771575 Vali Loss: 0.3774655 Test Loss: 0.4575008
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 46.05868911743164
Epoch: 24, Steps: 93 | Train Loss: 0.2771183 Vali Loss: 0.3758444 Test Loss: 0.4574519
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 50.99126076698303
Epoch: 25, Steps: 93 | Train Loss: 0.2771334 Vali Loss: 0.3766680 Test Loss: 0.4574160
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 43.02560567855835
Epoch: 26, Steps: 93 | Train Loss: 0.2770318 Vali Loss: 0.3750241 Test Loss: 0.4573675
Validation loss decreased (0.375372 --> 0.375024).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 48.35344123840332
Epoch: 27, Steps: 93 | Train Loss: 0.2770225 Vali Loss: 0.3775149 Test Loss: 0.4573176
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 46.53978681564331
Epoch: 28, Steps: 93 | Train Loss: 0.2769590 Vali Loss: 0.3756405 Test Loss: 0.4573669
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 47.836639642715454
Epoch: 29, Steps: 93 | Train Loss: 0.2767235 Vali Loss: 0.3747449 Test Loss: 0.4572636
Validation loss decreased (0.375024 --> 0.374745).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 47.199363470077515
Epoch: 30, Steps: 93 | Train Loss: 0.2767833 Vali Loss: 0.3752084 Test Loss: 0.4573141
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 47.16819763183594
Epoch: 31, Steps: 93 | Train Loss: 0.2767433 Vali Loss: 0.3763249 Test Loss: 0.4572803
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 47.36940383911133
Epoch: 32, Steps: 93 | Train Loss: 0.2768021 Vali Loss: 0.3764934 Test Loss: 0.4572449
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 45.651549100875854
Epoch: 33, Steps: 93 | Train Loss: 0.2767687 Vali Loss: 0.3756005 Test Loss: 0.4572527
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 46.18613290786743
Epoch: 34, Steps: 93 | Train Loss: 0.2768937 Vali Loss: 0.3759243 Test Loss: 0.4571773
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 46.950034618377686
Epoch: 35, Steps: 93 | Train Loss: 0.2769055 Vali Loss: 0.3754710 Test Loss: 0.4571701
EarlyStopping counter: 6 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 48.86076068878174
Epoch: 36, Steps: 93 | Train Loss: 0.2767258 Vali Loss: 0.3759424 Test Loss: 0.4572132
EarlyStopping counter: 7 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 45.68266534805298
Epoch: 37, Steps: 93 | Train Loss: 0.2766901 Vali Loss: 0.3755659 Test Loss: 0.4571222
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 47.88235592842102
Epoch: 38, Steps: 93 | Train Loss: 0.2767566 Vali Loss: 0.3758598 Test Loss: 0.4571704
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 46.924243688583374
Epoch: 39, Steps: 93 | Train Loss: 0.2767287 Vali Loss: 0.3754641 Test Loss: 0.4571333
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j96_H5_FITS_custom_ftM_sl180_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.45705729722976685, mae:0.3056066930294037, rse:0.5598075985908508, corr:[0.2890577  0.29021686 0.29254097 0.29227066 0.29108807 0.29199147
 0.29295948 0.29293734 0.29245603 0.2921148  0.29210928 0.29158482
 0.29076883 0.29098848 0.29121125 0.29071358 0.29039347 0.29078147
 0.29121253 0.29117465 0.2911125  0.29193166 0.29278153 0.29220212
 0.29117444 0.29046732 0.29095784 0.29097447 0.29046813 0.29076943
 0.29123223 0.29120505 0.29106802 0.29087046 0.2906931  0.29052672
 0.2901338  0.2898868  0.29021427 0.29068986 0.29068196 0.29070038
 0.29092252 0.29087567 0.2906692  0.29069299 0.2908577  0.2909675
 0.2907214  0.2899823  0.28992644 0.29050767 0.29059175 0.29031152
 0.29034725 0.29049906 0.29044798 0.29024872 0.29005402 0.2899542
 0.28972462 0.2892926  0.2893118  0.28997466 0.29023126 0.2898905
 0.28983256 0.29007202 0.29020414 0.2902427  0.28994662 0.28956965
 0.2895484  0.28947327 0.2890553  0.28905302 0.2895116  0.28946382
 0.289093   0.2891708  0.28953162 0.2896589  0.2894203  0.28929612
 0.28963852 0.28958866 0.288834   0.2889368  0.2898247  0.28925112
 0.28807202 0.28841195 0.288692   0.28729466 0.28640983 0.29027814]
