Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j720_H5', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j720_H5_FITS_custom_ftM_sl360_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11201
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=90, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2681164800.0
params:  24570.0
Trainable parameters:  24570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 125.39333772659302
Epoch: 1, Steps: 87 | Train Loss: 1.4226978 Vali Loss: 1.3859146 Test Loss: 1.6517247
Validation loss decreased (inf --> 1.385915).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 123.0783920288086
Epoch: 2, Steps: 87 | Train Loss: 0.9578116 Vali Loss: 1.1111059 Test Loss: 1.3071480
Validation loss decreased (1.385915 --> 1.111106).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 123.04759097099304
Epoch: 3, Steps: 87 | Train Loss: 0.7922923 Vali Loss: 1.0012357 Test Loss: 1.1712915
Validation loss decreased (1.111106 --> 1.001236).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 127.25781202316284
Epoch: 4, Steps: 87 | Train Loss: 0.7129195 Vali Loss: 0.9361718 Test Loss: 1.0916128
Validation loss decreased (1.001236 --> 0.936172).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 118.80727648735046
Epoch: 5, Steps: 87 | Train Loss: 0.6595665 Vali Loss: 0.8867941 Test Loss: 1.0313188
Validation loss decreased (0.936172 --> 0.886794).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 122.59327173233032
Epoch: 6, Steps: 87 | Train Loss: 0.6169131 Vali Loss: 0.8441738 Test Loss: 0.9803938
Validation loss decreased (0.886794 --> 0.844174).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 127.69384455680847
Epoch: 7, Steps: 87 | Train Loss: 0.5807914 Vali Loss: 0.8067070 Test Loss: 0.9364719
Validation loss decreased (0.844174 --> 0.806707).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 121.03429460525513
Epoch: 8, Steps: 87 | Train Loss: 0.5494816 Vali Loss: 0.7733986 Test Loss: 0.8967737
Validation loss decreased (0.806707 --> 0.773399).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 126.88865566253662
Epoch: 9, Steps: 87 | Train Loss: 0.5220270 Vali Loss: 0.7451471 Test Loss: 0.8633352
Validation loss decreased (0.773399 --> 0.745147).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 124.77764868736267
Epoch: 10, Steps: 87 | Train Loss: 0.4977505 Vali Loss: 0.7195245 Test Loss: 0.8331888
Validation loss decreased (0.745147 --> 0.719525).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 126.48938775062561
Epoch: 11, Steps: 87 | Train Loss: 0.4762025 Vali Loss: 0.6970599 Test Loss: 0.8062039
Validation loss decreased (0.719525 --> 0.697060).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 127.61956882476807
Epoch: 12, Steps: 87 | Train Loss: 0.4569510 Vali Loss: 0.6759467 Test Loss: 0.7821560
Validation loss decreased (0.697060 --> 0.675947).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 122.22883653640747
Epoch: 13, Steps: 87 | Train Loss: 0.4396138 Vali Loss: 0.6571474 Test Loss: 0.7600330
Validation loss decreased (0.675947 --> 0.657147).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 126.09150624275208
Epoch: 14, Steps: 87 | Train Loss: 0.4240377 Vali Loss: 0.6414641 Test Loss: 0.7411274
Validation loss decreased (0.657147 --> 0.641464).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 126.53570318222046
Epoch: 15, Steps: 87 | Train Loss: 0.4099687 Vali Loss: 0.6258835 Test Loss: 0.7230406
Validation loss decreased (0.641464 --> 0.625883).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 125.44764947891235
Epoch: 16, Steps: 87 | Train Loss: 0.3972240 Vali Loss: 0.6125594 Test Loss: 0.7075039
Validation loss decreased (0.625883 --> 0.612559).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 128.72454810142517
Epoch: 17, Steps: 87 | Train Loss: 0.3856928 Vali Loss: 0.5986985 Test Loss: 0.6925598
Validation loss decreased (0.612559 --> 0.598698).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 124.18918752670288
Epoch: 18, Steps: 87 | Train Loss: 0.3751718 Vali Loss: 0.5879198 Test Loss: 0.6792837
Validation loss decreased (0.598698 --> 0.587920).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 124.50182485580444
Epoch: 19, Steps: 87 | Train Loss: 0.3655669 Vali Loss: 0.5775119 Test Loss: 0.6671094
Validation loss decreased (0.587920 --> 0.577512).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 125.40899276733398
Epoch: 20, Steps: 87 | Train Loss: 0.3567869 Vali Loss: 0.5678933 Test Loss: 0.6561244
Validation loss decreased (0.577512 --> 0.567893).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 125.6377284526825
Epoch: 21, Steps: 87 | Train Loss: 0.3487188 Vali Loss: 0.5594926 Test Loss: 0.6459712
Validation loss decreased (0.567893 --> 0.559493).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 130.42237067222595
Epoch: 22, Steps: 87 | Train Loss: 0.3413211 Vali Loss: 0.5512357 Test Loss: 0.6367177
Validation loss decreased (0.559493 --> 0.551236).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 125.75434112548828
Epoch: 23, Steps: 87 | Train Loss: 0.3345475 Vali Loss: 0.5438795 Test Loss: 0.6282263
Validation loss decreased (0.551236 --> 0.543880).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 125.19897937774658
Epoch: 24, Steps: 87 | Train Loss: 0.3282457 Vali Loss: 0.5374489 Test Loss: 0.6203555
Validation loss decreased (0.543880 --> 0.537449).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 127.09317421913147
Epoch: 25, Steps: 87 | Train Loss: 0.3225341 Vali Loss: 0.5308061 Test Loss: 0.6130446
Validation loss decreased (0.537449 --> 0.530806).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 128.8969748020172
Epoch: 26, Steps: 87 | Train Loss: 0.3171922 Vali Loss: 0.5251336 Test Loss: 0.6063867
Validation loss decreased (0.530806 --> 0.525134).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 134.50912022590637
Epoch: 27, Steps: 87 | Train Loss: 0.3122176 Vali Loss: 0.5196576 Test Loss: 0.6001658
Validation loss decreased (0.525134 --> 0.519658).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 127.10950446128845
Epoch: 28, Steps: 87 | Train Loss: 0.3076769 Vali Loss: 0.5146731 Test Loss: 0.5945806
Validation loss decreased (0.519658 --> 0.514673).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 124.96563982963562
Epoch: 29, Steps: 87 | Train Loss: 0.3034319 Vali Loss: 0.5097739 Test Loss: 0.5890456
Validation loss decreased (0.514673 --> 0.509774).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 123.58743619918823
Epoch: 30, Steps: 87 | Train Loss: 0.2994962 Vali Loss: 0.5060712 Test Loss: 0.5844257
Validation loss decreased (0.509774 --> 0.506071).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 119.45110082626343
Epoch: 31, Steps: 87 | Train Loss: 0.2958497 Vali Loss: 0.5017638 Test Loss: 0.5796012
Validation loss decreased (0.506071 --> 0.501764).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 117.62623810768127
Epoch: 32, Steps: 87 | Train Loss: 0.2924169 Vali Loss: 0.4977987 Test Loss: 0.5754671
Validation loss decreased (0.501764 --> 0.497799).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 120.88947296142578
Epoch: 33, Steps: 87 | Train Loss: 0.2892910 Vali Loss: 0.4941601 Test Loss: 0.5713332
Validation loss decreased (0.497799 --> 0.494160).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 120.2867648601532
Epoch: 34, Steps: 87 | Train Loss: 0.2862759 Vali Loss: 0.4918095 Test Loss: 0.5678649
Validation loss decreased (0.494160 --> 0.491810).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 118.99089980125427
Epoch: 35, Steps: 87 | Train Loss: 0.2834801 Vali Loss: 0.4883201 Test Loss: 0.5642295
Validation loss decreased (0.491810 --> 0.488320).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 119.3499608039856
Epoch: 36, Steps: 87 | Train Loss: 0.2808979 Vali Loss: 0.4856229 Test Loss: 0.5610724
Validation loss decreased (0.488320 --> 0.485623).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 121.8119969367981
Epoch: 37, Steps: 87 | Train Loss: 0.2784577 Vali Loss: 0.4830198 Test Loss: 0.5580563
Validation loss decreased (0.485623 --> 0.483020).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 122.24025774002075
Epoch: 38, Steps: 87 | Train Loss: 0.2761947 Vali Loss: 0.4804298 Test Loss: 0.5552815
Validation loss decreased (0.483020 --> 0.480430).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 117.62819981575012
Epoch: 39, Steps: 87 | Train Loss: 0.2740538 Vali Loss: 0.4778416 Test Loss: 0.5526178
Validation loss decreased (0.480430 --> 0.477842).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 116.28737902641296
Epoch: 40, Steps: 87 | Train Loss: 0.2720118 Vali Loss: 0.4759015 Test Loss: 0.5502511
Validation loss decreased (0.477842 --> 0.475902).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 117.30653548240662
Epoch: 41, Steps: 87 | Train Loss: 0.2701539 Vali Loss: 0.4739878 Test Loss: 0.5477728
Validation loss decreased (0.475902 --> 0.473988).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 117.67523097991943
Epoch: 42, Steps: 87 | Train Loss: 0.2683692 Vali Loss: 0.4718217 Test Loss: 0.5456595
Validation loss decreased (0.473988 --> 0.471822).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 122.84969997406006
Epoch: 43, Steps: 87 | Train Loss: 0.2667316 Vali Loss: 0.4696757 Test Loss: 0.5435579
Validation loss decreased (0.471822 --> 0.469676).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 128.85203528404236
Epoch: 44, Steps: 87 | Train Loss: 0.2651678 Vali Loss: 0.4690027 Test Loss: 0.5416899
Validation loss decreased (0.469676 --> 0.469003).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 123.68107533454895
Epoch: 45, Steps: 87 | Train Loss: 0.2636891 Vali Loss: 0.4670534 Test Loss: 0.5398430
Validation loss decreased (0.469003 --> 0.467053).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 123.03419375419617
Epoch: 46, Steps: 87 | Train Loss: 0.2623106 Vali Loss: 0.4655586 Test Loss: 0.5381351
Validation loss decreased (0.467053 --> 0.465559).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 119.44946336746216
Epoch: 47, Steps: 87 | Train Loss: 0.2610241 Vali Loss: 0.4644387 Test Loss: 0.5365351
Validation loss decreased (0.465559 --> 0.464439).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 117.5675675868988
Epoch: 48, Steps: 87 | Train Loss: 0.2597773 Vali Loss: 0.4624970 Test Loss: 0.5349580
Validation loss decreased (0.464439 --> 0.462497).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 124.7171151638031
Epoch: 49, Steps: 87 | Train Loss: 0.2586287 Vali Loss: 0.4610515 Test Loss: 0.5335579
Validation loss decreased (0.462497 --> 0.461052).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 125.53173542022705
Epoch: 50, Steps: 87 | Train Loss: 0.2574905 Vali Loss: 0.4600391 Test Loss: 0.5322171
Validation loss decreased (0.461052 --> 0.460039).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 126.09295320510864
Epoch: 51, Steps: 87 | Train Loss: 0.2564611 Vali Loss: 0.4590959 Test Loss: 0.5309074
Validation loss decreased (0.460039 --> 0.459096).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 121.9157485961914
Epoch: 52, Steps: 87 | Train Loss: 0.2555106 Vali Loss: 0.4580027 Test Loss: 0.5297018
Validation loss decreased (0.459096 --> 0.458003).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 117.55529642105103
Epoch: 53, Steps: 87 | Train Loss: 0.2545435 Vali Loss: 0.4568830 Test Loss: 0.5285766
Validation loss decreased (0.458003 --> 0.456883).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 121.6554627418518
Epoch: 54, Steps: 87 | Train Loss: 0.2536868 Vali Loss: 0.4559404 Test Loss: 0.5275277
Validation loss decreased (0.456883 --> 0.455940).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 125.5726637840271
Epoch: 55, Steps: 87 | Train Loss: 0.2528374 Vali Loss: 0.4549326 Test Loss: 0.5265115
Validation loss decreased (0.455940 --> 0.454933).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 122.74913287162781
Epoch: 56, Steps: 87 | Train Loss: 0.2520617 Vali Loss: 0.4547825 Test Loss: 0.5255269
Validation loss decreased (0.454933 --> 0.454783).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 124.77109980583191
Epoch: 57, Steps: 87 | Train Loss: 0.2513014 Vali Loss: 0.4530754 Test Loss: 0.5246437
Validation loss decreased (0.454783 --> 0.453075).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 122.51688432693481
Epoch: 58, Steps: 87 | Train Loss: 0.2505915 Vali Loss: 0.4529495 Test Loss: 0.5237393
Validation loss decreased (0.453075 --> 0.452950).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 125.22016215324402
Epoch: 59, Steps: 87 | Train Loss: 0.2499670 Vali Loss: 0.4517207 Test Loss: 0.5229027
Validation loss decreased (0.452950 --> 0.451721).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 125.43157434463501
Epoch: 60, Steps: 87 | Train Loss: 0.2493222 Vali Loss: 0.4510102 Test Loss: 0.5221608
Validation loss decreased (0.451721 --> 0.451010).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 123.26023149490356
Epoch: 61, Steps: 87 | Train Loss: 0.2487071 Vali Loss: 0.4510655 Test Loss: 0.5214082
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 123.1710753440857
Epoch: 62, Steps: 87 | Train Loss: 0.2481568 Vali Loss: 0.4500011 Test Loss: 0.5207133
Validation loss decreased (0.451010 --> 0.450001).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 123.46111559867859
Epoch: 63, Steps: 87 | Train Loss: 0.2476085 Vali Loss: 0.4495602 Test Loss: 0.5200471
Validation loss decreased (0.450001 --> 0.449560).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 119.32960104942322
Epoch: 64, Steps: 87 | Train Loss: 0.2471002 Vali Loss: 0.4491893 Test Loss: 0.5194422
Validation loss decreased (0.449560 --> 0.449189).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 122.02390646934509
Epoch: 65, Steps: 87 | Train Loss: 0.2465798 Vali Loss: 0.4479167 Test Loss: 0.5188380
Validation loss decreased (0.449189 --> 0.447917).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 122.106201171875
Epoch: 66, Steps: 87 | Train Loss: 0.2461403 Vali Loss: 0.4480139 Test Loss: 0.5182905
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 124.18726325035095
Epoch: 67, Steps: 87 | Train Loss: 0.2457121 Vali Loss: 0.4474462 Test Loss: 0.5177171
Validation loss decreased (0.447917 --> 0.447446).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 108.48698163032532
Epoch: 68, Steps: 87 | Train Loss: 0.2452599 Vali Loss: 0.4470985 Test Loss: 0.5172398
Validation loss decreased (0.447446 --> 0.447098).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 99.21219277381897
Epoch: 69, Steps: 87 | Train Loss: 0.2448793 Vali Loss: 0.4464112 Test Loss: 0.5167557
Validation loss decreased (0.447098 --> 0.446411).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 100.23833966255188
Epoch: 70, Steps: 87 | Train Loss: 0.2445270 Vali Loss: 0.4457910 Test Loss: 0.5162856
Validation loss decreased (0.446411 --> 0.445791).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 99.50399470329285
Epoch: 71, Steps: 87 | Train Loss: 0.2441587 Vali Loss: 0.4456038 Test Loss: 0.5158637
Validation loss decreased (0.445791 --> 0.445604).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 98.9566581249237
Epoch: 72, Steps: 87 | Train Loss: 0.2438755 Vali Loss: 0.4450438 Test Loss: 0.5154361
Validation loss decreased (0.445604 --> 0.445044).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 97.58916211128235
Epoch: 73, Steps: 87 | Train Loss: 0.2435185 Vali Loss: 0.4452281 Test Loss: 0.5150484
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 99.25850820541382
Epoch: 74, Steps: 87 | Train Loss: 0.2431756 Vali Loss: 0.4449816 Test Loss: 0.5146841
Validation loss decreased (0.445044 --> 0.444982).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 99.49403786659241
Epoch: 75, Steps: 87 | Train Loss: 0.2429197 Vali Loss: 0.4442632 Test Loss: 0.5143522
Validation loss decreased (0.444982 --> 0.444263).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 100.39444923400879
Epoch: 76, Steps: 87 | Train Loss: 0.2426197 Vali Loss: 0.4441305 Test Loss: 0.5140097
Validation loss decreased (0.444263 --> 0.444131).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 99.92630290985107
Epoch: 77, Steps: 87 | Train Loss: 0.2423596 Vali Loss: 0.4439451 Test Loss: 0.5136963
Validation loss decreased (0.444131 --> 0.443945).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 101.49131989479065
Epoch: 78, Steps: 87 | Train Loss: 0.2421176 Vali Loss: 0.4436755 Test Loss: 0.5133884
Validation loss decreased (0.443945 --> 0.443675).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 101.25129413604736
Epoch: 79, Steps: 87 | Train Loss: 0.2419327 Vali Loss: 0.4439965 Test Loss: 0.5131010
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 98.68891882896423
Epoch: 80, Steps: 87 | Train Loss: 0.2417072 Vali Loss: 0.4432777 Test Loss: 0.5128241
Validation loss decreased (0.443675 --> 0.443278).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 88.08222818374634
Epoch: 81, Steps: 87 | Train Loss: 0.2414267 Vali Loss: 0.4432688 Test Loss: 0.5125716
Validation loss decreased (0.443278 --> 0.443269).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 78.71799898147583
Epoch: 82, Steps: 87 | Train Loss: 0.2412385 Vali Loss: 0.4426195 Test Loss: 0.5123374
Validation loss decreased (0.443269 --> 0.442620).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 77.67119145393372
Epoch: 83, Steps: 87 | Train Loss: 0.2410731 Vali Loss: 0.4428352 Test Loss: 0.5121053
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 79.56588959693909
Epoch: 84, Steps: 87 | Train Loss: 0.2409123 Vali Loss: 0.4423138 Test Loss: 0.5118766
Validation loss decreased (0.442620 --> 0.442314).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 77.49076437950134
Epoch: 85, Steps: 87 | Train Loss: 0.2407323 Vali Loss: 0.4423973 Test Loss: 0.5116653
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 78.86172008514404
Epoch: 86, Steps: 87 | Train Loss: 0.2405235 Vali Loss: 0.4422001 Test Loss: 0.5114742
Validation loss decreased (0.442314 --> 0.442200).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 79.6479275226593
Epoch: 87, Steps: 87 | Train Loss: 0.2404156 Vali Loss: 0.4420443 Test Loss: 0.5112839
Validation loss decreased (0.442200 --> 0.442044).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 78.84067463874817
Epoch: 88, Steps: 87 | Train Loss: 0.2402710 Vali Loss: 0.4414134 Test Loss: 0.5110975
Validation loss decreased (0.442044 --> 0.441413).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 78.70920205116272
Epoch: 89, Steps: 87 | Train Loss: 0.2401154 Vali Loss: 0.4414121 Test Loss: 0.5109255
Validation loss decreased (0.441413 --> 0.441412).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 79.46680927276611
Epoch: 90, Steps: 87 | Train Loss: 0.2399753 Vali Loss: 0.4418551 Test Loss: 0.5107720
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 77.26487565040588
Epoch: 91, Steps: 87 | Train Loss: 0.2398889 Vali Loss: 0.4413466 Test Loss: 0.5106146
Validation loss decreased (0.441412 --> 0.441347).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 68.57361626625061
Epoch: 92, Steps: 87 | Train Loss: 0.2397201 Vali Loss: 0.4406143 Test Loss: 0.5104675
Validation loss decreased (0.441347 --> 0.440614).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 61.11216402053833
Epoch: 93, Steps: 87 | Train Loss: 0.2396037 Vali Loss: 0.4412040 Test Loss: 0.5103275
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 62.160940170288086
Epoch: 94, Steps: 87 | Train Loss: 0.2395043 Vali Loss: 0.4411725 Test Loss: 0.5101990
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 60.9510555267334
Epoch: 95, Steps: 87 | Train Loss: 0.2394158 Vali Loss: 0.4409284 Test Loss: 0.5100674
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 60.43232226371765
Epoch: 96, Steps: 87 | Train Loss: 0.2392567 Vali Loss: 0.4406251 Test Loss: 0.5099481
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 60.62033176422119
Epoch: 97, Steps: 87 | Train Loss: 0.2391891 Vali Loss: 0.4407931 Test Loss: 0.5098301
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 61.31705570220947
Epoch: 98, Steps: 87 | Train Loss: 0.2391009 Vali Loss: 0.4403870 Test Loss: 0.5097323
Validation loss decreased (0.440614 --> 0.440387).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 59.77318072319031
Epoch: 99, Steps: 87 | Train Loss: 0.2390408 Vali Loss: 0.4406188 Test Loss: 0.5096226
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 60.881819009780884
Epoch: 100, Steps: 87 | Train Loss: 0.2389622 Vali Loss: 0.4404519 Test Loss: 0.5095277
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11201
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=90, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2681164800.0
params:  24570.0
Trainable parameters:  24570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 61.36364769935608
Epoch: 1, Steps: 87 | Train Loss: 0.3059102 Vali Loss: 0.4125762 Test Loss: 0.4787885
Validation loss decreased (inf --> 0.412576).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 61.306861877441406
Epoch: 2, Steps: 87 | Train Loss: 0.2920825 Vali Loss: 0.4060548 Test Loss: 0.4714576
Validation loss decreased (0.412576 --> 0.406055).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 61.269837379455566
Epoch: 3, Steps: 87 | Train Loss: 0.2886742 Vali Loss: 0.4051753 Test Loss: 0.4708608
Validation loss decreased (0.406055 --> 0.405175).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 60.98619341850281
Epoch: 4, Steps: 87 | Train Loss: 0.2880831 Vali Loss: 0.4055693 Test Loss: 0.4702042
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 60.652047634124756
Epoch: 5, Steps: 87 | Train Loss: 0.2878118 Vali Loss: 0.4045637 Test Loss: 0.4700365
Validation loss decreased (0.405175 --> 0.404564).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 61.591135025024414
Epoch: 6, Steps: 87 | Train Loss: 0.2878525 Vali Loss: 0.4046434 Test Loss: 0.4701207
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 60.45955753326416
Epoch: 7, Steps: 87 | Train Loss: 0.2878395 Vali Loss: 0.4051383 Test Loss: 0.4704171
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 60.757614850997925
Epoch: 8, Steps: 87 | Train Loss: 0.2878451 Vali Loss: 0.4051005 Test Loss: 0.4705825
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 60.900885820388794
Epoch: 9, Steps: 87 | Train Loss: 0.2877882 Vali Loss: 0.4052193 Test Loss: 0.4702697
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 60.700308322906494
Epoch: 10, Steps: 87 | Train Loss: 0.2878085 Vali Loss: 0.4049536 Test Loss: 0.4701999
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 61.7568097114563
Epoch: 11, Steps: 87 | Train Loss: 0.2877650 Vali Loss: 0.4045220 Test Loss: 0.4701321
Validation loss decreased (0.404564 --> 0.404522).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 61.05279588699341
Epoch: 12, Steps: 87 | Train Loss: 0.2877400 Vali Loss: 0.4047140 Test Loss: 0.4704233
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 60.93828868865967
Epoch: 13, Steps: 87 | Train Loss: 0.2877236 Vali Loss: 0.4050592 Test Loss: 0.4705511
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 60.921350955963135
Epoch: 14, Steps: 87 | Train Loss: 0.2877836 Vali Loss: 0.4049897 Test Loss: 0.4704704
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 61.61589241027832
Epoch: 15, Steps: 87 | Train Loss: 0.2876797 Vali Loss: 0.4046780 Test Loss: 0.4705041
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 60.84484934806824
Epoch: 16, Steps: 87 | Train Loss: 0.2877119 Vali Loss: 0.4049818 Test Loss: 0.4699900
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 62.58264398574829
Epoch: 17, Steps: 87 | Train Loss: 0.2876921 Vali Loss: 0.4044667 Test Loss: 0.4700105
Validation loss decreased (0.404522 --> 0.404467).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 62.68878102302551
Epoch: 18, Steps: 87 | Train Loss: 0.2876923 Vali Loss: 0.4046565 Test Loss: 0.4701084
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 61.66673183441162
Epoch: 19, Steps: 87 | Train Loss: 0.2876172 Vali Loss: 0.4048567 Test Loss: 0.4700617
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 63.63146233558655
Epoch: 20, Steps: 87 | Train Loss: 0.2876369 Vali Loss: 0.4046574 Test Loss: 0.4701943
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 62.70315432548523
Epoch: 21, Steps: 87 | Train Loss: 0.2875668 Vali Loss: 0.4047900 Test Loss: 0.4700834
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 62.52674388885498
Epoch: 22, Steps: 87 | Train Loss: 0.2876092 Vali Loss: 0.4048441 Test Loss: 0.4701929
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 62.72016215324402
Epoch: 23, Steps: 87 | Train Loss: 0.2876297 Vali Loss: 0.4043381 Test Loss: 0.4700412
Validation loss decreased (0.404467 --> 0.404338).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 62.62692332267761
Epoch: 24, Steps: 87 | Train Loss: 0.2876685 Vali Loss: 0.4050123 Test Loss: 0.4700896
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 63.418328523635864
Epoch: 25, Steps: 87 | Train Loss: 0.2875844 Vali Loss: 0.4049982 Test Loss: 0.4700238
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 62.1159348487854
Epoch: 26, Steps: 87 | Train Loss: 0.2875474 Vali Loss: 0.4043705 Test Loss: 0.4699750
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 62.949445724487305
Epoch: 27, Steps: 87 | Train Loss: 0.2875803 Vali Loss: 0.4047558 Test Loss: 0.4698371
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 62.74327826499939
Epoch: 28, Steps: 87 | Train Loss: 0.2875861 Vali Loss: 0.4046366 Test Loss: 0.4699576
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 61.995113134384155
Epoch: 29, Steps: 87 | Train Loss: 0.2876343 Vali Loss: 0.4044895 Test Loss: 0.4700946
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 63.15491771697998
Epoch: 30, Steps: 87 | Train Loss: 0.2875508 Vali Loss: 0.4047211 Test Loss: 0.4699074
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 64.5213565826416
Epoch: 31, Steps: 87 | Train Loss: 0.2875481 Vali Loss: 0.4045722 Test Loss: 0.4700210
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 64.85852360725403
Epoch: 32, Steps: 87 | Train Loss: 0.2875211 Vali Loss: 0.4050170 Test Loss: 0.4699637
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 64.10970330238342
Epoch: 33, Steps: 87 | Train Loss: 0.2875577 Vali Loss: 0.4048799 Test Loss: 0.4700272
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j720_H5_FITS_custom_ftM_sl360_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4693372845649719, mae:0.3157825469970703, rse:0.5602002739906311, corr:[0.26265818 0.26931095 0.26989275 0.27004662 0.26967657 0.2693744
 0.26957613 0.26992342 0.26988906 0.26958695 0.26946518 0.26952633
 0.2695575  0.26944977 0.2693243  0.26924273 0.26919508 0.26923156
 0.26935235 0.26943672 0.2693616  0.26909852 0.26900548 0.2696826
 0.27063233 0.27081406 0.27065197 0.2704003  0.2701326  0.2699129
 0.2697146  0.26953503 0.2694878  0.26963294 0.26982024 0.26980168
 0.2696516  0.26953718 0.26954716 0.2695953  0.26964623 0.26974487
 0.26983157 0.26977852 0.269537   0.26914445 0.2688146  0.26892653
 0.26928723 0.26934132 0.2692541  0.2691899  0.26924095 0.26939192
 0.2695106  0.26949012 0.26937428 0.2692652  0.26914492 0.26892394
 0.2687267  0.26865098 0.26871568 0.26878724 0.26881638 0.26887375
 0.2689536  0.26897734 0.2688732  0.26868957 0.2685454  0.2686193
 0.26869106 0.26862592 0.26854712 0.26853892 0.26856416 0.26855356
 0.2685306  0.26855135 0.26863313 0.26876107 0.26877102 0.26863587
 0.26847067 0.2683684  0.26833898 0.26833668 0.2683499  0.26830333
 0.2683683  0.26834366 0.26828182 0.268308   0.26840276 0.2684733
 0.26835057 0.26821455 0.26822975 0.26841715 0.26856545 0.26849693
 0.26831612 0.2682233  0.2682849  0.2684419  0.26843318 0.26823103
 0.26802894 0.26797283 0.2680834  0.268264   0.26836303 0.2683644
 0.26854724 0.26864582 0.2685871  0.26847067 0.2685506  0.26869038
 0.26862308 0.2684715  0.26837876 0.26837298 0.26837772 0.2682399
 0.26799056 0.2677714  0.26776537 0.26795757 0.26809373 0.26812932
 0.2681789  0.26833397 0.26859832 0.26887548 0.26903185 0.2687711
 0.2688107  0.26876077 0.2685963  0.26850465 0.26864174 0.26903722
 0.26932463 0.26948702 0.26957682 0.2695561  0.26947752 0.26940748
 0.26940554 0.26950347 0.2696206  0.26973295 0.26970977 0.26959527
 0.269524   0.26955745 0.26968354 0.26984954 0.27001256 0.2701433
 0.27021986 0.2701004  0.26981923 0.26961988 0.26981345 0.27055684
 0.27124673 0.27112564 0.27084357 0.27061284 0.2704673  0.270377
 0.27030656 0.2703043  0.27039096 0.27049    0.270502   0.2704146
 0.2703306  0.2702684  0.2702061  0.2701582  0.27020487 0.27041197
 0.27064684 0.27080145 0.27081722 0.27068746 0.27057475 0.27074504
 0.27109987 0.27109322 0.27092645 0.27080974 0.2707722  0.27075985
 0.2706849  0.27053604 0.27040404 0.27035272 0.2703397  0.2703219
 0.27030754 0.27024174 0.27015343 0.27010384 0.270155   0.27027407
 0.2703157  0.27023867 0.2701287  0.27003402 0.26996395 0.27004376
 0.2702012  0.27021036 0.2701866  0.27019343 0.2701912  0.27013656
 0.27000123 0.26980084 0.26961526 0.26951608 0.2694777  0.26942986
 0.2694068  0.2694068  0.2694321  0.26946688 0.26951107 0.26955092
 0.26952228 0.2694093  0.26935732 0.26931536 0.2693801  0.26959124
 0.2697722  0.26983842 0.26989996 0.26998538 0.27001515 0.2699529
 0.26987913 0.26987678 0.26992863 0.27000675 0.26997805 0.26981762
 0.26961324 0.26943225 0.26927602 0.26916617 0.26918006 0.26932126
 0.26950872 0.2695439  0.26946205 0.2694038  0.26943287 0.26948804
 0.26939654 0.26923656 0.2691389  0.2690738  0.2688778  0.26864544
 0.26853684 0.26867893 0.26895884 0.26927578 0.26939458 0.26932812
 0.2692617  0.2692964  0.26943538 0.26957914 0.26960346 0.26949334
 0.26952198 0.2695103  0.26939583 0.26923874 0.26916537 0.26911822
 0.2689997  0.26905453 0.26922005 0.26928702 0.26919642 0.26903987
 0.26894167 0.26896256 0.26907688 0.26926008 0.2694388  0.26960453
 0.26969847 0.26967084 0.26959807 0.26959848 0.26971605 0.26976168
 0.26995343 0.27002442 0.27002594 0.27003595 0.27022114 0.2705581
 0.2707653  0.2709259  0.27106625 0.2710621  0.27098513 0.27091303
 0.27085215 0.27079856 0.27074957 0.27075064 0.2708206  0.2709612
 0.27109355 0.2711743  0.27124804 0.27135178 0.271475   0.27150276
 0.27147433 0.27131253 0.27107212 0.27090523 0.27097443 0.27139422
 0.27178088 0.27177808 0.27178645 0.2717772  0.27163607 0.27142775
 0.27129218 0.27130967 0.27140132 0.27143785 0.2714161  0.271447
 0.27158275 0.27171406 0.27172053 0.2715976  0.2714936  0.27151066
 0.2715343  0.2714214  0.27119172 0.27099344 0.27095076 0.27122876
 0.27171087 0.27184287 0.27181014 0.27180725 0.27175957 0.2715899
 0.27131405 0.2710614  0.27096066 0.27101523 0.2711257  0.27126816
 0.27142552 0.27155814 0.2716116  0.27156106 0.27143022 0.27128205
 0.27109185 0.27087808 0.2707274  0.27063382 0.27055848 0.27060458
 0.27068633 0.2706258  0.2705768  0.2706169  0.27068263 0.27070603
 0.27066845 0.27059743 0.2705721  0.27064848 0.27078506 0.270902
 0.27095428 0.27089474 0.27079841 0.2707279  0.27069625 0.27064267
 0.27048132 0.27024472 0.27004153 0.26993704 0.26987478 0.26986232
 0.26979703 0.2696771  0.26965752 0.2697849  0.26994056 0.27003303
 0.27007008 0.27008188 0.27012965 0.27023315 0.27032313 0.27036473
 0.27036184 0.27024502 0.27006844 0.26997513 0.27006277 0.27025858
 0.27037022 0.27016598 0.26987427 0.26981142 0.2699364  0.2700497
 0.2699003  0.26968795 0.26964137 0.26979217 0.26990905 0.26986188
 0.2697439  0.26968572 0.26968554 0.2696847  0.26965252 0.26962927
 0.26966444 0.269715   0.26975927 0.26982346 0.26985267 0.26983348
 0.26985788 0.26967916 0.26944628 0.26936644 0.26953283 0.2697172
 0.26958707 0.26936215 0.2693318  0.26950195 0.2696787  0.26965773
 0.26945555 0.26924023 0.26914042 0.26917434 0.26929358 0.2694681
 0.26965317 0.2697573  0.2697655  0.26975438 0.26982507 0.27000612
 0.2701398  0.26999244 0.26968962 0.26955137 0.26978308 0.27028728
 0.27063176 0.27068242 0.270694   0.270718   0.27084976 0.27099976
 0.27100343 0.27080515 0.2705112  0.27028966 0.27026188 0.27040362
 0.27057105 0.2706432  0.27061793 0.27063787 0.2707482  0.27089944
 0.27092993 0.27074698 0.27048624 0.27037337 0.2705494  0.27102676
 0.27138785 0.27114582 0.27095893 0.27091977 0.27085483 0.2706829
 0.27043453 0.27023363 0.27016556 0.27020383 0.27025086 0.2702664
 0.2702218  0.27011234 0.27003288 0.27005103 0.27018872 0.27035952
 0.2703988  0.27029654 0.27015284 0.27006978 0.27007604 0.27021483
 0.27034226 0.2701248  0.26994392 0.2700108  0.2701532  0.27017695
 0.27002588 0.26979557 0.26959506 0.26946    0.26936778 0.26933357
 0.26931047 0.2692588  0.269206   0.2692296  0.26933873 0.26940623
 0.26926467 0.26894975 0.26870516 0.26870325 0.26884162 0.26905283
 0.2691829  0.26912317 0.26912063 0.26921767 0.26924434 0.26912946
 0.2689802  0.268923   0.26894993 0.26893747 0.26882192 0.26873922
 0.26879698 0.268867   0.26879713 0.268559   0.26831245 0.26824105
 0.2683102  0.26836184 0.26832515 0.26823446 0.26815727 0.26817307
 0.26820707 0.26822728 0.26832476 0.26849213 0.2685903  0.2684972
 0.2682427  0.26797596 0.26783097 0.267796   0.2677503  0.2676364
 0.2675122  0.2673826  0.2672496  0.26718602 0.26728556 0.26751304
 0.26765856 0.26759693 0.26743641 0.26734966 0.26731798 0.26723936
 0.2670644  0.26700592 0.26720324 0.26741892 0.2673781  0.26710516
 0.26689902 0.26695135 0.26714164 0.26722333 0.2670904  0.26688063
 0.2667818  0.26678988 0.26680073 0.26679215 0.26686952 0.2670472
 0.2671406  0.26695988 0.26668218 0.26658693 0.266811   0.26708078
 0.26709682 0.26704815 0.26716504 0.26736993 0.2674882  0.26743534
 0.26727602 0.26711327 0.26700836 0.26697466 0.26697934 0.26696107
 0.26685053 0.26664886 0.26655537 0.26673558 0.26703677 0.2672296
 0.26711947 0.26680517 0.2666322  0.26676333 0.26715246 0.26770517
 0.26809594 0.26829427 0.26854017 0.26868817 0.2687078  0.26863647
 0.26855    0.26845485 0.2682997  0.2681021  0.26798192 0.26796347
 0.26790294 0.26768503 0.26746053 0.26750132 0.26789516 0.26836073
 0.26853916 0.26843405 0.26828802 0.26831323 0.2685265  0.2691282
 0.26969907 0.26966715 0.2697204  0.2696699  0.26937646 0.26905057
 0.2689129  0.26890582 0.2688052  0.2685933  0.26849955 0.2686319
 0.26876488 0.26865453 0.26846242 0.2684711  0.26871413 0.26894134
 0.2689494  0.26889247 0.26899302 0.2691597  0.26922584 0.26941463
 0.26975057 0.2698402  0.26990908 0.26984242 0.26952046 0.26916465
 0.26901686 0.2690539  0.2691287  0.26925302 0.26946834 0.2696137
 0.26946104 0.26908407 0.26904103 0.26936993 0.26954487 0.26925156
 0.2689623  0.26907334 0.26883554 0.26758608 0.26713172 0.26977408]
