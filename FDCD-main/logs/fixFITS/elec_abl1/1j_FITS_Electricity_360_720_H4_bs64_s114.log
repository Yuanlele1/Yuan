Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_360_j720_H4', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_360_j720_H4_FITS_custom_ftM_sl360_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17333
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=74, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  674993664.0
params:  16650.0
Trainable parameters:  16650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7968951
	speed: 1.0433s/iter; left time: 13980.8106s
Epoch: 1 cost time: 140.20709109306335
Epoch: 1, Steps: 135 | Train Loss: 1.0201109 Vali Loss: 0.5609092 Test Loss: 0.6649024
Validation loss decreased (inf --> 0.560909).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5105659
	speed: 2.2881s/iter; left time: 30354.0452s
Epoch: 2 cost time: 132.62893319129944
Epoch: 2, Steps: 135 | Train Loss: 0.5723589 Vali Loss: 0.3961590 Test Loss: 0.4776265
Validation loss decreased (0.560909 --> 0.396159).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3970408
	speed: 2.3127s/iter; left time: 30368.2450s
Epoch: 3 cost time: 137.1824643611908
Epoch: 3, Steps: 135 | Train Loss: 0.4359030 Vali Loss: 0.3115366 Test Loss: 0.3788096
Validation loss decreased (0.396159 --> 0.311537).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3517166
	speed: 2.3829s/iter; left time: 30968.1244s
Epoch: 4 cost time: 138.42005228996277
Epoch: 4, Steps: 135 | Train Loss: 0.3565035 Vali Loss: 0.2605686 Test Loss: 0.3173934
Validation loss decreased (0.311537 --> 0.260569).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2864883
	speed: 2.2905s/iter; left time: 29458.2933s
Epoch: 5 cost time: 135.66572952270508
Epoch: 5, Steps: 135 | Train Loss: 0.3075794 Vali Loss: 0.2302239 Test Loss: 0.2794480
Validation loss decreased (0.260569 --> 0.230224).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2754310
	speed: 2.3196s/iter; left time: 29518.8791s
Epoch: 6 cost time: 135.0732936859131
Epoch: 6, Steps: 135 | Train Loss: 0.2780450 Vali Loss: 0.2126983 Test Loss: 0.2565502
Validation loss decreased (0.230224 --> 0.212698).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2552933
	speed: 2.4570s/iter; left time: 30936.0423s
Epoch: 7 cost time: 148.58817863464355
Epoch: 7, Steps: 135 | Train Loss: 0.2606135 Vali Loss: 0.2028753 Test Loss: 0.2430016
Validation loss decreased (0.212698 --> 0.202875).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2496345
	speed: 2.3564s/iter; left time: 29351.3996s
Epoch: 8 cost time: 137.52480959892273
Epoch: 8, Steps: 135 | Train Loss: 0.2505861 Vali Loss: 0.1974447 Test Loss: 0.2351747
Validation loss decreased (0.202875 --> 0.197445).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2531160
	speed: 2.2737s/iter; left time: 28014.2057s
Epoch: 9 cost time: 133.5263373851776
Epoch: 9, Steps: 135 | Train Loss: 0.2449800 Vali Loss: 0.1949041 Test Loss: 0.2307230
Validation loss decreased (0.197445 --> 0.194904).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2481712
	speed: 2.2797s/iter; left time: 27780.1608s
Epoch: 10 cost time: 136.7802619934082
Epoch: 10, Steps: 135 | Train Loss: 0.2418400 Vali Loss: 0.1936929 Test Loss: 0.2282321
Validation loss decreased (0.194904 --> 0.193693).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2429290
	speed: 2.2698s/iter; left time: 27352.9604s
Epoch: 11 cost time: 132.20047235488892
Epoch: 11, Steps: 135 | Train Loss: 0.2402483 Vali Loss: 0.1931210 Test Loss: 0.2268177
Validation loss decreased (0.193693 --> 0.193121).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2471383
	speed: 2.2906s/iter; left time: 27295.1310s
Epoch: 12 cost time: 133.5249171257019
Epoch: 12, Steps: 135 | Train Loss: 0.2394066 Vali Loss: 0.1925452 Test Loss: 0.2260317
Validation loss decreased (0.193121 --> 0.192545).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2282046
	speed: 2.2923s/iter; left time: 27005.9494s
Epoch: 13 cost time: 137.65182185173035
Epoch: 13, Steps: 135 | Train Loss: 0.2389616 Vali Loss: 0.1928806 Test Loss: 0.2255659
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2289803
	speed: 2.3080s/iter; left time: 26879.5378s
Epoch: 14 cost time: 135.8929374217987
Epoch: 14, Steps: 135 | Train Loss: 0.2386157 Vali Loss: 0.1928743 Test Loss: 0.2252915
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2552584
	speed: 2.2619s/iter; left time: 26037.3046s
Epoch: 15 cost time: 140.92651867866516
Epoch: 15, Steps: 135 | Train Loss: 0.2384834 Vali Loss: 0.1927890 Test Loss: 0.2251365
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2403636
	speed: 2.2725s/iter; left time: 25852.2463s
Epoch: 16 cost time: 131.14263796806335
Epoch: 16, Steps: 135 | Train Loss: 0.2382884 Vali Loss: 0.1927264 Test Loss: 0.2250276
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2189587
	speed: 2.3114s/iter; left time: 25982.9754s
Epoch: 17 cost time: 133.85785007476807
Epoch: 17, Steps: 135 | Train Loss: 0.2383132 Vali Loss: 0.1928107 Test Loss: 0.2249718
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2414167
	speed: 2.2519s/iter; left time: 25009.2972s
Epoch: 18 cost time: 133.88530492782593
Epoch: 18, Steps: 135 | Train Loss: 0.2383287 Vali Loss: 0.1930555 Test Loss: 0.2249096
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2405558
	speed: 2.2486s/iter; left time: 24669.6538s
Epoch: 19 cost time: 133.44861769676208
Epoch: 19, Steps: 135 | Train Loss: 0.2382238 Vali Loss: 0.1926178 Test Loss: 0.2248777
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2352124
	speed: 2.2742s/iter; left time: 24643.0722s
Epoch: 20 cost time: 126.36517095565796
Epoch: 20, Steps: 135 | Train Loss: 0.2382130 Vali Loss: 0.1926199 Test Loss: 0.2248544
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2363019
	speed: 2.1550s/iter; left time: 23060.7792s
Epoch: 21 cost time: 131.10553884506226
Epoch: 21, Steps: 135 | Train Loss: 0.2381694 Vali Loss: 0.1928113 Test Loss: 0.2248334
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2380603
	speed: 2.2173s/iter; left time: 23427.6526s
Epoch: 22 cost time: 130.54279112815857
Epoch: 22, Steps: 135 | Train Loss: 0.2381437 Vali Loss: 0.1927204 Test Loss: 0.2248144
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2408219
	speed: 2.2091s/iter; left time: 23043.6388s
Epoch: 23 cost time: 130.0625922679901
Epoch: 23, Steps: 135 | Train Loss: 0.2381053 Vali Loss: 0.1926342 Test Loss: 0.2247894
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2249087
	speed: 2.2289s/iter; left time: 22949.1218s
Epoch: 24 cost time: 133.52408003807068
Epoch: 24, Steps: 135 | Train Loss: 0.2381938 Vali Loss: 0.1927856 Test Loss: 0.2247757
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2421096
	speed: 2.2257s/iter; left time: 22614.9828s
Epoch: 25 cost time: 127.43808817863464
Epoch: 25, Steps: 135 | Train Loss: 0.2380454 Vali Loss: 0.1927968 Test Loss: 0.2247562
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2549155
	speed: 2.0143s/iter; left time: 20195.0050s
Epoch: 26 cost time: 120.08700656890869
Epoch: 26, Steps: 135 | Train Loss: 0.2380996 Vali Loss: 0.1927017 Test Loss: 0.2247485
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2409600
	speed: 2.4276s/iter; left time: 24011.4850s
Epoch: 27 cost time: 174.56347107887268
Epoch: 27, Steps: 135 | Train Loss: 0.2380644 Vali Loss: 0.1927861 Test Loss: 0.2247582
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2272781
	speed: 2.6343s/iter; left time: 25700.4764s
Epoch: 28 cost time: 138.56258606910706
Epoch: 28, Steps: 135 | Train Loss: 0.2380377 Vali Loss: 0.1926074 Test Loss: 0.2247284
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2411190
	speed: 2.5070s/iter; left time: 24119.8748s
Epoch: 29 cost time: 149.99738478660583
Epoch: 29, Steps: 135 | Train Loss: 0.2380348 Vali Loss: 0.1928843 Test Loss: 0.2247354
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2313832
	speed: 2.5014s/iter; left time: 23728.6476s
Epoch: 30 cost time: 148.70889616012573
Epoch: 30, Steps: 135 | Train Loss: 0.2380374 Vali Loss: 0.1928339 Test Loss: 0.2247126
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2199741
	speed: 2.5648s/iter; left time: 23983.7361s
Epoch: 31 cost time: 155.39645314216614
Epoch: 31, Steps: 135 | Train Loss: 0.2380550 Vali Loss: 0.1924075 Test Loss: 0.2247153
Validation loss decreased (0.192545 --> 0.192408).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2429276
	speed: 2.6612s/iter; left time: 24525.2400s
Epoch: 32 cost time: 172.6220588684082
Epoch: 32, Steps: 135 | Train Loss: 0.2380803 Vali Loss: 0.1928029 Test Loss: 0.2247027
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2443481
	speed: 2.8440s/iter; left time: 25826.3861s
Epoch: 33 cost time: 151.08991074562073
Epoch: 33, Steps: 135 | Train Loss: 0.2379578 Vali Loss: 0.1926947 Test Loss: 0.2247033
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2308985
	speed: 2.5522s/iter; left time: 22832.2072s
Epoch: 34 cost time: 145.33266425132751
Epoch: 34, Steps: 135 | Train Loss: 0.2380336 Vali Loss: 0.1926595 Test Loss: 0.2246942
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2313777
	speed: 2.4629s/iter; left time: 21700.2468s
Epoch: 35 cost time: 154.10085201263428
Epoch: 35, Steps: 135 | Train Loss: 0.2380240 Vali Loss: 0.1925887 Test Loss: 0.2246922
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2261992
	speed: 2.6478s/iter; left time: 22972.2775s
Epoch: 36 cost time: 162.87506914138794
Epoch: 36, Steps: 135 | Train Loss: 0.2379462 Vali Loss: 0.1924402 Test Loss: 0.2246920
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2275019
	speed: 2.6950s/iter; left time: 23018.3676s
Epoch: 37 cost time: 162.73020243644714
Epoch: 37, Steps: 135 | Train Loss: 0.2379395 Vali Loss: 0.1924540 Test Loss: 0.2246994
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2233958
	speed: 2.8062s/iter; left time: 23589.3175s
Epoch: 38 cost time: 160.00005078315735
Epoch: 38, Steps: 135 | Train Loss: 0.2379911 Vali Loss: 0.1926000 Test Loss: 0.2246860
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2449364
	speed: 2.5737s/iter; left time: 21286.9540s
Epoch: 39 cost time: 150.58465957641602
Epoch: 39, Steps: 135 | Train Loss: 0.2379720 Vali Loss: 0.1927668 Test Loss: 0.2246779
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2355283
	speed: 2.5181s/iter; left time: 20487.6315s
Epoch: 40 cost time: 147.7229790687561
Epoch: 40, Steps: 135 | Train Loss: 0.2379833 Vali Loss: 0.1927006 Test Loss: 0.2246770
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2400099
	speed: 2.4942s/iter; left time: 19956.1954s
Epoch: 41 cost time: 138.29573559761047
Epoch: 41, Steps: 135 | Train Loss: 0.2379357 Vali Loss: 0.1922189 Test Loss: 0.2246619
Validation loss decreased (0.192408 --> 0.192219).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2341551
	speed: 2.6281s/iter; left time: 20672.2956s
Epoch: 42 cost time: 156.5519347190857
Epoch: 42, Steps: 135 | Train Loss: 0.2380024 Vali Loss: 0.1926775 Test Loss: 0.2246601
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2545815
	speed: 2.5342s/iter; left time: 19592.0940s
Epoch: 43 cost time: 140.24991726875305
Epoch: 43, Steps: 135 | Train Loss: 0.2379197 Vali Loss: 0.1924828 Test Loss: 0.2246585
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2290462
	speed: 2.4327s/iter; left time: 18478.5307s
Epoch: 44 cost time: 148.3885622024536
Epoch: 44, Steps: 135 | Train Loss: 0.2380029 Vali Loss: 0.1929339 Test Loss: 0.2246552
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2224079
	speed: 2.4765s/iter; left time: 18477.1078s
Epoch: 45 cost time: 135.1604359149933
Epoch: 45, Steps: 135 | Train Loss: 0.2379198 Vali Loss: 0.1923314 Test Loss: 0.2246572
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2377641
	speed: 2.2499s/iter; left time: 16483.0890s
Epoch: 46 cost time: 132.4832420349121
Epoch: 46, Steps: 135 | Train Loss: 0.2379341 Vali Loss: 0.1922759 Test Loss: 0.2246566
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2367594
	speed: 2.2422s/iter; left time: 16123.8896s
Epoch: 47 cost time: 135.76275277137756
Epoch: 47, Steps: 135 | Train Loss: 0.2379467 Vali Loss: 0.1924531 Test Loss: 0.2246529
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2357325
	speed: 2.2381s/iter; left time: 15791.9693s
Epoch: 48 cost time: 133.61332178115845
Epoch: 48, Steps: 135 | Train Loss: 0.2379490 Vali Loss: 0.1925350 Test Loss: 0.2246496
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2367723
	speed: 2.2864s/iter; left time: 15824.1348s
Epoch: 49 cost time: 134.97740769386292
Epoch: 49, Steps: 135 | Train Loss: 0.2379363 Vali Loss: 0.1922556 Test Loss: 0.2246452
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2440816
	speed: 2.3238s/iter; left time: 15769.5753s
Epoch: 50 cost time: 135.97580909729004
Epoch: 50, Steps: 135 | Train Loss: 0.2379450 Vali Loss: 0.1925175 Test Loss: 0.2246391
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2303161
	speed: 2.0719s/iter; left time: 13780.5026s
Epoch: 51 cost time: 119.59598922729492
Epoch: 51, Steps: 135 | Train Loss: 0.2378716 Vali Loss: 0.1922431 Test Loss: 0.2246460
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2359876
	speed: 1.9782s/iter; left time: 12889.7707s
Epoch: 52 cost time: 116.33153772354126
Epoch: 52, Steps: 135 | Train Loss: 0.2378441 Vali Loss: 0.1924555 Test Loss: 0.2246453
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2267232
	speed: 1.9892s/iter; left time: 12692.9879s
Epoch: 53 cost time: 114.458172082901
Epoch: 53, Steps: 135 | Train Loss: 0.2379109 Vali Loss: 0.1924360 Test Loss: 0.2246421
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2388842
	speed: 2.0191s/iter; left time: 12611.0834s
Epoch: 54 cost time: 122.84460473060608
Epoch: 54, Steps: 135 | Train Loss: 0.2378760 Vali Loss: 0.1923724 Test Loss: 0.2246450
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2453428
	speed: 2.0341s/iter; left time: 12430.1149s
Epoch: 55 cost time: 120.17667531967163
Epoch: 55, Steps: 135 | Train Loss: 0.2378683 Vali Loss: 0.1922820 Test Loss: 0.2246406
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2242737
	speed: 2.0295s/iter; left time: 12128.2976s
Epoch: 56 cost time: 117.43490862846375
Epoch: 56, Steps: 135 | Train Loss: 0.2379304 Vali Loss: 0.1923420 Test Loss: 0.2246374
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2306623
	speed: 1.9854s/iter; left time: 11596.9026s
Epoch: 57 cost time: 111.68881464004517
Epoch: 57, Steps: 135 | Train Loss: 0.2378759 Vali Loss: 0.1924270 Test Loss: 0.2246363
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2539429
	speed: 1.9551s/iter; left time: 11155.6113s
Epoch: 58 cost time: 115.16228246688843
Epoch: 58, Steps: 135 | Train Loss: 0.2379128 Vali Loss: 0.1925171 Test Loss: 0.2246333
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2314030
	speed: 1.9221s/iter; left time: 10707.9197s
Epoch: 59 cost time: 112.87802958488464
Epoch: 59, Steps: 135 | Train Loss: 0.2378972 Vali Loss: 0.1924973 Test Loss: 0.2246365
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2311997
	speed: 1.7850s/iter; left time: 9703.3572s
Epoch: 60 cost time: 100.43583512306213
Epoch: 60, Steps: 135 | Train Loss: 0.2378962 Vali Loss: 0.1927304 Test Loss: 0.2246339
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2182670
	speed: 1.6953s/iter; left time: 8986.9172s
Epoch: 61 cost time: 98.70667505264282
Epoch: 61, Steps: 135 | Train Loss: 0.2378845 Vali Loss: 0.1923854 Test Loss: 0.2246326
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_360_j720_H4_FITS_custom_ftM_sl360_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.22307921946048737, mae:0.3170222043991089, rse:0.4711475670337677, corr:[0.44512522 0.4481433  0.44807646 0.4489282  0.44989535 0.45020816
 0.44993708 0.44940108 0.44887042 0.44854793 0.44848564 0.4485028
 0.4484059  0.44821486 0.44813558 0.44828215 0.44848198 0.44854212
 0.44845706 0.4483079  0.4482749  0.44843614 0.44864237 0.44871622
 0.4486822  0.44880515 0.44890735 0.4488955  0.44877017 0.44853747
 0.44822344 0.44793335 0.44775614 0.44770962 0.44770685 0.44768772
 0.44764334 0.44761303 0.44765842 0.44776747 0.44784278 0.44782892
 0.4477505  0.44762468 0.44755566 0.44759995 0.44766986 0.4476933
 0.44769007 0.44777414 0.44782054 0.4477545  0.4476225  0.44744918
 0.44727668 0.44714335 0.44709563 0.44712332 0.44714466 0.44714218
 0.44712114 0.44710603 0.44713286 0.4471887  0.4472185  0.4471943
 0.44712478 0.44698128 0.4468816  0.44687834 0.44690308 0.44687927
 0.44683015 0.4468573  0.44686344 0.44680813 0.44670862 0.44658512
 0.44645205 0.44634262 0.44629753 0.4463247  0.44637024 0.44640163
 0.4464027  0.4463813  0.44638422 0.44642413 0.4464651  0.44646612
 0.44642007 0.44630572 0.4461975  0.44620168 0.4462452  0.4462638
 0.4462624  0.44631225 0.4463345  0.44629782 0.44621566 0.44610173
 0.44598117 0.44589016 0.445834   0.44582713 0.4458559  0.44588417
 0.44588557 0.4458757  0.44587374 0.4458891  0.44589758 0.4458936
 0.44583178 0.44568104 0.44557106 0.44557694 0.4456459  0.4457212
 0.44577953 0.4458469  0.44589525 0.44592416 0.44591856 0.4458745
 0.44581032 0.44575053 0.44572708 0.44574317 0.4457703  0.44579867
 0.44582668 0.44583103 0.4458161  0.4457917  0.44576156 0.44574657
 0.44577333 0.44579875 0.44582018 0.445883   0.4460452  0.4462817
 0.446514   0.44667122 0.4467533  0.4468156  0.44684872 0.44684303
 0.4467995  0.44674593 0.4467178  0.44671923 0.44672716 0.44672978
 0.44673163 0.44671997 0.44672    0.4467076  0.44667575 0.44664717
 0.44665813 0.44661188 0.44649348 0.4463588  0.44624263 0.44613388
 0.4459841  0.44588315 0.44573227 0.4455876  0.44552124 0.44548202
 0.44539297 0.44524497 0.44510192 0.44501612 0.4449845  0.44497648
 0.44496882 0.44494087 0.4449153  0.44489086 0.44483852 0.44477153
 0.44468403 0.4444823  0.44427088 0.44416392 0.4441416  0.44413874
 0.44412357 0.4441539  0.4441214  0.44404072 0.44398174 0.4439549
 0.44390932 0.44382247 0.4437248  0.4436515  0.4436005  0.44358784
 0.443598   0.44358894 0.4435707  0.4435537  0.44351694 0.44346306
 0.4433831  0.44319025 0.4429958  0.44292998 0.4429534  0.44300273
 0.4430505  0.44311935 0.44311422 0.44304842 0.44299757 0.44299015
 0.4429752  0.44293144 0.4428897  0.44284686 0.4428312  0.44282615
 0.4428128  0.44277272 0.44272494 0.4426782  0.44263625 0.44259107
 0.4425463  0.44239748 0.4422391  0.44216144 0.44217116 0.44221213
 0.44225532 0.44231078 0.44228253 0.44219488 0.44213873 0.44214424
 0.44215542 0.44211668 0.44205746 0.44200358 0.4419737  0.44198105
 0.44198984 0.44197112 0.4419269  0.44188377 0.4418495  0.44182485
 0.44178894 0.4416899  0.44157878 0.44153547 0.44157213 0.44165236
 0.4417375  0.44181263 0.44180027 0.44172764 0.44166273 0.44164297
 0.44162956 0.44159847 0.4415354  0.4414531  0.44139487 0.44136825
 0.4413726  0.44137722 0.4413799  0.44135842 0.44131377 0.44127348
 0.44120497 0.44105542 0.44093043 0.44090897 0.4409812  0.44111815
 0.44128868 0.44144943 0.44152156 0.4415263  0.44152465 0.44154194
 0.441557   0.44154248 0.44149974 0.44145003 0.4414036  0.44138488
 0.44139448 0.44139808 0.44138664 0.44135237 0.44129148 0.4412409
 0.44124347 0.44125447 0.44125587 0.44129446 0.44143215 0.44167286
 0.4419556  0.4421609  0.442257   0.44229814 0.44231707 0.4423402
 0.44235313 0.44234046 0.44231647 0.4422795  0.4422274  0.44216564
 0.4421192  0.44208717 0.44208002 0.44206047 0.44200805 0.44195932
 0.4419502  0.44188392 0.4417214  0.44154775 0.4413758  0.44123343
 0.4411213  0.44109884 0.44101074 0.4408609  0.44074017 0.44065824
 0.44058436 0.4404845  0.440362   0.44023222 0.4401055  0.44000468
 0.4399412  0.43989363 0.43987453 0.4398576  0.43981346 0.43975946
 0.43971133 0.4395963  0.43946967 0.43943328 0.43942988 0.43941897
 0.43939924 0.43944794 0.4394394  0.43935478 0.4392632  0.43920925
 0.43916622 0.4391027  0.4390191  0.43891275 0.43878385 0.43868032
 0.43862453 0.4385799  0.43854517 0.43851614 0.43847647 0.43842348
 0.4383644  0.43824023 0.43812892 0.43810645 0.4381252  0.4381472
 0.438192   0.43829894 0.43837807 0.4383647  0.4383191  0.43827984
 0.4382458  0.43821874 0.43817854 0.4381166  0.43803555 0.437956
 0.4379055  0.43786258 0.43783328 0.43780655 0.43776858 0.43772784
 0.43768936 0.4375816  0.4374597  0.4374227  0.4374404  0.43747154
 0.43752897 0.43763152 0.43769774 0.43769175 0.43765754 0.4376425
 0.437621   0.43757015 0.43751377 0.43747678 0.4374459  0.4374023
 0.43736655 0.43731725 0.4372789  0.4372601  0.43725234 0.4372498
 0.4372277  0.43713215 0.43703985 0.43702796 0.43706968 0.43711507
 0.43717277 0.4372679  0.43733916 0.4373551  0.43734366 0.43732154
 0.43729195 0.43723848 0.4371754  0.4371067  0.43704596 0.43699613
 0.4369487  0.4368961  0.43685466 0.43682635 0.43680096 0.43679583
 0.4367578  0.436643   0.43656424 0.43658817 0.43668532 0.43679988
 0.4369493  0.4371146  0.43723938 0.43730888 0.43732744 0.43732023
 0.4373093  0.43728092 0.43723917 0.4371825  0.4371112  0.4370518
 0.43702102 0.43697265 0.43692794 0.436901   0.43689686 0.4369208
 0.43698227 0.43702745 0.43705383 0.43714473 0.43735182 0.43763164
 0.4379248  0.43813694 0.43826762 0.43836394 0.43842152 0.43844476
 0.43843272 0.43839782 0.4383617  0.4383214  0.4382646  0.43819883
 0.43812746 0.4380496  0.4380062  0.43798465 0.4379688  0.4379755
 0.43799436 0.4379183  0.4377694  0.43762964 0.43750143 0.4373611
 0.43719304 0.43710372 0.43701926 0.4369491  0.4369048  0.43685496
 0.43674666 0.4365761  0.43639767 0.4362558  0.43615246 0.4360708
 0.43600646 0.43593296 0.43587568 0.43584788 0.4358262  0.43581936
 0.43579608 0.43565995 0.43549722 0.43541732 0.43539408 0.43537349
 0.43534204 0.43536574 0.43537018 0.43532383 0.43527403 0.4352387
 0.43516743 0.43503013 0.43484336 0.43467185 0.43453747 0.43445963
 0.43441296 0.4343574  0.4343055  0.43426612 0.43423626 0.43422213
 0.4341866  0.43406886 0.43400592 0.43402076 0.43407568 0.43412563
 0.43416503 0.43421626 0.43424183 0.4342016  0.4341514  0.4341077
 0.4340634  0.43397933 0.43382853 0.43362966 0.43345076 0.4333276
 0.43325076 0.4331556  0.43305779 0.43296382 0.43286875 0.43277377
 0.43270242 0.43261364 0.43253753 0.43253252 0.43258783 0.43264347
 0.4327134  0.4328234  0.4329021  0.43290633 0.43289554 0.43288952
 0.432862   0.4327522  0.43257323 0.4323839  0.43222708 0.43214262
 0.4321114  0.43206078 0.43199912 0.4319457  0.43192762 0.43194866
 0.43198317 0.43195948 0.43191373 0.43191576 0.43195948 0.43201536
 0.43208674 0.43219206 0.432254   0.4322639  0.4322633  0.43225873
 0.43222454 0.43212086 0.43194926 0.43175346 0.4315824  0.43147835
 0.4314443  0.43143833 0.4314284  0.43140727 0.4313699  0.43137127
 0.4313949  0.43136096 0.43132612 0.4313721  0.43147835 0.43162358
 0.43181127 0.43198925 0.4320986  0.43212676 0.43213043 0.432133
 0.43211246 0.43202597 0.43187767 0.43169826 0.4315274  0.43142742
 0.43141612 0.4314318  0.43145466 0.43144736 0.43142065 0.43143165
 0.4315333  0.43166682 0.4317693  0.4318852  0.43208864 0.43237144
 0.4326769  0.4328789  0.43295902 0.43296722 0.4329353  0.4328979
 0.4328435  0.43273893 0.43258312 0.43238574 0.43219802 0.43207803
 0.43204498 0.43207154 0.43213022 0.43217963 0.43220472 0.4322478
 0.4323421  0.43238243 0.43235603 0.432279   0.4321501  0.43196434
 0.43172497 0.4315844  0.43144065 0.4312283  0.43100268 0.43080547
 0.43063003 0.43040726 0.43010268 0.42975593 0.42948547 0.42938355
 0.4294697  0.42962176 0.42975637 0.4298381  0.42987964 0.42993987
 0.4300157  0.43000847 0.4299464  0.4299028  0.4298213  0.4296864
 0.42956796 0.42956027 0.4294666  0.42915842 0.42877677 0.42853132
 0.42845502 0.42835045 0.42800868 0.4275812  0.42744723 0.4277769
 0.42832237 0.42872122 0.42883167 0.42876926 0.4285994  0.42421806
 0.42420024 0.42825103 0.42734712 0.42153072 0.421999   0.42749017]
