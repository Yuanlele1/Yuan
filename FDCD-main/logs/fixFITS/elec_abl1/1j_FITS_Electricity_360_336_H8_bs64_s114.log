Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=138, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_360_j336_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_360_j336_H8_FITS_custom_ftM_sl360_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17717
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=138, out_features=266, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1508258304.0
params:  36974.0
Trainable parameters:  36974
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5841610
	speed: 0.5912s/iter; left time: 8099.7970s
Epoch: 1 cost time: 83.81545495986938
Epoch: 1, Steps: 138 | Train Loss: 0.7577005 Vali Loss: 0.4326314 Test Loss: 0.5063379
Validation loss decreased (inf --> 0.432631).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3795447
	speed: 1.4536s/iter; left time: 19714.7397s
Epoch: 2 cost time: 85.00601291656494
Epoch: 2, Steps: 138 | Train Loss: 0.4208499 Vali Loss: 0.2841112 Test Loss: 0.3360431
Validation loss decreased (0.432631 --> 0.284111).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2810593
	speed: 1.4087s/iter; left time: 18911.9304s
Epoch: 3 cost time: 82.27105283737183
Epoch: 3, Steps: 138 | Train Loss: 0.2953464 Vali Loss: 0.2112538 Test Loss: 0.2511885
Validation loss decreased (0.284111 --> 0.211254).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2297677
	speed: 1.5115s/iter; left time: 20083.2045s
Epoch: 4 cost time: 89.11348724365234
Epoch: 4, Steps: 138 | Train Loss: 0.2336740 Vali Loss: 0.1765770 Test Loss: 0.2098750
Validation loss decreased (0.211254 --> 0.176577).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.1841746
	speed: 1.3877s/iter; left time: 18247.1346s
Epoch: 5 cost time: 81.37912487983704
Epoch: 5, Steps: 138 | Train Loss: 0.2044175 Vali Loss: 0.1608637 Test Loss: 0.1904729
Validation loss decreased (0.176577 --> 0.160864).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.1999972
	speed: 1.4602s/iter; left time: 18998.3476s
Epoch: 6 cost time: 81.65310859680176
Epoch: 6, Steps: 138 | Train Loss: 0.1912674 Vali Loss: 0.1542892 Test Loss: 0.1817728
Validation loss decreased (0.160864 --> 0.154289).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1968038
	speed: 1.3970s/iter; left time: 17984.0562s
Epoch: 7 cost time: 86.3163378238678
Epoch: 7, Steps: 138 | Train Loss: 0.1854582 Vali Loss: 0.1515754 Test Loss: 0.1779385
Validation loss decreased (0.154289 --> 0.151575).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1751666
	speed: 1.5095s/iter; left time: 19223.3036s
Epoch: 8 cost time: 86.18512010574341
Epoch: 8, Steps: 138 | Train Loss: 0.1830643 Vali Loss: 0.1503099 Test Loss: 0.1762526
Validation loss decreased (0.151575 --> 0.150310).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.1774072
	speed: 1.3976s/iter; left time: 17605.4048s
Epoch: 9 cost time: 80.91265416145325
Epoch: 9, Steps: 138 | Train Loss: 0.1820528 Vali Loss: 0.1498882 Test Loss: 0.1754664
Validation loss decreased (0.150310 --> 0.149888).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.1847082
	speed: 1.4309s/iter; left time: 17827.4115s
Epoch: 10 cost time: 80.75760912895203
Epoch: 10, Steps: 138 | Train Loss: 0.1815630 Vali Loss: 0.1498728 Test Loss: 0.1750595
Validation loss decreased (0.149888 --> 0.149873).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1846137
	speed: 1.3289s/iter; left time: 16373.0441s
Epoch: 11 cost time: 82.2165584564209
Epoch: 11, Steps: 138 | Train Loss: 0.1812708 Vali Loss: 0.1496487 Test Loss: 0.1748630
Validation loss decreased (0.149873 --> 0.149649).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.1767829
	speed: 1.4340s/iter; left time: 17470.4134s
Epoch: 12 cost time: 80.36192202568054
Epoch: 12, Steps: 138 | Train Loss: 0.1811442 Vali Loss: 0.1496335 Test Loss: 0.1747367
Validation loss decreased (0.149649 --> 0.149634).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1771518
	speed: 1.3872s/iter; left time: 16708.2520s
Epoch: 13 cost time: 83.34971857070923
Epoch: 13, Steps: 138 | Train Loss: 0.1810923 Vali Loss: 0.1496005 Test Loss: 0.1746417
Validation loss decreased (0.149634 --> 0.149600).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.1782421
	speed: 1.4823s/iter; left time: 17649.1547s
Epoch: 14 cost time: 89.7444498538971
Epoch: 14, Steps: 138 | Train Loss: 0.1809988 Vali Loss: 0.1496362 Test Loss: 0.1745886
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.1862044
	speed: 1.4182s/iter; left time: 16690.8695s
Epoch: 15 cost time: 82.97963452339172
Epoch: 15, Steps: 138 | Train Loss: 0.1809718 Vali Loss: 0.1496646 Test Loss: 0.1745260
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.1701385
	speed: 1.5111s/iter; left time: 17575.6475s
Epoch: 16 cost time: 85.96858835220337
Epoch: 16, Steps: 138 | Train Loss: 0.1808458 Vali Loss: 0.1498550 Test Loss: 0.1745136
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1701773
	speed: 1.4514s/iter; left time: 16681.0165s
Epoch: 17 cost time: 87.40978240966797
Epoch: 17, Steps: 138 | Train Loss: 0.1808660 Vali Loss: 0.1495347 Test Loss: 0.1744866
Validation loss decreased (0.149600 --> 0.149535).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1783417
	speed: 1.4005s/iter; left time: 15902.5497s
Epoch: 18 cost time: 80.27722263336182
Epoch: 18, Steps: 138 | Train Loss: 0.1808209 Vali Loss: 0.1496005 Test Loss: 0.1744689
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.1718008
	speed: 1.4073s/iter; left time: 15786.1703s
Epoch: 19 cost time: 82.84187865257263
Epoch: 19, Steps: 138 | Train Loss: 0.1807937 Vali Loss: 0.1494308 Test Loss: 0.1744345
Validation loss decreased (0.149535 --> 0.149431).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1836862
	speed: 1.3916s/iter; left time: 15417.0888s
Epoch: 20 cost time: 79.22336483001709
Epoch: 20, Steps: 138 | Train Loss: 0.1807440 Vali Loss: 0.1493407 Test Loss: 0.1744224
Validation loss decreased (0.149431 --> 0.149341).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1807233
	speed: 1.4342s/iter; left time: 15691.4917s
Epoch: 21 cost time: 86.82376909255981
Epoch: 21, Steps: 138 | Train Loss: 0.1806948 Vali Loss: 0.1495260 Test Loss: 0.1744277
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1786527
	speed: 1.4886s/iter; left time: 16081.6062s
Epoch: 22 cost time: 86.4608941078186
Epoch: 22, Steps: 138 | Train Loss: 0.1806521 Vali Loss: 0.1491728 Test Loss: 0.1744079
Validation loss decreased (0.149341 --> 0.149173).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1738916
	speed: 1.4816s/iter; left time: 15801.3199s
Epoch: 23 cost time: 90.39940977096558
Epoch: 23, Steps: 138 | Train Loss: 0.1807351 Vali Loss: 0.1494415 Test Loss: 0.1743870
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1761278
	speed: 1.4437s/iter; left time: 15198.1006s
Epoch: 24 cost time: 80.53925609588623
Epoch: 24, Steps: 138 | Train Loss: 0.1806936 Vali Loss: 0.1489936 Test Loss: 0.1743726
Validation loss decreased (0.149173 --> 0.148994).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1966711
	speed: 1.3915s/iter; left time: 14456.2508s
Epoch: 25 cost time: 83.68719410896301
Epoch: 25, Steps: 138 | Train Loss: 0.1805683 Vali Loss: 0.1492845 Test Loss: 0.1743427
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1742001
	speed: 1.4525s/iter; left time: 14889.9089s
Epoch: 26 cost time: 80.15881419181824
Epoch: 26, Steps: 138 | Train Loss: 0.1806129 Vali Loss: 0.1489831 Test Loss: 0.1743459
Validation loss decreased (0.148994 --> 0.148983).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.1793728
	speed: 1.3689s/iter; left time: 13843.5674s
Epoch: 27 cost time: 84.91855978965759
Epoch: 27, Steps: 138 | Train Loss: 0.1805815 Vali Loss: 0.1492672 Test Loss: 0.1743461
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1856836
	speed: 1.4720s/iter; left time: 14682.9838s
Epoch: 28 cost time: 86.61540460586548
Epoch: 28, Steps: 138 | Train Loss: 0.1805818 Vali Loss: 0.1493745 Test Loss: 0.1743348
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1761985
	speed: 1.5096s/iter; left time: 14849.8668s
Epoch: 29 cost time: 87.94654536247253
Epoch: 29, Steps: 138 | Train Loss: 0.1806239 Vali Loss: 0.1489784 Test Loss: 0.1743491
Validation loss decreased (0.148983 --> 0.148978).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1905887
	speed: 1.4693s/iter; left time: 14251.0064s
Epoch: 30 cost time: 83.5569281578064
Epoch: 30, Steps: 138 | Train Loss: 0.1805075 Vali Loss: 0.1493488 Test Loss: 0.1743139
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1818734
	speed: 1.3961s/iter; left time: 13348.2442s
Epoch: 31 cost time: 87.6931734085083
Epoch: 31, Steps: 138 | Train Loss: 0.1805745 Vali Loss: 0.1491782 Test Loss: 0.1743160
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1859109
	speed: 1.5434s/iter; left time: 14543.0787s
Epoch: 32 cost time: 87.80953812599182
Epoch: 32, Steps: 138 | Train Loss: 0.1805284 Vali Loss: 0.1492308 Test Loss: 0.1743221
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.1732322
	speed: 1.3720s/iter; left time: 12739.4214s
Epoch: 33 cost time: 77.47910523414612
Epoch: 33, Steps: 138 | Train Loss: 0.1805824 Vali Loss: 0.1489956 Test Loss: 0.1743218
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1956795
	speed: 1.3983s/iter; left time: 12790.1939s
Epoch: 34 cost time: 84.26987981796265
Epoch: 34, Steps: 138 | Train Loss: 0.1805704 Vali Loss: 0.1490565 Test Loss: 0.1743112
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2016293
	speed: 1.3998s/iter; left time: 12611.0879s
Epoch: 35 cost time: 82.34809994697571
Epoch: 35, Steps: 138 | Train Loss: 0.1805023 Vali Loss: 0.1493456 Test Loss: 0.1743010
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1867345
	speed: 1.3699s/iter; left time: 12152.5173s
Epoch: 36 cost time: 79.37840270996094
Epoch: 36, Steps: 138 | Train Loss: 0.1805195 Vali Loss: 0.1490845 Test Loss: 0.1742941
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1689946
	speed: 1.3368s/iter; left time: 11674.6592s
Epoch: 37 cost time: 78.70890140533447
Epoch: 37, Steps: 138 | Train Loss: 0.1805178 Vali Loss: 0.1491866 Test Loss: 0.1743179
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1959053
	speed: 1.3791s/iter; left time: 11853.0716s
Epoch: 38 cost time: 76.24632930755615
Epoch: 38, Steps: 138 | Train Loss: 0.1805110 Vali Loss: 0.1493630 Test Loss: 0.1742989
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1840423
	speed: 1.3708s/iter; left time: 11592.6368s
Epoch: 39 cost time: 85.59350991249084
Epoch: 39, Steps: 138 | Train Loss: 0.1805217 Vali Loss: 0.1491355 Test Loss: 0.1742952
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.1942620
	speed: 1.3826s/iter; left time: 11502.0345s
Epoch: 40 cost time: 78.51525712013245
Epoch: 40, Steps: 138 | Train Loss: 0.1805305 Vali Loss: 0.1494104 Test Loss: 0.1742932
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1774541
	speed: 1.3933s/iter; left time: 11398.2871s
Epoch: 41 cost time: 83.47766709327698
Epoch: 41, Steps: 138 | Train Loss: 0.1804167 Vali Loss: 0.1493082 Test Loss: 0.1742955
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.1936134
	speed: 1.3587s/iter; left time: 10928.3482s
Epoch: 42 cost time: 75.42851185798645
Epoch: 42, Steps: 138 | Train Loss: 0.1804957 Vali Loss: 0.1492085 Test Loss: 0.1742901
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1791621
	speed: 1.3063s/iter; left time: 10326.1146s
Epoch: 43 cost time: 80.86657309532166
Epoch: 43, Steps: 138 | Train Loss: 0.1805324 Vali Loss: 0.1488440 Test Loss: 0.1742872
Validation loss decreased (0.148978 --> 0.148844).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1696647
	speed: 1.3257s/iter; left time: 10296.6192s
Epoch: 44 cost time: 74.99946355819702
Epoch: 44, Steps: 138 | Train Loss: 0.1804410 Vali Loss: 0.1492009 Test Loss: 0.1742938
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.1917543
	speed: 1.3485s/iter; left time: 10287.5464s
Epoch: 45 cost time: 80.80406403541565
Epoch: 45, Steps: 138 | Train Loss: 0.1804675 Vali Loss: 0.1491953 Test Loss: 0.1742934
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1805434
	speed: 1.3586s/iter; left time: 10177.1311s
Epoch: 46 cost time: 81.40134334564209
Epoch: 46, Steps: 138 | Train Loss: 0.1804677 Vali Loss: 0.1490806 Test Loss: 0.1742944
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.1815234
	speed: 1.3501s/iter; left time: 9927.1684s
Epoch: 47 cost time: 81.02407383918762
Epoch: 47, Steps: 138 | Train Loss: 0.1804425 Vali Loss: 0.1490794 Test Loss: 0.1742834
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.1829641
	speed: 1.3176s/iter; left time: 9506.2006s
Epoch: 48 cost time: 74.08497619628906
Epoch: 48, Steps: 138 | Train Loss: 0.1804974 Vali Loss: 0.1491803 Test Loss: 0.1742793
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.1862606
	speed: 1.3244s/iter; left time: 9372.5823s
Epoch: 49 cost time: 82.26863431930542
Epoch: 49, Steps: 138 | Train Loss: 0.1804738 Vali Loss: 0.1488734 Test Loss: 0.1742841
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.1811294
	speed: 1.3473s/iter; left time: 9349.2202s
Epoch: 50 cost time: 77.35740637779236
Epoch: 50, Steps: 138 | Train Loss: 0.1804274 Vali Loss: 0.1492087 Test Loss: 0.1742862
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.1846460
	speed: 1.3430s/iter; left time: 9133.8053s
Epoch: 51 cost time: 81.45403671264648
Epoch: 51, Steps: 138 | Train Loss: 0.1805198 Vali Loss: 0.1495087 Test Loss: 0.1742795
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.1782348
	speed: 1.3686s/iter; left time: 9118.9146s
Epoch: 52 cost time: 81.17264747619629
Epoch: 52, Steps: 138 | Train Loss: 0.1804745 Vali Loss: 0.1490414 Test Loss: 0.1742767
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.1798968
	speed: 1.3753s/iter; left time: 8973.7270s
Epoch: 53 cost time: 81.26653838157654
Epoch: 53, Steps: 138 | Train Loss: 0.1804614 Vali Loss: 0.1493144 Test Loss: 0.1742845
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.1928232
	speed: 1.3482s/iter; left time: 8611.0541s
Epoch: 54 cost time: 77.00272154808044
Epoch: 54, Steps: 138 | Train Loss: 0.1804790 Vali Loss: 0.1493621 Test Loss: 0.1742783
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.1658663
	speed: 1.2362s/iter; left time: 7724.8611s
Epoch: 55 cost time: 70.84532427787781
Epoch: 55, Steps: 138 | Train Loss: 0.1804654 Vali Loss: 0.1491793 Test Loss: 0.1742786
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.1791555
	speed: 1.2060s/iter; left time: 7369.5726s
Epoch: 56 cost time: 68.02269291877747
Epoch: 56, Steps: 138 | Train Loss: 0.1804350 Vali Loss: 0.1490172 Test Loss: 0.1742751
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.1906095
	speed: 1.1992s/iter; left time: 7162.8622s
Epoch: 57 cost time: 72.93216896057129
Epoch: 57, Steps: 138 | Train Loss: 0.1804632 Vali Loss: 0.1489910 Test Loss: 0.1742757
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.1832599
	speed: 1.2133s/iter; left time: 7079.6139s
Epoch: 58 cost time: 71.41644835472107
Epoch: 58, Steps: 138 | Train Loss: 0.1804853 Vali Loss: 0.1489059 Test Loss: 0.1742729
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.1742303
	speed: 1.2479s/iter; left time: 7109.2645s
Epoch: 59 cost time: 78.20228576660156
Epoch: 59, Steps: 138 | Train Loss: 0.1804387 Vali Loss: 0.1491148 Test Loss: 0.1742713
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.1827264
	speed: 1.3196s/iter; left time: 7335.7670s
Epoch: 60 cost time: 73.84949398040771
Epoch: 60, Steps: 138 | Train Loss: 0.1803807 Vali Loss: 0.1490265 Test Loss: 0.1742761
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.1894349
	speed: 1.1379s/iter; left time: 6168.5783s
Epoch: 61 cost time: 68.5119116306305
Epoch: 61, Steps: 138 | Train Loss: 0.1804742 Vali Loss: 0.1492482 Test Loss: 0.1742729
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.1792859
	speed: 1.1746s/iter; left time: 6205.6529s
Epoch: 62 cost time: 67.34053111076355
Epoch: 62, Steps: 138 | Train Loss: 0.1804669 Vali Loss: 0.1490200 Test Loss: 0.1742755
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.1866763
	speed: 1.1534s/iter; left time: 5934.0670s
Epoch: 63 cost time: 66.17463970184326
Epoch: 63, Steps: 138 | Train Loss: 0.1804343 Vali Loss: 0.1492740 Test Loss: 0.1742731
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_360_j336_H8_FITS_custom_ftM_sl360_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.17214269936084747, mae:0.26735806465148926, rse:0.4129387140274048, corr:[0.45954195 0.45922354 0.4617051  0.4615924  0.4631565  0.4628948
 0.4630969  0.4632038  0.4626512  0.46247298 0.46223566 0.46194977
 0.4618848  0.4618645  0.4618117  0.4618168  0.4618019  0.46180716
 0.46180233 0.46152082 0.46144804 0.4616268  0.46154258 0.46185598
 0.4622407  0.46216333 0.46227756 0.4623433  0.4620088  0.46194103
 0.4618133  0.4614848  0.4613716  0.46127006 0.46111408 0.46109393
 0.4610983  0.46108252 0.46111134 0.46113127 0.4611381  0.46114874
 0.46102396 0.4608439  0.46079412 0.4607508  0.46075535 0.46098432
 0.4610554  0.46116596 0.46128118 0.46104994 0.4609376  0.46088904
 0.46065208 0.46055964 0.46054035 0.4604391  0.46040404 0.46040675
 0.46039113 0.46041155 0.46046898 0.46047708 0.46044827 0.46044043
 0.46033454 0.46018186 0.46004125 0.45991576 0.45996293 0.46004805
 0.4600468  0.4601547  0.46008715 0.45993504 0.4599317  0.45981696
 0.4597143  0.459714   0.4596066  0.459533   0.45952088 0.45950413
 0.4595251  0.4595376  0.45954254 0.45950758 0.45946068 0.45944583
 0.45947218 0.45950118 0.45946884 0.45947143 0.4595143  0.4595455
 0.45964527 0.45972618 0.45964608 0.45964882 0.45963603 0.45951542
 0.45948386 0.45947543 0.45944193 0.45942214 0.45935455 0.45935908
 0.45941076 0.45941263 0.4594059  0.45940432 0.45938608 0.45935404
 0.45933267 0.45924193 0.45912796 0.45912883 0.45912465 0.45919192
 0.45939144 0.45945063 0.45944804 0.45948458 0.45940885 0.45938498
 0.45939144 0.4593108  0.45931843 0.45933318 0.45929056 0.4593134
 0.45932022 0.45930523 0.4593013  0.45929587 0.45931768 0.45934924
 0.45934954 0.4592567  0.4591733  0.4592133  0.45920387 0.45942503
 0.45978943 0.45989692 0.45999736 0.46005425 0.46001154 0.46005923
 0.46001974 0.45995283 0.45999137 0.45996347 0.45991278 0.45992148
 0.45994285 0.45994505 0.4599227  0.45989978 0.45993972 0.45997664
 0.45991534 0.4597726  0.45968005 0.45957488 0.45945844 0.45942625
 0.4592806  0.4591241  0.45909163 0.45895788 0.45880294 0.4587156
 0.45857403 0.45844257 0.45838168 0.4582783  0.45813632 0.4580294
 0.45794168 0.4578574  0.45779228 0.45769486 0.4575608  0.4574407
 0.4572855  0.4570769  0.45693442 0.45690542 0.4568952  0.4569309
 0.45693228 0.45693657 0.456946   0.45694792 0.4569014  0.45680347
 0.45673856 0.45668685 0.45657906 0.45650822 0.45643386 0.4563722
 0.45634407 0.45629954 0.4563031  0.4563042  0.45624092 0.45622614
 0.4561926  0.4560136  0.45586872 0.45586124 0.4558869  0.4559929
 0.45613495 0.45625365 0.45628038 0.45626342 0.4561954  0.45613426
 0.45611703 0.45608073 0.45603362 0.45597193 0.45590162 0.4558495
 0.45578468 0.4557184  0.45573902 0.45573607 0.4557035  0.45570198
 0.45559937 0.45539176 0.45530266 0.45531568 0.4553233  0.45543733
 0.45555893 0.45561108 0.45570228 0.45578745 0.455717   0.4556877
 0.45569134 0.45561537 0.45556507 0.45549205 0.45535764 0.4552804
 0.45523375 0.45518485 0.4551369  0.45511603 0.45509803 0.45506138
 0.45505524 0.45506158 0.45508343 0.45514593 0.45518312 0.45528206
 0.45536906 0.45541933 0.4555013  0.45553043 0.45547533 0.45547336
 0.45542815 0.4553567  0.4552635  0.45514408 0.4550545  0.45495567
 0.45493773 0.4549547  0.45491683 0.45494506 0.45497096 0.4549317
 0.45495033 0.45488104 0.4547779  0.45479548 0.45489967 0.45508417
 0.45521513 0.4553449  0.45545596 0.45544463 0.45544764 0.45541054
 0.45528638 0.45521384 0.45511302 0.45498377 0.4548503  0.45478892
 0.45479524 0.45473242 0.4547779  0.4549109  0.45487943 0.4549703
 0.45508787 0.45493224 0.45484903 0.45494086 0.45505488 0.45535955
 0.45571384 0.45588052 0.4559075  0.4558997  0.45586112 0.45570177
 0.45555225 0.4554555  0.4553239  0.45520034 0.4551775  0.45521837
 0.455203   0.45533895 0.45549828 0.45533225 0.4554205  0.45548195
 0.45515165 0.4551017  0.45471132 0.45482323 0.45462757 0.4553831 ]
