Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_360_j336_H4', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_360_j336_H4_FITS_custom_ftM_sl360_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17717
val 2297
test 4925
Model(
  (freq_upsampler): Linear(in_features=74, out_features=143, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  434793216.0
params:  10725.0
Trainable parameters:  10725
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6737897
	speed: 0.7846s/iter; left time: 10749.8234s
Epoch: 1 cost time: 109.3018126487732
Epoch: 1, Steps: 138 | Train Loss: 0.8376900 Vali Loss: 0.4799229 Test Loss: 0.5578548
Validation loss decreased (inf --> 0.479923).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4242581
	speed: 1.8002s/iter; left time: 24416.5354s
Epoch: 2 cost time: 105.18604826927185
Epoch: 2, Steps: 138 | Train Loss: 0.4661207 Vali Loss: 0.3176811 Test Loss: 0.3729386
Validation loss decreased (0.479923 --> 0.317681).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3096320
	speed: 1.8174s/iter; left time: 24398.0274s
Epoch: 3 cost time: 106.89714932441711
Epoch: 3, Steps: 138 | Train Loss: 0.3287003 Vali Loss: 0.2366871 Test Loss: 0.2794987
Validation loss decreased (0.317681 --> 0.236687).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2475987
	speed: 1.8140s/iter; left time: 24102.4705s
Epoch: 4 cost time: 110.71158504486084
Epoch: 4, Steps: 138 | Train Loss: 0.2589299 Vali Loss: 0.1963851 Test Loss: 0.2321117
Validation loss decreased (0.236687 --> 0.196385).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2113661
	speed: 1.8938s/iter; left time: 24902.0941s
Epoch: 5 cost time: 112.67975926399231
Epoch: 5, Steps: 138 | Train Loss: 0.2243512 Vali Loss: 0.1770805 Test Loss: 0.2089378
Validation loss decreased (0.196385 --> 0.177081).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2123298
	speed: 1.7734s/iter; left time: 23073.6678s
Epoch: 6 cost time: 106.16241717338562
Epoch: 6, Steps: 138 | Train Loss: 0.2077966 Vali Loss: 0.1684578 Test Loss: 0.1979801
Validation loss decreased (0.177081 --> 0.168458).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.1939537
	speed: 1.8595s/iter; left time: 23937.8471s
Epoch: 7 cost time: 109.22952222824097
Epoch: 7, Steps: 138 | Train Loss: 0.2003699 Vali Loss: 0.1646022 Test Loss: 0.1929555
Validation loss decreased (0.168458 --> 0.164602).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.1956657
	speed: 1.8182s/iter; left time: 23154.5620s
Epoch: 8 cost time: 114.37072491645813
Epoch: 8, Steps: 138 | Train Loss: 0.1970621 Vali Loss: 0.1633038 Test Loss: 0.1906482
Validation loss decreased (0.164602 --> 0.163304).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2054853
	speed: 1.9342s/iter; left time: 24364.8616s
Epoch: 9 cost time: 112.88537859916687
Epoch: 9, Steps: 138 | Train Loss: 0.1955395 Vali Loss: 0.1625621 Test Loss: 0.1895707
Validation loss decreased (0.163304 --> 0.162562).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2042400
	speed: 1.7593s/iter; left time: 21919.6109s
Epoch: 10 cost time: 109.30007743835449
Epoch: 10, Steps: 138 | Train Loss: 0.1948584 Vali Loss: 0.1624004 Test Loss: 0.1890152
Validation loss decreased (0.162562 --> 0.162400).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.1944724
	speed: 1.7946s/iter; left time: 22110.9053s
Epoch: 11 cost time: 107.42697501182556
Epoch: 11, Steps: 138 | Train Loss: 0.1945220 Vali Loss: 0.1623848 Test Loss: 0.1887504
Validation loss decreased (0.162400 --> 0.162385).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2013669
	speed: 1.7628s/iter; left time: 21476.0000s
Epoch: 12 cost time: 103.17521500587463
Epoch: 12, Steps: 138 | Train Loss: 0.1943654 Vali Loss: 0.1618859 Test Loss: 0.1885461
Validation loss decreased (0.162385 --> 0.161886).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.1972771
	speed: 1.8205s/iter; left time: 21927.9255s
Epoch: 13 cost time: 108.0101580619812
Epoch: 13, Steps: 138 | Train Loss: 0.1941890 Vali Loss: 0.1622343 Test Loss: 0.1884695
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2023380
	speed: 1.6980s/iter; left time: 20217.9225s
Epoch: 14 cost time: 101.46761775016785
Epoch: 14, Steps: 138 | Train Loss: 0.1940781 Vali Loss: 0.1618696 Test Loss: 0.1883841
Validation loss decreased (0.161886 --> 0.161870).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2080541
	speed: 1.7331s/iter; left time: 20396.6862s
Epoch: 15 cost time: 103.2568986415863
Epoch: 15, Steps: 138 | Train Loss: 0.1939859 Vali Loss: 0.1619204 Test Loss: 0.1883122
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2010847
	speed: 1.6838s/iter; left time: 19583.9490s
Epoch: 16 cost time: 105.0321753025055
Epoch: 16, Steps: 138 | Train Loss: 0.1939511 Vali Loss: 0.1619525 Test Loss: 0.1882962
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.1876145
	speed: 1.8400s/iter; left time: 21147.0704s
Epoch: 17 cost time: 104.52854132652283
Epoch: 17, Steps: 138 | Train Loss: 0.1939130 Vali Loss: 0.1618446 Test Loss: 0.1882353
Validation loss decreased (0.161870 --> 0.161845).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.1921504
	speed: 1.7713s/iter; left time: 20113.1939s
Epoch: 18 cost time: 104.92358326911926
Epoch: 18, Steps: 138 | Train Loss: 0.1938186 Vali Loss: 0.1618091 Test Loss: 0.1882043
Validation loss decreased (0.161845 --> 0.161809).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2007662
	speed: 1.7571s/iter; left time: 19709.8165s
Epoch: 19 cost time: 108.82952904701233
Epoch: 19, Steps: 138 | Train Loss: 0.1938057 Vali Loss: 0.1617743 Test Loss: 0.1881959
Validation loss decreased (0.161809 --> 0.161774).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.1945291
	speed: 1.7877s/iter; left time: 19805.9062s
Epoch: 20 cost time: 103.14198422431946
Epoch: 20, Steps: 138 | Train Loss: 0.1937708 Vali Loss: 0.1618587 Test Loss: 0.1881390
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.1824263
	speed: 1.7751s/iter; left time: 19421.8903s
Epoch: 21 cost time: 104.83893394470215
Epoch: 21, Steps: 138 | Train Loss: 0.1937053 Vali Loss: 0.1617714 Test Loss: 0.1881328
Validation loss decreased (0.161774 --> 0.161771).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.1919482
	speed: 1.7912s/iter; left time: 19349.7943s
Epoch: 22 cost time: 103.55200505256653
Epoch: 22, Steps: 138 | Train Loss: 0.1937790 Vali Loss: 0.1618538 Test Loss: 0.1881146
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.1780671
	speed: 1.7891s/iter; left time: 19080.9890s
Epoch: 23 cost time: 106.56682014465332
Epoch: 23, Steps: 138 | Train Loss: 0.1936604 Vali Loss: 0.1615588 Test Loss: 0.1880972
Validation loss decreased (0.161771 --> 0.161559).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.1976770
	speed: 1.7720s/iter; left time: 18653.7501s
Epoch: 24 cost time: 106.20239686965942
Epoch: 24, Steps: 138 | Train Loss: 0.1936771 Vali Loss: 0.1618295 Test Loss: 0.1880888
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.1878616
	speed: 1.7485s/iter; left time: 18165.2627s
Epoch: 25 cost time: 103.76525068283081
Epoch: 25, Steps: 138 | Train Loss: 0.1936367 Vali Loss: 0.1618667 Test Loss: 0.1880777
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.1854140
	speed: 1.7402s/iter; left time: 17838.4768s
Epoch: 26 cost time: 100.14136123657227
Epoch: 26, Steps: 138 | Train Loss: 0.1935937 Vali Loss: 0.1613534 Test Loss: 0.1880637
Validation loss decreased (0.161559 --> 0.161353).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2010491
	speed: 1.7423s/iter; left time: 17619.5605s
Epoch: 27 cost time: 108.85603904724121
Epoch: 27, Steps: 138 | Train Loss: 0.1937204 Vali Loss: 0.1618479 Test Loss: 0.1880493
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.1798064
	speed: 1.7759s/iter; left time: 17714.2480s
Epoch: 28 cost time: 107.24956893920898
Epoch: 28, Steps: 138 | Train Loss: 0.1935943 Vali Loss: 0.1616043 Test Loss: 0.1880383
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.1852657
	speed: 1.7637s/iter; left time: 17349.1262s
Epoch: 29 cost time: 105.59940671920776
Epoch: 29, Steps: 138 | Train Loss: 0.1936292 Vali Loss: 0.1617792 Test Loss: 0.1880559
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.1891771
	speed: 1.6941s/iter; left time: 16431.2662s
Epoch: 30 cost time: 103.82829117774963
Epoch: 30, Steps: 138 | Train Loss: 0.1936028 Vali Loss: 0.1617471 Test Loss: 0.1880268
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.1992813
	speed: 1.8302s/iter; left time: 17498.0906s
Epoch: 31 cost time: 106.86081790924072
Epoch: 31, Steps: 138 | Train Loss: 0.1935415 Vali Loss: 0.1615661 Test Loss: 0.1880157
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.1841656
	speed: 1.7197s/iter; left time: 16204.8069s
Epoch: 32 cost time: 102.17592692375183
Epoch: 32, Steps: 138 | Train Loss: 0.1935600 Vali Loss: 0.1615470 Test Loss: 0.1880198
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2006034
	speed: 1.5818s/iter; left time: 14686.5924s
Epoch: 33 cost time: 95.87292408943176
Epoch: 33, Steps: 138 | Train Loss: 0.1936095 Vali Loss: 0.1615252 Test Loss: 0.1880123
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1923608
	speed: 1.6286s/iter; left time: 14897.1847s
Epoch: 34 cost time: 114.75830292701721
Epoch: 34, Steps: 138 | Train Loss: 0.1936197 Vali Loss: 0.1619076 Test Loss: 0.1880134
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.1838894
	speed: 2.3390s/iter; left time: 21072.2968s
Epoch: 35 cost time: 138.75255584716797
Epoch: 35, Steps: 138 | Train Loss: 0.1935333 Vali Loss: 0.1618602 Test Loss: 0.1880088
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.1909118
	speed: 2.3523s/iter; left time: 20866.9493s
Epoch: 36 cost time: 134.65581130981445
Epoch: 36, Steps: 138 | Train Loss: 0.1935098 Vali Loss: 0.1618352 Test Loss: 0.1879986
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.1851361
	speed: 2.3347s/iter; left time: 20388.8789s
Epoch: 37 cost time: 137.15928506851196
Epoch: 37, Steps: 138 | Train Loss: 0.1935442 Vali Loss: 0.1616967 Test Loss: 0.1879991
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.1912788
	speed: 2.2909s/iter; left time: 19690.0709s
Epoch: 38 cost time: 135.27361369132996
Epoch: 38, Steps: 138 | Train Loss: 0.1935791 Vali Loss: 0.1616988 Test Loss: 0.1879991
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1895242
	speed: 2.2687s/iter; left time: 19186.5207s
Epoch: 39 cost time: 130.3219428062439
Epoch: 39, Steps: 138 | Train Loss: 0.1935607 Vali Loss: 0.1616191 Test Loss: 0.1879911
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2151502
	speed: 2.2505s/iter; left time: 18722.2189s
Epoch: 40 cost time: 134.7976815700531
Epoch: 40, Steps: 138 | Train Loss: 0.1935182 Vali Loss: 0.1613865 Test Loss: 0.1879994
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.1984164
	speed: 2.3151s/iter; left time: 18939.5125s
Epoch: 41 cost time: 135.2947964668274
Epoch: 41, Steps: 138 | Train Loss: 0.1934993 Vali Loss: 0.1618916 Test Loss: 0.1879921
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2067152
	speed: 2.2784s/iter; left time: 18325.2781s
Epoch: 42 cost time: 133.82580423355103
Epoch: 42, Steps: 138 | Train Loss: 0.1934637 Vali Loss: 0.1616047 Test Loss: 0.1879857
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.1828481
	speed: 2.2949s/iter; left time: 18141.4831s
Epoch: 43 cost time: 134.8363742828369
Epoch: 43, Steps: 138 | Train Loss: 0.1935660 Vali Loss: 0.1615926 Test Loss: 0.1879885
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.1971295
	speed: 2.2540s/iter; left time: 17506.6298s
Epoch: 44 cost time: 132.81100702285767
Epoch: 44, Steps: 138 | Train Loss: 0.1935627 Vali Loss: 0.1614796 Test Loss: 0.1879830
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2014496
	speed: 2.1140s/iter; left time: 16127.4235s
Epoch: 45 cost time: 123.91909527778625
Epoch: 45, Steps: 138 | Train Loss: 0.1935319 Vali Loss: 0.1616364 Test Loss: 0.1879833
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.1942119
	speed: 2.0931s/iter; left time: 15679.2213s
Epoch: 46 cost time: 129.6785614490509
Epoch: 46, Steps: 138 | Train Loss: 0.1934863 Vali Loss: 0.1615703 Test Loss: 0.1879773
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_360_j336_H4_FITS_custom_ftM_sl360_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4925
mse:0.1859411597251892, mae:0.28807970881462097, rse:0.4291696846485138, corr:[0.45742077 0.45899037 0.4582374  0.46033335 0.46237049 0.46275946
 0.4621863  0.46159554 0.46123213 0.4610221  0.4608739  0.4607243
 0.46058422 0.46045142 0.4604161  0.46052137 0.4606093  0.46058387
 0.4604685  0.46028373 0.4602108  0.4603818  0.46064812 0.46080184
 0.46082473 0.4610283  0.4612645  0.4613393  0.46116337 0.46081397
 0.46043307 0.4601734  0.46006086 0.46003208 0.46000442 0.4599745
 0.45996153 0.4599514  0.45996726 0.46001032 0.46003532 0.46001238
 0.45992073 0.45975935 0.4596416  0.45965162 0.45972723 0.45978487
 0.45980307 0.45991698 0.46003494 0.46001533 0.45983785 0.45956844
 0.45934272 0.45924    0.45924407 0.45928344 0.45930362 0.45931232
 0.45932323 0.45932204 0.45931816 0.45932433 0.4593309  0.45932582
 0.45922068 0.45899567 0.4588178  0.4587685  0.4588039  0.45881617
 0.45877483 0.45878705 0.45881742 0.458795   0.45868143 0.45850426
 0.45833164 0.45822752 0.45820203 0.45823509 0.4582762  0.4583118
 0.45834154 0.45833412 0.4583124  0.4583073  0.45831993 0.45833138
 0.45834336 0.4583293  0.4583158  0.45837587 0.45848167 0.45855182
 0.45854744 0.45856243 0.45857996 0.45857015 0.45850676 0.45839408
 0.45826793 0.45818034 0.45813498 0.45813474 0.45815215 0.45817354
 0.45818305 0.45816165 0.4581291  0.45811573 0.4581329  0.45817214
 0.4581835  0.4580651  0.4579233  0.4579227  0.4580419  0.45818412
 0.45826203 0.45830783 0.45834914 0.4583806  0.45835894 0.4582785
 0.45819026 0.45814994 0.45817736 0.45822647 0.45823637 0.45821947
 0.45820215 0.45816702 0.45811218 0.45806912 0.45807484 0.45814064
 0.458179   0.45805794 0.45786154 0.45779774 0.45795473 0.4582517
 0.45851347 0.45866278 0.45876235 0.45886979 0.45894042 0.45894235
 0.45888156 0.45882043 0.45880145 0.45882514 0.45885277 0.4588653
 0.45885855 0.4588227  0.45877826 0.45873022 0.45870453 0.45872855
 0.45874915 0.4586182  0.45844913 0.45831966 0.45828223 0.45829916
 0.4582048  0.45806792 0.4578852  0.45773867 0.45767376 0.4576322
 0.4575579  0.45744598 0.45730653 0.45719114 0.4571038  0.45702636
 0.4569401  0.4568133  0.4566637  0.45651975 0.4563757  0.45624733
 0.4561422  0.4559923  0.45582667 0.45573333 0.45575324 0.4558211
 0.45584455 0.45585585 0.45580703 0.455749   0.45573843 0.45576346
 0.45575172 0.45567226 0.455546   0.45543262 0.45533767 0.45528567
 0.45525634 0.45519093 0.45510268 0.45501673 0.45496327 0.45496306
 0.45496234 0.4548662  0.45472422 0.45465708 0.45470786 0.45484176
 0.45495966 0.45504183 0.45503515 0.45499673 0.45500478 0.4550592
 0.4550776  0.45502663 0.45492798 0.45481    0.45473686 0.45470634
 0.4547056  0.45469832 0.45467165 0.45461845 0.4545689  0.4545537
 0.45452952 0.45439598 0.45422107 0.45414737 0.4541994  0.45433182
 0.454452   0.454526   0.45449257 0.4544297  0.45444116 0.454534
 0.454609   0.4545641  0.45440897 0.45421013 0.45405605 0.45400336
 0.45401695 0.45402858 0.453993   0.45392644 0.4538647  0.45385614
 0.45391834 0.4539794  0.45399123 0.4540126  0.4540941  0.4542286
 0.45433268 0.45439333 0.45437104 0.45433238 0.4543419  0.45441613
 0.4544664  0.4544218  0.45425937 0.4540271  0.45382085 0.45371422
 0.4537293  0.45378265 0.4538028  0.4537496  0.4536729  0.45366657
 0.45374578 0.45375183 0.4536823  0.4536458  0.4537241  0.4539209
 0.45413408 0.45428115 0.45429674 0.45424706 0.45423293 0.45426536
 0.45428172 0.45420447 0.45401946 0.45378458 0.45356497 0.45347255
 0.4535456  0.45368427 0.45377502 0.45376548 0.45371234 0.4537452
 0.45385882 0.45387283 0.4537453  0.4536517  0.4537617  0.45410421
 0.45450297 0.4547089  0.45469815 0.45459998 0.45453537 0.4545359
 0.45452282 0.45438197 0.45408118 0.45375893 0.45363617 0.45383072
 0.45424    0.45457858 0.45464143 0.45445842 0.4542361  0.45421386
 0.4543726  0.45420346 0.45338717 0.45236564 0.45284942 0.45347568]
