Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_180_j720_H6', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_180_j720_H6_FITS_custom_ftM_sl180_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17513
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=58, out_features=290, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  691100160.0
params:  17110.0
Trainable parameters:  17110
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6880940
	speed: 0.9034s/iter; left time: 12196.1374s
Epoch: 1 cost time: 119.46900343894958
Epoch: 1, Steps: 136 | Train Loss: 1.0308479 Vali Loss: 0.5034437 Test Loss: 0.5577198
Validation loss decreased (inf --> 0.503444).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4067653
	speed: 2.0505s/iter; left time: 27405.5052s
Epoch: 2 cost time: 119.5507299900055
Epoch: 2, Steps: 136 | Train Loss: 0.4618199 Vali Loss: 0.3155554 Test Loss: 0.3555475
Validation loss decreased (0.503444 --> 0.315555).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3196129
	speed: 2.0426s/iter; left time: 27022.2127s
Epoch: 3 cost time: 117.83161759376526
Epoch: 3, Steps: 136 | Train Loss: 0.3390342 Vali Loss: 0.2587517 Test Loss: 0.2946569
Validation loss decreased (0.315555 --> 0.258752).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2779757
	speed: 2.0032s/iter; left time: 26227.6448s
Epoch: 4 cost time: 112.11742639541626
Epoch: 4, Steps: 136 | Train Loss: 0.2968266 Vali Loss: 0.2339846 Test Loss: 0.2692844
Validation loss decreased (0.258752 --> 0.233985).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2768024
	speed: 1.9025s/iter; left time: 24650.7736s
Epoch: 5 cost time: 111.73629570007324
Epoch: 5, Steps: 136 | Train Loss: 0.2764845 Vali Loss: 0.2201573 Test Loss: 0.2554997
Validation loss decreased (0.233985 --> 0.220157).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2613792
	speed: 1.9110s/iter; left time: 24500.5969s
Epoch: 6 cost time: 106.36550784111023
Epoch: 6, Steps: 136 | Train Loss: 0.2647323 Vali Loss: 0.2119240 Test Loss: 0.2471527
Validation loss decreased (0.220157 --> 0.211924).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2534440
	speed: 1.8752s/iter; left time: 23786.5764s
Epoch: 7 cost time: 109.86273574829102
Epoch: 7, Steps: 136 | Train Loss: 0.2575884 Vali Loss: 0.2070059 Test Loss: 0.2418108
Validation loss decreased (0.211924 --> 0.207006).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2500105
	speed: 1.9132s/iter; left time: 24009.2280s
Epoch: 8 cost time: 109.58149480819702
Epoch: 8, Steps: 136 | Train Loss: 0.2529281 Vali Loss: 0.2039077 Test Loss: 0.2383254
Validation loss decreased (0.207006 --> 0.203908).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2601437
	speed: 1.9995s/iter; left time: 24819.2721s
Epoch: 9 cost time: 116.85460686683655
Epoch: 9, Steps: 136 | Train Loss: 0.2499382 Vali Loss: 0.2019977 Test Loss: 0.2359660
Validation loss decreased (0.203908 --> 0.201998).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2499121
	speed: 2.0896s/iter; left time: 25653.9689s
Epoch: 10 cost time: 117.39579582214355
Epoch: 10, Steps: 136 | Train Loss: 0.2478181 Vali Loss: 0.2006578 Test Loss: 0.2343482
Validation loss decreased (0.201998 --> 0.200658).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2470149
	speed: 1.9546s/iter; left time: 23731.1745s
Epoch: 11 cost time: 111.17207980155945
Epoch: 11, Steps: 136 | Train Loss: 0.2465458 Vali Loss: 0.1995315 Test Loss: 0.2331797
Validation loss decreased (0.200658 --> 0.199531).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2581810
	speed: 1.9716s/iter; left time: 23668.4959s
Epoch: 12 cost time: 111.62988209724426
Epoch: 12, Steps: 136 | Train Loss: 0.2455402 Vali Loss: 0.1989711 Test Loss: 0.2323752
Validation loss decreased (0.199531 --> 0.198971).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2465774
	speed: 1.9253s/iter; left time: 22851.8794s
Epoch: 13 cost time: 114.00971150398254
Epoch: 13, Steps: 136 | Train Loss: 0.2447908 Vali Loss: 0.1983538 Test Loss: 0.2317352
Validation loss decreased (0.198971 --> 0.198354).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2279814
	speed: 1.9887s/iter; left time: 23333.4275s
Epoch: 14 cost time: 112.68869996070862
Epoch: 14, Steps: 136 | Train Loss: 0.2443065 Vali Loss: 0.1981427 Test Loss: 0.2312567
Validation loss decreased (0.198354 --> 0.198143).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2335986
	speed: 1.9205s/iter; left time: 22271.7798s
Epoch: 15 cost time: 110.56664967536926
Epoch: 15, Steps: 136 | Train Loss: 0.2439218 Vali Loss: 0.1979553 Test Loss: 0.2308625
Validation loss decreased (0.198143 --> 0.197955).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2342115
	speed: 1.9350s/iter; left time: 22177.0478s
Epoch: 16 cost time: 111.1144917011261
Epoch: 16, Steps: 136 | Train Loss: 0.2435821 Vali Loss: 0.1976026 Test Loss: 0.2305572
Validation loss decreased (0.197955 --> 0.197603).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2537135
	speed: 1.9041s/iter; left time: 21564.1030s
Epoch: 17 cost time: 110.20006823539734
Epoch: 17, Steps: 136 | Train Loss: 0.2433061 Vali Loss: 0.1976352 Test Loss: 0.2303102
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2426661
	speed: 1.9457s/iter; left time: 21770.1770s
Epoch: 18 cost time: 105.96754908561707
Epoch: 18, Steps: 136 | Train Loss: 0.2431426 Vali Loss: 0.1972791 Test Loss: 0.2300945
Validation loss decreased (0.197603 --> 0.197279).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2570505
	speed: 1.8695s/iter; left time: 20663.5581s
Epoch: 19 cost time: 109.14094710350037
Epoch: 19, Steps: 136 | Train Loss: 0.2429198 Vali Loss: 0.1970568 Test Loss: 0.2299358
Validation loss decreased (0.197279 --> 0.197057).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2461439
	speed: 1.9047s/iter; left time: 20794.0216s
Epoch: 20 cost time: 110.51766300201416
Epoch: 20, Steps: 136 | Train Loss: 0.2427369 Vali Loss: 0.1966934 Test Loss: 0.2297748
Validation loss decreased (0.197057 --> 0.196693).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2430425
	speed: 1.8614s/iter; left time: 20067.8499s
Epoch: 21 cost time: 104.51603436470032
Epoch: 21, Steps: 136 | Train Loss: 0.2425560 Vali Loss: 0.1966297 Test Loss: 0.2296461
Validation loss decreased (0.196693 --> 0.196630).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2443383
	speed: 1.8245s/iter; left time: 19421.3429s
Epoch: 22 cost time: 104.99114608764648
Epoch: 22, Steps: 136 | Train Loss: 0.2424066 Vali Loss: 0.1966771 Test Loss: 0.2295263
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2424921
	speed: 1.6536s/iter; left time: 17377.8747s
Epoch: 23 cost time: 86.87230515480042
Epoch: 23, Steps: 136 | Train Loss: 0.2422722 Vali Loss: 0.1964700 Test Loss: 0.2294268
Validation loss decreased (0.196630 --> 0.196470).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2417767
	speed: 1.5042s/iter; left time: 15603.2914s
Epoch: 24 cost time: 83.52929377555847
Epoch: 24, Steps: 136 | Train Loss: 0.2422442 Vali Loss: 0.1965652 Test Loss: 0.2293527
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2546477
	speed: 1.4763s/iter; left time: 15112.4234s
Epoch: 25 cost time: 86.14658284187317
Epoch: 25, Steps: 136 | Train Loss: 0.2421540 Vali Loss: 0.1963753 Test Loss: 0.2292737
Validation loss decreased (0.196470 --> 0.196375).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2448361
	speed: 1.4972s/iter; left time: 15123.1187s
Epoch: 26 cost time: 86.15424537658691
Epoch: 26, Steps: 136 | Train Loss: 0.2420062 Vali Loss: 0.1963346 Test Loss: 0.2292043
Validation loss decreased (0.196375 --> 0.196335).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2421174
	speed: 1.4651s/iter; left time: 14600.1027s
Epoch: 27 cost time: 85.20608139038086
Epoch: 27, Steps: 136 | Train Loss: 0.2420038 Vali Loss: 0.1960571 Test Loss: 0.2291403
Validation loss decreased (0.196335 --> 0.196057).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2395632
	speed: 1.5150s/iter; left time: 14890.5149s
Epoch: 28 cost time: 82.9665880203247
Epoch: 28, Steps: 136 | Train Loss: 0.2417573 Vali Loss: 0.1959998 Test Loss: 0.2290881
Validation loss decreased (0.196057 --> 0.196000).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2464236
	speed: 1.5020s/iter; left time: 14558.4796s
Epoch: 29 cost time: 92.69112658500671
Epoch: 29, Steps: 136 | Train Loss: 0.2418274 Vali Loss: 0.1959484 Test Loss: 0.2290508
Validation loss decreased (0.196000 --> 0.195948).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2498148
	speed: 1.5277s/iter; left time: 14600.1755s
Epoch: 30 cost time: 81.05374717712402
Epoch: 30, Steps: 136 | Train Loss: 0.2418318 Vali Loss: 0.1960570 Test Loss: 0.2290149
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2407692
	speed: 1.4243s/iter; left time: 13418.2763s
Epoch: 31 cost time: 85.95104122161865
Epoch: 31, Steps: 136 | Train Loss: 0.2417924 Vali Loss: 0.1962435 Test Loss: 0.2289664
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2405001
	speed: 1.4844s/iter; left time: 13782.7116s
Epoch: 32 cost time: 85.26213455200195
Epoch: 32, Steps: 136 | Train Loss: 0.2417972 Vali Loss: 0.1959909 Test Loss: 0.2289512
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2386363
	speed: 1.4826s/iter; left time: 13564.7519s
Epoch: 33 cost time: 84.59615087509155
Epoch: 33, Steps: 136 | Train Loss: 0.2417368 Vali Loss: 0.1958810 Test Loss: 0.2289019
Validation loss decreased (0.195948 --> 0.195881).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2408924
	speed: 1.4873s/iter; left time: 13405.0342s
Epoch: 34 cost time: 84.44809556007385
Epoch: 34, Steps: 136 | Train Loss: 0.2415655 Vali Loss: 0.1960687 Test Loss: 0.2288681
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2386143
	speed: 1.4689s/iter; left time: 13039.2917s
Epoch: 35 cost time: 85.20080018043518
Epoch: 35, Steps: 136 | Train Loss: 0.2416782 Vali Loss: 0.1961180 Test Loss: 0.2288432
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2318393
	speed: 1.4918s/iter; left time: 13039.8798s
Epoch: 36 cost time: 85.70879030227661
Epoch: 36, Steps: 136 | Train Loss: 0.2415491 Vali Loss: 0.1959680 Test Loss: 0.2288318
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2493041
	speed: 1.4910s/iter; left time: 12830.0928s
Epoch: 37 cost time: 82.81588506698608
Epoch: 37, Steps: 136 | Train Loss: 0.2415798 Vali Loss: 0.1960699 Test Loss: 0.2288152
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2444508
	speed: 1.4037s/iter; left time: 11887.9815s
Epoch: 38 cost time: 77.47265148162842
Epoch: 38, Steps: 136 | Train Loss: 0.2414676 Vali Loss: 0.1961639 Test Loss: 0.2287999
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2453477
	speed: 1.3106s/iter; left time: 10921.2721s
Epoch: 39 cost time: 80.51037812232971
Epoch: 39, Steps: 136 | Train Loss: 0.2415314 Vali Loss: 0.1961102 Test Loss: 0.2287849
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2384034
	speed: 1.4481s/iter; left time: 11870.4432s
Epoch: 40 cost time: 85.04429626464844
Epoch: 40, Steps: 136 | Train Loss: 0.2415035 Vali Loss: 0.1958835 Test Loss: 0.2287688
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2422411
	speed: 1.3466s/iter; left time: 10855.1646s
Epoch: 41 cost time: 71.18588995933533
Epoch: 41, Steps: 136 | Train Loss: 0.2414662 Vali Loss: 0.1957029 Test Loss: 0.2287546
Validation loss decreased (0.195881 --> 0.195703).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2348936
	speed: 1.1613s/iter; left time: 9203.1338s
Epoch: 42 cost time: 66.26929092407227
Epoch: 42, Steps: 136 | Train Loss: 0.2415364 Vali Loss: 0.1957053 Test Loss: 0.2287443
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2401306
	speed: 1.1372s/iter; left time: 8857.7180s
Epoch: 43 cost time: 61.59346103668213
Epoch: 43, Steps: 136 | Train Loss: 0.2414632 Vali Loss: 0.1957467 Test Loss: 0.2287286
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2479349
	speed: 0.8748s/iter; left time: 6695.1193s
Epoch: 44 cost time: 47.280009508132935
Epoch: 44, Steps: 136 | Train Loss: 0.2414549 Vali Loss: 0.1956006 Test Loss: 0.2287196
Validation loss decreased (0.195703 --> 0.195601).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2408454
	speed: 0.8551s/iter; left time: 6427.9557s
Epoch: 45 cost time: 47.46972298622131
Epoch: 45, Steps: 136 | Train Loss: 0.2414342 Vali Loss: 0.1957837 Test Loss: 0.2287190
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2364473
	speed: 0.8477s/iter; left time: 6257.2260s
Epoch: 46 cost time: 49.210490703582764
Epoch: 46, Steps: 136 | Train Loss: 0.2414408 Vali Loss: 0.1959901 Test Loss: 0.2287105
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2449418
	speed: 0.8620s/iter; left time: 6245.2470s
Epoch: 47 cost time: 47.04927706718445
Epoch: 47, Steps: 136 | Train Loss: 0.2413302 Vali Loss: 0.1957817 Test Loss: 0.2286952
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2386931
	speed: 0.8512s/iter; left time: 6051.3897s
Epoch: 48 cost time: 47.78464674949646
Epoch: 48, Steps: 136 | Train Loss: 0.2413130 Vali Loss: 0.1955393 Test Loss: 0.2286847
Validation loss decreased (0.195601 --> 0.195539).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2348663
	speed: 0.8527s/iter; left time: 5945.5856s
Epoch: 49 cost time: 48.00690937042236
Epoch: 49, Steps: 136 | Train Loss: 0.2413438 Vali Loss: 0.1956140 Test Loss: 0.2286800
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2361956
	speed: 0.8525s/iter; left time: 5828.4270s
Epoch: 50 cost time: 48.28753876686096
Epoch: 50, Steps: 136 | Train Loss: 0.2413659 Vali Loss: 0.1957553 Test Loss: 0.2286723
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2445424
	speed: 0.8837s/iter; left time: 5921.6844s
Epoch: 51 cost time: 49.595197916030884
Epoch: 51, Steps: 136 | Train Loss: 0.2413830 Vali Loss: 0.1955083 Test Loss: 0.2286683
Validation loss decreased (0.195539 --> 0.195508).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2339624
	speed: 0.8949s/iter; left time: 5874.8363s
Epoch: 52 cost time: 51.14640712738037
Epoch: 52, Steps: 136 | Train Loss: 0.2414142 Vali Loss: 0.1956476 Test Loss: 0.2286684
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2497438
	speed: 0.8952s/iter; left time: 5755.3136s
Epoch: 53 cost time: 49.590693950653076
Epoch: 53, Steps: 136 | Train Loss: 0.2413924 Vali Loss: 0.1957256 Test Loss: 0.2286631
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2530212
	speed: 0.8569s/iter; left time: 5392.5456s
Epoch: 54 cost time: 49.36039185523987
Epoch: 54, Steps: 136 | Train Loss: 0.2413891 Vali Loss: 0.1955143 Test Loss: 0.2286621
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2457057
	speed: 0.8806s/iter; left time: 5421.6730s
Epoch: 55 cost time: 50.27855682373047
Epoch: 55, Steps: 136 | Train Loss: 0.2413854 Vali Loss: 0.1953669 Test Loss: 0.2286597
Validation loss decreased (0.195508 --> 0.195367).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2580147
	speed: 0.8716s/iter; left time: 5247.6272s
Epoch: 56 cost time: 47.455522537231445
Epoch: 56, Steps: 136 | Train Loss: 0.2412974 Vali Loss: 0.1957786 Test Loss: 0.2286626
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2492625
	speed: 0.8531s/iter; left time: 5020.5366s
Epoch: 57 cost time: 49.85050010681152
Epoch: 57, Steps: 136 | Train Loss: 0.2413313 Vali Loss: 0.1956453 Test Loss: 0.2286457
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2270982
	speed: 0.8600s/iter; left time: 4944.2585s
Epoch: 58 cost time: 47.194379806518555
Epoch: 58, Steps: 136 | Train Loss: 0.2414648 Vali Loss: 0.1955042 Test Loss: 0.2286437
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2390969
	speed: 0.8650s/iter; left time: 4855.5235s
Epoch: 59 cost time: 49.461668252944946
Epoch: 59, Steps: 136 | Train Loss: 0.2414018 Vali Loss: 0.1956793 Test Loss: 0.2286399
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2445536
	speed: 0.8639s/iter; left time: 4731.8315s
Epoch: 60 cost time: 49.52620267868042
Epoch: 60, Steps: 136 | Train Loss: 0.2413667 Vali Loss: 0.1959031 Test Loss: 0.2286432
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2417909
	speed: 0.8494s/iter; left time: 4536.4306s
Epoch: 61 cost time: 47.572256326675415
Epoch: 61, Steps: 136 | Train Loss: 0.2412803 Vali Loss: 0.1957013 Test Loss: 0.2286379
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2421335
	speed: 0.9125s/iter; left time: 4749.6725s
Epoch: 62 cost time: 57.198230266571045
Epoch: 62, Steps: 136 | Train Loss: 0.2412199 Vali Loss: 0.1953383 Test Loss: 0.2286430
Validation loss decreased (0.195367 --> 0.195338).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2385602
	speed: 0.9766s/iter; left time: 4950.2275s
Epoch: 63 cost time: 54.23141121864319
Epoch: 63, Steps: 136 | Train Loss: 0.2413013 Vali Loss: 0.1955089 Test Loss: 0.2286367
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2329995
	speed: 0.9824s/iter; left time: 4846.0553s
Epoch: 64 cost time: 57.31376051902771
Epoch: 64, Steps: 136 | Train Loss: 0.2412802 Vali Loss: 0.1956455 Test Loss: 0.2286308
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2424285
	speed: 0.9910s/iter; left time: 4753.8714s
Epoch: 65 cost time: 55.11695170402527
Epoch: 65, Steps: 136 | Train Loss: 0.2414016 Vali Loss: 0.1953869 Test Loss: 0.2286363
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2280455
	speed: 0.9815s/iter; left time: 4574.8067s
Epoch: 66 cost time: 58.25456404685974
Epoch: 66, Steps: 136 | Train Loss: 0.2413501 Vali Loss: 0.1955249 Test Loss: 0.2286297
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2449212
	speed: 0.9699s/iter; left time: 4388.6664s
Epoch: 67 cost time: 53.55731678009033
Epoch: 67, Steps: 136 | Train Loss: 0.2413041 Vali Loss: 0.1953521 Test Loss: 0.2286297
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2503242
	speed: 0.9673s/iter; left time: 4245.6957s
Epoch: 68 cost time: 56.07454037666321
Epoch: 68, Steps: 136 | Train Loss: 0.2413023 Vali Loss: 0.1957780 Test Loss: 0.2286288
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2447702
	speed: 0.9967s/iter; left time: 4238.8688s
Epoch: 69 cost time: 55.593912839889526
Epoch: 69, Steps: 136 | Train Loss: 0.2412963 Vali Loss: 0.1959604 Test Loss: 0.2286261
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2553440
	speed: 0.9760s/iter; left time: 4018.3090s
Epoch: 70 cost time: 56.17398738861084
Epoch: 70, Steps: 136 | Train Loss: 0.2412611 Vali Loss: 0.1959136 Test Loss: 0.2286291
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2423416
	speed: 0.9895s/iter; left time: 3939.2879s
Epoch: 71 cost time: 56.9521758556366
Epoch: 71, Steps: 136 | Train Loss: 0.2413002 Vali Loss: 0.1954316 Test Loss: 0.2286244
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2327932
	speed: 0.9345s/iter; left time: 3592.9783s
Epoch: 72 cost time: 52.2961368560791
Epoch: 72, Steps: 136 | Train Loss: 0.2413112 Vali Loss: 0.1955718 Test Loss: 0.2286256
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2512996
	speed: 0.9434s/iter; left time: 3499.0421s
Epoch: 73 cost time: 53.38343834877014
Epoch: 73, Steps: 136 | Train Loss: 0.2412113 Vali Loss: 0.1953932 Test Loss: 0.2286247
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.2355617
	speed: 0.9221s/iter; left time: 3294.6387s
Epoch: 74 cost time: 56.81112599372864
Epoch: 74, Steps: 136 | Train Loss: 0.2412877 Vali Loss: 0.1956785 Test Loss: 0.2286257
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2339673
	speed: 0.9866s/iter; left time: 3390.8739s
Epoch: 75 cost time: 53.96849274635315
Epoch: 75, Steps: 136 | Train Loss: 0.2413829 Vali Loss: 0.1954711 Test Loss: 0.2286232
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2530771
	speed: 0.9738s/iter; left time: 3214.6678s
Epoch: 76 cost time: 55.976157665252686
Epoch: 76, Steps: 136 | Train Loss: 0.2412479 Vali Loss: 0.1957962 Test Loss: 0.2286212
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2351348
	speed: 1.0047s/iter; left time: 3179.7931s
Epoch: 77 cost time: 56.86161208152771
Epoch: 77, Steps: 136 | Train Loss: 0.2412783 Vali Loss: 0.1958203 Test Loss: 0.2286229
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.2395826
	speed: 1.0209s/iter; left time: 3092.3249s
Epoch: 78 cost time: 59.51195740699768
Epoch: 78, Steps: 136 | Train Loss: 0.2412707 Vali Loss: 0.1956044 Test Loss: 0.2286219
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.2261048
	speed: 1.0196s/iter; left time: 2949.8043s
Epoch: 79 cost time: 57.99480223655701
Epoch: 79, Steps: 136 | Train Loss: 0.2412211 Vali Loss: 0.1953727 Test Loss: 0.2286225
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.2339067
	speed: 1.0032s/iter; left time: 2765.6973s
Epoch: 80 cost time: 55.932167768478394
Epoch: 80, Steps: 136 | Train Loss: 0.2412944 Vali Loss: 0.1953764 Test Loss: 0.2286220
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.2353769
	speed: 1.0072s/iter; left time: 2639.7896s
Epoch: 81 cost time: 59.52397608757019
Epoch: 81, Steps: 136 | Train Loss: 0.2412637 Vali Loss: 0.1956185 Test Loss: 0.2286208
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.2301047
	speed: 1.0137s/iter; left time: 2518.9270s
Epoch: 82 cost time: 57.156529903411865
Epoch: 82, Steps: 136 | Train Loss: 0.2413221 Vali Loss: 0.1956927 Test Loss: 0.2286194
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_180_j720_H6_FITS_custom_ftM_sl180_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2270604372024536, mae:0.31276050209999084, rse:0.47533315420150757, corr:[0.44717714 0.44672796 0.44836643 0.44824272 0.44849974 0.44907168
 0.44898418 0.4484268  0.44831812 0.44815376 0.44754887 0.44739836
 0.44744885 0.44692573 0.44700396 0.44712913 0.4469463  0.44700897
 0.44720575 0.44716412 0.44705912 0.44733116 0.4478399  0.44759753
 0.44714126 0.44723347 0.4470069  0.44667783 0.44662264 0.4465482
 0.44633546 0.44608137 0.44597265 0.44594923 0.4457159  0.44554904
 0.44564506 0.44547462 0.44549042 0.44553888 0.44542673 0.4453903
 0.44545493 0.44544178 0.44534573 0.4453981  0.44573247 0.44570318
 0.44533932 0.44534606 0.44535166 0.4451667  0.4450109  0.44496116
 0.4449499  0.4448429  0.44476476 0.444838   0.44478625 0.44462672
 0.44474646 0.44470164 0.4446323  0.44464895 0.44461694 0.44452697
 0.44448707 0.44442806 0.44437292 0.44438064 0.44455707 0.4445879
 0.44436988 0.444306   0.4442689  0.44416544 0.444018   0.443955
 0.44397137 0.44387716 0.44377032 0.4438095  0.44383743 0.44372305
 0.4437707  0.44376606 0.44370726 0.4436636  0.44371593 0.44372573
 0.44365296 0.44355455 0.44350561 0.44353044 0.44362235 0.44361413
 0.44351175 0.44346204 0.44334713 0.44333702 0.44330665 0.4432019
 0.44321808 0.44325945 0.44321558 0.44321263 0.44323725 0.44320273
 0.44322532 0.44321954 0.4432745  0.44322836 0.44315845 0.44314316
 0.4430647  0.44289118 0.4427951  0.4428007  0.44287762 0.44284937
 0.44279516 0.44292805 0.44289187 0.44288874 0.4429769  0.44299302
 0.4430322  0.44308984 0.4431255  0.44314685 0.44311392 0.4431288
 0.44325823 0.44324413 0.4432068  0.4431856  0.4430932  0.44304132
 0.44312444 0.44316894 0.44314474 0.443277   0.44359314 0.4436245
 0.4434629  0.44358718 0.44368622 0.44369638 0.44374728 0.44381407
 0.44387218 0.4438828  0.44391286 0.44400585 0.44405025 0.44413868
 0.44439387 0.44459888 0.44451407 0.44446194 0.44452915 0.44457662
 0.44475508 0.4449173  0.44499078 0.44529805 0.44569686 0.44519636
 0.44446406 0.4440289  0.44374546 0.44344947 0.4431518  0.44296798
 0.44288552 0.44270176 0.44250044 0.44242564 0.44233444 0.4421921
 0.44219643 0.44210938 0.44202816 0.44199568 0.44204208 0.44207698
 0.44205016 0.44195384 0.4418859  0.44191328 0.44203869 0.44180954
 0.44143575 0.44124466 0.44104567 0.44093603 0.4408518  0.44070965
 0.44065002 0.4405809  0.440445   0.44036382 0.44030467 0.44021013
 0.4402444  0.4401981  0.44023603 0.44023344 0.44023144 0.44028693
 0.44027936 0.4401102  0.43999782 0.44004008 0.44015524 0.44002947
 0.43982533 0.43984568 0.43971947 0.43956518 0.43950918 0.43944857
 0.43940738 0.4393756  0.4393653  0.4393835  0.43936595 0.43931633
 0.43935996 0.4392616  0.43927395 0.43930537 0.43925166 0.43922016
 0.4392424  0.439126   0.43899357 0.43897262 0.43910906 0.43903172
 0.438781   0.4388064  0.43878806 0.43866798 0.43859518 0.4385788
 0.43858272 0.43852383 0.4384746  0.43851525 0.4385206  0.43844944
 0.4385487  0.43852755 0.43846595 0.43843794 0.4384334  0.43841606
 0.43838954 0.43832296 0.4382359  0.43818983 0.438279   0.4382684
 0.43810782 0.43810767 0.43808356 0.43805385 0.438027   0.4380163
 0.438071   0.43806762 0.43800914 0.43804216 0.43805784 0.43799633
 0.43808421 0.4380785  0.43805522 0.4380143  0.43797904 0.43795055
 0.437863   0.43771166 0.4376325  0.43759048 0.4376425  0.43769482
 0.43765348 0.4377269  0.43772003 0.43779948 0.43789068 0.43788937
 0.4379576  0.4380295  0.43802744 0.43807414 0.43814567 0.43819413
 0.43831757 0.4383481  0.438316   0.43824843 0.43816173 0.43813273
 0.43817905 0.43817452 0.4381707  0.43831393 0.43855006 0.438557
 0.43844658 0.43857715 0.43865994 0.4387262  0.43882528 0.43887183
 0.4389063  0.43895948 0.4390857  0.43924856 0.43934637 0.43952444
 0.4398178  0.43998465 0.43985766 0.43977037 0.43979964 0.43985373
 0.44000098 0.44002396 0.4399288  0.44010958 0.44042507 0.439907
 0.43921363 0.43891075 0.43869716 0.43843186 0.4381876  0.43801242
 0.43790552 0.43771702 0.4374957  0.4373976  0.43726707 0.43708053
 0.43706393 0.43693694 0.4368198  0.4367653  0.43676966 0.4367876
 0.43680573 0.43676674 0.43671212 0.4367586  0.43692213 0.4367547
 0.43638664 0.4362183  0.43607584 0.4360239  0.4359868  0.43588892
 0.4358172  0.4356935  0.43551943 0.43543667 0.4353465  0.43518478
 0.43518952 0.43509892 0.4350823  0.4350755  0.43507665 0.43512198
 0.4351172  0.43500862 0.4349538  0.43498698 0.43509075 0.43505386
 0.43490586 0.43495497 0.43491438 0.43485376 0.43481785 0.4347356
 0.43468237 0.4346477  0.4345986  0.4346013  0.43458247 0.43447676
 0.434492   0.43441966 0.43442497 0.43441167 0.43434694 0.4343452
 0.43435997 0.43425682 0.43417555 0.43422773 0.43435717 0.43430313
 0.43415034 0.43422905 0.43418726 0.43413725 0.4341809  0.4341723
 0.43412352 0.4340402  0.43395168 0.43397135 0.43397108 0.4339145
 0.43404183 0.4339909  0.43392065 0.433908   0.4339164  0.4339392
 0.43395078 0.43388134 0.43380657 0.4337969  0.43395716 0.43401122
 0.43383548 0.43385828 0.43387863 0.43388182 0.4339125  0.43392658
 0.4339371  0.43388245 0.43378627 0.43379474 0.43379813 0.43370634
 0.43376338 0.43370697 0.43367133 0.433699   0.43370333 0.43367156
 0.4336122  0.4335292  0.43347213 0.43340313 0.43349043 0.4336383
 0.43362552 0.43370768 0.43374622 0.43385032 0.43394974 0.43397444
 0.4340277  0.43404102 0.43401045 0.4340619  0.43409663 0.43406227
 0.43420073 0.43424973 0.43419477 0.4341161  0.43407717 0.43409517
 0.43414703 0.43417433 0.4342303  0.43438137 0.43461543 0.43471053
 0.43468693 0.43482155 0.4348958  0.43498442 0.43510664 0.4351642
 0.4351964  0.43522453 0.43527228 0.43536124 0.43547523 0.43564475
 0.4358486  0.43594015 0.4358293  0.43572468 0.43569028 0.43575835
 0.43595785 0.43598914 0.43590164 0.43607295 0.43634993 0.43579748
 0.43505424 0.43468997 0.43440875 0.4341209  0.43389407 0.4337176
 0.43358225 0.43336958 0.43311724 0.4329664  0.43280643 0.4326525
 0.4326908  0.4325202  0.4324044  0.43242362 0.43244284 0.43246204
 0.43250173 0.4324792  0.4324345  0.43243995 0.4325502  0.4323345
 0.4318716  0.43166324 0.43152407 0.43138307 0.43127903 0.43118697
 0.4310863  0.43090373 0.43069613 0.43061137 0.43050548 0.43033114
 0.4304213  0.43039778 0.4303618  0.43036    0.43041354 0.43048948
 0.4304854  0.43041527 0.4304251  0.43045688 0.43053177 0.43049705
 0.4302848  0.43016833 0.42999724 0.42987117 0.42980325 0.42968005
 0.42961183 0.4295653  0.42948592 0.42945513 0.42940632 0.429228
 0.42921856 0.4291638  0.42915985 0.4290861  0.42897815 0.42893326
 0.42890915 0.42883044 0.4287817  0.4288382  0.42892855 0.4288409
 0.42866725 0.42868587 0.42862675 0.4285922  0.4285877  0.4284886
 0.4284355  0.42841688 0.4283514  0.42830098 0.42826328 0.42821535
 0.4283198  0.42825037 0.4282276  0.42823294 0.42820835 0.42821714
 0.4282484  0.4281925  0.42808568 0.42806834 0.42821756 0.42819613
 0.42798057 0.4280362  0.42803586 0.4279632  0.42798683 0.42801392
 0.42799    0.42792803 0.4278809  0.4278959  0.4278608  0.42781875
 0.4279819  0.42792353 0.42788696 0.42794532 0.42791373 0.4278506
 0.4278273  0.42775756 0.42766142 0.42758882 0.4276999  0.42783415
 0.42770758 0.42779785 0.4279663  0.42804706 0.42807326 0.42811447
 0.4281984  0.42819124 0.4281461  0.4282316  0.4283073  0.42826387
 0.42844766 0.42853075 0.4284553  0.4284092  0.4284337  0.4284302
 0.4284252  0.42846724 0.4285573  0.4286679  0.42883304 0.428981
 0.42898008 0.429048   0.42911783 0.42922068 0.42928022 0.4292958
 0.4293715  0.42944884 0.42952    0.4296799  0.42991978 0.43008125
 0.430254   0.43045914 0.43043265 0.4303005  0.4302995  0.43042722
 0.43057263 0.4305183  0.43047798 0.43069118 0.4308982  0.43032143
 0.42964873 0.42925236 0.42887843 0.42866412 0.42847797 0.42817825
 0.4279944  0.42784888 0.42761973 0.42742974 0.42732233 0.42723635
 0.42718995 0.42705458 0.42712927 0.42713314 0.42704466 0.42714486
 0.4272604  0.42291    0.42272398 0.4270164  0.42301002 0.42266902
 0.4222457  0.4221945  0.4218946  0.4216655  0.4216404  0.42139646
 0.42118448 0.4211368  0.42100826 0.42084876 0.42082042 0.42093292
 0.42098525 0.42091143 0.42127118 0.42131636 0.42093837 0.42096847
 0.42103222 0.420553   0.42034492 0.42068735 0.42029715 0.42082152]
