Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_90_j720_H4', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_90_j720_H4_FITS_custom_ftM_sl90_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17603
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=26, out_features=234, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  249979392.0
params:  6318.0
Trainable parameters:  6318
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.1737200
	speed: 0.9027s/iter; left time: 12277.4673s
Epoch: 1 cost time: 122.80817699432373
Epoch: 1, Steps: 137 | Train Loss: 1.7099480 Vali Loss: 0.8352575 Test Loss: 0.9190902
Validation loss decreased (inf --> 0.835257).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6527001
	speed: 2.4730s/iter; left time: 33296.9171s
Epoch: 2 cost time: 138.85675764083862
Epoch: 2, Steps: 137 | Train Loss: 0.7358399 Vali Loss: 0.4936226 Test Loss: 0.5477626
Validation loss decreased (0.835257 --> 0.493623).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4685062
	speed: 2.4637s/iter; left time: 32833.4181s
Epoch: 3 cost time: 143.20361733436584
Epoch: 3, Steps: 137 | Train Loss: 0.4986013 Vali Loss: 0.3730023 Test Loss: 0.4165269
Validation loss decreased (0.493623 --> 0.373002).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3937124
	speed: 2.4923s/iter; left time: 32874.0590s
Epoch: 4 cost time: 137.47540712356567
Epoch: 4, Steps: 137 | Train Loss: 0.4029584 Vali Loss: 0.3176734 Test Loss: 0.3565144
Validation loss decreased (0.373002 --> 0.317673).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3659874
	speed: 1.6296s/iter; left time: 21270.6718s
Epoch: 5 cost time: 89.11374688148499
Epoch: 5, Steps: 137 | Train Loss: 0.3574128 Vali Loss: 0.2900903 Test Loss: 0.3260592
Validation loss decreased (0.317673 --> 0.290090).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3161879
	speed: 1.5713s/iter; left time: 20294.5563s
Epoch: 6 cost time: 93.0618371963501
Epoch: 6, Steps: 137 | Train Loss: 0.3332015 Vali Loss: 0.2748876 Test Loss: 0.3090148
Validation loss decreased (0.290090 --> 0.274888).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3262968
	speed: 1.5430s/iter; left time: 19717.6796s
Epoch: 7 cost time: 95.14389324188232
Epoch: 7, Steps: 137 | Train Loss: 0.3190810 Vali Loss: 0.2646146 Test Loss: 0.2985047
Validation loss decreased (0.274888 --> 0.264615).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3170001
	speed: 2.3880s/iter; left time: 30189.5079s
Epoch: 8 cost time: 139.655677318573
Epoch: 8, Steps: 137 | Train Loss: 0.3100666 Vali Loss: 0.2582317 Test Loss: 0.2914641
Validation loss decreased (0.264615 --> 0.258232).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3054840
	speed: 2.2845s/iter; left time: 28568.0361s
Epoch: 9 cost time: 127.95023655891418
Epoch: 9, Steps: 137 | Train Loss: 0.3037654 Vali Loss: 0.2534024 Test Loss: 0.2865179
Validation loss decreased (0.258232 --> 0.253402).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2890730
	speed: 2.4018s/iter; left time: 29705.0582s
Epoch: 10 cost time: 135.0369439125061
Epoch: 10, Steps: 137 | Train Loss: 0.2992323 Vali Loss: 0.2503163 Test Loss: 0.2828723
Validation loss decreased (0.253402 --> 0.250316).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2862511
	speed: 2.3594s/iter; left time: 28858.0315s
Epoch: 11 cost time: 137.35369038581848
Epoch: 11, Steps: 137 | Train Loss: 0.2958697 Vali Loss: 0.2473191 Test Loss: 0.2801192
Validation loss decreased (0.250316 --> 0.247319).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3130682
	speed: 2.2990s/iter; left time: 27803.8211s
Epoch: 12 cost time: 129.7892575263977
Epoch: 12, Steps: 137 | Train Loss: 0.2932247 Vali Loss: 0.2455413 Test Loss: 0.2780175
Validation loss decreased (0.247319 --> 0.245541).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2984091
	speed: 2.2721s/iter; left time: 27168.0895s
Epoch: 13 cost time: 133.95191192626953
Epoch: 13, Steps: 137 | Train Loss: 0.2912789 Vali Loss: 0.2434087 Test Loss: 0.2763556
Validation loss decreased (0.245541 --> 0.243409).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2855988
	speed: 2.2799s/iter; left time: 26948.6022s
Epoch: 14 cost time: 127.3757872581482
Epoch: 14, Steps: 137 | Train Loss: 0.2896247 Vali Loss: 0.2426834 Test Loss: 0.2750109
Validation loss decreased (0.243409 --> 0.242683).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2950271
	speed: 2.3238s/iter; left time: 27149.1691s
Epoch: 15 cost time: 131.0173144340515
Epoch: 15, Steps: 137 | Train Loss: 0.2882977 Vali Loss: 0.2415504 Test Loss: 0.2739252
Validation loss decreased (0.242683 --> 0.241550).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2759439
	speed: 2.2793s/iter; left time: 26316.3520s
Epoch: 16 cost time: 116.62474250793457
Epoch: 16, Steps: 137 | Train Loss: 0.2873326 Vali Loss: 0.2406545 Test Loss: 0.2730402
Validation loss decreased (0.241550 --> 0.240654).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2800366
	speed: 2.3066s/iter; left time: 26315.9675s
Epoch: 17 cost time: 138.91845417022705
Epoch: 17, Steps: 137 | Train Loss: 0.2864220 Vali Loss: 0.2398923 Test Loss: 0.2722949
Validation loss decreased (0.240654 --> 0.239892).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2800796
	speed: 2.3526s/iter; left time: 26518.0304s
Epoch: 18 cost time: 138.66588759422302
Epoch: 18, Steps: 137 | Train Loss: 0.2856820 Vali Loss: 0.2396269 Test Loss: 0.2716635
Validation loss decreased (0.239892 --> 0.239627).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2881663
	speed: 2.2762s/iter; left time: 25345.5733s
Epoch: 19 cost time: 129.82540106773376
Epoch: 19, Steps: 137 | Train Loss: 0.2850378 Vali Loss: 0.2391069 Test Loss: 0.2711453
Validation loss decreased (0.239627 --> 0.239107).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2734790
	speed: 2.1015s/iter; left time: 23112.7666s
Epoch: 20 cost time: 120.07158493995667
Epoch: 20, Steps: 137 | Train Loss: 0.2845606 Vali Loss: 0.2385772 Test Loss: 0.2706784
Validation loss decreased (0.239107 --> 0.238577).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2915812
	speed: 2.1600s/iter; left time: 23459.7237s
Epoch: 21 cost time: 112.31861710548401
Epoch: 21, Steps: 137 | Train Loss: 0.2840515 Vali Loss: 0.2373858 Test Loss: 0.2702934
Validation loss decreased (0.238577 --> 0.237386).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2947017
	speed: 1.6490s/iter; left time: 17683.5167s
Epoch: 22 cost time: 91.13630747795105
Epoch: 22, Steps: 137 | Train Loss: 0.2836304 Vali Loss: 0.2376565 Test Loss: 0.2699061
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2838208
	speed: 1.5584s/iter; left time: 16498.7151s
Epoch: 23 cost time: 90.22123384475708
Epoch: 23, Steps: 137 | Train Loss: 0.2831755 Vali Loss: 0.2373473 Test Loss: 0.2696053
Validation loss decreased (0.237386 --> 0.237347).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2799577
	speed: 1.4650s/iter; left time: 15309.1377s
Epoch: 24 cost time: 80.39211225509644
Epoch: 24, Steps: 137 | Train Loss: 0.2829568 Vali Loss: 0.2373578 Test Loss: 0.2693276
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2759539
	speed: 1.3885s/iter; left time: 14319.2960s
Epoch: 25 cost time: 82.23646545410156
Epoch: 25, Steps: 137 | Train Loss: 0.2825872 Vali Loss: 0.2366268 Test Loss: 0.2690636
Validation loss decreased (0.237347 --> 0.236627).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2848523
	speed: 1.4068s/iter; left time: 14315.8693s
Epoch: 26 cost time: 84.73016595840454
Epoch: 26, Steps: 137 | Train Loss: 0.2823563 Vali Loss: 0.2363000 Test Loss: 0.2688458
Validation loss decreased (0.236627 --> 0.236300).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2834421
	speed: 1.5551s/iter; left time: 15611.8842s
Epoch: 27 cost time: 85.16015195846558
Epoch: 27, Steps: 137 | Train Loss: 0.2821411 Vali Loss: 0.2363774 Test Loss: 0.2686502
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2788357
	speed: 1.3199s/iter; left time: 13069.3497s
Epoch: 28 cost time: 74.6875352859497
Epoch: 28, Steps: 137 | Train Loss: 0.2819512 Vali Loss: 0.2361760 Test Loss: 0.2684476
Validation loss decreased (0.236300 --> 0.236176).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2786227
	speed: 1.4476s/iter; left time: 14135.9876s
Epoch: 29 cost time: 84.56598830223083
Epoch: 29, Steps: 137 | Train Loss: 0.2817809 Vali Loss: 0.2355342 Test Loss: 0.2682881
Validation loss decreased (0.236176 --> 0.235534).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2878762
	speed: 1.3451s/iter; left time: 12950.7342s
Epoch: 30 cost time: 76.61876320838928
Epoch: 30, Steps: 137 | Train Loss: 0.2814089 Vali Loss: 0.2362515 Test Loss: 0.2681150
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2779339
	speed: 1.3023s/iter; left time: 12360.1012s
Epoch: 31 cost time: 73.23283219337463
Epoch: 31, Steps: 137 | Train Loss: 0.2814556 Vali Loss: 0.2358509 Test Loss: 0.2679901
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2736872
	speed: 1.3884s/iter; left time: 12986.7202s
Epoch: 32 cost time: 80.2309308052063
Epoch: 32, Steps: 137 | Train Loss: 0.2812824 Vali Loss: 0.2360888 Test Loss: 0.2678373
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2829487
	speed: 1.3484s/iter; left time: 12427.9222s
Epoch: 33 cost time: 75.59484267234802
Epoch: 33, Steps: 137 | Train Loss: 0.2810754 Vali Loss: 0.2351186 Test Loss: 0.2677223
Validation loss decreased (0.235534 --> 0.235119).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2751718
	speed: 1.2456s/iter; left time: 11309.6638s
Epoch: 34 cost time: 70.86365580558777
Epoch: 34, Steps: 137 | Train Loss: 0.2809688 Vali Loss: 0.2350979 Test Loss: 0.2676150
Validation loss decreased (0.235119 --> 0.235098).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2794653
	speed: 1.1945s/iter; left time: 10682.1880s
Epoch: 35 cost time: 68.9531261920929
Epoch: 35, Steps: 137 | Train Loss: 0.2808812 Vali Loss: 0.2350921 Test Loss: 0.2675091
Validation loss decreased (0.235098 --> 0.235092).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2797049
	speed: 1.2412s/iter; left time: 10929.7474s
Epoch: 36 cost time: 75.26694226264954
Epoch: 36, Steps: 137 | Train Loss: 0.2807543 Vali Loss: 0.2352675 Test Loss: 0.2674203
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2913553
	speed: 1.2664s/iter; left time: 10978.6622s
Epoch: 37 cost time: 69.71722841262817
Epoch: 37, Steps: 137 | Train Loss: 0.2806132 Vali Loss: 0.2350849 Test Loss: 0.2673391
Validation loss decreased (0.235092 --> 0.235085).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2911507
	speed: 1.2033s/iter; left time: 10266.3230s
Epoch: 38 cost time: 68.6752302646637
Epoch: 38, Steps: 137 | Train Loss: 0.2804624 Vali Loss: 0.2349880 Test Loss: 0.2672448
Validation loss decreased (0.235085 --> 0.234988).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2813700
	speed: 1.0940s/iter; left time: 9184.0485s
Epoch: 39 cost time: 65.06300282478333
Epoch: 39, Steps: 137 | Train Loss: 0.2805103 Vali Loss: 0.2351322 Test Loss: 0.2671683
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2713977
	speed: 1.1811s/iter; left time: 9753.5404s
Epoch: 40 cost time: 69.24444484710693
Epoch: 40, Steps: 137 | Train Loss: 0.2802668 Vali Loss: 0.2350737 Test Loss: 0.2670975
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2707608
	speed: 1.1532s/iter; left time: 9365.5334s
Epoch: 41 cost time: 68.04060792922974
Epoch: 41, Steps: 137 | Train Loss: 0.2803410 Vali Loss: 0.2354058 Test Loss: 0.2670386
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2973854
	speed: 1.1476s/iter; left time: 9162.0905s
Epoch: 42 cost time: 65.80168294906616
Epoch: 42, Steps: 137 | Train Loss: 0.2802686 Vali Loss: 0.2348562 Test Loss: 0.2669755
Validation loss decreased (0.234988 --> 0.234856).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2825487
	speed: 1.1301s/iter; left time: 8867.5025s
Epoch: 43 cost time: 65.27660322189331
Epoch: 43, Steps: 137 | Train Loss: 0.2801949 Vali Loss: 0.2347849 Test Loss: 0.2669185
Validation loss decreased (0.234856 --> 0.234785).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2720167
	speed: 1.1447s/iter; left time: 8825.7486s
Epoch: 44 cost time: 67.23679542541504
Epoch: 44, Steps: 137 | Train Loss: 0.2802131 Vali Loss: 0.2351117 Test Loss: 0.2668792
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2844959
	speed: 1.0943s/iter; left time: 8287.4230s
Epoch: 45 cost time: 59.42986011505127
Epoch: 45, Steps: 137 | Train Loss: 0.2800730 Vali Loss: 0.2345261 Test Loss: 0.2668159
Validation loss decreased (0.234785 --> 0.234526).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2717464
	speed: 1.0429s/iter; left time: 7754.7391s
Epoch: 46 cost time: 61.152209520339966
Epoch: 46, Steps: 137 | Train Loss: 0.2800350 Vali Loss: 0.2349404 Test Loss: 0.2667705
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2832683
	speed: 1.0768s/iter; left time: 7859.3940s
Epoch: 47 cost time: 62.13416123390198
Epoch: 47, Steps: 137 | Train Loss: 0.2799195 Vali Loss: 0.2349535 Test Loss: 0.2667324
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2867771
	speed: 1.0418s/iter; left time: 7461.4176s
Epoch: 48 cost time: 59.065863609313965
Epoch: 48, Steps: 137 | Train Loss: 0.2798849 Vali Loss: 0.2344320 Test Loss: 0.2666972
Validation loss decreased (0.234526 --> 0.234432).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2992372
	speed: 1.0281s/iter; left time: 7222.0749s
Epoch: 49 cost time: 60.26500153541565
Epoch: 49, Steps: 137 | Train Loss: 0.2799162 Vali Loss: 0.2346996 Test Loss: 0.2666587
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2721080
	speed: 1.0760s/iter; left time: 7411.3524s
Epoch: 50 cost time: 60.53685975074768
Epoch: 50, Steps: 137 | Train Loss: 0.2798221 Vali Loss: 0.2346122 Test Loss: 0.2666351
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2812958
	speed: 1.0613s/iter; left time: 7165.0344s
Epoch: 51 cost time: 59.390844106674194
Epoch: 51, Steps: 137 | Train Loss: 0.2798485 Vali Loss: 0.2346231 Test Loss: 0.2666003
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2801503
	speed: 1.0868s/iter; left time: 7187.8877s
Epoch: 52 cost time: 59.328588247299194
Epoch: 52, Steps: 137 | Train Loss: 0.2797426 Vali Loss: 0.2343791 Test Loss: 0.2665700
Validation loss decreased (0.234432 --> 0.234379).  Saving model ...
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2697043
	speed: 1.0020s/iter; left time: 6490.0362s
Epoch: 53 cost time: 58.36707806587219
Epoch: 53, Steps: 137 | Train Loss: 0.2797290 Vali Loss: 0.2340351 Test Loss: 0.2665457
Validation loss decreased (0.234379 --> 0.234035).  Saving model ...
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2932208
	speed: 0.9991s/iter; left time: 6334.5292s
Epoch: 54 cost time: 59.11542892456055
Epoch: 54, Steps: 137 | Train Loss: 0.2796641 Vali Loss: 0.2342601 Test Loss: 0.2665246
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2890174
	speed: 1.0502s/iter; left time: 6514.5211s
Epoch: 55 cost time: 62.43988609313965
Epoch: 55, Steps: 137 | Train Loss: 0.2796809 Vali Loss: 0.2342228 Test Loss: 0.2665046
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2731108
	speed: 1.0188s/iter; left time: 6180.1795s
Epoch: 56 cost time: 57.26814365386963
Epoch: 56, Steps: 137 | Train Loss: 0.2797101 Vali Loss: 0.2342062 Test Loss: 0.2664864
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2823103
	speed: 0.9920s/iter; left time: 5881.5100s
Epoch: 57 cost time: 58.35925030708313
Epoch: 57, Steps: 137 | Train Loss: 0.2796154 Vali Loss: 0.2340556 Test Loss: 0.2664687
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2796502
	speed: 1.0159s/iter; left time: 5883.8176s
Epoch: 58 cost time: 59.68345928192139
Epoch: 58, Steps: 137 | Train Loss: 0.2796821 Vali Loss: 0.2347288 Test Loss: 0.2664564
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.3014129
	speed: 1.0161s/iter; left time: 5745.8359s
Epoch: 59 cost time: 57.51787281036377
Epoch: 59, Steps: 137 | Train Loss: 0.2795891 Vali Loss: 0.2339733 Test Loss: 0.2664402
Validation loss decreased (0.234035 --> 0.233973).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2778969
	speed: 1.0144s/iter; left time: 5597.4088s
Epoch: 60 cost time: 60.72712588310242
Epoch: 60, Steps: 137 | Train Loss: 0.2796351 Vali Loss: 0.2343766 Test Loss: 0.2664222
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2787279
	speed: 1.0626s/iter; left time: 5717.9251s
Epoch: 61 cost time: 61.38568925857544
Epoch: 61, Steps: 137 | Train Loss: 0.2795215 Vali Loss: 0.2344741 Test Loss: 0.2664082
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2870347
	speed: 1.0151s/iter; left time: 5323.1425s
Epoch: 62 cost time: 60.206366300582886
Epoch: 62, Steps: 137 | Train Loss: 0.2796159 Vali Loss: 0.2338637 Test Loss: 0.2663965
Validation loss decreased (0.233973 --> 0.233864).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2826652
	speed: 1.0057s/iter; left time: 5136.1832s
Epoch: 63 cost time: 59.452462673187256
Epoch: 63, Steps: 137 | Train Loss: 0.2795209 Vali Loss: 0.2345581 Test Loss: 0.2663841
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2845974
	speed: 1.0090s/iter; left time: 5014.7394s
Epoch: 64 cost time: 59.56064319610596
Epoch: 64, Steps: 137 | Train Loss: 0.2795765 Vali Loss: 0.2344865 Test Loss: 0.2663767
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2801803
	speed: 1.0057s/iter; left time: 4860.4503s
Epoch: 65 cost time: 57.047133922576904
Epoch: 65, Steps: 137 | Train Loss: 0.2795319 Vali Loss: 0.2340256 Test Loss: 0.2663675
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2924199
	speed: 1.0070s/iter; left time: 4728.8389s
Epoch: 66 cost time: 57.41568088531494
Epoch: 66, Steps: 137 | Train Loss: 0.2795301 Vali Loss: 0.2336318 Test Loss: 0.2663616
Validation loss decreased (0.233864 --> 0.233632).  Saving model ...
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2778239
	speed: 1.0363s/iter; left time: 4724.4331s
Epoch: 67 cost time: 61.116408586502075
Epoch: 67, Steps: 137 | Train Loss: 0.2795722 Vali Loss: 0.2337599 Test Loss: 0.2663555
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2813297
	speed: 0.9902s/iter; left time: 4378.7133s
Epoch: 68 cost time: 57.51103091239929
Epoch: 68, Steps: 137 | Train Loss: 0.2794555 Vali Loss: 0.2344540 Test Loss: 0.2663451
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2757638
	speed: 0.9376s/iter; left time: 4017.4444s
Epoch: 69 cost time: 52.86082696914673
Epoch: 69, Steps: 137 | Train Loss: 0.2795191 Vali Loss: 0.2343017 Test Loss: 0.2663378
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2752149
	speed: 0.8594s/iter; left time: 3564.8175s
Epoch: 70 cost time: 47.75137424468994
Epoch: 70, Steps: 137 | Train Loss: 0.2794602 Vali Loss: 0.2341457 Test Loss: 0.2663337
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2878207
	speed: 0.8615s/iter; left time: 3455.4964s
Epoch: 71 cost time: 49.62503361701965
Epoch: 71, Steps: 137 | Train Loss: 0.2794314 Vali Loss: 0.2342434 Test Loss: 0.2663290
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2823901
	speed: 0.8940s/iter; left time: 3463.3954s
Epoch: 72 cost time: 49.8160183429718
Epoch: 72, Steps: 137 | Train Loss: 0.2795034 Vali Loss: 0.2338307 Test Loss: 0.2663206
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2778837
	speed: 0.8106s/iter; left time: 3029.2549s
Epoch: 73 cost time: 44.817004442214966
Epoch: 73, Steps: 137 | Train Loss: 0.2795487 Vali Loss: 0.2337793 Test Loss: 0.2663177
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.2752432
	speed: 0.7862s/iter; left time: 2830.2162s
Epoch: 74 cost time: 43.7131929397583
Epoch: 74, Steps: 137 | Train Loss: 0.2794198 Vali Loss: 0.2341162 Test Loss: 0.2663116
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2656730
	speed: 0.7604s/iter; left time: 2633.2042s
Epoch: 75 cost time: 43.794975996017456
Epoch: 75, Steps: 137 | Train Loss: 0.2794633 Vali Loss: 0.2339014 Test Loss: 0.2663104
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2753929
	speed: 0.7636s/iter; left time: 2539.7206s
Epoch: 76 cost time: 43.93708252906799
Epoch: 76, Steps: 137 | Train Loss: 0.2794278 Vali Loss: 0.2341949 Test Loss: 0.2663050
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2893232
	speed: 0.7661s/iter; left time: 2443.1459s
Epoch: 77 cost time: 43.40470767021179
Epoch: 77, Steps: 137 | Train Loss: 0.2795359 Vali Loss: 0.2344359 Test Loss: 0.2663003
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.2837183
	speed: 0.7585s/iter; left time: 2315.0928s
Epoch: 78 cost time: 43.5497407913208
Epoch: 78, Steps: 137 | Train Loss: 0.2793876 Vali Loss: 0.2343963 Test Loss: 0.2662975
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.2715648
	speed: 0.7571s/iter; left time: 2207.0665s
Epoch: 79 cost time: 42.60974621772766
Epoch: 79, Steps: 137 | Train Loss: 0.2794031 Vali Loss: 0.2343388 Test Loss: 0.2662953
EarlyStopping counter: 13 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.2784981
	speed: 0.7518s/iter; left time: 2088.5270s
Epoch: 80 cost time: 43.38040518760681
Epoch: 80, Steps: 137 | Train Loss: 0.2793807 Vali Loss: 0.2343453 Test Loss: 0.2662939
EarlyStopping counter: 14 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.2773704
	speed: 0.7074s/iter; left time: 1868.1216s
Epoch: 81 cost time: 37.08724808692932
Epoch: 81, Steps: 137 | Train Loss: 0.2794659 Vali Loss: 0.2340111 Test Loss: 0.2662898
EarlyStopping counter: 15 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.2777991
	speed: 0.6586s/iter; left time: 1649.1564s
Epoch: 82 cost time: 36.34733176231384
Epoch: 82, Steps: 137 | Train Loss: 0.2793388 Vali Loss: 0.2334123 Test Loss: 0.2662871
Validation loss decreased (0.233632 --> 0.233412).  Saving model ...
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.3018380
	speed: 0.6521s/iter; left time: 1543.5631s
Epoch: 83 cost time: 36.658971548080444
Epoch: 83, Steps: 137 | Train Loss: 0.2795079 Vali Loss: 0.2344319 Test Loss: 0.2662829
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.2966425
	speed: 0.6470s/iter; left time: 1442.8235s
Epoch: 84 cost time: 36.43733096122742
Epoch: 84, Steps: 137 | Train Loss: 0.2793082 Vali Loss: 0.2336833 Test Loss: 0.2662817
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.2824580
	speed: 0.6664s/iter; left time: 1394.7330s
Epoch: 85 cost time: 36.01729893684387
Epoch: 85, Steps: 137 | Train Loss: 0.2794475 Vali Loss: 0.2343840 Test Loss: 0.2662801
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.2907930
	speed: 0.6625s/iter; left time: 1295.9018s
Epoch: 86 cost time: 37.03783130645752
Epoch: 86, Steps: 137 | Train Loss: 0.2795087 Vali Loss: 0.2341466 Test Loss: 0.2662780
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.2679462
	speed: 0.6405s/iter; left time: 1165.0335s
Epoch: 87 cost time: 35.04585337638855
Epoch: 87, Steps: 137 | Train Loss: 0.2793502 Vali Loss: 0.2343690 Test Loss: 0.2662765
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.2752230
	speed: 0.6713s/iter; left time: 1129.1758s
Epoch: 88 cost time: 36.75688910484314
Epoch: 88, Steps: 137 | Train Loss: 0.2794212 Vali Loss: 0.2338977 Test Loss: 0.2662755
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.766650946003179e-06
	iters: 100, epoch: 89 | loss: 0.2897068
	speed: 0.6497s/iter; left time: 1003.7441s
Epoch: 89 cost time: 36.766995668411255
Epoch: 89, Steps: 137 | Train Loss: 0.2794019 Vali Loss: 0.2342563 Test Loss: 0.2662725
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.47831839870302e-06
	iters: 100, epoch: 90 | loss: 0.2688390
	speed: 0.6537s/iter; left time: 920.4779s
Epoch: 90 cost time: 35.83327913284302
Epoch: 90, Steps: 137 | Train Loss: 0.2794351 Vali Loss: 0.2336880 Test Loss: 0.2662715
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.204402478767869e-06
	iters: 100, epoch: 91 | loss: 0.2817760
	speed: 0.6519s/iter; left time: 828.5126s
Epoch: 91 cost time: 37.21815299987793
Epoch: 91, Steps: 137 | Train Loss: 0.2794031 Vali Loss: 0.2339889 Test Loss: 0.2662708
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.944182354829475e-06
	iters: 100, epoch: 92 | loss: 0.2674214
	speed: 0.6405s/iter; left time: 726.3169s
Epoch: 92 cost time: 35.98041129112244
Epoch: 92, Steps: 137 | Train Loss: 0.2794247 Vali Loss: 0.2345220 Test Loss: 0.2662698
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.696973237088e-06
	iters: 100, epoch: 93 | loss: 0.2853279
	speed: 0.6702s/iter; left time: 668.2061s
Epoch: 93 cost time: 36.90411043167114
Epoch: 93, Steps: 137 | Train Loss: 0.2793857 Vali Loss: 0.2337692 Test Loss: 0.2662691
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.462124575233601e-06
	iters: 100, epoch: 94 | loss: 0.2666880
	speed: 0.6492s/iter; left time: 558.3073s
Epoch: 94 cost time: 36.77010655403137
Epoch: 94, Steps: 137 | Train Loss: 0.2794683 Vali Loss: 0.2339699 Test Loss: 0.2662672
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.239018346471921e-06
	iters: 100, epoch: 95 | loss: 0.2783223
	speed: 0.6588s/iter; left time: 476.3187s
Epoch: 95 cost time: 36.64185547828674
Epoch: 95, Steps: 137 | Train Loss: 0.2793985 Vali Loss: 0.2344279 Test Loss: 0.2662660
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.027067429148324e-06
	iters: 100, epoch: 96 | loss: 0.2750478
	speed: 0.6589s/iter; left time: 386.1172s
Epoch: 96 cost time: 36.092449426651
Epoch: 96, Steps: 137 | Train Loss: 0.2794174 Vali Loss: 0.2338578 Test Loss: 0.2662652
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.825714057690908e-06
	iters: 100, epoch: 97 | loss: 0.2778229
	speed: 0.6577s/iter; left time: 295.2850s
Epoch: 97 cost time: 36.57320308685303
Epoch: 97, Steps: 137 | Train Loss: 0.2794209 Vali Loss: 0.2343655 Test Loss: 0.2662642
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.6344283548063623e-06
	iters: 100, epoch: 98 | loss: 0.2935054
	speed: 0.6563s/iter; left time: 204.7725s
Epoch: 98 cost time: 36.16336274147034
Epoch: 98, Steps: 137 | Train Loss: 0.2793215 Vali Loss: 0.2343753 Test Loss: 0.2662630
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.452706937066044e-06
	iters: 100, epoch: 99 | loss: 0.2784452
	speed: 0.6570s/iter; left time: 114.9779s
Epoch: 99 cost time: 36.43808150291443
Epoch: 99, Steps: 137 | Train Loss: 0.2793759 Vali Loss: 0.2341862 Test Loss: 0.2662631
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.2800715902127414e-06
	iters: 100, epoch: 100 | loss: 0.2814038
	speed: 0.6531s/iter; left time: 24.8189s
Epoch: 100 cost time: 36.64770030975342
Epoch: 100, Steps: 137 | Train Loss: 0.2794261 Vali Loss: 0.2340329 Test Loss: 0.2662621
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : Electricity_90_j720_H4_FITS_custom_ftM_sl90_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.26497283577919006, mae:0.337246298789978, rse:0.5134853720664978, corr:[0.44281864 0.4394098  0.4388454  0.43912637 0.43764088 0.4363543
 0.43736532 0.4364064  0.43460152 0.4339387  0.43349394 0.43273687
 0.43234217 0.431974   0.43165332 0.4314768  0.43137413 0.4316971
 0.43208778 0.43193203 0.43179202 0.4320195  0.43195978 0.43076628
 0.42838827 0.42729372 0.42640796 0.4252179  0.42447057 0.42462805
 0.4258258  0.42572865 0.42525032 0.4251179  0.4248322  0.424234
 0.42417437 0.42423534 0.42393413 0.42369115 0.42372382 0.42391878
 0.42406857 0.42412335 0.42427576 0.42451954 0.4245648  0.4241801
 0.4229733  0.4226779  0.4227355  0.42281157 0.42336684 0.42476425
 0.42711082 0.42846864 0.42882308 0.42876783 0.42865255 0.4282555
 0.42803603 0.42802233 0.42794734 0.42802095 0.42827758 0.42855814
 0.42880693 0.42903683 0.42940494 0.42980447 0.4302213  0.4306303
 0.43062705 0.43119925 0.43195197 0.4330258  0.43463585 0.4370195
 0.44031852 0.44272918 0.44344097 0.44297722 0.44262463 0.4422058
 0.44179228 0.44161892 0.4415461  0.44162405 0.44189298 0.44213572
 0.44237056 0.44252157 0.44256103 0.44268534 0.44292206 0.44294384
 0.44252476 0.44253412 0.44257343 0.44243157 0.442345   0.44249755
 0.44300154 0.44302112 0.4427164  0.44225067 0.44194508 0.44164923
 0.44141096 0.44136155 0.4413512  0.4414105  0.44164622 0.44181854
 0.44191217 0.44203585 0.44209453 0.4421386  0.4423324  0.4424685
 0.442127   0.44213116 0.44221008 0.44211355 0.44197437 0.44199112
 0.44232646 0.44220233 0.44191775 0.4415991  0.44132295 0.44107008
 0.44091856 0.44095358 0.4410395  0.44113255 0.44140217 0.44169697
 0.44190368 0.44213676 0.44227567 0.4423278  0.44243965 0.44244277
 0.44192436 0.4417481  0.44178432 0.44174296 0.44163635 0.44165495
 0.44203198 0.44201028 0.44169632 0.44124678 0.44090986 0.44067675
 0.44055122 0.44062537 0.44082287 0.44098976 0.44131863 0.4417838
 0.44205847 0.44209847 0.44203207 0.44168076 0.44054908 0.43832412
 0.43531078 0.43307516 0.43148875 0.43002993 0.42889202 0.42824546
 0.4282833  0.42793477 0.42757174 0.42711276 0.4267377  0.42653316
 0.42642042 0.42626777 0.42609847 0.42586523 0.42590693 0.42605633
 0.42596823 0.4258279  0.42568767 0.4253156  0.42448807 0.42300692
 0.42070782 0.41921332 0.4182256  0.41736606 0.41687167 0.41715315
 0.4183094  0.41874948 0.4188621  0.4186099  0.4183069  0.41820064
 0.41824213 0.41816103 0.418021   0.4178695  0.41788676 0.41794696
 0.4178655  0.41790256 0.4180178  0.4179413  0.417683   0.4173015
 0.41624323 0.41572195 0.41573668 0.4160667  0.41679576 0.41829607
 0.42058948 0.42220396 0.4229801  0.42293116 0.42266098 0.42254415
 0.42243773 0.42224106 0.4221784  0.42229876 0.42253044 0.42271534
 0.42284006 0.42310733 0.42348537 0.4237313  0.42401823 0.42442736
 0.42447478 0.42498165 0.42573798 0.42690444 0.4285587  0.4310268
 0.43432334 0.4368224  0.43782377 0.4375017  0.43701705 0.436763
 0.43654114 0.43626493 0.436138   0.43624175 0.43652046 0.43672648
 0.43684486 0.43703666 0.43719622 0.43727398 0.43740085 0.4374512
 0.43713105 0.43705612 0.43702593 0.4369831  0.4369739  0.43718082
 0.43768343 0.4377145  0.43754858 0.43720764 0.4367657  0.43647003
 0.4363624  0.43623126 0.43614468 0.43617746 0.43638682 0.4365827
 0.43661898 0.4366859  0.4368043  0.43685576 0.4369669  0.4370875
 0.43684757 0.4368442  0.436848   0.4367821  0.43672302 0.43680164
 0.43715546 0.43700293 0.4367518  0.43654457 0.4361769  0.4358929
 0.4358653  0.435853   0.4358657  0.43599012 0.43626136 0.43654937
 0.4366943  0.43687958 0.4371156  0.43722022 0.43721348 0.43710524
 0.43666607 0.43650517 0.4364231  0.43636084 0.43634927 0.4364304
 0.43682146 0.4367543  0.43648684 0.43620375 0.43581337 0.43548292
 0.43544525 0.43549442 0.43558994 0.43576148 0.4360641  0.43648762
 0.43669865 0.43659827 0.43645567 0.43611094 0.43486848 0.43238002
 0.42929587 0.42716286 0.42548814 0.42393795 0.42291263 0.42235342
 0.42237294 0.42200878 0.42163992 0.4212376  0.4207858  0.42045364
 0.420371   0.42021173 0.41995    0.41972977 0.4197845  0.4198973
 0.4198156  0.41962624 0.4194855  0.41922835 0.41834548 0.41655543
 0.4140802  0.41263515 0.41156977 0.41060564 0.410235   0.41062465
 0.41176993 0.4121935  0.41226116 0.41204256 0.4117135  0.4114412
 0.41148907 0.41149715 0.41127232 0.4110546  0.41105667 0.41111878
 0.41106611 0.41107154 0.4111751  0.41123912 0.411025   0.41044343
 0.40924633 0.40884998 0.4089717  0.40935206 0.41027835 0.41198742
 0.41442698 0.41613665 0.41687396 0.41686672 0.41669604 0.41646624
 0.41632712 0.4162696  0.4161975  0.41621184 0.4164226  0.41666883
 0.41684082 0.4170616  0.4173985  0.41776636 0.41811034 0.4184211
 0.4184169  0.41906658 0.4199873  0.4212734  0.42313522 0.42583033
 0.4292744  0.4319036  0.4329476  0.43271625 0.43243217 0.43214312
 0.43180215 0.43159625 0.43154266 0.43160537 0.43183672 0.43206885
 0.43226522 0.43242276 0.43250963 0.43264756 0.43288994 0.43292674
 0.4325199  0.43249136 0.43257323 0.43255225 0.43260726 0.43288982
 0.43343067 0.4335148  0.43331242 0.43289408 0.43252006 0.4322165
 0.43198103 0.43187654 0.4318548  0.4318525  0.43201953 0.43221202
 0.4322998  0.43238625 0.43245155 0.43254143 0.4327678  0.43291676
 0.43261236 0.4326278  0.43274242 0.4327233  0.43268374 0.43279696
 0.43314627 0.43303484 0.4327702  0.4324546  0.432135   0.43189552
 0.4317731  0.43175873 0.43184313 0.43194956 0.43218625 0.4324462
 0.43263495 0.43288857 0.4330866  0.43319592 0.43332386 0.4333076
 0.4328052  0.43263546 0.43269792 0.43269885 0.4326592  0.43275663
 0.43315586 0.43314424 0.43286526 0.43246207 0.43211663 0.4319094
 0.43180043 0.43181708 0.43197387 0.4321231  0.4324308  0.43285373
 0.43305996 0.43301722 0.43290812 0.43253222 0.4313058  0.42892435
 0.42578048 0.42344958 0.42185578 0.4204915  0.41946036 0.4188937
 0.4189553  0.41859186 0.41818407 0.4177079  0.41728103 0.41705692
 0.41695663 0.41676107 0.4165575  0.41632506 0.4163555  0.41647148
 0.41635543 0.41619027 0.41607317 0.41571265 0.41475046 0.4130264
 0.41052398 0.40888152 0.40789413 0.40711215 0.40670967 0.40703884
 0.40812144 0.4084688  0.40846676 0.40813315 0.4077577  0.40764663
 0.40770566 0.40757167 0.40737006 0.40722185 0.40722963 0.4073001
 0.40721765 0.4072555  0.40745315 0.40745276 0.40715587 0.40664935
 0.40551555 0.40494463 0.40499264 0.40542388 0.4062988  0.4079074
 0.41030207 0.41192198 0.41265106 0.4125779  0.4122252  0.41204697
 0.4119615  0.4117151  0.41156304 0.41162518 0.41178444 0.4118695
 0.41193566 0.41222334 0.41264337 0.41294134 0.41320682 0.41355613
 0.41363803 0.41420248 0.41507038 0.41635767 0.41812432 0.42069897
 0.4240559  0.42653129 0.42755267 0.42732042 0.42680895 0.42650592
 0.42632785 0.4260711  0.42592204 0.42603144 0.42629358 0.42648724
 0.42454168 0.42266268 0.42701852 0.42716774 0.4231293  0.42315188
 0.4228398  0.4227794  0.42277625 0.42278334 0.42283088 0.42305022
 0.42352438 0.42347747 0.4232565  0.42292285 0.42242342 0.42206967
 0.42198402 0.42183676 0.42168176 0.42174304 0.42197022 0.42214048
 0.42219782 0.42230955 0.4224741  0.422582   0.4227155  0.42284387
 0.42266974 0.42270267 0.42271954 0.42270485 0.42270014 0.42278755
 0.42311683 0.4229115  0.42261901 0.4223994  0.42195678 0.4215932
 0.42160124 0.42160618 0.42155054 0.42167833 0.42196596 0.422219
 0.4223404  0.4225321  0.42282218 0.42302465 0.4230501  0.42289943
 0.42251402 0.4224417  0.42234606 0.4223141  0.42240825 0.42249176
 0.4227993  0.42264587 0.42233175 0.42206526 0.42164043 0.42125666
 0.42126337 0.4213158  0.42127663 0.42145458 0.42178673 0.42209086
 0.42222846 0.42213005 0.4220515  0.4218229  0.42063195 0.41794273
 0.4146986  0.41261563 0.41084528 0.40926588 0.4085138  0.4080473
 0.40796152 0.4075517  0.40720835 0.40676048 0.40617567 0.4057306
 0.40570104 0.40551507 0.40502372 0.40479213 0.40491757 0.40499458
 0.4049217  0.40475315 0.40465587 0.40457535 0.4038003  0.40180767
 0.3993605  0.3982608  0.39704105 0.39594662 0.39601707 0.39640644
 0.39713436 0.3974086  0.3975036  0.39715865 0.39654276 0.3960614
 0.39624998 0.3962088  0.39550734 0.39544672 0.39604717 0.3959433
 0.39551264 0.3958108  0.39639613 0.39584517 0.39565665 0.39716753]
