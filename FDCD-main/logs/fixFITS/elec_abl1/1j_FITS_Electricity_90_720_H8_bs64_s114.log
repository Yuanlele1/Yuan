Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_90_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_90_j720_H8_FITS_custom_ftM_sl90_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17603
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=42, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  652313088.0
params:  16254.0
Trainable parameters:  16254
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.9644446
	speed: 0.5675s/iter; left time: 7718.2093s
Epoch: 1 cost time: 76.2536141872406
Epoch: 1, Steps: 137 | Train Loss: 1.4456315 Vali Loss: 0.6554478 Test Loss: 0.7203459
Validation loss decreased (inf --> 0.655448).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5129743
	speed: 1.3344s/iter; left time: 17965.7487s
Epoch: 2 cost time: 76.78836226463318
Epoch: 2, Steps: 137 | Train Loss: 0.5819833 Vali Loss: 0.4005896 Test Loss: 0.4455000
Validation loss decreased (0.655448 --> 0.400590).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3765581
	speed: 1.3384s/iter; left time: 17836.6461s
Epoch: 3 cost time: 75.86526465415955
Epoch: 3, Steps: 137 | Train Loss: 0.4140637 Vali Loss: 0.3179387 Test Loss: 0.3563461
Validation loss decreased (0.400590 --> 0.317939).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3366711
	speed: 1.3468s/iter; left time: 17764.1829s
Epoch: 4 cost time: 75.60535168647766
Epoch: 4, Steps: 137 | Train Loss: 0.3518888 Vali Loss: 0.2826307 Test Loss: 0.3177567
Validation loss decreased (0.317939 --> 0.282631).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3206488
	speed: 1.3525s/iter; left time: 17653.7306s
Epoch: 5 cost time: 76.87137913703918
Epoch: 5, Steps: 137 | Train Loss: 0.3228817 Vali Loss: 0.2644656 Test Loss: 0.2980561
Validation loss decreased (0.282631 --> 0.264466).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2930573
	speed: 1.3377s/iter; left time: 17278.0137s
Epoch: 6 cost time: 79.18444418907166
Epoch: 6, Steps: 137 | Train Loss: 0.3072377 Vali Loss: 0.2539424 Test Loss: 0.2867265
Validation loss decreased (0.264466 --> 0.253942).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2940882
	speed: 1.5723s/iter; left time: 20092.7175s
Epoch: 7 cost time: 90.29673957824707
Epoch: 7, Steps: 137 | Train Loss: 0.2978508 Vali Loss: 0.2470885 Test Loss: 0.2796694
Validation loss decreased (0.253942 --> 0.247089).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2779841
	speed: 1.6839s/iter; left time: 21287.8030s
Epoch: 8 cost time: 94.51191735267639
Epoch: 8, Steps: 137 | Train Loss: 0.2918245 Vali Loss: 0.2428158 Test Loss: 0.2750400
Validation loss decreased (0.247089 --> 0.242816).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2942191
	speed: 1.5595s/iter; left time: 19501.6034s
Epoch: 9 cost time: 86.67903709411621
Epoch: 9, Steps: 137 | Train Loss: 0.2878984 Vali Loss: 0.2397132 Test Loss: 0.2718606
Validation loss decreased (0.242816 --> 0.239713).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2998489
	speed: 1.5273s/iter; left time: 18889.4626s
Epoch: 10 cost time: 87.59837126731873
Epoch: 10, Steps: 137 | Train Loss: 0.2850913 Vali Loss: 0.2377206 Test Loss: 0.2696279
Validation loss decreased (0.239713 --> 0.237721).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2855530
	speed: 1.5966s/iter; left time: 19527.8370s
Epoch: 11 cost time: 89.99625396728516
Epoch: 11, Steps: 137 | Train Loss: 0.2830641 Vali Loss: 0.2360771 Test Loss: 0.2680209
Validation loss decreased (0.237721 --> 0.236077).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2685826
	speed: 1.5912s/iter; left time: 19244.0413s
Epoch: 12 cost time: 86.97197341918945
Epoch: 12, Steps: 137 | Train Loss: 0.2815894 Vali Loss: 0.2348726 Test Loss: 0.2668176
Validation loss decreased (0.236077 --> 0.234873).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2678746
	speed: 1.5772s/iter; left time: 18859.1330s
Epoch: 13 cost time: 89.1212899684906
Epoch: 13, Steps: 137 | Train Loss: 0.2803868 Vali Loss: 0.2339971 Test Loss: 0.2659049
Validation loss decreased (0.234873 --> 0.233997).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2743731
	speed: 1.5872s/iter; left time: 18760.8715s
Epoch: 14 cost time: 87.30041456222534
Epoch: 14, Steps: 137 | Train Loss: 0.2796407 Vali Loss: 0.2335692 Test Loss: 0.2652018
Validation loss decreased (0.233997 --> 0.233569).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2761074
	speed: 1.5535s/iter; left time: 18149.0195s
Epoch: 15 cost time: 87.54042887687683
Epoch: 15, Steps: 137 | Train Loss: 0.2789473 Vali Loss: 0.2333287 Test Loss: 0.2646241
Validation loss decreased (0.233569 --> 0.233329).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2961639
	speed: 1.5067s/iter; left time: 17395.9202s
Epoch: 16 cost time: 84.55428576469421
Epoch: 16, Steps: 137 | Train Loss: 0.2784366 Vali Loss: 0.2326276 Test Loss: 0.2641885
Validation loss decreased (0.233329 --> 0.232628).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2800149
	speed: 1.5180s/iter; left time: 17319.1199s
Epoch: 17 cost time: 87.31885814666748
Epoch: 17, Steps: 137 | Train Loss: 0.2779900 Vali Loss: 0.2325688 Test Loss: 0.2638246
Validation loss decreased (0.232628 --> 0.232569).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2854527
	speed: 1.4382s/iter; left time: 16211.3863s
Epoch: 18 cost time: 83.65192723274231
Epoch: 18, Steps: 137 | Train Loss: 0.2775857 Vali Loss: 0.2320245 Test Loss: 0.2634890
Validation loss decreased (0.232569 --> 0.232025).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2788529
	speed: 1.4687s/iter; left time: 16353.5362s
Epoch: 19 cost time: 81.02700281143188
Epoch: 19, Steps: 137 | Train Loss: 0.2772868 Vali Loss: 0.2313079 Test Loss: 0.2632102
Validation loss decreased (0.232025 --> 0.231308).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2707737
	speed: 1.4260s/iter; left time: 15683.4417s
Epoch: 20 cost time: 79.60786461830139
Epoch: 20, Steps: 137 | Train Loss: 0.2770639 Vali Loss: 0.2320414 Test Loss: 0.2629887
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2692341
	speed: 1.6017s/iter; left time: 17396.4861s
Epoch: 21 cost time: 103.8932695388794
Epoch: 21, Steps: 137 | Train Loss: 0.2768464 Vali Loss: 0.2312639 Test Loss: 0.2627999
Validation loss decreased (0.231308 --> 0.231264).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2719184
	speed: 1.8546s/iter; left time: 19888.9728s
Epoch: 22 cost time: 107.76611328125
Epoch: 22, Steps: 137 | Train Loss: 0.2765002 Vali Loss: 0.2308303 Test Loss: 0.2626028
Validation loss decreased (0.231264 --> 0.230830).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2695518
	speed: 1.8830s/iter; left time: 19934.8007s
Epoch: 23 cost time: 104.99050164222717
Epoch: 23, Steps: 137 | Train Loss: 0.2763785 Vali Loss: 0.2311109 Test Loss: 0.2624483
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2784345
	speed: 1.9262s/iter; left time: 20129.2397s
Epoch: 24 cost time: 107.60291695594788
Epoch: 24, Steps: 137 | Train Loss: 0.2762215 Vali Loss: 0.2310750 Test Loss: 0.2623221
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2699310
	speed: 1.8004s/iter; left time: 18567.9024s
Epoch: 25 cost time: 101.05536246299744
Epoch: 25, Steps: 137 | Train Loss: 0.2760933 Vali Loss: 0.2308649 Test Loss: 0.2621742
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2873017
	speed: 1.8153s/iter; left time: 18472.5671s
Epoch: 26 cost time: 103.50213694572449
Epoch: 26, Steps: 137 | Train Loss: 0.2759038 Vali Loss: 0.2312353 Test Loss: 0.2620629
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2750017
	speed: 1.9044s/iter; left time: 19118.3875s
Epoch: 27 cost time: 106.71691012382507
Epoch: 27, Steps: 137 | Train Loss: 0.2758376 Vali Loss: 0.2309700 Test Loss: 0.2619701
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2671946
	speed: 1.8626s/iter; left time: 18443.5954s
Epoch: 28 cost time: 104.75109720230103
Epoch: 28, Steps: 137 | Train Loss: 0.2756570 Vali Loss: 0.2296283 Test Loss: 0.2618794
Validation loss decreased (0.230830 --> 0.229628).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2873811
	speed: 1.8844s/iter; left time: 18400.7843s
Epoch: 29 cost time: 109.30676484107971
Epoch: 29, Steps: 137 | Train Loss: 0.2756045 Vali Loss: 0.2304049 Test Loss: 0.2617768
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2805017
	speed: 1.8803s/iter; left time: 18103.2336s
Epoch: 30 cost time: 105.14487791061401
Epoch: 30, Steps: 137 | Train Loss: 0.2755082 Vali Loss: 0.2304276 Test Loss: 0.2617065
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2640413
	speed: 1.8028s/iter; left time: 17110.2441s
Epoch: 31 cost time: 103.95541024208069
Epoch: 31, Steps: 137 | Train Loss: 0.2754518 Vali Loss: 0.2300953 Test Loss: 0.2616494
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2729318
	speed: 1.8529s/iter; left time: 17332.2113s
Epoch: 32 cost time: 109.44543862342834
Epoch: 32, Steps: 137 | Train Loss: 0.2753921 Vali Loss: 0.2302821 Test Loss: 0.2615854
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2721934
	speed: 1.8568s/iter; left time: 17114.3957s
Epoch: 33 cost time: 104.9877998828888
Epoch: 33, Steps: 137 | Train Loss: 0.2753158 Vali Loss: 0.2301260 Test Loss: 0.2615151
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2782992
	speed: 1.8826s/iter; left time: 17093.7450s
Epoch: 34 cost time: 105.15593361854553
Epoch: 34, Steps: 137 | Train Loss: 0.2752233 Vali Loss: 0.2305189 Test Loss: 0.2614721
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2798493
	speed: 1.9199s/iter; left time: 17169.3906s
Epoch: 35 cost time: 106.33357119560242
Epoch: 35, Steps: 137 | Train Loss: 0.2751902 Vali Loss: 0.2302705 Test Loss: 0.2614388
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2741173
	speed: 1.9094s/iter; left time: 16813.9541s
Epoch: 36 cost time: 108.69312524795532
Epoch: 36, Steps: 137 | Train Loss: 0.2751156 Vali Loss: 0.2299561 Test Loss: 0.2613851
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2706984
	speed: 1.8445s/iter; left time: 15989.5963s
Epoch: 37 cost time: 103.08785033226013
Epoch: 37, Steps: 137 | Train Loss: 0.2751305 Vali Loss: 0.2299030 Test Loss: 0.2613472
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2752366
	speed: 1.8535s/iter; left time: 15813.8746s
Epoch: 38 cost time: 106.52665662765503
Epoch: 38, Steps: 137 | Train Loss: 0.2749869 Vali Loss: 0.2302936 Test Loss: 0.2613188
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2747755
	speed: 1.8838s/iter; left time: 15814.2490s
Epoch: 39 cost time: 103.78931403160095
Epoch: 39, Steps: 137 | Train Loss: 0.2750264 Vali Loss: 0.2298833 Test Loss: 0.2612728
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2832722
	speed: 1.8620s/iter; left time: 15376.5723s
Epoch: 40 cost time: 104.8985002040863
Epoch: 40, Steps: 137 | Train Loss: 0.2749450 Vali Loss: 0.2300195 Test Loss: 0.2612438
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2723058
	speed: 1.8091s/iter; left time: 14691.8212s
Epoch: 41 cost time: 102.97015047073364
Epoch: 41, Steps: 137 | Train Loss: 0.2749129 Vali Loss: 0.2296352 Test Loss: 0.2612256
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2726767
	speed: 1.5934s/iter; left time: 12721.9221s
Epoch: 42 cost time: 86.29542326927185
Epoch: 42, Steps: 137 | Train Loss: 0.2748756 Vali Loss: 0.2294394 Test Loss: 0.2612076
Validation loss decreased (0.229628 --> 0.229439).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2782572
	speed: 1.5335s/iter; left time: 12033.5966s
Epoch: 43 cost time: 85.1489028930664
Epoch: 43, Steps: 137 | Train Loss: 0.2749270 Vali Loss: 0.2298641 Test Loss: 0.2611789
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2637198
	speed: 1.4437s/iter; left time: 11130.9868s
Epoch: 44 cost time: 80.51743006706238
Epoch: 44, Steps: 137 | Train Loss: 0.2748034 Vali Loss: 0.2296428 Test Loss: 0.2611636
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2849743
	speed: 1.4203s/iter; left time: 10755.7918s
Epoch: 45 cost time: 80.60474348068237
Epoch: 45, Steps: 137 | Train Loss: 0.2748459 Vali Loss: 0.2296784 Test Loss: 0.2611330
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2606489
	speed: 1.4955s/iter; left time: 11120.2785s
Epoch: 46 cost time: 82.12336754798889
Epoch: 46, Steps: 137 | Train Loss: 0.2748084 Vali Loss: 0.2296842 Test Loss: 0.2611213
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2786052
	speed: 1.4433s/iter; left time: 10534.8070s
Epoch: 47 cost time: 80.17851042747498
Epoch: 47, Steps: 137 | Train Loss: 0.2747659 Vali Loss: 0.2294168 Test Loss: 0.2611069
Validation loss decreased (0.229439 --> 0.229417).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2751386
	speed: 1.4393s/iter; left time: 10307.9491s
Epoch: 48 cost time: 80.20750880241394
Epoch: 48, Steps: 137 | Train Loss: 0.2748036 Vali Loss: 0.2295422 Test Loss: 0.2610925
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2726937
	speed: 1.3791s/iter; left time: 9688.0335s
Epoch: 49 cost time: 77.27850818634033
Epoch: 49, Steps: 137 | Train Loss: 0.2747801 Vali Loss: 0.2293001 Test Loss: 0.2610863
Validation loss decreased (0.229417 --> 0.229300).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2773040
	speed: 1.4021s/iter; left time: 9657.9279s
Epoch: 50 cost time: 80.40191459655762
Epoch: 50, Steps: 137 | Train Loss: 0.2746675 Vali Loss: 0.2294248 Test Loss: 0.2610705
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2769447
	speed: 1.3439s/iter; left time: 9072.4471s
Epoch: 51 cost time: 70.75061631202698
Epoch: 51, Steps: 137 | Train Loss: 0.2747902 Vali Loss: 0.2299107 Test Loss: 0.2610543
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2681621
	speed: 1.2819s/iter; left time: 8478.6670s
Epoch: 52 cost time: 68.53910899162292
Epoch: 52, Steps: 137 | Train Loss: 0.2747080 Vali Loss: 0.2295945 Test Loss: 0.2610505
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2724560
	speed: 1.1877s/iter; left time: 7693.0392s
Epoch: 53 cost time: 65.93459987640381
Epoch: 53, Steps: 137 | Train Loss: 0.2746863 Vali Loss: 0.2299568 Test Loss: 0.2610496
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2890999
	speed: 1.2263s/iter; left time: 7774.7226s
Epoch: 54 cost time: 67.76551151275635
Epoch: 54, Steps: 137 | Train Loss: 0.2746868 Vali Loss: 0.2300657 Test Loss: 0.2610398
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2737980
	speed: 1.2053s/iter; left time: 7476.4115s
Epoch: 55 cost time: 69.44019150733948
Epoch: 55, Steps: 137 | Train Loss: 0.2747960 Vali Loss: 0.2292165 Test Loss: 0.2610254
Validation loss decreased (0.229300 --> 0.229217).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2765833
	speed: 1.2408s/iter; left time: 7526.5874s
Epoch: 56 cost time: 69.7268807888031
Epoch: 56, Steps: 137 | Train Loss: 0.2746610 Vali Loss: 0.2295393 Test Loss: 0.2610211
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2887988
	speed: 1.1533s/iter; left time: 6837.6229s
Epoch: 57 cost time: 63.95819282531738
Epoch: 57, Steps: 137 | Train Loss: 0.2747102 Vali Loss: 0.2298911 Test Loss: 0.2610186
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2677337
	speed: 1.4378s/iter; left time: 8327.7529s
Epoch: 58 cost time: 80.89368605613708
Epoch: 58, Steps: 137 | Train Loss: 0.2747196 Vali Loss: 0.2295743 Test Loss: 0.2610165
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2722706
	speed: 1.4409s/iter; left time: 8148.5563s
Epoch: 59 cost time: 80.98343753814697
Epoch: 59, Steps: 137 | Train Loss: 0.2746770 Vali Loss: 0.2295442 Test Loss: 0.2610015
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2654566
	speed: 1.3920s/iter; left time: 7681.0684s
Epoch: 60 cost time: 76.87195372581482
Epoch: 60, Steps: 137 | Train Loss: 0.2747021 Vali Loss: 0.2293690 Test Loss: 0.2610038
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2815430
	speed: 1.4728s/iter; left time: 7925.3987s
Epoch: 61 cost time: 82.39304733276367
Epoch: 61, Steps: 137 | Train Loss: 0.2746440 Vali Loss: 0.2295832 Test Loss: 0.2610004
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2798947
	speed: 1.4111s/iter; left time: 7399.9609s
Epoch: 62 cost time: 81.55114936828613
Epoch: 62, Steps: 137 | Train Loss: 0.2746610 Vali Loss: 0.2291892 Test Loss: 0.2609968
Validation loss decreased (0.229217 --> 0.229189).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2719743
	speed: 1.4968s/iter; left time: 7644.2934s
Epoch: 63 cost time: 85.76758480072021
Epoch: 63, Steps: 137 | Train Loss: 0.2746799 Vali Loss: 0.2295241 Test Loss: 0.2609910
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2592145
	speed: 1.4164s/iter; left time: 7039.5149s
Epoch: 64 cost time: 79.08054327964783
Epoch: 64, Steps: 137 | Train Loss: 0.2746704 Vali Loss: 0.2289929 Test Loss: 0.2609862
Validation loss decreased (0.229189 --> 0.228993).  Saving model ...
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2760490
	speed: 1.4413s/iter; left time: 6965.5828s
Epoch: 65 cost time: 80.92327046394348
Epoch: 65, Steps: 137 | Train Loss: 0.2746958 Vali Loss: 0.2294027 Test Loss: 0.2609871
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2824328
	speed: 1.4427s/iter; left time: 6774.9952s
Epoch: 66 cost time: 81.52416849136353
Epoch: 66, Steps: 137 | Train Loss: 0.2746745 Vali Loss: 0.2297842 Test Loss: 0.2609814
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2667646
	speed: 1.4698s/iter; left time: 6700.8261s
Epoch: 67 cost time: 83.87432408332825
Epoch: 67, Steps: 137 | Train Loss: 0.2746483 Vali Loss: 0.2291507 Test Loss: 0.2609800
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2858779
	speed: 1.4976s/iter; left time: 6622.3298s
Epoch: 68 cost time: 86.04754900932312
Epoch: 68, Steps: 137 | Train Loss: 0.2746439 Vali Loss: 0.2296729 Test Loss: 0.2609771
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2794500
	speed: 1.4800s/iter; left time: 6342.0065s
Epoch: 69 cost time: 82.82208752632141
Epoch: 69, Steps: 137 | Train Loss: 0.2745386 Vali Loss: 0.2294054 Test Loss: 0.2609748
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2834579
	speed: 1.4876s/iter; left time: 6170.5234s
Epoch: 70 cost time: 84.76215291023254
Epoch: 70, Steps: 137 | Train Loss: 0.2746447 Vali Loss: 0.2298163 Test Loss: 0.2609695
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2732594
	speed: 1.4605s/iter; left time: 5858.1310s
Epoch: 71 cost time: 78.2806510925293
Epoch: 71, Steps: 137 | Train Loss: 0.2746511 Vali Loss: 0.2293609 Test Loss: 0.2609703
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2701533
	speed: 1.3697s/iter; left time: 5306.4028s
Epoch: 72 cost time: 78.03945994377136
Epoch: 72, Steps: 137 | Train Loss: 0.2746419 Vali Loss: 0.2296180 Test Loss: 0.2609711
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2722064
	speed: 1.3789s/iter; left time: 5152.9416s
Epoch: 73 cost time: 75.50800013542175
Epoch: 73, Steps: 137 | Train Loss: 0.2746136 Vali Loss: 0.2295550 Test Loss: 0.2609649
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.2765365
	speed: 1.3767s/iter; left time: 4956.0155s
Epoch: 74 cost time: 74.78616738319397
Epoch: 74, Steps: 137 | Train Loss: 0.2746919 Vali Loss: 0.2290531 Test Loss: 0.2609684
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2858013
	speed: 1.3359s/iter; left time: 4626.2117s
Epoch: 75 cost time: 76.07334494590759
Epoch: 75, Steps: 137 | Train Loss: 0.2746203 Vali Loss: 0.2291838 Test Loss: 0.2609621
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2687920
	speed: 1.3274s/iter; left time: 4414.8122s
Epoch: 76 cost time: 72.54639196395874
Epoch: 76, Steps: 137 | Train Loss: 0.2745478 Vali Loss: 0.2294632 Test Loss: 0.2609635
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2802262
	speed: 1.3256s/iter; left time: 4227.3575s
Epoch: 77 cost time: 73.9345350265503
Epoch: 77, Steps: 137 | Train Loss: 0.2745720 Vali Loss: 0.2292258 Test Loss: 0.2609628
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.2796860
	speed: 1.2986s/iter; left time: 3963.3579s
Epoch: 78 cost time: 71.07247829437256
Epoch: 78, Steps: 137 | Train Loss: 0.2746011 Vali Loss: 0.2296777 Test Loss: 0.2609613
EarlyStopping counter: 14 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.2688865
	speed: 1.3185s/iter; left time: 3843.5599s
Epoch: 79 cost time: 75.36253476142883
Epoch: 79, Steps: 137 | Train Loss: 0.2745825 Vali Loss: 0.2291363 Test Loss: 0.2609608
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.2663310
	speed: 1.3383s/iter; left time: 3717.9166s
Epoch: 80 cost time: 76.6704490184784
Epoch: 80, Steps: 137 | Train Loss: 0.2745326 Vali Loss: 0.2296367 Test Loss: 0.2609590
EarlyStopping counter: 16 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.2576066
	speed: 1.3304s/iter; left time: 3513.6400s
Epoch: 81 cost time: 72.82254552841187
Epoch: 81, Steps: 137 | Train Loss: 0.2746682 Vali Loss: 0.2293818 Test Loss: 0.2609594
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.2735911
	speed: 1.2923s/iter; left time: 3235.8839s
Epoch: 82 cost time: 73.82617044448853
Epoch: 82, Steps: 137 | Train Loss: 0.2745027 Vali Loss: 0.2294622 Test Loss: 0.2609575
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.2766788
	speed: 1.3447s/iter; left time: 3182.9030s
Epoch: 83 cost time: 69.93714618682861
Epoch: 83, Steps: 137 | Train Loss: 0.2745541 Vali Loss: 0.2293016 Test Loss: 0.2609562
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.2795431
	speed: 1.2083s/iter; left time: 2694.5936s
Epoch: 84 cost time: 68.3928542137146
Epoch: 84, Steps: 137 | Train Loss: 0.2746159 Vali Loss: 0.2292934 Test Loss: 0.2609576
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_90_j720_H8_FITS_custom_ftM_sl90_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.26460784673690796, mae:0.3362441956996918, rse:0.5131315588951111, corr:[0.43731692 0.43516627 0.4330934  0.43112093 0.4300403  0.42954108
 0.42910978 0.42949447 0.42899513 0.42912614 0.42879057 0.42834994
 0.4281393  0.4275576  0.42738023 0.4274881  0.42781478 0.42806932
 0.42835936 0.4325713  0.43225104 0.43181616 0.43140575 0.42941937
 0.42713454 0.42531496 0.42350867 0.42229834 0.4219553  0.42254877
 0.42368647 0.4248685  0.42496642 0.42509338 0.42501274 0.4246515
 0.4244226  0.42399982 0.42366064 0.42362422 0.42379588 0.42421615
 0.42460665 0.42455474 0.4246572  0.4244129  0.42435923 0.4235916
 0.42218065 0.4216188  0.42092106 0.42060056 0.42147943 0.42300355
 0.42580023 0.42793286 0.42853075 0.42901722 0.42902377 0.42876402
 0.4283329  0.42794335 0.42792135 0.4279617  0.42839286 0.42889285
 0.42935565 0.42953837 0.42972183 0.42972633 0.42996135 0.42973313
 0.42943156 0.42949978 0.4298298  0.43081766 0.4329084  0.435647
 0.439557   0.4424927  0.4430863  0.44318396 0.44280204 0.44238526
 0.4419511  0.44131964 0.4413004  0.44139865 0.44186777 0.44250345
 0.44280255 0.44292498 0.4429781  0.4429363  0.44299418 0.44281527
 0.44248348 0.44216785 0.4418311  0.4415541  0.44142196 0.44202077
 0.44272715 0.44311884 0.44322762 0.44301254 0.4426034  0.44209242
 0.44176477 0.4411965  0.44094974 0.4411927  0.44152126 0.4419393
 0.44225466 0.44225314 0.44229048 0.44244957 0.44259128 0.44250426
 0.44216427 0.44203052 0.4417523  0.44137928 0.44138324 0.44166964
 0.44208094 0.44237396 0.44240448 0.44220045 0.44188145 0.44137836
 0.4410547  0.4406218  0.4404643  0.4407477  0.44118628 0.4416568
 0.44215286 0.4423928  0.44252536 0.4426944  0.4427631  0.44247836
 0.44182608 0.44160283 0.4412406  0.4408624  0.44109622 0.44139972
 0.44198173 0.44223097 0.442116   0.44186407 0.44140244 0.44073978
 0.44026428 0.44019866 0.44037047 0.44059935 0.44129562 0.4418198
 0.44223738 0.44245356 0.44228804 0.44173554 0.44116756 0.43871313
 0.43497705 0.43224722 0.42985556 0.4280453  0.42738932 0.42711797
 0.4276163  0.42796806 0.42755616 0.42732775 0.42687613 0.42661697
 0.42627215 0.42580497 0.42579243 0.42571822 0.42596236 0.42621008
 0.42633742 0.42612296 0.42586777 0.42535824 0.42474794 0.42265886
 0.42006955 0.4180057  0.41639295 0.41525567 0.41492915 0.41607654
 0.4176688  0.41859925 0.41892803 0.4189075  0.4186144  0.4183154
 0.41794088 0.41768476 0.4175672  0.4175014  0.41787365 0.41802433
 0.41827032 0.41831988 0.41812    0.41797206 0.41797408 0.41691473
 0.41550994 0.41453147 0.4139226  0.41407362 0.41505793 0.4173149
 0.42012432 0.42223495 0.42293742 0.42314598 0.42290327 0.42259037
 0.42245346 0.42188525 0.42164016 0.42185405 0.42231825 0.42277393
 0.42320263 0.42330965 0.42339832 0.42344564 0.42380878 0.42338455
 0.42300186 0.4233985  0.4237461  0.42474875 0.42688534 0.42996132
 0.43393865 0.43678528 0.43733752 0.43735647 0.43700472 0.4365776
 0.43613794 0.43566263 0.43569177 0.43594444 0.43646616 0.43690062
 0.43725088 0.43748027 0.43753356 0.43760523 0.43767184 0.43745396
 0.43687597 0.4366626  0.4364026  0.43607113 0.436268   0.43677235
 0.4375106  0.438048   0.43788356 0.43744093 0.4369154  0.43651962
 0.43600428 0.43543437 0.43556017 0.43573752 0.43616405 0.4365878
 0.43679395 0.43691793 0.43699154 0.43706587 0.43714565 0.4371036
 0.4366992  0.43647552 0.43619403 0.4358668  0.43593448 0.43634441
 0.43706834 0.437278   0.4370842  0.4367231  0.43624517 0.43578154
 0.4353182  0.43497562 0.4351983  0.43548748 0.4359299  0.43636364
 0.436815   0.43703747 0.43722126 0.43729156 0.43732506 0.4370229
 0.43642485 0.4360961  0.43566805 0.43555105 0.4356683  0.43615276
 0.43698925 0.4370147  0.4368569  0.436395   0.43581644 0.43519634
 0.4347551  0.4346216  0.4347187  0.43523863 0.4358894  0.43629062
 0.43689823 0.43690863 0.4366029  0.4359282  0.43507534 0.43224975
 0.42822874 0.42548016 0.4233213  0.42180118 0.4209663  0.42114946
 0.42181253 0.42201614 0.42153794 0.42114443 0.42065218 0.42002186
 0.41946024 0.4192304  0.4192137  0.41929275 0.41950744 0.41964626
 0.41993803 0.41979724 0.4194072  0.41889194 0.41836712 0.41598067
 0.4127014  0.41065577 0.40937373 0.4082205  0.40814772 0.4092053
 0.4109317  0.41224724 0.41216072 0.41199243 0.41183108 0.4113536
 0.41085058 0.41039103 0.41043594 0.41052744 0.41058004 0.41093883
 0.4112248  0.41128126 0.41109306 0.4108174  0.41070992 0.4096188
 0.40805754 0.4071757  0.40703052 0.40724322 0.40871942 0.41146776
 0.41431636 0.41660404 0.41694558 0.41699904 0.41681495 0.4162222
 0.41584507 0.41542557 0.41532946 0.41553733 0.41598895 0.41656923
 0.41683614 0.4169075  0.41706106 0.41719145 0.41763645 0.41712493
 0.41670185 0.41715276 0.4178676  0.41924223 0.42160273 0.42510432
 0.4293332  0.4323437  0.43290085 0.43269554 0.43211517 0.43151912
 0.43091452 0.43066847 0.43086565 0.4310595  0.4317131  0.4322255
 0.43254048 0.4326667  0.43264326 0.432877   0.4329435  0.43255195
 0.4320382  0.43190977 0.43161342 0.43139553 0.43175787 0.43261677
 0.4334516  0.43363768 0.43339774 0.43284342 0.43219647 0.43169463
 0.43115512 0.4309553  0.4310732  0.43135253 0.43187648 0.43220988
 0.43255377 0.4325447  0.4325145  0.43262944 0.43266895 0.43268055
 0.432136   0.43189976 0.43179762 0.43151996 0.43189594 0.4324176
 0.4330297  0.43315855 0.432868   0.43237892 0.43178993 0.4311748
 0.43094733 0.4308276  0.43097362 0.43134952 0.43190134 0.43231148
 0.43270057 0.43296704 0.4331859  0.4332341  0.43319473 0.43283373
 0.43224463 0.43203318 0.4318735  0.43169308 0.43207145 0.43258253
 0.43311274 0.43333557 0.43314296 0.43249315 0.43188408 0.4313178
 0.43106058 0.4307874  0.43114698 0.43175152 0.43226516 0.4328243
 0.43317455 0.43314138 0.43279737 0.43198293 0.43109968 0.42807612
 0.42432675 0.421764   0.41974616 0.41838136 0.41787025 0.41787887
 0.41834208 0.41880012 0.41826838 0.41772598 0.41700336 0.41629234
 0.4160778  0.4158508  0.41574737 0.41592738 0.41620022 0.41654807
 0.41654605 0.41618282 0.41576058 0.4152233  0.414461   0.41196162
 0.4089352  0.40682307 0.40560538 0.40505335 0.4048535  0.4062165
 0.40800202 0.40883136 0.40874884 0.4084006  0.4078886  0.4075588
 0.40714732 0.40695608 0.40685156 0.40682214 0.40708795 0.4071848
 0.40741232 0.40732506 0.40695843 0.40690815 0.40661526 0.40558478
 0.40405238 0.40327114 0.40332296 0.40362215 0.40521926 0.40762112
 0.41050923 0.41258505 0.41275728 0.41269794 0.4122179  0.4116575
 0.4113414  0.41110358 0.41109797 0.4113046  0.41161543 0.41173664
 0.41191322 0.41186368 0.41200298 0.4122403  0.41249195 0.41236788
 0.4120635  0.41231436 0.41338965 0.4147104  0.41731825 0.4207574
 0.42448747 0.42716563 0.42743486 0.4269948  0.42636156 0.42568484
 0.42535776 0.42505693 0.42517665 0.42562175 0.42621204 0.426801
 0.4270593  0.42687464 0.4270877  0.42713878 0.42711484 0.42683166
 0.4262268  0.42615813 0.4259632  0.4258701  0.42618555 0.42691508
 0.42760593 0.4276554  0.42739353 0.4265463  0.42592418 0.4253832
 0.42499983 0.42493507 0.42507023 0.42544237 0.425975   0.42632338
 0.4266159  0.426645   0.42666903 0.42668566 0.42679954 0.42663142
 0.42620695 0.42633754 0.4261641  0.4259995  0.42642257 0.42683065
 0.42730266 0.4272009  0.42679122 0.4261783  0.4255474  0.42481917
 0.42470604 0.4246332  0.42484546 0.42527592 0.42583996 0.42631045
 0.42650926 0.42676333 0.42680442 0.42695364 0.42695725 0.4266713
 0.42622203 0.42594627 0.42579937 0.42588493 0.42638174 0.42690292
 0.42749882 0.4275205  0.42694655 0.42631885 0.42552125 0.4249472
 0.42483267 0.42469436 0.42499343 0.42535517 0.42583892 0.42626074
 0.42627987 0.42640296 0.42603678 0.42534012 0.4241737  0.42139715
 0.41773298 0.41517702 0.41322196 0.41203147 0.4119251  0.41187328
 0.4123452  0.4126898  0.41198975 0.4115252  0.4105738  0.40978834
 0.40957275 0.40924364 0.40915972 0.40900755 0.40934992 0.4096323
 0.40944704 0.40894493 0.4085938  0.40800434 0.40696675 0.40542197
 0.4022733  0.4008979  0.3996984  0.399283   0.3995486  0.40071136
 0.40240827 0.4029392  0.40284592 0.4023953  0.40133807 0.40089232
 0.4000061  0.3996865  0.39908263 0.3982493  0.3986791  0.39706904
 0.39802936 0.39617464 0.3973001  0.3954839  0.39637762 0.39077643]
