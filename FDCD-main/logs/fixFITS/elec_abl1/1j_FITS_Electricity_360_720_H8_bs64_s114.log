Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=138, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_360_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_360_j720_H8_FITS_custom_ftM_sl360_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17333
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=138, out_features=414, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2347439616.0
params:  57546.0
Trainable parameters:  57546
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7227761
	speed: 0.8023s/iter; left time: 10751.8792s
Epoch: 1 cost time: 109.1880030632019
Epoch: 1, Steps: 135 | Train Loss: 0.9640201 Vali Loss: 0.5331492 Test Loss: 0.6350672
Validation loss decreased (inf --> 0.533149).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5120149
	speed: 1.9047s/iter; left time: 25267.6330s
Epoch: 2 cost time: 110.08543658256531
Epoch: 2, Steps: 135 | Train Loss: 0.5544761 Vali Loss: 0.3831937 Test Loss: 0.4627970
Validation loss decreased (0.533149 --> 0.383194).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4036005
	speed: 1.8813s/iter; left time: 24702.8726s
Epoch: 3 cost time: 114.6892204284668
Epoch: 3, Steps: 135 | Train Loss: 0.4215863 Vali Loss: 0.2987145 Test Loss: 0.3633513
Validation loss decreased (0.383194 --> 0.298714).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3097697
	speed: 1.8734s/iter; left time: 24346.1481s
Epoch: 4 cost time: 108.53706240653992
Epoch: 4, Steps: 135 | Train Loss: 0.3418869 Vali Loss: 0.2474468 Test Loss: 0.3019051
Validation loss decreased (0.298714 --> 0.247447).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2934265
	speed: 1.8582s/iter; left time: 23897.9820s
Epoch: 5 cost time: 102.15855956077576
Epoch: 5, Steps: 135 | Train Loss: 0.2931427 Vali Loss: 0.2173673 Test Loss: 0.2644518
Validation loss decreased (0.247447 --> 0.217367).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2538253
	speed: 1.8514s/iter; left time: 23560.7161s
Epoch: 6 cost time: 112.71698760986328
Epoch: 6, Steps: 135 | Train Loss: 0.2641319 Vali Loss: 0.2004304 Test Loss: 0.2421341
Validation loss decreased (0.217367 --> 0.200430).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2361178
	speed: 1.8441s/iter; left time: 23219.5628s
Epoch: 7 cost time: 105.68996453285217
Epoch: 7, Steps: 135 | Train Loss: 0.2472636 Vali Loss: 0.1907737 Test Loss: 0.2291641
Validation loss decreased (0.200430 --> 0.190774).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2405745
	speed: 1.8422s/iter; left time: 22947.0601s
Epoch: 8 cost time: 102.67525625228882
Epoch: 8, Steps: 135 | Train Loss: 0.2376412 Vali Loss: 0.1858913 Test Loss: 0.2218083
Validation loss decreased (0.190774 --> 0.185891).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2448193
	speed: 1.7873s/iter; left time: 22021.0172s
Epoch: 9 cost time: 107.60244107246399
Epoch: 9, Steps: 135 | Train Loss: 0.2325783 Vali Loss: 0.1836395 Test Loss: 0.2176700
Validation loss decreased (0.185891 --> 0.183639).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2235584
	speed: 1.8044s/iter; left time: 21988.0053s
Epoch: 10 cost time: 107.4704053401947
Epoch: 10, Steps: 135 | Train Loss: 0.2297052 Vali Loss: 0.1822312 Test Loss: 0.2153774
Validation loss decreased (0.183639 --> 0.182231).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2247332
	speed: 1.9493s/iter; left time: 23491.3170s
Epoch: 11 cost time: 113.15924048423767
Epoch: 11, Steps: 135 | Train Loss: 0.2281901 Vali Loss: 0.1821125 Test Loss: 0.2140977
Validation loss decreased (0.182231 --> 0.182112).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2254408
	speed: 1.9197s/iter; left time: 22875.3560s
Epoch: 12 cost time: 118.39321494102478
Epoch: 12, Steps: 135 | Train Loss: 0.2274540 Vali Loss: 0.1816211 Test Loss: 0.2134383
Validation loss decreased (0.182112 --> 0.181621).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2294528
	speed: 1.8591s/iter; left time: 21902.1291s
Epoch: 13 cost time: 107.58369708061218
Epoch: 13, Steps: 135 | Train Loss: 0.2269890 Vali Loss: 0.1816000 Test Loss: 0.2130068
Validation loss decreased (0.181621 --> 0.181600).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2206246
	speed: 1.8548s/iter; left time: 21601.5528s
Epoch: 14 cost time: 100.309002161026
Epoch: 14, Steps: 135 | Train Loss: 0.2267661 Vali Loss: 0.1817797 Test Loss: 0.2127395
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2143224
	speed: 1.8115s/iter; left time: 20852.0010s
Epoch: 15 cost time: 106.27348208427429
Epoch: 15, Steps: 135 | Train Loss: 0.2266342 Vali Loss: 0.1819213 Test Loss: 0.2126117
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2149513
	speed: 1.8119s/iter; left time: 20612.3831s
Epoch: 16 cost time: 107.92405486106873
Epoch: 16, Steps: 135 | Train Loss: 0.2265176 Vali Loss: 0.1818735 Test Loss: 0.2125065
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2141414
	speed: 1.9435s/iter; left time: 21847.2579s
Epoch: 17 cost time: 109.81015634536743
Epoch: 17, Steps: 135 | Train Loss: 0.2264614 Vali Loss: 0.1816383 Test Loss: 0.2124439
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2272165
	speed: 1.9331s/iter; left time: 21469.1109s
Epoch: 18 cost time: 116.12598919868469
Epoch: 18, Steps: 135 | Train Loss: 0.2264109 Vali Loss: 0.1816283 Test Loss: 0.2123839
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2374209
	speed: 1.8535s/iter; left time: 20335.1754s
Epoch: 19 cost time: 105.36679577827454
Epoch: 19, Steps: 135 | Train Loss: 0.2264809 Vali Loss: 0.1815632 Test Loss: 0.2123634
Validation loss decreased (0.181600 --> 0.181563).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2292069
	speed: 1.8962s/iter; left time: 20547.0627s
Epoch: 20 cost time: 109.58114409446716
Epoch: 20, Steps: 135 | Train Loss: 0.2263400 Vali Loss: 0.1817793 Test Loss: 0.2123155
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2140386
	speed: 1.8289s/iter; left time: 19570.7097s
Epoch: 21 cost time: 109.32429003715515
Epoch: 21, Steps: 135 | Train Loss: 0.2263015 Vali Loss: 0.1816609 Test Loss: 0.2122969
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2337297
	speed: 1.9176s/iter; left time: 20260.8716s
Epoch: 22 cost time: 115.90002918243408
Epoch: 22, Steps: 135 | Train Loss: 0.2262815 Vali Loss: 0.1819866 Test Loss: 0.2122733
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2389457
	speed: 2.0026s/iter; left time: 20889.1270s
Epoch: 23 cost time: 114.72451329231262
Epoch: 23, Steps: 135 | Train Loss: 0.2262569 Vali Loss: 0.1814750 Test Loss: 0.2122701
Validation loss decreased (0.181563 --> 0.181475).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2343203
	speed: 1.9032s/iter; left time: 19595.8472s
Epoch: 24 cost time: 116.13299512863159
Epoch: 24, Steps: 135 | Train Loss: 0.2262644 Vali Loss: 0.1816410 Test Loss: 0.2122464
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2087580
	speed: 1.9732s/iter; left time: 20049.3937s
Epoch: 25 cost time: 106.8760998249054
Epoch: 25, Steps: 135 | Train Loss: 0.2262895 Vali Loss: 0.1815735 Test Loss: 0.2122303
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2237024
	speed: 1.8226s/iter; left time: 18273.6936s
Epoch: 26 cost time: 110.27211236953735
Epoch: 26, Steps: 135 | Train Loss: 0.2262664 Vali Loss: 0.1815821 Test Loss: 0.2122178
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2183426
	speed: 1.8481s/iter; left time: 18279.9682s
Epoch: 27 cost time: 104.86039447784424
Epoch: 27, Steps: 135 | Train Loss: 0.2261655 Vali Loss: 0.1815825 Test Loss: 0.2122238
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2159234
	speed: 1.7977s/iter; left time: 17538.3351s
Epoch: 28 cost time: 102.24830389022827
Epoch: 28, Steps: 135 | Train Loss: 0.2262247 Vali Loss: 0.1818728 Test Loss: 0.2122125
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2320289
	speed: 1.7600s/iter; left time: 16932.9218s
Epoch: 29 cost time: 105.02665448188782
Epoch: 29, Steps: 135 | Train Loss: 0.2261469 Vali Loss: 0.1818306 Test Loss: 0.2122110
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2250734
	speed: 1.8548s/iter; left time: 17594.4707s
Epoch: 30 cost time: 110.79941058158875
Epoch: 30, Steps: 135 | Train Loss: 0.2261334 Vali Loss: 0.1814480 Test Loss: 0.2122198
Validation loss decreased (0.181475 --> 0.181448).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2153148
	speed: 1.8206s/iter; left time: 17024.0678s
Epoch: 31 cost time: 103.26168608665466
Epoch: 31, Steps: 135 | Train Loss: 0.2261755 Vali Loss: 0.1814118 Test Loss: 0.2122028
Validation loss decreased (0.181448 --> 0.181412).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2309906
	speed: 1.8164s/iter; left time: 16739.7660s
Epoch: 32 cost time: 99.15177989006042
Epoch: 32, Steps: 135 | Train Loss: 0.2261467 Vali Loss: 0.1817408 Test Loss: 0.2121872
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2363326
	speed: 1.7538s/iter; left time: 15926.6784s
Epoch: 33 cost time: 105.7193648815155
Epoch: 33, Steps: 135 | Train Loss: 0.2261446 Vali Loss: 0.1815149 Test Loss: 0.2121805
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2215191
	speed: 1.7467s/iter; left time: 15625.9017s
Epoch: 34 cost time: 103.36436033248901
Epoch: 34, Steps: 135 | Train Loss: 0.2261159 Vali Loss: 0.1814897 Test Loss: 0.2121848
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2286249
	speed: 1.7859s/iter; left time: 15735.1607s
Epoch: 35 cost time: 103.23284220695496
Epoch: 35, Steps: 135 | Train Loss: 0.2261547 Vali Loss: 0.1814496 Test Loss: 0.2121777
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2188653
	speed: 1.7908s/iter; left time: 15536.5766s
Epoch: 36 cost time: 101.17272138595581
Epoch: 36, Steps: 135 | Train Loss: 0.2260822 Vali Loss: 0.1815237 Test Loss: 0.2121674
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2207203
	speed: 1.7119s/iter; left time: 14621.7618s
Epoch: 37 cost time: 98.2306592464447
Epoch: 37, Steps: 135 | Train Loss: 0.2261186 Vali Loss: 0.1814054 Test Loss: 0.2121692
Validation loss decreased (0.181412 --> 0.181405).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2286511
	speed: 1.7910s/iter; left time: 15055.5273s
Epoch: 38 cost time: 102.79614210128784
Epoch: 38, Steps: 135 | Train Loss: 0.2261189 Vali Loss: 0.1815983 Test Loss: 0.2121683
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2270318
	speed: 1.8095s/iter; left time: 14966.2736s
Epoch: 39 cost time: 109.07860803604126
Epoch: 39, Steps: 135 | Train Loss: 0.2260591 Vali Loss: 0.1816358 Test Loss: 0.2121571
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2422891
	speed: 1.8264s/iter; left time: 14859.4327s
Epoch: 40 cost time: 104.79823470115662
Epoch: 40, Steps: 135 | Train Loss: 0.2260634 Vali Loss: 0.1817002 Test Loss: 0.2121519
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2343213
	speed: 1.8123s/iter; left time: 14499.9645s
Epoch: 41 cost time: 103.60360336303711
Epoch: 41, Steps: 135 | Train Loss: 0.2260740 Vali Loss: 0.1812303 Test Loss: 0.2121485
Validation loss decreased (0.181405 --> 0.181230).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2293612
	speed: 1.6599s/iter; left time: 13056.9738s
Epoch: 42 cost time: 95.19101881980896
Epoch: 42, Steps: 135 | Train Loss: 0.2260913 Vali Loss: 0.1815044 Test Loss: 0.2121488
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2185392
	speed: 1.5999s/iter; left time: 12368.5371s
Epoch: 43 cost time: 91.36587429046631
Epoch: 43, Steps: 135 | Train Loss: 0.2260537 Vali Loss: 0.1817332 Test Loss: 0.2121457
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2236544
	speed: 1.6233s/iter; left time: 12330.2232s
Epoch: 44 cost time: 91.67380237579346
Epoch: 44, Steps: 135 | Train Loss: 0.2260150 Vali Loss: 0.1813400 Test Loss: 0.2121495
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2209733
	speed: 1.6292s/iter; left time: 12155.1402s
Epoch: 45 cost time: 101.2513382434845
Epoch: 45, Steps: 135 | Train Loss: 0.2260688 Vali Loss: 0.1815260 Test Loss: 0.2121501
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2361104
	speed: 1.7367s/iter; left time: 12723.2771s
Epoch: 46 cost time: 89.8098030090332
Epoch: 46, Steps: 135 | Train Loss: 0.2260797 Vali Loss: 0.1815168 Test Loss: 0.2121394
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2159315
	speed: 1.5292s/iter; left time: 10996.4096s
Epoch: 47 cost time: 89.39751887321472
Epoch: 47, Steps: 135 | Train Loss: 0.2260275 Vali Loss: 0.1815006 Test Loss: 0.2121402
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2258815
	speed: 1.4946s/iter; left time: 10546.0436s
Epoch: 48 cost time: 86.35105919837952
Epoch: 48, Steps: 135 | Train Loss: 0.2260794 Vali Loss: 0.1816189 Test Loss: 0.2121359
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2205896
	speed: 1.5098s/iter; left time: 10449.5603s
Epoch: 49 cost time: 86.47993969917297
Epoch: 49, Steps: 135 | Train Loss: 0.2260819 Vali Loss: 0.1810823 Test Loss: 0.2121315
Validation loss decreased (0.181230 --> 0.181082).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2164358
	speed: 1.3541s/iter; left time: 9188.7806s
Epoch: 50 cost time: 83.45762991905212
Epoch: 50, Steps: 135 | Train Loss: 0.2260668 Vali Loss: 0.1813044 Test Loss: 0.2121413
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2304470
	speed: 1.3952s/iter; left time: 9279.3171s
Epoch: 51 cost time: 71.98182201385498
Epoch: 51, Steps: 135 | Train Loss: 0.2260570 Vali Loss: 0.1815644 Test Loss: 0.2121378
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2213330
	speed: 0.9459s/iter; left time: 6163.6838s
Epoch: 52 cost time: 53.71657419204712
Epoch: 52, Steps: 135 | Train Loss: 0.2260595 Vali Loss: 0.1814131 Test Loss: 0.2121380
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2139960
	speed: 0.8827s/iter; left time: 5632.4082s
Epoch: 53 cost time: 51.706852197647095
Epoch: 53, Steps: 135 | Train Loss: 0.2260480 Vali Loss: 0.1814002 Test Loss: 0.2121339
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2182721
	speed: 0.8922s/iter; left time: 5572.4179s
Epoch: 54 cost time: 54.90054965019226
Epoch: 54, Steps: 135 | Train Loss: 0.2260159 Vali Loss: 0.1812078 Test Loss: 0.2121338
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2296779
	speed: 1.0152s/iter; left time: 6203.9491s
Epoch: 55 cost time: 56.98966860771179
Epoch: 55, Steps: 135 | Train Loss: 0.2260879 Vali Loss: 0.1815647 Test Loss: 0.2121313
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2414246
	speed: 0.9544s/iter; left time: 5703.7191s
Epoch: 56 cost time: 55.1602098941803
Epoch: 56, Steps: 135 | Train Loss: 0.2259865 Vali Loss: 0.1816459 Test Loss: 0.2121274
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2241375
	speed: 0.9593s/iter; left time: 5603.4895s
Epoch: 57 cost time: 55.110432386398315
Epoch: 57, Steps: 135 | Train Loss: 0.2260432 Vali Loss: 0.1812837 Test Loss: 0.2121243
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2211520
	speed: 0.9294s/iter; left time: 5303.1275s
Epoch: 58 cost time: 56.5935378074646
Epoch: 58, Steps: 135 | Train Loss: 0.2260465 Vali Loss: 0.1814748 Test Loss: 0.2121244
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2197986
	speed: 1.0117s/iter; left time: 5636.0872s
Epoch: 59 cost time: 57.51039409637451
Epoch: 59, Steps: 135 | Train Loss: 0.2259644 Vali Loss: 0.1813797 Test Loss: 0.2121258
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2285101
	speed: 1.0276s/iter; left time: 5585.9280s
Epoch: 60 cost time: 63.436156034469604
Epoch: 60, Steps: 135 | Train Loss: 0.2260261 Vali Loss: 0.1813773 Test Loss: 0.2121231
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2150140
	speed: 1.0935s/iter; left time: 5796.7384s
Epoch: 61 cost time: 61.498404026031494
Epoch: 61, Steps: 135 | Train Loss: 0.2260753 Vali Loss: 0.1812573 Test Loss: 0.2121198
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2372194
	speed: 1.0240s/iter; left time: 5289.9033s
Epoch: 62 cost time: 57.1489782333374
Epoch: 62, Steps: 135 | Train Loss: 0.2260297 Vali Loss: 0.1813860 Test Loss: 0.2121207
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2260676
	speed: 1.0026s/iter; left time: 5044.2631s
Epoch: 63 cost time: 58.827048540115356
Epoch: 63, Steps: 135 | Train Loss: 0.2259970 Vali Loss: 0.1812284 Test Loss: 0.2121181
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2316526
	speed: 0.9929s/iter; left time: 4861.2440s
Epoch: 64 cost time: 59.20044541358948
Epoch: 64, Steps: 135 | Train Loss: 0.2260173 Vali Loss: 0.1811402 Test Loss: 0.2121222
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2333115
	speed: 0.9628s/iter; left time: 4583.8510s
Epoch: 65 cost time: 56.662428855895996
Epoch: 65, Steps: 135 | Train Loss: 0.2260331 Vali Loss: 0.1813434 Test Loss: 0.2121202
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2364726
	speed: 1.0181s/iter; left time: 4709.5860s
Epoch: 66 cost time: 58.12463927268982
Epoch: 66, Steps: 135 | Train Loss: 0.2259983 Vali Loss: 0.1812211 Test Loss: 0.2121204
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2363118
	speed: 0.9818s/iter; left time: 4409.2603s
Epoch: 67 cost time: 57.93632197380066
Epoch: 67, Steps: 135 | Train Loss: 0.2260450 Vali Loss: 0.1814373 Test Loss: 0.2121194
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2203734
	speed: 0.9970s/iter; left time: 4342.9275s
Epoch: 68 cost time: 59.019412994384766
Epoch: 68, Steps: 135 | Train Loss: 0.2260136 Vali Loss: 0.1813936 Test Loss: 0.2121202
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2215839
	speed: 0.9794s/iter; left time: 4134.1601s
Epoch: 69 cost time: 56.11902594566345
Epoch: 69, Steps: 135 | Train Loss: 0.2260117 Vali Loss: 0.1814764 Test Loss: 0.2121182
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_360_j720_H8_FITS_custom_ftM_sl360_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.21054169535636902, mae:0.29905590415000916, rse:0.4577163755893707, corr:[0.44666514 0.44745734 0.44927183 0.4495193  0.45043102 0.4504634
 0.45047957 0.45041248 0.45003867 0.44967777 0.44945213 0.44920146
 0.4490253  0.44907558 0.4490102  0.4490106  0.4490594  0.44901413
 0.4490646  0.44894654 0.44887596 0.4490314  0.44905117 0.44924796
 0.44946682 0.44952407 0.44963434 0.4496613  0.4494437  0.44928014
 0.44909942 0.4488071  0.44860357 0.44852346 0.44837466 0.4483137
 0.44836998 0.44834667 0.44836935 0.448389   0.4483285  0.44840407
 0.44840336 0.44824892 0.44824505 0.44825202 0.44826883 0.4484195
 0.44843477 0.4485118  0.4486067  0.44845122 0.44829208 0.4482109
 0.4480635  0.44792318 0.44784606 0.44774795 0.44769973 0.44776747
 0.44776085 0.44776538 0.447823   0.44778797 0.4477997  0.4478447
 0.44777292 0.44767994 0.44761977 0.4475591  0.4476168  0.44765422
 0.44761175 0.44769767 0.44767913 0.44754344 0.44743517 0.44732475
 0.44723132 0.44715568 0.4470477  0.44701195 0.4470308  0.4469763
 0.44695345 0.44703472 0.4470545  0.44705027 0.44710356 0.44708923
 0.44707054 0.44703254 0.44693005 0.44691294 0.44692552 0.4469297
 0.44699702 0.44704214 0.44699323 0.4469685  0.44684628 0.44668567
 0.44666827 0.44664785 0.44655836 0.44653252 0.4464894  0.4464451
 0.4464918  0.446527   0.44651788 0.4465667  0.44656384 0.44655874
 0.4465648  0.44643497 0.44636554 0.44642752 0.4464221  0.44646484
 0.4465854  0.44662985 0.4466552  0.4466579  0.44656006 0.44652784
 0.44655398 0.4465125  0.44648445 0.44646358 0.44640848 0.44642207
 0.44643995 0.44639602 0.44642422 0.44645876 0.44640958 0.4464309
 0.446496   0.44651595 0.44661173 0.44672754 0.44675246 0.4469345
 0.44723383 0.44739178 0.44750816 0.4475207  0.44746602 0.44749027
 0.44746983 0.4474242  0.44743228 0.44737685 0.44731605 0.44733593
 0.4473392  0.44732535 0.44738403 0.44737184 0.44735134 0.44736922
 0.44736663 0.44733804 0.4472777  0.4471802  0.44708776 0.44690466
 0.44661036 0.44647324 0.44640628 0.4462824  0.44618964 0.44607115
 0.44591406 0.44584385 0.44576758 0.445649   0.44563484 0.44562656
 0.4455582  0.44555748 0.44557792 0.445535   0.44551107 0.44548833
 0.4454113  0.44521716 0.4449793  0.44487113 0.44484696 0.4447698
 0.44470713 0.44473103 0.44470876 0.4447083  0.4446689  0.44454196
 0.4444693  0.44443157 0.4443402  0.44429    0.44426292 0.4442113
 0.44421494 0.44425112 0.4442023  0.44416937 0.4441755  0.4441213
 0.44407114 0.4439114  0.44371375 0.4436688  0.44366062 0.44364485
 0.44369224 0.4437653  0.44380492 0.4438308  0.44373983 0.44365114
 0.44365016 0.44361785 0.44356552 0.44353297 0.44350836 0.44348663
 0.44347483 0.4434193  0.44339356 0.44340587 0.4433471  0.44328052
 0.4432634  0.44308585 0.44296613 0.44294125 0.44287717 0.4428906
 0.44291022 0.44292074 0.4429837  0.44299215 0.4429033  0.44290492
 0.44287857 0.44278085 0.44276917 0.44273248 0.44265625 0.44268757
 0.4426906  0.44262376 0.44262445 0.44261944 0.44255602 0.44255564
 0.44252464 0.4424001  0.4423503  0.4423051  0.44228476 0.44236428
 0.44238588 0.44244152 0.44253457 0.4425022  0.4424378  0.44246867
 0.44241259 0.44235355 0.44233528 0.44223595 0.44218016 0.44218594
 0.44213665 0.44212946 0.44218707 0.4421458  0.44212452 0.4421672
 0.44206804 0.44192278 0.44187784 0.44183266 0.44186133 0.4419632
 0.4420679  0.44220284 0.4422899  0.44230762 0.4423149  0.44230592
 0.44229126 0.44231105 0.44226256 0.44218096 0.44216242 0.44213617
 0.44207248 0.44206503 0.44204512 0.4420086  0.442013   0.441966
 0.4419782  0.44207442 0.44207838 0.44214547 0.44226623 0.44241482
 0.4427326  0.44297355 0.44304964 0.44312328 0.44317934 0.44314402
 0.44311714 0.44315177 0.44311714 0.44303882 0.44300282 0.44294086
 0.44290274 0.4428861  0.4428659  0.44286367 0.4428345  0.44277793
 0.4427699  0.4426642  0.4424868  0.4424372  0.4423271  0.44206345
 0.44182804 0.44177043 0.44171572 0.44166163 0.44156572 0.44143313
 0.4413395  0.44120723 0.44105828 0.4409726  0.44085634 0.4407661
 0.44074935 0.440666   0.44060534 0.44061336 0.44055516 0.44053194
 0.4405313  0.44033614 0.44019672 0.44022527 0.4401847  0.44013807
 0.44007504 0.4400603  0.44011596 0.44014776 0.44007078 0.43999255
 0.43989325 0.43976676 0.43970153 0.43962005 0.43947512 0.43939593
 0.43936473 0.4393157  0.43928874 0.43922555 0.4391529  0.43917644
 0.43915436 0.43898776 0.43890727 0.43887103 0.43886307 0.43895072
 0.43895245 0.4390165  0.43915614 0.43914658 0.43910238 0.4390811
 0.43899292 0.43896896 0.43895411 0.4388247  0.4387131  0.43868098
 0.43861854 0.43856338 0.43856993 0.43853077 0.4385071  0.43851006
 0.4384162  0.43827644 0.43822375 0.4381939  0.43819803 0.43823367
 0.43825603 0.43835875 0.4384177  0.438424   0.43846717 0.43845657
 0.43839613 0.43837836 0.43826056 0.4381428  0.43814173 0.43810093
 0.43804786 0.43804994 0.43802765 0.43796068 0.43795425 0.4379589
 0.43792674 0.43783733 0.43775007 0.43776911 0.43783325 0.43785408
 0.43792996 0.43804786 0.43807003 0.43812808 0.43817794 0.43813264
 0.43812868 0.4380881  0.4379461  0.43786302 0.43782866 0.43773583
 0.43767565 0.43766147 0.4376281  0.4376356  0.43761215 0.43757263
 0.4375627  0.43743455 0.43730292 0.43735886 0.437454   0.43756643
 0.43778884 0.43791416 0.43797603 0.4380988  0.4381142  0.4380863
 0.4381458  0.43813708 0.43805695 0.43798602 0.4378574  0.43778425
 0.43782336 0.4377902  0.4377615  0.43777412 0.43771923 0.43770579
 0.4377761  0.4377992  0.4378797  0.43806747 0.43818998 0.43841723
 0.4387401  0.4389265  0.43907928 0.43915972 0.43912655 0.4391603
 0.43920475 0.4391574  0.4391183  0.43904376 0.43891022 0.43888423
 0.43886906 0.43879393 0.43883628 0.4388192  0.43873656 0.4387905
 0.43879455 0.4386771  0.43860334 0.43850946 0.4384124  0.4382842
 0.43801853 0.43791512 0.43790287 0.4378222  0.43773302 0.43763822
 0.43749768 0.43738452 0.4372667  0.43709087 0.43696177 0.43688816
 0.43678972 0.43674958 0.43675578 0.43667388 0.4366661  0.43671516
 0.43658447 0.436407   0.4363232  0.43628156 0.4363058  0.43628386
 0.43617254 0.43618187 0.43618658 0.4361772  0.436187   0.4361118
 0.43600902 0.43592283 0.4357471  0.4355653  0.43544716 0.43534073
 0.43525496 0.43523583 0.43515357 0.4350682  0.4351092  0.4351123
 0.43503454 0.43491805 0.4348086  0.43480778 0.4348818  0.43490785
 0.43497446 0.4350424  0.43502772 0.43505073 0.43500823 0.43486145
 0.43482816 0.43476215 0.43458456 0.43443304 0.4342822  0.43410164
 0.43400246 0.43390486 0.43380043 0.4337529  0.43368357 0.43357745
 0.43353823 0.43342698 0.433343   0.4333992  0.43342367 0.4335026
 0.43361378 0.43363258 0.43373382 0.433855   0.43375838 0.4336889
 0.43370712 0.43354034 0.43336138 0.43325832 0.43304613 0.4329213
 0.43290675 0.43276188 0.4327283  0.43279532 0.43271983 0.43272364
 0.43279338 0.43268216 0.43264782 0.4327029  0.4327374  0.43286735
 0.43294272 0.43299478 0.4331378  0.4331743  0.4330975  0.43310514
 0.43303162 0.4328426  0.43271002 0.4325255  0.4323509  0.43231437
 0.43220547 0.43210268 0.43213943 0.43208647 0.43204865 0.43218198
 0.4321462  0.4320492  0.4320975  0.43213764 0.4323076  0.43255675
 0.43270022 0.43288016 0.4330322  0.43301046 0.4330218  0.43300292
 0.4328494  0.43276617 0.43264806 0.4324016  0.43231043 0.43226254
 0.43211904 0.4321539  0.4321655  0.43206394 0.4321936  0.43226314
 0.43227112 0.43248683 0.432574   0.43271413 0.4330305  0.43326038
 0.43355572 0.43379128 0.43381488 0.43383428 0.43385231 0.43370226
 0.43357366 0.43352595 0.4333157  0.4331179  0.4330385  0.43288007
 0.43287647 0.43291804 0.43282267 0.43293163 0.4330505  0.4330266
 0.43317366 0.43315437 0.43300608 0.43310368 0.43308824 0.43287775
 0.43265578 0.4324096  0.43217582 0.43215153 0.43192482 0.43157518
 0.43141654 0.43107322 0.4307838  0.43072727 0.4304813  0.43041041
 0.43054602 0.43043414 0.43055022 0.43076244 0.43069616 0.4308726
 0.4309773  0.43067595 0.4306611  0.4307107  0.43054065 0.4306427
 0.430439   0.43004313 0.43013096 0.42987308 0.4294486  0.42949975
 0.42904517 0.42883077 0.42895493 0.42856205 0.42881447 0.42910004
 0.4290307  0.42957    0.4297399  0.4252747  0.42974758 0.42539072
 0.42442805 0.42887473 0.42336527 0.4276673  0.42302486 0.42546934]
