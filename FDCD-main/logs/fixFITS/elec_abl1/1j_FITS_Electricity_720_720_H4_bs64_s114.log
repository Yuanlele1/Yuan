Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H4', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_720_j720_H4_FITS_custom_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1475552256.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6612054
	speed: 0.6958s/iter; left time: 9115.0231s
Epoch: 1 cost time: 94.43540811538696
Epoch: 1, Steps: 132 | Train Loss: 0.8570885 Vali Loss: 0.5010784 Test Loss: 0.5960166
Validation loss decreased (inf --> 0.501078).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4841783
	speed: 1.7871s/iter; left time: 23176.8437s
Epoch: 2 cost time: 111.91926264762878
Epoch: 2, Steps: 132 | Train Loss: 0.5111663 Vali Loss: 0.3454027 Test Loss: 0.4169878
Validation loss decreased (0.501078 --> 0.345403).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3433834
	speed: 1.8098s/iter; left time: 23231.8385s
Epoch: 3 cost time: 110.32007622718811
Epoch: 3, Steps: 132 | Train Loss: 0.3753455 Vali Loss: 0.2643487 Test Loss: 0.3210910
Validation loss decreased (0.345403 --> 0.264349).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2916617
	speed: 1.9911s/iter; left time: 25297.2657s
Epoch: 4 cost time: 120.27083802223206
Epoch: 4, Steps: 132 | Train Loss: 0.3033880 Vali Loss: 0.2226614 Test Loss: 0.2705521
Validation loss decreased (0.264349 --> 0.222661).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2475312
	speed: 1.8826s/iter; left time: 23670.0025s
Epoch: 5 cost time: 116.40617322921753
Epoch: 5, Steps: 132 | Train Loss: 0.2663463 Vali Loss: 0.2026213 Test Loss: 0.2448364
Validation loss decreased (0.222661 --> 0.202621).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2482478
	speed: 1.8698s/iter; left time: 23261.8814s
Epoch: 6 cost time: 113.17954754829407
Epoch: 6, Steps: 132 | Train Loss: 0.2479761 Vali Loss: 0.1937508 Test Loss: 0.2322628
Validation loss decreased (0.202621 --> 0.193751).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2312679
	speed: 2.8540s/iter; left time: 35129.6517s
Epoch: 7 cost time: 213.40341448783875
Epoch: 7, Steps: 132 | Train Loss: 0.2393975 Vali Loss: 0.1898535 Test Loss: 0.2261251
Validation loss decreased (0.193751 --> 0.189854).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2238700
	speed: 3.4673s/iter; left time: 42221.0300s
Epoch: 8 cost time: 203.62663197517395
Epoch: 8, Steps: 132 | Train Loss: 0.2351986 Vali Loss: 0.1884522 Test Loss: 0.2232552
Validation loss decreased (0.189854 --> 0.188452).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2431905
	speed: 3.2330s/iter; left time: 38941.0252s
Epoch: 9 cost time: 198.16269636154175
Epoch: 9, Steps: 132 | Train Loss: 0.2335002 Vali Loss: 0.1881194 Test Loss: 0.2217860
Validation loss decreased (0.188452 --> 0.188119).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2444657
	speed: 3.2040s/iter; left time: 38169.3887s
Epoch: 10 cost time: 198.2644579410553
Epoch: 10, Steps: 132 | Train Loss: 0.2326083 Vali Loss: 0.1881198 Test Loss: 0.2210743
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2302402
	speed: 3.3437s/iter; left time: 39392.5398s
Epoch: 11 cost time: 206.9325816631317
Epoch: 11, Steps: 132 | Train Loss: 0.2321283 Vali Loss: 0.1880943 Test Loss: 0.2205741
Validation loss decreased (0.188119 --> 0.188094).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2314580
	speed: 3.3311s/iter; left time: 38803.9370s
Epoch: 12 cost time: 200.50453448295593
Epoch: 12, Steps: 132 | Train Loss: 0.2318256 Vali Loss: 0.1881271 Test Loss: 0.2203787
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2394344
	speed: 3.3068s/iter; left time: 38084.8456s
Epoch: 13 cost time: 191.85049057006836
Epoch: 13, Steps: 132 | Train Loss: 0.2316693 Vali Loss: 0.1876024 Test Loss: 0.2202192
Validation loss decreased (0.188094 --> 0.187602).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2171744
	speed: 3.3558s/iter; left time: 38205.2965s
Epoch: 14 cost time: 206.42069172859192
Epoch: 14, Steps: 132 | Train Loss: 0.2315108 Vali Loss: 0.1878857 Test Loss: 0.2200911
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2330197
	speed: 3.3514s/iter; left time: 37713.6824s
Epoch: 15 cost time: 192.1677167415619
Epoch: 15, Steps: 132 | Train Loss: 0.2313962 Vali Loss: 0.1878921 Test Loss: 0.2199977
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2257610
	speed: 3.3402s/iter; left time: 37145.8260s
Epoch: 16 cost time: 203.533522605896
Epoch: 16, Steps: 132 | Train Loss: 0.2313498 Vali Loss: 0.1879298 Test Loss: 0.2198915
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2281877
	speed: 3.3671s/iter; left time: 37000.8258s
Epoch: 17 cost time: 196.50178956985474
Epoch: 17, Steps: 132 | Train Loss: 0.2312446 Vali Loss: 0.1876176 Test Loss: 0.2198550
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2328632
	speed: 3.3222s/iter; left time: 36069.1078s
Epoch: 18 cost time: 204.9770529270172
Epoch: 18, Steps: 132 | Train Loss: 0.2310831 Vali Loss: 0.1876273 Test Loss: 0.2197909
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2326210
	speed: 3.2924s/iter; left time: 35310.6422s
Epoch: 19 cost time: 198.35777187347412
Epoch: 19, Steps: 132 | Train Loss: 0.2310779 Vali Loss: 0.1878861 Test Loss: 0.2197859
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2318609
	speed: 3.1954s/iter; left time: 33848.4866s
Epoch: 20 cost time: 190.65462064743042
Epoch: 20, Steps: 132 | Train Loss: 0.2310503 Vali Loss: 0.1875821 Test Loss: 0.2197247
Validation loss decreased (0.187602 --> 0.187582).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2231242
	speed: 3.1560s/iter; left time: 33014.5882s
Epoch: 21 cost time: 188.8914830684662
Epoch: 21, Steps: 132 | Train Loss: 0.2309821 Vali Loss: 0.1878424 Test Loss: 0.2197202
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2302731
	speed: 3.2326s/iter; left time: 33389.7560s
Epoch: 22 cost time: 193.58177208900452
Epoch: 22, Steps: 132 | Train Loss: 0.2309643 Vali Loss: 0.1872855 Test Loss: 0.2196778
Validation loss decreased (0.187582 --> 0.187285).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2321987
	speed: 3.2751s/iter; left time: 33396.0020s
Epoch: 23 cost time: 193.63201904296875
Epoch: 23, Steps: 132 | Train Loss: 0.2308418 Vali Loss: 0.1875073 Test Loss: 0.2196330
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2181121
	speed: 3.0530s/iter; left time: 30728.3397s
Epoch: 24 cost time: 181.66345524787903
Epoch: 24, Steps: 132 | Train Loss: 0.2308917 Vali Loss: 0.1873140 Test Loss: 0.2196398
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2312237
	speed: 3.1200s/iter; left time: 30991.1696s
Epoch: 25 cost time: 187.8503439426422
Epoch: 25, Steps: 132 | Train Loss: 0.2307706 Vali Loss: 0.1871150 Test Loss: 0.2196203
Validation loss decreased (0.187285 --> 0.187115).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2313304
	speed: 3.2123s/iter; left time: 31483.5732s
Epoch: 26 cost time: 190.44640111923218
Epoch: 26, Steps: 132 | Train Loss: 0.2308022 Vali Loss: 0.1871795 Test Loss: 0.2196189
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2192438
	speed: 3.2746s/iter; left time: 31662.0926s
Epoch: 27 cost time: 197.3810155391693
Epoch: 27, Steps: 132 | Train Loss: 0.2306968 Vali Loss: 0.1874836 Test Loss: 0.2196045
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2138515
	speed: 3.3434s/iter; left time: 31885.8774s
Epoch: 28 cost time: 196.34544205665588
Epoch: 28, Steps: 132 | Train Loss: 0.2307908 Vali Loss: 0.1870207 Test Loss: 0.2195965
Validation loss decreased (0.187115 --> 0.187021).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2353857
	speed: 3.1361s/iter; left time: 29494.6905s
Epoch: 29 cost time: 180.82620000839233
Epoch: 29, Steps: 132 | Train Loss: 0.2307864 Vali Loss: 0.1876547 Test Loss: 0.2195396
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2316723
	speed: 2.9160s/iter; left time: 27039.9509s
Epoch: 30 cost time: 164.84851264953613
Epoch: 30, Steps: 132 | Train Loss: 0.2307015 Vali Loss: 0.1872103 Test Loss: 0.2195684
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2323694
	speed: 2.6484s/iter; left time: 24208.7615s
Epoch: 31 cost time: 157.93809008598328
Epoch: 31, Steps: 132 | Train Loss: 0.2306023 Vali Loss: 0.1878744 Test Loss: 0.2195515
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2323308
	speed: 2.9045s/iter; left time: 26166.6226s
Epoch: 32 cost time: 159.58892822265625
Epoch: 32, Steps: 132 | Train Loss: 0.2306180 Vali Loss: 0.1871201 Test Loss: 0.2195724
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2234340
	speed: 2.5392s/iter; left time: 22540.2332s
Epoch: 33 cost time: 150.8953411579132
Epoch: 33, Steps: 132 | Train Loss: 0.2307206 Vali Loss: 0.1872071 Test Loss: 0.2195447
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2274615
	speed: 2.5672s/iter; left time: 22450.4610s
Epoch: 34 cost time: 153.21490001678467
Epoch: 34, Steps: 132 | Train Loss: 0.2306507 Vali Loss: 0.1873605 Test Loss: 0.2195149
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2269530
	speed: 2.8023s/iter; left time: 24136.4192s
Epoch: 35 cost time: 191.41565442085266
Epoch: 35, Steps: 132 | Train Loss: 0.2306287 Vali Loss: 0.1872673 Test Loss: 0.2195090
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2190079
	speed: 3.1978s/iter; left time: 27120.5194s
Epoch: 36 cost time: 182.74043011665344
Epoch: 36, Steps: 132 | Train Loss: 0.2305655 Vali Loss: 0.1872892 Test Loss: 0.2195038
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2357091
	speed: 2.9831s/iter; left time: 24906.1466s
Epoch: 37 cost time: 173.55242705345154
Epoch: 37, Steps: 132 | Train Loss: 0.2305894 Vali Loss: 0.1872664 Test Loss: 0.2194964
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2338374
	speed: 2.8140s/iter; left time: 23122.9949s
Epoch: 38 cost time: 168.05358052253723
Epoch: 38, Steps: 132 | Train Loss: 0.2305584 Vali Loss: 0.1873988 Test Loss: 0.2194854
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2363273
	speed: 2.7097s/iter; left time: 21907.7020s
Epoch: 39 cost time: 158.7445845603943
Epoch: 39, Steps: 132 | Train Loss: 0.2305950 Vali Loss: 0.1871668 Test Loss: 0.2194731
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2283055
	speed: 2.7289s/iter; left time: 21702.6619s
Epoch: 40 cost time: 153.35751032829285
Epoch: 40, Steps: 132 | Train Loss: 0.2306202 Vali Loss: 0.1872381 Test Loss: 0.2194815
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2389419
	speed: 2.1148s/iter; left time: 16539.8969s
Epoch: 41 cost time: 128.5362150669098
Epoch: 41, Steps: 132 | Train Loss: 0.2306063 Vali Loss: 0.1872512 Test Loss: 0.2194949
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2393197
	speed: 2.2325s/iter; left time: 17165.6437s
Epoch: 42 cost time: 132.03293418884277
Epoch: 42, Steps: 132 | Train Loss: 0.2305867 Vali Loss: 0.1869245 Test Loss: 0.2194864
Validation loss decreased (0.187021 --> 0.186925).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2236210
	speed: 2.1149s/iter; left time: 15982.0656s
Epoch: 43 cost time: 123.28583145141602
Epoch: 43, Steps: 132 | Train Loss: 0.2305111 Vali Loss: 0.1869973 Test Loss: 0.2194712
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2326787
	speed: 2.1769s/iter; left time: 16163.6024s
Epoch: 44 cost time: 131.72835230827332
Epoch: 44, Steps: 132 | Train Loss: 0.2305381 Vali Loss: 0.1871622 Test Loss: 0.2194667
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2338758
	speed: 2.0926s/iter; left time: 15261.0640s
Epoch: 45 cost time: 124.15433168411255
Epoch: 45, Steps: 132 | Train Loss: 0.2305223 Vali Loss: 0.1874343 Test Loss: 0.2194787
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2206219
	speed: 2.1927s/iter; left time: 15702.1760s
Epoch: 46 cost time: 129.29450249671936
Epoch: 46, Steps: 132 | Train Loss: 0.2305207 Vali Loss: 0.1872209 Test Loss: 0.2194700
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2359880
	speed: 2.1048s/iter; left time: 14794.3618s
Epoch: 47 cost time: 128.08616042137146
Epoch: 47, Steps: 132 | Train Loss: 0.2304764 Vali Loss: 0.1872119 Test Loss: 0.2194578
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2270322
	speed: 2.1658s/iter; left time: 14937.5502s
Epoch: 48 cost time: 131.00923538208008
Epoch: 48, Steps: 132 | Train Loss: 0.2305477 Vali Loss: 0.1873362 Test Loss: 0.2194411
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2267598
	speed: 2.1169s/iter; left time: 14320.5739s
Epoch: 49 cost time: 131.58522367477417
Epoch: 49, Steps: 132 | Train Loss: 0.2305103 Vali Loss: 0.1869241 Test Loss: 0.2194487
Validation loss decreased (0.186925 --> 0.186924).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2265660
	speed: 2.0876s/iter; left time: 13846.7946s
Epoch: 50 cost time: 119.45115828514099
Epoch: 50, Steps: 132 | Train Loss: 0.2304877 Vali Loss: 0.1871359 Test Loss: 0.2194583
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2334970
	speed: 2.0176s/iter; left time: 13116.4825s
Epoch: 51 cost time: 118.05361914634705
Epoch: 51, Steps: 132 | Train Loss: 0.2305029 Vali Loss: 0.1871956 Test Loss: 0.2194528
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2131966
	speed: 1.8245s/iter; left time: 11619.9451s
Epoch: 52 cost time: 104.5338454246521
Epoch: 52, Steps: 132 | Train Loss: 0.2305619 Vali Loss: 0.1875171 Test Loss: 0.2194424
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2337578
	speed: 1.7150s/iter; left time: 10696.4425s
Epoch: 53 cost time: 108.74875354766846
Epoch: 53, Steps: 132 | Train Loss: 0.2305665 Vali Loss: 0.1871319 Test Loss: 0.2194413
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2299181
	speed: 1.7118s/iter; left time: 10450.5194s
Epoch: 54 cost time: 102.16148042678833
Epoch: 54, Steps: 132 | Train Loss: 0.2304440 Vali Loss: 0.1872356 Test Loss: 0.2194489
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2331847
	speed: 1.7278s/iter; left time: 10320.3358s
Epoch: 55 cost time: 102.58685564994812
Epoch: 55, Steps: 132 | Train Loss: 0.2304617 Vali Loss: 0.1871507 Test Loss: 0.2194471
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2289382
	speed: 1.6506s/iter; left time: 9641.3824s
Epoch: 56 cost time: 94.6790726184845
Epoch: 56, Steps: 132 | Train Loss: 0.2303733 Vali Loss: 0.1874184 Test Loss: 0.2194478
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2212631
	speed: 1.5857s/iter; left time: 9052.6449s
Epoch: 57 cost time: 97.65843868255615
Epoch: 57, Steps: 132 | Train Loss: 0.2305220 Vali Loss: 0.1871897 Test Loss: 0.2194458
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2320987
	speed: 1.5903s/iter; left time: 8869.3421s
Epoch: 58 cost time: 93.32890939712524
Epoch: 58, Steps: 132 | Train Loss: 0.2304614 Vali Loss: 0.1870238 Test Loss: 0.2194346
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2176937
	speed: 1.4621s/iter; left time: 7960.9598s
Epoch: 59 cost time: 85.46778512001038
Epoch: 59, Steps: 132 | Train Loss: 0.2304597 Vali Loss: 0.1874717 Test Loss: 0.2194374
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2178592
	speed: 1.3668s/iter; left time: 7261.7725s
Epoch: 60 cost time: 84.64361667633057
Epoch: 60, Steps: 132 | Train Loss: 0.2304531 Vali Loss: 0.1872967 Test Loss: 0.2194313
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2394815
	speed: 1.5004s/iter; left time: 7773.3842s
Epoch: 61 cost time: 93.5408296585083
Epoch: 61, Steps: 132 | Train Loss: 0.2304661 Vali Loss: 0.1870136 Test Loss: 0.2194389
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2318330
	speed: 1.5682s/iter; left time: 7917.6367s
Epoch: 62 cost time: 88.98148918151855
Epoch: 62, Steps: 132 | Train Loss: 0.2304325 Vali Loss: 0.1870269 Test Loss: 0.2194360
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2219172
	speed: 1.4614s/iter; left time: 7185.6541s
Epoch: 63 cost time: 91.96826171875
Epoch: 63, Steps: 132 | Train Loss: 0.2305204 Vali Loss: 0.1871763 Test Loss: 0.2194334
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2352449
	speed: 1.5013s/iter; left time: 7183.5555s
Epoch: 64 cost time: 88.50398182868958
Epoch: 64, Steps: 132 | Train Loss: 0.2303743 Vali Loss: 0.1871977 Test Loss: 0.2194278
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2402363
	speed: 1.4126s/iter; left time: 6572.5987s
Epoch: 65 cost time: 81.57672309875488
Epoch: 65, Steps: 132 | Train Loss: 0.2304232 Vali Loss: 0.1872463 Test Loss: 0.2194271
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2179077
	speed: 1.5567s/iter; left time: 7038.0010s
Epoch: 66 cost time: 101.87886357307434
Epoch: 66, Steps: 132 | Train Loss: 0.2304560 Vali Loss: 0.1870833 Test Loss: 0.2194280
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2337536
	speed: 1.6630s/iter; left time: 7299.0118s
Epoch: 67 cost time: 101.84553098678589
Epoch: 67, Steps: 132 | Train Loss: 0.2304534 Vali Loss: 0.1874431 Test Loss: 0.2194312
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2252507
	speed: 1.6126s/iter; left time: 6864.9191s
Epoch: 68 cost time: 95.5436954498291
Epoch: 68, Steps: 132 | Train Loss: 0.2304626 Vali Loss: 0.1870951 Test Loss: 0.2194288
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2400065
	speed: 1.6812s/iter; left time: 6934.9409s
Epoch: 69 cost time: 100.57859444618225
Epoch: 69, Steps: 132 | Train Loss: 0.2304595 Vali Loss: 0.1870409 Test Loss: 0.2194314
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_720_j720_H4_FITS_custom_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2177649885416031, mae:0.3132237493991852, rse:0.4655018448829651, corr:[0.44648284 0.44749835 0.4476037  0.44932464 0.45074272 0.45117462
 0.45098665 0.45056835 0.4500347  0.4495571  0.44932425 0.4493315
 0.44934165 0.449226   0.44908717 0.44914085 0.44938293 0.449602
 0.44959715 0.44936717 0.44921988 0.44942215 0.44989863 0.4503443
 0.45058924 0.4507408  0.45060956 0.45035055 0.45020443 0.45013046
 0.44999602 0.44978237 0.44953758 0.44934803 0.44924316 0.44923565
 0.44930193 0.44937843 0.44942775 0.44943216 0.44941446 0.44945213
 0.44956878 0.44962633 0.4495834  0.44950148 0.44943258 0.4494513
 0.4496434  0.44994777 0.450026   0.44982868 0.4495679  0.4493751
 0.44925258 0.44916788 0.4491075  0.44907406 0.44905925 0.4490404
 0.44901538 0.4490012  0.44904676 0.4491489  0.4492268  0.44922537
 0.4491434  0.4489804  0.4489009  0.44897345 0.44909036 0.44911417
 0.44907147 0.4490486  0.4489629  0.4488523  0.44878253 0.44873258
 0.44866443 0.44856215 0.44847754 0.448435   0.44842756 0.44845307
 0.44850275 0.4485472  0.44857135 0.44856644 0.44855016 0.44857097
 0.44862354 0.4485926  0.44845998 0.4483391  0.4482631  0.44826907
 0.4483977  0.44855887 0.4485473  0.44839588 0.4482422  0.4481526
 0.4481113  0.44808447 0.44804984 0.4480238  0.44800952 0.4480027
 0.4479906  0.44799247 0.44803646 0.4480996  0.44812968 0.44810557
 0.44800094 0.44781882 0.44770718 0.44772834 0.4478129  0.44788817
 0.4479659  0.44798687 0.447934   0.44789648 0.4478937  0.44788867
 0.44786003 0.44781664 0.4477881  0.44778445 0.44779947 0.44782722
 0.4478632  0.44788155 0.44789225 0.4478895  0.44787535 0.44788423
 0.44794977 0.4479934  0.44796085 0.44786415 0.44778416 0.44778854
 0.44787514 0.44796997 0.44793415 0.44782782 0.4477546  0.44775015
 0.44777283 0.4477782  0.44776112 0.4477316  0.44769567 0.44766685
 0.44766548 0.4476944  0.44777128 0.4478321  0.44782698 0.4477693
 0.44772667 0.44765574 0.44756514 0.44749317 0.447409   0.44730368
 0.44719955 0.44720045 0.44711176 0.44698218 0.44694614 0.4469748
 0.44697964 0.44691503 0.44681847 0.44674885 0.44671625 0.4467222
 0.44674525 0.44674832 0.4467444  0.44672155 0.44666597 0.44661966
 0.4465828  0.44642666 0.446206   0.44603315 0.44593924 0.44591367
 0.4459877  0.44615677 0.44618908 0.44607738 0.44596374 0.44591203
 0.44589832 0.4458738  0.44583574 0.4457828  0.44573054 0.44569996
 0.44569194 0.445679   0.44569305 0.44572094 0.44571775 0.44566733
 0.44556248 0.4453343  0.44512236 0.4450672  0.44509906 0.445126
 0.44516414 0.44525522 0.445269   0.44521862 0.4451985  0.44522047
 0.44524193 0.44522715 0.44519913 0.44515303 0.44510505 0.4450635
 0.44503748 0.4450006  0.44495773 0.44489795 0.4448424  0.44481325
 0.44481325 0.44471025 0.44455773 0.44443992 0.4443941  0.44440305
 0.44449812 0.4446655  0.44471565 0.44464028 0.44455504 0.44452375
 0.44453612 0.4445309  0.44451326 0.44448084 0.44443765 0.44440016
 0.44436958 0.44434378 0.44433367 0.4443297  0.44429553 0.44422618
 0.44412413 0.44398034 0.44388306 0.4438946  0.44396767 0.44401893
 0.44406667 0.4441222  0.44412068 0.44409183 0.4440965  0.44411638
 0.44410717 0.444062   0.4440029  0.4439505  0.443916   0.4438973
 0.44389713 0.44388273 0.4438602  0.4438008  0.4437225  0.44367588
 0.44363612 0.44354364 0.44345453 0.44341242 0.4434286  0.44351912
 0.4437222  0.44391608 0.44398275 0.443973   0.44395337 0.443945
 0.4439454  0.44393265 0.44389805 0.44383922 0.44377518 0.4437241
 0.44368964 0.44366798 0.44367358 0.44368976 0.44368118 0.44364262
 0.44361308 0.44358116 0.4435625  0.44357985 0.44361407 0.44364002
 0.44365874 0.4436811  0.4436542  0.4436229  0.44363707 0.44368365
 0.44370902 0.44368136 0.4436337  0.44358528 0.44353873 0.4434925
 0.44347286 0.4434706  0.44348297 0.44346115 0.44339606 0.4433463
 0.44336405 0.4433483  0.44323042 0.44306841 0.44287008 0.44270337
 0.44264728 0.44277552 0.44280544 0.44270942 0.44260842 0.44254747
 0.4425044  0.44245002 0.4423729  0.4422801  0.4421784  0.44208285
 0.44200674 0.44193724 0.44191703 0.44192988 0.44193274 0.44191158
 0.44186082 0.44171885 0.4415623  0.44151708 0.4415377  0.4415613
 0.44159678 0.4416898  0.44169056 0.44161505 0.44156802 0.44156742
 0.44155598 0.44149542 0.4414018  0.44130433 0.44122797 0.44117686
 0.44115072 0.44110748 0.44105917 0.44099662 0.4409209  0.4408645
 0.4408482  0.44078928 0.4407248  0.44068968 0.44063708 0.44057834
 0.4406093  0.4407601  0.4408695  0.44087523 0.4408466  0.4408414
 0.44084516 0.44083267 0.4407869  0.44071108 0.44061974 0.44054195
 0.44048542 0.44042358 0.4403741  0.44033888 0.44031122 0.44029176
 0.44025433 0.4401189  0.43996707 0.43992645 0.43996423 0.44001448
 0.4400815  0.44017318 0.44020462 0.44019312 0.4402076  0.44025353
 0.44026482 0.44020057 0.44010448 0.44003254 0.439997   0.4399709
 0.43996993 0.43995535 0.43993187 0.4398913  0.43983057 0.43978393
 0.43976393 0.43970317 0.43964335 0.43962276 0.43961063 0.43960467
 0.43968654 0.43984684 0.4399641  0.4400044  0.44000807 0.4399902
 0.43995756 0.43990278 0.43984345 0.43977508 0.4396993  0.43962377
 0.43955272 0.4394872  0.43945885 0.4394608  0.43947336 0.43949255
 0.4394432  0.43927702 0.43913174 0.43910974 0.43921646 0.43939412
 0.43963024 0.43979555 0.43983927 0.43985498 0.4398993  0.43995395
 0.43998098 0.43993726 0.43983832 0.439715   0.4396014  0.43952355
 0.43950233 0.4394917  0.4394897  0.43946552 0.43940443 0.43935055
 0.43937957 0.43945912 0.43952787 0.43955752 0.43953454 0.43950236
 0.43954527 0.4396654  0.4397473  0.43977663 0.43979108 0.4398067
 0.43979782 0.4397455  0.43968096 0.4396256  0.4395798  0.43953174
 0.43947873 0.43942592 0.43941557 0.4394338  0.43945435 0.43947726
 0.43948713 0.4393846  0.43921527 0.4390818  0.4389915  0.438928
 0.4389015  0.4389857  0.4389649  0.43885928 0.4387906  0.43879077
 0.43878406 0.43870243 0.4385548  0.43839484 0.43825278 0.4381572
 0.43811473 0.43808094 0.43805507 0.43802184 0.43794703 0.43786567
 0.4377917  0.43766102 0.4375364  0.4374831  0.4374434  0.43738663
 0.4373949  0.43755388 0.43767375 0.43766797 0.43760115 0.4375345
 0.4374592  0.43736514 0.43724662 0.4371231  0.4370034  0.4369038
 0.43681988 0.43672377 0.4366522  0.43662447 0.43662703 0.4366488
 0.43661937 0.43645656 0.43632433 0.43629485 0.43634313 0.4364146
 0.43650687 0.43661422 0.4366268  0.43653768 0.43645725 0.43643695
 0.43645027 0.43641412 0.43627876 0.43607745 0.43588412 0.43574423
 0.43566486 0.4355899  0.4355252  0.43543735 0.43529713 0.43512714
 0.4349994  0.43492168 0.43491098 0.43496004 0.43499586 0.43497017
 0.4350045  0.43516493 0.43531144 0.4353521  0.4353378  0.43529737
 0.43524405 0.43515068 0.4350194  0.43489036 0.434772   0.434671
 0.43458092 0.43446857 0.43439227 0.43436667 0.4343821  0.43441746
 0.43444118 0.43437237 0.43427855 0.43425477 0.4342969  0.43437073
 0.43450832 0.43464497 0.4346553  0.43457705 0.43452317 0.43452767
 0.43452153 0.43443656 0.4342586  0.43405715 0.43388766 0.43379048
 0.4337649  0.43377134 0.4337946  0.43379694 0.43375552 0.43372017
 0.43370968 0.43369144 0.43371713 0.4338093  0.4338901  0.43394652
 0.4341127  0.43430275 0.43441752 0.43445444 0.4344432  0.43437484
 0.43425232 0.43408096 0.43390572 0.43374842 0.43361536 0.4335035
 0.43339884 0.43329817 0.43327078 0.43331805 0.43340603 0.4334966
 0.43358198 0.433625   0.43362507 0.4336461  0.43371978 0.43383208
 0.4339867  0.4341217  0.43409404 0.43396023 0.4338641  0.4338526
 0.43384787 0.4337357  0.43351227 0.43325752 0.4330688  0.43299115
 0.43300942 0.43307278 0.43314707 0.43318766 0.43317318 0.43315753
 0.43319914 0.43323797 0.4332789  0.43329853 0.4332155  0.43301833
 0.43284982 0.43293387 0.43299446 0.4328757  0.43264475 0.43237013
 0.43210527 0.4318659  0.4316792  0.43153194 0.43137679 0.43120682
 0.4310789  0.43104285 0.43115416 0.4313645  0.43156627 0.43169463
 0.43171132 0.43156517 0.43137407 0.43134022 0.43144244 0.43151677
 0.43150032 0.4314009  0.4311198  0.43078008 0.43062624 0.4306264
 0.43050078 0.43009338 0.4296244  0.4294416  0.42966384 0.4300884
 0.43040106 0.43050635 0.43055728 0.43066132 0.43073615 0.43081468
 0.43092462 0.43066466 0.42966998 0.42404187 0.42456752 0.4291159 ]
