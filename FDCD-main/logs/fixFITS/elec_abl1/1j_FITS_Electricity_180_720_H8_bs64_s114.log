Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_180_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_180_j720_H8_FITS_custom_ftM_sl180_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17513
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=74, out_features=370, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1124989440.0
params:  27750.0
Trainable parameters:  27750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7551667
	speed: 0.9569s/iter; left time: 12919.2304s
Epoch: 1 cost time: 128.91094517707825
Epoch: 1, Steps: 136 | Train Loss: 1.0906791 Vali Loss: 0.5396263 Test Loss: 0.6028688
Validation loss decreased (inf --> 0.539626).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4262696
	speed: 2.1808s/iter; left time: 29146.4779s
Epoch: 2 cost time: 117.66046524047852
Epoch: 2, Steps: 136 | Train Loss: 0.4937222 Vali Loss: 0.3337044 Test Loss: 0.3759248
Validation loss decreased (0.539626 --> 0.333704).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3431467
	speed: 2.1278s/iter; left time: 28149.0740s
Epoch: 3 cost time: 123.1049964427948
Epoch: 3, Steps: 136 | Train Loss: 0.3555493 Vali Loss: 0.2686515 Test Loss: 0.3057158
Validation loss decreased (0.333704 --> 0.268651).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2827774
	speed: 2.1807s/iter; left time: 28552.1228s
Epoch: 4 cost time: 123.84143447875977
Epoch: 4, Steps: 136 | Train Loss: 0.3067487 Vali Loss: 0.2401399 Test Loss: 0.2760489
Validation loss decreased (0.268651 --> 0.240140).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2742943
	speed: 1.9846s/iter; left time: 25714.4250s
Epoch: 5 cost time: 96.16278386116028
Epoch: 5, Steps: 136 | Train Loss: 0.2828876 Vali Loss: 0.2245274 Test Loss: 0.2598119
Validation loss decreased (0.240140 --> 0.224527).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2525417
	speed: 1.4350s/iter; left time: 18398.2548s
Epoch: 6 cost time: 80.28331232070923
Epoch: 6, Steps: 136 | Train Loss: 0.2689312 Vali Loss: 0.2142321 Test Loss: 0.2497689
Validation loss decreased (0.224527 --> 0.214232).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2664662
	speed: 1.4406s/iter; left time: 18273.4182s
Epoch: 7 cost time: 84.65082907676697
Epoch: 7, Steps: 136 | Train Loss: 0.2600784 Vali Loss: 0.2081214 Test Loss: 0.2432374
Validation loss decreased (0.214232 --> 0.208121).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2605788
	speed: 1.4769s/iter; left time: 18533.6166s
Epoch: 8 cost time: 82.8158745765686
Epoch: 8, Steps: 136 | Train Loss: 0.2543934 Vali Loss: 0.2041208 Test Loss: 0.2388772
Validation loss decreased (0.208121 --> 0.204121).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2488523
	speed: 1.4980s/iter; left time: 18594.7949s
Epoch: 9 cost time: 96.3312680721283
Epoch: 9, Steps: 136 | Train Loss: 0.2505637 Vali Loss: 0.2017246 Test Loss: 0.2358904
Validation loss decreased (0.204121 --> 0.201725).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2561578
	speed: 2.1762s/iter; left time: 26716.9168s
Epoch: 10 cost time: 123.36835765838623
Epoch: 10, Steps: 136 | Train Loss: 0.2478637 Vali Loss: 0.1996513 Test Loss: 0.2338112
Validation loss decreased (0.201725 --> 0.199651).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2475625
	speed: 1.7777s/iter; left time: 21582.9357s
Epoch: 11 cost time: 82.03582954406738
Epoch: 11, Steps: 136 | Train Loss: 0.2460882 Vali Loss: 0.1984090 Test Loss: 0.2322950
Validation loss decreased (0.199651 --> 0.198409).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2469801
	speed: 1.4694s/iter; left time: 17640.0382s
Epoch: 12 cost time: 82.64844393730164
Epoch: 12, Steps: 136 | Train Loss: 0.2447928 Vali Loss: 0.1981894 Test Loss: 0.2312179
Validation loss decreased (0.198409 --> 0.198189).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2304239
	speed: 1.4539s/iter; left time: 17256.5047s
Epoch: 13 cost time: 83.98616170883179
Epoch: 13, Steps: 136 | Train Loss: 0.2437691 Vali Loss: 0.1970547 Test Loss: 0.2304159
Validation loss decreased (0.198189 --> 0.197055).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2405853
	speed: 1.4458s/iter; left time: 16963.5551s
Epoch: 14 cost time: 81.9408950805664
Epoch: 14, Steps: 136 | Train Loss: 0.2431303 Vali Loss: 0.1967938 Test Loss: 0.2297861
Validation loss decreased (0.197055 --> 0.196794).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2310296
	speed: 1.5149s/iter; left time: 17568.8227s
Epoch: 15 cost time: 87.56748056411743
Epoch: 15, Steps: 136 | Train Loss: 0.2425963 Vali Loss: 0.1965210 Test Loss: 0.2292869
Validation loss decreased (0.196794 --> 0.196521).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2510381
	speed: 1.5280s/iter; left time: 17512.5344s
Epoch: 16 cost time: 87.4178376197815
Epoch: 16, Steps: 136 | Train Loss: 0.2421421 Vali Loss: 0.1959507 Test Loss: 0.2288878
Validation loss decreased (0.196521 --> 0.195951).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2587709
	speed: 1.5725s/iter; left time: 17808.1750s
Epoch: 17 cost time: 86.2396457195282
Epoch: 17, Steps: 136 | Train Loss: 0.2417749 Vali Loss: 0.1957204 Test Loss: 0.2285667
Validation loss decreased (0.195951 --> 0.195720).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2393134
	speed: 1.4838s/iter; left time: 16602.7762s
Epoch: 18 cost time: 82.44295287132263
Epoch: 18, Steps: 136 | Train Loss: 0.2414838 Vali Loss: 0.1957587 Test Loss: 0.2282892
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2238695
	speed: 1.3938s/iter; left time: 15405.3469s
Epoch: 19 cost time: 77.71774363517761
Epoch: 19, Steps: 136 | Train Loss: 0.2412448 Vali Loss: 0.1951414 Test Loss: 0.2280537
Validation loss decreased (0.195720 --> 0.195141).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2481025
	speed: 1.3881s/iter; left time: 15154.3525s
Epoch: 20 cost time: 79.56269812583923
Epoch: 20, Steps: 136 | Train Loss: 0.2410337 Vali Loss: 0.1952118 Test Loss: 0.2278559
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2410105
	speed: 1.3604s/iter; left time: 14665.9661s
Epoch: 21 cost time: 78.58195948600769
Epoch: 21, Steps: 136 | Train Loss: 0.2407418 Vali Loss: 0.1950476 Test Loss: 0.2276633
Validation loss decreased (0.195141 --> 0.195048).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2295321
	speed: 1.6051s/iter; left time: 17085.9666s
Epoch: 22 cost time: 103.47094368934631
Epoch: 22, Steps: 136 | Train Loss: 0.2406916 Vali Loss: 0.1946370 Test Loss: 0.2275249
Validation loss decreased (0.195048 --> 0.194637).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2471228
	speed: 2.1347s/iter; left time: 22433.1289s
Epoch: 23 cost time: 122.42874383926392
Epoch: 23, Steps: 136 | Train Loss: 0.2404027 Vali Loss: 0.1948936 Test Loss: 0.2273895
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2366055
	speed: 2.1192s/iter; left time: 21982.6240s
Epoch: 24 cost time: 120.64281558990479
Epoch: 24, Steps: 136 | Train Loss: 0.2402054 Vali Loss: 0.1946319 Test Loss: 0.2272706
Validation loss decreased (0.194637 --> 0.194632).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2264772
	speed: 1.9862s/iter; left time: 20333.1920s
Epoch: 25 cost time: 107.96209740638733
Epoch: 25, Steps: 136 | Train Loss: 0.2401961 Vali Loss: 0.1943901 Test Loss: 0.2271524
Validation loss decreased (0.194632 --> 0.194390).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2395176
	speed: 1.8246s/iter; left time: 18430.1514s
Epoch: 26 cost time: 100.3820595741272
Epoch: 26, Steps: 136 | Train Loss: 0.2399603 Vali Loss: 0.1942294 Test Loss: 0.2270541
Validation loss decreased (0.194390 --> 0.194229).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2343697
	speed: 1.4330s/iter; left time: 14280.2711s
Epoch: 27 cost time: 69.45417428016663
Epoch: 27, Steps: 136 | Train Loss: 0.2400284 Vali Loss: 0.1942490 Test Loss: 0.2269520
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2415239
	speed: 1.2358s/iter; left time: 12146.6053s
Epoch: 28 cost time: 66.83718538284302
Epoch: 28, Steps: 136 | Train Loss: 0.2398799 Vali Loss: 0.1945964 Test Loss: 0.2268611
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2296036
	speed: 1.8810s/iter; left time: 18232.7697s
Epoch: 29 cost time: 113.12495398521423
Epoch: 29, Steps: 136 | Train Loss: 0.2398660 Vali Loss: 0.1942096 Test Loss: 0.2267936
Validation loss decreased (0.194229 --> 0.194210).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2360009
	speed: 1.9652s/iter; left time: 18781.3185s
Epoch: 30 cost time: 112.39183330535889
Epoch: 30, Steps: 136 | Train Loss: 0.2397056 Vali Loss: 0.1942417 Test Loss: 0.2267403
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2408978
	speed: 2.1068s/iter; left time: 19848.5486s
Epoch: 31 cost time: 117.44029021263123
Epoch: 31, Steps: 136 | Train Loss: 0.2397354 Vali Loss: 0.1940306 Test Loss: 0.2266697
Validation loss decreased (0.194210 --> 0.194031).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2314796
	speed: 2.0240s/iter; left time: 18793.0108s
Epoch: 32 cost time: 113.61927533149719
Epoch: 32, Steps: 136 | Train Loss: 0.2396017 Vali Loss: 0.1942530 Test Loss: 0.2266199
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2583773
	speed: 1.9978s/iter; left time: 18278.1771s
Epoch: 33 cost time: 114.1781907081604
Epoch: 33, Steps: 136 | Train Loss: 0.2395240 Vali Loss: 0.1938575 Test Loss: 0.2265702
Validation loss decreased (0.194031 --> 0.193857).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2440701
	speed: 2.0447s/iter; left time: 18428.9619s
Epoch: 34 cost time: 111.34036588668823
Epoch: 34, Steps: 136 | Train Loss: 0.2394726 Vali Loss: 0.1938341 Test Loss: 0.2265322
Validation loss decreased (0.193857 --> 0.193834).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2442262
	speed: 1.8097s/iter; left time: 16064.3282s
Epoch: 35 cost time: 99.95134830474854
Epoch: 35, Steps: 136 | Train Loss: 0.2395173 Vali Loss: 0.1940324 Test Loss: 0.2264831
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2319928
	speed: 1.9600s/iter; left time: 17132.3215s
Epoch: 36 cost time: 116.80435943603516
Epoch: 36, Steps: 136 | Train Loss: 0.2394199 Vali Loss: 0.1939029 Test Loss: 0.2264531
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2417741
	speed: 1.8039s/iter; left time: 15522.8039s
Epoch: 37 cost time: 91.58121824264526
Epoch: 37, Steps: 136 | Train Loss: 0.2394079 Vali Loss: 0.1936736 Test Loss: 0.2264216
Validation loss decreased (0.193834 --> 0.193674).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2441556
	speed: 1.4599s/iter; left time: 12363.6998s
Epoch: 38 cost time: 76.19088292121887
Epoch: 38, Steps: 136 | Train Loss: 0.2393657 Vali Loss: 0.1934864 Test Loss: 0.2263809
Validation loss decreased (0.193674 --> 0.193486).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2455485
	speed: 1.2951s/iter; left time: 10791.7864s
Epoch: 39 cost time: 74.80386209487915
Epoch: 39, Steps: 136 | Train Loss: 0.2392982 Vali Loss: 0.1936532 Test Loss: 0.2263641
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2319134
	speed: 1.3731s/iter; left time: 11254.9001s
Epoch: 40 cost time: 76.31565237045288
Epoch: 40, Steps: 136 | Train Loss: 0.2392704 Vali Loss: 0.1937045 Test Loss: 0.2263391
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2448127
	speed: 1.3554s/iter; left time: 10925.5206s
Epoch: 41 cost time: 80.83340907096863
Epoch: 41, Steps: 136 | Train Loss: 0.2393252 Vali Loss: 0.1935820 Test Loss: 0.2263120
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2398624
	speed: 1.3309s/iter; left time: 10547.5425s
Epoch: 42 cost time: 71.56654334068298
Epoch: 42, Steps: 136 | Train Loss: 0.2391677 Vali Loss: 0.1935770 Test Loss: 0.2262941
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2386703
	speed: 1.2457s/iter; left time: 9702.8750s
Epoch: 43 cost time: 69.1294493675232
Epoch: 43, Steps: 136 | Train Loss: 0.2392361 Vali Loss: 0.1935878 Test Loss: 0.2262783
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2443682
	speed: 1.2128s/iter; left time: 9281.7591s
Epoch: 44 cost time: 67.93645715713501
Epoch: 44, Steps: 136 | Train Loss: 0.2391059 Vali Loss: 0.1932540 Test Loss: 0.2262536
Validation loss decreased (0.193486 --> 0.193254).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2340479
	speed: 1.1429s/iter; left time: 8591.2216s
Epoch: 45 cost time: 61.747726917266846
Epoch: 45, Steps: 136 | Train Loss: 0.2392186 Vali Loss: 0.1936903 Test Loss: 0.2262462
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2490454
	speed: 1.1673s/iter; left time: 8615.9968s
Epoch: 46 cost time: 66.04469299316406
Epoch: 46, Steps: 136 | Train Loss: 0.2390754 Vali Loss: 0.1933897 Test Loss: 0.2262308
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2416692
	speed: 1.2723s/iter; left time: 9217.9189s
Epoch: 47 cost time: 72.23914980888367
Epoch: 47, Steps: 136 | Train Loss: 0.2391665 Vali Loss: 0.1932988 Test Loss: 0.2262055
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2556181
	speed: 1.2490s/iter; left time: 8879.4671s
Epoch: 48 cost time: 67.6663761138916
Epoch: 48, Steps: 136 | Train Loss: 0.2391704 Vali Loss: 0.1935071 Test Loss: 0.2261961
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2394269
	speed: 1.2182s/iter; left time: 8494.7869s
Epoch: 49 cost time: 71.54284381866455
Epoch: 49, Steps: 136 | Train Loss: 0.2391292 Vali Loss: 0.1933037 Test Loss: 0.2261851
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2465201
	speed: 1.2254s/iter; left time: 8378.1834s
Epoch: 50 cost time: 69.84683465957642
Epoch: 50, Steps: 136 | Train Loss: 0.2390661 Vali Loss: 0.1934125 Test Loss: 0.2261782
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2399477
	speed: 1.1867s/iter; left time: 7952.0816s
Epoch: 51 cost time: 68.26336002349854
Epoch: 51, Steps: 136 | Train Loss: 0.2389185 Vali Loss: 0.1933111 Test Loss: 0.2261624
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2380325
	speed: 1.2171s/iter; left time: 7990.4204s
Epoch: 52 cost time: 68.98485255241394
Epoch: 52, Steps: 136 | Train Loss: 0.2391267 Vali Loss: 0.1937426 Test Loss: 0.2261550
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2341831
	speed: 1.3097s/iter; left time: 8420.0096s
Epoch: 53 cost time: 76.51712965965271
Epoch: 53, Steps: 136 | Train Loss: 0.2390254 Vali Loss: 0.1936022 Test Loss: 0.2261385
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2328950
	speed: 1.3392s/iter; left time: 8427.3455s
Epoch: 54 cost time: 76.37734937667847
Epoch: 54, Steps: 136 | Train Loss: 0.2391160 Vali Loss: 0.1933742 Test Loss: 0.2261412
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2332059
	speed: 1.6959s/iter; left time: 10441.7934s
Epoch: 55 cost time: 97.58375191688538
Epoch: 55, Steps: 136 | Train Loss: 0.2389372 Vali Loss: 0.1933274 Test Loss: 0.2261339
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2301256
	speed: 1.6346s/iter; left time: 9841.9676s
Epoch: 56 cost time: 90.04549360275269
Epoch: 56, Steps: 136 | Train Loss: 0.2390106 Vali Loss: 0.1934223 Test Loss: 0.2261307
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2393150
	speed: 1.6860s/iter; left time: 9922.1321s
Epoch: 57 cost time: 93.68227672576904
Epoch: 57, Steps: 136 | Train Loss: 0.2389680 Vali Loss: 0.1931900 Test Loss: 0.2261242
Validation loss decreased (0.193254 --> 0.193190).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2251628
	speed: 1.6128s/iter; left time: 9272.1251s
Epoch: 58 cost time: 88.53608083724976
Epoch: 58, Steps: 136 | Train Loss: 0.2389852 Vali Loss: 0.1937233 Test Loss: 0.2261182
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2318389
	speed: 1.5477s/iter; left time: 8687.0865s
Epoch: 59 cost time: 84.38167834281921
Epoch: 59, Steps: 136 | Train Loss: 0.2390146 Vali Loss: 0.1934961 Test Loss: 0.2261129
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2458198
	speed: 1.3181s/iter; left time: 7219.0053s
Epoch: 60 cost time: 73.09393763542175
Epoch: 60, Steps: 136 | Train Loss: 0.2389827 Vali Loss: 0.1935957 Test Loss: 0.2261066
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2420926
	speed: 1.3368s/iter; left time: 7139.8033s
Epoch: 61 cost time: 74.88534379005432
Epoch: 61, Steps: 136 | Train Loss: 0.2389166 Vali Loss: 0.1934695 Test Loss: 0.2260988
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2371345
	speed: 1.3511s/iter; left time: 7032.2530s
Epoch: 62 cost time: 76.63523316383362
Epoch: 62, Steps: 136 | Train Loss: 0.2389996 Vali Loss: 0.1934910 Test Loss: 0.2260969
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2301674
	speed: 1.3419s/iter; left time: 6801.9134s
Epoch: 63 cost time: 72.52824640274048
Epoch: 63, Steps: 136 | Train Loss: 0.2389535 Vali Loss: 0.1934739 Test Loss: 0.2260918
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2443783
	speed: 1.2646s/iter; left time: 6238.4907s
Epoch: 64 cost time: 76.31755995750427
Epoch: 64, Steps: 136 | Train Loss: 0.2389433 Vali Loss: 0.1933304 Test Loss: 0.2260874
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2560665
	speed: 1.3094s/iter; left time: 6281.4218s
Epoch: 65 cost time: 76.39513301849365
Epoch: 65, Steps: 136 | Train Loss: 0.2389948 Vali Loss: 0.1933500 Test Loss: 0.2260834
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2317598
	speed: 1.1850s/iter; left time: 5523.5072s
Epoch: 66 cost time: 62.59645938873291
Epoch: 66, Steps: 136 | Train Loss: 0.2390128 Vali Loss: 0.1933393 Test Loss: 0.2260857
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2539168
	speed: 1.2769s/iter; left time: 5777.9268s
Epoch: 67 cost time: 73.27153491973877
Epoch: 67, Steps: 136 | Train Loss: 0.2389064 Vali Loss: 0.1931923 Test Loss: 0.2260809
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2288165
	speed: 1.1351s/iter; left time: 4982.0110s
Epoch: 68 cost time: 63.11532926559448
Epoch: 68, Steps: 136 | Train Loss: 0.2390063 Vali Loss: 0.1931920 Test Loss: 0.2260783
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2403707
	speed: 1.1448s/iter; left time: 4868.6281s
Epoch: 69 cost time: 62.26531505584717
Epoch: 69, Steps: 136 | Train Loss: 0.2389653 Vali Loss: 0.1934436 Test Loss: 0.2260744
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2583415
	speed: 1.0351s/iter; left time: 4261.5601s
Epoch: 70 cost time: 57.678889751434326
Epoch: 70, Steps: 136 | Train Loss: 0.2389967 Vali Loss: 0.1936413 Test Loss: 0.2260704
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2428588
	speed: 1.0001s/iter; left time: 3981.3703s
Epoch: 71 cost time: 57.796720027923584
Epoch: 71, Steps: 136 | Train Loss: 0.2389312 Vali Loss: 0.1931630 Test Loss: 0.2260736
Validation loss decreased (0.193190 --> 0.193163).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2451970
	speed: 1.3375s/iter; left time: 5142.5077s
Epoch: 72 cost time: 86.65004348754883
Epoch: 72, Steps: 136 | Train Loss: 0.2388695 Vali Loss: 0.1933929 Test Loss: 0.2260681
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2402002
	speed: 1.3690s/iter; left time: 5077.7032s
Epoch: 73 cost time: 77.08085179328918
Epoch: 73, Steps: 136 | Train Loss: 0.2389716 Vali Loss: 0.1929132 Test Loss: 0.2260686
Validation loss decreased (0.193163 --> 0.192913).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.2414446
	speed: 1.2352s/iter; left time: 4413.2893s
Epoch: 74 cost time: 68.34223651885986
Epoch: 74, Steps: 136 | Train Loss: 0.2389734 Vali Loss: 0.1939161 Test Loss: 0.2260665
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2357105
	speed: 1.1463s/iter; left time: 3939.6790s
Epoch: 75 cost time: 61.22846055030823
Epoch: 75, Steps: 136 | Train Loss: 0.2388554 Vali Loss: 0.1931617 Test Loss: 0.2260637
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2413569
	speed: 1.0011s/iter; left time: 3304.6457s
Epoch: 76 cost time: 54.21503043174744
Epoch: 76, Steps: 136 | Train Loss: 0.2388843 Vali Loss: 0.1933974 Test Loss: 0.2260643
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2432806
	speed: 0.9181s/iter; left time: 2905.7076s
Epoch: 77 cost time: 51.3218719959259
Epoch: 77, Steps: 136 | Train Loss: 0.2389683 Vali Loss: 0.1932433 Test Loss: 0.2260608
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.2259533
	speed: 1.0051s/iter; left time: 3044.5140s
Epoch: 78 cost time: 57.39701819419861
Epoch: 78, Steps: 136 | Train Loss: 0.2388640 Vali Loss: 0.1930159 Test Loss: 0.2260596
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.2440207
	speed: 1.0768s/iter; left time: 3115.1889s
Epoch: 79 cost time: 57.03759479522705
Epoch: 79, Steps: 136 | Train Loss: 0.2390242 Vali Loss: 0.1932219 Test Loss: 0.2260579
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.2408836
	speed: 0.9853s/iter; left time: 2716.3470s
Epoch: 80 cost time: 57.79227614402771
Epoch: 80, Steps: 136 | Train Loss: 0.2389745 Vali Loss: 0.1935129 Test Loss: 0.2260569
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.2502043
	speed: 1.0614s/iter; left time: 2781.9317s
Epoch: 81 cost time: 59.8704776763916
Epoch: 81, Steps: 136 | Train Loss: 0.2389923 Vali Loss: 0.1933247 Test Loss: 0.2260532
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.2354825
	speed: 1.0204s/iter; left time: 2535.7393s
Epoch: 82 cost time: 56.16708946228027
Epoch: 82, Steps: 136 | Train Loss: 0.2388686 Vali Loss: 0.1932863 Test Loss: 0.2260561
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.2299925
	speed: 1.0531s/iter; left time: 2473.6147s
Epoch: 83 cost time: 58.71920609474182
Epoch: 83, Steps: 136 | Train Loss: 0.2389694 Vali Loss: 0.1934488 Test Loss: 0.2260537
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.2323688
	speed: 1.0877s/iter; left time: 2407.0173s
Epoch: 84 cost time: 59.84607267379761
Epoch: 84, Steps: 136 | Train Loss: 0.2388746 Vali Loss: 0.1931763 Test Loss: 0.2260519
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.2392764
	speed: 0.8518s/iter; left time: 1769.2410s
Epoch: 85 cost time: 47.37704944610596
Epoch: 85, Steps: 136 | Train Loss: 0.2389189 Vali Loss: 0.1935305 Test Loss: 0.2260506
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.2410026
	speed: 0.9928s/iter; left time: 1926.9566s
Epoch: 86 cost time: 58.63544464111328
Epoch: 86, Steps: 136 | Train Loss: 0.2389214 Vali Loss: 0.1932933 Test Loss: 0.2260508
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.2438653
	speed: 0.9451s/iter; left time: 1705.9870s
Epoch: 87 cost time: 53.92795276641846
Epoch: 87, Steps: 136 | Train Loss: 0.2389315 Vali Loss: 0.1932732 Test Loss: 0.2260497
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.2424758
	speed: 0.9546s/iter; left time: 1593.2678s
Epoch: 88 cost time: 53.19363975524902
Epoch: 88, Steps: 136 | Train Loss: 0.2389037 Vali Loss: 0.1933348 Test Loss: 0.2260509
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.766650946003179e-06
	iters: 100, epoch: 89 | loss: 0.2314187
	speed: 0.9711s/iter; left time: 1488.6935s
Epoch: 89 cost time: 52.58196139335632
Epoch: 89, Steps: 136 | Train Loss: 0.2389241 Vali Loss: 0.1931247 Test Loss: 0.2260493
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.47831839870302e-06
	iters: 100, epoch: 90 | loss: 0.2399208
	speed: 0.9625s/iter; left time: 1344.5785s
Epoch: 90 cost time: 57.240273237228394
Epoch: 90, Steps: 136 | Train Loss: 0.2389504 Vali Loss: 0.1932128 Test Loss: 0.2260470
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.204402478767869e-06
	iters: 100, epoch: 91 | loss: 0.2293508
	speed: 1.0606s/iter; left time: 1337.4183s
Epoch: 91 cost time: 61.05474328994751
Epoch: 91, Steps: 136 | Train Loss: 0.2388794 Vali Loss: 0.1935223 Test Loss: 0.2260484
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.944182354829475e-06
	iters: 100, epoch: 92 | loss: 0.2264766
	speed: 1.0108s/iter; left time: 1137.1555s
Epoch: 92 cost time: 54.87960433959961
Epoch: 92, Steps: 136 | Train Loss: 0.2388813 Vali Loss: 0.1933375 Test Loss: 0.2260471
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.696973237088e-06
	iters: 100, epoch: 93 | loss: 0.2507641
	speed: 0.9482s/iter; left time: 937.7725s
Epoch: 93 cost time: 52.15602254867554
Epoch: 93, Steps: 136 | Train Loss: 0.2388825 Vali Loss: 0.1934761 Test Loss: 0.2260467
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_180_j720_H8_FITS_custom_ftM_sl180_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.22448353469371796, mae:0.30888357758522034, rse:0.4726281762123108, corr:[0.44653797 0.44684225 0.44805643 0.4481885  0.4491031  0.4492176
 0.4492855  0.4493653  0.44861057 0.44850683 0.44808653 0.44772407
 0.44770366 0.44741952 0.44747093 0.44739047 0.44733667 0.44731253
 0.44724917 0.44729072 0.44731197 0.4473692  0.44798702 0.4479493
 0.4477212  0.4476834  0.44746956 0.4474067  0.44721806 0.44699138
 0.44683442 0.44658783 0.4463398  0.44623253 0.44605684 0.44594848
 0.44591266 0.4458486  0.4458511  0.44577748 0.44577625 0.44571948
 0.44567302 0.44566783 0.4456132  0.4456153  0.44598457 0.445956
 0.4458185  0.44583872 0.4456919  0.44561332 0.44546416 0.44533315
 0.44526938 0.44516566 0.44508177 0.44509065 0.4450324  0.4450263
 0.4450487  0.44500217 0.4450101  0.44491214 0.44490215 0.4448762
 0.44478557 0.44469607 0.44468176 0.44463554 0.4449045  0.444862
 0.4446694  0.4447395  0.44460618 0.44450754 0.4443711  0.4442627
 0.44422212 0.44417652 0.44409573 0.4441165  0.44411048 0.44406387
 0.44410005 0.44404554 0.44407314 0.4440172  0.44401175 0.44400764
 0.4439596  0.4438689  0.44385138 0.4438221  0.44407898 0.4441376
 0.44391537 0.44394392 0.44383177 0.44374567 0.44366756 0.44361237
 0.44357902 0.4435698  0.4435171  0.44353583 0.44357127 0.4435591
 0.44364473 0.4435639  0.44355327 0.4435074  0.44348532 0.44345078
 0.443365   0.44323286 0.4431945  0.44315326 0.4432962  0.4434365
 0.44326815 0.44335192 0.44335753 0.44332847 0.44332385 0.4433572
 0.4433695  0.44340536 0.44344035 0.4434756  0.4435116  0.44345605
 0.44358146 0.44357717 0.44355834 0.44350517 0.4434109  0.44339943
 0.44341415 0.4434387  0.44354996 0.4436769  0.4439315  0.4441301
 0.4440053  0.44407168 0.44414893 0.44415882 0.44417223 0.4442026
 0.4442219  0.444299   0.4443838  0.44444725 0.44454837 0.4446198
 0.444857   0.44500655 0.44489643 0.4448406  0.44483975 0.44496664
 0.44512632 0.44521585 0.4453852  0.44564554 0.4461332  0.44579956
 0.44501024 0.4445168  0.44418758 0.44382456 0.44351748 0.44328025
 0.44310033 0.44296333 0.44284305 0.44267803 0.44257239 0.44248158
 0.44251162 0.44239938 0.44233274 0.44232264 0.44230682 0.44236237
 0.44235536 0.44227394 0.44225672 0.4422919  0.4424778  0.44235542
 0.44193894 0.4416778  0.44153595 0.44132808 0.4411315  0.44104257
 0.44090784 0.440766   0.44071966 0.44064057 0.44058982 0.44050205
 0.4405658  0.44053283 0.44048175 0.44049543 0.4405335  0.44054565
 0.44056246 0.44049945 0.4404337  0.4404651  0.4405612  0.44048125
 0.4403213  0.44021297 0.44009757 0.44002485 0.43986723 0.4397759
 0.43975547 0.4396912  0.43967003 0.4396867  0.43970805 0.43963394
 0.43965057 0.43964353 0.43959045 0.43955007 0.43957075 0.4395553
 0.43952623 0.4394296  0.43935925 0.43933234 0.43947756 0.43939474
 0.43922058 0.4392143  0.43910053 0.43903267 0.43892664 0.43887576
 0.43890312 0.43888107 0.4388308  0.43880305 0.4388327  0.4388575
 0.43889073 0.43884802 0.43883044 0.43878752 0.43880352 0.43878925
 0.43875033 0.43869475 0.438659   0.43863532 0.43874872 0.4387518
 0.4385919  0.43853903 0.43844622 0.4384248  0.43837672 0.4383705
 0.43837595 0.43836036 0.43835223 0.43837902 0.43837717 0.43837878
 0.43843466 0.43835628 0.43837115 0.43831784 0.4382835  0.4382652
 0.43819827 0.43806887 0.437994   0.43798316 0.4380991  0.4381799
 0.43808752 0.43812957 0.43810344 0.43816176 0.438177   0.4382116
 0.43827233 0.43832412 0.43836322 0.43843994 0.43846384 0.4384706
 0.4386596  0.43866554 0.43862033 0.43854004 0.43845946 0.43848452
 0.43849003 0.43846896 0.4385191  0.43859848 0.43881765 0.43895438
 0.4388473  0.4389135  0.43895033 0.4390177  0.43907744 0.43914476
 0.43922237 0.43933684 0.43947452 0.43965438 0.4398105  0.4399507
 0.44021624 0.44033384 0.44019568 0.44013548 0.44010913 0.4401984
 0.4402804  0.44025084 0.44031653 0.4404401  0.44079635 0.44041976
 0.4396659  0.43932772 0.43905294 0.4387586  0.43853205 0.43833113
 0.43815115 0.43799734 0.4378176  0.43761858 0.4375056  0.4373556
 0.43735972 0.43723902 0.4371227  0.43709582 0.43708086 0.43709472
 0.4371555  0.43712533 0.43710893 0.4371859  0.43736163 0.4372505
 0.43682924 0.43666077 0.43655598 0.43638292 0.4362421  0.43619308
 0.43608508 0.43591663 0.43579394 0.4356815  0.4356281  0.43549815
 0.43553433 0.43547562 0.4354173  0.4354063  0.43540967 0.43539572
 0.435415   0.43536294 0.4353181  0.43541095 0.43550128 0.43548998
 0.43534    0.4352451  0.43517855 0.43515044 0.435075   0.43505657
 0.43501297 0.4349586  0.4349425  0.43486145 0.4348208  0.43478134
 0.43482947 0.43477035 0.4347295  0.43472943 0.43472984 0.43470562
 0.43470874 0.43467256 0.43459666 0.43461776 0.4347526  0.43475437
 0.43461245 0.43459082 0.43456113 0.43455473 0.43447417 0.43447262
 0.4344977  0.43443537 0.43434638 0.4343312  0.43436125 0.43433097
 0.43437374 0.43435115 0.43435305 0.43431422 0.43432337 0.43431917
 0.4343067  0.43427005 0.434233   0.4342352  0.43435454 0.43436903
 0.43424    0.43425936 0.43420777 0.4342289  0.43422747 0.4342532
 0.4342531  0.43420166 0.43417966 0.43416774 0.43411812 0.43408453
 0.4341268  0.43406615 0.43409714 0.43405268 0.43402785 0.434014
 0.43395212 0.43387413 0.4338291  0.4338005  0.43391585 0.43402147
 0.43393874 0.4340387  0.43408194 0.434181   0.4342179  0.43428135
 0.4343603  0.43438947 0.4344062  0.43445104 0.43445095 0.4344559
 0.434601   0.43456617 0.43457374 0.43452552 0.43444633 0.43444628
 0.43446097 0.43450806 0.43461272 0.43469796 0.43492472 0.43507358
 0.4349857  0.43510497 0.4351732  0.43527392 0.4353386  0.4354335
 0.43553153 0.43558288 0.4356327  0.4357524  0.4358617  0.43598667
 0.43626022 0.4363137  0.43619704 0.4361574  0.4361348  0.43622386
 0.43630615 0.43632925 0.4364124  0.43648905 0.4368088  0.4364138
 0.43557876 0.43518955 0.4348787  0.43456376 0.43433264 0.43415573
 0.4339894  0.43378252 0.43354434 0.43334365 0.4332239  0.43301448
 0.4330703  0.4329398  0.43284246 0.43286902 0.4328488  0.4328896
 0.43298584 0.43293163 0.43289077 0.4329293  0.43301958 0.43283767
 0.4323037  0.432077   0.43199116 0.43179566 0.4316517  0.43159252
 0.43143436 0.43126068 0.4310807  0.43086746 0.43086135 0.43073687
 0.43077898 0.43077645 0.43075335 0.4307723  0.430804   0.4308503
 0.43090856 0.43085802 0.43082535 0.4308931  0.43091175 0.43085566
 0.43062618 0.43047258 0.43041405 0.43029666 0.4301122  0.43005684
 0.42999956 0.42991477 0.42985705 0.42968076 0.42964697 0.4295685
 0.42955315 0.42950103 0.42943126 0.42938083 0.42936066 0.42926183
 0.42921802 0.42924282 0.42917857 0.42916372 0.4292397  0.42919478
 0.42905775 0.42897823 0.42892012 0.4289818  0.4288762  0.42881423
 0.4288401  0.42875066 0.42869112 0.42860174 0.42860702 0.42862892
 0.4286524  0.42861345 0.42863166 0.42861593 0.42862839 0.42858455
 0.42855617 0.4285826  0.4285151  0.4284829  0.42860663 0.4285533
 0.4284006  0.4284217  0.4283674  0.42837307 0.42836735 0.42838424
 0.42839617 0.4283231  0.4282516  0.42822334 0.4282257  0.4282483
 0.42829534 0.4282845  0.42831337 0.4282569  0.428278   0.42824626
 0.4281572  0.4281298  0.4280663  0.42796892 0.4280894  0.42817456
 0.42810947 0.4282272  0.4282577  0.42838675 0.42843786 0.42849308
 0.42854106 0.4285277  0.42854476 0.42858618 0.42859352 0.42864037
 0.42882055 0.42882583 0.4288719  0.42881313 0.42876714 0.42877445
 0.42874765 0.4288346  0.42887864 0.4288512  0.42913714 0.42930174
 0.42918482 0.42934787 0.42943668 0.42956957 0.42961454 0.42967945
 0.42979023 0.4298446  0.42988443 0.4300576  0.43020555 0.4303648
 0.43071654 0.43081874 0.4307657  0.4307678  0.4306949  0.43081385
 0.43089095 0.43084118 0.4309513  0.4309403  0.43125522 0.43088982
 0.43002608 0.42971972 0.4294063  0.42917243 0.42891547 0.42867297
 0.42848206 0.42820045 0.42795238 0.42779848 0.42764986 0.42754152
 0.4276832  0.4274993  0.42755228 0.42756933 0.4274749  0.42758656
 0.42547348 0.4274029  0.4274287  0.4272678  0.4273982  0.42316908
 0.422603   0.4225444  0.42233917 0.42220682 0.42206737 0.42181662
 0.42175722 0.42146423 0.42127222 0.42133674 0.4211696  0.42122695
 0.42150962 0.42136085 0.4215981  0.4214257  0.42122772 0.42136988
 0.42090663 0.4208287  0.42077625 0.42071363 0.42111546 0.42125338]
