Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_180_j720_H4', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_180_j720_H4_FITS_custom_ftM_sl180_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17513
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=42, out_features=210, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  362396160.0
params:  9030.0
Trainable parameters:  9030
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.9703825
	speed: 1.0979s/iter; left time: 14822.1453s
Epoch: 1 cost time: 149.46547603607178
Epoch: 1, Steps: 136 | Train Loss: 1.3046216 Vali Loss: 0.7584512 Test Loss: 0.8418390
Validation loss decreased (inf --> 0.758451).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5768036
	speed: 2.4468s/iter; left time: 32701.4943s
Epoch: 2 cost time: 146.6815903186798
Epoch: 2, Steps: 136 | Train Loss: 0.6484963 Vali Loss: 0.4967391 Test Loss: 0.5534124
Validation loss decreased (0.758451 --> 0.496739).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4321451
	speed: 2.7182s/iter; left time: 35959.6608s
Epoch: 3 cost time: 149.9183361530304
Epoch: 3, Steps: 136 | Train Loss: 0.4571361 Vali Loss: 0.3853991 Test Loss: 0.4303254
Validation loss decreased (0.496739 --> 0.385399).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3655898
	speed: 2.7497s/iter; left time: 36001.5328s
Epoch: 4 cost time: 155.1031937599182
Epoch: 4, Steps: 136 | Train Loss: 0.3713811 Vali Loss: 0.3297225 Test Loss: 0.3690953
Validation loss decreased (0.385399 --> 0.329722).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3210992
	speed: 2.7043s/iter; left time: 35040.0389s
Epoch: 5 cost time: 158.20203709602356
Epoch: 5, Steps: 136 | Train Loss: 0.3264382 Vali Loss: 0.2994918 Test Loss: 0.3352927
Validation loss decreased (0.329722 --> 0.299492).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3042083
	speed: 2.6510s/iter; left time: 33988.6681s
Epoch: 6 cost time: 157.26803016662598
Epoch: 6, Steps: 136 | Train Loss: 0.2999283 Vali Loss: 0.2793905 Test Loss: 0.3148684
Validation loss decreased (0.299492 --> 0.279391).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2812090
	speed: 2.6263s/iter; left time: 33314.8941s
Epoch: 7 cost time: 144.29996728897095
Epoch: 7, Steps: 136 | Train Loss: 0.2824564 Vali Loss: 0.2663386 Test Loss: 0.3011543
Validation loss decreased (0.279391 --> 0.266339).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2656946
	speed: 2.7499s/iter; left time: 34508.2805s
Epoch: 8 cost time: 151.88028860092163
Epoch: 8, Steps: 136 | Train Loss: 0.2698477 Vali Loss: 0.2558546 Test Loss: 0.2910625
Validation loss decreased (0.266339 --> 0.255855).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2594367
	speed: 2.6689s/iter; left time: 33128.9186s
Epoch: 9 cost time: 156.23827052116394
Epoch: 9, Steps: 136 | Train Loss: 0.2603827 Vali Loss: 0.2486580 Test Loss: 0.2834180
Validation loss decreased (0.255855 --> 0.248658).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2461397
	speed: 2.7303s/iter; left time: 33519.8369s
Epoch: 10 cost time: 159.42716336250305
Epoch: 10, Steps: 136 | Train Loss: 0.2526706 Vali Loss: 0.2420437 Test Loss: 0.2771608
Validation loss decreased (0.248658 --> 0.242044).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2481645
	speed: 2.7209s/iter; left time: 33034.7693s
Epoch: 11 cost time: 156.74504446983337
Epoch: 11, Steps: 136 | Train Loss: 0.2463742 Vali Loss: 0.2367460 Test Loss: 0.2721152
Validation loss decreased (0.242044 --> 0.236746).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2400753
	speed: 2.7745s/iter; left time: 33307.3412s
Epoch: 12 cost time: 159.5139057636261
Epoch: 12, Steps: 136 | Train Loss: 0.2410169 Vali Loss: 0.2321196 Test Loss: 0.2676800
Validation loss decreased (0.236746 --> 0.232120).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2225976
	speed: 2.6842s/iter; left time: 31858.2078s
Epoch: 13 cost time: 155.5338158607483
Epoch: 13, Steps: 136 | Train Loss: 0.2366951 Vali Loss: 0.2279552 Test Loss: 0.2640169
Validation loss decreased (0.232120 --> 0.227955).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2292681
	speed: 2.6567s/iter; left time: 31171.3185s
Epoch: 14 cost time: 145.4781014919281
Epoch: 14, Steps: 136 | Train Loss: 0.2329299 Vali Loss: 0.2248582 Test Loss: 0.2608989
Validation loss decreased (0.227955 --> 0.224858).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2325827
	speed: 2.3802s/iter; left time: 27602.9555s
Epoch: 15 cost time: 138.33946657180786
Epoch: 15, Steps: 136 | Train Loss: 0.2295740 Vali Loss: 0.2219377 Test Loss: 0.2581672
Validation loss decreased (0.224858 --> 0.221938).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2422089
	speed: 2.3704s/iter; left time: 27167.5694s
Epoch: 16 cost time: 125.5621657371521
Epoch: 16, Steps: 136 | Train Loss: 0.2267043 Vali Loss: 0.2196802 Test Loss: 0.2557933
Validation loss decreased (0.221938 --> 0.219680).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2338957
	speed: 1.7689s/iter; left time: 20033.2093s
Epoch: 17 cost time: 100.04485750198364
Epoch: 17, Steps: 136 | Train Loss: 0.2243046 Vali Loss: 0.2178263 Test Loss: 0.2537157
Validation loss decreased (0.219680 --> 0.217826).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2312095
	speed: 1.8369s/iter; left time: 20553.4597s
Epoch: 18 cost time: 111.51626372337341
Epoch: 18, Steps: 136 | Train Loss: 0.2221029 Vali Loss: 0.2156820 Test Loss: 0.2518689
Validation loss decreased (0.217826 --> 0.215682).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2201822
	speed: 2.0160s/iter; left time: 22283.2190s
Epoch: 19 cost time: 112.83285427093506
Epoch: 19, Steps: 136 | Train Loss: 0.2202142 Vali Loss: 0.2144347 Test Loss: 0.2502313
Validation loss decreased (0.215682 --> 0.214435).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2230576
	speed: 2.0511s/iter; left time: 22391.4293s
Epoch: 20 cost time: 116.0633773803711
Epoch: 20, Steps: 136 | Train Loss: 0.2184713 Vali Loss: 0.2125520 Test Loss: 0.2488146
Validation loss decreased (0.214435 --> 0.212552).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2165347
	speed: 2.0716s/iter; left time: 22333.7507s
Epoch: 21 cost time: 117.67748475074768
Epoch: 21, Steps: 136 | Train Loss: 0.2170320 Vali Loss: 0.2113973 Test Loss: 0.2475152
Validation loss decreased (0.212552 --> 0.211397).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2012134
	speed: 1.9751s/iter; left time: 21024.4671s
Epoch: 22 cost time: 117.71415710449219
Epoch: 22, Steps: 136 | Train Loss: 0.2156601 Vali Loss: 0.2104767 Test Loss: 0.2463932
Validation loss decreased (0.211397 --> 0.210477).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2114889
	speed: 1.8670s/iter; left time: 19620.7570s
Epoch: 23 cost time: 105.43283247947693
Epoch: 23, Steps: 136 | Train Loss: 0.2145536 Vali Loss: 0.2096723 Test Loss: 0.2453249
Validation loss decreased (0.210477 --> 0.209672).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2178385
	speed: 1.8248s/iter; left time: 18928.3662s
Epoch: 24 cost time: 101.42904949188232
Epoch: 24, Steps: 136 | Train Loss: 0.2134437 Vali Loss: 0.2082925 Test Loss: 0.2444132
Validation loss decreased (0.209672 --> 0.208292).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2139691
	speed: 1.8140s/iter; left time: 18569.5694s
Epoch: 25 cost time: 100.53624105453491
Epoch: 25, Steps: 136 | Train Loss: 0.2125774 Vali Loss: 0.2083368 Test Loss: 0.2435790
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2041017
	speed: 1.7672s/iter; left time: 17850.4860s
Epoch: 26 cost time: 106.12443828582764
Epoch: 26, Steps: 136 | Train Loss: 0.2117191 Vali Loss: 0.2073840 Test Loss: 0.2428147
Validation loss decreased (0.208292 --> 0.207384).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2047210
	speed: 1.7202s/iter; left time: 17141.7617s
Epoch: 27 cost time: 95.43646335601807
Epoch: 27, Steps: 136 | Train Loss: 0.2110028 Vali Loss: 0.2066956 Test Loss: 0.2421764
Validation loss decreased (0.207384 --> 0.206696).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2133127
	speed: 1.6241s/iter; left time: 15962.9217s
Epoch: 28 cost time: 94.27113461494446
Epoch: 28, Steps: 136 | Train Loss: 0.2103806 Vali Loss: 0.2062497 Test Loss: 0.2415658
Validation loss decreased (0.206696 --> 0.206250).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2081552
	speed: 1.6330s/iter; left time: 15828.5941s
Epoch: 29 cost time: 95.06204986572266
Epoch: 29, Steps: 136 | Train Loss: 0.2096566 Vali Loss: 0.2055956 Test Loss: 0.2410013
Validation loss decreased (0.206250 --> 0.205596).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2109091
	speed: 1.6367s/iter; left time: 15642.1166s
Epoch: 30 cost time: 94.22609519958496
Epoch: 30, Steps: 136 | Train Loss: 0.2091474 Vali Loss: 0.2051500 Test Loss: 0.2404940
Validation loss decreased (0.205596 --> 0.205150).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2077360
	speed: 1.5946s/iter; left time: 15022.5911s
Epoch: 31 cost time: 94.82020807266235
Epoch: 31, Steps: 136 | Train Loss: 0.2086597 Vali Loss: 0.2048705 Test Loss: 0.2400499
Validation loss decreased (0.205150 --> 0.204870).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2154348
	speed: 1.6464s/iter; left time: 15286.8992s
Epoch: 32 cost time: 92.65550994873047
Epoch: 32, Steps: 136 | Train Loss: 0.2082407 Vali Loss: 0.2042375 Test Loss: 0.2396172
Validation loss decreased (0.204870 --> 0.204237).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2257982
	speed: 1.6294s/iter; left time: 14906.9582s
Epoch: 33 cost time: 94.36141777038574
Epoch: 33, Steps: 136 | Train Loss: 0.2077801 Vali Loss: 0.2043107 Test Loss: 0.2392248
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.1913125
	speed: 1.6348s/iter; left time: 14734.5321s
Epoch: 34 cost time: 92.18593192100525
Epoch: 34, Steps: 136 | Train Loss: 0.2074834 Vali Loss: 0.2037346 Test Loss: 0.2389019
Validation loss decreased (0.204237 --> 0.203735).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2110939
	speed: 1.6015s/iter; left time: 14216.4853s
Epoch: 35 cost time: 91.9348509311676
Epoch: 35, Steps: 136 | Train Loss: 0.2071069 Vali Loss: 0.2034863 Test Loss: 0.2385680
Validation loss decreased (0.203735 --> 0.203486).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2038509
	speed: 1.5827s/iter; left time: 13833.9666s
Epoch: 36 cost time: 93.48151469230652
Epoch: 36, Steps: 136 | Train Loss: 0.2067904 Vali Loss: 0.2033079 Test Loss: 0.2382855
Validation loss decreased (0.203486 --> 0.203308).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2077999
	speed: 1.6531s/iter; left time: 14224.5053s
Epoch: 37 cost time: 93.30061459541321
Epoch: 37, Steps: 136 | Train Loss: 0.2066105 Vali Loss: 0.2031080 Test Loss: 0.2380317
Validation loss decreased (0.203308 --> 0.203108).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2057415
	speed: 1.6002s/iter; left time: 13552.3668s
Epoch: 38 cost time: 94.47449612617493
Epoch: 38, Steps: 136 | Train Loss: 0.2063482 Vali Loss: 0.2029261 Test Loss: 0.2377778
Validation loss decreased (0.203108 --> 0.202926).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.1964707
	speed: 1.5584s/iter; left time: 12986.3946s
Epoch: 39 cost time: 88.37318229675293
Epoch: 39, Steps: 136 | Train Loss: 0.2061636 Vali Loss: 0.2025788 Test Loss: 0.2375423
Validation loss decreased (0.202926 --> 0.202579).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2019218
	speed: 1.4895s/iter; left time: 12209.2970s
Epoch: 40 cost time: 88.24755501747131
Epoch: 40, Steps: 136 | Train Loss: 0.2058840 Vali Loss: 0.2028008 Test Loss: 0.2373310
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2004760
	speed: 1.5191s/iter; left time: 12245.3159s
Epoch: 41 cost time: 87.84703350067139
Epoch: 41, Steps: 136 | Train Loss: 0.2056286 Vali Loss: 0.2023610 Test Loss: 0.2371533
Validation loss decreased (0.202579 --> 0.202361).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2054020
	speed: 1.5160s/iter; left time: 12014.4769s
Epoch: 42 cost time: 85.95230507850647
Epoch: 42, Steps: 136 | Train Loss: 0.2055192 Vali Loss: 0.2026721 Test Loss: 0.2369646
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2129888
	speed: 1.5114s/iter; left time: 11772.0459s
Epoch: 43 cost time: 87.26308488845825
Epoch: 43, Steps: 136 | Train Loss: 0.2053390 Vali Loss: 0.2018068 Test Loss: 0.2368144
Validation loss decreased (0.202361 --> 0.201807).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2020289
	speed: 1.4513s/iter; left time: 11106.7319s
Epoch: 44 cost time: 84.9841775894165
Epoch: 44, Steps: 136 | Train Loss: 0.2052407 Vali Loss: 0.2019013 Test Loss: 0.2366475
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2070867
	speed: 1.4147s/iter; left time: 10634.4083s
Epoch: 45 cost time: 82.35512495040894
Epoch: 45, Steps: 136 | Train Loss: 0.2050439 Vali Loss: 0.2019875 Test Loss: 0.2365029
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2000141
	speed: 1.4387s/iter; left time: 10618.7152s
Epoch: 46 cost time: 82.84423565864563
Epoch: 46, Steps: 136 | Train Loss: 0.2049307 Vali Loss: 0.2019066 Test Loss: 0.2363776
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2061117
	speed: 1.4907s/iter; left time: 10799.8934s
Epoch: 47 cost time: 88.29440307617188
Epoch: 47, Steps: 136 | Train Loss: 0.2048307 Vali Loss: 0.2020171 Test Loss: 0.2362548
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2072257
	speed: 1.5105s/iter; left time: 10738.3251s
Epoch: 48 cost time: 87.86431288719177
Epoch: 48, Steps: 136 | Train Loss: 0.2047545 Vali Loss: 0.2018052 Test Loss: 0.2361312
Validation loss decreased (0.201807 --> 0.201805).  Saving model ...
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2015777
	speed: 1.4345s/iter; left time: 10002.9735s
Epoch: 49 cost time: 81.5901780128479
Epoch: 49, Steps: 136 | Train Loss: 0.2045799 Vali Loss: 0.2016022 Test Loss: 0.2360326
Validation loss decreased (0.201805 --> 0.201602).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2082078
	speed: 1.4095s/iter; left time: 9637.0446s
Epoch: 50 cost time: 80.48490238189697
Epoch: 50, Steps: 136 | Train Loss: 0.2045198 Vali Loss: 0.2014907 Test Loss: 0.2359554
Validation loss decreased (0.201602 --> 0.201491).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2028539
	speed: 1.3725s/iter; left time: 9197.3341s
Epoch: 51 cost time: 80.70932269096375
Epoch: 51, Steps: 136 | Train Loss: 0.2044326 Vali Loss: 0.2017717 Test Loss: 0.2358597
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2150487
	speed: 1.2789s/iter; left time: 8396.1773s
Epoch: 52 cost time: 75.370685338974
Epoch: 52, Steps: 136 | Train Loss: 0.2043094 Vali Loss: 0.2016336 Test Loss: 0.2357666
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2066948
	speed: 1.9248s/iter; left time: 12374.4634s
Epoch: 53 cost time: 115.40965032577515
Epoch: 53, Steps: 136 | Train Loss: 0.2043186 Vali Loss: 0.2015862 Test Loss: 0.2356798
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2010180
	speed: 1.9244s/iter; left time: 12110.4552s
Epoch: 54 cost time: 107.6636016368866
Epoch: 54, Steps: 136 | Train Loss: 0.2042306 Vali Loss: 0.2014671 Test Loss: 0.2356240
Validation loss decreased (0.201491 --> 0.201467).  Saving model ...
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2078951
	speed: 1.9296s/iter; left time: 11880.5924s
Epoch: 55 cost time: 112.53113150596619
Epoch: 55, Steps: 136 | Train Loss: 0.2041832 Vali Loss: 0.2013803 Test Loss: 0.2355577
Validation loss decreased (0.201467 --> 0.201380).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2095242
	speed: 1.8393s/iter; left time: 11074.6892s
Epoch: 56 cost time: 105.75455451011658
Epoch: 56, Steps: 136 | Train Loss: 0.2041172 Vali Loss: 0.2015027 Test Loss: 0.2354971
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.1990887
	speed: 1.8899s/iter; left time: 11121.8851s
Epoch: 57 cost time: 105.47261023521423
Epoch: 57, Steps: 136 | Train Loss: 0.2040072 Vali Loss: 0.2014907 Test Loss: 0.2354373
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.1977736
	speed: 1.9143s/iter; left time: 11005.3438s
Epoch: 58 cost time: 111.51032304763794
Epoch: 58, Steps: 136 | Train Loss: 0.2040821 Vali Loss: 0.2013604 Test Loss: 0.2353816
Validation loss decreased (0.201380 --> 0.201360).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2021280
	speed: 1.8848s/iter; left time: 10579.1579s
Epoch: 59 cost time: 112.86252450942993
Epoch: 59, Steps: 136 | Train Loss: 0.2039266 Vali Loss: 0.2013050 Test Loss: 0.2353357
Validation loss decreased (0.201360 --> 0.201305).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2082583
	speed: 1.9081s/iter; left time: 10450.9178s
Epoch: 60 cost time: 109.72682189941406
Epoch: 60, Steps: 136 | Train Loss: 0.2039515 Vali Loss: 0.2014611 Test Loss: 0.2352860
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.1963271
	speed: 1.9331s/iter; left time: 10324.8874s
Epoch: 61 cost time: 107.712082862854
Epoch: 61, Steps: 136 | Train Loss: 0.2039107 Vali Loss: 0.2013585 Test Loss: 0.2352454
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.1996659
	speed: 1.9197s/iter; left time: 9991.8368s
Epoch: 62 cost time: 110.21679520606995
Epoch: 62, Steps: 136 | Train Loss: 0.2038491 Vali Loss: 0.2014358 Test Loss: 0.2351997
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2023211
	speed: 1.7068s/iter; left time: 8651.6303s
Epoch: 63 cost time: 91.2838020324707
Epoch: 63, Steps: 136 | Train Loss: 0.2038293 Vali Loss: 0.2016434 Test Loss: 0.2351676
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.1945973
	speed: 1.5720s/iter; left time: 7754.4350s
Epoch: 64 cost time: 90.84997916221619
Epoch: 64, Steps: 136 | Train Loss: 0.2038508 Vali Loss: 0.2012658 Test Loss: 0.2351319
Validation loss decreased (0.201305 --> 0.201266).  Saving model ...
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2117185
	speed: 1.8239s/iter; left time: 8749.3285s
Epoch: 65 cost time: 132.14048600196838
Epoch: 65, Steps: 136 | Train Loss: 0.2037886 Vali Loss: 0.2011612 Test Loss: 0.2351016
Validation loss decreased (0.201266 --> 0.201161).  Saving model ...
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2042061
	speed: 2.5179s/iter; left time: 11736.0065s
Epoch: 66 cost time: 140.772625207901
Epoch: 66, Steps: 136 | Train Loss: 0.2038216 Vali Loss: 0.2013705 Test Loss: 0.2350791
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2101776
	speed: 2.4136s/iter; left time: 10921.4321s
Epoch: 67 cost time: 140.5152657032013
Epoch: 67, Steps: 136 | Train Loss: 0.2037999 Vali Loss: 0.2014808 Test Loss: 0.2350476
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2082610
	speed: 2.3115s/iter; left time: 10145.2000s
Epoch: 68 cost time: 129.49260306358337
Epoch: 68, Steps: 136 | Train Loss: 0.2037434 Vali Loss: 0.2007848 Test Loss: 0.2350160
Validation loss decreased (0.201161 --> 0.200785).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.1969607
	speed: 2.3305s/iter; left time: 9911.6512s
Epoch: 69 cost time: 134.1963291168213
Epoch: 69, Steps: 136 | Train Loss: 0.2036859 Vali Loss: 0.2012828 Test Loss: 0.2350005
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2045957
	speed: 2.2648s/iter; left time: 9324.0777s
Epoch: 70 cost time: 129.94220232963562
Epoch: 70, Steps: 136 | Train Loss: 0.2037123 Vali Loss: 0.2012766 Test Loss: 0.2349721
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2193554
	speed: 2.2802s/iter; left time: 9077.4940s
Epoch: 71 cost time: 129.25424718856812
Epoch: 71, Steps: 136 | Train Loss: 0.2036499 Vali Loss: 0.2009068 Test Loss: 0.2349527
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2032026
	speed: 2.0903s/iter; left time: 8037.0508s
Epoch: 72 cost time: 121.72074127197266
Epoch: 72, Steps: 136 | Train Loss: 0.2036117 Vali Loss: 0.2012115 Test Loss: 0.2349334
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2093698
	speed: 2.1290s/iter; left time: 7896.4402s
Epoch: 73 cost time: 119.20651340484619
Epoch: 73, Steps: 136 | Train Loss: 0.2037012 Vali Loss: 0.2010638 Test Loss: 0.2349216
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.1970369
	speed: 2.1149s/iter; left time: 7556.5964s
Epoch: 74 cost time: 130.2209210395813
Epoch: 74, Steps: 136 | Train Loss: 0.2036151 Vali Loss: 0.2014608 Test Loss: 0.2349028
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2044169
	speed: 2.1942s/iter; left time: 7541.5696s
Epoch: 75 cost time: 123.84248113632202
Epoch: 75, Steps: 136 | Train Loss: 0.2036024 Vali Loss: 0.2012837 Test Loss: 0.2348891
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.1992964
	speed: 2.2327s/iter; left time: 7370.0377s
Epoch: 76 cost time: 128.37626767158508
Epoch: 76, Steps: 136 | Train Loss: 0.2036363 Vali Loss: 0.2010859 Test Loss: 0.2348737
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2041460
	speed: 2.1789s/iter; left time: 6896.3718s
Epoch: 77 cost time: 128.42270708084106
Epoch: 77, Steps: 136 | Train Loss: 0.2035942 Vali Loss: 0.2012672 Test Loss: 0.2348609
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.2012368
	speed: 2.3471s/iter; left time: 7109.2445s
Epoch: 78 cost time: 139.78812718391418
Epoch: 78, Steps: 136 | Train Loss: 0.2036043 Vali Loss: 0.2010937 Test Loss: 0.2348493
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.2098402
	speed: 2.3114s/iter; left time: 6686.7651s
Epoch: 79 cost time: 130.62333369255066
Epoch: 79, Steps: 136 | Train Loss: 0.2036241 Vali Loss: 0.2014843 Test Loss: 0.2348356
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.2108742
	speed: 2.3055s/iter; left time: 6356.3998s
Epoch: 80 cost time: 137.89015293121338
Epoch: 80, Steps: 136 | Train Loss: 0.2035263 Vali Loss: 0.2010394 Test Loss: 0.2348283
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.2091267
	speed: 2.3160s/iter; left time: 6070.2947s
Epoch: 81 cost time: 133.46359372138977
Epoch: 81, Steps: 136 | Train Loss: 0.2035350 Vali Loss: 0.2011994 Test Loss: 0.2348161
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.2035005
	speed: 2.2165s/iter; left time: 5508.0881s
Epoch: 82 cost time: 127.63766026496887
Epoch: 82, Steps: 136 | Train Loss: 0.2035735 Vali Loss: 0.2013463 Test Loss: 0.2348080
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.2031998
	speed: 2.2751s/iter; left time: 5344.2143s
Epoch: 83 cost time: 127.27642583847046
Epoch: 83, Steps: 136 | Train Loss: 0.2034489 Vali Loss: 0.2010053 Test Loss: 0.2348015
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.2096848
	speed: 2.2546s/iter; left time: 4989.3810s
Epoch: 84 cost time: 131.9394974708557
Epoch: 84, Steps: 136 | Train Loss: 0.2035755 Vali Loss: 0.2011825 Test Loss: 0.2347911
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.2032648
	speed: 2.2806s/iter; left time: 4736.8579s
Epoch: 85 cost time: 132.16925621032715
Epoch: 85, Steps: 136 | Train Loss: 0.2036478 Vali Loss: 0.2012099 Test Loss: 0.2347809
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.1973361
	speed: 2.2185s/iter; left time: 4306.0976s
Epoch: 86 cost time: 125.2217800617218
Epoch: 86, Steps: 136 | Train Loss: 0.2035108 Vali Loss: 0.2009390 Test Loss: 0.2347763
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.1999854
	speed: 2.2158s/iter; left time: 3999.5827s
Epoch: 87 cost time: 125.70273637771606
Epoch: 87, Steps: 136 | Train Loss: 0.2035389 Vali Loss: 0.2012464 Test Loss: 0.2347713
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.1941007
	speed: 2.2049s/iter; left time: 3679.9187s
Epoch: 88 cost time: 128.33911085128784
Epoch: 88, Steps: 136 | Train Loss: 0.2034598 Vali Loss: 0.2012845 Test Loss: 0.2347632
EarlyStopping counter: 20 out of 20
Early stopping
train 17513
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=42, out_features=210, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  362396160.0
params:  9030.0
Trainable parameters:  9030
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.2361591
	speed: 0.9118s/iter; left time: 12309.9534s
Epoch: 1 cost time: 124.17784667015076
Epoch: 1, Steps: 136 | Train Loss: 0.2466093 Vali Loss: 0.2005750 Test Loss: 0.2337988
Validation loss decreased (inf --> 0.200575).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.2512985
	speed: 2.1654s/iter; left time: 28940.9244s
Epoch: 2 cost time: 122.8208315372467
Epoch: 2, Steps: 136 | Train Loss: 0.2462876 Vali Loss: 0.2002267 Test Loss: 0.2336577
Validation loss decreased (0.200575 --> 0.200227).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.2489389
	speed: 2.0184s/iter; left time: 26700.9627s
Epoch: 3 cost time: 116.79330515861511
Epoch: 3, Steps: 136 | Train Loss: 0.2461357 Vali Loss: 0.2000688 Test Loss: 0.2337151
Validation loss decreased (0.200227 --> 0.200069).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2390587
	speed: 2.0877s/iter; left time: 27334.0451s
Epoch: 4 cost time: 124.86222887039185
Epoch: 4, Steps: 136 | Train Loss: 0.2461215 Vali Loss: 0.1997445 Test Loss: 0.2336005
Validation loss decreased (0.200069 --> 0.199744).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2666601
	speed: 2.1766s/iter; left time: 28202.5799s
Epoch: 5 cost time: 132.15872597694397
Epoch: 5, Steps: 136 | Train Loss: 0.2460359 Vali Loss: 0.1996701 Test Loss: 0.2336018
Validation loss decreased (0.199744 --> 0.199670).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2428497
	speed: 2.2373s/iter; left time: 28684.3657s
Epoch: 6 cost time: 124.1625623703003
Epoch: 6, Steps: 136 | Train Loss: 0.2460848 Vali Loss: 0.1999471 Test Loss: 0.2336663
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2461022
	speed: 2.2638s/iter; left time: 28716.1644s
Epoch: 7 cost time: 134.56426167488098
Epoch: 7, Steps: 136 | Train Loss: 0.2460487 Vali Loss: 0.1994580 Test Loss: 0.2335822
Validation loss decreased (0.199670 --> 0.199458).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2520467
	speed: 2.3615s/iter; left time: 29634.2064s
Epoch: 8 cost time: 134.6398012638092
Epoch: 8, Steps: 136 | Train Loss: 0.2460043 Vali Loss: 0.2000785 Test Loss: 0.2336037
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2331707
	speed: 2.3393s/iter; left time: 29037.2893s
Epoch: 9 cost time: 135.00960230827332
Epoch: 9, Steps: 136 | Train Loss: 0.2459084 Vali Loss: 0.1994389 Test Loss: 0.2335699
Validation loss decreased (0.199458 --> 0.199439).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2535226
	speed: 2.4005s/iter; left time: 29471.4287s
Epoch: 10 cost time: 132.84515047073364
Epoch: 10, Steps: 136 | Train Loss: 0.2459381 Vali Loss: 0.2001543 Test Loss: 0.2335418
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2708798
	speed: 2.3143s/iter; left time: 28098.2170s
Epoch: 11 cost time: 132.49499821662903
Epoch: 11, Steps: 136 | Train Loss: 0.2460862 Vali Loss: 0.1998438 Test Loss: 0.2335774
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2399321
	speed: 2.3327s/iter; left time: 28004.6213s
Epoch: 12 cost time: 133.04352641105652
Epoch: 12, Steps: 136 | Train Loss: 0.2459417 Vali Loss: 0.1998625 Test Loss: 0.2335692
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2610543
	speed: 2.3328s/iter; left time: 27687.5503s
Epoch: 13 cost time: 134.24601864814758
Epoch: 13, Steps: 136 | Train Loss: 0.2459769 Vali Loss: 0.2001174 Test Loss: 0.2335275
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2529546
	speed: 2.2713s/iter; left time: 26648.9614s
Epoch: 14 cost time: 130.33092617988586
Epoch: 14, Steps: 136 | Train Loss: 0.2459811 Vali Loss: 0.1999840 Test Loss: 0.2335877
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2466665
	speed: 2.2396s/iter; left time: 25972.1221s
Epoch: 15 cost time: 131.1339452266693
Epoch: 15, Steps: 136 | Train Loss: 0.2459378 Vali Loss: 0.1998416 Test Loss: 0.2335792
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2451139
	speed: 2.1590s/iter; left time: 24744.5048s
Epoch: 16 cost time: 123.54197359085083
Epoch: 16, Steps: 136 | Train Loss: 0.2459424 Vali Loss: 0.1996861 Test Loss: 0.2336033
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2493873
	speed: 2.1597s/iter; left time: 24458.9133s
Epoch: 17 cost time: 127.66088533401489
Epoch: 17, Steps: 136 | Train Loss: 0.2458777 Vali Loss: 0.2000932 Test Loss: 0.2335792
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2454783
	speed: 2.1168s/iter; left time: 23684.4977s
Epoch: 18 cost time: 122.40069007873535
Epoch: 18, Steps: 136 | Train Loss: 0.2458885 Vali Loss: 0.1996467 Test Loss: 0.2335713
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2480994
	speed: 2.1132s/iter; left time: 23357.0796s
Epoch: 19 cost time: 118.76032161712646
Epoch: 19, Steps: 136 | Train Loss: 0.2458339 Vali Loss: 0.1997868 Test Loss: 0.2335570
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2467401
	speed: 1.8850s/iter; left time: 20578.7432s
Epoch: 20 cost time: 108.17299222946167
Epoch: 20, Steps: 136 | Train Loss: 0.2458606 Vali Loss: 0.1994403 Test Loss: 0.2335136
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2472875
	speed: 1.8938s/iter; left time: 20417.0100s
Epoch: 21 cost time: 107.43524384498596
Epoch: 21, Steps: 136 | Train Loss: 0.2458801 Vali Loss: 0.1997114 Test Loss: 0.2335550
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2421984
	speed: 1.8947s/iter; left time: 20169.0335s
Epoch: 22 cost time: 111.05919671058655
Epoch: 22, Steps: 136 | Train Loss: 0.2459083 Vali Loss: 0.1995947 Test Loss: 0.2335448
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2593032
	speed: 1.8732s/iter; left time: 19685.6511s
Epoch: 23 cost time: 110.52801656723022
Epoch: 23, Steps: 136 | Train Loss: 0.2459023 Vali Loss: 0.1998056 Test Loss: 0.2335590
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2493681
	speed: 1.8696s/iter; left time: 19393.6410s
Epoch: 24 cost time: 108.55530309677124
Epoch: 24, Steps: 136 | Train Loss: 0.2459013 Vali Loss: 0.1999234 Test Loss: 0.2335198
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2473473
	speed: 1.6853s/iter; left time: 17252.6717s
Epoch: 25 cost time: 94.35891771316528
Epoch: 25, Steps: 136 | Train Loss: 0.2458663 Vali Loss: 0.1995676 Test Loss: 0.2335801
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2559048
	speed: 1.6996s/iter; left time: 17167.4869s
Epoch: 26 cost time: 96.61279964447021
Epoch: 26, Steps: 136 | Train Loss: 0.2458728 Vali Loss: 0.1996152 Test Loss: 0.2335309
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2278251
	speed: 1.7051s/iter; left time: 16991.6382s
Epoch: 27 cost time: 98.80917692184448
Epoch: 27, Steps: 136 | Train Loss: 0.2458312 Vali Loss: 0.1997284 Test Loss: 0.2335474
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2511180
	speed: 1.7273s/iter; left time: 16977.4262s
Epoch: 28 cost time: 98.08970284461975
Epoch: 28, Steps: 136 | Train Loss: 0.2458524 Vali Loss: 0.1998930 Test Loss: 0.2335132
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2434989
	speed: 1.7720s/iter; left time: 17175.6214s
Epoch: 29 cost time: 106.0488612651825
Epoch: 29, Steps: 136 | Train Loss: 0.2459330 Vali Loss: 0.1994400 Test Loss: 0.2335837
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_180_j720_H4_FITS_custom_ftM_sl180_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.23198510706424713, mae:0.3194311857223511, rse:0.4804602265357971, corr:[0.44651243 0.44845164 0.44881618 0.44876185 0.44896227 0.44899738
 0.44862628 0.44828326 0.44824758 0.44801635 0.44748348 0.44723457
 0.44730917 0.4469772  0.44674614 0.44678432 0.44680914 0.4468023
 0.44688565 0.44696513 0.44704157 0.44730353 0.44752038 0.4473228
 0.4468835  0.4468413  0.4467406  0.44648442 0.44632912 0.44632792
 0.44623417 0.4460098  0.44582286 0.44563326 0.44532102 0.445123
 0.44523174 0.44510856 0.4449865  0.4449853  0.44503868 0.44509944
 0.4451302  0.44506413 0.44501373 0.4451715  0.44537884 0.44539908
 0.4452049  0.44520545 0.44512564 0.44496852 0.44487956 0.44486412
 0.44480968 0.44470876 0.44464862 0.44453543 0.4443315  0.44418773
 0.44429213 0.44424725 0.4442316  0.44422013 0.44414866 0.44411403
 0.4441365  0.44407564 0.4439851  0.4440582  0.44420168 0.44420412
 0.44404915 0.44410023 0.44405377 0.44393143 0.4439108  0.44390887
 0.4437759  0.44359744 0.4435706  0.44352493 0.4433229  0.44315016
 0.44320998 0.44311735 0.44308636 0.4431388  0.4431338  0.4430661
 0.44305098 0.4430405  0.44299588 0.44306505 0.44316974 0.44315273
 0.44298476 0.44308612 0.44311988 0.44301277 0.4429845  0.44309443
 0.44317237 0.4431397  0.44311458 0.44309732 0.44300187 0.4429215
 0.44302934 0.442968   0.44289866 0.44287094 0.44285327 0.44282618
 0.4427202  0.44248188 0.4422601  0.44226727 0.44243246 0.4425418
 0.44245246 0.44256225 0.44265094 0.44270033 0.44282117 0.4429798
 0.44307098 0.44308805 0.44315204 0.44317737 0.44307777 0.4429792
 0.44304183 0.44298458 0.44295081 0.44294456 0.44289494 0.44281378
 0.4427891  0.44280857 0.4428636  0.44305354 0.44327286 0.44332224
 0.4431801  0.4432681  0.44336998 0.44339788 0.44345003 0.4435713
 0.4436654  0.4437185  0.44382635 0.44392395 0.44396135 0.4440457
 0.44426772 0.4443931  0.44435412 0.44422725 0.44427046 0.44441295
 0.44452947 0.44457763 0.44476867 0.44517463 0.44524753 0.44471237
 0.4440279  0.44361088 0.4433046  0.44305617 0.4428933  0.44275668
 0.4425837  0.44242823 0.44237477 0.44229132 0.4421109  0.4419876
 0.44204116 0.44193187 0.44185328 0.44181964 0.4417816  0.44181722
 0.44189942 0.44184262 0.44174194 0.44180208 0.44182193 0.4415632
 0.44116572 0.44104883 0.44089952 0.44067344 0.44054332 0.44047526
 0.44033706 0.44015765 0.44007927 0.4400021  0.4398595  0.43981197
 0.43996325 0.4398818  0.43983915 0.43991175 0.43995562 0.4399523
 0.4399604  0.43987495 0.43977562 0.4398542  0.44001526 0.44003248
 0.43983793 0.43981567 0.43971506 0.43949813 0.4393805  0.43942496
 0.43943337 0.439329   0.43926147 0.43920353 0.4390959  0.43900603
 0.43908435 0.43900177 0.43895817 0.43897596 0.4389822  0.4389525
 0.4389098  0.43875778 0.4386413  0.43872425 0.43887722 0.43885314
 0.43860802 0.43857756 0.43856123 0.43848914 0.43846565 0.438477
 0.43841016 0.4382927  0.43827766 0.43826675 0.4381566  0.4380669
 0.43818507 0.43817016 0.43819156 0.43824622 0.43821102 0.43810886
 0.43803695 0.43796295 0.43786097 0.43783605 0.4378633  0.4378372
 0.43773967 0.43789488 0.43793887 0.43780708 0.43776962 0.43791723
 0.438025   0.4379718  0.4378854  0.4378261  0.4377646  0.4377607
 0.43792805 0.43788826 0.4378028  0.4377338  0.43767658 0.43764523
 0.43758956 0.4374435  0.43731648 0.43735763 0.4374804  0.43749171
 0.4373145  0.4374134  0.4375502  0.43762717 0.43772414 0.43784708
 0.43790656 0.4379087  0.43797034 0.4380232  0.43795878 0.43790594
 0.43803492 0.43803447 0.43799266 0.43794245 0.43786833 0.4377648
 0.43770903 0.43771258 0.43778586 0.43797603 0.43813965 0.43812266
 0.43792644 0.43797213 0.43807968 0.4381646  0.43829408 0.43846786
 0.43855995 0.4385803  0.43868384 0.43885225 0.43901816 0.4392072
 0.4394784  0.4396522  0.43963042 0.43948105 0.43950436 0.43963385
 0.43966624 0.43954033 0.43952522 0.43981135 0.43984595 0.43934196
 0.4387873  0.4385453  0.43835822 0.43813953 0.4379617  0.4378037
 0.43759742 0.43738094 0.43721998 0.43702468 0.436771   0.4366543
 0.4367628  0.43668455 0.436599   0.43656886 0.43656883 0.43660632
 0.43665558 0.43659368 0.4365017  0.43657827 0.4366206  0.43642333
 0.43610877 0.43607098 0.4359757  0.43576953 0.43565014 0.43566212
 0.43561262 0.4354432  0.43528742 0.4351464  0.43496677 0.43483981
 0.4349094  0.4348046  0.43473879 0.4347562  0.43475083 0.4347402
 0.43475577 0.43468606 0.43458143 0.43461138 0.4346926  0.43467796
 0.43456    0.43466532 0.4346735  0.43451297 0.43441412 0.43446308
 0.43450478 0.43448713 0.43449518 0.43448004 0.43440267 0.43436036
 0.43446016 0.43432787 0.4341746  0.4341308  0.4341405  0.43415287
 0.43414825 0.43404868 0.43393275 0.43398383 0.434113   0.43410513
 0.43389174 0.433875   0.4338956  0.433894   0.43396023 0.43406314
 0.43406916 0.43399528 0.4339939  0.4340544  0.43403614 0.43396384
 0.4340314  0.4339675  0.43392345 0.4339167  0.43386748 0.4338123
 0.43380973 0.43375656 0.43364546 0.43358386 0.4335464  0.43344423
 0.43328193 0.43340287 0.433496   0.4334597  0.4334719  0.43359578
 0.43367708 0.43364346 0.43358395 0.4335116  0.433401   0.43334982
 0.4334788  0.43346018 0.43343368 0.43344158 0.43342584 0.43341583
 0.43339947 0.43329608 0.43317705 0.43318653 0.43331206 0.43341452
 0.4334     0.433589   0.43370533 0.43367797 0.43365878 0.43373114
 0.433796   0.43380532 0.4338457  0.43384776 0.4337579  0.43373203
 0.43392918 0.43396935 0.43391773 0.43385565 0.43382347 0.43378863
 0.43377066 0.43375835 0.43380022 0.43401778 0.43429843 0.4344629
 0.43443927 0.43453956 0.43461508 0.43462658 0.4346704  0.43478101
 0.43486705 0.4349049  0.4349667  0.43503436 0.43513525 0.43534696
 0.43566635 0.43585342 0.43581498 0.4356302  0.43560377 0.43572107
 0.43578944 0.43571553 0.4357664  0.43606716 0.43606    0.43544954
 0.43473738 0.4343291  0.4340574  0.43383753 0.43370846 0.4336351
 0.43349254 0.43327442 0.43306723 0.43283057 0.43256605 0.43244785
 0.43256503 0.43249533 0.4323682  0.43227622 0.43226305 0.43237245
 0.43250433 0.4324694  0.43237284 0.43242994 0.43245605 0.43218485
 0.43172473 0.43155324 0.43145165 0.43129376 0.43117854 0.43113303
 0.43101555 0.43080705 0.4306352  0.43050855 0.4303389  0.43020365
 0.43027672 0.430236   0.43024528 0.43030986 0.430335   0.43035358
 0.43035778 0.4302573  0.4301787  0.4302505  0.4303453  0.43025726
 0.4299335  0.42978987 0.42967573 0.42952403 0.42944923 0.4294216
 0.4293362  0.42920431 0.42913496 0.42910084 0.42902678 0.42895538
 0.42906225 0.4290397  0.42901778 0.4289897  0.4288874  0.4287589
 0.428692   0.42864037 0.42859015 0.42865556 0.4287174  0.4285797
 0.42830133 0.42831355 0.42836878 0.42829913 0.42823538 0.42823783
 0.42819175 0.42805824 0.42799363 0.4280185  0.42803976 0.4280661
 0.42821673 0.42816654 0.42810822 0.4281078  0.42809504 0.42803633
 0.4279607  0.42783308 0.42771503 0.42773893 0.4278366  0.42785847
 0.4277083  0.4277038  0.42769182 0.427646   0.42768878 0.42780203
 0.42782557 0.42776057 0.4277411  0.42776832 0.42774126 0.4276784
 0.42775747 0.42776266 0.42781097 0.42785564 0.4278093  0.42771864
 0.42762145 0.42747572 0.42736682 0.42742747 0.42756155 0.42764363
 0.42760658 0.42775226 0.42784953 0.42786586 0.42797843 0.42816338
 0.42823562 0.42817378 0.42815542 0.42817867 0.4282058  0.42830402
 0.42852128 0.4285005  0.42839488 0.428338   0.4283385  0.42828423
 0.4281569  0.42799267 0.42791697 0.42808157 0.42833573 0.42844573
 0.42834258 0.42842728 0.42861012 0.42876706 0.4288809  0.42899105
 0.42909068 0.4291945  0.42937446 0.42957082 0.42973015 0.42990467
 0.43018317 0.43039805 0.43040633 0.4302774  0.4303001  0.43041083
 0.43040705 0.43024778 0.43029934 0.43065092 0.4306685  0.43006665
 0.4293584  0.42894152 0.42864    0.42840636 0.42830005 0.42822182
 0.42803305 0.42777172 0.42338002 0.42321822 0.42724556 0.4271388
 0.42298552 0.42289114 0.42287025 0.42707074 0.42282626 0.4227741
 0.4227399  0.42259017 0.42244083 0.42252576 0.42258298 0.4223204
 0.42188713 0.4217481  0.42161566 0.4213783  0.42122096 0.42118457
 0.42110884 0.4209839  0.42092848 0.42086813 0.42074737 0.42072332
 0.42088425 0.42079327 0.42065188 0.42063105 0.4206139  0.42048264
 0.4202167  0.41987932 0.41966042 0.41962427 0.41977194 0.42022023]
