Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j720_H10_FITS_custom_ftM_sl180_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11381
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=90, out_features=450, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4468608000.0
params:  40950.0
Trainable parameters:  40950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 132.99020409584045
Epoch: 1, Steps: 88 | Train Loss: 1.5942760 Vali Loss: 1.3614875 Test Loss: 1.6936702
Validation loss decreased (inf --> 1.361488).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 136.59050941467285
Epoch: 2, Steps: 88 | Train Loss: 0.8484972 Vali Loss: 0.9338099 Test Loss: 1.1557869
Validation loss decreased (1.361488 --> 0.933810).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 137.5009150505066
Epoch: 3, Steps: 88 | Train Loss: 0.6019582 Vali Loss: 0.7553861 Test Loss: 0.9335440
Validation loss decreased (0.933810 --> 0.755386).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 136.01975774765015
Epoch: 4, Steps: 88 | Train Loss: 0.4917229 Vali Loss: 0.6659014 Test Loss: 0.8236645
Validation loss decreased (0.755386 --> 0.665901).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 134.53169775009155
Epoch: 5, Steps: 88 | Train Loss: 0.4331915 Vali Loss: 0.6150912 Test Loss: 0.7596309
Validation loss decreased (0.665901 --> 0.615091).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 137.27030205726624
Epoch: 6, Steps: 88 | Train Loss: 0.3972957 Vali Loss: 0.5818491 Test Loss: 0.7178063
Validation loss decreased (0.615091 --> 0.581849).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 136.03343844413757
Epoch: 7, Steps: 88 | Train Loss: 0.3729398 Vali Loss: 0.5584553 Test Loss: 0.6867424
Validation loss decreased (0.581849 --> 0.558455).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 139.93995022773743
Epoch: 8, Steps: 88 | Train Loss: 0.3551912 Vali Loss: 0.5403420 Test Loss: 0.6629301
Validation loss decreased (0.558455 --> 0.540342).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 137.12208652496338
Epoch: 9, Steps: 88 | Train Loss: 0.3413927 Vali Loss: 0.5260357 Test Loss: 0.6438029
Validation loss decreased (0.540342 --> 0.526036).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 133.0931007862091
Epoch: 10, Steps: 88 | Train Loss: 0.3303622 Vali Loss: 0.5144328 Test Loss: 0.6283115
Validation loss decreased (0.526036 --> 0.514433).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 135.59221529960632
Epoch: 11, Steps: 88 | Train Loss: 0.3212404 Vali Loss: 0.5045932 Test Loss: 0.6155810
Validation loss decreased (0.514433 --> 0.504593).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 155.57992935180664
Epoch: 12, Steps: 88 | Train Loss: 0.3137617 Vali Loss: 0.4970620 Test Loss: 0.6045199
Validation loss decreased (0.504593 --> 0.497062).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 154.98657751083374
Epoch: 13, Steps: 88 | Train Loss: 0.3074534 Vali Loss: 0.4902932 Test Loss: 0.5952798
Validation loss decreased (0.497062 --> 0.490293).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 155.9813792705536
Epoch: 14, Steps: 88 | Train Loss: 0.3020202 Vali Loss: 0.4851473 Test Loss: 0.5875000
Validation loss decreased (0.490293 --> 0.485147).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 160.2054078578949
Epoch: 15, Steps: 88 | Train Loss: 0.2973467 Vali Loss: 0.4802381 Test Loss: 0.5806522
Validation loss decreased (0.485147 --> 0.480238).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 160.12070035934448
Epoch: 16, Steps: 88 | Train Loss: 0.2933432 Vali Loss: 0.4758222 Test Loss: 0.5746216
Validation loss decreased (0.480238 --> 0.475822).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 143.49783754348755
Epoch: 17, Steps: 88 | Train Loss: 0.2898258 Vali Loss: 0.4722795 Test Loss: 0.5693449
Validation loss decreased (0.475822 --> 0.472280).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 145.61282539367676
Epoch: 18, Steps: 88 | Train Loss: 0.2866848 Vali Loss: 0.4688841 Test Loss: 0.5645311
Validation loss decreased (0.472280 --> 0.468884).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 142.16761827468872
Epoch: 19, Steps: 88 | Train Loss: 0.2839062 Vali Loss: 0.4657789 Test Loss: 0.5606565
Validation loss decreased (0.468884 --> 0.465779).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 126.78501534461975
Epoch: 20, Steps: 88 | Train Loss: 0.2815076 Vali Loss: 0.4635700 Test Loss: 0.5568172
Validation loss decreased (0.465779 --> 0.463570).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 127.90918731689453
Epoch: 21, Steps: 88 | Train Loss: 0.2793776 Vali Loss: 0.4616069 Test Loss: 0.5535717
Validation loss decreased (0.463570 --> 0.461607).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 125.13066244125366
Epoch: 22, Steps: 88 | Train Loss: 0.2774852 Vali Loss: 0.4593155 Test Loss: 0.5505890
Validation loss decreased (0.461607 --> 0.459316).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 129.43651509284973
Epoch: 23, Steps: 88 | Train Loss: 0.2757357 Vali Loss: 0.4573909 Test Loss: 0.5478078
Validation loss decreased (0.459316 --> 0.457391).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 126.06423449516296
Epoch: 24, Steps: 88 | Train Loss: 0.2740771 Vali Loss: 0.4552343 Test Loss: 0.5454018
Validation loss decreased (0.457391 --> 0.455234).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 130.45691680908203
Epoch: 25, Steps: 88 | Train Loss: 0.2726265 Vali Loss: 0.4543244 Test Loss: 0.5431847
Validation loss decreased (0.455234 --> 0.454324).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 136.51443195343018
Epoch: 26, Steps: 88 | Train Loss: 0.2713452 Vali Loss: 0.4527336 Test Loss: 0.5410818
Validation loss decreased (0.454324 --> 0.452734).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 131.96826815605164
Epoch: 27, Steps: 88 | Train Loss: 0.2701019 Vali Loss: 0.4514934 Test Loss: 0.5391967
Validation loss decreased (0.452734 --> 0.451493).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 123.59950923919678
Epoch: 28, Steps: 88 | Train Loss: 0.2689565 Vali Loss: 0.4504999 Test Loss: 0.5375414
Validation loss decreased (0.451493 --> 0.450500).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 107.01031613349915
Epoch: 29, Steps: 88 | Train Loss: 0.2679466 Vali Loss: 0.4497214 Test Loss: 0.5359651
Validation loss decreased (0.450500 --> 0.449721).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 107.03420424461365
Epoch: 30, Steps: 88 | Train Loss: 0.2670316 Vali Loss: 0.4480810 Test Loss: 0.5345654
Validation loss decreased (0.449721 --> 0.448081).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 103.44081020355225
Epoch: 31, Steps: 88 | Train Loss: 0.2661761 Vali Loss: 0.4475541 Test Loss: 0.5332137
Validation loss decreased (0.448081 --> 0.447554).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 109.21392631530762
Epoch: 32, Steps: 88 | Train Loss: 0.2653443 Vali Loss: 0.4465751 Test Loss: 0.5319471
Validation loss decreased (0.447554 --> 0.446575).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 112.479323387146
Epoch: 33, Steps: 88 | Train Loss: 0.2646461 Vali Loss: 0.4457694 Test Loss: 0.5308824
Validation loss decreased (0.446575 --> 0.445769).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 105.24569392204285
Epoch: 34, Steps: 88 | Train Loss: 0.2639745 Vali Loss: 0.4454205 Test Loss: 0.5296522
Validation loss decreased (0.445769 --> 0.445421).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 103.04248929023743
Epoch: 35, Steps: 88 | Train Loss: 0.2633368 Vali Loss: 0.4444633 Test Loss: 0.5286648
Validation loss decreased (0.445421 --> 0.444463).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 114.51106667518616
Epoch: 36, Steps: 88 | Train Loss: 0.2627464 Vali Loss: 0.4438597 Test Loss: 0.5277829
Validation loss decreased (0.444463 --> 0.443860).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 172.44038248062134
Epoch: 37, Steps: 88 | Train Loss: 0.2621989 Vali Loss: 0.4436582 Test Loss: 0.5269300
Validation loss decreased (0.443860 --> 0.443658).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 166.3152105808258
Epoch: 38, Steps: 88 | Train Loss: 0.2616476 Vali Loss: 0.4428800 Test Loss: 0.5260766
Validation loss decreased (0.443658 --> 0.442880).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 176.4301552772522
Epoch: 39, Steps: 88 | Train Loss: 0.2611673 Vali Loss: 0.4423468 Test Loss: 0.5252699
Validation loss decreased (0.442880 --> 0.442347).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 160.58312702178955
Epoch: 40, Steps: 88 | Train Loss: 0.2607827 Vali Loss: 0.4420559 Test Loss: 0.5246103
Validation loss decreased (0.442347 --> 0.442056).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 179.41038751602173
Epoch: 41, Steps: 88 | Train Loss: 0.2602760 Vali Loss: 0.4416442 Test Loss: 0.5239578
Validation loss decreased (0.442056 --> 0.441644).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 170.7080478668213
Epoch: 42, Steps: 88 | Train Loss: 0.2599014 Vali Loss: 0.4410983 Test Loss: 0.5233515
Validation loss decreased (0.441644 --> 0.441098).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 171.88147234916687
Epoch: 43, Steps: 88 | Train Loss: 0.2595550 Vali Loss: 0.4409875 Test Loss: 0.5227600
Validation loss decreased (0.441098 --> 0.440988).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 167.1192729473114
Epoch: 44, Steps: 88 | Train Loss: 0.2592124 Vali Loss: 0.4408534 Test Loss: 0.5222799
Validation loss decreased (0.440988 --> 0.440853).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 172.0897011756897
Epoch: 45, Steps: 88 | Train Loss: 0.2589005 Vali Loss: 0.4400885 Test Loss: 0.5217630
Validation loss decreased (0.440853 --> 0.440089).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 168.34394764900208
Epoch: 46, Steps: 88 | Train Loss: 0.2585770 Vali Loss: 0.4402197 Test Loss: 0.5212083
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 167.888596534729
Epoch: 47, Steps: 88 | Train Loss: 0.2583514 Vali Loss: 0.4402365 Test Loss: 0.5207813
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 164.97520852088928
Epoch: 48, Steps: 88 | Train Loss: 0.2580922 Vali Loss: 0.4393310 Test Loss: 0.5203360
Validation loss decreased (0.440089 --> 0.439331).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 170.22607803344727
Epoch: 49, Steps: 88 | Train Loss: 0.2577067 Vali Loss: 0.4395644 Test Loss: 0.5200018
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 147.7387490272522
Epoch: 50, Steps: 88 | Train Loss: 0.2575830 Vali Loss: 0.4391624 Test Loss: 0.5196390
Validation loss decreased (0.439331 --> 0.439162).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 137.62617468833923
Epoch: 51, Steps: 88 | Train Loss: 0.2573732 Vali Loss: 0.4384384 Test Loss: 0.5192686
Validation loss decreased (0.439162 --> 0.438438).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 145.27855157852173
Epoch: 52, Steps: 88 | Train Loss: 0.2571536 Vali Loss: 0.4382490 Test Loss: 0.5189230
Validation loss decreased (0.438438 --> 0.438249).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 151.24186301231384
Epoch: 53, Steps: 88 | Train Loss: 0.2569091 Vali Loss: 0.4381186 Test Loss: 0.5185860
Validation loss decreased (0.438249 --> 0.438119).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 145.74176120758057
Epoch: 54, Steps: 88 | Train Loss: 0.2567409 Vali Loss: 0.4377886 Test Loss: 0.5182908
Validation loss decreased (0.438119 --> 0.437789).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 132.34200859069824
Epoch: 55, Steps: 88 | Train Loss: 0.2565796 Vali Loss: 0.4378909 Test Loss: 0.5180182
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 144.44264316558838
Epoch: 56, Steps: 88 | Train Loss: 0.2563438 Vali Loss: 0.4378030 Test Loss: 0.5177612
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 183.9180178642273
Epoch: 57, Steps: 88 | Train Loss: 0.2562151 Vali Loss: 0.4372222 Test Loss: 0.5175133
Validation loss decreased (0.437789 --> 0.437222).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 159.2347972393036
Epoch: 58, Steps: 88 | Train Loss: 0.2560314 Vali Loss: 0.4374255 Test Loss: 0.5172557
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 147.8855001926422
Epoch: 59, Steps: 88 | Train Loss: 0.2558999 Vali Loss: 0.4377432 Test Loss: 0.5170448
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 147.83840107917786
Epoch: 60, Steps: 88 | Train Loss: 0.2558502 Vali Loss: 0.4366783 Test Loss: 0.5167980
Validation loss decreased (0.437222 --> 0.436678).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 146.3007366657257
Epoch: 61, Steps: 88 | Train Loss: 0.2556672 Vali Loss: 0.4369817 Test Loss: 0.5166213
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 187.0571002960205
Epoch: 62, Steps: 88 | Train Loss: 0.2555703 Vali Loss: 0.4367527 Test Loss: 0.5164258
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 200.1434507369995
Epoch: 63, Steps: 88 | Train Loss: 0.2553985 Vali Loss: 0.4366145 Test Loss: 0.5162323
Validation loss decreased (0.436678 --> 0.436614).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 199.4136984348297
Epoch: 64, Steps: 88 | Train Loss: 0.2552382 Vali Loss: 0.4370930 Test Loss: 0.5161008
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 194.7343852519989
Epoch: 65, Steps: 88 | Train Loss: 0.2551376 Vali Loss: 0.4370744 Test Loss: 0.5159449
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 188.5583894252777
Epoch: 66, Steps: 88 | Train Loss: 0.2551115 Vali Loss: 0.4368720 Test Loss: 0.5157689
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 195.71844792366028
Epoch: 67, Steps: 88 | Train Loss: 0.2550952 Vali Loss: 0.4367164 Test Loss: 0.5156135
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 197.4306035041809
Epoch: 68, Steps: 88 | Train Loss: 0.2548716 Vali Loss: 0.4362482 Test Loss: 0.5154855
Validation loss decreased (0.436614 --> 0.436248).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 198.2047028541565
Epoch: 69, Steps: 88 | Train Loss: 0.2548591 Vali Loss: 0.4364639 Test Loss: 0.5153476
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 193.57214903831482
Epoch: 70, Steps: 88 | Train Loss: 0.2547185 Vali Loss: 0.4362756 Test Loss: 0.5152223
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 195.6581473350525
Epoch: 71, Steps: 88 | Train Loss: 0.2547027 Vali Loss: 0.4361420 Test Loss: 0.5151268
Validation loss decreased (0.436248 --> 0.436142).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 188.60637187957764
Epoch: 72, Steps: 88 | Train Loss: 0.2546250 Vali Loss: 0.4363525 Test Loss: 0.5150087
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 192.761146068573
Epoch: 73, Steps: 88 | Train Loss: 0.2545710 Vali Loss: 0.4359042 Test Loss: 0.5149023
Validation loss decreased (0.436142 --> 0.435904).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 189.97811484336853
Epoch: 74, Steps: 88 | Train Loss: 0.2545067 Vali Loss: 0.4356100 Test Loss: 0.5147822
Validation loss decreased (0.435904 --> 0.435610).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 185.2739384174347
Epoch: 75, Steps: 88 | Train Loss: 0.2544650 Vali Loss: 0.4361104 Test Loss: 0.5146878
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 194.08633732795715
Epoch: 76, Steps: 88 | Train Loss: 0.2543094 Vali Loss: 0.4358818 Test Loss: 0.5146018
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 191.0920078754425
Epoch: 77, Steps: 88 | Train Loss: 0.2542685 Vali Loss: 0.4356539 Test Loss: 0.5145136
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 174.7921974658966
Epoch: 78, Steps: 88 | Train Loss: 0.2542713 Vali Loss: 0.4356413 Test Loss: 0.5144271
EarlyStopping counter: 4 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 163.87014961242676
Epoch: 79, Steps: 88 | Train Loss: 0.2542558 Vali Loss: 0.4358844 Test Loss: 0.5143455
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 157.06798481941223
Epoch: 80, Steps: 88 | Train Loss: 0.2541372 Vali Loss: 0.4357138 Test Loss: 0.5142652
EarlyStopping counter: 6 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 168.0583987236023
Epoch: 81, Steps: 88 | Train Loss: 0.2540762 Vali Loss: 0.4356924 Test Loss: 0.5142065
EarlyStopping counter: 7 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 153.36252641677856
Epoch: 82, Steps: 88 | Train Loss: 0.2540724 Vali Loss: 0.4351959 Test Loss: 0.5141402
Validation loss decreased (0.435610 --> 0.435196).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 167.45450234413147
Epoch: 83, Steps: 88 | Train Loss: 0.2540376 Vali Loss: 0.4357581 Test Loss: 0.5140747
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 168.32721590995789
Epoch: 84, Steps: 88 | Train Loss: 0.2539222 Vali Loss: 0.4354083 Test Loss: 0.5140045
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 167.89192461967468
Epoch: 85, Steps: 88 | Train Loss: 0.2539557 Vali Loss: 0.4351887 Test Loss: 0.5139486
Validation loss decreased (0.435196 --> 0.435189).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 168.61122679710388
Epoch: 86, Steps: 88 | Train Loss: 0.2539087 Vali Loss: 0.4352308 Test Loss: 0.5139064
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 152.58278608322144
Epoch: 87, Steps: 88 | Train Loss: 0.2538758 Vali Loss: 0.4352421 Test Loss: 0.5138525
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 143.599125623703
Epoch: 88, Steps: 88 | Train Loss: 0.2538360 Vali Loss: 0.4351307 Test Loss: 0.5138041
Validation loss decreased (0.435189 --> 0.435131).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 149.44244122505188
Epoch: 89, Steps: 88 | Train Loss: 0.2538675 Vali Loss: 0.4350529 Test Loss: 0.5137572
Validation loss decreased (0.435131 --> 0.435053).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 149.66309428215027
Epoch: 90, Steps: 88 | Train Loss: 0.2538148 Vali Loss: 0.4352866 Test Loss: 0.5137129
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 152.96813416481018
Epoch: 91, Steps: 88 | Train Loss: 0.2537425 Vali Loss: 0.4356187 Test Loss: 0.5136690
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 132.75321626663208
Epoch: 92, Steps: 88 | Train Loss: 0.2537697 Vali Loss: 0.4354498 Test Loss: 0.5136313
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 138.48022961616516
Epoch: 93, Steps: 88 | Train Loss: 0.2538308 Vali Loss: 0.4354047 Test Loss: 0.5135887
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 146.4187889099121
Epoch: 94, Steps: 88 | Train Loss: 0.2537036 Vali Loss: 0.4353170 Test Loss: 0.5135574
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 146.75661325454712
Epoch: 95, Steps: 88 | Train Loss: 0.2537114 Vali Loss: 0.4349010 Test Loss: 0.5135238
Validation loss decreased (0.435053 --> 0.434901).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 139.56242871284485
Epoch: 96, Steps: 88 | Train Loss: 0.2536475 Vali Loss: 0.4354295 Test Loss: 0.5134914
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 137.7948853969574
Epoch: 97, Steps: 88 | Train Loss: 0.2536565 Vali Loss: 0.4348946 Test Loss: 0.5134601
Validation loss decreased (0.434901 --> 0.434895).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 126.7771532535553
Epoch: 98, Steps: 88 | Train Loss: 0.2536198 Vali Loss: 0.4350575 Test Loss: 0.5134348
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 117.48264789581299
Epoch: 99, Steps: 88 | Train Loss: 0.2535557 Vali Loss: 0.4348554 Test Loss: 0.5134083
Validation loss decreased (0.434895 --> 0.434855).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 128.4286608695984
Epoch: 100, Steps: 88 | Train Loss: 0.2536351 Vali Loss: 0.4348832 Test Loss: 0.5133762
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11381
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=90, out_features=450, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4468608000.0
params:  40950.0
Trainable parameters:  40950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 113.58501172065735
Epoch: 1, Steps: 88 | Train Loss: 0.3129520 Vali Loss: 0.4319625 Test Loss: 0.5075310
Validation loss decreased (inf --> 0.431962).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 110.25066995620728
Epoch: 2, Steps: 88 | Train Loss: 0.3109211 Vali Loss: 0.4313005 Test Loss: 0.5065957
Validation loss decreased (0.431962 --> 0.431300).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 169.16202354431152
Epoch: 3, Steps: 88 | Train Loss: 0.3104747 Vali Loss: 0.4307856 Test Loss: 0.5062002
Validation loss decreased (0.431300 --> 0.430786).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 194.73756098747253
Epoch: 4, Steps: 88 | Train Loss: 0.3103939 Vali Loss: 0.4308426 Test Loss: 0.5061655
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 172.8965835571289
Epoch: 5, Steps: 88 | Train Loss: 0.3103789 Vali Loss: 0.4307279 Test Loss: 0.5056726
Validation loss decreased (0.430786 --> 0.430728).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 176.09595012664795
Epoch: 6, Steps: 88 | Train Loss: 0.3103210 Vali Loss: 0.4305564 Test Loss: 0.5056482
Validation loss decreased (0.430728 --> 0.430556).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 172.4907350540161
Epoch: 7, Steps: 88 | Train Loss: 0.3102017 Vali Loss: 0.4306153 Test Loss: 0.5062182
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 162.79900336265564
Epoch: 8, Steps: 88 | Train Loss: 0.3103244 Vali Loss: 0.4309484 Test Loss: 0.5061902
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 166.98331332206726
Epoch: 9, Steps: 88 | Train Loss: 0.3101940 Vali Loss: 0.4310454 Test Loss: 0.5057858
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 160.6666965484619
Epoch: 10, Steps: 88 | Train Loss: 0.3102379 Vali Loss: 0.4308096 Test Loss: 0.5058418
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 169.8399076461792
Epoch: 11, Steps: 88 | Train Loss: 0.3102130 Vali Loss: 0.4305972 Test Loss: 0.5055804
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 168.02707171440125
Epoch: 12, Steps: 88 | Train Loss: 0.3101977 Vali Loss: 0.4302445 Test Loss: 0.5058103
Validation loss decreased (0.430556 --> 0.430245).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 161.96528840065002
Epoch: 13, Steps: 88 | Train Loss: 0.3101965 Vali Loss: 0.4311014 Test Loss: 0.5059257
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 166.89535450935364
Epoch: 14, Steps: 88 | Train Loss: 0.3102168 Vali Loss: 0.4308454 Test Loss: 0.5056899
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 171.86315488815308
Epoch: 15, Steps: 88 | Train Loss: 0.3101057 Vali Loss: 0.4308792 Test Loss: 0.5058597
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 176.9717128276825
Epoch: 16, Steps: 88 | Train Loss: 0.3101853 Vali Loss: 0.4305647 Test Loss: 0.5059113
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 163.56786584854126
Epoch: 17, Steps: 88 | Train Loss: 0.3101532 Vali Loss: 0.4308536 Test Loss: 0.5058376
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 158.44649076461792
Epoch: 18, Steps: 88 | Train Loss: 0.3101637 Vali Loss: 0.4308769 Test Loss: 0.5057568
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 154.70098233222961
Epoch: 19, Steps: 88 | Train Loss: 0.3100933 Vali Loss: 0.4304499 Test Loss: 0.5056930
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 146.80570030212402
Epoch: 20, Steps: 88 | Train Loss: 0.3099935 Vali Loss: 0.4307155 Test Loss: 0.5059313
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 128.86120891571045
Epoch: 21, Steps: 88 | Train Loss: 0.3101698 Vali Loss: 0.4303188 Test Loss: 0.5053952
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 129.76326894760132
Epoch: 22, Steps: 88 | Train Loss: 0.3100402 Vali Loss: 0.4304400 Test Loss: 0.5057783
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j720_H10_FITS_custom_ftM_sl180_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.5037015676498413, mae:0.32485002279281616, rse:0.5803465843200684, corr:[0.25029212 0.26641765 0.26559305 0.26564583 0.2651269  0.26524675
 0.26539713 0.2656949  0.26494366 0.26475886 0.26409468 0.26467854
 0.2647674  0.264702   0.26504543 0.26523906 0.26513806 0.26486027
 0.26531857 0.26480696 0.26520795 0.26571423 0.26601857 0.26591155
 0.2651973  0.26533794 0.26482207 0.26504263 0.26529202 0.26503998
 0.26544577 0.26521143 0.26467866 0.26447025 0.2642233  0.26452112
 0.26402315 0.26388106 0.26425454 0.2640461  0.26404196 0.26393786
 0.26392782 0.26413378 0.2640468  0.26418826 0.26491585 0.2648747
 0.2647646  0.26454532 0.26392537 0.26385707 0.2638263  0.26384345
 0.26425716 0.26438364 0.26387963 0.2638288  0.2638504  0.2641856
 0.26343086 0.26303056 0.2629152  0.262704   0.26281378 0.26256934
 0.26292437 0.26349854 0.26337078 0.26305476 0.26342177 0.2629499
 0.2628211  0.26338822 0.2627973  0.26239586 0.26217607 0.2621897
 0.26242968 0.2626471  0.26235408 0.26259422 0.26261613 0.2628825
 0.26295644 0.26322922 0.26332852 0.2629507  0.26254436 0.26280946
 0.26310122 0.26252732 0.26318958 0.26392123 0.26338133 0.2630721
 0.26303527 0.26323166 0.26320788 0.26306346 0.26222876 0.26258284
 0.2626429  0.2624249  0.26262107 0.26256096 0.26273978 0.26297042
 0.26321816 0.26295388 0.26286575 0.26313084 0.26318523 0.26285228
 0.26270738 0.26273477 0.26291105 0.26286075 0.26280072 0.26269695
 0.26273242 0.26291668 0.26279277 0.26247215 0.26230273 0.2622616
 0.26228815 0.26233038 0.2624855  0.26277423 0.26322097 0.26362976
 0.2635083  0.26378748 0.26370573 0.26321793 0.26320046 0.2622156
 0.2621583  0.26241472 0.2627053  0.26352587 0.26380366 0.26411545
 0.2642813  0.2638771  0.2628798  0.26297373 0.26297408 0.26272956
 0.26298934 0.2632554  0.26331457 0.26370478 0.2645528  0.26536193
 0.266314   0.26666054 0.26594374 0.26505244 0.26500767 0.26520023
 0.26529014 0.2654066  0.26594883 0.2669501  0.26825666 0.26925072
 0.2683418  0.26773486 0.26632765 0.2658461  0.26551446 0.26578858
 0.26614097 0.2663229  0.26643974 0.26648882 0.26714456 0.26738805
 0.26732776 0.26675636 0.26624385 0.26588726 0.26587746 0.26622894
 0.265785   0.26563692 0.26661608 0.2671846  0.26766616 0.26725438
 0.26635462 0.26608038 0.2657798  0.26629156 0.26635945 0.26601464
 0.26646435 0.26660612 0.2662412  0.26651606 0.266368   0.2661809
 0.2657085  0.26534903 0.26560783 0.26556867 0.26553014 0.2653038
 0.26504862 0.26561594 0.26582178 0.2654806  0.26590192 0.26583427
 0.26586694 0.26584828 0.26494226 0.26545498 0.26523405 0.26518822
 0.2659672  0.26565465 0.26509678 0.2651844  0.26553828 0.26579285
 0.26532584 0.26491103 0.2647047  0.26453045 0.2644195  0.26402682
 0.2642518  0.26454088 0.26485726 0.26500902 0.26526353 0.26533863
 0.265377   0.2650522  0.26410833 0.26410905 0.26452002 0.26419583
 0.26413396 0.26447552 0.2646672  0.26444617 0.2646224  0.26458907
 0.2644472  0.2645169  0.26449087 0.26406753 0.26388937 0.26396102
 0.26411158 0.2641472  0.26406983 0.26476884 0.26488146 0.26438984
 0.26438496 0.2644583  0.26478112 0.2648366  0.2650938  0.26517597
 0.26504612 0.2646633  0.26489592 0.26499388 0.2645828  0.26496497
 0.26483604 0.26413545 0.2639312  0.26486567 0.26520488 0.2644478
 0.2644865  0.26452416 0.26486456 0.26498324 0.26459777 0.26421353
 0.26351017 0.2637858  0.26402107 0.26384488 0.26394165 0.26388866
 0.26428074 0.26387155 0.263865   0.2641483  0.26416886 0.26471043
 0.26525244 0.26535428 0.26530293 0.26559106 0.2654495  0.26477945
 0.26443046 0.2646102  0.2650608  0.26528203 0.266187   0.26652178
 0.2660345  0.26563504 0.2651378  0.26530492 0.26556876 0.26570734
 0.2656826  0.26563403 0.26558766 0.26571417 0.26653007 0.26737463
 0.26806024 0.26817483 0.26749304 0.26721936 0.2675657  0.26794243
 0.2676452  0.26754758 0.26802114 0.26810932 0.2694747  0.26995903
 0.26854432 0.26789805 0.2671105  0.26703608 0.2667322  0.26688635
 0.2670253  0.26695278 0.2673423  0.26755133 0.26799393 0.2683751
 0.26836368 0.26731482 0.2663647  0.26641005 0.26678774 0.2670443
 0.2667558  0.26655868 0.26720956 0.2673611  0.26809222 0.26801905
 0.26710603 0.26733023 0.26674488 0.26695588 0.26660067 0.26626956
 0.26688406 0.26705116 0.26703376 0.26776966 0.26737863 0.26653087
 0.26616922 0.265402   0.26562893 0.2661047  0.26660255 0.2667472
 0.26678324 0.26637647 0.26633853 0.26617846 0.26630196 0.2664743
 0.2660944  0.2659536  0.26559645 0.265988   0.26635283 0.26631504
 0.26585716 0.2654192  0.26533148 0.26543775 0.26578042 0.2661895
 0.2656357  0.26465076 0.26410374 0.26397642 0.26427948 0.26497543
 0.26524583 0.2653128  0.26532334 0.2649111  0.2654574  0.26494747
 0.264012   0.26445362 0.2642898  0.26429364 0.26439467 0.26489115
 0.26494238 0.26467004 0.2649549  0.2651421  0.26532176 0.2654223
 0.26535144 0.26558217 0.26486555 0.2647023  0.26465073 0.26458052
 0.2647604  0.26463592 0.26498058 0.26560098 0.26538703 0.26507562
 0.26490366 0.26499286 0.2650159  0.26494238 0.2652323  0.26543862
 0.26523003 0.26497746 0.2648794  0.26513276 0.265115   0.2653717
 0.2651067  0.26483577 0.26476848 0.26507494 0.2653275  0.2650363
 0.26509213 0.26436725 0.26498508 0.26515645 0.2646018  0.26493794
 0.26443323 0.26494402 0.2652526  0.26506475 0.26486903 0.26442552
 0.26425585 0.2643434  0.26438954 0.26469305 0.26515043 0.26537955
 0.26569167 0.26541823 0.26572806 0.26528332 0.26518407 0.26517507
 0.2647941  0.26462933 0.2647786  0.26574087 0.2660104  0.26594174
 0.266151   0.26604298 0.26581076 0.26593035 0.2656706  0.26558554
 0.26487613 0.26472765 0.26519585 0.26519173 0.2657352  0.26654446
 0.2680132  0.26773772 0.26708335 0.26682797 0.26661426 0.2668937
 0.26687092 0.26699626 0.26749104 0.26840046 0.26886585 0.26883063
 0.2680288  0.26793647 0.26738185 0.2668697  0.26630932 0.26618862
 0.26639807 0.26648152 0.26693752 0.26715752 0.26745644 0.2675856
 0.26757452 0.26703185 0.26649085 0.26653984 0.26629156 0.26595315
 0.26627013 0.26622212 0.26626423 0.26659557 0.26650006 0.26656792
 0.26633623 0.26698685 0.26699817 0.26713112 0.26692483 0.26630518
 0.26669183 0.26705924 0.26674154 0.2665925  0.26630652 0.26580116
 0.2657399  0.26523778 0.2653795  0.26573697 0.2659817  0.2660541
 0.26573431 0.26616225 0.2661416  0.26529184 0.2655066  0.26547876
 0.26549554 0.2651735  0.2650974  0.26580793 0.2657181  0.26562655
 0.26625633 0.26652595 0.26653233 0.2664431  0.26648074 0.26612234
 0.2655647  0.26439357 0.264279   0.26440722 0.26456502 0.26470014
 0.26473662 0.26504245 0.2645788  0.26451328 0.26503825 0.26456082
 0.26461348 0.2643065  0.2639235  0.26474836 0.26445982 0.2644592
 0.26418507 0.26436704 0.26452285 0.26376566 0.26397917 0.26372385
 0.26383382 0.26357394 0.26332852 0.2636094  0.26266423 0.2628859
 0.26343194 0.26382315 0.26400945 0.263998   0.26366666 0.26304808
 0.26344964 0.26446483 0.26438105 0.26426587 0.26408073 0.26407722
 0.26358232 0.26352403 0.26368687 0.2632893  0.26370734 0.26361024
 0.2631662  0.26328993 0.26356274 0.2635389  0.26297936 0.26256973
 0.2628282  0.26333582 0.26345006 0.26350102 0.263656   0.2637494
 0.26356214 0.26394156 0.2640585  0.264212   0.2642064  0.26349744
 0.26296565 0.26300278 0.26246998 0.2620134  0.262799   0.26313558
 0.26317757 0.2629756  0.2621308  0.26231706 0.26244923 0.26214376
 0.26251176 0.26291275 0.26323372 0.26366624 0.26383904 0.26402724
 0.26453388 0.26523235 0.26453227 0.26434678 0.26405478 0.26398554
 0.2638344  0.26353243 0.26319543 0.26291123 0.26398385 0.26479653
 0.26569983 0.26580998 0.26531497 0.264562   0.26407126 0.26422793
 0.2642796  0.2646805  0.26501122 0.265579   0.2663156  0.26700822
 0.26660421 0.26627535 0.26536757 0.2654993  0.2651904  0.26456472
 0.26454103 0.26465157 0.26477885 0.26471338 0.26520157 0.2659215
 0.2662456  0.26551777 0.2656222  0.26473162 0.2646655  0.26515627
 0.26528165 0.26546898 0.26584685 0.26648277 0.26621896 0.26690224
 0.26694092 0.26694405 0.26686135 0.26652485 0.2661397  0.2661824
 0.2665021  0.2666114  0.26674548 0.26626918 0.26635924 0.26636437
 0.26587132 0.26499134 0.26538858 0.2652824  0.26583663 0.2654754
 0.26523617 0.26521027 0.26537105 0.26433328 0.2651825  0.26649377]
