Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j96_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12781322240.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 126.28992009162903
Epoch: 1, Steps: 89 | Train Loss: 0.9944167 Vali Loss: 1.0758046 Test Loss: 1.2322634
Validation loss decreased (inf --> 1.075805).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 113.03698515892029
Epoch: 2, Steps: 89 | Train Loss: 0.7575378 Vali Loss: 0.9860241 Test Loss: 1.1284417
Validation loss decreased (1.075805 --> 0.986024).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 123.06207585334778
Epoch: 3, Steps: 89 | Train Loss: 0.6628629 Vali Loss: 0.9256117 Test Loss: 1.0626554
Validation loss decreased (0.986024 --> 0.925612).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 114.6477587223053
Epoch: 4, Steps: 89 | Train Loss: 0.5919768 Vali Loss: 0.8799038 Test Loss: 1.0114622
Validation loss decreased (0.925612 --> 0.879904).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 115.76390862464905
Epoch: 5, Steps: 89 | Train Loss: 0.5333248 Vali Loss: 0.8345073 Test Loss: 0.9569717
Validation loss decreased (0.879904 --> 0.834507).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 116.38133239746094
Epoch: 6, Steps: 89 | Train Loss: 0.4834410 Vali Loss: 0.7893217 Test Loss: 0.9078155
Validation loss decreased (0.834507 --> 0.789322).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 113.05269527435303
Epoch: 7, Steps: 89 | Train Loss: 0.4405540 Vali Loss: 0.7538607 Test Loss: 0.8674932
Validation loss decreased (0.789322 --> 0.753861).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 111.33781218528748
Epoch: 8, Steps: 89 | Train Loss: 0.4032486 Vali Loss: 0.7228000 Test Loss: 0.8307551
Validation loss decreased (0.753861 --> 0.722800).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 114.5273380279541
Epoch: 9, Steps: 89 | Train Loss: 0.3707359 Vali Loss: 0.6889377 Test Loss: 0.7929401
Validation loss decreased (0.722800 --> 0.688938).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 109.78356885910034
Epoch: 10, Steps: 89 | Train Loss: 0.3420739 Vali Loss: 0.6645387 Test Loss: 0.7651642
Validation loss decreased (0.688938 --> 0.664539).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 122.0981240272522
Epoch: 11, Steps: 89 | Train Loss: 0.3167928 Vali Loss: 0.6364143 Test Loss: 0.7336221
Validation loss decreased (0.664539 --> 0.636414).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 102.5011465549469
Epoch: 12, Steps: 89 | Train Loss: 0.2943910 Vali Loss: 0.6173673 Test Loss: 0.7107608
Validation loss decreased (0.636414 --> 0.617367).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 99.62370729446411
Epoch: 13, Steps: 89 | Train Loss: 0.2744265 Vali Loss: 0.5970914 Test Loss: 0.6873900
Validation loss decreased (0.617367 --> 0.597091).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 115.53066778182983
Epoch: 14, Steps: 89 | Train Loss: 0.2565401 Vali Loss: 0.5782555 Test Loss: 0.6664745
Validation loss decreased (0.597091 --> 0.578256).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 100.08432626724243
Epoch: 15, Steps: 89 | Train Loss: 0.2405075 Vali Loss: 0.5631337 Test Loss: 0.6494590
Validation loss decreased (0.578256 --> 0.563134).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 94.11637091636658
Epoch: 16, Steps: 89 | Train Loss: 0.2261201 Vali Loss: 0.5461304 Test Loss: 0.6301259
Validation loss decreased (0.563134 --> 0.546130).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 86.77456831932068
Epoch: 17, Steps: 89 | Train Loss: 0.2131035 Vali Loss: 0.5341896 Test Loss: 0.6154418
Validation loss decreased (0.546130 --> 0.534190).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 86.75879168510437
Epoch: 18, Steps: 89 | Train Loss: 0.2013717 Vali Loss: 0.5211236 Test Loss: 0.6006696
Validation loss decreased (0.534190 --> 0.521124).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 86.70403099060059
Epoch: 19, Steps: 89 | Train Loss: 0.1907073 Vali Loss: 0.5094666 Test Loss: 0.5878996
Validation loss decreased (0.521124 --> 0.509467).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 87.6511743068695
Epoch: 20, Steps: 89 | Train Loss: 0.1810424 Vali Loss: 0.4997289 Test Loss: 0.5762048
Validation loss decreased (0.509467 --> 0.499729).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 87.46444010734558
Epoch: 21, Steps: 89 | Train Loss: 0.1722096 Vali Loss: 0.4907289 Test Loss: 0.5672063
Validation loss decreased (0.499729 --> 0.490729).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 84.95602464675903
Epoch: 22, Steps: 89 | Train Loss: 0.1641480 Vali Loss: 0.4824068 Test Loss: 0.5569999
Validation loss decreased (0.490729 --> 0.482407).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 85.64896059036255
Epoch: 23, Steps: 89 | Train Loss: 0.1567863 Vali Loss: 0.4737318 Test Loss: 0.5471068
Validation loss decreased (0.482407 --> 0.473732).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 86.38981914520264
Epoch: 24, Steps: 89 | Train Loss: 0.1500435 Vali Loss: 0.4660740 Test Loss: 0.5377603
Validation loss decreased (0.473732 --> 0.466074).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 90.72429299354553
Epoch: 25, Steps: 89 | Train Loss: 0.1438481 Vali Loss: 0.4584180 Test Loss: 0.5298563
Validation loss decreased (0.466074 --> 0.458418).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 89.3843457698822
Epoch: 26, Steps: 89 | Train Loss: 0.1381490 Vali Loss: 0.4528710 Test Loss: 0.5238602
Validation loss decreased (0.458418 --> 0.452871).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 92.91809821128845
Epoch: 27, Steps: 89 | Train Loss: 0.1329208 Vali Loss: 0.4456275 Test Loss: 0.5165959
Validation loss decreased (0.452871 --> 0.445627).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 86.97909760475159
Epoch: 28, Steps: 89 | Train Loss: 0.1280541 Vali Loss: 0.4408987 Test Loss: 0.5110577
Validation loss decreased (0.445627 --> 0.440899).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 72.07796740531921
Epoch: 29, Steps: 89 | Train Loss: 0.1235928 Vali Loss: 0.4375125 Test Loss: 0.5059919
Validation loss decreased (0.440899 --> 0.437513).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 72.7242202758789
Epoch: 30, Steps: 89 | Train Loss: 0.1194703 Vali Loss: 0.4321323 Test Loss: 0.5001524
Validation loss decreased (0.437513 --> 0.432132).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 71.31559491157532
Epoch: 31, Steps: 89 | Train Loss: 0.1156114 Vali Loss: 0.4272864 Test Loss: 0.4948302
Validation loss decreased (0.432132 --> 0.427286).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 72.95144581794739
Epoch: 32, Steps: 89 | Train Loss: 0.1120604 Vali Loss: 0.4241733 Test Loss: 0.4910376
Validation loss decreased (0.427286 --> 0.424173).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 75.33956503868103
Epoch: 33, Steps: 89 | Train Loss: 0.1087648 Vali Loss: 0.4206103 Test Loss: 0.4861248
Validation loss decreased (0.424173 --> 0.420610).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 72.40017867088318
Epoch: 34, Steps: 89 | Train Loss: 0.1056868 Vali Loss: 0.4148706 Test Loss: 0.4817568
Validation loss decreased (0.420610 --> 0.414871).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 73.80205464363098
Epoch: 35, Steps: 89 | Train Loss: 0.1028108 Vali Loss: 0.4118141 Test Loss: 0.4781189
Validation loss decreased (0.414871 --> 0.411814).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 73.7883460521698
Epoch: 36, Steps: 89 | Train Loss: 0.1001508 Vali Loss: 0.4091116 Test Loss: 0.4748444
Validation loss decreased (0.411814 --> 0.409112).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 72.0015516281128
Epoch: 37, Steps: 89 | Train Loss: 0.0976685 Vali Loss: 0.4059887 Test Loss: 0.4717168
Validation loss decreased (0.409112 --> 0.405989).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 73.26204872131348
Epoch: 38, Steps: 89 | Train Loss: 0.0953369 Vali Loss: 0.4032878 Test Loss: 0.4682798
Validation loss decreased (0.405989 --> 0.403288).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 75.06238055229187
Epoch: 39, Steps: 89 | Train Loss: 0.0931494 Vali Loss: 0.4019323 Test Loss: 0.4656755
Validation loss decreased (0.403288 --> 0.401932).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 65.50375938415527
Epoch: 40, Steps: 89 | Train Loss: 0.0911299 Vali Loss: 0.3993276 Test Loss: 0.4637160
Validation loss decreased (0.401932 --> 0.399328).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 100.2770504951477
Epoch: 41, Steps: 89 | Train Loss: 0.0892111 Vali Loss: 0.3971983 Test Loss: 0.4608261
Validation loss decreased (0.399328 --> 0.397198).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 105.07638216018677
Epoch: 42, Steps: 89 | Train Loss: 0.0874331 Vali Loss: 0.3953432 Test Loss: 0.4585897
Validation loss decreased (0.397198 --> 0.395343).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 103.82947564125061
Epoch: 43, Steps: 89 | Train Loss: 0.0857489 Vali Loss: 0.3932126 Test Loss: 0.4567128
Validation loss decreased (0.395343 --> 0.393213).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 106.70545220375061
Epoch: 44, Steps: 89 | Train Loss: 0.0841637 Vali Loss: 0.3913969 Test Loss: 0.4542824
Validation loss decreased (0.393213 --> 0.391397).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 106.18179535865784
Epoch: 45, Steps: 89 | Train Loss: 0.0826776 Vali Loss: 0.3903128 Test Loss: 0.4522238
Validation loss decreased (0.391397 --> 0.390313).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 104.12442183494568
Epoch: 46, Steps: 89 | Train Loss: 0.0812819 Vali Loss: 0.3877372 Test Loss: 0.4508788
Validation loss decreased (0.390313 --> 0.387737).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 131.7689733505249
Epoch: 47, Steps: 89 | Train Loss: 0.0799755 Vali Loss: 0.3862208 Test Loss: 0.4490976
Validation loss decreased (0.387737 --> 0.386221).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 130.26973366737366
Epoch: 48, Steps: 89 | Train Loss: 0.0787426 Vali Loss: 0.3849409 Test Loss: 0.4477426
Validation loss decreased (0.386221 --> 0.384941).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 135.5095226764679
Epoch: 49, Steps: 89 | Train Loss: 0.0775906 Vali Loss: 0.3829535 Test Loss: 0.4462831
Validation loss decreased (0.384941 --> 0.382954).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 133.09776711463928
Epoch: 50, Steps: 89 | Train Loss: 0.0764972 Vali Loss: 0.3816696 Test Loss: 0.4444830
Validation loss decreased (0.382954 --> 0.381670).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 257.56952595710754
Epoch: 51, Steps: 89 | Train Loss: 0.0754641 Vali Loss: 0.3813818 Test Loss: 0.4433030
Validation loss decreased (0.381670 --> 0.381382).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 232.4731502532959
Epoch: 52, Steps: 89 | Train Loss: 0.0744811 Vali Loss: 0.3805368 Test Loss: 0.4420259
Validation loss decreased (0.381382 --> 0.380537).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 137.0723786354065
Epoch: 53, Steps: 89 | Train Loss: 0.0735625 Vali Loss: 0.3796893 Test Loss: 0.4410271
Validation loss decreased (0.380537 --> 0.379689).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 126.90458679199219
Epoch: 54, Steps: 89 | Train Loss: 0.0726971 Vali Loss: 0.3782398 Test Loss: 0.4396161
Validation loss decreased (0.379689 --> 0.378240).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 128.42134380340576
Epoch: 55, Steps: 89 | Train Loss: 0.0718474 Vali Loss: 0.3762247 Test Loss: 0.4385658
Validation loss decreased (0.378240 --> 0.376225).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 133.05225014686584
Epoch: 56, Steps: 89 | Train Loss: 0.0710742 Vali Loss: 0.3751971 Test Loss: 0.4374548
Validation loss decreased (0.376225 --> 0.375197).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 136.82911610603333
Epoch: 57, Steps: 89 | Train Loss: 0.0703368 Vali Loss: 0.3751532 Test Loss: 0.4368082
Validation loss decreased (0.375197 --> 0.375153).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 217.7258288860321
Epoch: 58, Steps: 89 | Train Loss: 0.0696521 Vali Loss: 0.3744080 Test Loss: 0.4359034
Validation loss decreased (0.375153 --> 0.374408).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 148.80705976486206
Epoch: 59, Steps: 89 | Train Loss: 0.0689886 Vali Loss: 0.3741737 Test Loss: 0.4348238
Validation loss decreased (0.374408 --> 0.374174).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 147.77500534057617
Epoch: 60, Steps: 89 | Train Loss: 0.0683730 Vali Loss: 0.3723293 Test Loss: 0.4340707
Validation loss decreased (0.374174 --> 0.372329).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 150.21637272834778
Epoch: 61, Steps: 89 | Train Loss: 0.0677655 Vali Loss: 0.3717227 Test Loss: 0.4331593
Validation loss decreased (0.372329 --> 0.371723).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 200.84462070465088
Epoch: 62, Steps: 89 | Train Loss: 0.0672118 Vali Loss: 0.3717489 Test Loss: 0.4325290
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 127.2155978679657
Epoch: 63, Steps: 89 | Train Loss: 0.0666645 Vali Loss: 0.3704010 Test Loss: 0.4315915
Validation loss decreased (0.371723 --> 0.370401).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 145.1630618572235
Epoch: 64, Steps: 89 | Train Loss: 0.0661847 Vali Loss: 0.3702075 Test Loss: 0.4313579
Validation loss decreased (0.370401 --> 0.370208).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 129.66944074630737
Epoch: 65, Steps: 89 | Train Loss: 0.0656969 Vali Loss: 0.3697330 Test Loss: 0.4306163
Validation loss decreased (0.370208 --> 0.369733).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 132.5801169872284
Epoch: 66, Steps: 89 | Train Loss: 0.0652454 Vali Loss: 0.3683747 Test Loss: 0.4301504
Validation loss decreased (0.369733 --> 0.368375).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 140.47533559799194
Epoch: 67, Steps: 89 | Train Loss: 0.0648287 Vali Loss: 0.3683342 Test Loss: 0.4294540
Validation loss decreased (0.368375 --> 0.368334).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 152.25102615356445
Epoch: 68, Steps: 89 | Train Loss: 0.0644106 Vali Loss: 0.3671531 Test Loss: 0.4289162
Validation loss decreased (0.368334 --> 0.367153).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 157.6536283493042
Epoch: 69, Steps: 89 | Train Loss: 0.0640230 Vali Loss: 0.3671936 Test Loss: 0.4284150
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 184.5794198513031
Epoch: 70, Steps: 89 | Train Loss: 0.0636707 Vali Loss: 0.3668902 Test Loss: 0.4279868
Validation loss decreased (0.367153 --> 0.366890).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 139.35034823417664
Epoch: 71, Steps: 89 | Train Loss: 0.0633131 Vali Loss: 0.3668541 Test Loss: 0.4275434
Validation loss decreased (0.366890 --> 0.366854).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 157.62875652313232
Epoch: 72, Steps: 89 | Train Loss: 0.0629783 Vali Loss: 0.3667037 Test Loss: 0.4271124
Validation loss decreased (0.366854 --> 0.366704).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 139.8251667022705
Epoch: 73, Steps: 89 | Train Loss: 0.0626592 Vali Loss: 0.3654816 Test Loss: 0.4267769
Validation loss decreased (0.366704 --> 0.365482).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 137.16273736953735
Epoch: 74, Steps: 89 | Train Loss: 0.0623676 Vali Loss: 0.3662972 Test Loss: 0.4262640
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 215.23388290405273
Epoch: 75, Steps: 89 | Train Loss: 0.0620816 Vali Loss: 0.3651725 Test Loss: 0.4260031
Validation loss decreased (0.365482 --> 0.365173).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 234.75955724716187
Epoch: 76, Steps: 89 | Train Loss: 0.0618128 Vali Loss: 0.3648146 Test Loss: 0.4256313
Validation loss decreased (0.365173 --> 0.364815).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 231.1300745010376
Epoch: 77, Steps: 89 | Train Loss: 0.0615651 Vali Loss: 0.3639005 Test Loss: 0.4252758
Validation loss decreased (0.364815 --> 0.363901).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 218.81236696243286
Epoch: 78, Steps: 89 | Train Loss: 0.0613135 Vali Loss: 0.3645023 Test Loss: 0.4249814
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 154.82720494270325
Epoch: 79, Steps: 89 | Train Loss: 0.0610727 Vali Loss: 0.3650139 Test Loss: 0.4246936
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 146.29010128974915
Epoch: 80, Steps: 89 | Train Loss: 0.0608727 Vali Loss: 0.3632104 Test Loss: 0.4244240
Validation loss decreased (0.363901 --> 0.363210).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 124.74061322212219
Epoch: 81, Steps: 89 | Train Loss: 0.0606499 Vali Loss: 0.3634235 Test Loss: 0.4241541
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 185.74359583854675
Epoch: 82, Steps: 89 | Train Loss: 0.0604515 Vali Loss: 0.3627655 Test Loss: 0.4238826
Validation loss decreased (0.363210 --> 0.362765).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 147.58840894699097
Epoch: 83, Steps: 89 | Train Loss: 0.0602687 Vali Loss: 0.3632206 Test Loss: 0.4236155
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 124.36476039886475
Epoch: 84, Steps: 89 | Train Loss: 0.0601068 Vali Loss: 0.3637879 Test Loss: 0.4233899
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 126.17127275466919
Epoch: 85, Steps: 89 | Train Loss: 0.0599273 Vali Loss: 0.3617155 Test Loss: 0.4232031
Validation loss decreased (0.362765 --> 0.361716).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 101.74900364875793
Epoch: 86, Steps: 89 | Train Loss: 0.0597628 Vali Loss: 0.3626442 Test Loss: 0.4229526
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 124.34871578216553
Epoch: 87, Steps: 89 | Train Loss: 0.0596090 Vali Loss: 0.3622775 Test Loss: 0.4227455
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 104.31046724319458
Epoch: 88, Steps: 89 | Train Loss: 0.0594646 Vali Loss: 0.3622682 Test Loss: 0.4225838
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 106.1849992275238
Epoch: 89, Steps: 89 | Train Loss: 0.0593267 Vali Loss: 0.3617533 Test Loss: 0.4224040
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 107.06007361412048
Epoch: 90, Steps: 89 | Train Loss: 0.0592020 Vali Loss: 0.3622070 Test Loss: 0.4222388
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 100.55078840255737
Epoch: 91, Steps: 89 | Train Loss: 0.0590550 Vali Loss: 0.3618091 Test Loss: 0.4221164
EarlyStopping counter: 6 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 105.15362930297852
Epoch: 92, Steps: 89 | Train Loss: 0.0589430 Vali Loss: 0.3617188 Test Loss: 0.4219356
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 105.58180618286133
Epoch: 93, Steps: 89 | Train Loss: 0.0588305 Vali Loss: 0.3604971 Test Loss: 0.4217817
Validation loss decreased (0.361716 --> 0.360497).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 106.25638365745544
Epoch: 94, Steps: 89 | Train Loss: 0.0587297 Vali Loss: 0.3614244 Test Loss: 0.4216500
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 106.60713362693787
Epoch: 95, Steps: 89 | Train Loss: 0.0586141 Vali Loss: 0.3623803 Test Loss: 0.4215406
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 106.85724115371704
Epoch: 96, Steps: 89 | Train Loss: 0.0585325 Vali Loss: 0.3611933 Test Loss: 0.4213895
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 102.66898083686829
Epoch: 97, Steps: 89 | Train Loss: 0.0584364 Vali Loss: 0.3605874 Test Loss: 0.4212795
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 105.21155214309692
Epoch: 98, Steps: 89 | Train Loss: 0.0583419 Vali Loss: 0.3594416 Test Loss: 0.4211676
Validation loss decreased (0.360497 --> 0.359442).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 104.63277411460876
Epoch: 99, Steps: 89 | Train Loss: 0.0582618 Vali Loss: 0.3603789 Test Loss: 0.4210674
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 109.0844943523407
Epoch: 100, Steps: 89 | Train Loss: 0.0581866 Vali Loss: 0.3599389 Test Loss: 0.4209523
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12781322240.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 109.85285806655884
Epoch: 1, Steps: 89 | Train Loss: 0.2359679 Vali Loss: 0.3274354 Test Loss: 0.3911507
Validation loss decreased (inf --> 0.327435).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 108.87033748626709
Epoch: 2, Steps: 89 | Train Loss: 0.2320123 Vali Loss: 0.3264939 Test Loss: 0.3904891
Validation loss decreased (0.327435 --> 0.326494).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 114.27860188484192
Epoch: 3, Steps: 89 | Train Loss: 0.2316264 Vali Loss: 0.3255850 Test Loss: 0.3900664
Validation loss decreased (0.326494 --> 0.325585).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 105.87994861602783
Epoch: 4, Steps: 89 | Train Loss: 0.2314871 Vali Loss: 0.3252224 Test Loss: 0.3894411
Validation loss decreased (0.325585 --> 0.325222).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 119.25056171417236
Epoch: 5, Steps: 89 | Train Loss: 0.2311672 Vali Loss: 0.3256103 Test Loss: 0.3903221
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 110.52614903450012
Epoch: 6, Steps: 89 | Train Loss: 0.2312188 Vali Loss: 0.3251298 Test Loss: 0.3897491
Validation loss decreased (0.325222 --> 0.325130).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 95.06114459037781
Epoch: 7, Steps: 89 | Train Loss: 0.2310271 Vali Loss: 0.3253365 Test Loss: 0.3898276
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 101.15686297416687
Epoch: 8, Steps: 89 | Train Loss: 0.2311013 Vali Loss: 0.3247106 Test Loss: 0.3899044
Validation loss decreased (0.325130 --> 0.324711).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 106.713858127594
Epoch: 9, Steps: 89 | Train Loss: 0.2309843 Vali Loss: 0.3242050 Test Loss: 0.3890105
Validation loss decreased (0.324711 --> 0.324205).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 116.75818538665771
Epoch: 10, Steps: 89 | Train Loss: 0.2308912 Vali Loss: 0.3251638 Test Loss: 0.3891930
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 118.55477261543274
Epoch: 11, Steps: 89 | Train Loss: 0.2309249 Vali Loss: 0.3248295 Test Loss: 0.3894213
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 102.69541454315186
Epoch: 12, Steps: 89 | Train Loss: 0.2308795 Vali Loss: 0.3219463 Test Loss: 0.3895395
Validation loss decreased (0.324205 --> 0.321946).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 99.50912880897522
Epoch: 13, Steps: 89 | Train Loss: 0.2308748 Vali Loss: 0.3244590 Test Loss: 0.3891898
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 99.57195973396301
Epoch: 14, Steps: 89 | Train Loss: 0.2307613 Vali Loss: 0.3244052 Test Loss: 0.3888257
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 105.53334712982178
Epoch: 15, Steps: 89 | Train Loss: 0.2307306 Vali Loss: 0.3237462 Test Loss: 0.3892717
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 109.08847522735596
Epoch: 16, Steps: 89 | Train Loss: 0.2307174 Vali Loss: 0.3245477 Test Loss: 0.3887228
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 104.58662676811218
Epoch: 17, Steps: 89 | Train Loss: 0.2306720 Vali Loss: 0.3259671 Test Loss: 0.3897610
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 100.62888216972351
Epoch: 18, Steps: 89 | Train Loss: 0.2306010 Vali Loss: 0.3227394 Test Loss: 0.3888286
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 97.2395248413086
Epoch: 19, Steps: 89 | Train Loss: 0.2304472 Vali Loss: 0.3234828 Test Loss: 0.3897412
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 105.7157826423645
Epoch: 20, Steps: 89 | Train Loss: 0.2306183 Vali Loss: 0.3249685 Test Loss: 0.3892763
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 95.09145474433899
Epoch: 21, Steps: 89 | Train Loss: 0.2304997 Vali Loss: 0.3237415 Test Loss: 0.3887829
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 87.19033026695251
Epoch: 22, Steps: 89 | Train Loss: 0.2304876 Vali Loss: 0.3245647 Test Loss: 0.3887815
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.3865956664085388, mae:0.2699113190174103, rse:0.514851450920105, corr:[0.27554706 0.29143953 0.29241556 0.29282495 0.29299137 0.29265442
 0.29236975 0.2917877  0.29172304 0.2920582  0.29231617 0.29205656
 0.2913119  0.29085112 0.29150534 0.29192007 0.29204804 0.29259303
 0.29287142 0.29292437 0.29283145 0.2928334  0.2928753  0.2924995
 0.29306504 0.29289764 0.2925546  0.2924129  0.29258302 0.29218927
 0.29182217 0.29232952 0.29234782 0.29208848 0.29247388 0.2923703
 0.29186156 0.2912367  0.2915948  0.29270497 0.29281512 0.2926805
 0.29254302 0.29256535 0.29252794 0.29230624 0.29235694 0.2925146
 0.2931662  0.29303557 0.292214   0.29168242 0.29165155 0.29198018
 0.29201978 0.29182902 0.29219136 0.29253012 0.29228112 0.29160255
 0.29160145 0.29177237 0.29126957 0.29177216 0.2922836  0.2919419
 0.2919395  0.29240853 0.29267523 0.29242522 0.2926149  0.29282126
 0.29212466 0.2916896  0.2914892  0.2908531  0.29067728 0.29073748
 0.2909691  0.29156426 0.29162052 0.29128435 0.291393   0.29149473
 0.29069996 0.29014784 0.2905471  0.2907192  0.2906761  0.29091293
 0.2910808  0.29184666 0.29217398 0.29173115 0.2925885  0.29164723]
