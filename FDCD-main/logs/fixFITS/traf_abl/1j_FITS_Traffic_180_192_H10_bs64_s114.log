Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j192_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j192_H10_FITS_custom_ftM_sl180_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11909
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=90, out_features=186, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1847024640.0
params:  16926.0
Trainable parameters:  16926
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 146.38744616508484
Epoch: 1, Steps: 93 | Train Loss: 0.9153080 Vali Loss: 0.7440529 Test Loss: 0.9036055
Validation loss decreased (inf --> 0.744053).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 115.16526675224304
Epoch: 2, Steps: 93 | Train Loss: 0.4902689 Vali Loss: 0.5489930 Test Loss: 0.6754797
Validation loss decreased (0.744053 --> 0.548993).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 123.49878859519958
Epoch: 3, Steps: 93 | Train Loss: 0.3937794 Vali Loss: 0.4815202 Test Loss: 0.5937849
Validation loss decreased (0.548993 --> 0.481520).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 129.52658772468567
Epoch: 4, Steps: 93 | Train Loss: 0.3517678 Vali Loss: 0.4443267 Test Loss: 0.5480397
Validation loss decreased (0.481520 --> 0.444327).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 122.97317242622375
Epoch: 5, Steps: 93 | Train Loss: 0.3271031 Vali Loss: 0.4211503 Test Loss: 0.5197233
Validation loss decreased (0.444327 --> 0.421150).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 111.12678647041321
Epoch: 6, Steps: 93 | Train Loss: 0.3114583 Vali Loss: 0.4060533 Test Loss: 0.5015184
Validation loss decreased (0.421150 --> 0.406053).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 108.5063784122467
Epoch: 7, Steps: 93 | Train Loss: 0.3011922 Vali Loss: 0.3962981 Test Loss: 0.4895506
Validation loss decreased (0.406053 --> 0.396298).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 109.92250752449036
Epoch: 8, Steps: 93 | Train Loss: 0.2943094 Vali Loss: 0.3890413 Test Loss: 0.4816189
Validation loss decreased (0.396298 --> 0.389041).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 103.04498744010925
Epoch: 9, Steps: 93 | Train Loss: 0.2896140 Vali Loss: 0.3845696 Test Loss: 0.4760939
Validation loss decreased (0.389041 --> 0.384570).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 100.24258613586426
Epoch: 10, Steps: 93 | Train Loss: 0.2863072 Vali Loss: 0.3816037 Test Loss: 0.4724277
Validation loss decreased (0.384570 --> 0.381604).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 101.2559745311737
Epoch: 11, Steps: 93 | Train Loss: 0.2840554 Vali Loss: 0.3784847 Test Loss: 0.4698712
Validation loss decreased (0.381604 --> 0.378485).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 102.66393542289734
Epoch: 12, Steps: 93 | Train Loss: 0.2824733 Vali Loss: 0.3774123 Test Loss: 0.4681284
Validation loss decreased (0.378485 --> 0.377412).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 106.45521306991577
Epoch: 13, Steps: 93 | Train Loss: 0.2813120 Vali Loss: 0.3764677 Test Loss: 0.4668770
Validation loss decreased (0.377412 --> 0.376468).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 104.45715379714966
Epoch: 14, Steps: 93 | Train Loss: 0.2804943 Vali Loss: 0.3753768 Test Loss: 0.4660228
Validation loss decreased (0.376468 --> 0.375377).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 107.43072414398193
Epoch: 15, Steps: 93 | Train Loss: 0.2798701 Vali Loss: 0.3746503 Test Loss: 0.4653136
Validation loss decreased (0.375377 --> 0.374650).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 85.99546074867249
Epoch: 16, Steps: 93 | Train Loss: 0.2794249 Vali Loss: 0.3742742 Test Loss: 0.4648321
Validation loss decreased (0.374650 --> 0.374274).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 87.75947070121765
Epoch: 17, Steps: 93 | Train Loss: 0.2791148 Vali Loss: 0.3739829 Test Loss: 0.4645357
Validation loss decreased (0.374274 --> 0.373983).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 99.27816534042358
Epoch: 18, Steps: 93 | Train Loss: 0.2788306 Vali Loss: 0.3737785 Test Loss: 0.4642706
Validation loss decreased (0.373983 --> 0.373778).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 86.54547691345215
Epoch: 19, Steps: 93 | Train Loss: 0.2786602 Vali Loss: 0.3731019 Test Loss: 0.4641735
Validation loss decreased (0.373778 --> 0.373102).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 88.48218584060669
Epoch: 20, Steps: 93 | Train Loss: 0.2785064 Vali Loss: 0.3727798 Test Loss: 0.4640187
Validation loss decreased (0.373102 --> 0.372780).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 92.9998300075531
Epoch: 21, Steps: 93 | Train Loss: 0.2784030 Vali Loss: 0.3732417 Test Loss: 0.4638702
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 95.80699944496155
Epoch: 22, Steps: 93 | Train Loss: 0.2782922 Vali Loss: 0.3730496 Test Loss: 0.4637577
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 92.44749093055725
Epoch: 23, Steps: 93 | Train Loss: 0.2782126 Vali Loss: 0.3730195 Test Loss: 0.4637199
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 90.78450226783752
Epoch: 24, Steps: 93 | Train Loss: 0.2781424 Vali Loss: 0.3727328 Test Loss: 0.4636504
Validation loss decreased (0.372780 --> 0.372733).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 88.77670645713806
Epoch: 25, Steps: 93 | Train Loss: 0.2781027 Vali Loss: 0.3728018 Test Loss: 0.4635661
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 81.54257011413574
Epoch: 26, Steps: 93 | Train Loss: 0.2780434 Vali Loss: 0.3725251 Test Loss: 0.4634931
Validation loss decreased (0.372733 --> 0.372525).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 77.26126098632812
Epoch: 27, Steps: 93 | Train Loss: 0.2779789 Vali Loss: 0.3731239 Test Loss: 0.4635329
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 77.83841180801392
Epoch: 28, Steps: 93 | Train Loss: 0.2780054 Vali Loss: 0.3723392 Test Loss: 0.4634525
Validation loss decreased (0.372525 --> 0.372339).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 53.49530053138733
Epoch: 29, Steps: 93 | Train Loss: 0.2779506 Vali Loss: 0.3726052 Test Loss: 0.4634717
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 64.31857347488403
Epoch: 30, Steps: 93 | Train Loss: 0.2779048 Vali Loss: 0.3725592 Test Loss: 0.4634292
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 71.78212022781372
Epoch: 31, Steps: 93 | Train Loss: 0.2778924 Vali Loss: 0.3728157 Test Loss: 0.4633674
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 53.624433279037476
Epoch: 32, Steps: 93 | Train Loss: 0.2778746 Vali Loss: 0.3723901 Test Loss: 0.4634138
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 56.178375244140625
Epoch: 33, Steps: 93 | Train Loss: 0.2778730 Vali Loss: 0.3728123 Test Loss: 0.4633693
EarlyStopping counter: 5 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 56.46428036689758
Epoch: 34, Steps: 93 | Train Loss: 0.2778575 Vali Loss: 0.3731544 Test Loss: 0.4633336
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 67.63954257965088
Epoch: 35, Steps: 93 | Train Loss: 0.2778385 Vali Loss: 0.3723659 Test Loss: 0.4632611
EarlyStopping counter: 7 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 65.76114773750305
Epoch: 36, Steps: 93 | Train Loss: 0.2778254 Vali Loss: 0.3727229 Test Loss: 0.4632731
EarlyStopping counter: 8 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 74.33828139305115
Epoch: 37, Steps: 93 | Train Loss: 0.2777970 Vali Loss: 0.3725879 Test Loss: 0.4632853
EarlyStopping counter: 9 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 70.95503616333008
Epoch: 38, Steps: 93 | Train Loss: 0.2777891 Vali Loss: 0.3726130 Test Loss: 0.4632743
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j192_H10_FITS_custom_ftM_sl180_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.4625948369503021, mae:0.30258479714393616, rse:0.5613444447517395, corr:[0.2723263  0.2879715  0.28495052 0.28663868 0.28551307 0.287297
 0.28696856 0.28790385 0.287459   0.28764915 0.28754798 0.286854
 0.2866968  0.28603068 0.28598177 0.28496987 0.28517076 0.28457695
 0.28508002 0.2852722  0.28578776 0.28605264 0.28683525 0.28692245
 0.28733057 0.28680313 0.28657237 0.2862945  0.28603506 0.28649023
 0.28639922 0.28668067 0.28627568 0.28646153 0.2861429  0.28615215
 0.285499   0.28527224 0.28503355 0.2852689  0.28578812 0.28542423
 0.28557467 0.28545773 0.28544134 0.28560072 0.2857974  0.28565717
 0.28544518 0.28479725 0.28535697 0.28535095 0.2851706  0.28533322
 0.2852453  0.2854272  0.28514478 0.28516147 0.284934   0.2845771
 0.28436366 0.28468367 0.28458452 0.2846097  0.284687   0.28473675
 0.28443566 0.2843217  0.28420886 0.28429466 0.2843633  0.28452113
 0.28442642 0.28398436 0.2840918  0.28375465 0.28403416 0.28409168
 0.28357527 0.28346074 0.28302974 0.2832176  0.28356612 0.28357294
 0.28362736 0.2839793  0.28376552 0.28378195 0.2841381  0.28418255
 0.28377125 0.28353697 0.28350276 0.28351733 0.28335062 0.28362447
 0.2835179  0.28359354 0.28345412 0.28357968 0.28370714 0.2833681
 0.28332874 0.2831615  0.28281212 0.28265682 0.28252044 0.28262413
 0.28272197 0.28311667 0.2831714  0.28298756 0.28280285 0.28291294
 0.28259718 0.28278992 0.28290004 0.2829617  0.2826793  0.28294924
 0.28303537 0.28288615 0.28322804 0.2831042  0.2832745  0.28266597
 0.28253764 0.28281024 0.28278333 0.28280285 0.28275672 0.28299716
 0.283522   0.28401148 0.2836725  0.28361684 0.28327617 0.2831749
 0.2830196  0.28319696 0.28349188 0.28357667 0.28376085 0.284167
 0.28418052 0.28384882 0.2838228  0.2836255  0.28394338 0.28373253
 0.2838609  0.28344074 0.283278   0.28346825 0.28387466 0.28495932
 0.2861369  0.28690425 0.28632453 0.28565577 0.285157   0.2849394
 0.28498355 0.28530854 0.28564802 0.28622526 0.28707573 0.2876207
 0.2865343  0.28670076 0.2858633  0.285432   0.28591967 0.28553388
 0.28604838 0.28558096 0.28555506 0.2855036  0.28576136 0.28594223
 0.28640684 0.2868854  0.28655624 0.28602654 0.2857711  0.28457713
 0.28510988 0.2840902  0.2850608  0.2839922  0.28828278 0.28546807]
