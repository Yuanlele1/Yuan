Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=138, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j720_H8_FITS_custom_ftM_sl360_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11201
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=138, out_features=414, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6303716352.0
params:  57546.0
Trainable parameters:  57546
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 158.11126017570496
Epoch: 1, Steps: 87 | Train Loss: 1.2315565 Vali Loss: 1.0452662 Test Loss: 1.2334437
Validation loss decreased (inf --> 1.045266).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 156.3222963809967
Epoch: 2, Steps: 87 | Train Loss: 0.7350706 Vali Loss: 0.8149536 Test Loss: 0.9509259
Validation loss decreased (1.045266 --> 0.814954).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 156.79846453666687
Epoch: 3, Steps: 87 | Train Loss: 0.5948049 Vali Loss: 0.6973722 Test Loss: 0.8098719
Validation loss decreased (0.814954 --> 0.697372).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 162.25524377822876
Epoch: 4, Steps: 87 | Train Loss: 0.5088852 Vali Loss: 0.6167096 Test Loss: 0.7135416
Validation loss decreased (0.697372 --> 0.616710).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 151.39021229743958
Epoch: 5, Steps: 87 | Train Loss: 0.4480716 Vali Loss: 0.5586464 Test Loss: 0.6449694
Validation loss decreased (0.616710 --> 0.558646).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 147.59584856033325
Epoch: 6, Steps: 87 | Train Loss: 0.4039535 Vali Loss: 0.5159000 Test Loss: 0.5948938
Validation loss decreased (0.558646 --> 0.515900).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 148.6452317237854
Epoch: 7, Steps: 87 | Train Loss: 0.3716512 Vali Loss: 0.4839840 Test Loss: 0.5586170
Validation loss decreased (0.515900 --> 0.483984).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 151.06260323524475
Epoch: 8, Steps: 87 | Train Loss: 0.3478722 Vali Loss: 0.4613167 Test Loss: 0.5321203
Validation loss decreased (0.483984 --> 0.461317).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 154.24865913391113
Epoch: 9, Steps: 87 | Train Loss: 0.3305592 Vali Loss: 0.4442112 Test Loss: 0.5130370
Validation loss decreased (0.461317 --> 0.444211).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 151.66092824935913
Epoch: 10, Steps: 87 | Train Loss: 0.3178587 Vali Loss: 0.4319549 Test Loss: 0.4987876
Validation loss decreased (0.444211 --> 0.431955).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 149.92651271820068
Epoch: 11, Steps: 87 | Train Loss: 0.3086085 Vali Loss: 0.4228767 Test Loss: 0.4887078
Validation loss decreased (0.431955 --> 0.422877).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 156.2241313457489
Epoch: 12, Steps: 87 | Train Loss: 0.3018457 Vali Loss: 0.4163389 Test Loss: 0.4817869
Validation loss decreased (0.422877 --> 0.416339).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 149.6649625301361
Epoch: 13, Steps: 87 | Train Loss: 0.2969117 Vali Loss: 0.4118603 Test Loss: 0.4764146
Validation loss decreased (0.416339 --> 0.411860).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 145.9064507484436
Epoch: 14, Steps: 87 | Train Loss: 0.2934551 Vali Loss: 0.4083708 Test Loss: 0.4728392
Validation loss decreased (0.411860 --> 0.408371).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 147.97820043563843
Epoch: 15, Steps: 87 | Train Loss: 0.2908314 Vali Loss: 0.4065223 Test Loss: 0.4701964
Validation loss decreased (0.408371 --> 0.406522).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 155.31808137893677
Epoch: 16, Steps: 87 | Train Loss: 0.2889471 Vali Loss: 0.4040234 Test Loss: 0.4685588
Validation loss decreased (0.406522 --> 0.404023).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 154.6541290283203
Epoch: 17, Steps: 87 | Train Loss: 0.2875257 Vali Loss: 0.4024125 Test Loss: 0.4670837
Validation loss decreased (0.404023 --> 0.402413).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 154.49881958961487
Epoch: 18, Steps: 87 | Train Loss: 0.2866203 Vali Loss: 0.4020504 Test Loss: 0.4662308
Validation loss decreased (0.402413 --> 0.402050).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 156.09088730812073
Epoch: 19, Steps: 87 | Train Loss: 0.2858908 Vali Loss: 0.4015638 Test Loss: 0.4654543
Validation loss decreased (0.402050 --> 0.401564).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 158.14813041687012
Epoch: 20, Steps: 87 | Train Loss: 0.2853002 Vali Loss: 0.4010069 Test Loss: 0.4651348
Validation loss decreased (0.401564 --> 0.401007).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 154.2801206111908
Epoch: 21, Steps: 87 | Train Loss: 0.2849923 Vali Loss: 0.4003899 Test Loss: 0.4648602
Validation loss decreased (0.401007 --> 0.400390).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 156.63140082359314
Epoch: 22, Steps: 87 | Train Loss: 0.2846581 Vali Loss: 0.4000159 Test Loss: 0.4645517
Validation loss decreased (0.400390 --> 0.400016).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 155.31154823303223
Epoch: 23, Steps: 87 | Train Loss: 0.2844801 Vali Loss: 0.4004023 Test Loss: 0.4641682
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 138.15010857582092
Epoch: 24, Steps: 87 | Train Loss: 0.2843256 Vali Loss: 0.4001557 Test Loss: 0.4640839
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 140.29852557182312
Epoch: 25, Steps: 87 | Train Loss: 0.2841716 Vali Loss: 0.3995588 Test Loss: 0.4640748
Validation loss decreased (0.400016 --> 0.399559).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 138.75799083709717
Epoch: 26, Steps: 87 | Train Loss: 0.2840943 Vali Loss: 0.3996454 Test Loss: 0.4640482
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 132.3782229423523
Epoch: 27, Steps: 87 | Train Loss: 0.2841932 Vali Loss: 0.3999480 Test Loss: 0.4640021
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 111.61126661300659
Epoch: 28, Steps: 87 | Train Loss: 0.2839357 Vali Loss: 0.3996641 Test Loss: 0.4640083
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 113.22899961471558
Epoch: 29, Steps: 87 | Train Loss: 0.2839327 Vali Loss: 0.3994600 Test Loss: 0.4639826
Validation loss decreased (0.399559 --> 0.399460).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 112.96749758720398
Epoch: 30, Steps: 87 | Train Loss: 0.2839922 Vali Loss: 0.3993812 Test Loss: 0.4639960
Validation loss decreased (0.399460 --> 0.399381).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 104.36124324798584
Epoch: 31, Steps: 87 | Train Loss: 0.2839133 Vali Loss: 0.3995709 Test Loss: 0.4639652
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 99.34648251533508
Epoch: 32, Steps: 87 | Train Loss: 0.2838776 Vali Loss: 0.3995869 Test Loss: 0.4639601
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 102.32754874229431
Epoch: 33, Steps: 87 | Train Loss: 0.2839084 Vali Loss: 0.3992199 Test Loss: 0.4639387
Validation loss decreased (0.399381 --> 0.399220).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 103.93927526473999
Epoch: 34, Steps: 87 | Train Loss: 0.2838484 Vali Loss: 0.3995512 Test Loss: 0.4639995
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 104.90494537353516
Epoch: 35, Steps: 87 | Train Loss: 0.2837670 Vali Loss: 0.3991234 Test Loss: 0.4639739
Validation loss decreased (0.399220 --> 0.399123).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 107.67268538475037
Epoch: 36, Steps: 87 | Train Loss: 0.2838086 Vali Loss: 0.3991358 Test Loss: 0.4639702
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 110.73593997955322
Epoch: 37, Steps: 87 | Train Loss: 0.2837379 Vali Loss: 0.3991581 Test Loss: 0.4639836
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 111.2620005607605
Epoch: 38, Steps: 87 | Train Loss: 0.2838410 Vali Loss: 0.3997955 Test Loss: 0.4639497
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 109.66765546798706
Epoch: 39, Steps: 87 | Train Loss: 0.2838491 Vali Loss: 0.3989832 Test Loss: 0.4639141
Validation loss decreased (0.399123 --> 0.398983).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 109.50440716743469
Epoch: 40, Steps: 87 | Train Loss: 0.2837345 Vali Loss: 0.3996671 Test Loss: 0.4639168
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 111.33576107025146
Epoch: 41, Steps: 87 | Train Loss: 0.2837665 Vali Loss: 0.3992502 Test Loss: 0.4639542
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 101.47358798980713
Epoch: 42, Steps: 87 | Train Loss: 0.2837655 Vali Loss: 0.3990604 Test Loss: 0.4639174
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 100.38639259338379
Epoch: 43, Steps: 87 | Train Loss: 0.2837746 Vali Loss: 0.3994527 Test Loss: 0.4639364
EarlyStopping counter: 4 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 97.69514083862305
Epoch: 44, Steps: 87 | Train Loss: 0.2837522 Vali Loss: 0.3992519 Test Loss: 0.4639356
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 97.91069912910461
Epoch: 45, Steps: 87 | Train Loss: 0.2836694 Vali Loss: 0.3998350 Test Loss: 0.4639424
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 91.12456631660461
Epoch: 46, Steps: 87 | Train Loss: 0.2836513 Vali Loss: 0.3996565 Test Loss: 0.4639646
EarlyStopping counter: 7 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 87.20388269424438
Epoch: 47, Steps: 87 | Train Loss: 0.2837205 Vali Loss: 0.3990899 Test Loss: 0.4639032
EarlyStopping counter: 8 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 77.71986174583435
Epoch: 48, Steps: 87 | Train Loss: 0.2837158 Vali Loss: 0.3993333 Test Loss: 0.4638934
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 83.64379835128784
Epoch: 49, Steps: 87 | Train Loss: 0.2837361 Vali Loss: 0.3990946 Test Loss: 0.4639405
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j720_H8_FITS_custom_ftM_sl360_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.4632081687450409, mae:0.30731719732284546, rse:0.5565304160118103, corr:[0.2565173  0.2634802  0.26652724 0.26471668 0.26659477 0.26832798
 0.26711097 0.2683808  0.26854652 0.26708037 0.26760417 0.26682326
 0.26572028 0.26640883 0.2657703  0.265444   0.26645237 0.2661577
 0.26633057 0.26717407 0.26675436 0.26657495 0.26673573 0.26694563
 0.26841822 0.26868126 0.2681873  0.26869234 0.26853997 0.2677333
 0.26795375 0.2674577  0.2666022  0.2669848  0.2665718  0.26591885
 0.26649258 0.26632944 0.26611826 0.26678616 0.26675588 0.26691312
 0.26747057 0.26709127 0.2670067  0.26728672 0.26684785 0.26698968
 0.2676752  0.26737776 0.26748797 0.26741064 0.26670596 0.26681203
 0.26678938 0.26614332 0.2663245  0.26630393 0.26575434 0.26602313
 0.26612216 0.26589605 0.2664346  0.26649746 0.2662745  0.2668048
 0.2667333  0.26639685 0.26669896 0.2664993  0.2662052  0.2666259
 0.26655477 0.26630715 0.26640934 0.26607892 0.2659718  0.26599595
 0.2653894  0.2653531  0.26575896 0.2655557  0.2656374  0.265876
 0.26559183 0.26582077 0.26625183 0.26611897 0.26632136 0.26639706
 0.2661463  0.2663355  0.26649317 0.2662012  0.26620987 0.26624978
 0.26594254 0.2659251  0.265682   0.26543742 0.26559752 0.2653467
 0.2651144  0.26542556 0.2653581  0.26532343 0.2656651  0.26558813
 0.26563275 0.26604372 0.2659855  0.26609987 0.2665011  0.26621926
 0.26616392 0.266365   0.26604366 0.26581806 0.26595825 0.2657574
 0.2655316  0.2656445  0.2655465  0.26565263 0.26570845 0.26533288
 0.2654507  0.26575938 0.26568222 0.2658183  0.26591286 0.26580334
 0.26606292 0.26621178 0.26614514 0.26644123 0.2665727  0.2660892
 0.26627833 0.26626825 0.26600754 0.2660434  0.26613012 0.2664604
 0.26679927 0.26672244 0.26668233 0.2669079  0.26674953 0.2666505
 0.26677346 0.26666257 0.26670673 0.26689845 0.26673433 0.266939
 0.26740426 0.2673208  0.26737198 0.26772884 0.26766476 0.2675763
 0.267587   0.26738572 0.26741946 0.26737362 0.26710698 0.26770097
 0.26864123 0.2683213  0.2682418  0.26804602 0.26766178 0.26780197
 0.26780847 0.26755705 0.26781672 0.26799935 0.26790887 0.2680804
 0.26816115 0.26811743 0.26830965 0.26831782 0.26830104 0.26853982
 0.26841578 0.26820707 0.2681691  0.2678277  0.26751927 0.26771
 0.26818857 0.26814345 0.2680603  0.2678729  0.2679856  0.26815286
 0.2679885  0.26800734 0.2681499  0.26810935 0.26823702 0.26830813
 0.2681977  0.26830596 0.26838383 0.26832885 0.26855195 0.26848495
 0.2679604  0.2678177  0.26776722 0.26750284 0.26739097 0.26747778
 0.26778185 0.2678811  0.26760596 0.26746827 0.2676079  0.267449
 0.2674047  0.26762426 0.2675108  0.2675316  0.2678623  0.26782465
 0.26775235 0.26779792 0.2675809  0.26753637 0.26778385 0.26770058
 0.26750237 0.26740476 0.26716423 0.26696894 0.26695898 0.26699564
 0.26716843 0.26718327 0.26702747 0.26711312 0.2671306  0.26698825
 0.26710817 0.26717398 0.2671511  0.26744768 0.26755658 0.26745322
 0.26755688 0.26758814 0.26758718 0.26778126 0.26779127 0.26768777
 0.2677246  0.2674602  0.26719356 0.26724884 0.26710138 0.2669292
 0.26688576 0.2668104  0.26675564 0.26674536 0.26660305 0.26667202
 0.26686063 0.2669202  0.26711226 0.26725873 0.26713908 0.2672708
 0.26735017 0.26720923 0.2674736  0.26759595 0.26723173 0.26711577
 0.26717588 0.26676977 0.26657283 0.26660463 0.2665122  0.26661816
 0.26650372 0.2665183  0.26671836 0.26672983 0.26669475 0.26692104
 0.26696613 0.2669741  0.26722845 0.2672072  0.26712388 0.26742518
 0.2673976  0.26713154 0.26725516 0.267317   0.26728305 0.2672596
 0.2672825  0.26719138 0.26720408 0.2670856  0.2672062  0.267729
 0.26786822 0.26804072 0.26827946 0.2682876  0.26830837 0.26838097
 0.2683395  0.26856592 0.26874277 0.26861423 0.26873913 0.26891807
 0.2687653  0.26874495 0.26877683 0.26866236 0.26880056 0.26887238
 0.26876426 0.2687499  0.26866505 0.2683954  0.26835272 0.26881826
 0.26935875 0.26925382 0.2690456  0.2690017  0.26899078 0.2687713
 0.268756   0.26905745 0.2691084  0.2690796  0.26926738 0.26924938
 0.26923892 0.26940304 0.26922604 0.26906952 0.26927754 0.2692548
 0.26907817 0.26899952 0.26874197 0.26856518 0.26847792 0.26846313
 0.26891705 0.2689931  0.26897824 0.26917705 0.2691642  0.26904753
 0.269277   0.2693447  0.2691649  0.2692694  0.26934546 0.2692608
 0.26930833 0.2691792  0.2690039  0.26920456 0.26918498 0.2688743
 0.268841   0.26876855 0.26851735 0.26846135 0.26834995 0.2682934
 0.26849234 0.26842666 0.26850346 0.2686692  0.26857722 0.26864097
 0.26881325 0.2687387  0.26879403 0.2689129  0.26880053 0.2687687
 0.26870504 0.2684054  0.26832303 0.2682969  0.26808766 0.26809722
 0.2680873  0.2678021  0.2676786  0.2676559  0.2675966  0.26786858
 0.26795593 0.2677206  0.26784915 0.26808771 0.2681239  0.2681931
 0.2682165  0.26830417 0.26853424 0.2685198  0.2684006  0.26833212
 0.26807818 0.26795262 0.2680938  0.26803327 0.26801592 0.26811406
 0.26789603 0.2676566  0.2676956  0.26751402 0.2673315  0.26749703
 0.26741907 0.26741087 0.26754138 0.26761392 0.26773998 0.26774758
 0.2676636  0.26796815 0.26824123 0.26814216 0.26816767 0.2681863
 0.26797172 0.26786086 0.26782095 0.26780528 0.2677597  0.2673181
 0.26695725 0.26693013 0.26692697 0.26688555 0.267001   0.2669247
 0.26667368 0.26697806 0.26725718 0.26737824 0.26764223 0.26762506
 0.26753792 0.26775634 0.26779795 0.2677205  0.26775426 0.2676497
 0.26767573 0.26780227 0.2676241  0.26752096 0.26750845 0.26727682
 0.2673088  0.26746434 0.26732144 0.26737988 0.26782292 0.2681372
 0.2682228  0.26844007 0.26868266 0.26888362 0.26887104 0.26871112
 0.26879716 0.2688585  0.26874283 0.26887026 0.2689477  0.2687224
 0.26865488 0.26859483 0.2684254  0.26848322 0.26838765 0.2682213
 0.26828948 0.26818508 0.2680892  0.26822758 0.26823094 0.26846197
 0.26899806 0.2688041  0.2688109  0.26886624 0.26859146 0.26855624
 0.26874554 0.26865774 0.2686224  0.26863858 0.26847634 0.2685625
 0.26858637 0.26817784 0.2681736  0.26833722 0.26797184 0.26783532
 0.2678877  0.2675929  0.26751444 0.26763    0.26750132 0.26763117
 0.26814902 0.2681861  0.268297   0.2683181  0.2683604  0.26864213
 0.2685758  0.2683766  0.26853213 0.2684978  0.26829407 0.26835436
 0.26821992 0.26793793 0.26788527 0.26758015 0.26720724 0.26723322
 0.26713553 0.267004   0.26707047 0.26699325 0.2669894  0.26723793
 0.2673875  0.26752478 0.26777163 0.2678296  0.26799655 0.2680118
 0.2677417  0.26781094 0.26788732 0.26753026 0.2673053  0.26725504
 0.26709563 0.26704985 0.26695284 0.26665947 0.2666447  0.26658723
 0.26623768 0.26626918 0.26643047 0.2663425  0.26638052 0.2664604
 0.26651958 0.26671347 0.26675135 0.2669173  0.26724845 0.26707226
 0.26688966 0.26697376 0.26663825 0.26630405 0.26628095 0.26607785
 0.26596248 0.26599303 0.26573586 0.26551193 0.26545092 0.2652151
 0.26516324 0.2653118  0.26522228 0.2652788  0.26544738 0.26543862
 0.2655924  0.2657229  0.26562908 0.26585466 0.26599175 0.26569465
 0.26581877 0.26590824 0.26548737 0.26529968 0.2651318  0.26484776
 0.26485303 0.26465377 0.26448593 0.26481465 0.2648346  0.26462185
 0.264888   0.26500285 0.26486254 0.26501966 0.26512638 0.26519585
 0.2654507  0.26561734 0.26571518 0.26601347 0.26590014 0.26560327
 0.2655513  0.26528835 0.26520488 0.26517126 0.2646054  0.2645002
 0.2647166  0.26435754 0.2643573  0.26458117 0.26434138 0.26453474
 0.26486644 0.2647643  0.26509854 0.26558542 0.26572064 0.2663062
 0.26686087 0.26685387 0.26707667 0.26712722 0.26680973 0.26696846
 0.2668068  0.2662558  0.2663355  0.2660337  0.26540023 0.26553968
 0.2654187  0.26526028 0.26577654 0.26569858 0.26563767 0.26630735
 0.26638186 0.26649302 0.26715338 0.26702982 0.26669535 0.26759848
 0.26858616 0.26834714 0.26826048 0.26790974 0.26765177 0.2675312
 0.26684028 0.2665968  0.26661187 0.26590702 0.26591456 0.26617327
 0.26583856 0.2664334  0.26681992 0.26657084 0.2675276  0.26801142
 0.26773623 0.2685995  0.26894167 0.2683258  0.2684557  0.26869056
 0.26879892 0.2689628  0.2688566  0.2684756  0.26816952 0.26710787
 0.2665525  0.26676008 0.26570708 0.26572567 0.26668566 0.26623604
 0.26764685 0.26906246 0.2685735  0.27060732 0.2713379  0.26938102
 0.27053177 0.26801193 0.2631733  0.2651014  0.25656536 0.2702532 ]
