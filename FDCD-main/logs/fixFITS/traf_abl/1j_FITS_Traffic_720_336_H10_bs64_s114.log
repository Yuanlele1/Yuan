Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16559226880.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 190.59798192977905
Epoch: 1, Steps: 87 | Train Loss: 0.7964075 Vali Loss: 0.6942280 Test Loss: 0.7998554
Validation loss decreased (inf --> 0.694228).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 142.36748027801514
Epoch: 2, Steps: 87 | Train Loss: 0.4510764 Vali Loss: 0.4954154 Test Loss: 0.5750287
Validation loss decreased (0.694228 --> 0.495415).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 118.9951183795929
Epoch: 3, Steps: 87 | Train Loss: 0.3348819 Vali Loss: 0.4077104 Test Loss: 0.4797657
Validation loss decreased (0.495415 --> 0.407710).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 98.82004141807556
Epoch: 4, Steps: 87 | Train Loss: 0.2844705 Vali Loss: 0.3702969 Test Loss: 0.4390370
Validation loss decreased (0.407710 --> 0.370297).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 115.27996516227722
Epoch: 5, Steps: 87 | Train Loss: 0.2631204 Vali Loss: 0.3536645 Test Loss: 0.4232580
Validation loss decreased (0.370297 --> 0.353664).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 112.32667946815491
Epoch: 6, Steps: 87 | Train Loss: 0.2543903 Vali Loss: 0.3473550 Test Loss: 0.4174311
Validation loss decreased (0.353664 --> 0.347355).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 184.05151319503784
Epoch: 7, Steps: 87 | Train Loss: 0.2509312 Vali Loss: 0.3443252 Test Loss: 0.4151622
Validation loss decreased (0.347355 --> 0.344325).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 153.3977518081665
Epoch: 8, Steps: 87 | Train Loss: 0.2494771 Vali Loss: 0.3430964 Test Loss: 0.4146166
Validation loss decreased (0.344325 --> 0.343096).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 144.88126420974731
Epoch: 9, Steps: 87 | Train Loss: 0.2487837 Vali Loss: 0.3420026 Test Loss: 0.4139658
Validation loss decreased (0.343096 --> 0.342003).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 115.11997532844543
Epoch: 10, Steps: 87 | Train Loss: 0.2484616 Vali Loss: 0.3413159 Test Loss: 0.4137368
Validation loss decreased (0.342003 --> 0.341316).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 100.0562150478363
Epoch: 11, Steps: 87 | Train Loss: 0.2483375 Vali Loss: 0.3412708 Test Loss: 0.4136856
Validation loss decreased (0.341316 --> 0.341271).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 107.87301683425903
Epoch: 12, Steps: 87 | Train Loss: 0.2481166 Vali Loss: 0.3409836 Test Loss: 0.4135588
Validation loss decreased (0.341271 --> 0.340984).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 118.43030548095703
Epoch: 13, Steps: 87 | Train Loss: 0.2479947 Vali Loss: 0.3407952 Test Loss: 0.4132506
Validation loss decreased (0.340984 --> 0.340795).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 108.32355952262878
Epoch: 14, Steps: 87 | Train Loss: 0.2479041 Vali Loss: 0.3401939 Test Loss: 0.4133377
Validation loss decreased (0.340795 --> 0.340194).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 112.86539459228516
Epoch: 15, Steps: 87 | Train Loss: 0.2478781 Vali Loss: 0.3403396 Test Loss: 0.4131581
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 152.18999648094177
Epoch: 16, Steps: 87 | Train Loss: 0.2477794 Vali Loss: 0.3401717 Test Loss: 0.4131616
Validation loss decreased (0.340194 --> 0.340172).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 109.53464317321777
Epoch: 17, Steps: 87 | Train Loss: 0.2477625 Vali Loss: 0.3398911 Test Loss: 0.4130391
Validation loss decreased (0.340172 --> 0.339891).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 111.65007400512695
Epoch: 18, Steps: 87 | Train Loss: 0.2476612 Vali Loss: 0.3401456 Test Loss: 0.4132132
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 165.69840121269226
Epoch: 19, Steps: 87 | Train Loss: 0.2477094 Vali Loss: 0.3397559 Test Loss: 0.4131813
Validation loss decreased (0.339891 --> 0.339756).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 143.27271628379822
Epoch: 20, Steps: 87 | Train Loss: 0.2476856 Vali Loss: 0.3398758 Test Loss: 0.4129715
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 116.72870635986328
Epoch: 21, Steps: 87 | Train Loss: 0.2475724 Vali Loss: 0.3394703 Test Loss: 0.4129268
Validation loss decreased (0.339756 --> 0.339470).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 112.31928372383118
Epoch: 22, Steps: 87 | Train Loss: 0.2475740 Vali Loss: 0.3396489 Test Loss: 0.4127039
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 108.22231984138489
Epoch: 23, Steps: 87 | Train Loss: 0.2475149 Vali Loss: 0.3397006 Test Loss: 0.4129169
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 109.38835978507996
Epoch: 24, Steps: 87 | Train Loss: 0.2474868 Vali Loss: 0.3395779 Test Loss: 0.4127724
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 108.89648699760437
Epoch: 25, Steps: 87 | Train Loss: 0.2475153 Vali Loss: 0.3399472 Test Loss: 0.4127879
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 105.92367839813232
Epoch: 26, Steps: 87 | Train Loss: 0.2475018 Vali Loss: 0.3393018 Test Loss: 0.4128769
Validation loss decreased (0.339470 --> 0.339302).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 124.893394947052
Epoch: 27, Steps: 87 | Train Loss: 0.2474765 Vali Loss: 0.3391834 Test Loss: 0.4128489
Validation loss decreased (0.339302 --> 0.339183).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 139.82616424560547
Epoch: 28, Steps: 87 | Train Loss: 0.2475370 Vali Loss: 0.3399389 Test Loss: 0.4129221
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 91.06818270683289
Epoch: 29, Steps: 87 | Train Loss: 0.2474700 Vali Loss: 0.3394624 Test Loss: 0.4127702
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 153.86706113815308
Epoch: 30, Steps: 87 | Train Loss: 0.2474543 Vali Loss: 0.3396021 Test Loss: 0.4127125
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 167.74167847633362
Epoch: 31, Steps: 87 | Train Loss: 0.2473779 Vali Loss: 0.3397510 Test Loss: 0.4127609
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 162.74611043930054
Epoch: 32, Steps: 87 | Train Loss: 0.2473611 Vali Loss: 0.3395307 Test Loss: 0.4127532
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 179.62684273719788
Epoch: 33, Steps: 87 | Train Loss: 0.2474231 Vali Loss: 0.3392682 Test Loss: 0.4126585
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 166.9879171848297
Epoch: 34, Steps: 87 | Train Loss: 0.2473588 Vali Loss: 0.3395516 Test Loss: 0.4126821
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 194.7996666431427
Epoch: 35, Steps: 87 | Train Loss: 0.2473275 Vali Loss: 0.3397606 Test Loss: 0.4126877
EarlyStopping counter: 8 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 162.84688520431519
Epoch: 36, Steps: 87 | Train Loss: 0.2472788 Vali Loss: 0.3391630 Test Loss: 0.4127557
Validation loss decreased (0.339183 --> 0.339163).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 150.81299686431885
Epoch: 37, Steps: 87 | Train Loss: 0.2473532 Vali Loss: 0.3397082 Test Loss: 0.4126626
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 147.2231822013855
Epoch: 38, Steps: 87 | Train Loss: 0.2473379 Vali Loss: 0.3393642 Test Loss: 0.4127405
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 188.46705794334412
Epoch: 39, Steps: 87 | Train Loss: 0.2472503 Vali Loss: 0.3392758 Test Loss: 0.4126782
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 220.90629482269287
Epoch: 40, Steps: 87 | Train Loss: 0.2472584 Vali Loss: 0.3394971 Test Loss: 0.4126333
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 203.93216395378113
Epoch: 41, Steps: 87 | Train Loss: 0.2473021 Vali Loss: 0.3392783 Test Loss: 0.4125789
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 222.94840931892395
Epoch: 42, Steps: 87 | Train Loss: 0.2473282 Vali Loss: 0.3391952 Test Loss: 0.4126130
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 177.7868721485138
Epoch: 43, Steps: 87 | Train Loss: 0.2473123 Vali Loss: 0.3391206 Test Loss: 0.4126100
Validation loss decreased (0.339163 --> 0.339121).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 132.45315313339233
Epoch: 44, Steps: 87 | Train Loss: 0.2472362 Vali Loss: 0.3391382 Test Loss: 0.4126310
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 130.51284313201904
Epoch: 45, Steps: 87 | Train Loss: 0.2471388 Vali Loss: 0.3391034 Test Loss: 0.4126500
Validation loss decreased (0.339121 --> 0.339103).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 140.28898882865906
Epoch: 46, Steps: 87 | Train Loss: 0.2472378 Vali Loss: 0.3391263 Test Loss: 0.4126210
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 138.4093053340912
Epoch: 47, Steps: 87 | Train Loss: 0.2472974 Vali Loss: 0.3390372 Test Loss: 0.4126040
Validation loss decreased (0.339103 --> 0.339037).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 134.82761979103088
Epoch: 48, Steps: 87 | Train Loss: 0.2471912 Vali Loss: 0.3391649 Test Loss: 0.4126357
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 132.36758279800415
Epoch: 49, Steps: 87 | Train Loss: 0.2472441 Vali Loss: 0.3391512 Test Loss: 0.4125942
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 143.3515214920044
Epoch: 50, Steps: 87 | Train Loss: 0.2472423 Vali Loss: 0.3393432 Test Loss: 0.4125336
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 133.6037290096283
Epoch: 51, Steps: 87 | Train Loss: 0.2472472 Vali Loss: 0.3396258 Test Loss: 0.4126343
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 134.66512513160706
Epoch: 52, Steps: 87 | Train Loss: 0.2472002 Vali Loss: 0.3392065 Test Loss: 0.4126486
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 130.9158878326416
Epoch: 53, Steps: 87 | Train Loss: 0.2471289 Vali Loss: 0.3389491 Test Loss: 0.4126509
Validation loss decreased (0.339037 --> 0.338949).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 132.28604459762573
Epoch: 54, Steps: 87 | Train Loss: 0.2471684 Vali Loss: 0.3396639 Test Loss: 0.4126236
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 130.01209688186646
Epoch: 55, Steps: 87 | Train Loss: 0.2471757 Vali Loss: 0.3391253 Test Loss: 0.4126231
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 139.4242284297943
Epoch: 56, Steps: 87 | Train Loss: 0.2471849 Vali Loss: 0.3389987 Test Loss: 0.4126491
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 176.43576884269714
Epoch: 57, Steps: 87 | Train Loss: 0.2471769 Vali Loss: 0.3394096 Test Loss: 0.4125307
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 127.97975087165833
Epoch: 58, Steps: 87 | Train Loss: 0.2471954 Vali Loss: 0.3391298 Test Loss: 0.4126017
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 120.56659984588623
Epoch: 59, Steps: 87 | Train Loss: 0.2471443 Vali Loss: 0.3392301 Test Loss: 0.4125576
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 149.68037366867065
Epoch: 60, Steps: 87 | Train Loss: 0.2472300 Vali Loss: 0.3391410 Test Loss: 0.4125791
EarlyStopping counter: 7 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 122.41440892219543
Epoch: 61, Steps: 87 | Train Loss: 0.2471204 Vali Loss: 0.3391456 Test Loss: 0.4125391
EarlyStopping counter: 8 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 123.14343905448914
Epoch: 62, Steps: 87 | Train Loss: 0.2471691 Vali Loss: 0.3392152 Test Loss: 0.4125612
EarlyStopping counter: 9 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 118.71031165122986
Epoch: 63, Steps: 87 | Train Loss: 0.2471478 Vali Loss: 0.3392344 Test Loss: 0.4125517
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.41049960255622864, mae:0.27857506275177, rse:0.5265694260597229, corr:[0.2697783  0.28425318 0.28254825 0.28382245 0.2838715  0.28446203
 0.28510475 0.28469566 0.28494343 0.28430632 0.2846424  0.28414294
 0.2837266  0.2837939  0.28338242 0.28346422 0.28326    0.28336713
 0.28360733 0.28367743 0.283764   0.2836059  0.28349352 0.28349197
 0.2849678  0.28521118 0.2849839  0.28497565 0.2846441  0.28444698
 0.28448167 0.2843285  0.28441638 0.28414965 0.28391975 0.2838796
 0.28379855 0.28394896 0.28400603 0.28417006 0.28423402 0.28432766
 0.28445548 0.28440088 0.28433397 0.2841984  0.28429276 0.28439054
 0.28479737 0.28496334 0.28477427 0.28459474 0.28433928 0.28393242
 0.28397968 0.28413746 0.2841125  0.28411013 0.28410605 0.28407994
 0.2840484  0.28423706 0.2843739  0.28444755 0.28455317 0.2843257
 0.28431883 0.2842471  0.28405976 0.28403878 0.284055   0.28412953
 0.28400108 0.28388116 0.28376898 0.28362754 0.2836019  0.28350538
 0.28366563 0.28380167 0.28367335 0.2836473  0.28366345 0.2837384
 0.2838362  0.28375658 0.2838287  0.28402925 0.28415245 0.28420275
 0.2842232  0.28406456 0.28373337 0.28358155 0.2836609  0.2838085
 0.28354132 0.28329563 0.28341025 0.28355676 0.28349137 0.28338492
 0.28334627 0.28344196 0.28348696 0.28349152 0.28351066 0.28350002
 0.28349137 0.2834812  0.28354716 0.28374285 0.2837858  0.283632
 0.28369245 0.28371048 0.2835267  0.2833611  0.28345212 0.2836021
 0.28350124 0.28365412 0.28375176 0.28378013 0.28377968 0.28357863
 0.2835981  0.28381932 0.28375673 0.28374907 0.28395912 0.2840548
 0.28406477 0.28406957 0.2841015  0.28411093 0.28417572 0.28422877
 0.2840156  0.2839523  0.28395167 0.2838697  0.28398016 0.28397945
 0.28422225 0.28434518 0.28414485 0.28405806 0.28409883 0.284138
 0.28427607 0.2844136  0.28434104 0.28421256 0.28418186 0.2842606
 0.28431052 0.2843683  0.28450802 0.28451723 0.28431985 0.28426585
 0.28428346 0.28411707 0.28406802 0.2839026  0.283716   0.28397325
 0.28528476 0.28549048 0.28539696 0.28530017 0.28521413 0.2852972
 0.2853154  0.28529054 0.2853952  0.2852864  0.2851612  0.28528705
 0.28530732 0.28512055 0.28497592 0.28490248 0.28484344 0.28474694
 0.28469828 0.2846958  0.2845395  0.28433028 0.2842317  0.2843585
 0.28515407 0.28520414 0.28511307 0.28503695 0.28493842 0.2848861
 0.28485915 0.2846963  0.2845335  0.28449833 0.28453472 0.28463408
 0.28466552 0.28448737 0.28434172 0.28430745 0.28420419 0.28409338
 0.28396484 0.28388175 0.28366852 0.2835132  0.2835695  0.28356543
 0.2838568  0.28395927 0.28407517 0.28424868 0.28414017 0.2840908
 0.28417873 0.28419307 0.28417796 0.28416747 0.28399366 0.2837675
 0.28380513 0.28382087 0.28388414 0.28397322 0.28380603 0.2837487
 0.28363088 0.28341448 0.28349975 0.2833983  0.28328133 0.28340274
 0.28362978 0.28378874 0.28371638 0.2838119  0.28384966 0.2837439
 0.28381184 0.28362122 0.28355166 0.28361788 0.2835026  0.28340438
 0.2832753  0.2832728  0.2832672  0.2832248  0.2832489  0.28315789
 0.2830762  0.28296843 0.28296292 0.28306815 0.28306794 0.28311872
 0.28302962 0.28308955 0.28315502 0.2832434  0.2834198  0.2833202
 0.2834238  0.28337103 0.2830428  0.28309345 0.28282908 0.28255224
 0.2825957  0.28252143 0.28251448 0.28247187 0.28250915 0.28247553
 0.28233746 0.28238752 0.28238145 0.28248936 0.2827199  0.28268927
 0.28265715 0.2830621  0.28314295 0.28323504 0.2835038  0.28330106
 0.28303406 0.2829403  0.2826268  0.2826127  0.28258753 0.2825061
 0.28273124 0.28271326 0.28286952 0.2827854  0.28284276 0.2832987
 0.2831387  0.28339016 0.28336626 0.28316393 0.28349546 0.2831367
 0.2833373  0.2834956  0.2833662  0.28336626 0.2832788  0.2833373
 0.28290802 0.28277463 0.2825987  0.2823283  0.28279993 0.28278103
 0.2833708  0.28371212 0.2837146  0.28427517 0.28394032 0.28462645
 0.28374618 0.28327054 0.2824579  0.28130051 0.2826748  0.2843789 ]
