Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j96_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=96, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1661
test 3413
Model(
  (freq_upsampler): Linear(in_features=320, out_features=362, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  12781322240.0
params:  116202.0
Trainable parameters:  116202
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 142.72394251823425
Epoch: 1, Steps: 89 | Train Loss: 0.5814861 Vali Loss: 0.4210800 Test Loss: 0.4909311
Validation loss decreased (inf --> 0.421080).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 93.76256489753723
Epoch: 2, Steps: 89 | Train Loss: 0.2622747 Vali Loss: 0.3339008 Test Loss: 0.3994757
Validation loss decreased (0.421080 --> 0.333901).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 120.54731011390686
Epoch: 3, Steps: 89 | Train Loss: 0.2346836 Vali Loss: 0.3294279 Test Loss: 0.3931476
Validation loss decreased (0.333901 --> 0.329428).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 101.82382607460022
Epoch: 4, Steps: 89 | Train Loss: 0.2324134 Vali Loss: 0.3273665 Test Loss: 0.3921518
Validation loss decreased (0.329428 --> 0.327366).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 87.96023631095886
Epoch: 5, Steps: 89 | Train Loss: 0.2319767 Vali Loss: 0.3269015 Test Loss: 0.3912126
Validation loss decreased (0.327366 --> 0.326902).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 115.52872967720032
Epoch: 6, Steps: 89 | Train Loss: 0.2317064 Vali Loss: 0.3250308 Test Loss: 0.3909990
Validation loss decreased (0.326902 --> 0.325031).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 101.69725251197815
Epoch: 7, Steps: 89 | Train Loss: 0.2316379 Vali Loss: 0.3256717 Test Loss: 0.3903193
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 88.57687854766846
Epoch: 8, Steps: 89 | Train Loss: 0.2314621 Vali Loss: 0.3251551 Test Loss: 0.3900631
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 113.54036903381348
Epoch: 9, Steps: 89 | Train Loss: 0.2312230 Vali Loss: 0.3251684 Test Loss: 0.3899754
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 106.06877374649048
Epoch: 10, Steps: 89 | Train Loss: 0.2312491 Vali Loss: 0.3259025 Test Loss: 0.3897969
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 90.59834265708923
Epoch: 11, Steps: 89 | Train Loss: 0.2311441 Vali Loss: 0.3256373 Test Loss: 0.3898475
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 79.46813297271729
Epoch: 12, Steps: 89 | Train Loss: 0.2311034 Vali Loss: 0.3255497 Test Loss: 0.3897294
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 99.44641828536987
Epoch: 13, Steps: 89 | Train Loss: 0.2309809 Vali Loss: 0.3249656 Test Loss: 0.3900530
Validation loss decreased (0.325031 --> 0.324966).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 104.09890508651733
Epoch: 14, Steps: 89 | Train Loss: 0.2309715 Vali Loss: 0.3240261 Test Loss: 0.3897083
Validation loss decreased (0.324966 --> 0.324026).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 115.25345253944397
Epoch: 15, Steps: 89 | Train Loss: 0.2308349 Vali Loss: 0.3242977 Test Loss: 0.3894209
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 97.64505791664124
Epoch: 16, Steps: 89 | Train Loss: 0.2309037 Vali Loss: 0.3252003 Test Loss: 0.3894749
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 97.32458281517029
Epoch: 17, Steps: 89 | Train Loss: 0.2306785 Vali Loss: 0.3255705 Test Loss: 0.3896269
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 95.78719472885132
Epoch: 18, Steps: 89 | Train Loss: 0.2306372 Vali Loss: 0.3251597 Test Loss: 0.3893766
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 86.3101372718811
Epoch: 19, Steps: 89 | Train Loss: 0.2306753 Vali Loss: 0.3251027 Test Loss: 0.3891779
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 86.2848162651062
Epoch: 20, Steps: 89 | Train Loss: 0.2306950 Vali Loss: 0.3245645 Test Loss: 0.3889742
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 93.25650882720947
Epoch: 21, Steps: 89 | Train Loss: 0.2305492 Vali Loss: 0.3236336 Test Loss: 0.3892130
Validation loss decreased (0.324026 --> 0.323634).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 84.02417612075806
Epoch: 22, Steps: 89 | Train Loss: 0.2305685 Vali Loss: 0.3249078 Test Loss: 0.3889365
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 91.39359402656555
Epoch: 23, Steps: 89 | Train Loss: 0.2304206 Vali Loss: 0.3252477 Test Loss: 0.3890690
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 90.2055287361145
Epoch: 24, Steps: 89 | Train Loss: 0.2304666 Vali Loss: 0.3249559 Test Loss: 0.3890642
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 79.1669065952301
Epoch: 25, Steps: 89 | Train Loss: 0.2304298 Vali Loss: 0.3237076 Test Loss: 0.3888909
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 93.57615184783936
Epoch: 26, Steps: 89 | Train Loss: 0.2303950 Vali Loss: 0.3240103 Test Loss: 0.3891586
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 81.72793650627136
Epoch: 27, Steps: 89 | Train Loss: 0.2304983 Vali Loss: 0.3228284 Test Loss: 0.3890455
Validation loss decreased (0.323634 --> 0.322828).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 89.86873364448547
Epoch: 28, Steps: 89 | Train Loss: 0.2303460 Vali Loss: 0.3238509 Test Loss: 0.3889222
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 92.67093896865845
Epoch: 29, Steps: 89 | Train Loss: 0.2304174 Vali Loss: 0.3237567 Test Loss: 0.3885811
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 88.81597828865051
Epoch: 30, Steps: 89 | Train Loss: 0.2304271 Vali Loss: 0.3245772 Test Loss: 0.3889556
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 86.31250810623169
Epoch: 31, Steps: 89 | Train Loss: 0.2302324 Vali Loss: 0.3238238 Test Loss: 0.3885858
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 91.3801622390747
Epoch: 32, Steps: 89 | Train Loss: 0.2302813 Vali Loss: 0.3241567 Test Loss: 0.3888329
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 93.29727005958557
Epoch: 33, Steps: 89 | Train Loss: 0.2303722 Vali Loss: 0.3248312 Test Loss: 0.3887925
EarlyStopping counter: 6 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 92.84997487068176
Epoch: 34, Steps: 89 | Train Loss: 0.2303434 Vali Loss: 0.3234052 Test Loss: 0.3890391
EarlyStopping counter: 7 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 87.59352016448975
Epoch: 35, Steps: 89 | Train Loss: 0.2302465 Vali Loss: 0.3234675 Test Loss: 0.3888065
EarlyStopping counter: 8 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 88.96397519111633
Epoch: 36, Steps: 89 | Train Loss: 0.2303155 Vali Loss: 0.3238454 Test Loss: 0.3888935
EarlyStopping counter: 9 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 93.81699991226196
Epoch: 37, Steps: 89 | Train Loss: 0.2302864 Vali Loss: 0.3227915 Test Loss: 0.3886674
Validation loss decreased (0.322828 --> 0.322792).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 87.60486006736755
Epoch: 38, Steps: 89 | Train Loss: 0.2302049 Vali Loss: 0.3228338 Test Loss: 0.3888176
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 81.13795232772827
Epoch: 39, Steps: 89 | Train Loss: 0.2301872 Vali Loss: 0.3247676 Test Loss: 0.3887054
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 69.33846282958984
Epoch: 40, Steps: 89 | Train Loss: 0.2302673 Vali Loss: 0.3232453 Test Loss: 0.3887354
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 96.22687673568726
Epoch: 41, Steps: 89 | Train Loss: 0.2302015 Vali Loss: 0.3241615 Test Loss: 0.3887551
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 132.67331886291504
Epoch: 42, Steps: 89 | Train Loss: 0.2302470 Vali Loss: 0.3244089 Test Loss: 0.3887537
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 184.2029116153717
Epoch: 43, Steps: 89 | Train Loss: 0.2301763 Vali Loss: 0.3232689 Test Loss: 0.3887123
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 148.32129335403442
Epoch: 44, Steps: 89 | Train Loss: 0.2301865 Vali Loss: 0.3239244 Test Loss: 0.3886651
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 137.3909773826599
Epoch: 45, Steps: 89 | Train Loss: 0.2301150 Vali Loss: 0.3251087 Test Loss: 0.3885462
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 185.8085753917694
Epoch: 46, Steps: 89 | Train Loss: 0.2300536 Vali Loss: 0.3235322 Test Loss: 0.3886826
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 138.06995463371277
Epoch: 47, Steps: 89 | Train Loss: 0.2300022 Vali Loss: 0.3240763 Test Loss: 0.3886734
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j96_H10_FITS_custom_ftM_sl720_ll48_pl96_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
mse:0.38576334714889526, mae:0.2685497999191284, rse:0.5142969489097595, corr:[0.2761696  0.29280764 0.2918663  0.29248095 0.29212037 0.29249838
 0.29284346 0.29269907 0.29266936 0.29231855 0.2928769  0.29264277
 0.29289126 0.29249346 0.29188704 0.292166   0.2918204  0.29182658
 0.29205814 0.29220235 0.29209486 0.2918115  0.2917291  0.29160416
 0.2933478  0.2938797  0.2935495  0.2932627  0.29290614 0.29281157
 0.29279023 0.2928077  0.2926507  0.29242167 0.29253286 0.29226395
 0.29222244 0.29223377 0.29220226 0.2922256  0.29208174 0.29257688
 0.2927037  0.29257366 0.29254824 0.2923113  0.2921683  0.2917603
 0.29229844 0.29258814 0.29207733 0.29215845 0.29235408 0.2923532
 0.2921611  0.29176855 0.29189825 0.29181787 0.2919313  0.2921013
 0.29205504 0.29235163 0.292518   0.29278186 0.29259437 0.29237953
 0.29239002 0.29230082 0.29248044 0.2922917  0.2922766  0.29212746
 0.29179022 0.29190874 0.29148617 0.2914435  0.29145995 0.29143873
 0.2916233  0.29116246 0.29107937 0.29083508 0.29132232 0.2917932
 0.29150102 0.2917594  0.29161432 0.29226214 0.29185662 0.29163992
 0.2916539  0.29073587 0.2908363  0.28957778 0.2913449  0.29139605]
