Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=320, out_features=405, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14299545600.0
params:  130005.0
Trainable parameters:  130005
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 110.99972081184387
Epoch: 1, Steps: 88 | Train Loss: 0.7650930 Vali Loss: 0.6210801 Test Loss: 0.7114627
Validation loss decreased (inf --> 0.621080).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 93.1868646144867
Epoch: 2, Steps: 88 | Train Loss: 0.3832807 Vali Loss: 0.4140675 Test Loss: 0.4842838
Validation loss decreased (0.621080 --> 0.414067).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 105.74533343315125
Epoch: 3, Steps: 88 | Train Loss: 0.2780552 Vali Loss: 0.3527867 Test Loss: 0.4202823
Validation loss decreased (0.414067 --> 0.352787).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 168.70304417610168
Epoch: 4, Steps: 88 | Train Loss: 0.2486451 Vali Loss: 0.3364837 Test Loss: 0.4048944
Validation loss decreased (0.352787 --> 0.336484).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 107.50041365623474
Epoch: 5, Steps: 88 | Train Loss: 0.2413256 Vali Loss: 0.3322016 Test Loss: 0.4014718
Validation loss decreased (0.336484 --> 0.332202).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 135.0256109237671
Epoch: 6, Steps: 88 | Train Loss: 0.2393368 Vali Loss: 0.3303794 Test Loss: 0.4003264
Validation loss decreased (0.332202 --> 0.330379).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 92.15684342384338
Epoch: 7, Steps: 88 | Train Loss: 0.2386087 Vali Loss: 0.3298269 Test Loss: 0.3998908
Validation loss decreased (0.330379 --> 0.329827).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 104.06991457939148
Epoch: 8, Steps: 88 | Train Loss: 0.2382587 Vali Loss: 0.3293386 Test Loss: 0.3998444
Validation loss decreased (0.329827 --> 0.329339).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 88.56428956985474
Epoch: 9, Steps: 88 | Train Loss: 0.2379727 Vali Loss: 0.3289347 Test Loss: 0.3993053
Validation loss decreased (0.329339 --> 0.328935).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 128.36015057563782
Epoch: 10, Steps: 88 | Train Loss: 0.2378464 Vali Loss: 0.3284769 Test Loss: 0.3992384
Validation loss decreased (0.328935 --> 0.328477).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 93.81173133850098
Epoch: 11, Steps: 88 | Train Loss: 0.2377803 Vali Loss: 0.3285660 Test Loss: 0.3991547
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 94.10902810096741
Epoch: 12, Steps: 88 | Train Loss: 0.2375997 Vali Loss: 0.3275823 Test Loss: 0.3987817
Validation loss decreased (0.328477 --> 0.327582).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 96.45105934143066
Epoch: 13, Steps: 88 | Train Loss: 0.2375380 Vali Loss: 0.3284574 Test Loss: 0.3988953
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 88.7045226097107
Epoch: 14, Steps: 88 | Train Loss: 0.2375046 Vali Loss: 0.3280170 Test Loss: 0.3988211
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 99.36510372161865
Epoch: 15, Steps: 88 | Train Loss: 0.2372694 Vali Loss: 0.3277799 Test Loss: 0.3984676
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 105.74355721473694
Epoch: 16, Steps: 88 | Train Loss: 0.2373646 Vali Loss: 0.3279714 Test Loss: 0.3985915
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 95.82191371917725
Epoch: 17, Steps: 88 | Train Loss: 0.2374551 Vali Loss: 0.3275899 Test Loss: 0.3986707
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 145.23016214370728
Epoch: 18, Steps: 88 | Train Loss: 0.2372845 Vali Loss: 0.3276170 Test Loss: 0.3986673
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 107.1471734046936
Epoch: 19, Steps: 88 | Train Loss: 0.2372847 Vali Loss: 0.3280762 Test Loss: 0.3985843
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 103.97609043121338
Epoch: 20, Steps: 88 | Train Loss: 0.2371736 Vali Loss: 0.3275447 Test Loss: 0.3983713
Validation loss decreased (0.327582 --> 0.327545).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 130.81924748420715
Epoch: 21, Steps: 88 | Train Loss: 0.2370548 Vali Loss: 0.3271056 Test Loss: 0.3983524
Validation loss decreased (0.327545 --> 0.327106).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 102.8738043308258
Epoch: 22, Steps: 88 | Train Loss: 0.2371056 Vali Loss: 0.3272915 Test Loss: 0.3984385
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 111.39869117736816
Epoch: 23, Steps: 88 | Train Loss: 0.2370223 Vali Loss: 0.3270931 Test Loss: 0.3982524
Validation loss decreased (0.327106 --> 0.327093).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 99.10343289375305
Epoch: 24, Steps: 88 | Train Loss: 0.2371007 Vali Loss: 0.3274604 Test Loss: 0.3985050
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 143.42956805229187
Epoch: 25, Steps: 88 | Train Loss: 0.2371288 Vali Loss: 0.3272772 Test Loss: 0.3981152
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 112.68726515769958
Epoch: 26, Steps: 88 | Train Loss: 0.2369979 Vali Loss: 0.3281309 Test Loss: 0.3983927
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 90.26709699630737
Epoch: 27, Steps: 88 | Train Loss: 0.2370684 Vali Loss: 0.3275402 Test Loss: 0.3983432
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 119.96137022972107
Epoch: 28, Steps: 88 | Train Loss: 0.2370442 Vali Loss: 0.3272639 Test Loss: 0.3983680
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 101.25041580200195
Epoch: 29, Steps: 88 | Train Loss: 0.2369203 Vali Loss: 0.3272056 Test Loss: 0.3982173
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 108.74897623062134
Epoch: 30, Steps: 88 | Train Loss: 0.2369519 Vali Loss: 0.3276366 Test Loss: 0.3981838
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 123.55881142616272
Epoch: 31, Steps: 88 | Train Loss: 0.2368569 Vali Loss: 0.3276980 Test Loss: 0.3982581
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 95.98520636558533
Epoch: 32, Steps: 88 | Train Loss: 0.2369456 Vali Loss: 0.3277383 Test Loss: 0.3981459
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 96.69876718521118
Epoch: 33, Steps: 88 | Train Loss: 0.2368054 Vali Loss: 0.3272014 Test Loss: 0.3981838
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j192_H10_FITS_custom_ftM_sl720_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.39728572964668274, mae:0.27294638752937317, rse:0.5202121734619141, corr:[0.27540764 0.28962937 0.28771788 0.28873676 0.28913727 0.28924093
 0.29090387 0.2900761  0.2909401  0.29026532 0.29035994 0.2901699
 0.28915787 0.28969714 0.2894541  0.28964913 0.2891533  0.28878433
 0.28931448 0.28894818 0.28907722 0.28891772 0.28882164 0.28926888
 0.2907086  0.29145804 0.29083443 0.29030648 0.29024953 0.29024285
 0.29065105 0.29003686 0.29010233 0.29013178 0.28940925 0.28926152
 0.288892   0.28922224 0.28926152 0.28901944 0.28947303 0.28960088
 0.28974187 0.289399   0.28879315 0.28856888 0.28846526 0.28909943
 0.28993347 0.2902124  0.29010686 0.28961536 0.28996238 0.28984421
 0.2892656  0.2886867  0.28832707 0.28882006 0.28857058 0.28826508
 0.28840294 0.28826147 0.2884956  0.28864688 0.28878778 0.28871295
 0.28841677 0.28824204 0.28776833 0.28758025 0.28754807 0.287801
 0.28837973 0.2885434  0.28845948 0.28795835 0.28776804 0.28814754
 0.28804177 0.2879434  0.28769076 0.2877879  0.28839743 0.2881699
 0.28794834 0.28769884 0.28759775 0.28790274 0.28788334 0.28812027
 0.28811595 0.28794473 0.28813028 0.2878826  0.28750232 0.2876999
 0.28786555 0.28778836 0.2874561  0.2873457  0.2873593  0.28753692
 0.2876285  0.28762516 0.28761098 0.2871693  0.28713968 0.28707367
 0.28674036 0.28712034 0.28728154 0.28732947 0.28738734 0.28730595
 0.28739822 0.2870579  0.2869708  0.28675064 0.28661433 0.28739765
 0.28780535 0.2880631  0.2879161  0.2876837  0.28767166 0.2873168
 0.2873593  0.28733918 0.2872759  0.28753403 0.28734928 0.28737217
 0.2872234  0.28732666 0.2878027  0.28763884 0.28782707 0.28774327
 0.28753737 0.2875804  0.28732955 0.28765982 0.28767365 0.28766018
 0.28831655 0.2884781  0.2884309  0.28789204 0.28777492 0.28815052
 0.2881445  0.28821036 0.2877575  0.2881069  0.28827652 0.28744927
 0.28774932 0.28776625 0.288078   0.28814173 0.2875034  0.28784683
 0.28756362 0.2879258  0.28833994 0.2880812  0.2886546  0.2887503
 0.29005492 0.2901043  0.28976285 0.28928965 0.28842753 0.28916803
 0.28930384 0.28932306 0.28928033 0.28885472 0.28981093 0.2888984
 0.2889861  0.28947604 0.28944013 0.29036614 0.28902256 0.28978768
 0.28860173 0.287722   0.28675786 0.28546292 0.28798705 0.29092667]
