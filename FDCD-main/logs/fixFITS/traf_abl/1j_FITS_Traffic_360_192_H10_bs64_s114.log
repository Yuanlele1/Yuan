Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=170, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j192_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j192_H10_FITS_custom_ftM_sl360_ll48_pl192_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11729
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=170, out_features=260, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4876851200.0
params:  44460.0
Trainable parameters:  44460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 52.725178718566895
Epoch: 1, Steps: 91 | Train Loss: 0.9073266 Vali Loss: 0.7738411 Test Loss: 0.8966281
Validation loss decreased (inf --> 0.773841).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 49.57799029350281
Epoch: 2, Steps: 91 | Train Loss: 0.5133707 Vali Loss: 0.5437596 Test Loss: 0.6364298
Validation loss decreased (0.773841 --> 0.543760).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 51.07390832901001
Epoch: 3, Steps: 91 | Train Loss: 0.3738502 Vali Loss: 0.4345317 Test Loss: 0.5164099
Validation loss decreased (0.543760 --> 0.434532).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 48.099212884902954
Epoch: 4, Steps: 91 | Train Loss: 0.3075316 Vali Loss: 0.3831958 Test Loss: 0.4620307
Validation loss decreased (0.434532 --> 0.383196).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 51.02096104621887
Epoch: 5, Steps: 91 | Train Loss: 0.2771722 Vali Loss: 0.3601381 Test Loss: 0.4389580
Validation loss decreased (0.383196 --> 0.360138).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 62.951982736587524
Epoch: 6, Steps: 91 | Train Loss: 0.2638157 Vali Loss: 0.3498242 Test Loss: 0.4296645
Validation loss decreased (0.360138 --> 0.349824).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 55.23137426376343
Epoch: 7, Steps: 91 | Train Loss: 0.2581018 Vali Loss: 0.3443670 Test Loss: 0.4260990
Validation loss decreased (0.349824 --> 0.344367).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 51.777809619903564
Epoch: 8, Steps: 91 | Train Loss: 0.2555393 Vali Loss: 0.3430089 Test Loss: 0.4247546
Validation loss decreased (0.344367 --> 0.343009).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 65.96573543548584
Epoch: 9, Steps: 91 | Train Loss: 0.2544688 Vali Loss: 0.3418965 Test Loss: 0.4242728
Validation loss decreased (0.343009 --> 0.341897).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 53.19117093086243
Epoch: 10, Steps: 91 | Train Loss: 0.2537140 Vali Loss: 0.3412119 Test Loss: 0.4239564
Validation loss decreased (0.341897 --> 0.341212).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 48.43809914588928
Epoch: 11, Steps: 91 | Train Loss: 0.2534591 Vali Loss: 0.3407936 Test Loss: 0.4237559
Validation loss decreased (0.341212 --> 0.340794).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 50.25100064277649
Epoch: 12, Steps: 91 | Train Loss: 0.2532052 Vali Loss: 0.3406173 Test Loss: 0.4236794
Validation loss decreased (0.340794 --> 0.340617).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 67.92390489578247
Epoch: 13, Steps: 91 | Train Loss: 0.2530961 Vali Loss: 0.3407059 Test Loss: 0.4235747
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 73.3604302406311
Epoch: 14, Steps: 91 | Train Loss: 0.2530049 Vali Loss: 0.3404991 Test Loss: 0.4234157
Validation loss decreased (0.340617 --> 0.340499).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 74.26930546760559
Epoch: 15, Steps: 91 | Train Loss: 0.2528526 Vali Loss: 0.3402118 Test Loss: 0.4234036
Validation loss decreased (0.340499 --> 0.340212).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 71.99217319488525
Epoch: 16, Steps: 91 | Train Loss: 0.2527735 Vali Loss: 0.3402335 Test Loss: 0.4232773
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 72.64074921607971
Epoch: 17, Steps: 91 | Train Loss: 0.2527202 Vali Loss: 0.3395151 Test Loss: 0.4232063
Validation loss decreased (0.340212 --> 0.339515).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 83.11714768409729
Epoch: 18, Steps: 91 | Train Loss: 0.2527037 Vali Loss: 0.3395033 Test Loss: 0.4231466
Validation loss decreased (0.339515 --> 0.339503).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 121.19043922424316
Epoch: 19, Steps: 91 | Train Loss: 0.2525582 Vali Loss: 0.3397551 Test Loss: 0.4232478
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 132.95832085609436
Epoch: 20, Steps: 91 | Train Loss: 0.2525647 Vali Loss: 0.3397669 Test Loss: 0.4231042
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 93.66233587265015
Epoch: 21, Steps: 91 | Train Loss: 0.2526063 Vali Loss: 0.3396516 Test Loss: 0.4230482
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 73.84056377410889
Epoch: 22, Steps: 91 | Train Loss: 0.2524102 Vali Loss: 0.3399641 Test Loss: 0.4230316
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 81.12975144386292
Epoch: 23, Steps: 91 | Train Loss: 0.2524870 Vali Loss: 0.3396912 Test Loss: 0.4229942
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 81.18731427192688
Epoch: 24, Steps: 91 | Train Loss: 0.2524816 Vali Loss: 0.3397419 Test Loss: 0.4230047
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 131.2791130542755
Epoch: 25, Steps: 91 | Train Loss: 0.2524532 Vali Loss: 0.3398514 Test Loss: 0.4229119
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 83.93694639205933
Epoch: 26, Steps: 91 | Train Loss: 0.2524912 Vali Loss: 0.3396680 Test Loss: 0.4228988
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 75.23497128486633
Epoch: 27, Steps: 91 | Train Loss: 0.2524220 Vali Loss: 0.3394350 Test Loss: 0.4228449
Validation loss decreased (0.339503 --> 0.339435).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 79.98714780807495
Epoch: 28, Steps: 91 | Train Loss: 0.2523538 Vali Loss: 0.3396818 Test Loss: 0.4229003
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 81.36575269699097
Epoch: 29, Steps: 91 | Train Loss: 0.2524053 Vali Loss: 0.3396688 Test Loss: 0.4228152
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 79.62350249290466
Epoch: 30, Steps: 91 | Train Loss: 0.2522526 Vali Loss: 0.3394047 Test Loss: 0.4227947
Validation loss decreased (0.339435 --> 0.339405).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 128.91849899291992
Epoch: 31, Steps: 91 | Train Loss: 0.2522844 Vali Loss: 0.3392737 Test Loss: 0.4227759
Validation loss decreased (0.339405 --> 0.339274).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 86.76784062385559
Epoch: 32, Steps: 91 | Train Loss: 0.2523533 Vali Loss: 0.3394103 Test Loss: 0.4228047
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 73.89446210861206
Epoch: 33, Steps: 91 | Train Loss: 0.2522528 Vali Loss: 0.3390235 Test Loss: 0.4227338
Validation loss decreased (0.339274 --> 0.339024).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 100.47257494926453
Epoch: 34, Steps: 91 | Train Loss: 0.2523004 Vali Loss: 0.3396157 Test Loss: 0.4227456
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 99.01772212982178
Epoch: 35, Steps: 91 | Train Loss: 0.2522728 Vali Loss: 0.3391321 Test Loss: 0.4227466
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 73.09588098526001
Epoch: 36, Steps: 91 | Train Loss: 0.2521944 Vali Loss: 0.3396082 Test Loss: 0.4227082
EarlyStopping counter: 3 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 92.08560085296631
Epoch: 37, Steps: 91 | Train Loss: 0.2522418 Vali Loss: 0.3391601 Test Loss: 0.4227174
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 97.63670182228088
Epoch: 38, Steps: 91 | Train Loss: 0.2522006 Vali Loss: 0.3400072 Test Loss: 0.4227157
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 83.59470677375793
Epoch: 39, Steps: 91 | Train Loss: 0.2521298 Vali Loss: 0.3396675 Test Loss: 0.4227161
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 68.84987235069275
Epoch: 40, Steps: 91 | Train Loss: 0.2521709 Vali Loss: 0.3392661 Test Loss: 0.4227023
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 75.44627666473389
Epoch: 41, Steps: 91 | Train Loss: 0.2521799 Vali Loss: 0.3397714 Test Loss: 0.4227244
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 65.5203230381012
Epoch: 42, Steps: 91 | Train Loss: 0.2521775 Vali Loss: 0.3393249 Test Loss: 0.4226840
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 69.39719033241272
Epoch: 43, Steps: 91 | Train Loss: 0.2520845 Vali Loss: 0.3393898 Test Loss: 0.4226798
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j192_H10_FITS_custom_ftM_sl360_ll48_pl192_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.42170459032058716, mae:0.28371119499206543, rse:0.5359609723091125, corr:[0.27337646 0.2873146  0.2861876  0.28790084 0.28818995 0.28875908
 0.2899274  0.28913414 0.29015505 0.28874502 0.28956732 0.2882431
 0.28843004 0.28804338 0.28779852 0.28821406 0.2874422  0.28822687
 0.2874515  0.2880611  0.28797096 0.28779015 0.28754023 0.28736663
 0.28907314 0.2890763  0.28935128 0.28889915 0.28941333 0.28919926
 0.28889713 0.28907713 0.28823435 0.28855395 0.28795514 0.28808677
 0.2875516  0.28735882 0.28772268 0.28729483 0.287844   0.28742266
 0.2878778  0.28775382 0.28789783 0.28820217 0.2876802  0.28794327
 0.2881095  0.28843674 0.28799063 0.28804612 0.2882028  0.2879658
 0.28813908 0.28732714 0.2875433  0.28713086 0.28719723 0.28740448
 0.2871227  0.28723988 0.28695357 0.28751552 0.28708032 0.287233
 0.28720158 0.28691787 0.2871908  0.28681022 0.28703016 0.2869197
 0.28734085 0.28700474 0.28679207 0.28692952 0.28659806 0.2868101
 0.28622407 0.28652158 0.2862367  0.28612462 0.28639537 0.28618655
 0.2865262  0.28583422 0.28636155 0.28655353 0.2865162  0.28657934
 0.28605056 0.2862941  0.28580523 0.2861003  0.28600714 0.28613874
 0.28615957 0.28598294 0.28633    0.28578186 0.28605127 0.28581396
 0.28585827 0.28568587 0.28545985 0.28580377 0.28531832 0.28570947
 0.2855557  0.28570965 0.28574622 0.28559846 0.28571135 0.2851735
 0.2855682  0.2852511  0.2854679  0.28561628 0.28564712 0.286265
 0.28587347 0.28601566 0.28566995 0.2860522  0.2857265  0.2855381
 0.28581005 0.28520736 0.2856127  0.2852661  0.285408   0.28511584
 0.28516975 0.285634   0.2854649  0.2860736  0.2857539  0.28596273
 0.28588736 0.28611282 0.28635842 0.28591004 0.28655958 0.28672373
 0.2871321  0.28686997 0.28719246 0.2875471  0.28718376 0.28729805
 0.286604   0.28709716 0.2868121  0.28707093 0.28704077 0.28671005
 0.28722072 0.28692394 0.28781828 0.28730434 0.28762755 0.28751287
 0.28716516 0.2878904  0.28748876 0.2879607  0.287203   0.2878409
 0.28892708 0.28844592 0.28828594 0.28788668 0.28831562 0.28743017
 0.28805077 0.28697968 0.28750467 0.28785804 0.28785467 0.2886675
 0.28770092 0.2894036  0.28830007 0.28975633 0.288411   0.28841895
 0.28781804 0.28632388 0.28648832 0.28372103 0.28657216 0.28923425]
