Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=170, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_360_j336_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_360_j336_H10_FITS_custom_ftM_sl360_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11585
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=170, out_features=328, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6152335360.0
params:  56088.0
Trainable parameters:  56088
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 59.4045250415802
Epoch: 1, Steps: 90 | Train Loss: 1.0842545 Vali Loss: 0.9747413 Test Loss: 1.1315011
Validation loss decreased (inf --> 0.974741).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 59.44451117515564
Epoch: 2, Steps: 90 | Train Loss: 0.6911031 Vali Loss: 0.7552660 Test Loss: 0.8767111
Validation loss decreased (0.974741 --> 0.755266).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 60.41074991226196
Epoch: 3, Steps: 90 | Train Loss: 0.5447162 Vali Loss: 0.6227476 Test Loss: 0.7250920
Validation loss decreased (0.755266 --> 0.622748).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 57.42362833023071
Epoch: 4, Steps: 90 | Train Loss: 0.4512940 Vali Loss: 0.5350330 Test Loss: 0.6267334
Validation loss decreased (0.622748 --> 0.535033).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 58.1976363658905
Epoch: 5, Steps: 90 | Train Loss: 0.3893202 Vali Loss: 0.4766902 Test Loss: 0.5615777
Validation loss decreased (0.535033 --> 0.476690).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 57.29332613945007
Epoch: 6, Steps: 90 | Train Loss: 0.3477861 Vali Loss: 0.4369071 Test Loss: 0.5184864
Validation loss decreased (0.476690 --> 0.436907).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 65.47845244407654
Epoch: 7, Steps: 90 | Train Loss: 0.3198067 Vali Loss: 0.4102702 Test Loss: 0.4900761
Validation loss decreased (0.436907 --> 0.410270).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 58.42371368408203
Epoch: 8, Steps: 90 | Train Loss: 0.3011506 Vali Loss: 0.3919105 Test Loss: 0.4711889
Validation loss decreased (0.410270 --> 0.391910).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 59.37735414505005
Epoch: 9, Steps: 90 | Train Loss: 0.2886187 Vali Loss: 0.3796467 Test Loss: 0.4587379
Validation loss decreased (0.391910 --> 0.379647).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 57.341346740722656
Epoch: 10, Steps: 90 | Train Loss: 0.2801984 Vali Loss: 0.3716167 Test Loss: 0.4507309
Validation loss decreased (0.379647 --> 0.371617).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 57.43444037437439
Epoch: 11, Steps: 90 | Train Loss: 0.2747805 Vali Loss: 0.3660034 Test Loss: 0.4456159
Validation loss decreased (0.371617 --> 0.366003).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 83.94374227523804
Epoch: 12, Steps: 90 | Train Loss: 0.2710670 Vali Loss: 0.3620937 Test Loss: 0.4421965
Validation loss decreased (0.366003 --> 0.362094).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 114.57710719108582
Epoch: 13, Steps: 90 | Train Loss: 0.2685897 Vali Loss: 0.3592869 Test Loss: 0.4403073
Validation loss decreased (0.362094 --> 0.359287).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 85.68195629119873
Epoch: 14, Steps: 90 | Train Loss: 0.2669586 Vali Loss: 0.3577204 Test Loss: 0.4386468
Validation loss decreased (0.359287 --> 0.357720).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 87.19273781776428
Epoch: 15, Steps: 90 | Train Loss: 0.2659164 Vali Loss: 0.3564208 Test Loss: 0.4378177
Validation loss decreased (0.357720 --> 0.356421).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 97.04490876197815
Epoch: 16, Steps: 90 | Train Loss: 0.2652604 Vali Loss: 0.3557035 Test Loss: 0.4372295
Validation loss decreased (0.356421 --> 0.355704).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 92.35560774803162
Epoch: 17, Steps: 90 | Train Loss: 0.2647838 Vali Loss: 0.3551906 Test Loss: 0.4369448
Validation loss decreased (0.355704 --> 0.355191).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 87.6550669670105
Epoch: 18, Steps: 90 | Train Loss: 0.2644332 Vali Loss: 0.3547822 Test Loss: 0.4367517
Validation loss decreased (0.355191 --> 0.354782).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 88.45608353614807
Epoch: 19, Steps: 90 | Train Loss: 0.2642194 Vali Loss: 0.3545413 Test Loss: 0.4364466
Validation loss decreased (0.354782 --> 0.354541).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 98.59808731079102
Epoch: 20, Steps: 90 | Train Loss: 0.2639525 Vali Loss: 0.3539639 Test Loss: 0.4365023
Validation loss decreased (0.354541 --> 0.353964).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 104.49731874465942
Epoch: 21, Steps: 90 | Train Loss: 0.2637949 Vali Loss: 0.3539481 Test Loss: 0.4364346
Validation loss decreased (0.353964 --> 0.353948).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 89.89595556259155
Epoch: 22, Steps: 90 | Train Loss: 0.2637613 Vali Loss: 0.3537362 Test Loss: 0.4363090
Validation loss decreased (0.353948 --> 0.353736).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 89.59237027168274
Epoch: 23, Steps: 90 | Train Loss: 0.2637413 Vali Loss: 0.3543423 Test Loss: 0.4362186
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 87.60252094268799
Epoch: 24, Steps: 90 | Train Loss: 0.2637299 Vali Loss: 0.3536219 Test Loss: 0.4362260
Validation loss decreased (0.353736 --> 0.353622).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 87.27396702766418
Epoch: 25, Steps: 90 | Train Loss: 0.2637065 Vali Loss: 0.3537145 Test Loss: 0.4362862
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 95.17918825149536
Epoch: 26, Steps: 90 | Train Loss: 0.2636234 Vali Loss: 0.3536479 Test Loss: 0.4362259
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 100.33206248283386
Epoch: 27, Steps: 90 | Train Loss: 0.2635413 Vali Loss: 0.3537671 Test Loss: 0.4362011
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 88.74581980705261
Epoch: 28, Steps: 90 | Train Loss: 0.2635546 Vali Loss: 0.3535379 Test Loss: 0.4362176
Validation loss decreased (0.353622 --> 0.353538).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 93.66836929321289
Epoch: 29, Steps: 90 | Train Loss: 0.2634978 Vali Loss: 0.3538111 Test Loss: 0.4362598
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 111.2489013671875
Epoch: 30, Steps: 90 | Train Loss: 0.2634604 Vali Loss: 0.3537138 Test Loss: 0.4362609
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 125.24008679389954
Epoch: 31, Steps: 90 | Train Loss: 0.2634143 Vali Loss: 0.3535313 Test Loss: 0.4362301
Validation loss decreased (0.353538 --> 0.353531).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 85.98482155799866
Epoch: 32, Steps: 90 | Train Loss: 0.2635357 Vali Loss: 0.3532966 Test Loss: 0.4362836
Validation loss decreased (0.353531 --> 0.353297).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 93.41614103317261
Epoch: 33, Steps: 90 | Train Loss: 0.2634947 Vali Loss: 0.3534060 Test Loss: 0.4362293
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 80.57409024238586
Epoch: 34, Steps: 90 | Train Loss: 0.2633818 Vali Loss: 0.3535937 Test Loss: 0.4362003
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 84.49468612670898
Epoch: 35, Steps: 90 | Train Loss: 0.2634537 Vali Loss: 0.3530641 Test Loss: 0.4361627
Validation loss decreased (0.353297 --> 0.353064).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 118.41876006126404
Epoch: 36, Steps: 90 | Train Loss: 0.2633782 Vali Loss: 0.3535027 Test Loss: 0.4361618
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 97.8448805809021
Epoch: 37, Steps: 90 | Train Loss: 0.2634749 Vali Loss: 0.3534374 Test Loss: 0.4361869
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 87.87011742591858
Epoch: 38, Steps: 90 | Train Loss: 0.2634101 Vali Loss: 0.3531477 Test Loss: 0.4361331
EarlyStopping counter: 3 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 74.9087016582489
Epoch: 39, Steps: 90 | Train Loss: 0.2633515 Vali Loss: 0.3534088 Test Loss: 0.4361193
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 74.71913552284241
Epoch: 40, Steps: 90 | Train Loss: 0.2634690 Vali Loss: 0.3531196 Test Loss: 0.4361515
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 74.85658717155457
Epoch: 41, Steps: 90 | Train Loss: 0.2632914 Vali Loss: 0.3534753 Test Loss: 0.4361418
EarlyStopping counter: 6 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 74.59165167808533
Epoch: 42, Steps: 90 | Train Loss: 0.2633288 Vali Loss: 0.3532856 Test Loss: 0.4360906
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 62.4182391166687
Epoch: 43, Steps: 90 | Train Loss: 0.2632905 Vali Loss: 0.3531910 Test Loss: 0.4361135
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 61.383471965789795
Epoch: 44, Steps: 90 | Train Loss: 0.2633352 Vali Loss: 0.3532649 Test Loss: 0.4361443
EarlyStopping counter: 9 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 50.771990060806274
Epoch: 45, Steps: 90 | Train Loss: 0.2633659 Vali Loss: 0.3535208 Test Loss: 0.4361271
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_360_j336_H10_FITS_custom_ftM_sl360_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4336564540863037, mae:0.2899831235408783, rse:0.5412179231643677, corr:[0.26998818 0.2804796  0.27793694 0.28096867 0.28192455 0.28331414
 0.2850509  0.28404698 0.28565648 0.2834697  0.28486657 0.28259972
 0.28305098 0.28203315 0.2813874  0.28174725 0.28082052 0.2820677
 0.28107995 0.28230023 0.2818229  0.28215387 0.28200197 0.282142
 0.28434056 0.28426048 0.28490424 0.28385523 0.28424662 0.283648
 0.28317192 0.2833424  0.28222832 0.28283146 0.28199568 0.28244227
 0.2817433  0.28188214 0.2821938  0.28178397 0.28273764 0.28206882
 0.28293186 0.28266627 0.282968   0.283008   0.28270468 0.28314838
 0.28321686 0.2838994  0.28305256 0.2832403  0.28290632 0.28240448
 0.28286484 0.28216216 0.2826262  0.28187594 0.28216904 0.28231496
 0.28229168 0.2827219  0.28217104 0.282891   0.2825637  0.28295207
 0.2828424  0.28279617 0.28307718 0.28270048 0.28311238 0.28262746
 0.28317222 0.28281945 0.2827263  0.2827939  0.2822244  0.2823776
 0.28176972 0.28220427 0.28182507 0.281942   0.28222886 0.2819213
 0.28228983 0.2817108  0.28231815 0.2822424  0.28246307 0.28259876
 0.2822607  0.28276175 0.282331   0.28264397 0.28228527 0.282482
 0.28226605 0.28194132 0.28227046 0.28189814 0.28224197 0.28155518
 0.28184542 0.2819766  0.2818324  0.2822003  0.28178594 0.28220332
 0.2818343  0.28214443 0.282261   0.28208372 0.28233466 0.2819029
 0.28235978 0.28209066 0.28212675 0.28186783 0.28172082 0.28201753
 0.28157768 0.28224438 0.28198484 0.2821008  0.2819303  0.2816024
 0.28170946 0.28153598 0.2820408  0.28161666 0.2818733  0.2819142
 0.2819332  0.2821622  0.28186935 0.28234446 0.28212377 0.28222257
 0.2821859  0.2821834  0.2822809  0.2818384  0.28222194 0.28247452
 0.28295448 0.28304943 0.28318122 0.2830934  0.2828249  0.28309792
 0.28279278 0.2832628  0.28300568 0.28311834 0.28327543 0.28297633
 0.2833735  0.28312984 0.28348106 0.28327006 0.28336623 0.28339472
 0.28312314 0.28348008 0.28299704 0.28317866 0.28292778 0.28345776
 0.28478628 0.28444165 0.28420824 0.28371564 0.2839999  0.28365755
 0.28385803 0.28398558 0.28390056 0.2841279  0.28359565 0.2841125
 0.28387415 0.28407076 0.2840122  0.28354937 0.28381893 0.28336114
 0.2836494  0.28346422 0.28344145 0.28331795 0.28304365 0.28304455
 0.28344896 0.2841914  0.28378984 0.28365904 0.28359202 0.28351155
 0.28348503 0.28299132 0.28330088 0.28286114 0.28315908 0.2830463
 0.28300223 0.2833392  0.28290278 0.28313977 0.2826356  0.28289795
 0.2827263  0.28249565 0.28266007 0.28235582 0.28269377 0.28227553
 0.28285256 0.2828715  0.28275013 0.2828759  0.28266975 0.28296602
 0.28246605 0.28261116 0.28238532 0.28257045 0.2827047  0.28233317
 0.28271317 0.2822508  0.28256476 0.28218734 0.2821931  0.28236943
 0.28207648 0.28231665 0.28179833 0.28231525 0.28219044 0.28235433
 0.2824152  0.28220162 0.28250292 0.2820161  0.2823763  0.28202608
 0.28236654 0.28243142 0.28218806 0.28242704 0.28180277 0.2821338
 0.28172925 0.28181255 0.28167495 0.28166676 0.2821139  0.28153828
 0.28182456 0.28126255 0.28152162 0.28157273 0.28164995 0.28207827
 0.28132513 0.28168654 0.28140312 0.28174067 0.2815762  0.28146493
 0.2815889  0.28124785 0.28159586 0.28112933 0.2816386  0.2810882
 0.2808882  0.2809422  0.2805792  0.2813072  0.2807837  0.28129408
 0.28091872 0.28094584 0.28130862 0.28084114 0.2815407  0.28104964
 0.28103262 0.280812   0.28109795 0.28126314 0.2809442  0.2812397
 0.28055155 0.28102756 0.28037003 0.2805206  0.28027853 0.28002927
 0.2807401  0.28015652 0.28112507 0.28064555 0.28112814 0.2813132
 0.2811909  0.28177792 0.28121808 0.28187174 0.28152487 0.28230634
 0.2819898  0.28199205 0.28245732 0.28206062 0.282508   0.28134534
 0.28170142 0.28081545 0.28138536 0.2812126  0.2813309  0.2824144
 0.28183302 0.28353164 0.28235605 0.283926   0.28284276 0.2829501
 0.2824547  0.2810119  0.2810056  0.2784476  0.2809159  0.28372186]
