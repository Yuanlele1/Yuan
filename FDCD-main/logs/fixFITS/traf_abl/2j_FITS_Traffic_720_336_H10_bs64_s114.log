Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j336_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16559226880.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 139.0530743598938
Epoch: 1, Steps: 87 | Train Loss: 1.0291503 Vali Loss: 1.0951320 Test Loss: 1.2763714
Validation loss decreased (inf --> 1.095132).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 141.27516508102417
Epoch: 2, Steps: 87 | Train Loss: 0.7572469 Vali Loss: 0.9750941 Test Loss: 1.1348143
Validation loss decreased (1.095132 --> 0.975094).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 138.1788260936737
Epoch: 3, Steps: 87 | Train Loss: 0.6648705 Vali Loss: 0.9099098 Test Loss: 1.0600784
Validation loss decreased (0.975094 --> 0.909910).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 138.6368329524994
Epoch: 4, Steps: 87 | Train Loss: 0.5987974 Vali Loss: 0.8579247 Test Loss: 0.9986879
Validation loss decreased (0.909910 --> 0.857925).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 147.64161014556885
Epoch: 5, Steps: 87 | Train Loss: 0.5449042 Vali Loss: 0.8135108 Test Loss: 0.9482515
Validation loss decreased (0.857925 --> 0.813511).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 136.13084053993225
Epoch: 6, Steps: 87 | Train Loss: 0.4994966 Vali Loss: 0.7752937 Test Loss: 0.9035776
Validation loss decreased (0.813511 --> 0.775294).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 140.32385349273682
Epoch: 7, Steps: 87 | Train Loss: 0.4606157 Vali Loss: 0.7383649 Test Loss: 0.8606384
Validation loss decreased (0.775294 --> 0.738365).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 137.31085228919983
Epoch: 8, Steps: 87 | Train Loss: 0.4269883 Vali Loss: 0.7076118 Test Loss: 0.8254312
Validation loss decreased (0.738365 --> 0.707612).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 133.9633605480194
Epoch: 9, Steps: 87 | Train Loss: 0.3975394 Vali Loss: 0.6794654 Test Loss: 0.7927753
Validation loss decreased (0.707612 --> 0.679465).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 129.86768245697021
Epoch: 10, Steps: 87 | Train Loss: 0.3718050 Vali Loss: 0.6551141 Test Loss: 0.7650236
Validation loss decreased (0.679465 --> 0.655114).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 130.91107940673828
Epoch: 11, Steps: 87 | Train Loss: 0.3489972 Vali Loss: 0.6332963 Test Loss: 0.7396129
Validation loss decreased (0.655114 --> 0.633296).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 124.24408268928528
Epoch: 12, Steps: 87 | Train Loss: 0.3287436 Vali Loss: 0.6115574 Test Loss: 0.7142751
Validation loss decreased (0.633296 --> 0.611557).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 109.13551068305969
Epoch: 13, Steps: 87 | Train Loss: 0.3106820 Vali Loss: 0.5939856 Test Loss: 0.6940548
Validation loss decreased (0.611557 --> 0.593986).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 105.87590909004211
Epoch: 14, Steps: 87 | Train Loss: 0.2944856 Vali Loss: 0.5760298 Test Loss: 0.6736684
Validation loss decreased (0.593986 --> 0.576030).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 105.9133677482605
Epoch: 15, Steps: 87 | Train Loss: 0.2799908 Vali Loss: 0.5619362 Test Loss: 0.6568344
Validation loss decreased (0.576030 --> 0.561936).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 104.68570017814636
Epoch: 16, Steps: 87 | Train Loss: 0.2668844 Vali Loss: 0.5481313 Test Loss: 0.6414776
Validation loss decreased (0.561936 --> 0.548131).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 100.63073182106018
Epoch: 17, Steps: 87 | Train Loss: 0.2550416 Vali Loss: 0.5361082 Test Loss: 0.6275387
Validation loss decreased (0.548131 --> 0.536108).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 109.3922770023346
Epoch: 18, Steps: 87 | Train Loss: 0.2442885 Vali Loss: 0.5251675 Test Loss: 0.6148019
Validation loss decreased (0.536108 --> 0.525168).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 105.1484100818634
Epoch: 19, Steps: 87 | Train Loss: 0.2345592 Vali Loss: 0.5148314 Test Loss: 0.6032402
Validation loss decreased (0.525168 --> 0.514831).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 107.92728209495544
Epoch: 20, Steps: 87 | Train Loss: 0.2256801 Vali Loss: 0.5046325 Test Loss: 0.5911576
Validation loss decreased (0.514831 --> 0.504633).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 104.18439555168152
Epoch: 21, Steps: 87 | Train Loss: 0.2175448 Vali Loss: 0.4955123 Test Loss: 0.5811349
Validation loss decreased (0.504633 --> 0.495512).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 106.90241384506226
Epoch: 22, Steps: 87 | Train Loss: 0.2101398 Vali Loss: 0.4883177 Test Loss: 0.5721247
Validation loss decreased (0.495512 --> 0.488318).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 91.43129706382751
Epoch: 23, Steps: 87 | Train Loss: 0.2033472 Vali Loss: 0.4810532 Test Loss: 0.5640455
Validation loss decreased (0.488318 --> 0.481053).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 88.79348397254944
Epoch: 24, Steps: 87 | Train Loss: 0.1970814 Vali Loss: 0.4738527 Test Loss: 0.5560376
Validation loss decreased (0.481053 --> 0.473853).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 87.55867648124695
Epoch: 25, Steps: 87 | Train Loss: 0.1913780 Vali Loss: 0.4679210 Test Loss: 0.5486828
Validation loss decreased (0.473853 --> 0.467921).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 90.1619770526886
Epoch: 26, Steps: 87 | Train Loss: 0.1860599 Vali Loss: 0.4618816 Test Loss: 0.5427394
Validation loss decreased (0.467921 --> 0.461882).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 85.23765516281128
Epoch: 27, Steps: 87 | Train Loss: 0.1811803 Vali Loss: 0.4562162 Test Loss: 0.5363944
Validation loss decreased (0.461882 --> 0.456216).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 89.81851744651794
Epoch: 28, Steps: 87 | Train Loss: 0.1766855 Vali Loss: 0.4523219 Test Loss: 0.5308166
Validation loss decreased (0.456216 --> 0.452322).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 89.05843615531921
Epoch: 29, Steps: 87 | Train Loss: 0.1724935 Vali Loss: 0.4464966 Test Loss: 0.5247061
Validation loss decreased (0.452322 --> 0.446497).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 87.15592575073242
Epoch: 30, Steps: 87 | Train Loss: 0.1686154 Vali Loss: 0.4432745 Test Loss: 0.5209033
Validation loss decreased (0.446497 --> 0.443274).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 87.14417314529419
Epoch: 31, Steps: 87 | Train Loss: 0.1650057 Vali Loss: 0.4390404 Test Loss: 0.5160943
Validation loss decreased (0.443274 --> 0.439040).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 101.28473472595215
Epoch: 32, Steps: 87 | Train Loss: 0.1616370 Vali Loss: 0.4353889 Test Loss: 0.5121219
Validation loss decreased (0.439040 --> 0.435389).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 156.86974835395813
Epoch: 33, Steps: 87 | Train Loss: 0.1585585 Vali Loss: 0.4313807 Test Loss: 0.5078113
Validation loss decreased (0.435389 --> 0.431381).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 121.15817904472351
Epoch: 34, Steps: 87 | Train Loss: 0.1556519 Vali Loss: 0.4281690 Test Loss: 0.5039285
Validation loss decreased (0.431381 --> 0.428169).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 129.99957633018494
Epoch: 35, Steps: 87 | Train Loss: 0.1529506 Vali Loss: 0.4252537 Test Loss: 0.5004578
Validation loss decreased (0.428169 --> 0.425254).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 127.71632647514343
Epoch: 36, Steps: 87 | Train Loss: 0.1504015 Vali Loss: 0.4220551 Test Loss: 0.4976868
Validation loss decreased (0.425254 --> 0.422055).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 181.81157636642456
Epoch: 37, Steps: 87 | Train Loss: 0.1480676 Vali Loss: 0.4199986 Test Loss: 0.4946044
Validation loss decreased (0.422055 --> 0.419999).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 255.14965772628784
Epoch: 38, Steps: 87 | Train Loss: 0.1458606 Vali Loss: 0.4168125 Test Loss: 0.4915303
Validation loss decreased (0.419999 --> 0.416813).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 174.98730611801147
Epoch: 39, Steps: 87 | Train Loss: 0.1437673 Vali Loss: 0.4144808 Test Loss: 0.4888386
Validation loss decreased (0.416813 --> 0.414481).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 168.82828521728516
Epoch: 40, Steps: 87 | Train Loss: 0.1418258 Vali Loss: 0.4128119 Test Loss: 0.4867317
Validation loss decreased (0.414481 --> 0.412812).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 153.70885705947876
Epoch: 41, Steps: 87 | Train Loss: 0.1400355 Vali Loss: 0.4106388 Test Loss: 0.4844090
Validation loss decreased (0.412812 --> 0.410639).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 175.86067581176758
Epoch: 42, Steps: 87 | Train Loss: 0.1383349 Vali Loss: 0.4084129 Test Loss: 0.4820172
Validation loss decreased (0.410639 --> 0.408413).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 157.93681383132935
Epoch: 43, Steps: 87 | Train Loss: 0.1367266 Vali Loss: 0.4066434 Test Loss: 0.4803084
Validation loss decreased (0.408413 --> 0.406643).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 172.71698093414307
Epoch: 44, Steps: 87 | Train Loss: 0.1351922 Vali Loss: 0.4048732 Test Loss: 0.4782700
Validation loss decreased (0.406643 --> 0.404873).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 174.73746252059937
Epoch: 45, Steps: 87 | Train Loss: 0.1337470 Vali Loss: 0.4033397 Test Loss: 0.4765722
Validation loss decreased (0.404873 --> 0.403340).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 155.16564750671387
Epoch: 46, Steps: 87 | Train Loss: 0.1324351 Vali Loss: 0.4017937 Test Loss: 0.4748516
Validation loss decreased (0.403340 --> 0.401794).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 172.05076932907104
Epoch: 47, Steps: 87 | Train Loss: 0.1311916 Vali Loss: 0.4002377 Test Loss: 0.4731218
Validation loss decreased (0.401794 --> 0.400238).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 173.44402599334717
Epoch: 48, Steps: 87 | Train Loss: 0.1299751 Vali Loss: 0.3990746 Test Loss: 0.4718120
Validation loss decreased (0.400238 --> 0.399075).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 159.37303042411804
Epoch: 49, Steps: 87 | Train Loss: 0.1288856 Vali Loss: 0.3977848 Test Loss: 0.4702467
Validation loss decreased (0.399075 --> 0.397785).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 163.3861587047577
Epoch: 50, Steps: 87 | Train Loss: 0.1278274 Vali Loss: 0.3966267 Test Loss: 0.4688239
Validation loss decreased (0.397785 --> 0.396627).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 162.56929111480713
Epoch: 51, Steps: 87 | Train Loss: 0.1268225 Vali Loss: 0.3958190 Test Loss: 0.4676812
Validation loss decreased (0.396627 --> 0.395819).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 299.09683179855347
Epoch: 52, Steps: 87 | Train Loss: 0.1258657 Vali Loss: 0.3944145 Test Loss: 0.4665959
Validation loss decreased (0.395819 --> 0.394414).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 184.74565720558167
Epoch: 53, Steps: 87 | Train Loss: 0.1249503 Vali Loss: 0.3930331 Test Loss: 0.4653527
Validation loss decreased (0.394414 --> 0.393033).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 155.99600768089294
Epoch: 54, Steps: 87 | Train Loss: 0.1241334 Vali Loss: 0.3927355 Test Loss: 0.4643072
Validation loss decreased (0.393033 --> 0.392736).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 171.3498387336731
Epoch: 55, Steps: 87 | Train Loss: 0.1233366 Vali Loss: 0.3913738 Test Loss: 0.4632779
Validation loss decreased (0.392736 --> 0.391374).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 163.35329461097717
Epoch: 56, Steps: 87 | Train Loss: 0.1225895 Vali Loss: 0.3903531 Test Loss: 0.4622726
Validation loss decreased (0.391374 --> 0.390353).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 169.81553840637207
Epoch: 57, Steps: 87 | Train Loss: 0.1218684 Vali Loss: 0.3899508 Test Loss: 0.4613834
Validation loss decreased (0.390353 --> 0.389951).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 167.0341010093689
Epoch: 58, Steps: 87 | Train Loss: 0.1211995 Vali Loss: 0.3889518 Test Loss: 0.4605585
Validation loss decreased (0.389951 --> 0.388952).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 223.73986530303955
Epoch: 59, Steps: 87 | Train Loss: 0.1205496 Vali Loss: 0.3881617 Test Loss: 0.4596883
Validation loss decreased (0.388952 --> 0.388162).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 174.96664595603943
Epoch: 60, Steps: 87 | Train Loss: 0.1199602 Vali Loss: 0.3874481 Test Loss: 0.4589404
Validation loss decreased (0.388162 --> 0.387448).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 162.09637236595154
Epoch: 61, Steps: 87 | Train Loss: 0.1193616 Vali Loss: 0.3868340 Test Loss: 0.4581978
Validation loss decreased (0.387448 --> 0.386834).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 154.6408817768097
Epoch: 62, Steps: 87 | Train Loss: 0.1188326 Vali Loss: 0.3862492 Test Loss: 0.4575387
Validation loss decreased (0.386834 --> 0.386249).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 172.09754180908203
Epoch: 63, Steps: 87 | Train Loss: 0.1183076 Vali Loss: 0.3855971 Test Loss: 0.4568720
Validation loss decreased (0.386249 --> 0.385597).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 172.6950089931488
Epoch: 64, Steps: 87 | Train Loss: 0.1178443 Vali Loss: 0.3854604 Test Loss: 0.4562342
Validation loss decreased (0.385597 --> 0.385460).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 160.84680676460266
Epoch: 65, Steps: 87 | Train Loss: 0.1173631 Vali Loss: 0.3846119 Test Loss: 0.4556715
Validation loss decreased (0.385460 --> 0.384612).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 143.1313033103943
Epoch: 66, Steps: 87 | Train Loss: 0.1168982 Vali Loss: 0.3841269 Test Loss: 0.4550989
Validation loss decreased (0.384612 --> 0.384127).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 143.54460215568542
Epoch: 67, Steps: 87 | Train Loss: 0.1165055 Vali Loss: 0.3835241 Test Loss: 0.4545536
Validation loss decreased (0.384127 --> 0.383524).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 176.87134456634521
Epoch: 68, Steps: 87 | Train Loss: 0.1160968 Vali Loss: 0.3829358 Test Loss: 0.4540693
Validation loss decreased (0.383524 --> 0.382936).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 156.30246424674988
Epoch: 69, Steps: 87 | Train Loss: 0.1157006 Vali Loss: 0.3822024 Test Loss: 0.4536098
Validation loss decreased (0.382936 --> 0.382202).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 158.3922154903412
Epoch: 70, Steps: 87 | Train Loss: 0.1153742 Vali Loss: 0.3820973 Test Loss: 0.4531348
Validation loss decreased (0.382202 --> 0.382097).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 128.94049954414368
Epoch: 71, Steps: 87 | Train Loss: 0.1149907 Vali Loss: 0.3816849 Test Loss: 0.4526765
Validation loss decreased (0.382097 --> 0.381685).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 126.02217984199524
Epoch: 72, Steps: 87 | Train Loss: 0.1147201 Vali Loss: 0.3812881 Test Loss: 0.4522293
Validation loss decreased (0.381685 --> 0.381288).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 132.39172196388245
Epoch: 73, Steps: 87 | Train Loss: 0.1143854 Vali Loss: 0.3810310 Test Loss: 0.4518863
Validation loss decreased (0.381288 --> 0.381031).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 123.88803148269653
Epoch: 74, Steps: 87 | Train Loss: 0.1140768 Vali Loss: 0.3807369 Test Loss: 0.4515068
Validation loss decreased (0.381031 --> 0.380737).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 123.38907623291016
Epoch: 75, Steps: 87 | Train Loss: 0.1138081 Vali Loss: 0.3801953 Test Loss: 0.4512127
Validation loss decreased (0.380737 --> 0.380195).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 124.20289039611816
Epoch: 76, Steps: 87 | Train Loss: 0.1135449 Vali Loss: 0.3797971 Test Loss: 0.4508863
Validation loss decreased (0.380195 --> 0.379797).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 124.96904063224792
Epoch: 77, Steps: 87 | Train Loss: 0.1132999 Vali Loss: 0.3796555 Test Loss: 0.4505361
Validation loss decreased (0.379797 --> 0.379655).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 125.63906621932983
Epoch: 78, Steps: 87 | Train Loss: 0.1130487 Vali Loss: 0.3792824 Test Loss: 0.4502668
Validation loss decreased (0.379655 --> 0.379282).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 130.35523509979248
Epoch: 79, Steps: 87 | Train Loss: 0.1128477 Vali Loss: 0.3788587 Test Loss: 0.4499620
Validation loss decreased (0.379282 --> 0.378859).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 122.4217324256897
Epoch: 80, Steps: 87 | Train Loss: 0.1126305 Vali Loss: 0.3789679 Test Loss: 0.4496963
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 128.0888512134552
Epoch: 81, Steps: 87 | Train Loss: 0.1124274 Vali Loss: 0.3788321 Test Loss: 0.4494243
Validation loss decreased (0.378859 --> 0.378832).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 127.35258483886719
Epoch: 82, Steps: 87 | Train Loss: 0.1122505 Vali Loss: 0.3780996 Test Loss: 0.4491836
Validation loss decreased (0.378832 --> 0.378100).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 120.39100384712219
Epoch: 83, Steps: 87 | Train Loss: 0.1120263 Vali Loss: 0.3781509 Test Loss: 0.4489380
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 123.268137216568
Epoch: 84, Steps: 87 | Train Loss: 0.1118733 Vali Loss: 0.3777417 Test Loss: 0.4487530
Validation loss decreased (0.378100 --> 0.377742).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 128.0758399963379
Epoch: 85, Steps: 87 | Train Loss: 0.1117081 Vali Loss: 0.3779789 Test Loss: 0.4485178
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 146.92535305023193
Epoch: 86, Steps: 87 | Train Loss: 0.1115430 Vali Loss: 0.3774773 Test Loss: 0.4483172
Validation loss decreased (0.377742 --> 0.377477).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 133.6862416267395
Epoch: 87, Steps: 87 | Train Loss: 0.1113848 Vali Loss: 0.3776679 Test Loss: 0.4481302
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 124.69303131103516
Epoch: 88, Steps: 87 | Train Loss: 0.1112522 Vali Loss: 0.3776034 Test Loss: 0.4479416
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 120.40881967544556
Epoch: 89, Steps: 87 | Train Loss: 0.1111246 Vali Loss: 0.3770534 Test Loss: 0.4477703
Validation loss decreased (0.377477 --> 0.377053).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 122.13792252540588
Epoch: 90, Steps: 87 | Train Loss: 0.1110047 Vali Loss: 0.3769754 Test Loss: 0.4476175
Validation loss decreased (0.377053 --> 0.376975).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 120.20205402374268
Epoch: 91, Steps: 87 | Train Loss: 0.1108910 Vali Loss: 0.3770357 Test Loss: 0.4474404
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 119.17919278144836
Epoch: 92, Steps: 87 | Train Loss: 0.1107468 Vali Loss: 0.3769467 Test Loss: 0.4473065
Validation loss decreased (0.376975 --> 0.376947).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 129.78545379638672
Epoch: 93, Steps: 87 | Train Loss: 0.1106166 Vali Loss: 0.3763455 Test Loss: 0.4471560
Validation loss decreased (0.376947 --> 0.376346).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 118.07200956344604
Epoch: 94, Steps: 87 | Train Loss: 0.1105319 Vali Loss: 0.3764279 Test Loss: 0.4470365
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 117.68859100341797
Epoch: 95, Steps: 87 | Train Loss: 0.1104261 Vali Loss: 0.3766307 Test Loss: 0.4469109
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 126.93918180465698
Epoch: 96, Steps: 87 | Train Loss: 0.1103393 Vali Loss: 0.3759058 Test Loss: 0.4467932
Validation loss decreased (0.376346 --> 0.375906).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 131.09404587745667
Epoch: 97, Steps: 87 | Train Loss: 0.1102252 Vali Loss: 0.3761413 Test Loss: 0.4466663
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 118.69589757919312
Epoch: 98, Steps: 87 | Train Loss: 0.1101765 Vali Loss: 0.3762392 Test Loss: 0.4465722
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 105.49240446090698
Epoch: 99, Steps: 87 | Train Loss: 0.1100884 Vali Loss: 0.3755968 Test Loss: 0.4464659
Validation loss decreased (0.375906 --> 0.375597).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 93.41519927978516
Epoch: 100, Steps: 87 | Train Loss: 0.1100007 Vali Loss: 0.3759811 Test Loss: 0.4463788
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11225
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=320, out_features=469, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  16559226880.0
params:  150549.0
Trainable parameters:  150549
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 93.54236364364624
Epoch: 1, Steps: 87 | Train Loss: 0.2559516 Vali Loss: 0.3417487 Test Loss: 0.4142318
Validation loss decreased (inf --> 0.341749).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 97.76083898544312
Epoch: 2, Steps: 87 | Train Loss: 0.2484413 Vali Loss: 0.3409060 Test Loss: 0.4144574
Validation loss decreased (0.341749 --> 0.340906).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 97.27839875221252
Epoch: 3, Steps: 87 | Train Loss: 0.2482227 Vali Loss: 0.3404439 Test Loss: 0.4136669
Validation loss decreased (0.340906 --> 0.340444).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 101.47952437400818
Epoch: 4, Steps: 87 | Train Loss: 0.2480863 Vali Loss: 0.3397141 Test Loss: 0.4139610
Validation loss decreased (0.340444 --> 0.339714).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 165.1801233291626
Epoch: 5, Steps: 87 | Train Loss: 0.2479823 Vali Loss: 0.3397861 Test Loss: 0.4138253
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 139.19236516952515
Epoch: 6, Steps: 87 | Train Loss: 0.2479380 Vali Loss: 0.3400760 Test Loss: 0.4135738
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 204.33162021636963
Epoch: 7, Steps: 87 | Train Loss: 0.2478871 Vali Loss: 0.3394943 Test Loss: 0.4134283
Validation loss decreased (0.339714 --> 0.339494).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 120.33550214767456
Epoch: 8, Steps: 87 | Train Loss: 0.2478144 Vali Loss: 0.3394353 Test Loss: 0.4129148
Validation loss decreased (0.339494 --> 0.339435).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 128.05375838279724
Epoch: 9, Steps: 87 | Train Loss: 0.2478439 Vali Loss: 0.3392242 Test Loss: 0.4129586
Validation loss decreased (0.339435 --> 0.339224).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 126.43631601333618
Epoch: 10, Steps: 87 | Train Loss: 0.2478081 Vali Loss: 0.3393247 Test Loss: 0.4130713
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 132.42697548866272
Epoch: 11, Steps: 87 | Train Loss: 0.2477529 Vali Loss: 0.3395660 Test Loss: 0.4133988
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 136.25689959526062
Epoch: 12, Steps: 87 | Train Loss: 0.2477425 Vali Loss: 0.3396558 Test Loss: 0.4134419
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 192.19066715240479
Epoch: 13, Steps: 87 | Train Loss: 0.2476605 Vali Loss: 0.3391138 Test Loss: 0.4127718
Validation loss decreased (0.339224 --> 0.339114).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 120.7927474975586
Epoch: 14, Steps: 87 | Train Loss: 0.2476483 Vali Loss: 0.3395562 Test Loss: 0.4128900
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 113.61290264129639
Epoch: 15, Steps: 87 | Train Loss: 0.2475744 Vali Loss: 0.3393608 Test Loss: 0.4127328
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 105.35188126564026
Epoch: 16, Steps: 87 | Train Loss: 0.2475704 Vali Loss: 0.3391238 Test Loss: 0.4125811
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 143.78380274772644
Epoch: 17, Steps: 87 | Train Loss: 0.2475503 Vali Loss: 0.3392654 Test Loss: 0.4129490
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 138.32272148132324
Epoch: 18, Steps: 87 | Train Loss: 0.2475885 Vali Loss: 0.3389474 Test Loss: 0.4129791
Validation loss decreased (0.339114 --> 0.338947).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 109.01926136016846
Epoch: 19, Steps: 87 | Train Loss: 0.2474487 Vali Loss: 0.3395503 Test Loss: 0.4129171
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 116.14128160476685
Epoch: 20, Steps: 87 | Train Loss: 0.2475185 Vali Loss: 0.3393867 Test Loss: 0.4128524
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 109.28809881210327
Epoch: 21, Steps: 87 | Train Loss: 0.2475515 Vali Loss: 0.3394269 Test Loss: 0.4126816
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 105.29674029350281
Epoch: 22, Steps: 87 | Train Loss: 0.2474715 Vali Loss: 0.3396086 Test Loss: 0.4127579
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 132.0949990749359
Epoch: 23, Steps: 87 | Train Loss: 0.2474646 Vali Loss: 0.3392106 Test Loss: 0.4126475
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 150.51026248931885
Epoch: 24, Steps: 87 | Train Loss: 0.2474589 Vali Loss: 0.3393147 Test Loss: 0.4126932
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 125.1963882446289
Epoch: 25, Steps: 87 | Train Loss: 0.2474351 Vali Loss: 0.3388826 Test Loss: 0.4126586
Validation loss decreased (0.338947 --> 0.338883).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 104.46201825141907
Epoch: 26, Steps: 87 | Train Loss: 0.2474349 Vali Loss: 0.3390929 Test Loss: 0.4128604
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 102.77991604804993
Epoch: 27, Steps: 87 | Train Loss: 0.2473419 Vali Loss: 0.3391921 Test Loss: 0.4126584
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 105.61296653747559
Epoch: 28, Steps: 87 | Train Loss: 0.2473427 Vali Loss: 0.3394772 Test Loss: 0.4127675
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 125.52538561820984
Epoch: 29, Steps: 87 | Train Loss: 0.2474066 Vali Loss: 0.3389892 Test Loss: 0.4125904
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 132.14842534065247
Epoch: 30, Steps: 87 | Train Loss: 0.2473826 Vali Loss: 0.3391114 Test Loss: 0.4128348
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 104.82052755355835
Epoch: 31, Steps: 87 | Train Loss: 0.2473094 Vali Loss: 0.3388229 Test Loss: 0.4125713
Validation loss decreased (0.338883 --> 0.338823).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 101.20868515968323
Epoch: 32, Steps: 87 | Train Loss: 0.2473657 Vali Loss: 0.3390877 Test Loss: 0.4126277
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 111.40658617019653
Epoch: 33, Steps: 87 | Train Loss: 0.2472703 Vali Loss: 0.3389499 Test Loss: 0.4124454
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 116.42817425727844
Epoch: 34, Steps: 87 | Train Loss: 0.2473038 Vali Loss: 0.3391119 Test Loss: 0.4125505
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 115.26476120948792
Epoch: 35, Steps: 87 | Train Loss: 0.2472826 Vali Loss: 0.3393095 Test Loss: 0.4125935
EarlyStopping counter: 4 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 94.74351072311401
Epoch: 36, Steps: 87 | Train Loss: 0.2472246 Vali Loss: 0.3392716 Test Loss: 0.4125813
EarlyStopping counter: 5 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 72.58288192749023
Epoch: 37, Steps: 87 | Train Loss: 0.2472707 Vali Loss: 0.3391805 Test Loss: 0.4126291
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 70.37301182746887
Epoch: 38, Steps: 87 | Train Loss: 0.2472086 Vali Loss: 0.3391310 Test Loss: 0.4124697
EarlyStopping counter: 7 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 71.43393063545227
Epoch: 39, Steps: 87 | Train Loss: 0.2472260 Vali Loss: 0.3393697 Test Loss: 0.4126741
EarlyStopping counter: 8 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 70.14279556274414
Epoch: 40, Steps: 87 | Train Loss: 0.2471075 Vali Loss: 0.3391167 Test Loss: 0.4127352
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 70.2412519454956
Epoch: 41, Steps: 87 | Train Loss: 0.2471482 Vali Loss: 0.3391280 Test Loss: 0.4126183
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j336_H10_FITS_custom_ftM_sl720_ll48_pl336_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.4104170501232147, mae:0.27849826216697693, rse:0.5265164971351624, corr:[0.2692202  0.2841535  0.28460965 0.28454497 0.28426892 0.28409377
 0.283995   0.28408396 0.2840363  0.2838164  0.2839076  0.28364697
 0.28368706 0.2836213  0.28348118 0.283411   0.2832901  0.2835085
 0.28334165 0.2834116  0.28384295 0.28403538 0.2838678  0.28344584
 0.28487906 0.28543508 0.28527126 0.2849201  0.28482616 0.28484097
 0.28446552 0.2841479  0.28415692 0.28408116 0.28434572 0.2844213
 0.28418213 0.28433865 0.28468195 0.28469327 0.28412846 0.28408715
 0.28456262 0.28453767 0.28433198 0.2843034  0.28441274 0.28439963
 0.28460616 0.28476056 0.28483626 0.28464073 0.28456593 0.28438777
 0.2842766  0.28444386 0.28448611 0.28433818 0.28411576 0.2839975
 0.28405884 0.2842531  0.28443086 0.28451183 0.2843101  0.28394133
 0.283942   0.2839603  0.28420454 0.2840991  0.28363475 0.2836438
 0.2835126  0.28372812 0.2840151  0.2836815  0.28379345 0.28372046
 0.28352794 0.28372702 0.28370768 0.28367636 0.28367683 0.28368777
 0.28372836 0.28346053 0.28347725 0.2838886  0.28414193 0.28399235
 0.28369984 0.2837124  0.28362253 0.28330994 0.28356904 0.28385884
 0.28339827 0.28342268 0.28373095 0.28386077 0.28378934 0.2834734
 0.28352708 0.28361183 0.28347093 0.2834903  0.28334475 0.2832632
 0.28348064 0.28371125 0.28374338 0.28365374 0.28373468 0.2835443
 0.28324685 0.2832522  0.2831876  0.28305265 0.28317153 0.28327778
 0.28335145 0.28370938 0.28365773 0.28342673 0.2835578  0.28369403
 0.28356823 0.2837143  0.28404754 0.28395024 0.28369907 0.28372762
 0.2839241  0.28411385 0.2839317  0.28362852 0.28372702 0.28375536
 0.28368488 0.2837554  0.2836259  0.28359407 0.28371733 0.2837587
 0.2839881  0.28399172 0.28415447 0.2841662  0.2839255  0.28415644
 0.2842801  0.28436157 0.2846211  0.28450498 0.28440133 0.28445867
 0.28432414 0.2841516  0.28407228 0.28431842 0.28438967 0.2841389
 0.28434685 0.2844653  0.28434223 0.28433204 0.28423765 0.284318
 0.28546405 0.28564835 0.28567237 0.28538445 0.2852457  0.28553545
 0.28545895 0.2850751  0.28502926 0.28526044 0.28528363 0.28510448
 0.28508538 0.28508615 0.28506434 0.285005   0.2847599  0.28454775
 0.28445527 0.28438577 0.28442135 0.2844265  0.28421685 0.28427085
 0.2851591  0.28539243 0.28568193 0.28555077 0.2850574  0.2850271
 0.28515622 0.28493658 0.28467354 0.28477257 0.28494728 0.28469774
 0.28442398 0.28447092 0.2846831  0.28480798 0.28438684 0.2841169
 0.28443506 0.28439093 0.28398964 0.28413966 0.28448647 0.28416553
 0.28402537 0.28415254 0.28419125 0.2841969  0.2844272  0.28448132
 0.28420395 0.2839712  0.28393984 0.2839087  0.28390142 0.28405032
 0.28399256 0.28375605 0.28353587 0.2833436  0.283601   0.2838333
 0.28366557 0.28346348 0.28353846 0.28356    0.28358257 0.2838985
 0.28387055 0.28365877 0.28366563 0.2836538  0.28386828 0.28384346
 0.28360555 0.2835914  0.283412   0.28299317 0.28266516 0.28270134
 0.28290436 0.2827673  0.28276163 0.28318554 0.28337798 0.28318927
 0.28304067 0.2830384  0.28295067 0.28302893 0.2833096  0.2833553
 0.28297138 0.28298956 0.28321576 0.28307524 0.28303152 0.28300178
 0.28295657 0.28302798 0.28282312 0.28265053 0.28237122 0.2821416
 0.28238076 0.2825639  0.28250393 0.2825159  0.28268534 0.2827493
 0.282826   0.28296515 0.28283507 0.28265953 0.2826831  0.28271234
 0.282771   0.28297627 0.28287145 0.28293303 0.28314534 0.28314874
 0.28278163 0.28263587 0.2826983  0.282558   0.28252465 0.28224087
 0.2822315  0.28269616 0.28268376 0.28264344 0.28306428 0.28334284
 0.28321767 0.28301442 0.28285807 0.28296226 0.28311893 0.28307414
 0.28342673 0.28357702 0.28352648 0.28338143 0.28316024 0.2833151
 0.28332874 0.28311512 0.28330898 0.28318116 0.28290394 0.28279814
 0.28282008 0.28296492 0.2827643  0.28321865 0.28342366 0.28357145
 0.2834601  0.2829704  0.28315604 0.28293872 0.28325704 0.28350407]
