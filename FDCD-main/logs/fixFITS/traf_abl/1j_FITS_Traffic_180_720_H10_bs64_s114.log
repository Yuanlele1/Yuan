Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_180_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_180_j720_H10_FITS_custom_ftM_sl180_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11381
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=90, out_features=450, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4468608000.0
params:  40950.0
Trainable parameters:  40950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 198.75536131858826
Epoch: 1, Steps: 88 | Train Loss: 1.4609018 Vali Loss: 1.1435735 Test Loss: 1.4146907
Validation loss decreased (inf --> 1.143574).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 222.05776524543762
Epoch: 2, Steps: 88 | Train Loss: 0.7366576 Vali Loss: 0.7543075 Test Loss: 0.9240959
Validation loss decreased (1.143574 --> 0.754308).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 219.49550080299377
Epoch: 3, Steps: 88 | Train Loss: 0.5278262 Vali Loss: 0.6156021 Test Loss: 0.7498742
Validation loss decreased (0.754308 --> 0.615602).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 207.24127507209778
Epoch: 4, Steps: 88 | Train Loss: 0.4449966 Vali Loss: 0.5526481 Test Loss: 0.6707577
Validation loss decreased (0.615602 --> 0.552648).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 192.57309865951538
Epoch: 5, Steps: 88 | Train Loss: 0.4040257 Vali Loss: 0.5184781 Test Loss: 0.6259154
Validation loss decreased (0.552648 --> 0.518478).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 175.2692346572876
Epoch: 6, Steps: 88 | Train Loss: 0.3794632 Vali Loss: 0.4963323 Test Loss: 0.5969155
Validation loss decreased (0.518478 --> 0.496332).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 171.82284235954285
Epoch: 7, Steps: 88 | Train Loss: 0.3630110 Vali Loss: 0.4813829 Test Loss: 0.5763121
Validation loss decreased (0.496332 --> 0.481383).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 172.65197587013245
Epoch: 8, Steps: 88 | Train Loss: 0.3514255 Vali Loss: 0.4702879 Test Loss: 0.5611629
Validation loss decreased (0.481383 --> 0.470288).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 167.72072458267212
Epoch: 9, Steps: 88 | Train Loss: 0.3427814 Vali Loss: 0.4618315 Test Loss: 0.5498090
Validation loss decreased (0.470288 --> 0.461832).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 157.2825710773468
Epoch: 10, Steps: 88 | Train Loss: 0.3363314 Vali Loss: 0.4554521 Test Loss: 0.5412758
Validation loss decreased (0.461832 --> 0.455452).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 149.07031106948853
Epoch: 11, Steps: 88 | Train Loss: 0.3313170 Vali Loss: 0.4504207 Test Loss: 0.5348365
Validation loss decreased (0.455452 --> 0.450421).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 152.66726875305176
Epoch: 12, Steps: 88 | Train Loss: 0.3275138 Vali Loss: 0.4470593 Test Loss: 0.5296409
Validation loss decreased (0.450421 --> 0.447059).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 156.02953147888184
Epoch: 13, Steps: 88 | Train Loss: 0.3246187 Vali Loss: 0.4440914 Test Loss: 0.5256496
Validation loss decreased (0.447059 --> 0.444091).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 160.08500862121582
Epoch: 14, Steps: 88 | Train Loss: 0.3221794 Vali Loss: 0.4422551 Test Loss: 0.5225453
Validation loss decreased (0.444091 --> 0.442255).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 138.7507266998291
Epoch: 15, Steps: 88 | Train Loss: 0.3202998 Vali Loss: 0.4403840 Test Loss: 0.5199937
Validation loss decreased (0.442255 --> 0.440384).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 135.33243250846863
Epoch: 16, Steps: 88 | Train Loss: 0.3188013 Vali Loss: 0.4386958 Test Loss: 0.5179434
Validation loss decreased (0.440384 --> 0.438696).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 110.6592025756836
Epoch: 17, Steps: 88 | Train Loss: 0.3175745 Vali Loss: 0.4376219 Test Loss: 0.5161370
Validation loss decreased (0.438696 --> 0.437622).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 113.20680594444275
Epoch: 18, Steps: 88 | Train Loss: 0.3164674 Vali Loss: 0.4364922 Test Loss: 0.5146751
Validation loss decreased (0.437622 --> 0.436492).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 110.98948740959167
Epoch: 19, Steps: 88 | Train Loss: 0.3155893 Vali Loss: 0.4353167 Test Loss: 0.5136504
Validation loss decreased (0.436492 --> 0.435317).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 113.12215685844421
Epoch: 20, Steps: 88 | Train Loss: 0.3148928 Vali Loss: 0.4349520 Test Loss: 0.5125189
Validation loss decreased (0.435317 --> 0.434952).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 127.86502003669739
Epoch: 21, Steps: 88 | Train Loss: 0.3143395 Vali Loss: 0.4346420 Test Loss: 0.5117373
Validation loss decreased (0.434952 --> 0.434642).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 127.19213938713074
Epoch: 22, Steps: 88 | Train Loss: 0.3139266 Vali Loss: 0.4339373 Test Loss: 0.5110670
Validation loss decreased (0.434642 --> 0.433937).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 112.65251278877258
Epoch: 23, Steps: 88 | Train Loss: 0.3135231 Vali Loss: 0.4333861 Test Loss: 0.5103447
Validation loss decreased (0.433937 --> 0.433386).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 110.33159065246582
Epoch: 24, Steps: 88 | Train Loss: 0.3130442 Vali Loss: 0.4325423 Test Loss: 0.5098582
Validation loss decreased (0.433386 --> 0.432542).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 106.09885668754578
Epoch: 25, Steps: 88 | Train Loss: 0.3127022 Vali Loss: 0.4327958 Test Loss: 0.5094203
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 104.51627492904663
Epoch: 26, Steps: 88 | Train Loss: 0.3124533 Vali Loss: 0.4323230 Test Loss: 0.5090040
Validation loss decreased (0.432542 --> 0.432323).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 106.31882357597351
Epoch: 27, Steps: 88 | Train Loss: 0.3121919 Vali Loss: 0.4320856 Test Loss: 0.5086303
Validation loss decreased (0.432323 --> 0.432086).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 95.12956976890564
Epoch: 28, Steps: 88 | Train Loss: 0.3119202 Vali Loss: 0.4319740 Test Loss: 0.5083679
Validation loss decreased (0.432086 --> 0.431974).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 89.53809642791748
Epoch: 29, Steps: 88 | Train Loss: 0.3117410 Vali Loss: 0.4320567 Test Loss: 0.5080512
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 95.05048131942749
Epoch: 30, Steps: 88 | Train Loss: 0.3115925 Vali Loss: 0.4312653 Test Loss: 0.5078604
Validation loss decreased (0.431974 --> 0.431265).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 144.29350805282593
Epoch: 31, Steps: 88 | Train Loss: 0.3114459 Vali Loss: 0.4314697 Test Loss: 0.5076584
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 147.0755569934845
Epoch: 32, Steps: 88 | Train Loss: 0.3112775 Vali Loss: 0.4311562 Test Loss: 0.5074225
Validation loss decreased (0.431265 --> 0.431156).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 141.71019768714905
Epoch: 33, Steps: 88 | Train Loss: 0.3112030 Vali Loss: 0.4310182 Test Loss: 0.5073636
Validation loss decreased (0.431156 --> 0.431018).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 142.55355858802795
Epoch: 34, Steps: 88 | Train Loss: 0.3111233 Vali Loss: 0.4312870 Test Loss: 0.5071024
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 141.17245292663574
Epoch: 35, Steps: 88 | Train Loss: 0.3110132 Vali Loss: 0.4308885 Test Loss: 0.5069883
Validation loss decreased (0.431018 --> 0.430888).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 141.81795191764832
Epoch: 36, Steps: 88 | Train Loss: 0.3109363 Vali Loss: 0.4308023 Test Loss: 0.5069098
Validation loss decreased (0.430888 --> 0.430802).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 139.33792424201965
Epoch: 37, Steps: 88 | Train Loss: 0.3108707 Vali Loss: 0.4311321 Test Loss: 0.5067923
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 150.22470784187317
Epoch: 38, Steps: 88 | Train Loss: 0.3107594 Vali Loss: 0.4308057 Test Loss: 0.5066795
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 148.0877456665039
Epoch: 39, Steps: 88 | Train Loss: 0.3106877 Vali Loss: 0.4307112 Test Loss: 0.5065288
Validation loss decreased (0.430802 --> 0.430711).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 138.2404088973999
Epoch: 40, Steps: 88 | Train Loss: 0.3107175 Vali Loss: 0.4308018 Test Loss: 0.5064723
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 140.0919065475464
Epoch: 41, Steps: 88 | Train Loss: 0.3105485 Vali Loss: 0.4307787 Test Loss: 0.5064338
EarlyStopping counter: 2 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 128.70463037490845
Epoch: 42, Steps: 88 | Train Loss: 0.3105316 Vali Loss: 0.4306146 Test Loss: 0.5063947
Validation loss decreased (0.430711 --> 0.430615).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 133.00039911270142
Epoch: 43, Steps: 88 | Train Loss: 0.3105205 Vali Loss: 0.4308096 Test Loss: 0.5063125
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 130.3390130996704
Epoch: 44, Steps: 88 | Train Loss: 0.3104804 Vali Loss: 0.4310020 Test Loss: 0.5063229
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 122.97032833099365
Epoch: 45, Steps: 88 | Train Loss: 0.3104607 Vali Loss: 0.4305843 Test Loss: 0.5062781
Validation loss decreased (0.430615 --> 0.430584).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 119.15311670303345
Epoch: 46, Steps: 88 | Train Loss: 0.3103933 Vali Loss: 0.4309844 Test Loss: 0.5061693
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 169.14214062690735
Epoch: 47, Steps: 88 | Train Loss: 0.3104509 Vali Loss: 0.4312661 Test Loss: 0.5061538
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 166.97921013832092
Epoch: 48, Steps: 88 | Train Loss: 0.3104423 Vali Loss: 0.4306257 Test Loss: 0.5060956
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 179.21353220939636
Epoch: 49, Steps: 88 | Train Loss: 0.3102479 Vali Loss: 0.4310710 Test Loss: 0.5061244
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 157.1996247768402
Epoch: 50, Steps: 88 | Train Loss: 0.3103686 Vali Loss: 0.4309021 Test Loss: 0.5061216
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 188.30213141441345
Epoch: 51, Steps: 88 | Train Loss: 0.3103584 Vali Loss: 0.4303782 Test Loss: 0.5060808
Validation loss decreased (0.430584 --> 0.430378).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 174.54675006866455
Epoch: 52, Steps: 88 | Train Loss: 0.3103292 Vali Loss: 0.4304220 Test Loss: 0.5060508
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 198.22856831550598
Epoch: 53, Steps: 88 | Train Loss: 0.3102564 Vali Loss: 0.4304472 Test Loss: 0.5059999
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 183.8921480178833
Epoch: 54, Steps: 88 | Train Loss: 0.3102730 Vali Loss: 0.4303204 Test Loss: 0.5059890
Validation loss decreased (0.430378 --> 0.430320).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 209.33940863609314
Epoch: 55, Steps: 88 | Train Loss: 0.3102770 Vali Loss: 0.4305859 Test Loss: 0.5059752
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 256.9589161872864
Epoch: 56, Steps: 88 | Train Loss: 0.3101739 Vali Loss: 0.4306505 Test Loss: 0.5059723
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 246.34280109405518
Epoch: 57, Steps: 88 | Train Loss: 0.3102034 Vali Loss: 0.4302284 Test Loss: 0.5059542
Validation loss decreased (0.430320 --> 0.430228).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 180.19159579277039
Epoch: 58, Steps: 88 | Train Loss: 0.3101534 Vali Loss: 0.4305501 Test Loss: 0.5059318
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 227.97409319877625
Epoch: 59, Steps: 88 | Train Loss: 0.3101541 Vali Loss: 0.4310371 Test Loss: 0.5059245
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 246.54756927490234
Epoch: 60, Steps: 88 | Train Loss: 0.3102587 Vali Loss: 0.4300746 Test Loss: 0.5058805
Validation loss decreased (0.430228 --> 0.430075).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 214.79774713516235
Epoch: 61, Steps: 88 | Train Loss: 0.3101722 Vali Loss: 0.4305379 Test Loss: 0.5058911
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 184.3045551776886
Epoch: 62, Steps: 88 | Train Loss: 0.3101996 Vali Loss: 0.4304084 Test Loss: 0.5058761
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 237.36277270317078
Epoch: 63, Steps: 88 | Train Loss: 0.3101123 Vali Loss: 0.4303644 Test Loss: 0.5058545
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 203.94091248512268
Epoch: 64, Steps: 88 | Train Loss: 0.3100417 Vali Loss: 0.4309836 Test Loss: 0.5058873
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 169.27645087242126
Epoch: 65, Steps: 88 | Train Loss: 0.3100561 Vali Loss: 0.4310423 Test Loss: 0.5058894
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 142.91021585464478
Epoch: 66, Steps: 88 | Train Loss: 0.3101380 Vali Loss: 0.4309317 Test Loss: 0.5058562
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 153.5833466053009
Epoch: 67, Steps: 88 | Train Loss: 0.3102155 Vali Loss: 0.4308749 Test Loss: 0.5058417
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 201.70063018798828
Epoch: 68, Steps: 88 | Train Loss: 0.3100308 Vali Loss: 0.4304597 Test Loss: 0.5058401
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 162.1807758808136
Epoch: 69, Steps: 88 | Train Loss: 0.3101290 Vali Loss: 0.4307545 Test Loss: 0.5058362
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 145.3308928012848
Epoch: 70, Steps: 88 | Train Loss: 0.3100359 Vali Loss: 0.4306577 Test Loss: 0.5058251
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_180_j720_H10_FITS_custom_ftM_sl180_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.5037949681282043, mae:0.3250792622566223, rse:0.5804004073143005, corr:[0.2519518  0.26574114 0.26244208 0.26476574 0.26406384 0.26581305
 0.26558656 0.2662911  0.26601326 0.26608482 0.26568955 0.26544595
 0.26480943 0.2645872  0.26445973 0.26436967 0.26428556 0.2642177
 0.26435706 0.26449543 0.26479265 0.2649799  0.26595315 0.26578233
 0.26618198 0.26556545 0.2654276  0.26514643 0.26503965 0.26521736
 0.26524535 0.26530355 0.26501894 0.26507935 0.26487678 0.26459277
 0.26411036 0.26411393 0.26405275 0.26426265 0.26430565 0.26440138
 0.26433337 0.2644215  0.264249   0.26432994 0.26465216 0.2645455
 0.26447225 0.26428464 0.2644328  0.26408976 0.26418847 0.26415625
 0.2643101  0.26420647 0.26412514 0.26409405 0.263924   0.26376772
 0.26353157 0.26366696 0.2635149  0.26366317 0.2635603  0.26358923
 0.2636473  0.26370892 0.26353922 0.26369202 0.263831   0.26375782
 0.2637671  0.26338387 0.2634003  0.26309258 0.26304784 0.26289803
 0.26299402 0.26307508 0.26306644 0.26313153 0.26317793 0.26306888
 0.26293126 0.26301792 0.2628543  0.26310265 0.26313037 0.26319844
 0.26310116 0.26293427 0.26293185 0.2630594  0.26300544 0.26292092
 0.26287293 0.26258582 0.26282227 0.26263553 0.2626824  0.2624897
 0.2624359  0.2624579  0.26238412 0.26248747 0.26244503 0.26251063
 0.2623944  0.26251215 0.26238194 0.26259407 0.26261488 0.2626584
 0.26259652 0.26263127 0.26260245 0.26242417 0.2624353  0.26260647
 0.2629381  0.26284215 0.26292855 0.26275617 0.26281995 0.26262215
 0.2625117  0.262516   0.2622927  0.2624632  0.26279533 0.26297936
 0.2633344  0.26355967 0.26339796 0.26342946 0.2632153  0.26289183
 0.2629144  0.26301262 0.26302907 0.26335055 0.26382947 0.26413918
 0.26391792 0.26363337 0.26359922 0.26347148 0.26361775 0.2635077
 0.26347563 0.263479   0.26330727 0.26345202 0.2641348  0.26497176
 0.26606724 0.2667085  0.26629362 0.26580092 0.26538807 0.26524168
 0.26531684 0.26569822 0.2660707  0.2668536  0.26761973 0.2682008
 0.26754975 0.26680517 0.26610488 0.26584437 0.26580665 0.2658777
 0.265954   0.26595533 0.266047   0.26612982 0.26635805 0.26633537
 0.2663788  0.26638222 0.266124   0.2661124  0.26595598 0.26613307
 0.26613688 0.26601982 0.26598212 0.2662569  0.2664868  0.26643798
 0.26602584 0.26579452 0.26561683 0.26537344 0.26543018 0.26556614
 0.26575765 0.26580715 0.2658865  0.26595706 0.26602957 0.26588175
 0.26565355 0.26547548 0.26541135 0.26555648 0.26554796 0.26565543
 0.26556054 0.26545268 0.2653628  0.2654589  0.2654713  0.26539746
 0.26520804 0.26529026 0.26535434 0.26516223 0.26526845 0.26523545
 0.2652608  0.26527214 0.26536953 0.26530734 0.26536962 0.2652322
 0.26510903 0.26516587 0.26512852 0.26503325 0.2648492  0.26487008
 0.26481432 0.2646842  0.26463297 0.26479924 0.2648785  0.2649779
 0.26496595 0.26491028 0.26488295 0.26474437 0.26488325 0.26485002
 0.26490796 0.26488978 0.26490456 0.26484472 0.26497778 0.26505125
 0.26497295 0.2649872  0.26478693 0.26475745 0.26485023 0.26492995
 0.26477185 0.26455653 0.26450697 0.26479155 0.26481095 0.26487416
 0.2647438  0.2646553  0.2648237  0.26466507 0.26474357 0.26460898
 0.2645181  0.26448786 0.26450735 0.2645145  0.26450032 0.26454455
 0.26457864 0.26461115 0.26444468 0.26443255 0.26426184 0.2642789
 0.26431677 0.2642903  0.26410514 0.26399326 0.2638544  0.26412895
 0.26440135 0.26442447 0.26464528 0.26449564 0.26447684 0.26425627
 0.26419255 0.26402077 0.26399398 0.26411566 0.2643092  0.26452273
 0.26486948 0.26524356 0.26504287 0.26494035 0.26473576 0.26471996
 0.26482457 0.26485762 0.26485077 0.26525483 0.26563954 0.26572144
 0.26571143 0.2654845  0.26538268 0.2652501  0.26532674 0.2652621
 0.2651208  0.26504278 0.264958   0.26516864 0.2658378  0.26649207
 0.26751715 0.2678964  0.26744276 0.26700747 0.26652583 0.2662774
 0.26632825 0.26678848 0.26724228 0.26811713 0.26885977 0.26898152
 0.2681781  0.2675151  0.26700208 0.2666855  0.26657078 0.26664582
 0.26674092 0.26692516 0.26707453 0.26717654 0.26734352 0.2673996
 0.26726684 0.2671507  0.26683322 0.26675686 0.26667988 0.26659086
 0.26652345 0.26662132 0.2667042  0.26701653 0.26719338 0.26700985
 0.26657507 0.26633254 0.26620066 0.2661877  0.2662373  0.26626498
 0.26638556 0.266572   0.26671046 0.26673442 0.2668217  0.26669952
 0.26643202 0.26629448 0.26614884 0.26620224 0.2662019  0.26627174
 0.26618817 0.2660717  0.2658701  0.26599202 0.26594076 0.2658615
 0.26576898 0.26570034 0.26575562 0.26572642 0.26578376 0.2658166
 0.26602024 0.26599827 0.2661093  0.26619232 0.26617983 0.26609966
 0.26595542 0.26596832 0.2659157  0.2658657  0.2656864  0.2656163
 0.26545092 0.26541704 0.2654103  0.26541457 0.26532456 0.26539946
 0.26532477 0.2653887  0.26529402 0.2652254  0.26524535 0.26501375
 0.26518914 0.26532832 0.26540905 0.26539198 0.26542768 0.26537728
 0.26533648 0.265393   0.26528853 0.26527864 0.26526862 0.26523942
 0.26504582 0.26498732 0.2649061  0.26506105 0.26508737 0.2651916
 0.2651594  0.26501432 0.2652116  0.26500627 0.26502356 0.2650174
 0.2649002  0.26502028 0.26516163 0.26504305 0.26509053 0.2650839
 0.2650493  0.26520884 0.26497254 0.26484093 0.26482493 0.26475334
 0.2646406  0.2647064  0.26440257 0.26413953 0.26428536 0.26456994
 0.2648157  0.264939   0.2650371  0.26476422 0.26474896 0.2646529
 0.26461476 0.2645792  0.26444083 0.26444674 0.2647431  0.26500207
 0.26529858 0.26552773 0.2651372  0.26486987 0.26472232 0.2647818
 0.26482317 0.2649853  0.2649837  0.26526567 0.26554143 0.2656604
 0.26564184 0.26536784 0.26546088 0.26541144 0.2655038  0.26548705
 0.26528186 0.2650948  0.26504385 0.26526105 0.26590794 0.26655465
 0.26760143 0.26807946 0.2674918  0.26696566 0.26644364 0.2661425
 0.2662144  0.26649514 0.2668973  0.26773545 0.2683028  0.26850468
 0.26791173 0.26739055 0.26678574 0.26632878 0.26637957 0.26639855
 0.2664431  0.26654416 0.26655114 0.26666325 0.26701143 0.26702282
 0.26704666 0.26693615 0.2665515  0.26644906 0.26610047 0.26592287
 0.26586804 0.26595524 0.26589814 0.2661788  0.26653567 0.26650086
 0.26642677 0.26626974 0.2660235  0.26604113 0.26619875 0.26633814
 0.26644    0.26656225 0.2667871  0.266926   0.2668642  0.26674917
 0.26633492 0.26588547 0.26584503 0.2659311  0.26588467 0.26577765
 0.2656235  0.26561472 0.26548478 0.26560992 0.2656963  0.2657177
 0.2657219  0.26578924 0.26588893 0.26574245 0.26579326 0.26589406
 0.2660656  0.26615903 0.26629326 0.2660888  0.26600212 0.26587677
 0.26561734 0.26547754 0.2653751  0.26528522 0.265085   0.26486468
 0.26475713 0.264875   0.2647437  0.26489958 0.26497567 0.26494274
 0.26500824 0.26500478 0.26490584 0.26486158 0.264938   0.2649436
 0.26501623 0.26493075 0.2648411  0.26468438 0.26457787 0.2643474
 0.26416248 0.26404732 0.26381883 0.26382193 0.2638577  0.26368877
 0.26344842 0.26354408 0.26354164 0.26374963 0.26380253 0.2640615
 0.26405114 0.26378992 0.26395243 0.263858   0.26393193 0.2638333
 0.26368615 0.263748   0.26355445 0.26362532 0.26366356 0.26331887
 0.26309505 0.2629951  0.26283118 0.26284632 0.26281768 0.26291084
 0.26290253 0.26307905 0.26294908 0.26294503 0.26293325 0.26323012
 0.2636072  0.26347706 0.26351282 0.2633252  0.26334682 0.26338974
 0.26309136 0.26284721 0.26261172 0.262621   0.26291507 0.26291722
 0.26323137 0.26349896 0.26317018 0.26310498 0.26289374 0.26279175
 0.26284817 0.26320434 0.263388   0.26382816 0.264169   0.26465443
 0.2646464  0.2642858  0.2643027  0.26411682 0.26428804 0.26418495
 0.26386812 0.26361746 0.26343858 0.26348943 0.2639402  0.264167
 0.26495025 0.26557294 0.26504353 0.26452723 0.26402968 0.26398677
 0.2641885  0.26448575 0.26501527 0.26586282 0.2665348  0.26717424
 0.26650247 0.26601344 0.2654692  0.26479807 0.2648022  0.26496676
 0.2650022  0.26494783 0.26491424 0.2650239  0.26544112 0.26532242
 0.26546565 0.26559058 0.26558748 0.2653833  0.2653965  0.2652833
 0.2655664  0.26577872 0.26585335 0.26623818 0.26641405 0.2668115
 0.2664224  0.2664635  0.26641414 0.26599762 0.26596415 0.2660478
 0.26615846 0.26631784 0.26622388 0.26609018 0.26599127 0.2658618
 0.26602206 0.26609206 0.26643315 0.2662632  0.2664388  0.26582715
 0.26611337 0.26463577 0.2650592  0.2630799  0.26537383 0.26555246]
