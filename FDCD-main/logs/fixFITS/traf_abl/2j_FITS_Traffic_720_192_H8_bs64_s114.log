Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_720_j192_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_720_j192_H8_FITS_custom_ftM_sl720_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9280140288.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 133.8049521446228
Epoch: 1, Steps: 88 | Train Loss: 1.0522008 Vali Loss: 1.1144255 Test Loss: 1.2839990
Validation loss decreased (inf --> 1.114426).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 139.14134430885315
Epoch: 2, Steps: 88 | Train Loss: 0.7865706 Vali Loss: 0.9845249 Test Loss: 1.1334008
Validation loss decreased (1.114426 --> 0.984525).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 273.55190443992615
Epoch: 3, Steps: 88 | Train Loss: 0.6870537 Vali Loss: 0.9196377 Test Loss: 1.0584286
Validation loss decreased (0.984525 --> 0.919638).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 274.6181004047394
Epoch: 4, Steps: 88 | Train Loss: 0.6177072 Vali Loss: 0.8754579 Test Loss: 1.0075614
Validation loss decreased (0.919638 --> 0.875458).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 177.55566501617432
Epoch: 5, Steps: 88 | Train Loss: 0.5615694 Vali Loss: 0.8300891 Test Loss: 0.9565169
Validation loss decreased (0.875458 --> 0.830089).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 112.22529792785645
Epoch: 6, Steps: 88 | Train Loss: 0.5144151 Vali Loss: 0.7901347 Test Loss: 0.9119132
Validation loss decreased (0.830089 --> 0.790135).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 111.9953384399414
Epoch: 7, Steps: 88 | Train Loss: 0.4738081 Vali Loss: 0.7582349 Test Loss: 0.8751740
Validation loss decreased (0.790135 --> 0.758235).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 112.73901987075806
Epoch: 8, Steps: 88 | Train Loss: 0.4385315 Vali Loss: 0.7285273 Test Loss: 0.8410251
Validation loss decreased (0.758235 --> 0.728527).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 110.13872694969177
Epoch: 9, Steps: 88 | Train Loss: 0.4077260 Vali Loss: 0.6958749 Test Loss: 0.8040025
Validation loss decreased (0.728527 --> 0.695875).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 96.40673995018005
Epoch: 10, Steps: 88 | Train Loss: 0.3804183 Vali Loss: 0.6733811 Test Loss: 0.7777857
Validation loss decreased (0.695875 --> 0.673381).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 96.60094952583313
Epoch: 11, Steps: 88 | Train Loss: 0.3561936 Vali Loss: 0.6496080 Test Loss: 0.7511883
Validation loss decreased (0.673381 --> 0.649608).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 112.5639591217041
Epoch: 12, Steps: 88 | Train Loss: 0.3347260 Vali Loss: 0.6292385 Test Loss: 0.7279176
Validation loss decreased (0.649608 --> 0.629238).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 101.18691349029541
Epoch: 13, Steps: 88 | Train Loss: 0.3153610 Vali Loss: 0.6112363 Test Loss: 0.7075686
Validation loss decreased (0.629238 --> 0.611236).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 116.63065838813782
Epoch: 14, Steps: 88 | Train Loss: 0.2980057 Vali Loss: 0.5948117 Test Loss: 0.6883388
Validation loss decreased (0.611236 --> 0.594812).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 105.63544058799744
Epoch: 15, Steps: 88 | Train Loss: 0.2823409 Vali Loss: 0.5765361 Test Loss: 0.6672631
Validation loss decreased (0.594812 --> 0.576536).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 105.90150237083435
Epoch: 16, Steps: 88 | Train Loss: 0.2682011 Vali Loss: 0.5609559 Test Loss: 0.6509688
Validation loss decreased (0.576536 --> 0.560956).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 121.8024468421936
Epoch: 17, Steps: 88 | Train Loss: 0.2554198 Vali Loss: 0.5498796 Test Loss: 0.6369638
Validation loss decreased (0.560956 --> 0.549880).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 89.73638391494751
Epoch: 18, Steps: 88 | Train Loss: 0.2436942 Vali Loss: 0.5379754 Test Loss: 0.6234167
Validation loss decreased (0.549880 --> 0.537975).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 95.65543818473816
Epoch: 19, Steps: 88 | Train Loss: 0.2330753 Vali Loss: 0.5265632 Test Loss: 0.6105605
Validation loss decreased (0.537975 --> 0.526563).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 99.15884685516357
Epoch: 20, Steps: 88 | Train Loss: 0.2233795 Vali Loss: 0.5163770 Test Loss: 0.5990188
Validation loss decreased (0.526563 --> 0.516377).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 89.35093927383423
Epoch: 21, Steps: 88 | Train Loss: 0.2145148 Vali Loss: 0.5053968 Test Loss: 0.5865143
Validation loss decreased (0.516377 --> 0.505397).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 101.88152146339417
Epoch: 22, Steps: 88 | Train Loss: 0.2063959 Vali Loss: 0.4966075 Test Loss: 0.5767707
Validation loss decreased (0.505397 --> 0.496608).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 90.60327434539795
Epoch: 23, Steps: 88 | Train Loss: 0.1988739 Vali Loss: 0.4899801 Test Loss: 0.5687779
Validation loss decreased (0.496608 --> 0.489980).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 96.13878726959229
Epoch: 24, Steps: 88 | Train Loss: 0.1920475 Vali Loss: 0.4829611 Test Loss: 0.5611361
Validation loss decreased (0.489980 --> 0.482961).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 101.97971296310425
Epoch: 25, Steps: 88 | Train Loss: 0.1857458 Vali Loss: 0.4749447 Test Loss: 0.5520929
Validation loss decreased (0.482961 --> 0.474945).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 81.63890933990479
Epoch: 26, Steps: 88 | Train Loss: 0.1798877 Vali Loss: 0.4687923 Test Loss: 0.5453764
Validation loss decreased (0.474945 --> 0.468792).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 92.87006759643555
Epoch: 27, Steps: 88 | Train Loss: 0.1744925 Vali Loss: 0.4629105 Test Loss: 0.5386717
Validation loss decreased (0.468792 --> 0.462911).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 110.56432223320007
Epoch: 28, Steps: 88 | Train Loss: 0.1694945 Vali Loss: 0.4573362 Test Loss: 0.5328805
Validation loss decreased (0.462911 --> 0.457336).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 107.94709372520447
Epoch: 29, Steps: 88 | Train Loss: 0.1648861 Vali Loss: 0.4518845 Test Loss: 0.5263142
Validation loss decreased (0.457336 --> 0.451885).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 122.12469983100891
Epoch: 30, Steps: 88 | Train Loss: 0.1605936 Vali Loss: 0.4476075 Test Loss: 0.5213535
Validation loss decreased (0.451885 --> 0.447607).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 119.12505269050598
Epoch: 31, Steps: 88 | Train Loss: 0.1565955 Vali Loss: 0.4423276 Test Loss: 0.5161403
Validation loss decreased (0.447607 --> 0.442328).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 112.38068747520447
Epoch: 32, Steps: 88 | Train Loss: 0.1528786 Vali Loss: 0.4381960 Test Loss: 0.5115584
Validation loss decreased (0.442328 --> 0.438196).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 114.92664766311646
Epoch: 33, Steps: 88 | Train Loss: 0.1494469 Vali Loss: 0.4344441 Test Loss: 0.5068959
Validation loss decreased (0.438196 --> 0.434444).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 121.9837794303894
Epoch: 34, Steps: 88 | Train Loss: 0.1462343 Vali Loss: 0.4304638 Test Loss: 0.5030791
Validation loss decreased (0.434444 --> 0.430464).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 119.18263983726501
Epoch: 35, Steps: 88 | Train Loss: 0.1432197 Vali Loss: 0.4274752 Test Loss: 0.4990774
Validation loss decreased (0.430464 --> 0.427475).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 115.35046291351318
Epoch: 36, Steps: 88 | Train Loss: 0.1404106 Vali Loss: 0.4245874 Test Loss: 0.4957579
Validation loss decreased (0.427475 --> 0.424587).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 110.42559623718262
Epoch: 37, Steps: 88 | Train Loss: 0.1377753 Vali Loss: 0.4210946 Test Loss: 0.4926632
Validation loss decreased (0.424587 --> 0.421095).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 145.1459219455719
Epoch: 38, Steps: 88 | Train Loss: 0.1353273 Vali Loss: 0.4186766 Test Loss: 0.4895306
Validation loss decreased (0.421095 --> 0.418677).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 129.46460103988647
Epoch: 39, Steps: 88 | Train Loss: 0.1329885 Vali Loss: 0.4165659 Test Loss: 0.4872550
Validation loss decreased (0.418677 --> 0.416566).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 164.28298830986023
Epoch: 40, Steps: 88 | Train Loss: 0.1308543 Vali Loss: 0.4136417 Test Loss: 0.4840170
Validation loss decreased (0.416566 --> 0.413642).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 133.65789031982422
Epoch: 41, Steps: 88 | Train Loss: 0.1288297 Vali Loss: 0.4106829 Test Loss: 0.4811910
Validation loss decreased (0.413642 --> 0.410683).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 146.98973846435547
Epoch: 42, Steps: 88 | Train Loss: 0.1269277 Vali Loss: 0.4091030 Test Loss: 0.4789906
Validation loss decreased (0.410683 --> 0.409103).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 141.58934903144836
Epoch: 43, Steps: 88 | Train Loss: 0.1251406 Vali Loss: 0.4072467 Test Loss: 0.4768260
Validation loss decreased (0.409103 --> 0.407247).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 142.76450371742249
Epoch: 44, Steps: 88 | Train Loss: 0.1234446 Vali Loss: 0.4048382 Test Loss: 0.4744468
Validation loss decreased (0.407247 --> 0.404838).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 146.38060569763184
Epoch: 45, Steps: 88 | Train Loss: 0.1218778 Vali Loss: 0.4029383 Test Loss: 0.4725533
Validation loss decreased (0.404838 --> 0.402938).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 133.88731455802917
Epoch: 46, Steps: 88 | Train Loss: 0.1203768 Vali Loss: 0.4011290 Test Loss: 0.4708331
Validation loss decreased (0.402938 --> 0.401129).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 126.78742980957031
Epoch: 47, Steps: 88 | Train Loss: 0.1189609 Vali Loss: 0.3997863 Test Loss: 0.4688669
Validation loss decreased (0.401129 --> 0.399786).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 134.6836597919464
Epoch: 48, Steps: 88 | Train Loss: 0.1176623 Vali Loss: 0.3984997 Test Loss: 0.4673057
Validation loss decreased (0.399786 --> 0.398500).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 133.5449686050415
Epoch: 49, Steps: 88 | Train Loss: 0.1164075 Vali Loss: 0.3966949 Test Loss: 0.4659147
Validation loss decreased (0.398500 --> 0.396695).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 129.2134349346161
Epoch: 50, Steps: 88 | Train Loss: 0.1152258 Vali Loss: 0.3954047 Test Loss: 0.4645331
Validation loss decreased (0.396695 --> 0.395405).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 126.12399649620056
Epoch: 51, Steps: 88 | Train Loss: 0.1141075 Vali Loss: 0.3935129 Test Loss: 0.4629169
Validation loss decreased (0.395405 --> 0.393513).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 133.5247654914856
Epoch: 52, Steps: 88 | Train Loss: 0.1130583 Vali Loss: 0.3927973 Test Loss: 0.4617448
Validation loss decreased (0.393513 --> 0.392797).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 125.90868711471558
Epoch: 53, Steps: 88 | Train Loss: 0.1120738 Vali Loss: 0.3913437 Test Loss: 0.4603511
Validation loss decreased (0.392797 --> 0.391344).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 126.26369643211365
Epoch: 54, Steps: 88 | Train Loss: 0.1111121 Vali Loss: 0.3904100 Test Loss: 0.4592119
Validation loss decreased (0.391344 --> 0.390410).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 130.25693821907043
Epoch: 55, Steps: 88 | Train Loss: 0.1102346 Vali Loss: 0.3897450 Test Loss: 0.4582317
Validation loss decreased (0.390410 --> 0.389745).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 128.83730697631836
Epoch: 56, Steps: 88 | Train Loss: 0.1093817 Vali Loss: 0.3882990 Test Loss: 0.4569429
Validation loss decreased (0.389745 --> 0.388299).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 126.96665596961975
Epoch: 57, Steps: 88 | Train Loss: 0.1086019 Vali Loss: 0.3871275 Test Loss: 0.4559861
Validation loss decreased (0.388299 --> 0.387128).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 131.1530623435974
Epoch: 58, Steps: 88 | Train Loss: 0.1078479 Vali Loss: 0.3867164 Test Loss: 0.4551384
Validation loss decreased (0.387128 --> 0.386716).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 106.82338523864746
Epoch: 59, Steps: 88 | Train Loss: 0.1071231 Vali Loss: 0.3858764 Test Loss: 0.4541541
Validation loss decreased (0.386716 --> 0.385876).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 101.5526671409607
Epoch: 60, Steps: 88 | Train Loss: 0.1064764 Vali Loss: 0.3855754 Test Loss: 0.4533057
Validation loss decreased (0.385876 --> 0.385575).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 104.88444066047668
Epoch: 61, Steps: 88 | Train Loss: 0.1058092 Vali Loss: 0.3840971 Test Loss: 0.4524260
Validation loss decreased (0.385575 --> 0.384097).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 89.75367045402527
Epoch: 62, Steps: 88 | Train Loss: 0.1051902 Vali Loss: 0.3836136 Test Loss: 0.4517409
Validation loss decreased (0.384097 --> 0.383614).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 120.04311347007751
Epoch: 63, Steps: 88 | Train Loss: 0.1046221 Vali Loss: 0.3825597 Test Loss: 0.4510692
Validation loss decreased (0.383614 --> 0.382560).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 134.01047587394714
Epoch: 64, Steps: 88 | Train Loss: 0.1040653 Vali Loss: 0.3822879 Test Loss: 0.4503441
Validation loss decreased (0.382560 --> 0.382288).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 127.96527528762817
Epoch: 65, Steps: 88 | Train Loss: 0.1035330 Vali Loss: 0.3813419 Test Loss: 0.4496339
Validation loss decreased (0.382288 --> 0.381342).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 97.50987577438354
Epoch: 66, Steps: 88 | Train Loss: 0.1030432 Vali Loss: 0.3815193 Test Loss: 0.4491215
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 137.35140013694763
Epoch: 67, Steps: 88 | Train Loss: 0.1025949 Vali Loss: 0.3801574 Test Loss: 0.4485426
Validation loss decreased (0.381342 --> 0.380157).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 115.01433038711548
Epoch: 68, Steps: 88 | Train Loss: 0.1021552 Vali Loss: 0.3800202 Test Loss: 0.4479916
Validation loss decreased (0.380157 --> 0.380020).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 116.23383522033691
Epoch: 69, Steps: 88 | Train Loss: 0.1017255 Vali Loss: 0.3794291 Test Loss: 0.4474126
Validation loss decreased (0.380020 --> 0.379429).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 111.67267322540283
Epoch: 70, Steps: 88 | Train Loss: 0.1013388 Vali Loss: 0.3790435 Test Loss: 0.4469640
Validation loss decreased (0.379429 --> 0.379044).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 114.4300799369812
Epoch: 71, Steps: 88 | Train Loss: 0.1009572 Vali Loss: 0.3788632 Test Loss: 0.4464431
Validation loss decreased (0.379044 --> 0.378863).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 115.54771494865417
Epoch: 72, Steps: 88 | Train Loss: 0.1005913 Vali Loss: 0.3775322 Test Loss: 0.4460484
Validation loss decreased (0.378863 --> 0.377532).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 110.21375179290771
Epoch: 73, Steps: 88 | Train Loss: 0.1002574 Vali Loss: 0.3776941 Test Loss: 0.4455793
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 114.71719288825989
Epoch: 74, Steps: 88 | Train Loss: 0.0999466 Vali Loss: 0.3772259 Test Loss: 0.4451839
Validation loss decreased (0.377532 --> 0.377226).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 115.15103960037231
Epoch: 75, Steps: 88 | Train Loss: 0.0996015 Vali Loss: 0.3768385 Test Loss: 0.4447959
Validation loss decreased (0.377226 --> 0.376838).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 106.44370484352112
Epoch: 76, Steps: 88 | Train Loss: 0.0993505 Vali Loss: 0.3762661 Test Loss: 0.4443771
Validation loss decreased (0.376838 --> 0.376266).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 104.98094725608826
Epoch: 77, Steps: 88 | Train Loss: 0.0990565 Vali Loss: 0.3758572 Test Loss: 0.4440567
Validation loss decreased (0.376266 --> 0.375857).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 105.79646706581116
Epoch: 78, Steps: 88 | Train Loss: 0.0988021 Vali Loss: 0.3764310 Test Loss: 0.4436951
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 101.74692296981812
Epoch: 79, Steps: 88 | Train Loss: 0.0985159 Vali Loss: 0.3755058 Test Loss: 0.4434085
Validation loss decreased (0.375857 --> 0.375506).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 99.14652800559998
Epoch: 80, Steps: 88 | Train Loss: 0.0982984 Vali Loss: 0.3756566 Test Loss: 0.4431008
EarlyStopping counter: 1 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 95.41496205329895
Epoch: 81, Steps: 88 | Train Loss: 0.0980540 Vali Loss: 0.3747625 Test Loss: 0.4428307
Validation loss decreased (0.375506 --> 0.374763).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 147.38068675994873
Epoch: 82, Steps: 88 | Train Loss: 0.0978488 Vali Loss: 0.3743008 Test Loss: 0.4426114
Validation loss decreased (0.374763 --> 0.374301).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 134.03267884254456
Epoch: 83, Steps: 88 | Train Loss: 0.0976365 Vali Loss: 0.3743088 Test Loss: 0.4423369
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 89.07408165931702
Epoch: 84, Steps: 88 | Train Loss: 0.0974542 Vali Loss: 0.3739069 Test Loss: 0.4420672
Validation loss decreased (0.374301 --> 0.373907).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 129.64966750144958
Epoch: 85, Steps: 88 | Train Loss: 0.0972617 Vali Loss: 0.3739871 Test Loss: 0.4418836
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 90.70119524002075
Epoch: 86, Steps: 88 | Train Loss: 0.0970800 Vali Loss: 0.3738694 Test Loss: 0.4416442
Validation loss decreased (0.373907 --> 0.373869).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 81.11357021331787
Epoch: 87, Steps: 88 | Train Loss: 0.0969245 Vali Loss: 0.3735975 Test Loss: 0.4414426
Validation loss decreased (0.373869 --> 0.373598).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 104.90944266319275
Epoch: 88, Steps: 88 | Train Loss: 0.0967868 Vali Loss: 0.3736096 Test Loss: 0.4412473
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 92.24629545211792
Epoch: 89, Steps: 88 | Train Loss: 0.0966153 Vali Loss: 0.3730659 Test Loss: 0.4410387
Validation loss decreased (0.373598 --> 0.373066).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 81.6953535079956
Epoch: 90, Steps: 88 | Train Loss: 0.0964837 Vali Loss: 0.3732577 Test Loss: 0.4408605
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 80.88189268112183
Epoch: 91, Steps: 88 | Train Loss: 0.0963449 Vali Loss: 0.3729964 Test Loss: 0.4407075
Validation loss decreased (0.373066 --> 0.372996).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 103.44890260696411
Epoch: 92, Steps: 88 | Train Loss: 0.0962129 Vali Loss: 0.3726635 Test Loss: 0.4405536
Validation loss decreased (0.372996 --> 0.372664).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 121.18411374092102
Epoch: 93, Steps: 88 | Train Loss: 0.0960807 Vali Loss: 0.3722938 Test Loss: 0.4403831
Validation loss decreased (0.372664 --> 0.372294).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 120.24991250038147
Epoch: 94, Steps: 88 | Train Loss: 0.0959618 Vali Loss: 0.3727449 Test Loss: 0.4402546
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 122.82879543304443
Epoch: 95, Steps: 88 | Train Loss: 0.0958570 Vali Loss: 0.3725387 Test Loss: 0.4401105
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 122.0191879272461
Epoch: 96, Steps: 88 | Train Loss: 0.0957562 Vali Loss: 0.3721908 Test Loss: 0.4399689
Validation loss decreased (0.372294 --> 0.372191).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 114.8725152015686
Epoch: 97, Steps: 88 | Train Loss: 0.0956575 Vali Loss: 0.3722476 Test Loss: 0.4398583
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 118.1579167842865
Epoch: 98, Steps: 88 | Train Loss: 0.0955522 Vali Loss: 0.3724540 Test Loss: 0.4397176
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 116.35456037521362
Epoch: 99, Steps: 88 | Train Loss: 0.0954749 Vali Loss: 0.3722661 Test Loss: 0.4396248
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 152.04724788665771
Epoch: 100, Steps: 88 | Train Loss: 0.0953950 Vali Loss: 0.3716504 Test Loss: 0.4395091
Validation loss decreased (0.372191 --> 0.371650).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 11369
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=258, out_features=326, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9280140288.0
params:  84434.0
Trainable parameters:  84434
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 114.49380087852478
Epoch: 1, Steps: 88 | Train Loss: 0.2457993 Vali Loss: 0.3308962 Test Loss: 0.4013677
Validation loss decreased (inf --> 0.330896).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 109.52966642379761
Epoch: 2, Steps: 88 | Train Loss: 0.2390070 Vali Loss: 0.3294868 Test Loss: 0.4005156
Validation loss decreased (0.330896 --> 0.329487).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 103.9518141746521
Epoch: 3, Steps: 88 | Train Loss: 0.2385600 Vali Loss: 0.3297796 Test Loss: 0.4006076
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00045125
Epoch: 4 cost time: 100.87523531913757
Epoch: 4, Steps: 88 | Train Loss: 0.2386264 Vali Loss: 0.3292641 Test Loss: 0.4003583
Validation loss decreased (0.329487 --> 0.329264).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 102.45355606079102
Epoch: 5, Steps: 88 | Train Loss: 0.2385550 Vali Loss: 0.3294342 Test Loss: 0.4011015
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 104.4516351222992
Epoch: 6, Steps: 88 | Train Loss: 0.2384889 Vali Loss: 0.3289499 Test Loss: 0.3999653
Validation loss decreased (0.329264 --> 0.328950).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 104.02681040763855
Epoch: 7, Steps: 88 | Train Loss: 0.2384566 Vali Loss: 0.3282934 Test Loss: 0.4001196
Validation loss decreased (0.328950 --> 0.328293).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 100.28024625778198
Epoch: 8, Steps: 88 | Train Loss: 0.2382808 Vali Loss: 0.3286074 Test Loss: 0.3999601
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 138.79294681549072
Epoch: 9, Steps: 88 | Train Loss: 0.2382103 Vali Loss: 0.3294266 Test Loss: 0.4004391
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 121.80136585235596
Epoch: 10, Steps: 88 | Train Loss: 0.2382855 Vali Loss: 0.3292682 Test Loss: 0.3999997
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 114.74306178092957
Epoch: 11, Steps: 88 | Train Loss: 0.2381257 Vali Loss: 0.3289780 Test Loss: 0.4002258
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 115.76364302635193
Epoch: 12, Steps: 88 | Train Loss: 0.2380844 Vali Loss: 0.3286768 Test Loss: 0.4001573
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 125.94268894195557
Epoch: 13, Steps: 88 | Train Loss: 0.2381791 Vali Loss: 0.3287789 Test Loss: 0.3999113
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 129.0540153980255
Epoch: 14, Steps: 88 | Train Loss: 0.2382189 Vali Loss: 0.3282686 Test Loss: 0.4000527
Validation loss decreased (0.328293 --> 0.328269).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 129.81646704673767
Epoch: 15, Steps: 88 | Train Loss: 0.2381868 Vali Loss: 0.3290846 Test Loss: 0.3999634
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 126.56309032440186
Epoch: 16, Steps: 88 | Train Loss: 0.2380447 Vali Loss: 0.3286519 Test Loss: 0.4000104
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 138.42024564743042
Epoch: 17, Steps: 88 | Train Loss: 0.2379216 Vali Loss: 0.3288702 Test Loss: 0.3999274
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 125.1454713344574
Epoch: 18, Steps: 88 | Train Loss: 0.2379806 Vali Loss: 0.3287251 Test Loss: 0.3997796
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 135.72678112983704
Epoch: 19, Steps: 88 | Train Loss: 0.2380005 Vali Loss: 0.3287759 Test Loss: 0.3997524
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 150.01169776916504
Epoch: 20, Steps: 88 | Train Loss: 0.2380177 Vali Loss: 0.3285712 Test Loss: 0.4000733
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 126.35949563980103
Epoch: 21, Steps: 88 | Train Loss: 0.2378587 Vali Loss: 0.3284237 Test Loss: 0.3994954
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 134.67698335647583
Epoch: 22, Steps: 88 | Train Loss: 0.2380197 Vali Loss: 0.3287078 Test Loss: 0.3998339
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 127.0081901550293
Epoch: 23, Steps: 88 | Train Loss: 0.2379368 Vali Loss: 0.3288567 Test Loss: 0.3995860
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 128.1130886077881
Epoch: 24, Steps: 88 | Train Loss: 0.2379280 Vali Loss: 0.3284602 Test Loss: 0.3993781
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_720_j192_H8_FITS_custom_ftM_sl720_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.3991008698940277, mae:0.275484174489975, rse:0.521399199962616, corr:[0.27824864 0.29049537 0.2910205  0.2902817  0.29040006 0.29090348
 0.2909042  0.29100767 0.29095837 0.2907706  0.29061174 0.290104
 0.28987473 0.29010248 0.29027864 0.2904792  0.29042855 0.2901478
 0.2901905  0.29019436 0.2898135  0.28963694 0.28955624 0.28939697
 0.290297   0.29056948 0.29047462 0.29009333 0.28996757 0.29041296
 0.29092205 0.29102197 0.2908438  0.29072502 0.2905836  0.29018068
 0.28984782 0.2899954  0.29026556 0.29027507 0.29023066 0.29031888
 0.290514   0.2905246  0.29029632 0.29037014 0.29071626 0.29091927
 0.29103446 0.29058954 0.28994504 0.28975344 0.2903054  0.29063618
 0.29019132 0.28953356 0.28889465 0.2883676  0.28834903 0.2886004
 0.28864533 0.28865415 0.28868005 0.28856322 0.28880703 0.28931123
 0.28943777 0.28959    0.2899931  0.2899498  0.28954327 0.28957433
 0.2895291  0.2892287  0.28927702 0.28931454 0.2889051  0.28883788
 0.28924024 0.28916472 0.28885174 0.288887   0.28871223 0.28827897
 0.28829226 0.28843966 0.288218   0.28796813 0.28801742 0.28826973
 0.2885227  0.28852925 0.2884063  0.2883701  0.28823    0.28804025
 0.2878293  0.28773305 0.28758365 0.28758994 0.2876607  0.2876589
 0.2878623  0.28790444 0.28737298 0.28698272 0.28717795 0.2872126
 0.28704947 0.2872431  0.28737164 0.28712383 0.28710228 0.2873136
 0.2873924  0.28770617 0.28805655 0.28772047 0.2873759  0.28770125
 0.2880841  0.28834197 0.28851798 0.2884415  0.28829476 0.28844982
 0.28885183 0.28889006 0.28845856 0.28811318 0.28805804 0.2877504
 0.2871897  0.28721413 0.2877718  0.28795233 0.28787458 0.2881655
 0.28831366 0.28788424 0.28776145 0.2883407  0.28880557 0.28882366
 0.2886998  0.28830165 0.2882003  0.2884903  0.2886076  0.288703
 0.28914067 0.28934044 0.28888744 0.2882184  0.28780666 0.28786516
 0.28817558 0.28832564 0.28843123 0.2886141  0.2887377  0.2891547
 0.28986192 0.28990525 0.28918853 0.2888669  0.2890039  0.28906494
 0.28984442 0.29024225 0.2906894  0.29073426 0.2905271  0.29023004
 0.28981555 0.28984103 0.29034498 0.2906222  0.29020172 0.28929377
 0.28894314 0.2893218  0.289404   0.28919077 0.2891767  0.28939435
 0.2899879  0.2900779  0.28965387 0.29004335 0.28936422 0.29019564]
