Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=38, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=38, out_features=190, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6469120.0
params:  7410.0
Trainable parameters:  7410
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7210328
	speed: 0.0197s/iter; left time: 512.9489s
	iters: 200, epoch: 1 | loss: 0.5750328
	speed: 0.0150s/iter; left time: 390.4393s
Epoch: 1 cost time: 4.403776407241821
Epoch: 1, Steps: 262 | Train Loss: 0.6961704 Vali Loss: 1.1409943 Test Loss: 0.5640067
Validation loss decreased (inf --> 1.140994).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5377673
	speed: 0.0744s/iter; left time: 1922.3447s
	iters: 200, epoch: 2 | loss: 0.5069568
	speed: 0.0144s/iter; left time: 370.1345s
Epoch: 2 cost time: 4.419426441192627
Epoch: 2, Steps: 262 | Train Loss: 0.4876189 Vali Loss: 1.0404944 Test Loss: 0.4801999
Validation loss decreased (1.140994 --> 1.040494).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5039925
	speed: 0.0660s/iter; left time: 1689.0160s
	iters: 200, epoch: 3 | loss: 0.4648501
	speed: 0.0147s/iter; left time: 375.7120s
Epoch: 3 cost time: 4.392899513244629
Epoch: 3, Steps: 262 | Train Loss: 0.4599093 Vali Loss: 1.0072056 Test Loss: 0.4570816
Validation loss decreased (1.040494 --> 1.007206).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4215486
	speed: 0.0673s/iter; left time: 1704.8518s
	iters: 200, epoch: 4 | loss: 0.4572614
	speed: 0.0152s/iter; left time: 383.6778s
Epoch: 4 cost time: 4.414189338684082
Epoch: 4, Steps: 262 | Train Loss: 0.4497976 Vali Loss: 0.9929111 Test Loss: 0.4485102
Validation loss decreased (1.007206 --> 0.992911).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4490421
	speed: 0.0662s/iter; left time: 1658.5694s
	iters: 200, epoch: 5 | loss: 0.4714077
	speed: 0.0144s/iter; left time: 358.5006s
Epoch: 5 cost time: 4.370425224304199
Epoch: 5, Steps: 262 | Train Loss: 0.4455147 Vali Loss: 0.9866618 Test Loss: 0.4457602
Validation loss decreased (0.992911 --> 0.986662).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4701058
	speed: 0.0684s/iter; left time: 1695.9144s
	iters: 200, epoch: 6 | loss: 0.4510411
	speed: 0.0148s/iter; left time: 366.0194s
Epoch: 6 cost time: 4.3234803676605225
Epoch: 6, Steps: 262 | Train Loss: 0.4438524 Vali Loss: 0.9836623 Test Loss: 0.4447814
Validation loss decreased (0.986662 --> 0.983662).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4515786
	speed: 0.0689s/iter; left time: 1689.1843s
	iters: 200, epoch: 7 | loss: 0.4453292
	speed: 0.0153s/iter; left time: 374.8497s
Epoch: 7 cost time: 4.519479990005493
Epoch: 7, Steps: 262 | Train Loss: 0.4431453 Vali Loss: 0.9822736 Test Loss: 0.4445180
Validation loss decreased (0.983662 --> 0.982274).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4587780
	speed: 0.0704s/iter; left time: 1708.1485s
	iters: 200, epoch: 8 | loss: 0.4497384
	speed: 0.0144s/iter; left time: 347.7995s
Epoch: 8 cost time: 4.498371601104736
Epoch: 8, Steps: 262 | Train Loss: 0.4428657 Vali Loss: 0.9813313 Test Loss: 0.4445579
Validation loss decreased (0.982274 --> 0.981331).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4418246
	speed: 0.0685s/iter; left time: 1645.1873s
	iters: 200, epoch: 9 | loss: 0.5072346
	speed: 0.0153s/iter; left time: 366.4662s
Epoch: 9 cost time: 4.467948913574219
Epoch: 9, Steps: 262 | Train Loss: 0.4427434 Vali Loss: 0.9806031 Test Loss: 0.4446307
Validation loss decreased (0.981331 --> 0.980603).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4711280
	speed: 0.0684s/iter; left time: 1623.6839s
	iters: 200, epoch: 10 | loss: 0.4315886
	speed: 0.0145s/iter; left time: 343.8538s
Epoch: 10 cost time: 4.448312520980835
Epoch: 10, Steps: 262 | Train Loss: 0.4426431 Vali Loss: 0.9794270 Test Loss: 0.4446947
Validation loss decreased (0.980603 --> 0.979427).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4888375
	speed: 0.0661s/iter; left time: 1552.1460s
	iters: 200, epoch: 11 | loss: 0.4205756
	speed: 0.0151s/iter; left time: 353.0393s
Epoch: 11 cost time: 4.48763632774353
Epoch: 11, Steps: 262 | Train Loss: 0.4425713 Vali Loss: 0.9793622 Test Loss: 0.4447061
Validation loss decreased (0.979427 --> 0.979362).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4969894
	speed: 0.0685s/iter; left time: 1589.5794s
	iters: 200, epoch: 12 | loss: 0.4207534
	speed: 0.0149s/iter; left time: 343.8335s
Epoch: 12 cost time: 4.33933687210083
Epoch: 12, Steps: 262 | Train Loss: 0.4425312 Vali Loss: 0.9802702 Test Loss: 0.4444685
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4485137
	speed: 0.0703s/iter; left time: 1613.2414s
	iters: 200, epoch: 13 | loss: 0.4548875
	speed: 0.0145s/iter; left time: 332.2634s
Epoch: 13 cost time: 4.558605670928955
Epoch: 13, Steps: 262 | Train Loss: 0.4424081 Vali Loss: 0.9804762 Test Loss: 0.4448837
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4492405
	speed: 0.0675s/iter; left time: 1532.4320s
	iters: 200, epoch: 14 | loss: 0.4345892
	speed: 0.0145s/iter; left time: 327.2702s
Epoch: 14 cost time: 4.421117305755615
Epoch: 14, Steps: 262 | Train Loss: 0.4424413 Vali Loss: 0.9804061 Test Loss: 0.4447124
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4199088
	speed: 0.0700s/iter; left time: 1570.9168s
	iters: 200, epoch: 15 | loss: 0.4487490
	speed: 0.0142s/iter; left time: 316.7520s
Epoch: 15 cost time: 4.298553943634033
Epoch: 15, Steps: 262 | Train Loss: 0.4423286 Vali Loss: 0.9802974 Test Loss: 0.4444890
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4247348
	speed: 0.0676s/iter; left time: 1499.1392s
	iters: 200, epoch: 16 | loss: 0.4637707
	speed: 0.0145s/iter; left time: 318.9973s
Epoch: 16 cost time: 4.4629576206207275
Epoch: 16, Steps: 262 | Train Loss: 0.4423797 Vali Loss: 0.9798692 Test Loss: 0.4448572
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3813366
	speed: 0.0683s/iter; left time: 1495.7476s
	iters: 200, epoch: 17 | loss: 0.4522394
	speed: 0.0146s/iter; left time: 317.5177s
Epoch: 17 cost time: 4.341937065124512
Epoch: 17, Steps: 262 | Train Loss: 0.4421134 Vali Loss: 0.9802083 Test Loss: 0.4447113
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4186869
	speed: 0.0674s/iter; left time: 1458.8238s
	iters: 200, epoch: 18 | loss: 0.4698948
	speed: 0.0140s/iter; left time: 301.2297s
Epoch: 18 cost time: 4.3530051708221436
Epoch: 18, Steps: 262 | Train Loss: 0.4423254 Vali Loss: 0.9804565 Test Loss: 0.4446186
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4095258
	speed: 0.0674s/iter; left time: 1441.6661s
	iters: 200, epoch: 19 | loss: 0.4334374
	speed: 0.0152s/iter; left time: 322.9175s
Epoch: 19 cost time: 4.454797744750977
Epoch: 19, Steps: 262 | Train Loss: 0.4426359 Vali Loss: 0.9802598 Test Loss: 0.4448710
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4330156
	speed: 0.0675s/iter; left time: 1426.8489s
	iters: 200, epoch: 20 | loss: 0.4636255
	speed: 0.0203s/iter; left time: 426.8319s
Epoch: 20 cost time: 4.958992004394531
Epoch: 20, Steps: 262 | Train Loss: 0.4422874 Vali Loss: 0.9799626 Test Loss: 0.4447501
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4167272
	speed: 0.0673s/iter; left time: 1403.7616s
	iters: 200, epoch: 21 | loss: 0.4511839
	speed: 0.0151s/iter; left time: 313.3672s
Epoch: 21 cost time: 4.344706296920776
Epoch: 21, Steps: 262 | Train Loss: 0.4422311 Vali Loss: 0.9801986 Test Loss: 0.4448445
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4606498
	speed: 0.0643s/iter; left time: 1324.4805s
	iters: 200, epoch: 22 | loss: 0.4580162
	speed: 0.0142s/iter; left time: 291.5394s
Epoch: 22 cost time: 4.196440935134888
Epoch: 22, Steps: 262 | Train Loss: 0.4423543 Vali Loss: 0.9798325 Test Loss: 0.4448501
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4591074
	speed: 0.0661s/iter; left time: 1343.9774s
	iters: 200, epoch: 23 | loss: 0.4238938
	speed: 0.0142s/iter; left time: 288.0480s
Epoch: 23 cost time: 4.316106081008911
Epoch: 23, Steps: 262 | Train Loss: 0.4421841 Vali Loss: 0.9797580 Test Loss: 0.4448908
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4415509
	speed: 0.0645s/iter; left time: 1294.1721s
	iters: 200, epoch: 24 | loss: 0.4377269
	speed: 0.0144s/iter; left time: 287.5584s
Epoch: 24 cost time: 4.207932949066162
Epoch: 24, Steps: 262 | Train Loss: 0.4422655 Vali Loss: 0.9796269 Test Loss: 0.4448766
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4468280
	speed: 0.0680s/iter; left time: 1347.7963s
	iters: 200, epoch: 25 | loss: 0.4551901
	speed: 0.0151s/iter; left time: 298.3066s
Epoch: 25 cost time: 5.417078733444214
Epoch: 25, Steps: 262 | Train Loss: 0.4423591 Vali Loss: 0.9794175 Test Loss: 0.4447969
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4525981
	speed: 0.0762s/iter; left time: 1489.6294s
	iters: 200, epoch: 26 | loss: 0.4457394
	speed: 0.0146s/iter; left time: 283.2838s
Epoch: 26 cost time: 4.4581029415130615
Epoch: 26, Steps: 262 | Train Loss: 0.4422266 Vali Loss: 0.9796065 Test Loss: 0.4448418
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4854016
	speed: 0.0681s/iter; left time: 1313.6009s
	iters: 200, epoch: 27 | loss: 0.4415059
	speed: 0.0159s/iter; left time: 305.7532s
Epoch: 27 cost time: 4.919919013977051
Epoch: 27, Steps: 262 | Train Loss: 0.4422231 Vali Loss: 0.9802663 Test Loss: 0.4449072
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4109207
	speed: 0.0839s/iter; left time: 1595.5522s
	iters: 200, epoch: 28 | loss: 0.4806697
	speed: 0.0154s/iter; left time: 291.6097s
Epoch: 28 cost time: 4.617354869842529
Epoch: 28, Steps: 262 | Train Loss: 0.4422569 Vali Loss: 0.9794941 Test Loss: 0.4448821
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4197189
	speed: 0.0729s/iter; left time: 1368.4730s
	iters: 200, epoch: 29 | loss: 0.4507565
	speed: 0.0146s/iter; left time: 272.6053s
Epoch: 29 cost time: 4.550734043121338
Epoch: 29, Steps: 262 | Train Loss: 0.4423318 Vali Loss: 0.9797017 Test Loss: 0.4448921
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4092872
	speed: 0.0651s/iter; left time: 1204.1154s
	iters: 200, epoch: 30 | loss: 0.4262823
	speed: 0.0201s/iter; left time: 370.7622s
Epoch: 30 cost time: 5.02627158164978
Epoch: 30, Steps: 262 | Train Loss: 0.4422870 Vali Loss: 0.9794164 Test Loss: 0.4448668
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4606071
	speed: 0.0714s/iter; left time: 1302.8580s
	iters: 200, epoch: 31 | loss: 0.4472221
	speed: 0.0161s/iter; left time: 292.5358s
Epoch: 31 cost time: 4.705026626586914
Epoch: 31, Steps: 262 | Train Loss: 0.4423373 Vali Loss: 0.9793445 Test Loss: 0.4448617
Validation loss decreased (0.979362 --> 0.979344).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4361355
	speed: 0.0675s/iter; left time: 1212.8912s
	iters: 200, epoch: 32 | loss: 0.4351051
	speed: 0.0144s/iter; left time: 257.8338s
Epoch: 32 cost time: 4.2296881675720215
Epoch: 32, Steps: 262 | Train Loss: 0.4421964 Vali Loss: 0.9805974 Test Loss: 0.4448849
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4532860
	speed: 0.0712s/iter; left time: 1261.4072s
	iters: 200, epoch: 33 | loss: 0.4630352
	speed: 0.0152s/iter; left time: 267.3847s
Epoch: 33 cost time: 4.96344780921936
Epoch: 33, Steps: 262 | Train Loss: 0.4422611 Vali Loss: 0.9802463 Test Loss: 0.4448993
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4735213
	speed: 0.0805s/iter; left time: 1404.6722s
	iters: 200, epoch: 34 | loss: 0.4464200
	speed: 0.0150s/iter; left time: 260.5632s
Epoch: 34 cost time: 6.094003677368164
Epoch: 34, Steps: 262 | Train Loss: 0.4421853 Vali Loss: 0.9787838 Test Loss: 0.4448965
Validation loss decreased (0.979344 --> 0.978784).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4600829
	speed: 0.0783s/iter; left time: 1345.9603s
	iters: 200, epoch: 35 | loss: 0.4009083
	speed: 0.0166s/iter; left time: 283.8400s
Epoch: 35 cost time: 4.842647075653076
Epoch: 35, Steps: 262 | Train Loss: 0.4422562 Vali Loss: 0.9792444 Test Loss: 0.4449132
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4216907
	speed: 0.0734s/iter; left time: 1242.7177s
	iters: 200, epoch: 36 | loss: 0.4380240
	speed: 0.0146s/iter; left time: 245.8969s
Epoch: 36 cost time: 4.4109954833984375
Epoch: 36, Steps: 262 | Train Loss: 0.4423657 Vali Loss: 0.9794029 Test Loss: 0.4448613
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4168929
	speed: 0.0692s/iter; left time: 1153.5116s
	iters: 200, epoch: 37 | loss: 0.4030407
	speed: 0.0152s/iter; left time: 251.7141s
Epoch: 37 cost time: 4.552006006240845
Epoch: 37, Steps: 262 | Train Loss: 0.4422782 Vali Loss: 0.9798830 Test Loss: 0.4449159
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4646392
	speed: 0.0686s/iter; left time: 1126.1620s
	iters: 200, epoch: 38 | loss: 0.4098444
	speed: 0.0143s/iter; left time: 233.0770s
Epoch: 38 cost time: 4.2857420444488525
Epoch: 38, Steps: 262 | Train Loss: 0.4422551 Vali Loss: 0.9796647 Test Loss: 0.4449175
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4237547
	speed: 0.0678s/iter; left time: 1094.8263s
	iters: 200, epoch: 39 | loss: 0.4227253
	speed: 0.0149s/iter; left time: 239.5625s
Epoch: 39 cost time: 4.339602947235107
Epoch: 39, Steps: 262 | Train Loss: 0.4421390 Vali Loss: 0.9792685 Test Loss: 0.4448707
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4354188
	speed: 0.0684s/iter; left time: 1087.0497s
	iters: 200, epoch: 40 | loss: 0.4225392
	speed: 0.0147s/iter; left time: 231.6521s
Epoch: 40 cost time: 4.296104192733765
Epoch: 40, Steps: 262 | Train Loss: 0.4423071 Vali Loss: 0.9802620 Test Loss: 0.4448988
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4571126
	speed: 0.0672s/iter; left time: 1050.1696s
	iters: 200, epoch: 41 | loss: 0.4790593
	speed: 0.0145s/iter; left time: 224.7746s
Epoch: 41 cost time: 4.2968971729278564
Epoch: 41, Steps: 262 | Train Loss: 0.4422207 Vali Loss: 0.9795353 Test Loss: 0.4449244
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4231261
	speed: 0.0640s/iter; left time: 982.7175s
	iters: 200, epoch: 42 | loss: 0.4322149
	speed: 0.0144s/iter; left time: 219.6142s
Epoch: 42 cost time: 4.2564921379089355
Epoch: 42, Steps: 262 | Train Loss: 0.4421906 Vali Loss: 0.9788785 Test Loss: 0.4449239
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4168972
	speed: 0.0658s/iter; left time: 993.6324s
	iters: 200, epoch: 43 | loss: 0.4497169
	speed: 0.0169s/iter; left time: 252.9018s
Epoch: 43 cost time: 4.626249551773071
Epoch: 43, Steps: 262 | Train Loss: 0.4422521 Vali Loss: 0.9793090 Test Loss: 0.4449104
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4843195
	speed: 0.0662s/iter; left time: 981.8908s
	iters: 200, epoch: 44 | loss: 0.4555188
	speed: 0.0141s/iter; left time: 207.9652s
Epoch: 44 cost time: 4.265680551528931
Epoch: 44, Steps: 262 | Train Loss: 0.4422266 Vali Loss: 0.9798257 Test Loss: 0.4448898
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4453162
	speed: 0.0654s/iter; left time: 953.6782s
	iters: 200, epoch: 45 | loss: 0.4528735
	speed: 0.0139s/iter; left time: 201.3141s
Epoch: 45 cost time: 4.206081390380859
Epoch: 45, Steps: 262 | Train Loss: 0.4423705 Vali Loss: 0.9793822 Test Loss: 0.4449376
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4248680
	speed: 0.0647s/iter; left time: 926.6302s
	iters: 200, epoch: 46 | loss: 0.4873876
	speed: 0.0140s/iter; left time: 199.4824s
Epoch: 46 cost time: 4.3101232051849365
Epoch: 46, Steps: 262 | Train Loss: 0.4422750 Vali Loss: 0.9800075 Test Loss: 0.4449154
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4497966
	speed: 0.0653s/iter; left time: 916.7073s
	iters: 200, epoch: 47 | loss: 0.4765766
	speed: 0.0148s/iter; left time: 206.6207s
Epoch: 47 cost time: 4.440900802612305
Epoch: 47, Steps: 262 | Train Loss: 0.4420675 Vali Loss: 0.9793155 Test Loss: 0.4449576
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4189156
	speed: 0.0653s/iter; left time: 900.8838s
	iters: 200, epoch: 48 | loss: 0.4515797
	speed: 0.0140s/iter; left time: 191.0844s
Epoch: 48 cost time: 4.213685512542725
Epoch: 48, Steps: 262 | Train Loss: 0.4423448 Vali Loss: 0.9793698 Test Loss: 0.4449432
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4104729
	speed: 0.0656s/iter; left time: 887.2623s
	iters: 200, epoch: 49 | loss: 0.4409554
	speed: 0.0137s/iter; left time: 184.4718s
Epoch: 49 cost time: 4.20710825920105
Epoch: 49, Steps: 262 | Train Loss: 0.4423087 Vali Loss: 0.9794198 Test Loss: 0.4449571
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.4267659
	speed: 0.0658s/iter; left time: 872.7710s
	iters: 200, epoch: 50 | loss: 0.4020412
	speed: 0.0150s/iter; left time: 197.6618s
Epoch: 50 cost time: 4.461025714874268
Epoch: 50, Steps: 262 | Train Loss: 0.4423089 Vali Loss: 0.9792708 Test Loss: 0.4449521
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4116492
	speed: 0.0690s/iter; left time: 897.3818s
	iters: 200, epoch: 51 | loss: 0.4425280
	speed: 0.0142s/iter; left time: 182.9398s
Epoch: 51 cost time: 4.38066029548645
Epoch: 51, Steps: 262 | Train Loss: 0.4421498 Vali Loss: 0.9804944 Test Loss: 0.4449422
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4321615
	speed: 0.0668s/iter; left time: 850.5612s
	iters: 200, epoch: 52 | loss: 0.4308445
	speed: 0.0144s/iter; left time: 181.9034s
Epoch: 52 cost time: 4.324473142623901
Epoch: 52, Steps: 262 | Train Loss: 0.4422126 Vali Loss: 0.9791896 Test Loss: 0.4449301
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4662061
	speed: 0.0647s/iter; left time: 807.7977s
	iters: 200, epoch: 53 | loss: 0.4278680
	speed: 0.0130s/iter; left time: 160.4847s
Epoch: 53 cost time: 4.053699016571045
Epoch: 53, Steps: 262 | Train Loss: 0.4421555 Vali Loss: 0.9795200 Test Loss: 0.4449322
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4419291
	speed: 0.0663s/iter; left time: 809.4100s
	iters: 200, epoch: 54 | loss: 0.4279632
	speed: 0.0143s/iter; left time: 173.1386s
Epoch: 54 cost time: 4.406952381134033
Epoch: 54, Steps: 262 | Train Loss: 0.4420502 Vali Loss: 0.9792570 Test Loss: 0.4449403
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4430757462978363, mae:0.4244329631328583, rse:0.6333004236221313, corr:[0.53195053 0.5320381  0.5296436  0.5298296  0.52928567 0.52722305
 0.5257638  0.5261029  0.5262994  0.5258091  0.525694   0.52618074
 0.52644044 0.5253523  0.5234399  0.521565   0.51992553 0.51824284
 0.5164144  0.51460093 0.5128354  0.5109591  0.50879365 0.5062817
 0.5036123  0.5011997  0.49931368 0.49776027 0.4964364  0.49549532
 0.4953599  0.49584925 0.4964445  0.4968845  0.49704048 0.4971287
 0.49693474 0.49635342 0.49581656 0.49549478 0.4954961  0.49540466
 0.49506813 0.49478444 0.49473345 0.4948599  0.4950141  0.49503824
 0.49505442 0.49515295 0.49538356 0.49556884 0.4956373  0.49564388
 0.49580172 0.49601278 0.49615642 0.49592912 0.49561337 0.4954445
 0.49555358 0.49565035 0.49546868 0.49512148 0.49486193 0.49497822
 0.49524492 0.49540344 0.495678   0.4961267  0.4966524  0.49696568
 0.49704108 0.49705347 0.4972629  0.49740845 0.4974113  0.49723202
 0.49699515 0.496839   0.49688205 0.49699846 0.49703878 0.49680772
 0.49650818 0.49638152 0.4963552  0.49611944 0.49570778 0.4956183
 0.49585986 0.4960383  0.49583033 0.49539798 0.49501932 0.49451193
 0.49356478 0.49223724 0.49101195 0.49039274 0.49036196 0.49052253
 0.4906449  0.49095756 0.49177033 0.49291724 0.4940176  0.4949769
 0.4959085  0.49670887 0.49712786 0.49717432 0.496944   0.49677828
 0.49673334 0.49670947 0.49632168 0.4955961  0.4949367  0.49448463
 0.49409857 0.49334174 0.4923824  0.49178895 0.49149698 0.49110004
 0.49049804 0.4899871  0.48985264 0.4899944  0.49004847 0.48984438
 0.48956856 0.48948297 0.48930925 0.48870727 0.4880382  0.48761657
 0.4875707  0.48774773 0.4877297  0.48745936 0.48726663 0.4874354
 0.48782268 0.48815402 0.48814043 0.48801914 0.48813146 0.48809946
 0.48791388 0.4877527  0.48775667 0.4879951  0.48813146 0.48811403
 0.48793823 0.48786184 0.48788875 0.4880895  0.4882773  0.48840475
 0.48856163 0.4887874  0.48898014 0.48922098 0.48942497 0.48960292
 0.48988977 0.49011713 0.4900659  0.48999104 0.48998466 0.49007335
 0.49021032 0.49019304 0.49004024 0.4898824  0.48971048 0.48963398
 0.48964602 0.4896923  0.48965624 0.4895818  0.4896235  0.48986307
 0.49013996 0.4902138  0.49006143 0.48989922 0.48989895 0.48996714
 0.48990443 0.4897201  0.4894824  0.48908895 0.48857853 0.48807526
 0.48758918 0.4870858  0.48658407 0.48639005 0.48635355 0.48639256
 0.4863754  0.48605838 0.48554558 0.484938   0.48425952 0.48359892
 0.48299357 0.48246333 0.48178762 0.48081052 0.4796278  0.47847575
 0.47744232 0.47645733 0.4756742  0.4751952  0.47483867 0.47448847
 0.47423252 0.47428638 0.47455207 0.47476742 0.474735   0.47437617
 0.47394675 0.47381768 0.47375125 0.47345278 0.4730243  0.47275835
 0.47275248 0.47289324 0.47280794 0.47276142 0.47295192 0.47324404
 0.47355822 0.47353417 0.4733951  0.4732863  0.47333372 0.47346073
 0.4735439  0.47348377 0.47349092 0.47363377 0.4737585  0.47373712
 0.47364467 0.4737307  0.47381872 0.47388092 0.47376502 0.4735895
 0.47368398 0.47400078 0.47427818 0.47443637 0.47452837 0.47477564
 0.4752481  0.47557575 0.4756621  0.47566578 0.4757811  0.47590604
 0.47603258 0.47604266 0.47615167 0.47640112 0.47676644 0.47703692
 0.4771514  0.47732592 0.4775579  0.47775516 0.477882   0.47794047
 0.47807154 0.4781644  0.4781549  0.4779252  0.47742587 0.47677422
 0.47603884 0.4753793  0.4748362  0.47426018 0.47369048 0.47316882
 0.47290927 0.47287104 0.47323406 0.4739481  0.47469237 0.47523805
 0.47549227 0.4754932  0.475458   0.475445   0.475404   0.47525775
 0.47505304 0.47479734 0.4744628  0.47407046 0.47364774 0.47324425
 0.4728835  0.4724865  0.47216055 0.47198078 0.47162136 0.47129187
 0.4710735  0.47088534 0.4708093  0.47094348 0.47093627 0.47070163
 0.47026214 0.4698017  0.46941862 0.46914297 0.46883708 0.4684968
 0.4682126  0.4681132  0.46803698 0.46789128 0.46775332 0.46767595
 0.46779445 0.46804816 0.46825042 0.46829945 0.46831232 0.46823004
 0.46815518 0.46819353 0.46838406 0.4685562  0.46858868 0.46859133
 0.46858203 0.4686368  0.4687285  0.4686973  0.46863446 0.4686434
 0.4687438  0.46897498 0.46915972 0.46935847 0.46954474 0.469699
 0.46974158 0.46978155 0.46975273 0.46971035 0.46968243 0.4696368
 0.4695397  0.46952868 0.46968412 0.46994516 0.47027412 0.47048303
 0.47061458 0.47079456 0.4711518  0.47155708 0.47189158 0.47220314
 0.4725107  0.47271812 0.47278255 0.47272915 0.47258386 0.47236812
 0.47200847 0.4714454  0.47092962 0.47050864 0.47016874 0.46999115
 0.46993276 0.47009918 0.4704883  0.4711841  0.4718764  0.47241884
 0.47278205 0.47301635 0.47315952 0.47322935 0.47309923 0.47283253
 0.4725642  0.47231403 0.47202402 0.47161    0.471038   0.4704543
 0.46994323 0.4694086  0.46893096 0.46863848 0.46854755 0.4685128
 0.4685085  0.4683797  0.46827576 0.4682065  0.4680543  0.46793917
 0.46768352 0.4674257  0.46713    0.46678314 0.46639964 0.46613395
 0.465933   0.4658977  0.46585333 0.4657722  0.46564206 0.46571812
 0.46588767 0.46611437 0.46625486 0.466298   0.4661761  0.4661977
 0.46625125 0.46624416 0.46616802 0.46608892 0.4660962  0.46621463
 0.466307   0.4663397  0.46631807 0.46645513 0.46662253 0.46661508
 0.46651745 0.4664938  0.46664366 0.46699664 0.46728244 0.46755776
 0.46771502 0.46770144 0.46755907 0.4674014  0.46715873 0.4669654
 0.46693516 0.46703953 0.46712747 0.4672486  0.46740732 0.46755278
 0.46771035 0.467891   0.46808124 0.4681028  0.46805662 0.46808806
 0.46811804 0.46801916 0.46770388 0.46724233 0.4666823  0.4659336
 0.46485403 0.4635599  0.46231475 0.4612458  0.46025053 0.45937324
 0.45877552 0.45839655 0.45836127 0.45834804 0.45819652 0.45802698
 0.4580415  0.45806912 0.45798442 0.4578173  0.4575588  0.45721886
 0.45685944 0.45651245 0.45606953 0.4555065  0.45498157 0.45449454
 0.4540585  0.45356035 0.4530929  0.4528708  0.45275408 0.45263693
 0.45245144 0.4523659  0.45245674 0.45271504 0.45280796 0.45261186
 0.45225683 0.45208248 0.45203555 0.45180625 0.45136267 0.45093358
 0.4506452  0.45058528 0.4505369  0.4503904  0.45030975 0.45038626
 0.45050377 0.45056528 0.4504458  0.45025066 0.45019618 0.45029846
 0.45043635 0.45044848 0.45049986 0.4505195  0.45051727 0.45049644
 0.45052263 0.4504763  0.45049906 0.450483   0.45038295 0.45029548
 0.4503374  0.45055392 0.45083126 0.45098284 0.45096618 0.45102996
 0.4512399  0.45141295 0.4514091  0.4513616  0.45136043 0.45151728
 0.45176128 0.45190367 0.45184198 0.45182404 0.45197725 0.45225626
 0.45240325 0.4525464  0.45264506 0.45276457 0.45289314 0.453007
 0.453012   0.45281273 0.4524824  0.45201072 0.451305   0.45020014
 0.44873926 0.44729128 0.4460993  0.44502407 0.44404072 0.44329622
 0.44292125 0.44287166 0.44304785 0.4433163  0.44374874 0.44428027
 0.44470927 0.4448071  0.44467956 0.44453493 0.44444713 0.44433916
 0.44405568 0.44366306 0.44327676 0.44290122 0.44253966 0.44205728
 0.44157586 0.44108722 0.44067717 0.44049236 0.44032082 0.44014344
 0.4401385  0.4404176  0.4406925  0.44071132 0.44051278 0.44038573
 0.4403369  0.4402286  0.44005987 0.43969426 0.43928713 0.4389109
 0.4387199  0.43863556 0.4386147  0.43871227 0.43894523 0.43915832
 0.43912604 0.43896183 0.43889025 0.4389765  0.43918112 0.4392874
 0.4391668  0.4390089  0.43896824 0.43897852 0.43896124 0.4387214
 0.4384258  0.4383805  0.43838432 0.43824673 0.4381269  0.4380303
 0.4380932  0.43824297 0.43834049 0.43848026 0.43879783 0.4393566
 0.43985972 0.43992737 0.43972963 0.43969676 0.43992457 0.4402368
 0.44036442 0.44035846 0.4404522  0.44079715 0.4412989  0.4416642
 0.44174564 0.4417624  0.4419052  0.44200137 0.44195354 0.44180188
 0.44175968 0.44194263 0.4420432  0.44168448 0.44087622 0.43999454
 0.43922192 0.43839103 0.43728235 0.43608692 0.43540046 0.43534538
 0.43551677 0.43569526 0.43597874 0.43658412 0.43732655 0.4380094
 0.4384698  0.4386503  0.4387175  0.43863505 0.43831524 0.43774334
 0.43713614 0.4366673  0.4363051  0.43576473 0.43495354 0.43433917
 0.4340039  0.4335434  0.43283933 0.43210703 0.43177027 0.4318142
 0.4318279  0.43135387 0.43072224 0.4305371  0.43060738 0.43056902
 0.43002677 0.42923027 0.42893195 0.42901373 0.42874846 0.42809963
 0.42818278 0.42948017 0.43058714 0.4306973  0.43154812 0.43430853]
