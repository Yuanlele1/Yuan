Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=38, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_336_FITS_ETTm1_ftM_sl180_ll48_pl336_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34045
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=38, out_features=108, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3677184.0
params:  4212.0
Trainable parameters:  4212
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4871940
	speed: 0.0179s/iter; left time: 471.9963s
	iters: 200, epoch: 1 | loss: 0.4173657
	speed: 0.0116s/iter; left time: 304.6430s
Epoch: 1 cost time: 3.6827094554901123
Epoch: 1, Steps: 265 | Train Loss: 0.5356306 Vali Loss: 0.7784727 Test Loss: 0.4762265
Validation loss decreased (inf --> 0.778473).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3982158
	speed: 0.0542s/iter; left time: 1416.0611s
	iters: 200, epoch: 2 | loss: 0.3879741
	speed: 0.0113s/iter; left time: 293.1246s
Epoch: 2 cost time: 3.4752678871154785
Epoch: 2, Steps: 265 | Train Loss: 0.4034563 Vali Loss: 0.7028105 Test Loss: 0.4096374
Validation loss decreased (0.778473 --> 0.702811).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3891793
	speed: 0.0540s/iter; left time: 1395.8509s
	iters: 200, epoch: 3 | loss: 0.4242191
	speed: 0.0113s/iter; left time: 290.1070s
Epoch: 3 cost time: 3.5072693824768066
Epoch: 3, Steps: 265 | Train Loss: 0.3828767 Vali Loss: 0.6800602 Test Loss: 0.3916922
Validation loss decreased (0.702811 --> 0.680060).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3575906
	speed: 0.0544s/iter; left time: 1391.8178s
	iters: 200, epoch: 4 | loss: 0.4159828
	speed: 0.0110s/iter; left time: 281.0609s
Epoch: 4 cost time: 3.3907864093780518
Epoch: 4, Steps: 265 | Train Loss: 0.3769500 Vali Loss: 0.6722692 Test Loss: 0.3864875
Validation loss decreased (0.680060 --> 0.672269).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3596610
	speed: 0.0537s/iter; left time: 1361.0933s
	iters: 200, epoch: 5 | loss: 0.3495196
	speed: 0.0110s/iter; left time: 276.9475s
Epoch: 5 cost time: 3.526747703552246
Epoch: 5, Steps: 265 | Train Loss: 0.3749730 Vali Loss: 0.6695535 Test Loss: 0.3845441
Validation loss decreased (0.672269 --> 0.669553).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4053741
	speed: 0.0557s/iter; left time: 1395.9259s
	iters: 200, epoch: 6 | loss: 0.3326220
	speed: 0.0110s/iter; left time: 275.3163s
Epoch: 6 cost time: 3.5788309574127197
Epoch: 6, Steps: 265 | Train Loss: 0.3742338 Vali Loss: 0.6676791 Test Loss: 0.3838803
Validation loss decreased (0.669553 --> 0.667679).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4008616
	speed: 0.0554s/iter; left time: 1375.5355s
	iters: 200, epoch: 7 | loss: 0.3452217
	speed: 0.0112s/iter; left time: 276.8054s
Epoch: 7 cost time: 3.5348634719848633
Epoch: 7, Steps: 265 | Train Loss: 0.3739852 Vali Loss: 0.6672431 Test Loss: 0.3837914
Validation loss decreased (0.667679 --> 0.667243).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3529753
	speed: 0.0542s/iter; left time: 1331.2598s
	iters: 200, epoch: 8 | loss: 0.3826832
	speed: 0.0109s/iter; left time: 266.1240s
Epoch: 8 cost time: 3.514479875564575
Epoch: 8, Steps: 265 | Train Loss: 0.3738707 Vali Loss: 0.6660248 Test Loss: 0.3838281
Validation loss decreased (0.667243 --> 0.666025).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3679951
	speed: 0.0540s/iter; left time: 1311.0574s
	iters: 200, epoch: 9 | loss: 0.3583432
	speed: 0.0112s/iter; left time: 269.9082s
Epoch: 9 cost time: 3.5076448917388916
Epoch: 9, Steps: 265 | Train Loss: 0.3737004 Vali Loss: 0.6660424 Test Loss: 0.3836191
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3858195
	speed: 0.0553s/iter; left time: 1327.4700s
	iters: 200, epoch: 10 | loss: 0.3475938
	speed: 0.0114s/iter; left time: 272.1885s
Epoch: 10 cost time: 3.5276830196380615
Epoch: 10, Steps: 265 | Train Loss: 0.3734984 Vali Loss: 0.6645522 Test Loss: 0.3837272
Validation loss decreased (0.666025 --> 0.664552).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3604587
	speed: 0.0547s/iter; left time: 1298.6009s
	iters: 200, epoch: 11 | loss: 0.3326708
	speed: 0.0113s/iter; left time: 266.4800s
Epoch: 11 cost time: 3.553410768508911
Epoch: 11, Steps: 265 | Train Loss: 0.3734557 Vali Loss: 0.6649765 Test Loss: 0.3835662
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4081815
	speed: 0.0539s/iter; left time: 1266.5551s
	iters: 200, epoch: 12 | loss: 0.3778485
	speed: 0.0112s/iter; left time: 260.8815s
Epoch: 12 cost time: 3.535048007965088
Epoch: 12, Steps: 265 | Train Loss: 0.3737807 Vali Loss: 0.6648280 Test Loss: 0.3835703
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3831038
	speed: 0.0546s/iter; left time: 1268.2860s
	iters: 200, epoch: 13 | loss: 0.3884365
	speed: 0.0107s/iter; left time: 248.5002s
Epoch: 13 cost time: 3.4052019119262695
Epoch: 13, Steps: 265 | Train Loss: 0.3735851 Vali Loss: 0.6656860 Test Loss: 0.3834294
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3275903
	speed: 0.0551s/iter; left time: 1264.9791s
	iters: 200, epoch: 14 | loss: 0.3917273
	speed: 0.0108s/iter; left time: 246.8178s
Epoch: 14 cost time: 3.613588809967041
Epoch: 14, Steps: 265 | Train Loss: 0.3735093 Vali Loss: 0.6650503 Test Loss: 0.3835376
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3881454
	speed: 0.0548s/iter; left time: 1243.5973s
	iters: 200, epoch: 15 | loss: 0.4456509
	speed: 0.0117s/iter; left time: 264.3765s
Epoch: 15 cost time: 3.6222007274627686
Epoch: 15, Steps: 265 | Train Loss: 0.3735283 Vali Loss: 0.6650634 Test Loss: 0.3835278
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3815428
	speed: 0.0576s/iter; left time: 1290.9095s
	iters: 200, epoch: 16 | loss: 0.3681021
	speed: 0.0112s/iter; left time: 249.8582s
Epoch: 16 cost time: 4.089048147201538
Epoch: 16, Steps: 265 | Train Loss: 0.3733762 Vali Loss: 0.6653309 Test Loss: 0.3834483
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4146209
	speed: 0.0606s/iter; left time: 1344.0036s
	iters: 200, epoch: 17 | loss: 0.3765211
	speed: 0.0105s/iter; left time: 232.1376s
Epoch: 17 cost time: 3.4311227798461914
Epoch: 17, Steps: 265 | Train Loss: 0.3734567 Vali Loss: 0.6658059 Test Loss: 0.3834680
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3954002
	speed: 0.0545s/iter; left time: 1194.2286s
	iters: 200, epoch: 18 | loss: 0.3564038
	speed: 0.0113s/iter; left time: 245.3840s
Epoch: 18 cost time: 3.6869988441467285
Epoch: 18, Steps: 265 | Train Loss: 0.3735285 Vali Loss: 0.6653405 Test Loss: 0.3835409
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3838173
	speed: 0.0575s/iter; left time: 1243.3851s
	iters: 200, epoch: 19 | loss: 0.3715487
	speed: 0.0110s/iter; left time: 235.8492s
Epoch: 19 cost time: 3.485653877258301
Epoch: 19, Steps: 265 | Train Loss: 0.3734518 Vali Loss: 0.6651011 Test Loss: 0.3834093
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3478953
	speed: 0.0555s/iter; left time: 1186.1762s
	iters: 200, epoch: 20 | loss: 0.4037265
	speed: 0.0114s/iter; left time: 243.2026s
Epoch: 20 cost time: 3.716808319091797
Epoch: 20, Steps: 265 | Train Loss: 0.3734924 Vali Loss: 0.6648229 Test Loss: 0.3835082
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3961722
	speed: 0.0568s/iter; left time: 1199.0580s
	iters: 200, epoch: 21 | loss: 0.3739559
	speed: 0.0124s/iter; left time: 260.0425s
Epoch: 21 cost time: 3.833858013153076
Epoch: 21, Steps: 265 | Train Loss: 0.3734527 Vali Loss: 0.6647363 Test Loss: 0.3833982
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3565570
	speed: 0.0566s/iter; left time: 1178.7313s
	iters: 200, epoch: 22 | loss: 0.3848566
	speed: 0.0117s/iter; left time: 242.5318s
Epoch: 22 cost time: 3.595278263092041
Epoch: 22, Steps: 265 | Train Loss: 0.3733237 Vali Loss: 0.6652907 Test Loss: 0.3836071
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3825619
	speed: 0.0560s/iter; left time: 1151.0357s
	iters: 200, epoch: 23 | loss: 0.3499950
	speed: 0.0114s/iter; left time: 232.3890s
Epoch: 23 cost time: 3.5253114700317383
Epoch: 23, Steps: 265 | Train Loss: 0.3733362 Vali Loss: 0.6650176 Test Loss: 0.3833890
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4079817
	speed: 0.0557s/iter; left time: 1130.7592s
	iters: 200, epoch: 24 | loss: 0.4190623
	speed: 0.0117s/iter; left time: 236.2548s
Epoch: 24 cost time: 3.7339324951171875
Epoch: 24, Steps: 265 | Train Loss: 0.3733533 Vali Loss: 0.6648735 Test Loss: 0.3836276
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4262698
	speed: 0.0572s/iter; left time: 1146.2228s
	iters: 200, epoch: 25 | loss: 0.3610165
	speed: 0.0122s/iter; left time: 242.4179s
Epoch: 25 cost time: 3.957451105117798
Epoch: 25, Steps: 265 | Train Loss: 0.3733927 Vali Loss: 0.6652929 Test Loss: 0.3835083
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3839162
	speed: 0.0567s/iter; left time: 1121.4807s
	iters: 200, epoch: 26 | loss: 0.3817309
	speed: 0.0117s/iter; left time: 230.7497s
Epoch: 26 cost time: 3.6794309616088867
Epoch: 26, Steps: 265 | Train Loss: 0.3731235 Vali Loss: 0.6658694 Test Loss: 0.3834845
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4000087
	speed: 0.0574s/iter; left time: 1120.1102s
	iters: 200, epoch: 27 | loss: 0.3663296
	speed: 0.0125s/iter; left time: 242.8895s
Epoch: 27 cost time: 3.883427858352661
Epoch: 27, Steps: 265 | Train Loss: 0.3733485 Vali Loss: 0.6652712 Test Loss: 0.3834196
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3556861
	speed: 0.0625s/iter; left time: 1203.2163s
	iters: 200, epoch: 28 | loss: 0.3615210
	speed: 0.0129s/iter; left time: 246.7177s
Epoch: 28 cost time: 3.963907480239868
Epoch: 28, Steps: 265 | Train Loss: 0.3733647 Vali Loss: 0.6654338 Test Loss: 0.3834745
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3909742
	speed: 0.0583s/iter; left time: 1107.0611s
	iters: 200, epoch: 29 | loss: 0.3548915
	speed: 0.0112s/iter; left time: 211.8702s
Epoch: 29 cost time: 3.5834550857543945
Epoch: 29, Steps: 265 | Train Loss: 0.3733559 Vali Loss: 0.6650079 Test Loss: 0.3834096
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3664381
	speed: 0.0558s/iter; left time: 1044.0942s
	iters: 200, epoch: 30 | loss: 0.3650178
	speed: 0.0113s/iter; left time: 211.1570s
Epoch: 30 cost time: 3.563823699951172
Epoch: 30, Steps: 265 | Train Loss: 0.3733384 Vali Loss: 0.6647696 Test Loss: 0.3835073
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_336_FITS_ETTm1_ftM_sl180_ll48_pl336_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.38371947407722473, mae:0.39013364911079407, rse:0.5894612073898315, corr:[0.5472002  0.5495878  0.5452999  0.5441205  0.5443418  0.5427965
 0.5407934  0.5408976  0.5420052  0.5420487  0.5416056  0.54166555
 0.5420997  0.5411678  0.5392     0.53766656 0.5366367  0.5352367
 0.53297067 0.53056806 0.52869976 0.527192   0.52532727 0.5226658
 0.519656   0.5172498  0.5159139  0.51491    0.51358753 0.5122172
 0.51186556 0.5126886  0.5136544  0.5140042  0.5138015  0.51385427
 0.51419693 0.51417947 0.51357824 0.5126755  0.5122949  0.5124363
 0.5125245  0.5122272  0.5118015  0.51181966 0.51234996 0.51276773
 0.5126603  0.51228935 0.51231515 0.51275843 0.5130378  0.51267016
 0.5120412  0.5118139  0.51222694 0.5125526  0.51231474 0.51167536
 0.5113621  0.51158196 0.5118168  0.5116172  0.51116425 0.51111895
 0.5115457  0.511928   0.5120397  0.51206684 0.5123667  0.51291025
 0.5133289  0.5133287  0.5131807  0.513118   0.5132736  0.51333547
 0.5130438  0.5125404  0.51226205 0.51228493 0.5123044  0.5119126
 0.51127714 0.5109047  0.5109603  0.51104146 0.5108043  0.51050866
 0.5104454  0.5106181  0.5106601  0.51027864 0.50951093 0.5086743
 0.5079892  0.5072846  0.506311   0.5053024  0.504923   0.50530034
 0.50581276 0.5059711  0.50612676 0.50691503 0.508375   0.5098516
 0.5108327  0.511444   0.51205957 0.5127051  0.5129137  0.5125796
 0.5120606  0.5119066  0.51194876 0.51166415 0.5108829  0.5099563
 0.50949866 0.5094213  0.50927854 0.50887406 0.50828826 0.5078144
 0.50757366 0.5073536  0.5070047  0.50674313 0.5067617  0.5068618
 0.5066998  0.5063006  0.50588316 0.50564057 0.50561863 0.5053905
 0.5048584  0.5045264  0.5046987  0.50516737 0.50541574 0.50523514
 0.50494015 0.5050163  0.5053387  0.50554925 0.5054659  0.505005
 0.5047149  0.5048746  0.5051735  0.5052585  0.5049847  0.5047814
 0.5048613  0.50511503 0.50520104 0.50518423 0.5051771  0.5052832
 0.50546193 0.50564003 0.50582725 0.50618446 0.50654256 0.5067271
 0.5068236  0.5069565  0.5071691  0.50748193 0.5075712  0.5073499
 0.5070716  0.5069005  0.50685686 0.50679237 0.50654423 0.506286
 0.5061798  0.50624424 0.5063031  0.5062505  0.5061724  0.5062255
 0.50641876 0.506559   0.5065386  0.50649726 0.5067279  0.507219
 0.5075075  0.507251   0.5066003  0.50597066 0.5056318  0.5053587
 0.50483114 0.5042295  0.5039997  0.5043122  0.5045357  0.5042884
 0.50388926 0.5037117  0.50382966 0.50381905 0.5033341  0.50252545
 0.50178236 0.50129294 0.5007551  0.4998302  0.49857393 0.4973588
 0.49637958 0.49547186 0.49463043 0.49404916 0.49380672 0.49372706
 0.4935107  0.4931566  0.49290556 0.49302715 0.49337068 0.4934315
 0.49306452 0.49270397 0.49259    0.49262232 0.49253535 0.4921829
 0.4917445  0.49162027 0.49176347 0.49211925 0.4924196  0.49249607
 0.49262813 0.49277887 0.49293667 0.4928385  0.4925287  0.49232748
 0.4925005  0.492838   0.49303505 0.49293682 0.49272174 0.49265665
 0.49274454 0.49284703 0.4927224  0.4926302  0.49272853 0.49293357
 0.49307618 0.49304378 0.49302423 0.4933153  0.49382183 0.49422908
 0.49435642 0.49421355 0.4941631  0.49435636 0.4946654  0.49484393
 0.49497852 0.49507347 0.49521396 0.49535418 0.495521   0.4956998
 0.49590182 0.49616882 0.49635944 0.4963834  0.49635673 0.49645194
 0.4967583  0.4969871  0.4969036  0.49651286 0.4960187  0.49553454
 0.4948056  0.4937847  0.4927949  0.49209464 0.49169222 0.49123737
 0.49076116 0.4904873  0.49076712 0.49131477 0.49156848 0.49152038
 0.4915731  0.49194008 0.49241126 0.49251637 0.49216238 0.49172777
 0.49167156 0.49187583 0.49179035 0.49116597 0.490336   0.48980516
 0.48955843 0.4891208  0.48848042 0.48811454 0.48801738 0.48806125
 0.48782164 0.4872107  0.48676825 0.48681363 0.48677558 0.48644146
 0.4860977  0.48612937 0.48620877 0.4857325  0.48470068 0.48415232
 0.48458993 0.48522288 0.48499164 0.48474127 0.48576456 0.4856723 ]
