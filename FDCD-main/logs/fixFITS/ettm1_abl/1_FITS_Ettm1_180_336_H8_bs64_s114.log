Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_336_FITS_ETTm1_ftM_sl180_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34045
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=26, out_features=74, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1723904.0
params:  1998.0
Trainable parameters:  1998
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5950808
	speed: 0.0201s/iter; left time: 530.7686s
	iters: 200, epoch: 1 | loss: 0.4854828
	speed: 0.0141s/iter; left time: 371.6924s
Epoch: 1 cost time: 4.284325122833252
Epoch: 1, Steps: 265 | Train Loss: 0.5767793 Vali Loss: 0.8158973 Test Loss: 0.5056089
Validation loss decreased (inf --> 0.815897).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4647306
	speed: 0.0698s/iter; left time: 1824.5429s
	iters: 200, epoch: 2 | loss: 0.3721046
	speed: 0.0129s/iter; left time: 337.0136s
Epoch: 2 cost time: 4.058673143386841
Epoch: 2, Steps: 265 | Train Loss: 0.4120091 Vali Loss: 0.7131466 Test Loss: 0.4166924
Validation loss decreased (0.815897 --> 0.713147).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4457332
	speed: 0.0673s/iter; left time: 1741.0868s
	iters: 200, epoch: 3 | loss: 0.3728216
	speed: 0.0133s/iter; left time: 343.7890s
Epoch: 3 cost time: 3.9670097827911377
Epoch: 3, Steps: 265 | Train Loss: 0.3858232 Vali Loss: 0.6855204 Test Loss: 0.3943711
Validation loss decreased (0.713147 --> 0.685520).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3800494
	speed: 0.0659s/iter; left time: 1687.3563s
	iters: 200, epoch: 4 | loss: 0.4195414
	speed: 0.0132s/iter; left time: 335.4683s
Epoch: 4 cost time: 3.8535983562469482
Epoch: 4, Steps: 265 | Train Loss: 0.3783436 Vali Loss: 0.6755267 Test Loss: 0.3881313
Validation loss decreased (0.685520 --> 0.675527).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3784617
	speed: 0.0675s/iter; left time: 1710.8202s
	iters: 200, epoch: 5 | loss: 0.4002294
	speed: 0.0133s/iter; left time: 335.6349s
Epoch: 5 cost time: 4.003190040588379
Epoch: 5, Steps: 265 | Train Loss: 0.3760734 Vali Loss: 0.6716902 Test Loss: 0.3857871
Validation loss decreased (0.675527 --> 0.671690).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3815077
	speed: 0.0675s/iter; left time: 1693.1998s
	iters: 200, epoch: 6 | loss: 0.3845613
	speed: 0.0151s/iter; left time: 378.2104s
Epoch: 6 cost time: 4.356377601623535
Epoch: 6, Steps: 265 | Train Loss: 0.3749163 Vali Loss: 0.6697327 Test Loss: 0.3849728
Validation loss decreased (0.671690 --> 0.669733).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3734875
	speed: 0.0750s/iter; left time: 1860.9889s
	iters: 200, epoch: 7 | loss: 0.3656476
	speed: 0.0149s/iter; left time: 368.6635s
Epoch: 7 cost time: 4.52810525894165
Epoch: 7, Steps: 265 | Train Loss: 0.3747538 Vali Loss: 0.6687220 Test Loss: 0.3846743
Validation loss decreased (0.669733 --> 0.668722).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3467155
	speed: 0.0719s/iter; left time: 1764.2479s
	iters: 200, epoch: 8 | loss: 0.3762420
	speed: 0.0151s/iter; left time: 367.9685s
Epoch: 8 cost time: 4.638972043991089
Epoch: 8, Steps: 265 | Train Loss: 0.3745961 Vali Loss: 0.6672789 Test Loss: 0.3845205
Validation loss decreased (0.668722 --> 0.667279).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3752234
	speed: 0.0831s/iter; left time: 2018.0922s
	iters: 200, epoch: 9 | loss: 0.3877289
	speed: 0.0150s/iter; left time: 363.0148s
Epoch: 9 cost time: 4.5036842823028564
Epoch: 9, Steps: 265 | Train Loss: 0.3744024 Vali Loss: 0.6666963 Test Loss: 0.3842425
Validation loss decreased (0.667279 --> 0.666696).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3989545
	speed: 0.0686s/iter; left time: 1648.1951s
	iters: 200, epoch: 10 | loss: 0.3611651
	speed: 0.0137s/iter; left time: 327.9821s
Epoch: 10 cost time: 4.0464818477630615
Epoch: 10, Steps: 265 | Train Loss: 0.3741890 Vali Loss: 0.6672053 Test Loss: 0.3843617
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3827350
	speed: 0.0665s/iter; left time: 1579.0092s
	iters: 200, epoch: 11 | loss: 0.3807991
	speed: 0.0163s/iter; left time: 385.8048s
Epoch: 11 cost time: 4.248901605606079
Epoch: 11, Steps: 265 | Train Loss: 0.3743006 Vali Loss: 0.6669906 Test Loss: 0.3840657
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3990763
	speed: 0.0702s/iter; left time: 1648.4256s
	iters: 200, epoch: 12 | loss: 0.3693872
	speed: 0.0131s/iter; left time: 305.6985s
Epoch: 12 cost time: 4.052364349365234
Epoch: 12, Steps: 265 | Train Loss: 0.3742664 Vali Loss: 0.6668580 Test Loss: 0.3842098
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4028838
	speed: 0.0879s/iter; left time: 2041.6958s
	iters: 200, epoch: 13 | loss: 0.3491431
	speed: 0.0130s/iter; left time: 300.3684s
Epoch: 13 cost time: 3.9752495288848877
Epoch: 13, Steps: 265 | Train Loss: 0.3740697 Vali Loss: 0.6659535 Test Loss: 0.3840592
Validation loss decreased (0.666696 --> 0.665954).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3937541
	speed: 0.0681s/iter; left time: 1564.3040s
	iters: 200, epoch: 14 | loss: 0.4194166
	speed: 0.0133s/iter; left time: 304.7378s
Epoch: 14 cost time: 4.072731256484985
Epoch: 14, Steps: 265 | Train Loss: 0.3740079 Vali Loss: 0.6657903 Test Loss: 0.3841558
Validation loss decreased (0.665954 --> 0.665790).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4101846
	speed: 0.0678s/iter; left time: 1539.1989s
	iters: 200, epoch: 15 | loss: 0.3354690
	speed: 0.0138s/iter; left time: 312.7123s
Epoch: 15 cost time: 4.052623748779297
Epoch: 15, Steps: 265 | Train Loss: 0.3740477 Vali Loss: 0.6667420 Test Loss: 0.3841026
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3866497
	speed: 0.0679s/iter; left time: 1523.7905s
	iters: 200, epoch: 16 | loss: 0.3755637
	speed: 0.0269s/iter; left time: 599.5593s
Epoch: 16 cost time: 5.399157285690308
Epoch: 16, Steps: 265 | Train Loss: 0.3739303 Vali Loss: 0.6665514 Test Loss: 0.3839989
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3704920
	speed: 0.0673s/iter; left time: 1491.7558s
	iters: 200, epoch: 17 | loss: 0.3997747
	speed: 0.0132s/iter; left time: 290.4713s
Epoch: 17 cost time: 3.9509973526000977
Epoch: 17, Steps: 265 | Train Loss: 0.3740817 Vali Loss: 0.6657269 Test Loss: 0.3841775
Validation loss decreased (0.665790 --> 0.665727).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3746648
	speed: 0.0666s/iter; left time: 1459.2738s
	iters: 200, epoch: 18 | loss: 0.3555699
	speed: 0.0134s/iter; left time: 291.3830s
Epoch: 18 cost time: 3.943591833114624
Epoch: 18, Steps: 265 | Train Loss: 0.3739919 Vali Loss: 0.6658909 Test Loss: 0.3840566
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3787135
	speed: 0.0701s/iter; left time: 1516.9541s
	iters: 200, epoch: 19 | loss: 0.3259384
	speed: 0.0154s/iter; left time: 330.9127s
Epoch: 19 cost time: 4.476535081863403
Epoch: 19, Steps: 265 | Train Loss: 0.3738163 Vali Loss: 0.6663999 Test Loss: 0.3839645
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3769621
	speed: 0.0726s/iter; left time: 1551.0859s
	iters: 200, epoch: 20 | loss: 0.3627018
	speed: 0.0133s/iter; left time: 282.3366s
Epoch: 20 cost time: 4.1237030029296875
Epoch: 20, Steps: 265 | Train Loss: 0.3739805 Vali Loss: 0.6660799 Test Loss: 0.3840548
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3923953
	speed: 0.0675s/iter; left time: 1424.0950s
	iters: 200, epoch: 21 | loss: 0.3984603
	speed: 0.0128s/iter; left time: 269.7507s
Epoch: 21 cost time: 3.9357104301452637
Epoch: 21, Steps: 265 | Train Loss: 0.3737776 Vali Loss: 0.6663765 Test Loss: 0.3841152
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3709824
	speed: 0.0681s/iter; left time: 1419.6263s
	iters: 200, epoch: 22 | loss: 0.3796822
	speed: 0.0127s/iter; left time: 263.7668s
Epoch: 22 cost time: 3.9293031692504883
Epoch: 22, Steps: 265 | Train Loss: 0.3739696 Vali Loss: 0.6659712 Test Loss: 0.3840003
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3789811
	speed: 0.0670s/iter; left time: 1378.1825s
	iters: 200, epoch: 23 | loss: 0.3474436
	speed: 0.0125s/iter; left time: 254.9418s
Epoch: 23 cost time: 4.013343572616577
Epoch: 23, Steps: 265 | Train Loss: 0.3739845 Vali Loss: 0.6662171 Test Loss: 0.3839923
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3621924
	speed: 0.0692s/iter; left time: 1404.5511s
	iters: 200, epoch: 24 | loss: 0.3540345
	speed: 0.0139s/iter; left time: 280.2019s
Epoch: 24 cost time: 4.414084196090698
Epoch: 24, Steps: 265 | Train Loss: 0.3738894 Vali Loss: 0.6660079 Test Loss: 0.3841079
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3780191
	speed: 0.0722s/iter; left time: 1446.4963s
	iters: 200, epoch: 25 | loss: 0.4056421
	speed: 0.0130s/iter; left time: 259.3868s
Epoch: 25 cost time: 4.152822494506836
Epoch: 25, Steps: 265 | Train Loss: 0.3739453 Vali Loss: 0.6660875 Test Loss: 0.3839427
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3398855
	speed: 0.0673s/iter; left time: 1330.9129s
	iters: 200, epoch: 26 | loss: 0.3409074
	speed: 0.0129s/iter; left time: 253.8196s
Epoch: 26 cost time: 4.044816493988037
Epoch: 26, Steps: 265 | Train Loss: 0.3739046 Vali Loss: 0.6662826 Test Loss: 0.3839436
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3824953
	speed: 0.0691s/iter; left time: 1348.1694s
	iters: 200, epoch: 27 | loss: 0.3799914
	speed: 0.0137s/iter; left time: 266.6512s
Epoch: 27 cost time: 4.323239803314209
Epoch: 27, Steps: 265 | Train Loss: 0.3740179 Vali Loss: 0.6664613 Test Loss: 0.3839410
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3792425
	speed: 0.0675s/iter; left time: 1298.4849s
	iters: 200, epoch: 28 | loss: 0.3562171
	speed: 0.0128s/iter; left time: 245.6262s
Epoch: 28 cost time: 3.98626446723938
Epoch: 28, Steps: 265 | Train Loss: 0.3739487 Vali Loss: 0.6661017 Test Loss: 0.3839996
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4055595
	speed: 0.0670s/iter; left time: 1271.0172s
	iters: 200, epoch: 29 | loss: 0.4202275
	speed: 0.0157s/iter; left time: 295.6187s
Epoch: 29 cost time: 6.230628967285156
Epoch: 29, Steps: 265 | Train Loss: 0.3738803 Vali Loss: 0.6660475 Test Loss: 0.3840604
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3393377
	speed: 0.0870s/iter; left time: 1629.1982s
	iters: 200, epoch: 30 | loss: 0.3799465
	speed: 0.0132s/iter; left time: 245.4911s
Epoch: 30 cost time: 3.9414401054382324
Epoch: 30, Steps: 265 | Train Loss: 0.3737842 Vali Loss: 0.6659792 Test Loss: 0.3839702
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3669592
	speed: 0.0663s/iter; left time: 1222.8052s
	iters: 200, epoch: 31 | loss: 0.3495599
	speed: 0.0131s/iter; left time: 240.2380s
Epoch: 31 cost time: 3.966191530227661
Epoch: 31, Steps: 265 | Train Loss: 0.3738043 Vali Loss: 0.6661254 Test Loss: 0.3839126
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3596954
	speed: 0.0665s/iter; left time: 1209.4980s
	iters: 200, epoch: 32 | loss: 0.3527440
	speed: 0.0135s/iter; left time: 243.3083s
Epoch: 32 cost time: 4.089121580123901
Epoch: 32, Steps: 265 | Train Loss: 0.3738419 Vali Loss: 0.6662003 Test Loss: 0.3839563
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3264697
	speed: 0.0688s/iter; left time: 1232.1285s
	iters: 200, epoch: 33 | loss: 0.4001720
	speed: 0.0137s/iter; left time: 243.8511s
Epoch: 33 cost time: 4.092183589935303
Epoch: 33, Steps: 265 | Train Loss: 0.3738842 Vali Loss: 0.6665005 Test Loss: 0.3839273
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3642816
	speed: 0.0672s/iter; left time: 1185.8343s
	iters: 200, epoch: 34 | loss: 0.3501654
	speed: 0.0148s/iter; left time: 259.8104s
Epoch: 34 cost time: 4.328101396560669
Epoch: 34, Steps: 265 | Train Loss: 0.3739706 Vali Loss: 0.6665287 Test Loss: 0.3839437
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3389845
	speed: 0.0705s/iter; left time: 1226.1279s
	iters: 200, epoch: 35 | loss: 0.3841547
	speed: 0.0152s/iter; left time: 262.8164s
Epoch: 35 cost time: 4.47926139831543
Epoch: 35, Steps: 265 | Train Loss: 0.3739007 Vali Loss: 0.6664279 Test Loss: 0.3840068
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3933911
	speed: 0.0997s/iter; left time: 1706.6864s
	iters: 200, epoch: 36 | loss: 0.3429200
	speed: 0.0134s/iter; left time: 227.6154s
Epoch: 36 cost time: 3.9116051197052
Epoch: 36, Steps: 265 | Train Loss: 0.3737345 Vali Loss: 0.6660442 Test Loss: 0.3839395
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3611127
	speed: 0.0684s/iter; left time: 1154.0074s
	iters: 200, epoch: 37 | loss: 0.3511624
	speed: 0.0130s/iter; left time: 218.4586s
Epoch: 37 cost time: 3.963610887527466
Epoch: 37, Steps: 265 | Train Loss: 0.3739142 Vali Loss: 0.6661193 Test Loss: 0.3839437
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_336_FITS_ETTm1_ftM_sl180_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.38417166471481323, mae:0.39020609855651855, rse:0.5898084044456482, corr:[0.54567045 0.5484243  0.5464523  0.54336786 0.5417396  0.5416976
 0.54194057 0.5419013  0.5414892  0.541474   0.5422839  0.54303396
 0.54294646 0.54148257 0.5392795  0.5372214  0.5355639  0.53419024
 0.5327955  0.5311058  0.5290646  0.5267406  0.5243362  0.522053
 0.5198785  0.5177864  0.5158783  0.5141583  0.51279354 0.5119836
 0.5120194  0.512678   0.5134253  0.51387894 0.5137993  0.5133163
 0.51261544 0.5119865  0.5117599  0.5118198  0.512056   0.51214445
 0.5119656  0.51165265 0.51136535 0.5112895  0.511524   0.5119461
 0.5123469  0.5124891  0.51238996 0.51219296 0.51208216 0.5121152
 0.5122815  0.5124483  0.5125711  0.5124773  0.5122324  0.5119102
 0.5116938  0.5116516  0.5116834  0.511675   0.5115686  0.5115164
 0.51154995 0.5116694  0.51196426 0.5123688  0.51277435 0.5130448
 0.51313597 0.513084   0.5130832  0.51311177 0.5131972  0.5132465
 0.51318854 0.5130133  0.51283526 0.5127122  0.5126719  0.51258546
 0.5124164  0.5122312  0.5120587  0.5118039  0.51144475 0.5111389
 0.5109185  0.5107708  0.51062214 0.5103995  0.50996196 0.5091912
 0.5082104  0.50729036 0.506604   0.50615144 0.50595254 0.5059566
 0.50606865 0.5062919  0.50679255 0.5076944  0.5089376  0.5102419
 0.51123774 0.5116887  0.5116787  0.5116104  0.5116326  0.51181525
 0.51201504 0.5121109  0.5118811  0.51128113 0.5105257  0.50984395
 0.50944316 0.5091941  0.5089196  0.5085464  0.50796986 0.507238
 0.50660986 0.50630647 0.50634795 0.5065767  0.50670874 0.506517
 0.50600094 0.5054547  0.50507295 0.50489026 0.5049731  0.50508785
 0.50502324 0.50480014 0.50447303 0.5042306  0.5042316  0.5045043
 0.5048918  0.50519764 0.5051688  0.5048889  0.50465304 0.50444585
 0.5044069  0.5045872  0.504855   0.5050836  0.50506115 0.504888
 0.50468594 0.5046149  0.5046673  0.5048647  0.505101   0.5053087
 0.50545967 0.505566   0.50563556 0.5058039  0.5060351  0.5062794
 0.5065076  0.50662696 0.5066059  0.5066156  0.50664175 0.50665367
 0.50666386 0.5066321  0.5065895  0.5065677  0.5064926  0.50640225
 0.5062897  0.5061783  0.5060729  0.5060091  0.5059818  0.5059441
 0.5058811  0.5058294  0.5058851  0.5061173  0.5064651  0.50676656
 0.5068136  0.50664514 0.50634176 0.5058939  0.50545555 0.5051541
 0.50490355 0.5045415  0.5040741  0.50383186 0.50374967 0.5037395
 0.5036989  0.50335175 0.50281984 0.50231403 0.5018808  0.5015234
 0.5011685  0.50076294 0.50021493 0.49945047 0.49851412 0.4975351
 0.4965897  0.49570978 0.49500117 0.49455023 0.49430248 0.4941961
 0.49414977 0.4941382  0.4940744  0.4940224  0.49404675 0.49404076
 0.49388573 0.49365497 0.49331057 0.49291286 0.49261653 0.49252826
 0.49262583 0.49280488 0.4928091  0.49270725 0.49257478 0.49247345
 0.4925809  0.49275875 0.4929596  0.4930268  0.4929412  0.49276707
 0.49263313 0.49257714 0.49268153 0.4929046  0.49309352 0.49311858
 0.49298227 0.4928495  0.49274597 0.4927729  0.49288917 0.49304134
 0.49323454 0.49340257 0.49350676 0.49360207 0.49373314 0.49397826
 0.49434745 0.49465778 0.4948705  0.49499625 0.49508137 0.49509636
 0.49513584 0.4951826  0.4953016  0.49544257 0.49558532 0.49567536
 0.4957482  0.49593008 0.49620086 0.49649838 0.4967304  0.4967886
 0.49666965 0.4963878  0.4960498  0.4957099  0.49533197 0.49484962
 0.49411437 0.49325174 0.49247545 0.4918502  0.49150744 0.49140692
 0.49152532 0.49159035 0.49160576 0.4916566  0.49184817 0.4922727
 0.49279058 0.493073   0.49308795 0.49289975 0.49261326 0.49231297
 0.492137   0.492084   0.49204254 0.49188605 0.49151802 0.49093696
 0.49022347 0.48950988 0.4890768  0.4890755  0.48913786 0.48906526
 0.488685   0.48794314 0.48716167 0.48682445 0.4869224  0.48719943
 0.48716834 0.48655543 0.48546466 0.48447016 0.48412037 0.48451906
 0.48511606 0.48519957 0.48463526 0.4844241  0.48573482 0.48813105]
