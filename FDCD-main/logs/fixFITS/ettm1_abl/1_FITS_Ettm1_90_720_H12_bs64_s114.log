Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=22, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3902976.0
params:  4554.0
Trainable parameters:  4554
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.9322843
	speed: 0.0454s/iter; left time: 1189.9850s
	iters: 200, epoch: 1 | loss: 0.6747573
	speed: 0.0350s/iter; left time: 913.2278s
Epoch: 1 cost time: 10.69614291191101
Epoch: 1, Steps: 263 | Train Loss: 0.9528269 Vali Loss: 1.3274754 Test Loss: 0.7810034
Validation loss decreased (inf --> 1.327475).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5622802
	speed: 0.1205s/iter; left time: 3125.1602s
	iters: 200, epoch: 2 | loss: 0.5172660
	speed: 0.0216s/iter; left time: 557.2287s
Epoch: 2 cost time: 6.060586214065552
Epoch: 2, Steps: 263 | Train Loss: 0.5694015 Vali Loss: 1.0997190 Test Loss: 0.5621557
Validation loss decreased (1.327475 --> 1.099719).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5013169
	speed: 0.1063s/iter; left time: 2730.2157s
	iters: 200, epoch: 3 | loss: 0.4963494
	speed: 0.0281s/iter; left time: 717.3795s
Epoch: 3 cost time: 7.226337671279907
Epoch: 3, Steps: 263 | Train Loss: 0.5127975 Vali Loss: 1.0479953 Test Loss: 0.5169650
Validation loss decreased (1.099719 --> 1.047995).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.5142208
	speed: 0.1594s/iter; left time: 4049.9472s
	iters: 200, epoch: 4 | loss: 0.5339869
	speed: 0.0268s/iter; left time: 677.3764s
Epoch: 4 cost time: 10.436521768569946
Epoch: 4, Steps: 263 | Train Loss: 0.5001077 Vali Loss: 1.0285066 Test Loss: 0.5019934
Validation loss decreased (1.047995 --> 1.028507).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5473471
	speed: 0.1671s/iter; left time: 4202.4335s
	iters: 200, epoch: 5 | loss: 0.4927841
	speed: 0.0235s/iter; left time: 589.4705s
Epoch: 5 cost time: 8.384716749191284
Epoch: 5, Steps: 263 | Train Loss: 0.4957710 Vali Loss: 1.0192683 Test Loss: 0.4963308
Validation loss decreased (1.028507 --> 1.019268).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5103825
	speed: 0.1522s/iter; left time: 3788.3711s
	iters: 200, epoch: 6 | loss: 0.4883440
	speed: 0.0288s/iter; left time: 713.2795s
Epoch: 6 cost time: 8.864027976989746
Epoch: 6, Steps: 263 | Train Loss: 0.4944129 Vali Loss: 1.0159276 Test Loss: 0.4941997
Validation loss decreased (1.019268 --> 1.015928).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5188519
	speed: 0.1186s/iter; left time: 2920.9632s
	iters: 200, epoch: 7 | loss: 0.4776146
	speed: 0.0200s/iter; left time: 490.0974s
Epoch: 7 cost time: 6.1991918087005615
Epoch: 7, Steps: 263 | Train Loss: 0.4938919 Vali Loss: 1.0138296 Test Loss: 0.4936795
Validation loss decreased (1.015928 --> 1.013830).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4825749
	speed: 0.1295s/iter; left time: 3154.7214s
	iters: 200, epoch: 8 | loss: 0.4687178
	speed: 0.0225s/iter; left time: 546.0590s
Epoch: 8 cost time: 6.566230297088623
Epoch: 8, Steps: 263 | Train Loss: 0.4936835 Vali Loss: 1.0129291 Test Loss: 0.4930771
Validation loss decreased (1.013830 --> 1.012929).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4868039
	speed: 0.1306s/iter; left time: 3147.8116s
	iters: 200, epoch: 9 | loss: 0.5248047
	speed: 0.0344s/iter; left time: 825.8668s
Epoch: 9 cost time: 11.544990539550781
Epoch: 9, Steps: 263 | Train Loss: 0.4935739 Vali Loss: 1.0130911 Test Loss: 0.4929003
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4828570
	speed: 0.1348s/iter; left time: 3212.7230s
	iters: 200, epoch: 10 | loss: 0.4968382
	speed: 0.0210s/iter; left time: 499.5445s
Epoch: 10 cost time: 6.432178974151611
Epoch: 10, Steps: 263 | Train Loss: 0.4935336 Vali Loss: 1.0125563 Test Loss: 0.4931884
Validation loss decreased (1.012929 --> 1.012556).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5087517
	speed: 0.1620s/iter; left time: 3818.4076s
	iters: 200, epoch: 11 | loss: 0.5099636
	speed: 0.0382s/iter; left time: 897.4668s
Epoch: 11 cost time: 11.382444620132446
Epoch: 11, Steps: 263 | Train Loss: 0.4934418 Vali Loss: 1.0121816 Test Loss: 0.4932963
Validation loss decreased (1.012556 --> 1.012182).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4910008
	speed: 0.1767s/iter; left time: 4117.4182s
	iters: 200, epoch: 12 | loss: 0.5308095
	speed: 0.0318s/iter; left time: 737.0809s
Epoch: 12 cost time: 9.707359313964844
Epoch: 12, Steps: 263 | Train Loss: 0.4934431 Vali Loss: 1.0117722 Test Loss: 0.4933000
Validation loss decreased (1.012182 --> 1.011772).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4633235
	speed: 0.1873s/iter; left time: 4316.6665s
	iters: 200, epoch: 13 | loss: 0.4716548
	speed: 0.0496s/iter; left time: 1138.7887s
Epoch: 13 cost time: 11.793232440948486
Epoch: 13, Steps: 263 | Train Loss: 0.4934316 Vali Loss: 1.0124793 Test Loss: 0.4933479
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4923010
	speed: 0.1381s/iter; left time: 3145.3308s
	iters: 200, epoch: 14 | loss: 0.5303586
	speed: 0.0262s/iter; left time: 594.1803s
Epoch: 14 cost time: 7.05346417427063
Epoch: 14, Steps: 263 | Train Loss: 0.4933249 Vali Loss: 1.0123466 Test Loss: 0.4933996
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4464591
	speed: 0.1157s/iter; left time: 2606.4281s
	iters: 200, epoch: 15 | loss: 0.5119951
	speed: 0.0263s/iter; left time: 588.7681s
Epoch: 15 cost time: 8.43665862083435
Epoch: 15, Steps: 263 | Train Loss: 0.4932179 Vali Loss: 1.0125324 Test Loss: 0.4933068
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4850048
	speed: 0.1615s/iter; left time: 3594.4899s
	iters: 200, epoch: 16 | loss: 0.4626066
	speed: 0.0209s/iter; left time: 463.9973s
Epoch: 16 cost time: 7.84437370300293
Epoch: 16, Steps: 263 | Train Loss: 0.4932873 Vali Loss: 1.0130255 Test Loss: 0.4936467
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4965840
	speed: 0.1343s/iter; left time: 2954.4014s
	iters: 200, epoch: 17 | loss: 0.4989507
	speed: 0.0263s/iter; left time: 575.1693s
Epoch: 17 cost time: 7.629812955856323
Epoch: 17, Steps: 263 | Train Loss: 0.4934017 Vali Loss: 1.0128307 Test Loss: 0.4935354
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5089853
	speed: 0.1129s/iter; left time: 2454.3426s
	iters: 200, epoch: 18 | loss: 0.5238816
	speed: 0.0201s/iter; left time: 434.1159s
Epoch: 18 cost time: 6.196269273757935
Epoch: 18, Steps: 263 | Train Loss: 0.4932605 Vali Loss: 1.0128657 Test Loss: 0.4936548
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5142267
	speed: 0.1187s/iter; left time: 2547.8655s
	iters: 200, epoch: 19 | loss: 0.4819857
	speed: 0.0254s/iter; left time: 542.7247s
Epoch: 19 cost time: 6.996348857879639
Epoch: 19, Steps: 263 | Train Loss: 0.4933021 Vali Loss: 1.0124804 Test Loss: 0.4934735
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5183352
	speed: 0.1088s/iter; left time: 2307.8438s
	iters: 200, epoch: 20 | loss: 0.4755886
	speed: 0.0228s/iter; left time: 480.1825s
Epoch: 20 cost time: 6.963009834289551
Epoch: 20, Steps: 263 | Train Loss: 0.4933369 Vali Loss: 1.0122921 Test Loss: 0.4935110
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4695762
	speed: 0.1250s/iter; left time: 2617.0609s
	iters: 200, epoch: 21 | loss: 0.4991188
	speed: 0.0213s/iter; left time: 443.3978s
Epoch: 21 cost time: 6.49110221862793
Epoch: 21, Steps: 263 | Train Loss: 0.4933837 Vali Loss: 1.0119517 Test Loss: 0.4936028
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5191849
	speed: 0.1039s/iter; left time: 2148.1561s
	iters: 200, epoch: 22 | loss: 0.4757538
	speed: 0.0227s/iter; left time: 467.7584s
Epoch: 22 cost time: 7.86637806892395
Epoch: 22, Steps: 263 | Train Loss: 0.4930895 Vali Loss: 1.0124927 Test Loss: 0.4937376
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4876947
	speed: 0.1489s/iter; left time: 3040.6874s
	iters: 200, epoch: 23 | loss: 0.4451506
	speed: 0.0251s/iter; left time: 509.4849s
Epoch: 23 cost time: 8.037894487380981
Epoch: 23, Steps: 263 | Train Loss: 0.4933308 Vali Loss: 1.0130017 Test Loss: 0.4937819
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4991550
	speed: 0.1534s/iter; left time: 3090.6719s
	iters: 200, epoch: 24 | loss: 0.4932636
	speed: 0.0267s/iter; left time: 536.0598s
Epoch: 24 cost time: 10.156566143035889
Epoch: 24, Steps: 263 | Train Loss: 0.4934065 Vali Loss: 1.0120107 Test Loss: 0.4935420
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4856174
	speed: 0.1974s/iter; left time: 3927.0743s
	iters: 200, epoch: 25 | loss: 0.5126585
	speed: 0.0422s/iter; left time: 834.8819s
Epoch: 25 cost time: 10.264376640319824
Epoch: 25, Steps: 263 | Train Loss: 0.4932227 Vali Loss: 1.0121217 Test Loss: 0.4937156
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4479573
	speed: 0.1456s/iter; left time: 2858.0317s
	iters: 200, epoch: 26 | loss: 0.4761522
	speed: 0.0229s/iter; left time: 447.5069s
Epoch: 26 cost time: 8.07729959487915
Epoch: 26, Steps: 263 | Train Loss: 0.4931325 Vali Loss: 1.0123835 Test Loss: 0.4936618
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5044932
	speed: 0.1455s/iter; left time: 2818.2721s
	iters: 200, epoch: 27 | loss: 0.5233016
	speed: 0.0262s/iter; left time: 504.3947s
Epoch: 27 cost time: 8.436551094055176
Epoch: 27, Steps: 263 | Train Loss: 0.4932616 Vali Loss: 1.0129334 Test Loss: 0.4938551
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5018749
	speed: 0.1510s/iter; left time: 2884.4877s
	iters: 200, epoch: 28 | loss: 0.4819939
	speed: 0.0355s/iter; left time: 674.7318s
Epoch: 28 cost time: 9.536240816116333
Epoch: 28, Steps: 263 | Train Loss: 0.4931491 Vali Loss: 1.0121013 Test Loss: 0.4938118
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4676774
	speed: 0.1797s/iter; left time: 3384.4925s
	iters: 200, epoch: 29 | loss: 0.4659830
	speed: 0.0359s/iter; left time: 672.7349s
Epoch: 29 cost time: 8.474490880966187
Epoch: 29, Steps: 263 | Train Loss: 0.4930499 Vali Loss: 1.0117310 Test Loss: 0.4937882
Validation loss decreased (1.011772 --> 1.011731).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.5262567
	speed: 0.1305s/iter; left time: 2424.5766s
	iters: 200, epoch: 30 | loss: 0.5127437
	speed: 0.0227s/iter; left time: 419.4581s
Epoch: 30 cost time: 6.226399660110474
Epoch: 30, Steps: 263 | Train Loss: 0.4932133 Vali Loss: 1.0129621 Test Loss: 0.4938016
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.5553150
	speed: 0.1202s/iter; left time: 2201.0826s
	iters: 200, epoch: 31 | loss: 0.5302035
	speed: 0.0358s/iter; left time: 652.2659s
Epoch: 31 cost time: 8.91419792175293
Epoch: 31, Steps: 263 | Train Loss: 0.4932205 Vali Loss: 1.0124532 Test Loss: 0.4938121
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.5008402
	speed: 0.1437s/iter; left time: 2592.8063s
	iters: 200, epoch: 32 | loss: 0.5458984
	speed: 0.0279s/iter; left time: 500.6031s
Epoch: 32 cost time: 7.502384424209595
Epoch: 32, Steps: 263 | Train Loss: 0.4932966 Vali Loss: 1.0131524 Test Loss: 0.4939700
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.5138394
	speed: 0.1238s/iter; left time: 2202.6677s
	iters: 200, epoch: 33 | loss: 0.4587333
	speed: 0.0221s/iter; left time: 391.2693s
Epoch: 33 cost time: 7.457446575164795
Epoch: 33, Steps: 263 | Train Loss: 0.4933186 Vali Loss: 1.0128978 Test Loss: 0.4939556
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4814376
	speed: 0.1147s/iter; left time: 2010.4323s
	iters: 200, epoch: 34 | loss: 0.5030421
	speed: 0.0220s/iter; left time: 382.6011s
Epoch: 34 cost time: 6.205038785934448
Epoch: 34, Steps: 263 | Train Loss: 0.4931013 Vali Loss: 1.0124457 Test Loss: 0.4938505
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5256662
	speed: 0.1265s/iter; left time: 2183.0716s
	iters: 200, epoch: 35 | loss: 0.5124376
	speed: 0.0360s/iter; left time: 617.5146s
Epoch: 35 cost time: 7.728298902511597
Epoch: 35, Steps: 263 | Train Loss: 0.4932668 Vali Loss: 1.0132849 Test Loss: 0.4938221
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4932163
	speed: 0.1346s/iter; left time: 2287.6838s
	iters: 200, epoch: 36 | loss: 0.5007882
	speed: 0.0357s/iter; left time: 603.2743s
Epoch: 36 cost time: 7.738729953765869
Epoch: 36, Steps: 263 | Train Loss: 0.4930514 Vali Loss: 1.0128248 Test Loss: 0.4938660
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.5009089
	speed: 0.1148s/iter; left time: 1920.4048s
	iters: 200, epoch: 37 | loss: 0.5023877
	speed: 0.0289s/iter; left time: 479.8966s
Epoch: 37 cost time: 7.976167440414429
Epoch: 37, Steps: 263 | Train Loss: 0.4931270 Vali Loss: 1.0125803 Test Loss: 0.4939506
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.5088708
	speed: 0.1194s/iter; left time: 1966.5461s
	iters: 200, epoch: 38 | loss: 0.5490986
	speed: 0.0234s/iter; left time: 383.8424s
Epoch: 38 cost time: 6.908126592636108
Epoch: 38, Steps: 263 | Train Loss: 0.4931232 Vali Loss: 1.0132767 Test Loss: 0.4939382
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.5037970
	speed: 0.1332s/iter; left time: 2157.9719s
	iters: 200, epoch: 39 | loss: 0.4476881
	speed: 0.0440s/iter; left time: 709.3925s
Epoch: 39 cost time: 10.708352327346802
Epoch: 39, Steps: 263 | Train Loss: 0.4931867 Vali Loss: 1.0129956 Test Loss: 0.4939407
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4768893
	speed: 0.1923s/iter; left time: 3065.6681s
	iters: 200, epoch: 40 | loss: 0.4934964
	speed: 0.0308s/iter; left time: 488.7247s
Epoch: 40 cost time: 9.072948694229126
Epoch: 40, Steps: 263 | Train Loss: 0.4931244 Vali Loss: 1.0135652 Test Loss: 0.4939734
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4758481
	speed: 0.1733s/iter; left time: 2718.0908s
	iters: 200, epoch: 41 | loss: 0.4910410
	speed: 0.0266s/iter; left time: 413.7896s
Epoch: 41 cost time: 9.485577583312988
Epoch: 41, Steps: 263 | Train Loss: 0.4931443 Vali Loss: 1.0134112 Test Loss: 0.4939799
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.5112916
	speed: 0.1407s/iter; left time: 2168.7189s
	iters: 200, epoch: 42 | loss: 0.4798357
	speed: 0.0284s/iter; left time: 435.1842s
Epoch: 42 cost time: 8.818860292434692
Epoch: 42, Steps: 263 | Train Loss: 0.4929642 Vali Loss: 1.0131824 Test Loss: 0.4939980
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4856470
	speed: 0.1284s/iter; left time: 1945.7633s
	iters: 200, epoch: 43 | loss: 0.5210450
	speed: 0.0212s/iter; left time: 319.1776s
Epoch: 43 cost time: 7.2261576652526855
Epoch: 43, Steps: 263 | Train Loss: 0.4932256 Vali Loss: 1.0127634 Test Loss: 0.4939938
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.5068116
	speed: 0.1793s/iter; left time: 2670.8581s
	iters: 200, epoch: 44 | loss: 0.4946783
	speed: 0.0315s/iter; left time: 466.1972s
Epoch: 44 cost time: 10.046403169631958
Epoch: 44, Steps: 263 | Train Loss: 0.4932379 Vali Loss: 1.0129859 Test Loss: 0.4940345
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5008443
	speed: 0.1367s/iter; left time: 1999.7823s
	iters: 200, epoch: 45 | loss: 0.4868692
	speed: 0.0281s/iter; left time: 408.9458s
Epoch: 45 cost time: 7.5802247524261475
Epoch: 45, Steps: 263 | Train Loss: 0.4930843 Vali Loss: 1.0135405 Test Loss: 0.4940199
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4657579
	speed: 0.1468s/iter; left time: 2108.4889s
	iters: 200, epoch: 46 | loss: 0.5236917
	speed: 0.0342s/iter; left time: 487.3805s
Epoch: 46 cost time: 8.438314199447632
Epoch: 46, Steps: 263 | Train Loss: 0.4932189 Vali Loss: 1.0133489 Test Loss: 0.4940117
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4990047
	speed: 0.1298s/iter; left time: 1831.2536s
	iters: 200, epoch: 47 | loss: 0.4993288
	speed: 0.0259s/iter; left time: 362.2183s
Epoch: 47 cost time: 7.637326002120972
Epoch: 47, Steps: 263 | Train Loss: 0.4929593 Vali Loss: 1.0127779 Test Loss: 0.4939981
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4774149
	speed: 0.1099s/iter; left time: 1520.7764s
	iters: 200, epoch: 48 | loss: 0.5227023
	speed: 0.0297s/iter; left time: 407.3953s
Epoch: 48 cost time: 7.399311065673828
Epoch: 48, Steps: 263 | Train Loss: 0.4931256 Vali Loss: 1.0133024 Test Loss: 0.4939972
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.5216395
	speed: 0.1246s/iter; left time: 1691.5230s
	iters: 200, epoch: 49 | loss: 0.4531325
	speed: 0.0300s/iter; left time: 404.5698s
Epoch: 49 cost time: 7.568942070007324
Epoch: 49, Steps: 263 | Train Loss: 0.4931033 Vali Loss: 1.0120882 Test Loss: 0.4939914
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4919036626815796, mae:0.45243412256240845, rse:0.6672841310501099, corr:[0.53136206 0.5312081  0.52663875 0.5244332  0.5217469  0.51833993
 0.51574    0.513748   0.51190126 0.5104472  0.50950986 0.50850946
 0.5072003  0.5054885  0.5029805  0.49935153 0.4956566  0.49276176
 0.49010986 0.4870012  0.4837094  0.48086455 0.47818655 0.47485834
 0.47096607 0.46727172 0.46451575 0.46233422 0.46031278 0.4583761
 0.45736417 0.45760128 0.45770422 0.45756277 0.4578644  0.45877165
 0.45899567 0.45864356 0.4584024  0.45842895 0.45837533 0.45771942
 0.4572113  0.45726544 0.45714292 0.45645624 0.45584133 0.4559339
 0.4565922  0.4571835  0.45744696 0.45752475 0.45788464 0.4582679
 0.4586558  0.458986   0.45949236 0.4599886  0.4600152  0.4599401
 0.45994848 0.45962885 0.4592341  0.4588555  0.45889863 0.45896032
 0.4585927  0.45814645 0.45834106 0.45904753 0.45952696 0.4598725
 0.46047708 0.4612023  0.46201956 0.46262416 0.46337393 0.4643265
 0.46518216 0.46586996 0.46652442 0.4670921  0.46769038 0.46794185
 0.46804938 0.46831164 0.46895602 0.4698164  0.47065395 0.4715142
 0.47247988 0.47338983 0.47403154 0.4743087  0.4742884  0.47396335
 0.47339728 0.47274676 0.47194847 0.47111243 0.47059822 0.47057194
 0.4706319  0.47031593 0.46986458 0.46950108 0.46902215 0.4681856
 0.4671509  0.46626016 0.4654941  0.46456182 0.4634103  0.46235928
 0.46158928 0.46101436 0.46009234 0.45869234 0.45725334 0.45616516
 0.4552825  0.4539181  0.45222598 0.45117986 0.45069823 0.45000324
 0.44896492 0.4481542  0.44798    0.44816855 0.4483109  0.44836253
 0.44828743 0.44828227 0.44809574 0.44770113 0.447346   0.44716847
 0.44700992 0.4468452  0.44672686 0.44664153 0.4464701  0.4464023
 0.44652373 0.44681987 0.44713038 0.4473275  0.4478065  0.44820312
 0.44821283 0.44824    0.44848824 0.44890103 0.4490528  0.44898006
 0.44884822 0.44893047 0.44907287 0.44907182 0.44905433 0.44909403
 0.4492666  0.44951987 0.44982904 0.45018104 0.45068377 0.4513718
 0.45216405 0.45291632 0.4535688  0.45432642 0.4550739  0.45589456
 0.45668742 0.45733023 0.45769653 0.45793346 0.45818207 0.45862216
 0.45906147 0.45948863 0.45995554 0.46057743 0.46130836 0.46215236
 0.46316782 0.46400028 0.46413448 0.46364042 0.4633014  0.46362722
 0.4643447  0.4649897  0.46551257 0.46631256 0.46749714 0.46861336
 0.4690513  0.4687139  0.46814904 0.46751517 0.466608   0.46553713
 0.46453938 0.46356645 0.46212828 0.46029517 0.4584775  0.45705202
 0.45572627 0.4542258  0.45268944 0.45113054 0.44945443 0.44776633
 0.44594195 0.4442693  0.4428179  0.44159874 0.4403943  0.43948504
 0.43920305 0.43947196 0.439462   0.4391038  0.43915161 0.43920493
 0.4388856  0.43849242 0.43833354 0.43826273 0.43808714 0.4375427
 0.43702996 0.43686217 0.4366326  0.43626654 0.43599546 0.43577984
 0.43601418 0.43633965 0.43660435 0.43685114 0.4368629  0.43715096
 0.43787983 0.43885314 0.4394632  0.43967193 0.43951416 0.43961242
 0.43980938 0.43998176 0.4398488  0.44008052 0.4403795  0.44035926
 0.4402433  0.44045246 0.4407958  0.44129863 0.44188273 0.44246212
 0.4433654  0.44414428 0.44470042 0.4453886  0.44646496 0.44754738
 0.44859326 0.4493571  0.4499216  0.4503193  0.4507444  0.45127955
 0.45191044 0.45260322 0.45315233 0.4537517  0.45448688 0.4553178
 0.45609492 0.4566167  0.45691532 0.45700458 0.45667356 0.45559332
 0.4535931  0.45116976 0.44908682 0.44772562 0.44698608 0.4466598
 0.44646198 0.44619223 0.44598624 0.44570547 0.44509828 0.44415092
 0.44306722 0.4419888  0.44077858 0.439285   0.43783978 0.43671185
 0.43607754 0.4355195  0.43490458 0.43418494 0.43335834 0.43237993
 0.4312984  0.4302939  0.4294836  0.42882946 0.42796996 0.42722484
 0.42681482 0.4266882  0.4265847  0.42626497 0.42581084 0.42555916
 0.4254001  0.42530122 0.4250341  0.4246948  0.42443287 0.4241979
 0.42408726 0.42380482 0.42345524 0.42311653 0.42301464 0.42310423
 0.42339098 0.42348278 0.42361096 0.42358336 0.42376333 0.42411396
 0.4244782  0.42481682 0.42498046 0.42514366 0.42531103 0.4253427
 0.42521504 0.42507812 0.42518404 0.42527518 0.42512378 0.42496875
 0.4251253  0.42553157 0.4258704  0.42595223 0.42603827 0.42646888
 0.42716876 0.42784685 0.42848206 0.42917156 0.4301513  0.43120188
 0.43222064 0.43340293 0.43450773 0.43519732 0.43579388 0.4364797
 0.43743354 0.43843505 0.4396274  0.44104195 0.44247892 0.44385955
 0.44522968 0.44642127 0.44711822 0.44726813 0.44684905 0.44621938
 0.44576594 0.44570068 0.445917   0.44629997 0.44704378 0.44827303
 0.44942993 0.4499839  0.45010915 0.44996637 0.4495302  0.4487513
 0.44772243 0.446626   0.44541606 0.44417658 0.4429495  0.4418444
 0.4408551  0.4399788  0.439277   0.43856525 0.4376543  0.43662405
 0.43567362 0.43447468 0.433345   0.43277675 0.43234706 0.43181098
 0.43144137 0.43119052 0.43111733 0.43092304 0.43062222 0.43057925
 0.43058744 0.4304516  0.43003574 0.42965293 0.42947966 0.42938957
 0.42915374 0.42892757 0.4285236  0.42826518 0.42804614 0.42793608
 0.4280128  0.428233   0.4287198  0.42932442 0.42956603 0.4294212
 0.42922205 0.42952383 0.43031672 0.4311956  0.4316834  0.43169132
 0.43142733 0.43137842 0.43145397 0.43167216 0.4318607  0.4320138
 0.4320428  0.43220112 0.43240362 0.432615   0.43263018 0.43275213
 0.43312132 0.43346095 0.43377045 0.4343731  0.43528354 0.4362791
 0.43722475 0.43818545 0.43895793 0.4395617  0.44000933 0.44036078
 0.44079077 0.44147986 0.4426378  0.4438985  0.44491252 0.44576147
 0.4465732  0.44714245 0.4471883  0.4468692  0.44636294 0.44567668
 0.44462925 0.44325522 0.44189215 0.44093323 0.44048908 0.44041595
 0.44034675 0.44023722 0.44025505 0.4399978  0.43925905 0.43813568
 0.4368323  0.4354445  0.43393886 0.43236387 0.43082517 0.42929068
 0.42772952 0.42648098 0.4256419  0.4248535  0.42397943 0.42283168
 0.4216569  0.4204171  0.4192237  0.4182788  0.41751528 0.41687578
 0.41624096 0.41591522 0.41589296 0.41603708 0.4160966  0.41607302
 0.41584238 0.41565195 0.41558897 0.41531932 0.4150533  0.41489404
 0.41470268 0.4145165  0.41412508 0.41374445 0.4134347  0.41332775
 0.41331688 0.41353464 0.41394347 0.41427147 0.41437015 0.41438806
 0.41457576 0.4146598  0.41471305 0.41495365 0.41544425 0.41580126
 0.4159261  0.41576087 0.4155531  0.41545957 0.41529003 0.41516176
 0.41524526 0.4155707  0.41589546 0.41594264 0.41579083 0.41593468
 0.41640392 0.4168558  0.4174608  0.4184223  0.41955784 0.4204264
 0.42110103 0.422059   0.4231865  0.42396247 0.42432842 0.42468145
 0.42538187 0.42648306 0.42750984 0.4283987  0.42929694 0.43029806
 0.4313168  0.43199667 0.4321054  0.43154764 0.43039104 0.4288489
 0.42711833 0.42551085 0.42426658 0.4234153  0.4230496  0.42302966
 0.42304584 0.42315814 0.42361504 0.42405856 0.42386073 0.42289072
 0.42152256 0.4200545  0.4184193  0.41666418 0.41501638 0.413671
 0.41267374 0.41177917 0.41081995 0.40982708 0.4089371  0.4080399
 0.40692705 0.40556753 0.40441594 0.40366516 0.40288055 0.40207276
 0.40161282 0.40182567 0.40213406 0.40212837 0.40183428 0.40181127
 0.4018892  0.4018284  0.40159634 0.40133944 0.40140036 0.401364
 0.40116504 0.40098608 0.40081057 0.40071478 0.40070808 0.4007319
 0.4005468  0.40031627 0.40048176 0.40084308 0.4012891  0.40168768
 0.4018768  0.40205538 0.40253448 0.4027808  0.4026096  0.40220428
 0.40205374 0.4021392  0.4020758  0.40165314 0.40141654 0.4014431
 0.4013035  0.40096462 0.40100396 0.40171635 0.4023483  0.40256867
 0.4026877  0.40302238 0.40353394 0.40420786 0.40487215 0.40573657
 0.40664795 0.40754244 0.40825534 0.4088773  0.40952018 0.4101136
 0.41054308 0.41098788 0.4119594  0.41333607 0.41454738 0.41534084
 0.41593745 0.41648197 0.41667652 0.41617572 0.41511843 0.4137952
 0.41213012 0.41021433 0.40889344 0.40870953 0.40909693 0.4094105
 0.40996253 0.41102204 0.4117944  0.4114767  0.41041136 0.4094613
 0.40875    0.4075904  0.405685   0.40359253 0.40194798 0.40057808
 0.3993161  0.3982505  0.39752278 0.39679566 0.395584   0.39407718
 0.39269298 0.39155856 0.3905086  0.38974646 0.38927624 0.38861567
 0.38755524 0.38654867 0.386367   0.38652915 0.3861973  0.38592914
 0.38613066 0.38631308 0.38601696 0.38589823 0.38660312 0.38718897
 0.38675442 0.38647223 0.38725674 0.3875166  0.38703132 0.38925475]
