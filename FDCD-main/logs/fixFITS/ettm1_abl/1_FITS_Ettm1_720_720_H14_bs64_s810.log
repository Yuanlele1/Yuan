Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=810, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4590316
	speed: 0.0242s/iter; left time: 622.5696s
	iters: 200, epoch: 1 | loss: 0.4498338
	speed: 0.0180s/iter; left time: 459.8264s
Epoch: 1 cost time: 5.155066013336182
Epoch: 1, Steps: 258 | Train Loss: 0.5135781 Vali Loss: 1.0184423 Test Loss: 0.4504407
Validation loss decreased (inf --> 1.018442).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4120522
	speed: 0.0681s/iter; left time: 1733.3417s
	iters: 200, epoch: 2 | loss: 0.4010511
	speed: 0.0166s/iter; left time: 420.5785s
Epoch: 2 cost time: 4.767854452133179
Epoch: 2, Steps: 258 | Train Loss: 0.4156098 Vali Loss: 0.9651524 Test Loss: 0.4204005
Validation loss decreased (1.018442 --> 0.965152).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4440157
	speed: 0.0702s/iter; left time: 1768.6480s
	iters: 200, epoch: 3 | loss: 0.4022650
	speed: 0.0167s/iter; left time: 420.1018s
Epoch: 3 cost time: 4.815231800079346
Epoch: 3, Steps: 258 | Train Loss: 0.4032633 Vali Loss: 0.9486822 Test Loss: 0.4148730
Validation loss decreased (0.965152 --> 0.948682).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3800005
	speed: 0.0704s/iter; left time: 1755.1745s
	iters: 200, epoch: 4 | loss: 0.3746763
	speed: 0.0168s/iter; left time: 416.1126s
Epoch: 4 cost time: 4.831603765487671
Epoch: 4, Steps: 258 | Train Loss: 0.3994958 Vali Loss: 0.9426633 Test Loss: 0.4148143
Validation loss decreased (0.948682 --> 0.942663).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3997715
	speed: 0.0701s/iter; left time: 1728.3542s
	iters: 200, epoch: 5 | loss: 0.3738844
	speed: 0.0166s/iter; left time: 409.0535s
Epoch: 5 cost time: 4.767705678939819
Epoch: 5, Steps: 258 | Train Loss: 0.3983353 Vali Loss: 0.9388571 Test Loss: 0.4153440
Validation loss decreased (0.942663 --> 0.938857).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4172476
	speed: 0.0704s/iter; left time: 1719.4033s
	iters: 200, epoch: 6 | loss: 0.3684422
	speed: 0.0164s/iter; left time: 399.7898s
Epoch: 6 cost time: 4.8082897663116455
Epoch: 6, Steps: 258 | Train Loss: 0.3977343 Vali Loss: 0.9374312 Test Loss: 0.4156804
Validation loss decreased (0.938857 --> 0.937431).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3874782
	speed: 0.0698s/iter; left time: 1685.1542s
	iters: 200, epoch: 7 | loss: 0.3919099
	speed: 0.0165s/iter; left time: 397.2856s
Epoch: 7 cost time: 4.7690110206604
Epoch: 7, Steps: 258 | Train Loss: 0.3973898 Vali Loss: 0.9362667 Test Loss: 0.4157467
Validation loss decreased (0.937431 --> 0.936267).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3969052
	speed: 0.0703s/iter; left time: 1679.1114s
	iters: 200, epoch: 8 | loss: 0.4175311
	speed: 0.0169s/iter; left time: 402.8616s
Epoch: 8 cost time: 4.773739337921143
Epoch: 8, Steps: 258 | Train Loss: 0.3972798 Vali Loss: 0.9351956 Test Loss: 0.4163782
Validation loss decreased (0.936267 --> 0.935196).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3813867
	speed: 0.0726s/iter; left time: 1716.9588s
	iters: 200, epoch: 9 | loss: 0.3756909
	speed: 0.0170s/iter; left time: 400.4806s
Epoch: 9 cost time: 4.837379693984985
Epoch: 9, Steps: 258 | Train Loss: 0.3971702 Vali Loss: 0.9346515 Test Loss: 0.4161040
Validation loss decreased (0.935196 --> 0.934651).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4119387
	speed: 0.0703s/iter; left time: 1643.4539s
	iters: 200, epoch: 10 | loss: 0.3854609
	speed: 0.0166s/iter; left time: 386.3914s
Epoch: 10 cost time: 4.7765984535217285
Epoch: 10, Steps: 258 | Train Loss: 0.3969504 Vali Loss: 0.9339854 Test Loss: 0.4159244
Validation loss decreased (0.934651 --> 0.933985).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4321692
	speed: 0.0715s/iter; left time: 1652.8958s
	iters: 200, epoch: 11 | loss: 0.4099306
	speed: 0.0169s/iter; left time: 388.3115s
Epoch: 11 cost time: 4.860567808151245
Epoch: 11, Steps: 258 | Train Loss: 0.3970070 Vali Loss: 0.9316692 Test Loss: 0.4162619
Validation loss decreased (0.933985 --> 0.931669).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4213577
	speed: 0.0707s/iter; left time: 1615.3241s
	iters: 200, epoch: 12 | loss: 0.4234456
	speed: 0.0168s/iter; left time: 382.6587s
Epoch: 12 cost time: 4.740195989608765
Epoch: 12, Steps: 258 | Train Loss: 0.3968725 Vali Loss: 0.9327425 Test Loss: 0.4161350
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.3778342
	speed: 0.0704s/iter; left time: 1591.2280s
	iters: 200, epoch: 13 | loss: 0.4208555
	speed: 0.0170s/iter; left time: 382.4598s
Epoch: 13 cost time: 4.764415740966797
Epoch: 13, Steps: 258 | Train Loss: 0.3966525 Vali Loss: 0.9328851 Test Loss: 0.4162141
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4069936
	speed: 0.0706s/iter; left time: 1578.6367s
	iters: 200, epoch: 14 | loss: 0.3791398
	speed: 0.0169s/iter; left time: 375.3568s
Epoch: 14 cost time: 4.841137409210205
Epoch: 14, Steps: 258 | Train Loss: 0.3966523 Vali Loss: 0.9312015 Test Loss: 0.4163722
Validation loss decreased (0.931669 --> 0.931201).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4352792
	speed: 0.0708s/iter; left time: 1563.0385s
	iters: 200, epoch: 15 | loss: 0.3727580
	speed: 0.0166s/iter; left time: 365.8142s
Epoch: 15 cost time: 4.8708484172821045
Epoch: 15, Steps: 258 | Train Loss: 0.3966237 Vali Loss: 0.9320778 Test Loss: 0.4167092
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3610832
	speed: 0.0696s/iter; left time: 1518.7986s
	iters: 200, epoch: 16 | loss: 0.4162749
	speed: 0.0167s/iter; left time: 363.0785s
Epoch: 16 cost time: 4.747518539428711
Epoch: 16, Steps: 258 | Train Loss: 0.3966376 Vali Loss: 0.9318822 Test Loss: 0.4165617
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.3963167
	speed: 0.0716s/iter; left time: 1544.6120s
	iters: 200, epoch: 17 | loss: 0.3742717
	speed: 0.0170s/iter; left time: 366.0511s
Epoch: 17 cost time: 4.922635078430176
Epoch: 17, Steps: 258 | Train Loss: 0.3966011 Vali Loss: 0.9306032 Test Loss: 0.4163359
Validation loss decreased (0.931201 --> 0.930603).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3516748
	speed: 0.0710s/iter; left time: 1513.5213s
	iters: 200, epoch: 18 | loss: 0.4031520
	speed: 0.0170s/iter; left time: 361.1780s
Epoch: 18 cost time: 4.846845626831055
Epoch: 18, Steps: 258 | Train Loss: 0.3966642 Vali Loss: 0.9316878 Test Loss: 0.4164132
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4160146
	speed: 0.0712s/iter; left time: 1500.0144s
	iters: 200, epoch: 19 | loss: 0.4221562
	speed: 0.0173s/iter; left time: 362.3592s
Epoch: 19 cost time: 4.939305305480957
Epoch: 19, Steps: 258 | Train Loss: 0.3965297 Vali Loss: 0.9313689 Test Loss: 0.4161729
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4142998
	speed: 0.0716s/iter; left time: 1489.9863s
	iters: 200, epoch: 20 | loss: 0.4266380
	speed: 0.0175s/iter; left time: 363.2052s
Epoch: 20 cost time: 4.994717836380005
Epoch: 20, Steps: 258 | Train Loss: 0.3965455 Vali Loss: 0.9323850 Test Loss: 0.4162641
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.3745290
	speed: 0.0708s/iter; left time: 1453.6488s
	iters: 200, epoch: 21 | loss: 0.4479910
	speed: 0.0173s/iter; left time: 352.6114s
Epoch: 21 cost time: 4.831111907958984
Epoch: 21, Steps: 258 | Train Loss: 0.3964536 Vali Loss: 0.9303781 Test Loss: 0.4161294
Validation loss decreased (0.930603 --> 0.930378).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3891741
	speed: 0.0710s/iter; left time: 1439.3817s
	iters: 200, epoch: 22 | loss: 0.4224655
	speed: 0.0165s/iter; left time: 333.9874s
Epoch: 22 cost time: 4.767946243286133
Epoch: 22, Steps: 258 | Train Loss: 0.3963287 Vali Loss: 0.9315089 Test Loss: 0.4161080
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4009144
	speed: 0.0705s/iter; left time: 1412.6162s
	iters: 200, epoch: 23 | loss: 0.4047194
	speed: 0.0171s/iter; left time: 340.4552s
Epoch: 23 cost time: 4.773249387741089
Epoch: 23, Steps: 258 | Train Loss: 0.3965638 Vali Loss: 0.9307464 Test Loss: 0.4160513
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4003439
	speed: 0.0689s/iter; left time: 1362.6462s
	iters: 200, epoch: 24 | loss: 0.4427209
	speed: 0.0163s/iter; left time: 321.4180s
Epoch: 24 cost time: 4.682627439498901
Epoch: 24, Steps: 258 | Train Loss: 0.3963733 Vali Loss: 0.9314636 Test Loss: 0.4162318
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3940767
	speed: 0.0701s/iter; left time: 1366.7875s
	iters: 200, epoch: 25 | loss: 0.4187067
	speed: 0.0164s/iter; left time: 318.3596s
Epoch: 25 cost time: 4.781750917434692
Epoch: 25, Steps: 258 | Train Loss: 0.3963694 Vali Loss: 0.9312371 Test Loss: 0.4159770
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3963992
	speed: 0.0714s/iter; left time: 1375.0116s
	iters: 200, epoch: 26 | loss: 0.3765568
	speed: 0.0171s/iter; left time: 328.0599s
Epoch: 26 cost time: 4.841325759887695
Epoch: 26, Steps: 258 | Train Loss: 0.3963891 Vali Loss: 0.9306009 Test Loss: 0.4162239
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4157521
	speed: 0.0696s/iter; left time: 1322.1830s
	iters: 200, epoch: 27 | loss: 0.4277610
	speed: 0.0162s/iter; left time: 306.4178s
Epoch: 27 cost time: 4.6827452182769775
Epoch: 27, Steps: 258 | Train Loss: 0.3964805 Vali Loss: 0.9296102 Test Loss: 0.4161596
Validation loss decreased (0.930378 --> 0.929610).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3959826
	speed: 0.0672s/iter; left time: 1258.4295s
	iters: 200, epoch: 28 | loss: 0.4131195
	speed: 0.0165s/iter; left time: 307.5923s
Epoch: 28 cost time: 4.6999711990356445
Epoch: 28, Steps: 258 | Train Loss: 0.3964017 Vali Loss: 0.9309725 Test Loss: 0.4163252
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4143317
	speed: 0.0679s/iter; left time: 1254.2553s
	iters: 200, epoch: 29 | loss: 0.3596732
	speed: 0.0168s/iter; left time: 308.6097s
Epoch: 29 cost time: 4.74308443069458
Epoch: 29, Steps: 258 | Train Loss: 0.3963651 Vali Loss: 0.9309443 Test Loss: 0.4164275
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3644470
	speed: 0.0702s/iter; left time: 1278.4842s
	iters: 200, epoch: 30 | loss: 0.4070823
	speed: 0.0170s/iter; left time: 308.7623s
Epoch: 30 cost time: 4.857030868530273
Epoch: 30, Steps: 258 | Train Loss: 0.3963382 Vali Loss: 0.9303988 Test Loss: 0.4162784
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3971082
	speed: 0.0679s/iter; left time: 1219.7898s
	iters: 200, epoch: 31 | loss: 0.4210758
	speed: 0.0165s/iter; left time: 294.0884s
Epoch: 31 cost time: 4.718327760696411
Epoch: 31, Steps: 258 | Train Loss: 0.3963650 Vali Loss: 0.9314176 Test Loss: 0.4164459
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3823341
	speed: 0.0702s/iter; left time: 1242.9819s
	iters: 200, epoch: 32 | loss: 0.3962583
	speed: 0.0167s/iter; left time: 294.4531s
Epoch: 32 cost time: 4.832522869110107
Epoch: 32, Steps: 258 | Train Loss: 0.3962879 Vali Loss: 0.9305170 Test Loss: 0.4162339
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3645483
	speed: 0.0690s/iter; left time: 1204.2920s
	iters: 200, epoch: 33 | loss: 0.3878202
	speed: 0.0169s/iter; left time: 293.3899s
Epoch: 33 cost time: 4.800657033920288
Epoch: 33, Steps: 258 | Train Loss: 0.3962429 Vali Loss: 0.9304948 Test Loss: 0.4165119
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4275214
	speed: 0.0694s/iter; left time: 1193.5217s
	iters: 200, epoch: 34 | loss: 0.3928562
	speed: 0.0166s/iter; left time: 282.9715s
Epoch: 34 cost time: 4.743248462677002
Epoch: 34, Steps: 258 | Train Loss: 0.3961725 Vali Loss: 0.9308690 Test Loss: 0.4161674
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3902887
	speed: 0.0695s/iter; left time: 1176.2893s
	iters: 200, epoch: 35 | loss: 0.4315126
	speed: 0.0169s/iter; left time: 284.5509s
Epoch: 35 cost time: 4.721390962600708
Epoch: 35, Steps: 258 | Train Loss: 0.3962712 Vali Loss: 0.9303733 Test Loss: 0.4162623
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3752160
	speed: 0.0697s/iter; left time: 1161.9453s
	iters: 200, epoch: 36 | loss: 0.4118404
	speed: 0.0170s/iter; left time: 282.2719s
Epoch: 36 cost time: 4.8638105392456055
Epoch: 36, Steps: 258 | Train Loss: 0.3961929 Vali Loss: 0.9304669 Test Loss: 0.4162673
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4134779
	speed: 0.0688s/iter; left time: 1129.7300s
	iters: 200, epoch: 37 | loss: 0.4048457
	speed: 0.0166s/iter; left time: 271.3152s
Epoch: 37 cost time: 4.752965927124023
Epoch: 37, Steps: 258 | Train Loss: 0.3963303 Vali Loss: 0.9305605 Test Loss: 0.4162245
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3875392
	speed: 0.0687s/iter; left time: 1109.2112s
	iters: 200, epoch: 38 | loss: 0.3879304
	speed: 0.0170s/iter; left time: 272.4755s
Epoch: 38 cost time: 4.739226341247559
Epoch: 38, Steps: 258 | Train Loss: 0.3962342 Vali Loss: 0.9312654 Test Loss: 0.4163572
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4066606
	speed: 0.0691s/iter; left time: 1098.1330s
	iters: 200, epoch: 39 | loss: 0.4009266
	speed: 0.0170s/iter; left time: 268.9200s
Epoch: 39 cost time: 4.809746503829956
Epoch: 39, Steps: 258 | Train Loss: 0.3961898 Vali Loss: 0.9306889 Test Loss: 0.4162435
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4323106
	speed: 0.0690s/iter; left time: 1079.2111s
	iters: 200, epoch: 40 | loss: 0.4295192
	speed: 0.0170s/iter; left time: 264.7642s
Epoch: 40 cost time: 4.836083650588989
Epoch: 40, Steps: 258 | Train Loss: 0.3962524 Vali Loss: 0.9298120 Test Loss: 0.4162296
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3867231
	speed: 0.0697s/iter; left time: 1072.7906s
	iters: 200, epoch: 41 | loss: 0.3964748
	speed: 0.0169s/iter; left time: 257.9139s
Epoch: 41 cost time: 4.87217903137207
Epoch: 41, Steps: 258 | Train Loss: 0.3963211 Vali Loss: 0.9305273 Test Loss: 0.4162993
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4132802
	speed: 0.0686s/iter; left time: 1037.6541s
	iters: 200, epoch: 42 | loss: 0.3755149
	speed: 0.0169s/iter; left time: 253.5252s
Epoch: 42 cost time: 4.7615015506744385
Epoch: 42, Steps: 258 | Train Loss: 0.3961844 Vali Loss: 0.9298652 Test Loss: 0.4162206
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3984429
	speed: 0.0676s/iter; left time: 1005.4729s
	iters: 200, epoch: 43 | loss: 0.3952326
	speed: 0.0175s/iter; left time: 257.9329s
Epoch: 43 cost time: 4.856377363204956
Epoch: 43, Steps: 258 | Train Loss: 0.3961735 Vali Loss: 0.9306800 Test Loss: 0.4164106
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3911564
	speed: 0.0687s/iter; left time: 1004.1082s
	iters: 200, epoch: 44 | loss: 0.4519468
	speed: 0.0164s/iter; left time: 238.3055s
Epoch: 44 cost time: 4.733404636383057
Epoch: 44, Steps: 258 | Train Loss: 0.3962162 Vali Loss: 0.9305392 Test Loss: 0.4162697
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4183052
	speed: 0.0697s/iter; left time: 1000.1038s
	iters: 200, epoch: 45 | loss: 0.4394787
	speed: 0.0167s/iter; left time: 237.2959s
Epoch: 45 cost time: 4.775852203369141
Epoch: 45, Steps: 258 | Train Loss: 0.3962190 Vali Loss: 0.9310851 Test Loss: 0.4162432
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.3998990
	speed: 0.0675s/iter; left time: 951.5277s
	iters: 200, epoch: 46 | loss: 0.4012986
	speed: 0.0170s/iter; left time: 237.2257s
Epoch: 46 cost time: 4.751543998718262
Epoch: 46, Steps: 258 | Train Loss: 0.3960945 Vali Loss: 0.9303581 Test Loss: 0.4162075
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.3809755
	speed: 0.0702s/iter; left time: 971.1284s
	iters: 200, epoch: 47 | loss: 0.4087674
	speed: 0.0167s/iter; left time: 229.3455s
Epoch: 47 cost time: 4.82398247718811
Epoch: 47, Steps: 258 | Train Loss: 0.3962365 Vali Loss: 0.9300312 Test Loss: 0.4162506
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4151623249053955, mae:0.4115317761898041, rse:0.6130272150039673, corr:[0.52600414 0.5318651  0.53395927 0.5344278  0.53526765 0.53689885
 0.5384209  0.53912663 0.5393771  0.53969854 0.5403548  0.54106486
 0.54154366 0.54159766 0.5411801  0.5403115  0.5391791  0.5380622
 0.5370347  0.536047   0.53489053 0.5333832  0.5316655  0.53026253
 0.5291906  0.5284621  0.5277262  0.52670354 0.5258091  0.52552646
 0.52611214 0.52738065 0.5285437  0.5291279  0.52897054 0.52884126
 0.52899045 0.52938175 0.5295755  0.5291891  0.5285083  0.5278988
 0.5276901  0.52787125 0.52798283 0.5278271  0.5276633  0.5276854
 0.52777076 0.52762276 0.5272179  0.52667016 0.52637255 0.5264222
 0.5267374  0.5269454  0.52678674 0.5262437  0.5257168  0.5254354
 0.52550334 0.5257169  0.52583855 0.52570224 0.5255805  0.5256738
 0.5259148  0.52611893 0.5262344  0.52625144 0.52632844 0.52645016
 0.52650297 0.52637655 0.5261426  0.52583534 0.5255309  0.5253273
 0.52520776 0.52513295 0.5250727  0.5250126  0.5250091  0.525042
 0.52514917 0.52518827 0.5251135  0.52492434 0.52477103 0.5248808
 0.5252474  0.5257077  0.5260879  0.5262641  0.5262658  0.5261248
 0.52595115 0.52582264 0.5255359  0.5251335  0.52467775 0.52432764
 0.524226   0.52429634 0.52442265 0.5244628  0.5241854  0.52368987
 0.52309644 0.52264917 0.52241015 0.5222473  0.52203274 0.5217568
 0.52147585 0.5213232  0.52132595 0.52133244 0.5211734  0.52082926
 0.5203485  0.5199011  0.51962984 0.51968056 0.51991284 0.51996934
 0.5196693  0.519159   0.5186729  0.5184613  0.51846975 0.51854134
 0.5186077  0.51864594 0.5186688  0.5187693  0.51895785 0.518949
 0.5187099  0.5183946  0.5181833  0.51832855 0.5188277  0.51942086
 0.5198113  0.5198116  0.5195748  0.5193217  0.5192418  0.51926476
 0.5192798  0.51930994 0.51933193 0.5193421  0.5193041  0.5192644
 0.5191731  0.519024   0.5189365  0.5189003  0.51901615 0.5192409
 0.51950383 0.51972836 0.519895   0.5199921  0.52003324 0.520065
 0.52009916 0.5201569  0.5203332  0.5205905  0.5207546  0.52077687
 0.5207034  0.5206229  0.5205961  0.5206145  0.52059525 0.5205373
 0.5204529  0.5204222  0.52045435 0.52061105 0.520846   0.5210741
 0.52124625 0.5214376  0.52168524 0.5219283  0.5220633  0.521981
 0.5216586  0.521202   0.52065414 0.5200876  0.5195467  0.5190333
 0.51845795 0.5177905  0.5170449  0.51635796 0.51580846 0.51541877
 0.5150897  0.5146457  0.5139685  0.5131424  0.51233155 0.5116903
 0.5112158  0.5108456  0.5103887  0.50972325 0.50882924 0.5079831
 0.5074205  0.50711954 0.5069839  0.50682634 0.50661206 0.506478
 0.50657946 0.50677526 0.5069789  0.5071024  0.5070856  0.5070408
 0.50709665 0.50729364 0.507541   0.50765294 0.50760186 0.50748146
 0.50741035 0.50749993 0.50763774 0.50788164 0.5081035  0.5083064
 0.5084237  0.50835496 0.50818783 0.5080271  0.50792915 0.50782627
 0.5076828  0.5075318  0.50739354 0.5073232  0.5072606  0.50724894
 0.5071819  0.50713533 0.50707763 0.50702477 0.5070147  0.5070532
 0.507108   0.5071697  0.507222   0.50728667 0.50739765 0.50764954
 0.5079141  0.50814515 0.50832784 0.508491   0.5085749  0.50849664
 0.5083051  0.5080627  0.5079048  0.5079011  0.5079909  0.5080721
 0.50801086 0.50788635 0.5077072  0.50763094 0.5076479  0.5077086
 0.50783354 0.5079464  0.50801885 0.5080548  0.507981   0.5077374
 0.50735253 0.50692415 0.5064852  0.5059998  0.5054877  0.50496703
 0.5044648  0.5039467  0.5034835  0.5030298  0.50253755 0.5020026
 0.5014066  0.50085443 0.50047123 0.5002571  0.5001448  0.4999995
 0.4998623  0.49964878 0.49938592 0.4991198  0.4989281  0.49875224
 0.49856785 0.4984383  0.49833456 0.4982971  0.4983338  0.49843177
 0.49846277 0.49829975 0.49798387 0.4976752  0.49753496 0.49758196
 0.49771017 0.4978025  0.49770385 0.49746776 0.49720386 0.49703708
 0.49707243 0.49720064 0.49729565 0.4973713  0.4974564  0.49766043
 0.49785778 0.49781248 0.49758738 0.49726427 0.49701503 0.4969277
 0.49695605 0.49706545 0.49711066 0.49703643 0.496881   0.49676058
 0.4966705  0.49667197 0.49670976 0.49667445 0.49656862 0.49639747
 0.49619773 0.4960981  0.49614847 0.49632132 0.49655375 0.4967259
 0.4967119  0.49660498 0.49648058 0.49640718 0.49640277 0.49642187
 0.49634814 0.4962618  0.49620423 0.49615481 0.49617076 0.49623114
 0.4962795  0.49628353 0.49624428 0.49626154 0.4963745  0.4966326
 0.4970051  0.49740955 0.49776635 0.4980178  0.49815252 0.49818575
 0.4980902  0.49784634 0.4974665  0.49697843 0.4964434  0.49607176
 0.49584556 0.4956616  0.495367   0.49493584 0.49436918 0.4938088
 0.49337226 0.49310648 0.4929223  0.49265483 0.4921878  0.4916961
 0.49129125 0.4911152  0.49103373 0.4909411  0.49073732 0.49049407
 0.49028048 0.49019465 0.49021912 0.49030173 0.4903867  0.4903758
 0.49042583 0.4904427  0.4905129  0.49055094 0.49060306 0.4907059
 0.4907493  0.49078616 0.49076942 0.49075752 0.49074617 0.49081492
 0.49088645 0.49093863 0.49089962 0.49088955 0.49087152 0.4909566
 0.49101865 0.4909558  0.49084136 0.49077368 0.49074298 0.49069506
 0.4905573  0.49032897 0.49008715 0.48991412 0.48987338 0.48995906
 0.4900702  0.49016783 0.49019763 0.49015447 0.49007592 0.48998383
 0.48999247 0.49010128 0.49028772 0.49046007 0.4905518  0.49061057
 0.4905648  0.4905022  0.49045402 0.49046135 0.4904088  0.49033487
 0.4902499  0.49027035 0.49037445 0.49056128 0.49072927 0.49075037
 0.49062198 0.49038047 0.49020427 0.49021262 0.49041316 0.49067137
 0.49078786 0.4907366  0.49054816 0.4902643  0.489964   0.4897024
 0.4894226  0.48903692 0.48849148 0.487796   0.48706287 0.48646414
 0.48607805 0.48570737 0.485256   0.48469767 0.48398924 0.48320684
 0.48245364 0.48183817 0.4813269  0.4808366  0.48035955 0.47994152
 0.47953668 0.47917107 0.47880968 0.4783372  0.47799346 0.47782153
 0.47786704 0.4780332  0.47817272 0.47826228 0.47825092 0.47821784
 0.4782389  0.47829768 0.4784292  0.47872537 0.479062   0.47932163
 0.4794167  0.4794534  0.47951344 0.47955966 0.4796042  0.47968864
 0.47980446 0.47995317 0.4801144  0.48024648 0.48043457 0.4806962
 0.480903   0.480946   0.4808118  0.4806625  0.48059204 0.4805881
 0.4806192  0.48062822 0.480611   0.4805753  0.48053142 0.48049852
 0.4804907  0.48048246 0.4804542  0.48037905 0.48027065 0.48023754
 0.48028907 0.4804124  0.4805274  0.4805064  0.480355   0.4802496
 0.48018223 0.48019686 0.48028123 0.48038095 0.4803918  0.48034588
 0.4802774  0.48025858 0.48028716 0.48034155 0.48038688 0.4803922
 0.4803659  0.48045474 0.48054627 0.48062202 0.48063162 0.4805921
 0.48056343 0.48061252 0.48078784 0.48093325 0.48086005 0.48047358
 0.4798614  0.47921455 0.4786375  0.47814727 0.47771487 0.4773706
 0.4770232  0.47664165 0.47620082 0.47573295 0.4752873  0.4748124
 0.47431824 0.47373813 0.4730765  0.4723691  0.4717358  0.47120968
 0.4707905  0.47043753 0.47015676 0.4699289  0.46974215 0.46958873
 0.46950248 0.46938574 0.46922737 0.46906757 0.46904182 0.46918732
 0.46958452 0.47000763 0.47026774 0.47030947 0.47023028 0.47026315
 0.47047827 0.4707211  0.4708991  0.47089276 0.4707804  0.47071955
 0.4708831  0.47126457 0.4715977  0.47176948 0.47191456 0.4720838
 0.4722076  0.47230643 0.47234303 0.4723075  0.47228122 0.47226176
 0.47214058 0.47202617 0.47197795 0.47194663 0.47199175 0.4719707
 0.471757   0.47141993 0.47111028 0.47083983 0.47076485 0.4708123
 0.47087196 0.47082672 0.47066733 0.47047982 0.47032562 0.47028694
 0.47030306 0.47026286 0.47025225 0.47029194 0.4703384  0.470354
 0.47035727 0.47041205 0.47057074 0.47078937 0.47093984 0.47091854
 0.47075334 0.47064686 0.47068182 0.4708129  0.47105768 0.4712916
 0.47144133 0.47150293 0.47154766 0.47163    0.47167566 0.47163665
 0.4715156  0.47137386 0.47114933 0.4707428  0.47020045 0.4696651
 0.46927142 0.46896145 0.4687165  0.46838832 0.46782306 0.46732116
 0.46686438 0.46662194 0.46642026 0.4661838  0.4658013  0.46531412
 0.4649387  0.4647401  0.4646902  0.46450323 0.46421134 0.46394366
 0.46378216 0.46387    0.46405116 0.4641381  0.4641382  0.46398744
 0.46395457 0.46412823 0.46451467 0.46502754 0.46525425 0.4652184
 0.46518818 0.46551844 0.4662847  0.46714664 0.46779177 0.4679642
 0.4679983  0.46820986 0.46852404 0.4690291  0.46896753 0.4649823 ]
