Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=58, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9042432.0
params:  10266.0
Trainable parameters:  10266
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5366586
	speed: 0.0291s/iter; left time: 755.8405s
	iters: 200, epoch: 1 | loss: 0.4309441
	speed: 0.0267s/iter; left time: 690.8861s
Epoch: 1 cost time: 7.015427827835083
Epoch: 1, Steps: 261 | Train Loss: 0.5594487 Vali Loss: 1.0360963 Test Loss: 0.4584263
Validation loss decreased (inf --> 1.036096).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4135041
	speed: 0.1222s/iter; left time: 3146.5213s
	iters: 200, epoch: 2 | loss: 0.4169903
	speed: 0.0238s/iter; left time: 609.4500s
Epoch: 2 cost time: 6.984199047088623
Epoch: 2, Steps: 261 | Train Loss: 0.4329190 Vali Loss: 0.9890658 Test Loss: 0.4327212
Validation loss decreased (1.036096 --> 0.989066).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4266525
	speed: 0.1284s/iter; left time: 3272.1023s
	iters: 200, epoch: 3 | loss: 0.3967448
	speed: 0.0225s/iter; left time: 570.1899s
Epoch: 3 cost time: 7.000253915786743
Epoch: 3, Steps: 261 | Train Loss: 0.4208975 Vali Loss: 0.9763185 Test Loss: 0.4277084
Validation loss decreased (0.989066 --> 0.976318).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4695561
	speed: 0.1098s/iter; left time: 2767.9559s
	iters: 200, epoch: 4 | loss: 0.4185094
	speed: 0.0243s/iter; left time: 609.3491s
Epoch: 4 cost time: 6.555670738220215
Epoch: 4, Steps: 261 | Train Loss: 0.4170580 Vali Loss: 0.9685829 Test Loss: 0.4264745
Validation loss decreased (0.976318 --> 0.968583).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4141659
	speed: 0.1180s/iter; left time: 2945.9279s
	iters: 200, epoch: 5 | loss: 0.4209197
	speed: 0.0221s/iter; left time: 549.6027s
Epoch: 5 cost time: 7.15558385848999
Epoch: 5, Steps: 261 | Train Loss: 0.4153724 Vali Loss: 0.9670480 Test Loss: 0.4261367
Validation loss decreased (0.968583 --> 0.967048).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4034377
	speed: 0.1091s/iter; left time: 2695.4097s
	iters: 200, epoch: 6 | loss: 0.4115436
	speed: 0.0214s/iter; left time: 525.2638s
Epoch: 6 cost time: 6.293375730514526
Epoch: 6, Steps: 261 | Train Loss: 0.4146283 Vali Loss: 0.9646428 Test Loss: 0.4258630
Validation loss decreased (0.967048 --> 0.964643).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3874463
	speed: 0.1130s/iter; left time: 2760.4577s
	iters: 200, epoch: 7 | loss: 0.3828548
	speed: 0.0279s/iter; left time: 678.2103s
Epoch: 7 cost time: 6.9205732345581055
Epoch: 7, Steps: 261 | Train Loss: 0.4141713 Vali Loss: 0.9640263 Test Loss: 0.4262449
Validation loss decreased (0.964643 --> 0.964026).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3997116
	speed: 0.1123s/iter; left time: 2714.6815s
	iters: 200, epoch: 8 | loss: 0.4254701
	speed: 0.0212s/iter; left time: 509.4828s
Epoch: 8 cost time: 6.549576282501221
Epoch: 8, Steps: 261 | Train Loss: 0.4140484 Vali Loss: 0.9626976 Test Loss: 0.4262180
Validation loss decreased (0.964026 --> 0.962698).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4309849
	speed: 0.1255s/iter; left time: 3001.6052s
	iters: 200, epoch: 9 | loss: 0.4058508
	speed: 0.0213s/iter; left time: 507.6228s
Epoch: 9 cost time: 6.515329837799072
Epoch: 9, Steps: 261 | Train Loss: 0.4138340 Vali Loss: 0.9620782 Test Loss: 0.4266351
Validation loss decreased (0.962698 --> 0.962078).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4075810
	speed: 0.1128s/iter; left time: 2667.2514s
	iters: 200, epoch: 10 | loss: 0.4660316
	speed: 0.0206s/iter; left time: 486.2360s
Epoch: 10 cost time: 6.841510057449341
Epoch: 10, Steps: 261 | Train Loss: 0.4137130 Vali Loss: 0.9615371 Test Loss: 0.4263752
Validation loss decreased (0.962078 --> 0.961537).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4361242
	speed: 0.1163s/iter; left time: 2720.5612s
	iters: 200, epoch: 11 | loss: 0.4337719
	speed: 0.0248s/iter; left time: 577.6407s
Epoch: 11 cost time: 6.8850343227386475
Epoch: 11, Steps: 261 | Train Loss: 0.4137425 Vali Loss: 0.9613668 Test Loss: 0.4265211
Validation loss decreased (0.961537 --> 0.961367).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4052246
	speed: 0.1127s/iter; left time: 2606.2364s
	iters: 200, epoch: 12 | loss: 0.3943265
	speed: 0.0246s/iter; left time: 565.3876s
Epoch: 12 cost time: 7.0243308544158936
Epoch: 12, Steps: 261 | Train Loss: 0.4136949 Vali Loss: 0.9610658 Test Loss: 0.4267524
Validation loss decreased (0.961367 --> 0.961066).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4192241
	speed: 0.1278s/iter; left time: 2922.9747s
	iters: 200, epoch: 13 | loss: 0.3692229
	speed: 0.0206s/iter; left time: 469.8960s
Epoch: 13 cost time: 7.403096914291382
Epoch: 13, Steps: 261 | Train Loss: 0.4135729 Vali Loss: 0.9611194 Test Loss: 0.4265510
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3847275
	speed: 0.1079s/iter; left time: 2439.4628s
	iters: 200, epoch: 14 | loss: 0.4549825
	speed: 0.0193s/iter; left time: 433.4734s
Epoch: 14 cost time: 6.422996759414673
Epoch: 14, Steps: 261 | Train Loss: 0.4135806 Vali Loss: 0.9604643 Test Loss: 0.4268045
Validation loss decreased (0.961066 --> 0.960464).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4363319
	speed: 0.1055s/iter; left time: 2356.6889s
	iters: 200, epoch: 15 | loss: 0.3967497
	speed: 0.0254s/iter; left time: 564.2279s
Epoch: 15 cost time: 6.882110834121704
Epoch: 15, Steps: 261 | Train Loss: 0.4135369 Vali Loss: 0.9609353 Test Loss: 0.4265003
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4086736
	speed: 0.1091s/iter; left time: 2409.2904s
	iters: 200, epoch: 16 | loss: 0.3687368
	speed: 0.0221s/iter; left time: 485.0078s
Epoch: 16 cost time: 6.906869649887085
Epoch: 16, Steps: 261 | Train Loss: 0.4135495 Vali Loss: 0.9602171 Test Loss: 0.4266780
Validation loss decreased (0.960464 --> 0.960217).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4033020
	speed: 0.1507s/iter; left time: 3289.1645s
	iters: 200, epoch: 17 | loss: 0.4229129
	speed: 0.0239s/iter; left time: 519.8350s
Epoch: 17 cost time: 7.243125677108765
Epoch: 17, Steps: 261 | Train Loss: 0.4135102 Vali Loss: 0.9602836 Test Loss: 0.4270341
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4276116
	speed: 0.1366s/iter; left time: 2945.7142s
	iters: 200, epoch: 18 | loss: 0.4248410
	speed: 0.0255s/iter; left time: 546.4487s
Epoch: 18 cost time: 8.70215916633606
Epoch: 18, Steps: 261 | Train Loss: 0.4134915 Vali Loss: 0.9603429 Test Loss: 0.4268993
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4215144
	speed: 0.1165s/iter; left time: 2481.0654s
	iters: 200, epoch: 19 | loss: 0.3963551
	speed: 0.0213s/iter; left time: 451.5785s
Epoch: 19 cost time: 6.340125322341919
Epoch: 19, Steps: 261 | Train Loss: 0.4134389 Vali Loss: 0.9601163 Test Loss: 0.4268261
Validation loss decreased (0.960217 --> 0.960116).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4186953
	speed: 0.1039s/iter; left time: 2185.6425s
	iters: 200, epoch: 20 | loss: 0.4172510
	speed: 0.0249s/iter; left time: 521.2515s
Epoch: 20 cost time: 7.007631063461304
Epoch: 20, Steps: 261 | Train Loss: 0.4134143 Vali Loss: 0.9601654 Test Loss: 0.4266898
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4198425
	speed: 0.1351s/iter; left time: 2807.5425s
	iters: 200, epoch: 21 | loss: 0.4355614
	speed: 0.0289s/iter; left time: 596.6909s
Epoch: 21 cost time: 8.14420771598816
Epoch: 21, Steps: 261 | Train Loss: 0.4135222 Vali Loss: 0.9595343 Test Loss: 0.4269000
Validation loss decreased (0.960116 --> 0.959534).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4162243
	speed: 0.1477s/iter; left time: 3029.9521s
	iters: 200, epoch: 22 | loss: 0.4181631
	speed: 0.0221s/iter; left time: 450.5129s
Epoch: 22 cost time: 7.545000791549683
Epoch: 22, Steps: 261 | Train Loss: 0.4133249 Vali Loss: 0.9592875 Test Loss: 0.4268370
Validation loss decreased (0.959534 --> 0.959287).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4179758
	speed: 0.1177s/iter; left time: 2385.0690s
	iters: 200, epoch: 23 | loss: 0.3660882
	speed: 0.0270s/iter; left time: 544.4346s
Epoch: 23 cost time: 7.316231966018677
Epoch: 23, Steps: 261 | Train Loss: 0.4133377 Vali Loss: 0.9604865 Test Loss: 0.4268513
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4088863
	speed: 0.1277s/iter; left time: 2554.3365s
	iters: 200, epoch: 24 | loss: 0.4054366
	speed: 0.0261s/iter; left time: 519.1923s
Epoch: 24 cost time: 7.39257550239563
Epoch: 24, Steps: 261 | Train Loss: 0.4133720 Vali Loss: 0.9594061 Test Loss: 0.4266754
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4173364
	speed: 0.1108s/iter; left time: 2187.3473s
	iters: 200, epoch: 25 | loss: 0.4212804
	speed: 0.0242s/iter; left time: 475.2246s
Epoch: 25 cost time: 7.133270740509033
Epoch: 25, Steps: 261 | Train Loss: 0.4134136 Vali Loss: 0.9593180 Test Loss: 0.4270263
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4042749
	speed: 0.1236s/iter; left time: 2407.0123s
	iters: 200, epoch: 26 | loss: 0.3920017
	speed: 0.0270s/iter; left time: 523.8299s
Epoch: 26 cost time: 7.561939239501953
Epoch: 26, Steps: 261 | Train Loss: 0.4133090 Vali Loss: 0.9598103 Test Loss: 0.4270034
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4014343
	speed: 0.1269s/iter; left time: 2438.2012s
	iters: 200, epoch: 27 | loss: 0.4130600
	speed: 0.0332s/iter; left time: 634.6553s
Epoch: 27 cost time: 8.533808946609497
Epoch: 27, Steps: 261 | Train Loss: 0.4134364 Vali Loss: 0.9607986 Test Loss: 0.4269756
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4071127
	speed: 0.1392s/iter; left time: 2638.0038s
	iters: 200, epoch: 28 | loss: 0.4290701
	speed: 0.0300s/iter; left time: 566.3819s
Epoch: 28 cost time: 10.302417516708374
Epoch: 28, Steps: 261 | Train Loss: 0.4133202 Vali Loss: 0.9590966 Test Loss: 0.4270755
Validation loss decreased (0.959287 --> 0.959097).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4076132
	speed: 0.1454s/iter; left time: 2717.0521s
	iters: 200, epoch: 29 | loss: 0.4044932
	speed: 0.0519s/iter; left time: 964.4573s
Epoch: 29 cost time: 13.619171619415283
Epoch: 29, Steps: 261 | Train Loss: 0.4133577 Vali Loss: 0.9594232 Test Loss: 0.4271276
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3827988
	speed: 0.2339s/iter; left time: 4310.8458s
	iters: 200, epoch: 30 | loss: 0.4302806
	speed: 0.0472s/iter; left time: 864.4413s
Epoch: 30 cost time: 13.532860040664673
Epoch: 30, Steps: 261 | Train Loss: 0.4133040 Vali Loss: 0.9602235 Test Loss: 0.4268289
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4079688
	speed: 0.1481s/iter; left time: 2691.5116s
	iters: 200, epoch: 31 | loss: 0.3975365
	speed: 0.0245s/iter; left time: 443.4511s
Epoch: 31 cost time: 7.893232345581055
Epoch: 31, Steps: 261 | Train Loss: 0.4133077 Vali Loss: 0.9597701 Test Loss: 0.4269105
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3966317
	speed: 0.1197s/iter; left time: 2143.8170s
	iters: 200, epoch: 32 | loss: 0.4185130
	speed: 0.0260s/iter; left time: 463.4190s
Epoch: 32 cost time: 7.712441444396973
Epoch: 32, Steps: 261 | Train Loss: 0.4133487 Vali Loss: 0.9599229 Test Loss: 0.4270276
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3628951
	speed: 0.1296s/iter; left time: 2288.0818s
	iters: 200, epoch: 33 | loss: 0.3920215
	speed: 0.0299s/iter; left time: 524.0337s
Epoch: 33 cost time: 8.430842876434326
Epoch: 33, Steps: 261 | Train Loss: 0.4132343 Vali Loss: 0.9600978 Test Loss: 0.4269465
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4406408
	speed: 0.1238s/iter; left time: 2152.0458s
	iters: 200, epoch: 34 | loss: 0.4413170
	speed: 0.0260s/iter; left time: 449.3230s
Epoch: 34 cost time: 7.957547664642334
Epoch: 34, Steps: 261 | Train Loss: 0.4133131 Vali Loss: 0.9593847 Test Loss: 0.4269826
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3822242
	speed: 0.1197s/iter; left time: 2050.1012s
	iters: 200, epoch: 35 | loss: 0.3872403
	speed: 0.0318s/iter; left time: 541.7952s
Epoch: 35 cost time: 8.101518869400024
Epoch: 35, Steps: 261 | Train Loss: 0.4131551 Vali Loss: 0.9599577 Test Loss: 0.4267793
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3928595
	speed: 0.1128s/iter; left time: 1902.2757s
	iters: 200, epoch: 36 | loss: 0.3857150
	speed: 0.0223s/iter; left time: 374.5469s
Epoch: 36 cost time: 6.362566947937012
Epoch: 36, Steps: 261 | Train Loss: 0.4132954 Vali Loss: 0.9602209 Test Loss: 0.4269681
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3866484
	speed: 0.1103s/iter; left time: 1832.3410s
	iters: 200, epoch: 37 | loss: 0.4084234
	speed: 0.0214s/iter; left time: 353.3657s
Epoch: 37 cost time: 6.723071575164795
Epoch: 37, Steps: 261 | Train Loss: 0.4132818 Vali Loss: 0.9603469 Test Loss: 0.4269028
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4117839
	speed: 0.1197s/iter; left time: 1956.8881s
	iters: 200, epoch: 38 | loss: 0.4538380
	speed: 0.0290s/iter; left time: 470.3813s
Epoch: 38 cost time: 7.602738380432129
Epoch: 38, Steps: 261 | Train Loss: 0.4132373 Vali Loss: 0.9593105 Test Loss: 0.4268495
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4003055
	speed: 0.1456s/iter; left time: 2341.1449s
	iters: 200, epoch: 39 | loss: 0.4208210
	speed: 0.0504s/iter; left time: 806.1436s
Epoch: 39 cost time: 10.910768747329712
Epoch: 39, Steps: 261 | Train Loss: 0.4132679 Vali Loss: 0.9603131 Test Loss: 0.4269413
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4363013
	speed: 0.1452s/iter; left time: 2298.0265s
	iters: 200, epoch: 40 | loss: 0.4235017
	speed: 0.0395s/iter; left time: 621.6407s
Epoch: 40 cost time: 10.096771478652954
Epoch: 40, Steps: 261 | Train Loss: 0.4132825 Vali Loss: 0.9594720 Test Loss: 0.4269889
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4136288
	speed: 0.1389s/iter; left time: 2160.6730s
	iters: 200, epoch: 41 | loss: 0.4161577
	speed: 0.0282s/iter; left time: 435.2915s
Epoch: 41 cost time: 8.514131307601929
Epoch: 41, Steps: 261 | Train Loss: 0.4131606 Vali Loss: 0.9601381 Test Loss: 0.4269112
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4393985
	speed: 0.1413s/iter; left time: 2162.6208s
	iters: 200, epoch: 42 | loss: 0.3706617
	speed: 0.0276s/iter; left time: 419.2661s
Epoch: 42 cost time: 8.038762331008911
Epoch: 42, Steps: 261 | Train Loss: 0.4132529 Vali Loss: 0.9597503 Test Loss: 0.4269476
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4175180
	speed: 0.1751s/iter; left time: 2633.3573s
	iters: 200, epoch: 43 | loss: 0.4096385
	speed: 0.0611s/iter; left time: 912.7305s
Epoch: 43 cost time: 15.984162092208862
Epoch: 43, Steps: 261 | Train Loss: 0.4132277 Vali Loss: 0.9592365 Test Loss: 0.4269682
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4222744
	speed: 0.2183s/iter; left time: 3225.9358s
	iters: 200, epoch: 44 | loss: 0.4226723
	speed: 0.0351s/iter; left time: 515.0657s
Epoch: 44 cost time: 9.097670555114746
Epoch: 44, Steps: 261 | Train Loss: 0.4132436 Vali Loss: 0.9599242 Test Loss: 0.4270192
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4254507
	speed: 0.1390s/iter; left time: 2017.7519s
	iters: 200, epoch: 45 | loss: 0.4627122
	speed: 0.0281s/iter; left time: 405.2287s
Epoch: 45 cost time: 8.153632879257202
Epoch: 45, Steps: 261 | Train Loss: 0.4132713 Vali Loss: 0.9602717 Test Loss: 0.4270218
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4243129
	speed: 0.1287s/iter; left time: 1834.5190s
	iters: 200, epoch: 46 | loss: 0.3762117
	speed: 0.0294s/iter; left time: 415.7176s
Epoch: 46 cost time: 8.079221487045288
Epoch: 46, Steps: 261 | Train Loss: 0.4132811 Vali Loss: 0.9593500 Test Loss: 0.4269491
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4030386
	speed: 0.1562s/iter; left time: 2185.9836s
	iters: 200, epoch: 47 | loss: 0.3825371
	speed: 0.0231s/iter; left time: 320.6495s
Epoch: 47 cost time: 10.196376323699951
Epoch: 47, Steps: 261 | Train Loss: 0.4132216 Vali Loss: 0.9601609 Test Loss: 0.4269695
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.3978071
	speed: 0.1058s/iter; left time: 1453.6738s
	iters: 200, epoch: 48 | loss: 0.4123191
	speed: 0.0243s/iter; left time: 331.9728s
Epoch: 48 cost time: 7.2094995975494385
Epoch: 48, Steps: 261 | Train Loss: 0.4132315 Vali Loss: 0.9605455 Test Loss: 0.4269823
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4267483055591583, mae:0.4156379699707031, rse:0.6215221881866455, corr:[0.527512   0.532058   0.531987   0.53108925 0.5311966  0.53192127
 0.5323444  0.5322648  0.5322385  0.5327739  0.533773   0.53433084
 0.5342211  0.5338045  0.5334362  0.5329843  0.5322167  0.5311042
 0.52962726 0.5281638  0.5268988  0.52602804 0.5251721  0.5242151
 0.5230375  0.5216249  0.520349   0.5194296  0.5187919  0.5182142
 0.5179621  0.5182264  0.5187856  0.5192941  0.5194164  0.5193198
 0.5190752  0.5187572  0.5185481  0.51843005 0.51829034 0.51793575
 0.51738256 0.5169306  0.51672596 0.5167511  0.5168875  0.5168225
 0.5164833  0.51618624 0.5160939  0.51617837 0.5163705  0.5164451
 0.51636356 0.51627225 0.5162496  0.51634115 0.51650614 0.51647377
 0.5163551  0.5161028  0.5158663  0.5156271  0.5155419  0.5156478
 0.5158005  0.5158924  0.5161366  0.5164572  0.5168113  0.5170876
 0.5172874  0.51736325 0.51726764 0.5170547  0.5168634  0.51677155
 0.5167029  0.51653373 0.51624215 0.5159357  0.5156943  0.5155007
 0.51543087 0.5153013  0.5151382  0.5148995  0.5146314  0.51452094
 0.51458925 0.5146884  0.51471454 0.51465094 0.51443964 0.51401466
 0.5134601  0.5128797  0.51230234 0.51175815 0.5113824  0.5112391
 0.5113122  0.51147366 0.5118881  0.5125079  0.5130165  0.51335454
 0.51330596 0.5131226  0.51294947 0.5129794  0.5131502  0.5134122
 0.5135342  0.51348335 0.5132844  0.51308846 0.5129833  0.51313406
 0.5133519  0.5133549  0.5131262  0.5128822  0.5126477  0.5123573
 0.51202023 0.51170146 0.5114447  0.51133484 0.51136625 0.51141053
 0.5113634  0.5110512  0.51057297 0.510057   0.50970745 0.5095492
 0.50959265 0.5096637  0.50960314 0.5094901  0.5094012  0.50944585
 0.50958645 0.50972545 0.5097278  0.50963247 0.50958323 0.50953436
 0.50952667 0.50962555 0.5096538  0.5095568  0.5094294  0.5094272
 0.5095483  0.50972956 0.5098251  0.50984    0.50985956 0.5099106
 0.51002866 0.51015466 0.5103095  0.5104479  0.5105477  0.5107491
 0.51104814 0.5114035  0.51174843 0.51203245 0.51212525 0.51208264
 0.51196307 0.5118443  0.5117033  0.5115567  0.5113451  0.51116246
 0.5110178  0.5109919  0.510937   0.5108539  0.5107332  0.5106493
 0.5106304  0.5106944  0.51075566 0.51071644 0.51050234 0.51014614
 0.5097217  0.50934535 0.5089471  0.50840265 0.50779694 0.5073733
 0.50710094 0.50687826 0.5067182  0.50656074 0.50634223 0.50613564
 0.50597394 0.5057016  0.5053609  0.5048715  0.50431764 0.50381076
 0.50330466 0.50289476 0.5025137  0.5021938  0.5018415  0.50154114
 0.5012893  0.50103533 0.5007307  0.5005228  0.5003525  0.5002356
 0.5002531  0.50030243 0.5003087  0.50022453 0.5000919  0.49987847
 0.49970356 0.49967188 0.4997194  0.49970976 0.49962306 0.49942118
 0.49924755 0.49917084 0.49909005 0.49911925 0.49914795 0.4991199
 0.49903083 0.4989251  0.49888098 0.49883845 0.49874005 0.49866584
 0.4986254  0.49863803 0.4987251  0.4988717  0.49891436 0.49889404
 0.49873847 0.49861014 0.49851376 0.49848574 0.49843168 0.49846697
 0.4985062  0.49857795 0.4987003  0.49886218 0.49899206 0.49926662
 0.4996303  0.49998957 0.5002005  0.500347   0.50038797 0.5002858
 0.50021344 0.5001753  0.5001108  0.49999577 0.49977288 0.4995758
 0.49939695 0.4993626  0.49932945 0.49935615 0.49933165 0.49923766
 0.49916145 0.49903095 0.49886748 0.49863178 0.4982953  0.49783167
 0.49719515 0.49654528 0.49598026 0.49535042 0.49480155 0.49436823
 0.4940307  0.49362364 0.49326092 0.49290058 0.49259335 0.49227852
 0.4919361  0.49157968 0.49125826 0.4910012  0.49078193 0.4906051
 0.49052122 0.49045417 0.49042353 0.49044725 0.4905442  0.49066833
 0.49083832 0.49095964 0.49095196 0.49094936 0.49091753 0.4908621
 0.49077567 0.49065694 0.49050984 0.49039373 0.49031532 0.49022844
 0.49009433 0.4898771  0.48954502 0.4892025  0.48885354 0.4885965
 0.48842466 0.48832706 0.4882222  0.4880796  0.4879302  0.48782888
 0.48777884 0.48779407 0.4878348  0.48776117 0.48766845 0.48761326
 0.48756686 0.48757136 0.48764646 0.48770103 0.48777065 0.48786888
 0.48791382 0.4879837  0.4880569  0.48806006 0.4880126  0.48794672
 0.48785803 0.4878024  0.48786524 0.48800823 0.4881827  0.4883698
 0.4884772  0.48850787 0.4884839  0.48846045 0.4884798  0.48854345
 0.48858592 0.4885919  0.4885419  0.4883895  0.48825157 0.48820788
 0.48829737 0.48843107 0.48853797 0.48857585 0.48854735 0.488552
 0.48868537 0.48892984 0.4891828  0.48928905 0.48914114 0.48873645
 0.48811466 0.48745677 0.4868803  0.4863419  0.48586252 0.48558939
 0.4854615  0.48532635 0.4852204  0.48516807 0.48506036 0.4849084
 0.48464164 0.4843997  0.48419383 0.48402318 0.48384196 0.48369253
 0.48365748 0.4837078  0.48377585 0.48382127 0.48385805 0.4839246
 0.48399848 0.48393625 0.48381442 0.48374808 0.48368692 0.48360407
 0.48352587 0.4834212  0.4833654  0.48329094 0.48319262 0.48304066
 0.4828139  0.48259845 0.48235157 0.4821586  0.4819975  0.48186615
 0.48172495 0.48165792 0.4815388  0.48139504 0.4812267  0.48117763
 0.4810943  0.48100078 0.4809658  0.48092705 0.48080453 0.4806725
 0.48056796 0.48057806 0.48071265 0.48088446 0.48102778 0.48110235
 0.48100716 0.48086616 0.48072654 0.48063502 0.48060554 0.48062283
 0.4806714  0.48076087 0.4808692  0.4809842  0.48110324 0.481258
 0.48129994 0.48129162 0.4812293  0.4812392  0.48131523 0.48141426
 0.48150584 0.48159212 0.48157614 0.4815631  0.48158032 0.481653
 0.48176962 0.481846   0.48184344 0.48178104 0.48169675 0.48164245
 0.4815629  0.48145717 0.48124886 0.48086888 0.48030874 0.47961965
 0.4788349  0.478061   0.47727516 0.4764285  0.4755898  0.4748577
 0.47427934 0.4737518  0.4733033  0.47296977 0.47264338 0.47225505
 0.4718613  0.47150645 0.47112784 0.47070712 0.47037962 0.470095
 0.4698904  0.46989188 0.470073   0.470281   0.47052875 0.47080866
 0.4711017  0.47123024 0.47124857 0.47125557 0.4712778  0.47134128
 0.47138152 0.47135288 0.47130343 0.47134948 0.4714443  0.47161174
 0.471715   0.471651   0.47144827 0.47105014 0.47062805 0.47037455
 0.47028467 0.47026092 0.4702063  0.4700628  0.46988994 0.46981052
 0.46976462 0.4696698  0.46952078 0.46934578 0.46921316 0.46918273
 0.46923313 0.46926078 0.469257   0.46920595 0.46914172 0.46911433
 0.46914402 0.469141   0.46905673 0.46890116 0.46877465 0.4687014
 0.46864593 0.46860892 0.4686378  0.46869352 0.4687341  0.46886697
 0.46898708 0.46914074 0.46927568 0.4694154  0.46956596 0.46966875
 0.4697732  0.46990752 0.4699998  0.4700263  0.4699801  0.46991444
 0.46983668 0.4699029  0.46997812 0.470036   0.47002587 0.4699479
 0.4699013  0.46986353 0.4698339  0.46966493 0.46923897 0.46851054
 0.46757916 0.4666167  0.4657911  0.46495876 0.46418917 0.4635919
 0.4631719  0.4627949  0.46253353 0.46237177 0.46217254 0.46187228
 0.46152198 0.461131   0.46076265 0.46043673 0.4601136  0.45986688
 0.45957068 0.4594069  0.45935944 0.45942065 0.4595409  0.4597596
 0.46002954 0.46013084 0.46008116 0.4600536  0.46007314 0.46007636
 0.46003512 0.45996264 0.45982572 0.45965403 0.4594871  0.45938936
 0.4593047  0.45922205 0.45911768 0.45888698 0.45861143 0.4583076
 0.45806938 0.45791656 0.45784935 0.45771107 0.457585   0.45750195
 0.45743415 0.45743486 0.45746538 0.4574193  0.45734203 0.45724425
 0.4570904  0.45693335 0.45688424 0.4569069  0.45699427 0.45699474
 0.45684105 0.45659727 0.45635447 0.45615467 0.45611468 0.45610803
 0.45609313 0.45609325 0.45609537 0.45618063 0.45632213 0.45650962
 0.45674738 0.45694506 0.45706913 0.45716703 0.45722777 0.45730388
 0.45739114 0.45748383 0.45755473 0.4576474  0.45773855 0.45782596
 0.45785654 0.45781362 0.45773968 0.45766288 0.45770073 0.45783418
 0.45799303 0.45808744 0.45801356 0.4577878  0.45745075 0.45700255
 0.45638406 0.45575213 0.45512286 0.4544212  0.45385987 0.45353338
 0.4533648  0.4532229  0.45304507 0.45281467 0.45263508 0.4526064
 0.45265582 0.4526774  0.45255896 0.45218757 0.45171678 0.45121032
 0.4509605  0.45087036 0.45078358 0.45067883 0.45054013 0.4506577
 0.45089823 0.45089328 0.45057413 0.4501633  0.44970244 0.44921073
 0.44881338 0.44851658 0.44824472 0.44798034 0.44764805 0.4474451
 0.44744077 0.4475365  0.44750488 0.44729742 0.44712886 0.44714358
 0.4475217  0.44819865 0.44890124 0.44960788 0.45067382 0.45129856]
