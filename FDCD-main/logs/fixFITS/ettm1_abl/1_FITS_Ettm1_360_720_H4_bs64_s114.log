Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=26, out_features=78, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1817088.0
params:  2106.0
Trainable parameters:  2106
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6324213
	speed: 0.1255s/iter; left time: 3264.3942s
	iters: 200, epoch: 1 | loss: 0.4892401
	speed: 0.1098s/iter; left time: 2842.7877s
Epoch: 1 cost time: 29.93880271911621
Epoch: 1, Steps: 261 | Train Loss: 0.6007551 Vali Loss: 1.0760570 Test Loss: 0.4866279
Validation loss decreased (inf --> 1.076057).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4536410
	speed: 0.4456s/iter; left time: 11469.4948s
	iters: 200, epoch: 2 | loss: 0.4500107
	speed: 0.1105s/iter; left time: 2833.7226s
Epoch: 2 cost time: 26.623817920684814
Epoch: 2, Steps: 261 | Train Loss: 0.4434267 Vali Loss: 0.9989974 Test Loss: 0.4388226
Validation loss decreased (1.076057 --> 0.998997).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4178883
	speed: 0.4055s/iter; left time: 10330.7140s
	iters: 200, epoch: 3 | loss: 0.4020602
	speed: 0.1029s/iter; left time: 2610.2486s
Epoch: 3 cost time: 25.785900354385376
Epoch: 3, Steps: 261 | Train Loss: 0.4261396 Vali Loss: 0.9814531 Test Loss: 0.4320460
Validation loss decreased (0.998997 --> 0.981453).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4567399
	speed: 0.3938s/iter; left time: 9929.7439s
	iters: 200, epoch: 4 | loss: 0.4110372
	speed: 0.1113s/iter; left time: 2795.4238s
Epoch: 4 cost time: 29.033593893051147
Epoch: 4, Steps: 261 | Train Loss: 0.4213807 Vali Loss: 0.9731745 Test Loss: 0.4301906
Validation loss decreased (0.981453 --> 0.973175).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4439858
	speed: 0.5904s/iter; left time: 14734.6439s
	iters: 200, epoch: 5 | loss: 0.4187364
	speed: 0.1296s/iter; left time: 3220.5630s
Epoch: 5 cost time: 34.71934771537781
Epoch: 5, Steps: 261 | Train Loss: 0.4194903 Vali Loss: 0.9697973 Test Loss: 0.4296511
Validation loss decreased (0.973175 --> 0.969797).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4252049
	speed: 0.4476s/iter; left time: 11053.2028s
	iters: 200, epoch: 6 | loss: 0.4075831
	speed: 0.1078s/iter; left time: 2652.1372s
Epoch: 6 cost time: 27.895923137664795
Epoch: 6, Steps: 261 | Train Loss: 0.4184190 Vali Loss: 0.9687564 Test Loss: 0.4295026
Validation loss decreased (0.969797 --> 0.968756).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4164247
	speed: 0.4362s/iter; left time: 10659.2485s
	iters: 200, epoch: 7 | loss: 0.4178636
	speed: 0.1066s/iter; left time: 2593.1777s
Epoch: 7 cost time: 27.113597869873047
Epoch: 7, Steps: 261 | Train Loss: 0.4179184 Vali Loss: 0.9665416 Test Loss: 0.4294411
Validation loss decreased (0.968756 --> 0.966542).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4403259
	speed: 0.4507s/iter; left time: 10894.8635s
	iters: 200, epoch: 8 | loss: 0.4038189
	speed: 0.1160s/iter; left time: 2792.0038s
Epoch: 8 cost time: 30.01351547241211
Epoch: 8, Steps: 261 | Train Loss: 0.4177593 Vali Loss: 0.9673776 Test Loss: 0.4296309
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4359877
	speed: 0.5336s/iter; left time: 12759.0836s
	iters: 200, epoch: 9 | loss: 0.4355851
	speed: 0.1376s/iter; left time: 3276.6394s
Epoch: 9 cost time: 35.623223304748535
Epoch: 9, Steps: 261 | Train Loss: 0.4176298 Vali Loss: 0.9650662 Test Loss: 0.4299937
Validation loss decreased (0.966542 --> 0.965066).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4429263
	speed: 0.5728s/iter; left time: 13547.4625s
	iters: 200, epoch: 10 | loss: 0.4227698
	speed: 0.1416s/iter; left time: 3335.9358s
Epoch: 10 cost time: 34.63081169128418
Epoch: 10, Steps: 261 | Train Loss: 0.4174874 Vali Loss: 0.9652861 Test Loss: 0.4300831
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3986730
	speed: 0.4587s/iter; left time: 10729.8544s
	iters: 200, epoch: 11 | loss: 0.4071844
	speed: 0.1153s/iter; left time: 2686.0964s
Epoch: 11 cost time: 30.394789457321167
Epoch: 11, Steps: 261 | Train Loss: 0.4174384 Vali Loss: 0.9648195 Test Loss: 0.4300773
Validation loss decreased (0.965066 --> 0.964819).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.3833758
	speed: 0.4156s/iter; left time: 9612.5658s
	iters: 200, epoch: 12 | loss: 0.4248292
	speed: 0.1043s/iter; left time: 2401.9946s
Epoch: 12 cost time: 27.469499826431274
Epoch: 12, Steps: 261 | Train Loss: 0.4172962 Vali Loss: 0.9641066 Test Loss: 0.4301479
Validation loss decreased (0.964819 --> 0.964107).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4507919
	speed: 0.4284s/iter; left time: 9797.4636s
	iters: 200, epoch: 13 | loss: 0.4232748
	speed: 0.1037s/iter; left time: 2362.0449s
Epoch: 13 cost time: 26.733739614486694
Epoch: 13, Steps: 261 | Train Loss: 0.4173543 Vali Loss: 0.9642861 Test Loss: 0.4301136
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4362908
	speed: 0.4867s/iter; left time: 11002.9426s
	iters: 200, epoch: 14 | loss: 0.3957439
	speed: 0.1054s/iter; left time: 2372.6379s
Epoch: 14 cost time: 30.514710426330566
Epoch: 14, Steps: 261 | Train Loss: 0.4172822 Vali Loss: 0.9642024 Test Loss: 0.4304034
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4643798
	speed: 0.4397s/iter; left time: 9824.8789s
	iters: 200, epoch: 15 | loss: 0.3772341
	speed: 0.1091s/iter; left time: 2427.7754s
Epoch: 15 cost time: 27.761621236801147
Epoch: 15, Steps: 261 | Train Loss: 0.4172268 Vali Loss: 0.9649369 Test Loss: 0.4302432
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4275900
	speed: 0.4090s/iter; left time: 9032.8245s
	iters: 200, epoch: 16 | loss: 0.4164648
	speed: 0.1159s/iter; left time: 2548.0638s
Epoch: 16 cost time: 28.56757116317749
Epoch: 16, Steps: 261 | Train Loss: 0.4173636 Vali Loss: 0.9643049 Test Loss: 0.4302042
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4438427
	speed: 0.5123s/iter; left time: 11180.7546s
	iters: 200, epoch: 17 | loss: 0.4572465
	speed: 0.1126s/iter; left time: 2446.8458s
Epoch: 17 cost time: 30.63015103340149
Epoch: 17, Steps: 261 | Train Loss: 0.4173677 Vali Loss: 0.9647959 Test Loss: 0.4302734
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4322060
	speed: 0.4385s/iter; left time: 9456.7323s
	iters: 200, epoch: 18 | loss: 0.4082248
	speed: 0.0983s/iter; left time: 2109.4822s
Epoch: 18 cost time: 26.710148811340332
Epoch: 18, Steps: 261 | Train Loss: 0.4171382 Vali Loss: 0.9645566 Test Loss: 0.4303217
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4520333
	speed: 0.4092s/iter; left time: 8716.8811s
	iters: 200, epoch: 19 | loss: 0.4162471
	speed: 0.0903s/iter; left time: 1913.8548s
Epoch: 19 cost time: 24.959965705871582
Epoch: 19, Steps: 261 | Train Loss: 0.4172285 Vali Loss: 0.9638686 Test Loss: 0.4304854
Validation loss decreased (0.964107 --> 0.963869).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4227633
	speed: 0.4319s/iter; left time: 9088.0642s
	iters: 200, epoch: 20 | loss: 0.4074708
	speed: 0.1064s/iter; left time: 2227.5891s
Epoch: 20 cost time: 28.202253341674805
Epoch: 20, Steps: 261 | Train Loss: 0.4172019 Vali Loss: 0.9637680 Test Loss: 0.4301870
Validation loss decreased (0.963869 --> 0.963768).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4072012
	speed: 0.4505s/iter; left time: 9360.9727s
	iters: 200, epoch: 21 | loss: 0.4232123
	speed: 0.1050s/iter; left time: 2170.6100s
Epoch: 21 cost time: 28.25079655647278
Epoch: 21, Steps: 261 | Train Loss: 0.4171597 Vali Loss: 0.9639404 Test Loss: 0.4302071
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4229766
	speed: 0.4785s/iter; left time: 9819.4965s
	iters: 200, epoch: 22 | loss: 0.4114083
	speed: 0.1281s/iter; left time: 2614.9962s
Epoch: 22 cost time: 32.37961506843567
Epoch: 22, Steps: 261 | Train Loss: 0.4172155 Vali Loss: 0.9634277 Test Loss: 0.4303098
Validation loss decreased (0.963768 --> 0.963428).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4262264
	speed: 0.5504s/iter; left time: 11150.2804s
	iters: 200, epoch: 23 | loss: 0.4253397
	speed: 0.1464s/iter; left time: 2950.4153s
Epoch: 23 cost time: 36.41071581840515
Epoch: 23, Steps: 261 | Train Loss: 0.4172685 Vali Loss: 0.9636553 Test Loss: 0.4303870
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4466883
	speed: 0.5253s/iter; left time: 10505.6802s
	iters: 200, epoch: 24 | loss: 0.4001283
	speed: 0.1092s/iter; left time: 2172.6678s
Epoch: 24 cost time: 28.85627841949463
Epoch: 24, Steps: 261 | Train Loss: 0.4170996 Vali Loss: 0.9640857 Test Loss: 0.4305950
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4428936
	speed: 0.4315s/iter; left time: 8515.9994s
	iters: 200, epoch: 25 | loss: 0.3841141
	speed: 0.0941s/iter; left time: 1848.6771s
Epoch: 25 cost time: 26.164443016052246
Epoch: 25, Steps: 261 | Train Loss: 0.4171468 Vali Loss: 0.9645619 Test Loss: 0.4304163
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3990401
	speed: 0.3940s/iter; left time: 7672.7294s
	iters: 200, epoch: 26 | loss: 0.4223015
	speed: 0.0944s/iter; left time: 1829.3512s
Epoch: 26 cost time: 25.138949394226074
Epoch: 26, Steps: 261 | Train Loss: 0.4171049 Vali Loss: 0.9633527 Test Loss: 0.4303660
Validation loss decreased (0.963428 --> 0.963353).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4240641
	speed: 0.3631s/iter; left time: 6977.8585s
	iters: 200, epoch: 27 | loss: 0.3895800
	speed: 0.0974s/iter; left time: 1862.3067s
Epoch: 27 cost time: 26.087666988372803
Epoch: 27, Steps: 261 | Train Loss: 0.4170707 Vali Loss: 0.9640520 Test Loss: 0.4304568
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4301617
	speed: 0.4072s/iter; left time: 7718.8592s
	iters: 200, epoch: 28 | loss: 0.3867434
	speed: 0.0947s/iter; left time: 1784.6395s
Epoch: 28 cost time: 25.956934690475464
Epoch: 28, Steps: 261 | Train Loss: 0.4171755 Vali Loss: 0.9645368 Test Loss: 0.4304135
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3809510
	speed: 0.4270s/iter; left time: 7981.0203s
	iters: 200, epoch: 29 | loss: 0.4315388
	speed: 0.1165s/iter; left time: 2166.3264s
Epoch: 29 cost time: 32.53346037864685
Epoch: 29, Steps: 261 | Train Loss: 0.4170852 Vali Loss: 0.9632536 Test Loss: 0.4305329
Validation loss decreased (0.963353 --> 0.963254).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4167233
	speed: 0.6722s/iter; left time: 12389.5230s
	iters: 200, epoch: 30 | loss: 0.4196860
	speed: 0.1452s/iter; left time: 2662.2305s
Epoch: 30 cost time: 40.23146605491638
Epoch: 30, Steps: 261 | Train Loss: 0.4170629 Vali Loss: 0.9638203 Test Loss: 0.4303707
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4195516
	speed: 0.6425s/iter; left time: 11674.5973s
	iters: 200, epoch: 31 | loss: 0.4161959
	speed: 0.1452s/iter; left time: 2624.2590s
Epoch: 31 cost time: 40.94415831565857
Epoch: 31, Steps: 261 | Train Loss: 0.4171644 Vali Loss: 0.9636915 Test Loss: 0.4304114
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4312357
	speed: 0.6545s/iter; left time: 11722.9365s
	iters: 200, epoch: 32 | loss: 0.3796054
	speed: 0.1471s/iter; left time: 2620.0526s
Epoch: 32 cost time: 38.76094961166382
Epoch: 32, Steps: 261 | Train Loss: 0.4170807 Vali Loss: 0.9643000 Test Loss: 0.4304973
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4257649
	speed: 0.6150s/iter; left time: 10853.3757s
	iters: 200, epoch: 33 | loss: 0.4473058
	speed: 0.1528s/iter; left time: 2681.0434s
Epoch: 33 cost time: 40.95945358276367
Epoch: 33, Steps: 261 | Train Loss: 0.4171774 Vali Loss: 0.9637030 Test Loss: 0.4305100
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3956195
	speed: 0.6544s/iter; left time: 11379.3131s
	iters: 200, epoch: 34 | loss: 0.3894333
	speed: 0.1529s/iter; left time: 2644.0816s
Epoch: 34 cost time: 41.491069316864014
Epoch: 34, Steps: 261 | Train Loss: 0.4171311 Vali Loss: 0.9635720 Test Loss: 0.4305340
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4131168
	speed: 0.6731s/iter; left time: 11528.5363s
	iters: 200, epoch: 35 | loss: 0.3813036
	speed: 0.1661s/iter; left time: 2829.0165s
Epoch: 35 cost time: 44.48460507392883
Epoch: 35, Steps: 261 | Train Loss: 0.4171011 Vali Loss: 0.9639457 Test Loss: 0.4305675
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4098590
	speed: 0.6866s/iter; left time: 11579.4762s
	iters: 200, epoch: 36 | loss: 0.3889784
	speed: 0.1508s/iter; left time: 2529.1222s
Epoch: 36 cost time: 40.339030742645264
Epoch: 36, Steps: 261 | Train Loss: 0.4170642 Vali Loss: 0.9632760 Test Loss: 0.4304890
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3913257
	speed: 0.6653s/iter; left time: 11046.8019s
	iters: 200, epoch: 37 | loss: 0.4155326
	speed: 0.1480s/iter; left time: 2443.4605s
Epoch: 37 cost time: 40.54745650291443
Epoch: 37, Steps: 261 | Train Loss: 0.4171720 Vali Loss: 0.9634503 Test Loss: 0.4304503
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4531855
	speed: 0.6387s/iter; left time: 10439.1998s
	iters: 200, epoch: 38 | loss: 0.4543923
	speed: 0.1477s/iter; left time: 2399.3738s
Epoch: 38 cost time: 39.558910608291626
Epoch: 38, Steps: 261 | Train Loss: 0.4170332 Vali Loss: 0.9623395 Test Loss: 0.4305043
Validation loss decreased (0.963254 --> 0.962340).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4509534
	speed: 0.6790s/iter; left time: 10920.8362s
	iters: 200, epoch: 39 | loss: 0.3893980
	speed: 0.1605s/iter; left time: 2564.7664s
Epoch: 39 cost time: 40.119940519332886
Epoch: 39, Steps: 261 | Train Loss: 0.4170870 Vali Loss: 0.9637786 Test Loss: 0.4304696
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4719543
	speed: 0.6622s/iter; left time: 10477.8034s
	iters: 200, epoch: 40 | loss: 0.3775661
	speed: 0.1540s/iter; left time: 2421.5456s
Epoch: 40 cost time: 40.92327642440796
Epoch: 40, Steps: 261 | Train Loss: 0.4170775 Vali Loss: 0.9635481 Test Loss: 0.4304560
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4102339
	speed: 0.6452s/iter; left time: 10039.6984s
	iters: 200, epoch: 41 | loss: 0.4274149
	speed: 0.1520s/iter; left time: 2350.3187s
Epoch: 41 cost time: 41.074846029281616
Epoch: 41, Steps: 261 | Train Loss: 0.4170826 Vali Loss: 0.9639062 Test Loss: 0.4305365
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4473328
	speed: 0.6452s/iter; left time: 9871.5987s
	iters: 200, epoch: 42 | loss: 0.4319757
	speed: 0.1610s/iter; left time: 2447.6308s
Epoch: 42 cost time: 40.40149211883545
Epoch: 42, Steps: 261 | Train Loss: 0.4170998 Vali Loss: 0.9638538 Test Loss: 0.4304969
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4319814
	speed: 0.6310s/iter; left time: 9489.5986s
	iters: 200, epoch: 43 | loss: 0.4043413
	speed: 0.1708s/iter; left time: 2551.0343s
Epoch: 43 cost time: 41.82157039642334
Epoch: 43, Steps: 261 | Train Loss: 0.4170347 Vali Loss: 0.9632550 Test Loss: 0.4304319
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3950032
	speed: 0.6395s/iter; left time: 9449.9530s
	iters: 200, epoch: 44 | loss: 0.4755474
	speed: 0.1301s/iter; left time: 1909.3412s
Epoch: 44 cost time: 39.606924533843994
Epoch: 44, Steps: 261 | Train Loss: 0.4169429 Vali Loss: 0.9642171 Test Loss: 0.4305226
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.3792925
	speed: 0.6681s/iter; left time: 9698.9635s
	iters: 200, epoch: 45 | loss: 0.4210843
	speed: 0.1523s/iter; left time: 2196.2646s
Epoch: 45 cost time: 40.95350980758667
Epoch: 45, Steps: 261 | Train Loss: 0.4169886 Vali Loss: 0.9638932 Test Loss: 0.4305475
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.3985011
	speed: 0.6351s/iter; left time: 9054.4201s
	iters: 200, epoch: 46 | loss: 0.4562366
	speed: 0.1473s/iter; left time: 2085.0371s
Epoch: 46 cost time: 39.222893476486206
Epoch: 46, Steps: 261 | Train Loss: 0.4170458 Vali Loss: 0.9640354 Test Loss: 0.4305525
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4426116
	speed: 0.6755s/iter; left time: 9453.3139s
	iters: 200, epoch: 47 | loss: 0.3962447
	speed: 0.1602s/iter; left time: 2225.4599s
Epoch: 47 cost time: 41.134185791015625
Epoch: 47, Steps: 261 | Train Loss: 0.4170795 Vali Loss: 0.9638084 Test Loss: 0.4305176
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4094034
	speed: 0.6365s/iter; left time: 8741.0829s
	iters: 200, epoch: 48 | loss: 0.4430781
	speed: 0.1596s/iter; left time: 2175.5608s
Epoch: 48 cost time: 43.41282892227173
Epoch: 48, Steps: 261 | Train Loss: 0.4170575 Vali Loss: 0.9634363 Test Loss: 0.4304769
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4508486
	speed: 0.6912s/iter; left time: 9313.1180s
	iters: 200, epoch: 49 | loss: 0.4141155
	speed: 0.1488s/iter; left time: 1989.5958s
Epoch: 49 cost time: 42.152724504470825
Epoch: 49, Steps: 261 | Train Loss: 0.4170924 Vali Loss: 0.9631667 Test Loss: 0.4305439
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.4171016
	speed: 0.6795s/iter; left time: 8978.1178s
	iters: 200, epoch: 50 | loss: 0.3981218
	speed: 0.1431s/iter; left time: 1876.5344s
Epoch: 50 cost time: 40.532920837402344
Epoch: 50, Steps: 261 | Train Loss: 0.4169264 Vali Loss: 0.9631464 Test Loss: 0.4305224
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.3941393
	speed: 0.6358s/iter; left time: 8234.8576s
	iters: 200, epoch: 51 | loss: 0.4110546
	speed: 0.1643s/iter; left time: 2111.6815s
Epoch: 51 cost time: 41.493364334106445
Epoch: 51, Steps: 261 | Train Loss: 0.4169852 Vali Loss: 0.9636966 Test Loss: 0.4305477
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.4202390
	speed: 0.6790s/iter; left time: 8615.9685s
	iters: 200, epoch: 52 | loss: 0.4152803
	speed: 0.1697s/iter; left time: 2136.1856s
Epoch: 52 cost time: 43.933018922805786
Epoch: 52, Steps: 261 | Train Loss: 0.4170385 Vali Loss: 0.9637691 Test Loss: 0.4305142
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4306812
	speed: 0.6637s/iter; left time: 8248.6112s
	iters: 200, epoch: 53 | loss: 0.4309832
	speed: 0.1361s/iter; left time: 1677.8517s
Epoch: 53 cost time: 37.97071290016174
Epoch: 53, Steps: 261 | Train Loss: 0.4169287 Vali Loss: 0.9640597 Test Loss: 0.4305575
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4155444
	speed: 0.5784s/iter; left time: 7037.5557s
	iters: 200, epoch: 54 | loss: 0.3955053
	speed: 0.1314s/iter; left time: 1585.7104s
Epoch: 54 cost time: 31.36658501625061
Epoch: 54, Steps: 261 | Train Loss: 0.4170346 Vali Loss: 0.9632815 Test Loss: 0.4305410
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.4179766
	speed: 0.5347s/iter; left time: 6366.1428s
	iters: 200, epoch: 55 | loss: 0.4160158
	speed: 0.1325s/iter; left time: 1564.7379s
Epoch: 55 cost time: 37.49201560020447
Epoch: 55, Steps: 261 | Train Loss: 0.4170201 Vali Loss: 0.9634646 Test Loss: 0.4305163
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4199334
	speed: 0.5993s/iter; left time: 6979.6614s
	iters: 200, epoch: 56 | loss: 0.4418760
	speed: 0.1442s/iter; left time: 1664.8524s
Epoch: 56 cost time: 37.430989503860474
Epoch: 56, Steps: 261 | Train Loss: 0.4169625 Vali Loss: 0.9646685 Test Loss: 0.4305214
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.4202938
	speed: 0.5646s/iter; left time: 6427.9517s
	iters: 200, epoch: 57 | loss: 0.4488932
	speed: 0.1539s/iter; left time: 1737.3090s
Epoch: 57 cost time: 36.19218850135803
Epoch: 57, Steps: 261 | Train Loss: 0.4171340 Vali Loss: 0.9637437 Test Loss: 0.4305454
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.3656896
	speed: 0.5980s/iter; left time: 6652.4435s
	iters: 200, epoch: 58 | loss: 0.4077994
	speed: 0.1287s/iter; left time: 1419.0907s
Epoch: 58 cost time: 37.44153380393982
Epoch: 58, Steps: 261 | Train Loss: 0.4170226 Vali Loss: 0.9637544 Test Loss: 0.4305796
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4301971197128296, mae:0.41870376467704773, rse:0.6240286231040955, corr:[0.5279357  0.53077054 0.53230095 0.53264695 0.53220296 0.53152424
 0.53100926 0.53087366 0.53117406 0.53187054 0.5328785  0.5337385
 0.5341904  0.5340993  0.5334696  0.5323373  0.53093565 0.52947026
 0.52794135 0.52641207 0.52489686 0.52361095 0.522495   0.52165216
 0.52098626 0.5203222  0.51968616 0.5191342  0.51864773 0.51810217
 0.5176031  0.51725656 0.5170307  0.51692027 0.51687163 0.5169296
 0.51703495 0.51709795 0.5170815  0.5169376  0.5166855  0.51631325
 0.5158332  0.51534367 0.5148841  0.51450056 0.5142847  0.514195
 0.5141796  0.5142403  0.51430225 0.51430535 0.51426435 0.5141479
 0.513969   0.5137707  0.51355034 0.5133579  0.5132475  0.51317656
 0.5132212  0.5133259  0.51347226 0.5135721  0.5136652  0.5137672
 0.5138325  0.5138409  0.51393414 0.5140872  0.5143159  0.5145925
 0.5149109  0.51525104 0.51554984 0.51574457 0.5158026  0.5157314
 0.51553017 0.5152053  0.51479185 0.5143371  0.5138544  0.5133616
 0.51297855 0.51268834 0.51255906 0.5125435  0.512595   0.5127228
 0.5128916  0.51296747 0.5128528  0.51252747 0.51200664 0.5113149
 0.5105968  0.5099914  0.5095269  0.50923294 0.5091559  0.5092947
 0.5096002  0.5099314  0.51031286 0.5107284  0.5110525  0.51134735
 0.5115168  0.5116632  0.5117423  0.51175374 0.51166546 0.51156723
 0.5114689  0.5114083  0.511355   0.5113227  0.5112719  0.511294
 0.51132685 0.5112258  0.5109667  0.510683   0.510394   0.51008004
 0.5097843  0.50956905 0.5094415  0.50940937 0.5094418  0.5094806
 0.5095093  0.5094604  0.50936276 0.50919735 0.5089797  0.50868946
 0.5083805  0.508073   0.50777966 0.5075672  0.5074498  0.50743836
 0.50750387 0.50761265 0.50771266 0.50778997 0.5078666  0.5078702
 0.5078032  0.5077303  0.50763726 0.50752765 0.5074378  0.5074348
 0.50749654 0.5076351  0.5078068  0.508002   0.5082411  0.50847906
 0.5087028  0.50890094 0.5091263  0.509375   0.50962526 0.50991875
 0.5101911  0.5104085  0.5105841  0.51072466 0.5107815  0.5107596
 0.5106637  0.51052934 0.51037425 0.5102216  0.51004404 0.5098721
 0.5097022  0.50958675 0.50946563 0.50937676 0.50931644 0.5092887
 0.50926155 0.50922555 0.5091487  0.5090212  0.5087489  0.5083431
 0.50784403 0.50738984 0.5069868  0.5065513  0.50610477 0.50578517
 0.5055379  0.5052826  0.5050632  0.5048639  0.5046328  0.5043893
 0.50414056 0.50380534 0.50343364 0.5029802  0.50248164 0.5019626
 0.5013798  0.50079954 0.5002254  0.49976176 0.4993881  0.49916527
 0.49904594 0.49895015 0.49876636 0.49860165 0.49839634 0.4981545
 0.49796638 0.49782005 0.49773303 0.49770102 0.49774516 0.49780804
 0.4978701  0.49793723 0.4979423  0.4978709  0.4977535  0.49755698
 0.49735713 0.49716064 0.49693644 0.49675962 0.49660942 0.4964885
 0.49639282 0.49630824 0.4962723  0.4962733  0.4962881  0.49632946
 0.49637735 0.496405   0.4964109  0.49641532 0.4963879  0.49636877
 0.49632266 0.4963191  0.49634    0.49638617 0.49641424 0.49647644
 0.49654076 0.49663776 0.496787   0.49698913 0.49717337 0.49742338
 0.49766657 0.4978958  0.49806446 0.49821842 0.49832758 0.49832806
 0.4982699  0.49816558 0.49802488 0.4978656  0.4976757  0.4975302
 0.49740323 0.49736834 0.49732065 0.497309   0.49727622 0.49720043
 0.4971234  0.49695525 0.49669623 0.49632147 0.49581164 0.49517998
 0.4944235  0.4936971  0.4931012  0.49252227 0.49201712 0.4916117
 0.4912826  0.49092776 0.49061126 0.49029398 0.489996   0.4896915
 0.48938262 0.48907325 0.48880184 0.48858112 0.48837855 0.48819366
 0.4880498  0.4878965  0.48776898 0.48771626 0.4877549  0.48785517
 0.48800808 0.48811728 0.48806548 0.4879764  0.48782656 0.4876624
 0.48751017 0.48737866 0.4872697  0.48718718 0.4871137  0.48702547
 0.4869213  0.48679814 0.48663837 0.48647165 0.48625994 0.4860488
 0.48584038 0.48565805 0.48548734 0.48533878 0.48522082 0.48514304
 0.48507679 0.48503453 0.48501647 0.48497418 0.48494503 0.48494753
 0.48493212 0.48491126 0.48490342 0.48488024 0.4848703  0.48489696
 0.48490265 0.4849396  0.48498943 0.4850298  0.48508748 0.4851715
 0.4852573  0.48534766 0.48545596 0.48556253 0.48565987 0.4857829
 0.485866   0.4859386  0.48598632 0.4860219  0.48603424 0.4860226
 0.4859726  0.48591226 0.48586416 0.48579845 0.4857509  0.4857389
 0.48578215 0.48586905 0.48599946 0.4861682  0.48633292 0.4864843
 0.48661783 0.48669973 0.48671454 0.48656943 0.4862705  0.48586497
 0.48535827 0.48486564 0.48446167 0.48407754 0.48368317 0.4833847
 0.48315716 0.48290122 0.48264858 0.48244098 0.4822317  0.48204222
 0.4818212  0.48164552 0.48149812 0.4813678  0.48120853 0.48103422
 0.48090303 0.48083147 0.4808082  0.4808298  0.4809057  0.48105833
 0.48125914 0.48135343 0.48134974 0.48131922 0.481244   0.48111868
 0.48098764 0.48084798 0.48074374 0.48065862 0.48058292 0.4804992
 0.48037386 0.4802287  0.4800199  0.47978994 0.47953495 0.47928503
 0.4790372  0.47885555 0.4786926  0.4785728  0.4784771  0.47846305
 0.4784193  0.478339   0.47825295 0.4781736  0.47808287 0.4780154
 0.47795597 0.47791412 0.47790015 0.4778948  0.47791606 0.4779725
 0.47800264 0.4780337  0.47804114 0.4780407  0.47804746 0.47807646
 0.47813177 0.47822073 0.47831836 0.47842625 0.47853965 0.47870454
 0.47881848 0.47893542 0.47903636 0.47915766 0.47926098 0.4793267
 0.4793511  0.47936243 0.47933662 0.47932038 0.4793057  0.47931093
 0.47934046 0.47938788 0.47943258 0.479468   0.47946584 0.47941566
 0.47925812 0.479002   0.47863525 0.47812566 0.47748142 0.4767465
 0.47593164 0.47515747 0.47444302 0.4737585  0.47312427 0.472576
 0.47211587 0.4716446  0.47116596 0.47070852 0.47022468 0.46970734
 0.46923164 0.46884987 0.46852976 0.46823356 0.46801493 0.4678037
 0.46761927 0.46754614 0.46758953 0.4677035  0.46791235 0.46822056
 0.46860394 0.46889046 0.4690565  0.46912754 0.46910402 0.46904954
 0.46897882 0.4688968  0.46882984 0.46879753 0.4687652  0.46874171
 0.4687018  0.46862802 0.4685586  0.46842322 0.46822882 0.4680431
 0.46786866 0.4676953  0.4675357  0.4673671  0.4671723  0.46702898
 0.46688586 0.46672237 0.4665661  0.4664212  0.466309   0.46623552
 0.4662009  0.46616992 0.46614224 0.4661159  0.46608463 0.4660577
 0.4660503  0.46603435 0.46600696 0.4659814  0.465992   0.46602476
 0.46603873 0.4660269  0.46602425 0.46603045 0.46603656 0.46611795
 0.46619475 0.46631625 0.46646658 0.46665397 0.46685967 0.46701694
 0.46712175 0.46718925 0.4672123  0.46721303 0.46719784 0.46718904
 0.46717358 0.46722338 0.4672724  0.46732798 0.4673694  0.46736208
 0.4673025  0.46711966 0.46683478 0.46638545 0.46575296 0.4649405
 0.46403605 0.46316624 0.46245825 0.46179515 0.4612103  0.46076202
 0.46042332 0.4600766  0.4597535  0.4594555  0.45912817 0.4587577
 0.45838556 0.4580258  0.4576995  0.45742586 0.45717502 0.45698628
 0.45679018 0.4566892  0.4566564  0.45670855 0.45683092 0.45706657
 0.45738384 0.45758963 0.4576215  0.45756567 0.45746097 0.4572897
 0.45710307 0.45695385 0.45685276 0.45679003 0.45675272 0.45673922
 0.45669642 0.45661315 0.45649084 0.4562949  0.45606357 0.45580393
 0.4555548  0.45534492 0.4551829  0.45502248 0.45491832 0.45488057
 0.454875   0.45490822 0.45495892 0.45499274 0.45501292 0.45500296
 0.45492092 0.45476472 0.45457658 0.4543658  0.4541598  0.45394993
 0.45373988 0.45356846 0.45345438 0.45339727 0.4534449  0.4535302
 0.45362827 0.45372504 0.45380005 0.45387462 0.45393756 0.45401648
 0.45412928 0.454267   0.45444328 0.4546594  0.45486614 0.4550422
 0.4551657  0.45523342 0.4552377  0.45521063 0.4551532  0.4550985
 0.4550723  0.4551068  0.45521116 0.45533875 0.45547706 0.45558184
 0.45561057 0.4555335  0.4552902  0.45488194 0.45432562 0.4536276
 0.45280865 0.45209694 0.45157325 0.4511402  0.4508227  0.45059597
 0.45039925 0.4501993  0.4499911  0.44976428 0.4495223  0.4492994
 0.44908    0.44885907 0.44866243 0.4484447  0.44823566 0.44797984
 0.44781983 0.44774386 0.44773883 0.44779715 0.44787169 0.44807243
 0.44831234 0.4483594  0.44815123 0.4478374  0.44742587 0.44690695
 0.44638103 0.44593313 0.4455959  0.44536066 0.44515997 0.44498116
 0.4448151  0.44468084 0.44458705 0.4445508  0.4446149  0.44473258
 0.44494116 0.4453219  0.445873   0.4465366  0.4471015  0.44708538]
