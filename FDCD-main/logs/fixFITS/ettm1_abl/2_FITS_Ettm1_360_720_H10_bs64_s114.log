Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=50, out_features=150, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6720000.0
params:  7650.0
Trainable parameters:  7650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5659061
	speed: 0.0238s/iter; left time: 619.2730s
	iters: 200, epoch: 1 | loss: 0.4422568
	speed: 0.0170s/iter; left time: 440.6337s
Epoch: 1 cost time: 5.093541383743286
Epoch: 1, Steps: 261 | Train Loss: 0.5495204 Vali Loss: 1.1757894 Test Loss: 0.5646350
Validation loss decreased (inf --> 1.175789).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3841625
	speed: 0.0774s/iter; left time: 1992.7323s
	iters: 200, epoch: 2 | loss: 0.3308475
	speed: 0.0169s/iter; left time: 434.0734s
Epoch: 2 cost time: 4.894272089004517
Epoch: 2, Steps: 261 | Train Loss: 0.3546742 Vali Loss: 1.0542527 Test Loss: 0.4762204
Validation loss decreased (1.175789 --> 1.054253).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3225777
	speed: 0.0829s/iter; left time: 2112.4409s
	iters: 200, epoch: 3 | loss: 0.2898312
	speed: 0.0167s/iter; left time: 424.5184s
Epoch: 3 cost time: 4.958997488021851
Epoch: 3, Steps: 261 | Train Loss: 0.3158430 Vali Loss: 1.0121739 Test Loss: 0.4502425
Validation loss decreased (1.054253 --> 1.012174).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3010237
	speed: 0.0828s/iter; left time: 2088.5054s
	iters: 200, epoch: 4 | loss: 0.3078651
	speed: 0.0165s/iter; left time: 413.9537s
Epoch: 4 cost time: 4.851694345474243
Epoch: 4, Steps: 261 | Train Loss: 0.3015438 Vali Loss: 0.9913300 Test Loss: 0.4401352
Validation loss decreased (1.012174 --> 0.991330).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2700914
	speed: 0.0795s/iter; left time: 1984.3403s
	iters: 200, epoch: 5 | loss: 0.2791800
	speed: 0.0165s/iter; left time: 409.4301s
Epoch: 5 cost time: 4.808563709259033
Epoch: 5, Steps: 261 | Train Loss: 0.2943920 Vali Loss: 0.9813180 Test Loss: 0.4352061
Validation loss decreased (0.991330 --> 0.981318).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3164038
	speed: 0.0773s/iter; left time: 1907.8558s
	iters: 200, epoch: 6 | loss: 0.2836857
	speed: 0.0161s/iter; left time: 396.0119s
Epoch: 6 cost time: 4.776891469955444
Epoch: 6, Steps: 261 | Train Loss: 0.2903871 Vali Loss: 0.9755877 Test Loss: 0.4324948
Validation loss decreased (0.981318 --> 0.975588).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2909144
	speed: 0.0870s/iter; left time: 2125.2832s
	iters: 200, epoch: 7 | loss: 0.2935010
	speed: 0.0162s/iter; left time: 394.2294s
Epoch: 7 cost time: 4.7358903884887695
Epoch: 7, Steps: 261 | Train Loss: 0.2879416 Vali Loss: 0.9725006 Test Loss: 0.4311857
Validation loss decreased (0.975588 --> 0.972501).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2746052
	speed: 0.0792s/iter; left time: 1913.5184s
	iters: 200, epoch: 8 | loss: 0.2770609
	speed: 0.0165s/iter; left time: 397.4465s
Epoch: 8 cost time: 4.9094789028167725
Epoch: 8, Steps: 261 | Train Loss: 0.2864489 Vali Loss: 0.9692912 Test Loss: 0.4301666
Validation loss decreased (0.972501 --> 0.969291).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2977947
	speed: 0.0952s/iter; left time: 2277.6945s
	iters: 200, epoch: 9 | loss: 0.2862238
	speed: 0.0170s/iter; left time: 404.5465s
Epoch: 9 cost time: 4.9278883934021
Epoch: 9, Steps: 261 | Train Loss: 0.2854368 Vali Loss: 0.9674450 Test Loss: 0.4294939
Validation loss decreased (0.969291 --> 0.967445).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2804177
	speed: 0.0784s/iter; left time: 1853.6684s
	iters: 200, epoch: 10 | loss: 0.2744133
	speed: 0.0165s/iter; left time: 387.7646s
Epoch: 10 cost time: 4.885717153549194
Epoch: 10, Steps: 261 | Train Loss: 0.2847158 Vali Loss: 0.9654414 Test Loss: 0.4291830
Validation loss decreased (0.967445 --> 0.965441).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3099233
	speed: 0.0807s/iter; left time: 1888.0614s
	iters: 200, epoch: 11 | loss: 0.2644183
	speed: 0.0159s/iter; left time: 370.2206s
Epoch: 11 cost time: 4.63791036605835
Epoch: 11, Steps: 261 | Train Loss: 0.2842582 Vali Loss: 0.9660511 Test Loss: 0.4290618
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2564608
	speed: 0.0764s/iter; left time: 1766.2404s
	iters: 200, epoch: 12 | loss: 0.3059132
	speed: 0.0167s/iter; left time: 383.8818s
Epoch: 12 cost time: 4.977339029312134
Epoch: 12, Steps: 261 | Train Loss: 0.2838718 Vali Loss: 0.9656616 Test Loss: 0.4288961
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2510509
	speed: 0.0957s/iter; left time: 2187.9425s
	iters: 200, epoch: 13 | loss: 0.2967607
	speed: 0.0330s/iter; left time: 751.3213s
Epoch: 13 cost time: 8.27756953239441
Epoch: 13, Steps: 261 | Train Loss: 0.2835746 Vali Loss: 0.9637111 Test Loss: 0.4286599
Validation loss decreased (0.965441 --> 0.963711).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2709224
	speed: 0.0801s/iter; left time: 1811.5562s
	iters: 200, epoch: 14 | loss: 0.2955576
	speed: 0.0165s/iter; left time: 370.6899s
Epoch: 14 cost time: 4.951013803482056
Epoch: 14, Steps: 261 | Train Loss: 0.2834824 Vali Loss: 0.9647627 Test Loss: 0.4285844
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2911531
	speed: 0.0806s/iter; left time: 1800.4687s
	iters: 200, epoch: 15 | loss: 0.2528931
	speed: 0.0156s/iter; left time: 347.9355s
Epoch: 15 cost time: 4.634588956832886
Epoch: 15, Steps: 261 | Train Loss: 0.2833683 Vali Loss: 0.9653271 Test Loss: 0.4284383
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2770413
	speed: 0.0783s/iter; left time: 1728.9356s
	iters: 200, epoch: 16 | loss: 0.2861184
	speed: 0.0165s/iter; left time: 362.9339s
Epoch: 16 cost time: 4.780593633651733
Epoch: 16, Steps: 261 | Train Loss: 0.2832319 Vali Loss: 0.9635144 Test Loss: 0.4285966
Validation loss decreased (0.963711 --> 0.963514).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2730759
	speed: 0.0839s/iter; left time: 1830.6939s
	iters: 200, epoch: 17 | loss: 0.2734343
	speed: 0.0163s/iter; left time: 354.5420s
Epoch: 17 cost time: 4.873495817184448
Epoch: 17, Steps: 261 | Train Loss: 0.2831250 Vali Loss: 0.9647090 Test Loss: 0.4284359
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3031172
	speed: 0.0783s/iter; left time: 1687.5793s
	iters: 200, epoch: 18 | loss: 0.2613398
	speed: 0.0159s/iter; left time: 341.2282s
Epoch: 18 cost time: 4.752678155899048
Epoch: 18, Steps: 261 | Train Loss: 0.2831814 Vali Loss: 0.9633201 Test Loss: 0.4286442
Validation loss decreased (0.963514 --> 0.963320).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3089903
	speed: 0.0970s/iter; left time: 2066.0814s
	iters: 200, epoch: 19 | loss: 0.3038526
	speed: 0.0367s/iter; left time: 779.1607s
Epoch: 19 cost time: 8.718384981155396
Epoch: 19, Steps: 261 | Train Loss: 0.2830906 Vali Loss: 0.9639030 Test Loss: 0.4284558
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2707180
	speed: 0.0778s/iter; left time: 1637.6450s
	iters: 200, epoch: 20 | loss: 0.2770740
	speed: 0.0159s/iter; left time: 333.5175s
Epoch: 20 cost time: 4.748561859130859
Epoch: 20, Steps: 261 | Train Loss: 0.2830567 Vali Loss: 0.9646437 Test Loss: 0.4286010
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2930003
	speed: 0.0774s/iter; left time: 1607.9301s
	iters: 200, epoch: 21 | loss: 0.2985178
	speed: 0.0162s/iter; left time: 334.4323s
Epoch: 21 cost time: 4.809878349304199
Epoch: 21, Steps: 261 | Train Loss: 0.2830186 Vali Loss: 0.9639197 Test Loss: 0.4284283
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2793021
	speed: 0.0770s/iter; left time: 1579.7651s
	iters: 200, epoch: 22 | loss: 0.2754917
	speed: 0.0157s/iter; left time: 321.1286s
Epoch: 22 cost time: 5.564936637878418
Epoch: 22, Steps: 261 | Train Loss: 0.2829939 Vali Loss: 0.9644355 Test Loss: 0.4285508
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3088876
	speed: 0.0855s/iter; left time: 1731.3163s
	iters: 200, epoch: 23 | loss: 0.2715563
	speed: 0.0163s/iter; left time: 328.2162s
Epoch: 23 cost time: 5.017476797103882
Epoch: 23, Steps: 261 | Train Loss: 0.2829581 Vali Loss: 0.9637670 Test Loss: 0.4284608
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2977707
	speed: 0.0796s/iter; left time: 1591.4524s
	iters: 200, epoch: 24 | loss: 0.3012412
	speed: 0.0165s/iter; left time: 327.5473s
Epoch: 24 cost time: 4.862445116043091
Epoch: 24, Steps: 261 | Train Loss: 0.2830784 Vali Loss: 0.9649699 Test Loss: 0.4285098
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2589644
	speed: 0.0773s/iter; left time: 1525.4997s
	iters: 200, epoch: 25 | loss: 0.2590396
	speed: 0.0158s/iter; left time: 309.9592s
Epoch: 25 cost time: 4.733252763748169
Epoch: 25, Steps: 261 | Train Loss: 0.2830180 Vali Loss: 0.9645745 Test Loss: 0.4286780
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3002405
	speed: 0.1019s/iter; left time: 1985.3464s
	iters: 200, epoch: 26 | loss: 0.2919391
	speed: 0.0443s/iter; left time: 859.0424s
Epoch: 26 cost time: 11.79341173171997
Epoch: 26, Steps: 261 | Train Loss: 0.2829696 Vali Loss: 0.9633710 Test Loss: 0.4286764
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2813832
	speed: 0.0979s/iter; left time: 1881.7006s
	iters: 200, epoch: 27 | loss: 0.2669678
	speed: 0.0165s/iter; left time: 314.5404s
Epoch: 27 cost time: 4.846287250518799
Epoch: 27, Steps: 261 | Train Loss: 0.2829702 Vali Loss: 0.9640594 Test Loss: 0.4286664
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.3118056
	speed: 0.0770s/iter; left time: 1459.4220s
	iters: 200, epoch: 28 | loss: 0.2747220
	speed: 0.0155s/iter; left time: 292.7022s
Epoch: 28 cost time: 4.698308229446411
Epoch: 28, Steps: 261 | Train Loss: 0.2830104 Vali Loss: 0.9643186 Test Loss: 0.4284941
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3030519
	speed: 0.0763s/iter; left time: 1426.8980s
	iters: 200, epoch: 29 | loss: 0.2775344
	speed: 0.0161s/iter; left time: 299.5560s
Epoch: 29 cost time: 4.596741437911987
Epoch: 29, Steps: 261 | Train Loss: 0.2829868 Vali Loss: 0.9642070 Test Loss: 0.4286070
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2852789
	speed: 0.0805s/iter; left time: 1483.9175s
	iters: 200, epoch: 30 | loss: 0.2456774
	speed: 0.0168s/iter; left time: 307.1686s
Epoch: 30 cost time: 5.000091075897217
Epoch: 30, Steps: 261 | Train Loss: 0.2830055 Vali Loss: 0.9646596 Test Loss: 0.4286557
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2717969
	speed: 0.0803s/iter; left time: 1459.4556s
	iters: 200, epoch: 31 | loss: 0.3010237
	speed: 0.0365s/iter; left time: 659.9642s
Epoch: 31 cost time: 7.873492479324341
Epoch: 31, Steps: 261 | Train Loss: 0.2828622 Vali Loss: 0.9643362 Test Loss: 0.4285965
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2827012
	speed: 0.0886s/iter; left time: 1586.2833s
	iters: 200, epoch: 32 | loss: 0.2806875
	speed: 0.0164s/iter; left time: 292.5334s
Epoch: 32 cost time: 4.786154270172119
Epoch: 32, Steps: 261 | Train Loss: 0.2829603 Vali Loss: 0.9639470 Test Loss: 0.4286957
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2848061
	speed: 0.0781s/iter; left time: 1378.0691s
	iters: 200, epoch: 33 | loss: 0.2714782
	speed: 0.0160s/iter; left time: 280.5178s
Epoch: 33 cost time: 4.7943949699401855
Epoch: 33, Steps: 261 | Train Loss: 0.2829697 Vali Loss: 0.9638133 Test Loss: 0.4285122
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.3042292
	speed: 0.0781s/iter; left time: 1358.2773s
	iters: 200, epoch: 34 | loss: 0.3132904
	speed: 0.0161s/iter; left time: 278.2915s
Epoch: 34 cost time: 4.872508525848389
Epoch: 34, Steps: 261 | Train Loss: 0.2830134 Vali Loss: 0.9645001 Test Loss: 0.4285690
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2652716
	speed: 0.0771s/iter; left time: 1319.9198s
	iters: 200, epoch: 35 | loss: 0.2796451
	speed: 0.0182s/iter; left time: 309.2663s
Epoch: 35 cost time: 5.036891222000122
Epoch: 35, Steps: 261 | Train Loss: 0.2829122 Vali Loss: 0.9635970 Test Loss: 0.4285921
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2601046
	speed: 0.0827s/iter; left time: 1394.7361s
	iters: 200, epoch: 36 | loss: 0.2970511
	speed: 0.0161s/iter; left time: 269.3639s
Epoch: 36 cost time: 5.073694944381714
Epoch: 36, Steps: 261 | Train Loss: 0.2829223 Vali Loss: 0.9639083 Test Loss: 0.4287362
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2619926
	speed: 0.0783s/iter; left time: 1300.4495s
	iters: 200, epoch: 37 | loss: 0.2710401
	speed: 0.0160s/iter; left time: 263.8074s
Epoch: 37 cost time: 4.786487340927124
Epoch: 37, Steps: 261 | Train Loss: 0.2829948 Vali Loss: 0.9637195 Test Loss: 0.4285475
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2921491
	speed: 0.0775s/iter; left time: 1266.8062s
	iters: 200, epoch: 38 | loss: 0.2866493
	speed: 0.0165s/iter; left time: 268.2084s
Epoch: 38 cost time: 4.865111351013184
Epoch: 38, Steps: 261 | Train Loss: 0.2828932 Vali Loss: 0.9643863 Test Loss: 0.4286131
EarlyStopping counter: 20 out of 20
Early stopping
train 33481
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=50, out_features=150, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6720000.0
params:  7650.0
Trainable parameters:  7650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.3894787
	speed: 0.0226s/iter; left time: 586.8965s
	iters: 200, epoch: 1 | loss: 0.4083236
	speed: 0.0161s/iter; left time: 416.2803s
Epoch: 1 cost time: 4.841657400131226
Epoch: 1, Steps: 261 | Train Loss: 0.4145067 Vali Loss: 0.9630796 Test Loss: 0.4273580
Validation loss decreased (inf --> 0.963080).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.3963155
	speed: 0.0763s/iter; left time: 1965.0036s
	iters: 200, epoch: 2 | loss: 0.3805285
	speed: 0.0162s/iter; left time: 416.0606s
Epoch: 2 cost time: 4.711828708648682
Epoch: 2, Steps: 261 | Train Loss: 0.4143075 Vali Loss: 0.9617006 Test Loss: 0.4278155
Validation loss decreased (0.963080 --> 0.961701).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4013312
	speed: 0.0757s/iter; left time: 1927.9745s
	iters: 200, epoch: 3 | loss: 0.4075917
	speed: 0.0155s/iter; left time: 394.5214s
Epoch: 3 cost time: 4.68904709815979
Epoch: 3, Steps: 261 | Train Loss: 0.4139346 Vali Loss: 0.9612164 Test Loss: 0.4274411
Validation loss decreased (0.961701 --> 0.961216).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4073575
	speed: 0.0812s/iter; left time: 2048.6078s
	iters: 200, epoch: 4 | loss: 0.4078213
	speed: 0.0171s/iter; left time: 429.0170s
Epoch: 4 cost time: 5.696920394897461
Epoch: 4, Steps: 261 | Train Loss: 0.4138694 Vali Loss: 0.9612830 Test Loss: 0.4270427
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4447221
	speed: 0.0841s/iter; left time: 2097.9010s
	iters: 200, epoch: 5 | loss: 0.4254549
	speed: 0.0163s/iter; left time: 406.0480s
Epoch: 5 cost time: 5.000055551528931
Epoch: 5, Steps: 261 | Train Loss: 0.4137276 Vali Loss: 0.9610118 Test Loss: 0.4267306
Validation loss decreased (0.961216 --> 0.961012).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4382098
	speed: 0.0791s/iter; left time: 1954.5759s
	iters: 200, epoch: 6 | loss: 0.4079385
	speed: 0.0165s/iter; left time: 406.4160s
Epoch: 6 cost time: 4.922479629516602
Epoch: 6, Steps: 261 | Train Loss: 0.4137552 Vali Loss: 0.9604464 Test Loss: 0.4272668
Validation loss decreased (0.961012 --> 0.960446).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4433960
	speed: 0.0888s/iter; left time: 2170.7197s
	iters: 200, epoch: 7 | loss: 0.3965084
	speed: 0.0162s/iter; left time: 394.1477s
Epoch: 7 cost time: 4.771556377410889
Epoch: 7, Steps: 261 | Train Loss: 0.4136994 Vali Loss: 0.9611766 Test Loss: 0.4271184
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4145021
	speed: 0.0810s/iter; left time: 1957.0935s
	iters: 200, epoch: 8 | loss: 0.4172660
	speed: 0.0164s/iter; left time: 394.9932s
Epoch: 8 cost time: 4.910563707351685
Epoch: 8, Steps: 261 | Train Loss: 0.4136886 Vali Loss: 0.9599174 Test Loss: 0.4274402
Validation loss decreased (0.960446 --> 0.959917).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4153157
	speed: 0.0771s/iter; left time: 1844.5900s
	iters: 200, epoch: 9 | loss: 0.3986376
	speed: 0.0162s/iter; left time: 385.8162s
Epoch: 9 cost time: 4.758352279663086
Epoch: 9, Steps: 261 | Train Loss: 0.4136542 Vali Loss: 0.9593242 Test Loss: 0.4271765
Validation loss decreased (0.959917 --> 0.959324).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3997788
	speed: 0.0784s/iter; left time: 1854.5137s
	iters: 200, epoch: 10 | loss: 0.4235869
	speed: 0.0169s/iter; left time: 398.0796s
Epoch: 10 cost time: 4.894206762313843
Epoch: 10, Steps: 261 | Train Loss: 0.4137022 Vali Loss: 0.9597022 Test Loss: 0.4271725
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4294623
	speed: 0.0797s/iter; left time: 1864.9956s
	iters: 200, epoch: 11 | loss: 0.4271774
	speed: 0.0167s/iter; left time: 388.7084s
Epoch: 11 cost time: 4.9633097648620605
Epoch: 11, Steps: 261 | Train Loss: 0.4136691 Vali Loss: 0.9602510 Test Loss: 0.4276924
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4047019
	speed: 0.0871s/iter; left time: 2014.4635s
	iters: 200, epoch: 12 | loss: 0.4082226
	speed: 0.0201s/iter; left time: 463.5207s
Epoch: 12 cost time: 5.648805379867554
Epoch: 12, Steps: 261 | Train Loss: 0.4135553 Vali Loss: 0.9601997 Test Loss: 0.4275081
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4001270
	speed: 0.1130s/iter; left time: 2585.0190s
	iters: 200, epoch: 13 | loss: 0.3909542
	speed: 0.0157s/iter; left time: 358.5931s
Epoch: 13 cost time: 5.3672473430633545
Epoch: 13, Steps: 261 | Train Loss: 0.4136565 Vali Loss: 0.9601191 Test Loss: 0.4273852
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4032982
	speed: 0.0853s/iter; left time: 1929.1074s
	iters: 200, epoch: 14 | loss: 0.4285243
	speed: 0.0166s/iter; left time: 373.6050s
Epoch: 14 cost time: 4.930791139602661
Epoch: 14, Steps: 261 | Train Loss: 0.4135589 Vali Loss: 0.9609278 Test Loss: 0.4273162
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4206066
	speed: 0.0895s/iter; left time: 1999.5518s
	iters: 200, epoch: 15 | loss: 0.4160640
	speed: 0.0172s/iter; left time: 381.9899s
Epoch: 15 cost time: 5.988833665847778
Epoch: 15, Steps: 261 | Train Loss: 0.4135062 Vali Loss: 0.9605330 Test Loss: 0.4274911
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.3983141
	speed: 0.0802s/iter; left time: 1771.4661s
	iters: 200, epoch: 16 | loss: 0.4423732
	speed: 0.0167s/iter; left time: 366.5456s
Epoch: 16 cost time: 5.099130630493164
Epoch: 16, Steps: 261 | Train Loss: 0.4135176 Vali Loss: 0.9588801 Test Loss: 0.4277143
Validation loss decreased (0.959324 --> 0.958880).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4191094
	speed: 0.0878s/iter; left time: 1916.2916s
	iters: 200, epoch: 17 | loss: 0.3943889
	speed: 0.0161s/iter; left time: 348.7945s
Epoch: 17 cost time: 5.400983810424805
Epoch: 17, Steps: 261 | Train Loss: 0.4136049 Vali Loss: 0.9602875 Test Loss: 0.4275690
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4133315
	speed: 0.0759s/iter; left time: 1637.2563s
	iters: 200, epoch: 18 | loss: 0.3758539
	speed: 0.0159s/iter; left time: 341.4562s
Epoch: 18 cost time: 4.734721899032593
Epoch: 18, Steps: 261 | Train Loss: 0.4133057 Vali Loss: 0.9602290 Test Loss: 0.4274303
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4327145
	speed: 0.0799s/iter; left time: 1702.7917s
	iters: 200, epoch: 19 | loss: 0.4094079
	speed: 0.0168s/iter; left time: 356.3997s
Epoch: 19 cost time: 5.520659446716309
Epoch: 19, Steps: 261 | Train Loss: 0.4134430 Vali Loss: 0.9595863 Test Loss: 0.4273579
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4279030
	speed: 0.0838s/iter; left time: 1762.3361s
	iters: 200, epoch: 20 | loss: 0.3989182
	speed: 0.0161s/iter; left time: 337.0390s
Epoch: 20 cost time: 4.746015787124634
Epoch: 20, Steps: 261 | Train Loss: 0.4134781 Vali Loss: 0.9606388 Test Loss: 0.4272090
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4085717
	speed: 0.0786s/iter; left time: 1634.2710s
	iters: 200, epoch: 21 | loss: 0.4227849
	speed: 0.0161s/iter; left time: 332.5280s
Epoch: 21 cost time: 4.876993417739868
Epoch: 21, Steps: 261 | Train Loss: 0.4133938 Vali Loss: 0.9595526 Test Loss: 0.4274096
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4026013
	speed: 0.0792s/iter; left time: 1625.6770s
	iters: 200, epoch: 22 | loss: 0.3870826
	speed: 0.0159s/iter; left time: 324.9858s
Epoch: 22 cost time: 5.008135557174683
Epoch: 22, Steps: 261 | Train Loss: 0.4134458 Vali Loss: 0.9603724 Test Loss: 0.4271537
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4253226
	speed: 0.0778s/iter; left time: 1577.1614s
	iters: 200, epoch: 23 | loss: 0.3875222
	speed: 0.0156s/iter; left time: 315.1717s
Epoch: 23 cost time: 4.655242919921875
Epoch: 23, Steps: 261 | Train Loss: 0.4135405 Vali Loss: 0.9597468 Test Loss: 0.4273124
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4007045
	speed: 0.0770s/iter; left time: 1540.3608s
	iters: 200, epoch: 24 | loss: 0.4327408
	speed: 0.0171s/iter; left time: 339.7224s
Epoch: 24 cost time: 5.02598237991333
Epoch: 24, Steps: 261 | Train Loss: 0.4135433 Vali Loss: 0.9599015 Test Loss: 0.4270984
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4325476
	speed: 0.0775s/iter; left time: 1529.6157s
	iters: 200, epoch: 25 | loss: 0.4071104
	speed: 0.0164s/iter; left time: 322.5606s
Epoch: 25 cost time: 4.819266319274902
Epoch: 25, Steps: 261 | Train Loss: 0.4133888 Vali Loss: 0.9600800 Test Loss: 0.4275749
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3864445
	speed: 0.0774s/iter; left time: 1507.2836s
	iters: 200, epoch: 26 | loss: 0.4289651
	speed: 0.0163s/iter; left time: 314.9909s
Epoch: 26 cost time: 4.821059226989746
Epoch: 26, Steps: 261 | Train Loss: 0.4134282 Vali Loss: 0.9601630 Test Loss: 0.4275589
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4002833
	speed: 0.0798s/iter; left time: 1532.8227s
	iters: 200, epoch: 27 | loss: 0.4035963
	speed: 0.0169s/iter; left time: 323.7914s
Epoch: 27 cost time: 4.995557069778442
Epoch: 27, Steps: 261 | Train Loss: 0.4135049 Vali Loss: 0.9602556 Test Loss: 0.4273315
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4081923
	speed: 0.0794s/iter; left time: 1504.6742s
	iters: 200, epoch: 28 | loss: 0.3669802
	speed: 0.0167s/iter; left time: 314.5342s
Epoch: 28 cost time: 4.968954563140869
Epoch: 28, Steps: 261 | Train Loss: 0.4134440 Vali Loss: 0.9597297 Test Loss: 0.4274381
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4120561
	speed: 0.0802s/iter; left time: 1499.7583s
	iters: 200, epoch: 29 | loss: 0.4006870
	speed: 0.0176s/iter; left time: 327.7192s
Epoch: 29 cost time: 5.052227735519409
Epoch: 29, Steps: 261 | Train Loss: 0.4134314 Vali Loss: 0.9600577 Test Loss: 0.4273414
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4076290
	speed: 0.0809s/iter; left time: 1491.7546s
	iters: 200, epoch: 30 | loss: 0.4115254
	speed: 0.0219s/iter; left time: 401.8107s
Epoch: 30 cost time: 5.547741174697876
Epoch: 30, Steps: 261 | Train Loss: 0.4134090 Vali Loss: 0.9605079 Test Loss: 0.4274830
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.3963034
	speed: 0.0826s/iter; left time: 1501.4459s
	iters: 200, epoch: 31 | loss: 0.3915838
	speed: 0.0173s/iter; left time: 312.2000s
Epoch: 31 cost time: 5.101381778717041
Epoch: 31, Steps: 261 | Train Loss: 0.4134426 Vali Loss: 0.9598540 Test Loss: 0.4273940
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4125686
	speed: 0.0803s/iter; left time: 1437.8035s
	iters: 200, epoch: 32 | loss: 0.3779741
	speed: 0.0168s/iter; left time: 299.5568s
Epoch: 32 cost time: 4.959689617156982
Epoch: 32, Steps: 261 | Train Loss: 0.4133274 Vali Loss: 0.9591246 Test Loss: 0.4273282
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4591377
	speed: 0.0772s/iter; left time: 1362.4627s
	iters: 200, epoch: 33 | loss: 0.4278833
	speed: 0.0159s/iter; left time: 279.7862s
Epoch: 33 cost time: 4.791156530380249
Epoch: 33, Steps: 261 | Train Loss: 0.4132858 Vali Loss: 0.9601628 Test Loss: 0.4274493
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4397541
	speed: 0.0788s/iter; left time: 1370.3474s
	iters: 200, epoch: 34 | loss: 0.4568866
	speed: 0.0165s/iter; left time: 285.7786s
Epoch: 34 cost time: 4.835005521774292
Epoch: 34, Steps: 261 | Train Loss: 0.4133978 Vali Loss: 0.9597549 Test Loss: 0.4274202
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3659584
	speed: 0.0825s/iter; left time: 1413.1995s
	iters: 200, epoch: 35 | loss: 0.4361488
	speed: 0.0166s/iter; left time: 282.3016s
Epoch: 35 cost time: 4.8165905475616455
Epoch: 35, Steps: 261 | Train Loss: 0.4134335 Vali Loss: 0.9595351 Test Loss: 0.4274281
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4262976
	speed: 0.0778s/iter; left time: 1312.9435s
	iters: 200, epoch: 36 | loss: 0.4094742
	speed: 0.0168s/iter; left time: 281.9832s
Epoch: 36 cost time: 4.935861825942993
Epoch: 36, Steps: 261 | Train Loss: 0.4133967 Vali Loss: 0.9594812 Test Loss: 0.4274505
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_360_720_FITS_ETTm1_ftM_sl360_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.42737671732902527, mae:0.41605865955352783, rse:0.6219797134399414, corr:[0.52527577 0.53297806 0.53472596 0.5332804  0.5319873  0.5321683
 0.5333581  0.53437614 0.53445715 0.5339741  0.5338291  0.5341601
 0.5347702  0.53509873 0.5345987  0.5332056  0.5315382  0.53016675
 0.5290147  0.5279264  0.5266409  0.5253097  0.52388966 0.522611
 0.5214877  0.5203958  0.51943153 0.51869863 0.5181866  0.5177782
 0.517785   0.518337   0.5191129  0.51974416 0.51995426 0.51987934
 0.5195849  0.51916015 0.51880103 0.5185812  0.5184992  0.5183963
 0.5181832  0.51797694 0.5178458  0.5178684  0.51806325 0.5181649
 0.5179686  0.5175965  0.5171698  0.5168473  0.5167966  0.5168934
 0.51697385 0.5169391  0.51670635 0.5163909  0.5161748  0.51601475
 0.51602817 0.51605314 0.5160297  0.5158462  0.5156834  0.5156957
 0.5158512  0.5160573  0.5164232  0.5167788  0.5170497  0.51717556
 0.51725286 0.51734704 0.5174311  0.5174531  0.5174068  0.5172973
 0.5170996  0.51681453 0.5165102  0.5162456  0.5159909  0.5156592
 0.51535237 0.5149853  0.5146601  0.5143585  0.51411337 0.51408
 0.5142588  0.5144744  0.5145742  0.5144916  0.5141784  0.5136459
 0.5130384  0.51248527 0.5120153  0.511618   0.5113908  0.511352
 0.51142853 0.5114343  0.51150316 0.5116937  0.5119046  0.5122837
 0.5126558  0.51307184 0.51333565 0.51337826 0.51316094 0.5129232
 0.5127475  0.5127186  0.5127695  0.51283807 0.5128317  0.51289696
 0.5129721  0.5129168  0.5127427  0.51261127 0.512464   0.5121976
 0.51184946 0.51153237 0.511312   0.5112415  0.5112738  0.51127505
 0.51119316 0.5109092  0.51052976 0.5101325  0.50984925 0.50966704
 0.509628   0.5096243  0.50952727 0.50938165 0.50920475 0.5090922
 0.50908077 0.50917697 0.5092773  0.5093462  0.5094068  0.50934654
 0.5092392  0.5092514  0.5092971  0.50930524 0.5092547  0.5092016
 0.50912535 0.5090631  0.50899416 0.5089825  0.50907713 0.50921077
 0.5093686  0.50952786 0.5097852  0.5101199  0.51044106 0.51076883
 0.5110374  0.5112478  0.5114585  0.51171404 0.511904   0.51202077
 0.51203483 0.5119923  0.5118727  0.5117228  0.51149315 0.5112799
 0.51109076 0.51104105 0.5110045  0.5109976  0.510967   0.510905
 0.5107987  0.5107031  0.5106161  0.5105219  0.5103312  0.5100655
 0.50978196 0.5096256  0.50954044 0.50933594 0.5089912  0.50864136
 0.5081999  0.50763935 0.5071231  0.50672066 0.50640374 0.50619555
 0.5060555  0.50578266 0.50543797 0.504979   0.5045221  0.5041715
 0.5038291  0.5035367  0.5031977  0.5028501  0.502416   0.5020034
 0.5016204  0.50123715 0.5008172  0.5005464  0.5003616  0.5002388
 0.5002524  0.5003031  0.50033814 0.50032645 0.5003314  0.5002798
 0.50022244 0.50019366 0.5001349  0.5000069  0.49986383 0.499675
 0.49953124 0.49943155 0.49926656 0.4992105  0.49922216 0.49924338
 0.49918467 0.49901232 0.49880713 0.49860376 0.49843466 0.49840036
 0.49844867 0.49850643 0.49853426 0.4985306  0.498401   0.49825186
 0.49804655 0.49794587 0.49792045 0.4979691  0.49796978 0.49803272
 0.4980808  0.49816746 0.498324   0.49852526 0.49865457 0.49885246
 0.49908042 0.49930763 0.49945444 0.49963024 0.4997729  0.4997733
 0.4997578  0.4997251  0.49964327 0.49952275 0.49931175 0.49914256
 0.49899536 0.49899608 0.4989864  0.4990316  0.4990131  0.49891996
 0.49883166 0.49866924 0.4984488  0.49814007 0.497742   0.49728402
 0.496751   0.49630028 0.49597004 0.4954965  0.4949765  0.4944576
 0.49400046 0.4935283  0.4932021  0.49293926 0.4927151  0.4924105
 0.4920102  0.4915639  0.49116504 0.49088934 0.4907209  0.49064746
 0.49067408 0.49068207 0.49068567 0.49071628 0.49079657 0.49087733
 0.49097666 0.4910021  0.4908629  0.4907538  0.4906507  0.4905792
 0.4905341  0.49049097 0.49040768 0.49029422 0.4901491  0.48996007
 0.48974383 0.48949856 0.4891999  0.48892394 0.48863065 0.4883829
 0.48816457 0.48800078 0.48784387 0.48768118 0.48751062 0.48732966
 0.4871076  0.48689875 0.4867806  0.4866925  0.48675132 0.48694095
 0.4871085  0.48721838 0.48729256 0.48729327 0.48733243 0.48745203
 0.48754665 0.48763895 0.48767787 0.48759487 0.4874667  0.48738897
 0.48736408 0.4874198  0.48758566 0.48777285 0.48791546 0.48802847
 0.488091   0.48816383 0.48827076 0.48841122 0.48854646 0.48862585
 0.4886048  0.48855445 0.48852724 0.48846027 0.4883977  0.48833403
 0.48828498 0.48821932 0.48818806 0.48823023 0.48833656 0.4885175
 0.48875648 0.48899058 0.48916626 0.48918334 0.4890397  0.48878145
 0.48839617 0.48799965 0.48763454 0.48718837 0.48666695 0.48625255
 0.48593134 0.48561007 0.4853637  0.4852545  0.48517242 0.485114
 0.48496604 0.48482096 0.48464423 0.48443094 0.48417515 0.48395744
 0.4838828  0.4839103  0.4839432  0.48391375 0.4838263  0.48375183
 0.48369715 0.4835335  0.4833572  0.48332274 0.48339087 0.48346874
 0.48350978 0.48342276 0.48325664 0.48298725 0.48270512 0.4824902
 0.4823459  0.4822977  0.4821947  0.4820376  0.48178613 0.48148334
 0.48117214 0.4810312  0.4809858  0.48104227 0.48110545 0.4812031
 0.4810926  0.4808102  0.48055515 0.4804008  0.48031467 0.48031107
 0.48028624 0.48023278 0.4801632  0.48009357 0.48009425 0.48021263
 0.48031196 0.48040548 0.48041782 0.48035103 0.48025608 0.48018804
 0.4801967  0.48031577 0.4804917  0.4806768  0.48083243 0.4809806
 0.48098764 0.48094907 0.4808722  0.4808712  0.4809161  0.4809605
 0.4810139  0.481132   0.4812317  0.48137745 0.48151866 0.48163074
 0.48171481 0.4817463  0.48172832 0.4816949  0.48166788 0.48167548
 0.48165438 0.48160833 0.48146877 0.4811385  0.4805736  0.4798023
 0.47885895 0.47792384 0.47705132 0.47624207 0.4755694  0.47507533
 0.47472018 0.4743188  0.47383642 0.47329748 0.47263217 0.47188848
 0.47124958 0.4708333  0.47057468 0.47038013 0.4702969  0.47017917
 0.47002602 0.4699727  0.47004434 0.4701391  0.47031802 0.47059694
 0.47094387 0.47112316 0.471146   0.47109973 0.47103614 0.47104678
 0.47111475 0.47120264 0.47128853 0.47139484 0.47141966 0.471416
 0.4713455  0.47120884 0.47107822 0.47084188 0.47055    0.47031313
 0.47014463 0.47004694 0.47001597 0.4699998  0.4699631  0.46992102
 0.4697709  0.46948686 0.4691765  0.46894634 0.46886212 0.46893016
 0.46907917 0.46918482 0.46924925 0.46927223 0.4692803  0.46930918
 0.46937588 0.46940085 0.46934175 0.4692026  0.46904787 0.46889225
 0.4687048  0.4685242  0.46844745 0.4684698  0.46855927 0.46879604
 0.46903506 0.46929964 0.46953472 0.46975613 0.46995208 0.47004184
 0.47005162 0.4700363  0.46999717 0.4699811  0.46999535 0.47003135
 0.4700101  0.47003466 0.4700063  0.46998504 0.46999595 0.47003508
 0.47011602 0.47011322 0.46998984 0.46964344 0.46905246 0.46827278
 0.4674389  0.46668002 0.46609125 0.46542853 0.46470335 0.46399212
 0.4633246  0.4626563  0.46213222 0.4617928  0.46150595 0.46118328
 0.4608449  0.4604772  0.46014467 0.45987415 0.45963138 0.45951107
 0.45937613 0.45934153 0.4593429  0.45937872 0.4594448  0.45962378
 0.45987338 0.45994768 0.45984492 0.45972687 0.45963523 0.45954704
 0.4594758  0.45943594 0.4593719  0.4592555  0.4590989  0.4589794
 0.45887133 0.4587908  0.4587452  0.458648   0.45855516 0.45843813
 0.4583339  0.45824024 0.4581662  0.45801672 0.45789295 0.45781553
 0.4577262  0.45768008 0.4576824  0.45769247 0.4577659  0.45788127
 0.4579403  0.45792013 0.45788416 0.4578298  0.45781004 0.45776713
 0.45766383 0.45751777 0.45734772 0.45715335 0.45704827 0.45696497
 0.45691395 0.45693114 0.45698985 0.4571091  0.45721653 0.45729178
 0.45737436 0.4574591  0.4575822  0.4577791  0.45797953 0.45815146
 0.45825854 0.45830944 0.45830476 0.45829412 0.45826316 0.45822334
 0.4581783  0.45816413 0.4582158  0.45827717 0.45835564 0.4583979
 0.4583783  0.4582937  0.4581073  0.45782873 0.45746547 0.45700142
 0.45641103 0.4558719  0.45538932 0.45481262 0.45423228 0.4537171
 0.45330134 0.45301348 0.4528617  0.4527607  0.45267177 0.45259574
 0.45249492 0.45237812 0.45224345 0.45199797 0.45167655 0.4512156
 0.45086515 0.45066425 0.45060566 0.45067492 0.45075107 0.45093468
 0.4510444  0.4508323  0.4503729  0.44993082 0.44952038 0.44908392
 0.44870692 0.44848558 0.44843864 0.44851196 0.44848305 0.44827968
 0.44785583 0.44737566 0.44707373 0.447127   0.4475422  0.4479564
 0.44801483 0.44769523 0.4473989  0.44774088 0.44903293 0.4500827 ]
