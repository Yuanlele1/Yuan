Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=16, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=16, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2064384.0
params:  2448.0
Trainable parameters:  2448
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.0499259
	speed: 0.0229s/iter; left time: 600.1667s
	iters: 200, epoch: 1 | loss: 0.7547043
	speed: 0.0163s/iter; left time: 426.3087s
Epoch: 1 cost time: 4.965019941329956
Epoch: 1, Steps: 263 | Train Loss: 0.9956012 Vali Loss: 1.4042560 Test Loss: 0.8716388
Validation loss decreased (inf --> 1.404256).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6059533
	speed: 0.0751s/iter; left time: 1947.2922s
	iters: 200, epoch: 2 | loss: 0.4825083
	speed: 0.0158s/iter; left time: 408.9130s
Epoch: 2 cost time: 4.630936622619629
Epoch: 2, Steps: 263 | Train Loss: 0.5948271 Vali Loss: 1.1193297 Test Loss: 0.5909400
Validation loss decreased (1.404256 --> 1.119330).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4986655
	speed: 0.0730s/iter; left time: 1874.2328s
	iters: 200, epoch: 3 | loss: 0.4986950
	speed: 0.0159s/iter; left time: 407.0303s
Epoch: 3 cost time: 4.67024827003479
Epoch: 3, Steps: 263 | Train Loss: 0.5201082 Vali Loss: 1.0558214 Test Loss: 0.5292087
Validation loss decreased (1.119330 --> 1.055821).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4857827
	speed: 0.0750s/iter; left time: 1906.0112s
	iters: 200, epoch: 4 | loss: 0.5312636
	speed: 0.0153s/iter; left time: 387.4146s
Epoch: 4 cost time: 4.654894113540649
Epoch: 4, Steps: 263 | Train Loss: 0.5029158 Vali Loss: 1.0328444 Test Loss: 0.5085018
Validation loss decreased (1.055821 --> 1.032844).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4480601
	speed: 0.0750s/iter; left time: 1887.2263s
	iters: 200, epoch: 5 | loss: 0.5045907
	speed: 0.0266s/iter; left time: 666.6325s
Epoch: 5 cost time: 5.833362817764282
Epoch: 5, Steps: 263 | Train Loss: 0.4972826 Vali Loss: 1.0225354 Test Loss: 0.4996850
Validation loss decreased (1.032844 --> 1.022535).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5109619
	speed: 0.0773s/iter; left time: 1924.8982s
	iters: 200, epoch: 6 | loss: 0.4996968
	speed: 0.0155s/iter; left time: 384.7908s
Epoch: 6 cost time: 4.742677927017212
Epoch: 6, Steps: 263 | Train Loss: 0.4952427 Vali Loss: 1.0183587 Test Loss: 0.4963963
Validation loss decreased (1.022535 --> 1.018359).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.5064891
	speed: 0.0732s/iter; left time: 1802.1423s
	iters: 200, epoch: 7 | loss: 0.4632930
	speed: 0.0151s/iter; left time: 369.8053s
Epoch: 7 cost time: 4.642357587814331
Epoch: 7, Steps: 263 | Train Loss: 0.4946583 Vali Loss: 1.0152012 Test Loss: 0.4946986
Validation loss decreased (1.018359 --> 1.015201).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5335736
	speed: 0.0764s/iter; left time: 1860.3072s
	iters: 200, epoch: 8 | loss: 0.5415724
	speed: 0.0154s/iter; left time: 374.2775s
Epoch: 8 cost time: 4.65937614440918
Epoch: 8, Steps: 263 | Train Loss: 0.4943210 Vali Loss: 1.0150175 Test Loss: 0.4941870
Validation loss decreased (1.015201 --> 1.015018).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.5061598
	speed: 0.0734s/iter; left time: 1768.0965s
	iters: 200, epoch: 9 | loss: 0.4942445
	speed: 0.0154s/iter; left time: 368.9738s
Epoch: 9 cost time: 4.491607427597046
Epoch: 9, Steps: 263 | Train Loss: 0.4941963 Vali Loss: 1.0133440 Test Loss: 0.4942257
Validation loss decreased (1.015018 --> 1.013344).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4803708
	speed: 0.0736s/iter; left time: 1755.3706s
	iters: 200, epoch: 10 | loss: 0.4936422
	speed: 0.0157s/iter; left time: 372.1013s
Epoch: 10 cost time: 4.656105041503906
Epoch: 10, Steps: 263 | Train Loss: 0.4941021 Vali Loss: 1.0133625 Test Loss: 0.4940317
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5379229
	speed: 0.0760s/iter; left time: 1792.1772s
	iters: 200, epoch: 11 | loss: 0.5074159
	speed: 0.0168s/iter; left time: 395.2399s
Epoch: 11 cost time: 4.960988521575928
Epoch: 11, Steps: 263 | Train Loss: 0.4940885 Vali Loss: 1.0133524 Test Loss: 0.4939132
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4973438
	speed: 0.0751s/iter; left time: 1749.6776s
	iters: 200, epoch: 12 | loss: 0.4805786
	speed: 0.0155s/iter; left time: 359.0594s
Epoch: 12 cost time: 4.631568431854248
Epoch: 12, Steps: 263 | Train Loss: 0.4938391 Vali Loss: 1.0129126 Test Loss: 0.4941476
Validation loss decreased (1.013344 --> 1.012913).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4823488
	speed: 0.0764s/iter; left time: 1759.7936s
	iters: 200, epoch: 13 | loss: 0.5185322
	speed: 0.0154s/iter; left time: 352.7936s
Epoch: 13 cost time: 4.690732717514038
Epoch: 13, Steps: 263 | Train Loss: 0.4938466 Vali Loss: 1.0128465 Test Loss: 0.4940836
Validation loss decreased (1.012913 --> 1.012846).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4627396
	speed: 0.0727s/iter; left time: 1655.9971s
	iters: 200, epoch: 14 | loss: 0.5112553
	speed: 0.0152s/iter; left time: 345.5264s
Epoch: 14 cost time: 4.572332382202148
Epoch: 14, Steps: 263 | Train Loss: 0.4938759 Vali Loss: 1.0138391 Test Loss: 0.4941134
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4997066
	speed: 0.0781s/iter; left time: 1757.8904s
	iters: 200, epoch: 15 | loss: 0.4982057
	speed: 0.0154s/iter; left time: 346.1131s
Epoch: 15 cost time: 4.803079843521118
Epoch: 15, Steps: 263 | Train Loss: 0.4938664 Vali Loss: 1.0138148 Test Loss: 0.4942551
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5374679
	speed: 0.0785s/iter; left time: 1747.5373s
	iters: 200, epoch: 16 | loss: 0.4848463
	speed: 0.0166s/iter; left time: 368.0230s
Epoch: 16 cost time: 4.9826226234436035
Epoch: 16, Steps: 263 | Train Loss: 0.4938508 Vali Loss: 1.0133607 Test Loss: 0.4943371
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4727959
	speed: 0.0784s/iter; left time: 1724.5515s
	iters: 200, epoch: 17 | loss: 0.4874821
	speed: 0.0161s/iter; left time: 353.5011s
Epoch: 17 cost time: 4.80417013168335
Epoch: 17, Steps: 263 | Train Loss: 0.4938574 Vali Loss: 1.0133283 Test Loss: 0.4942986
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5173458
	speed: 0.0804s/iter; left time: 1747.2710s
	iters: 200, epoch: 18 | loss: 0.4649247
	speed: 0.0167s/iter; left time: 362.1138s
Epoch: 18 cost time: 4.993911266326904
Epoch: 18, Steps: 263 | Train Loss: 0.4938681 Vali Loss: 1.0133111 Test Loss: 0.4943711
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4895160
	speed: 0.0812s/iter; left time: 1742.3289s
	iters: 200, epoch: 19 | loss: 0.5212334
	speed: 0.0167s/iter; left time: 357.1814s
Epoch: 19 cost time: 4.961814880371094
Epoch: 19, Steps: 263 | Train Loss: 0.4938654 Vali Loss: 1.0135431 Test Loss: 0.4942827
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4372703
	speed: 0.0809s/iter; left time: 1715.1333s
	iters: 200, epoch: 20 | loss: 0.5159164
	speed: 0.0166s/iter; left time: 350.9442s
Epoch: 20 cost time: 4.950537204742432
Epoch: 20, Steps: 263 | Train Loss: 0.4937620 Vali Loss: 1.0137331 Test Loss: 0.4943237
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4736835
	speed: 0.0824s/iter; left time: 1725.6259s
	iters: 200, epoch: 21 | loss: 0.5229318
	speed: 0.0169s/iter; left time: 351.4152s
Epoch: 21 cost time: 5.0871734619140625
Epoch: 21, Steps: 263 | Train Loss: 0.4938098 Vali Loss: 1.0134777 Test Loss: 0.4943638
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5036877
	speed: 0.0792s/iter; left time: 1638.6955s
	iters: 200, epoch: 22 | loss: 0.4578066
	speed: 0.0163s/iter; left time: 334.4121s
Epoch: 22 cost time: 4.859794616699219
Epoch: 22, Steps: 263 | Train Loss: 0.4937062 Vali Loss: 1.0136629 Test Loss: 0.4944900
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5005102
	speed: 0.0759s/iter; left time: 1549.0245s
	iters: 200, epoch: 23 | loss: 0.4778355
	speed: 0.0167s/iter; left time: 338.8658s
Epoch: 23 cost time: 4.903746128082275
Epoch: 23, Steps: 263 | Train Loss: 0.4938900 Vali Loss: 1.0136904 Test Loss: 0.4943934
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5630337
	speed: 0.0768s/iter; left time: 1546.6737s
	iters: 200, epoch: 24 | loss: 0.4876815
	speed: 0.0156s/iter; left time: 312.4713s
Epoch: 24 cost time: 4.672434091567993
Epoch: 24, Steps: 263 | Train Loss: 0.4938508 Vali Loss: 1.0138524 Test Loss: 0.4944626
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4990238
	speed: 0.0759s/iter; left time: 1509.2918s
	iters: 200, epoch: 25 | loss: 0.4701098
	speed: 0.0160s/iter; left time: 316.0814s
Epoch: 25 cost time: 4.660876035690308
Epoch: 25, Steps: 263 | Train Loss: 0.4938788 Vali Loss: 1.0134817 Test Loss: 0.4945707
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4474194
	speed: 0.0745s/iter; left time: 1462.9496s
	iters: 200, epoch: 26 | loss: 0.4700327
	speed: 0.0161s/iter; left time: 313.6917s
Epoch: 26 cost time: 4.811031818389893
Epoch: 26, Steps: 263 | Train Loss: 0.4937582 Vali Loss: 1.0143616 Test Loss: 0.4944038
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5232025
	speed: 0.0746s/iter; left time: 1444.7004s
	iters: 200, epoch: 27 | loss: 0.4786783
	speed: 0.0152s/iter; left time: 292.6321s
Epoch: 27 cost time: 4.652123689651489
Epoch: 27, Steps: 263 | Train Loss: 0.4936702 Vali Loss: 1.0143093 Test Loss: 0.4944537
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4565217
	speed: 0.0802s/iter; left time: 1532.0208s
	iters: 200, epoch: 28 | loss: 0.5143753
	speed: 0.0162s/iter; left time: 308.1212s
Epoch: 28 cost time: 4.813363075256348
Epoch: 28, Steps: 263 | Train Loss: 0.4938271 Vali Loss: 1.0144856 Test Loss: 0.4944158
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.5015669
	speed: 0.0740s/iter; left time: 1394.6852s
	iters: 200, epoch: 29 | loss: 0.5055444
	speed: 0.0153s/iter; left time: 286.8571s
Epoch: 29 cost time: 4.509793519973755
Epoch: 29, Steps: 263 | Train Loss: 0.4938237 Vali Loss: 1.0142559 Test Loss: 0.4945107
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.5530978
	speed: 0.0724s/iter; left time: 1344.5710s
	iters: 200, epoch: 30 | loss: 0.4667023
	speed: 0.0156s/iter; left time: 287.5797s
Epoch: 30 cost time: 4.583299875259399
Epoch: 30, Steps: 263 | Train Loss: 0.4936550 Vali Loss: 1.0135262 Test Loss: 0.4944624
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4981465
	speed: 0.0757s/iter; left time: 1386.2995s
	iters: 200, epoch: 31 | loss: 0.4522855
	speed: 0.0155s/iter; left time: 282.1683s
Epoch: 31 cost time: 4.6459105014801025
Epoch: 31, Steps: 263 | Train Loss: 0.4937055 Vali Loss: 1.0127814 Test Loss: 0.4944345
Validation loss decreased (1.012846 --> 1.012781).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.5335782
	speed: 0.0748s/iter; left time: 1349.6814s
	iters: 200, epoch: 32 | loss: 0.4295530
	speed: 0.0152s/iter; left time: 272.6592s
Epoch: 32 cost time: 4.618722438812256
Epoch: 32, Steps: 263 | Train Loss: 0.4937087 Vali Loss: 1.0135185 Test Loss: 0.4945482
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4949693
	speed: 0.0749s/iter; left time: 1332.1549s
	iters: 200, epoch: 33 | loss: 0.4426334
	speed: 0.0168s/iter; left time: 296.8040s
Epoch: 33 cost time: 5.637269020080566
Epoch: 33, Steps: 263 | Train Loss: 0.4938233 Vali Loss: 1.0135244 Test Loss: 0.4945258
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.5021256
	speed: 0.0814s/iter; left time: 1426.2208s
	iters: 200, epoch: 34 | loss: 0.4445936
	speed: 0.0152s/iter; left time: 264.2983s
Epoch: 34 cost time: 4.545253038406372
Epoch: 34, Steps: 263 | Train Loss: 0.4936867 Vali Loss: 1.0135336 Test Loss: 0.4945580
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.5083672
	speed: 0.0729s/iter; left time: 1257.3724s
	iters: 200, epoch: 35 | loss: 0.5427014
	speed: 0.0151s/iter; left time: 259.6691s
Epoch: 35 cost time: 4.656844615936279
Epoch: 35, Steps: 263 | Train Loss: 0.4937659 Vali Loss: 1.0138928 Test Loss: 0.4945651
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.5194329
	speed: 0.0756s/iter; left time: 1284.7769s
	iters: 200, epoch: 36 | loss: 0.5167243
	speed: 0.0154s/iter; left time: 260.1447s
Epoch: 36 cost time: 4.804351568222046
Epoch: 36, Steps: 263 | Train Loss: 0.4937600 Vali Loss: 1.0142858 Test Loss: 0.4945425
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.5672187
	speed: 0.0717s/iter; left time: 1199.6537s
	iters: 200, epoch: 37 | loss: 0.4967162
	speed: 0.0153s/iter; left time: 254.7713s
Epoch: 37 cost time: 4.63100528717041
Epoch: 37, Steps: 263 | Train Loss: 0.4936709 Vali Loss: 1.0130584 Test Loss: 0.4945783
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4890738
	speed: 0.0808s/iter; left time: 1330.1626s
	iters: 200, epoch: 38 | loss: 0.5200463
	speed: 0.0160s/iter; left time: 261.2007s
Epoch: 38 cost time: 4.981865406036377
Epoch: 38, Steps: 263 | Train Loss: 0.4937700 Vali Loss: 1.0133151 Test Loss: 0.4946155
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.5238416
	speed: 0.0768s/iter; left time: 1244.2277s
	iters: 200, epoch: 39 | loss: 0.5063516
	speed: 0.0164s/iter; left time: 263.9707s
Epoch: 39 cost time: 4.714733839035034
Epoch: 39, Steps: 263 | Train Loss: 0.4936701 Vali Loss: 1.0146925 Test Loss: 0.4945674
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4804569
	speed: 0.0746s/iter; left time: 1189.0236s
	iters: 200, epoch: 40 | loss: 0.5415012
	speed: 0.0162s/iter; left time: 256.7307s
Epoch: 40 cost time: 4.7225706577301025
Epoch: 40, Steps: 263 | Train Loss: 0.4936678 Vali Loss: 1.0143714 Test Loss: 0.4946820
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4758197
	speed: 0.0796s/iter; left time: 1247.7529s
	iters: 200, epoch: 41 | loss: 0.5105190
	speed: 0.0168s/iter; left time: 261.5094s
Epoch: 41 cost time: 4.930760145187378
Epoch: 41, Steps: 263 | Train Loss: 0.4937231 Vali Loss: 1.0139036 Test Loss: 0.4945942
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4462054
	speed: 0.0816s/iter; left time: 1258.8269s
	iters: 200, epoch: 42 | loss: 0.4341205
	speed: 0.0163s/iter; left time: 249.7037s
Epoch: 42 cost time: 4.886235952377319
Epoch: 42, Steps: 263 | Train Loss: 0.4935789 Vali Loss: 1.0133492 Test Loss: 0.4946460
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.5228561
	speed: 0.0803s/iter; left time: 1217.0552s
	iters: 200, epoch: 43 | loss: 0.4794361
	speed: 0.0167s/iter; left time: 251.8131s
Epoch: 43 cost time: 4.918126106262207
Epoch: 43, Steps: 263 | Train Loss: 0.4936430 Vali Loss: 1.0140612 Test Loss: 0.4946641
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.5049210
	speed: 0.0814s/iter; left time: 1212.6580s
	iters: 200, epoch: 44 | loss: 0.4840602
	speed: 0.0175s/iter; left time: 258.4781s
Epoch: 44 cost time: 5.030886888504028
Epoch: 44, Steps: 263 | Train Loss: 0.4937727 Vali Loss: 1.0142623 Test Loss: 0.4946626
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5232309
	speed: 0.0777s/iter; left time: 1136.1696s
	iters: 200, epoch: 45 | loss: 0.5283281
	speed: 0.0168s/iter; left time: 244.7812s
Epoch: 45 cost time: 4.953957796096802
Epoch: 45, Steps: 263 | Train Loss: 0.4936656 Vali Loss: 1.0140648 Test Loss: 0.4946325
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4420945
	speed: 0.0744s/iter; left time: 1068.7715s
	iters: 200, epoch: 46 | loss: 0.5587395
	speed: 0.0153s/iter; left time: 217.9697s
Epoch: 46 cost time: 4.650042772293091
Epoch: 46, Steps: 263 | Train Loss: 0.4936261 Vali Loss: 1.0128508 Test Loss: 0.4947123
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.5121764
	speed: 0.0721s/iter; left time: 1016.9009s
	iters: 200, epoch: 47 | loss: 0.4817512
	speed: 0.0157s/iter; left time: 219.8997s
Epoch: 47 cost time: 4.620458126068115
Epoch: 47, Steps: 263 | Train Loss: 0.4936749 Vali Loss: 1.0135559 Test Loss: 0.4947180
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.5176077
	speed: 0.0745s/iter; left time: 1031.5842s
	iters: 200, epoch: 48 | loss: 0.5154506
	speed: 0.0165s/iter; left time: 227.0014s
Epoch: 48 cost time: 4.793498992919922
Epoch: 48, Steps: 263 | Train Loss: 0.4937238 Vali Loss: 1.0131723 Test Loss: 0.4946603
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4702086
	speed: 0.0763s/iter; left time: 1035.6012s
	iters: 200, epoch: 49 | loss: 0.4747986
	speed: 0.0166s/iter; left time: 223.3034s
Epoch: 49 cost time: 4.899247407913208
Epoch: 49, Steps: 263 | Train Loss: 0.4937228 Vali Loss: 1.0137078 Test Loss: 0.4946573
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.5018877
	speed: 0.0778s/iter; left time: 1035.5373s
	iters: 200, epoch: 50 | loss: 0.4885322
	speed: 0.0166s/iter; left time: 219.5984s
Epoch: 50 cost time: 4.868171691894531
Epoch: 50, Steps: 263 | Train Loss: 0.4936352 Vali Loss: 1.0131555 Test Loss: 0.4946894
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4889285
	speed: 0.0796s/iter; left time: 1039.3005s
	iters: 200, epoch: 51 | loss: 0.4693855
	speed: 0.0164s/iter; left time: 212.0250s
Epoch: 51 cost time: 4.7997541427612305
Epoch: 51, Steps: 263 | Train Loss: 0.4936883 Vali Loss: 1.0137684 Test Loss: 0.4946703
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4925523102283478, mae:0.4528559744358063, rse:0.6677238941192627, corr:[0.52889544 0.5316087  0.5282675  0.5239963  0.5212765  0.51979256
 0.5181002  0.5156022  0.5127637  0.5104917  0.5090581  0.5081498
 0.507132   0.5053693  0.5025069  0.4987939  0.49510077 0.49202546
 0.48946163 0.48690367 0.484064   0.48090008 0.47747272 0.47384533
 0.47020903 0.46663967 0.46347484 0.46090022 0.45896158 0.4573238
 0.45615488 0.45604327 0.45644557 0.4568568  0.45718044 0.4575739
 0.45762813 0.4575204  0.45740208 0.45732594 0.45744863 0.45743152
 0.45725337 0.456935   0.456373   0.4558425  0.45565012 0.45579195
 0.45614013 0.45657715 0.457009   0.45723552 0.45746362 0.45773083
 0.45829326 0.45902395 0.45979023 0.4602715  0.46010566 0.45960563
 0.45914462 0.45872363 0.45856938 0.4584677  0.4585248  0.458491
 0.45811898 0.457681   0.45779344 0.45857063 0.45946112 0.46016237
 0.46071556 0.46120325 0.4619881  0.46291262 0.46398646 0.46500126
 0.46569538 0.46615863 0.4666281  0.46711218 0.4676768  0.46799237
 0.46815306 0.46834868 0.46886528 0.46975178 0.47084594 0.47190723
 0.47272137 0.4732886  0.47382385 0.47441173 0.4749162  0.47488075
 0.4740841  0.47286886 0.471694   0.47102684 0.4709746  0.47101173
 0.47058427 0.4695534  0.4685955  0.46811712 0.4678839  0.46747366
 0.46666834 0.46560505 0.46458095 0.46378973 0.46313742 0.46240875
 0.46142498 0.46038547 0.45937812 0.4584305  0.4574183  0.45620164
 0.45487544 0.45346227 0.45214266 0.45121682 0.45049205 0.4496561
 0.44872463 0.44794524 0.44759136 0.44768953 0.4480125  0.44826442
 0.44813454 0.44784302 0.44753945 0.4473651  0.44731864 0.447297
 0.44714105 0.44686183 0.44658265 0.44649056 0.44655132 0.44669425
 0.44678584 0.44689873 0.44712943 0.4473768  0.44779015 0.44813272
 0.44830364 0.44855583 0.44886103 0.44910035 0.44907495 0.44892895
 0.4487383  0.44863212 0.4486014  0.44863582 0.44877937 0.44889104
 0.4489727  0.44915545 0.44960162 0.4502608  0.45102817 0.45177642
 0.45245123 0.4530911  0.45380178 0.45475733 0.45576006 0.4566408
 0.45720345 0.4575374  0.45780486 0.45808446 0.4582555  0.458413
 0.45862392 0.45901614 0.45956352 0.46028522 0.46112224 0.46190557
 0.4624896  0.46283728 0.46301433 0.46313646 0.46330425 0.46357906
 0.46401882 0.46481872 0.4658416  0.46688148 0.4677756  0.4684968
 0.46881476 0.46852756 0.4679382  0.4673695  0.46665886 0.4656733
 0.46441123 0.46303222 0.46162784 0.46030906 0.45888922 0.45739323
 0.45585242 0.45438498 0.4530584  0.4516659  0.44993204 0.4480324
 0.44600332 0.44417155 0.44270393 0.44166774 0.44074485 0.43981963
 0.43905634 0.4387578  0.4386542  0.43855435 0.43862295 0.43855307
 0.43834963 0.4382774  0.43831    0.43818903 0.4379022  0.43742275
 0.43697104 0.43671116 0.4365102  0.43649647 0.43664104 0.4365261
 0.4363816  0.43631    0.43655008 0.43720722 0.43779048 0.43820712
 0.4384082  0.4385818  0.43885645 0.4393875  0.4397695  0.43992916
 0.43971705 0.43947673 0.43927196 0.43948734 0.43972218 0.43972147
 0.4397303  0.43999442 0.440357   0.44091797 0.441635   0.4423594
 0.4432188  0.4439515  0.44461468 0.44546294 0.44657707 0.4475931
 0.44845772 0.4490738  0.4496441  0.45019066 0.4507397  0.45124304
 0.4517388  0.45230988 0.45287445 0.453541   0.45428702 0.45503727
 0.45576006 0.45635906 0.4567978  0.45696327 0.45661962 0.45544463
 0.45326602 0.4506304  0.44836855 0.44687325 0.44604716 0.44561827
 0.44530377 0.44481486 0.44445434 0.44423038 0.4438294  0.44307974
 0.44203952 0.4409161  0.43985984 0.43882513 0.4378366  0.43684244
 0.43607366 0.43540323 0.43477336 0.43407476 0.4332267  0.43221992
 0.43115488 0.430208   0.42950812 0.42897496 0.42818752 0.42727983
 0.42645365 0.42589366 0.42566225 0.42563546 0.4255771  0.425416
 0.4250698  0.42481327 0.42466888 0.4245783  0.42436653 0.42394024
 0.42362753 0.4234324  0.42339018 0.42334694 0.42331254 0.4232038
 0.4231385  0.4230363  0.42322573 0.42349365 0.42386344 0.42415458
 0.42436165 0.4246863  0.42503786 0.42531914 0.4253847  0.4252332
 0.42502514 0.42491648 0.42505863 0.42526346 0.4253287  0.4252734
 0.4252143  0.42532325 0.42572436 0.4263043  0.4268625  0.4272915
 0.42762202 0.4280448  0.42873967 0.4296363  0.43073484 0.43178314
 0.43265522 0.43353415 0.43442106 0.4351522  0.43582302 0.43640193
 0.43709072 0.43793982 0.4391235  0.44055778 0.4420334  0.4434353
 0.44461516 0.44544458 0.44589683 0.44602668 0.4458748  0.44554034
 0.445212   0.44513983 0.44550645 0.44618893 0.44696692 0.44781795
 0.44854116 0.44887206 0.44904944 0.4491509  0.4489673  0.44830543
 0.44723514 0.4460082  0.44479334 0.4437094  0.44267386 0.4416909
 0.4407717  0.43991187 0.4391857  0.4384695  0.43752992 0.43636644
 0.43522534 0.43404117 0.43302387 0.4324859  0.43209198 0.43163428
 0.43124622 0.4309488  0.43091705 0.43093812 0.4307547  0.43047678
 0.43020508 0.4301359  0.43015414 0.43011346 0.42986682 0.42942828
 0.42892092 0.42859587 0.4282776  0.4281805  0.428132   0.42804813
 0.427943   0.42792216 0.42822015 0.42878702 0.42920768 0.42934853
 0.42930877 0.4294779  0.43000308 0.43074337 0.4312799  0.4313413
 0.43101746 0.43084326 0.43095115 0.43132317 0.4315778  0.43153465
 0.43120623 0.43104485 0.43123674 0.43181384 0.43239623 0.43282497
 0.43308634 0.4332776  0.43368298 0.43449318 0.43548515 0.4363559
 0.43706337 0.4378083  0.43855894 0.43931547 0.4399601  0.4404254
 0.4408209  0.44130006 0.44212207 0.44317314 0.4442857  0.44530022
 0.4460174  0.44641888 0.44654062 0.44644794 0.44605738 0.44527003
 0.44397888 0.4425319  0.4411845  0.44025457 0.43970427 0.4395859
 0.4395627  0.43932703 0.4390476  0.43872738 0.43814605 0.43717182
 0.43585026 0.4343927  0.4330054  0.43173483 0.43043703 0.42898294
 0.42742693 0.42604956 0.42496946 0.42400432 0.42314947 0.4222178
 0.42125392 0.4201923  0.4191204  0.41818693 0.41736856 0.41671836
 0.41617972 0.4159127  0.41586128 0.41590396 0.41584432 0.4156924
 0.4154005  0.4152053  0.4151978  0.41504967 0.41477153 0.4144749
 0.41425923 0.41427347 0.41422966 0.4140576  0.41374552 0.41354784
 0.4135143  0.4137033  0.4140263  0.4143105  0.4144121  0.4143208
 0.41425738 0.41430628 0.4146251  0.41507506 0.4153747  0.41531134
 0.41521013 0.4151901  0.41523197 0.41522244 0.4150358  0.4148234
 0.4147211  0.4148354  0.4151486  0.41551337 0.41575655 0.41598764
 0.41631916 0.41677654 0.41748464 0.41839844 0.4193909  0.42028022
 0.4210854  0.42196402 0.42285696 0.423575   0.42409083 0.42449638
 0.42498755 0.4258269  0.42685485 0.4279423  0.4289381  0.42973655
 0.4303296  0.4307209  0.4309208  0.43077686 0.43007326 0.42866248
 0.42661926 0.42447656 0.4229604  0.42203134 0.4217692  0.4220047
 0.4224331  0.42267394 0.422763   0.42275825 0.42246237 0.42170557
 0.42050788 0.41906554 0.4175244  0.41606167 0.41473043 0.41342095
 0.41219065 0.41112164 0.41023406 0.40936664 0.40840974 0.40732428
 0.40620208 0.40506774 0.40403688 0.40320507 0.40245983 0.40189937
 0.4015633  0.40153047 0.4015352  0.40142298 0.40110323 0.40086567
 0.4008258  0.40110147 0.4015311  0.40173653 0.4017126  0.4014234
 0.40118414 0.40117678 0.40118274 0.40109158 0.4009188  0.4007969
 0.40074283 0.40090534 0.40137962 0.4017321  0.40187687 0.40194523
 0.4019608  0.40199164 0.40218893 0.40228277 0.40218547 0.40184268
 0.40150878 0.40138018 0.40139928 0.40122172 0.40089488 0.4005732
 0.40045264 0.4006311  0.40103483 0.40152687 0.4018198  0.40206778
 0.4025     0.40313494 0.4038338  0.40454614 0.40510225 0.4056604
 0.40631664 0.4072729  0.40833    0.409164   0.4096252  0.40989265
 0.41028586 0.41099772 0.4120559  0.4131533  0.41407967 0.41484535
 0.41548634 0.41595775 0.41603914 0.41547808 0.41426337 0.41266167
 0.41096407 0.40957573 0.4086525  0.40802386 0.40787536 0.40846252
 0.40956938 0.41056117 0.41106504 0.41088256 0.41013816 0.40918097
 0.4082809  0.40733787 0.4061037  0.4044155  0.40249225 0.4008432
 0.39993408 0.3994439  0.39872354 0.39741564 0.39574617 0.39435843
 0.39348075 0.39278615 0.39171854 0.39033508 0.3892746  0.38904974
 0.3894286  0.38946167 0.38868624 0.3872482  0.3860248  0.386211
 0.38748744 0.38830328 0.38750383 0.38566852 0.38476568 0.38594356
 0.38772804 0.3874343  0.38458842 0.3829608  0.3869208  0.3912452 ]
