Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.0313905
	speed: 0.0199s/iter; left time: 522.3262s
	iters: 200, epoch: 1 | loss: 0.8189461
	speed: 0.0153s/iter; left time: 398.6280s
Epoch: 1 cost time: 4.485582113265991
Epoch: 1, Steps: 263 | Train Loss: 1.0079996 Vali Loss: 1.4004401 Test Loss: 0.8666712
Validation loss decreased (inf --> 1.400440).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6231135
	speed: 0.0728s/iter; left time: 1887.0895s
	iters: 200, epoch: 2 | loss: 0.6360325
	speed: 0.0146s/iter; left time: 377.7958s
Epoch: 2 cost time: 4.820734739303589
Epoch: 2, Steps: 263 | Train Loss: 0.5926684 Vali Loss: 1.1221535 Test Loss: 0.5903959
Validation loss decreased (1.400440 --> 1.122154).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.5182607
	speed: 0.0807s/iter; left time: 2070.8255s
	iters: 200, epoch: 3 | loss: 0.5242403
	speed: 0.0150s/iter; left time: 382.9906s
Epoch: 3 cost time: 4.4799089431762695
Epoch: 3, Steps: 263 | Train Loss: 0.5200910 Vali Loss: 1.0575528 Test Loss: 0.5283039
Validation loss decreased (1.122154 --> 1.057553).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4516863
	speed: 0.0724s/iter; left time: 1839.9725s
	iters: 200, epoch: 4 | loss: 0.4539849
	speed: 0.0147s/iter; left time: 371.4826s
Epoch: 4 cost time: 4.348404884338379
Epoch: 4, Steps: 263 | Train Loss: 0.5028174 Vali Loss: 1.0333695 Test Loss: 0.5078665
Validation loss decreased (1.057553 --> 1.033370).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5094730
	speed: 0.0729s/iter; left time: 1833.9589s
	iters: 200, epoch: 5 | loss: 0.5105604
	speed: 0.0151s/iter; left time: 377.3521s
Epoch: 5 cost time: 4.513256549835205
Epoch: 5, Steps: 263 | Train Loss: 0.4971619 Vali Loss: 1.0217807 Test Loss: 0.4997546
Validation loss decreased (1.033370 --> 1.021781).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4779895
	speed: 0.0724s/iter; left time: 1802.0220s
	iters: 200, epoch: 6 | loss: 0.5008880
	speed: 0.0147s/iter; left time: 365.5703s
Epoch: 6 cost time: 4.524944543838501
Epoch: 6, Steps: 263 | Train Loss: 0.4952526 Vali Loss: 1.0173919 Test Loss: 0.4960304
Validation loss decreased (1.021781 --> 1.017392).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4420101
	speed: 0.0741s/iter; left time: 1823.3365s
	iters: 200, epoch: 7 | loss: 0.4909673
	speed: 0.0154s/iter; left time: 376.4612s
Epoch: 7 cost time: 4.615898609161377
Epoch: 7, Steps: 263 | Train Loss: 0.4944739 Vali Loss: 1.0156598 Test Loss: 0.4945624
Validation loss decreased (1.017392 --> 1.015660).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5125327
	speed: 0.0744s/iter; left time: 1812.7688s
	iters: 200, epoch: 8 | loss: 0.5089135
	speed: 0.0150s/iter; left time: 362.9527s
Epoch: 8 cost time: 4.453507423400879
Epoch: 8, Steps: 263 | Train Loss: 0.4940843 Vali Loss: 1.0142051 Test Loss: 0.4943219
Validation loss decreased (1.015660 --> 1.014205).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4808449
	speed: 0.0725s/iter; left time: 1746.5403s
	iters: 200, epoch: 9 | loss: 0.5012084
	speed: 0.0150s/iter; left time: 361.1458s
Epoch: 9 cost time: 4.538753271102905
Epoch: 9, Steps: 263 | Train Loss: 0.4941069 Vali Loss: 1.0145797 Test Loss: 0.4939832
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4986898
	speed: 0.0741s/iter; left time: 1766.0309s
	iters: 200, epoch: 10 | loss: 0.4701525
	speed: 0.0146s/iter; left time: 347.2322s
Epoch: 10 cost time: 4.327596426010132
Epoch: 10, Steps: 263 | Train Loss: 0.4939256 Vali Loss: 1.0131537 Test Loss: 0.4938679
Validation loss decreased (1.014205 --> 1.013154).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5363433
	speed: 0.0722s/iter; left time: 1702.1941s
	iters: 200, epoch: 11 | loss: 0.4961889
	speed: 0.0147s/iter; left time: 344.5937s
Epoch: 11 cost time: 4.477709531784058
Epoch: 11, Steps: 263 | Train Loss: 0.4937081 Vali Loss: 1.0135202 Test Loss: 0.4937994
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.5192953
	speed: 0.0734s/iter; left time: 1709.6748s
	iters: 200, epoch: 12 | loss: 0.5059097
	speed: 0.0150s/iter; left time: 348.2675s
Epoch: 12 cost time: 4.532532215118408
Epoch: 12, Steps: 263 | Train Loss: 0.4937559 Vali Loss: 1.0136523 Test Loss: 0.4939679
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4832652
	speed: 0.0727s/iter; left time: 1676.0436s
	iters: 200, epoch: 13 | loss: 0.4974621
	speed: 0.0148s/iter; left time: 340.0955s
Epoch: 13 cost time: 4.44884181022644
Epoch: 13, Steps: 263 | Train Loss: 0.4937206 Vali Loss: 1.0137709 Test Loss: 0.4938167
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4811751
	speed: 0.0744s/iter; left time: 1693.9168s
	iters: 200, epoch: 14 | loss: 0.5012154
	speed: 0.0149s/iter; left time: 337.5247s
Epoch: 14 cost time: 4.77315878868103
Epoch: 14, Steps: 263 | Train Loss: 0.4938563 Vali Loss: 1.0119811 Test Loss: 0.4936827
Validation loss decreased (1.013154 --> 1.011981).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5050156
	speed: 0.0751s/iter; left time: 1690.2378s
	iters: 200, epoch: 15 | loss: 0.5193960
	speed: 0.0148s/iter; left time: 332.1750s
Epoch: 15 cost time: 4.492635250091553
Epoch: 15, Steps: 263 | Train Loss: 0.4937813 Vali Loss: 1.0123084 Test Loss: 0.4937939
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4521354
	speed: 0.0736s/iter; left time: 1638.1450s
	iters: 200, epoch: 16 | loss: 0.4951733
	speed: 0.0147s/iter; left time: 326.4512s
Epoch: 16 cost time: 4.461904525756836
Epoch: 16, Steps: 263 | Train Loss: 0.4937516 Vali Loss: 1.0130267 Test Loss: 0.4940768
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4907741
	speed: 0.0721s/iter; left time: 1585.9843s
	iters: 200, epoch: 17 | loss: 0.5117498
	speed: 0.0148s/iter; left time: 325.0776s
Epoch: 17 cost time: 4.531710863113403
Epoch: 17, Steps: 263 | Train Loss: 0.4938586 Vali Loss: 1.0140563 Test Loss: 0.4940708
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5247530
	speed: 0.0751s/iter; left time: 1632.4765s
	iters: 200, epoch: 18 | loss: 0.5419809
	speed: 0.0166s/iter; left time: 358.3933s
Epoch: 18 cost time: 4.980763912200928
Epoch: 18, Steps: 263 | Train Loss: 0.4936048 Vali Loss: 1.0132492 Test Loss: 0.4938497
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4645022
	speed: 0.0889s/iter; left time: 1908.5863s
	iters: 200, epoch: 19 | loss: 0.5054101
	speed: 0.0434s/iter; left time: 927.5478s
Epoch: 19 cost time: 8.614165306091309
Epoch: 19, Steps: 263 | Train Loss: 0.4935774 Vali Loss: 1.0130689 Test Loss: 0.4942287
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4676709
	speed: 0.0794s/iter; left time: 1684.6476s
	iters: 200, epoch: 20 | loss: 0.4876927
	speed: 0.0146s/iter; left time: 307.9067s
Epoch: 20 cost time: 4.532179117202759
Epoch: 20, Steps: 263 | Train Loss: 0.4935232 Vali Loss: 1.0128969 Test Loss: 0.4941162
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5283542
	speed: 0.0737s/iter; left time: 1543.2476s
	iters: 200, epoch: 21 | loss: 0.5325164
	speed: 0.0150s/iter; left time: 312.2270s
Epoch: 21 cost time: 4.46354079246521
Epoch: 21, Steps: 263 | Train Loss: 0.4936039 Vali Loss: 1.0134230 Test Loss: 0.4941708
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4653004
	speed: 0.0717s/iter; left time: 1482.2084s
	iters: 200, epoch: 22 | loss: 0.5129408
	speed: 0.0149s/iter; left time: 306.3923s
Epoch: 22 cost time: 4.45233416557312
Epoch: 22, Steps: 263 | Train Loss: 0.4935401 Vali Loss: 1.0135610 Test Loss: 0.4940821
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5025094
	speed: 0.0725s/iter; left time: 1479.4440s
	iters: 200, epoch: 23 | loss: 0.5184434
	speed: 0.0148s/iter; left time: 300.8482s
Epoch: 23 cost time: 4.496939420700073
Epoch: 23, Steps: 263 | Train Loss: 0.4937479 Vali Loss: 1.0130434 Test Loss: 0.4939612
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4411818
	speed: 0.0714s/iter; left time: 1439.3149s
	iters: 200, epoch: 24 | loss: 0.5220447
	speed: 0.0146s/iter; left time: 291.8320s
Epoch: 24 cost time: 4.418355226516724
Epoch: 24, Steps: 263 | Train Loss: 0.4935965 Vali Loss: 1.0128148 Test Loss: 0.4941557
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5075727
	speed: 0.0732s/iter; left time: 1455.2456s
	iters: 200, epoch: 25 | loss: 0.5124164
	speed: 0.0147s/iter; left time: 290.9375s
Epoch: 25 cost time: 4.418109178543091
Epoch: 25, Steps: 263 | Train Loss: 0.4936018 Vali Loss: 1.0129950 Test Loss: 0.4941626
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.5235407
	speed: 0.0729s/iter; left time: 1430.5527s
	iters: 200, epoch: 26 | loss: 0.4666771
	speed: 0.0149s/iter; left time: 291.3913s
Epoch: 26 cost time: 4.48824667930603
Epoch: 26, Steps: 263 | Train Loss: 0.4935063 Vali Loss: 1.0130388 Test Loss: 0.4941204
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.5165012
	speed: 0.0774s/iter; left time: 1497.8642s
	iters: 200, epoch: 27 | loss: 0.4833211
	speed: 0.0184s/iter; left time: 354.1110s
Epoch: 27 cost time: 5.406796455383301
Epoch: 27, Steps: 263 | Train Loss: 0.4935188 Vali Loss: 1.0125897 Test Loss: 0.4942818
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4848365
	speed: 0.0724s/iter; left time: 1382.1553s
	iters: 200, epoch: 28 | loss: 0.4832561
	speed: 0.0149s/iter; left time: 283.8310s
Epoch: 28 cost time: 4.5106120109558105
Epoch: 28, Steps: 263 | Train Loss: 0.4936241 Vali Loss: 1.0133611 Test Loss: 0.4942749
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4792714
	speed: 0.0724s/iter; left time: 1364.6909s
	iters: 200, epoch: 29 | loss: 0.5131041
	speed: 0.0170s/iter; left time: 317.9632s
Epoch: 29 cost time: 4.749563455581665
Epoch: 29, Steps: 263 | Train Loss: 0.4934463 Vali Loss: 1.0136787 Test Loss: 0.4941822
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4776880
	speed: 0.0726s/iter; left time: 1349.3056s
	iters: 200, epoch: 30 | loss: 0.5080742
	speed: 0.0150s/iter; left time: 277.0252s
Epoch: 30 cost time: 4.519975423812866
Epoch: 30, Steps: 263 | Train Loss: 0.4935086 Vali Loss: 1.0141081 Test Loss: 0.4943276
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.5274315
	speed: 0.0718s/iter; left time: 1315.2602s
	iters: 200, epoch: 31 | loss: 0.4667419
	speed: 0.0148s/iter; left time: 268.9273s
Epoch: 31 cost time: 4.5194127559661865
Epoch: 31, Steps: 263 | Train Loss: 0.4935646 Vali Loss: 1.0134966 Test Loss: 0.4943923
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4870190
	speed: 0.0722s/iter; left time: 1302.4461s
	iters: 200, epoch: 32 | loss: 0.5056597
	speed: 0.0150s/iter; left time: 268.6427s
Epoch: 32 cost time: 4.424999475479126
Epoch: 32, Steps: 263 | Train Loss: 0.4936073 Vali Loss: 1.0135280 Test Loss: 0.4943002
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4595043
	speed: 0.0783s/iter; left time: 1392.3699s
	iters: 200, epoch: 33 | loss: 0.5167071
	speed: 0.0149s/iter; left time: 263.5480s
Epoch: 33 cost time: 4.4673357009887695
Epoch: 33, Steps: 263 | Train Loss: 0.4935360 Vali Loss: 1.0131195 Test Loss: 0.4942910
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.5521978
	speed: 0.0707s/iter; left time: 1238.0569s
	iters: 200, epoch: 34 | loss: 0.5173339
	speed: 0.0148s/iter; left time: 258.2279s
Epoch: 34 cost time: 4.443814277648926
Epoch: 34, Steps: 263 | Train Loss: 0.4936126 Vali Loss: 1.0127488 Test Loss: 0.4942548
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4918220639228821, mae:0.45223256945610046, rse:0.6672287583351135, corr:[0.5332794  0.5327978  0.5274947  0.52415544 0.5223153  0.5193014
 0.5155853  0.5129992  0.51177984 0.51046413 0.5085478  0.5069184
 0.50600433 0.50484455 0.5020075  0.4978266  0.49402428 0.4914463
 0.48935306 0.4865524  0.48304504 0.47966266 0.4769064  0.47430164
 0.4710213  0.46684515 0.46295866 0.46058738 0.45960814 0.45866427
 0.45740747 0.4568581  0.45714203 0.45781493 0.45824754 0.45827323
 0.45775503 0.45750993 0.45780018 0.4581726  0.45824778 0.45768374
 0.45711395 0.45700395 0.4569685  0.45670092 0.45627868 0.45605224
 0.4563294  0.4569772  0.45753416 0.45758197 0.45761833 0.45804557
 0.4591005  0.46013784 0.46066746 0.46065453 0.46035454 0.4603425
 0.46050984 0.4601241  0.45936096 0.45863223 0.45861775 0.45893902
 0.45879766 0.4581791  0.45793864 0.45858684 0.45963085 0.46053723
 0.46115026 0.4615452  0.4622125  0.4630486  0.4640407  0.465001
 0.46574926 0.4664162  0.46706426 0.46746147 0.46771953 0.46777126
 0.46795192 0.4684132  0.46914884 0.46995103 0.47069612 0.47147015
 0.472325   0.47316325 0.47381204 0.474068   0.47393268 0.473427
 0.47266012 0.47181705 0.47090933 0.4702292  0.47012568 0.47035953
 0.47032797 0.46970546 0.46897855 0.46850628 0.46817037 0.4676916
 0.46693274 0.46599853 0.46504825 0.4641592  0.46328464 0.46235928
 0.4613014  0.46032196 0.45939624 0.4584796  0.45746544 0.4562326
 0.4548522  0.45334566 0.45199183 0.4511234  0.45043173 0.4495547
 0.44857523 0.44780174 0.44741362 0.4473364  0.44742504 0.44760704
 0.44765025 0.44761086 0.4473534  0.44693732 0.44656977 0.44643432
 0.44642082 0.4463493  0.44618624 0.44617832 0.4464129  0.44679564
 0.44700894 0.44703895 0.447106   0.4473099  0.4478434  0.448283
 0.4483691  0.4484281  0.44862393 0.44895068 0.44910732 0.44907874
 0.44890147 0.44882342 0.44890952 0.44906738 0.4491962  0.44911215
 0.4489689  0.44906238 0.44953063 0.4501767  0.45085597 0.45156133
 0.4523501  0.45316014 0.45385626 0.45453337 0.45514452 0.45580706
 0.45646182 0.4570922  0.45762736 0.45799032 0.4580866  0.45817462
 0.4584585  0.45905498 0.4597702  0.46052355 0.4613619  0.46232614
 0.46329752 0.46397027 0.46410742 0.46385342 0.4636785  0.46388915
 0.46431464 0.46469846 0.46506724 0.4658408  0.46713287 0.46838588
 0.46874183 0.46816394 0.46749762 0.46712947 0.46668032 0.46579215
 0.4644687  0.4630412  0.4616724  0.46036613 0.45883465 0.45716035
 0.45554024 0.45423147 0.4532377  0.45209247 0.4503684  0.4483459
 0.4462768  0.4446447  0.4434585  0.44245481 0.44120455 0.43991968
 0.43916628 0.4392575  0.43953815 0.43949243 0.43934578 0.43905902
 0.43881142 0.43878382 0.4387758  0.43855715 0.4383191  0.43809134
 0.4378826  0.43759134 0.43709087 0.43683055 0.43705592 0.43725917
 0.43736395 0.43724918 0.43723696 0.43760294 0.43792853 0.438157
 0.43832606 0.43870938 0.4393462  0.44015068 0.44049296 0.44039997
 0.44005686 0.44003382 0.4402004  0.44056913 0.4405762  0.4402325
 0.44016516 0.44064674 0.44110742 0.44138095 0.44165188 0.4422239
 0.4433694  0.44442967 0.44500786 0.44539526 0.44615343 0.44727877
 0.44861022 0.4495922  0.45012033 0.4503305  0.4506303  0.451167
 0.4518119  0.4523939  0.45274314 0.45316637 0.4539273  0.45498466
 0.45603883 0.4567051  0.45689878 0.4567051  0.45615545 0.4550609
 0.45307827 0.45043516 0.447976   0.44637626 0.44568336 0.44552815
 0.44548464 0.44526675 0.4450584  0.4447515  0.44409904 0.44311747
 0.44199917 0.44094017 0.43995112 0.43886554 0.43773106 0.43664598
 0.4358974  0.4352852  0.43464538 0.43384913 0.43293476 0.43197837
 0.43100032 0.43003032 0.42914316 0.42845386 0.4278013  0.4273841
 0.42714453 0.426908   0.4265632  0.42612606 0.42575115 0.42561924
 0.4255196  0.42536587 0.42503497 0.42464623 0.42432567 0.4240598
 0.42397022 0.42381862 0.42360756 0.42336798 0.4233019  0.4232942
 0.42328075 0.4230769  0.42312896 0.42337394 0.42384496 0.42421997
 0.42439216 0.4246103  0.42486668 0.4251373  0.42529696 0.425346
 0.4253867  0.4254212  0.42545143 0.42532614 0.42507517 0.42494196
 0.42505652 0.42534995 0.42571205 0.42606536 0.42648372 0.42704195
 0.4276427  0.4281833  0.42874423 0.42945316 0.43051964 0.4316755
 0.432616   0.43342128 0.43415844 0.43480292 0.4355404  0.43628916
 0.43712923 0.43800414 0.4391006  0.4404539  0.44200557 0.44364497
 0.445042   0.44588006 0.44621104 0.44642156 0.44658315 0.4464103
 0.44571987 0.4450301  0.44511712 0.44609594 0.44736353 0.4483944
 0.44896308 0.44917074 0.449374   0.44948202 0.44918606 0.44839567
 0.44733974 0.4462713  0.4451992  0.44408315 0.44287333 0.44175467
 0.44084454 0.4400709  0.43934357 0.4384793  0.437407   0.43635342
 0.43556145 0.43471065 0.43382508 0.433241   0.4327723  0.4323181
 0.4319419  0.4315166  0.43119854 0.43094382 0.4307035  0.43062082
 0.43055084 0.43042487 0.4301435  0.42983988 0.429601   0.42939857
 0.42910865 0.42878443 0.42828184 0.42802283 0.4280087  0.42813495
 0.42821443 0.42816466 0.42819187 0.42837122 0.4285247  0.42872918
 0.429104   0.42977092 0.43044138 0.43079993 0.43074733 0.4305545
 0.43057066 0.4310122  0.43136358 0.43136597 0.43103004 0.430765
 0.43074167 0.43102324 0.43127817 0.43146124 0.43157473 0.43188393
 0.43240124 0.43285987 0.43326372 0.433887   0.4347894  0.43577847
 0.43665826 0.4374409  0.43803427 0.4385581  0.439093   0.4396732
 0.4403228  0.44102687 0.44193813 0.4429668  0.44406086 0.44517374
 0.4460775  0.44657233 0.44655314 0.4461522  0.445447   0.44447318
 0.44321075 0.44191065 0.44075122 0.43992946 0.4394613  0.43948752
 0.4397462  0.4398562  0.43970874 0.43915033 0.43823445 0.43713868
 0.43597418 0.43476874 0.43349418 0.43211895 0.43068305 0.42929643
 0.4280125  0.42690235 0.4258554  0.4247058  0.42365965 0.42271918
 0.4219174  0.42103556 0.4201041  0.41932106 0.4186854  0.4181589
 0.41757816 0.4171001  0.41677237 0.41658363 0.41644374 0.41641864
 0.4163953  0.41646555 0.41658795 0.41636908 0.4158813  0.4154029
 0.41519266 0.41536248 0.41539896 0.415091   0.41451085 0.4141149
 0.41403311 0.41418615 0.41430956 0.41425774 0.4141619  0.41425505
 0.41465446 0.4150246  0.41522342 0.4152688  0.4153197  0.41540247
 0.41565296 0.41577452 0.41558778 0.41526446 0.4150244  0.41503504
 0.4151305  0.41517437 0.41524622 0.4155264  0.41603842 0.41671747
 0.41727844 0.4175456  0.417858   0.41853172 0.41952392 0.4204687
 0.42122096 0.42197666 0.42279747 0.42350367 0.42402825 0.4244314
 0.42490715 0.42574644 0.42674628 0.42773932 0.42866015 0.42954126
 0.43040034 0.43105063 0.43125224 0.43075928 0.42953953 0.42785433
 0.42601264 0.42430168 0.4229269  0.4218317  0.42142853 0.4218472
 0.4225564  0.42291048 0.42295116 0.42292437 0.42273557 0.42214632
 0.420964   0.41927233 0.4173965  0.41577792 0.41454205 0.41344482
 0.41233334 0.4111887  0.41014117 0.40925127 0.4084801  0.4076296
 0.4065638  0.405278   0.40413734 0.40344566 0.40296394 0.40253416
 0.40211016 0.40190282 0.401827   0.4017618  0.4015471  0.40141317
 0.40143216 0.40167132 0.40190655 0.40177828 0.40146136 0.40115756
 0.40121666 0.40150455 0.40147373 0.40108457 0.40072325 0.40075347
 0.40097338 0.40110478 0.40108305 0.40083253 0.40078625 0.40115368
 0.4015229  0.40153694 0.40148047 0.4016257  0.4021696  0.40266302
 0.40264457 0.40209737 0.4015282  0.4012633  0.40139008 0.40145046
 0.40112826 0.4007013  0.4006967  0.40133998 0.40208948 0.40256283
 0.40277484 0.40301642 0.40357262 0.40453613 0.40544757 0.40614006
 0.40669763 0.40749475 0.40846843 0.40927723 0.40974444 0.4101049
 0.41069254 0.4115787  0.41264832 0.4135789  0.4142748  0.41492927
 0.41566148 0.4162967  0.41640827 0.4155967  0.41398498 0.41217294
 0.41062808 0.40939862 0.40836734 0.4077146  0.40801275 0.409242
 0.41048282 0.4110474  0.41118097 0.4111956  0.41110653 0.4106253
 0.40948352 0.40776974 0.4059443  0.40439278 0.4031214  0.40183797
 0.40050745 0.39921936 0.3982093  0.3974585  0.39660236 0.39540425
 0.39391562 0.39260662 0.3916421  0.39083597 0.38988045 0.38877025
 0.38782963 0.38714412 0.38675502 0.38618156 0.38511595 0.38432375
 0.38430408 0.38482842 0.38508797 0.38463107 0.38393414 0.38397986
 0.38515118 0.38658604 0.38694373 0.38651547 0.3888324  0.39479187]
