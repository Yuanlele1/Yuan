Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=122, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=514, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=122, out_features=244, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  26672128.0
params:  30012.0
Trainable parameters:  30012
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.4764260
	speed: 0.0212s/iter; left time: 544.6359s
	iters: 200, epoch: 1 | loss: 0.4220876
	speed: 0.0166s/iter; left time: 423.9851s
Epoch: 1 cost time: 4.694730520248413
Epoch: 1, Steps: 258 | Train Loss: 0.5104491 Vali Loss: 1.0145334 Test Loss: 0.4511632
Validation loss decreased (inf --> 1.014533).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4080989
	speed: 0.0691s/iter; left time: 1756.8862s
	iters: 200, epoch: 2 | loss: 0.4200498
	speed: 0.0186s/iter; left time: 470.7687s
Epoch: 2 cost time: 5.016613483428955
Epoch: 2, Steps: 258 | Train Loss: 0.4153620 Vali Loss: 0.9644344 Test Loss: 0.4207531
Validation loss decreased (1.014533 --> 0.964434).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4049719
	speed: 0.0696s/iter; left time: 1752.4207s
	iters: 200, epoch: 3 | loss: 0.3679887
	speed: 0.0166s/iter; left time: 416.4474s
Epoch: 3 cost time: 4.718648195266724
Epoch: 3, Steps: 258 | Train Loss: 0.4032217 Vali Loss: 0.9483619 Test Loss: 0.4144843
Validation loss decreased (0.964434 --> 0.948362).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3977990
	speed: 0.0710s/iter; left time: 1769.0094s
	iters: 200, epoch: 4 | loss: 0.4024889
	speed: 0.0171s/iter; left time: 423.4471s
Epoch: 4 cost time: 4.796826601028442
Epoch: 4, Steps: 258 | Train Loss: 0.3994908 Vali Loss: 0.9432371 Test Loss: 0.4143547
Validation loss decreased (0.948362 --> 0.943237).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3785858
	speed: 0.0701s/iter; left time: 1729.0947s
	iters: 200, epoch: 5 | loss: 0.3996916
	speed: 0.0165s/iter; left time: 405.6849s
Epoch: 5 cost time: 4.74575400352478
Epoch: 5, Steps: 258 | Train Loss: 0.3982990 Vali Loss: 0.9385560 Test Loss: 0.4147288
Validation loss decreased (0.943237 --> 0.938556).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3508725
	speed: 0.0705s/iter; left time: 1721.7470s
	iters: 200, epoch: 6 | loss: 0.4077694
	speed: 0.0166s/iter; left time: 404.2614s
Epoch: 6 cost time: 4.739087820053101
Epoch: 6, Steps: 258 | Train Loss: 0.3977744 Vali Loss: 0.9372450 Test Loss: 0.4154044
Validation loss decreased (0.938556 --> 0.937245).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4107512
	speed: 0.0697s/iter; left time: 1684.3832s
	iters: 200, epoch: 7 | loss: 0.4020625
	speed: 0.0172s/iter; left time: 413.6580s
Epoch: 7 cost time: 4.779407501220703
Epoch: 7, Steps: 258 | Train Loss: 0.3974324 Vali Loss: 0.9350239 Test Loss: 0.4155505
Validation loss decreased (0.937245 --> 0.935024).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4167462
	speed: 0.0697s/iter; left time: 1664.9231s
	iters: 200, epoch: 8 | loss: 0.3826614
	speed: 0.0167s/iter; left time: 396.8925s
Epoch: 8 cost time: 4.761603593826294
Epoch: 8, Steps: 258 | Train Loss: 0.3972469 Vali Loss: 0.9337514 Test Loss: 0.4159555
Validation loss decreased (0.935024 --> 0.933751).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3953822
	speed: 0.0686s/iter; left time: 1621.2020s
	iters: 200, epoch: 9 | loss: 0.3881524
	speed: 0.0168s/iter; left time: 395.6402s
Epoch: 9 cost time: 4.7458837032318115
Epoch: 9, Steps: 258 | Train Loss: 0.3971177 Vali Loss: 0.9337756 Test Loss: 0.4161848
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3865731
	speed: 0.0693s/iter; left time: 1620.5010s
	iters: 200, epoch: 10 | loss: 0.3675130
	speed: 0.0163s/iter; left time: 379.9550s
Epoch: 10 cost time: 4.657187223434448
Epoch: 10, Steps: 258 | Train Loss: 0.3970674 Vali Loss: 0.9322265 Test Loss: 0.4163889
Validation loss decreased (0.933751 --> 0.932227).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3961759
	speed: 0.0693s/iter; left time: 1602.2268s
	iters: 200, epoch: 11 | loss: 0.4291003
	speed: 0.0166s/iter; left time: 382.6839s
Epoch: 11 cost time: 4.6898274421691895
Epoch: 11, Steps: 258 | Train Loss: 0.3968990 Vali Loss: 0.9329215 Test Loss: 0.4160130
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4087445
	speed: 0.0701s/iter; left time: 1602.7620s
	iters: 200, epoch: 12 | loss: 0.3643031
	speed: 0.0167s/iter; left time: 379.7663s
Epoch: 12 cost time: 4.7853522300720215
Epoch: 12, Steps: 258 | Train Loss: 0.3968654 Vali Loss: 0.9327497 Test Loss: 0.4160334
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4110633
	speed: 0.0689s/iter; left time: 1557.2187s
	iters: 200, epoch: 13 | loss: 0.4088649
	speed: 0.0168s/iter; left time: 377.9161s
Epoch: 13 cost time: 4.75339412689209
Epoch: 13, Steps: 258 | Train Loss: 0.3967053 Vali Loss: 0.9322725 Test Loss: 0.4165550
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.3989136
	speed: 0.0693s/iter; left time: 1549.3353s
	iters: 200, epoch: 14 | loss: 0.3872226
	speed: 0.0165s/iter; left time: 367.5267s
Epoch: 14 cost time: 4.8167150020599365
Epoch: 14, Steps: 258 | Train Loss: 0.3968465 Vali Loss: 0.9331105 Test Loss: 0.4164408
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4142950
	speed: 0.0684s/iter; left time: 1510.6427s
	iters: 200, epoch: 15 | loss: 0.4097787
	speed: 0.0168s/iter; left time: 368.8772s
Epoch: 15 cost time: 4.704361915588379
Epoch: 15, Steps: 258 | Train Loss: 0.3966923 Vali Loss: 0.9314008 Test Loss: 0.4159080
Validation loss decreased (0.932227 --> 0.931401).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4315003
	speed: 0.0698s/iter; left time: 1523.6267s
	iters: 200, epoch: 16 | loss: 0.3886104
	speed: 0.0169s/iter; left time: 366.9146s
Epoch: 16 cost time: 4.7565648555755615
Epoch: 16, Steps: 258 | Train Loss: 0.3967616 Vali Loss: 0.9322231 Test Loss: 0.4165636
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4169569
	speed: 0.0706s/iter; left time: 1522.5789s
	iters: 200, epoch: 17 | loss: 0.4181330
	speed: 0.0169s/iter; left time: 362.9300s
Epoch: 17 cost time: 4.807893514633179
Epoch: 17, Steps: 258 | Train Loss: 0.3966003 Vali Loss: 0.9311098 Test Loss: 0.4162798
Validation loss decreased (0.931401 --> 0.931110).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4010194
	speed: 0.0698s/iter; left time: 1488.4975s
	iters: 200, epoch: 18 | loss: 0.3791884
	speed: 0.0167s/iter; left time: 354.8909s
Epoch: 18 cost time: 4.7559802532196045
Epoch: 18, Steps: 258 | Train Loss: 0.3965932 Vali Loss: 0.9319151 Test Loss: 0.4164588
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3895448
	speed: 0.0694s/iter; left time: 1460.7730s
	iters: 200, epoch: 19 | loss: 0.3997811
	speed: 0.0163s/iter; left time: 341.7559s
Epoch: 19 cost time: 4.737804174423218
Epoch: 19, Steps: 258 | Train Loss: 0.3964285 Vali Loss: 0.9320967 Test Loss: 0.4161211
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4069189
	speed: 0.0699s/iter; left time: 1454.8774s
	iters: 200, epoch: 20 | loss: 0.4142242
	speed: 0.0166s/iter; left time: 343.8022s
Epoch: 20 cost time: 4.797240257263184
Epoch: 20, Steps: 258 | Train Loss: 0.3965409 Vali Loss: 0.9313344 Test Loss: 0.4165910
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4233850
	speed: 0.0695s/iter; left time: 1427.2252s
	iters: 200, epoch: 21 | loss: 0.3990575
	speed: 0.0167s/iter; left time: 340.7048s
Epoch: 21 cost time: 4.7515709400177
Epoch: 21, Steps: 258 | Train Loss: 0.3965163 Vali Loss: 0.9315333 Test Loss: 0.4164336
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3939330
	speed: 0.0701s/iter; left time: 1421.4921s
	iters: 200, epoch: 22 | loss: 0.3705862
	speed: 0.0169s/iter; left time: 340.7309s
Epoch: 22 cost time: 4.83130669593811
Epoch: 22, Steps: 258 | Train Loss: 0.3965077 Vali Loss: 0.9306762 Test Loss: 0.4161431
Validation loss decreased (0.931110 --> 0.930676).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.3879079
	speed: 0.0706s/iter; left time: 1412.7891s
	iters: 200, epoch: 23 | loss: 0.3896194
	speed: 0.0166s/iter; left time: 330.1292s
Epoch: 23 cost time: 4.727408409118652
Epoch: 23, Steps: 258 | Train Loss: 0.3964946 Vali Loss: 0.9303185 Test Loss: 0.4164592
Validation loss decreased (0.930676 --> 0.930318).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3574162
	speed: 0.0692s/iter; left time: 1368.3539s
	iters: 200, epoch: 24 | loss: 0.3933842
	speed: 0.0166s/iter; left time: 326.0963s
Epoch: 24 cost time: 4.740610361099243
Epoch: 24, Steps: 258 | Train Loss: 0.3964559 Vali Loss: 0.9312919 Test Loss: 0.4162483
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3830279
	speed: 0.0682s/iter; left time: 1331.2309s
	iters: 200, epoch: 25 | loss: 0.3920402
	speed: 0.0168s/iter; left time: 325.6413s
Epoch: 25 cost time: 4.73986029624939
Epoch: 25, Steps: 258 | Train Loss: 0.3963936 Vali Loss: 0.9298846 Test Loss: 0.4164167
Validation loss decreased (0.930318 --> 0.929885).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3987381
	speed: 0.0698s/iter; left time: 1343.4273s
	iters: 200, epoch: 26 | loss: 0.3788242
	speed: 0.0169s/iter; left time: 322.7211s
Epoch: 26 cost time: 4.781045913696289
Epoch: 26, Steps: 258 | Train Loss: 0.3964670 Vali Loss: 0.9305617 Test Loss: 0.4161335
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3768074
	speed: 0.0708s/iter; left time: 1345.2381s
	iters: 200, epoch: 27 | loss: 0.4031264
	speed: 0.0166s/iter; left time: 314.2991s
Epoch: 27 cost time: 4.79522705078125
Epoch: 27, Steps: 258 | Train Loss: 0.3965098 Vali Loss: 0.9310144 Test Loss: 0.4160193
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4026043
	speed: 0.0690s/iter; left time: 1291.8335s
	iters: 200, epoch: 28 | loss: 0.4221931
	speed: 0.0168s/iter; left time: 312.3671s
Epoch: 28 cost time: 4.786921501159668
Epoch: 28, Steps: 258 | Train Loss: 0.3963498 Vali Loss: 0.9303530 Test Loss: 0.4159009
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4064547
	speed: 0.0699s/iter; left time: 1291.9973s
	iters: 200, epoch: 29 | loss: 0.3838620
	speed: 0.0170s/iter; left time: 311.6497s
Epoch: 29 cost time: 4.803039789199829
Epoch: 29, Steps: 258 | Train Loss: 0.3963156 Vali Loss: 0.9302720 Test Loss: 0.4159462
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.3778698
	speed: 0.0702s/iter; left time: 1278.3664s
	iters: 200, epoch: 30 | loss: 0.4259610
	speed: 0.0165s/iter; left time: 299.8217s
Epoch: 30 cost time: 4.714172124862671
Epoch: 30, Steps: 258 | Train Loss: 0.3964580 Vali Loss: 0.9307111 Test Loss: 0.4162019
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4157536
	speed: 0.0698s/iter; left time: 1254.5215s
	iters: 200, epoch: 31 | loss: 0.3742199
	speed: 0.0166s/iter; left time: 296.1479s
Epoch: 31 cost time: 4.808972120285034
Epoch: 31, Steps: 258 | Train Loss: 0.3962801 Vali Loss: 0.9302411 Test Loss: 0.4162816
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3914891
	speed: 0.0699s/iter; left time: 1238.0183s
	iters: 200, epoch: 32 | loss: 0.4052799
	speed: 0.0168s/iter; left time: 296.4838s
Epoch: 32 cost time: 4.742692708969116
Epoch: 32, Steps: 258 | Train Loss: 0.3961988 Vali Loss: 0.9301635 Test Loss: 0.4163961
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.3848755
	speed: 0.0698s/iter; left time: 1218.2700s
	iters: 200, epoch: 33 | loss: 0.3664436
	speed: 0.0168s/iter; left time: 291.4916s
Epoch: 33 cost time: 4.71300745010376
Epoch: 33, Steps: 258 | Train Loss: 0.3963679 Vali Loss: 0.9305476 Test Loss: 0.4163825
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4282869
	speed: 0.0703s/iter; left time: 1208.0031s
	iters: 200, epoch: 34 | loss: 0.4000168
	speed: 0.0173s/iter; left time: 295.0432s
Epoch: 34 cost time: 4.823451280593872
Epoch: 34, Steps: 258 | Train Loss: 0.3962637 Vali Loss: 0.9314130 Test Loss: 0.4163479
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3995997
	speed: 0.0703s/iter; left time: 1190.4934s
	iters: 200, epoch: 35 | loss: 0.3756698
	speed: 0.0167s/iter; left time: 280.6873s
Epoch: 35 cost time: 4.7255027294158936
Epoch: 35, Steps: 258 | Train Loss: 0.3962911 Vali Loss: 0.9301381 Test Loss: 0.4161208
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3964550
	speed: 0.0689s/iter; left time: 1147.8279s
	iters: 200, epoch: 36 | loss: 0.3722070
	speed: 0.0171s/iter; left time: 282.9369s
Epoch: 36 cost time: 4.813682317733765
Epoch: 36, Steps: 258 | Train Loss: 0.3962884 Vali Loss: 0.9302371 Test Loss: 0.4162770
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3970723
	speed: 0.0713s/iter; left time: 1169.9236s
	iters: 200, epoch: 37 | loss: 0.3429674
	speed: 0.0166s/iter; left time: 271.1728s
Epoch: 37 cost time: 4.797683000564575
Epoch: 37, Steps: 258 | Train Loss: 0.3961717 Vali Loss: 0.9314863 Test Loss: 0.4162688
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3776934
	speed: 0.0707s/iter; left time: 1141.4584s
	iters: 200, epoch: 38 | loss: 0.4020033
	speed: 0.0170s/iter; left time: 273.3769s
Epoch: 38 cost time: 4.838876724243164
Epoch: 38, Steps: 258 | Train Loss: 0.3962215 Vali Loss: 0.9311212 Test Loss: 0.4162809
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4086552
	speed: 0.0699s/iter; left time: 1111.1030s
	iters: 200, epoch: 39 | loss: 0.3751978
	speed: 0.0166s/iter; left time: 261.6966s
Epoch: 39 cost time: 4.853121519088745
Epoch: 39, Steps: 258 | Train Loss: 0.3961952 Vali Loss: 0.9308470 Test Loss: 0.4163369
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3909806
	speed: 0.0704s/iter; left time: 1101.0657s
	iters: 200, epoch: 40 | loss: 0.3770035
	speed: 0.0169s/iter; left time: 262.5855s
Epoch: 40 cost time: 4.7823145389556885
Epoch: 40, Steps: 258 | Train Loss: 0.3961892 Vali Loss: 0.9312019 Test Loss: 0.4161988
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3841710
	speed: 0.0699s/iter; left time: 1075.3434s
	iters: 200, epoch: 41 | loss: 0.4056030
	speed: 0.0165s/iter; left time: 252.5265s
Epoch: 41 cost time: 4.725042819976807
Epoch: 41, Steps: 258 | Train Loss: 0.3961294 Vali Loss: 0.9308356 Test Loss: 0.4162204
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4257701
	speed: 0.0703s/iter; left time: 1062.5812s
	iters: 200, epoch: 42 | loss: 0.3852739
	speed: 0.0172s/iter; left time: 258.4013s
Epoch: 42 cost time: 4.8704376220703125
Epoch: 42, Steps: 258 | Train Loss: 0.3961155 Vali Loss: 0.9305903 Test Loss: 0.4163077
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3962406
	speed: 0.0714s/iter; left time: 1061.1601s
	iters: 200, epoch: 43 | loss: 0.3577910
	speed: 0.0169s/iter; left time: 248.9877s
Epoch: 43 cost time: 4.80384087562561
Epoch: 43, Steps: 258 | Train Loss: 0.3961868 Vali Loss: 0.9308343 Test Loss: 0.4161431
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3728496
	speed: 0.0705s/iter; left time: 1029.7000s
	iters: 200, epoch: 44 | loss: 0.4148114
	speed: 0.0166s/iter; left time: 240.8808s
Epoch: 44 cost time: 4.810745000839233
Epoch: 44, Steps: 258 | Train Loss: 0.3963227 Vali Loss: 0.9301751 Test Loss: 0.4161741
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4381000
	speed: 0.0693s/iter; left time: 994.0648s
	iters: 200, epoch: 45 | loss: 0.3888519
	speed: 0.0167s/iter; left time: 238.2045s
Epoch: 45 cost time: 4.80651593208313
Epoch: 45, Steps: 258 | Train Loss: 0.3961531 Vali Loss: 0.9307639 Test Loss: 0.4164262
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4154137969017029, mae:0.41158026456832886, rse:0.6132128238677979, corr:[0.5241387  0.5309809  0.53346866 0.53415334 0.53521967 0.5370348
 0.53856754 0.5391043  0.53915346 0.5394037  0.5401334  0.5409652
 0.54152375 0.5415921  0.54119533 0.54039544 0.5392975  0.53810567
 0.5369211  0.53581727 0.5347337  0.5335288  0.5321848  0.5309994
 0.5298712  0.52893716 0.5280994  0.52722776 0.52664346 0.5265655
 0.52703434 0.5279199  0.52870584 0.5291339  0.52903104 0.5289899
 0.5291189  0.5293703  0.5294087  0.5289651  0.5283073  0.52769566
 0.5273975  0.5274287  0.5273971  0.5271633  0.5270042  0.527109
 0.5273826  0.5275463  0.52754    0.52736014 0.5272396  0.52721
 0.5273004  0.52735204 0.52723855 0.52692056 0.52662677 0.526372
 0.52615833 0.52586997 0.5254776  0.5250226  0.5248535  0.52509356
 0.5255156  0.5258097  0.5258853  0.52580637 0.5258358  0.52601475
 0.52622753 0.5263102  0.52627033 0.52610517 0.525871   0.52564603
 0.5254217  0.52519196 0.5249811  0.5248349  0.5248037  0.52479154
 0.524791   0.52468866 0.5245405  0.5244245  0.5244748  0.52478886
 0.52523327 0.52558804 0.5257381  0.525688   0.5255657  0.52543634
 0.52537644 0.5253951  0.5252429  0.5249424  0.52454185 0.5241598
 0.52391225 0.52376515 0.52371895 0.5237591  0.5237038  0.5235822
 0.52338195 0.5232014  0.52303547 0.5227846  0.52239525 0.52192533
 0.5214694  0.52117485 0.52108616 0.52108586 0.52101743 0.52082247
 0.52047515 0.52009636 0.5198231  0.51982445 0.5199815  0.51994354
 0.5195483  0.51901203 0.5186632  0.51877344 0.51915497 0.51941913
 0.51932853 0.5188691  0.51825905 0.51790506 0.5180061  0.51824576
 0.5184142  0.51847464 0.5184901  0.51868135 0.5190677  0.51943094
 0.51953065 0.5192559  0.5188374  0.5185346  0.518516   0.51864064
 0.5187491  0.5188671  0.5189903  0.5191209  0.51921505 0.5193023
 0.5193168  0.5192388  0.51917577 0.51910365 0.5191085  0.51918095
 0.5193453  0.5196113  0.5199566  0.5202674  0.520427   0.52041245
 0.5202703  0.52013457 0.5202096  0.5204739  0.52067906 0.5207071
 0.52060694 0.52052444 0.5205792  0.5207607  0.52091527 0.5209319
 0.5207789  0.52061063 0.52058065 0.5208202  0.5212458  0.5216545
 0.5218892  0.5220007  0.52210456 0.52225125 0.5224256  0.5225434
 0.52253443 0.52238744 0.52203506 0.52150273 0.5208544  0.52016866
 0.51943934 0.5186747  0.5178552  0.5170464  0.5162778  0.5155933
 0.51498455 0.5143849  0.5137251  0.5130445  0.5123733  0.51174456
 0.51113445 0.5105674  0.5099888  0.5093676  0.5086666  0.50806355
 0.50768375 0.5074571  0.5073209  0.5071554  0.5069696  0.50687474
 0.5069813  0.5071187  0.5071991  0.5071422  0.5068909  0.5065758
 0.50639945 0.50648934 0.5067838  0.50705075 0.5071711  0.5071674
 0.50715536 0.50730646 0.5075565  0.50794625 0.5082795  0.50850207
 0.50855094 0.50840306 0.5082068  0.5080808  0.5080674  0.5080725
 0.5080431  0.5079899  0.5079132  0.50785893 0.5077819  0.50774586
 0.50766206 0.50761247 0.5075576  0.50749576 0.5074569  0.50747913
 0.50755703 0.507656   0.5077057  0.5077018  0.50770605 0.5078795
 0.5081393  0.5084136  0.50859857 0.5086678  0.5086152  0.508467
 0.5083426  0.5082716  0.50825936 0.50826436 0.5082361  0.5081786
 0.50807625 0.5080656  0.5081155  0.50827396 0.5084351  0.50852704
 0.50862116 0.50870705 0.5087768  0.5087942  0.5086439  0.5082579
 0.50770056 0.50714684 0.5066885  0.506286   0.5058855  0.50539196
 0.5047692  0.5040084  0.50330925 0.5027679  0.50239563 0.5021213
 0.5017905  0.50137734 0.5009549  0.5005789  0.50027394 0.49998024
 0.49974522 0.49945852 0.4991288  0.498812   0.49860844 0.49845225
 0.4982833  0.49811438 0.49791002 0.49775958 0.49775586 0.49793994
 0.49817783 0.49826473 0.49814382 0.49789947 0.49767467 0.49755806
 0.49753416 0.49755764 0.49749407 0.49737555 0.49727562 0.49727884
 0.4974517  0.4976325  0.49764338 0.49749655 0.49728495 0.4972257
 0.49728388 0.49724007 0.4971295  0.4969765  0.49688342 0.49689582
 0.496963   0.49707752 0.49712276 0.49706993 0.49695745 0.49687013
 0.49677855 0.49673176 0.49670923 0.4966668  0.49664497 0.49662203
 0.49654314 0.49644133 0.49633422 0.49626315 0.49630308 0.49643707
 0.4965368  0.49662066 0.4966524  0.4966104  0.496505   0.49635738
 0.49612892 0.49592936 0.49578592 0.4956446  0.495552   0.49553117
 0.49560001 0.4957689  0.49599424 0.4962528  0.49647883 0.49670035
 0.4969519  0.4972705  0.49765164 0.4980317  0.49833685 0.4985021
 0.4984443  0.49813968 0.49765155 0.4970535  0.49644765 0.49605143
 0.49584025 0.49571532 0.4955368  0.49526548 0.4948358  0.49433798
 0.49386328 0.49348265 0.49317595 0.49285635 0.492425   0.4919947
 0.4916005  0.49135646 0.49117377 0.4910334  0.49086994 0.49071163
 0.49054685 0.49042383 0.49036044 0.49040577 0.49059165 0.4908205
 0.49111688 0.49123317 0.49119473 0.49097532 0.4907442  0.490664
 0.490684   0.49081135 0.49090293 0.49093685 0.49089086 0.49086693
 0.49084628 0.49085236 0.49083033 0.4908904  0.49096316 0.49108663
 0.49109125 0.49089077 0.4906123  0.4904209  0.49036735 0.49040896
 0.49042833 0.49036035 0.49023637 0.4901175  0.49008685 0.49017063
 0.49029642 0.4904307  0.4905004  0.4904739  0.49035195 0.49013555
 0.48996234 0.48989558 0.48998556 0.49018118 0.49040672 0.49063814
 0.49070242 0.49062622 0.49046934 0.4903675  0.49030632 0.4903562
 0.4904495  0.4905521  0.4905167  0.49035516 0.4901198  0.4898941
 0.4898121  0.48986858 0.49005428 0.49026102 0.4903908  0.49038368
 0.4902376  0.4900899  0.48999602 0.48989588 0.48974127 0.48952422
 0.4892292  0.4888559  0.48838842 0.48780417 0.48714298 0.4865154
 0.48599902 0.48546234 0.484884   0.48429134 0.48366416 0.4830545
 0.48251495 0.4820892  0.48170024 0.48125362 0.48074687 0.48028
 0.4798688  0.47957256 0.47934532 0.47902045 0.47874445 0.47849113
 0.47829863 0.4781278  0.47794864 0.47788197 0.47796604 0.47823942
 0.4786315  0.47895843 0.47914976 0.47930467 0.47940096 0.4794451
 0.47941852 0.47943947 0.47957793 0.47978148 0.48001364 0.48025057
 0.4804261  0.4805181  0.48054698 0.4805482  0.4806671  0.4809319
 0.48117822 0.4812447  0.48108226 0.48083442 0.48062408 0.4804998
 0.48049852 0.4805891  0.4807313  0.48085466 0.48090035 0.48087302
 0.4808279  0.48079354 0.48075798 0.48063904 0.4803866  0.48010302
 0.47989824 0.47990718 0.48014668 0.48044595 0.4806438  0.480736
 0.4806613  0.48055682 0.48056495 0.480726   0.4809067  0.48102173
 0.48099968 0.48088512 0.4807355  0.48061302 0.4805479  0.48052529
 0.48052537 0.48064762 0.48074177 0.48077393 0.48070717 0.4805994
 0.48055232 0.48064452 0.48090446 0.48114714 0.4811719  0.48088804
 0.48038092 0.47980505 0.47919607 0.47853172 0.47783718 0.47728214
 0.4769021  0.4766924  0.47652134 0.47623962 0.47576877 0.47507238
 0.47431913 0.47362143 0.4730432  0.47252387 0.4720547  0.47160518
 0.47120014 0.47085848 0.47062376 0.47043282 0.47019675 0.46990293
 0.46966544 0.46949863 0.4694581  0.4695337  0.46972552 0.46995318
 0.4702483  0.47043937 0.4704477  0.47032022 0.47019836 0.47029474
 0.47062212 0.47095823 0.47118297 0.471191   0.4710974  0.47106832
 0.4712356  0.47153535 0.4716861  0.47164038 0.47164568 0.47185802
 0.47219583 0.47254556 0.47270906 0.4726147  0.4724183  0.47227818
 0.47222018 0.4723482  0.47259635 0.47275713 0.47280142 0.47261295
 0.47217196 0.47164753 0.47123066 0.47091907 0.4708434  0.47090843
 0.47097507 0.4708931  0.47063264 0.47029844 0.47003478 0.47000417
 0.47014078 0.47025084 0.47033057 0.47035766 0.47031847 0.47024542
 0.4701845  0.4701571  0.47014412 0.4700946  0.46998012 0.4698311
 0.46973574 0.46982825 0.47002017 0.47011384 0.4701492  0.47018462
 0.47035518 0.47070345 0.47114223 0.47149408 0.47152647 0.47120708
 0.47070226 0.47028208 0.46997982 0.46964395 0.46920326 0.46873057
 0.46836424 0.46808228 0.46786296 0.46749353 0.46678993 0.46612602
 0.46562445 0.4655626  0.4657039  0.46577135 0.46546903 0.46482912
 0.4642374  0.46397907 0.46410042 0.4641753  0.46402135 0.46363363
 0.46317136 0.46299824 0.4631257  0.46339995 0.46370855 0.46379206
 0.4638214  0.46390587 0.4641596  0.46463725 0.46495917 0.4650596
 0.4650584  0.46518946 0.46559417 0.46617514 0.466846   0.46738842
 0.4679532  0.46851406 0.4687797  0.46892193 0.4685642  0.46471462]
