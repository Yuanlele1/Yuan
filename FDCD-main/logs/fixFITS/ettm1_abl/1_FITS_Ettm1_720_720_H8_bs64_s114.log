Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33121
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=74, out_features=148, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9812992.0
params:  11100.0
Trainable parameters:  11100
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5196680
	speed: 0.0432s/iter; left time: 1111.4966s
	iters: 200, epoch: 1 | loss: 0.4667911
	speed: 0.0320s/iter; left time: 819.4149s
Epoch: 1 cost time: 9.712927103042603
Epoch: 1, Steps: 258 | Train Loss: 0.5176355 Vali Loss: 1.0206122 Test Loss: 0.4514727
Validation loss decreased (inf --> 1.020612).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4174649
	speed: 0.1458s/iter; left time: 3710.3140s
	iters: 200, epoch: 2 | loss: 0.4121216
	speed: 0.0348s/iter; left time: 882.8271s
Epoch: 2 cost time: 9.26722502708435
Epoch: 2, Steps: 258 | Train Loss: 0.4166985 Vali Loss: 0.9676015 Test Loss: 0.4210175
Validation loss decreased (1.020612 --> 0.967601).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3875801
	speed: 0.1486s/iter; left time: 3743.5407s
	iters: 200, epoch: 3 | loss: 0.3956549
	speed: 0.0339s/iter; left time: 849.9769s
Epoch: 3 cost time: 9.562713384628296
Epoch: 3, Steps: 258 | Train Loss: 0.4042156 Vali Loss: 0.9514045 Test Loss: 0.4150321
Validation loss decreased (0.967601 --> 0.951404).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4018069
	speed: 0.1442s/iter; left time: 3595.2423s
	iters: 200, epoch: 4 | loss: 0.3505565
	speed: 0.0335s/iter; left time: 832.7637s
Epoch: 4 cost time: 9.211811542510986
Epoch: 4, Steps: 258 | Train Loss: 0.4003555 Vali Loss: 0.9460184 Test Loss: 0.4149036
Validation loss decreased (0.951404 --> 0.946018).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4199849
	speed: 0.1461s/iter; left time: 3603.4311s
	iters: 200, epoch: 5 | loss: 0.3736421
	speed: 0.0340s/iter; left time: 836.3117s
Epoch: 5 cost time: 9.54774808883667
Epoch: 5, Steps: 258 | Train Loss: 0.3992618 Vali Loss: 0.9404233 Test Loss: 0.4157246
Validation loss decreased (0.946018 --> 0.940423).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3788017
	speed: 0.1438s/iter; left time: 3510.0387s
	iters: 200, epoch: 6 | loss: 0.4061353
	speed: 0.0330s/iter; left time: 801.8288s
Epoch: 6 cost time: 9.264555931091309
Epoch: 6, Steps: 258 | Train Loss: 0.3986971 Vali Loss: 0.9398476 Test Loss: 0.4157449
Validation loss decreased (0.940423 --> 0.939848).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4119321
	speed: 0.1534s/iter; left time: 3705.1738s
	iters: 200, epoch: 7 | loss: 0.3948644
	speed: 0.0342s/iter; left time: 823.5649s
Epoch: 7 cost time: 10.175259590148926
Epoch: 7, Steps: 258 | Train Loss: 0.3982357 Vali Loss: 0.9368612 Test Loss: 0.4160545
Validation loss decreased (0.939848 --> 0.936861).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4190599
	speed: 0.1648s/iter; left time: 3936.7939s
	iters: 200, epoch: 8 | loss: 0.3847299
	speed: 0.0355s/iter; left time: 845.8107s
Epoch: 8 cost time: 10.140222787857056
Epoch: 8, Steps: 258 | Train Loss: 0.3980601 Vali Loss: 0.9358142 Test Loss: 0.4166324
Validation loss decreased (0.936861 --> 0.935814).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.3960741
	speed: 0.1571s/iter; left time: 3713.6866s
	iters: 200, epoch: 9 | loss: 0.4480030
	speed: 0.0324s/iter; left time: 763.2782s
Epoch: 9 cost time: 9.885690689086914
Epoch: 9, Steps: 258 | Train Loss: 0.3980240 Vali Loss: 0.9359609 Test Loss: 0.4164754
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3490745
	speed: 0.1536s/iter; left time: 3591.9265s
	iters: 200, epoch: 10 | loss: 0.4390539
	speed: 0.0340s/iter; left time: 791.7482s
Epoch: 10 cost time: 9.927631378173828
Epoch: 10, Steps: 258 | Train Loss: 0.3978440 Vali Loss: 0.9354459 Test Loss: 0.4166700
Validation loss decreased (0.935814 --> 0.935446).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.3847266
	speed: 0.1589s/iter; left time: 3673.4107s
	iters: 200, epoch: 11 | loss: 0.3752772
	speed: 0.0323s/iter; left time: 744.2913s
Epoch: 11 cost time: 9.235964059829712
Epoch: 11, Steps: 258 | Train Loss: 0.3976648 Vali Loss: 0.9348977 Test Loss: 0.4164940
Validation loss decreased (0.935446 --> 0.934898).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4123138
	speed: 0.1504s/iter; left time: 3438.7261s
	iters: 200, epoch: 12 | loss: 0.3995904
	speed: 0.0369s/iter; left time: 839.7570s
Epoch: 12 cost time: 10.039589405059814
Epoch: 12, Steps: 258 | Train Loss: 0.3976781 Vali Loss: 0.9342752 Test Loss: 0.4167763
Validation loss decreased (0.934898 --> 0.934275).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4063691
	speed: 0.1596s/iter; left time: 3607.2990s
	iters: 200, epoch: 13 | loss: 0.4343771
	speed: 0.0359s/iter; left time: 808.9641s
Epoch: 13 cost time: 10.360258102416992
Epoch: 13, Steps: 258 | Train Loss: 0.3976626 Vali Loss: 0.9340647 Test Loss: 0.4168991
Validation loss decreased (0.934275 --> 0.934065).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4053531
	speed: 0.1613s/iter; left time: 3605.0706s
	iters: 200, epoch: 14 | loss: 0.3834405
	speed: 0.0336s/iter; left time: 748.2225s
Epoch: 14 cost time: 9.140849828720093
Epoch: 14, Steps: 258 | Train Loss: 0.3975672 Vali Loss: 0.9331724 Test Loss: 0.4168690
Validation loss decreased (0.934065 --> 0.933172).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3921180
	speed: 0.1606s/iter; left time: 3546.8175s
	iters: 200, epoch: 15 | loss: 0.3754312
	speed: 0.0303s/iter; left time: 666.9363s
Epoch: 15 cost time: 9.17750358581543
Epoch: 15, Steps: 258 | Train Loss: 0.3975366 Vali Loss: 0.9336014 Test Loss: 0.4169332
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4267816
	speed: 0.1697s/iter; left time: 3703.7423s
	iters: 200, epoch: 16 | loss: 0.4057802
	speed: 0.0327s/iter; left time: 710.1906s
Epoch: 16 cost time: 9.630905866622925
Epoch: 16, Steps: 258 | Train Loss: 0.3974452 Vali Loss: 0.9332417 Test Loss: 0.4169562
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4076124
	speed: 0.1512s/iter; left time: 3262.2480s
	iters: 200, epoch: 17 | loss: 0.4106629
	speed: 0.0375s/iter; left time: 805.0851s
Epoch: 17 cost time: 10.504292011260986
Epoch: 17, Steps: 258 | Train Loss: 0.3974432 Vali Loss: 0.9323032 Test Loss: 0.4167919
Validation loss decreased (0.933172 --> 0.932303).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.3789071
	speed: 0.1584s/iter; left time: 3376.0875s
	iters: 200, epoch: 18 | loss: 0.3771601
	speed: 0.0380s/iter; left time: 805.4548s
Epoch: 18 cost time: 10.552907228469849
Epoch: 18, Steps: 258 | Train Loss: 0.3973831 Vali Loss: 0.9330792 Test Loss: 0.4168102
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4074772
	speed: 0.1625s/iter; left time: 3422.6378s
	iters: 200, epoch: 19 | loss: 0.3789864
	speed: 0.0389s/iter; left time: 816.2126s
Epoch: 19 cost time: 11.342309713363647
Epoch: 19, Steps: 258 | Train Loss: 0.3973709 Vali Loss: 0.9329987 Test Loss: 0.4166093
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.3783822
	speed: 0.1702s/iter; left time: 3539.3687s
	iters: 200, epoch: 20 | loss: 0.4166204
	speed: 0.0362s/iter; left time: 749.4865s
Epoch: 20 cost time: 9.858718633651733
Epoch: 20, Steps: 258 | Train Loss: 0.3972140 Vali Loss: 0.9332497 Test Loss: 0.4165465
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4250000
	speed: 0.1695s/iter; left time: 3481.7427s
	iters: 200, epoch: 21 | loss: 0.3964033
	speed: 0.0366s/iter; left time: 747.2763s
Epoch: 21 cost time: 10.037692785263062
Epoch: 21, Steps: 258 | Train Loss: 0.3972770 Vali Loss: 0.9326529 Test Loss: 0.4168737
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.3752448
	speed: 0.1739s/iter; left time: 3527.0302s
	iters: 200, epoch: 22 | loss: 0.4036723
	speed: 0.0346s/iter; left time: 698.8245s
Epoch: 22 cost time: 10.571908712387085
Epoch: 22, Steps: 258 | Train Loss: 0.3974220 Vali Loss: 0.9330766 Test Loss: 0.4166141
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4023841
	speed: 0.1611s/iter; left time: 3226.7714s
	iters: 200, epoch: 23 | loss: 0.3811596
	speed: 0.0381s/iter; left time: 758.3633s
Epoch: 23 cost time: 10.75365924835205
Epoch: 23, Steps: 258 | Train Loss: 0.3972984 Vali Loss: 0.9326361 Test Loss: 0.4167039
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3811318
	speed: 0.1512s/iter; left time: 2989.7112s
	iters: 200, epoch: 24 | loss: 0.4093972
	speed: 0.0362s/iter; left time: 711.5498s
Epoch: 24 cost time: 10.598727464675903
Epoch: 24, Steps: 258 | Train Loss: 0.3972545 Vali Loss: 0.9324028 Test Loss: 0.4170696
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3628721
	speed: 0.1559s/iter; left time: 3041.4726s
	iters: 200, epoch: 25 | loss: 0.3994403
	speed: 0.0391s/iter; left time: 758.3143s
Epoch: 25 cost time: 10.411481380462646
Epoch: 25, Steps: 258 | Train Loss: 0.3971918 Vali Loss: 0.9316946 Test Loss: 0.4169282
Validation loss decreased (0.932303 --> 0.931695).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.3792259
	speed: 0.1508s/iter; left time: 2903.2170s
	iters: 200, epoch: 26 | loss: 0.4142187
	speed: 0.0379s/iter; left time: 724.9304s
Epoch: 26 cost time: 10.752854585647583
Epoch: 26, Steps: 258 | Train Loss: 0.3972921 Vali Loss: 0.9329343 Test Loss: 0.4168684
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3767049
	speed: 0.1663s/iter; left time: 3158.8500s
	iters: 200, epoch: 27 | loss: 0.3543023
	speed: 0.0369s/iter; left time: 696.4132s
Epoch: 27 cost time: 10.001936912536621
Epoch: 27, Steps: 258 | Train Loss: 0.3971582 Vali Loss: 0.9330280 Test Loss: 0.4168954
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4109490
	speed: 0.1573s/iter; left time: 2947.5921s
	iters: 200, epoch: 28 | loss: 0.4214374
	speed: 0.0355s/iter; left time: 661.0816s
Epoch: 28 cost time: 9.519724130630493
Epoch: 28, Steps: 258 | Train Loss: 0.3972734 Vali Loss: 0.9323023 Test Loss: 0.4169523
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.3784261
	speed: 0.1452s/iter; left time: 2682.3200s
	iters: 200, epoch: 29 | loss: 0.3944302
	speed: 0.0398s/iter; left time: 731.5295s
Epoch: 29 cost time: 10.7423255443573
Epoch: 29, Steps: 258 | Train Loss: 0.3972240 Vali Loss: 0.9327058 Test Loss: 0.4169986
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4300629
	speed: 0.1604s/iter; left time: 2921.7012s
	iters: 200, epoch: 30 | loss: 0.4344100
	speed: 0.0447s/iter; left time: 809.5846s
Epoch: 30 cost time: 11.50588345527649
Epoch: 30, Steps: 258 | Train Loss: 0.3972050 Vali Loss: 0.9318134 Test Loss: 0.4168215
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4284801
	speed: 0.1605s/iter; left time: 2883.3975s
	iters: 200, epoch: 31 | loss: 0.4548613
	speed: 0.0392s/iter; left time: 699.4705s
Epoch: 31 cost time: 11.125102758407593
Epoch: 31, Steps: 258 | Train Loss: 0.3970596 Vali Loss: 0.9323602 Test Loss: 0.4165073
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.3702252
	speed: 0.1492s/iter; left time: 2640.8740s
	iters: 200, epoch: 32 | loss: 0.4219908
	speed: 0.0420s/iter; left time: 739.2380s
Epoch: 32 cost time: 10.848235368728638
Epoch: 32, Steps: 258 | Train Loss: 0.3972305 Vali Loss: 0.9324206 Test Loss: 0.4169280
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4263842
	speed: 0.1504s/iter; left time: 2623.8246s
	iters: 200, epoch: 33 | loss: 0.3795678
	speed: 0.0339s/iter; left time: 587.8055s
Epoch: 33 cost time: 9.36907148361206
Epoch: 33, Steps: 258 | Train Loss: 0.3970128 Vali Loss: 0.9315511 Test Loss: 0.4166771
Validation loss decreased (0.931695 --> 0.931551).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4214121
	speed: 0.1525s/iter; left time: 2620.3373s
	iters: 200, epoch: 34 | loss: 0.3977451
	speed: 0.0393s/iter; left time: 670.8874s
Epoch: 34 cost time: 10.667607069015503
Epoch: 34, Steps: 258 | Train Loss: 0.3972094 Vali Loss: 0.9320779 Test Loss: 0.4168666
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3983237
	speed: 0.1555s/iter; left time: 2633.2095s
	iters: 200, epoch: 35 | loss: 0.3992514
	speed: 0.0394s/iter; left time: 663.7520s
Epoch: 35 cost time: 10.647010564804077
Epoch: 35, Steps: 258 | Train Loss: 0.3971556 Vali Loss: 0.9315631 Test Loss: 0.4169666
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.3933507
	speed: 0.1557s/iter; left time: 2595.5855s
	iters: 200, epoch: 36 | loss: 0.4156719
	speed: 0.0418s/iter; left time: 692.7983s
Epoch: 36 cost time: 11.977037191390991
Epoch: 36, Steps: 258 | Train Loss: 0.3972137 Vali Loss: 0.9319011 Test Loss: 0.4169664
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3756413
	speed: 0.1757s/iter; left time: 2883.4600s
	iters: 200, epoch: 37 | loss: 0.4056172
	speed: 0.0347s/iter; left time: 565.9156s
Epoch: 37 cost time: 9.592581510543823
Epoch: 37, Steps: 258 | Train Loss: 0.3970096 Vali Loss: 0.9326159 Test Loss: 0.4169144
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.3826657
	speed: 0.1519s/iter; left time: 2453.9110s
	iters: 200, epoch: 38 | loss: 0.3972563
	speed: 0.0389s/iter; left time: 624.3452s
Epoch: 38 cost time: 10.44565463066101
Epoch: 38, Steps: 258 | Train Loss: 0.3969723 Vali Loss: 0.9321504 Test Loss: 0.4169599
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4130888
	speed: 0.1525s/iter; left time: 2424.7550s
	iters: 200, epoch: 39 | loss: 0.3860137
	speed: 0.0385s/iter; left time: 607.7939s
Epoch: 39 cost time: 10.69513750076294
Epoch: 39, Steps: 258 | Train Loss: 0.3972198 Vali Loss: 0.9315858 Test Loss: 0.4169529
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.3809833
	speed: 0.1645s/iter; left time: 2573.2189s
	iters: 200, epoch: 40 | loss: 0.3747838
	speed: 0.0408s/iter; left time: 634.4882s
Epoch: 40 cost time: 10.872966766357422
Epoch: 40, Steps: 258 | Train Loss: 0.3971378 Vali Loss: 0.9317375 Test Loss: 0.4169264
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.3920020
	speed: 0.1554s/iter; left time: 2390.9473s
	iters: 200, epoch: 41 | loss: 0.4201294
	speed: 0.0379s/iter; left time: 579.2224s
Epoch: 41 cost time: 10.560200452804565
Epoch: 41, Steps: 258 | Train Loss: 0.3971160 Vali Loss: 0.9315231 Test Loss: 0.4168783
Validation loss decreased (0.931551 --> 0.931523).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4015222
	speed: 0.1568s/iter; left time: 2371.1524s
	iters: 200, epoch: 42 | loss: 0.3900669
	speed: 0.0369s/iter; left time: 554.6943s
Epoch: 42 cost time: 10.192438840866089
Epoch: 42, Steps: 258 | Train Loss: 0.3971059 Vali Loss: 0.9327545 Test Loss: 0.4170501
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4012885
	speed: 0.1614s/iter; left time: 2399.9154s
	iters: 200, epoch: 43 | loss: 0.3821055
	speed: 0.0500s/iter; left time: 737.8893s
Epoch: 43 cost time: 12.323095321655273
Epoch: 43, Steps: 258 | Train Loss: 0.3969897 Vali Loss: 0.9315296 Test Loss: 0.4169074
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.3637903
	speed: 0.1692s/iter; left time: 2471.8821s
	iters: 200, epoch: 44 | loss: 0.4000810
	speed: 0.0370s/iter; left time: 536.0376s
Epoch: 44 cost time: 10.589592456817627
Epoch: 44, Steps: 258 | Train Loss: 0.3970046 Vali Loss: 0.9311286 Test Loss: 0.4167780
Validation loss decreased (0.931523 --> 0.931129).  Saving model ...
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.3636788
	speed: 0.1582s/iter; left time: 2270.6893s
	iters: 200, epoch: 45 | loss: 0.3682224
	speed: 0.0342s/iter; left time: 486.6279s
Epoch: 45 cost time: 10.712144136428833
Epoch: 45, Steps: 258 | Train Loss: 0.3970189 Vali Loss: 0.9312629 Test Loss: 0.4168584
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.3796723
	speed: 0.1617s/iter; left time: 2278.0755s
	iters: 200, epoch: 46 | loss: 0.3857168
	speed: 0.0398s/iter; left time: 557.2991s
Epoch: 46 cost time: 11.157174825668335
Epoch: 46, Steps: 258 | Train Loss: 0.3971420 Vali Loss: 0.9324293 Test Loss: 0.4170036
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.4396811
	speed: 0.1532s/iter; left time: 2119.4045s
	iters: 200, epoch: 47 | loss: 0.4272005
	speed: 0.0367s/iter; left time: 504.3641s
Epoch: 47 cost time: 10.464915037155151
Epoch: 47, Steps: 258 | Train Loss: 0.3971256 Vali Loss: 0.9315673 Test Loss: 0.4169736
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4054653
	speed: 0.1525s/iter; left time: 2069.6389s
	iters: 200, epoch: 48 | loss: 0.4721876
	speed: 0.0367s/iter; left time: 495.0929s
Epoch: 48 cost time: 10.437469005584717
Epoch: 48, Steps: 258 | Train Loss: 0.3970034 Vali Loss: 0.9316912 Test Loss: 0.4169144
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4259051
	speed: 0.1490s/iter; left time: 1984.7176s
	iters: 200, epoch: 49 | loss: 0.4144380
	speed: 0.0366s/iter; left time: 483.7704s
Epoch: 49 cost time: 10.30641794204712
Epoch: 49, Steps: 258 | Train Loss: 0.3971029 Vali Loss: 0.9316724 Test Loss: 0.4168281
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.3993031
	speed: 0.1528s/iter; left time: 1995.5188s
	iters: 200, epoch: 50 | loss: 0.4076801
	speed: 0.0355s/iter; left time: 460.5201s
Epoch: 50 cost time: 10.392885208129883
Epoch: 50, Steps: 258 | Train Loss: 0.3970388 Vali Loss: 0.9314280 Test Loss: 0.4169783
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.4054978
	speed: 0.1636s/iter; left time: 2093.7074s
	iters: 200, epoch: 51 | loss: 0.4141757
	speed: 0.0376s/iter; left time: 477.8530s
Epoch: 51 cost time: 11.425951719284058
Epoch: 51, Steps: 258 | Train Loss: 0.3970089 Vali Loss: 0.9323533 Test Loss: 0.4169059
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.3806919
	speed: 0.1592s/iter; left time: 1996.9443s
	iters: 200, epoch: 52 | loss: 0.4272448
	speed: 0.0340s/iter; left time: 423.4430s
Epoch: 52 cost time: 9.287637710571289
Epoch: 52, Steps: 258 | Train Loss: 0.3968899 Vali Loss: 0.9320963 Test Loss: 0.4169454
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.4064790
	speed: 0.1413s/iter; left time: 1735.4877s
	iters: 200, epoch: 53 | loss: 0.3746305
	speed: 0.0320s/iter; left time: 390.1384s
Epoch: 53 cost time: 8.954493284225464
Epoch: 53, Steps: 258 | Train Loss: 0.3970874 Vali Loss: 0.9321679 Test Loss: 0.4168948
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.4207936
	speed: 0.1543s/iter; left time: 1855.6270s
	iters: 200, epoch: 54 | loss: 0.3752894
	speed: 0.0336s/iter; left time: 401.2111s
Epoch: 54 cost time: 9.746506929397583
Epoch: 54, Steps: 258 | Train Loss: 0.3970839 Vali Loss: 0.9316674 Test Loss: 0.4168845
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.3576298
	speed: 0.1579s/iter; left time: 1858.4450s
	iters: 200, epoch: 55 | loss: 0.3852702
	speed: 0.0396s/iter; left time: 462.3252s
Epoch: 55 cost time: 11.086160659790039
Epoch: 55, Steps: 258 | Train Loss: 0.3968874 Vali Loss: 0.9319726 Test Loss: 0.4168918
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.4007693
	speed: 0.1468s/iter; left time: 1689.4952s
	iters: 200, epoch: 56 | loss: 0.3816929
	speed: 0.0360s/iter; left time: 410.9809s
Epoch: 56 cost time: 9.82446837425232
Epoch: 56, Steps: 258 | Train Loss: 0.3969776 Vali Loss: 0.9317837 Test Loss: 0.4168964
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.4020224
	speed: 0.1580s/iter; left time: 1777.7414s
	iters: 200, epoch: 57 | loss: 0.4277106
	speed: 0.0419s/iter; left time: 467.4908s
Epoch: 57 cost time: 11.331448316574097
Epoch: 57, Steps: 258 | Train Loss: 0.3971181 Vali Loss: 0.9320873 Test Loss: 0.4169151
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.3981210
	speed: 0.1651s/iter; left time: 1815.5287s
	iters: 200, epoch: 58 | loss: 0.4102337
	speed: 0.0342s/iter; left time: 373.0232s
Epoch: 58 cost time: 9.288326263427734
Epoch: 58, Steps: 258 | Train Loss: 0.3968641 Vali Loss: 0.9312696 Test Loss: 0.4169325
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.3577434
	speed: 0.1641s/iter; left time: 1762.2379s
	iters: 200, epoch: 59 | loss: 0.3879740
	speed: 0.0378s/iter; left time: 401.6886s
Epoch: 59 cost time: 10.921837568283081
Epoch: 59, Steps: 258 | Train Loss: 0.3969307 Vali Loss: 0.9323559 Test Loss: 0.4168676
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.4058948
	speed: 0.1599s/iter; left time: 1675.4241s
	iters: 200, epoch: 60 | loss: 0.4382022
	speed: 0.0341s/iter; left time: 353.4924s
Epoch: 60 cost time: 9.915602922439575
Epoch: 60, Steps: 258 | Train Loss: 0.3969308 Vali Loss: 0.9313517 Test Loss: 0.4169319
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.4095436
	speed: 0.1486s/iter; left time: 1518.4061s
	iters: 200, epoch: 61 | loss: 0.3712558
	speed: 0.0356s/iter; left time: 360.1627s
Epoch: 61 cost time: 9.990351915359497
Epoch: 61, Steps: 258 | Train Loss: 0.3968016 Vali Loss: 0.9321936 Test Loss: 0.4169493
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.4056490
	speed: 0.1565s/iter; left time: 1558.8971s
	iters: 200, epoch: 62 | loss: 0.3915380
	speed: 0.0423s/iter; left time: 417.4546s
Epoch: 62 cost time: 10.657155990600586
Epoch: 62, Steps: 258 | Train Loss: 0.3971211 Vali Loss: 0.9310838 Test Loss: 0.4169579
Validation loss decreased (0.931129 --> 0.931084).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.4053710
	speed: 0.1639s/iter; left time: 1590.5151s
	iters: 200, epoch: 63 | loss: 0.3943888
	speed: 0.0317s/iter; left time: 304.3919s
Epoch: 63 cost time: 9.60103702545166
Epoch: 63, Steps: 258 | Train Loss: 0.3969792 Vali Loss: 0.9311311 Test Loss: 0.4169629
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.3914680
	speed: 0.1505s/iter; left time: 1421.9242s
	iters: 200, epoch: 64 | loss: 0.4102129
	speed: 0.0400s/iter; left time: 374.2050s
Epoch: 64 cost time: 10.503383874893188
Epoch: 64, Steps: 258 | Train Loss: 0.3970540 Vali Loss: 0.9317537 Test Loss: 0.4169300
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.4240656
	speed: 0.1545s/iter; left time: 1420.0247s
	iters: 200, epoch: 65 | loss: 0.3848820
	speed: 0.0433s/iter; left time: 393.6658s
Epoch: 65 cost time: 11.870237350463867
Epoch: 65, Steps: 258 | Train Loss: 0.3969498 Vali Loss: 0.9311326 Test Loss: 0.4169502
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.4076307
	speed: 0.1604s/iter; left time: 1432.9699s
	iters: 200, epoch: 66 | loss: 0.3793345
	speed: 0.0405s/iter; left time: 357.9372s
Epoch: 66 cost time: 10.208811521530151
Epoch: 66, Steps: 258 | Train Loss: 0.3970346 Vali Loss: 0.9311054 Test Loss: 0.4169023
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.3580027
	speed: 0.1569s/iter; left time: 1360.8016s
	iters: 200, epoch: 67 | loss: 0.4161206
	speed: 0.0353s/iter; left time: 302.8240s
Epoch: 67 cost time: 9.489733934402466
Epoch: 67, Steps: 258 | Train Loss: 0.3968922 Vali Loss: 0.9323797 Test Loss: 0.4169143
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.3862643
	speed: 0.1543s/iter; left time: 1298.7524s
	iters: 200, epoch: 68 | loss: 0.4344997
	speed: 0.0353s/iter; left time: 293.1994s
Epoch: 68 cost time: 9.649878025054932
Epoch: 68, Steps: 258 | Train Loss: 0.3969328 Vali Loss: 0.9317253 Test Loss: 0.4169116
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.4256624
	speed: 0.1597s/iter; left time: 1302.6036s
	iters: 200, epoch: 69 | loss: 0.4237506
	speed: 0.0383s/iter; left time: 308.3234s
Epoch: 69 cost time: 10.3994779586792
Epoch: 69, Steps: 258 | Train Loss: 0.3969634 Vali Loss: 0.9322671 Test Loss: 0.4169089
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.3704141
	speed: 0.1490s/iter; left time: 1176.7082s
	iters: 200, epoch: 70 | loss: 0.4211783
	speed: 0.0345s/iter; left time: 268.9422s
Epoch: 70 cost time: 9.48688793182373
Epoch: 70, Steps: 258 | Train Loss: 0.3969881 Vali Loss: 0.9318208 Test Loss: 0.4169089
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.3991743
	speed: 0.1689s/iter; left time: 1290.8863s
	iters: 200, epoch: 71 | loss: 0.3994907
	speed: 0.0368s/iter; left time: 277.5190s
Epoch: 71 cost time: 11.163113594055176
Epoch: 71, Steps: 258 | Train Loss: 0.3969258 Vali Loss: 0.9322383 Test Loss: 0.4169109
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.3868336
	speed: 0.1648s/iter; left time: 1216.7331s
	iters: 200, epoch: 72 | loss: 0.3648108
	speed: 0.0350s/iter; left time: 254.5970s
Epoch: 72 cost time: 9.34647822380066
Epoch: 72, Steps: 258 | Train Loss: 0.3970185 Vali Loss: 0.9318177 Test Loss: 0.4169230
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.3737077
	speed: 0.1474s/iter; left time: 1050.4097s
	iters: 200, epoch: 73 | loss: 0.4149554
	speed: 0.0342s/iter; left time: 240.1794s
Epoch: 73 cost time: 10.085682153701782
Epoch: 73, Steps: 258 | Train Loss: 0.3969542 Vali Loss: 0.9328591 Test Loss: 0.4169275
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.3586691
	speed: 0.1660s/iter; left time: 1139.6145s
	iters: 200, epoch: 74 | loss: 0.3774197
	speed: 0.0309s/iter; left time: 209.1215s
Epoch: 74 cost time: 9.40843653678894
Epoch: 74, Steps: 258 | Train Loss: 0.3970002 Vali Loss: 0.9315886 Test Loss: 0.4169090
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.3915307
	speed: 0.1480s/iter; left time: 978.2859s
	iters: 200, epoch: 75 | loss: 0.3670693
	speed: 0.0342s/iter; left time: 222.4878s
Epoch: 75 cost time: 9.087400197982788
Epoch: 75, Steps: 258 | Train Loss: 0.3970121 Vali Loss: 0.9311158 Test Loss: 0.4169015
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.4132695
	speed: 0.1477s/iter; left time: 937.9327s
	iters: 200, epoch: 76 | loss: 0.3993413
	speed: 0.0369s/iter; left time: 230.9580s
Epoch: 76 cost time: 9.185106039047241
Epoch: 76, Steps: 258 | Train Loss: 0.3969714 Vali Loss: 0.9313587 Test Loss: 0.4169250
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.3925667
	speed: 0.1462s/iter; left time: 890.8051s
	iters: 200, epoch: 77 | loss: 0.3832948
	speed: 0.0337s/iter; left time: 201.6657s
Epoch: 77 cost time: 9.496231317520142
Epoch: 77, Steps: 258 | Train Loss: 0.3970641 Vali Loss: 0.9314192 Test Loss: 0.4169144
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.4099485
	speed: 0.1463s/iter; left time: 853.3978s
	iters: 200, epoch: 78 | loss: 0.3614183
	speed: 0.0364s/iter; left time: 208.9530s
Epoch: 78 cost time: 10.496687650680542
Epoch: 78, Steps: 258 | Train Loss: 0.3970154 Vali Loss: 0.9323626 Test Loss: 0.4169120
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.3927484
	speed: 0.1636s/iter; left time: 912.2748s
	iters: 200, epoch: 79 | loss: 0.4199518
	speed: 0.0421s/iter; left time: 230.3469s
Epoch: 79 cost time: 11.673744201660156
Epoch: 79, Steps: 258 | Train Loss: 0.3969345 Vali Loss: 0.9311382 Test Loss: 0.4169294
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.4075066
	speed: 0.1750s/iter; left time: 930.9018s
	iters: 200, epoch: 80 | loss: 0.4179650
	speed: 0.0372s/iter; left time: 194.3249s
Epoch: 80 cost time: 10.14495587348938
Epoch: 80, Steps: 258 | Train Loss: 0.3970161 Vali Loss: 0.9326390 Test Loss: 0.4169375
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.3710128
	speed: 0.1642s/iter; left time: 830.9607s
	iters: 200, epoch: 81 | loss: 0.3779599
	speed: 0.0346s/iter; left time: 171.6474s
Epoch: 81 cost time: 9.693575859069824
Epoch: 81, Steps: 258 | Train Loss: 0.3969076 Vali Loss: 0.9316421 Test Loss: 0.4169307
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.4027483
	speed: 0.1606s/iter; left time: 771.1853s
	iters: 200, epoch: 82 | loss: 0.3931774
	speed: 0.0394s/iter; left time: 185.3754s
Epoch: 82 cost time: 10.645057201385498
Epoch: 82, Steps: 258 | Train Loss: 0.3969508 Vali Loss: 0.9316586 Test Loss: 0.4169295
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_720_720_FITS_ETTm1_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.41596394777297974, mae:0.4120475947856903, rse:0.6136187314987183, corr:[0.52773577 0.5321832  0.53558034 0.5372991  0.5376621  0.5376901
 0.5379395  0.5385901  0.53964174 0.5408163  0.54184014 0.5423596
 0.54235756 0.5419114  0.5411398  0.54016954 0.5391412  0.5381074
 0.5370222  0.53588504 0.53472036 0.53345907 0.5320418  0.5306253
 0.5291528  0.52786064 0.52689046 0.5262872  0.5261799  0.5265501
 0.5272464  0.52815247 0.5289886  0.52965015 0.5299243  0.5299439
 0.52964914 0.5292219  0.52879494 0.5283817  0.52812994 0.52801245
 0.5280235  0.52817166 0.52828854 0.52825665 0.5281309  0.5279255
 0.5276349  0.5272775  0.5270257  0.52691853 0.52698976 0.52703965
 0.5270046  0.5268352  0.52656275 0.52621347 0.52592707 0.5257041
 0.5256127  0.52566445 0.52578586 0.52581745 0.52578336 0.5257215
 0.52563524 0.5255723  0.5256138  0.52573854 0.52594906 0.5261425
 0.5262088  0.52609473 0.5258759  0.5255805  0.52524275 0.5249428
 0.5247361  0.5246507  0.5246576  0.5246903  0.52469754 0.5245943
 0.5244476  0.52425605 0.5241076  0.5240346  0.52408916 0.5243394
 0.5247373  0.52516514 0.5255279  0.52573824 0.52579254 0.525653
 0.52539957 0.5251539  0.5248337  0.52454305 0.52428925 0.52408487
 0.52395594 0.5238564  0.5238011  0.5238334  0.5238436  0.5238297
 0.52371883 0.52353084 0.52327263 0.5229429  0.52258    0.5222402
 0.5219473  0.52172154 0.52155024 0.52139854 0.52124524 0.5211212
 0.52098846 0.52079934 0.5204941  0.5201657  0.51987803 0.51963854
 0.519471   0.5194216  0.51943815 0.5194849  0.51944435 0.51927763
 0.5190542  0.51883775 0.51862794 0.51850146 0.5185406  0.51864856
 0.51882046 0.51905185 0.519231   0.51934904 0.5194165  0.51946896
 0.51951754 0.51952004 0.5195191  0.51952064 0.51955277 0.5195568
 0.51948    0.5193726  0.5192365  0.5190905  0.51894933 0.51892394
 0.51898474 0.51908726 0.5192198  0.5192797  0.5193185  0.51934063
 0.5193774  0.5194435  0.5195681  0.51975423 0.5199971  0.52027875
 0.52049875 0.52057076 0.5205658  0.52054566 0.52050203 0.52047
 0.5204749  0.5205077  0.5205575  0.5206007  0.52059263 0.52055687
 0.52050537 0.5204826  0.5204756  0.52056694 0.52076924 0.52106476
 0.52141196 0.52176994 0.5220854  0.5222886  0.52231663 0.52218026
 0.52189916 0.5215738  0.521153   0.520605   0.5199488  0.5192553
 0.51854455 0.51785696 0.5171811  0.51654387 0.51592356 0.51530266
 0.51467013 0.5140218  0.513357   0.5127283  0.51215523 0.51163423
 0.5111188  0.51060677 0.51007235 0.50949854 0.5088308  0.5081554
 0.5075702  0.50709665 0.50679034 0.50662386 0.5065729  0.50664926
 0.5068667  0.5070851  0.50725555 0.5073611  0.5073566  0.50727624
 0.5071769  0.5071121  0.50710535 0.5071146  0.5071604  0.50725853
 0.50741476 0.50764936 0.5078521  0.50808185 0.50825727 0.5084067
 0.5084951  0.50844586 0.5083002  0.50813717 0.5080227  0.5079321
 0.5078529  0.50777924 0.5076952  0.50762326 0.5075329  0.5074862
 0.5074212  0.5074024  0.5073863  0.5073486  0.5072992  0.50725853
 0.507258   0.5073288  0.5074558  0.50760764 0.5077494  0.5079294
 0.50805026 0.5081043  0.50809103 0.50806123 0.50803167 0.5079866
 0.50796425 0.50795573 0.50796944 0.50799876 0.50802225 0.5080389
 0.50801307 0.5080248  0.50802267 0.5080804  0.50816745 0.5082462
 0.50833833 0.50836694 0.5082906  0.5081232  0.50784874 0.50745565
 0.5069846  0.5065289  0.5060987  0.50563616 0.50511944 0.5045621
 0.50400335 0.50343174 0.50292903 0.50247836 0.5020504  0.5016518
 0.5012442  0.500835   0.5004717  0.5001543  0.49987265 0.499588
 0.49937487 0.49917173 0.4989643  0.49875262 0.49859282 0.49846202
 0.4983567  0.49829802 0.49823755 0.49817207 0.49810404 0.4980786
 0.49808604 0.49808615 0.49806908 0.49803287 0.4979713  0.49787495
 0.49774742 0.4976293  0.4975019  0.4973943  0.49728304 0.49717346
 0.4971383  0.4971627  0.49720642 0.49726337 0.4973048  0.49738914
 0.4974663  0.49740845 0.49730355 0.49718162 0.49709612 0.4970568
 0.49701434 0.49699026 0.49696082 0.49693304 0.49690247 0.49690863
 0.4968913  0.49688774 0.49688315 0.49684328 0.4967938  0.49673304
 0.49664757 0.4965778  0.4965277  0.49650627 0.49654108 0.49662375
 0.49665457 0.49665812 0.49660715 0.4965219  0.49643344 0.49636164
 0.49627024 0.49621356 0.4962041  0.4961953  0.49620876 0.49623713
 0.49627712 0.49634543 0.49643984 0.49659055 0.49678394 0.49703696
 0.4973496  0.4976837  0.4980051  0.4982195  0.4982773  0.4981969
 0.4979852  0.49767214 0.49732634 0.49694115 0.49647662 0.49605414
 0.4956685  0.49532285 0.4950001  0.49471223 0.4943853  0.49401617
 0.49360216 0.4931685  0.4927482  0.4923672  0.49198794 0.49166626
 0.49137014 0.49113184 0.4908738  0.49062252 0.4903839  0.4902084
 0.4900927  0.49005175 0.4900696  0.49014345 0.4902528  0.4903253
 0.49044657 0.49051276 0.49057457 0.49056578 0.4905356  0.49055544
 0.49058115 0.4906607  0.4907396  0.4908096  0.49082696 0.49085173
 0.4908674  0.4909002  0.49090594 0.49095705 0.4909994  0.49111113
 0.4912025  0.49117064 0.4910369  0.49087062 0.4906889  0.490525
 0.49038628 0.4902854  0.49022833 0.4902057  0.49021628 0.49026194
 0.490299   0.49034485 0.49036518 0.4903625  0.49034128 0.49029395
 0.49027762 0.49029842 0.49035037 0.49041006 0.49045408 0.49052882
 0.49053273 0.49050024 0.49044684 0.4904351  0.49042124 0.4904428
 0.4904728  0.49052775 0.4905497  0.4905485  0.4905242  0.49047053
 0.49041966 0.49035358 0.49030703 0.49029136 0.4903133  0.49036226
 0.4903876  0.4903921  0.4903535  0.49020326 0.48991802 0.48952326
 0.48905388 0.48857152 0.48810613 0.48762125 0.48707178 0.48648292
 0.48591512 0.48530552 0.48468864 0.48410192 0.48350367 0.48288444
 0.48225456 0.48167256 0.4811452  0.48063436 0.4801398  0.4797018
 0.47931698 0.4790243  0.47881106 0.47854927 0.4783288  0.47811636
 0.47795644 0.47784758 0.47776362 0.47775516 0.47780767 0.47795108
 0.4781779  0.47839957 0.47857672 0.47877407 0.4789599  0.47911644
 0.4792047  0.4792851  0.47938678 0.47945294 0.4794875  0.4795571
 0.47969672 0.47991174 0.48017862 0.48040867 0.4806206  0.48083767
 0.4809966  0.48103693 0.4809289  0.48078442 0.4806853  0.48063374
 0.48063394 0.4806542  0.4806731  0.48068273 0.4806693  0.4806315
 0.48058063 0.48051938 0.48047054 0.48043063 0.48038584 0.48037082
 0.4803656  0.48037413 0.48039922 0.48040617 0.48040104 0.48045957
 0.48050666 0.4805487  0.48059142 0.48064205 0.4806627  0.480661
 0.4806226  0.4805725  0.4805219  0.48049805 0.48050627 0.48052815
 0.48052642 0.48058155 0.48061132 0.48064482 0.48068854 0.48074645
 0.48080894 0.4808334  0.48082817 0.48074612 0.48054728 0.48019367
 0.4797248  0.47922954 0.4787226  0.4781778  0.4775654  0.4769798
 0.4764224  0.4759301  0.47548473 0.47506502 0.47465453 0.47418204
 0.47367588 0.4731339  0.47257873 0.47202158 0.47153002 0.47111472
 0.47077674 0.47048736 0.47025576 0.4700709  0.46991405 0.46977112
 0.46967968 0.46961313 0.46959674 0.469631   0.46972743 0.46982592
 0.46997315 0.47011942 0.47025394 0.47038272 0.47049925 0.4706417
 0.47080603 0.4709254  0.47103214 0.47109374 0.47111827 0.47111857
 0.47116244 0.47130403 0.47147518 0.47167236 0.47195253 0.4722657
 0.4724833  0.47259784 0.4725959  0.4724853  0.47240576 0.47239774
 0.47239482 0.47242743 0.47246817 0.47243896 0.47235164 0.47216457
 0.47184998 0.47149363 0.47118762 0.4709131  0.47075307 0.4706692
 0.47063604 0.47061214 0.47058848 0.4705613  0.4705137  0.4704876
 0.47045317 0.47035417 0.47026354 0.47022045 0.47022372 0.4702534
 0.470295   0.470335   0.47036922 0.47040117 0.47042024 0.47040683
 0.47036746 0.47039133 0.47048107 0.47057962 0.47072542 0.47088614
 0.47103888 0.47115198 0.47121397 0.4712493  0.4712021  0.47104788
 0.47079352 0.47051948 0.4702132  0.46980873 0.46928415 0.4686771
 0.4680767  0.46752962 0.4671649  0.46697822 0.46682352 0.46676326
 0.46659628 0.466346   0.46594718 0.465501   0.46505976 0.46466616
 0.46439424 0.4642319  0.46417704 0.4640957  0.4639889  0.4638788
 0.46373194 0.4636422  0.4636115  0.46365649 0.463834   0.46403593
 0.46428168 0.4644936  0.46465793 0.46482795 0.46491703 0.46499866
 0.46511942 0.46535853 0.46572742 0.46616668 0.4666853  0.46717966
 0.46768373 0.46817756 0.4684995  0.46867016 0.4683359  0.46657416]
