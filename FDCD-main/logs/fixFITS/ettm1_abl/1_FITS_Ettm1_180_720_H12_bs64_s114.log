Args in experiment:
Namespace(H_order=12, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H12_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33661
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=34, out_features=170, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5178880.0
params:  5950.0
Trainable parameters:  5950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6398116
	speed: 0.0452s/iter; left time: 1180.3711s
	iters: 200, epoch: 1 | loss: 0.5267032
	speed: 0.0363s/iter; left time: 943.3767s
Epoch: 1 cost time: 11.073667764663696
Epoch: 1, Steps: 262 | Train Loss: 0.6720402 Vali Loss: 1.1188509 Test Loss: 0.5490044
Validation loss decreased (inf --> 1.118851).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4451869
	speed: 0.1424s/iter; left time: 3678.5916s
	iters: 200, epoch: 2 | loss: 0.4419571
	speed: 0.0326s/iter; left time: 838.6709s
Epoch: 2 cost time: 9.200581550598145
Epoch: 2, Steps: 262 | Train Loss: 0.4813731 Vali Loss: 1.0273330 Test Loss: 0.4727308
Validation loss decreased (1.118851 --> 1.027333).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4358627
	speed: 0.1462s/iter; left time: 3739.5657s
	iters: 200, epoch: 3 | loss: 0.4700446
	speed: 0.0525s/iter; left time: 1338.2949s
Epoch: 3 cost time: 12.83006477355957
Epoch: 3, Steps: 262 | Train Loss: 0.4557544 Vali Loss: 1.0003266 Test Loss: 0.4532412
Validation loss decreased (1.027333 --> 1.000327).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4122307
	speed: 0.1191s/iter; left time: 3014.0915s
	iters: 200, epoch: 4 | loss: 0.3894372
	speed: 0.0320s/iter; left time: 805.7091s
Epoch: 4 cost time: 9.310343027114868
Epoch: 4, Steps: 262 | Train Loss: 0.4474047 Vali Loss: 0.9890286 Test Loss: 0.4470087
Validation loss decreased (1.000327 --> 0.989029).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4692211
	speed: 0.1661s/iter; left time: 4160.7480s
	iters: 200, epoch: 5 | loss: 0.4729095
	speed: 0.0373s/iter; left time: 931.0834s
Epoch: 5 cost time: 9.889484882354736
Epoch: 5, Steps: 262 | Train Loss: 0.4444792 Vali Loss: 0.9843292 Test Loss: 0.4453851
Validation loss decreased (0.989029 --> 0.984329).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4625248
	speed: 0.1504s/iter; left time: 3729.5915s
	iters: 200, epoch: 6 | loss: 0.4799507
	speed: 0.0332s/iter; left time: 819.9337s
Epoch: 6 cost time: 8.538731336593628
Epoch: 6, Steps: 262 | Train Loss: 0.4434887 Vali Loss: 0.9823077 Test Loss: 0.4448756
Validation loss decreased (0.984329 --> 0.982308).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4100979
	speed: 0.1273s/iter; left time: 3121.3597s
	iters: 200, epoch: 7 | loss: 0.4446346
	speed: 0.0230s/iter; left time: 560.9881s
Epoch: 7 cost time: 7.187730073928833
Epoch: 7, Steps: 262 | Train Loss: 0.4430343 Vali Loss: 0.9812670 Test Loss: 0.4446156
Validation loss decreased (0.982308 --> 0.981267).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4309283
	speed: 0.1100s/iter; left time: 2670.4026s
	iters: 200, epoch: 8 | loss: 0.4756410
	speed: 0.0303s/iter; left time: 732.9878s
Epoch: 8 cost time: 8.246628522872925
Epoch: 8, Steps: 262 | Train Loss: 0.4428518 Vali Loss: 0.9818506 Test Loss: 0.4447004
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4975783
	speed: 0.1271s/iter; left time: 3050.8103s
	iters: 200, epoch: 9 | loss: 0.4827489
	speed: 0.0222s/iter; left time: 530.7984s
Epoch: 9 cost time: 6.691910266876221
Epoch: 9, Steps: 262 | Train Loss: 0.4426826 Vali Loss: 0.9806418 Test Loss: 0.4448949
Validation loss decreased (0.981267 --> 0.980642).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4255705
	speed: 0.1154s/iter; left time: 2738.9736s
	iters: 200, epoch: 10 | loss: 0.4383373
	speed: 0.0226s/iter; left time: 535.4580s
Epoch: 10 cost time: 7.494270086288452
Epoch: 10, Steps: 262 | Train Loss: 0.4428169 Vali Loss: 0.9805205 Test Loss: 0.4448500
Validation loss decreased (0.980642 --> 0.980521).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4058721
	speed: 0.1119s/iter; left time: 2627.7677s
	iters: 200, epoch: 11 | loss: 0.4565080
	speed: 0.0214s/iter; left time: 499.5227s
Epoch: 11 cost time: 6.3827338218688965
Epoch: 11, Steps: 262 | Train Loss: 0.4426339 Vali Loss: 0.9801932 Test Loss: 0.4447802
Validation loss decreased (0.980521 --> 0.980193).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4382127
	speed: 0.1326s/iter; left time: 3078.4057s
	iters: 200, epoch: 12 | loss: 0.4498850
	speed: 0.0275s/iter; left time: 636.4187s
Epoch: 12 cost time: 8.90661096572876
Epoch: 12, Steps: 262 | Train Loss: 0.4427374 Vali Loss: 0.9796295 Test Loss: 0.4446996
Validation loss decreased (0.980193 --> 0.979630).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4478979
	speed: 0.1425s/iter; left time: 3271.3722s
	iters: 200, epoch: 13 | loss: 0.4308167
	speed: 0.0332s/iter; left time: 757.7895s
Epoch: 13 cost time: 8.139351606369019
Epoch: 13, Steps: 262 | Train Loss: 0.4425880 Vali Loss: 0.9799784 Test Loss: 0.4447762
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4556010
	speed: 0.1402s/iter; left time: 3182.5919s
	iters: 200, epoch: 14 | loss: 0.3928167
	speed: 0.0359s/iter; left time: 810.8291s
Epoch: 14 cost time: 9.498213529586792
Epoch: 14, Steps: 262 | Train Loss: 0.4426147 Vali Loss: 0.9805161 Test Loss: 0.4447902
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4046039
	speed: 0.1189s/iter; left time: 2667.1209s
	iters: 200, epoch: 15 | loss: 0.4358519
	speed: 0.0298s/iter; left time: 664.6968s
Epoch: 15 cost time: 7.530780792236328
Epoch: 15, Steps: 262 | Train Loss: 0.4426309 Vali Loss: 0.9800858 Test Loss: 0.4448428
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4216690
	speed: 0.1183s/iter; left time: 2622.8957s
	iters: 200, epoch: 16 | loss: 0.4543682
	speed: 0.0312s/iter; left time: 687.8605s
Epoch: 16 cost time: 9.077661037445068
Epoch: 16, Steps: 262 | Train Loss: 0.4424097 Vali Loss: 0.9798646 Test Loss: 0.4449017
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4468917
	speed: 0.1659s/iter; left time: 3634.7128s
	iters: 200, epoch: 17 | loss: 0.4267759
	speed: 0.0360s/iter; left time: 784.3054s
Epoch: 17 cost time: 8.38757586479187
Epoch: 17, Steps: 262 | Train Loss: 0.4425243 Vali Loss: 0.9802580 Test Loss: 0.4449719
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4705438
	speed: 0.1781s/iter; left time: 3854.9344s
	iters: 200, epoch: 18 | loss: 0.4133229
	speed: 0.0292s/iter; left time: 628.4216s
Epoch: 18 cost time: 10.197681427001953
Epoch: 18, Steps: 262 | Train Loss: 0.4424478 Vali Loss: 0.9797795 Test Loss: 0.4449129
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4704635
	speed: 0.1881s/iter; left time: 4022.7784s
	iters: 200, epoch: 19 | loss: 0.3941246
	speed: 0.0254s/iter; left time: 540.6741s
Epoch: 19 cost time: 8.19400429725647
Epoch: 19, Steps: 262 | Train Loss: 0.4425557 Vali Loss: 0.9797070 Test Loss: 0.4448627
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4511268
	speed: 0.1153s/iter; left time: 2435.1919s
	iters: 200, epoch: 20 | loss: 0.4499965
	speed: 0.0327s/iter; left time: 687.2555s
Epoch: 20 cost time: 8.217957973480225
Epoch: 20, Steps: 262 | Train Loss: 0.4424770 Vali Loss: 0.9797300 Test Loss: 0.4449466
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4869328
	speed: 0.1006s/iter; left time: 2098.0320s
	iters: 200, epoch: 21 | loss: 0.4220774
	speed: 0.0346s/iter; left time: 717.9823s
Epoch: 21 cost time: 8.091874599456787
Epoch: 21, Steps: 262 | Train Loss: 0.4426368 Vali Loss: 0.9791833 Test Loss: 0.4449746
Validation loss decreased (0.979630 --> 0.979183).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4521408
	speed: 0.1488s/iter; left time: 3066.1070s
	iters: 200, epoch: 22 | loss: 0.4737845
	speed: 0.0316s/iter; left time: 646.7670s
Epoch: 22 cost time: 10.921279668807983
Epoch: 22, Steps: 262 | Train Loss: 0.4423727 Vali Loss: 0.9803551 Test Loss: 0.4449732
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4337156
	speed: 0.1686s/iter; left time: 3428.7278s
	iters: 200, epoch: 23 | loss: 0.4438009
	speed: 0.0328s/iter; left time: 663.9996s
Epoch: 23 cost time: 10.231061458587646
Epoch: 23, Steps: 262 | Train Loss: 0.4424890 Vali Loss: 0.9795899 Test Loss: 0.4449382
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4581662
	speed: 0.1593s/iter; left time: 3198.1906s
	iters: 200, epoch: 24 | loss: 0.4768339
	speed: 0.0362s/iter; left time: 723.6515s
Epoch: 24 cost time: 9.324589490890503
Epoch: 24, Steps: 262 | Train Loss: 0.4422901 Vali Loss: 0.9801008 Test Loss: 0.4449354
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4291925
	speed: 0.1387s/iter; left time: 2747.0911s
	iters: 200, epoch: 25 | loss: 0.4368422
	speed: 0.0255s/iter; left time: 502.4269s
Epoch: 25 cost time: 7.6309850215911865
Epoch: 25, Steps: 262 | Train Loss: 0.4423230 Vali Loss: 0.9790761 Test Loss: 0.4448754
Validation loss decreased (0.979183 --> 0.979076).  Saving model ...
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4331391
	speed: 0.1319s/iter; left time: 2579.2936s
	iters: 200, epoch: 26 | loss: 0.4308591
	speed: 0.0350s/iter; left time: 681.0192s
Epoch: 26 cost time: 10.657543420791626
Epoch: 26, Steps: 262 | Train Loss: 0.4425065 Vali Loss: 0.9792657 Test Loss: 0.4449420
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3882266
	speed: 0.1726s/iter; left time: 3328.7583s
	iters: 200, epoch: 27 | loss: 0.4402470
	speed: 0.0316s/iter; left time: 606.0803s
Epoch: 27 cost time: 9.866279602050781
Epoch: 27, Steps: 262 | Train Loss: 0.4424278 Vali Loss: 0.9790755 Test Loss: 0.4449522
Validation loss decreased (0.979076 --> 0.979075).  Saving model ...
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4547441
	speed: 0.1863s/iter; left time: 3544.0228s
	iters: 200, epoch: 28 | loss: 0.4900667
	speed: 0.0483s/iter; left time: 913.2409s
Epoch: 28 cost time: 14.017808675765991
Epoch: 28, Steps: 262 | Train Loss: 0.4420710 Vali Loss: 0.9798033 Test Loss: 0.4449035
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4430033
	speed: 0.1536s/iter; left time: 2882.6419s
	iters: 200, epoch: 29 | loss: 0.4322866
	speed: 0.0258s/iter; left time: 481.4580s
Epoch: 29 cost time: 7.211663246154785
Epoch: 29, Steps: 262 | Train Loss: 0.4423498 Vali Loss: 0.9787093 Test Loss: 0.4449207
Validation loss decreased (0.979075 --> 0.978709).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4231449
	speed: 0.1223s/iter; left time: 2262.3314s
	iters: 200, epoch: 30 | loss: 0.4054812
	speed: 0.0379s/iter; left time: 696.7512s
Epoch: 30 cost time: 9.354382753372192
Epoch: 30, Steps: 262 | Train Loss: 0.4421876 Vali Loss: 0.9790771 Test Loss: 0.4449791
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4258100
	speed: 0.1565s/iter; left time: 2854.8249s
	iters: 200, epoch: 31 | loss: 0.4689262
	speed: 0.0301s/iter; left time: 545.3891s
Epoch: 31 cost time: 8.826840162277222
Epoch: 31, Steps: 262 | Train Loss: 0.4422575 Vali Loss: 0.9790968 Test Loss: 0.4450085
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4410438
	speed: 0.1825s/iter; left time: 3280.7072s
	iters: 200, epoch: 32 | loss: 0.4135839
	speed: 0.0641s/iter; left time: 1146.4597s
Epoch: 32 cost time: 14.841451644897461
Epoch: 32, Steps: 262 | Train Loss: 0.4424408 Vali Loss: 0.9795510 Test Loss: 0.4449548
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4697686
	speed: 0.1753s/iter; left time: 3105.8766s
	iters: 200, epoch: 33 | loss: 0.4722168
	speed: 0.0378s/iter; left time: 666.0593s
Epoch: 33 cost time: 9.840443134307861
Epoch: 33, Steps: 262 | Train Loss: 0.4423485 Vali Loss: 0.9794754 Test Loss: 0.4449615
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4286509
	speed: 0.1524s/iter; left time: 2659.8330s
	iters: 200, epoch: 34 | loss: 0.4072345
	speed: 0.0224s/iter; left time: 388.8869s
Epoch: 34 cost time: 6.9516003131866455
Epoch: 34, Steps: 262 | Train Loss: 0.4424083 Vali Loss: 0.9795258 Test Loss: 0.4449764
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4291303
	speed: 0.1232s/iter; left time: 2118.7814s
	iters: 200, epoch: 35 | loss: 0.4360074
	speed: 0.0239s/iter; left time: 408.6915s
Epoch: 35 cost time: 7.0045084953308105
Epoch: 35, Steps: 262 | Train Loss: 0.4424339 Vali Loss: 0.9796406 Test Loss: 0.4450056
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4188322
	speed: 0.1217s/iter; left time: 2060.6844s
	iters: 200, epoch: 36 | loss: 0.4285268
	speed: 0.0319s/iter; left time: 537.0686s
Epoch: 36 cost time: 7.613682508468628
Epoch: 36, Steps: 262 | Train Loss: 0.4424280 Vali Loss: 0.9803261 Test Loss: 0.4450011
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.4450643
	speed: 0.1392s/iter; left time: 2319.7831s
	iters: 200, epoch: 37 | loss: 0.4401746
	speed: 0.0221s/iter; left time: 366.2761s
Epoch: 37 cost time: 7.428720474243164
Epoch: 37, Steps: 262 | Train Loss: 0.4421778 Vali Loss: 0.9808806 Test Loss: 0.4449694
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4015117
	speed: 0.1461s/iter; left time: 2396.6090s
	iters: 200, epoch: 38 | loss: 0.4104271
	speed: 0.0228s/iter; left time: 372.1984s
Epoch: 38 cost time: 7.4238080978393555
Epoch: 38, Steps: 262 | Train Loss: 0.4423626 Vali Loss: 0.9803987 Test Loss: 0.4449934
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.4428540
	speed: 0.1118s/iter; left time: 1805.7085s
	iters: 200, epoch: 39 | loss: 0.4272234
	speed: 0.0349s/iter; left time: 559.4250s
Epoch: 39 cost time: 8.109780311584473
Epoch: 39, Steps: 262 | Train Loss: 0.4422279 Vali Loss: 0.9794886 Test Loss: 0.4449972
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4311067
	speed: 0.1144s/iter; left time: 1817.1660s
	iters: 200, epoch: 40 | loss: 0.4372044
	speed: 0.0270s/iter; left time: 426.7946s
Epoch: 40 cost time: 7.724241256713867
Epoch: 40, Steps: 262 | Train Loss: 0.4423241 Vali Loss: 0.9800473 Test Loss: 0.4450023
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4745858
	speed: 0.1320s/iter; left time: 2061.4796s
	iters: 200, epoch: 41 | loss: 0.3923287
	speed: 0.0296s/iter; left time: 459.8117s
Epoch: 41 cost time: 7.942352771759033
Epoch: 41, Steps: 262 | Train Loss: 0.4421214 Vali Loss: 0.9791574 Test Loss: 0.4450176
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4645994
	speed: 0.1358s/iter; left time: 2086.0495s
	iters: 200, epoch: 42 | loss: 0.4336036
	speed: 0.0389s/iter; left time: 593.6390s
Epoch: 42 cost time: 9.5351402759552
Epoch: 42, Steps: 262 | Train Loss: 0.4423454 Vali Loss: 0.9793313 Test Loss: 0.4450117
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4246419
	speed: 0.1447s/iter; left time: 2184.1266s
	iters: 200, epoch: 43 | loss: 0.4218033
	speed: 0.0270s/iter; left time: 404.6682s
Epoch: 43 cost time: 7.337514400482178
Epoch: 43, Steps: 262 | Train Loss: 0.4422372 Vali Loss: 0.9801868 Test Loss: 0.4450189
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4601059
	speed: 0.1145s/iter; left time: 1699.2509s
	iters: 200, epoch: 44 | loss: 0.4439944
	speed: 0.0237s/iter; left time: 349.2149s
Epoch: 44 cost time: 6.561176776885986
Epoch: 44, Steps: 262 | Train Loss: 0.4423526 Vali Loss: 0.9800356 Test Loss: 0.4450375
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.4228180
	speed: 0.1092s/iter; left time: 1590.8196s
	iters: 200, epoch: 45 | loss: 0.3903082
	speed: 0.0332s/iter; left time: 481.0207s
Epoch: 45 cost time: 10.05330228805542
Epoch: 45, Steps: 262 | Train Loss: 0.4423731 Vali Loss: 0.9804671 Test Loss: 0.4450423
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4190759
	speed: 0.1570s/iter; left time: 2247.5371s
	iters: 200, epoch: 46 | loss: 0.4357761
	speed: 0.0274s/iter; left time: 390.0403s
Epoch: 46 cost time: 7.593413829803467
Epoch: 46, Steps: 262 | Train Loss: 0.4423432 Vali Loss: 0.9796297 Test Loss: 0.4450369
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.5041959
	speed: 0.1519s/iter; left time: 2133.9335s
	iters: 200, epoch: 47 | loss: 0.4503865
	speed: 0.0279s/iter; left time: 389.6251s
Epoch: 47 cost time: 9.922537326812744
Epoch: 47, Steps: 262 | Train Loss: 0.4423454 Vali Loss: 0.9792538 Test Loss: 0.4450332
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4362279
	speed: 0.1512s/iter; left time: 2084.1918s
	iters: 200, epoch: 48 | loss: 0.4749126
	speed: 0.0296s/iter; left time: 404.8972s
Epoch: 48 cost time: 9.088348388671875
Epoch: 48, Steps: 262 | Train Loss: 0.4423663 Vali Loss: 0.9801076 Test Loss: 0.4450277
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.4227807
	speed: 0.1509s/iter; left time: 2040.4990s
	iters: 200, epoch: 49 | loss: 0.4412016
	speed: 0.0362s/iter; left time: 485.9053s
Epoch: 49 cost time: 10.471229553222656
Epoch: 49, Steps: 262 | Train Loss: 0.4422541 Vali Loss: 0.9808856 Test Loss: 0.4450281
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_180_720_FITS_ETTm1_ftM_sl180_ll48_pl720_H12_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.44310253858566284, mae:0.4244157671928406, rse:0.6333195567131042, corr:[0.532154   0.5338989  0.53224    0.5306399  0.52880913 0.52706814
 0.52611095 0.5261659  0.52599096 0.52539307 0.5251816  0.52539265
 0.52556217 0.5244911  0.522398   0.5203713  0.5188262  0.51747745
 0.51586676 0.5139651  0.51210916 0.5103792  0.50850636 0.50621504
 0.5035022  0.5008922  0.49898592 0.49774718 0.4969044  0.496173
 0.49579838 0.49592078 0.4964046  0.49697366 0.4971989  0.49706873
 0.49660102 0.4960859  0.49594158 0.49590158 0.49578333 0.49538025
 0.4949458  0.49489465 0.4951379  0.4953416  0.49531114 0.4951065
 0.49503812 0.4952063  0.49554744 0.49580085 0.4958361  0.4957187
 0.4957116  0.4958416  0.4960569  0.49596792 0.49564683 0.4952791
 0.49520236 0.49536273 0.49542135 0.49520364 0.4948368  0.49479517
 0.49514842 0.49561775 0.49606663 0.4963663  0.4966048  0.49681696
 0.49702412 0.4971232  0.49718258 0.49711904 0.4971024  0.49712494
 0.49710712 0.49695414 0.4967638  0.49664715 0.4966829  0.4966434
 0.49643427 0.49611175 0.4958194  0.4955904  0.49542767 0.4954699
 0.49555337 0.49553627 0.49537945 0.49513158 0.4947597  0.49406502
 0.49304816 0.4919057  0.4908607  0.49014163 0.48986247 0.48996928
 0.49030972 0.4908653  0.49174866 0.49289405 0.49403223 0.49491435
 0.49552807 0.49602103 0.49652717 0.49711975 0.4974509  0.49738166
 0.49698794 0.4966601  0.49641985 0.4960937  0.49555835 0.49479178
 0.4941036  0.49353698 0.4930437  0.49259162 0.49196583 0.49118328
 0.4905512  0.49023715 0.490126   0.49004513 0.48990917 0.48975188
 0.48963833 0.489595   0.4893375  0.48866853 0.48798522 0.48755458
 0.48746172 0.48763713 0.48775804 0.48769426 0.48759088 0.48768976
 0.48801172 0.4884035  0.48851234 0.48839667 0.48839086 0.48830208
 0.4881821  0.48804846 0.48786354 0.4877857  0.4877961  0.48799863
 0.48817736 0.488178   0.48789454 0.48765388 0.48764613 0.48791647
 0.48832235 0.48869243 0.48894492 0.48929796 0.4897137  0.49006835
 0.4903351  0.49045086 0.49043235 0.49053183 0.49061182 0.49051347
 0.4903051  0.49005333 0.48987696 0.48973876 0.48943347 0.48903814
 0.48871648 0.4886268  0.48872614 0.48889646 0.48903093 0.48910734
 0.48916382 0.48925617 0.48942053 0.4896419  0.48982206 0.4898315
 0.48961085 0.48931727 0.48904428 0.48866075 0.48819155 0.48775268
 0.48734286 0.48695573 0.48663294 0.48661044 0.4865805  0.48642582
 0.48618072 0.48576534 0.48534647 0.48496357 0.48445293 0.48379797
 0.48310423 0.48253515 0.48197287 0.48115474 0.48000255 0.47875232
 0.47773516 0.47697562 0.47635624 0.47573307 0.47505346 0.47455165
 0.47439426 0.4745266  0.47463712 0.47459352 0.4744608  0.47425386
 0.4740012  0.4738696  0.47371802 0.4734568  0.4731779  0.47300068
 0.47291887 0.4729085  0.47275683 0.47270274 0.4728544  0.47309324
 0.4734512  0.47363585 0.47369188 0.47357768 0.47342426 0.47337154
 0.47347385 0.4735593  0.47362068 0.47365406 0.47363767 0.47357357
 0.47347128 0.47345066 0.4734035  0.4734505  0.4735199  0.47359568
 0.47378653 0.47404268 0.47428337 0.47453123 0.47478253 0.47509393
 0.475467   0.47570425 0.4758593  0.47602698 0.4762101  0.47624025
 0.47621498 0.476159   0.47626624 0.4764624  0.47669566 0.47686347
 0.47697195 0.47714847 0.4773007  0.47739145 0.47750834 0.47767577
 0.47792673 0.47805986 0.47804022 0.4778674  0.47753397 0.4770579
 0.47634897 0.47554988 0.47483337 0.47417212 0.47365183 0.47330028
 0.47328076 0.47344244 0.47381327 0.4743053  0.47477356 0.47519782
 0.47551    0.4755987  0.47555685 0.4753974  0.47515017 0.47488517
 0.47473    0.47462484 0.47442728 0.47409883 0.47367397 0.47324407
 0.4728543  0.47242022 0.47202262 0.47177088 0.47145134 0.4712676
 0.47120455 0.47103187 0.47076136 0.4706122  0.47049376 0.47040775
 0.47022432 0.46987727 0.46936762 0.46887213 0.4684841  0.46825466
 0.4681146  0.4680315  0.46792847 0.46788824 0.4680144  0.46818072
 0.46827692 0.46824047 0.4681264  0.4680716  0.46824837 0.46842048
 0.46842566 0.46826103 0.46812552 0.4681372  0.468271   0.468462
 0.4685081  0.46847388 0.46852508 0.46862265 0.46875843 0.4688376
 0.46881175 0.46884736 0.4689339  0.46915826 0.46940982 0.46957847
 0.4695513  0.46950975 0.46949393 0.46956122 0.46960863 0.46951964
 0.4693241  0.46928582 0.46948424 0.4697517  0.470004   0.47016776
 0.47036588 0.47065616 0.47103626 0.47136623 0.4715989  0.4718179
 0.4720273  0.47218323 0.47230762 0.4724131  0.47243077 0.4722841
 0.4719287  0.47139797 0.4709549  0.47054523 0.47015557 0.46999767
 0.47010994 0.47045863 0.4708465  0.471409   0.47205877 0.4727578
 0.47331974 0.47358683 0.47353417 0.47334766 0.47307596 0.4728364
 0.47264823 0.47242123 0.47208965 0.4716442  0.4711199  0.4706506
 0.47028112 0.46985865 0.46935794 0.46886975 0.4685495  0.46844646
 0.46854252 0.4685116  0.46834815 0.46812633 0.46793848 0.46795592
 0.4678638  0.4676154  0.4671598  0.46665576 0.46626505 0.4661033
 0.4659619  0.4658353  0.46567473 0.46562922 0.4656951  0.46592352
 0.46602976 0.4660015  0.46591815 0.4659473  0.4659926  0.4661209
 0.4661018  0.46593982 0.46580654 0.46581736 0.46595496 0.4661344
 0.46622    0.46621767 0.46615183 0.46623072 0.46639606 0.466521
 0.4666234  0.46669623 0.4667056  0.46673912 0.4667744  0.46702248
 0.467318   0.46745628 0.46739754 0.4672744  0.4670795  0.46692538
 0.46690956 0.46702883 0.4671323  0.46720058 0.46723107 0.46724695
 0.467352   0.46757895 0.46787772 0.46803337 0.46807146 0.4680694
 0.46798787 0.46784106 0.4675976  0.46720552 0.4665987  0.46575874
 0.46468407 0.4634659  0.46223173 0.46109623 0.46011388 0.45939898
 0.45893714 0.45846123 0.4581738  0.4580716  0.45813823 0.45827708
 0.45840797 0.45831838 0.45805186 0.45778564 0.4575066  0.4571084
 0.45660895 0.4561263  0.4556917  0.45524538 0.45478338 0.4542376
 0.45372844 0.45328268 0.45291615 0.45266873 0.4524248  0.45224383
 0.4521344  0.4521176  0.45213625 0.4522306  0.45224437 0.45210811
 0.4518513  0.4517061  0.45165384 0.45149136 0.45116413 0.45078543
 0.4504235  0.45026398 0.4502613  0.4502993  0.45036718 0.45043993
 0.45047855 0.45055377 0.45061374 0.45060608 0.45052788 0.45038572
 0.45028198 0.450273   0.450508   0.4507372  0.45080736 0.45070526
 0.45059648 0.45046622 0.45044193 0.45041847 0.45036232 0.45036823
 0.45052585 0.45083553 0.45116404 0.45133686 0.4513441  0.45138896
 0.4514964  0.45157802 0.4516135  0.4516806  0.45169878 0.45168787
 0.45176262 0.4519569  0.45210683 0.4521601  0.4521026  0.45208442
 0.45211548 0.45234576 0.45253417 0.45261082 0.45263276 0.45272774
 0.4528684  0.45284995 0.45254925 0.4519099  0.4510152  0.44996142
 0.4487745  0.44754145 0.4463244  0.44507587 0.44399878 0.44328803
 0.4429386  0.44283172 0.44297305 0.4433437  0.44396126 0.44457448
 0.44492626 0.4448828  0.44463465 0.44444057 0.44434178 0.44421813
 0.44389942 0.44343898 0.44298282 0.44257915 0.4422702  0.4419284
 0.4415952  0.44115    0.4406194  0.4402622  0.44007355 0.44002655
 0.44008356 0.4402416  0.44037697 0.44045207 0.44046855 0.4404797
 0.44037056 0.44013372 0.4399339  0.4396628  0.43933657 0.43897346
 0.4388062  0.4388394  0.43892208 0.4389146  0.43884754 0.4388624
 0.43900782 0.43927738 0.4394861  0.43943137 0.43921968 0.43904012
 0.43896824 0.43900645 0.43902007 0.4389056  0.43880573 0.43871117
 0.43869698 0.4388425  0.43889135 0.43872714 0.43857172 0.43849403
 0.43867052 0.43904454 0.43937275 0.439524   0.439537   0.4396542
 0.43993902 0.44019046 0.440371   0.4405414  0.44065207 0.4407206
 0.4407136  0.44068778 0.44070125 0.44081274 0.44106257 0.44136718
 0.4415989  0.441768   0.4419091  0.44199404 0.44213533 0.44232306
 0.44242913 0.44233164 0.44196123 0.4414105  0.44081804 0.4401751
 0.4393015  0.43820384 0.43709347 0.43624082 0.4358866  0.4358395
 0.43578127 0.43573475 0.43587345 0.4363692  0.43708476 0.43783164
 0.4382915  0.43828955 0.438096   0.4378326  0.43746123 0.43695983
 0.43642712 0.43594244 0.43556452 0.43516806 0.43463293 0.43411732
 0.43367836 0.43327513 0.43297544 0.43266636 0.43224573 0.43175578
 0.43140978 0.431221   0.43117332 0.4311765  0.4310311  0.4309274
 0.43083492 0.43051964 0.430086   0.42966545 0.42962742 0.43004152
 0.43062386 0.4309796  0.43091315 0.43113777 0.43174982 0.43113604]
