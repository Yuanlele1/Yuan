Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.0227991
	speed: 0.0236s/iter; left time: 618.3963s
	iters: 200, epoch: 1 | loss: 0.8044395
	speed: 0.0189s/iter; left time: 492.7740s
Epoch: 1 cost time: 5.628824234008789
Epoch: 1, Steps: 263 | Train Loss: 0.9880641 Vali Loss: 1.4646633 Test Loss: 0.9364796
Validation loss decreased (inf --> 1.464663).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5922533
	speed: 0.0960s/iter; left time: 2488.8387s
	iters: 200, epoch: 2 | loss: 0.5904092
	speed: 0.0182s/iter; left time: 469.7315s
Epoch: 2 cost time: 5.453692674636841
Epoch: 2, Steps: 263 | Train Loss: 0.5601097 Vali Loss: 1.1506345 Test Loss: 0.6268815
Validation loss decreased (1.464663 --> 1.150635).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4780323
	speed: 0.0805s/iter; left time: 2065.9645s
	iters: 200, epoch: 3 | loss: 0.4772952
	speed: 0.0233s/iter; left time: 596.1558s
Epoch: 3 cost time: 5.644525766372681
Epoch: 3, Steps: 263 | Train Loss: 0.4785559 Vali Loss: 1.0785440 Test Loss: 0.5569912
Validation loss decreased (1.150635 --> 1.078544).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4147629
	speed: 0.0881s/iter; left time: 2237.9278s
	iters: 200, epoch: 4 | loss: 0.4148446
	speed: 0.0178s/iter; left time: 450.8835s
Epoch: 4 cost time: 5.244997978210449
Epoch: 4, Steps: 263 | Train Loss: 0.4587238 Vali Loss: 1.0507529 Test Loss: 0.5316759
Validation loss decreased (1.078544 --> 1.050753).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4618751
	speed: 0.0880s/iter; left time: 2211.9912s
	iters: 200, epoch: 5 | loss: 0.4617074
	speed: 0.0176s/iter; left time: 441.2155s
Epoch: 5 cost time: 5.794251441955566
Epoch: 5, Steps: 263 | Train Loss: 0.4510535 Vali Loss: 1.0358115 Test Loss: 0.5186871
Validation loss decreased (1.050753 --> 1.035812).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4324215
	speed: 0.0872s/iter; left time: 2169.2359s
	iters: 200, epoch: 6 | loss: 0.4518600
	speed: 0.0167s/iter; left time: 414.3641s
Epoch: 6 cost time: 5.152247190475464
Epoch: 6, Steps: 263 | Train Loss: 0.4472758 Vali Loss: 1.0282432 Test Loss: 0.5104140
Validation loss decreased (1.035812 --> 1.028243).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3982282
	speed: 0.0912s/iter; left time: 2245.4576s
	iters: 200, epoch: 7 | loss: 0.4417367
	speed: 0.0191s/iter; left time: 467.4845s
Epoch: 7 cost time: 5.723518371582031
Epoch: 7, Steps: 263 | Train Loss: 0.4450533 Vali Loss: 1.0239741 Test Loss: 0.5052576
Validation loss decreased (1.028243 --> 1.023974).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4599763
	speed: 0.0843s/iter; left time: 2054.3593s
	iters: 200, epoch: 8 | loss: 0.4561328
	speed: 0.0213s/iter; left time: 517.8743s
Epoch: 8 cost time: 5.706886053085327
Epoch: 8, Steps: 263 | Train Loss: 0.4436588 Vali Loss: 1.0206153 Test Loss: 0.5023190
Validation loss decreased (1.023974 --> 1.020615).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4296603
	speed: 0.0870s/iter; left time: 2096.7121s
	iters: 200, epoch: 9 | loss: 0.4493105
	speed: 0.0226s/iter; left time: 541.8557s
Epoch: 9 cost time: 6.1541664600372314
Epoch: 9, Steps: 263 | Train Loss: 0.4430048 Vali Loss: 1.0196239 Test Loss: 0.4999749
Validation loss decreased (1.020615 --> 1.019624).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4471895
	speed: 0.0965s/iter; left time: 2300.3244s
	iters: 200, epoch: 10 | loss: 0.4216346
	speed: 0.0178s/iter; left time: 423.5299s
Epoch: 10 cost time: 5.869829177856445
Epoch: 10, Steps: 263 | Train Loss: 0.4424227 Vali Loss: 1.0172350 Test Loss: 0.4983828
Validation loss decreased (1.019624 --> 1.017235).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4800831
	speed: 0.0939s/iter; left time: 2212.2811s
	iters: 200, epoch: 11 | loss: 0.4444130
	speed: 0.0176s/iter; left time: 412.3887s
Epoch: 11 cost time: 5.7982189655303955
Epoch: 11, Steps: 263 | Train Loss: 0.4419815 Vali Loss: 1.0170358 Test Loss: 0.4973794
Validation loss decreased (1.017235 --> 1.017036).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4645760
	speed: 0.0851s/iter; left time: 1984.0729s
	iters: 200, epoch: 12 | loss: 0.4525676
	speed: 0.0169s/iter; left time: 392.8423s
Epoch: 12 cost time: 5.396468877792358
Epoch: 12, Steps: 263 | Train Loss: 0.4418809 Vali Loss: 1.0167319 Test Loss: 0.4968140
Validation loss decreased (1.017036 --> 1.016732).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4321056
	speed: 0.0855s/iter; left time: 1969.4677s
	iters: 200, epoch: 13 | loss: 0.4455540
	speed: 0.0177s/iter; left time: 405.4074s
Epoch: 13 cost time: 5.206647634506226
Epoch: 13, Steps: 263 | Train Loss: 0.4417746 Vali Loss: 1.0166092 Test Loss: 0.4962191
Validation loss decreased (1.016732 --> 1.016609).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4307168
	speed: 0.0895s/iter; left time: 2039.3251s
	iters: 200, epoch: 14 | loss: 0.4484722
	speed: 0.0180s/iter; left time: 407.5509s
Epoch: 14 cost time: 5.221410036087036
Epoch: 14, Steps: 263 | Train Loss: 0.4418589 Vali Loss: 1.0145518 Test Loss: 0.4957122
Validation loss decreased (1.016609 --> 1.014552).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4515691
	speed: 0.0869s/iter; left time: 1956.1131s
	iters: 200, epoch: 15 | loss: 0.4642078
	speed: 0.0163s/iter; left time: 364.4694s
Epoch: 15 cost time: 4.974355697631836
Epoch: 15, Steps: 263 | Train Loss: 0.4417806 Vali Loss: 1.0148137 Test Loss: 0.4956204
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4052289
	speed: 0.0854s/iter; left time: 1900.9401s
	iters: 200, epoch: 16 | loss: 0.4433399
	speed: 0.0203s/iter; left time: 448.9978s
Epoch: 16 cost time: 5.1462178230285645
Epoch: 16, Steps: 263 | Train Loss: 0.4417533 Vali Loss: 1.0154657 Test Loss: 0.4957612
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4392999
	speed: 0.0879s/iter; left time: 1934.0491s
	iters: 200, epoch: 17 | loss: 0.4580704
	speed: 0.0186s/iter; left time: 408.0621s
Epoch: 17 cost time: 5.0557334423065186
Epoch: 17, Steps: 263 | Train Loss: 0.4418504 Vali Loss: 1.0164424 Test Loss: 0.4956516
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4689505
	speed: 0.0826s/iter; left time: 1794.8864s
	iters: 200, epoch: 18 | loss: 0.4848674
	speed: 0.0162s/iter; left time: 349.8223s
Epoch: 18 cost time: 4.987246036529541
Epoch: 18, Steps: 263 | Train Loss: 0.4416288 Vali Loss: 1.0156227 Test Loss: 0.4953849
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4156891
	speed: 0.0969s/iter; left time: 2079.7141s
	iters: 200, epoch: 19 | loss: 0.4521991
	speed: 0.0218s/iter; left time: 465.8786s
Epoch: 19 cost time: 5.954884052276611
Epoch: 19, Steps: 263 | Train Loss: 0.4416102 Vali Loss: 1.0153986 Test Loss: 0.4957244
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4181882
	speed: 0.0956s/iter; left time: 2027.9570s
	iters: 200, epoch: 20 | loss: 0.4358031
	speed: 0.0167s/iter; left time: 352.4285s
Epoch: 20 cost time: 5.112684488296509
Epoch: 20, Steps: 263 | Train Loss: 0.4415686 Vali Loss: 1.0152212 Test Loss: 0.4955536
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4724044
	speed: 0.0856s/iter; left time: 1791.8621s
	iters: 200, epoch: 21 | loss: 0.4759518
	speed: 0.0171s/iter; left time: 355.4896s
Epoch: 21 cost time: 5.0723161697387695
Epoch: 21, Steps: 263 | Train Loss: 0.4416430 Vali Loss: 1.0158231 Test Loss: 0.4957036
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4161780
	speed: 0.0823s/iter; left time: 1701.7756s
	iters: 200, epoch: 22 | loss: 0.4580772
	speed: 0.0167s/iter; left time: 343.3515s
Epoch: 22 cost time: 5.367259979248047
Epoch: 22, Steps: 263 | Train Loss: 0.4415929 Vali Loss: 1.0159746 Test Loss: 0.4956005
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4496703
	speed: 0.0848s/iter; left time: 1731.1752s
	iters: 200, epoch: 23 | loss: 0.4634273
	speed: 0.0165s/iter; left time: 336.0682s
Epoch: 23 cost time: 4.961519956588745
Epoch: 23, Steps: 263 | Train Loss: 0.4417798 Vali Loss: 1.0154381 Test Loss: 0.4954804
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.3950930
	speed: 0.0849s/iter; left time: 1709.9956s
	iters: 200, epoch: 24 | loss: 0.4668165
	speed: 0.0167s/iter; left time: 334.2325s
Epoch: 24 cost time: 5.13996148109436
Epoch: 24, Steps: 263 | Train Loss: 0.4416462 Vali Loss: 1.0152155 Test Loss: 0.4956587
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4544752
	speed: 0.0836s/iter; left time: 1663.1657s
	iters: 200, epoch: 25 | loss: 0.4581128
	speed: 0.0193s/iter; left time: 382.1236s
Epoch: 25 cost time: 6.019180059432983
Epoch: 25, Steps: 263 | Train Loss: 0.4416568 Vali Loss: 1.0153673 Test Loss: 0.4956488
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4685323
	speed: 0.0973s/iter; left time: 1910.5288s
	iters: 200, epoch: 26 | loss: 0.4173779
	speed: 0.0172s/iter; left time: 335.8185s
Epoch: 26 cost time: 5.5246734619140625
Epoch: 26, Steps: 263 | Train Loss: 0.4415743 Vali Loss: 1.0154345 Test Loss: 0.4956236
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4623930
	speed: 0.0968s/iter; left time: 1873.9470s
	iters: 200, epoch: 27 | loss: 0.4324965
	speed: 0.0174s/iter; left time: 335.6641s
Epoch: 27 cost time: 5.966572523117065
Epoch: 27, Steps: 263 | Train Loss: 0.4415860 Vali Loss: 1.0149535 Test Loss: 0.4957601
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4330056
	speed: 0.0885s/iter; left time: 1690.0078s
	iters: 200, epoch: 28 | loss: 0.4323813
	speed: 0.0172s/iter; left time: 327.4650s
Epoch: 28 cost time: 5.214107990264893
Epoch: 28, Steps: 263 | Train Loss: 0.4416844 Vali Loss: 1.0157495 Test Loss: 0.4957751
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4286281
	speed: 0.0853s/iter; left time: 1605.9509s
	iters: 200, epoch: 29 | loss: 0.4589671
	speed: 0.0173s/iter; left time: 324.4988s
Epoch: 29 cost time: 5.263733148574829
Epoch: 29, Steps: 263 | Train Loss: 0.4415274 Vali Loss: 1.0160532 Test Loss: 0.4956677
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4275638
	speed: 0.0940s/iter; left time: 1746.3421s
	iters: 200, epoch: 30 | loss: 0.4547201
	speed: 0.0177s/iter; left time: 327.7025s
Epoch: 30 cost time: 6.447120189666748
Epoch: 30, Steps: 263 | Train Loss: 0.4415840 Vali Loss: 1.0164733 Test Loss: 0.4958088
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4712988
	speed: 0.0934s/iter; left time: 1709.8347s
	iters: 200, epoch: 31 | loss: 0.4177141
	speed: 0.0169s/iter; left time: 308.2157s
Epoch: 31 cost time: 5.706097841262817
Epoch: 31, Steps: 263 | Train Loss: 0.4416348 Vali Loss: 1.0158259 Test Loss: 0.4958527
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4352312
	speed: 0.0888s/iter; left time: 1602.6083s
	iters: 200, epoch: 32 | loss: 0.4525750
	speed: 0.0163s/iter; left time: 293.3046s
Epoch: 32 cost time: 5.05694055557251
Epoch: 32, Steps: 263 | Train Loss: 0.4416748 Vali Loss: 1.0158911 Test Loss: 0.4957905
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4114578
	speed: 0.0808s/iter; left time: 1436.4845s
	iters: 200, epoch: 33 | loss: 0.4617230
	speed: 0.0213s/iter; left time: 376.7178s
Epoch: 33 cost time: 5.416922569274902
Epoch: 33, Steps: 263 | Train Loss: 0.4416100 Vali Loss: 1.0154703 Test Loss: 0.4957559
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4943659
	speed: 0.0806s/iter; left time: 1412.1347s
	iters: 200, epoch: 34 | loss: 0.4632420
	speed: 0.0242s/iter; left time: 421.7433s
Epoch: 34 cost time: 5.799215316772461
Epoch: 34, Steps: 263 | Train Loss: 0.4416807 Vali Loss: 1.0151050 Test Loss: 0.4957327
EarlyStopping counter: 20 out of 20
Early stopping
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5114024
	speed: 0.0243s/iter; left time: 635.8023s
	iters: 200, epoch: 1 | loss: 0.5165283
	speed: 0.0178s/iter; left time: 465.4240s
Epoch: 1 cost time: 5.277254343032837
Epoch: 1, Steps: 263 | Train Loss: 0.4939768 Vali Loss: 1.0136248 Test Loss: 0.4942903
Validation loss decreased (inf --> 1.013625).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4790066
	speed: 0.0842s/iter; left time: 2182.7766s
	iters: 200, epoch: 2 | loss: 0.5102001
	speed: 0.0188s/iter; left time: 485.4064s
Epoch: 2 cost time: 5.143707036972046
Epoch: 2, Steps: 263 | Train Loss: 0.4937984 Vali Loss: 1.0129269 Test Loss: 0.4937421
Validation loss decreased (1.013625 --> 1.012927).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4948170
	speed: 0.0973s/iter; left time: 2498.6271s
	iters: 200, epoch: 3 | loss: 0.4886111
	speed: 0.0188s/iter; left time: 480.5404s
Epoch: 3 cost time: 5.334059715270996
Epoch: 3, Steps: 263 | Train Loss: 0.4937764 Vali Loss: 1.0128736 Test Loss: 0.4939775
Validation loss decreased (1.012927 --> 1.012874).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4965491
	speed: 0.0848s/iter; left time: 2155.9458s
	iters: 200, epoch: 4 | loss: 0.4736883
	speed: 0.0184s/iter; left time: 465.7632s
Epoch: 4 cost time: 5.746933460235596
Epoch: 4, Steps: 263 | Train Loss: 0.4936928 Vali Loss: 1.0140387 Test Loss: 0.4943284
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4925780
	speed: 0.0829s/iter; left time: 2083.9656s
	iters: 200, epoch: 5 | loss: 0.4955099
	speed: 0.0180s/iter; left time: 451.3816s
Epoch: 5 cost time: 5.647347688674927
Epoch: 5, Steps: 263 | Train Loss: 0.4936979 Vali Loss: 1.0131661 Test Loss: 0.4943649
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4702857
	speed: 0.0875s/iter; left time: 2176.4896s
	iters: 200, epoch: 6 | loss: 0.4940773
	speed: 0.0157s/iter; left time: 389.8989s
Epoch: 6 cost time: 4.949013710021973
Epoch: 6, Steps: 263 | Train Loss: 0.4936894 Vali Loss: 1.0129377 Test Loss: 0.4940609
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4694679
	speed: 0.0795s/iter; left time: 1958.0196s
	iters: 200, epoch: 7 | loss: 0.4830018
	speed: 0.0187s/iter; left time: 459.1883s
Epoch: 7 cost time: 5.4355902671813965
Epoch: 7, Steps: 263 | Train Loss: 0.4936295 Vali Loss: 1.0146004 Test Loss: 0.4941241
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5248659
	speed: 0.0807s/iter; left time: 1964.6549s
	iters: 200, epoch: 8 | loss: 0.5311892
	speed: 0.0176s/iter; left time: 426.5130s
Epoch: 8 cost time: 5.4178996086120605
Epoch: 8, Steps: 263 | Train Loss: 0.4937630 Vali Loss: 1.0129553 Test Loss: 0.4942707
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4923656
	speed: 0.0849s/iter; left time: 2045.5054s
	iters: 200, epoch: 9 | loss: 0.5064619
	speed: 0.0200s/iter; left time: 480.4990s
Epoch: 9 cost time: 5.489513635635376
Epoch: 9, Steps: 263 | Train Loss: 0.4937074 Vali Loss: 1.0131749 Test Loss: 0.4943386
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4887100
	speed: 0.0896s/iter; left time: 2135.5098s
	iters: 200, epoch: 10 | loss: 0.4923927
	speed: 0.0220s/iter; left time: 522.5050s
Epoch: 10 cost time: 6.199786424636841
Epoch: 10, Steps: 263 | Train Loss: 0.4935075 Vali Loss: 1.0134444 Test Loss: 0.4945557
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.5391038
	speed: 0.0835s/iter; left time: 1968.9221s
	iters: 200, epoch: 11 | loss: 0.4642975
	speed: 0.0164s/iter; left time: 384.5816s
Epoch: 11 cost time: 5.073199272155762
Epoch: 11, Steps: 263 | Train Loss: 0.4935274 Vali Loss: 1.0144259 Test Loss: 0.4946178
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4753766
	speed: 0.0867s/iter; left time: 2019.9148s
	iters: 200, epoch: 12 | loss: 0.5007022
	speed: 0.0184s/iter; left time: 426.3115s
Epoch: 12 cost time: 5.1650261878967285
Epoch: 12, Steps: 263 | Train Loss: 0.4935732 Vali Loss: 1.0137415 Test Loss: 0.4946034
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4664969
	speed: 0.0805s/iter; left time: 1854.1455s
	iters: 200, epoch: 13 | loss: 0.5077081
	speed: 0.0173s/iter; left time: 397.7382s
Epoch: 13 cost time: 5.130582332611084
Epoch: 13, Steps: 263 | Train Loss: 0.4935574 Vali Loss: 1.0135355 Test Loss: 0.4947179
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4710348
	speed: 0.0821s/iter; left time: 1871.0435s
	iters: 200, epoch: 14 | loss: 0.5013399
	speed: 0.0170s/iter; left time: 386.0616s
Epoch: 14 cost time: 4.822453260421753
Epoch: 14, Steps: 263 | Train Loss: 0.4934272 Vali Loss: 1.0131433 Test Loss: 0.4945277
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.5164108
	speed: 0.0792s/iter; left time: 1784.4452s
	iters: 200, epoch: 15 | loss: 0.5269679
	speed: 0.0165s/iter; left time: 369.0917s
Epoch: 15 cost time: 4.887744903564453
Epoch: 15, Steps: 263 | Train Loss: 0.4937096 Vali Loss: 1.0136335 Test Loss: 0.4947661
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.5293757
	speed: 0.0829s/iter; left time: 1844.6813s
	iters: 200, epoch: 16 | loss: 0.5380374
	speed: 0.0189s/iter; left time: 418.5260s
Epoch: 16 cost time: 5.242014408111572
Epoch: 16, Steps: 263 | Train Loss: 0.4935002 Vali Loss: 1.0144156 Test Loss: 0.4947444
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.5074033
	speed: 0.0899s/iter; left time: 1976.6752s
	iters: 200, epoch: 17 | loss: 0.5432196
	speed: 0.0171s/iter; left time: 373.5156s
Epoch: 17 cost time: 5.056338548660278
Epoch: 17, Steps: 263 | Train Loss: 0.4934299 Vali Loss: 1.0137846 Test Loss: 0.4945581
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.5241368
	speed: 0.0873s/iter; left time: 1896.7468s
	iters: 200, epoch: 18 | loss: 0.4811400
	speed: 0.0185s/iter; left time: 399.4168s
Epoch: 18 cost time: 5.575010061264038
Epoch: 18, Steps: 263 | Train Loss: 0.4935908 Vali Loss: 1.0139205 Test Loss: 0.4946846
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5153787
	speed: 0.0892s/iter; left time: 1915.4432s
	iters: 200, epoch: 19 | loss: 0.4679942
	speed: 0.0164s/iter; left time: 351.4832s
Epoch: 19 cost time: 5.528200387954712
Epoch: 19, Steps: 263 | Train Loss: 0.4935225 Vali Loss: 1.0130218 Test Loss: 0.4943092
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5319340
	speed: 0.0823s/iter; left time: 1744.2544s
	iters: 200, epoch: 20 | loss: 0.5197128
	speed: 0.0235s/iter; left time: 496.3699s
Epoch: 20 cost time: 5.738894701004028
Epoch: 20, Steps: 263 | Train Loss: 0.4934889 Vali Loss: 1.0131024 Test Loss: 0.4945143
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5179567
	speed: 0.0853s/iter; left time: 1785.3618s
	iters: 200, epoch: 21 | loss: 0.5011527
	speed: 0.0166s/iter; left time: 346.8229s
Epoch: 21 cost time: 5.036823749542236
Epoch: 21, Steps: 263 | Train Loss: 0.4935218 Vali Loss: 1.0134302 Test Loss: 0.4945603
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.5100183
	speed: 0.0817s/iter; left time: 1689.8399s
	iters: 200, epoch: 22 | loss: 0.4652494
	speed: 0.0190s/iter; left time: 391.9529s
Epoch: 22 cost time: 5.37715482711792
Epoch: 22, Steps: 263 | Train Loss: 0.4933692 Vali Loss: 1.0126306 Test Loss: 0.4948203
Validation loss decreased (1.012874 --> 1.012631).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.5024812
	speed: 0.0897s/iter; left time: 1831.5230s
	iters: 200, epoch: 23 | loss: 0.4493573
	speed: 0.0162s/iter; left time: 328.4527s
Epoch: 23 cost time: 5.8861682415008545
Epoch: 23, Steps: 263 | Train Loss: 0.4934314 Vali Loss: 1.0128908 Test Loss: 0.4945880
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5205833
	speed: 0.0895s/iter; left time: 1802.6519s
	iters: 200, epoch: 24 | loss: 0.5036386
	speed: 0.0195s/iter; left time: 391.3950s
Epoch: 24 cost time: 5.573055982589722
Epoch: 24, Steps: 263 | Train Loss: 0.4933402 Vali Loss: 1.0137198 Test Loss: 0.4947496
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.5210192
	speed: 0.0860s/iter; left time: 1709.7489s
	iters: 200, epoch: 25 | loss: 0.4956399
	speed: 0.0185s/iter; left time: 366.4834s
Epoch: 25 cost time: 5.492060899734497
Epoch: 25, Steps: 263 | Train Loss: 0.4934883 Vali Loss: 1.0142393 Test Loss: 0.4948038
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4852502
	speed: 0.0866s/iter; left time: 1699.5011s
	iters: 200, epoch: 26 | loss: 0.5001091
	speed: 0.0237s/iter; left time: 461.8994s
Epoch: 26 cost time: 5.967595338821411
Epoch: 26, Steps: 263 | Train Loss: 0.4934915 Vali Loss: 1.0140063 Test Loss: 0.4947271
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4836082
	speed: 0.0855s/iter; left time: 1656.1781s
	iters: 200, epoch: 27 | loss: 0.4930515
	speed: 0.0179s/iter; left time: 345.3019s
Epoch: 27 cost time: 5.335541248321533
Epoch: 27, Steps: 263 | Train Loss: 0.4936402 Vali Loss: 1.0133183 Test Loss: 0.4947224
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.5029899
	speed: 0.0851s/iter; left time: 1625.8073s
	iters: 200, epoch: 28 | loss: 0.5739787
	speed: 0.0170s/iter; left time: 322.4945s
Epoch: 28 cost time: 5.29499888420105
Epoch: 28, Steps: 263 | Train Loss: 0.4933964 Vali Loss: 1.0124639 Test Loss: 0.4946775
Validation loss decreased (1.012631 --> 1.012464).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4981991
	speed: 0.0817s/iter; left time: 1538.6068s
	iters: 200, epoch: 29 | loss: 0.4618422
	speed: 0.0172s/iter; left time: 322.8940s
Epoch: 29 cost time: 5.2401111125946045
Epoch: 29, Steps: 263 | Train Loss: 0.4934964 Vali Loss: 1.0139463 Test Loss: 0.4947718
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4788994
	speed: 0.0830s/iter; left time: 1541.6481s
	iters: 200, epoch: 30 | loss: 0.4954732
	speed: 0.0175s/iter; left time: 323.8563s
Epoch: 30 cost time: 5.097549676895142
Epoch: 30, Steps: 263 | Train Loss: 0.4933071 Vali Loss: 1.0146011 Test Loss: 0.4948309
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4950277
	speed: 0.0791s/iter; left time: 1448.7971s
	iters: 200, epoch: 31 | loss: 0.4863072
	speed: 0.0162s/iter; left time: 295.6694s
Epoch: 31 cost time: 4.840894460678101
Epoch: 31, Steps: 263 | Train Loss: 0.4934569 Vali Loss: 1.0134231 Test Loss: 0.4947353
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4923522
	speed: 0.0794s/iter; left time: 1432.4365s
	iters: 200, epoch: 32 | loss: 0.4588990
	speed: 0.0166s/iter; left time: 298.1170s
Epoch: 32 cost time: 4.98327112197876
Epoch: 32, Steps: 263 | Train Loss: 0.4934628 Vali Loss: 1.0136400 Test Loss: 0.4947115
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4803264
	speed: 0.0820s/iter; left time: 1458.3419s
	iters: 200, epoch: 33 | loss: 0.4500622
	speed: 0.0167s/iter; left time: 294.8944s
Epoch: 33 cost time: 4.9601054191589355
Epoch: 33, Steps: 263 | Train Loss: 0.4935696 Vali Loss: 1.0129665 Test Loss: 0.4947476
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.5108584
	speed: 0.0864s/iter; left time: 1514.3652s
	iters: 200, epoch: 34 | loss: 0.4917352
	speed: 0.0161s/iter; left time: 280.8492s
Epoch: 34 cost time: 5.292822360992432
Epoch: 34, Steps: 263 | Train Loss: 0.4934291 Vali Loss: 1.0145017 Test Loss: 0.4948181
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.4743622
	speed: 0.0896s/iter; left time: 1545.7699s
	iters: 200, epoch: 35 | loss: 0.4550993
	speed: 0.0226s/iter; left time: 388.5256s
Epoch: 35 cost time: 5.803073883056641
Epoch: 35, Steps: 263 | Train Loss: 0.4935321 Vali Loss: 1.0142059 Test Loss: 0.4948044
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.5040808
	speed: 0.0793s/iter; left time: 1347.0606s
	iters: 200, epoch: 36 | loss: 0.4986629
	speed: 0.0170s/iter; left time: 286.9133s
Epoch: 36 cost time: 6.236710071563721
Epoch: 36, Steps: 263 | Train Loss: 0.4934342 Vali Loss: 1.0137349 Test Loss: 0.4947933
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.5020873
	speed: 0.0942s/iter; left time: 1576.6768s
	iters: 200, epoch: 37 | loss: 0.4815824
	speed: 0.0196s/iter; left time: 326.7615s
Epoch: 37 cost time: 5.654195070266724
Epoch: 37, Steps: 263 | Train Loss: 0.4934416 Vali Loss: 1.0147123 Test Loss: 0.4948307
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.5210904
	speed: 0.0849s/iter; left time: 1398.3820s
	iters: 200, epoch: 38 | loss: 0.5117226
	speed: 0.0187s/iter; left time: 306.3729s
Epoch: 38 cost time: 5.586533308029175
Epoch: 38, Steps: 263 | Train Loss: 0.4934085 Vali Loss: 1.0127022 Test Loss: 0.4947672
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.5250580
	speed: 0.0894s/iter; left time: 1448.2744s
	iters: 200, epoch: 39 | loss: 0.5094483
	speed: 0.0223s/iter; left time: 358.4732s
Epoch: 39 cost time: 5.611186265945435
Epoch: 39, Steps: 263 | Train Loss: 0.4933660 Vali Loss: 1.0136745 Test Loss: 0.4948101
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4712179
	speed: 0.0818s/iter; left time: 1304.8430s
	iters: 200, epoch: 40 | loss: 0.4872372
	speed: 0.0159s/iter; left time: 252.6359s
Epoch: 40 cost time: 4.941836595535278
Epoch: 40, Steps: 263 | Train Loss: 0.4932931 Vali Loss: 1.0143007 Test Loss: 0.4948967
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.5129809
	speed: 0.0763s/iter; left time: 1196.6699s
	iters: 200, epoch: 41 | loss: 0.5653736
	speed: 0.0162s/iter; left time: 253.0219s
Epoch: 41 cost time: 4.837143421173096
Epoch: 41, Steps: 263 | Train Loss: 0.4933410 Vali Loss: 1.0132780 Test Loss: 0.4948627
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.5035426
	speed: 0.0796s/iter; left time: 1227.9913s
	iters: 200, epoch: 42 | loss: 0.4703648
	speed: 0.0159s/iter; left time: 242.9139s
Epoch: 42 cost time: 4.826301097869873
Epoch: 42, Steps: 263 | Train Loss: 0.4933495 Vali Loss: 1.0139644 Test Loss: 0.4948639
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.4883671
	speed: 0.0807s/iter; left time: 1222.2538s
	iters: 200, epoch: 43 | loss: 0.4735773
	speed: 0.0238s/iter; left time: 357.5942s
Epoch: 43 cost time: 6.473501920700073
Epoch: 43, Steps: 263 | Train Loss: 0.4934295 Vali Loss: 1.0133963 Test Loss: 0.4948430
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.4875119
	speed: 0.0983s/iter; left time: 1464.5164s
	iters: 200, epoch: 44 | loss: 0.4732208
	speed: 0.0166s/iter; left time: 245.5830s
Epoch: 44 cost time: 5.995344161987305
Epoch: 44, Steps: 263 | Train Loss: 0.4933192 Vali Loss: 1.0139620 Test Loss: 0.4948345
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.5131531
	speed: 0.0803s/iter; left time: 1174.6007s
	iters: 200, epoch: 45 | loss: 0.5337161
	speed: 0.0248s/iter; left time: 360.9593s
Epoch: 45 cost time: 5.957769870758057
Epoch: 45, Steps: 263 | Train Loss: 0.4934306 Vali Loss: 1.0139024 Test Loss: 0.4948562
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.4869850
	speed: 0.0841s/iter; left time: 1208.2135s
	iters: 200, epoch: 46 | loss: 0.4548452
	speed: 0.0170s/iter; left time: 242.4224s
Epoch: 46 cost time: 5.028826951980591
Epoch: 46, Steps: 263 | Train Loss: 0.4933556 Vali Loss: 1.0141020 Test Loss: 0.4948444
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.5012643
	speed: 0.0840s/iter; left time: 1185.0767s
	iters: 200, epoch: 47 | loss: 0.4683397
	speed: 0.0168s/iter; left time: 234.9666s
Epoch: 47 cost time: 5.573727369308472
Epoch: 47, Steps: 263 | Train Loss: 0.4934909 Vali Loss: 1.0131606 Test Loss: 0.4948680
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.4790090
	speed: 0.0858s/iter; left time: 1187.6333s
	iters: 200, epoch: 48 | loss: 0.4697450
	speed: 0.0170s/iter; left time: 233.7100s
Epoch: 48 cost time: 5.338750600814819
Epoch: 48, Steps: 263 | Train Loss: 0.4934254 Vali Loss: 1.0141051 Test Loss: 0.4948479
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.4927498400211334, mae:0.45305684208869934, rse:0.6678577661514282, corr:[0.5305616  0.53080714 0.52591616 0.52321625 0.52192754 0.51967424
 0.5163006  0.5136646  0.51231784 0.51142156 0.5102592  0.5088504
 0.5074591  0.5056776  0.502909   0.49933708 0.49581    0.49278155
 0.48999286 0.48703894 0.4838892  0.4808305  0.47793284 0.47478375
 0.47121248 0.46736082 0.46398816 0.46150362 0.45964357 0.45771483
 0.45605922 0.45573214 0.4562543  0.4567984  0.45703098 0.45728233
 0.45736244 0.4574419  0.4573859  0.45706907 0.4567997  0.45654997
 0.456522   0.45658374 0.45631608 0.4558532  0.455607   0.4556793
 0.45592675 0.45620665 0.45650086 0.4567223  0.45714098 0.4576107
 0.45821053 0.45871592 0.4591856  0.4595892  0.4596517  0.4595991
 0.4595483  0.45923465 0.45893124 0.45859706 0.45850748 0.45844522
 0.45813942 0.45782274 0.45804596 0.45884058 0.45962328 0.460205
 0.46073833 0.4612566  0.46199977 0.46270663 0.46352074 0.46447808
 0.46538454 0.4661776  0.46684638 0.4673134  0.467821   0.4681707
 0.4684391  0.4686521  0.46900848 0.4696588  0.47063932 0.47184268
 0.47297418 0.47378775 0.47428232 0.47456524 0.47478235 0.47477672
 0.47429788 0.47334534 0.4721176  0.4711356  0.47076446 0.47072452
 0.470539   0.4699673  0.46940362 0.46893686 0.46832335 0.4674901
 0.4666221  0.46590114 0.46522012 0.46436542 0.46331167 0.46227846
 0.4613642  0.46065265 0.45980802 0.45866656 0.4573416  0.45602974
 0.45487705 0.45363066 0.45228726 0.451249   0.45046532 0.44968864
 0.44887978 0.44816753 0.44775173 0.44770992 0.44794372 0.44822502
 0.448144   0.44784418 0.44747263 0.4472396  0.44718993 0.44720104
 0.44706705 0.44681254 0.44659907 0.44660982 0.44672614 0.44683886
 0.4468899  0.44709495 0.4475347  0.44787666 0.44810975 0.44807288
 0.44799182 0.44838706 0.44904655 0.44945458 0.4492425  0.44882724
 0.44863027 0.44881633 0.4490227  0.44895926 0.44878626 0.44870025
 0.44891104 0.44936943 0.4498859  0.4503103  0.4508166  0.4515744
 0.45249394 0.4532797  0.4537854  0.45434153 0.45506874 0.45599794
 0.45682302 0.45741886 0.45782623 0.4581482  0.45835456 0.45861715
 0.4589457  0.45935664 0.45975563 0.46029234 0.4611434  0.46225443
 0.46328264 0.46384308 0.46386632 0.46371245 0.46387935 0.46446142
 0.4650769  0.46552393 0.46587074 0.46643433 0.46729472 0.4681744
 0.46851104 0.4681296  0.46757257 0.46715268 0.46656638 0.4655898
 0.4642627  0.46281776 0.46131805 0.45986542 0.4583551  0.45691097
 0.45551968 0.45418555 0.45287967 0.45138636 0.44956774 0.4477716
 0.4459447  0.44424692 0.44273898 0.4415499  0.4404755  0.43954378
 0.43893057 0.4388324  0.43884006 0.43875757 0.4388871  0.43887964
 0.43864048 0.43841165 0.43822733 0.43795794 0.437701   0.4373542
 0.43699467 0.43670508 0.4363921  0.43637878 0.43673357 0.4368827
 0.4368887  0.43672192 0.43671587 0.43712455 0.43753985 0.4379532
 0.4382856  0.43861896 0.43897146 0.4395609  0.44003445 0.4403872
 0.44035128 0.44012982 0.43972296 0.43972525 0.4398849  0.43998924
 0.44017398 0.44049746 0.4406665  0.44095394 0.44159675 0.4425134
 0.44360918 0.44429314 0.44459894 0.44512475 0.4462543  0.44753757
 0.44867632 0.44939035 0.44991633 0.45039156 0.45093283 0.45146152
 0.45193854 0.4524474  0.45290905 0.4534804  0.45417753 0.45495299
 0.45577186 0.45650783 0.45704052 0.4571151  0.45647606 0.45504752
 0.45303094 0.4510211  0.44945252 0.44822344 0.44711697 0.4462094
 0.44562975 0.44521293 0.4450067  0.44476998 0.4442514  0.44345325
 0.4424884  0.4414397  0.44028485 0.438989   0.4378187  0.43690246
 0.436365   0.4357589  0.4349262  0.43394053 0.4330356  0.43224987
 0.43138567 0.4303491  0.42930079 0.42849457 0.4277448  0.42715898
 0.42666802 0.42628703 0.4261211  0.4261298  0.42612705 0.426001
 0.42557964 0.42516506 0.424892   0.42475665 0.42457458 0.4242072
 0.42398185 0.42386696 0.42384338 0.42370144 0.4235174  0.4233142
 0.4232981  0.42330134 0.42356506 0.42375374 0.4239745  0.42416492
 0.42440814 0.42487633 0.42530444 0.42554882 0.42556375 0.42548928
 0.4254784  0.42549896 0.42556724 0.42557007 0.42551097 0.4255122
 0.42556602 0.4256267  0.42577425 0.4260888  0.42661914 0.4272874
 0.42788357 0.4283329  0.42881414 0.42949712 0.43059027 0.4317769
 0.4327585  0.43366793 0.43454632 0.43528613 0.436045   0.43674797
 0.437566   0.4384875  0.4396935  0.4410987  0.442547   0.44397175
 0.44518846 0.44599083 0.44639343 0.44660187 0.4466678  0.44649675
 0.44608554 0.4457628  0.44592768 0.44657505 0.44743395 0.44836915
 0.44913012 0.44950384 0.44969139 0.44966432 0.44925162 0.44841665
 0.44735736 0.44628084 0.445164   0.44402826 0.44286048 0.44181395
 0.4409414  0.4401873  0.43959358 0.43900776 0.43818316 0.43709338
 0.43589744 0.43445364 0.4331406  0.4325294  0.43228415 0.43204397
 0.43176693 0.43135133 0.43103915 0.43078458 0.43051937 0.4304428
 0.43046963 0.43049407 0.4302498  0.42978385 0.42928082 0.42891902
 0.42864874 0.4284225  0.42793486 0.427654   0.42770404 0.4280589
 0.4284137  0.42849994 0.42851055 0.4286734  0.4289141  0.4292081
 0.42945063 0.4297288  0.4300405  0.43044516 0.4308735  0.4312254
 0.43142235 0.43162054 0.43163204 0.43151936 0.43129057 0.43115076
 0.43113244 0.43138915 0.43170902 0.43202877 0.43221018 0.43249124
 0.43298328 0.43350497 0.4339981  0.43458396 0.4352731  0.43603873
 0.43690178 0.43787205 0.43861362 0.43909404 0.4394788  0.440023
 0.4408538  0.4417835  0.4427258  0.4434856  0.44419134 0.44509527
 0.44609436 0.44687808 0.447147   0.4468819  0.44621348 0.44532275
 0.44421107 0.44304606 0.4418695  0.4409034  0.4402026  0.43992785
 0.439827   0.43967277 0.43956855 0.4393214  0.43868625 0.43763536
 0.43631873 0.43494925 0.433623   0.4322768  0.430825   0.42931354
 0.42788628 0.42678744 0.42595026 0.42499927 0.42391443 0.4226292
 0.42141777 0.4203302  0.4194351  0.41871697 0.41796893 0.4172236
 0.4165528  0.4162669  0.41626883 0.41630098 0.41616157 0.41601813
 0.41589049 0.4159045  0.41593164 0.41552112 0.41487643 0.41437742
 0.41422182 0.41441768 0.41440985 0.4141215  0.41369963 0.41349205
 0.413418   0.4133905  0.41337347 0.41342375 0.41355312 0.4136793
 0.4138226  0.4139135  0.41420418 0.41470736 0.4151681  0.4152334
 0.4151303  0.41493747 0.4147102  0.4144869  0.41426173 0.41426265
 0.41456622 0.41506281 0.4154887  0.4156375  0.41554072 0.4156252
 0.41608146 0.41670948 0.41742632 0.41818705 0.41903904 0.41996646
 0.42098507 0.42208564 0.4230222  0.42361817 0.42409655 0.4247234
 0.4255752  0.42661047 0.4274221  0.4280198  0.42866266 0.42953894
 0.43051767 0.43115976 0.431186   0.4305917  0.42965797 0.42857203
 0.42717463 0.42545435 0.42389584 0.4227442  0.4224027  0.42268836
 0.4230829  0.4232482  0.42335865 0.42342204 0.42311794 0.42224407
 0.42091727 0.4194477  0.4180105  0.4166973  0.41542488 0.4140497
 0.4127357  0.41167843 0.4109204  0.4102033  0.40927655 0.4080688
 0.40677318 0.40553188 0.4045285  0.4037704  0.40297946 0.40224764
 0.4017154  0.40155986 0.40153176 0.40152442 0.40150207 0.40171045
 0.4019097  0.40188837 0.40155414 0.4010382  0.40092808 0.40119913
 0.40164465 0.40183628 0.40145794 0.40093356 0.40081814 0.40119573
 0.4015284  0.40158713 0.40164    0.40168417 0.40190673 0.40229034
 0.4024699  0.4023634  0.40238467 0.4025175  0.4027164  0.40268207
 0.4024308  0.4021293  0.4018702  0.401445   0.40099573 0.4006494
 0.4005006  0.40058184 0.4008473  0.4012867  0.4016679  0.4020945
 0.40259528 0.40302652 0.40335763 0.4039038  0.40466997 0.40564686
 0.4065108  0.40727386 0.4079368  0.40858394 0.40928507 0.410023
 0.4106767  0.4112134  0.4119157  0.41289386 0.41407633 0.41522852
 0.41602972 0.41635302 0.41625997 0.41582847 0.4150343  0.4137582
 0.41203105 0.4104778  0.4097256  0.40959528 0.4097117  0.4099142
 0.41026852 0.4108184  0.4114542  0.41165218 0.4110989  0.4099997
 0.40885153 0.40786755 0.40677208 0.40510455 0.40296638 0.4010383
 0.40004137 0.39961758 0.39885566 0.39728123 0.39530158 0.393811
 0.39297688 0.39222357 0.39096835 0.38952696 0.38859478 0.3882501
 0.38781363 0.38667393 0.38552514 0.38518214 0.38554156 0.3859801
 0.38565952 0.38487247 0.38471374 0.3853989  0.38589457 0.38552654
 0.38521338 0.38628638 0.3879545  0.38830528 0.3882592  0.3918021 ]
