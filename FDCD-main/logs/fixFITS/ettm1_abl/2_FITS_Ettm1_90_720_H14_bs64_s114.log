Args in experiment:
Namespace(H_order=14, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=24, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H14_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=24, out_features=216, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4644864.0
params:  5400.0
Trainable parameters:  5400
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.9741027
	speed: 0.0277s/iter; left time: 725.7170s
	iters: 200, epoch: 1 | loss: 0.7562035
	speed: 0.0262s/iter; left time: 684.2594s
Epoch: 1 cost time: 6.875135183334351
Epoch: 1, Steps: 263 | Train Loss: 0.9362047 Vali Loss: 1.3646686 Test Loss: 0.8215088
Validation loss decreased (inf --> 1.364669).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5321582
	speed: 0.1496s/iter; left time: 3880.2937s
	iters: 200, epoch: 2 | loss: 0.4962235
	speed: 0.0486s/iter; left time: 1256.1759s
Epoch: 2 cost time: 11.806394577026367
Epoch: 2, Steps: 263 | Train Loss: 0.5271747 Vali Loss: 1.1109263 Test Loss: 0.5760040
Validation loss decreased (1.364669 --> 1.110926).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4507728
	speed: 0.1583s/iter; left time: 4063.9187s
	iters: 200, epoch: 3 | loss: 0.4970105
	speed: 0.0393s/iter; left time: 1004.8059s
Epoch: 3 cost time: 10.347495079040527
Epoch: 3, Steps: 263 | Train Loss: 0.4640780 Vali Loss: 1.0571554 Test Loss: 0.5276825
Validation loss decreased (1.110926 --> 1.057155).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4686525
	speed: 0.1330s/iter; left time: 3380.5516s
	iters: 200, epoch: 4 | loss: 0.4306762
	speed: 0.0248s/iter; left time: 627.6696s
Epoch: 4 cost time: 8.341856241226196
Epoch: 4, Steps: 263 | Train Loss: 0.4499927 Vali Loss: 1.0358485 Test Loss: 0.5101438
Validation loss decreased (1.057155 --> 1.035848).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4491390
	speed: 0.1244s/iter; left time: 3127.9064s
	iters: 200, epoch: 5 | loss: 0.4640975
	speed: 0.0257s/iter; left time: 643.9147s
Epoch: 5 cost time: 7.974161148071289
Epoch: 5, Steps: 263 | Train Loss: 0.4447777 Vali Loss: 1.0251299 Test Loss: 0.5018041
Validation loss decreased (1.035848 --> 1.025130).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4211553
	speed: 0.1281s/iter; left time: 3187.2816s
	iters: 200, epoch: 6 | loss: 0.4337043
	speed: 0.0292s/iter; left time: 722.6468s
Epoch: 6 cost time: 7.624542236328125
Epoch: 6, Steps: 263 | Train Loss: 0.4426720 Vali Loss: 1.0205706 Test Loss: 0.4981762
Validation loss decreased (1.025130 --> 1.020571).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4607939
	speed: 0.1278s/iter; left time: 3147.4949s
	iters: 200, epoch: 7 | loss: 0.4607859
	speed: 0.0276s/iter; left time: 675.6665s
Epoch: 7 cost time: 7.0350096225738525
Epoch: 7, Steps: 263 | Train Loss: 0.4416301 Vali Loss: 1.0169103 Test Loss: 0.4961894
Validation loss decreased (1.020571 --> 1.016910).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4382490
	speed: 0.1269s/iter; left time: 3091.2547s
	iters: 200, epoch: 8 | loss: 0.4508180
	speed: 0.0279s/iter; left time: 676.7638s
Epoch: 8 cost time: 8.828836679458618
Epoch: 8, Steps: 263 | Train Loss: 0.4412665 Vali Loss: 1.0153238 Test Loss: 0.4952791
Validation loss decreased (1.016910 --> 1.015324).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4423382
	speed: 0.1334s/iter; left time: 3214.3111s
	iters: 200, epoch: 9 | loss: 0.4535693
	speed: 0.0308s/iter; left time: 738.7031s
Epoch: 9 cost time: 8.626758575439453
Epoch: 9, Steps: 263 | Train Loss: 0.4411563 Vali Loss: 1.0148787 Test Loss: 0.4947477
Validation loss decreased (1.015324 --> 1.014879).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4421587
	speed: 0.1430s/iter; left time: 3407.9640s
	iters: 200, epoch: 10 | loss: 0.4366636
	speed: 0.0319s/iter; left time: 755.9971s
Epoch: 10 cost time: 9.284362077713013
Epoch: 10, Steps: 263 | Train Loss: 0.4408443 Vali Loss: 1.0138086 Test Loss: 0.4945759
Validation loss decreased (1.014879 --> 1.013809).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4304254
	speed: 0.1574s/iter; left time: 3709.5294s
	iters: 200, epoch: 11 | loss: 0.4812701
	speed: 0.0272s/iter; left time: 639.1327s
Epoch: 11 cost time: 8.452141523361206
Epoch: 11, Steps: 263 | Train Loss: 0.4410137 Vali Loss: 1.0141646 Test Loss: 0.4948525
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4066876
	speed: 0.1202s/iter; left time: 2802.1440s
	iters: 200, epoch: 12 | loss: 0.4358530
	speed: 0.0243s/iter; left time: 562.8711s
Epoch: 12 cost time: 7.016040563583374
Epoch: 12, Steps: 263 | Train Loss: 0.4409434 Vali Loss: 1.0146929 Test Loss: 0.4946628
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4754149
	speed: 0.1241s/iter; left time: 2860.9503s
	iters: 200, epoch: 13 | loss: 0.3951449
	speed: 0.0294s/iter; left time: 674.1609s
Epoch: 13 cost time: 8.866841316223145
Epoch: 13, Steps: 263 | Train Loss: 0.4409543 Vali Loss: 1.0149143 Test Loss: 0.4946520
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4384655
	speed: 0.1406s/iter; left time: 3203.1506s
	iters: 200, epoch: 14 | loss: 0.4235731
	speed: 0.0287s/iter; left time: 651.0488s
Epoch: 14 cost time: 8.343605518341064
Epoch: 14, Steps: 263 | Train Loss: 0.4408908 Vali Loss: 1.0150458 Test Loss: 0.4948306
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4322076
	speed: 0.1335s/iter; left time: 3005.3657s
	iters: 200, epoch: 15 | loss: 0.4652757
	speed: 0.0267s/iter; left time: 598.6478s
Epoch: 15 cost time: 8.659745693206787
Epoch: 15, Steps: 263 | Train Loss: 0.4408827 Vali Loss: 1.0147873 Test Loss: 0.4943774
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4050167
	speed: 0.1364s/iter; left time: 3035.5402s
	iters: 200, epoch: 16 | loss: 0.4920536
	speed: 0.0283s/iter; left time: 626.1337s
Epoch: 16 cost time: 7.9079506397247314
Epoch: 16, Steps: 263 | Train Loss: 0.4407368 Vali Loss: 1.0148370 Test Loss: 0.4948111
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4423548
	speed: 0.1112s/iter; left time: 2444.5491s
	iters: 200, epoch: 17 | loss: 0.4287091
	speed: 0.0252s/iter; left time: 552.1767s
Epoch: 17 cost time: 7.295475482940674
Epoch: 17, Steps: 263 | Train Loss: 0.4409331 Vali Loss: 1.0152344 Test Loss: 0.4949310
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4545240
	speed: 0.1718s/iter; left time: 3732.8883s
	iters: 200, epoch: 18 | loss: 0.4412324
	speed: 0.0333s/iter; left time: 720.4941s
Epoch: 18 cost time: 9.89490818977356
Epoch: 18, Steps: 263 | Train Loss: 0.4408273 Vali Loss: 1.0152010 Test Loss: 0.4946385
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.4267452
	speed: 0.1590s/iter; left time: 3413.5641s
	iters: 200, epoch: 19 | loss: 0.4521728
	speed: 0.0251s/iter; left time: 536.0632s
Epoch: 19 cost time: 9.765507459640503
Epoch: 19, Steps: 263 | Train Loss: 0.4408939 Vali Loss: 1.0149533 Test Loss: 0.4950857
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4581891
	speed: 0.1430s/iter; left time: 3032.2983s
	iters: 200, epoch: 20 | loss: 0.4313537
	speed: 0.0445s/iter; left time: 939.1019s
Epoch: 20 cost time: 10.930026531219482
Epoch: 20, Steps: 263 | Train Loss: 0.4409242 Vali Loss: 1.0148332 Test Loss: 0.4947688
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4242217
	speed: 0.1270s/iter; left time: 2660.4241s
	iters: 200, epoch: 21 | loss: 0.4447507
	speed: 0.0215s/iter; left time: 448.1051s
Epoch: 21 cost time: 6.227237701416016
Epoch: 21, Steps: 263 | Train Loss: 0.4407877 Vali Loss: 1.0142972 Test Loss: 0.4949616
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4556911
	speed: 0.1173s/iter; left time: 2424.7382s
	iters: 200, epoch: 22 | loss: 0.4090939
	speed: 0.0302s/iter; left time: 621.2520s
Epoch: 22 cost time: 7.558410167694092
Epoch: 22, Steps: 263 | Train Loss: 0.4408824 Vali Loss: 1.0146708 Test Loss: 0.4949167
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4333465
	speed: 0.1253s/iter; left time: 2558.3624s
	iters: 200, epoch: 23 | loss: 0.4548914
	speed: 0.0248s/iter; left time: 504.3561s
Epoch: 23 cost time: 8.048686504364014
Epoch: 23, Steps: 263 | Train Loss: 0.4408412 Vali Loss: 1.0154232 Test Loss: 0.4948777
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4779481
	speed: 0.1192s/iter; left time: 2402.9930s
	iters: 200, epoch: 24 | loss: 0.4059310
	speed: 0.0277s/iter; left time: 556.3702s
Epoch: 24 cost time: 8.374640464782715
Epoch: 24, Steps: 263 | Train Loss: 0.4406742 Vali Loss: 1.0156318 Test Loss: 0.4950443
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.4030191
	speed: 0.1347s/iter; left time: 2678.3082s
	iters: 200, epoch: 25 | loss: 0.4516888
	speed: 0.0273s/iter; left time: 540.5508s
Epoch: 25 cost time: 8.97786545753479
Epoch: 25, Steps: 263 | Train Loss: 0.4407500 Vali Loss: 1.0150219 Test Loss: 0.4950424
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4347983
	speed: 0.1305s/iter; left time: 2561.9531s
	iters: 200, epoch: 26 | loss: 0.4474813
	speed: 0.0453s/iter; left time: 884.3000s
Epoch: 26 cost time: 9.285661697387695
Epoch: 26, Steps: 263 | Train Loss: 0.4408338 Vali Loss: 1.0154521 Test Loss: 0.4951539
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.4076182
	speed: 0.1137s/iter; left time: 2200.6879s
	iters: 200, epoch: 27 | loss: 0.4104216
	speed: 0.0240s/iter; left time: 461.6809s
Epoch: 27 cost time: 7.357472658157349
Epoch: 27, Steps: 263 | Train Loss: 0.4408166 Vali Loss: 1.0161397 Test Loss: 0.4951208
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4454427
	speed: 0.1363s/iter; left time: 2602.8394s
	iters: 200, epoch: 28 | loss: 0.4150961
	speed: 0.0241s/iter; left time: 458.5589s
Epoch: 28 cost time: 8.046013832092285
Epoch: 28, Steps: 263 | Train Loss: 0.4406564 Vali Loss: 1.0145189 Test Loss: 0.4951899
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4427334
	speed: 0.1479s/iter; left time: 2786.4908s
	iters: 200, epoch: 29 | loss: 0.4548241
	speed: 0.0420s/iter; left time: 787.4450s
Epoch: 29 cost time: 10.89386773109436
Epoch: 29, Steps: 263 | Train Loss: 0.4406294 Vali Loss: 1.0148084 Test Loss: 0.4951422
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4061352
	speed: 0.1684s/iter; left time: 3128.3272s
	iters: 200, epoch: 30 | loss: 0.4465030
	speed: 0.0277s/iter; left time: 512.2793s
Epoch: 30 cost time: 8.585063695907593
Epoch: 30, Steps: 263 | Train Loss: 0.4406730 Vali Loss: 1.0148340 Test Loss: 0.4951348
EarlyStopping counter: 20 out of 20
Early stopping
train 33751
val 10801
test 10801
Model(
  (freq_upsampler): Linear(in_features=24, out_features=216, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4644864.0
params:  5400.0
Trainable parameters:  5400
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.5075000
	speed: 0.0322s/iter; left time: 843.3477s
	iters: 200, epoch: 1 | loss: 0.4853167
	speed: 0.0464s/iter; left time: 1211.5255s
Epoch: 1 cost time: 10.239215850830078
Epoch: 1, Steps: 263 | Train Loss: 0.4936482 Vali Loss: 1.0126375 Test Loss: 0.4930261
Validation loss decreased (inf --> 1.012637).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4763032
	speed: 0.1262s/iter; left time: 3272.4903s
	iters: 200, epoch: 2 | loss: 0.4876961
	speed: 0.0203s/iter; left time: 525.6005s
Epoch: 2 cost time: 6.659236907958984
Epoch: 2, Steps: 263 | Train Loss: 0.4932770 Vali Loss: 1.0125163 Test Loss: 0.4935332
Validation loss decreased (1.012637 --> 1.012516).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4547599
	speed: 0.1161s/iter; left time: 2981.8410s
	iters: 200, epoch: 3 | loss: 0.5051940
	speed: 0.0345s/iter; left time: 881.7406s
Epoch: 3 cost time: 9.743207931518555
Epoch: 3, Steps: 263 | Train Loss: 0.4932382 Vali Loss: 1.0122125 Test Loss: 0.4933536
Validation loss decreased (1.012516 --> 1.012213).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4864671
	speed: 0.1445s/iter; left time: 3671.9333s
	iters: 200, epoch: 4 | loss: 0.5193264
	speed: 0.0400s/iter; left time: 1012.5989s
Epoch: 4 cost time: 10.08972954750061
Epoch: 4, Steps: 263 | Train Loss: 0.4931592 Vali Loss: 1.0112721 Test Loss: 0.4931002
Validation loss decreased (1.012213 --> 1.011272).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.5269195
	speed: 0.1474s/iter; left time: 3708.0235s
	iters: 200, epoch: 5 | loss: 0.5035935
	speed: 0.0366s/iter; left time: 917.3833s
Epoch: 5 cost time: 9.413563251495361
Epoch: 5, Steps: 263 | Train Loss: 0.4933568 Vali Loss: 1.0117497 Test Loss: 0.4935067
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.5054138
	speed: 0.1084s/iter; left time: 2697.5757s
	iters: 200, epoch: 6 | loss: 0.5017055
	speed: 0.0373s/iter; left time: 925.1009s
Epoch: 6 cost time: 9.970961809158325
Epoch: 6, Steps: 263 | Train Loss: 0.4932558 Vali Loss: 1.0124100 Test Loss: 0.4934478
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4953791
	speed: 0.1314s/iter; left time: 3235.7634s
	iters: 200, epoch: 7 | loss: 0.5179411
	speed: 0.0376s/iter; left time: 922.6511s
Epoch: 7 cost time: 8.723371267318726
Epoch: 7, Steps: 263 | Train Loss: 0.4931582 Vali Loss: 1.0123322 Test Loss: 0.4936738
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.5338714
	speed: 0.1214s/iter; left time: 2957.8298s
	iters: 200, epoch: 8 | loss: 0.4784695
	speed: 0.0213s/iter; left time: 517.9172s
Epoch: 8 cost time: 6.569180488586426
Epoch: 8, Steps: 263 | Train Loss: 0.4932979 Vali Loss: 1.0130521 Test Loss: 0.4936168
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4402599
	speed: 0.1321s/iter; left time: 3183.6290s
	iters: 200, epoch: 9 | loss: 0.5419631
	speed: 0.0305s/iter; left time: 730.7091s
Epoch: 9 cost time: 11.128943920135498
Epoch: 9, Steps: 263 | Train Loss: 0.4931665 Vali Loss: 1.0134276 Test Loss: 0.4935215
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4953970
	speed: 0.1623s/iter; left time: 3867.5352s
	iters: 200, epoch: 10 | loss: 0.4859453
	speed: 0.0330s/iter; left time: 782.6759s
Epoch: 10 cost time: 9.634092807769775
Epoch: 10, Steps: 263 | Train Loss: 0.4931519 Vali Loss: 1.0117724 Test Loss: 0.4935972
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4811958
	speed: 0.1328s/iter; left time: 3131.3698s
	iters: 200, epoch: 11 | loss: 0.4739773
	speed: 0.0393s/iter; left time: 921.6672s
Epoch: 11 cost time: 9.761285781860352
Epoch: 11, Steps: 263 | Train Loss: 0.4931274 Vali Loss: 1.0132501 Test Loss: 0.4938000
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4737998
	speed: 0.1303s/iter; left time: 3036.5324s
	iters: 200, epoch: 12 | loss: 0.5223616
	speed: 0.0297s/iter; left time: 689.4836s
Epoch: 12 cost time: 7.778382301330566
Epoch: 12, Steps: 263 | Train Loss: 0.4932411 Vali Loss: 1.0117867 Test Loss: 0.4936499
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.5291492
	speed: 0.1180s/iter; left time: 2719.8180s
	iters: 200, epoch: 13 | loss: 0.4781952
	speed: 0.0291s/iter; left time: 667.9067s
Epoch: 13 cost time: 7.15777587890625
Epoch: 13, Steps: 263 | Train Loss: 0.4930179 Vali Loss: 1.0118059 Test Loss: 0.4937149
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4865648
	speed: 0.1177s/iter; left time: 2682.0051s
	iters: 200, epoch: 14 | loss: 0.5347670
	speed: 0.0338s/iter; left time: 766.9985s
Epoch: 14 cost time: 9.43656039237976
Epoch: 14, Steps: 263 | Train Loss: 0.4930242 Vali Loss: 1.0121977 Test Loss: 0.4937923
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.4861974
	speed: 0.1463s/iter; left time: 3295.3070s
	iters: 200, epoch: 15 | loss: 0.5064484
	speed: 0.0266s/iter; left time: 596.9137s
Epoch: 15 cost time: 8.144527435302734
Epoch: 15, Steps: 263 | Train Loss: 0.4930795 Vali Loss: 1.0135728 Test Loss: 0.4936906
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4321788
	speed: 0.1110s/iter; left time: 2469.4273s
	iters: 200, epoch: 16 | loss: 0.4702118
	speed: 0.0205s/iter; left time: 454.7928s
Epoch: 16 cost time: 6.639383316040039
Epoch: 16, Steps: 263 | Train Loss: 0.4929825 Vali Loss: 1.0126002 Test Loss: 0.4937616
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4735989
	speed: 0.1603s/iter; left time: 3524.4356s
	iters: 200, epoch: 17 | loss: 0.4568737
	speed: 0.0359s/iter; left time: 785.6997s
Epoch: 17 cost time: 10.17708945274353
Epoch: 17, Steps: 263 | Train Loss: 0.4930862 Vali Loss: 1.0126785 Test Loss: 0.4940131
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4677954
	speed: 0.1086s/iter; left time: 2359.8807s
	iters: 200, epoch: 18 | loss: 0.4697868
	speed: 0.0231s/iter; left time: 500.3059s
Epoch: 18 cost time: 6.781712293624878
Epoch: 18, Steps: 263 | Train Loss: 0.4931988 Vali Loss: 1.0124735 Test Loss: 0.4937505
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.5163628
	speed: 0.1105s/iter; left time: 2373.0473s
	iters: 200, epoch: 19 | loss: 0.4973756
	speed: 0.0377s/iter; left time: 805.7987s
Epoch: 19 cost time: 8.774306058883667
Epoch: 19, Steps: 263 | Train Loss: 0.4930250 Vali Loss: 1.0128887 Test Loss: 0.4939494
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.5057543
	speed: 0.1159s/iter; left time: 2457.4233s
	iters: 200, epoch: 20 | loss: 0.5101444
	speed: 0.0239s/iter; left time: 503.8124s
Epoch: 20 cost time: 6.796766996383667
Epoch: 20, Steps: 263 | Train Loss: 0.4929587 Vali Loss: 1.0118998 Test Loss: 0.4939941
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.5050007
	speed: 0.1058s/iter; left time: 2215.3439s
	iters: 200, epoch: 21 | loss: 0.4701019
	speed: 0.0291s/iter; left time: 606.9334s
Epoch: 21 cost time: 7.487243413925171
Epoch: 21, Steps: 263 | Train Loss: 0.4930661 Vali Loss: 1.0130594 Test Loss: 0.4940391
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4519137
	speed: 0.1206s/iter; left time: 2492.9370s
	iters: 200, epoch: 22 | loss: 0.4802962
	speed: 0.0241s/iter; left time: 495.8941s
Epoch: 22 cost time: 8.005563259124756
Epoch: 22, Steps: 263 | Train Loss: 0.4929591 Vali Loss: 1.0125723 Test Loss: 0.4940856
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4773889
	speed: 0.1156s/iter; left time: 2359.3718s
	iters: 200, epoch: 23 | loss: 0.5276994
	speed: 0.0224s/iter; left time: 455.3537s
Epoch: 23 cost time: 6.936700344085693
Epoch: 23, Steps: 263 | Train Loss: 0.4930199 Vali Loss: 1.0124183 Test Loss: 0.4940447
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.5310003
	speed: 0.1054s/iter; left time: 2123.1575s
	iters: 200, epoch: 24 | loss: 0.5316252
	speed: 0.0244s/iter; left time: 489.9329s
Epoch: 24 cost time: 6.730529308319092
Epoch: 24, Steps: 263 | Train Loss: 0.4930439 Vali Loss: 1.0128225 Test Loss: 0.4939770
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_720_FITS_ETTm1_ftM_sl90_ll48_pl720_H14_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10801
mse:0.49122774600982666, mae:0.4518023133277893, rse:0.6668254733085632, corr:[0.5303718  0.5314399  0.52550954 0.52390885 0.5220992  0.51841635
 0.5160074  0.51481944 0.5130391  0.511034   0.50996864 0.5095922
 0.50871205 0.5064525  0.50301176 0.49932063 0.49623325 0.49335182
 0.4902783  0.48728567 0.4844696  0.4814643  0.47810912 0.4746563
 0.4712592  0.4675631  0.4640644  0.461402   0.45958337 0.45789447
 0.45674074 0.45707148 0.4575155  0.45743868 0.4573588  0.45788792
 0.45798606 0.45767942 0.45770586 0.45806223 0.45815378 0.4573663
 0.45671073 0.45667896 0.45657492 0.45635265 0.45646852 0.45671698
 0.4566933  0.45669094 0.45711917 0.45753017 0.45785213 0.4581682
 0.4587266  0.45898184 0.45908016 0.45947734 0.45981106 0.45971
 0.4591961  0.45870665 0.45879945 0.45853344 0.45791277 0.4575648
 0.45762724 0.45759264 0.4575144  0.45790282 0.45866305 0.4594245
 0.46002048 0.4606525  0.4617422  0.4626134  0.46330386 0.46420106
 0.46526876 0.46616337 0.46676207 0.46714422 0.46777254 0.46833944
 0.46882814 0.46918672 0.46951884 0.47005373 0.47097132 0.47216788
 0.4732168  0.47381887 0.47414115 0.4745486  0.4751856  0.47553515
 0.4748711  0.47347996 0.47220042 0.4714699  0.4711381  0.47091737
 0.47053653 0.47007465 0.46978584 0.4694364  0.46882665 0.46805903
 0.4672309  0.46621    0.46506992 0.46413156 0.46334907 0.46235278
 0.46113586 0.46033207 0.45979127 0.45886448 0.45734936 0.45578665
 0.45478433 0.45390284 0.45267704 0.45151523 0.45063984 0.44997808
 0.44929302 0.44864097 0.44846132 0.44868675 0.44879934 0.4486511
 0.4483564  0.44824177 0.44805    0.4478538  0.4478158  0.44767848
 0.44714847 0.44670984 0.44689724 0.44733614 0.44723293 0.44681343
 0.44677395 0.4473019  0.4477904  0.44772372 0.44766864 0.44768694
 0.44779322 0.4483027  0.4488282  0.44890994 0.4485033  0.44840977
 0.44888818 0.44945544 0.44951084 0.4492749  0.44928783 0.44939548
 0.4494681  0.44969782 0.4501597  0.45056164 0.45099783 0.45188203
 0.45321724 0.45421657 0.45436573 0.45436397 0.4549017  0.45607215
 0.45710754 0.45773292 0.458161   0.45858145 0.45876056 0.45877323
 0.45893773 0.45963264 0.46054226 0.46123704 0.4618182  0.46267048
 0.46380153 0.46471304 0.465133   0.46526372 0.46531454 0.46538478
 0.46571207 0.46624088 0.46669206 0.46727172 0.46808147 0.46867526
 0.46885288 0.468816   0.46868548 0.46814877 0.46725485 0.46611732
 0.46485472 0.46363288 0.4622433  0.4606359  0.458969   0.4576019
 0.45636594 0.45510477 0.45389256 0.4524816  0.4504737  0.44820553
 0.44621554 0.44499755 0.44399142 0.4427658  0.44141546 0.44050792
 0.44011804 0.43995097 0.4395566  0.43931246 0.4396111  0.4394687
 0.43891197 0.43889156 0.4392322  0.438884   0.43800288 0.43742335
 0.43763736 0.43805414 0.43802792 0.43810543 0.43849093 0.43830505
 0.4378263  0.43756992 0.43786567 0.4383883  0.4384646  0.43852425
 0.43867275 0.43889228 0.43927082 0.44011408 0.44057432 0.44040275
 0.4399713  0.44020072 0.44056582 0.44076294 0.44046274 0.44020465
 0.44039324 0.44072375 0.44074175 0.44104743 0.441723   0.44215664
 0.44276032 0.44372153 0.44487128 0.4457661  0.44653955 0.44742697
 0.44860283 0.44938457 0.4497274  0.45003843 0.45067653 0.45123187
 0.4513959  0.45157805 0.45207092 0.45301238 0.4540628  0.45502582
 0.4559843  0.45674762 0.456984   0.45671335 0.45609844 0.4549422
 0.4528472  0.45041785 0.44852713 0.44723383 0.4461411  0.4455655
 0.4456916  0.4459052  0.44559273 0.4448523  0.444301   0.44405454
 0.4434838  0.44216177 0.44050986 0.43906084 0.43773133 0.43619835
 0.4351601  0.43490955 0.4349276  0.4342153  0.4327882  0.43150112
 0.430792   0.43026578 0.42958918 0.42890126 0.42805743 0.42728525
 0.42666492 0.42635414 0.4262982  0.42618936 0.4259388  0.42584735
 0.42582828 0.4258373  0.42568928 0.42551482 0.42531765 0.42498133
 0.42484793 0.42477927 0.42456532 0.42397588 0.42359164 0.4237788
 0.42427248 0.42423606 0.4241009  0.4240507  0.4243172  0.4246093
 0.4250118  0.42575482 0.42629337 0.42625985 0.4258869  0.4256937
 0.42571524 0.42565334 0.4256961  0.42590427 0.42600185 0.42582694
 0.4256728  0.42592    0.42647734 0.4268987  0.42718494 0.42761603
 0.42818713 0.42866734 0.42910346 0.429682   0.43068033 0.43171024
 0.4325423  0.43349546 0.43449402 0.43511623 0.43552434 0.435963
 0.43681553 0.43797863 0.43941912 0.44104335 0.4427564  0.4444215
 0.44568175 0.4463827  0.44678056 0.44713256 0.44692162 0.44631937
 0.44594362 0.4458631  0.44566718 0.44544443 0.44579017 0.44687384
 0.4481095  0.44891292 0.44914967 0.44900334 0.4486893  0.44812143
 0.44735765 0.44644687 0.44520572 0.443823   0.4426416  0.4416775
 0.44054484 0.43931216 0.43866587 0.4384042  0.43752226 0.4357791
 0.4342178  0.43324    0.4326441  0.4321225  0.4315244  0.43123442
 0.43127838 0.430955   0.43063322 0.4307578  0.43109378 0.4310042
 0.43028262 0.42995295 0.43033415 0.4306253  0.43014887 0.4294958
 0.42938486 0.4296247  0.4292393  0.42871937 0.42855808 0.42877665
 0.42882606 0.42836386 0.42790538 0.4279611  0.4283972  0.42895877
 0.42925173 0.42931598 0.42944983 0.43000725 0.43070248 0.4309616
 0.43067077 0.43055603 0.4306306  0.43073398 0.43068358 0.43071154
 0.43063083 0.43040168 0.43008533 0.4303225  0.43093342 0.43146697
 0.4317721  0.43215656 0.43296692 0.43410364 0.43508112 0.43572468
 0.43628395 0.4370628  0.43781412 0.43844897 0.43892527 0.43924335
 0.43956456 0.44021288 0.4414755  0.44292504 0.44400945 0.44482422
 0.44585177 0.44687387 0.44706494 0.44648185 0.44582972 0.44531494
 0.44417328 0.4422944  0.44082403 0.44027832 0.43991172 0.4393183
 0.43914163 0.4397677  0.4404632  0.44008186 0.43894613 0.43802363
 0.43725544 0.43587092 0.4338992  0.43222564 0.43108714 0.42977285
 0.42814577 0.4269876  0.4262908  0.42523628 0.42391047 0.42277244
 0.4218842  0.4204831  0.41883308 0.4180044  0.41798133 0.4179207
 0.4174117  0.41721842 0.41747454 0.41757208 0.41724303 0.41715363
 0.4172402  0.4170627  0.41670194 0.41654527 0.41672674 0.41641334
 0.4154801  0.41516024 0.4155364  0.41572613 0.41523403 0.41501915
 0.4154183  0.41568437 0.4152465  0.41475376 0.41483605 0.4149978
 0.41485107 0.41474766 0.41538745 0.41622722 0.41644934 0.41621462
 0.41643673 0.41675326 0.41647094 0.41582346 0.41540462 0.41534048
 0.41516033 0.4149545  0.4151624  0.4155843  0.41568774 0.41578433
 0.4164361  0.4174418  0.41828594 0.41875622 0.41935933 0.42042196
 0.42166466 0.42258036 0.42288172 0.4228613  0.42316744 0.4239915
 0.42502746 0.4261099  0.42702442 0.4279209  0.42881778 0.42953643
 0.43015707 0.43089813 0.43162593 0.43145147 0.42991504 0.42781013
 0.4259246  0.42423865 0.42250335 0.42117718 0.4207314  0.42137623
 0.42264956 0.42369235 0.4237878  0.4231312  0.4223585  0.4219345
 0.421549   0.42021024 0.41789848 0.4157387  0.41438204 0.41349468
 0.41256365 0.41141474 0.4101794  0.4089209  0.40783405 0.40696368
 0.40600985 0.40484074 0.404061   0.4038811  0.40345398 0.4024898
 0.40163004 0.4016321  0.40188837 0.40159327 0.40071926 0.40042728
 0.40096188 0.4016373  0.40170962 0.40135914 0.4014554  0.40173
 0.40175346 0.40142474 0.40104517 0.40117    0.40161943 0.4017708
 0.4013146  0.40091005 0.40123114 0.40168035 0.4019131  0.40221217
 0.4027191  0.40314806 0.40334105 0.4033259  0.40355614 0.40350735
 0.40260345 0.40147418 0.40134308 0.4018484  0.4017691  0.40079486
 0.40014556 0.40051234 0.40095642 0.40089747 0.4008866  0.40165305
 0.40268004 0.40334284 0.4039544  0.4048638  0.4053651  0.40541804
 0.40588441 0.40740904 0.40885225 0.40912724 0.40881595 0.4090274
 0.40983376 0.41056755 0.41149065 0.4130692  0.41463897 0.4155184
 0.4161477  0.41698638 0.41742724 0.4168142  0.4155948  0.41436073
 0.41286308 0.41101286 0.40966338 0.4093247  0.4092763  0.4092059
 0.4098605  0.41132545 0.41226318 0.4121143  0.4116797  0.4111725
 0.41       0.40819976 0.40644878 0.4048144  0.40297824 0.40105355
 0.39954224 0.39836848 0.3975003  0.39683163 0.3957893  0.3938969
 0.39195898 0.39116424 0.39101797 0.3903336  0.3892218  0.38860157
 0.38794807 0.38626283 0.38498127 0.3854372  0.3859475  0.38500643
 0.3839797  0.38450122 0.38482115 0.38380614 0.38401875 0.38584512
 0.38574618 0.38457263 0.38673565 0.38926727 0.38904288 0.39522678]
