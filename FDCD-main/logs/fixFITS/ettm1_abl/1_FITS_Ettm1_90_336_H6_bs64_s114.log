Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=96, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=16, d_ff=2048, d_layers=1, d_model=512, data='ETTm1', data_path='ETTm1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTm1_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTm1_90_336_FITS_ETTm1_ftM_sl90_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34135
val 11185
test 11185
Model(
  (freq_upsampler): Linear(in_features=16, out_features=75, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1075200.0
params:  1275.0
Trainable parameters:  1275
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7998748
	speed: 0.0201s/iter; left time: 531.8303s
	iters: 200, epoch: 1 | loss: 0.6705157
	speed: 0.0130s/iter; left time: 342.5476s
Epoch: 1 cost time: 4.197559118270874
Epoch: 1, Steps: 266 | Train Loss: 0.7352149 Vali Loss: 0.9670256 Test Loss: 0.6811761
Validation loss decreased (inf --> 0.967026).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5335642
	speed: 0.0697s/iter; left time: 1828.2879s
	iters: 200, epoch: 2 | loss: 0.4648074
	speed: 0.0144s/iter; left time: 375.7743s
Epoch: 2 cost time: 4.853896856307983
Epoch: 2, Steps: 266 | Train Loss: 0.4896710 Vali Loss: 0.7823769 Test Loss: 0.5049635
Validation loss decreased (0.967026 --> 0.782377).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3890596
	speed: 0.0741s/iter; left time: 1923.4538s
	iters: 200, epoch: 3 | loss: 0.4701850
	speed: 0.0144s/iter; left time: 373.3224s
Epoch: 3 cost time: 4.318040132522583
Epoch: 3, Steps: 266 | Train Loss: 0.4423019 Vali Loss: 0.7320120 Test Loss: 0.4588885
Validation loss decreased (0.782377 --> 0.732012).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.4090849
	speed: 0.0688s/iter; left time: 1769.2324s
	iters: 200, epoch: 4 | loss: 0.4287407
	speed: 0.0138s/iter; left time: 353.0562s
Epoch: 4 cost time: 4.240280628204346
Epoch: 4, Steps: 266 | Train Loss: 0.4305107 Vali Loss: 0.7117180 Test Loss: 0.4422706
Validation loss decreased (0.732012 --> 0.711718).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.4612268
	speed: 0.0685s/iter; left time: 1742.6134s
	iters: 200, epoch: 5 | loss: 0.3953387
	speed: 0.0135s/iter; left time: 342.5884s
Epoch: 5 cost time: 4.307903528213501
Epoch: 5, Steps: 266 | Train Loss: 0.4264311 Vali Loss: 0.7032357 Test Loss: 0.4357695
Validation loss decreased (0.711718 --> 0.703236).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.4070140
	speed: 0.0691s/iter; left time: 1738.2808s
	iters: 200, epoch: 6 | loss: 0.3879002
	speed: 0.0142s/iter; left time: 355.8279s
Epoch: 6 cost time: 4.28145432472229
Epoch: 6, Steps: 266 | Train Loss: 0.4250795 Vali Loss: 0.6988381 Test Loss: 0.4335041
Validation loss decreased (0.703236 --> 0.698838).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.4658219
	speed: 0.0696s/iter; left time: 1734.2938s
	iters: 200, epoch: 7 | loss: 0.4113357
	speed: 0.0140s/iter; left time: 348.1117s
Epoch: 7 cost time: 4.274149179458618
Epoch: 7, Steps: 266 | Train Loss: 0.4247092 Vali Loss: 0.6978437 Test Loss: 0.4323607
Validation loss decreased (0.698838 --> 0.697844).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.4350514
	speed: 0.0695s/iter; left time: 1711.6222s
	iters: 200, epoch: 8 | loss: 0.4665465
	speed: 0.0141s/iter; left time: 346.9439s
Epoch: 8 cost time: 4.303067445755005
Epoch: 8, Steps: 266 | Train Loss: 0.4244067 Vali Loss: 0.6962599 Test Loss: 0.4320512
Validation loss decreased (0.697844 --> 0.696260).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.4377796
	speed: 0.0689s/iter; left time: 1679.8487s
	iters: 200, epoch: 9 | loss: 0.4327521
	speed: 0.0142s/iter; left time: 344.5487s
Epoch: 9 cost time: 4.3137102127075195
Epoch: 9, Steps: 266 | Train Loss: 0.4242942 Vali Loss: 0.6957912 Test Loss: 0.4315490
Validation loss decreased (0.696260 --> 0.695791).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.4167755
	speed: 0.0692s/iter; left time: 1668.2515s
	iters: 200, epoch: 10 | loss: 0.3882682
	speed: 0.0142s/iter; left time: 341.4789s
Epoch: 10 cost time: 4.339614629745483
Epoch: 10, Steps: 266 | Train Loss: 0.4242631 Vali Loss: 0.6952080 Test Loss: 0.4319713
Validation loss decreased (0.695791 --> 0.695208).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.4115953
	speed: 0.0695s/iter; left time: 1656.8736s
	iters: 200, epoch: 11 | loss: 0.4260646
	speed: 0.0140s/iter; left time: 332.0626s
Epoch: 11 cost time: 4.132055759429932
Epoch: 11, Steps: 266 | Train Loss: 0.4242132 Vali Loss: 0.6961092 Test Loss: 0.4315293
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.4312048
	speed: 0.0710s/iter; left time: 1674.8681s
	iters: 200, epoch: 12 | loss: 0.3995051
	speed: 0.0142s/iter; left time: 334.0690s
Epoch: 12 cost time: 5.105207443237305
Epoch: 12, Steps: 266 | Train Loss: 0.4242018 Vali Loss: 0.6958712 Test Loss: 0.4316792
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.4348520
	speed: 0.0688s/iter; left time: 1604.1428s
	iters: 200, epoch: 13 | loss: 0.4242021
	speed: 0.0144s/iter; left time: 333.5161s
Epoch: 13 cost time: 4.394134044647217
Epoch: 13, Steps: 266 | Train Loss: 0.4241674 Vali Loss: 0.6962318 Test Loss: 0.4319191
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.4572105
	speed: 0.0673s/iter; left time: 1550.3612s
	iters: 200, epoch: 14 | loss: 0.3926288
	speed: 0.0148s/iter; left time: 339.0327s
Epoch: 14 cost time: 4.376323938369751
Epoch: 14, Steps: 266 | Train Loss: 0.4241536 Vali Loss: 0.6948807 Test Loss: 0.4316085
Validation loss decreased (0.695208 --> 0.694881).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.3934623
	speed: 0.0718s/iter; left time: 1636.2264s
	iters: 200, epoch: 15 | loss: 0.3978962
	speed: 0.0148s/iter; left time: 334.9914s
Epoch: 15 cost time: 4.638674259185791
Epoch: 15, Steps: 266 | Train Loss: 0.4240752 Vali Loss: 0.6957790 Test Loss: 0.4317886
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.4382250
	speed: 0.0712s/iter; left time: 1603.1438s
	iters: 200, epoch: 16 | loss: 0.4171416
	speed: 0.0155s/iter; left time: 348.1253s
Epoch: 16 cost time: 4.644862651824951
Epoch: 16, Steps: 266 | Train Loss: 0.4240028 Vali Loss: 0.6958347 Test Loss: 0.4319561
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.4664325
	speed: 0.0716s/iter; left time: 1591.8962s
	iters: 200, epoch: 17 | loss: 0.4071940
	speed: 0.0146s/iter; left time: 323.6994s
Epoch: 17 cost time: 4.375925779342651
Epoch: 17, Steps: 266 | Train Loss: 0.4241647 Vali Loss: 0.6957396 Test Loss: 0.4318489
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.4403787
	speed: 0.0709s/iter; left time: 1558.4870s
	iters: 200, epoch: 18 | loss: 0.4059308
	speed: 0.0142s/iter; left time: 310.5721s
Epoch: 18 cost time: 4.2501444816589355
Epoch: 18, Steps: 266 | Train Loss: 0.4240928 Vali Loss: 0.6952559 Test Loss: 0.4315825
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.3816760
	speed: 0.0679s/iter; left time: 1473.9530s
	iters: 200, epoch: 19 | loss: 0.4334995
	speed: 0.0148s/iter; left time: 320.9368s
Epoch: 19 cost time: 4.4164512157440186
Epoch: 19, Steps: 266 | Train Loss: 0.4239436 Vali Loss: 0.6954620 Test Loss: 0.4317138
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.4799208
	speed: 0.0813s/iter; left time: 1743.3868s
	iters: 200, epoch: 20 | loss: 0.4734576
	speed: 0.0145s/iter; left time: 309.4232s
Epoch: 20 cost time: 4.36491060256958
Epoch: 20, Steps: 266 | Train Loss: 0.4239118 Vali Loss: 0.6957065 Test Loss: 0.4317520
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.4338115
	speed: 0.0707s/iter; left time: 1498.0592s
	iters: 200, epoch: 21 | loss: 0.4419624
	speed: 0.0145s/iter; left time: 306.0028s
Epoch: 21 cost time: 4.34872579574585
Epoch: 21, Steps: 266 | Train Loss: 0.4240935 Vali Loss: 0.6955730 Test Loss: 0.4317363
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.4296566
	speed: 0.0704s/iter; left time: 1473.0783s
	iters: 200, epoch: 22 | loss: 0.4054177
	speed: 0.0146s/iter; left time: 303.8691s
Epoch: 22 cost time: 4.3879594802856445
Epoch: 22, Steps: 266 | Train Loss: 0.4241118 Vali Loss: 0.6955015 Test Loss: 0.4315799
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.4161661
	speed: 0.0708s/iter; left time: 1461.0071s
	iters: 200, epoch: 23 | loss: 0.4463397
	speed: 0.0146s/iter; left time: 300.3604s
Epoch: 23 cost time: 4.385002374649048
Epoch: 23, Steps: 266 | Train Loss: 0.4241642 Vali Loss: 0.6948050 Test Loss: 0.4315166
Validation loss decreased (0.694881 --> 0.694805).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.4223437
	speed: 0.0672s/iter; left time: 1370.6485s
	iters: 200, epoch: 24 | loss: 0.4172111
	speed: 0.0145s/iter; left time: 293.6365s
Epoch: 24 cost time: 4.245788097381592
Epoch: 24, Steps: 266 | Train Loss: 0.4240792 Vali Loss: 0.6956943 Test Loss: 0.4318220
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.3929963
	speed: 0.0688s/iter; left time: 1383.7130s
	iters: 200, epoch: 25 | loss: 0.3907695
	speed: 0.0141s/iter; left time: 283.1638s
Epoch: 25 cost time: 4.384666442871094
Epoch: 25, Steps: 266 | Train Loss: 0.4239380 Vali Loss: 0.6955249 Test Loss: 0.4317946
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.4604966
	speed: 0.0669s/iter; left time: 1327.9750s
	iters: 200, epoch: 26 | loss: 0.4166404
	speed: 0.0139s/iter; left time: 274.9252s
Epoch: 26 cost time: 4.326592922210693
Epoch: 26, Steps: 266 | Train Loss: 0.4239853 Vali Loss: 0.6957210 Test Loss: 0.4318819
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.3879389
	speed: 0.0697s/iter; left time: 1365.6186s
	iters: 200, epoch: 27 | loss: 0.4236026
	speed: 0.0147s/iter; left time: 286.0305s
Epoch: 27 cost time: 4.358546495437622
Epoch: 27, Steps: 266 | Train Loss: 0.4240268 Vali Loss: 0.6953938 Test Loss: 0.4318116
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.4042150
	speed: 0.0673s/iter; left time: 1299.7307s
	iters: 200, epoch: 28 | loss: 0.4462547
	speed: 0.0137s/iter; left time: 263.1687s
Epoch: 28 cost time: 4.2948527336120605
Epoch: 28, Steps: 266 | Train Loss: 0.4238205 Vali Loss: 0.6960574 Test Loss: 0.4317856
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.4117396
	speed: 0.0677s/iter; left time: 1290.1282s
	iters: 200, epoch: 29 | loss: 0.4105709
	speed: 0.0128s/iter; left time: 243.3459s
Epoch: 29 cost time: 4.162526845932007
Epoch: 29, Steps: 266 | Train Loss: 0.4238394 Vali Loss: 0.6956446 Test Loss: 0.4319042
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.4741797
	speed: 0.0706s/iter; left time: 1325.6673s
	iters: 200, epoch: 30 | loss: 0.3939970
	speed: 0.0264s/iter; left time: 493.7464s
Epoch: 30 cost time: 5.665679931640625
Epoch: 30, Steps: 266 | Train Loss: 0.4238525 Vali Loss: 0.6952717 Test Loss: 0.4317830
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.4133826
	speed: 0.0683s/iter; left time: 1264.4212s
	iters: 200, epoch: 31 | loss: 0.4080532
	speed: 0.0146s/iter; left time: 269.3135s
Epoch: 31 cost time: 4.418026685714722
Epoch: 31, Steps: 266 | Train Loss: 0.4240336 Vali Loss: 0.6954741 Test Loss: 0.4318426
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.4454956
	speed: 0.0701s/iter; left time: 1278.7904s
	iters: 200, epoch: 32 | loss: 0.3827814
	speed: 0.0141s/iter; left time: 256.8843s
Epoch: 32 cost time: 4.373337507247925
Epoch: 32, Steps: 266 | Train Loss: 0.4238610 Vali Loss: 0.6958871 Test Loss: 0.4318378
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.4317622
	speed: 0.0682s/iter; left time: 1227.3502s
	iters: 200, epoch: 33 | loss: 0.4589843
	speed: 0.0146s/iter; left time: 260.3405s
Epoch: 33 cost time: 4.397896766662598
Epoch: 33, Steps: 266 | Train Loss: 0.4240231 Vali Loss: 0.6952960 Test Loss: 0.4318093
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.4120395
	speed: 0.0705s/iter; left time: 1250.2625s
	iters: 200, epoch: 34 | loss: 0.4492662
	speed: 0.0123s/iter; left time: 217.0993s
Epoch: 34 cost time: 4.280214548110962
Epoch: 34, Steps: 266 | Train Loss: 0.4240621 Vali Loss: 0.6953241 Test Loss: 0.4318008
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.3836389
	speed: 0.0708s/iter; left time: 1236.7656s
	iters: 200, epoch: 35 | loss: 0.4203404
	speed: 0.0140s/iter; left time: 242.8160s
Epoch: 35 cost time: 4.426393508911133
Epoch: 35, Steps: 266 | Train Loss: 0.4239551 Vali Loss: 0.6952977 Test Loss: 0.4318674
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.4113508
	speed: 0.0696s/iter; left time: 1197.3172s
	iters: 200, epoch: 36 | loss: 0.4171066
	speed: 0.0145s/iter; left time: 248.0368s
Epoch: 36 cost time: 4.39642071723938
Epoch: 36, Steps: 266 | Train Loss: 0.4240680 Vali Loss: 0.6958924 Test Loss: 0.4319510
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.3984311
	speed: 0.0676s/iter; left time: 1143.3268s
	iters: 200, epoch: 37 | loss: 0.4186361
	speed: 0.0145s/iter; left time: 243.5681s
Epoch: 37 cost time: 4.350188493728638
Epoch: 37, Steps: 266 | Train Loss: 0.4238897 Vali Loss: 0.6950906 Test Loss: 0.4318762
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.4006619
	speed: 0.0679s/iter; left time: 1131.2927s
	iters: 200, epoch: 38 | loss: 0.4318015
	speed: 0.0145s/iter; left time: 239.9234s
Epoch: 38 cost time: 4.268452167510986
Epoch: 38, Steps: 266 | Train Loss: 0.4238473 Vali Loss: 0.6956474 Test Loss: 0.4318950
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.3844611
	speed: 0.0702s/iter; left time: 1151.4034s
	iters: 200, epoch: 39 | loss: 0.3913700
	speed: 0.0144s/iter; left time: 234.7120s
Epoch: 39 cost time: 4.37007999420166
Epoch: 39, Steps: 266 | Train Loss: 0.4239080 Vali Loss: 0.6957377 Test Loss: 0.4318730
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.4454257
	speed: 0.0691s/iter; left time: 1114.0252s
	iters: 200, epoch: 40 | loss: 0.4346701
	speed: 0.0144s/iter; left time: 230.9089s
Epoch: 40 cost time: 4.335726022720337
Epoch: 40, Steps: 266 | Train Loss: 0.4239912 Vali Loss: 0.6951844 Test Loss: 0.4319263
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.4103335
	speed: 0.0690s/iter; left time: 1094.7221s
	iters: 200, epoch: 41 | loss: 0.4042575
	speed: 0.0142s/iter; left time: 223.5434s
Epoch: 41 cost time: 4.3033716678619385
Epoch: 41, Steps: 266 | Train Loss: 0.4239290 Vali Loss: 0.6956024 Test Loss: 0.4319239
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.4370329
	speed: 0.0669s/iter; left time: 1044.0466s
	iters: 200, epoch: 42 | loss: 0.4184002
	speed: 0.0150s/iter; left time: 232.0953s
Epoch: 42 cost time: 4.437512159347534
Epoch: 42, Steps: 266 | Train Loss: 0.4238868 Vali Loss: 0.6955265 Test Loss: 0.4319338
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.3982837
	speed: 0.0694s/iter; left time: 1063.2530s
	iters: 200, epoch: 43 | loss: 0.3994267
	speed: 0.0146s/iter; left time: 221.7289s
Epoch: 43 cost time: 4.273348569869995
Epoch: 43, Steps: 266 | Train Loss: 0.4237889 Vali Loss: 0.6957396 Test Loss: 0.4318805
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTm1_90_336_FITS_ETTm1_ftM_sl90_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
mse:0.43162015080451965, mae:0.41902241110801697, rse:0.6251715421676636, corr:[0.54669213 0.5468476  0.5426353  0.5385383  0.5362431  0.53465664
 0.5326586  0.5303992  0.52845955 0.5271336  0.5260327  0.52491033
 0.52370787 0.52216524 0.5198785  0.5166698  0.51306057 0.50986016
 0.5073183  0.5050611  0.5026054  0.49956352 0.49590412 0.49195254
 0.48827538 0.4851092  0.4825065  0.48022026 0.4781383  0.4761553
 0.47484854 0.47491416 0.4756027  0.47611442 0.4762609  0.47639087
 0.47642517 0.47655478 0.47667864 0.47660616 0.4764515  0.47614706
 0.47592777 0.4758446  0.4755794  0.47511965 0.4747259  0.47461155
 0.47484583 0.47530818 0.47578683 0.47598457 0.47609738 0.47628742
 0.47684604 0.47754675 0.47814485 0.47836193 0.4780523  0.4777065
 0.47764567 0.4775885  0.47743812 0.47695953 0.4765523  0.47636962
 0.476276   0.4762524  0.47643092 0.47680733 0.47713313 0.47752658
 0.4781786  0.47899315 0.4799359  0.48068655 0.48140085 0.48218283
 0.48293194 0.4835982  0.4841405  0.48446357 0.484808   0.4851196
 0.4854655  0.48583916 0.48627612 0.48680362 0.4874896  0.48833922
 0.48920757 0.489932   0.49048582 0.49089995 0.49111807 0.4909463
 0.49029016 0.4893878  0.4884149  0.48753348 0.48704627 0.48687193
 0.4866968  0.48613116 0.48534858 0.48457745 0.48391405 0.48337206
 0.4828324  0.4821578  0.48130926 0.48037693 0.4794794  0.47868273
 0.47786447 0.47701988 0.47603384 0.4749142  0.4737235  0.47250924
 0.47136015 0.47016117 0.4689162  0.46785694 0.46695903 0.46615833
 0.46553558 0.46513066 0.46489996 0.46475166 0.46464294 0.46461707
 0.4645996  0.4646702  0.46466357 0.4645046  0.464231   0.46397102
 0.4637982  0.46370706 0.46358195 0.46342894 0.46328193 0.4632909
 0.46347538 0.46383026 0.4642276  0.46443662 0.46461576 0.464733
 0.4648484  0.46517825 0.4655675  0.46580097 0.46573216 0.46558696
 0.4654967  0.4655274  0.4655778  0.46556804 0.46558326 0.46561322
 0.46573398 0.46599203 0.46642455 0.4669649  0.46759865 0.4683081
 0.46903726 0.46971655 0.4703259  0.47100094 0.47167987 0.4723661
 0.47296238 0.47346708 0.4738775  0.4742213  0.47449467 0.47484407
 0.47525486 0.47568437 0.47603267 0.47643125 0.4770679  0.4779431
 0.47885746 0.47953215 0.47985995 0.48000875 0.48027292 0.48078755
 0.48145664 0.48231906 0.48330444 0.48438907 0.48551854 0.48657787
 0.4872114  0.4871555  0.4866842  0.48612127 0.48550668 0.48482418
 0.4840124  0.48300254 0.48172584 0.48030424 0.4787813  0.4773764
 0.47612467 0.47494453 0.47368255 0.47214022 0.47026098 0.46840662
 0.46667308 0.46520594 0.46397012 0.46294275 0.46195853 0.46111223
 0.46059424 0.4605787  0.4606726  0.4605681  0.46042547 0.46014336
 0.45986113 0.45979145 0.45980373 0.45960364 0.45922428 0.4587737
 0.45851752 0.45852026 0.45851004 0.45847175 0.458449   0.45824856
 0.45816812 0.45821372 0.4584218  0.45879245 0.45904577 0.45932573
 0.45972064 0.46026698 0.4607761  0.46118492 0.46123388 0.4611362
 0.46098876 0.4610956  0.46127498 0.46156153 0.46159962 0.4613851
 0.46131504 0.46161556 0.46202365 0.46246502 0.46288714 0.4633132
 0.46393207 0.46455038 0.46516806 0.4659339  0.4669187  0.4679113
 0.46889883 0.46974054 0.47043264 0.4708944  0.47120264 0.47149286
 0.47193262 0.472566   0.47322384 0.4738819  0.47451967 0.475148
 0.4757568  0.4762653  0.4765591  0.4765236  0.47601864 0.47480533
 0.47269252 0.4701085  0.46779814 0.46612567 0.46510404 0.46465263
 0.46459502 0.46457872 0.46453285 0.46426603 0.46365318 0.46283534
 0.46199942 0.46115986 0.46021265 0.45905933 0.45782948 0.4566839
 0.45589632 0.45531163 0.4547406  0.4540423  0.45320886 0.4523016
 0.451376   0.45050326 0.4497365  0.44907612 0.44830805 0.4476544
 0.44724262 0.44700053 0.446735   0.4462928  0.44571325 0.44533506
 0.4452782  0.44554034 0.44570488 0.44553038 0.44515666 0.44497648
 0.44540384 0.44601133 0.4462432  0.44597498 0.44596016 0.44681343]
