Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j720_H8_FITS_custom_ftM_sl90_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11471
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=42, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1751694336.0
params:  16254.0
Trainable parameters:  16254
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 35.980931758880615
Epoch: 1, Steps: 89 | Train Loss: 2.3829801 Vali Loss: 1.9633617 Test Loss: 2.4456365
Validation loss decreased (inf --> 1.963362).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 34.962385416030884
Epoch: 2, Steps: 89 | Train Loss: 1.2329335 Vali Loss: 1.2502286 Test Loss: 1.5403275
Validation loss decreased (1.963362 --> 1.250229).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 36.68996500968933
Epoch: 3, Steps: 89 | Train Loss: 0.8225685 Vali Loss: 0.9516538 Test Loss: 1.1563883
Validation loss decreased (1.250229 --> 0.951654).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 37.064406871795654
Epoch: 4, Steps: 89 | Train Loss: 0.6393078 Vali Loss: 0.8051673 Test Loss: 0.9693508
Validation loss decreased (0.951654 --> 0.805167).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 36.551005363464355
Epoch: 5, Steps: 89 | Train Loss: 0.5454730 Vali Loss: 0.7260232 Test Loss: 0.8696752
Validation loss decreased (0.805167 --> 0.726023).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 36.295411825180054
Epoch: 6, Steps: 89 | Train Loss: 0.4927585 Vali Loss: 0.6800185 Test Loss: 0.8114084
Validation loss decreased (0.726023 --> 0.680019).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 35.76522088050842
Epoch: 7, Steps: 89 | Train Loss: 0.4606836 Vali Loss: 0.6507442 Test Loss: 0.7755799
Validation loss decreased (0.680019 --> 0.650744).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 36.38189935684204
Epoch: 8, Steps: 89 | Train Loss: 0.4397319 Vali Loss: 0.6318520 Test Loss: 0.7523825
Validation loss decreased (0.650744 --> 0.631852).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 35.79380202293396
Epoch: 9, Steps: 89 | Train Loss: 0.4257616 Vali Loss: 0.6187495 Test Loss: 0.7362128
Validation loss decreased (0.631852 --> 0.618749).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 36.341132164001465
Epoch: 10, Steps: 89 | Train Loss: 0.4157916 Vali Loss: 0.6083221 Test Loss: 0.7244301
Validation loss decreased (0.618749 --> 0.608322).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 36.19800639152527
Epoch: 11, Steps: 89 | Train Loss: 0.4084458 Vali Loss: 0.6006937 Test Loss: 0.7159069
Validation loss decreased (0.608322 --> 0.600694).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 35.90541338920593
Epoch: 12, Steps: 89 | Train Loss: 0.4027013 Vali Loss: 0.5963098 Test Loss: 0.7093015
Validation loss decreased (0.600694 --> 0.596310).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 37.797364711761475
Epoch: 13, Steps: 89 | Train Loss: 0.3984316 Vali Loss: 0.5915371 Test Loss: 0.7040720
Validation loss decreased (0.596310 --> 0.591537).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 36.51739025115967
Epoch: 14, Steps: 89 | Train Loss: 0.3949114 Vali Loss: 0.5879242 Test Loss: 0.6998847
Validation loss decreased (0.591537 --> 0.587924).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 36.29587411880493
Epoch: 15, Steps: 89 | Train Loss: 0.3922211 Vali Loss: 0.5853100 Test Loss: 0.6964074
Validation loss decreased (0.587924 --> 0.585310).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 38.34482216835022
Epoch: 16, Steps: 89 | Train Loss: 0.3898861 Vali Loss: 0.5825881 Test Loss: 0.6936021
Validation loss decreased (0.585310 --> 0.582588).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 36.064314126968384
Epoch: 17, Steps: 89 | Train Loss: 0.3878333 Vali Loss: 0.5804654 Test Loss: 0.6915916
Validation loss decreased (0.582588 --> 0.580465).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 35.55070471763611
Epoch: 18, Steps: 89 | Train Loss: 0.3862412 Vali Loss: 0.5783476 Test Loss: 0.6893914
Validation loss decreased (0.580465 --> 0.578348).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 36.42842483520508
Epoch: 19, Steps: 89 | Train Loss: 0.3848581 Vali Loss: 0.5776652 Test Loss: 0.6876025
Validation loss decreased (0.578348 --> 0.577665).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 36.536277294158936
Epoch: 20, Steps: 89 | Train Loss: 0.3837104 Vali Loss: 0.5761368 Test Loss: 0.6861083
Validation loss decreased (0.577665 --> 0.576137).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 38.04168701171875
Epoch: 21, Steps: 89 | Train Loss: 0.3825628 Vali Loss: 0.5744317 Test Loss: 0.6848911
Validation loss decreased (0.576137 --> 0.574432).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 37.157376766204834
Epoch: 22, Steps: 89 | Train Loss: 0.3816536 Vali Loss: 0.5741591 Test Loss: 0.6836900
Validation loss decreased (0.574432 --> 0.574159).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 37.25144577026367
Epoch: 23, Steps: 89 | Train Loss: 0.3808170 Vali Loss: 0.5731990 Test Loss: 0.6827331
Validation loss decreased (0.574159 --> 0.573199).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 38.68476319313049
Epoch: 24, Steps: 89 | Train Loss: 0.3800652 Vali Loss: 0.5724863 Test Loss: 0.6816601
Validation loss decreased (0.573199 --> 0.572486).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 37.39523935317993
Epoch: 25, Steps: 89 | Train Loss: 0.3794619 Vali Loss: 0.5711324 Test Loss: 0.6809242
Validation loss decreased (0.572486 --> 0.571132).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 36.45089387893677
Epoch: 26, Steps: 89 | Train Loss: 0.3788294 Vali Loss: 0.5709349 Test Loss: 0.6802146
Validation loss decreased (0.571132 --> 0.570935).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 38.495763540267944
Epoch: 27, Steps: 89 | Train Loss: 0.3783758 Vali Loss: 0.5696985 Test Loss: 0.6796107
Validation loss decreased (0.570935 --> 0.569698).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 39.57307457923889
Epoch: 28, Steps: 89 | Train Loss: 0.3778525 Vali Loss: 0.5697092 Test Loss: 0.6789690
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 38.6544508934021
Epoch: 29, Steps: 89 | Train Loss: 0.3772819 Vali Loss: 0.5693154 Test Loss: 0.6784670
Validation loss decreased (0.569698 --> 0.569315).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 37.62536382675171
Epoch: 30, Steps: 89 | Train Loss: 0.3770667 Vali Loss: 0.5690362 Test Loss: 0.6779289
Validation loss decreased (0.569315 --> 0.569036).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 39.486639738082886
Epoch: 31, Steps: 89 | Train Loss: 0.3765475 Vali Loss: 0.5679799 Test Loss: 0.6775802
Validation loss decreased (0.569036 --> 0.567980).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 39.876803159713745
Epoch: 32, Steps: 89 | Train Loss: 0.3762508 Vali Loss: 0.5685559 Test Loss: 0.6772003
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 38.21417951583862
Epoch: 33, Steps: 89 | Train Loss: 0.3759114 Vali Loss: 0.5678701 Test Loss: 0.6767456
Validation loss decreased (0.567980 --> 0.567870).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 38.73973727226257
Epoch: 34, Steps: 89 | Train Loss: 0.3756302 Vali Loss: 0.5672610 Test Loss: 0.6764221
Validation loss decreased (0.567870 --> 0.567261).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 37.40053653717041
Epoch: 35, Steps: 89 | Train Loss: 0.3753339 Vali Loss: 0.5670315 Test Loss: 0.6761644
Validation loss decreased (0.567261 --> 0.567032).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 39.139843225479126
Epoch: 36, Steps: 89 | Train Loss: 0.3751785 Vali Loss: 0.5668357 Test Loss: 0.6757841
Validation loss decreased (0.567032 --> 0.566836).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 38.686516761779785
Epoch: 37, Steps: 89 | Train Loss: 0.3749518 Vali Loss: 0.5670129 Test Loss: 0.6755065
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 36.69534611701965
Epoch: 38, Steps: 89 | Train Loss: 0.3747241 Vali Loss: 0.5665227 Test Loss: 0.6752571
Validation loss decreased (0.566836 --> 0.566523).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 35.72851896286011
Epoch: 39, Steps: 89 | Train Loss: 0.3743716 Vali Loss: 0.5660337 Test Loss: 0.6750043
Validation loss decreased (0.566523 --> 0.566034).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 37.09761881828308
Epoch: 40, Steps: 89 | Train Loss: 0.3742337 Vali Loss: 0.5660059 Test Loss: 0.6747747
Validation loss decreased (0.566034 --> 0.566006).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 39.29149627685547
Epoch: 41, Steps: 89 | Train Loss: 0.3740394 Vali Loss: 0.5658250 Test Loss: 0.6746641
Validation loss decreased (0.566006 --> 0.565825).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 38.504721879959106
Epoch: 42, Steps: 89 | Train Loss: 0.3738563 Vali Loss: 0.5657527 Test Loss: 0.6744645
Validation loss decreased (0.565825 --> 0.565753).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 36.670960426330566
Epoch: 43, Steps: 89 | Train Loss: 0.3737434 Vali Loss: 0.5655338 Test Loss: 0.6742324
Validation loss decreased (0.565753 --> 0.565534).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 36.21615290641785
Epoch: 44, Steps: 89 | Train Loss: 0.3733853 Vali Loss: 0.5656923 Test Loss: 0.6740438
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 37.642216205596924
Epoch: 45, Steps: 89 | Train Loss: 0.3734007 Vali Loss: 0.5653687 Test Loss: 0.6739183
Validation loss decreased (0.565534 --> 0.565369).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 38.6378288269043
Epoch: 46, Steps: 89 | Train Loss: 0.3733789 Vali Loss: 0.5647541 Test Loss: 0.6737466
Validation loss decreased (0.565369 --> 0.564754).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 37.13879323005676
Epoch: 47, Steps: 89 | Train Loss: 0.3733526 Vali Loss: 0.5649271 Test Loss: 0.6736094
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 36.70518517494202
Epoch: 48, Steps: 89 | Train Loss: 0.3731022 Vali Loss: 0.5650945 Test Loss: 0.6734686
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 37.71959853172302
Epoch: 49, Steps: 89 | Train Loss: 0.3729620 Vali Loss: 0.5650342 Test Loss: 0.6733510
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 41.60015034675598
Epoch: 50, Steps: 89 | Train Loss: 0.3729707 Vali Loss: 0.5645217 Test Loss: 0.6732115
Validation loss decreased (0.564754 --> 0.564522).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 38.04414343833923
Epoch: 51, Steps: 89 | Train Loss: 0.3727838 Vali Loss: 0.5645247 Test Loss: 0.6731523
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 37.90982961654663
Epoch: 52, Steps: 89 | Train Loss: 0.3726554 Vali Loss: 0.5643262 Test Loss: 0.6730176
Validation loss decreased (0.564522 --> 0.564326).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 40.78390574455261
Epoch: 53, Steps: 89 | Train Loss: 0.3726181 Vali Loss: 0.5644413 Test Loss: 0.6729086
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 37.23565483093262
Epoch: 54, Steps: 89 | Train Loss: 0.3724838 Vali Loss: 0.5645714 Test Loss: 0.6727986
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 40.84450387954712
Epoch: 55, Steps: 89 | Train Loss: 0.3724774 Vali Loss: 0.5637808 Test Loss: 0.6727200
Validation loss decreased (0.564326 --> 0.563781).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 38.5785448551178
Epoch: 56, Steps: 89 | Train Loss: 0.3724315 Vali Loss: 0.5635090 Test Loss: 0.6726288
Validation loss decreased (0.563781 --> 0.563509).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 38.94834566116333
Epoch: 57, Steps: 89 | Train Loss: 0.3723047 Vali Loss: 0.5632435 Test Loss: 0.6725474
Validation loss decreased (0.563509 --> 0.563244).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 37.446343660354614
Epoch: 58, Steps: 89 | Train Loss: 0.3722294 Vali Loss: 0.5631391 Test Loss: 0.6724465
Validation loss decreased (0.563244 --> 0.563139).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 37.66136431694031
Epoch: 59, Steps: 89 | Train Loss: 0.3722046 Vali Loss: 0.5636352 Test Loss: 0.6723712
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 40.09943342208862
Epoch: 60, Steps: 89 | Train Loss: 0.3721033 Vali Loss: 0.5629761 Test Loss: 0.6723115
Validation loss decreased (0.563139 --> 0.562976).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 42.050044775009155
Epoch: 61, Steps: 89 | Train Loss: 0.3719928 Vali Loss: 0.5632928 Test Loss: 0.6722527
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 41.72999453544617
Epoch: 62, Steps: 89 | Train Loss: 0.3720107 Vali Loss: 0.5634102 Test Loss: 0.6721876
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 45.106345653533936
Epoch: 63, Steps: 89 | Train Loss: 0.3720166 Vali Loss: 0.5636008 Test Loss: 0.6721173
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 44.983500242233276
Epoch: 64, Steps: 89 | Train Loss: 0.3717324 Vali Loss: 0.5631635 Test Loss: 0.6720746
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 39.30394744873047
Epoch: 65, Steps: 89 | Train Loss: 0.3717738 Vali Loss: 0.5629507 Test Loss: 0.6720105
Validation loss decreased (0.562976 --> 0.562951).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 39.52606177330017
Epoch: 66, Steps: 89 | Train Loss: 0.3717430 Vali Loss: 0.5634245 Test Loss: 0.6719626
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 38.46783471107483
Epoch: 67, Steps: 89 | Train Loss: 0.3716933 Vali Loss: 0.5632780 Test Loss: 0.6719192
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 39.14632868766785
Epoch: 68, Steps: 89 | Train Loss: 0.3715942 Vali Loss: 0.5631458 Test Loss: 0.6718441
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 40.25121092796326
Epoch: 69, Steps: 89 | Train Loss: 0.3716179 Vali Loss: 0.5629917 Test Loss: 0.6718032
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 38.41219615936279
Epoch: 70, Steps: 89 | Train Loss: 0.3715989 Vali Loss: 0.5635594 Test Loss: 0.6717640
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 44.53108882904053
Epoch: 71, Steps: 89 | Train Loss: 0.3715042 Vali Loss: 0.5629750 Test Loss: 0.6717227
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 36.97610950469971
Epoch: 72, Steps: 89 | Train Loss: 0.3715405 Vali Loss: 0.5632559 Test Loss: 0.6716831
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 41.8046977519989
Epoch: 73, Steps: 89 | Train Loss: 0.3714934 Vali Loss: 0.5632669 Test Loss: 0.6716349
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 39.429598331451416
Epoch: 74, Steps: 89 | Train Loss: 0.3715587 Vali Loss: 0.5626458 Test Loss: 0.6715997
Validation loss decreased (0.562951 --> 0.562646).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 45.10365676879883
Epoch: 75, Steps: 89 | Train Loss: 0.3714515 Vali Loss: 0.5623268 Test Loss: 0.6715627
Validation loss decreased (0.562646 --> 0.562327).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 38.69993829727173
Epoch: 76, Steps: 89 | Train Loss: 0.3714775 Vali Loss: 0.5622889 Test Loss: 0.6715339
Validation loss decreased (0.562327 --> 0.562289).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 40.921359062194824
Epoch: 77, Steps: 89 | Train Loss: 0.3712470 Vali Loss: 0.5630286 Test Loss: 0.6714912
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 44.87410092353821
Epoch: 78, Steps: 89 | Train Loss: 0.3712572 Vali Loss: 0.5629702 Test Loss: 0.6714675
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 38.94891309738159
Epoch: 79, Steps: 89 | Train Loss: 0.3713359 Vali Loss: 0.5629977 Test Loss: 0.6714486
EarlyStopping counter: 3 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 37.619993925094604
Epoch: 80, Steps: 89 | Train Loss: 0.3712449 Vali Loss: 0.5632865 Test Loss: 0.6714199
EarlyStopping counter: 4 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 42.672128200531006
Epoch: 81, Steps: 89 | Train Loss: 0.3713188 Vali Loss: 0.5623738 Test Loss: 0.6713937
EarlyStopping counter: 5 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 39.55821752548218
Epoch: 82, Steps: 89 | Train Loss: 0.3711138 Vali Loss: 0.5622602 Test Loss: 0.6713682
Validation loss decreased (0.562289 --> 0.562260).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 37.450873374938965
Epoch: 83, Steps: 89 | Train Loss: 0.3712778 Vali Loss: 0.5628842 Test Loss: 0.6713466
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 42.481582164764404
Epoch: 84, Steps: 89 | Train Loss: 0.3710873 Vali Loss: 0.5629572 Test Loss: 0.6713203
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 38.552268743515015
Epoch: 85, Steps: 89 | Train Loss: 0.3711479 Vali Loss: 0.5624606 Test Loss: 0.6712951
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 41.29071402549744
Epoch: 86, Steps: 89 | Train Loss: 0.3711497 Vali Loss: 0.5627812 Test Loss: 0.6712747
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 37.984349489212036
Epoch: 87, Steps: 89 | Train Loss: 0.3711719 Vali Loss: 0.5626224 Test Loss: 0.6712544
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 45.01132941246033
Epoch: 88, Steps: 89 | Train Loss: 0.3710418 Vali Loss: 0.5627260 Test Loss: 0.6712370
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 45.60429072380066
Epoch: 89, Steps: 89 | Train Loss: 0.3711174 Vali Loss: 0.5624360 Test Loss: 0.6712288
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 37.09942650794983
Epoch: 90, Steps: 89 | Train Loss: 0.3711406 Vali Loss: 0.5624505 Test Loss: 0.6712083
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 38.67791771888733
Epoch: 91, Steps: 89 | Train Loss: 0.3711102 Vali Loss: 0.5623556 Test Loss: 0.6711975
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 40.28373646736145
Epoch: 92, Steps: 89 | Train Loss: 0.3710461 Vali Loss: 0.5627879 Test Loss: 0.6711765
EarlyStopping counter: 10 out of 10
Early stopping
train 11471
val 1037
test 2789
Model(
  (freq_upsampler): Linear(in_features=42, out_features=378, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1751694336.0
params:  16254.0
Trainable parameters:  16254
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 37.70514464378357
Epoch: 1, Steps: 89 | Train Loss: 0.4153936 Vali Loss: 0.5608313 Test Loss: 0.6694050
Validation loss decreased (inf --> 0.560831).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 39.204588651657104
Epoch: 2, Steps: 89 | Train Loss: 0.4144105 Vali Loss: 0.5602385 Test Loss: 0.6694080
Validation loss decreased (0.560831 --> 0.560239).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 38.20131993293762
Epoch: 3, Steps: 89 | Train Loss: 0.4140866 Vali Loss: 0.5603262 Test Loss: 0.6694273
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00045125
Epoch: 4 cost time: 45.20883536338806
Epoch: 4, Steps: 89 | Train Loss: 0.4139252 Vali Loss: 0.5604075 Test Loss: 0.6699348
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 37.86519694328308
Epoch: 5, Steps: 89 | Train Loss: 0.4140599 Vali Loss: 0.5605475 Test Loss: 0.6697485
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 38.12824082374573
Epoch: 6, Steps: 89 | Train Loss: 0.4139855 Vali Loss: 0.5602474 Test Loss: 0.6696011
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 41.00686812400818
Epoch: 7, Steps: 89 | Train Loss: 0.4139337 Vali Loss: 0.5608396 Test Loss: 0.6697557
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 43.397467613220215
Epoch: 8, Steps: 89 | Train Loss: 0.4138176 Vali Loss: 0.5606033 Test Loss: 0.6695330
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 45.3912787437439
Epoch: 9, Steps: 89 | Train Loss: 0.4139602 Vali Loss: 0.5602605 Test Loss: 0.6691335
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 43.082924365997314
Epoch: 10, Steps: 89 | Train Loss: 0.4137867 Vali Loss: 0.5600690 Test Loss: 0.6692616
Validation loss decreased (0.560239 --> 0.560069).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 42.431344985961914
Epoch: 11, Steps: 89 | Train Loss: 0.4137991 Vali Loss: 0.5598036 Test Loss: 0.6690820
Validation loss decreased (0.560069 --> 0.559804).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 43.61916399002075
Epoch: 12, Steps: 89 | Train Loss: 0.4139221 Vali Loss: 0.5597372 Test Loss: 0.6690009
Validation loss decreased (0.559804 --> 0.559737).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 44.518731117248535
Epoch: 13, Steps: 89 | Train Loss: 0.4138587 Vali Loss: 0.5587233 Test Loss: 0.6694694
Validation loss decreased (0.559737 --> 0.558723).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 38.34671449661255
Epoch: 14, Steps: 89 | Train Loss: 0.4137907 Vali Loss: 0.5599180 Test Loss: 0.6691657
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 39.74052333831787
Epoch: 15, Steps: 89 | Train Loss: 0.4137681 Vali Loss: 0.5598741 Test Loss: 0.6693190
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 38.33423829078674
Epoch: 16, Steps: 89 | Train Loss: 0.4137650 Vali Loss: 0.5596065 Test Loss: 0.6689438
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 44.14023303985596
Epoch: 17, Steps: 89 | Train Loss: 0.4137307 Vali Loss: 0.5593781 Test Loss: 0.6691148
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 38.92618751525879
Epoch: 18, Steps: 89 | Train Loss: 0.4137830 Vali Loss: 0.5601767 Test Loss: 0.6691280
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 37.631173610687256
Epoch: 19, Steps: 89 | Train Loss: 0.4137069 Vali Loss: 0.5595874 Test Loss: 0.6690063
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 39.26494765281677
Epoch: 20, Steps: 89 | Train Loss: 0.4136772 Vali Loss: 0.5598565 Test Loss: 0.6690722
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 38.88016867637634
Epoch: 21, Steps: 89 | Train Loss: 0.4137893 Vali Loss: 0.5599040 Test Loss: 0.6690134
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 39.90238070487976
Epoch: 22, Steps: 89 | Train Loss: 0.4137796 Vali Loss: 0.5596547 Test Loss: 0.6692132
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 37.831422567367554
Epoch: 23, Steps: 89 | Train Loss: 0.4137455 Vali Loss: 0.5593393 Test Loss: 0.6688233
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j720_H8_FITS_custom_ftM_sl90_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
mse:0.6683222055435181, mae:0.3961702287197113, rse:0.6684882640838623, corr:[0.24583101 0.26313946 0.26338974 0.2614123  0.26074907 0.26161602
 0.26336625 0.26397222 0.2639806  0.26385662 0.26334858 0.2634938
 0.26237112 0.26132542 0.26202446 0.2627488  0.263435   0.26401886
 0.26543584 0.26612294 0.2678627  0.26847667 0.2682529  0.26849514
 0.27203545 0.2748043  0.273352   0.27210698 0.27262288 0.27457875
 0.27684832 0.2780729  0.27870187 0.27856642 0.27772993 0.27686718
 0.27596378 0.27554733 0.2757823  0.2761821  0.276393   0.2771179
 0.27786696 0.2777015  0.27816355 0.27823162 0.27764487 0.2768831
 0.27691668 0.27544713 0.27371523 0.27390584 0.2735471  0.26988238
 0.26733208 0.2679831  0.2681947  0.26825884 0.2680036  0.26723787
 0.26585728 0.26425695 0.26322833 0.2638214  0.26451096 0.26527655
 0.26548132 0.26484528 0.2644446  0.26398614 0.26327467 0.26258776
 0.26129377 0.25985414 0.2595176  0.26085073 0.2618385  0.26163772
 0.26261112 0.26432216 0.26527306 0.26525947 0.26370013 0.26309064
 0.26210326 0.26131153 0.26078224 0.26039496 0.25996736 0.2596062
 0.2599431  0.26051322 0.2611464  0.26146057 0.26163402 0.26155162
 0.261171   0.26100764 0.2610591  0.26123962 0.26123047 0.26144448
 0.26234797 0.26329893 0.26328644 0.2629295  0.26257378 0.26234633
 0.26130706 0.26042888 0.26006606 0.26073763 0.26082835 0.2608619
 0.26162735 0.26162642 0.26215932 0.26218492 0.26266155 0.26274568
 0.2627983  0.2633338  0.26351294 0.26329708 0.26290995 0.26277357
 0.26312062 0.2630235  0.26239204 0.2619729  0.26139963 0.26164243
 0.261264   0.26056585 0.2602491  0.26079917 0.2614002  0.26104078
 0.2620607  0.26249215 0.26328182 0.26360053 0.2636294  0.26255623
 0.2615697  0.26161093 0.26155573 0.261389   0.2615519  0.26204646
 0.2624656  0.2622106  0.2614252  0.26095837 0.26045483 0.26042956
 0.26024398 0.26006588 0.26046377 0.2611098  0.26187918 0.26250824
 0.26394105 0.26523638 0.26654512 0.26658303 0.26640034 0.26517603
 0.26407576 0.26278964 0.26085612 0.25979272 0.25996035 0.26212165
 0.26420864 0.26482528 0.26483336 0.26529428 0.26544154 0.26557776
 0.26521763 0.26473707 0.2647607  0.26555344 0.26618862 0.26678166
 0.2679838  0.2689434  0.27013785 0.27030867 0.2699501  0.26983142
 0.27276728 0.274889   0.27307773 0.27137297 0.27155775 0.27336156
 0.27518725 0.27649602 0.2767245  0.2772306  0.27743563 0.27694428
 0.27624646 0.27577284 0.27579126 0.2760314  0.2760898  0.27673748
 0.27719203 0.27735788 0.2780617  0.2777538  0.27721265 0.27628642
 0.27568573 0.27381405 0.27116895 0.2708812  0.2710476  0.26794514
 0.26602483 0.26657286 0.2668784  0.2669007  0.26639768 0.26601064
 0.2650831  0.26398376 0.2630194  0.2631761  0.26324567 0.26358372
 0.2646842  0.2643867  0.26452762 0.2644655  0.26380518 0.26266766
 0.26089975 0.25932556 0.25872666 0.25969088 0.26130334 0.26208952
 0.26341358 0.26513118 0.26583573 0.26600948 0.2649167  0.2640014
 0.2629239  0.26250377 0.2616656  0.26142833 0.26133353 0.26147598
 0.26212475 0.26230437 0.26288852 0.2629692  0.26330352 0.26314864
 0.2627796  0.26273957 0.2625347  0.26259074 0.26276448 0.26305503
 0.26406553 0.26474333 0.26429385 0.26437247 0.26365983 0.26300967
 0.26308507 0.26308125 0.26211393 0.2621165  0.26210433 0.26159933
 0.2620633  0.26261824 0.26328593 0.26344252 0.26376817 0.26414964
 0.26429933 0.26439005 0.26424792 0.26399225 0.26378945 0.26361486
 0.2636981  0.26387492 0.263263   0.26258665 0.26208997 0.26248667
 0.2627418  0.2624041  0.26219156 0.26245973 0.26257893 0.26275414
 0.26354375 0.26378435 0.2648184  0.26524112 0.26508835 0.26410845
 0.2632084  0.26300865 0.26311043 0.26283634 0.2629933  0.263642
 0.26428637 0.26433608 0.26364666 0.2630822  0.26236087 0.26194865
 0.26185673 0.2621025  0.26220918 0.26259783 0.26341206 0.26365545
 0.2648762  0.2662981  0.26742044 0.26752487 0.26699382 0.26543778
 0.26438665 0.2633755  0.2608719  0.259103   0.2600411  0.26184878
 0.26391682 0.2651975  0.26544848 0.26585782 0.26531792 0.26484674
 0.26475415 0.26469895 0.26500782 0.2652504  0.2656186  0.2659239
 0.26701692 0.26815104 0.2693036  0.26972243 0.26904482 0.26864615
 0.27165988 0.27523825 0.27363822 0.2712873  0.27157748 0.27292085
 0.27509135 0.27696356 0.27745408 0.27811977 0.2782225  0.27768978
 0.27686825 0.276569   0.2763274  0.27645963 0.2768487  0.2773654
 0.27798113 0.27787414 0.2785002  0.27849108 0.27773148 0.27650994
 0.27579892 0.27381343 0.27100372 0.2702221  0.26941726 0.266259
 0.26516852 0.26594004 0.2665954  0.26709446 0.26662064 0.26640794
 0.26578608 0.265021   0.26416942 0.26402968 0.26425803 0.26481757
 0.26504526 0.26475415 0.26493147 0.26426533 0.26381567 0.2630145
 0.2615543  0.26027825 0.25935236 0.26055005 0.26195142 0.26218107
 0.2637393  0.26572222 0.26647073 0.26636574 0.2653832  0.26470736
 0.26391938 0.26347166 0.2626239  0.26280233 0.26270095 0.26238838
 0.26258087 0.26300627 0.26323202 0.26360017 0.26403996 0.26411772
 0.2636891  0.26338482 0.26340073 0.26334694 0.2633192  0.26364908
 0.2643024  0.26521543 0.2651214  0.26513746 0.2646198  0.2644262
 0.26402086 0.26322234 0.26233175 0.26222894 0.2621336  0.2619358
 0.2622462  0.2628094  0.2633311  0.2634816  0.2639963  0.26410636
 0.26406547 0.26383004 0.2641002  0.26449183 0.2642131  0.26430544
 0.26448166 0.26433203 0.26359087 0.2633588  0.26320377 0.26270363
 0.26230457 0.2621818  0.2618727  0.26245672 0.26304787 0.26301348
 0.26383954 0.26478285 0.26566464 0.2655164  0.2653783  0.2649934
 0.26434922 0.26393896 0.26370704 0.2634242  0.26345083 0.2638947
 0.2645519  0.264459   0.26358157 0.26304346 0.26283395 0.26297542
 0.26293325 0.26253292 0.2625583  0.26321682 0.2639099  0.2643163
 0.26574963 0.2667587  0.268115   0.2683978  0.26746896 0.2659195
 0.26485646 0.2637442  0.2611954  0.25934577 0.25959727 0.26136538
 0.2638136  0.26485518 0.26448616 0.26457158 0.2647948  0.2651658
 0.26443374 0.2638365  0.2638669  0.26413447 0.26505157 0.26591316
 0.26647252 0.26698366 0.2683687  0.26879224 0.2683373  0.26826265
 0.2709031  0.27371663 0.27244377 0.27082473 0.27109906 0.27302888
 0.27483395 0.27579823 0.27640015 0.2773869  0.2773418  0.2768109
 0.27584964 0.27535254 0.27526864 0.2758155  0.27636108 0.27675307
 0.27684364 0.27675614 0.27640006 0.27645138 0.2762506  0.27495357
 0.2743556  0.2723405  0.26968747 0.2691768  0.26862538 0.26573977
 0.26361758 0.26429406 0.26512253 0.2654778  0.26527575 0.26476067
 0.26361412 0.26208326 0.26132923 0.26157618 0.2616237  0.2623318
 0.26252747 0.2623304  0.2624267  0.26232383 0.2620023  0.26083866
 0.25957927 0.2580253  0.2571523  0.2586163  0.26025152 0.26049262
 0.26261306 0.26517183 0.26561177 0.2651732  0.26449418 0.26415265
 0.26336893 0.26227447 0.26117852 0.26098743 0.26096156 0.26091135
 0.26107004 0.26139113 0.26181957 0.26231545 0.26220012 0.2619752
 0.26189762 0.2615239  0.26151827 0.26203018 0.2624251  0.2623421
 0.2630429  0.26422885 0.26399705 0.26375538 0.26323068 0.2627157
 0.26188746 0.2611704  0.26064122 0.26065743 0.26100978 0.26086923
 0.26091942 0.2611919  0.26144505 0.26199964 0.2626519  0.26306507
 0.26303858 0.26297814 0.26322174 0.26340285 0.26332137 0.2632244
 0.26362252 0.26353794 0.26239136 0.26198626 0.26167247 0.2615827
 0.26077917 0.259939   0.2600966  0.2607004  0.2613538  0.26141956
 0.26206404 0.2628421  0.26333848 0.26360217 0.26421282 0.26352566
 0.26289946 0.26260793 0.2620626  0.2621308  0.26236445 0.26230663
 0.26289734 0.26324257 0.2627657  0.26213226 0.2613849  0.26078093
 0.26021403 0.26010332 0.25981182 0.26033694 0.26140982 0.26180744
 0.263137   0.26463246 0.2657743  0.26630265 0.26592737 0.2646546
 0.2639558  0.2627082  0.26036268 0.25887838 0.2587013  0.26037064
 0.2627553  0.26368827 0.2637456  0.26399124 0.26421437 0.2642952
 0.2631962  0.26239768 0.26250172 0.26315984 0.26418918 0.26530743
 0.2659828  0.26603737 0.26682428 0.2679595  0.26792958 0.26766717
 0.270422   0.27349174 0.27325752 0.2717747  0.27147606 0.27317092
 0.27488792 0.27627814 0.27667528 0.27759323 0.2781369  0.27765313
 0.2762234  0.27521667 0.27508482 0.274887   0.27504218 0.27547324
 0.27508074 0.2739999  0.2740216  0.27502835 0.27699843 0.27622846]
