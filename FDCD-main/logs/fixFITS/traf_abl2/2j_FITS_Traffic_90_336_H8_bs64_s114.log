Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j336_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j336_H8_FITS_custom_ftM_sl90_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11855
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=42, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  917554176.0
params:  8514.0
Trainable parameters:  8514
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 22.8078453540802
Epoch: 1, Steps: 92 | Train Loss: 1.8667892 Vali Loss: 1.7887428 Test Loss: 2.1180112
Validation loss decreased (inf --> 1.788743).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 22.964088678359985
Epoch: 2, Steps: 92 | Train Loss: 1.1590687 Vali Loss: 1.3032711 Test Loss: 1.5358329
Validation loss decreased (1.788743 --> 1.303271).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 22.542076587677002
Epoch: 3, Steps: 92 | Train Loss: 0.8675656 Vali Loss: 1.0652949 Test Loss: 1.2548144
Validation loss decreased (1.303271 --> 1.065295).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 22.388577938079834
Epoch: 4, Steps: 92 | Train Loss: 0.7151959 Vali Loss: 0.9303698 Test Loss: 1.0975884
Validation loss decreased (1.065295 --> 0.930370).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 21.942602157592773
Epoch: 5, Steps: 92 | Train Loss: 0.6230255 Vali Loss: 0.8439230 Test Loss: 0.9991261
Validation loss decreased (0.930370 --> 0.843923).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 21.7922945022583
Epoch: 6, Steps: 92 | Train Loss: 0.5620070 Vali Loss: 0.7844520 Test Loss: 0.9330769
Validation loss decreased (0.843923 --> 0.784452).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 22.13525366783142
Epoch: 7, Steps: 92 | Train Loss: 0.5190805 Vali Loss: 0.7423229 Test Loss: 0.8856724
Validation loss decreased (0.784452 --> 0.742323).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 22.179981470108032
Epoch: 8, Steps: 92 | Train Loss: 0.4872104 Vali Loss: 0.7102628 Test Loss: 0.8506378
Validation loss decreased (0.742323 --> 0.710263).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 22.031954050064087
Epoch: 9, Steps: 92 | Train Loss: 0.4629239 Vali Loss: 0.6852229 Test Loss: 0.8234604
Validation loss decreased (0.710263 --> 0.685223).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 21.55504870414734
Epoch: 10, Steps: 92 | Train Loss: 0.4438053 Vali Loss: 0.6652299 Test Loss: 0.8019226
Validation loss decreased (0.685223 --> 0.665230).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 21.596800088882446
Epoch: 11, Steps: 92 | Train Loss: 0.4286379 Vali Loss: 0.6496318 Test Loss: 0.7843807
Validation loss decreased (0.665230 --> 0.649632).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 22.11041569709778
Epoch: 12, Steps: 92 | Train Loss: 0.4160950 Vali Loss: 0.6366768 Test Loss: 0.7699783
Validation loss decreased (0.649632 --> 0.636677).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 22.291011571884155
Epoch: 13, Steps: 92 | Train Loss: 0.4056516 Vali Loss: 0.6249952 Test Loss: 0.7576683
Validation loss decreased (0.636677 --> 0.624995).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 22.104347944259644
Epoch: 14, Steps: 92 | Train Loss: 0.3968729 Vali Loss: 0.6151053 Test Loss: 0.7471622
Validation loss decreased (0.624995 --> 0.615105).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 22.19155216217041
Epoch: 15, Steps: 92 | Train Loss: 0.3893317 Vali Loss: 0.6075765 Test Loss: 0.7381867
Validation loss decreased (0.615105 --> 0.607576).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 21.61805033683777
Epoch: 16, Steps: 92 | Train Loss: 0.3829689 Vali Loss: 0.6005288 Test Loss: 0.7302585
Validation loss decreased (0.607576 --> 0.600529).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 21.77294659614563
Epoch: 17, Steps: 92 | Train Loss: 0.3771864 Vali Loss: 0.5946014 Test Loss: 0.7234421
Validation loss decreased (0.600529 --> 0.594601).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 21.924365758895874
Epoch: 18, Steps: 92 | Train Loss: 0.3723709 Vali Loss: 0.5887702 Test Loss: 0.7172269
Validation loss decreased (0.594601 --> 0.588770).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 21.985406398773193
Epoch: 19, Steps: 92 | Train Loss: 0.3680610 Vali Loss: 0.5843248 Test Loss: 0.7117416
Validation loss decreased (0.588770 --> 0.584325).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 22.27772569656372
Epoch: 20, Steps: 92 | Train Loss: 0.3642032 Vali Loss: 0.5800075 Test Loss: 0.7068843
Validation loss decreased (0.584325 --> 0.580007).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 21.862022161483765
Epoch: 21, Steps: 92 | Train Loss: 0.3607993 Vali Loss: 0.5748812 Test Loss: 0.7024000
Validation loss decreased (0.580007 --> 0.574881).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 22.43726658821106
Epoch: 22, Steps: 92 | Train Loss: 0.3576464 Vali Loss: 0.5717898 Test Loss: 0.6984798
Validation loss decreased (0.574881 --> 0.571790).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 22.69883441925049
Epoch: 23, Steps: 92 | Train Loss: 0.3548571 Vali Loss: 0.5687044 Test Loss: 0.6947972
Validation loss decreased (0.571790 --> 0.568704).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 22.946853399276733
Epoch: 24, Steps: 92 | Train Loss: 0.3523191 Vali Loss: 0.5652007 Test Loss: 0.6915253
Validation loss decreased (0.568704 --> 0.565201).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 23.0167293548584
Epoch: 25, Steps: 92 | Train Loss: 0.3500366 Vali Loss: 0.5628860 Test Loss: 0.6885629
Validation loss decreased (0.565201 --> 0.562886).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 22.93285346031189
Epoch: 26, Steps: 92 | Train Loss: 0.3479677 Vali Loss: 0.5604209 Test Loss: 0.6857979
Validation loss decreased (0.562886 --> 0.560421).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 22.393003702163696
Epoch: 27, Steps: 92 | Train Loss: 0.3460242 Vali Loss: 0.5581344 Test Loss: 0.6832501
Validation loss decreased (0.560421 --> 0.558134).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 22.12312150001526
Epoch: 28, Steps: 92 | Train Loss: 0.3442839 Vali Loss: 0.5560528 Test Loss: 0.6809306
Validation loss decreased (0.558134 --> 0.556053).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 21.92410445213318
Epoch: 29, Steps: 92 | Train Loss: 0.3427231 Vali Loss: 0.5534037 Test Loss: 0.6788036
Validation loss decreased (0.556053 --> 0.553404).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 21.96519422531128
Epoch: 30, Steps: 92 | Train Loss: 0.3412551 Vali Loss: 0.5523638 Test Loss: 0.6768061
Validation loss decreased (0.553404 --> 0.552364).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 21.93510365486145
Epoch: 31, Steps: 92 | Train Loss: 0.3398866 Vali Loss: 0.5505715 Test Loss: 0.6750072
Validation loss decreased (0.552364 --> 0.550572).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 22.18946647644043
Epoch: 32, Steps: 92 | Train Loss: 0.3386826 Vali Loss: 0.5488970 Test Loss: 0.6733320
Validation loss decreased (0.550572 --> 0.548897).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 21.975512266159058
Epoch: 33, Steps: 92 | Train Loss: 0.3374093 Vali Loss: 0.5477214 Test Loss: 0.6717308
Validation loss decreased (0.548897 --> 0.547721).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 22.045793533325195
Epoch: 34, Steps: 92 | Train Loss: 0.3364406 Vali Loss: 0.5464078 Test Loss: 0.6702453
Validation loss decreased (0.547721 --> 0.546408).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 22.226597547531128
Epoch: 35, Steps: 92 | Train Loss: 0.3353560 Vali Loss: 0.5449842 Test Loss: 0.6689230
Validation loss decreased (0.546408 --> 0.544984).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 22.00655198097229
Epoch: 36, Steps: 92 | Train Loss: 0.3344773 Vali Loss: 0.5437684 Test Loss: 0.6677011
Validation loss decreased (0.544984 --> 0.543768).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 22.158674716949463
Epoch: 37, Steps: 92 | Train Loss: 0.3334994 Vali Loss: 0.5431934 Test Loss: 0.6665117
Validation loss decreased (0.543768 --> 0.543193).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 21.92403745651245
Epoch: 38, Steps: 92 | Train Loss: 0.3327443 Vali Loss: 0.5419540 Test Loss: 0.6654270
Validation loss decreased (0.543193 --> 0.541954).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 22.29039454460144
Epoch: 39, Steps: 92 | Train Loss: 0.3320036 Vali Loss: 0.5408531 Test Loss: 0.6643938
Validation loss decreased (0.541954 --> 0.540853).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 22.142292737960815
Epoch: 40, Steps: 92 | Train Loss: 0.3313255 Vali Loss: 0.5402935 Test Loss: 0.6634292
Validation loss decreased (0.540853 --> 0.540294).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 21.95973491668701
Epoch: 41, Steps: 92 | Train Loss: 0.3307107 Vali Loss: 0.5391157 Test Loss: 0.6625205
Validation loss decreased (0.540294 --> 0.539116).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 21.728275537490845
Epoch: 42, Steps: 92 | Train Loss: 0.3300529 Vali Loss: 0.5381046 Test Loss: 0.6617010
Validation loss decreased (0.539116 --> 0.538105).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 22.050042629241943
Epoch: 43, Steps: 92 | Train Loss: 0.3295053 Vali Loss: 0.5370312 Test Loss: 0.6609344
Validation loss decreased (0.538105 --> 0.537031).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 21.95731520652771
Epoch: 44, Steps: 92 | Train Loss: 0.3289632 Vali Loss: 0.5370031 Test Loss: 0.6601989
Validation loss decreased (0.537031 --> 0.537003).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 22.26120376586914
Epoch: 45, Steps: 92 | Train Loss: 0.3283567 Vali Loss: 0.5362824 Test Loss: 0.6594928
Validation loss decreased (0.537003 --> 0.536282).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 22.203652143478394
Epoch: 46, Steps: 92 | Train Loss: 0.3279746 Vali Loss: 0.5360470 Test Loss: 0.6588530
Validation loss decreased (0.536282 --> 0.536047).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 22.255183219909668
Epoch: 47, Steps: 92 | Train Loss: 0.3274671 Vali Loss: 0.5355594 Test Loss: 0.6582272
Validation loss decreased (0.536047 --> 0.535559).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 21.99095845222473
Epoch: 48, Steps: 92 | Train Loss: 0.3270923 Vali Loss: 0.5346274 Test Loss: 0.6576530
Validation loss decreased (0.535559 --> 0.534627).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 21.350692749023438
Epoch: 49, Steps: 92 | Train Loss: 0.3265389 Vali Loss: 0.5340709 Test Loss: 0.6571088
Validation loss decreased (0.534627 --> 0.534071).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 24.555840015411377
Epoch: 50, Steps: 92 | Train Loss: 0.3261654 Vali Loss: 0.5337035 Test Loss: 0.6565694
Validation loss decreased (0.534071 --> 0.533704).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 24.55940580368042
Epoch: 51, Steps: 92 | Train Loss: 0.3258263 Vali Loss: 0.5333545 Test Loss: 0.6560939
Validation loss decreased (0.533704 --> 0.533355).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 23.942702293395996
Epoch: 52, Steps: 92 | Train Loss: 0.3255647 Vali Loss: 0.5326152 Test Loss: 0.6556362
Validation loss decreased (0.533355 --> 0.532615).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 24.107743978500366
Epoch: 53, Steps: 92 | Train Loss: 0.3252133 Vali Loss: 0.5327535 Test Loss: 0.6551895
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 24.077710390090942
Epoch: 54, Steps: 92 | Train Loss: 0.3249320 Vali Loss: 0.5322035 Test Loss: 0.6548074
Validation loss decreased (0.532615 --> 0.532203).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 23.82034969329834
Epoch: 55, Steps: 92 | Train Loss: 0.3247653 Vali Loss: 0.5318049 Test Loss: 0.6543979
Validation loss decreased (0.532203 --> 0.531805).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 23.73634433746338
Epoch: 56, Steps: 92 | Train Loss: 0.3243971 Vali Loss: 0.5312265 Test Loss: 0.6540249
Validation loss decreased (0.531805 --> 0.531227).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 24.62810707092285
Epoch: 57, Steps: 92 | Train Loss: 0.3241013 Vali Loss: 0.5311143 Test Loss: 0.6536773
Validation loss decreased (0.531227 --> 0.531114).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 24.241687774658203
Epoch: 58, Steps: 92 | Train Loss: 0.3238710 Vali Loss: 0.5307819 Test Loss: 0.6533446
Validation loss decreased (0.531114 --> 0.530782).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 24.104372024536133
Epoch: 59, Steps: 92 | Train Loss: 0.3236258 Vali Loss: 0.5306113 Test Loss: 0.6530268
Validation loss decreased (0.530782 --> 0.530611).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 23.380163192749023
Epoch: 60, Steps: 92 | Train Loss: 0.3234811 Vali Loss: 0.5302756 Test Loss: 0.6527235
Validation loss decreased (0.530611 --> 0.530276).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 23.97507071495056
Epoch: 61, Steps: 92 | Train Loss: 0.3232360 Vali Loss: 0.5302209 Test Loss: 0.6524370
Validation loss decreased (0.530276 --> 0.530221).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 24.12940764427185
Epoch: 62, Steps: 92 | Train Loss: 0.3230800 Vali Loss: 0.5292229 Test Loss: 0.6521669
Validation loss decreased (0.530221 --> 0.529223).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 24.143120288848877
Epoch: 63, Steps: 92 | Train Loss: 0.3229112 Vali Loss: 0.5292665 Test Loss: 0.6518952
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 24.45507025718689
Epoch: 64, Steps: 92 | Train Loss: 0.3226173 Vali Loss: 0.5291682 Test Loss: 0.6516469
Validation loss decreased (0.529223 --> 0.529168).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 24.006848573684692
Epoch: 65, Steps: 92 | Train Loss: 0.3225239 Vali Loss: 0.5289564 Test Loss: 0.6514175
Validation loss decreased (0.529168 --> 0.528956).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 24.173402309417725
Epoch: 66, Steps: 92 | Train Loss: 0.3222692 Vali Loss: 0.5290738 Test Loss: 0.6511944
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 23.955015897750854
Epoch: 67, Steps: 92 | Train Loss: 0.3220436 Vali Loss: 0.5288363 Test Loss: 0.6509696
Validation loss decreased (0.528956 --> 0.528836).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 23.709075450897217
Epoch: 68, Steps: 92 | Train Loss: 0.3220697 Vali Loss: 0.5281610 Test Loss: 0.6507605
Validation loss decreased (0.528836 --> 0.528161).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 23.61901831626892
Epoch: 69, Steps: 92 | Train Loss: 0.3218834 Vali Loss: 0.5281277 Test Loss: 0.6505722
Validation loss decreased (0.528161 --> 0.528128).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 23.39433240890503
Epoch: 70, Steps: 92 | Train Loss: 0.3217406 Vali Loss: 0.5284373 Test Loss: 0.6503844
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 23.653888702392578
Epoch: 71, Steps: 92 | Train Loss: 0.3215661 Vali Loss: 0.5278703 Test Loss: 0.6502152
Validation loss decreased (0.528128 --> 0.527870).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 23.190876245498657
Epoch: 72, Steps: 92 | Train Loss: 0.3214524 Vali Loss: 0.5275304 Test Loss: 0.6500424
Validation loss decreased (0.527870 --> 0.527530).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 23.96133518218994
Epoch: 73, Steps: 92 | Train Loss: 0.3213312 Vali Loss: 0.5276130 Test Loss: 0.6498874
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 24.73075556755066
Epoch: 74, Steps: 92 | Train Loss: 0.3212875 Vali Loss: 0.5277570 Test Loss: 0.6497394
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 24.191052198410034
Epoch: 75, Steps: 92 | Train Loss: 0.3211624 Vali Loss: 0.5271583 Test Loss: 0.6495913
Validation loss decreased (0.527530 --> 0.527158).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 24.08874273300171
Epoch: 76, Steps: 92 | Train Loss: 0.3210419 Vali Loss: 0.5276116 Test Loss: 0.6494507
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 24.51168966293335
Epoch: 77, Steps: 92 | Train Loss: 0.3209831 Vali Loss: 0.5270807 Test Loss: 0.6493205
Validation loss decreased (0.527158 --> 0.527081).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 24.228658437728882
Epoch: 78, Steps: 92 | Train Loss: 0.3209455 Vali Loss: 0.5266634 Test Loss: 0.6491879
Validation loss decreased (0.527081 --> 0.526663).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 23.48898434638977
Epoch: 79, Steps: 92 | Train Loss: 0.3207362 Vali Loss: 0.5268446 Test Loss: 0.6490669
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 23.87493085861206
Epoch: 80, Steps: 92 | Train Loss: 0.3207548 Vali Loss: 0.5268801 Test Loss: 0.6489536
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 24.445566177368164
Epoch: 81, Steps: 92 | Train Loss: 0.3207370 Vali Loss: 0.5273036 Test Loss: 0.6488439
EarlyStopping counter: 3 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 24.04186463356018
Epoch: 82, Steps: 92 | Train Loss: 0.3205724 Vali Loss: 0.5262977 Test Loss: 0.6487323
Validation loss decreased (0.526663 --> 0.526298).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 24.039182901382446
Epoch: 83, Steps: 92 | Train Loss: 0.3204874 Vali Loss: 0.5263166 Test Loss: 0.6486328
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 24.42647409439087
Epoch: 84, Steps: 92 | Train Loss: 0.3203705 Vali Loss: 0.5265284 Test Loss: 0.6485429
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 24.02555203437805
Epoch: 85, Steps: 92 | Train Loss: 0.3203833 Vali Loss: 0.5263327 Test Loss: 0.6484490
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 24.00696301460266
Epoch: 86, Steps: 92 | Train Loss: 0.3202765 Vali Loss: 0.5266057 Test Loss: 0.6483582
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 23.845924377441406
Epoch: 87, Steps: 92 | Train Loss: 0.3202941 Vali Loss: 0.5259460 Test Loss: 0.6482766
Validation loss decreased (0.526298 --> 0.525946).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 23.698744535446167
Epoch: 88, Steps: 92 | Train Loss: 0.3202339 Vali Loss: 0.5262397 Test Loss: 0.6481989
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 23.840144157409668
Epoch: 89, Steps: 92 | Train Loss: 0.3201521 Vali Loss: 0.5261748 Test Loss: 0.6481264
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 23.71807336807251
Epoch: 90, Steps: 92 | Train Loss: 0.3201282 Vali Loss: 0.5257731 Test Loss: 0.6480550
Validation loss decreased (0.525946 --> 0.525773).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 24.446860551834106
Epoch: 91, Steps: 92 | Train Loss: 0.3200016 Vali Loss: 0.5262776 Test Loss: 0.6479834
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 23.947615385055542
Epoch: 92, Steps: 92 | Train Loss: 0.3200567 Vali Loss: 0.5259289 Test Loss: 0.6479218
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 23.568159580230713
Epoch: 93, Steps: 92 | Train Loss: 0.3199245 Vali Loss: 0.5262402 Test Loss: 0.6478623
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 23.17954397201538
Epoch: 94, Steps: 92 | Train Loss: 0.3199243 Vali Loss: 0.5258184 Test Loss: 0.6478015
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 23.585129261016846
Epoch: 95, Steps: 92 | Train Loss: 0.3198852 Vali Loss: 0.5253181 Test Loss: 0.6477435
Validation loss decreased (0.525773 --> 0.525318).  Saving model ...
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 23.101489067077637
Epoch: 96, Steps: 92 | Train Loss: 0.3198377 Vali Loss: 0.5256512 Test Loss: 0.6476930
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 23.39993166923523
Epoch: 97, Steps: 92 | Train Loss: 0.3198033 Vali Loss: 0.5259250 Test Loss: 0.6476416
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 23.079228401184082
Epoch: 98, Steps: 92 | Train Loss: 0.3197172 Vali Loss: 0.5256776 Test Loss: 0.6475943
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 22.752254962921143
Epoch: 99, Steps: 92 | Train Loss: 0.3196995 Vali Loss: 0.5253832 Test Loss: 0.6475471
EarlyStopping counter: 4 out of 10
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 23.025770902633667
Epoch: 100, Steps: 92 | Train Loss: 0.3197479 Vali Loss: 0.5259190 Test Loss: 0.6475050
EarlyStopping counter: 5 out of 10
Updating learning rate to 3.1160680107021042e-06
train 11855
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=42, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  917554176.0
params:  8514.0
Trainable parameters:  8514
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 23.706539392471313
Epoch: 1, Steps: 92 | Train Loss: 0.3957265 Vali Loss: 0.5150959 Test Loss: 0.6366305
Validation loss decreased (inf --> 0.515096).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 23.751513719558716
Epoch: 2, Steps: 92 | Train Loss: 0.3906214 Vali Loss: 0.5113100 Test Loss: 0.6330082
Validation loss decreased (0.515096 --> 0.511310).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 24.655394315719604
Epoch: 3, Steps: 92 | Train Loss: 0.3885768 Vali Loss: 0.5098936 Test Loss: 0.6318399
Validation loss decreased (0.511310 --> 0.509894).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 24.75495195388794
Epoch: 4, Steps: 92 | Train Loss: 0.3879265 Vali Loss: 0.5098851 Test Loss: 0.6316709
Validation loss decreased (0.509894 --> 0.509885).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 24.18263530731201
Epoch: 5, Steps: 92 | Train Loss: 0.3876014 Vali Loss: 0.5093918 Test Loss: 0.6317690
Validation loss decreased (0.509885 --> 0.509392).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 24.461036920547485
Epoch: 6, Steps: 92 | Train Loss: 0.3874755 Vali Loss: 0.5093538 Test Loss: 0.6316473
Validation loss decreased (0.509392 --> 0.509354).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 24.102030038833618
Epoch: 7, Steps: 92 | Train Loss: 0.3876287 Vali Loss: 0.5093389 Test Loss: 0.6317153
Validation loss decreased (0.509354 --> 0.509339).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 24.023210287094116
Epoch: 8, Steps: 92 | Train Loss: 0.3874960 Vali Loss: 0.5085235 Test Loss: 0.6315331
Validation loss decreased (0.509339 --> 0.508523).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 23.96668767929077
Epoch: 9, Steps: 92 | Train Loss: 0.3873944 Vali Loss: 0.5098258 Test Loss: 0.6318725
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 24.180052518844604
Epoch: 10, Steps: 92 | Train Loss: 0.3875150 Vali Loss: 0.5084574 Test Loss: 0.6315816
Validation loss decreased (0.508523 --> 0.508457).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 23.784114599227905
Epoch: 11, Steps: 92 | Train Loss: 0.3873963 Vali Loss: 0.5091473 Test Loss: 0.6315552
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 24.084737539291382
Epoch: 12, Steps: 92 | Train Loss: 0.3873383 Vali Loss: 0.5094253 Test Loss: 0.6317187
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 23.866467475891113
Epoch: 13, Steps: 92 | Train Loss: 0.3874281 Vali Loss: 0.5090588 Test Loss: 0.6317050
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 24.944719552993774
Epoch: 14, Steps: 92 | Train Loss: 0.3873538 Vali Loss: 0.5091496 Test Loss: 0.6315594
EarlyStopping counter: 4 out of 10
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 25.038328170776367
Epoch: 15, Steps: 92 | Train Loss: 0.3873729 Vali Loss: 0.5094271 Test Loss: 0.6315854
EarlyStopping counter: 5 out of 10
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 25.764629364013672
Epoch: 16, Steps: 92 | Train Loss: 0.3874461 Vali Loss: 0.5095587 Test Loss: 0.6314847
EarlyStopping counter: 6 out of 10
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 24.121085166931152
Epoch: 17, Steps: 92 | Train Loss: 0.3873643 Vali Loss: 0.5094224 Test Loss: 0.6314445
EarlyStopping counter: 7 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 24.174490213394165
Epoch: 18, Steps: 92 | Train Loss: 0.3873214 Vali Loss: 0.5086910 Test Loss: 0.6314057
EarlyStopping counter: 8 out of 10
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 24.212138175964355
Epoch: 19, Steps: 92 | Train Loss: 0.3873236 Vali Loss: 0.5091549 Test Loss: 0.6313629
EarlyStopping counter: 9 out of 10
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 23.953284740447998
Epoch: 20, Steps: 92 | Train Loss: 0.3872685 Vali Loss: 0.5095708 Test Loss: 0.6317418
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j336_H8_FITS_custom_ftM_sl90_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.629700243473053, mae:0.3769499957561493, rse:0.6521781086921692, corr:[0.2612641  0.2789545  0.27945164 0.27792478 0.2776149  0.27872112
 0.28001192 0.2805471  0.28023434 0.28007802 0.28034648 0.28022283
 0.27923915 0.27862954 0.27871388 0.27904844 0.27934182 0.28018698
 0.28121462 0.28290212 0.2847046  0.2849382  0.2852146  0.28501746
 0.28893977 0.29330662 0.29324368 0.29142597 0.2908704  0.29188707
 0.29413787 0.29548973 0.2951735  0.29533103 0.29516384 0.2944443
 0.29352957 0.29262793 0.29223365 0.29254824 0.2929976  0.29328272
 0.29480374 0.2952236  0.29547444 0.295603   0.29534593 0.29458752
 0.2948202  0.29282972 0.29088995 0.29060867 0.2889802  0.2848849
 0.28300768 0.2840216  0.2847724  0.2845966  0.2838169  0.28280157
 0.2812709  0.28032225 0.2794973  0.27963167 0.27953017 0.27956623
 0.28085282 0.28166682 0.28147417 0.28138623 0.2818183  0.2802568
 0.2782115  0.27659947 0.275659   0.27695546 0.2785589  0.2787999
 0.2797265  0.28148764 0.28213313 0.28138608 0.28017423 0.27938324
 0.27818632 0.27745095 0.27659672 0.2765876  0.27596486 0.27560848
 0.27595222 0.27647638 0.2770167  0.27702942 0.27779263 0.27811953
 0.27749953 0.27736464 0.27717602 0.2774743  0.2781369  0.27853274
 0.27961048 0.2804651  0.28016067 0.27976286 0.2785651  0.27792236
 0.27750155 0.27705717 0.27638504 0.2765847  0.27665693 0.27663618
 0.27727214 0.27751482 0.278318   0.2787965  0.2791898  0.27942103
 0.27957487 0.27939594 0.27919146 0.27923515 0.279545   0.2797814
 0.27974826 0.27936995 0.2784733  0.27784526 0.27723438 0.27740178
 0.27727973 0.27692214 0.276769   0.27659723 0.27684978 0.27725837
 0.2780949  0.279059   0.28031194 0.2805134  0.28042173 0.27914783
 0.2781468  0.27787867 0.2780001  0.27820376 0.27824485 0.278206
 0.27862862 0.2785908  0.27796355 0.27722278 0.2763834  0.27562404
 0.2754744  0.2754795  0.27602118 0.27640995 0.27714407 0.27803114
 0.27916425 0.2799571  0.28159362 0.28197363 0.28136918 0.28009745
 0.27915    0.27799913 0.2763371  0.2759545  0.27667028 0.27824566
 0.28030312 0.28158867 0.28188697 0.28203553 0.2821208  0.28134963
 0.2808803  0.28045678 0.28053883 0.28103083 0.28168622 0.28198132
 0.2828441  0.28360656 0.28490904 0.2851893  0.28515795 0.28446203
 0.28702068 0.2899328  0.28900766 0.28788686 0.28801316 0.28928754
 0.29105023 0.29213834 0.29237887 0.29291043 0.29291078 0.29268643
 0.29216444 0.29131997 0.2908496  0.29120246 0.29165575 0.29195702
 0.2929343  0.2934989  0.2936751  0.2932487  0.292524   0.2916941
 0.29124218 0.2889188  0.28721923 0.28681666 0.28482687 0.28077155
 0.27891347 0.27977362 0.28041714 0.2804848  0.27992022 0.27936164
 0.27801657 0.27760154 0.27796185 0.2778323  0.2776111  0.27798045
 0.27841946 0.27876043 0.27870947 0.27866933 0.27818817 0.27713615
 0.27584162 0.27441692 0.27344543 0.2744747  0.2759929  0.27641273
 0.27787778 0.27994522 0.2806839  0.280532   0.2792277  0.2786972
 0.27742255 0.27652022 0.2761316  0.27577657 0.27554074 0.27558574
 0.2753571  0.27568677 0.27635533 0.27650356 0.27652344 0.2767914
 0.27661532 0.27632836 0.2761878  0.2764375  0.27664968 0.2769754
 0.27827978 0.2796201  0.2795912  0.27934328 0.2780957  0.27755055
 0.27703932 0.276516   0.27621093 0.27634755 0.27608272 0.27617794
 0.27661145 0.2769742  0.2773863  0.2773588  0.2776888  0.27777958
 0.2777309  0.27771488 0.27754062 0.27790508 0.27806813 0.27810526
 0.2782589  0.27787402 0.2770991  0.27650103 0.27634504 0.27629894
 0.27605245 0.27606133 0.27588195 0.27609584 0.2767058  0.27717555
 0.27741036 0.27766132 0.2782986  0.2780669  0.27842042 0.2773805
 0.27627316 0.27637008 0.27626845 0.27612445 0.27636346 0.2767883
 0.277393   0.27746013 0.27756104 0.27743703 0.27710935 0.27649257
 0.27669263 0.27641386 0.27643692 0.27640325 0.27696377 0.27786255
 0.27758834 0.27706927 0.27684596 0.2768335  0.2750933  0.27440566]
