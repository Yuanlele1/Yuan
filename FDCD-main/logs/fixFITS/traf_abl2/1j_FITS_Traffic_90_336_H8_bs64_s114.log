Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j336_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j336_H8_FITS_custom_ftM_sl90_ll48_pl336_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11855
val 1421
test 3173
Model(
  (freq_upsampler): Linear(in_features=42, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  917554176.0
params:  8514.0
Trainable parameters:  8514
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 25.407540798187256
Epoch: 1, Steps: 92 | Train Loss: 1.7267277 Vali Loss: 1.4879107 Test Loss: 1.7650931
Validation loss decreased (inf --> 1.487911).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 25.678284883499146
Epoch: 2, Steps: 92 | Train Loss: 0.9716252 Vali Loss: 1.0020229 Test Loss: 1.1902020
Validation loss decreased (1.487911 --> 1.002023).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 26.425954580307007
Epoch: 3, Steps: 92 | Train Loss: 0.7019882 Vali Loss: 0.7932687 Test Loss: 0.9485269
Validation loss decreased (1.002023 --> 0.793269).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 25.59132981300354
Epoch: 4, Steps: 92 | Train Loss: 0.5777042 Vali Loss: 0.6910180 Test Loss: 0.8318768
Validation loss decreased (0.793269 --> 0.691018).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 25.8669114112854
Epoch: 5, Steps: 92 | Train Loss: 0.5135680 Vali Loss: 0.6360697 Test Loss: 0.7704176
Validation loss decreased (0.691018 --> 0.636070).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 25.293485403060913
Epoch: 6, Steps: 92 | Train Loss: 0.4776653 Vali Loss: 0.6036077 Test Loss: 0.7349110
Validation loss decreased (0.636070 --> 0.603608).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 25.678642988204956
Epoch: 7, Steps: 92 | Train Loss: 0.4560561 Vali Loss: 0.5837366 Test Loss: 0.7124364
Validation loss decreased (0.603608 --> 0.583737).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 26.88007950782776
Epoch: 8, Steps: 92 | Train Loss: 0.4417145 Vali Loss: 0.5696787 Test Loss: 0.6969159
Validation loss decreased (0.583737 --> 0.569679).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 25.769515991210938
Epoch: 9, Steps: 92 | Train Loss: 0.4317831 Vali Loss: 0.5592864 Test Loss: 0.6856464
Validation loss decreased (0.569679 --> 0.559286).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 25.898622035980225
Epoch: 10, Steps: 92 | Train Loss: 0.4244911 Vali Loss: 0.5512605 Test Loss: 0.6770651
Validation loss decreased (0.559286 --> 0.551261).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 26.44454312324524
Epoch: 11, Steps: 92 | Train Loss: 0.4189620 Vali Loss: 0.5453970 Test Loss: 0.6703483
Validation loss decreased (0.551261 --> 0.545397).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 25.495671033859253
Epoch: 12, Steps: 92 | Train Loss: 0.4145259 Vali Loss: 0.5407902 Test Loss: 0.6650293
Validation loss decreased (0.545397 --> 0.540790).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 25.746913194656372
Epoch: 13, Steps: 92 | Train Loss: 0.4110149 Vali Loss: 0.5363762 Test Loss: 0.6605176
Validation loss decreased (0.540790 --> 0.536376).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 26.09553337097168
Epoch: 14, Steps: 92 | Train Loss: 0.4080436 Vali Loss: 0.5328689 Test Loss: 0.6570017
Validation loss decreased (0.536376 --> 0.532869).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 26.19721007347107
Epoch: 15, Steps: 92 | Train Loss: 0.4055680 Vali Loss: 0.5309134 Test Loss: 0.6539794
Validation loss decreased (0.532869 --> 0.530913).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 26.934200763702393
Epoch: 16, Steps: 92 | Train Loss: 0.4036447 Vali Loss: 0.5286437 Test Loss: 0.6513775
Validation loss decreased (0.530913 --> 0.528644).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 25.677029609680176
Epoch: 17, Steps: 92 | Train Loss: 0.4017652 Vali Loss: 0.5268689 Test Loss: 0.6492674
Validation loss decreased (0.528644 --> 0.526869).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 26.343392372131348
Epoch: 18, Steps: 92 | Train Loss: 0.4004021 Vali Loss: 0.5249884 Test Loss: 0.6473756
Validation loss decreased (0.526869 --> 0.524988).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 25.80733060836792
Epoch: 19, Steps: 92 | Train Loss: 0.3992172 Vali Loss: 0.5238596 Test Loss: 0.6457828
Validation loss decreased (0.524988 --> 0.523860).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 25.902589559555054
Epoch: 20, Steps: 92 | Train Loss: 0.3980504 Vali Loss: 0.5226878 Test Loss: 0.6444160
Validation loss decreased (0.523860 --> 0.522688).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 24.96161198616028
Epoch: 21, Steps: 92 | Train Loss: 0.3971644 Vali Loss: 0.5205564 Test Loss: 0.6431630
Validation loss decreased (0.522688 --> 0.520556).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 25.680243253707886
Epoch: 22, Steps: 92 | Train Loss: 0.3962387 Vali Loss: 0.5201700 Test Loss: 0.6422067
Validation loss decreased (0.520556 --> 0.520170).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 25.073089361190796
Epoch: 23, Steps: 92 | Train Loss: 0.3955224 Vali Loss: 0.5194114 Test Loss: 0.6412011
Validation loss decreased (0.520170 --> 0.519411).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 24.722609996795654
Epoch: 24, Steps: 92 | Train Loss: 0.3948782 Vali Loss: 0.5181026 Test Loss: 0.6403997
Validation loss decreased (0.519411 --> 0.518103).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 24.639827251434326
Epoch: 25, Steps: 92 | Train Loss: 0.3942523 Vali Loss: 0.5178702 Test Loss: 0.6396685
Validation loss decreased (0.518103 --> 0.517870).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 24.744372844696045
Epoch: 26, Steps: 92 | Train Loss: 0.3937390 Vali Loss: 0.5170031 Test Loss: 0.6389823
Validation loss decreased (0.517870 --> 0.517003).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 24.587209224700928
Epoch: 27, Steps: 92 | Train Loss: 0.3932607 Vali Loss: 0.5165337 Test Loss: 0.6383869
Validation loss decreased (0.517003 --> 0.516534).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 26.186788082122803
Epoch: 28, Steps: 92 | Train Loss: 0.3928246 Vali Loss: 0.5159862 Test Loss: 0.6378256
Validation loss decreased (0.516534 --> 0.515986).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 25.94347310066223
Epoch: 29, Steps: 92 | Train Loss: 0.3925134 Vali Loss: 0.5150177 Test Loss: 0.6373447
Validation loss decreased (0.515986 --> 0.515018).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 25.487688064575195
Epoch: 30, Steps: 92 | Train Loss: 0.3921142 Vali Loss: 0.5152431 Test Loss: 0.6368816
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 25.408782958984375
Epoch: 31, Steps: 92 | Train Loss: 0.3918036 Vali Loss: 0.5147515 Test Loss: 0.6365404
Validation loss decreased (0.515018 --> 0.514751).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 25.692490100860596
Epoch: 32, Steps: 92 | Train Loss: 0.3915770 Vali Loss: 0.5142732 Test Loss: 0.6361712
Validation loss decreased (0.514751 --> 0.514273).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 25.807806491851807
Epoch: 33, Steps: 92 | Train Loss: 0.3911498 Vali Loss: 0.5142217 Test Loss: 0.6357854
Validation loss decreased (0.514273 --> 0.514222).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 25.59190034866333
Epoch: 34, Steps: 92 | Train Loss: 0.3910267 Vali Loss: 0.5136941 Test Loss: 0.6354589
Validation loss decreased (0.514222 --> 0.513694).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 25.808523178100586
Epoch: 35, Steps: 92 | Train Loss: 0.3907504 Vali Loss: 0.5134347 Test Loss: 0.6351912
Validation loss decreased (0.513694 --> 0.513435).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 24.784536838531494
Epoch: 36, Steps: 92 | Train Loss: 0.3905629 Vali Loss: 0.5130575 Test Loss: 0.6349905
Validation loss decreased (0.513435 --> 0.513058).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 24.623501300811768
Epoch: 37, Steps: 92 | Train Loss: 0.3902018 Vali Loss: 0.5133052 Test Loss: 0.6347215
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 25.061020374298096
Epoch: 38, Steps: 92 | Train Loss: 0.3900988 Vali Loss: 0.5127733 Test Loss: 0.6345205
Validation loss decreased (0.513058 --> 0.512773).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 24.779909372329712
Epoch: 39, Steps: 92 | Train Loss: 0.3899654 Vali Loss: 0.5123319 Test Loss: 0.6343002
Validation loss decreased (0.512773 --> 0.512332).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 24.69008207321167
Epoch: 40, Steps: 92 | Train Loss: 0.3898311 Vali Loss: 0.5125339 Test Loss: 0.6340816
EarlyStopping counter: 1 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 25.578216075897217
Epoch: 41, Steps: 92 | Train Loss: 0.3897256 Vali Loss: 0.5118498 Test Loss: 0.6339040
Validation loss decreased (0.512332 --> 0.511850).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 24.682900428771973
Epoch: 42, Steps: 92 | Train Loss: 0.3895509 Vali Loss: 0.5116197 Test Loss: 0.6337604
Validation loss decreased (0.511850 --> 0.511620).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 25.870267391204834
Epoch: 43, Steps: 92 | Train Loss: 0.3894714 Vali Loss: 0.5110260 Test Loss: 0.6336167
Validation loss decreased (0.511620 --> 0.511026).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 24.84213662147522
Epoch: 44, Steps: 92 | Train Loss: 0.3893785 Vali Loss: 0.5115513 Test Loss: 0.6334829
EarlyStopping counter: 1 out of 10
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 25.28933620452881
Epoch: 45, Steps: 92 | Train Loss: 0.3891221 Vali Loss: 0.5114096 Test Loss: 0.6333606
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 25.593433141708374
Epoch: 46, Steps: 92 | Train Loss: 0.3891645 Vali Loss: 0.5116796 Test Loss: 0.6332660
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 25.459065437316895
Epoch: 47, Steps: 92 | Train Loss: 0.3889920 Vali Loss: 0.5116657 Test Loss: 0.6331382
EarlyStopping counter: 4 out of 10
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 25.503801107406616
Epoch: 48, Steps: 92 | Train Loss: 0.3889980 Vali Loss: 0.5111748 Test Loss: 0.6330364
EarlyStopping counter: 5 out of 10
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 24.445501565933228
Epoch: 49, Steps: 92 | Train Loss: 0.3887112 Vali Loss: 0.5109496 Test Loss: 0.6329496
Validation loss decreased (0.511026 --> 0.510950).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 25.40243411064148
Epoch: 50, Steps: 92 | Train Loss: 0.3886564 Vali Loss: 0.5109251 Test Loss: 0.6328357
Validation loss decreased (0.510950 --> 0.510925).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 26.708377599716187
Epoch: 51, Steps: 92 | Train Loss: 0.3885835 Vali Loss: 0.5109591 Test Loss: 0.6327640
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 28.000190496444702
Epoch: 52, Steps: 92 | Train Loss: 0.3886159 Vali Loss: 0.5104985 Test Loss: 0.6326966
Validation loss decreased (0.510925 --> 0.510498).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 26.94002103805542
Epoch: 53, Steps: 92 | Train Loss: 0.3885245 Vali Loss: 0.5109063 Test Loss: 0.6326178
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 27.078426837921143
Epoch: 54, Steps: 92 | Train Loss: 0.3884756 Vali Loss: 0.5109233 Test Loss: 0.6325885
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 26.795029401779175
Epoch: 55, Steps: 92 | Train Loss: 0.3885823 Vali Loss: 0.5107328 Test Loss: 0.6325107
EarlyStopping counter: 3 out of 10
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 27.66295886039734
Epoch: 56, Steps: 92 | Train Loss: 0.3884147 Vali Loss: 0.5103728 Test Loss: 0.6324624
Validation loss decreased (0.510498 --> 0.510373).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 26.87584161758423
Epoch: 57, Steps: 92 | Train Loss: 0.3883223 Vali Loss: 0.5106328 Test Loss: 0.6324126
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 26.960349082946777
Epoch: 58, Steps: 92 | Train Loss: 0.3882894 Vali Loss: 0.5104155 Test Loss: 0.6323631
EarlyStopping counter: 2 out of 10
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 27.955074310302734
Epoch: 59, Steps: 92 | Train Loss: 0.3882280 Vali Loss: 0.5105539 Test Loss: 0.6323159
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 27.938934326171875
Epoch: 60, Steps: 92 | Train Loss: 0.3882969 Vali Loss: 0.5104752 Test Loss: 0.6322777
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 26.759857654571533
Epoch: 61, Steps: 92 | Train Loss: 0.3881900 Vali Loss: 0.5105824 Test Loss: 0.6322506
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 27.24695897102356
Epoch: 62, Steps: 92 | Train Loss: 0.3882206 Vali Loss: 0.5097700 Test Loss: 0.6322119
Validation loss decreased (0.510373 --> 0.509770).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 27.288508892059326
Epoch: 63, Steps: 92 | Train Loss: 0.3882182 Vali Loss: 0.5099859 Test Loss: 0.6321663
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 26.79473376274109
Epoch: 64, Steps: 92 | Train Loss: 0.3880611 Vali Loss: 0.5100814 Test Loss: 0.6321315
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 26.87355351448059
Epoch: 65, Steps: 92 | Train Loss: 0.3881344 Vali Loss: 0.5101354 Test Loss: 0.6321139
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 26.376948595046997
Epoch: 66, Steps: 92 | Train Loss: 0.3879473 Vali Loss: 0.5103772 Test Loss: 0.6320920
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 26.466856718063354
Epoch: 67, Steps: 92 | Train Loss: 0.3878753 Vali Loss: 0.5103797 Test Loss: 0.6320538
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 26.51740789413452
Epoch: 68, Steps: 92 | Train Loss: 0.3880539 Vali Loss: 0.5098726 Test Loss: 0.6320245
EarlyStopping counter: 6 out of 10
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 26.691487789154053
Epoch: 69, Steps: 92 | Train Loss: 0.3879602 Vali Loss: 0.5099373 Test Loss: 0.6320111
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 26.262796878814697
Epoch: 70, Steps: 92 | Train Loss: 0.3879316 Vali Loss: 0.5104700 Test Loss: 0.6319880
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 26.503060579299927
Epoch: 71, Steps: 92 | Train Loss: 0.3878492 Vali Loss: 0.5098921 Test Loss: 0.6319793
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 28.410463094711304
Epoch: 72, Steps: 92 | Train Loss: 0.3878224 Vali Loss: 0.5097318 Test Loss: 0.6319557
Validation loss decreased (0.509770 --> 0.509732).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 25.880661010742188
Epoch: 73, Steps: 92 | Train Loss: 0.3878044 Vali Loss: 0.5099193 Test Loss: 0.6319472
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 25.934645891189575
Epoch: 74, Steps: 92 | Train Loss: 0.3878620 Vali Loss: 0.5101579 Test Loss: 0.6319379
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 26.43538498878479
Epoch: 75, Steps: 92 | Train Loss: 0.3878204 Vali Loss: 0.5097738 Test Loss: 0.6319225
EarlyStopping counter: 3 out of 10
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 25.66399574279785
Epoch: 76, Steps: 92 | Train Loss: 0.3877806 Vali Loss: 0.5102503 Test Loss: 0.6319107
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 26.048027992248535
Epoch: 77, Steps: 92 | Train Loss: 0.3878073 Vali Loss: 0.5099425 Test Loss: 0.6319053
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 26.478818655014038
Epoch: 78, Steps: 92 | Train Loss: 0.3878660 Vali Loss: 0.5095858 Test Loss: 0.6318869
Validation loss decreased (0.509732 --> 0.509586).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 26.13764977455139
Epoch: 79, Steps: 92 | Train Loss: 0.3877028 Vali Loss: 0.5098460 Test Loss: 0.6318781
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 25.51092219352722
Epoch: 80, Steps: 92 | Train Loss: 0.3878069 Vali Loss: 0.5100228 Test Loss: 0.6318676
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 27.617521286010742
Epoch: 81, Steps: 92 | Train Loss: 0.3878760 Vali Loss: 0.5105345 Test Loss: 0.6318625
EarlyStopping counter: 3 out of 10
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 26.275003910064697
Epoch: 82, Steps: 92 | Train Loss: 0.3877840 Vali Loss: 0.5095351 Test Loss: 0.6318453
Validation loss decreased (0.509586 --> 0.509535).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 26.451727390289307
Epoch: 83, Steps: 92 | Train Loss: 0.3877258 Vali Loss: 0.5096811 Test Loss: 0.6318374
EarlyStopping counter: 1 out of 10
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 25.968878984451294
Epoch: 84, Steps: 92 | Train Loss: 0.3876749 Vali Loss: 0.5099428 Test Loss: 0.6318368
EarlyStopping counter: 2 out of 10
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 26.470178365707397
Epoch: 85, Steps: 92 | Train Loss: 0.3877613 Vali Loss: 0.5098155 Test Loss: 0.6318265
EarlyStopping counter: 3 out of 10
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 26.034448385238647
Epoch: 86, Steps: 92 | Train Loss: 0.3876882 Vali Loss: 0.5101236 Test Loss: 0.6318141
EarlyStopping counter: 4 out of 10
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 26.04973864555359
Epoch: 87, Steps: 92 | Train Loss: 0.3877654 Vali Loss: 0.5095987 Test Loss: 0.6318116
EarlyStopping counter: 5 out of 10
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 26.08974289894104
Epoch: 88, Steps: 92 | Train Loss: 0.3877314 Vali Loss: 0.5099269 Test Loss: 0.6318075
EarlyStopping counter: 6 out of 10
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 26.14803409576416
Epoch: 89, Steps: 92 | Train Loss: 0.3876863 Vali Loss: 0.5098927 Test Loss: 0.6318052
EarlyStopping counter: 7 out of 10
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 26.118656158447266
Epoch: 90, Steps: 92 | Train Loss: 0.3877197 Vali Loss: 0.5095985 Test Loss: 0.6318015
EarlyStopping counter: 8 out of 10
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 25.85766863822937
Epoch: 91, Steps: 92 | Train Loss: 0.3876260 Vali Loss: 0.5102388 Test Loss: 0.6317923
EarlyStopping counter: 9 out of 10
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 25.50870966911316
Epoch: 92, Steps: 92 | Train Loss: 0.3877307 Vali Loss: 0.5098368 Test Loss: 0.6317919
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j336_H8_FITS_custom_ftM_sl90_ll48_pl336_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3173
mse:0.6299620866775513, mae:0.37804117798805237, rse:0.6523136496543884, corr:[0.26328775 0.27795985 0.2754921  0.27499217 0.27581912 0.27702183
 0.28032342 0.2803076  0.2815223  0.28043234 0.28032672 0.27973053
 0.27865833 0.2784824  0.27744442 0.278488   0.27841902 0.27965274
 0.28118873 0.28224182 0.28418165 0.28467265 0.28427798 0.283957
 0.2888629  0.29266235 0.2923254  0.29059714 0.29064658 0.29238436
 0.29345205 0.29492515 0.29476982 0.2945076  0.29408765 0.29360685
 0.29297704 0.29221267 0.29232094 0.29239145 0.29280305 0.29352182
 0.29433253 0.29478762 0.2951173  0.29504842 0.29429653 0.2934718
 0.29416656 0.29276848 0.29108378 0.29013264 0.28869915 0.28475338
 0.2826236  0.28334364 0.28382817 0.28366303 0.2821513  0.28157935
 0.2801531  0.27951825 0.27932784 0.27902126 0.27924916 0.27939102
 0.28000018 0.28038245 0.2805383  0.28101525 0.28083554 0.2798185
 0.2778343  0.2763595  0.2757022  0.27649456 0.27797872 0.2778226
 0.2785632  0.28022707 0.2810026  0.28069815 0.2792553  0.278465
 0.2769064  0.27644417 0.27575788 0.2754637  0.27538142 0.27547434
 0.2760537  0.27645746 0.27712587 0.2777135  0.27824953 0.27825215
 0.27778614 0.2777114  0.27741405 0.27745646 0.27767104 0.27794677
 0.2788513  0.27974942 0.27950352 0.2789929  0.27828097 0.2776716
 0.2767652  0.27650777 0.27597463 0.2760436  0.27617308 0.27638426
 0.27701002 0.2774901  0.27815178 0.27850598 0.2792962  0.27919888
 0.2792194  0.27930245 0.2792289  0.27927276 0.27910024 0.2790865
 0.2790474  0.27883768 0.27801058 0.27733374 0.2768946  0.2762348
 0.2759041  0.27585068 0.27591982 0.27632508 0.276741   0.27735668
 0.2782923  0.27883962 0.27965736 0.28004062 0.28037438 0.27914456
 0.2782615  0.27791023 0.27782923 0.27786404 0.27793887 0.2782862
 0.27850524 0.27844486 0.27775726 0.27692148 0.2763681  0.27568245
 0.2755241  0.2755445  0.2760478  0.2765348  0.27733058 0.27824345
 0.2796851  0.2807158  0.28217715 0.2824853  0.28197604 0.28045508
 0.2793541  0.27808282 0.2760376  0.27480406 0.2754708  0.27749446
 0.2797675  0.28111497 0.28088576 0.28074914 0.28069603 0.2804879
 0.2803199  0.28011417 0.2803926  0.2806075  0.2811616  0.28190932
 0.28304222 0.28377333 0.2848357  0.2847931  0.28417107 0.28399795
 0.2866512  0.28944165 0.28874332 0.2872215  0.28753307 0.2887347
 0.29001084 0.29143924 0.29175985 0.2920351  0.2919637  0.2920495
 0.2915617  0.29125676 0.29123747 0.2912699  0.2916031  0.2920083
 0.2926121  0.29267842 0.29282415 0.29252312 0.291718   0.2906279
 0.29000267 0.2879936  0.28601903 0.2852388  0.28408158 0.28049898
 0.27867037 0.27963614 0.2805686  0.280709   0.279857   0.27951157
 0.27831635 0.27800494 0.2776736  0.27735287 0.27752453 0.27769566
 0.27816436 0.2782966  0.2785076  0.2786115  0.2782029  0.27695468
 0.27493408 0.27361596 0.2729028  0.27370268 0.27537692 0.27569994
 0.2769515  0.27903536 0.28011054 0.28010312 0.279075   0.2783517
 0.27712032 0.27674535 0.2759357  0.27556118 0.27529272 0.27532935
 0.27565765 0.27591518 0.2763951  0.27658275 0.27690348 0.27660865
 0.27611467 0.27607688 0.27585465 0.27614832 0.27641916 0.27680945
 0.27766412 0.27866864 0.27866697 0.2783337  0.27802745 0.2774776
 0.2769204  0.27663997 0.27604032 0.27610743 0.2760412  0.27621207
 0.27663437 0.27688834 0.27720353 0.27718052 0.2776857  0.27737594
 0.27747086 0.27754954 0.27771246 0.27793583 0.27787334 0.27782753
 0.27761996 0.27747366 0.27683446 0.27661273 0.27655622 0.27605903
 0.27623865 0.27593264 0.27604035 0.27637678 0.276679   0.27718693
 0.2778006  0.27787054 0.27825233 0.2783343  0.27799064 0.27685598
 0.2759749  0.27581692 0.2761664  0.2758931  0.276056   0.2760222
 0.27627134 0.2767325  0.27667895 0.2770934  0.27684233 0.27719906
 0.2770209  0.2772035  0.27755615 0.2777625  0.2781844  0.27833918
 0.27759612 0.27724597 0.2768684  0.27558714 0.2758629  0.27421406]
