Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='traffic.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=862, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Traffic_90_j192_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=10, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Traffic_90_j192_H8_FITS_custom_ftM_sl90_ll48_pl192_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11999
val 1565
test 3317
Model(
  (freq_upsampler): Linear(in_features=42, out_features=131, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  607068672.0
params:  5633.0
Trainable parameters:  5633
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 16.692840576171875
Epoch: 1, Steps: 93 | Train Loss: 1.1619066 Vali Loss: 1.0253731 Test Loss: 1.2188592
Validation loss decreased (inf --> 1.025373).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 18.200701236724854
Epoch: 2, Steps: 93 | Train Loss: 0.6716231 Vali Loss: 0.7359655 Test Loss: 0.8776081
Validation loss decreased (1.025373 --> 0.735966).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 17.374878406524658
Epoch: 3, Steps: 93 | Train Loss: 0.5209836 Vali Loss: 0.6296993 Test Loss: 0.7543176
Validation loss decreased (0.735966 --> 0.629699).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 20.943479299545288
Epoch: 4, Steps: 93 | Train Loss: 0.4613365 Vali Loss: 0.5832234 Test Loss: 0.7014053
Validation loss decreased (0.629699 --> 0.583223).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 20.464831352233887
Epoch: 5, Steps: 93 | Train Loss: 0.4332613 Vali Loss: 0.5597873 Test Loss: 0.6744330
Validation loss decreased (0.583223 --> 0.559787).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 20.59245204925537
Epoch: 6, Steps: 93 | Train Loss: 0.4178845 Vali Loss: 0.5457138 Test Loss: 0.6586872
Validation loss decreased (0.559787 --> 0.545714).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 20.832912921905518
Epoch: 7, Steps: 93 | Train Loss: 0.4083761 Vali Loss: 0.5361918 Test Loss: 0.6485698
Validation loss decreased (0.545714 --> 0.536192).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 19.42947030067444
Epoch: 8, Steps: 93 | Train Loss: 0.4020540 Vali Loss: 0.5301716 Test Loss: 0.6416798
Validation loss decreased (0.536192 --> 0.530172).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 20.848724365234375
Epoch: 9, Steps: 93 | Train Loss: 0.3977971 Vali Loss: 0.5255151 Test Loss: 0.6368440
Validation loss decreased (0.530172 --> 0.525515).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 20.08110237121582
Epoch: 10, Steps: 93 | Train Loss: 0.3945659 Vali Loss: 0.5227311 Test Loss: 0.6333125
Validation loss decreased (0.525515 --> 0.522731).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 19.9439537525177
Epoch: 11, Steps: 93 | Train Loss: 0.3922664 Vali Loss: 0.5193422 Test Loss: 0.6307214
Validation loss decreased (0.522731 --> 0.519342).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 20.205930709838867
Epoch: 12, Steps: 93 | Train Loss: 0.3907048 Vali Loss: 0.5180545 Test Loss: 0.6287066
Validation loss decreased (0.519342 --> 0.518054).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 20.47848916053772
Epoch: 13, Steps: 93 | Train Loss: 0.3893389 Vali Loss: 0.5161328 Test Loss: 0.6271347
Validation loss decreased (0.518054 --> 0.516133).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 20.807841300964355
Epoch: 14, Steps: 93 | Train Loss: 0.3882466 Vali Loss: 0.5153828 Test Loss: 0.6260208
Validation loss decreased (0.516133 --> 0.515383).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 20.433030605316162
Epoch: 15, Steps: 93 | Train Loss: 0.3874318 Vali Loss: 0.5141932 Test Loss: 0.6250321
Validation loss decreased (0.515383 --> 0.514193).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 20.693861961364746
Epoch: 16, Steps: 93 | Train Loss: 0.3867386 Vali Loss: 0.5129491 Test Loss: 0.6241775
Validation loss decreased (0.514193 --> 0.512949).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 20.890764713287354
Epoch: 17, Steps: 93 | Train Loss: 0.3862258 Vali Loss: 0.5129816 Test Loss: 0.6235794
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 20.925323247909546
Epoch: 18, Steps: 93 | Train Loss: 0.3856444 Vali Loss: 0.5120761 Test Loss: 0.6230599
Validation loss decreased (0.512949 --> 0.512076).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 20.41275453567505
Epoch: 19, Steps: 93 | Train Loss: 0.3851897 Vali Loss: 0.5118908 Test Loss: 0.6226003
Validation loss decreased (0.512076 --> 0.511891).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 20.265677452087402
Epoch: 20, Steps: 93 | Train Loss: 0.3850823 Vali Loss: 0.5108765 Test Loss: 0.6222535
Validation loss decreased (0.511891 --> 0.510877).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 20.243016242980957
Epoch: 21, Steps: 93 | Train Loss: 0.3847590 Vali Loss: 0.5107151 Test Loss: 0.6218897
Validation loss decreased (0.510877 --> 0.510715).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 20.8203067779541
Epoch: 22, Steps: 93 | Train Loss: 0.3844787 Vali Loss: 0.5106084 Test Loss: 0.6216589
Validation loss decreased (0.510715 --> 0.510608).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 20.43122887611389
Epoch: 23, Steps: 93 | Train Loss: 0.3842352 Vali Loss: 0.5105533 Test Loss: 0.6214815
Validation loss decreased (0.510608 --> 0.510553).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 21.295698881149292
Epoch: 24, Steps: 93 | Train Loss: 0.3841131 Vali Loss: 0.5106657 Test Loss: 0.6212485
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 20.224624156951904
Epoch: 25, Steps: 93 | Train Loss: 0.3838942 Vali Loss: 0.5099131 Test Loss: 0.6211121
Validation loss decreased (0.510553 --> 0.509913).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 20.267850160598755
Epoch: 26, Steps: 93 | Train Loss: 0.3837617 Vali Loss: 0.5102929 Test Loss: 0.6209787
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 19.78650951385498
Epoch: 27, Steps: 93 | Train Loss: 0.3836318 Vali Loss: 0.5103019 Test Loss: 0.6208497
EarlyStopping counter: 2 out of 10
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 19.791027307510376
Epoch: 28, Steps: 93 | Train Loss: 0.3836194 Vali Loss: 0.5102592 Test Loss: 0.6208020
EarlyStopping counter: 3 out of 10
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 19.66589856147766
Epoch: 29, Steps: 93 | Train Loss: 0.3834345 Vali Loss: 0.5092450 Test Loss: 0.6206955
Validation loss decreased (0.509913 --> 0.509245).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 19.772178888320923
Epoch: 30, Steps: 93 | Train Loss: 0.3833537 Vali Loss: 0.5097553 Test Loss: 0.6206267
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 20.428019523620605
Epoch: 31, Steps: 93 | Train Loss: 0.3834263 Vali Loss: 0.5091497 Test Loss: 0.6205711
Validation loss decreased (0.509245 --> 0.509150).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 19.874847650527954
Epoch: 32, Steps: 93 | Train Loss: 0.3832328 Vali Loss: 0.5097205 Test Loss: 0.6204849
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 19.62679672241211
Epoch: 33, Steps: 93 | Train Loss: 0.3831258 Vali Loss: 0.5083658 Test Loss: 0.6205007
Validation loss decreased (0.509150 --> 0.508366).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 19.874427795410156
Epoch: 34, Steps: 93 | Train Loss: 0.3832086 Vali Loss: 0.5099011 Test Loss: 0.6204030
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 20.21406364440918
Epoch: 35, Steps: 93 | Train Loss: 0.3831124 Vali Loss: 0.5092448 Test Loss: 0.6203998
EarlyStopping counter: 2 out of 10
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 19.836853981018066
Epoch: 36, Steps: 93 | Train Loss: 0.3830492 Vali Loss: 0.5091674 Test Loss: 0.6203754
EarlyStopping counter: 3 out of 10
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 19.495755195617676
Epoch: 37, Steps: 93 | Train Loss: 0.3831266 Vali Loss: 0.5095088 Test Loss: 0.6203505
EarlyStopping counter: 4 out of 10
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 19.597075700759888
Epoch: 38, Steps: 93 | Train Loss: 0.3830940 Vali Loss: 0.5088666 Test Loss: 0.6203436
EarlyStopping counter: 5 out of 10
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 20.95659375190735
Epoch: 39, Steps: 93 | Train Loss: 0.3829593 Vali Loss: 0.5091274 Test Loss: 0.6203024
EarlyStopping counter: 6 out of 10
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 19.556846380233765
Epoch: 40, Steps: 93 | Train Loss: 0.3830822 Vali Loss: 0.5088828 Test Loss: 0.6202822
EarlyStopping counter: 7 out of 10
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 16.581775188446045
Epoch: 41, Steps: 93 | Train Loss: 0.3829796 Vali Loss: 0.5090553 Test Loss: 0.6202788
EarlyStopping counter: 8 out of 10
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 21.028777837753296
Epoch: 42, Steps: 93 | Train Loss: 0.3829443 Vali Loss: 0.5090744 Test Loss: 0.6202723
EarlyStopping counter: 9 out of 10
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 21.240864515304565
Epoch: 43, Steps: 93 | Train Loss: 0.3830294 Vali Loss: 0.5094505 Test Loss: 0.6202568
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : Traffic_90_j192_H8_FITS_custom_ftM_sl90_ll48_pl192_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3317
mse:0.6203141808509827, mae:0.374651700258255, rse:0.6500322222709656, corr:[0.26731083 0.28362522 0.28398976 0.28180107 0.28253376 0.28298527
 0.2859596  0.28637275 0.2858615  0.28596267 0.28485227 0.28525078
 0.28329086 0.2841348  0.28316405 0.28435147 0.2848655  0.2858906
 0.28772852 0.28852105 0.29049274 0.29049143 0.29067257 0.2905836
 0.29438323 0.29847944 0.29744643 0.2964206  0.2964441  0.2974532
 0.29854387 0.29996067 0.30025893 0.29996768 0.29985735 0.29923618
 0.2986908  0.2983115  0.29811177 0.29868624 0.29891345 0.2993395
 0.30006593 0.30026636 0.30063817 0.300189   0.29979247 0.2986358
 0.29865763 0.29742476 0.2957389  0.2948984  0.29346806 0.28862315
 0.28641942 0.2873834  0.28799808 0.28803465 0.2868614  0.28634125
 0.28509617 0.2843787  0.28423062 0.2841726  0.28433675 0.28456718
 0.2848796  0.28501552 0.28499988 0.28494138 0.28501713 0.28363195
 0.28159207 0.28008464 0.27938303 0.28039584 0.28196612 0.2817068
 0.28241092 0.28413787 0.28507498 0.2848509  0.28362814 0.28288096
 0.28167245 0.28089258 0.28010762 0.27983037 0.27947557 0.2798129
 0.2799877  0.2802965  0.28068435 0.2810839  0.2816533  0.28138945
 0.28108576 0.2808247  0.28073254 0.28088713 0.28102487 0.28141648
 0.2825095  0.28351387 0.28323337 0.28299412 0.28223875 0.28181165
 0.28102067 0.28048795 0.280254   0.27996787 0.28008273 0.28058857
 0.28069517 0.28146127 0.2818337  0.28215507 0.2827032  0.28268635
 0.2826988  0.28252837 0.28269207 0.28271642 0.28272712 0.28260648
 0.28238952 0.28234783 0.28155586 0.2813596  0.28081658 0.28061786
 0.28021783 0.28014016 0.28038612 0.2805031  0.28107926 0.28157356
 0.2823136  0.28281212 0.28319296 0.28371713 0.28376135 0.2828303
 0.28171262 0.28149766 0.28156912 0.28132412 0.28159726 0.2815447
 0.2818904  0.28179374 0.28122056 0.28115863 0.28069386 0.2803366
 0.28015837 0.28047222 0.28079906 0.28136784 0.28211367 0.2827226
 0.28408778 0.2847209  0.28617743 0.28623605 0.28575024 0.284476
 0.28285214 0.28285092 0.28038773 0.27896544 0.27966163 0.28101453
 0.28405032 0.28527796 0.28607026 0.2863593  0.28711444 0.28716987
 0.28708524 0.28704295 0.28656876 0.28688505 0.28627422 0.28644907
 0.28536558 0.28558823 0.2845252  0.284449   0.28156638 0.282525  ]
