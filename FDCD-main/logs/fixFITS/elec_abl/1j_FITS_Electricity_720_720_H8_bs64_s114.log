Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=258, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=258, out_features=516, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5469963264.0
params:  133644.0
Trainable parameters:  133644
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7172241
	speed: 0.7851s/iter; left time: 10285.7949s
Epoch: 1 cost time: 103.24387168884277
Epoch: 1, Steps: 132 | Train Loss: 0.8742875 Vali Loss: 0.5199524 Test Loss: 0.6248928
Validation loss decreased (inf --> 0.519952).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4850123
	speed: 1.7736s/iter; left time: 23002.1525s
Epoch: 2 cost time: 109.77499556541443
Epoch: 2, Steps: 132 | Train Loss: 0.5296564 Vali Loss: 0.3526422 Test Loss: 0.4301560
Validation loss decreased (0.519952 --> 0.352642).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3494590
	speed: 1.6683s/iter; left time: 21415.7318s
Epoch: 3 cost time: 96.48327016830444
Epoch: 3, Steps: 132 | Train Loss: 0.3817529 Vali Loss: 0.2635232 Test Loss: 0.3238501
Validation loss decreased (0.352642 --> 0.263523).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3017209
	speed: 1.5957s/iter; left time: 20273.2532s
Epoch: 4 cost time: 93.41784763336182
Epoch: 4, Steps: 132 | Train Loss: 0.3019923 Vali Loss: 0.2174571 Test Loss: 0.2667438
Validation loss decreased (0.263523 --> 0.217457).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2532004
	speed: 1.7239s/iter; left time: 21675.1555s
Epoch: 5 cost time: 112.77007842063904
Epoch: 5, Steps: 132 | Train Loss: 0.2602467 Vali Loss: 0.1949395 Test Loss: 0.2371086
Validation loss decreased (0.217457 --> 0.194940).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2487303
	speed: 2.6178s/iter; left time: 32567.7502s
Epoch: 6 cost time: 168.54439544677734
Epoch: 6, Steps: 132 | Train Loss: 0.2394642 Vali Loss: 0.1845832 Test Loss: 0.2223287
Validation loss decreased (0.194940 --> 0.184583).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2171396
	speed: 2.8866s/iter; left time: 35531.2149s
Epoch: 7 cost time: 208.5324831008911
Epoch: 7, Steps: 132 | Train Loss: 0.2294503 Vali Loss: 0.1800127 Test Loss: 0.2150906
Validation loss decreased (0.184583 --> 0.180013).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2313829
	speed: 3.2797s/iter; left time: 39937.3798s
Epoch: 8 cost time: 157.20364046096802
Epoch: 8, Steps: 132 | Train Loss: 0.2246913 Vali Loss: 0.1784351 Test Loss: 0.2116423
Validation loss decreased (0.180013 --> 0.178435).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2237817
	speed: 2.6849s/iter; left time: 32339.6765s
Epoch: 9 cost time: 201.5820598602295
Epoch: 9, Steps: 132 | Train Loss: 0.2225334 Vali Loss: 0.1777382 Test Loss: 0.2099228
Validation loss decreased (0.178435 --> 0.177738).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2246087
	speed: 4.0876s/iter; left time: 48695.3031s
Epoch: 10 cost time: 243.43448567390442
Epoch: 10, Steps: 132 | Train Loss: 0.2213911 Vali Loss: 0.1772819 Test Loss: 0.2091040
Validation loss decreased (0.177738 --> 0.177282).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2239826
	speed: 4.0267s/iter; left time: 47438.5119s
Epoch: 11 cost time: 258.5229241847992
Epoch: 11, Steps: 132 | Train Loss: 0.2208930 Vali Loss: 0.1771668 Test Loss: 0.2085588
Validation loss decreased (0.177282 --> 0.177167).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2069366
	speed: 4.4515s/iter; left time: 51855.3084s
Epoch: 12 cost time: 271.8762674331665
Epoch: 12, Steps: 132 | Train Loss: 0.2205978 Vali Loss: 0.1773190 Test Loss: 0.2083508
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2220350
	speed: 4.2160s/iter; left time: 48555.6600s
Epoch: 13 cost time: 263.66913866996765
Epoch: 13, Steps: 132 | Train Loss: 0.2203437 Vali Loss: 0.1769768 Test Loss: 0.2081918
Validation loss decreased (0.177167 --> 0.176977).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2137191
	speed: 4.4038s/iter; left time: 50137.6500s
Epoch: 14 cost time: 261.2914640903473
Epoch: 14, Steps: 132 | Train Loss: 0.2201360 Vali Loss: 0.1771416 Test Loss: 0.2081289
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2269254
	speed: 4.7632s/iter; left time: 53600.1184s
Epoch: 15 cost time: 256.4112722873688
Epoch: 15, Steps: 132 | Train Loss: 0.2201268 Vali Loss: 0.1769869 Test Loss: 0.2080201
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2166365
	speed: 3.2451s/iter; left time: 36088.9701s
Epoch: 16 cost time: 209.93266916275024
Epoch: 16, Steps: 132 | Train Loss: 0.2200023 Vali Loss: 0.1773255 Test Loss: 0.2079130
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2179082
	speed: 3.4643s/iter; left time: 38068.7451s
Epoch: 17 cost time: 190.74507570266724
Epoch: 17, Steps: 132 | Train Loss: 0.2198881 Vali Loss: 0.1770936 Test Loss: 0.2078517
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2216657
	speed: 2.6642s/iter; left time: 28925.1860s
Epoch: 18 cost time: 163.96018195152283
Epoch: 18, Steps: 132 | Train Loss: 0.2199244 Vali Loss: 0.1771040 Test Loss: 0.2078208
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2109982
	speed: 3.3996s/iter; left time: 36461.1506s
Epoch: 19 cost time: 200.14377737045288
Epoch: 19, Steps: 132 | Train Loss: 0.2198329 Vali Loss: 0.1769421 Test Loss: 0.2078323
Validation loss decreased (0.176977 --> 0.176942).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2163908
	speed: 3.4891s/iter; left time: 36960.3842s
Epoch: 20 cost time: 225.4395468235016
Epoch: 20, Steps: 132 | Train Loss: 0.2197196 Vali Loss: 0.1770416 Test Loss: 0.2078191
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2135296
	speed: 3.7435s/iter; left time: 39160.5233s
Epoch: 21 cost time: 228.47608828544617
Epoch: 21, Steps: 132 | Train Loss: 0.2197542 Vali Loss: 0.1769227 Test Loss: 0.2077864
Validation loss decreased (0.176942 --> 0.176923).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2102437
	speed: 2.7905s/iter; left time: 28823.1350s
Epoch: 22 cost time: 134.87395763397217
Epoch: 22, Steps: 132 | Train Loss: 0.2196100 Vali Loss: 0.1771021 Test Loss: 0.2077335
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2090869
	speed: 3.0843s/iter; left time: 31450.6907s
Epoch: 23 cost time: 154.3625078201294
Epoch: 23, Steps: 132 | Train Loss: 0.2197094 Vali Loss: 0.1767951 Test Loss: 0.2076907
Validation loss decreased (0.176923 --> 0.176795).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2246884
	speed: 2.1673s/iter; left time: 21814.3273s
Epoch: 24 cost time: 138.32249522209167
Epoch: 24, Steps: 132 | Train Loss: 0.2196046 Vali Loss: 0.1768739 Test Loss: 0.2077113
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2268825
	speed: 2.3727s/iter; left time: 23567.8972s
Epoch: 25 cost time: 141.67872738838196
Epoch: 25, Steps: 132 | Train Loss: 0.2196201 Vali Loss: 0.1769700 Test Loss: 0.2076641
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2152254
	speed: 2.1087s/iter; left time: 20666.9059s
Epoch: 26 cost time: 130.70565557479858
Epoch: 26, Steps: 132 | Train Loss: 0.2195462 Vali Loss: 0.1768222 Test Loss: 0.2076550
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2282707
	speed: 2.1118s/iter; left time: 20418.9818s
Epoch: 27 cost time: 117.85581231117249
Epoch: 27, Steps: 132 | Train Loss: 0.2194172 Vali Loss: 0.1769576 Test Loss: 0.2076378
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2108339
	speed: 2.2158s/iter; left time: 21132.0999s
Epoch: 28 cost time: 145.4378378391266
Epoch: 28, Steps: 132 | Train Loss: 0.2194585 Vali Loss: 0.1770575 Test Loss: 0.2076176
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2219720
	speed: 2.3641s/iter; left time: 22234.4812s
Epoch: 29 cost time: 121.85114336013794
Epoch: 29, Steps: 132 | Train Loss: 0.2195288 Vali Loss: 0.1768154 Test Loss: 0.2076055
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2202783
	speed: 1.9965s/iter; left time: 18513.2124s
Epoch: 30 cost time: 119.611483335495
Epoch: 30, Steps: 132 | Train Loss: 0.2194054 Vali Loss: 0.1768097 Test Loss: 0.2075664
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2202337
	speed: 2.0696s/iter; left time: 18917.9365s
Epoch: 31 cost time: 125.67847204208374
Epoch: 31, Steps: 132 | Train Loss: 0.2194366 Vali Loss: 0.1768663 Test Loss: 0.2076152
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2053862
	speed: 2.6806s/iter; left time: 24149.6664s
Epoch: 32 cost time: 185.15386700630188
Epoch: 32, Steps: 132 | Train Loss: 0.2193458 Vali Loss: 0.1772072 Test Loss: 0.2076059
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2266201
	speed: 3.1195s/iter; left time: 27692.0177s
Epoch: 33 cost time: 191.8501935005188
Epoch: 33, Steps: 132 | Train Loss: 0.2193198 Vali Loss: 0.1767946 Test Loss: 0.2075799
Validation loss decreased (0.176795 --> 0.176795).  Saving model ...
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2210426
	speed: 2.9248s/iter; left time: 25577.6654s
Epoch: 34 cost time: 184.76709842681885
Epoch: 34, Steps: 132 | Train Loss: 0.2193524 Vali Loss: 0.1768845 Test Loss: 0.2075415
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2267420
	speed: 2.7161s/iter; left time: 23393.7080s
Epoch: 35 cost time: 154.7496292591095
Epoch: 35, Steps: 132 | Train Loss: 0.2194021 Vali Loss: 0.1766327 Test Loss: 0.2075713
Validation loss decreased (0.176795 --> 0.176633).  Saving model ...
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2197462
	speed: 2.3043s/iter; left time: 19542.6826s
Epoch: 36 cost time: 137.67491626739502
Epoch: 36, Steps: 132 | Train Loss: 0.2193107 Vali Loss: 0.1769452 Test Loss: 0.2075367
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2077134
	speed: 2.3592s/iter; left time: 19697.0443s
Epoch: 37 cost time: 143.89286756515503
Epoch: 37, Steps: 132 | Train Loss: 0.2193804 Vali Loss: 0.1766453 Test Loss: 0.2075470
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2238128
	speed: 2.3565s/iter; left time: 19363.7080s
Epoch: 38 cost time: 141.69807887077332
Epoch: 38, Steps: 132 | Train Loss: 0.2192781 Vali Loss: 0.1766321 Test Loss: 0.2075168
Validation loss decreased (0.176633 --> 0.176632).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2054041
	speed: 2.2873s/iter; left time: 18493.0502s
Epoch: 39 cost time: 133.8930892944336
Epoch: 39, Steps: 132 | Train Loss: 0.2193243 Vali Loss: 0.1765759 Test Loss: 0.2075282
Validation loss decreased (0.176632 --> 0.176576).  Saving model ...
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2230296
	speed: 2.2397s/iter; left time: 17812.7070s
Epoch: 40 cost time: 137.96053051948547
Epoch: 40, Steps: 132 | Train Loss: 0.2192776 Vali Loss: 0.1767859 Test Loss: 0.2075173
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2152259
	speed: 2.2843s/iter; left time: 17865.6608s
Epoch: 41 cost time: 144.69353127479553
Epoch: 41, Steps: 132 | Train Loss: 0.2192553 Vali Loss: 0.1768548 Test Loss: 0.2075301
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2090665
	speed: 2.3142s/iter; left time: 17794.2451s
Epoch: 42 cost time: 141.769713640213
Epoch: 42, Steps: 132 | Train Loss: 0.2192765 Vali Loss: 0.1764341 Test Loss: 0.2075010
Validation loss decreased (0.176576 --> 0.176434).  Saving model ...
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2205849
	speed: 2.2712s/iter; left time: 17163.3812s
Epoch: 43 cost time: 141.60865020751953
Epoch: 43, Steps: 132 | Train Loss: 0.2191556 Vali Loss: 0.1763415 Test Loss: 0.2075017
Validation loss decreased (0.176434 --> 0.176342).  Saving model ...
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2072898
	speed: 2.3263s/iter; left time: 17272.4433s
Epoch: 44 cost time: 134.06817865371704
Epoch: 44, Steps: 132 | Train Loss: 0.2192146 Vali Loss: 0.1767748 Test Loss: 0.2075159
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2255693
	speed: 2.1900s/iter; left time: 15971.7185s
Epoch: 45 cost time: 136.46926426887512
Epoch: 45, Steps: 132 | Train Loss: 0.2193072 Vali Loss: 0.1762335 Test Loss: 0.2075201
Validation loss decreased (0.176342 --> 0.176234).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2130097
	speed: 2.2621s/iter; left time: 16199.0628s
Epoch: 46 cost time: 137.98827147483826
Epoch: 46, Steps: 132 | Train Loss: 0.2193031 Vali Loss: 0.1766208 Test Loss: 0.2074939
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2280185
	speed: 2.1963s/iter; left time: 15437.7277s
Epoch: 47 cost time: 131.41438555717468
Epoch: 47, Steps: 132 | Train Loss: 0.2192254 Vali Loss: 0.1764798 Test Loss: 0.2074892
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2128159
	speed: 2.2395s/iter; left time: 15445.5124s
Epoch: 48 cost time: 132.66513919830322
Epoch: 48, Steps: 132 | Train Loss: 0.2192096 Vali Loss: 0.1766195 Test Loss: 0.2074767
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2173372
	speed: 2.1582s/iter; left time: 14600.1864s
Epoch: 49 cost time: 130.59795308113098
Epoch: 49, Steps: 132 | Train Loss: 0.2192532 Vali Loss: 0.1769267 Test Loss: 0.2074900
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2109863
	speed: 2.2448s/iter; left time: 14889.8381s
Epoch: 50 cost time: 133.1653332710266
Epoch: 50, Steps: 132 | Train Loss: 0.2191522 Vali Loss: 0.1765250 Test Loss: 0.2074696
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2096276
	speed: 2.2994s/iter; left time: 14948.2640s
Epoch: 51 cost time: 146.38923931121826
Epoch: 51, Steps: 132 | Train Loss: 0.2191964 Vali Loss: 0.1767207 Test Loss: 0.2074750
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2148271
	speed: 2.8141s/iter; left time: 17922.7540s
Epoch: 52 cost time: 177.03655123710632
Epoch: 52, Steps: 132 | Train Loss: 0.2190771 Vali Loss: 0.1766223 Test Loss: 0.2074819
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2190737
	speed: 3.1207s/iter; left time: 19463.9970s
Epoch: 53 cost time: 192.13665580749512
Epoch: 53, Steps: 132 | Train Loss: 0.2191804 Vali Loss: 0.1765976 Test Loss: 0.2074741
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2116140
	speed: 2.2490s/iter; left time: 13729.9863s
Epoch: 54 cost time: 131.7474455833435
Epoch: 54, Steps: 132 | Train Loss: 0.2191597 Vali Loss: 0.1767241 Test Loss: 0.2074748
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2186683
	speed: 2.1896s/iter; left time: 13078.6821s
Epoch: 55 cost time: 133.2789716720581
Epoch: 55, Steps: 132 | Train Loss: 0.2191571 Vali Loss: 0.1766201 Test Loss: 0.2074697
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2090394
	speed: 2.2360s/iter; left time: 13060.1879s
Epoch: 56 cost time: 138.94030499458313
Epoch: 56, Steps: 132 | Train Loss: 0.2192091 Vali Loss: 0.1763531 Test Loss: 0.2074676
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2117062
	speed: 1.9727s/iter; left time: 11262.2515s
Epoch: 57 cost time: 123.88017892837524
Epoch: 57, Steps: 132 | Train Loss: 0.2191411 Vali Loss: 0.1761236 Test Loss: 0.2074695
Validation loss decreased (0.176234 --> 0.176124).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2158957
	speed: 1.9216s/iter; left time: 10717.0121s
Epoch: 58 cost time: 117.64514708518982
Epoch: 58, Steps: 132 | Train Loss: 0.2192416 Vali Loss: 0.1767972 Test Loss: 0.2074684
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2148678
	speed: 1.9339s/iter; left time: 10529.8966s
Epoch: 59 cost time: 119.00388979911804
Epoch: 59, Steps: 132 | Train Loss: 0.2191751 Vali Loss: 0.1765024 Test Loss: 0.2074694
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2223555
	speed: 1.9320s/iter; left time: 10264.9192s
Epoch: 60 cost time: 115.62416553497314
Epoch: 60, Steps: 132 | Train Loss: 0.2191973 Vali Loss: 0.1765842 Test Loss: 0.2074594
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2183286
	speed: 1.9107s/iter; left time: 9899.3881s
Epoch: 61 cost time: 117.8585159778595
Epoch: 61, Steps: 132 | Train Loss: 0.2191574 Vali Loss: 0.1765915 Test Loss: 0.2074602
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2184394
	speed: 1.8976s/iter; left time: 9580.8755s
Epoch: 62 cost time: 117.41455388069153
Epoch: 62, Steps: 132 | Train Loss: 0.2191591 Vali Loss: 0.1768606 Test Loss: 0.2074587
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2184550
	speed: 1.8924s/iter; left time: 9305.0856s
Epoch: 63 cost time: 116.86138796806335
Epoch: 63, Steps: 132 | Train Loss: 0.2191545 Vali Loss: 0.1764531 Test Loss: 0.2074529
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2213151
	speed: 1.8674s/iter; left time: 8935.4164s
Epoch: 64 cost time: 112.75386357307434
Epoch: 64, Steps: 132 | Train Loss: 0.2191169 Vali Loss: 0.1766838 Test Loss: 0.2074493
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2131527
	speed: 1.8283s/iter; left time: 8507.1682s
Epoch: 65 cost time: 113.92774152755737
Epoch: 65, Steps: 132 | Train Loss: 0.2191889 Vali Loss: 0.1766291 Test Loss: 0.2074533
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2285673
	speed: 1.8468s/iter; left time: 8349.3989s
Epoch: 66 cost time: 112.00115275382996
Epoch: 66, Steps: 132 | Train Loss: 0.2191481 Vali Loss: 0.1765895 Test Loss: 0.2074517
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2116131
	speed: 1.8171s/iter; left time: 7975.2258s
Epoch: 67 cost time: 110.0939736366272
Epoch: 67, Steps: 132 | Train Loss: 0.2191729 Vali Loss: 0.1762802 Test Loss: 0.2074521
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2216130
	speed: 1.7916s/iter; left time: 7626.9952s
Epoch: 68 cost time: 107.24521017074585
Epoch: 68, Steps: 132 | Train Loss: 0.2191300 Vali Loss: 0.1765449 Test Loss: 0.2074532
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2200502
	speed: 1.7769s/iter; left time: 7329.5209s
Epoch: 69 cost time: 103.54538774490356
Epoch: 69, Steps: 132 | Train Loss: 0.2191382 Vali Loss: 0.1764938 Test Loss: 0.2074501
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2375552
	speed: 1.6067s/iter; left time: 6415.6566s
Epoch: 70 cost time: 96.92879343032837
Epoch: 70, Steps: 132 | Train Loss: 0.2190781 Vali Loss: 0.1766640 Test Loss: 0.2074519
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2203693
	speed: 1.7677s/iter; left time: 6825.1631s
Epoch: 71 cost time: 121.46659350395203
Epoch: 71, Steps: 132 | Train Loss: 0.2191733 Vali Loss: 0.1765289 Test Loss: 0.2074496
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2224912
	speed: 2.1268s/iter; left time: 7930.8026s
Epoch: 72 cost time: 127.53205275535583
Epoch: 72, Steps: 132 | Train Loss: 0.2191568 Vali Loss: 0.1765890 Test Loss: 0.2074446
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2242494
	speed: 2.1760s/iter; left time: 7827.0570s
Epoch: 73 cost time: 132.20207524299622
Epoch: 73, Steps: 132 | Train Loss: 0.2191782 Vali Loss: 0.1766140 Test Loss: 0.2074489
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.2007362
	speed: 2.2048s/iter; left time: 7639.5424s
Epoch: 74 cost time: 137.63829398155212
Epoch: 74, Steps: 132 | Train Loss: 0.2191043 Vali Loss: 0.1763950 Test Loss: 0.2074463
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2225570
	speed: 2.1405s/iter; left time: 7134.3687s
Epoch: 75 cost time: 129.79415273666382
Epoch: 75, Steps: 132 | Train Loss: 0.2191356 Vali Loss: 0.1764524 Test Loss: 0.2074457
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2225278
	speed: 1.9992s/iter; left time: 6399.4715s
Epoch: 76 cost time: 117.33185529708862
Epoch: 76, Steps: 132 | Train Loss: 0.2191606 Vali Loss: 0.1767395 Test Loss: 0.2074444
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2127273
	speed: 2.0545s/iter; left time: 6305.2231s
Epoch: 77 cost time: 126.07089161872864
Epoch: 77, Steps: 132 | Train Loss: 0.2191907 Vali Loss: 0.1764694 Test Loss: 0.2074438
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_720_j720_H8_FITS_custom_ftM_sl720_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2057802677154541, mae:0.29610657691955566, rse:0.45251110196113586, corr:[0.44798565 0.4460921  0.4495648  0.44936633 0.4512325  0.45133018
 0.45134905 0.45152792 0.45111942 0.45059717 0.4503129  0.45011663
 0.44976988 0.4498132  0.44987136 0.44977474 0.44995254 0.44995356
 0.4499211  0.4501406  0.4500846  0.4501406  0.45055044 0.45067236
 0.4507975  0.4512938  0.4513607  0.45119727 0.45113063 0.4509
 0.4505674  0.45035174 0.45016438 0.44999424 0.4498823  0.44987434
 0.44985706 0.4498617  0.4499499  0.45000696 0.45006183 0.45012286
 0.45008898 0.45005867 0.45009267 0.45006782 0.45013732 0.45027095
 0.4502685  0.45038468 0.4504972  0.4503338  0.45014793 0.45010844
 0.44996274 0.44974902 0.44964707 0.44961178 0.449579   0.4495888
 0.44965935 0.4497227  0.44975716 0.44980368 0.44983855 0.44987917
 0.44988966 0.44972932 0.44965902 0.44970486 0.4496572  0.44963756
 0.44972107 0.44971392 0.44963554 0.44962206 0.4494711  0.44931537
 0.4492858  0.44918922 0.44907752 0.44907844 0.44911438 0.44911548
 0.44910747 0.44915137 0.44920588 0.44923183 0.44926432 0.44927156
 0.44925505 0.4491701  0.44906154 0.44906077 0.44907913 0.4490241
 0.44904605 0.4491346  0.4490586  0.44897148 0.44891945 0.44880053
 0.44868702 0.44864935 0.4486159  0.448597   0.4485864  0.4486005
 0.44862124 0.44861877 0.44862884 0.44868428 0.44870943 0.4486814
 0.4486259  0.44853592 0.44846296 0.4484071  0.44841594 0.44853914
 0.4486898  0.44873688 0.4487586  0.44870296 0.44857937 0.44855335
 0.4485271  0.44845885 0.4484281  0.4484182  0.4484308  0.4484485
 0.44843394 0.44846523 0.44853896 0.44855505 0.44856656 0.44860104
 0.44863772 0.44861683 0.44859368 0.44859785 0.44856814 0.44849673
 0.44845092 0.44847423 0.44849476 0.4485048  0.44842973 0.44834962
 0.448355   0.4483834  0.44836754 0.44832847 0.44830394 0.4483105
 0.44831434 0.44830272 0.4483355  0.44837523 0.4483973  0.44838727
 0.44843608 0.4483966  0.44826344 0.448217   0.4481482  0.4479218
 0.44769585 0.44774917 0.4477938  0.44770575 0.44762614 0.44760036
 0.4474953  0.44738528 0.44738573 0.44737685 0.44729945 0.44728553
 0.44732818 0.44734916 0.44732738 0.4473019  0.44731045 0.44727987
 0.44717574 0.44698375 0.44681624 0.44668245 0.44659328 0.44656283
 0.44655147 0.44662097 0.44669437 0.44671884 0.44661593 0.44654825
 0.44652236 0.44642702 0.4463631  0.44637024 0.4463436  0.44629183
 0.44626978 0.44627312 0.446295   0.44628224 0.44625494 0.44624233
 0.44618696 0.4459463  0.4457549  0.44569656 0.44563258 0.4456102
 0.44568312 0.4458001  0.44586635 0.44591704 0.44588    0.44581154
 0.44578612 0.44574463 0.44572333 0.44572625 0.44570923 0.44567478
 0.445651   0.44562176 0.4456078  0.44559303 0.4455692  0.44551536
 0.44545925 0.4453093  0.44516075 0.44508383 0.44507593 0.44505537
 0.44506282 0.4451856  0.44526544 0.4452624  0.44520652 0.44518527
 0.44519854 0.44515765 0.44509906 0.44507492 0.4450647  0.44502604
 0.4449601  0.44491848 0.444898   0.44488254 0.44486332 0.4448202
 0.4447556  0.44465888 0.44460937 0.444585   0.4445439  0.44454363
 0.44463962 0.4447526  0.44478825 0.44479755 0.4447876  0.44478568
 0.44475445 0.4447064  0.44468367 0.4446516  0.44459227 0.44456252
 0.44453624 0.44447565 0.44448256 0.44447818 0.44440135 0.4443707
 0.44436264 0.4442204  0.44408134 0.4440811  0.4441311  0.44422483
 0.44443846 0.44457424 0.44461954 0.44466734 0.44468066 0.4446823
 0.44468424 0.4446772  0.44463694 0.4445672  0.4444996  0.44444165
 0.44440162 0.44436795 0.44431934 0.44430533 0.44433022 0.44429964
 0.44428957 0.4443019  0.44428062 0.44425896 0.4442249  0.444179
 0.44419253 0.44428152 0.44435155 0.44439527 0.44437885 0.44437313
 0.44437844 0.44435135 0.44433948 0.4443116  0.44425505 0.444194
 0.44412148 0.44408602 0.44409668 0.4440581  0.44404265 0.4440597
 0.444045   0.4439239  0.44379616 0.44373098 0.44361064 0.44341868
 0.44328734 0.44335476 0.44339916 0.44341826 0.44336528 0.4432676
 0.44316387 0.4430788  0.44300783 0.4429209  0.44281256 0.44274557
 0.4427014  0.44264376 0.4425985  0.4425426  0.4424906  0.44247115
 0.4424657  0.44234186 0.44221133 0.4421961  0.44217998 0.4421183
 0.44212177 0.44224346 0.442292   0.44234607 0.4423564  0.44229302
 0.44221705 0.4421538  0.44209126 0.44202232 0.44191667 0.44180882
 0.44176474 0.44173747 0.44170833 0.44166136 0.44160727 0.44152158
 0.4414413  0.44134974 0.44128895 0.44122517 0.44118392 0.44121853
 0.44127506 0.44136354 0.44147077 0.44154707 0.44153625 0.44151905
 0.44147316 0.44140726 0.44136053 0.441301   0.4412206  0.44118065
 0.44112837 0.44104397 0.4410032  0.44096887 0.4409333  0.4409194
 0.44088972 0.4407524  0.44064215 0.4406273  0.44061965 0.44062194
 0.44067523 0.44077078 0.44084343 0.44094753 0.44098535 0.4409706
 0.4409637  0.44090176 0.4407745  0.4407143  0.44068503 0.44060898
 0.44056845 0.44052523 0.44048917 0.440474   0.44042936 0.44037378
 0.44038284 0.44033137 0.44025406 0.4402283  0.44022167 0.44025642
 0.44037017 0.44049922 0.4405935  0.4406747  0.44071445 0.44074672
 0.44072983 0.44065467 0.4405899  0.44049942 0.44039673 0.44033417
 0.44026268 0.44020027 0.44017553 0.44012985 0.44011718 0.44014212
 0.44008055 0.4399409  0.43986663 0.43984976 0.43988627 0.44000366
 0.4402469  0.44043937 0.44052327 0.44059712 0.4406544  0.44067714
 0.44066292 0.44064033 0.44059065 0.44047654 0.4403578  0.44028476
 0.44021714 0.44013652 0.44014838 0.4401821  0.44013372 0.44009352
 0.4401703  0.44020176 0.4401957  0.44023508 0.44021374 0.44022453
 0.44030204 0.44037193 0.4404578  0.44056886 0.44057822 0.44058257
 0.44060394 0.44055808 0.44047654 0.44040006 0.4403295  0.44023737
 0.44016874 0.44017157 0.44014847 0.44010216 0.4401493  0.4401673
 0.440142   0.44008887 0.4400048  0.4399337  0.4398925  0.43974686
 0.43955845 0.4395946  0.4396511  0.43968236 0.43966377 0.4396254
 0.4395394  0.4394098  0.43928325 0.43916187 0.43902686 0.4389267
 0.4388337  0.43873027 0.43868938 0.43866912 0.4386185  0.43860522
 0.4385913  0.43842688 0.43829912 0.43828523 0.43824717 0.4381736
 0.4381431  0.43825886 0.43839538 0.43848586 0.43843833 0.4383617
 0.43826634 0.4381299  0.43800578 0.43785727 0.4376678  0.43757483
 0.43752292 0.43742555 0.4373913  0.43738437 0.43731993 0.4372721
 0.43725345 0.4371384  0.43705714 0.43706915 0.4371103  0.43711582
 0.4371599  0.43728736 0.43736213 0.437379   0.43733492 0.43725547
 0.43718636 0.4370876  0.43695349 0.43682235 0.43666798 0.43646276
 0.4363146  0.4362544  0.43619776 0.43609264 0.43601602 0.43590343
 0.43579122 0.43576044 0.43574768 0.4357179  0.43574908 0.43579814
 0.4358736  0.43600127 0.43612167 0.43622357 0.43622974 0.43615288
 0.43605936 0.43594015 0.4357959  0.4356335  0.43546703 0.43535227
 0.43521684 0.4351051  0.43514362 0.43512076 0.43503478 0.43507174
 0.43512285 0.43503782 0.435039   0.43509638 0.43508083 0.43510464
 0.4352357  0.4353848  0.43547034 0.43550384 0.43547496 0.43543673
 0.43534702 0.4351829  0.43500954 0.4348638  0.43468615 0.43454435
 0.43448982 0.43440944 0.43436214 0.43445292 0.43449375 0.43445608
 0.4345126  0.43449116 0.4343841  0.43447825 0.43462884 0.4347296
 0.43501875 0.43520582 0.43523476 0.43528968 0.43525103 0.43513232
 0.43503997 0.43492216 0.4347379  0.43451017 0.43435055 0.43425086
 0.43412134 0.4340996  0.43416435 0.43412605 0.4341685  0.43427458
 0.43429682 0.43436232 0.43447646 0.43451026 0.4345837  0.43465218
 0.43462884 0.4347394  0.43486324 0.43484423 0.4347738  0.43468815
 0.43452364 0.4343746  0.43423888 0.43398082 0.43377787 0.4337565
 0.4336674  0.4335815  0.43374762 0.4338907  0.43387154 0.4339694
 0.43412936 0.43404514 0.43402064 0.43415144 0.43413532 0.43400696
 0.43384284 0.43374887 0.4337146  0.43369976 0.43343607 0.43317112
 0.43301243 0.43266922 0.43239084 0.43226928 0.43198952 0.43186653
 0.4320344  0.43205568 0.43205506 0.43228695 0.43243223 0.43242788
 0.4325298  0.43250388 0.43226856 0.432279   0.43241245 0.432191
 0.43208835 0.4321715  0.4318536  0.43171665 0.43160072 0.43107376
 0.43093345 0.4308784  0.43051004 0.4306243  0.4307469  0.4306762
 0.4311382  0.43160143 0.4316148  0.4316926  0.4319588  0.43135875
 0.43070307 0.43095547 0.4251081  0.43009847 0.42454845 0.43160787]
