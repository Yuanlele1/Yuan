Args in experiment:
Namespace(H_order=8, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=138, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_360_j720_H8', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_360_j720_H8_FITS_custom_ftM_sl360_ll48_pl720_H8_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17333
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=138, out_features=414, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2347439616.0
params:  57546.0
Trainable parameters:  57546
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.7227761
	speed: 0.7100s/iter; left time: 9514.4202s
Epoch: 1 cost time: 95.97518515586853
Epoch: 1, Steps: 135 | Train Loss: 0.9640201 Vali Loss: 0.5331492 Test Loss: 0.6350673
Validation loss decreased (inf --> 0.533149).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.5120149
	speed: 1.8203s/iter; left time: 24148.1281s
Epoch: 2 cost time: 120.98410391807556
Epoch: 2, Steps: 135 | Train Loss: 0.5544761 Vali Loss: 0.3831937 Test Loss: 0.4627970
Validation loss decreased (0.533149 --> 0.383194).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4036005
	speed: 1.8111s/iter; left time: 23781.2839s
Epoch: 3 cost time: 96.4199628829956
Epoch: 3, Steps: 135 | Train Loss: 0.4215863 Vali Loss: 0.2987145 Test Loss: 0.3633513
Validation loss decreased (0.383194 --> 0.298714).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3097697
	speed: 1.7368s/iter; left time: 22571.3558s
Epoch: 4 cost time: 100.73000288009644
Epoch: 4, Steps: 135 | Train Loss: 0.3418869 Vali Loss: 0.2474468 Test Loss: 0.3019051
Validation loss decreased (0.298714 --> 0.247447).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2934265
	speed: 1.7116s/iter; left time: 22012.6465s
Epoch: 5 cost time: 102.05225133895874
Epoch: 5, Steps: 135 | Train Loss: 0.2931427 Vali Loss: 0.2173673 Test Loss: 0.2644518
Validation loss decreased (0.247447 --> 0.217367).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2538253
	speed: 2.1588s/iter; left time: 27472.6532s
Epoch: 6 cost time: 116.06454634666443
Epoch: 6, Steps: 135 | Train Loss: 0.2641319 Vali Loss: 0.2004303 Test Loss: 0.2421341
Validation loss decreased (0.217367 --> 0.200430).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2361178
	speed: 1.7865s/iter; left time: 22493.9282s
Epoch: 7 cost time: 94.9224328994751
Epoch: 7, Steps: 135 | Train Loss: 0.2472636 Vali Loss: 0.1907737 Test Loss: 0.2291641
Validation loss decreased (0.200430 --> 0.190774).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2405745
	speed: 1.6543s/iter; left time: 20606.5541s
Epoch: 8 cost time: 94.7328929901123
Epoch: 8, Steps: 135 | Train Loss: 0.2376412 Vali Loss: 0.1858913 Test Loss: 0.2218082
Validation loss decreased (0.190774 --> 0.185891).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2448193
	speed: 1.6167s/iter; left time: 19918.7631s
Epoch: 9 cost time: 96.00594210624695
Epoch: 9, Steps: 135 | Train Loss: 0.2325783 Vali Loss: 0.1836394 Test Loss: 0.2176700
Validation loss decreased (0.185891 --> 0.183639).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2235584
	speed: 1.9355s/iter; left time: 23585.4166s
Epoch: 10 cost time: 129.876451253891
Epoch: 10, Steps: 135 | Train Loss: 0.2297052 Vali Loss: 0.1822312 Test Loss: 0.2153774
Validation loss decreased (0.183639 --> 0.182231).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2247332
	speed: 2.0079s/iter; left time: 24197.1088s
Epoch: 11 cost time: 94.73667359352112
Epoch: 11, Steps: 135 | Train Loss: 0.2281901 Vali Loss: 0.1821125 Test Loss: 0.2140977
Validation loss decreased (0.182231 --> 0.182112).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2254408
	speed: 1.6422s/iter; left time: 19568.3539s
Epoch: 12 cost time: 106.95514392852783
Epoch: 12, Steps: 135 | Train Loss: 0.2274540 Vali Loss: 0.1816211 Test Loss: 0.2134383
Validation loss decreased (0.182112 --> 0.181621).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2294528
	speed: 2.2063s/iter; left time: 25992.6504s
Epoch: 13 cost time: 115.99902272224426
Epoch: 13, Steps: 135 | Train Loss: 0.2269890 Vali Loss: 0.1816000 Test Loss: 0.2130068
Validation loss decreased (0.181621 --> 0.181600).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2206246
	speed: 1.8092s/iter; left time: 21069.3944s
Epoch: 14 cost time: 106.79032468795776
Epoch: 14, Steps: 135 | Train Loss: 0.2267661 Vali Loss: 0.1817797 Test Loss: 0.2127395
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2143224
	speed: 1.6875s/iter; left time: 19424.7400s
Epoch: 15 cost time: 97.3473916053772
Epoch: 15, Steps: 135 | Train Loss: 0.2266342 Vali Loss: 0.1819213 Test Loss: 0.2126117
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2149513
	speed: 1.7381s/iter; left time: 19772.6488s
Epoch: 16 cost time: 102.3627667427063
Epoch: 16, Steps: 135 | Train Loss: 0.2265176 Vali Loss: 0.1818735 Test Loss: 0.2125065
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2141414
	speed: 1.8038s/iter; left time: 20276.2675s
Epoch: 17 cost time: 102.39669704437256
Epoch: 17, Steps: 135 | Train Loss: 0.2264614 Vali Loss: 0.1816383 Test Loss: 0.2124439
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2272165
	speed: 1.6682s/iter; left time: 18527.0647s
Epoch: 18 cost time: 101.16727209091187
Epoch: 18, Steps: 135 | Train Loss: 0.2264109 Vali Loss: 0.1816283 Test Loss: 0.2123839
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2374209
	speed: 1.7147s/iter; left time: 18811.5625s
Epoch: 19 cost time: 102.83401656150818
Epoch: 19, Steps: 135 | Train Loss: 0.2264809 Vali Loss: 0.1815632 Test Loss: 0.2123634
Validation loss decreased (0.181600 --> 0.181563).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2292069
	speed: 1.7898s/iter; left time: 19394.3293s
Epoch: 20 cost time: 100.8286714553833
Epoch: 20, Steps: 135 | Train Loss: 0.2263400 Vali Loss: 0.1817793 Test Loss: 0.2123155
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2140386
	speed: 1.7791s/iter; left time: 19037.9684s
Epoch: 21 cost time: 105.81006121635437
Epoch: 21, Steps: 135 | Train Loss: 0.2263015 Vali Loss: 0.1816609 Test Loss: 0.2122969
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2337297
	speed: 1.8235s/iter; left time: 19267.2363s
Epoch: 22 cost time: 104.45063924789429
Epoch: 22, Steps: 135 | Train Loss: 0.2262815 Vali Loss: 0.1819866 Test Loss: 0.2122733
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2389457
	speed: 1.8474s/iter; left time: 19269.8308s
Epoch: 23 cost time: 102.6742594242096
Epoch: 23, Steps: 135 | Train Loss: 0.2262569 Vali Loss: 0.1814750 Test Loss: 0.2122701
Validation loss decreased (0.181563 --> 0.181475).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2343203
	speed: 1.7997s/iter; left time: 18530.2195s
Epoch: 24 cost time: 99.48809862136841
Epoch: 24, Steps: 135 | Train Loss: 0.2262644 Vali Loss: 0.1816410 Test Loss: 0.2122464
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2087580
	speed: 1.6963s/iter; left time: 17236.4285s
Epoch: 25 cost time: 99.01874566078186
Epoch: 25, Steps: 135 | Train Loss: 0.2262895 Vali Loss: 0.1815735 Test Loss: 0.2122303
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2237024
	speed: 1.6818s/iter; left time: 16862.1394s
Epoch: 26 cost time: 96.31801772117615
Epoch: 26, Steps: 135 | Train Loss: 0.2262664 Vali Loss: 0.1815821 Test Loss: 0.2122178
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2183426
	speed: 1.7435s/iter; left time: 17244.5841s
Epoch: 27 cost time: 101.0700261592865
Epoch: 27, Steps: 135 | Train Loss: 0.2261655 Vali Loss: 0.1815825 Test Loss: 0.2122238
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2159234
	speed: 1.8110s/iter; left time: 17668.4464s
Epoch: 28 cost time: 105.28564238548279
Epoch: 28, Steps: 135 | Train Loss: 0.2262247 Vali Loss: 0.1818728 Test Loss: 0.2122125
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2320289
	speed: 1.7963s/iter; left time: 17282.0398s
Epoch: 29 cost time: 103.53145694732666
Epoch: 29, Steps: 135 | Train Loss: 0.2261469 Vali Loss: 0.1818306 Test Loss: 0.2122110
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2250734
	speed: 1.6847s/iter; left time: 15980.8146s
Epoch: 30 cost time: 96.08881664276123
Epoch: 30, Steps: 135 | Train Loss: 0.2261334 Vali Loss: 0.1814480 Test Loss: 0.2122198
Validation loss decreased (0.181475 --> 0.181448).  Saving model ...
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2153148
	speed: 1.6601s/iter; left time: 15523.4821s
Epoch: 31 cost time: 104.64150285720825
Epoch: 31, Steps: 135 | Train Loss: 0.2261755 Vali Loss: 0.1814118 Test Loss: 0.2122028
Validation loss decreased (0.181448 --> 0.181412).  Saving model ...
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2309906
	speed: 2.4336s/iter; left time: 22428.3056s
Epoch: 32 cost time: 145.27710032463074
Epoch: 32, Steps: 135 | Train Loss: 0.2261467 Vali Loss: 0.1817408 Test Loss: 0.2121872
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2363326
	speed: 1.9272s/iter; left time: 17500.5753s
Epoch: 33 cost time: 98.22756910324097
Epoch: 33, Steps: 135 | Train Loss: 0.2261446 Vali Loss: 0.1815149 Test Loss: 0.2121805
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2215191
	speed: 1.6888s/iter; left time: 15107.7312s
Epoch: 34 cost time: 98.45121574401855
Epoch: 34, Steps: 135 | Train Loss: 0.2261159 Vali Loss: 0.1814898 Test Loss: 0.2121848
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2286249
	speed: 1.7222s/iter; left time: 15174.3671s
Epoch: 35 cost time: 106.15421724319458
Epoch: 35, Steps: 135 | Train Loss: 0.2261547 Vali Loss: 0.1814496 Test Loss: 0.2121777
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2188653
	speed: 2.0707s/iter; left time: 17965.1973s
Epoch: 36 cost time: 140.9896366596222
Epoch: 36, Steps: 135 | Train Loss: 0.2260822 Vali Loss: 0.1815237 Test Loss: 0.2121674
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2207203
	speed: 2.4520s/iter; left time: 20942.1436s
Epoch: 37 cost time: 124.34678721427917
Epoch: 37, Steps: 135 | Train Loss: 0.2261186 Vali Loss: 0.1814054 Test Loss: 0.2121692
Validation loss decreased (0.181412 --> 0.181405).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2286511
	speed: 1.7685s/iter; left time: 14865.8506s
Epoch: 38 cost time: 97.55202865600586
Epoch: 38, Steps: 135 | Train Loss: 0.2261189 Vali Loss: 0.1815983 Test Loss: 0.2121683
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2270318
	speed: 1.8926s/iter; left time: 15653.3104s
Epoch: 39 cost time: 98.9116563796997
Epoch: 39, Steps: 135 | Train Loss: 0.2260591 Vali Loss: 0.1816358 Test Loss: 0.2121571
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2422891
	speed: 1.6451s/iter; left time: 13384.5275s
Epoch: 40 cost time: 100.61752700805664
Epoch: 40, Steps: 135 | Train Loss: 0.2260634 Vali Loss: 0.1817002 Test Loss: 0.2121519
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2343213
	speed: 1.6810s/iter; left time: 13449.3012s
Epoch: 41 cost time: 91.94886302947998
Epoch: 41, Steps: 135 | Train Loss: 0.2260740 Vali Loss: 0.1812303 Test Loss: 0.2121485
Validation loss decreased (0.181405 --> 0.181230).  Saving model ...
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2293612
	speed: 1.6643s/iter; left time: 13091.4895s
Epoch: 42 cost time: 103.92643070220947
Epoch: 42, Steps: 135 | Train Loss: 0.2260913 Vali Loss: 0.1815044 Test Loss: 0.2121488
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2185392
	speed: 1.5969s/iter; left time: 12345.8055s
Epoch: 43 cost time: 83.23921966552734
Epoch: 43, Steps: 135 | Train Loss: 0.2260537 Vali Loss: 0.1817332 Test Loss: 0.2121456
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2236544
	speed: 1.4010s/iter; left time: 10641.6358s
Epoch: 44 cost time: 80.35066318511963
Epoch: 44, Steps: 135 | Train Loss: 0.2260150 Vali Loss: 0.1813400 Test Loss: 0.2121495
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2209733
	speed: 1.4146s/iter; left time: 10554.3539s
Epoch: 45 cost time: 84.70144295692444
Epoch: 45, Steps: 135 | Train Loss: 0.2260688 Vali Loss: 0.1815260 Test Loss: 0.2121501
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2361104
	speed: 1.5437s/iter; left time: 11309.5007s
Epoch: 46 cost time: 108.25885915756226
Epoch: 46, Steps: 135 | Train Loss: 0.2260797 Vali Loss: 0.1815168 Test Loss: 0.2121394
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2159315
	speed: 1.8575s/iter; left time: 13357.5077s
Epoch: 47 cost time: 88.77468204498291
Epoch: 47, Steps: 135 | Train Loss: 0.2260275 Vali Loss: 0.1815006 Test Loss: 0.2121402
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2258815
	speed: 1.5630s/iter; left time: 11028.3615s
Epoch: 48 cost time: 96.54117941856384
Epoch: 48, Steps: 135 | Train Loss: 0.2260794 Vali Loss: 0.1816189 Test Loss: 0.2121359
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2205896
	speed: 1.6034s/iter; left time: 11097.0358s
Epoch: 49 cost time: 111.44310903549194
Epoch: 49, Steps: 135 | Train Loss: 0.2260819 Vali Loss: 0.1810823 Test Loss: 0.2121315
Validation loss decreased (0.181230 --> 0.181082).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2164358
	speed: 2.1388s/iter; left time: 14513.7624s
Epoch: 50 cost time: 119.97152638435364
Epoch: 50, Steps: 135 | Train Loss: 0.2260668 Vali Loss: 0.1813044 Test Loss: 0.2121413
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2304470
	speed: 2.1552s/iter; left time: 14334.1712s
Epoch: 51 cost time: 121.13469576835632
Epoch: 51, Steps: 135 | Train Loss: 0.2260570 Vali Loss: 0.1815644 Test Loss: 0.2121378
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2213330
	speed: 2.1284s/iter; left time: 13868.9396s
Epoch: 52 cost time: 104.69930148124695
Epoch: 52, Steps: 135 | Train Loss: 0.2260595 Vali Loss: 0.1814131 Test Loss: 0.2121380
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2139960
	speed: 1.4607s/iter; left time: 9320.8985s
Epoch: 53 cost time: 89.20410013198853
Epoch: 53, Steps: 135 | Train Loss: 0.2260480 Vali Loss: 0.1814002 Test Loss: 0.2121339
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2182721
	speed: 1.4855s/iter; left time: 9278.2639s
Epoch: 54 cost time: 99.28500175476074
Epoch: 54, Steps: 135 | Train Loss: 0.2260159 Vali Loss: 0.1812078 Test Loss: 0.2121338
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2296779
	speed: 1.5979s/iter; left time: 9764.6985s
Epoch: 55 cost time: 82.97375297546387
Epoch: 55, Steps: 135 | Train Loss: 0.2260879 Vali Loss: 0.1815647 Test Loss: 0.2121313
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2414246
	speed: 1.3748s/iter; left time: 8216.0246s
Epoch: 56 cost time: 79.61802887916565
Epoch: 56, Steps: 135 | Train Loss: 0.2259865 Vali Loss: 0.1816459 Test Loss: 0.2121274
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2241375
	speed: 1.5995s/iter; left time: 9342.7292s
Epoch: 57 cost time: 105.4607286453247
Epoch: 57, Steps: 135 | Train Loss: 0.2260432 Vali Loss: 0.1812837 Test Loss: 0.2121243
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2211520
	speed: 1.6741s/iter; left time: 9552.4148s
Epoch: 58 cost time: 89.70159649848938
Epoch: 58, Steps: 135 | Train Loss: 0.2260465 Vali Loss: 0.1814748 Test Loss: 0.2121244
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2197986
	speed: 1.1956s/iter; left time: 6660.9320s
Epoch: 59 cost time: 67.54124546051025
Epoch: 59, Steps: 135 | Train Loss: 0.2259644 Vali Loss: 0.1813797 Test Loss: 0.2121259
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2285101
	speed: 1.3230s/iter; left time: 7191.9079s
Epoch: 60 cost time: 81.61736011505127
Epoch: 60, Steps: 135 | Train Loss: 0.2260261 Vali Loss: 0.1813773 Test Loss: 0.2121231
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2150140
	speed: 1.2426s/iter; left time: 6586.7901s
Epoch: 61 cost time: 73.4623396396637
Epoch: 61, Steps: 135 | Train Loss: 0.2260753 Vali Loss: 0.1812573 Test Loss: 0.2121199
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2372194
	speed: 1.0310s/iter; left time: 5325.9685s
Epoch: 62 cost time: 61.460745096206665
Epoch: 62, Steps: 135 | Train Loss: 0.2260297 Vali Loss: 0.1813860 Test Loss: 0.2121207
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2260676
	speed: 1.0401s/iter; left time: 5232.9112s
Epoch: 63 cost time: 54.26777362823486
Epoch: 63, Steps: 135 | Train Loss: 0.2259970 Vali Loss: 0.1812284 Test Loss: 0.2121182
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2316526
	speed: 0.8832s/iter; left time: 4324.1502s
Epoch: 64 cost time: 50.90329194068909
Epoch: 64, Steps: 135 | Train Loss: 0.2260173 Vali Loss: 0.1811402 Test Loss: 0.2121222
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2333115
	speed: 0.8812s/iter; left time: 4195.2997s
Epoch: 65 cost time: 52.72220516204834
Epoch: 65, Steps: 135 | Train Loss: 0.2260331 Vali Loss: 0.1813434 Test Loss: 0.2121202
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2364726
	speed: 0.9240s/iter; left time: 4274.6488s
Epoch: 66 cost time: 53.41499471664429
Epoch: 66, Steps: 135 | Train Loss: 0.2259983 Vali Loss: 0.1812211 Test Loss: 0.2121204
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2363118
	speed: 0.8923s/iter; left time: 4007.2229s
Epoch: 67 cost time: 51.27151107788086
Epoch: 67, Steps: 135 | Train Loss: 0.2260450 Vali Loss: 0.1814373 Test Loss: 0.2121194
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2203734
	speed: 0.8851s/iter; left time: 3855.5306s
Epoch: 68 cost time: 51.30209994316101
Epoch: 68, Steps: 135 | Train Loss: 0.2260136 Vali Loss: 0.1813936 Test Loss: 0.2121202
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2215839
	speed: 0.8690s/iter; left time: 3668.0372s
Epoch: 69 cost time: 53.72668504714966
Epoch: 69, Steps: 135 | Train Loss: 0.2260117 Vali Loss: 0.1814764 Test Loss: 0.2121182
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_360_j720_H8_FITS_custom_ftM_sl360_ll48_pl720_H8_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.21054169535636902, mae:0.29905590415000916, rse:0.4577163755893707, corr:[0.44666514 0.44745734 0.44927183 0.4495193  0.45043102 0.4504634
 0.45047957 0.45041248 0.45003867 0.44967777 0.44945213 0.44920146
 0.4490253  0.44907558 0.4490102  0.4490106  0.4490594  0.44901413
 0.4490646  0.44894654 0.44887596 0.4490314  0.44905117 0.44924796
 0.44946682 0.44952407 0.44963434 0.4496613  0.4494437  0.44928014
 0.44909942 0.4488071  0.44860357 0.44852346 0.44837466 0.4483137
 0.44836998 0.44834667 0.44836935 0.448389   0.4483285  0.44840407
 0.44840336 0.44824892 0.44824505 0.44825202 0.44826883 0.4484195
 0.44843477 0.4485118  0.4486067  0.44845122 0.44829208 0.4482109
 0.4480635  0.44792318 0.44784606 0.44774795 0.44769973 0.44776747
 0.44776085 0.44776538 0.447823   0.44778797 0.4477997  0.4478447
 0.44777292 0.44767994 0.44761977 0.4475591  0.4476168  0.44765422
 0.44761175 0.44769767 0.44767913 0.44754344 0.44743517 0.44732475
 0.44723132 0.44715568 0.4470477  0.44701195 0.4470308  0.4469763
 0.44695345 0.44703472 0.4470545  0.44705027 0.44710356 0.44708923
 0.44707054 0.44703254 0.44693005 0.44691294 0.44692552 0.4469297
 0.44699702 0.44704214 0.44699323 0.4469685  0.44684628 0.44668567
 0.44666827 0.44664785 0.44655836 0.44653252 0.4464894  0.4464451
 0.4464918  0.446527   0.44651788 0.4465667  0.44656384 0.44655874
 0.4465648  0.44643497 0.44636554 0.44642752 0.4464221  0.44646484
 0.4465854  0.44662985 0.4466552  0.4466579  0.44656006 0.44652784
 0.44655398 0.4465125  0.44648445 0.44646358 0.44640848 0.44642207
 0.44643995 0.44639602 0.44642422 0.44645876 0.44640958 0.4464309
 0.446496   0.44651595 0.44661173 0.44672754 0.44675246 0.4469345
 0.44723383 0.44739178 0.44750816 0.4475207  0.44746602 0.44749027
 0.44746983 0.4474242  0.44743228 0.44737685 0.44731605 0.44733593
 0.4473392  0.44732535 0.44738403 0.44737184 0.44735134 0.44736922
 0.44736663 0.44733804 0.4472777  0.4471802  0.44708776 0.44690466
 0.44661036 0.44647324 0.44640628 0.4462824  0.44618964 0.44607115
 0.44591406 0.44584385 0.44576758 0.445649   0.44563484 0.44562656
 0.4455582  0.44555748 0.44557792 0.445535   0.44551107 0.44548833
 0.4454113  0.44521716 0.4449793  0.44487113 0.44484696 0.4447698
 0.44470713 0.44473103 0.44470876 0.4447083  0.4446689  0.44454196
 0.4444693  0.44443157 0.4443402  0.44429    0.44426292 0.4442113
 0.44421494 0.44425112 0.4442023  0.44416937 0.4441755  0.4441213
 0.44407114 0.4439114  0.44371375 0.4436688  0.44366062 0.44364485
 0.44369224 0.4437653  0.44380492 0.4438308  0.44373983 0.44365114
 0.44365016 0.44361785 0.44356552 0.44353297 0.44350836 0.44348663
 0.44347483 0.4434193  0.44339356 0.44340587 0.4433471  0.44328052
 0.4432634  0.44308585 0.44296613 0.44294125 0.44287717 0.4428906
 0.44291022 0.44292074 0.4429837  0.44299215 0.4429033  0.44290492
 0.44287857 0.44278085 0.44276917 0.44273248 0.44265625 0.44268757
 0.4426906  0.44262376 0.44262445 0.44261944 0.44255602 0.44255564
 0.44252464 0.4424001  0.4423503  0.4423051  0.44228476 0.44236428
 0.44238588 0.44244152 0.44253457 0.4425022  0.4424378  0.44246867
 0.44241259 0.44235355 0.44233528 0.44223595 0.44218016 0.44218594
 0.44213665 0.44212946 0.44218707 0.4421458  0.44212452 0.4421672
 0.44206804 0.44192278 0.44187784 0.44183266 0.44186133 0.4419632
 0.4420679  0.44220284 0.4422899  0.44230762 0.4423149  0.44230592
 0.44229126 0.44231105 0.44226256 0.44218096 0.44216242 0.44213617
 0.44207248 0.44206503 0.44204512 0.4420086  0.442013   0.441966
 0.4419782  0.44207442 0.44207838 0.44214547 0.44226623 0.44241482
 0.4427326  0.44297355 0.44304964 0.44312328 0.44317934 0.44314402
 0.44311714 0.44315177 0.44311714 0.44303882 0.44300282 0.44294086
 0.44290274 0.4428861  0.4428659  0.44286367 0.4428345  0.44277793
 0.4427699  0.4426642  0.4424868  0.4424372  0.4423271  0.44206345
 0.44182804 0.44177043 0.44171572 0.44166163 0.44156572 0.44143313
 0.4413395  0.44120723 0.44105828 0.4409726  0.44085634 0.4407661
 0.44074935 0.440666   0.44060534 0.44061336 0.44055516 0.44053194
 0.4405313  0.44033614 0.44019672 0.44022527 0.4401847  0.44013807
 0.44007504 0.4400603  0.44011596 0.44014776 0.44007078 0.43999255
 0.43989325 0.43976676 0.43970153 0.43962005 0.43947512 0.43939593
 0.43936473 0.4393157  0.43928874 0.43922555 0.4391529  0.43917644
 0.43915436 0.43898776 0.43890727 0.43887103 0.43886307 0.43895072
 0.43895245 0.4390165  0.43915614 0.43914658 0.43910238 0.4390811
 0.43899292 0.43896896 0.43895411 0.4388247  0.4387131  0.43868098
 0.43861854 0.43856338 0.43856993 0.43853077 0.4385071  0.43851006
 0.4384162  0.43827644 0.43822375 0.4381939  0.43819803 0.43823367
 0.43825603 0.43835875 0.4384177  0.438424   0.43846717 0.43845657
 0.43839613 0.43837836 0.43826056 0.4381428  0.43814173 0.43810093
 0.43804786 0.43804994 0.43802765 0.43796068 0.43795425 0.4379589
 0.43792674 0.43783733 0.43775007 0.43776911 0.43783325 0.43785408
 0.43792996 0.43804786 0.43807003 0.43812808 0.43817794 0.43813264
 0.43812868 0.4380881  0.4379461  0.43786302 0.43782866 0.43773583
 0.43767565 0.43766147 0.4376281  0.4376356  0.43761215 0.43757263
 0.4375627  0.43743455 0.43730292 0.43735886 0.437454   0.43756643
 0.43778884 0.43791416 0.43797603 0.4380988  0.4381142  0.4380863
 0.4381458  0.43813708 0.43805695 0.43798602 0.4378574  0.43778425
 0.43782336 0.4377902  0.4377615  0.43777412 0.43771923 0.43770579
 0.4377761  0.4377992  0.4378797  0.43806747 0.43818998 0.43841723
 0.4387401  0.4389265  0.43907928 0.43915972 0.43912655 0.4391603
 0.43920475 0.4391574  0.4391183  0.43904376 0.43891022 0.43888423
 0.43886906 0.43879393 0.43883628 0.4388192  0.43873656 0.4387905
 0.43879455 0.4386771  0.43860334 0.43850946 0.4384124  0.4382842
 0.43801853 0.43791512 0.43790287 0.4378222  0.43773302 0.43763822
 0.43749768 0.43738452 0.4372667  0.43709087 0.43696177 0.43688816
 0.43678972 0.43674958 0.43675578 0.43667388 0.4366661  0.43671516
 0.43658447 0.436407   0.4363232  0.43628156 0.4363058  0.43628386
 0.43617254 0.43618187 0.43618658 0.4361772  0.436187   0.4361118
 0.43600902 0.43592283 0.4357471  0.4355653  0.43544716 0.43534073
 0.43525496 0.43523583 0.43515357 0.4350682  0.4351092  0.4351123
 0.43503454 0.43491805 0.4348086  0.43480778 0.4348818  0.43490785
 0.43497446 0.4350424  0.43502772 0.43505073 0.43500823 0.43486145
 0.43482816 0.43476215 0.43458456 0.43443304 0.4342822  0.43410164
 0.43400246 0.43390486 0.43380043 0.4337529  0.43368357 0.43357745
 0.43353823 0.43342698 0.433343   0.4333992  0.43342367 0.4335026
 0.43361378 0.43363258 0.43373382 0.433855   0.43375838 0.4336889
 0.43370712 0.43354034 0.43336138 0.43325832 0.43304613 0.4329213
 0.43290675 0.43276188 0.4327283  0.43279532 0.43271983 0.43272364
 0.43279338 0.43268216 0.43264782 0.4327029  0.4327374  0.43286735
 0.43294272 0.43299478 0.4331378  0.4331743  0.4330975  0.43310514
 0.43303162 0.4328426  0.43271002 0.4325255  0.4323509  0.43231437
 0.43220547 0.43210268 0.43213943 0.43208647 0.43204865 0.43218198
 0.4321462  0.4320492  0.4320975  0.43213764 0.4323076  0.43255675
 0.43270022 0.43288016 0.4330322  0.43301046 0.4330218  0.43300292
 0.4328494  0.43276617 0.43264806 0.4324016  0.43231043 0.43226254
 0.43211904 0.4321539  0.4321655  0.43206394 0.4321936  0.43226314
 0.43227112 0.43248683 0.432574   0.43271413 0.4330305  0.43326038
 0.43355572 0.43379128 0.43381488 0.43383428 0.43385231 0.43370226
 0.43357366 0.43352595 0.4333157  0.4331179  0.4330385  0.43288007
 0.43287647 0.43291804 0.43282267 0.43293163 0.4330505  0.4330266
 0.43317366 0.43315437 0.43300608 0.43310368 0.43308824 0.43287775
 0.43265578 0.4324096  0.43217582 0.43215153 0.43192482 0.43157518
 0.43141654 0.43107322 0.4307838  0.43072727 0.4304813  0.43041041
 0.43054602 0.43043414 0.43055022 0.43076244 0.43069616 0.4308726
 0.4309773  0.43067595 0.4306611  0.4307107  0.43054065 0.4306427
 0.430439   0.43004313 0.43013096 0.42987308 0.4294486  0.42949975
 0.42904517 0.42883077 0.42895493 0.42856205 0.42881447 0.42910004
 0.4290307  0.42957    0.4297399  0.4252747  0.42974758 0.42539072
 0.42442805 0.42887473 0.42336527 0.4276673  0.42302486 0.42546934]
