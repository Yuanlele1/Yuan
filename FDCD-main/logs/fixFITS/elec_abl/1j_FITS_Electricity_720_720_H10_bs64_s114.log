Args in experiment:
Namespace(H_order=10, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=320, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_720_j720_H10', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16973
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=320, out_features=640, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  8414822400.0
params:  205440.0
Trainable parameters:  205440
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 0.6589374
	speed: 1.0553s/iter; left time: 13825.4905s
Epoch: 1 cost time: 142.1416232585907
Epoch: 1, Steps: 132 | Train Loss: 0.8137545 Vali Loss: 0.4816649 Test Loss: 0.5806739
Validation loss decreased (inf --> 0.481665).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.4219459
	speed: 2.5020s/iter; left time: 32448.8232s
Epoch: 2 cost time: 149.3746178150177
Epoch: 2, Steps: 132 | Train Loss: 0.4926358 Vali Loss: 0.3275862 Test Loss: 0.4009639
Validation loss decreased (0.481665 --> 0.327586).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.3240111
	speed: 2.3156s/iter; left time: 29725.5139s
Epoch: 3 cost time: 140.8455080986023
Epoch: 3, Steps: 132 | Train Loss: 0.3580753 Vali Loss: 0.2480149 Test Loss: 0.3054498
Validation loss decreased (0.327586 --> 0.248015).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.2575290
	speed: 2.5862s/iter; left time: 32857.8534s
Epoch: 4 cost time: 153.691645860672
Epoch: 4, Steps: 132 | Train Loss: 0.2875190 Vali Loss: 0.2081973 Test Loss: 0.2555170
Validation loss decreased (0.248015 --> 0.208197).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.2538275
	speed: 2.5635s/iter; left time: 32230.6138s
Epoch: 5 cost time: 149.36447668075562
Epoch: 5, Steps: 132 | Train Loss: 0.2515650 Vali Loss: 0.1892601 Test Loss: 0.2301757
Validation loss decreased (0.208197 --> 0.189260).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.2306989
	speed: 2.4537s/iter; left time: 30526.7636s
Epoch: 6 cost time: 144.1780788898468
Epoch: 6, Steps: 132 | Train Loss: 0.2340171 Vali Loss: 0.1810928 Test Loss: 0.2178037
Validation loss decreased (0.189260 --> 0.181093).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.2292382
	speed: 2.4184s/iter; left time: 29768.6517s
Epoch: 7 cost time: 143.7129623889923
Epoch: 7, Steps: 132 | Train Loss: 0.2258182 Vali Loss: 0.1776637 Test Loss: 0.2118319
Validation loss decreased (0.181093 --> 0.177664).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.2192130
	speed: 2.3197s/iter; left time: 28246.6451s
Epoch: 8 cost time: 137.46038365364075
Epoch: 8, Steps: 132 | Train Loss: 0.2219206 Vali Loss: 0.1761249 Test Loss: 0.2090770
Validation loss decreased (0.177664 --> 0.176125).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2162636
	speed: 2.3125s/iter; left time: 27853.9040s
Epoch: 9 cost time: 136.72522020339966
Epoch: 9, Steps: 132 | Train Loss: 0.2201855 Vali Loss: 0.1759907 Test Loss: 0.2077698
Validation loss decreased (0.176125 --> 0.175991).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.2127883
	speed: 2.2659s/iter; left time: 26993.3063s
Epoch: 10 cost time: 135.46726489067078
Epoch: 10, Steps: 132 | Train Loss: 0.2193846 Vali Loss: 0.1755020 Test Loss: 0.2071066
Validation loss decreased (0.175991 --> 0.175502).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2181262
	speed: 2.2558s/iter; left time: 26575.9127s
Epoch: 11 cost time: 133.0665910243988
Epoch: 11, Steps: 132 | Train Loss: 0.2190297 Vali Loss: 0.1758899 Test Loss: 0.2066770
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2142608
	speed: 2.2831s/iter; left time: 26595.9982s
Epoch: 12 cost time: 141.20748949050903
Epoch: 12, Steps: 132 | Train Loss: 0.2186491 Vali Loss: 0.1754694 Test Loss: 0.2064670
Validation loss decreased (0.175502 --> 0.175469).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2150149
	speed: 2.2410s/iter; left time: 25809.4598s
Epoch: 13 cost time: 132.16251921653748
Epoch: 13, Steps: 132 | Train Loss: 0.2185347 Vali Loss: 0.1755703 Test Loss: 0.2063287
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2101608
	speed: 2.2551s/iter; left time: 25674.5358s
Epoch: 14 cost time: 140.42852878570557
Epoch: 14, Steps: 132 | Train Loss: 0.2184024 Vali Loss: 0.1755098 Test Loss: 0.2062106
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2238202
	speed: 2.3043s/iter; left time: 25930.2059s
Epoch: 15 cost time: 131.34649658203125
Epoch: 15, Steps: 132 | Train Loss: 0.2183422 Vali Loss: 0.1754784 Test Loss: 0.2061171
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2314316
	speed: 2.2861s/iter; left time: 25423.1899s
Epoch: 16 cost time: 136.32745718955994
Epoch: 16, Steps: 132 | Train Loss: 0.2182091 Vali Loss: 0.1752967 Test Loss: 0.2060418
Validation loss decreased (0.175469 --> 0.175297).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2222465
	speed: 2.2282s/iter; left time: 24485.5945s
Epoch: 17 cost time: 129.87116074562073
Epoch: 17, Steps: 132 | Train Loss: 0.2181435 Vali Loss: 0.1754942 Test Loss: 0.2060485
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2242414
	speed: 2.0938s/iter; left time: 22732.5873s
Epoch: 18 cost time: 122.61478614807129
Epoch: 18, Steps: 132 | Train Loss: 0.2180844 Vali Loss: 0.1755115 Test Loss: 0.2059958
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2131103
	speed: 2.0454s/iter; left time: 21936.5007s
Epoch: 19 cost time: 120.79192447662354
Epoch: 19, Steps: 132 | Train Loss: 0.2179313 Vali Loss: 0.1750812 Test Loss: 0.2059686
Validation loss decreased (0.175297 --> 0.175081).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2159420
	speed: 2.0862s/iter; left time: 22099.5041s
Epoch: 20 cost time: 123.34120225906372
Epoch: 20, Steps: 132 | Train Loss: 0.2179004 Vali Loss: 0.1753421 Test Loss: 0.2059273
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2198100
	speed: 2.0659s/iter; left time: 21610.8665s
Epoch: 21 cost time: 123.71962761878967
Epoch: 21, Steps: 132 | Train Loss: 0.2178287 Vali Loss: 0.1750812 Test Loss: 0.2059035
Validation loss decreased (0.175081 --> 0.175081).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2063030
	speed: 2.0315s/iter; left time: 20983.7669s
Epoch: 22 cost time: 121.04401850700378
Epoch: 22, Steps: 132 | Train Loss: 0.2178408 Vali Loss: 0.1752932 Test Loss: 0.2058972
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2231795
	speed: 2.1067s/iter; left time: 21482.2279s
Epoch: 23 cost time: 123.93695712089539
Epoch: 23, Steps: 132 | Train Loss: 0.2179112 Vali Loss: 0.1751250 Test Loss: 0.2059032
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2126078
	speed: 1.9793s/iter; left time: 19921.4759s
Epoch: 24 cost time: 118.7709755897522
Epoch: 24, Steps: 132 | Train Loss: 0.2178136 Vali Loss: 0.1751547 Test Loss: 0.2058488
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2278450
	speed: 2.4921s/iter; left time: 24754.2920s
Epoch: 25 cost time: 175.50543880462646
Epoch: 25, Steps: 132 | Train Loss: 0.2178493 Vali Loss: 0.1753040 Test Loss: 0.2058679
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2315290
	speed: 2.1035s/iter; left time: 20616.3316s
Epoch: 26 cost time: 121.23183965682983
Epoch: 26, Steps: 132 | Train Loss: 0.2177816 Vali Loss: 0.1749001 Test Loss: 0.2058717
Validation loss decreased (0.175081 --> 0.174900).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2219791
	speed: 2.0961s/iter; left time: 20267.1490s
Epoch: 27 cost time: 122.72668194770813
Epoch: 27, Steps: 132 | Train Loss: 0.2177472 Vali Loss: 0.1753002 Test Loss: 0.2058095
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2156884
	speed: 2.1287s/iter; left time: 20301.0358s
Epoch: 28 cost time: 126.72606468200684
Epoch: 28, Steps: 132 | Train Loss: 0.2176880 Vali Loss: 0.1751656 Test Loss: 0.2058141
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2213006
	speed: 2.1159s/iter; left time: 19900.3828s
Epoch: 29 cost time: 124.69574189186096
Epoch: 29, Steps: 132 | Train Loss: 0.2176942 Vali Loss: 0.1750145 Test Loss: 0.2057811
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2197079
	speed: 1.9396s/iter; left time: 17985.9576s
Epoch: 30 cost time: 120.86169791221619
Epoch: 30, Steps: 132 | Train Loss: 0.2176120 Vali Loss: 0.1749762 Test Loss: 0.2057977
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2190106
	speed: 2.0382s/iter; left time: 18631.3104s
Epoch: 31 cost time: 131.00364112854004
Epoch: 31, Steps: 132 | Train Loss: 0.2176558 Vali Loss: 0.1751281 Test Loss: 0.2057679
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2129652
	speed: 2.2302s/iter; left time: 20091.9461s
Epoch: 32 cost time: 130.73647046089172
Epoch: 32, Steps: 132 | Train Loss: 0.2176226 Vali Loss: 0.1747525 Test Loss: 0.2057772
Validation loss decreased (0.174900 --> 0.174752).  Saving model ...
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2170143
	speed: 1.9325s/iter; left time: 17155.1693s
Epoch: 33 cost time: 111.12896418571472
Epoch: 33, Steps: 132 | Train Loss: 0.2176769 Vali Loss: 0.1753490 Test Loss: 0.2057691
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2261249
	speed: 1.8421s/iter; left time: 16109.3256s
Epoch: 34 cost time: 109.98964476585388
Epoch: 34, Steps: 132 | Train Loss: 0.2176577 Vali Loss: 0.1751642 Test Loss: 0.2057482
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2280063
	speed: 2.0292s/iter; left time: 17477.9197s
Epoch: 35 cost time: 124.595374584198
Epoch: 35, Steps: 132 | Train Loss: 0.2175501 Vali Loss: 0.1748839 Test Loss: 0.2057479
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2076886
	speed: 1.8511s/iter; left time: 15698.9228s
Epoch: 36 cost time: 111.04790043830872
Epoch: 36, Steps: 132 | Train Loss: 0.2176402 Vali Loss: 0.1750676 Test Loss: 0.2057416
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2137058
	speed: 1.7980s/iter; left time: 15011.1749s
Epoch: 37 cost time: 110.31006741523743
Epoch: 37, Steps: 132 | Train Loss: 0.2175102 Vali Loss: 0.1749819 Test Loss: 0.2057454
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2246822
	speed: 1.8433s/iter; left time: 15145.9942s
Epoch: 38 cost time: 109.74229264259338
Epoch: 38, Steps: 132 | Train Loss: 0.2175536 Vali Loss: 0.1745901 Test Loss: 0.2057417
Validation loss decreased (0.174752 --> 0.174590).  Saving model ...
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2103685
	speed: 1.9041s/iter; left time: 15394.6300s
Epoch: 39 cost time: 111.61026406288147
Epoch: 39, Steps: 132 | Train Loss: 0.2174939 Vali Loss: 0.1751103 Test Loss: 0.2057198
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2109315
	speed: 1.8123s/iter; left time: 14413.3906s
Epoch: 40 cost time: 101.39750242233276
Epoch: 40, Steps: 132 | Train Loss: 0.2174454 Vali Loss: 0.1751084 Test Loss: 0.2057080
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2170068
	speed: 1.2428s/iter; left time: 9720.0490s
Epoch: 41 cost time: 75.63359880447388
Epoch: 41, Steps: 132 | Train Loss: 0.2175760 Vali Loss: 0.1751450 Test Loss: 0.2057061
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2192093
	speed: 1.2783s/iter; left time: 9829.0032s
Epoch: 42 cost time: 83.20196485519409
Epoch: 42, Steps: 132 | Train Loss: 0.2176163 Vali Loss: 0.1750861 Test Loss: 0.2056933
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2154690
	speed: 2.2873s/iter; left time: 17284.9586s
Epoch: 43 cost time: 162.82513403892517
Epoch: 43, Steps: 132 | Train Loss: 0.2175173 Vali Loss: 0.1749322 Test Loss: 0.2056989
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2109498
	speed: 2.5587s/iter; left time: 18998.2848s
Epoch: 44 cost time: 127.17627787590027
Epoch: 44, Steps: 132 | Train Loss: 0.2174909 Vali Loss: 0.1750290 Test Loss: 0.2057043
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2097021
	speed: 1.9341s/iter; left time: 14105.0881s
Epoch: 45 cost time: 114.97166991233826
Epoch: 45, Steps: 132 | Train Loss: 0.2174518 Vali Loss: 0.1747779 Test Loss: 0.2057046
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2184335
	speed: 1.9467s/iter; left time: 13940.1070s
Epoch: 46 cost time: 115.81414580345154
Epoch: 46, Steps: 132 | Train Loss: 0.2174509 Vali Loss: 0.1748007 Test Loss: 0.2056882
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2115132
	speed: 2.0583s/iter; left time: 14467.8296s
Epoch: 47 cost time: 131.7916021347046
Epoch: 47, Steps: 132 | Train Loss: 0.2174606 Vali Loss: 0.1750014 Test Loss: 0.2056998
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2094418
	speed: 1.9854s/iter; left time: 13693.1312s
Epoch: 48 cost time: 115.23558187484741
Epoch: 48, Steps: 132 | Train Loss: 0.2175532 Vali Loss: 0.1750128 Test Loss: 0.2056911
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2312212
	speed: 1.9768s/iter; left time: 13373.1518s
Epoch: 49 cost time: 120.0001163482666
Epoch: 49, Steps: 132 | Train Loss: 0.2174434 Vali Loss: 0.1748620 Test Loss: 0.2056837
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2228709
	speed: 1.9823s/iter; left time: 13148.4283s
Epoch: 50 cost time: 122.95226240158081
Epoch: 50, Steps: 132 | Train Loss: 0.2174980 Vali Loss: 0.1750132 Test Loss: 0.2057011
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2174204
	speed: 1.9313s/iter; left time: 12555.1291s
Epoch: 51 cost time: 115.16030979156494
Epoch: 51, Steps: 132 | Train Loss: 0.2174334 Vali Loss: 0.1749287 Test Loss: 0.2056713
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2209295
	speed: 1.8949s/iter; left time: 12068.4966s
Epoch: 52 cost time: 114.86735510826111
Epoch: 52, Steps: 132 | Train Loss: 0.2175226 Vali Loss: 0.1750449 Test Loss: 0.2056677
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2089390
	speed: 1.9538s/iter; left time: 12185.8383s
Epoch: 53 cost time: 117.96913599967957
Epoch: 53, Steps: 132 | Train Loss: 0.2175220 Vali Loss: 0.1748771 Test Loss: 0.2056676
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2135102
	speed: 2.5480s/iter; left time: 15555.5836s
Epoch: 54 cost time: 122.85385465621948
Epoch: 54, Steps: 132 | Train Loss: 0.2175096 Vali Loss: 0.1750339 Test Loss: 0.2056691
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2125955
	speed: 1.1813s/iter; left time: 7056.1604s
Epoch: 55 cost time: 72.44422698020935
Epoch: 55, Steps: 132 | Train Loss: 0.2175137 Vali Loss: 0.1748537 Test Loss: 0.2056730
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2236319
	speed: 1.2260s/iter; left time: 7161.3105s
Epoch: 56 cost time: 72.92619848251343
Epoch: 56, Steps: 132 | Train Loss: 0.2174589 Vali Loss: 0.1747944 Test Loss: 0.2056704
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2104155
	speed: 1.1748s/iter; left time: 6706.6666s
Epoch: 57 cost time: 72.98263716697693
Epoch: 57, Steps: 132 | Train Loss: 0.2173641 Vali Loss: 0.1749405 Test Loss: 0.2056586
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2199940
	speed: 1.1200s/iter; left time: 6246.1993s
Epoch: 58 cost time: 67.96587634086609
Epoch: 58, Steps: 132 | Train Loss: 0.2174823 Vali Loss: 0.1749023 Test Loss: 0.2056644
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : Electricity_720_j720_H10_FITS_custom_ftM_sl720_ll48_pl720_H10_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.20405422151088715, mae:0.2933899760246277, rse:0.45060932636260986, corr:[0.44767734 0.44715294 0.44840252 0.45024395 0.45055568 0.45127177
 0.45161876 0.45130545 0.4513     0.45076373 0.45040697 0.4503037
 0.44995302 0.45006156 0.449917   0.4500912  0.45010063 0.45012385
 0.4503087  0.45013714 0.45042685 0.45043102 0.45069966 0.45083448
 0.450984   0.45143306 0.4514216  0.45148712 0.45122778 0.4510332
 0.45088774 0.45053735 0.4503819  0.45024315 0.45009673 0.45015478
 0.45005447 0.45004076 0.45014232 0.45021465 0.4502898  0.45022964
 0.4502846  0.45015547 0.45018727 0.45025083 0.4502594  0.4504015
 0.4503644  0.45067358 0.45066428 0.4504951  0.45047128 0.45019755
 0.4500965  0.44996324 0.4498051  0.44983333 0.44980717 0.44980517
 0.44994816 0.44995683 0.44996879 0.45002514 0.4500413  0.45010746
 0.4501636  0.45012692 0.45001984 0.44996744 0.44991854 0.44994906
 0.44991902 0.44998422 0.45003468 0.44983363 0.44974    0.4495323
 0.44941744 0.44936785 0.44921988 0.44923735 0.44932804 0.44927502
 0.44921392 0.4493573  0.44939625 0.44941714 0.44943878 0.44940364
 0.44944438 0.44941568 0.44931662 0.44932985 0.44933903 0.44928554
 0.44932163 0.44933203 0.44940326 0.44927132 0.449094   0.44910666
 0.44896117 0.4488827  0.4488366  0.4487813  0.44877687 0.44879824
 0.44879422 0.4487734  0.4487941  0.44882408 0.44881698 0.4488198
 0.44882503 0.44870064 0.448688   0.4486716  0.4486796  0.44877505
 0.4488474  0.44897366 0.44897926 0.4489377  0.44889784 0.44878024
 0.44875473 0.44865885 0.44864443 0.44865307 0.44859084 0.44861656
 0.44866237 0.44871074 0.44881147 0.44879863 0.44880804 0.4488403
 0.44882354 0.4488437  0.4488045  0.4488045  0.44878012 0.4487486
 0.44868714 0.44860432 0.44869632 0.44865015 0.4486325  0.44862637
 0.44854176 0.4485211  0.44851214 0.44850203 0.4484612  0.4484342
 0.44842175 0.44849935 0.44856685 0.44856712 0.4486008  0.44860104
 0.4486422  0.4486455  0.44855353 0.44845122 0.44838005 0.44817102
 0.44796187 0.44796446 0.44803783 0.44798145 0.44786155 0.44782242
 0.4477439  0.44765872 0.447593   0.44756106 0.44754887 0.4474319
 0.44744402 0.4475578  0.44752905 0.44752383 0.44747698 0.44741946
 0.44737026 0.44715694 0.44698933 0.4469328  0.44691062 0.44682536
 0.4467399  0.44688225 0.44694808 0.44689628 0.4468663  0.44675857
 0.44668922 0.44664717 0.4465331  0.44650677 0.4464406  0.446406
 0.44640002 0.4463617  0.44643795 0.44641986 0.4463928  0.4463877
 0.4463097  0.4461238  0.4459552  0.44595358 0.44593874 0.44583648
 0.4458249  0.44598278 0.44611007 0.44613293 0.44611263 0.44609836
 0.4460838  0.44604698 0.44603103 0.44594255 0.44590816 0.44588602
 0.4458668  0.44582927 0.4457879  0.44579569 0.44574744 0.44574454
 0.44568765 0.44550455 0.44538736 0.44529387 0.4452819  0.44520453
 0.44520244 0.44536978 0.44547796 0.4454606  0.44537795 0.44532192
 0.44530737 0.44527262 0.44528568 0.4452801  0.4451847  0.44517222
 0.4451555  0.44512886 0.44515887 0.44512844 0.445044   0.44504103
 0.44500777 0.4448967  0.4448725  0.4448099  0.44480938 0.44479975
 0.4448036  0.4449508  0.44497302 0.4449504  0.44494337 0.4449173
 0.4448903  0.44486034 0.44481036 0.44477537 0.44476795 0.44471964
 0.4446763  0.44469047 0.44471768 0.44468024 0.44469914 0.44467142
 0.4445866  0.44449523 0.4444159  0.44444105 0.4444586  0.44450477
 0.44470993 0.44481757 0.44483855 0.44485384 0.44487974 0.4448484
 0.44486427 0.44485888 0.44477874 0.4447431  0.4447027  0.44456068
 0.44457552 0.4446407  0.44463283 0.4446326  0.44450846 0.444498
 0.44453964 0.4445091  0.44452313 0.44451118 0.4444907  0.44445372
 0.4444316  0.44451323 0.44463888 0.44462276 0.4446241  0.4445914
 0.44452754 0.44459105 0.44456255 0.44443873 0.44438145 0.44435298
 0.4443022  0.44432792 0.4443563  0.44433758 0.44434458 0.44436446
 0.44433826 0.44419262 0.44410604 0.444024   0.4439565  0.4438065
 0.44351116 0.44353193 0.44362667 0.44356558 0.44344687 0.44337028
 0.44332656 0.44327366 0.44312674 0.4430121  0.44294703 0.4428843
 0.442851   0.44284546 0.4428288  0.44278193 0.44279873 0.44280502
 0.44274747 0.4425992  0.4425079  0.44250888 0.44250247 0.44247934
 0.44233638 0.4424173  0.44252577 0.44248712 0.44248843 0.4424378
 0.44237503 0.44229597 0.44219851 0.44215685 0.44206375 0.44194752
 0.44193512 0.44188118 0.44188362 0.44189405 0.4418041  0.44180462
 0.44174388 0.44155768 0.44142702 0.44142464 0.44144398 0.44136322
 0.44142517 0.44152695 0.4416232  0.4416801  0.44169974 0.44170347
 0.44162712 0.44163424 0.44159847 0.44149378 0.44140613 0.44130483
 0.44124496 0.44127145 0.4411961  0.4411826  0.44118094 0.44110644
 0.44112945 0.44098765 0.44087672 0.44077533 0.4407484  0.4408754
 0.44082084 0.44091547 0.44109216 0.44112808 0.4411102  0.44115442
 0.44117522 0.4411149  0.44101432 0.44096777 0.4409251  0.44084883
 0.4408132  0.4407695  0.44084606 0.4408209  0.44073105 0.44073352
 0.44069618 0.44054857 0.44046733 0.44054282 0.44056076 0.44059595
 0.44062674 0.44073707 0.44092155 0.44090196 0.44094583 0.4409491
 0.44087642 0.44087547 0.4407966  0.4406808  0.44062212 0.44052786
 0.4404906  0.4404891  0.44048724 0.44052473 0.44043925 0.44045684
 0.44039595 0.44021076 0.44015253 0.44012928 0.4402233  0.44029728
 0.44055402 0.4406876  0.4407562  0.44086486 0.44083065 0.440863
 0.4408713  0.44080612 0.44077718 0.4406354  0.44051614 0.44044787
 0.4403484  0.44038856 0.44039685 0.4403292  0.4403387  0.4403249
 0.4403419  0.44032398 0.44038397 0.4404469  0.44045985 0.44050774
 0.44043723 0.44056857 0.4407117  0.44070876 0.44079277 0.4408115
 0.44078517 0.44079033 0.44071954 0.4406207  0.44055125 0.44050327
 0.44041142 0.44038153 0.44046435 0.44044796 0.44040164 0.44036606
 0.440379   0.44031858 0.44018555 0.44016027 0.44014084 0.44003323
 0.43978095 0.4397681  0.4398842  0.4398237  0.4397804  0.4397566
 0.4396833  0.43963858 0.43944675 0.4392732  0.43914786 0.4390297
 0.43904564 0.43896624 0.43894947 0.43899763 0.43881798 0.43880153
 0.43876556 0.43856257 0.43851885 0.43845198 0.43848166 0.43852496
 0.43840116 0.4384409  0.43858358 0.43857235 0.43853807 0.43858343
 0.43849322 0.43838942 0.43827266 0.4380763  0.43795374 0.43778354
 0.4377517  0.43779445 0.4376333  0.43761542 0.43753463 0.43746212
 0.43750668 0.4372779  0.4372507  0.43733555 0.4373405  0.43738803
 0.43736827 0.43748954 0.4376034  0.43758202 0.4375745  0.43747526
 0.4374195  0.43740848 0.43723843 0.43705457 0.43687725 0.43671623
 0.4365528  0.43649685 0.43649885 0.4363348  0.43622836 0.43608874
 0.4360281  0.43595225 0.43587193 0.43591034 0.43592554 0.4360473
 0.43603978 0.4361314  0.43635476 0.43635076 0.4363577  0.43631393
 0.43624282 0.4361559  0.43594322 0.43585047 0.43568432 0.4354582
 0.43549824 0.43536612 0.43529308 0.43531302 0.43521813 0.43531024
 0.43519706 0.43506125 0.43510482 0.43515533 0.43525994 0.43526787
 0.4354114  0.43556747 0.43567494 0.43567058 0.4356236  0.43563092
 0.43551895 0.43541613 0.4352675  0.43506193 0.43502527 0.43483087
 0.4346728  0.4347774  0.4346754  0.43473902 0.4347197  0.43463993
 0.4347391  0.43460953 0.43465728 0.43466234 0.43475142 0.43492842
 0.43515995 0.43539852 0.4354335  0.43549204 0.4354511  0.43532386
 0.4352947  0.43512717 0.43496218 0.43479457 0.4345438  0.43445498
 0.43435463 0.4343263  0.43436468 0.4342994  0.43442962 0.4344114
 0.43450063 0.43453902 0.4345257  0.4347329  0.43466923 0.43482113
 0.43484473 0.43488407 0.43510768 0.43504253 0.43498358 0.43485427
 0.4347406  0.43461558 0.43439114 0.43429655 0.43408996 0.43394843
 0.43399432 0.4339064  0.4340059  0.43403655 0.43408749 0.43427488
 0.4342789  0.43434134 0.4342624  0.4342874  0.43441296 0.43429756
 0.43409178 0.4339644  0.4340231  0.43384582 0.4336065  0.43348083
 0.43308467 0.43294582 0.4325742  0.43227816 0.43226343 0.4319588
 0.43207008 0.43213454 0.4322027  0.43239284 0.43233386 0.43261066
 0.43264252 0.43250704 0.43252733 0.43240887 0.43270335 0.43251812
 0.43225333 0.4322465  0.43199036 0.43198806 0.43147856 0.43143386
 0.4311829  0.43088016 0.43078184 0.430457   0.43092477 0.43077427
 0.4310784  0.43146995 0.43148226 0.4319565  0.43163824 0.43153918
 0.43133652 0.43018532 0.42838603 0.42928877 0.4246858  0.4277322 ]
