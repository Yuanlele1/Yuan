Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20290816.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9861862659454346
Epoch: 1, Steps: 60 | Train Loss: 0.6734298 Vali Loss: 1.2617162 Test Loss: 0.6033155
Validation loss decreased (inf --> 1.261716).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.0215017795562744
Epoch: 2, Steps: 60 | Train Loss: 0.4966861 Vali Loss: 1.1084697 Test Loss: 0.4966564
Validation loss decreased (1.261716 --> 1.108470).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0284931659698486
Epoch: 3, Steps: 60 | Train Loss: 0.4437891 Vali Loss: 1.0418119 Test Loss: 0.4527515
Validation loss decreased (1.108470 --> 1.041812).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9078919887542725
Epoch: 4, Steps: 60 | Train Loss: 0.4196114 Vali Loss: 1.0090718 Test Loss: 0.4332653
Validation loss decreased (1.041812 --> 1.009072).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.2306292057037354
Epoch: 5, Steps: 60 | Train Loss: 0.4073554 Vali Loss: 0.9922374 Test Loss: 0.4249711
Validation loss decreased (1.009072 --> 0.992237).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.137298583984375
Epoch: 6, Steps: 60 | Train Loss: 0.4010972 Vali Loss: 0.9825665 Test Loss: 0.4215073
Validation loss decreased (0.992237 --> 0.982567).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.036609172821045
Epoch: 7, Steps: 60 | Train Loss: 0.3969151 Vali Loss: 0.9788165 Test Loss: 0.4204703
Validation loss decreased (0.982567 --> 0.978817).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.988046407699585
Epoch: 8, Steps: 60 | Train Loss: 0.3942146 Vali Loss: 0.9752460 Test Loss: 0.4202857
Validation loss decreased (0.978817 --> 0.975246).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.9572701454162598
Epoch: 9, Steps: 60 | Train Loss: 0.3925142 Vali Loss: 0.9733477 Test Loss: 0.4203229
Validation loss decreased (0.975246 --> 0.973348).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.1415436267852783
Epoch: 10, Steps: 60 | Train Loss: 0.3916086 Vali Loss: 0.9721976 Test Loss: 0.4202271
Validation loss decreased (0.973348 --> 0.972198).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.059030532836914
Epoch: 11, Steps: 60 | Train Loss: 0.3906205 Vali Loss: 0.9713603 Test Loss: 0.4202922
Validation loss decreased (0.972198 --> 0.971360).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9616491794586182
Epoch: 12, Steps: 60 | Train Loss: 0.3898615 Vali Loss: 0.9699330 Test Loss: 0.4206758
Validation loss decreased (0.971360 --> 0.969933).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9603254795074463
Epoch: 13, Steps: 60 | Train Loss: 0.3890036 Vali Loss: 0.9694897 Test Loss: 0.4203894
Validation loss decreased (0.969933 --> 0.969490).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.073622465133667
Epoch: 14, Steps: 60 | Train Loss: 0.3885249 Vali Loss: 0.9697640 Test Loss: 0.4206954
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.0723211765289307
Epoch: 15, Steps: 60 | Train Loss: 0.3877465 Vali Loss: 0.9698510 Test Loss: 0.4205955
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7687759399414062
Epoch: 16, Steps: 60 | Train Loss: 0.3870440 Vali Loss: 0.9683975 Test Loss: 0.4208271
Validation loss decreased (0.969490 --> 0.968397).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.9279253482818604
Epoch: 17, Steps: 60 | Train Loss: 0.3866998 Vali Loss: 0.9683545 Test Loss: 0.4208604
Validation loss decreased (0.968397 --> 0.968355).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.071823835372925
Epoch: 18, Steps: 60 | Train Loss: 0.3863004 Vali Loss: 0.9677371 Test Loss: 0.4209073
Validation loss decreased (0.968355 --> 0.967737).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.146402597427368
Epoch: 19, Steps: 60 | Train Loss: 0.3861211 Vali Loss: 0.9678200 Test Loss: 0.4208314
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.0397233963012695
Epoch: 20, Steps: 60 | Train Loss: 0.3857464 Vali Loss: 0.9680539 Test Loss: 0.4209970
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.072331428527832
Epoch: 21, Steps: 60 | Train Loss: 0.3854976 Vali Loss: 0.9679763 Test Loss: 0.4210305
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.7094030380249023
Epoch: 22, Steps: 60 | Train Loss: 0.3851054 Vali Loss: 0.9672536 Test Loss: 0.4211235
Validation loss decreased (0.967737 --> 0.967254).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9694888591766357
Epoch: 23, Steps: 60 | Train Loss: 0.3849478 Vali Loss: 0.9672363 Test Loss: 0.4211022
Validation loss decreased (0.967254 --> 0.967236).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8936450481414795
Epoch: 24, Steps: 60 | Train Loss: 0.3848242 Vali Loss: 0.9674015 Test Loss: 0.4210465
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9743142127990723
Epoch: 25, Steps: 60 | Train Loss: 0.3843032 Vali Loss: 0.9674492 Test Loss: 0.4212388
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.9406685829162598
Epoch: 26, Steps: 60 | Train Loss: 0.3845121 Vali Loss: 0.9675915 Test Loss: 0.4214166
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.07690691947937
Epoch: 27, Steps: 60 | Train Loss: 0.3841860 Vali Loss: 0.9673781 Test Loss: 0.4212831
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.0179355144500732
Epoch: 28, Steps: 60 | Train Loss: 0.3843577 Vali Loss: 0.9671896 Test Loss: 0.4213037
Validation loss decreased (0.967236 --> 0.967190).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.1212637424468994
Epoch: 29, Steps: 60 | Train Loss: 0.3840610 Vali Loss: 0.9669777 Test Loss: 0.4212466
Validation loss decreased (0.967190 --> 0.966978).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.900378942489624
Epoch: 30, Steps: 60 | Train Loss: 0.3837283 Vali Loss: 0.9672841 Test Loss: 0.4214328
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.9271113872528076
Epoch: 31, Steps: 60 | Train Loss: 0.3836254 Vali Loss: 0.9669728 Test Loss: 0.4214168
Validation loss decreased (0.966978 --> 0.966973).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.0135486125946045
Epoch: 32, Steps: 60 | Train Loss: 0.3835886 Vali Loss: 0.9660348 Test Loss: 0.4213560
Validation loss decreased (0.966973 --> 0.966035).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.050701379776001
Epoch: 33, Steps: 60 | Train Loss: 0.3836391 Vali Loss: 0.9661340 Test Loss: 0.4213862
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.06557035446167
Epoch: 34, Steps: 60 | Train Loss: 0.3837442 Vali Loss: 0.9660981 Test Loss: 0.4212844
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9508826732635498
Epoch: 35, Steps: 60 | Train Loss: 0.3835750 Vali Loss: 0.9666594 Test Loss: 0.4212674
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.130795478820801
Epoch: 36, Steps: 60 | Train Loss: 0.3832377 Vali Loss: 0.9659715 Test Loss: 0.4214299
Validation loss decreased (0.966035 --> 0.965972).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9543988704681396
Epoch: 37, Steps: 60 | Train Loss: 0.3828282 Vali Loss: 0.9661227 Test Loss: 0.4213301
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.157639265060425
Epoch: 38, Steps: 60 | Train Loss: 0.3830242 Vali Loss: 0.9663176 Test Loss: 0.4213223
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.0012896060943604
Epoch: 39, Steps: 60 | Train Loss: 0.3829163 Vali Loss: 0.9662846 Test Loss: 0.4214211
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.003131866455078
Epoch: 40, Steps: 60 | Train Loss: 0.3833984 Vali Loss: 0.9662148 Test Loss: 0.4214352
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.0057852268218994
Epoch: 41, Steps: 60 | Train Loss: 0.3830046 Vali Loss: 0.9657650 Test Loss: 0.4214888
Validation loss decreased (0.965972 --> 0.965765).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.8970091342926025
Epoch: 42, Steps: 60 | Train Loss: 0.3827027 Vali Loss: 0.9662632 Test Loss: 0.4215190
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.943967580795288
Epoch: 43, Steps: 60 | Train Loss: 0.3830518 Vali Loss: 0.9662106 Test Loss: 0.4215019
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.0262632369995117
Epoch: 44, Steps: 60 | Train Loss: 0.3828759 Vali Loss: 0.9662524 Test Loss: 0.4215112
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.0324699878692627
Epoch: 45, Steps: 60 | Train Loss: 0.3827884 Vali Loss: 0.9661630 Test Loss: 0.4215041
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.0587360858917236
Epoch: 46, Steps: 60 | Train Loss: 0.3824262 Vali Loss: 0.9658133 Test Loss: 0.4215989
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.002326726913452
Epoch: 47, Steps: 60 | Train Loss: 0.3826615 Vali Loss: 0.9663138 Test Loss: 0.4215975
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9204535484313965
Epoch: 48, Steps: 60 | Train Loss: 0.3823827 Vali Loss: 0.9660421 Test Loss: 0.4215448
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9921977519989014
Epoch: 49, Steps: 60 | Train Loss: 0.3826572 Vali Loss: 0.9662722 Test Loss: 0.4216313
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.053908586502075
Epoch: 50, Steps: 60 | Train Loss: 0.3826005 Vali Loss: 0.9656278 Test Loss: 0.4216162
Validation loss decreased (0.965765 --> 0.965628).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9253201484680176
Epoch: 51, Steps: 60 | Train Loss: 0.3821752 Vali Loss: 0.9658710 Test Loss: 0.4216844
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.935971975326538
Epoch: 52, Steps: 60 | Train Loss: 0.3825069 Vali Loss: 0.9657754 Test Loss: 0.4216337
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.0164530277252197
Epoch: 53, Steps: 60 | Train Loss: 0.3825636 Vali Loss: 0.9661571 Test Loss: 0.4216171
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9072813987731934
Epoch: 54, Steps: 60 | Train Loss: 0.3822539 Vali Loss: 0.9654266 Test Loss: 0.4216558
Validation loss decreased (0.965628 --> 0.965427).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.8559296131134033
Epoch: 55, Steps: 60 | Train Loss: 0.3822348 Vali Loss: 0.9656239 Test Loss: 0.4216550
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.007631778717041
Epoch: 56, Steps: 60 | Train Loss: 0.3821823 Vali Loss: 0.9661531 Test Loss: 0.4216539
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.9145689010620117
Epoch: 57, Steps: 60 | Train Loss: 0.3820841 Vali Loss: 0.9657582 Test Loss: 0.4216598
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.003753662109375
Epoch: 58, Steps: 60 | Train Loss: 0.3822066 Vali Loss: 0.9660128 Test Loss: 0.4216797
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.160069227218628
Epoch: 59, Steps: 60 | Train Loss: 0.3816573 Vali Loss: 0.9659680 Test Loss: 0.4216852
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9871861934661865
Epoch: 60, Steps: 60 | Train Loss: 0.3820790 Vali Loss: 0.9660551 Test Loss: 0.4216425
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.054596185684204
Epoch: 61, Steps: 60 | Train Loss: 0.3822753 Vali Loss: 0.9661081 Test Loss: 0.4216898
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.9330909252166748
Epoch: 62, Steps: 60 | Train Loss: 0.3824545 Vali Loss: 0.9658583 Test Loss: 0.4216923
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.8811707496643066
Epoch: 63, Steps: 60 | Train Loss: 0.3823630 Vali Loss: 0.9662040 Test Loss: 0.4217072
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.0197300910949707
Epoch: 64, Steps: 60 | Train Loss: 0.3820845 Vali Loss: 0.9654346 Test Loss: 0.4216909
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.9517450332641602
Epoch: 65, Steps: 60 | Train Loss: 0.3820087 Vali Loss: 0.9659675 Test Loss: 0.4217043
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.0266032218933105
Epoch: 66, Steps: 60 | Train Loss: 0.3823921 Vali Loss: 0.9656412 Test Loss: 0.4216976
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.9765751361846924
Epoch: 67, Steps: 60 | Train Loss: 0.3820764 Vali Loss: 0.9654348 Test Loss: 0.4216973
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.059173822402954
Epoch: 68, Steps: 60 | Train Loss: 0.3823035 Vali Loss: 0.9655455 Test Loss: 0.4217222
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.1656689643859863
Epoch: 69, Steps: 60 | Train Loss: 0.3820965 Vali Loss: 0.9660929 Test Loss: 0.4217241
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9086666107177734
Epoch: 70, Steps: 60 | Train Loss: 0.3820374 Vali Loss: 0.9656309 Test Loss: 0.4217197
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.1532175540924072
Epoch: 71, Steps: 60 | Train Loss: 0.3824059 Vali Loss: 0.9657964 Test Loss: 0.4217372
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.1241040229797363
Epoch: 72, Steps: 60 | Train Loss: 0.3824696 Vali Loss: 0.9659541 Test Loss: 0.4217295
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.9737846851348877
Epoch: 73, Steps: 60 | Train Loss: 0.3820592 Vali Loss: 0.9656224 Test Loss: 0.4217068
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.060659646987915
Epoch: 74, Steps: 60 | Train Loss: 0.3823232 Vali Loss: 0.9659342 Test Loss: 0.4217136
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41793301701545715, mae:0.4258669316768646, rse:0.6139184832572937, corr:[0.2574452  0.26823318 0.26944613 0.26651937 0.2641978  0.26310274
 0.262294   0.261513   0.260819   0.26072672 0.26108292 0.2611917
 0.2609581  0.26065928 0.26060638 0.26086217 0.26098803 0.26088312
 0.26050648 0.26002687 0.2597763  0.259851   0.25980112 0.2597453
 0.25970608 0.2597305  0.2596521  0.25928727 0.25878772 0.25833783
 0.25805432 0.25781938 0.2576827  0.25761703 0.25745758 0.2573557
 0.25741163 0.25761926 0.2578157  0.25794703 0.25811824 0.25825155
 0.258261   0.25829577 0.25850412 0.25876972 0.2589915  0.2589347
 0.2582193  0.2572748  0.25626773 0.2552859  0.25430706 0.25317186
 0.25221264 0.25172696 0.25160688 0.25173017 0.25171825 0.25154257
 0.25127506 0.25104177 0.25094154 0.25108236 0.2512856  0.25143307
 0.25155    0.25150475 0.2514451  0.25142527 0.25136256 0.25106034
 0.25056762 0.24991895 0.24917637 0.24859506 0.24819578 0.24788252
 0.24761371 0.24734831 0.24702685 0.24667558 0.24635813 0.24617885
 0.24621487 0.24625945 0.24614671 0.24587812 0.24560215 0.24550949
 0.24552608 0.24557127 0.24558893 0.24561489 0.24577312 0.24610138
 0.24650344 0.24676679 0.2467635  0.24664979 0.24646756 0.24615751
 0.24585097 0.2456757  0.24565896 0.24574992 0.24564233 0.24521223
 0.24477626 0.24456476 0.24474399 0.24523458 0.24553244 0.2454565
 0.2452266  0.24503383 0.24499445 0.24506536 0.24506027 0.24480194
 0.2442754  0.2434034  0.24226332 0.24118268 0.24038875 0.23987518
 0.23969957 0.2396808  0.23943715 0.23907498 0.23869948 0.23845261
 0.23833467 0.23825581 0.23810868 0.23790602 0.23794137 0.23822784
 0.23854613 0.2385297  0.2382257  0.23778671 0.23737644 0.2369539
 0.23640248 0.23553008 0.23438333 0.23322183 0.23250073 0.23205662
 0.23166198 0.23126374 0.23102014 0.23111908 0.2312372  0.23108034
 0.23053789 0.22992767 0.22970903 0.22991276 0.2300822  0.23001464
 0.22950286 0.2289506  0.22895172 0.22936784 0.22954288 0.22881655
 0.22765902 0.2270057  0.2270334  0.22721937 0.22672807 0.22555913
 0.2243983  0.22406818 0.22425744 0.22444986 0.22402872 0.22334534
 0.22307907 0.2232039  0.22279666 0.22160414 0.2207958  0.22166626
 0.22347364 0.22330427 0.22038816 0.21834032 0.22305095 0.23115367]
