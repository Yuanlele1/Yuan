Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=26, out_features=74, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1723904.0
params:  1998.0
Trainable parameters:  1998
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.2949917316436768
Epoch: 1, Steps: 63 | Train Loss: 0.9187018 Vali Loss: 1.9156327 Test Loss: 0.9151871
Validation loss decreased (inf --> 1.915633).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.2015838623046875
Epoch: 2, Steps: 63 | Train Loss: 0.7099132 Vali Loss: 1.6160634 Test Loss: 0.7072577
Validation loss decreased (1.915633 --> 1.616063).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.1910173892974854
Epoch: 3, Steps: 63 | Train Loss: 0.6106943 Vali Loss: 1.4671950 Test Loss: 0.6052049
Validation loss decreased (1.616063 --> 1.467195).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3121709823608398
Epoch: 4, Steps: 63 | Train Loss: 0.5595225 Vali Loss: 1.3922406 Test Loss: 0.5506289
Validation loss decreased (1.467195 --> 1.392241).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.1222326755523682
Epoch: 5, Steps: 63 | Train Loss: 0.5311863 Vali Loss: 1.3449106 Test Loss: 0.5198244
Validation loss decreased (1.392241 --> 1.344911).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.1241202354431152
Epoch: 6, Steps: 63 | Train Loss: 0.5151894 Vali Loss: 1.3218858 Test Loss: 0.5015798
Validation loss decreased (1.344911 --> 1.321886).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.1823863983154297
Epoch: 7, Steps: 63 | Train Loss: 0.5046441 Vali Loss: 1.3003980 Test Loss: 0.4900062
Validation loss decreased (1.321886 --> 1.300398).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.308178424835205
Epoch: 8, Steps: 63 | Train Loss: 0.4982730 Vali Loss: 1.2940975 Test Loss: 0.4822531
Validation loss decreased (1.300398 --> 1.294098).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.123013973236084
Epoch: 9, Steps: 63 | Train Loss: 0.4932519 Vali Loss: 1.2780327 Test Loss: 0.4767358
Validation loss decreased (1.294098 --> 1.278033).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.2098166942596436
Epoch: 10, Steps: 63 | Train Loss: 0.4897052 Vali Loss: 1.2726095 Test Loss: 0.4726706
Validation loss decreased (1.278033 --> 1.272609).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.1341135501861572
Epoch: 11, Steps: 63 | Train Loss: 0.4865697 Vali Loss: 1.2723299 Test Loss: 0.4696608
Validation loss decreased (1.272609 --> 1.272330).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.210994005203247
Epoch: 12, Steps: 63 | Train Loss: 0.4847308 Vali Loss: 1.2601569 Test Loss: 0.4672500
Validation loss decreased (1.272330 --> 1.260157).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.2654759883880615
Epoch: 13, Steps: 63 | Train Loss: 0.4830207 Vali Loss: 1.2620555 Test Loss: 0.4653507
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.1992640495300293
Epoch: 14, Steps: 63 | Train Loss: 0.4817614 Vali Loss: 1.2610769 Test Loss: 0.4638320
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1492507457733154
Epoch: 15, Steps: 63 | Train Loss: 0.4806991 Vali Loss: 1.2542347 Test Loss: 0.4626760
Validation loss decreased (1.260157 --> 1.254235).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.2217442989349365
Epoch: 16, Steps: 63 | Train Loss: 0.4796068 Vali Loss: 1.2454963 Test Loss: 0.4617102
Validation loss decreased (1.254235 --> 1.245496).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.224478006362915
Epoch: 17, Steps: 63 | Train Loss: 0.4789091 Vali Loss: 1.2506738 Test Loss: 0.4609852
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.1449270248413086
Epoch: 18, Steps: 63 | Train Loss: 0.4782232 Vali Loss: 1.2497984 Test Loss: 0.4603527
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.1617851257324219
Epoch: 19, Steps: 63 | Train Loss: 0.4775004 Vali Loss: 1.2482148 Test Loss: 0.4598958
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.176252841949463
Epoch: 20, Steps: 63 | Train Loss: 0.4769220 Vali Loss: 1.2421314 Test Loss: 0.4595004
Validation loss decreased (1.245496 --> 1.242131).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.1762595176696777
Epoch: 21, Steps: 63 | Train Loss: 0.4767655 Vali Loss: 1.2501998 Test Loss: 0.4591705
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.1346471309661865
Epoch: 22, Steps: 63 | Train Loss: 0.4764306 Vali Loss: 1.2433845 Test Loss: 0.4589984
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.1653437614440918
Epoch: 23, Steps: 63 | Train Loss: 0.4761430 Vali Loss: 1.2507683 Test Loss: 0.4587715
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.1995370388031006
Epoch: 24, Steps: 63 | Train Loss: 0.4756319 Vali Loss: 1.2483606 Test Loss: 0.4586425
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.1200697422027588
Epoch: 25, Steps: 63 | Train Loss: 0.4754293 Vali Loss: 1.2475120 Test Loss: 0.4585465
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2314722537994385
Epoch: 26, Steps: 63 | Train Loss: 0.4750939 Vali Loss: 1.2435156 Test Loss: 0.4584555
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.1587040424346924
Epoch: 27, Steps: 63 | Train Loss: 0.4755023 Vali Loss: 1.2444756 Test Loss: 0.4584154
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.111217975616455
Epoch: 28, Steps: 63 | Train Loss: 0.4747205 Vali Loss: 1.2399914 Test Loss: 0.4583546
Validation loss decreased (1.242131 --> 1.239991).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.1867244243621826
Epoch: 29, Steps: 63 | Train Loss: 0.4745552 Vali Loss: 1.2436090 Test Loss: 0.4583476
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.1993956565856934
Epoch: 30, Steps: 63 | Train Loss: 0.4742877 Vali Loss: 1.2492285 Test Loss: 0.4583557
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.213613748550415
Epoch: 31, Steps: 63 | Train Loss: 0.4742084 Vali Loss: 1.2432915 Test Loss: 0.4583322
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.2916710376739502
Epoch: 32, Steps: 63 | Train Loss: 0.4744480 Vali Loss: 1.2426230 Test Loss: 0.4583502
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.15647554397583
Epoch: 33, Steps: 63 | Train Loss: 0.4742132 Vali Loss: 1.2430054 Test Loss: 0.4583569
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.2185401916503906
Epoch: 34, Steps: 63 | Train Loss: 0.4736063 Vali Loss: 1.2412446 Test Loss: 0.4583808
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.153806209564209
Epoch: 35, Steps: 63 | Train Loss: 0.4736175 Vali Loss: 1.2411242 Test Loss: 0.4583921
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.2063159942626953
Epoch: 36, Steps: 63 | Train Loss: 0.4737371 Vali Loss: 1.2389036 Test Loss: 0.4584094
Validation loss decreased (1.239991 --> 1.238904).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.1708154678344727
Epoch: 37, Steps: 63 | Train Loss: 0.4737483 Vali Loss: 1.2404819 Test Loss: 0.4584439
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.3134644031524658
Epoch: 38, Steps: 63 | Train Loss: 0.4738324 Vali Loss: 1.2407013 Test Loss: 0.4584725
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.2225801944732666
Epoch: 39, Steps: 63 | Train Loss: 0.4735149 Vali Loss: 1.2400268 Test Loss: 0.4584948
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.155212640762329
Epoch: 40, Steps: 63 | Train Loss: 0.4734466 Vali Loss: 1.2411320 Test Loss: 0.4585425
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.2160727977752686
Epoch: 41, Steps: 63 | Train Loss: 0.4732910 Vali Loss: 1.2421111 Test Loss: 0.4585662
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.2211861610412598
Epoch: 42, Steps: 63 | Train Loss: 0.4735455 Vali Loss: 1.2352163 Test Loss: 0.4585938
Validation loss decreased (1.238904 --> 1.235216).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.0855727195739746
Epoch: 43, Steps: 63 | Train Loss: 0.4729382 Vali Loss: 1.2375134 Test Loss: 0.4586120
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.1922855377197266
Epoch: 44, Steps: 63 | Train Loss: 0.4735445 Vali Loss: 1.2350067 Test Loss: 0.4586391
Validation loss decreased (1.235216 --> 1.235007).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.2093863487243652
Epoch: 45, Steps: 63 | Train Loss: 0.4731102 Vali Loss: 1.2430372 Test Loss: 0.4586697
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.1838152408599854
Epoch: 46, Steps: 63 | Train Loss: 0.4731910 Vali Loss: 1.2380735 Test Loss: 0.4587103
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.1513290405273438
Epoch: 47, Steps: 63 | Train Loss: 0.4731772 Vali Loss: 1.2406522 Test Loss: 0.4587204
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.1161785125732422
Epoch: 48, Steps: 63 | Train Loss: 0.4732918 Vali Loss: 1.2400209 Test Loss: 0.4587592
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.187755823135376
Epoch: 49, Steps: 63 | Train Loss: 0.4729500 Vali Loss: 1.2395169 Test Loss: 0.4587995
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2141997814178467
Epoch: 50, Steps: 63 | Train Loss: 0.4731964 Vali Loss: 1.2340010 Test Loss: 0.4588228
Validation loss decreased (1.235007 --> 1.234001).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.135972261428833
Epoch: 51, Steps: 63 | Train Loss: 0.4732741 Vali Loss: 1.2411069 Test Loss: 0.4588385
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.1702117919921875
Epoch: 52, Steps: 63 | Train Loss: 0.4731110 Vali Loss: 1.2398860 Test Loss: 0.4588717
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.1662743091583252
Epoch: 53, Steps: 63 | Train Loss: 0.4733600 Vali Loss: 1.2390286 Test Loss: 0.4588985
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.1737101078033447
Epoch: 54, Steps: 63 | Train Loss: 0.4732120 Vali Loss: 1.2381318 Test Loss: 0.4589225
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.1774401664733887
Epoch: 55, Steps: 63 | Train Loss: 0.4731835 Vali Loss: 1.2410913 Test Loss: 0.4589470
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.166196346282959
Epoch: 56, Steps: 63 | Train Loss: 0.4730331 Vali Loss: 1.2356032 Test Loss: 0.4589582
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.2466034889221191
Epoch: 57, Steps: 63 | Train Loss: 0.4730117 Vali Loss: 1.2377455 Test Loss: 0.4589773
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.331397294998169
Epoch: 58, Steps: 63 | Train Loss: 0.4728682 Vali Loss: 1.2415888 Test Loss: 0.4590072
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.1491122245788574
Epoch: 59, Steps: 63 | Train Loss: 0.4726736 Vali Loss: 1.2395855 Test Loss: 0.4590307
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.107374906539917
Epoch: 60, Steps: 63 | Train Loss: 0.4731811 Vali Loss: 1.2386477 Test Loss: 0.4590439
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.067819595336914
Epoch: 61, Steps: 63 | Train Loss: 0.4731280 Vali Loss: 1.2398348 Test Loss: 0.4590693
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.2817630767822266
Epoch: 62, Steps: 63 | Train Loss: 0.4730154 Vali Loss: 1.2372352 Test Loss: 0.4590854
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.2633717060089111
Epoch: 63, Steps: 63 | Train Loss: 0.4730593 Vali Loss: 1.2379081 Test Loss: 0.4590999
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.2140586376190186
Epoch: 64, Steps: 63 | Train Loss: 0.4729614 Vali Loss: 1.2437119 Test Loss: 0.4591238
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.2006652355194092
Epoch: 65, Steps: 63 | Train Loss: 0.4730649 Vali Loss: 1.2404288 Test Loss: 0.4591310
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.2312841415405273
Epoch: 66, Steps: 63 | Train Loss: 0.4725404 Vali Loss: 1.2418265 Test Loss: 0.4591490
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.5812442302703857
Epoch: 67, Steps: 63 | Train Loss: 0.4730124 Vali Loss: 1.2372230 Test Loss: 0.4591725
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 4.098021507263184
Epoch: 68, Steps: 63 | Train Loss: 0.4727078 Vali Loss: 1.2349913 Test Loss: 0.4591834
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 4.789979696273804
Epoch: 69, Steps: 63 | Train Loss: 0.4729196 Vali Loss: 1.2377958 Test Loss: 0.4591998
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.850574016571045
Epoch: 70, Steps: 63 | Train Loss: 0.4727219 Vali Loss: 1.2357950 Test Loss: 0.4592063
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4580236077308655, mae:0.4387005865573883, rse:0.6443116068840027, corr:[0.25065988 0.25585383 0.25622156 0.25325775 0.24984136 0.24800886
 0.2481459  0.2492808  0.24976012 0.24934655 0.2486603  0.24837777
 0.24846269 0.24819353 0.24755816 0.24693301 0.246514   0.24636984
 0.24643582 0.24665673 0.24702281 0.24735638 0.2473921  0.24680535
 0.24594799 0.24573225 0.24582216 0.24570298 0.24502026 0.2440786
 0.24328078 0.24286744 0.2428279  0.24302551 0.243228   0.24347585
 0.24361157 0.24350373 0.24354918 0.24384378 0.2442435  0.24446441
 0.24451672 0.24470262 0.24519283 0.24573466 0.24611646 0.24569447
 0.24436462 0.24317506 0.24184717 0.24043201 0.2389601  0.2377579
 0.23726505 0.23744795 0.23766744 0.23771037 0.23749912 0.23776336
 0.23825537 0.23838922 0.23820223 0.23794723 0.23781268 0.23778775
 0.2378461  0.23789443 0.23804198 0.23808959 0.23786314 0.23693466
 0.23553525 0.23470561 0.23431905 0.23411939 0.23379995 0.23329572
 0.23295785 0.23283474 0.23270978 0.23250438 0.2321444  0.23205753
 0.23232287 0.23238558 0.23234893 0.23228838 0.23215087 0.23193736
 0.23172276 0.2317779  0.2320985  0.23267373 0.23324901 0.23332736
 0.23272783 0.23227862 0.23192272 0.23123355 0.23038027 0.22973499
 0.22960244 0.2301083  0.23055948 0.23062192 0.2303358  0.23035038
 0.23064466 0.23074846 0.23073636 0.2307369  0.23063095 0.23037481
 0.23016804 0.2301208  0.2302294  0.23033871 0.23023394 0.22955938
 0.22827914 0.22720319 0.22618465 0.22520907 0.22434583 0.22383642
 0.22373928 0.22398563 0.22409433 0.22410226 0.2241596  0.2246434
 0.22529079 0.22549585 0.22563869 0.22581545 0.22588588 0.22582579
 0.22570898 0.22572738 0.22594921 0.2262214  0.22629945 0.22575133
 0.22453856 0.22354448 0.22275437 0.22168788 0.22071679 0.22008054
 0.2200818  0.2205827  0.22121716 0.2215933  0.22161208 0.22174692
 0.22202928 0.22212829 0.22223684 0.22231351 0.22220287 0.22210203
 0.22209626 0.2222961  0.22265857 0.22297563 0.22301088 0.22242984
 0.22139399 0.22072601 0.22026494 0.21967626 0.21909663 0.21875699
 0.2188795  0.21943384 0.21994492 0.22042814 0.22084229 0.22153634
 0.22232477 0.2225908  0.22254568 0.2225209  0.22252996 0.2226003
 0.22271116 0.22291504 0.2232707  0.22367811 0.22384326 0.22344801
 0.22245409 0.22169322 0.22097412 0.22003192 0.21898106 0.21814203
 0.21785526 0.2181315  0.21864718 0.21904162 0.21909481 0.2193689
 0.21980505 0.21989015 0.21981187 0.21965648 0.2194114  0.2190867
 0.21879071 0.2186497  0.21877371 0.21898739 0.21908727 0.21872663
 0.21794666 0.21743734 0.2170498  0.21648633 0.21599832 0.21562925
 0.2157119  0.21604525 0.21630603 0.21627401 0.21608938 0.21635029
 0.21705887 0.21729709 0.21719071 0.21703511 0.21681002 0.216609
 0.21653198 0.21661642 0.21689376 0.21717675 0.2172204  0.21669225
 0.21573132 0.2151424  0.21478334 0.21433412 0.21377751 0.21340285
 0.21348101 0.21388286 0.21437007 0.21481776 0.21523193 0.21591167
 0.21666326 0.21687992 0.21696459 0.21705304 0.2170824  0.21703213
 0.21697932 0.2169285  0.21700303 0.21712366 0.21715005 0.21669298
 0.21578573 0.21518081 0.2146048  0.21382478 0.21301912 0.21252334
 0.21262346 0.2130995  0.21340619 0.2134001  0.21322738 0.2133917
 0.21408327 0.21441205 0.2144084  0.21422921 0.2139413  0.21372877
 0.21373247 0.21392383 0.21432331 0.21474354 0.21506058 0.21490459
 0.21429007 0.21414256 0.21426076 0.21415403 0.2137937  0.21368429
 0.21387723 0.21425216 0.2146679  0.21506715 0.21531145 0.21566097
 0.21611398 0.21608183 0.2159557  0.21603046 0.2162608  0.21642804
 0.21652965 0.21660645 0.2167828  0.21692981 0.21697763 0.21652502
 0.21549869 0.21491468 0.21454743 0.21389598 0.2133318  0.21287838
 0.21290433 0.21318766 0.21315095 0.21276449 0.21243535 0.21310136
 0.21482733 0.21587366 0.2161261  0.21620968 0.21614848 0.21593754
 0.21562591 0.2153593  0.21520948 0.21522991 0.21417871 0.20797628]
