Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=58, out_features=290, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  15070720.0
params:  17110.0
Trainable parameters:  17110
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.3892631530761719
Epoch: 1, Steps: 60 | Train Loss: 1.1303340 Vali Loss: 2.1289461 Test Loss: 0.9281396
Validation loss decreased (inf --> 2.128946).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.30995512008667
Epoch: 2, Steps: 60 | Train Loss: 0.8262310 Vali Loss: 1.8171943 Test Loss: 0.6783015
Validation loss decreased (2.128946 --> 1.817194).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.3423430919647217
Epoch: 3, Steps: 60 | Train Loss: 0.7098094 Vali Loss: 1.6745126 Test Loss: 0.5701265
Validation loss decreased (1.817194 --> 1.674513).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3029067516326904
Epoch: 4, Steps: 60 | Train Loss: 0.6573082 Vali Loss: 1.6106710 Test Loss: 0.5170210
Validation loss decreased (1.674513 --> 1.610671).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.275691270828247
Epoch: 5, Steps: 60 | Train Loss: 0.6308572 Vali Loss: 1.5809762 Test Loss: 0.4890348
Validation loss decreased (1.610671 --> 1.580976).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.4994771480560303
Epoch: 6, Steps: 60 | Train Loss: 0.6163013 Vali Loss: 1.5513380 Test Loss: 0.4732237
Validation loss decreased (1.580976 --> 1.551338).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.463054895401001
Epoch: 7, Steps: 60 | Train Loss: 0.6083662 Vali Loss: 1.5408517 Test Loss: 0.4636674
Validation loss decreased (1.551338 --> 1.540852).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.3862333297729492
Epoch: 8, Steps: 60 | Train Loss: 0.6030410 Vali Loss: 1.5350864 Test Loss: 0.4573595
Validation loss decreased (1.540852 --> 1.535086).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4443011283874512
Epoch: 9, Steps: 60 | Train Loss: 0.5992194 Vali Loss: 1.5319594 Test Loss: 0.4530023
Validation loss decreased (1.535086 --> 1.531959).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.2574164867401123
Epoch: 10, Steps: 60 | Train Loss: 0.5963395 Vali Loss: 1.5213811 Test Loss: 0.4498325
Validation loss decreased (1.531959 --> 1.521381).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.4006505012512207
Epoch: 11, Steps: 60 | Train Loss: 0.5948889 Vali Loss: 1.5218389 Test Loss: 0.4474309
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.387380838394165
Epoch: 12, Steps: 60 | Train Loss: 0.5925214 Vali Loss: 1.5198700 Test Loss: 0.4455335
Validation loss decreased (1.521381 --> 1.519870).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.3735370635986328
Epoch: 13, Steps: 60 | Train Loss: 0.5916775 Vali Loss: 1.5144160 Test Loss: 0.4441303
Validation loss decreased (1.519870 --> 1.514416).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.3681578636169434
Epoch: 14, Steps: 60 | Train Loss: 0.5908223 Vali Loss: 1.5120101 Test Loss: 0.4430156
Validation loss decreased (1.514416 --> 1.512010).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.2926180362701416
Epoch: 15, Steps: 60 | Train Loss: 0.5901313 Vali Loss: 1.5143237 Test Loss: 0.4420231
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.2725434303283691
Epoch: 16, Steps: 60 | Train Loss: 0.5888078 Vali Loss: 1.5077026 Test Loss: 0.4413379
Validation loss decreased (1.512010 --> 1.507703).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3996992111206055
Epoch: 17, Steps: 60 | Train Loss: 0.5881721 Vali Loss: 1.5098598 Test Loss: 0.4407403
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.3504407405853271
Epoch: 18, Steps: 60 | Train Loss: 0.5875450 Vali Loss: 1.5032537 Test Loss: 0.4402623
Validation loss decreased (1.507703 --> 1.503254).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.288926601409912
Epoch: 19, Steps: 60 | Train Loss: 0.5877498 Vali Loss: 1.5008733 Test Loss: 0.4398971
Validation loss decreased (1.503254 --> 1.500873).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3915932178497314
Epoch: 20, Steps: 60 | Train Loss: 0.5868197 Vali Loss: 1.4993672 Test Loss: 0.4396318
Validation loss decreased (1.500873 --> 1.499367).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.3912949562072754
Epoch: 21, Steps: 60 | Train Loss: 0.5857418 Vali Loss: 1.5024675 Test Loss: 0.4393770
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.39725661277771
Epoch: 22, Steps: 60 | Train Loss: 0.5862114 Vali Loss: 1.4997740 Test Loss: 0.4391897
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.340050220489502
Epoch: 23, Steps: 60 | Train Loss: 0.5859072 Vali Loss: 1.5014889 Test Loss: 0.4391174
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.3649353981018066
Epoch: 24, Steps: 60 | Train Loss: 0.5855939 Vali Loss: 1.5046579 Test Loss: 0.4389551
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.3276135921478271
Epoch: 25, Steps: 60 | Train Loss: 0.5857577 Vali Loss: 1.5014882 Test Loss: 0.4388762
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.357576847076416
Epoch: 26, Steps: 60 | Train Loss: 0.5858008 Vali Loss: 1.4998379 Test Loss: 0.4387911
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.3024065494537354
Epoch: 27, Steps: 60 | Train Loss: 0.5851557 Vali Loss: 1.5066181 Test Loss: 0.4387664
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.4147148132324219
Epoch: 28, Steps: 60 | Train Loss: 0.5850068 Vali Loss: 1.5014727 Test Loss: 0.4387178
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.2310035228729248
Epoch: 29, Steps: 60 | Train Loss: 0.5852239 Vali Loss: 1.5024328 Test Loss: 0.4387169
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.3357980251312256
Epoch: 30, Steps: 60 | Train Loss: 0.5850566 Vali Loss: 1.5007184 Test Loss: 0.4387373
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.4218401908874512
Epoch: 31, Steps: 60 | Train Loss: 0.5843177 Vali Loss: 1.5008304 Test Loss: 0.4387172
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.4211738109588623
Epoch: 32, Steps: 60 | Train Loss: 0.5847320 Vali Loss: 1.4918816 Test Loss: 0.4387047
Validation loss decreased (1.499367 --> 1.491882).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.3125998973846436
Epoch: 33, Steps: 60 | Train Loss: 0.5844908 Vali Loss: 1.4979315 Test Loss: 0.4387225
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.347334861755371
Epoch: 34, Steps: 60 | Train Loss: 0.5843522 Vali Loss: 1.4959515 Test Loss: 0.4387348
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.4139187335968018
Epoch: 35, Steps: 60 | Train Loss: 0.5839289 Vali Loss: 1.4918728 Test Loss: 0.4387559
Validation loss decreased (1.491882 --> 1.491873).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.4772701263427734
Epoch: 36, Steps: 60 | Train Loss: 0.5844898 Vali Loss: 1.4930520 Test Loss: 0.4387618
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.2628729343414307
Epoch: 37, Steps: 60 | Train Loss: 0.5841583 Vali Loss: 1.5015792 Test Loss: 0.4387747
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.2477478981018066
Epoch: 38, Steps: 60 | Train Loss: 0.5841330 Vali Loss: 1.4956913 Test Loss: 0.4388156
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3156299591064453
Epoch: 39, Steps: 60 | Train Loss: 0.5838660 Vali Loss: 1.5038047 Test Loss: 0.4388244
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.2921802997589111
Epoch: 40, Steps: 60 | Train Loss: 0.5837453 Vali Loss: 1.4966661 Test Loss: 0.4388667
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.3857371807098389
Epoch: 41, Steps: 60 | Train Loss: 0.5842211 Vali Loss: 1.5022631 Test Loss: 0.4388842
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3900883197784424
Epoch: 42, Steps: 60 | Train Loss: 0.5838936 Vali Loss: 1.4999926 Test Loss: 0.4389223
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.3601596355438232
Epoch: 43, Steps: 60 | Train Loss: 0.5838168 Vali Loss: 1.4957983 Test Loss: 0.4389487
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.2797932624816895
Epoch: 44, Steps: 60 | Train Loss: 0.5835794 Vali Loss: 1.4964751 Test Loss: 0.4389776
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.3432600498199463
Epoch: 45, Steps: 60 | Train Loss: 0.5838671 Vali Loss: 1.4950864 Test Loss: 0.4389896
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.3509178161621094
Epoch: 46, Steps: 60 | Train Loss: 0.5840150 Vali Loss: 1.4989638 Test Loss: 0.4390223
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.4126148223876953
Epoch: 47, Steps: 60 | Train Loss: 0.5839562 Vali Loss: 1.4979279 Test Loss: 0.4390439
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.4392554759979248
Epoch: 48, Steps: 60 | Train Loss: 0.5836766 Vali Loss: 1.4992771 Test Loss: 0.4390564
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.7261533737182617
Epoch: 49, Steps: 60 | Train Loss: 0.5835835 Vali Loss: 1.4998903 Test Loss: 0.4390812
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2859973907470703
Epoch: 50, Steps: 60 | Train Loss: 0.5836936 Vali Loss: 1.4973737 Test Loss: 0.4391086
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3526370525360107
Epoch: 51, Steps: 60 | Train Loss: 0.5835696 Vali Loss: 1.5007842 Test Loss: 0.4391257
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3234984874725342
Epoch: 52, Steps: 60 | Train Loss: 0.5834122 Vali Loss: 1.4977797 Test Loss: 0.4391465
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.307779312133789
Epoch: 53, Steps: 60 | Train Loss: 0.5832387 Vali Loss: 1.4934707 Test Loss: 0.4391611
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.330390214920044
Epoch: 54, Steps: 60 | Train Loss: 0.5838295 Vali Loss: 1.4892361 Test Loss: 0.4391730
Validation loss decreased (1.491873 --> 1.489236).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.3843505382537842
Epoch: 55, Steps: 60 | Train Loss: 0.5834205 Vali Loss: 1.4999455 Test Loss: 0.4391894
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.3728363513946533
Epoch: 56, Steps: 60 | Train Loss: 0.5835963 Vali Loss: 1.4933015 Test Loss: 0.4392124
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.339797019958496
Epoch: 57, Steps: 60 | Train Loss: 0.5833752 Vali Loss: 1.4944360 Test Loss: 0.4392337
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.397106647491455
Epoch: 58, Steps: 60 | Train Loss: 0.5837000 Vali Loss: 1.4961455 Test Loss: 0.4392512
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.379831075668335
Epoch: 59, Steps: 60 | Train Loss: 0.5831132 Vali Loss: 1.4986651 Test Loss: 0.4392609
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.4314994812011719
Epoch: 60, Steps: 60 | Train Loss: 0.5832370 Vali Loss: 1.4983774 Test Loss: 0.4392731
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.4139134883880615
Epoch: 61, Steps: 60 | Train Loss: 0.5832994 Vali Loss: 1.5025321 Test Loss: 0.4392920
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.3182389736175537
Epoch: 62, Steps: 60 | Train Loss: 0.5835095 Vali Loss: 1.4922612 Test Loss: 0.4393020
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.2819914817810059
Epoch: 63, Steps: 60 | Train Loss: 0.5837847 Vali Loss: 1.4913751 Test Loss: 0.4393238
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.3149795532226562
Epoch: 64, Steps: 60 | Train Loss: 0.5832691 Vali Loss: 1.4904199 Test Loss: 0.4393383
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.5316567420959473
Epoch: 65, Steps: 60 | Train Loss: 0.5832112 Vali Loss: 1.4937418 Test Loss: 0.4393477
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.3864150047302246
Epoch: 66, Steps: 60 | Train Loss: 0.5831619 Vali Loss: 1.5007985 Test Loss: 0.4393665
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.2642402648925781
Epoch: 67, Steps: 60 | Train Loss: 0.5831733 Vali Loss: 1.4956501 Test Loss: 0.4393716
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.291574239730835
Epoch: 68, Steps: 60 | Train Loss: 0.5837331 Vali Loss: 1.4918214 Test Loss: 0.4393857
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3670923709869385
Epoch: 69, Steps: 60 | Train Loss: 0.5833672 Vali Loss: 1.4904356 Test Loss: 0.4393943
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.3619894981384277
Epoch: 70, Steps: 60 | Train Loss: 0.5838367 Vali Loss: 1.5028052 Test Loss: 0.4394047
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.3433644771575928
Epoch: 71, Steps: 60 | Train Loss: 0.5829085 Vali Loss: 1.4927338 Test Loss: 0.4394129
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.3687975406646729
Epoch: 72, Steps: 60 | Train Loss: 0.5835784 Vali Loss: 1.4969525 Test Loss: 0.4394270
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.4141020774841309
Epoch: 73, Steps: 60 | Train Loss: 0.5833385 Vali Loss: 1.4960277 Test Loss: 0.4394303
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.5033206939697266
Epoch: 74, Steps: 60 | Train Loss: 0.5829142 Vali Loss: 1.5007483 Test Loss: 0.4394388
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4380011260509491, mae:0.4492550194263458, rse:0.6335626840591431, corr:[0.22731383 0.23345849 0.23354487 0.23446214 0.23253411 0.22975984
 0.2298477  0.23092599 0.23084317 0.23164913 0.23077457 0.2304627
 0.23131108 0.23006436 0.22918934 0.22984526 0.22948922 0.22899352
 0.22934727 0.22934037 0.22914663 0.22964309 0.23038387 0.23049194
 0.2298895  0.22912572 0.22844835 0.22812179 0.22766283 0.22717594
 0.22698358 0.22674099 0.22628377 0.22655065 0.22677122 0.22677834
 0.22740716 0.22760221 0.22705989 0.22695827 0.22739138 0.22731763
 0.22728352 0.2277908  0.22806047 0.22831464 0.22906984 0.22956344
 0.22880763 0.22727554 0.2256434  0.2246431  0.22355886 0.22223479
 0.22174074 0.22162955 0.2212157  0.22154795 0.22177856 0.22199276
 0.22238666 0.22252077 0.22214206 0.2219632  0.22217073 0.22213182
 0.22208269 0.22206815 0.22188656 0.22189432 0.22190446 0.22142513
 0.22049277 0.2195603  0.21845464 0.21819016 0.2182613  0.21788274
 0.2177623  0.217897   0.21756373 0.21738979 0.21735774 0.21741194
 0.21735813 0.21742523 0.21746187 0.21700466 0.21671326 0.21668598
 0.21639118 0.21624166 0.21639675 0.21691854 0.21770172 0.2183481
 0.2181909  0.21783954 0.21762498 0.21740691 0.21735066 0.2171095
 0.21674092 0.21698263 0.21711272 0.21724918 0.21709308 0.21735352
 0.21780454 0.21765484 0.21737346 0.21724659 0.21709906 0.21690474
 0.21688925 0.21685039 0.2168317  0.2169454  0.21691759 0.21669438
 0.21617736 0.215221   0.21384507 0.21332179 0.21296038 0.21234472
 0.21222363 0.21268792 0.21259493 0.21263178 0.21267489 0.2131163
 0.21392836 0.2140159  0.21363063 0.21359219 0.21374409 0.21347676
 0.21338876 0.21361803 0.21361606 0.21377505 0.21408643 0.21400656
 0.21347009 0.21284768 0.21200196 0.21098664 0.21029602 0.2097688
 0.20959991 0.20982431 0.21005096 0.2102844  0.21045823 0.21073194
 0.21094625 0.21091934 0.21076986 0.21042308 0.2103396  0.21043412
 0.21046743 0.21054882 0.21067029 0.21104391 0.21137956 0.21150142
 0.21108869 0.21058895 0.21014705 0.21002358 0.20976055 0.20919707
 0.2091632  0.209834   0.21016407 0.21059147 0.21100707 0.21153964
 0.21206251 0.21217304 0.2120241  0.2117613  0.21170749 0.2118045
 0.21190026 0.2120643  0.21225025 0.21248254 0.21268709 0.21243419
 0.21157657 0.21082921 0.20999533 0.20936677 0.20879926 0.20811026
 0.20787187 0.20817202 0.20834398 0.20859511 0.208829   0.20941289
 0.20986742 0.20989704 0.20966786 0.20939575 0.20924148 0.20908682
 0.20889208 0.20876797 0.20872554 0.2089591  0.2091055  0.20887126
 0.20822307 0.2077601  0.20743112 0.20722407 0.20690113 0.20627758
 0.20620799 0.20644939 0.20647451 0.20659277 0.20652683 0.20667148
 0.20717956 0.20722426 0.2070047  0.20690659 0.20672771 0.20635036
 0.20618194 0.20613836 0.20596723 0.20596899 0.20612392 0.20589219
 0.20534626 0.20494731 0.20438598 0.20410444 0.2041609  0.20399505
 0.20400253 0.20436613 0.20455724 0.20473388 0.2050719  0.2057057
 0.20631403 0.20666665 0.20680584 0.20666265 0.20654964 0.20646127
 0.20630282 0.20605348 0.20582773 0.20575605 0.20582308 0.20575601
 0.20523112 0.20466785 0.20411849 0.20386979 0.2036543  0.20326021
 0.20316997 0.203448   0.2034366  0.2035716  0.20372188 0.20395067
 0.20431031 0.20438658 0.20429018 0.2039862  0.20366552 0.20354669
 0.20359538 0.2037141  0.20374082 0.20415069 0.20465179 0.20485592
 0.20470458 0.20465642 0.2047996  0.20496058 0.2048486  0.20469615
 0.20497338 0.20537263 0.2054962  0.2058225  0.20604767 0.2062366
 0.2066635  0.206978   0.20705274 0.20701674 0.20695768 0.20678236
 0.20670457 0.20679925 0.20675887 0.20676616 0.20676287 0.20683834
 0.20676464 0.20656249 0.2061989  0.20595567 0.2057645  0.2051002
 0.20490739 0.20521122 0.20509791 0.20503509 0.20511495 0.20537677
 0.20584948 0.20610327 0.2061964  0.20623408 0.20623904 0.2061472
 0.20620894 0.20621687 0.20608462 0.20631759 0.2065725  0.20642221
 0.20604457 0.20575179 0.20531568 0.20494762 0.20446938 0.20401798
 0.20399022 0.2040913  0.2041629  0.20443302 0.20483963 0.20523302
 0.20551717 0.20575182 0.20581439 0.20576842 0.20581181 0.20574924
 0.20556045 0.20566362 0.20559339 0.20559524 0.20557179 0.20546505
 0.20495626 0.20460173 0.20443694 0.20415269 0.20354861 0.20302786
 0.20304196 0.20301004 0.20281413 0.20272775 0.20250133 0.20243403
 0.20262912 0.20248277 0.20233488 0.20217273 0.20204684 0.20214945
 0.20244929 0.20266712 0.20274687 0.20313123 0.2035471  0.20418102
 0.20462005 0.20485261 0.20503618 0.20522104 0.20483555 0.20384564
 0.20357966 0.20366716 0.20323706 0.20327543 0.20331073 0.20359755
 0.20437317 0.20472507 0.20467098 0.20459688 0.20465969 0.20466593
 0.20488562 0.20537065 0.20559785 0.205806   0.20609155 0.20633167
 0.2063396  0.20630784 0.20610502 0.20590587 0.20588534 0.20556794
 0.20550936 0.20589839 0.20609285 0.20618661 0.20640402 0.20670965
 0.20705709 0.20722744 0.20706663 0.20680857 0.20671003 0.2066884
 0.2068147  0.20731688 0.20751369 0.20804903 0.20884593 0.20936981
 0.20929347 0.20921518 0.20891531 0.20855264 0.20850097 0.20824705
 0.2081878  0.20849831 0.20857416 0.20881887 0.2089536  0.2092877
 0.20967355 0.20962961 0.20947255 0.2093144  0.2092166  0.209203
 0.20928745 0.20949906 0.20959699 0.20992757 0.21055605 0.2110988
 0.21104196 0.21098556 0.21082    0.21037355 0.20977116 0.2091078
 0.20885907 0.20893884 0.20916589 0.209588   0.20999418 0.21073042
 0.21139474 0.21132684 0.21125352 0.2114403  0.21145359 0.21133667
 0.21141738 0.2115942  0.21154678 0.21145433 0.21164049 0.21171686
 0.21138747 0.21092497 0.21026203 0.20984896 0.20953454 0.20902568
 0.20920354 0.20964445 0.20969576 0.2100241  0.2102148  0.21038452
 0.21085235 0.21092805 0.21080028 0.21077524 0.21074139 0.21054143
 0.2107217  0.21123412 0.21144046 0.21173595 0.21243826 0.21306573
 0.2131969  0.21326746 0.21314965 0.21299648 0.21294634 0.21245782
 0.21197744 0.21176031 0.21169616 0.21168506 0.21168333 0.21195132
 0.21242741 0.2125975  0.21268152 0.21280116 0.21309108 0.21330659
 0.21350133 0.21395414 0.21433324 0.21472497 0.21531565 0.21562836
 0.21509738 0.21454018 0.21412124 0.2138656  0.21334459 0.2125401
 0.21210834 0.21209094 0.21198806 0.21219009 0.21234298 0.21265788
 0.21308105 0.21312551 0.21322824 0.2134081  0.21355209 0.21362625
 0.21370596 0.21378475 0.21368112 0.21347934 0.21343763 0.21313827
 0.21217148 0.21144827 0.21098062 0.21036416 0.20955083 0.20881526
 0.20859857 0.20850295 0.20853822 0.20881565 0.20877425 0.20907286
 0.20983936 0.20976011 0.20957942 0.20943877 0.2092252  0.20906252
 0.20908017 0.2090417  0.20891343 0.2087953  0.20896654 0.20896198
 0.20814487 0.20727113 0.20651764 0.20607509 0.20562896 0.2045985
 0.20412235 0.20401855 0.20392597 0.20405841 0.20393367 0.20396562
 0.20402521 0.20356894 0.2031375  0.2028528  0.20249328 0.20221692
 0.20226254 0.20226087 0.20199159 0.20189463 0.20189855 0.20147298
 0.20050973 0.19970462 0.19895764 0.19816576 0.19741611 0.19664256
 0.19615595 0.1960908  0.19594498 0.19586347 0.19593927 0.19607812
 0.19637756 0.19627908 0.19606145 0.19580062 0.19554774 0.19527237
 0.19500639 0.19499126 0.19499037 0.19495375 0.19502391 0.1948218
 0.19392423 0.19305521 0.19233653 0.19145307 0.19072072 0.1902098
 0.19019365 0.18999578 0.18976413 0.19008993 0.19000469 0.1902589
 0.19109662 0.19093823 0.19052303 0.1904305  0.1900285  0.18959361
 0.189632   0.18979457 0.18966268 0.1897034  0.1898684  0.18977344
 0.1890773  0.18800479 0.18673457 0.18619466 0.18575059 0.18495692
 0.18502149 0.18514436 0.18480559 0.18509051 0.1855325  0.18555425
 0.18598601 0.1862204  0.18588339 0.18545668 0.1855014  0.18538134
 0.18528551 0.18554308 0.18549886 0.18533857 0.18550935 0.18519245
 0.18379314 0.1825456  0.18125637 0.17973617 0.17900728 0.17805399
 0.17730401 0.17744899 0.17796811 0.17814615 0.1785814  0.17952655
 0.18013266 0.17997497 0.17992786 0.17947145 0.17910796 0.17935282
 0.1792793  0.17904186 0.17937024 0.17961346 0.1794824  0.17958426
 0.17923309 0.17837249 0.1777126  0.1774745  0.17659496 0.17578655
 0.17578004 0.17567988 0.17564866 0.1756671  0.1752411  0.1753936
 0.17589413 0.174918   0.17444965 0.17427823 0.17352802 0.17349325
 0.174143   0.17342444 0.17346731 0.17376378 0.17224798 0.1722409 ]
