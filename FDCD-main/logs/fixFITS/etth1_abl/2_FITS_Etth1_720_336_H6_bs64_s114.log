Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 6.376275539398193
Epoch: 1, Steps: 59 | Train Loss: 0.6972898 Vali Loss: 1.7439628 Test Loss: 0.8336298
Validation loss decreased (inf --> 1.743963).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 6.261037826538086
Epoch: 2, Steps: 59 | Train Loss: 0.5448621 Vali Loss: 1.5902860 Test Loss: 0.7523731
Validation loss decreased (1.743963 --> 1.590286).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 6.560471057891846
Epoch: 3, Steps: 59 | Train Loss: 0.4775600 Vali Loss: 1.5370713 Test Loss: 0.7241384
Validation loss decreased (1.590286 --> 1.537071).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 6.329367399215698
Epoch: 4, Steps: 59 | Train Loss: 0.4390399 Vali Loss: 1.5024250 Test Loss: 0.7098185
Validation loss decreased (1.537071 --> 1.502425).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 6.462772607803345
Epoch: 5, Steps: 59 | Train Loss: 0.4118537 Vali Loss: 1.4796511 Test Loss: 0.6965988
Validation loss decreased (1.502425 --> 1.479651).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 6.557917594909668
Epoch: 6, Steps: 59 | Train Loss: 0.3896029 Vali Loss: 1.4581313 Test Loss: 0.6859003
Validation loss decreased (1.479651 --> 1.458131).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 6.577686309814453
Epoch: 7, Steps: 59 | Train Loss: 0.3712651 Vali Loss: 1.4384146 Test Loss: 0.6764168
Validation loss decreased (1.458131 --> 1.438415).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 6.63476824760437
Epoch: 8, Steps: 59 | Train Loss: 0.3550440 Vali Loss: 1.4240794 Test Loss: 0.6668438
Validation loss decreased (1.438415 --> 1.424079).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 6.717280387878418
Epoch: 9, Steps: 59 | Train Loss: 0.3408564 Vali Loss: 1.4156084 Test Loss: 0.6577775
Validation loss decreased (1.424079 --> 1.415608).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 6.35740852355957
Epoch: 10, Steps: 59 | Train Loss: 0.3286909 Vali Loss: 1.4032580 Test Loss: 0.6503685
Validation loss decreased (1.415608 --> 1.403258).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.554868936538696
Epoch: 11, Steps: 59 | Train Loss: 0.3178524 Vali Loss: 1.3941361 Test Loss: 0.6426209
Validation loss decreased (1.403258 --> 1.394136).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.411637544631958
Epoch: 12, Steps: 59 | Train Loss: 0.3083536 Vali Loss: 1.3899472 Test Loss: 0.6347566
Validation loss decreased (1.394136 --> 1.389947).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 6.450660467147827
Epoch: 13, Steps: 59 | Train Loss: 0.2995149 Vali Loss: 1.3771263 Test Loss: 0.6282579
Validation loss decreased (1.389947 --> 1.377126).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 6.485725402832031
Epoch: 14, Steps: 59 | Train Loss: 0.2916750 Vali Loss: 1.3683099 Test Loss: 0.6206926
Validation loss decreased (1.377126 --> 1.368310).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 6.587510347366333
Epoch: 15, Steps: 59 | Train Loss: 0.2845381 Vali Loss: 1.3602605 Test Loss: 0.6145959
Validation loss decreased (1.368310 --> 1.360260).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 6.5347981452941895
Epoch: 16, Steps: 59 | Train Loss: 0.2780200 Vali Loss: 1.3531462 Test Loss: 0.6094917
Validation loss decreased (1.360260 --> 1.353146).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 6.624585151672363
Epoch: 17, Steps: 59 | Train Loss: 0.2722595 Vali Loss: 1.3517215 Test Loss: 0.6035663
Validation loss decreased (1.353146 --> 1.351722).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 6.721799612045288
Epoch: 18, Steps: 59 | Train Loss: 0.2669629 Vali Loss: 1.3472823 Test Loss: 0.5982636
Validation loss decreased (1.351722 --> 1.347282).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 6.5675270557403564
Epoch: 19, Steps: 59 | Train Loss: 0.2620166 Vali Loss: 1.3398430 Test Loss: 0.5941816
Validation loss decreased (1.347282 --> 1.339843).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 6.584668159484863
Epoch: 20, Steps: 59 | Train Loss: 0.2574825 Vali Loss: 1.3324746 Test Loss: 0.5893289
Validation loss decreased (1.339843 --> 1.332475).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 6.566264629364014
Epoch: 21, Steps: 59 | Train Loss: 0.2534100 Vali Loss: 1.3313769 Test Loss: 0.5859726
Validation loss decreased (1.332475 --> 1.331377).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 6.512321710586548
Epoch: 22, Steps: 59 | Train Loss: 0.2496904 Vali Loss: 1.3274704 Test Loss: 0.5814878
Validation loss decreased (1.331377 --> 1.327470).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 6.861781120300293
Epoch: 23, Steps: 59 | Train Loss: 0.2463594 Vali Loss: 1.3246529 Test Loss: 0.5775658
Validation loss decreased (1.327470 --> 1.324653).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 6.763242244720459
Epoch: 24, Steps: 59 | Train Loss: 0.2431819 Vali Loss: 1.3181318 Test Loss: 0.5739667
Validation loss decreased (1.324653 --> 1.318132).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 6.505658864974976
Epoch: 25, Steps: 59 | Train Loss: 0.2402860 Vali Loss: 1.3157943 Test Loss: 0.5713010
Validation loss decreased (1.318132 --> 1.315794).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 6.4516332149505615
Epoch: 26, Steps: 59 | Train Loss: 0.2374534 Vali Loss: 1.3179752 Test Loss: 0.5682806
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 6.498401403427124
Epoch: 27, Steps: 59 | Train Loss: 0.2350316 Vali Loss: 1.3096126 Test Loss: 0.5653464
Validation loss decreased (1.315794 --> 1.309613).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 6.409851551055908
Epoch: 28, Steps: 59 | Train Loss: 0.2325347 Vali Loss: 1.3131803 Test Loss: 0.5627657
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 6.4695658683776855
Epoch: 29, Steps: 59 | Train Loss: 0.2305051 Vali Loss: 1.3027960 Test Loss: 0.5605190
Validation loss decreased (1.309613 --> 1.302796).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 6.4331793785095215
Epoch: 30, Steps: 59 | Train Loss: 0.2284464 Vali Loss: 1.3030524 Test Loss: 0.5579947
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 6.589106798171997
Epoch: 31, Steps: 59 | Train Loss: 0.2263940 Vali Loss: 1.3019422 Test Loss: 0.5554804
Validation loss decreased (1.302796 --> 1.301942).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 6.376525163650513
Epoch: 32, Steps: 59 | Train Loss: 0.2246964 Vali Loss: 1.3052547 Test Loss: 0.5534272
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 6.5534279346466064
Epoch: 33, Steps: 59 | Train Loss: 0.2231238 Vali Loss: 1.3026925 Test Loss: 0.5512910
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 6.264169454574585
Epoch: 34, Steps: 59 | Train Loss: 0.2215539 Vali Loss: 1.3001274 Test Loss: 0.5497807
Validation loss decreased (1.301942 --> 1.300127).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 6.525304794311523
Epoch: 35, Steps: 59 | Train Loss: 0.2202362 Vali Loss: 1.2950456 Test Loss: 0.5478334
Validation loss decreased (1.300127 --> 1.295046).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 6.69253945350647
Epoch: 36, Steps: 59 | Train Loss: 0.2187685 Vali Loss: 1.2960730 Test Loss: 0.5462382
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 6.601710796356201
Epoch: 37, Steps: 59 | Train Loss: 0.2174530 Vali Loss: 1.2958900 Test Loss: 0.5445907
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 6.177592754364014
Epoch: 38, Steps: 59 | Train Loss: 0.2164966 Vali Loss: 1.2987547 Test Loss: 0.5432994
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 6.305712699890137
Epoch: 39, Steps: 59 | Train Loss: 0.2151940 Vali Loss: 1.2926203 Test Loss: 0.5418223
Validation loss decreased (1.295046 --> 1.292620).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 6.5358381271362305
Epoch: 40, Steps: 59 | Train Loss: 0.2142161 Vali Loss: 1.2914789 Test Loss: 0.5404100
Validation loss decreased (1.292620 --> 1.291479).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 6.385266304016113
Epoch: 41, Steps: 59 | Train Loss: 0.2132120 Vali Loss: 1.2885237 Test Loss: 0.5391291
Validation loss decreased (1.291479 --> 1.288524).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 6.596324920654297
Epoch: 42, Steps: 59 | Train Loss: 0.2121656 Vali Loss: 1.2940803 Test Loss: 0.5380729
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 6.658099889755249
Epoch: 43, Steps: 59 | Train Loss: 0.2114905 Vali Loss: 1.2925439 Test Loss: 0.5370535
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 6.482237815856934
Epoch: 44, Steps: 59 | Train Loss: 0.2106792 Vali Loss: 1.2861149 Test Loss: 0.5358738
Validation loss decreased (1.288524 --> 1.286115).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 6.335186243057251
Epoch: 45, Steps: 59 | Train Loss: 0.2098373 Vali Loss: 1.2878476 Test Loss: 0.5347160
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 6.403712511062622
Epoch: 46, Steps: 59 | Train Loss: 0.2090711 Vali Loss: 1.2888627 Test Loss: 0.5339191
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 6.323474168777466
Epoch: 47, Steps: 59 | Train Loss: 0.2083670 Vali Loss: 1.2890970 Test Loss: 0.5330639
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 6.480530738830566
Epoch: 48, Steps: 59 | Train Loss: 0.2078153 Vali Loss: 1.2840986 Test Loss: 0.5322391
Validation loss decreased (1.286115 --> 1.284099).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 6.641845226287842
Epoch: 49, Steps: 59 | Train Loss: 0.2072109 Vali Loss: 1.2770410 Test Loss: 0.5313438
Validation loss decreased (1.284099 --> 1.277041).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 6.2812910079956055
Epoch: 50, Steps: 59 | Train Loss: 0.2066077 Vali Loss: 1.2849860 Test Loss: 0.5305065
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 6.518732786178589
Epoch: 51, Steps: 59 | Train Loss: 0.2058660 Vali Loss: 1.2852654 Test Loss: 0.5299143
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 6.468115329742432
Epoch: 52, Steps: 59 | Train Loss: 0.2055006 Vali Loss: 1.2873322 Test Loss: 0.5291695
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 6.311938285827637
Epoch: 53, Steps: 59 | Train Loss: 0.2049301 Vali Loss: 1.2851806 Test Loss: 0.5285938
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 6.459158420562744
Epoch: 54, Steps: 59 | Train Loss: 0.2045298 Vali Loss: 1.2781779 Test Loss: 0.5280393
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 6.456678152084351
Epoch: 55, Steps: 59 | Train Loss: 0.2040628 Vali Loss: 1.2810440 Test Loss: 0.5272942
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 6.390099048614502
Epoch: 56, Steps: 59 | Train Loss: 0.2036157 Vali Loss: 1.2772171 Test Loss: 0.5267766
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 6.464991092681885
Epoch: 57, Steps: 59 | Train Loss: 0.2031902 Vali Loss: 1.2822988 Test Loss: 0.5261601
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 6.344061851501465
Epoch: 58, Steps: 59 | Train Loss: 0.2028453 Vali Loss: 1.2760515 Test Loss: 0.5256922
Validation loss decreased (1.277041 --> 1.276052).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 6.606367111206055
Epoch: 59, Steps: 59 | Train Loss: 0.2025645 Vali Loss: 1.2779034 Test Loss: 0.5253195
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 6.519862651824951
Epoch: 60, Steps: 59 | Train Loss: 0.2022512 Vali Loss: 1.2834539 Test Loss: 0.5247750
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 6.41245698928833
Epoch: 61, Steps: 59 | Train Loss: 0.2018190 Vali Loss: 1.2799642 Test Loss: 0.5243369
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 6.511848449707031
Epoch: 62, Steps: 59 | Train Loss: 0.2014358 Vali Loss: 1.2776861 Test Loss: 0.5239295
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 6.383653163909912
Epoch: 63, Steps: 59 | Train Loss: 0.2011530 Vali Loss: 1.2785984 Test Loss: 0.5235049
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 6.668132543563843
Epoch: 64, Steps: 59 | Train Loss: 0.2010147 Vali Loss: 1.2821239 Test Loss: 0.5231479
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 6.315101385116577
Epoch: 65, Steps: 59 | Train Loss: 0.2006971 Vali Loss: 1.2782907 Test Loss: 0.5228609
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 6.403678894042969
Epoch: 66, Steps: 59 | Train Loss: 0.2005129 Vali Loss: 1.2786059 Test Loss: 0.5225462
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 6.5586771965026855
Epoch: 67, Steps: 59 | Train Loss: 0.2001185 Vali Loss: 1.2786514 Test Loss: 0.5221902
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 6.62159538269043
Epoch: 68, Steps: 59 | Train Loss: 0.1999343 Vali Loss: 1.2842817 Test Loss: 0.5218654
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 6.541393280029297
Epoch: 69, Steps: 59 | Train Loss: 0.1997803 Vali Loss: 1.2813559 Test Loss: 0.5216197
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 6.238757133483887
Epoch: 70, Steps: 59 | Train Loss: 0.1995359 Vali Loss: 1.2769456 Test Loss: 0.5213789
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 6.504292011260986
Epoch: 71, Steps: 59 | Train Loss: 0.1993494 Vali Loss: 1.2768799 Test Loss: 0.5211242
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 6.339022397994995
Epoch: 72, Steps: 59 | Train Loss: 0.1991664 Vali Loss: 1.2783204 Test Loss: 0.5208662
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 6.756187200546265
Epoch: 73, Steps: 59 | Train Loss: 0.1990045 Vali Loss: 1.2768319 Test Loss: 0.5205795
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 6.461223602294922
Epoch: 74, Steps: 59 | Train Loss: 0.1988999 Vali Loss: 1.2776572 Test Loss: 0.5203811
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 6.340944766998291
Epoch: 75, Steps: 59 | Train Loss: 0.1986493 Vali Loss: 1.2734362 Test Loss: 0.5201335
Validation loss decreased (1.276052 --> 1.273436).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 6.236909627914429
Epoch: 76, Steps: 59 | Train Loss: 0.1985145 Vali Loss: 1.2792901 Test Loss: 0.5200080
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 5.392232418060303
Epoch: 77, Steps: 59 | Train Loss: 0.1983004 Vali Loss: 1.2808880 Test Loss: 0.5197843
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 6.581416845321655
Epoch: 78, Steps: 59 | Train Loss: 0.1982979 Vali Loss: 1.2808772 Test Loss: 0.5196060
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 6.345789432525635
Epoch: 79, Steps: 59 | Train Loss: 0.1981262 Vali Loss: 1.2758634 Test Loss: 0.5194389
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 6.48458456993103
Epoch: 80, Steps: 59 | Train Loss: 0.1980534 Vali Loss: 1.2798400 Test Loss: 0.5192770
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 6.2111101150512695
Epoch: 81, Steps: 59 | Train Loss: 0.1979127 Vali Loss: 1.2802489 Test Loss: 0.5191191
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 6.672624111175537
Epoch: 82, Steps: 59 | Train Loss: 0.1977413 Vali Loss: 1.2773033 Test Loss: 0.5189835
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 6.192975759506226
Epoch: 83, Steps: 59 | Train Loss: 0.1976391 Vali Loss: 1.2738794 Test Loss: 0.5188320
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 5.534982204437256
Epoch: 84, Steps: 59 | Train Loss: 0.1976005 Vali Loss: 1.2777162 Test Loss: 0.5187144
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 6.799372434616089
Epoch: 85, Steps: 59 | Train Loss: 0.1974345 Vali Loss: 1.2795929 Test Loss: 0.5185839
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 6.3025431632995605
Epoch: 86, Steps: 59 | Train Loss: 0.1974005 Vali Loss: 1.2753687 Test Loss: 0.5184652
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 5.9389472007751465
Epoch: 87, Steps: 59 | Train Loss: 0.1973534 Vali Loss: 1.2782542 Test Loss: 0.5183460
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 6.704826831817627
Epoch: 88, Steps: 59 | Train Loss: 0.1972047 Vali Loss: 1.2778521 Test Loss: 0.5182334
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 6.574589729309082
Epoch: 89, Steps: 59 | Train Loss: 0.1971691 Vali Loss: 1.2756938 Test Loss: 0.5181222
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 6.016477823257446
Epoch: 90, Steps: 59 | Train Loss: 0.1970900 Vali Loss: 1.2765031 Test Loss: 0.5180159
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 6.557929754257202
Epoch: 91, Steps: 59 | Train Loss: 0.1970001 Vali Loss: 1.2770748 Test Loss: 0.5179187
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 6.565989255905151
Epoch: 92, Steps: 59 | Train Loss: 0.1969312 Vali Loss: 1.2788630 Test Loss: 0.5178362
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 6.46390438079834
Epoch: 93, Steps: 59 | Train Loss: 0.1969516 Vali Loss: 1.2746587 Test Loss: 0.5177540
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 6.714304685592651
Epoch: 94, Steps: 59 | Train Loss: 0.1967492 Vali Loss: 1.2819296 Test Loss: 0.5176689
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 6.796715974807739
Epoch: 95, Steps: 59 | Train Loss: 0.1966529 Vali Loss: 1.2737546 Test Loss: 0.5175871
EarlyStopping counter: 20 out of 20
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 6.496383905410767
Epoch: 1, Steps: 59 | Train Loss: 0.4652638 Vali Loss: 1.2247998 Test Loss: 0.4670337
Validation loss decreased (inf --> 1.224800).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.981141805648804
Epoch: 2, Steps: 59 | Train Loss: 0.4455027 Vali Loss: 1.2080574 Test Loss: 0.4476643
Validation loss decreased (1.224800 --> 1.208057).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.08954119682312
Epoch: 3, Steps: 59 | Train Loss: 0.4377864 Vali Loss: 1.2048428 Test Loss: 0.4420428
Validation loss decreased (1.208057 --> 1.204843).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.317929029464722
Epoch: 4, Steps: 59 | Train Loss: 0.4351172 Vali Loss: 1.2052197 Test Loss: 0.4409074
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 6.125446557998657
Epoch: 5, Steps: 59 | Train Loss: 0.4335073 Vali Loss: 1.2045795 Test Loss: 0.4410383
Validation loss decreased (1.204843 --> 1.204579).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 6.330782175064087
Epoch: 6, Steps: 59 | Train Loss: 0.4333192 Vali Loss: 1.2122319 Test Loss: 0.4414076
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 6.382902145385742
Epoch: 7, Steps: 59 | Train Loss: 0.4324648 Vali Loss: 1.2143358 Test Loss: 0.4420772
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 6.73511266708374
Epoch: 8, Steps: 59 | Train Loss: 0.4325681 Vali Loss: 1.2087717 Test Loss: 0.4420984
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 6.2048351764678955
Epoch: 9, Steps: 59 | Train Loss: 0.4325391 Vali Loss: 1.2153779 Test Loss: 0.4424124
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 6.410949230194092
Epoch: 10, Steps: 59 | Train Loss: 0.4318488 Vali Loss: 1.2145078 Test Loss: 0.4425879
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.078760623931885
Epoch: 11, Steps: 59 | Train Loss: 0.4319504 Vali Loss: 1.2145828 Test Loss: 0.4425809
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.667418479919434
Epoch: 12, Steps: 59 | Train Loss: 0.4319925 Vali Loss: 1.2117500 Test Loss: 0.4428911
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 6.2800328731536865
Epoch: 13, Steps: 59 | Train Loss: 0.4315441 Vali Loss: 1.2132102 Test Loss: 0.4428841
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 6.666989326477051
Epoch: 14, Steps: 59 | Train Loss: 0.4313704 Vali Loss: 1.2174684 Test Loss: 0.4430098
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 6.889617681503296
Epoch: 15, Steps: 59 | Train Loss: 0.4311861 Vali Loss: 1.2158921 Test Loss: 0.4431837
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 6.424095153808594
Epoch: 16, Steps: 59 | Train Loss: 0.4312571 Vali Loss: 1.2196764 Test Loss: 0.4432298
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 6.675078392028809
Epoch: 17, Steps: 59 | Train Loss: 0.4313388 Vali Loss: 1.2156723 Test Loss: 0.4433155
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 6.536171197891235
Epoch: 18, Steps: 59 | Train Loss: 0.4313028 Vali Loss: 1.2160020 Test Loss: 0.4430470
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 6.335149049758911
Epoch: 19, Steps: 59 | Train Loss: 0.4306806 Vali Loss: 1.2168777 Test Loss: 0.4433252
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 6.6221699714660645
Epoch: 20, Steps: 59 | Train Loss: 0.4307414 Vali Loss: 1.2186403 Test Loss: 0.4434052
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 6.665601968765259
Epoch: 21, Steps: 59 | Train Loss: 0.4307923 Vali Loss: 1.2159235 Test Loss: 0.4434311
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 6.615238428115845
Epoch: 22, Steps: 59 | Train Loss: 0.4307527 Vali Loss: 1.2164193 Test Loss: 0.4435085
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 6.566100597381592
Epoch: 23, Steps: 59 | Train Loss: 0.4307338 Vali Loss: 1.2192949 Test Loss: 0.4435982
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 6.393719911575317
Epoch: 24, Steps: 59 | Train Loss: 0.4308469 Vali Loss: 1.2230604 Test Loss: 0.4434679
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 6.4623212814331055
Epoch: 25, Steps: 59 | Train Loss: 0.4306995 Vali Loss: 1.2188987 Test Loss: 0.4434865
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.439988911151886, mae:0.4390421509742737, rse:0.6314993500709534, corr:[0.25404385 0.26113236 0.2597179  0.25963956 0.25831416 0.25485215
 0.25262877 0.2527395  0.25287884 0.25239977 0.25241485 0.25292295
 0.2528862  0.25214455 0.25161013 0.25145695 0.25131392 0.25113872
 0.25073928 0.25019598 0.24996707 0.2500572  0.24985284 0.24947093
 0.24966706 0.25004756 0.24991284 0.24954313 0.24953426 0.24952865
 0.24917483 0.24869981 0.24861781 0.24868073 0.24838258 0.24793023
 0.2476488  0.24768128 0.2479182  0.2480005  0.24789275 0.24789485
 0.24839641 0.24892196 0.24869184 0.24822518 0.24862468 0.2495913
 0.24986342 0.24912196 0.24780744 0.24653232 0.24539584 0.2444128
 0.24373302 0.24308194 0.24230056 0.24195032 0.2419285  0.2418416
 0.2412907  0.24106036 0.24138416 0.24170856 0.24159788 0.24143691
 0.24177125 0.24221599 0.24243316 0.24220668 0.24175481 0.24156973
 0.24150096 0.24074434 0.23953186 0.23920216 0.23962493 0.23964934
 0.23901057 0.23857167 0.23852737 0.23816101 0.23738168 0.2370207
 0.23729828 0.23744217 0.23715648 0.23690046 0.23701525 0.23733725
 0.2374596  0.23727663 0.23693988 0.23694314 0.23743245 0.23833725
 0.23904093 0.23901562 0.23876993 0.23891677 0.23908506 0.23897295
 0.23884143 0.23867035 0.23844355 0.23839924 0.23822528 0.23770526
 0.23719904 0.23708849 0.2373325  0.23758855 0.23789681 0.23830697
 0.23847426 0.23817973 0.23789012 0.23800115 0.23818645 0.23811178
 0.23791303 0.23749264 0.23687986 0.2361421  0.2351764  0.23431417
 0.23432155 0.23484935 0.23432827 0.23286676 0.23194915 0.23252979
 0.23366603 0.233992   0.23351488 0.23318477 0.23349573 0.23400365
 0.2340654  0.23350938 0.23293969 0.23267525 0.23224398 0.23175076
 0.23177364 0.2320213  0.23147978 0.22972386 0.22821215 0.22807063
 0.22872008 0.22886494 0.22850545 0.22828963 0.22802675 0.22757451
 0.22726957 0.22722165 0.22727221 0.22743434 0.22778082 0.2280952
 0.2281217  0.22843273 0.22892757 0.22884478 0.22816478 0.2280957
 0.22885129 0.22913527 0.22867721 0.22843657 0.22828521 0.2274497
 0.2265474  0.22681992 0.22753903 0.22748931 0.22682521 0.22656655
 0.2264157  0.2258473  0.22538847 0.2257371  0.22635135 0.2268265
 0.22709331 0.22701353 0.22667769 0.22680892 0.2276256  0.22818704
 0.22763553 0.2265433  0.22574578 0.2253059  0.22490095 0.22420755
 0.2232474  0.22255519 0.22256857 0.22285962 0.22232366 0.2215101
 0.22175811 0.22322144 0.22390145 0.22336473 0.22288783 0.222948
 0.22292471 0.22270823 0.22278215 0.22284167 0.22257172 0.22261646
 0.22317284 0.22344986 0.2229483  0.2221613  0.22149797 0.22077747
 0.22049427 0.22084989 0.2210291  0.22029956 0.21935865 0.21919952
 0.21947283 0.21967953 0.21992916 0.21983775 0.21929103 0.21884948
 0.21866308 0.21827053 0.21788682 0.2181216  0.21862835 0.21812561
 0.21723524 0.21721984 0.21800663 0.2179909  0.21694835 0.21621998
 0.21666153 0.21761155 0.21811107 0.21787623 0.21729036 0.21687709
 0.21675307 0.21651731 0.21631543 0.2165294  0.21721734 0.21780366
 0.21808217 0.21788576 0.21738274 0.21688212 0.21709864 0.21775621
 0.2178712  0.2172199  0.21652685 0.21596912 0.21486324 0.2138132
 0.2136777  0.21361952 0.21302028 0.21291399 0.21355642 0.21417858
 0.21423519 0.2137942  0.21292087 0.21189114 0.21186386 0.21309365
 0.21355943 0.21244366 0.21208589 0.21337509 0.21417524 0.2132918
 0.21251962 0.21323001 0.21369463 0.21328047 0.21299529 0.21330494
 0.21306454 0.21258304 0.21249495 0.21213704 0.21109024 0.21056475
 0.21064672 0.21011445 0.21022867 0.21167058 0.2130187  0.21248473
 0.21174978 0.21179447 0.21163023 0.21047561 0.21047193 0.21173938
 0.2117717  0.21044067 0.20993678 0.20990352 0.20873621 0.20727167
 0.20743397 0.20771235 0.207356   0.20763849 0.20760266 0.20598255
 0.20444697 0.20422347 0.20353661 0.20271267 0.20282197 0.20293716
 0.2017304  0.20263979 0.20474488 0.20065413 0.19909531 0.21330084]
