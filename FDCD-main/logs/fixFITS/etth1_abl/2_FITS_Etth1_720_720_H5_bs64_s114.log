Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  48787200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9488608837127686
Epoch: 1, Steps: 56 | Train Loss: 0.8500241 Vali Loss: 2.1584811 Test Loss: 0.9179301
Validation loss decreased (inf --> 2.158481).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8405795097351074
Epoch: 2, Steps: 56 | Train Loss: 0.6665536 Vali Loss: 1.9439707 Test Loss: 0.7988142
Validation loss decreased (2.158481 --> 1.943971).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.7283141613006592
Epoch: 3, Steps: 56 | Train Loss: 0.5866258 Vali Loss: 1.8558017 Test Loss: 0.7517191
Validation loss decreased (1.943971 --> 1.855802).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.701378345489502
Epoch: 4, Steps: 56 | Train Loss: 0.5453304 Vali Loss: 1.8147414 Test Loss: 0.7266480
Validation loss decreased (1.855802 --> 1.814741).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7144458293914795
Epoch: 5, Steps: 56 | Train Loss: 0.5187674 Vali Loss: 1.7861726 Test Loss: 0.7092136
Validation loss decreased (1.814741 --> 1.786173).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.606438398361206
Epoch: 6, Steps: 56 | Train Loss: 0.4989087 Vali Loss: 1.7649906 Test Loss: 0.6955765
Validation loss decreased (1.786173 --> 1.764991).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.6276047229766846
Epoch: 7, Steps: 56 | Train Loss: 0.4822639 Vali Loss: 1.7466089 Test Loss: 0.6832268
Validation loss decreased (1.764991 --> 1.746609).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.7172081470489502
Epoch: 8, Steps: 56 | Train Loss: 0.4684833 Vali Loss: 1.7301512 Test Loss: 0.6718956
Validation loss decreased (1.746609 --> 1.730151).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7551820278167725
Epoch: 9, Steps: 56 | Train Loss: 0.4561546 Vali Loss: 1.7076516 Test Loss: 0.6606207
Validation loss decreased (1.730151 --> 1.707652).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.7156808376312256
Epoch: 10, Steps: 56 | Train Loss: 0.4456720 Vali Loss: 1.7059996 Test Loss: 0.6514239
Validation loss decreased (1.707652 --> 1.706000).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.859250545501709
Epoch: 11, Steps: 56 | Train Loss: 0.4363164 Vali Loss: 1.6829023 Test Loss: 0.6420156
Validation loss decreased (1.706000 --> 1.682902).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.7525660991668701
Epoch: 12, Steps: 56 | Train Loss: 0.4279712 Vali Loss: 1.6746235 Test Loss: 0.6343434
Validation loss decreased (1.682902 --> 1.674623).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6611781120300293
Epoch: 13, Steps: 56 | Train Loss: 0.4204547 Vali Loss: 1.6719128 Test Loss: 0.6265706
Validation loss decreased (1.674623 --> 1.671913).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.8704521656036377
Epoch: 14, Steps: 56 | Train Loss: 0.4137388 Vali Loss: 1.6572131 Test Loss: 0.6195869
Validation loss decreased (1.671913 --> 1.657213).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6884939670562744
Epoch: 15, Steps: 56 | Train Loss: 0.4075470 Vali Loss: 1.6524770 Test Loss: 0.6136820
Validation loss decreased (1.657213 --> 1.652477).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.5432002544403076
Epoch: 16, Steps: 56 | Train Loss: 0.4020772 Vali Loss: 1.6417238 Test Loss: 0.6064141
Validation loss decreased (1.652477 --> 1.641724).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.5311346054077148
Epoch: 17, Steps: 56 | Train Loss: 0.3970805 Vali Loss: 1.6376116 Test Loss: 0.6006646
Validation loss decreased (1.641724 --> 1.637612).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8444314002990723
Epoch: 18, Steps: 56 | Train Loss: 0.3925948 Vali Loss: 1.6269881 Test Loss: 0.5955242
Validation loss decreased (1.637612 --> 1.626988).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6517481803894043
Epoch: 19, Steps: 56 | Train Loss: 0.3884750 Vali Loss: 1.6262512 Test Loss: 0.5907844
Validation loss decreased (1.626988 --> 1.626251).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.697723627090454
Epoch: 20, Steps: 56 | Train Loss: 0.3847317 Vali Loss: 1.6218792 Test Loss: 0.5862482
Validation loss decreased (1.626251 --> 1.621879).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.639922857284546
Epoch: 21, Steps: 56 | Train Loss: 0.3811335 Vali Loss: 1.6055794 Test Loss: 0.5818498
Validation loss decreased (1.621879 --> 1.605579).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.547708511352539
Epoch: 22, Steps: 56 | Train Loss: 0.3781207 Vali Loss: 1.6050489 Test Loss: 0.5776817
Validation loss decreased (1.605579 --> 1.605049).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5437729358673096
Epoch: 23, Steps: 56 | Train Loss: 0.3751916 Vali Loss: 1.6028416 Test Loss: 0.5738787
Validation loss decreased (1.605049 --> 1.602842).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.5123026371002197
Epoch: 24, Steps: 56 | Train Loss: 0.3723664 Vali Loss: 1.5991689 Test Loss: 0.5701019
Validation loss decreased (1.602842 --> 1.599169).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.6357934474945068
Epoch: 25, Steps: 56 | Train Loss: 0.3698410 Vali Loss: 1.5922637 Test Loss: 0.5668866
Validation loss decreased (1.599169 --> 1.592264).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.6144437789916992
Epoch: 26, Steps: 56 | Train Loss: 0.3675767 Vali Loss: 1.5933378 Test Loss: 0.5639362
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5297586917877197
Epoch: 27, Steps: 56 | Train Loss: 0.3654212 Vali Loss: 1.5839249 Test Loss: 0.5609009
Validation loss decreased (1.592264 --> 1.583925).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6558055877685547
Epoch: 28, Steps: 56 | Train Loss: 0.3633202 Vali Loss: 1.5850468 Test Loss: 0.5581009
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.5200300216674805
Epoch: 29, Steps: 56 | Train Loss: 0.3614348 Vali Loss: 1.5829923 Test Loss: 0.5554431
Validation loss decreased (1.583925 --> 1.582992).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.7652266025543213
Epoch: 30, Steps: 56 | Train Loss: 0.3598087 Vali Loss: 1.5766075 Test Loss: 0.5530702
Validation loss decreased (1.582992 --> 1.576607).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6873488426208496
Epoch: 31, Steps: 56 | Train Loss: 0.3581089 Vali Loss: 1.5779357 Test Loss: 0.5506358
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.676325798034668
Epoch: 32, Steps: 56 | Train Loss: 0.3567443 Vali Loss: 1.5725853 Test Loss: 0.5486900
Validation loss decreased (1.576607 --> 1.572585).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.8640100955963135
Epoch: 33, Steps: 56 | Train Loss: 0.3552455 Vali Loss: 1.5698130 Test Loss: 0.5464835
Validation loss decreased (1.572585 --> 1.569813).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7129456996917725
Epoch: 34, Steps: 56 | Train Loss: 0.3540234 Vali Loss: 1.5665536 Test Loss: 0.5446452
Validation loss decreased (1.569813 --> 1.566554).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.5859663486480713
Epoch: 35, Steps: 56 | Train Loss: 0.3528655 Vali Loss: 1.5607852 Test Loss: 0.5429721
Validation loss decreased (1.566554 --> 1.560785).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.5535814762115479
Epoch: 36, Steps: 56 | Train Loss: 0.3514836 Vali Loss: 1.5705003 Test Loss: 0.5411806
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5723068714141846
Epoch: 37, Steps: 56 | Train Loss: 0.3503659 Vali Loss: 1.5598848 Test Loss: 0.5395753
Validation loss decreased (1.560785 --> 1.559885).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7060024738311768
Epoch: 38, Steps: 56 | Train Loss: 0.3492500 Vali Loss: 1.5601779 Test Loss: 0.5380366
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7262721061706543
Epoch: 39, Steps: 56 | Train Loss: 0.3484078 Vali Loss: 1.5591042 Test Loss: 0.5366703
Validation loss decreased (1.559885 --> 1.559104).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7882695198059082
Epoch: 40, Steps: 56 | Train Loss: 0.3475049 Vali Loss: 1.5540299 Test Loss: 0.5352942
Validation loss decreased (1.559104 --> 1.554030).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6720457077026367
Epoch: 41, Steps: 56 | Train Loss: 0.3466940 Vali Loss: 1.5530262 Test Loss: 0.5340066
Validation loss decreased (1.554030 --> 1.553026).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.5007379055023193
Epoch: 42, Steps: 56 | Train Loss: 0.3458203 Vali Loss: 1.5545793 Test Loss: 0.5327566
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.6717960834503174
Epoch: 43, Steps: 56 | Train Loss: 0.3450040 Vali Loss: 1.5562329 Test Loss: 0.5316504
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.840461254119873
Epoch: 44, Steps: 56 | Train Loss: 0.3445938 Vali Loss: 1.5455418 Test Loss: 0.5305610
Validation loss decreased (1.553026 --> 1.545542).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7817492485046387
Epoch: 45, Steps: 56 | Train Loss: 0.3437297 Vali Loss: 1.5405087 Test Loss: 0.5295728
Validation loss decreased (1.545542 --> 1.540509).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.9178235530853271
Epoch: 46, Steps: 56 | Train Loss: 0.3430976 Vali Loss: 1.5498829 Test Loss: 0.5285464
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.8829069137573242
Epoch: 47, Steps: 56 | Train Loss: 0.3423918 Vali Loss: 1.5468626 Test Loss: 0.5276146
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9711525440216064
Epoch: 48, Steps: 56 | Train Loss: 0.3418940 Vali Loss: 1.5415204 Test Loss: 0.5266384
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9311580657958984
Epoch: 49, Steps: 56 | Train Loss: 0.3414376 Vali Loss: 1.5390588 Test Loss: 0.5258279
Validation loss decreased (1.540509 --> 1.539059).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.8737082481384277
Epoch: 50, Steps: 56 | Train Loss: 0.3408436 Vali Loss: 1.5419340 Test Loss: 0.5250888
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.8234107494354248
Epoch: 51, Steps: 56 | Train Loss: 0.3403521 Vali Loss: 1.5372752 Test Loss: 0.5243515
Validation loss decreased (1.539059 --> 1.537275).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.9216625690460205
Epoch: 52, Steps: 56 | Train Loss: 0.3396893 Vali Loss: 1.5363997 Test Loss: 0.5236096
Validation loss decreased (1.537275 --> 1.536400).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.871919870376587
Epoch: 53, Steps: 56 | Train Loss: 0.3394538 Vali Loss: 1.5361720 Test Loss: 0.5229135
Validation loss decreased (1.536400 --> 1.536172).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.9891071319580078
Epoch: 54, Steps: 56 | Train Loss: 0.3390390 Vali Loss: 1.5487430 Test Loss: 0.5222960
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.7567179203033447
Epoch: 55, Steps: 56 | Train Loss: 0.3387247 Vali Loss: 1.5393851 Test Loss: 0.5216888
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7347686290740967
Epoch: 56, Steps: 56 | Train Loss: 0.3383334 Vali Loss: 1.5385094 Test Loss: 0.5211374
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.8760387897491455
Epoch: 57, Steps: 56 | Train Loss: 0.3380410 Vali Loss: 1.5342793 Test Loss: 0.5205644
Validation loss decreased (1.536172 --> 1.534279).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.663949966430664
Epoch: 58, Steps: 56 | Train Loss: 0.3376411 Vali Loss: 1.5392983 Test Loss: 0.5200596
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.940782070159912
Epoch: 59, Steps: 56 | Train Loss: 0.3373639 Vali Loss: 1.5367661 Test Loss: 0.5195456
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.8191556930541992
Epoch: 60, Steps: 56 | Train Loss: 0.3370019 Vali Loss: 1.5323050 Test Loss: 0.5190718
Validation loss decreased (1.534279 --> 1.532305).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6410112380981445
Epoch: 61, Steps: 56 | Train Loss: 0.3366096 Vali Loss: 1.5353152 Test Loss: 0.5186689
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.592376470565796
Epoch: 62, Steps: 56 | Train Loss: 0.3362631 Vali Loss: 1.5371815 Test Loss: 0.5181876
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.6645679473876953
Epoch: 63, Steps: 56 | Train Loss: 0.3361319 Vali Loss: 1.5339313 Test Loss: 0.5178104
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.6381502151489258
Epoch: 64, Steps: 56 | Train Loss: 0.3358169 Vali Loss: 1.5303572 Test Loss: 0.5174622
Validation loss decreased (1.532305 --> 1.530357).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.963514804840088
Epoch: 65, Steps: 56 | Train Loss: 0.3356292 Vali Loss: 1.5315603 Test Loss: 0.5170824
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.7550675868988037
Epoch: 66, Steps: 56 | Train Loss: 0.3354707 Vali Loss: 1.5325401 Test Loss: 0.5167732
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.888167142868042
Epoch: 67, Steps: 56 | Train Loss: 0.3353268 Vali Loss: 1.5330834 Test Loss: 0.5164278
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.8032207489013672
Epoch: 68, Steps: 56 | Train Loss: 0.3349661 Vali Loss: 1.5304431 Test Loss: 0.5161235
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.7140765190124512
Epoch: 69, Steps: 56 | Train Loss: 0.3349605 Vali Loss: 1.5272361 Test Loss: 0.5157979
Validation loss decreased (1.530357 --> 1.527236).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.7852518558502197
Epoch: 70, Steps: 56 | Train Loss: 0.3347026 Vali Loss: 1.5265080 Test Loss: 0.5155249
Validation loss decreased (1.527236 --> 1.526508).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.9112646579742432
Epoch: 71, Steps: 56 | Train Loss: 0.3346173 Vali Loss: 1.5342543 Test Loss: 0.5152639
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.8584156036376953
Epoch: 72, Steps: 56 | Train Loss: 0.3343797 Vali Loss: 1.5293057 Test Loss: 0.5149830
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.5945484638214111
Epoch: 73, Steps: 56 | Train Loss: 0.3341232 Vali Loss: 1.5291643 Test Loss: 0.5147725
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.667754888534546
Epoch: 74, Steps: 56 | Train Loss: 0.3340795 Vali Loss: 1.5287251 Test Loss: 0.5145321
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.8180272579193115
Epoch: 75, Steps: 56 | Train Loss: 0.3338931 Vali Loss: 1.5315853 Test Loss: 0.5143247
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.8614330291748047
Epoch: 76, Steps: 56 | Train Loss: 0.3336754 Vali Loss: 1.5260955 Test Loss: 0.5141047
Validation loss decreased (1.526508 --> 1.526096).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.9397149085998535
Epoch: 77, Steps: 56 | Train Loss: 0.3338166 Vali Loss: 1.5268698 Test Loss: 0.5139124
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.8487911224365234
Epoch: 78, Steps: 56 | Train Loss: 0.3336457 Vali Loss: 1.5253782 Test Loss: 0.5137448
Validation loss decreased (1.526096 --> 1.525378).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.6217968463897705
Epoch: 79, Steps: 56 | Train Loss: 0.3334117 Vali Loss: 1.5291376 Test Loss: 0.5135624
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.859304666519165
Epoch: 80, Steps: 56 | Train Loss: 0.3332444 Vali Loss: 1.5309310 Test Loss: 0.5133964
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.7950255870819092
Epoch: 81, Steps: 56 | Train Loss: 0.3334126 Vali Loss: 1.5302987 Test Loss: 0.5132208
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.8654818534851074
Epoch: 82, Steps: 56 | Train Loss: 0.3331106 Vali Loss: 1.5287364 Test Loss: 0.5130646
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.7886335849761963
Epoch: 83, Steps: 56 | Train Loss: 0.3329697 Vali Loss: 1.5333424 Test Loss: 0.5129151
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.7714831829071045
Epoch: 84, Steps: 56 | Train Loss: 0.3328268 Vali Loss: 1.5316848 Test Loss: 0.5127869
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.8372693061828613
Epoch: 85, Steps: 56 | Train Loss: 0.3330378 Vali Loss: 1.5308919 Test Loss: 0.5126529
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.649409532546997
Epoch: 86, Steps: 56 | Train Loss: 0.3329076 Vali Loss: 1.5271599 Test Loss: 0.5125221
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.7812151908874512
Epoch: 87, Steps: 56 | Train Loss: 0.3328766 Vali Loss: 1.5257711 Test Loss: 0.5124009
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.9169731140136719
Epoch: 88, Steps: 56 | Train Loss: 0.3327295 Vali Loss: 1.5255275 Test Loss: 0.5122994
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.7590038776397705
Epoch: 89, Steps: 56 | Train Loss: 0.3326365 Vali Loss: 1.5277333 Test Loss: 0.5121856
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.6277499198913574
Epoch: 90, Steps: 56 | Train Loss: 0.3325333 Vali Loss: 1.5254880 Test Loss: 0.5120891
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.644129753112793
Epoch: 91, Steps: 56 | Train Loss: 0.3323005 Vali Loss: 1.5260589 Test Loss: 0.5119909
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.8236191272735596
Epoch: 92, Steps: 56 | Train Loss: 0.3323995 Vali Loss: 1.5272436 Test Loss: 0.5119005
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.8125638961791992
Epoch: 93, Steps: 56 | Train Loss: 0.3323385 Vali Loss: 1.5312599 Test Loss: 0.5118089
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.652137279510498
Epoch: 94, Steps: 56 | Train Loss: 0.3322750 Vali Loss: 1.5252520 Test Loss: 0.5117270
Validation loss decreased (1.525378 --> 1.525252).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.5739648342132568
Epoch: 95, Steps: 56 | Train Loss: 0.3323067 Vali Loss: 1.5289136 Test Loss: 0.5116506
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.6243617534637451
Epoch: 96, Steps: 56 | Train Loss: 0.3320773 Vali Loss: 1.5245390 Test Loss: 0.5115720
Validation loss decreased (1.525252 --> 1.524539).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.8152031898498535
Epoch: 97, Steps: 56 | Train Loss: 0.3322410 Vali Loss: 1.5243115 Test Loss: 0.5115014
Validation loss decreased (1.524539 --> 1.524312).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.7633562088012695
Epoch: 98, Steps: 56 | Train Loss: 0.3320791 Vali Loss: 1.5248803 Test Loss: 0.5114300
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.815608024597168
Epoch: 99, Steps: 56 | Train Loss: 0.3321134 Vali Loss: 1.5310746 Test Loss: 0.5113580
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.70420503616333
Epoch: 100, Steps: 56 | Train Loss: 0.3318718 Vali Loss: 1.5251827 Test Loss: 0.5113079
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=165, out_features=330, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  48787200.0
params:  54780.0
Trainable parameters:  54780
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.5890710353851318
Epoch: 1, Steps: 56 | Train Loss: 0.5880612 Vali Loss: 1.4879407 Test Loss: 0.4784424
Validation loss decreased (inf --> 1.487941).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5194261074066162
Epoch: 2, Steps: 56 | Train Loss: 0.5731658 Vali Loss: 1.4617450 Test Loss: 0.4579722
Validation loss decreased (1.487941 --> 1.461745).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8011398315429688
Epoch: 3, Steps: 56 | Train Loss: 0.5640842 Vali Loss: 1.4553528 Test Loss: 0.4461334
Validation loss decreased (1.461745 --> 1.455353).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6211366653442383
Epoch: 4, Steps: 56 | Train Loss: 0.5583624 Vali Loss: 1.4435546 Test Loss: 0.4396481
Validation loss decreased (1.455353 --> 1.443555).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.606121301651001
Epoch: 5, Steps: 56 | Train Loss: 0.5550155 Vali Loss: 1.4414275 Test Loss: 0.4359268
Validation loss decreased (1.443555 --> 1.441427).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.668473720550537
Epoch: 6, Steps: 56 | Train Loss: 0.5530260 Vali Loss: 1.4395490 Test Loss: 0.4342285
Validation loss decreased (1.441427 --> 1.439549).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.7629845142364502
Epoch: 7, Steps: 56 | Train Loss: 0.5513515 Vali Loss: 1.4402783 Test Loss: 0.4334985
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.7198965549468994
Epoch: 8, Steps: 56 | Train Loss: 0.5505819 Vali Loss: 1.4367933 Test Loss: 0.4331924
Validation loss decreased (1.439549 --> 1.436793).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.6250169277191162
Epoch: 9, Steps: 56 | Train Loss: 0.5501008 Vali Loss: 1.4380668 Test Loss: 0.4334151
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.5806760787963867
Epoch: 10, Steps: 56 | Train Loss: 0.5497113 Vali Loss: 1.4375782 Test Loss: 0.4336470
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.6993544101715088
Epoch: 11, Steps: 56 | Train Loss: 0.5493876 Vali Loss: 1.4404657 Test Loss: 0.4338697
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.7041077613830566
Epoch: 12, Steps: 56 | Train Loss: 0.5490993 Vali Loss: 1.4357996 Test Loss: 0.4340394
Validation loss decreased (1.436793 --> 1.435800).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.8085269927978516
Epoch: 13, Steps: 56 | Train Loss: 0.5484297 Vali Loss: 1.4378459 Test Loss: 0.4341891
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6721956729888916
Epoch: 14, Steps: 56 | Train Loss: 0.5485168 Vali Loss: 1.4361289 Test Loss: 0.4345908
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6105382442474365
Epoch: 15, Steps: 56 | Train Loss: 0.5486033 Vali Loss: 1.4428288 Test Loss: 0.4345577
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7267937660217285
Epoch: 16, Steps: 56 | Train Loss: 0.5482294 Vali Loss: 1.4364897 Test Loss: 0.4347378
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.5967507362365723
Epoch: 17, Steps: 56 | Train Loss: 0.5485735 Vali Loss: 1.4417421 Test Loss: 0.4349087
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6523022651672363
Epoch: 18, Steps: 56 | Train Loss: 0.5480614 Vali Loss: 1.4449686 Test Loss: 0.4349323
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.828002691268921
Epoch: 19, Steps: 56 | Train Loss: 0.5482125 Vali Loss: 1.4363397 Test Loss: 0.4350858
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.817443609237671
Epoch: 20, Steps: 56 | Train Loss: 0.5481842 Vali Loss: 1.4414160 Test Loss: 0.4351558
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.6798391342163086
Epoch: 21, Steps: 56 | Train Loss: 0.5477152 Vali Loss: 1.4429150 Test Loss: 0.4351867
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.6713852882385254
Epoch: 22, Steps: 56 | Train Loss: 0.5474605 Vali Loss: 1.4470615 Test Loss: 0.4352503
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8477668762207031
Epoch: 23, Steps: 56 | Train Loss: 0.5474309 Vali Loss: 1.4460126 Test Loss: 0.4353574
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.7024471759796143
Epoch: 24, Steps: 56 | Train Loss: 0.5473198 Vali Loss: 1.4439456 Test Loss: 0.4353777
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.7347960472106934
Epoch: 25, Steps: 56 | Train Loss: 0.5476087 Vali Loss: 1.4422526 Test Loss: 0.4354722
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7386596202850342
Epoch: 26, Steps: 56 | Train Loss: 0.5475314 Vali Loss: 1.4401917 Test Loss: 0.4355127
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6471168994903564
Epoch: 27, Steps: 56 | Train Loss: 0.5471906 Vali Loss: 1.4451908 Test Loss: 0.4355527
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.523193359375
Epoch: 28, Steps: 56 | Train Loss: 0.5474062 Vali Loss: 1.4436784 Test Loss: 0.4355051
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.5638515949249268
Epoch: 29, Steps: 56 | Train Loss: 0.5471852 Vali Loss: 1.4440765 Test Loss: 0.4356708
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.695892572402954
Epoch: 30, Steps: 56 | Train Loss: 0.5473500 Vali Loss: 1.4457524 Test Loss: 0.4357601
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6729047298431396
Epoch: 31, Steps: 56 | Train Loss: 0.5474302 Vali Loss: 1.4467357 Test Loss: 0.4357679
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.831477403640747
Epoch: 32, Steps: 56 | Train Loss: 0.5471766 Vali Loss: 1.4466590 Test Loss: 0.4358166
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4331030249595642, mae:0.45596280694007874, rse:0.630010187625885, corr:[0.22386532 0.2329607  0.23269616 0.231759   0.23179796 0.23088816
 0.22966218 0.22969492 0.230529   0.23136003 0.23083429 0.22954226
 0.22915903 0.2295756  0.22990097 0.22979568 0.22934717 0.22901188
 0.22878587 0.228699   0.22874737 0.22891065 0.22884539 0.22912067
 0.22980303 0.23038942 0.23033029 0.22991115 0.22982544 0.22990248
 0.22968936 0.22903123 0.22851305 0.22845194 0.22845213 0.22811495
 0.22767581 0.2273669  0.22740331 0.22754881 0.22774975 0.22776544
 0.22772016 0.22781825 0.22820115 0.22872654 0.22922839 0.2294794
 0.22934702 0.2293113  0.2291327  0.22847223 0.22733389 0.22601858
 0.2252247  0.2249414  0.22489674 0.22454333 0.22366497 0.22302859
 0.2229103  0.22304066 0.22296192 0.2227447  0.22246778 0.22234832
 0.22253177 0.2227274  0.2226856  0.22255185 0.22254503 0.2229162
 0.22309129 0.22268891 0.2217714  0.22108059 0.22083455 0.22060026
 0.22021581 0.21989919 0.21982288 0.21985582 0.21951887 0.21878985
 0.21809949 0.21767339 0.21748525 0.21738741 0.21729763 0.217273
 0.21719146 0.21723872 0.21750164 0.21793765 0.21840663 0.21912915
 0.22025546 0.22145806 0.22216678 0.2222994  0.22190152 0.2214443
 0.22134288 0.22153041 0.22154948 0.22125721 0.22083919 0.22054136
 0.22041261 0.22023365 0.22015482 0.22030687 0.2205113  0.22049414
 0.22041437 0.22038092 0.22037996 0.22026794 0.22019784 0.22043039
 0.22071558 0.22044754 0.21953993 0.21873821 0.21832162 0.21808572
 0.21786264 0.2177657  0.21776986 0.21786806 0.2177217  0.21721679
 0.21664846 0.21624844 0.21610244 0.21612859 0.21621694 0.2161761
 0.21602294 0.21584436 0.21593563 0.21628103 0.21645443 0.21632712
 0.21602067 0.21562035 0.2152915  0.21483274 0.21423417 0.21372665
 0.21365799 0.21400513 0.21407597 0.21357913 0.21297097 0.2128777
 0.21311045 0.21310282 0.2127925  0.21262094 0.2127493  0.21300945
 0.21318804 0.21343948 0.21378988 0.21400486 0.21375869 0.21365902
 0.21400644 0.21459451 0.2149774  0.21515359 0.21499786 0.2146085
 0.21427342 0.21412677 0.21401387 0.21404794 0.21414785 0.21413179
 0.21395336 0.21380872 0.21392852 0.21427564 0.21461114 0.21467526
 0.21445575 0.21424432 0.21440016 0.2147603  0.21500024 0.21493691
 0.21466143 0.21429256 0.21369624 0.21280767 0.21187077 0.21126948
 0.21125081 0.21162271 0.21171951 0.21136217 0.21087274 0.21068609
 0.21073423 0.21084154 0.2109291  0.21107225 0.21119764 0.21112889
 0.21097302 0.21093963 0.21109591 0.21104471 0.21082498 0.21082927
 0.21100551 0.21105435 0.21074808 0.21041149 0.2103397  0.2103145
 0.21016863 0.20984133 0.20949598 0.20921169 0.20878924 0.2082303
 0.2078959  0.20795274 0.20816939 0.20807752 0.20762128 0.20734508
 0.20758924 0.20805545 0.20836015 0.20843537 0.20861225 0.20889474
 0.20919101 0.20937055 0.2094461  0.20927276 0.20859425 0.20761758
 0.20717931 0.20765005 0.2083997  0.20847398 0.20783639 0.20716105
 0.20702375 0.20723024 0.20743085 0.20735997 0.2071911  0.20715776
 0.20723024 0.20709728 0.20690213 0.20678288 0.20684762 0.20715836
 0.20740137 0.20721267 0.20667559 0.20624666 0.20607239 0.20597948
 0.20579733 0.20548339 0.20507662 0.20485151 0.20464763 0.20446126
 0.20453054 0.20494048 0.20535496 0.2056331  0.20587188 0.20617336
 0.20623332 0.20599933 0.20580204 0.2059562  0.20643218 0.20698445
 0.20724706 0.20736453 0.20764627 0.20798254 0.20787922 0.2075383
 0.2074882  0.20779663 0.20794407 0.20779186 0.20767473 0.2078428
 0.20803553 0.20784847 0.20766261 0.20787875 0.20832872 0.20854862
 0.20850417 0.20841342 0.2085028  0.20853366 0.20822248 0.2079987
 0.2083297  0.2088744  0.20887415 0.20816219 0.20717661 0.20667842
 0.20682846 0.20706981 0.20691846 0.20647159 0.20609127 0.20593889
 0.2058832  0.20584628 0.20578265 0.20587951 0.20585445 0.2057721
 0.20564584 0.20547983 0.20533095 0.20518494 0.20499319 0.20473266
 0.20457846 0.20458175 0.20471467 0.20473123 0.2043141  0.20353311
 0.20288883 0.2026906  0.20288126 0.20322885 0.20336516 0.20316516
 0.20280056 0.20277856 0.20318444 0.20366792 0.2038323  0.2040325
 0.20442718 0.20482455 0.20490961 0.20462106 0.2039338  0.20337111
 0.20308658 0.20293014 0.20295674 0.20312113 0.2029054  0.2019304
 0.20066135 0.19999737 0.20026936 0.20090637 0.20120685 0.20102517
 0.20057784 0.20007136 0.19954988 0.1990201  0.19870217 0.19855943
 0.19840744 0.19818568 0.19834964 0.19924062 0.20012623 0.20064418
 0.20091492 0.20152591 0.20255315 0.2032594  0.20292406 0.20165631
 0.2005575  0.20014848 0.19998246 0.19955614 0.19884425 0.19847833
 0.1986676  0.19909132 0.19938965 0.19916593 0.19857879 0.19853793
 0.1994753  0.2007871  0.20141432 0.20097531 0.20020624 0.20030977
 0.20110723 0.20141438 0.2010271  0.20095392 0.20158643 0.2018456
 0.20116617 0.20013902 0.1998475  0.20024973 0.20036031 0.1996973
 0.19897614 0.19897169 0.19911797 0.19889215 0.19861978 0.19908352
 0.2001433  0.20091182 0.20081325 0.20087476 0.20140828 0.20208041
 0.20233722 0.20206793 0.2017644  0.2016755  0.20125802 0.20031583
 0.1995822  0.19971919 0.20017642 0.20040658 0.20026363 0.20022136
 0.20058553 0.20108463 0.2012898  0.20096602 0.20047747 0.2005319
 0.201233   0.20175797 0.20143777 0.20062053 0.20040971 0.20161556
 0.20318896 0.20373517 0.20325364 0.20288864 0.20299831 0.20266268
 0.2014747  0.20015737 0.19975296 0.20033075 0.20079091 0.20076917
 0.20075195 0.2010516  0.20113145 0.20061201 0.20003836 0.20026493
 0.20087945 0.20087719 0.2001411  0.19947842 0.1998375  0.20057155
 0.20088503 0.20083849 0.20070882 0.20041054 0.1996536  0.19883947
 0.19887877 0.19973177 0.20024692 0.20010403 0.1997298  0.19960244
 0.19954199 0.19921365 0.19889712 0.19875914 0.19858888 0.19819921
 0.19793028 0.19816366 0.19866228 0.19884242 0.198734   0.19915308
 0.20036322 0.20173721 0.20234083 0.20222393 0.20185608 0.2013141
 0.20086855 0.20080402 0.20095016 0.20085686 0.20046306 0.2002968
 0.20074429 0.20140909 0.2015233  0.20066153 0.19981237 0.19986263
 0.20051754 0.20088771 0.2009227  0.20091031 0.20157161 0.20207624
 0.20245767 0.20265245 0.20262048 0.20221815 0.20125611 0.20054309
 0.2004524  0.2005495  0.20014589 0.19985569 0.20031223 0.201103
 0.20105869 0.20002285 0.19930594 0.19942935 0.19980724 0.19975922
 0.19987941 0.20075561 0.20151864 0.20093885 0.19933136 0.19834049
 0.1986737  0.19944151 0.19928408 0.19816811 0.19682571 0.19566074
 0.19481578 0.19465633 0.19517983 0.1955378  0.19487517 0.19363505
 0.1930805  0.19351593 0.19419697 0.19401184 0.19357587 0.19342528
 0.19323269 0.19234347 0.1918049  0.19240162 0.19342896 0.19318064
 0.19165985 0.19089496 0.19172084 0.19316328 0.19326404 0.19188015
 0.19076787 0.19054995 0.1907074  0.19065593 0.1906689  0.1904671
 0.18974338 0.18860714 0.18839741 0.1892355  0.1898845  0.1893896
 0.18871364 0.18926527 0.1907026  0.19132061 0.1903776  0.18897732
 0.18828996 0.18803053 0.18774636 0.18775116 0.1880451  0.18739668
 0.18526849 0.18306197 0.18249464 0.18364055 0.18461879 0.18415575
 0.18304393 0.18272127 0.18343465 0.18402773 0.18374307 0.18284997
 0.18206432 0.1819198  0.18270917 0.18401453 0.18496606 0.18498866
 0.18461172 0.184726   0.18498735 0.18446973 0.18292299 0.18134922
 0.18078947 0.18073976 0.18022314 0.17962421 0.17968567 0.1802218
 0.18014863 0.17935407 0.17898014 0.17922941 0.17922342 0.17867613
 0.1783609  0.17903282 0.18001124 0.18026315 0.18013433 0.18042737
 0.18080552 0.18060741 0.17989935 0.17949517 0.17879446 0.17708051
 0.17519133 0.17462733 0.1753528  0.17537083 0.1739979  0.17288457
 0.17347808 0.17461635 0.17464244 0.17375588 0.17377098 0.17498296
 0.17583026 0.1754788  0.17478372 0.17476538 0.17469968 0.17383014
 0.17252064 0.17150244 0.17009017 0.16762762 0.16529264 0.16438828
 0.16528091 0.1659666  0.16557491 0.16481839 0.16480084 0.16480964
 0.16398485 0.16322337 0.16379309 0.16434698 0.16351758 0.16255262
 0.16320436 0.16495651 0.16565564 0.16517292 0.16540381 0.16633384
 0.16574848 0.16394038 0.16311935 0.16426791 0.16395548 0.16087827
 0.15902822 0.16096364 0.16363163 0.16269039 0.15995543 0.15957566
 0.16117348 0.16032572 0.15840958 0.15970235 0.1636888  0.16403386
 0.1615038  0.16284704 0.16698484 0.16292684 0.15903997 0.1788136 ]
