Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_192_FITS_ETTh1_ftM_sl180_ll48_pl192_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8269
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=26, out_features=53, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1234688.0
params:  1431.0
Trainable parameters:  1431
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.294975757598877
Epoch: 1, Steps: 64 | Train Loss: 0.7247647 Vali Loss: 1.4259974 Test Loss: 0.7170187
Validation loss decreased (inf --> 1.425997).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.018190860748291
Epoch: 2, Steps: 64 | Train Loss: 0.5590651 Vali Loss: 1.2148510 Test Loss: 0.5729826
Validation loss decreased (1.425997 --> 1.214851).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 0.9780304431915283
Epoch: 3, Steps: 64 | Train Loss: 0.4930171 Vali Loss: 1.1266764 Test Loss: 0.5146171
Validation loss decreased (1.214851 --> 1.126676).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.0663022994995117
Epoch: 4, Steps: 64 | Train Loss: 0.4637639 Vali Loss: 1.0822498 Test Loss: 0.4869924
Validation loss decreased (1.126676 --> 1.082250).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 0.9877443313598633
Epoch: 5, Steps: 64 | Train Loss: 0.4476539 Vali Loss: 1.0569204 Test Loss: 0.4718393
Validation loss decreased (1.082250 --> 1.056920).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.0315725803375244
Epoch: 6, Steps: 64 | Train Loss: 0.4382585 Vali Loss: 1.0427092 Test Loss: 0.4628987
Validation loss decreased (1.056920 --> 1.042709).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 0.8930840492248535
Epoch: 7, Steps: 64 | Train Loss: 0.4329569 Vali Loss: 1.0318502 Test Loss: 0.4566328
Validation loss decreased (1.042709 --> 1.031850).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.1901288032531738
Epoch: 8, Steps: 64 | Train Loss: 0.4288971 Vali Loss: 1.0236241 Test Loss: 0.4522462
Validation loss decreased (1.031850 --> 1.023624).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.00935959815979
Epoch: 9, Steps: 64 | Train Loss: 0.4255716 Vali Loss: 1.0178432 Test Loss: 0.4490070
Validation loss decreased (1.023624 --> 1.017843).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.0525181293487549
Epoch: 10, Steps: 64 | Train Loss: 0.4238916 Vali Loss: 1.0131776 Test Loss: 0.4465461
Validation loss decreased (1.017843 --> 1.013178).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.0819425582885742
Epoch: 11, Steps: 64 | Train Loss: 0.4225491 Vali Loss: 1.0086747 Test Loss: 0.4445477
Validation loss decreased (1.013178 --> 1.008675).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.0162270069122314
Epoch: 12, Steps: 64 | Train Loss: 0.4212910 Vali Loss: 1.0054424 Test Loss: 0.4433412
Validation loss decreased (1.008675 --> 1.005442).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.0745062828063965
Epoch: 13, Steps: 64 | Train Loss: 0.4194196 Vali Loss: 1.0027733 Test Loss: 0.4421783
Validation loss decreased (1.005442 --> 1.002773).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.0751612186431885
Epoch: 14, Steps: 64 | Train Loss: 0.4187332 Vali Loss: 1.0002919 Test Loss: 0.4411019
Validation loss decreased (1.002773 --> 1.000292).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.0106477737426758
Epoch: 15, Steps: 64 | Train Loss: 0.4185539 Vali Loss: 0.9985628 Test Loss: 0.4404548
Validation loss decreased (1.000292 --> 0.998563).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.0378046035766602
Epoch: 16, Steps: 64 | Train Loss: 0.4175004 Vali Loss: 0.9968108 Test Loss: 0.4401246
Validation loss decreased (0.998563 --> 0.996811).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.0027055740356445
Epoch: 17, Steps: 64 | Train Loss: 0.4168708 Vali Loss: 0.9952747 Test Loss: 0.4397109
Validation loss decreased (0.996811 --> 0.995275).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 0.9942502975463867
Epoch: 18, Steps: 64 | Train Loss: 0.4166334 Vali Loss: 0.9939008 Test Loss: 0.4393093
Validation loss decreased (0.995275 --> 0.993901).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.0321900844573975
Epoch: 19, Steps: 64 | Train Loss: 0.4157241 Vali Loss: 0.9929952 Test Loss: 0.4391956
Validation loss decreased (0.993901 --> 0.992995).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 0.971916675567627
Epoch: 20, Steps: 64 | Train Loss: 0.4163342 Vali Loss: 0.9922151 Test Loss: 0.4390925
Validation loss decreased (0.992995 --> 0.992215).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 0.9830353260040283
Epoch: 21, Steps: 64 | Train Loss: 0.4159186 Vali Loss: 0.9912252 Test Loss: 0.4389406
Validation loss decreased (0.992215 --> 0.991225).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 0.9757609367370605
Epoch: 22, Steps: 64 | Train Loss: 0.4155146 Vali Loss: 0.9907677 Test Loss: 0.4386916
Validation loss decreased (0.991225 --> 0.990768).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.034179925918579
Epoch: 23, Steps: 64 | Train Loss: 0.4153313 Vali Loss: 0.9899071 Test Loss: 0.4388704
Validation loss decreased (0.990768 --> 0.989907).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.0238070487976074
Epoch: 24, Steps: 64 | Train Loss: 0.4153765 Vali Loss: 0.9891919 Test Loss: 0.4386418
Validation loss decreased (0.989907 --> 0.989192).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.0430889129638672
Epoch: 25, Steps: 64 | Train Loss: 0.4145719 Vali Loss: 0.9885921 Test Loss: 0.4386063
Validation loss decreased (0.989192 --> 0.988592).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 0.9823276996612549
Epoch: 26, Steps: 64 | Train Loss: 0.4145887 Vali Loss: 0.9877945 Test Loss: 0.4386155
Validation loss decreased (0.988592 --> 0.987795).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 0.9689152240753174
Epoch: 27, Steps: 64 | Train Loss: 0.4147784 Vali Loss: 0.9874673 Test Loss: 0.4385863
Validation loss decreased (0.987795 --> 0.987467).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 0.9824726581573486
Epoch: 28, Steps: 64 | Train Loss: 0.4141984 Vali Loss: 0.9871615 Test Loss: 0.4385132
Validation loss decreased (0.987467 --> 0.987161).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.1199636459350586
Epoch: 29, Steps: 64 | Train Loss: 0.4150412 Vali Loss: 0.9863201 Test Loss: 0.4385640
Validation loss decreased (0.987161 --> 0.986320).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.087456464767456
Epoch: 30, Steps: 64 | Train Loss: 0.4147078 Vali Loss: 0.9864190 Test Loss: 0.4385334
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 0.9825503826141357
Epoch: 31, Steps: 64 | Train Loss: 0.4146208 Vali Loss: 0.9862047 Test Loss: 0.4385899
Validation loss decreased (0.986320 --> 0.986205).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.0494310855865479
Epoch: 32, Steps: 64 | Train Loss: 0.4146403 Vali Loss: 0.9858047 Test Loss: 0.4386471
Validation loss decreased (0.986205 --> 0.985805).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.0604369640350342
Epoch: 33, Steps: 64 | Train Loss: 0.4147118 Vali Loss: 0.9852904 Test Loss: 0.4385493
Validation loss decreased (0.985805 --> 0.985290).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 0.9645044803619385
Epoch: 34, Steps: 64 | Train Loss: 0.4140909 Vali Loss: 0.9847067 Test Loss: 0.4386307
Validation loss decreased (0.985290 --> 0.984707).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.054325819015503
Epoch: 35, Steps: 64 | Train Loss: 0.4147559 Vali Loss: 0.9849569 Test Loss: 0.4386170
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.0522716045379639
Epoch: 36, Steps: 64 | Train Loss: 0.4138211 Vali Loss: 0.9848238 Test Loss: 0.4386282
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.0545728206634521
Epoch: 37, Steps: 64 | Train Loss: 0.4143613 Vali Loss: 0.9842814 Test Loss: 0.4385900
Validation loss decreased (0.984707 --> 0.984281).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.1064748764038086
Epoch: 38, Steps: 64 | Train Loss: 0.4146367 Vali Loss: 0.9841686 Test Loss: 0.4385869
Validation loss decreased (0.984281 --> 0.984169).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 0.959423303604126
Epoch: 39, Steps: 64 | Train Loss: 0.4145092 Vali Loss: 0.9838672 Test Loss: 0.4385868
Validation loss decreased (0.984169 --> 0.983867).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.070122480392456
Epoch: 40, Steps: 64 | Train Loss: 0.4141853 Vali Loss: 0.9841274 Test Loss: 0.4385577
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.0835580825805664
Epoch: 41, Steps: 64 | Train Loss: 0.4138061 Vali Loss: 0.9838861 Test Loss: 0.4386016
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.124572992324829
Epoch: 42, Steps: 64 | Train Loss: 0.4138613 Vali Loss: 0.9833284 Test Loss: 0.4385113
Validation loss decreased (0.983867 --> 0.983328).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.0757369995117188
Epoch: 43, Steps: 64 | Train Loss: 0.4141853 Vali Loss: 0.9836844 Test Loss: 0.4386029
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.0887150764465332
Epoch: 44, Steps: 64 | Train Loss: 0.4143466 Vali Loss: 0.9829338 Test Loss: 0.4385848
Validation loss decreased (0.983328 --> 0.982934).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.0207922458648682
Epoch: 45, Steps: 64 | Train Loss: 0.4140237 Vali Loss: 0.9831555 Test Loss: 0.4386198
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.1608567237854004
Epoch: 46, Steps: 64 | Train Loss: 0.4139378 Vali Loss: 0.9827688 Test Loss: 0.4385324
Validation loss decreased (0.982934 --> 0.982769).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.0816326141357422
Epoch: 47, Steps: 64 | Train Loss: 0.4137625 Vali Loss: 0.9832384 Test Loss: 0.4385504
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.1709516048431396
Epoch: 48, Steps: 64 | Train Loss: 0.4139167 Vali Loss: 0.9826301 Test Loss: 0.4386510
Validation loss decreased (0.982769 --> 0.982630).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.050541639328003
Epoch: 49, Steps: 64 | Train Loss: 0.4139729 Vali Loss: 0.9829587 Test Loss: 0.4385803
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.1103861331939697
Epoch: 50, Steps: 64 | Train Loss: 0.4136070 Vali Loss: 0.9828120 Test Loss: 0.4386024
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.0471549034118652
Epoch: 51, Steps: 64 | Train Loss: 0.4134052 Vali Loss: 0.9822347 Test Loss: 0.4385752
Validation loss decreased (0.982630 --> 0.982235).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.0987012386322021
Epoch: 52, Steps: 64 | Train Loss: 0.4139608 Vali Loss: 0.9822927 Test Loss: 0.4385963
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.0990288257598877
Epoch: 53, Steps: 64 | Train Loss: 0.4139088 Vali Loss: 0.9821664 Test Loss: 0.4386367
Validation loss decreased (0.982235 --> 0.982166).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.116713523864746
Epoch: 54, Steps: 64 | Train Loss: 0.4136457 Vali Loss: 0.9825807 Test Loss: 0.4385904
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.0662775039672852
Epoch: 55, Steps: 64 | Train Loss: 0.4140049 Vali Loss: 0.9823986 Test Loss: 0.4385794
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.0363385677337646
Epoch: 56, Steps: 64 | Train Loss: 0.4131343 Vali Loss: 0.9824210 Test Loss: 0.4385817
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 0.9733452796936035
Epoch: 57, Steps: 64 | Train Loss: 0.4135232 Vali Loss: 0.9822103 Test Loss: 0.4385905
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.0140271186828613
Epoch: 58, Steps: 64 | Train Loss: 0.4139494 Vali Loss: 0.9819740 Test Loss: 0.4386119
Validation loss decreased (0.982166 --> 0.981974).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.2695229053497314
Epoch: 59, Steps: 64 | Train Loss: 0.4132464 Vali Loss: 0.9819070 Test Loss: 0.4386080
Validation loss decreased (0.981974 --> 0.981907).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.238178014755249
Epoch: 60, Steps: 64 | Train Loss: 0.4132093 Vali Loss: 0.9818020 Test Loss: 0.4386269
Validation loss decreased (0.981907 --> 0.981802).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 0.9945573806762695
Epoch: 61, Steps: 64 | Train Loss: 0.4140005 Vali Loss: 0.9822212 Test Loss: 0.4386330
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.0843586921691895
Epoch: 62, Steps: 64 | Train Loss: 0.4138894 Vali Loss: 0.9821357 Test Loss: 0.4385971
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.2190508842468262
Epoch: 63, Steps: 64 | Train Loss: 0.4138793 Vali Loss: 0.9821205 Test Loss: 0.4386250
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.0063483715057373
Epoch: 64, Steps: 64 | Train Loss: 0.4138454 Vali Loss: 0.9819958 Test Loss: 0.4385937
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.1561031341552734
Epoch: 65, Steps: 64 | Train Loss: 0.4134755 Vali Loss: 0.9820269 Test Loss: 0.4386133
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.229428768157959
Epoch: 66, Steps: 64 | Train Loss: 0.4131769 Vali Loss: 0.9817847 Test Loss: 0.4386309
Validation loss decreased (0.981802 --> 0.981785).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.0803608894348145
Epoch: 67, Steps: 64 | Train Loss: 0.4138855 Vali Loss: 0.9819461 Test Loss: 0.4386103
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.089564323425293
Epoch: 68, Steps: 64 | Train Loss: 0.4136145 Vali Loss: 0.9818196 Test Loss: 0.4386103
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.0714449882507324
Epoch: 69, Steps: 64 | Train Loss: 0.4130407 Vali Loss: 0.9814318 Test Loss: 0.4386264
Validation loss decreased (0.981785 --> 0.981432).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.1280450820922852
Epoch: 70, Steps: 64 | Train Loss: 0.4135637 Vali Loss: 0.9816527 Test Loss: 0.4386182
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.1055631637573242
Epoch: 71, Steps: 64 | Train Loss: 0.4135505 Vali Loss: 0.9813373 Test Loss: 0.4386356
Validation loss decreased (0.981432 --> 0.981337).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 0.9612443447113037
Epoch: 72, Steps: 64 | Train Loss: 0.4138061 Vali Loss: 0.9814134 Test Loss: 0.4386292
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.0329818725585938
Epoch: 73, Steps: 64 | Train Loss: 0.4132700 Vali Loss: 0.9819090 Test Loss: 0.4386151
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.2318038940429688
Epoch: 74, Steps: 64 | Train Loss: 0.4133899 Vali Loss: 0.9816802 Test Loss: 0.4386088
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.1230075359344482
Epoch: 75, Steps: 64 | Train Loss: 0.4140808 Vali Loss: 0.9814712 Test Loss: 0.4386210
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.014988899230957
Epoch: 76, Steps: 64 | Train Loss: 0.4134995 Vali Loss: 0.9816471 Test Loss: 0.4386351
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.1007170677185059
Epoch: 77, Steps: 64 | Train Loss: 0.4142044 Vali Loss: 0.9817489 Test Loss: 0.4386263
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 0.9354162216186523
Epoch: 78, Steps: 64 | Train Loss: 0.4138280 Vali Loss: 0.9813449 Test Loss: 0.4386173
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.0460312366485596
Epoch: 79, Steps: 64 | Train Loss: 0.4138711 Vali Loss: 0.9813570 Test Loss: 0.4386300
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 0.9759695529937744
Epoch: 80, Steps: 64 | Train Loss: 0.4131712 Vali Loss: 0.9812937 Test Loss: 0.4386220
Validation loss decreased (0.981337 --> 0.981294).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 0.9402668476104736
Epoch: 81, Steps: 64 | Train Loss: 0.4135108 Vali Loss: 0.9816399 Test Loss: 0.4386400
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.0515153408050537
Epoch: 82, Steps: 64 | Train Loss: 0.4136705 Vali Loss: 0.9813535 Test Loss: 0.4386161
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.0512685775756836
Epoch: 83, Steps: 64 | Train Loss: 0.4134852 Vali Loss: 0.9817451 Test Loss: 0.4386237
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.094632625579834
Epoch: 84, Steps: 64 | Train Loss: 0.4136646 Vali Loss: 0.9816113 Test Loss: 0.4386195
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.145533800125122
Epoch: 85, Steps: 64 | Train Loss: 0.4138656 Vali Loss: 0.9812568 Test Loss: 0.4386378
Validation loss decreased (0.981294 --> 0.981257).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 0.9718196392059326
Epoch: 86, Steps: 64 | Train Loss: 0.4136617 Vali Loss: 0.9814738 Test Loss: 0.4386305
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.0609726905822754
Epoch: 87, Steps: 64 | Train Loss: 0.4138722 Vali Loss: 0.9815251 Test Loss: 0.4386429
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.0262365341186523
Epoch: 88, Steps: 64 | Train Loss: 0.4135071 Vali Loss: 0.9814245 Test Loss: 0.4386315
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.1612451076507568
Epoch: 89, Steps: 64 | Train Loss: 0.4135588 Vali Loss: 0.9815539 Test Loss: 0.4386273
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.118967056274414
Epoch: 90, Steps: 64 | Train Loss: 0.4136001 Vali Loss: 0.9815069 Test Loss: 0.4386336
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 0.9375269412994385
Epoch: 91, Steps: 64 | Train Loss: 0.4138900 Vali Loss: 0.9815278 Test Loss: 0.4386351
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.046781063079834
Epoch: 92, Steps: 64 | Train Loss: 0.4136192 Vali Loss: 0.9813221 Test Loss: 0.4386377
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 0.975616455078125
Epoch: 93, Steps: 64 | Train Loss: 0.4135612 Vali Loss: 0.9815223 Test Loss: 0.4386371
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.0044760704040527
Epoch: 94, Steps: 64 | Train Loss: 0.4134120 Vali Loss: 0.9816329 Test Loss: 0.4386351
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.0180277824401855
Epoch: 95, Steps: 64 | Train Loss: 0.4136132 Vali Loss: 0.9814615 Test Loss: 0.4386383
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 0.9875013828277588
Epoch: 96, Steps: 64 | Train Loss: 0.4135334 Vali Loss: 0.9815537 Test Loss: 0.4386368
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.1738758087158203
Epoch: 97, Steps: 64 | Train Loss: 0.4138836 Vali Loss: 0.9815287 Test Loss: 0.4386340
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 0.9909451007843018
Epoch: 98, Steps: 64 | Train Loss: 0.4133987 Vali Loss: 0.9814352 Test Loss: 0.4386359
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.1255779266357422
Epoch: 99, Steps: 64 | Train Loss: 0.4137129 Vali Loss: 0.9809657 Test Loss: 0.4386282
Validation loss decreased (0.981257 --> 0.980966).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.0728394985198975
Epoch: 100, Steps: 64 | Train Loss: 0.4133806 Vali Loss: 0.9811795 Test Loss: 0.4386379
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_180_192_FITS_ETTh1_ftM_sl180_ll48_pl192_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4322604238986969, mae:0.4251600503921509, rse:0.6243528127670288, corr:[0.263035   0.2676294  0.26765788 0.265027   0.2619456  0.26005173
 0.25973246 0.26043847 0.26087874 0.2606677  0.26013297 0.25976115
 0.25956115 0.25904742 0.25853962 0.25838196 0.2584437  0.2585469
 0.25856444 0.25853822 0.2585721  0.2587048  0.25860655 0.25795016
 0.25694674 0.25662482 0.25666168 0.2566087  0.25611615 0.25547358
 0.2549331  0.25453636 0.25425956 0.25411072 0.25410733 0.25441495
 0.2548204  0.254986   0.25503573 0.2552313  0.255587   0.2558328
 0.25589997 0.25606167 0.25645292 0.25679758 0.25686908 0.25612256
 0.2545488  0.25329348 0.2520974  0.25080517 0.24939932 0.24817657
 0.24757685 0.24761504 0.24779871 0.24793714 0.24786039 0.24816683
 0.24869049 0.24869803 0.24838945 0.24815471 0.24818186 0.24832457
 0.24844708 0.24846496 0.24846129 0.24835488 0.24798737 0.24688809
 0.24533075 0.24447191 0.24413133 0.24391007 0.24354951 0.24304669
 0.2427626  0.24268582 0.24254087 0.24226359 0.24187565 0.24184783
 0.24231325 0.24250889 0.24248822 0.24241757 0.24233934 0.2422249
 0.24207057 0.2420814  0.24233282 0.24270618 0.24301592 0.24278146
 0.2418662  0.24118717 0.24075311 0.24008396 0.2392703  0.23865168
 0.23849206 0.23888624 0.23929362 0.23943329 0.23927434 0.23934115
 0.2395915  0.2395371  0.23938802 0.2394438  0.23954667 0.23950396
 0.23941122 0.23939544 0.2394527  0.23945329 0.23923166 0.23841417
 0.23698811 0.23588578 0.23493692 0.2340657  0.23325229 0.23279789
 0.23281358 0.23313692 0.23329921 0.23324478 0.23314886 0.23357143
 0.23434034 0.23459756 0.23463601 0.23468095 0.23475045 0.23473662
 0.23461331 0.23453332 0.23461604 0.23473558 0.23466948 0.23392835
 0.23255986 0.23158142 0.23092401 0.22999573 0.22912042 0.22853395
 0.22851086 0.22886676 0.22937067 0.22971837 0.22980398 0.23011307
 0.23056422 0.23051687 0.23024042 0.22995535 0.22976248 0.22978464
 0.22985996 0.22996995 0.2301662  0.23029338 0.23017251 0.22932217
 0.22798954 0.22727762 0.22693986 0.22642176 0.22578506 0.22534063
 0.22547184 0.22622027 0.22697394 0.22762081 0.22805285 0.22877409
 0.2298112  0.23005788 0.22984943 0.22967298 0.22949004 0.22924753
 0.22882703 0.2284594  0.22873807 0.2298689  0.23096731 0.22969964]
