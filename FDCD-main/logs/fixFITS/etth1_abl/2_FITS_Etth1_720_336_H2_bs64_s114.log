Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=72, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=72, out_features=105, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6773760.0
params:  7665.0
Trainable parameters:  7665
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.502356052398682
Epoch: 1, Steps: 59 | Train Loss: 0.7258945 Vali Loss: 1.8999534 Test Loss: 0.9178747
Validation loss decreased (inf --> 1.899953).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.460811138153076
Epoch: 2, Steps: 59 | Train Loss: 0.5956257 Vali Loss: 1.7117987 Test Loss: 0.8173845
Validation loss decreased (1.899953 --> 1.711799).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.498014450073242
Epoch: 3, Steps: 59 | Train Loss: 0.5194168 Vali Loss: 1.6110605 Test Loss: 0.7637137
Validation loss decreased (1.711799 --> 1.611061).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.302724599838257
Epoch: 4, Steps: 59 | Train Loss: 0.4715035 Vali Loss: 1.5525621 Test Loss: 0.7335363
Validation loss decreased (1.611061 --> 1.552562).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.65100622177124
Epoch: 5, Steps: 59 | Train Loss: 0.4386518 Vali Loss: 1.5110554 Test Loss: 0.7138699
Validation loss decreased (1.552562 --> 1.511055).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.235220193862915
Epoch: 6, Steps: 59 | Train Loss: 0.4146364 Vali Loss: 1.4907387 Test Loss: 0.7003350
Validation loss decreased (1.511055 --> 1.490739).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.4716901779174805
Epoch: 7, Steps: 59 | Train Loss: 0.3955546 Vali Loss: 1.4721377 Test Loss: 0.6892973
Validation loss decreased (1.490739 --> 1.472138).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.309159517288208
Epoch: 8, Steps: 59 | Train Loss: 0.3795504 Vali Loss: 1.4583778 Test Loss: 0.6797309
Validation loss decreased (1.472138 --> 1.458378).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.335354328155518
Epoch: 9, Steps: 59 | Train Loss: 0.3660558 Vali Loss: 1.4472992 Test Loss: 0.6717740
Validation loss decreased (1.458378 --> 1.447299).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.442461729049683
Epoch: 10, Steps: 59 | Train Loss: 0.3540820 Vali Loss: 1.4331274 Test Loss: 0.6639094
Validation loss decreased (1.447299 --> 1.433127).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.337910413742065
Epoch: 11, Steps: 59 | Train Loss: 0.3438965 Vali Loss: 1.4226811 Test Loss: 0.6565426
Validation loss decreased (1.433127 --> 1.422681).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.362013101577759
Epoch: 12, Steps: 59 | Train Loss: 0.3346218 Vali Loss: 1.4100342 Test Loss: 0.6489512
Validation loss decreased (1.422681 --> 1.410034).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.8958351612091064
Epoch: 13, Steps: 59 | Train Loss: 0.3264749 Vali Loss: 1.4027143 Test Loss: 0.6428249
Validation loss decreased (1.410034 --> 1.402714).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.313906192779541
Epoch: 14, Steps: 59 | Train Loss: 0.3190127 Vali Loss: 1.4014516 Test Loss: 0.6370224
Validation loss decreased (1.402714 --> 1.401452).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.298271656036377
Epoch: 15, Steps: 59 | Train Loss: 0.3125358 Vali Loss: 1.3876209 Test Loss: 0.6304630
Validation loss decreased (1.401452 --> 1.387621).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.298940658569336
Epoch: 16, Steps: 59 | Train Loss: 0.3063202 Vali Loss: 1.3812828 Test Loss: 0.6251170
Validation loss decreased (1.387621 --> 1.381283).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.060284852981567
Epoch: 17, Steps: 59 | Train Loss: 0.3007861 Vali Loss: 1.3777080 Test Loss: 0.6199691
Validation loss decreased (1.381283 --> 1.377708).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.023283958435059
Epoch: 18, Steps: 59 | Train Loss: 0.2957449 Vali Loss: 1.3732566 Test Loss: 0.6148236
Validation loss decreased (1.377708 --> 1.373257).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.196778059005737
Epoch: 19, Steps: 59 | Train Loss: 0.2912430 Vali Loss: 1.3631283 Test Loss: 0.6104509
Validation loss decreased (1.373257 --> 1.363128).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.111337900161743
Epoch: 20, Steps: 59 | Train Loss: 0.2870175 Vali Loss: 1.3601342 Test Loss: 0.6065289
Validation loss decreased (1.363128 --> 1.360134).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.269562244415283
Epoch: 21, Steps: 59 | Train Loss: 0.2831849 Vali Loss: 1.3573428 Test Loss: 0.6027269
Validation loss decreased (1.360134 --> 1.357343).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.0476062297821045
Epoch: 22, Steps: 59 | Train Loss: 0.2795291 Vali Loss: 1.3478267 Test Loss: 0.5988388
Validation loss decreased (1.357343 --> 1.347827).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.060063362121582
Epoch: 23, Steps: 59 | Train Loss: 0.2764457 Vali Loss: 1.3492217 Test Loss: 0.5950557
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.90193510055542
Epoch: 24, Steps: 59 | Train Loss: 0.2733307 Vali Loss: 1.3474826 Test Loss: 0.5917593
Validation loss decreased (1.347827 --> 1.347483).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.187378644943237
Epoch: 25, Steps: 59 | Train Loss: 0.2705448 Vali Loss: 1.3423743 Test Loss: 0.5889289
Validation loss decreased (1.347483 --> 1.342374).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.385067939758301
Epoch: 26, Steps: 59 | Train Loss: 0.2680478 Vali Loss: 1.3336947 Test Loss: 0.5862193
Validation loss decreased (1.342374 --> 1.333695).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.24877405166626
Epoch: 27, Steps: 59 | Train Loss: 0.2656054 Vali Loss: 1.3314534 Test Loss: 0.5836154
Validation loss decreased (1.333695 --> 1.331453).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.214911699295044
Epoch: 28, Steps: 59 | Train Loss: 0.2634846 Vali Loss: 1.3320718 Test Loss: 0.5808094
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.342544078826904
Epoch: 29, Steps: 59 | Train Loss: 0.2613806 Vali Loss: 1.3267576 Test Loss: 0.5785629
Validation loss decreased (1.331453 --> 1.326758).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.057586193084717
Epoch: 30, Steps: 59 | Train Loss: 0.2594476 Vali Loss: 1.3231782 Test Loss: 0.5760864
Validation loss decreased (1.326758 --> 1.323178).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.06162166595459
Epoch: 31, Steps: 59 | Train Loss: 0.2576147 Vali Loss: 1.3249110 Test Loss: 0.5740117
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.330175161361694
Epoch: 32, Steps: 59 | Train Loss: 0.2560127 Vali Loss: 1.3243449 Test Loss: 0.5723134
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.34039831161499
Epoch: 33, Steps: 59 | Train Loss: 0.2543653 Vali Loss: 1.3198925 Test Loss: 0.5703915
Validation loss decreased (1.323178 --> 1.319893).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.272411108016968
Epoch: 34, Steps: 59 | Train Loss: 0.2529506 Vali Loss: 1.3193778 Test Loss: 0.5686099
Validation loss decreased (1.319893 --> 1.319378).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.372394561767578
Epoch: 35, Steps: 59 | Train Loss: 0.2515927 Vali Loss: 1.3185728 Test Loss: 0.5667430
Validation loss decreased (1.319378 --> 1.318573).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.554869890213013
Epoch: 36, Steps: 59 | Train Loss: 0.2502836 Vali Loss: 1.3178749 Test Loss: 0.5652173
Validation loss decreased (1.318573 --> 1.317875).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.100589752197266
Epoch: 37, Steps: 59 | Train Loss: 0.2491031 Vali Loss: 1.3135115 Test Loss: 0.5636822
Validation loss decreased (1.317875 --> 1.313511).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.381248950958252
Epoch: 38, Steps: 59 | Train Loss: 0.2479328 Vali Loss: 1.3098216 Test Loss: 0.5621566
Validation loss decreased (1.313511 --> 1.309822).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.949756145477295
Epoch: 39, Steps: 59 | Train Loss: 0.2468249 Vali Loss: 1.3116843 Test Loss: 0.5608686
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.454508304595947
Epoch: 40, Steps: 59 | Train Loss: 0.2458161 Vali Loss: 1.3098006 Test Loss: 0.5597628
Validation loss decreased (1.309822 --> 1.309801).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.023379325866699
Epoch: 41, Steps: 59 | Train Loss: 0.2449340 Vali Loss: 1.3089061 Test Loss: 0.5584231
Validation loss decreased (1.309801 --> 1.308906).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.056407928466797
Epoch: 42, Steps: 59 | Train Loss: 0.2440245 Vali Loss: 1.3015020 Test Loss: 0.5573957
Validation loss decreased (1.308906 --> 1.301502).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.1469340324401855
Epoch: 43, Steps: 59 | Train Loss: 0.2431760 Vali Loss: 1.3053411 Test Loss: 0.5562845
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.242466449737549
Epoch: 44, Steps: 59 | Train Loss: 0.2424169 Vali Loss: 1.3037002 Test Loss: 0.5554265
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.010679721832275
Epoch: 45, Steps: 59 | Train Loss: 0.2416183 Vali Loss: 1.3066103 Test Loss: 0.5543596
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.91729474067688
Epoch: 46, Steps: 59 | Train Loss: 0.2409599 Vali Loss: 1.3057563 Test Loss: 0.5533382
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.566649675369263
Epoch: 47, Steps: 59 | Train Loss: 0.2403201 Vali Loss: 1.3004255 Test Loss: 0.5526027
Validation loss decreased (1.301502 --> 1.300426).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.317876815795898
Epoch: 48, Steps: 59 | Train Loss: 0.2395993 Vali Loss: 1.2982185 Test Loss: 0.5517833
Validation loss decreased (1.300426 --> 1.298218).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 4.074848890304565
Epoch: 49, Steps: 59 | Train Loss: 0.2391263 Vali Loss: 1.3026623 Test Loss: 0.5508744
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.049054145812988
Epoch: 50, Steps: 59 | Train Loss: 0.2385008 Vali Loss: 1.3032247 Test Loss: 0.5501522
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.654689788818359
Epoch: 51, Steps: 59 | Train Loss: 0.2381184 Vali Loss: 1.3038278 Test Loss: 0.5496288
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.112905740737915
Epoch: 52, Steps: 59 | Train Loss: 0.2375689 Vali Loss: 1.3010135 Test Loss: 0.5488479
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.224442958831787
Epoch: 53, Steps: 59 | Train Loss: 0.2370459 Vali Loss: 1.2978156 Test Loss: 0.5482303
Validation loss decreased (1.298218 --> 1.297816).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 4.083655595779419
Epoch: 54, Steps: 59 | Train Loss: 0.2363982 Vali Loss: 1.2967787 Test Loss: 0.5477216
Validation loss decreased (1.297816 --> 1.296779).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.266348600387573
Epoch: 55, Steps: 59 | Train Loss: 0.2361568 Vali Loss: 1.3015362 Test Loss: 0.5470493
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 4.369488954544067
Epoch: 56, Steps: 59 | Train Loss: 0.2356895 Vali Loss: 1.2969005 Test Loss: 0.5466396
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.002154588699341
Epoch: 57, Steps: 59 | Train Loss: 0.2353884 Vali Loss: 1.2981966 Test Loss: 0.5459819
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 4.132563591003418
Epoch: 58, Steps: 59 | Train Loss: 0.2350454 Vali Loss: 1.2940804 Test Loss: 0.5456077
Validation loss decreased (1.296779 --> 1.294080).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.336963415145874
Epoch: 59, Steps: 59 | Train Loss: 0.2345507 Vali Loss: 1.3025649 Test Loss: 0.5450708
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 4.1859376430511475
Epoch: 60, Steps: 59 | Train Loss: 0.2342957 Vali Loss: 1.2939519 Test Loss: 0.5446982
Validation loss decreased (1.294080 --> 1.293952).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 4.035578012466431
Epoch: 61, Steps: 59 | Train Loss: 0.2339203 Vali Loss: 1.2972499 Test Loss: 0.5442892
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 4.37625789642334
Epoch: 62, Steps: 59 | Train Loss: 0.2336943 Vali Loss: 1.2942190 Test Loss: 0.5438734
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 4.44711446762085
Epoch: 63, Steps: 59 | Train Loss: 0.2334494 Vali Loss: 1.2897072 Test Loss: 0.5435404
Validation loss decreased (1.293952 --> 1.289707).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.8870160579681396
Epoch: 64, Steps: 59 | Train Loss: 0.2331138 Vali Loss: 1.2983356 Test Loss: 0.5431622
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 4.08390998840332
Epoch: 65, Steps: 59 | Train Loss: 0.2329026 Vali Loss: 1.2949915 Test Loss: 0.5428235
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 4.226902484893799
Epoch: 66, Steps: 59 | Train Loss: 0.2326385 Vali Loss: 1.2911885 Test Loss: 0.5425559
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 4.159765243530273
Epoch: 67, Steps: 59 | Train Loss: 0.2324650 Vali Loss: 1.2951798 Test Loss: 0.5422734
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 4.066051959991455
Epoch: 68, Steps: 59 | Train Loss: 0.2321655 Vali Loss: 1.2910123 Test Loss: 0.5419492
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 4.112070083618164
Epoch: 69, Steps: 59 | Train Loss: 0.2320612 Vali Loss: 1.2919068 Test Loss: 0.5417009
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.113631010055542
Epoch: 70, Steps: 59 | Train Loss: 0.2318738 Vali Loss: 1.2920333 Test Loss: 0.5414404
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 4.202650308609009
Epoch: 71, Steps: 59 | Train Loss: 0.2317138 Vali Loss: 1.2977946 Test Loss: 0.5411409
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 4.279576778411865
Epoch: 72, Steps: 59 | Train Loss: 0.2315618 Vali Loss: 1.2930464 Test Loss: 0.5409744
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 4.372951507568359
Epoch: 73, Steps: 59 | Train Loss: 0.2312538 Vali Loss: 1.2930974 Test Loss: 0.5406988
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 4.204294443130493
Epoch: 74, Steps: 59 | Train Loss: 0.2310896 Vali Loss: 1.2946469 Test Loss: 0.5404958
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 4.26424765586853
Epoch: 75, Steps: 59 | Train Loss: 0.2309986 Vali Loss: 1.2933282 Test Loss: 0.5403188
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 4.26371955871582
Epoch: 76, Steps: 59 | Train Loss: 0.2308684 Vali Loss: 1.2912937 Test Loss: 0.5401517
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 4.280476808547974
Epoch: 77, Steps: 59 | Train Loss: 0.2306754 Vali Loss: 1.2922547 Test Loss: 0.5399556
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.975888967514038
Epoch: 78, Steps: 59 | Train Loss: 0.2305792 Vali Loss: 1.2908300 Test Loss: 0.5397931
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 4.087108135223389
Epoch: 79, Steps: 59 | Train Loss: 0.2304188 Vali Loss: 1.2960135 Test Loss: 0.5396349
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 4.220223665237427
Epoch: 80, Steps: 59 | Train Loss: 0.2301757 Vali Loss: 1.2934370 Test Loss: 0.5394751
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 4.2007200717926025
Epoch: 81, Steps: 59 | Train Loss: 0.2302207 Vali Loss: 1.2931105 Test Loss: 0.5393492
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 4.12955904006958
Epoch: 82, Steps: 59 | Train Loss: 0.2301305 Vali Loss: 1.2940574 Test Loss: 0.5392224
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 4.03277587890625
Epoch: 83, Steps: 59 | Train Loss: 0.2300865 Vali Loss: 1.2917416 Test Loss: 0.5390562
EarlyStopping counter: 20 out of 20
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=72, out_features=105, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6773760.0
params:  7665.0
Trainable parameters:  7665
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.167098045349121
Epoch: 1, Steps: 59 | Train Loss: 0.4805500 Vali Loss: 1.2378010 Test Loss: 0.4905502
Validation loss decreased (inf --> 1.237801).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.117226600646973
Epoch: 2, Steps: 59 | Train Loss: 0.4601602 Vali Loss: 1.2213041 Test Loss: 0.4700786
Validation loss decreased (1.237801 --> 1.221304).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.04854154586792
Epoch: 3, Steps: 59 | Train Loss: 0.4519739 Vali Loss: 1.2114950 Test Loss: 0.4632348
Validation loss decreased (1.221304 --> 1.211495).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.098628759384155
Epoch: 4, Steps: 59 | Train Loss: 0.4489922 Vali Loss: 1.2150409 Test Loss: 0.4610803
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.034131050109863
Epoch: 5, Steps: 59 | Train Loss: 0.4476206 Vali Loss: 1.2187098 Test Loss: 0.4610007
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.180887699127197
Epoch: 6, Steps: 59 | Train Loss: 0.4468882 Vali Loss: 1.2166764 Test Loss: 0.4611606
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.3549439907073975
Epoch: 7, Steps: 59 | Train Loss: 0.4467280 Vali Loss: 1.2185177 Test Loss: 0.4622066
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.2866809368133545
Epoch: 8, Steps: 59 | Train Loss: 0.4463801 Vali Loss: 1.2255772 Test Loss: 0.4622560
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.084360122680664
Epoch: 9, Steps: 59 | Train Loss: 0.4460424 Vali Loss: 1.2200741 Test Loss: 0.4622881
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.292478561401367
Epoch: 10, Steps: 59 | Train Loss: 0.4457685 Vali Loss: 1.2237468 Test Loss: 0.4625283
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.358365774154663
Epoch: 11, Steps: 59 | Train Loss: 0.4453992 Vali Loss: 1.2253255 Test Loss: 0.4628370
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.196349620819092
Epoch: 12, Steps: 59 | Train Loss: 0.4456440 Vali Loss: 1.2228780 Test Loss: 0.4627095
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.309942960739136
Epoch: 13, Steps: 59 | Train Loss: 0.4453031 Vali Loss: 1.2261375 Test Loss: 0.4628815
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.060658931732178
Epoch: 14, Steps: 59 | Train Loss: 0.4452987 Vali Loss: 1.2236913 Test Loss: 0.4629186
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.025839805603027
Epoch: 15, Steps: 59 | Train Loss: 0.4451408 Vali Loss: 1.2260879 Test Loss: 0.4632291
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.438870429992676
Epoch: 16, Steps: 59 | Train Loss: 0.4450518 Vali Loss: 1.2278346 Test Loss: 0.4634013
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.219610929489136
Epoch: 17, Steps: 59 | Train Loss: 0.4452444 Vali Loss: 1.2269527 Test Loss: 0.4634990
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.701459646224976
Epoch: 18, Steps: 59 | Train Loss: 0.4448315 Vali Loss: 1.2295401 Test Loss: 0.4634005
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.738539218902588
Epoch: 19, Steps: 59 | Train Loss: 0.4448051 Vali Loss: 1.2311043 Test Loss: 0.4634793
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.543428659439087
Epoch: 20, Steps: 59 | Train Loss: 0.4448189 Vali Loss: 1.2280403 Test Loss: 0.4635617
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.590121030807495
Epoch: 21, Steps: 59 | Train Loss: 0.4448255 Vali Loss: 1.2301641 Test Loss: 0.4636072
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.479207754135132
Epoch: 22, Steps: 59 | Train Loss: 0.4445745 Vali Loss: 1.2266474 Test Loss: 0.4635184
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.643839359283447
Epoch: 23, Steps: 59 | Train Loss: 0.4448669 Vali Loss: 1.2244444 Test Loss: 0.4637116
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.462104469537735, mae:0.45904314517974854, rse:0.6471756100654602, corr:[0.24770132 0.254358   0.25681183 0.25618666 0.25453314 0.2528851
 0.25159913 0.2507362  0.2500632  0.24954511 0.24911377 0.2487418
 0.24851678 0.24834573 0.24815078 0.2478937  0.2475526  0.24721904
 0.24695373 0.24680398 0.2468479  0.24711072 0.24729493 0.24726483
 0.2469486  0.24637233 0.24565582 0.24492858 0.24433973 0.24398695
 0.24392983 0.24402566 0.24423239 0.24444659 0.24447232 0.2444443
 0.2445485  0.24468957 0.24483712 0.24489179 0.24492158 0.24493055
 0.24491231 0.24487543 0.24485475 0.24488401 0.24489538 0.24477334
 0.24423036 0.24352708 0.24263628 0.24175906 0.2410139  0.24029967
 0.23966485 0.23914126 0.2386088  0.23807949 0.2375346  0.23711383
 0.23687182 0.23693068 0.23716018 0.23754299 0.23786703 0.23802629
 0.23805095 0.23783146 0.23760012 0.237477   0.23747289 0.23749135
 0.23745176 0.23725633 0.23687643 0.23645647 0.23597577 0.23541929
 0.23493297 0.23462377 0.2344309  0.23428354 0.23407118 0.23378797
 0.23349945 0.23319927 0.23288183 0.23258483 0.23231998 0.23221005
 0.23227338 0.23255968 0.2329989  0.23366684 0.23445667 0.2351082
 0.23558335 0.23582794 0.23571514 0.23545305 0.23521645 0.23505594
 0.2349928  0.23495385 0.23480056 0.23453373 0.23413014 0.23369008
 0.23342016 0.23334764 0.23341112 0.23362298 0.2338539  0.23400702
 0.23408365 0.23400511 0.23377822 0.2334657  0.23308606 0.23263152
 0.23216018 0.23163778 0.23112945 0.2307073  0.23041189 0.23009722
 0.22974314 0.2293965  0.22888288 0.22834508 0.22790435 0.22766097
 0.22762299 0.22767934 0.22781676 0.22795384 0.22802538 0.22802441
 0.22799404 0.22793819 0.22792993 0.2279957  0.22808765 0.2280298
 0.227813   0.22742245 0.22688718 0.22623685 0.22572064 0.22528818
 0.22493434 0.2247128  0.22449148 0.22423366 0.22389385 0.2235522
 0.22325704 0.22304325 0.22295286 0.22295588 0.22286199 0.22291273
 0.22302577 0.22315662 0.22327268 0.22334681 0.22337078 0.22326425
 0.2231212  0.22296083 0.22273631 0.22247879 0.22216915 0.22183971
 0.2214964  0.22120702 0.22077452 0.22029836 0.2197994  0.21941239
 0.21917291 0.2191011  0.21924679 0.21962449 0.22019723 0.22085313
 0.2213965  0.221694   0.22178386 0.22172284 0.22145006 0.22096555
 0.22035629 0.21978694 0.21919356 0.21855587 0.21800472 0.21754466
 0.21713758 0.21687372 0.21667875 0.21653764 0.21636829 0.21631715
 0.21629739 0.21635425 0.21648706 0.21658823 0.21668413 0.21675567
 0.21685255 0.21683642 0.21680032 0.21676889 0.2167699  0.2167078
 0.21646939 0.21615416 0.21571249 0.2152125  0.21492352 0.21472876
 0.2146531  0.21466278 0.21469396 0.2145758  0.21426639 0.21392983
 0.21365705 0.21340606 0.21336217 0.2134997  0.21359254 0.21369974
 0.21379243 0.21376961 0.21369861 0.21358164 0.213542   0.21344087
 0.21337931 0.21340963 0.21353398 0.21370426 0.21380955 0.21374638
 0.21350342 0.21318218 0.21277364 0.21228208 0.21179217 0.21149278
 0.21145701 0.21151577 0.21162598 0.21163103 0.2114889  0.21119602
 0.21087037 0.21046835 0.21022664 0.21023956 0.210492   0.21073629
 0.21089959 0.21095228 0.21082798 0.21052903 0.20997493 0.20933698
 0.20892484 0.20879503 0.20883965 0.20897846 0.20902655 0.20901224
 0.20899339 0.2089653  0.20893209 0.20891204 0.20894139 0.20909481
 0.2092467  0.20912626 0.20889194 0.20854597 0.2082135  0.20768124
 0.2069679  0.20639923 0.2059064  0.20575425 0.20589693 0.20634256
 0.20696463 0.2075086  0.2077195  0.20750898 0.2068649  0.20607477
 0.20543924 0.20506954 0.2051927  0.20556062 0.20598833 0.20625478
 0.20642023 0.20610929 0.20561385 0.20494431 0.20435429 0.20397894
 0.20406602 0.20449156 0.20469724 0.20432542 0.20353469 0.20235737
 0.20115072 0.20007822 0.1994127  0.1991345  0.19901432 0.1990146
 0.19908635 0.19877762 0.19805685 0.19746505 0.1967745  0.19657812
 0.19707152 0.19796692 0.19860888 0.19774339 0.1937041  0.18357144]
