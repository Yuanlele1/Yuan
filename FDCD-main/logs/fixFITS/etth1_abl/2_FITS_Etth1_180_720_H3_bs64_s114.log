Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=34, out_features=170, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5178880.0
params:  5950.0
Trainable parameters:  5950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9649200439453125
Epoch: 1, Steps: 60 | Train Loss: 1.1879026 Vali Loss: 2.5873799 Test Loss: 1.2610289
Validation loss decreased (inf --> 2.587380).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.72243332862854
Epoch: 2, Steps: 60 | Train Loss: 0.9151588 Vali Loss: 2.2055712 Test Loss: 0.9737564
Validation loss decreased (2.587380 --> 2.205571).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8033599853515625
Epoch: 3, Steps: 60 | Train Loss: 0.7703418 Vali Loss: 1.9902048 Test Loss: 0.8169675
Validation loss decreased (2.205571 --> 1.990205).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6044366359710693
Epoch: 4, Steps: 60 | Train Loss: 0.6856132 Vali Loss: 1.8643250 Test Loss: 0.7202799
Validation loss decreased (1.990205 --> 1.864325).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5288007259368896
Epoch: 5, Steps: 60 | Train Loss: 0.6320995 Vali Loss: 1.7885535 Test Loss: 0.6561527
Validation loss decreased (1.864325 --> 1.788553).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.6395795345306396
Epoch: 6, Steps: 60 | Train Loss: 0.5968816 Vali Loss: 1.7337098 Test Loss: 0.6126110
Validation loss decreased (1.788553 --> 1.733710).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4735651016235352
Epoch: 7, Steps: 60 | Train Loss: 0.5735963 Vali Loss: 1.6814971 Test Loss: 0.5815606
Validation loss decreased (1.733710 --> 1.681497).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.4444758892059326
Epoch: 8, Steps: 60 | Train Loss: 0.5565897 Vali Loss: 1.6596525 Test Loss: 0.5591327
Validation loss decreased (1.681497 --> 1.659652).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4240381717681885
Epoch: 9, Steps: 60 | Train Loss: 0.5445338 Vali Loss: 1.6361729 Test Loss: 0.5425508
Validation loss decreased (1.659652 --> 1.636173).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6275393962860107
Epoch: 10, Steps: 60 | Train Loss: 0.5354706 Vali Loss: 1.6145808 Test Loss: 0.5298488
Validation loss decreased (1.636173 --> 1.614581).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7327606678009033
Epoch: 11, Steps: 60 | Train Loss: 0.5289159 Vali Loss: 1.6019721 Test Loss: 0.5199252
Validation loss decreased (1.614581 --> 1.601972).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.5167031288146973
Epoch: 12, Steps: 60 | Train Loss: 0.5231604 Vali Loss: 1.5959127 Test Loss: 0.5121493
Validation loss decreased (1.601972 --> 1.595913).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.626300573348999
Epoch: 13, Steps: 60 | Train Loss: 0.5190916 Vali Loss: 1.5798650 Test Loss: 0.5059168
Validation loss decreased (1.595913 --> 1.579865).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5787608623504639
Epoch: 14, Steps: 60 | Train Loss: 0.5152369 Vali Loss: 1.5726949 Test Loss: 0.5008292
Validation loss decreased (1.579865 --> 1.572695).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4989416599273682
Epoch: 15, Steps: 60 | Train Loss: 0.5130642 Vali Loss: 1.5742242 Test Loss: 0.4965174
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.556614637374878
Epoch: 16, Steps: 60 | Train Loss: 0.5106756 Vali Loss: 1.5666373 Test Loss: 0.4929348
Validation loss decreased (1.572695 --> 1.566637).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.5458359718322754
Epoch: 17, Steps: 60 | Train Loss: 0.5077070 Vali Loss: 1.5552180 Test Loss: 0.4898245
Validation loss decreased (1.566637 --> 1.555218).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.5690529346466064
Epoch: 18, Steps: 60 | Train Loss: 0.5062998 Vali Loss: 1.5548778 Test Loss: 0.4871634
Validation loss decreased (1.555218 --> 1.554878).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.842207670211792
Epoch: 19, Steps: 60 | Train Loss: 0.5048626 Vali Loss: 1.5550107 Test Loss: 0.4848553
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.4943654537200928
Epoch: 20, Steps: 60 | Train Loss: 0.5035104 Vali Loss: 1.5478461 Test Loss: 0.4827943
Validation loss decreased (1.554878 --> 1.547846).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.5415418148040771
Epoch: 21, Steps: 60 | Train Loss: 0.5022151 Vali Loss: 1.5446401 Test Loss: 0.4809614
Validation loss decreased (1.547846 --> 1.544640).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.408817768096924
Epoch: 22, Steps: 60 | Train Loss: 0.5012585 Vali Loss: 1.5435222 Test Loss: 0.4793296
Validation loss decreased (1.544640 --> 1.543522).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6007144451141357
Epoch: 23, Steps: 60 | Train Loss: 0.5003547 Vali Loss: 1.5466540 Test Loss: 0.4777817
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4771077632904053
Epoch: 24, Steps: 60 | Train Loss: 0.4990775 Vali Loss: 1.5408965 Test Loss: 0.4764156
Validation loss decreased (1.543522 --> 1.540897).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.4413278102874756
Epoch: 25, Steps: 60 | Train Loss: 0.4983394 Vali Loss: 1.5408113 Test Loss: 0.4752145
Validation loss decreased (1.540897 --> 1.540811).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.3823521137237549
Epoch: 26, Steps: 60 | Train Loss: 0.4976681 Vali Loss: 1.5397849 Test Loss: 0.4740881
Validation loss decreased (1.540811 --> 1.539785).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5484442710876465
Epoch: 27, Steps: 60 | Train Loss: 0.4967197 Vali Loss: 1.5400331 Test Loss: 0.4730577
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5205764770507812
Epoch: 28, Steps: 60 | Train Loss: 0.4962010 Vali Loss: 1.5284293 Test Loss: 0.4720710
Validation loss decreased (1.539785 --> 1.528429).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.589257001876831
Epoch: 29, Steps: 60 | Train Loss: 0.4958731 Vali Loss: 1.5369360 Test Loss: 0.4712253
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5293021202087402
Epoch: 30, Steps: 60 | Train Loss: 0.4954255 Vali Loss: 1.5331835 Test Loss: 0.4704157
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.062866449356079
Epoch: 31, Steps: 60 | Train Loss: 0.4951235 Vali Loss: 1.5343513 Test Loss: 0.4696305
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.5947706699371338
Epoch: 32, Steps: 60 | Train Loss: 0.4943668 Vali Loss: 1.5256228 Test Loss: 0.4689447
Validation loss decreased (1.528429 --> 1.525623).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.100266695022583
Epoch: 33, Steps: 60 | Train Loss: 0.4938991 Vali Loss: 1.5292138 Test Loss: 0.4682575
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6701018810272217
Epoch: 34, Steps: 60 | Train Loss: 0.4932303 Vali Loss: 1.5313972 Test Loss: 0.4676472
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.5073745250701904
Epoch: 35, Steps: 60 | Train Loss: 0.4928506 Vali Loss: 1.5298154 Test Loss: 0.4670651
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6916911602020264
Epoch: 36, Steps: 60 | Train Loss: 0.4926579 Vali Loss: 1.5276285 Test Loss: 0.4665242
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6866905689239502
Epoch: 37, Steps: 60 | Train Loss: 0.4925750 Vali Loss: 1.5266205 Test Loss: 0.4660344
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.858579397201538
Epoch: 38, Steps: 60 | Train Loss: 0.4920135 Vali Loss: 1.5258968 Test Loss: 0.4655716
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.527881383895874
Epoch: 39, Steps: 60 | Train Loss: 0.4913371 Vali Loss: 1.5259994 Test Loss: 0.4651128
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.646207571029663
Epoch: 40, Steps: 60 | Train Loss: 0.4915350 Vali Loss: 1.5303347 Test Loss: 0.4647007
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6111531257629395
Epoch: 41, Steps: 60 | Train Loss: 0.4914290 Vali Loss: 1.5294911 Test Loss: 0.4643199
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.6277391910552979
Epoch: 42, Steps: 60 | Train Loss: 0.4916143 Vali Loss: 1.5246785 Test Loss: 0.4639599
Validation loss decreased (1.525623 --> 1.524678).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.4824187755584717
Epoch: 43, Steps: 60 | Train Loss: 0.4909328 Vali Loss: 1.5217243 Test Loss: 0.4635764
Validation loss decreased (1.524678 --> 1.521724).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.5986402034759521
Epoch: 44, Steps: 60 | Train Loss: 0.4903063 Vali Loss: 1.5185680 Test Loss: 0.4632438
Validation loss decreased (1.521724 --> 1.518568).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5069124698638916
Epoch: 45, Steps: 60 | Train Loss: 0.4902762 Vali Loss: 1.5200180 Test Loss: 0.4629514
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6021645069122314
Epoch: 46, Steps: 60 | Train Loss: 0.4902788 Vali Loss: 1.5158663 Test Loss: 0.4626515
Validation loss decreased (1.518568 --> 1.515866).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.610851764678955
Epoch: 47, Steps: 60 | Train Loss: 0.4898538 Vali Loss: 1.5229745 Test Loss: 0.4623884
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.7115733623504639
Epoch: 48, Steps: 60 | Train Loss: 0.4898354 Vali Loss: 1.5239365 Test Loss: 0.4621331
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.5993297100067139
Epoch: 49, Steps: 60 | Train Loss: 0.4893080 Vali Loss: 1.5245175 Test Loss: 0.4618843
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.4542531967163086
Epoch: 50, Steps: 60 | Train Loss: 0.4892187 Vali Loss: 1.5223716 Test Loss: 0.4616461
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.5192515850067139
Epoch: 51, Steps: 60 | Train Loss: 0.4892216 Vali Loss: 1.5237472 Test Loss: 0.4614316
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6297616958618164
Epoch: 52, Steps: 60 | Train Loss: 0.4891624 Vali Loss: 1.5268284 Test Loss: 0.4612369
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.4872825145721436
Epoch: 53, Steps: 60 | Train Loss: 0.4891733 Vali Loss: 1.5225809 Test Loss: 0.4610381
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5470869541168213
Epoch: 54, Steps: 60 | Train Loss: 0.4888933 Vali Loss: 1.5230715 Test Loss: 0.4608485
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.4405465126037598
Epoch: 55, Steps: 60 | Train Loss: 0.4887436 Vali Loss: 1.5093679 Test Loss: 0.4606804
Validation loss decreased (1.515866 --> 1.509368).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.597912073135376
Epoch: 56, Steps: 60 | Train Loss: 0.4887064 Vali Loss: 1.5153146 Test Loss: 0.4605106
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.6208884716033936
Epoch: 57, Steps: 60 | Train Loss: 0.4884515 Vali Loss: 1.5188440 Test Loss: 0.4603481
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.551361322402954
Epoch: 58, Steps: 60 | Train Loss: 0.4884563 Vali Loss: 1.5178885 Test Loss: 0.4601875
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.6198136806488037
Epoch: 59, Steps: 60 | Train Loss: 0.4884666 Vali Loss: 1.5226139 Test Loss: 0.4600614
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.7051668167114258
Epoch: 60, Steps: 60 | Train Loss: 0.4883345 Vali Loss: 1.5207584 Test Loss: 0.4599285
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6770031452178955
Epoch: 61, Steps: 60 | Train Loss: 0.4879579 Vali Loss: 1.5199248 Test Loss: 0.4598027
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.5823020935058594
Epoch: 62, Steps: 60 | Train Loss: 0.4881659 Vali Loss: 1.5124120 Test Loss: 0.4596749
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.7881357669830322
Epoch: 63, Steps: 60 | Train Loss: 0.4879615 Vali Loss: 1.5227005 Test Loss: 0.4595622
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.5384695529937744
Epoch: 64, Steps: 60 | Train Loss: 0.4882311 Vali Loss: 1.5167673 Test Loss: 0.4594465
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.974134922027588
Epoch: 65, Steps: 60 | Train Loss: 0.4876056 Vali Loss: 1.5147165 Test Loss: 0.4593485
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.4949450492858887
Epoch: 66, Steps: 60 | Train Loss: 0.4882815 Vali Loss: 1.5172663 Test Loss: 0.4592569
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.6951861381530762
Epoch: 67, Steps: 60 | Train Loss: 0.4877600 Vali Loss: 1.5179373 Test Loss: 0.4591556
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.5220527648925781
Epoch: 68, Steps: 60 | Train Loss: 0.4877574 Vali Loss: 1.5154665 Test Loss: 0.4590692
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.4861063957214355
Epoch: 69, Steps: 60 | Train Loss: 0.4875427 Vali Loss: 1.5187796 Test Loss: 0.4589849
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5079545974731445
Epoch: 70, Steps: 60 | Train Loss: 0.4876798 Vali Loss: 1.5192136 Test Loss: 0.4589040
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.5505971908569336
Epoch: 71, Steps: 60 | Train Loss: 0.4874272 Vali Loss: 1.5093415 Test Loss: 0.4588307
Validation loss decreased (1.509368 --> 1.509341).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.5504350662231445
Epoch: 72, Steps: 60 | Train Loss: 0.4875094 Vali Loss: 1.5165460 Test Loss: 0.4587560
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.4963011741638184
Epoch: 73, Steps: 60 | Train Loss: 0.4873720 Vali Loss: 1.5217130 Test Loss: 0.4586906
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.4869911670684814
Epoch: 74, Steps: 60 | Train Loss: 0.4872495 Vali Loss: 1.5166190 Test Loss: 0.4586234
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.131784677505493
Epoch: 75, Steps: 60 | Train Loss: 0.4874458 Vali Loss: 1.5231078 Test Loss: 0.4585621
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.5752947330474854
Epoch: 76, Steps: 60 | Train Loss: 0.4870680 Vali Loss: 1.5152465 Test Loss: 0.4584982
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.5953476428985596
Epoch: 77, Steps: 60 | Train Loss: 0.4869682 Vali Loss: 1.5134913 Test Loss: 0.4584418
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.4738073348999023
Epoch: 78, Steps: 60 | Train Loss: 0.4872566 Vali Loss: 1.5162823 Test Loss: 0.4583934
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.0574803352355957
Epoch: 79, Steps: 60 | Train Loss: 0.4874026 Vali Loss: 1.5178639 Test Loss: 0.4583407
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.6955170631408691
Epoch: 80, Steps: 60 | Train Loss: 0.4871836 Vali Loss: 1.5078630 Test Loss: 0.4582940
Validation loss decreased (1.509341 --> 1.507863).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.5544829368591309
Epoch: 81, Steps: 60 | Train Loss: 0.4873818 Vali Loss: 1.5153580 Test Loss: 0.4582509
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.9643497467041016
Epoch: 82, Steps: 60 | Train Loss: 0.4872492 Vali Loss: 1.5107373 Test Loss: 0.4582070
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.7216713428497314
Epoch: 83, Steps: 60 | Train Loss: 0.4869577 Vali Loss: 1.5172358 Test Loss: 0.4581685
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.0124170780181885
Epoch: 84, Steps: 60 | Train Loss: 0.4873316 Vali Loss: 1.5129482 Test Loss: 0.4581283
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.6142609119415283
Epoch: 85, Steps: 60 | Train Loss: 0.4873439 Vali Loss: 1.5107625 Test Loss: 0.4580920
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.5851314067840576
Epoch: 86, Steps: 60 | Train Loss: 0.4872599 Vali Loss: 1.5161026 Test Loss: 0.4580553
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.343799352645874
Epoch: 87, Steps: 60 | Train Loss: 0.4871608 Vali Loss: 1.5141814 Test Loss: 0.4580212
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.5856337547302246
Epoch: 88, Steps: 60 | Train Loss: 0.4871855 Vali Loss: 1.5188098 Test Loss: 0.4579928
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.7532963752746582
Epoch: 89, Steps: 60 | Train Loss: 0.4874813 Vali Loss: 1.5146351 Test Loss: 0.4579610
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.138167142868042
Epoch: 90, Steps: 60 | Train Loss: 0.4866901 Vali Loss: 1.5194905 Test Loss: 0.4579321
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.6688988208770752
Epoch: 91, Steps: 60 | Train Loss: 0.4870495 Vali Loss: 1.5270405 Test Loss: 0.4579068
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.6250064373016357
Epoch: 92, Steps: 60 | Train Loss: 0.4870399 Vali Loss: 1.5138369 Test Loss: 0.4578783
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.20888614654541
Epoch: 93, Steps: 60 | Train Loss: 0.4869797 Vali Loss: 1.5151336 Test Loss: 0.4578549
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.9033112525939941
Epoch: 94, Steps: 60 | Train Loss: 0.4869661 Vali Loss: 1.5103884 Test Loss: 0.4578320
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.502479076385498
Epoch: 95, Steps: 60 | Train Loss: 0.4860194 Vali Loss: 1.5117548 Test Loss: 0.4578106
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.599536418914795
Epoch: 96, Steps: 60 | Train Loss: 0.4866987 Vali Loss: 1.5151378 Test Loss: 0.4577892
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.5261025428771973
Epoch: 97, Steps: 60 | Train Loss: 0.4866290 Vali Loss: 1.5137255 Test Loss: 0.4577679
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.5363531112670898
Epoch: 98, Steps: 60 | Train Loss: 0.4869526 Vali Loss: 1.5208130 Test Loss: 0.4577491
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.550593376159668
Epoch: 99, Steps: 60 | Train Loss: 0.4868607 Vali Loss: 1.5196042 Test Loss: 0.4577318
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.4712772369384766
Epoch: 100, Steps: 60 | Train Loss: 0.4867273 Vali Loss: 1.5171888 Test Loss: 0.4577154
EarlyStopping counter: 20 out of 20
Early stopping
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=34, out_features=170, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5178880.0
params:  5950.0
Trainable parameters:  5950
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.5851566791534424
Epoch: 1, Steps: 60 | Train Loss: 0.5963900 Vali Loss: 1.5084282 Test Loss: 0.4516419
Validation loss decreased (inf --> 1.508428).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5308358669281006
Epoch: 2, Steps: 60 | Train Loss: 0.5926722 Vali Loss: 1.4944274 Test Loss: 0.4479716
Validation loss decreased (1.508428 --> 1.494427).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.505091905593872
Epoch: 3, Steps: 60 | Train Loss: 0.5907257 Vali Loss: 1.4973361 Test Loss: 0.4458463
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6458680629730225
Epoch: 4, Steps: 60 | Train Loss: 0.5899487 Vali Loss: 1.4972465 Test Loss: 0.4446253
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.4669482707977295
Epoch: 5, Steps: 60 | Train Loss: 0.5891482 Vali Loss: 1.4887649 Test Loss: 0.4442358
Validation loss decreased (1.494427 --> 1.488765).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.4480669498443604
Epoch: 6, Steps: 60 | Train Loss: 0.5887007 Vali Loss: 1.4927783 Test Loss: 0.4439282
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4998903274536133
Epoch: 7, Steps: 60 | Train Loss: 0.5883682 Vali Loss: 1.4903762 Test Loss: 0.4437875
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.4600574970245361
Epoch: 8, Steps: 60 | Train Loss: 0.5883582 Vali Loss: 1.4890130 Test Loss: 0.4439414
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.5371956825256348
Epoch: 9, Steps: 60 | Train Loss: 0.5879186 Vali Loss: 1.4953854 Test Loss: 0.4441276
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.5625548362731934
Epoch: 10, Steps: 60 | Train Loss: 0.5881610 Vali Loss: 1.4945111 Test Loss: 0.4441335
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.618058204650879
Epoch: 11, Steps: 60 | Train Loss: 0.5881164 Vali Loss: 1.4978920 Test Loss: 0.4442141
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.5917518138885498
Epoch: 12, Steps: 60 | Train Loss: 0.5879868 Vali Loss: 1.4921360 Test Loss: 0.4443701
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.545431137084961
Epoch: 13, Steps: 60 | Train Loss: 0.5877825 Vali Loss: 1.4959050 Test Loss: 0.4444548
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5832226276397705
Epoch: 14, Steps: 60 | Train Loss: 0.5881430 Vali Loss: 1.4924946 Test Loss: 0.4444327
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6532890796661377
Epoch: 15, Steps: 60 | Train Loss: 0.5875704 Vali Loss: 1.4920118 Test Loss: 0.4444788
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.535628318786621
Epoch: 16, Steps: 60 | Train Loss: 0.5878233 Vali Loss: 1.4869173 Test Loss: 0.4445973
Validation loss decreased (1.488765 --> 1.486917).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.6299946308135986
Epoch: 17, Steps: 60 | Train Loss: 0.5878242 Vali Loss: 1.4960439 Test Loss: 0.4446138
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.612757682800293
Epoch: 18, Steps: 60 | Train Loss: 0.5876431 Vali Loss: 1.4993899 Test Loss: 0.4446454
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5562121868133545
Epoch: 19, Steps: 60 | Train Loss: 0.5870332 Vali Loss: 1.4946238 Test Loss: 0.4446834
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.505129098892212
Epoch: 20, Steps: 60 | Train Loss: 0.5875934 Vali Loss: 1.4938154 Test Loss: 0.4447426
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4577412605285645
Epoch: 21, Steps: 60 | Train Loss: 0.5872579 Vali Loss: 1.4925356 Test Loss: 0.4447334
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5175831317901611
Epoch: 22, Steps: 60 | Train Loss: 0.5878835 Vali Loss: 1.4894500 Test Loss: 0.4447139
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6256229877471924
Epoch: 23, Steps: 60 | Train Loss: 0.5878499 Vali Loss: 1.4935137 Test Loss: 0.4448026
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.639172077178955
Epoch: 24, Steps: 60 | Train Loss: 0.5875939 Vali Loss: 1.4882064 Test Loss: 0.4449002
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.5542426109313965
Epoch: 25, Steps: 60 | Train Loss: 0.5875903 Vali Loss: 1.4909773 Test Loss: 0.4448636
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.6162934303283691
Epoch: 26, Steps: 60 | Train Loss: 0.5872426 Vali Loss: 1.4975264 Test Loss: 0.4448974
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5163233280181885
Epoch: 27, Steps: 60 | Train Loss: 0.5873854 Vali Loss: 1.4936671 Test Loss: 0.4448932
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.294621467590332
Epoch: 28, Steps: 60 | Train Loss: 0.5874974 Vali Loss: 1.4956509 Test Loss: 0.4449456
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.5239217281341553
Epoch: 29, Steps: 60 | Train Loss: 0.5875408 Vali Loss: 1.4916482 Test Loss: 0.4449238
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.490461826324463
Epoch: 30, Steps: 60 | Train Loss: 0.5877401 Vali Loss: 1.4936173 Test Loss: 0.4449767
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.615046739578247
Epoch: 31, Steps: 60 | Train Loss: 0.5874316 Vali Loss: 1.4943900 Test Loss: 0.4450239
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.5951719284057617
Epoch: 32, Steps: 60 | Train Loss: 0.5874185 Vali Loss: 1.4981999 Test Loss: 0.4449848
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4765205383300781
Epoch: 33, Steps: 60 | Train Loss: 0.5871174 Vali Loss: 1.4933605 Test Loss: 0.4450198
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.4857666492462158
Epoch: 34, Steps: 60 | Train Loss: 0.5877273 Vali Loss: 1.4949393 Test Loss: 0.4450062
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.5425424575805664
Epoch: 35, Steps: 60 | Train Loss: 0.5868430 Vali Loss: 1.4950538 Test Loss: 0.4450657
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.5374839305877686
Epoch: 36, Steps: 60 | Train Loss: 0.5875901 Vali Loss: 1.4967902 Test Loss: 0.4450244
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4434675872325897, mae:0.45265865325927734, rse:0.6375040411949158, corr:[0.22988953 0.23358388 0.23415105 0.23231399 0.229991   0.2286905
 0.22899243 0.22976117 0.22949499 0.22940905 0.22962892 0.22983804
 0.22948934 0.22842997 0.22781621 0.2280785  0.22832838 0.22802621
 0.22734377 0.22687165 0.2270781  0.2278357  0.22841191 0.22819689
 0.22722708 0.22676413 0.22663543 0.22630222 0.2254183  0.22458692
 0.22428018 0.22438611 0.22420317 0.22385615 0.2238833  0.22465423
 0.22557317 0.2253884  0.22469875 0.22441003 0.22486523 0.22539422
 0.22541851 0.2253334  0.22570768 0.22633511 0.22692989 0.22668318
 0.22533874 0.2244721  0.2236282  0.2225735  0.2212494  0.21993646
 0.21919286 0.21907589 0.2190326  0.21910995 0.21905391 0.21961491
 0.22037059 0.22035183 0.21995233 0.21968973 0.21972154 0.21986589
 0.21982533 0.21959981 0.21961874 0.21985133 0.21995868 0.21941192
 0.21807024 0.21717218 0.21656734 0.2162367  0.21599385 0.2156783
 0.21568632 0.21589239 0.2156634  0.21523471 0.2147012  0.21474364
 0.21524596 0.2151259  0.21472324 0.21445264 0.21438675 0.2142945
 0.2138629  0.21348208 0.21371719 0.21447279 0.215426   0.21590175
 0.21552423 0.21539158 0.21556681 0.2155302  0.21524824 0.2148959
 0.21463895 0.2148736  0.21488634 0.21469095 0.21439403 0.21470973
 0.21538219 0.21542543 0.21504396 0.21471971 0.21456237 0.214446
 0.21430208 0.2142445  0.21452639 0.21484463 0.21494398 0.2144901
 0.21336094 0.21258637 0.21176969 0.2110299  0.21025033 0.20975378
 0.20980518 0.21027622 0.21031615 0.21014939 0.2100767  0.21072803
 0.21163729 0.2116734  0.21137418 0.21122995 0.2113645  0.21155359
 0.21151058 0.21138391 0.21151978 0.21185473 0.21212631 0.2119051
 0.21106938 0.21042514 0.20987017 0.20889801 0.20796204 0.20739079
 0.2074276  0.20781869 0.20815659 0.20826891 0.20814806 0.2083777
 0.20889145 0.2087773  0.20841143 0.20812008 0.2080574  0.20824184
 0.20838855 0.20845029 0.20869312 0.20900747 0.20919234 0.20893979
 0.2082784  0.20806734 0.20791109 0.20754303 0.20709491 0.20678467
 0.2069502  0.20762104 0.20803873 0.20834167 0.20857729 0.20914525
 0.20987949 0.20993261 0.20975158 0.20975164 0.20992696 0.2101343
 0.21016178 0.2101076  0.2103454  0.2106706  0.21076442 0.21027666
 0.20917314 0.20851736 0.20800081 0.2072789  0.2063574  0.20552713
 0.20519866 0.20544443 0.20572384 0.20587179 0.20583598 0.20636638
 0.20710357 0.20724502 0.20703965 0.20677637 0.20662546 0.20656514
 0.20637634 0.20615223 0.20618972 0.20635435 0.20639035 0.20606706
 0.20539455 0.20518632 0.20506759 0.20468013 0.20418161 0.2036517
 0.20356794 0.20385401 0.20397538 0.20385988 0.20366496 0.20395745
 0.2045931  0.20455398 0.20424987 0.2041019  0.20404632 0.20392181
 0.20364226 0.20338456 0.20342468 0.20361133 0.20371267 0.20338313
 0.20270275 0.20240061 0.2022112  0.20194913 0.20158544 0.20128438
 0.2014511  0.20188165 0.2021643  0.20230712 0.20245098 0.2031372
 0.2041156  0.20442052 0.20441525 0.20431608 0.20428099 0.20427433
 0.20415705 0.20383361 0.2037894  0.20392925 0.20397374 0.20367828
 0.20304108 0.20283918 0.2026796  0.20226596 0.20167911 0.20116512
 0.20112902 0.20156698 0.20173219 0.20168123 0.20150813 0.20160982
 0.20218356 0.20236675 0.20227483 0.20208414 0.2018503  0.20167592
 0.20153531 0.20157653 0.20195481 0.20241353 0.20274183 0.20276189
 0.20249893 0.20261866 0.20290828 0.20293789 0.2026979  0.20261471
 0.20293103 0.20346816 0.20368984 0.2036214  0.20351359 0.20397359
 0.20482098 0.20504184 0.20495294 0.20492615 0.20498727 0.20506899
 0.20502986 0.20490888 0.20489046 0.2049941  0.20500948 0.20482333
 0.20438474 0.20429172 0.20414639 0.20363005 0.20300953 0.20241946
 0.20229702 0.20254683 0.20255396 0.20233072 0.20211318 0.20248373
 0.20336503 0.20372953 0.20382898 0.20400907 0.20405191 0.20391247
 0.20367116 0.20363931 0.20394985 0.20440382 0.2046214  0.2043012
 0.20355542 0.20323755 0.20305459 0.20271651 0.20218633 0.20171738
 0.20173544 0.2021184  0.20235197 0.20229097 0.20225835 0.2026968
 0.20335099 0.20350592 0.20337173 0.20322414 0.20311052 0.20313808
 0.20307726 0.20297271 0.20304683 0.20324934 0.20316549 0.20280279
 0.20229007 0.20216724 0.2020593  0.20167463 0.20112771 0.20073804
 0.2007434  0.20076528 0.20041339 0.19986263 0.19949377 0.19962229
 0.20006002 0.19995494 0.19969818 0.19961278 0.19967447 0.19970407
 0.19968449 0.19980672 0.20026281 0.20095398 0.20143887 0.20167631
 0.20170896 0.20207684 0.20239046 0.20226505 0.2018284  0.20120716
 0.20093212 0.20095596 0.20073368 0.20034593 0.1999858  0.20041044
 0.20143014 0.20174292 0.20170791 0.20165484 0.2017513  0.20204791
 0.20225887 0.20242232 0.20281726 0.20342255 0.20388052 0.20399526
 0.20373744 0.2038231  0.20394973 0.20363113 0.20307861 0.2025676
 0.20261016 0.2030624  0.20330888 0.203252   0.20327272 0.20375842
 0.20448822 0.20459059 0.20429805 0.20404132 0.20392214 0.20393902
 0.2039766  0.20420149 0.20460346 0.20536616 0.20605259 0.20631097
 0.20600565 0.20596984 0.20596483 0.20567684 0.20537929 0.20512937
 0.2050919  0.20532846 0.20538484 0.20540118 0.20549968 0.2060699
 0.2067239  0.20656027 0.20614338 0.206086   0.20638742 0.20674641
 0.20682043 0.20672372 0.2069045  0.20747282 0.20809244 0.20844182
 0.20824659 0.20829882 0.20821574 0.2077294  0.20705506 0.20633611
 0.20605162 0.20626634 0.2066073  0.20690386 0.20718768 0.20779347
 0.20857804 0.20875067 0.20873363 0.20877703 0.2088995  0.20896398
 0.20883149 0.20876531 0.20895913 0.20919366 0.2093731  0.20916322
 0.20844668 0.2080194  0.20771088 0.2072524  0.20678897 0.20635639
 0.20640615 0.20685469 0.20720199 0.20733236 0.20726156 0.2075661
 0.20829569 0.20842682 0.20826559 0.20821139 0.20824338 0.20821866
 0.20809461 0.2081569  0.20866959 0.20950276 0.21030858 0.2106897
 0.21054612 0.21077609 0.21098553 0.21073519 0.2101735  0.20951849
 0.20919381 0.20914197 0.20907663 0.20877957 0.20854487 0.20892078
 0.20961273 0.20974351 0.20983583 0.21021226 0.21071638 0.21104306
 0.2111468  0.21141383 0.21202517 0.21275637 0.21328524 0.2131539
 0.21238595 0.21210381 0.2120676  0.21168868 0.21086858 0.20996857
 0.20957562 0.20968887 0.20968555 0.20953305 0.20940447 0.20983724
 0.2106882  0.21086486 0.21075061 0.2107814  0.21100351 0.2112401
 0.21118519 0.2110373  0.2110979  0.2112413  0.21122482 0.21067132
 0.20942234 0.20875716 0.2085299  0.20803432 0.20716776 0.20642368
 0.20619291 0.2063073  0.20650652 0.20652969 0.20640823 0.20683454
 0.20780087 0.20779    0.20734353 0.20699763 0.20697469 0.20705478
 0.20692784 0.20656785 0.20650408 0.20670034 0.20687917 0.20644093
 0.205296   0.20471174 0.20441242 0.20388225 0.20314962 0.20216022
 0.20171006 0.20170909 0.2017625  0.20162126 0.20131093 0.20152591
 0.20202483 0.20183118 0.20120314 0.20069367 0.20048708 0.20044328
 0.20032729 0.20014419 0.20004041 0.20003146 0.19992277 0.19940229
 0.19823064 0.19743735 0.19689062 0.19599298 0.19490589 0.1940365
 0.19363767 0.19363508 0.193609   0.19343175 0.19328466 0.19357267
 0.1942871  0.19425994 0.19379075 0.19344848 0.19331405 0.19327708
 0.19310533 0.1929481  0.19301493 0.19318189 0.19319052 0.19255716
 0.19128338 0.19050519 0.19013949 0.18951766 0.18876465 0.18808603
 0.18796797 0.18816051 0.18818273 0.18809967 0.18789734 0.18820316
 0.18904956 0.18903625 0.1884989  0.18810819 0.18790546 0.1878599
 0.18775323 0.18768767 0.18777217 0.18799761 0.18813893 0.18772131
 0.1865299  0.18561336 0.18492725 0.18430616 0.18368329 0.18312109
 0.18304364 0.18330307 0.18358357 0.18363416 0.1835785  0.18396285
 0.18471935 0.18473816 0.18431772 0.18393297 0.18382415 0.18383272
 0.18374272 0.1836168  0.1836223  0.18376759 0.18366699 0.18282995
 0.18103063 0.17973337 0.17899466 0.17793821 0.17672493 0.17567113
 0.17549518 0.1758784  0.17638952 0.17666435 0.17686993 0.1776292
 0.17864604 0.17853986 0.17791812 0.17748915 0.17736493 0.17736764
 0.17729488 0.17731267 0.17754796 0.17793234 0.17817219 0.17773141
 0.17664203 0.17609546 0.17580143 0.17522193 0.17437863 0.17364614
 0.17329767 0.17333412 0.17350239 0.1733382  0.17289692 0.1729169
 0.17342535 0.17279536 0.17159389 0.17077358 0.17076403 0.17115602
 0.17138444 0.17114908 0.17095655 0.17123696 0.17198607 0.17088233]
