Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=26, out_features=123, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2865408.0
params:  3321.0
Trainable parameters:  3321
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.1064858436584473
Epoch: 1, Steps: 64 | Train Loss: 1.0358706 Vali Loss: 2.0240285 Test Loss: 1.0323370
Validation loss decreased (inf --> 2.024029).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.27702260017395
Epoch: 2, Steps: 64 | Train Loss: 0.7846496 Vali Loss: 1.7122298 Test Loss: 0.7712848
Validation loss decreased (2.024029 --> 1.712230).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.4384028911590576
Epoch: 3, Steps: 64 | Train Loss: 0.6617534 Vali Loss: 1.5557045 Test Loss: 0.6472989
Validation loss decreased (1.712230 --> 1.555704).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.284515380859375
Epoch: 4, Steps: 64 | Train Loss: 0.5968588 Vali Loss: 1.4617275 Test Loss: 0.5823210
Validation loss decreased (1.555704 --> 1.461727).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.2674121856689453
Epoch: 5, Steps: 64 | Train Loss: 0.5601527 Vali Loss: 1.4108435 Test Loss: 0.5457404
Validation loss decreased (1.461727 --> 1.410843).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.712921619415283
Epoch: 6, Steps: 64 | Train Loss: 0.5379909 Vali Loss: 1.3799541 Test Loss: 0.5244087
Validation loss decreased (1.410843 --> 1.379954).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.7844302654266357
Epoch: 7, Steps: 64 | Train Loss: 0.5244727 Vali Loss: 1.3587700 Test Loss: 0.5115622
Validation loss decreased (1.379954 --> 1.358770).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.5983211994171143
Epoch: 8, Steps: 64 | Train Loss: 0.5154794 Vali Loss: 1.3464117 Test Loss: 0.5035803
Validation loss decreased (1.358770 --> 1.346412).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.9332284927368164
Epoch: 9, Steps: 64 | Train Loss: 0.5103173 Vali Loss: 1.3390208 Test Loss: 0.4983596
Validation loss decreased (1.346412 --> 1.339021).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.1411235332489014
Epoch: 10, Steps: 64 | Train Loss: 0.5059844 Vali Loss: 1.3341892 Test Loss: 0.4948406
Validation loss decreased (1.339021 --> 1.334189).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.087676525115967
Epoch: 11, Steps: 64 | Train Loss: 0.5028861 Vali Loss: 1.3183491 Test Loss: 0.4924246
Validation loss decreased (1.334189 --> 1.318349).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.6906607151031494
Epoch: 12, Steps: 64 | Train Loss: 0.5006053 Vali Loss: 1.3262948 Test Loss: 0.4906673
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.876213312149048
Epoch: 13, Steps: 64 | Train Loss: 0.4989793 Vali Loss: 1.3214132 Test Loss: 0.4893247
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.1300528049468994
Epoch: 14, Steps: 64 | Train Loss: 0.4977948 Vali Loss: 1.3131611 Test Loss: 0.4883170
Validation loss decreased (1.318349 --> 1.313161).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.3991127014160156
Epoch: 15, Steps: 64 | Train Loss: 0.4964004 Vali Loss: 1.3139660 Test Loss: 0.4874844
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.298011302947998
Epoch: 16, Steps: 64 | Train Loss: 0.4954736 Vali Loss: 1.3136917 Test Loss: 0.4867992
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.0291976928710938
Epoch: 17, Steps: 64 | Train Loss: 0.4946163 Vali Loss: 1.3148379 Test Loss: 0.4862340
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.3587381839752197
Epoch: 18, Steps: 64 | Train Loss: 0.4940255 Vali Loss: 1.3053980 Test Loss: 0.4857644
Validation loss decreased (1.313161 --> 1.305398).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.774495601654053
Epoch: 19, Steps: 64 | Train Loss: 0.4930664 Vali Loss: 1.3109838 Test Loss: 0.4853323
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.75774884223938
Epoch: 20, Steps: 64 | Train Loss: 0.4929011 Vali Loss: 1.3097699 Test Loss: 0.4849575
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.8004777431488037
Epoch: 21, Steps: 64 | Train Loss: 0.4923643 Vali Loss: 1.3073905 Test Loss: 0.4846081
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.531895875930786
Epoch: 22, Steps: 64 | Train Loss: 0.4917733 Vali Loss: 1.3048646 Test Loss: 0.4843508
Validation loss decreased (1.305398 --> 1.304865).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.371854782104492
Epoch: 23, Steps: 64 | Train Loss: 0.4916454 Vali Loss: 1.3035698 Test Loss: 0.4841011
Validation loss decreased (1.304865 --> 1.303570).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.899590253829956
Epoch: 24, Steps: 64 | Train Loss: 0.4913091 Vali Loss: 1.3049278 Test Loss: 0.4838921
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.5963494777679443
Epoch: 25, Steps: 64 | Train Loss: 0.4909577 Vali Loss: 1.2988765 Test Loss: 0.4836570
Validation loss decreased (1.303570 --> 1.298877).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.6484553813934326
Epoch: 26, Steps: 64 | Train Loss: 0.4905019 Vali Loss: 1.3055389 Test Loss: 0.4834934
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1612234115600586
Epoch: 27, Steps: 64 | Train Loss: 0.4901804 Vali Loss: 1.3046721 Test Loss: 0.4833625
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.348471164703369
Epoch: 28, Steps: 64 | Train Loss: 0.4900610 Vali Loss: 1.2982181 Test Loss: 0.4831962
Validation loss decreased (1.298877 --> 1.298218).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.7155966758728027
Epoch: 29, Steps: 64 | Train Loss: 0.4900489 Vali Loss: 1.3026131 Test Loss: 0.4830550
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.6677417755126953
Epoch: 30, Steps: 64 | Train Loss: 0.4896267 Vali Loss: 1.3025160 Test Loss: 0.4829374
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.7904868125915527
Epoch: 31, Steps: 64 | Train Loss: 0.4894758 Vali Loss: 1.3012612 Test Loss: 0.4828342
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.1484200954437256
Epoch: 32, Steps: 64 | Train Loss: 0.4891400 Vali Loss: 1.3029000 Test Loss: 0.4827308
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.3637259006500244
Epoch: 33, Steps: 64 | Train Loss: 0.4893970 Vali Loss: 1.2965854 Test Loss: 0.4826464
Validation loss decreased (1.298218 --> 1.296585).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.328845262527466
Epoch: 34, Steps: 64 | Train Loss: 0.4891583 Vali Loss: 1.3002698 Test Loss: 0.4825485
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.2528998851776123
Epoch: 35, Steps: 64 | Train Loss: 0.4888049 Vali Loss: 1.3004612 Test Loss: 0.4824796
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.664574384689331
Epoch: 36, Steps: 64 | Train Loss: 0.4889605 Vali Loss: 1.2980685 Test Loss: 0.4824066
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.6528587341308594
Epoch: 37, Steps: 64 | Train Loss: 0.4888537 Vali Loss: 1.2986679 Test Loss: 0.4823589
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.7291390895843506
Epoch: 38, Steps: 64 | Train Loss: 0.4885921 Vali Loss: 1.2911959 Test Loss: 0.4823054
Validation loss decreased (1.296585 --> 1.291196).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.7420384883880615
Epoch: 39, Steps: 64 | Train Loss: 0.4881816 Vali Loss: 1.2997085 Test Loss: 0.4822547
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.8387336730957031
Epoch: 40, Steps: 64 | Train Loss: 0.4886235 Vali Loss: 1.3003500 Test Loss: 0.4822031
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.1879630088806152
Epoch: 41, Steps: 64 | Train Loss: 0.4884332 Vali Loss: 1.3009672 Test Loss: 0.4821304
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.8971822261810303
Epoch: 42, Steps: 64 | Train Loss: 0.4883214 Vali Loss: 1.3009623 Test Loss: 0.4821120
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.4637176990509033
Epoch: 43, Steps: 64 | Train Loss: 0.4882199 Vali Loss: 1.3020263 Test Loss: 0.4820639
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.0144197940826416
Epoch: 44, Steps: 64 | Train Loss: 0.4884015 Vali Loss: 1.2918741 Test Loss: 0.4820275
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.957381010055542
Epoch: 45, Steps: 64 | Train Loss: 0.4880660 Vali Loss: 1.2956210 Test Loss: 0.4820054
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8672850131988525
Epoch: 46, Steps: 64 | Train Loss: 0.4880906 Vali Loss: 1.2927117 Test Loss: 0.4819604
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.018981456756592
Epoch: 47, Steps: 64 | Train Loss: 0.4881356 Vali Loss: 1.2997288 Test Loss: 0.4819211
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.5104641914367676
Epoch: 48, Steps: 64 | Train Loss: 0.4877671 Vali Loss: 1.2937148 Test Loss: 0.4819230
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.0952043533325195
Epoch: 49, Steps: 64 | Train Loss: 0.4878666 Vali Loss: 1.3018634 Test Loss: 0.4818851
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.809182643890381
Epoch: 50, Steps: 64 | Train Loss: 0.4878249 Vali Loss: 1.2993964 Test Loss: 0.4818753
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.3334155082702637
Epoch: 51, Steps: 64 | Train Loss: 0.4879100 Vali Loss: 1.3021878 Test Loss: 0.4818489
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.415001630783081
Epoch: 52, Steps: 64 | Train Loss: 0.4878536 Vali Loss: 1.2991545 Test Loss: 0.4818251
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.035114288330078
Epoch: 53, Steps: 64 | Train Loss: 0.4875732 Vali Loss: 1.2967868 Test Loss: 0.4818026
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.3280556201934814
Epoch: 54, Steps: 64 | Train Loss: 0.4877831 Vali Loss: 1.3002063 Test Loss: 0.4817880
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.394273519515991
Epoch: 55, Steps: 64 | Train Loss: 0.4877844 Vali Loss: 1.3004822 Test Loss: 0.4817626
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.75812029838562
Epoch: 56, Steps: 64 | Train Loss: 0.4875278 Vali Loss: 1.2999263 Test Loss: 0.4817596
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.0019986629486084
Epoch: 57, Steps: 64 | Train Loss: 0.4877713 Vali Loss: 1.2985564 Test Loss: 0.4817524
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.5907390117645264
Epoch: 58, Steps: 64 | Train Loss: 0.4876395 Vali Loss: 1.2961421 Test Loss: 0.4817422
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4817957580089569, mae:0.447652667760849, rse:0.6608205437660217, corr:[0.2515798  0.2528879  0.24974748 0.25067988 0.24963062 0.24635005
 0.24567892 0.24635172 0.24622877 0.24529104 0.24485365 0.24492833
 0.24457441 0.24377595 0.2433711  0.2437143  0.24417634 0.24419881
 0.24437015 0.24441203 0.2436295  0.2428752  0.242263   0.24172439
 0.24042234 0.23931791 0.23906183 0.23995525 0.24045175 0.24000975
 0.23992644 0.24043857 0.2407594  0.24012218 0.23955636 0.23979503
 0.23996684 0.23966064 0.2395461  0.23987126 0.24042179 0.24077235
 0.24116457 0.24131984 0.2410234  0.2406904  0.24050932 0.23998077
 0.23867083 0.23750408 0.2366618  0.23635511 0.2356879  0.23467824
 0.23454326 0.23460253 0.2345561  0.23427474 0.23387653 0.23399302
 0.23406908 0.23396663 0.2337114  0.23362575 0.23409112 0.23452105
 0.23479892 0.23481533 0.23441805 0.23398599 0.23358293 0.23267215
 0.23093194 0.22974734 0.22907947 0.22909562 0.22894505 0.22865556
 0.22887594 0.228933   0.22898965 0.22870605 0.2282567  0.22809343
 0.22795898 0.22787064 0.22790591 0.22789699 0.22795762 0.228067
 0.22812752 0.22818027 0.22807299 0.22809254 0.2280631  0.22751169
 0.22638872 0.2259564  0.2256822  0.22528577 0.22552964 0.22593746
 0.22621444 0.22608984 0.22611181 0.22614294 0.22585076 0.22567825
 0.22562832 0.2255491  0.22553854 0.22543725 0.22569086 0.22605209
 0.22620238 0.22615814 0.22611073 0.22614199 0.22583699 0.22460526
 0.22254291 0.2212913  0.22038332 0.21927585 0.21896611 0.21940011
 0.2200887  0.22016887 0.2202705  0.22041509 0.22013687 0.21993831
 0.21980478 0.21961427 0.21960142 0.21964811 0.21987106 0.22015058
 0.22036493 0.22049776 0.22044244 0.22046095 0.220372   0.21938308
 0.21731031 0.21601002 0.2155396  0.21458456 0.21377712 0.21407546
 0.21514125 0.21524402 0.21523169 0.21539079 0.2152321  0.21496
 0.21461563 0.2143778  0.21438548 0.21443197 0.21447213 0.21471974
 0.21509099 0.21518315 0.21508092 0.21513546 0.2149676  0.21382663
 0.21176347 0.21070759 0.21080178 0.21054295 0.20963277 0.20959163
 0.21057512 0.2106114  0.21042374 0.21058705 0.21062234 0.21049495
 0.21038516 0.21030134 0.21028897 0.21004629 0.2099551  0.21048962
 0.2111557  0.21130599 0.21131094 0.21155974 0.21160735 0.21062857
 0.20845567 0.2074573  0.20734923 0.20681779 0.2061032  0.20659487
 0.2077557  0.2078832  0.20762289 0.20760801 0.20748113 0.20728803
 0.20710357 0.20692295 0.20687866 0.20680341 0.20673461 0.20688349
 0.20726241 0.20735317 0.20713602 0.20716058 0.20710805 0.20620012
 0.2042406  0.2032609  0.20365712 0.2040312  0.2040437  0.20515005
 0.2072493  0.20801987 0.20805927 0.2081959  0.20821668 0.2081282
 0.20811278 0.2080925  0.20810336 0.20807739 0.20802198 0.20808864
 0.20844954 0.20867723 0.2086166  0.2086993  0.20876223 0.20788182
 0.20574965 0.20413555 0.20382296 0.20383129 0.20351575 0.20413394
 0.20600997 0.20674741 0.20670244 0.20686671 0.20702189 0.20675375
 0.20624484 0.20583786 0.2059353  0.20617932 0.20624034 0.20633677
 0.20681392 0.20706052 0.20682454 0.20663796 0.2065207  0.20577267
 0.20394716 0.20276998 0.20271052 0.2032296  0.2031799  0.20374049
 0.20568787 0.20669939 0.20660238 0.20655054 0.2067619  0.20658946
 0.20613606 0.20595531 0.20598204 0.20607305 0.20616487 0.20638411
 0.20680036 0.20698947 0.20692958 0.20705277 0.20714611 0.2064545
 0.20500322 0.20422277 0.20481814 0.20573092 0.20588802 0.20636567
 0.20789228 0.20862967 0.20857622 0.20820846 0.20808853 0.20802227
 0.20771295 0.20728268 0.2069554  0.20707105 0.20743011 0.20710966
 0.20695879 0.20735931 0.2073617  0.20704304 0.20688409 0.20591357
 0.20398413 0.20236568 0.20175216 0.20239559 0.20293066 0.20253469
 0.2036346  0.20494662 0.20528656 0.2045456  0.20397902 0.20406362
 0.20416194 0.20387647 0.20343591 0.20388982 0.20424801 0.20323958
 0.20256247 0.20366237 0.2023611  0.19820493 0.20008363 0.19744848]
