Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  35629440.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.169121026992798
Epoch: 1, Steps: 59 | Train Loss: 0.6962596 Vali Loss: 1.7527851 Test Loss: 0.8182055
Validation loss decreased (inf --> 1.752785).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6236553192138672
Epoch: 2, Steps: 59 | Train Loss: 0.5423968 Vali Loss: 1.5808245 Test Loss: 0.7303903
Validation loss decreased (1.752785 --> 1.580824).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.5857763290405273
Epoch: 3, Steps: 59 | Train Loss: 0.4706721 Vali Loss: 1.5129902 Test Loss: 0.6970912
Validation loss decreased (1.580824 --> 1.512990).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4903080463409424
Epoch: 4, Steps: 59 | Train Loss: 0.4299541 Vali Loss: 1.4757469 Test Loss: 0.6812934
Validation loss decreased (1.512990 --> 1.475747).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5803418159484863
Epoch: 5, Steps: 59 | Train Loss: 0.4018007 Vali Loss: 1.4478732 Test Loss: 0.6679874
Validation loss decreased (1.475747 --> 1.447873).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.591440200805664
Epoch: 6, Steps: 59 | Train Loss: 0.3795445 Vali Loss: 1.4241967 Test Loss: 0.6575477
Validation loss decreased (1.447873 --> 1.424197).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.571256399154663
Epoch: 7, Steps: 59 | Train Loss: 0.3611442 Vali Loss: 1.4092607 Test Loss: 0.6492633
Validation loss decreased (1.424197 --> 1.409261).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.548530101776123
Epoch: 8, Steps: 59 | Train Loss: 0.3455014 Vali Loss: 1.3934594 Test Loss: 0.6395286
Validation loss decreased (1.409261 --> 1.393459).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7207629680633545
Epoch: 9, Steps: 59 | Train Loss: 0.3318667 Vali Loss: 1.3788916 Test Loss: 0.6316644
Validation loss decreased (1.393459 --> 1.378892).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6985125541687012
Epoch: 10, Steps: 59 | Train Loss: 0.3197550 Vali Loss: 1.3705527 Test Loss: 0.6239483
Validation loss decreased (1.378892 --> 1.370553).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.589472770690918
Epoch: 11, Steps: 59 | Train Loss: 0.3091826 Vali Loss: 1.3550975 Test Loss: 0.6164389
Validation loss decreased (1.370553 --> 1.355098).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.436288833618164
Epoch: 12, Steps: 59 | Train Loss: 0.2998052 Vali Loss: 1.3477557 Test Loss: 0.6100980
Validation loss decreased (1.355098 --> 1.347756).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5237226486206055
Epoch: 13, Steps: 59 | Train Loss: 0.2913450 Vali Loss: 1.3375652 Test Loss: 0.6032534
Validation loss decreased (1.347756 --> 1.337565).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5875020027160645
Epoch: 14, Steps: 59 | Train Loss: 0.2836585 Vali Loss: 1.3334696 Test Loss: 0.5972108
Validation loss decreased (1.337565 --> 1.333470).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6455905437469482
Epoch: 15, Steps: 59 | Train Loss: 0.2769612 Vali Loss: 1.3277863 Test Loss: 0.5920216
Validation loss decreased (1.333470 --> 1.327786).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.675832748413086
Epoch: 16, Steps: 59 | Train Loss: 0.2706441 Vali Loss: 1.3194646 Test Loss: 0.5861349
Validation loss decreased (1.327786 --> 1.319465).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4427680969238281
Epoch: 17, Steps: 59 | Train Loss: 0.2650002 Vali Loss: 1.3150533 Test Loss: 0.5804267
Validation loss decreased (1.319465 --> 1.315053).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4212064743041992
Epoch: 18, Steps: 59 | Train Loss: 0.2600361 Vali Loss: 1.3056092 Test Loss: 0.5766737
Validation loss decreased (1.315053 --> 1.305609).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5895740985870361
Epoch: 19, Steps: 59 | Train Loss: 0.2552904 Vali Loss: 1.3034430 Test Loss: 0.5721794
Validation loss decreased (1.305609 --> 1.303443).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.813068151473999
Epoch: 20, Steps: 59 | Train Loss: 0.2508777 Vali Loss: 1.3024539 Test Loss: 0.5680583
Validation loss decreased (1.303443 --> 1.302454).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4040932655334473
Epoch: 21, Steps: 59 | Train Loss: 0.2471227 Vali Loss: 1.2949814 Test Loss: 0.5644069
Validation loss decreased (1.302454 --> 1.294981).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5487210750579834
Epoch: 22, Steps: 59 | Train Loss: 0.2434989 Vali Loss: 1.2915004 Test Loss: 0.5608063
Validation loss decreased (1.294981 --> 1.291500).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.4517402648925781
Epoch: 23, Steps: 59 | Train Loss: 0.2401775 Vali Loss: 1.2945272 Test Loss: 0.5572639
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.5009961128234863
Epoch: 24, Steps: 59 | Train Loss: 0.2372727 Vali Loss: 1.2862158 Test Loss: 0.5543125
Validation loss decreased (1.291500 --> 1.286216).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.6946659088134766
Epoch: 25, Steps: 59 | Train Loss: 0.2344221 Vali Loss: 1.2887379 Test Loss: 0.5513920
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5945630073547363
Epoch: 26, Steps: 59 | Train Loss: 0.2318494 Vali Loss: 1.2801002 Test Loss: 0.5485946
Validation loss decreased (1.286216 --> 1.280100).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.7569637298583984
Epoch: 27, Steps: 59 | Train Loss: 0.2293868 Vali Loss: 1.2798028 Test Loss: 0.5461030
Validation loss decreased (1.280100 --> 1.279803).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.348658800125122
Epoch: 28, Steps: 59 | Train Loss: 0.2272124 Vali Loss: 1.2749166 Test Loss: 0.5432562
Validation loss decreased (1.279803 --> 1.274917).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.5500609874725342
Epoch: 29, Steps: 59 | Train Loss: 0.2251473 Vali Loss: 1.2784256 Test Loss: 0.5413684
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4420368671417236
Epoch: 30, Steps: 59 | Train Loss: 0.2231184 Vali Loss: 1.2712587 Test Loss: 0.5391580
Validation loss decreased (1.274917 --> 1.271259).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.3978238105773926
Epoch: 31, Steps: 59 | Train Loss: 0.2213686 Vali Loss: 1.2756709 Test Loss: 0.5372521
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.7642371654510498
Epoch: 32, Steps: 59 | Train Loss: 0.2197126 Vali Loss: 1.2713218 Test Loss: 0.5352873
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.7974483966827393
Epoch: 33, Steps: 59 | Train Loss: 0.2180237 Vali Loss: 1.2657292 Test Loss: 0.5335335
Validation loss decreased (1.271259 --> 1.265729).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.8613166809082031
Epoch: 34, Steps: 59 | Train Loss: 0.2167107 Vali Loss: 1.2683580 Test Loss: 0.5316693
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.8849527835845947
Epoch: 35, Steps: 59 | Train Loss: 0.2153070 Vali Loss: 1.2626134 Test Loss: 0.5301419
Validation loss decreased (1.265729 --> 1.262613).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.0992393493652344
Epoch: 36, Steps: 59 | Train Loss: 0.2141035 Vali Loss: 1.2609130 Test Loss: 0.5286900
Validation loss decreased (1.262613 --> 1.260913).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5714364051818848
Epoch: 37, Steps: 59 | Train Loss: 0.2129788 Vali Loss: 1.2671508 Test Loss: 0.5272449
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.637058973312378
Epoch: 38, Steps: 59 | Train Loss: 0.2116221 Vali Loss: 1.2609564 Test Loss: 0.5258930
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.4154024124145508
Epoch: 39, Steps: 59 | Train Loss: 0.2106097 Vali Loss: 1.2566818 Test Loss: 0.5245240
Validation loss decreased (1.260913 --> 1.256682).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.4997549057006836
Epoch: 40, Steps: 59 | Train Loss: 0.2096276 Vali Loss: 1.2618377 Test Loss: 0.5233310
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6134719848632812
Epoch: 41, Steps: 59 | Train Loss: 0.2087295 Vali Loss: 1.2607819 Test Loss: 0.5223906
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.7190830707550049
Epoch: 42, Steps: 59 | Train Loss: 0.2078662 Vali Loss: 1.2593076 Test Loss: 0.5212240
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.5202298164367676
Epoch: 43, Steps: 59 | Train Loss: 0.2069637 Vali Loss: 1.2595738 Test Loss: 0.5201479
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.4813189506530762
Epoch: 44, Steps: 59 | Train Loss: 0.2062236 Vali Loss: 1.2560710 Test Loss: 0.5193070
Validation loss decreased (1.256682 --> 1.256071).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5216875076293945
Epoch: 45, Steps: 59 | Train Loss: 0.2055028 Vali Loss: 1.2572988 Test Loss: 0.5182693
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.5396437644958496
Epoch: 46, Steps: 59 | Train Loss: 0.2048819 Vali Loss: 1.2592269 Test Loss: 0.5175411
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.63126802444458
Epoch: 47, Steps: 59 | Train Loss: 0.2043143 Vali Loss: 1.2516317 Test Loss: 0.5166253
Validation loss decreased (1.256071 --> 1.251632).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.439687967300415
Epoch: 48, Steps: 59 | Train Loss: 0.2034966 Vali Loss: 1.2545117 Test Loss: 0.5158362
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.1310737133026123
Epoch: 49, Steps: 59 | Train Loss: 0.2029728 Vali Loss: 1.2548305 Test Loss: 0.5151283
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.4124853610992432
Epoch: 50, Steps: 59 | Train Loss: 0.2024267 Vali Loss: 1.2534362 Test Loss: 0.5144393
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.068798542022705
Epoch: 51, Steps: 59 | Train Loss: 0.2018660 Vali Loss: 1.2522930 Test Loss: 0.5136807
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.4331910610198975
Epoch: 52, Steps: 59 | Train Loss: 0.2013906 Vali Loss: 1.2531998 Test Loss: 0.5132147
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.3613247871398926
Epoch: 53, Steps: 59 | Train Loss: 0.2009540 Vali Loss: 1.2509426 Test Loss: 0.5126272
Validation loss decreased (1.251632 --> 1.250943).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.3540313243865967
Epoch: 54, Steps: 59 | Train Loss: 0.2004747 Vali Loss: 1.2545520 Test Loss: 0.5119445
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.102978229522705
Epoch: 55, Steps: 59 | Train Loss: 0.1999855 Vali Loss: 1.2496629 Test Loss: 0.5114965
Validation loss decreased (1.250943 --> 1.249663).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.4066004753112793
Epoch: 56, Steps: 59 | Train Loss: 0.1996552 Vali Loss: 1.2523481 Test Loss: 0.5110096
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.1245746612548828
Epoch: 57, Steps: 59 | Train Loss: 0.1994348 Vali Loss: 1.2506144 Test Loss: 0.5105115
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.197213888168335
Epoch: 58, Steps: 59 | Train Loss: 0.1989350 Vali Loss: 1.2537127 Test Loss: 0.5101022
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.174931287765503
Epoch: 59, Steps: 59 | Train Loss: 0.1985418 Vali Loss: 1.2501783 Test Loss: 0.5096882
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 0.9712085723876953
Epoch: 60, Steps: 59 | Train Loss: 0.1982097 Vali Loss: 1.2498649 Test Loss: 0.5092675
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 0.9595091342926025
Epoch: 61, Steps: 59 | Train Loss: 0.1979910 Vali Loss: 1.2492552 Test Loss: 0.5089133
Validation loss decreased (1.249663 --> 1.249255).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 0.9827065467834473
Epoch: 62, Steps: 59 | Train Loss: 0.1976279 Vali Loss: 1.2522007 Test Loss: 0.5084620
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 0.8317756652832031
Epoch: 63, Steps: 59 | Train Loss: 0.1974941 Vali Loss: 1.2507696 Test Loss: 0.5081431
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 0.8114173412322998
Epoch: 64, Steps: 59 | Train Loss: 0.1971658 Vali Loss: 1.2481478 Test Loss: 0.5078158
Validation loss decreased (1.249255 --> 1.248148).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 0.9491968154907227
Epoch: 65, Steps: 59 | Train Loss: 0.1969350 Vali Loss: 1.2460623 Test Loss: 0.5074914
Validation loss decreased (1.248148 --> 1.246062).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 0.89766526222229
Epoch: 66, Steps: 59 | Train Loss: 0.1965094 Vali Loss: 1.2536834 Test Loss: 0.5071834
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 0.9855246543884277
Epoch: 67, Steps: 59 | Train Loss: 0.1964763 Vali Loss: 1.2515298 Test Loss: 0.5069087
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.1120796203613281
Epoch: 68, Steps: 59 | Train Loss: 0.1962076 Vali Loss: 1.2523813 Test Loss: 0.5066313
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 0.9468808174133301
Epoch: 69, Steps: 59 | Train Loss: 0.1961235 Vali Loss: 1.2523371 Test Loss: 0.5064079
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.3094513416290283
Epoch: 70, Steps: 59 | Train Loss: 0.1958076 Vali Loss: 1.2490429 Test Loss: 0.5061386
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 0.876453161239624
Epoch: 71, Steps: 59 | Train Loss: 0.1955774 Vali Loss: 1.2508997 Test Loss: 0.5059032
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.0975921154022217
Epoch: 72, Steps: 59 | Train Loss: 0.1954791 Vali Loss: 1.2471546 Test Loss: 0.5056540
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 0.9831936359405518
Epoch: 73, Steps: 59 | Train Loss: 0.1952103 Vali Loss: 1.2499875 Test Loss: 0.5054879
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.0266087055206299
Epoch: 74, Steps: 59 | Train Loss: 0.1952048 Vali Loss: 1.2417556 Test Loss: 0.5052478
Validation loss decreased (1.246062 --> 1.241756).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.15095853805542
Epoch: 75, Steps: 59 | Train Loss: 0.1949994 Vali Loss: 1.2417440 Test Loss: 0.5050772
Validation loss decreased (1.241756 --> 1.241744).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.2919108867645264
Epoch: 76, Steps: 59 | Train Loss: 0.1947628 Vali Loss: 1.2510195 Test Loss: 0.5049030
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 0.8843326568603516
Epoch: 77, Steps: 59 | Train Loss: 0.1947726 Vali Loss: 1.2477183 Test Loss: 0.5047545
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 0.9728777408599854
Epoch: 78, Steps: 59 | Train Loss: 0.1946745 Vali Loss: 1.2534024 Test Loss: 0.5045862
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.2257671356201172
Epoch: 79, Steps: 59 | Train Loss: 0.1945170 Vali Loss: 1.2522312 Test Loss: 0.5044242
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 0.9407670497894287
Epoch: 80, Steps: 59 | Train Loss: 0.1942954 Vali Loss: 1.2476170 Test Loss: 0.5042855
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.0699396133422852
Epoch: 81, Steps: 59 | Train Loss: 0.1942366 Vali Loss: 1.2465073 Test Loss: 0.5041463
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.0922174453735352
Epoch: 82, Steps: 59 | Train Loss: 0.1941504 Vali Loss: 1.2469239 Test Loss: 0.5040046
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.1960768699645996
Epoch: 83, Steps: 59 | Train Loss: 0.1940222 Vali Loss: 1.2465969 Test Loss: 0.5038733
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.0431876182556152
Epoch: 84, Steps: 59 | Train Loss: 0.1939125 Vali Loss: 1.2445003 Test Loss: 0.5037477
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.0250537395477295
Epoch: 85, Steps: 59 | Train Loss: 0.1937796 Vali Loss: 1.2504284 Test Loss: 0.5036416
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.2306056022644043
Epoch: 86, Steps: 59 | Train Loss: 0.1937195 Vali Loss: 1.2520989 Test Loss: 0.5035370
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.0954077243804932
Epoch: 87, Steps: 59 | Train Loss: 0.1936752 Vali Loss: 1.2491392 Test Loss: 0.5034300
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.0822422504425049
Epoch: 88, Steps: 59 | Train Loss: 0.1935834 Vali Loss: 1.2455801 Test Loss: 0.5033387
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.2354736328125
Epoch: 89, Steps: 59 | Train Loss: 0.1935325 Vali Loss: 1.2437208 Test Loss: 0.5032448
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 0.7359485626220703
Epoch: 90, Steps: 59 | Train Loss: 0.1934917 Vali Loss: 1.2447624 Test Loss: 0.5031646
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.147228717803955
Epoch: 91, Steps: 59 | Train Loss: 0.1934794 Vali Loss: 1.2545884 Test Loss: 0.5030727
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.0668306350708008
Epoch: 92, Steps: 59 | Train Loss: 0.1933060 Vali Loss: 1.2454472 Test Loss: 0.5030087
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 0.880181074142456
Epoch: 93, Steps: 59 | Train Loss: 0.1932789 Vali Loss: 1.2475895 Test Loss: 0.5029230
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 0.9026186466217041
Epoch: 94, Steps: 59 | Train Loss: 0.1930694 Vali Loss: 1.2502706 Test Loss: 0.5028473
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.1270272731781006
Epoch: 95, Steps: 59 | Train Loss: 0.1930730 Vali Loss: 1.2514969 Test Loss: 0.5027877
EarlyStopping counter: 20 out of 20
Early stopping
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  35629440.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.178271770477295
Epoch: 1, Steps: 59 | Train Loss: 0.4610852 Vali Loss: 1.2026739 Test Loss: 0.4596830
Validation loss decreased (inf --> 1.202674).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.296729326248169
Epoch: 2, Steps: 59 | Train Loss: 0.4433078 Vali Loss: 1.1990408 Test Loss: 0.4444121
Validation loss decreased (1.202674 --> 1.199041).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.0270576477050781
Epoch: 3, Steps: 59 | Train Loss: 0.4372941 Vali Loss: 1.1995711 Test Loss: 0.4412888
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4107367992401123
Epoch: 4, Steps: 59 | Train Loss: 0.4355110 Vali Loss: 1.1989914 Test Loss: 0.4414425
Validation loss decreased (1.199041 --> 1.198991).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.1713941097259521
Epoch: 5, Steps: 59 | Train Loss: 0.4345407 Vali Loss: 1.2045379 Test Loss: 0.4416856
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.2649767398834229
Epoch: 6, Steps: 59 | Train Loss: 0.4339489 Vali Loss: 1.2106549 Test Loss: 0.4424681
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 0.9107890129089355
Epoch: 7, Steps: 59 | Train Loss: 0.4339014 Vali Loss: 1.2095108 Test Loss: 0.4429442
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.0737452507019043
Epoch: 8, Steps: 59 | Train Loss: 0.4333099 Vali Loss: 1.2129871 Test Loss: 0.4431213
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.089670181274414
Epoch: 9, Steps: 59 | Train Loss: 0.4329879 Vali Loss: 1.2162633 Test Loss: 0.4434235
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 0.9413063526153564
Epoch: 10, Steps: 59 | Train Loss: 0.4330336 Vali Loss: 1.2102089 Test Loss: 0.4435723
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 0.9791500568389893
Epoch: 11, Steps: 59 | Train Loss: 0.4326830 Vali Loss: 1.2116017 Test Loss: 0.4438654
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.0231060981750488
Epoch: 12, Steps: 59 | Train Loss: 0.4324439 Vali Loss: 1.2090473 Test Loss: 0.4438024
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.0255858898162842
Epoch: 13, Steps: 59 | Train Loss: 0.4321959 Vali Loss: 1.2179857 Test Loss: 0.4437892
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.0869572162628174
Epoch: 14, Steps: 59 | Train Loss: 0.4321568 Vali Loss: 1.2162615 Test Loss: 0.4441039
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1559703350067139
Epoch: 15, Steps: 59 | Train Loss: 0.4324672 Vali Loss: 1.2160832 Test Loss: 0.4441554
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 0.9368901252746582
Epoch: 16, Steps: 59 | Train Loss: 0.4319942 Vali Loss: 1.2201722 Test Loss: 0.4442128
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.0421061515808105
Epoch: 17, Steps: 59 | Train Loss: 0.4322183 Vali Loss: 1.2156428 Test Loss: 0.4442298
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.333360195159912
Epoch: 18, Steps: 59 | Train Loss: 0.4318145 Vali Loss: 1.2177382 Test Loss: 0.4445278
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 0.87884521484375
Epoch: 19, Steps: 59 | Train Loss: 0.4317479 Vali Loss: 1.2157446 Test Loss: 0.4443091
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.2539145946502686
Epoch: 20, Steps: 59 | Train Loss: 0.4317246 Vali Loss: 1.2190742 Test Loss: 0.4443824
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.1409921646118164
Epoch: 21, Steps: 59 | Train Loss: 0.4315990 Vali Loss: 1.2202656 Test Loss: 0.4444570
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.1628406047821045
Epoch: 22, Steps: 59 | Train Loss: 0.4315396 Vali Loss: 1.2181007 Test Loss: 0.4445813
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 0.9629266262054443
Epoch: 23, Steps: 59 | Train Loss: 0.4311998 Vali Loss: 1.2152680 Test Loss: 0.4445126
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 0.9412076473236084
Epoch: 24, Steps: 59 | Train Loss: 0.4314515 Vali Loss: 1.2176729 Test Loss: 0.4445235
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.44038689136505127, mae:0.440108060836792, rse:0.6317849159240723, corr:[0.25381047 0.26140183 0.26087645 0.2583295  0.25670308 0.25521344
 0.25365895 0.25265393 0.2526253  0.25343913 0.25380564 0.2532292
 0.25252    0.25208876 0.25179064 0.25146765 0.25120756 0.25101376
 0.2507423  0.25035602 0.25023192 0.25027037 0.2500776  0.24956004
 0.24931589 0.2494572  0.24948347 0.24914865 0.24869062 0.24850377
 0.24845791 0.24825886 0.24811153 0.24812515 0.24811828 0.24797021
 0.24768703 0.24757585 0.24791957 0.24842082 0.24866775 0.24846008
 0.2481614  0.24817766 0.24853206 0.24896091 0.24925794 0.24922964
 0.24871428 0.24805368 0.24728493 0.24633707 0.2451621  0.24373701
 0.24261196 0.24224536 0.24225423 0.24212946 0.2415407  0.24116729
 0.24125409 0.241615   0.24169116 0.24158847 0.24169298 0.24222568
 0.24281935 0.24277996 0.24237391 0.24206328 0.24197662 0.24186645
 0.24155013 0.2409833  0.2403842  0.24010925 0.24000402 0.23982303
 0.23942491 0.23883086 0.23817739 0.23775643 0.23761526 0.23761123
 0.23738725 0.23687343 0.23648019 0.23653196 0.23680204 0.23691924
 0.23666096 0.23637213 0.2363457  0.23655055 0.23683564 0.23735368
 0.23825952 0.2392386  0.23981303 0.24006839 0.24011703 0.24002908
 0.23989396 0.23959626 0.23910326 0.23870343 0.2384743  0.23832135
 0.2380455  0.23753127 0.23718256 0.23738714 0.23798412 0.23846889
 0.23851416 0.23831052 0.23822843 0.23834436 0.23831269 0.23795968
 0.23742038 0.23665893 0.23596685 0.23550028 0.23509538 0.23459148
 0.23419483 0.23408598 0.23396908 0.23383763 0.23353842 0.23304063
 0.23252706 0.23232347 0.23256    0.2329898  0.23328952 0.23336554
 0.23335037 0.23330098 0.23328844 0.23325334 0.23306425 0.232656
 0.23186159 0.23059797 0.22936918 0.22874233 0.2288081  0.22870223
 0.22822902 0.2280627  0.22865565 0.22956966 0.22987512 0.22953533
 0.22906415 0.22883205 0.22872119 0.22852443 0.22812285 0.22777683
 0.22739404 0.22714978 0.22731449 0.22785197 0.22811612 0.22788548
 0.22774714 0.22823839 0.22908145 0.22951712 0.2290343  0.22814097
 0.2277788  0.22821042 0.22858526 0.22848964 0.22786927 0.22717057
 0.22664663 0.22626236 0.22591995 0.22568525 0.22569774 0.2261075
 0.22681819 0.22747377 0.22771406 0.22741735 0.22687326 0.2265907
 0.22661203 0.22653522 0.22603445 0.22533192 0.22476995 0.22417109
 0.22343297 0.2228445  0.22249573 0.22223401 0.22174025 0.22146747
 0.22169794 0.22231275 0.22269796 0.22280553 0.223006   0.22320715
 0.2230232  0.22237451 0.2219502  0.22206834 0.22228411 0.22206126
 0.22162904 0.22164258 0.22193472 0.22174604 0.22092214 0.22005355
 0.22024104 0.22120592 0.2216545  0.22097524 0.21986008 0.21949258
 0.21967742 0.21941416 0.2186873  0.2180353  0.21781039 0.21803564
 0.21846813 0.21884537 0.21903257 0.21900354 0.219203   0.2196967
 0.22026117 0.22025946 0.21978638 0.21922535 0.21867204 0.2178969
 0.21712859 0.21698608 0.2175414  0.21798915 0.21761292 0.21685922
 0.21649274 0.21643864 0.21620065 0.21561879 0.21539317 0.21580657
 0.21629368 0.21616994 0.21620838 0.21708407 0.21833456 0.21861814
 0.21777312 0.21676637 0.21625187 0.21586017 0.21490738 0.21400373
 0.21412824 0.21477753 0.21487857 0.21448909 0.21397397 0.21365221
 0.21327408 0.2127386  0.2126206  0.2130905  0.21339244 0.21292374
 0.21206021 0.21153383 0.21158709 0.21129166 0.21074781 0.21110733
 0.21267012 0.21389043 0.21303205 0.2112489  0.21046141 0.21104577
 0.2112245  0.21036348 0.20985244 0.21097913 0.21239772 0.21243326
 0.21134108 0.21042126 0.21071023 0.21133922 0.21167906 0.21151672
 0.21126066 0.21071804 0.21067108 0.21092452 0.21102473 0.21065518
 0.21040405 0.21086155 0.21128677 0.21049015 0.20862874 0.20665652
 0.20578584 0.20530364 0.20448932 0.20384887 0.20374063 0.20358291
 0.20296596 0.20252898 0.20295638 0.20333531 0.20145819 0.19936949
 0.2002238  0.20235284 0.20052572 0.1960121  0.19816782 0.20571786]
