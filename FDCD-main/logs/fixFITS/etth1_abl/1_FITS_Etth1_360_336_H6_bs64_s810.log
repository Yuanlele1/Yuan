Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=810, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=106, out_features=204, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19375104.0
params:  21828.0
Trainable parameters:  21828
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.7798280715942383
Epoch: 1, Steps: 62 | Train Loss: 0.7613073 Vali Loss: 1.5528591 Test Loss: 0.6862090
Validation loss decreased (inf --> 1.552859).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.749157190322876
Epoch: 2, Steps: 62 | Train Loss: 0.5951239 Vali Loss: 1.4031315 Test Loss: 0.5832358
Validation loss decreased (1.552859 --> 1.403131).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8692059516906738
Epoch: 3, Steps: 62 | Train Loss: 0.5457447 Vali Loss: 1.3345468 Test Loss: 0.5383676
Validation loss decreased (1.403131 --> 1.334547).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.5567066669464111
Epoch: 4, Steps: 62 | Train Loss: 0.5195583 Vali Loss: 1.2895526 Test Loss: 0.5088346
Validation loss decreased (1.334547 --> 1.289553).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7043991088867188
Epoch: 5, Steps: 62 | Train Loss: 0.5022146 Vali Loss: 1.2553320 Test Loss: 0.4876765
Validation loss decreased (1.289553 --> 1.255332).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.8816471099853516
Epoch: 6, Steps: 62 | Train Loss: 0.4896581 Vali Loss: 1.2369080 Test Loss: 0.4720336
Validation loss decreased (1.255332 --> 1.236908).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5967860221862793
Epoch: 7, Steps: 62 | Train Loss: 0.4806257 Vali Loss: 1.2204111 Test Loss: 0.4605466
Validation loss decreased (1.236908 --> 1.220411).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.457423210144043
Epoch: 8, Steps: 62 | Train Loss: 0.4741015 Vali Loss: 1.2078480 Test Loss: 0.4521615
Validation loss decreased (1.220411 --> 1.207848).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.5937457084655762
Epoch: 9, Steps: 62 | Train Loss: 0.4690753 Vali Loss: 1.1996633 Test Loss: 0.4461121
Validation loss decreased (1.207848 --> 1.199663).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.014866590499878
Epoch: 10, Steps: 62 | Train Loss: 0.4653852 Vali Loss: 1.1937521 Test Loss: 0.4414554
Validation loss decreased (1.199663 --> 1.193752).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.5732355117797852
Epoch: 11, Steps: 62 | Train Loss: 0.4627257 Vali Loss: 1.1821978 Test Loss: 0.4381107
Validation loss decreased (1.193752 --> 1.182198).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.5643911361694336
Epoch: 12, Steps: 62 | Train Loss: 0.4607591 Vali Loss: 1.1800222 Test Loss: 0.4355759
Validation loss decreased (1.182198 --> 1.180022).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.221000909805298
Epoch: 13, Steps: 62 | Train Loss: 0.4590697 Vali Loss: 1.1812105 Test Loss: 0.4338250
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.71201753616333
Epoch: 14, Steps: 62 | Train Loss: 0.4578032 Vali Loss: 1.1752958 Test Loss: 0.4325360
Validation loss decreased (1.180022 --> 1.175296).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6094253063201904
Epoch: 15, Steps: 62 | Train Loss: 0.4568690 Vali Loss: 1.1765822 Test Loss: 0.4315382
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.5795361995697021
Epoch: 16, Steps: 62 | Train Loss: 0.4562626 Vali Loss: 1.1702673 Test Loss: 0.4308225
Validation loss decreased (1.175296 --> 1.170267).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.6393976211547852
Epoch: 17, Steps: 62 | Train Loss: 0.4556518 Vali Loss: 1.1713303 Test Loss: 0.4302854
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7983176708221436
Epoch: 18, Steps: 62 | Train Loss: 0.4550698 Vali Loss: 1.1682653 Test Loss: 0.4298947
Validation loss decreased (1.170267 --> 1.168265).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5564546585083008
Epoch: 19, Steps: 62 | Train Loss: 0.4547406 Vali Loss: 1.1623796 Test Loss: 0.4295743
Validation loss decreased (1.168265 --> 1.162380).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.7142913341522217
Epoch: 20, Steps: 62 | Train Loss: 0.4543947 Vali Loss: 1.1708275 Test Loss: 0.4294745
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.6335110664367676
Epoch: 21, Steps: 62 | Train Loss: 0.4542402 Vali Loss: 1.1668366 Test Loss: 0.4291949
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5665490627288818
Epoch: 22, Steps: 62 | Train Loss: 0.4539782 Vali Loss: 1.1649543 Test Loss: 0.4291055
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6386020183563232
Epoch: 23, Steps: 62 | Train Loss: 0.4537231 Vali Loss: 1.1625363 Test Loss: 0.4290919
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6006133556365967
Epoch: 24, Steps: 62 | Train Loss: 0.4537625 Vali Loss: 1.1652892 Test Loss: 0.4289861
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.6258325576782227
Epoch: 25, Steps: 62 | Train Loss: 0.4535149 Vali Loss: 1.1644801 Test Loss: 0.4289133
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5754740238189697
Epoch: 26, Steps: 62 | Train Loss: 0.4533187 Vali Loss: 1.1641552 Test Loss: 0.4288599
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5723764896392822
Epoch: 27, Steps: 62 | Train Loss: 0.4533600 Vali Loss: 1.1605858 Test Loss: 0.4287691
Validation loss decreased (1.162380 --> 1.160586).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.4961388111114502
Epoch: 28, Steps: 62 | Train Loss: 0.4531881 Vali Loss: 1.1643018 Test Loss: 0.4287923
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.3180015087127686
Epoch: 29, Steps: 62 | Train Loss: 0.4531326 Vali Loss: 1.1638103 Test Loss: 0.4287379
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.606776237487793
Epoch: 30, Steps: 62 | Train Loss: 0.4527551 Vali Loss: 1.1602678 Test Loss: 0.4287205
Validation loss decreased (1.160586 --> 1.160268).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.0116822719573975
Epoch: 31, Steps: 62 | Train Loss: 0.4528659 Vali Loss: 1.1637933 Test Loss: 0.4287071
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.7217023372650146
Epoch: 32, Steps: 62 | Train Loss: 0.4526571 Vali Loss: 1.1642421 Test Loss: 0.4287171
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.714540958404541
Epoch: 33, Steps: 62 | Train Loss: 0.4527025 Vali Loss: 1.1629716 Test Loss: 0.4286774
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5909364223480225
Epoch: 34, Steps: 62 | Train Loss: 0.4525973 Vali Loss: 1.1614043 Test Loss: 0.4286724
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.7846510410308838
Epoch: 35, Steps: 62 | Train Loss: 0.4526507 Vali Loss: 1.1571074 Test Loss: 0.4286749
Validation loss decreased (1.160268 --> 1.157107).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6567695140838623
Epoch: 36, Steps: 62 | Train Loss: 0.4524858 Vali Loss: 1.1616853 Test Loss: 0.4286752
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6065170764923096
Epoch: 37, Steps: 62 | Train Loss: 0.4525710 Vali Loss: 1.1610165 Test Loss: 0.4286597
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.6615808010101318
Epoch: 38, Steps: 62 | Train Loss: 0.4526048 Vali Loss: 1.1632342 Test Loss: 0.4286556
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.5655434131622314
Epoch: 39, Steps: 62 | Train Loss: 0.4526178 Vali Loss: 1.1596968 Test Loss: 0.4286800
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.6946196556091309
Epoch: 40, Steps: 62 | Train Loss: 0.4523694 Vali Loss: 1.1615520 Test Loss: 0.4286497
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6882545948028564
Epoch: 41, Steps: 62 | Train Loss: 0.4524550 Vali Loss: 1.1562370 Test Loss: 0.4286337
Validation loss decreased (1.157107 --> 1.156237).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.5720884799957275
Epoch: 42, Steps: 62 | Train Loss: 0.4524241 Vali Loss: 1.1576970 Test Loss: 0.4286424
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.560967206954956
Epoch: 43, Steps: 62 | Train Loss: 0.4523975 Vali Loss: 1.1570514 Test Loss: 0.4286569
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.5580673217773438
Epoch: 44, Steps: 62 | Train Loss: 0.4523454 Vali Loss: 1.1622119 Test Loss: 0.4286420
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5579144954681396
Epoch: 45, Steps: 62 | Train Loss: 0.4522996 Vali Loss: 1.1608529 Test Loss: 0.4286301
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.5771915912628174
Epoch: 46, Steps: 62 | Train Loss: 0.4523430 Vali Loss: 1.1578829 Test Loss: 0.4286594
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.9282667636871338
Epoch: 47, Steps: 62 | Train Loss: 0.4522872 Vali Loss: 1.1595606 Test Loss: 0.4286428
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.4992685317993164
Epoch: 48, Steps: 62 | Train Loss: 0.4522199 Vali Loss: 1.1610202 Test Loss: 0.4286148
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.6011381149291992
Epoch: 49, Steps: 62 | Train Loss: 0.4521331 Vali Loss: 1.1599009 Test Loss: 0.4286566
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6615681648254395
Epoch: 50, Steps: 62 | Train Loss: 0.4522591 Vali Loss: 1.1592062 Test Loss: 0.4286308
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6189303398132324
Epoch: 51, Steps: 62 | Train Loss: 0.4519570 Vali Loss: 1.1594025 Test Loss: 0.4286194
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6157939434051514
Epoch: 52, Steps: 62 | Train Loss: 0.4521191 Vali Loss: 1.1593648 Test Loss: 0.4286213
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.69282865524292
Epoch: 53, Steps: 62 | Train Loss: 0.4520812 Vali Loss: 1.1643795 Test Loss: 0.4286229
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5530574321746826
Epoch: 54, Steps: 62 | Train Loss: 0.4519182 Vali Loss: 1.1640596 Test Loss: 0.4286145
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.4840240478515625
Epoch: 55, Steps: 62 | Train Loss: 0.4520422 Vali Loss: 1.1640611 Test Loss: 0.4286384
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.6057701110839844
Epoch: 56, Steps: 62 | Train Loss: 0.4519880 Vali Loss: 1.1592209 Test Loss: 0.4286192
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.5519485473632812
Epoch: 57, Steps: 62 | Train Loss: 0.4519737 Vali Loss: 1.1629468 Test Loss: 0.4286185
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.6096372604370117
Epoch: 58, Steps: 62 | Train Loss: 0.4521536 Vali Loss: 1.1660318 Test Loss: 0.4286119
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.653364658355713
Epoch: 59, Steps: 62 | Train Loss: 0.4521167 Vali Loss: 1.1621449 Test Loss: 0.4286291
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.079465389251709
Epoch: 60, Steps: 62 | Train Loss: 0.4518164 Vali Loss: 1.1617110 Test Loss: 0.4286110
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.5287957191467285
Epoch: 61, Steps: 62 | Train Loss: 0.4521369 Vali Loss: 1.1595063 Test Loss: 0.4286222
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4274798035621643, mae:0.42715054750442505, rse:0.62245774269104, corr:[0.24977218 0.2575228  0.2568323  0.26011962 0.2569969  0.25397873
 0.25494215 0.25535417 0.25387517 0.25367048 0.25409943 0.25335997
 0.2527157  0.25270694 0.2524492  0.2521774  0.25223795 0.25197402
 0.25171715 0.2518495  0.2519763  0.2518092  0.25225577 0.252713
 0.25216913 0.25182396 0.2519495  0.2513734  0.25061288 0.2508008
 0.25099057 0.25021684 0.24962443 0.24994369 0.2500549  0.2496339
 0.2496388  0.249944   0.2499231  0.24991357 0.25021648 0.25031576
 0.25025606 0.25035328 0.25030398 0.250009   0.24998672 0.2501408
 0.2498338  0.24912044 0.24837449 0.2478095  0.24735092 0.24670807
 0.24604309 0.2454994  0.24524625 0.24508971 0.24470952 0.24447224
 0.2443755  0.24437395 0.24427354 0.2442651  0.24438794 0.24460821
 0.24495225 0.24498074 0.2448213  0.24480452 0.2447958  0.24449159
 0.24393703 0.24335112 0.24280821 0.24249315 0.24254373 0.24257061
 0.2420811  0.2414388  0.24124627 0.24108148 0.24058728 0.24020529
 0.24011941 0.24008897 0.24002138 0.24006592 0.24003208 0.23988058
 0.23974515 0.23961002 0.23932874 0.23916167 0.2393193  0.2399804
 0.24069357 0.24097826 0.24115247 0.24132898 0.24155132 0.24157867
 0.24134769 0.24124166 0.24117    0.24090572 0.24056321 0.24037464
 0.24026373 0.24026293 0.24051055 0.2408     0.24085203 0.24079823
 0.24082989 0.24068971 0.2403305  0.24001439 0.23974502 0.23967296
 0.23971239 0.23933728 0.23866457 0.23816991 0.23798823 0.23760007
 0.23720592 0.23718713 0.23705636 0.23663345 0.23633257 0.2362646
 0.23618184 0.23614958 0.2362642  0.2363133  0.23632796 0.23647128
 0.23674375 0.23675674 0.23653889 0.23640649 0.23620732 0.23611982
 0.23611303 0.23579709 0.23541303 0.2350494  0.23460658 0.23400497
 0.23378539 0.23400182 0.23411283 0.23410717 0.23412736 0.23416024
 0.23405644 0.2340828  0.23418826 0.23403503 0.23377092 0.23388109
 0.23401134 0.23381451 0.23351678 0.23323564 0.23276274 0.2326855
 0.23303182 0.23323333 0.2335457  0.2341746  0.23437732 0.23402502
 0.23403777 0.23447262 0.2344672  0.23423171 0.23434968 0.23442766
 0.23408267 0.23395745 0.23424457 0.2341998  0.234013   0.2344252
 0.2347768  0.2345966  0.23451181 0.23446698 0.23397775 0.23359182
 0.23350371 0.23311956 0.23241152 0.23211215 0.2318452  0.23112334
 0.23068586 0.23090805 0.23082417 0.2305095  0.23056585 0.23067094
 0.23040399 0.23055816 0.23100601 0.23083329 0.23055406 0.23097987
 0.23135775 0.23080951 0.23024453 0.23007564 0.22965074 0.22953351
 0.23000775 0.22994654 0.22963184 0.22956789 0.2295442  0.22913174
 0.22882827 0.22905917 0.22906587 0.22854923 0.22828692 0.22826716
 0.22779022 0.22760081 0.22807282 0.22807881 0.22777729 0.22802684
 0.22833587 0.22794405 0.22760956 0.22767878 0.22758375 0.22742143
 0.22777048 0.22780456 0.22756244 0.22764596 0.22775286 0.2275551
 0.22759122 0.22799142 0.22804134 0.22773604 0.22767591 0.22750504
 0.22703357 0.2270525  0.22753678 0.22748382 0.22741461 0.22794302
 0.22821015 0.22780073 0.22774689 0.22800836 0.22795044 0.22800104
 0.22809996 0.22762056 0.2270724  0.22708313 0.22671475 0.22589432
 0.22563604 0.22572519 0.22530787 0.22493884 0.22512953 0.22498044
 0.22453766 0.22474726 0.22506359 0.22461073 0.22431138 0.22475415
 0.22493328 0.22445315 0.22454906 0.22495435 0.22487849 0.22514787
 0.2260222  0.22625822 0.22624506 0.22654195 0.22655225 0.22619216
 0.22622082 0.22644588 0.22625408 0.22624452 0.22643335 0.22615133
 0.22575353 0.22599904 0.22622235 0.22564359 0.22589569 0.22663297
 0.2264202  0.22597331 0.22662716 0.22676034 0.22578351 0.22573833
 0.22660905 0.22659513 0.22611806 0.22624119 0.22572732 0.22476009
 0.22455586 0.22448128 0.22398801 0.22424573 0.22406536 0.22303349
 0.2232494  0.2242897  0.22339354 0.22255844 0.22422403 0.2244869
 0.22197025 0.22246478 0.22365512 0.21852843 0.21994138 0.22383085]
