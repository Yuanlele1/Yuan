Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=106, out_features=204, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19375104.0
params:  21828.0
Trainable parameters:  21828
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.3716764450073242
Epoch: 1, Steps: 62 | Train Loss: 0.7565879 Vali Loss: 1.5434778 Test Loss: 0.6804612
Validation loss decreased (inf --> 1.543478).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.2660021781921387
Epoch: 2, Steps: 62 | Train Loss: 0.5957253 Vali Loss: 1.4048530 Test Loss: 0.5834005
Validation loss decreased (1.543478 --> 1.404853).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.306734323501587
Epoch: 3, Steps: 62 | Train Loss: 0.5478099 Vali Loss: 1.3373632 Test Loss: 0.5395368
Validation loss decreased (1.404853 --> 1.337363).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3093438148498535
Epoch: 4, Steps: 62 | Train Loss: 0.5218338 Vali Loss: 1.3001084 Test Loss: 0.5103132
Validation loss decreased (1.337363 --> 1.300108).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.3297059535980225
Epoch: 5, Steps: 62 | Train Loss: 0.5041712 Vali Loss: 1.2686281 Test Loss: 0.4891769
Validation loss decreased (1.300108 --> 1.268628).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.4116308689117432
Epoch: 6, Steps: 62 | Train Loss: 0.4914117 Vali Loss: 1.2414641 Test Loss: 0.4734465
Validation loss decreased (1.268628 --> 1.241464).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.3313820362091064
Epoch: 7, Steps: 62 | Train Loss: 0.4822832 Vali Loss: 1.2205075 Test Loss: 0.4617109
Validation loss decreased (1.241464 --> 1.220508).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.3903019428253174
Epoch: 8, Steps: 62 | Train Loss: 0.4753202 Vali Loss: 1.2114041 Test Loss: 0.4532281
Validation loss decreased (1.220508 --> 1.211404).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3362958431243896
Epoch: 9, Steps: 62 | Train Loss: 0.4701485 Vali Loss: 1.2040612 Test Loss: 0.4468189
Validation loss decreased (1.211404 --> 1.204061).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.273653507232666
Epoch: 10, Steps: 62 | Train Loss: 0.4664044 Vali Loss: 1.1929889 Test Loss: 0.4420062
Validation loss decreased (1.204061 --> 1.192989).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.290236234664917
Epoch: 11, Steps: 62 | Train Loss: 0.4633928 Vali Loss: 1.1922286 Test Loss: 0.4385828
Validation loss decreased (1.192989 --> 1.192229).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.3293373584747314
Epoch: 12, Steps: 62 | Train Loss: 0.4612859 Vali Loss: 1.1784645 Test Loss: 0.4360485
Validation loss decreased (1.192229 --> 1.178465).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.251901626586914
Epoch: 13, Steps: 62 | Train Loss: 0.4595477 Vali Loss: 1.1779909 Test Loss: 0.4340645
Validation loss decreased (1.178465 --> 1.177991).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.253305435180664
Epoch: 14, Steps: 62 | Train Loss: 0.4582651 Vali Loss: 1.1737089 Test Loss: 0.4327321
Validation loss decreased (1.177991 --> 1.173709).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1765918731689453
Epoch: 15, Steps: 62 | Train Loss: 0.4572149 Vali Loss: 1.1748203 Test Loss: 0.4317226
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.1799306869506836
Epoch: 16, Steps: 62 | Train Loss: 0.4564830 Vali Loss: 1.1732857 Test Loss: 0.4309645
Validation loss decreased (1.173709 --> 1.173286).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.8530170917510986
Epoch: 17, Steps: 62 | Train Loss: 0.4557467 Vali Loss: 1.1687024 Test Loss: 0.4304066
Validation loss decreased (1.173286 --> 1.168702).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4397625923156738
Epoch: 18, Steps: 62 | Train Loss: 0.4552741 Vali Loss: 1.1686518 Test Loss: 0.4299087
Validation loss decreased (1.168702 --> 1.168652).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.298401117324829
Epoch: 19, Steps: 62 | Train Loss: 0.4550795 Vali Loss: 1.1663452 Test Loss: 0.4296492
Validation loss decreased (1.168652 --> 1.166345).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.2913169860839844
Epoch: 20, Steps: 62 | Train Loss: 0.4546584 Vali Loss: 1.1624843 Test Loss: 0.4293736
Validation loss decreased (1.166345 --> 1.162484).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.3195183277130127
Epoch: 21, Steps: 62 | Train Loss: 0.4543172 Vali Loss: 1.1690500 Test Loss: 0.4292671
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2171356678009033
Epoch: 22, Steps: 62 | Train Loss: 0.4541124 Vali Loss: 1.1673740 Test Loss: 0.4291633
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.248563289642334
Epoch: 23, Steps: 62 | Train Loss: 0.4538959 Vali Loss: 1.1640728 Test Loss: 0.4289684
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.302823543548584
Epoch: 24, Steps: 62 | Train Loss: 0.4537058 Vali Loss: 1.1632211 Test Loss: 0.4289385
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.2304532527923584
Epoch: 25, Steps: 62 | Train Loss: 0.4534776 Vali Loss: 1.1665827 Test Loss: 0.4288573
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2530062198638916
Epoch: 26, Steps: 62 | Train Loss: 0.4534257 Vali Loss: 1.1679277 Test Loss: 0.4288252
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.2844953536987305
Epoch: 27, Steps: 62 | Train Loss: 0.4534451 Vali Loss: 1.1643820 Test Loss: 0.4287823
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.279543399810791
Epoch: 28, Steps: 62 | Train Loss: 0.4532456 Vali Loss: 1.1638578 Test Loss: 0.4287473
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.2896690368652344
Epoch: 29, Steps: 62 | Train Loss: 0.4530925 Vali Loss: 1.1615926 Test Loss: 0.4286905
Validation loss decreased (1.162484 --> 1.161593).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.2559223175048828
Epoch: 30, Steps: 62 | Train Loss: 0.4530343 Vali Loss: 1.1602418 Test Loss: 0.4286978
Validation loss decreased (1.161593 --> 1.160242).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.227952480316162
Epoch: 31, Steps: 62 | Train Loss: 0.4531302 Vali Loss: 1.1604820 Test Loss: 0.4286385
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.3639857769012451
Epoch: 32, Steps: 62 | Train Loss: 0.4530247 Vali Loss: 1.1643300 Test Loss: 0.4286592
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.3042569160461426
Epoch: 33, Steps: 62 | Train Loss: 0.4528589 Vali Loss: 1.1610515 Test Loss: 0.4286567
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3382377624511719
Epoch: 34, Steps: 62 | Train Loss: 0.4528106 Vali Loss: 1.1628866 Test Loss: 0.4286389
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.3443400859832764
Epoch: 35, Steps: 62 | Train Loss: 0.4527808 Vali Loss: 1.1615947 Test Loss: 0.4286216
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3475470542907715
Epoch: 36, Steps: 62 | Train Loss: 0.4528059 Vali Loss: 1.1622299 Test Loss: 0.4286145
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.3872356414794922
Epoch: 37, Steps: 62 | Train Loss: 0.4525489 Vali Loss: 1.1636299 Test Loss: 0.4286159
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.3923234939575195
Epoch: 38, Steps: 62 | Train Loss: 0.4525949 Vali Loss: 1.1651769 Test Loss: 0.4286095
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3142390251159668
Epoch: 39, Steps: 62 | Train Loss: 0.4526596 Vali Loss: 1.1652844 Test Loss: 0.4286133
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.3660225868225098
Epoch: 40, Steps: 62 | Train Loss: 0.4525750 Vali Loss: 1.1675096 Test Loss: 0.4285834
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.3861937522888184
Epoch: 41, Steps: 62 | Train Loss: 0.4525649 Vali Loss: 1.1650622 Test Loss: 0.4285823
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.2099993228912354
Epoch: 42, Steps: 62 | Train Loss: 0.4525417 Vali Loss: 1.1620003 Test Loss: 0.4285653
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.387967824935913
Epoch: 43, Steps: 62 | Train Loss: 0.4523037 Vali Loss: 1.1621375 Test Loss: 0.4285943
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.7995588779449463
Epoch: 44, Steps: 62 | Train Loss: 0.4523170 Vali Loss: 1.1650468 Test Loss: 0.4285741
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.2598223686218262
Epoch: 45, Steps: 62 | Train Loss: 0.4522154 Vali Loss: 1.1619716 Test Loss: 0.4285616
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.3402585983276367
Epoch: 46, Steps: 62 | Train Loss: 0.4522560 Vali Loss: 1.1583045 Test Loss: 0.4285774
Validation loss decreased (1.160242 --> 1.158304).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.4110357761383057
Epoch: 47, Steps: 62 | Train Loss: 0.4523883 Vali Loss: 1.1632307 Test Loss: 0.4285947
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.3480455875396729
Epoch: 48, Steps: 62 | Train Loss: 0.4523789 Vali Loss: 1.1615793 Test Loss: 0.4285563
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.3126015663146973
Epoch: 49, Steps: 62 | Train Loss: 0.4521969 Vali Loss: 1.1611041 Test Loss: 0.4285758
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2676849365234375
Epoch: 50, Steps: 62 | Train Loss: 0.4521551 Vali Loss: 1.1620213 Test Loss: 0.4285920
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.428407907485962
Epoch: 51, Steps: 62 | Train Loss: 0.4520518 Vali Loss: 1.1582485 Test Loss: 0.4285849
Validation loss decreased (1.158304 --> 1.158249).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3495240211486816
Epoch: 52, Steps: 62 | Train Loss: 0.4520151 Vali Loss: 1.1622314 Test Loss: 0.4285688
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.37935471534729
Epoch: 53, Steps: 62 | Train Loss: 0.4521347 Vali Loss: 1.1607848 Test Loss: 0.4285725
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.270009994506836
Epoch: 54, Steps: 62 | Train Loss: 0.4520860 Vali Loss: 1.1589931 Test Loss: 0.4285619
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.2673933506011963
Epoch: 55, Steps: 62 | Train Loss: 0.4520026 Vali Loss: 1.1610051 Test Loss: 0.4285767
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.3976531028747559
Epoch: 56, Steps: 62 | Train Loss: 0.4521288 Vali Loss: 1.1604389 Test Loss: 0.4285676
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.3184781074523926
Epoch: 57, Steps: 62 | Train Loss: 0.4521703 Vali Loss: 1.1615775 Test Loss: 0.4285735
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.2926700115203857
Epoch: 58, Steps: 62 | Train Loss: 0.4519726 Vali Loss: 1.1592164 Test Loss: 0.4285792
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.3360297679901123
Epoch: 59, Steps: 62 | Train Loss: 0.4520672 Vali Loss: 1.1546093 Test Loss: 0.4285733
Validation loss decreased (1.158249 --> 1.154609).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.381556749343872
Epoch: 60, Steps: 62 | Train Loss: 0.4520003 Vali Loss: 1.1582453 Test Loss: 0.4285781
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.3262584209442139
Epoch: 61, Steps: 62 | Train Loss: 0.4518654 Vali Loss: 1.1581149 Test Loss: 0.4285764
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.343811273574829
Epoch: 62, Steps: 62 | Train Loss: 0.4521000 Vali Loss: 1.1638569 Test Loss: 0.4285680
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.2964482307434082
Epoch: 63, Steps: 62 | Train Loss: 0.4520971 Vali Loss: 1.1597452 Test Loss: 0.4285849
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.3028144836425781
Epoch: 64, Steps: 62 | Train Loss: 0.4520404 Vali Loss: 1.1618651 Test Loss: 0.4285792
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.305401086807251
Epoch: 65, Steps: 62 | Train Loss: 0.4519056 Vali Loss: 1.1673859 Test Loss: 0.4285611
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.3198966979980469
Epoch: 66, Steps: 62 | Train Loss: 0.4519012 Vali Loss: 1.1616056 Test Loss: 0.4285706
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.2200820446014404
Epoch: 67, Steps: 62 | Train Loss: 0.4518355 Vali Loss: 1.1580728 Test Loss: 0.4285701
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.324230432510376
Epoch: 68, Steps: 62 | Train Loss: 0.4520553 Vali Loss: 1.1608144 Test Loss: 0.4285730
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3669755458831787
Epoch: 69, Steps: 62 | Train Loss: 0.4519618 Vali Loss: 1.1600999 Test Loss: 0.4285726
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.1399741172790527
Epoch: 70, Steps: 62 | Train Loss: 0.4520294 Vali Loss: 1.1607926 Test Loss: 0.4285740
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.3532359600067139
Epoch: 71, Steps: 62 | Train Loss: 0.4519817 Vali Loss: 1.1626673 Test Loss: 0.4285806
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.2244548797607422
Epoch: 72, Steps: 62 | Train Loss: 0.4518680 Vali Loss: 1.1587343 Test Loss: 0.4285709
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.3084626197814941
Epoch: 73, Steps: 62 | Train Loss: 0.4520077 Vali Loss: 1.1615638 Test Loss: 0.4285664
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.2688970565795898
Epoch: 74, Steps: 62 | Train Loss: 0.4518685 Vali Loss: 1.1582259 Test Loss: 0.4285721
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.4115500450134277
Epoch: 75, Steps: 62 | Train Loss: 0.4519910 Vali Loss: 1.1625751 Test Loss: 0.4285699
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.2303354740142822
Epoch: 76, Steps: 62 | Train Loss: 0.4519568 Vali Loss: 1.1555929 Test Loss: 0.4285679
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.2800500392913818
Epoch: 77, Steps: 62 | Train Loss: 0.4519600 Vali Loss: 1.1647024 Test Loss: 0.4285689
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.3072795867919922
Epoch: 78, Steps: 62 | Train Loss: 0.4519442 Vali Loss: 1.1624403 Test Loss: 0.4285631
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.2641804218292236
Epoch: 79, Steps: 62 | Train Loss: 0.4518583 Vali Loss: 1.1592394 Test Loss: 0.4285682
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.427424818277359, mae:0.4270228147506714, rse:0.6224176287651062, corr:[0.24928461 0.2577509  0.25712618 0.26024786 0.25752744 0.2544911
 0.254684   0.25479665 0.25355414 0.25336045 0.25366908 0.2530012
 0.25240013 0.25231862 0.25211683 0.25196934 0.25194287 0.25166973
 0.25169024 0.2520068  0.25200757 0.25185442 0.25256252 0.2531315
 0.25239712 0.25184894 0.2519948  0.2515441  0.25076005 0.25069687
 0.250688   0.24997106 0.24946092 0.24969988 0.24972627 0.24935009
 0.24945034 0.24980275 0.24980028 0.24978408 0.25010136 0.25029913
 0.25035572 0.25040755 0.2502435  0.25003254 0.25022393 0.2503744
 0.24989049 0.24915674 0.2485202  0.2479024  0.24720444 0.24643186
 0.24584655 0.24537233 0.24510296 0.24501796 0.24473865 0.24450524
 0.24437891 0.24443124 0.24434793 0.2442737  0.24443224 0.24480386
 0.24517965 0.24511401 0.24495463 0.2450531  0.24508041 0.24468562
 0.24407166 0.24349965 0.2429472  0.2425326  0.2424274  0.2423594
 0.24192777 0.24139047 0.24120492 0.24102539 0.24062835 0.24037807
 0.24031469 0.24021037 0.24008182 0.24009307 0.240047   0.23990041
 0.23977487 0.23965992 0.23941617 0.23927097 0.23939954 0.24001226
 0.24073495 0.24108021 0.24125293 0.24128048 0.24133253 0.24133787
 0.2411839  0.24112485 0.24110204 0.2409407  0.24062908 0.2404162
 0.24036638 0.24039844 0.24047343 0.2406229  0.24081649 0.24095397
 0.24095224 0.24071264 0.2403526  0.24003296 0.23972699 0.23965833
 0.2397168  0.23932561 0.23863265 0.23806643 0.23775518 0.23733032
 0.23703752 0.23709333 0.23695469 0.23655988 0.23633298 0.23632638
 0.23630401 0.23629455 0.2363441  0.2363818  0.23654492 0.2367758
 0.236933   0.23685926 0.2367258  0.23665279 0.23634653 0.23612364
 0.2361191  0.23585026 0.23544492 0.23498243 0.23444095 0.23384057
 0.2336764  0.23387602 0.23394442 0.23398478 0.23408672 0.23417906
 0.23414306 0.234199   0.2342362  0.23406331 0.23389798 0.23400533
 0.2339593  0.23368846 0.23354125 0.23336393 0.2328005  0.23263639
 0.23302981 0.23328352 0.23357844 0.234155   0.23434125 0.23402691
 0.23405059 0.23444538 0.23444471 0.23428354 0.23442529 0.23449731
 0.23420392 0.23408645 0.2342666  0.23418276 0.23411062 0.23457026
 0.23480853 0.23456393 0.23455651 0.23455209 0.2340073  0.23362502
 0.23364541 0.23333947 0.23256686 0.23209217 0.23168391 0.23094009
 0.23056209 0.23088762 0.23091966 0.23063378 0.23061323 0.23068464
 0.2304685  0.23062971 0.23107415 0.2310303  0.2308368  0.23106714
 0.23123823 0.2307875  0.23040755 0.23019536 0.2296476  0.22958578
 0.23013975 0.22998111 0.22953098 0.2294524  0.22949663 0.22914632
 0.22884576 0.22910778 0.22926685 0.22886083 0.22852695 0.22845004
 0.2280625  0.22789094 0.22822979 0.22820999 0.2279894  0.22815338
 0.22829069 0.22788315 0.2275865  0.22758636 0.22745912 0.2273451
 0.22762007 0.22749059 0.22728007 0.22754037 0.22769786 0.22743209
 0.2274585  0.22797625 0.22815618 0.2278514  0.22770874 0.22756456
 0.2271718  0.22713672 0.22753    0.2275598  0.22756231 0.22793253
 0.22807954 0.22780475 0.22785994 0.2279981  0.22782104 0.22793515
 0.22809185 0.22754243 0.22692218 0.22692505 0.22661032 0.22588211
 0.22572576 0.2259201  0.22558889 0.22520298 0.2253575  0.22527896
 0.22482254 0.22481288 0.22506624 0.22482517 0.22455749 0.22472504
 0.22481613 0.22451085 0.22463425 0.22490434 0.224844   0.2251773
 0.22592404 0.22604917 0.22607663 0.22639391 0.22638366 0.22607827
 0.22623351 0.22660868 0.22649533 0.22638085 0.22644643 0.22620194
 0.22578748 0.22589494 0.22623037 0.22589596 0.22601944 0.22647266
 0.22634052 0.22601949 0.22644694 0.22649583 0.22588032 0.22599386
 0.22655542 0.22642043 0.22621396 0.22647177 0.22581756 0.22472504
 0.22457254 0.22457017 0.22400512 0.22414109 0.22407389 0.2231027
 0.22291549 0.22365133 0.22306935 0.22231133 0.22345592 0.22392416
 0.22229075 0.22262996 0.22362539 0.22026597 0.22138241 0.22416759]
