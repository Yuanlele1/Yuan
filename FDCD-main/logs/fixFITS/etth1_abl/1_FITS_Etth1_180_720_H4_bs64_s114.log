Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=210, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7902720.0
params:  9030.0
Trainable parameters:  9030
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.349555015563965
Epoch: 1, Steps: 60 | Train Loss: 1.1904696 Vali Loss: 2.3055809 Test Loss: 1.0356913
Validation loss decreased (inf --> 2.305581).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9214603900909424
Epoch: 2, Steps: 60 | Train Loss: 0.8875961 Vali Loss: 1.9344954 Test Loss: 0.7584870
Validation loss decreased (2.305581 --> 1.934495).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.1097161769866943
Epoch: 3, Steps: 60 | Train Loss: 0.7568832 Vali Loss: 1.7629123 Test Loss: 0.6274790
Validation loss decreased (1.934495 --> 1.762912).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.860121726989746
Epoch: 4, Steps: 60 | Train Loss: 0.6913249 Vali Loss: 1.6661474 Test Loss: 0.5562192
Validation loss decreased (1.762912 --> 1.666147).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.3659377098083496
Epoch: 5, Steps: 60 | Train Loss: 0.6548780 Vali Loss: 1.6168230 Test Loss: 0.5153553
Validation loss decreased (1.666147 --> 1.616823).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.073979377746582
Epoch: 6, Steps: 60 | Train Loss: 0.6340185 Vali Loss: 1.5924177 Test Loss: 0.4911947
Validation loss decreased (1.616823 --> 1.592418).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.993232011795044
Epoch: 7, Steps: 60 | Train Loss: 0.6220279 Vali Loss: 1.5667044 Test Loss: 0.4762489
Validation loss decreased (1.592418 --> 1.566704).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.59834885597229
Epoch: 8, Steps: 60 | Train Loss: 0.6135603 Vali Loss: 1.5467247 Test Loss: 0.4664216
Validation loss decreased (1.566704 --> 1.546725).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.5506627559661865
Epoch: 9, Steps: 60 | Train Loss: 0.6079815 Vali Loss: 1.5427704 Test Loss: 0.4598793
Validation loss decreased (1.546725 --> 1.542770).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.1716933250427246
Epoch: 10, Steps: 60 | Train Loss: 0.6044667 Vali Loss: 1.5357622 Test Loss: 0.4552237
Validation loss decreased (1.542770 --> 1.535762).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.814955711364746
Epoch: 11, Steps: 60 | Train Loss: 0.6012515 Vali Loss: 1.5247192 Test Loss: 0.4517981
Validation loss decreased (1.535762 --> 1.524719).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.826874256134033
Epoch: 12, Steps: 60 | Train Loss: 0.5987014 Vali Loss: 1.5264363 Test Loss: 0.4492531
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.9856650829315186
Epoch: 13, Steps: 60 | Train Loss: 0.5970997 Vali Loss: 1.5209920 Test Loss: 0.4472765
Validation loss decreased (1.524719 --> 1.520992).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.1768720149993896
Epoch: 14, Steps: 60 | Train Loss: 0.5963417 Vali Loss: 1.5109301 Test Loss: 0.4457734
Validation loss decreased (1.520992 --> 1.510930).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.0837931632995605
Epoch: 15, Steps: 60 | Train Loss: 0.5952693 Vali Loss: 1.5151715 Test Loss: 0.4445183
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.078544855117798
Epoch: 16, Steps: 60 | Train Loss: 0.5935207 Vali Loss: 1.5230664 Test Loss: 0.4435743
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.8817901611328125
Epoch: 17, Steps: 60 | Train Loss: 0.5924887 Vali Loss: 1.5146677 Test Loss: 0.4427739
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.486851453781128
Epoch: 18, Steps: 60 | Train Loss: 0.5920867 Vali Loss: 1.5156188 Test Loss: 0.4421392
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.9760522842407227
Epoch: 19, Steps: 60 | Train Loss: 0.5916969 Vali Loss: 1.5088048 Test Loss: 0.4416937
Validation loss decreased (1.510930 --> 1.508805).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.1156363487243652
Epoch: 20, Steps: 60 | Train Loss: 0.5910887 Vali Loss: 1.5169020 Test Loss: 0.4412234
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.584699869155884
Epoch: 21, Steps: 60 | Train Loss: 0.5905379 Vali Loss: 1.5066741 Test Loss: 0.4409154
Validation loss decreased (1.508805 --> 1.506674).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.5661323070526123
Epoch: 22, Steps: 60 | Train Loss: 0.5904378 Vali Loss: 1.5108664 Test Loss: 0.4406697
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.8688509464263916
Epoch: 23, Steps: 60 | Train Loss: 0.5898659 Vali Loss: 1.5046570 Test Loss: 0.4404902
Validation loss decreased (1.506674 --> 1.504657).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.395240068435669
Epoch: 24, Steps: 60 | Train Loss: 0.5895715 Vali Loss: 1.5063003 Test Loss: 0.4402711
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8430821895599365
Epoch: 25, Steps: 60 | Train Loss: 0.5895470 Vali Loss: 1.5163484 Test Loss: 0.4401757
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.5147736072540283
Epoch: 26, Steps: 60 | Train Loss: 0.5889111 Vali Loss: 1.5025541 Test Loss: 0.4400569
Validation loss decreased (1.504657 --> 1.502554).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.8361928462982178
Epoch: 27, Steps: 60 | Train Loss: 0.5890285 Vali Loss: 1.4986521 Test Loss: 0.4399707
Validation loss decreased (1.502554 --> 1.498652).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.5816092491149902
Epoch: 28, Steps: 60 | Train Loss: 0.5889807 Vali Loss: 1.5048735 Test Loss: 0.4399385
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.4714062213897705
Epoch: 29, Steps: 60 | Train Loss: 0.5883894 Vali Loss: 1.4938959 Test Loss: 0.4398757
Validation loss decreased (1.498652 --> 1.493896).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.5556511878967285
Epoch: 30, Steps: 60 | Train Loss: 0.5880056 Vali Loss: 1.5063152 Test Loss: 0.4398628
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.677471399307251
Epoch: 31, Steps: 60 | Train Loss: 0.5880094 Vali Loss: 1.5031791 Test Loss: 0.4398513
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.9741697311401367
Epoch: 32, Steps: 60 | Train Loss: 0.5881997 Vali Loss: 1.5045043 Test Loss: 0.4398235
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.2328643798828125
Epoch: 33, Steps: 60 | Train Loss: 0.5881687 Vali Loss: 1.5070367 Test Loss: 0.4398334
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9159014225006104
Epoch: 34, Steps: 60 | Train Loss: 0.5875608 Vali Loss: 1.5050548 Test Loss: 0.4398494
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.0221071243286133
Epoch: 35, Steps: 60 | Train Loss: 0.5876413 Vali Loss: 1.4995011 Test Loss: 0.4398558
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.7617194652557373
Epoch: 36, Steps: 60 | Train Loss: 0.5876272 Vali Loss: 1.5039594 Test Loss: 0.4398662
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9004321098327637
Epoch: 37, Steps: 60 | Train Loss: 0.5871771 Vali Loss: 1.5035195 Test Loss: 0.4398738
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.615607738494873
Epoch: 38, Steps: 60 | Train Loss: 0.5876346 Vali Loss: 1.5008385 Test Loss: 0.4398791
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.543712615966797
Epoch: 39, Steps: 60 | Train Loss: 0.5870590 Vali Loss: 1.5094159 Test Loss: 0.4399024
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.540937662124634
Epoch: 40, Steps: 60 | Train Loss: 0.5874710 Vali Loss: 1.5005493 Test Loss: 0.4399148
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.0911197662353516
Epoch: 41, Steps: 60 | Train Loss: 0.5870589 Vali Loss: 1.5009925 Test Loss: 0.4399440
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.7696943283081055
Epoch: 42, Steps: 60 | Train Loss: 0.5869278 Vali Loss: 1.5048141 Test Loss: 0.4399631
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0981245040893555
Epoch: 43, Steps: 60 | Train Loss: 0.5870251 Vali Loss: 1.4997811 Test Loss: 0.4399855
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.3324036598205566
Epoch: 44, Steps: 60 | Train Loss: 0.5868475 Vali Loss: 1.4963635 Test Loss: 0.4400039
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.0394935607910156
Epoch: 45, Steps: 60 | Train Loss: 0.5869458 Vali Loss: 1.4987726 Test Loss: 0.4400209
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.8116719722747803
Epoch: 46, Steps: 60 | Train Loss: 0.5869280 Vali Loss: 1.4944174 Test Loss: 0.4400547
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.0621707439422607
Epoch: 47, Steps: 60 | Train Loss: 0.5869395 Vali Loss: 1.4977111 Test Loss: 0.4400623
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.5200374126434326
Epoch: 48, Steps: 60 | Train Loss: 0.5871488 Vali Loss: 1.5002911 Test Loss: 0.4401060
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.040092945098877
Epoch: 49, Steps: 60 | Train Loss: 0.5873854 Vali Loss: 1.4995821 Test Loss: 0.4401183
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4386919140815735, mae:0.4518507719039917, rse:0.6340620517730713, corr:[0.22367375 0.23307359 0.23168741 0.23016025 0.23047794 0.23017886
 0.22917217 0.22967222 0.23130174 0.23216724 0.23117383 0.23022822
 0.23031116 0.22958235 0.2284587  0.22865658 0.22983043 0.23020455
 0.22933613 0.22880414 0.22944547 0.23038302 0.23053211 0.22992389
 0.22921747 0.22871958 0.22818872 0.22777753 0.227553   0.22763239
 0.22756658 0.22707051 0.22654653 0.22659138 0.22677068 0.2267525
 0.22683808 0.22699639 0.22688751 0.2263043  0.22610326 0.2266897
 0.22725244 0.22712149 0.22690123 0.22756256 0.22855435 0.22881079
 0.22791062 0.22693546 0.22572213 0.2243545  0.22299264 0.22187479
 0.22147526 0.22166626 0.22175765 0.22175445 0.22147444 0.22170177
 0.22197819 0.22180337 0.22178544 0.22206174 0.22218038 0.22204188
 0.22188687 0.22184539 0.22183163 0.221731   0.22131354 0.22055832
 0.21943384 0.21857958 0.21785975 0.21766026 0.21781367 0.21759927
 0.21714084 0.21685348 0.21693498 0.21727641 0.21697679 0.21655872
 0.2165109  0.21649997 0.21641633 0.21620554 0.21600229 0.21578524
 0.21536382 0.21508169 0.21531709 0.21622317 0.21708116 0.2173778
 0.21706851 0.21695447 0.217058   0.21693286 0.21668462 0.21643047
 0.21630733 0.21670696 0.21683832 0.21678232 0.21652539 0.21663293
 0.21688198 0.21679854 0.21655518 0.21617715 0.21570241 0.21536753
 0.21537273 0.21549241 0.21554327 0.21552537 0.21531963 0.21502298
 0.21449396 0.213939   0.21291673 0.21199192 0.21134996 0.21096134
 0.21082321 0.21110566 0.21133855 0.21162555 0.2117555  0.21218322
 0.21277252 0.21285571 0.21270631 0.21250759 0.21235088 0.21223232
 0.21207532 0.21198954 0.21211427 0.21244602 0.21262532 0.21255781
 0.21212925 0.21149969 0.21065551 0.20956156 0.20884702 0.2085173
 0.20842446 0.20841266 0.2086462  0.20906721 0.20922649 0.20943506
 0.20972136 0.2096758  0.20951477 0.20937833 0.2093046  0.20938207
 0.20943023 0.2094525  0.209779   0.21035998 0.21053883 0.21037443
 0.21000293 0.20996706 0.20980962 0.20941557 0.20904046 0.20877844
 0.208851   0.20940965 0.20986734 0.2103657  0.21070321 0.21116953
 0.21168554 0.21183322 0.21184976 0.21171366 0.21150325 0.21133146
 0.21130817 0.21148998 0.21185642 0.2121821  0.21221708 0.21178558
 0.21088056 0.21013853 0.20933217 0.20852692 0.20790327 0.2074367
 0.20716505 0.20725039 0.20753399 0.20787643 0.20786732 0.20821825
 0.20880982 0.20908187 0.20895906 0.20858032 0.2082613  0.20800403
 0.20768374 0.20740172 0.20745686 0.20780651 0.20787    0.20761678
 0.20720975 0.20712127 0.20688589 0.20627888 0.20568281 0.2051813
 0.20521127 0.20553045 0.20566852 0.20562638 0.20546477 0.2056535
 0.20607881 0.20619798 0.20616442 0.20595138 0.2055383  0.20518494
 0.20500734 0.20485905 0.20470566 0.20459619 0.20451283 0.20440072
 0.20405859 0.20373407 0.20327877 0.20297159 0.20292531 0.2028297
 0.20279033 0.20286173 0.20313923 0.20365228 0.2040387  0.20464002
 0.20532995 0.2056297  0.20566615 0.20543428 0.20518099 0.20492567
 0.20462564 0.20423563 0.20400737 0.20406468 0.20400599 0.20368926
 0.20312922 0.20276992 0.20234899 0.20189449 0.20156673 0.2012285
 0.2010731  0.20126836 0.20134704 0.20150834 0.20158006 0.20171598
 0.20215806 0.20233148 0.20216782 0.20171508 0.20137464 0.201409
 0.20150651 0.20155881 0.20178743 0.20240282 0.20292386 0.20315427
 0.2030812  0.20314577 0.20329751 0.2034048  0.20345975 0.203602
 0.2039171  0.2043761  0.20481248 0.20520382 0.20531829 0.20556675
 0.20607883 0.20645724 0.20675792 0.20681936 0.20652223 0.20622927
 0.20616437 0.2060852  0.20587698 0.20596908 0.20600457 0.20595019
 0.20571603 0.20560913 0.20540383 0.20506778 0.20493944 0.20456949
 0.20429216 0.20429158 0.20430285 0.2044348  0.20455278 0.20499837
 0.20573232 0.2060667  0.20612675 0.20603956 0.20574056 0.20539035
 0.20515078 0.20517819 0.20567285 0.20629978 0.20643856 0.20609467
 0.20560807 0.20553009 0.20530425 0.20474552 0.20407604 0.20368364
 0.20375395 0.20387667 0.20388879 0.20411801 0.20463958 0.20530722
 0.20573652 0.2059331  0.20602983 0.2059062  0.20552364 0.2052327
 0.20510302 0.20506826 0.20504151 0.20514314 0.20482543 0.20441587
 0.20393297 0.20361453 0.20323852 0.20278944 0.20244578 0.20218702
 0.20211346 0.20211737 0.20212214 0.20207967 0.20200078 0.20211092
 0.20232198 0.20217834 0.20199493 0.20181833 0.20164919 0.2015356
 0.20160478 0.201692   0.20187762 0.20229158 0.20262086 0.20327783
 0.20400503 0.20468259 0.20477392 0.20439257 0.20402005 0.20349492
 0.2031062  0.20293722 0.20277154 0.2028388  0.20277238 0.20316082
 0.2040056  0.20448327 0.20452039 0.20404264 0.20364976 0.20373026
 0.20407882 0.20412196 0.20406787 0.20433953 0.2046968  0.20492792
 0.20482412 0.20482233 0.20478912 0.20444468 0.20411147 0.20376931
 0.2038286  0.20432119 0.2047819  0.20496765 0.20511983 0.20555401
 0.20597492 0.20588244 0.20559171 0.2054129  0.20534243 0.20541924
 0.2057084  0.20606719 0.20625597 0.20677507 0.20731649 0.2078709
 0.20803976 0.20802413 0.2076557  0.2072272  0.20738524 0.20762521
 0.20766966 0.20776637 0.20797329 0.20839855 0.20852785 0.20880564
 0.20922735 0.20921808 0.20881575 0.20842102 0.20842665 0.20873901
 0.20889814 0.20872675 0.20869096 0.20925754 0.20989503 0.21032384
 0.2103651  0.21057123 0.21042758 0.2097408  0.20902812 0.20844613
 0.20829912 0.20847753 0.20865051 0.20880792 0.20907532 0.209735
 0.21047467 0.21068166 0.21060374 0.21043982 0.21030623 0.21038072
 0.2105016  0.2104907  0.21037357 0.2102414  0.21029608 0.21035112
 0.2100859  0.20968612 0.20896104 0.20828564 0.20815887 0.20810714
 0.20813857 0.20827791 0.20858628 0.20910543 0.20934261 0.20960341
 0.20995326 0.2098586  0.20968999 0.20959379 0.20968904 0.20978361
 0.20981374 0.20991246 0.21037647 0.21115817 0.21185128 0.21223861
 0.21230944 0.21260194 0.2126978  0.21239226 0.21206273 0.21167219
 0.21143185 0.21125044 0.2112111  0.21116397 0.21119465 0.21163991
 0.21223415 0.21250626 0.2126709  0.21270917 0.21271107 0.21288663
 0.21314603 0.21336535 0.21366784 0.21426967 0.21493578 0.21513933
 0.21457903 0.21415104 0.21376966 0.21328565 0.2126959  0.21203645
 0.2116448  0.21174207 0.21205942 0.21244432 0.21251695 0.21267931
 0.21309228 0.2131483  0.21322155 0.21330959 0.21328531 0.21321394
 0.21311094 0.21306117 0.21305543 0.21300378 0.21281849 0.21230753
 0.21137533 0.21090345 0.21052785 0.20977277 0.20892678 0.20835598
 0.20807664 0.2078985  0.20816594 0.20865233 0.208753   0.20889169
 0.20940582 0.2093545  0.20912188 0.20885488 0.20871395 0.20871292
 0.208644   0.20832995 0.20815748 0.20827131 0.20846766 0.20827581
 0.20740816 0.20675872 0.20613296 0.20544565 0.20493318 0.20419097
 0.20373408 0.20357461 0.20370457 0.20381562 0.20345934 0.20333529
 0.20340545 0.20295785 0.20233326 0.20187896 0.20167284 0.20158933
 0.20148394 0.20129374 0.2011215  0.20102401 0.20074792 0.20020303
 0.19932307 0.1987401  0.19802776 0.19689962 0.19606675 0.19569078
 0.1953431  0.19495273 0.19474445 0.19493972 0.1951658  0.19524875
 0.19540212 0.1951592  0.19498663 0.19490415 0.19468634 0.19444717
 0.19426197 0.1940309  0.19382769 0.19393495 0.19413036 0.19379891
 0.19268571 0.1918301  0.19128299 0.19038822 0.18945721 0.18878774
 0.18886928 0.18905862 0.1888801  0.18879871 0.18879196 0.18935125
 0.19012205 0.18979862 0.18913487 0.18891948 0.18887053 0.18871708
 0.18845001 0.18848369 0.1888479  0.18910374 0.18902645 0.18873037
 0.18806386 0.18722479 0.185828   0.18460622 0.18432122 0.1843384
 0.18407501 0.1835792  0.18363424 0.18415037 0.18441272 0.18456903
 0.18497516 0.18499511 0.18476592 0.18439528 0.18422273 0.18437383
 0.18454586 0.18436112 0.18407607 0.18428423 0.18455307 0.1839159
 0.18208921 0.1807523  0.17997183 0.1786585  0.17725846 0.17620625
 0.17606007 0.17617016 0.1763676  0.17660485 0.17707445 0.1781319
 0.17914896 0.17889492 0.17858095 0.17888267 0.17908494 0.17864747
 0.17801219 0.17802988 0.17856082 0.17891642 0.1788808  0.1785278
 0.17790844 0.17754067 0.17670436 0.17558128 0.17502995 0.17490599
 0.17469092 0.17436288 0.17445809 0.17465349 0.17430134 0.17392725
 0.17410716 0.1737069  0.1733334  0.17303617 0.1726101  0.17232469
 0.1725287  0.17244172 0.17162561 0.17138304 0.17254284 0.16568942]
