Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=26, out_features=74, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1723904.0
params:  1998.0
Trainable parameters:  1998
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.2594943046569824
Epoch: 1, Steps: 63 | Train Loss: 0.8790056 Vali Loss: 2.1351068 Test Loss: 1.0808548
Validation loss decreased (inf --> 2.135107).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.2814207077026367
Epoch: 2, Steps: 63 | Train Loss: 0.6997279 Vali Loss: 1.8577057 Test Loss: 0.8936943
Validation loss decreased (2.135107 --> 1.857706).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.304471492767334
Epoch: 3, Steps: 63 | Train Loss: 0.5944175 Vali Loss: 1.6934698 Test Loss: 0.7823930
Validation loss decreased (1.857706 --> 1.693470).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.199336051940918
Epoch: 4, Steps: 63 | Train Loss: 0.5281497 Vali Loss: 1.5948908 Test Loss: 0.7109464
Validation loss decreased (1.693470 --> 1.594891).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.1661927700042725
Epoch: 5, Steps: 63 | Train Loss: 0.4838241 Vali Loss: 1.5204104 Test Loss: 0.6615008
Validation loss decreased (1.594891 --> 1.520410).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.094031572341919
Epoch: 6, Steps: 63 | Train Loss: 0.4532881 Vali Loss: 1.4736762 Test Loss: 0.6263131
Validation loss decreased (1.520410 --> 1.473676).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.29996657371521
Epoch: 7, Steps: 63 | Train Loss: 0.4307273 Vali Loss: 1.4313798 Test Loss: 0.6003032
Validation loss decreased (1.473676 --> 1.431380).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.1552608013153076
Epoch: 8, Steps: 63 | Train Loss: 0.4143833 Vali Loss: 1.4080212 Test Loss: 0.5803240
Validation loss decreased (1.431380 --> 1.408021).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.1888399124145508
Epoch: 9, Steps: 63 | Train Loss: 0.4016456 Vali Loss: 1.3780308 Test Loss: 0.5652570
Validation loss decreased (1.408021 --> 1.378031).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.1795079708099365
Epoch: 10, Steps: 63 | Train Loss: 0.3919893 Vali Loss: 1.3621632 Test Loss: 0.5533153
Validation loss decreased (1.378031 --> 1.362163).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.0890424251556396
Epoch: 11, Steps: 63 | Train Loss: 0.3840391 Vali Loss: 1.3525147 Test Loss: 0.5436343
Validation loss decreased (1.362163 --> 1.352515).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.076441764831543
Epoch: 12, Steps: 63 | Train Loss: 0.3780390 Vali Loss: 1.3332913 Test Loss: 0.5358707
Validation loss decreased (1.352515 --> 1.333291).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.2147836685180664
Epoch: 13, Steps: 63 | Train Loss: 0.3729744 Vali Loss: 1.3292875 Test Loss: 0.5293589
Validation loss decreased (1.333291 --> 1.329288).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.2360198497772217
Epoch: 14, Steps: 63 | Train Loss: 0.3689843 Vali Loss: 1.3229991 Test Loss: 0.5237109
Validation loss decreased (1.329288 --> 1.322999).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1986074447631836
Epoch: 15, Steps: 63 | Train Loss: 0.3655926 Vali Loss: 1.3120685 Test Loss: 0.5191568
Validation loss decreased (1.322999 --> 1.312068).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.1561949253082275
Epoch: 16, Steps: 63 | Train Loss: 0.3625882 Vali Loss: 1.2997487 Test Loss: 0.5152534
Validation loss decreased (1.312068 --> 1.299749).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.2345857620239258
Epoch: 17, Steps: 63 | Train Loss: 0.3601394 Vali Loss: 1.3017359 Test Loss: 0.5118622
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.2884798049926758
Epoch: 18, Steps: 63 | Train Loss: 0.3579306 Vali Loss: 1.2983786 Test Loss: 0.5087062
Validation loss decreased (1.299749 --> 1.298379).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.2462005615234375
Epoch: 19, Steps: 63 | Train Loss: 0.3559594 Vali Loss: 1.2945203 Test Loss: 0.5061097
Validation loss decreased (1.298379 --> 1.294520).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.1996808052062988
Epoch: 20, Steps: 63 | Train Loss: 0.3542081 Vali Loss: 1.2856359 Test Loss: 0.5037391
Validation loss decreased (1.294520 --> 1.285636).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.068373680114746
Epoch: 21, Steps: 63 | Train Loss: 0.3529505 Vali Loss: 1.2921121 Test Loss: 0.5016354
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.092991590499878
Epoch: 22, Steps: 63 | Train Loss: 0.3516375 Vali Loss: 1.2836057 Test Loss: 0.4997898
Validation loss decreased (1.285636 --> 1.283606).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.120079755783081
Epoch: 23, Steps: 63 | Train Loss: 0.3504550 Vali Loss: 1.2895375 Test Loss: 0.4981255
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.1704933643341064
Epoch: 24, Steps: 63 | Train Loss: 0.3492773 Vali Loss: 1.2860870 Test Loss: 0.4966417
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.1860084533691406
Epoch: 25, Steps: 63 | Train Loss: 0.3483539 Vali Loss: 1.2837744 Test Loss: 0.4951873
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.1988611221313477
Epoch: 26, Steps: 63 | Train Loss: 0.3474069 Vali Loss: 1.2784898 Test Loss: 0.4939196
Validation loss decreased (1.283606 --> 1.278490).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.2430999279022217
Epoch: 27, Steps: 63 | Train Loss: 0.3469817 Vali Loss: 1.2785567 Test Loss: 0.4927817
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.147123098373413
Epoch: 28, Steps: 63 | Train Loss: 0.3458782 Vali Loss: 1.2729914 Test Loss: 0.4916815
Validation loss decreased (1.278490 --> 1.272991).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.169612169265747
Epoch: 29, Steps: 63 | Train Loss: 0.3451703 Vali Loss: 1.2758876 Test Loss: 0.4907005
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.0594847202301025
Epoch: 30, Steps: 63 | Train Loss: 0.3445063 Vali Loss: 1.2807717 Test Loss: 0.4898541
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.1153564453125
Epoch: 31, Steps: 63 | Train Loss: 0.3439277 Vali Loss: 1.2738692 Test Loss: 0.4890036
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.1130402088165283
Epoch: 32, Steps: 63 | Train Loss: 0.3436305 Vali Loss: 1.2725567 Test Loss: 0.4882314
Validation loss decreased (1.272991 --> 1.272557).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.1742057800292969
Epoch: 33, Steps: 63 | Train Loss: 0.3430684 Vali Loss: 1.2722000 Test Loss: 0.4874896
Validation loss decreased (1.272557 --> 1.272200).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.182365894317627
Epoch: 34, Steps: 63 | Train Loss: 0.3422610 Vali Loss: 1.2698866 Test Loss: 0.4868313
Validation loss decreased (1.272200 --> 1.269887).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.112649917602539
Epoch: 35, Steps: 63 | Train Loss: 0.3418826 Vali Loss: 1.2691826 Test Loss: 0.4862002
Validation loss decreased (1.269887 --> 1.269183).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.1513307094573975
Epoch: 36, Steps: 63 | Train Loss: 0.3416395 Vali Loss: 1.2665495 Test Loss: 0.4856308
Validation loss decreased (1.269183 --> 1.266549).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.1840331554412842
Epoch: 37, Steps: 63 | Train Loss: 0.3413232 Vali Loss: 1.2676408 Test Loss: 0.4850608
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.2427895069122314
Epoch: 38, Steps: 63 | Train Loss: 0.3410718 Vali Loss: 1.2673107 Test Loss: 0.4845759
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.210780382156372
Epoch: 39, Steps: 63 | Train Loss: 0.3405405 Vali Loss: 1.2664145 Test Loss: 0.4840812
Validation loss decreased (1.266549 --> 1.266415).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.2041256427764893
Epoch: 40, Steps: 63 | Train Loss: 0.3402686 Vali Loss: 1.2671995 Test Loss: 0.4836476
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.188222885131836
Epoch: 41, Steps: 63 | Train Loss: 0.3398935 Vali Loss: 1.2674412 Test Loss: 0.4832006
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.2588067054748535
Epoch: 42, Steps: 63 | Train Loss: 0.3398127 Vali Loss: 1.2602587 Test Loss: 0.4827972
Validation loss decreased (1.266415 --> 1.260259).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.11181640625
Epoch: 43, Steps: 63 | Train Loss: 0.3392186 Vali Loss: 1.2621484 Test Loss: 0.4823995
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.2210912704467773
Epoch: 44, Steps: 63 | Train Loss: 0.3393873 Vali Loss: 1.2594632 Test Loss: 0.4820730
Validation loss decreased (1.260259 --> 1.259463).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.146784782409668
Epoch: 45, Steps: 63 | Train Loss: 0.3388839 Vali Loss: 1.2670813 Test Loss: 0.4817485
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.1255803108215332
Epoch: 46, Steps: 63 | Train Loss: 0.3387384 Vali Loss: 1.2617975 Test Loss: 0.4814246
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.2179811000823975
Epoch: 47, Steps: 63 | Train Loss: 0.3385401 Vali Loss: 1.2640009 Test Loss: 0.4811185
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.1138215065002441
Epoch: 48, Steps: 63 | Train Loss: 0.3384555 Vali Loss: 1.2631968 Test Loss: 0.4808390
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.2177987098693848
Epoch: 49, Steps: 63 | Train Loss: 0.3380931 Vali Loss: 1.2623382 Test Loss: 0.4805902
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2143580913543701
Epoch: 50, Steps: 63 | Train Loss: 0.3380724 Vali Loss: 1.2568028 Test Loss: 0.4803522
Validation loss decreased (1.259463 --> 1.256803).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.1354258060455322
Epoch: 51, Steps: 63 | Train Loss: 0.3379743 Vali Loss: 1.2636863 Test Loss: 0.4801117
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.1591289043426514
Epoch: 52, Steps: 63 | Train Loss: 0.3377396 Vali Loss: 1.2621459 Test Loss: 0.4798938
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.0699138641357422
Epoch: 53, Steps: 63 | Train Loss: 0.3377762 Vali Loss: 1.2611227 Test Loss: 0.4796740
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.109076976776123
Epoch: 54, Steps: 63 | Train Loss: 0.3375451 Vali Loss: 1.2600747 Test Loss: 0.4794683
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.1900074481964111
Epoch: 55, Steps: 63 | Train Loss: 0.3373881 Vali Loss: 1.2628336 Test Loss: 0.4792962
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.2121903896331787
Epoch: 56, Steps: 63 | Train Loss: 0.3372107 Vali Loss: 1.2570975 Test Loss: 0.4791105
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.151280403137207
Epoch: 57, Steps: 63 | Train Loss: 0.3370749 Vali Loss: 1.2592255 Test Loss: 0.4789414
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.1116852760314941
Epoch: 58, Steps: 63 | Train Loss: 0.3368750 Vali Loss: 1.2628021 Test Loss: 0.4787900
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.2059121131896973
Epoch: 59, Steps: 63 | Train Loss: 0.3366682 Vali Loss: 1.2605492 Test Loss: 0.4786302
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.2426471710205078
Epoch: 60, Steps: 63 | Train Loss: 0.3368850 Vali Loss: 1.2596042 Test Loss: 0.4784885
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.1639087200164795
Epoch: 61, Steps: 63 | Train Loss: 0.3367687 Vali Loss: 1.2606986 Test Loss: 0.4783646
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.2523279190063477
Epoch: 62, Steps: 63 | Train Loss: 0.3366136 Vali Loss: 1.2578927 Test Loss: 0.4782310
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.2117421627044678
Epoch: 63, Steps: 63 | Train Loss: 0.3365394 Vali Loss: 1.2585714 Test Loss: 0.4781068
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.1729967594146729
Epoch: 64, Steps: 63 | Train Loss: 0.3364440 Vali Loss: 1.2640998 Test Loss: 0.4779941
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.1635520458221436
Epoch: 65, Steps: 63 | Train Loss: 0.3363980 Vali Loss: 1.2607175 Test Loss: 0.4778745
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.205976963043213
Epoch: 66, Steps: 63 | Train Loss: 0.3360042 Vali Loss: 1.2620391 Test Loss: 0.4777670
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.1476507186889648
Epoch: 67, Steps: 63 | Train Loss: 0.3362458 Vali Loss: 1.2573866 Test Loss: 0.4776797
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.1168899536132812
Epoch: 68, Steps: 63 | Train Loss: 0.3359989 Vali Loss: 1.2549340 Test Loss: 0.4775845
Validation loss decreased (1.256803 --> 1.254934).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3097906112670898
Epoch: 69, Steps: 63 | Train Loss: 0.3360215 Vali Loss: 1.2577759 Test Loss: 0.4774932
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.186856985092163
Epoch: 70, Steps: 63 | Train Loss: 0.3358891 Vali Loss: 1.2556429 Test Loss: 0.4774079
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.2410132884979248
Epoch: 71, Steps: 63 | Train Loss: 0.3359799 Vali Loss: 1.2575080 Test Loss: 0.4773307
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.1580679416656494
Epoch: 72, Steps: 63 | Train Loss: 0.3360327 Vali Loss: 1.2561139 Test Loss: 0.4772585
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.1867353916168213
Epoch: 73, Steps: 63 | Train Loss: 0.3355065 Vali Loss: 1.2575272 Test Loss: 0.4771894
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.0561435222625732
Epoch: 74, Steps: 63 | Train Loss: 0.3355703 Vali Loss: 1.2609189 Test Loss: 0.4771160
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.1302108764648438
Epoch: 75, Steps: 63 | Train Loss: 0.3355360 Vali Loss: 1.2559661 Test Loss: 0.4770504
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.2059659957885742
Epoch: 76, Steps: 63 | Train Loss: 0.3358994 Vali Loss: 1.2542937 Test Loss: 0.4769864
Validation loss decreased (1.254934 --> 1.254294).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.5313286781311035
Epoch: 77, Steps: 63 | Train Loss: 0.3355696 Vali Loss: 1.2617885 Test Loss: 0.4769274
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.9639780521392822
Epoch: 78, Steps: 63 | Train Loss: 0.3356516 Vali Loss: 1.2527081 Test Loss: 0.4768744
Validation loss decreased (1.254294 --> 1.252708).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 4.108318567276001
Epoch: 79, Steps: 63 | Train Loss: 0.3356077 Vali Loss: 1.2562078 Test Loss: 0.4768220
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.4292373657226562
Epoch: 80, Steps: 63 | Train Loss: 0.3355716 Vali Loss: 1.2567556 Test Loss: 0.4767721
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.4399802684783936
Epoch: 81, Steps: 63 | Train Loss: 0.3353211 Vali Loss: 1.2568122 Test Loss: 0.4767266
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.4483397006988525
Epoch: 82, Steps: 63 | Train Loss: 0.3355392 Vali Loss: 1.2568916 Test Loss: 0.4766797
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.4045453071594238
Epoch: 83, Steps: 63 | Train Loss: 0.3354116 Vali Loss: 1.2548838 Test Loss: 0.4766393
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.2032201290130615
Epoch: 84, Steps: 63 | Train Loss: 0.3351870 Vali Loss: 1.2535722 Test Loss: 0.4765974
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.5728707313537598
Epoch: 85, Steps: 63 | Train Loss: 0.3352784 Vali Loss: 1.2568544 Test Loss: 0.4765585
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.323211669921875
Epoch: 86, Steps: 63 | Train Loss: 0.3351419 Vali Loss: 1.2539147 Test Loss: 0.4765243
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.5725228786468506
Epoch: 87, Steps: 63 | Train Loss: 0.3352075 Vali Loss: 1.2607100 Test Loss: 0.4764912
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.5834505558013916
Epoch: 88, Steps: 63 | Train Loss: 0.3353777 Vali Loss: 1.2571295 Test Loss: 0.4764567
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.065647602081299
Epoch: 89, Steps: 63 | Train Loss: 0.3352216 Vali Loss: 1.2503484 Test Loss: 0.4764256
Validation loss decreased (1.252708 --> 1.250348).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.15191912651062
Epoch: 90, Steps: 63 | Train Loss: 0.3351223 Vali Loss: 1.2573545 Test Loss: 0.4763962
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.4954745769500732
Epoch: 91, Steps: 63 | Train Loss: 0.3353890 Vali Loss: 1.2568610 Test Loss: 0.4763675
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.4498968124389648
Epoch: 92, Steps: 63 | Train Loss: 0.3350995 Vali Loss: 1.2551934 Test Loss: 0.4763421
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.4883697032928467
Epoch: 93, Steps: 63 | Train Loss: 0.3352092 Vali Loss: 1.2586577 Test Loss: 0.4763130
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.39841890335083
Epoch: 94, Steps: 63 | Train Loss: 0.3350434 Vali Loss: 1.2560377 Test Loss: 0.4762922
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.3783469200134277
Epoch: 95, Steps: 63 | Train Loss: 0.3350646 Vali Loss: 1.2514081 Test Loss: 0.4762668
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.723066806793213
Epoch: 96, Steps: 63 | Train Loss: 0.3351529 Vali Loss: 1.2564524 Test Loss: 0.4762455
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.5498838424682617
Epoch: 97, Steps: 63 | Train Loss: 0.3351356 Vali Loss: 1.2567719 Test Loss: 0.4762267
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.288360118865967
Epoch: 98, Steps: 63 | Train Loss: 0.3349201 Vali Loss: 1.2539870 Test Loss: 0.4762062
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.4919748306274414
Epoch: 99, Steps: 63 | Train Loss: 0.3351095 Vali Loss: 1.2575287 Test Loss: 0.4761878
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.72025990486145
Epoch: 100, Steps: 63 | Train Loss: 0.3349180 Vali Loss: 1.2548398 Test Loss: 0.4761707
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.1160680107021042e-06
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=26, out_features=74, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1723904.0
params:  1998.0
Trainable parameters:  1998
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.9125630855560303
Epoch: 1, Steps: 63 | Train Loss: 0.4814181 Vali Loss: 1.2511734 Test Loss: 0.4678980
Validation loss decreased (inf --> 1.251173).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5342376232147217
Epoch: 2, Steps: 63 | Train Loss: 0.4775688 Vali Loss: 1.2379533 Test Loss: 0.4642604
Validation loss decreased (1.251173 --> 1.237953).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.5943055152893066
Epoch: 3, Steps: 63 | Train Loss: 0.4750593 Vali Loss: 1.2319841 Test Loss: 0.4621699
Validation loss decreased (1.237953 --> 1.231984).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3691225051879883
Epoch: 4, Steps: 63 | Train Loss: 0.4740555 Vali Loss: 1.2425611 Test Loss: 0.4612899
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.6141197681427
Epoch: 5, Steps: 63 | Train Loss: 0.4731160 Vali Loss: 1.2355205 Test Loss: 0.4608019
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.495478868484497
Epoch: 6, Steps: 63 | Train Loss: 0.4728180 Vali Loss: 1.2314944 Test Loss: 0.4605736
Validation loss decreased (1.231984 --> 1.231494).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.0540900230407715
Epoch: 7, Steps: 63 | Train Loss: 0.4725117 Vali Loss: 1.2311198 Test Loss: 0.4608355
Validation loss decreased (1.231494 --> 1.231120).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.147897243499756
Epoch: 8, Steps: 63 | Train Loss: 0.4723806 Vali Loss: 1.2356873 Test Loss: 0.4606388
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.1816749572753906
Epoch: 9, Steps: 63 | Train Loss: 0.4717412 Vali Loss: 1.2320867 Test Loss: 0.4608573
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.119063377380371
Epoch: 10, Steps: 63 | Train Loss: 0.4720424 Vali Loss: 1.2351364 Test Loss: 0.4608625
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.1101422309875488
Epoch: 11, Steps: 63 | Train Loss: 0.4720498 Vali Loss: 1.2315366 Test Loss: 0.4607429
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.1801066398620605
Epoch: 12, Steps: 63 | Train Loss: 0.4722018 Vali Loss: 1.2317798 Test Loss: 0.4609531
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.2543971538543701
Epoch: 13, Steps: 63 | Train Loss: 0.4716006 Vali Loss: 1.2311622 Test Loss: 0.4609833
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.1051440238952637
Epoch: 14, Steps: 63 | Train Loss: 0.4717251 Vali Loss: 1.2271764 Test Loss: 0.4608910
Validation loss decreased (1.231120 --> 1.227176).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.2028768062591553
Epoch: 15, Steps: 63 | Train Loss: 0.4722939 Vali Loss: 1.2327662 Test Loss: 0.4609572
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.225066900253296
Epoch: 16, Steps: 63 | Train Loss: 0.4719515 Vali Loss: 1.2345401 Test Loss: 0.4609652
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.2424736022949219
Epoch: 17, Steps: 63 | Train Loss: 0.4719578 Vali Loss: 1.2324133 Test Loss: 0.4609089
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.1070785522460938
Epoch: 18, Steps: 63 | Train Loss: 0.4722029 Vali Loss: 1.2346746 Test Loss: 0.4610663
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.2783799171447754
Epoch: 19, Steps: 63 | Train Loss: 0.4718298 Vali Loss: 1.2322695 Test Loss: 0.4610806
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.1568031311035156
Epoch: 20, Steps: 63 | Train Loss: 0.4720378 Vali Loss: 1.2298563 Test Loss: 0.4611236
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.2738654613494873
Epoch: 21, Steps: 63 | Train Loss: 0.4719937 Vali Loss: 1.2290517 Test Loss: 0.4610907
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2308802604675293
Epoch: 22, Steps: 63 | Train Loss: 0.4716990 Vali Loss: 1.2347342 Test Loss: 0.4610958
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.1596364974975586
Epoch: 23, Steps: 63 | Train Loss: 0.4719595 Vali Loss: 1.2320541 Test Loss: 0.4610991
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.1899914741516113
Epoch: 24, Steps: 63 | Train Loss: 0.4715434 Vali Loss: 1.2300853 Test Loss: 0.4612363
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.2152771949768066
Epoch: 25, Steps: 63 | Train Loss: 0.4716437 Vali Loss: 1.2345407 Test Loss: 0.4612161
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2164249420166016
Epoch: 26, Steps: 63 | Train Loss: 0.4721769 Vali Loss: 1.2291042 Test Loss: 0.4612310
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.153369426727295
Epoch: 27, Steps: 63 | Train Loss: 0.4720649 Vali Loss: 1.2296956 Test Loss: 0.4612699
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.1171345710754395
Epoch: 28, Steps: 63 | Train Loss: 0.4715620 Vali Loss: 1.2352612 Test Loss: 0.4612404
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.1578054428100586
Epoch: 29, Steps: 63 | Train Loss: 0.4720638 Vali Loss: 1.2310959 Test Loss: 0.4612164
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.2500073909759521
Epoch: 30, Steps: 63 | Train Loss: 0.4710966 Vali Loss: 1.2327183 Test Loss: 0.4612225
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.1502752304077148
Epoch: 31, Steps: 63 | Train Loss: 0.4717513 Vali Loss: 1.2300744 Test Loss: 0.4612488
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.1511363983154297
Epoch: 32, Steps: 63 | Train Loss: 0.4721048 Vali Loss: 1.2345082 Test Loss: 0.4612904
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.2663631439208984
Epoch: 33, Steps: 63 | Train Loss: 0.4719382 Vali Loss: 1.2304735 Test Loss: 0.4612038
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.1338505744934082
Epoch: 34, Steps: 63 | Train Loss: 0.4716273 Vali Loss: 1.2316204 Test Loss: 0.4612680
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4601515531539917, mae:0.43944796919822693, rse:0.645806610584259, corr:[0.2560815  0.25784728 0.2573037  0.25479084 0.25155663 0.24913958
 0.24837759 0.24897157 0.24936712 0.24905065 0.24840403 0.2481104
 0.2482919  0.24825856 0.24798441 0.24772395 0.2474313  0.24705914
 0.24664615 0.24634875 0.24637124 0.24670596 0.24699144 0.24658939
 0.24563096 0.24518453 0.2450332  0.244776   0.24409956 0.24332942
 0.24276903 0.24247797 0.24237064 0.24241619 0.24255428 0.24291028
 0.24322966 0.24314813 0.24306995 0.24325626 0.24369305 0.2440785
 0.24423994 0.24436904 0.24475749 0.24517307 0.24548526 0.24495733
 0.24350116 0.24238247 0.24124132 0.2400056  0.23860224 0.23733296
 0.23670408 0.23678531 0.23699358 0.23712304 0.23701212 0.23742543
 0.23809038 0.23832276 0.23818243 0.23797247 0.23791218 0.2379227
 0.23794298 0.23789032 0.23795983 0.2380164  0.23787747 0.23693317
 0.23539007 0.23444502 0.23394765 0.23364381 0.23323022 0.23267388
 0.23236783 0.23231046 0.23220609 0.23197553 0.23156793 0.23152536
 0.23196034 0.23207887 0.23198625 0.23187208 0.23178683 0.2317477
 0.23168747 0.23174453 0.23200835 0.23251745 0.23307262 0.23307475
 0.23231463 0.23180789 0.23154163 0.23100768 0.2302897  0.22969267
 0.22949794 0.22987626 0.23022278 0.23023668 0.22992544 0.22995459
 0.23028274 0.23033047 0.2302255  0.23023748 0.23029844 0.23024774
 0.23013757 0.23009826 0.23022294 0.23038805 0.23039064 0.22975431
 0.22842973 0.2273489  0.22633219 0.2253589  0.224504   0.22403319
 0.2240214  0.22437215 0.22452474 0.22447042 0.22437473 0.2247389
 0.22536226 0.22548334 0.22551854 0.22563902 0.22576445 0.22584608
 0.22585025 0.22589909 0.22609681 0.22634509 0.22644408 0.22585897
 0.22455141 0.22352909 0.22280416 0.22184312 0.22097866 0.22040589
 0.22040449 0.2208462  0.22141862 0.22175218 0.22171594 0.22182587
 0.22209367 0.22203639 0.22193132 0.22183497 0.22166531 0.22155948
 0.22146933 0.22149613 0.22169948 0.22193787 0.22200532 0.22142786
 0.22037706 0.2197859  0.21941637 0.21886575 0.21828113 0.2179344
 0.21810597 0.21877979 0.2194168  0.21997502 0.22038071 0.22107255
 0.22193576 0.22221558 0.22212277 0.2220701  0.22207972 0.22218984
 0.22232944 0.22252482 0.22287014 0.22328663 0.22343104 0.22291192
 0.22171915 0.22086833 0.22018422 0.21937513 0.21850245 0.21781728
 0.21759813 0.21784833 0.21830946 0.21868205 0.2187433  0.21909195
 0.21959125 0.21957308 0.21932979 0.21911357 0.21898358 0.21890391
 0.218808   0.21871556 0.21879612 0.2189442  0.2190199  0.21861024
 0.2177576  0.21727017 0.21693079 0.21635568 0.21578945 0.2152913
 0.21527143 0.21561006 0.21599078 0.2160941  0.21594606 0.2161832
 0.21688488 0.21705125 0.21683961 0.2166311  0.21641183 0.2162465
 0.21620248 0.21632646 0.21668284 0.21708806 0.21727325 0.21680087
 0.21581632 0.2151817  0.21476038 0.21427776 0.21375155 0.21347097
 0.21366109 0.21409234 0.21448435 0.214764   0.21503723 0.21570918
 0.21657258 0.21678196 0.21675791 0.21674617 0.21678251 0.21683696
 0.21688029 0.21680908 0.21680439 0.21684067 0.21683313 0.21637617
 0.21552078 0.21511734 0.21482573 0.21429242 0.21361552 0.21309622
 0.21306162 0.21340016 0.21362177 0.21357876 0.21334621 0.2134164
 0.2140505  0.21429846 0.21424037 0.21408367 0.21384539 0.21362384
 0.2135586  0.21369383 0.21414642 0.2146971  0.21517126 0.21507993
 0.21439385 0.21411774 0.2140957  0.21392216 0.21361555 0.2136773
 0.21407652 0.21454106 0.21483363 0.21497358 0.21499102 0.21529664
 0.21588498 0.21584766 0.21551469 0.2153029  0.21537837 0.21559772
 0.21587373 0.2161135  0.2163715  0.21650515 0.21655096 0.21612167
 0.21514855 0.21475121 0.2146581  0.21417497 0.21360111 0.21299566
 0.2128631  0.21314579 0.21331176 0.21318579 0.21288988 0.21327618
 0.21458857 0.21519984 0.21511441 0.21505366 0.21502605 0.2148402
 0.21446821 0.21424738 0.2142784  0.21454467 0.21394943 0.20967315]
