Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_96_FITS_ETTh1_ftM_sl180_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8365
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=50, out_features=76, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3404800.0
params:  3876.0
Trainable parameters:  3876
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.153473615646362
Epoch: 1, Steps: 65 | Train Loss: 0.5599885 Vali Loss: 0.9913509 Test Loss: 0.5179991
Validation loss decreased (inf --> 0.991351).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.2386221885681152
Epoch: 2, Steps: 65 | Train Loss: 0.4255992 Vali Loss: 0.8459331 Test Loss: 0.4309363
Validation loss decreased (0.991351 --> 0.845933).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.9774320125579834
Epoch: 3, Steps: 65 | Train Loss: 0.3885976 Vali Loss: 0.8032054 Test Loss: 0.4041203
Validation loss decreased (0.845933 --> 0.803205).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0904338359832764
Epoch: 4, Steps: 65 | Train Loss: 0.3738718 Vali Loss: 0.7722560 Test Loss: 0.3930853
Validation loss decreased (0.803205 --> 0.772256).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.064420223236084
Epoch: 5, Steps: 65 | Train Loss: 0.3666659 Vali Loss: 0.7640284 Test Loss: 0.3873377
Validation loss decreased (0.772256 --> 0.764028).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.243614673614502
Epoch: 6, Steps: 65 | Train Loss: 0.3617454 Vali Loss: 0.7519917 Test Loss: 0.3843756
Validation loss decreased (0.764028 --> 0.751992).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.7723405361175537
Epoch: 7, Steps: 65 | Train Loss: 0.3592169 Vali Loss: 0.7450206 Test Loss: 0.3828797
Validation loss decreased (0.751992 --> 0.745021).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.0812551975250244
Epoch: 8, Steps: 65 | Train Loss: 0.3570655 Vali Loss: 0.7350477 Test Loss: 0.3821761
Validation loss decreased (0.745021 --> 0.735048).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.594799518585205
Epoch: 9, Steps: 65 | Train Loss: 0.3552971 Vali Loss: 0.7331521 Test Loss: 0.3820458
Validation loss decreased (0.735048 --> 0.733152).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.2980308532714844
Epoch: 10, Steps: 65 | Train Loss: 0.3546403 Vali Loss: 0.7358126 Test Loss: 0.3819529
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9785265922546387
Epoch: 11, Steps: 65 | Train Loss: 0.3539327 Vali Loss: 0.7305878 Test Loss: 0.3819834
Validation loss decreased (0.733152 --> 0.730588).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.196239471435547
Epoch: 12, Steps: 65 | Train Loss: 0.3531491 Vali Loss: 0.7244195 Test Loss: 0.3819333
Validation loss decreased (0.730588 --> 0.724419).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9351990222930908
Epoch: 13, Steps: 65 | Train Loss: 0.3527044 Vali Loss: 0.7253914 Test Loss: 0.3819986
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.3625223636627197
Epoch: 14, Steps: 65 | Train Loss: 0.3521689 Vali Loss: 0.7231999 Test Loss: 0.3820756
Validation loss decreased (0.724419 --> 0.723200).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.997296094894409
Epoch: 15, Steps: 65 | Train Loss: 0.3519186 Vali Loss: 0.7254569 Test Loss: 0.3822056
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.4979875087738037
Epoch: 16, Steps: 65 | Train Loss: 0.3512249 Vali Loss: 0.7191007 Test Loss: 0.3821088
Validation loss decreased (0.723200 --> 0.719101).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.0595548152923584
Epoch: 17, Steps: 65 | Train Loss: 0.3514006 Vali Loss: 0.7187894 Test Loss: 0.3822236
Validation loss decreased (0.719101 --> 0.718789).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.2679128646850586
Epoch: 18, Steps: 65 | Train Loss: 0.3512051 Vali Loss: 0.7179313 Test Loss: 0.3824124
Validation loss decreased (0.718789 --> 0.717931).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9294891357421875
Epoch: 19, Steps: 65 | Train Loss: 0.3508746 Vali Loss: 0.7167235 Test Loss: 0.3823527
Validation loss decreased (0.717931 --> 0.716724).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.8869054317474365
Epoch: 20, Steps: 65 | Train Loss: 0.3502586 Vali Loss: 0.7161173 Test Loss: 0.3823909
Validation loss decreased (0.716724 --> 0.716117).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.3620147705078125
Epoch: 21, Steps: 65 | Train Loss: 0.3508204 Vali Loss: 0.7133908 Test Loss: 0.3824057
Validation loss decreased (0.716117 --> 0.713391).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.2900071144104004
Epoch: 22, Steps: 65 | Train Loss: 0.3506516 Vali Loss: 0.7148386 Test Loss: 0.3825448
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.221827983856201
Epoch: 23, Steps: 65 | Train Loss: 0.3500766 Vali Loss: 0.7180003 Test Loss: 0.3825296
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.4219555854797363
Epoch: 24, Steps: 65 | Train Loss: 0.3502116 Vali Loss: 0.7159113 Test Loss: 0.3825808
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.9148237705230713
Epoch: 25, Steps: 65 | Train Loss: 0.3498443 Vali Loss: 0.7152448 Test Loss: 0.3826246
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.817248821258545
Epoch: 26, Steps: 65 | Train Loss: 0.3501228 Vali Loss: 0.7156785 Test Loss: 0.3826247
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.7103455066680908
Epoch: 27, Steps: 65 | Train Loss: 0.3499301 Vali Loss: 0.7110686 Test Loss: 0.3826918
Validation loss decreased (0.713391 --> 0.711069).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.2259044647216797
Epoch: 28, Steps: 65 | Train Loss: 0.3500234 Vali Loss: 0.7152576 Test Loss: 0.3826597
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.221193552017212
Epoch: 29, Steps: 65 | Train Loss: 0.3496207 Vali Loss: 0.7122890 Test Loss: 0.3825938
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.3035783767700195
Epoch: 30, Steps: 65 | Train Loss: 0.3499289 Vali Loss: 0.7125332 Test Loss: 0.3826537
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.281693458557129
Epoch: 31, Steps: 65 | Train Loss: 0.3499529 Vali Loss: 0.7130377 Test Loss: 0.3826363
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.7172884941101074
Epoch: 32, Steps: 65 | Train Loss: 0.3497132 Vali Loss: 0.7143031 Test Loss: 0.3827111
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.2054247856140137
Epoch: 33, Steps: 65 | Train Loss: 0.3491904 Vali Loss: 0.7157838 Test Loss: 0.3826892
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9441108703613281
Epoch: 34, Steps: 65 | Train Loss: 0.3495372 Vali Loss: 0.7082789 Test Loss: 0.3826740
Validation loss decreased (0.711069 --> 0.708279).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.5605404376983643
Epoch: 35, Steps: 65 | Train Loss: 0.3491593 Vali Loss: 0.7134826 Test Loss: 0.3826929
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.8632216453552246
Epoch: 36, Steps: 65 | Train Loss: 0.3491744 Vali Loss: 0.7119150 Test Loss: 0.3826742
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.683633804321289
Epoch: 37, Steps: 65 | Train Loss: 0.3495259 Vali Loss: 0.7121925 Test Loss: 0.3826780
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.205669641494751
Epoch: 38, Steps: 65 | Train Loss: 0.3492987 Vali Loss: 0.7085845 Test Loss: 0.3826964
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.0917108058929443
Epoch: 39, Steps: 65 | Train Loss: 0.3491186 Vali Loss: 0.7108135 Test Loss: 0.3826996
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.853309392929077
Epoch: 40, Steps: 65 | Train Loss: 0.3489959 Vali Loss: 0.7117098 Test Loss: 0.3826797
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.1536049842834473
Epoch: 41, Steps: 65 | Train Loss: 0.3494611 Vali Loss: 0.7108656 Test Loss: 0.3827018
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.326287031173706
Epoch: 42, Steps: 65 | Train Loss: 0.3492578 Vali Loss: 0.7137958 Test Loss: 0.3827325
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.680415391921997
Epoch: 43, Steps: 65 | Train Loss: 0.3493920 Vali Loss: 0.7087106 Test Loss: 0.3826876
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.6440889835357666
Epoch: 44, Steps: 65 | Train Loss: 0.3492814 Vali Loss: 0.7095256 Test Loss: 0.3826685
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.041478157043457
Epoch: 45, Steps: 65 | Train Loss: 0.3493953 Vali Loss: 0.7074195 Test Loss: 0.3826382
Validation loss decreased (0.708279 --> 0.707420).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.204881191253662
Epoch: 46, Steps: 65 | Train Loss: 0.3491095 Vali Loss: 0.7064242 Test Loss: 0.3826728
Validation loss decreased (0.707420 --> 0.706424).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.189056634902954
Epoch: 47, Steps: 65 | Train Loss: 0.3489099 Vali Loss: 0.7112601 Test Loss: 0.3826999
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.0202248096466064
Epoch: 48, Steps: 65 | Train Loss: 0.3488528 Vali Loss: 0.7104756 Test Loss: 0.3826672
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9191434383392334
Epoch: 49, Steps: 65 | Train Loss: 0.3491157 Vali Loss: 0.7096297 Test Loss: 0.3826860
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6902046203613281
Epoch: 50, Steps: 65 | Train Loss: 0.3489483 Vali Loss: 0.7068818 Test Loss: 0.3826493
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.720578908920288
Epoch: 51, Steps: 65 | Train Loss: 0.3487953 Vali Loss: 0.7096608 Test Loss: 0.3826698
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.088481664657593
Epoch: 52, Steps: 65 | Train Loss: 0.3490143 Vali Loss: 0.7085652 Test Loss: 0.3826838
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.5394797325134277
Epoch: 53, Steps: 65 | Train Loss: 0.3488288 Vali Loss: 0.7141861 Test Loss: 0.3826550
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.1385657787323
Epoch: 54, Steps: 65 | Train Loss: 0.3490753 Vali Loss: 0.7095667 Test Loss: 0.3826736
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.5757973194122314
Epoch: 55, Steps: 65 | Train Loss: 0.3489000 Vali Loss: 0.7081749 Test Loss: 0.3827130
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.8922982215881348
Epoch: 56, Steps: 65 | Train Loss: 0.3489934 Vali Loss: 0.7098967 Test Loss: 0.3827063
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.6343798637390137
Epoch: 57, Steps: 65 | Train Loss: 0.3488605 Vali Loss: 0.7104676 Test Loss: 0.3826898
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.153007745742798
Epoch: 58, Steps: 65 | Train Loss: 0.3492707 Vali Loss: 0.7060252 Test Loss: 0.3827016
Validation loss decreased (0.706424 --> 0.706025).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.7739291191101074
Epoch: 59, Steps: 65 | Train Loss: 0.3488004 Vali Loss: 0.7090740 Test Loss: 0.3827052
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.7508535385131836
Epoch: 60, Steps: 65 | Train Loss: 0.3488950 Vali Loss: 0.7097512 Test Loss: 0.3826885
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.9203362464904785
Epoch: 61, Steps: 65 | Train Loss: 0.3490444 Vali Loss: 0.7149926 Test Loss: 0.3826888
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.2238283157348633
Epoch: 62, Steps: 65 | Train Loss: 0.3487933 Vali Loss: 0.7116059 Test Loss: 0.3826703
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.47837233543396
Epoch: 63, Steps: 65 | Train Loss: 0.3488959 Vali Loss: 0.7048450 Test Loss: 0.3826944
Validation loss decreased (0.706025 --> 0.704845).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.080564498901367
Epoch: 64, Steps: 65 | Train Loss: 0.3489435 Vali Loss: 0.7094649 Test Loss: 0.3826808
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.8035435676574707
Epoch: 65, Steps: 65 | Train Loss: 0.3490460 Vali Loss: 0.7083242 Test Loss: 0.3826782
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.1245555877685547
Epoch: 66, Steps: 65 | Train Loss: 0.3487396 Vali Loss: 0.7082477 Test Loss: 0.3826874
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.0078318119049072
Epoch: 67, Steps: 65 | Train Loss: 0.3490399 Vali Loss: 0.7113644 Test Loss: 0.3826914
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.5440022945404053
Epoch: 68, Steps: 65 | Train Loss: 0.3490312 Vali Loss: 0.7098539 Test Loss: 0.3826879
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.713400363922119
Epoch: 69, Steps: 65 | Train Loss: 0.3491238 Vali Loss: 0.7095011 Test Loss: 0.3826778
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.388871192932129
Epoch: 70, Steps: 65 | Train Loss: 0.3486338 Vali Loss: 0.7101791 Test Loss: 0.3826965
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.6349987983703613
Epoch: 71, Steps: 65 | Train Loss: 0.3489897 Vali Loss: 0.7088544 Test Loss: 0.3826974
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.6951475143432617
Epoch: 72, Steps: 65 | Train Loss: 0.3490525 Vali Loss: 0.7102462 Test Loss: 0.3826947
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.738851308822632
Epoch: 73, Steps: 65 | Train Loss: 0.3489043 Vali Loss: 0.7106938 Test Loss: 0.3826850
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.6160318851470947
Epoch: 74, Steps: 65 | Train Loss: 0.3488375 Vali Loss: 0.7076952 Test Loss: 0.3826917
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.233102560043335
Epoch: 75, Steps: 65 | Train Loss: 0.3489590 Vali Loss: 0.7105401 Test Loss: 0.3826893
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.3123104572296143
Epoch: 76, Steps: 65 | Train Loss: 0.3487683 Vali Loss: 0.7081922 Test Loss: 0.3826905
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.1573939323425293
Epoch: 77, Steps: 65 | Train Loss: 0.3490582 Vali Loss: 0.7051272 Test Loss: 0.3826794
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.8882777690887451
Epoch: 78, Steps: 65 | Train Loss: 0.3489939 Vali Loss: 0.7068032 Test Loss: 0.3826860
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.46221661567688
Epoch: 79, Steps: 65 | Train Loss: 0.3490042 Vali Loss: 0.7108178 Test Loss: 0.3826818
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.7631759643554688
Epoch: 80, Steps: 65 | Train Loss: 0.3490684 Vali Loss: 0.7083496 Test Loss: 0.3826769
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.9773662090301514
Epoch: 81, Steps: 65 | Train Loss: 0.3490770 Vali Loss: 0.7061938 Test Loss: 0.3826773
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.1072237491607666
Epoch: 82, Steps: 65 | Train Loss: 0.3487464 Vali Loss: 0.7082447 Test Loss: 0.3826802
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.462193250656128
Epoch: 83, Steps: 65 | Train Loss: 0.3487335 Vali Loss: 0.7087862 Test Loss: 0.3826876
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_96_FITS_ETTh1_ftM_sl180_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.3820364475250244, mae:0.3960804045200348, rse:0.5870975255966187, corr:[0.27132824 0.27699763 0.27701226 0.27630207 0.27424407 0.2717605
 0.27113834 0.27182972 0.2713524  0.27060875 0.27066204 0.27069268
 0.2702376  0.26971942 0.26972118 0.26981458 0.26984757 0.2699971
 0.27007848 0.2698078  0.2694123  0.2695985  0.26989338 0.2693939
 0.26828706 0.26785502 0.2674905  0.26701114 0.2665577  0.26624826
 0.26575974 0.26538697 0.26529458 0.26503322 0.26478684 0.26512623
 0.26541036 0.26521778 0.26527688 0.26551926 0.26569307 0.26588002
 0.26619825 0.26638535 0.2663027  0.26641455 0.26693982 0.2669644
 0.2657584  0.26426005 0.2626445  0.2613988  0.26038516 0.25933972
 0.2584623  0.25793818 0.25798106 0.25830793 0.25817883 0.2586953
 0.25922132 0.25888833 0.25882062 0.25898328 0.25889918 0.25878054
 0.2590644  0.25909275 0.258739   0.25855982 0.2586306  0.25802273
 0.2567961  0.25580674 0.2547029  0.2540097  0.25379845 0.2535118
 0.25324655 0.25276917 0.25231647 0.25241548 0.2521548  0.2520589
 0.252545   0.2524181  0.25219595 0.25215155 0.25211605 0.25176802
 0.251225   0.25081533 0.25034305 0.24949415 0.24977756 0.2522909 ]
