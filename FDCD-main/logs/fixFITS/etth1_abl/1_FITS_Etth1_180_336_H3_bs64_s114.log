Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=34, out_features=97, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2955008.0
params:  3395.0
Trainable parameters:  3395
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.493847131729126
Epoch: 1, Steps: 63 | Train Loss: 0.8659465 Vali Loss: 1.7841421 Test Loss: 0.8213255
Validation loss decreased (inf --> 1.784142).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5247344970703125
Epoch: 2, Steps: 63 | Train Loss: 0.6603382 Vali Loss: 1.5233982 Test Loss: 0.6366886
Validation loss decreased (1.784142 --> 1.523398).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.6212806701660156
Epoch: 3, Steps: 63 | Train Loss: 0.5743003 Vali Loss: 1.4087417 Test Loss: 0.5532247
Validation loss decreased (1.523398 --> 1.408742).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4952125549316406
Epoch: 4, Steps: 63 | Train Loss: 0.5327093 Vali Loss: 1.3486261 Test Loss: 0.5121183
Validation loss decreased (1.408742 --> 1.348626).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.483428716659546
Epoch: 5, Steps: 63 | Train Loss: 0.5117833 Vali Loss: 1.3149129 Test Loss: 0.4905562
Validation loss decreased (1.348626 --> 1.314913).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.5125582218170166
Epoch: 6, Steps: 63 | Train Loss: 0.4995831 Vali Loss: 1.3012428 Test Loss: 0.4784918
Validation loss decreased (1.314913 --> 1.301243).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4518370628356934
Epoch: 7, Steps: 63 | Train Loss: 0.4925858 Vali Loss: 1.2834405 Test Loss: 0.4713399
Validation loss decreased (1.301243 --> 1.283440).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.448997974395752
Epoch: 8, Steps: 63 | Train Loss: 0.4879418 Vali Loss: 1.2763423 Test Loss: 0.4666497
Validation loss decreased (1.283440 --> 1.276342).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3734793663024902
Epoch: 9, Steps: 63 | Train Loss: 0.4843969 Vali Loss: 1.2708006 Test Loss: 0.4634002
Validation loss decreased (1.276342 --> 1.270801).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.4959540367126465
Epoch: 10, Steps: 63 | Train Loss: 0.4821084 Vali Loss: 1.2593732 Test Loss: 0.4611992
Validation loss decreased (1.270801 --> 1.259373).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.464637279510498
Epoch: 11, Steps: 63 | Train Loss: 0.4799378 Vali Loss: 1.2611047 Test Loss: 0.4595450
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.5957961082458496
Epoch: 12, Steps: 63 | Train Loss: 0.4786205 Vali Loss: 1.2567935 Test Loss: 0.4583550
Validation loss decreased (1.259373 --> 1.256793).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.4734387397766113
Epoch: 13, Steps: 63 | Train Loss: 0.4773816 Vali Loss: 1.2573842 Test Loss: 0.4574696
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.529784917831421
Epoch: 14, Steps: 63 | Train Loss: 0.4760483 Vali Loss: 1.2530998 Test Loss: 0.4567529
Validation loss decreased (1.256793 --> 1.253100).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5595555305480957
Epoch: 15, Steps: 63 | Train Loss: 0.4750783 Vali Loss: 1.2547157 Test Loss: 0.4563092
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4864003658294678
Epoch: 16, Steps: 63 | Train Loss: 0.4750067 Vali Loss: 1.2532680 Test Loss: 0.4559901
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3952107429504395
Epoch: 17, Steps: 63 | Train Loss: 0.4742121 Vali Loss: 1.2469125 Test Loss: 0.4557258
Validation loss decreased (1.253100 --> 1.246912).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.5650625228881836
Epoch: 18, Steps: 63 | Train Loss: 0.4730661 Vali Loss: 1.2430394 Test Loss: 0.4555393
Validation loss decreased (1.246912 --> 1.243039).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5446009635925293
Epoch: 19, Steps: 63 | Train Loss: 0.4727857 Vali Loss: 1.2436343 Test Loss: 0.4554787
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.388545274734497
Epoch: 20, Steps: 63 | Train Loss: 0.4728987 Vali Loss: 1.2451283 Test Loss: 0.4553664
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.5089058876037598
Epoch: 21, Steps: 63 | Train Loss: 0.4719152 Vali Loss: 1.2389269 Test Loss: 0.4553000
Validation loss decreased (1.243039 --> 1.238927).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5825364589691162
Epoch: 22, Steps: 63 | Train Loss: 0.4720027 Vali Loss: 1.2430470 Test Loss: 0.4552894
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5761737823486328
Epoch: 23, Steps: 63 | Train Loss: 0.4716132 Vali Loss: 1.2459065 Test Loss: 0.4552916
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.649627685546875
Epoch: 24, Steps: 63 | Train Loss: 0.4717173 Vali Loss: 1.2404875 Test Loss: 0.4553122
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.635375738143921
Epoch: 25, Steps: 63 | Train Loss: 0.4712398 Vali Loss: 1.2474021 Test Loss: 0.4553511
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.6423008441925049
Epoch: 26, Steps: 63 | Train Loss: 0.4713830 Vali Loss: 1.2416793 Test Loss: 0.4553581
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.46480393409729
Epoch: 27, Steps: 63 | Train Loss: 0.4709720 Vali Loss: 1.2487839 Test Loss: 0.4554047
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.561739206314087
Epoch: 28, Steps: 63 | Train Loss: 0.4710229 Vali Loss: 1.2406206 Test Loss: 0.4554567
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.393934726715088
Epoch: 29, Steps: 63 | Train Loss: 0.4705457 Vali Loss: 1.2382107 Test Loss: 0.4555041
Validation loss decreased (1.238927 --> 1.238211).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5128812789916992
Epoch: 30, Steps: 63 | Train Loss: 0.4705791 Vali Loss: 1.2412913 Test Loss: 0.4555468
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.528759479522705
Epoch: 31, Steps: 63 | Train Loss: 0.4705657 Vali Loss: 1.2384437 Test Loss: 0.4555999
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.473848819732666
Epoch: 32, Steps: 63 | Train Loss: 0.4706313 Vali Loss: 1.2363073 Test Loss: 0.4556671
Validation loss decreased (1.238211 --> 1.236307).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.3818910121917725
Epoch: 33, Steps: 63 | Train Loss: 0.4703936 Vali Loss: 1.2406046 Test Loss: 0.4557061
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3946516513824463
Epoch: 34, Steps: 63 | Train Loss: 0.4700347 Vali Loss: 1.2388272 Test Loss: 0.4557656
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.3999028205871582
Epoch: 35, Steps: 63 | Train Loss: 0.4702930 Vali Loss: 1.2355865 Test Loss: 0.4558106
Validation loss decreased (1.236307 --> 1.235587).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.4626836776733398
Epoch: 36, Steps: 63 | Train Loss: 0.4702701 Vali Loss: 1.2379400 Test Loss: 0.4558554
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.476172685623169
Epoch: 37, Steps: 63 | Train Loss: 0.4699334 Vali Loss: 1.2433994 Test Loss: 0.4558947
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.5332767963409424
Epoch: 38, Steps: 63 | Train Loss: 0.4698373 Vali Loss: 1.2339448 Test Loss: 0.4559715
Validation loss decreased (1.235587 --> 1.233945).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.818033218383789
Epoch: 39, Steps: 63 | Train Loss: 0.4699100 Vali Loss: 1.2369100 Test Loss: 0.4560010
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.469074010848999
Epoch: 40, Steps: 63 | Train Loss: 0.4696731 Vali Loss: 1.2376961 Test Loss: 0.4560309
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6200072765350342
Epoch: 41, Steps: 63 | Train Loss: 0.4697114 Vali Loss: 1.2393268 Test Loss: 0.4560837
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.4321033954620361
Epoch: 42, Steps: 63 | Train Loss: 0.4698155 Vali Loss: 1.2420506 Test Loss: 0.4561356
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.4571630954742432
Epoch: 43, Steps: 63 | Train Loss: 0.4700678 Vali Loss: 1.2345966 Test Loss: 0.4561700
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.3545002937316895
Epoch: 44, Steps: 63 | Train Loss: 0.4694067 Vali Loss: 1.2384789 Test Loss: 0.4562024
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.608079195022583
Epoch: 45, Steps: 63 | Train Loss: 0.4697794 Vali Loss: 1.2373329 Test Loss: 0.4562437
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.9251012802124023
Epoch: 46, Steps: 63 | Train Loss: 0.4695712 Vali Loss: 1.2408018 Test Loss: 0.4562790
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.4068644046783447
Epoch: 47, Steps: 63 | Train Loss: 0.4693623 Vali Loss: 1.2421533 Test Loss: 0.4563130
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.5481064319610596
Epoch: 48, Steps: 63 | Train Loss: 0.4694312 Vali Loss: 1.2406200 Test Loss: 0.4563597
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.637794017791748
Epoch: 49, Steps: 63 | Train Loss: 0.4698443 Vali Loss: 1.2416792 Test Loss: 0.4563858
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.7845916748046875
Epoch: 50, Steps: 63 | Train Loss: 0.4700358 Vali Loss: 1.2398248 Test Loss: 0.4564179
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.5629241466522217
Epoch: 51, Steps: 63 | Train Loss: 0.4695567 Vali Loss: 1.2365549 Test Loss: 0.4564567
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.5498323440551758
Epoch: 52, Steps: 63 | Train Loss: 0.4692218 Vali Loss: 1.2349372 Test Loss: 0.4564641
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.457108974456787
Epoch: 53, Steps: 63 | Train Loss: 0.4692457 Vali Loss: 1.2317314 Test Loss: 0.4565023
Validation loss decreased (1.233945 --> 1.231731).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.4620778560638428
Epoch: 54, Steps: 63 | Train Loss: 0.4691190 Vali Loss: 1.2322588 Test Loss: 0.4565304
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.4624741077423096
Epoch: 55, Steps: 63 | Train Loss: 0.4691472 Vali Loss: 1.2376065 Test Loss: 0.4565487
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.516145944595337
Epoch: 56, Steps: 63 | Train Loss: 0.4696289 Vali Loss: 1.2388315 Test Loss: 0.4565917
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.3860065937042236
Epoch: 57, Steps: 63 | Train Loss: 0.4696653 Vali Loss: 1.2358049 Test Loss: 0.4566090
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.4447596073150635
Epoch: 58, Steps: 63 | Train Loss: 0.4695046 Vali Loss: 1.2364922 Test Loss: 0.4566218
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.4554736614227295
Epoch: 59, Steps: 63 | Train Loss: 0.4694712 Vali Loss: 1.2330052 Test Loss: 0.4566361
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.4398322105407715
Epoch: 60, Steps: 63 | Train Loss: 0.4695344 Vali Loss: 1.2344130 Test Loss: 0.4566676
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.5387532711029053
Epoch: 61, Steps: 63 | Train Loss: 0.4696449 Vali Loss: 1.2344449 Test Loss: 0.4566899
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.535160779953003
Epoch: 62, Steps: 63 | Train Loss: 0.4695632 Vali Loss: 1.2364359 Test Loss: 0.4567072
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.410207986831665
Epoch: 63, Steps: 63 | Train Loss: 0.4692044 Vali Loss: 1.2409556 Test Loss: 0.4567271
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.519000768661499
Epoch: 64, Steps: 63 | Train Loss: 0.4693493 Vali Loss: 1.2339081 Test Loss: 0.4567495
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.490583896636963
Epoch: 65, Steps: 63 | Train Loss: 0.4691196 Vali Loss: 1.2374440 Test Loss: 0.4567593
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.563232660293579
Epoch: 66, Steps: 63 | Train Loss: 0.4691461 Vali Loss: 1.2353988 Test Loss: 0.4567803
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.5254192352294922
Epoch: 67, Steps: 63 | Train Loss: 0.4689965 Vali Loss: 1.2366780 Test Loss: 0.4567877
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.4935266971588135
Epoch: 68, Steps: 63 | Train Loss: 0.4690838 Vali Loss: 1.2338177 Test Loss: 0.4568092
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.4935410022735596
Epoch: 69, Steps: 63 | Train Loss: 0.4692630 Vali Loss: 1.2373158 Test Loss: 0.4568202
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.4795212745666504
Epoch: 70, Steps: 63 | Train Loss: 0.4695603 Vali Loss: 1.2374109 Test Loss: 0.4568461
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.5404698848724365
Epoch: 71, Steps: 63 | Train Loss: 0.4690635 Vali Loss: 1.2358580 Test Loss: 0.4568511
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.5402603149414062
Epoch: 72, Steps: 63 | Train Loss: 0.4689972 Vali Loss: 1.2359791 Test Loss: 0.4568669
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.5321855545043945
Epoch: 73, Steps: 63 | Train Loss: 0.4692207 Vali Loss: 1.2352289 Test Loss: 0.4568737
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4557094871997833, mae:0.43654629588127136, rse:0.642681896686554, corr:[0.25286853 0.25729075 0.25682202 0.25436994 0.25217396 0.25087646
 0.25048357 0.25032574 0.24955992 0.249095   0.24936754 0.2499138
 0.24976857 0.24868588 0.24788839 0.24799252 0.2483522  0.24838232
 0.24812183 0.24790533 0.24812375 0.24865106 0.248942   0.24848565
 0.24753436 0.24708727 0.2467576  0.24609518 0.24504146 0.24424928
 0.24415813 0.24445462 0.24439505 0.24389541 0.24351373 0.24396563
 0.24477522 0.24494462 0.24481863 0.24485223 0.24518031 0.24548756
 0.24542911 0.24540976 0.24594602 0.24667056 0.24708572 0.2465262
 0.24495678 0.24381025 0.24271026 0.24145883 0.23995847 0.23857191
 0.23786089 0.2378011  0.23781419 0.23795766 0.23806964 0.238696
 0.23920202 0.23897444 0.23858388 0.23852494 0.2387991  0.23904672
 0.23900111 0.23868687 0.23865293 0.23887205 0.23895578 0.23823766
 0.23678075 0.2356789  0.23491304 0.23437625 0.23389952 0.23343271
 0.23332514 0.23347613 0.23335806 0.23293757 0.23245637 0.23260835
 0.23321576 0.23327912 0.23303811 0.23285441 0.23275624 0.23266387
 0.23244306 0.23237087 0.23273076 0.23339833 0.23387249 0.23367646
 0.23278137 0.23228286 0.23210378 0.2316193  0.23084624 0.23012349
 0.2298801  0.23030807 0.23072219 0.23089413 0.23080945 0.23105304
 0.23132981 0.23106481 0.23073179 0.23069398 0.23077671 0.23074971
 0.2305676  0.23041232 0.23054397 0.2308276  0.2308383  0.2302023
 0.22886167 0.2276974  0.22654985 0.22543515 0.22445679 0.22391216
 0.22394693 0.22442351 0.22455989 0.22446613 0.2245137  0.22528481
 0.2262766  0.22642812 0.22626176 0.22616287 0.22620746 0.22634542
 0.22636564 0.22635427 0.22657377 0.22690481 0.22694503 0.22630613
 0.22502166 0.22409727 0.22339909 0.22234432 0.22133867 0.22068052
 0.22070606 0.22114292 0.22157632 0.22186473 0.22203031 0.22248884
 0.2229614  0.22283712 0.22262888 0.22263156 0.22268888 0.2227775
 0.22273326 0.22272184 0.22302018 0.22347747 0.2236447  0.22314978
 0.2221098  0.2214174  0.22085416 0.2201173  0.21943194 0.21905965
 0.21926515 0.21994647 0.220415   0.22078323 0.22119571 0.2220906
 0.22306731 0.2232182  0.22296762 0.22281405 0.22286327 0.22308065
 0.22323221 0.22338495 0.22378686 0.22425283 0.22433166 0.22367747
 0.22237961 0.22162196 0.2211169  0.22037128 0.21938351 0.21850191
 0.21821035 0.21851833 0.21901332 0.21938719 0.21946245 0.21986957
 0.22033235 0.22018045 0.21991692 0.21979414 0.21973361 0.21961273
 0.21934411 0.21909697 0.21924795 0.2196368  0.2198417  0.21944952
 0.21852121 0.21792713 0.2174973  0.21688512 0.21631378 0.21584909
 0.21593435 0.21634002 0.21659684 0.2165358  0.2164334  0.21695688
 0.2177746  0.21773662 0.21738179 0.21726839 0.21732645 0.21739152
 0.21730913 0.21721527 0.21739724 0.21770595 0.21777934 0.21722394
 0.21622838 0.21568066 0.21532744 0.21478723 0.21413046 0.21374781
 0.21396428 0.2145602  0.21513158 0.21553643 0.2158071  0.21640971
 0.2171589  0.21733244 0.21740417 0.2175629  0.21769382 0.21767086
 0.21751486 0.21728666 0.2173466  0.21764223 0.21781097 0.21735595
 0.21631922 0.21566829 0.21518905 0.21459354 0.21393907 0.21342456
 0.213397   0.2137512  0.21393278 0.21399029 0.21412018 0.2146764
 0.21551937 0.2154819  0.21504901 0.21475317 0.21475299 0.21493861
 0.21500659 0.21493559 0.21514806 0.21559459 0.21593592 0.2156761
 0.21489199 0.21468748 0.21477161 0.21457106 0.21405977 0.21389188
 0.21426779 0.21492162 0.21536978 0.21549484 0.2154048  0.21576694
 0.21656209 0.216802   0.21687175 0.2170088  0.2171362  0.21716283
 0.21711625 0.21714136 0.2174547  0.2177672  0.21779527 0.2172067
 0.21606326 0.21559717 0.21546245 0.21491578 0.21425754 0.21349248
 0.21320392 0.21332979 0.21339983 0.21357147 0.21400335 0.21507604
 0.21632123 0.2161671  0.2155588  0.21592686 0.217064   0.21789296
 0.21742216 0.21588561 0.21454957 0.21485548 0.21583983 0.21157773]
