Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=106, out_features=318, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30202368.0
params:  34026.0
Trainable parameters:  34026
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.5006277561187744
Epoch: 1, Steps: 59 | Train Loss: 0.9150369 Vali Loss: 2.1521435 Test Loss: 0.9210757
Validation loss decreased (inf --> 2.152143).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.4564321041107178
Epoch: 2, Steps: 59 | Train Loss: 0.6980272 Vali Loss: 1.9043932 Test Loss: 0.7522359
Validation loss decreased (2.152143 --> 1.904393).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.3062822818756104
Epoch: 3, Steps: 59 | Train Loss: 0.6018159 Vali Loss: 1.7944441 Test Loss: 0.6812022
Validation loss decreased (1.904393 --> 1.794444).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.359114646911621
Epoch: 4, Steps: 59 | Train Loss: 0.5564136 Vali Loss: 1.7435832 Test Loss: 0.6474265
Validation loss decreased (1.794444 --> 1.743583).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.442002534866333
Epoch: 5, Steps: 59 | Train Loss: 0.5316817 Vali Loss: 1.7104638 Test Loss: 0.6269674
Validation loss decreased (1.743583 --> 1.710464).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.4370946884155273
Epoch: 6, Steps: 59 | Train Loss: 0.5154327 Vali Loss: 1.6885446 Test Loss: 0.6123671
Validation loss decreased (1.710464 --> 1.688545).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4075801372528076
Epoch: 7, Steps: 59 | Train Loss: 0.5033839 Vali Loss: 1.6712211 Test Loss: 0.6006219
Validation loss decreased (1.688545 --> 1.671221).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5865352153778076
Epoch: 8, Steps: 59 | Train Loss: 0.4936577 Vali Loss: 1.6604444 Test Loss: 0.5897684
Validation loss decreased (1.671221 --> 1.660444).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3976798057556152
Epoch: 9, Steps: 59 | Train Loss: 0.4854200 Vali Loss: 1.6431873 Test Loss: 0.5806557
Validation loss decreased (1.660444 --> 1.643187).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.32759690284729
Epoch: 10, Steps: 59 | Train Loss: 0.4783999 Vali Loss: 1.6389279 Test Loss: 0.5720057
Validation loss decreased (1.643187 --> 1.638928).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.2900640964508057
Epoch: 11, Steps: 59 | Train Loss: 0.4724957 Vali Loss: 1.6269941 Test Loss: 0.5641721
Validation loss decreased (1.638928 --> 1.626994).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.326887607574463
Epoch: 12, Steps: 59 | Train Loss: 0.4671060 Vali Loss: 1.6183342 Test Loss: 0.5568284
Validation loss decreased (1.626994 --> 1.618334).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.4266736507415771
Epoch: 13, Steps: 59 | Train Loss: 0.4624648 Vali Loss: 1.6068350 Test Loss: 0.5504766
Validation loss decreased (1.618334 --> 1.606835).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.2598154544830322
Epoch: 14, Steps: 59 | Train Loss: 0.4582110 Vali Loss: 1.6011188 Test Loss: 0.5443511
Validation loss decreased (1.606835 --> 1.601119).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.364788293838501
Epoch: 15, Steps: 59 | Train Loss: 0.4544308 Vali Loss: 1.5918379 Test Loss: 0.5388179
Validation loss decreased (1.601119 --> 1.591838).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.292053461074829
Epoch: 16, Steps: 59 | Train Loss: 0.4511327 Vali Loss: 1.5857589 Test Loss: 0.5338532
Validation loss decreased (1.591838 --> 1.585759).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3882462978363037
Epoch: 17, Steps: 59 | Train Loss: 0.4479338 Vali Loss: 1.5784514 Test Loss: 0.5291445
Validation loss decreased (1.585759 --> 1.578451).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.3750207424163818
Epoch: 18, Steps: 59 | Train Loss: 0.4452116 Vali Loss: 1.5761484 Test Loss: 0.5248515
Validation loss decreased (1.578451 --> 1.576148).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4373273849487305
Epoch: 19, Steps: 59 | Train Loss: 0.4426365 Vali Loss: 1.5727048 Test Loss: 0.5209073
Validation loss decreased (1.576148 --> 1.572705).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3747870922088623
Epoch: 20, Steps: 59 | Train Loss: 0.4402474 Vali Loss: 1.5674074 Test Loss: 0.5174822
Validation loss decreased (1.572705 --> 1.567407).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.367262601852417
Epoch: 21, Steps: 59 | Train Loss: 0.4381257 Vali Loss: 1.5603857 Test Loss: 0.5139846
Validation loss decreased (1.567407 --> 1.560386).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.3729777336120605
Epoch: 22, Steps: 59 | Train Loss: 0.4363186 Vali Loss: 1.5527177 Test Loss: 0.5108101
Validation loss decreased (1.560386 --> 1.552718).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.406717300415039
Epoch: 23, Steps: 59 | Train Loss: 0.4343886 Vali Loss: 1.5547762 Test Loss: 0.5078487
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4504408836364746
Epoch: 24, Steps: 59 | Train Loss: 0.4327812 Vali Loss: 1.5501707 Test Loss: 0.5052581
Validation loss decreased (1.552718 --> 1.550171).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.4183509349822998
Epoch: 25, Steps: 59 | Train Loss: 0.4312194 Vali Loss: 1.5431725 Test Loss: 0.5026859
Validation loss decreased (1.550171 --> 1.543172).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4096426963806152
Epoch: 26, Steps: 59 | Train Loss: 0.4297546 Vali Loss: 1.5476825 Test Loss: 0.5002694
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.3504419326782227
Epoch: 27, Steps: 59 | Train Loss: 0.4284429 Vali Loss: 1.5388701 Test Loss: 0.4981661
Validation loss decreased (1.543172 --> 1.538870).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.576836109161377
Epoch: 28, Steps: 59 | Train Loss: 0.4270972 Vali Loss: 1.5330107 Test Loss: 0.4960123
Validation loss decreased (1.538870 --> 1.533011).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.3110296726226807
Epoch: 29, Steps: 59 | Train Loss: 0.4259302 Vali Loss: 1.5368372 Test Loss: 0.4940473
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.334928035736084
Epoch: 30, Steps: 59 | Train Loss: 0.4249413 Vali Loss: 1.5333443 Test Loss: 0.4923458
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.4741642475128174
Epoch: 31, Steps: 59 | Train Loss: 0.4238552 Vali Loss: 1.5321050 Test Loss: 0.4905493
Validation loss decreased (1.533011 --> 1.532105).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.4374537467956543
Epoch: 32, Steps: 59 | Train Loss: 0.4229256 Vali Loss: 1.5296185 Test Loss: 0.4891109
Validation loss decreased (1.532105 --> 1.529619).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.42927885055542
Epoch: 33, Steps: 59 | Train Loss: 0.4220084 Vali Loss: 1.5252318 Test Loss: 0.4875675
Validation loss decreased (1.529619 --> 1.525232).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3919847011566162
Epoch: 34, Steps: 59 | Train Loss: 0.4211875 Vali Loss: 1.5267570 Test Loss: 0.4861555
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.5571067333221436
Epoch: 35, Steps: 59 | Train Loss: 0.4203560 Vali Loss: 1.5182372 Test Loss: 0.4848472
Validation loss decreased (1.525232 --> 1.518237).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.2851195335388184
Epoch: 36, Steps: 59 | Train Loss: 0.4197612 Vali Loss: 1.5227792 Test Loss: 0.4836356
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.4997925758361816
Epoch: 37, Steps: 59 | Train Loss: 0.4190767 Vali Loss: 1.5179889 Test Loss: 0.4825113
Validation loss decreased (1.518237 --> 1.517989).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.3864641189575195
Epoch: 38, Steps: 59 | Train Loss: 0.4183867 Vali Loss: 1.5145943 Test Loss: 0.4813749
Validation loss decreased (1.517989 --> 1.514594).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.4041614532470703
Epoch: 39, Steps: 59 | Train Loss: 0.4176239 Vali Loss: 1.5155175 Test Loss: 0.4803154
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.3337302207946777
Epoch: 40, Steps: 59 | Train Loss: 0.4171005 Vali Loss: 1.5153534 Test Loss: 0.4793372
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.4052865505218506
Epoch: 41, Steps: 59 | Train Loss: 0.4165415 Vali Loss: 1.5184294 Test Loss: 0.4784545
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3808984756469727
Epoch: 42, Steps: 59 | Train Loss: 0.4161307 Vali Loss: 1.5120721 Test Loss: 0.4775750
Validation loss decreased (1.514594 --> 1.512072).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.3857367038726807
Epoch: 43, Steps: 59 | Train Loss: 0.4157118 Vali Loss: 1.5112770 Test Loss: 0.4767125
Validation loss decreased (1.512072 --> 1.511277).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.4974730014801025
Epoch: 44, Steps: 59 | Train Loss: 0.4151551 Vali Loss: 1.5062512 Test Loss: 0.4759425
Validation loss decreased (1.511277 --> 1.506251).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.3568503856658936
Epoch: 45, Steps: 59 | Train Loss: 0.4146493 Vali Loss: 1.5114948 Test Loss: 0.4752222
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.4598808288574219
Epoch: 46, Steps: 59 | Train Loss: 0.4142999 Vali Loss: 1.5058315 Test Loss: 0.4745330
Validation loss decreased (1.506251 --> 1.505831).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.3580753803253174
Epoch: 47, Steps: 59 | Train Loss: 0.4139908 Vali Loss: 1.5070707 Test Loss: 0.4738797
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.2848169803619385
Epoch: 48, Steps: 59 | Train Loss: 0.4135432 Vali Loss: 1.5034556 Test Loss: 0.4732925
Validation loss decreased (1.505831 --> 1.503456).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.8847928047180176
Epoch: 49, Steps: 59 | Train Loss: 0.4132659 Vali Loss: 1.5066054 Test Loss: 0.4726643
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.3458154201507568
Epoch: 50, Steps: 59 | Train Loss: 0.4129903 Vali Loss: 1.5054550 Test Loss: 0.4721110
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3343303203582764
Epoch: 51, Steps: 59 | Train Loss: 0.4126807 Vali Loss: 1.5107348 Test Loss: 0.4715999
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.4893386363983154
Epoch: 52, Steps: 59 | Train Loss: 0.4122581 Vali Loss: 1.4975576 Test Loss: 0.4710677
Validation loss decreased (1.503456 --> 1.497558).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.3625974655151367
Epoch: 53, Steps: 59 | Train Loss: 0.4118369 Vali Loss: 1.5027184 Test Loss: 0.4706188
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.4725308418273926
Epoch: 54, Steps: 59 | Train Loss: 0.4117845 Vali Loss: 1.5051588 Test Loss: 0.4701727
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.409071445465088
Epoch: 55, Steps: 59 | Train Loss: 0.4114345 Vali Loss: 1.5009576 Test Loss: 0.4697358
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.3350722789764404
Epoch: 56, Steps: 59 | Train Loss: 0.4112224 Vali Loss: 1.5035219 Test Loss: 0.4693213
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.8625097274780273
Epoch: 57, Steps: 59 | Train Loss: 0.4109447 Vali Loss: 1.5029814 Test Loss: 0.4689551
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.4865717887878418
Epoch: 58, Steps: 59 | Train Loss: 0.4107918 Vali Loss: 1.4984879 Test Loss: 0.4686089
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.3968780040740967
Epoch: 59, Steps: 59 | Train Loss: 0.4106583 Vali Loss: 1.5095903 Test Loss: 0.4682486
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.3867182731628418
Epoch: 60, Steps: 59 | Train Loss: 0.4104182 Vali Loss: 1.5031929 Test Loss: 0.4679309
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.357400894165039
Epoch: 61, Steps: 59 | Train Loss: 0.4103168 Vali Loss: 1.4959877 Test Loss: 0.4676148
Validation loss decreased (1.497558 --> 1.495988).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.4445350170135498
Epoch: 62, Steps: 59 | Train Loss: 0.4101390 Vali Loss: 1.4933188 Test Loss: 0.4673133
Validation loss decreased (1.495988 --> 1.493319).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.5161116123199463
Epoch: 63, Steps: 59 | Train Loss: 0.4099761 Vali Loss: 1.4954222 Test Loss: 0.4670330
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.4577991962432861
Epoch: 64, Steps: 59 | Train Loss: 0.4097346 Vali Loss: 1.4927051 Test Loss: 0.4667575
Validation loss decreased (1.493319 --> 1.492705).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.3328001499176025
Epoch: 65, Steps: 59 | Train Loss: 0.4096230 Vali Loss: 1.4968958 Test Loss: 0.4665010
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.429731845855713
Epoch: 66, Steps: 59 | Train Loss: 0.4095272 Vali Loss: 1.4961970 Test Loss: 0.4662534
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.469893455505371
Epoch: 67, Steps: 59 | Train Loss: 0.4093479 Vali Loss: 1.4944344 Test Loss: 0.4660192
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.385296106338501
Epoch: 68, Steps: 59 | Train Loss: 0.4091730 Vali Loss: 1.4960132 Test Loss: 0.4657992
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3924720287322998
Epoch: 69, Steps: 59 | Train Loss: 0.4090911 Vali Loss: 1.4905012 Test Loss: 0.4655979
Validation loss decreased (1.492705 --> 1.490501).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.397456169128418
Epoch: 70, Steps: 59 | Train Loss: 0.4089682 Vali Loss: 1.4941795 Test Loss: 0.4653995
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.2753608226776123
Epoch: 71, Steps: 59 | Train Loss: 0.4088140 Vali Loss: 1.4966505 Test Loss: 0.4652149
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.284374713897705
Epoch: 72, Steps: 59 | Train Loss: 0.4088017 Vali Loss: 1.4971886 Test Loss: 0.4650277
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.2911365032196045
Epoch: 73, Steps: 59 | Train Loss: 0.4086307 Vali Loss: 1.4974884 Test Loss: 0.4648521
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.3433136940002441
Epoch: 74, Steps: 59 | Train Loss: 0.4084873 Vali Loss: 1.4990582 Test Loss: 0.4646947
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.3810606002807617
Epoch: 75, Steps: 59 | Train Loss: 0.4083332 Vali Loss: 1.4925561 Test Loss: 0.4645432
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.238933801651001
Epoch: 76, Steps: 59 | Train Loss: 0.4082103 Vali Loss: 1.4954453 Test Loss: 0.4643980
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.3372776508331299
Epoch: 77, Steps: 59 | Train Loss: 0.4082040 Vali Loss: 1.4971617 Test Loss: 0.4642586
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.3448951244354248
Epoch: 78, Steps: 59 | Train Loss: 0.4082266 Vali Loss: 1.4956136 Test Loss: 0.4641303
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.1694629192352295
Epoch: 79, Steps: 59 | Train Loss: 0.4081938 Vali Loss: 1.4975455 Test Loss: 0.4640025
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.2750070095062256
Epoch: 80, Steps: 59 | Train Loss: 0.4081133 Vali Loss: 1.4907764 Test Loss: 0.4638838
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.3837313652038574
Epoch: 81, Steps: 59 | Train Loss: 0.4079934 Vali Loss: 1.4959430 Test Loss: 0.4637704
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.3321826457977295
Epoch: 82, Steps: 59 | Train Loss: 0.4079639 Vali Loss: 1.4964781 Test Loss: 0.4636627
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.19722580909729
Epoch: 83, Steps: 59 | Train Loss: 0.4078796 Vali Loss: 1.4883449 Test Loss: 0.4635626
Validation loss decreased (1.490501 --> 1.488345).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.306814193725586
Epoch: 84, Steps: 59 | Train Loss: 0.4078279 Vali Loss: 1.4898672 Test Loss: 0.4634647
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.244060754776001
Epoch: 85, Steps: 59 | Train Loss: 0.4077374 Vali Loss: 1.4977436 Test Loss: 0.4633712
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.338038682937622
Epoch: 86, Steps: 59 | Train Loss: 0.4075759 Vali Loss: 1.4992779 Test Loss: 0.4632860
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.2703673839569092
Epoch: 87, Steps: 59 | Train Loss: 0.4077270 Vali Loss: 1.4930664 Test Loss: 0.4632001
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.3692197799682617
Epoch: 88, Steps: 59 | Train Loss: 0.4076848 Vali Loss: 1.4970417 Test Loss: 0.4631210
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.2769474983215332
Epoch: 89, Steps: 59 | Train Loss: 0.4074697 Vali Loss: 1.4945452 Test Loss: 0.4630459
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.2668097019195557
Epoch: 90, Steps: 59 | Train Loss: 0.4075370 Vali Loss: 1.4889917 Test Loss: 0.4629715
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.415856122970581
Epoch: 91, Steps: 59 | Train Loss: 0.4074905 Vali Loss: 1.4951189 Test Loss: 0.4629050
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.317664623260498
Epoch: 92, Steps: 59 | Train Loss: 0.4074586 Vali Loss: 1.4944757 Test Loss: 0.4628361
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.3029069900512695
Epoch: 93, Steps: 59 | Train Loss: 0.4073709 Vali Loss: 1.4926941 Test Loss: 0.4627783
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.3728053569793701
Epoch: 94, Steps: 59 | Train Loss: 0.4073086 Vali Loss: 1.4928919 Test Loss: 0.4627163
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.4045395851135254
Epoch: 95, Steps: 59 | Train Loss: 0.4073287 Vali Loss: 1.4933436 Test Loss: 0.4626597
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.3231847286224365
Epoch: 96, Steps: 59 | Train Loss: 0.4073747 Vali Loss: 1.4940028 Test Loss: 0.4626051
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.3398947715759277
Epoch: 97, Steps: 59 | Train Loss: 0.4073192 Vali Loss: 1.4873462 Test Loss: 0.4625559
Validation loss decreased (1.488345 --> 1.487346).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.2486929893493652
Epoch: 98, Steps: 59 | Train Loss: 0.4072865 Vali Loss: 1.4928544 Test Loss: 0.4625076
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.2941079139709473
Epoch: 99, Steps: 59 | Train Loss: 0.4072510 Vali Loss: 1.4905868 Test Loss: 0.4624607
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.258772611618042
Epoch: 100, Steps: 59 | Train Loss: 0.4072200 Vali Loss: 1.4937036 Test Loss: 0.4624171
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=106, out_features=318, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30202368.0
params:  34026.0
Trainable parameters:  34026
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.2562973499298096
Epoch: 1, Steps: 59 | Train Loss: 0.5860224 Vali Loss: 1.4646475 Test Loss: 0.4440536
Validation loss decreased (inf --> 1.464648).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3285763263702393
Epoch: 2, Steps: 59 | Train Loss: 0.5777876 Vali Loss: 1.4535360 Test Loss: 0.4346181
Validation loss decreased (1.464648 --> 1.453536).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.3864097595214844
Epoch: 3, Steps: 59 | Train Loss: 0.5732890 Vali Loss: 1.4412014 Test Loss: 0.4297530
Validation loss decreased (1.453536 --> 1.441201).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.257887840270996
Epoch: 4, Steps: 59 | Train Loss: 0.5706785 Vali Loss: 1.4358767 Test Loss: 0.4273799
Validation loss decreased (1.441201 --> 1.435877).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.4090113639831543
Epoch: 5, Steps: 59 | Train Loss: 0.5693673 Vali Loss: 1.4353855 Test Loss: 0.4264686
Validation loss decreased (1.435877 --> 1.435385).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.3300807476043701
Epoch: 6, Steps: 59 | Train Loss: 0.5687393 Vali Loss: 1.4331725 Test Loss: 0.4261460
Validation loss decreased (1.435385 --> 1.433172).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.3823142051696777
Epoch: 7, Steps: 59 | Train Loss: 0.5684243 Vali Loss: 1.4286903 Test Loss: 0.4260703
Validation loss decreased (1.433172 --> 1.428690).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.389063835144043
Epoch: 8, Steps: 59 | Train Loss: 0.5679918 Vali Loss: 1.4335968 Test Loss: 0.4262710
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3643548488616943
Epoch: 9, Steps: 59 | Train Loss: 0.5678124 Vali Loss: 1.4312143 Test Loss: 0.4263231
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.3981728553771973
Epoch: 10, Steps: 59 | Train Loss: 0.5678123 Vali Loss: 1.4328406 Test Loss: 0.4265824
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.415787935256958
Epoch: 11, Steps: 59 | Train Loss: 0.5677331 Vali Loss: 1.4255310 Test Loss: 0.4265651
Validation loss decreased (1.428690 --> 1.425531).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4409980773925781
Epoch: 12, Steps: 59 | Train Loss: 0.5674834 Vali Loss: 1.4303861 Test Loss: 0.4266199
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.3418006896972656
Epoch: 13, Steps: 59 | Train Loss: 0.5674118 Vali Loss: 1.4296972 Test Loss: 0.4267864
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.3978052139282227
Epoch: 14, Steps: 59 | Train Loss: 0.5674360 Vali Loss: 1.4288923 Test Loss: 0.4267587
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.3624918460845947
Epoch: 15, Steps: 59 | Train Loss: 0.5673471 Vali Loss: 1.4299607 Test Loss: 0.4268134
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.3909311294555664
Epoch: 16, Steps: 59 | Train Loss: 0.5672973 Vali Loss: 1.4238913 Test Loss: 0.4268868
Validation loss decreased (1.425531 --> 1.423891).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3973727226257324
Epoch: 17, Steps: 59 | Train Loss: 0.5674225 Vali Loss: 1.4238501 Test Loss: 0.4269913
Validation loss decreased (1.423891 --> 1.423850).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4446237087249756
Epoch: 18, Steps: 59 | Train Loss: 0.5673066 Vali Loss: 1.4251109 Test Loss: 0.4269660
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5541801452636719
Epoch: 19, Steps: 59 | Train Loss: 0.5672682 Vali Loss: 1.4211211 Test Loss: 0.4269651
Validation loss decreased (1.423850 --> 1.421121).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.4031662940979004
Epoch: 20, Steps: 59 | Train Loss: 0.5673277 Vali Loss: 1.4306166 Test Loss: 0.4269975
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.3471732139587402
Epoch: 21, Steps: 59 | Train Loss: 0.5671561 Vali Loss: 1.4279208 Test Loss: 0.4269715
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.3016421794891357
Epoch: 22, Steps: 59 | Train Loss: 0.5670481 Vali Loss: 1.4309385 Test Loss: 0.4269520
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3587937355041504
Epoch: 23, Steps: 59 | Train Loss: 0.5671118 Vali Loss: 1.4287856 Test Loss: 0.4270616
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.2793247699737549
Epoch: 24, Steps: 59 | Train Loss: 0.5670879 Vali Loss: 1.4300036 Test Loss: 0.4270126
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.416679859161377
Epoch: 25, Steps: 59 | Train Loss: 0.5670325 Vali Loss: 1.4270263 Test Loss: 0.4270609
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4859020709991455
Epoch: 26, Steps: 59 | Train Loss: 0.5670861 Vali Loss: 1.4321655 Test Loss: 0.4270923
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.245415449142456
Epoch: 27, Steps: 59 | Train Loss: 0.5671232 Vali Loss: 1.4323893 Test Loss: 0.4271114
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.3742210865020752
Epoch: 28, Steps: 59 | Train Loss: 0.5668402 Vali Loss: 1.4268140 Test Loss: 0.4271215
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.2146782875061035
Epoch: 29, Steps: 59 | Train Loss: 0.5669109 Vali Loss: 1.4291948 Test Loss: 0.4271051
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.262300968170166
Epoch: 30, Steps: 59 | Train Loss: 0.5667943 Vali Loss: 1.4273727 Test Loss: 0.4271050
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.3331644535064697
Epoch: 31, Steps: 59 | Train Loss: 0.5668276 Vali Loss: 1.4308709 Test Loss: 0.4271463
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.4620428085327148
Epoch: 32, Steps: 59 | Train Loss: 0.5669237 Vali Loss: 1.4297619 Test Loss: 0.4271708
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.2891032695770264
Epoch: 33, Steps: 59 | Train Loss: 0.5667712 Vali Loss: 1.4304596 Test Loss: 0.4271735
EarlyStopping counter: 14 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.2626564502716064
Epoch: 34, Steps: 59 | Train Loss: 0.5669541 Vali Loss: 1.4308093 Test Loss: 0.4272073
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.281080961227417
Epoch: 35, Steps: 59 | Train Loss: 0.5667903 Vali Loss: 1.4302946 Test Loss: 0.4272116
EarlyStopping counter: 16 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.464773416519165
Epoch: 36, Steps: 59 | Train Loss: 0.5667948 Vali Loss: 1.4330925 Test Loss: 0.4271891
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.2357685565948486
Epoch: 37, Steps: 59 | Train Loss: 0.5665982 Vali Loss: 1.4303930 Test Loss: 0.4271928
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.2829513549804688
Epoch: 38, Steps: 59 | Train Loss: 0.5665276 Vali Loss: 1.4302523 Test Loss: 0.4272429
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.38594388961792
Epoch: 39, Steps: 59 | Train Loss: 0.5668160 Vali Loss: 1.4241276 Test Loss: 0.4272403
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4259626269340515, mae:0.44634804129600525, rse:0.6247952580451965, corr:[0.22799398 0.23534346 0.2351954  0.23658983 0.23451126 0.23196106
 0.23218843 0.23284289 0.2319809  0.23201147 0.2322558  0.23170309
 0.23161617 0.23181692 0.23131204 0.23109832 0.23138553 0.2310225
 0.23055236 0.23060454 0.23051149 0.23018314 0.23082565 0.23191173
 0.23174052 0.23153119 0.23188432 0.2317595  0.231392   0.23152204
 0.23139206 0.23062351 0.23009603 0.23013966 0.23011637 0.22987142
 0.23002386 0.23013623 0.22992769 0.22986646 0.23011488 0.22994833
 0.22973908 0.22972427 0.22955799 0.22938034 0.2298065  0.23045315
 0.23054126 0.23024707 0.22985154 0.2294582  0.22896181 0.22811344
 0.22754663 0.22705491 0.22694385 0.22677535 0.22630818 0.22607757
 0.22586328 0.22558337 0.22531468 0.22544809 0.22575109 0.22593729
 0.2260303  0.22597858 0.22602473 0.22631174 0.22632864 0.22605838
 0.22562942 0.2251212  0.22442828 0.22395603 0.22390257 0.22387698
 0.22353746 0.22299477 0.222765   0.22262882 0.22223093 0.22192083
 0.22177826 0.22162424 0.22146821 0.22134966 0.22109815 0.22086088
 0.2207611  0.22059762 0.22025247 0.22020476 0.22078113 0.22200288
 0.2232149  0.22389907 0.22451578 0.22506082 0.2253019  0.22531874
 0.22517218 0.22512695 0.22503294 0.2247654  0.22429143 0.22385776
 0.22377364 0.22388639 0.22388524 0.223866   0.22402456 0.22415869
 0.22410674 0.22391483 0.22382511 0.22377186 0.2236853  0.22374415
 0.22378384 0.22351547 0.22321239 0.22304456 0.22260152 0.22192441
 0.22160818 0.22162928 0.22138047 0.2209276  0.22070922 0.22056147
 0.22023824 0.21998918 0.21990731 0.21991009 0.22012915 0.22030288
 0.22031291 0.22021985 0.22017787 0.2201177  0.21981639 0.21976152
 0.21996076 0.2197763  0.21933031 0.2187225  0.21801697 0.21746053
 0.21739918 0.21763456 0.21770446 0.21767102 0.217543   0.21737032
 0.21716237 0.2170309  0.21701312 0.2170816  0.21718152 0.21716078
 0.2169203  0.21662726 0.21645096 0.21637522 0.21626644 0.21660654
 0.21715313 0.21772571 0.21855089 0.21934713 0.21938406 0.2191252
 0.2192596  0.21950892 0.21952452 0.21943082 0.21934539 0.21914996
 0.21894795 0.21887241 0.21884762 0.21875101 0.2189277  0.21930817
 0.21936335 0.21929845 0.21949506 0.21948369 0.21921052 0.2192402
 0.21925658 0.2188699  0.2182622  0.21773422 0.21705276 0.21639654
 0.21628435 0.21648313 0.2161856  0.21574527 0.21570209 0.21584825
 0.21579531 0.21590145 0.21615432 0.21630202 0.21619828 0.2158478
 0.21556644 0.21551996 0.21560301 0.2155253  0.21517156 0.21517362
 0.21533132 0.21521157 0.21524091 0.21516447 0.21486291 0.21470033
 0.2146963  0.21469975 0.21456727 0.21428983 0.21391039 0.21357208
 0.21320924 0.21298167 0.21292223 0.21288392 0.21283886 0.21264696
 0.2124136  0.21221973 0.21206485 0.21185215 0.211825   0.21207963
 0.21233763 0.21215    0.2121655  0.21264024 0.2130453  0.21303481
 0.2129726  0.21315564 0.21335754 0.21307942 0.21253133 0.21221894
 0.21222094 0.21224253 0.21232708 0.21245605 0.21261851 0.21256474
 0.21242076 0.21232939 0.2122696  0.212244   0.21237805 0.2126174
 0.21228756 0.21170284 0.21162438 0.21174474 0.21144192 0.21113826
 0.21107727 0.21099819 0.21082002 0.2108591  0.21086751 0.21063404
 0.2103925  0.21042429 0.2104491  0.21022902 0.20982984 0.20962116
 0.2097014  0.2098232  0.20989339 0.21001028 0.21029139 0.21087375
 0.211402   0.2117427  0.21220699 0.21266305 0.21294448 0.21301489
 0.213031   0.21318606 0.21325944 0.21302266 0.21272738 0.21288744
 0.21312945 0.21289569 0.21260007 0.21258532 0.2127512  0.21271981
 0.21279496 0.21298218 0.21295254 0.2127582  0.21275552 0.2132608
 0.21377756 0.21405874 0.21441318 0.21478805 0.21475488 0.21442142
 0.2141188  0.21411626 0.21412341 0.21371382 0.21300222 0.21272169
 0.21271402 0.21254253 0.21237963 0.21265107 0.21288115 0.21286592
 0.2131584  0.21363254 0.21354121 0.21302846 0.21288225 0.2131109
 0.21320577 0.21317075 0.21322554 0.21296756 0.21237756 0.21182561
 0.21171437 0.21182239 0.21175388 0.2116001  0.21161367 0.21169911
 0.2116065  0.2114123  0.21145572 0.21189001 0.21212223 0.21207362
 0.21207967 0.21225397 0.21220693 0.21181332 0.21115214 0.21097583
 0.21116681 0.21126038 0.21124753 0.21106593 0.21069726 0.21026258
 0.21009797 0.20998125 0.2096706  0.20930801 0.20906879 0.2089325
 0.20872529 0.20830901 0.20799214 0.20781282 0.20769088 0.2075064
 0.20730175 0.20746103 0.20788997 0.20807888 0.20800047 0.20869993
 0.2098549  0.21063533 0.21126482 0.21159805 0.21141966 0.21080984
 0.21045062 0.2101613  0.20979877 0.209426   0.20932849 0.20950879
 0.20937915 0.20904073 0.20897049 0.20901819 0.20902266 0.20929904
 0.2096586  0.2099274  0.21018267 0.21068613 0.21099676 0.21117122
 0.21135466 0.21151704 0.2114207  0.21119349 0.21107697 0.21091175
 0.21070859 0.21074514 0.21061343 0.21015348 0.20985463 0.21003583
 0.21029872 0.21027896 0.20999902 0.20968358 0.20943241 0.20945811
 0.20974779 0.21010119 0.21015565 0.2104421  0.21094301 0.21167527
 0.21238957 0.21275197 0.21278839 0.21278647 0.212722   0.21230249
 0.21177612 0.21177739 0.21182811 0.2115931  0.21140476 0.21149816
 0.21153691 0.21124972 0.21114202 0.21131454 0.21138072 0.21130767
 0.21136142 0.21154113 0.21175265 0.2119507  0.21207258 0.21263772
 0.21342237 0.21397266 0.21418847 0.21420057 0.21393895 0.21366793
 0.21329485 0.21284164 0.21258861 0.21265066 0.21279745 0.2128812
 0.21300419 0.21306156 0.21301952 0.21297055 0.21302952 0.21316591
 0.21327305 0.21335162 0.2134514  0.21327417 0.21303654 0.21303874
 0.21328445 0.21345167 0.21326354 0.21290883 0.21262935 0.21259202
 0.21266998 0.21252179 0.21220033 0.2120506  0.21186319 0.21171051
 0.21155079 0.21126816 0.211152   0.21134964 0.21150652 0.21144834
 0.21136121 0.21135095 0.21147259 0.21174502 0.21228082 0.2133103
 0.21431625 0.21496357 0.2154469  0.21576868 0.21571155 0.21530956
 0.21481128 0.21435167 0.2138981  0.2134004  0.21308698 0.21313381
 0.21315974 0.21294643 0.21302515 0.2135048  0.21397953 0.2141628
 0.21429202 0.21451618 0.2146092  0.21464893 0.21506312 0.21562822
 0.21569388 0.2154053  0.21536297 0.21536814 0.21483825 0.21419907
 0.21381566 0.21360193 0.21331167 0.2131131  0.21283609 0.21269393
 0.21271287 0.21297982 0.2131863  0.21342301 0.21379724 0.21405256
 0.2139911  0.2138946  0.2139174  0.21393993 0.21370025 0.21358137
 0.21348691 0.21324787 0.21305582 0.21261448 0.2118211  0.21122451
 0.21077389 0.21031886 0.20997341 0.20948988 0.20893805 0.20858063
 0.20840997 0.20813447 0.2080261  0.20799167 0.2079819  0.208048
 0.20834352 0.20850402 0.20843805 0.20828983 0.20837991 0.20861019
 0.20863172 0.20851932 0.20827045 0.20811628 0.2078902  0.20739327
 0.20692901 0.2064413  0.20613554 0.20595439 0.20575435 0.20548277
 0.20539127 0.20519987 0.20519102 0.20515192 0.20501463 0.20491901
 0.20501468 0.20515956 0.20522083 0.20515959 0.20504706 0.20495266
 0.20479968 0.20456396 0.20446822 0.20394313 0.2028691  0.20209567
 0.20201291 0.20193574 0.20130467 0.20078567 0.20061795 0.20046972
 0.20025325 0.19987825 0.19962215 0.19975223 0.19984974 0.19968137
 0.19961269 0.19980983 0.20003442 0.20015764 0.2004454  0.2009384
 0.20092256 0.20047453 0.20050085 0.20033161 0.19951852 0.19888932
 0.1988757  0.19855158 0.19779672 0.19751613 0.19741137 0.19713123
 0.19700514 0.1969697  0.19691773 0.19672765 0.19668269 0.19686292
 0.19673023 0.19660555 0.19684835 0.19705783 0.1973343  0.19808806
 0.19863412 0.19845058 0.1979266  0.19778036 0.19731039 0.19699802
 0.19680558 0.19638854 0.19563948 0.19524142 0.1953307  0.19513607
 0.19474547 0.19463594 0.19460449 0.19420095 0.1942085  0.1945346
 0.1947876  0.19483754 0.19496183 0.19494526 0.19479261 0.19492006
 0.19473486 0.19387865 0.1930074  0.1922793  0.19158769 0.1907057
 0.18992269 0.1894052  0.18912917 0.18892594 0.1887655  0.1887135
 0.18903817 0.18903324 0.18853389 0.18831007 0.18864022 0.1891785
 0.18924211 0.18931542 0.18980204 0.19017099 0.19019674 0.19035792
 0.19060846 0.19035666 0.1901803  0.19027548 0.1899329  0.18900183
 0.18851914 0.18854685 0.18845417 0.18794021 0.18731017 0.1869005
 0.18685211 0.18658039 0.18629421 0.18570863 0.18587585 0.18588994
 0.18549587 0.18557298 0.18627925 0.1847903  0.1844165  0.18485914]
