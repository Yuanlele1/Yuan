Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=74, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14719488.0
params:  16650.0
Trainable parameters:  16650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.243696689605713
Epoch: 1, Steps: 59 | Train Loss: 0.9881937 Vali Loss: 2.0318120 Test Loss: 0.8317787
Validation loss decreased (inf --> 2.031812).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.4398319721221924
Epoch: 2, Steps: 59 | Train Loss: 0.7698972 Vali Loss: 1.7731456 Test Loss: 0.6510254
Validation loss decreased (2.031812 --> 1.773146).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.4589829444885254
Epoch: 3, Steps: 59 | Train Loss: 0.6921316 Vali Loss: 1.6706722 Test Loss: 0.5801046
Validation loss decreased (1.773146 --> 1.670672).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.3133370876312256
Epoch: 4, Steps: 59 | Train Loss: 0.6586897 Vali Loss: 1.6218591 Test Loss: 0.5430496
Validation loss decreased (1.670672 --> 1.621859).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.645576238632202
Epoch: 5, Steps: 59 | Train Loss: 0.6400807 Vali Loss: 1.5893767 Test Loss: 0.5186846
Validation loss decreased (1.621859 --> 1.589377).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.9122097492218018
Epoch: 6, Steps: 59 | Train Loss: 0.6269318 Vali Loss: 1.5605850 Test Loss: 0.5004914
Validation loss decreased (1.589377 --> 1.560585).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.213317394256592
Epoch: 7, Steps: 59 | Train Loss: 0.6173490 Vali Loss: 1.5422423 Test Loss: 0.4863792
Validation loss decreased (1.560585 --> 1.542242).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.579524040222168
Epoch: 8, Steps: 59 | Train Loss: 0.6096174 Vali Loss: 1.5338554 Test Loss: 0.4751469
Validation loss decreased (1.542242 --> 1.533855).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.302647352218628
Epoch: 9, Steps: 59 | Train Loss: 0.6032693 Vali Loss: 1.5125308 Test Loss: 0.4661355
Validation loss decreased (1.533855 --> 1.512531).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.8126895427703857
Epoch: 10, Steps: 59 | Train Loss: 0.5984536 Vali Loss: 1.5010570 Test Loss: 0.4589813
Validation loss decreased (1.512531 --> 1.501057).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.342092275619507
Epoch: 11, Steps: 59 | Train Loss: 0.5940821 Vali Loss: 1.4948627 Test Loss: 0.4531139
Validation loss decreased (1.501057 --> 1.494863).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.130375862121582
Epoch: 12, Steps: 59 | Train Loss: 0.5911132 Vali Loss: 1.4906420 Test Loss: 0.4484125
Validation loss decreased (1.494863 --> 1.490642).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.865025520324707
Epoch: 13, Steps: 59 | Train Loss: 0.5882378 Vali Loss: 1.4778764 Test Loss: 0.4447064
Validation loss decreased (1.490642 --> 1.477876).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.966742753982544
Epoch: 14, Steps: 59 | Train Loss: 0.5858072 Vali Loss: 1.4739513 Test Loss: 0.4416278
Validation loss decreased (1.477876 --> 1.473951).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.187004327774048
Epoch: 15, Steps: 59 | Train Loss: 0.5839546 Vali Loss: 1.4746664 Test Loss: 0.4392015
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.8171288967132568
Epoch: 16, Steps: 59 | Train Loss: 0.5824801 Vali Loss: 1.4648882 Test Loss: 0.4372152
Validation loss decreased (1.473951 --> 1.464888).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.115326166152954
Epoch: 17, Steps: 59 | Train Loss: 0.5810711 Vali Loss: 1.4610574 Test Loss: 0.4355887
Validation loss decreased (1.464888 --> 1.461057).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.4822959899902344
Epoch: 18, Steps: 59 | Train Loss: 0.5801142 Vali Loss: 1.4607556 Test Loss: 0.4342822
Validation loss decreased (1.461057 --> 1.460756).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.4571924209594727
Epoch: 19, Steps: 59 | Train Loss: 0.5791891 Vali Loss: 1.4544383 Test Loss: 0.4332367
Validation loss decreased (1.460756 --> 1.454438).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.0305874347686768
Epoch: 20, Steps: 59 | Train Loss: 0.5782996 Vali Loss: 1.4615564 Test Loss: 0.4324514
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.919574499130249
Epoch: 21, Steps: 59 | Train Loss: 0.5776983 Vali Loss: 1.4536352 Test Loss: 0.4317062
Validation loss decreased (1.454438 --> 1.453635).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.4393157958984375
Epoch: 22, Steps: 59 | Train Loss: 0.5772046 Vali Loss: 1.4537740 Test Loss: 0.4311872
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.3897006511688232
Epoch: 23, Steps: 59 | Train Loss: 0.5767948 Vali Loss: 1.4534159 Test Loss: 0.4307710
Validation loss decreased (1.453635 --> 1.453416).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.075326919555664
Epoch: 24, Steps: 59 | Train Loss: 0.5762075 Vali Loss: 1.4555298 Test Loss: 0.4304118
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8440423011779785
Epoch: 25, Steps: 59 | Train Loss: 0.5760062 Vali Loss: 1.4461668 Test Loss: 0.4301159
Validation loss decreased (1.453416 --> 1.446167).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.629351854324341
Epoch: 26, Steps: 59 | Train Loss: 0.5756420 Vali Loss: 1.4535022 Test Loss: 0.4298979
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.162890672683716
Epoch: 27, Steps: 59 | Train Loss: 0.5752553 Vali Loss: 1.4516002 Test Loss: 0.4297693
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 5.414048194885254
Epoch: 28, Steps: 59 | Train Loss: 0.5751003 Vali Loss: 1.4483432 Test Loss: 0.4295997
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.411989212036133
Epoch: 29, Steps: 59 | Train Loss: 0.5749089 Vali Loss: 1.4491227 Test Loss: 0.4295067
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.96905517578125
Epoch: 30, Steps: 59 | Train Loss: 0.5745417 Vali Loss: 1.4458929 Test Loss: 0.4294242
Validation loss decreased (1.446167 --> 1.445893).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.96510910987854
Epoch: 31, Steps: 59 | Train Loss: 0.5744125 Vali Loss: 1.4397414 Test Loss: 0.4293544
Validation loss decreased (1.445893 --> 1.439741).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.336979627609253
Epoch: 32, Steps: 59 | Train Loss: 0.5742505 Vali Loss: 1.4451563 Test Loss: 0.4293230
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.6051011085510254
Epoch: 33, Steps: 59 | Train Loss: 0.5742558 Vali Loss: 1.4456708 Test Loss: 0.4292783
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.1799259185791016
Epoch: 34, Steps: 59 | Train Loss: 0.5740727 Vali Loss: 1.4440022 Test Loss: 0.4292918
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9506990909576416
Epoch: 35, Steps: 59 | Train Loss: 0.5740844 Vali Loss: 1.4422270 Test Loss: 0.4292701
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.9641904830932617
Epoch: 36, Steps: 59 | Train Loss: 0.5738829 Vali Loss: 1.4414773 Test Loss: 0.4292617
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8947253227233887
Epoch: 37, Steps: 59 | Train Loss: 0.5738910 Vali Loss: 1.4390523 Test Loss: 0.4292672
Validation loss decreased (1.439741 --> 1.439052).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7948570251464844
Epoch: 38, Steps: 59 | Train Loss: 0.5737072 Vali Loss: 1.4427495 Test Loss: 0.4292667
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.9192712306976318
Epoch: 39, Steps: 59 | Train Loss: 0.5738701 Vali Loss: 1.4405816 Test Loss: 0.4292679
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.057856559753418
Epoch: 40, Steps: 59 | Train Loss: 0.5737682 Vali Loss: 1.4415268 Test Loss: 0.4293054
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.260082483291626
Epoch: 41, Steps: 59 | Train Loss: 0.5734152 Vali Loss: 1.4388759 Test Loss: 0.4293278
Validation loss decreased (1.439052 --> 1.438876).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.617645502090454
Epoch: 42, Steps: 59 | Train Loss: 0.5735842 Vali Loss: 1.4428184 Test Loss: 0.4293384
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.7002696990966797
Epoch: 43, Steps: 59 | Train Loss: 0.5736371 Vali Loss: 1.4438531 Test Loss: 0.4293559
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.382951259613037
Epoch: 44, Steps: 59 | Train Loss: 0.5736394 Vali Loss: 1.4398416 Test Loss: 0.4293686
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.314652919769287
Epoch: 45, Steps: 59 | Train Loss: 0.5733878 Vali Loss: 1.4363576 Test Loss: 0.4293662
Validation loss decreased (1.438876 --> 1.436358).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.556384563446045
Epoch: 46, Steps: 59 | Train Loss: 0.5732630 Vali Loss: 1.4396158 Test Loss: 0.4294022
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.094154119491577
Epoch: 47, Steps: 59 | Train Loss: 0.5734654 Vali Loss: 1.4421358 Test Loss: 0.4294107
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9568548202514648
Epoch: 48, Steps: 59 | Train Loss: 0.5733201 Vali Loss: 1.4370894 Test Loss: 0.4294138
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.4295895099639893
Epoch: 49, Steps: 59 | Train Loss: 0.5733217 Vali Loss: 1.4463661 Test Loss: 0.4294275
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.183303117752075
Epoch: 50, Steps: 59 | Train Loss: 0.5730029 Vali Loss: 1.4407907 Test Loss: 0.4294617
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.8656408786773682
Epoch: 51, Steps: 59 | Train Loss: 0.5732050 Vali Loss: 1.4423647 Test Loss: 0.4294788
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.4056930541992188
Epoch: 52, Steps: 59 | Train Loss: 0.5733508 Vali Loss: 1.4355925 Test Loss: 0.4294951
Validation loss decreased (1.436358 --> 1.435593).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.829840898513794
Epoch: 53, Steps: 59 | Train Loss: 0.5733745 Vali Loss: 1.4365495 Test Loss: 0.4295111
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.1033935546875
Epoch: 54, Steps: 59 | Train Loss: 0.5732676 Vali Loss: 1.4352865 Test Loss: 0.4295229
Validation loss decreased (1.435593 --> 1.435287).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.197807788848877
Epoch: 55, Steps: 59 | Train Loss: 0.5730607 Vali Loss: 1.4457107 Test Loss: 0.4295498
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9316034317016602
Epoch: 56, Steps: 59 | Train Loss: 0.5732057 Vali Loss: 1.4426037 Test Loss: 0.4295579
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.872039556503296
Epoch: 57, Steps: 59 | Train Loss: 0.5730411 Vali Loss: 1.4422058 Test Loss: 0.4295788
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.810490608215332
Epoch: 58, Steps: 59 | Train Loss: 0.5729451 Vali Loss: 1.4398872 Test Loss: 0.4295838
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.064650774002075
Epoch: 59, Steps: 59 | Train Loss: 0.5730575 Vali Loss: 1.4423616 Test Loss: 0.4295912
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9649786949157715
Epoch: 60, Steps: 59 | Train Loss: 0.5730259 Vali Loss: 1.4392008 Test Loss: 0.4295945
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.457698345184326
Epoch: 61, Steps: 59 | Train Loss: 0.5729315 Vali Loss: 1.4421637 Test Loss: 0.4296182
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.057249069213867
Epoch: 62, Steps: 59 | Train Loss: 0.5728953 Vali Loss: 1.4424380 Test Loss: 0.4296328
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.5205633640289307
Epoch: 63, Steps: 59 | Train Loss: 0.5731919 Vali Loss: 1.4476026 Test Loss: 0.4296409
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.04535174369812
Epoch: 64, Steps: 59 | Train Loss: 0.5731248 Vali Loss: 1.4438772 Test Loss: 0.4296521
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.424473285675049
Epoch: 65, Steps: 59 | Train Loss: 0.5731582 Vali Loss: 1.4387586 Test Loss: 0.4296603
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.5068655014038086
Epoch: 66, Steps: 59 | Train Loss: 0.5729504 Vali Loss: 1.4477801 Test Loss: 0.4296743
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.3410356044769287
Epoch: 67, Steps: 59 | Train Loss: 0.5729510 Vali Loss: 1.4397421 Test Loss: 0.4296816
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.9557793140411377
Epoch: 68, Steps: 59 | Train Loss: 0.5728187 Vali Loss: 1.4446421 Test Loss: 0.4296877
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.093240737915039
Epoch: 69, Steps: 59 | Train Loss: 0.5729926 Vali Loss: 1.4430635 Test Loss: 0.4296934
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.6847691535949707
Epoch: 70, Steps: 59 | Train Loss: 0.5729594 Vali Loss: 1.4413123 Test Loss: 0.4297020
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.70839524269104
Epoch: 71, Steps: 59 | Train Loss: 0.5729458 Vali Loss: 1.4401126 Test Loss: 0.4297039
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.6826601028442383
Epoch: 72, Steps: 59 | Train Loss: 0.5729429 Vali Loss: 1.4412100 Test Loss: 0.4297196
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.2710208892822266
Epoch: 73, Steps: 59 | Train Loss: 0.5729893 Vali Loss: 1.4421639 Test Loss: 0.4297257
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.5033230781555176
Epoch: 74, Steps: 59 | Train Loss: 0.5728781 Vali Loss: 1.4394925 Test Loss: 0.4297286
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.42848336696624756, mae:0.44906798005104065, rse:0.6266412734985352, corr:[0.21945125 0.2355347  0.23565419 0.23254429 0.23187429 0.23193568
 0.23113021 0.23028524 0.23004879 0.23040602 0.23026627 0.22937723
 0.22869386 0.22880118 0.22915895 0.22910754 0.22859345 0.22837819
 0.22883213 0.22947663 0.22975591 0.22973949 0.2298953  0.23032792
 0.23049073 0.23027544 0.23009308 0.23009337 0.2300969  0.22984293
 0.22934753 0.22894561 0.2288101  0.22877607 0.22865194 0.22843838
 0.22835691 0.22845967 0.22857152 0.2285433  0.22847055 0.22843878
 0.22848608 0.22849652 0.228298   0.22807328 0.22810075 0.22827353
 0.22818254 0.22774045 0.22717932 0.22684358 0.22664098 0.22618482
 0.22561178 0.22497426 0.22477464 0.22458485 0.22421327 0.22384891
 0.22359644 0.22350198 0.2234703  0.22354482 0.22362854 0.22376907
 0.22392976 0.22395733 0.22393663 0.22390178 0.2237559  0.22348367
 0.22287999 0.22219488 0.22178803 0.22171955 0.22173749 0.22161275
 0.22131811 0.22101833 0.22080109 0.22057997 0.22025664 0.21998471
 0.21987031 0.21985286 0.21975353 0.21952112 0.21922885 0.21896672
 0.21875006 0.218681   0.21876706 0.21902601 0.21955006 0.22045882
 0.22145285 0.2220925  0.22252506 0.22291599 0.22319409 0.22324747
 0.22305281 0.22289754 0.22281782 0.22266696 0.22236586 0.2220162
 0.22181171 0.2218558  0.22195688 0.22199298 0.2218625  0.221693
 0.22163469 0.22156784 0.22141689 0.2211203  0.22086254 0.22088991
 0.22094074 0.22067432 0.22015505 0.21986638 0.21974987 0.21958299
 0.21924421 0.21889272 0.21859731 0.21836284 0.21811686 0.21782537
 0.21760724 0.21755269 0.21766603 0.21779998 0.21776514 0.21761037
 0.21751918 0.21757388 0.21767014 0.2175851  0.21721555 0.21686506
 0.21665917 0.21637754 0.2159769  0.21550977 0.215118   0.21484037
 0.21467942 0.21465257 0.21472238 0.21484493 0.21487789 0.21482772
 0.21472055 0.21466534 0.21467532 0.2147244  0.21461381 0.21446885
 0.21430781 0.21416529 0.21405186 0.21383418 0.21351667 0.21358716
 0.21413784 0.21494602 0.21559907 0.21609466 0.21642242 0.21660385
 0.21660212 0.21656223 0.21659276 0.21664245 0.21655759 0.21634547
 0.21614197 0.21608321 0.21618488 0.2162723  0.2163093  0.2163409
 0.21635896 0.21636632 0.21639112 0.21623448 0.21595058 0.21569818
 0.21550508 0.2152431  0.21481195 0.21438907 0.21400875 0.21366793
 0.21339327 0.21332091 0.21332403 0.21328321 0.21316573 0.21304248
 0.21299341 0.2131161  0.21331984 0.21345204 0.21343249 0.2132883
 0.21306257 0.21275266 0.21242537 0.21207568 0.21178192 0.21181387
 0.21202718 0.21205167 0.2119475  0.21186076 0.21191056 0.21200821
 0.2119531  0.21178854 0.2116079  0.2114248  0.21118195 0.21090378
 0.21059847 0.21034501 0.2102953  0.2103718  0.21033747 0.21011798
 0.2098204  0.209478   0.20920394 0.20902157 0.20899592 0.20908527
 0.20937014 0.20956193 0.20964009 0.20979322 0.21005392 0.21028858
 0.21047667 0.2105592  0.21053597 0.21038236 0.21022342 0.21008207
 0.20998098 0.20987797 0.20998232 0.21019147 0.2104083  0.21042937
 0.21023622 0.20992252 0.20988834 0.2099948  0.20994563 0.20973308
 0.20951915 0.20936929 0.2092265  0.20908833 0.20895258 0.20893414
 0.20899704 0.20899735 0.20880808 0.20861703 0.20854947 0.20865265
 0.20872259 0.208609   0.20830576 0.20803104 0.20783915 0.20771717
 0.20753008 0.20734994 0.20737661 0.20756724 0.20794585 0.20850413
 0.20918207 0.20978336 0.21020769 0.21036564 0.21042413 0.21058221
 0.2108256  0.2109982  0.21094523 0.21080169 0.21070641 0.21078014
 0.21085018 0.21075052 0.2106812  0.21074753 0.21084172 0.2108598
 0.21076892 0.2105565  0.21041027 0.21040858 0.21048318 0.21077158
 0.21126826 0.21177647 0.21205795 0.21217798 0.21210763 0.21195647
 0.21181442 0.211732   0.21151586 0.21115142 0.21078233 0.2106435
 0.21065557 0.21065536 0.21056634 0.210687   0.21093655 0.21119528
 0.21128424 0.21118161 0.21109857 0.21109223 0.2111138  0.21105453
 0.21098739 0.21096267 0.21097156 0.21083494 0.21053797 0.21012007
 0.20985883 0.20982167 0.20976228 0.20957962 0.20947896 0.20964555
 0.2099357  0.21015584 0.2101401  0.21004313 0.20998384 0.21020675
 0.21040954 0.21022125 0.20972791 0.20934428 0.20909712 0.209139
 0.20930755 0.20934875 0.20924173 0.2090575  0.20876436 0.20829651
 0.20797831 0.20789264 0.20781185 0.20756476 0.20717877 0.20691548
 0.20686682 0.20680332 0.20653524 0.20606014 0.20572908 0.20580393
 0.20610628 0.20620616 0.20598769 0.20576626 0.20588763 0.2068168
 0.20815837 0.20915185 0.20968445 0.20981868 0.20972842 0.20927703
 0.2087795  0.20829418 0.20791921 0.20761456 0.20727505 0.2070695
 0.20710996 0.20726973 0.20739001 0.2073598  0.20731512 0.2075478
 0.20794372 0.20825481 0.2084006  0.20858134 0.20887144 0.20944518
 0.20996076 0.21015467 0.20999654 0.20975931 0.20959318 0.20937295
 0.20910867 0.2089949  0.20892416 0.20880184 0.20865816 0.20855756
 0.20858866 0.20873468 0.20863964 0.20825426 0.20780209 0.20766973
 0.20788127 0.2082542  0.2083995  0.20873414 0.20926158 0.2100408
 0.21075448 0.21099176 0.21075189 0.2105525  0.21059816 0.21067142
 0.21050456 0.21032314 0.21010073 0.20995787 0.20989145 0.20979401
 0.20968631 0.20957553 0.20950368 0.20941131 0.20926635 0.20918535
 0.209251   0.20937149 0.20951456 0.20975415 0.21017803 0.2109959
 0.21180923 0.21219744 0.21210872 0.21197857 0.21179491 0.21145456
 0.21103628 0.21078242 0.21074113 0.21085653 0.21093383 0.21086626
 0.21078399 0.21081771 0.21096285 0.2110587  0.2110144  0.21096455
 0.2109788  0.21103063 0.21100657 0.21080494 0.21081926 0.21111907
 0.21136917 0.21130776 0.21097477 0.21079311 0.21076803 0.21066318
 0.21046233 0.21022253 0.20996396 0.2098232  0.20964755 0.2093926
 0.20910448 0.20892978 0.20897336 0.2090918  0.20907621 0.20887774
 0.20869902 0.20871115 0.20897767 0.20942356 0.21005762 0.21091533
 0.21183284 0.21261291 0.21305472 0.21319559 0.21302831 0.21256816
 0.2120168  0.21156476 0.21117629 0.21080324 0.21051504 0.21039079
 0.21035115 0.21034801 0.21057028 0.21097682 0.21132554 0.21142785
 0.21132892 0.21139438 0.21183424 0.21244536 0.21294114 0.21305344
 0.21290754 0.21277988 0.21277554 0.21278821 0.21243282 0.21170297
 0.21107449 0.21085666 0.21071926 0.21055943 0.21034977 0.21027024
 0.21021612 0.21021341 0.21034679 0.2106692  0.2110454  0.21125309
 0.21114367 0.21097834 0.21090794 0.21093327 0.21083957 0.21065563
 0.2103787  0.21015264 0.20999125 0.20971614 0.20911022 0.20842817
 0.20784912 0.20741603 0.20707673 0.20661937 0.2061531  0.20585285
 0.2057597  0.20550138 0.20518629 0.20497753 0.20513631 0.20543332
 0.20557275 0.20543857 0.205374   0.20549005 0.20576935 0.20582385
 0.20562464 0.20550069 0.2053322  0.20526929 0.20513235 0.20447707
 0.20376147 0.20329878 0.20314054 0.20307551 0.20298462 0.2027767
 0.20267734 0.20248134 0.20233579 0.20218948 0.20213333 0.20222782
 0.20234403 0.2022905  0.20219558 0.2021723  0.20222986 0.20227459
 0.20207393 0.20152491 0.200934   0.20054258 0.20017198 0.19953853
 0.19879535 0.19828786 0.19800033 0.19786586 0.19766712 0.19737607
 0.19720781 0.19704461 0.19680582 0.19666599 0.19660817 0.19665462
 0.19676314 0.19678885 0.19679351 0.19700027 0.19742715 0.19793642
 0.19805276 0.19747822 0.19688205 0.19662277 0.19647539 0.19605763
 0.19546121 0.19488356 0.19438706 0.19414529 0.19399248 0.19392748
 0.19402084 0.19393253 0.19375204 0.19351757 0.19334903 0.19352
 0.19372097 0.19376571 0.1937367  0.19390222 0.19448678 0.19519837
 0.19550475 0.19529618 0.1947372  0.1946408  0.19444256 0.19404687
 0.19344486 0.19283873 0.19236445 0.19208802 0.19201306 0.19193467
 0.19178994 0.19154835 0.19131166 0.19104747 0.19114286 0.19141671
 0.1916171  0.19161174 0.19158721 0.19163574 0.19174606 0.19192846
 0.19169164 0.19081755 0.18972132 0.18878368 0.18821485 0.18760188
 0.1868652  0.18619381 0.1857625  0.18556187 0.1857208  0.18600461
 0.18619527 0.18595186 0.18563278 0.18566652 0.18588272 0.18633723
 0.18675116 0.18695341 0.18701242 0.18721378 0.18759616 0.18797898
 0.18826541 0.1879836  0.18728079 0.18680163 0.186601   0.18629506
 0.18593815 0.1854838  0.18505414 0.18473765 0.1846245  0.18458577
 0.18452285 0.18405657 0.1836741  0.1831198  0.18321249 0.18396497
 0.18474609 0.18394867 0.18185268 0.18118723 0.18357652 0.1760645 ]
