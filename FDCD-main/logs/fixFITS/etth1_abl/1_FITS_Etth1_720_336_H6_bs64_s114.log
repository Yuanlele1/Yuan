Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=196, out_features=287, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  50401792.0
params:  56539.0
Trainable parameters:  56539
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.393334150314331
Epoch: 1, Steps: 59 | Train Loss: 0.7460661 Vali Loss: 1.4660884 Test Loss: 0.6399646
Validation loss decreased (inf --> 1.466088).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.3088080883026123
Epoch: 2, Steps: 59 | Train Loss: 0.5799197 Vali Loss: 1.3280987 Test Loss: 0.5459132
Validation loss decreased (1.466088 --> 1.328099).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.9170317649841309
Epoch: 3, Steps: 59 | Train Loss: 0.5266714 Vali Loss: 1.2706199 Test Loss: 0.4994324
Validation loss decreased (1.328099 --> 1.270620).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.8569574356079102
Epoch: 4, Steps: 59 | Train Loss: 0.4973493 Vali Loss: 1.2314873 Test Loss: 0.4723166
Validation loss decreased (1.270620 --> 1.231487).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.0409717559814453
Epoch: 5, Steps: 59 | Train Loss: 0.4795559 Vali Loss: 1.2109989 Test Loss: 0.4559413
Validation loss decreased (1.231487 --> 1.210999).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.710195779800415
Epoch: 6, Steps: 59 | Train Loss: 0.4679473 Vali Loss: 1.1958604 Test Loss: 0.4463244
Validation loss decreased (1.210999 --> 1.195860).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.018115997314453
Epoch: 7, Steps: 59 | Train Loss: 0.4601214 Vali Loss: 1.1835699 Test Loss: 0.4410014
Validation loss decreased (1.195860 --> 1.183570).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.9217734336853027
Epoch: 8, Steps: 59 | Train Loss: 0.4549546 Vali Loss: 1.1817029 Test Loss: 0.4378861
Validation loss decreased (1.183570 --> 1.181703).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.174058675765991
Epoch: 9, Steps: 59 | Train Loss: 0.4506532 Vali Loss: 1.1837502 Test Loss: 0.4362238
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.64005446434021
Epoch: 10, Steps: 59 | Train Loss: 0.4483190 Vali Loss: 1.1818227 Test Loss: 0.4357681
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.073540449142456
Epoch: 11, Steps: 59 | Train Loss: 0.4466168 Vali Loss: 1.1811122 Test Loss: 0.4356518
Validation loss decreased (1.181703 --> 1.181112).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.8530049324035645
Epoch: 12, Steps: 59 | Train Loss: 0.4451970 Vali Loss: 1.1880554 Test Loss: 0.4357254
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.3619890213012695
Epoch: 13, Steps: 59 | Train Loss: 0.4437039 Vali Loss: 1.1863438 Test Loss: 0.4358904
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.9831242561340332
Epoch: 14, Steps: 59 | Train Loss: 0.4429679 Vali Loss: 1.1868228 Test Loss: 0.4361485
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.6759297847747803
Epoch: 15, Steps: 59 | Train Loss: 0.4420880 Vali Loss: 1.1862656 Test Loss: 0.4362741
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.042250156402588
Epoch: 16, Steps: 59 | Train Loss: 0.4412723 Vali Loss: 1.1853867 Test Loss: 0.4367154
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.9972875118255615
Epoch: 17, Steps: 59 | Train Loss: 0.4408728 Vali Loss: 1.1926906 Test Loss: 0.4370151
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.9041683673858643
Epoch: 18, Steps: 59 | Train Loss: 0.4400714 Vali Loss: 1.1954669 Test Loss: 0.4372552
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.485457181930542
Epoch: 19, Steps: 59 | Train Loss: 0.4395046 Vali Loss: 1.1938031 Test Loss: 0.4374538
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9022338390350342
Epoch: 20, Steps: 59 | Train Loss: 0.4389818 Vali Loss: 1.1908821 Test Loss: 0.4375098
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.8075995445251465
Epoch: 21, Steps: 59 | Train Loss: 0.4383749 Vali Loss: 1.1944995 Test Loss: 0.4378999
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.9747440814971924
Epoch: 22, Steps: 59 | Train Loss: 0.4378672 Vali Loss: 1.1949450 Test Loss: 0.4379322
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.944645881652832
Epoch: 23, Steps: 59 | Train Loss: 0.4379413 Vali Loss: 1.1983569 Test Loss: 0.4381074
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.170319080352783
Epoch: 24, Steps: 59 | Train Loss: 0.4378172 Vali Loss: 1.1946789 Test Loss: 0.4382659
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.7602488994598389
Epoch: 25, Steps: 59 | Train Loss: 0.4374030 Vali Loss: 1.1961273 Test Loss: 0.4384350
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8580915927886963
Epoch: 26, Steps: 59 | Train Loss: 0.4371344 Vali Loss: 1.2005262 Test Loss: 0.4385532
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8556926250457764
Epoch: 27, Steps: 59 | Train Loss: 0.4370140 Vali Loss: 1.1949515 Test Loss: 0.4387317
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6902899742126465
Epoch: 28, Steps: 59 | Train Loss: 0.4364498 Vali Loss: 1.2034760 Test Loss: 0.4388163
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.684628963470459
Epoch: 29, Steps: 59 | Train Loss: 0.4365058 Vali Loss: 1.1960117 Test Loss: 0.4388517
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.8539402484893799
Epoch: 30, Steps: 59 | Train Loss: 0.4362385 Vali Loss: 1.1997650 Test Loss: 0.4390670
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6051959991455078
Epoch: 31, Steps: 59 | Train Loss: 0.4356377 Vali Loss: 1.2001660 Test Loss: 0.4391194
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43451520800590515, mae:0.43806079030036926, rse:0.6275589466094971, corr:[0.251044   0.26165813 0.2565178  0.25838822 0.25785884 0.2539699
 0.25343555 0.25502408 0.25491402 0.25375652 0.2534441  0.2533852
 0.25276184 0.2521301  0.25169834 0.25128156 0.25118932 0.25110233
 0.25039908 0.24957366 0.24945931 0.24913757 0.24841064 0.24795637
 0.24832006 0.24846278 0.24837849 0.24865527 0.24911287 0.24915798
 0.24896953 0.24907374 0.24920507 0.2488952  0.24842882 0.2483261
 0.24847646 0.24864978 0.24885541 0.24885818 0.24855931 0.24834874
 0.24860089 0.24894178 0.24902768 0.24939299 0.2500302  0.25025144
 0.25014415 0.24969135 0.24887085 0.2479758  0.24701235 0.24610141
 0.24552646 0.24527587 0.2451349  0.24485558 0.24451497 0.24425107
 0.2441112  0.24410579 0.24411966 0.24413328 0.24433796 0.24478272
 0.24519838 0.24499774 0.24464093 0.2446147  0.244867   0.2448537
 0.24416278 0.2431985  0.24273708 0.24285905 0.24282704 0.24236329
 0.24179456 0.24161646 0.24187091 0.24191421 0.24136217 0.24070205
 0.24041827 0.24039686 0.24018493 0.23981592 0.2396708  0.23969835
 0.23946589 0.23922567 0.2392319  0.23938999 0.23913053 0.23915686
 0.24013215 0.24128728 0.24178447 0.24182902 0.24174364 0.24157013
 0.24151799 0.2416785  0.24173166 0.2415358  0.24126653 0.24107364
 0.24088275 0.24082759 0.2410951  0.2414902  0.24167384 0.24157023
 0.24136858 0.24121866 0.2412     0.24129878 0.2413419  0.24132772
 0.24118221 0.24063389 0.23980707 0.23903985 0.23833789 0.23758559
 0.2370752  0.23691355 0.23686811 0.23678331 0.23662008 0.23644894
 0.23641613 0.2364783  0.23639958 0.2360838  0.23580895 0.23606914
 0.23659007 0.23678838 0.23659338 0.23640712 0.23604745 0.2356331
 0.23538987 0.2351823  0.23482877 0.23401208 0.23295091 0.23241699
 0.2328718  0.23333612 0.23310146 0.23264578 0.23253946 0.23257487
 0.23235568 0.23229171 0.23259072 0.2326948  0.23228647 0.23213275
 0.23244962 0.23265375 0.23233928 0.23202646 0.23185852 0.2317399
 0.23159437 0.23181623 0.23246612 0.23301706 0.2328851  0.23222597
 0.23158547 0.23133153 0.23129751 0.23135366 0.23133482 0.23116925
 0.23080118 0.23039488 0.2300667  0.22995706 0.2301832  0.23070706
 0.23114458 0.23137695 0.2316954  0.23218626 0.23241597 0.23219426
 0.23172082 0.23119873 0.23044913 0.22945632 0.22872955 0.22850066
 0.22832927 0.22786777 0.22731595 0.22716853 0.22726932 0.22736073
 0.22725786 0.22736692 0.22769426 0.22792782 0.22798228 0.22784694
 0.22764659 0.22728531 0.2268996  0.22670838 0.22656186 0.22626545
 0.22586715 0.22578526 0.22614059 0.2265166  0.22622204 0.22513086
 0.22433417 0.22450182 0.22484562 0.22469418 0.22439978 0.22456531
 0.2247228  0.22434415 0.22364101 0.22320026 0.22330509 0.22357321
 0.22328822 0.22289342 0.2232999  0.22402276 0.2239985  0.22345701
 0.22359455 0.22403002 0.22391179 0.22321548 0.22280148 0.22289735
 0.22286092 0.22259548 0.22242895 0.22244844 0.22233663 0.2218246
 0.22090389 0.22016479 0.22041446 0.22097051 0.22080076 0.220339
 0.22083741 0.22150859 0.22116607 0.22037597 0.22076212 0.22148801
 0.22118132 0.22045523 0.22027332 0.2199821  0.2187482  0.21766737
 0.21757181 0.21733291 0.21656738 0.21618663 0.21597753 0.21552597
 0.21544193 0.21596925 0.21594226 0.2153731  0.21574108 0.21691847
 0.21693294 0.21610488 0.21643025 0.21708553 0.21639852 0.21550325
 0.21588579 0.2162621  0.21512756 0.21435031 0.21498422 0.21525142
 0.21408533 0.21324018 0.21327281 0.21325143 0.21346638 0.21448591
 0.21460146 0.21337335 0.21339059 0.21460176 0.21489629 0.21428983
 0.21462247 0.21432714 0.21271993 0.21177074 0.21259673 0.21223636
 0.21001069 0.20905553 0.2097611  0.20876363 0.20646177 0.20559889
 0.2052716  0.20286655 0.20115201 0.20212673 0.20189187 0.2004539
 0.20141204 0.20219988 0.1996982  0.19883348 0.20001984 0.19707336
 0.19227722 0.19403383 0.19306162 0.17966457 0.1813881  0.19500409]
