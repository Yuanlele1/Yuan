Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_192_FITS_ETTh1_ftM_sl180_ll48_pl192_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8269
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=34, out_features=70, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2132480.0
params:  2450.0
Trainable parameters:  2450
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.6451847553253174
Epoch: 1, Steps: 64 | Train Loss: 0.6965695 Vali Loss: 1.3566068 Test Loss: 0.6517814
Validation loss decreased (inf --> 1.356607).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.0742063522338867
Epoch: 2, Steps: 64 | Train Loss: 0.5269910 Vali Loss: 1.1659180 Test Loss: 0.5248770
Validation loss decreased (1.356607 --> 1.165918).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.3027653694152832
Epoch: 3, Steps: 64 | Train Loss: 0.4700283 Vali Loss: 1.0945802 Test Loss: 0.4814368
Validation loss decreased (1.165918 --> 1.094580).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.2708690166473389
Epoch: 4, Steps: 64 | Train Loss: 0.4462396 Vali Loss: 1.0603231 Test Loss: 0.4629747
Validation loss decreased (1.094580 --> 1.060323).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.377351999282837
Epoch: 5, Steps: 64 | Train Loss: 0.4350228 Vali Loss: 1.0420053 Test Loss: 0.4533190
Validation loss decreased (1.060323 --> 1.042005).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.4068036079406738
Epoch: 6, Steps: 64 | Train Loss: 0.4281356 Vali Loss: 1.0301166 Test Loss: 0.4474410
Validation loss decreased (1.042005 --> 1.030117).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4134478569030762
Epoch: 7, Steps: 64 | Train Loss: 0.4233781 Vali Loss: 1.0214220 Test Loss: 0.4437858
Validation loss decreased (1.030117 --> 1.021422).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2954421043395996
Epoch: 8, Steps: 64 | Train Loss: 0.4208128 Vali Loss: 1.0132552 Test Loss: 0.4409875
Validation loss decreased (1.021422 --> 1.013255).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.2430813312530518
Epoch: 9, Steps: 64 | Train Loss: 0.4187156 Vali Loss: 1.0091630 Test Loss: 0.4392820
Validation loss decreased (1.013255 --> 1.009163).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.218379020690918
Epoch: 10, Steps: 64 | Train Loss: 0.4170624 Vali Loss: 1.0055321 Test Loss: 0.4377850
Validation loss decreased (1.009163 --> 1.005532).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.587902307510376
Epoch: 11, Steps: 64 | Train Loss: 0.4161420 Vali Loss: 1.0023576 Test Loss: 0.4370902
Validation loss decreased (1.005532 --> 1.002358).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.3684561252593994
Epoch: 12, Steps: 64 | Train Loss: 0.4151975 Vali Loss: 0.9997953 Test Loss: 0.4362710
Validation loss decreased (1.002358 --> 0.999795).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.3031623363494873
Epoch: 13, Steps: 64 | Train Loss: 0.4140219 Vali Loss: 0.9975222 Test Loss: 0.4360285
Validation loss decreased (0.999795 --> 0.997522).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.4180212020874023
Epoch: 14, Steps: 64 | Train Loss: 0.4136813 Vali Loss: 0.9953681 Test Loss: 0.4355765
Validation loss decreased (0.997522 --> 0.995368).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.3895626068115234
Epoch: 15, Steps: 64 | Train Loss: 0.4131470 Vali Loss: 0.9940270 Test Loss: 0.4355650
Validation loss decreased (0.995368 --> 0.994027).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.3937678337097168
Epoch: 16, Steps: 64 | Train Loss: 0.4126816 Vali Loss: 0.9921853 Test Loss: 0.4354250
Validation loss decreased (0.994027 --> 0.992185).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4624123573303223
Epoch: 17, Steps: 64 | Train Loss: 0.4127036 Vali Loss: 0.9912868 Test Loss: 0.4352943
Validation loss decreased (0.992185 --> 0.991287).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7143902778625488
Epoch: 18, Steps: 64 | Train Loss: 0.4121734 Vali Loss: 0.9901805 Test Loss: 0.4352384
Validation loss decreased (0.991287 --> 0.990180).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.3771121501922607
Epoch: 19, Steps: 64 | Train Loss: 0.4121570 Vali Loss: 0.9889148 Test Loss: 0.4353127
Validation loss decreased (0.990180 --> 0.988915).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.2671217918395996
Epoch: 20, Steps: 64 | Train Loss: 0.4117611 Vali Loss: 0.9885454 Test Loss: 0.4352287
Validation loss decreased (0.988915 --> 0.988545).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.3400440216064453
Epoch: 21, Steps: 64 | Train Loss: 0.4113845 Vali Loss: 0.9873161 Test Loss: 0.4352814
Validation loss decreased (0.988545 --> 0.987316).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.3558940887451172
Epoch: 22, Steps: 64 | Train Loss: 0.4113734 Vali Loss: 0.9869193 Test Loss: 0.4352065
Validation loss decreased (0.987316 --> 0.986919).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.1876022815704346
Epoch: 23, Steps: 64 | Train Loss: 0.4109437 Vali Loss: 0.9859664 Test Loss: 0.4351977
Validation loss decreased (0.986919 --> 0.985966).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.3062107563018799
Epoch: 24, Steps: 64 | Train Loss: 0.4111941 Vali Loss: 0.9859418 Test Loss: 0.4352561
Validation loss decreased (0.985966 --> 0.985942).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.2998063564300537
Epoch: 25, Steps: 64 | Train Loss: 0.4111339 Vali Loss: 0.9852377 Test Loss: 0.4352415
Validation loss decreased (0.985942 --> 0.985238).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.3105945587158203
Epoch: 26, Steps: 64 | Train Loss: 0.4108638 Vali Loss: 0.9850211 Test Loss: 0.4350738
Validation loss decreased (0.985238 --> 0.985021).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.348973274230957
Epoch: 27, Steps: 64 | Train Loss: 0.4112048 Vali Loss: 0.9841132 Test Loss: 0.4351758
Validation loss decreased (0.985021 --> 0.984113).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.356679916381836
Epoch: 28, Steps: 64 | Train Loss: 0.4105777 Vali Loss: 0.9840531 Test Loss: 0.4352442
Validation loss decreased (0.984113 --> 0.984053).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.384753704071045
Epoch: 29, Steps: 64 | Train Loss: 0.4105318 Vali Loss: 0.9836402 Test Loss: 0.4353082
Validation loss decreased (0.984053 --> 0.983640).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.2375006675720215
Epoch: 30, Steps: 64 | Train Loss: 0.4102792 Vali Loss: 0.9833570 Test Loss: 0.4352442
Validation loss decreased (0.983640 --> 0.983357).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.4048583507537842
Epoch: 31, Steps: 64 | Train Loss: 0.4105176 Vali Loss: 0.9830135 Test Loss: 0.4353633
Validation loss decreased (0.983357 --> 0.983014).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.16732120513916
Epoch: 32, Steps: 64 | Train Loss: 0.4100532 Vali Loss: 0.9827741 Test Loss: 0.4353728
Validation loss decreased (0.983014 --> 0.982774).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.7212669849395752
Epoch: 33, Steps: 64 | Train Loss: 0.4098699 Vali Loss: 0.9825868 Test Loss: 0.4353958
Validation loss decreased (0.982774 --> 0.982587).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3145675659179688
Epoch: 34, Steps: 64 | Train Loss: 0.4103911 Vali Loss: 0.9824477 Test Loss: 0.4354000
Validation loss decreased (0.982587 --> 0.982448).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.382216453552246
Epoch: 35, Steps: 64 | Train Loss: 0.4106853 Vali Loss: 0.9821522 Test Loss: 0.4354536
Validation loss decreased (0.982448 --> 0.982152).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.2855446338653564
Epoch: 36, Steps: 64 | Train Loss: 0.4105380 Vali Loss: 0.9816924 Test Loss: 0.4353900
Validation loss decreased (0.982152 --> 0.981692).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.3709933757781982
Epoch: 37, Steps: 64 | Train Loss: 0.4100675 Vali Loss: 0.9813007 Test Loss: 0.4354610
Validation loss decreased (0.981692 --> 0.981301).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.3563191890716553
Epoch: 38, Steps: 64 | Train Loss: 0.4103815 Vali Loss: 0.9812327 Test Loss: 0.4354629
Validation loss decreased (0.981301 --> 0.981233).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.2087335586547852
Epoch: 39, Steps: 64 | Train Loss: 0.4105657 Vali Loss: 0.9814464 Test Loss: 0.4354374
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.3770160675048828
Epoch: 40, Steps: 64 | Train Loss: 0.4106062 Vali Loss: 0.9811672 Test Loss: 0.4354825
Validation loss decreased (0.981233 --> 0.981167).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.339998722076416
Epoch: 41, Steps: 64 | Train Loss: 0.4102999 Vali Loss: 0.9810956 Test Loss: 0.4354025
Validation loss decreased (0.981167 --> 0.981096).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.2107274532318115
Epoch: 42, Steps: 64 | Train Loss: 0.4099485 Vali Loss: 0.9804651 Test Loss: 0.4354930
Validation loss decreased (0.981096 --> 0.980465).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.2517354488372803
Epoch: 43, Steps: 64 | Train Loss: 0.4097360 Vali Loss: 0.9802924 Test Loss: 0.4355103
Validation loss decreased (0.980465 --> 0.980292).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.324843406677246
Epoch: 44, Steps: 64 | Train Loss: 0.4100962 Vali Loss: 0.9807358 Test Loss: 0.4355126
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.3167142868041992
Epoch: 45, Steps: 64 | Train Loss: 0.4100322 Vali Loss: 0.9806881 Test Loss: 0.4354849
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.4122376441955566
Epoch: 46, Steps: 64 | Train Loss: 0.4101854 Vali Loss: 0.9805725 Test Loss: 0.4354561
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.303170919418335
Epoch: 47, Steps: 64 | Train Loss: 0.4095584 Vali Loss: 0.9800056 Test Loss: 0.4354882
Validation loss decreased (0.980292 --> 0.980006).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.2460317611694336
Epoch: 48, Steps: 64 | Train Loss: 0.4100644 Vali Loss: 0.9798322 Test Loss: 0.4354689
Validation loss decreased (0.980006 --> 0.979832).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.3308172225952148
Epoch: 49, Steps: 64 | Train Loss: 0.4100040 Vali Loss: 0.9802324 Test Loss: 0.4354368
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2825727462768555
Epoch: 50, Steps: 64 | Train Loss: 0.4099491 Vali Loss: 0.9796235 Test Loss: 0.4354812
Validation loss decreased (0.979832 --> 0.979623).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3878107070922852
Epoch: 51, Steps: 64 | Train Loss: 0.4102869 Vali Loss: 0.9793788 Test Loss: 0.4354370
Validation loss decreased (0.979623 --> 0.979379).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.0464024543762207
Epoch: 52, Steps: 64 | Train Loss: 0.4100997 Vali Loss: 0.9795986 Test Loss: 0.4354751
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.4255969524383545
Epoch: 53, Steps: 64 | Train Loss: 0.4101783 Vali Loss: 0.9793736 Test Loss: 0.4355313
Validation loss decreased (0.979379 --> 0.979374).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.3809847831726074
Epoch: 54, Steps: 64 | Train Loss: 0.4093209 Vali Loss: 0.9798114 Test Loss: 0.4355386
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.3173160552978516
Epoch: 55, Steps: 64 | Train Loss: 0.4099427 Vali Loss: 0.9792567 Test Loss: 0.4355622
Validation loss decreased (0.979374 --> 0.979257).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.27504301071167
Epoch: 56, Steps: 64 | Train Loss: 0.4099130 Vali Loss: 0.9795930 Test Loss: 0.4355384
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.037315845489502
Epoch: 57, Steps: 64 | Train Loss: 0.4098602 Vali Loss: 0.9794963 Test Loss: 0.4355400
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.3895986080169678
Epoch: 58, Steps: 64 | Train Loss: 0.4099228 Vali Loss: 0.9795315 Test Loss: 0.4355155
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.271275520324707
Epoch: 59, Steps: 64 | Train Loss: 0.4101993 Vali Loss: 0.9794572 Test Loss: 0.4355197
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.320007085800171
Epoch: 60, Steps: 64 | Train Loss: 0.4098533 Vali Loss: 0.9794580 Test Loss: 0.4355308
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.2639424800872803
Epoch: 61, Steps: 64 | Train Loss: 0.4098705 Vali Loss: 0.9792488 Test Loss: 0.4355091
Validation loss decreased (0.979257 --> 0.979249).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.2401843070983887
Epoch: 62, Steps: 64 | Train Loss: 0.4098565 Vali Loss: 0.9789765 Test Loss: 0.4354928
Validation loss decreased (0.979249 --> 0.978977).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.333498477935791
Epoch: 63, Steps: 64 | Train Loss: 0.4094080 Vali Loss: 0.9790276 Test Loss: 0.4355245
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.385833740234375
Epoch: 64, Steps: 64 | Train Loss: 0.4100869 Vali Loss: 0.9793718 Test Loss: 0.4355276
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.3051540851593018
Epoch: 65, Steps: 64 | Train Loss: 0.4097351 Vali Loss: 0.9792541 Test Loss: 0.4355409
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.3321068286895752
Epoch: 66, Steps: 64 | Train Loss: 0.4100460 Vali Loss: 0.9789560 Test Loss: 0.4355267
Validation loss decreased (0.978977 --> 0.978956).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.3134551048278809
Epoch: 67, Steps: 64 | Train Loss: 0.4099681 Vali Loss: 0.9786673 Test Loss: 0.4355126
Validation loss decreased (0.978956 --> 0.978667).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.3776872158050537
Epoch: 68, Steps: 64 | Train Loss: 0.4098302 Vali Loss: 0.9791811 Test Loss: 0.4355429
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3715007305145264
Epoch: 69, Steps: 64 | Train Loss: 0.4100862 Vali Loss: 0.9792297 Test Loss: 0.4355324
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.3194563388824463
Epoch: 70, Steps: 64 | Train Loss: 0.4096628 Vali Loss: 0.9788026 Test Loss: 0.4355308
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.4083037376403809
Epoch: 71, Steps: 64 | Train Loss: 0.4099114 Vali Loss: 0.9791071 Test Loss: 0.4355550
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.3718383312225342
Epoch: 72, Steps: 64 | Train Loss: 0.4100904 Vali Loss: 0.9789420 Test Loss: 0.4355664
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.3704192638397217
Epoch: 73, Steps: 64 | Train Loss: 0.4095739 Vali Loss: 0.9789857 Test Loss: 0.4355618
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.2848076820373535
Epoch: 74, Steps: 64 | Train Loss: 0.4098873 Vali Loss: 0.9786668 Test Loss: 0.4355662
Validation loss decreased (0.978667 --> 0.978667).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.1974122524261475
Epoch: 75, Steps: 64 | Train Loss: 0.4104102 Vali Loss: 0.9782045 Test Loss: 0.4355533
Validation loss decreased (0.978667 --> 0.978205).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.2754855155944824
Epoch: 76, Steps: 64 | Train Loss: 0.4096054 Vali Loss: 0.9786155 Test Loss: 0.4355546
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.2804934978485107
Epoch: 77, Steps: 64 | Train Loss: 0.4097528 Vali Loss: 0.9789190 Test Loss: 0.4355677
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.3773832321166992
Epoch: 78, Steps: 64 | Train Loss: 0.4099799 Vali Loss: 0.9783394 Test Loss: 0.4355655
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.2982871532440186
Epoch: 79, Steps: 64 | Train Loss: 0.4095701 Vali Loss: 0.9790002 Test Loss: 0.4355645
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.2451505661010742
Epoch: 80, Steps: 64 | Train Loss: 0.4099798 Vali Loss: 0.9785863 Test Loss: 0.4355688
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.260610818862915
Epoch: 81, Steps: 64 | Train Loss: 0.4099868 Vali Loss: 0.9788612 Test Loss: 0.4355603
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.2447748184204102
Epoch: 82, Steps: 64 | Train Loss: 0.4096026 Vali Loss: 0.9782348 Test Loss: 0.4355720
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.2301149368286133
Epoch: 83, Steps: 64 | Train Loss: 0.4097383 Vali Loss: 0.9782890 Test Loss: 0.4355593
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.2652878761291504
Epoch: 84, Steps: 64 | Train Loss: 0.4095790 Vali Loss: 0.9781153 Test Loss: 0.4355697
Validation loss decreased (0.978205 --> 0.978115).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.210646629333496
Epoch: 85, Steps: 64 | Train Loss: 0.4098347 Vali Loss: 0.9788240 Test Loss: 0.4355647
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.2124218940734863
Epoch: 86, Steps: 64 | Train Loss: 0.4098627 Vali Loss: 0.9784898 Test Loss: 0.4355637
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.3044683933258057
Epoch: 87, Steps: 64 | Train Loss: 0.4095404 Vali Loss: 0.9788398 Test Loss: 0.4355676
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.334040880203247
Epoch: 88, Steps: 64 | Train Loss: 0.4096154 Vali Loss: 0.9788929 Test Loss: 0.4355657
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.3011701107025146
Epoch: 89, Steps: 64 | Train Loss: 0.4096589 Vali Loss: 0.9784742 Test Loss: 0.4355644
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.2741453647613525
Epoch: 90, Steps: 64 | Train Loss: 0.4098973 Vali Loss: 0.9784818 Test Loss: 0.4355717
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.36372971534729
Epoch: 91, Steps: 64 | Train Loss: 0.4096123 Vali Loss: 0.9783317 Test Loss: 0.4355703
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.3492660522460938
Epoch: 92, Steps: 64 | Train Loss: 0.4100279 Vali Loss: 0.9784666 Test Loss: 0.4355756
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.3758959770202637
Epoch: 93, Steps: 64 | Train Loss: 0.4092722 Vali Loss: 0.9787665 Test Loss: 0.4355678
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.2117650508880615
Epoch: 94, Steps: 64 | Train Loss: 0.4094813 Vali Loss: 0.9787501 Test Loss: 0.4355712
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.4012446403503418
Epoch: 95, Steps: 64 | Train Loss: 0.4100254 Vali Loss: 0.9783544 Test Loss: 0.4355737
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.2448110580444336
Epoch: 96, Steps: 64 | Train Loss: 0.4095992 Vali Loss: 0.9787955 Test Loss: 0.4355852
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.4421231746673584
Epoch: 97, Steps: 64 | Train Loss: 0.4095010 Vali Loss: 0.9787610 Test Loss: 0.4355789
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.2568671703338623
Epoch: 98, Steps: 64 | Train Loss: 0.4099689 Vali Loss: 0.9782969 Test Loss: 0.4355810
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.282796859741211
Epoch: 99, Steps: 64 | Train Loss: 0.4098215 Vali Loss: 0.9784760 Test Loss: 0.4355742
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.3884365558624268
Epoch: 100, Steps: 64 | Train Loss: 0.4093088 Vali Loss: 0.9786663 Test Loss: 0.4355790
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_180_192_FITS_ETTh1_ftM_sl180_ll48_pl192_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.42913204431533813, mae:0.42282208800315857, rse:0.6220894455909729, corr:[0.2633292  0.2677955  0.26760495 0.26548672 0.26344073 0.2620882
 0.26156658 0.2612667  0.2606523  0.26046753 0.26072568 0.26095745
 0.26049668 0.25931597 0.25876138 0.2592133  0.2597782  0.259832
 0.25942993 0.25903377 0.2590872  0.25951555 0.25968128 0.2591439
 0.25812206 0.25773713 0.25756234 0.25707337 0.25611827 0.25535166
 0.25519234 0.2553383  0.25521857 0.25481156 0.2546169  0.25515506
 0.25582722 0.25581613 0.25550875 0.25551227 0.25595233 0.25637826
 0.2563846  0.25637498 0.25678673 0.25728795 0.25740916 0.2565498
 0.2548004  0.25362262 0.25262517 0.25142828 0.24999784 0.24873877
 0.24817777 0.24817522 0.248158   0.24816908 0.248121   0.24863677
 0.24921854 0.24896333 0.24846685 0.24834335 0.24865587 0.24896973
 0.2489262  0.24858992 0.24852146 0.24876146 0.24884775 0.24799116
 0.2463145  0.24513274 0.24445653 0.2440004  0.24356484 0.24310441
 0.24296308 0.24303375 0.24288249 0.24252412 0.24215503 0.24233003
 0.242889   0.2428568  0.24256471 0.24245055 0.24254237 0.24263705
 0.24250522 0.24237794 0.24262112 0.2430576  0.24329683 0.2428699
 0.24177764 0.24112193 0.2408389  0.24029504 0.23954406 0.2389387
 0.23882662 0.23924081 0.23955327 0.23958343 0.23939578 0.23957716
 0.23986055 0.23959109 0.2392925  0.23942018 0.2397034  0.23978373
 0.23958889 0.2393948  0.23950994 0.23977672 0.23974827 0.23898172
 0.23747125 0.23631243 0.23533542 0.23439625 0.23346512 0.23291278
 0.23296541 0.23340406 0.23355754 0.23343179 0.2333453  0.23396441
 0.23486944 0.23493637 0.23470767 0.23463522 0.23481826 0.2350795
 0.23512118 0.23500967 0.23508134 0.23526907 0.23519199 0.23439023
 0.2329884  0.2321163  0.23152553 0.23051651 0.22953337 0.22893533
 0.22907776 0.22956133 0.22992161 0.22999132 0.2299474  0.2304508
 0.23117918 0.23111597 0.23070277 0.23038898 0.23032148 0.23050426
 0.23053548 0.23042072 0.23054473 0.23083037 0.23088898 0.2301335
 0.22877298 0.22805254 0.22766802 0.22704922 0.22634304 0.22588399
 0.22605777 0.22681381 0.22738056 0.22783282 0.2281946  0.22900249
 0.23005417 0.22999324 0.22946788 0.22931926 0.2296528  0.23021142
 0.2302336  0.2296221  0.22936065 0.23006429 0.23112737 0.23034139]
