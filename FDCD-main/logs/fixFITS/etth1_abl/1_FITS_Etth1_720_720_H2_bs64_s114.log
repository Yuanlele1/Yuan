Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=72, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=72, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9289728.0
params:  10512.0
Trainable parameters:  10512
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.104444980621338
Epoch: 1, Steps: 56 | Train Loss: 0.9327158 Vali Loss: 1.9400247 Test Loss: 0.7762587
Validation loss decreased (inf --> 1.940025).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.9902853965759277
Epoch: 2, Steps: 56 | Train Loss: 0.7550117 Vali Loss: 1.7195978 Test Loss: 0.6407903
Validation loss decreased (1.940025 --> 1.719598).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.074362516403198
Epoch: 3, Steps: 56 | Train Loss: 0.6888426 Vali Loss: 1.6308300 Test Loss: 0.5839286
Validation loss decreased (1.719598 --> 1.630830).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.9432995319366455
Epoch: 4, Steps: 56 | Train Loss: 0.6566969 Vali Loss: 1.5848666 Test Loss: 0.5506426
Validation loss decreased (1.630830 --> 1.584867).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.98093843460083
Epoch: 5, Steps: 56 | Train Loss: 0.6363374 Vali Loss: 1.5551510 Test Loss: 0.5273069
Validation loss decreased (1.584867 --> 1.555151).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.191698789596558
Epoch: 6, Steps: 56 | Train Loss: 0.6223861 Vali Loss: 1.5306877 Test Loss: 0.5097031
Validation loss decreased (1.555151 --> 1.530688).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.900273084640503
Epoch: 7, Steps: 56 | Train Loss: 0.6114536 Vali Loss: 1.5059278 Test Loss: 0.4961655
Validation loss decreased (1.530688 --> 1.505928).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.190613508224487
Epoch: 8, Steps: 56 | Train Loss: 0.6035244 Vali Loss: 1.4861726 Test Loss: 0.4857982
Validation loss decreased (1.505928 --> 1.486173).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.034349203109741
Epoch: 9, Steps: 56 | Train Loss: 0.5967482 Vali Loss: 1.4821790 Test Loss: 0.4776880
Validation loss decreased (1.486173 --> 1.482179).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.9845306873321533
Epoch: 10, Steps: 56 | Train Loss: 0.5922753 Vali Loss: 1.4702131 Test Loss: 0.4715202
Validation loss decreased (1.482179 --> 1.470213).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.1756272315979
Epoch: 11, Steps: 56 | Train Loss: 0.5876205 Vali Loss: 1.4642102 Test Loss: 0.4667346
Validation loss decreased (1.470213 --> 1.464210).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.000427007675171
Epoch: 12, Steps: 56 | Train Loss: 0.5845861 Vali Loss: 1.4529591 Test Loss: 0.4629317
Validation loss decreased (1.464210 --> 1.452959).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.129709720611572
Epoch: 13, Steps: 56 | Train Loss: 0.5816486 Vali Loss: 1.4539709 Test Loss: 0.4601473
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.9871857166290283
Epoch: 14, Steps: 56 | Train Loss: 0.5797719 Vali Loss: 1.4511926 Test Loss: 0.4579394
Validation loss decreased (1.452959 --> 1.451193).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.9887583255767822
Epoch: 15, Steps: 56 | Train Loss: 0.5779379 Vali Loss: 1.4444158 Test Loss: 0.4561827
Validation loss decreased (1.451193 --> 1.444416).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.162673711776733
Epoch: 16, Steps: 56 | Train Loss: 0.5762628 Vali Loss: 1.4424034 Test Loss: 0.4548548
Validation loss decreased (1.444416 --> 1.442403).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.357960939407349
Epoch: 17, Steps: 56 | Train Loss: 0.5747473 Vali Loss: 1.4440453 Test Loss: 0.4538761
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.254319190979004
Epoch: 18, Steps: 56 | Train Loss: 0.5736698 Vali Loss: 1.4417655 Test Loss: 0.4531692
Validation loss decreased (1.442403 --> 1.441766).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.837251663208008
Epoch: 19, Steps: 56 | Train Loss: 0.5728950 Vali Loss: 1.4445746 Test Loss: 0.4525915
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.265059471130371
Epoch: 20, Steps: 56 | Train Loss: 0.5722178 Vali Loss: 1.4397457 Test Loss: 0.4520409
Validation loss decreased (1.441766 --> 1.439746).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.115612030029297
Epoch: 21, Steps: 56 | Train Loss: 0.5716183 Vali Loss: 1.4387326 Test Loss: 0.4516889
Validation loss decreased (1.439746 --> 1.438733).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.057610273361206
Epoch: 22, Steps: 56 | Train Loss: 0.5707019 Vali Loss: 1.4418216 Test Loss: 0.4515845
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.164519786834717
Epoch: 23, Steps: 56 | Train Loss: 0.5703139 Vali Loss: 1.4394927 Test Loss: 0.4512984
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.084803819656372
Epoch: 24, Steps: 56 | Train Loss: 0.5698695 Vali Loss: 1.4380474 Test Loss: 0.4511706
Validation loss decreased (1.438733 --> 1.438047).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 5.18803858757019
Epoch: 25, Steps: 56 | Train Loss: 0.5690285 Vali Loss: 1.4348661 Test Loss: 0.4511182
Validation loss decreased (1.438047 --> 1.434866).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.464704751968384
Epoch: 26, Steps: 56 | Train Loss: 0.5688177 Vali Loss: 1.4389529 Test Loss: 0.4510152
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 5.659628629684448
Epoch: 27, Steps: 56 | Train Loss: 0.5686333 Vali Loss: 1.4355824 Test Loss: 0.4509546
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 6.505311489105225
Epoch: 28, Steps: 56 | Train Loss: 0.5682322 Vali Loss: 1.4414289 Test Loss: 0.4509814
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 5.983365774154663
Epoch: 29, Steps: 56 | Train Loss: 0.5677577 Vali Loss: 1.4421360 Test Loss: 0.4508513
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 6.178934335708618
Epoch: 30, Steps: 56 | Train Loss: 0.5677990 Vali Loss: 1.4374754 Test Loss: 0.4508845
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 6.65711236000061
Epoch: 31, Steps: 56 | Train Loss: 0.5675182 Vali Loss: 1.4364562 Test Loss: 0.4508985
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 7.580177307128906
Epoch: 32, Steps: 56 | Train Loss: 0.5673917 Vali Loss: 1.4402158 Test Loss: 0.4509104
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 9.673070907592773
Epoch: 33, Steps: 56 | Train Loss: 0.5671651 Vali Loss: 1.4349641 Test Loss: 0.4509400
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 5.901662588119507
Epoch: 34, Steps: 56 | Train Loss: 0.5666888 Vali Loss: 1.4400918 Test Loss: 0.4509443
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 9.288130760192871
Epoch: 35, Steps: 56 | Train Loss: 0.5664409 Vali Loss: 1.4371302 Test Loss: 0.4509481
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 9.51483941078186
Epoch: 36, Steps: 56 | Train Loss: 0.5665560 Vali Loss: 1.4350893 Test Loss: 0.4510029
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 9.029033899307251
Epoch: 37, Steps: 56 | Train Loss: 0.5661345 Vali Loss: 1.4401972 Test Loss: 0.4510194
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 7.051469564437866
Epoch: 38, Steps: 56 | Train Loss: 0.5661558 Vali Loss: 1.4367198 Test Loss: 0.4510610
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 7.797804117202759
Epoch: 39, Steps: 56 | Train Loss: 0.5658588 Vali Loss: 1.4370562 Test Loss: 0.4510849
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 8.078015089035034
Epoch: 40, Steps: 56 | Train Loss: 0.5659577 Vali Loss: 1.4416510 Test Loss: 0.4510968
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 7.306935787200928
Epoch: 41, Steps: 56 | Train Loss: 0.5660871 Vali Loss: 1.4383425 Test Loss: 0.4511471
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 7.2784929275512695
Epoch: 42, Steps: 56 | Train Loss: 0.5659403 Vali Loss: 1.4372234 Test Loss: 0.4511416
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 7.4797682762146
Epoch: 43, Steps: 56 | Train Loss: 0.5659167 Vali Loss: 1.4368644 Test Loss: 0.4511883
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 6.663680791854858
Epoch: 44, Steps: 56 | Train Loss: 0.5657516 Vali Loss: 1.4388399 Test Loss: 0.4511727
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 6.752834796905518
Epoch: 45, Steps: 56 | Train Loss: 0.5656097 Vali Loss: 1.4381189 Test Loss: 0.4512005
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_720_FITS_ETTh1_ftM_sl720_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4502863585948944, mae:0.47236156463623047, rse:0.6423864364624023, corr:[0.20830676 0.22359149 0.23127858 0.23377848 0.23307584 0.23111682
 0.22956769 0.22883962 0.22850937 0.22818236 0.22767322 0.22697534
 0.2262058  0.22539079 0.22473173 0.22442444 0.22465457 0.22525541
 0.22596246 0.22657262 0.22695047 0.22705889 0.22679856 0.22636722
 0.22593164 0.225672   0.2256232  0.2257058  0.22586492 0.226031
 0.22617349 0.2261734  0.22601697 0.22572057 0.22537763 0.22520243
 0.22543935 0.22573453 0.22598375 0.22608458 0.2260493  0.2258277
 0.22559537 0.22550586 0.2256245  0.22590648 0.22624452 0.22652948
 0.2265006  0.22626531 0.22582729 0.22523557 0.22458452 0.22382222
 0.22303775 0.22221056 0.22175473 0.22139668 0.22111028 0.22088094
 0.22079563 0.22081685 0.22082156 0.2208417  0.22085111 0.22087808
 0.22095737 0.22105028 0.22117372 0.2212787  0.22123475 0.22119018
 0.22081643 0.22023149 0.21965389 0.21914193 0.21874334 0.2184141
 0.21822752 0.21822198 0.21828128 0.21826918 0.21810797 0.21780917
 0.21748479 0.21713884 0.21680267 0.21658601 0.21642534 0.21635623
 0.21642251 0.21673442 0.21719417 0.21782804 0.21859378 0.21947116
 0.2203413  0.22105458 0.22149366 0.22172771 0.22173433 0.22160608
 0.22141859 0.22120698 0.22103289 0.22078393 0.22048531 0.22020626
 0.22004741 0.22001709 0.21991384 0.21970971 0.21946003 0.21915327
 0.21898943 0.21897489 0.21908161 0.21923605 0.21936445 0.21949288
 0.21951962 0.21936017 0.21897018 0.21853167 0.21798399 0.21742763
 0.21695614 0.21670608 0.21660309 0.21653135 0.21651018 0.21650338
 0.21651706 0.21641582 0.2162412  0.21606645 0.2158571  0.21562022
 0.2154121  0.21527892 0.21516691 0.21497014 0.21461667 0.21420103
 0.21377136 0.2133112  0.2128818  0.21250951 0.21237847 0.2124863
 0.21269168 0.21298528 0.21323293 0.21327244 0.21309152 0.2127915
 0.21241239 0.21204594 0.21182431 0.21182759 0.21185672 0.21193805
 0.21207918 0.21226913 0.2124725  0.21267316 0.21282266 0.21303786
 0.21329641 0.21358213 0.21373858 0.21382938 0.21376088 0.21358068
 0.21331924 0.21312441 0.21292095 0.21271162 0.2125185  0.21237639
 0.21229142 0.2122846  0.21238631 0.21256982 0.2128288  0.21310638
 0.2133661  0.21355881 0.2136969  0.21371426 0.21356258 0.2132397
 0.21280493 0.21228184 0.21171291 0.21114181 0.2106508  0.21031423
 0.21013315 0.21024235 0.21053623 0.21080998 0.2110309  0.21118256
 0.21129175 0.21144287 0.21161029 0.21171737 0.21176407 0.21173126
 0.21159206 0.21128961 0.2109437  0.21061304 0.21027347 0.21000642
 0.20980993 0.20970708 0.20964426 0.20963377 0.20973407 0.20984304
 0.20991336 0.20990984 0.20979038 0.20956808 0.20923005 0.2089085
 0.20868993 0.20854732 0.20847559 0.20845968 0.20830743 0.20803922
 0.20775415 0.20750108 0.20735742 0.20737255 0.20759748 0.20798992
 0.20849231 0.20891373 0.20916025 0.20922546 0.20900124 0.20850627
 0.20800258 0.20758398 0.20729141 0.20708875 0.20699549 0.20702593
 0.20716678 0.20724933 0.20727868 0.20719828 0.20708166 0.20697384
 0.20697968 0.20699202 0.20713255 0.20731512 0.2074967  0.20753159
 0.20739019 0.20713945 0.2068218  0.20653902 0.20628549 0.2061916
 0.20631954 0.2065913  0.2067961  0.20688705 0.2068594  0.20675299
 0.20657775 0.20647915 0.20635219 0.20627151 0.206177   0.20615418
 0.20608623 0.206075   0.20612471 0.20627779 0.20661423 0.20708248
 0.20761272 0.2081383  0.20854428 0.20880817 0.20892315 0.20885035
 0.20867555 0.20851266 0.2083291  0.20815624 0.20808107 0.20814937
 0.2083139  0.20847888 0.20861141 0.20874429 0.20873743 0.20860183
 0.20858707 0.20862213 0.20882477 0.20928226 0.209801   0.21029645
 0.21068236 0.21075496 0.21032777 0.20953992 0.20859316 0.20772702
 0.20711212 0.20683412 0.20680593 0.20686404 0.20695132 0.2069923
 0.20699686 0.20695539 0.20683888 0.2068288  0.20682575 0.20689893
 0.20709684 0.20730327 0.20743969 0.20747533 0.20731857 0.2070459
 0.2066998  0.20632012 0.20591958 0.20562978 0.2054776  0.20537835
 0.20530552 0.20516211 0.20486315 0.20450953 0.20420335 0.20408249
 0.20409343 0.20424737 0.20446512 0.20470598 0.20484102 0.20489639
 0.20478605 0.20457079 0.20441893 0.20445621 0.2046285  0.20487705
 0.20511311 0.20511709 0.2047994  0.20420218 0.20343632 0.20259434
 0.20183565 0.20129198 0.20098567 0.20084089 0.20078464 0.20082524
 0.20073909 0.20046188 0.20002672 0.19946381 0.19895084 0.19862941
 0.19860798 0.1988298  0.1993642  0.2001591  0.20101073 0.20187028
 0.20249166 0.2026701  0.20238891 0.20182294 0.20132245 0.20093736
 0.20076747 0.20069912 0.20058675 0.20030628 0.19984233 0.19934721
 0.19895966 0.19869412 0.19863987 0.19879583 0.19912916 0.19956432
 0.19996946 0.20015883 0.20018929 0.2002152  0.20015487 0.20021985
 0.20047374 0.20082468 0.20115086 0.20138226 0.20149994 0.20134169
 0.20099878 0.2005077  0.19998524 0.19949341 0.19916828 0.1991131
 0.19924834 0.19951807 0.19962022 0.19959378 0.19942236 0.19925019
 0.19906938 0.19899923 0.1990504  0.19959776 0.2004187  0.20121609
 0.20181942 0.20200258 0.20168081 0.20116132 0.20062123 0.20013368
 0.19985838 0.19991079 0.20013101 0.20041052 0.20050584 0.20039327
 0.1999944  0.19940512 0.1988307  0.19839975 0.1982172  0.1983112
 0.19870144 0.19922495 0.1998392  0.20048115 0.201014   0.20133805
 0.20137243 0.20119841 0.20093013 0.20062988 0.20045385 0.20035827
 0.20029554 0.20018904 0.19989423 0.19953252 0.19912358 0.19890366
 0.19891445 0.19916773 0.19956268 0.19998749 0.20027143 0.20030332
 0.19998714 0.19946606 0.19889916 0.19846435 0.19849274 0.19882165
 0.19930813 0.19978341 0.19996662 0.19975445 0.19919057 0.19839859
 0.19746341 0.1967657  0.19636843 0.19637619 0.19678755 0.1974121
 0.19801278 0.19833003 0.19825815 0.19783594 0.19727395 0.1966929
 0.19634569 0.19636762 0.19682439 0.19766979 0.19869228 0.19958742
 0.2001103  0.20026326 0.2000287  0.19956179 0.19916026 0.1988395
 0.19867273 0.19871351 0.19874676 0.19862948 0.19843982 0.1983046
 0.19816773 0.19807346 0.19824417 0.1985597  0.19915292 0.19985479
 0.20046213 0.20091587 0.20141819 0.20140567 0.2014631  0.20111786
 0.20098013 0.20084289 0.20062774 0.20057832 0.20044978 0.20025036
 0.19977935 0.19924283 0.1986648  0.19825165 0.19805829 0.1983139
 0.19892688 0.19963649 0.20034593 0.2007469  0.20086904 0.20070072
 0.20040925 0.20014651 0.19997877 0.19997957 0.200085   0.20008382
 0.1997813  0.19919726 0.1981991  0.19694594 0.1956879  0.1947142
 0.19424386 0.19437425 0.19459629 0.1947772  0.19475538 0.19454326
 0.19398922 0.19304238 0.19227956 0.19169752 0.19172488 0.19209027
 0.19276538 0.19325238 0.1935499  0.1935119  0.19318052 0.19254743
 0.19185182 0.19136626 0.19092774 0.19070408 0.19066145 0.19055004
 0.19047986 0.19018313 0.18974319 0.18906522 0.1884453  0.18806487
 0.1880917  0.18834436 0.1888708  0.18941717 0.18973175 0.18966043
 0.18930037 0.18872394 0.18815327 0.1878746  0.18790701 0.1881464
 0.18839426 0.18835351 0.18773183 0.18649104 0.1848335  0.18303615
 0.1815478  0.18074335 0.18061456 0.18106109 0.18166736 0.18221626
 0.18248993 0.182313   0.18180099 0.18112575 0.18059693 0.18046477
 0.18086828 0.18165223 0.18258035 0.18331045 0.18364471 0.18333162
 0.18241139 0.18121244 0.17998637 0.17921114 0.17897816 0.17911069
 0.1794916  0.17983274 0.17977555 0.17920943 0.17825504 0.17728944
 0.17663455 0.17620476 0.17622814 0.17671782 0.17737038 0.17792612
 0.17806518 0.17775245 0.17717372 0.1765797  0.17628455 0.17644872
 0.17694452 0.17760512 0.17801757 0.17811118 0.17755516 0.176319
 0.17465442 0.17297274 0.17173828 0.17102733 0.17081434 0.17109755
 0.17162251 0.17191938 0.17188756 0.17145158 0.17088805 0.17047697
 0.170396   0.17080517 0.171553   0.17244154 0.17290913 0.17258869
 0.171003   0.16847378 0.16538371 0.1624271  0.16032133 0.15913397
 0.15919095 0.16011533 0.1613777  0.16194688 0.16176441 0.16106124
 0.16011459 0.1591177  0.15868378 0.15904902 0.16010195 0.16143563
 0.16242939 0.16263732 0.16187817 0.16038127 0.15875275 0.1576745
 0.15745237 0.15813413 0.15901835 0.15975232 0.15963127 0.15823868
 0.15592086 0.15317014 0.15061738 0.14865805 0.1478172  0.14825758
 0.14970234 0.15091631 0.15109512 0.14992553 0.1480323  0.14628431
 0.14564785 0.1464598  0.1485855  0.14978714 0.14392875 0.11842743]
