Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=1919, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=106, out_features=318, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30202368.0
params:  34026.0
Trainable parameters:  34026
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8697447776794434
Epoch: 1, Steps: 59 | Train Loss: 0.9922597 Vali Loss: 1.9947500 Test Loss: 0.8129471
Validation loss decreased (inf --> 1.994750).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9772181510925293
Epoch: 2, Steps: 59 | Train Loss: 0.7592954 Vali Loss: 1.7596006 Test Loss: 0.6431001
Validation loss decreased (1.994750 --> 1.759601).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.7967777252197266
Epoch: 3, Steps: 59 | Train Loss: 0.6874618 Vali Loss: 1.6691592 Test Loss: 0.5801966
Validation loss decreased (1.759601 --> 1.669159).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.8006184101104736
Epoch: 4, Steps: 59 | Train Loss: 0.6569318 Vali Loss: 1.6222935 Test Loss: 0.5448439
Validation loss decreased (1.669159 --> 1.622293).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.733722448348999
Epoch: 5, Steps: 59 | Train Loss: 0.6383116 Vali Loss: 1.5893517 Test Loss: 0.5203809
Validation loss decreased (1.622293 --> 1.589352).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.6982395648956299
Epoch: 6, Steps: 59 | Train Loss: 0.6251412 Vali Loss: 1.5626700 Test Loss: 0.5016116
Validation loss decreased (1.589352 --> 1.562670).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.6850335597991943
Epoch: 7, Steps: 59 | Train Loss: 0.6147584 Vali Loss: 1.5466304 Test Loss: 0.4867389
Validation loss decreased (1.562670 --> 1.546630).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.1526875495910645
Epoch: 8, Steps: 59 | Train Loss: 0.6067448 Vali Loss: 1.5206214 Test Loss: 0.4750526
Validation loss decreased (1.546630 --> 1.520621).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7234935760498047
Epoch: 9, Steps: 59 | Train Loss: 0.6003300 Vali Loss: 1.5110517 Test Loss: 0.4654795
Validation loss decreased (1.520621 --> 1.511052).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6021549701690674
Epoch: 10, Steps: 59 | Train Loss: 0.5949208 Vali Loss: 1.4976975 Test Loss: 0.4578487
Validation loss decreased (1.511052 --> 1.497697).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7045378684997559
Epoch: 11, Steps: 59 | Train Loss: 0.5905506 Vali Loss: 1.4884331 Test Loss: 0.4516084
Validation loss decreased (1.497697 --> 1.488433).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.728832721710205
Epoch: 12, Steps: 59 | Train Loss: 0.5869683 Vali Loss: 1.4847434 Test Loss: 0.4466085
Validation loss decreased (1.488433 --> 1.484743).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6756720542907715
Epoch: 13, Steps: 59 | Train Loss: 0.5842232 Vali Loss: 1.4800992 Test Loss: 0.4424554
Validation loss decreased (1.484743 --> 1.480099).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5506458282470703
Epoch: 14, Steps: 59 | Train Loss: 0.5817428 Vali Loss: 1.4669751 Test Loss: 0.4391812
Validation loss decreased (1.480099 --> 1.466975).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.8387181758880615
Epoch: 15, Steps: 59 | Train Loss: 0.5797153 Vali Loss: 1.4627368 Test Loss: 0.4364873
Validation loss decreased (1.466975 --> 1.462737).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7224364280700684
Epoch: 16, Steps: 59 | Train Loss: 0.5780714 Vali Loss: 1.4640050 Test Loss: 0.4342919
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.6682212352752686
Epoch: 17, Steps: 59 | Train Loss: 0.5765769 Vali Loss: 1.4615979 Test Loss: 0.4325320
Validation loss decreased (1.462737 --> 1.461598).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.5974900722503662
Epoch: 18, Steps: 59 | Train Loss: 0.5753925 Vali Loss: 1.4587255 Test Loss: 0.4310876
Validation loss decreased (1.461598 --> 1.458725).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9555716514587402
Epoch: 19, Steps: 59 | Train Loss: 0.5744844 Vali Loss: 1.4529634 Test Loss: 0.4299650
Validation loss decreased (1.458725 --> 1.452963).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5443994998931885
Epoch: 20, Steps: 59 | Train Loss: 0.5735352 Vali Loss: 1.4545485 Test Loss: 0.4289589
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.5834989547729492
Epoch: 21, Steps: 59 | Train Loss: 0.5729218 Vali Loss: 1.4491249 Test Loss: 0.4282489
Validation loss decreased (1.452963 --> 1.449125).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.7223405838012695
Epoch: 22, Steps: 59 | Train Loss: 0.5723092 Vali Loss: 1.4486644 Test Loss: 0.4276283
Validation loss decreased (1.449125 --> 1.448664).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6493017673492432
Epoch: 23, Steps: 59 | Train Loss: 0.5718303 Vali Loss: 1.4449635 Test Loss: 0.4270971
Validation loss decreased (1.448664 --> 1.444963).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8337244987487793
Epoch: 24, Steps: 59 | Train Loss: 0.5714278 Vali Loss: 1.4469342 Test Loss: 0.4267056
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.1395130157470703
Epoch: 25, Steps: 59 | Train Loss: 0.5709763 Vali Loss: 1.4430254 Test Loss: 0.4264072
Validation loss decreased (1.444963 --> 1.443025).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8247790336608887
Epoch: 26, Steps: 59 | Train Loss: 0.5706661 Vali Loss: 1.4450712 Test Loss: 0.4261345
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6920921802520752
Epoch: 27, Steps: 59 | Train Loss: 0.5702141 Vali Loss: 1.4429135 Test Loss: 0.4259362
Validation loss decreased (1.443025 --> 1.442914).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6663260459899902
Epoch: 28, Steps: 59 | Train Loss: 0.5700638 Vali Loss: 1.4445844 Test Loss: 0.4257945
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6264989376068115
Epoch: 29, Steps: 59 | Train Loss: 0.5697030 Vali Loss: 1.4449465 Test Loss: 0.4256685
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.7586960792541504
Epoch: 30, Steps: 59 | Train Loss: 0.5696433 Vali Loss: 1.4382977 Test Loss: 0.4255481
Validation loss decreased (1.442914 --> 1.438298).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5849339962005615
Epoch: 31, Steps: 59 | Train Loss: 0.5692814 Vali Loss: 1.4367551 Test Loss: 0.4254813
Validation loss decreased (1.438298 --> 1.436755).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.5619258880615234
Epoch: 32, Steps: 59 | Train Loss: 0.5693734 Vali Loss: 1.4390795 Test Loss: 0.4254432
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5795893669128418
Epoch: 33, Steps: 59 | Train Loss: 0.5693287 Vali Loss: 1.4392713 Test Loss: 0.4253921
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.8358213901519775
Epoch: 34, Steps: 59 | Train Loss: 0.5691140 Vali Loss: 1.4417024 Test Loss: 0.4253910
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.5737416744232178
Epoch: 35, Steps: 59 | Train Loss: 0.5689842 Vali Loss: 1.4426401 Test Loss: 0.4253685
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.891549825668335
Epoch: 36, Steps: 59 | Train Loss: 0.5688215 Vali Loss: 1.4418814 Test Loss: 0.4253309
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5911569595336914
Epoch: 37, Steps: 59 | Train Loss: 0.5688115 Vali Loss: 1.4347743 Test Loss: 0.4253380
Validation loss decreased (1.436755 --> 1.434774).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.632354974746704
Epoch: 38, Steps: 59 | Train Loss: 0.5687902 Vali Loss: 1.4438723 Test Loss: 0.4253578
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7920546531677246
Epoch: 39, Steps: 59 | Train Loss: 0.5684523 Vali Loss: 1.4336561 Test Loss: 0.4253521
Validation loss decreased (1.434774 --> 1.433656).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7329349517822266
Epoch: 40, Steps: 59 | Train Loss: 0.5686993 Vali Loss: 1.4344184 Test Loss: 0.4253739
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6419615745544434
Epoch: 41, Steps: 59 | Train Loss: 0.5684336 Vali Loss: 1.4351230 Test Loss: 0.4253833
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.6923067569732666
Epoch: 42, Steps: 59 | Train Loss: 0.5684602 Vali Loss: 1.4405223 Test Loss: 0.4253838
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.658778429031372
Epoch: 43, Steps: 59 | Train Loss: 0.5683857 Vali Loss: 1.4400985 Test Loss: 0.4253939
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.641239881515503
Epoch: 44, Steps: 59 | Train Loss: 0.5685197 Vali Loss: 1.4356961 Test Loss: 0.4254135
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.669776201248169
Epoch: 45, Steps: 59 | Train Loss: 0.5681851 Vali Loss: 1.4348141 Test Loss: 0.4254356
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.9344851970672607
Epoch: 46, Steps: 59 | Train Loss: 0.5682871 Vali Loss: 1.4405565 Test Loss: 0.4254614
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.5826196670532227
Epoch: 47, Steps: 59 | Train Loss: 0.5682725 Vali Loss: 1.4403110 Test Loss: 0.4254651
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.6837966442108154
Epoch: 48, Steps: 59 | Train Loss: 0.5681970 Vali Loss: 1.4403913 Test Loss: 0.4254787
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.5841455459594727
Epoch: 49, Steps: 59 | Train Loss: 0.5681340 Vali Loss: 1.4358284 Test Loss: 0.4255041
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.645794153213501
Epoch: 50, Steps: 59 | Train Loss: 0.5681850 Vali Loss: 1.4374310 Test Loss: 0.4255155
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6504454612731934
Epoch: 51, Steps: 59 | Train Loss: 0.5682274 Vali Loss: 1.4369265 Test Loss: 0.4255480
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.9424688816070557
Epoch: 52, Steps: 59 | Train Loss: 0.5680812 Vali Loss: 1.4325194 Test Loss: 0.4255509
Validation loss decreased (1.433656 --> 1.432519).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.602071762084961
Epoch: 53, Steps: 59 | Train Loss: 0.5681302 Vali Loss: 1.4372604 Test Loss: 0.4255572
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5551846027374268
Epoch: 54, Steps: 59 | Train Loss: 0.5681776 Vali Loss: 1.4384303 Test Loss: 0.4255767
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.632983684539795
Epoch: 55, Steps: 59 | Train Loss: 0.5680999 Vali Loss: 1.4311807 Test Loss: 0.4255964
Validation loss decreased (1.432519 --> 1.431181).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7409982681274414
Epoch: 56, Steps: 59 | Train Loss: 0.5680672 Vali Loss: 1.4367986 Test Loss: 0.4256115
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.82869291305542
Epoch: 57, Steps: 59 | Train Loss: 0.5681359 Vali Loss: 1.4385951 Test Loss: 0.4256263
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.594832181930542
Epoch: 58, Steps: 59 | Train Loss: 0.5678159 Vali Loss: 1.4436983 Test Loss: 0.4256429
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.5957977771759033
Epoch: 59, Steps: 59 | Train Loss: 0.5677269 Vali Loss: 1.4320220 Test Loss: 0.4256504
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.6948344707489014
Epoch: 60, Steps: 59 | Train Loss: 0.5678574 Vali Loss: 1.4345117 Test Loss: 0.4256671
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6075513362884521
Epoch: 61, Steps: 59 | Train Loss: 0.5678978 Vali Loss: 1.4331633 Test Loss: 0.4256757
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.884836196899414
Epoch: 62, Steps: 59 | Train Loss: 0.5680335 Vali Loss: 1.4340506 Test Loss: 0.4256890
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.8144128322601318
Epoch: 63, Steps: 59 | Train Loss: 0.5679175 Vali Loss: 1.4379177 Test Loss: 0.4257016
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.6273713111877441
Epoch: 64, Steps: 59 | Train Loss: 0.5679555 Vali Loss: 1.4346794 Test Loss: 0.4257195
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.0464913845062256
Epoch: 65, Steps: 59 | Train Loss: 0.5681236 Vali Loss: 1.4396949 Test Loss: 0.4257252
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.7106053829193115
Epoch: 66, Steps: 59 | Train Loss: 0.5680569 Vali Loss: 1.4379748 Test Loss: 0.4257329
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.6262459754943848
Epoch: 67, Steps: 59 | Train Loss: 0.5679101 Vali Loss: 1.4324827 Test Loss: 0.4257465
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.8650414943695068
Epoch: 68, Steps: 59 | Train Loss: 0.5680735 Vali Loss: 1.4348228 Test Loss: 0.4257472
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.5970869064331055
Epoch: 69, Steps: 59 | Train Loss: 0.5679204 Vali Loss: 1.4370434 Test Loss: 0.4257608
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.6215307712554932
Epoch: 70, Steps: 59 | Train Loss: 0.5678217 Vali Loss: 1.4337193 Test Loss: 0.4257702
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.6386864185333252
Epoch: 71, Steps: 59 | Train Loss: 0.5678916 Vali Loss: 1.4397652 Test Loss: 0.4257747
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.6495275497436523
Epoch: 72, Steps: 59 | Train Loss: 0.5678892 Vali Loss: 1.4381707 Test Loss: 0.4257954
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.6390061378479004
Epoch: 73, Steps: 59 | Train Loss: 0.5678766 Vali Loss: 1.4384980 Test Loss: 0.4257894
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.001372814178467
Epoch: 74, Steps: 59 | Train Loss: 0.5679767 Vali Loss: 1.4365041 Test Loss: 0.4258003
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.6722986698150635
Epoch: 75, Steps: 59 | Train Loss: 0.5678356 Vali Loss: 1.4352431 Test Loss: 0.4258087
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4245518147945404, mae:0.44606098532676697, rse:0.6237596869468689, corr:[0.22486527 0.23253103 0.23226963 0.23639414 0.23389935 0.2313571
 0.23320958 0.23413679 0.2327524  0.23288697 0.23353852 0.2326743
 0.2317351  0.23165487 0.23147246 0.23106103 0.23073469 0.2303698
 0.23028708 0.23038316 0.23033804 0.23051532 0.23150942 0.23215185
 0.23179582 0.23201399 0.23258081 0.23213062 0.2316399  0.23206739
 0.23210984 0.23119977 0.23074858 0.23117328 0.23113793 0.23048478
 0.23043273 0.23075718 0.23054515 0.23011388 0.23013885 0.23029307
 0.23023118 0.2299878  0.2297148  0.22976823 0.23009562 0.23029488
 0.2302766  0.23011346 0.22970037 0.22923143 0.22901607 0.228708
 0.22821538 0.22741885 0.2272702  0.22714323 0.22656403 0.22605538
 0.22582003 0.22562838 0.22524875 0.2250929  0.22531879 0.22568393
 0.22586465 0.22574976 0.22576934 0.2260739  0.22607219 0.22571966
 0.22529316 0.22490835 0.22440721 0.22408366 0.22412413 0.22405168
 0.2235082  0.22300154 0.22302118 0.22289993 0.22238603 0.22211462
 0.22206259 0.22173871 0.22130929 0.22123021 0.2212599  0.22106177
 0.22069833 0.22051008 0.22059828 0.22084802 0.22114259 0.22205135
 0.2233165  0.22404699 0.22448099 0.2250215  0.22546437 0.22546291
 0.2251566  0.22517034 0.22526918 0.22494292 0.22439116 0.2241109
 0.22399263 0.22375792 0.22356246 0.22358568 0.22374037 0.22373499
 0.22362213 0.22348432 0.22337148 0.2231662  0.22287731 0.22296813
 0.22323391 0.22300096 0.22249521 0.22234051 0.22226156 0.22186878
 0.22146447 0.22140214 0.22125955 0.22077909 0.22034849 0.22017133
 0.22003259 0.2197547  0.21958147 0.21966203 0.21975724 0.21971498
 0.21976419 0.21990342 0.21992548 0.2197788  0.21945745 0.21936643
 0.21928896 0.21896175 0.2187582  0.2184759  0.21784708 0.21727096
 0.21719691 0.21732862 0.21733156 0.21733709 0.21736898 0.21734293
 0.21723536 0.21713659 0.21701697 0.2169172  0.21686433 0.2168127
 0.21664487 0.2165579  0.21651691 0.21623534 0.21566415 0.21583498
 0.21639755 0.21683927 0.21762013 0.21864763 0.21890432 0.21871993
 0.21892487 0.21926883 0.21920735 0.2189682  0.21893558 0.2188442
 0.21849218 0.21829025 0.21840122 0.2184374  0.2184699  0.21858154
 0.21854907 0.21856828 0.2187159  0.21851474 0.21810392 0.2180926
 0.21810228 0.21773645 0.21731588 0.21707413 0.21660012 0.2159971
 0.21582352 0.2159488  0.21577804 0.21554689 0.21553135 0.21540655
 0.21518265 0.21536535 0.215652   0.21563354 0.21551472 0.21552917
 0.2154822  0.21519084 0.21488743 0.2146287  0.21429658 0.21434198
 0.21454193 0.21451905 0.21458939 0.21458435 0.21439566 0.21427402
 0.21430574 0.21437372 0.21421273 0.21382424 0.21340558 0.213032
 0.21261594 0.21243791 0.21251333 0.21253896 0.21238773 0.21221559
 0.21216524 0.2119872  0.21162765 0.21131314 0.21133228 0.21155103
 0.21178757 0.2118844  0.21216573 0.21246058 0.21259016 0.21275201
 0.21299952 0.21307513 0.21298112 0.21280672 0.21254377 0.21218432
 0.21198991 0.21206787 0.21231097 0.212406   0.21244985 0.21246083
 0.21244717 0.21236837 0.21225697 0.21215715 0.21209253 0.21211392
 0.21191701 0.21169803 0.21175264 0.2117328  0.2114501  0.21138051
 0.21144398 0.21129166 0.21106277 0.21106291 0.21093191 0.21063751
 0.21049319 0.210553   0.21047068 0.21025133 0.21002099 0.20992494
 0.20992433 0.20994952 0.20997532 0.21006662 0.21029049 0.21080373
 0.21137846 0.21191768 0.21241778 0.21261004 0.21271287 0.21299092
 0.21316266 0.21309392 0.2130914  0.21320242 0.21307036 0.21284628
 0.21276699 0.21280628 0.21289766 0.2129581  0.21301596 0.2129532
 0.212935   0.2129281  0.21283025 0.21269727 0.21257724 0.21288478
 0.21352373 0.21409123 0.21443741 0.21452673 0.2143948  0.21427414
 0.21409467 0.21389887 0.21377271 0.21353179 0.21302073 0.21269679
 0.2126531  0.21267182 0.21269235 0.21288568 0.21302792 0.21311185
 0.213344   0.21348794 0.21333757 0.21324901 0.21331312 0.21321215
 0.21309724 0.21321256 0.21328892 0.21292943 0.21257886 0.21237093
 0.21205746 0.2117023  0.2116068  0.21164137 0.21159114 0.21162316
 0.21177475 0.21183024 0.21186325 0.21211953 0.21220599 0.212198
 0.21232575 0.21238253 0.21208602 0.2118188  0.21151744 0.21133357
 0.2113527  0.21160613 0.21176538 0.21146104 0.21105772 0.21080408
 0.21051893 0.21006906 0.20986904 0.20983042 0.20943058 0.2089857
 0.20892955 0.20880163 0.208416   0.20806736 0.20798329 0.20799343
 0.2080217  0.20811619 0.20809266 0.20800903 0.2080855  0.20888121
 0.20996736 0.2109141  0.21178553 0.21212214 0.21201989 0.21169557
 0.21124262 0.21053146 0.21011995 0.20989871 0.20945115 0.20911795
 0.20907724 0.2091367  0.20920473 0.20933855 0.20950699 0.2097564
 0.21004899 0.21036115 0.21052997 0.2108105  0.21119039 0.21169835
 0.21205659 0.21226065 0.21225332 0.21204275 0.21187197 0.21167926
 0.21134403 0.2112132  0.2112472  0.21109499 0.21074526 0.21063751
 0.21079747 0.21077785 0.21045603 0.21025705 0.21021502 0.21018738
 0.21020544 0.21041568 0.21053259 0.21094844 0.21149516 0.21213484
 0.21266158 0.21290836 0.21295379 0.2129506  0.21294452 0.21277647
 0.21244703 0.21239655 0.21240093 0.21217294 0.21181811 0.21173461
 0.21189684 0.21180868 0.21159895 0.2115794  0.21164078 0.21157993
 0.21150176 0.2115764  0.21180719 0.21204287 0.2122386  0.21295595
 0.21379238 0.2142219  0.21433319 0.21431503 0.21398371 0.21355695
 0.21316727 0.21299352 0.2129834  0.21290867 0.21281308 0.21289739
 0.21305288 0.21297608 0.21293256 0.21315265 0.21334743 0.21334302
 0.21335576 0.2134676  0.21348056 0.21320947 0.21316832 0.21343488
 0.2136121  0.21348892 0.21327607 0.2132251  0.21314406 0.21288554
 0.21260422 0.21243165 0.21231808 0.21213473 0.21174362 0.21156709
 0.21152177 0.2112557  0.21109918 0.21129964 0.21141484 0.2112446
 0.21115442 0.21132123 0.21156819 0.21176924 0.21218105 0.21321201
 0.2142463  0.21482728 0.21522176 0.21553941 0.21551387 0.21503483
 0.21442485 0.2140775  0.21387431 0.21333562 0.21270293 0.21264318
 0.21284162 0.21273579 0.21278185 0.21327083 0.21368164 0.21366341
 0.2137219  0.21411067 0.21434097 0.21441393 0.21487264 0.21543314
 0.21536681 0.21506025 0.21518904 0.21537958 0.2149211  0.21417329
 0.21361463 0.21340679 0.21318845 0.21288411 0.21254611 0.21255542
 0.21253225 0.21248077 0.21265468 0.21318014 0.21353899 0.21352735
 0.21355748 0.2137632  0.21369627 0.21345067 0.21328022 0.21330778
 0.21303537 0.21265101 0.2125472  0.21219194 0.21145855 0.21093175
 0.21041775 0.20983307 0.2094832  0.20901431 0.2084389  0.20819402
 0.20812453 0.20771961 0.20750564 0.2076062  0.20765232 0.20757848
 0.20787466 0.20818044 0.20810899 0.20792785 0.20813474 0.20831293
 0.20809352 0.20799041 0.20790456 0.20770605 0.20734014 0.20683154
 0.20643526 0.20598133 0.2056578  0.20541881 0.20520867 0.20499116
 0.20482364 0.20444468 0.20444362 0.20459498 0.20451315 0.2044024
 0.20460847 0.20477884 0.20465696 0.20453975 0.20462042 0.20459954
 0.20433973 0.20400333 0.20382701 0.2033453  0.20259629 0.20202273
 0.20165557 0.2012984  0.20075999 0.2003676  0.20013048 0.19990978
 0.19967242 0.19930668 0.19921291 0.19945008 0.19939089 0.1991482
 0.19923307 0.19935557 0.19934614 0.19958216 0.2000214  0.20034105
 0.20032558 0.20006768 0.19995493 0.19952437 0.1988915  0.19850773
 0.19832397 0.19779532 0.19711526 0.1968472  0.19665432 0.1965237
 0.19654636 0.19645472 0.19649746 0.19638117 0.19605123 0.19607791
 0.19626996 0.19624135 0.1961955  0.19644451 0.19691762 0.19746129
 0.19783336 0.19783299 0.19737075 0.19712892 0.19677176 0.19665569
 0.19629881 0.1955936  0.19491911 0.19464794 0.19458705 0.1943098
 0.1940375  0.19403064 0.19409834 0.19374461 0.19372934 0.19406445
 0.19426966 0.1940312  0.19400308 0.19425584 0.19425325 0.19417985
 0.19395752 0.19322206 0.19228646 0.19146144 0.19094348 0.19013005
 0.18911608 0.18854319 0.18847807 0.18824519 0.18791029 0.1878625
 0.1882348  0.1882866  0.18802631 0.18794118 0.18808047 0.18855177
 0.18875854 0.1887865  0.18926223 0.18986923 0.18981397 0.18971331
 0.19018483 0.19009174 0.18941773 0.18924658 0.18924485 0.18841349
 0.18765152 0.18781774 0.18812121 0.1875926  0.18695892 0.18697636
 0.18713228 0.18669493 0.18639089 0.18564335 0.18572019 0.18611151
 0.1859073  0.18602659 0.1869819  0.1844189  0.18386516 0.18305841]
