Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=26, out_features=130, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3028480.0
params:  3510.0
Trainable parameters:  3510
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.8670380115509033
Epoch: 1, Steps: 60 | Train Loss: 1.1922138 Vali Loss: 2.4059451 Test Loss: 1.1026591
Validation loss decreased (inf --> 2.405945).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.1896352767944336
Epoch: 2, Steps: 60 | Train Loss: 0.9243645 Vali Loss: 2.0071878 Test Loss: 0.8152260
Validation loss decreased (2.405945 --> 2.007188).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.6687636375427246
Epoch: 3, Steps: 60 | Train Loss: 0.7928689 Vali Loss: 1.8170681 Test Loss: 0.6724511
Validation loss decreased (2.007188 --> 1.817068).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.346139907836914
Epoch: 4, Steps: 60 | Train Loss: 0.7209187 Vali Loss: 1.7133737 Test Loss: 0.5924242
Validation loss decreased (1.817068 --> 1.713374).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.122194051742554
Epoch: 5, Steps: 60 | Train Loss: 0.6790680 Vali Loss: 1.6518924 Test Loss: 0.5439522
Validation loss decreased (1.713374 --> 1.651892).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.034692764282227
Epoch: 6, Steps: 60 | Train Loss: 0.6540045 Vali Loss: 1.6087242 Test Loss: 0.5132610
Validation loss decreased (1.651892 --> 1.608724).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.3111212253570557
Epoch: 7, Steps: 60 | Train Loss: 0.6375752 Vali Loss: 1.5881524 Test Loss: 0.4934919
Validation loss decreased (1.608724 --> 1.588152).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.7246811389923096
Epoch: 8, Steps: 60 | Train Loss: 0.6272025 Vali Loss: 1.5658540 Test Loss: 0.4802972
Validation loss decreased (1.588152 --> 1.565854).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.044254541397095
Epoch: 9, Steps: 60 | Train Loss: 0.6185776 Vali Loss: 1.5600445 Test Loss: 0.4711429
Validation loss decreased (1.565854 --> 1.560045).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.7833642959594727
Epoch: 10, Steps: 60 | Train Loss: 0.6141652 Vali Loss: 1.5486622 Test Loss: 0.4647223
Validation loss decreased (1.560045 --> 1.548662).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.897338628768921
Epoch: 11, Steps: 60 | Train Loss: 0.6099981 Vali Loss: 1.5357661 Test Loss: 0.4600431
Validation loss decreased (1.548662 --> 1.535766).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.5300843715667725
Epoch: 12, Steps: 60 | Train Loss: 0.6068772 Vali Loss: 1.5296195 Test Loss: 0.4565154
Validation loss decreased (1.535766 --> 1.529619).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.06955361366272
Epoch: 13, Steps: 60 | Train Loss: 0.6047218 Vali Loss: 1.5347941 Test Loss: 0.4538945
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6597929000854492
Epoch: 14, Steps: 60 | Train Loss: 0.6028092 Vali Loss: 1.5251075 Test Loss: 0.4518374
Validation loss decreased (1.529619 --> 1.525108).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.8268933296203613
Epoch: 15, Steps: 60 | Train Loss: 0.6014466 Vali Loss: 1.5209107 Test Loss: 0.4502871
Validation loss decreased (1.525108 --> 1.520911).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.8988893032073975
Epoch: 16, Steps: 60 | Train Loss: 0.6002786 Vali Loss: 1.5234492 Test Loss: 0.4489563
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.958475112915039
Epoch: 17, Steps: 60 | Train Loss: 0.5993655 Vali Loss: 1.5233196 Test Loss: 0.4479705
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.3487918376922607
Epoch: 18, Steps: 60 | Train Loss: 0.5983802 Vali Loss: 1.5111315 Test Loss: 0.4470969
Validation loss decreased (1.520911 --> 1.511132).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.3958313465118408
Epoch: 19, Steps: 60 | Train Loss: 0.5981474 Vali Loss: 1.5171793 Test Loss: 0.4464061
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.4224872589111328
Epoch: 20, Steps: 60 | Train Loss: 0.5971570 Vali Loss: 1.5094012 Test Loss: 0.4458556
Validation loss decreased (1.511132 --> 1.509401).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.394317626953125
Epoch: 21, Steps: 60 | Train Loss: 0.5973481 Vali Loss: 1.5087255 Test Loss: 0.4453888
Validation loss decreased (1.509401 --> 1.508726).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2037534713745117
Epoch: 22, Steps: 60 | Train Loss: 0.5964109 Vali Loss: 1.5157018 Test Loss: 0.4450065
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3623831272125244
Epoch: 23, Steps: 60 | Train Loss: 0.5961363 Vali Loss: 1.5137485 Test Loss: 0.4446844
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.285459280014038
Epoch: 24, Steps: 60 | Train Loss: 0.5950251 Vali Loss: 1.5116271 Test Loss: 0.4444410
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.305309772491455
Epoch: 25, Steps: 60 | Train Loss: 0.5955424 Vali Loss: 1.5124791 Test Loss: 0.4442185
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2782056331634521
Epoch: 26, Steps: 60 | Train Loss: 0.5946608 Vali Loss: 1.5144584 Test Loss: 0.4440517
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.3415234088897705
Epoch: 27, Steps: 60 | Train Loss: 0.5953004 Vali Loss: 1.5098276 Test Loss: 0.4439039
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.3637919425964355
Epoch: 28, Steps: 60 | Train Loss: 0.5947220 Vali Loss: 1.5066801 Test Loss: 0.4437923
Validation loss decreased (1.508726 --> 1.506680).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.4663057327270508
Epoch: 29, Steps: 60 | Train Loss: 0.5943127 Vali Loss: 1.5112252 Test Loss: 0.4436664
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4737248420715332
Epoch: 30, Steps: 60 | Train Loss: 0.5938020 Vali Loss: 1.5083392 Test Loss: 0.4436109
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.3622245788574219
Epoch: 31, Steps: 60 | Train Loss: 0.5941195 Vali Loss: 1.5083218 Test Loss: 0.4435206
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.294262409210205
Epoch: 32, Steps: 60 | Train Loss: 0.5941084 Vali Loss: 1.5078691 Test Loss: 0.4435065
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4239699840545654
Epoch: 33, Steps: 60 | Train Loss: 0.5937398 Vali Loss: 1.4993120 Test Loss: 0.4434428
Validation loss decreased (1.506680 --> 1.499312).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.344691276550293
Epoch: 34, Steps: 60 | Train Loss: 0.5934625 Vali Loss: 1.5039123 Test Loss: 0.4434249
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.2940237522125244
Epoch: 35, Steps: 60 | Train Loss: 0.5940052 Vali Loss: 1.5062422 Test Loss: 0.4434126
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.2802989482879639
Epoch: 36, Steps: 60 | Train Loss: 0.5936180 Vali Loss: 1.5019766 Test Loss: 0.4433937
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.3042893409729004
Epoch: 37, Steps: 60 | Train Loss: 0.5935228 Vali Loss: 1.5060788 Test Loss: 0.4433758
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.447798728942871
Epoch: 38, Steps: 60 | Train Loss: 0.5931124 Vali Loss: 1.5074170 Test Loss: 0.4433571
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.327470302581787
Epoch: 39, Steps: 60 | Train Loss: 0.5929315 Vali Loss: 1.4959573 Test Loss: 0.4433559
Validation loss decreased (1.499312 --> 1.495957).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.280486822128296
Epoch: 40, Steps: 60 | Train Loss: 0.5929022 Vali Loss: 1.4955856 Test Loss: 0.4433651
Validation loss decreased (1.495957 --> 1.495586).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.3669512271881104
Epoch: 41, Steps: 60 | Train Loss: 0.5934641 Vali Loss: 1.5028545 Test Loss: 0.4433661
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.312373399734497
Epoch: 42, Steps: 60 | Train Loss: 0.5929688 Vali Loss: 1.4960101 Test Loss: 0.4433860
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.248155117034912
Epoch: 43, Steps: 60 | Train Loss: 0.5928192 Vali Loss: 1.5055925 Test Loss: 0.4433734
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.423896312713623
Epoch: 44, Steps: 60 | Train Loss: 0.5928786 Vali Loss: 1.5074756 Test Loss: 0.4433988
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.287463903427124
Epoch: 45, Steps: 60 | Train Loss: 0.5927513 Vali Loss: 1.4987984 Test Loss: 0.4434052
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.279906988143921
Epoch: 46, Steps: 60 | Train Loss: 0.5925935 Vali Loss: 1.4979141 Test Loss: 0.4434275
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.3802199363708496
Epoch: 47, Steps: 60 | Train Loss: 0.5930265 Vali Loss: 1.5080650 Test Loss: 0.4434324
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.2524535655975342
Epoch: 48, Steps: 60 | Train Loss: 0.5929067 Vali Loss: 1.5012796 Test Loss: 0.4434303
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.3419485092163086
Epoch: 49, Steps: 60 | Train Loss: 0.5929272 Vali Loss: 1.5019042 Test Loss: 0.4434546
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.4005939960479736
Epoch: 50, Steps: 60 | Train Loss: 0.5926483 Vali Loss: 1.5029929 Test Loss: 0.4434603
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3736329078674316
Epoch: 51, Steps: 60 | Train Loss: 0.5920460 Vali Loss: 1.5058571 Test Loss: 0.4434832
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.4040491580963135
Epoch: 52, Steps: 60 | Train Loss: 0.5922339 Vali Loss: 1.5014255 Test Loss: 0.4434834
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6060802936553955
Epoch: 53, Steps: 60 | Train Loss: 0.5925491 Vali Loss: 1.5017461 Test Loss: 0.4435001
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.4213943481445312
Epoch: 54, Steps: 60 | Train Loss: 0.5921540 Vali Loss: 1.5040216 Test Loss: 0.4435029
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.4721548557281494
Epoch: 55, Steps: 60 | Train Loss: 0.5924578 Vali Loss: 1.5070374 Test Loss: 0.4435203
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.357783555984497
Epoch: 56, Steps: 60 | Train Loss: 0.5924256 Vali Loss: 1.4983923 Test Loss: 0.4435323
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.3540632724761963
Epoch: 57, Steps: 60 | Train Loss: 0.5922448 Vali Loss: 1.5027416 Test Loss: 0.4435405
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.510951280593872
Epoch: 58, Steps: 60 | Train Loss: 0.5917349 Vali Loss: 1.5027673 Test Loss: 0.4435698
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.3423962593078613
Epoch: 59, Steps: 60 | Train Loss: 0.5922762 Vali Loss: 1.5044243 Test Loss: 0.4435709
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.4356341361999512
Epoch: 60, Steps: 60 | Train Loss: 0.5923552 Vali Loss: 1.5059178 Test Loss: 0.4435898
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.44225314259529114, mae:0.4542301595211029, rse:0.6366304755210876, corr:[0.2274303  0.23252992 0.23171665 0.22870569 0.22646052 0.22597276
 0.22701266 0.2288786  0.2299373  0.2300475  0.22942838 0.22842783
 0.22736494 0.22630414 0.22565041 0.2256167  0.2258713  0.22614467
 0.22622402 0.2262044  0.22631189 0.22663534 0.2268613  0.226678
 0.2261764  0.2260934  0.22612035 0.22584821 0.2250353  0.22412881
 0.22353448 0.22339433 0.22350056 0.22380468 0.22425199 0.22474046
 0.22511098 0.22496341 0.22463132 0.22440109 0.22441684 0.22448568
 0.22469333 0.22521067 0.22583124 0.22628233 0.22641627 0.22589773
 0.22453508 0.22340646 0.22237222 0.22150928 0.22072208 0.21999359
 0.21953157 0.21934755 0.219119   0.2189124  0.21863674 0.21880049
 0.21925275 0.21949224 0.21957631 0.21965122 0.2196648  0.21958072
 0.21947406 0.21944341 0.21949312 0.21944189 0.21907684 0.21826106
 0.21700437 0.21626766 0.21583572 0.21555206 0.21519867 0.214693
 0.21444097 0.21450159 0.21451044 0.2144902  0.21433292 0.2143657
 0.21462056 0.21455733 0.21430653 0.21394311 0.213542   0.2131659
 0.21296446 0.21318562 0.21366379 0.21429016 0.21493912 0.21532948
 0.21518354 0.21517242 0.21531229 0.21519099 0.21481597 0.21444596
 0.21422338 0.21445385 0.21450923 0.2143448  0.21404593 0.2141572
 0.21460475 0.21485758 0.21489011 0.2147939  0.21444955 0.21384445
 0.2133527  0.2131871  0.21329777 0.21345346 0.2134245  0.21296777
 0.2119672  0.21112858 0.2102185  0.20944463 0.20876677 0.20840238
 0.20846115 0.20885707 0.20901687 0.20903902 0.20906797 0.20948324
 0.21008493 0.21027166 0.21030068 0.2102206  0.20998324 0.20964596
 0.20940816 0.20946124 0.20971645 0.20995563 0.20997773 0.20962708
 0.20881437 0.20809194 0.20747584 0.2065901  0.20579015 0.20531185
 0.20533204 0.20574038 0.20630066 0.20673418 0.20686597 0.20702499
 0.2072665  0.20723923 0.20718057 0.20710918 0.20691425 0.20670019
 0.20659791 0.20671743 0.20706373 0.2074719  0.20778474 0.20784305
 0.20763119 0.20769137 0.20767793 0.20732051 0.20674379 0.20625143
 0.20622118 0.20678753 0.20737131 0.20791705 0.20834276 0.2089209
 0.20958567 0.20981586 0.20977473 0.20961446 0.20935601 0.20909388
 0.20904024 0.20928706 0.2096982  0.21000378 0.2100043  0.20950729
 0.20849182 0.20762604 0.20682867 0.20602094 0.2052781  0.20476238
 0.20464014 0.20492248 0.20525655 0.20549367 0.20556961 0.20599498
 0.20654845 0.20677549 0.20672433 0.20643346 0.20602433 0.20553191
 0.205146   0.20503841 0.20515807 0.20534572 0.20534769 0.20507585
 0.20455158 0.20426065 0.20402914 0.20362557 0.20322412 0.20282032
 0.20274061 0.20296293 0.20315744 0.20321356 0.20315881 0.20335528
 0.2038206  0.20393646 0.20380458 0.2035212  0.20307302 0.20260575
 0.20233151 0.20230518 0.20243448 0.20251669 0.20242381 0.20207447
 0.20150071 0.20119135 0.2009886  0.20080501 0.20059144 0.20048273
 0.20075952 0.20120302 0.20152205 0.20171167 0.20188472 0.20244874
 0.20324083 0.20361596 0.20376226 0.20370397 0.20349365 0.2031389
 0.20277871 0.20245206 0.20229955 0.20225704 0.20214519 0.20185669
 0.20132595 0.20102121 0.20070882 0.2002129  0.19964433 0.19921461
 0.19920762 0.1996138  0.19987087 0.19995543 0.1999274  0.20005314
 0.20046693 0.20067723 0.20067798 0.2004524  0.20000601 0.19956222
 0.19937338 0.19962618 0.20015909 0.20073901 0.20122698 0.20147173
 0.20141564 0.20147897 0.2016156  0.20164274 0.20161618 0.20182946
 0.20232081 0.20291291 0.20330575 0.2035074  0.20356812 0.20382397
 0.20426822 0.20446828 0.20456141 0.20462747 0.20453951 0.20432699
 0.20415252 0.20402469 0.20389546 0.20391385 0.20392834 0.20391539
 0.20375702 0.20381445 0.20383714 0.20356496 0.20320638 0.20274526
 0.20255436 0.2026811  0.20269565 0.20258959 0.20245336 0.20264354
 0.2031756  0.20349492 0.20373875 0.2040073  0.20400456 0.20370024
 0.20333421 0.20312367 0.20315157 0.2032962  0.20341085 0.2033796
 0.20310159 0.2029458  0.20265743 0.20215574 0.20153736 0.20105703
 0.2010555  0.20141973 0.20183156 0.20218047 0.20253618 0.20304216
 0.20351146 0.20367168 0.20370388 0.203656   0.20341465 0.20307766
 0.20275919 0.20261098 0.20251773 0.20249264 0.20227705 0.20197164
 0.20156749 0.20137003 0.20120396 0.20086561 0.20042676 0.20011343
 0.20017162 0.20046495 0.20069227 0.20072986 0.20065908 0.20068145
 0.20079984 0.20075881 0.20071131 0.20060256 0.2003647  0.19993353
 0.19954723 0.19933595 0.1994777  0.19994588 0.20047063 0.20101346
 0.20139329 0.20188151 0.20213564 0.20191304 0.20138599 0.20071149
 0.20046555 0.20062587 0.20077255 0.20087823 0.20090127 0.20133093
 0.20208375 0.20231512 0.20219742 0.20184483 0.2014383  0.20108032
 0.20098571 0.20113224 0.2014507  0.2017819  0.20193884 0.20197916
 0.20185031 0.20196356 0.20216213 0.20205858 0.2017515  0.20142028
 0.20149377 0.20194504 0.20241587 0.20273937 0.20303194 0.20350292
 0.20400043 0.20402162 0.20371522 0.20336623 0.20302263 0.20277208
 0.20278285 0.20311473 0.20351273 0.20416255 0.20465912 0.20484558
 0.20462404 0.20465595 0.20472097 0.20453495 0.20431344 0.20416485
 0.20429808 0.20479019 0.20522602 0.20557877 0.20577107 0.20616055
 0.20659551 0.20657778 0.20632519 0.20608218 0.2058298  0.2055936
 0.2055564  0.205788   0.2062164  0.20676751 0.20721023 0.20746006
 0.20734146 0.2073613  0.20721775 0.20668639 0.20597881 0.2053101
 0.20509951 0.20538685 0.20588796 0.20636697 0.20679927 0.20741354
 0.20815404 0.20842664 0.20846802 0.20837854 0.20820817 0.2080226
 0.20784807 0.20782606 0.20794262 0.20799655 0.20801449 0.20781492
 0.20724538 0.20684543 0.20653775 0.20615612 0.20583345 0.20558903
 0.20572408 0.20617098 0.20660932 0.20690186 0.20695496 0.20720607
 0.20771016 0.20783706 0.20772873 0.2075305  0.20730996 0.20709954
 0.20713237 0.20753115 0.20818067 0.20889892 0.20953916 0.20990217
 0.20986883 0.21006162 0.21024449 0.21006437 0.20965664 0.20918801
 0.2089591  0.20894025 0.20898639 0.20884904 0.20877779 0.20918863
 0.20983751 0.21004516 0.21006007 0.21005645 0.21005453 0.21008423
 0.21024618 0.21066771 0.21127443 0.21191192 0.21244563 0.21253248
 0.21201625 0.21168256 0.21137471 0.21076147 0.20987432 0.20905055
 0.2087999  0.20914449 0.20953009 0.2097391  0.20976666 0.21006404
 0.21069181 0.21089278 0.21085459 0.21071093 0.21059108 0.21058248
 0.21063797 0.2107794  0.21093158 0.2109474  0.21075909 0.21017928
 0.20907159 0.20836332 0.20801751 0.20752668 0.2067947  0.20622514
 0.20604031 0.20607373 0.20628057 0.2064693  0.20652162 0.20690267
 0.20759958 0.20757781 0.20717616 0.2067669  0.20658502 0.20655687
 0.20658895 0.20660791 0.20669298 0.2067489  0.20671634 0.20626712
 0.20529798 0.2048201  0.20457989 0.20409131 0.20335628 0.2023033
 0.20168145 0.2015328  0.20163007 0.20164153 0.20140001 0.20141704
 0.20160253 0.20126127 0.20058489 0.19997014 0.19955571 0.19930044
 0.19924569 0.19933864 0.19944154 0.19943927 0.19921385 0.19859971
 0.19741017 0.19656566 0.19601706 0.19519801 0.19416216 0.19331118
 0.19292967 0.1929816  0.1930425  0.19284993 0.19249484 0.19245148
 0.19292282 0.19296418 0.19269802 0.19240224 0.19210978 0.191848
 0.19166562 0.19165525 0.19178617 0.19190843 0.1918573  0.19128104
 0.19010904 0.1892657  0.18883514 0.18832943 0.18781559 0.18729319
 0.18705507 0.18701437 0.18696655 0.18693024 0.18681638 0.18707849
 0.18779284 0.187808   0.1872854  0.18672162 0.18618314 0.18587524
 0.18587187 0.18618163 0.18658046 0.18686716 0.18688114 0.18633927
 0.18518904 0.18432264 0.18369685 0.1830804  0.18238671 0.18174213
 0.18157527 0.18178895 0.18217899 0.18238682 0.18239751 0.18267924
 0.18327066 0.18331373 0.18304057 0.18278986 0.18271299 0.18267557
 0.18260233 0.1825163  0.18242437 0.18237127 0.18215932 0.18147334
 0.18000813 0.17876537 0.17796662 0.17698227 0.17595373 0.1749838
 0.17449342 0.1743954  0.17470448 0.17505404 0.17547137 0.1763093
 0.17730108 0.17731594 0.17674787 0.17619154 0.17583469 0.17572437
 0.17584135 0.17619474 0.17657049 0.1768163  0.17679504 0.17624299
 0.17529804 0.17492889 0.17472671 0.17410679 0.17309168 0.17218946
 0.17180295 0.17189646 0.17221949 0.17218786 0.17170185 0.17142527
 0.1716216  0.17123553 0.17076206 0.17069438 0.17091008 0.17084867
 0.1703541  0.16995342 0.1704599  0.17131959 0.16998339 0.16147603]
