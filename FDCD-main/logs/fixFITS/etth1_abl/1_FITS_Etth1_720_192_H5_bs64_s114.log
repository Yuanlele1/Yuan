Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=165, out_features=209, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30898560.0
params:  34694.0
Trainable parameters:  34694
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.465608835220337
Epoch: 1, Steps: 60 | Train Loss: 0.6655381 Vali Loss: 1.2275273 Test Loss: 0.5779789
Validation loss decreased (inf --> 1.227527).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.4721341133117676
Epoch: 2, Steps: 60 | Train Loss: 0.4920808 Vali Loss: 1.0790782 Test Loss: 0.4819149
Validation loss decreased (1.227527 --> 1.079078).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.522515058517456
Epoch: 3, Steps: 60 | Train Loss: 0.4403676 Vali Loss: 1.0186995 Test Loss: 0.4430842
Validation loss decreased (1.079078 --> 1.018700).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4269795417785645
Epoch: 4, Steps: 60 | Train Loss: 0.4177092 Vali Loss: 0.9919845 Test Loss: 0.4267361
Validation loss decreased (1.018700 --> 0.991984).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.4500691890716553
Epoch: 5, Steps: 60 | Train Loss: 0.4060380 Vali Loss: 0.9798768 Test Loss: 0.4202099
Validation loss decreased (0.991984 --> 0.979877).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.6806044578552246
Epoch: 6, Steps: 60 | Train Loss: 0.3997125 Vali Loss: 0.9737169 Test Loss: 0.4180034
Validation loss decreased (0.979877 --> 0.973717).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.624798059463501
Epoch: 7, Steps: 60 | Train Loss: 0.3962976 Vali Loss: 0.9710673 Test Loss: 0.4176112
Validation loss decreased (0.973717 --> 0.971067).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5378704071044922
Epoch: 8, Steps: 60 | Train Loss: 0.3937539 Vali Loss: 0.9693570 Test Loss: 0.4172577
Validation loss decreased (0.971067 --> 0.969357).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4782826900482178
Epoch: 9, Steps: 60 | Train Loss: 0.3921207 Vali Loss: 0.9678934 Test Loss: 0.4175183
Validation loss decreased (0.969357 --> 0.967893).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.519779920578003
Epoch: 10, Steps: 60 | Train Loss: 0.3899451 Vali Loss: 0.9672929 Test Loss: 0.4175478
Validation loss decreased (0.967893 --> 0.967293).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.431588888168335
Epoch: 11, Steps: 60 | Train Loss: 0.3889831 Vali Loss: 0.9669358 Test Loss: 0.4178579
Validation loss decreased (0.967293 --> 0.966936).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.524545669555664
Epoch: 12, Steps: 60 | Train Loss: 0.3882584 Vali Loss: 0.9665352 Test Loss: 0.4179288
Validation loss decreased (0.966936 --> 0.966535).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5002565383911133
Epoch: 13, Steps: 60 | Train Loss: 0.3874881 Vali Loss: 0.9663764 Test Loss: 0.4180741
Validation loss decreased (0.966535 --> 0.966376).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5788030624389648
Epoch: 14, Steps: 60 | Train Loss: 0.3866891 Vali Loss: 0.9656243 Test Loss: 0.4180373
Validation loss decreased (0.966376 --> 0.965624).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4194855690002441
Epoch: 15, Steps: 60 | Train Loss: 0.3858703 Vali Loss: 0.9656535 Test Loss: 0.4185954
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.5897619724273682
Epoch: 16, Steps: 60 | Train Loss: 0.3853838 Vali Loss: 0.9657326 Test Loss: 0.4184775
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4720699787139893
Epoch: 17, Steps: 60 | Train Loss: 0.3849124 Vali Loss: 0.9654961 Test Loss: 0.4185349
Validation loss decreased (0.965624 --> 0.965496).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4463953971862793
Epoch: 18, Steps: 60 | Train Loss: 0.3845697 Vali Loss: 0.9647729 Test Loss: 0.4184176
Validation loss decreased (0.965496 --> 0.964773).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5640780925750732
Epoch: 19, Steps: 60 | Train Loss: 0.3838670 Vali Loss: 0.9649199 Test Loss: 0.4184723
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.4550983905792236
Epoch: 20, Steps: 60 | Train Loss: 0.3836915 Vali Loss: 0.9647510 Test Loss: 0.4184307
Validation loss decreased (0.964773 --> 0.964751).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.606616735458374
Epoch: 21, Steps: 60 | Train Loss: 0.3835811 Vali Loss: 0.9645138 Test Loss: 0.4186647
Validation loss decreased (0.964751 --> 0.964514).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5499544143676758
Epoch: 22, Steps: 60 | Train Loss: 0.3834006 Vali Loss: 0.9645660 Test Loss: 0.4188207
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.4589736461639404
Epoch: 23, Steps: 60 | Train Loss: 0.3828880 Vali Loss: 0.9648080 Test Loss: 0.4187119
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.548724889755249
Epoch: 24, Steps: 60 | Train Loss: 0.3824739 Vali Loss: 0.9641589 Test Loss: 0.4187404
Validation loss decreased (0.964514 --> 0.964159).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.551820993423462
Epoch: 25, Steps: 60 | Train Loss: 0.3822454 Vali Loss: 0.9641309 Test Loss: 0.4186991
Validation loss decreased (0.964159 --> 0.964131).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4993808269500732
Epoch: 26, Steps: 60 | Train Loss: 0.3821216 Vali Loss: 0.9648530 Test Loss: 0.4186716
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6600558757781982
Epoch: 27, Steps: 60 | Train Loss: 0.3823190 Vali Loss: 0.9646676 Test Loss: 0.4188627
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5661494731903076
Epoch: 28, Steps: 60 | Train Loss: 0.3820787 Vali Loss: 0.9644588 Test Loss: 0.4187843
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.5675621032714844
Epoch: 29, Steps: 60 | Train Loss: 0.3816908 Vali Loss: 0.9649794 Test Loss: 0.4189646
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4284331798553467
Epoch: 30, Steps: 60 | Train Loss: 0.3816334 Vali Loss: 0.9646782 Test Loss: 0.4188969
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.4905765056610107
Epoch: 31, Steps: 60 | Train Loss: 0.3813298 Vali Loss: 0.9644102 Test Loss: 0.4190160
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.433030605316162
Epoch: 32, Steps: 60 | Train Loss: 0.3813736 Vali Loss: 0.9640848 Test Loss: 0.4191031
Validation loss decreased (0.964131 --> 0.964085).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5864641666412354
Epoch: 33, Steps: 60 | Train Loss: 0.3811141 Vali Loss: 0.9643297 Test Loss: 0.4191620
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5660021305084229
Epoch: 34, Steps: 60 | Train Loss: 0.3812105 Vali Loss: 0.9642867 Test Loss: 0.4191192
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.448664665222168
Epoch: 35, Steps: 60 | Train Loss: 0.3810616 Vali Loss: 0.9643431 Test Loss: 0.4192007
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3342952728271484
Epoch: 36, Steps: 60 | Train Loss: 0.3807489 Vali Loss: 0.9644231 Test Loss: 0.4192390
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5600402355194092
Epoch: 37, Steps: 60 | Train Loss: 0.3806822 Vali Loss: 0.9643542 Test Loss: 0.4192350
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.514901876449585
Epoch: 38, Steps: 60 | Train Loss: 0.3808086 Vali Loss: 0.9641557 Test Loss: 0.4191513
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.508699655532837
Epoch: 39, Steps: 60 | Train Loss: 0.3807599 Vali Loss: 0.9636076 Test Loss: 0.4191897
Validation loss decreased (0.964085 --> 0.963608).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.5205087661743164
Epoch: 40, Steps: 60 | Train Loss: 0.3806164 Vali Loss: 0.9641490 Test Loss: 0.4192920
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6785109043121338
Epoch: 41, Steps: 60 | Train Loss: 0.3803245 Vali Loss: 0.9638634 Test Loss: 0.4192333
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.6388168334960938
Epoch: 42, Steps: 60 | Train Loss: 0.3803657 Vali Loss: 0.9641981 Test Loss: 0.4192727
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.543044090270996
Epoch: 43, Steps: 60 | Train Loss: 0.3802985 Vali Loss: 0.9644576 Test Loss: 0.4193583
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.522681713104248
Epoch: 44, Steps: 60 | Train Loss: 0.3803074 Vali Loss: 0.9638900 Test Loss: 0.4192495
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.552290439605713
Epoch: 45, Steps: 60 | Train Loss: 0.3803407 Vali Loss: 0.9643630 Test Loss: 0.4193725
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6142768859863281
Epoch: 46, Steps: 60 | Train Loss: 0.3801284 Vali Loss: 0.9643301 Test Loss: 0.4193953
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.4762940406799316
Epoch: 47, Steps: 60 | Train Loss: 0.3799011 Vali Loss: 0.9641314 Test Loss: 0.4193565
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.6410083770751953
Epoch: 48, Steps: 60 | Train Loss: 0.3802284 Vali Loss: 0.9634085 Test Loss: 0.4193005
Validation loss decreased (0.963608 --> 0.963409).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.610536813735962
Epoch: 49, Steps: 60 | Train Loss: 0.3800755 Vali Loss: 0.9639201 Test Loss: 0.4193473
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6696820259094238
Epoch: 50, Steps: 60 | Train Loss: 0.3802227 Vali Loss: 0.9640945 Test Loss: 0.4193671
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6246519088745117
Epoch: 51, Steps: 60 | Train Loss: 0.3798304 Vali Loss: 0.9643407 Test Loss: 0.4193650
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6000308990478516
Epoch: 52, Steps: 60 | Train Loss: 0.3801488 Vali Loss: 0.9639553 Test Loss: 0.4193772
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6699600219726562
Epoch: 53, Steps: 60 | Train Loss: 0.3799151 Vali Loss: 0.9639065 Test Loss: 0.4193903
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5226378440856934
Epoch: 54, Steps: 60 | Train Loss: 0.3799422 Vali Loss: 0.9641623 Test Loss: 0.4194062
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.5858337879180908
Epoch: 55, Steps: 60 | Train Loss: 0.3799456 Vali Loss: 0.9641188 Test Loss: 0.4193992
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.553898811340332
Epoch: 56, Steps: 60 | Train Loss: 0.3797363 Vali Loss: 0.9642537 Test Loss: 0.4193860
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.569028377532959
Epoch: 57, Steps: 60 | Train Loss: 0.3800915 Vali Loss: 0.9638658 Test Loss: 0.4194494
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.5129637718200684
Epoch: 58, Steps: 60 | Train Loss: 0.3799848 Vali Loss: 0.9642106 Test Loss: 0.4194024
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.6705658435821533
Epoch: 59, Steps: 60 | Train Loss: 0.3799453 Vali Loss: 0.9642668 Test Loss: 0.4194643
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.4739139080047607
Epoch: 60, Steps: 60 | Train Loss: 0.3800548 Vali Loss: 0.9642096 Test Loss: 0.4194535
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6134755611419678
Epoch: 61, Steps: 60 | Train Loss: 0.3796487 Vali Loss: 0.9642270 Test Loss: 0.4194151
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6954920291900635
Epoch: 62, Steps: 60 | Train Loss: 0.3800253 Vali Loss: 0.9639117 Test Loss: 0.4194733
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.6535389423370361
Epoch: 63, Steps: 60 | Train Loss: 0.3798305 Vali Loss: 0.9640567 Test Loss: 0.4194531
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.554805040359497
Epoch: 64, Steps: 60 | Train Loss: 0.3794843 Vali Loss: 0.9638585 Test Loss: 0.4194639
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.4859368801116943
Epoch: 65, Steps: 60 | Train Loss: 0.3800494 Vali Loss: 0.9642852 Test Loss: 0.4194580
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.6496636867523193
Epoch: 66, Steps: 60 | Train Loss: 0.3796016 Vali Loss: 0.9642456 Test Loss: 0.4194545
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.4949743747711182
Epoch: 67, Steps: 60 | Train Loss: 0.3796726 Vali Loss: 0.9643438 Test Loss: 0.4194709
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.5421373844146729
Epoch: 68, Steps: 60 | Train Loss: 0.3792060 Vali Loss: 0.9644498 Test Loss: 0.4194896
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.41554492712020874, mae:0.42386674880981445, rse:0.6121619343757629, corr:[0.25661024 0.26842177 0.2685895  0.2677363  0.26700714 0.26486233
 0.26281512 0.26240155 0.26302382 0.26347005 0.2631988  0.26265162
 0.26253545 0.26259843 0.26266354 0.26275238 0.2627769  0.26266217
 0.2623205  0.2619302  0.26173556 0.26175782 0.26163206 0.2616161
 0.26157504 0.2613767  0.2610604  0.26092127 0.26084858 0.26054046
 0.25996983 0.2594797  0.25943005 0.2595881  0.25946125 0.25928804
 0.2593278  0.2594879  0.2595227  0.25950035 0.25980648 0.2601994
 0.26030082 0.26013508 0.2601523  0.2604314  0.26079297 0.2608572
 0.26032406 0.25951573 0.25851193 0.25739533 0.25624812 0.25515902
 0.25444296 0.25410342 0.25382122 0.25365964 0.25350037 0.25347587
 0.25340027 0.25321808 0.2529833  0.25291306 0.2529597  0.2531457
 0.25352082 0.25366017 0.25351018 0.25326994 0.25321603 0.25312802
 0.25275987 0.251942   0.2510376  0.25070405 0.25069967 0.2504237
 0.24980545 0.24925719 0.2490306  0.24891989 0.24858189 0.24823669
 0.24821872 0.24832502 0.24817564 0.24783894 0.2476494  0.24770819
 0.24762401 0.2473414  0.24726258 0.24753872 0.24787246 0.24817671
 0.24863362 0.24906474 0.24910785 0.24876077 0.24832647 0.24814801
 0.2483254  0.24834292 0.24789983 0.24748239 0.24736084 0.24730948
 0.24711892 0.24692045 0.24709874 0.24752736 0.2475733  0.24736944
 0.24742556 0.24763662 0.24754627 0.24717599 0.246963   0.24700189
 0.24677783 0.24575806 0.24443187 0.24365585 0.24329706 0.24257803
 0.2418405  0.24168947 0.24187851 0.24175858 0.24106002 0.24044237
 0.2404246  0.24062055 0.24041198 0.24009502 0.24033816 0.24075723
 0.24063686 0.24010366 0.24000818 0.2402759  0.23990205 0.23882732
 0.23804696 0.23788351 0.23737605 0.23588933 0.23447493 0.23398577
 0.23404326 0.2335792  0.23283066 0.23286398 0.2334027  0.23331925
 0.2324731  0.23194405 0.2321936  0.23217905 0.23146234 0.23113498
 0.23162109 0.23181105 0.230964   0.2301029  0.23040971 0.2309176
 0.23011038 0.22869012 0.22837014 0.22915414 0.22864711 0.22659285
 0.22535223 0.2262814  0.22660235 0.225088   0.22366287 0.22447133
 0.225333   0.22382733 0.22194959 0.2226662  0.22389388 0.22238667
 0.22089586 0.2231086  0.2242209  0.21960439 0.21996425 0.2350725 ]
