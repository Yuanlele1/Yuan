Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=34, out_features=160, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4874240.0
params:  5600.0
Trainable parameters:  5600
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.1318209171295166
Epoch: 1, Steps: 64 | Train Loss: 0.9709324 Vali Loss: 2.0318842 Test Loss: 1.0557264
Validation loss decreased (inf --> 2.031884).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.1983447074890137
Epoch: 2, Steps: 64 | Train Loss: 0.7272453 Vali Loss: 1.7354921 Test Loss: 0.8158104
Validation loss decreased (2.031884 --> 1.735492).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.1786034107208252
Epoch: 3, Steps: 64 | Train Loss: 0.6009485 Vali Loss: 1.6000435 Test Loss: 0.6945851
Validation loss decreased (1.735492 --> 1.600044).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.1102802753448486
Epoch: 4, Steps: 64 | Train Loss: 0.5307977 Vali Loss: 1.5068889 Test Loss: 0.6266422
Validation loss decreased (1.600044 --> 1.506889).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.085916519165039
Epoch: 5, Steps: 64 | Train Loss: 0.4891703 Vali Loss: 1.4553614 Test Loss: 0.5849313
Validation loss decreased (1.506889 --> 1.455361).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.2031950950622559
Epoch: 6, Steps: 64 | Train Loss: 0.4629252 Vali Loss: 1.4211375 Test Loss: 0.5581419
Validation loss decreased (1.455361 --> 1.421137).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.1627488136291504
Epoch: 7, Steps: 64 | Train Loss: 0.4456895 Vali Loss: 1.3994694 Test Loss: 0.5401960
Validation loss decreased (1.421137 --> 1.399469).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.060211420059204
Epoch: 8, Steps: 64 | Train Loss: 0.4340730 Vali Loss: 1.3764321 Test Loss: 0.5276277
Validation loss decreased (1.399469 --> 1.376432).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.0927822589874268
Epoch: 9, Steps: 64 | Train Loss: 0.4258930 Vali Loss: 1.3675114 Test Loss: 0.5185765
Validation loss decreased (1.376432 --> 1.367511).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.192577838897705
Epoch: 10, Steps: 64 | Train Loss: 0.4193757 Vali Loss: 1.3611056 Test Loss: 0.5118827
Validation loss decreased (1.367511 --> 1.361106).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.1122105121612549
Epoch: 11, Steps: 64 | Train Loss: 0.4151418 Vali Loss: 1.3408624 Test Loss: 0.5066186
Validation loss decreased (1.361106 --> 1.340862).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.0664784908294678
Epoch: 12, Steps: 64 | Train Loss: 0.4112760 Vali Loss: 1.3390938 Test Loss: 0.5027140
Validation loss decreased (1.340862 --> 1.339094).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.12180757522583
Epoch: 13, Steps: 64 | Train Loss: 0.4083540 Vali Loss: 1.3374369 Test Loss: 0.4995343
Validation loss decreased (1.339094 --> 1.337437).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.1551156044006348
Epoch: 14, Steps: 64 | Train Loss: 0.4061556 Vali Loss: 1.3305790 Test Loss: 0.4970680
Validation loss decreased (1.337437 --> 1.330579).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.186955213546753
Epoch: 15, Steps: 64 | Train Loss: 0.4040840 Vali Loss: 1.3311077 Test Loss: 0.4949017
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.115790843963623
Epoch: 16, Steps: 64 | Train Loss: 0.4026214 Vali Loss: 1.3318270 Test Loss: 0.4931914
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.2502734661102295
Epoch: 17, Steps: 64 | Train Loss: 0.4013113 Vali Loss: 1.3265555 Test Loss: 0.4917526
Validation loss decreased (1.330579 --> 1.326555).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.1012442111968994
Epoch: 18, Steps: 64 | Train Loss: 0.3998908 Vali Loss: 1.3228482 Test Loss: 0.4905409
Validation loss decreased (1.326555 --> 1.322848).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.0683238506317139
Epoch: 19, Steps: 64 | Train Loss: 0.3987853 Vali Loss: 1.3214846 Test Loss: 0.4894807
Validation loss decreased (1.322848 --> 1.321485).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.1237804889678955
Epoch: 20, Steps: 64 | Train Loss: 0.3978190 Vali Loss: 1.3164569 Test Loss: 0.4886339
Validation loss decreased (1.321485 --> 1.316457).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.136131763458252
Epoch: 21, Steps: 64 | Train Loss: 0.3973376 Vali Loss: 1.3125932 Test Loss: 0.4878744
Validation loss decreased (1.316457 --> 1.312593).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.1992921829223633
Epoch: 22, Steps: 64 | Train Loss: 0.3964271 Vali Loss: 1.3193020 Test Loss: 0.4871601
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3453030586242676
Epoch: 23, Steps: 64 | Train Loss: 0.3956418 Vali Loss: 1.3128463 Test Loss: 0.4865356
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.2538976669311523
Epoch: 24, Steps: 64 | Train Loss: 0.3950902 Vali Loss: 1.3133759 Test Loss: 0.4860738
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.1758489608764648
Epoch: 25, Steps: 64 | Train Loss: 0.3946910 Vali Loss: 1.3096508 Test Loss: 0.4855526
Validation loss decreased (1.312593 --> 1.309651).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.3886313438415527
Epoch: 26, Steps: 64 | Train Loss: 0.3942165 Vali Loss: 1.3147020 Test Loss: 0.4851602
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.1215200424194336
Epoch: 27, Steps: 64 | Train Loss: 0.3937388 Vali Loss: 1.3095599 Test Loss: 0.4847566
Validation loss decreased (1.309651 --> 1.309560).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.1989986896514893
Epoch: 28, Steps: 64 | Train Loss: 0.3933716 Vali Loss: 1.3110055 Test Loss: 0.4844321
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.275770664215088
Epoch: 29, Steps: 64 | Train Loss: 0.3927583 Vali Loss: 1.3131489 Test Loss: 0.4841387
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.1048662662506104
Epoch: 30, Steps: 64 | Train Loss: 0.3925860 Vali Loss: 1.3123730 Test Loss: 0.4838453
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.110290288925171
Epoch: 31, Steps: 64 | Train Loss: 0.3922841 Vali Loss: 1.3036579 Test Loss: 0.4836141
Validation loss decreased (1.309560 --> 1.303658).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.1809861660003662
Epoch: 32, Steps: 64 | Train Loss: 0.3920462 Vali Loss: 1.3082290 Test Loss: 0.4834158
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.1630125045776367
Epoch: 33, Steps: 64 | Train Loss: 0.3919391 Vali Loss: 1.2980348 Test Loss: 0.4831624
Validation loss decreased (1.303658 --> 1.298035).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.0892243385314941
Epoch: 34, Steps: 64 | Train Loss: 0.3914952 Vali Loss: 1.3071340 Test Loss: 0.4830041
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.2031903266906738
Epoch: 35, Steps: 64 | Train Loss: 0.3912988 Vali Loss: 1.3048971 Test Loss: 0.4828563
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.2486705780029297
Epoch: 36, Steps: 64 | Train Loss: 0.3911211 Vali Loss: 1.3028382 Test Loss: 0.4826891
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.202073335647583
Epoch: 37, Steps: 64 | Train Loss: 0.3910124 Vali Loss: 1.2986752 Test Loss: 0.4825527
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.157625675201416
Epoch: 38, Steps: 64 | Train Loss: 0.3906414 Vali Loss: 1.3065572 Test Loss: 0.4824041
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.2320010662078857
Epoch: 39, Steps: 64 | Train Loss: 0.3905465 Vali Loss: 1.3046712 Test Loss: 0.4822792
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.2214787006378174
Epoch: 40, Steps: 64 | Train Loss: 0.3903799 Vali Loss: 1.3043859 Test Loss: 0.4821911
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.139580249786377
Epoch: 41, Steps: 64 | Train Loss: 0.3902614 Vali Loss: 1.3013860 Test Loss: 0.4820843
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.1759490966796875
Epoch: 42, Steps: 64 | Train Loss: 0.3899831 Vali Loss: 1.3057154 Test Loss: 0.4819852
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.1380081176757812
Epoch: 43, Steps: 64 | Train Loss: 0.3897968 Vali Loss: 1.3037510 Test Loss: 0.4818927
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.1697278022766113
Epoch: 44, Steps: 64 | Train Loss: 0.3897730 Vali Loss: 1.3023961 Test Loss: 0.4818142
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.1602494716644287
Epoch: 45, Steps: 64 | Train Loss: 0.3896729 Vali Loss: 1.2968613 Test Loss: 0.4817262
Validation loss decreased (1.298035 --> 1.296861).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.1977970600128174
Epoch: 46, Steps: 64 | Train Loss: 0.3896304 Vali Loss: 1.2974261 Test Loss: 0.4816629
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.1189002990722656
Epoch: 47, Steps: 64 | Train Loss: 0.3894002 Vali Loss: 1.2980570 Test Loss: 0.4816133
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.2213730812072754
Epoch: 48, Steps: 64 | Train Loss: 0.3893503 Vali Loss: 1.3005817 Test Loss: 0.4815518
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.24155592918396
Epoch: 49, Steps: 64 | Train Loss: 0.3891699 Vali Loss: 1.3037775 Test Loss: 0.4814769
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2116308212280273
Epoch: 50, Steps: 64 | Train Loss: 0.3893194 Vali Loss: 1.2980517 Test Loss: 0.4814279
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.4370994567871094
Epoch: 51, Steps: 64 | Train Loss: 0.3892700 Vali Loss: 1.3079575 Test Loss: 0.4813848
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3620784282684326
Epoch: 52, Steps: 64 | Train Loss: 0.3888323 Vali Loss: 1.3039852 Test Loss: 0.4813372
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.3665351867675781
Epoch: 53, Steps: 64 | Train Loss: 0.3890389 Vali Loss: 1.3036634 Test Loss: 0.4812947
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.2714831829071045
Epoch: 54, Steps: 64 | Train Loss: 0.3887966 Vali Loss: 1.2982469 Test Loss: 0.4812494
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.3459153175354004
Epoch: 55, Steps: 64 | Train Loss: 0.3889689 Vali Loss: 1.2961746 Test Loss: 0.4812156
Validation loss decreased (1.296861 --> 1.296175).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.206864595413208
Epoch: 56, Steps: 64 | Train Loss: 0.3888596 Vali Loss: 1.3037705 Test Loss: 0.4811806
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.109419584274292
Epoch: 57, Steps: 64 | Train Loss: 0.3886528 Vali Loss: 1.2970467 Test Loss: 0.4811491
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.6470608711242676
Epoch: 58, Steps: 64 | Train Loss: 0.3885878 Vali Loss: 1.3033940 Test Loss: 0.4811173
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.1652536392211914
Epoch: 59, Steps: 64 | Train Loss: 0.3886848 Vali Loss: 1.3048971 Test Loss: 0.4810860
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.254406452178955
Epoch: 60, Steps: 64 | Train Loss: 0.3887033 Vali Loss: 1.3035967 Test Loss: 0.4810620
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.0820298194885254
Epoch: 61, Steps: 64 | Train Loss: 0.3885303 Vali Loss: 1.3001549 Test Loss: 0.4810351
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.1231694221496582
Epoch: 62, Steps: 64 | Train Loss: 0.3885292 Vali Loss: 1.2950425 Test Loss: 0.4810131
Validation loss decreased (1.296175 --> 1.295043).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.0887057781219482
Epoch: 63, Steps: 64 | Train Loss: 0.3884474 Vali Loss: 1.2977434 Test Loss: 0.4809883
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.113520622253418
Epoch: 64, Steps: 64 | Train Loss: 0.3885246 Vali Loss: 1.3001034 Test Loss: 0.4809714
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.0600743293762207
Epoch: 65, Steps: 64 | Train Loss: 0.3883487 Vali Loss: 1.3018347 Test Loss: 0.4809447
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.1622138023376465
Epoch: 66, Steps: 64 | Train Loss: 0.3883193 Vali Loss: 1.3011378 Test Loss: 0.4809312
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.1442995071411133
Epoch: 67, Steps: 64 | Train Loss: 0.3882549 Vali Loss: 1.3051647 Test Loss: 0.4809098
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.107731819152832
Epoch: 68, Steps: 64 | Train Loss: 0.3881768 Vali Loss: 1.3071922 Test Loss: 0.4808926
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.1408460140228271
Epoch: 69, Steps: 64 | Train Loss: 0.3882138 Vali Loss: 1.2908502 Test Loss: 0.4808768
Validation loss decreased (1.295043 --> 1.290850).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.1864221096038818
Epoch: 70, Steps: 64 | Train Loss: 0.3881341 Vali Loss: 1.3026344 Test Loss: 0.4808587
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.0565178394317627
Epoch: 71, Steps: 64 | Train Loss: 0.3879136 Vali Loss: 1.3025092 Test Loss: 0.4808415
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.3918578624725342
Epoch: 72, Steps: 64 | Train Loss: 0.3881477 Vali Loss: 1.2959256 Test Loss: 0.4808323
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.2481589317321777
Epoch: 73, Steps: 64 | Train Loss: 0.3880277 Vali Loss: 1.3017453 Test Loss: 0.4808177
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.1510045528411865
Epoch: 74, Steps: 64 | Train Loss: 0.3879306 Vali Loss: 1.2916839 Test Loss: 0.4808064
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.1450278759002686
Epoch: 75, Steps: 64 | Train Loss: 0.3881317 Vali Loss: 1.3020101 Test Loss: 0.4807920
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.1657383441925049
Epoch: 76, Steps: 64 | Train Loss: 0.3881156 Vali Loss: 1.2963970 Test Loss: 0.4807809
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.0803885459899902
Epoch: 77, Steps: 64 | Train Loss: 0.3878531 Vali Loss: 1.3044742 Test Loss: 0.4807689
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.2817864418029785
Epoch: 78, Steps: 64 | Train Loss: 0.3880109 Vali Loss: 1.3018078 Test Loss: 0.4807647
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.374387502670288
Epoch: 79, Steps: 64 | Train Loss: 0.3881503 Vali Loss: 1.2990555 Test Loss: 0.4807529
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.1231184005737305
Epoch: 80, Steps: 64 | Train Loss: 0.3879577 Vali Loss: 1.3019087 Test Loss: 0.4807464
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.1201908588409424
Epoch: 81, Steps: 64 | Train Loss: 0.3880743 Vali Loss: 1.3008916 Test Loss: 0.4807339
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.1478221416473389
Epoch: 82, Steps: 64 | Train Loss: 0.3879854 Vali Loss: 1.2901238 Test Loss: 0.4807245
Validation loss decreased (1.290850 --> 1.290124).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.1675820350646973
Epoch: 83, Steps: 64 | Train Loss: 0.3878955 Vali Loss: 1.2990386 Test Loss: 0.4807186
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.1029973030090332
Epoch: 84, Steps: 64 | Train Loss: 0.3878828 Vali Loss: 1.2968862 Test Loss: 0.4807137
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.057586431503296
Epoch: 85, Steps: 64 | Train Loss: 0.3877931 Vali Loss: 1.3011461 Test Loss: 0.4807027
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.1392500400543213
Epoch: 86, Steps: 64 | Train Loss: 0.3878586 Vali Loss: 1.2967620 Test Loss: 0.4806996
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.0577378273010254
Epoch: 87, Steps: 64 | Train Loss: 0.3878664 Vali Loss: 1.3002824 Test Loss: 0.4806952
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.0845839977264404
Epoch: 88, Steps: 64 | Train Loss: 0.3875125 Vali Loss: 1.3008801 Test Loss: 0.4806881
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.0692644119262695
Epoch: 89, Steps: 64 | Train Loss: 0.3879009 Vali Loss: 1.2951856 Test Loss: 0.4806844
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.2264020442962646
Epoch: 90, Steps: 64 | Train Loss: 0.3878847 Vali Loss: 1.3002207 Test Loss: 0.4806770
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.1632730960845947
Epoch: 91, Steps: 64 | Train Loss: 0.3879399 Vali Loss: 1.3055667 Test Loss: 0.4806741
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.1006267070770264
Epoch: 92, Steps: 64 | Train Loss: 0.3879007 Vali Loss: 1.3001465 Test Loss: 0.4806680
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.1255416870117188
Epoch: 93, Steps: 64 | Train Loss: 0.3876447 Vali Loss: 1.2994057 Test Loss: 0.4806620
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.1030778884887695
Epoch: 94, Steps: 64 | Train Loss: 0.3881343 Vali Loss: 1.2965786 Test Loss: 0.4806598
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.160569429397583
Epoch: 95, Steps: 64 | Train Loss: 0.3877350 Vali Loss: 1.2968912 Test Loss: 0.4806568
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.0886039733886719
Epoch: 96, Steps: 64 | Train Loss: 0.3879743 Vali Loss: 1.2946041 Test Loss: 0.4806515
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.0666797161102295
Epoch: 97, Steps: 64 | Train Loss: 0.3875217 Vali Loss: 1.3027759 Test Loss: 0.4806476
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.1162054538726807
Epoch: 98, Steps: 64 | Train Loss: 0.3877524 Vali Loss: 1.3020453 Test Loss: 0.4806469
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.1638209819793701
Epoch: 99, Steps: 64 | Train Loss: 0.3876553 Vali Loss: 1.2936436 Test Loss: 0.4806407
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.0885286331176758
Epoch: 100, Steps: 64 | Train Loss: 0.3877053 Vali Loss: 1.2999575 Test Loss: 0.4806391
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1160680107021042e-06
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=34, out_features=160, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4874240.0
params:  5600.0
Trainable parameters:  5600
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.2440061569213867
Epoch: 1, Steps: 64 | Train Loss: 0.4872848 Vali Loss: 1.2964543 Test Loss: 0.4796092
Validation loss decreased (inf --> 1.296454).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.1641552448272705
Epoch: 2, Steps: 64 | Train Loss: 0.4857436 Vali Loss: 1.2927172 Test Loss: 0.4795980
Validation loss decreased (1.296454 --> 1.292717).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.1623616218566895
Epoch: 3, Steps: 64 | Train Loss: 0.4848474 Vali Loss: 1.2917337 Test Loss: 0.4791504
Validation loss decreased (1.292717 --> 1.291734).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.1995768547058105
Epoch: 4, Steps: 64 | Train Loss: 0.4847769 Vali Loss: 1.2961985 Test Loss: 0.4790655
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.2591736316680908
Epoch: 5, Steps: 64 | Train Loss: 0.4844441 Vali Loss: 1.2971700 Test Loss: 0.4790530
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.0780367851257324
Epoch: 6, Steps: 64 | Train Loss: 0.4842031 Vali Loss: 1.2910480 Test Loss: 0.4796178
Validation loss decreased (1.291734 --> 1.291048).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.168898105621338
Epoch: 7, Steps: 64 | Train Loss: 0.4841453 Vali Loss: 1.2935870 Test Loss: 0.4792470
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.1669721603393555
Epoch: 8, Steps: 64 | Train Loss: 0.4840385 Vali Loss: 1.2879012 Test Loss: 0.4793824
Validation loss decreased (1.291048 --> 1.287901).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.2049424648284912
Epoch: 9, Steps: 64 | Train Loss: 0.4842518 Vali Loss: 1.2910963 Test Loss: 0.4795097
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.1187913417816162
Epoch: 10, Steps: 64 | Train Loss: 0.4838389 Vali Loss: 1.2909815 Test Loss: 0.4795499
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.2380023002624512
Epoch: 11, Steps: 64 | Train Loss: 0.4841699 Vali Loss: 1.2904508 Test Loss: 0.4795051
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.2726895809173584
Epoch: 12, Steps: 64 | Train Loss: 0.4838415 Vali Loss: 1.2938131 Test Loss: 0.4794437
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.1374738216400146
Epoch: 13, Steps: 64 | Train Loss: 0.4838580 Vali Loss: 1.2896594 Test Loss: 0.4795123
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.0743167400360107
Epoch: 14, Steps: 64 | Train Loss: 0.4840058 Vali Loss: 1.2915717 Test Loss: 0.4796241
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.085700273513794
Epoch: 15, Steps: 64 | Train Loss: 0.4840712 Vali Loss: 1.2902629 Test Loss: 0.4796191
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.0917754173278809
Epoch: 16, Steps: 64 | Train Loss: 0.4835938 Vali Loss: 1.2926306 Test Loss: 0.4797059
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.1226096153259277
Epoch: 17, Steps: 64 | Train Loss: 0.4839185 Vali Loss: 1.2885795 Test Loss: 0.4797146
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.15407395362854
Epoch: 18, Steps: 64 | Train Loss: 0.4836936 Vali Loss: 1.2954830 Test Loss: 0.4798357
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.0929007530212402
Epoch: 19, Steps: 64 | Train Loss: 0.4838507 Vali Loss: 1.2911824 Test Loss: 0.4799263
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 0.9849793910980225
Epoch: 20, Steps: 64 | Train Loss: 0.4835421 Vali Loss: 1.2923874 Test Loss: 0.4798513
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.1546378135681152
Epoch: 21, Steps: 64 | Train Loss: 0.4836725 Vali Loss: 1.2923685 Test Loss: 0.4798621
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.1074683666229248
Epoch: 22, Steps: 64 | Train Loss: 0.4835835 Vali Loss: 1.2933098 Test Loss: 0.4798794
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.084907054901123
Epoch: 23, Steps: 64 | Train Loss: 0.4837689 Vali Loss: 1.2929575 Test Loss: 0.4798829
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.0665040016174316
Epoch: 24, Steps: 64 | Train Loss: 0.4839603 Vali Loss: 1.2895374 Test Loss: 0.4799868
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.123075246810913
Epoch: 25, Steps: 64 | Train Loss: 0.4839522 Vali Loss: 1.2909508 Test Loss: 0.4799148
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.166154384613037
Epoch: 26, Steps: 64 | Train Loss: 0.4836626 Vali Loss: 1.2922447 Test Loss: 0.4799252
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.1625216007232666
Epoch: 27, Steps: 64 | Train Loss: 0.4835014 Vali Loss: 1.2917391 Test Loss: 0.4800013
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.2110168933868408
Epoch: 28, Steps: 64 | Train Loss: 0.4836801 Vali Loss: 1.2929375 Test Loss: 0.4799987
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_336_FITS_ETTh1_ftM_sl90_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.47889286279678345, mae:0.44381383061408997, rse:0.6588267087936401, corr:[0.2536431  0.2559167  0.25364515 0.25409275 0.2507475  0.2485945
 0.24768245 0.24720663 0.24700359 0.24625291 0.24628986 0.24658285
 0.24605243 0.2458909  0.24607529 0.24587864 0.24578659 0.2458148
 0.2458127  0.24574849 0.24511671 0.24500673 0.244767   0.24425445
 0.24261114 0.24126688 0.2414     0.24177404 0.2415897  0.24182867
 0.24210018 0.2420405  0.24196611 0.24155973 0.24125616 0.24148397
 0.24163583 0.24140263 0.2417397  0.2418082  0.24195668 0.2426174
 0.24290445 0.24299349 0.2429566  0.24287489 0.24274534 0.2423842
 0.24120925 0.23976223 0.23885506 0.23854345 0.23767813 0.23672567
 0.23673749 0.23651838 0.23625013 0.23619458 0.23593675 0.23604512
 0.23620208 0.23628224 0.23613098 0.23560278 0.23580708 0.23613514
 0.23629272 0.23650335 0.23606995 0.23590617 0.23595262 0.23497814
 0.23306033 0.23218629 0.23175552 0.23152858 0.23112093 0.23116636
 0.23163374 0.23142762 0.23119125 0.23091677 0.23054385 0.23010772
 0.2299003  0.22994198 0.23035522 0.23025146 0.23006974 0.23032896
 0.230444   0.23052485 0.23033994 0.23047876 0.23063184 0.23036426
 0.22944795 0.22869992 0.22837281 0.2281196  0.22808817 0.22842744
 0.22878845 0.22842579 0.22804292 0.22800797 0.22777864 0.22738314
 0.22715597 0.22712316 0.2274982  0.22723982 0.2273237  0.22752716
 0.22762252 0.22794521 0.2279936  0.22793615 0.22785157 0.22694074
 0.22514027 0.2238855  0.22298427 0.22208291 0.22157484 0.22160552
 0.22233933 0.22257507 0.22262567 0.22257705 0.22227658 0.22214235
 0.22189967 0.22144458 0.2214317  0.2212141  0.2215317  0.22192112
 0.22194843 0.222232   0.22220261 0.22208813 0.22209579 0.22145206
 0.21959506 0.21809965 0.21761587 0.21718013 0.2165753  0.21652482
 0.21736485 0.21759151 0.21761622 0.21746752 0.21729647 0.21717632
 0.21685895 0.21671881 0.21667692 0.21630839 0.2164622  0.21662135
 0.21660393 0.21697524 0.21687086 0.2167098  0.21677837 0.21593481
 0.21403088 0.21308553 0.21311596 0.21277334 0.21210949 0.21223079
 0.2126891  0.21261325 0.21264161 0.212758   0.21291706 0.2130006
 0.2128583  0.21265735 0.21261114 0.21197219 0.21204334 0.21287456
 0.21316218 0.21347047 0.21378037 0.2138234  0.21392804 0.21326722
 0.21128689 0.2103842  0.21036299 0.2097275  0.20880938 0.20915496
 0.21008304 0.21012571 0.20979086 0.20935304 0.20911023 0.20927297
 0.20913135 0.20890857 0.20913117 0.20888534 0.20876211 0.2090694
 0.20927933 0.2093857  0.20931196 0.20954275 0.2096184  0.20884283
 0.20744516 0.2067732  0.20668234 0.20722035 0.20792665 0.20868331
 0.20991428 0.21070573 0.21078807 0.21046735 0.21031252 0.21005727
 0.20991008 0.20992029 0.20986927 0.20975515 0.20993009 0.20988616
 0.20972747 0.20995739 0.21021307 0.2105081  0.21070476 0.20973743
 0.2079027  0.20673522 0.20627315 0.20639238 0.20655893 0.20701215
 0.2080901  0.20886418 0.2092759  0.20917147 0.20878331 0.2085544
 0.20824933 0.20775358 0.20769235 0.20742828 0.20723301 0.20736593
 0.20749934 0.2077662  0.20784175 0.20761886 0.20774858 0.20730318
 0.20558384 0.20461528 0.20460391 0.205051   0.20541453 0.20624892
 0.20738977 0.20814249 0.20840353 0.2083919  0.20832743 0.20791708
 0.20751494 0.20734003 0.20698245 0.20684971 0.20721358 0.2074983
 0.2075763  0.20784032 0.20783602 0.20792468 0.20846307 0.20811313
 0.20640509 0.20545745 0.20662494 0.20767348 0.20760247 0.208443
 0.20974517 0.21005763 0.20999943 0.209638   0.20956601 0.20938271
 0.20887978 0.20853786 0.20834522 0.20778394 0.20811608 0.20845944
 0.20833527 0.20859377 0.20891184 0.20892724 0.2090768  0.2082314
 0.20636185 0.20482303 0.20481704 0.20491856 0.20451908 0.20491664
 0.20631923 0.20644659 0.20634286 0.20628113 0.20580032 0.20500003
 0.20458618 0.20448655 0.20429273 0.20425922 0.20443042 0.20481046
 0.20514844 0.20483148 0.2046872  0.20455419 0.20370853 0.20569152]
