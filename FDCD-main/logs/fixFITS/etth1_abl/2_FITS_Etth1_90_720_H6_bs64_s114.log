Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=34, out_features=306, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9321984.0
params:  10710.0
Trainable parameters:  10710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.5028812885284424
Epoch: 1, Steps: 61 | Train Loss: 1.5258628 Vali Loss: 2.8863039 Test Loss: 1.5358953
Validation loss decreased (inf --> 2.886304).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3693408966064453
Epoch: 2, Steps: 61 | Train Loss: 1.1072276 Vali Loss: 2.3430588 Test Loss: 1.0818228
Validation loss decreased (2.886304 --> 2.343059).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2753081321716309
Epoch: 3, Steps: 61 | Train Loss: 0.8890099 Vali Loss: 2.0689614 Test Loss: 0.8485389
Validation loss decreased (2.343059 --> 2.068961).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.2621369361877441
Epoch: 4, Steps: 61 | Train Loss: 0.7674916 Vali Loss: 1.9087045 Test Loss: 0.7180462
Validation loss decreased (2.068961 --> 1.908705).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.258166790008545
Epoch: 5, Steps: 61 | Train Loss: 0.6960123 Vali Loss: 1.8160501 Test Loss: 0.6390734
Validation loss decreased (1.908705 --> 1.816050).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.2160816192626953
Epoch: 6, Steps: 61 | Train Loss: 0.6513325 Vali Loss: 1.7466900 Test Loss: 0.5892784
Validation loss decreased (1.816050 --> 1.746690).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.1698875427246094
Epoch: 7, Steps: 61 | Train Loss: 0.6231886 Vali Loss: 1.7051463 Test Loss: 0.5569072
Validation loss decreased (1.746690 --> 1.705146).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2219226360321045
Epoch: 8, Steps: 61 | Train Loss: 0.6044322 Vali Loss: 1.6847128 Test Loss: 0.5350462
Validation loss decreased (1.705146 --> 1.684713).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.2440464496612549
Epoch: 9, Steps: 61 | Train Loss: 0.5916409 Vali Loss: 1.6638691 Test Loss: 0.5197816
Validation loss decreased (1.684713 --> 1.663869).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.1699769496917725
Epoch: 10, Steps: 61 | Train Loss: 0.5824495 Vali Loss: 1.6490042 Test Loss: 0.5089841
Validation loss decreased (1.663869 --> 1.649004).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.3556873798370361
Epoch: 11, Steps: 61 | Train Loss: 0.5756965 Vali Loss: 1.6355880 Test Loss: 0.5010804
Validation loss decreased (1.649004 --> 1.635588).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4274168014526367
Epoch: 12, Steps: 61 | Train Loss: 0.5708812 Vali Loss: 1.6258948 Test Loss: 0.4951947
Validation loss decreased (1.635588 --> 1.625895).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.1746485233306885
Epoch: 13, Steps: 61 | Train Loss: 0.5669595 Vali Loss: 1.6246928 Test Loss: 0.4906999
Validation loss decreased (1.625895 --> 1.624693).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.1780481338500977
Epoch: 14, Steps: 61 | Train Loss: 0.5639299 Vali Loss: 1.6122246 Test Loss: 0.4872653
Validation loss decreased (1.624693 --> 1.612225).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.2282731533050537
Epoch: 15, Steps: 61 | Train Loss: 0.5619236 Vali Loss: 1.6117053 Test Loss: 0.4845380
Validation loss decreased (1.612225 --> 1.611705).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.2485299110412598
Epoch: 16, Steps: 61 | Train Loss: 0.5599550 Vali Loss: 1.6108063 Test Loss: 0.4824247
Validation loss decreased (1.611705 --> 1.610806).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.244192361831665
Epoch: 17, Steps: 61 | Train Loss: 0.5581518 Vali Loss: 1.5965116 Test Loss: 0.4806403
Validation loss decreased (1.610806 --> 1.596512).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.2340576648712158
Epoch: 18, Steps: 61 | Train Loss: 0.5566904 Vali Loss: 1.5985943 Test Loss: 0.4792085
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.2349622249603271
Epoch: 19, Steps: 61 | Train Loss: 0.5556325 Vali Loss: 1.6004174 Test Loss: 0.4780547
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.1626510620117188
Epoch: 20, Steps: 61 | Train Loss: 0.5547513 Vali Loss: 1.6023893 Test Loss: 0.4770630
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.1815025806427002
Epoch: 21, Steps: 61 | Train Loss: 0.5538520 Vali Loss: 1.6030077 Test Loss: 0.4762602
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.1669766902923584
Epoch: 22, Steps: 61 | Train Loss: 0.5528164 Vali Loss: 1.5922284 Test Loss: 0.4755552
Validation loss decreased (1.596512 --> 1.592228).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.2950000762939453
Epoch: 23, Steps: 61 | Train Loss: 0.5523794 Vali Loss: 1.5951962 Test Loss: 0.4749305
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.2945756912231445
Epoch: 24, Steps: 61 | Train Loss: 0.5518057 Vali Loss: 1.5799367 Test Loss: 0.4744446
Validation loss decreased (1.592228 --> 1.579937).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.3097233772277832
Epoch: 25, Steps: 61 | Train Loss: 0.5513950 Vali Loss: 1.5893452 Test Loss: 0.4740077
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2502481937408447
Epoch: 26, Steps: 61 | Train Loss: 0.5505043 Vali Loss: 1.5886234 Test Loss: 0.4735757
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.3980097770690918
Epoch: 27, Steps: 61 | Train Loss: 0.5505082 Vali Loss: 1.5870250 Test Loss: 0.4732467
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.247330904006958
Epoch: 28, Steps: 61 | Train Loss: 0.5497890 Vali Loss: 1.5920153 Test Loss: 0.4729203
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.2936365604400635
Epoch: 29, Steps: 61 | Train Loss: 0.5495201 Vali Loss: 1.5824434 Test Loss: 0.4726501
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.3489201068878174
Epoch: 30, Steps: 61 | Train Loss: 0.5492205 Vali Loss: 1.5861959 Test Loss: 0.4724398
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.2563128471374512
Epoch: 31, Steps: 61 | Train Loss: 0.5487540 Vali Loss: 1.5896628 Test Loss: 0.4722137
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.172861099243164
Epoch: 32, Steps: 61 | Train Loss: 0.5485752 Vali Loss: 1.5732346 Test Loss: 0.4720340
Validation loss decreased (1.579937 --> 1.573235).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.1669588088989258
Epoch: 33, Steps: 61 | Train Loss: 0.5484354 Vali Loss: 1.5847193 Test Loss: 0.4718438
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.2832090854644775
Epoch: 34, Steps: 61 | Train Loss: 0.5480471 Vali Loss: 1.5795453 Test Loss: 0.4716623
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.3132007122039795
Epoch: 35, Steps: 61 | Train Loss: 0.5477795 Vali Loss: 1.5767947 Test Loss: 0.4715481
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3994739055633545
Epoch: 36, Steps: 61 | Train Loss: 0.5475963 Vali Loss: 1.5796531 Test Loss: 0.4714121
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.4776248931884766
Epoch: 37, Steps: 61 | Train Loss: 0.5474381 Vali Loss: 1.5858681 Test Loss: 0.4713052
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.16831636428833
Epoch: 38, Steps: 61 | Train Loss: 0.5473517 Vali Loss: 1.5784055 Test Loss: 0.4711935
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.2590689659118652
Epoch: 39, Steps: 61 | Train Loss: 0.5474994 Vali Loss: 1.5778921 Test Loss: 0.4710945
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.1020538806915283
Epoch: 40, Steps: 61 | Train Loss: 0.5470764 Vali Loss: 1.5868980 Test Loss: 0.4710260
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.2837016582489014
Epoch: 41, Steps: 61 | Train Loss: 0.5474206 Vali Loss: 1.5808973 Test Loss: 0.4709139
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.1069397926330566
Epoch: 42, Steps: 61 | Train Loss: 0.5467072 Vali Loss: 1.5785270 Test Loss: 0.4708309
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.651728868484497
Epoch: 43, Steps: 61 | Train Loss: 0.5466321 Vali Loss: 1.5785413 Test Loss: 0.4707653
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.3465428352355957
Epoch: 44, Steps: 61 | Train Loss: 0.5465096 Vali Loss: 1.5794965 Test Loss: 0.4706967
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.2863452434539795
Epoch: 45, Steps: 61 | Train Loss: 0.5461097 Vali Loss: 1.5795212 Test Loss: 0.4706392
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.2989895343780518
Epoch: 46, Steps: 61 | Train Loss: 0.5461938 Vali Loss: 1.5753310 Test Loss: 0.4705937
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.3713061809539795
Epoch: 47, Steps: 61 | Train Loss: 0.5460106 Vali Loss: 1.5730071 Test Loss: 0.4705365
Validation loss decreased (1.573235 --> 1.573007).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.390458583831787
Epoch: 48, Steps: 61 | Train Loss: 0.5459752 Vali Loss: 1.5776526 Test Loss: 0.4704945
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.2811052799224854
Epoch: 49, Steps: 61 | Train Loss: 0.5462489 Vali Loss: 1.5767716 Test Loss: 0.4704456
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2991161346435547
Epoch: 50, Steps: 61 | Train Loss: 0.5459108 Vali Loss: 1.5716631 Test Loss: 0.4704082
Validation loss decreased (1.573007 --> 1.571663).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3851466178894043
Epoch: 51, Steps: 61 | Train Loss: 0.5458035 Vali Loss: 1.5797493 Test Loss: 0.4703647
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.378037929534912
Epoch: 52, Steps: 61 | Train Loss: 0.5456450 Vali Loss: 1.5725050 Test Loss: 0.4703343
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.3978469371795654
Epoch: 53, Steps: 61 | Train Loss: 0.5461694 Vali Loss: 1.5737095 Test Loss: 0.4702910
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.2326955795288086
Epoch: 54, Steps: 61 | Train Loss: 0.5458835 Vali Loss: 1.5745447 Test Loss: 0.4702561
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.2827167510986328
Epoch: 55, Steps: 61 | Train Loss: 0.5456035 Vali Loss: 1.5804435 Test Loss: 0.4702283
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.2823920249938965
Epoch: 56, Steps: 61 | Train Loss: 0.5453682 Vali Loss: 1.5743037 Test Loss: 0.4702003
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.3125064373016357
Epoch: 57, Steps: 61 | Train Loss: 0.5455746 Vali Loss: 1.5736628 Test Loss: 0.4701639
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.373732089996338
Epoch: 58, Steps: 61 | Train Loss: 0.5452255 Vali Loss: 1.5761826 Test Loss: 0.4701456
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.4092154502868652
Epoch: 59, Steps: 61 | Train Loss: 0.5453292 Vali Loss: 1.5662699 Test Loss: 0.4701204
Validation loss decreased (1.571663 --> 1.566270).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.2770264148712158
Epoch: 60, Steps: 61 | Train Loss: 0.5454103 Vali Loss: 1.5768830 Test Loss: 0.4700998
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.3836870193481445
Epoch: 61, Steps: 61 | Train Loss: 0.5451443 Vali Loss: 1.5763556 Test Loss: 0.4700780
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.3034770488739014
Epoch: 62, Steps: 61 | Train Loss: 0.5451745 Vali Loss: 1.5737239 Test Loss: 0.4700519
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.2099266052246094
Epoch: 63, Steps: 61 | Train Loss: 0.5452821 Vali Loss: 1.5732446 Test Loss: 0.4700358
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.173520803451538
Epoch: 64, Steps: 61 | Train Loss: 0.5451941 Vali Loss: 1.5759807 Test Loss: 0.4700193
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.2640979290008545
Epoch: 65, Steps: 61 | Train Loss: 0.5451030 Vali Loss: 1.5780228 Test Loss: 0.4700046
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.3311495780944824
Epoch: 66, Steps: 61 | Train Loss: 0.5449336 Vali Loss: 1.5769051 Test Loss: 0.4699927
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.343144416809082
Epoch: 67, Steps: 61 | Train Loss: 0.5452221 Vali Loss: 1.5734228 Test Loss: 0.4699719
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.3844454288482666
Epoch: 68, Steps: 61 | Train Loss: 0.5453174 Vali Loss: 1.5812800 Test Loss: 0.4699534
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.2409541606903076
Epoch: 69, Steps: 61 | Train Loss: 0.5449689 Vali Loss: 1.5760956 Test Loss: 0.4699393
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.3104567527770996
Epoch: 70, Steps: 61 | Train Loss: 0.5450464 Vali Loss: 1.5789381 Test Loss: 0.4699289
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.342841386795044
Epoch: 71, Steps: 61 | Train Loss: 0.5449763 Vali Loss: 1.5776694 Test Loss: 0.4699163
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.3453621864318848
Epoch: 72, Steps: 61 | Train Loss: 0.5450521 Vali Loss: 1.5697358 Test Loss: 0.4699019
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.3733112812042236
Epoch: 73, Steps: 61 | Train Loss: 0.5449394 Vali Loss: 1.5813760 Test Loss: 0.4698921
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.2703945636749268
Epoch: 74, Steps: 61 | Train Loss: 0.5448772 Vali Loss: 1.5774059 Test Loss: 0.4698827
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.5881633758544922
Epoch: 75, Steps: 61 | Train Loss: 0.5447171 Vali Loss: 1.5799445 Test Loss: 0.4698699
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.3951754570007324
Epoch: 76, Steps: 61 | Train Loss: 0.5449995 Vali Loss: 1.5758586 Test Loss: 0.4698599
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.3328235149383545
Epoch: 77, Steps: 61 | Train Loss: 0.5447882 Vali Loss: 1.5796105 Test Loss: 0.4698498
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.4595730304718018
Epoch: 78, Steps: 61 | Train Loss: 0.5447782 Vali Loss: 1.5740606 Test Loss: 0.4698460
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.4714252948760986
Epoch: 79, Steps: 61 | Train Loss: 0.5448826 Vali Loss: 1.5774976 Test Loss: 0.4698329
EarlyStopping counter: 20 out of 20
Early stopping
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=34, out_features=306, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9321984.0
params:  10710.0
Trainable parameters:  10710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.4678845405578613
Epoch: 1, Steps: 61 | Train Loss: 0.6115471 Vali Loss: 1.5672376 Test Loss: 0.4694940
Validation loss decreased (inf --> 1.567238).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.215517282485962
Epoch: 2, Steps: 61 | Train Loss: 0.6100526 Vali Loss: 1.5739861 Test Loss: 0.4691968
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2907276153564453
Epoch: 3, Steps: 61 | Train Loss: 0.6091854 Vali Loss: 1.5678326 Test Loss: 0.4691252
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4570021629333496
Epoch: 4, Steps: 61 | Train Loss: 0.6089079 Vali Loss: 1.5774158 Test Loss: 0.4689821
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.3111510276794434
Epoch: 5, Steps: 61 | Train Loss: 0.6086965 Vali Loss: 1.5703037 Test Loss: 0.4692346
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.3366808891296387
Epoch: 6, Steps: 61 | Train Loss: 0.6084432 Vali Loss: 1.5691824 Test Loss: 0.4693465
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.2974867820739746
Epoch: 7, Steps: 61 | Train Loss: 0.6085288 Vali Loss: 1.5707266 Test Loss: 0.4694517
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.3401515483856201
Epoch: 8, Steps: 61 | Train Loss: 0.6082991 Vali Loss: 1.5690428 Test Loss: 0.4695928
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.2182865142822266
Epoch: 9, Steps: 61 | Train Loss: 0.6081908 Vali Loss: 1.5631320 Test Loss: 0.4697020
Validation loss decreased (1.567238 --> 1.563132).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.4359288215637207
Epoch: 10, Steps: 61 | Train Loss: 0.6079431 Vali Loss: 1.5705501 Test Loss: 0.4698894
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.422598123550415
Epoch: 11, Steps: 61 | Train Loss: 0.6081126 Vali Loss: 1.5682278 Test Loss: 0.4699367
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4056036472320557
Epoch: 12, Steps: 61 | Train Loss: 0.6078788 Vali Loss: 1.5668499 Test Loss: 0.4699126
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.2344799041748047
Epoch: 13, Steps: 61 | Train Loss: 0.6077180 Vali Loss: 1.5624901 Test Loss: 0.4700474
Validation loss decreased (1.563132 --> 1.562490).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.4845216274261475
Epoch: 14, Steps: 61 | Train Loss: 0.6078161 Vali Loss: 1.5701305 Test Loss: 0.4701493
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.2527735233306885
Epoch: 15, Steps: 61 | Train Loss: 0.6076892 Vali Loss: 1.5674086 Test Loss: 0.4701945
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.3939950466156006
Epoch: 16, Steps: 61 | Train Loss: 0.6078844 Vali Loss: 1.5584652 Test Loss: 0.4702816
Validation loss decreased (1.562490 --> 1.558465).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3261709213256836
Epoch: 17, Steps: 61 | Train Loss: 0.6077527 Vali Loss: 1.5664539 Test Loss: 0.4704043
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.3270509243011475
Epoch: 18, Steps: 61 | Train Loss: 0.6075243 Vali Loss: 1.5656753 Test Loss: 0.4703643
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.648125410079956
Epoch: 19, Steps: 61 | Train Loss: 0.6078169 Vali Loss: 1.5687096 Test Loss: 0.4705203
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.4350697994232178
Epoch: 20, Steps: 61 | Train Loss: 0.6076510 Vali Loss: 1.5769677 Test Loss: 0.4705839
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4294509887695312
Epoch: 21, Steps: 61 | Train Loss: 0.6078005 Vali Loss: 1.5662397 Test Loss: 0.4706041
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.3507606983184814
Epoch: 22, Steps: 61 | Train Loss: 0.6076830 Vali Loss: 1.5605785 Test Loss: 0.4707771
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.4275906085968018
Epoch: 23, Steps: 61 | Train Loss: 0.6078313 Vali Loss: 1.5680392 Test Loss: 0.4707915
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.3476717472076416
Epoch: 24, Steps: 61 | Train Loss: 0.6075814 Vali Loss: 1.5628574 Test Loss: 0.4708006
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.6016414165496826
Epoch: 25, Steps: 61 | Train Loss: 0.6074235 Vali Loss: 1.5690041 Test Loss: 0.4708688
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5225152969360352
Epoch: 26, Steps: 61 | Train Loss: 0.6074281 Vali Loss: 1.5736878 Test Loss: 0.4709731
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4804317951202393
Epoch: 27, Steps: 61 | Train Loss: 0.6073572 Vali Loss: 1.5714055 Test Loss: 0.4709578
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.628844976425171
Epoch: 28, Steps: 61 | Train Loss: 0.6071621 Vali Loss: 1.5727563 Test Loss: 0.4709721
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6057205200195312
Epoch: 29, Steps: 61 | Train Loss: 0.6075224 Vali Loss: 1.5714893 Test Loss: 0.4710363
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4136686325073242
Epoch: 30, Steps: 61 | Train Loss: 0.6075809 Vali Loss: 1.5667882 Test Loss: 0.4710831
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.3096647262573242
Epoch: 31, Steps: 61 | Train Loss: 0.6075392 Vali Loss: 1.5633807 Test Loss: 0.4711328
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.295196533203125
Epoch: 32, Steps: 61 | Train Loss: 0.6074850 Vali Loss: 1.5657561 Test Loss: 0.4711692
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.2977490425109863
Epoch: 33, Steps: 61 | Train Loss: 0.6078080 Vali Loss: 1.5628121 Test Loss: 0.4712085
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.356226921081543
Epoch: 34, Steps: 61 | Train Loss: 0.6072932 Vali Loss: 1.5700790 Test Loss: 0.4712189
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.3983917236328125
Epoch: 35, Steps: 61 | Train Loss: 0.6072972 Vali Loss: 1.5602233 Test Loss: 0.4712368
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.13586688041687
Epoch: 36, Steps: 61 | Train Loss: 0.6074611 Vali Loss: 1.5687511 Test Loss: 0.4712803
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.46904411911964417, mae:0.4604049026966095, rse:0.6556299924850464, corr:[0.22739896 0.23068286 0.22939806 0.22982897 0.22679932 0.22499776
 0.22462432 0.22484903 0.22515509 0.22496949 0.22457384 0.22410114
 0.22385761 0.2236824  0.22365613 0.22340359 0.22327188 0.22336826
 0.22328126 0.22352839 0.223035   0.22297256 0.22310734 0.22277834
 0.22135107 0.22068126 0.2203812  0.22041473 0.2208597  0.22096807
 0.22095728 0.2210003  0.22085907 0.22077554 0.22021252 0.21976012
 0.21999724 0.2194285  0.21960323 0.21968058 0.21965875 0.22010867
 0.22030973 0.22079    0.2209936  0.22079904 0.2208349  0.22063251
 0.21937425 0.21841992 0.21787088 0.21720645 0.2165289  0.21584299
 0.21575814 0.21553908 0.21560672 0.21578945 0.21504492 0.21479055
 0.21455142 0.21410064 0.21415411 0.2137946  0.21386369 0.21410732
 0.21431811 0.21489145 0.2147056  0.21442667 0.21424343 0.2135005
 0.21205272 0.211283   0.2106343  0.21084018 0.21087398 0.2111252
 0.21180238 0.21152571 0.21127415 0.21137956 0.21076208 0.21017995
 0.2102508  0.20991975 0.21015573 0.21008275 0.20968637 0.20982756
 0.20986123 0.21007292 0.21016046 0.21038736 0.21056552 0.21064822
 0.21018447 0.20983179 0.20991457 0.21027513 0.21044673 0.21058154
 0.2111656  0.2112062  0.2109047  0.21079639 0.21016313 0.20960633
 0.20979215 0.20949632 0.20973341 0.20943744 0.20945542 0.20969854
 0.20983034 0.21015164 0.21027343 0.21027596 0.21024536 0.20957357
 0.20838486 0.20735772 0.20612453 0.20548645 0.20541318 0.20560132
 0.20694105 0.20761123 0.20752901 0.20738359 0.20687927 0.20645586
 0.20630448 0.20575805 0.20579693 0.20565636 0.20575388 0.20587857
 0.20568264 0.20610517 0.20629181 0.2064641  0.20680207 0.20632505
 0.20496356 0.20416416 0.20391645 0.20336713 0.20246768 0.20224614
 0.20328811 0.20370603 0.20364465 0.20346323 0.20317061 0.20315179
 0.20306484 0.20271164 0.20255429 0.2018934  0.20205717 0.20264667
 0.2026633  0.20273668 0.2026044  0.20269117 0.20298038 0.20253631
 0.20137571 0.20099178 0.20134184 0.2012886  0.2006766  0.20052522
 0.20077074 0.20080854 0.20098652 0.200945   0.20084478 0.20093516
 0.20084947 0.20026393 0.20031767 0.1997977  0.19962204 0.20032507
 0.2006071  0.20104833 0.20156558 0.20167345 0.20166528 0.20130321
 0.19999762 0.19893457 0.19826387 0.197609   0.19677486 0.19673723
 0.19717081 0.19740584 0.19757976 0.19712411 0.1965986  0.19671641
 0.19660997 0.19613004 0.19617213 0.19585794 0.1957258  0.19601242
 0.19618669 0.1964007  0.19617602 0.19627352 0.19653478 0.19602582
 0.19490871 0.19440775 0.19429208 0.19450998 0.19494791 0.19578607
 0.19690987 0.1976164  0.19808179 0.19798078 0.1972993  0.19691955
 0.19694872 0.19666174 0.19682413 0.1967156  0.1964838  0.19647674
 0.19629501 0.19629323 0.19641717 0.19652936 0.19654012 0.19600372
 0.19512677 0.19435965 0.19383562 0.19392507 0.1940126  0.19449148
 0.19581878 0.19629486 0.19622274 0.19609152 0.19565898 0.19536592
 0.19506413 0.19451934 0.19434541 0.19368152 0.19350973 0.1940575
 0.19411245 0.19414891 0.1943704  0.19425073 0.19426219 0.1941115
 0.19289704 0.1923408  0.19256881 0.19318154 0.19358887 0.19399768
 0.19501974 0.19622274 0.19649051 0.19618508 0.19588847 0.19560067
 0.195413   0.19493078 0.19460367 0.19446628 0.19431329 0.194444
 0.19464248 0.19502838 0.19541553 0.19585104 0.1962289  0.19630781
 0.19577953 0.1955546  0.19651628 0.19718872 0.19714268 0.19773279
 0.19872853 0.19927076 0.19951032 0.19909774 0.19869424 0.19857205
 0.1980903  0.19752304 0.19759263 0.19713792 0.19706467 0.19743791
 0.19739977 0.19741851 0.19752951 0.19749881 0.19762257 0.19740713
 0.1965842  0.19567    0.19555831 0.19559386 0.19533001 0.19543913
 0.19631489 0.19650394 0.19648333 0.19611764 0.19559215 0.19519815
 0.19499269 0.19487387 0.19477941 0.19439358 0.1942238  0.19459741
 0.19483337 0.19501641 0.19485965 0.19483617 0.19497782 0.19454189
 0.19346806 0.19271743 0.19196193 0.19172513 0.1918561  0.19244461
 0.19363706 0.19454537 0.19502166 0.19474217 0.1943664  0.1944053
 0.19443186 0.19394821 0.19393793 0.19398814 0.193842   0.19399534
 0.19421148 0.19451387 0.1944371  0.1942096  0.1940162  0.19327411
 0.19189803 0.19122815 0.19118775 0.1905914  0.18993944 0.18989049
 0.19071002 0.1910642  0.1913461  0.19125591 0.19080265 0.19039509
 0.19048278 0.19022165 0.18978699 0.18910678 0.18902175 0.189397
 0.18950006 0.18961206 0.18966211 0.19002496 0.19014369 0.19000101
 0.18964072 0.18971017 0.19022352 0.19090572 0.19111927 0.19117153
 0.19213699 0.19265346 0.19266301 0.19237624 0.19198272 0.19168165
 0.19127172 0.19127075 0.19185154 0.19173147 0.19148117 0.19203043
 0.19230005 0.19269347 0.19305481 0.19318634 0.19322367 0.19311593
 0.19249065 0.19238268 0.19261673 0.19262384 0.19275838 0.19349787
 0.19483276 0.19533497 0.19557339 0.19556522 0.19550836 0.19532593
 0.19521277 0.19513643 0.19488971 0.1943852  0.19456095 0.19504137
 0.19497852 0.19539101 0.19558914 0.1956835  0.19599411 0.19592819
 0.19494809 0.19448262 0.19492872 0.19544582 0.19531591 0.19580455
 0.19753636 0.1984938  0.19870181 0.19855604 0.19820155 0.19809179
 0.19832444 0.19804546 0.19792062 0.1976035  0.19744764 0.19763812
 0.19780153 0.19813135 0.1982866  0.19829607 0.19836839 0.19872224
 0.19868146 0.19878696 0.19902055 0.19941528 0.19934326 0.19927065
 0.20016953 0.20068447 0.20095032 0.20109183 0.20127597 0.20146176
 0.20126072 0.20123547 0.20171663 0.20142923 0.20154718 0.20186868
 0.20158488 0.20211686 0.20261261 0.20198625 0.2020244  0.20188592
 0.20042555 0.19984823 0.19973299 0.19967346 0.19962142 0.19975694
 0.20133655 0.20204775 0.20182592 0.20151035 0.2012505  0.20119078
 0.20132051 0.20087937 0.20078659 0.20062071 0.20035617 0.20066686
 0.20095377 0.20087388 0.20117243 0.20160368 0.201806   0.20210034
 0.2019541  0.20192091 0.20248517 0.2031541  0.20318522 0.20329927
 0.20370404 0.20364827 0.20403898 0.20374353 0.20278451 0.20282191
 0.20336366 0.20323607 0.20349286 0.20349762 0.20369805 0.20411725
 0.20408814 0.20440057 0.20485866 0.20511746 0.20555423 0.2059041
 0.20501065 0.20405975 0.20371014 0.20385818 0.20355707 0.20348115
 0.20419955 0.20454863 0.20461227 0.20445575 0.20409825 0.20423579
 0.20422445 0.20419353 0.20455942 0.20435667 0.20445551 0.20484127
 0.20506984 0.20530732 0.20522882 0.20521106 0.20503561 0.20444134
 0.20334946 0.20229858 0.20189668 0.20179364 0.20117776 0.20081615
 0.20142521 0.20153485 0.20161062 0.20136264 0.20079    0.20074697
 0.20070297 0.20035665 0.2004685  0.20014119 0.20001489 0.20041052
 0.20040862 0.20080814 0.20114599 0.20075032 0.20075768 0.20034613
 0.19892253 0.19832925 0.19819361 0.19821441 0.19804302 0.1979995
 0.19885977 0.1991757  0.19931917 0.1993893  0.19891217 0.19900289
 0.198893   0.1984862  0.19862252 0.19834548 0.19826019 0.19862051
 0.19863041 0.19880474 0.19885495 0.19853137 0.19831896 0.19771364
 0.19624065 0.19500461 0.19463159 0.19462492 0.19424698 0.19408123
 0.19483623 0.19510527 0.19503485 0.19488804 0.19458409 0.19456927
 0.19469592 0.19455738 0.1946092  0.19443378 0.19434093 0.19461365
 0.19457743 0.1949112  0.1950588  0.19495106 0.19517739 0.19472234
 0.1932071  0.1922983  0.19178884 0.19150598 0.19106065 0.19106197
 0.19187573 0.1918034  0.191428   0.19114816 0.1907554  0.19089618
 0.19080585 0.19021343 0.19016889 0.18986084 0.18963186 0.18970326
 0.18951467 0.18953843 0.18956146 0.18980014 0.19021082 0.19010112
 0.18911703 0.18828632 0.18769819 0.18783197 0.18766125 0.18761533
 0.18854837 0.18850224 0.18830606 0.18789536 0.18733473 0.18719609
 0.18684015 0.1861381  0.18600996 0.18572405 0.18553787 0.18544357
 0.18523477 0.1855475  0.18537858 0.18488193 0.18517195 0.18452795
 0.18194392 0.17959207 0.17824751 0.17689146 0.17523082 0.17403074
 0.17392024 0.17342962 0.17309982 0.1726383  0.172183   0.17255382
 0.17252289 0.17209864 0.17210735 0.17195252 0.17190905 0.1719251
 0.17154355 0.17182577 0.17221828 0.17244287 0.17282492 0.17216226
 0.17068753 0.16979285 0.16895787 0.16833714 0.16755101 0.16717021
 0.16830057 0.16825281 0.16793966 0.16739744 0.16639581 0.16606134
 0.16562338 0.16487509 0.16430818 0.16316913 0.16321424 0.16316533
 0.16316697 0.16343376 0.1637769  0.16471124 0.16397946 0.16551013]
