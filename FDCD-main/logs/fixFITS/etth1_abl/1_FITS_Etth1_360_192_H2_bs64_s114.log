Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=42, out_features=64, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2408448.0
params:  2752.0
Trainable parameters:  2752
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.228013277053833
Epoch: 1, Steps: 63 | Train Loss: 0.6857432 Vali Loss: 1.3774536 Test Loss: 0.6689341
Validation loss decreased (inf --> 1.377454).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.230135440826416
Epoch: 2, Steps: 63 | Train Loss: 0.5415679 Vali Loss: 1.1824805 Test Loss: 0.5485747
Validation loss decreased (1.377454 --> 1.182480).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.1879444122314453
Epoch: 3, Steps: 63 | Train Loss: 0.4830782 Vali Loss: 1.0925496 Test Loss: 0.4948483
Validation loss decreased (1.182480 --> 1.092550).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.2699391841888428
Epoch: 4, Steps: 63 | Train Loss: 0.4544368 Vali Loss: 1.0406162 Test Loss: 0.4662712
Validation loss decreased (1.092550 --> 1.040616).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.2067151069641113
Epoch: 5, Steps: 63 | Train Loss: 0.4386866 Vali Loss: 1.0080907 Test Loss: 0.4505031
Validation loss decreased (1.040616 --> 1.008091).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.2272734642028809
Epoch: 6, Steps: 63 | Train Loss: 0.4293650 Vali Loss: 0.9886360 Test Loss: 0.4412663
Validation loss decreased (1.008091 --> 0.988636).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.3137660026550293
Epoch: 7, Steps: 63 | Train Loss: 0.4233179 Vali Loss: 0.9754504 Test Loss: 0.4361799
Validation loss decreased (0.988636 --> 0.975450).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.1831386089324951
Epoch: 8, Steps: 63 | Train Loss: 0.4196500 Vali Loss: 0.9669862 Test Loss: 0.4335110
Validation loss decreased (0.975450 --> 0.966986).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.1620087623596191
Epoch: 9, Steps: 63 | Train Loss: 0.4171801 Vali Loss: 0.9599125 Test Loss: 0.4319211
Validation loss decreased (0.966986 --> 0.959913).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.236985445022583
Epoch: 10, Steps: 63 | Train Loss: 0.4156898 Vali Loss: 0.9561003 Test Loss: 0.4312280
Validation loss decreased (0.959913 --> 0.956100).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.1952896118164062
Epoch: 11, Steps: 63 | Train Loss: 0.4146770 Vali Loss: 0.9527002 Test Loss: 0.4307979
Validation loss decreased (0.956100 --> 0.952700).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.1357147693634033
Epoch: 12, Steps: 63 | Train Loss: 0.4140944 Vali Loss: 0.9502608 Test Loss: 0.4306191
Validation loss decreased (0.952700 --> 0.950261).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.1577503681182861
Epoch: 13, Steps: 63 | Train Loss: 0.4133554 Vali Loss: 0.9482670 Test Loss: 0.4303745
Validation loss decreased (0.950261 --> 0.948267).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.2242848873138428
Epoch: 14, Steps: 63 | Train Loss: 0.4127992 Vali Loss: 0.9467846 Test Loss: 0.4304222
Validation loss decreased (0.948267 --> 0.946785).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.203467845916748
Epoch: 15, Steps: 63 | Train Loss: 0.4124974 Vali Loss: 0.9455488 Test Loss: 0.4303903
Validation loss decreased (0.946785 --> 0.945549).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.1398298740386963
Epoch: 16, Steps: 63 | Train Loss: 0.4121262 Vali Loss: 0.9442102 Test Loss: 0.4303570
Validation loss decreased (0.945549 --> 0.944210).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.1825029850006104
Epoch: 17, Steps: 63 | Train Loss: 0.4119749 Vali Loss: 0.9421095 Test Loss: 0.4302763
Validation loss decreased (0.944210 --> 0.942110).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.1404573917388916
Epoch: 18, Steps: 63 | Train Loss: 0.4116313 Vali Loss: 0.9419654 Test Loss: 0.4301606
Validation loss decreased (0.942110 --> 0.941965).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.2374234199523926
Epoch: 19, Steps: 63 | Train Loss: 0.4113618 Vali Loss: 0.9414080 Test Loss: 0.4302887
Validation loss decreased (0.941965 --> 0.941408).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.2466356754302979
Epoch: 20, Steps: 63 | Train Loss: 0.4114415 Vali Loss: 0.9407429 Test Loss: 0.4302365
Validation loss decreased (0.941408 --> 0.940743).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.144240379333496
Epoch: 21, Steps: 63 | Train Loss: 0.4112901 Vali Loss: 0.9398732 Test Loss: 0.4302844
Validation loss decreased (0.940743 --> 0.939873).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.342599868774414
Epoch: 22, Steps: 63 | Train Loss: 0.4110077 Vali Loss: 0.9391233 Test Loss: 0.4302425
Validation loss decreased (0.939873 --> 0.939123).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.2387290000915527
Epoch: 23, Steps: 63 | Train Loss: 0.4107011 Vali Loss: 0.9380545 Test Loss: 0.4301429
Validation loss decreased (0.939123 --> 0.938055).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.343810796737671
Epoch: 24, Steps: 63 | Train Loss: 0.4108592 Vali Loss: 0.9383428 Test Loss: 0.4302658
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.1709327697753906
Epoch: 25, Steps: 63 | Train Loss: 0.4109829 Vali Loss: 0.9378389 Test Loss: 0.4301632
Validation loss decreased (0.938055 --> 0.937839).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.360703945159912
Epoch: 26, Steps: 63 | Train Loss: 0.4105705 Vali Loss: 0.9369920 Test Loss: 0.4301166
Validation loss decreased (0.937839 --> 0.936992).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.333653211593628
Epoch: 27, Steps: 63 | Train Loss: 0.4106644 Vali Loss: 0.9370218 Test Loss: 0.4302130
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.3063008785247803
Epoch: 28, Steps: 63 | Train Loss: 0.4104464 Vali Loss: 0.9368017 Test Loss: 0.4301330
Validation loss decreased (0.936992 --> 0.936802).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.2176880836486816
Epoch: 29, Steps: 63 | Train Loss: 0.4104397 Vali Loss: 0.9367523 Test Loss: 0.4301084
Validation loss decreased (0.936802 --> 0.936752).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.378960371017456
Epoch: 30, Steps: 63 | Train Loss: 0.4106679 Vali Loss: 0.9362167 Test Loss: 0.4301164
Validation loss decreased (0.936752 --> 0.936217).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.3780004978179932
Epoch: 31, Steps: 63 | Train Loss: 0.4102812 Vali Loss: 0.9357337 Test Loss: 0.4301231
Validation loss decreased (0.936217 --> 0.935734).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.2874705791473389
Epoch: 32, Steps: 63 | Train Loss: 0.4105613 Vali Loss: 0.9359944 Test Loss: 0.4301693
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4281272888183594
Epoch: 33, Steps: 63 | Train Loss: 0.4100363 Vali Loss: 0.9355526 Test Loss: 0.4301624
Validation loss decreased (0.935734 --> 0.935553).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.2929763793945312
Epoch: 34, Steps: 63 | Train Loss: 0.4099313 Vali Loss: 0.9354812 Test Loss: 0.4301412
Validation loss decreased (0.935553 --> 0.935481).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.2353236675262451
Epoch: 35, Steps: 63 | Train Loss: 0.4101573 Vali Loss: 0.9351550 Test Loss: 0.4301408
Validation loss decreased (0.935481 --> 0.935155).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.2222309112548828
Epoch: 36, Steps: 63 | Train Loss: 0.4102579 Vali Loss: 0.9347860 Test Loss: 0.4301209
Validation loss decreased (0.935155 --> 0.934786).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.2839977741241455
Epoch: 37, Steps: 63 | Train Loss: 0.4102540 Vali Loss: 0.9346582 Test Loss: 0.4300962
Validation loss decreased (0.934786 --> 0.934658).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.195868968963623
Epoch: 38, Steps: 63 | Train Loss: 0.4101387 Vali Loss: 0.9341723 Test Loss: 0.4300385
Validation loss decreased (0.934658 --> 0.934172).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.249403953552246
Epoch: 39, Steps: 63 | Train Loss: 0.4102149 Vali Loss: 0.9343940 Test Loss: 0.4300406
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.1669142246246338
Epoch: 40, Steps: 63 | Train Loss: 0.4101288 Vali Loss: 0.9344112 Test Loss: 0.4301183
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.2478222846984863
Epoch: 41, Steps: 63 | Train Loss: 0.4100374 Vali Loss: 0.9341180 Test Loss: 0.4300951
Validation loss decreased (0.934172 --> 0.934118).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.2460930347442627
Epoch: 42, Steps: 63 | Train Loss: 0.4100023 Vali Loss: 0.9338023 Test Loss: 0.4300411
Validation loss decreased (0.934118 --> 0.933802).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.197402000427246
Epoch: 43, Steps: 63 | Train Loss: 0.4098115 Vali Loss: 0.9332277 Test Loss: 0.4300902
Validation loss decreased (0.933802 --> 0.933228).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.1823546886444092
Epoch: 44, Steps: 63 | Train Loss: 0.4099259 Vali Loss: 0.9335954 Test Loss: 0.4301021
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.3273992538452148
Epoch: 45, Steps: 63 | Train Loss: 0.4098135 Vali Loss: 0.9338825 Test Loss: 0.4300580
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.2398204803466797
Epoch: 46, Steps: 63 | Train Loss: 0.4098168 Vali Loss: 0.9336421 Test Loss: 0.4300768
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.1392250061035156
Epoch: 47, Steps: 63 | Train Loss: 0.4097694 Vali Loss: 0.9337559 Test Loss: 0.4300960
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.173245906829834
Epoch: 48, Steps: 63 | Train Loss: 0.4097476 Vali Loss: 0.9333266 Test Loss: 0.4300895
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.2181365489959717
Epoch: 49, Steps: 63 | Train Loss: 0.4098239 Vali Loss: 0.9333282 Test Loss: 0.4300831
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2242884635925293
Epoch: 50, Steps: 63 | Train Loss: 0.4095906 Vali Loss: 0.9334538 Test Loss: 0.4300729
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.1680078506469727
Epoch: 51, Steps: 63 | Train Loss: 0.4096706 Vali Loss: 0.9329623 Test Loss: 0.4300965
Validation loss decreased (0.933228 --> 0.932962).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.2119691371917725
Epoch: 52, Steps: 63 | Train Loss: 0.4097739 Vali Loss: 0.9333711 Test Loss: 0.4300755
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.2547202110290527
Epoch: 53, Steps: 63 | Train Loss: 0.4097052 Vali Loss: 0.9332301 Test Loss: 0.4300922
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.3157958984375
Epoch: 54, Steps: 63 | Train Loss: 0.4095612 Vali Loss: 0.9329583 Test Loss: 0.4300630
Validation loss decreased (0.932962 --> 0.932958).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.2685301303863525
Epoch: 55, Steps: 63 | Train Loss: 0.4095735 Vali Loss: 0.9332008 Test Loss: 0.4300880
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.301396369934082
Epoch: 56, Steps: 63 | Train Loss: 0.4097372 Vali Loss: 0.9330354 Test Loss: 0.4300756
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.2165031433105469
Epoch: 57, Steps: 63 | Train Loss: 0.4098002 Vali Loss: 0.9327172 Test Loss: 0.4300631
Validation loss decreased (0.932958 --> 0.932717).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.1738247871398926
Epoch: 58, Steps: 63 | Train Loss: 0.4096291 Vali Loss: 0.9324431 Test Loss: 0.4300560
Validation loss decreased (0.932717 --> 0.932443).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.209524154663086
Epoch: 59, Steps: 63 | Train Loss: 0.4095953 Vali Loss: 0.9329219 Test Loss: 0.4300810
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.1945655345916748
Epoch: 60, Steps: 63 | Train Loss: 0.4097031 Vali Loss: 0.9330395 Test Loss: 0.4300790
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.1795997619628906
Epoch: 61, Steps: 63 | Train Loss: 0.4093825 Vali Loss: 0.9324508 Test Loss: 0.4300697
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.1898117065429688
Epoch: 62, Steps: 63 | Train Loss: 0.4095108 Vali Loss: 0.9327408 Test Loss: 0.4300991
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.1652753353118896
Epoch: 63, Steps: 63 | Train Loss: 0.4095430 Vali Loss: 0.9325653 Test Loss: 0.4300853
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.2103731632232666
Epoch: 64, Steps: 63 | Train Loss: 0.4095839 Vali Loss: 0.9327244 Test Loss: 0.4300887
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.3159430027008057
Epoch: 65, Steps: 63 | Train Loss: 0.4095180 Vali Loss: 0.9328324 Test Loss: 0.4300927
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.2221410274505615
Epoch: 66, Steps: 63 | Train Loss: 0.4096173 Vali Loss: 0.9327363 Test Loss: 0.4300931
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.2364554405212402
Epoch: 67, Steps: 63 | Train Loss: 0.4095042 Vali Loss: 0.9325508 Test Loss: 0.4300809
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.2112722396850586
Epoch: 68, Steps: 63 | Train Loss: 0.4094795 Vali Loss: 0.9326555 Test Loss: 0.4301001
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3560001850128174
Epoch: 69, Steps: 63 | Train Loss: 0.4097120 Vali Loss: 0.9326780 Test Loss: 0.4300728
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.2640395164489746
Epoch: 70, Steps: 63 | Train Loss: 0.4095168 Vali Loss: 0.9321977 Test Loss: 0.4301007
Validation loss decreased (0.932443 --> 0.932198).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.1362762451171875
Epoch: 71, Steps: 63 | Train Loss: 0.4095438 Vali Loss: 0.9323037 Test Loss: 0.4300888
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.282078742980957
Epoch: 72, Steps: 63 | Train Loss: 0.4094332 Vali Loss: 0.9316833 Test Loss: 0.4300942
Validation loss decreased (0.932198 --> 0.931683).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.20613431930542
Epoch: 73, Steps: 63 | Train Loss: 0.4096113 Vali Loss: 0.9327342 Test Loss: 0.4300886
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.1858468055725098
Epoch: 74, Steps: 63 | Train Loss: 0.4094236 Vali Loss: 0.9325829 Test Loss: 0.4300974
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.2320561408996582
Epoch: 75, Steps: 63 | Train Loss: 0.4096484 Vali Loss: 0.9321247 Test Loss: 0.4301007
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.2200353145599365
Epoch: 76, Steps: 63 | Train Loss: 0.4095694 Vali Loss: 0.9326620 Test Loss: 0.4300863
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.2205719947814941
Epoch: 77, Steps: 63 | Train Loss: 0.4095700 Vali Loss: 0.9326100 Test Loss: 0.4300959
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.21919584274292
Epoch: 78, Steps: 63 | Train Loss: 0.4094681 Vali Loss: 0.9323161 Test Loss: 0.4300985
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.3405606746673584
Epoch: 79, Steps: 63 | Train Loss: 0.4094811 Vali Loss: 0.9325477 Test Loss: 0.4300850
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.2360496520996094
Epoch: 80, Steps: 63 | Train Loss: 0.4095694 Vali Loss: 0.9325673 Test Loss: 0.4300978
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.2074503898620605
Epoch: 81, Steps: 63 | Train Loss: 0.4095578 Vali Loss: 0.9321322 Test Loss: 0.4300885
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.2422621250152588
Epoch: 82, Steps: 63 | Train Loss: 0.4094825 Vali Loss: 0.9320096 Test Loss: 0.4300923
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.361558198928833
Epoch: 83, Steps: 63 | Train Loss: 0.4096639 Vali Loss: 0.9322127 Test Loss: 0.4300944
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.2797565460205078
Epoch: 84, Steps: 63 | Train Loss: 0.4094079 Vali Loss: 0.9325293 Test Loss: 0.4300936
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.311063528060913
Epoch: 85, Steps: 63 | Train Loss: 0.4097891 Vali Loss: 0.9324297 Test Loss: 0.4300942
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.1077766418457031
Epoch: 86, Steps: 63 | Train Loss: 0.4093867 Vali Loss: 0.9325568 Test Loss: 0.4300898
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.141052007675171
Epoch: 87, Steps: 63 | Train Loss: 0.4093051 Vali Loss: 0.9322009 Test Loss: 0.4300890
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.2352449893951416
Epoch: 88, Steps: 63 | Train Loss: 0.4095305 Vali Loss: 0.9314818 Test Loss: 0.4300880
Validation loss decreased (0.931683 --> 0.931482).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.2505404949188232
Epoch: 89, Steps: 63 | Train Loss: 0.4093186 Vali Loss: 0.9321830 Test Loss: 0.4300876
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.1815056800842285
Epoch: 90, Steps: 63 | Train Loss: 0.4094882 Vali Loss: 0.9321410 Test Loss: 0.4300892
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.158553123474121
Epoch: 91, Steps: 63 | Train Loss: 0.4090200 Vali Loss: 0.9322971 Test Loss: 0.4300959
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.2328855991363525
Epoch: 92, Steps: 63 | Train Loss: 0.4093498 Vali Loss: 0.9316451 Test Loss: 0.4300942
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.107555627822876
Epoch: 93, Steps: 63 | Train Loss: 0.4096093 Vali Loss: 0.9323573 Test Loss: 0.4300942
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.1136507987976074
Epoch: 94, Steps: 63 | Train Loss: 0.4096101 Vali Loss: 0.9324639 Test Loss: 0.4300979
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.1877048015594482
Epoch: 95, Steps: 63 | Train Loss: 0.4094009 Vali Loss: 0.9321885 Test Loss: 0.4300947
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.2341113090515137
Epoch: 96, Steps: 63 | Train Loss: 0.4095830 Vali Loss: 0.9322829 Test Loss: 0.4300932
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.2574706077575684
Epoch: 97, Steps: 63 | Train Loss: 0.4095600 Vali Loss: 0.9323260 Test Loss: 0.4300901
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.2481000423431396
Epoch: 98, Steps: 63 | Train Loss: 0.4096342 Vali Loss: 0.9322607 Test Loss: 0.4300925
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.2138004302978516
Epoch: 99, Steps: 63 | Train Loss: 0.4091896 Vali Loss: 0.9319009 Test Loss: 0.4300923
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.1657910346984863
Epoch: 100, Steps: 63 | Train Loss: 0.4096872 Vali Loss: 0.9323796 Test Loss: 0.4300919
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4264375865459442, mae:0.43538662791252136, rse:0.6201333403587341, corr:[0.26005137 0.26731578 0.26916605 0.26736626 0.26436457 0.2621147
 0.26108536 0.26115605 0.26161307 0.26206973 0.26224926 0.26202023
 0.26158693 0.2612124  0.26103458 0.26102152 0.26090637 0.26066
 0.26040924 0.2602244  0.2602336  0.2604971  0.26077467 0.2609957
 0.260986   0.26071423 0.26023337 0.25967273 0.2590743  0.25854388
 0.25813496 0.25782174 0.25762224 0.2575277  0.2575225  0.2575917
 0.2577422  0.25805736 0.25836742 0.25853673 0.25857037 0.25844127
 0.25829616 0.25827572 0.25843325 0.25868002 0.25889844 0.2588488
 0.2582786  0.25721282 0.25574106 0.25432554 0.25324833 0.25251338
 0.2522401  0.25225863 0.2523106  0.252262   0.25204515 0.25179684
 0.25161907 0.2515307  0.2515529  0.25171128 0.25188342 0.2520459
 0.25219697 0.25218043 0.2521094  0.25212178 0.25216824 0.25202864
 0.25159588 0.25092572 0.25018427 0.24950041 0.2489636  0.24855481
 0.24834095 0.24821666 0.2481129  0.24793261 0.24767606 0.24741276
 0.24732591 0.24739197 0.24747652 0.24747607 0.24737823 0.24724549
 0.24707079 0.24690092 0.24680273 0.24695753 0.24736173 0.24784853
 0.24834369 0.24861787 0.24870314 0.24856582 0.2482869  0.2479616
 0.2476649  0.24745554 0.24728654 0.24718328 0.24711521 0.24704364
 0.2469962  0.24706134 0.24725278 0.24754487 0.24768205 0.24769919
 0.2477247  0.24773282 0.24769089 0.24762815 0.2475232  0.24725693
 0.2467667  0.24601945 0.24514191 0.24432021 0.24374533 0.24338736
 0.24327636 0.24326137 0.24316724 0.24290372 0.2426121  0.24242724
 0.24234617 0.24235395 0.2424783  0.24259916 0.24278042 0.24291751
 0.24299887 0.24297969 0.24292435 0.24293348 0.24296844 0.24280562
 0.24243486 0.24170041 0.24072504 0.239713   0.23905674 0.23871869
 0.23871006 0.23896651 0.23923121 0.23940618 0.23937318 0.23916684
 0.23883772 0.23859863 0.23852581 0.23858789 0.23863392 0.2387753
 0.2389628  0.23905963 0.23916824 0.239388   0.2397289  0.23998155
 0.24012928 0.24005209 0.23962179 0.23891991 0.23822875 0.23784252
 0.237856   0.23813379 0.2383908  0.23864856 0.2388304  0.23880754
 0.2386462  0.23838933 0.23820466 0.23811293 0.23814729 0.238502
 0.23910634 0.23970774 0.24016388 0.24009413 0.23826647 0.23281087]
