Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=74, out_features=113, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7492352.0
params:  8475.0
Trainable parameters:  8475
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.0888400077819824
Epoch: 1, Steps: 63 | Train Loss: 0.6808535 Vali Loss: 1.7086354 Test Loss: 0.9152221
Validation loss decreased (inf --> 1.708635).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8592946529388428
Epoch: 2, Steps: 63 | Train Loss: 0.5532325 Vali Loss: 1.5305530 Test Loss: 0.8070747
Validation loss decreased (1.708635 --> 1.530553).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.3782479763031006
Epoch: 3, Steps: 63 | Train Loss: 0.4823095 Vali Loss: 1.4373714 Test Loss: 0.7518930
Validation loss decreased (1.530553 --> 1.437371).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.1534111499786377
Epoch: 4, Steps: 63 | Train Loss: 0.4395577 Vali Loss: 1.3802636 Test Loss: 0.7179531
Validation loss decreased (1.437371 --> 1.380264).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.603835105895996
Epoch: 5, Steps: 63 | Train Loss: 0.4100153 Vali Loss: 1.3434050 Test Loss: 0.6964639
Validation loss decreased (1.380264 --> 1.343405).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.54677677154541
Epoch: 6, Steps: 63 | Train Loss: 0.3878646 Vali Loss: 1.3166647 Test Loss: 0.6809302
Validation loss decreased (1.343405 --> 1.316665).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.8753318786621094
Epoch: 7, Steps: 63 | Train Loss: 0.3698196 Vali Loss: 1.2920611 Test Loss: 0.6657987
Validation loss decreased (1.316665 --> 1.292061).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.992384672164917
Epoch: 8, Steps: 63 | Train Loss: 0.3544074 Vali Loss: 1.2718867 Test Loss: 0.6535080
Validation loss decreased (1.292061 --> 1.271887).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9353325366973877
Epoch: 9, Steps: 63 | Train Loss: 0.3412960 Vali Loss: 1.2531893 Test Loss: 0.6411438
Validation loss decreased (1.271887 --> 1.253189).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.9802465438842773
Epoch: 10, Steps: 63 | Train Loss: 0.3293149 Vali Loss: 1.2370020 Test Loss: 0.6305434
Validation loss decreased (1.253189 --> 1.237002).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7717454433441162
Epoch: 11, Steps: 63 | Train Loss: 0.3191585 Vali Loss: 1.2211970 Test Loss: 0.6199599
Validation loss decreased (1.237002 --> 1.221197).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1850271224975586
Epoch: 12, Steps: 63 | Train Loss: 0.3098909 Vali Loss: 1.2087095 Test Loss: 0.6113093
Validation loss decreased (1.221197 --> 1.208709).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.935488224029541
Epoch: 13, Steps: 63 | Train Loss: 0.3015754 Vali Loss: 1.1954474 Test Loss: 0.6026184
Validation loss decreased (1.208709 --> 1.195447).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.0960497856140137
Epoch: 14, Steps: 63 | Train Loss: 0.2940516 Vali Loss: 1.1849078 Test Loss: 0.5955069
Validation loss decreased (1.195447 --> 1.184908).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.119471311569214
Epoch: 15, Steps: 63 | Train Loss: 0.2874634 Vali Loss: 1.1750640 Test Loss: 0.5886816
Validation loss decreased (1.184908 --> 1.175064).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.0052952766418457
Epoch: 16, Steps: 63 | Train Loss: 0.2814132 Vali Loss: 1.1656287 Test Loss: 0.5818759
Validation loss decreased (1.175064 --> 1.165629).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.6230080127716064
Epoch: 17, Steps: 63 | Train Loss: 0.2757892 Vali Loss: 1.1569510 Test Loss: 0.5759097
Validation loss decreased (1.165629 --> 1.156951).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.989403486251831
Epoch: 18, Steps: 63 | Train Loss: 0.2708375 Vali Loss: 1.1478393 Test Loss: 0.5697616
Validation loss decreased (1.156951 --> 1.147839).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.9760076999664307
Epoch: 19, Steps: 63 | Train Loss: 0.2661242 Vali Loss: 1.1404021 Test Loss: 0.5641218
Validation loss decreased (1.147839 --> 1.140402).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.275435209274292
Epoch: 20, Steps: 63 | Train Loss: 0.2619389 Vali Loss: 1.1329938 Test Loss: 0.5594025
Validation loss decreased (1.140402 --> 1.132994).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.35900616645813
Epoch: 21, Steps: 63 | Train Loss: 0.2580310 Vali Loss: 1.1264566 Test Loss: 0.5545239
Validation loss decreased (1.132994 --> 1.126457).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.7188668251037598
Epoch: 22, Steps: 63 | Train Loss: 0.2543666 Vali Loss: 1.1205274 Test Loss: 0.5503833
Validation loss decreased (1.126457 --> 1.120527).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.494293212890625
Epoch: 23, Steps: 63 | Train Loss: 0.2510783 Vali Loss: 1.1146613 Test Loss: 0.5462371
Validation loss decreased (1.120527 --> 1.114661).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.5287749767303467
Epoch: 24, Steps: 63 | Train Loss: 0.2481516 Vali Loss: 1.1087365 Test Loss: 0.5419178
Validation loss decreased (1.114661 --> 1.108737).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.068044662475586
Epoch: 25, Steps: 63 | Train Loss: 0.2453086 Vali Loss: 1.1038831 Test Loss: 0.5387352
Validation loss decreased (1.108737 --> 1.103883).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.322490692138672
Epoch: 26, Steps: 63 | Train Loss: 0.2425958 Vali Loss: 1.0993373 Test Loss: 0.5352612
Validation loss decreased (1.103883 --> 1.099337).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1073992252349854
Epoch: 27, Steps: 63 | Train Loss: 0.2404281 Vali Loss: 1.0953153 Test Loss: 0.5323387
Validation loss decreased (1.099337 --> 1.095315).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.0568857192993164
Epoch: 28, Steps: 63 | Train Loss: 0.2380631 Vali Loss: 1.0905275 Test Loss: 0.5292316
Validation loss decreased (1.095315 --> 1.090528).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.9913039207458496
Epoch: 29, Steps: 63 | Train Loss: 0.2360163 Vali Loss: 1.0868874 Test Loss: 0.5264266
Validation loss decreased (1.090528 --> 1.086887).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.7863476276397705
Epoch: 30, Steps: 63 | Train Loss: 0.2340537 Vali Loss: 1.0826616 Test Loss: 0.5237718
Validation loss decreased (1.086887 --> 1.082662).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.211742877960205
Epoch: 31, Steps: 63 | Train Loss: 0.2322347 Vali Loss: 1.0792083 Test Loss: 0.5212811
Validation loss decreased (1.082662 --> 1.079208).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.0923733711242676
Epoch: 32, Steps: 63 | Train Loss: 0.2306102 Vali Loss: 1.0765207 Test Loss: 0.5190421
Validation loss decreased (1.079208 --> 1.076521).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.167532444000244
Epoch: 33, Steps: 63 | Train Loss: 0.2289671 Vali Loss: 1.0734060 Test Loss: 0.5168055
Validation loss decreased (1.076521 --> 1.073406).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.849076747894287
Epoch: 34, Steps: 63 | Train Loss: 0.2274301 Vali Loss: 1.0704925 Test Loss: 0.5148001
Validation loss decreased (1.073406 --> 1.070493).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.521909236907959
Epoch: 35, Steps: 63 | Train Loss: 0.2261731 Vali Loss: 1.0677570 Test Loss: 0.5128545
Validation loss decreased (1.070493 --> 1.067757).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.7937753200531006
Epoch: 36, Steps: 63 | Train Loss: 0.2247416 Vali Loss: 1.0650041 Test Loss: 0.5110258
Validation loss decreased (1.067757 --> 1.065004).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.6335608959198
Epoch: 37, Steps: 63 | Train Loss: 0.2236738 Vali Loss: 1.0625045 Test Loss: 0.5093297
Validation loss decreased (1.065004 --> 1.062505).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.7433841228485107
Epoch: 38, Steps: 63 | Train Loss: 0.2224002 Vali Loss: 1.0602789 Test Loss: 0.5076961
Validation loss decreased (1.062505 --> 1.060279).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.114865779876709
Epoch: 39, Steps: 63 | Train Loss: 0.2213041 Vali Loss: 1.0582707 Test Loss: 0.5061083
Validation loss decreased (1.060279 --> 1.058271).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.985200643539429
Epoch: 40, Steps: 63 | Train Loss: 0.2204036 Vali Loss: 1.0557647 Test Loss: 0.5046927
Validation loss decreased (1.058271 --> 1.055765).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.763674020767212
Epoch: 41, Steps: 63 | Train Loss: 0.2194582 Vali Loss: 1.0538400 Test Loss: 0.5033872
Validation loss decreased (1.055765 --> 1.053840).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.7661631107330322
Epoch: 42, Steps: 63 | Train Loss: 0.2185152 Vali Loss: 1.0522649 Test Loss: 0.5020403
Validation loss decreased (1.053840 --> 1.052265).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.3886067867279053
Epoch: 43, Steps: 63 | Train Loss: 0.2176988 Vali Loss: 1.0503420 Test Loss: 0.5007023
Validation loss decreased (1.052265 --> 1.050342).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.8247401714324951
Epoch: 44, Steps: 63 | Train Loss: 0.2168950 Vali Loss: 1.0485477 Test Loss: 0.4996148
Validation loss decreased (1.050342 --> 1.048548).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9771771430969238
Epoch: 45, Steps: 63 | Train Loss: 0.2160524 Vali Loss: 1.0472609 Test Loss: 0.4985544
Validation loss decreased (1.048548 --> 1.047261).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.2483818531036377
Epoch: 46, Steps: 63 | Train Loss: 0.2153146 Vali Loss: 1.0458571 Test Loss: 0.4973893
Validation loss decreased (1.047261 --> 1.045857).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.9016978740692139
Epoch: 47, Steps: 63 | Train Loss: 0.2146909 Vali Loss: 1.0443196 Test Loss: 0.4964018
Validation loss decreased (1.045857 --> 1.044320).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.909489631652832
Epoch: 48, Steps: 63 | Train Loss: 0.2139371 Vali Loss: 1.0432795 Test Loss: 0.4955279
Validation loss decreased (1.044320 --> 1.043280).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.199744939804077
Epoch: 49, Steps: 63 | Train Loss: 0.2135054 Vali Loss: 1.0414487 Test Loss: 0.4945135
Validation loss decreased (1.043280 --> 1.041449).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.644686222076416
Epoch: 50, Steps: 63 | Train Loss: 0.2129449 Vali Loss: 1.0404475 Test Loss: 0.4936669
Validation loss decreased (1.041449 --> 1.040447).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.866833209991455
Epoch: 51, Steps: 63 | Train Loss: 0.2123764 Vali Loss: 1.0390737 Test Loss: 0.4928533
Validation loss decreased (1.040447 --> 1.039074).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.828806161880493
Epoch: 52, Steps: 63 | Train Loss: 0.2118199 Vali Loss: 1.0378011 Test Loss: 0.4920990
Validation loss decreased (1.039074 --> 1.037801).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.419048547744751
Epoch: 53, Steps: 63 | Train Loss: 0.2114036 Vali Loss: 1.0368973 Test Loss: 0.4913374
Validation loss decreased (1.037801 --> 1.036897).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.409543037414551
Epoch: 54, Steps: 63 | Train Loss: 0.2109909 Vali Loss: 1.0363513 Test Loss: 0.4907320
Validation loss decreased (1.036897 --> 1.036351).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.0451364517211914
Epoch: 55, Steps: 63 | Train Loss: 0.2104841 Vali Loss: 1.0352536 Test Loss: 0.4900182
Validation loss decreased (1.036351 --> 1.035254).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.005911111831665
Epoch: 56, Steps: 63 | Train Loss: 0.2101278 Vali Loss: 1.0344409 Test Loss: 0.4893687
Validation loss decreased (1.035254 --> 1.034441).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.1129159927368164
Epoch: 57, Steps: 63 | Train Loss: 0.2097007 Vali Loss: 1.0335169 Test Loss: 0.4887570
Validation loss decreased (1.034441 --> 1.033517).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.3161535263061523
Epoch: 58, Steps: 63 | Train Loss: 0.2093719 Vali Loss: 1.0326707 Test Loss: 0.4882161
Validation loss decreased (1.033517 --> 1.032671).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.2531533241271973
Epoch: 59, Steps: 63 | Train Loss: 0.2090007 Vali Loss: 1.0315859 Test Loss: 0.4877051
Validation loss decreased (1.032671 --> 1.031586).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.874845266342163
Epoch: 60, Steps: 63 | Train Loss: 0.2087281 Vali Loss: 1.0309712 Test Loss: 0.4872035
Validation loss decreased (1.031586 --> 1.030971).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.3803186416625977
Epoch: 61, Steps: 63 | Train Loss: 0.2083077 Vali Loss: 1.0305086 Test Loss: 0.4867301
Validation loss decreased (1.030971 --> 1.030509).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.575897216796875
Epoch: 62, Steps: 63 | Train Loss: 0.2080321 Vali Loss: 1.0300058 Test Loss: 0.4863008
Validation loss decreased (1.030509 --> 1.030006).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.022719621658325
Epoch: 63, Steps: 63 | Train Loss: 0.2077156 Vali Loss: 1.0288442 Test Loss: 0.4858455
Validation loss decreased (1.030006 --> 1.028844).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.8725814819335938
Epoch: 64, Steps: 63 | Train Loss: 0.2074514 Vali Loss: 1.0290375 Test Loss: 0.4855169
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.8116450309753418
Epoch: 65, Steps: 63 | Train Loss: 0.2072810 Vali Loss: 1.0280550 Test Loss: 0.4850929
Validation loss decreased (1.028844 --> 1.028055).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.732182741165161
Epoch: 66, Steps: 63 | Train Loss: 0.2069814 Vali Loss: 1.0277249 Test Loss: 0.4846371
Validation loss decreased (1.028055 --> 1.027725).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.251821756362915
Epoch: 67, Steps: 63 | Train Loss: 0.2067077 Vali Loss: 1.0267190 Test Loss: 0.4843408
Validation loss decreased (1.027725 --> 1.026719).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.2949564456939697
Epoch: 68, Steps: 63 | Train Loss: 0.2065676 Vali Loss: 1.0268421 Test Loss: 0.4840254
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.191084623336792
Epoch: 69, Steps: 63 | Train Loss: 0.2062727 Vali Loss: 1.0261796 Test Loss: 0.4836525
Validation loss decreased (1.026719 --> 1.026180).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.432224750518799
Epoch: 70, Steps: 63 | Train Loss: 0.2060817 Vali Loss: 1.0258131 Test Loss: 0.4833684
Validation loss decreased (1.026180 --> 1.025813).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.9201934337615967
Epoch: 71, Steps: 63 | Train Loss: 0.2058707 Vali Loss: 1.0252835 Test Loss: 0.4830446
Validation loss decreased (1.025813 --> 1.025283).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.38366436958313
Epoch: 72, Steps: 63 | Train Loss: 0.2057584 Vali Loss: 1.0247866 Test Loss: 0.4828171
Validation loss decreased (1.025283 --> 1.024787).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.8057103157043457
Epoch: 73, Steps: 63 | Train Loss: 0.2055932 Vali Loss: 1.0245531 Test Loss: 0.4825532
Validation loss decreased (1.024787 --> 1.024553).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.72267746925354
Epoch: 74, Steps: 63 | Train Loss: 0.2053895 Vali Loss: 1.0241121 Test Loss: 0.4822739
Validation loss decreased (1.024553 --> 1.024112).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.8376939296722412
Epoch: 75, Steps: 63 | Train Loss: 0.2052223 Vali Loss: 1.0238502 Test Loss: 0.4820282
Validation loss decreased (1.024112 --> 1.023850).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.5091946125030518
Epoch: 76, Steps: 63 | Train Loss: 0.2050250 Vali Loss: 1.0231694 Test Loss: 0.4818110
Validation loss decreased (1.023850 --> 1.023169).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.0436055660247803
Epoch: 77, Steps: 63 | Train Loss: 0.2048977 Vali Loss: 1.0233020 Test Loss: 0.4815932
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.8784751892089844
Epoch: 78, Steps: 63 | Train Loss: 0.2048355 Vali Loss: 1.0229019 Test Loss: 0.4813975
Validation loss decreased (1.023169 --> 1.022902).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.0902135372161865
Epoch: 79, Steps: 63 | Train Loss: 0.2046017 Vali Loss: 1.0226364 Test Loss: 0.4811892
Validation loss decreased (1.022902 --> 1.022636).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.878988265991211
Epoch: 80, Steps: 63 | Train Loss: 0.2046786 Vali Loss: 1.0220194 Test Loss: 0.4810139
Validation loss decreased (1.022636 --> 1.022019).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.324084997177124
Epoch: 81, Steps: 63 | Train Loss: 0.2045026 Vali Loss: 1.0221335 Test Loss: 0.4808047
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.7207186222076416
Epoch: 82, Steps: 63 | Train Loss: 0.2043034 Vali Loss: 1.0210842 Test Loss: 0.4806360
Validation loss decreased (1.022019 --> 1.021084).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.3493452072143555
Epoch: 83, Steps: 63 | Train Loss: 0.2041522 Vali Loss: 1.0212374 Test Loss: 0.4804946
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.408064842224121
Epoch: 84, Steps: 63 | Train Loss: 0.2041672 Vali Loss: 1.0212523 Test Loss: 0.4803550
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.7705397605895996
Epoch: 85, Steps: 63 | Train Loss: 0.2040937 Vali Loss: 1.0208204 Test Loss: 0.4802054
Validation loss decreased (1.021084 --> 1.020820).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.6249637603759766
Epoch: 86, Steps: 63 | Train Loss: 0.2039902 Vali Loss: 1.0207294 Test Loss: 0.4800628
Validation loss decreased (1.020820 --> 1.020729).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.378866672515869
Epoch: 87, Steps: 63 | Train Loss: 0.2039150 Vali Loss: 1.0204332 Test Loss: 0.4799438
Validation loss decreased (1.020729 --> 1.020433).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.799705743789673
Epoch: 88, Steps: 63 | Train Loss: 0.2038604 Vali Loss: 1.0204775 Test Loss: 0.4798124
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.4689059257507324
Epoch: 89, Steps: 63 | Train Loss: 0.2037238 Vali Loss: 1.0204458 Test Loss: 0.4797066
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 4.397832870483398
Epoch: 90, Steps: 63 | Train Loss: 0.2036407 Vali Loss: 1.0197523 Test Loss: 0.4795930
Validation loss decreased (1.020433 --> 1.019752).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.4679195880889893
Epoch: 91, Steps: 63 | Train Loss: 0.2036408 Vali Loss: 1.0195793 Test Loss: 0.4794739
Validation loss decreased (1.019752 --> 1.019579).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.6777584552764893
Epoch: 92, Steps: 63 | Train Loss: 0.2035443 Vali Loss: 1.0199963 Test Loss: 0.4793763
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.9673528671264648
Epoch: 93, Steps: 63 | Train Loss: 0.2034796 Vali Loss: 1.0193566 Test Loss: 0.4792829
Validation loss decreased (1.019579 --> 1.019357).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.2395272254943848
Epoch: 94, Steps: 63 | Train Loss: 0.2034430 Vali Loss: 1.0197899 Test Loss: 0.4791802
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.3475117683410645
Epoch: 95, Steps: 63 | Train Loss: 0.2032767 Vali Loss: 1.0196366 Test Loss: 0.4790987
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.8566453456878662
Epoch: 96, Steps: 63 | Train Loss: 0.2032913 Vali Loss: 1.0192907 Test Loss: 0.4790133
Validation loss decreased (1.019357 --> 1.019291).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.825415849685669
Epoch: 97, Steps: 63 | Train Loss: 0.2032357 Vali Loss: 1.0194540 Test Loss: 0.4789346
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.6072871685028076
Epoch: 98, Steps: 63 | Train Loss: 0.2031389 Vali Loss: 1.0188590 Test Loss: 0.4788568
Validation loss decreased (1.019291 --> 1.018859).  Saving model ...
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.805391311645508
Epoch: 99, Steps: 63 | Train Loss: 0.2031314 Vali Loss: 1.0186201 Test Loss: 0.4787881
Validation loss decreased (1.018859 --> 1.018620).  Saving model ...
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.90139102935791
Epoch: 100, Steps: 63 | Train Loss: 0.2030646 Vali Loss: 1.0187156 Test Loss: 0.4787229
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1160680107021042e-06
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=74, out_features=113, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7492352.0
params:  8475.0
Trainable parameters:  8475
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.136479616165161
Epoch: 1, Steps: 63 | Train Loss: 0.4296909 Vali Loss: 0.9558177 Test Loss: 0.4346595
Validation loss decreased (inf --> 0.955818).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.1675491333007812
Epoch: 2, Steps: 63 | Train Loss: 0.4109957 Vali Loss: 0.9317678 Test Loss: 0.4186281
Validation loss decreased (0.955818 --> 0.931768).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.327483892440796
Epoch: 3, Steps: 63 | Train Loss: 0.4040026 Vali Loss: 0.9245099 Test Loss: 0.4146664
Validation loss decreased (0.931768 --> 0.924510).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.8966238498687744
Epoch: 4, Steps: 63 | Train Loss: 0.4011281 Vali Loss: 0.9217949 Test Loss: 0.4136533
Validation loss decreased (0.924510 --> 0.921795).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.9893076419830322
Epoch: 5, Steps: 63 | Train Loss: 0.4002011 Vali Loss: 0.9200475 Test Loss: 0.4135257
Validation loss decreased (0.921795 --> 0.920047).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.1112825870513916
Epoch: 6, Steps: 63 | Train Loss: 0.3998338 Vali Loss: 0.9195404 Test Loss: 0.4135585
Validation loss decreased (0.920047 --> 0.919540).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.4974591732025146
Epoch: 7, Steps: 63 | Train Loss: 0.3995878 Vali Loss: 0.9199510 Test Loss: 0.4135342
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.9092624187469482
Epoch: 8, Steps: 63 | Train Loss: 0.3994472 Vali Loss: 0.9193066 Test Loss: 0.4137889
Validation loss decreased (0.919540 --> 0.919307).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.3575432300567627
Epoch: 9, Steps: 63 | Train Loss: 0.3990571 Vali Loss: 0.9190617 Test Loss: 0.4137334
Validation loss decreased (0.919307 --> 0.919062).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.2661499977111816
Epoch: 10, Steps: 63 | Train Loss: 0.3988765 Vali Loss: 0.9185836 Test Loss: 0.4134381
Validation loss decreased (0.919062 --> 0.918584).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.8973441123962402
Epoch: 11, Steps: 63 | Train Loss: 0.3991602 Vali Loss: 0.9188288 Test Loss: 0.4134435
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.622816324234009
Epoch: 12, Steps: 63 | Train Loss: 0.3990559 Vali Loss: 0.9190454 Test Loss: 0.4135137
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.1412229537963867
Epoch: 13, Steps: 63 | Train Loss: 0.3987663 Vali Loss: 0.9189614 Test Loss: 0.4136049
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.269470453262329
Epoch: 14, Steps: 63 | Train Loss: 0.3988370 Vali Loss: 0.9188834 Test Loss: 0.4134370
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.4971542358398438
Epoch: 15, Steps: 63 | Train Loss: 0.3987407 Vali Loss: 0.9193581 Test Loss: 0.4133194
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.6170918941497803
Epoch: 16, Steps: 63 | Train Loss: 0.3987364 Vali Loss: 0.9187355 Test Loss: 0.4134447
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.3664159774780273
Epoch: 17, Steps: 63 | Train Loss: 0.3986336 Vali Loss: 0.9181638 Test Loss: 0.4135008
Validation loss decreased (0.918584 --> 0.918164).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8237934112548828
Epoch: 18, Steps: 63 | Train Loss: 0.3985468 Vali Loss: 0.9184620 Test Loss: 0.4135357
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9518609046936035
Epoch: 19, Steps: 63 | Train Loss: 0.3984373 Vali Loss: 0.9181110 Test Loss: 0.4133753
Validation loss decreased (0.918164 --> 0.918111).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.427086114883423
Epoch: 20, Steps: 63 | Train Loss: 0.3982810 Vali Loss: 0.9187264 Test Loss: 0.4133053
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.7569565773010254
Epoch: 21, Steps: 63 | Train Loss: 0.3985055 Vali Loss: 0.9179521 Test Loss: 0.4133266
Validation loss decreased (0.918111 --> 0.917952).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.8228673934936523
Epoch: 22, Steps: 63 | Train Loss: 0.3980734 Vali Loss: 0.9178501 Test Loss: 0.4132660
Validation loss decreased (0.917952 --> 0.917850).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.941457986831665
Epoch: 23, Steps: 63 | Train Loss: 0.3982307 Vali Loss: 0.9183925 Test Loss: 0.4133518
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.150991439819336
Epoch: 24, Steps: 63 | Train Loss: 0.3984904 Vali Loss: 0.9182162 Test Loss: 0.4134785
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.6673572063446045
Epoch: 25, Steps: 63 | Train Loss: 0.3982513 Vali Loss: 0.9185349 Test Loss: 0.4134903
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.0486555099487305
Epoch: 26, Steps: 63 | Train Loss: 0.3982593 Vali Loss: 0.9178954 Test Loss: 0.4133860
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.4714581966400146
Epoch: 27, Steps: 63 | Train Loss: 0.3977759 Vali Loss: 0.9182284 Test Loss: 0.4133531
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.021138906478882
Epoch: 28, Steps: 63 | Train Loss: 0.3980824 Vali Loss: 0.9180341 Test Loss: 0.4133925
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.1384873390197754
Epoch: 29, Steps: 63 | Train Loss: 0.3981949 Vali Loss: 0.9181956 Test Loss: 0.4134467
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.4147520065307617
Epoch: 30, Steps: 63 | Train Loss: 0.3982910 Vali Loss: 0.9180368 Test Loss: 0.4134057
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.8396403789520264
Epoch: 31, Steps: 63 | Train Loss: 0.3981996 Vali Loss: 0.9180498 Test Loss: 0.4134179
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.402862787246704
Epoch: 32, Steps: 63 | Train Loss: 0.3979896 Vali Loss: 0.9180349 Test Loss: 0.4133955
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.924299955368042
Epoch: 33, Steps: 63 | Train Loss: 0.3981875 Vali Loss: 0.9183111 Test Loss: 0.4133697
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.415174245834351
Epoch: 34, Steps: 63 | Train Loss: 0.3979584 Vali Loss: 0.9182256 Test Loss: 0.4134023
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.4810216426849365
Epoch: 35, Steps: 63 | Train Loss: 0.3981839 Vali Loss: 0.9183261 Test Loss: 0.4133745
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.874430179595947
Epoch: 36, Steps: 63 | Train Loss: 0.3982609 Vali Loss: 0.9175285 Test Loss: 0.4134931
Validation loss decreased (0.917850 --> 0.917529).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.9330625534057617
Epoch: 37, Steps: 63 | Train Loss: 0.3980432 Vali Loss: 0.9179643 Test Loss: 0.4134089
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.231830596923828
Epoch: 38, Steps: 63 | Train Loss: 0.3979860 Vali Loss: 0.9177775 Test Loss: 0.4133982
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.3054122924804688
Epoch: 39, Steps: 63 | Train Loss: 0.3981818 Vali Loss: 0.9180211 Test Loss: 0.4134646
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.9561114311218262
Epoch: 40, Steps: 63 | Train Loss: 0.3980197 Vali Loss: 0.9180270 Test Loss: 0.4134513
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.8264784812927246
Epoch: 41, Steps: 63 | Train Loss: 0.3982119 Vali Loss: 0.9178897 Test Loss: 0.4134490
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.7440006732940674
Epoch: 42, Steps: 63 | Train Loss: 0.3979906 Vali Loss: 0.9181752 Test Loss: 0.4134515
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.2214999198913574
Epoch: 43, Steps: 63 | Train Loss: 0.3979634 Vali Loss: 0.9182614 Test Loss: 0.4134517
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.33748722076416
Epoch: 44, Steps: 63 | Train Loss: 0.3977630 Vali Loss: 0.9181367 Test Loss: 0.4134563
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.342021942138672
Epoch: 45, Steps: 63 | Train Loss: 0.3980313 Vali Loss: 0.9179050 Test Loss: 0.4134346
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.578115940093994
Epoch: 46, Steps: 63 | Train Loss: 0.3980937 Vali Loss: 0.9180478 Test Loss: 0.4134253
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.599210023880005
Epoch: 47, Steps: 63 | Train Loss: 0.3976378 Vali Loss: 0.9181092 Test Loss: 0.4134124
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.423616886138916
Epoch: 48, Steps: 63 | Train Loss: 0.3977129 Vali Loss: 0.9179443 Test Loss: 0.4134221
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.323028802871704
Epoch: 49, Steps: 63 | Train Loss: 0.3978410 Vali Loss: 0.9182062 Test Loss: 0.4134655
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.8561341762542725
Epoch: 50, Steps: 63 | Train Loss: 0.3978392 Vali Loss: 0.9180839 Test Loss: 0.4134496
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.1923763751983643
Epoch: 51, Steps: 63 | Train Loss: 0.3975912 Vali Loss: 0.9179443 Test Loss: 0.4134540
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.352543592453003
Epoch: 52, Steps: 63 | Train Loss: 0.3980365 Vali Loss: 0.9180608 Test Loss: 0.4134672
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.072793960571289
Epoch: 53, Steps: 63 | Train Loss: 0.3980071 Vali Loss: 0.9180076 Test Loss: 0.4134405
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.912785530090332
Epoch: 54, Steps: 63 | Train Loss: 0.3978244 Vali Loss: 0.9178571 Test Loss: 0.4133905
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.0338053703308105
Epoch: 55, Steps: 63 | Train Loss: 0.3976840 Vali Loss: 0.9181369 Test Loss: 0.4134291
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.0810720920562744
Epoch: 56, Steps: 63 | Train Loss: 0.3980029 Vali Loss: 0.9180077 Test Loss: 0.4134502
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.409368097782135, mae:0.4174181818962097, rse:0.6075952053070068, corr:[0.26184517 0.26961678 0.26972246 0.26786685 0.2660245  0.26433507
 0.2626699  0.26178196 0.2615043  0.261796   0.26201358 0.26164606
 0.26113966 0.26098377 0.2611417  0.26124924 0.2610135  0.2606853
 0.2604953  0.26026607 0.26003504 0.25995827 0.2601308  0.26050636
 0.26049006 0.26014376 0.25982043 0.25963193 0.25933632 0.25887266
 0.25831646 0.25786754 0.2577074  0.25773305 0.25781202 0.25781232
 0.25780603 0.25793272 0.25811645 0.25827157 0.25854874 0.2587584
 0.25882044 0.25867093 0.25846317 0.25830075 0.25838977 0.25847638
 0.25804308 0.2571927  0.25618926 0.2554005  0.254717   0.2538527
 0.2531905  0.25281855 0.25259936 0.25248128 0.2522779  0.25222537
 0.25224686 0.2521935  0.25201634 0.25194523 0.25207865 0.25238785
 0.2526624  0.25254256 0.25240707 0.2526149  0.25294697 0.25280413
 0.25196224 0.25084054 0.25004774 0.24976087 0.24966495 0.24934554
 0.24888231 0.24843505 0.24820997 0.24808027 0.247837   0.24755208
 0.24749118 0.24761537 0.24769282 0.24758273 0.24746862 0.24752237
 0.24752176 0.24725859 0.24696428 0.24695255 0.24736659 0.24803951
 0.2486035  0.24857478 0.2484468  0.24837823 0.24831586 0.24813908
 0.2478511  0.2476864  0.24767216 0.24766965 0.24752285 0.24720258
 0.24687982 0.24682817 0.24704006 0.24739942 0.24763094 0.24781352
 0.24792355 0.24778597 0.24742092 0.24705867 0.24694575 0.24709272
 0.2471666  0.24670774 0.2458176  0.24493869 0.2443658  0.24397032
 0.24381045 0.24379581 0.24369363 0.24332671 0.2429038  0.2425831
 0.24236128 0.24231817 0.24241109 0.24240917 0.24259691 0.24291696
 0.24324648 0.24330139 0.24312656 0.2429582  0.2429149  0.24289648
 0.24280609 0.24226625 0.24153501 0.24088821 0.24049361 0.24001066
 0.23957486 0.23944972 0.23955478 0.23978077 0.23976293 0.23953353
 0.23926207 0.23930554 0.23942529 0.23932233 0.23908462 0.23920593
 0.23958735 0.23964429 0.23935512 0.23902676 0.2390347  0.23929666
 0.2395617  0.23959292 0.23952879 0.23956054 0.23960109 0.23937134
 0.23891419 0.23867092 0.23881917 0.23920652 0.23923536 0.2387929
 0.23855951 0.23875102 0.2388205  0.23831467 0.23787236 0.23863316
 0.23976825 0.23937243 0.23788007 0.2379978  0.23942809 0.23449582]
