Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=0, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=106, out_features=204, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19375104.0
params:  21828.0
Trainable parameters:  21828
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.704310655593872
Epoch: 1, Steps: 62 | Train Loss: 0.7588316 Vali Loss: 1.5237540 Test Loss: 0.6762496
Validation loss decreased (inf --> 1.523754).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.7560522556304932
Epoch: 2, Steps: 62 | Train Loss: 0.5896510 Vali Loss: 1.3732725 Test Loss: 0.5741923
Validation loss decreased (1.523754 --> 1.373273).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8317971229553223
Epoch: 3, Steps: 62 | Train Loss: 0.5406132 Vali Loss: 1.3120525 Test Loss: 0.5312616
Validation loss decreased (1.373273 --> 1.312052).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6426219940185547
Epoch: 4, Steps: 62 | Train Loss: 0.5153852 Vali Loss: 1.2684927 Test Loss: 0.5032984
Validation loss decreased (1.312052 --> 1.268493).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.6279394626617432
Epoch: 5, Steps: 62 | Train Loss: 0.4989326 Vali Loss: 1.2508082 Test Loss: 0.4831390
Validation loss decreased (1.268493 --> 1.250808).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.65097975730896
Epoch: 6, Steps: 62 | Train Loss: 0.4869582 Vali Loss: 1.2288173 Test Loss: 0.4688320
Validation loss decreased (1.250808 --> 1.228817).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5788309574127197
Epoch: 7, Steps: 62 | Train Loss: 0.4784598 Vali Loss: 1.2109442 Test Loss: 0.4580894
Validation loss decreased (1.228817 --> 1.210944).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5531184673309326
Epoch: 8, Steps: 62 | Train Loss: 0.4723859 Vali Loss: 1.2006029 Test Loss: 0.4501892
Validation loss decreased (1.210944 --> 1.200603).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.6329014301300049
Epoch: 9, Steps: 62 | Train Loss: 0.4678142 Vali Loss: 1.1906389 Test Loss: 0.4443364
Validation loss decreased (1.200603 --> 1.190639).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.5631473064422607
Epoch: 10, Steps: 62 | Train Loss: 0.4643248 Vali Loss: 1.1896063 Test Loss: 0.4403301
Validation loss decreased (1.190639 --> 1.189606).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.6373286247253418
Epoch: 11, Steps: 62 | Train Loss: 0.4618791 Vali Loss: 1.1752144 Test Loss: 0.4372052
Validation loss decreased (1.189606 --> 1.175214).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.5424771308898926
Epoch: 12, Steps: 62 | Train Loss: 0.4600538 Vali Loss: 1.1767393 Test Loss: 0.4349680
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.574714183807373
Epoch: 13, Steps: 62 | Train Loss: 0.4586742 Vali Loss: 1.1708810 Test Loss: 0.4333648
Validation loss decreased (1.175214 --> 1.170881).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5541276931762695
Epoch: 14, Steps: 62 | Train Loss: 0.4574900 Vali Loss: 1.1730088 Test Loss: 0.4322019
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5367932319641113
Epoch: 15, Steps: 62 | Train Loss: 0.4566568 Vali Loss: 1.1724056 Test Loss: 0.4314725
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.57763671875
Epoch: 16, Steps: 62 | Train Loss: 0.4560212 Vali Loss: 1.1721283 Test Loss: 0.4306507
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4643487930297852
Epoch: 17, Steps: 62 | Train Loss: 0.4555559 Vali Loss: 1.1690942 Test Loss: 0.4302959
Validation loss decreased (1.170881 --> 1.169094).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6400630474090576
Epoch: 18, Steps: 62 | Train Loss: 0.4550234 Vali Loss: 1.1694334 Test Loss: 0.4299019
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5894091129302979
Epoch: 19, Steps: 62 | Train Loss: 0.4547954 Vali Loss: 1.1688255 Test Loss: 0.4295512
Validation loss decreased (1.169094 --> 1.168826).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.579582691192627
Epoch: 20, Steps: 62 | Train Loss: 0.4543658 Vali Loss: 1.1667144 Test Loss: 0.4293432
Validation loss decreased (1.168826 --> 1.166714).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0117220878601074
Epoch: 21, Steps: 62 | Train Loss: 0.4541856 Vali Loss: 1.1624823 Test Loss: 0.4292800
Validation loss decreased (1.166714 --> 1.162482).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5850563049316406
Epoch: 22, Steps: 62 | Train Loss: 0.4540885 Vali Loss: 1.1624480 Test Loss: 0.4291276
Validation loss decreased (1.162482 --> 1.162448).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5955455303192139
Epoch: 23, Steps: 62 | Train Loss: 0.4538337 Vali Loss: 1.1657636 Test Loss: 0.4290545
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6414053440093994
Epoch: 24, Steps: 62 | Train Loss: 0.4536734 Vali Loss: 1.1647986 Test Loss: 0.4290277
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.540823221206665
Epoch: 25, Steps: 62 | Train Loss: 0.4535914 Vali Loss: 1.1576784 Test Loss: 0.4288541
Validation loss decreased (1.162448 --> 1.157678).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5486531257629395
Epoch: 26, Steps: 62 | Train Loss: 0.4533866 Vali Loss: 1.1619773 Test Loss: 0.4288091
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5332489013671875
Epoch: 27, Steps: 62 | Train Loss: 0.4534321 Vali Loss: 1.1615999 Test Loss: 0.4288088
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.574066400527954
Epoch: 28, Steps: 62 | Train Loss: 0.4531181 Vali Loss: 1.1585861 Test Loss: 0.4287576
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7066259384155273
Epoch: 29, Steps: 62 | Train Loss: 0.4530732 Vali Loss: 1.1599635 Test Loss: 0.4286957
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.6394195556640625
Epoch: 30, Steps: 62 | Train Loss: 0.4531891 Vali Loss: 1.1614937 Test Loss: 0.4287023
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5712151527404785
Epoch: 31, Steps: 62 | Train Loss: 0.4529866 Vali Loss: 1.1601874 Test Loss: 0.4287626
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.8700368404388428
Epoch: 32, Steps: 62 | Train Loss: 0.4527376 Vali Loss: 1.1624267 Test Loss: 0.4286936
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.549285888671875
Epoch: 33, Steps: 62 | Train Loss: 0.4527231 Vali Loss: 1.1625465 Test Loss: 0.4287201
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.500856876373291
Epoch: 34, Steps: 62 | Train Loss: 0.4528100 Vali Loss: 1.1666008 Test Loss: 0.4287186
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.502302646636963
Epoch: 35, Steps: 62 | Train Loss: 0.4528671 Vali Loss: 1.1601774 Test Loss: 0.4286577
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.5264098644256592
Epoch: 36, Steps: 62 | Train Loss: 0.4526564 Vali Loss: 1.1609486 Test Loss: 0.4286312
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6384525299072266
Epoch: 37, Steps: 62 | Train Loss: 0.4525579 Vali Loss: 1.1602668 Test Loss: 0.4286471
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.705026626586914
Epoch: 38, Steps: 62 | Train Loss: 0.4526159 Vali Loss: 1.1616589 Test Loss: 0.4286323
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.96933913230896
Epoch: 39, Steps: 62 | Train Loss: 0.4525818 Vali Loss: 1.1629032 Test Loss: 0.4286195
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7816364765167236
Epoch: 40, Steps: 62 | Train Loss: 0.4526832 Vali Loss: 1.1610636 Test Loss: 0.4286512
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.582190752029419
Epoch: 41, Steps: 62 | Train Loss: 0.4523056 Vali Loss: 1.1609783 Test Loss: 0.4286238
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.6671574115753174
Epoch: 42, Steps: 62 | Train Loss: 0.4523802 Vali Loss: 1.1636487 Test Loss: 0.4286297
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.5865280628204346
Epoch: 43, Steps: 62 | Train Loss: 0.4524216 Vali Loss: 1.1627522 Test Loss: 0.4286045
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.8773622512817383
Epoch: 44, Steps: 62 | Train Loss: 0.4523405 Vali Loss: 1.1599571 Test Loss: 0.4285908
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5187091827392578
Epoch: 45, Steps: 62 | Train Loss: 0.4522054 Vali Loss: 1.1582894 Test Loss: 0.4286003
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.42766568064689636, mae:0.42762482166290283, rse:0.6225929856300354, corr:[0.25007746 0.2584464  0.25651482 0.259889   0.2568646  0.2538395
 0.25507048 0.25555617 0.25401428 0.25389677 0.25457802 0.2539123
 0.25296995 0.25267613 0.25241965 0.25224072 0.2521371  0.25166878
 0.2515095  0.25188452 0.2519598  0.25160196 0.2521191  0.252768
 0.25225788 0.2518488  0.25203857 0.2515909  0.25092053 0.25107867
 0.2511783  0.2503557  0.24977447 0.2501805  0.25032175 0.24981153
 0.24970976 0.25006884 0.250205   0.25020248 0.25027874 0.25024122
 0.25025648 0.2504207  0.2503056  0.24998912 0.25000188 0.25014544
 0.2497683  0.2490154  0.24823865 0.24762158 0.24720915 0.24672092
 0.24614343 0.24554494 0.24520259 0.24507397 0.24483143 0.24457006
 0.24426435 0.24418262 0.244233   0.24429119 0.2442723  0.24444659
 0.24490564 0.24495374 0.24475159 0.24482958 0.24489214 0.24439494
 0.24371006 0.24331921 0.2430486  0.24272871 0.2425805  0.24252899
 0.24211065 0.24151105 0.24128178 0.24115948 0.24082157 0.24054125
 0.24038737 0.2402062  0.24007295 0.24011675 0.24009164 0.23994568
 0.23976205 0.23951289 0.23918074 0.23908724 0.23925516 0.23973587
 0.24031885 0.24071524 0.24097686 0.24112377 0.24135083 0.24142043
 0.2411301  0.24092501 0.24092886 0.24085005 0.24055287 0.24029122
 0.24020442 0.24028142 0.24046145 0.24060306 0.24062432 0.24063991
 0.24065687 0.24045512 0.24018875 0.23999947 0.23958513 0.23916578
 0.2390872  0.23889525 0.23830022 0.23770806 0.2375532  0.23738863
 0.23710673 0.23698378 0.23680589 0.23649189 0.23620987 0.2360162
 0.23591106 0.23599811 0.23616529 0.23617452 0.23614888 0.23634543
 0.23670503 0.2367538  0.23646915 0.2361922  0.23591587 0.23591283
 0.23603839 0.23574734 0.23529196 0.2348883  0.23448215 0.23395443
 0.23373808 0.23385708 0.23392822 0.2339596  0.23397243 0.23399791
 0.23403879 0.23421243 0.23426203 0.23401408 0.23376186 0.23379648
 0.23370486 0.23349655 0.23343368 0.23317498 0.23243488 0.2322546
 0.23275228 0.23298019 0.23321488 0.23388559 0.2341121  0.23369564
 0.23370631 0.23421495 0.23426165 0.23406056 0.23415662 0.23415433
 0.23378721 0.23371905 0.23398747 0.2338606  0.23373297 0.23428863
 0.2346219  0.23429173 0.23412402 0.23404124 0.2334755  0.23304068
 0.23299974 0.23267637 0.23199554 0.23172739 0.23150094 0.23078725
 0.23035814 0.23067088 0.23078614 0.23054627 0.23042165 0.23034576
 0.23017983 0.23051324 0.23094532 0.23074934 0.23060022 0.23101911
 0.23117492 0.23056997 0.23014651 0.22992007 0.22928715 0.22919221
 0.22974895 0.22951818 0.2290718  0.22916599 0.22937839 0.2291535
 0.22897986 0.22922209 0.22926156 0.22891818 0.22870168 0.22845016
 0.2278441  0.22776982 0.22824433 0.22809316 0.22772823 0.22792587
 0.2280963  0.22772063 0.22754557 0.22763209 0.22752818 0.22749332
 0.22777104 0.22741625 0.2270485  0.227402   0.22764853 0.22742745
 0.22760348 0.22819805 0.22827987 0.22794724 0.22784148 0.22754563
 0.22705565 0.22722535 0.22779676 0.22776851 0.22779693 0.22828268
 0.2283132  0.22789475 0.22809236 0.22840358 0.22822103 0.22836879
 0.22861283 0.22793029 0.22709379 0.22711892 0.22698854 0.22639254
 0.22614837 0.2260481  0.2255778  0.22536172 0.22550781 0.22508515
 0.22459115 0.2249506  0.2252557  0.22473417 0.22451049 0.22496516
 0.22507076 0.2247076  0.22497514 0.22533481 0.22524796 0.22561239
 0.22632459 0.22622545 0.22613867 0.22654577 0.22658813 0.22632116
 0.22656046 0.22697064 0.22684312 0.22671948 0.22659646 0.22615296
 0.22593027 0.2262905  0.22644247 0.22597302 0.22638921 0.22686681
 0.22635037 0.22596505 0.22656399 0.22656217 0.22603041 0.22639835
 0.22674696 0.22628443 0.22635624 0.22692874 0.22607923 0.22495474
 0.22500993 0.22488922 0.22415558 0.22448128 0.22423202 0.22280246
 0.22287533 0.2238738  0.22272432 0.22195211 0.22362399 0.22336613
 0.22092617 0.22214061 0.22274229 0.21677534 0.21928741 0.22161926]
