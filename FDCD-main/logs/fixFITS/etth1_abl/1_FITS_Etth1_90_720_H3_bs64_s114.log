Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=22, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3902976.0
params:  4554.0
Trainable parameters:  4554
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7256412506103516
Epoch: 1, Steps: 61 | Train Loss: 1.5082247 Vali Loss: 2.8215299 Test Loss: 1.4468517
Validation loss decreased (inf --> 2.821530).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5331699848175049
Epoch: 2, Steps: 61 | Train Loss: 1.1246690 Vali Loss: 2.2696567 Test Loss: 1.0135132
Validation loss decreased (2.821530 --> 2.269657).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.4925282001495361
Epoch: 3, Steps: 61 | Train Loss: 0.9228916 Vali Loss: 2.0023494 Test Loss: 0.7969196
Validation loss decreased (2.269657 --> 2.002349).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6229636669158936
Epoch: 4, Steps: 61 | Train Loss: 0.8106555 Vali Loss: 1.8505801 Test Loss: 0.6762601
Validation loss decreased (2.002349 --> 1.850580).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5528171062469482
Epoch: 5, Steps: 61 | Train Loss: 0.7435939 Vali Loss: 1.7648597 Test Loss: 0.6055143
Validation loss decreased (1.850580 --> 1.764860).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.5704576969146729
Epoch: 6, Steps: 61 | Train Loss: 0.7021549 Vali Loss: 1.7034500 Test Loss: 0.5615619
Validation loss decreased (1.764860 --> 1.703450).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5019841194152832
Epoch: 7, Steps: 61 | Train Loss: 0.6756830 Vali Loss: 1.6673853 Test Loss: 0.5336515
Validation loss decreased (1.703450 --> 1.667385).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5465657711029053
Epoch: 8, Steps: 61 | Train Loss: 0.6582229 Vali Loss: 1.6393770 Test Loss: 0.5153003
Validation loss decreased (1.667385 --> 1.639377).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.4854934215545654
Epoch: 9, Steps: 61 | Train Loss: 0.6463824 Vali Loss: 1.6281123 Test Loss: 0.5028808
Validation loss decreased (1.639377 --> 1.628112).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.383091688156128
Epoch: 10, Steps: 61 | Train Loss: 0.6381849 Vali Loss: 1.6174126 Test Loss: 0.4944789
Validation loss decreased (1.628112 --> 1.617413).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.400507926940918
Epoch: 11, Steps: 61 | Train Loss: 0.6326856 Vali Loss: 1.6078293 Test Loss: 0.4885051
Validation loss decreased (1.617413 --> 1.607829).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4704434871673584
Epoch: 12, Steps: 61 | Train Loss: 0.6278089 Vali Loss: 1.5989823 Test Loss: 0.4843186
Validation loss decreased (1.607829 --> 1.598982).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.517493724822998
Epoch: 13, Steps: 61 | Train Loss: 0.6252271 Vali Loss: 1.6011845 Test Loss: 0.4813169
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.4933159351348877
Epoch: 14, Steps: 61 | Train Loss: 0.6229921 Vali Loss: 1.5954125 Test Loss: 0.4790692
Validation loss decreased (1.598982 --> 1.595412).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4987797737121582
Epoch: 15, Steps: 61 | Train Loss: 0.6209413 Vali Loss: 1.5913029 Test Loss: 0.4774217
Validation loss decreased (1.595412 --> 1.591303).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4664947986602783
Epoch: 16, Steps: 61 | Train Loss: 0.6196076 Vali Loss: 1.5886980 Test Loss: 0.4761637
Validation loss decreased (1.591303 --> 1.588698).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4113974571228027
Epoch: 17, Steps: 61 | Train Loss: 0.6186091 Vali Loss: 1.5816953 Test Loss: 0.4752394
Validation loss decreased (1.588698 --> 1.581695).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.3602089881896973
Epoch: 18, Steps: 61 | Train Loss: 0.6179770 Vali Loss: 1.5895880 Test Loss: 0.4745084
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.524552822113037
Epoch: 19, Steps: 61 | Train Loss: 0.6167513 Vali Loss: 1.5804358 Test Loss: 0.4739557
Validation loss decreased (1.581695 --> 1.580436).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.4853017330169678
Epoch: 20, Steps: 61 | Train Loss: 0.6164642 Vali Loss: 1.5839356 Test Loss: 0.4734846
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4442384243011475
Epoch: 21, Steps: 61 | Train Loss: 0.6159102 Vali Loss: 1.5849926 Test Loss: 0.4731258
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.4072816371917725
Epoch: 22, Steps: 61 | Train Loss: 0.6153078 Vali Loss: 1.5788529 Test Loss: 0.4727950
Validation loss decreased (1.580436 --> 1.578853).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5437893867492676
Epoch: 23, Steps: 61 | Train Loss: 0.6146743 Vali Loss: 1.5854499 Test Loss: 0.4725890
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8640272617340088
Epoch: 24, Steps: 61 | Train Loss: 0.6147713 Vali Loss: 1.5835114 Test Loss: 0.4723926
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.5923404693603516
Epoch: 25, Steps: 61 | Train Loss: 0.6145399 Vali Loss: 1.5855854 Test Loss: 0.4722485
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 5.8671557903289795
Epoch: 26, Steps: 61 | Train Loss: 0.6141613 Vali Loss: 1.5809895 Test Loss: 0.4721102
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8515987396240234
Epoch: 27, Steps: 61 | Train Loss: 0.6137283 Vali Loss: 1.5774646 Test Loss: 0.4720516
Validation loss decreased (1.578853 --> 1.577465).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.0821659564971924
Epoch: 28, Steps: 61 | Train Loss: 0.6140546 Vali Loss: 1.5765917 Test Loss: 0.4719467
Validation loss decreased (1.577465 --> 1.576592).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 5.480759143829346
Epoch: 29, Steps: 61 | Train Loss: 0.6137907 Vali Loss: 1.5797522 Test Loss: 0.4718679
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.207167863845825
Epoch: 30, Steps: 61 | Train Loss: 0.6135115 Vali Loss: 1.5773158 Test Loss: 0.4718262
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.3483541011810303
Epoch: 31, Steps: 61 | Train Loss: 0.6132435 Vali Loss: 1.5770469 Test Loss: 0.4717902
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.4535276889801025
Epoch: 32, Steps: 61 | Train Loss: 0.6129630 Vali Loss: 1.5788873 Test Loss: 0.4717475
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.1222970485687256
Epoch: 33, Steps: 61 | Train Loss: 0.6132409 Vali Loss: 1.5812731 Test Loss: 0.4717143
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.1813390254974365
Epoch: 34, Steps: 61 | Train Loss: 0.6131351 Vali Loss: 1.5777850 Test Loss: 0.4716658
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.140209913253784
Epoch: 35, Steps: 61 | Train Loss: 0.6127707 Vali Loss: 1.5799127 Test Loss: 0.4716496
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 5.327211856842041
Epoch: 36, Steps: 61 | Train Loss: 0.6128319 Vali Loss: 1.5700450 Test Loss: 0.4716453
Validation loss decreased (1.576592 --> 1.570045).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.4867913722991943
Epoch: 37, Steps: 61 | Train Loss: 0.6126858 Vali Loss: 1.5734007 Test Loss: 0.4716154
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.7632575035095215
Epoch: 38, Steps: 61 | Train Loss: 0.6126035 Vali Loss: 1.5812179 Test Loss: 0.4716309
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 5.473700284957886
Epoch: 39, Steps: 61 | Train Loss: 0.6121905 Vali Loss: 1.5703402 Test Loss: 0.4716234
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.5050995349884033
Epoch: 40, Steps: 61 | Train Loss: 0.6122946 Vali Loss: 1.5801715 Test Loss: 0.4716207
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.2421576976776123
Epoch: 41, Steps: 61 | Train Loss: 0.6122881 Vali Loss: 1.5748749 Test Loss: 0.4715930
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.145512819290161
Epoch: 42, Steps: 61 | Train Loss: 0.6125641 Vali Loss: 1.5817268 Test Loss: 0.4715780
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.911391258239746
Epoch: 43, Steps: 61 | Train Loss: 0.6119460 Vali Loss: 1.5722746 Test Loss: 0.4715855
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.011904716491699
Epoch: 44, Steps: 61 | Train Loss: 0.6119973 Vali Loss: 1.5762138 Test Loss: 0.4715873
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 5.067356824874878
Epoch: 45, Steps: 61 | Train Loss: 0.6119049 Vali Loss: 1.5748200 Test Loss: 0.4715815
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.2225406169891357
Epoch: 46, Steps: 61 | Train Loss: 0.6118423 Vali Loss: 1.5628440 Test Loss: 0.4715858
Validation loss decreased (1.570045 --> 1.562844).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.04610276222229
Epoch: 47, Steps: 61 | Train Loss: 0.6118600 Vali Loss: 1.5750756 Test Loss: 0.4715821
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.580049514770508
Epoch: 48, Steps: 61 | Train Loss: 0.6122374 Vali Loss: 1.5738176 Test Loss: 0.4715941
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 5.457116365432739
Epoch: 49, Steps: 61 | Train Loss: 0.6119439 Vali Loss: 1.5785470 Test Loss: 0.4715940
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.4925484657287598
Epoch: 50, Steps: 61 | Train Loss: 0.6119512 Vali Loss: 1.5741048 Test Loss: 0.4715954
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.5187759399414062
Epoch: 51, Steps: 61 | Train Loss: 0.6115419 Vali Loss: 1.5704395 Test Loss: 0.4716022
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.4853506088256836
Epoch: 52, Steps: 61 | Train Loss: 0.6115848 Vali Loss: 1.5733054 Test Loss: 0.4715959
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.4530706405639648
Epoch: 53, Steps: 61 | Train Loss: 0.6118377 Vali Loss: 1.5765007 Test Loss: 0.4716199
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.2764532566070557
Epoch: 54, Steps: 61 | Train Loss: 0.6116966 Vali Loss: 1.5661311 Test Loss: 0.4716190
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.5308923721313477
Epoch: 55, Steps: 61 | Train Loss: 0.6118094 Vali Loss: 1.5724361 Test Loss: 0.4716073
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.3826117515563965
Epoch: 56, Steps: 61 | Train Loss: 0.6119851 Vali Loss: 1.5702221 Test Loss: 0.4716135
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.5153429508209229
Epoch: 57, Steps: 61 | Train Loss: 0.6118367 Vali Loss: 1.5732604 Test Loss: 0.4716199
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.509429931640625
Epoch: 58, Steps: 61 | Train Loss: 0.6117890 Vali Loss: 1.5666640 Test Loss: 0.4716265
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.4674913883209229
Epoch: 59, Steps: 61 | Train Loss: 0.6115456 Vali Loss: 1.5750465 Test Loss: 0.4716312
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.4619979858398438
Epoch: 60, Steps: 61 | Train Loss: 0.6114386 Vali Loss: 1.5781178 Test Loss: 0.4716367
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.545046091079712
Epoch: 61, Steps: 61 | Train Loss: 0.6115135 Vali Loss: 1.5734791 Test Loss: 0.4716379
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.4587514400482178
Epoch: 62, Steps: 61 | Train Loss: 0.6114481 Vali Loss: 1.5721308 Test Loss: 0.4716466
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.4842770099639893
Epoch: 63, Steps: 61 | Train Loss: 0.6116936 Vali Loss: 1.5695276 Test Loss: 0.4716552
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.3292763233184814
Epoch: 64, Steps: 61 | Train Loss: 0.6112989 Vali Loss: 1.5772305 Test Loss: 0.4716590
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.4363105297088623
Epoch: 65, Steps: 61 | Train Loss: 0.6114982 Vali Loss: 1.5684662 Test Loss: 0.4716580
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.480393648147583
Epoch: 66, Steps: 61 | Train Loss: 0.6117488 Vali Loss: 1.5673062 Test Loss: 0.4716599
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4703086018562317, mae:0.4626242220401764, rse:0.6565131545066833, corr:[0.22175124 0.23115571 0.23044893 0.22729783 0.22552934 0.22451028
 0.22411764 0.22357568 0.22330633 0.22374265 0.22351573 0.22276174
 0.22194469 0.22179264 0.22235717 0.22259092 0.22239359 0.22243942
 0.22266759 0.22297622 0.22300395 0.22274852 0.22203965 0.22109462
 0.21962686 0.21888395 0.21864925 0.21848476 0.21826564 0.21857488
 0.21925299 0.2195224  0.21948993 0.21939032 0.2192288  0.21904768
 0.21876036 0.2185599  0.21892801 0.21924326 0.21942475 0.21960165
 0.2199538  0.22016214 0.22026376 0.22022933 0.2199714  0.21929306
 0.21788529 0.21702626 0.21642698 0.21559381 0.21456864 0.21405216
 0.21449044 0.21440691 0.21408598 0.21415119 0.21405591 0.21373852
 0.21322885 0.21294819 0.2131018  0.21320282 0.21333644 0.21353921
 0.21382213 0.21393935 0.21381405 0.21366428 0.21311544 0.21190822
 0.20998536 0.20909066 0.20875973 0.20865001 0.2084531  0.20866394
 0.20959456 0.20986491 0.2096493  0.2093669  0.20904094 0.20879972
 0.20865077 0.2085858  0.20869589 0.2086617  0.20858453 0.20871584
 0.20884646 0.2088868  0.20891859 0.20921434 0.20945981 0.2090257
 0.20773469 0.20741545 0.2079479  0.20819004 0.20827702 0.20861085
 0.20944966 0.20982303 0.20970613 0.2094845  0.20917661 0.20885095
 0.2087028  0.20858717 0.2086938  0.2085947  0.20863658 0.20871341
 0.20892856 0.2090061  0.20900665 0.2090022  0.2087387  0.20779946
 0.20600247 0.2048558  0.20421918 0.2036491  0.20331971 0.2035825
 0.20482707 0.20550625 0.2054817  0.2052306  0.20495486 0.20486027
 0.20483798 0.20472676 0.20468947 0.20455432 0.20471327 0.20501007
 0.20522323 0.20530568 0.20529789 0.20549472 0.2055929  0.20507398
 0.20356648 0.20250055 0.20218188 0.20161074 0.20070963 0.20048939
 0.20144531 0.20195068 0.20206816 0.2020169  0.20175977 0.20153628
 0.20138854 0.20119981 0.20100352 0.20083435 0.20095181 0.20117904
 0.20135798 0.20147249 0.20159417 0.20164901 0.20133321 0.2005179
 0.19935848 0.1991374  0.19955747 0.19959158 0.19874777 0.19827253
 0.19907306 0.1994968  0.19953752 0.19947068 0.19931398 0.19905196
 0.19886923 0.19881661 0.19883654 0.19856817 0.19846238 0.19889526
 0.19945692 0.1997275  0.19983795 0.19986297 0.19970323 0.19901249
 0.19734873 0.19636178 0.19578958 0.19512267 0.1944284  0.19451676
 0.19526835 0.19567782 0.19573095 0.19555542 0.19524272 0.19501007
 0.19473107 0.19450189 0.19440822 0.1942463  0.19426906 0.19447573
 0.1946301  0.19462405 0.19456436 0.19464475 0.19451223 0.19373342
 0.19213426 0.19135186 0.19151208 0.19196132 0.19232008 0.1932013
 0.194827   0.19585916 0.19633833 0.19636504 0.1960194  0.19576448
 0.19563028 0.19543222 0.19540988 0.19539896 0.1952451  0.19507928
 0.19508621 0.19517054 0.19527547 0.1953837  0.19523515 0.19443487
 0.19298024 0.19217046 0.19199018 0.19190182 0.1918099  0.19223672
 0.19357467 0.19435385 0.1946642  0.19459148 0.1942346  0.19398774
 0.19383907 0.19361563 0.19349295 0.19330598 0.19320637 0.19329527
 0.19351062 0.19350024 0.19335297 0.19329946 0.19314966 0.19247773
 0.19098097 0.19047196 0.19079237 0.19113058 0.19107313 0.1916673
 0.19316976 0.19422396 0.19460092 0.19449596 0.19416131 0.19387126
 0.19368368 0.19352767 0.19329664 0.19312677 0.19312102 0.19336168
 0.19369714 0.19402575 0.19436795 0.19466068 0.19478336 0.19450101
 0.1938269  0.19402295 0.1953233  0.19607946 0.19593431 0.19617765
 0.19734873 0.19800161 0.198176   0.19785434 0.19723722 0.19674952
 0.1965002  0.19636755 0.196235   0.19608507 0.19610429 0.1961374
 0.19616175 0.19614999 0.19613571 0.19622159 0.19603805 0.19522156
 0.19394945 0.19338357 0.19367158 0.19370967 0.19341373 0.19328518
 0.19426212 0.19483082 0.19489117 0.19463474 0.19414105 0.19371523
 0.19354025 0.19342515 0.19315322 0.19290297 0.19289036 0.19316377
 0.19334981 0.193203   0.19290645 0.1928784  0.1929135  0.19235423
 0.19078991 0.18972683 0.18928401 0.18927479 0.18931498 0.18980652
 0.19113345 0.19205225 0.19258493 0.1926469  0.1924234  0.19208294
 0.19180632 0.1918086  0.19198285 0.19194701 0.19168141 0.19162801
 0.19174276 0.19176912 0.19156247 0.19139333 0.1908681  0.1898348
 0.18834926 0.18756382 0.187321   0.18676576 0.1862774  0.18630493
 0.18758352 0.18827869 0.18857858 0.18857084 0.18837826 0.18810458
 0.187799   0.18749934 0.18735912 0.1872264  0.18722418 0.18725419
 0.18724477 0.18719204 0.18719573 0.18737467 0.18717569 0.18664415
 0.18582302 0.18601793 0.18667482 0.18691875 0.18693694 0.1873192
 0.18884176 0.18959713 0.18964304 0.18952432 0.18951538 0.18957748
 0.18947595 0.18941756 0.18962139 0.18982609 0.1900102  0.19026439
 0.19053838 0.19081914 0.19101405 0.1912395  0.19128925 0.19088806
 0.18983203 0.18968555 0.19017349 0.19018176 0.19003418 0.1906566
 0.19246648 0.19339456 0.1935968  0.19343063 0.19333711 0.19339626
 0.19339032 0.19332415 0.19309902 0.19296397 0.19306654 0.19327071
 0.19341396 0.19354434 0.19363254 0.19405116 0.19424891 0.19387928
 0.19277841 0.19232601 0.1927037  0.19313799 0.1933408  0.19414262
 0.19574575 0.19670667 0.19702196 0.19696331 0.19671522 0.19660905
 0.19659112 0.19636646 0.19611987 0.19603166 0.1961409  0.19627081
 0.19627497 0.19616488 0.19625622 0.19668226 0.1968525  0.19655919
 0.19577667 0.19604175 0.1968044  0.19709775 0.1967749  0.19676572
 0.19791895 0.19864248 0.19897045 0.19901162 0.19888414 0.19891475
 0.19918539 0.19948919 0.1995238  0.19940746 0.19960348 0.20000736
 0.20022939 0.20017181 0.2000482  0.19994405 0.19985107 0.19924793
 0.19783121 0.19711134 0.19697084 0.19700421 0.1970375  0.19752288
 0.199068   0.19966766 0.19968326 0.19941677 0.19905856 0.19891252
 0.19891393 0.19892985 0.19885655 0.1987054  0.19872342 0.19887722
 0.19897048 0.19890854 0.19893354 0.19934712 0.19991699 0.20006704
 0.1994119  0.19956774 0.20051381 0.2012193  0.20125546 0.20130403
 0.20220642 0.20245144 0.20239605 0.20214954 0.2019103  0.20189373
 0.2019727  0.20212084 0.20242228 0.20256276 0.20260622 0.20272724
 0.20300801 0.20332856 0.2036002  0.20403427 0.20447217 0.20426968
 0.20297885 0.20215702 0.20215951 0.20240633 0.20216186 0.20211047
 0.20292209 0.20342706 0.20351206 0.20341104 0.20324264 0.20319594
 0.20311567 0.20315367 0.20334184 0.20339437 0.20346685 0.20357116
 0.20367147 0.20372024 0.20363188 0.2035289  0.20325437 0.20246008
 0.2009483  0.20013234 0.20013341 0.19978257 0.19905634 0.1989343
 0.19986297 0.20014085 0.2000348  0.19978231 0.19963893 0.19965635
 0.1995795  0.19931112 0.19917177 0.1991338  0.19923173 0.19937824
 0.19950058 0.19946933 0.19945598 0.19957995 0.19960093 0.19888367
 0.19722547 0.19657393 0.19660823 0.19662935 0.19643599 0.19648479
 0.19750796 0.19797726 0.19797738 0.19795588 0.19783983 0.19777577
 0.19762468 0.1973706  0.19724283 0.19720563 0.19729021 0.19740815
 0.19750865 0.19744192 0.19720462 0.19711593 0.19701675 0.19621423
 0.19426586 0.19307052 0.19308712 0.19307569 0.1926771  0.19263488
 0.19347882 0.19375569 0.1935907  0.19345048 0.1933991  0.19345774
 0.19343989 0.19322854 0.19307362 0.1931808  0.19345053 0.19365545
 0.19365825 0.19360454 0.19355981 0.19381224 0.19405246 0.19343126
 0.19163652 0.19050592 0.1902539  0.19010271 0.18950674 0.18928619
 0.19016805 0.19038051 0.19004713 0.18976572 0.18952394 0.18954733
 0.18964258 0.18932973 0.18891916 0.18870077 0.18875456 0.18881412
 0.18857272 0.18841487 0.18854663 0.18900956 0.18938793 0.18906376
 0.18744129 0.18614301 0.18595958 0.18637359 0.18613055 0.18575472
 0.18670307 0.18697198 0.18688564 0.18661648 0.18609662 0.1856033
 0.18544589 0.18537495 0.1851451  0.18473749 0.18454786 0.18475778
 0.18497463 0.18487844 0.18448333 0.18435176 0.18443693 0.18381132
 0.1813161  0.17837252 0.17666271 0.17575823 0.17454937 0.1732484
 0.17281543 0.17230463 0.17186315 0.17160532 0.17143308 0.17132322
 0.1710628  0.17075254 0.17047882 0.17037916 0.17033577 0.1703517
 0.17038864 0.17039582 0.17023039 0.17026915 0.1705131  0.17022511
 0.16852856 0.16709688 0.16635515 0.16582808 0.16541307 0.16522554
 0.16597304 0.16596748 0.16540591 0.1649072  0.16448106 0.16389608
 0.162892   0.16175687 0.16146395 0.16133007 0.16071816 0.1599748
 0.16090107 0.16207485 0.16055427 0.15811983 0.16158834 0.15730476]
