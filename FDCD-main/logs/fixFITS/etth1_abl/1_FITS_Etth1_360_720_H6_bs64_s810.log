Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=810, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=106, out_features=318, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30202368.0
params:  34026.0
Trainable parameters:  34026
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8639118671417236
Epoch: 1, Steps: 59 | Train Loss: 0.9708993 Vali Loss: 1.9354429 Test Loss: 0.7749650
Validation loss decreased (inf --> 1.935443).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6644957065582275
Epoch: 2, Steps: 59 | Train Loss: 0.7409239 Vali Loss: 1.7033918 Test Loss: 0.6084894
Validation loss decreased (1.935443 --> 1.703392).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0880343914031982
Epoch: 3, Steps: 59 | Train Loss: 0.6715080 Vali Loss: 1.6253431 Test Loss: 0.5494802
Validation loss decreased (1.703392 --> 1.625343).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.7322056293487549
Epoch: 4, Steps: 59 | Train Loss: 0.6426871 Vali Loss: 1.5830886 Test Loss: 0.5181857
Validation loss decreased (1.625343 --> 1.583089).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7919847965240479
Epoch: 5, Steps: 59 | Train Loss: 0.6258460 Vali Loss: 1.5544242 Test Loss: 0.4971407
Validation loss decreased (1.583089 --> 1.554424).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.841557264328003
Epoch: 6, Steps: 59 | Train Loss: 0.6139918 Vali Loss: 1.5360303 Test Loss: 0.4813791
Validation loss decreased (1.554424 --> 1.536030).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.7787537574768066
Epoch: 7, Steps: 59 | Train Loss: 0.6050309 Vali Loss: 1.5165347 Test Loss: 0.4692292
Validation loss decreased (1.536030 --> 1.516535).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.69991135597229
Epoch: 8, Steps: 59 | Train Loss: 0.5982274 Vali Loss: 1.5050476 Test Loss: 0.4597628
Validation loss decreased (1.516535 --> 1.505048).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.699122667312622
Epoch: 9, Steps: 59 | Train Loss: 0.5925129 Vali Loss: 1.4931023 Test Loss: 0.4522868
Validation loss decreased (1.505048 --> 1.493102).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.669259786605835
Epoch: 10, Steps: 59 | Train Loss: 0.5883739 Vali Loss: 1.4850011 Test Loss: 0.4464185
Validation loss decreased (1.493102 --> 1.485001).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.6495802402496338
Epoch: 11, Steps: 59 | Train Loss: 0.5847875 Vali Loss: 1.4743071 Test Loss: 0.4418704
Validation loss decreased (1.485001 --> 1.474307).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9720141887664795
Epoch: 12, Steps: 59 | Train Loss: 0.5820697 Vali Loss: 1.4733694 Test Loss: 0.4381934
Validation loss decreased (1.474307 --> 1.473369).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6452221870422363
Epoch: 13, Steps: 59 | Train Loss: 0.5796221 Vali Loss: 1.4707466 Test Loss: 0.4353384
Validation loss decreased (1.473369 --> 1.470747).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6480813026428223
Epoch: 14, Steps: 59 | Train Loss: 0.5778938 Vali Loss: 1.4616221 Test Loss: 0.4330567
Validation loss decreased (1.470747 --> 1.461622).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6611144542694092
Epoch: 15, Steps: 59 | Train Loss: 0.5763443 Vali Loss: 1.4545085 Test Loss: 0.4312733
Validation loss decreased (1.461622 --> 1.454509).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.893965721130371
Epoch: 16, Steps: 59 | Train Loss: 0.5750720 Vali Loss: 1.4554069 Test Loss: 0.4298704
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.643122673034668
Epoch: 17, Steps: 59 | Train Loss: 0.5738819 Vali Loss: 1.4549580 Test Loss: 0.4287911
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6059656143188477
Epoch: 18, Steps: 59 | Train Loss: 0.5732396 Vali Loss: 1.4495026 Test Loss: 0.4279182
Validation loss decreased (1.454509 --> 1.449503).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6812222003936768
Epoch: 19, Steps: 59 | Train Loss: 0.5725696 Vali Loss: 1.4469852 Test Loss: 0.4272878
Validation loss decreased (1.449503 --> 1.446985).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.6921288967132568
Epoch: 20, Steps: 59 | Train Loss: 0.5719475 Vali Loss: 1.4488323 Test Loss: 0.4267423
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.7049438953399658
Epoch: 21, Steps: 59 | Train Loss: 0.5711342 Vali Loss: 1.4484739 Test Loss: 0.4263142
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.7323551177978516
Epoch: 22, Steps: 59 | Train Loss: 0.5709332 Vali Loss: 1.4473870 Test Loss: 0.4260485
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.684166669845581
Epoch: 23, Steps: 59 | Train Loss: 0.5706093 Vali Loss: 1.4459096 Test Loss: 0.4258072
Validation loss decreased (1.446985 --> 1.445910).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6368002891540527
Epoch: 24, Steps: 59 | Train Loss: 0.5699911 Vali Loss: 1.4435003 Test Loss: 0.4256376
Validation loss decreased (1.445910 --> 1.443500).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.7196681499481201
Epoch: 25, Steps: 59 | Train Loss: 0.5700684 Vali Loss: 1.4415436 Test Loss: 0.4254989
Validation loss decreased (1.443500 --> 1.441544).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7600905895233154
Epoch: 26, Steps: 59 | Train Loss: 0.5698133 Vali Loss: 1.4427636 Test Loss: 0.4254004
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.7526495456695557
Epoch: 27, Steps: 59 | Train Loss: 0.5694665 Vali Loss: 1.4416132 Test Loss: 0.4253512
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5831177234649658
Epoch: 28, Steps: 59 | Train Loss: 0.5693503 Vali Loss: 1.4449068 Test Loss: 0.4253367
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.61112642288208
Epoch: 29, Steps: 59 | Train Loss: 0.5691400 Vali Loss: 1.4369671 Test Loss: 0.4252757
Validation loss decreased (1.441544 --> 1.436967).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5381147861480713
Epoch: 30, Steps: 59 | Train Loss: 0.5690168 Vali Loss: 1.4364594 Test Loss: 0.4252850
Validation loss decreased (1.436967 --> 1.436459).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5749051570892334
Epoch: 31, Steps: 59 | Train Loss: 0.5689286 Vali Loss: 1.4364669 Test Loss: 0.4252972
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.6201543807983398
Epoch: 32, Steps: 59 | Train Loss: 0.5689762 Vali Loss: 1.4396565 Test Loss: 0.4253039
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.9561500549316406
Epoch: 33, Steps: 59 | Train Loss: 0.5689178 Vali Loss: 1.4419141 Test Loss: 0.4253234
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7394895553588867
Epoch: 34, Steps: 59 | Train Loss: 0.5688155 Vali Loss: 1.4447954 Test Loss: 0.4253459
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.8322575092315674
Epoch: 35, Steps: 59 | Train Loss: 0.5686310 Vali Loss: 1.4367862 Test Loss: 0.4253571
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.5941307544708252
Epoch: 36, Steps: 59 | Train Loss: 0.5685747 Vali Loss: 1.4370584 Test Loss: 0.4253553
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.710329532623291
Epoch: 37, Steps: 59 | Train Loss: 0.5683451 Vali Loss: 1.4416080 Test Loss: 0.4254030
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.6869418621063232
Epoch: 38, Steps: 59 | Train Loss: 0.5685452 Vali Loss: 1.4417489 Test Loss: 0.4254114
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.821742057800293
Epoch: 39, Steps: 59 | Train Loss: 0.5682755 Vali Loss: 1.4378277 Test Loss: 0.4254554
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7455062866210938
Epoch: 40, Steps: 59 | Train Loss: 0.5682572 Vali Loss: 1.4394857 Test Loss: 0.4254670
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6673927307128906
Epoch: 41, Steps: 59 | Train Loss: 0.5683912 Vali Loss: 1.4363581 Test Loss: 0.4254992
Validation loss decreased (1.436459 --> 1.436358).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.753415584564209
Epoch: 42, Steps: 59 | Train Loss: 0.5684232 Vali Loss: 1.4331380 Test Loss: 0.4255160
Validation loss decreased (1.436358 --> 1.433138).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.7200567722320557
Epoch: 43, Steps: 59 | Train Loss: 0.5683260 Vali Loss: 1.4351025 Test Loss: 0.4255330
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.617748737335205
Epoch: 44, Steps: 59 | Train Loss: 0.5682567 Vali Loss: 1.4349796 Test Loss: 0.4255460
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.6853203773498535
Epoch: 45, Steps: 59 | Train Loss: 0.5682341 Vali Loss: 1.4389837 Test Loss: 0.4255848
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6796259880065918
Epoch: 46, Steps: 59 | Train Loss: 0.5681966 Vali Loss: 1.4350283 Test Loss: 0.4256034
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.745851993560791
Epoch: 47, Steps: 59 | Train Loss: 0.5682539 Vali Loss: 1.4347162 Test Loss: 0.4256194
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.7897047996520996
Epoch: 48, Steps: 59 | Train Loss: 0.5680916 Vali Loss: 1.4317603 Test Loss: 0.4256333
Validation loss decreased (1.433138 --> 1.431760).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.7122135162353516
Epoch: 49, Steps: 59 | Train Loss: 0.5680228 Vali Loss: 1.4421217 Test Loss: 0.4256381
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.6675808429718018
Epoch: 50, Steps: 59 | Train Loss: 0.5678551 Vali Loss: 1.4389160 Test Loss: 0.4256668
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6042096614837646
Epoch: 51, Steps: 59 | Train Loss: 0.5680669 Vali Loss: 1.4331248 Test Loss: 0.4256894
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.0415070056915283
Epoch: 52, Steps: 59 | Train Loss: 0.5681832 Vali Loss: 1.4382362 Test Loss: 0.4257149
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6720798015594482
Epoch: 53, Steps: 59 | Train Loss: 0.5680182 Vali Loss: 1.4323853 Test Loss: 0.4257074
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.646636724472046
Epoch: 54, Steps: 59 | Train Loss: 0.5679419 Vali Loss: 1.4383758 Test Loss: 0.4257398
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.9383485317230225
Epoch: 55, Steps: 59 | Train Loss: 0.5680317 Vali Loss: 1.4339931 Test Loss: 0.4257491
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.5880002975463867
Epoch: 56, Steps: 59 | Train Loss: 0.5680167 Vali Loss: 1.4339972 Test Loss: 0.4257568
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.6611247062683105
Epoch: 57, Steps: 59 | Train Loss: 0.5679813 Vali Loss: 1.4349856 Test Loss: 0.4257692
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.5702052116394043
Epoch: 58, Steps: 59 | Train Loss: 0.5678833 Vali Loss: 1.4368780 Test Loss: 0.4257809
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.6930456161499023
Epoch: 59, Steps: 59 | Train Loss: 0.5681194 Vali Loss: 1.4389645 Test Loss: 0.4257961
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.7562904357910156
Epoch: 60, Steps: 59 | Train Loss: 0.5679055 Vali Loss: 1.4342010 Test Loss: 0.4258041
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.7295277118682861
Epoch: 61, Steps: 59 | Train Loss: 0.5679335 Vali Loss: 1.4390979 Test Loss: 0.4258173
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.668020486831665
Epoch: 62, Steps: 59 | Train Loss: 0.5677462 Vali Loss: 1.4342619 Test Loss: 0.4258228
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.7532963752746582
Epoch: 63, Steps: 59 | Train Loss: 0.5676998 Vali Loss: 1.4372195 Test Loss: 0.4258412
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.6522853374481201
Epoch: 64, Steps: 59 | Train Loss: 0.5678700 Vali Loss: 1.4383726 Test Loss: 0.4258408
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.632429838180542
Epoch: 65, Steps: 59 | Train Loss: 0.5677916 Vali Loss: 1.4348373 Test Loss: 0.4258471
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.5482242107391357
Epoch: 66, Steps: 59 | Train Loss: 0.5678637 Vali Loss: 1.4381235 Test Loss: 0.4258539
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.4917073249816895
Epoch: 67, Steps: 59 | Train Loss: 0.5679122 Vali Loss: 1.4350542 Test Loss: 0.4258692
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.7063329219818115
Epoch: 68, Steps: 59 | Train Loss: 0.5678145 Vali Loss: 1.4336716 Test Loss: 0.4258738
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.42458996176719666, mae:0.4458956718444824, rse:0.6237877607345581, corr:[0.22528431 0.23384362 0.23344995 0.23614553 0.2337753  0.2311214
 0.23243697 0.23336111 0.23218945 0.2324182  0.23306088 0.23233883
 0.23177736 0.2318806  0.23148769 0.23100774 0.23091075 0.2305779
 0.23033129 0.23058823 0.2307321  0.23066951 0.23145816 0.23224258
 0.23174109 0.23156507 0.23221707 0.23207334 0.23145251 0.2316341
 0.23187992 0.23128414 0.23077299 0.23101668 0.23110372 0.23070781
 0.23064102 0.23074508 0.23053554 0.2303516  0.23041752 0.2303242
 0.23022962 0.23026502 0.23007722 0.22993605 0.230251   0.23051208
 0.23027128 0.22994037 0.22968118 0.2293482  0.22899379 0.22855107
 0.22812594 0.2274115  0.22717683 0.22697702 0.22654353 0.22623506
 0.226053   0.22585721 0.22557089 0.22546887 0.22552118 0.22567672
 0.2259034  0.22593139 0.22590142 0.22607273 0.22606103 0.22566274
 0.22507504 0.22468291 0.224393   0.22416097 0.22412485 0.2241319
 0.22382908 0.22337238 0.2231756  0.22291635 0.22246456 0.22226535
 0.222251   0.22204219 0.22174062 0.22161655 0.2214778  0.22122698
 0.22096308 0.2208164  0.2208253  0.22106336 0.22140265 0.22219741
 0.22328472 0.22400203 0.22445263 0.22490941 0.22535871 0.22556278
 0.22540937 0.22531742 0.22527364 0.22499277 0.2244995  0.22415416
 0.22405685 0.22402737 0.22396268 0.22389154 0.22388597 0.2237962
 0.22363958 0.22350647 0.22348672 0.22340131 0.22314087 0.2231323
 0.22327118 0.22299804 0.22255522 0.22243463 0.22228564 0.22190475
 0.22165796 0.22163974 0.22135587 0.22083059 0.22045572 0.22020929
 0.22002426 0.21994002 0.21988508 0.21980111 0.21980001 0.2198361
 0.21979386 0.2197325  0.21982495 0.2198694  0.21945399 0.21919708
 0.21920156 0.21893787 0.21856502 0.21820943 0.21773022 0.21725246
 0.21716094 0.21732835 0.21737829 0.21736887 0.21736969 0.21733737
 0.21725446 0.21724081 0.21719615 0.21705545 0.21696149 0.216989
 0.2168923  0.21675913 0.21668188 0.21644099 0.21582685 0.21588182
 0.21644104 0.21693483 0.21766214 0.21862939 0.21891809 0.21878347
 0.21898858 0.21931593 0.2192567  0.21904506 0.21903189 0.21896726
 0.21869205 0.21855481 0.21860892 0.21851406 0.21850614 0.2186487
 0.21859711 0.21861114 0.21886386 0.21870692 0.21813641 0.2179607
 0.218002   0.21770273 0.21729587 0.21710856 0.21672916 0.21620029
 0.21605775 0.21614166 0.21587238 0.21558547 0.21559732 0.21554059
 0.21537763 0.21562238 0.21591693 0.21585947 0.21570992 0.21568151
 0.21560222 0.21534596 0.21509402 0.21478643 0.2143617  0.2144132
 0.21466123 0.21461399 0.21469423 0.2147907  0.21463196 0.21442646
 0.21443777 0.2145541  0.21438259 0.21395223 0.2135702  0.21326658
 0.21284679 0.21263671 0.21272042 0.2127801  0.21265092 0.2124268
 0.21232578 0.2122249  0.21198279 0.21161254 0.21147273 0.21168192
 0.21196987 0.21197025 0.21217364 0.21257123 0.21278329 0.21285923
 0.21303879 0.21315795 0.21310313 0.21291168 0.21267696 0.21237352
 0.21217543 0.21221818 0.21245769 0.21254969 0.21253088 0.21247527
 0.21251844 0.21257539 0.21249522 0.21229854 0.21219751 0.21230838
 0.21212551 0.21179536 0.21184179 0.21197511 0.2117555  0.21159002
 0.21158862 0.21148594 0.21124597 0.21111162 0.21093377 0.21079583
 0.21082108 0.21087371 0.21068856 0.21043874 0.21019733 0.21003036
 0.21002077 0.21014577 0.21017545 0.21011941 0.21028003 0.21085939
 0.21135965 0.2117561  0.2123749  0.21283336 0.21299341 0.21318354
 0.21342523 0.21347016 0.21335417 0.21327832 0.21314655 0.21301717
 0.21295318 0.21300602 0.21317045 0.21322373 0.21315229 0.21300448
 0.21307607 0.21319625 0.21307743 0.21284829 0.21275483 0.21314031
 0.2136849  0.21408717 0.21444991 0.21465147 0.21450101 0.21435548
 0.21433789 0.2143182  0.21412396 0.21373186 0.21322347 0.2129581
 0.21285793 0.2128343  0.21293499 0.21318771 0.21325915 0.21325806
 0.2135112  0.21374096 0.21362026 0.21350108 0.21357849 0.21354145
 0.21341218 0.21343234 0.21353205 0.21328783 0.21289842 0.21257207
 0.21235904 0.21222158 0.21210283 0.2119491  0.21187042 0.21199037
 0.21212599 0.21215497 0.2122288  0.2124292  0.21236368 0.21233837
 0.21255308 0.21257718 0.21218814 0.21196088 0.21173178 0.21147555
 0.21138582 0.21167101 0.21197098 0.2117303  0.21126482 0.2109698
 0.21077077 0.21040909 0.21013522 0.20997812 0.20961656 0.20927471
 0.20919858 0.20904535 0.20879245 0.20853928 0.20836337 0.20835052
 0.20855817 0.2087215  0.20851177 0.20830552 0.20844942 0.20922102
 0.2101202  0.21097514 0.21188232 0.21220957 0.21205917 0.21178828
 0.21149746 0.21089862 0.21045244 0.21017198 0.20977351 0.20954674
 0.20954435 0.2095645  0.2095668  0.20960815 0.20971884 0.21004221
 0.21045211 0.21075884 0.21084554 0.2111379  0.21156915 0.21200235
 0.21222761 0.2124055  0.21244822 0.212246   0.21203586 0.21183334
 0.21152394 0.21139616 0.21140473 0.21128923 0.21106231 0.21104904
 0.2111784  0.21110305 0.21075398 0.21045065 0.21028188 0.21032786
 0.21056642 0.21083954 0.21084881 0.21124391 0.21185118 0.21242353
 0.21287647 0.21323718 0.21338968 0.21330686 0.21320345 0.21307023
 0.21276058 0.21261603 0.21253848 0.21236563 0.21212865 0.21209353
 0.21223205 0.2120909  0.2117864  0.21164584 0.21168035 0.21175684
 0.21185449 0.21196873 0.2121266  0.21234617 0.21259981 0.21330492
 0.21404526 0.2143986  0.21446803 0.21442866 0.21413192 0.21377018
 0.2133757  0.21311964 0.21308792 0.21309797 0.21305786 0.21312033
 0.21329638 0.21332486 0.21330795 0.21338715 0.21344455 0.21347621
 0.2135811  0.21366045 0.21356402 0.2132806  0.21329303 0.21356548
 0.21373941 0.21366154 0.21346597 0.21337016 0.21329677 0.21313924
 0.21289834 0.21263975 0.21248077 0.21237138 0.21202682 0.2118582
 0.21189708 0.21171483 0.21146837 0.21147613 0.2114986  0.21136089
 0.21133465 0.21151277 0.21169657 0.2118059  0.21213406 0.21311627
 0.21418594 0.21483819 0.21526963 0.21558659 0.21563588 0.21530849
 0.21471623 0.21418874 0.21390122 0.21353891 0.21301828 0.21284726
 0.2130343  0.21307223 0.21308874 0.21335691 0.21371439 0.21383055
 0.21396351 0.21433048 0.21453993 0.2145602  0.21495478 0.21554667
 0.21556658 0.21520261 0.2152151  0.2153853  0.21491742 0.21415631
 0.21361066 0.21338378 0.21317309 0.21299806 0.2127456  0.21263957
 0.21255445 0.21262845 0.21281801 0.21316314 0.21348919 0.21358953
 0.21355955 0.21362095 0.21358663 0.2133724  0.21304134 0.21298395
 0.21287021 0.21262044 0.21255009 0.21227892 0.21159127 0.21096863
 0.21040162 0.20986636 0.2095029  0.20896536 0.20842059 0.20825529
 0.208241   0.20787975 0.20767125 0.20772158 0.20775823 0.20770115
 0.2079508  0.2082052  0.20812662 0.2078468  0.20788884 0.2081136
 0.20806369 0.20789368 0.20768231 0.20758721 0.20733917 0.20674224
 0.2062518  0.20584415 0.20557758 0.20537731 0.20527273 0.2052004
 0.20511413 0.20475619 0.20468454 0.20466705 0.20444459 0.20430925
 0.20453958 0.2047469  0.2046404  0.20442235 0.20436296 0.2044045
 0.20429642 0.20394912 0.2037283  0.20328236 0.20247392 0.20175548
 0.2013355  0.20102558 0.20052993 0.20017804 0.2000325  0.19994004
 0.19980243 0.1994456  0.19922522 0.19932638 0.19925779 0.19905297
 0.1991548  0.19935632 0.19935587 0.19939396 0.19973989 0.20027013
 0.20036417 0.19992645 0.19976367 0.19949411 0.19881755 0.19824894
 0.19810414 0.19775744 0.19708616 0.19668932 0.1964995  0.19647367
 0.19651109 0.19634868 0.19635135 0.19630435 0.19603764 0.1960498
 0.19628003 0.19636597 0.19625384 0.19626157 0.19670704 0.1974416
 0.19774553 0.19748773 0.19709377 0.19706102 0.19661911 0.19630472
 0.19595325 0.19539987 0.194826   0.1945822  0.19449833 0.1942407
 0.19403207 0.19400255 0.19397265 0.19360307 0.19361278 0.19395328
 0.19426325 0.19408385 0.19382833 0.19393098 0.19424129 0.19441196
 0.19395201 0.19309469 0.19240858 0.19160779 0.19087574 0.19011247
 0.18921824 0.18855023 0.18848276 0.18841995 0.18810928 0.18797664
 0.18833084 0.18833466 0.18806866 0.18812998 0.18831359 0.1886784
 0.18897454 0.18902622 0.18909135 0.189532   0.18989855 0.18988153
 0.1898898  0.18988577 0.18968534 0.18937296 0.18912356 0.18865292
 0.18815753 0.18805695 0.1881746  0.18771963 0.18703777 0.18691707
 0.18697844 0.18653113 0.18639797 0.18588592 0.18593702 0.18639511
 0.18617201 0.18561533 0.18673462 0.18487234 0.18388292 0.1827377 ]
