Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=50, out_features=143, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6406400.0
params:  7293.0
Trainable parameters:  7293
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7044601440429688
Epoch: 1, Steps: 63 | Train Loss: 0.8155237 Vali Loss: 1.6626475 Test Loss: 0.7381043
Validation loss decreased (inf --> 1.662647).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.0104944705963135
Epoch: 2, Steps: 63 | Train Loss: 0.6188797 Vali Loss: 1.4432598 Test Loss: 0.5848409
Validation loss decreased (1.662647 --> 1.443260).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.990530014038086
Epoch: 3, Steps: 63 | Train Loss: 0.5454623 Vali Loss: 1.3558584 Test Loss: 0.5224677
Validation loss decreased (1.443260 --> 1.355858).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.027506113052368
Epoch: 4, Steps: 63 | Train Loss: 0.5140740 Vali Loss: 1.3132943 Test Loss: 0.4948404
Validation loss decreased (1.355858 --> 1.313294).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8017055988311768
Epoch: 5, Steps: 63 | Train Loss: 0.4986281 Vali Loss: 1.2967467 Test Loss: 0.4808217
Validation loss decreased (1.313294 --> 1.296747).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.0084729194641113
Epoch: 6, Steps: 63 | Train Loss: 0.4903310 Vali Loss: 1.2828815 Test Loss: 0.4727574
Validation loss decreased (1.296747 --> 1.282881).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.881894826889038
Epoch: 7, Steps: 63 | Train Loss: 0.4851805 Vali Loss: 1.2713007 Test Loss: 0.4676037
Validation loss decreased (1.282881 --> 1.271301).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.9637324810028076
Epoch: 8, Steps: 63 | Train Loss: 0.4811300 Vali Loss: 1.2705822 Test Loss: 0.4639798
Validation loss decreased (1.271301 --> 1.270582).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.0025267601013184
Epoch: 9, Steps: 63 | Train Loss: 0.4783518 Vali Loss: 1.2664636 Test Loss: 0.4613187
Validation loss decreased (1.270582 --> 1.266464).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.1606321334838867
Epoch: 10, Steps: 63 | Train Loss: 0.4759886 Vali Loss: 1.2573075 Test Loss: 0.4594052
Validation loss decreased (1.266464 --> 1.257308).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.065669298171997
Epoch: 11, Steps: 63 | Train Loss: 0.4746642 Vali Loss: 1.2524424 Test Loss: 0.4579566
Validation loss decreased (1.257308 --> 1.252442).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.500685453414917
Epoch: 12, Steps: 63 | Train Loss: 0.4734816 Vali Loss: 1.2494558 Test Loss: 0.4568920
Validation loss decreased (1.252442 --> 1.249456).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.860856294631958
Epoch: 13, Steps: 63 | Train Loss: 0.4723833 Vali Loss: 1.2489710 Test Loss: 0.4560291
Validation loss decreased (1.249456 --> 1.248971).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.7728919982910156
Epoch: 14, Steps: 63 | Train Loss: 0.4717433 Vali Loss: 1.2456138 Test Loss: 0.4554929
Validation loss decreased (1.248971 --> 1.245614).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6151902675628662
Epoch: 15, Steps: 63 | Train Loss: 0.4706034 Vali Loss: 1.2520883 Test Loss: 0.4550129
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.8543155193328857
Epoch: 16, Steps: 63 | Train Loss: 0.4704323 Vali Loss: 1.2438759 Test Loss: 0.4547017
Validation loss decreased (1.245614 --> 1.243876).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.366952657699585
Epoch: 17, Steps: 63 | Train Loss: 0.4699597 Vali Loss: 1.2490379 Test Loss: 0.4545124
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.796854257583618
Epoch: 18, Steps: 63 | Train Loss: 0.4693818 Vali Loss: 1.2447894 Test Loss: 0.4543765
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.3313732147216797
Epoch: 19, Steps: 63 | Train Loss: 0.4687814 Vali Loss: 1.2422556 Test Loss: 0.4542469
Validation loss decreased (1.243876 --> 1.242256).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8384630680084229
Epoch: 20, Steps: 63 | Train Loss: 0.4684285 Vali Loss: 1.2425715 Test Loss: 0.4541744
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.3282546997070312
Epoch: 21, Steps: 63 | Train Loss: 0.4683211 Vali Loss: 1.2397654 Test Loss: 0.4541640
Validation loss decreased (1.242256 --> 1.239765).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.621304988861084
Epoch: 22, Steps: 63 | Train Loss: 0.4681381 Vali Loss: 1.2402909 Test Loss: 0.4541474
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.7509057521820068
Epoch: 23, Steps: 63 | Train Loss: 0.4676841 Vali Loss: 1.2388955 Test Loss: 0.4541702
Validation loss decreased (1.239765 --> 1.238896).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.7358355522155762
Epoch: 24, Steps: 63 | Train Loss: 0.4673128 Vali Loss: 1.2386850 Test Loss: 0.4541899
Validation loss decreased (1.238896 --> 1.238685).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.331667900085449
Epoch: 25, Steps: 63 | Train Loss: 0.4675692 Vali Loss: 1.2425945 Test Loss: 0.4542345
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.9627141952514648
Epoch: 26, Steps: 63 | Train Loss: 0.4670142 Vali Loss: 1.2373991 Test Loss: 0.4541911
Validation loss decreased (1.238685 --> 1.237399).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6583635807037354
Epoch: 27, Steps: 63 | Train Loss: 0.4675524 Vali Loss: 1.2379570 Test Loss: 0.4542682
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.9997496604919434
Epoch: 28, Steps: 63 | Train Loss: 0.4673371 Vali Loss: 1.2410427 Test Loss: 0.4542915
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7716891765594482
Epoch: 29, Steps: 63 | Train Loss: 0.4668318 Vali Loss: 1.2385472 Test Loss: 0.4543617
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9254717826843262
Epoch: 30, Steps: 63 | Train Loss: 0.4669951 Vali Loss: 1.2378373 Test Loss: 0.4543597
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.8528320789337158
Epoch: 31, Steps: 63 | Train Loss: 0.4667837 Vali Loss: 1.2345940 Test Loss: 0.4544307
Validation loss decreased (1.237399 --> 1.234594).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.008134126663208
Epoch: 32, Steps: 63 | Train Loss: 0.4669272 Vali Loss: 1.2306687 Test Loss: 0.4544762
Validation loss decreased (1.234594 --> 1.230669).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.3174872398376465
Epoch: 33, Steps: 63 | Train Loss: 0.4667128 Vali Loss: 1.2301923 Test Loss: 0.4545117
Validation loss decreased (1.230669 --> 1.230192).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.3659186363220215
Epoch: 34, Steps: 63 | Train Loss: 0.4666975 Vali Loss: 1.2351308 Test Loss: 0.4544997
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.040127754211426
Epoch: 35, Steps: 63 | Train Loss: 0.4666470 Vali Loss: 1.2373337 Test Loss: 0.4545357
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.0703036785125732
Epoch: 36, Steps: 63 | Train Loss: 0.4664231 Vali Loss: 1.2348316 Test Loss: 0.4545890
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8863444328308105
Epoch: 37, Steps: 63 | Train Loss: 0.4664631 Vali Loss: 1.2300538 Test Loss: 0.4546307
Validation loss decreased (1.230192 --> 1.230054).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.8272202014923096
Epoch: 38, Steps: 63 | Train Loss: 0.4662994 Vali Loss: 1.2364283 Test Loss: 0.4546697
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.8757190704345703
Epoch: 39, Steps: 63 | Train Loss: 0.4662574 Vali Loss: 1.2330111 Test Loss: 0.4546840
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.1511597633361816
Epoch: 40, Steps: 63 | Train Loss: 0.4666437 Vali Loss: 1.2329412 Test Loss: 0.4546951
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.8450779914855957
Epoch: 41, Steps: 63 | Train Loss: 0.4660199 Vali Loss: 1.2365935 Test Loss: 0.4547203
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.263869524002075
Epoch: 42, Steps: 63 | Train Loss: 0.4665543 Vali Loss: 1.2404219 Test Loss: 0.4547467
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.896510124206543
Epoch: 43, Steps: 63 | Train Loss: 0.4661425 Vali Loss: 1.2324027 Test Loss: 0.4547977
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.889772653579712
Epoch: 44, Steps: 63 | Train Loss: 0.4660755 Vali Loss: 1.2377634 Test Loss: 0.4547922
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.8080973625183105
Epoch: 45, Steps: 63 | Train Loss: 0.4662784 Vali Loss: 1.2426101 Test Loss: 0.4548346
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.0830986499786377
Epoch: 46, Steps: 63 | Train Loss: 0.4664043 Vali Loss: 1.2339702 Test Loss: 0.4548531
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.7941009998321533
Epoch: 47, Steps: 63 | Train Loss: 0.4664214 Vali Loss: 1.2343282 Test Loss: 0.4548489
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.8145897388458252
Epoch: 48, Steps: 63 | Train Loss: 0.4655688 Vali Loss: 1.2373595 Test Loss: 0.4548896
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.8565788269042969
Epoch: 49, Steps: 63 | Train Loss: 0.4657431 Vali Loss: 1.2351338 Test Loss: 0.4548930
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.8906936645507812
Epoch: 50, Steps: 63 | Train Loss: 0.4659926 Vali Loss: 1.2374303 Test Loss: 0.4549070
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6857118606567383
Epoch: 51, Steps: 63 | Train Loss: 0.4659072 Vali Loss: 1.2366258 Test Loss: 0.4549292
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.9319419860839844
Epoch: 52, Steps: 63 | Train Loss: 0.4656776 Vali Loss: 1.2331895 Test Loss: 0.4549433
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.7111170291900635
Epoch: 53, Steps: 63 | Train Loss: 0.4663412 Vali Loss: 1.2353894 Test Loss: 0.4549526
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.4145514965057373
Epoch: 54, Steps: 63 | Train Loss: 0.4661417 Vali Loss: 1.2331657 Test Loss: 0.4549751
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.69643235206604
Epoch: 55, Steps: 63 | Train Loss: 0.4660495 Vali Loss: 1.2310598 Test Loss: 0.4549860
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.3246874809265137
Epoch: 56, Steps: 63 | Train Loss: 0.4659061 Vali Loss: 1.2350951 Test Loss: 0.4549979
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.3263566493988037
Epoch: 57, Steps: 63 | Train Loss: 0.4659657 Vali Loss: 1.2377770 Test Loss: 0.4550189
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4538794755935669, mae:0.43441835045814514, rse:0.6413902044296265, corr:[0.25515348 0.25670132 0.25528044 0.25638756 0.25487503 0.25164416
 0.25096422 0.25217688 0.251829   0.25104573 0.25116047 0.2511218
 0.25042567 0.24988702 0.250026   0.2502093  0.25011426 0.25000244
 0.25000834 0.25001314 0.2498082  0.24970433 0.24993113 0.24999012
 0.24934213 0.2484445  0.24771419 0.2476387  0.24751835 0.24688289
 0.24615544 0.24608505 0.24622107 0.24583048 0.24551378 0.24617828
 0.24668552 0.24628994 0.24623998 0.24673015 0.24708098 0.24710552
 0.247214   0.24744874 0.24765316 0.2479952  0.24844132 0.24836132
 0.24742801 0.24632436 0.24462497 0.24307558 0.24201314 0.24103917
 0.24012806 0.23963578 0.23972705 0.24001005 0.23972799 0.24012716
 0.24068509 0.2405383  0.2404073  0.24045022 0.24044017 0.24044773
 0.24067412 0.24061    0.24031833 0.24035183 0.24060138 0.24005502
 0.23870237 0.23779522 0.23706323 0.23653047 0.23619764 0.2358778
 0.23561938 0.23523898 0.23490204 0.2348824  0.2347037  0.23466206
 0.23475927 0.23465869 0.23482649 0.23485391 0.23446348 0.23423947
 0.23440233 0.23444302 0.23430493 0.23474666 0.2354847  0.23566553
 0.23500055 0.23449954 0.23413594 0.23354246 0.23309176 0.23269852
 0.23229736 0.23238893 0.23264964 0.23282613 0.23273318 0.23301637
 0.2332342  0.23288676 0.23270786 0.23283695 0.23282988 0.23260781
 0.23255703 0.23271787 0.2327919  0.23282586 0.2328229  0.23241487
 0.23137522 0.23033002 0.22909664 0.22797354 0.2271514  0.22666638
 0.22634095 0.22635436 0.22655943 0.22688363 0.22688659 0.22736281
 0.22827382 0.22849329 0.22841763 0.22833908 0.22837934 0.22848968
 0.22858399 0.22860026 0.22865753 0.22904608 0.22926828 0.22885674
 0.22790821 0.22715728 0.22616267 0.22471343 0.22384496 0.223499
 0.22337344 0.22339858 0.22375518 0.22414026 0.22423227 0.22457494
 0.2249479  0.22483747 0.22490768 0.22500278 0.22478174 0.22462933
 0.22476241 0.2249271  0.22495916 0.2252626  0.22565365 0.22547764
 0.22443026 0.2234698  0.22276203 0.22223428 0.22178148 0.22122921
 0.22096437 0.22139876 0.22202718 0.22266056 0.22311552 0.22387137
 0.2245262  0.22449358 0.22450161 0.22467992 0.22472498 0.22479783
 0.22500993 0.2252484  0.22556445 0.2260622  0.2263534  0.22591326
 0.22483627 0.22415145 0.22331415 0.22218949 0.22133237 0.22094186
 0.2208167  0.22081736 0.22105058 0.22132182 0.2212402  0.22170138
 0.2223897  0.22235873 0.22202839 0.22169235 0.22155023 0.22157113
 0.22158857 0.22143857 0.2213145  0.2215923  0.22194454 0.22170253
 0.220862   0.22038823 0.2199103  0.21919116 0.2186951  0.21820956
 0.21794555 0.21796176 0.21832673 0.21864302 0.21873988 0.21914703
 0.21961597 0.21958968 0.21976505 0.21979444 0.2193552  0.21913052
 0.21938099 0.21952643 0.21945402 0.21970312 0.22014414 0.21988519
 0.2187677  0.21786146 0.21724942 0.21675137 0.2163654  0.21611463
 0.2160742  0.21620454 0.21658233 0.21707956 0.21749271 0.21835794
 0.21927206 0.21931717 0.21924905 0.21939136 0.21945514 0.2193603
 0.21941303 0.2194522  0.21938851 0.21947984 0.2196761  0.21936613
 0.21853775 0.21797973 0.2171323  0.21622427 0.21574736 0.21547627
 0.21525471 0.21526705 0.21552035 0.2158482  0.21582332 0.21596447
 0.21676181 0.2170888  0.21694635 0.21656619 0.21641876 0.2165552
 0.21661714 0.21653104 0.21668959 0.21725464 0.21773604 0.21753651
 0.21682376 0.21656503 0.21630907 0.21584256 0.21558706 0.21578859
 0.21583694 0.21593016 0.21648009 0.21701856 0.21714626 0.21759762
 0.21824075 0.21821655 0.21843034 0.21868798 0.21852039 0.21835002
 0.21878725 0.21911922 0.21888074 0.21873707 0.21905099 0.21890646
 0.2178589  0.21708272 0.21660922 0.21607986 0.21562031 0.2147839
 0.21442804 0.21462171 0.21506327 0.21548648 0.21551758 0.21637174
 0.21798761 0.21789601 0.21735704 0.21809483 0.21880047 0.21808353
 0.21761918 0.21833512 0.21735601 0.21481107 0.21539633 0.21468505]
