Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=58, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9042432.0
params:  10266.0
Trainable parameters:  10266
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9370625019073486
Epoch: 1, Steps: 59 | Train Loss: 0.9768826 Vali Loss: 2.3574286 Test Loss: 1.0655813
Validation loss decreased (inf --> 2.357429).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6468195915222168
Epoch: 2, Steps: 59 | Train Loss: 0.7816517 Vali Loss: 2.0717201 Test Loss: 0.8837707
Validation loss decreased (2.357429 --> 2.071720).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.753098726272583
Epoch: 3, Steps: 59 | Train Loss: 0.6734757 Vali Loss: 1.9209023 Test Loss: 0.7828374
Validation loss decreased (2.071720 --> 1.920902).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.8091957569122314
Epoch: 4, Steps: 59 | Train Loss: 0.6128299 Vali Loss: 1.8435690 Test Loss: 0.7263794
Validation loss decreased (1.920902 --> 1.843569).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8940668106079102
Epoch: 5, Steps: 59 | Train Loss: 0.5768466 Vali Loss: 1.7915207 Test Loss: 0.6918364
Validation loss decreased (1.843569 --> 1.791521).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.8638839721679688
Epoch: 6, Steps: 59 | Train Loss: 0.5538191 Vali Loss: 1.7562559 Test Loss: 0.6686523
Validation loss decreased (1.791521 --> 1.756256).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9804372787475586
Epoch: 7, Steps: 59 | Train Loss: 0.5378161 Vali Loss: 1.7349998 Test Loss: 0.6516643
Validation loss decreased (1.756256 --> 1.735000).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.4488906860351562
Epoch: 8, Steps: 59 | Train Loss: 0.5260102 Vali Loss: 1.7229559 Test Loss: 0.6380701
Validation loss decreased (1.735000 --> 1.722956).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.8717045783996582
Epoch: 9, Steps: 59 | Train Loss: 0.5163856 Vali Loss: 1.6991704 Test Loss: 0.6268288
Validation loss decreased (1.722956 --> 1.699170).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.7396068572998047
Epoch: 10, Steps: 59 | Train Loss: 0.5086343 Vali Loss: 1.6868622 Test Loss: 0.6167532
Validation loss decreased (1.699170 --> 1.686862).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.2375242710113525
Epoch: 11, Steps: 59 | Train Loss: 0.5018472 Vali Loss: 1.6751616 Test Loss: 0.6078999
Validation loss decreased (1.686862 --> 1.675162).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.085233211517334
Epoch: 12, Steps: 59 | Train Loss: 0.4962030 Vali Loss: 1.6637516 Test Loss: 0.5998108
Validation loss decreased (1.675162 --> 1.663752).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.5342812538146973
Epoch: 13, Steps: 59 | Train Loss: 0.4909342 Vali Loss: 1.6552589 Test Loss: 0.5921387
Validation loss decreased (1.663752 --> 1.655259).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.9360442161560059
Epoch: 14, Steps: 59 | Train Loss: 0.4863596 Vali Loss: 1.6505415 Test Loss: 0.5854424
Validation loss decreased (1.655259 --> 1.650542).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.3126654624938965
Epoch: 15, Steps: 59 | Train Loss: 0.4821460 Vali Loss: 1.6429708 Test Loss: 0.5792575
Validation loss decreased (1.650542 --> 1.642971).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.7063748836517334
Epoch: 16, Steps: 59 | Train Loss: 0.4785351 Vali Loss: 1.6319835 Test Loss: 0.5733972
Validation loss decreased (1.642971 --> 1.631984).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.11833119392395
Epoch: 17, Steps: 59 | Train Loss: 0.4751409 Vali Loss: 1.6287853 Test Loss: 0.5681151
Validation loss decreased (1.631984 --> 1.628785).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.448819398880005
Epoch: 18, Steps: 59 | Train Loss: 0.4721743 Vali Loss: 1.6177846 Test Loss: 0.5628916
Validation loss decreased (1.628785 --> 1.617785).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.884110689163208
Epoch: 19, Steps: 59 | Train Loss: 0.4692380 Vali Loss: 1.6146815 Test Loss: 0.5584836
Validation loss decreased (1.617785 --> 1.614681).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.584507703781128
Epoch: 20, Steps: 59 | Train Loss: 0.4666650 Vali Loss: 1.6048483 Test Loss: 0.5542983
Validation loss decreased (1.614681 --> 1.604848).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0068929195404053
Epoch: 21, Steps: 59 | Train Loss: 0.4643272 Vali Loss: 1.6027552 Test Loss: 0.5503211
Validation loss decreased (1.604848 --> 1.602755).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0533010959625244
Epoch: 22, Steps: 59 | Train Loss: 0.4621249 Vali Loss: 1.5977213 Test Loss: 0.5467901
Validation loss decreased (1.602755 --> 1.597721).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8041093349456787
Epoch: 23, Steps: 59 | Train Loss: 0.4600865 Vali Loss: 1.5971540 Test Loss: 0.5432962
Validation loss decreased (1.597721 --> 1.597154).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.04864239692688
Epoch: 24, Steps: 59 | Train Loss: 0.4582062 Vali Loss: 1.5872869 Test Loss: 0.5401299
Validation loss decreased (1.597154 --> 1.587287).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.943460464477539
Epoch: 25, Steps: 59 | Train Loss: 0.4565885 Vali Loss: 1.5853810 Test Loss: 0.5370325
Validation loss decreased (1.587287 --> 1.585381).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.1796298027038574
Epoch: 26, Steps: 59 | Train Loss: 0.4549082 Vali Loss: 1.5847400 Test Loss: 0.5341500
Validation loss decreased (1.585381 --> 1.584740).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.9472050666809082
Epoch: 27, Steps: 59 | Train Loss: 0.4534112 Vali Loss: 1.5836289 Test Loss: 0.5315012
Validation loss decreased (1.584740 --> 1.583629).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.7701492309570312
Epoch: 28, Steps: 59 | Train Loss: 0.4520701 Vali Loss: 1.5763739 Test Loss: 0.5291579
Validation loss decreased (1.583629 --> 1.576374).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.1709775924682617
Epoch: 29, Steps: 59 | Train Loss: 0.4507293 Vali Loss: 1.5704658 Test Loss: 0.5267674
Validation loss decreased (1.576374 --> 1.570466).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.1932029724121094
Epoch: 30, Steps: 59 | Train Loss: 0.4494408 Vali Loss: 1.5726099 Test Loss: 0.5246701
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.847062349319458
Epoch: 31, Steps: 59 | Train Loss: 0.4484090 Vali Loss: 1.5706947 Test Loss: 0.5225332
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.9565110206604004
Epoch: 32, Steps: 59 | Train Loss: 0.4474342 Vali Loss: 1.5640402 Test Loss: 0.5206847
Validation loss decreased (1.570466 --> 1.564040).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.7955694198608398
Epoch: 33, Steps: 59 | Train Loss: 0.4463752 Vali Loss: 1.5587652 Test Loss: 0.5188865
Validation loss decreased (1.564040 --> 1.558765).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6145520210266113
Epoch: 34, Steps: 59 | Train Loss: 0.4453062 Vali Loss: 1.5580168 Test Loss: 0.5171471
Validation loss decreased (1.558765 --> 1.558017).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.857985496520996
Epoch: 35, Steps: 59 | Train Loss: 0.4443795 Vali Loss: 1.5563701 Test Loss: 0.5156273
Validation loss decreased (1.558017 --> 1.556370).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.3863706588745117
Epoch: 36, Steps: 59 | Train Loss: 0.4436572 Vali Loss: 1.5546410 Test Loss: 0.5140996
Validation loss decreased (1.556370 --> 1.554641).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.5031678676605225
Epoch: 37, Steps: 59 | Train Loss: 0.4427747 Vali Loss: 1.5506070 Test Loss: 0.5127728
Validation loss decreased (1.554641 --> 1.550607).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6084325313568115
Epoch: 38, Steps: 59 | Train Loss: 0.4421566 Vali Loss: 1.5510261 Test Loss: 0.5113917
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.447159767150879
Epoch: 39, Steps: 59 | Train Loss: 0.4414093 Vali Loss: 1.5487850 Test Loss: 0.5101283
Validation loss decreased (1.550607 --> 1.548785).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.4652392864227295
Epoch: 40, Steps: 59 | Train Loss: 0.4407359 Vali Loss: 1.5486362 Test Loss: 0.5089139
Validation loss decreased (1.548785 --> 1.548636).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.2889604568481445
Epoch: 41, Steps: 59 | Train Loss: 0.4401132 Vali Loss: 1.5452927 Test Loss: 0.5077701
Validation loss decreased (1.548636 --> 1.545293).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.7635326385498047
Epoch: 42, Steps: 59 | Train Loss: 0.4396401 Vali Loss: 1.5462470 Test Loss: 0.5067124
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9410889148712158
Epoch: 43, Steps: 59 | Train Loss: 0.4390543 Vali Loss: 1.5423387 Test Loss: 0.5057511
Validation loss decreased (1.545293 --> 1.542339).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.108487844467163
Epoch: 44, Steps: 59 | Train Loss: 0.4385418 Vali Loss: 1.5476629 Test Loss: 0.5048134
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.443664073944092
Epoch: 45, Steps: 59 | Train Loss: 0.4380450 Vali Loss: 1.5409658 Test Loss: 0.5039282
Validation loss decreased (1.542339 --> 1.540966).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.712037563323975
Epoch: 46, Steps: 59 | Train Loss: 0.4375853 Vali Loss: 1.5430820 Test Loss: 0.5030267
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.277100563049316
Epoch: 47, Steps: 59 | Train Loss: 0.4369963 Vali Loss: 1.5450158 Test Loss: 0.5022645
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.116483926773071
Epoch: 48, Steps: 59 | Train Loss: 0.4367043 Vali Loss: 1.5381293 Test Loss: 0.5015258
Validation loss decreased (1.540966 --> 1.538129).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 5.5870373249053955
Epoch: 49, Steps: 59 | Train Loss: 0.4362575 Vali Loss: 1.5362384 Test Loss: 0.5007285
Validation loss decreased (1.538129 --> 1.536238).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 5.143783092498779
Epoch: 50, Steps: 59 | Train Loss: 0.4358748 Vali Loss: 1.5381479 Test Loss: 0.5000742
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.862914562225342
Epoch: 51, Steps: 59 | Train Loss: 0.4356510 Vali Loss: 1.5397223 Test Loss: 0.4994712
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.7455942630767822
Epoch: 52, Steps: 59 | Train Loss: 0.4352637 Vali Loss: 1.5363874 Test Loss: 0.4987881
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.081289291381836
Epoch: 53, Steps: 59 | Train Loss: 0.4348911 Vali Loss: 1.5366094 Test Loss: 0.4982419
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.8909523487091064
Epoch: 54, Steps: 59 | Train Loss: 0.4345801 Vali Loss: 1.5358708 Test Loss: 0.4976827
Validation loss decreased (1.536238 --> 1.535871).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.0365188121795654
Epoch: 55, Steps: 59 | Train Loss: 0.4343874 Vali Loss: 1.5358645 Test Loss: 0.4971229
Validation loss decreased (1.535871 --> 1.535864).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.6480369567871094
Epoch: 56, Steps: 59 | Train Loss: 0.4340618 Vali Loss: 1.5351413 Test Loss: 0.4966401
Validation loss decreased (1.535864 --> 1.535141).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.215916633605957
Epoch: 57, Steps: 59 | Train Loss: 0.4337194 Vali Loss: 1.5344231 Test Loss: 0.4961822
Validation loss decreased (1.535141 --> 1.534423).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.8779141902923584
Epoch: 58, Steps: 59 | Train Loss: 0.4336214 Vali Loss: 1.5352606 Test Loss: 0.4957244
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.1390318870544434
Epoch: 59, Steps: 59 | Train Loss: 0.4331798 Vali Loss: 1.5296714 Test Loss: 0.4952933
Validation loss decreased (1.534423 --> 1.529671).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.056525707244873
Epoch: 60, Steps: 59 | Train Loss: 0.4330694 Vali Loss: 1.5277719 Test Loss: 0.4948729
Validation loss decreased (1.529671 --> 1.527772).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.5589373111724854
Epoch: 61, Steps: 59 | Train Loss: 0.4327784 Vali Loss: 1.5305640 Test Loss: 0.4944868
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6918048858642578
Epoch: 62, Steps: 59 | Train Loss: 0.4327281 Vali Loss: 1.5318844 Test Loss: 0.4941396
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.7297301292419434
Epoch: 63, Steps: 59 | Train Loss: 0.4325516 Vali Loss: 1.5285194 Test Loss: 0.4937911
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.739992618560791
Epoch: 64, Steps: 59 | Train Loss: 0.4322888 Vali Loss: 1.5280564 Test Loss: 0.4934506
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.8106088638305664
Epoch: 65, Steps: 59 | Train Loss: 0.4321305 Vali Loss: 1.5292000 Test Loss: 0.4931136
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.829268455505371
Epoch: 66, Steps: 59 | Train Loss: 0.4318635 Vali Loss: 1.5254011 Test Loss: 0.4928434
Validation loss decreased (1.527772 --> 1.525401).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.04988431930542
Epoch: 67, Steps: 59 | Train Loss: 0.4318413 Vali Loss: 1.5239003 Test Loss: 0.4925468
Validation loss decreased (1.525401 --> 1.523900).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.7345831394195557
Epoch: 68, Steps: 59 | Train Loss: 0.4316761 Vali Loss: 1.5280268 Test Loss: 0.4922755
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.2228827476501465
Epoch: 69, Steps: 59 | Train Loss: 0.4314422 Vali Loss: 1.5281579 Test Loss: 0.4920186
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.8414955139160156
Epoch: 70, Steps: 59 | Train Loss: 0.4312256 Vali Loss: 1.5293460 Test Loss: 0.4917781
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.013400077819824
Epoch: 71, Steps: 59 | Train Loss: 0.4312387 Vali Loss: 1.5288495 Test Loss: 0.4915377
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.8051915168762207
Epoch: 72, Steps: 59 | Train Loss: 0.4310646 Vali Loss: 1.5244170 Test Loss: 0.4913107
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.718980312347412
Epoch: 73, Steps: 59 | Train Loss: 0.4309980 Vali Loss: 1.5268786 Test Loss: 0.4911152
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.9172513484954834
Epoch: 74, Steps: 59 | Train Loss: 0.4309467 Vali Loss: 1.5243725 Test Loss: 0.4909066
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.3273675441741943
Epoch: 75, Steps: 59 | Train Loss: 0.4307559 Vali Loss: 1.5229324 Test Loss: 0.4907147
Validation loss decreased (1.523900 --> 1.522932).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.105135917663574
Epoch: 76, Steps: 59 | Train Loss: 0.4307664 Vali Loss: 1.5244857 Test Loss: 0.4905380
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.623133420944214
Epoch: 77, Steps: 59 | Train Loss: 0.4306029 Vali Loss: 1.5264198 Test Loss: 0.4903629
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.343127727508545
Epoch: 78, Steps: 59 | Train Loss: 0.4304484 Vali Loss: 1.5263757 Test Loss: 0.4902050
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 3.1410043239593506
Epoch: 79, Steps: 59 | Train Loss: 0.4304916 Vali Loss: 1.5216761 Test Loss: 0.4900449
Validation loss decreased (1.522932 --> 1.521676).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.600141763687134
Epoch: 80, Steps: 59 | Train Loss: 0.4303993 Vali Loss: 1.5249605 Test Loss: 0.4898984
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.670147180557251
Epoch: 81, Steps: 59 | Train Loss: 0.4302008 Vali Loss: 1.5223138 Test Loss: 0.4897670
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.9251575469970703
Epoch: 82, Steps: 59 | Train Loss: 0.4302404 Vali Loss: 1.5258900 Test Loss: 0.4896287
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.1137659549713135
Epoch: 83, Steps: 59 | Train Loss: 0.4301329 Vali Loss: 1.5213149 Test Loss: 0.4894988
Validation loss decreased (1.521676 --> 1.521315).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.395087242126465
Epoch: 84, Steps: 59 | Train Loss: 0.4300834 Vali Loss: 1.5177530 Test Loss: 0.4893798
Validation loss decreased (1.521315 --> 1.517753).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.586062431335449
Epoch: 85, Steps: 59 | Train Loss: 0.4299948 Vali Loss: 1.5221200 Test Loss: 0.4892667
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.378042697906494
Epoch: 86, Steps: 59 | Train Loss: 0.4298203 Vali Loss: 1.5231478 Test Loss: 0.4891523
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.3824093341827393
Epoch: 87, Steps: 59 | Train Loss: 0.4299412 Vali Loss: 1.5268576 Test Loss: 0.4890559
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.998084545135498
Epoch: 88, Steps: 59 | Train Loss: 0.4298755 Vali Loss: 1.5235157 Test Loss: 0.4889566
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.9237134456634521
Epoch: 89, Steps: 59 | Train Loss: 0.4298078 Vali Loss: 1.5258069 Test Loss: 0.4888596
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.7004694938659668
Epoch: 90, Steps: 59 | Train Loss: 0.4297304 Vali Loss: 1.5262113 Test Loss: 0.4887747
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.6637241840362549
Epoch: 91, Steps: 59 | Train Loss: 0.4297539 Vali Loss: 1.5156291 Test Loss: 0.4886871
Validation loss decreased (1.517753 --> 1.515629).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.7383122444152832
Epoch: 92, Steps: 59 | Train Loss: 0.4296616 Vali Loss: 1.5212703 Test Loss: 0.4886062
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.8897266387939453
Epoch: 93, Steps: 59 | Train Loss: 0.4297281 Vali Loss: 1.5222762 Test Loss: 0.4885279
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.7913060188293457
Epoch: 94, Steps: 59 | Train Loss: 0.4296872 Vali Loss: 1.5231742 Test Loss: 0.4884605
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.905876874923706
Epoch: 95, Steps: 59 | Train Loss: 0.4295528 Vali Loss: 1.5229783 Test Loss: 0.4883898
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.005112648010254
Epoch: 96, Steps: 59 | Train Loss: 0.4294626 Vali Loss: 1.5166420 Test Loss: 0.4883244
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.9850428104400635
Epoch: 97, Steps: 59 | Train Loss: 0.4294067 Vali Loss: 1.5155373 Test Loss: 0.4882596
Validation loss decreased (1.515629 --> 1.515537).  Saving model ...
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.7133772373199463
Epoch: 98, Steps: 59 | Train Loss: 0.4293891 Vali Loss: 1.5282450 Test Loss: 0.4881999
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.642648458480835
Epoch: 99, Steps: 59 | Train Loss: 0.4294583 Vali Loss: 1.5232370 Test Loss: 0.4881432
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.251702308654785
Epoch: 100, Steps: 59 | Train Loss: 0.4294370 Vali Loss: 1.5204220 Test Loss: 0.4880905
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=58, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9042432.0
params:  10266.0
Trainable parameters:  10266
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7976040840148926
Epoch: 1, Steps: 59 | Train Loss: 0.6009555 Vali Loss: 1.4854934 Test Loss: 0.4637319
Validation loss decreased (inf --> 1.485493).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.4411323070526123
Epoch: 2, Steps: 59 | Train Loss: 0.5906252 Vali Loss: 1.4713944 Test Loss: 0.4502941
Validation loss decreased (1.485493 --> 1.471394).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.877812385559082
Epoch: 3, Steps: 59 | Train Loss: 0.5849930 Vali Loss: 1.4530947 Test Loss: 0.4423258
Validation loss decreased (1.471394 --> 1.453095).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.117779016494751
Epoch: 4, Steps: 59 | Train Loss: 0.5814645 Vali Loss: 1.4493905 Test Loss: 0.4376311
Validation loss decreased (1.453095 --> 1.449391).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.0960066318511963
Epoch: 5, Steps: 59 | Train Loss: 0.5792902 Vali Loss: 1.4435738 Test Loss: 0.4349854
Validation loss decreased (1.449391 --> 1.443574).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.118651866912842
Epoch: 6, Steps: 59 | Train Loss: 0.5777538 Vali Loss: 1.4472635 Test Loss: 0.4334689
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.743255376815796
Epoch: 7, Steps: 59 | Train Loss: 0.5772034 Vali Loss: 1.4441004 Test Loss: 0.4329390
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.626133918762207
Epoch: 8, Steps: 59 | Train Loss: 0.5765171 Vali Loss: 1.4356170 Test Loss: 0.4326849
Validation loss decreased (1.443574 --> 1.435617).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.2447268962860107
Epoch: 9, Steps: 59 | Train Loss: 0.5760603 Vali Loss: 1.4337698 Test Loss: 0.4325086
Validation loss decreased (1.435617 --> 1.433770).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.2736661434173584
Epoch: 10, Steps: 59 | Train Loss: 0.5758909 Vali Loss: 1.4383237 Test Loss: 0.4325755
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.8986780643463135
Epoch: 11, Steps: 59 | Train Loss: 0.5757467 Vali Loss: 1.4329534 Test Loss: 0.4326140
Validation loss decreased (1.433770 --> 1.432953).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.809889793395996
Epoch: 12, Steps: 59 | Train Loss: 0.5756740 Vali Loss: 1.4344835 Test Loss: 0.4326445
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.1552133560180664
Epoch: 13, Steps: 59 | Train Loss: 0.5756923 Vali Loss: 1.4373292 Test Loss: 0.4327157
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.7272400856018066
Epoch: 14, Steps: 59 | Train Loss: 0.5755310 Vali Loss: 1.4335859 Test Loss: 0.4327665
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.8424994945526123
Epoch: 15, Steps: 59 | Train Loss: 0.5754755 Vali Loss: 1.4347854 Test Loss: 0.4328417
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.663478136062622
Epoch: 16, Steps: 59 | Train Loss: 0.5754499 Vali Loss: 1.4391688 Test Loss: 0.4328633
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.8396258354187012
Epoch: 17, Steps: 59 | Train Loss: 0.5755203 Vali Loss: 1.4364706 Test Loss: 0.4329590
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7488512992858887
Epoch: 18, Steps: 59 | Train Loss: 0.5752970 Vali Loss: 1.4339980 Test Loss: 0.4329867
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6929843425750732
Epoch: 19, Steps: 59 | Train Loss: 0.5752900 Vali Loss: 1.4302237 Test Loss: 0.4329976
Validation loss decreased (1.432953 --> 1.430224).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9817359447479248
Epoch: 20, Steps: 59 | Train Loss: 0.5753559 Vali Loss: 1.4365088 Test Loss: 0.4330415
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.8012332916259766
Epoch: 21, Steps: 59 | Train Loss: 0.5751877 Vali Loss: 1.4356667 Test Loss: 0.4330444
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.9040601253509521
Epoch: 22, Steps: 59 | Train Loss: 0.5752487 Vali Loss: 1.4354782 Test Loss: 0.4330722
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.7178282737731934
Epoch: 23, Steps: 59 | Train Loss: 0.5753666 Vali Loss: 1.4324501 Test Loss: 0.4331497
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.7612552642822266
Epoch: 24, Steps: 59 | Train Loss: 0.5752714 Vali Loss: 1.4335207 Test Loss: 0.4331426
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.872738838195801
Epoch: 25, Steps: 59 | Train Loss: 0.5750307 Vali Loss: 1.4316287 Test Loss: 0.4331236
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7299799919128418
Epoch: 26, Steps: 59 | Train Loss: 0.5752358 Vali Loss: 1.4300978 Test Loss: 0.4331956
Validation loss decreased (1.430224 --> 1.430098).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1281697750091553
Epoch: 27, Steps: 59 | Train Loss: 0.5750076 Vali Loss: 1.4338052 Test Loss: 0.4332048
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.753190279006958
Epoch: 28, Steps: 59 | Train Loss: 0.5751826 Vali Loss: 1.4321271 Test Loss: 0.4331906
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.3401694297790527
Epoch: 29, Steps: 59 | Train Loss: 0.5750667 Vali Loss: 1.4342526 Test Loss: 0.4332516
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.575268507003784
Epoch: 30, Steps: 59 | Train Loss: 0.5749726 Vali Loss: 1.4334840 Test Loss: 0.4332214
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.849705934524536
Epoch: 31, Steps: 59 | Train Loss: 0.5751052 Vali Loss: 1.4283442 Test Loss: 0.4332102
Validation loss decreased (1.430098 --> 1.428344).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.7308340072631836
Epoch: 32, Steps: 59 | Train Loss: 0.5750863 Vali Loss: 1.4354696 Test Loss: 0.4332226
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.5350208282470703
Epoch: 33, Steps: 59 | Train Loss: 0.5749319 Vali Loss: 1.4316455 Test Loss: 0.4332328
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.992720127105713
Epoch: 34, Steps: 59 | Train Loss: 0.5751604 Vali Loss: 1.4364058 Test Loss: 0.4332536
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.3650619983673096
Epoch: 35, Steps: 59 | Train Loss: 0.5748600 Vali Loss: 1.4297442 Test Loss: 0.4332832
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.6611998081207275
Epoch: 36, Steps: 59 | Train Loss: 0.5750665 Vali Loss: 1.4325252 Test Loss: 0.4332982
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8967185020446777
Epoch: 37, Steps: 59 | Train Loss: 0.5751337 Vali Loss: 1.4340729 Test Loss: 0.4332868
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7649304866790771
Epoch: 38, Steps: 59 | Train Loss: 0.5750318 Vali Loss: 1.4371386 Test Loss: 0.4332834
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.9388387203216553
Epoch: 39, Steps: 59 | Train Loss: 0.5749532 Vali Loss: 1.4334435 Test Loss: 0.4332970
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.845853567123413
Epoch: 40, Steps: 59 | Train Loss: 0.5749151 Vali Loss: 1.4343433 Test Loss: 0.4333427
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.8244731426239014
Epoch: 41, Steps: 59 | Train Loss: 0.5750699 Vali Loss: 1.4352145 Test Loss: 0.4333500
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.160989999771118
Epoch: 42, Steps: 59 | Train Loss: 0.5750053 Vali Loss: 1.4342414 Test Loss: 0.4333478
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.7163152694702148
Epoch: 43, Steps: 59 | Train Loss: 0.5750050 Vali Loss: 1.4276299 Test Loss: 0.4333751
Validation loss decreased (1.428344 --> 1.427630).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.807739496231079
Epoch: 44, Steps: 59 | Train Loss: 0.5749779 Vali Loss: 1.4342865 Test Loss: 0.4333742
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.2796802520751953
Epoch: 45, Steps: 59 | Train Loss: 0.5749305 Vali Loss: 1.4343679 Test Loss: 0.4333737
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8766605854034424
Epoch: 46, Steps: 59 | Train Loss: 0.5750086 Vali Loss: 1.4360434 Test Loss: 0.4333841
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.992997646331787
Epoch: 47, Steps: 59 | Train Loss: 0.5749525 Vali Loss: 1.4284987 Test Loss: 0.4333798
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.83253812789917
Epoch: 48, Steps: 59 | Train Loss: 0.5748441 Vali Loss: 1.4322999 Test Loss: 0.4333972
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.3462233543395996
Epoch: 49, Steps: 59 | Train Loss: 0.5749142 Vali Loss: 1.4334683 Test Loss: 0.4333895
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.8979134559631348
Epoch: 50, Steps: 59 | Train Loss: 0.5749905 Vali Loss: 1.4303763 Test Loss: 0.4334109
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.389730930328369
Epoch: 51, Steps: 59 | Train Loss: 0.5748602 Vali Loss: 1.4367681 Test Loss: 0.4333965
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.3213107585906982
Epoch: 52, Steps: 59 | Train Loss: 0.5750079 Vali Loss: 1.4352384 Test Loss: 0.4334108
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.594221830368042
Epoch: 53, Steps: 59 | Train Loss: 0.5748548 Vali Loss: 1.4243731 Test Loss: 0.4334044
Validation loss decreased (1.427630 --> 1.424373).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.372321367263794
Epoch: 54, Steps: 59 | Train Loss: 0.5749472 Vali Loss: 1.4296761 Test Loss: 0.4334162
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.7236146926879883
Epoch: 55, Steps: 59 | Train Loss: 0.5748346 Vali Loss: 1.4309782 Test Loss: 0.4334284
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9354150295257568
Epoch: 56, Steps: 59 | Train Loss: 0.5749902 Vali Loss: 1.4356061 Test Loss: 0.4334093
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.6174089908599854
Epoch: 57, Steps: 59 | Train Loss: 0.5748957 Vali Loss: 1.4315290 Test Loss: 0.4334314
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.3997418880462646
Epoch: 58, Steps: 59 | Train Loss: 0.5748652 Vali Loss: 1.4344641 Test Loss: 0.4334403
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.020299196243286
Epoch: 59, Steps: 59 | Train Loss: 0.5747374 Vali Loss: 1.4299326 Test Loss: 0.4334351
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9214329719543457
Epoch: 60, Steps: 59 | Train Loss: 0.5747955 Vali Loss: 1.4352396 Test Loss: 0.4334452
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.8972182273864746
Epoch: 61, Steps: 59 | Train Loss: 0.5747398 Vali Loss: 1.4313822 Test Loss: 0.4334419
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.0444743633270264
Epoch: 62, Steps: 59 | Train Loss: 0.5748658 Vali Loss: 1.4392784 Test Loss: 0.4334491
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.1642165184020996
Epoch: 63, Steps: 59 | Train Loss: 0.5747526 Vali Loss: 1.4366701 Test Loss: 0.4334399
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.8658952713012695
Epoch: 64, Steps: 59 | Train Loss: 0.5748722 Vali Loss: 1.4334034 Test Loss: 0.4334442
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.8746628761291504
Epoch: 65, Steps: 59 | Train Loss: 0.5748609 Vali Loss: 1.4356942 Test Loss: 0.4334543
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.6066460609436035
Epoch: 66, Steps: 59 | Train Loss: 0.5748877 Vali Loss: 1.4347692 Test Loss: 0.4334563
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.3797783851623535
Epoch: 67, Steps: 59 | Train Loss: 0.5748340 Vali Loss: 1.4289246 Test Loss: 0.4334528
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.6486473083496094
Epoch: 68, Steps: 59 | Train Loss: 0.5748781 Vali Loss: 1.4319620 Test Loss: 0.4334644
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.5166740417480469
Epoch: 69, Steps: 59 | Train Loss: 0.5748236 Vali Loss: 1.4327735 Test Loss: 0.4334618
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.7103254795074463
Epoch: 70, Steps: 59 | Train Loss: 0.5746474 Vali Loss: 1.4321231 Test Loss: 0.4334652
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.7623207569122314
Epoch: 71, Steps: 59 | Train Loss: 0.5749397 Vali Loss: 1.4311224 Test Loss: 0.4334626
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.6293551921844482
Epoch: 72, Steps: 59 | Train Loss: 0.5746641 Vali Loss: 1.4339471 Test Loss: 0.4334648
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.5772223472595215
Epoch: 73, Steps: 59 | Train Loss: 0.5748293 Vali Loss: 1.4357296 Test Loss: 0.4334670
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4324356019496918, mae:0.45117056369781494, rse:0.6295245885848999, corr:[0.22666116 0.23380736 0.2346214  0.23193409 0.22917157 0.22791837
 0.22790238 0.22840862 0.2283265  0.2279384  0.22758985 0.22737993
 0.22741231 0.22754145 0.22752152 0.22735722 0.22703433 0.22665575
 0.22637269 0.22618532 0.22616939 0.22654113 0.22717759 0.22793487
 0.2282822  0.2282536  0.22816457 0.2280397  0.22783023 0.22756985
 0.2272072  0.22679685 0.22643642 0.22613809 0.22604132 0.22604476
 0.22618364 0.22632226 0.22643524 0.22647855 0.22660589 0.22654454
 0.22640604 0.226264   0.22616048 0.22618154 0.22653781 0.22708373
 0.22727376 0.22701211 0.22624135 0.22543842 0.22494186 0.2245038
 0.2241743  0.22374861 0.22346559 0.2231912  0.22282963 0.22259182
 0.22241655 0.2223332  0.22225702 0.22233815 0.22245795 0.22253878
 0.22262973 0.22264114 0.22266153 0.2227803  0.2228538  0.22283223
 0.22231436 0.22150722 0.22070943 0.22020262 0.21995357 0.21979573
 0.21965514 0.2194032  0.2190343  0.21865408 0.2182517  0.21798666
 0.21791247 0.2178973  0.21781026 0.2175558  0.21724634 0.217007
 0.21679138 0.21662992 0.2165997  0.21693279 0.21774769 0.21895798
 0.2202828  0.22126201 0.22178344 0.22188963 0.22179647 0.22173618
 0.2217065  0.22172712 0.22159238 0.22125444 0.22084567 0.2204614
 0.22027206 0.2203182  0.2204771  0.22072889 0.2209142  0.22091559
 0.22081302 0.22061384 0.22045553 0.22037408 0.22046277 0.22066715
 0.22070171 0.2203511  0.2196088  0.21893142 0.21849313 0.21827823
 0.21818691 0.21809536 0.21783192 0.2173708  0.2169435  0.216639
 0.2164422  0.21633203 0.2163483  0.21640584 0.21650425 0.21654017
 0.21653329 0.21650575 0.21649213 0.21649033 0.21645717 0.21637805
 0.21622512 0.21580654 0.21517968 0.21445638 0.2139108  0.21355683
 0.21338601 0.21340016 0.21345231 0.21349366 0.21344425 0.21333644
 0.21320589 0.21309936 0.21305159 0.21303813 0.21302077 0.21304213
 0.21303253 0.2129307  0.21283008 0.21278363 0.21289945 0.21329431
 0.21393721 0.21466385 0.21516351 0.21546835 0.21564701 0.21575972
 0.2158134  0.2158069  0.21577917 0.21577242 0.2157635  0.21567014
 0.21552372 0.21536452 0.21531692 0.21539961 0.21557206 0.2158099
 0.2160036  0.2160441  0.21604426 0.21601208 0.21596149 0.21582672
 0.21550411 0.21500604 0.21431154 0.21368176 0.21319418 0.21279947
 0.21249704 0.21240413 0.2123585  0.21226156 0.21211338 0.2120055
 0.21197186 0.2120645  0.21218234 0.21224687 0.21233015 0.21233892
 0.21222901 0.211933   0.21159358 0.21139155 0.21127075 0.21132782
 0.21142805 0.21128753 0.21103542 0.21078873 0.21071349 0.21078221
 0.21077894 0.2106474  0.21038146 0.21005133 0.20967446 0.20939228
 0.20923096 0.20910877 0.20904902 0.20896596 0.20881578 0.20862232
 0.20846653 0.20825689 0.20806178 0.20798212 0.20815954 0.20844175
 0.20876577 0.20888178 0.20881946 0.20880802 0.20891437 0.20907095
 0.20928144 0.20943943 0.2094478  0.20915304 0.2087737  0.20851636
 0.20848827 0.20854934 0.20874704 0.20888746 0.20899038 0.20902064
 0.20897263 0.20873699 0.20859091 0.2086041  0.20868683 0.208737
 0.20863244 0.20835061 0.20796874 0.2076671  0.2074534  0.20734242
 0.20731771 0.20729573 0.207146   0.20692547 0.2067483  0.20671892
 0.20682064 0.20689927 0.20680462 0.20659046 0.20636362 0.20620061
 0.20609298 0.20601262 0.206038   0.20622037 0.20671567 0.20741007
 0.2081645  0.20871082 0.2090149  0.20906813 0.2090921  0.20921063
 0.20941064 0.20958532 0.20958175 0.20944524 0.20923126 0.20912778
 0.20917805 0.20924863 0.20933914 0.20936649 0.2093516  0.20931014
 0.20932142 0.20930324 0.20925796 0.20927249 0.20940341 0.20974582
 0.21026121 0.21072239 0.21086013 0.21084528 0.21075359 0.21060027
 0.21040866 0.21024017 0.20999739 0.20970903 0.20940551 0.2091763
 0.20906423 0.20907868 0.20913249 0.20934473 0.2095675  0.20970674
 0.20977671 0.20980112 0.20981231 0.20979826 0.20975566 0.20966053
 0.20956267 0.20945042 0.20929879 0.20904607 0.2087447  0.20832293
 0.20796235 0.20781675 0.2078444  0.20793474 0.20807272 0.20818189
 0.2081902  0.20819691 0.20826119 0.20840347 0.20843035 0.20849895
 0.20857456 0.20849973 0.2082622  0.20807531 0.20784113 0.20772123
 0.20772451 0.20765676 0.20742123 0.20707189 0.20673458 0.2063064
 0.20591076 0.2055925  0.20532393 0.20513415 0.20501253 0.2048881
 0.20466863 0.20433742 0.20405613 0.20388529 0.20390566 0.20403928
 0.20408878 0.20399967 0.20393223 0.20413388 0.20456766 0.20540182
 0.20643613 0.2072431  0.20771688 0.20774789 0.2075284  0.20706902
 0.20661055 0.20607513 0.20559472 0.20527065 0.20504598 0.20492956
 0.20495082 0.20499243 0.20504576 0.20511152 0.20523514 0.20551234
 0.205799   0.20602822 0.20622346 0.20656589 0.2069218  0.20736347
 0.20761661 0.20766146 0.20751095 0.20731582 0.20716855 0.20694634
 0.20666568 0.20646226 0.20629095 0.20616736 0.2061633  0.20619689
 0.20619291 0.20618244 0.20602019 0.20577902 0.20557632 0.20553724
 0.20560674 0.20581198 0.2059987  0.20655517 0.20726207 0.20801477
 0.20860699 0.2087985  0.20858087 0.20831881 0.20822582 0.20817868
 0.20805512 0.20798884 0.20785171 0.20768799 0.20761614 0.20759583
 0.20761976 0.207581   0.20751037 0.20742628 0.20736179 0.20735031
 0.2074274  0.20752467 0.2077045  0.20809041 0.20865427 0.20948869
 0.2102623  0.21066995 0.21051587 0.2101852  0.20977116 0.20938711
 0.20914249 0.2090914  0.20914003 0.20919919 0.20922917 0.20922692
 0.20928468 0.20942968 0.20964605 0.2097891  0.20981032 0.2097932
 0.20979986 0.20984966 0.20989257 0.20980892 0.20984353 0.21001434
 0.21011797 0.21004304 0.2097022  0.20937198 0.2091263  0.20890635
 0.2087685  0.20868148 0.2084885  0.20825699 0.20797116 0.20777944
 0.20771196 0.20771267 0.20774049 0.20773605 0.20770353 0.20764993
 0.2076514  0.20773451 0.20798999 0.20849623 0.20931517 0.21031693
 0.21122293 0.21187669 0.21217346 0.21220535 0.21199173 0.21153867
 0.21102697 0.21059567 0.21024972 0.20992751 0.20966655 0.2095268
 0.20952283 0.20964527 0.20992483 0.21019976 0.21039997 0.21055304
 0.21070325 0.2109705  0.21137533 0.21182749 0.21232036 0.21266375
 0.21275665 0.21260671 0.21231563 0.21203104 0.21160357 0.21098407
 0.21043201 0.21017644 0.20999749 0.20983198 0.20966639 0.20964736
 0.20969366 0.20985325 0.21008533 0.21031645 0.21056804 0.21081075
 0.21092895 0.21094929 0.21084744 0.21072504 0.21061777 0.2106058
 0.2104736  0.21012849 0.2096174  0.20906061 0.20845203 0.20793965
 0.20745815 0.2069248  0.20640941 0.20589834 0.20556974 0.2054068
 0.2053361  0.2051178  0.20485973 0.20463975 0.2046896  0.20487714
 0.20510818 0.20523569 0.2053414  0.20541635 0.2055649  0.20562549
 0.20557989 0.2055785  0.20532367 0.20494886 0.20457003 0.20396127
 0.20340867 0.20300442 0.2027767  0.2026405  0.20256649 0.20245181
 0.20240732 0.20222469 0.20209764 0.20198604 0.2019191  0.20194614
 0.20203982 0.20208631 0.20209081 0.2020396  0.20193873 0.20187786
 0.20173028 0.20134251 0.20077917 0.2001736  0.19955596 0.19889995
 0.1983394  0.198019   0.19781734 0.19764955 0.19738169 0.19702518
 0.19681759 0.1967358  0.19668521 0.19668204 0.19657178 0.19637552
 0.19629093 0.19642597 0.19675745 0.19717751 0.19749175 0.19771822
 0.19771416 0.19732565 0.1969099  0.19657977 0.19617952 0.1956254
 0.19514921 0.19483021 0.19450885 0.19417353 0.1938088  0.1935529
 0.19357772 0.19361955 0.19366899 0.19361138 0.19342615 0.19335824
 0.19333301 0.19340265 0.19361608 0.19398941 0.19449666 0.19497396
 0.19521256 0.19519038 0.19480938 0.1945882  0.19414108 0.19362356
 0.19316335 0.19285356 0.19261867 0.19227192 0.19191362 0.19162059
 0.19147521 0.19142696 0.19145352 0.19133663 0.19130939 0.19129854
 0.1912802  0.19128823 0.19141953 0.19161    0.19162151 0.19155577
 0.1912295  0.19051671 0.18955532 0.1885297  0.1876888  0.18696392
 0.18639965 0.18608052 0.18590799 0.18565944 0.18544643 0.18534343
 0.1853967  0.18534315 0.18525839 0.18530588 0.18532845 0.18546505
 0.18564302 0.1858751  0.18616876 0.18658522 0.1869935  0.18725343
 0.18745142 0.18725283 0.18667963 0.1861415  0.18574947 0.18539445
 0.18522112 0.18512757 0.18490091 0.18439306 0.18384449 0.18339492
 0.18314335 0.18283619 0.18269494 0.1824584  0.18240793 0.1823228
 0.18226181 0.18207897 0.18222553 0.18276177 0.18292324 0.17962995]
