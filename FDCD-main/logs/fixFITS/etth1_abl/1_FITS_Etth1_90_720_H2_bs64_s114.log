Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.6343543529510498
Epoch: 1, Steps: 61 | Train Loss: 1.5937327 Vali Loss: 3.0295093 Test Loss: 1.6273364
Validation loss decreased (inf --> 3.029509).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3731179237365723
Epoch: 2, Steps: 61 | Train Loss: 1.2073283 Vali Loss: 2.4327686 Test Loss: 1.1481129
Validation loss decreased (3.029509 --> 2.432769).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2897160053253174
Epoch: 3, Steps: 61 | Train Loss: 0.9898611 Vali Loss: 2.1169755 Test Loss: 0.8920476
Validation loss decreased (2.432769 --> 2.116976).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4495079517364502
Epoch: 4, Steps: 61 | Train Loss: 0.8627532 Vali Loss: 1.9227241 Test Loss: 0.7438905
Validation loss decreased (2.116976 --> 1.922724).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.2750847339630127
Epoch: 5, Steps: 61 | Train Loss: 0.7841863 Vali Loss: 1.8140049 Test Loss: 0.6543911
Validation loss decreased (1.922724 --> 1.814005).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.2024331092834473
Epoch: 6, Steps: 61 | Train Loss: 0.7336985 Vali Loss: 1.7435250 Test Loss: 0.5977508
Validation loss decreased (1.814005 --> 1.743525).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.171889305114746
Epoch: 7, Steps: 61 | Train Loss: 0.7005435 Vali Loss: 1.6933572 Test Loss: 0.5608255
Validation loss decreased (1.743525 --> 1.693357).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2122776508331299
Epoch: 8, Steps: 61 | Train Loss: 0.6776227 Vali Loss: 1.6708682 Test Loss: 0.5360556
Validation loss decreased (1.693357 --> 1.670868).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3221752643585205
Epoch: 9, Steps: 61 | Train Loss: 0.6617789 Vali Loss: 1.6386497 Test Loss: 0.5190133
Validation loss decreased (1.670868 --> 1.638650).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.246100902557373
Epoch: 10, Steps: 61 | Train Loss: 0.6504896 Vali Loss: 1.6237519 Test Loss: 0.5070479
Validation loss decreased (1.638650 --> 1.623752).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.3031611442565918
Epoch: 11, Steps: 61 | Train Loss: 0.6427145 Vali Loss: 1.6219119 Test Loss: 0.4985509
Validation loss decreased (1.623752 --> 1.621912).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.246809720993042
Epoch: 12, Steps: 61 | Train Loss: 0.6364508 Vali Loss: 1.6030040 Test Loss: 0.4924081
Validation loss decreased (1.621912 --> 1.603004).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.1754097938537598
Epoch: 13, Steps: 61 | Train Loss: 0.6323191 Vali Loss: 1.6013372 Test Loss: 0.4879298
Validation loss decreased (1.603004 --> 1.601337).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.1959800720214844
Epoch: 14, Steps: 61 | Train Loss: 0.6285526 Vali Loss: 1.5956430 Test Loss: 0.4846044
Validation loss decreased (1.601337 --> 1.595643).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1618363857269287
Epoch: 15, Steps: 61 | Train Loss: 0.6260258 Vali Loss: 1.5940022 Test Loss: 0.4820654
Validation loss decreased (1.595643 --> 1.594002).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.1842350959777832
Epoch: 16, Steps: 61 | Train Loss: 0.6238638 Vali Loss: 1.5987750 Test Loss: 0.4801288
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.1761980056762695
Epoch: 17, Steps: 61 | Train Loss: 0.6226529 Vali Loss: 1.5911913 Test Loss: 0.4787127
Validation loss decreased (1.594002 --> 1.591191).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.1798791885375977
Epoch: 18, Steps: 61 | Train Loss: 0.6212959 Vali Loss: 1.5853388 Test Loss: 0.4775523
Validation loss decreased (1.591191 --> 1.585339).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.1733834743499756
Epoch: 19, Steps: 61 | Train Loss: 0.6203969 Vali Loss: 1.5884161 Test Loss: 0.4766665
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3380506038665771
Epoch: 20, Steps: 61 | Train Loss: 0.6193055 Vali Loss: 1.5856709 Test Loss: 0.4759262
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.2903666496276855
Epoch: 21, Steps: 61 | Train Loss: 0.6187840 Vali Loss: 1.5865014 Test Loss: 0.4753936
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.371755599975586
Epoch: 22, Steps: 61 | Train Loss: 0.6182499 Vali Loss: 1.5868707 Test Loss: 0.4749708
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3646812438964844
Epoch: 23, Steps: 61 | Train Loss: 0.6177816 Vali Loss: 1.5734427 Test Loss: 0.4745966
Validation loss decreased (1.585339 --> 1.573443).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.3481168746948242
Epoch: 24, Steps: 61 | Train Loss: 0.6168632 Vali Loss: 1.5753967 Test Loss: 0.4743472
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.1074130535125732
Epoch: 25, Steps: 61 | Train Loss: 0.6168453 Vali Loss: 1.5834448 Test Loss: 0.4741412
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2256672382354736
Epoch: 26, Steps: 61 | Train Loss: 0.6162252 Vali Loss: 1.5789698 Test Loss: 0.4739430
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.3130922317504883
Epoch: 27, Steps: 61 | Train Loss: 0.6164826 Vali Loss: 1.5725274 Test Loss: 0.4737965
Validation loss decreased (1.573443 --> 1.572527).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.8220901489257812
Epoch: 28, Steps: 61 | Train Loss: 0.6158237 Vali Loss: 1.5796216 Test Loss: 0.4736448
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.4472451210021973
Epoch: 29, Steps: 61 | Train Loss: 0.6157958 Vali Loss: 1.5804549 Test Loss: 0.4735732
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.3895883560180664
Epoch: 30, Steps: 61 | Train Loss: 0.6154413 Vali Loss: 1.5762430 Test Loss: 0.4735018
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.4103868007659912
Epoch: 31, Steps: 61 | Train Loss: 0.6151895 Vali Loss: 1.5760679 Test Loss: 0.4734271
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.348660945892334
Epoch: 32, Steps: 61 | Train Loss: 0.6153362 Vali Loss: 1.5767432 Test Loss: 0.4733567
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.2864248752593994
Epoch: 33, Steps: 61 | Train Loss: 0.6149768 Vali Loss: 1.5802315 Test Loss: 0.4733045
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.241051197052002
Epoch: 34, Steps: 61 | Train Loss: 0.6148199 Vali Loss: 1.5767710 Test Loss: 0.4733018
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.3631970882415771
Epoch: 35, Steps: 61 | Train Loss: 0.6149015 Vali Loss: 1.5753015 Test Loss: 0.4732473
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.2885925769805908
Epoch: 36, Steps: 61 | Train Loss: 0.6149427 Vali Loss: 1.5779574 Test Loss: 0.4732304
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.28743314743042
Epoch: 37, Steps: 61 | Train Loss: 0.6144266 Vali Loss: 1.5677088 Test Loss: 0.4732141
Validation loss decreased (1.572527 --> 1.567709).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.3803906440734863
Epoch: 38, Steps: 61 | Train Loss: 0.6146396 Vali Loss: 1.5723809 Test Loss: 0.4732148
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3335955142974854
Epoch: 39, Steps: 61 | Train Loss: 0.6144555 Vali Loss: 1.5734015 Test Loss: 0.4731833
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.3533899784088135
Epoch: 40, Steps: 61 | Train Loss: 0.6143806 Vali Loss: 1.5701847 Test Loss: 0.4731688
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.3929965496063232
Epoch: 41, Steps: 61 | Train Loss: 0.6147532 Vali Loss: 1.5667540 Test Loss: 0.4731829
Validation loss decreased (1.567709 --> 1.566754).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.2036497592926025
Epoch: 42, Steps: 61 | Train Loss: 0.6143022 Vali Loss: 1.5761509 Test Loss: 0.4731775
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.3168933391571045
Epoch: 43, Steps: 61 | Train Loss: 0.6141006 Vali Loss: 1.5723231 Test Loss: 0.4731643
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.2994849681854248
Epoch: 44, Steps: 61 | Train Loss: 0.6142141 Vali Loss: 1.5795985 Test Loss: 0.4731788
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.2560248374938965
Epoch: 45, Steps: 61 | Train Loss: 0.6142150 Vali Loss: 1.5806830 Test Loss: 0.4731753
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.194033145904541
Epoch: 46, Steps: 61 | Train Loss: 0.6141569 Vali Loss: 1.5752181 Test Loss: 0.4731886
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.3921103477478027
Epoch: 47, Steps: 61 | Train Loss: 0.6140119 Vali Loss: 1.5756438 Test Loss: 0.4731910
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.346163272857666
Epoch: 48, Steps: 61 | Train Loss: 0.6137269 Vali Loss: 1.5712997 Test Loss: 0.4731853
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.2838692665100098
Epoch: 49, Steps: 61 | Train Loss: 0.6139358 Vali Loss: 1.5663438 Test Loss: 0.4731825
Validation loss decreased (1.566754 --> 1.566344).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.227660894393921
Epoch: 50, Steps: 61 | Train Loss: 0.6140814 Vali Loss: 1.5832170 Test Loss: 0.4731955
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.2075598239898682
Epoch: 51, Steps: 61 | Train Loss: 0.6135849 Vali Loss: 1.5777907 Test Loss: 0.4731970
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3169817924499512
Epoch: 52, Steps: 61 | Train Loss: 0.6137411 Vali Loss: 1.5807678 Test Loss: 0.4731852
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.2592473030090332
Epoch: 53, Steps: 61 | Train Loss: 0.6138478 Vali Loss: 1.5766466 Test Loss: 0.4732084
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.2359375953674316
Epoch: 54, Steps: 61 | Train Loss: 0.6139762 Vali Loss: 1.5769746 Test Loss: 0.4732035
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.3588533401489258
Epoch: 55, Steps: 61 | Train Loss: 0.6137257 Vali Loss: 1.5689076 Test Loss: 0.4732127
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.0448358058929443
Epoch: 56, Steps: 61 | Train Loss: 0.6140138 Vali Loss: 1.5696504 Test Loss: 0.4732091
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.6369216442108154
Epoch: 57, Steps: 61 | Train Loss: 0.6138870 Vali Loss: 1.5588479 Test Loss: 0.4732262
Validation loss decreased (1.566344 --> 1.558848).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.4823312759399414
Epoch: 58, Steps: 61 | Train Loss: 0.6139194 Vali Loss: 1.5762023 Test Loss: 0.4732192
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.2421319484710693
Epoch: 59, Steps: 61 | Train Loss: 0.6138461 Vali Loss: 1.5709286 Test Loss: 0.4732215
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.4495117664337158
Epoch: 60, Steps: 61 | Train Loss: 0.6138520 Vali Loss: 1.5722668 Test Loss: 0.4732335
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.5618689060211182
Epoch: 61, Steps: 61 | Train Loss: 0.6134504 Vali Loss: 1.5782390 Test Loss: 0.4732426
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.3274133205413818
Epoch: 62, Steps: 61 | Train Loss: 0.6135641 Vali Loss: 1.5704892 Test Loss: 0.4732305
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.391172170639038
Epoch: 63, Steps: 61 | Train Loss: 0.6135457 Vali Loss: 1.5752841 Test Loss: 0.4732401
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.302609920501709
Epoch: 64, Steps: 61 | Train Loss: 0.6136371 Vali Loss: 1.5693831 Test Loss: 0.4732347
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.324747085571289
Epoch: 65, Steps: 61 | Train Loss: 0.6137546 Vali Loss: 1.5741533 Test Loss: 0.4732415
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.8680627346038818
Epoch: 66, Steps: 61 | Train Loss: 0.6134345 Vali Loss: 1.5781977 Test Loss: 0.4732442
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.2247438430786133
Epoch: 67, Steps: 61 | Train Loss: 0.6134581 Vali Loss: 1.5725063 Test Loss: 0.4732555
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.36435866355896
Epoch: 68, Steps: 61 | Train Loss: 0.6139475 Vali Loss: 1.5741348 Test Loss: 0.4732520
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.2488374710083008
Epoch: 69, Steps: 61 | Train Loss: 0.6136051 Vali Loss: 1.5672903 Test Loss: 0.4732574
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.35298490524292
Epoch: 70, Steps: 61 | Train Loss: 0.6132743 Vali Loss: 1.5674050 Test Loss: 0.4732629
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.2851650714874268
Epoch: 71, Steps: 61 | Train Loss: 0.6135121 Vali Loss: 1.5674852 Test Loss: 0.4732628
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.4735710620880127
Epoch: 72, Steps: 61 | Train Loss: 0.6135759 Vali Loss: 1.5730207 Test Loss: 0.4732704
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.381291151046753
Epoch: 73, Steps: 61 | Train Loss: 0.6136893 Vali Loss: 1.5704753 Test Loss: 0.4732691
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.152771234512329
Epoch: 74, Steps: 61 | Train Loss: 0.6134080 Vali Loss: 1.5740210 Test Loss: 0.4732709
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.22078537940979
Epoch: 75, Steps: 61 | Train Loss: 0.6134238 Vali Loss: 1.5807564 Test Loss: 0.4732754
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.3454749584197998
Epoch: 76, Steps: 61 | Train Loss: 0.6134791 Vali Loss: 1.5757432 Test Loss: 0.4732770
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.3252918720245361
Epoch: 77, Steps: 61 | Train Loss: 0.6134762 Vali Loss: 1.5743732 Test Loss: 0.4732789
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4719685912132263, mae:0.463664174079895, rse:0.6576706767082214, corr:[0.21397644 0.22809692 0.23153782 0.2282933  0.22375149 0.2214953
 0.22223358 0.22267638 0.22222213 0.22163299 0.22068475 0.2198646
 0.219621   0.21998312 0.2205691  0.22073007 0.2204368  0.22025777
 0.2205541  0.22137755 0.22180332 0.22147325 0.22044942 0.2193661
 0.21839899 0.21865678 0.21915403 0.2187346  0.21762979 0.21741433
 0.21834232 0.21894144 0.2188236  0.21827582 0.21763256 0.21743168
 0.21765359 0.21786512 0.21824062 0.21852237 0.21870974 0.21864179
 0.21858065 0.21899049 0.2196381  0.21976696 0.2192518  0.21811941
 0.2164783  0.21601224 0.2160681  0.21537264 0.21373662 0.21241042
 0.21256046 0.21270373 0.21260542 0.21271946 0.212701   0.21256974
 0.21214207 0.2115953  0.21149501 0.21183534 0.21239643 0.21265897
 0.21259648 0.21255188 0.21261968 0.212653   0.21215987 0.2108845
 0.20888492 0.20798604 0.20794414 0.20806172 0.20768453 0.20735687
 0.20780508 0.20796405 0.20788799 0.20796673 0.20810302 0.2083016
 0.20826538 0.20776875 0.20736453 0.20742978 0.20787615 0.20822018
 0.20804986 0.20787096 0.20796783 0.20825388 0.20862588 0.20858583
 0.20783618 0.20770797 0.20804565 0.2080455  0.20795359 0.2081607
 0.20874712 0.20882532 0.20840555 0.20805734 0.20795679 0.20807086
 0.20815402 0.20780447 0.2074856  0.20736997 0.20768921 0.20792812
 0.20797515 0.2079305  0.20799929 0.20795007 0.20758104 0.20681411
 0.20553845 0.20471345 0.20385131 0.20285174 0.20227209 0.2025777
 0.20383698 0.20453875 0.20448123 0.20416123 0.20385943 0.20391096
 0.20402522 0.20379117 0.2036037  0.20357855 0.20380661 0.20391276
 0.20386341 0.2039509  0.20409185 0.20410343 0.2038291  0.20317365
 0.20205729 0.20158136 0.20134412 0.20024061 0.1989007  0.19865182
 0.1997401  0.20047286 0.2007013  0.20069928 0.20054275 0.20050427
 0.20048396 0.2002414  0.19999737 0.19991924 0.20004685 0.20017794
 0.20025058 0.20045929 0.20072576 0.20063345 0.20000468 0.19889088
 0.19776897 0.19800907 0.1988631  0.19887526 0.19764875 0.19689178
 0.19760501 0.1981429  0.19822313 0.19824328 0.19829574 0.19834226
 0.19824065 0.19782512 0.19758627 0.19763902 0.19796823 0.19833875
 0.19847606 0.19858305 0.19892542 0.19925724 0.19918394 0.19830294
 0.19647262 0.19561048 0.19533023 0.19470286 0.19370134 0.19328813
 0.19379222 0.19421835 0.19429031 0.19435714 0.19437973 0.19448057
 0.19431518 0.19376585 0.19330074 0.19326212 0.1936085  0.19378328
 0.19361264 0.19351779 0.19361141 0.1936543  0.19340728 0.19273828
 0.19162995 0.19131021 0.19146875 0.19146432 0.1913912  0.192117
 0.19367996 0.19465776 0.19506496 0.19519733 0.19506784 0.19501892
 0.1950624  0.1947281  0.19432022 0.1941634  0.19425716 0.19426519
 0.1941014  0.1940123  0.19410802 0.19414285 0.19392681 0.19324581
 0.19220525 0.19180979 0.19170383 0.19135846 0.1908929  0.19117375
 0.19255583 0.19347978 0.19379236 0.19361176 0.19323172 0.19316907
 0.19326437 0.19308135 0.19277975 0.19244058 0.19236709 0.19254144
 0.19281925 0.19291744 0.19283223 0.19252893 0.19206858 0.19135258
 0.19020885 0.19001977 0.19041869 0.19066928 0.19052699 0.19100656
 0.1923781  0.1933952  0.19378477 0.19379039 0.19353426 0.19332606
 0.19317791 0.19300945 0.19284658 0.19282201 0.19286925 0.19294658
 0.19304174 0.19336593 0.19380021 0.1939809  0.19391434 0.19349563
 0.19297847 0.19352719 0.19494277 0.1956143  0.19526102 0.19531815
 0.19631408 0.19692586 0.19716904 0.19710344 0.19671828 0.19630514
 0.19589984 0.19542225 0.19514439 0.19527769 0.19558933 0.1956331
 0.19545831 0.19540331 0.19554897 0.19570147 0.19549146 0.1946249
 0.19345142 0.19314106 0.19348484 0.19341768 0.19290622 0.19265026
 0.19336955 0.19371669 0.19359112 0.19338167 0.19316146 0.19303897
 0.19290252 0.19248065 0.19203703 0.19200201 0.19219404 0.19231465
 0.1922327  0.1921258  0.19210312 0.19211279 0.19194381 0.1912874
 0.19006696 0.18944286 0.18907943 0.18870701 0.1884669  0.18900599
 0.19042957 0.19133621 0.19171482 0.19179961 0.19182009 0.19186737
 0.19174905 0.19133803 0.19099401 0.19089948 0.19097017 0.19112228
 0.19117482 0.19111232 0.19086301 0.19051072 0.18987046 0.18889844
 0.1877836  0.18741825 0.18721774 0.18631501 0.18530704 0.18510786
 0.18646166 0.1874562  0.1879301  0.18792571 0.18763705 0.18743959
 0.18734403 0.18698335 0.1865877  0.18630812 0.18632539 0.1865077
 0.18666424 0.18681043 0.18687661 0.18682317 0.18647912 0.18610229
 0.18566181 0.18615194 0.18686937 0.18698634 0.18664537 0.18666144
 0.18796022 0.18883409 0.18904574 0.18895708 0.18878987 0.18877864
 0.18873864 0.18863684 0.18864505 0.18871492 0.18890575 0.18921624
 0.18945561 0.18976623 0.19010028 0.1903238  0.19027187 0.18977802
 0.18878707 0.18877973 0.18950273 0.1898035  0.18968612 0.18996738
 0.19129394 0.19205672 0.19233866 0.19242562 0.19249313 0.19253404
 0.19238067 0.1921089  0.191869   0.19204956 0.19246778 0.19273645
 0.19271867 0.19280171 0.19290777 0.19316898 0.19315456 0.19271448
 0.191816   0.19173455 0.19226573 0.19257122 0.19258863 0.19319509
 0.19460851 0.19550745 0.1957852  0.19580148 0.19577326 0.19584873
 0.19579294 0.19528975 0.19479686 0.194727   0.19500625 0.19526559
 0.19531472 0.19524013 0.19522116 0.19533099 0.19541055 0.19550113
 0.19528642 0.19565582 0.19602482 0.19596662 0.19566733 0.19586115
 0.19692373 0.19748506 0.19767281 0.1977858  0.19795181 0.19830479
 0.19857483 0.19858865 0.19849746 0.1985687  0.1989095  0.19923212
 0.19937775 0.19946486 0.19945495 0.19914119 0.19865404 0.19789079
 0.1967658  0.1964018  0.19638631 0.19625254 0.19606358 0.19650094
 0.19797733 0.19864583 0.19871569 0.19850494 0.19817045 0.19799624
 0.19791293 0.1977527  0.19764233 0.19765711 0.19772978 0.19772387
 0.19775173 0.19799615 0.19836536 0.19869076 0.19888762 0.19876127
 0.19828457 0.1989005  0.19997728 0.20028366 0.19975178 0.19963306
 0.20063171 0.20109533 0.201174   0.20101406 0.20080662 0.20075774
 0.2007007  0.2005322  0.20063108 0.20102185 0.2014569  0.20159239
 0.20155782 0.20179868 0.2022681  0.2027411  0.20301192 0.2026338
 0.20140554 0.20090283 0.20112427 0.20129757 0.20089433 0.20077494
 0.20161778 0.20217119 0.20227078 0.20226915 0.20233195 0.20254743
 0.20247659 0.20207605 0.20181718 0.2020088  0.20250498 0.20275477
 0.2025737  0.2023828  0.2023111  0.20223337 0.20192994 0.20109512
 0.19960149 0.19884954 0.19891973 0.19864887 0.19793226 0.19774908
 0.19857804 0.1988805  0.19876122 0.19847523 0.19832583 0.19851154
 0.19863969 0.19826175 0.19776323 0.19756687 0.19784221 0.19820738
 0.19836088 0.19831668 0.1982783  0.19819286 0.19798122 0.19722368
 0.19580413 0.19538437 0.19555837 0.19555643 0.1952203  0.19519132
 0.19630724 0.19695587 0.1969392  0.19681767 0.19670603 0.1967659
 0.19668299 0.19629663 0.19594924 0.19586205 0.19600984 0.1960848
 0.19603997 0.1960536  0.19606172 0.19591871 0.19542773 0.19444425
 0.19286425 0.19220407 0.19224061 0.19190669 0.1912551  0.1911894
 0.1920598  0.19241105 0.19230027 0.1921948  0.19214323 0.19215532
 0.19207409 0.19192886 0.19193111 0.19216219 0.1923115  0.19214389
 0.1918646  0.1920106  0.19240199 0.19272824 0.19261831 0.19167833
 0.19002406 0.18937185 0.18929982 0.18897153 0.18809125 0.18772054
 0.18862997 0.1890253  0.18878248 0.18854292 0.18840131 0.18844995
 0.18834573 0.18783338 0.18750279 0.187563   0.18766028 0.18745635
 0.18692322 0.18679409 0.18714878 0.18769823 0.18789679 0.18729196
 0.1856801  0.18487686 0.18513812 0.18559717 0.18520552 0.18473661
 0.18545541 0.1856328  0.18542235 0.18520914 0.18512668 0.18505642
 0.18468125 0.18383431 0.18329297 0.1834818  0.18399668 0.18408616
 0.18345124 0.18278661 0.18266238 0.18305393 0.1832742  0.1822693
 0.17949343 0.17688635 0.17550144 0.17442274 0.1729001  0.1717536
 0.17153095 0.1710564  0.17038545 0.16980419 0.16976155 0.17030668
 0.17060575 0.16997547 0.16881216 0.16817987 0.16849524 0.1692032
 0.16942312 0.16906202 0.16853893 0.16846788 0.16885234 0.16878918
 0.16724263 0.16582386 0.16489454 0.16409189 0.16370912 0.16412637
 0.16528696 0.16519432 0.16442049 0.16365167 0.16310905 0.16308215
 0.1631795  0.16231844 0.16058183 0.15873289 0.15836109 0.15958981
 0.16088983 0.16012357 0.15799402 0.15820856 0.16236061 0.16041334]
