Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=34, out_features=97, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2955008.0
params:  3395.0
Trainable parameters:  3395
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.6246159076690674
Epoch: 1, Steps: 63 | Train Loss: 0.8341578 Vali Loss: 2.0021706 Test Loss: 0.9788758
Validation loss decreased (inf --> 2.002171).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.611055850982666
Epoch: 2, Steps: 63 | Train Loss: 0.6497045 Vali Loss: 1.7490118 Test Loss: 0.8059953
Validation loss decreased (2.002171 --> 1.749012).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.4738423824310303
Epoch: 3, Steps: 63 | Train Loss: 0.5487939 Vali Loss: 1.6094205 Test Loss: 0.7073815
Validation loss decreased (1.749012 --> 1.609421).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.659512996673584
Epoch: 4, Steps: 63 | Train Loss: 0.4868572 Vali Loss: 1.5174661 Test Loss: 0.6442520
Validation loss decreased (1.609421 --> 1.517466).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.6513822078704834
Epoch: 5, Steps: 63 | Train Loss: 0.4466844 Vali Loss: 1.4530150 Test Loss: 0.6008517
Validation loss decreased (1.517466 --> 1.453015).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.470207691192627
Epoch: 6, Steps: 63 | Train Loss: 0.4189997 Vali Loss: 1.4142226 Test Loss: 0.5708960
Validation loss decreased (1.453015 --> 1.414223).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.6412560939788818
Epoch: 7, Steps: 63 | Train Loss: 0.3997278 Vali Loss: 1.3771040 Test Loss: 0.5490816
Validation loss decreased (1.414223 --> 1.377104).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.7589235305786133
Epoch: 8, Steps: 63 | Train Loss: 0.3857485 Vali Loss: 1.3541517 Test Loss: 0.5331862
Validation loss decreased (1.377104 --> 1.354152).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7741787433624268
Epoch: 9, Steps: 63 | Train Loss: 0.3753470 Vali Loss: 1.3369398 Test Loss: 0.5211869
Validation loss decreased (1.354152 --> 1.336940).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.8287055492401123
Epoch: 10, Steps: 63 | Train Loss: 0.3676167 Vali Loss: 1.3167531 Test Loss: 0.5121148
Validation loss decreased (1.336940 --> 1.316753).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.700639009475708
Epoch: 11, Steps: 63 | Train Loss: 0.3614110 Vali Loss: 1.3115410 Test Loss: 0.5051178
Validation loss decreased (1.316753 --> 1.311541).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.709803819656372
Epoch: 12, Steps: 63 | Train Loss: 0.3566755 Vali Loss: 1.3021545 Test Loss: 0.4993810
Validation loss decreased (1.311541 --> 1.302155).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9028968811035156
Epoch: 13, Steps: 63 | Train Loss: 0.3528248 Vali Loss: 1.2983276 Test Loss: 0.4948494
Validation loss decreased (1.302155 --> 1.298328).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.4105885028839111
Epoch: 14, Steps: 63 | Train Loss: 0.3494427 Vali Loss: 1.2906204 Test Loss: 0.4911629
Validation loss decreased (1.298328 --> 1.290620).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.593008041381836
Epoch: 15, Steps: 63 | Train Loss: 0.3467457 Vali Loss: 1.2891921 Test Loss: 0.4880172
Validation loss decreased (1.290620 --> 1.289192).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.5506291389465332
Epoch: 16, Steps: 63 | Train Loss: 0.3449164 Vali Loss: 1.2853333 Test Loss: 0.4853940
Validation loss decreased (1.289192 --> 1.285333).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4523072242736816
Epoch: 17, Steps: 63 | Train Loss: 0.3428851 Vali Loss: 1.2768799 Test Loss: 0.4831533
Validation loss decreased (1.285333 --> 1.276880).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8637328147888184
Epoch: 18, Steps: 63 | Train Loss: 0.3408483 Vali Loss: 1.2712443 Test Loss: 0.4811693
Validation loss decreased (1.276880 --> 1.271244).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4870226383209229
Epoch: 19, Steps: 63 | Train Loss: 0.3395037 Vali Loss: 1.2702787 Test Loss: 0.4795043
Validation loss decreased (1.271244 --> 1.270279).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.0012989044189453
Epoch: 20, Steps: 63 | Train Loss: 0.3385663 Vali Loss: 1.2705810 Test Loss: 0.4779491
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4083631038665771
Epoch: 21, Steps: 63 | Train Loss: 0.3369911 Vali Loss: 1.2632668 Test Loss: 0.4765676
Validation loss decreased (1.270279 --> 1.263267).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.420583963394165
Epoch: 22, Steps: 63 | Train Loss: 0.3362503 Vali Loss: 1.2660404 Test Loss: 0.4753525
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.4931087493896484
Epoch: 23, Steps: 63 | Train Loss: 0.3352425 Vali Loss: 1.2678900 Test Loss: 0.4742979
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4721322059631348
Epoch: 24, Steps: 63 | Train Loss: 0.3346655 Vali Loss: 1.2617800 Test Loss: 0.4732789
Validation loss decreased (1.263267 --> 1.261780).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.5191941261291504
Epoch: 25, Steps: 63 | Train Loss: 0.3337203 Vali Loss: 1.2677642 Test Loss: 0.4724413
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.0069892406463623
Epoch: 26, Steps: 63 | Train Loss: 0.3332598 Vali Loss: 1.2614444 Test Loss: 0.4715731
Validation loss decreased (1.261780 --> 1.261444).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4026556015014648
Epoch: 27, Steps: 63 | Train Loss: 0.3324953 Vali Loss: 1.2674892 Test Loss: 0.4708187
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.541917324066162
Epoch: 28, Steps: 63 | Train Loss: 0.3320275 Vali Loss: 1.2588947 Test Loss: 0.4701434
Validation loss decreased (1.261444 --> 1.258895).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6837127208709717
Epoch: 29, Steps: 63 | Train Loss: 0.3313398 Vali Loss: 1.2557427 Test Loss: 0.4695252
Validation loss decreased (1.258895 --> 1.255743).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.3624041080474854
Epoch: 30, Steps: 63 | Train Loss: 0.3309103 Vali Loss: 1.2585188 Test Loss: 0.4689520
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.4464449882507324
Epoch: 31, Steps: 63 | Train Loss: 0.3305447 Vali Loss: 1.2553313 Test Loss: 0.4684027
Validation loss decreased (1.255743 --> 1.255331).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.06036376953125
Epoch: 32, Steps: 63 | Train Loss: 0.3302146 Vali Loss: 1.2526947 Test Loss: 0.4679121
Validation loss decreased (1.255331 --> 1.252695).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.9856865406036377
Epoch: 33, Steps: 63 | Train Loss: 0.3297349 Vali Loss: 1.2565756 Test Loss: 0.4674564
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5163242816925049
Epoch: 34, Steps: 63 | Train Loss: 0.3292177 Vali Loss: 1.2544492 Test Loss: 0.4670164
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9614877700805664
Epoch: 35, Steps: 63 | Train Loss: 0.3290932 Vali Loss: 1.2508477 Test Loss: 0.4666220
Validation loss decreased (1.252695 --> 1.250848).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6300075054168701
Epoch: 36, Steps: 63 | Train Loss: 0.3287983 Vali Loss: 1.2529868 Test Loss: 0.4662584
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5106229782104492
Epoch: 37, Steps: 63 | Train Loss: 0.3283547 Vali Loss: 1.2577688 Test Loss: 0.4659068
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4520900249481201
Epoch: 38, Steps: 63 | Train Loss: 0.3280562 Vali Loss: 1.2482822 Test Loss: 0.4656176
Validation loss decreased (1.250848 --> 1.248282).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7386558055877686
Epoch: 39, Steps: 63 | Train Loss: 0.3278898 Vali Loss: 1.2506870 Test Loss: 0.4652864
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.4789080619812012
Epoch: 40, Steps: 63 | Train Loss: 0.3275172 Vali Loss: 1.2512792 Test Loss: 0.4650198
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.8931002616882324
Epoch: 41, Steps: 63 | Train Loss: 0.3273600 Vali Loss: 1.2528446 Test Loss: 0.4647769
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.4575555324554443
Epoch: 42, Steps: 63 | Train Loss: 0.3272421 Vali Loss: 1.2550416 Test Loss: 0.4645402
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.6959071159362793
Epoch: 43, Steps: 63 | Train Loss: 0.3272106 Vali Loss: 1.2476135 Test Loss: 0.4643121
Validation loss decreased (1.248282 --> 1.247614).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.1607656478881836
Epoch: 44, Steps: 63 | Train Loss: 0.3266406 Vali Loss: 1.2512627 Test Loss: 0.4641069
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5180602073669434
Epoch: 45, Steps: 63 | Train Loss: 0.3267338 Vali Loss: 1.2499253 Test Loss: 0.4638906
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.5612258911132812
Epoch: 46, Steps: 63 | Train Loss: 0.3264469 Vali Loss: 1.2534716 Test Loss: 0.4637067
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.2983202934265137
Epoch: 47, Steps: 63 | Train Loss: 0.3261827 Vali Loss: 1.2543318 Test Loss: 0.4635239
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.8845834732055664
Epoch: 48, Steps: 63 | Train Loss: 0.3261163 Vali Loss: 1.2527622 Test Loss: 0.4633659
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.4696564674377441
Epoch: 49, Steps: 63 | Train Loss: 0.3262092 Vali Loss: 1.2536559 Test Loss: 0.4632081
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.51741361618042
Epoch: 50, Steps: 63 | Train Loss: 0.3262623 Vali Loss: 1.2516607 Test Loss: 0.4630685
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9317853450775146
Epoch: 51, Steps: 63 | Train Loss: 0.3258344 Vali Loss: 1.2480403 Test Loss: 0.4629367
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3855221271514893
Epoch: 52, Steps: 63 | Train Loss: 0.3255027 Vali Loss: 1.2465026 Test Loss: 0.4628003
Validation loss decreased (1.247614 --> 1.246503).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.3724820613861084
Epoch: 53, Steps: 63 | Train Loss: 0.3254272 Vali Loss: 1.2432981 Test Loss: 0.4626804
Validation loss decreased (1.246503 --> 1.243298).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.6538610458374023
Epoch: 54, Steps: 63 | Train Loss: 0.3252413 Vali Loss: 1.2433999 Test Loss: 0.4625601
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.4878220558166504
Epoch: 55, Steps: 63 | Train Loss: 0.3251785 Vali Loss: 1.2485697 Test Loss: 0.4624593
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.6554527282714844
Epoch: 56, Steps: 63 | Train Loss: 0.3253979 Vali Loss: 1.2496496 Test Loss: 0.4623624
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.4547145366668701
Epoch: 57, Steps: 63 | Train Loss: 0.3253616 Vali Loss: 1.2467555 Test Loss: 0.4622627
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.4452903270721436
Epoch: 58, Steps: 63 | Train Loss: 0.3251693 Vali Loss: 1.2474977 Test Loss: 0.4621595
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.5984106063842773
Epoch: 59, Steps: 63 | Train Loss: 0.3250738 Vali Loss: 1.2438298 Test Loss: 0.4620774
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.5008177757263184
Epoch: 60, Steps: 63 | Train Loss: 0.3250537 Vali Loss: 1.2450391 Test Loss: 0.4620024
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6275012493133545
Epoch: 61, Steps: 63 | Train Loss: 0.3250415 Vali Loss: 1.2449380 Test Loss: 0.4619266
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.5189692974090576
Epoch: 62, Steps: 63 | Train Loss: 0.3249316 Vali Loss: 1.2469181 Test Loss: 0.4618517
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.4813003540039062
Epoch: 63, Steps: 63 | Train Loss: 0.3246357 Vali Loss: 1.2511622 Test Loss: 0.4617844
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.6685106754302979
Epoch: 64, Steps: 63 | Train Loss: 0.3246903 Vali Loss: 1.2440156 Test Loss: 0.4617167
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.517517328262329
Epoch: 65, Steps: 63 | Train Loss: 0.3244618 Vali Loss: 1.2474636 Test Loss: 0.4616597
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.8335773944854736
Epoch: 66, Steps: 63 | Train Loss: 0.3244328 Vali Loss: 1.2456261 Test Loss: 0.4616078
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.6341733932495117
Epoch: 67, Steps: 63 | Train Loss: 0.3242811 Vali Loss: 1.2465693 Test Loss: 0.4615439
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.6840789318084717
Epoch: 68, Steps: 63 | Train Loss: 0.3243036 Vali Loss: 1.2438927 Test Loss: 0.4614954
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.7133045196533203
Epoch: 69, Steps: 63 | Train Loss: 0.3243808 Vali Loss: 1.2473829 Test Loss: 0.4614490
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.478827953338623
Epoch: 70, Steps: 63 | Train Loss: 0.3245191 Vali Loss: 1.2471571 Test Loss: 0.4614109
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.8416693210601807
Epoch: 71, Steps: 63 | Train Loss: 0.3241408 Vali Loss: 1.2457868 Test Loss: 0.4613602
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.8332486152648926
Epoch: 72, Steps: 63 | Train Loss: 0.3240914 Vali Loss: 1.2457806 Test Loss: 0.4613212
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.044032573699951
Epoch: 73, Steps: 63 | Train Loss: 0.3241799 Vali Loss: 1.2449459 Test Loss: 0.4612820
EarlyStopping counter: 20 out of 20
Early stopping
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=34, out_features=97, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2955008.0
params:  3395.0
Trainable parameters:  3395
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.5168671607971191
Epoch: 1, Steps: 63 | Train Loss: 0.4747692 Vali Loss: 1.2430464 Test Loss: 0.4578280
Validation loss decreased (inf --> 1.243046).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8561427593231201
Epoch: 2, Steps: 63 | Train Loss: 0.4718068 Vali Loss: 1.2363496 Test Loss: 0.4564033
Validation loss decreased (1.243046 --> 1.236350).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.682722568511963
Epoch: 3, Steps: 63 | Train Loss: 0.4700811 Vali Loss: 1.2375416 Test Loss: 0.4567798
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.183380365371704
Epoch: 4, Steps: 63 | Train Loss: 0.4694694 Vali Loss: 1.2350887 Test Loss: 0.4568605
Validation loss decreased (1.236350 --> 1.235089).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.6415352821350098
Epoch: 5, Steps: 63 | Train Loss: 0.4693435 Vali Loss: 1.2326567 Test Loss: 0.4573482
Validation loss decreased (1.235089 --> 1.232657).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.5034441947937012
Epoch: 6, Steps: 63 | Train Loss: 0.4692198 Vali Loss: 1.2324404 Test Loss: 0.4576226
Validation loss decreased (1.232657 --> 1.232440).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4974360466003418
Epoch: 7, Steps: 63 | Train Loss: 0.4693126 Vali Loss: 1.2320062 Test Loss: 0.4579992
Validation loss decreased (1.232440 --> 1.232006).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5030696392059326
Epoch: 8, Steps: 63 | Train Loss: 0.4689406 Vali Loss: 1.2312778 Test Loss: 0.4581607
Validation loss decreased (1.232006 --> 1.231278).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.6221277713775635
Epoch: 9, Steps: 63 | Train Loss: 0.4697208 Vali Loss: 1.2330552 Test Loss: 0.4582430
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.5234742164611816
Epoch: 10, Steps: 63 | Train Loss: 0.4684101 Vali Loss: 1.2275833 Test Loss: 0.4585572
Validation loss decreased (1.231278 --> 1.227583).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.3990874290466309
Epoch: 11, Steps: 63 | Train Loss: 0.4688637 Vali Loss: 1.2301818 Test Loss: 0.4584538
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.505786657333374
Epoch: 12, Steps: 63 | Train Loss: 0.4688839 Vali Loss: 1.2311792 Test Loss: 0.4587145
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5727050304412842
Epoch: 13, Steps: 63 | Train Loss: 0.4688270 Vali Loss: 1.2352942 Test Loss: 0.4586280
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5528247356414795
Epoch: 14, Steps: 63 | Train Loss: 0.4685088 Vali Loss: 1.2314779 Test Loss: 0.4587523
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5768654346466064
Epoch: 15, Steps: 63 | Train Loss: 0.4687381 Vali Loss: 1.2267196 Test Loss: 0.4588346
Validation loss decreased (1.227583 --> 1.226720).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4864559173583984
Epoch: 16, Steps: 63 | Train Loss: 0.4687504 Vali Loss: 1.2283580 Test Loss: 0.4586591
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4884421825408936
Epoch: 17, Steps: 63 | Train Loss: 0.4684911 Vali Loss: 1.2237613 Test Loss: 0.4588608
Validation loss decreased (1.226720 --> 1.223761).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4405741691589355
Epoch: 18, Steps: 63 | Train Loss: 0.4683734 Vali Loss: 1.2322983 Test Loss: 0.4588961
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9840846061706543
Epoch: 19, Steps: 63 | Train Loss: 0.4681207 Vali Loss: 1.2385408 Test Loss: 0.4589748
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5164105892181396
Epoch: 20, Steps: 63 | Train Loss: 0.4681676 Vali Loss: 1.2301202 Test Loss: 0.4589997
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4031567573547363
Epoch: 21, Steps: 63 | Train Loss: 0.4684783 Vali Loss: 1.2293640 Test Loss: 0.4589268
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.462636947631836
Epoch: 22, Steps: 63 | Train Loss: 0.4682930 Vali Loss: 1.2333885 Test Loss: 0.4589263
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6897847652435303
Epoch: 23, Steps: 63 | Train Loss: 0.4685830 Vali Loss: 1.2273136 Test Loss: 0.4589346
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.5285608768463135
Epoch: 24, Steps: 63 | Train Loss: 0.4685938 Vali Loss: 1.2302796 Test Loss: 0.4590153
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.779310941696167
Epoch: 25, Steps: 63 | Train Loss: 0.4682617 Vali Loss: 1.2289538 Test Loss: 0.4589654
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5806760787963867
Epoch: 26, Steps: 63 | Train Loss: 0.4685020 Vali Loss: 1.2331718 Test Loss: 0.4590202
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.742971658706665
Epoch: 27, Steps: 63 | Train Loss: 0.4685363 Vali Loss: 1.2302507 Test Loss: 0.4589635
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5319807529449463
Epoch: 28, Steps: 63 | Train Loss: 0.4683213 Vali Loss: 1.2268381 Test Loss: 0.4590136
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.8080565929412842
Epoch: 29, Steps: 63 | Train Loss: 0.4684778 Vali Loss: 1.2252477 Test Loss: 0.4590229
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.753284215927124
Epoch: 30, Steps: 63 | Train Loss: 0.4680889 Vali Loss: 1.2259005 Test Loss: 0.4590723
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5494558811187744
Epoch: 31, Steps: 63 | Train Loss: 0.4684178 Vali Loss: 1.2344489 Test Loss: 0.4590384
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.4223315715789795
Epoch: 32, Steps: 63 | Train Loss: 0.4687519 Vali Loss: 1.2276274 Test Loss: 0.4590293
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.3909039497375488
Epoch: 33, Steps: 63 | Train Loss: 0.4682969 Vali Loss: 1.2306055 Test Loss: 0.4590835
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5034410953521729
Epoch: 34, Steps: 63 | Train Loss: 0.4683142 Vali Loss: 1.2302756 Test Loss: 0.4591083
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.4611201286315918
Epoch: 35, Steps: 63 | Train Loss: 0.4682228 Vali Loss: 1.2304926 Test Loss: 0.4590707
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.5390686988830566
Epoch: 36, Steps: 63 | Train Loss: 0.4679702 Vali Loss: 1.2333275 Test Loss: 0.4590138
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5094280242919922
Epoch: 37, Steps: 63 | Train Loss: 0.4686137 Vali Loss: 1.2326275 Test Loss: 0.4590616
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4581330418586731, mae:0.4376721978187561, rse:0.6443885564804077, corr:[0.2548343  0.2582696  0.25843453 0.25604555 0.25286674 0.2505519
 0.24990772 0.24995746 0.24950963 0.24925977 0.24947801 0.24988405
 0.2497466  0.2488324  0.2482822  0.24858278 0.24895981 0.24875887
 0.2480922  0.2474954  0.24752153 0.2480888  0.2484822  0.24795918
 0.24673618 0.24619238 0.24607304 0.24577327 0.24493471 0.2440923
 0.24375856 0.24382578 0.24369839 0.24330068 0.2431019  0.24371941
 0.24458657 0.24464479 0.24436873 0.24432482 0.24467526 0.24500719
 0.24489345 0.24476175 0.24526834 0.24603057 0.24657299 0.24602148
 0.24431913 0.24310769 0.24198702 0.24074115 0.23927516 0.23795456
 0.23734958 0.23738116 0.23734803 0.23730859 0.23718809 0.23779032
 0.23847127 0.23835519 0.23797046 0.23787244 0.23815396 0.23843093
 0.23839569 0.23804457 0.23799428 0.23826368 0.2384624  0.23776293
 0.2362641  0.23528652 0.23472749 0.23434375 0.23386328 0.23323987
 0.23295233 0.23303084 0.23297536 0.2326946  0.23227079 0.23242004
 0.23302871 0.23300764 0.23265927 0.23242871 0.23237051 0.23233286
 0.23204756 0.23176494 0.23200548 0.23273776 0.23345515 0.23344028
 0.23252325 0.23193042 0.23167723 0.23118342 0.23048662 0.22985636
 0.22964448 0.2300382  0.23036072 0.23041713 0.23025374 0.23056157
 0.23104739 0.230918   0.23060165 0.23055254 0.2306685  0.23071447
 0.23058994 0.230491   0.23070598 0.23107491 0.23119213 0.2305623
 0.2291324  0.22798458 0.22690624 0.22582303 0.2247933  0.22419252
 0.2242354  0.22477216 0.22494869 0.22482103 0.22473891 0.22538173
 0.22629993 0.22632465 0.22606292 0.22601925 0.22628763 0.22667737
 0.22683385 0.22681549 0.2269527  0.227231   0.22733524 0.22670926
 0.22536743 0.22446322 0.22381848 0.22275172 0.22168888 0.22097276
 0.22097209 0.22136976 0.22172014 0.22189161 0.2219457  0.22242205
 0.22305502 0.22301744 0.22279455 0.22267836 0.22259195 0.2225665
 0.22241552 0.22230363 0.22253637 0.2229526  0.22311778 0.2225131
 0.22129816 0.22056518 0.22008875 0.21948299 0.21889398 0.21854647
 0.21874374 0.21941808 0.21983838 0.2201069  0.22037746 0.22122437
 0.22229321 0.22246552 0.22218016 0.22206204 0.22220972 0.222535
 0.22268844 0.22272012 0.22300436 0.22347091 0.22364004 0.22302397
 0.22169134 0.22102632 0.22070448 0.22014464 0.21929245 0.21848327
 0.2182141  0.2184673  0.21882129 0.21904919 0.21904466 0.2195376
 0.22018223 0.22008991 0.21977413 0.21959321 0.21954608 0.2195078
 0.2192851  0.21900463 0.21911447 0.219505   0.21978925 0.21944652
 0.21853593 0.21800695 0.21759751 0.21693069 0.21630321 0.21582443
 0.21596561 0.21642993 0.21666858 0.21646973 0.21613413 0.21651419
 0.21740824 0.21750492 0.2173118  0.21737856 0.21751098 0.21747713
 0.21715192 0.21682112 0.2169603  0.21744354 0.21776453 0.21729125
 0.21619207 0.21554035 0.21515779 0.21466045 0.21405919 0.21366237
 0.21379368 0.21428241 0.21479495 0.21521327 0.21552478 0.21617496
 0.21695407 0.21704403 0.21703292 0.21719608 0.21745546 0.21760988
 0.21755995 0.21730822 0.21728104 0.21748897 0.21767916 0.21730989
 0.21642682 0.21602315 0.21570373 0.21506912 0.2142555  0.21365094
 0.21372496 0.21427886 0.2145335  0.21443316 0.21427697 0.21468157
 0.21567361 0.21582624 0.2154463  0.21504764 0.21486227 0.21490026
 0.21489681 0.21479702 0.21505144 0.21556729 0.21597958 0.21570767
 0.21475893 0.21438096 0.21440311 0.21429321 0.2139799  0.21399853
 0.21441112 0.21490644 0.21512505 0.21516252 0.21516742 0.21566895
 0.21647619 0.21650404 0.21636549 0.21647228 0.21673626 0.21685617
 0.21674283 0.21663198 0.21687591 0.21722844 0.21739382 0.21688768
 0.21579772 0.21546209 0.21544152 0.2148558  0.21406825 0.2132596
 0.21309234 0.21339004 0.21350072 0.21346985 0.21358156 0.21455364
 0.21600774 0.21597727 0.21518804 0.21515857 0.21588226 0.21652664
 0.21615098 0.21492054 0.21392953 0.21437915 0.2154234  0.2130585 ]
