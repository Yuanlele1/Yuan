Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_192_FITS_ETTh1_ftM_sl180_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8269
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=42, out_features=86, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3236352.0
params:  3698.0
Trainable parameters:  3698
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9991912841796875
Epoch: 1, Steps: 64 | Train Loss: 0.6806855 Vali Loss: 1.3133116 Test Loss: 0.6609091
Validation loss decreased (inf --> 1.313312).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.7641935348510742
Epoch: 2, Steps: 64 | Train Loss: 0.5266499 Vali Loss: 1.1526432 Test Loss: 0.5457169
Validation loss decreased (1.313312 --> 1.152643).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.794234037399292
Epoch: 3, Steps: 64 | Train Loss: 0.4736343 Vali Loss: 1.0928510 Test Loss: 0.5037081
Validation loss decreased (1.152643 --> 1.092851).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6718473434448242
Epoch: 4, Steps: 64 | Train Loss: 0.4515930 Vali Loss: 1.0638421 Test Loss: 0.4828899
Validation loss decreased (1.092851 --> 1.063842).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.874753475189209
Epoch: 5, Steps: 64 | Train Loss: 0.4399234 Vali Loss: 1.0457991 Test Loss: 0.4703575
Validation loss decreased (1.063842 --> 1.045799).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.6370985507965088
Epoch: 6, Steps: 64 | Train Loss: 0.4329484 Vali Loss: 1.0333784 Test Loss: 0.4614032
Validation loss decreased (1.045799 --> 1.033378).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.8602969646453857
Epoch: 7, Steps: 64 | Train Loss: 0.4273260 Vali Loss: 1.0242319 Test Loss: 0.4548641
Validation loss decreased (1.033378 --> 1.024232).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.6972014904022217
Epoch: 8, Steps: 64 | Train Loss: 0.4233212 Vali Loss: 1.0164822 Test Loss: 0.4500723
Validation loss decreased (1.024232 --> 1.016482).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.040821075439453
Epoch: 9, Steps: 64 | Train Loss: 0.4202401 Vali Loss: 1.0107206 Test Loss: 0.4461308
Validation loss decreased (1.016482 --> 1.010721).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.4386277198791504
Epoch: 10, Steps: 64 | Train Loss: 0.4178646 Vali Loss: 1.0061128 Test Loss: 0.4432180
Validation loss decreased (1.010721 --> 1.006113).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.353020429611206
Epoch: 11, Steps: 64 | Train Loss: 0.4162939 Vali Loss: 1.0022300 Test Loss: 0.4410407
Validation loss decreased (1.006113 --> 1.002230).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9264674186706543
Epoch: 12, Steps: 64 | Train Loss: 0.4150246 Vali Loss: 0.9991060 Test Loss: 0.4394588
Validation loss decreased (1.002230 --> 0.999106).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.036815881729126
Epoch: 13, Steps: 64 | Train Loss: 0.4132881 Vali Loss: 0.9958073 Test Loss: 0.4379059
Validation loss decreased (0.999106 --> 0.995807).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.407952308654785
Epoch: 14, Steps: 64 | Train Loss: 0.4125775 Vali Loss: 0.9935666 Test Loss: 0.4368885
Validation loss decreased (0.995807 --> 0.993567).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.600816488265991
Epoch: 15, Steps: 64 | Train Loss: 0.4117460 Vali Loss: 0.9916593 Test Loss: 0.4362004
Validation loss decreased (0.993567 --> 0.991659).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.5999374389648438
Epoch: 16, Steps: 64 | Train Loss: 0.4107374 Vali Loss: 0.9904331 Test Loss: 0.4355460
Validation loss decreased (0.991659 --> 0.990433).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.471846342086792
Epoch: 17, Steps: 64 | Train Loss: 0.4104462 Vali Loss: 0.9884021 Test Loss: 0.4349219
Validation loss decreased (0.990433 --> 0.988402).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8467469215393066
Epoch: 18, Steps: 64 | Train Loss: 0.4100291 Vali Loss: 0.9877675 Test Loss: 0.4348181
Validation loss decreased (0.988402 --> 0.987767).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9482460021972656
Epoch: 19, Steps: 64 | Train Loss: 0.4096599 Vali Loss: 0.9867190 Test Loss: 0.4346753
Validation loss decreased (0.987767 --> 0.986719).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.6562762260437012
Epoch: 20, Steps: 64 | Train Loss: 0.4096798 Vali Loss: 0.9857796 Test Loss: 0.4343967
Validation loss decreased (0.986719 --> 0.985780).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0453670024871826
Epoch: 21, Steps: 64 | Train Loss: 0.4094443 Vali Loss: 0.9845346 Test Loss: 0.4341308
Validation loss decreased (0.985780 --> 0.984535).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.743948221206665
Epoch: 22, Steps: 64 | Train Loss: 0.4088815 Vali Loss: 0.9840057 Test Loss: 0.4338826
Validation loss decreased (0.984535 --> 0.984006).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.9232637882232666
Epoch: 23, Steps: 64 | Train Loss: 0.4084497 Vali Loss: 0.9837558 Test Loss: 0.4337799
Validation loss decreased (0.984006 --> 0.983756).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.532414674758911
Epoch: 24, Steps: 64 | Train Loss: 0.4086132 Vali Loss: 0.9832179 Test Loss: 0.4337191
Validation loss decreased (0.983756 --> 0.983218).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.677114248275757
Epoch: 25, Steps: 64 | Train Loss: 0.4085716 Vali Loss: 0.9825667 Test Loss: 0.4337170
Validation loss decreased (0.983218 --> 0.982567).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.276062488555908
Epoch: 26, Steps: 64 | Train Loss: 0.4080634 Vali Loss: 0.9820684 Test Loss: 0.4335605
Validation loss decreased (0.982567 --> 0.982068).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.3396706581115723
Epoch: 27, Steps: 64 | Train Loss: 0.4079860 Vali Loss: 0.9813271 Test Loss: 0.4336544
Validation loss decreased (0.982068 --> 0.981327).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.5972142219543457
Epoch: 28, Steps: 64 | Train Loss: 0.4079297 Vali Loss: 0.9813026 Test Loss: 0.4335715
Validation loss decreased (0.981327 --> 0.981303).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.0319693088531494
Epoch: 29, Steps: 64 | Train Loss: 0.4079418 Vali Loss: 0.9808663 Test Loss: 0.4336457
Validation loss decreased (0.981303 --> 0.980866).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.2117021083831787
Epoch: 30, Steps: 64 | Train Loss: 0.4077606 Vali Loss: 0.9800858 Test Loss: 0.4336037
Validation loss decreased (0.980866 --> 0.980086).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.108061075210571
Epoch: 31, Steps: 64 | Train Loss: 0.4074563 Vali Loss: 0.9803568 Test Loss: 0.4336084
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.783716917037964
Epoch: 32, Steps: 64 | Train Loss: 0.4075950 Vali Loss: 0.9799371 Test Loss: 0.4335769
Validation loss decreased (0.980086 --> 0.979937).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.307772159576416
Epoch: 33, Steps: 64 | Train Loss: 0.4079501 Vali Loss: 0.9797617 Test Loss: 0.4336633
Validation loss decreased (0.979937 --> 0.979762).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.94526743888855
Epoch: 34, Steps: 64 | Train Loss: 0.4077335 Vali Loss: 0.9796872 Test Loss: 0.4336963
Validation loss decreased (0.979762 --> 0.979687).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.810077428817749
Epoch: 35, Steps: 64 | Train Loss: 0.4072025 Vali Loss: 0.9792716 Test Loss: 0.4337285
Validation loss decreased (0.979687 --> 0.979272).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.509547233581543
Epoch: 36, Steps: 64 | Train Loss: 0.4075612 Vali Loss: 0.9791688 Test Loss: 0.4336780
Validation loss decreased (0.979272 --> 0.979169).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.910132646560669
Epoch: 37, Steps: 64 | Train Loss: 0.4075321 Vali Loss: 0.9789904 Test Loss: 0.4336325
Validation loss decreased (0.979169 --> 0.978990).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.8454737663269043
Epoch: 38, Steps: 64 | Train Loss: 0.4077468 Vali Loss: 0.9787884 Test Loss: 0.4336191
Validation loss decreased (0.978990 --> 0.978788).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.8822453022003174
Epoch: 39, Steps: 64 | Train Loss: 0.4070592 Vali Loss: 0.9786906 Test Loss: 0.4336388
Validation loss decreased (0.978788 --> 0.978691).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.658529281616211
Epoch: 40, Steps: 64 | Train Loss: 0.4075826 Vali Loss: 0.9783930 Test Loss: 0.4336071
Validation loss decreased (0.978691 --> 0.978393).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6924514770507812
Epoch: 41, Steps: 64 | Train Loss: 0.4075029 Vali Loss: 0.9778203 Test Loss: 0.4336028
Validation loss decreased (0.978393 --> 0.977820).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.4861414432525635
Epoch: 42, Steps: 64 | Train Loss: 0.4067934 Vali Loss: 0.9779951 Test Loss: 0.4336739
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.1704113483428955
Epoch: 43, Steps: 64 | Train Loss: 0.4074922 Vali Loss: 0.9779354 Test Loss: 0.4335680
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.4278032779693604
Epoch: 44, Steps: 64 | Train Loss: 0.4076222 Vali Loss: 0.9774898 Test Loss: 0.4336754
Validation loss decreased (0.977820 --> 0.977490).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.8660435676574707
Epoch: 45, Steps: 64 | Train Loss: 0.4070886 Vali Loss: 0.9778430 Test Loss: 0.4336495
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.9035122394561768
Epoch: 46, Steps: 64 | Train Loss: 0.4071859 Vali Loss: 0.9765344 Test Loss: 0.4337120
Validation loss decreased (0.977490 --> 0.976534).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.5898830890655518
Epoch: 47, Steps: 64 | Train Loss: 0.4070134 Vali Loss: 0.9777542 Test Loss: 0.4336657
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.6801583766937256
Epoch: 48, Steps: 64 | Train Loss: 0.4068678 Vali Loss: 0.9771745 Test Loss: 0.4336877
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9511713981628418
Epoch: 49, Steps: 64 | Train Loss: 0.4074501 Vali Loss: 0.9772304 Test Loss: 0.4336959
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.373267650604248
Epoch: 50, Steps: 64 | Train Loss: 0.4073446 Vali Loss: 0.9772046 Test Loss: 0.4337347
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.253675699234009
Epoch: 51, Steps: 64 | Train Loss: 0.4069539 Vali Loss: 0.9772994 Test Loss: 0.4336693
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.7476489543914795
Epoch: 52, Steps: 64 | Train Loss: 0.4070280 Vali Loss: 0.9774274 Test Loss: 0.4337400
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.24969220161438
Epoch: 53, Steps: 64 | Train Loss: 0.4073452 Vali Loss: 0.9773182 Test Loss: 0.4337113
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.7398498058319092
Epoch: 54, Steps: 64 | Train Loss: 0.4070367 Vali Loss: 0.9771525 Test Loss: 0.4336879
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.1511847972869873
Epoch: 55, Steps: 64 | Train Loss: 0.4071368 Vali Loss: 0.9768701 Test Loss: 0.4336756
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.2362778186798096
Epoch: 56, Steps: 64 | Train Loss: 0.4072856 Vali Loss: 0.9768755 Test Loss: 0.4336815
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.9746921062469482
Epoch: 57, Steps: 64 | Train Loss: 0.4069809 Vali Loss: 0.9768437 Test Loss: 0.4336808
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.7113933563232422
Epoch: 58, Steps: 64 | Train Loss: 0.4068389 Vali Loss: 0.9770914 Test Loss: 0.4336745
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.172978639602661
Epoch: 59, Steps: 64 | Train Loss: 0.4066265 Vali Loss: 0.9768030 Test Loss: 0.4336812
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.9085445404052734
Epoch: 60, Steps: 64 | Train Loss: 0.4068452 Vali Loss: 0.9765527 Test Loss: 0.4337208
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.9762547016143799
Epoch: 61, Steps: 64 | Train Loss: 0.4066835 Vali Loss: 0.9765240 Test Loss: 0.4337377
Validation loss decreased (0.976534 --> 0.976524).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.9874064922332764
Epoch: 62, Steps: 64 | Train Loss: 0.4066758 Vali Loss: 0.9768141 Test Loss: 0.4337005
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.6553020477294922
Epoch: 63, Steps: 64 | Train Loss: 0.4068490 Vali Loss: 0.9767956 Test Loss: 0.4337283
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.7923731803894043
Epoch: 64, Steps: 64 | Train Loss: 0.4073111 Vali Loss: 0.9764819 Test Loss: 0.4337163
Validation loss decreased (0.976524 --> 0.976482).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.6965854167938232
Epoch: 65, Steps: 64 | Train Loss: 0.4065619 Vali Loss: 0.9764108 Test Loss: 0.4337133
Validation loss decreased (0.976482 --> 0.976411).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.4528515338897705
Epoch: 66, Steps: 64 | Train Loss: 0.4072979 Vali Loss: 0.9766350 Test Loss: 0.4337149
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.8367762565612793
Epoch: 67, Steps: 64 | Train Loss: 0.4066989 Vali Loss: 0.9765123 Test Loss: 0.4337204
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.7380242347717285
Epoch: 68, Steps: 64 | Train Loss: 0.4071071 Vali Loss: 0.9765077 Test Loss: 0.4337275
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.2772958278656006
Epoch: 69, Steps: 64 | Train Loss: 0.4072181 Vali Loss: 0.9767016 Test Loss: 0.4337056
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.4755618572235107
Epoch: 70, Steps: 64 | Train Loss: 0.4069180 Vali Loss: 0.9762963 Test Loss: 0.4337566
Validation loss decreased (0.976411 --> 0.976296).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.6092207431793213
Epoch: 71, Steps: 64 | Train Loss: 0.4071275 Vali Loss: 0.9766208 Test Loss: 0.4337179
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.9202854633331299
Epoch: 72, Steps: 64 | Train Loss: 0.4071153 Vali Loss: 0.9763802 Test Loss: 0.4337387
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.7581441402435303
Epoch: 73, Steps: 64 | Train Loss: 0.4071144 Vali Loss: 0.9762140 Test Loss: 0.4337144
Validation loss decreased (0.976296 --> 0.976214).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.5203757286071777
Epoch: 74, Steps: 64 | Train Loss: 0.4067607 Vali Loss: 0.9765652 Test Loss: 0.4337298
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.9373493194580078
Epoch: 75, Steps: 64 | Train Loss: 0.4066054 Vali Loss: 0.9765220 Test Loss: 0.4337199
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.3470840454101562
Epoch: 76, Steps: 64 | Train Loss: 0.4074397 Vali Loss: 0.9765882 Test Loss: 0.4337285
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 4.403628587722778
Epoch: 77, Steps: 64 | Train Loss: 0.4065948 Vali Loss: 0.9764645 Test Loss: 0.4337347
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.8707332611083984
Epoch: 78, Steps: 64 | Train Loss: 0.4070389 Vali Loss: 0.9757907 Test Loss: 0.4337275
Validation loss decreased (0.976214 --> 0.975791).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.577066659927368
Epoch: 79, Steps: 64 | Train Loss: 0.4070023 Vali Loss: 0.9763475 Test Loss: 0.4337244
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.776895523071289
Epoch: 80, Steps: 64 | Train Loss: 0.4068855 Vali Loss: 0.9760923 Test Loss: 0.4337090
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.2011594772338867
Epoch: 81, Steps: 64 | Train Loss: 0.4070965 Vali Loss: 0.9762359 Test Loss: 0.4337294
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.7734556198120117
Epoch: 82, Steps: 64 | Train Loss: 0.4072278 Vali Loss: 0.9751857 Test Loss: 0.4337270
Validation loss decreased (0.975791 --> 0.975186).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 4.136869668960571
Epoch: 83, Steps: 64 | Train Loss: 0.4072531 Vali Loss: 0.9762706 Test Loss: 0.4337272
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.442781925201416
Epoch: 84, Steps: 64 | Train Loss: 0.4072639 Vali Loss: 0.9763222 Test Loss: 0.4337340
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.0278053283691406
Epoch: 85, Steps: 64 | Train Loss: 0.4065853 Vali Loss: 0.9764535 Test Loss: 0.4337262
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.3328747749328613
Epoch: 86, Steps: 64 | Train Loss: 0.4069671 Vali Loss: 0.9762748 Test Loss: 0.4337277
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.4420740604400635
Epoch: 87, Steps: 64 | Train Loss: 0.4067658 Vali Loss: 0.9763081 Test Loss: 0.4337275
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.02720308303833
Epoch: 88, Steps: 64 | Train Loss: 0.4073979 Vali Loss: 0.9762936 Test Loss: 0.4337383
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.8107502460479736
Epoch: 89, Steps: 64 | Train Loss: 0.4068550 Vali Loss: 0.9760617 Test Loss: 0.4337371
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.6637814044952393
Epoch: 90, Steps: 64 | Train Loss: 0.4072461 Vali Loss: 0.9762856 Test Loss: 0.4337267
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.979642391204834
Epoch: 91, Steps: 64 | Train Loss: 0.4070303 Vali Loss: 0.9763542 Test Loss: 0.4337403
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.6737477779388428
Epoch: 92, Steps: 64 | Train Loss: 0.4070436 Vali Loss: 0.9763390 Test Loss: 0.4337430
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.6081256866455078
Epoch: 93, Steps: 64 | Train Loss: 0.4062723 Vali Loss: 0.9760482 Test Loss: 0.4337328
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.6195027828216553
Epoch: 94, Steps: 64 | Train Loss: 0.4068881 Vali Loss: 0.9758098 Test Loss: 0.4337393
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.7280395030975342
Epoch: 95, Steps: 64 | Train Loss: 0.4067606 Vali Loss: 0.9762444 Test Loss: 0.4337373
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.3283188343048096
Epoch: 96, Steps: 64 | Train Loss: 0.4065241 Vali Loss: 0.9761406 Test Loss: 0.4337426
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.892014503479004
Epoch: 97, Steps: 64 | Train Loss: 0.4067640 Vali Loss: 0.9758818 Test Loss: 0.4337395
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.7143356800079346
Epoch: 98, Steps: 64 | Train Loss: 0.4068080 Vali Loss: 0.9762831 Test Loss: 0.4337398
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.995955228805542
Epoch: 99, Steps: 64 | Train Loss: 0.4069170 Vali Loss: 0.9762679 Test Loss: 0.4337425
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.7305583953857422
Epoch: 100, Steps: 64 | Train Loss: 0.4066757 Vali Loss: 0.9761024 Test Loss: 0.4337409
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_180_192_FITS_ETTh1_ftM_sl180_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.42760521173477173, mae:0.42131564021110535, rse:0.620981752872467, corr:[0.26377016 0.26816067 0.26795718 0.26711988 0.26566043 0.2634175
 0.26189393 0.26211783 0.26299074 0.26321167 0.2623316  0.26180953
 0.26202545 0.2615686  0.2608887  0.26085067 0.26125124 0.26137707
 0.2611162  0.26086077 0.26087055 0.2610994  0.26117277 0.26068696
 0.25972936 0.2590461  0.25844884 0.25803858 0.25772557 0.2575244
 0.2572206  0.2567176  0.25633073 0.25639838 0.25666326 0.25696924
 0.25708842 0.25702345 0.25705433 0.2572085  0.25745997 0.2578099
 0.2580238  0.25803053 0.25816935 0.2585379  0.25885233 0.25847772
 0.25710663 0.25575927 0.25424764 0.25282124 0.25168654 0.25080708
 0.25026944 0.24999477 0.24981989 0.24987884 0.24975556 0.2501647
 0.25071356 0.25054154 0.2502127  0.25007403 0.2501589  0.25032303
 0.25046617 0.25043607 0.25037074 0.2503593  0.25032675 0.24959095
 0.24818604 0.24708387 0.24617818 0.2456434  0.24546398 0.24519703
 0.24484822 0.24449602 0.24429366 0.24432983 0.24407475 0.243898
 0.24407445 0.24413446 0.24424699 0.24428594 0.24417387 0.24413732
 0.24410187 0.2439455  0.24396376 0.24430633 0.24473463 0.24466115
 0.24377611 0.24296647 0.24234037 0.24168941 0.24128863 0.24105258
 0.24087848 0.24096532 0.24105345 0.24114563 0.24104618 0.24125229
 0.24153432 0.24125613 0.24095935 0.24102722 0.24116    0.24119525
 0.24120823 0.24124746 0.24127942 0.24124497 0.24106246 0.24053697
 0.23956539 0.23868738 0.23745134 0.236215   0.23537898 0.23505092
 0.23497936 0.2350398  0.23507005 0.23524047 0.23531651 0.23575974
 0.23644015 0.23659246 0.23660776 0.23655285 0.23650402 0.23665334
 0.23683317 0.23684108 0.23685026 0.23700225 0.23706971 0.23653013
 0.23535125 0.2343529  0.23343298 0.23230462 0.23157907 0.23118655
 0.23110318 0.23112695 0.23136905 0.2316542  0.23173827 0.23222773
 0.23283641 0.23268239 0.23237208 0.2322582  0.23227845 0.23238163
 0.2322931  0.232039   0.2319834  0.23226261 0.23252337 0.23208557
 0.23095247 0.23016231 0.22940448 0.22854432 0.22802424 0.22780927
 0.22785394 0.22827213 0.2287732  0.22955279 0.23003758 0.23069444
 0.23177932 0.23214696 0.23198032 0.2314813  0.23080206 0.23074201
 0.23122327 0.23144042 0.23073593 0.22994947 0.23131923 0.2323668 ]
