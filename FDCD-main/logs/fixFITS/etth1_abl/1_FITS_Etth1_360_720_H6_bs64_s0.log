Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=0, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=106, out_features=318, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30202368.0
params:  34026.0
Trainable parameters:  34026
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.763538122177124
Epoch: 1, Steps: 59 | Train Loss: 0.9725946 Vali Loss: 1.9389850 Test Loss: 0.7669820
Validation loss decreased (inf --> 1.938985).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6878154277801514
Epoch: 2, Steps: 59 | Train Loss: 0.7377627 Vali Loss: 1.6962130 Test Loss: 0.5990623
Validation loss decreased (1.938985 --> 1.696213).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.7575602531433105
Epoch: 3, Steps: 59 | Train Loss: 0.6667470 Vali Loss: 1.6131651 Test Loss: 0.5413015
Validation loss decreased (1.696213 --> 1.613165).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.5855844020843506
Epoch: 4, Steps: 59 | Train Loss: 0.6382385 Vali Loss: 1.5705724 Test Loss: 0.5115970
Validation loss decreased (1.613165 --> 1.570572).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5972421169281006
Epoch: 5, Steps: 59 | Train Loss: 0.6219917 Vali Loss: 1.5503294 Test Loss: 0.4916799
Validation loss decreased (1.570572 --> 1.550329).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.646390438079834
Epoch: 6, Steps: 59 | Train Loss: 0.6105822 Vali Loss: 1.5231097 Test Loss: 0.4769852
Validation loss decreased (1.550329 --> 1.523110).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.6303772926330566
Epoch: 7, Steps: 59 | Train Loss: 0.6021698 Vali Loss: 1.5072144 Test Loss: 0.4655893
Validation loss decreased (1.523110 --> 1.507214).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.6715328693389893
Epoch: 8, Steps: 59 | Train Loss: 0.5956372 Vali Loss: 1.4999577 Test Loss: 0.4567690
Validation loss decreased (1.507214 --> 1.499958).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.6019270420074463
Epoch: 9, Steps: 59 | Train Loss: 0.5905275 Vali Loss: 1.4897074 Test Loss: 0.4497724
Validation loss decreased (1.499958 --> 1.489707).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6340301036834717
Epoch: 10, Steps: 59 | Train Loss: 0.5864730 Vali Loss: 1.4767468 Test Loss: 0.4444305
Validation loss decreased (1.489707 --> 1.476747).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.6062498092651367
Epoch: 11, Steps: 59 | Train Loss: 0.5831410 Vali Loss: 1.4716213 Test Loss: 0.4401162
Validation loss decreased (1.476747 --> 1.471621).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.5417578220367432
Epoch: 12, Steps: 59 | Train Loss: 0.5806037 Vali Loss: 1.4662695 Test Loss: 0.4367517
Validation loss decreased (1.471621 --> 1.466269).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6281847953796387
Epoch: 13, Steps: 59 | Train Loss: 0.5784531 Vali Loss: 1.4618065 Test Loss: 0.4341486
Validation loss decreased (1.466269 --> 1.461807).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6934001445770264
Epoch: 14, Steps: 59 | Train Loss: 0.5769791 Vali Loss: 1.4587593 Test Loss: 0.4320589
Validation loss decreased (1.461807 --> 1.458759).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6438243389129639
Epoch: 15, Steps: 59 | Train Loss: 0.5752833 Vali Loss: 1.4525681 Test Loss: 0.4303999
Validation loss decreased (1.458759 --> 1.452568).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.609410047531128
Epoch: 16, Steps: 59 | Train Loss: 0.5741448 Vali Loss: 1.4492331 Test Loss: 0.4291834
Validation loss decreased (1.452568 --> 1.449233).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.8378684520721436
Epoch: 17, Steps: 59 | Train Loss: 0.5734533 Vali Loss: 1.4498708 Test Loss: 0.4282264
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6551074981689453
Epoch: 18, Steps: 59 | Train Loss: 0.5726908 Vali Loss: 1.4497696 Test Loss: 0.4274161
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.5275475978851318
Epoch: 19, Steps: 59 | Train Loss: 0.5720317 Vali Loss: 1.4459183 Test Loss: 0.4268987
Validation loss decreased (1.449233 --> 1.445918).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.6391432285308838
Epoch: 20, Steps: 59 | Train Loss: 0.5715066 Vali Loss: 1.4380901 Test Loss: 0.4264583
Validation loss decreased (1.445918 --> 1.438090).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.6338624954223633
Epoch: 21, Steps: 59 | Train Loss: 0.5709941 Vali Loss: 1.4471956 Test Loss: 0.4261336
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.817436695098877
Epoch: 22, Steps: 59 | Train Loss: 0.5706110 Vali Loss: 1.4443756 Test Loss: 0.4258462
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6195344924926758
Epoch: 23, Steps: 59 | Train Loss: 0.5702815 Vali Loss: 1.4418793 Test Loss: 0.4256839
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6066234111785889
Epoch: 24, Steps: 59 | Train Loss: 0.5700570 Vali Loss: 1.4381067 Test Loss: 0.4255291
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.0085608959198
Epoch: 25, Steps: 59 | Train Loss: 0.5695421 Vali Loss: 1.4454843 Test Loss: 0.4254268
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5368468761444092
Epoch: 26, Steps: 59 | Train Loss: 0.5695989 Vali Loss: 1.4384364 Test Loss: 0.4252969
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6490697860717773
Epoch: 27, Steps: 59 | Train Loss: 0.5692622 Vali Loss: 1.4381241 Test Loss: 0.4252993
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.6134779453277588
Epoch: 28, Steps: 59 | Train Loss: 0.5693687 Vali Loss: 1.4407823 Test Loss: 0.4252778
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.0381102561950684
Epoch: 29, Steps: 59 | Train Loss: 0.5692956 Vali Loss: 1.4407749 Test Loss: 0.4252737
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.673414707183838
Epoch: 30, Steps: 59 | Train Loss: 0.5688103 Vali Loss: 1.4345526 Test Loss: 0.4252748
Validation loss decreased (1.438090 --> 1.434553).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5693233013153076
Epoch: 31, Steps: 59 | Train Loss: 0.5688960 Vali Loss: 1.4393764 Test Loss: 0.4252691
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.5653855800628662
Epoch: 32, Steps: 59 | Train Loss: 0.5689331 Vali Loss: 1.4402640 Test Loss: 0.4252901
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.612553358078003
Epoch: 33, Steps: 59 | Train Loss: 0.5688145 Vali Loss: 1.4402866 Test Loss: 0.4252684
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.8425874710083008
Epoch: 34, Steps: 59 | Train Loss: 0.5685966 Vali Loss: 1.4341974 Test Loss: 0.4253359
Validation loss decreased (1.434553 --> 1.434197).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.648280382156372
Epoch: 35, Steps: 59 | Train Loss: 0.5684602 Vali Loss: 1.4403970 Test Loss: 0.4253502
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.5685617923736572
Epoch: 36, Steps: 59 | Train Loss: 0.5683769 Vali Loss: 1.4378693 Test Loss: 0.4253761
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5712122917175293
Epoch: 37, Steps: 59 | Train Loss: 0.5684987 Vali Loss: 1.4387829 Test Loss: 0.4253924
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.6286394596099854
Epoch: 38, Steps: 59 | Train Loss: 0.5683369 Vali Loss: 1.4406004 Test Loss: 0.4254054
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.5890676975250244
Epoch: 39, Steps: 59 | Train Loss: 0.5682026 Vali Loss: 1.4330930 Test Loss: 0.4254630
Validation loss decreased (1.434197 --> 1.433093).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7284162044525146
Epoch: 40, Steps: 59 | Train Loss: 0.5684252 Vali Loss: 1.4426728 Test Loss: 0.4254509
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.526991367340088
Epoch: 41, Steps: 59 | Train Loss: 0.5682024 Vali Loss: 1.4422560 Test Loss: 0.4254771
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.6253666877746582
Epoch: 42, Steps: 59 | Train Loss: 0.5682263 Vali Loss: 1.4370198 Test Loss: 0.4255064
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.6111977100372314
Epoch: 43, Steps: 59 | Train Loss: 0.5681872 Vali Loss: 1.4358441 Test Loss: 0.4255210
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.5581753253936768
Epoch: 44, Steps: 59 | Train Loss: 0.5681786 Vali Loss: 1.4401881 Test Loss: 0.4255466
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5157523155212402
Epoch: 45, Steps: 59 | Train Loss: 0.5683358 Vali Loss: 1.4325585 Test Loss: 0.4255785
Validation loss decreased (1.433093 --> 1.432559).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6666526794433594
Epoch: 46, Steps: 59 | Train Loss: 0.5681387 Vali Loss: 1.4346058 Test Loss: 0.4255798
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.6035382747650146
Epoch: 47, Steps: 59 | Train Loss: 0.5683149 Vali Loss: 1.4403307 Test Loss: 0.4256080
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.548346757888794
Epoch: 48, Steps: 59 | Train Loss: 0.5680225 Vali Loss: 1.4357438 Test Loss: 0.4256237
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.4796757698059082
Epoch: 49, Steps: 59 | Train Loss: 0.5678681 Vali Loss: 1.4350666 Test Loss: 0.4256411
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.542464017868042
Epoch: 50, Steps: 59 | Train Loss: 0.5680411 Vali Loss: 1.4387610 Test Loss: 0.4256433
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6190600395202637
Epoch: 51, Steps: 59 | Train Loss: 0.5680436 Vali Loss: 1.4371161 Test Loss: 0.4256650
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6170728206634521
Epoch: 52, Steps: 59 | Train Loss: 0.5681396 Vali Loss: 1.4348919 Test Loss: 0.4256828
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.6905202865600586
Epoch: 53, Steps: 59 | Train Loss: 0.5681020 Vali Loss: 1.4400029 Test Loss: 0.4257006
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.5616509914398193
Epoch: 54, Steps: 59 | Train Loss: 0.5679225 Vali Loss: 1.4381428 Test Loss: 0.4257033
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.6795270442962646
Epoch: 55, Steps: 59 | Train Loss: 0.5679851 Vali Loss: 1.4391160 Test Loss: 0.4257202
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.5675079822540283
Epoch: 56, Steps: 59 | Train Loss: 0.5679695 Vali Loss: 1.4388847 Test Loss: 0.4257340
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.677729845046997
Epoch: 57, Steps: 59 | Train Loss: 0.5680609 Vali Loss: 1.4346515 Test Loss: 0.4257320
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.6119718551635742
Epoch: 58, Steps: 59 | Train Loss: 0.5678948 Vali Loss: 1.4375278 Test Loss: 0.4257463
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.4895524978637695
Epoch: 59, Steps: 59 | Train Loss: 0.5679128 Vali Loss: 1.4358768 Test Loss: 0.4257649
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.612403392791748
Epoch: 60, Steps: 59 | Train Loss: 0.5680271 Vali Loss: 1.4372200 Test Loss: 0.4257663
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.6793243885040283
Epoch: 61, Steps: 59 | Train Loss: 0.5679783 Vali Loss: 1.4410310 Test Loss: 0.4257790
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6391091346740723
Epoch: 62, Steps: 59 | Train Loss: 0.5677815 Vali Loss: 1.4381421 Test Loss: 0.4257865
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.5783321857452393
Epoch: 63, Steps: 59 | Train Loss: 0.5678381 Vali Loss: 1.4366182 Test Loss: 0.4257882
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.665630578994751
Epoch: 64, Steps: 59 | Train Loss: 0.5680483 Vali Loss: 1.4388111 Test Loss: 0.4258000
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.5417523384094238
Epoch: 65, Steps: 59 | Train Loss: 0.5677623 Vali Loss: 1.4373760 Test Loss: 0.4258017
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.42453619837760925, mae:0.4458204209804535, rse:0.623748242855072, corr:[0.22432011 0.23485138 0.23431234 0.23661648 0.23439035 0.23166643
 0.23243485 0.23287089 0.2318627  0.23225872 0.23251674 0.23154433
 0.23134547 0.23174866 0.23135202 0.23113196 0.23145077 0.23112437
 0.2305727  0.23072737 0.23105665 0.23113231 0.23175266 0.23236203
 0.23208103 0.23217553 0.23261923 0.23221195 0.23169644 0.23194443
 0.23189163 0.23111379 0.23079014 0.23111066 0.23101087 0.2306285
 0.23084378 0.231061   0.23079266 0.23066373 0.23082109 0.23063503
 0.23036943 0.2304043  0.23030317 0.23007533 0.23017913 0.23037387
 0.23020427 0.22984597 0.22951414 0.2292153  0.2289119  0.22839993
 0.22791757 0.22723849 0.22702047 0.22680013 0.22643419 0.22623274
 0.22604783 0.22583893 0.22568601 0.2257525  0.22584245 0.22596975
 0.22621804 0.22625937 0.226113   0.22609136 0.22600742 0.22572826
 0.22525178 0.22482648 0.22445124 0.22416681 0.22409531 0.22407594
 0.22377309 0.22333665 0.2231568  0.22293766 0.22261246 0.22250845
 0.2223793  0.22198288 0.22171555 0.2217657  0.22162597 0.2212016
 0.22088836 0.22081667 0.22076096 0.22079594 0.22110325 0.22207011
 0.22328755 0.22403196 0.22451703 0.22497655 0.22532858 0.22542988
 0.22523214 0.22514115 0.22513741 0.2249324  0.22452828 0.22425306
 0.22415389 0.22405829 0.2240034  0.22406997 0.22413713 0.22397172
 0.2237968  0.2237396  0.22362992 0.2232631  0.22286166 0.22296697
 0.22320125 0.2228915  0.2223706  0.22219785 0.22204119 0.22170301
 0.22148311 0.22145496 0.22121847 0.22079313 0.22048491 0.22028722
 0.22017898 0.22013868 0.2200804  0.21999806 0.21999359 0.22004649
 0.22006091 0.21998836 0.21992339 0.21981521 0.21940297 0.21920651
 0.21925892 0.2190932  0.21875213 0.2183027  0.2178061  0.21742366
 0.21730007 0.21731037 0.21733423 0.21742503 0.21748552 0.21748886
 0.21745327 0.21741647 0.21730992 0.21718648 0.21712688 0.21712436
 0.21699716 0.2168248  0.21663274 0.21632014 0.21578646 0.21589616
 0.21642272 0.21694778 0.21771926 0.21859582 0.21878198 0.21867262
 0.21891187 0.21921508 0.21919434 0.21908224 0.2190671  0.21892467
 0.21866322 0.21863933 0.21876937 0.21865866 0.21864858 0.21887994
 0.21888167 0.21872711 0.21867412 0.21838948 0.21792375 0.2178471
 0.21791978 0.21768808 0.2173348  0.21709482 0.216653   0.2161371
 0.2160153  0.21609052 0.21586837 0.21565253 0.21566789 0.2155686
 0.21542406 0.21572444 0.21603605 0.21595342 0.21581364 0.21583469
 0.21571808 0.21530141 0.21493866 0.2146536  0.21424922 0.21428114
 0.2146136  0.21466891 0.2146559  0.21459717 0.21449983 0.21446161
 0.21450803 0.21456487 0.2144125  0.21403505 0.21364084 0.213311
 0.21294881 0.21282536 0.2128958  0.2128659  0.21271123 0.21253406
 0.2123854  0.21212131 0.21181743 0.21151066 0.21134242 0.21146299
 0.21182367 0.21196267 0.21213429 0.21243908 0.21268071 0.21282461
 0.21303144 0.21322496 0.21323405 0.21300586 0.21273886 0.21250921
 0.21240571 0.21249342 0.2127757  0.21292067 0.21293955 0.2128662
 0.21275991 0.21259555 0.21243118 0.21230188 0.21218187 0.21219712
 0.21210629 0.21195339 0.21196844 0.21192521 0.21168166 0.21162383
 0.21167435 0.21155195 0.21134207 0.21130632 0.21119526 0.21101812
 0.21096264 0.21099877 0.2108297  0.21057338 0.21036546 0.2102454
 0.21013479 0.21012439 0.21024042 0.21033743 0.21042734 0.21088998
 0.21149033 0.21199849 0.21250774 0.21285182 0.21303834 0.21325864
 0.21345109 0.21345153 0.21336958 0.21336325 0.2132712  0.2131079
 0.21304548 0.2131896  0.21334092 0.21325673 0.2132097  0.21326773
 0.21333024 0.21321456 0.21303649 0.21290521 0.21272305 0.21301158
 0.21372274 0.21425658 0.21448909 0.21461762 0.21458587 0.21441752
 0.21424286 0.21424533 0.21420196 0.2138084  0.21325693 0.21312223
 0.21320033 0.21315809 0.21306269 0.2131994  0.21333921 0.21339038
 0.21349406 0.21354543 0.21343128 0.21336856 0.21339029 0.21336053
 0.21336426 0.2134032  0.21336137 0.21309705 0.2128258  0.21253696
 0.21228957 0.21216924 0.2120659  0.21189588 0.21185973 0.21202701
 0.21210855 0.21209848 0.21227077 0.21260646 0.21255201 0.21241634
 0.21252793 0.21258213 0.21226685 0.21191941 0.21150512 0.21136144
 0.21152662 0.2117457  0.21177123 0.21151231 0.2112295  0.21095984
 0.21067435 0.21037084 0.21020266 0.21003026 0.2096553  0.20940582
 0.2094136  0.20926838 0.20897807 0.20869634 0.20851608 0.2084299
 0.20844412 0.20847759 0.20835054 0.20821226 0.20825748 0.20898837
 0.21001598 0.21095414 0.2118542  0.21225305 0.21218249 0.21182401
 0.21144903 0.21096815 0.21065837 0.2103809  0.2099736  0.20975266
 0.20969541 0.20968609 0.20973672 0.209792   0.20983312 0.21008709
 0.21047357 0.21077606 0.21088426 0.2111557  0.21150912 0.21191435
 0.21223623 0.2124895  0.2125285  0.21233247 0.2121475  0.2119458
 0.21164958 0.2115208  0.21145959 0.21126924 0.21104278 0.21104296
 0.2111684  0.21111844 0.21082151 0.21058562 0.21046709 0.21046267
 0.21055195 0.21074651 0.21079965 0.21118937 0.21171778 0.21232846
 0.2129049  0.21328187 0.21339399 0.2133505  0.21328777 0.2131297
 0.21285006 0.21279159 0.21274012 0.2125636  0.21235824 0.2123019
 0.21233326 0.21216485 0.21192396 0.21178411 0.21174215 0.21175633
 0.21179393 0.21183556 0.21194485 0.21213113 0.21237308 0.21313097
 0.21395342 0.21431562 0.2143529  0.21432287 0.21408811 0.21380842
 0.21350987 0.2133166  0.21325731 0.2132203  0.21317014 0.2131843
 0.21323235 0.21317239 0.21319382 0.21335398 0.21340778 0.21338159
 0.21346448 0.21356884 0.21347368 0.21315248 0.21316646 0.21349506
 0.21372852 0.21366856 0.21347539 0.2133787  0.21328151 0.21313146
 0.21292782 0.21266687 0.21246225 0.21233016 0.21200773 0.2118282
 0.2118564  0.2117394  0.21155146 0.21152219 0.21148992 0.21134011
 0.21127751 0.2113951  0.21158233 0.21178603 0.21221073 0.21317224
 0.21412387 0.21470322 0.2151695  0.21552128 0.21550809 0.2151531
 0.2147055  0.21433046 0.2139599  0.2134099  0.21285717 0.21275194
 0.21293177 0.21296766 0.21305868 0.2133899  0.21370898 0.21375364
 0.21385238 0.214195   0.2143791  0.21438184 0.21476871 0.21538194
 0.21548319 0.21520908 0.21520111 0.21527933 0.21485403 0.21428905
 0.21388644 0.21361865 0.21326967 0.21299393 0.21270554 0.21261576
 0.21259196 0.21271914 0.21291502 0.21326324 0.21357335 0.21360537
 0.2135164  0.21358068 0.21358654 0.2134113  0.21315633 0.21315119
 0.2129661  0.2125907  0.2124884  0.21226403 0.21163233 0.2110689
 0.21054304 0.20999853 0.2096262  0.20914842 0.20857164 0.20821139
 0.20808493 0.20784229 0.20770857 0.20766632 0.20769817 0.20778991
 0.20802446 0.20804732 0.20788836 0.20780818 0.20801523 0.20817214
 0.20800808 0.20788015 0.20777729 0.2076865  0.20742849 0.20694338
 0.20654461 0.20610383 0.20580865 0.20563297 0.2054519  0.20520417
 0.2050881  0.20485935 0.20483637 0.2047677  0.20460048 0.2045528
 0.20466867 0.2046352  0.2044165  0.20428218 0.20437549 0.20447469
 0.20430014 0.20390499 0.20377012 0.20339191 0.2025509  0.20184977
 0.20154798 0.20126408 0.20068552 0.20030425 0.20011133 0.19987242
 0.19967645 0.19942875 0.19927944 0.19938318 0.19937639 0.19916818
 0.19908507 0.19916902 0.19930127 0.1994693  0.19977029 0.20022707
 0.20033114 0.19996542 0.19988818 0.19959569 0.19882503 0.19828998
 0.19825128 0.19784863 0.19707605 0.19676532 0.19663782 0.19648112
 0.19651769 0.19658224 0.1966281  0.19636837 0.1960618  0.19620222
 0.19633287 0.19621032 0.19617473 0.1964256  0.19691965 0.19755453
 0.19781187 0.19756924 0.19719264 0.1971599  0.19674861 0.19652633
 0.196229   0.19558759 0.19491069 0.19466212 0.19450754 0.19406192
 0.19384733 0.19398117 0.19394161 0.19352606 0.1936546  0.19395107
 0.19398318 0.19389515 0.19410135 0.19429523 0.19432722 0.19446287
 0.19412632 0.19321482 0.19248691 0.19179974 0.19105935 0.19019175
 0.18942446 0.18890703 0.1887256  0.18855116 0.18824632 0.18801025
 0.18827346 0.18841563 0.18824108 0.18822186 0.18838586 0.18874243
 0.18892154 0.18909267 0.1894749  0.1898633  0.18994558 0.1899718
 0.1902004  0.19003095 0.18959771 0.18943149 0.1893754  0.18882458
 0.18820238 0.18806203 0.18819675 0.18778749 0.18699953 0.18672667
 0.18698283 0.18666568 0.18624818 0.18560869 0.18588148 0.18630826
 0.18624242 0.18625902 0.18678448 0.18419921 0.18377715 0.18272237]
