Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=90, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  21772800.0
params:  24570.0
Trainable parameters:  24570
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.5726523399353027
Epoch: 1, Steps: 59 | Train Loss: 0.9769760 Vali Loss: 1.9912949 Test Loss: 0.8055325
Validation loss decreased (inf --> 1.991295).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.141671895980835
Epoch: 2, Steps: 59 | Train Loss: 0.7583164 Vali Loss: 1.7480924 Test Loss: 0.6366853
Validation loss decreased (1.991295 --> 1.748092).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.3744003772735596
Epoch: 3, Steps: 59 | Train Loss: 0.6862230 Vali Loss: 1.6634924 Test Loss: 0.5731154
Validation loss decreased (1.748092 --> 1.663492).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.38671875
Epoch: 4, Steps: 59 | Train Loss: 0.6556529 Vali Loss: 1.6162058 Test Loss: 0.5385049
Validation loss decreased (1.663492 --> 1.616206).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.674288034439087
Epoch: 5, Steps: 59 | Train Loss: 0.6374322 Vali Loss: 1.5835645 Test Loss: 0.5150319
Validation loss decreased (1.616206 --> 1.583565).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.035146951675415
Epoch: 6, Steps: 59 | Train Loss: 0.6248113 Vali Loss: 1.5664062 Test Loss: 0.4973530
Validation loss decreased (1.583565 --> 1.566406).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.9426183700561523
Epoch: 7, Steps: 59 | Train Loss: 0.6148230 Vali Loss: 1.5381421 Test Loss: 0.4834314
Validation loss decreased (1.566406 --> 1.538142).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.5268049240112305
Epoch: 8, Steps: 59 | Train Loss: 0.6071502 Vali Loss: 1.5247915 Test Loss: 0.4724848
Validation loss decreased (1.538142 --> 1.524791).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.614436149597168
Epoch: 9, Steps: 59 | Train Loss: 0.6010670 Vali Loss: 1.5108291 Test Loss: 0.4635158
Validation loss decreased (1.524791 --> 1.510829).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.459714651107788
Epoch: 10, Steps: 59 | Train Loss: 0.5958366 Vali Loss: 1.5032704 Test Loss: 0.4564172
Validation loss decreased (1.510829 --> 1.503270).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.4186065196990967
Epoch: 11, Steps: 59 | Train Loss: 0.5917667 Vali Loss: 1.4957323 Test Loss: 0.4507343
Validation loss decreased (1.503270 --> 1.495732).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.5691680908203125
Epoch: 12, Steps: 59 | Train Loss: 0.5884388 Vali Loss: 1.4817843 Test Loss: 0.4460133
Validation loss decreased (1.495732 --> 1.481784).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.2758429050445557
Epoch: 13, Steps: 59 | Train Loss: 0.5856485 Vali Loss: 1.4791106 Test Loss: 0.4422874
Validation loss decreased (1.481784 --> 1.479111).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.076301097869873
Epoch: 14, Steps: 59 | Train Loss: 0.5835391 Vali Loss: 1.4695189 Test Loss: 0.4392147
Validation loss decreased (1.479111 --> 1.469519).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.0797581672668457
Epoch: 15, Steps: 59 | Train Loss: 0.5817223 Vali Loss: 1.4707859 Test Loss: 0.4368533
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.9904241561889648
Epoch: 16, Steps: 59 | Train Loss: 0.5799647 Vali Loss: 1.4659834 Test Loss: 0.4349000
Validation loss decreased (1.469519 --> 1.465983).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.0705440044403076
Epoch: 17, Steps: 59 | Train Loss: 0.5786457 Vali Loss: 1.4582808 Test Loss: 0.4332748
Validation loss decreased (1.465983 --> 1.458281).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.2656145095825195
Epoch: 18, Steps: 59 | Train Loss: 0.5776089 Vali Loss: 1.4580884 Test Loss: 0.4320192
Validation loss decreased (1.458281 --> 1.458088).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.1504082679748535
Epoch: 19, Steps: 59 | Train Loss: 0.5767246 Vali Loss: 1.4537091 Test Loss: 0.4310044
Validation loss decreased (1.458088 --> 1.453709).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.3714005947113037
Epoch: 20, Steps: 59 | Train Loss: 0.5758545 Vali Loss: 1.4560657 Test Loss: 0.4301504
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.9313580989837646
Epoch: 21, Steps: 59 | Train Loss: 0.5751111 Vali Loss: 1.4506389 Test Loss: 0.4295389
Validation loss decreased (1.453709 --> 1.450639).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.3173062801361084
Epoch: 22, Steps: 59 | Train Loss: 0.5745286 Vali Loss: 1.4514582 Test Loss: 0.4289772
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.567962408065796
Epoch: 23, Steps: 59 | Train Loss: 0.5739792 Vali Loss: 1.4461113 Test Loss: 0.4285771
Validation loss decreased (1.450639 --> 1.446111).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.2131752967834473
Epoch: 24, Steps: 59 | Train Loss: 0.5737775 Vali Loss: 1.4482806 Test Loss: 0.4282846
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.1726250648498535
Epoch: 25, Steps: 59 | Train Loss: 0.5733358 Vali Loss: 1.4436059 Test Loss: 0.4279753
Validation loss decreased (1.446111 --> 1.443606).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.364805221557617
Epoch: 26, Steps: 59 | Train Loss: 0.5729100 Vali Loss: 1.4449517 Test Loss: 0.4277894
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.5592222213745117
Epoch: 27, Steps: 59 | Train Loss: 0.5728933 Vali Loss: 1.4462082 Test Loss: 0.4276137
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.023226022720337
Epoch: 28, Steps: 59 | Train Loss: 0.5726179 Vali Loss: 1.4460028 Test Loss: 0.4275105
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.4525787830352783
Epoch: 29, Steps: 59 | Train Loss: 0.5724454 Vali Loss: 1.4441918 Test Loss: 0.4273750
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.448817729949951
Epoch: 30, Steps: 59 | Train Loss: 0.5721920 Vali Loss: 1.4366872 Test Loss: 0.4273397
Validation loss decreased (1.443606 --> 1.436687).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.733271598815918
Epoch: 31, Steps: 59 | Train Loss: 0.5721826 Vali Loss: 1.4421593 Test Loss: 0.4272688
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.5025887489318848
Epoch: 32, Steps: 59 | Train Loss: 0.5718158 Vali Loss: 1.4361186 Test Loss: 0.4272712
Validation loss decreased (1.436687 --> 1.436119).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.426146984100342
Epoch: 33, Steps: 59 | Train Loss: 0.5717767 Vali Loss: 1.4463277 Test Loss: 0.4272288
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9202075004577637
Epoch: 34, Steps: 59 | Train Loss: 0.5716533 Vali Loss: 1.4460139 Test Loss: 0.4272064
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.7813549041748047
Epoch: 35, Steps: 59 | Train Loss: 0.5714858 Vali Loss: 1.4441261 Test Loss: 0.4272123
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.546729564666748
Epoch: 36, Steps: 59 | Train Loss: 0.5713489 Vali Loss: 1.4391457 Test Loss: 0.4272160
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.4925734996795654
Epoch: 37, Steps: 59 | Train Loss: 0.5714764 Vali Loss: 1.4451895 Test Loss: 0.4272156
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.8960366249084473
Epoch: 38, Steps: 59 | Train Loss: 0.5712128 Vali Loss: 1.4425273 Test Loss: 0.4272632
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.949815034866333
Epoch: 39, Steps: 59 | Train Loss: 0.5712230 Vali Loss: 1.4375877 Test Loss: 0.4272709
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.5293493270874023
Epoch: 40, Steps: 59 | Train Loss: 0.5710427 Vali Loss: 1.4445597 Test Loss: 0.4272738
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.13972544670105
Epoch: 41, Steps: 59 | Train Loss: 0.5710429 Vali Loss: 1.4411592 Test Loss: 0.4273069
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.6820802688598633
Epoch: 42, Steps: 59 | Train Loss: 0.5709383 Vali Loss: 1.4407849 Test Loss: 0.4273145
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.7014405727386475
Epoch: 43, Steps: 59 | Train Loss: 0.5711489 Vali Loss: 1.4396005 Test Loss: 0.4273616
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.8107762336730957
Epoch: 44, Steps: 59 | Train Loss: 0.5709910 Vali Loss: 1.4399230 Test Loss: 0.4273380
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.2660863399505615
Epoch: 45, Steps: 59 | Train Loss: 0.5707597 Vali Loss: 1.4387130 Test Loss: 0.4273779
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.5350732803344727
Epoch: 46, Steps: 59 | Train Loss: 0.5707914 Vali Loss: 1.4373367 Test Loss: 0.4273987
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.872342586517334
Epoch: 47, Steps: 59 | Train Loss: 0.5708911 Vali Loss: 1.4368777 Test Loss: 0.4274236
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.8355894088745117
Epoch: 48, Steps: 59 | Train Loss: 0.5706243 Vali Loss: 1.4402831 Test Loss: 0.4274544
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.2155208587646484
Epoch: 49, Steps: 59 | Train Loss: 0.5707758 Vali Loss: 1.4365722 Test Loss: 0.4274620
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.2592387199401855
Epoch: 50, Steps: 59 | Train Loss: 0.5707514 Vali Loss: 1.4325044 Test Loss: 0.4274869
Validation loss decreased (1.436119 --> 1.432504).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.9748752117156982
Epoch: 51, Steps: 59 | Train Loss: 0.5706245 Vali Loss: 1.4371296 Test Loss: 0.4275125
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.4168615341186523
Epoch: 52, Steps: 59 | Train Loss: 0.5706643 Vali Loss: 1.4359045 Test Loss: 0.4275367
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.4079484939575195
Epoch: 53, Steps: 59 | Train Loss: 0.5707039 Vali Loss: 1.4374084 Test Loss: 0.4275378
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.250836133956909
Epoch: 54, Steps: 59 | Train Loss: 0.5707251 Vali Loss: 1.4474396 Test Loss: 0.4275738
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.004124164581299
Epoch: 55, Steps: 59 | Train Loss: 0.5706639 Vali Loss: 1.4427510 Test Loss: 0.4275879
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.0660910606384277
Epoch: 56, Steps: 59 | Train Loss: 0.5704939 Vali Loss: 1.4336128 Test Loss: 0.4275905
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.0752179622650146
Epoch: 57, Steps: 59 | Train Loss: 0.5705521 Vali Loss: 1.4344662 Test Loss: 0.4276126
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.2773187160491943
Epoch: 58, Steps: 59 | Train Loss: 0.5705088 Vali Loss: 1.4412754 Test Loss: 0.4276325
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.816152572631836
Epoch: 59, Steps: 59 | Train Loss: 0.5706845 Vali Loss: 1.4376283 Test Loss: 0.4276506
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.3289451599121094
Epoch: 60, Steps: 59 | Train Loss: 0.5704264 Vali Loss: 1.4402139 Test Loss: 0.4276571
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.0030558109283447
Epoch: 61, Steps: 59 | Train Loss: 0.5706754 Vali Loss: 1.4400491 Test Loss: 0.4276828
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.510436773300171
Epoch: 62, Steps: 59 | Train Loss: 0.5705140 Vali Loss: 1.4355710 Test Loss: 0.4276895
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.4721264839172363
Epoch: 63, Steps: 59 | Train Loss: 0.5707104 Vali Loss: 1.4384776 Test Loss: 0.4277007
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.894573450088501
Epoch: 64, Steps: 59 | Train Loss: 0.5704400 Vali Loss: 1.4355245 Test Loss: 0.4277124
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.4571943283081055
Epoch: 65, Steps: 59 | Train Loss: 0.5706220 Vali Loss: 1.4439156 Test Loss: 0.4277258
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.1569814682006836
Epoch: 66, Steps: 59 | Train Loss: 0.5706488 Vali Loss: 1.4383653 Test Loss: 0.4277362
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.8050341606140137
Epoch: 67, Steps: 59 | Train Loss: 0.5704025 Vali Loss: 1.4377898 Test Loss: 0.4277469
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 4.465768814086914
Epoch: 68, Steps: 59 | Train Loss: 0.5706154 Vali Loss: 1.4314821 Test Loss: 0.4277590
Validation loss decreased (1.432504 --> 1.431482).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.7830092906951904
Epoch: 69, Steps: 59 | Train Loss: 0.5704754 Vali Loss: 1.4414718 Test Loss: 0.4277647
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9944305419921875
Epoch: 70, Steps: 59 | Train Loss: 0.5704483 Vali Loss: 1.4421868 Test Loss: 0.4277717
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.6794395446777344
Epoch: 71, Steps: 59 | Train Loss: 0.5704884 Vali Loss: 1.4416649 Test Loss: 0.4277851
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.0490996837615967
Epoch: 72, Steps: 59 | Train Loss: 0.5706074 Vali Loss: 1.4384005 Test Loss: 0.4277966
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.5847437381744385
Epoch: 73, Steps: 59 | Train Loss: 0.5702239 Vali Loss: 1.4368320 Test Loss: 0.4277961
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.378232717514038
Epoch: 74, Steps: 59 | Train Loss: 0.5703786 Vali Loss: 1.4418926 Test Loss: 0.4278066
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.288137674331665
Epoch: 75, Steps: 59 | Train Loss: 0.5705156 Vali Loss: 1.4359257 Test Loss: 0.4278168
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.210386037826538
Epoch: 76, Steps: 59 | Train Loss: 0.5704637 Vali Loss: 1.4363260 Test Loss: 0.4278200
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.4717118740081787
Epoch: 77, Steps: 59 | Train Loss: 0.5705669 Vali Loss: 1.4416372 Test Loss: 0.4278276
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.722738265991211
Epoch: 78, Steps: 59 | Train Loss: 0.5704789 Vali Loss: 1.4355656 Test Loss: 0.4278344
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.558279514312744
Epoch: 79, Steps: 59 | Train Loss: 0.5705353 Vali Loss: 1.4359756 Test Loss: 0.4278398
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.6096160411834717
Epoch: 80, Steps: 59 | Train Loss: 0.5704605 Vali Loss: 1.4321716 Test Loss: 0.4278455
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.202538013458252
Epoch: 81, Steps: 59 | Train Loss: 0.5705044 Vali Loss: 1.4402313 Test Loss: 0.4278510
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.199103593826294
Epoch: 82, Steps: 59 | Train Loss: 0.5704300 Vali Loss: 1.4402944 Test Loss: 0.4278571
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.476318359375
Epoch: 83, Steps: 59 | Train Loss: 0.5704536 Vali Loss: 1.4372245 Test Loss: 0.4278574
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.4747533798217773
Epoch: 84, Steps: 59 | Train Loss: 0.5704077 Vali Loss: 1.4397385 Test Loss: 0.4278672
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.657113552093506
Epoch: 85, Steps: 59 | Train Loss: 0.5702915 Vali Loss: 1.4338709 Test Loss: 0.4278712
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.5102789402008057
Epoch: 86, Steps: 59 | Train Loss: 0.5702218 Vali Loss: 1.4405782 Test Loss: 0.4278783
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.0079102516174316
Epoch: 87, Steps: 59 | Train Loss: 0.5703747 Vali Loss: 1.4405761 Test Loss: 0.4278818
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.2590067386627197
Epoch: 88, Steps: 59 | Train Loss: 0.5704413 Vali Loss: 1.4402281 Test Loss: 0.4278863
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4267195165157318, mae:0.4472176432609558, rse:0.6253500580787659, corr:[0.22254215 0.2353667  0.23446925 0.2346062  0.23460235 0.23261876
 0.23135808 0.23220319 0.2326751  0.23219255 0.23153493 0.23126464
 0.23120315 0.23107134 0.2308966  0.23072271 0.23033948 0.23012252
 0.23029466 0.23042692 0.23035248 0.2305105  0.23124899 0.23183975
 0.23162018 0.23139116 0.23167978 0.23179483 0.23133758 0.23079145
 0.23064342 0.23063245 0.23039076 0.23005413 0.22994989 0.23002431
 0.23008771 0.23002872 0.2299181  0.22976223 0.22965284 0.22965401
 0.22971845 0.22953866 0.22912225 0.22902843 0.22948238 0.22980092
 0.22952178 0.22915854 0.22896497 0.22871794 0.22817504 0.2275236
 0.22721057 0.22677748 0.22646503 0.22609302 0.22571947 0.2255449
 0.22538239 0.22518148 0.22492892 0.22485903 0.22499196 0.22533362
 0.22556122 0.22531234 0.22502153 0.22512807 0.2252907  0.22501993
 0.22426255 0.2237453  0.22365087 0.22353567 0.22319713 0.2229401
 0.22284943 0.22265881 0.22231796 0.22195405 0.22170305 0.2215898
 0.22145756 0.22120057 0.22091354 0.2206691  0.22051072 0.22039585
 0.22017373 0.21991852 0.21984789 0.22013682 0.22071674 0.2215797
 0.22250845 0.22331685 0.22405899 0.22451137 0.22458908 0.22456442
 0.22457758 0.22459963 0.22441107 0.2240965  0.22381075 0.2235966
 0.22343302 0.22332223 0.223259   0.22326756 0.2232616  0.22322625
 0.22316055 0.22293894 0.22273679 0.2226333  0.2225733  0.2225473
 0.22245073 0.22226185 0.22196463 0.22164461 0.22127679 0.22096832
 0.22079067 0.22061479 0.22027749 0.21988845 0.21961077 0.21941362
 0.21928182 0.219196   0.21914305 0.2191244  0.21915996 0.21924272
 0.21925016 0.21911487 0.21904247 0.2190715  0.21887176 0.21855074
 0.21831857 0.21814047 0.21786758 0.21731475 0.21666612 0.21628697
 0.21629438 0.21645078 0.21650065 0.21648082 0.21641877 0.21642137
 0.2164417  0.21635087 0.21610516 0.215948   0.21596241 0.21607617
 0.21595794 0.21564923 0.21549024 0.21544664 0.2151608  0.21506792
 0.21552189 0.21646868 0.21727803 0.21772768 0.21793436 0.2181413
 0.21832477 0.21836717 0.218328   0.21826568 0.21818578 0.21808155
 0.2179482  0.21778667 0.21768436 0.21766043 0.2178005  0.21801133
 0.21804616 0.21793012 0.21794435 0.21792364 0.2177231  0.21738176
 0.2170984  0.21696131 0.2166807  0.21619403 0.21564457 0.21527089
 0.2151221  0.21510117 0.21497408 0.21478395 0.21463995 0.21460526
 0.21467617 0.2148301  0.21488993 0.21483819 0.21483384 0.21489817
 0.21481746 0.2144853  0.2141246  0.21386373 0.2136067  0.21351059
 0.21357028 0.21362095 0.21369393 0.21362334 0.2134827  0.21344572
 0.21346432 0.21345952 0.21332295 0.21302764 0.21261872 0.21227212
 0.21204051 0.21192813 0.21188724 0.21179587 0.21167919 0.21163315
 0.21153945 0.2112066  0.21081653 0.2106201  0.21066545 0.2107256
 0.21087798 0.21105303 0.21127759 0.21150893 0.21169332 0.21180858
 0.21198572 0.21214227 0.21219796 0.21202484 0.21173929 0.21146975
 0.21135707 0.2113959  0.21157879 0.21162531 0.21168207 0.2118104
 0.21183853 0.21156372 0.21142083 0.21153018 0.21154405 0.2113454
 0.2110596  0.21089329 0.21081308 0.21071117 0.21052165 0.21042162
 0.21045242 0.2104466  0.21028917 0.21014711 0.21004063 0.20995423
 0.2099046  0.20986333 0.20971933 0.20950015 0.20926654 0.20919263
 0.20914827 0.20904435 0.20905781 0.2092474  0.20955414 0.21001694
 0.21057764 0.21112391 0.21157643 0.21180712 0.21194568 0.21212602
 0.21234557 0.21246448 0.212433   0.21233541 0.21216536 0.21206053
 0.21207087 0.2121089  0.21217191 0.21217334 0.21215081 0.21218947
 0.21228349 0.2122258  0.21209441 0.21204393 0.21201828 0.21225831
 0.21274434 0.21324123 0.21355133 0.2136769  0.21356814 0.21339364
 0.2133112  0.21328844 0.21311457 0.2127662  0.21231169 0.21202627
 0.21199022 0.2120634  0.21204738 0.2121493  0.21238163 0.2126884
 0.21283723 0.21273519 0.21267164 0.21276957 0.21276668 0.21253283
 0.2123674  0.21243542 0.21254927 0.21235229 0.21195099 0.21154414
 0.21136199 0.2112694  0.21109939 0.21097848 0.21107647 0.21126284
 0.21133685 0.21137537 0.21142817 0.21154003 0.211599   0.21181932
 0.21192582 0.21170923 0.21138509 0.2112314  0.21088272 0.21060532
 0.21065019 0.21087518 0.21097757 0.21078092 0.2103771  0.20991082
 0.20970532 0.20956743 0.20932268 0.20906647 0.20888321 0.20871884
 0.20853877 0.20829606 0.20802717 0.20768963 0.20749146 0.20755054
 0.20769052 0.20764443 0.20747875 0.20746776 0.20759572 0.2082339
 0.20928751 0.21036252 0.2111882  0.21140972 0.21120484 0.21077254
 0.21047571 0.21004726 0.20955926 0.20918703 0.20892003 0.2087729
 0.20873785 0.20877023 0.20885266 0.20889089 0.2089886  0.20936136
 0.20974761 0.20992666 0.20999667 0.21035211 0.21080966 0.21127428
 0.21152414 0.21164611 0.21166095 0.21152592 0.21125032 0.21092126
 0.21073611 0.21072783 0.21058886 0.21032926 0.21022029 0.21026891
 0.21032892 0.21029101 0.2099917  0.20962934 0.20949237 0.20965433
 0.20980911 0.20988123 0.20985565 0.21035074 0.2110346  0.21161622
 0.21198766 0.2122354  0.21237071 0.21236585 0.21222472 0.21205765
 0.21197368 0.21200593 0.21177176 0.2114658  0.2113586  0.21139774
 0.21140657 0.21124515 0.21106356 0.21097185 0.21099268 0.21107431
 0.21108395 0.21099222 0.21103273 0.21132445 0.21172582 0.21240324
 0.21306528 0.21348844 0.21361169 0.2135552  0.21320148 0.2127773
 0.21254128 0.21244569 0.21230713 0.2122362  0.21230139 0.21238007
 0.21242097 0.21245998 0.2125487  0.21259609 0.21259643 0.21269275
 0.21280658 0.21279615 0.21264651 0.21238613 0.21248989 0.21276675
 0.21278723 0.21258344 0.21240632 0.21242422 0.21236283 0.21209833
 0.21194349 0.21182476 0.2115137  0.21122472 0.21100947 0.21092695
 0.2108531  0.21070285 0.2106087  0.21059541 0.21060045 0.2105859
 0.21056016 0.21052083 0.21057887 0.21086355 0.21145952 0.21235004
 0.21318328 0.21386209 0.21445529 0.2148772  0.21478578 0.2142204
 0.21373975 0.21345018 0.21306485 0.21253662 0.21215108 0.21206194
 0.21210523 0.21217638 0.21241277 0.21268909 0.21290056 0.21308011
 0.21323583 0.21340445 0.2135711  0.21383347 0.21430956 0.21463253
 0.21452668 0.2142711  0.21426214 0.21439886 0.21402662 0.21321061
 0.21263467 0.21251374 0.21232793 0.21200909 0.21172485 0.21169753
 0.21169986 0.2117339  0.21193731 0.21235032 0.21271442 0.21283306
 0.2127324  0.21268329 0.21262655 0.21248104 0.2121946  0.21201666
 0.21182738 0.21161102 0.21148145 0.21126015 0.21069224 0.2100636
 0.20956294 0.20912655 0.20871541 0.2081447  0.20764433 0.20736901
 0.20729557 0.2070631  0.20685394 0.2067567  0.20689136 0.2070267
 0.20711453 0.20708936 0.20708974 0.20702711 0.20702478 0.20705035
 0.20703243 0.20699628 0.20673603 0.20661837 0.20654678 0.2060074
 0.20542923 0.2050037  0.20472421 0.20451555 0.20439257 0.20426954
 0.20422189 0.20395964 0.20383352 0.2037875  0.20378488 0.20382741
 0.20388536 0.20383291 0.20375498 0.20369959 0.20362428 0.20354003
 0.2033637  0.20300491 0.20273073 0.20241883 0.20177388 0.20092086
 0.20039676 0.20019786 0.19977368 0.19924821 0.19897842 0.1989603
 0.19892836 0.1985473  0.19821426 0.19833302 0.19846414 0.19832052
 0.19814701 0.19819109 0.19841121 0.19863226 0.19882388 0.19918062
 0.19937536 0.19900146 0.19867502 0.19842604 0.1979632  0.19729455
 0.197002   0.19686578 0.19636083 0.19578524 0.19546264 0.19558413
 0.19583535 0.19563349 0.19537538 0.1952842  0.19523433 0.19524243
 0.19517024 0.19518344 0.1953598  0.19553836 0.19582947 0.19635865
 0.19674806 0.19663306 0.19607423 0.19594187 0.19571277 0.1953811
 0.19498889 0.1946004  0.19407535 0.1935992  0.19345815 0.19344155
 0.19331823 0.19304141 0.19285975 0.19272953 0.19292231 0.19307013
 0.19307348 0.19303744 0.19317874 0.19327505 0.1932353  0.19333868
 0.19311155 0.1923329  0.19148834 0.19073078 0.19006464 0.18913706
 0.18830217 0.18787012 0.18765554 0.18733205 0.18718927 0.18735866
 0.18765213 0.18747576 0.18711911 0.18718341 0.18736836 0.18763301
 0.18782724 0.18809816 0.18841472 0.18866883 0.18880425 0.18899265
 0.18932404 0.18927653 0.18882045 0.18853529 0.18835159 0.18779872
 0.1873128  0.1870435  0.18696412 0.1867128  0.18625262 0.18584299
 0.18575412 0.18531132 0.18502031 0.18472394 0.18482055 0.18467808
 0.18502496 0.18566585 0.18488209 0.18217649 0.18364154 0.1817539 ]
