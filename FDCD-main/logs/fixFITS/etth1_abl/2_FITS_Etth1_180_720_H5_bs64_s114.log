Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=50, out_features=250, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11200000.0
params:  12750.0
Trainable parameters:  12750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.363572835922241
Epoch: 1, Steps: 60 | Train Loss: 1.1327114 Vali Loss: 2.4103007 Test Loss: 1.1100950
Validation loss decreased (inf --> 2.410301).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.6574463844299316
Epoch: 2, Steps: 60 | Train Loss: 0.8450070 Vali Loss: 2.0485663 Test Loss: 0.8461735
Validation loss decreased (2.410301 --> 2.048566).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.7304904460906982
Epoch: 3, Steps: 60 | Train Loss: 0.7066394 Vali Loss: 1.8667691 Test Loss: 0.7113974
Validation loss decreased (2.048566 --> 1.866769).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9987430572509766
Epoch: 4, Steps: 60 | Train Loss: 0.6305777 Vali Loss: 1.7679410 Test Loss: 0.6330683
Validation loss decreased (1.866769 --> 1.767941).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.894735813140869
Epoch: 5, Steps: 60 | Train Loss: 0.5861155 Vali Loss: 1.6962481 Test Loss: 0.5851107
Validation loss decreased (1.767941 --> 1.696248).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.8179683685302734
Epoch: 6, Steps: 60 | Train Loss: 0.5587691 Vali Loss: 1.6558591 Test Loss: 0.5547437
Validation loss decreased (1.696248 --> 1.655859).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.1208741664886475
Epoch: 7, Steps: 60 | Train Loss: 0.5416447 Vali Loss: 1.6219451 Test Loss: 0.5346063
Validation loss decreased (1.655859 --> 1.621945).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.8545262813568115
Epoch: 8, Steps: 60 | Train Loss: 0.5299817 Vali Loss: 1.6108129 Test Loss: 0.5206103
Validation loss decreased (1.621945 --> 1.610813).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.2996232509613037
Epoch: 9, Steps: 60 | Train Loss: 0.5218683 Vali Loss: 1.5934563 Test Loss: 0.5105947
Validation loss decreased (1.610813 --> 1.593456).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.123427629470825
Epoch: 10, Steps: 60 | Train Loss: 0.5158466 Vali Loss: 1.5852754 Test Loss: 0.5029071
Validation loss decreased (1.593456 --> 1.585275).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.5562028884887695
Epoch: 11, Steps: 60 | Train Loss: 0.5115168 Vali Loss: 1.5694207 Test Loss: 0.4970852
Validation loss decreased (1.585275 --> 1.569421).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.9209792613983154
Epoch: 12, Steps: 60 | Train Loss: 0.5079681 Vali Loss: 1.5648750 Test Loss: 0.4923069
Validation loss decreased (1.569421 --> 1.564875).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.691854476928711
Epoch: 13, Steps: 60 | Train Loss: 0.5048099 Vali Loss: 1.5589960 Test Loss: 0.4884113
Validation loss decreased (1.564875 --> 1.558996).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.6466286182403564
Epoch: 14, Steps: 60 | Train Loss: 0.5025520 Vali Loss: 1.5535049 Test Loss: 0.4851536
Validation loss decreased (1.558996 --> 1.553505).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.261854648590088
Epoch: 15, Steps: 60 | Train Loss: 0.5005110 Vali Loss: 1.5516132 Test Loss: 0.4823874
Validation loss decreased (1.553505 --> 1.551613).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.0777206420898438
Epoch: 16, Steps: 60 | Train Loss: 0.4989558 Vali Loss: 1.5493026 Test Loss: 0.4797678
Validation loss decreased (1.551613 --> 1.549303).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.8453307151794434
Epoch: 17, Steps: 60 | Train Loss: 0.4973196 Vali Loss: 1.5416503 Test Loss: 0.4776569
Validation loss decreased (1.549303 --> 1.541650).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.395716428756714
Epoch: 18, Steps: 60 | Train Loss: 0.4955801 Vali Loss: 1.5392079 Test Loss: 0.4757370
Validation loss decreased (1.541650 --> 1.539208).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.628783702850342
Epoch: 19, Steps: 60 | Train Loss: 0.4950597 Vali Loss: 1.5410610 Test Loss: 0.4739135
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.274068832397461
Epoch: 20, Steps: 60 | Train Loss: 0.4932310 Vali Loss: 1.5374937 Test Loss: 0.4723541
Validation loss decreased (1.539208 --> 1.537494).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0676357746124268
Epoch: 21, Steps: 60 | Train Loss: 0.4927458 Vali Loss: 1.5412343 Test Loss: 0.4708393
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.764852523803711
Epoch: 22, Steps: 60 | Train Loss: 0.4915535 Vali Loss: 1.5328149 Test Loss: 0.4696110
Validation loss decreased (1.537494 --> 1.532815).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.5328712463378906
Epoch: 23, Steps: 60 | Train Loss: 0.4903200 Vali Loss: 1.5282941 Test Loss: 0.4683715
Validation loss decreased (1.532815 --> 1.528294).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8414514064788818
Epoch: 24, Steps: 60 | Train Loss: 0.4902505 Vali Loss: 1.5292447 Test Loss: 0.4672664
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9865264892578125
Epoch: 25, Steps: 60 | Train Loss: 0.4892835 Vali Loss: 1.5241691 Test Loss: 0.4662775
Validation loss decreased (1.528294 --> 1.524169).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.257230758666992
Epoch: 26, Steps: 60 | Train Loss: 0.4889358 Vali Loss: 1.5319536 Test Loss: 0.4653030
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.5271835327148438
Epoch: 27, Steps: 60 | Train Loss: 0.4882604 Vali Loss: 1.5290730 Test Loss: 0.4644735
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.156858444213867
Epoch: 28, Steps: 60 | Train Loss: 0.4881720 Vali Loss: 1.5257202 Test Loss: 0.4636734
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.2695043087005615
Epoch: 29, Steps: 60 | Train Loss: 0.4874594 Vali Loss: 1.5184336 Test Loss: 0.4629172
Validation loss decreased (1.524169 --> 1.518434).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.8031363487243652
Epoch: 30, Steps: 60 | Train Loss: 0.4870160 Vali Loss: 1.5245347 Test Loss: 0.4622150
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.5978124141693115
Epoch: 31, Steps: 60 | Train Loss: 0.4865116 Vali Loss: 1.5268728 Test Loss: 0.4615609
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.002033233642578
Epoch: 32, Steps: 60 | Train Loss: 0.4861330 Vali Loss: 1.5254508 Test Loss: 0.4609889
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.8457911014556885
Epoch: 33, Steps: 60 | Train Loss: 0.4852857 Vali Loss: 1.5200548 Test Loss: 0.4604061
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.162853717803955
Epoch: 34, Steps: 60 | Train Loss: 0.4848157 Vali Loss: 1.5162733 Test Loss: 0.4598658
Validation loss decreased (1.518434 --> 1.516273).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.0297129154205322
Epoch: 35, Steps: 60 | Train Loss: 0.4850922 Vali Loss: 1.5206823 Test Loss: 0.4593827
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.451693534851074
Epoch: 36, Steps: 60 | Train Loss: 0.4849443 Vali Loss: 1.5228938 Test Loss: 0.4589529
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.249067544937134
Epoch: 37, Steps: 60 | Train Loss: 0.4844410 Vali Loss: 1.5184519 Test Loss: 0.4585325
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.1905603408813477
Epoch: 38, Steps: 60 | Train Loss: 0.4842878 Vali Loss: 1.5108089 Test Loss: 0.4581118
Validation loss decreased (1.516273 --> 1.510809).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.1384739875793457
Epoch: 39, Steps: 60 | Train Loss: 0.4839760 Vali Loss: 1.5199444 Test Loss: 0.4577305
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.0060007572174072
Epoch: 40, Steps: 60 | Train Loss: 0.4839006 Vali Loss: 1.5183642 Test Loss: 0.4573772
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.3120882511138916
Epoch: 41, Steps: 60 | Train Loss: 0.4838739 Vali Loss: 1.5154080 Test Loss: 0.4570436
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.8863472938537598
Epoch: 42, Steps: 60 | Train Loss: 0.4831433 Vali Loss: 1.5145772 Test Loss: 0.4567602
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.1498093605041504
Epoch: 43, Steps: 60 | Train Loss: 0.4829877 Vali Loss: 1.5184664 Test Loss: 0.4564527
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.4635839462280273
Epoch: 44, Steps: 60 | Train Loss: 0.4825974 Vali Loss: 1.5192891 Test Loss: 0.4561831
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.575462818145752
Epoch: 45, Steps: 60 | Train Loss: 0.4827175 Vali Loss: 1.5122778 Test Loss: 0.4559282
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.24955153465271
Epoch: 46, Steps: 60 | Train Loss: 0.4826461 Vali Loss: 1.5119953 Test Loss: 0.4557051
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.5032167434692383
Epoch: 47, Steps: 60 | Train Loss: 0.4824218 Vali Loss: 1.5168105 Test Loss: 0.4554641
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.407065391540527
Epoch: 48, Steps: 60 | Train Loss: 0.4821000 Vali Loss: 1.5150690 Test Loss: 0.4552465
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 4.484947204589844
Epoch: 49, Steps: 60 | Train Loss: 0.4820690 Vali Loss: 1.5119661 Test Loss: 0.4550583
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.6726202964782715
Epoch: 50, Steps: 60 | Train Loss: 0.4818254 Vali Loss: 1.5151445 Test Loss: 0.4548399
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.6098644733428955
Epoch: 51, Steps: 60 | Train Loss: 0.4822800 Vali Loss: 1.5180061 Test Loss: 0.4546761
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.374222278594971
Epoch: 52, Steps: 60 | Train Loss: 0.4823360 Vali Loss: 1.5117284 Test Loss: 0.4545049
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.478691339492798
Epoch: 53, Steps: 60 | Train Loss: 0.4817454 Vali Loss: 1.5137630 Test Loss: 0.4543213
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.613363265991211
Epoch: 54, Steps: 60 | Train Loss: 0.4813089 Vali Loss: 1.5162160 Test Loss: 0.4541840
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.9386775493621826
Epoch: 55, Steps: 60 | Train Loss: 0.4812701 Vali Loss: 1.5117197 Test Loss: 0.4540379
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9781651496887207
Epoch: 56, Steps: 60 | Train Loss: 0.4816658 Vali Loss: 1.5131522 Test Loss: 0.4539122
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.2922780513763428
Epoch: 57, Steps: 60 | Train Loss: 0.4804588 Vali Loss: 1.5105175 Test Loss: 0.4537698
Validation loss decreased (1.510809 --> 1.510517).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.8070333003997803
Epoch: 58, Steps: 60 | Train Loss: 0.4812609 Vali Loss: 1.5122349 Test Loss: 0.4536557
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.9361209869384766
Epoch: 59, Steps: 60 | Train Loss: 0.4811200 Vali Loss: 1.5123676 Test Loss: 0.4535390
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.3063509464263916
Epoch: 60, Steps: 60 | Train Loss: 0.4812169 Vali Loss: 1.5176069 Test Loss: 0.4534331
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.056527853012085
Epoch: 61, Steps: 60 | Train Loss: 0.4813474 Vali Loss: 1.5098035 Test Loss: 0.4533365
Validation loss decreased (1.510517 --> 1.509804).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.9880645275115967
Epoch: 62, Steps: 60 | Train Loss: 0.4809409 Vali Loss: 1.5076754 Test Loss: 0.4532410
Validation loss decreased (1.509804 --> 1.507675).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.9899840354919434
Epoch: 63, Steps: 60 | Train Loss: 0.4805048 Vali Loss: 1.5119262 Test Loss: 0.4531445
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.6812593936920166
Epoch: 64, Steps: 60 | Train Loss: 0.4804850 Vali Loss: 1.5126100 Test Loss: 0.4530600
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.7261948585510254
Epoch: 65, Steps: 60 | Train Loss: 0.4811312 Vali Loss: 1.5103161 Test Loss: 0.4529735
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.1742162704467773
Epoch: 66, Steps: 60 | Train Loss: 0.4808838 Vali Loss: 1.5125386 Test Loss: 0.4528888
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.5087175369262695
Epoch: 67, Steps: 60 | Train Loss: 0.4805801 Vali Loss: 1.5151107 Test Loss: 0.4528206
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.7966115474700928
Epoch: 68, Steps: 60 | Train Loss: 0.4803766 Vali Loss: 1.5091321 Test Loss: 0.4527433
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.816103935241699
Epoch: 69, Steps: 60 | Train Loss: 0.4802553 Vali Loss: 1.5133684 Test Loss: 0.4526807
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.2602500915527344
Epoch: 70, Steps: 60 | Train Loss: 0.4801983 Vali Loss: 1.5114999 Test Loss: 0.4526139
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.2617340087890625
Epoch: 71, Steps: 60 | Train Loss: 0.4805008 Vali Loss: 1.5123457 Test Loss: 0.4525559
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.5203869342803955
Epoch: 72, Steps: 60 | Train Loss: 0.4802142 Vali Loss: 1.5095541 Test Loss: 0.4524945
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.1800451278686523
Epoch: 73, Steps: 60 | Train Loss: 0.4804290 Vali Loss: 1.5133463 Test Loss: 0.4524420
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.2456467151641846
Epoch: 74, Steps: 60 | Train Loss: 0.4802530 Vali Loss: 1.5096307 Test Loss: 0.4523892
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.8818066120147705
Epoch: 75, Steps: 60 | Train Loss: 0.4803248 Vali Loss: 1.5071774 Test Loss: 0.4523385
Validation loss decreased (1.507675 --> 1.507177).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.1545872688293457
Epoch: 76, Steps: 60 | Train Loss: 0.4802516 Vali Loss: 1.5086509 Test Loss: 0.4522950
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.766047954559326
Epoch: 77, Steps: 60 | Train Loss: 0.4802392 Vali Loss: 1.5138402 Test Loss: 0.4522491
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.573772668838501
Epoch: 78, Steps: 60 | Train Loss: 0.4804397 Vali Loss: 1.5196906 Test Loss: 0.4522110
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 3.6111814975738525
Epoch: 79, Steps: 60 | Train Loss: 0.4801452 Vali Loss: 1.5131099 Test Loss: 0.4521675
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.7243759632110596
Epoch: 80, Steps: 60 | Train Loss: 0.4803768 Vali Loss: 1.5147889 Test Loss: 0.4521330
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.1952309608459473
Epoch: 81, Steps: 60 | Train Loss: 0.4801601 Vali Loss: 1.5111679 Test Loss: 0.4520971
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 4.162421703338623
Epoch: 82, Steps: 60 | Train Loss: 0.4803647 Vali Loss: 1.5099106 Test Loss: 0.4520609
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.7792584896087646
Epoch: 83, Steps: 60 | Train Loss: 0.4802914 Vali Loss: 1.5144777 Test Loss: 0.4520314
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.319202423095703
Epoch: 84, Steps: 60 | Train Loss: 0.4798278 Vali Loss: 1.5058523 Test Loss: 0.4520014
Validation loss decreased (1.507177 --> 1.505852).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.623152256011963
Epoch: 85, Steps: 60 | Train Loss: 0.4799709 Vali Loss: 1.5091879 Test Loss: 0.4519716
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.4972360134124756
Epoch: 86, Steps: 60 | Train Loss: 0.4800150 Vali Loss: 1.5152587 Test Loss: 0.4519437
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.5270824432373047
Epoch: 87, Steps: 60 | Train Loss: 0.4802658 Vali Loss: 1.5142121 Test Loss: 0.4519182
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.1316537857055664
Epoch: 88, Steps: 60 | Train Loss: 0.4802149 Vali Loss: 1.5104039 Test Loss: 0.4518917
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.2797415256500244
Epoch: 89, Steps: 60 | Train Loss: 0.4798976 Vali Loss: 1.5098150 Test Loss: 0.4518696
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.387831211090088
Epoch: 90, Steps: 60 | Train Loss: 0.4801338 Vali Loss: 1.5129672 Test Loss: 0.4518449
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.3689465522766113
Epoch: 91, Steps: 60 | Train Loss: 0.4795161 Vali Loss: 1.5135944 Test Loss: 0.4518258
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.3381900787353516
Epoch: 92, Steps: 60 | Train Loss: 0.4799965 Vali Loss: 1.5118916 Test Loss: 0.4518049
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.7831649780273438
Epoch: 93, Steps: 60 | Train Loss: 0.4792923 Vali Loss: 1.5095931 Test Loss: 0.4517884
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.850222587585449
Epoch: 94, Steps: 60 | Train Loss: 0.4798841 Vali Loss: 1.5149091 Test Loss: 0.4517685
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 3.9505157470703125
Epoch: 95, Steps: 60 | Train Loss: 0.4798633 Vali Loss: 1.5167272 Test Loss: 0.4517513
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 4.381622552871704
Epoch: 96, Steps: 60 | Train Loss: 0.4800914 Vali Loss: 1.5140342 Test Loss: 0.4517353
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.275439500808716
Epoch: 97, Steps: 60 | Train Loss: 0.4794960 Vali Loss: 1.5106285 Test Loss: 0.4517207
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.6894068717956543
Epoch: 98, Steps: 60 | Train Loss: 0.4797411 Vali Loss: 1.5126723 Test Loss: 0.4517060
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 3.209357738494873
Epoch: 99, Steps: 60 | Train Loss: 0.4796656 Vali Loss: 1.5090406 Test Loss: 0.4516917
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.8770132064819336
Epoch: 100, Steps: 60 | Train Loss: 0.4799832 Vali Loss: 1.5092485 Test Loss: 0.4516799
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=50, out_features=250, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11200000.0
params:  12750.0
Trainable parameters:  12750
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.583804130554199
Epoch: 1, Steps: 60 | Train Loss: 0.5914783 Vali Loss: 1.5086205 Test Loss: 0.4465680
Validation loss decreased (inf --> 1.508621).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.5770509243011475
Epoch: 2, Steps: 60 | Train Loss: 0.5891093 Vali Loss: 1.4994991 Test Loss: 0.4436489
Validation loss decreased (1.508621 --> 1.499499).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.262036085128784
Epoch: 3, Steps: 60 | Train Loss: 0.5876405 Vali Loss: 1.4952545 Test Loss: 0.4423923
Validation loss decreased (1.499499 --> 1.495255).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9271447658538818
Epoch: 4, Steps: 60 | Train Loss: 0.5866972 Vali Loss: 1.4899215 Test Loss: 0.4414936
Validation loss decreased (1.495255 --> 1.489921).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.2142772674560547
Epoch: 5, Steps: 60 | Train Loss: 0.5854571 Vali Loss: 1.4960146 Test Loss: 0.4412057
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.173680305480957
Epoch: 6, Steps: 60 | Train Loss: 0.5854821 Vali Loss: 1.4956397 Test Loss: 0.4411623
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.6040961742401123
Epoch: 7, Steps: 60 | Train Loss: 0.5856754 Vali Loss: 1.4941363 Test Loss: 0.4413629
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.485023260116577
Epoch: 8, Steps: 60 | Train Loss: 0.5859117 Vali Loss: 1.4936938 Test Loss: 0.4412634
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.6248624324798584
Epoch: 9, Steps: 60 | Train Loss: 0.5850313 Vali Loss: 1.4971937 Test Loss: 0.4414240
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.9286859035491943
Epoch: 10, Steps: 60 | Train Loss: 0.5856441 Vali Loss: 1.4918578 Test Loss: 0.4414965
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.9255905151367188
Epoch: 11, Steps: 60 | Train Loss: 0.5851896 Vali Loss: 1.4971712 Test Loss: 0.4415896
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.0097429752349854
Epoch: 12, Steps: 60 | Train Loss: 0.5851722 Vali Loss: 1.4867556 Test Loss: 0.4416349
Validation loss decreased (1.489921 --> 1.486756).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.0537502765655518
Epoch: 13, Steps: 60 | Train Loss: 0.5852185 Vali Loss: 1.4921744 Test Loss: 0.4417000
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.2847704887390137
Epoch: 14, Steps: 60 | Train Loss: 0.5849158 Vali Loss: 1.4895852 Test Loss: 0.4418461
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.175931930541992
Epoch: 15, Steps: 60 | Train Loss: 0.5841877 Vali Loss: 1.4928361 Test Loss: 0.4419423
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.009196996688843
Epoch: 16, Steps: 60 | Train Loss: 0.5848261 Vali Loss: 1.4911535 Test Loss: 0.4419083
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.22477126121521
Epoch: 17, Steps: 60 | Train Loss: 0.5846155 Vali Loss: 1.4961386 Test Loss: 0.4420602
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.324833869934082
Epoch: 18, Steps: 60 | Train Loss: 0.5845957 Vali Loss: 1.4868140 Test Loss: 0.4420077
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.03263783454895
Epoch: 19, Steps: 60 | Train Loss: 0.5845990 Vali Loss: 1.4919960 Test Loss: 0.4420355
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.9535346031188965
Epoch: 20, Steps: 60 | Train Loss: 0.5849584 Vali Loss: 1.4912872 Test Loss: 0.4420381
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.9981207847595215
Epoch: 21, Steps: 60 | Train Loss: 0.5843920 Vali Loss: 1.4858088 Test Loss: 0.4421099
Validation loss decreased (1.486756 --> 1.485809).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.93845796585083
Epoch: 22, Steps: 60 | Train Loss: 0.5850804 Vali Loss: 1.4879007 Test Loss: 0.4421722
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.6173765659332275
Epoch: 23, Steps: 60 | Train Loss: 0.5851472 Vali Loss: 1.4858085 Test Loss: 0.4422171
Validation loss decreased (1.485809 --> 1.485808).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.9189023971557617
Epoch: 24, Steps: 60 | Train Loss: 0.5843915 Vali Loss: 1.4992913 Test Loss: 0.4422918
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.242746591567993
Epoch: 25, Steps: 60 | Train Loss: 0.5847179 Vali Loss: 1.4890840 Test Loss: 0.4422520
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.6140918731689453
Epoch: 26, Steps: 60 | Train Loss: 0.5848115 Vali Loss: 1.4964819 Test Loss: 0.4422739
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.522068738937378
Epoch: 27, Steps: 60 | Train Loss: 0.5849111 Vali Loss: 1.4935861 Test Loss: 0.4422919
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.1073834896087646
Epoch: 28, Steps: 60 | Train Loss: 0.5841932 Vali Loss: 1.4925231 Test Loss: 0.4423189
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.7849676609039307
Epoch: 29, Steps: 60 | Train Loss: 0.5845779 Vali Loss: 1.4958122 Test Loss: 0.4423730
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.061788320541382
Epoch: 30, Steps: 60 | Train Loss: 0.5848835 Vali Loss: 1.4886317 Test Loss: 0.4423889
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.790848970413208
Epoch: 31, Steps: 60 | Train Loss: 0.5843658 Vali Loss: 1.4876351 Test Loss: 0.4423544
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.93906569480896
Epoch: 32, Steps: 60 | Train Loss: 0.5849368 Vali Loss: 1.4887593 Test Loss: 0.4423645
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.637874126434326
Epoch: 33, Steps: 60 | Train Loss: 0.5846700 Vali Loss: 1.4877334 Test Loss: 0.4424319
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.8235998153686523
Epoch: 34, Steps: 60 | Train Loss: 0.5840865 Vali Loss: 1.4964505 Test Loss: 0.4423660
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.9785375595092773
Epoch: 35, Steps: 60 | Train Loss: 0.5846235 Vali Loss: 1.4924766 Test Loss: 0.4424028
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.1741034984588623
Epoch: 36, Steps: 60 | Train Loss: 0.5842670 Vali Loss: 1.4950111 Test Loss: 0.4424671
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.2991297245025635
Epoch: 37, Steps: 60 | Train Loss: 0.5844613 Vali Loss: 1.4936445 Test Loss: 0.4424818
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.3772435188293457
Epoch: 38, Steps: 60 | Train Loss: 0.5840605 Vali Loss: 1.4916461 Test Loss: 0.4425170
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.7260777950286865
Epoch: 39, Steps: 60 | Train Loss: 0.5848998 Vali Loss: 1.4872599 Test Loss: 0.4424851
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.3170669078826904
Epoch: 40, Steps: 60 | Train Loss: 0.5846789 Vali Loss: 1.4857142 Test Loss: 0.4425197
Validation loss decreased (1.485808 --> 1.485714).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.3582732677459717
Epoch: 41, Steps: 60 | Train Loss: 0.5848587 Vali Loss: 1.4861591 Test Loss: 0.4425088
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.800915002822876
Epoch: 42, Steps: 60 | Train Loss: 0.5845500 Vali Loss: 1.4885280 Test Loss: 0.4425409
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.3529486656188965
Epoch: 43, Steps: 60 | Train Loss: 0.5845659 Vali Loss: 1.4952254 Test Loss: 0.4425755
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.4317121505737305
Epoch: 44, Steps: 60 | Train Loss: 0.5839954 Vali Loss: 1.4959990 Test Loss: 0.4425793
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.633765459060669
Epoch: 45, Steps: 60 | Train Loss: 0.5846859 Vali Loss: 1.4892951 Test Loss: 0.4425428
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.040794849395752
Epoch: 46, Steps: 60 | Train Loss: 0.5844081 Vali Loss: 1.4901190 Test Loss: 0.4425690
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.788491249084473
Epoch: 47, Steps: 60 | Train Loss: 0.5845963 Vali Loss: 1.4908996 Test Loss: 0.4425681
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.04213547706604
Epoch: 48, Steps: 60 | Train Loss: 0.5846824 Vali Loss: 1.4930334 Test Loss: 0.4425790
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9077446460723877
Epoch: 49, Steps: 60 | Train Loss: 0.5843990 Vali Loss: 1.4923768 Test Loss: 0.4425820
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.978585958480835
Epoch: 50, Steps: 60 | Train Loss: 0.5843790 Vali Loss: 1.4927789 Test Loss: 0.4426102
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.0419528484344482
Epoch: 51, Steps: 60 | Train Loss: 0.5849157 Vali Loss: 1.4915996 Test Loss: 0.4426062
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.288299798965454
Epoch: 52, Steps: 60 | Train Loss: 0.5839198 Vali Loss: 1.4930573 Test Loss: 0.4426036
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.8725383281707764
Epoch: 53, Steps: 60 | Train Loss: 0.5846151 Vali Loss: 1.4877990 Test Loss: 0.4426292
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.422189474105835
Epoch: 54, Steps: 60 | Train Loss: 0.5845752 Vali Loss: 1.4891682 Test Loss: 0.4426160
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.2508363723754883
Epoch: 55, Steps: 60 | Train Loss: 0.5845694 Vali Loss: 1.4837726 Test Loss: 0.4426383
Validation loss decreased (1.485714 --> 1.483773).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.6799724102020264
Epoch: 56, Steps: 60 | Train Loss: 0.5841402 Vali Loss: 1.4897674 Test Loss: 0.4426253
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.3510985374450684
Epoch: 57, Steps: 60 | Train Loss: 0.5845673 Vali Loss: 1.4918661 Test Loss: 0.4426401
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.436965227127075
Epoch: 58, Steps: 60 | Train Loss: 0.5838604 Vali Loss: 1.4928794 Test Loss: 0.4426349
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.801196813583374
Epoch: 59, Steps: 60 | Train Loss: 0.5846536 Vali Loss: 1.4932704 Test Loss: 0.4426729
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.0429229736328125
Epoch: 60, Steps: 60 | Train Loss: 0.5844322 Vali Loss: 1.4911836 Test Loss: 0.4426616
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 4.233407258987427
Epoch: 61, Steps: 60 | Train Loss: 0.5840713 Vali Loss: 1.4875579 Test Loss: 0.4426664
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.3048572540283203
Epoch: 62, Steps: 60 | Train Loss: 0.5843427 Vali Loss: 1.4920282 Test Loss: 0.4426656
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.2288734912872314
Epoch: 63, Steps: 60 | Train Loss: 0.5839252 Vali Loss: 1.4905138 Test Loss: 0.4426749
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.514230251312256
Epoch: 64, Steps: 60 | Train Loss: 0.5847284 Vali Loss: 1.4938874 Test Loss: 0.4426766
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.8723609447479248
Epoch: 65, Steps: 60 | Train Loss: 0.5843920 Vali Loss: 1.4870582 Test Loss: 0.4426829
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.9511504173278809
Epoch: 66, Steps: 60 | Train Loss: 0.5843679 Vali Loss: 1.4884524 Test Loss: 0.4426742
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.9383728504180908
Epoch: 67, Steps: 60 | Train Loss: 0.5842178 Vali Loss: 1.4956304 Test Loss: 0.4426848
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.8979179859161377
Epoch: 68, Steps: 60 | Train Loss: 0.5840961 Vali Loss: 1.4822866 Test Loss: 0.4426751
Validation loss decreased (1.483773 --> 1.482287).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.8486099243164062
Epoch: 69, Steps: 60 | Train Loss: 0.5842991 Vali Loss: 1.4936452 Test Loss: 0.4426894
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.331164836883545
Epoch: 70, Steps: 60 | Train Loss: 0.5842181 Vali Loss: 1.4875845 Test Loss: 0.4426849
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.02666974067688
Epoch: 71, Steps: 60 | Train Loss: 0.5842098 Vali Loss: 1.4816253 Test Loss: 0.4426948
Validation loss decreased (1.482287 --> 1.481625).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.550126314163208
Epoch: 72, Steps: 60 | Train Loss: 0.5842969 Vali Loss: 1.4868439 Test Loss: 0.4426863
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.5791220664978027
Epoch: 73, Steps: 60 | Train Loss: 0.5844112 Vali Loss: 1.4956564 Test Loss: 0.4426982
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.152787208557129
Epoch: 74, Steps: 60 | Train Loss: 0.5850009 Vali Loss: 1.4880579 Test Loss: 0.4427033
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 4.172080039978027
Epoch: 75, Steps: 60 | Train Loss: 0.5848204 Vali Loss: 1.4852130 Test Loss: 0.4426999
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.8219945430755615
Epoch: 76, Steps: 60 | Train Loss: 0.5840758 Vali Loss: 1.4844730 Test Loss: 0.4427095
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.730196714401245
Epoch: 77, Steps: 60 | Train Loss: 0.5839901 Vali Loss: 1.4883652 Test Loss: 0.4427025
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.1979949474334717
Epoch: 78, Steps: 60 | Train Loss: 0.5844606 Vali Loss: 1.4918660 Test Loss: 0.4427036
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 3.180856227874756
Epoch: 79, Steps: 60 | Train Loss: 0.5843842 Vali Loss: 1.4895966 Test Loss: 0.4427118
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.5927488803863525
Epoch: 80, Steps: 60 | Train Loss: 0.5843548 Vali Loss: 1.4915644 Test Loss: 0.4427123
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.4471435546875
Epoch: 81, Steps: 60 | Train Loss: 0.5845020 Vali Loss: 1.4856126 Test Loss: 0.4427049
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.288846492767334
Epoch: 82, Steps: 60 | Train Loss: 0.5845171 Vali Loss: 1.4856493 Test Loss: 0.4427112
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.9100847244262695
Epoch: 83, Steps: 60 | Train Loss: 0.5846458 Vali Loss: 1.4867949 Test Loss: 0.4427105
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.2575085163116455
Epoch: 84, Steps: 60 | Train Loss: 0.5842783 Vali Loss: 1.4883430 Test Loss: 0.4427141
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.0988259315490723
Epoch: 85, Steps: 60 | Train Loss: 0.5841291 Vali Loss: 1.4904273 Test Loss: 0.4427130
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.7536556720733643
Epoch: 86, Steps: 60 | Train Loss: 0.5847878 Vali Loss: 1.4935158 Test Loss: 0.4427116
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.036226987838745
Epoch: 87, Steps: 60 | Train Loss: 0.5843813 Vali Loss: 1.4950695 Test Loss: 0.4427135
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.8199193477630615
Epoch: 88, Steps: 60 | Train Loss: 0.5845759 Vali Loss: 1.4869066 Test Loss: 0.4427157
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.8491146564483643
Epoch: 89, Steps: 60 | Train Loss: 0.5840989 Vali Loss: 1.4876342 Test Loss: 0.4427193
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.490739583969116
Epoch: 90, Steps: 60 | Train Loss: 0.5845810 Vali Loss: 1.4923935 Test Loss: 0.4427167
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.436126947402954
Epoch: 91, Steps: 60 | Train Loss: 0.5846277 Vali Loss: 1.4915390 Test Loss: 0.4427162
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4415724277496338, mae:0.45077070593833923, rse:0.6361403465270996, corr:[0.22750017 0.23457643 0.234678   0.2337503  0.23219728 0.22973794
 0.22936605 0.23099211 0.23081067 0.23073986 0.23095132 0.230781
 0.23064274 0.23030399 0.22976351 0.22947082 0.2295559  0.22941563
 0.22896762 0.22865461 0.22868668 0.22908914 0.22964948 0.22978595
 0.22913623 0.22868031 0.22823238 0.22789052 0.22750287 0.22709958
 0.22667088 0.22643588 0.22609392 0.22584942 0.22592251 0.22639039
 0.22676839 0.22659528 0.22658636 0.226528   0.22649969 0.22660989
 0.2268155  0.22687663 0.22697371 0.22742443 0.22836642 0.22873724
 0.22777349 0.22664104 0.22524865 0.22401974 0.22297719 0.22197641
 0.22127356 0.22103101 0.22103617 0.22127154 0.22099933 0.22152738
 0.2221441  0.22176388 0.22142847 0.22141695 0.22131784 0.22125204
 0.22145274 0.2214552  0.22124869 0.22121209 0.22144258 0.22111021
 0.21988483 0.21898617 0.21809117 0.21764094 0.21760677 0.21751012
 0.21744885 0.21724194 0.2169523  0.21703029 0.2166084  0.21644579
 0.21681915 0.21667469 0.21650577 0.21638529 0.21606658 0.215758
 0.21559635 0.21545883 0.2154403  0.21580426 0.21664926 0.21734911
 0.21727222 0.21721728 0.21709773 0.21670315 0.2165245  0.21637686
 0.21601452 0.21619187 0.21631612 0.21640438 0.21622074 0.2164758
 0.21689443 0.21668383 0.21636379 0.21629177 0.21638311 0.21634397
 0.21626271 0.21626963 0.21637502 0.21640337 0.2164793  0.21626332
 0.21545589 0.2147549  0.21365833 0.21275373 0.21214859 0.21187845
 0.21181333 0.21209979 0.21224304 0.21232335 0.21213502 0.21265523
 0.21351449 0.21343906 0.21327001 0.21330462 0.21324636 0.2131114
 0.21319488 0.2132662  0.21320255 0.21337207 0.21374276 0.2136772
 0.21301809 0.21242452 0.21168841 0.21052471 0.20969145 0.2093192
 0.20922382 0.20922732 0.20955205 0.2099746  0.20989108 0.21008787
 0.21066652 0.21055073 0.21035305 0.21016872 0.20996182 0.20979434
 0.20980538 0.20987082 0.20992269 0.21015136 0.21048737 0.21046253
 0.2098827  0.20961338 0.20932841 0.20901772 0.20882098 0.20856996
 0.2084996  0.20897585 0.2094196  0.20994125 0.21028545 0.21084455
 0.21147355 0.2114436  0.21130905 0.21121138 0.21106853 0.21115823
 0.21138488 0.21147186 0.21163622 0.211938   0.21222213 0.21194394
 0.210901   0.21022466 0.2094873  0.20857717 0.20776159 0.2072468
 0.20702945 0.2071304  0.20739497 0.20770715 0.2076729  0.20822433
 0.20895211 0.20902303 0.20886143 0.20860876 0.2083301  0.20820576
 0.20814052 0.20794863 0.20781673 0.20802243 0.20825821 0.20796742
 0.2071733  0.20686539 0.2065447  0.2060053  0.20564808 0.20525217
 0.20506883 0.20517373 0.20539075 0.20547305 0.20522174 0.20542338
 0.20601995 0.20592746 0.20577167 0.20573767 0.2054677  0.20507045
 0.20488268 0.20483884 0.20474756 0.20474271 0.20496404 0.20489626
 0.20431897 0.20393066 0.20355193 0.20332849 0.20328109 0.20319606
 0.2032706  0.2034618  0.20367905 0.20396376 0.20412797 0.20469928
 0.20548052 0.2056921  0.205819   0.20580302 0.20563842 0.20555721
 0.20559342 0.20538847 0.20517744 0.20516336 0.20530836 0.20518824
 0.20459428 0.2043218  0.20387739 0.20342268 0.203239   0.2030895
 0.20298678 0.20315218 0.20324938 0.20339082 0.20334566 0.20350803
 0.20414686 0.20422728 0.20409815 0.20391622 0.20360449 0.20337583
 0.20336923 0.2034061  0.20345892 0.20374724 0.20417465 0.20433092
 0.20410226 0.20415853 0.2043257  0.20433532 0.20432478 0.20451647
 0.20475632 0.20500293 0.20529358 0.20557301 0.20553052 0.20577548
 0.20638965 0.20646231 0.20646821 0.20653673 0.2064768  0.20626892
 0.20625766 0.20634475 0.20630564 0.20636813 0.20652628 0.20650327
 0.20609182 0.20594646 0.20571531 0.20527731 0.20501393 0.20457898
 0.20427714 0.2043316  0.20437333 0.20432195 0.20413442 0.2044424
 0.20514666 0.20530643 0.20541716 0.20560852 0.20564382 0.20560648
 0.20563665 0.20568125 0.2055488  0.20557088 0.20576106 0.20559843
 0.20494409 0.20466317 0.20436375 0.20398276 0.2035932  0.20331353
 0.20324838 0.20327817 0.20343065 0.20362179 0.20376828 0.20427582
 0.20485306 0.20491588 0.2049676  0.20507193 0.20489638 0.20473245
 0.20478664 0.20486166 0.20480609 0.20491754 0.20486265 0.20457013
 0.20409887 0.20394738 0.20368646 0.20322256 0.20282413 0.20256168
 0.20247148 0.20237504 0.20223401 0.20197617 0.2015349  0.20141248
 0.20174001 0.20156921 0.20129591 0.20110227 0.20096272 0.20095284
 0.2012411  0.20161535 0.20174487 0.20206144 0.20258015 0.20319091
 0.20353    0.20397659 0.20413634 0.20397243 0.20367569 0.20309159
 0.20271897 0.20257534 0.20235452 0.20222414 0.20198871 0.20247838
 0.2033905  0.20353669 0.20364375 0.20365225 0.20348926 0.20362574
 0.20398203 0.20435041 0.20459713 0.2050057  0.20546582 0.20568372
 0.20554683 0.20563324 0.20558292 0.205202   0.20503454 0.20490627
 0.20490465 0.20511156 0.20537029 0.20545107 0.20540899 0.2057765
 0.2064041  0.20639403 0.20620084 0.2060084  0.2057197  0.20563066
 0.20585327 0.20622532 0.20640796 0.2069965  0.2077806  0.20823115
 0.2080462  0.2079305  0.20761982 0.2071414  0.20704436 0.20697336
 0.20689014 0.20707345 0.20731303 0.20756963 0.20752743 0.2079063
 0.20856273 0.20854823 0.20833269 0.20817073 0.20796464 0.20789327
 0.208022   0.20816661 0.20829982 0.20872186 0.20934153 0.2098345
 0.20980611 0.20992751 0.2096528  0.20900722 0.20851745 0.20807295
 0.20776449 0.20769446 0.20789999 0.20824808 0.2086137  0.20933598
 0.21009828 0.21017636 0.21023041 0.21032698 0.21033257 0.210325
 0.2103799  0.2104627  0.21050619 0.21055041 0.21076226 0.21071461
 0.21022487 0.20988111 0.20924175 0.20852192 0.2083009  0.20819066
 0.20822525 0.20842358 0.20872411 0.20902395 0.2090103  0.20933121
 0.209935   0.20986257 0.20978999 0.20984127 0.20963535 0.20943117
 0.2095809  0.20988418 0.21012242 0.2106384  0.21143739 0.21202213
 0.21203916 0.21221365 0.21213835 0.21181446 0.21167424 0.21136825
 0.21092062 0.21054798 0.21058148 0.21050769 0.21026485 0.21061277
 0.21136315 0.21145232 0.21156952 0.21192661 0.2121942  0.21226674
 0.2124425  0.21283008 0.2132113  0.2137253  0.21443677 0.21469934
 0.21408436 0.21364337 0.21316192 0.21261758 0.21209002 0.21145386
 0.21096158 0.21087267 0.21100816 0.21122696 0.21121165 0.21149342
 0.21211489 0.2121837  0.21233469 0.2126391  0.21277148 0.21278906
 0.21283613 0.21288326 0.21279192 0.21267411 0.21270598 0.21231262
 0.2111436  0.2104961  0.21009772 0.20937331 0.20861287 0.20813268
 0.20784815 0.2076309  0.20786779 0.20815754 0.20799176 0.20825277
 0.20912096 0.20900865 0.20880754 0.20868467 0.20837589 0.20809078
 0.2081353  0.20802678 0.20784102 0.20789617 0.20823976 0.20810059
 0.20709987 0.20639932 0.20569028 0.20493503 0.20447204 0.20382223
 0.20330831 0.20295495 0.20293702 0.20309144 0.20287684 0.20292848
 0.2031277  0.20269153 0.2021753  0.20193635 0.20172662 0.20151931
 0.20150614 0.2015778  0.2014169  0.20127241 0.20128186 0.20089717
 0.19970624 0.1988467  0.19824992 0.19731443 0.19637159 0.1957468
 0.19541475 0.19526047 0.19514202 0.19510333 0.19509138 0.19531387
 0.1957961  0.19555636 0.19530334 0.19529724 0.19504787 0.19466189
 0.1944544  0.19438785 0.19432174 0.19439718 0.19451074 0.19410563
 0.19314022 0.1924995  0.19183117 0.19076462 0.19011821 0.18982373
 0.18971147 0.18946172 0.18932801 0.18960468 0.18958353 0.18988585
 0.19067198 0.19053724 0.1901619  0.19005187 0.1897552  0.18936597
 0.1892162  0.1893226  0.18931623 0.18939137 0.18961206 0.18944661
 0.18840824 0.18746217 0.18654779 0.18572024 0.18524194 0.18500867
 0.18497667 0.18480426 0.18481779 0.18506658 0.18518375 0.18543135
 0.18595432 0.1858701  0.18558116 0.18537073 0.18530303 0.18524231
 0.18523197 0.18528941 0.1853186  0.18544313 0.18547463 0.1850057
 0.18361454 0.18227543 0.18102925 0.17964707 0.17881048 0.1780309
 0.17755714 0.1774642  0.17794628 0.17830272 0.17841715 0.17919077
 0.18022566 0.17994678 0.17952336 0.17949803 0.17935583 0.179081
 0.17903097 0.1791659  0.17921737 0.17940944 0.1797609  0.17958944
 0.17877728 0.1784027  0.17784967 0.17690355 0.17619938 0.17578262
 0.17540084 0.17513268 0.17525661 0.1752556  0.17467691 0.17455722
 0.17521206 0.17463604 0.17376204 0.17350566 0.17353468 0.1730294
 0.17266637 0.17303634 0.1732813  0.17275697 0.17287846 0.17305118]
