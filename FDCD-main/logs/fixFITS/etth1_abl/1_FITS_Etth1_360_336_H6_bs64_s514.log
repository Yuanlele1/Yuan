Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=514, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=106, out_features=204, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19375104.0
params:  21828.0
Trainable parameters:  21828
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7138454914093018
Epoch: 1, Steps: 62 | Train Loss: 0.7429280 Vali Loss: 1.5120527 Test Loss: 0.6529566
Validation loss decreased (inf --> 1.512053).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6095905303955078
Epoch: 2, Steps: 62 | Train Loss: 0.5861134 Vali Loss: 1.3749737 Test Loss: 0.5622959
Validation loss decreased (1.512053 --> 1.374974).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.6472628116607666
Epoch: 3, Steps: 62 | Train Loss: 0.5394058 Vali Loss: 1.3114779 Test Loss: 0.5225676
Validation loss decreased (1.374974 --> 1.311478).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6855034828186035
Epoch: 4, Steps: 62 | Train Loss: 0.5146516 Vali Loss: 1.2742306 Test Loss: 0.4966250
Validation loss decreased (1.311478 --> 1.274231).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8026549816131592
Epoch: 5, Steps: 62 | Train Loss: 0.4982391 Vali Loss: 1.2414811 Test Loss: 0.4780441
Validation loss decreased (1.274231 --> 1.241481).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.7348887920379639
Epoch: 6, Steps: 62 | Train Loss: 0.4866613 Vali Loss: 1.2229966 Test Loss: 0.4645450
Validation loss decreased (1.241481 --> 1.222997).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.04887056350708
Epoch: 7, Steps: 62 | Train Loss: 0.4783342 Vali Loss: 1.2106646 Test Loss: 0.4547202
Validation loss decreased (1.222997 --> 1.210665).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.748767375946045
Epoch: 8, Steps: 62 | Train Loss: 0.4722807 Vali Loss: 1.1976962 Test Loss: 0.4474837
Validation loss decreased (1.210665 --> 1.197696).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.8393816947937012
Epoch: 9, Steps: 62 | Train Loss: 0.4677699 Vali Loss: 1.1910391 Test Loss: 0.4422823
Validation loss decreased (1.197696 --> 1.191039).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.0269033908843994
Epoch: 10, Steps: 62 | Train Loss: 0.4644545 Vali Loss: 1.1832943 Test Loss: 0.4384801
Validation loss decreased (1.191039 --> 1.183294).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.809218168258667
Epoch: 11, Steps: 62 | Train Loss: 0.4620167 Vali Loss: 1.1797123 Test Loss: 0.4356975
Validation loss decreased (1.183294 --> 1.179712).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.8118112087249756
Epoch: 12, Steps: 62 | Train Loss: 0.4601317 Vali Loss: 1.1753737 Test Loss: 0.4337747
Validation loss decreased (1.179712 --> 1.175374).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.760939359664917
Epoch: 13, Steps: 62 | Train Loss: 0.4587419 Vali Loss: 1.1768996 Test Loss: 0.4323516
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6569311618804932
Epoch: 14, Steps: 62 | Train Loss: 0.4576463 Vali Loss: 1.1765743 Test Loss: 0.4312608
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5070033073425293
Epoch: 15, Steps: 62 | Train Loss: 0.4568567 Vali Loss: 1.1672039 Test Loss: 0.4305763
Validation loss decreased (1.175374 --> 1.167204).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.6338133811950684
Epoch: 16, Steps: 62 | Train Loss: 0.4561544 Vali Loss: 1.1711971 Test Loss: 0.4300932
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.705453634262085
Epoch: 17, Steps: 62 | Train Loss: 0.4555964 Vali Loss: 1.1652645 Test Loss: 0.4297110
Validation loss decreased (1.167204 --> 1.165264).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6813733577728271
Epoch: 18, Steps: 62 | Train Loss: 0.4551006 Vali Loss: 1.1654602 Test Loss: 0.4294006
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4798238277435303
Epoch: 19, Steps: 62 | Train Loss: 0.4546523 Vali Loss: 1.1657226 Test Loss: 0.4292693
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.354644775390625
Epoch: 20, Steps: 62 | Train Loss: 0.4546036 Vali Loss: 1.1644927 Test Loss: 0.4291168
Validation loss decreased (1.165264 --> 1.164493).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.629561424255371
Epoch: 21, Steps: 62 | Train Loss: 0.4543529 Vali Loss: 1.1588011 Test Loss: 0.4290052
Validation loss decreased (1.164493 --> 1.158801).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.518294334411621
Epoch: 22, Steps: 62 | Train Loss: 0.4540754 Vali Loss: 1.1627299 Test Loss: 0.4289470
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.7681524753570557
Epoch: 23, Steps: 62 | Train Loss: 0.4538728 Vali Loss: 1.1659938 Test Loss: 0.4288465
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.598100185394287
Epoch: 24, Steps: 62 | Train Loss: 0.4538542 Vali Loss: 1.1647806 Test Loss: 0.4287604
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.7289018630981445
Epoch: 25, Steps: 62 | Train Loss: 0.4535186 Vali Loss: 1.1610181 Test Loss: 0.4287100
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5727424621582031
Epoch: 26, Steps: 62 | Train Loss: 0.4534832 Vali Loss: 1.1650302 Test Loss: 0.4286775
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.5352883338928223
Epoch: 27, Steps: 62 | Train Loss: 0.4534616 Vali Loss: 1.1614300 Test Loss: 0.4286596
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.64192795753479
Epoch: 28, Steps: 62 | Train Loss: 0.4532547 Vali Loss: 1.1641454 Test Loss: 0.4286669
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7722272872924805
Epoch: 29, Steps: 62 | Train Loss: 0.4531408 Vali Loss: 1.1554289 Test Loss: 0.4286489
Validation loss decreased (1.158801 --> 1.155429).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.614391565322876
Epoch: 30, Steps: 62 | Train Loss: 0.4531039 Vali Loss: 1.1602488 Test Loss: 0.4286260
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.670590877532959
Epoch: 31, Steps: 62 | Train Loss: 0.4528589 Vali Loss: 1.1617364 Test Loss: 0.4286325
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.5210256576538086
Epoch: 32, Steps: 62 | Train Loss: 0.4528649 Vali Loss: 1.1603917 Test Loss: 0.4285463
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5837054252624512
Epoch: 33, Steps: 62 | Train Loss: 0.4528982 Vali Loss: 1.1625832 Test Loss: 0.4285876
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5468969345092773
Epoch: 34, Steps: 62 | Train Loss: 0.4527518 Vali Loss: 1.1614554 Test Loss: 0.4286071
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.54398512840271
Epoch: 35, Steps: 62 | Train Loss: 0.4527304 Vali Loss: 1.1592727 Test Loss: 0.4286129
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.7363955974578857
Epoch: 36, Steps: 62 | Train Loss: 0.4527032 Vali Loss: 1.1575860 Test Loss: 0.4285935
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.6577787399291992
Epoch: 37, Steps: 62 | Train Loss: 0.4527438 Vali Loss: 1.1631504 Test Loss: 0.4285689
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.6770524978637695
Epoch: 38, Steps: 62 | Train Loss: 0.4524225 Vali Loss: 1.1599951 Test Loss: 0.4286141
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.8138840198516846
Epoch: 39, Steps: 62 | Train Loss: 0.4526069 Vali Loss: 1.1584984 Test Loss: 0.4285708
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.6961946487426758
Epoch: 40, Steps: 62 | Train Loss: 0.4526115 Vali Loss: 1.1582166 Test Loss: 0.4286022
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.643005132675171
Epoch: 41, Steps: 62 | Train Loss: 0.4525824 Vali Loss: 1.1623932 Test Loss: 0.4286180
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.616837978363037
Epoch: 42, Steps: 62 | Train Loss: 0.4525353 Vali Loss: 1.1623771 Test Loss: 0.4286159
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.6445209980010986
Epoch: 43, Steps: 62 | Train Loss: 0.4524099 Vali Loss: 1.1591015 Test Loss: 0.4285979
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.624363899230957
Epoch: 44, Steps: 62 | Train Loss: 0.4523767 Vali Loss: 1.1629852 Test Loss: 0.4285983
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.7189714908599854
Epoch: 45, Steps: 62 | Train Loss: 0.4522199 Vali Loss: 1.1600015 Test Loss: 0.4285883
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.6062750816345215
Epoch: 46, Steps: 62 | Train Loss: 0.4523381 Vali Loss: 1.1578825 Test Loss: 0.4285807
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.667337417602539
Epoch: 47, Steps: 62 | Train Loss: 0.4521448 Vali Loss: 1.1609056 Test Loss: 0.4286269
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.6388723850250244
Epoch: 48, Steps: 62 | Train Loss: 0.4523822 Vali Loss: 1.1635118 Test Loss: 0.4285761
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.712524175643921
Epoch: 49, Steps: 62 | Train Loss: 0.4521941 Vali Loss: 1.1641390 Test Loss: 0.4285975
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4274795651435852, mae:0.4273194670677185, rse:0.6224575042724609, corr:[0.2504923  0.25786635 0.25711593 0.260394   0.25718206 0.25421932
 0.25530398 0.2556274  0.2541604  0.25403765 0.25436306 0.25350264
 0.2528207  0.25278157 0.25241148 0.25208753 0.25212994 0.25182402
 0.2516132  0.25192952 0.25208887 0.25178918 0.2522529  0.2527824
 0.25215635 0.2517806  0.25213322 0.2517018  0.2508553  0.25101033
 0.25131503 0.25053394 0.24982494 0.2501295  0.25026578 0.24979654
 0.24975723 0.2500886  0.25007415 0.25003797 0.2502846  0.25033903
 0.25026724 0.25029406 0.2500591  0.24973199 0.24988401 0.250041
 0.2495145  0.2487691  0.24820952 0.24769776 0.2471311  0.24655683
 0.24604043 0.24541502 0.24501178 0.2448811  0.24455823 0.24420297
 0.24399273 0.2440451  0.24402528 0.2440384  0.24422634 0.24459335
 0.24500719 0.24498348 0.24480118 0.24483423 0.24482134 0.24443752
 0.24391866 0.24342863 0.24290211 0.24254754 0.24258302 0.24258895
 0.24203852 0.24134469 0.24113366 0.24098417 0.24054521 0.24021615
 0.2401105  0.24000081 0.2398996  0.2399599  0.23997988 0.23990396
 0.23973536 0.23934934 0.23887298 0.23882999 0.23912759 0.23962267
 0.24024728 0.24076374 0.24107443 0.24115299 0.24132119 0.24138077
 0.24116649 0.24112743 0.2411853  0.24092834 0.24046203 0.2402541
 0.24022478 0.24011469 0.24011353 0.24034409 0.24057439 0.24071322
 0.24078411 0.24052255 0.24001616 0.23966989 0.23942812 0.23924848
 0.23918183 0.23894086 0.23844402 0.23794328 0.23771971 0.23747097
 0.23729004 0.23727743 0.23691669 0.23629956 0.23600173 0.23597805
 0.23584397 0.23577592 0.23594028 0.2360757  0.23625408 0.23662785
 0.23691376 0.23669513 0.23645458 0.2365646  0.23628429 0.23574144
 0.23564272 0.23559155 0.23525105 0.23477936 0.23441277 0.23393151
 0.23366052 0.23380344 0.23392336 0.23389232 0.23394512 0.2341861
 0.23422305 0.23410167 0.23401304 0.23392156 0.23383328 0.23393859
 0.23389831 0.23363368 0.23346215 0.23328073 0.23267767 0.23238683
 0.2326727  0.23288025 0.23318191 0.23385589 0.23415004 0.23382244
 0.23379165 0.23418781 0.23419076 0.23405133 0.23425233 0.23425563
 0.23374571 0.23359667 0.23400928 0.23402905 0.23388799 0.23441556
 0.23476963 0.23440419 0.23426303 0.23433743 0.23378429 0.23315464
 0.23304579 0.23287109 0.23234268 0.23215026 0.23188701 0.2310204
 0.23046167 0.230729   0.2306767  0.23026113 0.23027836 0.2305099
 0.2303444  0.2304806  0.23093204 0.23086706 0.23069249 0.23104893
 0.23121409 0.23055698 0.23010637 0.23003414 0.2294765  0.22919601
 0.22965775 0.22961167 0.22936366 0.22943887 0.22944386 0.22896205
 0.22867975 0.22905038 0.22919755 0.22873493 0.22835985 0.22814615
 0.22767007 0.22774166 0.22832017 0.2281032  0.22764699 0.22803308
 0.22845043 0.22794324 0.22742979 0.22738032 0.2272307  0.22704408
 0.2273563  0.22734772 0.2273026  0.22763647 0.22764312 0.22728923
 0.2275081  0.22801174 0.22780801 0.22743255 0.22763433 0.22752316
 0.22687757 0.22692718 0.22755748 0.22750905 0.22748788 0.22808413
 0.22818996 0.22764356 0.2277297  0.2279895  0.22762485 0.22761592
 0.22797939 0.22758752 0.22701795 0.22713834 0.22680938 0.22597562
 0.22590505 0.2261593  0.22561282 0.2251135  0.22537704 0.22529188
 0.22482683 0.22500318 0.22522172 0.22466122 0.22442368 0.22494204
 0.22508015 0.224644   0.22481191 0.22500174 0.22462173 0.22487722
 0.22579148 0.22596404 0.22608763 0.22653575 0.22643375 0.22611974
 0.22649033 0.22681856 0.22647548 0.22650415 0.22668856 0.22612703
 0.22573298 0.22643723 0.22686741 0.22605297 0.22608896 0.22668001
 0.22639796 0.22615984 0.22686963 0.22658908 0.2254738  0.22585194
 0.22672464 0.2264012  0.22629547 0.22693363 0.22616191 0.22491643
 0.22497545 0.2248762  0.22403489 0.22442113 0.22448866 0.22321771
 0.22317089 0.22393493 0.22259825 0.22188145 0.22383063 0.22349496
 0.22067018 0.22188531 0.22278942 0.21736862 0.2202043  0.2233145 ]
