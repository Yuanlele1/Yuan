Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=103, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=103, out_features=130, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11997440.0
params:  13520.0
Trainable parameters:  13520
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.199156999588013
Epoch: 1, Steps: 60 | Train Loss: 0.6708983 Vali Loss: 1.2334956 Test Loss: 0.5755963
Validation loss decreased (inf --> 1.233496).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.298564434051514
Epoch: 2, Steps: 60 | Train Loss: 0.4977540 Vali Loss: 1.0774504 Test Loss: 0.4801382
Validation loss decreased (1.233496 --> 1.077450).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.697950124740601
Epoch: 3, Steps: 60 | Train Loss: 0.4467941 Vali Loss: 1.0212977 Test Loss: 0.4450198
Validation loss decreased (1.077450 --> 1.021298).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.207003355026245
Epoch: 4, Steps: 60 | Train Loss: 0.4248502 Vali Loss: 0.9953027 Test Loss: 0.4311429
Validation loss decreased (1.021298 --> 0.995303).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.473666429519653
Epoch: 5, Steps: 60 | Train Loss: 0.4143520 Vali Loss: 0.9840177 Test Loss: 0.4260045
Validation loss decreased (0.995303 --> 0.984018).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.433957099914551
Epoch: 6, Steps: 60 | Train Loss: 0.4082034 Vali Loss: 0.9776656 Test Loss: 0.4239839
Validation loss decreased (0.984018 --> 0.977666).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.515264511108398
Epoch: 7, Steps: 60 | Train Loss: 0.4041715 Vali Loss: 0.9751495 Test Loss: 0.4233285
Validation loss decreased (0.977666 --> 0.975150).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.393263101577759
Epoch: 8, Steps: 60 | Train Loss: 0.4015809 Vali Loss: 0.9739321 Test Loss: 0.4233106
Validation loss decreased (0.975150 --> 0.973932).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.504998207092285
Epoch: 9, Steps: 60 | Train Loss: 0.3997099 Vali Loss: 0.9728725 Test Loss: 0.4232318
Validation loss decreased (0.973932 --> 0.972872).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.510793447494507
Epoch: 10, Steps: 60 | Train Loss: 0.3982369 Vali Loss: 0.9710905 Test Loss: 0.4231427
Validation loss decreased (0.972872 --> 0.971090).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.913167715072632
Epoch: 11, Steps: 60 | Train Loss: 0.3967283 Vali Loss: 0.9712088 Test Loss: 0.4233858
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.441980600357056
Epoch: 12, Steps: 60 | Train Loss: 0.3957520 Vali Loss: 0.9705058 Test Loss: 0.4234191
Validation loss decreased (0.971090 --> 0.970506).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.230260610580444
Epoch: 13, Steps: 60 | Train Loss: 0.3947525 Vali Loss: 0.9700605 Test Loss: 0.4234646
Validation loss decreased (0.970506 --> 0.970060).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.704729080200195
Epoch: 14, Steps: 60 | Train Loss: 0.3938060 Vali Loss: 0.9698876 Test Loss: 0.4234464
Validation loss decreased (0.970060 --> 0.969888).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.519449234008789
Epoch: 15, Steps: 60 | Train Loss: 0.3931990 Vali Loss: 0.9696319 Test Loss: 0.4237105
Validation loss decreased (0.969888 --> 0.969632).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.336827278137207
Epoch: 16, Steps: 60 | Train Loss: 0.3930209 Vali Loss: 0.9692172 Test Loss: 0.4238474
Validation loss decreased (0.969632 --> 0.969217).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.763065576553345
Epoch: 17, Steps: 60 | Train Loss: 0.3922233 Vali Loss: 0.9687463 Test Loss: 0.4237164
Validation loss decreased (0.969217 --> 0.968746).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.744909048080444
Epoch: 18, Steps: 60 | Train Loss: 0.3920139 Vali Loss: 0.9686005 Test Loss: 0.4238938
Validation loss decreased (0.968746 --> 0.968601).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.5128724575042725
Epoch: 19, Steps: 60 | Train Loss: 0.3909701 Vali Loss: 0.9688913 Test Loss: 0.4242802
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.223648309707642
Epoch: 20, Steps: 60 | Train Loss: 0.3906027 Vali Loss: 0.9684764 Test Loss: 0.4241348
Validation loss decreased (0.968601 --> 0.968476).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.239182949066162
Epoch: 21, Steps: 60 | Train Loss: 0.3908090 Vali Loss: 0.9684731 Test Loss: 0.4242338
Validation loss decreased (0.968476 --> 0.968473).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.853419542312622
Epoch: 22, Steps: 60 | Train Loss: 0.3906242 Vali Loss: 0.9685494 Test Loss: 0.4242011
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.6400675773620605
Epoch: 23, Steps: 60 | Train Loss: 0.3904384 Vali Loss: 0.9685299 Test Loss: 0.4241928
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.459620237350464
Epoch: 24, Steps: 60 | Train Loss: 0.3898865 Vali Loss: 0.9687877 Test Loss: 0.4243476
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.4769065380096436
Epoch: 25, Steps: 60 | Train Loss: 0.3899186 Vali Loss: 0.9686930 Test Loss: 0.4243037
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.893600702285767
Epoch: 26, Steps: 60 | Train Loss: 0.3894420 Vali Loss: 0.9684278 Test Loss: 0.4242546
Validation loss decreased (0.968473 --> 0.968428).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.723058700561523
Epoch: 27, Steps: 60 | Train Loss: 0.3894994 Vali Loss: 0.9682249 Test Loss: 0.4244773
Validation loss decreased (0.968428 --> 0.968225).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.338513135910034
Epoch: 28, Steps: 60 | Train Loss: 0.3892237 Vali Loss: 0.9684994 Test Loss: 0.4245283
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 4.470307350158691
Epoch: 29, Steps: 60 | Train Loss: 0.3888938 Vali Loss: 0.9685203 Test Loss: 0.4245318
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 4.831259727478027
Epoch: 30, Steps: 60 | Train Loss: 0.3892660 Vali Loss: 0.9686704 Test Loss: 0.4246084
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.648777008056641
Epoch: 31, Steps: 60 | Train Loss: 0.3885272 Vali Loss: 0.9687798 Test Loss: 0.4245747
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.476538181304932
Epoch: 32, Steps: 60 | Train Loss: 0.3883820 Vali Loss: 0.9684146 Test Loss: 0.4245938
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.456510782241821
Epoch: 33, Steps: 60 | Train Loss: 0.3888016 Vali Loss: 0.9683923 Test Loss: 0.4245725
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.8045690059661865
Epoch: 34, Steps: 60 | Train Loss: 0.3883890 Vali Loss: 0.9684877 Test Loss: 0.4245729
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.739027738571167
Epoch: 35, Steps: 60 | Train Loss: 0.3885553 Vali Loss: 0.9686810 Test Loss: 0.4246494
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.492570161819458
Epoch: 36, Steps: 60 | Train Loss: 0.3884392 Vali Loss: 0.9686180 Test Loss: 0.4246229
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.70607328414917
Epoch: 37, Steps: 60 | Train Loss: 0.3882942 Vali Loss: 0.9686888 Test Loss: 0.4246784
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.736311435699463
Epoch: 38, Steps: 60 | Train Loss: 0.3881119 Vali Loss: 0.9685032 Test Loss: 0.4247583
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.9597508907318115
Epoch: 39, Steps: 60 | Train Loss: 0.3882321 Vali Loss: 0.9684306 Test Loss: 0.4247976
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.600714445114136
Epoch: 40, Steps: 60 | Train Loss: 0.3881457 Vali Loss: 0.9684111 Test Loss: 0.4246704
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.5015177726745605
Epoch: 41, Steps: 60 | Train Loss: 0.3881686 Vali Loss: 0.9683892 Test Loss: 0.4247626
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.614020824432373
Epoch: 42, Steps: 60 | Train Loss: 0.3872982 Vali Loss: 0.9688370 Test Loss: 0.4247911
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.1693642139434814
Epoch: 43, Steps: 60 | Train Loss: 0.3878241 Vali Loss: 0.9688414 Test Loss: 0.4248038
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.275177240371704
Epoch: 44, Steps: 60 | Train Loss: 0.3879153 Vali Loss: 0.9684962 Test Loss: 0.4248466
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 4.5057313442230225
Epoch: 45, Steps: 60 | Train Loss: 0.3878628 Vali Loss: 0.9682012 Test Loss: 0.4248273
Validation loss decreased (0.968225 --> 0.968201).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.278226852416992
Epoch: 46, Steps: 60 | Train Loss: 0.3875573 Vali Loss: 0.9687984 Test Loss: 0.4248640
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 4.942870378494263
Epoch: 47, Steps: 60 | Train Loss: 0.3878542 Vali Loss: 0.9683222 Test Loss: 0.4249138
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.89011549949646
Epoch: 48, Steps: 60 | Train Loss: 0.3875506 Vali Loss: 0.9688596 Test Loss: 0.4248843
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 5.128665447235107
Epoch: 49, Steps: 60 | Train Loss: 0.3874562 Vali Loss: 0.9682366 Test Loss: 0.4249106
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.917259693145752
Epoch: 50, Steps: 60 | Train Loss: 0.3872193 Vali Loss: 0.9687684 Test Loss: 0.4249141
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 4.697584390640259
Epoch: 51, Steps: 60 | Train Loss: 0.3876061 Vali Loss: 0.9686574 Test Loss: 0.4249391
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.443032264709473
Epoch: 52, Steps: 60 | Train Loss: 0.3872504 Vali Loss: 0.9687491 Test Loss: 0.4248929
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.336624622344971
Epoch: 53, Steps: 60 | Train Loss: 0.3875623 Vali Loss: 0.9683063 Test Loss: 0.4249701
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 4.601903915405273
Epoch: 54, Steps: 60 | Train Loss: 0.3876477 Vali Loss: 0.9686478 Test Loss: 0.4249994
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.194282531738281
Epoch: 55, Steps: 60 | Train Loss: 0.3872634 Vali Loss: 0.9683500 Test Loss: 0.4249477
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 4.961851119995117
Epoch: 56, Steps: 60 | Train Loss: 0.3872095 Vali Loss: 0.9688256 Test Loss: 0.4249600
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.276857376098633
Epoch: 57, Steps: 60 | Train Loss: 0.3869594 Vali Loss: 0.9686834 Test Loss: 0.4249742
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 4.30445671081543
Epoch: 58, Steps: 60 | Train Loss: 0.3874882 Vali Loss: 0.9689536 Test Loss: 0.4249656
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.732203245162964
Epoch: 59, Steps: 60 | Train Loss: 0.3874401 Vali Loss: 0.9685243 Test Loss: 0.4249568
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 4.60124945640564
Epoch: 60, Steps: 60 | Train Loss: 0.3869568 Vali Loss: 0.9689386 Test Loss: 0.4250007
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 4.133004903793335
Epoch: 61, Steps: 60 | Train Loss: 0.3871234 Vali Loss: 0.9689949 Test Loss: 0.4250116
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 4.075327396392822
Epoch: 62, Steps: 60 | Train Loss: 0.3870993 Vali Loss: 0.9688915 Test Loss: 0.4250157
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 4.392102956771851
Epoch: 63, Steps: 60 | Train Loss: 0.3872885 Vali Loss: 0.9691193 Test Loss: 0.4249781
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 4.641000032424927
Epoch: 64, Steps: 60 | Train Loss: 0.3871363 Vali Loss: 0.9690725 Test Loss: 0.4250187
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 4.461641073226929
Epoch: 65, Steps: 60 | Train Loss: 0.3870941 Vali Loss: 0.9686309 Test Loss: 0.4250264
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.42055395245552063, mae:0.4276999533176422, rse:0.6158404350280762, corr:[0.25756    0.26677057 0.26961914 0.26662353 0.26292148 0.26067507
 0.25995687 0.2603876  0.26087835 0.26103273 0.2608505  0.26039585
 0.25995037 0.25974938 0.25974244 0.2597387  0.2596649  0.25961015
 0.25943848 0.25923127 0.25911757 0.25911778 0.25897005 0.25889435
 0.25886366 0.25886226 0.25880888 0.25854662 0.25805375 0.2574824
 0.25707486 0.25679335 0.2566716  0.25667405 0.25659576 0.25650558
 0.25657701 0.25685808 0.25715983 0.25737786 0.2575548  0.25755978
 0.2573961  0.25731364 0.25748318 0.25784236 0.25823462 0.25838313
 0.25781792 0.25678223 0.25544485 0.25416517 0.25314447 0.2522739
 0.25164264 0.2513032  0.25112522 0.25106782 0.25095987 0.25080082
 0.25066695 0.2505651  0.2504469  0.25044218 0.2505726  0.2508103
 0.2511446  0.25136346 0.25144753 0.25136185 0.2510924  0.2506016
 0.25000304 0.2494177  0.2489039  0.24854422 0.24818097 0.24767824
 0.24712265 0.24664672 0.2463211  0.24614988 0.24602956 0.24588075
 0.24580978 0.24579036 0.2457854  0.24573894 0.2456283  0.24548772
 0.24526775 0.24510019 0.24509075 0.24536294 0.24590833 0.24649628
 0.24694498 0.24704874 0.2466847  0.24614689 0.24573578 0.24558102
 0.24566522 0.2457961  0.2457121  0.24544209 0.24500903 0.24456143
 0.24440351 0.24456601 0.24487808 0.2451495  0.2451947  0.24508928
 0.24507959 0.24522878 0.2454407  0.24553744 0.24534132 0.24476846
 0.24389516 0.24278955 0.24168566 0.24087588 0.24034305 0.2398645
 0.239449   0.23910068 0.2387199  0.23850168 0.23839961 0.2382712
 0.23802754 0.23775344 0.23764735 0.23776709 0.23818284 0.23857789
 0.2386124  0.23815578 0.23754573 0.23712526 0.2369958  0.23685373
 0.23645703 0.23552181 0.23402059 0.23228075 0.23097017 0.23029348
 0.23029713 0.2306398  0.23082048 0.23065567 0.23009965 0.22948976
 0.22916338 0.22927144 0.22950408 0.22937152 0.22884005 0.22842929
 0.22830842 0.22855482 0.22904713 0.22921818 0.22871892 0.22750844
 0.2262123  0.22552465 0.22557834 0.22603188 0.22603472 0.22515757
 0.22360256 0.22218235 0.22146662 0.22197208 0.22293772 0.22321089
 0.22230878 0.22068954 0.21932647 0.21923071 0.22042501 0.22129026
 0.22012655 0.2172832  0.21510214 0.21650364 0.22188647 0.22209232]
