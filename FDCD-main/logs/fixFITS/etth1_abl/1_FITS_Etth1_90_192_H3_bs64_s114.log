Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_192_FITS_ETTh1_ftM_sl90_ll48_pl192_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8359
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=22, out_features=68, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1340416.0
params:  1564.0
Trainable parameters:  1564
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.2911269664764404
Epoch: 1, Steps: 65 | Train Loss: 0.8812684 Vali Loss: 1.6794379 Test Loss: 0.9584505
Validation loss decreased (inf --> 1.679438).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.2510600090026855
Epoch: 2, Steps: 65 | Train Loss: 0.6886158 Vali Loss: 1.4151723 Test Loss: 0.7366209
Validation loss decreased (1.679438 --> 1.415172).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2583470344543457
Epoch: 3, Steps: 65 | Train Loss: 0.5903741 Vali Loss: 1.2771280 Test Loss: 0.6231193
Validation loss decreased (1.415172 --> 1.277128).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.2810280323028564
Epoch: 4, Steps: 65 | Train Loss: 0.5367727 Vali Loss: 1.1975619 Test Loss: 0.5591269
Validation loss decreased (1.277128 --> 1.197562).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.168705940246582
Epoch: 5, Steps: 65 | Train Loss: 0.5049277 Vali Loss: 1.1469377 Test Loss: 0.5208487
Validation loss decreased (1.197562 --> 1.146938).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.1877217292785645
Epoch: 6, Steps: 65 | Train Loss: 0.4849038 Vali Loss: 1.1133116 Test Loss: 0.4971859
Validation loss decreased (1.146938 --> 1.113312).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.6152865886688232
Epoch: 7, Steps: 65 | Train Loss: 0.4716303 Vali Loss: 1.0918348 Test Loss: 0.4821447
Validation loss decreased (1.113312 --> 1.091835).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2496740818023682
Epoch: 8, Steps: 65 | Train Loss: 0.4625832 Vali Loss: 1.0751398 Test Loss: 0.4721186
Validation loss decreased (1.091835 --> 1.075140).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.2299869060516357
Epoch: 9, Steps: 65 | Train Loss: 0.4559173 Vali Loss: 1.0646712 Test Loss: 0.4653649
Validation loss decreased (1.075140 --> 1.064671).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.2726259231567383
Epoch: 10, Steps: 65 | Train Loss: 0.4512664 Vali Loss: 1.0559664 Test Loss: 0.4605658
Validation loss decreased (1.064671 --> 1.055966).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.1924829483032227
Epoch: 11, Steps: 65 | Train Loss: 0.4474012 Vali Loss: 1.0496387 Test Loss: 0.4570163
Validation loss decreased (1.055966 --> 1.049639).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.2655599117279053
Epoch: 12, Steps: 65 | Train Loss: 0.4439680 Vali Loss: 1.0438240 Test Loss: 0.4542756
Validation loss decreased (1.049639 --> 1.043824).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.2570059299468994
Epoch: 13, Steps: 65 | Train Loss: 0.4418375 Vali Loss: 1.0394399 Test Loss: 0.4521837
Validation loss decreased (1.043824 --> 1.039440).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.1429989337921143
Epoch: 14, Steps: 65 | Train Loss: 0.4399215 Vali Loss: 1.0361531 Test Loss: 0.4505240
Validation loss decreased (1.039440 --> 1.036153).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1906073093414307
Epoch: 15, Steps: 65 | Train Loss: 0.4382713 Vali Loss: 1.0331950 Test Loss: 0.4490374
Validation loss decreased (1.036153 --> 1.033195).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.2266077995300293
Epoch: 16, Steps: 65 | Train Loss: 0.4367392 Vali Loss: 1.0305861 Test Loss: 0.4479578
Validation loss decreased (1.033195 --> 1.030586).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.2535309791564941
Epoch: 17, Steps: 65 | Train Loss: 0.4356240 Vali Loss: 1.0278431 Test Loss: 0.4469866
Validation loss decreased (1.030586 --> 1.027843).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.2026419639587402
Epoch: 18, Steps: 65 | Train Loss: 0.4344477 Vali Loss: 1.0264994 Test Loss: 0.4461782
Validation loss decreased (1.027843 --> 1.026499).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.183412790298462
Epoch: 19, Steps: 65 | Train Loss: 0.4333337 Vali Loss: 1.0245105 Test Loss: 0.4454393
Validation loss decreased (1.026499 --> 1.024511).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.2739875316619873
Epoch: 20, Steps: 65 | Train Loss: 0.4329364 Vali Loss: 1.0234900 Test Loss: 0.4447964
Validation loss decreased (1.024511 --> 1.023490).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.2920491695404053
Epoch: 21, Steps: 65 | Train Loss: 0.4321452 Vali Loss: 1.0211201 Test Loss: 0.4443039
Validation loss decreased (1.023490 --> 1.021120).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.241844654083252
Epoch: 22, Steps: 65 | Train Loss: 0.4313819 Vali Loss: 1.0208181 Test Loss: 0.4438577
Validation loss decreased (1.021120 --> 1.020818).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.2276966571807861
Epoch: 23, Steps: 65 | Train Loss: 0.4307161 Vali Loss: 1.0197061 Test Loss: 0.4434060
Validation loss decreased (1.020818 --> 1.019706).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.2048101425170898
Epoch: 24, Steps: 65 | Train Loss: 0.4305030 Vali Loss: 1.0181262 Test Loss: 0.4430518
Validation loss decreased (1.019706 --> 1.018126).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.3044345378875732
Epoch: 25, Steps: 65 | Train Loss: 0.4301138 Vali Loss: 1.0173331 Test Loss: 0.4427781
Validation loss decreased (1.018126 --> 1.017333).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2070114612579346
Epoch: 26, Steps: 65 | Train Loss: 0.4295964 Vali Loss: 1.0173612 Test Loss: 0.4425069
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.7427403926849365
Epoch: 27, Steps: 65 | Train Loss: 0.4291370 Vali Loss: 1.0168508 Test Loss: 0.4422389
Validation loss decreased (1.017333 --> 1.016851).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.2554354667663574
Epoch: 28, Steps: 65 | Train Loss: 0.4288871 Vali Loss: 1.0160784 Test Loss: 0.4420225
Validation loss decreased (1.016851 --> 1.016078).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.1705372333526611
Epoch: 29, Steps: 65 | Train Loss: 0.4286915 Vali Loss: 1.0157170 Test Loss: 0.4418176
Validation loss decreased (1.016078 --> 1.015717).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.270982265472412
Epoch: 30, Steps: 65 | Train Loss: 0.4279635 Vali Loss: 1.0152619 Test Loss: 0.4416491
Validation loss decreased (1.015717 --> 1.015262).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.1723108291625977
Epoch: 31, Steps: 65 | Train Loss: 0.4280196 Vali Loss: 1.0144871 Test Loss: 0.4414815
Validation loss decreased (1.015262 --> 1.014487).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.173126459121704
Epoch: 32, Steps: 65 | Train Loss: 0.4280261 Vali Loss: 1.0140544 Test Loss: 0.4413540
Validation loss decreased (1.014487 --> 1.014054).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.273970365524292
Epoch: 33, Steps: 65 | Train Loss: 0.4277305 Vali Loss: 1.0130064 Test Loss: 0.4412318
Validation loss decreased (1.014054 --> 1.013006).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3219075202941895
Epoch: 34, Steps: 65 | Train Loss: 0.4275755 Vali Loss: 1.0132719 Test Loss: 0.4410797
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.2955901622772217
Epoch: 35, Steps: 65 | Train Loss: 0.4271480 Vali Loss: 1.0133852 Test Loss: 0.4409897
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3098623752593994
Epoch: 36, Steps: 65 | Train Loss: 0.4269297 Vali Loss: 1.0128390 Test Loss: 0.4409452
Validation loss decreased (1.013006 --> 1.012839).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.1912364959716797
Epoch: 37, Steps: 65 | Train Loss: 0.4271669 Vali Loss: 1.0124955 Test Loss: 0.4408079
Validation loss decreased (1.012839 --> 1.012496).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.2521347999572754
Epoch: 38, Steps: 65 | Train Loss: 0.4267393 Vali Loss: 1.0124189 Test Loss: 0.4407435
Validation loss decreased (1.012496 --> 1.012419).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.1927061080932617
Epoch: 39, Steps: 65 | Train Loss: 0.4265895 Vali Loss: 1.0123138 Test Loss: 0.4406745
Validation loss decreased (1.012419 --> 1.012314).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.2833576202392578
Epoch: 40, Steps: 65 | Train Loss: 0.4267173 Vali Loss: 1.0118901 Test Loss: 0.4406172
Validation loss decreased (1.012314 --> 1.011890).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.1911661624908447
Epoch: 41, Steps: 65 | Train Loss: 0.4265780 Vali Loss: 1.0117666 Test Loss: 0.4405384
Validation loss decreased (1.011890 --> 1.011767).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.301715612411499
Epoch: 42, Steps: 65 | Train Loss: 0.4265163 Vali Loss: 1.0114077 Test Loss: 0.4404823
Validation loss decreased (1.011767 --> 1.011408).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.2431395053863525
Epoch: 43, Steps: 65 | Train Loss: 0.4266567 Vali Loss: 1.0111567 Test Loss: 0.4404489
Validation loss decreased (1.011408 --> 1.011157).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.1931653022766113
Epoch: 44, Steps: 65 | Train Loss: 0.4264237 Vali Loss: 1.0113070 Test Loss: 0.4404067
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.267498254776001
Epoch: 45, Steps: 65 | Train Loss: 0.4262399 Vali Loss: 1.0111680 Test Loss: 0.4403461
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.2396094799041748
Epoch: 46, Steps: 65 | Train Loss: 0.4259591 Vali Loss: 1.0108197 Test Loss: 0.4402791
Validation loss decreased (1.011157 --> 1.010820).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.2784686088562012
Epoch: 47, Steps: 65 | Train Loss: 0.4263486 Vali Loss: 1.0107099 Test Loss: 0.4402516
Validation loss decreased (1.010820 --> 1.010710).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.1919100284576416
Epoch: 48, Steps: 65 | Train Loss: 0.4260055 Vali Loss: 1.0106089 Test Loss: 0.4402339
Validation loss decreased (1.010710 --> 1.010609).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.2832436561584473
Epoch: 49, Steps: 65 | Train Loss: 0.4261475 Vali Loss: 1.0105696 Test Loss: 0.4401965
Validation loss decreased (1.010609 --> 1.010570).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2026190757751465
Epoch: 50, Steps: 65 | Train Loss: 0.4258208 Vali Loss: 1.0104965 Test Loss: 0.4401708
Validation loss decreased (1.010570 --> 1.010496).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.2415950298309326
Epoch: 51, Steps: 65 | Train Loss: 0.4258991 Vali Loss: 1.0102475 Test Loss: 0.4401317
Validation loss decreased (1.010496 --> 1.010247).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.1839914321899414
Epoch: 52, Steps: 65 | Train Loss: 0.4258527 Vali Loss: 1.0098037 Test Loss: 0.4401296
Validation loss decreased (1.010247 --> 1.009804).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.1988930702209473
Epoch: 53, Steps: 65 | Train Loss: 0.4254801 Vali Loss: 1.0097233 Test Loss: 0.4401098
Validation loss decreased (1.009804 --> 1.009723).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.3098056316375732
Epoch: 54, Steps: 65 | Train Loss: 0.4257634 Vali Loss: 1.0100137 Test Loss: 0.4400896
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.127668857574463
Epoch: 55, Steps: 65 | Train Loss: 0.4259437 Vali Loss: 1.0096364 Test Loss: 0.4400600
Validation loss decreased (1.009723 --> 1.009636).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.2469456195831299
Epoch: 56, Steps: 65 | Train Loss: 0.4257745 Vali Loss: 1.0097635 Test Loss: 0.4400288
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.200650691986084
Epoch: 57, Steps: 65 | Train Loss: 0.4254385 Vali Loss: 1.0097892 Test Loss: 0.4400190
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.264390468597412
Epoch: 58, Steps: 65 | Train Loss: 0.4255509 Vali Loss: 1.0097840 Test Loss: 0.4400120
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.2225971221923828
Epoch: 59, Steps: 65 | Train Loss: 0.4255465 Vali Loss: 1.0097300 Test Loss: 0.4399968
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.157160997390747
Epoch: 60, Steps: 65 | Train Loss: 0.4254654 Vali Loss: 1.0096357 Test Loss: 0.4399793
Validation loss decreased (1.009636 --> 1.009636).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.3273212909698486
Epoch: 61, Steps: 65 | Train Loss: 0.4255756 Vali Loss: 1.0094678 Test Loss: 0.4399773
Validation loss decreased (1.009636 --> 1.009468).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.2716906070709229
Epoch: 62, Steps: 65 | Train Loss: 0.4253095 Vali Loss: 1.0095892 Test Loss: 0.4399559
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.2636384963989258
Epoch: 63, Steps: 65 | Train Loss: 0.4255590 Vali Loss: 1.0095209 Test Loss: 0.4399632
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.2352650165557861
Epoch: 64, Steps: 65 | Train Loss: 0.4248806 Vali Loss: 1.0092461 Test Loss: 0.4399377
Validation loss decreased (1.009468 --> 1.009246).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.178502082824707
Epoch: 65, Steps: 65 | Train Loss: 0.4250871 Vali Loss: 1.0094436 Test Loss: 0.4399166
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.2126884460449219
Epoch: 66, Steps: 65 | Train Loss: 0.4254771 Vali Loss: 1.0092859 Test Loss: 0.4399246
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.1853039264678955
Epoch: 67, Steps: 65 | Train Loss: 0.4255683 Vali Loss: 1.0082937 Test Loss: 0.4399023
Validation loss decreased (1.009246 --> 1.008294).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.570491075515747
Epoch: 68, Steps: 65 | Train Loss: 0.4252081 Vali Loss: 1.0092003 Test Loss: 0.4399073
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.4665169715881348
Epoch: 69, Steps: 65 | Train Loss: 0.4250241 Vali Loss: 1.0091571 Test Loss: 0.4398831
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 3.109592914581299
Epoch: 70, Steps: 65 | Train Loss: 0.4254609 Vali Loss: 1.0090790 Test Loss: 0.4398910
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.735978603363037
Epoch: 71, Steps: 65 | Train Loss: 0.4252464 Vali Loss: 1.0092325 Test Loss: 0.4398871
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.903071403503418
Epoch: 72, Steps: 65 | Train Loss: 0.4255875 Vali Loss: 1.0091876 Test Loss: 0.4398699
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 5.157569646835327
Epoch: 73, Steps: 65 | Train Loss: 0.4254012 Vali Loss: 1.0088190 Test Loss: 0.4398720
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.3156721591949463
Epoch: 74, Steps: 65 | Train Loss: 0.4250639 Vali Loss: 1.0089440 Test Loss: 0.4398592
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.818993091583252
Epoch: 75, Steps: 65 | Train Loss: 0.4254629 Vali Loss: 1.0091387 Test Loss: 0.4398529
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.3203232288360596
Epoch: 76, Steps: 65 | Train Loss: 0.4254266 Vali Loss: 1.0082033 Test Loss: 0.4398594
Validation loss decreased (1.008294 --> 1.008203).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.396028518676758
Epoch: 77, Steps: 65 | Train Loss: 0.4252233 Vali Loss: 1.0088243 Test Loss: 0.4398580
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.077054500579834
Epoch: 78, Steps: 65 | Train Loss: 0.4254481 Vali Loss: 1.0090427 Test Loss: 0.4398462
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.7772457599639893
Epoch: 79, Steps: 65 | Train Loss: 0.4251198 Vali Loss: 1.0090284 Test Loss: 0.4398414
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 5.488119840621948
Epoch: 80, Steps: 65 | Train Loss: 0.4251475 Vali Loss: 1.0085665 Test Loss: 0.4398427
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.8363394737243652
Epoch: 81, Steps: 65 | Train Loss: 0.4251875 Vali Loss: 1.0088501 Test Loss: 0.4398384
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.3836793899536133
Epoch: 82, Steps: 65 | Train Loss: 0.4250597 Vali Loss: 1.0090052 Test Loss: 0.4398361
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 3.5401439666748047
Epoch: 83, Steps: 65 | Train Loss: 0.4252158 Vali Loss: 1.0088724 Test Loss: 0.4398304
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 5.615369558334351
Epoch: 84, Steps: 65 | Train Loss: 0.4255075 Vali Loss: 1.0088730 Test Loss: 0.4398274
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.3675293922424316
Epoch: 85, Steps: 65 | Train Loss: 0.4252713 Vali Loss: 1.0085673 Test Loss: 0.4398273
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.654507875442505
Epoch: 86, Steps: 65 | Train Loss: 0.4251962 Vali Loss: 1.0079832 Test Loss: 0.4398286
Validation loss decreased (1.008203 --> 1.007983).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.0392658710479736
Epoch: 87, Steps: 65 | Train Loss: 0.4251563 Vali Loss: 1.0089735 Test Loss: 0.4398205
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 4.391382455825806
Epoch: 88, Steps: 65 | Train Loss: 0.4249700 Vali Loss: 1.0082697 Test Loss: 0.4398267
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.5480172634124756
Epoch: 89, Steps: 65 | Train Loss: 0.4249877 Vali Loss: 1.0088587 Test Loss: 0.4398241
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.204613208770752
Epoch: 90, Steps: 65 | Train Loss: 0.4254109 Vali Loss: 1.0088955 Test Loss: 0.4398212
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 4.042322635650635
Epoch: 91, Steps: 65 | Train Loss: 0.4247552 Vali Loss: 1.0087068 Test Loss: 0.4398247
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.942734479904175
Epoch: 92, Steps: 65 | Train Loss: 0.4252480 Vali Loss: 1.0082910 Test Loss: 0.4398192
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 3.2437832355499268
Epoch: 93, Steps: 65 | Train Loss: 0.4250142 Vali Loss: 1.0087408 Test Loss: 0.4398215
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 3.791100025177002
Epoch: 94, Steps: 65 | Train Loss: 0.4249871 Vali Loss: 1.0084296 Test Loss: 0.4398211
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.3127775192260742
Epoch: 95, Steps: 65 | Train Loss: 0.4251486 Vali Loss: 1.0087963 Test Loss: 0.4398184
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.2393851280212402
Epoch: 96, Steps: 65 | Train Loss: 0.4252013 Vali Loss: 1.0087541 Test Loss: 0.4398141
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.308065414428711
Epoch: 97, Steps: 65 | Train Loss: 0.4251841 Vali Loss: 1.0087563 Test Loss: 0.4398152
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.2331137657165527
Epoch: 98, Steps: 65 | Train Loss: 0.4252036 Vali Loss: 1.0087373 Test Loss: 0.4398136
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.294952392578125
Epoch: 99, Steps: 65 | Train Loss: 0.4250323 Vali Loss: 1.0087599 Test Loss: 0.4398134
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.2114620208740234
Epoch: 100, Steps: 65 | Train Loss: 0.4248234 Vali Loss: 1.0083184 Test Loss: 0.4398144
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_90_192_FITS_ETTh1_ftM_sl90_ll48_pl192_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.44036924839019775, mae:0.4264833629131317, rse:0.6301817297935486, corr:[0.26240796 0.26442337 0.262509   0.26262635 0.26114047 0.2584676
 0.25740838 0.25747707 0.25781944 0.25787804 0.2570547  0.25646135
 0.25613222 0.25574145 0.25548783 0.25540587 0.2555681  0.25591856
 0.256134   0.25608292 0.25574422 0.25535044 0.25462344 0.25379473
 0.25243023 0.25195763 0.2522376  0.2525523  0.25249034 0.25251994
 0.25273657 0.25254133 0.25246525 0.25256646 0.25230062 0.25190505
 0.25164926 0.25165537 0.25186753 0.25202724 0.25223905 0.2526337
 0.25312492 0.25328445 0.25319573 0.25307858 0.25289068 0.25216317
 0.25055248 0.24957508 0.2492032  0.24862066 0.24740848 0.24635397
 0.24647506 0.24624069 0.24580285 0.24582055 0.24572308 0.24544287
 0.24514268 0.24505131 0.245021   0.24483098 0.24492036 0.24533416
 0.2458319  0.24590254 0.24556959 0.24530877 0.2450064  0.2439146
 0.24196416 0.24094118 0.24067926 0.24039136 0.23975809 0.23945644
 0.23996459 0.23987225 0.23949732 0.2392452  0.23910302 0.23884673
 0.23860502 0.23864187 0.23886001 0.23879541 0.23875117 0.23904379
 0.23932613 0.23930047 0.23910838 0.23910184 0.2391843  0.23867695
 0.23723833 0.23652892 0.23656665 0.23625748 0.23590723 0.23597732
 0.23652332 0.23644836 0.23610422 0.23599428 0.23593327 0.23573366
 0.23547855 0.23538607 0.2356249  0.23570037 0.23581934 0.23605633
 0.2364133  0.23654996 0.23648831 0.23639233 0.23597638 0.2348577
 0.23287734 0.23164932 0.23100588 0.23016706 0.22944944 0.2294272
 0.23028316 0.23054418 0.23041916 0.23029141 0.23007159 0.22991027
 0.22976549 0.229626   0.22958599 0.22945413 0.22958037 0.22987029
 0.23020358 0.23035051 0.23027089 0.2303275  0.23015088 0.22914642
 0.22714837 0.22592004 0.2255159  0.22479826 0.22404361 0.22409509
 0.22503829 0.22519454 0.2252307  0.2253848  0.22524501 0.22495107
 0.22473815 0.22464937 0.22443499 0.22409366 0.22410533 0.22444479
 0.22474562 0.22468938 0.22446714 0.22454715 0.22444957 0.22326791
 0.22121015 0.22038238 0.22056362 0.22023448 0.21919067 0.21908236
 0.22018306 0.22028174 0.22002704 0.22032195 0.22071676 0.22059055
 0.2203727  0.22034428 0.2202221  0.21940519 0.21872379 0.21917684
 0.22001037 0.21963342 0.2180202  0.21752222 0.21937662 0.21800323]
