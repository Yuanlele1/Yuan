Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=106, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  15386112.0
params:  17334.0
Trainable parameters:  17334
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.3256657123565674
Epoch: 1, Steps: 63 | Train Loss: 0.6804307 Vali Loss: 1.3373019 Test Loss: 0.6389167
Validation loss decreased (inf --> 1.337302).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.2386014461517334
Epoch: 2, Steps: 63 | Train Loss: 0.5212813 Vali Loss: 1.1689467 Test Loss: 0.5297207
Validation loss decreased (1.337302 --> 1.168947).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2431423664093018
Epoch: 3, Steps: 63 | Train Loss: 0.4696709 Vali Loss: 1.0864404 Test Loss: 0.4786719
Validation loss decreased (1.168947 --> 1.086440).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.228646993637085
Epoch: 4, Steps: 63 | Train Loss: 0.4421288 Vali Loss: 1.0360150 Test Loss: 0.4492799
Validation loss decreased (1.086440 --> 1.036015).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.2214715480804443
Epoch: 5, Steps: 63 | Train Loss: 0.4256812 Vali Loss: 1.0034602 Test Loss: 0.4318041
Validation loss decreased (1.036015 --> 1.003460).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.1411325931549072
Epoch: 6, Steps: 63 | Train Loss: 0.4152600 Vali Loss: 0.9819390 Test Loss: 0.4217059
Validation loss decreased (1.003460 --> 0.981939).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.2164232730865479
Epoch: 7, Steps: 63 | Train Loss: 0.4088895 Vali Loss: 0.9679319 Test Loss: 0.4159355
Validation loss decreased (0.981939 --> 0.967932).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2735908031463623
Epoch: 8, Steps: 63 | Train Loss: 0.4051654 Vali Loss: 0.9591187 Test Loss: 0.4128371
Validation loss decreased (0.967932 --> 0.959119).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.1985366344451904
Epoch: 9, Steps: 63 | Train Loss: 0.4022080 Vali Loss: 0.9522211 Test Loss: 0.4112962
Validation loss decreased (0.959119 --> 0.952221).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.2311127185821533
Epoch: 10, Steps: 63 | Train Loss: 0.4005356 Vali Loss: 0.9481651 Test Loss: 0.4103140
Validation loss decreased (0.952221 --> 0.948165).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.285181999206543
Epoch: 11, Steps: 63 | Train Loss: 0.3993131 Vali Loss: 0.9439246 Test Loss: 0.4098160
Validation loss decreased (0.948165 --> 0.943925).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.1867120265960693
Epoch: 12, Steps: 63 | Train Loss: 0.3984298 Vali Loss: 0.9418023 Test Loss: 0.4094515
Validation loss decreased (0.943925 --> 0.941802).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.254075288772583
Epoch: 13, Steps: 63 | Train Loss: 0.3976485 Vali Loss: 0.9395484 Test Loss: 0.4094911
Validation loss decreased (0.941802 --> 0.939548).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.228569507598877
Epoch: 14, Steps: 63 | Train Loss: 0.3972262 Vali Loss: 0.9371760 Test Loss: 0.4093508
Validation loss decreased (0.939548 --> 0.937176).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.1791932582855225
Epoch: 15, Steps: 63 | Train Loss: 0.3971243 Vali Loss: 0.9353391 Test Loss: 0.4091513
Validation loss decreased (0.937176 --> 0.935339).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.1429226398468018
Epoch: 16, Steps: 63 | Train Loss: 0.3964695 Vali Loss: 0.9349114 Test Loss: 0.4091842
Validation loss decreased (0.935339 --> 0.934911).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.1189517974853516
Epoch: 17, Steps: 63 | Train Loss: 0.3962147 Vali Loss: 0.9331967 Test Loss: 0.4089175
Validation loss decreased (0.934911 --> 0.933197).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.1312243938446045
Epoch: 18, Steps: 63 | Train Loss: 0.3959798 Vali Loss: 0.9321359 Test Loss: 0.4091239
Validation loss decreased (0.933197 --> 0.932136).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.1386620998382568
Epoch: 19, Steps: 63 | Train Loss: 0.3957385 Vali Loss: 0.9309269 Test Loss: 0.4088382
Validation loss decreased (0.932136 --> 0.930927).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3059217929840088
Epoch: 20, Steps: 63 | Train Loss: 0.3953896 Vali Loss: 0.9304980 Test Loss: 0.4088985
Validation loss decreased (0.930927 --> 0.930498).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.2052264213562012
Epoch: 21, Steps: 63 | Train Loss: 0.3954193 Vali Loss: 0.9295915 Test Loss: 0.4089276
Validation loss decreased (0.930498 --> 0.929591).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2880163192749023
Epoch: 22, Steps: 63 | Train Loss: 0.3951461 Vali Loss: 0.9293680 Test Loss: 0.4089679
Validation loss decreased (0.929591 --> 0.929368).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.2318181991577148
Epoch: 23, Steps: 63 | Train Loss: 0.3950356 Vali Loss: 0.9285318 Test Loss: 0.4088924
Validation loss decreased (0.929368 --> 0.928532).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.1538076400756836
Epoch: 24, Steps: 63 | Train Loss: 0.3949372 Vali Loss: 0.9279904 Test Loss: 0.4087492
Validation loss decreased (0.928532 --> 0.927990).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.6916577816009521
Epoch: 25, Steps: 63 | Train Loss: 0.3948018 Vali Loss: 0.9276159 Test Loss: 0.4087689
Validation loss decreased (0.927990 --> 0.927616).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.2361211776733398
Epoch: 26, Steps: 63 | Train Loss: 0.3945009 Vali Loss: 0.9271669 Test Loss: 0.4087385
Validation loss decreased (0.927616 --> 0.927167).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.205397129058838
Epoch: 27, Steps: 63 | Train Loss: 0.3945132 Vali Loss: 0.9262541 Test Loss: 0.4087127
Validation loss decreased (0.927167 --> 0.926254).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.1867384910583496
Epoch: 28, Steps: 63 | Train Loss: 0.3944115 Vali Loss: 0.9260834 Test Loss: 0.4087232
Validation loss decreased (0.926254 --> 0.926083).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.281135082244873
Epoch: 29, Steps: 63 | Train Loss: 0.3943567 Vali Loss: 0.9261203 Test Loss: 0.4086496
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.1328151226043701
Epoch: 30, Steps: 63 | Train Loss: 0.3941428 Vali Loss: 0.9258842 Test Loss: 0.4086797
Validation loss decreased (0.926083 --> 0.925884).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.231440544128418
Epoch: 31, Steps: 63 | Train Loss: 0.3942084 Vali Loss: 0.9249002 Test Loss: 0.4085528
Validation loss decreased (0.925884 --> 0.924900).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.1436569690704346
Epoch: 32, Steps: 63 | Train Loss: 0.3942036 Vali Loss: 0.9253417 Test Loss: 0.4086541
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.2641263008117676
Epoch: 33, Steps: 63 | Train Loss: 0.3941943 Vali Loss: 0.9248154 Test Loss: 0.4086061
Validation loss decreased (0.924900 --> 0.924815).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.1737775802612305
Epoch: 34, Steps: 63 | Train Loss: 0.3940119 Vali Loss: 0.9247434 Test Loss: 0.4085900
Validation loss decreased (0.924815 --> 0.924743).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.2120554447174072
Epoch: 35, Steps: 63 | Train Loss: 0.3938126 Vali Loss: 0.9243346 Test Loss: 0.4085867
Validation loss decreased (0.924743 --> 0.924335).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.2459087371826172
Epoch: 36, Steps: 63 | Train Loss: 0.3940028 Vali Loss: 0.9240910 Test Loss: 0.4086281
Validation loss decreased (0.924335 --> 0.924091).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.1918423175811768
Epoch: 37, Steps: 63 | Train Loss: 0.3936967 Vali Loss: 0.9241959 Test Loss: 0.4085795
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.2408106327056885
Epoch: 38, Steps: 63 | Train Loss: 0.3939125 Vali Loss: 0.9241654 Test Loss: 0.4086008
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.246330976486206
Epoch: 39, Steps: 63 | Train Loss: 0.3937503 Vali Loss: 0.9239125 Test Loss: 0.4085453
Validation loss decreased (0.924091 --> 0.923913).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.3145365715026855
Epoch: 40, Steps: 63 | Train Loss: 0.3935599 Vali Loss: 0.9236629 Test Loss: 0.4085366
Validation loss decreased (0.923913 --> 0.923663).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.1758372783660889
Epoch: 41, Steps: 63 | Train Loss: 0.3934747 Vali Loss: 0.9233499 Test Loss: 0.4085063
Validation loss decreased (0.923663 --> 0.923350).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3349924087524414
Epoch: 42, Steps: 63 | Train Loss: 0.3934213 Vali Loss: 0.9235021 Test Loss: 0.4085374
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.1765587329864502
Epoch: 43, Steps: 63 | Train Loss: 0.3933649 Vali Loss: 0.9232623 Test Loss: 0.4085689
Validation loss decreased (0.923350 --> 0.923262).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.243237018585205
Epoch: 44, Steps: 63 | Train Loss: 0.3934736 Vali Loss: 0.9233335 Test Loss: 0.4085616
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.2231395244598389
Epoch: 45, Steps: 63 | Train Loss: 0.3935005 Vali Loss: 0.9226658 Test Loss: 0.4085177
Validation loss decreased (0.923262 --> 0.922666).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.2790734767913818
Epoch: 46, Steps: 63 | Train Loss: 0.3934849 Vali Loss: 0.9225776 Test Loss: 0.4085403
Validation loss decreased (0.922666 --> 0.922578).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.1890521049499512
Epoch: 47, Steps: 63 | Train Loss: 0.3934825 Vali Loss: 0.9226376 Test Loss: 0.4085574
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.254333257675171
Epoch: 48, Steps: 63 | Train Loss: 0.3932762 Vali Loss: 0.9226811 Test Loss: 0.4085377
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.1893408298492432
Epoch: 49, Steps: 63 | Train Loss: 0.3933134 Vali Loss: 0.9229344 Test Loss: 0.4085455
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.20979905128479
Epoch: 50, Steps: 63 | Train Loss: 0.3933214 Vali Loss: 0.9226490 Test Loss: 0.4085104
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3419771194458008
Epoch: 51, Steps: 63 | Train Loss: 0.3932091 Vali Loss: 0.9217461 Test Loss: 0.4085511
Validation loss decreased (0.922578 --> 0.921746).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.276630163192749
Epoch: 52, Steps: 63 | Train Loss: 0.3934586 Vali Loss: 0.9222102 Test Loss: 0.4085553
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.3621556758880615
Epoch: 53, Steps: 63 | Train Loss: 0.3934098 Vali Loss: 0.9225318 Test Loss: 0.4085144
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.3002610206604004
Epoch: 54, Steps: 63 | Train Loss: 0.3932564 Vali Loss: 0.9222582 Test Loss: 0.4085359
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.2136170864105225
Epoch: 55, Steps: 63 | Train Loss: 0.3933747 Vali Loss: 0.9224063 Test Loss: 0.4085318
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.3400681018829346
Epoch: 56, Steps: 63 | Train Loss: 0.3931788 Vali Loss: 0.9218897 Test Loss: 0.4085239
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.2982323169708252
Epoch: 57, Steps: 63 | Train Loss: 0.3933451 Vali Loss: 0.9217951 Test Loss: 0.4085115
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.2849645614624023
Epoch: 58, Steps: 63 | Train Loss: 0.3934738 Vali Loss: 0.9222543 Test Loss: 0.4085139
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.2554075717926025
Epoch: 59, Steps: 63 | Train Loss: 0.3934799 Vali Loss: 0.9221493 Test Loss: 0.4085164
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.1268513202667236
Epoch: 60, Steps: 63 | Train Loss: 0.3933637 Vali Loss: 0.9222758 Test Loss: 0.4084922
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.3046727180480957
Epoch: 61, Steps: 63 | Train Loss: 0.3932032 Vali Loss: 0.9220977 Test Loss: 0.4085035
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.3565902709960938
Epoch: 62, Steps: 63 | Train Loss: 0.3933306 Vali Loss: 0.9217573 Test Loss: 0.4085183
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.3060731887817383
Epoch: 63, Steps: 63 | Train Loss: 0.3932881 Vali Loss: 0.9217340 Test Loss: 0.4085073
Validation loss decreased (0.921746 --> 0.921734).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.332395315170288
Epoch: 64, Steps: 63 | Train Loss: 0.3932354 Vali Loss: 0.9215965 Test Loss: 0.4085100
Validation loss decreased (0.921734 --> 0.921597).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.2776033878326416
Epoch: 65, Steps: 63 | Train Loss: 0.3930730 Vali Loss: 0.9216401 Test Loss: 0.4085091
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.343824863433838
Epoch: 66, Steps: 63 | Train Loss: 0.3930328 Vali Loss: 0.9218814 Test Loss: 0.4084806
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.2644424438476562
Epoch: 67, Steps: 63 | Train Loss: 0.3931778 Vali Loss: 0.9219087 Test Loss: 0.4085075
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.2013776302337646
Epoch: 68, Steps: 63 | Train Loss: 0.3933384 Vali Loss: 0.9216077 Test Loss: 0.4084969
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3320937156677246
Epoch: 69, Steps: 63 | Train Loss: 0.3929803 Vali Loss: 0.9215818 Test Loss: 0.4084932
Validation loss decreased (0.921597 --> 0.921582).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.2185158729553223
Epoch: 70, Steps: 63 | Train Loss: 0.3931838 Vali Loss: 0.9215049 Test Loss: 0.4084834
Validation loss decreased (0.921582 --> 0.921505).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.3786826133728027
Epoch: 71, Steps: 63 | Train Loss: 0.3932201 Vali Loss: 0.9215894 Test Loss: 0.4084932
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.2915432453155518
Epoch: 72, Steps: 63 | Train Loss: 0.3930168 Vali Loss: 0.9217503 Test Loss: 0.4084936
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.3342061042785645
Epoch: 73, Steps: 63 | Train Loss: 0.3930784 Vali Loss: 0.9214922 Test Loss: 0.4084938
Validation loss decreased (0.921505 --> 0.921492).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.2680459022521973
Epoch: 74, Steps: 63 | Train Loss: 0.3930164 Vali Loss: 0.9207833 Test Loss: 0.4084899
Validation loss decreased (0.921492 --> 0.920783).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.2421197891235352
Epoch: 75, Steps: 63 | Train Loss: 0.3932244 Vali Loss: 0.9214082 Test Loss: 0.4084884
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.242849349975586
Epoch: 76, Steps: 63 | Train Loss: 0.3930038 Vali Loss: 0.9213834 Test Loss: 0.4084987
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.2507200241088867
Epoch: 77, Steps: 63 | Train Loss: 0.3931171 Vali Loss: 0.9215865 Test Loss: 0.4084873
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.2631072998046875
Epoch: 78, Steps: 63 | Train Loss: 0.3930817 Vali Loss: 0.9212720 Test Loss: 0.4084827
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.287424087524414
Epoch: 79, Steps: 63 | Train Loss: 0.3931272 Vali Loss: 0.9216433 Test Loss: 0.4084886
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.1555285453796387
Epoch: 80, Steps: 63 | Train Loss: 0.3933957 Vali Loss: 0.9214275 Test Loss: 0.4084939
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.2356057167053223
Epoch: 81, Steps: 63 | Train Loss: 0.3933193 Vali Loss: 0.9213842 Test Loss: 0.4084951
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.2686495780944824
Epoch: 82, Steps: 63 | Train Loss: 0.3931452 Vali Loss: 0.9216657 Test Loss: 0.4084846
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.215500831604004
Epoch: 83, Steps: 63 | Train Loss: 0.3933141 Vali Loss: 0.9216776 Test Loss: 0.4084886
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.2111742496490479
Epoch: 84, Steps: 63 | Train Loss: 0.3930427 Vali Loss: 0.9216534 Test Loss: 0.4084798
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.212888240814209
Epoch: 85, Steps: 63 | Train Loss: 0.3931316 Vali Loss: 0.9209799 Test Loss: 0.4084896
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.132124423980713
Epoch: 86, Steps: 63 | Train Loss: 0.3929041 Vali Loss: 0.9212651 Test Loss: 0.4084935
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.22379469871521
Epoch: 87, Steps: 63 | Train Loss: 0.3931088 Vali Loss: 0.9211383 Test Loss: 0.4084866
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.284425973892212
Epoch: 88, Steps: 63 | Train Loss: 0.3931465 Vali Loss: 0.9211580 Test Loss: 0.4084838
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.201768159866333
Epoch: 89, Steps: 63 | Train Loss: 0.3931553 Vali Loss: 0.9214647 Test Loss: 0.4084906
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.2366280555725098
Epoch: 90, Steps: 63 | Train Loss: 0.3929943 Vali Loss: 0.9214715 Test Loss: 0.4084842
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.311659336090088
Epoch: 91, Steps: 63 | Train Loss: 0.3929557 Vali Loss: 0.9215104 Test Loss: 0.4084874
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.2473094463348389
Epoch: 92, Steps: 63 | Train Loss: 0.3930200 Vali Loss: 0.9215511 Test Loss: 0.4084885
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.1870558261871338
Epoch: 93, Steps: 63 | Train Loss: 0.3929109 Vali Loss: 0.9212107 Test Loss: 0.4084860
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.1373419761657715
Epoch: 94, Steps: 63 | Train Loss: 0.3929426 Vali Loss: 0.9204743 Test Loss: 0.4084908
Validation loss decreased (0.920783 --> 0.920474).  Saving model ...
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.234809398651123
Epoch: 95, Steps: 63 | Train Loss: 0.3931434 Vali Loss: 0.9214257 Test Loss: 0.4084843
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.2472941875457764
Epoch: 96, Steps: 63 | Train Loss: 0.3928869 Vali Loss: 0.9209354 Test Loss: 0.4084855
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.1996474266052246
Epoch: 97, Steps: 63 | Train Loss: 0.3931247 Vali Loss: 0.9212041 Test Loss: 0.4084877
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.1951165199279785
Epoch: 98, Steps: 63 | Train Loss: 0.3929382 Vali Loss: 0.9214120 Test Loss: 0.4084863
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.2244751453399658
Epoch: 99, Steps: 63 | Train Loss: 0.3930142 Vali Loss: 0.9212108 Test Loss: 0.4084883
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.2718706130981445
Epoch: 100, Steps: 63 | Train Loss: 0.3930999 Vali Loss: 0.9212623 Test Loss: 0.4084864
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4043901264667511, mae:0.4130895733833313, rse:0.6038896441459656, corr:[0.26024845 0.26911223 0.2690541  0.27142695 0.26877636 0.2662225
 0.2660145  0.26569837 0.2648351  0.26501387 0.26511034 0.26437744
 0.26409787 0.2641279  0.2637547  0.26364484 0.26377165 0.26348865
 0.26325938 0.26332474 0.2631968  0.26292923 0.26335043 0.2638536
 0.26333636 0.26290083 0.262905   0.2625391  0.26194802 0.2618118
 0.2616737  0.26102743 0.26056623 0.2606793  0.2607077  0.26050127
 0.26065326 0.26096287 0.2609123  0.26089314 0.26123676 0.26139203
 0.26139733 0.26141492 0.2612153  0.26086113 0.26091418 0.26108548
 0.2606079  0.25965118 0.2588542  0.25835255 0.2576711  0.25665286
 0.25605682 0.2557353  0.25538057 0.25516823 0.25497127 0.25489113
 0.25480178 0.25475815 0.25466356 0.25459698 0.25471944 0.2550764
 0.25542164 0.25532934 0.25516266 0.25527054 0.25535962 0.25511032
 0.25452077 0.25372124 0.25300127 0.25263903 0.2525179  0.25224727
 0.25181952 0.2513768  0.25108445 0.2507958  0.25052992 0.2503897
 0.25030327 0.2502382  0.25030592 0.2503744  0.25031906 0.25030392
 0.25026563 0.2499977  0.2496462  0.24951558 0.24962175 0.25013414
 0.25076967 0.25085306 0.2508766  0.2509087  0.25085464 0.2506793
 0.25051016 0.25051507 0.25043038 0.25025168 0.25006452 0.24984281
 0.24959695 0.24950363 0.2496319  0.24995743 0.25030142 0.25054064
 0.2505026  0.25026304 0.25003162 0.24978927 0.24944891 0.24940114
 0.2495026  0.24895763 0.24815334 0.24763925 0.24717557 0.24646544
 0.2462261  0.246394   0.24618275 0.24569654 0.24552353 0.24535765
 0.2450032  0.24503271 0.24524404 0.24506196 0.2452685  0.24572946
 0.24589579 0.24578142 0.24581255 0.24579744 0.24537611 0.24520475
 0.2453093  0.24471338 0.24412873 0.24381028 0.24312466 0.2420966
 0.24198239 0.24234053 0.24210906 0.24208215 0.24241821 0.2423876
 0.24198015 0.24220067 0.24245334 0.24207835 0.24199194 0.24240372
 0.2423749  0.24207811 0.24228525 0.24221312 0.24159366 0.24163161
 0.24203146 0.2417177  0.24184404 0.24258022 0.24232446 0.24149843
 0.24172239 0.24211888 0.24169078 0.24182412 0.24236983 0.24209626
 0.24189588 0.24238557 0.24233086 0.24195759 0.2424792  0.24253686
 0.24179605 0.24264476 0.24260038 0.23992655 0.24172115 0.23770356]
