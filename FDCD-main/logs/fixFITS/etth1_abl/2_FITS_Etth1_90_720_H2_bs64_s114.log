Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.3087632656097412
Epoch: 1, Steps: 61 | Train Loss: 1.5730398 Vali Loss: 3.1772647 Test Loss: 1.7478344
Validation loss decreased (inf --> 3.177265).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3332812786102295
Epoch: 2, Steps: 61 | Train Loss: 1.2184261 Vali Loss: 2.5940154 Test Loss: 1.2777028
Validation loss decreased (3.177265 --> 2.594015).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2791821956634521
Epoch: 3, Steps: 61 | Train Loss: 0.9992276 Vali Loss: 2.2590737 Test Loss: 1.0044353
Validation loss decreased (2.594015 --> 2.259074).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3348612785339355
Epoch: 4, Steps: 61 | Train Loss: 0.8617078 Vali Loss: 2.0394475 Test Loss: 0.8354958
Validation loss decreased (2.259074 --> 2.039448).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.389115571975708
Epoch: 5, Steps: 61 | Train Loss: 0.7719374 Vali Loss: 1.9089746 Test Loss: 0.7281282
Validation loss decreased (2.039448 --> 1.908975).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.2722718715667725
Epoch: 6, Steps: 61 | Train Loss: 0.7119034 Vali Loss: 1.8202916 Test Loss: 0.6569289
Validation loss decreased (1.908975 --> 1.820292).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.2925384044647217
Epoch: 7, Steps: 61 | Train Loss: 0.6710699 Vali Loss: 1.7564094 Test Loss: 0.6087013
Validation loss decreased (1.820292 --> 1.756409).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2939822673797607
Epoch: 8, Steps: 61 | Train Loss: 0.6421311 Vali Loss: 1.7230109 Test Loss: 0.5750919
Validation loss decreased (1.756409 --> 1.723011).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.404062032699585
Epoch: 9, Steps: 61 | Train Loss: 0.6216764 Vali Loss: 1.6822927 Test Loss: 0.5513758
Validation loss decreased (1.723011 --> 1.682293).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.390596866607666
Epoch: 10, Steps: 61 | Train Loss: 0.6067665 Vali Loss: 1.6608951 Test Loss: 0.5341048
Validation loss decreased (1.682293 --> 1.660895).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.253906488418579
Epoch: 11, Steps: 61 | Train Loss: 0.5961479 Vali Loss: 1.6537952 Test Loss: 0.5215581
Validation loss decreased (1.660895 --> 1.653795).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.2926063537597656
Epoch: 12, Steps: 61 | Train Loss: 0.5876654 Vali Loss: 1.6308554 Test Loss: 0.5121031
Validation loss decreased (1.653795 --> 1.630855).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.3838787078857422
Epoch: 13, Steps: 61 | Train Loss: 0.5817132 Vali Loss: 1.6256862 Test Loss: 0.5049669
Validation loss decreased (1.630855 --> 1.625686).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5712010860443115
Epoch: 14, Steps: 61 | Train Loss: 0.5765366 Vali Loss: 1.6174383 Test Loss: 0.4995642
Validation loss decreased (1.625686 --> 1.617438).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4412333965301514
Epoch: 15, Steps: 61 | Train Loss: 0.5728261 Vali Loss: 1.6137371 Test Loss: 0.4953092
Validation loss decreased (1.617438 --> 1.613737).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.3269844055175781
Epoch: 16, Steps: 61 | Train Loss: 0.5696928 Vali Loss: 1.6165494 Test Loss: 0.4919482
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.7671411037445068
Epoch: 17, Steps: 61 | Train Loss: 0.5675961 Vali Loss: 1.6074133 Test Loss: 0.4893329
Validation loss decreased (1.613737 --> 1.607413).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.2246387004852295
Epoch: 18, Steps: 61 | Train Loss: 0.5655541 Vali Loss: 1.6003785 Test Loss: 0.4871770
Validation loss decreased (1.607413 --> 1.600379).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4295780658721924
Epoch: 19, Steps: 61 | Train Loss: 0.5640297 Vali Loss: 1.6024294 Test Loss: 0.4854381
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.43324613571167
Epoch: 20, Steps: 61 | Train Loss: 0.5624502 Vali Loss: 1.5984694 Test Loss: 0.4839591
Validation loss decreased (1.600379 --> 1.598469).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.364264965057373
Epoch: 21, Steps: 61 | Train Loss: 0.5614592 Vali Loss: 1.5984802 Test Loss: 0.4827799
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.3147616386413574
Epoch: 22, Steps: 61 | Train Loss: 0.5605209 Vali Loss: 1.5980878 Test Loss: 0.4818153
Validation loss decreased (1.598469 --> 1.598088).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.2731530666351318
Epoch: 23, Steps: 61 | Train Loss: 0.5596870 Vali Loss: 1.5837834 Test Loss: 0.4809223
Validation loss decreased (1.598088 --> 1.583783).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.3404061794281006
Epoch: 24, Steps: 61 | Train Loss: 0.5585087 Vali Loss: 1.5852793 Test Loss: 0.4802803
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.3074700832366943
Epoch: 25, Steps: 61 | Train Loss: 0.5581654 Vali Loss: 1.5928664 Test Loss: 0.4796912
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4874343872070312
Epoch: 26, Steps: 61 | Train Loss: 0.5573298 Vali Loss: 1.5878534 Test Loss: 0.4791600
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4501404762268066
Epoch: 27, Steps: 61 | Train Loss: 0.5572979 Vali Loss: 1.5809152 Test Loss: 0.4787127
Validation loss decreased (1.583783 --> 1.580915).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.348886251449585
Epoch: 28, Steps: 61 | Train Loss: 0.5564698 Vali Loss: 1.5877242 Test Loss: 0.4782970
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.2997043132781982
Epoch: 29, Steps: 61 | Train Loss: 0.5562433 Vali Loss: 1.5881853 Test Loss: 0.4779685
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4452855587005615
Epoch: 30, Steps: 61 | Train Loss: 0.5557316 Vali Loss: 1.5836163 Test Loss: 0.4776771
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5579054355621338
Epoch: 31, Steps: 61 | Train Loss: 0.5553315 Vali Loss: 1.5832497 Test Loss: 0.4773911
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.3713791370391846
Epoch: 32, Steps: 61 | Train Loss: 0.5552982 Vali Loss: 1.5836239 Test Loss: 0.4771393
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.378016471862793
Epoch: 33, Steps: 61 | Train Loss: 0.5548256 Vali Loss: 1.5868759 Test Loss: 0.4769168
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3098573684692383
Epoch: 34, Steps: 61 | Train Loss: 0.5545529 Vali Loss: 1.5830426 Test Loss: 0.4767655
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.4507923126220703
Epoch: 35, Steps: 61 | Train Loss: 0.5544884 Vali Loss: 1.5814397 Test Loss: 0.4765649
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.4329020977020264
Epoch: 36, Steps: 61 | Train Loss: 0.5544133 Vali Loss: 1.5840564 Test Loss: 0.4764168
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.391376256942749
Epoch: 37, Steps: 61 | Train Loss: 0.5538434 Vali Loss: 1.5734552 Test Loss: 0.4762769
Validation loss decreased (1.580915 --> 1.573455).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4805107116699219
Epoch: 38, Steps: 61 | Train Loss: 0.5539328 Vali Loss: 1.5780630 Test Loss: 0.4761594
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.5445315837860107
Epoch: 39, Steps: 61 | Train Loss: 0.5536650 Vali Loss: 1.5789566 Test Loss: 0.4760287
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.5246288776397705
Epoch: 40, Steps: 61 | Train Loss: 0.5535088 Vali Loss: 1.5754350 Test Loss: 0.4759204
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.3787977695465088
Epoch: 41, Steps: 61 | Train Loss: 0.5537586 Vali Loss: 1.5719539 Test Loss: 0.4758507
Validation loss decreased (1.573455 --> 1.571954).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3628334999084473
Epoch: 42, Steps: 61 | Train Loss: 0.5532842 Vali Loss: 1.5812891 Test Loss: 0.4757538
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.376915454864502
Epoch: 43, Steps: 61 | Train Loss: 0.5530410 Vali Loss: 1.5772393 Test Loss: 0.4756718
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.2933855056762695
Epoch: 44, Steps: 61 | Train Loss: 0.5530641 Vali Loss: 1.5844977 Test Loss: 0.4756107
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.4099085330963135
Epoch: 45, Steps: 61 | Train Loss: 0.5529972 Vali Loss: 1.5854870 Test Loss: 0.4755391
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.3662712574005127
Epoch: 46, Steps: 61 | Train Loss: 0.5528828 Vali Loss: 1.5798569 Test Loss: 0.4754878
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.3325979709625244
Epoch: 47, Steps: 61 | Train Loss: 0.5526984 Vali Loss: 1.5802497 Test Loss: 0.4754367
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.374115228652954
Epoch: 48, Steps: 61 | Train Loss: 0.5523887 Vali Loss: 1.5758522 Test Loss: 0.4753738
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.2969310283660889
Epoch: 49, Steps: 61 | Train Loss: 0.5525215 Vali Loss: 1.5707099 Test Loss: 0.4753219
Validation loss decreased (1.571954 --> 1.570710).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.3291707038879395
Epoch: 50, Steps: 61 | Train Loss: 0.5526031 Vali Loss: 1.5875816 Test Loss: 0.4752858
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.348271369934082
Epoch: 51, Steps: 61 | Train Loss: 0.5521252 Vali Loss: 1.5819447 Test Loss: 0.4752416
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3443334102630615
Epoch: 52, Steps: 61 | Train Loss: 0.5522128 Vali Loss: 1.5848799 Test Loss: 0.4751910
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.3780121803283691
Epoch: 53, Steps: 61 | Train Loss: 0.5522638 Vali Loss: 1.5806839 Test Loss: 0.4751720
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.4814248085021973
Epoch: 54, Steps: 61 | Train Loss: 0.5523479 Vali Loss: 1.5809705 Test Loss: 0.4751298
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.426795482635498
Epoch: 55, Steps: 61 | Train Loss: 0.5520890 Vali Loss: 1.5729320 Test Loss: 0.4751031
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.4712433815002441
Epoch: 56, Steps: 61 | Train Loss: 0.5523103 Vali Loss: 1.5735506 Test Loss: 0.4750675
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.3791658878326416
Epoch: 57, Steps: 61 | Train Loss: 0.5521638 Vali Loss: 1.5627334 Test Loss: 0.4750509
Validation loss decreased (1.570710 --> 1.562733).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.4802112579345703
Epoch: 58, Steps: 61 | Train Loss: 0.5521553 Vali Loss: 1.5800688 Test Loss: 0.4750172
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.3053314685821533
Epoch: 59, Steps: 61 | Train Loss: 0.5520627 Vali Loss: 1.5745848 Test Loss: 0.4749929
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.3679368495941162
Epoch: 60, Steps: 61 | Train Loss: 0.5520460 Vali Loss: 1.5759661 Test Loss: 0.4749782
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.405250072479248
Epoch: 61, Steps: 61 | Train Loss: 0.5516646 Vali Loss: 1.5818741 Test Loss: 0.4749627
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.239553451538086
Epoch: 62, Steps: 61 | Train Loss: 0.5517449 Vali Loss: 1.5740175 Test Loss: 0.4749295
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.4143404960632324
Epoch: 63, Steps: 61 | Train Loss: 0.5516982 Vali Loss: 1.5788870 Test Loss: 0.4749174
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.3939402103424072
Epoch: 64, Steps: 61 | Train Loss: 0.5517609 Vali Loss: 1.5728924 Test Loss: 0.4748914
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.3407905101776123
Epoch: 65, Steps: 61 | Train Loss: 0.5518440 Vali Loss: 1.5777357 Test Loss: 0.4748797
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.3341827392578125
Epoch: 66, Steps: 61 | Train Loss: 0.5515395 Vali Loss: 1.5817466 Test Loss: 0.4748651
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.4038825035095215
Epoch: 67, Steps: 61 | Train Loss: 0.5515360 Vali Loss: 1.5759789 Test Loss: 0.4748584
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.560389518737793
Epoch: 68, Steps: 61 | Train Loss: 0.5519563 Vali Loss: 1.5775779 Test Loss: 0.4748395
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.4624385833740234
Epoch: 69, Steps: 61 | Train Loss: 0.5516309 Vali Loss: 1.5707200 Test Loss: 0.4748284
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.26385498046875
Epoch: 70, Steps: 61 | Train Loss: 0.5513238 Vali Loss: 1.5706465 Test Loss: 0.4748206
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.3187401294708252
Epoch: 71, Steps: 61 | Train Loss: 0.5515168 Vali Loss: 1.5708215 Test Loss: 0.4748057
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.3241517543792725
Epoch: 72, Steps: 61 | Train Loss: 0.5515588 Vali Loss: 1.5762862 Test Loss: 0.4747994
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.3936591148376465
Epoch: 73, Steps: 61 | Train Loss: 0.5516458 Vali Loss: 1.5737371 Test Loss: 0.4747867
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.1505908966064453
Epoch: 74, Steps: 61 | Train Loss: 0.5513791 Vali Loss: 1.5772338 Test Loss: 0.4747776
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.3234341144561768
Epoch: 75, Steps: 61 | Train Loss: 0.5513850 Vali Loss: 1.5839640 Test Loss: 0.4747714
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.4599275588989258
Epoch: 76, Steps: 61 | Train Loss: 0.5514213 Vali Loss: 1.5789422 Test Loss: 0.4747633
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.395608901977539
Epoch: 77, Steps: 61 | Train Loss: 0.5514067 Vali Loss: 1.5775214 Test Loss: 0.4747556
EarlyStopping counter: 20 out of 20
Early stopping
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.4939460754394531
Epoch: 1, Steps: 61 | Train Loss: 0.6161416 Vali Loss: 1.5730156 Test Loss: 0.4743111
Validation loss decreased (inf --> 1.573016).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5832428932189941
Epoch: 2, Steps: 61 | Train Loss: 0.6140721 Vali Loss: 1.5819440 Test Loss: 0.4737095
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.3453357219696045
Epoch: 3, Steps: 61 | Train Loss: 0.6136175 Vali Loss: 1.5768790 Test Loss: 0.4735663
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.270869255065918
Epoch: 4, Steps: 61 | Train Loss: 0.6134363 Vali Loss: 1.5767305 Test Loss: 0.4735001
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.2888543605804443
Epoch: 5, Steps: 61 | Train Loss: 0.6131614 Vali Loss: 1.5702152 Test Loss: 0.4736855
Validation loss decreased (1.573016 --> 1.570215).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.3802194595336914
Epoch: 6, Steps: 61 | Train Loss: 0.6130262 Vali Loss: 1.5739188 Test Loss: 0.4738187
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.3608489036560059
Epoch: 7, Steps: 61 | Train Loss: 0.6129950 Vali Loss: 1.5682549 Test Loss: 0.4737958
Validation loss decreased (1.570215 --> 1.568255).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.368314266204834
Epoch: 8, Steps: 61 | Train Loss: 0.6128350 Vali Loss: 1.5707607 Test Loss: 0.4740455
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.5480515956878662
Epoch: 9, Steps: 61 | Train Loss: 0.6132226 Vali Loss: 1.5707777 Test Loss: 0.4741313
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.4938857555389404
Epoch: 10, Steps: 61 | Train Loss: 0.6125295 Vali Loss: 1.5706558 Test Loss: 0.4742022
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.4500787258148193
Epoch: 11, Steps: 61 | Train Loss: 0.6127478 Vali Loss: 1.5704542 Test Loss: 0.4742235
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.3947944641113281
Epoch: 12, Steps: 61 | Train Loss: 0.6125709 Vali Loss: 1.5707835 Test Loss: 0.4744189
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.3359858989715576
Epoch: 13, Steps: 61 | Train Loss: 0.6126144 Vali Loss: 1.5628946 Test Loss: 0.4744768
Validation loss decreased (1.568255 --> 1.562895).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.3655674457550049
Epoch: 14, Steps: 61 | Train Loss: 0.6123654 Vali Loss: 1.5713561 Test Loss: 0.4746105
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4046568870544434
Epoch: 15, Steps: 61 | Train Loss: 0.6124190 Vali Loss: 1.5745395 Test Loss: 0.4747782
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.5285975933074951
Epoch: 16, Steps: 61 | Train Loss: 0.6124724 Vali Loss: 1.5744824 Test Loss: 0.4747559
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3931195735931396
Epoch: 17, Steps: 61 | Train Loss: 0.6121809 Vali Loss: 1.5716207 Test Loss: 0.4748273
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7386465072631836
Epoch: 18, Steps: 61 | Train Loss: 0.6121778 Vali Loss: 1.5753520 Test Loss: 0.4749008
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.829390525817871
Epoch: 19, Steps: 61 | Train Loss: 0.6126223 Vali Loss: 1.5655572 Test Loss: 0.4749462
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.352360725402832
Epoch: 20, Steps: 61 | Train Loss: 0.6124383 Vali Loss: 1.5635998 Test Loss: 0.4749873
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4314684867858887
Epoch: 21, Steps: 61 | Train Loss: 0.6121085 Vali Loss: 1.5713816 Test Loss: 0.4751394
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2648651599884033
Epoch: 22, Steps: 61 | Train Loss: 0.6118789 Vali Loss: 1.5605197 Test Loss: 0.4751321
Validation loss decreased (1.562895 --> 1.560520).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.4335711002349854
Epoch: 23, Steps: 61 | Train Loss: 0.6119883 Vali Loss: 1.5671802 Test Loss: 0.4753281
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.4688467979431152
Epoch: 24, Steps: 61 | Train Loss: 0.6122560 Vali Loss: 1.5716304 Test Loss: 0.4753051
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.538057804107666
Epoch: 25, Steps: 61 | Train Loss: 0.6122542 Vali Loss: 1.5725893 Test Loss: 0.4753859
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5002772808074951
Epoch: 26, Steps: 61 | Train Loss: 0.6123206 Vali Loss: 1.5744296 Test Loss: 0.4753866
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.067429304122925
Epoch: 27, Steps: 61 | Train Loss: 0.6120803 Vali Loss: 1.5649513 Test Loss: 0.4754871
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5694184303283691
Epoch: 28, Steps: 61 | Train Loss: 0.6119536 Vali Loss: 1.5649967 Test Loss: 0.4754795
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.4460303783416748
Epoch: 29, Steps: 61 | Train Loss: 0.6118051 Vali Loss: 1.5617043 Test Loss: 0.4755927
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5132346153259277
Epoch: 30, Steps: 61 | Train Loss: 0.6119624 Vali Loss: 1.5719273 Test Loss: 0.4756044
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5378785133361816
Epoch: 31, Steps: 61 | Train Loss: 0.6123024 Vali Loss: 1.5708306 Test Loss: 0.4756258
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.4500114917755127
Epoch: 32, Steps: 61 | Train Loss: 0.6122094 Vali Loss: 1.5690913 Test Loss: 0.4756811
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4492006301879883
Epoch: 33, Steps: 61 | Train Loss: 0.6122680 Vali Loss: 1.5653472 Test Loss: 0.4756800
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.4921717643737793
Epoch: 34, Steps: 61 | Train Loss: 0.6120869 Vali Loss: 1.5701449 Test Loss: 0.4757163
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.4663317203521729
Epoch: 35, Steps: 61 | Train Loss: 0.6122389 Vali Loss: 1.5683641 Test Loss: 0.4757523
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.4437406063079834
Epoch: 36, Steps: 61 | Train Loss: 0.6121375 Vali Loss: 1.5718497 Test Loss: 0.4757767
EarlyStopping counter: 14 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.4751546382904053
Epoch: 37, Steps: 61 | Train Loss: 0.6123885 Vali Loss: 1.5663183 Test Loss: 0.4758103
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.407031774520874
Epoch: 38, Steps: 61 | Train Loss: 0.6117747 Vali Loss: 1.5710020 Test Loss: 0.4758490
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.4648869037628174
Epoch: 39, Steps: 61 | Train Loss: 0.6121405 Vali Loss: 1.5599689 Test Loss: 0.4758313
Validation loss decreased (1.560520 --> 1.559969).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.508845329284668
Epoch: 40, Steps: 61 | Train Loss: 0.6121594 Vali Loss: 1.5592163 Test Loss: 0.4758800
Validation loss decreased (1.559969 --> 1.559216).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.5265929698944092
Epoch: 41, Steps: 61 | Train Loss: 0.6117543 Vali Loss: 1.5667921 Test Loss: 0.4758834
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.572544813156128
Epoch: 42, Steps: 61 | Train Loss: 0.6120658 Vali Loss: 1.5634310 Test Loss: 0.4759106
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.5632758140563965
Epoch: 43, Steps: 61 | Train Loss: 0.6120073 Vali Loss: 1.5617673 Test Loss: 0.4759493
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.5452969074249268
Epoch: 44, Steps: 61 | Train Loss: 0.6119991 Vali Loss: 1.5731485 Test Loss: 0.4759810
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.5445313453674316
Epoch: 45, Steps: 61 | Train Loss: 0.6122346 Vali Loss: 1.5639434 Test Loss: 0.4759938
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.5036201477050781
Epoch: 46, Steps: 61 | Train Loss: 0.6120275 Vali Loss: 1.5682570 Test Loss: 0.4759680
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.4637691974639893
Epoch: 47, Steps: 61 | Train Loss: 0.6121473 Vali Loss: 1.5777042 Test Loss: 0.4760095
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.6027157306671143
Epoch: 48, Steps: 61 | Train Loss: 0.6118568 Vali Loss: 1.5646558 Test Loss: 0.4760326
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.4416348934173584
Epoch: 49, Steps: 61 | Train Loss: 0.6120098 Vali Loss: 1.5667760 Test Loss: 0.4760457
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.4894962310791016
Epoch: 50, Steps: 61 | Train Loss: 0.6116417 Vali Loss: 1.5612066 Test Loss: 0.4760600
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.5203824043273926
Epoch: 51, Steps: 61 | Train Loss: 0.6120009 Vali Loss: 1.5616322 Test Loss: 0.4760775
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.4869167804718018
Epoch: 52, Steps: 61 | Train Loss: 0.6119860 Vali Loss: 1.5647528 Test Loss: 0.4761127
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.7038092613220215
Epoch: 53, Steps: 61 | Train Loss: 0.6119786 Vali Loss: 1.5608883 Test Loss: 0.4760913
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.6027984619140625
Epoch: 54, Steps: 61 | Train Loss: 0.6120378 Vali Loss: 1.5678630 Test Loss: 0.4761107
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.5150153636932373
Epoch: 55, Steps: 61 | Train Loss: 0.6118405 Vali Loss: 1.5678806 Test Loss: 0.4761197
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.3421480655670166
Epoch: 56, Steps: 61 | Train Loss: 0.6118320 Vali Loss: 1.5642128 Test Loss: 0.4761412
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.2216150760650635
Epoch: 57, Steps: 61 | Train Loss: 0.6119865 Vali Loss: 1.5598195 Test Loss: 0.4761373
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.306725263595581
Epoch: 58, Steps: 61 | Train Loss: 0.6121950 Vali Loss: 1.5683928 Test Loss: 0.4761513
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.2996182441711426
Epoch: 59, Steps: 61 | Train Loss: 0.6118303 Vali Loss: 1.5600731 Test Loss: 0.4761657
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.276956558227539
Epoch: 60, Steps: 61 | Train Loss: 0.6122230 Vali Loss: 1.5700703 Test Loss: 0.4761730
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_90_720_FITS_ETTh1_ftM_sl90_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.47466322779655457, mae:0.4650725722312927, rse:0.6595454812049866, corr:[0.22616132 0.22914866 0.2294249  0.2270656  0.22366029 0.22214253
 0.22320053 0.22338241 0.2222664  0.22198004 0.22211532 0.22223619
 0.22182989 0.22089738 0.22048612 0.22085997 0.22134042 0.22125949
 0.22064301 0.2205877  0.22088705 0.2211964  0.22097932 0.22023241
 0.21874395 0.21828674 0.2186236  0.21881247 0.21848539 0.21854424
 0.21908756 0.2190336  0.21850596 0.21809885 0.21785992 0.21784212
 0.21782415 0.2173936  0.21723254 0.21743703 0.21805166 0.21841487
 0.21833873 0.21837434 0.21876067 0.21893676 0.21892141 0.21854593
 0.21739806 0.21691163 0.21654512 0.21556611 0.21413259 0.21331319
 0.21385162 0.2139113  0.21328354 0.21294114 0.2127472  0.21273555
 0.21239117 0.21164678 0.21113284 0.21106355 0.21162395 0.2121824
 0.21238717 0.21241163 0.21246    0.21260385 0.21232192 0.21127573
 0.2095365  0.20893098 0.20890379 0.20890658 0.20854688 0.20846105
 0.20923556 0.2093561  0.20882519 0.20828539 0.20791133 0.2079389
 0.20795226 0.20753376 0.20718646 0.20708425 0.2073273  0.20758834
 0.20741868 0.20729873 0.20745727 0.20782484 0.20829992 0.20823957
 0.20747982 0.20761031 0.20818052 0.2081554  0.20792377 0.2081126
 0.20890146 0.20914821 0.20869464 0.20814691 0.20772512 0.20756648
 0.20752443 0.2070868  0.20687836 0.20688455 0.20725295 0.20735915
 0.20732675 0.20740108 0.20773482 0.20793985 0.2077507  0.20700838
 0.2056969  0.2050306  0.20438999 0.2035338  0.202918   0.2030939
 0.20431978 0.20494014 0.20471655 0.2043011  0.20387891 0.20379606
 0.20370926 0.20319887 0.20291407 0.2029133  0.20327584 0.20338298
 0.20321651 0.20334539 0.20368543 0.20399366 0.20402087 0.20344655
 0.20218416 0.20170017 0.20164269 0.2007883  0.19957305 0.19929914
 0.20031375 0.20078455 0.20066448 0.20044301 0.20021312 0.20020376
 0.20014629 0.19966434 0.19919384 0.19907573 0.19943804 0.19975767
 0.19980855 0.19995417 0.20030199 0.20052904 0.20041066 0.19961536
 0.19847424 0.19853069 0.19914134 0.19913286 0.19811189 0.1976024
 0.1984488  0.19883294 0.19856355 0.1983249  0.19831707 0.19841146
 0.19831365 0.19763692 0.1970719  0.19689065 0.19728486 0.19798183
 0.19840045 0.19857791 0.19892932 0.19935653 0.19944514 0.19876085
 0.19711399 0.19643453 0.19613124 0.19534591 0.19421397 0.19384615
 0.19448368 0.19484712 0.19461033 0.19430952 0.1940228  0.19402602
 0.19386703 0.19334188 0.19293338 0.19288437 0.19324972 0.19343
 0.19333203 0.19344787 0.19369933 0.19386236 0.19363438 0.19291689
 0.19180593 0.19170965 0.19209577 0.1921169  0.19194396 0.19264694
 0.19434465 0.19531931 0.19548292 0.19526352 0.19478664 0.19462033
 0.19471975 0.19437274 0.19400279 0.19389687 0.19401203 0.19394596
 0.19371088 0.19369337 0.19395508 0.19414449 0.19409269 0.19344664
 0.19236882 0.1920619  0.192152   0.19196336 0.19152376 0.19174372
 0.19314419 0.19404589 0.19427995 0.19396563 0.193345   0.19302343
 0.19285321 0.19235198 0.19192949 0.1916412  0.19169216 0.19183774
 0.19195576 0.19198862 0.19208623 0.1921628  0.19220366 0.19177827
 0.19056486 0.19024198 0.19059566 0.19096136 0.19097498 0.19154195
 0.19291921 0.19379078 0.19392194 0.19379011 0.19344674 0.19315967
 0.19288452 0.19242752 0.19199613 0.19190913 0.19206804 0.19224447
 0.19228321 0.19253391 0.19300829 0.19337368 0.19361898 0.19346936
 0.19304162 0.1935109  0.19471517 0.1952718  0.19500439 0.1953019
 0.19650945 0.19701803 0.19696353 0.19667695 0.19623159 0.19601335
 0.19579507 0.19525592 0.19483522 0.19476156 0.19505523 0.19515559
 0.19507307 0.19516256 0.1954378  0.195678   0.19557954 0.19468917
 0.19348428 0.19326782 0.19365434 0.1934418  0.1927794  0.1925838
 0.19364026 0.19413355 0.19377966 0.19322965 0.19272245 0.19255492
 0.19247742 0.1920605  0.19161083 0.19164354 0.19195424 0.19221464
 0.19228406 0.1923465  0.19244014 0.19257028 0.19251941 0.19188112
 0.19062181 0.19014645 0.18997875 0.18969208 0.18934126 0.18971433
 0.19112337 0.19192645 0.19202603 0.19183965 0.19163373 0.19161159
 0.19154093 0.19116709 0.19094971 0.1910164  0.19118848 0.19134454
 0.19142595 0.19158755 0.19165653 0.1915725  0.1911128  0.1901114
 0.18887617 0.18856658 0.18860342 0.18797405 0.18709032 0.18673176
 0.18781634 0.18839172 0.18845186 0.18821703 0.18786709 0.18767333
 0.18753374 0.18695962 0.18639268 0.18611568 0.1863359  0.18670681
 0.18684961 0.18698455 0.18711257 0.18727599 0.18728498 0.18723093
 0.18691598 0.18744837 0.18816498 0.1883479  0.18810442 0.1881321
 0.18933803 0.18990543 0.18964283 0.18918559 0.18884435 0.18889661
 0.18892917 0.18872207 0.18858087 0.18857042 0.18886457 0.18938495
 0.18964979 0.18992598 0.19021188 0.19049382 0.19070306 0.19049475
 0.18966158 0.18968835 0.19027162 0.19039498 0.19021021 0.19059019
 0.19212049 0.19284713 0.19283645 0.19257551 0.19247004 0.19258024
 0.1925241  0.19220233 0.19167738 0.19146396 0.19166711 0.19191523
 0.19196881 0.19223759 0.19255342 0.19300586 0.19319354 0.1929111
 0.1920859  0.1919998  0.19240461 0.19253473 0.19247389 0.19320709
 0.19486539 0.1957554  0.19573477 0.19543229 0.19527265 0.19549724
 0.19561549 0.19511315 0.1945512  0.19443147 0.19472006 0.19499771
 0.19509378 0.1951753  0.19535461 0.19565625 0.19582081 0.1958578
 0.19554678 0.19608565 0.19673021 0.19679244 0.19638017 0.19639005
 0.19746603 0.19802211 0.19808531 0.19804016 0.1980328  0.19825453
 0.19845243 0.19838068 0.19822338 0.19827455 0.19868396 0.19899991
 0.19909951 0.19921692 0.19932723 0.19921632 0.19902597 0.1984693
 0.19733717 0.19700657 0.19705622 0.19695783 0.19671933 0.19698842
 0.19836956 0.19885646 0.19872965 0.19848245 0.19819425 0.19811857
 0.19805267 0.19764888 0.19727518 0.19726975 0.19754167 0.19770646
 0.1977424  0.19789656 0.19815782 0.1984663  0.19888327 0.19908243
 0.19877382 0.19939116 0.20028472 0.20045441 0.19995318 0.19994336
 0.20104574 0.20128109 0.20090815 0.2003369  0.19998056 0.2000774
 0.20025332 0.20009463 0.20005807 0.20032932 0.20081499 0.20105833
 0.20108257 0.2013696  0.20188993 0.20245737 0.20293641 0.20280363
 0.20170704 0.20123322 0.20128845 0.20121466 0.20067501 0.20057262
 0.20156305 0.20208205 0.20192167 0.20160168 0.20143707 0.20167804
 0.20180804 0.20160048 0.20146087 0.20160706 0.20204006 0.20228297
 0.20222746 0.20231673 0.2025266  0.20262143 0.20236434 0.20150587
 0.20001863 0.19940577 0.19954576 0.19914444 0.19818014 0.19782437
 0.19874685 0.19906473 0.19881693 0.19832712 0.19797726 0.19807142
 0.19820665 0.19783273 0.19739501 0.1973021  0.1976482  0.1980054
 0.19812982 0.19816938 0.19835901 0.19856405 0.19864024 0.19797313
 0.19642347 0.19591197 0.19612575 0.19621682 0.195878   0.19570483
 0.19672002 0.19724198 0.19698103 0.1966916  0.19651188 0.19662914
 0.19657302 0.19607837 0.19566937 0.19569628 0.19606885 0.19631577
 0.1963439  0.19643696 0.19654341 0.196516   0.19618678 0.19527254
 0.19352336 0.19264582 0.19254367 0.19220518 0.1916061  0.19156198
 0.1924391  0.1926921  0.1923929  0.19218853 0.1921192  0.1921583
 0.19205536 0.19172975 0.19155188 0.19178605 0.19213022 0.19221361
 0.19206348 0.19217499 0.192409   0.19262202 0.19260617 0.19189432
 0.19036008 0.18967435 0.18945062 0.18906482 0.18829452 0.18810178
 0.18906382 0.18921338 0.18853842 0.18799137 0.18781325 0.18809703
 0.18819436 0.18759368 0.18697555 0.1868165  0.18699503 0.18708088
 0.1867894  0.18674313 0.18702637 0.18750623 0.18776202 0.18740505
 0.18604639 0.18530452 0.18529493 0.18539977 0.1848797  0.18462019
 0.18568708 0.18585949 0.18533857 0.18469545 0.18430139 0.18425536
 0.1841612  0.18356197 0.18299048 0.18281618 0.182995   0.1831178
 0.1829223  0.18281052 0.1829163  0.18312989 0.1830184  0.18183483
 0.17918222 0.1769384  0.1757129  0.1743394  0.17237987 0.1710783
 0.17112446 0.17083542 0.17015447 0.16947025 0.1691907  0.16946904
 0.16970104 0.16943015 0.16898409 0.16888164 0.16905378 0.16907828
 0.16885705 0.16889878 0.16926633 0.16976301 0.1700024  0.16939431
 0.16760017 0.16669092 0.16650648 0.16574724 0.16460426 0.16422568
 0.16535829 0.16560476 0.16496804 0.1641362  0.16345324 0.16312335
 0.16289733 0.1620822  0.16126162 0.16063385 0.16052866 0.16055523
 0.16088139 0.16137269 0.16183153 0.1619024  0.16259326 0.16327983]
