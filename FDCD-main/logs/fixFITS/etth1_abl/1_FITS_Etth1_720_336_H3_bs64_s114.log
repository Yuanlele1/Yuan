Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=103, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=103, out_features=151, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  13935488.0
params:  15704.0
Trainable parameters:  15704
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.6658289432525635
Epoch: 1, Steps: 59 | Train Loss: 0.7507766 Vali Loss: 1.4554592 Test Loss: 0.6324889
Validation loss decreased (inf --> 1.455459).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.523089170455933
Epoch: 2, Steps: 59 | Train Loss: 0.5831972 Vali Loss: 1.3000016 Test Loss: 0.5312834
Validation loss decreased (1.455459 --> 1.300002).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.372153043746948
Epoch: 3, Steps: 59 | Train Loss: 0.5272893 Vali Loss: 1.2406368 Test Loss: 0.4874448
Validation loss decreased (1.300002 --> 1.240637).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.552083730697632
Epoch: 4, Steps: 59 | Train Loss: 0.4994747 Vali Loss: 1.2093900 Test Loss: 0.4642933
Validation loss decreased (1.240637 --> 1.209390).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.306203842163086
Epoch: 5, Steps: 59 | Train Loss: 0.4837626 Vali Loss: 1.1912124 Test Loss: 0.4519592
Validation loss decreased (1.209390 --> 1.191212).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.694589614868164
Epoch: 6, Steps: 59 | Train Loss: 0.4740071 Vali Loss: 1.1837639 Test Loss: 0.4451724
Validation loss decreased (1.191212 --> 1.183764).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.223572492599487
Epoch: 7, Steps: 59 | Train Loss: 0.4674345 Vali Loss: 1.1796815 Test Loss: 0.4417221
Validation loss decreased (1.183764 --> 1.179682).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.554598808288574
Epoch: 8, Steps: 59 | Train Loss: 0.4630505 Vali Loss: 1.1797343 Test Loss: 0.4401655
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.66324520111084
Epoch: 9, Steps: 59 | Train Loss: 0.4597902 Vali Loss: 1.1837862 Test Loss: 0.4396226
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.602752447128296
Epoch: 10, Steps: 59 | Train Loss: 0.4574254 Vali Loss: 1.1817669 Test Loss: 0.4396769
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.766922950744629
Epoch: 11, Steps: 59 | Train Loss: 0.4559430 Vali Loss: 1.1831270 Test Loss: 0.4398713
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.925938606262207
Epoch: 12, Steps: 59 | Train Loss: 0.4542844 Vali Loss: 1.1818515 Test Loss: 0.4401147
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.405752658843994
Epoch: 13, Steps: 59 | Train Loss: 0.4530704 Vali Loss: 1.1894592 Test Loss: 0.4402919
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.478593587875366
Epoch: 14, Steps: 59 | Train Loss: 0.4517883 Vali Loss: 1.1873795 Test Loss: 0.4408359
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.7061240673065186
Epoch: 15, Steps: 59 | Train Loss: 0.4508953 Vali Loss: 1.1869770 Test Loss: 0.4410613
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.555964231491089
Epoch: 16, Steps: 59 | Train Loss: 0.4497793 Vali Loss: 1.1881549 Test Loss: 0.4412419
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.3565826416015625
Epoch: 17, Steps: 59 | Train Loss: 0.4492871 Vali Loss: 1.1936973 Test Loss: 0.4415916
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.180382490158081
Epoch: 18, Steps: 59 | Train Loss: 0.4489435 Vali Loss: 1.1925117 Test Loss: 0.4418927
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.4898741245269775
Epoch: 19, Steps: 59 | Train Loss: 0.4479807 Vali Loss: 1.1929318 Test Loss: 0.4421578
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.682412624359131
Epoch: 20, Steps: 59 | Train Loss: 0.4478166 Vali Loss: 1.1928502 Test Loss: 0.4424318
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.649769306182861
Epoch: 21, Steps: 59 | Train Loss: 0.4472446 Vali Loss: 1.1982248 Test Loss: 0.4426061
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.5208210945129395
Epoch: 22, Steps: 59 | Train Loss: 0.4468347 Vali Loss: 1.1927227 Test Loss: 0.4428017
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.58760929107666
Epoch: 23, Steps: 59 | Train Loss: 0.4465542 Vali Loss: 1.2003673 Test Loss: 0.4429050
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.730289936065674
Epoch: 24, Steps: 59 | Train Loss: 0.4462657 Vali Loss: 1.1976055 Test Loss: 0.4430617
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 4.5628063678741455
Epoch: 25, Steps: 59 | Train Loss: 0.4458184 Vali Loss: 1.1991718 Test Loss: 0.4433307
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.518916368484497
Epoch: 26, Steps: 59 | Train Loss: 0.4453733 Vali Loss: 1.1944125 Test Loss: 0.4434909
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.549077033996582
Epoch: 27, Steps: 59 | Train Loss: 0.4450213 Vali Loss: 1.2029613 Test Loss: 0.4435071
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4404735267162323, mae:0.44588252902030945, rse:0.6318470239639282, corr:[0.24726193 0.25801194 0.25667807 0.25207147 0.24862401 0.24796087
 0.24915539 0.2502319  0.2503257  0.24961704 0.24858066 0.24796487
 0.24806418 0.24850535 0.24861856 0.24808812 0.24713391 0.24609664
 0.24540296 0.24520084 0.24528413 0.2450413  0.24439636 0.24340925
 0.24261911 0.24235961 0.24268845 0.2432965  0.24396312 0.24451889
 0.24483648 0.24496555 0.24516088 0.24538572 0.24546966 0.2454551
 0.24546641 0.24548975 0.24557443 0.24572489 0.24579984 0.24573044
 0.2455443  0.24541609 0.24539688 0.24568425 0.24621707 0.24666202
 0.24670672 0.2462845  0.24553847 0.24484418 0.24425448 0.2437365
 0.24331176 0.24296764 0.24271244 0.2424718  0.24230345 0.24226506
 0.24236238 0.24255618 0.24267466 0.24277507 0.24280198 0.24281877
 0.24284802 0.24277456 0.24279733 0.24272057 0.24245426 0.2419985
 0.24146622 0.24093229 0.24047627 0.2401399  0.23980986 0.23948105
 0.23922041 0.23912792 0.239115   0.23914094 0.23915264 0.23919176
 0.2391273  0.23895547 0.23869984 0.23847654 0.23825268 0.23803012
 0.23784183 0.237771   0.23767152 0.237651   0.23770213 0.23791635
 0.23818263 0.23832273 0.2383037  0.23829328 0.2384143  0.23858778
 0.23874538 0.23880842 0.23874734 0.23861231 0.23851609 0.23860121
 0.23887052 0.23931323 0.23965587 0.23976938 0.23959209 0.2392844
 0.2389899  0.2387721  0.23876119 0.23888381 0.23902051 0.23904128
 0.23879169 0.23818354 0.23738687 0.23665445 0.2361571  0.23586045
 0.23573469 0.23573841 0.23566847 0.2354739  0.23519394 0.23484305
 0.23447184 0.2341281  0.23404779 0.23421207 0.23443618 0.2346037
 0.23450296 0.23423894 0.23394363 0.23366176 0.23335862 0.23306194
 0.23264764 0.23200826 0.2312317  0.23053572 0.23014879 0.23000492
 0.23004726 0.2301539  0.23017733 0.22997135 0.22957245 0.22923212
 0.22896914 0.22889437 0.22901066 0.22916232 0.22909607 0.22896819
 0.22877578 0.22857532 0.22848463 0.22858272 0.22876085 0.2289056
 0.2289424  0.22892368 0.22882403 0.22872509 0.2286409  0.22862497
 0.22861654 0.22864854 0.22855395 0.22839282 0.22821392 0.22812772
 0.22805145 0.22798778 0.22797886 0.22811331 0.22840562 0.22874437
 0.22894801 0.22898714 0.22894426 0.22886151 0.2287111  0.22847731
 0.22810087 0.22755447 0.2268405  0.22610082 0.2255621  0.22529238
 0.22527708 0.22551595 0.22584493 0.22607295 0.22613035 0.2261511
 0.22616588 0.22633874 0.22663438 0.22693907 0.22720987 0.22725861
 0.22698684 0.22641222 0.2258405  0.22552429 0.22542968 0.22540462
 0.2251857  0.22471948 0.22406758 0.22344251 0.22310327 0.22288722
 0.22259519 0.22224575 0.22190054 0.22167985 0.22157805 0.22184162
 0.2222906  0.22260523 0.22263926 0.22242507 0.22193316 0.2215152
 0.2213555  0.22144121 0.22165231 0.22178161 0.2217695  0.2215506
 0.22134784 0.22124797 0.22140568 0.22167666 0.22173977 0.22150755
 0.22115472 0.2210003  0.22103927 0.2210949  0.22097476 0.22069213
 0.22033986 0.22001758 0.22003786 0.22036879 0.22090615 0.22135183
 0.22145975 0.22097135 0.2202045  0.21952862 0.21933095 0.21939209
 0.21952984 0.21952564 0.2191685  0.21855265 0.21778192 0.21718109
 0.2170716  0.21723206 0.21722245 0.21694927 0.2164827  0.21625744
 0.21659136 0.21740526 0.21807861 0.2182445  0.21780655 0.21711145
 0.2164505  0.21604505 0.21609685 0.21634658 0.21646683 0.21610516
 0.21535328 0.21468179 0.21419902 0.21414186 0.21425295 0.21431243
 0.21410343 0.21371523 0.21334635 0.2133131  0.2135628  0.21386564
 0.21384282 0.21327436 0.21267407 0.21246414 0.2130333  0.21407805
 0.21512537 0.21505615 0.21391773 0.21205358 0.21041368 0.2098205
 0.21049976 0.21157977 0.21171615 0.21044451 0.20843664 0.20652103
 0.2056134  0.20565164 0.20588602 0.20518903 0.20333831 0.20145361
 0.20110846 0.20220917 0.20350662 0.20329921 0.19970916 0.19370593
 0.1883144  0.18679364 0.18972988 0.19180049 0.18171924 0.1454868 ]
