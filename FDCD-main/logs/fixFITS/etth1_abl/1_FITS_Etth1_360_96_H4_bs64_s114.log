Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_96_FITS_ETTh1_ftM_sl360_ll48_pl96_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8185
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=74, out_features=93, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6166272.0
params:  6975.0
Trainable parameters:  6975
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.4510657787323
Epoch: 1, Steps: 63 | Train Loss: 0.5801242 Vali Loss: 1.0428568 Test Loss: 0.5408780
Validation loss decreased (inf --> 1.042857).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8137636184692383
Epoch: 2, Steps: 63 | Train Loss: 0.4299817 Vali Loss: 0.8688841 Test Loss: 0.4398086
Validation loss decreased (1.042857 --> 0.868884).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.6983680725097656
Epoch: 3, Steps: 63 | Train Loss: 0.3853855 Vali Loss: 0.8034701 Test Loss: 0.4043517
Validation loss decreased (0.868884 --> 0.803470).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.954493522644043
Epoch: 4, Steps: 63 | Train Loss: 0.3676955 Vali Loss: 0.7684347 Test Loss: 0.3904336
Validation loss decreased (0.803470 --> 0.768435).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.8555934429168701
Epoch: 5, Steps: 63 | Train Loss: 0.3595317 Vali Loss: 0.7511699 Test Loss: 0.3845413
Validation loss decreased (0.768435 --> 0.751170).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.873950958251953
Epoch: 6, Steps: 63 | Train Loss: 0.3544358 Vali Loss: 0.7415091 Test Loss: 0.3819354
Validation loss decreased (0.751170 --> 0.741509).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.894406318664551
Epoch: 7, Steps: 63 | Train Loss: 0.3523204 Vali Loss: 0.7301972 Test Loss: 0.3808751
Validation loss decreased (0.741509 --> 0.730197).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.8251137733459473
Epoch: 8, Steps: 63 | Train Loss: 0.3507304 Vali Loss: 0.7265394 Test Loss: 0.3799821
Validation loss decreased (0.730197 --> 0.726539).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.130950689315796
Epoch: 9, Steps: 63 | Train Loss: 0.3495026 Vali Loss: 0.7161216 Test Loss: 0.3797311
Validation loss decreased (0.726539 --> 0.716122).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.3216373920440674
Epoch: 10, Steps: 63 | Train Loss: 0.3485856 Vali Loss: 0.7136399 Test Loss: 0.3792931
Validation loss decreased (0.716122 --> 0.713640).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9607281684875488
Epoch: 11, Steps: 63 | Train Loss: 0.3479066 Vali Loss: 0.7093410 Test Loss: 0.3790718
Validation loss decreased (0.713640 --> 0.709341).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3516767024993896
Epoch: 12, Steps: 63 | Train Loss: 0.3473089 Vali Loss: 0.7061351 Test Loss: 0.3788750
Validation loss decreased (0.709341 --> 0.706135).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.508471727371216
Epoch: 13, Steps: 63 | Train Loss: 0.3473341 Vali Loss: 0.7039701 Test Loss: 0.3789274
Validation loss decreased (0.706135 --> 0.703970).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.9545581340789795
Epoch: 14, Steps: 63 | Train Loss: 0.3466247 Vali Loss: 0.7046590 Test Loss: 0.3786859
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.5615882873535156
Epoch: 15, Steps: 63 | Train Loss: 0.3458019 Vali Loss: 0.7033680 Test Loss: 0.3785588
Validation loss decreased (0.703970 --> 0.703368).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.4793529510498047
Epoch: 16, Steps: 63 | Train Loss: 0.3457810 Vali Loss: 0.7020218 Test Loss: 0.3784634
Validation loss decreased (0.703368 --> 0.702022).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.7356064319610596
Epoch: 17, Steps: 63 | Train Loss: 0.3455128 Vali Loss: 0.6956722 Test Loss: 0.3783267
Validation loss decreased (0.702022 --> 0.695672).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.1075456142425537
Epoch: 18, Steps: 63 | Train Loss: 0.3456725 Vali Loss: 0.6916302 Test Loss: 0.3783583
Validation loss decreased (0.695672 --> 0.691630).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.409771203994751
Epoch: 19, Steps: 63 | Train Loss: 0.3446057 Vali Loss: 0.6944531 Test Loss: 0.3783092
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.2450413703918457
Epoch: 20, Steps: 63 | Train Loss: 0.3453515 Vali Loss: 0.6953163 Test Loss: 0.3784190
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.498918294906616
Epoch: 21, Steps: 63 | Train Loss: 0.3453089 Vali Loss: 0.6960425 Test Loss: 0.3783789
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.051438570022583
Epoch: 22, Steps: 63 | Train Loss: 0.3456967 Vali Loss: 0.6938977 Test Loss: 0.3781562
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9723284244537354
Epoch: 23, Steps: 63 | Train Loss: 0.3445066 Vali Loss: 0.6888240 Test Loss: 0.3783380
Validation loss decreased (0.691630 --> 0.688824).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.0506370067596436
Epoch: 24, Steps: 63 | Train Loss: 0.3447679 Vali Loss: 0.6936510 Test Loss: 0.3781956
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.7571375370025635
Epoch: 25, Steps: 63 | Train Loss: 0.3440907 Vali Loss: 0.6880043 Test Loss: 0.3782898
Validation loss decreased (0.688824 --> 0.688004).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8971526622772217
Epoch: 26, Steps: 63 | Train Loss: 0.3437837 Vali Loss: 0.6839204 Test Loss: 0.3782704
Validation loss decreased (0.688004 --> 0.683920).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.7924175262451172
Epoch: 27, Steps: 63 | Train Loss: 0.3442663 Vali Loss: 0.6872827 Test Loss: 0.3781292
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.7419912815093994
Epoch: 28, Steps: 63 | Train Loss: 0.3440905 Vali Loss: 0.6924905 Test Loss: 0.3781405
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.8200044631958008
Epoch: 29, Steps: 63 | Train Loss: 0.3440476 Vali Loss: 0.6887353 Test Loss: 0.3783362
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9727795124053955
Epoch: 30, Steps: 63 | Train Loss: 0.3438288 Vali Loss: 0.6912214 Test Loss: 0.3782718
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.4716849327087402
Epoch: 31, Steps: 63 | Train Loss: 0.3445727 Vali Loss: 0.6871483 Test Loss: 0.3782940
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.210245370864868
Epoch: 32, Steps: 63 | Train Loss: 0.3442671 Vali Loss: 0.6882991 Test Loss: 0.3782715
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.975654363632202
Epoch: 33, Steps: 63 | Train Loss: 0.3437783 Vali Loss: 0.6856487 Test Loss: 0.3782933
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.6206815242767334
Epoch: 34, Steps: 63 | Train Loss: 0.3437314 Vali Loss: 0.6889594 Test Loss: 0.3782676
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.7239172458648682
Epoch: 35, Steps: 63 | Train Loss: 0.3437521 Vali Loss: 0.6874458 Test Loss: 0.3782698
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.345714807510376
Epoch: 36, Steps: 63 | Train Loss: 0.3435468 Vali Loss: 0.6879131 Test Loss: 0.3782736
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.3064136505126953
Epoch: 37, Steps: 63 | Train Loss: 0.3435566 Vali Loss: 0.6878018 Test Loss: 0.3781809
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7081310749053955
Epoch: 38, Steps: 63 | Train Loss: 0.3436356 Vali Loss: 0.6853827 Test Loss: 0.3782057
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.120816230773926
Epoch: 39, Steps: 63 | Train Loss: 0.3437917 Vali Loss: 0.6861376 Test Loss: 0.3781932
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.5632271766662598
Epoch: 40, Steps: 63 | Train Loss: 0.3438197 Vali Loss: 0.6883938 Test Loss: 0.3781547
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9184553623199463
Epoch: 41, Steps: 63 | Train Loss: 0.3432478 Vali Loss: 0.6863858 Test Loss: 0.3782596
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.084115982055664
Epoch: 42, Steps: 63 | Train Loss: 0.3435091 Vali Loss: 0.6847963 Test Loss: 0.3781942
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.279937267303467
Epoch: 43, Steps: 63 | Train Loss: 0.3430683 Vali Loss: 0.6852719 Test Loss: 0.3782822
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.7966578006744385
Epoch: 44, Steps: 63 | Train Loss: 0.3437489 Vali Loss: 0.6874232 Test Loss: 0.3782961
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7567198276519775
Epoch: 45, Steps: 63 | Train Loss: 0.3433692 Vali Loss: 0.6872939 Test Loss: 0.3782514
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.574890375137329
Epoch: 46, Steps: 63 | Train Loss: 0.3430184 Vali Loss: 0.6846146 Test Loss: 0.3782047
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_96_FITS_ETTh1_ftM_sl360_ll48_pl96_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.377323180437088, mae:0.3989507555961609, rse:0.5834646821022034, corr:[0.26932663 0.27855346 0.27838573 0.27632377 0.27487    0.2733763
 0.2717164  0.2708381  0.27068081 0.2709329  0.27107427 0.2707159
 0.27027565 0.2700767  0.27003697 0.27001524 0.26979417 0.26955473
 0.26949158 0.2694288  0.26927754 0.26907724 0.26897106 0.26902035
 0.26890197 0.26863703 0.26839635 0.2682162  0.26792967 0.26755708
 0.2670794  0.26655802 0.26618424 0.2660008  0.26604712 0.26611435
 0.26610997 0.26601547 0.26595694 0.2660277  0.26638964 0.26673314
 0.2668501  0.26667416 0.2663759  0.26609728 0.26602706 0.26590466
 0.265326   0.2644829  0.2635764  0.26287267 0.26220536 0.2613621
 0.26077718 0.26044682 0.2601732  0.25998622 0.25981355 0.25991228
 0.2600545  0.26004565 0.25985545 0.25980118 0.26004115 0.26052123
 0.26092097 0.26078692 0.26052576 0.2606258  0.2609385  0.26086
 0.26015246 0.2591566  0.2583167  0.25786233 0.25761658 0.25719014
 0.2566431  0.2560519  0.25564867 0.25542882 0.2550923  0.25470778
 0.25460857 0.25475204 0.2547809  0.25436375 0.25412303 0.25454453
 0.2549023  0.2539735  0.2522002  0.25156668 0.253232   0.25414875]
