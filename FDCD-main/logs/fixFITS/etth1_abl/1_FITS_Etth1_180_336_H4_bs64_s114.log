Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=42, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4515840.0
params:  5160.0
Trainable parameters:  5160
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.213789463043213
Epoch: 1, Steps: 63 | Train Loss: 0.8527225 Vali Loss: 1.7320398 Test Loss: 0.7904757
Validation loss decreased (inf --> 1.732040).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.7103664875030518
Epoch: 2, Steps: 63 | Train Loss: 0.6435637 Vali Loss: 1.4891585 Test Loss: 0.6175233
Validation loss decreased (1.732040 --> 1.489159).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.268895149230957
Epoch: 3, Steps: 63 | Train Loss: 0.5608818 Vali Loss: 1.3897358 Test Loss: 0.5431880
Validation loss decreased (1.489159 --> 1.389736).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.4414706230163574
Epoch: 4, Steps: 63 | Train Loss: 0.5236814 Vali Loss: 1.3341281 Test Loss: 0.5083524
Validation loss decreased (1.389736 --> 1.334128).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.0745742321014404
Epoch: 5, Steps: 63 | Train Loss: 0.5048745 Vali Loss: 1.3145975 Test Loss: 0.4902823
Validation loss decreased (1.334128 --> 1.314597).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.8435568809509277
Epoch: 6, Steps: 63 | Train Loss: 0.4951943 Vali Loss: 1.2948023 Test Loss: 0.4799506
Validation loss decreased (1.314597 --> 1.294802).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.901031255722046
Epoch: 7, Steps: 63 | Train Loss: 0.4887553 Vali Loss: 1.2795750 Test Loss: 0.4733960
Validation loss decreased (1.294802 --> 1.279575).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.7058138847351074
Epoch: 8, Steps: 63 | Train Loss: 0.4843019 Vali Loss: 1.2751411 Test Loss: 0.4689359
Validation loss decreased (1.279575 --> 1.275141).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.068340301513672
Epoch: 9, Steps: 63 | Train Loss: 0.4813262 Vali Loss: 1.2690907 Test Loss: 0.4656252
Validation loss decreased (1.275141 --> 1.269091).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.168888807296753
Epoch: 10, Steps: 63 | Train Loss: 0.4785268 Vali Loss: 1.2625961 Test Loss: 0.4632217
Validation loss decreased (1.269091 --> 1.262596).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.211667060852051
Epoch: 11, Steps: 63 | Train Loss: 0.4770041 Vali Loss: 1.2539495 Test Loss: 0.4612764
Validation loss decreased (1.262596 --> 1.253950).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.2045793533325195
Epoch: 12, Steps: 63 | Train Loss: 0.4754815 Vali Loss: 1.2585362 Test Loss: 0.4598447
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.852696180343628
Epoch: 13, Steps: 63 | Train Loss: 0.4740941 Vali Loss: 1.2505522 Test Loss: 0.4587336
Validation loss decreased (1.253950 --> 1.250552).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.0270071029663086
Epoch: 14, Steps: 63 | Train Loss: 0.4728158 Vali Loss: 1.2532979 Test Loss: 0.4578633
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.232931137084961
Epoch: 15, Steps: 63 | Train Loss: 0.4725815 Vali Loss: 1.2505606 Test Loss: 0.4571637
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.1843225955963135
Epoch: 16, Steps: 63 | Train Loss: 0.4716604 Vali Loss: 1.2459865 Test Loss: 0.4566733
Validation loss decreased (1.250552 --> 1.245986).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.8699889183044434
Epoch: 17, Steps: 63 | Train Loss: 0.4709945 Vali Loss: 1.2477287 Test Loss: 0.4563311
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.9030027389526367
Epoch: 18, Steps: 63 | Train Loss: 0.4703255 Vali Loss: 1.2458622 Test Loss: 0.4560305
Validation loss decreased (1.245986 --> 1.245862).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.7920351028442383
Epoch: 19, Steps: 63 | Train Loss: 0.4698961 Vali Loss: 1.2463752 Test Loss: 0.4558102
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.5270400047302246
Epoch: 20, Steps: 63 | Train Loss: 0.4696571 Vali Loss: 1.2446045 Test Loss: 0.4556219
Validation loss decreased (1.245862 --> 1.244604).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.9644291400909424
Epoch: 21, Steps: 63 | Train Loss: 0.4693268 Vali Loss: 1.2463987 Test Loss: 0.4555067
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.550267219543457
Epoch: 22, Steps: 63 | Train Loss: 0.4692257 Vali Loss: 1.2426528 Test Loss: 0.4554667
Validation loss decreased (1.244604 --> 1.242653).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.7765369415283203
Epoch: 23, Steps: 63 | Train Loss: 0.4689384 Vali Loss: 1.2422440 Test Loss: 0.4554084
Validation loss decreased (1.242653 --> 1.242244).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.897580623626709
Epoch: 24, Steps: 63 | Train Loss: 0.4686545 Vali Loss: 1.2437876 Test Loss: 0.4553806
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.899585723876953
Epoch: 25, Steps: 63 | Train Loss: 0.4682913 Vali Loss: 1.2416080 Test Loss: 0.4553255
Validation loss decreased (1.242244 --> 1.241608).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.3282084465026855
Epoch: 26, Steps: 63 | Train Loss: 0.4683791 Vali Loss: 1.2390759 Test Loss: 0.4553393
Validation loss decreased (1.241608 --> 1.239076).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.822286605834961
Epoch: 27, Steps: 63 | Train Loss: 0.4677908 Vali Loss: 1.2378240 Test Loss: 0.4553327
Validation loss decreased (1.239076 --> 1.237824).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.1393187046051025
Epoch: 28, Steps: 63 | Train Loss: 0.4678715 Vali Loss: 1.2327833 Test Loss: 0.4553682
Validation loss decreased (1.237824 --> 1.232783).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.9768481254577637
Epoch: 29, Steps: 63 | Train Loss: 0.4679707 Vali Loss: 1.2404696 Test Loss: 0.4553478
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.1733217239379883
Epoch: 30, Steps: 63 | Train Loss: 0.4678867 Vali Loss: 1.2450420 Test Loss: 0.4553884
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.0876851081848145
Epoch: 31, Steps: 63 | Train Loss: 0.4677845 Vali Loss: 1.2421007 Test Loss: 0.4554066
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.627389907836914
Epoch: 32, Steps: 63 | Train Loss: 0.4678935 Vali Loss: 1.2365670 Test Loss: 0.4554595
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.427345037460327
Epoch: 33, Steps: 63 | Train Loss: 0.4674213 Vali Loss: 1.2374514 Test Loss: 0.4554909
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.2894670963287354
Epoch: 34, Steps: 63 | Train Loss: 0.4676409 Vali Loss: 1.2353247 Test Loss: 0.4554875
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9265327453613281
Epoch: 35, Steps: 63 | Train Loss: 0.4675878 Vali Loss: 1.2399986 Test Loss: 0.4555341
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.839712142944336
Epoch: 36, Steps: 63 | Train Loss: 0.4671967 Vali Loss: 1.2353338 Test Loss: 0.4555630
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.813399076461792
Epoch: 37, Steps: 63 | Train Loss: 0.4673411 Vali Loss: 1.2370062 Test Loss: 0.4555608
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.761986494064331
Epoch: 38, Steps: 63 | Train Loss: 0.4675406 Vali Loss: 1.2400999 Test Loss: 0.4556109
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7478611469268799
Epoch: 39, Steps: 63 | Train Loss: 0.4670973 Vali Loss: 1.2334273 Test Loss: 0.4556505
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.800013542175293
Epoch: 40, Steps: 63 | Train Loss: 0.4673886 Vali Loss: 1.2354958 Test Loss: 0.4556848
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.061253070831299
Epoch: 41, Steps: 63 | Train Loss: 0.4670848 Vali Loss: 1.2343574 Test Loss: 0.4557000
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.6189260482788086
Epoch: 42, Steps: 63 | Train Loss: 0.4668475 Vali Loss: 1.2352464 Test Loss: 0.4557140
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.3328731060028076
Epoch: 43, Steps: 63 | Train Loss: 0.4669074 Vali Loss: 1.2344131 Test Loss: 0.4557364
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.56526780128479
Epoch: 44, Steps: 63 | Train Loss: 0.4673793 Vali Loss: 1.2350957 Test Loss: 0.4557708
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.8551530838012695
Epoch: 45, Steps: 63 | Train Loss: 0.4668896 Vali Loss: 1.2371639 Test Loss: 0.4557908
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.7854394912719727
Epoch: 46, Steps: 63 | Train Loss: 0.4669679 Vali Loss: 1.2391392 Test Loss: 0.4557857
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.503880500793457
Epoch: 47, Steps: 63 | Train Loss: 0.4667409 Vali Loss: 1.2358700 Test Loss: 0.4558070
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.415935754776001
Epoch: 48, Steps: 63 | Train Loss: 0.4669103 Vali Loss: 1.2328787 Test Loss: 0.4558322
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_336_FITS_ETTh1_ftM_sl180_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4545701742172241, mae:0.435921847820282, rse:0.6418780088424683, corr:[0.25403976 0.25815672 0.25594008 0.25470045 0.25439593 0.25294748
 0.25114018 0.2506667  0.25167763 0.25236383 0.25146073 0.25036868
 0.25035918 0.25016832 0.24963969 0.24958599 0.25023785 0.2506852
 0.25046587 0.25015154 0.25012538 0.25025186 0.25023744 0.24986017
 0.24931149 0.24877132 0.24785005 0.24697618 0.24654956 0.24667938
 0.24671768 0.24614398 0.24550815 0.24565554 0.24618913 0.24650964
 0.24643755 0.24632381 0.24650434 0.24669154 0.24685119 0.24723008
 0.24760349 0.24768738 0.24781054 0.24819757 0.2485007  0.24809596
 0.24683775 0.24578238 0.2445881  0.24323651 0.24175832 0.24042813
 0.23978397 0.23982784 0.23990464 0.23985973 0.23959133 0.24005885
 0.2405887  0.24043256 0.24015798 0.24015346 0.24036734 0.24064729
 0.24084066 0.240731   0.24063426 0.24070041 0.24072765 0.24005558
 0.2386635  0.23747532 0.23660308 0.23629804 0.23631495 0.23603816
 0.23550953 0.23504595 0.23503631 0.23531057 0.2350208  0.23462306
 0.23465036 0.23479812 0.23493394 0.23487073 0.23463437 0.23455484
 0.23453286 0.2344608  0.23453967 0.23499864 0.23546557 0.23549107
 0.23494823 0.23453678 0.23408803 0.23335515 0.23276813 0.23238137
 0.23222058 0.23245504 0.23259185 0.23265277 0.2326101  0.23293844
 0.23324667 0.23299946 0.23268081 0.23257661 0.23256142 0.23259038
 0.23264603 0.23266031 0.23270364 0.23278259 0.23260541 0.23195583
 0.23085424 0.2299158  0.22880422 0.22767812 0.22680551 0.22626601
 0.2259489  0.22603719 0.22625121 0.2265668  0.2267126  0.22720048
 0.2279475  0.22813337 0.22805482 0.22797741 0.22804868 0.22826959
 0.22834176 0.22826414 0.22838296 0.22875398 0.22889523 0.2285068
 0.22762464 0.22669186 0.22559644 0.22432847 0.22360402 0.22327015
 0.22315556 0.22307827 0.22326785 0.22376066 0.22409406 0.22440308
 0.22460195 0.22449411 0.22453696 0.22465955 0.22459167 0.22460489
 0.22467472 0.22470373 0.22489016 0.22531267 0.22547811 0.22507524
 0.22419585 0.22358765 0.22301584 0.22226629 0.22158979 0.22107866
 0.22095364 0.22144975 0.22206725 0.22267121 0.22306487 0.22383666
 0.22475022 0.22490579 0.22465925 0.22453195 0.22469461 0.22500291
 0.22510651 0.22512385 0.225454   0.22601953 0.2263113  0.22596958
 0.2250207  0.22421956 0.2232963  0.22226425 0.22149415 0.22101781
 0.22070473 0.22059894 0.22093531 0.22155885 0.22173476 0.22193623
 0.2222662  0.22230819 0.22216946 0.22182083 0.22147867 0.22144747
 0.22150442 0.22135575 0.22133803 0.22161256 0.221806   0.22152588
 0.22087039 0.22045666 0.21998432 0.21928647 0.21871236 0.21813504
 0.21798846 0.21818362 0.21846433 0.21849863 0.2183904  0.2188627
 0.21958095 0.21948878 0.21919997 0.21919553 0.21921207 0.21911202
 0.2188949  0.21877453 0.21906903 0.21962224 0.21989645 0.21944715
 0.21843378 0.21763545 0.21695828 0.21638149 0.21606876 0.21591695
 0.21586807 0.21600087 0.21650234 0.21715127 0.21748407 0.21800567
 0.21889602 0.21941325 0.219584   0.21939884 0.2191748  0.21925116
 0.21942134 0.21929109 0.21909478 0.21918398 0.21951921 0.21944547
 0.21870606 0.21791534 0.21691766 0.21603383 0.21561876 0.21541767
 0.21521427 0.21499658 0.21493223 0.21539232 0.21590826 0.21620372
 0.21647614 0.21635713 0.2164895  0.21669596 0.21655841 0.21631543
 0.21630734 0.21648742 0.21691665 0.21740407 0.21764292 0.21736234
 0.21667673 0.21644205 0.2162817  0.2158541  0.21534123 0.21524526
 0.21544316 0.21584707 0.2162382  0.21637522 0.21630931 0.21688646
 0.21796049 0.21811694 0.21782659 0.2178158  0.21822079 0.2185753
 0.21855804 0.21829633 0.21832241 0.21874094 0.21915232 0.21885137
 0.2177735  0.21706939 0.21658464 0.21597078 0.2156261  0.21487015
 0.2142157  0.21414411 0.21480942 0.2156439  0.21549879 0.21544589
 0.21691898 0.21820989 0.21855035 0.21822983 0.21805678 0.21872894
 0.21918024 0.21811135 0.21559793 0.21443912 0.2162868  0.21125966]
