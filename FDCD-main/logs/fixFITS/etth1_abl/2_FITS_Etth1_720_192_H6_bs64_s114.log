Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_192', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 7.915536165237427
Epoch: 1, Steps: 60 | Train Loss: 0.6342490 Vali Loss: 1.5169913 Test Loss: 0.7889140
Validation loss decreased (inf --> 1.516991).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 7.568104267120361
Epoch: 2, Steps: 60 | Train Loss: 0.4939151 Vali Loss: 1.3757616 Test Loss: 0.7187268
Validation loss decreased (1.516991 --> 1.375762).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 7.8477537631988525
Epoch: 3, Steps: 60 | Train Loss: 0.4300952 Vali Loss: 1.3238534 Test Loss: 0.6970771
Validation loss decreased (1.375762 --> 1.323853).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 7.705794811248779
Epoch: 4, Steps: 60 | Train Loss: 0.3923500 Vali Loss: 1.2960604 Test Loss: 0.6845494
Validation loss decreased (1.323853 --> 1.296060).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 7.8445703983306885
Epoch: 5, Steps: 60 | Train Loss: 0.3649291 Vali Loss: 1.2788004 Test Loss: 0.6769071
Validation loss decreased (1.296060 --> 1.278800).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 7.897369861602783
Epoch: 6, Steps: 60 | Train Loss: 0.3425029 Vali Loss: 1.2621199 Test Loss: 0.6669786
Validation loss decreased (1.278800 --> 1.262120).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 7.588589906692505
Epoch: 7, Steps: 60 | Train Loss: 0.3236919 Vali Loss: 1.2500410 Test Loss: 0.6591768
Validation loss decreased (1.262120 --> 1.250041).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 7.843427896499634
Epoch: 8, Steps: 60 | Train Loss: 0.3073657 Vali Loss: 1.2356665 Test Loss: 0.6500521
Validation loss decreased (1.250041 --> 1.235667).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 7.691244125366211
Epoch: 9, Steps: 60 | Train Loss: 0.2927960 Vali Loss: 1.2219743 Test Loss: 0.6403986
Validation loss decreased (1.235667 --> 1.221974).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 7.904186010360718
Epoch: 10, Steps: 60 | Train Loss: 0.2801399 Vali Loss: 1.2118477 Test Loss: 0.6336150
Validation loss decreased (1.221974 --> 1.211848).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 7.852321147918701
Epoch: 11, Steps: 60 | Train Loss: 0.2687744 Vali Loss: 1.2004507 Test Loss: 0.6253151
Validation loss decreased (1.211848 --> 1.200451).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 7.15565299987793
Epoch: 12, Steps: 60 | Train Loss: 0.2586351 Vali Loss: 1.1911318 Test Loss: 0.6183091
Validation loss decreased (1.200451 --> 1.191132).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 7.753127574920654
Epoch: 13, Steps: 60 | Train Loss: 0.2495426 Vali Loss: 1.1790359 Test Loss: 0.6090131
Validation loss decreased (1.191132 --> 1.179036).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 7.264537334442139
Epoch: 14, Steps: 60 | Train Loss: 0.2413184 Vali Loss: 1.1695349 Test Loss: 0.6022046
Validation loss decreased (1.179036 --> 1.169535).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 7.627281665802002
Epoch: 15, Steps: 60 | Train Loss: 0.2337908 Vali Loss: 1.1626740 Test Loss: 0.5967424
Validation loss decreased (1.169535 --> 1.162674).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 6.978057146072388
Epoch: 16, Steps: 60 | Train Loss: 0.2270627 Vali Loss: 1.1522688 Test Loss: 0.5886586
Validation loss decreased (1.162674 --> 1.152269).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 7.283692359924316
Epoch: 17, Steps: 60 | Train Loss: 0.2208535 Vali Loss: 1.1454352 Test Loss: 0.5837966
Validation loss decreased (1.152269 --> 1.145435).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 7.138575792312622
Epoch: 18, Steps: 60 | Train Loss: 0.2152613 Vali Loss: 1.1377512 Test Loss: 0.5771306
Validation loss decreased (1.145435 --> 1.137751).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 7.143506765365601
Epoch: 19, Steps: 60 | Train Loss: 0.2101381 Vali Loss: 1.1314603 Test Loss: 0.5724184
Validation loss decreased (1.137751 --> 1.131460).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 7.306638479232788
Epoch: 20, Steps: 60 | Train Loss: 0.2051243 Vali Loss: 1.1250646 Test Loss: 0.5678493
Validation loss decreased (1.131460 --> 1.125065).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 7.5644495487213135
Epoch: 21, Steps: 60 | Train Loss: 0.2009613 Vali Loss: 1.1192757 Test Loss: 0.5633654
Validation loss decreased (1.125065 --> 1.119276).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 7.418821334838867
Epoch: 22, Steps: 60 | Train Loss: 0.1968971 Vali Loss: 1.1142949 Test Loss: 0.5588880
Validation loss decreased (1.119276 --> 1.114295).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 7.055374622344971
Epoch: 23, Steps: 60 | Train Loss: 0.1932484 Vali Loss: 1.1076654 Test Loss: 0.5537985
Validation loss decreased (1.114295 --> 1.107665).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 7.300753355026245
Epoch: 24, Steps: 60 | Train Loss: 0.1897916 Vali Loss: 1.1033150 Test Loss: 0.5500947
Validation loss decreased (1.107665 --> 1.103315).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 7.085435628890991
Epoch: 25, Steps: 60 | Train Loss: 0.1866274 Vali Loss: 1.0984824 Test Loss: 0.5462306
Validation loss decreased (1.103315 --> 1.098482).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 7.121835231781006
Epoch: 26, Steps: 60 | Train Loss: 0.1837053 Vali Loss: 1.0944124 Test Loss: 0.5431884
Validation loss decreased (1.098482 --> 1.094412).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 7.045348167419434
Epoch: 27, Steps: 60 | Train Loss: 0.1809789 Vali Loss: 1.0901277 Test Loss: 0.5395590
Validation loss decreased (1.094412 --> 1.090128).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 6.990875720977783
Epoch: 28, Steps: 60 | Train Loss: 0.1784647 Vali Loss: 1.0863134 Test Loss: 0.5366172
Validation loss decreased (1.090128 --> 1.086313).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 6.947413682937622
Epoch: 29, Steps: 60 | Train Loss: 0.1761301 Vali Loss: 1.0822245 Test Loss: 0.5336269
Validation loss decreased (1.086313 --> 1.082224).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 7.154524803161621
Epoch: 30, Steps: 60 | Train Loss: 0.1739047 Vali Loss: 1.0795738 Test Loss: 0.5310605
Validation loss decreased (1.082224 --> 1.079574).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 7.161617040634155
Epoch: 31, Steps: 60 | Train Loss: 0.1718055 Vali Loss: 1.0756840 Test Loss: 0.5283179
Validation loss decreased (1.079574 --> 1.075684).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 6.993751287460327
Epoch: 32, Steps: 60 | Train Loss: 0.1699640 Vali Loss: 1.0734066 Test Loss: 0.5260071
Validation loss decreased (1.075684 --> 1.073407).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 6.908553600311279
Epoch: 33, Steps: 60 | Train Loss: 0.1681833 Vali Loss: 1.0702162 Test Loss: 0.5234556
Validation loss decreased (1.073407 --> 1.070216).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 7.238603115081787
Epoch: 34, Steps: 60 | Train Loss: 0.1664309 Vali Loss: 1.0677770 Test Loss: 0.5216870
Validation loss decreased (1.070216 --> 1.067777).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 6.931230545043945
Epoch: 35, Steps: 60 | Train Loss: 0.1648483 Vali Loss: 1.0654161 Test Loss: 0.5196642
Validation loss decreased (1.067777 --> 1.065416).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 6.845582485198975
Epoch: 36, Steps: 60 | Train Loss: 0.1633939 Vali Loss: 1.0629530 Test Loss: 0.5175620
Validation loss decreased (1.065416 --> 1.062953).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 7.164817571640015
Epoch: 37, Steps: 60 | Train Loss: 0.1620444 Vali Loss: 1.0599638 Test Loss: 0.5157697
Validation loss decreased (1.062953 --> 1.059964).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 6.860958576202393
Epoch: 38, Steps: 60 | Train Loss: 0.1607277 Vali Loss: 1.0585951 Test Loss: 0.5139752
Validation loss decreased (1.059964 --> 1.058595).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 7.296370983123779
Epoch: 39, Steps: 60 | Train Loss: 0.1594815 Vali Loss: 1.0565006 Test Loss: 0.5125808
Validation loss decreased (1.058595 --> 1.056501).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 6.859459638595581
Epoch: 40, Steps: 60 | Train Loss: 0.1582983 Vali Loss: 1.0547308 Test Loss: 0.5110557
Validation loss decreased (1.056501 --> 1.054731).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 6.903502464294434
Epoch: 41, Steps: 60 | Train Loss: 0.1573703 Vali Loss: 1.0532278 Test Loss: 0.5096344
Validation loss decreased (1.054731 --> 1.053228).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 6.989374399185181
Epoch: 42, Steps: 60 | Train Loss: 0.1562072 Vali Loss: 1.0514234 Test Loss: 0.5082523
Validation loss decreased (1.053228 --> 1.051423).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 6.7329771518707275
Epoch: 43, Steps: 60 | Train Loss: 0.1553247 Vali Loss: 1.0495481 Test Loss: 0.5070819
Validation loss decreased (1.051423 --> 1.049548).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 6.662842273712158
Epoch: 44, Steps: 60 | Train Loss: 0.1544255 Vali Loss: 1.0480094 Test Loss: 0.5056087
Validation loss decreased (1.049548 --> 1.048009).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 6.566314220428467
Epoch: 45, Steps: 60 | Train Loss: 0.1535902 Vali Loss: 1.0467842 Test Loss: 0.5044069
Validation loss decreased (1.048009 --> 1.046784).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 6.934657573699951
Epoch: 46, Steps: 60 | Train Loss: 0.1527030 Vali Loss: 1.0455669 Test Loss: 0.5034238
Validation loss decreased (1.046784 --> 1.045567).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 6.748305797576904
Epoch: 47, Steps: 60 | Train Loss: 0.1519541 Vali Loss: 1.0445036 Test Loss: 0.5024542
Validation loss decreased (1.045567 --> 1.044504).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 7.084082126617432
Epoch: 48, Steps: 60 | Train Loss: 0.1510900 Vali Loss: 1.0430838 Test Loss: 0.5013899
Validation loss decreased (1.044504 --> 1.043084).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 7.002750873565674
Epoch: 49, Steps: 60 | Train Loss: 0.1505654 Vali Loss: 1.0420763 Test Loss: 0.5003421
Validation loss decreased (1.043084 --> 1.042076).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 7.057574033737183
Epoch: 50, Steps: 60 | Train Loss: 0.1498524 Vali Loss: 1.0409654 Test Loss: 0.4996451
Validation loss decreased (1.042076 --> 1.040965).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 6.910104274749756
Epoch: 51, Steps: 60 | Train Loss: 0.1493583 Vali Loss: 1.0400443 Test Loss: 0.4987988
Validation loss decreased (1.040965 --> 1.040044).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 6.899309158325195
Epoch: 52, Steps: 60 | Train Loss: 0.1487479 Vali Loss: 1.0390391 Test Loss: 0.4978695
Validation loss decreased (1.040044 --> 1.039039).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.532265901565552
Epoch: 53, Steps: 60 | Train Loss: 0.1482416 Vali Loss: 1.0381083 Test Loss: 0.4972956
Validation loss decreased (1.039039 --> 1.038108).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 6.818212032318115
Epoch: 54, Steps: 60 | Train Loss: 0.1476338 Vali Loss: 1.0372301 Test Loss: 0.4965102
Validation loss decreased (1.038108 --> 1.037230).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 5.895709037780762
Epoch: 55, Steps: 60 | Train Loss: 0.1471527 Vali Loss: 1.0365927 Test Loss: 0.4958611
Validation loss decreased (1.037230 --> 1.036593).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 6.613147735595703
Epoch: 56, Steps: 60 | Train Loss: 0.1467366 Vali Loss: 1.0356312 Test Loss: 0.4951150
Validation loss decreased (1.036593 --> 1.035631).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 6.606849431991577
Epoch: 57, Steps: 60 | Train Loss: 0.1461731 Vali Loss: 1.0349836 Test Loss: 0.4945399
Validation loss decreased (1.035631 --> 1.034984).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 6.609045743942261
Epoch: 58, Steps: 60 | Train Loss: 0.1458450 Vali Loss: 1.0341618 Test Loss: 0.4939396
Validation loss decreased (1.034984 --> 1.034162).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 6.403856039047241
Epoch: 59, Steps: 60 | Train Loss: 0.1455076 Vali Loss: 1.0335596 Test Loss: 0.4933450
Validation loss decreased (1.034162 --> 1.033560).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 6.543233156204224
Epoch: 60, Steps: 60 | Train Loss: 0.1450911 Vali Loss: 1.0329778 Test Loss: 0.4928935
Validation loss decreased (1.033560 --> 1.032978).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 6.60527229309082
Epoch: 61, Steps: 60 | Train Loss: 0.1447167 Vali Loss: 1.0319400 Test Loss: 0.4923851
Validation loss decreased (1.032978 --> 1.031940).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 6.358195781707764
Epoch: 62, Steps: 60 | Train Loss: 0.1443687 Vali Loss: 1.0315824 Test Loss: 0.4919246
Validation loss decreased (1.031940 --> 1.031582).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 5.999340534210205
Epoch: 63, Steps: 60 | Train Loss: 0.1439900 Vali Loss: 1.0315275 Test Loss: 0.4915138
Validation loss decreased (1.031582 --> 1.031528).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 6.063695669174194
Epoch: 64, Steps: 60 | Train Loss: 0.1437765 Vali Loss: 1.0307161 Test Loss: 0.4911037
Validation loss decreased (1.031528 --> 1.030716).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 6.685526132583618
Epoch: 65, Steps: 60 | Train Loss: 0.1434646 Vali Loss: 1.0299350 Test Loss: 0.4907103
Validation loss decreased (1.030716 --> 1.029935).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 6.540735721588135
Epoch: 66, Steps: 60 | Train Loss: 0.1430737 Vali Loss: 1.0295969 Test Loss: 0.4902848
Validation loss decreased (1.029935 --> 1.029597).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 6.419888973236084
Epoch: 67, Steps: 60 | Train Loss: 0.1429079 Vali Loss: 1.0293949 Test Loss: 0.4899373
Validation loss decreased (1.029597 --> 1.029395).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 6.69425630569458
Epoch: 68, Steps: 60 | Train Loss: 0.1426318 Vali Loss: 1.0286036 Test Loss: 0.4895881
Validation loss decreased (1.029395 --> 1.028604).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 6.581034421920776
Epoch: 69, Steps: 60 | Train Loss: 0.1423618 Vali Loss: 1.0281831 Test Loss: 0.4892300
Validation loss decreased (1.028604 --> 1.028183).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 6.560593843460083
Epoch: 70, Steps: 60 | Train Loss: 0.1422520 Vali Loss: 1.0283862 Test Loss: 0.4888913
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 6.650149583816528
Epoch: 71, Steps: 60 | Train Loss: 0.1419838 Vali Loss: 1.0280387 Test Loss: 0.4886207
Validation loss decreased (1.028183 --> 1.028039).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 6.551449775695801
Epoch: 72, Steps: 60 | Train Loss: 0.1418705 Vali Loss: 1.0274096 Test Loss: 0.4883247
Validation loss decreased (1.028039 --> 1.027410).  Saving model ...
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 6.786948204040527
Epoch: 73, Steps: 60 | Train Loss: 0.1415716 Vali Loss: 1.0268543 Test Loss: 0.4880546
Validation loss decreased (1.027410 --> 1.026854).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 6.9041290283203125
Epoch: 74, Steps: 60 | Train Loss: 0.1413892 Vali Loss: 1.0268803 Test Loss: 0.4877818
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 6.7228522300720215
Epoch: 75, Steps: 60 | Train Loss: 0.1412798 Vali Loss: 1.0268219 Test Loss: 0.4875333
Validation loss decreased (1.026854 --> 1.026822).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 6.9608213901519775
Epoch: 76, Steps: 60 | Train Loss: 0.1410422 Vali Loss: 1.0259256 Test Loss: 0.4872831
Validation loss decreased (1.026822 --> 1.025926).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 6.816430568695068
Epoch: 77, Steps: 60 | Train Loss: 0.1409091 Vali Loss: 1.0260690 Test Loss: 0.4871213
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 6.744050741195679
Epoch: 78, Steps: 60 | Train Loss: 0.1407508 Vali Loss: 1.0258658 Test Loss: 0.4869408
Validation loss decreased (1.025926 --> 1.025866).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 6.855844020843506
Epoch: 79, Steps: 60 | Train Loss: 0.1405621 Vali Loss: 1.0255653 Test Loss: 0.4866563
Validation loss decreased (1.025866 --> 1.025565).  Saving model ...
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 6.864048957824707
Epoch: 80, Steps: 60 | Train Loss: 0.1404412 Vali Loss: 1.0256312 Test Loss: 0.4865472
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 6.877676486968994
Epoch: 81, Steps: 60 | Train Loss: 0.1403101 Vali Loss: 1.0250723 Test Loss: 0.4863118
Validation loss decreased (1.025565 --> 1.025072).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 6.694159269332886
Epoch: 82, Steps: 60 | Train Loss: 0.1401629 Vali Loss: 1.0246394 Test Loss: 0.4861973
Validation loss decreased (1.025072 --> 1.024639).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 6.387935400009155
Epoch: 83, Steps: 60 | Train Loss: 0.1400605 Vali Loss: 1.0244437 Test Loss: 0.4859795
Validation loss decreased (1.024639 --> 1.024444).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 6.720072984695435
Epoch: 84, Steps: 60 | Train Loss: 0.1400136 Vali Loss: 1.0248150 Test Loss: 0.4858397
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 6.9174559116363525
Epoch: 85, Steps: 60 | Train Loss: 0.1398287 Vali Loss: 1.0245767 Test Loss: 0.4856932
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 6.40854024887085
Epoch: 86, Steps: 60 | Train Loss: 0.1397026 Vali Loss: 1.0240766 Test Loss: 0.4855095
Validation loss decreased (1.024444 --> 1.024077).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 6.289868354797363
Epoch: 87, Steps: 60 | Train Loss: 0.1396105 Vali Loss: 1.0241089 Test Loss: 0.4854289
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 6.427603006362915
Epoch: 88, Steps: 60 | Train Loss: 0.1395215 Vali Loss: 1.0239207 Test Loss: 0.4852934
Validation loss decreased (1.024077 --> 1.023921).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 6.604497671127319
Epoch: 89, Steps: 60 | Train Loss: 0.1395006 Vali Loss: 1.0239892 Test Loss: 0.4851846
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 6.5316009521484375
Epoch: 90, Steps: 60 | Train Loss: 0.1394297 Vali Loss: 1.0237337 Test Loss: 0.4850693
Validation loss decreased (1.023921 --> 1.023734).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 6.6176910400390625
Epoch: 91, Steps: 60 | Train Loss: 0.1392707 Vali Loss: 1.0235846 Test Loss: 0.4849488
Validation loss decreased (1.023734 --> 1.023585).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 6.412129163742065
Epoch: 92, Steps: 60 | Train Loss: 0.1392669 Vali Loss: 1.0228504 Test Loss: 0.4848339
Validation loss decreased (1.023585 --> 1.022850).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 6.678478240966797
Epoch: 93, Steps: 60 | Train Loss: 0.1391146 Vali Loss: 1.0227954 Test Loss: 0.4847372
Validation loss decreased (1.022850 --> 1.022795).  Saving model ...
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 6.650055408477783
Epoch: 94, Steps: 60 | Train Loss: 0.1391312 Vali Loss: 1.0229967 Test Loss: 0.4846446
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 6.856682538986206
Epoch: 95, Steps: 60 | Train Loss: 0.1390827 Vali Loss: 1.0230782 Test Loss: 0.4845580
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 6.8246283531188965
Epoch: 96, Steps: 60 | Train Loss: 0.1389861 Vali Loss: 1.0224922 Test Loss: 0.4844628
Validation loss decreased (1.022795 --> 1.022492).  Saving model ...
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 6.951851844787598
Epoch: 97, Steps: 60 | Train Loss: 0.1388527 Vali Loss: 1.0229772 Test Loss: 0.4843869
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 6.681992769241333
Epoch: 98, Steps: 60 | Train Loss: 0.1388469 Vali Loss: 1.0228969 Test Loss: 0.4843175
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 7.051141023635864
Epoch: 99, Steps: 60 | Train Loss: 0.1388031 Vali Loss: 1.0226980 Test Loss: 0.4842547
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 6.64741063117981
Epoch: 100, Steps: 60 | Train Loss: 0.1388216 Vali Loss: 1.0221219 Test Loss: 0.4841633
Validation loss decreased (1.022492 --> 1.022122).  Saving model ...
Updating learning rate to 3.1160680107021042e-06
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 6.673602104187012
Epoch: 1, Steps: 60 | Train Loss: 0.4047595 Vali Loss: 0.9557734 Test Loss: 0.4265982
Validation loss decreased (inf --> 0.955773).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 6.732402086257935
Epoch: 2, Steps: 60 | Train Loss: 0.3857755 Vali Loss: 0.9508854 Test Loss: 0.4183190
Validation loss decreased (0.955773 --> 0.950885).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 6.711565017700195
Epoch: 3, Steps: 60 | Train Loss: 0.3820347 Vali Loss: 0.9524803 Test Loss: 0.4183106
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 6.870075225830078
Epoch: 4, Steps: 60 | Train Loss: 0.3808215 Vali Loss: 0.9538734 Test Loss: 0.4183450
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 6.793372869491577
Epoch: 5, Steps: 60 | Train Loss: 0.3802768 Vali Loss: 0.9568662 Test Loss: 0.4183838
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 6.8099260330200195
Epoch: 6, Steps: 60 | Train Loss: 0.3796585 Vali Loss: 0.9574786 Test Loss: 0.4189220
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 6.9477386474609375
Epoch: 7, Steps: 60 | Train Loss: 0.3788665 Vali Loss: 0.9583579 Test Loss: 0.4188362
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 6.702592849731445
Epoch: 8, Steps: 60 | Train Loss: 0.3790372 Vali Loss: 0.9574707 Test Loss: 0.4192019
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 6.859240293502808
Epoch: 9, Steps: 60 | Train Loss: 0.3787612 Vali Loss: 0.9590566 Test Loss: 0.4193488
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 6.871279001235962
Epoch: 10, Steps: 60 | Train Loss: 0.3784481 Vali Loss: 0.9599420 Test Loss: 0.4188109
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.56321120262146
Epoch: 11, Steps: 60 | Train Loss: 0.3782635 Vali Loss: 0.9585394 Test Loss: 0.4187863
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.752989292144775
Epoch: 12, Steps: 60 | Train Loss: 0.3781441 Vali Loss: 0.9583899 Test Loss: 0.4188163
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 6.80247950553894
Epoch: 13, Steps: 60 | Train Loss: 0.3780676 Vali Loss: 0.9585893 Test Loss: 0.4188489
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 7.021244049072266
Epoch: 14, Steps: 60 | Train Loss: 0.3776471 Vali Loss: 0.9577058 Test Loss: 0.4190125
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 7.101099729537964
Epoch: 15, Steps: 60 | Train Loss: 0.3776803 Vali Loss: 0.9587588 Test Loss: 0.4188588
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 6.893747568130493
Epoch: 16, Steps: 60 | Train Loss: 0.3775213 Vali Loss: 0.9579887 Test Loss: 0.4192072
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 6.8814568519592285
Epoch: 17, Steps: 60 | Train Loss: 0.3774279 Vali Loss: 0.9595838 Test Loss: 0.4190361
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 6.624118328094482
Epoch: 18, Steps: 60 | Train Loss: 0.3773480 Vali Loss: 0.9584646 Test Loss: 0.4187189
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.118472576141357
Epoch: 19, Steps: 60 | Train Loss: 0.3771997 Vali Loss: 0.9585508 Test Loss: 0.4189014
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 6.952160120010376
Epoch: 20, Steps: 60 | Train Loss: 0.3771558 Vali Loss: 0.9589292 Test Loss: 0.4186988
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 6.792006254196167
Epoch: 21, Steps: 60 | Train Loss: 0.3767073 Vali Loss: 0.9582374 Test Loss: 0.4191379
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 6.925624847412109
Epoch: 22, Steps: 60 | Train Loss: 0.3769661 Vali Loss: 0.9591340 Test Loss: 0.4190111
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_192_FITS_ETTh1_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4155079424381256, mae:0.4242774546146393, rse:0.6121347546577454, corr:[0.2672235  0.27318814 0.27085125 0.27096385 0.2698134  0.26666933
 0.2644002  0.26460925 0.2649479  0.26461136 0.2643164  0.26456922
 0.26473626 0.2640602  0.2632679  0.26309383 0.2631486  0.26284924
 0.26199555 0.2610223  0.26044434 0.26019448 0.25985715 0.2597456
 0.26008472 0.26058337 0.26071167 0.26044247 0.26023608 0.26024184
 0.26035655 0.2603539  0.2604653  0.26055348 0.26037264 0.26034266
 0.26072714 0.26107302 0.26099148 0.26066244 0.2606025  0.26060182
 0.26047024 0.2602686  0.26011172 0.26000014 0.26018858 0.26054138
 0.26047307 0.25990313 0.25902864 0.258104   0.25692102 0.25537363
 0.25453314 0.25445387 0.25421005 0.25365803 0.2532655  0.2535199
 0.25362656 0.25348637 0.25354916 0.2537358  0.25341347 0.25299555
 0.25331858 0.25371855 0.25366816 0.25349534 0.25363022 0.2536018
 0.2531072  0.25248697 0.25203246 0.2517396  0.2511214  0.25036508
 0.24996051 0.24994476 0.25004348 0.2498175  0.24899001 0.24814807
 0.24794443 0.24806018 0.24781054 0.24707311 0.24636245 0.24607757
 0.24601175 0.24593036 0.24575213 0.24556619 0.24575268 0.24666934
 0.2477889  0.24823315 0.24797215 0.24787113 0.24782054 0.24727325
 0.24683183 0.24702117 0.24717301 0.24686079 0.24635397 0.24605168
 0.24583705 0.24552503 0.24579965 0.24657607 0.24700949 0.2470758
 0.24705392 0.24664561 0.24590051 0.24558221 0.24594355 0.24629334
 0.2461729  0.24542104 0.24456964 0.24400438 0.24313955 0.24175312
 0.24069229 0.24035318 0.24048585 0.24096532 0.24107721 0.24070893
 0.24008344 0.23960662 0.23938946 0.23945667 0.23971048 0.23940837
 0.23877889 0.23845915 0.23877569 0.23945531 0.23995802 0.24006362
 0.23959625 0.23792085 0.23583049 0.23467617 0.23439182 0.2338518
 0.2335458  0.23398708 0.23446192 0.23417905 0.23300637 0.23207101
 0.23212026 0.23296292 0.23359662 0.23298526 0.2319584  0.23204921
 0.23285143 0.23272312 0.23163676 0.23086177 0.23070154 0.22966641
 0.22829632 0.22832139 0.22884515 0.22840418 0.22755276 0.22762963
 0.22781225 0.22751708 0.2267352  0.22605212 0.2250603  0.22450651
 0.22491546 0.2249594  0.22436868 0.22460912 0.22480458 0.22302309
 0.22146793 0.22289    0.22300114 0.21893051 0.21920171 0.22265829]
