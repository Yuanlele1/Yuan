Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=165, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=165, out_features=241, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  35629440.0
params:  40006.0
Trainable parameters:  40006
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.676792860031128
Epoch: 1, Steps: 59 | Train Loss: 0.7521963 Vali Loss: 1.4572152 Test Loss: 0.6289529
Validation loss decreased (inf --> 1.457215).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.521629810333252
Epoch: 2, Steps: 59 | Train Loss: 0.5801878 Vali Loss: 1.3179400 Test Loss: 0.5373616
Validation loss decreased (1.457215 --> 1.317940).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.5345637798309326
Epoch: 3, Steps: 59 | Train Loss: 0.5273133 Vali Loss: 1.2610601 Test Loss: 0.4939988
Validation loss decreased (1.317940 --> 1.261060).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.6870148181915283
Epoch: 4, Steps: 59 | Train Loss: 0.4986917 Vali Loss: 1.2258958 Test Loss: 0.4689401
Validation loss decreased (1.261060 --> 1.225896).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.7682416439056396
Epoch: 5, Steps: 59 | Train Loss: 0.4814391 Vali Loss: 1.2061039 Test Loss: 0.4540612
Validation loss decreased (1.225896 --> 1.206104).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.791353702545166
Epoch: 6, Steps: 59 | Train Loss: 0.4700821 Vali Loss: 1.1915095 Test Loss: 0.4454938
Validation loss decreased (1.206104 --> 1.191509).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.7279043197631836
Epoch: 7, Steps: 59 | Train Loss: 0.4624320 Vali Loss: 1.1841871 Test Loss: 0.4406020
Validation loss decreased (1.191509 --> 1.184187).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.4877433776855469
Epoch: 8, Steps: 59 | Train Loss: 0.4573565 Vali Loss: 1.1827427 Test Loss: 0.4381053
Validation loss decreased (1.184187 --> 1.182743).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.5547373294830322
Epoch: 9, Steps: 59 | Train Loss: 0.4536750 Vali Loss: 1.1782051 Test Loss: 0.4366494
Validation loss decreased (1.182743 --> 1.178205).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.6213645935058594
Epoch: 10, Steps: 59 | Train Loss: 0.4508471 Vali Loss: 1.1812454 Test Loss: 0.4361360
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.5656421184539795
Epoch: 11, Steps: 59 | Train Loss: 0.4489969 Vali Loss: 1.1774951 Test Loss: 0.4361019
Validation loss decreased (1.178205 --> 1.177495).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.6550514698028564
Epoch: 12, Steps: 59 | Train Loss: 0.4472882 Vali Loss: 1.1791084 Test Loss: 0.4363629
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6114401817321777
Epoch: 13, Steps: 59 | Train Loss: 0.4459393 Vali Loss: 1.1800628 Test Loss: 0.4365221
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6828324794769287
Epoch: 14, Steps: 59 | Train Loss: 0.4445703 Vali Loss: 1.1829097 Test Loss: 0.4367727
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.582270622253418
Epoch: 15, Steps: 59 | Train Loss: 0.4441048 Vali Loss: 1.1868080 Test Loss: 0.4372604
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.5806708335876465
Epoch: 16, Steps: 59 | Train Loss: 0.4430228 Vali Loss: 1.1878079 Test Loss: 0.4374110
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.6160461902618408
Epoch: 17, Steps: 59 | Train Loss: 0.4425217 Vali Loss: 1.1871855 Test Loss: 0.4377480
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.7260446548461914
Epoch: 18, Steps: 59 | Train Loss: 0.4417717 Vali Loss: 1.1850048 Test Loss: 0.4379077
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6462595462799072
Epoch: 19, Steps: 59 | Train Loss: 0.4410531 Vali Loss: 1.1889561 Test Loss: 0.4382196
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.5733675956726074
Epoch: 20, Steps: 59 | Train Loss: 0.4404352 Vali Loss: 1.1919171 Test Loss: 0.4384238
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.5615475177764893
Epoch: 21, Steps: 59 | Train Loss: 0.4400826 Vali Loss: 1.1907740 Test Loss: 0.4385471
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.6631770133972168
Epoch: 22, Steps: 59 | Train Loss: 0.4397734 Vali Loss: 1.1934116 Test Loss: 0.4388518
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5438389778137207
Epoch: 23, Steps: 59 | Train Loss: 0.4393229 Vali Loss: 1.1986666 Test Loss: 0.4391005
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6071820259094238
Epoch: 24, Steps: 59 | Train Loss: 0.4393111 Vali Loss: 1.1939945 Test Loss: 0.4392695
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.7145271301269531
Epoch: 25, Steps: 59 | Train Loss: 0.4389660 Vali Loss: 1.2002728 Test Loss: 0.4393348
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7128255367279053
Epoch: 26, Steps: 59 | Train Loss: 0.4387156 Vali Loss: 1.1957642 Test Loss: 0.4394453
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.6694750785827637
Epoch: 27, Steps: 59 | Train Loss: 0.4383242 Vali Loss: 1.1978511 Test Loss: 0.4395650
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.5443387031555176
Epoch: 28, Steps: 59 | Train Loss: 0.4381284 Vali Loss: 1.1962129 Test Loss: 0.4396511
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.561739206314087
Epoch: 29, Steps: 59 | Train Loss: 0.4379987 Vali Loss: 1.2015415 Test Loss: 0.4397564
EarlyStopping counter: 18 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.609896183013916
Epoch: 30, Steps: 59 | Train Loss: 0.4375609 Vali Loss: 1.1973228 Test Loss: 0.4398671
EarlyStopping counter: 19 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.6225261688232422
Epoch: 31, Steps: 59 | Train Loss: 0.4374402 Vali Loss: 1.2047733 Test Loss: 0.4399955
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_720_336_FITS_ETTh1_ftM_sl720_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43496736884117126, mae:0.43890878558158875, rse:0.6278854012489319, corr:[0.24941103 0.26241335 0.25762728 0.25467175 0.25616634 0.2555178
 0.25344232 0.25285536 0.2535365  0.2536606  0.25288242 0.25243947
 0.25244787 0.25201723 0.25133273 0.2510289  0.2512481  0.25074986
 0.24950452 0.24863148 0.24900195 0.24926816 0.24870832 0.24782659
 0.24791293 0.24848649 0.2485465  0.2480627  0.24769199 0.24784511
 0.24809463 0.24820529 0.24838029 0.24856357 0.24848123 0.24821384
 0.24814226 0.24830686 0.24833286 0.24806447 0.24782358 0.24805859
 0.24853556 0.24874288 0.2486512  0.24890621 0.24960554 0.24993835
 0.24963634 0.2490471  0.24856336 0.2480077  0.24697985 0.245661
 0.24476522 0.24450196 0.24451078 0.24439417 0.24412443 0.2438623
 0.24360338 0.24347965 0.24353437 0.24386114 0.24414554 0.24427328
 0.24432431 0.24440458 0.2447754  0.24503067 0.24477325 0.24415329
 0.2436622  0.24331068 0.24304509 0.24288256 0.24263233 0.24225171
 0.24190776 0.24173532 0.24168484 0.24161191 0.24142568 0.24121779
 0.24091455 0.24055521 0.24012478 0.23982906 0.23968813 0.23970136
 0.23956193 0.23923624 0.23888662 0.23888467 0.23924197 0.23985536
 0.24049959 0.24098371 0.24134883 0.24155252 0.24153672 0.24133043
 0.24122731 0.2413365  0.24157315 0.2416782  0.24148364 0.24123296
 0.24112685 0.24121395 0.24136312 0.24147879 0.24139781 0.24124898
 0.24116318 0.2411495  0.24122226 0.24120538 0.24101415 0.2409087
 0.24094066 0.24069631 0.24013203 0.23953365 0.23909214 0.23876493
 0.23868826 0.23876926 0.23860197 0.23814575 0.2375303  0.23699395
 0.23671381 0.2366235  0.23656797 0.23647027 0.23634729 0.23642632
 0.23654445 0.23644744 0.23613566 0.2358452  0.2357282  0.23573141
 0.23550588 0.2348934  0.23417    0.23362324 0.23340993 0.23315243
 0.23280352 0.23265138 0.2327685  0.2329056  0.23283309 0.23275699
 0.23270757 0.2326016  0.23239581 0.23241286 0.23261812 0.23291439
 0.23295031 0.23277156 0.23267537 0.23265934 0.23226272 0.23173311
 0.23152067 0.23176378 0.23205979 0.23203747 0.23163913 0.23127246
 0.23126133 0.2314185  0.2312738  0.23100238 0.23092462 0.23109649
 0.23111038 0.23091108 0.23075555 0.23087662 0.2311556  0.23141435
 0.23153913 0.23163117 0.23176622 0.23193298 0.23210144 0.23209797
 0.23179029 0.23119017 0.23040342 0.22963865 0.2290384  0.22841103
 0.22773151 0.22734079 0.22738636 0.22749467 0.22721498 0.22684127
 0.22679745 0.22728716 0.22785088 0.2282157  0.22827892 0.22803883
 0.22766607 0.22739612 0.22738592 0.227339   0.2269591  0.22662035
 0.22670992 0.22695689 0.22681442 0.22621505 0.22558421 0.22520442
 0.2252721  0.2254309  0.22526857 0.22493407 0.22475618 0.22482255
 0.22455189 0.22391888 0.22351901 0.22370331 0.22399698 0.22419935
 0.22419173 0.22395654 0.22364564 0.22349589 0.22376953 0.22398394
 0.22379835 0.22315165 0.22259682 0.22235113 0.22216785 0.22182535
 0.2216216  0.22200525 0.22263719 0.2227102  0.22205104 0.22139907
 0.22129767 0.2214417  0.22148633 0.2214058  0.2214383  0.22143881
 0.22120342 0.22085544 0.22105058 0.22157666 0.22184217 0.22152165
 0.22130157 0.22148837 0.22140346 0.22057126 0.21927698 0.21845172
 0.21850602 0.21843985 0.21767211 0.21698461 0.21686503 0.2171061
 0.2171001  0.21699212 0.21695364 0.21686228 0.2163745  0.21590887
 0.2159172  0.21621408 0.21628286 0.21577899 0.21539022 0.21538395
 0.21552667 0.2157405  0.21594    0.21639925 0.21651183 0.21595336
 0.21520787 0.21519025 0.2157852  0.21583845 0.2148118  0.21391726
 0.21426    0.21492265 0.21479058 0.21410112 0.21398744 0.2142294
 0.21406469 0.21312958 0.21307614 0.21382204 0.2138984  0.21273588
 0.21186899 0.2121512  0.21215849 0.21045297 0.20806032 0.2066624
 0.20651215 0.20546396 0.20345096 0.20261079 0.20326929 0.20301956
 0.20116277 0.19976118 0.20012075 0.20037076 0.19799834 0.19609568
 0.1968334  0.1956876  0.18846698 0.18313001 0.18935892 0.18658891]
