Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=90, out_features=138, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11128320.0
params:  12558.0
Trainable parameters:  12558
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.136404037475586
Epoch: 1, Steps: 63 | Train Loss: 0.6925219 Vali Loss: 1.3708187 Test Loss: 0.6582708
Validation loss decreased (inf --> 1.370819).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.651540756225586
Epoch: 2, Steps: 63 | Train Loss: 0.5335141 Vali Loss: 1.1906950 Test Loss: 0.5422249
Validation loss decreased (1.370819 --> 1.190695).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.603631019592285
Epoch: 3, Steps: 63 | Train Loss: 0.4777467 Vali Loss: 1.1017259 Test Loss: 0.4871533
Validation loss decreased (1.190695 --> 1.101726).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.5124220848083496
Epoch: 4, Steps: 63 | Train Loss: 0.4480928 Vali Loss: 1.0471755 Test Loss: 0.4552298
Validation loss decreased (1.101726 --> 1.047176).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.553170919418335
Epoch: 5, Steps: 63 | Train Loss: 0.4302724 Vali Loss: 1.0120821 Test Loss: 0.4364786
Validation loss decreased (1.047176 --> 1.012082).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.616114377975464
Epoch: 6, Steps: 63 | Train Loss: 0.4194038 Vali Loss: 0.9895171 Test Loss: 0.4253795
Validation loss decreased (1.012082 --> 0.989517).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.8746683597564697
Epoch: 7, Steps: 63 | Train Loss: 0.4125678 Vali Loss: 0.9749286 Test Loss: 0.4192810
Validation loss decreased (0.989517 --> 0.974929).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.662095308303833
Epoch: 8, Steps: 63 | Train Loss: 0.4083154 Vali Loss: 0.9641519 Test Loss: 0.4156555
Validation loss decreased (0.974929 --> 0.964152).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.20971417427063
Epoch: 9, Steps: 63 | Train Loss: 0.4053109 Vali Loss: 0.9565341 Test Loss: 0.4136496
Validation loss decreased (0.964152 --> 0.956534).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.313105344772339
Epoch: 10, Steps: 63 | Train Loss: 0.4031896 Vali Loss: 0.9510628 Test Loss: 0.4124426
Validation loss decreased (0.956534 --> 0.951063).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.1495659351348877
Epoch: 11, Steps: 63 | Train Loss: 0.4020088 Vali Loss: 0.9479410 Test Loss: 0.4121447
Validation loss decreased (0.951063 --> 0.947941).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.3076934814453125
Epoch: 12, Steps: 63 | Train Loss: 0.4011339 Vali Loss: 0.9451905 Test Loss: 0.4118766
Validation loss decreased (0.947941 --> 0.945191).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.112227439880371
Epoch: 13, Steps: 63 | Train Loss: 0.4007703 Vali Loss: 0.9425222 Test Loss: 0.4116065
Validation loss decreased (0.945191 --> 0.942522).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.8483805656433105
Epoch: 14, Steps: 63 | Train Loss: 0.3998390 Vali Loss: 0.9407928 Test Loss: 0.4114385
Validation loss decreased (0.942522 --> 0.940793).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.594033718109131
Epoch: 15, Steps: 63 | Train Loss: 0.3995113 Vali Loss: 0.9391381 Test Loss: 0.4114735
Validation loss decreased (0.940793 --> 0.939138).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.190047264099121
Epoch: 16, Steps: 63 | Train Loss: 0.3993077 Vali Loss: 0.9377769 Test Loss: 0.4112969
Validation loss decreased (0.939138 --> 0.937777).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.395434617996216
Epoch: 17, Steps: 63 | Train Loss: 0.3991429 Vali Loss: 0.9359556 Test Loss: 0.4113388
Validation loss decreased (0.937777 --> 0.935956).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.907606601715088
Epoch: 18, Steps: 63 | Train Loss: 0.3987270 Vali Loss: 0.9355268 Test Loss: 0.4112995
Validation loss decreased (0.935956 --> 0.935527).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0174927711486816
Epoch: 19, Steps: 63 | Train Loss: 0.3986513 Vali Loss: 0.9339392 Test Loss: 0.4112545
Validation loss decreased (0.935527 --> 0.933939).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.6251983642578125
Epoch: 20, Steps: 63 | Train Loss: 0.3983071 Vali Loss: 0.9326507 Test Loss: 0.4111506
Validation loss decreased (0.933939 --> 0.932651).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.206756591796875
Epoch: 21, Steps: 63 | Train Loss: 0.3980432 Vali Loss: 0.9324332 Test Loss: 0.4111807
Validation loss decreased (0.932651 --> 0.932433).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.1414988040924072
Epoch: 22, Steps: 63 | Train Loss: 0.3980979 Vali Loss: 0.9319730 Test Loss: 0.4112373
Validation loss decreased (0.932433 --> 0.931973).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.17175030708313
Epoch: 23, Steps: 63 | Train Loss: 0.3978428 Vali Loss: 0.9310579 Test Loss: 0.4110962
Validation loss decreased (0.931973 --> 0.931058).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.455155372619629
Epoch: 24, Steps: 63 | Train Loss: 0.3974959 Vali Loss: 0.9308352 Test Loss: 0.4112119
Validation loss decreased (0.931058 --> 0.930835).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.8295586109161377
Epoch: 25, Steps: 63 | Train Loss: 0.3977973 Vali Loss: 0.9292119 Test Loss: 0.4111019
Validation loss decreased (0.930835 --> 0.929212).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7776212692260742
Epoch: 26, Steps: 63 | Train Loss: 0.3976945 Vali Loss: 0.9297641 Test Loss: 0.4111342
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.817138671875
Epoch: 27, Steps: 63 | Train Loss: 0.3975626 Vali Loss: 0.9291741 Test Loss: 0.4110470
Validation loss decreased (0.929212 --> 0.929174).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.3313162326812744
Epoch: 28, Steps: 63 | Train Loss: 0.3970525 Vali Loss: 0.9289191 Test Loss: 0.4110467
Validation loss decreased (0.929174 --> 0.928919).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.3671035766601562
Epoch: 29, Steps: 63 | Train Loss: 0.3970192 Vali Loss: 0.9283492 Test Loss: 0.4110114
Validation loss decreased (0.928919 --> 0.928349).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.149083375930786
Epoch: 30, Steps: 63 | Train Loss: 0.3972910 Vali Loss: 0.9280389 Test Loss: 0.4109948
Validation loss decreased (0.928349 --> 0.928039).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.6229865550994873
Epoch: 31, Steps: 63 | Train Loss: 0.3971880 Vali Loss: 0.9276896 Test Loss: 0.4111211
Validation loss decreased (0.928039 --> 0.927690).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.9882285594940186
Epoch: 32, Steps: 63 | Train Loss: 0.3969951 Vali Loss: 0.9275965 Test Loss: 0.4109510
Validation loss decreased (0.927690 --> 0.927597).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.361563205718994
Epoch: 33, Steps: 63 | Train Loss: 0.3969100 Vali Loss: 0.9276399 Test Loss: 0.4109763
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.0746750831604004
Epoch: 34, Steps: 63 | Train Loss: 0.3970836 Vali Loss: 0.9268462 Test Loss: 0.4109359
Validation loss decreased (0.927597 --> 0.926846).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.323045253753662
Epoch: 35, Steps: 63 | Train Loss: 0.3968061 Vali Loss: 0.9271884 Test Loss: 0.4110394
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.5208945274353027
Epoch: 36, Steps: 63 | Train Loss: 0.3969221 Vali Loss: 0.9267493 Test Loss: 0.4109600
Validation loss decreased (0.926846 --> 0.926749).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9072589874267578
Epoch: 37, Steps: 63 | Train Loss: 0.3967816 Vali Loss: 0.9266019 Test Loss: 0.4108754
Validation loss decreased (0.926749 --> 0.926602).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.9709115028381348
Epoch: 38, Steps: 63 | Train Loss: 0.3966528 Vali Loss: 0.9265359 Test Loss: 0.4109157
Validation loss decreased (0.926602 --> 0.926536).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.7811713218688965
Epoch: 39, Steps: 63 | Train Loss: 0.3969192 Vali Loss: 0.9258792 Test Loss: 0.4109171
Validation loss decreased (0.926536 --> 0.925879).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7908170223236084
Epoch: 40, Steps: 63 | Train Loss: 0.3966690 Vali Loss: 0.9261485 Test Loss: 0.4109387
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.0783705711364746
Epoch: 41, Steps: 63 | Train Loss: 0.3962780 Vali Loss: 0.9259713 Test Loss: 0.4109206
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.2918124198913574
Epoch: 42, Steps: 63 | Train Loss: 0.3966082 Vali Loss: 0.9259731 Test Loss: 0.4109300
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.924363613128662
Epoch: 43, Steps: 63 | Train Loss: 0.3966827 Vali Loss: 0.9254159 Test Loss: 0.4109342
Validation loss decreased (0.925879 --> 0.925416).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.060267925262451
Epoch: 44, Steps: 63 | Train Loss: 0.3967135 Vali Loss: 0.9250901 Test Loss: 0.4109402
Validation loss decreased (0.925416 --> 0.925090).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.2946321964263916
Epoch: 45, Steps: 63 | Train Loss: 0.3964853 Vali Loss: 0.9255725 Test Loss: 0.4109151
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.123389720916748
Epoch: 46, Steps: 63 | Train Loss: 0.3962466 Vali Loss: 0.9251724 Test Loss: 0.4109423
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.9041223526000977
Epoch: 47, Steps: 63 | Train Loss: 0.3965791 Vali Loss: 0.9252055 Test Loss: 0.4109100
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.87933611869812
Epoch: 48, Steps: 63 | Train Loss: 0.3963453 Vali Loss: 0.9252471 Test Loss: 0.4109086
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.24605131149292
Epoch: 49, Steps: 63 | Train Loss: 0.3965605 Vali Loss: 0.9247386 Test Loss: 0.4109257
Validation loss decreased (0.925090 --> 0.924739).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.5004405975341797
Epoch: 50, Steps: 63 | Train Loss: 0.3960858 Vali Loss: 0.9246110 Test Loss: 0.4109119
Validation loss decreased (0.924739 --> 0.924611).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.8920354843139648
Epoch: 51, Steps: 63 | Train Loss: 0.3964110 Vali Loss: 0.9250851 Test Loss: 0.4109249
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.623300313949585
Epoch: 52, Steps: 63 | Train Loss: 0.3963009 Vali Loss: 0.9245515 Test Loss: 0.4109135
Validation loss decreased (0.924611 --> 0.924551).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.7834181785583496
Epoch: 53, Steps: 63 | Train Loss: 0.3963344 Vali Loss: 0.9248961 Test Loss: 0.4109027
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.6720361709594727
Epoch: 54, Steps: 63 | Train Loss: 0.3959701 Vali Loss: 0.9248855 Test Loss: 0.4109080
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.179859161376953
Epoch: 55, Steps: 63 | Train Loss: 0.3963326 Vali Loss: 0.9244275 Test Loss: 0.4109124
Validation loss decreased (0.924551 --> 0.924428).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.8430774211883545
Epoch: 56, Steps: 63 | Train Loss: 0.3963694 Vali Loss: 0.9247293 Test Loss: 0.4109212
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.43698787689209
Epoch: 57, Steps: 63 | Train Loss: 0.3961884 Vali Loss: 0.9245625 Test Loss: 0.4109211
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.037468671798706
Epoch: 58, Steps: 63 | Train Loss: 0.3964926 Vali Loss: 0.9245574 Test Loss: 0.4108972
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.642864465713501
Epoch: 59, Steps: 63 | Train Loss: 0.3961647 Vali Loss: 0.9243428 Test Loss: 0.4109146
Validation loss decreased (0.924428 --> 0.924343).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.6878409385681152
Epoch: 60, Steps: 63 | Train Loss: 0.3963627 Vali Loss: 0.9242902 Test Loss: 0.4108906
Validation loss decreased (0.924343 --> 0.924290).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.9794692993164062
Epoch: 61, Steps: 63 | Train Loss: 0.3960632 Vali Loss: 0.9245385 Test Loss: 0.4109002
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.1622209548950195
Epoch: 62, Steps: 63 | Train Loss: 0.3964627 Vali Loss: 0.9244935 Test Loss: 0.4109018
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.0536441802978516
Epoch: 63, Steps: 63 | Train Loss: 0.3962926 Vali Loss: 0.9244860 Test Loss: 0.4109028
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.7850594520568848
Epoch: 64, Steps: 63 | Train Loss: 0.3963514 Vali Loss: 0.9243109 Test Loss: 0.4108948
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.9951584339141846
Epoch: 65, Steps: 63 | Train Loss: 0.3962815 Vali Loss: 0.9243938 Test Loss: 0.4108896
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.0801031589508057
Epoch: 66, Steps: 63 | Train Loss: 0.3959950 Vali Loss: 0.9242725 Test Loss: 0.4108990
Validation loss decreased (0.924290 --> 0.924273).  Saving model ...
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.7756657600402832
Epoch: 67, Steps: 63 | Train Loss: 0.3962109 Vali Loss: 0.9243341 Test Loss: 0.4109052
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.7622272968292236
Epoch: 68, Steps: 63 | Train Loss: 0.3961536 Vali Loss: 0.9241700 Test Loss: 0.4109049
Validation loss decreased (0.924273 --> 0.924170).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.1141295433044434
Epoch: 69, Steps: 63 | Train Loss: 0.3961285 Vali Loss: 0.9241716 Test Loss: 0.4109032
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.076155185699463
Epoch: 70, Steps: 63 | Train Loss: 0.3962700 Vali Loss: 0.9239396 Test Loss: 0.4108956
Validation loss decreased (0.924170 --> 0.923940).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.4333620071411133
Epoch: 71, Steps: 63 | Train Loss: 0.3961067 Vali Loss: 0.9242171 Test Loss: 0.4108860
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.9903476238250732
Epoch: 72, Steps: 63 | Train Loss: 0.3960169 Vali Loss: 0.9240264 Test Loss: 0.4108998
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 4.732337951660156
Epoch: 73, Steps: 63 | Train Loss: 0.3960759 Vali Loss: 0.9240877 Test Loss: 0.4108977
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.482795238494873
Epoch: 74, Steps: 63 | Train Loss: 0.3962232 Vali Loss: 0.9240664 Test Loss: 0.4109041
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.9106807708740234
Epoch: 75, Steps: 63 | Train Loss: 0.3959273 Vali Loss: 0.9240438 Test Loss: 0.4108857
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.382019281387329
Epoch: 76, Steps: 63 | Train Loss: 0.3959753 Vali Loss: 0.9241323 Test Loss: 0.4108967
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.3517608642578125
Epoch: 77, Steps: 63 | Train Loss: 0.3963644 Vali Loss: 0.9238042 Test Loss: 0.4109016
Validation loss decreased (0.923940 --> 0.923804).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.7490792274475098
Epoch: 78, Steps: 63 | Train Loss: 0.3961137 Vali Loss: 0.9235286 Test Loss: 0.4109001
Validation loss decreased (0.923804 --> 0.923529).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.2279675006866455
Epoch: 79, Steps: 63 | Train Loss: 0.3961372 Vali Loss: 0.9237990 Test Loss: 0.4108901
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.642134666442871
Epoch: 80, Steps: 63 | Train Loss: 0.3962828 Vali Loss: 0.9235684 Test Loss: 0.4109024
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.1632118225097656
Epoch: 81, Steps: 63 | Train Loss: 0.3958898 Vali Loss: 0.9233556 Test Loss: 0.4108992
Validation loss decreased (0.923529 --> 0.923356).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.6860878467559814
Epoch: 82, Steps: 63 | Train Loss: 0.3962848 Vali Loss: 0.9238831 Test Loss: 0.4109039
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.7907426357269287
Epoch: 83, Steps: 63 | Train Loss: 0.3960862 Vali Loss: 0.9239796 Test Loss: 0.4109104
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.14367413520813
Epoch: 84, Steps: 63 | Train Loss: 0.3961620 Vali Loss: 0.9234338 Test Loss: 0.4108941
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.904237985610962
Epoch: 85, Steps: 63 | Train Loss: 0.3958030 Vali Loss: 0.9238625 Test Loss: 0.4109004
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.572765350341797
Epoch: 86, Steps: 63 | Train Loss: 0.3962591 Vali Loss: 0.9239936 Test Loss: 0.4108979
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.4684245586395264
Epoch: 87, Steps: 63 | Train Loss: 0.3962237 Vali Loss: 0.9238991 Test Loss: 0.4108977
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.4322779178619385
Epoch: 88, Steps: 63 | Train Loss: 0.3962083 Vali Loss: 0.9238477 Test Loss: 0.4109001
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.2037765979766846
Epoch: 89, Steps: 63 | Train Loss: 0.3961513 Vali Loss: 0.9239714 Test Loss: 0.4108956
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.970046043395996
Epoch: 90, Steps: 63 | Train Loss: 0.3962405 Vali Loss: 0.9233640 Test Loss: 0.4108977
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.923985242843628
Epoch: 91, Steps: 63 | Train Loss: 0.3962220 Vali Loss: 0.9238737 Test Loss: 0.4108973
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.451899290084839
Epoch: 92, Steps: 63 | Train Loss: 0.3959184 Vali Loss: 0.9239261 Test Loss: 0.4108976
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.1171023845672607
Epoch: 93, Steps: 63 | Train Loss: 0.3960724 Vali Loss: 0.9237031 Test Loss: 0.4108979
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.6982498168945312
Epoch: 94, Steps: 63 | Train Loss: 0.3959174 Vali Loss: 0.9238118 Test Loss: 0.4108983
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.8206334114074707
Epoch: 95, Steps: 63 | Train Loss: 0.3961056 Vali Loss: 0.9238188 Test Loss: 0.4109015
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.145993947982788
Epoch: 96, Steps: 63 | Train Loss: 0.3963231 Vali Loss: 0.9238563 Test Loss: 0.4108990
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.0249180793762207
Epoch: 97, Steps: 63 | Train Loss: 0.3957441 Vali Loss: 0.9238316 Test Loss: 0.4108944
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.6941232681274414
Epoch: 98, Steps: 63 | Train Loss: 0.3959960 Vali Loss: 0.9239022 Test Loss: 0.4108997
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.8842644691467285
Epoch: 99, Steps: 63 | Train Loss: 0.3963981 Vali Loss: 0.9233713 Test Loss: 0.4108979
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.000947952270508
Epoch: 100, Steps: 63 | Train Loss: 0.3960650 Vali Loss: 0.9238251 Test Loss: 0.4109001
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4067719578742981, mae:0.4150561988353729, rse:0.6056655049324036, corr:[0.2608135  0.26926255 0.26845142 0.2695735  0.26870206 0.26584116
 0.2642293  0.26486212 0.26519507 0.26459843 0.26391515 0.26373035
 0.26381963 0.26354754 0.26299632 0.2627328  0.26274198 0.26282236
 0.2626958  0.2623145  0.26212996 0.26229113 0.2626222  0.26270124
 0.26234463 0.2621335  0.26214686 0.26192662 0.26135886 0.26094532
 0.26089844 0.26077017 0.2603159  0.2598527  0.25982556 0.26014775
 0.26037794 0.26031244 0.26014996 0.26022404 0.2605792  0.26080337
 0.26073897 0.26048613 0.26027864 0.26018575 0.26023495 0.26015034
 0.25959703 0.258784   0.2579641  0.2573186  0.25661716 0.2557809
 0.25527632 0.2549499  0.25454083 0.25417188 0.2539454  0.2540582
 0.25409728 0.2538425  0.2534885  0.25346455 0.25379664 0.2542071
 0.25440142 0.2542312  0.25420564 0.2544494  0.25457615 0.25420925
 0.25352326 0.2528326  0.25221804 0.25174686 0.25152546 0.2514197
 0.2511867  0.2506444  0.2501012  0.24979547 0.24966983 0.24957006
 0.2494787  0.24937059 0.24934448 0.24937795 0.24946949 0.24952316
 0.24933068 0.24900024 0.24884357 0.24882106 0.24885586 0.24924044
 0.24990362 0.2501526  0.25012934 0.24998048 0.24993621 0.24998774
 0.24993652 0.24977896 0.24950403 0.24933065 0.24928863 0.24918245
 0.24889657 0.24861953 0.24864528 0.24915652 0.24964182 0.24978513
 0.24962695 0.2494041  0.24927354 0.24907617 0.24873509 0.24856237
 0.24862714 0.24834721 0.24753444 0.24664934 0.24619365 0.24593279
 0.24572831 0.24548297 0.24517176 0.24483296 0.24463566 0.24448161
 0.24423595 0.24409345 0.24412715 0.24418794 0.24450466 0.24477765
 0.2449526  0.24502796 0.24507168 0.24498807 0.24464549 0.24438095
 0.24440902 0.24409759 0.2434228  0.24267109 0.24218935 0.24168293
 0.24127454 0.24113567 0.24113384 0.24132131 0.24143581 0.2414557
 0.24125703 0.24118188 0.24125107 0.2413628  0.24134962 0.24133968
 0.24145067 0.24159029 0.24160245 0.24121061 0.24067715 0.2406185
 0.24104816 0.24125734 0.24124318 0.24133366 0.2414517  0.24121408
 0.24084626 0.24072717 0.24075516 0.24104078 0.24142669 0.24147813
 0.2410845  0.2407717  0.24131854 0.24198331 0.24142888 0.24074094
 0.24170345 0.24268788 0.24078739 0.23843998 0.24085523 0.23763795]
