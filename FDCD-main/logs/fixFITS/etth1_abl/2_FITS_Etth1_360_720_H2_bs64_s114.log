Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4741632.0
params:  5418.0
Trainable parameters:  5418
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.1894524097442627
Epoch: 1, Steps: 59 | Train Loss: 0.9731854 Vali Loss: 2.4484763 Test Loss: 1.0659398
Validation loss decreased (inf --> 2.448476).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.756808519363403
Epoch: 2, Steps: 59 | Train Loss: 0.7995689 Vali Loss: 2.1693919 Test Loss: 0.9009990
Validation loss decreased (2.448476 --> 2.169392).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.5140433311462402
Epoch: 3, Steps: 59 | Train Loss: 0.6956350 Vali Loss: 2.0046697 Test Loss: 0.8037194
Validation loss decreased (2.169392 --> 2.004670).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.4484446048736572
Epoch: 4, Steps: 59 | Train Loss: 0.6331792 Vali Loss: 1.9095010 Test Loss: 0.7433739
Validation loss decreased (2.004670 --> 1.909501).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.632286787033081
Epoch: 5, Steps: 59 | Train Loss: 0.5939300 Vali Loss: 1.8470953 Test Loss: 0.7050428
Validation loss decreased (1.909501 --> 1.847095).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.9806811809539795
Epoch: 6, Steps: 59 | Train Loss: 0.5682649 Vali Loss: 1.8021822 Test Loss: 0.6793696
Validation loss decreased (1.847095 --> 1.802182).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.5510244369506836
Epoch: 7, Steps: 59 | Train Loss: 0.5503203 Vali Loss: 1.7791243 Test Loss: 0.6609309
Validation loss decreased (1.802182 --> 1.779124).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.880596876144409
Epoch: 8, Steps: 59 | Train Loss: 0.5373329 Vali Loss: 1.7501109 Test Loss: 0.6462804
Validation loss decreased (1.779124 --> 1.750111).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.933103322982788
Epoch: 9, Steps: 59 | Train Loss: 0.5272436 Vali Loss: 1.7320036 Test Loss: 0.6347875
Validation loss decreased (1.750111 --> 1.732004).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.9552719593048096
Epoch: 10, Steps: 59 | Train Loss: 0.5192350 Vali Loss: 1.7176366 Test Loss: 0.6250960
Validation loss decreased (1.732004 --> 1.717637).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.8122451305389404
Epoch: 11, Steps: 59 | Train Loss: 0.5123137 Vali Loss: 1.7114707 Test Loss: 0.6164161
Validation loss decreased (1.717637 --> 1.711471).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3758418560028076
Epoch: 12, Steps: 59 | Train Loss: 0.5065812 Vali Loss: 1.6927856 Test Loss: 0.6084478
Validation loss decreased (1.711471 --> 1.692786).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.8436455726623535
Epoch: 13, Steps: 59 | Train Loss: 0.5013719 Vali Loss: 1.6831061 Test Loss: 0.6016226
Validation loss decreased (1.692786 --> 1.683106).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.897019624710083
Epoch: 14, Steps: 59 | Train Loss: 0.4969613 Vali Loss: 1.6760082 Test Loss: 0.5950141
Validation loss decreased (1.683106 --> 1.676008).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.872382164001465
Epoch: 15, Steps: 59 | Train Loss: 0.4930897 Vali Loss: 1.6600518 Test Loss: 0.5888928
Validation loss decreased (1.676008 --> 1.660052).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.625990867614746
Epoch: 16, Steps: 59 | Train Loss: 0.4894174 Vali Loss: 1.6567724 Test Loss: 0.5834131
Validation loss decreased (1.660052 --> 1.656772).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.9926519393920898
Epoch: 17, Steps: 59 | Train Loss: 0.4861597 Vali Loss: 1.6495273 Test Loss: 0.5784505
Validation loss decreased (1.656772 --> 1.649527).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.5531482696533203
Epoch: 18, Steps: 59 | Train Loss: 0.4830628 Vali Loss: 1.6352365 Test Loss: 0.5736987
Validation loss decreased (1.649527 --> 1.635237).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.6155505180358887
Epoch: 19, Steps: 59 | Train Loss: 0.4804662 Vali Loss: 1.6348937 Test Loss: 0.5693822
Validation loss decreased (1.635237 --> 1.634894).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.4098939895629883
Epoch: 20, Steps: 59 | Train Loss: 0.4778955 Vali Loss: 1.6280060 Test Loss: 0.5653938
Validation loss decreased (1.634894 --> 1.628006).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.9875783920288086
Epoch: 21, Steps: 59 | Train Loss: 0.4756627 Vali Loss: 1.6233274 Test Loss: 0.5617536
Validation loss decreased (1.628006 --> 1.623327).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.0490455627441406
Epoch: 22, Steps: 59 | Train Loss: 0.4734864 Vali Loss: 1.6179173 Test Loss: 0.5580282
Validation loss decreased (1.623327 --> 1.617917).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.993309259414673
Epoch: 23, Steps: 59 | Train Loss: 0.4715341 Vali Loss: 1.6153814 Test Loss: 0.5548709
Validation loss decreased (1.617917 --> 1.615381).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.5776970386505127
Epoch: 24, Steps: 59 | Train Loss: 0.4696002 Vali Loss: 1.6095943 Test Loss: 0.5517455
Validation loss decreased (1.615381 --> 1.609594).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.761629581451416
Epoch: 25, Steps: 59 | Train Loss: 0.4680220 Vali Loss: 1.6020331 Test Loss: 0.5488271
Validation loss decreased (1.609594 --> 1.602033).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.4730591773986816
Epoch: 26, Steps: 59 | Train Loss: 0.4664179 Vali Loss: 1.6056309 Test Loss: 0.5462475
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.7965450286865234
Epoch: 27, Steps: 59 | Train Loss: 0.4649744 Vali Loss: 1.6010745 Test Loss: 0.5437998
Validation loss decreased (1.602033 --> 1.601074).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.3923239707946777
Epoch: 28, Steps: 59 | Train Loss: 0.4634862 Vali Loss: 1.5954897 Test Loss: 0.5414079
Validation loss decreased (1.601074 --> 1.595490).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.3603768348693848
Epoch: 29, Steps: 59 | Train Loss: 0.4622408 Vali Loss: 1.5948679 Test Loss: 0.5391819
Validation loss decreased (1.595490 --> 1.594868).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.276838779449463
Epoch: 30, Steps: 59 | Train Loss: 0.4610300 Vali Loss: 1.5840379 Test Loss: 0.5370893
Validation loss decreased (1.594868 --> 1.584038).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.333254337310791
Epoch: 31, Steps: 59 | Train Loss: 0.4598558 Vali Loss: 1.5917463 Test Loss: 0.5352714
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.2852492332458496
Epoch: 32, Steps: 59 | Train Loss: 0.4588786 Vali Loss: 1.5848057 Test Loss: 0.5333653
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4052367210388184
Epoch: 33, Steps: 59 | Train Loss: 0.4579540 Vali Loss: 1.5798457 Test Loss: 0.5316795
Validation loss decreased (1.584038 --> 1.579846).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3351635932922363
Epoch: 34, Steps: 59 | Train Loss: 0.4569046 Vali Loss: 1.5752209 Test Loss: 0.5300109
Validation loss decreased (1.579846 --> 1.575221).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.4244084358215332
Epoch: 35, Steps: 59 | Train Loss: 0.4560838 Vali Loss: 1.5771561 Test Loss: 0.5285189
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3820304870605469
Epoch: 36, Steps: 59 | Train Loss: 0.4553143 Vali Loss: 1.5756488 Test Loss: 0.5270281
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.342026710510254
Epoch: 37, Steps: 59 | Train Loss: 0.4545380 Vali Loss: 1.5707057 Test Loss: 0.5256312
Validation loss decreased (1.575221 --> 1.570706).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.29384446144104
Epoch: 38, Steps: 59 | Train Loss: 0.4537824 Vali Loss: 1.5670766 Test Loss: 0.5244543
Validation loss decreased (1.570706 --> 1.567077).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3037407398223877
Epoch: 39, Steps: 59 | Train Loss: 0.4530152 Vali Loss: 1.5633295 Test Loss: 0.5232651
Validation loss decreased (1.567077 --> 1.563329).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.402806282043457
Epoch: 40, Steps: 59 | Train Loss: 0.4524535 Vali Loss: 1.5658958 Test Loss: 0.5221145
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.397446870803833
Epoch: 41, Steps: 59 | Train Loss: 0.4517945 Vali Loss: 1.5696639 Test Loss: 0.5210136
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3008029460906982
Epoch: 42, Steps: 59 | Train Loss: 0.4513242 Vali Loss: 1.5666841 Test Loss: 0.5200497
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.3030955791473389
Epoch: 43, Steps: 59 | Train Loss: 0.4507076 Vali Loss: 1.5616217 Test Loss: 0.5190907
Validation loss decreased (1.563329 --> 1.561622).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.3610594272613525
Epoch: 44, Steps: 59 | Train Loss: 0.4500765 Vali Loss: 1.5604205 Test Loss: 0.5181848
Validation loss decreased (1.561622 --> 1.560421).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.280557632446289
Epoch: 45, Steps: 59 | Train Loss: 0.4497435 Vali Loss: 1.5625688 Test Loss: 0.5172819
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.3812916278839111
Epoch: 46, Steps: 59 | Train Loss: 0.4492286 Vali Loss: 1.5566013 Test Loss: 0.5164841
Validation loss decreased (1.560421 --> 1.556601).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.3157713413238525
Epoch: 47, Steps: 59 | Train Loss: 0.4488176 Vali Loss: 1.5584261 Test Loss: 0.5157263
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.3871872425079346
Epoch: 48, Steps: 59 | Train Loss: 0.4483649 Vali Loss: 1.5544220 Test Loss: 0.5149986
Validation loss decreased (1.556601 --> 1.554422).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.3686614036560059
Epoch: 49, Steps: 59 | Train Loss: 0.4479550 Vali Loss: 1.5521057 Test Loss: 0.5142984
Validation loss decreased (1.554422 --> 1.552106).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.3162646293640137
Epoch: 50, Steps: 59 | Train Loss: 0.4476560 Vali Loss: 1.5554972 Test Loss: 0.5136414
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.426159143447876
Epoch: 51, Steps: 59 | Train Loss: 0.4472211 Vali Loss: 1.5479589 Test Loss: 0.5130000
Validation loss decreased (1.552106 --> 1.547959).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3889830112457275
Epoch: 52, Steps: 59 | Train Loss: 0.4469447 Vali Loss: 1.5490776 Test Loss: 0.5124390
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.3334603309631348
Epoch: 53, Steps: 59 | Train Loss: 0.4465495 Vali Loss: 1.5539162 Test Loss: 0.5118815
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.3309009075164795
Epoch: 54, Steps: 59 | Train Loss: 0.4462690 Vali Loss: 1.5498079 Test Loss: 0.5113497
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.3324215412139893
Epoch: 55, Steps: 59 | Train Loss: 0.4459491 Vali Loss: 1.5437732 Test Loss: 0.5108379
Validation loss decreased (1.547959 --> 1.543773).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.2831525802612305
Epoch: 56, Steps: 59 | Train Loss: 0.4456718 Vali Loss: 1.5517547 Test Loss: 0.5103711
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.3307204246520996
Epoch: 57, Steps: 59 | Train Loss: 0.4454274 Vali Loss: 1.5474129 Test Loss: 0.5099097
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.2828185558319092
Epoch: 58, Steps: 59 | Train Loss: 0.4453418 Vali Loss: 1.5485160 Test Loss: 0.5094805
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.3333466053009033
Epoch: 59, Steps: 59 | Train Loss: 0.4450111 Vali Loss: 1.5467896 Test Loss: 0.5090507
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.3955621719360352
Epoch: 60, Steps: 59 | Train Loss: 0.4447984 Vali Loss: 1.5475082 Test Loss: 0.5086970
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.36545729637146
Epoch: 61, Steps: 59 | Train Loss: 0.4445862 Vali Loss: 1.5448308 Test Loss: 0.5083207
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.3475310802459717
Epoch: 62, Steps: 59 | Train Loss: 0.4442104 Vali Loss: 1.5435550 Test Loss: 0.5079565
Validation loss decreased (1.543773 --> 1.543555).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.3132903575897217
Epoch: 63, Steps: 59 | Train Loss: 0.4440160 Vali Loss: 1.5410736 Test Loss: 0.5076452
Validation loss decreased (1.543555 --> 1.541074).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.3044397830963135
Epoch: 64, Steps: 59 | Train Loss: 0.4440758 Vali Loss: 1.5460775 Test Loss: 0.5073046
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.3199183940887451
Epoch: 65, Steps: 59 | Train Loss: 0.4437080 Vali Loss: 1.5452752 Test Loss: 0.5070161
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.3446102142333984
Epoch: 66, Steps: 59 | Train Loss: 0.4435978 Vali Loss: 1.5426227 Test Loss: 0.5067170
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.3183355331420898
Epoch: 67, Steps: 59 | Train Loss: 0.4434942 Vali Loss: 1.5408589 Test Loss: 0.5064464
Validation loss decreased (1.541074 --> 1.540859).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.3614046573638916
Epoch: 68, Steps: 59 | Train Loss: 0.4433109 Vali Loss: 1.5467008 Test Loss: 0.5061703
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3567755222320557
Epoch: 69, Steps: 59 | Train Loss: 0.4431957 Vali Loss: 1.5399189 Test Loss: 0.5059453
Validation loss decreased (1.540859 --> 1.539919).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.2626357078552246
Epoch: 70, Steps: 59 | Train Loss: 0.4430609 Vali Loss: 1.5421056 Test Loss: 0.5057052
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.333104133605957
Epoch: 71, Steps: 59 | Train Loss: 0.4428941 Vali Loss: 1.5402417 Test Loss: 0.5054878
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.280132532119751
Epoch: 72, Steps: 59 | Train Loss: 0.4428414 Vali Loss: 1.5415277 Test Loss: 0.5052714
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.3674676418304443
Epoch: 73, Steps: 59 | Train Loss: 0.4426994 Vali Loss: 1.5400229 Test Loss: 0.5050673
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.3185770511627197
Epoch: 74, Steps: 59 | Train Loss: 0.4426042 Vali Loss: 1.5436701 Test Loss: 0.5048869
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.3384602069854736
Epoch: 75, Steps: 59 | Train Loss: 0.4425346 Vali Loss: 1.5340974 Test Loss: 0.5046972
Validation loss decreased (1.539919 --> 1.534097).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.3818917274475098
Epoch: 76, Steps: 59 | Train Loss: 0.4424936 Vali Loss: 1.5388603 Test Loss: 0.5045308
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.3591420650482178
Epoch: 77, Steps: 59 | Train Loss: 0.4422386 Vali Loss: 1.5286955 Test Loss: 0.5043664
Validation loss decreased (1.534097 --> 1.528695).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.3925926685333252
Epoch: 78, Steps: 59 | Train Loss: 0.4421772 Vali Loss: 1.5439336 Test Loss: 0.5042087
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.4509167671203613
Epoch: 79, Steps: 59 | Train Loss: 0.4420667 Vali Loss: 1.5386498 Test Loss: 0.5040569
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.4050350189208984
Epoch: 80, Steps: 59 | Train Loss: 0.4420117 Vali Loss: 1.5381203 Test Loss: 0.5039257
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.35011625289917
Epoch: 81, Steps: 59 | Train Loss: 0.4418995 Vali Loss: 1.5368764 Test Loss: 0.5037900
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.367922067642212
Epoch: 82, Steps: 59 | Train Loss: 0.4417907 Vali Loss: 1.5373658 Test Loss: 0.5036589
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.3664114475250244
Epoch: 83, Steps: 59 | Train Loss: 0.4418937 Vali Loss: 1.5333339 Test Loss: 0.5035380
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.2873272895812988
Epoch: 84, Steps: 59 | Train Loss: 0.4418094 Vali Loss: 1.5397564 Test Loss: 0.5034159
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.3159770965576172
Epoch: 85, Steps: 59 | Train Loss: 0.4416487 Vali Loss: 1.5423857 Test Loss: 0.5033109
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.4210295677185059
Epoch: 86, Steps: 59 | Train Loss: 0.4417404 Vali Loss: 1.5361083 Test Loss: 0.5032041
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.351733684539795
Epoch: 87, Steps: 59 | Train Loss: 0.4416637 Vali Loss: 1.5399499 Test Loss: 0.5031111
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.372267246246338
Epoch: 88, Steps: 59 | Train Loss: 0.4415840 Vali Loss: 1.5362890 Test Loss: 0.5030157
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.3459062576293945
Epoch: 89, Steps: 59 | Train Loss: 0.4414654 Vali Loss: 1.5323217 Test Loss: 0.5029287
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.3593559265136719
Epoch: 90, Steps: 59 | Train Loss: 0.4413709 Vali Loss: 1.5367043 Test Loss: 0.5028404
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.4046721458435059
Epoch: 91, Steps: 59 | Train Loss: 0.4413945 Vali Loss: 1.5346949 Test Loss: 0.5027580
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.259840965270996
Epoch: 92, Steps: 59 | Train Loss: 0.4413780 Vali Loss: 1.5369937 Test Loss: 0.5026784
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.388596534729004
Epoch: 93, Steps: 59 | Train Loss: 0.4412416 Vali Loss: 1.5370564 Test Loss: 0.5026102
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.411243200302124
Epoch: 94, Steps: 59 | Train Loss: 0.4412122 Vali Loss: 1.5357363 Test Loss: 0.5025402
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.34102201461792
Epoch: 95, Steps: 59 | Train Loss: 0.4412397 Vali Loss: 1.5349810 Test Loss: 0.5024754
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.321425437927246
Epoch: 96, Steps: 59 | Train Loss: 0.4412727 Vali Loss: 1.5375648 Test Loss: 0.5024076
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.3873121738433838
Epoch: 97, Steps: 59 | Train Loss: 0.4412434 Vali Loss: 1.5387678 Test Loss: 0.5023496
EarlyStopping counter: 20 out of 20
Early stopping
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=126, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4741632.0
params:  5418.0
Trainable parameters:  5418
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.1898486614227295
Epoch: 1, Steps: 59 | Train Loss: 0.6094891 Vali Loss: 1.5009174 Test Loss: 0.4800073
Validation loss decreased (inf --> 1.500917).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.3651008605957031
Epoch: 2, Steps: 59 | Train Loss: 0.5989056 Vali Loss: 1.4813601 Test Loss: 0.4665335
Validation loss decreased (1.500917 --> 1.481360).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.329235553741455
Epoch: 3, Steps: 59 | Train Loss: 0.5926056 Vali Loss: 1.4647475 Test Loss: 0.4581199
Validation loss decreased (1.481360 --> 1.464748).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3349013328552246
Epoch: 4, Steps: 59 | Train Loss: 0.5885845 Vali Loss: 1.4574282 Test Loss: 0.4531755
Validation loss decreased (1.464748 --> 1.457428).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.3984456062316895
Epoch: 5, Steps: 59 | Train Loss: 0.5860018 Vali Loss: 1.4495423 Test Loss: 0.4503321
Validation loss decreased (1.457428 --> 1.449542).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.3890416622161865
Epoch: 6, Steps: 59 | Train Loss: 0.5841915 Vali Loss: 1.4527507 Test Loss: 0.4484602
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.344151258468628
Epoch: 7, Steps: 59 | Train Loss: 0.5832141 Vali Loss: 1.4417329 Test Loss: 0.4476961
Validation loss decreased (1.449542 --> 1.441733).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.3883638381958008
Epoch: 8, Steps: 59 | Train Loss: 0.5824169 Vali Loss: 1.4438326 Test Loss: 0.4471667
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3185842037200928
Epoch: 9, Steps: 59 | Train Loss: 0.5820182 Vali Loss: 1.4333835 Test Loss: 0.4470541
Validation loss decreased (1.441733 --> 1.433383).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.408977746963501
Epoch: 10, Steps: 59 | Train Loss: 0.5815740 Vali Loss: 1.4376148 Test Loss: 0.4470615
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.2974843978881836
Epoch: 11, Steps: 59 | Train Loss: 0.5811663 Vali Loss: 1.4320014 Test Loss: 0.4472099
Validation loss decreased (1.433383 --> 1.432001).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.3190650939941406
Epoch: 12, Steps: 59 | Train Loss: 0.5811694 Vali Loss: 1.4382710 Test Loss: 0.4473384
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.3415961265563965
Epoch: 13, Steps: 59 | Train Loss: 0.5810622 Vali Loss: 1.4405911 Test Loss: 0.4474199
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.2771193981170654
Epoch: 14, Steps: 59 | Train Loss: 0.5810209 Vali Loss: 1.4394821 Test Loss: 0.4475504
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4219281673431396
Epoch: 15, Steps: 59 | Train Loss: 0.5808332 Vali Loss: 1.4396272 Test Loss: 0.4476786
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4000144004821777
Epoch: 16, Steps: 59 | Train Loss: 0.5805362 Vali Loss: 1.4408536 Test Loss: 0.4476510
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.3215644359588623
Epoch: 17, Steps: 59 | Train Loss: 0.5807771 Vali Loss: 1.4320843 Test Loss: 0.4477829
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.351370096206665
Epoch: 18, Steps: 59 | Train Loss: 0.5806445 Vali Loss: 1.4381330 Test Loss: 0.4478968
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.2736425399780273
Epoch: 19, Steps: 59 | Train Loss: 0.5805561 Vali Loss: 1.4383321 Test Loss: 0.4479533
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.3257038593292236
Epoch: 20, Steps: 59 | Train Loss: 0.5806179 Vali Loss: 1.4367167 Test Loss: 0.4480005
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.3349189758300781
Epoch: 21, Steps: 59 | Train Loss: 0.5805134 Vali Loss: 1.4353273 Test Loss: 0.4480577
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.3882660865783691
Epoch: 22, Steps: 59 | Train Loss: 0.5806850 Vali Loss: 1.4271934 Test Loss: 0.4480346
Validation loss decreased (1.432001 --> 1.427193).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.2997288703918457
Epoch: 23, Steps: 59 | Train Loss: 0.5806403 Vali Loss: 1.4347380 Test Loss: 0.4481328
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.3086907863616943
Epoch: 24, Steps: 59 | Train Loss: 0.5805741 Vali Loss: 1.4397688 Test Loss: 0.4481410
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.3972468376159668
Epoch: 25, Steps: 59 | Train Loss: 0.5805856 Vali Loss: 1.4358577 Test Loss: 0.4481660
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.394737958908081
Epoch: 26, Steps: 59 | Train Loss: 0.5805819 Vali Loss: 1.4343817 Test Loss: 0.4482034
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.4029362201690674
Epoch: 27, Steps: 59 | Train Loss: 0.5804466 Vali Loss: 1.4373534 Test Loss: 0.4482386
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.3086793422698975
Epoch: 28, Steps: 59 | Train Loss: 0.5804569 Vali Loss: 1.4320266 Test Loss: 0.4482614
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.242873191833496
Epoch: 29, Steps: 59 | Train Loss: 0.5804696 Vali Loss: 1.4279692 Test Loss: 0.4482303
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.3724937438964844
Epoch: 30, Steps: 59 | Train Loss: 0.5804608 Vali Loss: 1.4366009 Test Loss: 0.4482676
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.2237765789031982
Epoch: 31, Steps: 59 | Train Loss: 0.5805150 Vali Loss: 1.4350502 Test Loss: 0.4482739
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.3269789218902588
Epoch: 32, Steps: 59 | Train Loss: 0.5806843 Vali Loss: 1.4385743 Test Loss: 0.4482721
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.3522469997406006
Epoch: 33, Steps: 59 | Train Loss: 0.5804774 Vali Loss: 1.4380041 Test Loss: 0.4483289
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.3733494281768799
Epoch: 34, Steps: 59 | Train Loss: 0.5801943 Vali Loss: 1.4293481 Test Loss: 0.4483250
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.3341715335845947
Epoch: 35, Steps: 59 | Train Loss: 0.5803596 Vali Loss: 1.4378990 Test Loss: 0.4483517
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3401556015014648
Epoch: 36, Steps: 59 | Train Loss: 0.5803530 Vali Loss: 1.4342439 Test Loss: 0.4483405
EarlyStopping counter: 14 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.3772172927856445
Epoch: 37, Steps: 59 | Train Loss: 0.5802603 Vali Loss: 1.4355314 Test Loss: 0.4483821
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4299139976501465
Epoch: 38, Steps: 59 | Train Loss: 0.5802565 Vali Loss: 1.4356115 Test Loss: 0.4483888
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.3472442626953125
Epoch: 39, Steps: 59 | Train Loss: 0.5802501 Vali Loss: 1.4373291 Test Loss: 0.4484112
EarlyStopping counter: 17 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.2589869499206543
Epoch: 40, Steps: 59 | Train Loss: 0.5803603 Vali Loss: 1.4359996 Test Loss: 0.4484162
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.336458444595337
Epoch: 41, Steps: 59 | Train Loss: 0.5802926 Vali Loss: 1.4346981 Test Loss: 0.4484314
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.2947049140930176
Epoch: 42, Steps: 59 | Train Loss: 0.5802074 Vali Loss: 1.4324632 Test Loss: 0.4484203
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4471307694911957, mae:0.465288370847702, rse:0.6401315331459045, corr:[0.2255195  0.23149133 0.23343444 0.23232405 0.23003367 0.2281435
 0.22729747 0.22745892 0.22784628 0.22824228 0.22840467 0.22817765
 0.22776574 0.22740324 0.227101   0.22690266 0.22669216 0.22644573
 0.22627838 0.22624515 0.22642504 0.22693288 0.22752531 0.22817394
 0.22860354 0.22872646 0.22858487 0.22818027 0.22758794 0.22701216
 0.22657043 0.22628683 0.22613591 0.22607252 0.22611502 0.22617176
 0.22625682 0.2263395  0.22641951 0.22642016 0.22638193 0.22617303
 0.22601064 0.22601715 0.22622484 0.22654049 0.22693197 0.2272198
 0.2270581  0.22652826 0.22561349 0.22465955 0.2239805  0.22346537
 0.22319873 0.22297342 0.22291839 0.22278073 0.2224678  0.22215557
 0.2218709  0.22170275 0.2216128  0.22170854 0.22185193 0.22197902
 0.22210725 0.22213016 0.22210851 0.2221376  0.22211167 0.22209872
 0.22177576 0.2212782  0.22074682 0.22029069 0.21988519 0.2194836
 0.21916567 0.21891314 0.2186821  0.21846326 0.21821223 0.21797726
 0.21781744 0.21770842 0.21754533 0.21727072 0.21692644 0.2166016
 0.21635985 0.21630949 0.21650267 0.21707231 0.21800248 0.21914977
 0.2203449  0.22137746 0.22211824 0.22248763 0.2224746  0.22223374
 0.2218921  0.22160782 0.22134124 0.22107378 0.22082964 0.22062778
 0.22053054 0.22053236 0.22052734 0.22056949 0.22053464 0.22041474
 0.22036207 0.22034046 0.22037533 0.22038543 0.22037074 0.22029416
 0.22006951 0.21970278 0.21920374 0.21881425 0.21852338 0.21827501
 0.21806253 0.21787687 0.21761662 0.21724737 0.21689828 0.21669361
 0.2166358  0.21664193 0.21669172 0.21670438 0.21665268 0.2165227
 0.21639241 0.21633191 0.21637434 0.2164975  0.21659258 0.2165191
 0.21624559 0.2156898  0.2149806  0.214246   0.21373142 0.21345353
 0.21334839 0.21339281 0.21343984 0.21341787 0.2132587  0.21303628
 0.21282926 0.2127071  0.21271755 0.21282202 0.21281838 0.21277355
 0.21271972 0.2126726  0.21275482 0.21300149 0.21346278 0.21407886
 0.21474041 0.21531804 0.21555723 0.21554187 0.21538149 0.21525015
 0.21521775 0.21528657 0.21535237 0.21536843 0.21531537 0.21518153
 0.21503301 0.2149322  0.21496837 0.21508452 0.21524431 0.21542165
 0.21557403 0.21563642 0.21568495 0.21569882 0.21562947 0.21542245
 0.2150712  0.21462284 0.21404062 0.21342152 0.21283984 0.21233267
 0.21193178 0.21179159 0.21177293 0.2117737  0.21175434 0.21174741
 0.21172464 0.21174821 0.21177846 0.21177322 0.21178025 0.21176587
 0.21170591 0.21152295 0.21128596 0.21110195 0.21087804 0.21077655
 0.21082051 0.2108385  0.21084289 0.21077533 0.21071108 0.21063982
 0.21047878 0.21024923 0.2099923  0.20973437 0.20947056 0.20929106
 0.20919198 0.20906633 0.20894872 0.20880222 0.20854695 0.2082897
 0.20815599 0.2081198  0.20818907 0.2083185  0.20852545 0.20865665
 0.20877783 0.20878707 0.20871046 0.20869003 0.2087254  0.20876409
 0.20889688 0.20901884 0.20905013 0.20883958 0.20849742 0.20818585
 0.20805441 0.20801705 0.20815855 0.20829564 0.20844036 0.20852458
 0.2085479  0.2083697  0.20821013 0.20815799 0.2081776  0.20822355
 0.2082329  0.20814021 0.2079294  0.2076637  0.20732969 0.20702991
 0.20687349 0.20685965 0.20685595 0.20681566 0.20673998 0.20670572
 0.20670533 0.20671408 0.20662363 0.20642863 0.20613599 0.20585418
 0.2056151  0.20549464 0.20560473 0.20599191 0.20672725 0.20760818
 0.20849466 0.20917226 0.2095607  0.20959428 0.20944506 0.20925297
 0.20916463 0.20921183 0.20928346 0.20932347 0.20926772 0.20919439
 0.20914426 0.20909052 0.20913029 0.20927638 0.20936373 0.20934317
 0.20930816 0.20920034 0.20911227 0.20918955 0.20941778 0.20978236
 0.21026458 0.21069376 0.210822   0.21075177 0.21055079 0.21025583
 0.20995927 0.20973302 0.20949066 0.2092331  0.20901746 0.20893697
 0.20897214 0.2090671  0.20911744 0.2092491  0.20933564 0.20936543
 0.20938985 0.20940074 0.20941268 0.2094291  0.20942903 0.20934881
 0.20918962 0.20896758 0.20869897 0.20839511 0.20817433 0.20794712
 0.2078322  0.20784913 0.20789295 0.20792216 0.2079869  0.20809433
 0.20814984 0.2081921  0.2082109  0.20824276 0.2081546  0.20811047
 0.20811327 0.20805374 0.20791288 0.20785482 0.20773678 0.20763707
 0.20756987 0.20739797 0.20704862 0.20657438 0.20611879 0.205685
 0.20540997 0.20525838 0.2051163  0.20493788 0.20471573 0.2045129
 0.20429166 0.2040333  0.20382352 0.20368947 0.20365664 0.20368448
 0.20368737 0.2036253  0.20367071 0.20403425 0.2046611  0.20564294
 0.2067582  0.2075339  0.20783445 0.20755962 0.20700628 0.2062994
 0.20580707 0.20546873 0.20525838 0.20512456 0.20498154 0.20483215
 0.20473291 0.2046526  0.20463292 0.20469354 0.2048739  0.20524341
 0.20566832 0.20599775 0.20618394 0.20640734 0.20664446 0.2070442
 0.20742314 0.20768923 0.20771636 0.20751743 0.2071773  0.20672768
 0.20634013 0.20616153 0.20609024 0.20608664 0.20614    0.20616364
 0.20609987 0.20599791 0.20577142 0.20551525 0.20532078 0.20531218
 0.2054724  0.20580912 0.20616616 0.20682165 0.2075361  0.20822573
 0.20876497 0.209007   0.20887282 0.2085901  0.20831268 0.20801044
 0.20773733 0.20769392 0.20770925 0.20771512 0.20770852 0.20764881
 0.20756702 0.2074179  0.20726416 0.20712611 0.20701016 0.20695253
 0.20703146 0.20723115 0.20763631 0.20832486 0.2091169  0.20997682
 0.21062484 0.21087782 0.2106296  0.21019655 0.2096788  0.2092264
 0.20895986 0.20891449 0.20896633 0.20904183 0.20911022 0.20912947
 0.20913094 0.20916931 0.20926711 0.20935912 0.20940979 0.20945604
 0.20947392 0.20949875 0.20954508 0.20955485 0.20972118 0.21000534
 0.21018513 0.2101555  0.20981237 0.20939851 0.20901899 0.20870808
 0.20850971 0.20841207 0.2082591  0.20809525 0.20790426 0.2077438
 0.20761922 0.20751247 0.20742005 0.20735484 0.20734423 0.20733646
 0.20737758 0.20747659 0.20774586 0.20830914 0.20925775 0.21042478
 0.21149786 0.2122583  0.21250325 0.2123219  0.21184482 0.21120235
 0.21061938 0.21022321 0.20994765 0.20972091 0.2096053  0.20965005
 0.20977542 0.20988588 0.2099827  0.21006839 0.21021709 0.21046674
 0.21081695 0.21125814 0.21175207 0.21218832 0.21261041 0.21288888
 0.21298902 0.21282022 0.21243565 0.2119816  0.211436   0.21079165
 0.2102512  0.21000224 0.20984176 0.20971386 0.20961715 0.20968822
 0.20987217 0.21016818 0.21048187 0.21071894 0.21087171 0.21098936
 0.21102327 0.21099551 0.2108946  0.21080203 0.21070924 0.21067217
 0.21050838 0.21015443 0.20956926 0.20887211 0.20806201 0.20739947
 0.20693187 0.20660436 0.20629366 0.205926   0.20557821 0.20532474
 0.20518947 0.20500493 0.20489655 0.2048393  0.20494504 0.20507635
 0.20522502 0.20531167 0.20546405 0.20569962 0.20605838 0.20624551
 0.20618679 0.20602556 0.20552628 0.20484954 0.20421167 0.20348628
 0.202975   0.20272845 0.20265314 0.20256393 0.20243116 0.2021982
 0.20201229 0.20181186 0.20176312 0.20180313 0.20188355 0.20196414
 0.20202225 0.20196645 0.20186022 0.20176244 0.201696   0.20169519
 0.20158812 0.20121263 0.20057283 0.1998131  0.1990476  0.19831915
 0.197746   0.19742492 0.19723251 0.19710957 0.19696106 0.19676325
 0.19663456 0.19655126 0.19641769 0.19633895 0.19627886 0.19623028
 0.19629821 0.19646196 0.196696   0.19697526 0.19721323 0.19744077
 0.1975198  0.19728315 0.19691238 0.19649485 0.19595821 0.19525732
 0.19465679 0.19429904 0.1940512  0.19388318 0.19374155 0.19361968
 0.19359724 0.19347534 0.19330767 0.19316263 0.19302435 0.19304769
 0.19313902 0.19327879 0.19352265 0.19391428 0.19445083 0.19499037
 0.19533794 0.19538936 0.1949632  0.19454005 0.19389428 0.19325662
 0.19279855 0.19254044 0.19238718 0.1921288  0.19183528 0.19160585
 0.19147263 0.19136739 0.19130345 0.19115722 0.19112164 0.1911498
 0.19121076 0.1912802  0.19140628 0.19155839 0.19153222 0.191393
 0.19102229 0.19030748 0.18934362 0.18822739 0.18721677 0.18639554
 0.18586257 0.18569167 0.18570317 0.18561406 0.18544193 0.18526977
 0.18518408 0.18502405 0.1848907  0.18494113 0.18502589 0.18521836
 0.18544926 0.1857517  0.18610597 0.18657705 0.1869981  0.18716383
 0.18720068 0.18690082 0.18624075 0.1855498  0.1849721  0.18452516
 0.18442355 0.18456255 0.18463074 0.18433452 0.1837824  0.18313639
 0.18262546 0.18214098 0.18186666 0.18161532 0.18159135 0.18171674
 0.18214041 0.18261825 0.1830612  0.18286489 0.18124045 0.17632349]
