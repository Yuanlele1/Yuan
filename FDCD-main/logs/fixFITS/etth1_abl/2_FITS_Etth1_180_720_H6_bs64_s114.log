Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=58, out_features=290, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  15070720.0
params:  17110.0
Trainable parameters:  17110
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.300328016281128
Epoch: 1, Steps: 60 | Train Loss: 1.1243041 Vali Loss: 2.3226948 Test Loss: 1.0741987
Validation loss decreased (inf --> 2.322695).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.249729871749878
Epoch: 2, Steps: 60 | Train Loss: 0.8204484 Vali Loss: 1.9712186 Test Loss: 0.8034111
Validation loss decreased (2.322695 --> 1.971219).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2331349849700928
Epoch: 3, Steps: 60 | Train Loss: 0.6786472 Vali Loss: 1.7916746 Test Loss: 0.6713871
Validation loss decreased (1.971219 --> 1.791675).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.2796883583068848
Epoch: 4, Steps: 60 | Train Loss: 0.6053620 Vali Loss: 1.7010477 Test Loss: 0.5996804
Validation loss decreased (1.791675 --> 1.701048).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.1742467880249023
Epoch: 5, Steps: 60 | Train Loss: 0.5644799 Vali Loss: 1.6528229 Test Loss: 0.5580314
Validation loss decreased (1.701048 --> 1.652823).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.3024697303771973
Epoch: 6, Steps: 60 | Train Loss: 0.5403284 Vali Loss: 1.6106468 Test Loss: 0.5329424
Validation loss decreased (1.652823 --> 1.610647).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.3483223915100098
Epoch: 7, Steps: 60 | Train Loss: 0.5260261 Vali Loss: 1.5922160 Test Loss: 0.5171205
Validation loss decreased (1.610647 --> 1.592216).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.2999401092529297
Epoch: 8, Steps: 60 | Train Loss: 0.5164698 Vali Loss: 1.5806561 Test Loss: 0.5063176
Validation loss decreased (1.592216 --> 1.580656).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3896446228027344
Epoch: 9, Steps: 60 | Train Loss: 0.5097837 Vali Loss: 1.5732162 Test Loss: 0.4986880
Validation loss decreased (1.580656 --> 1.573216).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.3075971603393555
Epoch: 10, Steps: 60 | Train Loss: 0.5048368 Vali Loss: 1.5597088 Test Loss: 0.4930268
Validation loss decreased (1.573216 --> 1.559709).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.2272942066192627
Epoch: 11, Steps: 60 | Train Loss: 0.5016619 Vali Loss: 1.5577317 Test Loss: 0.4885371
Validation loss decreased (1.559709 --> 1.557732).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.3576445579528809
Epoch: 12, Steps: 60 | Train Loss: 0.4981990 Vali Loss: 1.5540302 Test Loss: 0.4847682
Validation loss decreased (1.557732 --> 1.554030).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.287520408630371
Epoch: 13, Steps: 60 | Train Loss: 0.4961902 Vali Loss: 1.5469604 Test Loss: 0.4818789
Validation loss decreased (1.554030 --> 1.546960).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.3314344882965088
Epoch: 14, Steps: 60 | Train Loss: 0.4944226 Vali Loss: 1.5429586 Test Loss: 0.4791861
Validation loss decreased (1.546960 --> 1.542959).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4439308643341064
Epoch: 15, Steps: 60 | Train Loss: 0.4928855 Vali Loss: 1.5442698 Test Loss: 0.4769130
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.439033031463623
Epoch: 16, Steps: 60 | Train Loss: 0.4909993 Vali Loss: 1.5366592 Test Loss: 0.4749186
Validation loss decreased (1.542959 --> 1.536659).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.2882380485534668
Epoch: 17, Steps: 60 | Train Loss: 0.4897449 Vali Loss: 1.5378379 Test Loss: 0.4730633
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.3792312145233154
Epoch: 18, Steps: 60 | Train Loss: 0.4885463 Vali Loss: 1.5300406 Test Loss: 0.4714109
Validation loss decreased (1.536659 --> 1.530041).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.3554344177246094
Epoch: 19, Steps: 60 | Train Loss: 0.4880680 Vali Loss: 1.5269330 Test Loss: 0.4700136
Validation loss decreased (1.530041 --> 1.526933).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.382215976715088
Epoch: 20, Steps: 60 | Train Loss: 0.4867884 Vali Loss: 1.5246150 Test Loss: 0.4686656
Validation loss decreased (1.526933 --> 1.524615).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.270383596420288
Epoch: 21, Steps: 60 | Train Loss: 0.4854060 Vali Loss: 1.5268691 Test Loss: 0.4674575
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.2600071430206299
Epoch: 22, Steps: 60 | Train Loss: 0.4852656 Vali Loss: 1.5234888 Test Loss: 0.4663855
Validation loss decreased (1.524615 --> 1.523489).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.2283260822296143
Epoch: 23, Steps: 60 | Train Loss: 0.4845684 Vali Loss: 1.5244565 Test Loss: 0.4653671
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.1927342414855957
Epoch: 24, Steps: 60 | Train Loss: 0.4838992 Vali Loss: 1.5271574 Test Loss: 0.4644205
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.2869963645935059
Epoch: 25, Steps: 60 | Train Loss: 0.4836065 Vali Loss: 1.5233551 Test Loss: 0.4635848
Validation loss decreased (1.523489 --> 1.523355).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.3173563480377197
Epoch: 26, Steps: 60 | Train Loss: 0.4832876 Vali Loss: 1.5211612 Test Loss: 0.4627488
Validation loss decreased (1.523355 --> 1.521161).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.2876620292663574
Epoch: 27, Steps: 60 | Train Loss: 0.4824311 Vali Loss: 1.5275385 Test Loss: 0.4620245
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.3098654747009277
Epoch: 28, Steps: 60 | Train Loss: 0.4820077 Vali Loss: 1.5218201 Test Loss: 0.4613621
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.2596845626831055
Epoch: 29, Steps: 60 | Train Loss: 0.4818628 Vali Loss: 1.5222807 Test Loss: 0.4607136
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.2445068359375
Epoch: 30, Steps: 60 | Train Loss: 0.4814425 Vali Loss: 1.5201136 Test Loss: 0.4601277
Validation loss decreased (1.521161 --> 1.520114).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.2329604625701904
Epoch: 31, Steps: 60 | Train Loss: 0.4806156 Vali Loss: 1.5197414 Test Loss: 0.4595800
Validation loss decreased (1.520114 --> 1.519741).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.3070271015167236
Epoch: 32, Steps: 60 | Train Loss: 0.4806805 Vali Loss: 1.5104530 Test Loss: 0.4590512
Validation loss decreased (1.519741 --> 1.510453).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.2539856433868408
Epoch: 33, Steps: 60 | Train Loss: 0.4802574 Vali Loss: 1.5162195 Test Loss: 0.4585990
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.1989614963531494
Epoch: 34, Steps: 60 | Train Loss: 0.4799229 Vali Loss: 1.5139688 Test Loss: 0.4581338
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.2516720294952393
Epoch: 35, Steps: 60 | Train Loss: 0.4794022 Vali Loss: 1.5093960 Test Loss: 0.4577170
Validation loss decreased (1.510453 --> 1.509396).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.291067123413086
Epoch: 36, Steps: 60 | Train Loss: 0.4796295 Vali Loss: 1.5103483 Test Loss: 0.4573270
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.261078119277954
Epoch: 37, Steps: 60 | Train Loss: 0.4791974 Vali Loss: 1.5185101 Test Loss: 0.4569532
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.3858952522277832
Epoch: 38, Steps: 60 | Train Loss: 0.4790269 Vali Loss: 1.5124562 Test Loss: 0.4566202
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.447192907333374
Epoch: 39, Steps: 60 | Train Loss: 0.4786286 Vali Loss: 1.5201976 Test Loss: 0.4562877
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.3543004989624023
Epoch: 40, Steps: 60 | Train Loss: 0.4783975 Vali Loss: 1.5127599 Test Loss: 0.4559898
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.1978144645690918
Epoch: 41, Steps: 60 | Train Loss: 0.4786294 Vali Loss: 1.5182114 Test Loss: 0.4557014
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.1473729610443115
Epoch: 42, Steps: 60 | Train Loss: 0.4782249 Vali Loss: 1.5156671 Test Loss: 0.4554507
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.2706589698791504
Epoch: 43, Steps: 60 | Train Loss: 0.4780320 Vali Loss: 1.5113292 Test Loss: 0.4551943
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.3420088291168213
Epoch: 44, Steps: 60 | Train Loss: 0.4777311 Vali Loss: 1.5118089 Test Loss: 0.4549767
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.2285552024841309
Epoch: 45, Steps: 60 | Train Loss: 0.4778472 Vali Loss: 1.5101451 Test Loss: 0.4547397
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.3212790489196777
Epoch: 46, Steps: 60 | Train Loss: 0.4778414 Vali Loss: 1.5139341 Test Loss: 0.4545300
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.2685930728912354
Epoch: 47, Steps: 60 | Train Loss: 0.4777006 Vali Loss: 1.5126951 Test Loss: 0.4543422
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.2398996353149414
Epoch: 48, Steps: 60 | Train Loss: 0.4773984 Vali Loss: 1.5139313 Test Loss: 0.4541375
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.2063636779785156
Epoch: 49, Steps: 60 | Train Loss: 0.4772424 Vali Loss: 1.5143187 Test Loss: 0.4539612
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.2417457103729248
Epoch: 50, Steps: 60 | Train Loss: 0.4772202 Vali Loss: 1.5117183 Test Loss: 0.4537959
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3654961585998535
Epoch: 51, Steps: 60 | Train Loss: 0.4770347 Vali Loss: 1.5148728 Test Loss: 0.4536401
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3147554397583008
Epoch: 52, Steps: 60 | Train Loss: 0.4768326 Vali Loss: 1.5118780 Test Loss: 0.4534952
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.2225751876831055
Epoch: 53, Steps: 60 | Train Loss: 0.4766188 Vali Loss: 1.5071890 Test Loss: 0.4533475
Validation loss decreased (1.509396 --> 1.507189).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.3258342742919922
Epoch: 54, Steps: 60 | Train Loss: 0.4770342 Vali Loss: 1.5029995 Test Loss: 0.4532142
Validation loss decreased (1.507189 --> 1.503000).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.3253233432769775
Epoch: 55, Steps: 60 | Train Loss: 0.4766154 Vali Loss: 1.5137670 Test Loss: 0.4530863
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.2577128410339355
Epoch: 56, Steps: 60 | Train Loss: 0.4767084 Vali Loss: 1.5068269 Test Loss: 0.4529737
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.3098435401916504
Epoch: 57, Steps: 60 | Train Loss: 0.4764710 Vali Loss: 1.5079224 Test Loss: 0.4528608
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.3856596946716309
Epoch: 58, Steps: 60 | Train Loss: 0.4766766 Vali Loss: 1.5095057 Test Loss: 0.4527592
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.2818572521209717
Epoch: 59, Steps: 60 | Train Loss: 0.4761411 Vali Loss: 1.5119272 Test Loss: 0.4526516
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.2686359882354736
Epoch: 60, Steps: 60 | Train Loss: 0.4761974 Vali Loss: 1.5115188 Test Loss: 0.4525543
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.3590996265411377
Epoch: 61, Steps: 60 | Train Loss: 0.4761951 Vali Loss: 1.5157290 Test Loss: 0.4524648
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.3902223110198975
Epoch: 62, Steps: 60 | Train Loss: 0.4763073 Vali Loss: 1.5052199 Test Loss: 0.4523735
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.4355933666229248
Epoch: 63, Steps: 60 | Train Loss: 0.4765048 Vali Loss: 1.5043137 Test Loss: 0.4523017
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.315889835357666
Epoch: 64, Steps: 60 | Train Loss: 0.4760462 Vali Loss: 1.5032126 Test Loss: 0.4522265
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.4393084049224854
Epoch: 65, Steps: 60 | Train Loss: 0.4759482 Vali Loss: 1.5065055 Test Loss: 0.4521477
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.3506529331207275
Epoch: 66, Steps: 60 | Train Loss: 0.4758654 Vali Loss: 1.5134395 Test Loss: 0.4520902
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.3722734451293945
Epoch: 67, Steps: 60 | Train Loss: 0.4758490 Vali Loss: 1.5083828 Test Loss: 0.4520173
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.4005546569824219
Epoch: 68, Steps: 60 | Train Loss: 0.4762509 Vali Loss: 1.5044944 Test Loss: 0.4519599
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.3079049587249756
Epoch: 69, Steps: 60 | Train Loss: 0.4759258 Vali Loss: 1.5029919 Test Loss: 0.4519003
Validation loss decreased (1.503000 --> 1.502992).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5770421028137207
Epoch: 70, Steps: 60 | Train Loss: 0.4762583 Vali Loss: 1.5152063 Test Loss: 0.4518422
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.4377384185791016
Epoch: 71, Steps: 60 | Train Loss: 0.4755094 Vali Loss: 1.5051988 Test Loss: 0.4517880
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.4000589847564697
Epoch: 72, Steps: 60 | Train Loss: 0.4760089 Vali Loss: 1.5093713 Test Loss: 0.4517439
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.5731737613677979
Epoch: 73, Steps: 60 | Train Loss: 0.4757952 Vali Loss: 1.5084743 Test Loss: 0.4516893
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.5000770092010498
Epoch: 74, Steps: 60 | Train Loss: 0.4754278 Vali Loss: 1.5131109 Test Loss: 0.4516446
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.4740402698516846
Epoch: 75, Steps: 60 | Train Loss: 0.4754358 Vali Loss: 1.5117142 Test Loss: 0.4515999
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.484813928604126
Epoch: 76, Steps: 60 | Train Loss: 0.4758358 Vali Loss: 1.5130265 Test Loss: 0.4515638
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.4193222522735596
Epoch: 77, Steps: 60 | Train Loss: 0.4759133 Vali Loss: 1.5098696 Test Loss: 0.4515221
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.3404078483581543
Epoch: 78, Steps: 60 | Train Loss: 0.4760776 Vali Loss: 1.5088770 Test Loss: 0.4514900
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.40865159034729
Epoch: 79, Steps: 60 | Train Loss: 0.4755390 Vali Loss: 1.5121746 Test Loss: 0.4514502
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.2795076370239258
Epoch: 80, Steps: 60 | Train Loss: 0.4756383 Vali Loss: 1.5121403 Test Loss: 0.4514214
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.383124828338623
Epoch: 81, Steps: 60 | Train Loss: 0.4757526 Vali Loss: 1.5103233 Test Loss: 0.4513896
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.3780300617218018
Epoch: 82, Steps: 60 | Train Loss: 0.4754164 Vali Loss: 1.5129777 Test Loss: 0.4513620
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.315624475479126
Epoch: 83, Steps: 60 | Train Loss: 0.4756775 Vali Loss: 1.5131216 Test Loss: 0.4513323
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.4364137649536133
Epoch: 84, Steps: 60 | Train Loss: 0.4755992 Vali Loss: 1.5107994 Test Loss: 0.4513056
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.4851937294006348
Epoch: 85, Steps: 60 | Train Loss: 0.4758271 Vali Loss: 1.5096879 Test Loss: 0.4512794
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.3142194747924805
Epoch: 86, Steps: 60 | Train Loss: 0.4757401 Vali Loss: 1.5103910 Test Loss: 0.4512574
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.4377250671386719
Epoch: 87, Steps: 60 | Train Loss: 0.4756645 Vali Loss: 1.5047867 Test Loss: 0.4512356
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.3697822093963623
Epoch: 88, Steps: 60 | Train Loss: 0.4752869 Vali Loss: 1.5120171 Test Loss: 0.4512130
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.3664953708648682
Epoch: 89, Steps: 60 | Train Loss: 0.4758106 Vali Loss: 1.5057318 Test Loss: 0.4511895
EarlyStopping counter: 20 out of 20
Early stopping
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=58, out_features=290, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  15070720.0
params:  17110.0
Trainable parameters:  17110
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.3586702346801758
Epoch: 1, Steps: 60 | Train Loss: 0.5885102 Vali Loss: 1.5028461 Test Loss: 0.4467601
Validation loss decreased (inf --> 1.502846).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.2933030128479004
Epoch: 2, Steps: 60 | Train Loss: 0.5864659 Vali Loss: 1.5000600 Test Loss: 0.4438110
Validation loss decreased (1.502846 --> 1.500060).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.2494165897369385
Epoch: 3, Steps: 60 | Train Loss: 0.5851173 Vali Loss: 1.4963493 Test Loss: 0.4420299
Validation loss decreased (1.500060 --> 1.496349).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4122285842895508
Epoch: 4, Steps: 60 | Train Loss: 0.5844509 Vali Loss: 1.4939356 Test Loss: 0.4413517
Validation loss decreased (1.496349 --> 1.493936).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.4180963039398193
Epoch: 5, Steps: 60 | Train Loss: 0.5842429 Vali Loss: 1.4912324 Test Loss: 0.4411852
Validation loss decreased (1.493936 --> 1.491232).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.3532886505126953
Epoch: 6, Steps: 60 | Train Loss: 0.5833078 Vali Loss: 1.4948971 Test Loss: 0.4409205
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.4272041320800781
Epoch: 7, Steps: 60 | Train Loss: 0.5835243 Vali Loss: 1.4918144 Test Loss: 0.4408512
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.6949474811553955
Epoch: 8, Steps: 60 | Train Loss: 0.5833251 Vali Loss: 1.4939878 Test Loss: 0.4407823
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.058319568634033
Epoch: 9, Steps: 60 | Train Loss: 0.5824888 Vali Loss: 1.4874980 Test Loss: 0.4409727
Validation loss decreased (1.491232 --> 1.487498).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.855668783187866
Epoch: 10, Steps: 60 | Train Loss: 0.5834284 Vali Loss: 1.4974453 Test Loss: 0.4409101
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.126645803451538
Epoch: 11, Steps: 60 | Train Loss: 0.5829903 Vali Loss: 1.5002735 Test Loss: 0.4410191
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.0072736740112305
Epoch: 12, Steps: 60 | Train Loss: 0.5830871 Vali Loss: 1.4893076 Test Loss: 0.4409434
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.2660577297210693
Epoch: 13, Steps: 60 | Train Loss: 0.5830662 Vali Loss: 1.4943897 Test Loss: 0.4411637
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.8248910903930664
Epoch: 14, Steps: 60 | Train Loss: 0.5828225 Vali Loss: 1.4919553 Test Loss: 0.4411896
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.795999526977539
Epoch: 15, Steps: 60 | Train Loss: 0.5828729 Vali Loss: 1.4912529 Test Loss: 0.4411376
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.0743000507354736
Epoch: 16, Steps: 60 | Train Loss: 0.5829030 Vali Loss: 1.4883105 Test Loss: 0.4413125
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.7836828231811523
Epoch: 17, Steps: 60 | Train Loss: 0.5832109 Vali Loss: 1.4903260 Test Loss: 0.4412611
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8396031856536865
Epoch: 18, Steps: 60 | Train Loss: 0.5828501 Vali Loss: 1.4903021 Test Loss: 0.4411526
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.8493707180023193
Epoch: 19, Steps: 60 | Train Loss: 0.5826010 Vali Loss: 1.4857025 Test Loss: 0.4413435
Validation loss decreased (1.487498 --> 1.485703).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8925495147705078
Epoch: 20, Steps: 60 | Train Loss: 0.5825326 Vali Loss: 1.4919393 Test Loss: 0.4413464
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.56087064743042
Epoch: 21, Steps: 60 | Train Loss: 0.5829971 Vali Loss: 1.4821484 Test Loss: 0.4413107
Validation loss decreased (1.485703 --> 1.482148).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.9797606468200684
Epoch: 22, Steps: 60 | Train Loss: 0.5833982 Vali Loss: 1.4981449 Test Loss: 0.4413931
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.6479225158691406
Epoch: 23, Steps: 60 | Train Loss: 0.5831016 Vali Loss: 1.4903140 Test Loss: 0.4415239
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.1421797275543213
Epoch: 24, Steps: 60 | Train Loss: 0.5827606 Vali Loss: 1.4952323 Test Loss: 0.4414494
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.260026454925537
Epoch: 25, Steps: 60 | Train Loss: 0.5829234 Vali Loss: 1.4930096 Test Loss: 0.4414057
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.920623540878296
Epoch: 26, Steps: 60 | Train Loss: 0.5826050 Vali Loss: 1.4882317 Test Loss: 0.4414384
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.395664691925049
Epoch: 27, Steps: 60 | Train Loss: 0.5826849 Vali Loss: 1.4920442 Test Loss: 0.4415030
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.1547420024871826
Epoch: 28, Steps: 60 | Train Loss: 0.5823306 Vali Loss: 1.4957895 Test Loss: 0.4415214
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.0435802936553955
Epoch: 29, Steps: 60 | Train Loss: 0.5822030 Vali Loss: 1.4911174 Test Loss: 0.4415541
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.2359697818756104
Epoch: 30, Steps: 60 | Train Loss: 0.5824728 Vali Loss: 1.4909316 Test Loss: 0.4415603
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.182234525680542
Epoch: 31, Steps: 60 | Train Loss: 0.5826158 Vali Loss: 1.4891711 Test Loss: 0.4415171
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.185594081878662
Epoch: 32, Steps: 60 | Train Loss: 0.5824010 Vali Loss: 1.4864275 Test Loss: 0.4415530
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.1290180683135986
Epoch: 33, Steps: 60 | Train Loss: 0.5821669 Vali Loss: 1.4893216 Test Loss: 0.4415686
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.4871177673339844
Epoch: 34, Steps: 60 | Train Loss: 0.5825407 Vali Loss: 1.4929473 Test Loss: 0.4415517
EarlyStopping counter: 13 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.4409923553466797
Epoch: 35, Steps: 60 | Train Loss: 0.5827529 Vali Loss: 1.4841502 Test Loss: 0.4416242
EarlyStopping counter: 14 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3031120300292969
Epoch: 36, Steps: 60 | Train Loss: 0.5821928 Vali Loss: 1.4835804 Test Loss: 0.4416384
EarlyStopping counter: 15 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.4377496242523193
Epoch: 37, Steps: 60 | Train Loss: 0.5826185 Vali Loss: 1.4835954 Test Loss: 0.4416281
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4492974281311035
Epoch: 38, Steps: 60 | Train Loss: 0.5823747 Vali Loss: 1.4929473 Test Loss: 0.4416553
EarlyStopping counter: 17 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.4727587699890137
Epoch: 39, Steps: 60 | Train Loss: 0.5825271 Vali Loss: 1.4899099 Test Loss: 0.4416842
EarlyStopping counter: 18 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.397782802581787
Epoch: 40, Steps: 60 | Train Loss: 0.5818303 Vali Loss: 1.4891002 Test Loss: 0.4417057
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.2468860149383545
Epoch: 41, Steps: 60 | Train Loss: 0.5824623 Vali Loss: 1.4928278 Test Loss: 0.4416804
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_180_720_FITS_ETTh1_ftM_sl180_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4401760995388031, mae:0.45009323954582214, rse:0.6351338028907776, corr:[0.2283089  0.23557146 0.23429239 0.2346373  0.23275262 0.22945938
 0.2297243  0.23132415 0.23100045 0.2314548  0.23075284 0.23050426
 0.23136535 0.23071699 0.23008478 0.23012738 0.22983006 0.22985324
 0.22987054 0.22930557 0.2291277  0.22954533 0.23005046 0.23029514
 0.2299653  0.22920603 0.22833547 0.2279507  0.22747019 0.22706422
 0.22692601 0.22649373 0.22578533 0.2260043  0.22628148 0.22620593
 0.22654377 0.22666864 0.22651874 0.22648297 0.22660032 0.22657667
 0.22681145 0.22721775 0.22731946 0.2274299  0.22825406 0.22889574
 0.22827299 0.22687246 0.22507101 0.22414282 0.22335584 0.22192605
 0.22133406 0.22155783 0.22125003 0.22149107 0.22163193 0.22183125
 0.22211324 0.22214264 0.22180888 0.22156289 0.22167028 0.22169508
 0.22178836 0.2217986  0.2215204  0.22130422 0.22130983 0.22091836
 0.22006533 0.21930118 0.21809295 0.21762091 0.21776155 0.21761869
 0.21761478 0.21765979 0.21724355 0.2170868  0.21679077 0.21674994
 0.21686952 0.21669054 0.21644796 0.2161416  0.21598572 0.21588476
 0.21569137 0.21571395 0.21575604 0.21591443 0.2167127  0.21760055
 0.21759616 0.21748471 0.21739592 0.21696092 0.21663512 0.21640788
 0.21622716 0.21651325 0.21640827 0.21642455 0.21621536 0.21640009
 0.21696459 0.21689205 0.21657732 0.2165842  0.2167706  0.21663983
 0.21648005 0.216518   0.21662155 0.21652082 0.21648347 0.21646106
 0.21600658 0.2150099  0.2135339  0.21305892 0.21275729 0.2121852
 0.2120892  0.21254154 0.21240157 0.21250309 0.21263263 0.2131429
 0.21380487 0.21372965 0.21348538 0.21347626 0.21353824 0.21343844
 0.21343969 0.213392   0.21323794 0.2135101  0.21399972 0.21394052
 0.21332946 0.21270351 0.21193431 0.21095794 0.21021734 0.20966993
 0.20955531 0.20965296 0.20980024 0.2101912  0.21036264 0.21061483
 0.21097872 0.21085054 0.21062094 0.21041507 0.21052735 0.2104364
 0.21013159 0.21023908 0.21051748 0.2106201  0.21078531 0.21095422
 0.21061456 0.21022443 0.20979176 0.2096293  0.20948426 0.20915927
 0.20931318 0.20991856 0.21001913 0.21051174 0.21102022 0.21139024
 0.21186298 0.21194972 0.21172449 0.2114937  0.2115106  0.21164739
 0.21170323 0.21183582 0.21218649 0.21246426 0.21251766 0.21224236
 0.21162759 0.21106704 0.20988521 0.20873657 0.20802766 0.20752561
 0.20737302 0.20761156 0.20783114 0.20813146 0.20808935 0.20854086
 0.20920078 0.20933862 0.20904802 0.20879576 0.20876026 0.20866832
 0.20850416 0.20847058 0.20839092 0.20838268 0.20859511 0.20858419
 0.20804748 0.2075588  0.20694584 0.20653896 0.20635189 0.20586151
 0.20577405 0.20593858 0.20590389 0.2059874  0.20580377 0.20582208
 0.20636219 0.20644026 0.20633088 0.20619732 0.20590305 0.20562124
 0.20551284 0.2053534  0.20509748 0.20504154 0.20531434 0.20533554
 0.20492333 0.2044099  0.20371608 0.20357855 0.20385976 0.20374547
 0.20360467 0.20370607 0.20378748 0.20408954 0.20439489 0.20493603
 0.20568494 0.20609692 0.20624433 0.20614412 0.206031   0.20597093
 0.20595656 0.20579791 0.20557846 0.20544565 0.20563565 0.20564893
 0.20513384 0.20472533 0.20424718 0.20393525 0.20362517 0.2031787
 0.20323451 0.20371172 0.2036764  0.20371212 0.20373891 0.20380065
 0.20421909 0.20442706 0.20445451 0.20427206 0.20395964 0.2036564
 0.2035455  0.20378526 0.2040265  0.2042452  0.2045616  0.20480724
 0.2046902  0.2045993  0.20459211 0.20465028 0.2046799  0.20479584
 0.20513652 0.2054561  0.20553905 0.20590033 0.20619243 0.20643857
 0.2067737  0.20688604 0.20701705 0.2070682  0.20694791 0.20671621
 0.20671417 0.20685177 0.20672022 0.20660537 0.20681041 0.2069801
 0.20676737 0.2065253  0.20609182 0.20565853 0.20544474 0.2049608
 0.20490116 0.20513599 0.2048832  0.20474918 0.20468505 0.2048945
 0.20560145 0.20592132 0.20592956 0.20606428 0.20632145 0.20627567
 0.20614544 0.20624761 0.20636776 0.20644061 0.20634401 0.20601812
 0.20556627 0.20528468 0.20478392 0.20437045 0.20400096 0.20365097
 0.20358962 0.20365268 0.20372432 0.20399591 0.20437711 0.20469525
 0.20480834 0.2049348  0.20535009 0.20575622 0.20573476 0.20547011
 0.20533961 0.20539577 0.20522875 0.20531921 0.20541991 0.20518161
 0.20462154 0.2043338  0.20405619 0.20355588 0.2029038  0.20258975
 0.20296274 0.20307554 0.20270337 0.20242956 0.2020326  0.2017933
 0.20209916 0.20207937 0.20187527 0.20171279 0.2017718  0.20184915
 0.20188388 0.20210382 0.20223524 0.20236084 0.20266666 0.20351815
 0.20415115 0.20434277 0.20433487 0.20449314 0.20420498 0.20318218
 0.20292303 0.2031566  0.20252901 0.20222028 0.20242502 0.20302148
 0.20376451 0.20414346 0.20430163 0.20406908 0.20401004 0.20442876
 0.20482242 0.20520517 0.20540442 0.20553452 0.20572963 0.20604314
 0.20611547 0.20605792 0.2059267  0.20586306 0.20580928 0.20535396
 0.20526022 0.20565411 0.20581527 0.20576198 0.2057611  0.20619632
 0.2069304  0.20707005 0.20671937 0.20648395 0.20636609 0.20617697
 0.2062891  0.20701869 0.20726812 0.20749705 0.20816721 0.20878941
 0.20866962 0.20842242 0.20819427 0.2081134  0.20804639 0.20755985
 0.20744993 0.2079229  0.20809823 0.20831819 0.20833996 0.20848314
 0.20881216 0.208893   0.20891836 0.20882064 0.20864925 0.20859295
 0.20854235 0.20852295 0.20862076 0.2090608  0.20970352 0.21032926
 0.2105162  0.21050991 0.21002515 0.20948869 0.20910749 0.2085512
 0.20835447 0.20839077 0.20827472 0.20860955 0.20931415 0.2099967
 0.21047407 0.21058132 0.21061634 0.21066007 0.2108159  0.21083409
 0.21078473 0.21104711 0.21119533 0.21102138 0.21125183 0.211559
 0.21117054 0.21053094 0.20984386 0.20938341 0.20906168 0.2087561
 0.20898356 0.20916192 0.20908596 0.20948026 0.20954593 0.20954336
 0.21010637 0.21037327 0.2103478  0.21027099 0.21015789 0.2100913
 0.21024352 0.2104878  0.21061867 0.21111982 0.21200441 0.21271378
 0.21278171 0.21263075 0.21235727 0.2123482  0.21240443 0.21182269
 0.21127047 0.21101046 0.21094045 0.21105658 0.21102124 0.2110387
 0.21155101 0.21200606 0.21230078 0.21249172 0.21275039 0.21289921
 0.21306829 0.21341395 0.21355578 0.21391071 0.21477243 0.21520168
 0.21460359 0.21413496 0.21381032 0.21349882 0.21300933 0.21226805
 0.2116694  0.21150655 0.21146743 0.21170272 0.21179254 0.21209562
 0.21257001 0.21267307 0.21285717 0.21310009 0.21328552 0.21332413
 0.21329097 0.21338862 0.21346562 0.21331906 0.21324556 0.21293382
 0.21190143 0.21109758 0.21064061 0.21009181 0.20920946 0.20835756
 0.20802838 0.20779386 0.20785199 0.2083452  0.20839337 0.20859781
 0.20933801 0.20929708 0.20922501 0.20908853 0.20877212 0.20855321
 0.20858438 0.20848316 0.20836088 0.20833111 0.20860028 0.20862712
 0.20783351 0.20685573 0.2057913  0.20519546 0.20487066 0.20388441
 0.20333542 0.20330824 0.20341688 0.20359652 0.20347966 0.20366363
 0.20384306 0.20335737 0.20285581 0.20257725 0.20224659 0.20199054
 0.2020109  0.2019999  0.20179163 0.2017824  0.20184061 0.20145813
 0.20055147 0.19963305 0.19859181 0.19763048 0.19690979 0.1962334
 0.19590482 0.1959377  0.1958021  0.19565445 0.19566776 0.19595091
 0.19640262 0.19615869 0.19586137 0.1958038  0.19558722 0.19516006
 0.19492091 0.19493833 0.19468275 0.19455035 0.19491267 0.19478896
 0.19378753 0.19287735 0.19202763 0.1911339  0.19063114 0.19008665
 0.18987964 0.18986467 0.18963932 0.1896863  0.18973474 0.19017856
 0.19088812 0.19082315 0.1906368  0.1904678  0.18998525 0.18968967
 0.18978742 0.18995868 0.18987195 0.18989702 0.19006363 0.18995504
 0.18911229 0.18814914 0.1870306  0.1861856  0.18578511 0.18562877
 0.18570757 0.18536861 0.18503857 0.18526956 0.18560563 0.18590637
 0.18627712 0.1861592  0.18612902 0.1861044  0.18584228 0.18558489
 0.18577845 0.18600404 0.1858704  0.18584949 0.18591319 0.18552606
 0.18431902 0.1830048  0.1817095  0.18050033 0.17957875 0.17843607
 0.17822869 0.178542   0.1786542  0.17870769 0.17901522 0.17986093
 0.18084979 0.18073075 0.18009026 0.1796944  0.17976049 0.17975293
 0.1795231  0.17957143 0.1797877  0.18008418 0.18043056 0.18031
 0.1795672  0.17909823 0.17846116 0.17768398 0.17685913 0.17606485
 0.17581606 0.17593881 0.17589146 0.17555963 0.17536403 0.17561503
 0.1757872  0.17495587 0.17456773 0.17412587 0.17361799 0.17371172
 0.17381172 0.17342837 0.17387052 0.17380492 0.17282446 0.17350683]
