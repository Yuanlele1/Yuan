Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=106, out_features=318, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30202368.0
params:  34026.0
Trainable parameters:  34026
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.3992652893066406
Epoch: 1, Steps: 59 | Train Loss: 0.9589105 Vali Loss: 1.9185230 Test Loss: 0.7594299
Validation loss decreased (inf --> 1.918523).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.368431568145752
Epoch: 2, Steps: 59 | Train Loss: 0.7348416 Vali Loss: 1.7043655 Test Loss: 0.5996220
Validation loss decreased (1.918523 --> 1.704365).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.3820128440856934
Epoch: 3, Steps: 59 | Train Loss: 0.6678845 Vali Loss: 1.6251980 Test Loss: 0.5434414
Validation loss decreased (1.704365 --> 1.625198).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.4100582599639893
Epoch: 4, Steps: 59 | Train Loss: 0.6401762 Vali Loss: 1.5845764 Test Loss: 0.5137089
Validation loss decreased (1.625198 --> 1.584576).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.3714780807495117
Epoch: 5, Steps: 59 | Train Loss: 0.6242144 Vali Loss: 1.5543312 Test Loss: 0.4933959
Validation loss decreased (1.584576 --> 1.554331).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.482290267944336
Epoch: 6, Steps: 59 | Train Loss: 0.6127074 Vali Loss: 1.5327283 Test Loss: 0.4782152
Validation loss decreased (1.554331 --> 1.532728).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.520568609237671
Epoch: 7, Steps: 59 | Train Loss: 0.6041169 Vali Loss: 1.5148427 Test Loss: 0.4664600
Validation loss decreased (1.532728 --> 1.514843).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.4473004341125488
Epoch: 8, Steps: 59 | Train Loss: 0.5973166 Vali Loss: 1.5047231 Test Loss: 0.4573565
Validation loss decreased (1.514843 --> 1.504723).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.483773946762085
Epoch: 9, Steps: 59 | Train Loss: 0.5918708 Vali Loss: 1.4889054 Test Loss: 0.4500861
Validation loss decreased (1.504723 --> 1.488905).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.4369428157806396
Epoch: 10, Steps: 59 | Train Loss: 0.5875313 Vali Loss: 1.4870604 Test Loss: 0.4445024
Validation loss decreased (1.488905 --> 1.487060).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.4134609699249268
Epoch: 11, Steps: 59 | Train Loss: 0.5843528 Vali Loss: 1.4778932 Test Loss: 0.4401385
Validation loss decreased (1.487060 --> 1.477893).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4520783424377441
Epoch: 12, Steps: 59 | Train Loss: 0.5815964 Vali Loss: 1.4727349 Test Loss: 0.4366758
Validation loss decreased (1.477893 --> 1.472735).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.3830337524414062
Epoch: 13, Steps: 59 | Train Loss: 0.5794603 Vali Loss: 1.4646273 Test Loss: 0.4340324
Validation loss decreased (1.472735 --> 1.464627).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.3981366157531738
Epoch: 14, Steps: 59 | Train Loss: 0.5775589 Vali Loss: 1.4625251 Test Loss: 0.4319274
Validation loss decreased (1.464627 --> 1.462525).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.3241181373596191
Epoch: 15, Steps: 59 | Train Loss: 0.5760502 Vali Loss: 1.4564241 Test Loss: 0.4302011
Validation loss decreased (1.462525 --> 1.456424).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.4078459739685059
Epoch: 16, Steps: 59 | Train Loss: 0.5749821 Vali Loss: 1.4549341 Test Loss: 0.4290031
Validation loss decreased (1.456424 --> 1.454934).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.8437774181365967
Epoch: 17, Steps: 59 | Train Loss: 0.5738388 Vali Loss: 1.4510040 Test Loss: 0.4280063
Validation loss decreased (1.454934 --> 1.451004).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.244128704071045
Epoch: 18, Steps: 59 | Train Loss: 0.5731110 Vali Loss: 1.4522754 Test Loss: 0.4272722
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.4163086414337158
Epoch: 19, Steps: 59 | Train Loss: 0.5724276 Vali Loss: 1.4521729 Test Loss: 0.4266964
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.4542577266693115
Epoch: 20, Steps: 59 | Train Loss: 0.5717039 Vali Loss: 1.4494905 Test Loss: 0.4262467
Validation loss decreased (1.451004 --> 1.449491).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4843602180480957
Epoch: 21, Steps: 59 | Train Loss: 0.5712885 Vali Loss: 1.4459569 Test Loss: 0.4258775
Validation loss decreased (1.449491 --> 1.445957).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.4165112972259521
Epoch: 22, Steps: 59 | Train Loss: 0.5711206 Vali Loss: 1.4410090 Test Loss: 0.4256172
Validation loss decreased (1.445957 --> 1.441009).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.476304054260254
Epoch: 23, Steps: 59 | Train Loss: 0.5705490 Vali Loss: 1.4461267 Test Loss: 0.4254735
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.3570492267608643
Epoch: 24, Steps: 59 | Train Loss: 0.5704260 Vali Loss: 1.4447938 Test Loss: 0.4253380
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.4892501831054688
Epoch: 25, Steps: 59 | Train Loss: 0.5701219 Vali Loss: 1.4400272 Test Loss: 0.4252198
Validation loss decreased (1.441009 --> 1.440027).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.4104351997375488
Epoch: 26, Steps: 59 | Train Loss: 0.5698994 Vali Loss: 1.4467514 Test Loss: 0.4251782
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.387770652770996
Epoch: 27, Steps: 59 | Train Loss: 0.5697317 Vali Loss: 1.4411364 Test Loss: 0.4251598
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.405036211013794
Epoch: 28, Steps: 59 | Train Loss: 0.5693988 Vali Loss: 1.4369717 Test Loss: 0.4251456
Validation loss decreased (1.440027 --> 1.436972).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.3776359558105469
Epoch: 29, Steps: 59 | Train Loss: 0.5692577 Vali Loss: 1.4434760 Test Loss: 0.4251074
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.4243810176849365
Epoch: 30, Steps: 59 | Train Loss: 0.5692588 Vali Loss: 1.4414928 Test Loss: 0.4250882
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.3954389095306396
Epoch: 31, Steps: 59 | Train Loss: 0.5690496 Vali Loss: 1.4419397 Test Loss: 0.4251301
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.420670509338379
Epoch: 32, Steps: 59 | Train Loss: 0.5689753 Vali Loss: 1.4412961 Test Loss: 0.4251202
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4309334754943848
Epoch: 33, Steps: 59 | Train Loss: 0.5688464 Vali Loss: 1.4385097 Test Loss: 0.4251744
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5008556842803955
Epoch: 34, Steps: 59 | Train Loss: 0.5688216 Vali Loss: 1.4417598 Test Loss: 0.4251884
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.6900913715362549
Epoch: 35, Steps: 59 | Train Loss: 0.5686382 Vali Loss: 1.4352986 Test Loss: 0.4252026
Validation loss decreased (1.436972 --> 1.435299).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3805441856384277
Epoch: 36, Steps: 59 | Train Loss: 0.5688045 Vali Loss: 1.4406266 Test Loss: 0.4252520
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.445539951324463
Epoch: 37, Steps: 59 | Train Loss: 0.5687459 Vali Loss: 1.4372208 Test Loss: 0.4252703
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4566855430603027
Epoch: 38, Steps: 59 | Train Loss: 0.5686249 Vali Loss: 1.4350280 Test Loss: 0.4252971
Validation loss decreased (1.435299 --> 1.435028).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.4204633235931396
Epoch: 39, Steps: 59 | Train Loss: 0.5683313 Vali Loss: 1.4371319 Test Loss: 0.4252886
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.3359127044677734
Epoch: 40, Steps: 59 | Train Loss: 0.5683888 Vali Loss: 1.4379264 Test Loss: 0.4253467
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.4954414367675781
Epoch: 41, Steps: 59 | Train Loss: 0.5683286 Vali Loss: 1.4424930 Test Loss: 0.4253769
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.4155123233795166
Epoch: 42, Steps: 59 | Train Loss: 0.5684190 Vali Loss: 1.4371778 Test Loss: 0.4253958
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.389155626296997
Epoch: 43, Steps: 59 | Train Loss: 0.5685271 Vali Loss: 1.4379946 Test Loss: 0.4254393
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.4544131755828857
Epoch: 44, Steps: 59 | Train Loss: 0.5683180 Vali Loss: 1.4336407 Test Loss: 0.4254573
Validation loss decreased (1.435028 --> 1.433641).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.4862279891967773
Epoch: 45, Steps: 59 | Train Loss: 0.5681704 Vali Loss: 1.4395804 Test Loss: 0.4254647
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.35111665725708
Epoch: 46, Steps: 59 | Train Loss: 0.5682659 Vali Loss: 1.4346668 Test Loss: 0.4254985
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.486252784729004
Epoch: 47, Steps: 59 | Train Loss: 0.5683264 Vali Loss: 1.4368643 Test Loss: 0.4255255
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.4191806316375732
Epoch: 48, Steps: 59 | Train Loss: 0.5681564 Vali Loss: 1.4336910 Test Loss: 0.4255350
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.384101152420044
Epoch: 49, Steps: 59 | Train Loss: 0.5682596 Vali Loss: 1.4375813 Test Loss: 0.4255451
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.351393699645996
Epoch: 50, Steps: 59 | Train Loss: 0.5683371 Vali Loss: 1.4373171 Test Loss: 0.4255784
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.3474736213684082
Epoch: 51, Steps: 59 | Train Loss: 0.5683018 Vali Loss: 1.4429057 Test Loss: 0.4255948
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.37725830078125
Epoch: 52, Steps: 59 | Train Loss: 0.5680834 Vali Loss: 1.4307261 Test Loss: 0.4256016
Validation loss decreased (1.433641 --> 1.430726).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.385300874710083
Epoch: 53, Steps: 59 | Train Loss: 0.5678935 Vali Loss: 1.4361916 Test Loss: 0.4256184
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.3176279067993164
Epoch: 54, Steps: 59 | Train Loss: 0.5681745 Vali Loss: 1.4392935 Test Loss: 0.4256301
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.4350287914276123
Epoch: 55, Steps: 59 | Train Loss: 0.5679834 Vali Loss: 1.4354565 Test Loss: 0.4256506
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.3990602493286133
Epoch: 56, Steps: 59 | Train Loss: 0.5680315 Vali Loss: 1.4385660 Test Loss: 0.4256671
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.4147450923919678
Epoch: 57, Steps: 59 | Train Loss: 0.5679598 Vali Loss: 1.4385278 Test Loss: 0.4256718
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.472289800643921
Epoch: 58, Steps: 59 | Train Loss: 0.5680044 Vali Loss: 1.4340919 Test Loss: 0.4256845
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.4691712856292725
Epoch: 59, Steps: 59 | Train Loss: 0.5681648 Vali Loss: 1.4457181 Test Loss: 0.4256915
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.492976188659668
Epoch: 60, Steps: 59 | Train Loss: 0.5680295 Vali Loss: 1.4400812 Test Loss: 0.4257047
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.320652723312378
Epoch: 61, Steps: 59 | Train Loss: 0.5681462 Vali Loss: 1.4335701 Test Loss: 0.4257171
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.4839649200439453
Epoch: 62, Steps: 59 | Train Loss: 0.5681082 Vali Loss: 1.4308912 Test Loss: 0.4257303
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.5006272792816162
Epoch: 63, Steps: 59 | Train Loss: 0.5680817 Vali Loss: 1.4335064 Test Loss: 0.4257338
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.3279590606689453
Epoch: 64, Steps: 59 | Train Loss: 0.5679755 Vali Loss: 1.4308449 Test Loss: 0.4257514
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.409348964691162
Epoch: 65, Steps: 59 | Train Loss: 0.5680131 Vali Loss: 1.4353280 Test Loss: 0.4257605
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.4216687679290771
Epoch: 66, Steps: 59 | Train Loss: 0.5680877 Vali Loss: 1.4352341 Test Loss: 0.4257677
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.44809889793396
Epoch: 67, Steps: 59 | Train Loss: 0.5679954 Vali Loss: 1.4342344 Test Loss: 0.4257698
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.382434368133545
Epoch: 68, Steps: 59 | Train Loss: 0.5679415 Vali Loss: 1.4351721 Test Loss: 0.4257757
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.4542925357818604
Epoch: 69, Steps: 59 | Train Loss: 0.5679813 Vali Loss: 1.4302156 Test Loss: 0.4257859
Validation loss decreased (1.430726 --> 1.430216).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.2882940769195557
Epoch: 70, Steps: 59 | Train Loss: 0.5679640 Vali Loss: 1.4337629 Test Loss: 0.4257866
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.421522855758667
Epoch: 71, Steps: 59 | Train Loss: 0.5679011 Vali Loss: 1.4367491 Test Loss: 0.4257992
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.3223445415496826
Epoch: 72, Steps: 59 | Train Loss: 0.5680438 Vali Loss: 1.4377035 Test Loss: 0.4258019
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.511195421218872
Epoch: 73, Steps: 59 | Train Loss: 0.5679277 Vali Loss: 1.4381058 Test Loss: 0.4258068
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.5044224262237549
Epoch: 74, Steps: 59 | Train Loss: 0.5678348 Vali Loss: 1.4396449 Test Loss: 0.4258116
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.2504498958587646
Epoch: 75, Steps: 59 | Train Loss: 0.5677513 Vali Loss: 1.4340423 Test Loss: 0.4258161
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.505807638168335
Epoch: 76, Steps: 59 | Train Loss: 0.5676866 Vali Loss: 1.4362437 Test Loss: 0.4258261
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.4534480571746826
Epoch: 77, Steps: 59 | Train Loss: 0.5677910 Vali Loss: 1.4382083 Test Loss: 0.4258288
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.2129242420196533
Epoch: 78, Steps: 59 | Train Loss: 0.5679530 Vali Loss: 1.4368865 Test Loss: 0.4258318
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.3786437511444092
Epoch: 79, Steps: 59 | Train Loss: 0.5680116 Vali Loss: 1.4395560 Test Loss: 0.4258364
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.3025610446929932
Epoch: 80, Steps: 59 | Train Loss: 0.5680089 Vali Loss: 1.4326811 Test Loss: 0.4258412
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.383962869644165
Epoch: 81, Steps: 59 | Train Loss: 0.5678989 Vali Loss: 1.4377578 Test Loss: 0.4258440
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.302910566329956
Epoch: 82, Steps: 59 | Train Loss: 0.5679167 Vali Loss: 1.4386225 Test Loss: 0.4258458
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.359057903289795
Epoch: 83, Steps: 59 | Train Loss: 0.5678953 Vali Loss: 1.4304689 Test Loss: 0.4258535
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.3254919052124023
Epoch: 84, Steps: 59 | Train Loss: 0.5678760 Vali Loss: 1.4324315 Test Loss: 0.4258555
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.5194225311279297
Epoch: 85, Steps: 59 | Train Loss: 0.5678304 Vali Loss: 1.4397993 Test Loss: 0.4258563
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.4866902828216553
Epoch: 86, Steps: 59 | Train Loss: 0.5676994 Vali Loss: 1.4419141 Test Loss: 0.4258631
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.4117469787597656
Epoch: 87, Steps: 59 | Train Loss: 0.5680019 Vali Loss: 1.4354014 Test Loss: 0.4258620
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.4414143562316895
Epoch: 88, Steps: 59 | Train Loss: 0.5679878 Vali Loss: 1.4395962 Test Loss: 0.4258657
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.4104931354522705
Epoch: 89, Steps: 59 | Train Loss: 0.5677241 Vali Loss: 1.4375055 Test Loss: 0.4258675
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_720_FITS_ETTh1_ftM_sl360_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4247494041919708, mae:0.44563350081443787, rse:0.623904824256897, corr:[0.22477056 0.23368639 0.23414485 0.23635858 0.23353069 0.23143628
 0.23272325 0.23284374 0.23185363 0.23273    0.23304208 0.23182943
 0.2316279  0.23206344 0.23146932 0.23096924 0.23119266 0.23102559
 0.23072335 0.23086788 0.23096848 0.23103173 0.2318181  0.23242934
 0.23201007 0.23192172 0.23223327 0.2319597  0.23167062 0.2318578
 0.23166364 0.23104355 0.23088016 0.2310268  0.23076402 0.23050083
 0.23075986 0.23079453 0.23040281 0.23032552 0.23057054 0.2305031
 0.23036355 0.2303648  0.23017655 0.23005754 0.2303219  0.23054546
 0.23042613 0.23011658 0.22974467 0.22942768 0.22913316 0.22855143
 0.22801076 0.22745347 0.2273532  0.22702326 0.22645739 0.22626613
 0.22618711 0.22589166 0.22552875 0.22556669 0.22582895 0.22602294
 0.22609499 0.22598177 0.22591792 0.22603138 0.22596042 0.22563861
 0.2251388  0.22464435 0.22425632 0.22408602 0.22405054 0.22389312
 0.2235523  0.223298   0.22321823 0.22286825 0.22240835 0.2223292
 0.22228071 0.2218931  0.22152755 0.22148998 0.22141978 0.22118582
 0.22097316 0.2208772  0.22084777 0.22099169 0.22135068 0.22223745
 0.22328173 0.22393794 0.22454187 0.22510105 0.22531293 0.22531101
 0.22527926 0.22532545 0.22519957 0.22487089 0.22452539 0.2243096
 0.22416289 0.2240088  0.22387654 0.22387002 0.22402151 0.22403345
 0.22387716 0.22371194 0.2236261  0.22343948 0.2231575  0.22320864
 0.2233479  0.22307725 0.22272818 0.22260559 0.22228189 0.22180997
 0.22163238 0.22161947 0.2212588  0.22074273 0.22047094 0.22032657
 0.22018155 0.2200269  0.21988453 0.21986438 0.21999134 0.22005397
 0.22002196 0.21999486 0.2199863  0.21988493 0.21951616 0.2193247
 0.21921237 0.2189421  0.21873324 0.21835117 0.21769206 0.21725902
 0.21730494 0.21737272 0.21729381 0.21736526 0.21743946 0.21735232
 0.21724683 0.2172513  0.21715473 0.21699956 0.2169711  0.21700512
 0.21684358 0.21668775 0.21660794 0.21636684 0.2158038  0.21589285
 0.21645527 0.21704908 0.21787341 0.21873912 0.21892895 0.21889469
 0.2191546  0.21933727 0.21917501 0.21902163 0.21905226 0.21897967
 0.21873516 0.21863613 0.21868274 0.21857509 0.21857876 0.21876588
 0.21877554 0.21875636 0.21886614 0.21870849 0.21831864 0.21818726
 0.2181267  0.21786228 0.21752055 0.21717402 0.21663614 0.21615998
 0.21606053 0.21608461 0.21585679 0.2156574  0.21562244 0.21551967
 0.21542637 0.21565516 0.21584712 0.21580678 0.21577303 0.21579926
 0.21570341 0.21538009 0.2150615  0.21479471 0.21444379 0.21441318
 0.21457762 0.21462423 0.21472295 0.21466035 0.21448913 0.21445107
 0.2144692  0.21447225 0.21437483 0.21407247 0.21362871 0.21326989
 0.21299092 0.2128569  0.21280482 0.2127788  0.21274509 0.21261686
 0.21242608 0.21212696 0.2118205  0.21155272 0.21145551 0.2115708
 0.2118716  0.21200627 0.21219015 0.21246202 0.21267909 0.21278869
 0.21290177 0.21303892 0.21312518 0.2129926  0.2127286  0.2124469
 0.21228369 0.2122963  0.21249917 0.2126006  0.21264835 0.21269679
 0.21273132 0.21259832 0.21240947 0.21233456 0.21227382 0.21222612
 0.21204731 0.21186468 0.21183874 0.21179794 0.21163003 0.211546
 0.21146482 0.21136783 0.21129526 0.21123064 0.21103147 0.210926
 0.2109193  0.21085039 0.2106666  0.21050252 0.21025726 0.2100766
 0.21007742 0.21012238 0.2101124  0.21016979 0.21037745 0.21090275
 0.21148618 0.21199147 0.21246646 0.21275602 0.21294017 0.21310394
 0.21320942 0.21331483 0.21339422 0.21330892 0.21306899 0.21298566
 0.21301778 0.21303868 0.21312056 0.21317331 0.21321563 0.2131927
 0.21318878 0.21311048 0.21301574 0.21298717 0.2128822  0.21313928
 0.21374983 0.21421191 0.21444738 0.21466364 0.2147072  0.21449463
 0.21423793 0.21427217 0.21429609 0.2138603  0.21323626 0.21308
 0.21310952 0.21302813 0.21302082 0.2132612  0.21340427 0.21349856
 0.21371259 0.21378085 0.21363123 0.21361093 0.213649   0.21348718
 0.21339482 0.21350467 0.21358337 0.21336557 0.21304666 0.21265127
 0.21237288 0.212343   0.21227996 0.21203287 0.21198352 0.21228345
 0.21246226 0.21239498 0.21243595 0.21267027 0.2126456  0.21263207
 0.21274072 0.2126331  0.21227288 0.21208447 0.21175937 0.21148065
 0.21152033 0.21178207 0.21191618 0.21170245 0.21135637 0.21099325
 0.21074766 0.21055776 0.21039228 0.21017545 0.20985278 0.20964994
 0.20959184 0.2093855  0.2090838  0.208754   0.20855162 0.20854387
 0.2085947  0.20860988 0.2085658  0.20851809 0.20850042 0.20918864
 0.21030553 0.21122667 0.21202894 0.21246769 0.21243472 0.21194884
 0.21150503 0.21111593 0.21080555 0.2103816  0.20993793 0.20981069
 0.20979047 0.20977016 0.20986272 0.2099745  0.21006002 0.21033242
 0.21063781 0.2108482  0.21098629 0.21131526 0.21165109 0.21205007
 0.21238752 0.21258208 0.21257764 0.21241947 0.21220201 0.21189544
 0.21164915 0.21167284 0.21161196 0.21135864 0.21120939 0.21128689
 0.21134226 0.21125127 0.21100146 0.2107538  0.21062623 0.210688
 0.2107641  0.21085031 0.21091032 0.2113782  0.21187758 0.21238525
 0.21288678 0.21320733 0.2133275  0.21336502 0.2133114  0.21311694
 0.21294583 0.21303758 0.21290548 0.21254863 0.21234973 0.21239567
 0.21242446 0.21223404 0.21204637 0.21194308 0.21191013 0.21196061
 0.21198756 0.2119615  0.21208385 0.21233657 0.21254179 0.21320146
 0.21400054 0.2144267  0.21455051 0.21453756 0.21419851 0.21382375
 0.21355997 0.21341968 0.21333756 0.21329562 0.2133192  0.21337536
 0.21344241 0.21343295 0.21343571 0.21351306 0.21359515 0.2136384
 0.21364874 0.2136743  0.21366489 0.21340643 0.21329516 0.21351624
 0.21373424 0.21364534 0.2134702  0.21346195 0.21337572 0.21316767
 0.21302089 0.21285278 0.2125897  0.21237803 0.21209773 0.21194942
 0.211923   0.21179615 0.21164018 0.21159595 0.21158426 0.21151988
 0.2114531  0.21147476 0.21164003 0.21186826 0.2122708  0.21323839
 0.21423616 0.21482034 0.2152789  0.21566889 0.2156696  0.21524729
 0.21472271 0.21432225 0.21396142 0.2134749  0.21302374 0.21293436
 0.21305005 0.21307687 0.21320666 0.21352763 0.21383609 0.21390583
 0.21398145 0.2142621  0.21445277 0.21452661 0.21492018 0.21543781
 0.21541013 0.21510166 0.21522993 0.21544513 0.2149583  0.2142367
 0.21379527 0.2135689  0.21323268 0.21297342 0.21274091 0.21265426
 0.21256556 0.21266614 0.2128818  0.21323852 0.21358687 0.21367688
 0.21355833 0.2135287  0.21350043 0.21336019 0.21310513 0.2130715
 0.2128935  0.21259107 0.21256815 0.21232662 0.21160252 0.21103886
 0.21064258 0.21013357 0.20962179 0.2090688  0.20861901 0.2083679
 0.20820722 0.20787582 0.20771791 0.2077049  0.20773813 0.20774545
 0.20790175 0.20802289 0.208034   0.20793672 0.20800866 0.20816179
 0.20810953 0.2080121  0.2078619  0.20776376 0.20754266 0.20707533
 0.2066637  0.20613745 0.20572601 0.20556654 0.20549363 0.205278
 0.20516218 0.20493203 0.2048414  0.20471483 0.20459585 0.2045926
 0.20470071 0.2047593  0.20470977 0.2045967  0.2045889  0.20462243
 0.20437793 0.20389901 0.20377809 0.20348147 0.20262586 0.20187147
 0.20158921 0.2013025  0.20066309 0.20028341 0.20015341 0.19994818
 0.19979021 0.1995373  0.19925226 0.19922318 0.19925475 0.19913198
 0.19903469 0.19912647 0.19932817 0.19950792 0.19978806 0.20025349
 0.20030023 0.19982313 0.19971217 0.19948547 0.19876623 0.19824398
 0.1982599  0.19793254 0.19721065 0.1969459  0.19682118 0.19663568
 0.19663405 0.19654733 0.19640806 0.19615762 0.19595999 0.19605196
 0.19608058 0.19606172 0.19617486 0.19638875 0.19680925 0.19746873
 0.19776723 0.19758396 0.1972646  0.1971845  0.19666201 0.19646503
 0.19630642 0.19573109 0.19505922 0.19484496 0.1946968  0.19426095
 0.19403024 0.19401798 0.19382459 0.19342025 0.19363509 0.19394472
 0.19400607 0.1939525  0.19406246 0.19415994 0.19427846 0.19449621
 0.19409959 0.19318171 0.19253448 0.19182378 0.1909847  0.1901646
 0.1894865  0.18892317 0.18869741 0.18858123 0.18833847 0.18818747
 0.18852551 0.18851835 0.18811642 0.18811636 0.18843454 0.18878122
 0.18889517 0.18912432 0.18956906 0.19001195 0.19025585 0.19038844
 0.19049892 0.19029483 0.19010067 0.19002241 0.18967526 0.18892534
 0.18849191 0.1883732  0.18822338 0.18776508 0.18725091 0.18703409
 0.18703294 0.18664713 0.18635675 0.18566608 0.18570569 0.18597306
 0.18602502 0.1862366  0.18711789 0.18549947 0.18495052 0.18415137]
