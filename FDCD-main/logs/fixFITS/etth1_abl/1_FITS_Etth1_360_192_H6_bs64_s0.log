Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=0, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=106, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  15386112.0
params:  17334.0
Trainable parameters:  17334
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.582683801651001
Epoch: 1, Steps: 63 | Train Loss: 0.6993160 Vali Loss: 1.3743699 Test Loss: 0.6623852
Validation loss decreased (inf --> 1.374370).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.6295936107635498
Epoch: 2, Steps: 63 | Train Loss: 0.5336740 Vali Loss: 1.1932758 Test Loss: 0.5458431
Validation loss decreased (1.374370 --> 1.193276).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.6451334953308105
Epoch: 3, Steps: 63 | Train Loss: 0.4785599 Vali Loss: 1.1026243 Test Loss: 0.4900571
Validation loss decreased (1.193276 --> 1.102624).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.5857009887695312
Epoch: 4, Steps: 63 | Train Loss: 0.4486685 Vali Loss: 1.0464822 Test Loss: 0.4572689
Validation loss decreased (1.102624 --> 1.046482).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5021748542785645
Epoch: 5, Steps: 63 | Train Loss: 0.4302227 Vali Loss: 1.0109637 Test Loss: 0.4371035
Validation loss decreased (1.046482 --> 1.010964).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.606755018234253
Epoch: 6, Steps: 63 | Train Loss: 0.4184554 Vali Loss: 0.9873903 Test Loss: 0.4252045
Validation loss decreased (1.010964 --> 0.987390).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.9170360565185547
Epoch: 7, Steps: 63 | Train Loss: 0.4109720 Vali Loss: 0.9711526 Test Loss: 0.4180776
Validation loss decreased (0.987390 --> 0.971153).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8057746887207031
Epoch: 8, Steps: 63 | Train Loss: 0.4062322 Vali Loss: 0.9601232 Test Loss: 0.4138550
Validation loss decreased (0.971153 --> 0.960123).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.386054754257202
Epoch: 9, Steps: 63 | Train Loss: 0.4032326 Vali Loss: 0.9530409 Test Loss: 0.4117072
Validation loss decreased (0.960123 --> 0.953041).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.7838530540466309
Epoch: 10, Steps: 63 | Train Loss: 0.4010281 Vali Loss: 0.9476010 Test Loss: 0.4101780
Validation loss decreased (0.953041 --> 0.947601).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7265923023223877
Epoch: 11, Steps: 63 | Train Loss: 0.3995248 Vali Loss: 0.9431285 Test Loss: 0.4095817
Validation loss decreased (0.947601 --> 0.943128).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.4925460815429688
Epoch: 12, Steps: 63 | Train Loss: 0.3984920 Vali Loss: 0.9404538 Test Loss: 0.4092833
Validation loss decreased (0.943128 --> 0.940454).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.5549440383911133
Epoch: 13, Steps: 63 | Train Loss: 0.3977049 Vali Loss: 0.9375957 Test Loss: 0.4091608
Validation loss decreased (0.940454 --> 0.937596).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.626112699508667
Epoch: 14, Steps: 63 | Train Loss: 0.3972539 Vali Loss: 0.9361538 Test Loss: 0.4091082
Validation loss decreased (0.937596 --> 0.936154).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.5592482089996338
Epoch: 15, Steps: 63 | Train Loss: 0.3967048 Vali Loss: 0.9348413 Test Loss: 0.4090075
Validation loss decreased (0.936154 --> 0.934841).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7450919151306152
Epoch: 16, Steps: 63 | Train Loss: 0.3963266 Vali Loss: 0.9334099 Test Loss: 0.4090070
Validation loss decreased (0.934841 --> 0.933410).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.52059006690979
Epoch: 17, Steps: 63 | Train Loss: 0.3961658 Vali Loss: 0.9324235 Test Loss: 0.4087752
Validation loss decreased (0.933410 --> 0.932424).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6127681732177734
Epoch: 18, Steps: 63 | Train Loss: 0.3958721 Vali Loss: 0.9310790 Test Loss: 0.4087784
Validation loss decreased (0.932424 --> 0.931079).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.6513152122497559
Epoch: 19, Steps: 63 | Train Loss: 0.3957557 Vali Loss: 0.9305552 Test Loss: 0.4087943
Validation loss decreased (0.931079 --> 0.930555).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.6728551387786865
Epoch: 20, Steps: 63 | Train Loss: 0.3953900 Vali Loss: 0.9300886 Test Loss: 0.4088624
Validation loss decreased (0.930555 --> 0.930089).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.56719970703125
Epoch: 21, Steps: 63 | Train Loss: 0.3950336 Vali Loss: 0.9290776 Test Loss: 0.4086468
Validation loss decreased (0.930089 --> 0.929078).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.6026370525360107
Epoch: 22, Steps: 63 | Train Loss: 0.3952130 Vali Loss: 0.9280633 Test Loss: 0.4087612
Validation loss decreased (0.929078 --> 0.928063).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.5093309879302979
Epoch: 23, Steps: 63 | Train Loss: 0.3948655 Vali Loss: 0.9271781 Test Loss: 0.4086880
Validation loss decreased (0.928063 --> 0.927178).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.403449535369873
Epoch: 24, Steps: 63 | Train Loss: 0.3948113 Vali Loss: 0.9271223 Test Loss: 0.4085303
Validation loss decreased (0.927178 --> 0.927122).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.5013325214385986
Epoch: 25, Steps: 63 | Train Loss: 0.3946091 Vali Loss: 0.9265529 Test Loss: 0.4086740
Validation loss decreased (0.927122 --> 0.926553).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.5458757877349854
Epoch: 26, Steps: 63 | Train Loss: 0.3946699 Vali Loss: 0.9265001 Test Loss: 0.4086535
Validation loss decreased (0.926553 --> 0.926500).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.567775011062622
Epoch: 27, Steps: 63 | Train Loss: 0.3944345 Vali Loss: 0.9262252 Test Loss: 0.4085856
Validation loss decreased (0.926500 --> 0.926225).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.1459667682647705
Epoch: 28, Steps: 63 | Train Loss: 0.3942328 Vali Loss: 0.9256251 Test Loss: 0.4085755
Validation loss decreased (0.926225 --> 0.925625).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6388347148895264
Epoch: 29, Steps: 63 | Train Loss: 0.3942456 Vali Loss: 0.9250400 Test Loss: 0.4085154
Validation loss decreased (0.925625 --> 0.925040).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5675890445709229
Epoch: 30, Steps: 63 | Train Loss: 0.3941906 Vali Loss: 0.9250118 Test Loss: 0.4085490
Validation loss decreased (0.925040 --> 0.925012).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.4981372356414795
Epoch: 31, Steps: 63 | Train Loss: 0.3941357 Vali Loss: 0.9244931 Test Loss: 0.4085458
Validation loss decreased (0.925012 --> 0.924493).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.6209518909454346
Epoch: 32, Steps: 63 | Train Loss: 0.3940206 Vali Loss: 0.9245462 Test Loss: 0.4084704
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.5312840938568115
Epoch: 33, Steps: 63 | Train Loss: 0.3939167 Vali Loss: 0.9245300 Test Loss: 0.4085322
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5350089073181152
Epoch: 34, Steps: 63 | Train Loss: 0.3940871 Vali Loss: 0.9237163 Test Loss: 0.4085258
Validation loss decreased (0.924493 --> 0.923716).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.5697541236877441
Epoch: 35, Steps: 63 | Train Loss: 0.3939132 Vali Loss: 0.9234716 Test Loss: 0.4084781
Validation loss decreased (0.923716 --> 0.923472).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6283247470855713
Epoch: 36, Steps: 63 | Train Loss: 0.3939621 Vali Loss: 0.9238756 Test Loss: 0.4084983
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.5660436153411865
Epoch: 37, Steps: 63 | Train Loss: 0.3933003 Vali Loss: 0.9236382 Test Loss: 0.4084633
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.7035434246063232
Epoch: 38, Steps: 63 | Train Loss: 0.3937616 Vali Loss: 0.9233281 Test Loss: 0.4084606
Validation loss decreased (0.923472 --> 0.923328).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.5998773574829102
Epoch: 39, Steps: 63 | Train Loss: 0.3936416 Vali Loss: 0.9233890 Test Loss: 0.4084509
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.5307679176330566
Epoch: 40, Steps: 63 | Train Loss: 0.3935581 Vali Loss: 0.9233206 Test Loss: 0.4084483
Validation loss decreased (0.923328 --> 0.923321).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.543865442276001
Epoch: 41, Steps: 63 | Train Loss: 0.3937135 Vali Loss: 0.9231002 Test Loss: 0.4084513
Validation loss decreased (0.923321 --> 0.923100).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.4973201751708984
Epoch: 42, Steps: 63 | Train Loss: 0.3934427 Vali Loss: 0.9229643 Test Loss: 0.4084772
Validation loss decreased (0.923100 --> 0.922964).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9055888652801514
Epoch: 43, Steps: 63 | Train Loss: 0.3934583 Vali Loss: 0.9220611 Test Loss: 0.4084464
Validation loss decreased (0.922964 --> 0.922061).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.3912749290466309
Epoch: 44, Steps: 63 | Train Loss: 0.3934043 Vali Loss: 0.9226485 Test Loss: 0.4084549
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.442204236984253
Epoch: 45, Steps: 63 | Train Loss: 0.3934372 Vali Loss: 0.9222031 Test Loss: 0.4084475
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.4551291465759277
Epoch: 46, Steps: 63 | Train Loss: 0.3934367 Vali Loss: 0.9222236 Test Loss: 0.4084229
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.3056952953338623
Epoch: 47, Steps: 63 | Train Loss: 0.3934714 Vali Loss: 0.9225516 Test Loss: 0.4084561
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.466019868850708
Epoch: 48, Steps: 63 | Train Loss: 0.3933674 Vali Loss: 0.9220111 Test Loss: 0.4084276
Validation loss decreased (0.922061 --> 0.922011).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.4989924430847168
Epoch: 49, Steps: 63 | Train Loss: 0.3933483 Vali Loss: 0.9223861 Test Loss: 0.4084284
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3993778228759766
Epoch: 50, Steps: 63 | Train Loss: 0.3933573 Vali Loss: 0.9217839 Test Loss: 0.4084369
Validation loss decreased (0.922011 --> 0.921784).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.124403476715088
Epoch: 51, Steps: 63 | Train Loss: 0.3932124 Vali Loss: 0.9218176 Test Loss: 0.4084179
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.8200092315673828
Epoch: 52, Steps: 63 | Train Loss: 0.3934779 Vali Loss: 0.9221848 Test Loss: 0.4084001
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.849853515625
Epoch: 53, Steps: 63 | Train Loss: 0.3933420 Vali Loss: 0.9221646 Test Loss: 0.4084234
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.885450839996338
Epoch: 54, Steps: 63 | Train Loss: 0.3932187 Vali Loss: 0.9214895 Test Loss: 0.4084135
Validation loss decreased (0.921784 --> 0.921490).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.6918323040008545
Epoch: 55, Steps: 63 | Train Loss: 0.3936132 Vali Loss: 0.9218872 Test Loss: 0.4083892
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.9077763557434082
Epoch: 56, Steps: 63 | Train Loss: 0.3934543 Vali Loss: 0.9220148 Test Loss: 0.4084095
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.0085928440093994
Epoch: 57, Steps: 63 | Train Loss: 0.3932541 Vali Loss: 0.9217772 Test Loss: 0.4084153
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.5846543312072754
Epoch: 58, Steps: 63 | Train Loss: 0.3933874 Vali Loss: 0.9219111 Test Loss: 0.4084010
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.6761667728424072
Epoch: 59, Steps: 63 | Train Loss: 0.3931347 Vali Loss: 0.9214395 Test Loss: 0.4084173
Validation loss decreased (0.921490 --> 0.921439).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.723701000213623
Epoch: 60, Steps: 63 | Train Loss: 0.3931499 Vali Loss: 0.9216838 Test Loss: 0.4084184
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.5941345691680908
Epoch: 61, Steps: 63 | Train Loss: 0.3932452 Vali Loss: 0.9216390 Test Loss: 0.4084190
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.6204779148101807
Epoch: 62, Steps: 63 | Train Loss: 0.3933510 Vali Loss: 0.9211901 Test Loss: 0.4084065
Validation loss decreased (0.921439 --> 0.921190).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.6857657432556152
Epoch: 63, Steps: 63 | Train Loss: 0.3932584 Vali Loss: 0.9216607 Test Loss: 0.4084190
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.5277578830718994
Epoch: 64, Steps: 63 | Train Loss: 0.3932341 Vali Loss: 0.9212815 Test Loss: 0.4084142
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.6796669960021973
Epoch: 65, Steps: 63 | Train Loss: 0.3932725 Vali Loss: 0.9212741 Test Loss: 0.4084212
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.6899547576904297
Epoch: 66, Steps: 63 | Train Loss: 0.3931428 Vali Loss: 0.9212920 Test Loss: 0.4084286
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.8886663913726807
Epoch: 67, Steps: 63 | Train Loss: 0.3933419 Vali Loss: 0.9213037 Test Loss: 0.4084055
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.6733310222625732
Epoch: 68, Steps: 63 | Train Loss: 0.3933288 Vali Loss: 0.9211570 Test Loss: 0.4084220
Validation loss decreased (0.921190 --> 0.921157).  Saving model ...
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.5531151294708252
Epoch: 69, Steps: 63 | Train Loss: 0.3930618 Vali Loss: 0.9213616 Test Loss: 0.4084113
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.7545275688171387
Epoch: 70, Steps: 63 | Train Loss: 0.3932746 Vali Loss: 0.9212520 Test Loss: 0.4084058
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.5344433784484863
Epoch: 71, Steps: 63 | Train Loss: 0.3933070 Vali Loss: 0.9209359 Test Loss: 0.4084168
Validation loss decreased (0.921157 --> 0.920936).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.4874155521392822
Epoch: 72, Steps: 63 | Train Loss: 0.3932392 Vali Loss: 0.9209982 Test Loss: 0.4084061
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.4140567779541016
Epoch: 73, Steps: 63 | Train Loss: 0.3933564 Vali Loss: 0.9211656 Test Loss: 0.4083979
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.5202662944793701
Epoch: 74, Steps: 63 | Train Loss: 0.3931972 Vali Loss: 0.9211030 Test Loss: 0.4083999
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.5710957050323486
Epoch: 75, Steps: 63 | Train Loss: 0.3932024 Vali Loss: 0.9210066 Test Loss: 0.4084056
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.6915876865386963
Epoch: 76, Steps: 63 | Train Loss: 0.3932729 Vali Loss: 0.9212378 Test Loss: 0.4084095
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.4608197212219238
Epoch: 77, Steps: 63 | Train Loss: 0.3930052 Vali Loss: 0.9209910 Test Loss: 0.4084045
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.6034588813781738
Epoch: 78, Steps: 63 | Train Loss: 0.3930704 Vali Loss: 0.9212618 Test Loss: 0.4084143
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.5416722297668457
Epoch: 79, Steps: 63 | Train Loss: 0.3932277 Vali Loss: 0.9209639 Test Loss: 0.4084163
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.1221258640289307
Epoch: 80, Steps: 63 | Train Loss: 0.3930378 Vali Loss: 0.9210714 Test Loss: 0.4084077
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.615790605545044
Epoch: 81, Steps: 63 | Train Loss: 0.3931933 Vali Loss: 0.9212529 Test Loss: 0.4084058
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.585078477859497
Epoch: 82, Steps: 63 | Train Loss: 0.3929851 Vali Loss: 0.9209449 Test Loss: 0.4084062
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.448873519897461
Epoch: 83, Steps: 63 | Train Loss: 0.3931611 Vali Loss: 0.9208829 Test Loss: 0.4084107
Validation loss decreased (0.920936 --> 0.920883).  Saving model ...
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.4517369270324707
Epoch: 84, Steps: 63 | Train Loss: 0.3931094 Vali Loss: 0.9211778 Test Loss: 0.4084033
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.5435538291931152
Epoch: 85, Steps: 63 | Train Loss: 0.3931665 Vali Loss: 0.9208804 Test Loss: 0.4084040
Validation loss decreased (0.920883 --> 0.920880).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.5383987426757812
Epoch: 86, Steps: 63 | Train Loss: 0.3930477 Vali Loss: 0.9210311 Test Loss: 0.4084020
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.5747694969177246
Epoch: 87, Steps: 63 | Train Loss: 0.3930253 Vali Loss: 0.9208252 Test Loss: 0.4084067
Validation loss decreased (0.920880 --> 0.920825).  Saving model ...
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.5518672466278076
Epoch: 88, Steps: 63 | Train Loss: 0.3932149 Vali Loss: 0.9212725 Test Loss: 0.4083995
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.5617718696594238
Epoch: 89, Steps: 63 | Train Loss: 0.3930025 Vali Loss: 0.9212667 Test Loss: 0.4084010
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.834280014038086
Epoch: 90, Steps: 63 | Train Loss: 0.3930516 Vali Loss: 0.9211263 Test Loss: 0.4083975
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.5198986530303955
Epoch: 91, Steps: 63 | Train Loss: 0.3932780 Vali Loss: 0.9207060 Test Loss: 0.4084036
Validation loss decreased (0.920825 --> 0.920706).  Saving model ...
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.5745391845703125
Epoch: 92, Steps: 63 | Train Loss: 0.3932541 Vali Loss: 0.9212402 Test Loss: 0.4084024
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.5328338146209717
Epoch: 93, Steps: 63 | Train Loss: 0.3931260 Vali Loss: 0.9209102 Test Loss: 0.4084001
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.6689226627349854
Epoch: 94, Steps: 63 | Train Loss: 0.3930315 Vali Loss: 0.9211578 Test Loss: 0.4084091
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.6138701438903809
Epoch: 95, Steps: 63 | Train Loss: 0.3927793 Vali Loss: 0.9211944 Test Loss: 0.4084044
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.683992862701416
Epoch: 96, Steps: 63 | Train Loss: 0.3930056 Vali Loss: 0.9208799 Test Loss: 0.4084036
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.8222615718841553
Epoch: 97, Steps: 63 | Train Loss: 0.3930196 Vali Loss: 0.9207991 Test Loss: 0.4084059
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.5045216083526611
Epoch: 98, Steps: 63 | Train Loss: 0.3930087 Vali Loss: 0.9211422 Test Loss: 0.4084052
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.7955868244171143
Epoch: 99, Steps: 63 | Train Loss: 0.3929431 Vali Loss: 0.9210773 Test Loss: 0.4084022
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.568645715713501
Epoch: 100, Steps: 63 | Train Loss: 0.3930729 Vali Loss: 0.9209486 Test Loss: 0.4084059
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh1_360_192_FITS_ETTh1_ftM_sl360_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.4043785035610199, mae:0.4130987823009491, rse:0.6038810014724731, corr:[0.26048028 0.26883253 0.26917455 0.27138305 0.26850575 0.26611018
 0.2662013  0.26587304 0.26477    0.2648367  0.26496196 0.26422942
 0.2639191  0.26396132 0.26358333 0.26347086 0.2637336  0.26352265
 0.26317346 0.26318792 0.26319408 0.262923   0.26313564 0.26354933
 0.2631657  0.2628558  0.26285613 0.2624362  0.2618945  0.26194042
 0.26185215 0.26102474 0.26047644 0.2607073  0.26079434 0.26049763
 0.2606372  0.261015   0.26099816 0.2610362  0.26145813 0.26155272
 0.2614571  0.26157093 0.2615027  0.26102296 0.26086977 0.26106095
 0.26068687 0.2597137  0.2588157  0.25828373 0.25770685 0.25675765
 0.25608963 0.25570098 0.25539732 0.2552302  0.2549802  0.25486422
 0.2548255  0.25482115 0.25472012 0.25464585 0.25472888 0.25501835
 0.25538102 0.25537995 0.25523013 0.25525296 0.25528318 0.25505707
 0.2544889  0.2536994  0.25302303 0.2527441  0.2526901  0.25242034
 0.25192145 0.2514059  0.25108972 0.25079912 0.25051996 0.2503796
 0.25031856 0.25029123 0.25041538 0.25051606 0.25039366 0.2502991
 0.25030258 0.25013143 0.24976172 0.24952196 0.24959607 0.25015026
 0.25076947 0.2507972  0.25087175 0.25102353 0.25101653 0.25082272
 0.25062802 0.2505908  0.25044224 0.25025377 0.25012943 0.24992503
 0.24964023 0.24958743 0.24982    0.25013062 0.25034463 0.25053784
 0.25056484 0.25034052 0.25005922 0.24980105 0.2494766  0.24942534
 0.24950092 0.24893646 0.24816759 0.24773075 0.24731702 0.2466244
 0.2463995  0.24654889 0.24626154 0.24573442 0.24559505 0.24542324
 0.24502257 0.24512035 0.24543117 0.24514875 0.24517645 0.24567533
 0.24598722 0.2458518  0.24576475 0.24576768 0.24547422 0.24533887
 0.24540387 0.24482888 0.24428527 0.24392618 0.24322078 0.24226516
 0.24212885 0.24230953 0.24202397 0.24216084 0.24256028 0.24236663
 0.24191102 0.24229938 0.24259079 0.24205892 0.24193934 0.2425271
 0.24258298 0.24216688 0.24224928 0.24219221 0.24162824 0.24161792
 0.24198516 0.24175754 0.24193706 0.24258202 0.2422785  0.2415455
 0.24171127 0.24195048 0.24163255 0.24199422 0.24244653 0.24210307
 0.24219324 0.24273793 0.24226135 0.24176982 0.24268004 0.24285312
 0.24171503 0.24234644 0.24254635 0.23963405 0.24103895 0.2381288 ]
