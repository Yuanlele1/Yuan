Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh1_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=74, out_features=143, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9481472.0
params:  10725.0
Trainable parameters:  10725
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.7326691150665283
Epoch: 1, Steps: 62 | Train Loss: 0.7794268 Vali Loss: 1.5975466 Test Loss: 0.7217425
Validation loss decreased (inf --> 1.597547).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.089543581008911
Epoch: 2, Steps: 62 | Train Loss: 0.6157170 Vali Loss: 1.4311342 Test Loss: 0.6073958
Validation loss decreased (1.597547 --> 1.431134).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.321354389190674
Epoch: 3, Steps: 62 | Train Loss: 0.5604462 Vali Loss: 1.3602360 Test Loss: 0.5582200
Validation loss decreased (1.431134 --> 1.360236).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.7870299816131592
Epoch: 4, Steps: 62 | Train Loss: 0.5322135 Vali Loss: 1.3159494 Test Loss: 0.5261256
Validation loss decreased (1.360236 --> 1.315949).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.094491958618164
Epoch: 5, Steps: 62 | Train Loss: 0.5136323 Vali Loss: 1.2807938 Test Loss: 0.5027383
Validation loss decreased (1.315949 --> 1.280794).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.847965717315674
Epoch: 6, Steps: 62 | Train Loss: 0.4999561 Vali Loss: 1.2515453 Test Loss: 0.4854242
Validation loss decreased (1.280794 --> 1.251545).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.764007329940796
Epoch: 7, Steps: 62 | Train Loss: 0.4902875 Vali Loss: 1.2330238 Test Loss: 0.4725688
Validation loss decreased (1.251545 --> 1.233024).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.822265148162842
Epoch: 8, Steps: 62 | Train Loss: 0.4828303 Vali Loss: 1.2199016 Test Loss: 0.4627643
Validation loss decreased (1.233024 --> 1.219902).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.8258776664733887
Epoch: 9, Steps: 62 | Train Loss: 0.4771502 Vali Loss: 1.2058651 Test Loss: 0.4554385
Validation loss decreased (1.219902 --> 1.205865).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.940690279006958
Epoch: 10, Steps: 62 | Train Loss: 0.4732779 Vali Loss: 1.2003889 Test Loss: 0.4497983
Validation loss decreased (1.205865 --> 1.200389).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.719849109649658
Epoch: 11, Steps: 62 | Train Loss: 0.4700286 Vali Loss: 1.1941551 Test Loss: 0.4456818
Validation loss decreased (1.200389 --> 1.194155).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.9966599941253662
Epoch: 12, Steps: 62 | Train Loss: 0.4675450 Vali Loss: 1.1886147 Test Loss: 0.4426301
Validation loss decreased (1.194155 --> 1.188615).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.162113189697266
Epoch: 13, Steps: 62 | Train Loss: 0.4658410 Vali Loss: 1.1851145 Test Loss: 0.4402553
Validation loss decreased (1.188615 --> 1.185115).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.8264002799987793
Epoch: 14, Steps: 62 | Train Loss: 0.4641952 Vali Loss: 1.1791931 Test Loss: 0.4385086
Validation loss decreased (1.185115 --> 1.179193).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.6642847061157227
Epoch: 15, Steps: 62 | Train Loss: 0.4632442 Vali Loss: 1.1766893 Test Loss: 0.4371164
Validation loss decreased (1.179193 --> 1.176689).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.254025936126709
Epoch: 16, Steps: 62 | Train Loss: 0.4622929 Vali Loss: 1.1774181 Test Loss: 0.4362940
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.3640241622924805
Epoch: 17, Steps: 62 | Train Loss: 0.4616210 Vali Loss: 1.1727971 Test Loss: 0.4354395
Validation loss decreased (1.176689 --> 1.172797).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.347888946533203
Epoch: 18, Steps: 62 | Train Loss: 0.4611000 Vali Loss: 1.1718891 Test Loss: 0.4349312
Validation loss decreased (1.172797 --> 1.171889).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.3907992839813232
Epoch: 19, Steps: 62 | Train Loss: 0.4605642 Vali Loss: 1.1762656 Test Loss: 0.4343649
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.2976510524749756
Epoch: 20, Steps: 62 | Train Loss: 0.4600517 Vali Loss: 1.1723036 Test Loss: 0.4340525
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.8893296718597412
Epoch: 21, Steps: 62 | Train Loss: 0.4599405 Vali Loss: 1.1676260 Test Loss: 0.4337447
Validation loss decreased (1.171889 --> 1.167626).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.0249483585357666
Epoch: 22, Steps: 62 | Train Loss: 0.4596652 Vali Loss: 1.1684673 Test Loss: 0.4336542
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9350309371948242
Epoch: 23, Steps: 62 | Train Loss: 0.4595533 Vali Loss: 1.1673958 Test Loss: 0.4334768
Validation loss decreased (1.167626 --> 1.167396).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8902678489685059
Epoch: 24, Steps: 62 | Train Loss: 0.4592530 Vali Loss: 1.1690960 Test Loss: 0.4333370
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.1908669471740723
Epoch: 25, Steps: 62 | Train Loss: 0.4591557 Vali Loss: 1.1687828 Test Loss: 0.4332443
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8620150089263916
Epoch: 26, Steps: 62 | Train Loss: 0.4588656 Vali Loss: 1.1656137 Test Loss: 0.4332172
Validation loss decreased (1.167396 --> 1.165614).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.7844772338867188
Epoch: 27, Steps: 62 | Train Loss: 0.4588439 Vali Loss: 1.1669385 Test Loss: 0.4331220
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.962153434753418
Epoch: 28, Steps: 62 | Train Loss: 0.4586053 Vali Loss: 1.1661911 Test Loss: 0.4331360
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.5856120586395264
Epoch: 29, Steps: 62 | Train Loss: 0.4585759 Vali Loss: 1.1676868 Test Loss: 0.4330420
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.9915235042572021
Epoch: 30, Steps: 62 | Train Loss: 0.4585950 Vali Loss: 1.1686341 Test Loss: 0.4330154
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.2450995445251465
Epoch: 31, Steps: 62 | Train Loss: 0.4586152 Vali Loss: 1.1683552 Test Loss: 0.4329991
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.1683647632598877
Epoch: 32, Steps: 62 | Train Loss: 0.4584201 Vali Loss: 1.1647928 Test Loss: 0.4329878
Validation loss decreased (1.165614 --> 1.164793).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.521533727645874
Epoch: 33, Steps: 62 | Train Loss: 0.4583915 Vali Loss: 1.1657035 Test Loss: 0.4329525
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.3585431575775146
Epoch: 34, Steps: 62 | Train Loss: 0.4582437 Vali Loss: 1.1658138 Test Loss: 0.4328980
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.5453813076019287
Epoch: 35, Steps: 62 | Train Loss: 0.4581850 Vali Loss: 1.1683271 Test Loss: 0.4329166
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.360553026199341
Epoch: 36, Steps: 62 | Train Loss: 0.4579703 Vali Loss: 1.1673962 Test Loss: 0.4329191
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.0862574577331543
Epoch: 37, Steps: 62 | Train Loss: 0.4580863 Vali Loss: 1.1618699 Test Loss: 0.4329764
Validation loss decreased (1.164793 --> 1.161870).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.2866098880767822
Epoch: 38, Steps: 62 | Train Loss: 0.4580251 Vali Loss: 1.1614791 Test Loss: 0.4329075
Validation loss decreased (1.161870 --> 1.161479).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.0067458152770996
Epoch: 39, Steps: 62 | Train Loss: 0.4580454 Vali Loss: 1.1666133 Test Loss: 0.4329155
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.8622238636016846
Epoch: 40, Steps: 62 | Train Loss: 0.4577363 Vali Loss: 1.1653332 Test Loss: 0.4329284
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.3959901332855225
Epoch: 41, Steps: 62 | Train Loss: 0.4578104 Vali Loss: 1.1663357 Test Loss: 0.4329051
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.457984685897827
Epoch: 42, Steps: 62 | Train Loss: 0.4579969 Vali Loss: 1.1698517 Test Loss: 0.4329093
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.3229827880859375
Epoch: 43, Steps: 62 | Train Loss: 0.4579344 Vali Loss: 1.1691371 Test Loss: 0.4329096
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.4121172428131104
Epoch: 44, Steps: 62 | Train Loss: 0.4579463 Vali Loss: 1.1670548 Test Loss: 0.4329042
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.8704736232757568
Epoch: 45, Steps: 62 | Train Loss: 0.4577022 Vali Loss: 1.1619409 Test Loss: 0.4329349
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.458277940750122
Epoch: 46, Steps: 62 | Train Loss: 0.4577682 Vali Loss: 1.1649987 Test Loss: 0.4329186
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.8303980827331543
Epoch: 47, Steps: 62 | Train Loss: 0.4576664 Vali Loss: 1.1628853 Test Loss: 0.4329172
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9074947834014893
Epoch: 48, Steps: 62 | Train Loss: 0.4577523 Vali Loss: 1.1633509 Test Loss: 0.4329058
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.0623621940612793
Epoch: 49, Steps: 62 | Train Loss: 0.4576522 Vali Loss: 1.1620698 Test Loss: 0.4328960
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.1599371433258057
Epoch: 50, Steps: 62 | Train Loss: 0.4577031 Vali Loss: 1.1677991 Test Loss: 0.4328884
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.492326498031616
Epoch: 51, Steps: 62 | Train Loss: 0.4577360 Vali Loss: 1.1658760 Test Loss: 0.4329131
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.353484869003296
Epoch: 52, Steps: 62 | Train Loss: 0.4575386 Vali Loss: 1.1658013 Test Loss: 0.4328887
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.4643826484680176
Epoch: 53, Steps: 62 | Train Loss: 0.4575306 Vali Loss: 1.1670378 Test Loss: 0.4329023
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.4409847259521484
Epoch: 54, Steps: 62 | Train Loss: 0.4576923 Vali Loss: 1.1663384 Test Loss: 0.4329050
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.2767648696899414
Epoch: 55, Steps: 62 | Train Loss: 0.4576289 Vali Loss: 1.1616396 Test Loss: 0.4329033
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.873805522918701
Epoch: 56, Steps: 62 | Train Loss: 0.4575255 Vali Loss: 1.1599716 Test Loss: 0.4329260
Validation loss decreased (1.161479 --> 1.159972).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.6729700565338135
Epoch: 57, Steps: 62 | Train Loss: 0.4577468 Vali Loss: 1.1658158 Test Loss: 0.4329100
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.6842753887176514
Epoch: 58, Steps: 62 | Train Loss: 0.4574481 Vali Loss: 1.1589227 Test Loss: 0.4329104
Validation loss decreased (1.159972 --> 1.158923).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.8337671756744385
Epoch: 59, Steps: 62 | Train Loss: 0.4576968 Vali Loss: 1.1642040 Test Loss: 0.4329029
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.190403699874878
Epoch: 60, Steps: 62 | Train Loss: 0.4575809 Vali Loss: 1.1639591 Test Loss: 0.4329177
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.0603690147399902
Epoch: 61, Steps: 62 | Train Loss: 0.4576010 Vali Loss: 1.1640760 Test Loss: 0.4329133
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.6245810985565186
Epoch: 62, Steps: 62 | Train Loss: 0.4575835 Vali Loss: 1.1629317 Test Loss: 0.4329110
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.575260639190674
Epoch: 63, Steps: 62 | Train Loss: 0.4575560 Vali Loss: 1.1667852 Test Loss: 0.4329109
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.227344036102295
Epoch: 64, Steps: 62 | Train Loss: 0.4574755 Vali Loss: 1.1608690 Test Loss: 0.4329186
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.6828906536102295
Epoch: 65, Steps: 62 | Train Loss: 0.4575519 Vali Loss: 1.1633961 Test Loss: 0.4329065
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.119746685028076
Epoch: 66, Steps: 62 | Train Loss: 0.4576299 Vali Loss: 1.1633862 Test Loss: 0.4328990
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.127314329147339
Epoch: 67, Steps: 62 | Train Loss: 0.4574824 Vali Loss: 1.1600893 Test Loss: 0.4329082
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.3534743785858154
Epoch: 68, Steps: 62 | Train Loss: 0.4576167 Vali Loss: 1.1641556 Test Loss: 0.4329078
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.6999270915985107
Epoch: 69, Steps: 62 | Train Loss: 0.4574123 Vali Loss: 1.1642398 Test Loss: 0.4329076
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.5433690547943115
Epoch: 70, Steps: 62 | Train Loss: 0.4575752 Vali Loss: 1.1663601 Test Loss: 0.4329038
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.986668586730957
Epoch: 71, Steps: 62 | Train Loss: 0.4574878 Vali Loss: 1.1638093 Test Loss: 0.4329037
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.3175528049468994
Epoch: 72, Steps: 62 | Train Loss: 0.4573376 Vali Loss: 1.1653161 Test Loss: 0.4329082
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.259981870651245
Epoch: 73, Steps: 62 | Train Loss: 0.4575200 Vali Loss: 1.1598485 Test Loss: 0.4329066
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.533827304840088
Epoch: 74, Steps: 62 | Train Loss: 0.4575990 Vali Loss: 1.1639738 Test Loss: 0.4329106
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.069408416748047
Epoch: 75, Steps: 62 | Train Loss: 0.4574917 Vali Loss: 1.1632583 Test Loss: 0.4329082
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.4182868003845215
Epoch: 76, Steps: 62 | Train Loss: 0.4574292 Vali Loss: 1.1658823 Test Loss: 0.4329145
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.0970964431762695
Epoch: 77, Steps: 62 | Train Loss: 0.4574365 Vali Loss: 1.1594622 Test Loss: 0.4329117
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.0031378269195557
Epoch: 78, Steps: 62 | Train Loss: 0.4575260 Vali Loss: 1.1602507 Test Loss: 0.4329121
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh1_360_336_FITS_ETTh1_ftM_sl360_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.43175286054611206, mae:0.43074777722358704, rse:0.625560998916626, corr:[0.24793832 0.25870907 0.25633073 0.25420088 0.25446174 0.25418758
 0.25255448 0.2512336  0.25088915 0.2513795  0.25165644 0.25108948
 0.25020516 0.24960348 0.24938424 0.24940448 0.24944772 0.24950604
 0.24957843 0.24949114 0.24938245 0.24947271 0.24987406 0.25019357
 0.24994032 0.24932687 0.2489633  0.24896942 0.2489343  0.24853505
 0.2478311  0.24727146 0.2471414  0.2473056  0.24741733 0.24729629
 0.24704833 0.24694318 0.24711156 0.24742654 0.24777679 0.24799408
 0.24799387 0.24780405 0.24759583 0.24755894 0.247745   0.2477691
 0.24724226 0.24636835 0.24554567 0.24512559 0.24476314 0.24404351
 0.24328089 0.24277888 0.24255493 0.24248351 0.24233058 0.24213895
 0.24180071 0.24153677 0.24146016 0.24168141 0.24208069 0.24244542
 0.24258459 0.2423183  0.24215885 0.24228027 0.24242038 0.24213842
 0.24136151 0.240432   0.23989657 0.23988464 0.23997751 0.23973516
 0.23919216 0.2386618  0.23840345 0.23830853 0.23811641 0.23781982
 0.23754732 0.23741879 0.23739709 0.23742603 0.23744445 0.23739976
 0.2371787  0.23684463 0.23660858 0.23669283 0.23710898 0.23778518
 0.23842876 0.23864388 0.2387137  0.2388084  0.23891874 0.2388806
 0.2386723  0.2385004  0.23840055 0.23832881 0.23819532 0.23798272
 0.23775162 0.23766519 0.23779164 0.23810184 0.23836151 0.23847583
 0.2383915  0.2380741  0.2376958  0.23740543 0.23728149 0.23733743
 0.23728885 0.23677829 0.23602173 0.23541403 0.23514315 0.2349562
 0.23473424 0.2345133  0.23421255 0.23394443 0.2338201  0.23377821
 0.23367214 0.2335434  0.23353654 0.23367183 0.23391652 0.23414458
 0.23422275 0.23409191 0.23390567 0.23377183 0.23367582 0.23363382
 0.23354374 0.23309477 0.23247327 0.23194492 0.23166159 0.23142135
 0.23122784 0.23115718 0.23117892 0.2313279  0.23145938 0.23150624
 0.23139906 0.2312803  0.2312009  0.23118608 0.23112324 0.231186
 0.2311882  0.230988   0.23067816 0.23037794 0.23022157 0.23038028
 0.23077422 0.23110537 0.23125796 0.23137143 0.23157017 0.23175187
 0.23176652 0.23167673 0.23162828 0.23175617 0.2318706  0.23181361
 0.23161472 0.23143117 0.23136583 0.23143612 0.23160718 0.23190719
 0.2321492  0.232159   0.2320261  0.23179778 0.23152973 0.23132518
 0.23107632 0.23061064 0.22986461 0.22917995 0.22870395 0.22836474
 0.22811651 0.22805066 0.22800374 0.22795002 0.22788464 0.22789921
 0.22797148 0.2280377  0.22806172 0.22805317 0.22811416 0.22826375
 0.22840717 0.22822765 0.22778654 0.22729343 0.22700474 0.2271318
 0.22739732 0.22723247 0.22686122 0.22650501 0.22640121 0.22647278
 0.22648153 0.22639583 0.22628438 0.22608303 0.22578534 0.2256065
 0.22547853 0.22529165 0.22518818 0.22515768 0.22518922 0.22530705
 0.2254284  0.22527607 0.22494815 0.22469684 0.2247765  0.22501661
 0.22531159 0.22521879 0.22482899 0.22457163 0.22464381 0.22489406
 0.22510491 0.22527803 0.22538455 0.22524898 0.22498214 0.22478369
 0.22471759 0.22470628 0.22478794 0.22484455 0.22498536 0.22524595
 0.22551818 0.22548757 0.22534421 0.22531377 0.2254595  0.22562422
 0.22559963 0.2252593  0.22461887 0.22403066 0.22362614 0.22342123
 0.22336513 0.22328399 0.22307804 0.22277127 0.22248928 0.2223769
 0.22249585 0.22258973 0.22241664 0.2220769  0.22187154 0.2219723
 0.2223306  0.22243227 0.22230355 0.22217956 0.2224825  0.2231105
 0.22382928 0.22418466 0.2241307  0.22385837 0.2237205  0.22386213
 0.22411482 0.2242573  0.2242072  0.22409482 0.22387676 0.22373144
 0.22374631 0.22375967 0.22377989 0.22358166 0.22345564 0.22364253
 0.22415644 0.22441615 0.22424926 0.22377697 0.22357555 0.22384271
 0.22433671 0.22456302 0.22409399 0.2235408  0.2231713  0.2227815
 0.22229122 0.2219035  0.22159582 0.22147289 0.22109163 0.22074693
 0.22100477 0.22133146 0.22093466 0.22022806 0.22015247 0.22147425
 0.22256418 0.22090691 0.21725383 0.21651915 0.22099996 0.21814677]
