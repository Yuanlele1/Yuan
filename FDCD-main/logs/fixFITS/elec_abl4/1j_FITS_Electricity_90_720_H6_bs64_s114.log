Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='electricity.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=321, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='Electricity_90_j720_H6', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : Electricity_90_j720_H6_FITS_custom_ftM_sl90_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17603
val 1913
test 4541
Model(
  (freq_upsampler): Linear(in_features=34, out_features=306, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  427479552.0
params:  10710.0
Trainable parameters:  10710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
	iters: 100, epoch: 1 | loss: 1.1286688
	speed: 0.2095s/iter; left time: 2849.2850s
Epoch: 1 cost time: 28.366727590560913
Epoch: 1, Steps: 137 | Train Loss: 1.6267902 Vali Loss: 0.7945746 Test Loss: 0.8785440
Validation loss decreased (inf --> 0.794575).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 2 | loss: 0.6374929
	speed: 0.5027s/iter; left time: 6768.9495s
Epoch: 2 cost time: 27.53712248802185
Epoch: 2, Steps: 137 | Train Loss: 0.7159106 Vali Loss: 0.4856268 Test Loss: 0.5452188
Validation loss decreased (0.794575 --> 0.485627).  Saving model ...
Updating learning rate to 0.000475
	iters: 100, epoch: 3 | loss: 0.4650989
	speed: 0.5029s/iter; left time: 6702.7376s
Epoch: 3 cost time: 28.41701889038086
Epoch: 3, Steps: 137 | Train Loss: 0.4942121 Vali Loss: 0.3668094 Test Loss: 0.4142938
Validation loss decreased (0.485627 --> 0.366809).  Saving model ...
Updating learning rate to 0.00045125
	iters: 100, epoch: 4 | loss: 0.3795920
	speed: 0.5110s/iter; left time: 6740.4554s
Epoch: 4 cost time: 28.876434326171875
Epoch: 4, Steps: 137 | Train Loss: 0.3978013 Vali Loss: 0.3104789 Test Loss: 0.3515716
Validation loss decreased (0.366809 --> 0.310479).  Saving model ...
Updating learning rate to 0.0004286875
	iters: 100, epoch: 5 | loss: 0.3466503
	speed: 0.5163s/iter; left time: 6738.6633s
Epoch: 5 cost time: 28.263088941574097
Epoch: 5, Steps: 137 | Train Loss: 0.3505341 Vali Loss: 0.2824244 Test Loss: 0.3195308
Validation loss decreased (0.310479 --> 0.282424).  Saving model ...
Updating learning rate to 0.00040725312499999993
	iters: 100, epoch: 6 | loss: 0.3172655
	speed: 0.5150s/iter; left time: 6651.9335s
Epoch: 6 cost time: 28.41705346107483
Epoch: 6, Steps: 137 | Train Loss: 0.3258439 Vali Loss: 0.2666197 Test Loss: 0.3020876
Validation loss decreased (0.282424 --> 0.266620).  Saving model ...
Updating learning rate to 0.0003868904687499999
	iters: 100, epoch: 7 | loss: 0.3022641
	speed: 0.5150s/iter; left time: 6581.4959s
Epoch: 7 cost time: 28.613008975982666
Epoch: 7, Steps: 137 | Train Loss: 0.3119167 Vali Loss: 0.2578827 Test Loss: 0.2918535
Validation loss decreased (0.266620 --> 0.257883).  Saving model ...
Updating learning rate to 0.00036754594531249993
	iters: 100, epoch: 8 | loss: 0.3169173
	speed: 0.4995s/iter; left time: 6314.5297s
Epoch: 8 cost time: 27.7944757938385
Epoch: 8, Steps: 137 | Train Loss: 0.3034715 Vali Loss: 0.2523242 Test Loss: 0.2853840
Validation loss decreased (0.257883 --> 0.252324).  Saving model ...
Updating learning rate to 0.00034916864804687486
	iters: 100, epoch: 9 | loss: 0.2919911
	speed: 0.5077s/iter; left time: 6349.3746s
Epoch: 9 cost time: 27.87454605102539
Epoch: 9, Steps: 137 | Train Loss: 0.2979744 Vali Loss: 0.2483328 Test Loss: 0.2809913
Validation loss decreased (0.252324 --> 0.248333).  Saving model ...
Updating learning rate to 0.00033171021564453113
	iters: 100, epoch: 10 | loss: 0.3030028
	speed: 0.5142s/iter; left time: 6359.4096s
Epoch: 10 cost time: 28.285202503204346
Epoch: 10, Steps: 137 | Train Loss: 0.2940872 Vali Loss: 0.2450709 Test Loss: 0.2778316
Validation loss decreased (0.248333 --> 0.245071).  Saving model ...
Updating learning rate to 0.00031512470486230455
	iters: 100, epoch: 11 | loss: 0.2879471
	speed: 0.5042s/iter; left time: 6166.4745s
Epoch: 11 cost time: 27.91697096824646
Epoch: 11, Steps: 137 | Train Loss: 0.2911230 Vali Loss: 0.2424822 Test Loss: 0.2754689
Validation loss decreased (0.245071 --> 0.242482).  Saving model ...
Updating learning rate to 0.00029936846961918935
	iters: 100, epoch: 12 | loss: 0.2902735
	speed: 0.5093s/iter; left time: 6159.5124s
Epoch: 12 cost time: 28.589205980300903
Epoch: 12, Steps: 137 | Train Loss: 0.2890532 Vali Loss: 0.2411439 Test Loss: 0.2736743
Validation loss decreased (0.242482 --> 0.241144).  Saving model ...
Updating learning rate to 0.0002844000461382298
	iters: 100, epoch: 13 | loss: 0.2879125
	speed: 0.5107s/iter; left time: 6106.4863s
Epoch: 13 cost time: 27.930927276611328
Epoch: 13, Steps: 137 | Train Loss: 0.2872849 Vali Loss: 0.2394777 Test Loss: 0.2722470
Validation loss decreased (0.241144 --> 0.239478).  Saving model ...
Updating learning rate to 0.0002701800438313183
	iters: 100, epoch: 14 | loss: 0.2931021
	speed: 0.5066s/iter; left time: 5988.4579s
Epoch: 14 cost time: 27.888668537139893
Epoch: 14, Steps: 137 | Train Loss: 0.2859235 Vali Loss: 0.2389261 Test Loss: 0.2711097
Validation loss decreased (0.239478 --> 0.238926).  Saving model ...
Updating learning rate to 0.0002566710416397524
	iters: 100, epoch: 15 | loss: 0.2781073
	speed: 0.5199s/iter; left time: 6074.1619s
Epoch: 15 cost time: 28.629597663879395
Epoch: 15, Steps: 137 | Train Loss: 0.2848550 Vali Loss: 0.2381614 Test Loss: 0.2701734
Validation loss decreased (0.238926 --> 0.238161).  Saving model ...
Updating learning rate to 0.00024383748955776477
	iters: 100, epoch: 16 | loss: 0.2756285
	speed: 0.5065s/iter; left time: 5848.5777s
Epoch: 16 cost time: 27.82991600036621
Epoch: 16, Steps: 137 | Train Loss: 0.2838249 Vali Loss: 0.2376994 Test Loss: 0.2693789
Validation loss decreased (0.238161 --> 0.237699).  Saving model ...
Updating learning rate to 0.0002316456150798765
	iters: 100, epoch: 17 | loss: 0.2883615
	speed: 0.5022s/iter; left time: 5729.7792s
Epoch: 17 cost time: 27.76613140106201
Epoch: 17, Steps: 137 | Train Loss: 0.2831163 Vali Loss: 0.2369585 Test Loss: 0.2687367
Validation loss decreased (0.237699 --> 0.236958).  Saving model ...
Updating learning rate to 0.00022006333432588268
	iters: 100, epoch: 18 | loss: 0.2705061
	speed: 0.5145s/iter; left time: 5799.5644s
Epoch: 18 cost time: 28.275651216506958
Epoch: 18, Steps: 137 | Train Loss: 0.2825406 Vali Loss: 0.2367119 Test Loss: 0.2681791
Validation loss decreased (0.236958 --> 0.236712).  Saving model ...
Updating learning rate to 0.00020906016760958854
	iters: 100, epoch: 19 | loss: 0.2970542
	speed: 0.5025s/iter; left time: 5595.1203s
Epoch: 19 cost time: 27.72317910194397
Epoch: 19, Steps: 137 | Train Loss: 0.2819366 Vali Loss: 0.2358283 Test Loss: 0.2677115
Validation loss decreased (0.236712 --> 0.235828).  Saving model ...
Updating learning rate to 0.0001986071592291091
	iters: 100, epoch: 20 | loss: 0.2803335
	speed: 0.4731s/iter; left time: 5203.3067s
Epoch: 20 cost time: 24.02976703643799
Epoch: 20, Steps: 137 | Train Loss: 0.2814278 Vali Loss: 0.2354512 Test Loss: 0.2672877
Validation loss decreased (0.235828 --> 0.235451).  Saving model ...
Updating learning rate to 0.00018867680126765363
	iters: 100, epoch: 21 | loss: 0.2760885
	speed: 0.4848s/iter; left time: 5265.8840s
Epoch: 21 cost time: 28.14084768295288
Epoch: 21, Steps: 137 | Train Loss: 0.2810874 Vali Loss: 0.2352731 Test Loss: 0.2669104
Validation loss decreased (0.235451 --> 0.235273).  Saving model ...
Updating learning rate to 0.00017924296120427094
	iters: 100, epoch: 22 | loss: 0.2766550
	speed: 0.4964s/iter; left time: 5323.0496s
Epoch: 22 cost time: 28.22917890548706
Epoch: 22, Steps: 137 | Train Loss: 0.2806976 Vali Loss: 0.2346839 Test Loss: 0.2665673
Validation loss decreased (0.235273 --> 0.234684).  Saving model ...
Updating learning rate to 0.0001702808131440574
	iters: 100, epoch: 23 | loss: 0.2661615
	speed: 0.5106s/iter; left time: 5405.4417s
Epoch: 23 cost time: 28.38886833190918
Epoch: 23, Steps: 137 | Train Loss: 0.2803662 Vali Loss: 0.2346467 Test Loss: 0.2662808
Validation loss decreased (0.234684 --> 0.234647).  Saving model ...
Updating learning rate to 0.0001617667724868545
	iters: 100, epoch: 24 | loss: 0.2794116
	speed: 0.5042s/iter; left time: 5268.9630s
Epoch: 24 cost time: 27.31214427947998
Epoch: 24, Steps: 137 | Train Loss: 0.2800243 Vali Loss: 0.2343008 Test Loss: 0.2660164
Validation loss decreased (0.234647 --> 0.234301).  Saving model ...
Updating learning rate to 0.00015367843386251178
	iters: 100, epoch: 25 | loss: 0.2865242
	speed: 0.5026s/iter; left time: 5183.3783s
Epoch: 25 cost time: 27.92864418029785
Epoch: 25, Steps: 137 | Train Loss: 0.2797867 Vali Loss: 0.2343169 Test Loss: 0.2657782
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
	iters: 100, epoch: 26 | loss: 0.2713962
	speed: 0.4991s/iter; left time: 5078.3912s
Epoch: 26 cost time: 27.34844160079956
Epoch: 26, Steps: 137 | Train Loss: 0.2795440 Vali Loss: 0.2338773 Test Loss: 0.2655552
Validation loss decreased (0.234301 --> 0.233877).  Saving model ...
Updating learning rate to 0.00013869478656091687
	iters: 100, epoch: 27 | loss: 0.2892005
	speed: 0.5127s/iter; left time: 5147.0888s
Epoch: 27 cost time: 27.861547708511353
Epoch: 27, Steps: 137 | Train Loss: 0.2792797 Vali Loss: 0.2340151 Test Loss: 0.2653351
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
	iters: 100, epoch: 28 | loss: 0.2900513
	speed: 0.4992s/iter; left time: 4942.8472s
Epoch: 28 cost time: 27.561400175094604
Epoch: 28, Steps: 137 | Train Loss: 0.2790023 Vali Loss: 0.2331247 Test Loss: 0.2651711
Validation loss decreased (0.233877 --> 0.233125).  Saving model ...
Updating learning rate to 0.00012517204487122748
	iters: 100, epoch: 29 | loss: 0.2520241
	speed: 0.5119s/iter; left time: 4998.8662s
Epoch: 29 cost time: 27.776180744171143
Epoch: 29, Steps: 137 | Train Loss: 0.2788362 Vali Loss: 0.2325441 Test Loss: 0.2649947
Validation loss decreased (0.233125 --> 0.232544).  Saving model ...
Updating learning rate to 0.00011891344262766608
	iters: 100, epoch: 30 | loss: 0.2777522
	speed: 0.5038s/iter; left time: 4850.7720s
Epoch: 30 cost time: 26.702730655670166
Epoch: 30, Steps: 137 | Train Loss: 0.2786977 Vali Loss: 0.2331404 Test Loss: 0.2648352
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
	iters: 100, epoch: 31 | loss: 0.2759024
	speed: 0.5029s/iter; left time: 4772.5991s
Epoch: 31 cost time: 27.602160215377808
Epoch: 31, Steps: 137 | Train Loss: 0.2785232 Vali Loss: 0.2326136 Test Loss: 0.2646947
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
	iters: 100, epoch: 32 | loss: 0.2774491
	speed: 0.5050s/iter; left time: 4723.9940s
Epoch: 32 cost time: 27.70225167274475
Epoch: 32, Steps: 137 | Train Loss: 0.2783071 Vali Loss: 0.2329615 Test Loss: 0.2645583
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
	iters: 100, epoch: 33 | loss: 0.2626617
	speed: 0.5177s/iter; left time: 4771.3348s
Epoch: 33 cost time: 28.188356637954712
Epoch: 33, Steps: 137 | Train Loss: 0.2781944 Vali Loss: 0.2331461 Test Loss: 0.2644309
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
	iters: 100, epoch: 34 | loss: 0.2687684
	speed: 0.5101s/iter; left time: 4632.1536s
Epoch: 34 cost time: 27.77393627166748
Epoch: 34, Steps: 137 | Train Loss: 0.2780879 Vali Loss: 0.2325238 Test Loss: 0.2643047
Validation loss decreased (0.232544 --> 0.232524).  Saving model ...
Updating learning rate to 9.201295511778792e-05
	iters: 100, epoch: 35 | loss: 0.2830887
	speed: 0.5112s/iter; left time: 4571.9720s
Epoch: 35 cost time: 28.152281761169434
Epoch: 35, Steps: 137 | Train Loss: 0.2779259 Vali Loss: 0.2326756 Test Loss: 0.2642010
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
	iters: 100, epoch: 36 | loss: 0.2845350
	speed: 0.5051s/iter; left time: 4447.4984s
Epoch: 36 cost time: 27.724015712738037
Epoch: 36, Steps: 137 | Train Loss: 0.2778130 Vali Loss: 0.2324047 Test Loss: 0.2641001
Validation loss decreased (0.232524 --> 0.232405).  Saving model ...
Updating learning rate to 8.304169199380359e-05
	iters: 100, epoch: 37 | loss: 0.2768412
	speed: 0.4988s/iter; left time: 4324.2285s
Epoch: 37 cost time: 27.580021858215332
Epoch: 37, Steps: 137 | Train Loss: 0.2777167 Vali Loss: 0.2319014 Test Loss: 0.2639997
Validation loss decreased (0.232405 --> 0.231901).  Saving model ...
Updating learning rate to 7.88896073941134e-05
	iters: 100, epoch: 38 | loss: 0.2827231
	speed: 0.5035s/iter; left time: 4295.9305s
Epoch: 38 cost time: 28.356709957122803
Epoch: 38, Steps: 137 | Train Loss: 0.2776464 Vali Loss: 0.2324659 Test Loss: 0.2639143
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
	iters: 100, epoch: 39 | loss: 0.2712233
	speed: 0.5019s/iter; left time: 4213.1779s
Epoch: 39 cost time: 28.02126979827881
Epoch: 39, Steps: 137 | Train Loss: 0.2775769 Vali Loss: 0.2322875 Test Loss: 0.2638290
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
	iters: 100, epoch: 40 | loss: 0.2897795
	speed: 0.4934s/iter; left time: 4074.4201s
Epoch: 40 cost time: 27.920260667800903
Epoch: 40, Steps: 137 | Train Loss: 0.2773929 Vali Loss: 0.2315591 Test Loss: 0.2637617
Validation loss decreased (0.231901 --> 0.231559).  Saving model ...
Updating learning rate to 6.763797713952796e-05
	iters: 100, epoch: 41 | loss: 0.2786480
	speed: 0.5084s/iter; left time: 4128.9618s
Epoch: 41 cost time: 28.613006353378296
Epoch: 41, Steps: 137 | Train Loss: 0.2773845 Vali Loss: 0.2317073 Test Loss: 0.2636859
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
	iters: 100, epoch: 42 | loss: 0.2770194
	speed: 0.4975s/iter; left time: 3972.2915s
Epoch: 42 cost time: 27.977664947509766
Epoch: 42, Steps: 137 | Train Loss: 0.2772323 Vali Loss: 0.2320163 Test Loss: 0.2636169
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
	iters: 100, epoch: 43 | loss: 0.2747834
	speed: 0.4985s/iter; left time: 3911.3989s
Epoch: 43 cost time: 27.857879877090454
Epoch: 43, Steps: 137 | Train Loss: 0.2772496 Vali Loss: 0.2317879 Test Loss: 0.2635526
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
	iters: 100, epoch: 44 | loss: 0.2682481
	speed: 0.5130s/iter; left time: 3955.0821s
Epoch: 44 cost time: 28.529237747192383
Epoch: 44, Steps: 137 | Train Loss: 0.2771928 Vali Loss: 0.2320979 Test Loss: 0.2634903
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
	iters: 100, epoch: 45 | loss: 0.2830383
	speed: 0.5035s/iter; left time: 3813.1176s
Epoch: 45 cost time: 28.16976237297058
Epoch: 45, Steps: 137 | Train Loss: 0.2771343 Vali Loss: 0.2319482 Test Loss: 0.2634363
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
	iters: 100, epoch: 46 | loss: 0.2826298
	speed: 0.4985s/iter; left time: 3707.0205s
Epoch: 46 cost time: 27.601802825927734
Epoch: 46, Steps: 137 | Train Loss: 0.2769593 Vali Loss: 0.2314234 Test Loss: 0.2633884
Validation loss decreased (0.231559 --> 0.231423).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
	iters: 100, epoch: 47 | loss: 0.2767251
	speed: 0.5028s/iter; left time: 3670.1156s
Epoch: 47 cost time: 27.593270301818848
Epoch: 47, Steps: 137 | Train Loss: 0.2770080 Vali Loss: 0.2311207 Test Loss: 0.2633436
Validation loss decreased (0.231423 --> 0.231121).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
	iters: 100, epoch: 48 | loss: 0.2702306
	speed: 0.4953s/iter; left time: 3547.0744s
Epoch: 48 cost time: 27.371134757995605
Epoch: 48, Steps: 137 | Train Loss: 0.2769290 Vali Loss: 0.2318528 Test Loss: 0.2632920
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
	iters: 100, epoch: 49 | loss: 0.2756482
	speed: 0.4983s/iter; left time: 3500.4210s
Epoch: 49 cost time: 27.74048137664795
Epoch: 49, Steps: 137 | Train Loss: 0.2769022 Vali Loss: 0.2311075 Test Loss: 0.2632417
Validation loss decreased (0.231121 --> 0.231108).  Saving model ...
Updating learning rate to 4.26287951671541e-05
	iters: 100, epoch: 50 | loss: 0.2721318
	speed: 0.5135s/iter; left time: 3537.1829s
Epoch: 50 cost time: 27.691706657409668
Epoch: 50, Steps: 137 | Train Loss: 0.2768227 Vali Loss: 0.2315242 Test Loss: 0.2632063
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
	iters: 100, epoch: 51 | loss: 0.2675645
	speed: 0.5040s/iter; left time: 3402.8291s
Epoch: 51 cost time: 27.40675950050354
Epoch: 51, Steps: 137 | Train Loss: 0.2768132 Vali Loss: 0.2316315 Test Loss: 0.2631742
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
	iters: 100, epoch: 52 | loss: 0.2818135
	speed: 0.5017s/iter; left time: 3318.1405s
Epoch: 52 cost time: 27.64125347137451
Epoch: 52, Steps: 137 | Train Loss: 0.2767612 Vali Loss: 0.2312444 Test Loss: 0.2631307
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
	iters: 100, epoch: 53 | loss: 0.2747962
	speed: 0.5205s/iter; left time: 3371.0845s
Epoch: 53 cost time: 28.11410355567932
Epoch: 53, Steps: 137 | Train Loss: 0.2766329 Vali Loss: 0.2309657 Test Loss: 0.2631038
Validation loss decreased (0.231108 --> 0.230966).  Saving model ...
Updating learning rate to 3.47214200936168e-05
	iters: 100, epoch: 54 | loss: 0.2708250
	speed: 0.5181s/iter; left time: 3284.6755s
Epoch: 54 cost time: 28.047868728637695
Epoch: 54, Steps: 137 | Train Loss: 0.2766407 Vali Loss: 0.2312768 Test Loss: 0.2630795
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
	iters: 100, epoch: 55 | loss: 0.2755595
	speed: 0.5113s/iter; left time: 3171.3792s
Epoch: 55 cost time: 27.82319402694702
Epoch: 55, Steps: 137 | Train Loss: 0.2766007 Vali Loss: 0.2311974 Test Loss: 0.2630465
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
	iters: 100, epoch: 56 | loss: 0.2876836
	speed: 0.5102s/iter; left time: 3095.0645s
Epoch: 56 cost time: 27.667603015899658
Epoch: 56, Steps: 137 | Train Loss: 0.2765776 Vali Loss: 0.2312393 Test Loss: 0.2630257
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
	iters: 100, epoch: 57 | loss: 0.2821214
	speed: 0.5003s/iter; left time: 2966.4859s
Epoch: 57 cost time: 27.20163869857788
Epoch: 57, Steps: 137 | Train Loss: 0.2766045 Vali Loss: 0.2313351 Test Loss: 0.2630037
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
	iters: 100, epoch: 58 | loss: 0.2667536
	speed: 0.4977s/iter; left time: 2882.5367s
Epoch: 58 cost time: 27.28042507171631
Epoch: 58, Steps: 137 | Train Loss: 0.2765995 Vali Loss: 0.2311823 Test Loss: 0.2629745
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
	iters: 100, epoch: 59 | loss: 0.2812779
	speed: 0.5140s/iter; left time: 2906.5494s
Epoch: 59 cost time: 28.142773866653442
Epoch: 59, Steps: 137 | Train Loss: 0.2765277 Vali Loss: 0.2310236 Test Loss: 0.2629563
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
	iters: 100, epoch: 60 | loss: 0.2887310
	speed: 0.5048s/iter; left time: 2785.5342s
Epoch: 60 cost time: 27.611572265625
Epoch: 60, Steps: 137 | Train Loss: 0.2765087 Vali Loss: 0.2311557 Test Loss: 0.2629325
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
	iters: 100, epoch: 61 | loss: 0.2768727
	speed: 0.4998s/iter; left time: 2689.5793s
Epoch: 61 cost time: 28.119134426116943
Epoch: 61, Steps: 137 | Train Loss: 0.2764688 Vali Loss: 0.2315054 Test Loss: 0.2629161
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
	iters: 100, epoch: 62 | loss: 0.2645195
	speed: 0.5164s/iter; left time: 2708.1487s
Epoch: 62 cost time: 28.431743383407593
Epoch: 62, Steps: 137 | Train Loss: 0.2764676 Vali Loss: 0.2309840 Test Loss: 0.2628958
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
	iters: 100, epoch: 63 | loss: 0.2895798
	speed: 0.4947s/iter; left time: 2526.2450s
Epoch: 63 cost time: 27.777961492538452
Epoch: 63, Steps: 137 | Train Loss: 0.2765272 Vali Loss: 0.2312901 Test Loss: 0.2628830
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
	iters: 100, epoch: 64 | loss: 0.2734511
	speed: 0.5004s/iter; left time: 2487.1556s
Epoch: 64 cost time: 28.80488872528076
Epoch: 64, Steps: 137 | Train Loss: 0.2763619 Vali Loss: 0.2309128 Test Loss: 0.2628762
Validation loss decreased (0.230966 --> 0.230913).  Saving model ...
Updating learning rate to 1.974954695321896e-05
	iters: 100, epoch: 65 | loss: 0.2826816
	speed: 0.5066s/iter; left time: 2448.3335s
Epoch: 65 cost time: 27.860804557800293
Epoch: 65, Steps: 137 | Train Loss: 0.2763858 Vali Loss: 0.2314190 Test Loss: 0.2628593
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
	iters: 100, epoch: 66 | loss: 0.2837515
	speed: 0.4965s/iter; left time: 2331.3664s
Epoch: 66 cost time: 28.026135444641113
Epoch: 66, Steps: 137 | Train Loss: 0.2764418 Vali Loss: 0.2314565 Test Loss: 0.2628483
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
	iters: 100, epoch: 67 | loss: 0.2799351
	speed: 0.4999s/iter; left time: 2279.0847s
Epoch: 67 cost time: 27.915061950683594
Epoch: 67, Steps: 137 | Train Loss: 0.2763560 Vali Loss: 0.2311747 Test Loss: 0.2628362
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
	iters: 100, epoch: 68 | loss: 0.2749719
	speed: 0.4997s/iter; left time: 2209.8570s
Epoch: 68 cost time: 28.128450393676758
Epoch: 68, Steps: 137 | Train Loss: 0.2763800 Vali Loss: 0.2315700 Test Loss: 0.2628229
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
	iters: 100, epoch: 69 | loss: 0.2847558
	speed: 0.4984s/iter; left time: 2135.5500s
Epoch: 69 cost time: 27.63791012763977
Epoch: 69, Steps: 137 | Train Loss: 0.2764410 Vali Loss: 0.2312891 Test Loss: 0.2628140
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.5281822956662033e-05
	iters: 100, epoch: 70 | loss: 0.2774619
	speed: 0.5084s/iter; left time: 2108.8852s
Epoch: 70 cost time: 27.668142557144165
Epoch: 70, Steps: 137 | Train Loss: 0.2763860 Vali Loss: 0.2308793 Test Loss: 0.2628081
Validation loss decreased (0.230913 --> 0.230879).  Saving model ...
Updating learning rate to 1.451773180882893e-05
	iters: 100, epoch: 71 | loss: 0.2791342
	speed: 0.4995s/iter; left time: 2003.3694s
Epoch: 71 cost time: 27.567015647888184
Epoch: 71, Steps: 137 | Train Loss: 0.2763742 Vali Loss: 0.2311286 Test Loss: 0.2628022
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
	iters: 100, epoch: 72 | loss: 0.2698748
	speed: 0.4960s/iter; left time: 1921.5705s
Epoch: 72 cost time: 27.219643115997314
Epoch: 72, Steps: 137 | Train Loss: 0.2763245 Vali Loss: 0.2309904 Test Loss: 0.2627920
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
	iters: 100, epoch: 73 | loss: 0.2653261
	speed: 0.5154s/iter; left time: 1926.2005s
Epoch: 73 cost time: 28.190441131591797
Epoch: 73, Steps: 137 | Train Loss: 0.2763705 Vali Loss: 0.2312364 Test Loss: 0.2627839
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.2447140309594702e-05
	iters: 100, epoch: 74 | loss: 0.2827207
	speed: 0.5149s/iter; left time: 1853.6768s
Epoch: 74 cost time: 27.60173487663269
Epoch: 74, Steps: 137 | Train Loss: 0.2763377 Vali Loss: 0.2312873 Test Loss: 0.2627778
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1824783294114967e-05
	iters: 100, epoch: 75 | loss: 0.2805384
	speed: 0.5027s/iter; left time: 1740.9582s
Epoch: 75 cost time: 27.587347745895386
Epoch: 75, Steps: 137 | Train Loss: 0.2764221 Vali Loss: 0.2312032 Test Loss: 0.2627710
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1233544129409218e-05
	iters: 100, epoch: 76 | loss: 0.2682815
	speed: 0.5039s/iter; left time: 1676.0450s
Epoch: 76 cost time: 27.227071046829224
Epoch: 76, Steps: 137 | Train Loss: 0.2763472 Vali Loss: 0.2308079 Test Loss: 0.2627625
Validation loss decreased (0.230879 --> 0.230808).  Saving model ...
Updating learning rate to 1.0671866922938755e-05
	iters: 100, epoch: 77 | loss: 0.2790099
	speed: 0.4984s/iter; left time: 1589.2900s
Epoch: 77 cost time: 26.583373308181763
Epoch: 77, Steps: 137 | Train Loss: 0.2762716 Vali Loss: 0.2314152 Test Loss: 0.2627574
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0138273576791817e-05
	iters: 100, epoch: 78 | loss: 0.2885334
	speed: 0.5068s/iter; left time: 1546.8781s
Epoch: 78 cost time: 27.72287917137146
Epoch: 78, Steps: 137 | Train Loss: 0.2762324 Vali Loss: 0.2310704 Test Loss: 0.2627515
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.631359897952226e-06
	iters: 100, epoch: 79 | loss: 0.2800083
	speed: 0.5104s/iter; left time: 1487.7787s
Epoch: 79 cost time: 27.640544414520264
Epoch: 79, Steps: 137 | Train Loss: 0.2761920 Vali Loss: 0.2309100 Test Loss: 0.2627485
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.149791903054614e-06
	iters: 100, epoch: 80 | loss: 0.2794166
	speed: 0.4992s/iter; left time: 1386.6798s
Epoch: 80 cost time: 27.37339448928833
Epoch: 80, Steps: 137 | Train Loss: 0.2762726 Vali Loss: 0.2314621 Test Loss: 0.2627430
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.692302307901884e-06
	iters: 100, epoch: 81 | loss: 0.2692493
	speed: 0.5153s/iter; left time: 1361.0279s
Epoch: 81 cost time: 28.737063884735107
Epoch: 81, Steps: 137 | Train Loss: 0.2763721 Vali Loss: 0.2312764 Test Loss: 0.2627386
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.25768719250679e-06
	iters: 100, epoch: 82 | loss: 0.2672350
	speed: 0.5017s/iter; left time: 1256.3082s
Epoch: 82 cost time: 27.431787252426147
Epoch: 82, Steps: 137 | Train Loss: 0.2762129 Vali Loss: 0.2311125 Test Loss: 0.2627361
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.84480283288145e-06
	iters: 100, epoch: 83 | loss: 0.2720484
	speed: 0.4985s/iter; left time: 1179.9230s
Epoch: 83 cost time: 27.80724859237671
Epoch: 83, Steps: 137 | Train Loss: 0.2762841 Vali Loss: 0.2308280 Test Loss: 0.2627325
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.452562691237377e-06
	iters: 100, epoch: 84 | loss: 0.2832231
	speed: 0.5143s/iter; left time: 1146.8847s
Epoch: 84 cost time: 29.377401113510132
Epoch: 84, Steps: 137 | Train Loss: 0.2761801 Vali Loss: 0.2311304 Test Loss: 0.2627307
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.079934556675507e-06
	iters: 100, epoch: 85 | loss: 0.2917970
	speed: 0.5100s/iter; left time: 1067.5167s
Epoch: 85 cost time: 28.123364210128784
Epoch: 85, Steps: 137 | Train Loss: 0.2761768 Vali Loss: 0.2307440 Test Loss: 0.2627264
Validation loss decreased (0.230808 --> 0.230744).  Saving model ...
Updating learning rate to 6.725937828841732e-06
	iters: 100, epoch: 86 | loss: 0.2893567
	speed: 0.4994s/iter; left time: 976.7358s
Epoch: 86 cost time: 28.289778232574463
Epoch: 86, Steps: 137 | Train Loss: 0.2761515 Vali Loss: 0.2309180 Test Loss: 0.2627254
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.389640937399644e-06
	iters: 100, epoch: 87 | loss: 0.2652648
	speed: 0.5126s/iter; left time: 932.4978s
Epoch: 87 cost time: 28.124021291732788
Epoch: 87, Steps: 137 | Train Loss: 0.2763015 Vali Loss: 0.2308111 Test Loss: 0.2627224
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.070158890529662e-06
	iters: 100, epoch: 88 | loss: 0.2758240
	speed: 0.4998s/iter; left time: 840.6477s
Epoch: 88 cost time: 28.549669981002808
Epoch: 88, Steps: 137 | Train Loss: 0.2761904 Vali Loss: 0.2313104 Test Loss: 0.2627192
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.766650946003179e-06
	iters: 100, epoch: 89 | loss: 0.2845458
	speed: 0.4979s/iter; left time: 769.2640s
Epoch: 89 cost time: 27.847166061401367
Epoch: 89, Steps: 137 | Train Loss: 0.2762325 Vali Loss: 0.2312041 Test Loss: 0.2627183
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.47831839870302e-06
	iters: 100, epoch: 90 | loss: 0.2773794
	speed: 0.4984s/iter; left time: 701.7269s
Epoch: 90 cost time: 27.59618616104126
Epoch: 90, Steps: 137 | Train Loss: 0.2762281 Vali Loss: 0.2307006 Test Loss: 0.2627156
Validation loss decreased (0.230744 --> 0.230701).  Saving model ...
Updating learning rate to 5.204402478767869e-06
	iters: 100, epoch: 91 | loss: 0.2779594
	speed: 0.4905s/iter; left time: 623.4881s
Epoch: 91 cost time: 27.755152463912964
Epoch: 91, Steps: 137 | Train Loss: 0.2761388 Vali Loss: 0.2307385 Test Loss: 0.2627147
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.944182354829475e-06
	iters: 100, epoch: 92 | loss: 0.2810991
	speed: 0.4939s/iter; left time: 560.1030s
Epoch: 92 cost time: 27.25495433807373
Epoch: 92, Steps: 137 | Train Loss: 0.2761379 Vali Loss: 0.2308119 Test Loss: 0.2627118
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.696973237088e-06
	iters: 100, epoch: 93 | loss: 0.2740157
	speed: 0.4944s/iter; left time: 492.9079s
Epoch: 93 cost time: 26.964757204055786
Epoch: 93, Steps: 137 | Train Loss: 0.2762071 Vali Loss: 0.2311395 Test Loss: 0.2627099
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.462124575233601e-06
	iters: 100, epoch: 94 | loss: 0.2646753
	speed: 0.4910s/iter; left time: 422.2282s
Epoch: 94 cost time: 26.977211952209473
Epoch: 94, Steps: 137 | Train Loss: 0.2762993 Vali Loss: 0.2305809 Test Loss: 0.2627085
Validation loss decreased (0.230701 --> 0.230581).  Saving model ...
Updating learning rate to 4.239018346471921e-06
	iters: 100, epoch: 95 | loss: 0.2668577
	speed: 0.4980s/iter; left time: 360.0502s
Epoch: 95 cost time: 27.157782077789307
Epoch: 95, Steps: 137 | Train Loss: 0.2762517 Vali Loss: 0.2312363 Test Loss: 0.2627063
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.027067429148324e-06
	iters: 100, epoch: 96 | loss: 0.2931013
	speed: 0.4963s/iter; left time: 290.8288s
Epoch: 96 cost time: 26.94190216064453
Epoch: 96, Steps: 137 | Train Loss: 0.2761593 Vali Loss: 0.2308778 Test Loss: 0.2627047
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.825714057690908e-06
	iters: 100, epoch: 97 | loss: 0.2777226
	speed: 0.4957s/iter; left time: 222.5568s
Epoch: 97 cost time: 26.983701944351196
Epoch: 97, Steps: 137 | Train Loss: 0.2762091 Vali Loss: 0.2311675 Test Loss: 0.2627034
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.6344283548063623e-06
	iters: 100, epoch: 98 | loss: 0.2771156
	speed: 0.4877s/iter; left time: 152.1594s
Epoch: 98 cost time: 26.469130992889404
Epoch: 98, Steps: 137 | Train Loss: 0.2762099 Vali Loss: 0.2308327 Test Loss: 0.2627018
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.452706937066044e-06
	iters: 100, epoch: 99 | loss: 0.2720945
	speed: 0.4894s/iter; left time: 85.6475s
Epoch: 99 cost time: 26.9225013256073
Epoch: 99, Steps: 137 | Train Loss: 0.2762310 Vali Loss: 0.2305969 Test Loss: 0.2627005
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.2800715902127414e-06
	iters: 100, epoch: 100 | loss: 0.2747132
	speed: 0.4970s/iter; left time: 18.8874s
Epoch: 100 cost time: 27.11092782020569
Epoch: 100, Steps: 137 | Train Loss: 0.2761740 Vali Loss: 0.2313514 Test Loss: 0.2626997
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : Electricity_90_j720_H6_FITS_custom_ftM_sl90_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 4541
mse:0.2629830241203308, mae:0.3341121971607208, rse:0.5115537047386169, corr:[0.44354084 0.44009563 0.43988985 0.437941   0.43807417 0.4370886
 0.43722236 0.4367308  0.43567348 0.4346334  0.43412295 0.4332198
 0.43313968 0.4327906  0.43234134 0.4322473  0.43220112 0.4322167
 0.43239978 0.43237665 0.43204185 0.43223548 0.4321724  0.4308999
 0.42915174 0.4278518  0.42636454 0.42583308 0.42525312 0.425289
 0.42635283 0.42634666 0.42587867 0.42576072 0.42519572 0.4247013
 0.42504916 0.42452303 0.42452785 0.42440036 0.42418212 0.4243851
 0.42447174 0.42447066 0.42452708 0.42497024 0.42488796 0.4244371
 0.42366603 0.42310366 0.4229328  0.42326343 0.4239125  0.42549747
 0.42760643 0.428926   0.42907825 0.4291349  0.42882377 0.4284931
 0.42860726 0.42800373 0.4282726  0.42844132 0.42839447 0.42877394
 0.42916474 0.42931092 0.42961347 0.43028536 0.4304954  0.43081796
 0.43106985 0.4313644  0.43196085 0.43306446 0.43480483 0.4375051
 0.4406969  0.443141   0.44341335 0.44314036 0.44262382 0.4421258
 0.44205582 0.44153112 0.4418283  0.44187716 0.441998   0.44237524
 0.44249815 0.4426189  0.442744   0.44289774 0.4430791  0.44317994
 0.44292104 0.44287464 0.44272676 0.44259983 0.44261524 0.44280884
 0.4431841  0.44330046 0.4429079  0.4425664  0.44204038 0.44170606
 0.4417041  0.44129378 0.441623   0.4415921  0.44169533 0.44208372
 0.44208598 0.44213003 0.44229877 0.44244784 0.4425542  0.44268432
 0.44244865 0.4424709  0.44237027 0.44223234 0.44228855 0.44236878
 0.44255224 0.44253874 0.4421936  0.44193217 0.44144374 0.4411398
 0.44123527 0.4410087  0.4413812  0.4413846  0.44165027 0.44209638
 0.44220805 0.4424493  0.4426527  0.44278845 0.44282335 0.44273162
 0.44227275 0.44210666 0.44192672 0.4417583  0.4419233  0.44205475
 0.44228515 0.4424191  0.44200447 0.44165888 0.44121286 0.4408213
 0.44085723 0.44076082 0.44112042 0.4411714  0.4415822  0.4419936
 0.44215903 0.44238    0.44210845 0.44177982 0.44091123 0.438431
 0.43529776 0.43283045 0.43102005 0.42940316 0.4285761  0.4282651
 0.4282047  0.42816857 0.42778262 0.42721462 0.4268484  0.4265586
 0.42640102 0.42608893 0.426223   0.42589816 0.42592758 0.42616755
 0.42603287 0.4259049  0.42555454 0.42530215 0.42464995 0.42291382
 0.42046657 0.41889182 0.41771466 0.41676554 0.4166702  0.41714042
 0.41815615 0.41899112 0.41892835 0.4186545  0.41841894 0.4181315
 0.41818473 0.41799557 0.4180779  0.41776615 0.41783836 0.41803294
 0.4179761  0.41812128 0.41805977 0.4180628  0.41794354 0.4173372
 0.41603464 0.4155276  0.41541493 0.41546106 0.41651767 0.41821048
 0.42045128 0.42239815 0.42276332 0.42274225 0.42270187 0.42239955
 0.422323   0.42212093 0.42235255 0.42240068 0.42268756 0.4229976
 0.4232111  0.42349467 0.42369887 0.42408088 0.424282   0.4242987
 0.4241888  0.42462552 0.42509952 0.42607012 0.428063   0.4307197
 0.43404326 0.43682018 0.4372882  0.43699613 0.4367326  0.4362718
 0.4360911  0.4358382  0.43599558 0.4359774  0.43635    0.43668827
 0.43680143 0.43708032 0.4371634  0.43729994 0.43754524 0.43745512
 0.4370608  0.4370694  0.43680492 0.43666098 0.43687266 0.4370844
 0.43760914 0.43793654 0.43758    0.43715265 0.4367165  0.43624157
 0.4361552  0.43589282 0.43603727 0.4360424  0.43628538 0.43656766
 0.43663463 0.43679255 0.43683192 0.43693182 0.4371196  0.4370887
 0.4367875  0.43688345 0.43666595 0.43652713 0.43672636 0.43677562
 0.43710387 0.43722138 0.4368935  0.4366443  0.43620184 0.43576097
 0.43582782 0.43561038 0.43582323 0.4360226  0.43632087 0.43663925
 0.43688428 0.4371623  0.43726125 0.43743196 0.4375229  0.43724868
 0.43677202 0.43663874 0.43631735 0.43619674 0.4364792  0.43658268
 0.43699127 0.4371864  0.43671593 0.43640247 0.4359961  0.43546543
 0.43552557 0.43533164 0.4354901  0.43569332 0.43607226 0.43642727
 0.4366276  0.43669748 0.43629107 0.43593222 0.43494117 0.43212432
 0.42896956 0.42649296 0.42432955 0.42286956 0.4221769  0.4218386
 0.4221027  0.42215458 0.42175537 0.42130435 0.4208335  0.42032287
 0.4202263  0.4198719  0.41985407 0.4196714  0.41969466 0.4198054
 0.41974804 0.41972107 0.41929516 0.41900694 0.41829312 0.4162158
 0.41366744 0.4119275  0.41053733 0.40968665 0.40950683 0.41008535
 0.41140175 0.4121869  0.41209713 0.4118989  0.41161337 0.41119125
 0.41130984 0.41104785 0.4110616  0.41095006 0.4109722  0.4111706
 0.41121444 0.41135433 0.41127315 0.4113716  0.41119057 0.410376
 0.4092205  0.40858677 0.40835792 0.40864515 0.40974373 0.41165736
 0.41412207 0.4161013  0.41662833 0.41672572 0.4166169  0.41627067
 0.41630256 0.4159795  0.4161172  0.41625693 0.4164511  0.4168496
 0.41723505 0.41749123 0.4176512  0.4181692  0.41836756 0.4182922
 0.41833937 0.41868156 0.41919252 0.4204434  0.42254412 0.4254711
 0.42902732 0.43186465 0.43251398 0.43246284 0.43221337 0.43169993
 0.43160942 0.4312694  0.43147323 0.4315503  0.43174887 0.43214852
 0.43233195 0.43253264 0.4326442  0.43281886 0.43304288 0.43295017
 0.43263546 0.43253475 0.43231836 0.4322853  0.4325     0.43282142
 0.43339455 0.4336913  0.43339893 0.4330498  0.4325651  0.43214956
 0.4321031  0.43164054 0.43181577 0.4318783  0.43195635 0.43238002
 0.4325455  0.4325676  0.43262297 0.43278643 0.43297422 0.4329988
 0.43275097 0.43269837 0.43260038 0.43260482 0.4327165  0.43283048
 0.43318084 0.43325418 0.43294302 0.43270645 0.43220958 0.43180996
 0.43192667 0.43161768 0.4318875  0.43203086 0.43222266 0.43266714
 0.43288225 0.43311226 0.43334043 0.43356377 0.43363443 0.43343168
 0.4330272  0.43281597 0.43263945 0.4325461  0.43271384 0.43291506
 0.43326724 0.43340045 0.43304327 0.43277395 0.43233478 0.4319511
 0.4320062  0.43178198 0.4321003  0.4322175  0.43250963 0.43294436
 0.4331539  0.43325466 0.43292293 0.4326307  0.43169066 0.4289544
 0.4258273  0.42312613 0.42114684 0.41973972 0.41897038 0.41878545
 0.41884983 0.41877168 0.418455   0.4179579  0.41757175 0.41727367
 0.4171114  0.4167294  0.41684183 0.41651988 0.4164749  0.4167031
 0.41661754 0.41646022 0.41605744 0.4158338  0.41511407 0.41314715
 0.41058624 0.40866125 0.4074882  0.4066051  0.4063863  0.40700892
 0.40800852 0.40869957 0.40867677 0.40836063 0.40805358 0.40782726
 0.40789488 0.40760708 0.40773067 0.40743843 0.40736124 0.4075944
 0.4075615  0.40764537 0.4076527  0.4077762  0.40761924 0.40690887
 0.40566635 0.40493098 0.40482384 0.4050006  0.40612718 0.40792578
 0.41012686 0.41209003 0.41261566 0.41253364 0.41239345 0.4121334
 0.4119907  0.4116756  0.4119221  0.41183648 0.41195643 0.4122496
 0.41242057 0.4126936  0.41292235 0.4133707  0.41366577 0.41371223
 0.41364908 0.41405013 0.41473866 0.4158334  0.41793036 0.42070088
 0.42402136 0.4268366  0.4274691  0.4272959  0.42704904 0.42668524
 0.42642853 0.4261187  0.42634624 0.42625183 0.4265448  0.4268734
 0.42693308 0.42719853 0.42730188 0.42740676 0.42765263 0.427563
 0.4272096  0.42715675 0.42700127 0.42691875 0.42719018 0.42742148
 0.42784217 0.4281081  0.42780548 0.42745963 0.4270969  0.42672905
 0.42652023 0.42625305 0.4264367  0.42634752 0.42660078 0.42686003
 0.42686316 0.4270584  0.42707872 0.42710322 0.42732525 0.4273411
 0.42706993 0.42719012 0.4271257  0.4270215  0.42729446 0.42738956
 0.42759594 0.42764395 0.42727157 0.42703402 0.42667213 0.4221479
 0.4263059  0.42616165 0.42216888 0.42213377 0.42245215 0.42269987
 0.4227846  0.42312357 0.4253491  0.42339534 0.4235408  0.4232673
 0.4228318  0.4228384  0.422656   0.42256212 0.42283684 0.4228824
 0.4231942  0.4233297  0.4228728  0.4226359  0.42231777 0.4218166
 0.4218562  0.42182928 0.42188904 0.42203635 0.42233312 0.4224233
 0.42256632 0.42264968 0.42229563 0.421962   0.42105606 0.4183128
 0.41513798 0.41281542 0.41081405 0.40945497 0.40874815 0.40819946
 0.4083341  0.40818036 0.40758976 0.40722242 0.40685457 0.4063744
 0.40633112 0.405974   0.40576327 0.40557784 0.4054478  0.40543905
 0.4054115  0.40518752 0.40475655 0.40469527 0.40421227 0.40224147
 0.40002027 0.39846972 0.39703616 0.3965285  0.39634234 0.39678767
 0.39789474 0.39811835 0.3979469  0.39766768 0.39715946 0.39681074
 0.39688042 0.39619827 0.39630103 0.39620066 0.39568532 0.39619467
 0.39612484 0.3961246  0.39659968 0.3968438  0.39753714 0.39693367]
