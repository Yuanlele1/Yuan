Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=210, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7902720.0
params:  9030.0
Trainable parameters:  9030
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9517536163330078
Epoch: 1, Steps: 60 | Train Loss: 1.2134406 Vali Loss: 0.7960566 Test Loss: 0.5766687
Validation loss decreased (inf --> 0.796057).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.215531349182129
Epoch: 2, Steps: 60 | Train Loss: 1.0155050 Vali Loss: 0.7249583 Test Loss: 0.4989063
Validation loss decreased (0.796057 --> 0.724958).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.8697571754455566
Epoch: 3, Steps: 60 | Train Loss: 0.9248464 Vali Loss: 0.6873021 Test Loss: 0.4609875
Validation loss decreased (0.724958 --> 0.687302).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.473712921142578
Epoch: 4, Steps: 60 | Train Loss: 0.8806836 Vali Loss: 0.6723394 Test Loss: 0.4407567
Validation loss decreased (0.687302 --> 0.672339).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.3147084712982178
Epoch: 5, Steps: 60 | Train Loss: 0.8573565 Vali Loss: 0.6560880 Test Loss: 0.4290974
Validation loss decreased (0.672339 --> 0.656088).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.471489191055298
Epoch: 6, Steps: 60 | Train Loss: 0.8443671 Vali Loss: 0.6503953 Test Loss: 0.4223425
Validation loss decreased (0.656088 --> 0.650395).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.4408295154571533
Epoch: 7, Steps: 60 | Train Loss: 0.8355480 Vali Loss: 0.6452091 Test Loss: 0.4181048
Validation loss decreased (0.650395 --> 0.645209).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.3933398723602295
Epoch: 8, Steps: 60 | Train Loss: 0.8329036 Vali Loss: 0.6446531 Test Loss: 0.4153186
Validation loss decreased (0.645209 --> 0.644653).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.23645281791687
Epoch: 9, Steps: 60 | Train Loss: 0.8287300 Vali Loss: 0.6440641 Test Loss: 0.4135153
Validation loss decreased (0.644653 --> 0.644064).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.233987331390381
Epoch: 10, Steps: 60 | Train Loss: 0.8268603 Vali Loss: 0.6396954 Test Loss: 0.4121850
Validation loss decreased (0.644064 --> 0.639695).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.627021551132202
Epoch: 11, Steps: 60 | Train Loss: 0.8253337 Vali Loss: 0.6407436 Test Loss: 0.4112423
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.589709758758545
Epoch: 12, Steps: 60 | Train Loss: 0.8215259 Vali Loss: 0.6343024 Test Loss: 0.4104726
Validation loss decreased (0.639695 --> 0.634302).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.4740564823150635
Epoch: 13, Steps: 60 | Train Loss: 0.8209775 Vali Loss: 0.6371692 Test Loss: 0.4099136
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.360900640487671
Epoch: 14, Steps: 60 | Train Loss: 0.8203407 Vali Loss: 0.6344392 Test Loss: 0.4094231
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.3609976768493652
Epoch: 15, Steps: 60 | Train Loss: 0.8202692 Vali Loss: 0.6341908 Test Loss: 0.4090268
Validation loss decreased (0.634302 --> 0.634191).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.0550127029418945
Epoch: 16, Steps: 60 | Train Loss: 0.8204051 Vali Loss: 0.6339290 Test Loss: 0.4087137
Validation loss decreased (0.634191 --> 0.633929).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.0440666675567627
Epoch: 17, Steps: 60 | Train Loss: 0.8194729 Vali Loss: 0.6319071 Test Loss: 0.4084565
Validation loss decreased (0.633929 --> 0.631907).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.544323444366455
Epoch: 18, Steps: 60 | Train Loss: 0.8186113 Vali Loss: 0.6345845 Test Loss: 0.4082147
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.635183334350586
Epoch: 19, Steps: 60 | Train Loss: 0.8187434 Vali Loss: 0.6349361 Test Loss: 0.4080010
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.534674882888794
Epoch: 20, Steps: 60 | Train Loss: 0.8167513 Vali Loss: 0.6317439 Test Loss: 0.4078140
Validation loss decreased (0.631907 --> 0.631744).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.2050416469573975
Epoch: 21, Steps: 60 | Train Loss: 0.8192136 Vali Loss: 0.6290220 Test Loss: 0.4077038
Validation loss decreased (0.631744 --> 0.629022).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.8410699367523193
Epoch: 22, Steps: 60 | Train Loss: 0.8166886 Vali Loss: 0.6348462 Test Loss: 0.4075400
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.339712619781494
Epoch: 23, Steps: 60 | Train Loss: 0.8167598 Vali Loss: 0.6324599 Test Loss: 0.4074056
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.784379005432129
Epoch: 24, Steps: 60 | Train Loss: 0.8155759 Vali Loss: 0.6373929 Test Loss: 0.4073151
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.367894172668457
Epoch: 25, Steps: 60 | Train Loss: 0.8160626 Vali Loss: 0.6280184 Test Loss: 0.4072246
Validation loss decreased (0.629022 --> 0.628018).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.29097580909729
Epoch: 26, Steps: 60 | Train Loss: 0.8137622 Vali Loss: 0.6345693 Test Loss: 0.4071219
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.545095920562744
Epoch: 27, Steps: 60 | Train Loss: 0.8158595 Vali Loss: 0.6306484 Test Loss: 0.4070570
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.18886661529541
Epoch: 28, Steps: 60 | Train Loss: 0.8156911 Vali Loss: 0.6314671 Test Loss: 0.4069856
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.492335796356201
Epoch: 29, Steps: 60 | Train Loss: 0.8172292 Vali Loss: 0.6310968 Test Loss: 0.4069011
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.308901309967041
Epoch: 30, Steps: 60 | Train Loss: 0.8165024 Vali Loss: 0.6294413 Test Loss: 0.4068359
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.9654860496520996
Epoch: 31, Steps: 60 | Train Loss: 0.8154961 Vali Loss: 0.6284344 Test Loss: 0.4067846
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.0408987998962402
Epoch: 32, Steps: 60 | Train Loss: 0.8148893 Vali Loss: 0.6311972 Test Loss: 0.4067390
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.2699694633483887
Epoch: 33, Steps: 60 | Train Loss: 0.8154540 Vali Loss: 0.6298389 Test Loss: 0.4066880
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.614285469055176
Epoch: 34, Steps: 60 | Train Loss: 0.8156983 Vali Loss: 0.6320546 Test Loss: 0.4066357
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.5438642501831055
Epoch: 35, Steps: 60 | Train Loss: 0.8141096 Vali Loss: 0.6292681 Test Loss: 0.4065981
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.0795066356658936
Epoch: 36, Steps: 60 | Train Loss: 0.8150988 Vali Loss: 0.6312385 Test Loss: 0.4065673
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.712533712387085
Epoch: 37, Steps: 60 | Train Loss: 0.8149427 Vali Loss: 0.6277229 Test Loss: 0.4065289
Validation loss decreased (0.628018 --> 0.627723).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6157376766204834
Epoch: 38, Steps: 60 | Train Loss: 0.8150872 Vali Loss: 0.6317656 Test Loss: 0.4065115
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.017345905303955
Epoch: 39, Steps: 60 | Train Loss: 0.8143875 Vali Loss: 0.6210549 Test Loss: 0.4064714
Validation loss decreased (0.627723 --> 0.621055).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.4819939136505127
Epoch: 40, Steps: 60 | Train Loss: 0.8131493 Vali Loss: 0.6251855 Test Loss: 0.4064357
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.3772761821746826
Epoch: 41, Steps: 60 | Train Loss: 0.8140266 Vali Loss: 0.6279320 Test Loss: 0.4064165
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.502922296524048
Epoch: 42, Steps: 60 | Train Loss: 0.8162079 Vali Loss: 0.6309875 Test Loss: 0.4064034
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.1881728172302246
Epoch: 43, Steps: 60 | Train Loss: 0.8148594 Vali Loss: 0.6319794 Test Loss: 0.4063715
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.361793041229248
Epoch: 44, Steps: 60 | Train Loss: 0.8149080 Vali Loss: 0.6300380 Test Loss: 0.4063509
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.1655378341674805
Epoch: 45, Steps: 60 | Train Loss: 0.8127692 Vali Loss: 0.6268797 Test Loss: 0.4063411
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.4525158405303955
Epoch: 46, Steps: 60 | Train Loss: 0.8132687 Vali Loss: 0.6304126 Test Loss: 0.4063146
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.136078119277954
Epoch: 47, Steps: 60 | Train Loss: 0.8123907 Vali Loss: 0.6308184 Test Loss: 0.4062969
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.3971428871154785
Epoch: 48, Steps: 60 | Train Loss: 0.8150211 Vali Loss: 0.6314541 Test Loss: 0.4062882
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.3515801429748535
Epoch: 49, Steps: 60 | Train Loss: 0.8139582 Vali Loss: 0.6279007 Test Loss: 0.4062706
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3151514530181885
Epoch: 50, Steps: 60 | Train Loss: 0.8138886 Vali Loss: 0.6278977 Test Loss: 0.4062604
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.1973092555999756
Epoch: 51, Steps: 60 | Train Loss: 0.8142871 Vali Loss: 0.6286169 Test Loss: 0.4062473
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.9380719661712646
Epoch: 52, Steps: 60 | Train Loss: 0.8139163 Vali Loss: 0.6306287 Test Loss: 0.4062272
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.760679006576538
Epoch: 53, Steps: 60 | Train Loss: 0.8137344 Vali Loss: 0.6274772 Test Loss: 0.4062183
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.50256085395813
Epoch: 54, Steps: 60 | Train Loss: 0.8150228 Vali Loss: 0.6247651 Test Loss: 0.4062082
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.0667037963867188
Epoch: 55, Steps: 60 | Train Loss: 0.8123822 Vali Loss: 0.6279655 Test Loss: 0.4062003
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.0920162200927734
Epoch: 56, Steps: 60 | Train Loss: 0.8144282 Vali Loss: 0.6318668 Test Loss: 0.4061894
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.313258409500122
Epoch: 57, Steps: 60 | Train Loss: 0.8127792 Vali Loss: 0.6271855 Test Loss: 0.4061778
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.9957754611968994
Epoch: 58, Steps: 60 | Train Loss: 0.8150871 Vali Loss: 0.6286225 Test Loss: 0.4061679
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.0973663330078125
Epoch: 59, Steps: 60 | Train Loss: 0.8129834 Vali Loss: 0.6292737 Test Loss: 0.4061621
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4046306908130646, mae:0.4309277832508087, rse:0.5084347128868103, corr:[ 2.17138261e-01  2.22185791e-01  2.19668120e-01  2.18443543e-01
  2.18774378e-01  2.17778355e-01  2.15854242e-01  2.15111554e-01
  2.14958161e-01  2.13699892e-01  2.11955503e-01  2.10896417e-01
  2.10260525e-01  2.09188044e-01  2.08010033e-01  2.07593277e-01
  2.07488298e-01  2.06710473e-01  2.05412328e-01  2.04521522e-01
  2.04290420e-01  2.03962296e-01  2.02908918e-01  2.01204166e-01
  1.99408665e-01  1.97911948e-01  1.96694598e-01  1.95653945e-01
  1.94590867e-01  1.93388700e-01  1.92109615e-01  1.90840259e-01
  1.89556673e-01  1.88286707e-01  1.87213838e-01  1.86325848e-01
  1.85342506e-01  1.84052005e-01  1.82948813e-01  1.82442456e-01
  1.82166472e-01  1.81491271e-01  1.80515006e-01  1.79934248e-01
  1.79706842e-01  1.79173872e-01  1.77982748e-01  1.76111832e-01
  1.74037486e-01  1.71999112e-01  1.70021892e-01  1.68808788e-01
  1.68073386e-01  1.67255402e-01  1.65863663e-01  1.64617062e-01
  1.64277658e-01  1.64204210e-01  1.63821369e-01  1.62966818e-01
  1.62665442e-01  1.62642092e-01  1.62677526e-01  1.62320033e-01
  1.62155256e-01  1.62183002e-01  1.62061423e-01  1.61804050e-01
  1.61944106e-01  1.62304327e-01  1.62046030e-01  1.61019698e-01
  1.59751758e-01  1.59048975e-01  1.58443272e-01  1.57593355e-01
  1.56897381e-01  1.56925038e-01  1.57046959e-01  1.56710565e-01
  1.56261131e-01  1.56084359e-01  1.56341925e-01  1.56383216e-01
  1.56153351e-01  1.55833870e-01  1.55946359e-01  1.56104147e-01
  1.55860141e-01  1.55272871e-01  1.55032739e-01  1.55274376e-01
  1.55687571e-01  1.55732736e-01  1.55493602e-01  1.55099854e-01
  1.54294357e-01  1.53428078e-01  1.52622044e-01  1.52427912e-01
  1.52313396e-01  1.51733682e-01  1.51088104e-01  1.50907546e-01
  1.51267827e-01  1.51394576e-01  1.51290625e-01  1.51181459e-01
  1.51543856e-01  1.51674166e-01  1.51331007e-01  1.50757924e-01
  1.50627717e-01  1.50821924e-01  1.50767207e-01  1.50354937e-01
  1.50167391e-01  1.50237203e-01  1.50060713e-01  1.49026349e-01
  1.47452384e-01  1.46318793e-01  1.45519346e-01  1.44824520e-01
  1.43886089e-01  1.43076450e-01  1.42517954e-01  1.41947031e-01
  1.41305000e-01  1.40770063e-01  1.40746638e-01  1.40618518e-01
  1.40080035e-01  1.39335290e-01  1.39046535e-01  1.39144152e-01
  1.38999432e-01  1.38291344e-01  1.37574151e-01  1.37452573e-01
  1.37763962e-01  1.37863398e-01  1.37356028e-01  1.36387780e-01
  1.34632915e-01  1.33043006e-01  1.31715477e-01  1.31236523e-01
  1.31051764e-01  1.30497336e-01  1.29561856e-01  1.28827065e-01
  1.28856137e-01  1.29099905e-01  1.29190519e-01  1.28857240e-01
  1.28816947e-01  1.28702134e-01  1.28398299e-01  1.28040776e-01
  1.27958521e-01  1.27896100e-01  1.27644598e-01  1.27392665e-01
  1.27582029e-01  1.28070757e-01  1.27973020e-01  1.26904070e-01
  1.25178069e-01  1.24233492e-01  1.23761281e-01  1.23510055e-01
  1.23031817e-01  1.22426137e-01  1.21861540e-01  1.21314228e-01
  1.20975196e-01  1.20700918e-01  1.21011585e-01  1.21201225e-01
  1.21120319e-01  1.20708674e-01  1.20602004e-01  1.20785475e-01
  1.20984316e-01  1.20858401e-01  1.20778568e-01  1.20905332e-01
  1.21185802e-01  1.21480621e-01  1.21562444e-01  1.21551991e-01
  1.21269770e-01  1.20985553e-01  1.20655850e-01  1.20887771e-01
  1.21330164e-01  1.21586822e-01  1.21662393e-01  1.21550508e-01
  1.21654652e-01  1.21717967e-01  1.22092567e-01  1.22245766e-01
  1.22417189e-01  1.22227758e-01  1.21944211e-01  1.21833257e-01
  1.22049756e-01  1.22388825e-01  1.22552820e-01  1.22753248e-01
  1.23159692e-01  1.23562932e-01  1.23589940e-01  1.22931428e-01
  1.21890441e-01  1.21201478e-01  1.20860450e-01  1.20703474e-01
  1.20376796e-01  1.20308995e-01  1.20268494e-01  1.20395206e-01
  1.20779790e-01  1.21365890e-01  1.21996790e-01  1.22170351e-01
  1.22361019e-01  1.22591347e-01  1.22985944e-01  1.23248681e-01
  1.23530120e-01  1.23969868e-01  1.24529138e-01  1.24892950e-01
  1.25055924e-01  1.25332355e-01  1.25812277e-01  1.26222670e-01
  1.25870883e-01  1.25318050e-01  1.24828964e-01  1.24986418e-01
  1.25127718e-01  1.25050142e-01  1.24878950e-01  1.25255764e-01
  1.26143008e-01  1.26895651e-01  1.27989009e-01  1.28721535e-01
  1.29299417e-01  1.29459783e-01  1.29723176e-01  1.30244061e-01
  1.30721197e-01  1.30895659e-01  1.30987823e-01  1.31415457e-01
  1.32136092e-01  1.32686540e-01  1.33051440e-01  1.33457810e-01
  1.33641019e-01  1.33786336e-01  1.33575976e-01  1.33917764e-01
  1.34385780e-01  1.35101989e-01  1.35413483e-01  1.35656342e-01
  1.36453256e-01  1.37301564e-01  1.38112009e-01  1.38608754e-01
  1.39618054e-01  1.40451834e-01  1.40732333e-01  1.40533760e-01
  1.40778318e-01  1.41554117e-01  1.42176107e-01  1.42305270e-01
  1.42443031e-01  1.43043041e-01  1.43846199e-01  1.44228667e-01
  1.44034892e-01  1.43972933e-01  1.43965095e-01  1.44221708e-01
  1.44146323e-01  1.44302592e-01  1.44763798e-01  1.45175785e-01
  1.45820707e-01  1.46552294e-01  1.47709608e-01  1.48670122e-01
  1.49255425e-01  1.49353400e-01  1.49835408e-01  1.50562137e-01
  1.51040882e-01  1.51347518e-01  1.51801959e-01  1.52356103e-01
  1.52474195e-01  1.52281553e-01  1.52543008e-01  1.53264135e-01
  1.53498396e-01  1.53012678e-01  1.52292624e-01  1.52302772e-01
  1.52534410e-01  1.52731761e-01  1.52552575e-01  1.53127804e-01
  1.54195875e-01  1.54669315e-01  1.54685646e-01  1.54997766e-01
  1.56221554e-01  1.56967461e-01  1.57218277e-01  1.57404870e-01
  1.58059105e-01  1.58680201e-01  1.58837765e-01  1.58911362e-01
  1.59482941e-01  1.60363778e-01  1.60787553e-01  1.60642758e-01
  1.60412326e-01  1.60732552e-01  1.61013350e-01  1.61112607e-01
  1.61201030e-01  1.61623374e-01  1.62235871e-01  1.62382454e-01
  1.62430391e-01  1.62887365e-01  1.63843974e-01  1.64480358e-01
  1.64861709e-01  1.65153950e-01  1.65733352e-01  1.66209430e-01
  1.66372642e-01  1.66575983e-01  1.67213887e-01  1.67951941e-01
  1.68355525e-01  1.68643951e-01  1.69250920e-01  1.69780150e-01
  1.69753984e-01  1.69265077e-01  1.68941364e-01  1.69195071e-01
  1.69667467e-01  1.70064405e-01  1.70550480e-01  1.71343327e-01
  1.72117561e-01  1.72744945e-01  1.73271850e-01  1.73804864e-01
  1.74428120e-01  1.74688965e-01  1.74918994e-01  1.75314352e-01
  1.75733283e-01  1.75865918e-01  1.75794765e-01  1.75976068e-01
  1.76495224e-01  1.77048519e-01  1.77341864e-01  1.77473828e-01
  1.77402288e-01  1.77294269e-01  1.76980868e-01  1.76976576e-01
  1.77094683e-01  1.77417964e-01  1.77834332e-01  1.78310424e-01
  1.78995535e-01  1.79586858e-01  1.80247784e-01  1.80393994e-01
  1.80530280e-01  1.80485189e-01  1.80509582e-01  1.80344746e-01
  1.80025190e-01  1.79880023e-01  1.79999724e-01  1.80114076e-01
  1.80004060e-01  1.79822415e-01  1.79818049e-01  1.79850057e-01
  1.79519758e-01  1.79147735e-01  1.78908363e-01  1.79024786e-01
  1.79064780e-01  1.78952441e-01  1.78811416e-01  1.78914040e-01
  1.79009587e-01  1.78809956e-01  1.78439975e-01  1.77951157e-01
  1.77578241e-01  1.77157551e-01  1.76714182e-01  1.76087916e-01
  1.75418451e-01  1.74841642e-01  1.74349710e-01  1.74043626e-01
  1.73993200e-01  1.74100608e-01  1.73956826e-01  1.73308387e-01
  1.72463074e-01  1.71961382e-01  1.71540052e-01  1.70913026e-01
  1.70042709e-01  1.69356778e-01  1.68973878e-01  1.68480188e-01
  1.67830840e-01  1.67181283e-01  1.66930437e-01  1.66504517e-01
  1.65906966e-01  1.65260002e-01  1.64975911e-01  1.64862707e-01
  1.64461464e-01  1.63986757e-01  1.63874865e-01  1.64160147e-01
  1.64219573e-01  1.64026424e-01  1.63891077e-01  1.63804337e-01
  1.63302362e-01  1.62610263e-01  1.61938563e-01  1.61858976e-01
  1.61617547e-01  1.60853133e-01  1.59899980e-01  1.59501359e-01
  1.59241349e-01  1.58634931e-01  1.58010796e-01  1.57513887e-01
  1.57264501e-01  1.56554520e-01  1.55613869e-01  1.54935449e-01
  1.54823810e-01  1.54774904e-01  1.54415101e-01  1.53947160e-01
  1.53876364e-01  1.53953955e-01  1.53444692e-01  1.52341872e-01
  1.51233181e-01  1.50456786e-01  1.49461240e-01  1.48147658e-01
  1.46951467e-01  1.46154091e-01  1.45234764e-01  1.43976912e-01
  1.42963931e-01  1.42270312e-01  1.41767129e-01  1.40708059e-01
  1.39645368e-01  1.39036953e-01  1.38799071e-01  1.38256297e-01
  1.37415826e-01  1.36992827e-01  1.36931211e-01  1.36728823e-01
  1.36379704e-01  1.36195317e-01  1.36031151e-01  1.35089725e-01
  1.33133620e-01  1.31255865e-01  1.29971981e-01  1.28971875e-01
  1.27689764e-01  1.26457438e-01  1.25591800e-01  1.25269532e-01
  1.24749541e-01  1.23834789e-01  1.23220772e-01  1.22773543e-01
  1.22193471e-01  1.20968409e-01  1.19917296e-01  1.19735122e-01
  1.20051563e-01  1.19776160e-01  1.18832372e-01  1.18195467e-01
  1.18091211e-01  1.17972933e-01  1.17083758e-01  1.15595780e-01
  1.13857336e-01  1.12198092e-01  1.10260025e-01  1.08302288e-01
  1.06417902e-01  1.04572751e-01  1.02806240e-01  1.01431288e-01
  1.00657515e-01  9.98435095e-02  9.86623093e-02  9.70137790e-02
  9.57529694e-02  9.48502794e-02  9.42016765e-02  9.34051275e-02
  9.27570835e-02  9.23048183e-02  9.16265175e-02  9.07587484e-02
  9.01004374e-02  8.96181166e-02  8.85545388e-02  8.65393057e-02
  8.42823237e-02  8.27168077e-02  8.12610835e-02  7.96027705e-02
  7.78608769e-02  7.66676962e-02  7.57721364e-02  7.44881779e-02
  7.33005181e-02  7.25762323e-02  7.24972785e-02  7.18221515e-02
  7.05733597e-02  6.95344284e-02  6.94766864e-02  6.94808289e-02
  6.87014312e-02  6.75259158e-02  6.70837611e-02  6.73030093e-02
  6.71815723e-02  6.63729012e-02  6.53268546e-02  6.40140921e-02
  6.18788898e-02  5.96073046e-02  5.79618923e-02  5.71204275e-02
  5.58840707e-02  5.36307804e-02  5.13857007e-02  5.02594076e-02
  4.98416275e-02  4.88534309e-02  4.73378822e-02  4.62787524e-02
  4.61149737e-02  4.56437394e-02  4.46079038e-02  4.38891165e-02
  4.41372581e-02  4.43536304e-02  4.36102226e-02  4.27209511e-02
  4.26767208e-02  4.28681858e-02  4.18916233e-02  3.99356112e-02
  3.80083062e-02  3.65152918e-02  3.44257914e-02  3.22870240e-02
  3.05583440e-02  2.94437092e-02  2.76552401e-02  2.53643561e-02
  2.37520598e-02  2.35054698e-02  2.37922352e-02  2.30090357e-02
  2.20721196e-02  2.18912046e-02  2.26486400e-02  2.26945970e-02
  2.18495335e-02  2.15088297e-02  2.24405359e-02  2.32748240e-02
  2.30103601e-02  2.24302318e-02  2.19778027e-02  2.12940387e-02
  1.93002764e-02  1.73529126e-02  1.59156136e-02  1.49050104e-02
  1.34155946e-02  1.17309475e-02  1.08109601e-02  1.06652724e-02
  1.07272305e-02  1.02265868e-02  9.93992668e-03  1.03104208e-02
  1.09282220e-02  1.03453035e-02  9.37470142e-03  9.46010835e-03
  1.02753090e-02  1.03530129e-02  9.25056171e-03  8.15809518e-03
  8.45351722e-03  9.14188661e-03  8.77461303e-03  7.25627039e-03
  5.29051898e-03  3.76908435e-03  2.12123035e-03  8.68917152e-04
  2.78609132e-05 -6.30038150e-04 -1.95702142e-03 -3.39952204e-03
 -3.87087394e-03 -3.76380654e-03 -3.42539931e-03 -4.05706419e-03
 -4.38332977e-03 -4.17532772e-03 -3.63398390e-03 -3.69267608e-03
 -3.80743830e-03 -3.64467083e-03 -3.38091981e-03 -3.55872000e-03
 -3.91052524e-03 -3.70688573e-03 -3.79616581e-03 -4.75138798e-03
 -6.43639127e-03 -7.91076105e-03 -9.61480569e-03 -1.13852601e-02
 -1.29838996e-02 -1.40856523e-02 -1.49304830e-02 -1.55774979e-02
 -1.56312585e-02 -1.51204700e-02 -1.41763855e-02 -1.39399823e-02
 -1.39388107e-02 -1.45049775e-02 -1.41600929e-02 -1.33431945e-02
 -1.26641672e-02 -1.25966808e-02 -1.27363261e-02 -1.26728350e-02
 -1.24728521e-02 -1.20563861e-02 -1.25711737e-02 -1.41329402e-02
 -1.66698825e-02 -1.82037409e-02 -1.88863538e-02 -1.94429606e-02
 -2.11054683e-02 -2.25480460e-02 -2.31929962e-02 -2.32014060e-02
 -2.29297802e-02 -2.29279138e-02 -2.26800349e-02 -2.25225277e-02
 -2.15641372e-02 -2.17006058e-02 -2.18741540e-02 -2.22830251e-02
 -2.19494179e-02 -2.14424562e-02 -2.22221743e-02 -2.44023819e-02
 -2.67039873e-02 -2.68946495e-02 -2.69616079e-02 -2.78893039e-02]
