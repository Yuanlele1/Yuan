Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=30, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=30, out_features=270, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7257600.0
params:  8370.0
Trainable parameters:  8370
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.8961174488067627
Epoch: 1, Steps: 61 | Train Loss: 1.3445763 Vali Loss: 0.8446838 Test Loss: 0.6732123
Validation loss decreased (inf --> 0.844684).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.723440885543823
Epoch: 2, Steps: 61 | Train Loss: 1.1252809 Vali Loss: 0.7641224 Test Loss: 0.5725786
Validation loss decreased (0.844684 --> 0.764122).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.565337419509888
Epoch: 3, Steps: 61 | Train Loss: 1.0161119 Vali Loss: 0.7147278 Test Loss: 0.5177101
Validation loss decreased (0.764122 --> 0.714728).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 4.411368131637573
Epoch: 4, Steps: 61 | Train Loss: 0.9601716 Vali Loss: 0.6898647 Test Loss: 0.4850459
Validation loss decreased (0.714728 --> 0.689865).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.9936983585357666
Epoch: 5, Steps: 61 | Train Loss: 0.9233258 Vali Loss: 0.6723705 Test Loss: 0.4650664
Validation loss decreased (0.689865 --> 0.672370).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.197758913040161
Epoch: 6, Steps: 61 | Train Loss: 0.9038887 Vali Loss: 0.6587769 Test Loss: 0.4523142
Validation loss decreased (0.672370 --> 0.658777).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.6128101348876953
Epoch: 7, Steps: 61 | Train Loss: 0.8896351 Vali Loss: 0.6528359 Test Loss: 0.4439518
Validation loss decreased (0.658777 --> 0.652836).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.6568918228149414
Epoch: 8, Steps: 61 | Train Loss: 0.8806981 Vali Loss: 0.6489090 Test Loss: 0.4383948
Validation loss decreased (0.652836 --> 0.648909).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.4022905826568604
Epoch: 9, Steps: 61 | Train Loss: 0.8734719 Vali Loss: 0.6401471 Test Loss: 0.4345627
Validation loss decreased (0.648909 --> 0.640147).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2418909072875977
Epoch: 10, Steps: 61 | Train Loss: 0.8708126 Vali Loss: 0.6434650 Test Loss: 0.4318567
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.175173044204712
Epoch: 11, Steps: 61 | Train Loss: 0.8664825 Vali Loss: 0.6400845 Test Loss: 0.4299454
Validation loss decreased (0.640147 --> 0.640085).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3214237689971924
Epoch: 12, Steps: 61 | Train Loss: 0.8647901 Vali Loss: 0.6421961 Test Loss: 0.4285390
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.830179691314697
Epoch: 13, Steps: 61 | Train Loss: 0.8629500 Vali Loss: 0.6390463 Test Loss: 0.4274573
Validation loss decreased (0.640085 --> 0.639046).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.98787784576416
Epoch: 14, Steps: 61 | Train Loss: 0.8625990 Vali Loss: 0.6375687 Test Loss: 0.4266034
Validation loss decreased (0.639046 --> 0.637569).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.633995532989502
Epoch: 15, Steps: 61 | Train Loss: 0.8608633 Vali Loss: 0.6376977 Test Loss: 0.4259475
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.694146156311035
Epoch: 16, Steps: 61 | Train Loss: 0.8593832 Vali Loss: 0.6330964 Test Loss: 0.4253977
Validation loss decreased (0.637569 --> 0.633096).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.43910813331604
Epoch: 17, Steps: 61 | Train Loss: 0.8571761 Vali Loss: 0.6345069 Test Loss: 0.4249330
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 6.1152122020721436
Epoch: 18, Steps: 61 | Train Loss: 0.8576822 Vali Loss: 0.6326734 Test Loss: 0.4245716
Validation loss decreased (0.633096 --> 0.632673).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.234137773513794
Epoch: 19, Steps: 61 | Train Loss: 0.8566474 Vali Loss: 0.6319987 Test Loss: 0.4242477
Validation loss decreased (0.632673 --> 0.631999).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.8941550254821777
Epoch: 20, Steps: 61 | Train Loss: 0.8568710 Vali Loss: 0.6327896 Test Loss: 0.4239724
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.483088731765747
Epoch: 21, Steps: 61 | Train Loss: 0.8567280 Vali Loss: 0.6347823 Test Loss: 0.4237193
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.3256094455718994
Epoch: 22, Steps: 61 | Train Loss: 0.8548315 Vali Loss: 0.6308444 Test Loss: 0.4235126
Validation loss decreased (0.631999 --> 0.630844).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.223696231842041
Epoch: 23, Steps: 61 | Train Loss: 0.8562655 Vali Loss: 0.6308985 Test Loss: 0.4233338
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.388700246810913
Epoch: 24, Steps: 61 | Train Loss: 0.8557648 Vali Loss: 0.6315325 Test Loss: 0.4231587
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.5254533290863037
Epoch: 25, Steps: 61 | Train Loss: 0.8535144 Vali Loss: 0.6324232 Test Loss: 0.4229833
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.067568778991699
Epoch: 26, Steps: 61 | Train Loss: 0.8535953 Vali Loss: 0.6301459 Test Loss: 0.4228604
Validation loss decreased (0.630844 --> 0.630146).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.310093402862549
Epoch: 27, Steps: 61 | Train Loss: 0.8541597 Vali Loss: 0.6338081 Test Loss: 0.4227129
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 4.200067520141602
Epoch: 28, Steps: 61 | Train Loss: 0.8546300 Vali Loss: 0.6282494 Test Loss: 0.4226190
Validation loss decreased (0.630146 --> 0.628249).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.8085520267486572
Epoch: 29, Steps: 61 | Train Loss: 0.8539461 Vali Loss: 0.6271833 Test Loss: 0.4225120
Validation loss decreased (0.628249 --> 0.627183).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.3655526638031006
Epoch: 30, Steps: 61 | Train Loss: 0.8545624 Vali Loss: 0.6305492 Test Loss: 0.4224072
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.0667035579681396
Epoch: 31, Steps: 61 | Train Loss: 0.8534653 Vali Loss: 0.6287286 Test Loss: 0.4223152
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.556861877441406
Epoch: 32, Steps: 61 | Train Loss: 0.8540869 Vali Loss: 0.6257910 Test Loss: 0.4222276
Validation loss decreased (0.627183 --> 0.625791).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.028830051422119
Epoch: 33, Steps: 61 | Train Loss: 0.8532571 Vali Loss: 0.6270308 Test Loss: 0.4221672
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.191267251968384
Epoch: 34, Steps: 61 | Train Loss: 0.8518052 Vali Loss: 0.6305401 Test Loss: 0.4220822
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.7846055030822754
Epoch: 35, Steps: 61 | Train Loss: 0.8535843 Vali Loss: 0.6280468 Test Loss: 0.4220239
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.871830940246582
Epoch: 36, Steps: 61 | Train Loss: 0.8520238 Vali Loss: 0.6284955 Test Loss: 0.4219637
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 4.203205108642578
Epoch: 37, Steps: 61 | Train Loss: 0.8516238 Vali Loss: 0.6346059 Test Loss: 0.4219017
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.293835401535034
Epoch: 38, Steps: 61 | Train Loss: 0.8523338 Vali Loss: 0.6320701 Test Loss: 0.4218477
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.238961935043335
Epoch: 39, Steps: 61 | Train Loss: 0.8524369 Vali Loss: 0.6319326 Test Loss: 0.4217991
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.163694858551025
Epoch: 40, Steps: 61 | Train Loss: 0.8518642 Vali Loss: 0.6257145 Test Loss: 0.4217498
Validation loss decreased (0.625791 --> 0.625715).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 5.180056095123291
Epoch: 41, Steps: 61 | Train Loss: 0.8519313 Vali Loss: 0.6270331 Test Loss: 0.4217155
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.839349746704102
Epoch: 42, Steps: 61 | Train Loss: 0.8505310 Vali Loss: 0.6260270 Test Loss: 0.4216702
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.9470553398132324
Epoch: 43, Steps: 61 | Train Loss: 0.8496714 Vali Loss: 0.6280232 Test Loss: 0.4216326
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.3130955696105957
Epoch: 44, Steps: 61 | Train Loss: 0.8513981 Vali Loss: 0.6270285 Test Loss: 0.4215957
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.3171498775482178
Epoch: 45, Steps: 61 | Train Loss: 0.8509294 Vali Loss: 0.6260211 Test Loss: 0.4215640
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.4869191646575928
Epoch: 46, Steps: 61 | Train Loss: 0.8528876 Vali Loss: 0.6218663 Test Loss: 0.4215282
Validation loss decreased (0.625715 --> 0.621866).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.8451457023620605
Epoch: 47, Steps: 61 | Train Loss: 0.8518458 Vali Loss: 0.6266704 Test Loss: 0.4214973
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.1032021045684814
Epoch: 48, Steps: 61 | Train Loss: 0.8520378 Vali Loss: 0.6265157 Test Loss: 0.4214723
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 6.245874643325806
Epoch: 49, Steps: 61 | Train Loss: 0.8496025 Vali Loss: 0.6277314 Test Loss: 0.4214469
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 6.029148578643799
Epoch: 50, Steps: 61 | Train Loss: 0.8504671 Vali Loss: 0.6269248 Test Loss: 0.4214276
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 5.3919501304626465
Epoch: 51, Steps: 61 | Train Loss: 0.8525775 Vali Loss: 0.6283094 Test Loss: 0.4213964
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.555427551269531
Epoch: 52, Steps: 61 | Train Loss: 0.8511392 Vali Loss: 0.6280882 Test Loss: 0.4213730
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 4.1467766761779785
Epoch: 53, Steps: 61 | Train Loss: 0.8514203 Vali Loss: 0.6275520 Test Loss: 0.4213565
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 5.823702096939087
Epoch: 54, Steps: 61 | Train Loss: 0.8512114 Vali Loss: 0.6270587 Test Loss: 0.4213349
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.3272972106933594
Epoch: 55, Steps: 61 | Train Loss: 0.8522670 Vali Loss: 0.6291787 Test Loss: 0.4213162
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.9808359146118164
Epoch: 56, Steps: 61 | Train Loss: 0.8525451 Vali Loss: 0.6275760 Test Loss: 0.4213025
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.3382952213287354
Epoch: 57, Steps: 61 | Train Loss: 0.8505840 Vali Loss: 0.6295427 Test Loss: 0.4212855
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 4.015699625015259
Epoch: 58, Steps: 61 | Train Loss: 0.8503340 Vali Loss: 0.6276603 Test Loss: 0.4212683
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.5998692512512207
Epoch: 59, Steps: 61 | Train Loss: 0.8520033 Vali Loss: 0.6302119 Test Loss: 0.4212553
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.8607680797576904
Epoch: 60, Steps: 61 | Train Loss: 0.8501818 Vali Loss: 0.6284157 Test Loss: 0.4212403
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.8985538482666016
Epoch: 61, Steps: 61 | Train Loss: 0.8516846 Vali Loss: 0.6278182 Test Loss: 0.4212261
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.2701308727264404
Epoch: 62, Steps: 61 | Train Loss: 0.8513968 Vali Loss: 0.6279945 Test Loss: 0.4212154
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.021465539932251
Epoch: 63, Steps: 61 | Train Loss: 0.8521191 Vali Loss: 0.6264258 Test Loss: 0.4212010
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.144002676010132
Epoch: 64, Steps: 61 | Train Loss: 0.8512019 Vali Loss: 0.6286120 Test Loss: 0.4211929
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.665494918823242
Epoch: 65, Steps: 61 | Train Loss: 0.8514689 Vali Loss: 0.6261693 Test Loss: 0.4211809
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.9103751182556152
Epoch: 66, Steps: 61 | Train Loss: 0.8505047 Vali Loss: 0.6279793 Test Loss: 0.4211723
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4198882281780243, mae:0.4371836185455322, rse:0.5179318189620972, corr:[ 0.21909805  0.22081329  0.21774074  0.21881942  0.2169415   0.21543902
  0.21569684  0.21380366  0.21272948  0.21241026  0.21054937  0.20932889
  0.20872468  0.2071636   0.20639178  0.20611769  0.20525807  0.20494707
  0.20422946  0.20284472  0.20221706  0.2013075   0.1997481   0.1982007
  0.19554213  0.19314502  0.19208881  0.19089858  0.18979006  0.18991092
  0.18927625  0.1879304   0.18764669  0.18694445  0.18600331  0.18550827
  0.1846283   0.18374205  0.18341242  0.18301731  0.1825615   0.18230562
  0.1817503   0.18122464  0.18091667  0.18013766  0.17941557  0.17771532
  0.17451388  0.17245783  0.17099506  0.169549    0.16858877  0.16820972
  0.16775377  0.16667163  0.1666452   0.1664907   0.16624315  0.16588193
  0.16563676  0.16563612  0.1655522   0.16588217  0.16608404  0.16577032
  0.16568868  0.1655739   0.16508308  0.164699    0.16444881  0.16336116
  0.16122518  0.16043329  0.15981257  0.15906753  0.15877976  0.15890332
  0.15908957  0.15878463  0.1587158   0.15857077  0.1585432   0.1583436
  0.15800537  0.15798639  0.15801732  0.15814185  0.1581965   0.1579454
  0.15771212  0.15730618  0.1567137   0.15621504  0.15600517  0.15505828
  0.15316315  0.1523308   0.1513584   0.15022863  0.14981958  0.14948979
  0.14912902  0.14884557  0.14921272  0.14902769  0.14914167  0.1492209
  0.14889269  0.14866042  0.14840451  0.14838453  0.1483009   0.14786403
  0.1475392   0.14713445  0.14656274  0.14588296  0.14507402  0.14351667
  0.14112605  0.13975883  0.13848361  0.13729134  0.1364798   0.13623008
  0.13592505  0.13567223  0.13578758  0.13537078  0.13524213  0.1351154
  0.13459072  0.13395973  0.13355193  0.13330963  0.13289617  0.13217634
  0.13153048  0.13093199  0.1303853   0.12983897  0.1290053   0.12732472
  0.12427773  0.12220214  0.12066914  0.11968946  0.11903601  0.11860458
  0.11848881  0.11801435  0.11793991  0.11781766  0.11800995  0.11781043
  0.1173249   0.11721804  0.11715791  0.11725957  0.11715409  0.11679582
  0.11666384  0.11640117  0.11610509  0.11563282  0.1149595   0.11319438
  0.11031464  0.10897491  0.10791232  0.10722304  0.1070847   0.10720045
  0.10754518  0.10774633  0.10823732  0.10799509  0.10807361  0.10804881
  0.10780019  0.10746453  0.10724249  0.10751571  0.10761385  0.10757826
  0.107701    0.10779824  0.10771368  0.10771013  0.10765866  0.10713881
  0.1054749   0.10502969  0.10488944  0.10481058  0.1054352   0.10629618
  0.10732552  0.10819817  0.10933512  0.10955865  0.10989813  0.11034552
  0.11031373  0.11010742  0.11044184  0.11085641  0.11087609  0.11119084
  0.11144827  0.11134185  0.11134023  0.11114325  0.11079598  0.10978178
  0.10790624  0.10676058  0.10584465  0.10546916  0.10555867  0.10667815
  0.10803486  0.10909075  0.1101727   0.1107253   0.11111628  0.11148065
  0.11182617  0.11185716  0.11223338  0.11296114  0.11337472  0.1137251
  0.11431597  0.11457918  0.11455085  0.11470911  0.11471671  0.11434812
  0.11312315  0.11240121  0.11185038  0.1119044   0.11222046  0.11324795
  0.11504722  0.11572383  0.11658873  0.11721503  0.11785281  0.11817362
  0.11846664  0.11890897  0.11892494  0.1193044   0.11964986  0.11996233
  0.12014522  0.12022725  0.12016761  0.12024686  0.12034185  0.11994023
  0.11869888  0.11837783  0.11827111  0.11848903  0.11905543  0.12048434
  0.12214335  0.1229257   0.12375215  0.12406991  0.12436804  0.12419653
  0.12423285  0.12425002  0.124406    0.12467532  0.12485804  0.12476384
  0.12471194  0.12466218  0.12439691  0.12411562  0.12408444  0.12365235
  0.1226602   0.12232661  0.12200709  0.1215665   0.121719    0.12239929
  0.12283785  0.12309865  0.12387122  0.12417275  0.12431213  0.12448028
  0.12454501  0.12452246  0.12468358  0.12501164  0.12512173  0.1252641
  0.12529805  0.12501651  0.12483484  0.12453786  0.12437966  0.12397317
  0.12242879  0.12173002  0.12180182  0.12196615  0.12201561  0.12260496
  0.1235628   0.12413747  0.12518942  0.12562986  0.1259038   0.12638408
  0.12676522  0.12674458  0.12700711  0.12754522  0.12777819  0.12782142
  0.12787512  0.12808448  0.12850237  0.12859601  0.128435    0.12852421
  0.12754157  0.12698019  0.12697771  0.12729943  0.12806045  0.12934451
  0.13134657  0.13247265  0.13329005  0.13364066  0.13390823  0.13455561
  0.1351162   0.13514803  0.13521616  0.13563143  0.13598718  0.13614485
  0.13645148  0.13681708  0.13718618  0.13766322  0.13793582  0.13802399
  0.13746013  0.1371757   0.13728887  0.1377257   0.1385911   0.14039311
  0.1427983   0.1446092   0.14607118  0.14691673  0.14768375  0.148563
  0.1493381   0.14990804  0.15049638  0.15111838  0.15173557  0.15253736
  0.15294434  0.15329786  0.15398079  0.15431225  0.15435188  0.15476614
  0.15486662  0.15487878  0.15536645  0.15616396  0.15708984  0.15871407
  0.16106798  0.16243598  0.16358778  0.16434237  0.16492063  0.16556637
  0.1662131   0.16630282  0.16637346  0.16681406  0.16679913  0.1666136
  0.16691309  0.1670981   0.16712897  0.16722322  0.16714422  0.16689327
  0.16615362  0.16612986  0.16629942  0.16658178  0.16695371  0.16777171
  0.16912286  0.16983892  0.17024258  0.17068805  0.17102835  0.17091821
  0.1708063   0.1708208   0.17073774  0.17068808  0.17066208  0.17050901
  0.17031431  0.1703571   0.17046249  0.17039856  0.1702468   0.16996957
  0.16926458  0.16882436  0.16874704  0.16863231  0.16851202  0.16890848
  0.16997018  0.17012972  0.17028478  0.17032987  0.17029239  0.1702577
  0.1703158   0.17026064  0.17013502  0.17025232  0.17013492  0.16996329
  0.16999723  0.17000629  0.1697732   0.16960178  0.16951148  0.16915113
  0.16846384  0.16802748  0.16742487  0.1671377   0.16705212  0.16708025
  0.16726585  0.16733491  0.16747944  0.16741794  0.1673858   0.16710696
  0.16694835  0.16683476  0.16643824  0.16622661  0.16614902  0.16578187
  0.16533303  0.16499971  0.16467309  0.16439186  0.16404414  0.16309401
  0.16147304  0.16029264  0.1594056   0.15830061  0.15742466  0.15704206
  0.15689567  0.15626058  0.15579137  0.15544264  0.15521468  0.15463935
  0.15411095  0.15397933  0.15364233  0.15319155  0.15294898  0.15258023
  0.15202756  0.15170301  0.151363    0.1509022   0.15037486  0.14885622
  0.14643064  0.14458291  0.14321479  0.14184685  0.14073715  0.14033322
  0.1399816   0.13949142  0.1392656   0.13909842  0.1389936   0.1386257
  0.13813886  0.1377696   0.13745253  0.13714409  0.13685502  0.13627653
  0.13556792  0.13501815  0.13429259  0.13380292  0.13317883  0.13120426
  0.12808636  0.12577894  0.1238552   0.1217839   0.11994239  0.11833145
  0.1174852   0.1168488   0.1168215   0.11646005  0.11612623  0.11581964
  0.1153926   0.11485349  0.11407621  0.11366137  0.11314061  0.11218665
  0.11122778  0.11068608  0.10976628  0.1085244   0.10728057  0.10505746
  0.10173141  0.09965726  0.09740974  0.09525358  0.09392749  0.09334954
  0.09294558  0.09220468  0.09225655  0.09210776  0.09193039  0.09139579
  0.09077954  0.09034931  0.0895739   0.08884308  0.08839669  0.08757292
  0.08669677  0.08604761  0.08494804  0.08366364  0.08247213  0.08018681
  0.07699226  0.07484075  0.07287372  0.07075132  0.06941991  0.0686201
  0.06800687  0.06771345  0.06809495  0.06795951  0.06759315  0.06726978
  0.0669941   0.06668011  0.0660924   0.06566946  0.06538708  0.06470408
  0.06408983  0.06385984  0.06321594  0.06217646  0.06106626  0.05917095
  0.05619837  0.05395507  0.05245535  0.0509034   0.04925987  0.04858922
  0.04817081  0.04765877  0.04800932  0.04807689  0.04799416  0.0480357
  0.04819932  0.0479414   0.04736507  0.04736945  0.04736207  0.04670123
  0.04622966  0.04621372  0.04564058  0.04483775  0.04384303  0.04216729
  0.03934427  0.03711322  0.03524414  0.03353016  0.03195515  0.03130125
  0.03091777  0.03011132  0.03041876  0.03052453  0.03017752  0.03005964
  0.03005223  0.02930386  0.02821972  0.0280802   0.02754606  0.02606494
  0.02550439  0.02526071  0.02395667  0.02321207  0.02338826  0.02141239
  0.01812649  0.01595558  0.01380572  0.01176509  0.01051122  0.00975763
  0.00867611  0.00801361  0.00852033  0.00862113  0.00849995  0.0082403
  0.00818674  0.00773845  0.00693649  0.00734821  0.00752815  0.00601094
  0.00531081  0.0055689   0.00463157  0.00425602  0.00458842  0.00261296
 -0.00072144 -0.00265468 -0.0045647  -0.00714965 -0.00891876 -0.00931068
 -0.01013857 -0.01081348 -0.00942281 -0.0088318  -0.00875834 -0.00858315
 -0.00863543 -0.00918471 -0.00999755 -0.01014093 -0.01007218 -0.01092841
 -0.01151008 -0.01161618 -0.01232397 -0.01256454 -0.01265561 -0.01440184
 -0.0180785  -0.02035123 -0.02179458 -0.02359029 -0.02492832 -0.02405426
 -0.02450159 -0.02570292 -0.02298775 -0.0222087  -0.02270302 -0.02196114
 -0.02160474 -0.02341494 -0.02389601 -0.02349166 -0.02495654 -0.02537521
 -0.02443043 -0.02590343 -0.02588511 -0.02292587 -0.02485418 -0.01981179]
