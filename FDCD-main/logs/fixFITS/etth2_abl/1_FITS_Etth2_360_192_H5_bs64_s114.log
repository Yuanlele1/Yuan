Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_192_FITS_ETTh2_ftM_sl360_ll48_pl192_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=90, out_features=138, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  11128320.0
params:  12558.0
Trainable parameters:  12558
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.1431360244750977
Epoch: 1, Steps: 63 | Train Loss: 0.7113783 Vali Loss: 0.3731182 Test Loss: 0.4260834
Validation loss decreased (inf --> 0.373118).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.672703266143799
Epoch: 2, Steps: 63 | Train Loss: 0.5942561 Vali Loss: 0.3341954 Test Loss: 0.3939134
Validation loss decreased (0.373118 --> 0.334195).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.0318164825439453
Epoch: 3, Steps: 63 | Train Loss: 0.5589422 Vali Loss: 0.3167872 Test Loss: 0.3825372
Validation loss decreased (0.334195 --> 0.316787).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.19677734375
Epoch: 4, Steps: 63 | Train Loss: 0.5449403 Vali Loss: 0.3077760 Test Loss: 0.3759621
Validation loss decreased (0.316787 --> 0.307776).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.343137502670288
Epoch: 5, Steps: 63 | Train Loss: 0.5380538 Vali Loss: 0.3020031 Test Loss: 0.3717733
Validation loss decreased (0.307776 --> 0.302003).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.578930139541626
Epoch: 6, Steps: 63 | Train Loss: 0.5322138 Vali Loss: 0.2982552 Test Loss: 0.3687412
Validation loss decreased (0.302003 --> 0.298255).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.5687544345855713
Epoch: 7, Steps: 63 | Train Loss: 0.5272284 Vali Loss: 0.2956628 Test Loss: 0.3665614
Validation loss decreased (0.298255 --> 0.295663).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.437465190887451
Epoch: 8, Steps: 63 | Train Loss: 0.5247131 Vali Loss: 0.2937618 Test Loss: 0.3650274
Validation loss decreased (0.295663 --> 0.293762).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.4249496459960938
Epoch: 9, Steps: 63 | Train Loss: 0.5225038 Vali Loss: 0.2919634 Test Loss: 0.3638763
Validation loss decreased (0.293762 --> 0.291963).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.0413060188293457
Epoch: 10, Steps: 63 | Train Loss: 0.5196724 Vali Loss: 0.2908960 Test Loss: 0.3627799
Validation loss decreased (0.291963 --> 0.290896).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.1920294761657715
Epoch: 11, Steps: 63 | Train Loss: 0.5192886 Vali Loss: 0.2897542 Test Loss: 0.3620112
Validation loss decreased (0.290896 --> 0.289754).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.3488967418670654
Epoch: 12, Steps: 63 | Train Loss: 0.5174795 Vali Loss: 0.2889083 Test Loss: 0.3614019
Validation loss decreased (0.289754 --> 0.288908).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.812044382095337
Epoch: 13, Steps: 63 | Train Loss: 0.5164613 Vali Loss: 0.2881387 Test Loss: 0.3608104
Validation loss decreased (0.288908 --> 0.288139).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.5249075889587402
Epoch: 14, Steps: 63 | Train Loss: 0.5155941 Vali Loss: 0.2874436 Test Loss: 0.3603022
Validation loss decreased (0.288139 --> 0.287444).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.6414661407470703
Epoch: 15, Steps: 63 | Train Loss: 0.5143878 Vali Loss: 0.2867922 Test Loss: 0.3600670
Validation loss decreased (0.287444 --> 0.286792).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 4.656354665756226
Epoch: 16, Steps: 63 | Train Loss: 0.5128239 Vali Loss: 0.2863192 Test Loss: 0.3597002
Validation loss decreased (0.286792 --> 0.286319).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 5.800835847854614
Epoch: 17, Steps: 63 | Train Loss: 0.5141002 Vali Loss: 0.2860433 Test Loss: 0.3592255
Validation loss decreased (0.286319 --> 0.286043).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.656978130340576
Epoch: 18, Steps: 63 | Train Loss: 0.5134531 Vali Loss: 0.2854841 Test Loss: 0.3590642
Validation loss decreased (0.286043 --> 0.285484).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.4042985439300537
Epoch: 19, Steps: 63 | Train Loss: 0.5131700 Vali Loss: 0.2851433 Test Loss: 0.3588391
Validation loss decreased (0.285484 --> 0.285143).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.4696314334869385
Epoch: 20, Steps: 63 | Train Loss: 0.5114945 Vali Loss: 0.2849030 Test Loss: 0.3585256
Validation loss decreased (0.285143 --> 0.284903).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.1858553886413574
Epoch: 21, Steps: 63 | Train Loss: 0.5118842 Vali Loss: 0.2844626 Test Loss: 0.3584200
Validation loss decreased (0.284903 --> 0.284463).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.978607177734375
Epoch: 22, Steps: 63 | Train Loss: 0.5118407 Vali Loss: 0.2842329 Test Loss: 0.3583083
Validation loss decreased (0.284463 --> 0.284233).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.0613656044006348
Epoch: 23, Steps: 63 | Train Loss: 0.5112018 Vali Loss: 0.2839042 Test Loss: 0.3580149
Validation loss decreased (0.284233 --> 0.283904).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.076472759246826
Epoch: 24, Steps: 63 | Train Loss: 0.5106066 Vali Loss: 0.2838300 Test Loss: 0.3578657
Validation loss decreased (0.283904 --> 0.283830).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.604954957962036
Epoch: 25, Steps: 63 | Train Loss: 0.5097716 Vali Loss: 0.2835280 Test Loss: 0.3577529
Validation loss decreased (0.283830 --> 0.283528).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.0511083602905273
Epoch: 26, Steps: 63 | Train Loss: 0.5097604 Vali Loss: 0.2833069 Test Loss: 0.3575356
Validation loss decreased (0.283528 --> 0.283307).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.736787796020508
Epoch: 27, Steps: 63 | Train Loss: 0.5090545 Vali Loss: 0.2832001 Test Loss: 0.3575041
Validation loss decreased (0.283307 --> 0.283200).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.634669303894043
Epoch: 28, Steps: 63 | Train Loss: 0.5102315 Vali Loss: 0.2830347 Test Loss: 0.3573370
Validation loss decreased (0.283200 --> 0.283035).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.3729851245880127
Epoch: 29, Steps: 63 | Train Loss: 0.5093210 Vali Loss: 0.2828778 Test Loss: 0.3573332
Validation loss decreased (0.283035 --> 0.282878).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.7565646171569824
Epoch: 30, Steps: 63 | Train Loss: 0.5082612 Vali Loss: 0.2827063 Test Loss: 0.3572476
Validation loss decreased (0.282878 --> 0.282706).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.703030586242676
Epoch: 31, Steps: 63 | Train Loss: 0.5094649 Vali Loss: 0.2826664 Test Loss: 0.3571559
Validation loss decreased (0.282706 --> 0.282666).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.2651445865631104
Epoch: 32, Steps: 63 | Train Loss: 0.5096207 Vali Loss: 0.2825656 Test Loss: 0.3570452
Validation loss decreased (0.282666 --> 0.282566).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.304607629776001
Epoch: 33, Steps: 63 | Train Loss: 0.5084883 Vali Loss: 0.2824156 Test Loss: 0.3569581
Validation loss decreased (0.282566 --> 0.282416).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.1063392162323
Epoch: 34, Steps: 63 | Train Loss: 0.5071305 Vali Loss: 0.2822154 Test Loss: 0.3569470
Validation loss decreased (0.282416 --> 0.282215).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.112488269805908
Epoch: 35, Steps: 63 | Train Loss: 0.5085944 Vali Loss: 0.2822061 Test Loss: 0.3568813
Validation loss decreased (0.282215 --> 0.282206).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.5742764472961426
Epoch: 36, Steps: 63 | Train Loss: 0.5085940 Vali Loss: 0.2821917 Test Loss: 0.3567432
Validation loss decreased (0.282206 --> 0.282192).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.7332613468170166
Epoch: 37, Steps: 63 | Train Loss: 0.5074414 Vali Loss: 0.2820833 Test Loss: 0.3567050
Validation loss decreased (0.282192 --> 0.282083).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.708702802658081
Epoch: 38, Steps: 63 | Train Loss: 0.5082585 Vali Loss: 0.2820123 Test Loss: 0.3566887
Validation loss decreased (0.282083 --> 0.282012).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.377235174179077
Epoch: 39, Steps: 63 | Train Loss: 0.5086306 Vali Loss: 0.2819543 Test Loss: 0.3566050
Validation loss decreased (0.282012 --> 0.281954).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.4712882041931152
Epoch: 40, Steps: 63 | Train Loss: 0.5071512 Vali Loss: 0.2818295 Test Loss: 0.3566211
Validation loss decreased (0.281954 --> 0.281830).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.264078378677368
Epoch: 41, Steps: 63 | Train Loss: 0.5081683 Vali Loss: 0.2816575 Test Loss: 0.3565990
Validation loss decreased (0.281830 --> 0.281658).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.158820152282715
Epoch: 42, Steps: 63 | Train Loss: 0.5067191 Vali Loss: 0.2812949 Test Loss: 0.3565592
Validation loss decreased (0.281658 --> 0.281295).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.9615814685821533
Epoch: 43, Steps: 63 | Train Loss: 0.5080971 Vali Loss: 0.2815537 Test Loss: 0.3565479
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.9169981479644775
Epoch: 44, Steps: 63 | Train Loss: 0.5082114 Vali Loss: 0.2816498 Test Loss: 0.3564599
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.2116646766662598
Epoch: 45, Steps: 63 | Train Loss: 0.5082620 Vali Loss: 0.2813361 Test Loss: 0.3564096
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.5293009281158447
Epoch: 46, Steps: 63 | Train Loss: 0.5065823 Vali Loss: 0.2814502 Test Loss: 0.3564371
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.2487549781799316
Epoch: 47, Steps: 63 | Train Loss: 0.5081522 Vali Loss: 0.2814903 Test Loss: 0.3563728
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.540729522705078
Epoch: 48, Steps: 63 | Train Loss: 0.5060590 Vali Loss: 0.2813692 Test Loss: 0.3564028
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.5975215435028076
Epoch: 49, Steps: 63 | Train Loss: 0.5068177 Vali Loss: 0.2813808 Test Loss: 0.3563838
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.423738479614258
Epoch: 50, Steps: 63 | Train Loss: 0.5071458 Vali Loss: 0.2813070 Test Loss: 0.3563526
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.4537153244018555
Epoch: 51, Steps: 63 | Train Loss: 0.5079543 Vali Loss: 0.2812597 Test Loss: 0.3563433
Validation loss decreased (0.281295 --> 0.281260).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.7962346076965332
Epoch: 52, Steps: 63 | Train Loss: 0.5072964 Vali Loss: 0.2813010 Test Loss: 0.3562939
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.2015347480773926
Epoch: 53, Steps: 63 | Train Loss: 0.5077063 Vali Loss: 0.2812487 Test Loss: 0.3562758
Validation loss decreased (0.281260 --> 0.281249).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 5.759024143218994
Epoch: 54, Steps: 63 | Train Loss: 0.5079882 Vali Loss: 0.2811904 Test Loss: 0.3562633
Validation loss decreased (0.281249 --> 0.281190).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 6.298241138458252
Epoch: 55, Steps: 63 | Train Loss: 0.5068233 Vali Loss: 0.2811857 Test Loss: 0.3562593
Validation loss decreased (0.281190 --> 0.281186).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.9362120628356934
Epoch: 56, Steps: 63 | Train Loss: 0.5073471 Vali Loss: 0.2809036 Test Loss: 0.3562259
Validation loss decreased (0.281186 --> 0.280904).  Saving model ...
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.6693270206451416
Epoch: 57, Steps: 63 | Train Loss: 0.5075396 Vali Loss: 0.2811232 Test Loss: 0.3562152
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.780524492263794
Epoch: 58, Steps: 63 | Train Loss: 0.5076472 Vali Loss: 0.2810658 Test Loss: 0.3562014
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.3698408603668213
Epoch: 59, Steps: 63 | Train Loss: 0.5070673 Vali Loss: 0.2810363 Test Loss: 0.3562080
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.8896484375
Epoch: 60, Steps: 63 | Train Loss: 0.5069400 Vali Loss: 0.2810010 Test Loss: 0.3561876
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.3097727298736572
Epoch: 61, Steps: 63 | Train Loss: 0.5078416 Vali Loss: 0.2810120 Test Loss: 0.3561636
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.2203359603881836
Epoch: 62, Steps: 63 | Train Loss: 0.5073801 Vali Loss: 0.2810161 Test Loss: 0.3561515
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.398083209991455
Epoch: 63, Steps: 63 | Train Loss: 0.5063874 Vali Loss: 0.2809711 Test Loss: 0.3561396
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.466153144836426
Epoch: 64, Steps: 63 | Train Loss: 0.5074640 Vali Loss: 0.2808988 Test Loss: 0.3561345
Validation loss decreased (0.280904 --> 0.280899).  Saving model ...
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.181607484817505
Epoch: 65, Steps: 63 | Train Loss: 0.5062611 Vali Loss: 0.2809312 Test Loss: 0.3561259
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.996913194656372
Epoch: 66, Steps: 63 | Train Loss: 0.5075495 Vali Loss: 0.2809702 Test Loss: 0.3561117
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.638035774230957
Epoch: 67, Steps: 63 | Train Loss: 0.5062357 Vali Loss: 0.2809594 Test Loss: 0.3560994
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.2351315021514893
Epoch: 68, Steps: 63 | Train Loss: 0.5072785 Vali Loss: 0.2809586 Test Loss: 0.3561046
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.670715093612671
Epoch: 69, Steps: 63 | Train Loss: 0.5077175 Vali Loss: 0.2808952 Test Loss: 0.3560893
Validation loss decreased (0.280899 --> 0.280895).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 3.2937402725219727
Epoch: 70, Steps: 63 | Train Loss: 0.5065900 Vali Loss: 0.2808912 Test Loss: 0.3560906
Validation loss decreased (0.280895 --> 0.280891).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.2415361404418945
Epoch: 71, Steps: 63 | Train Loss: 0.5069808 Vali Loss: 0.2809138 Test Loss: 0.3560807
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.295247793197632
Epoch: 72, Steps: 63 | Train Loss: 0.5071933 Vali Loss: 0.2808992 Test Loss: 0.3560663
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.3847429752349854
Epoch: 73, Steps: 63 | Train Loss: 0.5074784 Vali Loss: 0.2805879 Test Loss: 0.3560666
Validation loss decreased (0.280891 --> 0.280588).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.5191142559051514
Epoch: 74, Steps: 63 | Train Loss: 0.5051161 Vali Loss: 0.2808899 Test Loss: 0.3560573
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.075279474258423
Epoch: 75, Steps: 63 | Train Loss: 0.5051686 Vali Loss: 0.2807758 Test Loss: 0.3560627
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.866565227508545
Epoch: 76, Steps: 63 | Train Loss: 0.5067277 Vali Loss: 0.2808429 Test Loss: 0.3560531
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 6.242487907409668
Epoch: 77, Steps: 63 | Train Loss: 0.5076215 Vali Loss: 0.2808560 Test Loss: 0.3560520
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 6.648778438568115
Epoch: 78, Steps: 63 | Train Loss: 0.5073337 Vali Loss: 0.2807700 Test Loss: 0.3560455
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 6.395416736602783
Epoch: 79, Steps: 63 | Train Loss: 0.5067503 Vali Loss: 0.2808414 Test Loss: 0.3560408
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 5.270991086959839
Epoch: 80, Steps: 63 | Train Loss: 0.5073790 Vali Loss: 0.2807913 Test Loss: 0.3560336
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.2858386039733887
Epoch: 81, Steps: 63 | Train Loss: 0.5067452 Vali Loss: 0.2807690 Test Loss: 0.3560401
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.750063896179199
Epoch: 82, Steps: 63 | Train Loss: 0.5072754 Vali Loss: 0.2807737 Test Loss: 0.3560269
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.474297523498535
Epoch: 83, Steps: 63 | Train Loss: 0.5063288 Vali Loss: 0.2808093 Test Loss: 0.3560277
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.781820058822632
Epoch: 84, Steps: 63 | Train Loss: 0.5065961 Vali Loss: 0.2807791 Test Loss: 0.3560246
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.2953579425811768
Epoch: 85, Steps: 63 | Train Loss: 0.5074691 Vali Loss: 0.2807902 Test Loss: 0.3560169
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.173527956008911
Epoch: 86, Steps: 63 | Train Loss: 0.5065297 Vali Loss: 0.2807355 Test Loss: 0.3560186
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.341632127761841
Epoch: 87, Steps: 63 | Train Loss: 0.5075242 Vali Loss: 0.2807777 Test Loss: 0.3560103
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.3246920108795166
Epoch: 88, Steps: 63 | Train Loss: 0.5070761 Vali Loss: 0.2806650 Test Loss: 0.3560122
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 3.0035481452941895
Epoch: 89, Steps: 63 | Train Loss: 0.5065894 Vali Loss: 0.2807177 Test Loss: 0.3560104
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.763686180114746
Epoch: 90, Steps: 63 | Train Loss: 0.5068330 Vali Loss: 0.2806982 Test Loss: 0.3560017
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 3.0191450119018555
Epoch: 91, Steps: 63 | Train Loss: 0.5059721 Vali Loss: 0.2807343 Test Loss: 0.3560044
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.5118534564971924
Epoch: 92, Steps: 63 | Train Loss: 0.5072869 Vali Loss: 0.2807438 Test Loss: 0.3560010
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.2724251747131348
Epoch: 93, Steps: 63 | Train Loss: 0.5059932 Vali Loss: 0.2807616 Test Loss: 0.3559963
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_192_FITS_ETTh2_ftM_sl360_ll48_pl192_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3362860381603241, mae:0.3772534132003784, rse:0.465045303106308, corr:[0.26440218 0.27073866 0.26721272 0.2665797  0.26690453 0.26522276
 0.2635803  0.26287827 0.2618917  0.26017028 0.25857675 0.25705293
 0.2557184  0.25474852 0.2539484  0.2531562  0.2525589  0.25203168
 0.25098357 0.24947564 0.24812284 0.24706915 0.24531515 0.24268389
 0.24032629 0.23862918 0.23672289 0.23467632 0.23344816 0.23254377
 0.23092416 0.22867511 0.22726154 0.22638386 0.2249039  0.2230668
 0.2218832  0.22112656 0.21991262 0.21865138 0.21842183 0.21841604
 0.21734251 0.21578169 0.2149302  0.21434177 0.212758   0.21041009
 0.20857981 0.20722339 0.2054993  0.20365483 0.20233338 0.20103647
 0.19893019 0.19676456 0.19554202 0.19441654 0.19306497 0.1919504
 0.1917456  0.19169803 0.19136114 0.19091555 0.19070001 0.19033954
 0.18936296 0.18855913 0.18851729 0.18829441 0.1871155  0.18578164
 0.1851172  0.18468781 0.1836061  0.18240789 0.1820163  0.18173015
 0.18072908 0.179715   0.17969242 0.17960332 0.17911142 0.17867552
 0.17878248 0.1787709  0.17840685 0.17816992 0.17812164 0.17775168
 0.17678465 0.17616634 0.17658557 0.1768089  0.1762167  0.17532492
 0.17469501 0.17380092 0.17253706 0.17140487 0.17086864 0.17036943
 0.16954172 0.1686652  0.16855022 0.1685236  0.16804977 0.16798048
 0.16800568 0.16767123 0.16667224 0.16618088 0.16608845 0.16590849
 0.16521841 0.16448648 0.16435291 0.16377153 0.16234595 0.16081667
 0.15987259 0.15879212 0.1575679  0.15658487 0.15585656 0.15501761
 0.15399908 0.15319724 0.15278427 0.15224901 0.15159895 0.1511143
 0.15097007 0.15044841 0.14961576 0.14935602 0.14953688 0.14931954
 0.14835443 0.1481236  0.14870787 0.14852487 0.14727698 0.14600594
 0.14497317 0.14343514 0.14155245 0.14057419 0.14058726 0.14018317
 0.13942619 0.13863342 0.1386978  0.13877633 0.13817975 0.13767368
 0.13761485 0.13777278 0.13753211 0.1380103  0.13839561 0.13801609
 0.13748986 0.13771585 0.13841288 0.13826601 0.1376841  0.13696162
 0.13613763 0.1344558  0.13274477 0.13220014 0.13194975 0.13040951
 0.12847395 0.12749921 0.1268329  0.12551107 0.12446214 0.12403461
 0.1232654  0.12176825 0.12130814 0.12208761 0.12068677 0.11737666
 0.11689895 0.11766737 0.11481346 0.11162394 0.11449862 0.11067227]
