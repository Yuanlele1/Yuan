Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_96_FITS_ETTh2_ftM_sl360_ll48_pl96_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8185
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=74, out_features=93, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  6166272.0
params:  6975.0
Trainable parameters:  6975
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.3042972087860107
Epoch: 1, Steps: 63 | Train Loss: 0.5989535 Vali Loss: 0.2872980 Test Loss: 0.3484854
Validation loss decreased (inf --> 0.287298).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.344444751739502
Epoch: 2, Steps: 63 | Train Loss: 0.4861746 Vali Loss: 0.2514616 Test Loss: 0.3180889
Validation loss decreased (0.287298 --> 0.251462).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.307645559310913
Epoch: 3, Steps: 63 | Train Loss: 0.4541026 Vali Loss: 0.2386464 Test Loss: 0.3066768
Validation loss decreased (0.251462 --> 0.238646).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.3318557739257812
Epoch: 4, Steps: 63 | Train Loss: 0.4439295 Vali Loss: 0.2321466 Test Loss: 0.3004568
Validation loss decreased (0.238646 --> 0.232147).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.9101059436798096
Epoch: 5, Steps: 63 | Train Loss: 0.4334542 Vali Loss: 0.2289719 Test Loss: 0.2962562
Validation loss decreased (0.232147 --> 0.228972).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.482700824737549
Epoch: 6, Steps: 63 | Train Loss: 0.4286527 Vali Loss: 0.2257329 Test Loss: 0.2934274
Validation loss decreased (0.228972 --> 0.225733).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 5.112379312515259
Epoch: 7, Steps: 63 | Train Loss: 0.4215787 Vali Loss: 0.2240731 Test Loss: 0.2912210
Validation loss decreased (0.225733 --> 0.224073).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.4999496936798096
Epoch: 8, Steps: 63 | Train Loss: 0.4222881 Vali Loss: 0.2228804 Test Loss: 0.2893326
Validation loss decreased (0.224073 --> 0.222880).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.5940895080566406
Epoch: 9, Steps: 63 | Train Loss: 0.4183380 Vali Loss: 0.2221135 Test Loss: 0.2876339
Validation loss decreased (0.222880 --> 0.222113).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.932570219039917
Epoch: 10, Steps: 63 | Train Loss: 0.4163398 Vali Loss: 0.2205163 Test Loss: 0.2863555
Validation loss decreased (0.222113 --> 0.220516).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.1334340572357178
Epoch: 11, Steps: 63 | Train Loss: 0.4152536 Vali Loss: 0.2204166 Test Loss: 0.2854497
Validation loss decreased (0.220516 --> 0.220417).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.600269079208374
Epoch: 12, Steps: 63 | Train Loss: 0.4129759 Vali Loss: 0.2193605 Test Loss: 0.2843302
Validation loss decreased (0.220417 --> 0.219361).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.115028142929077
Epoch: 13, Steps: 63 | Train Loss: 0.4133119 Vali Loss: 0.2194598 Test Loss: 0.2836505
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.4025509357452393
Epoch: 14, Steps: 63 | Train Loss: 0.4128377 Vali Loss: 0.2191257 Test Loss: 0.2830726
Validation loss decreased (0.219361 --> 0.219126).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.495454549789429
Epoch: 15, Steps: 63 | Train Loss: 0.4085122 Vali Loss: 0.2175340 Test Loss: 0.2825582
Validation loss decreased (0.219126 --> 0.217534).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.3473565578460693
Epoch: 16, Steps: 63 | Train Loss: 0.4093672 Vali Loss: 0.2170461 Test Loss: 0.2819441
Validation loss decreased (0.217534 --> 0.217046).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.476259469985962
Epoch: 17, Steps: 63 | Train Loss: 0.4084905 Vali Loss: 0.2173120 Test Loss: 0.2815820
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.5654866695404053
Epoch: 18, Steps: 63 | Train Loss: 0.4077956 Vali Loss: 0.2161217 Test Loss: 0.2811288
Validation loss decreased (0.217046 --> 0.216122).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.9770028591156006
Epoch: 19, Steps: 63 | Train Loss: 0.4073392 Vali Loss: 0.2170658 Test Loss: 0.2806374
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 4.94066309928894
Epoch: 20, Steps: 63 | Train Loss: 0.4093977 Vali Loss: 0.2163253 Test Loss: 0.2804433
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.7336511611938477
Epoch: 21, Steps: 63 | Train Loss: 0.4062582 Vali Loss: 0.2159857 Test Loss: 0.2802215
Validation loss decreased (0.216122 --> 0.215986).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.216066360473633
Epoch: 22, Steps: 63 | Train Loss: 0.4046927 Vali Loss: 0.2156475 Test Loss: 0.2799371
Validation loss decreased (0.215986 --> 0.215648).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.6811983585357666
Epoch: 23, Steps: 63 | Train Loss: 0.4065842 Vali Loss: 0.2149277 Test Loss: 0.2797014
Validation loss decreased (0.215648 --> 0.214928).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.4667186737060547
Epoch: 24, Steps: 63 | Train Loss: 0.4064531 Vali Loss: 0.2154787 Test Loss: 0.2794560
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.4487791061401367
Epoch: 25, Steps: 63 | Train Loss: 0.4046420 Vali Loss: 0.2158780 Test Loss: 0.2793051
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.127984046936035
Epoch: 26, Steps: 63 | Train Loss: 0.4031627 Vali Loss: 0.2154000 Test Loss: 0.2792625
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.134951114654541
Epoch: 27, Steps: 63 | Train Loss: 0.4066599 Vali Loss: 0.2149314 Test Loss: 0.2790270
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.9178991317749023
Epoch: 28, Steps: 63 | Train Loss: 0.4027274 Vali Loss: 0.2138828 Test Loss: 0.2789427
Validation loss decreased (0.214928 --> 0.213883).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.257666826248169
Epoch: 29, Steps: 63 | Train Loss: 0.4044979 Vali Loss: 0.2134382 Test Loss: 0.2787774
Validation loss decreased (0.213883 --> 0.213438).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.886204719543457
Epoch: 30, Steps: 63 | Train Loss: 0.4045002 Vali Loss: 0.2146682 Test Loss: 0.2786100
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.046483039855957
Epoch: 31, Steps: 63 | Train Loss: 0.4014215 Vali Loss: 0.2136198 Test Loss: 0.2785238
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.0108954906463623
Epoch: 32, Steps: 63 | Train Loss: 0.4024357 Vali Loss: 0.2144169 Test Loss: 0.2784531
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.7806644439697266
Epoch: 33, Steps: 63 | Train Loss: 0.4005530 Vali Loss: 0.2143095 Test Loss: 0.2783312
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.266662120819092
Epoch: 34, Steps: 63 | Train Loss: 0.4039701 Vali Loss: 0.2143055 Test Loss: 0.2782875
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.394993782043457
Epoch: 35, Steps: 63 | Train Loss: 0.4011788 Vali Loss: 0.2137692 Test Loss: 0.2781984
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.9699690341949463
Epoch: 36, Steps: 63 | Train Loss: 0.4038278 Vali Loss: 0.2143764 Test Loss: 0.2780690
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.8697803020477295
Epoch: 37, Steps: 63 | Train Loss: 0.4024889 Vali Loss: 0.2146922 Test Loss: 0.2779933
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.849421739578247
Epoch: 38, Steps: 63 | Train Loss: 0.4030607 Vali Loss: 0.2135091 Test Loss: 0.2779922
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.6855204105377197
Epoch: 39, Steps: 63 | Train Loss: 0.4036296 Vali Loss: 0.2137583 Test Loss: 0.2779722
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.3452165126800537
Epoch: 40, Steps: 63 | Train Loss: 0.4030268 Vali Loss: 0.2131727 Test Loss: 0.2778572
Validation loss decreased (0.213438 --> 0.213173).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.92301344871521
Epoch: 41, Steps: 63 | Train Loss: 0.4040916 Vali Loss: 0.2133998 Test Loss: 0.2777874
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.840186834335327
Epoch: 42, Steps: 63 | Train Loss: 0.4035311 Vali Loss: 0.2140768 Test Loss: 0.2777344
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.7237727642059326
Epoch: 43, Steps: 63 | Train Loss: 0.4038219 Vali Loss: 0.2130420 Test Loss: 0.2777215
Validation loss decreased (0.213173 --> 0.213042).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.116882801055908
Epoch: 44, Steps: 63 | Train Loss: 0.4019408 Vali Loss: 0.2130607 Test Loss: 0.2776872
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.709426164627075
Epoch: 45, Steps: 63 | Train Loss: 0.4041699 Vali Loss: 0.2133988 Test Loss: 0.2776432
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.6415369510650635
Epoch: 46, Steps: 63 | Train Loss: 0.4046649 Vali Loss: 0.2116547 Test Loss: 0.2776419
Validation loss decreased (0.213042 --> 0.211655).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.638885736465454
Epoch: 47, Steps: 63 | Train Loss: 0.4032109 Vali Loss: 0.2131911 Test Loss: 0.2775734
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 4.948455810546875
Epoch: 48, Steps: 63 | Train Loss: 0.4025554 Vali Loss: 0.2142775 Test Loss: 0.2774976
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.7559401988983154
Epoch: 49, Steps: 63 | Train Loss: 0.4008424 Vali Loss: 0.2133779 Test Loss: 0.2775106
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.344311475753784
Epoch: 50, Steps: 63 | Train Loss: 0.4003758 Vali Loss: 0.2132446 Test Loss: 0.2774794
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.160167932510376
Epoch: 51, Steps: 63 | Train Loss: 0.4023972 Vali Loss: 0.2132504 Test Loss: 0.2774279
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.4236810207366943
Epoch: 52, Steps: 63 | Train Loss: 0.4024504 Vali Loss: 0.2142148 Test Loss: 0.2774031
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.2838454246520996
Epoch: 53, Steps: 63 | Train Loss: 0.4032776 Vali Loss: 0.2136241 Test Loss: 0.2773857
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.370731830596924
Epoch: 54, Steps: 63 | Train Loss: 0.4036314 Vali Loss: 0.2134744 Test Loss: 0.2773650
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.1354851722717285
Epoch: 55, Steps: 63 | Train Loss: 0.4012722 Vali Loss: 0.2133252 Test Loss: 0.2773328
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.136533498764038
Epoch: 56, Steps: 63 | Train Loss: 0.4037562 Vali Loss: 0.2132705 Test Loss: 0.2773357
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.941605806350708
Epoch: 57, Steps: 63 | Train Loss: 0.4023785 Vali Loss: 0.2136479 Test Loss: 0.2773097
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.2379515171051025
Epoch: 58, Steps: 63 | Train Loss: 0.4038874 Vali Loss: 0.2130870 Test Loss: 0.2772917
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.5459022521972656
Epoch: 59, Steps: 63 | Train Loss: 0.4019011 Vali Loss: 0.2127153 Test Loss: 0.2772710
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.3362905979156494
Epoch: 60, Steps: 63 | Train Loss: 0.3996871 Vali Loss: 0.2141813 Test Loss: 0.2772379
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.0183095932006836
Epoch: 61, Steps: 63 | Train Loss: 0.4027936 Vali Loss: 0.2123524 Test Loss: 0.2772381
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.6375439167022705
Epoch: 62, Steps: 63 | Train Loss: 0.4017011 Vali Loss: 0.2128821 Test Loss: 0.2772214
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.369130849838257
Epoch: 63, Steps: 63 | Train Loss: 0.4034917 Vali Loss: 0.2129940 Test Loss: 0.2772053
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 4.110403299331665
Epoch: 64, Steps: 63 | Train Loss: 0.4008900 Vali Loss: 0.2131592 Test Loss: 0.2772027
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.228724241256714
Epoch: 65, Steps: 63 | Train Loss: 0.4013955 Vali Loss: 0.2126844 Test Loss: 0.2771845
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.3489205837249756
Epoch: 66, Steps: 63 | Train Loss: 0.4032366 Vali Loss: 0.2129059 Test Loss: 0.2771784
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_96_FITS_ETTh2_ftM_sl360_ll48_pl96_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.27729278802871704, mae:0.33969801664352417, rse:0.4243757724761963, corr:[0.2724115  0.27796525 0.2759813  0.27370587 0.27310345 0.27263504
 0.27111706 0.26954547 0.2685466  0.26726252 0.265567   0.263826
 0.26262343 0.26154354 0.26046237 0.25987294 0.25967517 0.25936934
 0.25818664 0.25659838 0.2553211  0.25440785 0.25303814 0.2505756
 0.24786966 0.24600117 0.24491535 0.24359187 0.24185461 0.24039203
 0.23948413 0.23845914 0.23658046 0.23438624 0.23305212 0.23264517
 0.23194431 0.2302202  0.22855999 0.22794156 0.22824748 0.22822744
 0.22704883 0.22549571 0.22480306 0.22462785 0.22372402 0.22146146
 0.21892817 0.2175105  0.21705963 0.21614459 0.21435207 0.2125127
 0.21117598 0.2099273  0.20823666 0.2060358  0.20467249 0.20471923
 0.20513959 0.20470913 0.20406732 0.20402104 0.20452657 0.204637
 0.20361023 0.20230016 0.20208597 0.2026568  0.20267004 0.20145005
 0.19994198 0.19943553 0.19973393 0.1991411  0.19776265 0.196797
 0.19686353 0.19704168 0.19667377 0.19533528 0.19467966 0.19542846
 0.1965512  0.1963243  0.19541639 0.19538225 0.1960671  0.19628763
 0.1946285  0.19220962 0.19241513 0.1939042  0.19195281 0.18170248]
