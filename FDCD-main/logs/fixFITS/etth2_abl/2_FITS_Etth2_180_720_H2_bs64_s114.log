Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=26, out_features=130, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3028480.0
params:  3510.0
Trainable parameters:  3510
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0198612213134766
Epoch: 1, Steps: 60 | Train Loss: 1.0818756 Vali Loss: 0.8151577 Test Loss: 0.6021374
Validation loss decreased (inf --> 0.815158).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.4049365520477295
Epoch: 2, Steps: 60 | Train Loss: 0.9274435 Vali Loss: 0.7550030 Test Loss: 0.5366031
Validation loss decreased (0.815158 --> 0.755003).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.2331228256225586
Epoch: 3, Steps: 60 | Train Loss: 0.8407243 Vali Loss: 0.7199051 Test Loss: 0.4964372
Validation loss decreased (0.755003 --> 0.719905).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.205902576446533
Epoch: 4, Steps: 60 | Train Loss: 0.7830783 Vali Loss: 0.6949687 Test Loss: 0.4709029
Validation loss decreased (0.719905 --> 0.694969).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.1080081462860107
Epoch: 5, Steps: 60 | Train Loss: 0.7464745 Vali Loss: 0.6790962 Test Loss: 0.4539402
Validation loss decreased (0.694969 --> 0.679096).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.5462470054626465
Epoch: 6, Steps: 60 | Train Loss: 0.7233807 Vali Loss: 0.6703737 Test Loss: 0.4424772
Validation loss decreased (0.679096 --> 0.670374).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.236271381378174
Epoch: 7, Steps: 60 | Train Loss: 0.7076119 Vali Loss: 0.6637273 Test Loss: 0.4345524
Validation loss decreased (0.670374 --> 0.663727).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.4381000995635986
Epoch: 8, Steps: 60 | Train Loss: 0.6964024 Vali Loss: 0.6537628 Test Loss: 0.4290153
Validation loss decreased (0.663727 --> 0.653763).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.5780186653137207
Epoch: 9, Steps: 60 | Train Loss: 0.6902922 Vali Loss: 0.6526264 Test Loss: 0.4249581
Validation loss decreased (0.653763 --> 0.652626).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.1817147731781006
Epoch: 10, Steps: 60 | Train Loss: 0.6810893 Vali Loss: 0.6473121 Test Loss: 0.4220213
Validation loss decreased (0.652626 --> 0.647312).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.4404258728027344
Epoch: 11, Steps: 60 | Train Loss: 0.6785759 Vali Loss: 0.6463249 Test Loss: 0.4198134
Validation loss decreased (0.647312 --> 0.646325).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.464897394180298
Epoch: 12, Steps: 60 | Train Loss: 0.6754444 Vali Loss: 0.6472868 Test Loss: 0.4181248
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.7206568717956543
Epoch: 13, Steps: 60 | Train Loss: 0.6705503 Vali Loss: 0.6405567 Test Loss: 0.4168120
Validation loss decreased (0.646325 --> 0.640557).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.2489125728607178
Epoch: 14, Steps: 60 | Train Loss: 0.6723768 Vali Loss: 0.6464416 Test Loss: 0.4157628
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.630873680114746
Epoch: 15, Steps: 60 | Train Loss: 0.6690244 Vali Loss: 0.6406482 Test Loss: 0.4149647
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.108532428741455
Epoch: 16, Steps: 60 | Train Loss: 0.6695450 Vali Loss: 0.6441498 Test Loss: 0.4142405
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.1710493564605713
Epoch: 17, Steps: 60 | Train Loss: 0.6681889 Vali Loss: 0.6381596 Test Loss: 0.4136710
Validation loss decreased (0.640557 --> 0.638160).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.182131767272949
Epoch: 18, Steps: 60 | Train Loss: 0.6689291 Vali Loss: 0.6371007 Test Loss: 0.4131846
Validation loss decreased (0.638160 --> 0.637101).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.281583070755005
Epoch: 19, Steps: 60 | Train Loss: 0.6683948 Vali Loss: 0.6377580 Test Loss: 0.4127467
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.6734426021575928
Epoch: 20, Steps: 60 | Train Loss: 0.6654262 Vali Loss: 0.6366535 Test Loss: 0.4124036
Validation loss decreased (0.637101 --> 0.636654).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.417755365371704
Epoch: 21, Steps: 60 | Train Loss: 0.6666145 Vali Loss: 0.6376503 Test Loss: 0.4120841
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.425560474395752
Epoch: 22, Steps: 60 | Train Loss: 0.6655667 Vali Loss: 0.6323705 Test Loss: 0.4117985
Validation loss decreased (0.636654 --> 0.632371).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.213334798812866
Epoch: 23, Steps: 60 | Train Loss: 0.6639232 Vali Loss: 0.6359811 Test Loss: 0.4115655
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.2898542881011963
Epoch: 24, Steps: 60 | Train Loss: 0.6625963 Vali Loss: 0.6355642 Test Loss: 0.4113437
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.5271875858306885
Epoch: 25, Steps: 60 | Train Loss: 0.6648201 Vali Loss: 0.6328965 Test Loss: 0.4111613
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.0965659618377686
Epoch: 26, Steps: 60 | Train Loss: 0.6627552 Vali Loss: 0.6297678 Test Loss: 0.4109601
Validation loss decreased (0.632371 --> 0.629768).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.935450792312622
Epoch: 27, Steps: 60 | Train Loss: 0.6626909 Vali Loss: 0.6310945 Test Loss: 0.4108039
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.0193848609924316
Epoch: 28, Steps: 60 | Train Loss: 0.6618138 Vali Loss: 0.6316681 Test Loss: 0.4106529
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.7489397525787354
Epoch: 29, Steps: 60 | Train Loss: 0.6620581 Vali Loss: 0.6302601 Test Loss: 0.4105171
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.528202772140503
Epoch: 30, Steps: 60 | Train Loss: 0.6620002 Vali Loss: 0.6318919 Test Loss: 0.4104076
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.518010377883911
Epoch: 31, Steps: 60 | Train Loss: 0.6613386 Vali Loss: 0.6352360 Test Loss: 0.4102834
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.0305581092834473
Epoch: 32, Steps: 60 | Train Loss: 0.6623069 Vali Loss: 0.6333810 Test Loss: 0.4101804
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.107470989227295
Epoch: 33, Steps: 60 | Train Loss: 0.6610519 Vali Loss: 0.6333998 Test Loss: 0.4100842
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.833951473236084
Epoch: 34, Steps: 60 | Train Loss: 0.6617349 Vali Loss: 0.6316111 Test Loss: 0.4099791
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.3691530227661133
Epoch: 35, Steps: 60 | Train Loss: 0.6615173 Vali Loss: 0.6302217 Test Loss: 0.4099185
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.7689008712768555
Epoch: 36, Steps: 60 | Train Loss: 0.6606797 Vali Loss: 0.6355197 Test Loss: 0.4098308
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.0031330585479736
Epoch: 37, Steps: 60 | Train Loss: 0.6617555 Vali Loss: 0.6294720 Test Loss: 0.4097641
Validation loss decreased (0.629768 --> 0.629472).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.168672800064087
Epoch: 38, Steps: 60 | Train Loss: 0.6617636 Vali Loss: 0.6311623 Test Loss: 0.4096934
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.208338737487793
Epoch: 39, Steps: 60 | Train Loss: 0.6612091 Vali Loss: 0.6333374 Test Loss: 0.4096348
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.0246596336364746
Epoch: 40, Steps: 60 | Train Loss: 0.6604140 Vali Loss: 0.6345177 Test Loss: 0.4095632
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.0678203105926514
Epoch: 41, Steps: 60 | Train Loss: 0.6607724 Vali Loss: 0.6369603 Test Loss: 0.4095100
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.9737508296966553
Epoch: 42, Steps: 60 | Train Loss: 0.6611188 Vali Loss: 0.6317083 Test Loss: 0.4094636
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9232032299041748
Epoch: 43, Steps: 60 | Train Loss: 0.6598731 Vali Loss: 0.6310105 Test Loss: 0.4094081
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.4696402549743652
Epoch: 44, Steps: 60 | Train Loss: 0.6609725 Vali Loss: 0.6285415 Test Loss: 0.4093717
Validation loss decreased (0.629472 --> 0.628541).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.4538300037384033
Epoch: 45, Steps: 60 | Train Loss: 0.6607917 Vali Loss: 0.6324200 Test Loss: 0.4093240
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.602285623550415
Epoch: 46, Steps: 60 | Train Loss: 0.6605336 Vali Loss: 0.6324276 Test Loss: 0.4092781
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.302067279815674
Epoch: 47, Steps: 60 | Train Loss: 0.6608211 Vali Loss: 0.6279824 Test Loss: 0.4092410
Validation loss decreased (0.628541 --> 0.627982).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.2901644706726074
Epoch: 48, Steps: 60 | Train Loss: 0.6609933 Vali Loss: 0.6310961 Test Loss: 0.4092069
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.362316370010376
Epoch: 49, Steps: 60 | Train Loss: 0.6612867 Vali Loss: 0.6291966 Test Loss: 0.4091734
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.431584358215332
Epoch: 50, Steps: 60 | Train Loss: 0.6603937 Vali Loss: 0.6298325 Test Loss: 0.4091421
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.0990898609161377
Epoch: 51, Steps: 60 | Train Loss: 0.6584438 Vali Loss: 0.6301414 Test Loss: 0.4091065
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.3457186222076416
Epoch: 52, Steps: 60 | Train Loss: 0.6618144 Vali Loss: 0.6314260 Test Loss: 0.4090814
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.3118693828582764
Epoch: 53, Steps: 60 | Train Loss: 0.6602059 Vali Loss: 0.6284939 Test Loss: 0.4090527
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.1030566692352295
Epoch: 54, Steps: 60 | Train Loss: 0.6619452 Vali Loss: 0.6286374 Test Loss: 0.4090236
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.5232391357421875
Epoch: 55, Steps: 60 | Train Loss: 0.6579994 Vali Loss: 0.6310508 Test Loss: 0.4090000
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.1355369091033936
Epoch: 56, Steps: 60 | Train Loss: 0.6591745 Vali Loss: 0.6329831 Test Loss: 0.4089723
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.2225184440612793
Epoch: 57, Steps: 60 | Train Loss: 0.6612801 Vali Loss: 0.6309990 Test Loss: 0.4089501
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.647733449935913
Epoch: 58, Steps: 60 | Train Loss: 0.6603161 Vali Loss: 0.6292421 Test Loss: 0.4089321
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.516310453414917
Epoch: 59, Steps: 60 | Train Loss: 0.6608333 Vali Loss: 0.6319692 Test Loss: 0.4089148
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.2551729679107666
Epoch: 60, Steps: 60 | Train Loss: 0.6604066 Vali Loss: 0.6303647 Test Loss: 0.4088971
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.3899528980255127
Epoch: 61, Steps: 60 | Train Loss: 0.6603973 Vali Loss: 0.6284815 Test Loss: 0.4088761
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.575604200363159
Epoch: 62, Steps: 60 | Train Loss: 0.6603457 Vali Loss: 0.6290914 Test Loss: 0.4088634
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.0770232677459717
Epoch: 63, Steps: 60 | Train Loss: 0.6583643 Vali Loss: 0.6352471 Test Loss: 0.4088474
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.9152181148529053
Epoch: 64, Steps: 60 | Train Loss: 0.6602855 Vali Loss: 0.6325971 Test Loss: 0.4088280
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.169632911682129
Epoch: 65, Steps: 60 | Train Loss: 0.6601150 Vali Loss: 0.6301957 Test Loss: 0.4088145
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.0270776748657227
Epoch: 66, Steps: 60 | Train Loss: 0.6591885 Vali Loss: 0.6283799 Test Loss: 0.4088021
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.0333056449890137
Epoch: 67, Steps: 60 | Train Loss: 0.6604089 Vali Loss: 0.6291232 Test Loss: 0.4087887
EarlyStopping counter: 20 out of 20
Early stopping
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=26, out_features=130, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3028480.0
params:  3510.0
Trainable parameters:  3510
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.9389824867248535
Epoch: 1, Steps: 60 | Train Loss: 0.8162328 Vali Loss: 0.6313404 Test Loss: 0.4083133
Validation loss decreased (inf --> 0.631340).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.40995717048645
Epoch: 2, Steps: 60 | Train Loss: 0.8154882 Vali Loss: 0.6248312 Test Loss: 0.4077108
Validation loss decreased (0.631340 --> 0.624831).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9622154235839844
Epoch: 3, Steps: 60 | Train Loss: 0.8128783 Vali Loss: 0.6308316 Test Loss: 0.4073709
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.8056254386901855
Epoch: 4, Steps: 60 | Train Loss: 0.8135796 Vali Loss: 0.6327926 Test Loss: 0.4071362
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.5575942993164062
Epoch: 5, Steps: 60 | Train Loss: 0.8148424 Vali Loss: 0.6249150 Test Loss: 0.4070043
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.493645429611206
Epoch: 6, Steps: 60 | Train Loss: 0.8116633 Vali Loss: 0.6273874 Test Loss: 0.4068349
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.2808947563171387
Epoch: 7, Steps: 60 | Train Loss: 0.8132007 Vali Loss: 0.6312579 Test Loss: 0.4067167
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.5339443683624268
Epoch: 8, Steps: 60 | Train Loss: 0.8144105 Vali Loss: 0.6279258 Test Loss: 0.4066678
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.4491336345672607
Epoch: 9, Steps: 60 | Train Loss: 0.8111884 Vali Loss: 0.6258720 Test Loss: 0.4065507
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.653419256210327
Epoch: 10, Steps: 60 | Train Loss: 0.8123724 Vali Loss: 0.6308419 Test Loss: 0.4065381
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.4678919315338135
Epoch: 11, Steps: 60 | Train Loss: 0.8125245 Vali Loss: 0.6283611 Test Loss: 0.4064864
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.7643113136291504
Epoch: 12, Steps: 60 | Train Loss: 0.8128690 Vali Loss: 0.6307369 Test Loss: 0.4064592
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.653369665145874
Epoch: 13, Steps: 60 | Train Loss: 0.8128155 Vali Loss: 0.6272159 Test Loss: 0.4064458
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.097266674041748
Epoch: 14, Steps: 60 | Train Loss: 0.8125351 Vali Loss: 0.6249392 Test Loss: 0.4064069
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.5015697479248047
Epoch: 15, Steps: 60 | Train Loss: 0.8131725 Vali Loss: 0.6232874 Test Loss: 0.4063656
Validation loss decreased (0.624831 --> 0.623287).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9126577377319336
Epoch: 16, Steps: 60 | Train Loss: 0.8109578 Vali Loss: 0.6253011 Test Loss: 0.4064113
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.077821969985962
Epoch: 17, Steps: 60 | Train Loss: 0.8105791 Vali Loss: 0.6247028 Test Loss: 0.4063620
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.3975582122802734
Epoch: 18, Steps: 60 | Train Loss: 0.8123937 Vali Loss: 0.6228250 Test Loss: 0.4063423
Validation loss decreased (0.623287 --> 0.622825).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.845057964324951
Epoch: 19, Steps: 60 | Train Loss: 0.8103363 Vali Loss: 0.6262249 Test Loss: 0.4063195
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.588265895843506
Epoch: 20, Steps: 60 | Train Loss: 0.8112630 Vali Loss: 0.6274140 Test Loss: 0.4063077
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.1839375495910645
Epoch: 21, Steps: 60 | Train Loss: 0.8111986 Vali Loss: 0.6236223 Test Loss: 0.4063136
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.3292722702026367
Epoch: 22, Steps: 60 | Train Loss: 0.8117386 Vali Loss: 0.6262928 Test Loss: 0.4062877
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.3132846355438232
Epoch: 23, Steps: 60 | Train Loss: 0.8089195 Vali Loss: 0.6274165 Test Loss: 0.4062961
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.1433987617492676
Epoch: 24, Steps: 60 | Train Loss: 0.8106882 Vali Loss: 0.6284317 Test Loss: 0.4062767
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.0697481632232666
Epoch: 25, Steps: 60 | Train Loss: 0.8105597 Vali Loss: 0.6248475 Test Loss: 0.4062572
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.320920467376709
Epoch: 26, Steps: 60 | Train Loss: 0.8119540 Vali Loss: 0.6259871 Test Loss: 0.4062475
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1039087772369385
Epoch: 27, Steps: 60 | Train Loss: 0.8114515 Vali Loss: 0.6261579 Test Loss: 0.4062423
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.5614495277404785
Epoch: 28, Steps: 60 | Train Loss: 0.8131138 Vali Loss: 0.6251262 Test Loss: 0.4062361
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.1902222633361816
Epoch: 29, Steps: 60 | Train Loss: 0.8099495 Vali Loss: 0.6306926 Test Loss: 0.4062162
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.170335292816162
Epoch: 30, Steps: 60 | Train Loss: 0.8101621 Vali Loss: 0.6245542 Test Loss: 0.4062070
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.2225589752197266
Epoch: 31, Steps: 60 | Train Loss: 0.8098997 Vali Loss: 0.6202724 Test Loss: 0.4062342
Validation loss decreased (0.622825 --> 0.620272).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.7215123176574707
Epoch: 32, Steps: 60 | Train Loss: 0.8115378 Vali Loss: 0.6262312 Test Loss: 0.4061972
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.6402554512023926
Epoch: 33, Steps: 60 | Train Loss: 0.8122427 Vali Loss: 0.6248618 Test Loss: 0.4062017
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.057079553604126
Epoch: 34, Steps: 60 | Train Loss: 0.8104573 Vali Loss: 0.6236804 Test Loss: 0.4061860
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.9540042877197266
Epoch: 35, Steps: 60 | Train Loss: 0.8095792 Vali Loss: 0.6239772 Test Loss: 0.4061942
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.1618573665618896
Epoch: 36, Steps: 60 | Train Loss: 0.8115932 Vali Loss: 0.6264034 Test Loss: 0.4061866
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.5879385471343994
Epoch: 37, Steps: 60 | Train Loss: 0.8120122 Vali Loss: 0.6234546 Test Loss: 0.4061903
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.7789180278778076
Epoch: 38, Steps: 60 | Train Loss: 0.8084725 Vali Loss: 0.6276259 Test Loss: 0.4061860
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.047855854034424
Epoch: 39, Steps: 60 | Train Loss: 0.8091668 Vali Loss: 0.6273060 Test Loss: 0.4061854
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.117421865463257
Epoch: 40, Steps: 60 | Train Loss: 0.8101601 Vali Loss: 0.6243740 Test Loss: 0.4061846
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.245391845703125
Epoch: 41, Steps: 60 | Train Loss: 0.8109793 Vali Loss: 0.6239961 Test Loss: 0.4061841
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.0423221588134766
Epoch: 42, Steps: 60 | Train Loss: 0.8098594 Vali Loss: 0.6235538 Test Loss: 0.4061778
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9986259937286377
Epoch: 43, Steps: 60 | Train Loss: 0.8080987 Vali Loss: 0.6229724 Test Loss: 0.4061566
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.981196403503418
Epoch: 44, Steps: 60 | Train Loss: 0.8096791 Vali Loss: 0.6265120 Test Loss: 0.4061707
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.2087390422821045
Epoch: 45, Steps: 60 | Train Loss: 0.8103230 Vali Loss: 0.6261113 Test Loss: 0.4061538
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.9419548511505127
Epoch: 46, Steps: 60 | Train Loss: 0.8099802 Vali Loss: 0.6248666 Test Loss: 0.4061642
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.3403618335723877
Epoch: 47, Steps: 60 | Train Loss: 0.8090706 Vali Loss: 0.6192228 Test Loss: 0.4061499
Validation loss decreased (0.620272 --> 0.619223).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.0525412559509277
Epoch: 48, Steps: 60 | Train Loss: 0.8104788 Vali Loss: 0.6224123 Test Loss: 0.4061563
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.6471710205078125
Epoch: 49, Steps: 60 | Train Loss: 0.8090411 Vali Loss: 0.6227236 Test Loss: 0.4061562
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.0554006099700928
Epoch: 50, Steps: 60 | Train Loss: 0.8120562 Vali Loss: 0.6277117 Test Loss: 0.4061574
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.4751739501953125
Epoch: 51, Steps: 60 | Train Loss: 0.8111270 Vali Loss: 0.6229711 Test Loss: 0.4061543
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.16947603225708
Epoch: 52, Steps: 60 | Train Loss: 0.8105224 Vali Loss: 0.6249490 Test Loss: 0.4061543
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.1589553356170654
Epoch: 53, Steps: 60 | Train Loss: 0.8109543 Vali Loss: 0.6270318 Test Loss: 0.4061510
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.862217664718628
Epoch: 54, Steps: 60 | Train Loss: 0.8118680 Vali Loss: 0.6231620 Test Loss: 0.4061506
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.6998791694641113
Epoch: 55, Steps: 60 | Train Loss: 0.8092606 Vali Loss: 0.6227781 Test Loss: 0.4061456
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.191683053970337
Epoch: 56, Steps: 60 | Train Loss: 0.8079166 Vali Loss: 0.6280829 Test Loss: 0.4061452
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.2734732627868652
Epoch: 57, Steps: 60 | Train Loss: 0.8097300 Vali Loss: 0.6238946 Test Loss: 0.4061443
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.5737385749816895
Epoch: 58, Steps: 60 | Train Loss: 0.8107800 Vali Loss: 0.6225411 Test Loss: 0.4061433
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.503077268600464
Epoch: 59, Steps: 60 | Train Loss: 0.8090996 Vali Loss: 0.6243747 Test Loss: 0.4061424
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.185439348220825
Epoch: 60, Steps: 60 | Train Loss: 0.8100980 Vali Loss: 0.6183359 Test Loss: 0.4061392
Validation loss decreased (0.619223 --> 0.618336).  Saving model ...
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.1090407371520996
Epoch: 61, Steps: 60 | Train Loss: 0.8105882 Vali Loss: 0.6257555 Test Loss: 0.4061418
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.3517637252807617
Epoch: 62, Steps: 60 | Train Loss: 0.8114066 Vali Loss: 0.6250885 Test Loss: 0.4061410
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.331839084625244
Epoch: 63, Steps: 60 | Train Loss: 0.8113724 Vali Loss: 0.6257902 Test Loss: 0.4061385
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.1600093841552734
Epoch: 64, Steps: 60 | Train Loss: 0.8093390 Vali Loss: 0.6227386 Test Loss: 0.4061381
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.9100635051727295
Epoch: 65, Steps: 60 | Train Loss: 0.8118114 Vali Loss: 0.6251326 Test Loss: 0.4061383
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.1573245525360107
Epoch: 66, Steps: 60 | Train Loss: 0.8110274 Vali Loss: 0.6280727 Test Loss: 0.4061374
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.044430732727051
Epoch: 67, Steps: 60 | Train Loss: 0.8099315 Vali Loss: 0.6248726 Test Loss: 0.4061372
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.956076145172119
Epoch: 68, Steps: 60 | Train Loss: 0.8085639 Vali Loss: 0.6221522 Test Loss: 0.4061352
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.711271047592163
Epoch: 69, Steps: 60 | Train Loss: 0.8120042 Vali Loss: 0.6216630 Test Loss: 0.4061357
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.2875492572784424
Epoch: 70, Steps: 60 | Train Loss: 0.8105716 Vali Loss: 0.6252719 Test Loss: 0.4061362
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.304229736328125
Epoch: 71, Steps: 60 | Train Loss: 0.8113978 Vali Loss: 0.6249735 Test Loss: 0.4061349
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.6987733840942383
Epoch: 72, Steps: 60 | Train Loss: 0.8113930 Vali Loss: 0.6279501 Test Loss: 0.4061370
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.778989315032959
Epoch: 73, Steps: 60 | Train Loss: 0.8095532 Vali Loss: 0.6222231 Test Loss: 0.4061334
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.06587290763855
Epoch: 74, Steps: 60 | Train Loss: 0.8089924 Vali Loss: 0.6243948 Test Loss: 0.4061352
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.821784734725952
Epoch: 75, Steps: 60 | Train Loss: 0.8082650 Vali Loss: 0.6218013 Test Loss: 0.4061345
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.465754270553589
Epoch: 76, Steps: 60 | Train Loss: 0.8104637 Vali Loss: 0.6216805 Test Loss: 0.4061343
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.1066677570343018
Epoch: 77, Steps: 60 | Train Loss: 0.8103165 Vali Loss: 0.6255152 Test Loss: 0.4061333
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.986337661743164
Epoch: 78, Steps: 60 | Train Loss: 0.8101668 Vali Loss: 0.6247644 Test Loss: 0.4061328
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.1693310737609863
Epoch: 79, Steps: 60 | Train Loss: 0.8097965 Vali Loss: 0.6221121 Test Loss: 0.4061323
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.468874931335449
Epoch: 80, Steps: 60 | Train Loss: 0.8080476 Vali Loss: 0.6203493 Test Loss: 0.4061301
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.404412180185318, mae:0.4306691884994507, rse:0.508297324180603, corr:[ 2.19352260e-01  2.20828563e-01  2.20127001e-01  2.18365684e-01
  2.16920927e-01  2.16213092e-01  2.15968877e-01  2.15359047e-01
  2.14332536e-01  2.12705970e-01  2.11298063e-01  2.10099757e-01
  2.09463820e-01  2.08902538e-01  2.08239898e-01  2.07298249e-01
  2.06346691e-01  2.05492675e-01  2.04884976e-01  2.04477236e-01
  2.04026878e-01  2.03299925e-01  2.02136740e-01  2.00526386e-01
  1.98532104e-01  1.96876720e-01  1.95439443e-01  1.94404349e-01
  1.93367198e-01  1.92365736e-01  1.91315740e-01  1.90202475e-01
  1.88855261e-01  1.87549308e-01  1.86628699e-01  1.85784340e-01
  1.84998512e-01  1.83872044e-01  1.82905987e-01  1.81814834e-01
  1.81053221e-01  1.80461556e-01  1.80107877e-01  1.79717913e-01
  1.79121286e-01  1.78170517e-01  1.76684797e-01  1.74782306e-01
  1.72461137e-01  1.70582905e-01  1.68850213e-01  1.67469352e-01
  1.66027844e-01  1.64949983e-01  1.64161175e-01  1.63482636e-01
  1.62953585e-01  1.62284613e-01  1.61897033e-01  1.61457092e-01
  1.61479741e-01  1.61324114e-01  1.61406994e-01  1.61142513e-01
  1.60823584e-01  1.60487875e-01  1.60420775e-01  1.60533503e-01
  1.60653934e-01  1.60721362e-01  1.60287738e-01  1.59624413e-01
  1.58508778e-01  1.57613292e-01  1.56934559e-01  1.56504363e-01
  1.56281233e-01  1.56443462e-01  1.56536683e-01  1.56349033e-01
  1.56226188e-01  1.56136468e-01  1.56343326e-01  1.56536713e-01
  1.56890497e-01  1.56938002e-01  1.56940088e-01  1.56622097e-01
  1.56406015e-01  1.56280890e-01  1.56398162e-01  1.56524792e-01
  1.56753778e-01  1.56910747e-01  1.56743988e-01  1.56515956e-01
  1.55858010e-01  1.55416116e-01  1.54805660e-01  1.54471517e-01
  1.54197648e-01  1.53759584e-01  1.53462157e-01  1.53432667e-01
  1.53784305e-01  1.54104471e-01  1.54568925e-01  1.54770896e-01
  1.54938906e-01  1.54699087e-01  1.54459327e-01  1.54135734e-01
  1.53929844e-01  1.53716490e-01  1.53568715e-01  1.53374791e-01
  1.53035492e-01  1.52566850e-01  1.51908562e-01  1.51153892e-01
  1.49972290e-01  1.48769677e-01  1.47380218e-01  1.46384537e-01
  1.45613402e-01  1.45046532e-01  1.44581556e-01  1.44133240e-01
  1.43676609e-01  1.43031418e-01  1.42659977e-01  1.42363787e-01
  1.42170995e-01  1.41755000e-01  1.41157329e-01  1.40388116e-01
  1.39800444e-01  1.39231771e-01  1.38849288e-01  1.38640583e-01
  1.38640001e-01  1.38494045e-01  1.37619525e-01  1.36344969e-01
  1.34366855e-01  1.32922471e-01  1.31774947e-01  1.31225333e-01
  1.30865335e-01  1.30483523e-01  1.30059510e-01  1.29521132e-01
  1.29207671e-01  1.29056036e-01  1.29167587e-01  1.29035503e-01
  1.29095078e-01  1.28912702e-01  1.28428593e-01  1.27720281e-01
  1.27235949e-01  1.26964569e-01  1.26946777e-01  1.27111509e-01
  1.27215087e-01  1.27108231e-01  1.26462623e-01  1.25578105e-01
  1.24222793e-01  1.23345286e-01  1.22470446e-01  1.21934332e-01
  1.21585280e-01  1.21215731e-01  1.20765619e-01  1.20327845e-01
  1.20157823e-01  1.19794950e-01  1.19744562e-01  1.19659878e-01
  1.19725250e-01  1.19510248e-01  1.19173117e-01  1.18682422e-01
  1.18255340e-01  1.17910385e-01  1.17980666e-01  1.18274227e-01
  1.18702456e-01  1.19192481e-01  1.19359560e-01  1.19352169e-01
  1.18999504e-01  1.18936718e-01  1.18943796e-01  1.19262666e-01
  1.19534560e-01  1.19693995e-01  1.19979881e-01  1.20109305e-01
  1.20253086e-01  1.20171994e-01  1.20441832e-01  1.20469593e-01
  1.20643213e-01  1.20571874e-01  1.20493487e-01  1.20237164e-01
  1.20040387e-01  1.20132461e-01  1.20417461e-01  1.20861158e-01
  1.21280909e-01  1.21418655e-01  1.21272191e-01  1.20926388e-01
  1.20350108e-01  1.19793415e-01  1.19256519e-01  1.19067006e-01
  1.18824385e-01  1.18809029e-01  1.18837036e-01  1.19152360e-01
  1.19659781e-01  1.20131776e-01  1.20641984e-01  1.20819226e-01
  1.20994538e-01  1.20938711e-01  1.21035427e-01  1.21270604e-01
  1.21717609e-01  1.22117609e-01  1.22477055e-01  1.22784272e-01
  1.23140454e-01  1.23549514e-01  1.23734131e-01  1.23974748e-01
  1.23801678e-01  1.23711459e-01  1.23377651e-01  1.23173818e-01
  1.22988127e-01  1.23059772e-01  1.23271108e-01  1.23771377e-01
  1.24476746e-01  1.24877557e-01  1.25632092e-01  1.26227573e-01
  1.27110451e-01  1.27841413e-01  1.28399640e-01  1.28672391e-01
  1.28808036e-01  1.29006848e-01  1.29313186e-01  1.29841894e-01
  1.30599529e-01  1.31255254e-01  1.31633013e-01  1.31906018e-01
  1.31842420e-01  1.32002279e-01  1.32090971e-01  1.32605448e-01
  1.32946670e-01  1.33606106e-01  1.34278640e-01  1.34861320e-01
  1.35526136e-01  1.36032283e-01  1.36847153e-01  1.37467071e-01
  1.38336241e-01  1.38905168e-01  1.39460996e-01  1.39720291e-01
  1.39983445e-01  1.40338108e-01  1.40783787e-01  1.41242519e-01
  1.41683832e-01  1.42201990e-01  1.42664835e-01  1.43310830e-01
  1.43572912e-01  1.43720999e-01  1.43433392e-01  1.43476680e-01
  1.43439233e-01  1.43665612e-01  1.44098714e-01  1.44550085e-01
  1.45423785e-01  1.46079794e-01  1.46734610e-01  1.47274688e-01
  1.48137972e-01  1.48721293e-01  1.49292916e-01  1.49732143e-01
  1.49965182e-01  1.50100023e-01  1.50223061e-01  1.50548756e-01
  1.50960684e-01  1.51285991e-01  1.51505932e-01  1.51686862e-01
  1.51547626e-01  1.51402935e-01  1.51167929e-01  1.51096091e-01
  1.50940761e-01  1.51090711e-01  1.51087314e-01  1.51354611e-01
  1.51779994e-01  1.52104139e-01  1.52596936e-01  1.53135836e-01
  1.54049352e-01  1.54439881e-01  1.54714808e-01  1.54806763e-01
  1.54970914e-01  1.55144483e-01  1.55442029e-01  1.55828193e-01
  1.56285524e-01  1.56784981e-01  1.57167763e-01  1.57617927e-01
  1.57661378e-01  1.57751948e-01  1.57695249e-01  1.57854542e-01
  1.58114776e-01  1.58377260e-01  1.58817351e-01  1.59227759e-01
  1.59637749e-01  1.59875482e-01  1.60260782e-01  1.60614893e-01
  1.61289781e-01  1.61786482e-01  1.62234813e-01  1.62492201e-01
  1.62618876e-01  1.62658110e-01  1.62851423e-01  1.63401872e-01
  1.64150983e-01  1.65021151e-01  1.65673107e-01  1.65848881e-01
  1.65587306e-01  1.65276006e-01  1.65020153e-01  1.65095493e-01
  1.65443555e-01  1.66074976e-01  1.66867003e-01  1.67698935e-01
  1.68372840e-01  1.69085220e-01  1.69887125e-01  1.70572177e-01
  1.71403572e-01  1.71786934e-01  1.72082290e-01  1.72228664e-01
  1.72426790e-01  1.72602579e-01  1.72804713e-01  1.73224241e-01
  1.73571348e-01  1.73964560e-01  1.74142942e-01  1.74429819e-01
  1.74424723e-01  1.74428344e-01  1.74190238e-01  1.74369648e-01
  1.74587965e-01  1.74941495e-01  1.75539330e-01  1.76338762e-01
  1.77244499e-01  1.77828461e-01  1.78578854e-01  1.78882763e-01
  1.79246783e-01  1.79137051e-01  1.79003343e-01  1.78848878e-01
  1.78622171e-01  1.78374797e-01  1.78210199e-01  1.78177759e-01
  1.78170174e-01  1.78105831e-01  1.77870139e-01  1.77730456e-01
  1.77439004e-01  1.77311584e-01  1.77118227e-01  1.77083120e-01
  1.77015528e-01  1.77051574e-01  1.77088410e-01  1.77216455e-01
  1.77293122e-01  1.77176610e-01  1.77036658e-01  1.76732346e-01
  1.76420346e-01  1.75742835e-01  1.75094157e-01  1.74322098e-01
  1.73581123e-01  1.72817498e-01  1.72263652e-01  1.71970055e-01
  1.71802238e-01  1.71674103e-01  1.71373859e-01  1.70990482e-01
  1.70317784e-01  1.69620305e-01  1.68798655e-01  1.68116584e-01
  1.67546123e-01  1.67132810e-01  1.66850790e-01  1.66452363e-01
  1.66034251e-01  1.65473670e-01  1.65108502e-01  1.64673775e-01
  1.64331406e-01  1.63770914e-01  1.63194880e-01  1.62667274e-01
  1.62268266e-01  1.62035346e-01  1.61929429e-01  1.62072331e-01
  1.62150532e-01  1.62187994e-01  1.62173942e-01  1.62191898e-01
  1.61974758e-01  1.61824763e-01  1.61379293e-01  1.61016628e-01
  1.60546064e-01  1.60051614e-01  1.59526750e-01  1.59208596e-01
  1.58772603e-01  1.58136532e-01  1.57681793e-01  1.57086328e-01
  1.56566411e-01  1.55754000e-01  1.55109391e-01  1.54537544e-01
  1.54097676e-01  1.53716594e-01  1.53496712e-01  1.53319299e-01
  1.53085694e-01  1.52871922e-01  1.52445585e-01  1.51883215e-01
  1.50809348e-01  1.49633795e-01  1.48420051e-01  1.47301316e-01
  1.46288291e-01  1.45507798e-01  1.44738540e-01  1.43816441e-01
  1.42923996e-01  1.41867504e-01  1.41159207e-01  1.40427724e-01
  1.39819339e-01  1.39002696e-01  1.38186961e-01  1.37393922e-01
  1.36677161e-01  1.36152580e-01  1.35889798e-01  1.35849580e-01
  1.35801330e-01  1.35554403e-01  1.34932071e-01  1.33918867e-01
  1.32359013e-01  1.30965739e-01  1.29795298e-01  1.28822759e-01
  1.27868444e-01  1.27100676e-01  1.26318440e-01  1.25867248e-01
  1.25568897e-01  1.25121757e-01  1.24631584e-01  1.23697095e-01
  1.22875676e-01  1.21941917e-01  1.21308483e-01  1.20727606e-01
  1.20236881e-01  1.19774386e-01  1.19367443e-01  1.19078755e-01
  1.18611902e-01  1.18095092e-01  1.17217742e-01  1.16070241e-01
  1.14357576e-01  1.12595841e-01  1.10702999e-01  1.08844362e-01
  1.06928810e-01  1.05146557e-01  1.03683300e-01  1.02496468e-01
  1.01385638e-01  1.00086495e-01  9.89643261e-02  9.78002399e-02
  9.67067704e-02  9.52567458e-02  9.39218402e-02  9.25911814e-02
  9.14194956e-02  9.06142294e-02  9.00652185e-02  8.96628052e-02
  8.90824422e-02  8.80934894e-02  8.65598544e-02  8.47129598e-02
  8.26219842e-02  8.09797868e-02  7.95431733e-02  7.84159377e-02
  7.71782547e-02  7.59212226e-02  7.47244358e-02  7.36166686e-02
  7.29843155e-02  7.24483207e-02  7.21974596e-02  7.16003403e-02
  7.08462745e-02  6.97315931e-02  6.86595961e-02  6.76186383e-02
  6.69708475e-02  6.65112957e-02  6.60668015e-02  6.55555502e-02
  6.49025068e-02  6.41859099e-02  6.32302240e-02  6.20741881e-02
  6.04377203e-02  5.87566271e-02  5.68856448e-02  5.50773479e-02
  5.34933545e-02  5.22396713e-02  5.12468554e-02  5.03066331e-02
  4.93751951e-02  4.83830646e-02  4.75445911e-02  4.67595346e-02
  4.61295731e-02  4.53008749e-02  4.45217155e-02  4.36499305e-02
  4.29599695e-02  4.25205752e-02  4.23063748e-02  4.22356874e-02
  4.19996046e-02  4.14844304e-02  4.03634161e-02  3.88097316e-02
  3.68925333e-02  3.51775400e-02  3.33939344e-02  3.19296047e-02
  3.02997362e-02  2.87935510e-02  2.71740835e-02  2.57904865e-02
  2.47153491e-02  2.40196418e-02  2.38148049e-02  2.34052967e-02
  2.31095850e-02  2.25823447e-02  2.22357418e-02  2.18667332e-02
  2.16458533e-02  2.16377582e-02  2.19158456e-02  2.21681669e-02
  2.21981686e-02  2.20920201e-02  2.14005820e-02  2.04225369e-02
  1.88331194e-02  1.74436588e-02  1.59311369e-02  1.44913374e-02
  1.31923696e-02  1.23082707e-02  1.18692480e-02  1.15712071e-02
  1.15548130e-02  1.15549359e-02  1.16322143e-02  1.14551913e-02
  1.12073421e-02  1.05556846e-02  9.81445331e-03  9.09110345e-03
  8.33450817e-03  7.75421131e-03  7.54840532e-03  7.47508975e-03
  7.57660437e-03  7.45751243e-03  6.79737097e-03  5.57615794e-03
  3.84640275e-03  2.45307642e-03  1.12778251e-03  1.25203806e-04
 -8.46960873e-04 -1.63403107e-03 -2.49934872e-03 -3.16554750e-03
 -3.32574570e-03 -3.47556523e-03 -3.26913549e-03 -3.59257543e-03
 -3.92153580e-03 -4.44461824e-03 -4.73881047e-03 -5.06334612e-03
 -5.23548108e-03 -5.45069249e-03 -5.52799227e-03 -5.50996233e-03
 -5.59392897e-03 -5.52158663e-03 -5.67448186e-03 -6.10793754e-03
 -7.14842882e-03 -8.48639011e-03 -1.03541566e-02 -1.23213930e-02
 -1.40580963e-02 -1.50239449e-02 -1.52105438e-02 -1.49197876e-02
 -1.44629963e-02 -1.42351175e-02 -1.40610225e-02 -1.43270381e-02
 -1.42174121e-02 -1.43654654e-02 -1.42259914e-02 -1.43796559e-02
 -1.46562085e-02 -1.49323782e-02 -1.49556287e-02 -1.45857614e-02
 -1.41057009e-02 -1.35593815e-02 -1.39702354e-02 -1.51739670e-02
 -1.71450526e-02 -1.87146366e-02 -1.97487250e-02 -2.00541466e-02
 -2.04982776e-02 -2.10509636e-02 -2.18853746e-02 -2.27818489e-02
 -2.33746413e-02 -2.38510240e-02 -2.37665027e-02 -2.38066502e-02
 -2.35890076e-02 -2.42550839e-02 -2.49451082e-02 -2.59582829e-02
 -2.67129280e-02 -2.69487612e-02 -2.71022711e-02 -2.76405346e-02
 -2.84957644e-02 -2.84308698e-02 -2.84447707e-02 -2.79227067e-02]
