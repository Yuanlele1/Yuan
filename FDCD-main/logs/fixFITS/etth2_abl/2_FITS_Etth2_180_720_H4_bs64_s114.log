Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=210, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7902720.0
params:  9030.0
Trainable parameters:  9030
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.5525150299072266
Epoch: 1, Steps: 60 | Train Loss: 1.1210251 Vali Loss: 0.8250648 Test Loss: 0.6080339
Validation loss decreased (inf --> 0.825065).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.8483126163482666
Epoch: 2, Steps: 60 | Train Loss: 0.9269415 Vali Loss: 0.7531514 Test Loss: 0.5315858
Validation loss decreased (0.825065 --> 0.753151).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.4388957023620605
Epoch: 3, Steps: 60 | Train Loss: 0.8223995 Vali Loss: 0.7106311 Test Loss: 0.4885400
Validation loss decreased (0.753151 --> 0.710631).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.651524543762207
Epoch: 4, Steps: 60 | Train Loss: 0.7642163 Vali Loss: 0.6913321 Test Loss: 0.4627934
Validation loss decreased (0.710631 --> 0.691332).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.6163382530212402
Epoch: 5, Steps: 60 | Train Loss: 0.7296913 Vali Loss: 0.6714605 Test Loss: 0.4464901
Validation loss decreased (0.691332 --> 0.671461).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.608510732650757
Epoch: 6, Steps: 60 | Train Loss: 0.7082655 Vali Loss: 0.6628753 Test Loss: 0.4361809
Validation loss decreased (0.671461 --> 0.662875).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.0645320415496826
Epoch: 7, Steps: 60 | Train Loss: 0.6938068 Vali Loss: 0.6555330 Test Loss: 0.4293050
Validation loss decreased (0.662875 --> 0.655533).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.9652118682861328
Epoch: 8, Steps: 60 | Train Loss: 0.6866918 Vali Loss: 0.6534009 Test Loss: 0.4245867
Validation loss decreased (0.655533 --> 0.653401).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.103583335876465
Epoch: 9, Steps: 60 | Train Loss: 0.6798924 Vali Loss: 0.6515036 Test Loss: 0.4213413
Validation loss decreased (0.653401 --> 0.651504).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.139381170272827
Epoch: 10, Steps: 60 | Train Loss: 0.6759573 Vali Loss: 0.6462380 Test Loss: 0.4189544
Validation loss decreased (0.651504 --> 0.646238).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6912243366241455
Epoch: 11, Steps: 60 | Train Loss: 0.6730495 Vali Loss: 0.6465222 Test Loss: 0.4172190
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.2142794132232666
Epoch: 12, Steps: 60 | Train Loss: 0.6687585 Vali Loss: 0.6394573 Test Loss: 0.4158411
Validation loss decreased (0.646238 --> 0.639457).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.0536205768585205
Epoch: 13, Steps: 60 | Train Loss: 0.6673822 Vali Loss: 0.6418800 Test Loss: 0.4148116
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.300701856613159
Epoch: 14, Steps: 60 | Train Loss: 0.6661816 Vali Loss: 0.6386867 Test Loss: 0.4139455
Validation loss decreased (0.639457 --> 0.638687).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.2473368644714355
Epoch: 15, Steps: 60 | Train Loss: 0.6656093 Vali Loss: 0.6380827 Test Loss: 0.4132292
Validation loss decreased (0.638687 --> 0.638083).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7638230323791504
Epoch: 16, Steps: 60 | Train Loss: 0.6652789 Vali Loss: 0.6375495 Test Loss: 0.4126613
Validation loss decreased (0.638083 --> 0.637550).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.322228193283081
Epoch: 17, Steps: 60 | Train Loss: 0.6642185 Vali Loss: 0.6353043 Test Loss: 0.4121878
Validation loss decreased (0.637550 --> 0.635304).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.6983795166015625
Epoch: 18, Steps: 60 | Train Loss: 0.6632516 Vali Loss: 0.6377901 Test Loss: 0.4117548
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.092677116394043
Epoch: 19, Steps: 60 | Train Loss: 0.6631286 Vali Loss: 0.6379442 Test Loss: 0.4113840
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.755152940750122
Epoch: 20, Steps: 60 | Train Loss: 0.6613472 Vali Loss: 0.6345386 Test Loss: 0.4110530
Validation loss decreased (0.635304 --> 0.634539).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0743658542633057
Epoch: 21, Steps: 60 | Train Loss: 0.6631768 Vali Loss: 0.6317579 Test Loss: 0.4108129
Validation loss decreased (0.634539 --> 0.631758).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.459446907043457
Epoch: 22, Steps: 60 | Train Loss: 0.6610113 Vali Loss: 0.6374269 Test Loss: 0.4105361
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.091083526611328
Epoch: 23, Steps: 60 | Train Loss: 0.6609409 Vali Loss: 0.6349123 Test Loss: 0.4102953
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.263767719268799
Epoch: 24, Steps: 60 | Train Loss: 0.6598904 Vali Loss: 0.6397951 Test Loss: 0.4101074
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.5156946182250977
Epoch: 25, Steps: 60 | Train Loss: 0.6601925 Vali Loss: 0.6303031 Test Loss: 0.4099320
Validation loss decreased (0.631758 --> 0.630303).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.1220481395721436
Epoch: 26, Steps: 60 | Train Loss: 0.6582641 Vali Loss: 0.6368096 Test Loss: 0.4097502
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.708521842956543
Epoch: 27, Steps: 60 | Train Loss: 0.6598691 Vali Loss: 0.6328841 Test Loss: 0.4096102
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.4020497798919678
Epoch: 28, Steps: 60 | Train Loss: 0.6596570 Vali Loss: 0.6335508 Test Loss: 0.4094686
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.2514238357543945
Epoch: 29, Steps: 60 | Train Loss: 0.6608313 Vali Loss: 0.6330931 Test Loss: 0.4093217
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.8558096885681152
Epoch: 30, Steps: 60 | Train Loss: 0.6601962 Vali Loss: 0.6314268 Test Loss: 0.4091966
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.488009452819824
Epoch: 31, Steps: 60 | Train Loss: 0.6593384 Vali Loss: 0.6303202 Test Loss: 0.4090869
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.344348907470703
Epoch: 32, Steps: 60 | Train Loss: 0.6587986 Vali Loss: 0.6330624 Test Loss: 0.4089888
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.207357406616211
Epoch: 33, Steps: 60 | Train Loss: 0.6592021 Vali Loss: 0.6316999 Test Loss: 0.4088894
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.6853582859039307
Epoch: 34, Steps: 60 | Train Loss: 0.6593676 Vali Loss: 0.6338526 Test Loss: 0.4087913
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.9329781532287598
Epoch: 35, Steps: 60 | Train Loss: 0.6580525 Vali Loss: 0.6310321 Test Loss: 0.4087094
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.9634876251220703
Epoch: 36, Steps: 60 | Train Loss: 0.6588176 Vali Loss: 0.6329408 Test Loss: 0.4086362
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.579831123352051
Epoch: 37, Steps: 60 | Train Loss: 0.6586509 Vali Loss: 0.6294066 Test Loss: 0.4085600
Validation loss decreased (0.630303 --> 0.629407).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.938563346862793
Epoch: 38, Steps: 60 | Train Loss: 0.6587336 Vali Loss: 0.6334172 Test Loss: 0.4085053
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.1601545810699463
Epoch: 39, Steps: 60 | Train Loss: 0.6581427 Vali Loss: 0.6226503 Test Loss: 0.4084305
Validation loss decreased (0.629407 --> 0.622650).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.5132734775543213
Epoch: 40, Steps: 60 | Train Loss: 0.6571315 Vali Loss: 0.6267904 Test Loss: 0.4083633
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.0244743824005127
Epoch: 41, Steps: 60 | Train Loss: 0.6577944 Vali Loss: 0.6294951 Test Loss: 0.4083134
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.1964304447174072
Epoch: 42, Steps: 60 | Train Loss: 0.6595270 Vali Loss: 0.6325575 Test Loss: 0.4082690
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.9109904766082764
Epoch: 43, Steps: 60 | Train Loss: 0.6584212 Vali Loss: 0.6335267 Test Loss: 0.4082102
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.2907280921936035
Epoch: 44, Steps: 60 | Train Loss: 0.6584426 Vali Loss: 0.6315792 Test Loss: 0.4081620
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.709500312805176
Epoch: 45, Steps: 60 | Train Loss: 0.6567118 Vali Loss: 0.6284199 Test Loss: 0.4081280
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.1286232471466064
Epoch: 46, Steps: 60 | Train Loss: 0.6570785 Vali Loss: 0.6318805 Test Loss: 0.4080773
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.984632968902588
Epoch: 47, Steps: 60 | Train Loss: 0.6563780 Vali Loss: 0.6322896 Test Loss: 0.4080366
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.9362156391143799
Epoch: 48, Steps: 60 | Train Loss: 0.6584432 Vali Loss: 0.6328793 Test Loss: 0.4080073
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.922307014465332
Epoch: 49, Steps: 60 | Train Loss: 0.6575909 Vali Loss: 0.6293737 Test Loss: 0.4079679
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.439685106277466
Epoch: 50, Steps: 60 | Train Loss: 0.6575167 Vali Loss: 0.6293207 Test Loss: 0.4079387
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.0927114486694336
Epoch: 51, Steps: 60 | Train Loss: 0.6578270 Vali Loss: 0.6300418 Test Loss: 0.4079072
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.6088767051696777
Epoch: 52, Steps: 60 | Train Loss: 0.6575162 Vali Loss: 0.6319970 Test Loss: 0.4078689
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.6303632259368896
Epoch: 53, Steps: 60 | Train Loss: 0.6573499 Vali Loss: 0.6288450 Test Loss: 0.4078433
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.155451774597168
Epoch: 54, Steps: 60 | Train Loss: 0.6583735 Vali Loss: 0.6261520 Test Loss: 0.4078184
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.7421884536743164
Epoch: 55, Steps: 60 | Train Loss: 0.6562493 Vali Loss: 0.6293044 Test Loss: 0.4077945
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.2647743225097656
Epoch: 56, Steps: 60 | Train Loss: 0.6578691 Vali Loss: 0.6332102 Test Loss: 0.4077698
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.046252727508545
Epoch: 57, Steps: 60 | Train Loss: 0.6565462 Vali Loss: 0.6285020 Test Loss: 0.4077445
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.539395809173584
Epoch: 58, Steps: 60 | Train Loss: 0.6583770 Vali Loss: 0.6299404 Test Loss: 0.4077217
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.356053590774536
Epoch: 59, Steps: 60 | Train Loss: 0.6566919 Vali Loss: 0.6306040 Test Loss: 0.4077034
EarlyStopping counter: 20 out of 20
Early stopping
train 7741
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=42, out_features=210, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7902720.0
params:  9030.0
Trainable parameters:  9030
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.2331278324127197
Epoch: 1, Steps: 60 | Train Loss: 0.8141734 Vali Loss: 0.6298361 Test Loss: 0.4074175
Validation loss decreased (inf --> 0.629836).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.595639228820801
Epoch: 2, Steps: 60 | Train Loss: 0.8154684 Vali Loss: 0.6326667 Test Loss: 0.4067363
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.405656099319458
Epoch: 3, Steps: 60 | Train Loss: 0.8134115 Vali Loss: 0.6278549 Test Loss: 0.4063274
Validation loss decreased (0.629836 --> 0.627855).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.1616780757904053
Epoch: 4, Steps: 60 | Train Loss: 0.8131391 Vali Loss: 0.6288179 Test Loss: 0.4060260
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.8453991413116455
Epoch: 5, Steps: 60 | Train Loss: 0.8134132 Vali Loss: 0.6302766 Test Loss: 0.4058373
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.2845091819763184
Epoch: 6, Steps: 60 | Train Loss: 0.8104131 Vali Loss: 0.6261836 Test Loss: 0.4057097
Validation loss decreased (0.627855 --> 0.626184).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.5955758094787598
Epoch: 7, Steps: 60 | Train Loss: 0.8115234 Vali Loss: 0.6278176 Test Loss: 0.4055830
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.9675543308258057
Epoch: 8, Steps: 60 | Train Loss: 0.8095284 Vali Loss: 0.6294485 Test Loss: 0.4055097
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.0822017192840576
Epoch: 9, Steps: 60 | Train Loss: 0.8118938 Vali Loss: 0.6229779 Test Loss: 0.4054300
Validation loss decreased (0.626184 --> 0.622978).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.0820000171661377
Epoch: 10, Steps: 60 | Train Loss: 0.8126760 Vali Loss: 0.6247480 Test Loss: 0.4053760
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.8578298091888428
Epoch: 11, Steps: 60 | Train Loss: 0.8124720 Vali Loss: 0.6246367 Test Loss: 0.4053516
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.044055938720703
Epoch: 12, Steps: 60 | Train Loss: 0.8116435 Vali Loss: 0.6256183 Test Loss: 0.4052989
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.23298978805542
Epoch: 13, Steps: 60 | Train Loss: 0.8108329 Vali Loss: 0.6230370 Test Loss: 0.4052653
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.266963005065918
Epoch: 14, Steps: 60 | Train Loss: 0.8096054 Vali Loss: 0.6277217 Test Loss: 0.4052223
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.2561686038970947
Epoch: 15, Steps: 60 | Train Loss: 0.8109058 Vali Loss: 0.6253816 Test Loss: 0.4052160
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.2390172481536865
Epoch: 16, Steps: 60 | Train Loss: 0.8102302 Vali Loss: 0.6212200 Test Loss: 0.4051898
Validation loss decreased (0.622978 --> 0.621220).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.3264896869659424
Epoch: 17, Steps: 60 | Train Loss: 0.8095054 Vali Loss: 0.6215952 Test Loss: 0.4051724
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.5532028675079346
Epoch: 18, Steps: 60 | Train Loss: 0.8084092 Vali Loss: 0.6255181 Test Loss: 0.4051609
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.3935205936431885
Epoch: 19, Steps: 60 | Train Loss: 0.8098643 Vali Loss: 0.6232070 Test Loss: 0.4051414
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.536147356033325
Epoch: 20, Steps: 60 | Train Loss: 0.8096800 Vali Loss: 0.6220046 Test Loss: 0.4051382
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.3793368339538574
Epoch: 21, Steps: 60 | Train Loss: 0.8092856 Vali Loss: 0.6248543 Test Loss: 0.4051071
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.467285633087158
Epoch: 22, Steps: 60 | Train Loss: 0.8096839 Vali Loss: 0.6228302 Test Loss: 0.4051213
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.552480697631836
Epoch: 23, Steps: 60 | Train Loss: 0.8090872 Vali Loss: 0.6263351 Test Loss: 0.4050836
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.258741617202759
Epoch: 24, Steps: 60 | Train Loss: 0.8114088 Vali Loss: 0.6236318 Test Loss: 0.4050748
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.190114974975586
Epoch: 25, Steps: 60 | Train Loss: 0.8083183 Vali Loss: 0.6248661 Test Loss: 0.4050680
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.7716634273529053
Epoch: 26, Steps: 60 | Train Loss: 0.8106094 Vali Loss: 0.6265534 Test Loss: 0.4050432
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1853058338165283
Epoch: 27, Steps: 60 | Train Loss: 0.8112870 Vali Loss: 0.6247644 Test Loss: 0.4050327
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.09055233001709
Epoch: 28, Steps: 60 | Train Loss: 0.8081801 Vali Loss: 0.6233369 Test Loss: 0.4050152
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.0960657596588135
Epoch: 29, Steps: 60 | Train Loss: 0.8106971 Vali Loss: 0.6279558 Test Loss: 0.4050117
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.8394842147827148
Epoch: 30, Steps: 60 | Train Loss: 0.8087532 Vali Loss: 0.6234227 Test Loss: 0.4050156
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.4198896884918213
Epoch: 31, Steps: 60 | Train Loss: 0.8113387 Vali Loss: 0.6256754 Test Loss: 0.4050035
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.2911994457244873
Epoch: 32, Steps: 60 | Train Loss: 0.8108532 Vali Loss: 0.6251004 Test Loss: 0.4050026
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.0819361209869385
Epoch: 33, Steps: 60 | Train Loss: 0.8088730 Vali Loss: 0.6249893 Test Loss: 0.4050014
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.2600674629211426
Epoch: 34, Steps: 60 | Train Loss: 0.8097240 Vali Loss: 0.6232281 Test Loss: 0.4050098
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.7820780277252197
Epoch: 35, Steps: 60 | Train Loss: 0.8088872 Vali Loss: 0.6245370 Test Loss: 0.4049975
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.9778146743774414
Epoch: 36, Steps: 60 | Train Loss: 0.8098082 Vali Loss: 0.6261047 Test Loss: 0.4049767
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_720_FITS_ETTh2_ftM_sl180_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4034248888492584, mae:0.4300509989261627, rse:0.5076765418052673, corr:[ 2.20218763e-01  2.22374588e-01  2.20389575e-01  2.19367117e-01
  2.18961954e-01  2.17695951e-01  2.16196716e-01  2.15417355e-01
  2.15019017e-01  2.13801086e-01  2.12426484e-01  2.11308107e-01
  2.10441604e-01  2.09303409e-01  2.08347604e-01  2.08025470e-01
  2.07944021e-01  2.07217872e-01  2.05981493e-01  2.04966083e-01
  2.04429761e-01  2.03996927e-01  2.03146547e-01  2.01683074e-01
  1.99739560e-01  1.98029205e-01  1.96583375e-01  1.95604682e-01
  1.94688872e-01  1.93646580e-01  1.92393586e-01  1.91126153e-01
  1.89947560e-01  1.88939065e-01  1.88023880e-01  1.86921567e-01
  1.85849130e-01  1.84692964e-01  1.83842152e-01  1.83015421e-01
  1.82452083e-01  1.81992501e-01  1.81624845e-01  1.81124926e-01
  1.80470064e-01  1.79633141e-01  1.78522736e-01  1.76839992e-01
  1.74470693e-01  1.72245979e-01  1.70341909e-01  1.69184566e-01
  1.68081313e-01  1.67070210e-01  1.65998429e-01  1.65095344e-01
  1.64575934e-01  1.64032474e-01  1.63769513e-01  1.63431436e-01
  1.63400650e-01  1.62975192e-01  1.62833109e-01  1.62711695e-01
  1.62810400e-01  1.62646025e-01  1.62241548e-01  1.61891520e-01
  1.62117675e-01  1.62571460e-01  1.62266880e-01  1.61402926e-01
  1.60167515e-01  1.59429759e-01  1.58820361e-01  1.58239901e-01
  1.57849774e-01  1.57861754e-01  1.57630503e-01  1.57167122e-01
  1.57194331e-01  1.57505870e-01  1.57874793e-01  1.57884076e-01
  1.58095315e-01  1.58311352e-01  1.58579692e-01  1.58372566e-01
  1.58018664e-01  1.57690898e-01  1.57601595e-01  1.57440051e-01
  1.57516241e-01  1.57754987e-01  1.57820553e-01  1.57521978e-01
  1.56450197e-01  1.55612424e-01  1.55062288e-01  1.55069172e-01
  1.54906526e-01  1.54339835e-01  1.54017374e-01  1.54153064e-01
  1.54533476e-01  1.54534861e-01  1.54643074e-01  1.54855624e-01
  1.55337498e-01  1.55259624e-01  1.54902264e-01  1.54505670e-01
  1.54431507e-01  1.54334843e-01  1.54097974e-01  1.53795302e-01
  1.53734118e-01  1.53762907e-01  1.53353736e-01  1.52293608e-01
  1.50643781e-01  1.49315283e-01  1.48194522e-01  1.47514388e-01
  1.46834016e-01  1.46114901e-01  1.45419344e-01  1.44811556e-01
  1.44391835e-01  1.43938631e-01  1.43640846e-01  1.43160194e-01
  1.42711908e-01  1.42303422e-01  1.42145932e-01  1.42115027e-01
  1.42028913e-01  1.41425118e-01  1.40561417e-01  1.39861733e-01
  1.39775530e-01  1.39926806e-01  1.39572427e-01  1.38610691e-01
  1.36487126e-01  1.34718552e-01  1.33299723e-01  1.32672906e-01
  1.32263184e-01  1.31834328e-01  1.31468788e-01  1.31155074e-01
  1.31134436e-01  1.31088376e-01  1.31094441e-01  1.30772725e-01
  1.30699858e-01  1.30413041e-01  1.30070850e-01  1.29750103e-01
  1.29598841e-01  1.29295141e-01  1.28926516e-01  1.28715917e-01
  1.28748536e-01  1.28863543e-01  1.28493577e-01  1.27655119e-01
  1.26277983e-01  1.25457153e-01  1.24594249e-01  1.23972222e-01
  1.23549134e-01  1.23242661e-01  1.22848205e-01  1.22289121e-01
  1.21948771e-01  1.21619955e-01  1.21848583e-01  1.21930800e-01
  1.21892057e-01  1.21499024e-01  1.21310577e-01  1.21336266e-01
  1.21476993e-01  1.21319987e-01  1.21126637e-01  1.20881200e-01
  1.20877586e-01  1.21298045e-01  1.21718578e-01  1.21883370e-01
  1.21403523e-01  1.21051706e-01  1.20858073e-01  1.21278100e-01
  1.21783756e-01  1.22125134e-01  1.22394457e-01  1.22325122e-01
  1.22265749e-01  1.22101374e-01  1.22509137e-01  1.22735351e-01
  1.22967742e-01  1.22790694e-01  1.22596331e-01  1.22420944e-01
  1.22454882e-01  1.22644819e-01  1.22770354e-01  1.22916535e-01
  1.23173080e-01  1.23420283e-01  1.23530909e-01  1.23208530e-01
  1.22353800e-01  1.21529490e-01  1.20971993e-01  1.20929770e-01
  1.20812692e-01  1.20758556e-01  1.20599285e-01  1.20828882e-01
  1.21459179e-01  1.22081123e-01  1.22578830e-01  1.22764736e-01
  1.23205096e-01  1.23490505e-01  1.23648688e-01  1.23680525e-01
  1.24003671e-01  1.24494627e-01  1.24932222e-01  1.25135720e-01
  1.25349954e-01  1.25718698e-01  1.25983745e-01  1.26159757e-01
  1.25878707e-01  1.25871480e-01  1.25840053e-01  1.25994921e-01
  1.25802428e-01  1.25478134e-01  1.25199929e-01  1.25465155e-01
  1.26233906e-01  1.26939103e-01  1.28170639e-01  1.29078627e-01
  1.29862785e-01  1.30134135e-01  1.30385444e-01  1.30871639e-01
  1.31531954e-01  1.32019043e-01  1.32149622e-01  1.32181033e-01
  1.32534847e-01  1.33186683e-01  1.33929923e-01  1.34457961e-01
  1.34303898e-01  1.34185493e-01  1.34133682e-01  1.34854257e-01
  1.35390714e-01  1.35939434e-01  1.36285335e-01  1.36790335e-01
  1.37689859e-01  1.38356298e-01  1.39086068e-01  1.39674008e-01
  1.40693679e-01  1.41205892e-01  1.41317084e-01  1.41369194e-01
  1.42038435e-01  1.42955184e-01  1.43469229e-01  1.43484592e-01
  1.43525705e-01  1.43929660e-01  1.44477800e-01  1.45037666e-01
  1.45252824e-01  1.45482868e-01  1.45360559e-01  1.45404264e-01
  1.45209134e-01  1.45293966e-01  1.45656988e-01  1.46120980e-01
  1.47058234e-01  1.47915021e-01  1.48808107e-01  1.49367362e-01
  1.49948776e-01  1.50214061e-01  1.50784925e-01  1.51529104e-01
  1.52141050e-01  1.52514338e-01  1.52588174e-01  1.52553812e-01
  1.52555153e-01  1.52851790e-01  1.53429121e-01  1.53761283e-01
  1.53360128e-01  1.52973577e-01  1.53018877e-01  1.53566495e-01
  1.53633907e-01  1.53412625e-01  1.52985021e-01  1.53383598e-01
  1.54286087e-01  1.54872149e-01  1.55342668e-01  1.55846924e-01
  1.56667426e-01  1.56822935e-01  1.57032311e-01  1.57529682e-01
  1.58136696e-01  1.58221543e-01  1.58090681e-01  1.58308178e-01
  1.58837646e-01  1.59231767e-01  1.59321174e-01  1.59669876e-01
  1.60123885e-01  1.60605371e-01  1.60420954e-01  1.60198018e-01
  1.60416350e-01  1.60964698e-01  1.61381111e-01  1.61282510e-01
  1.61352649e-01  1.61871091e-01  1.62679777e-01  1.63106740e-01
  1.63608506e-01  1.64075226e-01  1.64549902e-01  1.64643079e-01
  1.64611980e-01  1.64963484e-01  1.65744811e-01  1.66439265e-01
  1.66651517e-01  1.66868031e-01  1.67384624e-01  1.67792663e-01
  1.67763665e-01  1.67614281e-01  1.67651102e-01  1.68003589e-01
  1.68278351e-01  1.68450654e-01  1.68837264e-01  1.69658110e-01
  1.70553297e-01  1.71397477e-01  1.72175154e-01  1.72813982e-01
  1.73473552e-01  1.73668876e-01  1.73913091e-01  1.74275234e-01
  1.74612060e-01  1.74610883e-01  1.74469486e-01  1.74713016e-01
  1.75212994e-01  1.75856203e-01  1.76249534e-01  1.76508367e-01
  1.76329896e-01  1.76088542e-01  1.75769597e-01  1.76088125e-01
  1.76563665e-01  1.77043721e-01  1.77418202e-01  1.77749738e-01
  1.78307265e-01  1.78892925e-01  1.79776952e-01  1.80194303e-01
  1.80558562e-01  1.80503100e-01  1.80481449e-01  1.80335060e-01
  1.79998502e-01  1.79714903e-01  1.79623753e-01  1.79636821e-01
  1.79577544e-01  1.79493383e-01  1.79418832e-01  1.79345429e-01
  1.78940162e-01  1.78675815e-01  1.78548560e-01  1.78687587e-01
  1.78662539e-01  1.78539753e-01  1.78422824e-01  1.78489298e-01
  1.78500950e-01  1.78373769e-01  1.78298697e-01  1.78083912e-01
  1.77743092e-01  1.77048773e-01  1.76473513e-01  1.75882980e-01
  1.75224230e-01  1.74402133e-01  1.73704252e-01  1.73381403e-01
  1.73406705e-01  1.73451304e-01  1.73148900e-01  1.72540948e-01
  1.71800509e-01  1.71275511e-01  1.70690000e-01  1.70082361e-01
  1.69468552e-01  1.68990687e-01  1.68557540e-01  1.67937770e-01
  1.67321578e-01  1.66786775e-01  1.66634753e-01  1.66312993e-01
  1.65884227e-01  1.65214136e-01  1.64761901e-01  1.64567828e-01
  1.64394006e-01  1.64158300e-01  1.63972944e-01  1.64037317e-01
  1.64028734e-01  1.63974375e-01  1.63904563e-01  1.63841248e-01
  1.63528636e-01  1.63188592e-01  1.62550315e-01  1.62200645e-01
  1.61931396e-01  1.61695540e-01  1.61248639e-01  1.60655096e-01
  1.59728214e-01  1.58834964e-01  1.58538178e-01  1.58277512e-01
  1.57879919e-01  1.57080486e-01  1.56540081e-01  1.56201690e-01
  1.55772597e-01  1.55006558e-01  1.54290482e-01  1.53972089e-01
  1.54071778e-01  1.54260069e-01  1.53917640e-01  1.53036430e-01
  1.51878491e-01  1.51039213e-01  1.50296152e-01  1.49385139e-01
  1.48206279e-01  1.47077769e-01  1.46112457e-01  1.45360425e-01
  1.44787699e-01  1.43847287e-01  1.42817467e-01  1.41605988e-01
  1.40717417e-01  1.39938101e-01  1.39238268e-01  1.38685629e-01
  1.38488531e-01  1.38425767e-01  1.37789443e-01  1.36567384e-01
  1.35816604e-01  1.36140540e-01  1.36656821e-01  1.36069641e-01
  1.34135261e-01  1.32180348e-01  1.30784467e-01  1.29888594e-01
  1.29151374e-01  1.28628999e-01  1.27944797e-01  1.27210855e-01
  1.26383767e-01  1.25791028e-01  1.25768796e-01  1.25487357e-01
  1.24719761e-01  1.23299778e-01  1.22186705e-01  1.21677950e-01
  1.21523187e-01  1.20986395e-01  1.20010421e-01  1.19374514e-01
  1.19250417e-01  1.19349495e-01  1.18724905e-01  1.17275693e-01
  1.15440965e-01  1.14127897e-01  1.12826936e-01  1.11062266e-01
  1.08715981e-01  1.06543317e-01  1.05147235e-01  1.04255676e-01
  1.03211604e-01  1.01568207e-01  9.99945030e-02  9.86795053e-02
  9.77147743e-02  9.64102373e-02  9.50996205e-02  9.38737020e-02
  9.29714739e-02  9.22941566e-02  9.15386975e-02  9.08885375e-02
  9.03720856e-02  8.97385553e-02  8.85635987e-02  8.67814049e-02
  8.46033394e-02  8.28184932e-02  8.12951103e-02  8.01728293e-02
  7.89332017e-02  7.74081275e-02  7.57193193e-02  7.43675679e-02
  7.40803257e-02  7.41431043e-02  7.40700513e-02  7.31830224e-02
  7.22369626e-02  7.14506656e-02  7.07551911e-02  6.95966780e-02
  6.84873313e-02  6.79458827e-02  6.78988546e-02  6.76508620e-02
  6.69454634e-02  6.62419647e-02  6.56256899e-02  6.45761788e-02
  6.25927746e-02  6.06249310e-02  5.91417626e-02  5.82145154e-02
  5.71472384e-02  5.56064099e-02  5.39274141e-02  5.24734408e-02
  5.13290614e-02  5.03163077e-02  4.95264195e-02  4.87312563e-02
  4.78393845e-02  4.65266034e-02  4.55852039e-02  4.53652218e-02
  4.54816967e-02  4.49890904e-02  4.38661911e-02  4.31669615e-02
  4.32608202e-02  4.33464125e-02  4.23481129e-02  4.04804461e-02
  3.82908471e-02  3.63906324e-02  3.44358347e-02  3.30927372e-02
  3.19329500e-02  3.06723248e-02  2.86140740e-02  2.66184993e-02
  2.55322754e-02  2.53807865e-02  2.55316347e-02  2.49615610e-02
  2.43078489e-02  2.35988349e-02  2.33470891e-02  2.32632328e-02
  2.34364271e-02  2.37373710e-02  2.38982532e-02  2.35711839e-02
  2.33026687e-02  2.36985888e-02  2.37743035e-02  2.27635242e-02
  2.03514714e-02  1.85104273e-02  1.73279755e-02  1.63224451e-02
  1.48579227e-02  1.35575701e-02  1.30882096e-02  1.30515015e-02
  1.30725978e-02  1.27214873e-02  1.24539370e-02  1.22193629e-02
  1.21243577e-02  1.16138328e-02  1.11799408e-02  1.10610388e-02
  1.09544182e-02  1.06495274e-02  1.01373503e-02  9.49744880e-03
  9.38256923e-03  9.44511686e-03  8.98503698e-03  7.61331245e-03
  5.60484221e-03  4.16143658e-03  3.01053468e-03  2.22757342e-03
  1.23975473e-03  8.29805576e-05 -1.27617968e-03 -2.07763119e-03
 -1.94876804e-03 -1.82565139e-03 -1.71192980e-03 -2.31928425e-03
 -2.70518987e-03 -2.98954267e-03 -2.79534189e-03 -2.50269729e-03
 -2.32184003e-03 -2.93627055e-03 -3.96595243e-03 -4.44816565e-03
 -3.83724482e-03 -2.57468061e-03 -2.34520505e-03 -3.42498510e-03
 -5.15111070e-03 -6.54351339e-03 -8.18664022e-03 -9.97550599e-03
 -1.15520367e-02 -1.24432268e-02 -1.29940026e-02 -1.35555705e-02
 -1.38196349e-02 -1.36014745e-02 -1.28596006e-02 -1.25791170e-02
 -1.22438939e-02 -1.25516756e-02 -1.27585111e-02 -1.31896529e-02
 -1.32302064e-02 -1.26874428e-02 -1.20779928e-02 -1.20888390e-02
 -1.25511102e-02 -1.23998178e-02 -1.23812864e-02 -1.32526774e-02
 -1.55335553e-02 -1.73463356e-02 -1.81427822e-02 -1.82312969e-02
 -1.92352068e-02 -2.06378214e-02 -2.17080191e-02 -2.19428036e-02
 -2.15867925e-02 -2.16321908e-02 -2.17904020e-02 -2.22370401e-02
 -2.19769329e-02 -2.24003866e-02 -2.25707591e-02 -2.31978539e-02
 -2.38621142e-02 -2.43765488e-02 -2.47412156e-02 -2.51547601e-02
 -2.62600854e-02 -2.71749124e-02 -2.79459301e-02 -2.70062760e-02]
