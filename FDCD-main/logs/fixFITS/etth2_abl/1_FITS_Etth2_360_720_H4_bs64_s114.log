Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=74, out_features=222, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14719488.0
params:  16650.0
Trainable parameters:  16650
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.280075550079346
Epoch: 1, Steps: 59 | Train Loss: 1.0739610 Vali Loss: 0.7761908 Test Loss: 0.4838076
Validation loss decreased (inf --> 0.776191).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.413956642150879
Epoch: 2, Steps: 59 | Train Loss: 0.9153995 Vali Loss: 0.7183006 Test Loss: 0.4357376
Validation loss decreased (0.776191 --> 0.718301).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9298436641693115
Epoch: 3, Steps: 59 | Train Loss: 0.8603883 Vali Loss: 0.6961185 Test Loss: 0.4175262
Validation loss decreased (0.718301 --> 0.696118).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0370616912841797
Epoch: 4, Steps: 59 | Train Loss: 0.8408913 Vali Loss: 0.6835383 Test Loss: 0.4092769
Validation loss decreased (0.696118 --> 0.683538).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.4374234676361084
Epoch: 5, Steps: 59 | Train Loss: 0.8296761 Vali Loss: 0.6753409 Test Loss: 0.4045137
Validation loss decreased (0.683538 --> 0.675341).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.3755056858062744
Epoch: 6, Steps: 59 | Train Loss: 0.8239564 Vali Loss: 0.6713005 Test Loss: 0.4014955
Validation loss decreased (0.675341 --> 0.671300).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.730146884918213
Epoch: 7, Steps: 59 | Train Loss: 0.8199040 Vali Loss: 0.6690701 Test Loss: 0.3991325
Validation loss decreased (0.671300 --> 0.669070).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.2959020137786865
Epoch: 8, Steps: 59 | Train Loss: 0.8162214 Vali Loss: 0.6582475 Test Loss: 0.3973244
Validation loss decreased (0.669070 --> 0.658247).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.429712772369385
Epoch: 9, Steps: 59 | Train Loss: 0.8139864 Vali Loss: 0.6582026 Test Loss: 0.3958991
Validation loss decreased (0.658247 --> 0.658203).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.197160959243774
Epoch: 10, Steps: 59 | Train Loss: 0.8118005 Vali Loss: 0.6584765 Test Loss: 0.3946816
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.523571014404297
Epoch: 11, Steps: 59 | Train Loss: 0.8103105 Vali Loss: 0.6562549 Test Loss: 0.3937109
Validation loss decreased (0.658203 --> 0.656255).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.8502190113067627
Epoch: 12, Steps: 59 | Train Loss: 0.8080607 Vali Loss: 0.6514378 Test Loss: 0.3928656
Validation loss decreased (0.656255 --> 0.651438).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.855191707611084
Epoch: 13, Steps: 59 | Train Loss: 0.8074733 Vali Loss: 0.6550046 Test Loss: 0.3922299
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.565034866333008
Epoch: 14, Steps: 59 | Train Loss: 0.8064031 Vali Loss: 0.6554075 Test Loss: 0.3916150
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 4.940434217453003
Epoch: 15, Steps: 59 | Train Loss: 0.8053097 Vali Loss: 0.6476098 Test Loss: 0.3911510
Validation loss decreased (0.651438 --> 0.647610).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.4394114017486572
Epoch: 16, Steps: 59 | Train Loss: 0.8045551 Vali Loss: 0.6540449 Test Loss: 0.3907756
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.2632687091827393
Epoch: 17, Steps: 59 | Train Loss: 0.8043046 Vali Loss: 0.6510150 Test Loss: 0.3903798
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.983675479888916
Epoch: 18, Steps: 59 | Train Loss: 0.8035302 Vali Loss: 0.6455663 Test Loss: 0.3900989
Validation loss decreased (0.647610 --> 0.645566).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.083169221878052
Epoch: 19, Steps: 59 | Train Loss: 0.8030004 Vali Loss: 0.6489685 Test Loss: 0.3897988
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.175370931625366
Epoch: 20, Steps: 59 | Train Loss: 0.8028451 Vali Loss: 0.6415551 Test Loss: 0.3895999
Validation loss decreased (0.645566 --> 0.641555).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.641483783721924
Epoch: 21, Steps: 59 | Train Loss: 0.8020550 Vali Loss: 0.6473584 Test Loss: 0.3893420
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.5433316230773926
Epoch: 22, Steps: 59 | Train Loss: 0.8014928 Vali Loss: 0.6469166 Test Loss: 0.3892159
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.0306854248046875
Epoch: 23, Steps: 59 | Train Loss: 0.8009789 Vali Loss: 0.6442064 Test Loss: 0.3890330
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.54121994972229
Epoch: 24, Steps: 59 | Train Loss: 0.8007706 Vali Loss: 0.6448292 Test Loss: 0.3889211
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.940110206604004
Epoch: 25, Steps: 59 | Train Loss: 0.8008897 Vali Loss: 0.6437180 Test Loss: 0.3888207
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.8379883766174316
Epoch: 26, Steps: 59 | Train Loss: 0.8010442 Vali Loss: 0.6431657 Test Loss: 0.3887076
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.0758216381073
Epoch: 27, Steps: 59 | Train Loss: 0.8007586 Vali Loss: 0.6411781 Test Loss: 0.3886176
Validation loss decreased (0.641555 --> 0.641178).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.260084390640259
Epoch: 28, Steps: 59 | Train Loss: 0.8000705 Vali Loss: 0.6427857 Test Loss: 0.3884895
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.7863335609436035
Epoch: 29, Steps: 59 | Train Loss: 0.7999525 Vali Loss: 0.6413330 Test Loss: 0.3884396
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.3369357585906982
Epoch: 30, Steps: 59 | Train Loss: 0.8000579 Vali Loss: 0.6473463 Test Loss: 0.3883832
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.8891947269439697
Epoch: 31, Steps: 59 | Train Loss: 0.7997013 Vali Loss: 0.6459607 Test Loss: 0.3882883
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.818424940109253
Epoch: 32, Steps: 59 | Train Loss: 0.7994661 Vali Loss: 0.6460974 Test Loss: 0.3882691
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.4127657413482666
Epoch: 33, Steps: 59 | Train Loss: 0.7995557 Vali Loss: 0.6441954 Test Loss: 0.3881925
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.9281032085418701
Epoch: 34, Steps: 59 | Train Loss: 0.7988565 Vali Loss: 0.6442528 Test Loss: 0.3881392
EarlyStopping counter: 7 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9505410194396973
Epoch: 35, Steps: 59 | Train Loss: 0.7991595 Vali Loss: 0.6434126 Test Loss: 0.3881236
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.709177017211914
Epoch: 36, Steps: 59 | Train Loss: 0.7993468 Vali Loss: 0.6432914 Test Loss: 0.3880633
EarlyStopping counter: 9 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.363640308380127
Epoch: 37, Steps: 59 | Train Loss: 0.7988620 Vali Loss: 0.6460671 Test Loss: 0.3880290
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.470089912414551
Epoch: 38, Steps: 59 | Train Loss: 0.7989201 Vali Loss: 0.6425176 Test Loss: 0.3879993
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.657076358795166
Epoch: 39, Steps: 59 | Train Loss: 0.7990067 Vali Loss: 0.6443823 Test Loss: 0.3879593
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.461353302001953
Epoch: 40, Steps: 59 | Train Loss: 0.7988419 Vali Loss: 0.6429352 Test Loss: 0.3879497
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.814002513885498
Epoch: 41, Steps: 59 | Train Loss: 0.7992119 Vali Loss: 0.6420251 Test Loss: 0.3879209
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.2461109161376953
Epoch: 42, Steps: 59 | Train Loss: 0.7981539 Vali Loss: 0.6416960 Test Loss: 0.3878975
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.805177927017212
Epoch: 43, Steps: 59 | Train Loss: 0.7989158 Vali Loss: 0.6423212 Test Loss: 0.3878624
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.543750286102295
Epoch: 44, Steps: 59 | Train Loss: 0.7980931 Vali Loss: 0.6461174 Test Loss: 0.3878389
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.5522985458374023
Epoch: 45, Steps: 59 | Train Loss: 0.7985250 Vali Loss: 0.6454903 Test Loss: 0.3878263
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.0311195850372314
Epoch: 46, Steps: 59 | Train Loss: 0.7986332 Vali Loss: 0.6420364 Test Loss: 0.3878026
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.0952937602996826
Epoch: 47, Steps: 59 | Train Loss: 0.7977364 Vali Loss: 0.6438493 Test Loss: 0.3878072
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.38655591011047363, mae:0.42406633496284485, rse:0.49694910645484924, corr:[ 0.21564907  0.22126445  0.22022426  0.21788694  0.21773143  0.21809416
  0.21701747  0.21516424  0.21404393  0.21331678  0.21224755  0.2106431
  0.20933016  0.20851545  0.20784855  0.20703767  0.20624723  0.20551565
  0.20478748  0.20400102  0.20311935  0.2022466   0.20104112  0.1994235
  0.19759253  0.1962475   0.19538943  0.19462787  0.19363625  0.19238295
  0.19126602  0.19035138  0.18940187  0.18804932  0.18673326  0.18582551
  0.18506685  0.18400355  0.1826127   0.18137151  0.18072625  0.18045686
  0.17994726  0.17919046  0.17847955  0.17804037  0.17729595  0.17563799
  0.1736581   0.17231403  0.17156927  0.17087579  0.16969748  0.16839367
  0.16753931  0.16717753  0.16683735  0.16614167  0.16542499  0.16526903
  0.1654935   0.16547891  0.16516362  0.16491573  0.16502851  0.1652925
  0.16523771  0.16496634  0.16487245  0.16515414  0.16526029  0.16469702
  0.16363052  0.16292337  0.16296552  0.16320123  0.1629921   0.16234127
  0.16180174  0.16175853  0.16187015  0.16166799  0.16128789  0.16126552
  0.16172582  0.16209894  0.16188714  0.16133423  0.16096818  0.16105722
  0.16119754  0.16108595  0.16098052  0.1611596   0.16153707  0.16154581
  0.16092527  0.1601163   0.15968767  0.15968987  0.15969048  0.15917982
  0.15857694  0.15836476  0.1586493   0.15876879  0.15847717  0.1581595
  0.15816906  0.158407    0.1583578   0.1577914   0.15709823  0.15695567
  0.15726158  0.15741158  0.15697531  0.15617712  0.15566437  0.1553279
  0.15467069  0.15373832  0.15300435  0.15273958  0.15255806  0.15206844
  0.1511791   0.15056193  0.15057151  0.15066585  0.15032612  0.14940321
  0.14849374  0.14803116  0.14788117  0.14760062  0.1468573   0.14616175
  0.14580856  0.1459575   0.14599451  0.14558648  0.14488505  0.14409962
  0.14302638  0.14191963  0.14123704  0.14131439  0.14166017  0.14149298
  0.14069796  0.13988     0.13974817  0.14002892  0.13990653  0.13900223
  0.13804018  0.13782766  0.1381121   0.13826494  0.13784103  0.13735555
  0.13748494  0.13823456  0.13892484  0.13891064  0.13835184  0.13766739
  0.13725808  0.1369095   0.13656469  0.13646138  0.13660476  0.13661905
  0.13601892  0.13518949  0.1347345   0.1348557   0.1350157   0.13474539
  0.13416003  0.13380806  0.13403305  0.1344826   0.13467078  0.13446449
  0.1344045   0.13481587  0.13534403  0.13567655  0.13569021  0.13565202
  0.13582723  0.13605489  0.13621229  0.13636295  0.13667181  0.13699196
  0.13710414  0.13687871  0.13665049  0.13691732  0.1375637   0.13771626
  0.13730691  0.13689919  0.13703157  0.13760164  0.13802297  0.13800451
  0.13793768  0.13839139  0.13902102  0.13927689  0.1389379   0.13848567
  0.13838904  0.13845225  0.13838801  0.13811536  0.13811241  0.13856912
  0.13908373  0.1392221   0.13904946  0.1391412   0.13976534  0.14024684
  0.14013852  0.13975263  0.13996959  0.14099693  0.14203882  0.14239466
  0.14232627  0.14262424  0.14339145  0.14417261  0.14455435  0.14462236
  0.14478388  0.1451565   0.14551781  0.14557813  0.14558232  0.14579223
  0.14628647  0.14675805  0.14693962  0.14722763  0.1479867   0.14878637
  0.1491066   0.14904346  0.14924489  0.15032117  0.15153152  0.15204732
  0.15198113  0.15227123  0.15339991  0.15478846  0.15556808  0.15536872
  0.15506072  0.15528694  0.15592425  0.15643083  0.15655367  0.15682144
  0.15742037  0.15818019  0.15861519  0.15892988  0.15955777  0.16022186
  0.160571    0.16068646  0.16084237  0.16152075  0.16256551  0.1633489
  0.16332969  0.16312395  0.16338521  0.16437449  0.16555506  0.1661473
  0.16601436  0.16583227  0.16588081  0.1660301   0.16584828  0.16557188
  0.16553695  0.1659087   0.16651157  0.16697225  0.16727453  0.16757235
  0.16758557  0.16734858  0.16727276  0.16775155  0.16848752  0.16899723
  0.16873185  0.16837616  0.16852273  0.16914724  0.1697501   0.16958174
  0.1690371   0.16870973  0.16897352  0.16923845  0.16894796  0.16847017
  0.16841584  0.16878073  0.16899939  0.16869403  0.16831023  0.1683574
  0.16880965  0.16911317  0.16917822  0.16907318  0.16938522  0.16998155
  0.17030361  0.1703795   0.17040262  0.17087385  0.17176794  0.17225835
  0.17202516  0.17139803  0.1712589   0.17169094  0.1721819   0.1721876
  0.17207819  0.17230865  0.17286043  0.17331836  0.17313783  0.17266522
  0.1724284   0.17263637  0.1731569   0.17371622  0.17429426  0.17500906
  0.1756833   0.176102    0.1761014   0.17622402  0.1766155   0.17701375
  0.17687513  0.17616494  0.17560789  0.17568848  0.17633638  0.17697015
  0.17721716  0.17727187  0.17727941  0.17750101  0.1774251   0.17706165
  0.17669469  0.17671008  0.17696711  0.17695354  0.17653058  0.176094
  0.17598787  0.17623033  0.17635605  0.17617697  0.17590708  0.17592879
  0.17608887  0.17587772  0.17522308  0.17462718  0.17462084  0.17517488
  0.17580362  0.17612775  0.17627099  0.1765019   0.17671598  0.17652036
  0.1760784   0.17563371  0.17574869  0.1759643   0.17560074  0.17476548
  0.17405115  0.17390428  0.17400166  0.17374851  0.17310442  0.17260507
  0.17245567  0.17242467  0.17196576  0.1711671   0.17066541  0.17072125
  0.17086495  0.17067365  0.1700452   0.16938348  0.16909327  0.16881762
  0.16801405  0.16677892  0.16573466  0.16511372  0.1643715   0.163058
  0.16153148  0.16065674  0.1605534   0.16049966  0.15959358  0.15809035
  0.15685648  0.1562982   0.15590821  0.15505993  0.15397736  0.15329131
  0.1531729   0.15293162  0.15209429  0.15094337  0.15011     0.1496417
  0.14901583  0.14822756  0.14751785  0.1474294   0.1475125   0.14709328
  0.14606044  0.14535354  0.14527534  0.14545476  0.1450714   0.14400128
  0.14303286  0.14283615  0.14302133  0.14280434  0.14188565  0.14101276
  0.14092523  0.14128833  0.14106865  0.14015517  0.1392044   0.13869232
  0.13841306  0.13789146  0.13733114  0.13711919  0.13728389  0.13709272
  0.13617095  0.13496435  0.13437271  0.13458374  0.13463691  0.13384366
  0.13266535  0.1319728   0.13181013  0.13123159  0.12996808  0.12862156
  0.12802157  0.12787619  0.12742057  0.12625334  0.1249759   0.12411344
  0.12388598  0.12349156  0.12270047  0.12182788  0.12118365  0.12064477
  0.11981864  0.11887939  0.11855897  0.1187233   0.11832212  0.11652603
  0.11398103  0.11241488  0.11222364  0.11219968  0.1112408   0.10973101
  0.10865305  0.10862581  0.1088053   0.10818013  0.106834    0.10557238
  0.10505389  0.10467186  0.10384244  0.10262174  0.10166331  0.10124481
  0.10068219  0.09968567  0.09858449  0.09804883  0.09763303  0.0964843
  0.09446897  0.0925994   0.09178204  0.09129252  0.09021185  0.088429
  0.086989    0.0865269   0.0865455   0.08587731  0.0842864   0.08286818
  0.08228762  0.08207159  0.08139256  0.08002108  0.07853553  0.07786982
  0.077391    0.07669287  0.07582071  0.07508443  0.07460035  0.07352991
  0.07164257  0.06985021  0.06895008  0.0689472   0.06858221  0.06715061
  0.06541337  0.06456017  0.06485021  0.06482857  0.06387181  0.06252509
  0.06187541  0.06216376  0.06228252  0.06143677  0.06036856  0.05985209
  0.05966915  0.05925133  0.05832114  0.05744402  0.05670731  0.05566232
  0.05380942  0.05182204  0.05054778  0.05046601  0.05072149  0.05000031
  0.04826455  0.04659903  0.04605902  0.04624123  0.04572376  0.04444012
  0.04317832  0.0429327   0.04327378  0.04327878  0.04246554  0.04163931
  0.041486    0.04158008  0.04123402  0.04044118  0.03942425  0.03833885
  0.03711985  0.0357233   0.03445856  0.03404306  0.03388564  0.03319832
  0.03171957  0.03005848  0.02928418  0.02963068  0.02992378  0.02937254
  0.02821436  0.02756288  0.02794266  0.02853437  0.02851066  0.02819801
  0.02818748  0.02867536  0.02900058  0.02879283  0.02819514  0.02753665
  0.02697552  0.0262335   0.02546812  0.0251552   0.02535929  0.02558085
  0.02516065  0.02415075  0.02371487  0.02419237  0.02515156  0.02543227
  0.0246127   0.02347585  0.02309843  0.02391005  0.02438799  0.02395362
  0.02290207  0.02216259  0.02223895  0.02261754  0.02256794  0.02180503
  0.02064411  0.01937394  0.01805935  0.01740362  0.01757339  0.01752926
  0.01686146  0.01570845  0.01517643  0.01582366  0.01710777  0.01721683
  0.01569374  0.01412164  0.01387637  0.01507542  0.01605929  0.01581594
  0.0147322   0.01408006  0.01428095  0.01525338  0.0155804   0.014985
  0.01397733  0.01285469  0.01180548  0.01096844  0.01087613  0.01096917
  0.01068982  0.00977311  0.00910638  0.0099402   0.01150011  0.01252566
  0.01181595  0.01022582  0.00984677  0.01119356  0.01307126  0.01365112
  0.01277633  0.01176684  0.01191014  0.01299446  0.01333933  0.01232225
  0.01012211  0.00814017  0.00656319  0.00533392  0.00513818  0.0058847
  0.00641433  0.00509942  0.00326497  0.00290081  0.00460719  0.00695309
  0.00678122  0.00402127  0.00174228  0.00310167  0.00626384  0.00732639
  0.00323808 -0.00202934 -0.00292795 -0.00047495 -0.00196388 -0.01626868]
