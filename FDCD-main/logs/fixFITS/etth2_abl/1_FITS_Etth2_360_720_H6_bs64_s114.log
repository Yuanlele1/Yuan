Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=106, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7561
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=106, out_features=318, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  30202368.0
params:  34026.0
Trainable parameters:  34026
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.8010189533233643
Epoch: 1, Steps: 59 | Train Loss: 1.0611025 Vali Loss: 0.7687409 Test Loss: 0.4740008
Validation loss decreased (inf --> 0.768741).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5604817867279053
Epoch: 2, Steps: 59 | Train Loss: 0.8958102 Vali Loss: 0.7117941 Test Loss: 0.4276525
Validation loss decreased (0.768741 --> 0.711794).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.6225929260253906
Epoch: 3, Steps: 59 | Train Loss: 0.8487343 Vali Loss: 0.6896477 Test Loss: 0.4119897
Validation loss decreased (0.711794 --> 0.689648).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3855066299438477
Epoch: 4, Steps: 59 | Train Loss: 0.8312133 Vali Loss: 0.6741798 Test Loss: 0.4051404
Validation loss decreased (0.689648 --> 0.674180).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.2692453861236572
Epoch: 5, Steps: 59 | Train Loss: 0.8225376 Vali Loss: 0.6720164 Test Loss: 0.4011634
Validation loss decreased (0.674180 --> 0.672016).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.630770206451416
Epoch: 6, Steps: 59 | Train Loss: 0.8174711 Vali Loss: 0.6679441 Test Loss: 0.3984048
Validation loss decreased (0.672016 --> 0.667944).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.8146588802337646
Epoch: 7, Steps: 59 | Train Loss: 0.8136678 Vali Loss: 0.6642960 Test Loss: 0.3964468
Validation loss decreased (0.667944 --> 0.664296).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.5058157444000244
Epoch: 8, Steps: 59 | Train Loss: 0.8112015 Vali Loss: 0.6599143 Test Loss: 0.3948758
Validation loss decreased (0.664296 --> 0.659914).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.03952956199646
Epoch: 9, Steps: 59 | Train Loss: 0.8087343 Vali Loss: 0.6604638 Test Loss: 0.3935630
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.3497037887573242
Epoch: 10, Steps: 59 | Train Loss: 0.8073884 Vali Loss: 0.6526891 Test Loss: 0.3926270
Validation loss decreased (0.659914 --> 0.652689).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.7530946731567383
Epoch: 11, Steps: 59 | Train Loss: 0.8059457 Vali Loss: 0.6524023 Test Loss: 0.3916790
Validation loss decreased (0.652689 --> 0.652402).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.0917468070983887
Epoch: 12, Steps: 59 | Train Loss: 0.8042207 Vali Loss: 0.6488966 Test Loss: 0.3910827
Validation loss decreased (0.652402 --> 0.648897).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6328177452087402
Epoch: 13, Steps: 59 | Train Loss: 0.8036606 Vali Loss: 0.6488301 Test Loss: 0.3904718
Validation loss decreased (0.648897 --> 0.648830).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.7108128070831299
Epoch: 14, Steps: 59 | Train Loss: 0.8025628 Vali Loss: 0.6470691 Test Loss: 0.3899962
Validation loss decreased (0.648830 --> 0.647069).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.7063910961151123
Epoch: 15, Steps: 59 | Train Loss: 0.8020676 Vali Loss: 0.6496789 Test Loss: 0.3895857
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.7867257595062256
Epoch: 16, Steps: 59 | Train Loss: 0.8013933 Vali Loss: 0.6462372 Test Loss: 0.3892759
Validation loss decreased (0.647069 --> 0.646237).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.8766331672668457
Epoch: 17, Steps: 59 | Train Loss: 0.8007129 Vali Loss: 0.6494218 Test Loss: 0.3889998
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.1881980895996094
Epoch: 18, Steps: 59 | Train Loss: 0.8005781 Vali Loss: 0.6460844 Test Loss: 0.3887825
Validation loss decreased (0.646237 --> 0.646084).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.168386220932007
Epoch: 19, Steps: 59 | Train Loss: 0.7997104 Vali Loss: 0.6477453 Test Loss: 0.3885489
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.893190622329712
Epoch: 20, Steps: 59 | Train Loss: 0.7997399 Vali Loss: 0.6426091 Test Loss: 0.3883918
Validation loss decreased (0.646084 --> 0.642609).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0428788661956787
Epoch: 21, Steps: 59 | Train Loss: 0.7990494 Vali Loss: 0.6474342 Test Loss: 0.3882201
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.731999397277832
Epoch: 22, Steps: 59 | Train Loss: 0.7992111 Vali Loss: 0.6455513 Test Loss: 0.3880832
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.586437702178955
Epoch: 23, Steps: 59 | Train Loss: 0.7992608 Vali Loss: 0.6435965 Test Loss: 0.3879519
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.5258514881134033
Epoch: 24, Steps: 59 | Train Loss: 0.7986540 Vali Loss: 0.6458512 Test Loss: 0.3878661
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.3952679634094238
Epoch: 25, Steps: 59 | Train Loss: 0.7984923 Vali Loss: 0.6447921 Test Loss: 0.3877327
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.409393072128296
Epoch: 26, Steps: 59 | Train Loss: 0.7979238 Vali Loss: 0.6425997 Test Loss: 0.3876703
Validation loss decreased (0.642609 --> 0.642600).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8093671798706055
Epoch: 27, Steps: 59 | Train Loss: 0.7982162 Vali Loss: 0.6426094 Test Loss: 0.3875841
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.677337408065796
Epoch: 28, Steps: 59 | Train Loss: 0.7978846 Vali Loss: 0.6470747 Test Loss: 0.3875024
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.9088399410247803
Epoch: 29, Steps: 59 | Train Loss: 0.7978508 Vali Loss: 0.6448660 Test Loss: 0.3874437
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.6634583473205566
Epoch: 30, Steps: 59 | Train Loss: 0.7980915 Vali Loss: 0.6437535 Test Loss: 0.3873983
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.5261671543121338
Epoch: 31, Steps: 59 | Train Loss: 0.7973794 Vali Loss: 0.6423481 Test Loss: 0.3873456
Validation loss decreased (0.642600 --> 0.642348).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.398514986038208
Epoch: 32, Steps: 59 | Train Loss: 0.7975674 Vali Loss: 0.6394426 Test Loss: 0.3873048
Validation loss decreased (0.642348 --> 0.639443).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.94537353515625
Epoch: 33, Steps: 59 | Train Loss: 0.7976584 Vali Loss: 0.6389487 Test Loss: 0.3872695
Validation loss decreased (0.639443 --> 0.638949).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.6786494255065918
Epoch: 34, Steps: 59 | Train Loss: 0.7971870 Vali Loss: 0.6471717 Test Loss: 0.3872396
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.3029654026031494
Epoch: 35, Steps: 59 | Train Loss: 0.7970241 Vali Loss: 0.6446701 Test Loss: 0.3871841
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.2585318088531494
Epoch: 36, Steps: 59 | Train Loss: 0.7963434 Vali Loss: 0.6417681 Test Loss: 0.3871382
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.2858388423919678
Epoch: 37, Steps: 59 | Train Loss: 0.7969361 Vali Loss: 0.6425295 Test Loss: 0.3871361
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.6637961864471436
Epoch: 38, Steps: 59 | Train Loss: 0.7966577 Vali Loss: 0.6447651 Test Loss: 0.3871088
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7994043827056885
Epoch: 39, Steps: 59 | Train Loss: 0.7969381 Vali Loss: 0.6419543 Test Loss: 0.3870657
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.6652369499206543
Epoch: 40, Steps: 59 | Train Loss: 0.7965392 Vali Loss: 0.6448454 Test Loss: 0.3870288
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.558875560760498
Epoch: 41, Steps: 59 | Train Loss: 0.7965604 Vali Loss: 0.6391566 Test Loss: 0.3870324
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.6210746765136719
Epoch: 42, Steps: 59 | Train Loss: 0.7969613 Vali Loss: 0.6407449 Test Loss: 0.3870166
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.619100570678711
Epoch: 43, Steps: 59 | Train Loss: 0.7966699 Vali Loss: 0.6422803 Test Loss: 0.3869968
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.0755233764648438
Epoch: 44, Steps: 59 | Train Loss: 0.7961792 Vali Loss: 0.6447018 Test Loss: 0.3869630
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.3709652423858643
Epoch: 45, Steps: 59 | Train Loss: 0.7966976 Vali Loss: 0.6413174 Test Loss: 0.3869502
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.9309883117675781
Epoch: 46, Steps: 59 | Train Loss: 0.7957075 Vali Loss: 0.6446769 Test Loss: 0.3869387
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.242964267730713
Epoch: 47, Steps: 59 | Train Loss: 0.7965893 Vali Loss: 0.6430118 Test Loss: 0.3869348
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.6578667163848877
Epoch: 48, Steps: 59 | Train Loss: 0.7966488 Vali Loss: 0.6416179 Test Loss: 0.3868949
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.6595358848571777
Epoch: 49, Steps: 59 | Train Loss: 0.7961651 Vali Loss: 0.6406420 Test Loss: 0.3868901
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.9642724990844727
Epoch: 50, Steps: 59 | Train Loss: 0.7955945 Vali Loss: 0.6417135 Test Loss: 0.3868891
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.6779279708862305
Epoch: 51, Steps: 59 | Train Loss: 0.7959675 Vali Loss: 0.6375225 Test Loss: 0.3868683
Validation loss decreased (0.638949 --> 0.637523).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.286977767944336
Epoch: 52, Steps: 59 | Train Loss: 0.7962225 Vali Loss: 0.6432539 Test Loss: 0.3868605
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.8201091289520264
Epoch: 53, Steps: 59 | Train Loss: 0.7950627 Vali Loss: 0.6412247 Test Loss: 0.3868503
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.7565407752990723
Epoch: 54, Steps: 59 | Train Loss: 0.7955020 Vali Loss: 0.6448874 Test Loss: 0.3868424
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.8884658813476562
Epoch: 55, Steps: 59 | Train Loss: 0.7961349 Vali Loss: 0.6428677 Test Loss: 0.3868298
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7340731620788574
Epoch: 56, Steps: 59 | Train Loss: 0.7956317 Vali Loss: 0.6432182 Test Loss: 0.3868104
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.1661550998687744
Epoch: 57, Steps: 59 | Train Loss: 0.7956012 Vali Loss: 0.6417365 Test Loss: 0.3868070
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.0569188594818115
Epoch: 58, Steps: 59 | Train Loss: 0.7957166 Vali Loss: 0.6412452 Test Loss: 0.3868128
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.7177531719207764
Epoch: 59, Steps: 59 | Train Loss: 0.7941678 Vali Loss: 0.6372812 Test Loss: 0.3868004
Validation loss decreased (0.637523 --> 0.637281).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.6274747848510742
Epoch: 60, Steps: 59 | Train Loss: 0.7955233 Vali Loss: 0.6395632 Test Loss: 0.3867857
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.369431972503662
Epoch: 61, Steps: 59 | Train Loss: 0.7958723 Vali Loss: 0.6405154 Test Loss: 0.3867910
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.150240898132324
Epoch: 62, Steps: 59 | Train Loss: 0.7955992 Vali Loss: 0.6414722 Test Loss: 0.3867850
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.283156394958496
Epoch: 63, Steps: 59 | Train Loss: 0.7962734 Vali Loss: 0.6402395 Test Loss: 0.3867733
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.011683940887451
Epoch: 64, Steps: 59 | Train Loss: 0.7957243 Vali Loss: 0.6461127 Test Loss: 0.3867752
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.503838300704956
Epoch: 65, Steps: 59 | Train Loss: 0.7957897 Vali Loss: 0.6404595 Test Loss: 0.3867650
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.84830904006958
Epoch: 66, Steps: 59 | Train Loss: 0.7956043 Vali Loss: 0.6409929 Test Loss: 0.3867649
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.6070361137390137
Epoch: 67, Steps: 59 | Train Loss: 0.7956430 Vali Loss: 0.6425689 Test Loss: 0.3867592
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.9697260856628418
Epoch: 68, Steps: 59 | Train Loss: 0.7949476 Vali Loss: 0.6401260 Test Loss: 0.3867587
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.78177809715271
Epoch: 69, Steps: 59 | Train Loss: 0.7960883 Vali Loss: 0.6425439 Test Loss: 0.3867466
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9260063171386719
Epoch: 70, Steps: 59 | Train Loss: 0.7958887 Vali Loss: 0.6436489 Test Loss: 0.3867478
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.3111462593078613
Epoch: 71, Steps: 59 | Train Loss: 0.7952774 Vali Loss: 0.6432955 Test Loss: 0.3867450
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.7173032760620117
Epoch: 72, Steps: 59 | Train Loss: 0.7955711 Vali Loss: 0.6429098 Test Loss: 0.3867416
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.3112967014312744
Epoch: 73, Steps: 59 | Train Loss: 0.7949695 Vali Loss: 0.6410156 Test Loss: 0.3867365
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.469243288040161
Epoch: 74, Steps: 59 | Train Loss: 0.7960480 Vali Loss: 0.6395750 Test Loss: 0.3867345
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.4157731533050537
Epoch: 75, Steps: 59 | Train Loss: 0.7949261 Vali Loss: 0.6409065 Test Loss: 0.3867325
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.9649522304534912
Epoch: 76, Steps: 59 | Train Loss: 0.7952573 Vali Loss: 0.6411830 Test Loss: 0.3867381
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.94942307472229
Epoch: 77, Steps: 59 | Train Loss: 0.7953818 Vali Loss: 0.6361918 Test Loss: 0.3867289
Validation loss decreased (0.637281 --> 0.636192).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.8769824504852295
Epoch: 78, Steps: 59 | Train Loss: 0.7945924 Vali Loss: 0.6418128 Test Loss: 0.3867245
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 4.043420791625977
Epoch: 79, Steps: 59 | Train Loss: 0.7952470 Vali Loss: 0.6419810 Test Loss: 0.3867238
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 5.210903644561768
Epoch: 80, Steps: 59 | Train Loss: 0.7948088 Vali Loss: 0.6422815 Test Loss: 0.3867228
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 4.334413290023804
Epoch: 81, Steps: 59 | Train Loss: 0.7951244 Vali Loss: 0.6405816 Test Loss: 0.3867168
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.794127941131592
Epoch: 82, Steps: 59 | Train Loss: 0.7960885 Vali Loss: 0.6393695 Test Loss: 0.3867170
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 3.5632197856903076
Epoch: 83, Steps: 59 | Train Loss: 0.7958220 Vali Loss: 0.6425527 Test Loss: 0.3867161
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.4666340351104736
Epoch: 84, Steps: 59 | Train Loss: 0.7958081 Vali Loss: 0.6424109 Test Loss: 0.3867145
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.1338019371032715
Epoch: 85, Steps: 59 | Train Loss: 0.7957986 Vali Loss: 0.6404117 Test Loss: 0.3867117
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.812173366546631
Epoch: 86, Steps: 59 | Train Loss: 0.7948925 Vali Loss: 0.6354709 Test Loss: 0.3867106
Validation loss decreased (0.636192 --> 0.635471).  Saving model ...
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.3812520503997803
Epoch: 87, Steps: 59 | Train Loss: 0.7951792 Vali Loss: 0.6423846 Test Loss: 0.3867084
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 4.287570476531982
Epoch: 88, Steps: 59 | Train Loss: 0.7955938 Vali Loss: 0.6402808 Test Loss: 0.3867065
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 3.8633170127868652
Epoch: 89, Steps: 59 | Train Loss: 0.7955205 Vali Loss: 0.6402860 Test Loss: 0.3867064
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 4.211351633071899
Epoch: 90, Steps: 59 | Train Loss: 0.7955874 Vali Loss: 0.6429899 Test Loss: 0.3867051
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.9509663581848145
Epoch: 91, Steps: 59 | Train Loss: 0.7951577 Vali Loss: 0.6415203 Test Loss: 0.3867039
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.9500114917755127
Epoch: 92, Steps: 59 | Train Loss: 0.7952236 Vali Loss: 0.6383227 Test Loss: 0.3867018
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 3.59350323677063
Epoch: 93, Steps: 59 | Train Loss: 0.7948945 Vali Loss: 0.6434283 Test Loss: 0.3867022
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 3.1991958618164062
Epoch: 94, Steps: 59 | Train Loss: 0.7949455 Vali Loss: 0.6399848 Test Loss: 0.3867007
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.639343500137329
Epoch: 95, Steps: 59 | Train Loss: 0.7956341 Vali Loss: 0.6396266 Test Loss: 0.3866988
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.9050636291503906
Epoch: 96, Steps: 59 | Train Loss: 0.7951470 Vali Loss: 0.6375753 Test Loss: 0.3866985
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.92991042137146
Epoch: 97, Steps: 59 | Train Loss: 0.7957431 Vali Loss: 0.6402026 Test Loss: 0.3866968
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.9502086639404297
Epoch: 98, Steps: 59 | Train Loss: 0.7958533 Vali Loss: 0.6388409 Test Loss: 0.3866961
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 1.6393685340881348
Epoch: 99, Steps: 59 | Train Loss: 0.7958729 Vali Loss: 0.6413639 Test Loss: 0.3866963
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 1.6841402053833008
Epoch: 100, Steps: 59 | Train Loss: 0.7959512 Vali Loss: 0.6406568 Test Loss: 0.3866943
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_360_720_FITS_ETTh2_ftM_sl360_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3847094476222992, mae:0.42271241545677185, rse:0.4957607388496399, corr:[ 0.21765323  0.22125933  0.2190907   0.22002676  0.21871324  0.2173267
  0.2176782   0.21671145  0.21502987  0.2144708   0.21362987  0.21184045
  0.210622    0.20984462  0.20895025  0.20833296  0.20798187  0.20712814
  0.20614259  0.20540845  0.20441654  0.20356713  0.20267344  0.20127857
  0.19931795  0.1979688   0.19701885  0.19599444  0.19515304  0.19444194
  0.19358233  0.19237931  0.19115435  0.18998963  0.18907352  0.18817037
  0.18688719  0.18580456  0.18507269  0.18423313  0.18342037  0.18303914
  0.18263975  0.18195263  0.18121591  0.18072656  0.17996733  0.17847143
  0.17685817  0.1754271   0.17418881  0.17323986  0.17231509  0.17153047
  0.17076536  0.16982883  0.16884793  0.1682774   0.16793764  0.16763327
  0.1674281   0.16735318  0.16732937  0.16735269  0.16751912  0.16746168
  0.16714415  0.16729398  0.16763997  0.16753078  0.16713518  0.16684239
  0.16638973  0.16587926  0.16572101  0.1654298   0.16487615  0.16474554
  0.1648356   0.16437633  0.16376314  0.16385823  0.16410665  0.16384283
  0.16380565  0.16420399  0.16419812  0.16389574  0.16406843  0.16422269
  0.16381286  0.16379358  0.16432239  0.16436544  0.16425888  0.16443753
  0.16410942  0.16319911  0.162917    0.16309051  0.16270503  0.16213545
  0.16228464  0.1622687   0.16183263  0.16171482  0.1620202   0.16195937
  0.16166118  0.16168532  0.16146758  0.16100776  0.16093145  0.16105512
  0.16065627  0.16034918  0.16044529  0.16008615  0.1595444   0.15904745
  0.15807308  0.15714438  0.15708426  0.15696082  0.15588883  0.15517598
  0.1552375   0.1548005   0.15387146  0.1535837   0.15365     0.1529722
  0.15216719  0.15173475  0.15111533  0.15048827  0.15033163  0.15026253
  0.14953265  0.14915834  0.14934604  0.14922564  0.1488281   0.148064
  0.14650506  0.14517036  0.14486583  0.14490448  0.14449053  0.14417748
  0.14405966  0.14345951  0.14305155  0.14310415  0.14276665  0.14184588
  0.14137556  0.1413815   0.14101323  0.1408297   0.14094923  0.1409069
  0.14072531  0.14097643  0.14143296  0.14154063  0.14151198  0.14113888
  0.14035511  0.13965227  0.13962005  0.13970624  0.13932684  0.13904966
  0.1389087   0.13827735  0.13756426  0.1374775   0.13732938  0.13688639
  0.13668486  0.13663404  0.13646087  0.13651146  0.13677263  0.13680607
  0.1369825   0.13753143  0.13780464  0.1378573   0.13842748  0.13902426
  0.1387764   0.13844146  0.13900198  0.13951677  0.1394536   0.1397165
  0.14017057  0.13983454  0.13939072  0.13976604  0.14012271  0.13978982
  0.13979682  0.14003721  0.13981134  0.13978934  0.1402333   0.14060584
  0.14078782  0.14127612  0.14156029  0.14164743  0.14185698  0.14184283
  0.14125593  0.14083797  0.14107417  0.14090028  0.14059275  0.1409426
  0.14131369  0.1412729   0.14153558  0.14217056  0.14233193  0.14214951
  0.14246176  0.14261875  0.14239776  0.142791    0.14362483  0.14402673
  0.14432329  0.14502853  0.14548713  0.1457139   0.14626563  0.14673781
  0.14699285  0.14739016  0.14769349  0.14765581  0.14784582  0.14812268
  0.14785801  0.14776543  0.14856999  0.14945796  0.14957489  0.14972901
  0.15046461  0.15085244  0.1508698   0.15186568  0.15293913  0.15320791
  0.15341386  0.15401566  0.15445706  0.15501638  0.15618142  0.15679342
  0.15681699  0.15706247  0.15759034  0.15778738  0.15796007  0.15863141
  0.15905076  0.15940423  0.1599581   0.16068117  0.16127405  0.16173036
  0.16215454  0.16238484  0.16274871  0.1635579   0.1640256   0.16424188
  0.16477206  0.16538838  0.16536498  0.1656024   0.1664917   0.16696654
  0.16688734  0.16713691  0.16732328  0.16699779  0.16694266  0.16740201
  0.16729587  0.1669694   0.16746974  0.16813302  0.1682922   0.16834821
  0.16835622  0.16817673  0.16837603  0.16904913  0.1692847   0.1692381
  0.16934602  0.1696107   0.1695482   0.16970049  0.17031625  0.17034544
  0.17007327  0.17012988  0.17037115  0.17012808  0.16982578  0.16978796
  0.16959363  0.1695736   0.17000349  0.1700551   0.16957736  0.16947168
  0.1697495   0.16964588  0.16977115  0.17042564  0.17103931  0.17110631
  0.17118196  0.1716342   0.17178623  0.17210399  0.17284587  0.17306182
  0.1727899   0.17265153  0.17280346  0.17285581  0.17312992  0.17346752
  0.17352502  0.17341037  0.17357938  0.17385192  0.17382246  0.1738863
  0.17396106  0.17388704  0.17419945  0.1749303   0.17536594  0.17562166
  0.17613345  0.17665768  0.17667972  0.17695034  0.17741963  0.1775739
  0.17726238  0.17693385  0.17673156  0.17640124  0.1766022   0.1771929
  0.17747867  0.1777794   0.17815344  0.17829125  0.17780462  0.17752208
  0.17763169  0.17754786  0.1772574   0.17725521  0.17746139  0.17734405
  0.1770579   0.17704128  0.17703359  0.17700367  0.17695706  0.176976
  0.17690456  0.176635    0.17613976  0.17567597  0.1757723   0.17624243
  0.1765314   0.17674999  0.17726316  0.17765623  0.17755216  0.17749111
  0.17763036  0.17726757  0.17686023  0.17690524  0.17672104  0.17600232
  0.1754919   0.1754252   0.17507336  0.17466131  0.17453595  0.17419168
  0.17364682  0.1735299   0.17347828  0.173018    0.17270608  0.17255364
  0.17202371  0.1717281   0.17193946  0.17171693  0.17080435  0.17017367
  0.16976619  0.16886428  0.16791415  0.16736323  0.16641821  0.16515896
  0.16434084  0.16376817  0.16294461  0.16238533  0.1619493   0.16101404
  0.15985587  0.15928602  0.15880701  0.15789329  0.1573148   0.15695164
  0.15621339  0.15549846  0.15525377  0.15448022  0.1531691   0.15263417
  0.15244378  0.15178455  0.15117685  0.1512413   0.1508435   0.14992319
  0.14954408  0.14944863  0.14879566  0.14845887  0.14858797  0.14819051
  0.1475077   0.1473467   0.14710835  0.14655393  0.14639471  0.14632091
  0.14563258  0.14520416  0.14535294  0.14498264  0.14410803  0.14374956
  0.14338586  0.1422206   0.14156939  0.14171061  0.14132336  0.1404802
  0.14039983  0.14031798  0.1394829   0.13923745  0.1393838   0.13850652
  0.13723771  0.13686042  0.13645324  0.13542661  0.1348854   0.13451023
  0.13353103  0.13256314  0.13206734  0.13118231  0.13026305  0.1299291
  0.12945402  0.12818703  0.1275527   0.12739606  0.12642074  0.12543328
  0.12527272  0.12477869  0.12387396  0.12353659  0.12302344  0.12140066
  0.11967359  0.11875436  0.1176022   0.11642341  0.11607032  0.11568443
  0.11465768  0.114266    0.11421629  0.11338919  0.11252072  0.11209479
  0.11113996  0.10970601  0.10923032  0.10890278  0.10777583  0.1069808
  0.10672083  0.105918    0.10487436  0.10470667  0.10411076  0.10236442
  0.10075923  0.09987801  0.09880171  0.09753226  0.09669148  0.09534444
  0.09381182  0.09318903  0.09288514  0.0916746   0.09049029  0.09004708
  0.08883528  0.08733569  0.0869167   0.0864355   0.0851245   0.08456751
  0.08413804  0.08306716  0.08221191  0.08205343  0.08137556  0.07946309
  0.0777907   0.07676613  0.07549906  0.07470059  0.07435192  0.07337821
  0.07211488  0.07153094  0.0711339   0.07018124  0.06980377  0.06973471
  0.0689368   0.06841867  0.06855411  0.06795292  0.06689465  0.06659027
  0.06631994  0.06522222  0.06414621  0.0637653   0.06273108  0.0610478
  0.0597313   0.05880176  0.05751653  0.05659504  0.05597352  0.05479806
  0.05377588  0.05334683  0.05263821  0.05169745  0.05124619  0.05100886
  0.04993849  0.04918349  0.04917491  0.04881549  0.04790969  0.0477233
  0.04782538  0.04717749  0.0465092   0.04636573  0.04553084  0.04402808
  0.04298269  0.04227652  0.04137976  0.04068971  0.03968719  0.03844603
  0.03766213  0.03713156  0.03625017  0.03556714  0.03550866  0.03537901
  0.03451663  0.03401111  0.0343358   0.03433084  0.03395969  0.03418125
  0.03447924  0.03440292  0.0343735   0.03455919  0.03424871  0.03354267
  0.03308968  0.03257055  0.0320739   0.03178223  0.0312541   0.03067466
  0.03069643  0.03077604  0.03047872  0.03024745  0.03078912  0.03095907
  0.03037249  0.03000994  0.02979681  0.02932653  0.02884808  0.0290288
  0.02867907  0.02777419  0.02763658  0.02784395  0.02724716  0.02622405
  0.02538323  0.0243997   0.0234569   0.02309292  0.0224614   0.02137186
  0.02140139  0.02132862  0.02035852  0.02011753  0.02094133  0.02031893
  0.01885193  0.01904067  0.01960105  0.01907508  0.01862202  0.01907052
  0.01880752  0.01810333  0.01824522  0.01886455  0.01813311  0.01732792
  0.01705587  0.01634454  0.01573122  0.01545103  0.01471335  0.01352614
  0.01340821  0.01337735  0.01295339  0.01371497  0.01444731  0.01377016
  0.01276942  0.01324544  0.01370962  0.01294247  0.0132527   0.01433692
  0.01408641  0.01345261  0.01401959  0.0142431   0.01321826  0.0128942
  0.01213326  0.01055786  0.00928924  0.00863029  0.00799123  0.00780079
  0.00802809  0.00691156  0.00628408  0.00676057  0.00586133  0.00452266
  0.00488514  0.00544857  0.00360735  0.00316571  0.0045738   0.00369606
  0.0015107   0.00304267  0.00195903 -0.00357109 -0.00206372 -0.00338953]
