Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=34, out_features=160, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4874240.0
params:  5600.0
Trainable parameters:  5600
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.7657816410064697
Epoch: 1, Steps: 64 | Train Loss: 0.8255096 Vali Loss: 0.4907076 Test Loss: 0.5737152
Validation loss decreased (inf --> 0.490708).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.6332879066467285
Epoch: 2, Steps: 64 | Train Loss: 0.6992263 Vali Loss: 0.4480530 Test Loss: 0.5181964
Validation loss decreased (0.490708 --> 0.448053).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.181223392486572
Epoch: 3, Steps: 64 | Train Loss: 0.6333814 Vali Loss: 0.4233163 Test Loss: 0.4862421
Validation loss decreased (0.448053 --> 0.423316).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.345581531524658
Epoch: 4, Steps: 64 | Train Loss: 0.5941469 Vali Loss: 0.4066360 Test Loss: 0.4670256
Validation loss decreased (0.423316 --> 0.406636).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.9068663120269775
Epoch: 5, Steps: 64 | Train Loss: 0.5717624 Vali Loss: 0.3917727 Test Loss: 0.4550202
Validation loss decreased (0.406636 --> 0.391773).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.5959982872009277
Epoch: 6, Steps: 64 | Train Loss: 0.5562791 Vali Loss: 0.3892970 Test Loss: 0.4471376
Validation loss decreased (0.391773 --> 0.389297).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.7688608169555664
Epoch: 7, Steps: 64 | Train Loss: 0.5487279 Vali Loss: 0.3830477 Test Loss: 0.4419272
Validation loss decreased (0.389297 --> 0.383048).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.536245346069336
Epoch: 8, Steps: 64 | Train Loss: 0.5416288 Vali Loss: 0.3804035 Test Loss: 0.4382215
Validation loss decreased (0.383048 --> 0.380403).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.263000965118408
Epoch: 9, Steps: 64 | Train Loss: 0.5373869 Vali Loss: 0.3782905 Test Loss: 0.4357363
Validation loss decreased (0.380403 --> 0.378291).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.948745012283325
Epoch: 10, Steps: 64 | Train Loss: 0.5336640 Vali Loss: 0.3786465 Test Loss: 0.4338149
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 4.145292520523071
Epoch: 11, Steps: 64 | Train Loss: 0.5307413 Vali Loss: 0.3791586 Test Loss: 0.4323480
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 4.08829665184021
Epoch: 12, Steps: 64 | Train Loss: 0.5300428 Vali Loss: 0.3795525 Test Loss: 0.4311986
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.0914926528930664
Epoch: 13, Steps: 64 | Train Loss: 0.5283061 Vali Loss: 0.3752441 Test Loss: 0.4303358
Validation loss decreased (0.378291 --> 0.375244).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.8940346240997314
Epoch: 14, Steps: 64 | Train Loss: 0.5258385 Vali Loss: 0.3740268 Test Loss: 0.4295827
Validation loss decreased (0.375244 --> 0.374027).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.54268217086792
Epoch: 15, Steps: 64 | Train Loss: 0.5262723 Vali Loss: 0.3715493 Test Loss: 0.4289919
Validation loss decreased (0.374027 --> 0.371549).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.9589807987213135
Epoch: 16, Steps: 64 | Train Loss: 0.5249846 Vali Loss: 0.3724676 Test Loss: 0.4284623
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.8987104892730713
Epoch: 17, Steps: 64 | Train Loss: 0.5247451 Vali Loss: 0.3708602 Test Loss: 0.4279640
Validation loss decreased (0.371549 --> 0.370860).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.121925354003906
Epoch: 18, Steps: 64 | Train Loss: 0.5233500 Vali Loss: 0.3702935 Test Loss: 0.4276373
Validation loss decreased (0.370860 --> 0.370294).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 4.524097204208374
Epoch: 19, Steps: 64 | Train Loss: 0.5232402 Vali Loss: 0.3717833 Test Loss: 0.4272822
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.832059144973755
Epoch: 20, Steps: 64 | Train Loss: 0.5222011 Vali Loss: 0.3700067 Test Loss: 0.4269624
Validation loss decreased (0.370294 --> 0.370007).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.666020393371582
Epoch: 21, Steps: 64 | Train Loss: 0.5223868 Vali Loss: 0.3718306 Test Loss: 0.4266791
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.578679800033569
Epoch: 22, Steps: 64 | Train Loss: 0.5199678 Vali Loss: 0.3718358 Test Loss: 0.4264609
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.691802740097046
Epoch: 23, Steps: 64 | Train Loss: 0.5203402 Vali Loss: 0.3713074 Test Loss: 0.4262526
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.75358247756958
Epoch: 24, Steps: 64 | Train Loss: 0.5213762 Vali Loss: 0.3702817 Test Loss: 0.4260193
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.7192132472991943
Epoch: 25, Steps: 64 | Train Loss: 0.5209320 Vali Loss: 0.3717447 Test Loss: 0.4258428
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.154754877090454
Epoch: 26, Steps: 64 | Train Loss: 0.5203520 Vali Loss: 0.3678234 Test Loss: 0.4257097
Validation loss decreased (0.370007 --> 0.367823).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.51702618598938
Epoch: 27, Steps: 64 | Train Loss: 0.5202741 Vali Loss: 0.3685144 Test Loss: 0.4255653
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.2676548957824707
Epoch: 28, Steps: 64 | Train Loss: 0.5203163 Vali Loss: 0.3673645 Test Loss: 0.4254224
Validation loss decreased (0.367823 --> 0.367364).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.9190673828125
Epoch: 29, Steps: 64 | Train Loss: 0.5202127 Vali Loss: 0.3680586 Test Loss: 0.4253215
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.6185245513916016
Epoch: 30, Steps: 64 | Train Loss: 0.5194636 Vali Loss: 0.3673571 Test Loss: 0.4251758
Validation loss decreased (0.367364 --> 0.367357).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.0057778358459473
Epoch: 31, Steps: 64 | Train Loss: 0.5181054 Vali Loss: 0.3708635 Test Loss: 0.4250810
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.2421653270721436
Epoch: 32, Steps: 64 | Train Loss: 0.5186657 Vali Loss: 0.3669202 Test Loss: 0.4249640
Validation loss decreased (0.367357 --> 0.366920).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.451190233230591
Epoch: 33, Steps: 64 | Train Loss: 0.5188403 Vali Loss: 0.3661296 Test Loss: 0.4248836
Validation loss decreased (0.366920 --> 0.366130).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.174560070037842
Epoch: 34, Steps: 64 | Train Loss: 0.5187387 Vali Loss: 0.3647393 Test Loss: 0.4247983
Validation loss decreased (0.366130 --> 0.364739).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.666104316711426
Epoch: 35, Steps: 64 | Train Loss: 0.5173555 Vali Loss: 0.3680549 Test Loss: 0.4247082
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.78851580619812
Epoch: 36, Steps: 64 | Train Loss: 0.5182385 Vali Loss: 0.3665110 Test Loss: 0.4246633
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.0535032749176025
Epoch: 37, Steps: 64 | Train Loss: 0.5184129 Vali Loss: 0.3671655 Test Loss: 0.4245924
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.972243070602417
Epoch: 38, Steps: 64 | Train Loss: 0.5181209 Vali Loss: 0.3662713 Test Loss: 0.4245233
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 4.842381000518799
Epoch: 39, Steps: 64 | Train Loss: 0.5181315 Vali Loss: 0.3691405 Test Loss: 0.4244761
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.5500781536102295
Epoch: 40, Steps: 64 | Train Loss: 0.5186769 Vali Loss: 0.3681984 Test Loss: 0.4243992
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.7375545501708984
Epoch: 41, Steps: 64 | Train Loss: 0.5168662 Vali Loss: 0.3690082 Test Loss: 0.4243352
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.825371742248535
Epoch: 42, Steps: 64 | Train Loss: 0.5178282 Vali Loss: 0.3686251 Test Loss: 0.4243067
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.1494109630584717
Epoch: 43, Steps: 64 | Train Loss: 0.5179622 Vali Loss: 0.3660217 Test Loss: 0.4242758
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.4067814350128174
Epoch: 44, Steps: 64 | Train Loss: 0.5184921 Vali Loss: 0.3658963 Test Loss: 0.4242187
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.5505218505859375
Epoch: 45, Steps: 64 | Train Loss: 0.5181776 Vali Loss: 0.3689246 Test Loss: 0.4241820
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.2323312759399414
Epoch: 46, Steps: 64 | Train Loss: 0.5172651 Vali Loss: 0.3671491 Test Loss: 0.4241399
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.867806911468506
Epoch: 47, Steps: 64 | Train Loss: 0.5179674 Vali Loss: 0.3645718 Test Loss: 0.4241031
Validation loss decreased (0.364739 --> 0.364572).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.5197181701660156
Epoch: 48, Steps: 64 | Train Loss: 0.5178500 Vali Loss: 0.3685461 Test Loss: 0.4240587
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.368332624435425
Epoch: 49, Steps: 64 | Train Loss: 0.5158784 Vali Loss: 0.3662294 Test Loss: 0.4240305
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.9555327892303467
Epoch: 50, Steps: 64 | Train Loss: 0.5176435 Vali Loss: 0.3682244 Test Loss: 0.4240018
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.6243414878845215
Epoch: 51, Steps: 64 | Train Loss: 0.5175783 Vali Loss: 0.3663032 Test Loss: 0.4239830
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.3712406158447266
Epoch: 52, Steps: 64 | Train Loss: 0.5170536 Vali Loss: 0.3662375 Test Loss: 0.4239494
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.167111396789551
Epoch: 53, Steps: 64 | Train Loss: 0.5169697 Vali Loss: 0.3639341 Test Loss: 0.4239210
Validation loss decreased (0.364572 --> 0.363934).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.91616153717041
Epoch: 54, Steps: 64 | Train Loss: 0.5162873 Vali Loss: 0.3666552 Test Loss: 0.4238939
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.906254768371582
Epoch: 55, Steps: 64 | Train Loss: 0.5173068 Vali Loss: 0.3672451 Test Loss: 0.4238781
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.585279941558838
Epoch: 56, Steps: 64 | Train Loss: 0.5177613 Vali Loss: 0.3674658 Test Loss: 0.4238548
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.6372766494750977
Epoch: 57, Steps: 64 | Train Loss: 0.5173806 Vali Loss: 0.3667759 Test Loss: 0.4238372
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.7351319789886475
Epoch: 58, Steps: 64 | Train Loss: 0.5177301 Vali Loss: 0.3676956 Test Loss: 0.4238124
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 4.130727291107178
Epoch: 59, Steps: 64 | Train Loss: 0.5178852 Vali Loss: 0.3654614 Test Loss: 0.4237887
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.8084208965301514
Epoch: 60, Steps: 64 | Train Loss: 0.5170154 Vali Loss: 0.3673179 Test Loss: 0.4237739
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.2644355297088623
Epoch: 61, Steps: 64 | Train Loss: 0.5154933 Vali Loss: 0.3666140 Test Loss: 0.4237641
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.6840579509735107
Epoch: 62, Steps: 64 | Train Loss: 0.5173901 Vali Loss: 0.3658912 Test Loss: 0.4237448
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.018711805343628
Epoch: 63, Steps: 64 | Train Loss: 0.5167186 Vali Loss: 0.3678106 Test Loss: 0.4237267
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 3.0779011249542236
Epoch: 64, Steps: 64 | Train Loss: 0.5164079 Vali Loss: 0.3660153 Test Loss: 0.4237114
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.065237283706665
Epoch: 65, Steps: 64 | Train Loss: 0.5171894 Vali Loss: 0.3657120 Test Loss: 0.4236976
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.803365707397461
Epoch: 66, Steps: 64 | Train Loss: 0.5172898 Vali Loss: 0.3673999 Test Loss: 0.4236877
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.0973544120788574
Epoch: 67, Steps: 64 | Train Loss: 0.5165890 Vali Loss: 0.3675626 Test Loss: 0.4236779
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 3.51643967628479
Epoch: 68, Steps: 64 | Train Loss: 0.5160190 Vali Loss: 0.3671775 Test Loss: 0.4236654
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.401648998260498
Epoch: 69, Steps: 64 | Train Loss: 0.5172324 Vali Loss: 0.3677849 Test Loss: 0.4236517
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.8643875122070312
Epoch: 70, Steps: 64 | Train Loss: 0.5152312 Vali Loss: 0.3680002 Test Loss: 0.4236470
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.8369908332824707
Epoch: 71, Steps: 64 | Train Loss: 0.5163499 Vali Loss: 0.3677340 Test Loss: 0.4236335
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.9522054195404053
Epoch: 72, Steps: 64 | Train Loss: 0.5165059 Vali Loss: 0.3682193 Test Loss: 0.4236245
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.4957449436187744
Epoch: 73, Steps: 64 | Train Loss: 0.5159957 Vali Loss: 0.3651044 Test Loss: 0.4236162
EarlyStopping counter: 20 out of 20
Early stopping
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=34, out_features=160, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4874240.0
params:  5600.0
Trainable parameters:  5600
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 4.204077243804932
Epoch: 1, Steps: 64 | Train Loss: 0.6509746 Vali Loss: 0.3642399 Test Loss: 0.4231055
Validation loss decreased (inf --> 0.364240).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.600671052932739
Epoch: 2, Steps: 64 | Train Loss: 0.6506678 Vali Loss: 0.3678282 Test Loss: 0.4227965
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 4.077279090881348
Epoch: 3, Steps: 64 | Train Loss: 0.6499031 Vali Loss: 0.3642794 Test Loss: 0.4223990
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0549418926239014
Epoch: 4, Steps: 64 | Train Loss: 0.6494640 Vali Loss: 0.3654321 Test Loss: 0.4222078
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.4656941890716553
Epoch: 5, Steps: 64 | Train Loss: 0.6497753 Vali Loss: 0.3659225 Test Loss: 0.4220198
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.7221128940582275
Epoch: 6, Steps: 64 | Train Loss: 0.6486796 Vali Loss: 0.3630484 Test Loss: 0.4218291
Validation loss decreased (0.364240 --> 0.363048).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1735446453094482
Epoch: 7, Steps: 64 | Train Loss: 0.6484320 Vali Loss: 0.3616840 Test Loss: 0.4218003
Validation loss decreased (0.363048 --> 0.361684).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.987661361694336
Epoch: 8, Steps: 64 | Train Loss: 0.6471575 Vali Loss: 0.3648264 Test Loss: 0.4216410
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.1479439735412598
Epoch: 9, Steps: 64 | Train Loss: 0.6476225 Vali Loss: 0.3616602 Test Loss: 0.4216074
Validation loss decreased (0.361684 --> 0.361660).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.755847454071045
Epoch: 10, Steps: 64 | Train Loss: 0.6478892 Vali Loss: 0.3623174 Test Loss: 0.4214937
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.7451131343841553
Epoch: 11, Steps: 64 | Train Loss: 0.6470425 Vali Loss: 0.3631608 Test Loss: 0.4213859
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.5000081062316895
Epoch: 12, Steps: 64 | Train Loss: 0.6473509 Vali Loss: 0.3633241 Test Loss: 0.4213811
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.160871505737305
Epoch: 13, Steps: 64 | Train Loss: 0.6470507 Vali Loss: 0.3607999 Test Loss: 0.4213316
Validation loss decreased (0.361660 --> 0.360800).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.6297574043273926
Epoch: 14, Steps: 64 | Train Loss: 0.6469533 Vali Loss: 0.3621365 Test Loss: 0.4212151
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.943575859069824
Epoch: 15, Steps: 64 | Train Loss: 0.6467257 Vali Loss: 0.3644920 Test Loss: 0.4212483
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.7910454273223877
Epoch: 16, Steps: 64 | Train Loss: 0.6465680 Vali Loss: 0.3633927 Test Loss: 0.4211935
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.425469160079956
Epoch: 17, Steps: 64 | Train Loss: 0.6464157 Vali Loss: 0.3630098 Test Loss: 0.4211023
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.254850149154663
Epoch: 18, Steps: 64 | Train Loss: 0.6453556 Vali Loss: 0.3576129 Test Loss: 0.4210959
Validation loss decreased (0.360800 --> 0.357613).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.6258609294891357
Epoch: 19, Steps: 64 | Train Loss: 0.6455605 Vali Loss: 0.3634315 Test Loss: 0.4210547
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.4801933765411377
Epoch: 20, Steps: 64 | Train Loss: 0.6452885 Vali Loss: 0.3623289 Test Loss: 0.4210676
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.197974920272827
Epoch: 21, Steps: 64 | Train Loss: 0.6458033 Vali Loss: 0.3596895 Test Loss: 0.4209946
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.877253770828247
Epoch: 22, Steps: 64 | Train Loss: 0.6455027 Vali Loss: 0.3630351 Test Loss: 0.4209836
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.8259847164154053
Epoch: 23, Steps: 64 | Train Loss: 0.6452891 Vali Loss: 0.3612210 Test Loss: 0.4209542
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.872537851333618
Epoch: 24, Steps: 64 | Train Loss: 0.6455228 Vali Loss: 0.3612607 Test Loss: 0.4209519
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.516572952270508
Epoch: 25, Steps: 64 | Train Loss: 0.6454375 Vali Loss: 0.3606243 Test Loss: 0.4209116
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.07571268081665
Epoch: 26, Steps: 64 | Train Loss: 0.6457541 Vali Loss: 0.3623617 Test Loss: 0.4209349
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.6233723163604736
Epoch: 27, Steps: 64 | Train Loss: 0.6444456 Vali Loss: 0.3609778 Test Loss: 0.4208919
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.0802488327026367
Epoch: 28, Steps: 64 | Train Loss: 0.6458226 Vali Loss: 0.3612746 Test Loss: 0.4208898
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.771618366241455
Epoch: 29, Steps: 64 | Train Loss: 0.6456677 Vali Loss: 0.3601577 Test Loss: 0.4208640
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.123485565185547
Epoch: 30, Steps: 64 | Train Loss: 0.6455907 Vali Loss: 0.3624589 Test Loss: 0.4208722
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.307890892028809
Epoch: 31, Steps: 64 | Train Loss: 0.6446749 Vali Loss: 0.3620510 Test Loss: 0.4208680
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.823307991027832
Epoch: 32, Steps: 64 | Train Loss: 0.6455685 Vali Loss: 0.3596400 Test Loss: 0.4208393
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.8210031986236572
Epoch: 33, Steps: 64 | Train Loss: 0.6457448 Vali Loss: 0.3611045 Test Loss: 0.4208478
EarlyStopping counter: 15 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.599233865737915
Epoch: 34, Steps: 64 | Train Loss: 0.6452919 Vali Loss: 0.3632918 Test Loss: 0.4208242
EarlyStopping counter: 16 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.2865703105926514
Epoch: 35, Steps: 64 | Train Loss: 0.6459292 Vali Loss: 0.3610165 Test Loss: 0.4208209
EarlyStopping counter: 17 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.9973082542419434
Epoch: 36, Steps: 64 | Train Loss: 0.6450462 Vali Loss: 0.3629149 Test Loss: 0.4208055
EarlyStopping counter: 18 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.53745174407959
Epoch: 37, Steps: 64 | Train Loss: 0.6454423 Vali Loss: 0.3620942 Test Loss: 0.4208018
EarlyStopping counter: 19 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 4.909634351730347
Epoch: 38, Steps: 64 | Train Loss: 0.6434452 Vali Loss: 0.3630604 Test Loss: 0.4207909
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4163687527179718, mae:0.42497020959854126, rse:0.5159155130386353, corr:[0.2648387  0.26554745 0.26429367 0.2640979  0.2615842  0.26089552
 0.2593835  0.25845954 0.25743783 0.25589356 0.25536302 0.2537253
 0.25196865 0.25078478 0.24963503 0.24895994 0.24819036 0.24750854
 0.24671243 0.24550837 0.24451761 0.24352661 0.24276386 0.24109039
 0.23797834 0.23590145 0.23393957 0.23249881 0.23150188 0.23049413
 0.22980182 0.22867846 0.22734989 0.22611436 0.22534257 0.22445332
 0.22326131 0.22222528 0.22145571 0.22050296 0.21961243 0.21904871
 0.21844755 0.21780214 0.21692304 0.2154928  0.21469302 0.21273653
 0.20894074 0.20607312 0.20343739 0.20136236 0.19960448 0.19792667
 0.1963113  0.193928   0.19316867 0.19157338 0.19041774 0.18952861
 0.18845685 0.1879614  0.18720624 0.18699408 0.18699817 0.1861926
 0.18554498 0.18496227 0.18391916 0.18336903 0.1828276  0.18110365
 0.1781585  0.17655803 0.17525272 0.1741324  0.17284006 0.17244083
 0.17259899 0.17162855 0.17140153 0.17075285 0.17044148 0.170175
 0.16914408 0.1689573  0.16918847 0.16888972 0.16839178 0.16787708
 0.16748498 0.1670659  0.16676168 0.16648278 0.16619562 0.16505863
 0.1629965  0.16100702 0.15880911 0.15773004 0.15693997 0.156108
 0.15634364 0.15600853 0.15625468 0.15583348 0.1557919  0.15575002
 0.15490234 0.15435566 0.15371384 0.15345801 0.15299517 0.15217741
 0.1518107  0.15109123 0.15030767 0.14969788 0.14852586 0.14626363
 0.14340448 0.14135697 0.13950484 0.13828993 0.13686927 0.13596329
 0.13570428 0.13511992 0.13482781 0.13404085 0.13367493 0.13311274
 0.13212854 0.13184455 0.13161977 0.13116568 0.1307731  0.13007039
 0.12971592 0.12935638 0.12846896 0.12799229 0.1271627  0.1242652
 0.12062886 0.11830032 0.11582188 0.11440749 0.11325316 0.11222266
 0.11199203 0.11139737 0.11139646 0.11090104 0.11056379 0.11023618
 0.10958526 0.10955825 0.1095198  0.10919372 0.1089775  0.10881934
 0.10854861 0.10813371 0.10794719 0.10797523 0.10776246 0.10552185
 0.10228983 0.10068078 0.09898293 0.09805891 0.09738914 0.09667712
 0.09688888 0.09690228 0.09700853 0.09663705 0.09676178 0.09660199
 0.0962168  0.09598119 0.0957781  0.09617871 0.09618881 0.09622785
 0.09642384 0.09620629 0.09637308 0.09696389 0.09717651 0.09631019
 0.09453346 0.09387296 0.09307039 0.09274602 0.09253349 0.09255396
 0.09427767 0.09505067 0.09518716 0.09547721 0.09597848 0.09561445
 0.09511941 0.09484221 0.09439111 0.09434906 0.09437157 0.09450913
 0.09452496 0.09468333 0.09488352 0.09472477 0.09463479 0.09298652
 0.09045964 0.0891304  0.08728126 0.08642016 0.08616285 0.08622099
 0.08749302 0.08833923 0.08871011 0.08911695 0.08935    0.08867521
 0.08801708 0.08763901 0.08747598 0.08760677 0.08797375 0.0884939
 0.08865821 0.08900741 0.08949261 0.08969501 0.08964372 0.08833556
 0.08580025 0.08455762 0.08299206 0.08244202 0.08287997 0.08303212
 0.0846834  0.08635496 0.08731212 0.08766838 0.08775175 0.08774677
 0.08790974 0.08807131 0.08840793 0.08932802 0.08957303 0.09070095
 0.09124191 0.09114909 0.09140591 0.09163547 0.09188361 0.09179613
 0.09063487 0.08970214 0.08934603 0.08969191 0.08973346 0.09021082
 0.09193824 0.09315184 0.09365117 0.0940688  0.09451894 0.09394921
 0.09363738 0.09348648 0.09347238 0.09361142 0.09362856 0.09393949
 0.09458175 0.09463663 0.09452745 0.09485855 0.09527431 0.09457672
 0.09335885 0.09294156 0.09196139 0.09141661 0.09151664 0.09144777
 0.09138802 0.09132992 0.09205577 0.09288757 0.092957   0.09273005
 0.09301766 0.09281369 0.09275268 0.09385603 0.09436734 0.09513032
 0.09576894 0.09597531 0.0968704  0.09718863 0.0976178  0.09696535
 0.0952987  0.09525142 0.09435896 0.09404811 0.09433591 0.09433564
 0.09573778 0.09688326 0.09752113 0.0987472  0.09889267 0.09807997
 0.09877784 0.09831689 0.09804576 0.09873079 0.09841246 0.09874908
 0.09895836 0.09829814 0.0990978  0.09926187 0.09994334 0.10087506]
