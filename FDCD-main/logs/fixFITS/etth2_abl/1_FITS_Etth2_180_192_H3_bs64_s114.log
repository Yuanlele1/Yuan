Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_192_FITS_ETTh2_ftM_sl180_ll48_pl192_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8269
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=34, out_features=70, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2132480.0
params:  2450.0
Trainable parameters:  2450
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.7358965873718262
Epoch: 1, Steps: 64 | Train Loss: 0.6952651 Vali Loss: 0.3411551 Test Loss: 0.4693364
Validation loss decreased (inf --> 0.341155).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.961085319519043
Epoch: 2, Steps: 64 | Train Loss: 0.5989336 Vali Loss: 0.3103121 Test Loss: 0.4300639
Validation loss decreased (0.341155 --> 0.310312).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0713000297546387
Epoch: 3, Steps: 64 | Train Loss: 0.5658321 Vali Loss: 0.2974187 Test Loss: 0.4141764
Validation loss decreased (0.310312 --> 0.297419).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.5716543197631836
Epoch: 4, Steps: 64 | Train Loss: 0.5519039 Vali Loss: 0.2906393 Test Loss: 0.4064673
Validation loss decreased (0.297419 --> 0.290639).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.6560964584350586
Epoch: 5, Steps: 64 | Train Loss: 0.5428237 Vali Loss: 0.2869327 Test Loss: 0.4019721
Validation loss decreased (0.290639 --> 0.286933).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.6812751293182373
Epoch: 6, Steps: 64 | Train Loss: 0.5381796 Vali Loss: 0.2845646 Test Loss: 0.3990276
Validation loss decreased (0.286933 --> 0.284565).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.1788806915283203
Epoch: 7, Steps: 64 | Train Loss: 0.5332148 Vali Loss: 0.2826734 Test Loss: 0.3969949
Validation loss decreased (0.284565 --> 0.282673).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.8397340774536133
Epoch: 8, Steps: 64 | Train Loss: 0.5325534 Vali Loss: 0.2814077 Test Loss: 0.3952591
Validation loss decreased (0.282673 --> 0.281408).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.553985834121704
Epoch: 9, Steps: 64 | Train Loss: 0.5318123 Vali Loss: 0.2804146 Test Loss: 0.3938290
Validation loss decreased (0.281408 --> 0.280415).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.5503146648406982
Epoch: 10, Steps: 64 | Train Loss: 0.5302504 Vali Loss: 0.2796220 Test Loss: 0.3928253
Validation loss decreased (0.280415 --> 0.279622).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.792572021484375
Epoch: 11, Steps: 64 | Train Loss: 0.5293180 Vali Loss: 0.2789633 Test Loss: 0.3917748
Validation loss decreased (0.279622 --> 0.278963).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.7747979164123535
Epoch: 12, Steps: 64 | Train Loss: 0.5256315 Vali Loss: 0.2781538 Test Loss: 0.3910342
Validation loss decreased (0.278963 --> 0.278154).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.6576335430145264
Epoch: 13, Steps: 64 | Train Loss: 0.5256364 Vali Loss: 0.2777781 Test Loss: 0.3901928
Validation loss decreased (0.278154 --> 0.277778).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.5539321899414062
Epoch: 14, Steps: 64 | Train Loss: 0.5229556 Vali Loss: 0.2773146 Test Loss: 0.3895252
Validation loss decreased (0.277778 --> 0.277315).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.175734519958496
Epoch: 15, Steps: 64 | Train Loss: 0.5243177 Vali Loss: 0.2769012 Test Loss: 0.3889233
Validation loss decreased (0.277315 --> 0.276901).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.9070343971252441
Epoch: 16, Steps: 64 | Train Loss: 0.5232998 Vali Loss: 0.2766218 Test Loss: 0.3883581
Validation loss decreased (0.276901 --> 0.276622).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.167381525039673
Epoch: 17, Steps: 64 | Train Loss: 0.5233878 Vali Loss: 0.2763804 Test Loss: 0.3878957
Validation loss decreased (0.276622 --> 0.276380).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.6719114780426025
Epoch: 18, Steps: 64 | Train Loss: 0.5229020 Vali Loss: 0.2760645 Test Loss: 0.3874900
Validation loss decreased (0.276380 --> 0.276065).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.8158354759216309
Epoch: 19, Steps: 64 | Train Loss: 0.5213077 Vali Loss: 0.2759094 Test Loss: 0.3870858
Validation loss decreased (0.276065 --> 0.275909).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.7180922031402588
Epoch: 20, Steps: 64 | Train Loss: 0.5234507 Vali Loss: 0.2755045 Test Loss: 0.3867992
Validation loss decreased (0.275909 --> 0.275505).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.9468483924865723
Epoch: 21, Steps: 64 | Train Loss: 0.5195734 Vali Loss: 0.2755663 Test Loss: 0.3864113
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.5316288471221924
Epoch: 22, Steps: 64 | Train Loss: 0.5219334 Vali Loss: 0.2753417 Test Loss: 0.3861533
Validation loss decreased (0.275505 --> 0.275342).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6605041027069092
Epoch: 23, Steps: 64 | Train Loss: 0.5213865 Vali Loss: 0.2751330 Test Loss: 0.3859589
Validation loss decreased (0.275342 --> 0.275133).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6210992336273193
Epoch: 24, Steps: 64 | Train Loss: 0.5212072 Vali Loss: 0.2750122 Test Loss: 0.3856588
Validation loss decreased (0.275133 --> 0.275012).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9598844051361084
Epoch: 25, Steps: 64 | Train Loss: 0.5209303 Vali Loss: 0.2749342 Test Loss: 0.3853856
Validation loss decreased (0.275012 --> 0.274934).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.5373988151550293
Epoch: 26, Steps: 64 | Train Loss: 0.5172296 Vali Loss: 0.2747591 Test Loss: 0.3851866
Validation loss decreased (0.274934 --> 0.274759).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.2733848094940186
Epoch: 27, Steps: 64 | Train Loss: 0.5210861 Vali Loss: 0.2747008 Test Loss: 0.3849826
Validation loss decreased (0.274759 --> 0.274701).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.7721257209777832
Epoch: 28, Steps: 64 | Train Loss: 0.5201354 Vali Loss: 0.2744865 Test Loss: 0.3848444
Validation loss decreased (0.274701 --> 0.274487).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.7634422779083252
Epoch: 29, Steps: 64 | Train Loss: 0.5176962 Vali Loss: 0.2744467 Test Loss: 0.3846977
Validation loss decreased (0.274487 --> 0.274447).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.892040729522705
Epoch: 30, Steps: 64 | Train Loss: 0.5198197 Vali Loss: 0.2743500 Test Loss: 0.3844982
Validation loss decreased (0.274447 --> 0.274350).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.7102868556976318
Epoch: 31, Steps: 64 | Train Loss: 0.5189478 Vali Loss: 0.2740174 Test Loss: 0.3843354
Validation loss decreased (0.274350 --> 0.274017).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.348041534423828
Epoch: 32, Steps: 64 | Train Loss: 0.5185602 Vali Loss: 0.2742539 Test Loss: 0.3841771
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.613952398300171
Epoch: 33, Steps: 64 | Train Loss: 0.5143822 Vali Loss: 0.2741566 Test Loss: 0.3840789
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.100554943084717
Epoch: 34, Steps: 64 | Train Loss: 0.5199001 Vali Loss: 0.2738602 Test Loss: 0.3839803
Validation loss decreased (0.274017 --> 0.273860).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.8700952529907227
Epoch: 35, Steps: 64 | Train Loss: 0.5189599 Vali Loss: 0.2740214 Test Loss: 0.3838802
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.6759729385375977
Epoch: 36, Steps: 64 | Train Loss: 0.5175396 Vali Loss: 0.2739952 Test Loss: 0.3837602
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.603339433670044
Epoch: 37, Steps: 64 | Train Loss: 0.5193432 Vali Loss: 0.2739545 Test Loss: 0.3836667
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.0493078231811523
Epoch: 38, Steps: 64 | Train Loss: 0.5192903 Vali Loss: 0.2738698 Test Loss: 0.3835782
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.4293041229248047
Epoch: 39, Steps: 64 | Train Loss: 0.5180304 Vali Loss: 0.2738333 Test Loss: 0.3834889
Validation loss decreased (0.273860 --> 0.273833).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.7282648086547852
Epoch: 40, Steps: 64 | Train Loss: 0.5178806 Vali Loss: 0.2737171 Test Loss: 0.3834279
Validation loss decreased (0.273833 --> 0.273717).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.7027759552001953
Epoch: 41, Steps: 64 | Train Loss: 0.5181871 Vali Loss: 0.2737816 Test Loss: 0.3833485
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.419879913330078
Epoch: 42, Steps: 64 | Train Loss: 0.5179335 Vali Loss: 0.2736836 Test Loss: 0.3832984
Validation loss decreased (0.273717 --> 0.273684).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.9539141654968262
Epoch: 43, Steps: 64 | Train Loss: 0.5165075 Vali Loss: 0.2737229 Test Loss: 0.3832120
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.1198043823242188
Epoch: 44, Steps: 64 | Train Loss: 0.5185495 Vali Loss: 0.2736691 Test Loss: 0.3831325
Validation loss decreased (0.273684 --> 0.273669).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9406137466430664
Epoch: 45, Steps: 64 | Train Loss: 0.5175073 Vali Loss: 0.2736077 Test Loss: 0.3830909
Validation loss decreased (0.273669 --> 0.273608).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8633720874786377
Epoch: 46, Steps: 64 | Train Loss: 0.5185621 Vali Loss: 0.2735624 Test Loss: 0.3830462
Validation loss decreased (0.273608 --> 0.273562).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.913090705871582
Epoch: 47, Steps: 64 | Train Loss: 0.5181611 Vali Loss: 0.2736145 Test Loss: 0.3829836
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.0254411697387695
Epoch: 48, Steps: 64 | Train Loss: 0.5161275 Vali Loss: 0.2734585 Test Loss: 0.3829432
Validation loss decreased (0.273562 --> 0.273459).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9054102897644043
Epoch: 49, Steps: 64 | Train Loss: 0.5169950 Vali Loss: 0.2732022 Test Loss: 0.3829216
Validation loss decreased (0.273459 --> 0.273202).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.8198821544647217
Epoch: 50, Steps: 64 | Train Loss: 0.5189545 Vali Loss: 0.2734319 Test Loss: 0.3828696
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.4640564918518066
Epoch: 51, Steps: 64 | Train Loss: 0.5185868 Vali Loss: 0.2734395 Test Loss: 0.3828260
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.5422286987304688
Epoch: 52, Steps: 64 | Train Loss: 0.5195159 Vali Loss: 0.2734351 Test Loss: 0.3827777
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.9702849388122559
Epoch: 53, Steps: 64 | Train Loss: 0.5164652 Vali Loss: 0.2733979 Test Loss: 0.3827588
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.7844796180725098
Epoch: 54, Steps: 64 | Train Loss: 0.5185589 Vali Loss: 0.2730147 Test Loss: 0.3827275
Validation loss decreased (0.273202 --> 0.273015).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.602867841720581
Epoch: 55, Steps: 64 | Train Loss: 0.5175224 Vali Loss: 0.2733900 Test Loss: 0.3826884
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.7741360664367676
Epoch: 56, Steps: 64 | Train Loss: 0.5174279 Vali Loss: 0.2733949 Test Loss: 0.3826633
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.4290504455566406
Epoch: 57, Steps: 64 | Train Loss: 0.5183090 Vali Loss: 0.2733793 Test Loss: 0.3826281
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.879394769668579
Epoch: 58, Steps: 64 | Train Loss: 0.5186176 Vali Loss: 0.2732955 Test Loss: 0.3826017
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.6657896041870117
Epoch: 59, Steps: 64 | Train Loss: 0.5178316 Vali Loss: 0.2729998 Test Loss: 0.3825760
Validation loss decreased (0.273015 --> 0.273000).  Saving model ...
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.7869126796722412
Epoch: 60, Steps: 64 | Train Loss: 0.5179346 Vali Loss: 0.2731877 Test Loss: 0.3825491
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.051239252090454
Epoch: 61, Steps: 64 | Train Loss: 0.5169215 Vali Loss: 0.2732754 Test Loss: 0.3825291
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.9587435722351074
Epoch: 62, Steps: 64 | Train Loss: 0.5175595 Vali Loss: 0.2732445 Test Loss: 0.3825134
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.6451003551483154
Epoch: 63, Steps: 64 | Train Loss: 0.5156332 Vali Loss: 0.2733044 Test Loss: 0.3824846
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.7819912433624268
Epoch: 64, Steps: 64 | Train Loss: 0.5193248 Vali Loss: 0.2732823 Test Loss: 0.3824606
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.1624255180358887
Epoch: 65, Steps: 64 | Train Loss: 0.5151218 Vali Loss: 0.2732667 Test Loss: 0.3824352
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.994370937347412
Epoch: 66, Steps: 64 | Train Loss: 0.5172213 Vali Loss: 0.2732857 Test Loss: 0.3824121
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.8091835975646973
Epoch: 67, Steps: 64 | Train Loss: 0.5169978 Vali Loss: 0.2732317 Test Loss: 0.3824022
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.7275941371917725
Epoch: 68, Steps: 64 | Train Loss: 0.5173385 Vali Loss: 0.2732583 Test Loss: 0.3823850
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.295685052871704
Epoch: 69, Steps: 64 | Train Loss: 0.5142202 Vali Loss: 0.2732004 Test Loss: 0.3823721
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9420263767242432
Epoch: 70, Steps: 64 | Train Loss: 0.5184634 Vali Loss: 0.2731959 Test Loss: 0.3823624
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.8397982120513916
Epoch: 71, Steps: 64 | Train Loss: 0.5168008 Vali Loss: 0.2731768 Test Loss: 0.3823434
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.703404426574707
Epoch: 72, Steps: 64 | Train Loss: 0.5163431 Vali Loss: 0.2732253 Test Loss: 0.3823261
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.9978792667388916
Epoch: 73, Steps: 64 | Train Loss: 0.5178489 Vali Loss: 0.2732100 Test Loss: 0.3823116
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.585953950881958
Epoch: 74, Steps: 64 | Train Loss: 0.5174634 Vali Loss: 0.2731678 Test Loss: 0.3823054
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.6141130924224854
Epoch: 75, Steps: 64 | Train Loss: 0.5189985 Vali Loss: 0.2731592 Test Loss: 0.3822908
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.22249174118042
Epoch: 76, Steps: 64 | Train Loss: 0.5183765 Vali Loss: 0.2732269 Test Loss: 0.3822844
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.4026968479156494
Epoch: 77, Steps: 64 | Train Loss: 0.5178636 Vali Loss: 0.2732124 Test Loss: 0.3822724
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.2887370586395264
Epoch: 78, Steps: 64 | Train Loss: 0.5183710 Vali Loss: 0.2731631 Test Loss: 0.3822644
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.162717819213867
Epoch: 79, Steps: 64 | Train Loss: 0.5174693 Vali Loss: 0.2731774 Test Loss: 0.3822565
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_192_FITS_ETTh2_ftM_sl180_ll48_pl192_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.36078953742980957, mae:0.38782623410224915, rse:0.4816901981830597, corr:[0.26517344 0.27009466 0.26920286 0.2668301  0.26619384 0.26625082
 0.26558638 0.2641122  0.262809   0.2617603  0.26052946 0.2588545
 0.25739002 0.25630435 0.25553277 0.2548311  0.25414062 0.25340828
 0.25249588 0.25145015 0.25015974 0.24873829 0.24724415 0.24529481
 0.24276464 0.24004665 0.23768707 0.23619302 0.23495851 0.23334803
 0.23110433 0.22853915 0.2266808  0.22551055 0.2245233  0.22279511
 0.22050028 0.21859531 0.21791166 0.21767272 0.21714091 0.2159089
 0.21453907 0.21366164 0.21316369 0.21211168 0.20998563 0.20719756
 0.2044087  0.20241684 0.20058335 0.19850083 0.19613892 0.19416285
 0.1924983  0.19081433 0.18910533 0.1872824  0.18606038 0.18528293
 0.18493879 0.18407263 0.1832039  0.1824909  0.1823091  0.18224522
 0.18169212 0.18061039 0.17976584 0.17943636 0.17883329 0.1775294
 0.17516783 0.1730582  0.17183323 0.17128274 0.17057577 0.16950643
 0.16820107 0.16723192 0.16724144 0.16733395 0.16706762 0.16633731
 0.16587064 0.16580126 0.1661134  0.16607915 0.16547367 0.16455592
 0.16409773 0.16409925 0.16433375 0.16410612 0.16315913 0.16208757
 0.16084823 0.15983862 0.15854807 0.15720731 0.15590815 0.15506828
 0.15478107 0.15452854 0.15422714 0.15372397 0.15362059 0.15377517
 0.1540471  0.15366727 0.15288529 0.15212066 0.15180168 0.15165895
 0.15147212 0.1509433  0.15030923 0.14981107 0.14906584 0.14770174
 0.14562364 0.14361101 0.14213115 0.14126253 0.140246   0.13882685
 0.13741486 0.13636717 0.13582097 0.13543692 0.13503914 0.13433877
 0.13382435 0.1333113  0.13310075 0.1328801  0.13248    0.13174564
 0.13113576 0.13097742 0.13105589 0.13091148 0.12988041 0.12827477
 0.12601343 0.12420103 0.12243879 0.12104011 0.12006357 0.11952016
 0.11929962 0.11885013 0.11847787 0.11825688 0.11857766 0.11893117
 0.11970423 0.11986053 0.11959144 0.11930616 0.11940518 0.11944459
 0.11919755 0.11899895 0.11913103 0.11957795 0.11958587 0.11862806
 0.11646739 0.11495421 0.11385436 0.11317234 0.11219063 0.11060586
 0.10921919 0.10843648 0.10849667 0.10858044 0.10878167 0.10847674
 0.10826266 0.10796142 0.10800311 0.10790852 0.10720591 0.1059256
 0.10555688 0.10602043 0.10693207 0.10731582 0.10602953 0.10291079]
