Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=58, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=58, out_features=112, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5820416.0
params:  6608.0
Trainable parameters:  6608
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.9479994773864746
Epoch: 1, Steps: 62 | Train Loss: 0.6799610 Vali Loss: 0.5174893 Test Loss: 0.4534515
Validation loss decreased (inf --> 0.517489).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.967989683151245
Epoch: 2, Steps: 62 | Train Loss: 0.5606857 Vali Loss: 0.4728334 Test Loss: 0.4204542
Validation loss decreased (0.517489 --> 0.472833).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.4405386447906494
Epoch: 3, Steps: 62 | Train Loss: 0.4934529 Vali Loss: 0.4496458 Test Loss: 0.4039345
Validation loss decreased (0.472833 --> 0.449646).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.3132433891296387
Epoch: 4, Steps: 62 | Train Loss: 0.4530145 Vali Loss: 0.4343747 Test Loss: 0.3956952
Validation loss decreased (0.449646 --> 0.434375).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.324876070022583
Epoch: 5, Steps: 62 | Train Loss: 0.4267042 Vali Loss: 0.4230520 Test Loss: 0.3914983
Validation loss decreased (0.434375 --> 0.423052).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.1715736389160156
Epoch: 6, Steps: 62 | Train Loss: 0.4087906 Vali Loss: 0.4213251 Test Loss: 0.3891359
Validation loss decreased (0.423052 --> 0.421325).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.9045588970184326
Epoch: 7, Steps: 62 | Train Loss: 0.3959770 Vali Loss: 0.4160840 Test Loss: 0.3876818
Validation loss decreased (0.421325 --> 0.416084).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.50349497795105
Epoch: 8, Steps: 62 | Train Loss: 0.3857449 Vali Loss: 0.4125690 Test Loss: 0.3865022
Validation loss decreased (0.416084 --> 0.412569).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.709198951721191
Epoch: 9, Steps: 62 | Train Loss: 0.3773847 Vali Loss: 0.4088361 Test Loss: 0.3855490
Validation loss decreased (0.412569 --> 0.408836).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 6.154759645462036
Epoch: 10, Steps: 62 | Train Loss: 0.3707057 Vali Loss: 0.4063582 Test Loss: 0.3846452
Validation loss decreased (0.408836 --> 0.406358).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 6.334253787994385
Epoch: 11, Steps: 62 | Train Loss: 0.3648688 Vali Loss: 0.4012206 Test Loss: 0.3838654
Validation loss decreased (0.406358 --> 0.401221).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 5.768047571182251
Epoch: 12, Steps: 62 | Train Loss: 0.3599545 Vali Loss: 0.4007186 Test Loss: 0.3829426
Validation loss decreased (0.401221 --> 0.400719).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.0978844165802
Epoch: 13, Steps: 62 | Train Loss: 0.3560166 Vali Loss: 0.4010918 Test Loss: 0.3821405
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.045533657073975
Epoch: 14, Steps: 62 | Train Loss: 0.3521879 Vali Loss: 0.3968542 Test Loss: 0.3814142
Validation loss decreased (0.400719 --> 0.396854).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.146912097930908
Epoch: 15, Steps: 62 | Train Loss: 0.3487217 Vali Loss: 0.3965707 Test Loss: 0.3806373
Validation loss decreased (0.396854 --> 0.396571).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.0329785346984863
Epoch: 16, Steps: 62 | Train Loss: 0.3462184 Vali Loss: 0.3948028 Test Loss: 0.3799714
Validation loss decreased (0.396571 --> 0.394803).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.479119062423706
Epoch: 17, Steps: 62 | Train Loss: 0.3432258 Vali Loss: 0.3938995 Test Loss: 0.3792717
Validation loss decreased (0.394803 --> 0.393900).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.6691579818725586
Epoch: 18, Steps: 62 | Train Loss: 0.3413992 Vali Loss: 0.3950840 Test Loss: 0.3787028
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.226297616958618
Epoch: 19, Steps: 62 | Train Loss: 0.3392657 Vali Loss: 0.3945024 Test Loss: 0.3780770
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8375074863433838
Epoch: 20, Steps: 62 | Train Loss: 0.3376318 Vali Loss: 0.3942582 Test Loss: 0.3775122
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.016545057296753
Epoch: 21, Steps: 62 | Train Loss: 0.3359770 Vali Loss: 0.3933820 Test Loss: 0.3770066
Validation loss decreased (0.393900 --> 0.393382).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.884732961654663
Epoch: 22, Steps: 62 | Train Loss: 0.3344404 Vali Loss: 0.3892858 Test Loss: 0.3765119
Validation loss decreased (0.393382 --> 0.389286).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.4211037158966064
Epoch: 23, Steps: 62 | Train Loss: 0.3332694 Vali Loss: 0.3922921 Test Loss: 0.3760995
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.245431423187256
Epoch: 24, Steps: 62 | Train Loss: 0.3318305 Vali Loss: 0.3901716 Test Loss: 0.3756696
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.293606758117676
Epoch: 25, Steps: 62 | Train Loss: 0.3307867 Vali Loss: 0.3905009 Test Loss: 0.3752460
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.719064712524414
Epoch: 26, Steps: 62 | Train Loss: 0.3295322 Vali Loss: 0.3894972 Test Loss: 0.3750415
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.2552247047424316
Epoch: 27, Steps: 62 | Train Loss: 0.3290776 Vali Loss: 0.3880131 Test Loss: 0.3746427
Validation loss decreased (0.389286 --> 0.388013).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.921841859817505
Epoch: 28, Steps: 62 | Train Loss: 0.3282072 Vali Loss: 0.3872371 Test Loss: 0.3742952
Validation loss decreased (0.388013 --> 0.387237).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.7900660037994385
Epoch: 29, Steps: 62 | Train Loss: 0.3270773 Vali Loss: 0.3889363 Test Loss: 0.3740736
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.310199737548828
Epoch: 30, Steps: 62 | Train Loss: 0.3265616 Vali Loss: 0.3894562 Test Loss: 0.3738101
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.325556516647339
Epoch: 31, Steps: 62 | Train Loss: 0.3262348 Vali Loss: 0.3868256 Test Loss: 0.3735671
Validation loss decreased (0.387237 --> 0.386826).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.7091991901397705
Epoch: 32, Steps: 62 | Train Loss: 0.3252578 Vali Loss: 0.3853567 Test Loss: 0.3733458
Validation loss decreased (0.386826 --> 0.385357).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.599421501159668
Epoch: 33, Steps: 62 | Train Loss: 0.3250985 Vali Loss: 0.3853869 Test Loss: 0.3730958
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.021697759628296
Epoch: 34, Steps: 62 | Train Loss: 0.3242226 Vali Loss: 0.3886335 Test Loss: 0.3728622
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.239565849304199
Epoch: 35, Steps: 62 | Train Loss: 0.3241792 Vali Loss: 0.3881271 Test Loss: 0.3727102
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.4174277782440186
Epoch: 36, Steps: 62 | Train Loss: 0.3234747 Vali Loss: 0.3845805 Test Loss: 0.3725054
Validation loss decreased (0.385357 --> 0.384581).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.9713194370269775
Epoch: 37, Steps: 62 | Train Loss: 0.3233021 Vali Loss: 0.3838381 Test Loss: 0.3723604
Validation loss decreased (0.384581 --> 0.383838).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9572100639343262
Epoch: 38, Steps: 62 | Train Loss: 0.3229771 Vali Loss: 0.3876615 Test Loss: 0.3722413
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.1806955337524414
Epoch: 39, Steps: 62 | Train Loss: 0.3226827 Vali Loss: 0.3844982 Test Loss: 0.3720704
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.437222480773926
Epoch: 40, Steps: 62 | Train Loss: 0.3221286 Vali Loss: 0.3854582 Test Loss: 0.3719126
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.9459080696105957
Epoch: 41, Steps: 62 | Train Loss: 0.3220586 Vali Loss: 0.3876508 Test Loss: 0.3718093
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.9797396659851074
Epoch: 42, Steps: 62 | Train Loss: 0.3218194 Vali Loss: 0.3892440 Test Loss: 0.3717387
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.8974814414978027
Epoch: 43, Steps: 62 | Train Loss: 0.3215617 Vali Loss: 0.3854983 Test Loss: 0.3715774
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.4604618549346924
Epoch: 44, Steps: 62 | Train Loss: 0.3212934 Vali Loss: 0.3816562 Test Loss: 0.3714728
Validation loss decreased (0.383838 --> 0.381656).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.2870187759399414
Epoch: 45, Steps: 62 | Train Loss: 0.3207171 Vali Loss: 0.3843138 Test Loss: 0.3713790
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.396695613861084
Epoch: 46, Steps: 62 | Train Loss: 0.3207375 Vali Loss: 0.3853803 Test Loss: 0.3712898
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.8996472358703613
Epoch: 47, Steps: 62 | Train Loss: 0.3205593 Vali Loss: 0.3851919 Test Loss: 0.3712288
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.651437520980835
Epoch: 48, Steps: 62 | Train Loss: 0.3204728 Vali Loss: 0.3847126 Test Loss: 0.3711206
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.40866756439209
Epoch: 49, Steps: 62 | Train Loss: 0.3202838 Vali Loss: 0.3838664 Test Loss: 0.3710455
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.710493564605713
Epoch: 50, Steps: 62 | Train Loss: 0.3198567 Vali Loss: 0.3842169 Test Loss: 0.3709883
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.393718957901001
Epoch: 51, Steps: 62 | Train Loss: 0.3199076 Vali Loss: 0.3831214 Test Loss: 0.3709032
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.9079318046569824
Epoch: 52, Steps: 62 | Train Loss: 0.3197912 Vali Loss: 0.3838767 Test Loss: 0.3708435
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.7223048210144043
Epoch: 53, Steps: 62 | Train Loss: 0.3193441 Vali Loss: 0.3831137 Test Loss: 0.3707717
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.6553990840911865
Epoch: 54, Steps: 62 | Train Loss: 0.3195224 Vali Loss: 0.3851112 Test Loss: 0.3707196
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.232679605484009
Epoch: 55, Steps: 62 | Train Loss: 0.3190938 Vali Loss: 0.3843843 Test Loss: 0.3706845
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.827324628829956
Epoch: 56, Steps: 62 | Train Loss: 0.3189511 Vali Loss: 0.3856808 Test Loss: 0.3706144
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.694676160812378
Epoch: 57, Steps: 62 | Train Loss: 0.3191911 Vali Loss: 0.3838330 Test Loss: 0.3705763
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.1571271419525146
Epoch: 58, Steps: 62 | Train Loss: 0.3191554 Vali Loss: 0.3833185 Test Loss: 0.3705224
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.536311626434326
Epoch: 59, Steps: 62 | Train Loss: 0.3186050 Vali Loss: 0.3845834 Test Loss: 0.3704928
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.7077009677886963
Epoch: 60, Steps: 62 | Train Loss: 0.3187655 Vali Loss: 0.3834611 Test Loss: 0.3704274
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.756103754043579
Epoch: 61, Steps: 62 | Train Loss: 0.3185601 Vali Loss: 0.3853058 Test Loss: 0.3703955
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.8194262981414795
Epoch: 62, Steps: 62 | Train Loss: 0.3184733 Vali Loss: 0.3831951 Test Loss: 0.3703533
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.4751698970794678
Epoch: 63, Steps: 62 | Train Loss: 0.3185202 Vali Loss: 0.3845414 Test Loss: 0.3703257
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 4.382540225982666
Epoch: 64, Steps: 62 | Train Loss: 0.3187048 Vali Loss: 0.3832864 Test Loss: 0.3702975
EarlyStopping counter: 20 out of 20
Early stopping
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=58, out_features=112, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  5820416.0
params:  6608.0
Trainable parameters:  6608
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.0298750400543213
Epoch: 1, Steps: 62 | Train Loss: 0.6181828 Vali Loss: 0.3821931 Test Loss: 0.3668505
Validation loss decreased (inf --> 0.382193).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.4709343910217285
Epoch: 2, Steps: 62 | Train Loss: 0.6136947 Vali Loss: 0.3805111 Test Loss: 0.3653423
Validation loss decreased (0.382193 --> 0.380511).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.8092238903045654
Epoch: 3, Steps: 62 | Train Loss: 0.6123646 Vali Loss: 0.3804210 Test Loss: 0.3645912
Validation loss decreased (0.380511 --> 0.380421).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.8231773376464844
Epoch: 4, Steps: 62 | Train Loss: 0.6104832 Vali Loss: 0.3789242 Test Loss: 0.3640631
Validation loss decreased (0.380421 --> 0.378924).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.160362482070923
Epoch: 5, Steps: 62 | Train Loss: 0.6100295 Vali Loss: 0.3764429 Test Loss: 0.3638088
Validation loss decreased (0.378924 --> 0.376443).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.9698336124420166
Epoch: 6, Steps: 62 | Train Loss: 0.6089160 Vali Loss: 0.3765396 Test Loss: 0.3639606
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.332186698913574
Epoch: 7, Steps: 62 | Train Loss: 0.6089207 Vali Loss: 0.3742462 Test Loss: 0.3637138
Validation loss decreased (0.376443 --> 0.374246).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.4376049041748047
Epoch: 8, Steps: 62 | Train Loss: 0.6087220 Vali Loss: 0.3759249 Test Loss: 0.3638854
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.9873759746551514
Epoch: 9, Steps: 62 | Train Loss: 0.6081030 Vali Loss: 0.3744842 Test Loss: 0.3635810
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.112582206726074
Epoch: 10, Steps: 62 | Train Loss: 0.6086762 Vali Loss: 0.3756893 Test Loss: 0.3636781
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.345630645751953
Epoch: 11, Steps: 62 | Train Loss: 0.6083573 Vali Loss: 0.3745456 Test Loss: 0.3636083
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.4181792736053467
Epoch: 12, Steps: 62 | Train Loss: 0.6078167 Vali Loss: 0.3747356 Test Loss: 0.3636567
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.1559324264526367
Epoch: 13, Steps: 62 | Train Loss: 0.6080904 Vali Loss: 0.3767122 Test Loss: 0.3636170
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.649384021759033
Epoch: 14, Steps: 62 | Train Loss: 0.6081088 Vali Loss: 0.3747961 Test Loss: 0.3635952
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.192596435546875
Epoch: 15, Steps: 62 | Train Loss: 0.6070437 Vali Loss: 0.3749974 Test Loss: 0.3636448
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.0003225803375244
Epoch: 16, Steps: 62 | Train Loss: 0.6079384 Vali Loss: 0.3709157 Test Loss: 0.3636891
Validation loss decreased (0.374246 --> 0.370916).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.620786666870117
Epoch: 17, Steps: 62 | Train Loss: 0.6068883 Vali Loss: 0.3738396 Test Loss: 0.3637189
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.850243091583252
Epoch: 18, Steps: 62 | Train Loss: 0.6077083 Vali Loss: 0.3738859 Test Loss: 0.3636182
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.935153007507324
Epoch: 19, Steps: 62 | Train Loss: 0.6068617 Vali Loss: 0.3750271 Test Loss: 0.3635559
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.090428590774536
Epoch: 20, Steps: 62 | Train Loss: 0.6064138 Vali Loss: 0.3725582 Test Loss: 0.3636313
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.8259012699127197
Epoch: 21, Steps: 62 | Train Loss: 0.6074837 Vali Loss: 0.3735863 Test Loss: 0.3635678
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.1867740154266357
Epoch: 22, Steps: 62 | Train Loss: 0.6072830 Vali Loss: 0.3731037 Test Loss: 0.3636257
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 4.339390277862549
Epoch: 23, Steps: 62 | Train Loss: 0.6070111 Vali Loss: 0.3734813 Test Loss: 0.3636283
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.7445640563964844
Epoch: 24, Steps: 62 | Train Loss: 0.6071414 Vali Loss: 0.3713637 Test Loss: 0.3636124
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8141677379608154
Epoch: 25, Steps: 62 | Train Loss: 0.6072773 Vali Loss: 0.3727230 Test Loss: 0.3635995
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.504138708114624
Epoch: 26, Steps: 62 | Train Loss: 0.6066170 Vali Loss: 0.3726578 Test Loss: 0.3636331
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.320706844329834
Epoch: 27, Steps: 62 | Train Loss: 0.6063748 Vali Loss: 0.3737383 Test Loss: 0.3635735
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.9038493633270264
Epoch: 28, Steps: 62 | Train Loss: 0.6069107 Vali Loss: 0.3718541 Test Loss: 0.3636758
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.5446016788482666
Epoch: 29, Steps: 62 | Train Loss: 0.6067977 Vali Loss: 0.3724165 Test Loss: 0.3635823
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.4436018466949463
Epoch: 30, Steps: 62 | Train Loss: 0.6068766 Vali Loss: 0.3712712 Test Loss: 0.3637047
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.6340856552124023
Epoch: 31, Steps: 62 | Train Loss: 0.6065994 Vali Loss: 0.3735001 Test Loss: 0.3636411
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.395857095718384
Epoch: 32, Steps: 62 | Train Loss: 0.6063647 Vali Loss: 0.3726852 Test Loss: 0.3636182
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.7172114849090576
Epoch: 33, Steps: 62 | Train Loss: 0.6060516 Vali Loss: 0.3744324 Test Loss: 0.3635808
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.956550359725952
Epoch: 34, Steps: 62 | Train Loss: 0.6064474 Vali Loss: 0.3731878 Test Loss: 0.3636625
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.786100625991821
Epoch: 35, Steps: 62 | Train Loss: 0.6069635 Vali Loss: 0.3750590 Test Loss: 0.3636134
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.1713693141937256
Epoch: 36, Steps: 62 | Train Loss: 0.6068174 Vali Loss: 0.3735813 Test Loss: 0.3636736
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.359291136264801, mae:0.39716529846191406, rse:0.4792507290840149, corr:[0.26316163 0.26592717 0.26494658 0.2623261  0.26066515 0.26010254
 0.25983536 0.25885713 0.25733784 0.25562266 0.25420892 0.2529144
 0.2518771  0.25083154 0.24988924 0.24916917 0.24847214 0.24777491
 0.24697477 0.24607523 0.24510439 0.24389663 0.24229726 0.24034622
 0.23830143 0.23666647 0.23561263 0.23479812 0.23389035 0.23277469
 0.23150401 0.23022427 0.22900602 0.22801423 0.2270693  0.22599337
 0.22468922 0.22345492 0.22259243 0.22190674 0.22112982 0.22000319
 0.21852884 0.21710582 0.21601431 0.21513961 0.21402216 0.2122091
 0.20986694 0.20763473 0.20591977 0.20463008 0.20337786 0.20181203
 0.19984797 0.19770506 0.1960361  0.1947343  0.19389097 0.19335586
 0.19292077 0.19246444 0.19215372 0.19195516 0.19162808 0.19127756
 0.19061    0.18986125 0.18910566 0.18857296 0.18802251 0.18719742
 0.18599468 0.18467164 0.18363184 0.18303712 0.18291011 0.1827619
 0.18224297 0.18133596 0.18065408 0.18015623 0.17994839 0.17978539
 0.17965992 0.17937464 0.179086   0.17881536 0.17847629 0.17808282
 0.17755495 0.17687811 0.17659299 0.17668812 0.17702329 0.177207
 0.1769636  0.17604643 0.17493919 0.17393495 0.17333439 0.17314488
 0.17322952 0.17294267 0.17260285 0.17236698 0.17246819 0.17292257
 0.17306069 0.17265084 0.17168269 0.17081976 0.17015661 0.16995174
 0.16992831 0.16955158 0.16882953 0.16756336 0.1662363  0.16511334
 0.16431203 0.16350155 0.16265939 0.16185313 0.16105975 0.1605111
 0.16007331 0.15967849 0.15904948 0.15812646 0.1573024  0.15678246
 0.15658087 0.15638515 0.15599157 0.1554125  0.15477635 0.1543567
 0.1539811  0.1538133  0.15366569 0.15311381 0.1521614  0.15104935
 0.14984939 0.14851777 0.1470031  0.145693   0.14487241 0.14457092
 0.14474936 0.14483935 0.14464006 0.14403412 0.14316808 0.14247598
 0.14228705 0.14259453 0.14270166 0.14249831 0.1419176  0.14130019
 0.14103423 0.1410861  0.14117421 0.14084564 0.14010666 0.1390346
 0.13821836 0.13764742 0.13693891 0.13594073 0.13458624 0.13313052
 0.13209224 0.13154614 0.13144694 0.13122776 0.13073276 0.12983982
 0.12910865 0.12873778 0.12864287 0.12869391 0.12849891 0.12812239
 0.12809354 0.12851074 0.12914398 0.12972419 0.12978585 0.1291166
 0.12828647 0.12765235 0.12745093 0.12754136 0.12740703 0.12683718
 0.12618504 0.12572668 0.12563094 0.12579134 0.12601091 0.1257482
 0.12526084 0.12482727 0.12466165 0.12499067 0.12528154 0.12531586
 0.12511835 0.12507974 0.12533192 0.12546065 0.12536752 0.12461448
 0.12358519 0.12236531 0.12152173 0.1212815  0.12117073 0.12118827
 0.121002   0.120902   0.12073697 0.12035832 0.1199301  0.11922473
 0.11835264 0.1174853  0.11690985 0.11702474 0.11747174 0.11802098
 0.11834068 0.11869814 0.11898154 0.11906659 0.11898488 0.11859963
 0.11771209 0.11663408 0.11580147 0.11530931 0.11524439 0.11492041
 0.11444809 0.11405888 0.11377731 0.11340779 0.1133155  0.11350402
 0.11375541 0.11414912 0.11448756 0.11530084 0.11624413 0.11712983
 0.11743842 0.1174341  0.11774817 0.11826736 0.11929282 0.12010803
 0.12041578 0.11987654 0.11901709 0.11849609 0.11846357 0.11894298
 0.11935323 0.11952011 0.11918712 0.1185305  0.11852052 0.11855679
 0.1188175  0.11902827 0.11890299 0.11868033 0.11875363 0.11923222
 0.11953279 0.11957687 0.11912092 0.11862177 0.11862213 0.11938559
 0.11971528 0.11913801 0.11800081 0.11691085 0.11586957 0.11531068
 0.1150005  0.11455149 0.11396986 0.11271678 0.11169749 0.11139974
 0.11171321 0.11228249 0.11247216 0.11249764 0.11197075 0.11203572
 0.11223033 0.11270031 0.11310469 0.11268275 0.11247894 0.11238509
 0.1123483  0.11168034 0.11039305 0.10889731 0.10755438 0.10718936
 0.10770207 0.10855267 0.10841812 0.10731949 0.10645837 0.10649552
 0.10729072 0.10810469 0.10855717 0.10859761 0.10850755 0.10909334
 0.11021139 0.11133393 0.11169556 0.11159209 0.1144757  0.12035173]
