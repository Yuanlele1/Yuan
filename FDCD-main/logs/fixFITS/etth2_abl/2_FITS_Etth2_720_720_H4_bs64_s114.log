Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.133864641189575
Epoch: 1, Steps: 56 | Train Loss: 0.8417158 Vali Loss: 0.8353835 Test Loss: 0.4804295
Validation loss decreased (inf --> 0.835384).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.151257276535034
Epoch: 2, Steps: 56 | Train Loss: 0.6830677 Vali Loss: 0.7765285 Test Loss: 0.4401139
Validation loss decreased (0.835384 --> 0.776528).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.0906381607055664
Epoch: 3, Steps: 56 | Train Loss: 0.6075099 Vali Loss: 0.7435510 Test Loss: 0.4224230
Validation loss decreased (0.776528 --> 0.743551).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.2719480991363525
Epoch: 4, Steps: 56 | Train Loss: 0.5685769 Vali Loss: 0.7243325 Test Loss: 0.4140650
Validation loss decreased (0.743551 --> 0.724333).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.2827348709106445
Epoch: 5, Steps: 56 | Train Loss: 0.5446489 Vali Loss: 0.7180061 Test Loss: 0.4095120
Validation loss decreased (0.724333 --> 0.718006).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.1726064682006836
Epoch: 6, Steps: 56 | Train Loss: 0.5270405 Vali Loss: 0.7098574 Test Loss: 0.4067297
Validation loss decreased (0.718006 --> 0.709857).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.130330801010132
Epoch: 7, Steps: 56 | Train Loss: 0.5142410 Vali Loss: 0.7014766 Test Loss: 0.4045421
Validation loss decreased (0.709857 --> 0.701477).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.187731981277466
Epoch: 8, Steps: 56 | Train Loss: 0.5041089 Vali Loss: 0.7047244 Test Loss: 0.4028775
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.975736141204834
Epoch: 9, Steps: 56 | Train Loss: 0.4949522 Vali Loss: 0.6961182 Test Loss: 0.4014755
Validation loss decreased (0.701477 --> 0.696118).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.0633254051208496
Epoch: 10, Steps: 56 | Train Loss: 0.4877785 Vali Loss: 0.6962790 Test Loss: 0.4001950
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.072855234146118
Epoch: 11, Steps: 56 | Train Loss: 0.4802343 Vali Loss: 0.6872272 Test Loss: 0.3990386
Validation loss decreased (0.696118 --> 0.687227).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.23960018157959
Epoch: 12, Steps: 56 | Train Loss: 0.4750307 Vali Loss: 0.6873041 Test Loss: 0.3981201
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.0463030338287354
Epoch: 13, Steps: 56 | Train Loss: 0.4711595 Vali Loss: 0.6834909 Test Loss: 0.3971513
Validation loss decreased (0.687227 --> 0.683491).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.138244390487671
Epoch: 14, Steps: 56 | Train Loss: 0.4670333 Vali Loss: 0.6834712 Test Loss: 0.3963175
Validation loss decreased (0.683491 --> 0.683471).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.1148531436920166
Epoch: 15, Steps: 56 | Train Loss: 0.4622039 Vali Loss: 0.6811182 Test Loss: 0.3955883
Validation loss decreased (0.683471 --> 0.681118).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.0845088958740234
Epoch: 16, Steps: 56 | Train Loss: 0.4588421 Vali Loss: 0.6772654 Test Loss: 0.3948758
Validation loss decreased (0.681118 --> 0.677265).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.0399587154388428
Epoch: 17, Steps: 56 | Train Loss: 0.4556504 Vali Loss: 0.6819358 Test Loss: 0.3942312
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.1603121757507324
Epoch: 18, Steps: 56 | Train Loss: 0.4542521 Vali Loss: 0.6770487 Test Loss: 0.3936475
Validation loss decreased (0.677265 --> 0.677049).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.0526201725006104
Epoch: 19, Steps: 56 | Train Loss: 0.4520541 Vali Loss: 0.6720715 Test Loss: 0.3930666
Validation loss decreased (0.677049 --> 0.672072).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.179705858230591
Epoch: 20, Steps: 56 | Train Loss: 0.4492662 Vali Loss: 0.6741558 Test Loss: 0.3925930
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0381224155426025
Epoch: 21, Steps: 56 | Train Loss: 0.4475699 Vali Loss: 0.6712701 Test Loss: 0.3921640
Validation loss decreased (0.672072 --> 0.671270).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.21783185005188
Epoch: 22, Steps: 56 | Train Loss: 0.4459183 Vali Loss: 0.6732116 Test Loss: 0.3917080
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.102250099182129
Epoch: 23, Steps: 56 | Train Loss: 0.4439808 Vali Loss: 0.6712357 Test Loss: 0.3913118
Validation loss decreased (0.671270 --> 0.671236).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.153608560562134
Epoch: 24, Steps: 56 | Train Loss: 0.4431379 Vali Loss: 0.6706901 Test Loss: 0.3909878
Validation loss decreased (0.671236 --> 0.670690).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.0122339725494385
Epoch: 25, Steps: 56 | Train Loss: 0.4411828 Vali Loss: 0.6714365 Test Loss: 0.3906265
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.1070287227630615
Epoch: 26, Steps: 56 | Train Loss: 0.4400375 Vali Loss: 0.6694183 Test Loss: 0.3903027
Validation loss decreased (0.670690 --> 0.669418).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.2161357402801514
Epoch: 27, Steps: 56 | Train Loss: 0.4390210 Vali Loss: 0.6691751 Test Loss: 0.3900384
Validation loss decreased (0.669418 --> 0.669175).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.018231153488159
Epoch: 28, Steps: 56 | Train Loss: 0.4372441 Vali Loss: 0.6698018 Test Loss: 0.3897972
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.1773948669433594
Epoch: 29, Steps: 56 | Train Loss: 0.4365781 Vali Loss: 0.6666747 Test Loss: 0.3895107
Validation loss decreased (0.669175 --> 0.666675).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.0506105422973633
Epoch: 30, Steps: 56 | Train Loss: 0.4365778 Vali Loss: 0.6672580 Test Loss: 0.3893200
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.067446708679199
Epoch: 31, Steps: 56 | Train Loss: 0.4350910 Vali Loss: 0.6643916 Test Loss: 0.3891244
Validation loss decreased (0.666675 --> 0.664392).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.1897480487823486
Epoch: 32, Steps: 56 | Train Loss: 0.4340870 Vali Loss: 0.6661913 Test Loss: 0.3889069
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.108255624771118
Epoch: 33, Steps: 56 | Train Loss: 0.4341931 Vali Loss: 0.6655419 Test Loss: 0.3887392
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.1416983604431152
Epoch: 34, Steps: 56 | Train Loss: 0.4334736 Vali Loss: 0.6615690 Test Loss: 0.3885547
Validation loss decreased (0.664392 --> 0.661569).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.1727583408355713
Epoch: 35, Steps: 56 | Train Loss: 0.4332793 Vali Loss: 0.6630744 Test Loss: 0.3883979
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.126767635345459
Epoch: 36, Steps: 56 | Train Loss: 0.4319108 Vali Loss: 0.6661771 Test Loss: 0.3882712
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.2449920177459717
Epoch: 37, Steps: 56 | Train Loss: 0.4323279 Vali Loss: 0.6642375 Test Loss: 0.3881407
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.2056846618652344
Epoch: 38, Steps: 56 | Train Loss: 0.4316341 Vali Loss: 0.6612014 Test Loss: 0.3879877
Validation loss decreased (0.661569 --> 0.661201).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.1816020011901855
Epoch: 39, Steps: 56 | Train Loss: 0.4312690 Vali Loss: 0.6614470 Test Loss: 0.3878904
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.0844128131866455
Epoch: 40, Steps: 56 | Train Loss: 0.4300622 Vali Loss: 0.6627403 Test Loss: 0.3877739
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.314617872238159
Epoch: 41, Steps: 56 | Train Loss: 0.4295381 Vali Loss: 0.6639651 Test Loss: 0.3876792
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.2887158393859863
Epoch: 42, Steps: 56 | Train Loss: 0.4298476 Vali Loss: 0.6667232 Test Loss: 0.3875659
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.1394457817077637
Epoch: 43, Steps: 56 | Train Loss: 0.4299209 Vali Loss: 0.6629946 Test Loss: 0.3874847
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.2304985523223877
Epoch: 44, Steps: 56 | Train Loss: 0.4288035 Vali Loss: 0.6629473 Test Loss: 0.3873953
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.2478067874908447
Epoch: 45, Steps: 56 | Train Loss: 0.4285306 Vali Loss: 0.6585441 Test Loss: 0.3873072
Validation loss decreased (0.661201 --> 0.658544).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.209916591644287
Epoch: 46, Steps: 56 | Train Loss: 0.4289869 Vali Loss: 0.6626981 Test Loss: 0.3872316
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.150017261505127
Epoch: 47, Steps: 56 | Train Loss: 0.4273707 Vali Loss: 0.6572842 Test Loss: 0.3871716
Validation loss decreased (0.658544 --> 0.657284).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.2269721031188965
Epoch: 48, Steps: 56 | Train Loss: 0.4269062 Vali Loss: 0.6596893 Test Loss: 0.3871154
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.1137304306030273
Epoch: 49, Steps: 56 | Train Loss: 0.4286299 Vali Loss: 0.6622950 Test Loss: 0.3870481
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.0857343673706055
Epoch: 50, Steps: 56 | Train Loss: 0.4275234 Vali Loss: 0.6568338 Test Loss: 0.3869818
Validation loss decreased (0.657284 --> 0.656834).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.101139783859253
Epoch: 51, Steps: 56 | Train Loss: 0.4274152 Vali Loss: 0.6600474 Test Loss: 0.3869333
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.2351977825164795
Epoch: 52, Steps: 56 | Train Loss: 0.4275212 Vali Loss: 0.6634299 Test Loss: 0.3868810
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.1457808017730713
Epoch: 53, Steps: 56 | Train Loss: 0.4272452 Vali Loss: 0.6582149 Test Loss: 0.3868341
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.0331273078918457
Epoch: 54, Steps: 56 | Train Loss: 0.4263118 Vali Loss: 0.6603060 Test Loss: 0.3867937
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.2050490379333496
Epoch: 55, Steps: 56 | Train Loss: 0.4265914 Vali Loss: 0.6591218 Test Loss: 0.3867416
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.112025260925293
Epoch: 56, Steps: 56 | Train Loss: 0.4264879 Vali Loss: 0.6608009 Test Loss: 0.3867080
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.1790945529937744
Epoch: 57, Steps: 56 | Train Loss: 0.4270209 Vali Loss: 0.6559367 Test Loss: 0.3866657
Validation loss decreased (0.656834 --> 0.655937).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.072998046875
Epoch: 58, Steps: 56 | Train Loss: 0.4263642 Vali Loss: 0.6597635 Test Loss: 0.3866304
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.1850337982177734
Epoch: 59, Steps: 56 | Train Loss: 0.4263156 Vali Loss: 0.6582098 Test Loss: 0.3866024
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.0730466842651367
Epoch: 60, Steps: 56 | Train Loss: 0.4266838 Vali Loss: 0.6598092 Test Loss: 0.3865690
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.1132843494415283
Epoch: 61, Steps: 56 | Train Loss: 0.4258570 Vali Loss: 0.6549348 Test Loss: 0.3865350
Validation loss decreased (0.655937 --> 0.654935).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.1937692165374756
Epoch: 62, Steps: 56 | Train Loss: 0.4256454 Vali Loss: 0.6597101 Test Loss: 0.3865122
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.1774814128875732
Epoch: 63, Steps: 56 | Train Loss: 0.4261305 Vali Loss: 0.6555175 Test Loss: 0.3864813
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.0941741466522217
Epoch: 64, Steps: 56 | Train Loss: 0.4257733 Vali Loss: 0.6583117 Test Loss: 0.3864585
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.153695583343506
Epoch: 65, Steps: 56 | Train Loss: 0.4259803 Vali Loss: 0.6551488 Test Loss: 0.3864350
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.944354772567749
Epoch: 66, Steps: 56 | Train Loss: 0.4256624 Vali Loss: 0.6589633 Test Loss: 0.3864205
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.143751382827759
Epoch: 67, Steps: 56 | Train Loss: 0.4253927 Vali Loss: 0.6578743 Test Loss: 0.3863933
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.2176032066345215
Epoch: 68, Steps: 56 | Train Loss: 0.4250497 Vali Loss: 0.6583130 Test Loss: 0.3863772
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.198943853378296
Epoch: 69, Steps: 56 | Train Loss: 0.4253505 Vali Loss: 0.6583649 Test Loss: 0.3863626
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.0831422805786133
Epoch: 70, Steps: 56 | Train Loss: 0.4254980 Vali Loss: 0.6603070 Test Loss: 0.3863386
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.2155728340148926
Epoch: 71, Steps: 56 | Train Loss: 0.4251243 Vali Loss: 0.6582021 Test Loss: 0.3863223
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.224318027496338
Epoch: 72, Steps: 56 | Train Loss: 0.4246644 Vali Loss: 0.6605854 Test Loss: 0.3863052
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.165297746658325
Epoch: 73, Steps: 56 | Train Loss: 0.4252800 Vali Loss: 0.6593230 Test Loss: 0.3862920
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.199869394302368
Epoch: 74, Steps: 56 | Train Loss: 0.4245057 Vali Loss: 0.6565547 Test Loss: 0.3862782
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.0639657974243164
Epoch: 75, Steps: 56 | Train Loss: 0.4252636 Vali Loss: 0.6578697 Test Loss: 0.3862616
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.1970789432525635
Epoch: 76, Steps: 56 | Train Loss: 0.4248251 Vali Loss: 0.6589339 Test Loss: 0.3862452
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.2380142211914062
Epoch: 77, Steps: 56 | Train Loss: 0.4242943 Vali Loss: 0.6555791 Test Loss: 0.3862348
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.1491897106170654
Epoch: 78, Steps: 56 | Train Loss: 0.4256302 Vali Loss: 0.6565998 Test Loss: 0.3862256
EarlyStopping counter: 17 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.0414693355560303
Epoch: 79, Steps: 56 | Train Loss: 0.4241024 Vali Loss: 0.6578127 Test Loss: 0.3862183
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.191622257232666
Epoch: 80, Steps: 56 | Train Loss: 0.4252273 Vali Loss: 0.6600109 Test Loss: 0.3862060
EarlyStopping counter: 19 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.167611837387085
Epoch: 81, Steps: 56 | Train Loss: 0.4240580 Vali Loss: 0.6557764 Test Loss: 0.3862002
EarlyStopping counter: 20 out of 20
Early stopping
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=134, out_features=268, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  32177152.0
params:  36180.0
Trainable parameters:  36180
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.139850378036499
Epoch: 1, Steps: 56 | Train Loss: 0.8118499 Vali Loss: 0.6543614 Test Loss: 0.3840746
Validation loss decreased (inf --> 0.654361).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.14314341545105
Epoch: 2, Steps: 56 | Train Loss: 0.8075988 Vali Loss: 0.6503489 Test Loss: 0.3826325
Validation loss decreased (0.654361 --> 0.650349).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.052591323852539
Epoch: 3, Steps: 56 | Train Loss: 0.8047913 Vali Loss: 0.6509064 Test Loss: 0.3815951
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.0516834259033203
Epoch: 4, Steps: 56 | Train Loss: 0.8034096 Vali Loss: 0.6459296 Test Loss: 0.3810510
Validation loss decreased (0.650349 --> 0.645930).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.082568407058716
Epoch: 5, Steps: 56 | Train Loss: 0.8016945 Vali Loss: 0.6463479 Test Loss: 0.3808022
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.1251697540283203
Epoch: 6, Steps: 56 | Train Loss: 0.8005645 Vali Loss: 0.6449725 Test Loss: 0.3805192
Validation loss decreased (0.645930 --> 0.644973).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.065777540206909
Epoch: 7, Steps: 56 | Train Loss: 0.7993743 Vali Loss: 0.6437363 Test Loss: 0.3803429
Validation loss decreased (0.644973 --> 0.643736).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.0031962394714355
Epoch: 8, Steps: 56 | Train Loss: 0.8013250 Vali Loss: 0.6399723 Test Loss: 0.3803561
Validation loss decreased (0.643736 --> 0.639972).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.0348353385925293
Epoch: 9, Steps: 56 | Train Loss: 0.7994701 Vali Loss: 0.6434591 Test Loss: 0.3802294
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.1385903358459473
Epoch: 10, Steps: 56 | Train Loss: 0.7989485 Vali Loss: 0.6423014 Test Loss: 0.3802496
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.121769428253174
Epoch: 11, Steps: 56 | Train Loss: 0.7992614 Vali Loss: 0.6426857 Test Loss: 0.3801754
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1392276287078857
Epoch: 12, Steps: 56 | Train Loss: 0.8003183 Vali Loss: 0.6453389 Test Loss: 0.3802441
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9419686794281006
Epoch: 13, Steps: 56 | Train Loss: 0.8000035 Vali Loss: 0.6374475 Test Loss: 0.3802486
Validation loss decreased (0.639972 --> 0.637447).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.0461349487304688
Epoch: 14, Steps: 56 | Train Loss: 0.7993458 Vali Loss: 0.6407769 Test Loss: 0.3801819
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.1042535305023193
Epoch: 15, Steps: 56 | Train Loss: 0.7994777 Vali Loss: 0.6457438 Test Loss: 0.3802049
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.138627529144287
Epoch: 16, Steps: 56 | Train Loss: 0.7998566 Vali Loss: 0.6414924 Test Loss: 0.3802457
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.2049448490142822
Epoch: 17, Steps: 56 | Train Loss: 0.7994565 Vali Loss: 0.6391228 Test Loss: 0.3802377
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.259822130203247
Epoch: 18, Steps: 56 | Train Loss: 0.7995802 Vali Loss: 0.6394774 Test Loss: 0.3802410
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.1882412433624268
Epoch: 19, Steps: 56 | Train Loss: 0.7979871 Vali Loss: 0.6445475 Test Loss: 0.3802546
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.1960232257843018
Epoch: 20, Steps: 56 | Train Loss: 0.7980986 Vali Loss: 0.6416294 Test Loss: 0.3802294
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.310283660888672
Epoch: 21, Steps: 56 | Train Loss: 0.7992012 Vali Loss: 0.6438510 Test Loss: 0.3802774
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.191581964492798
Epoch: 22, Steps: 56 | Train Loss: 0.7979272 Vali Loss: 0.6409685 Test Loss: 0.3802882
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.2170283794403076
Epoch: 23, Steps: 56 | Train Loss: 0.7979600 Vali Loss: 0.6434550 Test Loss: 0.3802382
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.2000746726989746
Epoch: 24, Steps: 56 | Train Loss: 0.7997221 Vali Loss: 0.6392129 Test Loss: 0.3802895
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.101586103439331
Epoch: 25, Steps: 56 | Train Loss: 0.7994331 Vali Loss: 0.6392013 Test Loss: 0.3802507
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.1099159717559814
Epoch: 26, Steps: 56 | Train Loss: 0.7994363 Vali Loss: 0.6411096 Test Loss: 0.3802882
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.9831950664520264
Epoch: 27, Steps: 56 | Train Loss: 0.7986236 Vali Loss: 0.6441866 Test Loss: 0.3803041
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.1439731121063232
Epoch: 28, Steps: 56 | Train Loss: 0.7989476 Vali Loss: 0.6352913 Test Loss: 0.3802984
Validation loss decreased (0.637447 --> 0.635291).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.1193692684173584
Epoch: 29, Steps: 56 | Train Loss: 0.7989681 Vali Loss: 0.6413836 Test Loss: 0.3802903
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.108339548110962
Epoch: 30, Steps: 56 | Train Loss: 0.7985469 Vali Loss: 0.6427912 Test Loss: 0.3802828
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.204822301864624
Epoch: 31, Steps: 56 | Train Loss: 0.7980096 Vali Loss: 0.6412562 Test Loss: 0.3802594
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.1229419708251953
Epoch: 32, Steps: 56 | Train Loss: 0.7975823 Vali Loss: 0.6397275 Test Loss: 0.3802842
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.1853671073913574
Epoch: 33, Steps: 56 | Train Loss: 0.7977600 Vali Loss: 0.6370117 Test Loss: 0.3802968
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.0364770889282227
Epoch: 34, Steps: 56 | Train Loss: 0.7985189 Vali Loss: 0.6367407 Test Loss: 0.3802936
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.304314374923706
Epoch: 35, Steps: 56 | Train Loss: 0.7986171 Vali Loss: 0.6365061 Test Loss: 0.3802714
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.097951889038086
Epoch: 36, Steps: 56 | Train Loss: 0.7977937 Vali Loss: 0.6411477 Test Loss: 0.3802882
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.1353323459625244
Epoch: 37, Steps: 56 | Train Loss: 0.7986662 Vali Loss: 0.6402266 Test Loss: 0.3802762
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.067911386489868
Epoch: 38, Steps: 56 | Train Loss: 0.7977789 Vali Loss: 0.6373113 Test Loss: 0.3802857
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.0555732250213623
Epoch: 39, Steps: 56 | Train Loss: 0.7985505 Vali Loss: 0.6416568 Test Loss: 0.3802938
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.1113338470458984
Epoch: 40, Steps: 56 | Train Loss: 0.7991917 Vali Loss: 0.6403918 Test Loss: 0.3802797
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.114234447479248
Epoch: 41, Steps: 56 | Train Loss: 0.7979773 Vali Loss: 0.6377859 Test Loss: 0.3802880
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.170454263687134
Epoch: 42, Steps: 56 | Train Loss: 0.7983131 Vali Loss: 0.6399438 Test Loss: 0.3802853
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.136967658996582
Epoch: 43, Steps: 56 | Train Loss: 0.7971815 Vali Loss: 0.6438245 Test Loss: 0.3803074
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.039027214050293
Epoch: 44, Steps: 56 | Train Loss: 0.7972962 Vali Loss: 0.6428764 Test Loss: 0.3802883
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.1671011447906494
Epoch: 45, Steps: 56 | Train Loss: 0.7996197 Vali Loss: 0.6400633 Test Loss: 0.3803165
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.1031107902526855
Epoch: 46, Steps: 56 | Train Loss: 0.7973332 Vali Loss: 0.6380520 Test Loss: 0.3802925
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.2335479259490967
Epoch: 47, Steps: 56 | Train Loss: 0.7979075 Vali Loss: 0.6412584 Test Loss: 0.3803032
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.112809181213379
Epoch: 48, Steps: 56 | Train Loss: 0.7966669 Vali Loss: 0.6386589 Test Loss: 0.3802976
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37888985872268677, mae:0.4232420027256012, rse:0.49199676513671875, corr:[0.21901378 0.2214113  0.22048916 0.21895084 0.21830739 0.21829279
 0.2177793  0.2164127  0.21509397 0.21397649 0.21308105 0.21182619
 0.21034288 0.20911652 0.2083756  0.20804775 0.20751156 0.20650299
 0.20531738 0.20440717 0.20374416 0.20299037 0.20184612 0.20030087
 0.1987916  0.19758764 0.19666746 0.19578786 0.19500597 0.19426696
 0.19358562 0.19278812 0.19183114 0.19086389 0.19005872 0.18932496
 0.18851854 0.1877355  0.18711588 0.18656214 0.18603992 0.18543765
 0.18468763 0.18399827 0.18347849 0.18294296 0.18216756 0.18067335
 0.17886516 0.17725338 0.17618588 0.1755243  0.17516358 0.17479753
 0.1739884  0.17300472 0.17218423 0.17165862 0.17135859 0.17114441
 0.17088878 0.17047268 0.17029946 0.17065294 0.17093101 0.17121373
 0.17126901 0.17108774 0.1708418  0.17068715 0.17054974 0.17024143
 0.16979851 0.16932684 0.1690328  0.16867572 0.16841033 0.16822606
 0.16805655 0.16787505 0.16789582 0.16795076 0.16778016 0.16756079
 0.16757771 0.16791332 0.16827032 0.16842183 0.16823843 0.16804367
 0.16803859 0.16808866 0.1683639  0.16851023 0.16851959 0.16843492
 0.16839406 0.16841479 0.16823864 0.16778585 0.16733679 0.16690874
 0.16683108 0.16686872 0.1670212  0.16705617 0.16712414 0.1672961
 0.16721877 0.1668687  0.16644834 0.16623777 0.16617352 0.16624051
 0.1661856  0.1658881  0.16547346 0.16491987 0.16454954 0.16413574
 0.16351277 0.16263317 0.16176352 0.16110894 0.1604596  0.15980089
 0.15912175 0.15857321 0.15833485 0.15814742 0.15795007 0.15730567
 0.15642336 0.1555468  0.15504026 0.15496604 0.15466247 0.15404697
 0.15332773 0.152897   0.1530252  0.15314002 0.15276003 0.15166913
 0.15005341 0.14885421 0.14829703 0.148029   0.14754598 0.14687584
 0.14633277 0.14596702 0.14582793 0.14556086 0.14496316 0.1441277
 0.14350624 0.14331312 0.14329337 0.14308906 0.1425084  0.1421543
 0.14231333 0.14280298 0.14335129 0.1434403  0.14306067 0.14225702
 0.14149079 0.14093384 0.14064887 0.14045852 0.14027359 0.13990462
 0.13927342 0.13842061 0.13762288 0.13700564 0.13635217 0.13569069
 0.13512407 0.13477828 0.13484147 0.13510104 0.13527802 0.13541333
 0.13539924 0.13540259 0.13565302 0.13620809 0.13701741 0.13746816
 0.13734347 0.1371162  0.13711011 0.13749887 0.1380053  0.13813314
 0.13808778 0.137886   0.13780928 0.13777266 0.1377683  0.1373866
 0.13683046 0.13661861 0.13679288 0.13720581 0.13765097 0.13798258
 0.13824582 0.13857593 0.1390988  0.13950805 0.13960184 0.1392089
 0.13861373 0.13802487 0.13754176 0.13741986 0.13736643 0.13761593
 0.13778324 0.13808905 0.13821894 0.13814813 0.13807967 0.13805412
 0.1382979  0.13867253 0.13912992 0.139634   0.14004435 0.14042293
 0.14091524 0.14159045 0.14247866 0.14329354 0.14385939 0.1441346
 0.1443154  0.14456657 0.14504857 0.14572456 0.14607835 0.1458904
 0.14571762 0.14583358 0.14653754 0.14738971 0.14803301 0.1483932
 0.14870714 0.14934415 0.1502254  0.1513034  0.15202641 0.15252206
 0.15305448 0.15388584 0.15479772 0.1554318  0.15590586 0.15622209
 0.15671961 0.1573041  0.1578395  0.1582445  0.15834467 0.15852632
 0.15890762 0.15931836 0.1596209  0.15973273 0.16002138 0.16059314
 0.16132136 0.16215846 0.16278104 0.16326386 0.1637613  0.16426434
 0.16456619 0.1646755  0.16467938 0.16484563 0.16542484 0.16601591
 0.16631441 0.16621988 0.16585241 0.16573146 0.16572924 0.16574553
 0.16573866 0.16577312 0.16583464 0.16591457 0.16607921 0.16634895
 0.16654018 0.16684385 0.1672138  0.16751713 0.16753763 0.16759399
 0.16759512 0.16794465 0.16847974 0.16905668 0.16948931 0.16958031
 0.1695171  0.16916242 0.16885152 0.16848356 0.1682087  0.16807246
 0.16799119 0.16789426 0.16755922 0.16734008 0.16731244 0.16754045
 0.16774954 0.16796121 0.16852947 0.16911604 0.16940893 0.16925243
 0.16883294 0.16893925 0.16976237 0.17104    0.17230731 0.17289913
 0.17260753 0.17207167 0.17208257 0.17253292 0.17316014 0.17343748
 0.17343175 0.17326452 0.17325637 0.17353527 0.17371131 0.17393705
 0.17398514 0.17412195 0.17458524 0.17541684 0.17636791 0.17733514
 0.17810419 0.17879187 0.17926435 0.17950822 0.17955816 0.17951277
 0.17951974 0.17961863 0.17986608 0.18007922 0.18030182 0.18057878
 0.18103433 0.18171091 0.18224247 0.18269224 0.18279651 0.1826826
 0.18246447 0.18244845 0.18274981 0.18318355 0.1835164  0.18353702
 0.18317801 0.18287492 0.18287149 0.18310636 0.18337524 0.1836082
 0.18367107 0.18357725 0.1834821  0.18350896 0.18370357 0.18402204
 0.18443067 0.18491067 0.18530554 0.18549211 0.18560512 0.18560621
 0.18563075 0.18540043 0.18515289 0.18504375 0.18491088 0.18475036
 0.18448976 0.18424541 0.1840043  0.18383479 0.18376486 0.18377149
 0.18364881 0.18336993 0.18301755 0.18266842 0.18249513 0.18244357
 0.18239185 0.1824108  0.18246123 0.18249586 0.1823536  0.18214361
 0.18170924 0.18120451 0.1806519  0.17995203 0.17892165 0.17752357
 0.17629379 0.1753957  0.17490238 0.17454767 0.17386165 0.17300507
 0.17214295 0.17157687 0.17154834 0.17154157 0.17124897 0.17072384
 0.17020582 0.16994548 0.169498   0.16890323 0.16799691 0.16717844
 0.16662356 0.16661224 0.16662945 0.1663687  0.16565594 0.16484302
 0.16444442 0.16464975 0.16476503 0.16447888 0.16389546 0.1634263
 0.16338886 0.16356501 0.16360514 0.16352165 0.1631734  0.16253617
 0.16182771 0.16139409 0.1610006  0.16086307 0.16082793 0.1604808
 0.15994354 0.15918264 0.15862502 0.15834136 0.15829985 0.15823385
 0.158014   0.157528   0.15686159 0.1564272  0.15625    0.15608248
 0.15556113 0.1547187  0.15390503 0.15302849 0.15236014 0.15188445
 0.15142787 0.15072377 0.1499162  0.1490687  0.14829627 0.14740053
 0.14643164 0.14573018 0.14549029 0.14548787 0.1451206  0.14445494
 0.1437147  0.14313005 0.14284058 0.1426305  0.14227432 0.14137036
 0.14005575 0.1388935  0.13818236 0.13763772 0.13706918 0.13655332
 0.13592255 0.13536027 0.13483468 0.13423666 0.13361965 0.13282374
 0.13200162 0.13119292 0.13051033 0.12973744 0.1289146  0.12838341
 0.12801322 0.12773864 0.12728184 0.126571   0.12554657 0.12441136
 0.12329283 0.12242185 0.12169715 0.12049826 0.11910344 0.11786535
 0.11702512 0.11628485 0.11519951 0.11373672 0.1122769  0.11117547
 0.11044089 0.10971725 0.10885305 0.10777795 0.10653725 0.10560743
 0.10479993 0.10421638 0.10382385 0.10327855 0.1027112  0.10182175
 0.10064191 0.09912948 0.09762578 0.09649638 0.09592244 0.09568229
 0.09524626 0.09412327 0.09294973 0.09200195 0.09166963 0.09154066
 0.0912443  0.09059731 0.08986281 0.08917946 0.08889226 0.08861317
 0.08783361 0.08656631 0.08518808 0.08422753 0.08349901 0.08266409
 0.08139998 0.08016467 0.07910649 0.07823063 0.07737187 0.07626226
 0.07492591 0.07360753 0.07256972 0.07187863 0.07122584 0.07055638
 0.06962442 0.06869931 0.06805004 0.06760155 0.06728286 0.06696455
 0.06644477 0.06591872 0.06532551 0.06462577 0.06364024 0.06253588
 0.06153613 0.06076203 0.05972871 0.05856513 0.05709214 0.05574659
 0.05450653 0.05337207 0.05221662 0.05128522 0.05068374 0.04990336
 0.04900124 0.0481601  0.04769925 0.04762397 0.04770152 0.0480228
 0.0481308  0.0479863  0.04758942 0.0473279  0.04702336 0.04690985
 0.04660252 0.04585546 0.04473888 0.04352242 0.04245179 0.0417496
 0.04122426 0.04075181 0.04049126 0.04054102 0.04043825 0.04004254
 0.03920527 0.03811792 0.03741622 0.03720819 0.03675767 0.03628968
 0.03596376 0.03572273 0.03575148 0.03590924 0.0354743  0.03448433
 0.03337258 0.03264233 0.03219008 0.0314426  0.03054331 0.02956336
 0.02880557 0.02850317 0.02851792 0.02860951 0.02861924 0.02843477
 0.02787306 0.02726485 0.02712401 0.02725347 0.02755278 0.02773387
 0.02775994 0.0277755  0.02769587 0.02787877 0.02808975 0.02800973
 0.02740944 0.02663747 0.0261179  0.02551797 0.02506142 0.02424571
 0.02354006 0.02263745 0.02229137 0.02237638 0.02219441 0.02140579
 0.02047248 0.01951682 0.01943602 0.01988328 0.02009135 0.01975675
 0.01916408 0.0183356  0.01803648 0.01796707 0.01758378 0.01623719
 0.01425521 0.01228185 0.01114537 0.01038336 0.00974607 0.00861176
 0.0073218  0.00571158 0.00443628 0.00387708 0.00364332 0.00363716
 0.00328116 0.00204435 0.00112122 0.00101478 0.00195982 0.00343031
 0.00394168 0.00306159 0.00199947 0.00265729 0.0043114  0.00361716]
