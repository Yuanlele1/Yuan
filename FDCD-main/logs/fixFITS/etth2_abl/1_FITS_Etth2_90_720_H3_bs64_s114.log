Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=22, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=22, out_features=198, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3902976.0
params:  4554.0
Trainable parameters:  4554
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 5.006619930267334
Epoch: 1, Steps: 61 | Train Loss: 1.3623103 Vali Loss: 0.8416666 Test Loss: 0.6803987
Validation loss decreased (inf --> 0.841667).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 5.501631736755371
Epoch: 2, Steps: 61 | Train Loss: 1.1516637 Vali Loss: 0.7666597 Test Loss: 0.5830038
Validation loss decreased (0.841667 --> 0.766660).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 5.85244607925415
Epoch: 3, Steps: 61 | Train Loss: 1.0412163 Vali Loss: 0.7195002 Test Loss: 0.5279104
Validation loss decreased (0.766660 --> 0.719500).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 5.611970663070679
Epoch: 4, Steps: 61 | Train Loss: 0.9771655 Vali Loss: 0.6899379 Test Loss: 0.4937325
Validation loss decreased (0.719500 --> 0.689938).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.771131992340088
Epoch: 5, Steps: 61 | Train Loss: 0.9391848 Vali Loss: 0.6718247 Test Loss: 0.4720973
Validation loss decreased (0.689938 --> 0.671825).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.938573122024536
Epoch: 6, Steps: 61 | Train Loss: 0.9133924 Vali Loss: 0.6628738 Test Loss: 0.4578113
Validation loss decreased (0.671825 --> 0.662874).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.729111433029175
Epoch: 7, Steps: 61 | Train Loss: 0.8982437 Vali Loss: 0.6521964 Test Loss: 0.4481337
Validation loss decreased (0.662874 --> 0.652196).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 5.458043575286865
Epoch: 8, Steps: 61 | Train Loss: 0.8868500 Vali Loss: 0.6499954 Test Loss: 0.4414941
Validation loss decreased (0.652196 --> 0.649995).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.72862434387207
Epoch: 9, Steps: 61 | Train Loss: 0.8786938 Vali Loss: 0.6468594 Test Loss: 0.4367839
Validation loss decreased (0.649995 --> 0.646859).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 6.422145366668701
Epoch: 10, Steps: 61 | Train Loss: 0.8725816 Vali Loss: 0.6435769 Test Loss: 0.4334772
Validation loss decreased (0.646859 --> 0.643577).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 5.814577102661133
Epoch: 11, Steps: 61 | Train Loss: 0.8683780 Vali Loss: 0.6337594 Test Loss: 0.4310603
Validation loss decreased (0.643577 --> 0.633759).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 6.142286539077759
Epoch: 12, Steps: 61 | Train Loss: 0.8663502 Vali Loss: 0.6376683 Test Loss: 0.4292457
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 5.8063435554504395
Epoch: 13, Steps: 61 | Train Loss: 0.8646281 Vali Loss: 0.6319516 Test Loss: 0.4279349
Validation loss decreased (0.633759 --> 0.631952).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.626028060913086
Epoch: 14, Steps: 61 | Train Loss: 0.8626955 Vali Loss: 0.6371317 Test Loss: 0.4268963
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 5.578836441040039
Epoch: 15, Steps: 61 | Train Loss: 0.8616303 Vali Loss: 0.6342344 Test Loss: 0.4260968
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.674619674682617
Epoch: 16, Steps: 61 | Train Loss: 0.8593591 Vali Loss: 0.6329118 Test Loss: 0.4254529
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.2149434089660645
Epoch: 17, Steps: 61 | Train Loss: 0.8585079 Vali Loss: 0.6364576 Test Loss: 0.4249556
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.812715768814087
Epoch: 18, Steps: 61 | Train Loss: 0.8584783 Vali Loss: 0.6334684 Test Loss: 0.4245482
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.720754861831665
Epoch: 19, Steps: 61 | Train Loss: 0.8573801 Vali Loss: 0.6309621 Test Loss: 0.4242185
Validation loss decreased (0.631952 --> 0.630962).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.8185317516326904
Epoch: 20, Steps: 61 | Train Loss: 0.8561949 Vali Loss: 0.6313098 Test Loss: 0.4239349
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0660173892974854
Epoch: 21, Steps: 61 | Train Loss: 0.8561492 Vali Loss: 0.6311129 Test Loss: 0.4237013
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.177884817123413
Epoch: 22, Steps: 61 | Train Loss: 0.8566631 Vali Loss: 0.6329383 Test Loss: 0.4234850
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.8604590892791748
Epoch: 23, Steps: 61 | Train Loss: 0.8562561 Vali Loss: 0.6229143 Test Loss: 0.4233158
Validation loss decreased (0.630962 --> 0.622914).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.1362357139587402
Epoch: 24, Steps: 61 | Train Loss: 0.8556223 Vali Loss: 0.6287887 Test Loss: 0.4231439
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.13893461227417
Epoch: 25, Steps: 61 | Train Loss: 0.8565445 Vali Loss: 0.6268361 Test Loss: 0.4230392
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.8763368129730225
Epoch: 26, Steps: 61 | Train Loss: 0.8544385 Vali Loss: 0.6331819 Test Loss: 0.4229110
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.0203857421875
Epoch: 27, Steps: 61 | Train Loss: 0.8541167 Vali Loss: 0.6255358 Test Loss: 0.4227957
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.8136084079742432
Epoch: 28, Steps: 61 | Train Loss: 0.8542058 Vali Loss: 0.6323857 Test Loss: 0.4227017
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.2836623191833496
Epoch: 29, Steps: 61 | Train Loss: 0.8538937 Vali Loss: 0.6264908 Test Loss: 0.4226105
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.419936180114746
Epoch: 30, Steps: 61 | Train Loss: 0.8554493 Vali Loss: 0.6339811 Test Loss: 0.4225345
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.9252517223358154
Epoch: 31, Steps: 61 | Train Loss: 0.8551127 Vali Loss: 0.6251987 Test Loss: 0.4224489
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.031728982925415
Epoch: 32, Steps: 61 | Train Loss: 0.8551739 Vali Loss: 0.6316144 Test Loss: 0.4223981
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.0159289836883545
Epoch: 33, Steps: 61 | Train Loss: 0.8539317 Vali Loss: 0.6283590 Test Loss: 0.4223238
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.7733163833618164
Epoch: 34, Steps: 61 | Train Loss: 0.8551734 Vali Loss: 0.6276001 Test Loss: 0.4222737
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.8019771575927734
Epoch: 35, Steps: 61 | Train Loss: 0.8540791 Vali Loss: 0.6305142 Test Loss: 0.4222298
EarlyStopping counter: 12 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.9975602626800537
Epoch: 36, Steps: 61 | Train Loss: 0.8542445 Vali Loss: 0.6312338 Test Loss: 0.4221762
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9082763195037842
Epoch: 37, Steps: 61 | Train Loss: 0.8531745 Vali Loss: 0.6293366 Test Loss: 0.4221251
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.002946138381958
Epoch: 38, Steps: 61 | Train Loss: 0.8538836 Vali Loss: 0.6214529 Test Loss: 0.4220866
Validation loss decreased (0.622914 --> 0.621453).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7502684593200684
Epoch: 39, Steps: 61 | Train Loss: 0.8531186 Vali Loss: 0.6298336 Test Loss: 0.4220518
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.9902055263519287
Epoch: 40, Steps: 61 | Train Loss: 0.8541658 Vali Loss: 0.6222421 Test Loss: 0.4220125
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9116122722625732
Epoch: 41, Steps: 61 | Train Loss: 0.8534401 Vali Loss: 0.6339816 Test Loss: 0.4219823
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.8057398796081543
Epoch: 42, Steps: 61 | Train Loss: 0.8537728 Vali Loss: 0.6308183 Test Loss: 0.4219516
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.120734930038452
Epoch: 43, Steps: 61 | Train Loss: 0.8527407 Vali Loss: 0.6295192 Test Loss: 0.4219189
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.9269428253173828
Epoch: 44, Steps: 61 | Train Loss: 0.8539496 Vali Loss: 0.6281968 Test Loss: 0.4218900
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.9416866302490234
Epoch: 45, Steps: 61 | Train Loss: 0.8519536 Vali Loss: 0.6262524 Test Loss: 0.4218651
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8802361488342285
Epoch: 46, Steps: 61 | Train Loss: 0.8534887 Vali Loss: 0.6287482 Test Loss: 0.4218383
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.0052084922790527
Epoch: 47, Steps: 61 | Train Loss: 0.8526315 Vali Loss: 0.6244270 Test Loss: 0.4218177
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.0430524349212646
Epoch: 48, Steps: 61 | Train Loss: 0.8536216 Vali Loss: 0.6270146 Test Loss: 0.4217985
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.6083104610443115
Epoch: 49, Steps: 61 | Train Loss: 0.8520406 Vali Loss: 0.6265830 Test Loss: 0.4217770
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 5.6705732345581055
Epoch: 50, Steps: 61 | Train Loss: 0.8524939 Vali Loss: 0.6237051 Test Loss: 0.4217546
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 5.212396144866943
Epoch: 51, Steps: 61 | Train Loss: 0.8523004 Vali Loss: 0.6283047 Test Loss: 0.4217396
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.197840929031372
Epoch: 52, Steps: 61 | Train Loss: 0.8513791 Vali Loss: 0.6319010 Test Loss: 0.4217260
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 5.398826837539673
Epoch: 53, Steps: 61 | Train Loss: 0.8511523 Vali Loss: 0.6252498 Test Loss: 0.4217059
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 6.046387434005737
Epoch: 54, Steps: 61 | Train Loss: 0.8525954 Vali Loss: 0.6291339 Test Loss: 0.4216863
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.60072922706604
Epoch: 55, Steps: 61 | Train Loss: 0.8519142 Vali Loss: 0.6268145 Test Loss: 0.4216776
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.7507615089416504
Epoch: 56, Steps: 61 | Train Loss: 0.8521865 Vali Loss: 0.6298025 Test Loss: 0.4216640
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 4.462610483169556
Epoch: 57, Steps: 61 | Train Loss: 0.8517039 Vali Loss: 0.6269341 Test Loss: 0.4216512
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 6.093963146209717
Epoch: 58, Steps: 61 | Train Loss: 0.8513854 Vali Loss: 0.6292975 Test Loss: 0.4216391
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4204407334327698, mae:0.43748003244400024, rse:0.5182724595069885, corr:[ 0.2169753   0.22208986  0.21854943  0.21674228  0.21686868  0.21564896
  0.2138363   0.21296178  0.21252625  0.21113938  0.2097041   0.2088406
  0.20793873  0.2067045   0.20582254  0.20566274  0.20525786  0.20421736
  0.20326276  0.20278941  0.20214914  0.20080654  0.19918945  0.19751644
  0.19522525  0.19292083  0.19107972  0.19042307  0.1900238   0.18903963
  0.18827474  0.18774393  0.18729389  0.18641101  0.18560621  0.18507494
  0.18455124  0.18394719  0.18329519  0.18294509  0.182736    0.18229045
  0.1816611   0.18096222  0.18052891  0.17998491  0.17875727  0.17650104
  0.17367564  0.17175879  0.17002414  0.16887225  0.16819757  0.16764057
  0.16716509  0.16599035  0.16559786  0.16564378  0.16584417  0.16534206
  0.16456307  0.16474162  0.16525188  0.16548677  0.16557278  0.16568227
  0.16557217  0.16486546  0.16427957  0.16425441  0.1638726   0.16239762
  0.16012068  0.15932441  0.15908854  0.15859379  0.1579125   0.15796453
  0.15861215  0.15830816  0.15788637  0.15783092  0.15819848  0.15822892
  0.15764374  0.15735596  0.15763044  0.1579901   0.15804075  0.1578072
  0.15748908  0.15689445  0.15636542  0.15593733  0.15550047  0.15464157
  0.15288457  0.15172492  0.15091561  0.1503601   0.14964914  0.14888589
  0.14894706  0.14856584  0.1484509   0.1484027   0.14879236  0.14901945
  0.14867315  0.14835776  0.14815812  0.14812109  0.14791855  0.14742355
  0.14698802  0.1464435   0.14595047  0.14519969  0.14398633  0.14238966
  0.14025515  0.13886474  0.1376294   0.13688707  0.13615513  0.1356328
  0.13547444  0.13505338  0.13507842  0.1350387   0.13497637  0.1345821
  0.13403411  0.1336306   0.13323538  0.13288842  0.13264656  0.13214716
  0.13128784  0.13020857  0.1295749   0.1291739   0.12811503  0.12627353
  0.1233696   0.12120503  0.11970644  0.11909463  0.11881322  0.11833737
  0.11806452  0.11738908  0.11733336  0.11755303  0.11780756  0.11751711
  0.11708319  0.11710872  0.11710516  0.11698422  0.116878    0.11682443
  0.11668325  0.11606019  0.1156503   0.11525404  0.11440795  0.11277333
  0.11020109  0.1087091   0.10775251  0.10721977  0.1068643   0.10684161
  0.10726212  0.10718843  0.10741037  0.10771021  0.10823691  0.10808083
  0.10754593  0.10730425  0.1073855   0.10767525  0.10766118  0.10762089
  0.10775274  0.10762537  0.10725015  0.107117    0.10702778  0.10680901
  0.10576371  0.10508572  0.1045958   0.10471756  0.10536774  0.1061425
  0.10742697  0.10807725  0.10863061  0.1090774   0.10970747  0.1100462
  0.11015143  0.11022191  0.11042517  0.11076856  0.11101488  0.11125031
  0.11130747  0.11111216  0.11091295  0.11052711  0.11003655  0.10908584
  0.10757727  0.10660364  0.10576617  0.10556386  0.10564677  0.10643821
  0.1078911   0.10903288  0.10987045  0.11047109  0.11110348  0.11138992
  0.11153007  0.11174631  0.11216385  0.11271935  0.11322267  0.11351887
  0.11370095  0.11387312  0.1140853   0.11426747  0.11407384  0.11358317
  0.11255635  0.11201852  0.11134837  0.11115064  0.11163286  0.11272622
  0.11439481  0.11506502  0.11578336  0.11645005  0.11732045  0.11770193
  0.11774468  0.118078    0.11834135  0.11865693  0.11887995  0.11933021
  0.119605    0.11957397  0.11942659  0.11927553  0.11906847  0.11869097
  0.11771885  0.1174854   0.11746854  0.11776616  0.11824343  0.11941785
  0.12106338  0.12195905  0.1227449   0.12310414  0.12357406  0.12370461
  0.12380515  0.1237634   0.1237339   0.12380897  0.12403991  0.12418254
  0.12407965  0.12381703  0.12378455  0.12386513  0.12359626  0.12279771
  0.12176704  0.12147164  0.12130177  0.12109087  0.12104686  0.12140294
  0.12212796  0.12251062  0.12319287  0.12381262  0.12415242  0.12413977
  0.12427085  0.12474957  0.12508     0.12504369  0.12482222  0.12498073
  0.12519974  0.12485001  0.12437735  0.12395754  0.12361806  0.12315333
  0.12204656  0.12168416  0.12174478  0.12186033  0.12182006  0.12226752
  0.12337668  0.12394696  0.12458587  0.12523296  0.1259359   0.12613587
  0.12623388  0.12661532  0.12713987  0.12749104  0.12766266  0.12791976
  0.12823053  0.12811497  0.12768759  0.12752798  0.12766299  0.12779878
  0.12685682  0.12635441  0.12646464  0.12704046  0.12799422  0.12909022
  0.13087061  0.13188596  0.13248283  0.1330155   0.13377818  0.13428053
  0.13429582  0.13438837  0.13479257  0.13518445  0.13547024  0.1358151
  0.1363637   0.13663027  0.13661857  0.13680021  0.13700956  0.13694422
  0.13638926  0.13632964  0.13653646  0.1371343   0.13821991  0.13988969
  0.142148    0.14397512  0.14541052  0.14634623  0.14718165  0.14787087
  0.14841887  0.14903174  0.1497217   0.15041153  0.15100263  0.15171844
  0.15239516  0.15292975  0.1532679   0.15345998  0.15377717  0.15421653
  0.15436965  0.15455411  0.15477586  0.15532951  0.15658897  0.15834613
  0.16063169  0.16212848  0.16322693  0.16399318  0.16478252  0.16536999
  0.16571681  0.16584481  0.1658446   0.16602497  0.16635272  0.16657422
  0.16666488  0.16665554  0.16673443  0.1667373   0.16653778  0.16643193
  0.16598758  0.166052    0.16593531  0.1658563   0.16629907  0.16735141
  0.16869098  0.16930757  0.16978803  0.17029218  0.17056192  0.17042132
  0.1703233   0.17055859  0.17063814  0.17030545  0.16996856  0.16999498
  0.17010702  0.16996135  0.16976564  0.16985507  0.16987346  0.16952495
  0.16886598  0.16866837  0.16860951  0.16839813  0.16814539  0.1682552
  0.16927522  0.16966479  0.16985281  0.16984881  0.16988236  0.16987404
  0.16980498  0.1697784   0.16966444  0.16963516  0.16959408  0.16956568
  0.16944295  0.16936438  0.16935124  0.16928546  0.16899511  0.16845216
  0.16765651  0.1673006   0.16695459  0.16686021  0.16679859  0.16678186
  0.16698949  0.16686176  0.1669373   0.1670659   0.16703299  0.16659765
  0.16632171  0.16640317  0.16627447  0.16590887  0.16552775  0.16531862
  0.1651566   0.16474864  0.16435057  0.16404842  0.16346996  0.16241466
  0.16079588  0.1596596   0.1588068   0.15782844  0.15694846  0.15631709
  0.15622732  0.15586981  0.15574774  0.15549052  0.1548391   0.15386117
  0.15331414  0.1533427   0.15323392  0.15282275  0.1524637   0.15229619
  0.1519755   0.15132877  0.1508675   0.15054354  0.14971657  0.14801793
  0.14569873  0.1438311   0.14224556  0.14094755  0.14008798  0.13962513
  0.13927297  0.13871734  0.13855837  0.13865317  0.13857405  0.13790923
  0.13729748  0.13716221  0.1370998   0.13667336  0.13602759  0.13539985
  0.13472028  0.1340614   0.13361467  0.13333303  0.13217537  0.12987876
  0.12699977  0.12494985  0.12306627  0.12077898  0.11882824  0.1175121
  0.11689675  0.11587842  0.11543649  0.1155013   0.1155679   0.11491653
  0.11412002  0.11406847  0.11398198  0.11324812  0.11206362  0.11128766
  0.1106883   0.1099003   0.10879881  0.10755379  0.10591474  0.1035095
  0.1002904   0.09830109  0.09638491  0.09442003  0.09271482  0.09197756
  0.09209003  0.09146415  0.0911213   0.09098163  0.09123179  0.09093998
  0.08998289  0.08929964  0.08901768  0.08864484  0.08805513  0.08734916
  0.08655829  0.08555397  0.08438046  0.08302274  0.08115341  0.07860144
  0.07556035  0.07355316  0.07192563  0.0701261   0.06853773  0.06754005
  0.06738315  0.06708058  0.06700452  0.06694599  0.0668018   0.06635313
  0.06566889  0.06533272  0.0652399   0.06509941  0.06469678  0.06418338
  0.06374348  0.06326462  0.06263801  0.06182029  0.06034251  0.05790059
  0.05479975  0.05297983  0.05163505  0.05006661  0.04826242  0.04718011
  0.04712151  0.04692565  0.04679123  0.04671767  0.04700428  0.04715089
  0.04701792  0.04681458  0.04658528  0.04657842  0.04655856  0.04624598
  0.04560661  0.0449664   0.04465209  0.0443532   0.04299165  0.04065592
  0.03757876  0.03566496  0.0342971   0.03284026  0.0310503   0.02982293
  0.02982138  0.02974425  0.02994985  0.02994032  0.029701    0.02937138
  0.02903856  0.02866127  0.02804502  0.0274424   0.02679958  0.02605989
  0.02524633  0.02436928  0.02397878  0.02351948  0.02234777  0.01996645
  0.0167397   0.01423169  0.01260125  0.0115856   0.01010134  0.00822434
  0.00716368  0.00697656  0.00762653  0.00778607  0.00722931  0.00632188
  0.00614641  0.0066385   0.00666289  0.0062461   0.00598783  0.00588903
  0.0054767   0.00451159  0.00388397  0.00397553  0.00351933  0.00153701
 -0.00188465 -0.00494848 -0.00712834 -0.00844307 -0.00965194 -0.01112616
 -0.01196416 -0.0122778  -0.0111311  -0.00979339 -0.00944833 -0.0104218
 -0.01126898 -0.01126912 -0.01115064 -0.01150811 -0.01179823 -0.01147197
 -0.01091879 -0.01141085 -0.01233026 -0.01286361 -0.01360617 -0.01493868
 -0.01816419 -0.02153365 -0.02449075 -0.02528516 -0.02479963 -0.02510736
 -0.02569172 -0.02602539 -0.02413003 -0.02217251 -0.02168348 -0.02342453
 -0.0244687  -0.02350378 -0.02254013 -0.02369716 -0.02575714 -0.02508114
 -0.02212989 -0.02193828 -0.02592294 -0.0280461  -0.02315237 -0.02087728]
