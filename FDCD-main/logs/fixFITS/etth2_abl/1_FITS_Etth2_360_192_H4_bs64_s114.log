Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_192_FITS_ETTh2_ftM_sl360_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8089
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=74, out_features=113, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  7492352.0
params:  8475.0
Trainable parameters:  8475
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.4232280254364014
Epoch: 1, Steps: 63 | Train Loss: 0.7041614 Vali Loss: 0.3755871 Test Loss: 0.4248841
Validation loss decreased (inf --> 0.375587).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.320136070251465
Epoch: 2, Steps: 63 | Train Loss: 0.5945325 Vali Loss: 0.3367986 Test Loss: 0.3941087
Validation loss decreased (0.375587 --> 0.336799).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.9743335247039795
Epoch: 3, Steps: 63 | Train Loss: 0.5626057 Vali Loss: 0.3194487 Test Loss: 0.3822468
Validation loss decreased (0.336799 --> 0.319449).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.4390485286712646
Epoch: 4, Steps: 63 | Train Loss: 0.5485957 Vali Loss: 0.3096358 Test Loss: 0.3755502
Validation loss decreased (0.319449 --> 0.309636).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.4443359375
Epoch: 5, Steps: 63 | Train Loss: 0.5384989 Vali Loss: 0.3034340 Test Loss: 0.3710040
Validation loss decreased (0.309636 --> 0.303434).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.907569408416748
Epoch: 6, Steps: 63 | Train Loss: 0.5328656 Vali Loss: 0.2993408 Test Loss: 0.3677566
Validation loss decreased (0.303434 --> 0.299341).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.972155809402466
Epoch: 7, Steps: 63 | Train Loss: 0.5291196 Vali Loss: 0.2963261 Test Loss: 0.3656683
Validation loss decreased (0.299341 --> 0.296326).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.2940802574157715
Epoch: 8, Steps: 63 | Train Loss: 0.5261894 Vali Loss: 0.2943568 Test Loss: 0.3638192
Validation loss decreased (0.296326 --> 0.294357).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.795444965362549
Epoch: 9, Steps: 63 | Train Loss: 0.5234792 Vali Loss: 0.2926118 Test Loss: 0.3625171
Validation loss decreased (0.294357 --> 0.292612).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.613762617111206
Epoch: 10, Steps: 63 | Train Loss: 0.5204833 Vali Loss: 0.2911831 Test Loss: 0.3617027
Validation loss decreased (0.292612 --> 0.291183).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.345494270324707
Epoch: 11, Steps: 63 | Train Loss: 0.5200875 Vali Loss: 0.2900065 Test Loss: 0.3609935
Validation loss decreased (0.291183 --> 0.290006).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.485659599304199
Epoch: 12, Steps: 63 | Train Loss: 0.5176586 Vali Loss: 0.2891676 Test Loss: 0.3603395
Validation loss decreased (0.290006 --> 0.289168).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.9843025207519531
Epoch: 13, Steps: 63 | Train Loss: 0.5166855 Vali Loss: 0.2883883 Test Loss: 0.3597250
Validation loss decreased (0.289168 --> 0.288388).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.4904701709747314
Epoch: 14, Steps: 63 | Train Loss: 0.5164899 Vali Loss: 0.2878000 Test Loss: 0.3594192
Validation loss decreased (0.288388 --> 0.287800).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.1486785411834717
Epoch: 15, Steps: 63 | Train Loss: 0.5149741 Vali Loss: 0.2867723 Test Loss: 0.3591727
Validation loss decreased (0.287800 --> 0.286772).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.448430299758911
Epoch: 16, Steps: 63 | Train Loss: 0.5146905 Vali Loss: 0.2866940 Test Loss: 0.3586200
Validation loss decreased (0.286772 --> 0.286694).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.033389091491699
Epoch: 17, Steps: 63 | Train Loss: 0.5142955 Vali Loss: 0.2860705 Test Loss: 0.3585333
Validation loss decreased (0.286694 --> 0.286070).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.103861093521118
Epoch: 18, Steps: 63 | Train Loss: 0.5136360 Vali Loss: 0.2856040 Test Loss: 0.3582291
Validation loss decreased (0.286070 --> 0.285604).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.9863405227661133
Epoch: 19, Steps: 63 | Train Loss: 0.5129881 Vali Loss: 0.2853163 Test Loss: 0.3579545
Validation loss decreased (0.285604 --> 0.285316).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.464643955230713
Epoch: 20, Steps: 63 | Train Loss: 0.5131685 Vali Loss: 0.2848820 Test Loss: 0.3578579
Validation loss decreased (0.285316 --> 0.284882).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.547475576400757
Epoch: 21, Steps: 63 | Train Loss: 0.5121492 Vali Loss: 0.2845457 Test Loss: 0.3576308
Validation loss decreased (0.284882 --> 0.284546).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.270247220993042
Epoch: 22, Steps: 63 | Train Loss: 0.5105661 Vali Loss: 0.2843093 Test Loss: 0.3575162
Validation loss decreased (0.284546 --> 0.284309).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.244483232498169
Epoch: 23, Steps: 63 | Train Loss: 0.5102611 Vali Loss: 0.2841303 Test Loss: 0.3572581
Validation loss decreased (0.284309 --> 0.284130).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.8320555686950684
Epoch: 24, Steps: 63 | Train Loss: 0.5098879 Vali Loss: 0.2837794 Test Loss: 0.3572224
Validation loss decreased (0.284130 --> 0.283779).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.1536197662353516
Epoch: 25, Steps: 63 | Train Loss: 0.5097211 Vali Loss: 0.2835539 Test Loss: 0.3571857
Validation loss decreased (0.283779 --> 0.283554).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.9561069011688232
Epoch: 26, Steps: 63 | Train Loss: 0.5104057 Vali Loss: 0.2834155 Test Loss: 0.3569961
Validation loss decreased (0.283554 --> 0.283415).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.8465113639831543
Epoch: 27, Steps: 63 | Train Loss: 0.5109321 Vali Loss: 0.2831439 Test Loss: 0.3568856
Validation loss decreased (0.283415 --> 0.283144).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.0898778438568115
Epoch: 28, Steps: 63 | Train Loss: 0.5091831 Vali Loss: 0.2830722 Test Loss: 0.3567794
Validation loss decreased (0.283144 --> 0.283072).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.8134050369262695
Epoch: 29, Steps: 63 | Train Loss: 0.5100776 Vali Loss: 0.2829298 Test Loss: 0.3567744
Validation loss decreased (0.283072 --> 0.282930).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.9180526733398438
Epoch: 30, Steps: 63 | Train Loss: 0.5087544 Vali Loss: 0.2828004 Test Loss: 0.3566384
Validation loss decreased (0.282930 --> 0.282800).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.5939435958862305
Epoch: 31, Steps: 63 | Train Loss: 0.5095921 Vali Loss: 0.2826575 Test Loss: 0.3565872
Validation loss decreased (0.282800 --> 0.282658).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.267563581466675
Epoch: 32, Steps: 63 | Train Loss: 0.5092650 Vali Loss: 0.2825511 Test Loss: 0.3565636
Validation loss decreased (0.282658 --> 0.282551).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.380263328552246
Epoch: 33, Steps: 63 | Train Loss: 0.5100153 Vali Loss: 0.2824775 Test Loss: 0.3564667
Validation loss decreased (0.282551 --> 0.282477).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.4580929279327393
Epoch: 34, Steps: 63 | Train Loss: 0.5096852 Vali Loss: 0.2824422 Test Loss: 0.3564042
Validation loss decreased (0.282477 --> 0.282442).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.1509673595428467
Epoch: 35, Steps: 63 | Train Loss: 0.5091927 Vali Loss: 0.2822646 Test Loss: 0.3563946
Validation loss decreased (0.282442 --> 0.282265).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.925706148147583
Epoch: 36, Steps: 63 | Train Loss: 0.5079644 Vali Loss: 0.2821521 Test Loss: 0.3563996
Validation loss decreased (0.282265 --> 0.282152).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.9083945751190186
Epoch: 37, Steps: 63 | Train Loss: 0.5092305 Vali Loss: 0.2820834 Test Loss: 0.3563568
Validation loss decreased (0.282152 --> 0.282083).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9509658813476562
Epoch: 38, Steps: 63 | Train Loss: 0.5086354 Vali Loss: 0.2820217 Test Loss: 0.3562810
Validation loss decreased (0.282083 --> 0.282022).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.7422208786010742
Epoch: 39, Steps: 63 | Train Loss: 0.5085939 Vali Loss: 0.2818989 Test Loss: 0.3562572
Validation loss decreased (0.282022 --> 0.281899).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.280054807662964
Epoch: 40, Steps: 63 | Train Loss: 0.5091857 Vali Loss: 0.2817157 Test Loss: 0.3561986
Validation loss decreased (0.281899 --> 0.281716).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.808940887451172
Epoch: 41, Steps: 63 | Train Loss: 0.5081917 Vali Loss: 0.2817569 Test Loss: 0.3561760
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.5991852283477783
Epoch: 42, Steps: 63 | Train Loss: 0.5086944 Vali Loss: 0.2817235 Test Loss: 0.3561704
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.774851083755493
Epoch: 43, Steps: 63 | Train Loss: 0.5089557 Vali Loss: 0.2816986 Test Loss: 0.3561047
Validation loss decreased (0.281716 --> 0.281699).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.8666577339172363
Epoch: 44, Steps: 63 | Train Loss: 0.5077684 Vali Loss: 0.2817303 Test Loss: 0.3560299
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.8238890171051025
Epoch: 45, Steps: 63 | Train Loss: 0.5080241 Vali Loss: 0.2815960 Test Loss: 0.3560623
Validation loss decreased (0.281699 --> 0.281596).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 4.6427223682403564
Epoch: 46, Steps: 63 | Train Loss: 0.5077313 Vali Loss: 0.2815437 Test Loss: 0.3560816
Validation loss decreased (0.281596 --> 0.281544).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.9235641956329346
Epoch: 47, Steps: 63 | Train Loss: 0.5074782 Vali Loss: 0.2814889 Test Loss: 0.3560252
Validation loss decreased (0.281544 --> 0.281489).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.3299214839935303
Epoch: 48, Steps: 63 | Train Loss: 0.5080756 Vali Loss: 0.2812723 Test Loss: 0.3560460
Validation loss decreased (0.281489 --> 0.281272).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.6928720474243164
Epoch: 49, Steps: 63 | Train Loss: 0.5069266 Vali Loss: 0.2813638 Test Loss: 0.3559996
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3185267448425293
Epoch: 50, Steps: 63 | Train Loss: 0.5081835 Vali Loss: 0.2810391 Test Loss: 0.3560031
Validation loss decreased (0.281272 --> 0.281039).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.8912291526794434
Epoch: 51, Steps: 63 | Train Loss: 0.5075126 Vali Loss: 0.2812948 Test Loss: 0.3559606
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.649946689605713
Epoch: 52, Steps: 63 | Train Loss: 0.5077889 Vali Loss: 0.2812737 Test Loss: 0.3559573
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.5001108646392822
Epoch: 53, Steps: 63 | Train Loss: 0.5067587 Vali Loss: 0.2813124 Test Loss: 0.3559147
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.830066442489624
Epoch: 54, Steps: 63 | Train Loss: 0.5083349 Vali Loss: 0.2811168 Test Loss: 0.3559341
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.252856731414795
Epoch: 55, Steps: 63 | Train Loss: 0.5082653 Vali Loss: 0.2812063 Test Loss: 0.3558959
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.1869771480560303
Epoch: 56, Steps: 63 | Train Loss: 0.5082487 Vali Loss: 0.2811710 Test Loss: 0.3559095
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.4611427783966064
Epoch: 57, Steps: 63 | Train Loss: 0.5078638 Vali Loss: 0.2811516 Test Loss: 0.3559038
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.4900546073913574
Epoch: 58, Steps: 63 | Train Loss: 0.5082880 Vali Loss: 0.2811255 Test Loss: 0.3558946
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.422426462173462
Epoch: 59, Steps: 63 | Train Loss: 0.5075323 Vali Loss: 0.2810443 Test Loss: 0.3558689
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.714080572128296
Epoch: 60, Steps: 63 | Train Loss: 0.5081376 Vali Loss: 0.2810928 Test Loss: 0.3558424
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 3.2349891662597656
Epoch: 61, Steps: 63 | Train Loss: 0.5075881 Vali Loss: 0.2810778 Test Loss: 0.3558535
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.931213617324829
Epoch: 62, Steps: 63 | Train Loss: 0.5072007 Vali Loss: 0.2810444 Test Loss: 0.3558576
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.2638213634490967
Epoch: 63, Steps: 63 | Train Loss: 0.5061309 Vali Loss: 0.2809903 Test Loss: 0.3558364
Validation loss decreased (0.281039 --> 0.280990).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 5.499474287033081
Epoch: 64, Steps: 63 | Train Loss: 0.5077012 Vali Loss: 0.2809986 Test Loss: 0.3558267
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.9805667400360107
Epoch: 65, Steps: 63 | Train Loss: 0.5079509 Vali Loss: 0.2809674 Test Loss: 0.3558334
Validation loss decreased (0.280990 --> 0.280967).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.252948045730591
Epoch: 66, Steps: 63 | Train Loss: 0.5078535 Vali Loss: 0.2809922 Test Loss: 0.3558187
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.5738203525543213
Epoch: 67, Steps: 63 | Train Loss: 0.5071891 Vali Loss: 0.2808950 Test Loss: 0.3558106
Validation loss decreased (0.280967 --> 0.280895).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 4.099001407623291
Epoch: 68, Steps: 63 | Train Loss: 0.5080814 Vali Loss: 0.2809397 Test Loss: 0.3558069
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.6052277088165283
Epoch: 69, Steps: 63 | Train Loss: 0.5070367 Vali Loss: 0.2809621 Test Loss: 0.3557904
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.9335525035858154
Epoch: 70, Steps: 63 | Train Loss: 0.5073962 Vali Loss: 0.2809334 Test Loss: 0.3557933
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.185795545578003
Epoch: 71, Steps: 63 | Train Loss: 0.5060101 Vali Loss: 0.2808191 Test Loss: 0.3557739
Validation loss decreased (0.280895 --> 0.280819).  Saving model ...
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.8742737770080566
Epoch: 72, Steps: 63 | Train Loss: 0.5067052 Vali Loss: 0.2808922 Test Loss: 0.3557839
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.913212776184082
Epoch: 73, Steps: 63 | Train Loss: 0.5076302 Vali Loss: 0.2808723 Test Loss: 0.3557748
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.1622297763824463
Epoch: 74, Steps: 63 | Train Loss: 0.5079204 Vali Loss: 0.2809066 Test Loss: 0.3557592
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.2549281120300293
Epoch: 75, Steps: 63 | Train Loss: 0.5071176 Vali Loss: 0.2807845 Test Loss: 0.3557592
Validation loss decreased (0.280819 --> 0.280785).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 4.231395721435547
Epoch: 76, Steps: 63 | Train Loss: 0.5075995 Vali Loss: 0.2808855 Test Loss: 0.3557512
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.542179584503174
Epoch: 77, Steps: 63 | Train Loss: 0.5072934 Vali Loss: 0.2807725 Test Loss: 0.3557586
Validation loss decreased (0.280785 --> 0.280773).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.839754581451416
Epoch: 78, Steps: 63 | Train Loss: 0.5058926 Vali Loss: 0.2808177 Test Loss: 0.3557515
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.3736398220062256
Epoch: 79, Steps: 63 | Train Loss: 0.5073590 Vali Loss: 0.2807918 Test Loss: 0.3557479
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.851652145385742
Epoch: 80, Steps: 63 | Train Loss: 0.5067360 Vali Loss: 0.2808504 Test Loss: 0.3557482
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.3097329139709473
Epoch: 81, Steps: 63 | Train Loss: 0.5078746 Vali Loss: 0.2807820 Test Loss: 0.3557410
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.539611577987671
Epoch: 82, Steps: 63 | Train Loss: 0.5076014 Vali Loss: 0.2808385 Test Loss: 0.3557379
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 3.236572742462158
Epoch: 83, Steps: 63 | Train Loss: 0.5078854 Vali Loss: 0.2808246 Test Loss: 0.3557347
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.200162172317505
Epoch: 84, Steps: 63 | Train Loss: 0.5075205 Vali Loss: 0.2808282 Test Loss: 0.3557312
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.5234696865081787
Epoch: 85, Steps: 63 | Train Loss: 0.5079762 Vali Loss: 0.2807605 Test Loss: 0.3557349
Validation loss decreased (0.280773 --> 0.280761).  Saving model ...
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 3.1160528659820557
Epoch: 86, Steps: 63 | Train Loss: 0.5073077 Vali Loss: 0.2807764 Test Loss: 0.3557307
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.3430633544921875
Epoch: 87, Steps: 63 | Train Loss: 0.5065499 Vali Loss: 0.2808042 Test Loss: 0.3557279
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 3.5804598331451416
Epoch: 88, Steps: 63 | Train Loss: 0.5079940 Vali Loss: 0.2808086 Test Loss: 0.3557213
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.5126702785491943
Epoch: 89, Steps: 63 | Train Loss: 0.5073603 Vali Loss: 0.2807461 Test Loss: 0.3557243
Validation loss decreased (0.280761 --> 0.280746).  Saving model ...
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.8032333850860596
Epoch: 90, Steps: 63 | Train Loss: 0.5073719 Vali Loss: 0.2807572 Test Loss: 0.3557224
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.521937608718872
Epoch: 91, Steps: 63 | Train Loss: 0.5074536 Vali Loss: 0.2808091 Test Loss: 0.3557195
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 3.6494836807250977
Epoch: 92, Steps: 63 | Train Loss: 0.5075507 Vali Loss: 0.2806729 Test Loss: 0.3557219
Validation loss decreased (0.280746 --> 0.280673).  Saving model ...
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.0965688228607178
Epoch: 93, Steps: 63 | Train Loss: 0.5074508 Vali Loss: 0.2807471 Test Loss: 0.3557149
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.738551378250122
Epoch: 94, Steps: 63 | Train Loss: 0.5073068 Vali Loss: 0.2807251 Test Loss: 0.3557150
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 3.622156858444214
Epoch: 95, Steps: 63 | Train Loss: 0.5070362 Vali Loss: 0.2807654 Test Loss: 0.3557165
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.3062777519226074
Epoch: 96, Steps: 63 | Train Loss: 0.5075549 Vali Loss: 0.2807550 Test Loss: 0.3557106
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.0584161281585693
Epoch: 97, Steps: 63 | Train Loss: 0.5078660 Vali Loss: 0.2807099 Test Loss: 0.3557130
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 5.086711645126343
Epoch: 98, Steps: 63 | Train Loss: 0.5076750 Vali Loss: 0.2807605 Test Loss: 0.3557113
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.506281614303589
Epoch: 99, Steps: 63 | Train Loss: 0.5067571 Vali Loss: 0.2807222 Test Loss: 0.3557070
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 4.0303380489349365
Epoch: 100, Steps: 63 | Train Loss: 0.5067012 Vali Loss: 0.2806976 Test Loss: 0.3557066
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_360_192_FITS_ETTh2_ftM_sl360_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.33608466386795044, mae:0.3769241273403168, rse:0.46490606665611267, corr:[0.26436308 0.27024248 0.26774943 0.26565474 0.26560047 0.2654849
 0.2640141  0.2623808  0.26136616 0.2602004  0.2585473  0.25679561
 0.25557306 0.25462696 0.25371343 0.25300688 0.2525408  0.25200975
 0.25089276 0.24941105 0.24801768 0.24682297 0.24522114 0.24277827
 0.24018462 0.23819563 0.2368062  0.23536965 0.23375182 0.23227127
 0.23109065 0.2296647  0.22783859 0.22593334 0.22468759 0.2239508
 0.22285976 0.2212425  0.21987168 0.21924983 0.21915424 0.21873856
 0.21752237 0.21608274 0.21511637 0.21447143 0.21327549 0.21111788
 0.2087023  0.20695466 0.20585272 0.20465097 0.20293741 0.2009727
 0.19910003 0.19741464 0.19591081 0.19430378 0.1931906  0.1926872
 0.19253595 0.19208792 0.19161627 0.19131762 0.19107729 0.19064182
 0.18970934 0.18871792 0.18822636 0.18806477 0.18759595 0.18650879
 0.1851206  0.18415482 0.18360229 0.18291093 0.18208957 0.18132907
 0.18078813 0.1803431  0.1800127  0.1793402  0.17890216 0.1788953
 0.1791345  0.17897047 0.17848273 0.17811851 0.17798425 0.17786802
 0.17725259 0.17635722 0.17610025 0.17635529 0.17647485 0.17574073
 0.174382   0.17308728 0.1723795  0.17178148 0.17091802 0.16997875
 0.16948515 0.16914777 0.1688727  0.1683635  0.16782714 0.16793145
 0.16803946 0.16774245 0.16678149 0.16601726 0.16567665 0.16582009
 0.16577697 0.16497417 0.16404575 0.16325809 0.16253434 0.161328
 0.15969104 0.15800829 0.15713318 0.15679495 0.15600686 0.15467308
 0.15358052 0.15314326 0.15291543 0.15218124 0.15121178 0.15055414
 0.15062359 0.15068242 0.15007608 0.14918055 0.14872941 0.14900163
 0.1490148  0.14861399 0.14804369 0.14763318 0.14738572 0.14672473
 0.14504446 0.14282188 0.14127669 0.14094093 0.14099845 0.1402829
 0.13944173 0.1387977  0.138906   0.13901055 0.13835512 0.1375573
 0.13756594 0.13844213 0.13863024 0.13828166 0.13789779 0.13817938
 0.13874511 0.13855775 0.1378576  0.13745447 0.13784507 0.13761139
 0.13619736 0.13403453 0.13246305 0.1320722  0.13186042 0.13042922
 0.12819336 0.126566   0.12598816 0.12542081 0.1242072  0.12290522
 0.12287898 0.12335458 0.12250122 0.12066676 0.11956602 0.11962465
 0.11912064 0.11565263 0.11212034 0.1132142  0.11622878 0.1076448 ]
