Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=196, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=514, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=196, out_features=248, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  43552768.0
params:  48856.0
Trainable parameters:  48856
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.542609453201294
Epoch: 1, Steps: 60 | Train Loss: 0.6955974 Vali Loss: 0.3725170 Test Loss: 0.3860604
Validation loss decreased (inf --> 0.372517).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.5314080715179443
Epoch: 2, Steps: 60 | Train Loss: 0.5736405 Vali Loss: 0.3294178 Test Loss: 0.3676430
Validation loss decreased (0.372517 --> 0.329418).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.5388264656066895
Epoch: 3, Steps: 60 | Train Loss: 0.5491613 Vali Loss: 0.3149129 Test Loss: 0.3618506
Validation loss decreased (0.329418 --> 0.314913).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.403343677520752
Epoch: 4, Steps: 60 | Train Loss: 0.5403667 Vali Loss: 0.3080841 Test Loss: 0.3589315
Validation loss decreased (0.314913 --> 0.308084).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.4411332607269287
Epoch: 5, Steps: 60 | Train Loss: 0.5335164 Vali Loss: 0.3036487 Test Loss: 0.3573122
Validation loss decreased (0.308084 --> 0.303649).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.404675006866455
Epoch: 6, Steps: 60 | Train Loss: 0.5301860 Vali Loss: 0.2995537 Test Loss: 0.3567124
Validation loss decreased (0.303649 --> 0.299554).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.405085563659668
Epoch: 7, Steps: 60 | Train Loss: 0.5258340 Vali Loss: 0.2978610 Test Loss: 0.3558181
Validation loss decreased (0.299554 --> 0.297861).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.4179234504699707
Epoch: 8, Steps: 60 | Train Loss: 0.5248650 Vali Loss: 0.2955465 Test Loss: 0.3555767
Validation loss decreased (0.297861 --> 0.295547).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.3818345069885254
Epoch: 9, Steps: 60 | Train Loss: 0.5222064 Vali Loss: 0.2942955 Test Loss: 0.3550602
Validation loss decreased (0.295547 --> 0.294296).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.360360860824585
Epoch: 10, Steps: 60 | Train Loss: 0.5199621 Vali Loss: 0.2929455 Test Loss: 0.3547195
Validation loss decreased (0.294296 --> 0.292946).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.3575515747070312
Epoch: 11, Steps: 60 | Train Loss: 0.5174183 Vali Loss: 0.2917179 Test Loss: 0.3543537
Validation loss decreased (0.292946 --> 0.291718).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.418764352798462
Epoch: 12, Steps: 60 | Train Loss: 0.5175888 Vali Loss: 0.2908764 Test Loss: 0.3540480
Validation loss decreased (0.291718 --> 0.290876).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.387274980545044
Epoch: 13, Steps: 60 | Train Loss: 0.5178650 Vali Loss: 0.2899487 Test Loss: 0.3540865
Validation loss decreased (0.290876 --> 0.289949).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.3771491050720215
Epoch: 14, Steps: 60 | Train Loss: 0.5172351 Vali Loss: 0.2894054 Test Loss: 0.3538943
Validation loss decreased (0.289949 --> 0.289405).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.4231886863708496
Epoch: 15, Steps: 60 | Train Loss: 0.5165703 Vali Loss: 0.2883901 Test Loss: 0.3536846
Validation loss decreased (0.289405 --> 0.288390).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.3175392150878906
Epoch: 16, Steps: 60 | Train Loss: 0.5158074 Vali Loss: 0.2882590 Test Loss: 0.3535523
Validation loss decreased (0.288390 --> 0.288259).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.4180066585540771
Epoch: 17, Steps: 60 | Train Loss: 0.5153127 Vali Loss: 0.2879043 Test Loss: 0.3533872
Validation loss decreased (0.288259 --> 0.287904).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.4152023792266846
Epoch: 18, Steps: 60 | Train Loss: 0.5142519 Vali Loss: 0.2875516 Test Loss: 0.3532577
Validation loss decreased (0.287904 --> 0.287552).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.340763807296753
Epoch: 19, Steps: 60 | Train Loss: 0.5143771 Vali Loss: 0.2871085 Test Loss: 0.3530822
Validation loss decreased (0.287552 --> 0.287108).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 1.327643632888794
Epoch: 20, Steps: 60 | Train Loss: 0.5139669 Vali Loss: 0.2867879 Test Loss: 0.3530522
Validation loss decreased (0.287108 --> 0.286788).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.4248759746551514
Epoch: 21, Steps: 60 | Train Loss: 0.5114277 Vali Loss: 0.2864071 Test Loss: 0.3530002
Validation loss decreased (0.286788 --> 0.286407).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.3930268287658691
Epoch: 22, Steps: 60 | Train Loss: 0.5142822 Vali Loss: 0.2860294 Test Loss: 0.3529344
Validation loss decreased (0.286407 --> 0.286029).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.3655812740325928
Epoch: 23, Steps: 60 | Train Loss: 0.5118976 Vali Loss: 0.2857237 Test Loss: 0.3528799
Validation loss decreased (0.286029 --> 0.285724).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.3248779773712158
Epoch: 24, Steps: 60 | Train Loss: 0.5128021 Vali Loss: 0.2857144 Test Loss: 0.3528653
Validation loss decreased (0.285724 --> 0.285714).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.4069087505340576
Epoch: 25, Steps: 60 | Train Loss: 0.5097139 Vali Loss: 0.2851533 Test Loss: 0.3528130
Validation loss decreased (0.285714 --> 0.285153).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.328521490097046
Epoch: 26, Steps: 60 | Train Loss: 0.5127079 Vali Loss: 0.2851088 Test Loss: 0.3526109
Validation loss decreased (0.285153 --> 0.285109).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.3514888286590576
Epoch: 27, Steps: 60 | Train Loss: 0.5118178 Vali Loss: 0.2849727 Test Loss: 0.3525722
Validation loss decreased (0.285109 --> 0.284973).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.3673887252807617
Epoch: 28, Steps: 60 | Train Loss: 0.5126657 Vali Loss: 0.2846787 Test Loss: 0.3525227
Validation loss decreased (0.284973 --> 0.284679).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.3731663227081299
Epoch: 29, Steps: 60 | Train Loss: 0.5118242 Vali Loss: 0.2846835 Test Loss: 0.3525226
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.371201515197754
Epoch: 30, Steps: 60 | Train Loss: 0.5092376 Vali Loss: 0.2845420 Test Loss: 0.3524544
Validation loss decreased (0.284679 --> 0.284542).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.421854019165039
Epoch: 31, Steps: 60 | Train Loss: 0.5119868 Vali Loss: 0.2844067 Test Loss: 0.3524330
Validation loss decreased (0.284542 --> 0.284407).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.403791904449463
Epoch: 32, Steps: 60 | Train Loss: 0.5100065 Vali Loss: 0.2842057 Test Loss: 0.3523535
Validation loss decreased (0.284407 --> 0.284206).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.4495916366577148
Epoch: 33, Steps: 60 | Train Loss: 0.5119371 Vali Loss: 0.2842046 Test Loss: 0.3523108
Validation loss decreased (0.284206 --> 0.284205).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.4268226623535156
Epoch: 34, Steps: 60 | Train Loss: 0.5102340 Vali Loss: 0.2840801 Test Loss: 0.3522690
Validation loss decreased (0.284205 --> 0.284080).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.3145592212677002
Epoch: 35, Steps: 60 | Train Loss: 0.5099269 Vali Loss: 0.2838807 Test Loss: 0.3523368
Validation loss decreased (0.284080 --> 0.283881).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.3491308689117432
Epoch: 36, Steps: 60 | Train Loss: 0.5100303 Vali Loss: 0.2838215 Test Loss: 0.3523122
Validation loss decreased (0.283881 --> 0.283821).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.402195930480957
Epoch: 37, Steps: 60 | Train Loss: 0.5110596 Vali Loss: 0.2836907 Test Loss: 0.3522527
Validation loss decreased (0.283821 --> 0.283691).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.4236998558044434
Epoch: 38, Steps: 60 | Train Loss: 0.5107634 Vali Loss: 0.2836714 Test Loss: 0.3522368
Validation loss decreased (0.283691 --> 0.283671).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.4460468292236328
Epoch: 39, Steps: 60 | Train Loss: 0.5107242 Vali Loss: 0.2835881 Test Loss: 0.3521939
Validation loss decreased (0.283671 --> 0.283588).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.435870885848999
Epoch: 40, Steps: 60 | Train Loss: 0.5097585 Vali Loss: 0.2835840 Test Loss: 0.3521778
Validation loss decreased (0.283588 --> 0.283584).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.4381623268127441
Epoch: 41, Steps: 60 | Train Loss: 0.5105495 Vali Loss: 0.2834831 Test Loss: 0.3521603
Validation loss decreased (0.283584 --> 0.283483).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.3858726024627686
Epoch: 42, Steps: 60 | Train Loss: 0.5108724 Vali Loss: 0.2834683 Test Loss: 0.3521534
Validation loss decreased (0.283483 --> 0.283468).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.416743516921997
Epoch: 43, Steps: 60 | Train Loss: 0.5068439 Vali Loss: 0.2832291 Test Loss: 0.3522120
Validation loss decreased (0.283468 --> 0.283229).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.3953979015350342
Epoch: 44, Steps: 60 | Train Loss: 0.5099070 Vali Loss: 0.2833670 Test Loss: 0.3521282
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.348858118057251
Epoch: 45, Steps: 60 | Train Loss: 0.5078578 Vali Loss: 0.2831349 Test Loss: 0.3521609
Validation loss decreased (0.283229 --> 0.283135).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.4062919616699219
Epoch: 46, Steps: 60 | Train Loss: 0.5089718 Vali Loss: 0.2831404 Test Loss: 0.3521226
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.36639404296875
Epoch: 47, Steps: 60 | Train Loss: 0.5107827 Vali Loss: 0.2831276 Test Loss: 0.3521163
Validation loss decreased (0.283135 --> 0.283128).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.4152967929840088
Epoch: 48, Steps: 60 | Train Loss: 0.5089194 Vali Loss: 0.2830503 Test Loss: 0.3521206
Validation loss decreased (0.283128 --> 0.283050).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.4242830276489258
Epoch: 49, Steps: 60 | Train Loss: 0.5100726 Vali Loss: 0.2830198 Test Loss: 0.3521045
Validation loss decreased (0.283050 --> 0.283020).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.3804609775543213
Epoch: 50, Steps: 60 | Train Loss: 0.5086364 Vali Loss: 0.2830049 Test Loss: 0.3520868
Validation loss decreased (0.283020 --> 0.283005).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.390932559967041
Epoch: 51, Steps: 60 | Train Loss: 0.5082393 Vali Loss: 0.2829338 Test Loss: 0.3520969
Validation loss decreased (0.283005 --> 0.282934).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.3113925457000732
Epoch: 52, Steps: 60 | Train Loss: 0.5087338 Vali Loss: 0.2829891 Test Loss: 0.3520685
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 1.42449951171875
Epoch: 53, Steps: 60 | Train Loss: 0.5087235 Vali Loss: 0.2829351 Test Loss: 0.3520613
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.409496784210205
Epoch: 54, Steps: 60 | Train Loss: 0.5082399 Vali Loss: 0.2828967 Test Loss: 0.3520670
Validation loss decreased (0.282934 --> 0.282897).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.365861177444458
Epoch: 55, Steps: 60 | Train Loss: 0.5101642 Vali Loss: 0.2828607 Test Loss: 0.3520548
Validation loss decreased (0.282897 --> 0.282861).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.5368340015411377
Epoch: 56, Steps: 60 | Train Loss: 0.5104007 Vali Loss: 0.2828654 Test Loss: 0.3520445
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 1.3474299907684326
Epoch: 57, Steps: 60 | Train Loss: 0.5084815 Vali Loss: 0.2828279 Test Loss: 0.3520597
Validation loss decreased (0.282861 --> 0.282828).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.3981037139892578
Epoch: 58, Steps: 60 | Train Loss: 0.5089744 Vali Loss: 0.2827671 Test Loss: 0.3520463
Validation loss decreased (0.282828 --> 0.282767).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 1.3723561763763428
Epoch: 59, Steps: 60 | Train Loss: 0.5088279 Vali Loss: 0.2828034 Test Loss: 0.3520336
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 1.3623690605163574
Epoch: 60, Steps: 60 | Train Loss: 0.5079976 Vali Loss: 0.2827719 Test Loss: 0.3520350
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.4069180488586426
Epoch: 61, Steps: 60 | Train Loss: 0.5102512 Vali Loss: 0.2827034 Test Loss: 0.3520329
Validation loss decreased (0.282767 --> 0.282703).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 1.432321548461914
Epoch: 62, Steps: 60 | Train Loss: 0.5077696 Vali Loss: 0.2826641 Test Loss: 0.3520257
Validation loss decreased (0.282703 --> 0.282664).  Saving model ...
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.4112727642059326
Epoch: 63, Steps: 60 | Train Loss: 0.5088543 Vali Loss: 0.2826230 Test Loss: 0.3520208
Validation loss decreased (0.282664 --> 0.282623).  Saving model ...
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.437891960144043
Epoch: 64, Steps: 60 | Train Loss: 0.5095553 Vali Loss: 0.2826751 Test Loss: 0.3520060
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 1.3000962734222412
Epoch: 65, Steps: 60 | Train Loss: 0.5098143 Vali Loss: 0.2824022 Test Loss: 0.3520009
Validation loss decreased (0.282623 --> 0.282402).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.4188246726989746
Epoch: 66, Steps: 60 | Train Loss: 0.5094096 Vali Loss: 0.2825985 Test Loss: 0.3519993
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.403815507888794
Epoch: 67, Steps: 60 | Train Loss: 0.5081522 Vali Loss: 0.2826076 Test Loss: 0.3519920
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 1.3944153785705566
Epoch: 68, Steps: 60 | Train Loss: 0.5087792 Vali Loss: 0.2825447 Test Loss: 0.3519965
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.489013433456421
Epoch: 69, Steps: 60 | Train Loss: 0.5071545 Vali Loss: 0.2825423 Test Loss: 0.3519894
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 1.5170600414276123
Epoch: 70, Steps: 60 | Train Loss: 0.5103017 Vali Loss: 0.2825476 Test Loss: 0.3519816
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 1.3961918354034424
Epoch: 71, Steps: 60 | Train Loss: 0.5079393 Vali Loss: 0.2825367 Test Loss: 0.3519796
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.4087064266204834
Epoch: 72, Steps: 60 | Train Loss: 0.5096224 Vali Loss: 0.2824606 Test Loss: 0.3519832
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.3996362686157227
Epoch: 73, Steps: 60 | Train Loss: 0.5074015 Vali Loss: 0.2824900 Test Loss: 0.3519764
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 1.3698134422302246
Epoch: 74, Steps: 60 | Train Loss: 0.5092138 Vali Loss: 0.2824947 Test Loss: 0.3519700
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 1.3539972305297852
Epoch: 75, Steps: 60 | Train Loss: 0.5056830 Vali Loss: 0.2824689 Test Loss: 0.3519725
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 1.3806357383728027
Epoch: 76, Steps: 60 | Train Loss: 0.5088878 Vali Loss: 0.2825089 Test Loss: 0.3519698
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.3777780532836914
Epoch: 77, Steps: 60 | Train Loss: 0.5081529 Vali Loss: 0.2825095 Test Loss: 0.3519637
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 1.399850845336914
Epoch: 78, Steps: 60 | Train Loss: 0.5094322 Vali Loss: 0.2822125 Test Loss: 0.3519624
Validation loss decreased (0.282402 --> 0.282212).  Saving model ...
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 1.4310758113861084
Epoch: 79, Steps: 60 | Train Loss: 0.5092164 Vali Loss: 0.2824709 Test Loss: 0.3519494
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.4229998588562012
Epoch: 80, Steps: 60 | Train Loss: 0.5092445 Vali Loss: 0.2824261 Test Loss: 0.3519486
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 1.4017524719238281
Epoch: 81, Steps: 60 | Train Loss: 0.5086147 Vali Loss: 0.2824601 Test Loss: 0.3519497
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 1.3949942588806152
Epoch: 82, Steps: 60 | Train Loss: 0.5099187 Vali Loss: 0.2824846 Test Loss: 0.3519466
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.3663833141326904
Epoch: 83, Steps: 60 | Train Loss: 0.5086401 Vali Loss: 0.2824621 Test Loss: 0.3519473
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 1.3812401294708252
Epoch: 84, Steps: 60 | Train Loss: 0.5071056 Vali Loss: 0.2824312 Test Loss: 0.3519439
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.4257538318634033
Epoch: 85, Steps: 60 | Train Loss: 0.5094968 Vali Loss: 0.2823466 Test Loss: 0.3519431
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 1.3867037296295166
Epoch: 86, Steps: 60 | Train Loss: 0.5093027 Vali Loss: 0.2824203 Test Loss: 0.3519461
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 1.3014161586761475
Epoch: 87, Steps: 60 | Train Loss: 0.5087333 Vali Loss: 0.2823057 Test Loss: 0.3519396
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 1.3760337829589844
Epoch: 88, Steps: 60 | Train Loss: 0.5084403 Vali Loss: 0.2824390 Test Loss: 0.3519396
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 1.4783952236175537
Epoch: 89, Steps: 60 | Train Loss: 0.5086836 Vali Loss: 0.2823928 Test Loss: 0.3519371
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 1.3626267910003662
Epoch: 90, Steps: 60 | Train Loss: 0.5092852 Vali Loss: 0.2824149 Test Loss: 0.3519365
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 1.402536153793335
Epoch: 91, Steps: 60 | Train Loss: 0.5089707 Vali Loss: 0.2824104 Test Loss: 0.3519355
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 1.3825750350952148
Epoch: 92, Steps: 60 | Train Loss: 0.5068259 Vali Loss: 0.2823631 Test Loss: 0.3519368
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.4399983882904053
Epoch: 93, Steps: 60 | Train Loss: 0.5074988 Vali Loss: 0.2824098 Test Loss: 0.3519332
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 1.3641102313995361
Epoch: 94, Steps: 60 | Train Loss: 0.5086359 Vali Loss: 0.2824273 Test Loss: 0.3519334
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 1.430363655090332
Epoch: 95, Steps: 60 | Train Loss: 0.5079118 Vali Loss: 0.2823620 Test Loss: 0.3519360
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.3851478099822998
Epoch: 96, Steps: 60 | Train Loss: 0.5097246 Vali Loss: 0.2823825 Test Loss: 0.3519312
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 1.4376745223999023
Epoch: 97, Steps: 60 | Train Loss: 0.5094537 Vali Loss: 0.2823866 Test Loss: 0.3519306
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 1.3191652297973633
Epoch: 98, Steps: 60 | Train Loss: 0.5072336 Vali Loss: 0.2823533 Test Loss: 0.3519334
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.33142682909965515, mae:0.37445929646492004, rse:0.46167322993278503, corr:[0.2630003  0.26738477 0.26578474 0.26712814 0.26685938 0.26546067
 0.2654887  0.26541123 0.26398516 0.2627266  0.26200572 0.2608173
 0.25926626 0.2580578  0.25752863 0.25721583 0.25674647 0.2561529
 0.25527954 0.25400043 0.2526331  0.25159767 0.2504327  0.24861364
 0.24649505 0.24487177 0.24354611 0.24186186 0.2402286  0.23910782
 0.23805934 0.23642011 0.23511623 0.23426008 0.23305623 0.2316009
 0.23074184 0.23017517 0.2289488  0.2277481  0.22738676 0.22702816
 0.22602473 0.22504589 0.22436878 0.22335583 0.22186184 0.22034086
 0.2188091  0.21696554 0.2151633  0.21381505 0.21249744 0.2105012
 0.20838284 0.20694284 0.20531091 0.20333801 0.20213708 0.20136267
 0.20041013 0.19980961 0.20007464 0.20009388 0.19923982 0.19878073
 0.19855379 0.19778799 0.19694017 0.19657557 0.1959718  0.19464456
 0.19343959 0.19294341 0.19207652 0.1903332  0.18935293 0.18925779
 0.18868089 0.18749155 0.18722744 0.18715452 0.18644899 0.18585134
 0.18587375 0.18589486 0.18541063 0.18508056 0.18481244 0.18426575
 0.1835819  0.18323259 0.1832068  0.18269975 0.18224274 0.18212427
 0.18151501 0.18028057 0.17938294 0.17883709 0.17779289 0.17635526
 0.17571682 0.17535813 0.17466024 0.1738579  0.17368518 0.17376174
 0.17294668 0.17208034 0.17157361 0.17113969 0.17033136 0.17000581
 0.16976488 0.16897184 0.16809477 0.16726942 0.16624063 0.16455092
 0.1630303  0.16183716 0.16071598 0.159465   0.15845281 0.1575713
 0.1565265  0.15550233 0.1550203  0.1543212  0.15320288 0.15217784
 0.15175737 0.15120015 0.15046696 0.15019302 0.1499129  0.14917086
 0.14832397 0.14781417 0.14738698 0.14674957 0.14614505 0.14499044
 0.14291419 0.1411439  0.14013728 0.13886568 0.13718177 0.13634606
 0.13638929 0.13537917 0.13432662 0.1342058  0.13403113 0.13280466
 0.13215354 0.13265586 0.1329791  0.13274588 0.13242587 0.1326208
 0.13218576 0.13140437 0.13155122 0.13175814 0.13130479 0.13044621
 0.12970269 0.12802106 0.12659593 0.12625639 0.12596726 0.12374817
 0.1219431  0.12140335 0.12025753 0.11857879 0.1178157  0.11822321
 0.11720393 0.1164631  0.11805589 0.11887078 0.11710737 0.11744483
 0.11868002 0.11677992 0.1178311  0.12197961 0.12088479 0.12482184]
