Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_192_FITS_ETTh2_ftM_sl90_ll48_pl192_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8359
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=34, out_features=106, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3229184.0
params:  3710.0
Trainable parameters:  3710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.8462235927581787
Epoch: 1, Steps: 65 | Train Loss: 0.7225557 Vali Loss: 0.3527668 Test Loss: 0.4928523
Validation loss decreased (inf --> 0.352767).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.5684313774108887
Epoch: 2, Steps: 65 | Train Loss: 0.6354541 Vali Loss: 0.3200568 Test Loss: 0.4487780
Validation loss decreased (0.352767 --> 0.320057).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.5145034790039062
Epoch: 3, Steps: 65 | Train Loss: 0.5967020 Vali Loss: 0.3044857 Test Loss: 0.4283936
Validation loss decreased (0.320057 --> 0.304486).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.3124916553497314
Epoch: 4, Steps: 65 | Train Loss: 0.5780120 Vali Loss: 0.2960097 Test Loss: 0.4176717
Validation loss decreased (0.304486 --> 0.296010).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.9188852310180664
Epoch: 5, Steps: 65 | Train Loss: 0.5691687 Vali Loss: 0.2909744 Test Loss: 0.4115187
Validation loss decreased (0.296010 --> 0.290974).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.1622302532196045
Epoch: 6, Steps: 65 | Train Loss: 0.5612784 Vali Loss: 0.2878054 Test Loss: 0.4077073
Validation loss decreased (0.290974 --> 0.287805).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.7100517749786377
Epoch: 7, Steps: 65 | Train Loss: 0.5603758 Vali Loss: 0.2856763 Test Loss: 0.4053006
Validation loss decreased (0.287805 --> 0.285676).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.4304139614105225
Epoch: 8, Steps: 65 | Train Loss: 0.5562753 Vali Loss: 0.2840978 Test Loss: 0.4035319
Validation loss decreased (0.285676 --> 0.284098).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.465825319290161
Epoch: 9, Steps: 65 | Train Loss: 0.5539634 Vali Loss: 0.2828244 Test Loss: 0.4022168
Validation loss decreased (0.284098 --> 0.282824).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.6098265647888184
Epoch: 10, Steps: 65 | Train Loss: 0.5515671 Vali Loss: 0.2819637 Test Loss: 0.4013355
Validation loss decreased (0.282824 --> 0.281964).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.544247627258301
Epoch: 11, Steps: 65 | Train Loss: 0.5495045 Vali Loss: 0.2811903 Test Loss: 0.4006070
Validation loss decreased (0.281964 --> 0.281190).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.331552028656006
Epoch: 12, Steps: 65 | Train Loss: 0.5482459 Vali Loss: 0.2805827 Test Loss: 0.3999892
Validation loss decreased (0.281190 --> 0.280583).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.537708282470703
Epoch: 13, Steps: 65 | Train Loss: 0.5487368 Vali Loss: 0.2799995 Test Loss: 0.3994764
Validation loss decreased (0.280583 --> 0.279999).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 4.5713300704956055
Epoch: 14, Steps: 65 | Train Loss: 0.5482746 Vali Loss: 0.2795988 Test Loss: 0.3991280
Validation loss decreased (0.279999 --> 0.279599).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.116284132003784
Epoch: 15, Steps: 65 | Train Loss: 0.5465092 Vali Loss: 0.2789314 Test Loss: 0.3988030
Validation loss decreased (0.279599 --> 0.278931).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.19623064994812
Epoch: 16, Steps: 65 | Train Loss: 0.5448478 Vali Loss: 0.2788409 Test Loss: 0.3984993
Validation loss decreased (0.278931 --> 0.278841).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.839172601699829
Epoch: 17, Steps: 65 | Train Loss: 0.5452169 Vali Loss: 0.2785056 Test Loss: 0.3982858
Validation loss decreased (0.278841 --> 0.278506).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 5.271730422973633
Epoch: 18, Steps: 65 | Train Loss: 0.5458946 Vali Loss: 0.2781813 Test Loss: 0.3980465
Validation loss decreased (0.278506 --> 0.278181).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.419912338256836
Epoch: 19, Steps: 65 | Train Loss: 0.5438111 Vali Loss: 0.2780062 Test Loss: 0.3978893
Validation loss decreased (0.278181 --> 0.278006).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.4416911602020264
Epoch: 20, Steps: 65 | Train Loss: 0.5434509 Vali Loss: 0.2776259 Test Loss: 0.3977162
Validation loss decreased (0.278006 --> 0.277626).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.593651533126831
Epoch: 21, Steps: 65 | Train Loss: 0.5437497 Vali Loss: 0.2775910 Test Loss: 0.3975919
Validation loss decreased (0.277626 --> 0.277591).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.025951385498047
Epoch: 22, Steps: 65 | Train Loss: 0.5438986 Vali Loss: 0.2774382 Test Loss: 0.3975165
Validation loss decreased (0.277591 --> 0.277438).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.65730619430542
Epoch: 23, Steps: 65 | Train Loss: 0.5436803 Vali Loss: 0.2772569 Test Loss: 0.3973777
Validation loss decreased (0.277438 --> 0.277257).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.191716194152832
Epoch: 24, Steps: 65 | Train Loss: 0.5433024 Vali Loss: 0.2767885 Test Loss: 0.3972993
Validation loss decreased (0.277257 --> 0.276788).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.1444759368896484
Epoch: 25, Steps: 65 | Train Loss: 0.5436440 Vali Loss: 0.2770039 Test Loss: 0.3972273
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 3.4923646450042725
Epoch: 26, Steps: 65 | Train Loss: 0.5420091 Vali Loss: 0.2768838 Test Loss: 0.3971404
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.936556816101074
Epoch: 27, Steps: 65 | Train Loss: 0.5410493 Vali Loss: 0.2766444 Test Loss: 0.3970910
Validation loss decreased (0.276788 --> 0.276644).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.0652248859405518
Epoch: 28, Steps: 65 | Train Loss: 0.5427804 Vali Loss: 0.2766421 Test Loss: 0.3970262
Validation loss decreased (0.276644 --> 0.276642).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.0512027740478516
Epoch: 29, Steps: 65 | Train Loss: 0.5413093 Vali Loss: 0.2762223 Test Loss: 0.3969914
Validation loss decreased (0.276642 --> 0.276222).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.3651106357574463
Epoch: 30, Steps: 65 | Train Loss: 0.5417708 Vali Loss: 0.2764891 Test Loss: 0.3969489
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.1454575061798096
Epoch: 31, Steps: 65 | Train Loss: 0.5413151 Vali Loss: 0.2763413 Test Loss: 0.3968748
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.865111827850342
Epoch: 32, Steps: 65 | Train Loss: 0.5406050 Vali Loss: 0.2762073 Test Loss: 0.3968435
Validation loss decreased (0.276222 --> 0.276207).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.834672212600708
Epoch: 33, Steps: 65 | Train Loss: 0.5414646 Vali Loss: 0.2762312 Test Loss: 0.3968368
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.13891077041626
Epoch: 34, Steps: 65 | Train Loss: 0.5411257 Vali Loss: 0.2761777 Test Loss: 0.3968046
Validation loss decreased (0.276207 --> 0.276178).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.404496192932129
Epoch: 35, Steps: 65 | Train Loss: 0.5408269 Vali Loss: 0.2761189 Test Loss: 0.3967505
Validation loss decreased (0.276178 --> 0.276119).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 4.143805027008057
Epoch: 36, Steps: 65 | Train Loss: 0.5416923 Vali Loss: 0.2759801 Test Loss: 0.3967201
Validation loss decreased (0.276119 --> 0.275980).  Saving model ...
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.648496150970459
Epoch: 37, Steps: 65 | Train Loss: 0.5411523 Vali Loss: 0.2759269 Test Loss: 0.3967007
Validation loss decreased (0.275980 --> 0.275927).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.9421303272247314
Epoch: 38, Steps: 65 | Train Loss: 0.5401043 Vali Loss: 0.2759016 Test Loss: 0.3966645
Validation loss decreased (0.275927 --> 0.275902).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.9624695777893066
Epoch: 39, Steps: 65 | Train Loss: 0.5398281 Vali Loss: 0.2758380 Test Loss: 0.3966509
Validation loss decreased (0.275902 --> 0.275838).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.9673433303833008
Epoch: 40, Steps: 65 | Train Loss: 0.5411043 Vali Loss: 0.2758042 Test Loss: 0.3966354
Validation loss decreased (0.275838 --> 0.275804).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.0120365619659424
Epoch: 41, Steps: 65 | Train Loss: 0.5381232 Vali Loss: 0.2757009 Test Loss: 0.3966076
Validation loss decreased (0.275804 --> 0.275701).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.829961061477661
Epoch: 42, Steps: 65 | Train Loss: 0.5411938 Vali Loss: 0.2755826 Test Loss: 0.3965973
Validation loss decreased (0.275701 --> 0.275583).  Saving model ...
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.163703441619873
Epoch: 43, Steps: 65 | Train Loss: 0.5402850 Vali Loss: 0.2756874 Test Loss: 0.3965768
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 4.70059609413147
Epoch: 44, Steps: 65 | Train Loss: 0.5393272 Vali Loss: 0.2756620 Test Loss: 0.3965571
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.782956123352051
Epoch: 45, Steps: 65 | Train Loss: 0.5406351 Vali Loss: 0.2756185 Test Loss: 0.3965306
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.7150845527648926
Epoch: 46, Steps: 65 | Train Loss: 0.5393930 Vali Loss: 0.2755724 Test Loss: 0.3965229
Validation loss decreased (0.275583 --> 0.275572).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.3149726390838623
Epoch: 47, Steps: 65 | Train Loss: 0.5407207 Vali Loss: 0.2755243 Test Loss: 0.3965161
Validation loss decreased (0.275572 --> 0.275524).  Saving model ...
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.703639030456543
Epoch: 48, Steps: 65 | Train Loss: 0.5386762 Vali Loss: 0.2755296 Test Loss: 0.3965005
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.5626871585845947
Epoch: 49, Steps: 65 | Train Loss: 0.5387250 Vali Loss: 0.2754844 Test Loss: 0.3964863
Validation loss decreased (0.275524 --> 0.275484).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 4.550263166427612
Epoch: 50, Steps: 65 | Train Loss: 0.5402808 Vali Loss: 0.2754417 Test Loss: 0.3964758
Validation loss decreased (0.275484 --> 0.275442).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.3338723182678223
Epoch: 51, Steps: 65 | Train Loss: 0.5406746 Vali Loss: 0.2754835 Test Loss: 0.3964779
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.3879637718200684
Epoch: 52, Steps: 65 | Train Loss: 0.5398436 Vali Loss: 0.2754496 Test Loss: 0.3964673
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.46175479888916
Epoch: 53, Steps: 65 | Train Loss: 0.5397751 Vali Loss: 0.2750634 Test Loss: 0.3964674
Validation loss decreased (0.275442 --> 0.275063).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.677694320678711
Epoch: 54, Steps: 65 | Train Loss: 0.5402519 Vali Loss: 0.2753323 Test Loss: 0.3964522
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.893860101699829
Epoch: 55, Steps: 65 | Train Loss: 0.5365968 Vali Loss: 0.2753916 Test Loss: 0.3964455
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 4.152093410491943
Epoch: 56, Steps: 65 | Train Loss: 0.5403844 Vali Loss: 0.2754019 Test Loss: 0.3964445
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.7094266414642334
Epoch: 57, Steps: 65 | Train Loss: 0.5406255 Vali Loss: 0.2753337 Test Loss: 0.3964354
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.2824926376342773
Epoch: 58, Steps: 65 | Train Loss: 0.5402462 Vali Loss: 0.2753446 Test Loss: 0.3964285
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.871762990951538
Epoch: 59, Steps: 65 | Train Loss: 0.5392606 Vali Loss: 0.2753275 Test Loss: 0.3964174
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.8667666912078857
Epoch: 60, Steps: 65 | Train Loss: 0.5397263 Vali Loss: 0.2753375 Test Loss: 0.3964162
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.3097290992736816
Epoch: 61, Steps: 65 | Train Loss: 0.5403162 Vali Loss: 0.2752776 Test Loss: 0.3964163
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.026177167892456
Epoch: 62, Steps: 65 | Train Loss: 0.5399713 Vali Loss: 0.2752138 Test Loss: 0.3964134
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.840085983276367
Epoch: 63, Steps: 65 | Train Loss: 0.5389937 Vali Loss: 0.2752659 Test Loss: 0.3964095
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.473954916000366
Epoch: 64, Steps: 65 | Train Loss: 0.5391949 Vali Loss: 0.2752896 Test Loss: 0.3964036
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.6648271083831787
Epoch: 65, Steps: 65 | Train Loss: 0.5388833 Vali Loss: 0.2749389 Test Loss: 0.3964026
Validation loss decreased (0.275063 --> 0.274939).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 4.138710975646973
Epoch: 66, Steps: 65 | Train Loss: 0.5384796 Vali Loss: 0.2751257 Test Loss: 0.3963904
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.7527122497558594
Epoch: 67, Steps: 65 | Train Loss: 0.5399238 Vali Loss: 0.2751336 Test Loss: 0.3963871
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 3.931196928024292
Epoch: 68, Steps: 65 | Train Loss: 0.5394270 Vali Loss: 0.2752087 Test Loss: 0.3963870
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 4.760302543640137
Epoch: 69, Steps: 65 | Train Loss: 0.5405340 Vali Loss: 0.2751587 Test Loss: 0.3963854
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.601189136505127
Epoch: 70, Steps: 65 | Train Loss: 0.5396804 Vali Loss: 0.2751895 Test Loss: 0.3963793
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.440725564956665
Epoch: 71, Steps: 65 | Train Loss: 0.5390345 Vali Loss: 0.2751952 Test Loss: 0.3963768
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 4.243608713150024
Epoch: 72, Steps: 65 | Train Loss: 0.5401886 Vali Loss: 0.2751453 Test Loss: 0.3963733
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 1.996244192123413
Epoch: 73, Steps: 65 | Train Loss: 0.5393525 Vali Loss: 0.2750872 Test Loss: 0.3963749
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.4121594429016113
Epoch: 74, Steps: 65 | Train Loss: 0.5392385 Vali Loss: 0.2751493 Test Loss: 0.3963696
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.0127105712890625
Epoch: 75, Steps: 65 | Train Loss: 0.5399533 Vali Loss: 0.2751872 Test Loss: 0.3963712
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.3424232006073
Epoch: 76, Steps: 65 | Train Loss: 0.5398709 Vali Loss: 0.2751054 Test Loss: 0.3963687
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.1126039028167725
Epoch: 77, Steps: 65 | Train Loss: 0.5385896 Vali Loss: 0.2748052 Test Loss: 0.3963651
Validation loss decreased (0.274939 --> 0.274805).  Saving model ...
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 4.461466312408447
Epoch: 78, Steps: 65 | Train Loss: 0.5395888 Vali Loss: 0.2751583 Test Loss: 0.3963667
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 4.852790355682373
Epoch: 79, Steps: 65 | Train Loss: 0.5362900 Vali Loss: 0.2751723 Test Loss: 0.3963642
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 3.5125818252563477
Epoch: 80, Steps: 65 | Train Loss: 0.5394539 Vali Loss: 0.2750497 Test Loss: 0.3963645
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.400008201599121
Epoch: 81, Steps: 65 | Train Loss: 0.5390572 Vali Loss: 0.2751223 Test Loss: 0.3963610
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.0825846195220947
Epoch: 82, Steps: 65 | Train Loss: 0.5393835 Vali Loss: 0.2751626 Test Loss: 0.3963620
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 3.968573808670044
Epoch: 83, Steps: 65 | Train Loss: 0.5402360 Vali Loss: 0.2750948 Test Loss: 0.3963594
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.9009597301483154
Epoch: 84, Steps: 65 | Train Loss: 0.5381865 Vali Loss: 0.2750230 Test Loss: 0.3963570
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.8526408672332764
Epoch: 85, Steps: 65 | Train Loss: 0.5374052 Vali Loss: 0.2750674 Test Loss: 0.3963548
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.2345213890075684
Epoch: 86, Steps: 65 | Train Loss: 0.5401006 Vali Loss: 0.2751246 Test Loss: 0.3963559
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.753770112991333
Epoch: 87, Steps: 65 | Train Loss: 0.5398120 Vali Loss: 0.2751105 Test Loss: 0.3963546
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 3.2159276008605957
Epoch: 88, Steps: 65 | Train Loss: 0.5380469 Vali Loss: 0.2747699 Test Loss: 0.3963529
Validation loss decreased (0.274805 --> 0.274770).  Saving model ...
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 3.949552059173584
Epoch: 89, Steps: 65 | Train Loss: 0.5395820 Vali Loss: 0.2750762 Test Loss: 0.3963526
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.986159324645996
Epoch: 90, Steps: 65 | Train Loss: 0.5383723 Vali Loss: 0.2751209 Test Loss: 0.3963500
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 3.8677515983581543
Epoch: 91, Steps: 65 | Train Loss: 0.5389683 Vali Loss: 0.2750844 Test Loss: 0.3963511
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 3.7130324840545654
Epoch: 92, Steps: 65 | Train Loss: 0.5392999 Vali Loss: 0.2750039 Test Loss: 0.3963506
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.8516016006469727
Epoch: 93, Steps: 65 | Train Loss: 0.5395187 Vali Loss: 0.2750878 Test Loss: 0.3963495
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 3.3790781497955322
Epoch: 94, Steps: 65 | Train Loss: 0.5389362 Vali Loss: 0.2751007 Test Loss: 0.3963476
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 3.408358573913574
Epoch: 95, Steps: 65 | Train Loss: 0.5398540 Vali Loss: 0.2750339 Test Loss: 0.3963477
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 3.617065906524658
Epoch: 96, Steps: 65 | Train Loss: 0.5394365 Vali Loss: 0.2751113 Test Loss: 0.3963467
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 3.2907164096832275
Epoch: 97, Steps: 65 | Train Loss: 0.5391385 Vali Loss: 0.2750618 Test Loss: 0.3963467
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.871291399002075
Epoch: 98, Steps: 65 | Train Loss: 0.5401378 Vali Loss: 0.2750972 Test Loss: 0.3963457
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.2775776386260986
Epoch: 99, Steps: 65 | Train Loss: 0.5403389 Vali Loss: 0.2750795 Test Loss: 0.3963445
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.6757912635803223
Epoch: 100, Steps: 65 | Train Loss: 0.5398174 Vali Loss: 0.2750919 Test Loss: 0.3963453
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_90_192_FITS_ETTh2_ftM_sl90_ll48_pl192_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.37858089804649353, mae:0.3913610875606537, rse:0.4934239089488983, corr:[0.2674639  0.26916274 0.2676901  0.26820484 0.26543638 0.26575992
 0.26393276 0.26353928 0.26275    0.2611691  0.26068622 0.2585994
 0.25702348 0.25602493 0.25473836 0.25422144 0.25340068 0.25286883
 0.2520886  0.25068602 0.2498535  0.24873047 0.24753253 0.24543948
 0.24216014 0.23973565 0.23721965 0.23543306 0.2339586  0.23221475
 0.23138578 0.23016965 0.228867   0.22762577 0.22640964 0.2252746
 0.22350736 0.2221705  0.22157602 0.22044116 0.22003804 0.21973027
 0.21867223 0.21826936 0.21723329 0.21578136 0.215217   0.21272448
 0.20904407 0.20633943 0.20341542 0.20140336 0.19938879 0.19757463
 0.19600461 0.19365962 0.19297647 0.19143388 0.19036672 0.18964635
 0.18812342 0.18786502 0.18780644 0.18717451 0.18726248 0.1865423
 0.1857173  0.18553011 0.18446864 0.18384081 0.1832928  0.18115965
 0.1786452  0.17697057 0.17530118 0.17429489 0.17277029 0.1723812
 0.1724804  0.1713456  0.17154284 0.17096025 0.17068164 0.17086412
 0.16964932 0.1695871  0.16991855 0.16939989 0.1695633  0.16904125
 0.1681888  0.1680233  0.16720441 0.16680425 0.16659552 0.16463812
 0.16250509 0.16057745 0.15822901 0.15698858 0.15537255 0.15443443
 0.1544675  0.15350883 0.15394887 0.15362924 0.15336968 0.15364663
 0.15265235 0.15246388 0.15218258 0.15144184 0.15151669 0.15077177
 0.15012094 0.15008463 0.14889444 0.14803983 0.14720763 0.14450277
 0.14200148 0.13999204 0.13773768 0.13682541 0.13540362 0.13428558
 0.13393568 0.13334371 0.1334269  0.13289046 0.13278608 0.13269292
 0.13156976 0.13128431 0.13103384 0.13021117 0.13022012 0.12944323
 0.12859868 0.12858057 0.1275556  0.12715136 0.1267133  0.12405519
 0.12103996 0.1185839  0.11619665 0.11494692 0.11334839 0.11250193
 0.11247386 0.11168805 0.11178655 0.11147051 0.11192167 0.11215596
 0.11119928 0.1113111  0.11126039 0.11088579 0.11107294 0.11022059
 0.10973547 0.10973544 0.10877005 0.10891817 0.10911319 0.10691445
 0.10421328 0.10224272 0.10060886 0.09981803 0.09882192 0.09833132
 0.09807948 0.09774979 0.09782881 0.09727485 0.09817719 0.09777684
 0.09689502 0.09715715 0.09607437 0.09626244 0.09579466 0.09399001
 0.09496233 0.09373326 0.09320349 0.09666321 0.09658668 0.1022852 ]
