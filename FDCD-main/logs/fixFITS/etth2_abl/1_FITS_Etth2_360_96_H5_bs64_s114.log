Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_96_FITS_ETTh2_ftM_sl360_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8185
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=90, out_features=114, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9192960.0
params:  10374.0
Trainable parameters:  10374
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.329695224761963
Epoch: 1, Steps: 63 | Train Loss: 0.5992205 Vali Loss: 0.2839224 Test Loss: 0.3443493
Validation loss decreased (inf --> 0.283922).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.700516700744629
Epoch: 2, Steps: 63 | Train Loss: 0.4814200 Vali Loss: 0.2500034 Test Loss: 0.3150189
Validation loss decreased (0.283922 --> 0.250003).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.897993564605713
Epoch: 3, Steps: 63 | Train Loss: 0.4543090 Vali Loss: 0.2370839 Test Loss: 0.3045402
Validation loss decreased (0.250003 --> 0.237084).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.186253070831299
Epoch: 4, Steps: 63 | Train Loss: 0.4384129 Vali Loss: 0.2320050 Test Loss: 0.2989638
Validation loss decreased (0.237084 --> 0.232005).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.447474718093872
Epoch: 5, Steps: 63 | Train Loss: 0.4294484 Vali Loss: 0.2281762 Test Loss: 0.2947851
Validation loss decreased (0.232005 --> 0.228176).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.751720428466797
Epoch: 6, Steps: 63 | Train Loss: 0.4271394 Vali Loss: 0.2260414 Test Loss: 0.2923586
Validation loss decreased (0.228176 --> 0.226041).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.8687219619750977
Epoch: 7, Steps: 63 | Train Loss: 0.4220096 Vali Loss: 0.2247538 Test Loss: 0.2899245
Validation loss decreased (0.226041 --> 0.224754).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.340785264968872
Epoch: 8, Steps: 63 | Train Loss: 0.4219430 Vali Loss: 0.2237805 Test Loss: 0.2883660
Validation loss decreased (0.224754 --> 0.223780).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.4702703952789307
Epoch: 9, Steps: 63 | Train Loss: 0.4194490 Vali Loss: 0.2222869 Test Loss: 0.2870086
Validation loss decreased (0.223780 --> 0.222287).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.2686688899993896
Epoch: 10, Steps: 63 | Train Loss: 0.4175650 Vali Loss: 0.2204000 Test Loss: 0.2858850
Validation loss decreased (0.222287 --> 0.220400).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.790982723236084
Epoch: 11, Steps: 63 | Train Loss: 0.4112046 Vali Loss: 0.2199547 Test Loss: 0.2848646
Validation loss decreased (0.220400 --> 0.219955).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.88645339012146
Epoch: 12, Steps: 63 | Train Loss: 0.4122608 Vali Loss: 0.2182273 Test Loss: 0.2837852
Validation loss decreased (0.219955 --> 0.218227).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.30662202835083
Epoch: 13, Steps: 63 | Train Loss: 0.4107179 Vali Loss: 0.2192248 Test Loss: 0.2831594
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.8453724384307861
Epoch: 14, Steps: 63 | Train Loss: 0.4115412 Vali Loss: 0.2177575 Test Loss: 0.2825879
Validation loss decreased (0.218227 --> 0.217757).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.297839403152466
Epoch: 15, Steps: 63 | Train Loss: 0.4114517 Vali Loss: 0.2180348 Test Loss: 0.2820173
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.0141592025756836
Epoch: 16, Steps: 63 | Train Loss: 0.4107400 Vali Loss: 0.2175137 Test Loss: 0.2816379
Validation loss decreased (0.217757 --> 0.217514).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.0884270668029785
Epoch: 17, Steps: 63 | Train Loss: 0.4082872 Vali Loss: 0.2171409 Test Loss: 0.2812248
Validation loss decreased (0.217514 --> 0.217141).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.2894933223724365
Epoch: 18, Steps: 63 | Train Loss: 0.4091514 Vali Loss: 0.2160882 Test Loss: 0.2808234
Validation loss decreased (0.217141 --> 0.216088).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.250378370285034
Epoch: 19, Steps: 63 | Train Loss: 0.4066743 Vali Loss: 0.2155206 Test Loss: 0.2806089
Validation loss decreased (0.216088 --> 0.215521).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.6974236965179443
Epoch: 20, Steps: 63 | Train Loss: 0.4071735 Vali Loss: 0.2156292 Test Loss: 0.2803796
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.7869040966033936
Epoch: 21, Steps: 63 | Train Loss: 0.4068981 Vali Loss: 0.2151284 Test Loss: 0.2800218
Validation loss decreased (0.215521 --> 0.215128).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.6491425037384033
Epoch: 22, Steps: 63 | Train Loss: 0.4035037 Vali Loss: 0.2154896 Test Loss: 0.2797613
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.0680346488952637
Epoch: 23, Steps: 63 | Train Loss: 0.4039146 Vali Loss: 0.2154736 Test Loss: 0.2796186
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.0518176555633545
Epoch: 24, Steps: 63 | Train Loss: 0.4043738 Vali Loss: 0.2146416 Test Loss: 0.2794249
Validation loss decreased (0.215128 --> 0.214642).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.40022611618042
Epoch: 25, Steps: 63 | Train Loss: 0.4020097 Vali Loss: 0.2144518 Test Loss: 0.2791400
Validation loss decreased (0.214642 --> 0.214452).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.9976143836975098
Epoch: 26, Steps: 63 | Train Loss: 0.4048181 Vali Loss: 0.2144327 Test Loss: 0.2790196
Validation loss decreased (0.214452 --> 0.214433).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.2079946994781494
Epoch: 27, Steps: 63 | Train Loss: 0.4054246 Vali Loss: 0.2143870 Test Loss: 0.2788937
Validation loss decreased (0.214433 --> 0.214387).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.3259785175323486
Epoch: 28, Steps: 63 | Train Loss: 0.4008557 Vali Loss: 0.2140896 Test Loss: 0.2786905
Validation loss decreased (0.214387 --> 0.214090).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.669973611831665
Epoch: 29, Steps: 63 | Train Loss: 0.4036875 Vali Loss: 0.2145513 Test Loss: 0.2786016
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.8197357654571533
Epoch: 30, Steps: 63 | Train Loss: 0.4034202 Vali Loss: 0.2142311 Test Loss: 0.2785107
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.237455368041992
Epoch: 31, Steps: 63 | Train Loss: 0.4041752 Vali Loss: 0.2143816 Test Loss: 0.2783591
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.7325456142425537
Epoch: 32, Steps: 63 | Train Loss: 0.4023433 Vali Loss: 0.2143893 Test Loss: 0.2783014
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.414426326751709
Epoch: 33, Steps: 63 | Train Loss: 0.4042346 Vali Loss: 0.2132596 Test Loss: 0.2781210
Validation loss decreased (0.214090 --> 0.213260).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.9341015815734863
Epoch: 34, Steps: 63 | Train Loss: 0.4031338 Vali Loss: 0.2133975 Test Loss: 0.2780362
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.283029556274414
Epoch: 35, Steps: 63 | Train Loss: 0.4018311 Vali Loss: 0.2130481 Test Loss: 0.2781095
Validation loss decreased (0.213260 --> 0.213048).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.249291181564331
Epoch: 36, Steps: 63 | Train Loss: 0.4005724 Vali Loss: 0.2135125 Test Loss: 0.2779634
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.5877737998962402
Epoch: 37, Steps: 63 | Train Loss: 0.4048878 Vali Loss: 0.2135902 Test Loss: 0.2778691
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.284554958343506
Epoch: 38, Steps: 63 | Train Loss: 0.4024380 Vali Loss: 0.2135122 Test Loss: 0.2779100
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.4672141075134277
Epoch: 39, Steps: 63 | Train Loss: 0.4029869 Vali Loss: 0.2136156 Test Loss: 0.2778053
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.289034605026245
Epoch: 40, Steps: 63 | Train Loss: 0.4025167 Vali Loss: 0.2127850 Test Loss: 0.2777572
Validation loss decreased (0.213048 --> 0.212785).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.1514947414398193
Epoch: 41, Steps: 63 | Train Loss: 0.4008233 Vali Loss: 0.2128927 Test Loss: 0.2776653
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.917245388031006
Epoch: 42, Steps: 63 | Train Loss: 0.4040713 Vali Loss: 0.2132036 Test Loss: 0.2775921
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 1.981947898864746
Epoch: 43, Steps: 63 | Train Loss: 0.4019830 Vali Loss: 0.2137714 Test Loss: 0.2775639
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.0695080757141113
Epoch: 44, Steps: 63 | Train Loss: 0.4026291 Vali Loss: 0.2131806 Test Loss: 0.2775195
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.140471935272217
Epoch: 45, Steps: 63 | Train Loss: 0.4030159 Vali Loss: 0.2135199 Test Loss: 0.2774823
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.3644142150878906
Epoch: 46, Steps: 63 | Train Loss: 0.4012483 Vali Loss: 0.2140322 Test Loss: 0.2774437
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.008065938949585
Epoch: 47, Steps: 63 | Train Loss: 0.4013762 Vali Loss: 0.2137368 Test Loss: 0.2774174
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.514824867248535
Epoch: 48, Steps: 63 | Train Loss: 0.3994669 Vali Loss: 0.2127319 Test Loss: 0.2774265
Validation loss decreased (0.212785 --> 0.212732).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.0363452434539795
Epoch: 49, Steps: 63 | Train Loss: 0.4018659 Vali Loss: 0.2136579 Test Loss: 0.2773920
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.650728702545166
Epoch: 50, Steps: 63 | Train Loss: 0.4031767 Vali Loss: 0.2131805 Test Loss: 0.2773362
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.244175910949707
Epoch: 51, Steps: 63 | Train Loss: 0.4012745 Vali Loss: 0.2133418 Test Loss: 0.2772770
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.9620330333709717
Epoch: 52, Steps: 63 | Train Loss: 0.4025145 Vali Loss: 0.2126339 Test Loss: 0.2772751
Validation loss decreased (0.212732 --> 0.212634).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.572927474975586
Epoch: 53, Steps: 63 | Train Loss: 0.4022168 Vali Loss: 0.2124016 Test Loss: 0.2772701
Validation loss decreased (0.212634 --> 0.212402).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 1.8359198570251465
Epoch: 54, Steps: 63 | Train Loss: 0.4023560 Vali Loss: 0.2141421 Test Loss: 0.2772426
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.131150007247925
Epoch: 55, Steps: 63 | Train Loss: 0.4015150 Vali Loss: 0.2122220 Test Loss: 0.2772206
Validation loss decreased (0.212402 --> 0.212222).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.4370076656341553
Epoch: 56, Steps: 63 | Train Loss: 0.4027375 Vali Loss: 0.2125655 Test Loss: 0.2772133
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.1905510425567627
Epoch: 57, Steps: 63 | Train Loss: 0.4016204 Vali Loss: 0.2121988 Test Loss: 0.2771871
Validation loss decreased (0.212222 --> 0.212199).  Saving model ...
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.403536081314087
Epoch: 58, Steps: 63 | Train Loss: 0.4002694 Vali Loss: 0.2123899 Test Loss: 0.2771713
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.4559686183929443
Epoch: 59, Steps: 63 | Train Loss: 0.3979562 Vali Loss: 0.2133360 Test Loss: 0.2771563
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.4526407718658447
Epoch: 60, Steps: 63 | Train Loss: 0.3996735 Vali Loss: 0.2138404 Test Loss: 0.2771349
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.0507283210754395
Epoch: 61, Steps: 63 | Train Loss: 0.3987058 Vali Loss: 0.2131770 Test Loss: 0.2771257
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.801780939102173
Epoch: 62, Steps: 63 | Train Loss: 0.4012981 Vali Loss: 0.2125714 Test Loss: 0.2770857
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.2373669147491455
Epoch: 63, Steps: 63 | Train Loss: 0.4015676 Vali Loss: 0.2131811 Test Loss: 0.2770776
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.9936802387237549
Epoch: 64, Steps: 63 | Train Loss: 0.4018669 Vali Loss: 0.2126147 Test Loss: 0.2770813
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.1750354766845703
Epoch: 65, Steps: 63 | Train Loss: 0.4029007 Vali Loss: 0.2118887 Test Loss: 0.2770556
Validation loss decreased (0.212199 --> 0.211889).  Saving model ...
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.6039650440216064
Epoch: 66, Steps: 63 | Train Loss: 0.4000154 Vali Loss: 0.2120770 Test Loss: 0.2770498
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 3.0343334674835205
Epoch: 67, Steps: 63 | Train Loss: 0.4006599 Vali Loss: 0.2138578 Test Loss: 0.2770489
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.1777005195617676
Epoch: 68, Steps: 63 | Train Loss: 0.4001308 Vali Loss: 0.2133203 Test Loss: 0.2770258
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.0533342361450195
Epoch: 69, Steps: 63 | Train Loss: 0.4021155 Vali Loss: 0.2126375 Test Loss: 0.2770268
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.3404340744018555
Epoch: 70, Steps: 63 | Train Loss: 0.4027553 Vali Loss: 0.2125274 Test Loss: 0.2769985
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.362088441848755
Epoch: 71, Steps: 63 | Train Loss: 0.3998969 Vali Loss: 0.2123231 Test Loss: 0.2769975
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.523160457611084
Epoch: 72, Steps: 63 | Train Loss: 0.4024248 Vali Loss: 0.2121889 Test Loss: 0.2769893
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.960547685623169
Epoch: 73, Steps: 63 | Train Loss: 0.4005727 Vali Loss: 0.2125137 Test Loss: 0.2769824
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.532223701477051
Epoch: 74, Steps: 63 | Train Loss: 0.4018577 Vali Loss: 0.2115815 Test Loss: 0.2769723
Validation loss decreased (0.211889 --> 0.211581).  Saving model ...
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.5252418518066406
Epoch: 75, Steps: 63 | Train Loss: 0.4019844 Vali Loss: 0.2126790 Test Loss: 0.2769594
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.19931697845459
Epoch: 76, Steps: 63 | Train Loss: 0.4008543 Vali Loss: 0.2124416 Test Loss: 0.2769443
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.241375684738159
Epoch: 77, Steps: 63 | Train Loss: 0.4023196 Vali Loss: 0.2135384 Test Loss: 0.2769584
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.359863042831421
Epoch: 78, Steps: 63 | Train Loss: 0.4010579 Vali Loss: 0.2121561 Test Loss: 0.2769348
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.949800491333008
Epoch: 79, Steps: 63 | Train Loss: 0.4006644 Vali Loss: 0.2131148 Test Loss: 0.2769331
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.198845863342285
Epoch: 80, Steps: 63 | Train Loss: 0.4017886 Vali Loss: 0.2132467 Test Loss: 0.2769355
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.4022481441497803
Epoch: 81, Steps: 63 | Train Loss: 0.4016483 Vali Loss: 0.2111298 Test Loss: 0.2769245
Validation loss decreased (0.211581 --> 0.211130).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.4564762115478516
Epoch: 82, Steps: 63 | Train Loss: 0.4025837 Vali Loss: 0.2120838 Test Loss: 0.2769212
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.028034210205078
Epoch: 83, Steps: 63 | Train Loss: 0.4019460 Vali Loss: 0.2122756 Test Loss: 0.2769116
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.4670684337615967
Epoch: 84, Steps: 63 | Train Loss: 0.4015641 Vali Loss: 0.2130806 Test Loss: 0.2769047
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.231112241744995
Epoch: 85, Steps: 63 | Train Loss: 0.4000446 Vali Loss: 0.2122833 Test Loss: 0.2769100
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 3.241163969039917
Epoch: 86, Steps: 63 | Train Loss: 0.4018366 Vali Loss: 0.2127722 Test Loss: 0.2769082
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 3.1464803218841553
Epoch: 87, Steps: 63 | Train Loss: 0.3993204 Vali Loss: 0.2126933 Test Loss: 0.2769037
EarlyStopping counter: 6 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.86106276512146
Epoch: 88, Steps: 63 | Train Loss: 0.4022242 Vali Loss: 0.2129084 Test Loss: 0.2768998
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.573655605316162
Epoch: 89, Steps: 63 | Train Loss: 0.4001919 Vali Loss: 0.2125197 Test Loss: 0.2768970
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.37849760055542
Epoch: 90, Steps: 63 | Train Loss: 0.4010293 Vali Loss: 0.2124446 Test Loss: 0.2768879
EarlyStopping counter: 9 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.9372782707214355
Epoch: 91, Steps: 63 | Train Loss: 0.3999845 Vali Loss: 0.2123108 Test Loss: 0.2768844
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.6001319885253906
Epoch: 92, Steps: 63 | Train Loss: 0.4007198 Vali Loss: 0.2129047 Test Loss: 0.2768861
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 1.8204963207244873
Epoch: 93, Steps: 63 | Train Loss: 0.4019534 Vali Loss: 0.2132439 Test Loss: 0.2768784
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 3.353008508682251
Epoch: 94, Steps: 63 | Train Loss: 0.4000850 Vali Loss: 0.2124014 Test Loss: 0.2768789
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.3305256366729736
Epoch: 95, Steps: 63 | Train Loss: 0.3989618 Vali Loss: 0.2130283 Test Loss: 0.2768728
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 1.9965226650238037
Epoch: 96, Steps: 63 | Train Loss: 0.4022008 Vali Loss: 0.2122835 Test Loss: 0.2768695
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.037269115447998
Epoch: 97, Steps: 63 | Train Loss: 0.4025824 Vali Loss: 0.2119637 Test Loss: 0.2768686
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.1455793380737305
Epoch: 98, Steps: 63 | Train Loss: 0.3994312 Vali Loss: 0.2114453 Test Loss: 0.2768643
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.0506064891815186
Epoch: 99, Steps: 63 | Train Loss: 0.3968847 Vali Loss: 0.2129426 Test Loss: 0.2768651
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.484884262084961
Epoch: 100, Steps: 63 | Train Loss: 0.4021699 Vali Loss: 0.2126774 Test Loss: 0.2768668
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_360_96_FITS_ETTh2_ftM_sl360_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.27659422159194946, mae:0.33916160464286804, rse:0.42384088039398193, corr:[0.2721219  0.27807832 0.27482533 0.27411708 0.2742887  0.27257338
 0.27089807 0.2701067  0.26923063 0.26758623 0.26589498 0.26430655
 0.2630277  0.26201442 0.2611028  0.26047817 0.2600877  0.2596158
 0.25860858 0.2573352  0.25619295 0.25510663 0.25349313 0.25128722
 0.24905792 0.2471246  0.24529491 0.24370727 0.24270791 0.24170396
 0.2401393  0.23833923 0.23688668 0.23565458 0.23415849 0.23278025
 0.23179042 0.23067965 0.22945766 0.22859056 0.22844625 0.2282055
 0.22734989 0.22641103 0.22585839 0.22502725 0.22361438 0.22191045
 0.22027622 0.21860193 0.21699803 0.21578674 0.21488255 0.21361133
 0.21168955 0.20986567 0.2086233  0.20707712 0.20558406 0.20479883
 0.20460224 0.20437779 0.20442617 0.20461111 0.20456399 0.20411623
 0.20361559 0.20326631 0.20307176 0.20259354 0.20199081 0.20144205
 0.200701   0.19987667 0.1993131  0.19872671 0.19806762 0.19739561
 0.19703916 0.19663163 0.19624802 0.19555461 0.19562314 0.19591345
 0.19547226 0.19488096 0.19554615 0.19627939 0.19515713 0.19404809
 0.19417049 0.19351527 0.19175795 0.19094515 0.1912763  0.18319528]
