Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=72, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=72, out_features=144, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9289728.0
params:  10512.0
Trainable parameters:  10512
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.6446213722229004
Epoch: 1, Steps: 56 | Train Loss: 1.0782390 Vali Loss: 0.8073469 Test Loss: 0.4590122
Validation loss decreased (inf --> 0.807347).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.9477014541625977
Epoch: 2, Steps: 56 | Train Loss: 0.9292853 Vali Loss: 0.7514465 Test Loss: 0.4204385
Validation loss decreased (0.807347 --> 0.751446).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.9610061645507812
Epoch: 3, Steps: 56 | Train Loss: 0.8795668 Vali Loss: 0.7283872 Test Loss: 0.4060760
Validation loss decreased (0.751446 --> 0.728387).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.7116386890411377
Epoch: 4, Steps: 56 | Train Loss: 0.8576248 Vali Loss: 0.7096438 Test Loss: 0.3986745
Validation loss decreased (0.728387 --> 0.709644).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.8735849857330322
Epoch: 5, Steps: 56 | Train Loss: 0.8457353 Vali Loss: 0.6955113 Test Loss: 0.3942312
Validation loss decreased (0.709644 --> 0.695511).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.463930368423462
Epoch: 6, Steps: 56 | Train Loss: 0.8382694 Vali Loss: 0.6892155 Test Loss: 0.3911877
Validation loss decreased (0.695511 --> 0.689216).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.2793667316436768
Epoch: 7, Steps: 56 | Train Loss: 0.8334613 Vali Loss: 0.6842747 Test Loss: 0.3889954
Validation loss decreased (0.689216 --> 0.684275).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.68707013130188
Epoch: 8, Steps: 56 | Train Loss: 0.8294250 Vali Loss: 0.6846862 Test Loss: 0.3873465
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.4415395259857178
Epoch: 9, Steps: 56 | Train Loss: 0.8254999 Vali Loss: 0.6770161 Test Loss: 0.3861417
Validation loss decreased (0.684275 --> 0.677016).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.59562349319458
Epoch: 10, Steps: 56 | Train Loss: 0.8227974 Vali Loss: 0.6762897 Test Loss: 0.3851617
Validation loss decreased (0.677016 --> 0.676290).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.6869266033172607
Epoch: 11, Steps: 56 | Train Loss: 0.8231211 Vali Loss: 0.6735775 Test Loss: 0.3843414
Validation loss decreased (0.676290 --> 0.673578).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.2011849880218506
Epoch: 12, Steps: 56 | Train Loss: 0.8193606 Vali Loss: 0.6701263 Test Loss: 0.3837546
Validation loss decreased (0.673578 --> 0.670126).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.615858554840088
Epoch: 13, Steps: 56 | Train Loss: 0.8189313 Vali Loss: 0.6687587 Test Loss: 0.3832746
Validation loss decreased (0.670126 --> 0.668759).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.7504334449768066
Epoch: 14, Steps: 56 | Train Loss: 0.8179844 Vali Loss: 0.6652793 Test Loss: 0.3828871
Validation loss decreased (0.668759 --> 0.665279).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.6160390377044678
Epoch: 15, Steps: 56 | Train Loss: 0.8153799 Vali Loss: 0.6650371 Test Loss: 0.3825656
Validation loss decreased (0.665279 --> 0.665037).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.429034471511841
Epoch: 16, Steps: 56 | Train Loss: 0.8150199 Vali Loss: 0.6653910 Test Loss: 0.3822760
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.2169079780578613
Epoch: 17, Steps: 56 | Train Loss: 0.8142122 Vali Loss: 0.6607325 Test Loss: 0.3821326
Validation loss decreased (0.665037 --> 0.660733).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.5870361328125
Epoch: 18, Steps: 56 | Train Loss: 0.8150976 Vali Loss: 0.6592669 Test Loss: 0.3819232
Validation loss decreased (0.660733 --> 0.659267).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.6657276153564453
Epoch: 19, Steps: 56 | Train Loss: 0.8132135 Vali Loss: 0.6618875 Test Loss: 0.3817794
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.5284385681152344
Epoch: 20, Steps: 56 | Train Loss: 0.8124773 Vali Loss: 0.6634001 Test Loss: 0.3817162
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.373974084854126
Epoch: 21, Steps: 56 | Train Loss: 0.8100700 Vali Loss: 0.6607148 Test Loss: 0.3815995
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.771174669265747
Epoch: 22, Steps: 56 | Train Loss: 0.8119235 Vali Loss: 0.6586536 Test Loss: 0.3814924
Validation loss decreased (0.659267 --> 0.658654).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.212249755859375
Epoch: 23, Steps: 56 | Train Loss: 0.8112878 Vali Loss: 0.6592038 Test Loss: 0.3814643
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.858865261077881
Epoch: 24, Steps: 56 | Train Loss: 0.8105460 Vali Loss: 0.6573948 Test Loss: 0.3814317
Validation loss decreased (0.658654 --> 0.657395).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.5836362838745117
Epoch: 25, Steps: 56 | Train Loss: 0.8106825 Vali Loss: 0.6603332 Test Loss: 0.3813792
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.40895938873291
Epoch: 26, Steps: 56 | Train Loss: 0.8092020 Vali Loss: 0.6549720 Test Loss: 0.3813007
Validation loss decreased (0.657395 --> 0.654972).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.419304609298706
Epoch: 27, Steps: 56 | Train Loss: 0.8104462 Vali Loss: 0.6569605 Test Loss: 0.3812897
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.007708787918091
Epoch: 28, Steps: 56 | Train Loss: 0.8085187 Vali Loss: 0.6525995 Test Loss: 0.3812641
Validation loss decreased (0.654972 --> 0.652600).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.5910019874572754
Epoch: 29, Steps: 56 | Train Loss: 0.8103141 Vali Loss: 0.6572747 Test Loss: 0.3812297
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.665266752243042
Epoch: 30, Steps: 56 | Train Loss: 0.8074182 Vali Loss: 0.6584981 Test Loss: 0.3812206
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.968705892562866
Epoch: 31, Steps: 56 | Train Loss: 0.8092009 Vali Loss: 0.6575516 Test Loss: 0.3812257
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.073286294937134
Epoch: 32, Steps: 56 | Train Loss: 0.8084217 Vali Loss: 0.6546723 Test Loss: 0.3811819
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.3567192554473877
Epoch: 33, Steps: 56 | Train Loss: 0.8089436 Vali Loss: 0.6569256 Test Loss: 0.3812078
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.2388792037963867
Epoch: 34, Steps: 56 | Train Loss: 0.8082603 Vali Loss: 0.6557101 Test Loss: 0.3811771
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.531299591064453
Epoch: 35, Steps: 56 | Train Loss: 0.8079048 Vali Loss: 0.6540090 Test Loss: 0.3811666
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.9430384635925293
Epoch: 36, Steps: 56 | Train Loss: 0.8071637 Vali Loss: 0.6567587 Test Loss: 0.3811626
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.498831272125244
Epoch: 37, Steps: 56 | Train Loss: 0.8075675 Vali Loss: 0.6542820 Test Loss: 0.3811566
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6813552379608154
Epoch: 38, Steps: 56 | Train Loss: 0.8086698 Vali Loss: 0.6564740 Test Loss: 0.3811367
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.6546406745910645
Epoch: 39, Steps: 56 | Train Loss: 0.8081611 Vali Loss: 0.6544670 Test Loss: 0.3811398
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.87162709236145
Epoch: 40, Steps: 56 | Train Loss: 0.8064529 Vali Loss: 0.6518251 Test Loss: 0.3811285
Validation loss decreased (0.652600 --> 0.651825).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.6260457038879395
Epoch: 41, Steps: 56 | Train Loss: 0.8063917 Vali Loss: 0.6526609 Test Loss: 0.3811290
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.6431760787963867
Epoch: 42, Steps: 56 | Train Loss: 0.8075388 Vali Loss: 0.6535321 Test Loss: 0.3811229
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.6385226249694824
Epoch: 43, Steps: 56 | Train Loss: 0.8067763 Vali Loss: 0.6539638 Test Loss: 0.3811038
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.685788869857788
Epoch: 44, Steps: 56 | Train Loss: 0.8075926 Vali Loss: 0.6551780 Test Loss: 0.3811247
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.646137237548828
Epoch: 45, Steps: 56 | Train Loss: 0.8073026 Vali Loss: 0.6538033 Test Loss: 0.3811136
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.1195597648620605
Epoch: 46, Steps: 56 | Train Loss: 0.8074565 Vali Loss: 0.6502740 Test Loss: 0.3811128
Validation loss decreased (0.651825 --> 0.650274).  Saving model ...
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.56304931640625
Epoch: 47, Steps: 56 | Train Loss: 0.8065465 Vali Loss: 0.6525719 Test Loss: 0.3811153
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.3135697841644287
Epoch: 48, Steps: 56 | Train Loss: 0.8065098 Vali Loss: 0.6531859 Test Loss: 0.3811267
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.835991382598877
Epoch: 49, Steps: 56 | Train Loss: 0.8059730 Vali Loss: 0.6554847 Test Loss: 0.3811269
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.4291131496429443
Epoch: 50, Steps: 56 | Train Loss: 0.8068081 Vali Loss: 0.6540452 Test Loss: 0.3811329
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.350008487701416
Epoch: 51, Steps: 56 | Train Loss: 0.8066304 Vali Loss: 0.6539956 Test Loss: 0.3811343
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.0835700035095215
Epoch: 52, Steps: 56 | Train Loss: 0.8068455 Vali Loss: 0.6523695 Test Loss: 0.3811307
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.102663516998291
Epoch: 53, Steps: 56 | Train Loss: 0.8061922 Vali Loss: 0.6543686 Test Loss: 0.3811380
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.091264247894287
Epoch: 54, Steps: 56 | Train Loss: 0.8079145 Vali Loss: 0.6534338 Test Loss: 0.3811397
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.7947657108306885
Epoch: 55, Steps: 56 | Train Loss: 0.8066157 Vali Loss: 0.6518475 Test Loss: 0.3811422
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.265597343444824
Epoch: 56, Steps: 56 | Train Loss: 0.8082237 Vali Loss: 0.6561956 Test Loss: 0.3811361
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.8905398845672607
Epoch: 57, Steps: 56 | Train Loss: 0.8069365 Vali Loss: 0.6529219 Test Loss: 0.3811541
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.425135374069214
Epoch: 58, Steps: 56 | Train Loss: 0.8050466 Vali Loss: 0.6551256 Test Loss: 0.3811346
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.521446466445923
Epoch: 59, Steps: 56 | Train Loss: 0.8054985 Vali Loss: 0.6530265 Test Loss: 0.3811420
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.6541013717651367
Epoch: 60, Steps: 56 | Train Loss: 0.8067897 Vali Loss: 0.6545900 Test Loss: 0.3811375
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.436744451522827
Epoch: 61, Steps: 56 | Train Loss: 0.8050943 Vali Loss: 0.6527848 Test Loss: 0.3811366
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.0984370708465576
Epoch: 62, Steps: 56 | Train Loss: 0.8063657 Vali Loss: 0.6535023 Test Loss: 0.3811504
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.6641972064971924
Epoch: 63, Steps: 56 | Train Loss: 0.8055072 Vali Loss: 0.6559151 Test Loss: 0.3811427
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.7307045459747314
Epoch: 64, Steps: 56 | Train Loss: 0.8070261 Vali Loss: 0.6535049 Test Loss: 0.3811469
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.666534662246704
Epoch: 65, Steps: 56 | Train Loss: 0.8069362 Vali Loss: 0.6529615 Test Loss: 0.3811484
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.6636862754821777
Epoch: 66, Steps: 56 | Train Loss: 0.8065772 Vali Loss: 0.6532317 Test Loss: 0.3811471
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.37971001863479614, mae:0.42401453852653503, rse:0.4925289452075958, corr:[ 2.15038300e-01  2.16734514e-01  2.18450442e-01  2.18913391e-01
  2.17904836e-01  2.16219440e-01  2.14551046e-01  2.13091642e-01
  2.11857721e-01  2.10740149e-01  2.09845573e-01  2.09052667e-01
  2.08286032e-01  2.07405657e-01  2.06394136e-01  2.05387697e-01
  2.04467267e-01  2.03618288e-01  2.02870876e-01  2.02147707e-01
  2.01330498e-01  2.00363159e-01  1.99229598e-01  1.98011354e-01
  1.96840376e-01  1.95754200e-01  1.94705561e-01  1.93615049e-01
  1.92547336e-01  1.91465542e-01  1.90464839e-01  1.89562261e-01
  1.88764542e-01  1.87987193e-01  1.87256977e-01  1.86552316e-01
  1.85837358e-01  1.85123891e-01  1.84475094e-01  1.83859155e-01
  1.83247060e-01  1.82580054e-01  1.81842044e-01  1.81073830e-01
  1.80182353e-01  1.79221302e-01  1.78199530e-01  1.76981047e-01
  1.75763294e-01  1.74684539e-01  1.73774317e-01  1.72954455e-01
  1.72181353e-01  1.71416983e-01  1.70583069e-01  1.69810802e-01
  1.69162720e-01  1.68661326e-01  1.68366760e-01  1.68266237e-01
  1.68342873e-01  1.68384492e-01  1.68384984e-01  1.68394223e-01
  1.68309927e-01  1.68246493e-01  1.68199778e-01  1.68185130e-01
  1.68139309e-01  1.68070659e-01  1.67948544e-01  1.67748049e-01
  1.67503417e-01  1.67154759e-01  1.66768342e-01  1.66261420e-01
  1.65771842e-01  1.65414482e-01  1.65195569e-01  1.65182069e-01
  1.65370211e-01  1.65679201e-01  1.65951595e-01  1.66104198e-01
  1.66090727e-01  1.65901035e-01  1.65603906e-01  1.65290803e-01
  1.65060014e-01  1.65013865e-01  1.65147364e-01  1.65355161e-01
  1.65633842e-01  1.65769041e-01  1.65743306e-01  1.65474996e-01
  1.65040523e-01  1.64568648e-01  1.64110854e-01  1.63781583e-01
  1.63730949e-01  1.63757548e-01  1.63917825e-01  1.64066985e-01
  1.64189473e-01  1.64134800e-01  1.63954675e-01  1.63700774e-01
  1.63354769e-01  1.62974253e-01  1.62702218e-01  1.62590533e-01
  1.62578315e-01  1.62602574e-01  1.62553102e-01  1.62395805e-01
  1.62059993e-01  1.61475450e-01  1.60774171e-01  1.59969658e-01
  1.59133554e-01  1.58394605e-01  1.57737300e-01  1.57219917e-01
  1.56738803e-01  1.56352252e-01  1.55937329e-01  1.55520186e-01
  1.55110165e-01  1.54529750e-01  1.53965756e-01  1.53321102e-01
  1.52692407e-01  1.52028263e-01  1.51441857e-01  1.51050761e-01
  1.50702566e-01  1.50445193e-01  1.50192261e-01  1.49881080e-01
  1.49488956e-01  1.48905575e-01  1.48105651e-01  1.47155821e-01
  1.46087915e-01  1.45191252e-01  1.44540653e-01  1.44073695e-01
  1.43646911e-01  1.43197790e-01  1.42780364e-01  1.42407313e-01
  1.42105639e-01  1.41779214e-01  1.41442403e-01  1.40952215e-01
  1.40448123e-01  1.39979109e-01  1.39608398e-01  1.39347985e-01
  1.39139146e-01  1.39085770e-01  1.39161810e-01  1.39281243e-01
  1.39438629e-01  1.39478266e-01  1.39396653e-01  1.39121875e-01
  1.38744593e-01  1.38275504e-01  1.37803525e-01  1.37311116e-01
  1.36826456e-01  1.36285707e-01  1.35631248e-01  1.35001674e-01
  1.34503841e-01  1.34147510e-01  1.33780614e-01  1.33457765e-01
  1.33198053e-01  1.33026868e-01  1.32995427e-01  1.33002669e-01
  1.33080199e-01  1.33279160e-01  1.33577213e-01  1.33885682e-01
  1.34213537e-01  1.34542108e-01  1.34880841e-01  1.35116130e-01
  1.35203674e-01  1.35238260e-01  1.35255367e-01  1.35318145e-01
  1.35449171e-01  1.35471404e-01  1.35499343e-01  1.35421291e-01
  1.35212988e-01  1.34891570e-01  1.34661719e-01  1.34367123e-01
  1.34169355e-01  1.34219378e-01  1.34398386e-01  1.34702206e-01
  1.35099798e-01  1.35525614e-01  1.35771528e-01  1.35908529e-01
  1.35989934e-01  1.36031970e-01  1.35992602e-01  1.35806322e-01
  1.35531083e-01  1.35212883e-01  1.34902954e-01  1.34754971e-01
  1.34648725e-01  1.34720504e-01  1.34766161e-01  1.34921178e-01
  1.35058224e-01  1.35119498e-01  1.35192931e-01  1.35210171e-01
  1.35305569e-01  1.35435104e-01  1.35689527e-01  1.36092052e-01
  1.36586860e-01  1.37150571e-01  1.37781128e-01  1.38441071e-01
  1.39137357e-01  1.39751419e-01  1.40255347e-01  1.40642479e-01
  1.40906617e-01  1.40973046e-01  1.40944302e-01  1.40997663e-01
  1.41146898e-01  1.41364068e-01  1.41715556e-01  1.42126217e-01
  1.42597780e-01  1.43068895e-01  1.43555433e-01  1.44023880e-01
  1.44517452e-01  1.45190060e-01  1.45911992e-01  1.46745369e-01
  1.47506222e-01  1.48186028e-01  1.48773640e-01  1.49369061e-01
  1.49982363e-01  1.50572062e-01  1.51201949e-01  1.51694641e-01
  1.52114585e-01  1.52443752e-01  1.52753040e-01  1.53124690e-01
  1.53392047e-01  1.53628692e-01  1.53836206e-01  1.54006898e-01
  1.54177785e-01  1.54394895e-01  1.54695734e-01  1.55139074e-01
  1.55535817e-01  1.56167448e-01  1.56789899e-01  1.57311633e-01
  1.57788306e-01  1.58246562e-01  1.58662215e-01  1.59104288e-01
  1.59542754e-01  1.59956589e-01  1.60428107e-01  1.60781831e-01
  1.60956353e-01  1.60937518e-01  1.60767272e-01  1.60667717e-01
  1.60592884e-01  1.60551846e-01  1.60574362e-01  1.60740599e-01
  1.60982341e-01  1.61279231e-01  1.61581054e-01  1.61919311e-01
  1.62035093e-01  1.62245885e-01  1.62556589e-01  1.62877932e-01
  1.63108453e-01  1.63390905e-01  1.63664967e-01  1.64029703e-01
  1.64460599e-01  1.64885819e-01  1.65227517e-01  1.65345356e-01
  1.65367395e-01  1.65202364e-01  1.64986894e-01  1.64657414e-01
  1.64298117e-01  1.63988784e-01  1.63752720e-01  1.63608626e-01
  1.63492948e-01  1.63462430e-01  1.63527921e-01  1.63703978e-01
  1.63898960e-01  1.64010614e-01  1.64359152e-01  1.64685011e-01
  1.65014774e-01  1.65323317e-01  1.65624976e-01  1.66065708e-01
  1.66668072e-01  1.67279303e-01  1.67895719e-01  1.68358326e-01
  1.68530762e-01  1.68558553e-01  1.68568969e-01  1.68540925e-01
  1.68592617e-01  1.68608382e-01  1.68745473e-01  1.68921694e-01
  1.68995321e-01  1.69032633e-01  1.68910235e-01  1.69021562e-01
  1.69318780e-01  1.69720262e-01  1.70212880e-01  1.70800820e-01
  1.71408057e-01  1.72035083e-01  1.72578618e-01  1.73143744e-01
  1.73656493e-01  1.74079031e-01  1.74420550e-01  1.74675122e-01
  1.74848869e-01  1.75006047e-01  1.75303668e-01  1.75675139e-01
  1.76200017e-01  1.76718026e-01  1.77143499e-01  1.77388161e-01
  1.77358851e-01  1.77377313e-01  1.77310184e-01  1.77290022e-01
  1.77238598e-01  1.77264526e-01  1.77361965e-01  1.77477553e-01
  1.77576989e-01  1.77603945e-01  1.77529946e-01  1.77483529e-01
  1.77520886e-01  1.77630261e-01  1.77688763e-01  1.77789375e-01
  1.77915737e-01  1.78005576e-01  1.78146675e-01  1.78339705e-01
  1.78592265e-01  1.78855777e-01  1.79108977e-01  1.79386541e-01
  1.79675728e-01  1.79938614e-01  1.80178985e-01  1.80267006e-01
  1.80277541e-01  1.79975778e-01  1.79643080e-01  1.79374442e-01
  1.79102927e-01  1.78881928e-01  1.78685620e-01  1.78534299e-01
  1.78487048e-01  1.78435907e-01  1.78266928e-01  1.78104267e-01
  1.77837163e-01  1.77575171e-01  1.77330300e-01  1.77121535e-01
  1.77018791e-01  1.77012473e-01  1.77068159e-01  1.77204430e-01
  1.77298337e-01  1.77220359e-01  1.76872358e-01  1.76338673e-01
  1.75563291e-01  1.74725741e-01  1.73878491e-01  1.73031524e-01
  1.72149852e-01  1.71162456e-01  1.70251951e-01  1.69432327e-01
  1.68699980e-01  1.68101117e-01  1.67502537e-01  1.67008907e-01
  1.66555613e-01  1.66113243e-01  1.65751755e-01  1.65363505e-01
  1.64912075e-01  1.64378569e-01  1.63818777e-01  1.63305014e-01
  1.62734985e-01  1.62260398e-01  1.61857262e-01  1.61568508e-01
  1.61207646e-01  1.60976753e-01  1.60624936e-01  1.60222903e-01
  1.59683123e-01  1.59121469e-01  1.58565149e-01  1.58309698e-01
  1.58152938e-01  1.58152387e-01  1.58210754e-01  1.58233419e-01
  1.58219129e-01  1.58049181e-01  1.57691106e-01  1.57335788e-01
  1.56956062e-01  1.56518802e-01  1.56110659e-01  1.55870274e-01
  1.55589134e-01  1.55395910e-01  1.55229166e-01  1.54921442e-01
  1.54556379e-01  1.54053852e-01  1.53628930e-01  1.53194994e-01
  1.52786016e-01  1.52385443e-01  1.52172893e-01  1.52069300e-01
  1.52002573e-01  1.51961997e-01  1.51786283e-01  1.51442319e-01
  1.50891304e-01  1.50186464e-01  1.49524122e-01  1.48725420e-01
  1.47900581e-01  1.47133321e-01  1.46506175e-01  1.45870641e-01
  1.45309120e-01  1.44738942e-01  1.44255623e-01  1.43632129e-01
  1.42892897e-01  1.42081708e-01  1.41459569e-01  1.41046107e-01
  1.40641451e-01  1.40335456e-01  1.40076473e-01  1.39833376e-01
  1.39528930e-01  1.38999328e-01  1.38341293e-01  1.37411848e-01
  1.36216909e-01  1.35055438e-01  1.34107336e-01  1.33351520e-01
  1.32735342e-01  1.32319495e-01  1.31888181e-01  1.31491393e-01
  1.31077677e-01  1.30585924e-01  1.30049318e-01  1.29290327e-01
  1.28469229e-01  1.27629712e-01  1.26923323e-01  1.26273215e-01
  1.25616133e-01  1.25056699e-01  1.24531716e-01  1.24021448e-01
  1.23455197e-01  1.22861713e-01  1.22098088e-01  1.21050686e-01
  1.19655587e-01  1.18138790e-01  1.16798624e-01  1.15482986e-01
  1.14324644e-01  1.13348864e-01  1.12536229e-01  1.11868232e-01
  1.11150131e-01  1.10288039e-01  1.09315649e-01  1.08146228e-01
  1.06870599e-01  1.05584517e-01  1.04525447e-01  1.03751659e-01
  1.03165403e-01  1.02804802e-01  1.02401905e-01  1.01917520e-01
  1.01349697e-01  1.00494631e-01  9.95157063e-02  9.83150974e-02
  9.71062705e-02  9.59266648e-02  9.48303416e-02  9.37531441e-02
  9.26668271e-02  9.15981531e-02  9.06268954e-02  8.96022841e-02
  8.88673887e-02  8.82531554e-02  8.78925472e-02  8.74901116e-02
  8.70825425e-02  8.66590738e-02  8.62703174e-02  8.57695863e-02
  8.53230581e-02  8.48126188e-02  8.40835273e-02  8.31375942e-02
  8.19905251e-02  8.08618367e-02  7.97571614e-02  7.86999315e-02
  7.75616989e-02  7.65124410e-02  7.54752308e-02  7.43898004e-02
  7.33026117e-02  7.22099990e-02  7.11716190e-02  7.01969415e-02
  6.93365335e-02  6.85800388e-02  6.78250194e-02  6.71617612e-02
  6.64573535e-02  6.57623932e-02  6.51867688e-02  6.46672249e-02
  6.42201379e-02  6.38316199e-02  6.33987412e-02  6.30011633e-02
  6.25660047e-02  6.20944425e-02  6.13816567e-02  6.04475290e-02
  5.93261719e-02  5.81845939e-02  5.68329282e-02  5.54642752e-02
  5.39547727e-02  5.25043719e-02  5.10884523e-02  4.97696772e-02
  4.84682545e-02  4.73994762e-02  4.67565581e-02  4.61898558e-02
  4.56794016e-02  4.52504493e-02  4.50557061e-02  4.50610183e-02
  4.51529063e-02  4.54700477e-02  4.57729436e-02  4.60283160e-02
  4.60114144e-02  4.57886383e-02  4.51540761e-02  4.44572568e-02
  4.35629748e-02  4.25031595e-02  4.13289592e-02  4.01372090e-02
  3.90539914e-02  3.83166037e-02  3.78814265e-02  3.76307145e-02
  3.75490710e-02  3.76114249e-02  3.75748873e-02  3.72739919e-02
  3.65238301e-02  3.54584195e-02  3.44453529e-02  3.39007787e-02
  3.35068852e-02  3.34308781e-02  3.35407667e-02  3.35973538e-02
  3.36700939e-02  3.37476544e-02  3.35167125e-02  3.28781307e-02
  3.18887457e-02  3.08573376e-02  2.99667567e-02  2.90405285e-02
  2.82423533e-02  2.74849050e-02  2.67676711e-02  2.61856187e-02
  2.57143565e-02  2.52522863e-02  2.48504020e-02  2.44818423e-02
  2.40586475e-02  2.38276403e-02  2.40230132e-02  2.44402569e-02
  2.50035468e-02  2.55574007e-02  2.61056386e-02  2.66161170e-02
  2.68535968e-02  2.68379636e-02  2.64830813e-02  2.58010887e-02
  2.48176977e-02  2.37528719e-02  2.28052493e-02  2.16653980e-02
  2.06747744e-02  1.97325181e-02  1.92329269e-02  1.87695138e-02
  1.85802449e-02  1.85924303e-02  1.86100658e-02  1.84475314e-02
  1.83647424e-02  1.81685854e-02  1.81884412e-02  1.82787441e-02
  1.83303673e-02  1.83683373e-02  1.83688216e-02  1.79580171e-02
  1.73590109e-02  1.66365989e-02  1.59105472e-02  1.49425371e-02
  1.36403590e-02  1.20109916e-02  1.04162861e-02  8.71203560e-03
  7.21684331e-03  5.76338358e-03  4.61424654e-03  3.38877528e-03
  2.18831748e-03  1.08899793e-03 -2.72451125e-05 -1.07352389e-03
 -1.91396708e-03 -2.48535699e-03 -2.12407787e-03 -8.27263400e-04
  1.33009511e-03  4.03639954e-03  6.12008758e-03  6.34120405e-03
  3.79835931e-03 -1.42345426e-03 -9.43121780e-03 -2.01581120e-02]
