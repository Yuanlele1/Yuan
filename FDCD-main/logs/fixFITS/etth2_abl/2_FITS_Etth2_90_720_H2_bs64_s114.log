Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.60983943939209
Epoch: 1, Steps: 61 | Train Loss: 1.3219156 Vali Loss: 0.8892136 Test Loss: 0.7218886
Validation loss decreased (inf --> 0.889214).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.9757180213928223
Epoch: 2, Steps: 61 | Train Loss: 1.1330605 Vali Loss: 0.8092511 Test Loss: 0.6264672
Validation loss decreased (0.889214 --> 0.809251).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.228231191635132
Epoch: 3, Steps: 61 | Train Loss: 1.0153156 Vali Loss: 0.7538829 Test Loss: 0.5664712
Validation loss decreased (0.809251 --> 0.753883).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.9035742282867432
Epoch: 4, Steps: 61 | Train Loss: 0.9431148 Vali Loss: 0.7183366 Test Loss: 0.5268559
Validation loss decreased (0.753883 --> 0.718337).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.9732322692871094
Epoch: 5, Steps: 61 | Train Loss: 0.8929062 Vali Loss: 0.6994082 Test Loss: 0.4992983
Validation loss decreased (0.718337 --> 0.699408).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.737825632095337
Epoch: 6, Steps: 61 | Train Loss: 0.8600968 Vali Loss: 0.6845266 Test Loss: 0.4800625
Validation loss decreased (0.699408 --> 0.684527).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.010191917419434
Epoch: 7, Steps: 61 | Train Loss: 0.8368016 Vali Loss: 0.6688268 Test Loss: 0.4664090
Validation loss decreased (0.684527 --> 0.668827).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.533673048019409
Epoch: 8, Steps: 61 | Train Loss: 0.8192367 Vali Loss: 0.6652493 Test Loss: 0.4565559
Validation loss decreased (0.668827 --> 0.665249).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.480612277984619
Epoch: 9, Steps: 61 | Train Loss: 0.8062743 Vali Loss: 0.6592631 Test Loss: 0.4492095
Validation loss decreased (0.665249 --> 0.659263).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 4.161726951599121
Epoch: 10, Steps: 61 | Train Loss: 0.7978730 Vali Loss: 0.6494061 Test Loss: 0.4437562
Validation loss decreased (0.659263 --> 0.649406).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6397886276245117
Epoch: 11, Steps: 61 | Train Loss: 0.7919301 Vali Loss: 0.6459375 Test Loss: 0.4396914
Validation loss decreased (0.649406 --> 0.645938).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.880279302597046
Epoch: 12, Steps: 61 | Train Loss: 0.7870684 Vali Loss: 0.6420902 Test Loss: 0.4365633
Validation loss decreased (0.645938 --> 0.642090).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.082167863845825
Epoch: 13, Steps: 61 | Train Loss: 0.7819803 Vali Loss: 0.6448255 Test Loss: 0.4341268
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.520603656768799
Epoch: 14, Steps: 61 | Train Loss: 0.7789425 Vali Loss: 0.6408921 Test Loss: 0.4322350
Validation loss decreased (0.642090 --> 0.640892).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.5305440425872803
Epoch: 15, Steps: 61 | Train Loss: 0.7761451 Vali Loss: 0.6399745 Test Loss: 0.4307299
Validation loss decreased (0.640892 --> 0.639974).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.8143939971923828
Epoch: 16, Steps: 61 | Train Loss: 0.7752748 Vali Loss: 0.6352099 Test Loss: 0.4295067
Validation loss decreased (0.639974 --> 0.635210).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.0330820083618164
Epoch: 17, Steps: 61 | Train Loss: 0.7721445 Vali Loss: 0.6373516 Test Loss: 0.4285017
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.0269222259521484
Epoch: 18, Steps: 61 | Train Loss: 0.7723458 Vali Loss: 0.6408848 Test Loss: 0.4277045
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.0668327808380127
Epoch: 19, Steps: 61 | Train Loss: 0.7712134 Vali Loss: 0.6377082 Test Loss: 0.4270387
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.648550510406494
Epoch: 20, Steps: 61 | Train Loss: 0.7694714 Vali Loss: 0.6361316 Test Loss: 0.4264636
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0527822971343994
Epoch: 21, Steps: 61 | Train Loss: 0.7693099 Vali Loss: 0.6338621 Test Loss: 0.4260013
Validation loss decreased (0.635210 --> 0.633862).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.371166944503784
Epoch: 22, Steps: 61 | Train Loss: 0.7689150 Vali Loss: 0.6298394 Test Loss: 0.4255986
Validation loss decreased (0.633862 --> 0.629839).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.3756749629974365
Epoch: 23, Steps: 61 | Train Loss: 0.7670347 Vali Loss: 0.6348252 Test Loss: 0.4252593
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.4841535091400146
Epoch: 24, Steps: 61 | Train Loss: 0.7674579 Vali Loss: 0.6333179 Test Loss: 0.4249723
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.4067063331604004
Epoch: 25, Steps: 61 | Train Loss: 0.7658007 Vali Loss: 0.6309919 Test Loss: 0.4247077
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.52126145362854
Epoch: 26, Steps: 61 | Train Loss: 0.7665959 Vali Loss: 0.6334810 Test Loss: 0.4244779
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.1312003135681152
Epoch: 27, Steps: 61 | Train Loss: 0.7667301 Vali Loss: 0.6367716 Test Loss: 0.4242844
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.7000110149383545
Epoch: 28, Steps: 61 | Train Loss: 0.7647468 Vali Loss: 0.6293828 Test Loss: 0.4240896
Validation loss decreased (0.629839 --> 0.629383).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.753272294998169
Epoch: 29, Steps: 61 | Train Loss: 0.7642347 Vali Loss: 0.6297745 Test Loss: 0.4239349
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.3802030086517334
Epoch: 30, Steps: 61 | Train Loss: 0.7643828 Vali Loss: 0.6337925 Test Loss: 0.4237862
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.9232494831085205
Epoch: 31, Steps: 61 | Train Loss: 0.7645862 Vali Loss: 0.6301253 Test Loss: 0.4236715
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.8304831981658936
Epoch: 32, Steps: 61 | Train Loss: 0.7643127 Vali Loss: 0.6316270 Test Loss: 0.4235428
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.2471818923950195
Epoch: 33, Steps: 61 | Train Loss: 0.7646683 Vali Loss: 0.6253920 Test Loss: 0.4234470
Validation loss decreased (0.629383 --> 0.625392).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.5828328132629395
Epoch: 34, Steps: 61 | Train Loss: 0.7654567 Vali Loss: 0.6329881 Test Loss: 0.4233472
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.055741548538208
Epoch: 35, Steps: 61 | Train Loss: 0.7641405 Vali Loss: 0.6342674 Test Loss: 0.4232664
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.530122995376587
Epoch: 36, Steps: 61 | Train Loss: 0.7646221 Vali Loss: 0.6297715 Test Loss: 0.4231803
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.5246236324310303
Epoch: 37, Steps: 61 | Train Loss: 0.7636547 Vali Loss: 0.6320323 Test Loss: 0.4231060
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.5187923908233643
Epoch: 38, Steps: 61 | Train Loss: 0.7644395 Vali Loss: 0.6295872 Test Loss: 0.4230332
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.634349822998047
Epoch: 39, Steps: 61 | Train Loss: 0.7633218 Vali Loss: 0.6268449 Test Loss: 0.4229718
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.095712184906006
Epoch: 40, Steps: 61 | Train Loss: 0.7640720 Vali Loss: 0.6302333 Test Loss: 0.4229105
EarlyStopping counter: 7 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.5036427974700928
Epoch: 41, Steps: 61 | Train Loss: 0.7636927 Vali Loss: 0.6303624 Test Loss: 0.4228566
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.439357042312622
Epoch: 42, Steps: 61 | Train Loss: 0.7635130 Vali Loss: 0.6310863 Test Loss: 0.4228041
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.2299351692199707
Epoch: 43, Steps: 61 | Train Loss: 0.7624190 Vali Loss: 0.6290491 Test Loss: 0.4227586
EarlyStopping counter: 10 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.917344570159912
Epoch: 44, Steps: 61 | Train Loss: 0.7632170 Vali Loss: 0.6300263 Test Loss: 0.4227096
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.289918899536133
Epoch: 45, Steps: 61 | Train Loss: 0.7640516 Vali Loss: 0.6323573 Test Loss: 0.4226740
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.6154229640960693
Epoch: 46, Steps: 61 | Train Loss: 0.7631281 Vali Loss: 0.6338341 Test Loss: 0.4226376
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 1.9603614807128906
Epoch: 47, Steps: 61 | Train Loss: 0.7639451 Vali Loss: 0.6303297 Test Loss: 0.4226021
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.1378173828125
Epoch: 48, Steps: 61 | Train Loss: 0.7632446 Vali Loss: 0.6280232 Test Loss: 0.4225633
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.93416428565979
Epoch: 49, Steps: 61 | Train Loss: 0.7620302 Vali Loss: 0.6333066 Test Loss: 0.4225354
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.162029504776001
Epoch: 50, Steps: 61 | Train Loss: 0.7617120 Vali Loss: 0.6252276 Test Loss: 0.4224997
Validation loss decreased (0.625392 --> 0.625228).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.2577590942382812
Epoch: 51, Steps: 61 | Train Loss: 0.7622379 Vali Loss: 0.6290814 Test Loss: 0.4224768
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.9984328746795654
Epoch: 52, Steps: 61 | Train Loss: 0.7624731 Vali Loss: 0.6285147 Test Loss: 0.4224499
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.4105546474456787
Epoch: 53, Steps: 61 | Train Loss: 0.7615050 Vali Loss: 0.6280181 Test Loss: 0.4224239
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.8850595951080322
Epoch: 54, Steps: 61 | Train Loss: 0.7627356 Vali Loss: 0.6321031 Test Loss: 0.4224040
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.363619327545166
Epoch: 55, Steps: 61 | Train Loss: 0.7628802 Vali Loss: 0.6280212 Test Loss: 0.4223779
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.060216188430786
Epoch: 56, Steps: 61 | Train Loss: 0.7633216 Vali Loss: 0.6316525 Test Loss: 0.4223589
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.2757320404052734
Epoch: 57, Steps: 61 | Train Loss: 0.7625474 Vali Loss: 0.6333857 Test Loss: 0.4223399
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.425420045852661
Epoch: 58, Steps: 61 | Train Loss: 0.7619947 Vali Loss: 0.6280607 Test Loss: 0.4223201
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.2019152641296387
Epoch: 59, Steps: 61 | Train Loss: 0.7611087 Vali Loss: 0.6289284 Test Loss: 0.4222995
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.8283350467681885
Epoch: 60, Steps: 61 | Train Loss: 0.7628431 Vali Loss: 0.6308825 Test Loss: 0.4222848
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.8303399085998535
Epoch: 61, Steps: 61 | Train Loss: 0.7629793 Vali Loss: 0.6281716 Test Loss: 0.4222690
EarlyStopping counter: 11 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.1787986755371094
Epoch: 62, Steps: 61 | Train Loss: 0.7628194 Vali Loss: 0.6292999 Test Loss: 0.4222571
EarlyStopping counter: 12 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.7241320610046387
Epoch: 63, Steps: 61 | Train Loss: 0.7626343 Vali Loss: 0.6302708 Test Loss: 0.4222407
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 1.7098956108093262
Epoch: 64, Steps: 61 | Train Loss: 0.7614107 Vali Loss: 0.6311594 Test Loss: 0.4222281
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.216153860092163
Epoch: 65, Steps: 61 | Train Loss: 0.7624220 Vali Loss: 0.6284822 Test Loss: 0.4222139
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.2126448154449463
Epoch: 66, Steps: 61 | Train Loss: 0.7623835 Vali Loss: 0.6280868 Test Loss: 0.4222038
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.310832977294922
Epoch: 67, Steps: 61 | Train Loss: 0.7623272 Vali Loss: 0.6273416 Test Loss: 0.4221930
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 3.339370012283325
Epoch: 68, Steps: 61 | Train Loss: 0.7618839 Vali Loss: 0.6304612 Test Loss: 0.4221805
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.554257392883301
Epoch: 69, Steps: 61 | Train Loss: 0.7625887 Vali Loss: 0.6271884 Test Loss: 0.4221733
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.514561176300049
Epoch: 70, Steps: 61 | Train Loss: 0.7606401 Vali Loss: 0.6327459 Test Loss: 0.4221632
EarlyStopping counter: 20 out of 20
Early stopping
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=18, out_features=162, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2612736.0
params:  3078.0
Trainable parameters:  3078
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.7149600982666016
Epoch: 1, Steps: 61 | Train Loss: 0.8547353 Vali Loss: 0.6283538 Test Loss: 0.4219003
Validation loss decreased (inf --> 0.628354).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.5130808353424072
Epoch: 2, Steps: 61 | Train Loss: 0.8525753 Vali Loss: 0.6293945 Test Loss: 0.4215459
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.019801378250122
Epoch: 3, Steps: 61 | Train Loss: 0.8529384 Vali Loss: 0.6273416 Test Loss: 0.4212701
Validation loss decreased (0.628354 --> 0.627342).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.7143585681915283
Epoch: 4, Steps: 61 | Train Loss: 0.8506975 Vali Loss: 0.6281573 Test Loss: 0.4210471
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.419661045074463
Epoch: 5, Steps: 61 | Train Loss: 0.8498243 Vali Loss: 0.6259586 Test Loss: 0.4209377
Validation loss decreased (0.627342 --> 0.625959).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.9394335746765137
Epoch: 6, Steps: 61 | Train Loss: 0.8499043 Vali Loss: 0.6285652 Test Loss: 0.4208195
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.173849582672119
Epoch: 7, Steps: 61 | Train Loss: 0.8508657 Vali Loss: 0.6264578 Test Loss: 0.4206605
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.8616254329681396
Epoch: 8, Steps: 61 | Train Loss: 0.8497265 Vali Loss: 0.6257315 Test Loss: 0.4205873
Validation loss decreased (0.625959 --> 0.625732).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.003443956375122
Epoch: 9, Steps: 61 | Train Loss: 0.8493412 Vali Loss: 0.6231695 Test Loss: 0.4205011
Validation loss decreased (0.625732 --> 0.623170).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.9266667366027832
Epoch: 10, Steps: 61 | Train Loss: 0.8479783 Vali Loss: 0.6229194 Test Loss: 0.4204463
Validation loss decreased (0.623170 --> 0.622919).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.455780506134033
Epoch: 11, Steps: 61 | Train Loss: 0.8502298 Vali Loss: 0.6232686 Test Loss: 0.4203725
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1815547943115234
Epoch: 12, Steps: 61 | Train Loss: 0.8503581 Vali Loss: 0.6241667 Test Loss: 0.4203588
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.6850364208221436
Epoch: 13, Steps: 61 | Train Loss: 0.8488300 Vali Loss: 0.6265543 Test Loss: 0.4202773
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.548790693283081
Epoch: 14, Steps: 61 | Train Loss: 0.8483688 Vali Loss: 0.6256093 Test Loss: 0.4202453
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.32437801361084
Epoch: 15, Steps: 61 | Train Loss: 0.8476673 Vali Loss: 0.6256425 Test Loss: 0.4202030
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.230938196182251
Epoch: 16, Steps: 61 | Train Loss: 0.8470793 Vali Loss: 0.6231495 Test Loss: 0.4201668
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.823791027069092
Epoch: 17, Steps: 61 | Train Loss: 0.8480252 Vali Loss: 0.6247932 Test Loss: 0.4201328
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.5631415843963623
Epoch: 18, Steps: 61 | Train Loss: 0.8487133 Vali Loss: 0.6262550 Test Loss: 0.4201159
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.111543893814087
Epoch: 19, Steps: 61 | Train Loss: 0.8480432 Vali Loss: 0.6258121 Test Loss: 0.4200895
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.7508912086486816
Epoch: 20, Steps: 61 | Train Loss: 0.8466687 Vali Loss: 0.6243777 Test Loss: 0.4200699
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.7245233058929443
Epoch: 21, Steps: 61 | Train Loss: 0.8478002 Vali Loss: 0.6247900 Test Loss: 0.4200792
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.93853759765625
Epoch: 22, Steps: 61 | Train Loss: 0.8469133 Vali Loss: 0.6242078 Test Loss: 0.4200351
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.7589361667633057
Epoch: 23, Steps: 61 | Train Loss: 0.8477547 Vali Loss: 0.6288097 Test Loss: 0.4200207
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.8277978897094727
Epoch: 24, Steps: 61 | Train Loss: 0.8467570 Vali Loss: 0.6223240 Test Loss: 0.4199966
Validation loss decreased (0.622919 --> 0.622324).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.9699406623840332
Epoch: 25, Steps: 61 | Train Loss: 0.8480652 Vali Loss: 0.6187313 Test Loss: 0.4199798
Validation loss decreased (0.622324 --> 0.618731).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.372643232345581
Epoch: 26, Steps: 61 | Train Loss: 0.8474931 Vali Loss: 0.6240055 Test Loss: 0.4199736
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8680436611175537
Epoch: 27, Steps: 61 | Train Loss: 0.8469324 Vali Loss: 0.6194938 Test Loss: 0.4199736
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.60642671585083
Epoch: 28, Steps: 61 | Train Loss: 0.8478557 Vali Loss: 0.6201143 Test Loss: 0.4199456
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.388151168823242
Epoch: 29, Steps: 61 | Train Loss: 0.8469704 Vali Loss: 0.6245160 Test Loss: 0.4199307
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.5063469409942627
Epoch: 30, Steps: 61 | Train Loss: 0.8467829 Vali Loss: 0.6207469 Test Loss: 0.4199297
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.102749824523926
Epoch: 31, Steps: 61 | Train Loss: 0.8473186 Vali Loss: 0.6201527 Test Loss: 0.4199228
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.827080249786377
Epoch: 32, Steps: 61 | Train Loss: 0.8476962 Vali Loss: 0.6236957 Test Loss: 0.4199202
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.059957265853882
Epoch: 33, Steps: 61 | Train Loss: 0.8454622 Vali Loss: 0.6219871 Test Loss: 0.4199045
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.79593563079834
Epoch: 34, Steps: 61 | Train Loss: 0.8472033 Vali Loss: 0.6208943 Test Loss: 0.4198979
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.372115135192871
Epoch: 35, Steps: 61 | Train Loss: 0.8461829 Vali Loss: 0.6247439 Test Loss: 0.4198997
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.4998836517333984
Epoch: 36, Steps: 61 | Train Loss: 0.8469559 Vali Loss: 0.6206269 Test Loss: 0.4198950
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.8433234691619873
Epoch: 37, Steps: 61 | Train Loss: 0.8458948 Vali Loss: 0.6240686 Test Loss: 0.4198872
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.056286573410034
Epoch: 38, Steps: 61 | Train Loss: 0.8458628 Vali Loss: 0.6225604 Test Loss: 0.4198840
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.291207790374756
Epoch: 39, Steps: 61 | Train Loss: 0.8470910 Vali Loss: 0.6185002 Test Loss: 0.4198753
Validation loss decreased (0.618731 --> 0.618500).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.6153767108917236
Epoch: 40, Steps: 61 | Train Loss: 0.8464138 Vali Loss: 0.6236814 Test Loss: 0.4198652
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.729109287261963
Epoch: 41, Steps: 61 | Train Loss: 0.8469446 Vali Loss: 0.6208715 Test Loss: 0.4198649
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.2700746059417725
Epoch: 42, Steps: 61 | Train Loss: 0.8461380 Vali Loss: 0.6256360 Test Loss: 0.4198573
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.218726873397827
Epoch: 43, Steps: 61 | Train Loss: 0.8470061 Vali Loss: 0.6231230 Test Loss: 0.4198573
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.263400077819824
Epoch: 44, Steps: 61 | Train Loss: 0.8452203 Vali Loss: 0.6240121 Test Loss: 0.4198533
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.8650336265563965
Epoch: 45, Steps: 61 | Train Loss: 0.8460151 Vali Loss: 0.6225677 Test Loss: 0.4198506
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.614171028137207
Epoch: 46, Steps: 61 | Train Loss: 0.8450153 Vali Loss: 0.6258361 Test Loss: 0.4198433
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.1009061336517334
Epoch: 47, Steps: 61 | Train Loss: 0.8454806 Vali Loss: 0.6217303 Test Loss: 0.4198435
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.7933659553527832
Epoch: 48, Steps: 61 | Train Loss: 0.8453438 Vali Loss: 0.6240774 Test Loss: 0.4198389
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9506404399871826
Epoch: 49, Steps: 61 | Train Loss: 0.8457479 Vali Loss: 0.6234992 Test Loss: 0.4198400
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.9681670665740967
Epoch: 50, Steps: 61 | Train Loss: 0.8469885 Vali Loss: 0.6239325 Test Loss: 0.4198352
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.1726815700531006
Epoch: 51, Steps: 61 | Train Loss: 0.8458389 Vali Loss: 0.6177735 Test Loss: 0.4198367
Validation loss decreased (0.618500 --> 0.617774).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.8386499881744385
Epoch: 52, Steps: 61 | Train Loss: 0.8466023 Vali Loss: 0.6264495 Test Loss: 0.4198352
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.3906617164611816
Epoch: 53, Steps: 61 | Train Loss: 0.8470811 Vali Loss: 0.6210160 Test Loss: 0.4198314
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.137946605682373
Epoch: 54, Steps: 61 | Train Loss: 0.8464750 Vali Loss: 0.6171831 Test Loss: 0.4198305
Validation loss decreased (0.617774 --> 0.617183).  Saving model ...
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 1.9840519428253174
Epoch: 55, Steps: 61 | Train Loss: 0.8471084 Vali Loss: 0.6233457 Test Loss: 0.4198281
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.002854824066162
Epoch: 56, Steps: 61 | Train Loss: 0.8441872 Vali Loss: 0.6219219 Test Loss: 0.4198262
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.2952022552490234
Epoch: 57, Steps: 61 | Train Loss: 0.8470181 Vali Loss: 0.6217443 Test Loss: 0.4198241
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.2657322883605957
Epoch: 58, Steps: 61 | Train Loss: 0.8463510 Vali Loss: 0.6220324 Test Loss: 0.4198232
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.284499168395996
Epoch: 59, Steps: 61 | Train Loss: 0.8455578 Vali Loss: 0.6280701 Test Loss: 0.4198194
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.3472719192504883
Epoch: 60, Steps: 61 | Train Loss: 0.8467971 Vali Loss: 0.6255044 Test Loss: 0.4198172
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.0763168334960938
Epoch: 61, Steps: 61 | Train Loss: 0.8456511 Vali Loss: 0.6237124 Test Loss: 0.4198175
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.59273099899292
Epoch: 62, Steps: 61 | Train Loss: 0.8456358 Vali Loss: 0.6235011 Test Loss: 0.4198163
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.921616792678833
Epoch: 63, Steps: 61 | Train Loss: 0.8453966 Vali Loss: 0.6207951 Test Loss: 0.4198158
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.2584121227264404
Epoch: 64, Steps: 61 | Train Loss: 0.8467041 Vali Loss: 0.6240978 Test Loss: 0.4198094
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.426511764526367
Epoch: 65, Steps: 61 | Train Loss: 0.8457414 Vali Loss: 0.6213633 Test Loss: 0.4198128
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 1.9911723136901855
Epoch: 66, Steps: 61 | Train Loss: 0.8436349 Vali Loss: 0.6234421 Test Loss: 0.4198113
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 1.6627118587493896
Epoch: 67, Steps: 61 | Train Loss: 0.8467796 Vali Loss: 0.6165252 Test Loss: 0.4198098
Validation loss decreased (0.617183 --> 0.616525).  Saving model ...
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.329500675201416
Epoch: 68, Steps: 61 | Train Loss: 0.8452605 Vali Loss: 0.6212733 Test Loss: 0.4198097
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.3206098079681396
Epoch: 69, Steps: 61 | Train Loss: 0.8464150 Vali Loss: 0.6236346 Test Loss: 0.4198076
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 3.198695659637451
Epoch: 70, Steps: 61 | Train Loss: 0.8450092 Vali Loss: 0.6225204 Test Loss: 0.4198069
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.1100194454193115
Epoch: 71, Steps: 61 | Train Loss: 0.8468272 Vali Loss: 0.6250604 Test Loss: 0.4198058
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.984137535095215
Epoch: 72, Steps: 61 | Train Loss: 0.8451966 Vali Loss: 0.6219373 Test Loss: 0.4198058
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.883162260055542
Epoch: 73, Steps: 61 | Train Loss: 0.8461403 Vali Loss: 0.6230677 Test Loss: 0.4198053
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.1858198642730713
Epoch: 74, Steps: 61 | Train Loss: 0.8454962 Vali Loss: 0.6227580 Test Loss: 0.4198055
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.5692899227142334
Epoch: 75, Steps: 61 | Train Loss: 0.8451230 Vali Loss: 0.6254380 Test Loss: 0.4198049
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.7252907752990723
Epoch: 76, Steps: 61 | Train Loss: 0.8467373 Vali Loss: 0.6229974 Test Loss: 0.4198039
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.6722869873046875
Epoch: 77, Steps: 61 | Train Loss: 0.8469515 Vali Loss: 0.6245518 Test Loss: 0.4198023
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.8854258060455322
Epoch: 78, Steps: 61 | Train Loss: 0.8462445 Vali Loss: 0.6255020 Test Loss: 0.4198030
EarlyStopping counter: 11 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.5623667240142822
Epoch: 79, Steps: 61 | Train Loss: 0.8456459 Vali Loss: 0.6235214 Test Loss: 0.4198018
EarlyStopping counter: 12 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.9496746063232422
Epoch: 80, Steps: 61 | Train Loss: 0.8448200 Vali Loss: 0.6230841 Test Loss: 0.4198016
EarlyStopping counter: 13 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.146465539932251
Epoch: 81, Steps: 61 | Train Loss: 0.8447199 Vali Loss: 0.6204826 Test Loss: 0.4198007
EarlyStopping counter: 14 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.5772554874420166
Epoch: 82, Steps: 61 | Train Loss: 0.8455549 Vali Loss: 0.6227728 Test Loss: 0.4197999
EarlyStopping counter: 15 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.7796270847320557
Epoch: 83, Steps: 61 | Train Loss: 0.8467577 Vali Loss: 0.6186925 Test Loss: 0.4198008
EarlyStopping counter: 16 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.1714794635772705
Epoch: 84, Steps: 61 | Train Loss: 0.8450829 Vali Loss: 0.6200209 Test Loss: 0.4198002
EarlyStopping counter: 17 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.425781726837158
Epoch: 85, Steps: 61 | Train Loss: 0.8463281 Vali Loss: 0.6223766 Test Loss: 0.4198001
EarlyStopping counter: 18 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.4756722450256348
Epoch: 86, Steps: 61 | Train Loss: 0.8460441 Vali Loss: 0.6192427 Test Loss: 0.4197986
EarlyStopping counter: 19 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.3725266456604004
Epoch: 87, Steps: 61 | Train Loss: 0.8466275 Vali Loss: 0.6243930 Test Loss: 0.4197981
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.41822582483291626, mae:0.43658703565597534, rse:0.5169054865837097, corr:[ 0.21887793  0.22084852  0.21886425  0.21755153  0.21724115  0.21667086
  0.21564624  0.21415386  0.21366099  0.21314311  0.21207559  0.21032085
  0.20866166  0.2079214   0.2074394   0.20688978  0.20621191  0.20581682
  0.2054664   0.20461336  0.20357461  0.20269781  0.20186214  0.20045073
  0.19792116  0.19590318  0.19434065  0.19351728  0.19307229  0.1924316
  0.19190305  0.19075799  0.18980183  0.18896633  0.18830776  0.18731222
  0.18610163  0.18533142  0.18468456  0.18401217  0.18345666  0.18310185
  0.18297903  0.18234709  0.18180662  0.18134509  0.18049219  0.17861277
  0.17543972  0.17315319  0.17124353  0.17001686  0.16922212  0.16869187
  0.16849512  0.16703962  0.16596071  0.16520202  0.1651592   0.1647997
  0.1639268   0.16377148  0.16403894  0.16436517  0.16468969  0.16493347
  0.16504927  0.1646412   0.16432312  0.16440095  0.16416025  0.16314861
  0.16106525  0.16015953  0.1596898   0.15933761  0.15898278  0.15909101
  0.15958592  0.1588967   0.1583381   0.15805145  0.15803576  0.15786958
  0.15754053  0.15769278  0.15794148  0.15788503  0.15774919  0.15776624
  0.15782028  0.1573546   0.15703556  0.15698622  0.15692034  0.15635186
  0.15465306  0.15354596  0.15255658  0.15192266  0.15136239  0.15078242
  0.15104292  0.15050645  0.15031499  0.15023443  0.1505271   0.15044348
  0.14975548  0.14947212  0.14939132  0.14936517  0.1491489   0.14883468
  0.14878659  0.14848235  0.14815201  0.14760081  0.14655371  0.145151
  0.14308904  0.14187986  0.14055374  0.1396628   0.13884617  0.13838777
  0.13834968  0.13759147  0.13718185  0.1368137   0.13684252  0.13642903
  0.13550371  0.13481341  0.13437079  0.13408259  0.13380507  0.13326989
  0.13263664  0.13185337  0.13159238  0.13148898  0.13053522  0.12877488
  0.125894    0.12406148  0.12265921  0.12183687  0.12129148  0.12072271
  0.12072357  0.12011138  0.11991842  0.11973441  0.11973311  0.11927748
  0.11847329  0.11810111  0.11781623  0.11768929  0.11764824  0.11761361
  0.11758783  0.11722825  0.11736397  0.11749745  0.1170468   0.11566052
  0.11311371  0.11174171  0.1108354   0.11040617  0.11020266  0.11030018
  0.11082181  0.11047567  0.11033041  0.11006377  0.11019837  0.10974452
  0.10897676  0.10854112  0.10832079  0.1084524   0.10846588  0.10864501
  0.10903372  0.10908602  0.10905579  0.1094647   0.10975182  0.10957927
  0.10831337  0.10790537  0.10782145  0.10809425  0.10861244  0.10926377
  0.11078147  0.11142682  0.11194152  0.11203587  0.11232598  0.11221606
  0.11179674  0.11160888  0.11154046  0.11166392  0.11179324  0.11217333
  0.11253405  0.11247125  0.11256688  0.11253613  0.11238563  0.11150671
  0.10974555  0.10883338  0.10798658  0.10763153  0.10742052  0.10819635
  0.10986691  0.11089087  0.1113885   0.11153357  0.11187132  0.11174615
  0.11141405  0.1113051   0.1114694   0.11184214  0.11234     0.11283938
  0.11326974  0.11344462  0.11368861  0.11417221  0.11436839  0.11409257
  0.11282391  0.11223846  0.11181775  0.11190538  0.11221689  0.11288899
  0.11454208  0.11515863  0.11562329  0.11577155  0.11630682  0.11650166
  0.11631303  0.11651269  0.11660542  0.11688333  0.11711384  0.11770155
  0.1181467   0.1182779   0.11844674  0.11877494  0.1190349   0.11891836
  0.1178451   0.11771356  0.11792757  0.11842305  0.118917    0.12018423
  0.1221201   0.12289437  0.12339174  0.12341899  0.12371276  0.1234006
  0.12306239  0.12296691  0.12331463  0.12368309  0.12391856  0.12412483
  0.12451613  0.12461299  0.1247542   0.12510411  0.12536918  0.1252221
  0.12440959  0.12410871  0.12384732  0.12371805  0.12375396  0.12393026
  0.12453478  0.12467252  0.12518851  0.12551482  0.12572913  0.12550312
  0.12508287  0.12513034  0.12543112  0.12580475  0.12590751  0.12619625
  0.12658465  0.1264594   0.12645553  0.12653838  0.12658629  0.12642409
  0.12520257  0.12473228  0.12469628  0.12487055  0.12487607  0.12524584
  0.12624952  0.12652066  0.12679757  0.12704405  0.12758054  0.12765105
  0.12738039  0.12724708  0.12744181  0.12783074  0.12811005  0.1283381
  0.12865168  0.12883161  0.12905379  0.12954025  0.12998454  0.13025695
  0.12923723  0.12889327  0.1289608   0.12912793  0.12973754  0.13076934
  0.13268204  0.13357516  0.1338943   0.13392894  0.13435887  0.13471238
  0.13458514  0.13441645  0.13453291  0.1347321   0.13503441  0.13552505
  0.13621847  0.13648447  0.13660675  0.13728686  0.13806556  0.13835184
  0.13771492  0.13762337  0.1378089   0.13823998  0.13903086  0.14055115
  0.1430178   0.1449132   0.1463308   0.14701077  0.14754386  0.1480032
  0.14847997  0.14920282  0.14989538  0.15042695  0.15082774  0.15168387
  0.1526213   0.1532729   0.15368693  0.1542263   0.15484487  0.15541044
  0.15530412  0.15547955  0.1559095   0.15658337  0.15778193  0.15944947
  0.16188127  0.16316001  0.16414124  0.16491733  0.16565864  0.16583696
  0.16573493  0.16602288  0.16629337  0.16635843  0.16625127  0.16623421
  0.1666338   0.16680132  0.16684005  0.16691129  0.16689637  0.16691542
  0.16609432  0.16602954  0.1661678   0.16637804  0.16674985  0.16755058
  0.16894506  0.16952923  0.16977991  0.17001235  0.17037906  0.17038332
  0.16993801  0.16957085  0.16944717  0.16946977  0.16940305  0.16930126
  0.16932575  0.16927794  0.1693154   0.16957913  0.16968413  0.16950363
  0.16893414  0.16874689  0.16865359  0.16854641  0.16848437  0.16866799
  0.16976053  0.1700154   0.1699876   0.169809    0.1697685   0.16966118
  0.16932708  0.16916026  0.16900441  0.1689715   0.16891284  0.16893367
  0.16898729  0.16893047  0.16888714  0.1689857   0.16909227  0.16897075
  0.16824193  0.1677965   0.16725183  0.16711284  0.16713092  0.16720882
  0.16765127  0.16753635  0.16738737  0.16723463  0.16723455  0.16678111
  0.16611882  0.16577615  0.16551419  0.16538675  0.16519956  0.16496299
  0.1647825   0.16450974  0.16432214  0.16429691  0.16395435  0.16316232
  0.16164257  0.16071908  0.15999259  0.15903509  0.15821181  0.1575621
  0.15741858  0.1566236   0.15588549  0.15529534  0.15485297  0.15416403
  0.15324245  0.15272774  0.15244913  0.15227678  0.15201327  0.15167639
  0.1514258   0.15114515  0.15100476  0.15092486  0.15035559  0.14899233
  0.14675035  0.14503847  0.14376596  0.14267486  0.14173178  0.14115995
  0.14097428  0.14046428  0.1401168   0.1397293   0.13937567  0.13864255
  0.1377365   0.13706587  0.13670936  0.13649411  0.13621488  0.13573061
  0.13499743  0.13427496  0.13386345  0.13386439  0.13320147  0.13142583
  0.12846968  0.1262053   0.12456782  0.12280301  0.12098512  0.11930125
  0.11858137  0.11764058  0.11713952  0.11671395  0.11632497  0.11555503
  0.11452734  0.11386099  0.11311835  0.11226914  0.11139841  0.11091603
  0.11043317  0.10980888  0.10908111  0.10832313  0.10714921  0.10505541
  0.10165893  0.09929515  0.0972928   0.09579388  0.09459315  0.09384035
  0.09354472  0.09239936  0.09189817  0.09158654  0.09148932  0.09093371
  0.08994352  0.08928757  0.08877915  0.08803136  0.08730773  0.08683907
  0.08646658  0.08578485  0.08485845  0.08376031  0.08221501  0.08017322
  0.07731114  0.07522395  0.07353175  0.07198502  0.07082896  0.07000918
  0.06977891  0.06899676  0.06848972  0.06818912  0.06786927  0.06709629
  0.06595395  0.06520927  0.06482467  0.06458644  0.06432891  0.06405568
  0.06383219  0.06352247  0.06322656  0.06272977  0.06138396  0.05920077
  0.05642496  0.05478103  0.05328991  0.05187398  0.05053924  0.04964026
  0.04936825  0.04857638  0.04800559  0.04766133  0.04769051  0.04727997
  0.04651783  0.04606382  0.04579595  0.04563111  0.04533761  0.04502118
  0.04470186  0.04430736  0.04412937  0.04414156  0.04307071  0.04086157
  0.03773586  0.03590264  0.03440054  0.03288483  0.03140128  0.03047394
  0.03033453  0.02942974  0.02880702  0.02845557  0.02827085  0.02779741
  0.02683016  0.02582577  0.02499217  0.02445363  0.02383291  0.02306265
  0.02225815  0.02133793  0.02114615  0.02133695  0.02082975  0.01870719
  0.01539239  0.01317911  0.01172662  0.01060962  0.00927683  0.00795431
  0.00732846  0.00671     0.00654632  0.00632841  0.00613035  0.00543838
  0.00459691  0.00419218  0.00386986  0.00361722  0.00348774  0.00348616
  0.00351344  0.00312019  0.00275437  0.00297388  0.0029223   0.00164337
 -0.0012711  -0.0039924  -0.00600512 -0.00725005 -0.00822459 -0.00922378
 -0.00962418 -0.01021821 -0.00990864 -0.00929092 -0.00898735 -0.00973691
 -0.01083359 -0.01130051 -0.01127741 -0.01144715 -0.01179849 -0.01175484
 -0.01111676 -0.01098954 -0.01138033 -0.01179592 -0.01267299 -0.01399474
 -0.01695798 -0.01941827 -0.02158658 -0.02274697 -0.02289994 -0.02269522
 -0.02219178 -0.02282038 -0.02284086 -0.02255935 -0.02193766 -0.02259601
 -0.02377073 -0.02449363 -0.02459451 -0.02488855 -0.02581231 -0.02612846
 -0.02523687 -0.02433151 -0.02461337 -0.02574789 -0.02530903 -0.02101212]
