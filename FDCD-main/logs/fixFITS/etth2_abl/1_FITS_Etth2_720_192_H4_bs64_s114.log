Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_192', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=192, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7729
val 2689
test 2689
Model(
  (freq_upsampler): Linear(in_features=134, out_features=169, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  20290816.0
params:  22815.0
Trainable parameters:  22815
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 1.789930820465088
Epoch: 1, Steps: 60 | Train Loss: 0.7170463 Vali Loss: 0.3830353 Test Loss: 0.3975316
Validation loss decreased (inf --> 0.383035).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8333308696746826
Epoch: 2, Steps: 60 | Train Loss: 0.5901807 Vali Loss: 0.3381181 Test Loss: 0.3746203
Validation loss decreased (0.383035 --> 0.338118).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.7610044479370117
Epoch: 3, Steps: 60 | Train Loss: 0.5614665 Vali Loss: 0.3214540 Test Loss: 0.3667433
Validation loss decreased (0.338118 --> 0.321454).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.8835210800170898
Epoch: 4, Steps: 60 | Train Loss: 0.5486720 Vali Loss: 0.3127768 Test Loss: 0.3629096
Validation loss decreased (0.321454 --> 0.312777).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.0155832767486572
Epoch: 5, Steps: 60 | Train Loss: 0.5406499 Vali Loss: 0.3077451 Test Loss: 0.3608119
Validation loss decreased (0.312777 --> 0.307745).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.8332915306091309
Epoch: 6, Steps: 60 | Train Loss: 0.5352027 Vali Loss: 0.3034539 Test Loss: 0.3594156
Validation loss decreased (0.307745 --> 0.303454).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.712937593460083
Epoch: 7, Steps: 60 | Train Loss: 0.5324242 Vali Loss: 0.2998640 Test Loss: 0.3587346
Validation loss decreased (0.303454 --> 0.299864).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 1.7622430324554443
Epoch: 8, Steps: 60 | Train Loss: 0.5280949 Vali Loss: 0.2979407 Test Loss: 0.3580570
Validation loss decreased (0.299864 --> 0.297941).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.7255778312683105
Epoch: 9, Steps: 60 | Train Loss: 0.5261908 Vali Loss: 0.2967356 Test Loss: 0.3574413
Validation loss decreased (0.297941 --> 0.296736).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.768728494644165
Epoch: 10, Steps: 60 | Train Loss: 0.5236733 Vali Loss: 0.2949209 Test Loss: 0.3570849
Validation loss decreased (0.296736 --> 0.294921).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.8565051555633545
Epoch: 11, Steps: 60 | Train Loss: 0.5241383 Vali Loss: 0.2936459 Test Loss: 0.3567907
Validation loss decreased (0.294921 --> 0.293646).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.805910348892212
Epoch: 12, Steps: 60 | Train Loss: 0.5213227 Vali Loss: 0.2929058 Test Loss: 0.3564339
Validation loss decreased (0.293646 --> 0.292906).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.476407766342163
Epoch: 13, Steps: 60 | Train Loss: 0.5224838 Vali Loss: 0.2919727 Test Loss: 0.3563277
Validation loss decreased (0.292906 --> 0.291973).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.8780725002288818
Epoch: 14, Steps: 60 | Train Loss: 0.5209989 Vali Loss: 0.2909401 Test Loss: 0.3561246
Validation loss decreased (0.291973 --> 0.290940).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.6630284786224365
Epoch: 15, Steps: 60 | Train Loss: 0.5194643 Vali Loss: 0.2902671 Test Loss: 0.3558236
Validation loss decreased (0.290940 --> 0.290267).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.9725730419158936
Epoch: 16, Steps: 60 | Train Loss: 0.5184551 Vali Loss: 0.2898085 Test Loss: 0.3556789
Validation loss decreased (0.290267 --> 0.289808).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.9212281703948975
Epoch: 17, Steps: 60 | Train Loss: 0.5192034 Vali Loss: 0.2893532 Test Loss: 0.3554396
Validation loss decreased (0.289808 --> 0.289353).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8669648170471191
Epoch: 18, Steps: 60 | Train Loss: 0.5189951 Vali Loss: 0.2888948 Test Loss: 0.3551828
Validation loss decreased (0.289353 --> 0.288895).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.8208131790161133
Epoch: 19, Steps: 60 | Train Loss: 0.5182752 Vali Loss: 0.2881970 Test Loss: 0.3552517
Validation loss decreased (0.288895 --> 0.288197).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.0484023094177246
Epoch: 20, Steps: 60 | Train Loss: 0.5170317 Vali Loss: 0.2880411 Test Loss: 0.3550214
Validation loss decreased (0.288197 --> 0.288041).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.817631483078003
Epoch: 21, Steps: 60 | Train Loss: 0.5163485 Vali Loss: 0.2874451 Test Loss: 0.3550293
Validation loss decreased (0.288041 --> 0.287445).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.8169567584991455
Epoch: 22, Steps: 60 | Train Loss: 0.5157097 Vali Loss: 0.2873270 Test Loss: 0.3549433
Validation loss decreased (0.287445 --> 0.287327).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6580700874328613
Epoch: 23, Steps: 60 | Train Loss: 0.5155720 Vali Loss: 0.2869189 Test Loss: 0.3546869
Validation loss decreased (0.287327 --> 0.286919).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.759213924407959
Epoch: 24, Steps: 60 | Train Loss: 0.5131974 Vali Loss: 0.2867777 Test Loss: 0.3546688
Validation loss decreased (0.286919 --> 0.286778).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 1.7930235862731934
Epoch: 25, Steps: 60 | Train Loss: 0.5161285 Vali Loss: 0.2864388 Test Loss: 0.3545591
Validation loss decreased (0.286778 --> 0.286439).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.947209358215332
Epoch: 26, Steps: 60 | Train Loss: 0.5143997 Vali Loss: 0.2862588 Test Loss: 0.3545614
Validation loss decreased (0.286439 --> 0.286259).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.8531267642974854
Epoch: 27, Steps: 60 | Train Loss: 0.5154020 Vali Loss: 0.2859859 Test Loss: 0.3544790
Validation loss decreased (0.286259 --> 0.285986).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.7331013679504395
Epoch: 28, Steps: 60 | Train Loss: 0.5141795 Vali Loss: 0.2858420 Test Loss: 0.3544441
Validation loss decreased (0.285986 --> 0.285842).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.613497257232666
Epoch: 29, Steps: 60 | Train Loss: 0.5147370 Vali Loss: 0.2855192 Test Loss: 0.3543465
Validation loss decreased (0.285842 --> 0.285519).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.7072958946228027
Epoch: 30, Steps: 60 | Train Loss: 0.5142830 Vali Loss: 0.2855657 Test Loss: 0.3543648
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.797558307647705
Epoch: 31, Steps: 60 | Train Loss: 0.5136790 Vali Loss: 0.2851320 Test Loss: 0.3542446
Validation loss decreased (0.285519 --> 0.285132).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.9040541648864746
Epoch: 32, Steps: 60 | Train Loss: 0.5147708 Vali Loss: 0.2851609 Test Loss: 0.3542537
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.8816046714782715
Epoch: 33, Steps: 60 | Train Loss: 0.5142019 Vali Loss: 0.2852274 Test Loss: 0.3541971
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.8885676860809326
Epoch: 34, Steps: 60 | Train Loss: 0.5138597 Vali Loss: 0.2849614 Test Loss: 0.3541890
Validation loss decreased (0.285132 --> 0.284961).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.7331006526947021
Epoch: 35, Steps: 60 | Train Loss: 0.5127677 Vali Loss: 0.2846847 Test Loss: 0.3540876
Validation loss decreased (0.284961 --> 0.284685).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.9519107341766357
Epoch: 36, Steps: 60 | Train Loss: 0.5124188 Vali Loss: 0.2847154 Test Loss: 0.3540932
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.7887396812438965
Epoch: 37, Steps: 60 | Train Loss: 0.5134074 Vali Loss: 0.2845430 Test Loss: 0.3540614
Validation loss decreased (0.284685 --> 0.284543).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.9577116966247559
Epoch: 38, Steps: 60 | Train Loss: 0.5135041 Vali Loss: 0.2845968 Test Loss: 0.3540187
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 1.8094096183776855
Epoch: 39, Steps: 60 | Train Loss: 0.5137841 Vali Loss: 0.2843814 Test Loss: 0.3540424
Validation loss decreased (0.284543 --> 0.284381).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.823622703552246
Epoch: 40, Steps: 60 | Train Loss: 0.5131674 Vali Loss: 0.2843769 Test Loss: 0.3539679
Validation loss decreased (0.284381 --> 0.284377).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.9548163414001465
Epoch: 41, Steps: 60 | Train Loss: 0.5131801 Vali Loss: 0.2842444 Test Loss: 0.3540311
Validation loss decreased (0.284377 --> 0.284244).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 1.7049229145050049
Epoch: 42, Steps: 60 | Train Loss: 0.5135692 Vali Loss: 0.2842702 Test Loss: 0.3539725
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0135042667388916
Epoch: 43, Steps: 60 | Train Loss: 0.5121262 Vali Loss: 0.2841415 Test Loss: 0.3539147
Validation loss decreased (0.284244 --> 0.284142).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.1667072772979736
Epoch: 44, Steps: 60 | Train Loss: 0.5112767 Vali Loss: 0.2841591 Test Loss: 0.3538857
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.048579454421997
Epoch: 45, Steps: 60 | Train Loss: 0.5105165 Vali Loss: 0.2840525 Test Loss: 0.3538928
Validation loss decreased (0.284142 --> 0.284053).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.9477427005767822
Epoch: 46, Steps: 60 | Train Loss: 0.5116690 Vali Loss: 0.2840853 Test Loss: 0.3539002
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.0156171321868896
Epoch: 47, Steps: 60 | Train Loss: 0.5122386 Vali Loss: 0.2840559 Test Loss: 0.3538700
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.8249144554138184
Epoch: 48, Steps: 60 | Train Loss: 0.5134516 Vali Loss: 0.2839689 Test Loss: 0.3538588
Validation loss decreased (0.284053 --> 0.283969).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.07881236076355
Epoch: 49, Steps: 60 | Train Loss: 0.5110383 Vali Loss: 0.2838893 Test Loss: 0.3538373
Validation loss decreased (0.283969 --> 0.283889).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3042690753936768
Epoch: 50, Steps: 60 | Train Loss: 0.5112824 Vali Loss: 0.2838951 Test Loss: 0.3538119
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.7697415351867676
Epoch: 51, Steps: 60 | Train Loss: 0.5120662 Vali Loss: 0.2838269 Test Loss: 0.3538227
Validation loss decreased (0.283889 --> 0.283827).  Saving model ...
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.1510732173919678
Epoch: 52, Steps: 60 | Train Loss: 0.5108822 Vali Loss: 0.2836761 Test Loss: 0.3538190
Validation loss decreased (0.283827 --> 0.283676).  Saving model ...
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.783975839614868
Epoch: 53, Steps: 60 | Train Loss: 0.5121488 Vali Loss: 0.2835482 Test Loss: 0.3537825
Validation loss decreased (0.283676 --> 0.283548).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.225929021835327
Epoch: 54, Steps: 60 | Train Loss: 0.5113252 Vali Loss: 0.2836812 Test Loss: 0.3537857
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.3859517574310303
Epoch: 55, Steps: 60 | Train Loss: 0.5104001 Vali Loss: 0.2836712 Test Loss: 0.3537655
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.25423264503479
Epoch: 56, Steps: 60 | Train Loss: 0.5126813 Vali Loss: 0.2836869 Test Loss: 0.3537635
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.2268130779266357
Epoch: 57, Steps: 60 | Train Loss: 0.5115325 Vali Loss: 0.2836706 Test Loss: 0.3537393
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.5191190242767334
Epoch: 58, Steps: 60 | Train Loss: 0.5119600 Vali Loss: 0.2833314 Test Loss: 0.3537610
Validation loss decreased (0.283548 --> 0.283331).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.271733045578003
Epoch: 59, Steps: 60 | Train Loss: 0.5119979 Vali Loss: 0.2835885 Test Loss: 0.3537431
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.020050525665283
Epoch: 60, Steps: 60 | Train Loss: 0.5105538 Vali Loss: 0.2835287 Test Loss: 0.3537373
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 1.9435369968414307
Epoch: 61, Steps: 60 | Train Loss: 0.5113768 Vali Loss: 0.2833506 Test Loss: 0.3537345
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.1253867149353027
Epoch: 62, Steps: 60 | Train Loss: 0.5116665 Vali Loss: 0.2835416 Test Loss: 0.3537209
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.9374356269836426
Epoch: 63, Steps: 60 | Train Loss: 0.5119482 Vali Loss: 0.2833527 Test Loss: 0.3537171
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.3408517837524414
Epoch: 64, Steps: 60 | Train Loss: 0.5103912 Vali Loss: 0.2834833 Test Loss: 0.3537005
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.339888572692871
Epoch: 65, Steps: 60 | Train Loss: 0.5104996 Vali Loss: 0.2834526 Test Loss: 0.3537059
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.1415951251983643
Epoch: 66, Steps: 60 | Train Loss: 0.5123845 Vali Loss: 0.2834559 Test Loss: 0.3536953
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.288503646850586
Epoch: 67, Steps: 60 | Train Loss: 0.5125752 Vali Loss: 0.2834398 Test Loss: 0.3536948
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.11251163482666
Epoch: 68, Steps: 60 | Train Loss: 0.5116686 Vali Loss: 0.2833773 Test Loss: 0.3536863
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.108231782913208
Epoch: 69, Steps: 60 | Train Loss: 0.5108249 Vali Loss: 0.2833056 Test Loss: 0.3536906
Validation loss decreased (0.283331 --> 0.283306).  Saving model ...
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.2831344604492188
Epoch: 70, Steps: 60 | Train Loss: 0.5118748 Vali Loss: 0.2833522 Test Loss: 0.3536816
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.1463565826416016
Epoch: 71, Steps: 60 | Train Loss: 0.5116639 Vali Loss: 0.2834259 Test Loss: 0.3536798
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 1.9283483028411865
Epoch: 72, Steps: 60 | Train Loss: 0.5112003 Vali Loss: 0.2833401 Test Loss: 0.3536785
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.088195323944092
Epoch: 73, Steps: 60 | Train Loss: 0.5120324 Vali Loss: 0.2833520 Test Loss: 0.3536713
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.2801947593688965
Epoch: 74, Steps: 60 | Train Loss: 0.5111641 Vali Loss: 0.2833615 Test Loss: 0.3536749
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.178870916366577
Epoch: 75, Steps: 60 | Train Loss: 0.5109404 Vali Loss: 0.2832844 Test Loss: 0.3536749
Validation loss decreased (0.283306 --> 0.283284).  Saving model ...
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.151663303375244
Epoch: 76, Steps: 60 | Train Loss: 0.5123691 Vali Loss: 0.2832957 Test Loss: 0.3536704
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 1.9862675666809082
Epoch: 77, Steps: 60 | Train Loss: 0.5117919 Vali Loss: 0.2833250 Test Loss: 0.3536624
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.0702197551727295
Epoch: 78, Steps: 60 | Train Loss: 0.5119108 Vali Loss: 0.2832964 Test Loss: 0.3536697
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.235490083694458
Epoch: 79, Steps: 60 | Train Loss: 0.5104373 Vali Loss: 0.2833444 Test Loss: 0.3536590
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 1.9670453071594238
Epoch: 80, Steps: 60 | Train Loss: 0.5117238 Vali Loss: 0.2832672 Test Loss: 0.3536536
Validation loss decreased (0.283284 --> 0.283267).  Saving model ...
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 2.3333609104156494
Epoch: 81, Steps: 60 | Train Loss: 0.5111442 Vali Loss: 0.2829769 Test Loss: 0.3536637
Validation loss decreased (0.283267 --> 0.282977).  Saving model ...
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.2551238536834717
Epoch: 82, Steps: 60 | Train Loss: 0.5128581 Vali Loss: 0.2832881 Test Loss: 0.3536505
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 1.8953917026519775
Epoch: 83, Steps: 60 | Train Loss: 0.5114247 Vali Loss: 0.2832993 Test Loss: 0.3536469
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.6398236751556396
Epoch: 84, Steps: 60 | Train Loss: 0.5101080 Vali Loss: 0.2828862 Test Loss: 0.3536491
Validation loss decreased (0.282977 --> 0.282886).  Saving model ...
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 1.9539592266082764
Epoch: 85, Steps: 60 | Train Loss: 0.5119745 Vali Loss: 0.2833047 Test Loss: 0.3536439
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.1709580421447754
Epoch: 86, Steps: 60 | Train Loss: 0.5104232 Vali Loss: 0.2831998 Test Loss: 0.3536472
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.032700538635254
Epoch: 87, Steps: 60 | Train Loss: 0.5114059 Vali Loss: 0.2831952 Test Loss: 0.3536450
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.326009750366211
Epoch: 88, Steps: 60 | Train Loss: 0.5107329 Vali Loss: 0.2832502 Test Loss: 0.3536428
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.12082839012146
Epoch: 89, Steps: 60 | Train Loss: 0.5108060 Vali Loss: 0.2832123 Test Loss: 0.3536404
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.515817403793335
Epoch: 90, Steps: 60 | Train Loss: 0.5122239 Vali Loss: 0.2828551 Test Loss: 0.3536399
Validation loss decreased (0.282886 --> 0.282855).  Saving model ...
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 2.1261465549468994
Epoch: 91, Steps: 60 | Train Loss: 0.5106222 Vali Loss: 0.2832219 Test Loss: 0.3536421
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.031338691711426
Epoch: 92, Steps: 60 | Train Loss: 0.5099750 Vali Loss: 0.2832316 Test Loss: 0.3536333
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.223522186279297
Epoch: 93, Steps: 60 | Train Loss: 0.5098454 Vali Loss: 0.2831947 Test Loss: 0.3536372
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.0217506885528564
Epoch: 94, Steps: 60 | Train Loss: 0.5123547 Vali Loss: 0.2832301 Test Loss: 0.3536374
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.3489155769348145
Epoch: 95, Steps: 60 | Train Loss: 0.5112857 Vali Loss: 0.2831834 Test Loss: 0.3536360
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.3877644538879395
Epoch: 96, Steps: 60 | Train Loss: 0.5115915 Vali Loss: 0.2831811 Test Loss: 0.3536309
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.01747989654541
Epoch: 97, Steps: 60 | Train Loss: 0.5098046 Vali Loss: 0.2831988 Test Loss: 0.3536311
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.3966925144195557
Epoch: 98, Steps: 60 | Train Loss: 0.5112767 Vali Loss: 0.2831425 Test Loss: 0.3536287
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.2353367805480957
Epoch: 99, Steps: 60 | Train Loss: 0.5107735 Vali Loss: 0.2832285 Test Loss: 0.3536265
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.0520403385162354
Epoch: 100, Steps: 60 | Train Loss: 0.5112868 Vali Loss: 0.2831937 Test Loss: 0.3536270
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.1160680107021042e-06
>>>>>>>testing : ETTh2_720_192_FITS_ETTh2_ftM_sl720_ll48_pl192_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2689
mse:0.3323095738887787, mae:0.37508752942085266, rse:0.46228766441345215, corr:[0.26227802 0.26763964 0.26667842 0.26537    0.2654177  0.26593336
 0.26559454 0.26443028 0.26339072 0.26235718 0.26118582 0.2598091
 0.2586803  0.25791228 0.25722608 0.25665084 0.25604832 0.25527495
 0.25433108 0.2532929  0.25213975 0.25086394 0.24935536 0.24758665
 0.2457668  0.24402867 0.24255784 0.24131724 0.24013281 0.23871702
 0.23712441 0.23551202 0.23438488 0.23353817 0.23262756 0.23121738
 0.22962393 0.22861017 0.2283221  0.22808573 0.22736599 0.22617677
 0.22494364 0.22412127 0.22359012 0.22275805 0.2212251  0.21918382
 0.21728666 0.21589294 0.21470337 0.21312863 0.21129651 0.20948793
 0.20763308 0.20590498 0.2041601  0.20235682 0.20102817 0.20029676
 0.20002422 0.19954605 0.19894232 0.19849826 0.19812375 0.19797668
 0.19760922 0.19684324 0.19592884 0.19520633 0.19463831 0.19403464
 0.19301271 0.19163138 0.19026521 0.18912031 0.18847735 0.18796161
 0.18726848 0.18632203 0.18586576 0.18565194 0.18535152 0.1848342
 0.18433142 0.18414675 0.18409793 0.18393154 0.1833578  0.18264554
 0.18209416 0.18173517 0.18180606 0.18177919 0.18130568 0.18045938
 0.17954105 0.17884196 0.17823426 0.17732601 0.17610118 0.17487526
 0.17420189 0.1737894  0.17344697 0.17286138 0.17219013 0.17186734
 0.17145818 0.17090425 0.17006347 0.16930692 0.16868055 0.16851102
 0.16842566 0.16791698 0.16694716 0.16557463 0.16435224 0.16317312
 0.16195668 0.16036932 0.15884203 0.15773806 0.1569771  0.15627125
 0.15536878 0.15407638 0.1529355  0.15226929 0.15203652 0.15142202
 0.15036824 0.1492387  0.14861637 0.14863937 0.14858407 0.14793831
 0.14683594 0.14597674 0.14578635 0.14571215 0.14501889 0.14336275
 0.14118192 0.13949427 0.13843536 0.13739252 0.13595559 0.13450585
 0.133841   0.13349237 0.1333374  0.13271202 0.13164374 0.1306919
 0.13074121 0.131296   0.13149816 0.13123873 0.13061796 0.13055797
 0.13087109 0.13074303 0.13004008 0.12931547 0.12935899 0.12938456
 0.12858258 0.12651695 0.12458622 0.12369812 0.12400222 0.12345956
 0.12131299 0.11815178 0.11637558 0.1170791  0.11777018 0.11698142
 0.1149232  0.1141414  0.11618432 0.11884236 0.11838266 0.1156902
 0.11459662 0.11674933 0.11964101 0.11861584 0.11718127 0.12570089]
