Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=42, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8125
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=42, out_features=120, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  4515840.0
params:  5160.0
Trainable parameters:  5160
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.009756326675415
Epoch: 1, Steps: 63 | Train Loss: 0.8499374 Vali Loss: 0.4542868 Test Loss: 0.4868397
Validation loss decreased (inf --> 0.454287).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.682551622390747
Epoch: 2, Steps: 63 | Train Loss: 0.7288924 Vali Loss: 0.4194717 Test Loss: 0.4402660
Validation loss decreased (0.454287 --> 0.419472).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.073068857192993
Epoch: 3, Steps: 63 | Train Loss: 0.6821894 Vali Loss: 0.3996654 Test Loss: 0.4211261
Validation loss decreased (0.419472 --> 0.399665).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.1309456825256348
Epoch: 4, Steps: 63 | Train Loss: 0.6605464 Vali Loss: 0.3892165 Test Loss: 0.4122117
Validation loss decreased (0.399665 --> 0.389216).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.072580337524414
Epoch: 5, Steps: 63 | Train Loss: 0.6510488 Vali Loss: 0.3849758 Test Loss: 0.4073376
Validation loss decreased (0.389216 --> 0.384976).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.1041176319122314
Epoch: 6, Steps: 63 | Train Loss: 0.6441440 Vali Loss: 0.3817457 Test Loss: 0.4047011
Validation loss decreased (0.384976 --> 0.381746).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.6487443447113037
Epoch: 7, Steps: 63 | Train Loss: 0.6419822 Vali Loss: 0.3814415 Test Loss: 0.4027213
Validation loss decreased (0.381746 --> 0.381442).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.111313819885254
Epoch: 8, Steps: 63 | Train Loss: 0.6387554 Vali Loss: 0.3777156 Test Loss: 0.4013347
Validation loss decreased (0.381442 --> 0.377716).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.1301727294921875
Epoch: 9, Steps: 63 | Train Loss: 0.6349523 Vali Loss: 0.3747272 Test Loss: 0.4003535
Validation loss decreased (0.377716 --> 0.374727).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.388601779937744
Epoch: 10, Steps: 63 | Train Loss: 0.6334869 Vali Loss: 0.3759253 Test Loss: 0.3995778
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.715700387954712
Epoch: 11, Steps: 63 | Train Loss: 0.6322586 Vali Loss: 0.3742958 Test Loss: 0.3988397
Validation loss decreased (0.374727 --> 0.374296).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.3622467517852783
Epoch: 12, Steps: 63 | Train Loss: 0.6313939 Vali Loss: 0.3746615 Test Loss: 0.3984401
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 1.7929139137268066
Epoch: 13, Steps: 63 | Train Loss: 0.6303715 Vali Loss: 0.3735636 Test Loss: 0.3979858
Validation loss decreased (0.374296 --> 0.373564).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.372924327850342
Epoch: 14, Steps: 63 | Train Loss: 0.6291494 Vali Loss: 0.3732433 Test Loss: 0.3975983
Validation loss decreased (0.373564 --> 0.373243).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.262192726135254
Epoch: 15, Steps: 63 | Train Loss: 0.6306584 Vali Loss: 0.3727459 Test Loss: 0.3972193
Validation loss decreased (0.373243 --> 0.372746).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.972449779510498
Epoch: 16, Steps: 63 | Train Loss: 0.6286543 Vali Loss: 0.3725219 Test Loss: 0.3968346
Validation loss decreased (0.372746 --> 0.372522).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.9361472129821777
Epoch: 17, Steps: 63 | Train Loss: 0.6282411 Vali Loss: 0.3722123 Test Loss: 0.3966530
Validation loss decreased (0.372522 --> 0.372212).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.8379218578338623
Epoch: 18, Steps: 63 | Train Loss: 0.6272752 Vali Loss: 0.3699650 Test Loss: 0.3963380
Validation loss decreased (0.372212 --> 0.369965).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.1926748752593994
Epoch: 19, Steps: 63 | Train Loss: 0.6285301 Vali Loss: 0.3726013 Test Loss: 0.3962223
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.5007619857788086
Epoch: 20, Steps: 63 | Train Loss: 0.6257115 Vali Loss: 0.3691511 Test Loss: 0.3959907
Validation loss decreased (0.369965 --> 0.369151).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0599353313446045
Epoch: 21, Steps: 63 | Train Loss: 0.6254252 Vali Loss: 0.3688936 Test Loss: 0.3958810
Validation loss decreased (0.369151 --> 0.368894).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.035731792449951
Epoch: 22, Steps: 63 | Train Loss: 0.6254878 Vali Loss: 0.3683802 Test Loss: 0.3956833
Validation loss decreased (0.368894 --> 0.368380).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.4426395893096924
Epoch: 23, Steps: 63 | Train Loss: 0.6252949 Vali Loss: 0.3708073 Test Loss: 0.3956080
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.646841287612915
Epoch: 24, Steps: 63 | Train Loss: 0.6261387 Vali Loss: 0.3668570 Test Loss: 0.3953678
Validation loss decreased (0.368380 --> 0.366857).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.9336557388305664
Epoch: 25, Steps: 63 | Train Loss: 0.6245764 Vali Loss: 0.3675921 Test Loss: 0.3952750
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.0466058254241943
Epoch: 26, Steps: 63 | Train Loss: 0.6240232 Vali Loss: 0.3684435 Test Loss: 0.3951527
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.5918986797332764
Epoch: 27, Steps: 63 | Train Loss: 0.6255200 Vali Loss: 0.3671780 Test Loss: 0.3950754
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.1492717266082764
Epoch: 28, Steps: 63 | Train Loss: 0.6247880 Vali Loss: 0.3693230 Test Loss: 0.3949733
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.6175153255462646
Epoch: 29, Steps: 63 | Train Loss: 0.6236895 Vali Loss: 0.3691377 Test Loss: 0.3949485
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.4178640842437744
Epoch: 30, Steps: 63 | Train Loss: 0.6231173 Vali Loss: 0.3677393 Test Loss: 0.3949016
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.750910520553589
Epoch: 31, Steps: 63 | Train Loss: 0.6244686 Vali Loss: 0.3681161 Test Loss: 0.3948000
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.364537000656128
Epoch: 32, Steps: 63 | Train Loss: 0.6237915 Vali Loss: 0.3676809 Test Loss: 0.3947480
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.296941041946411
Epoch: 33, Steps: 63 | Train Loss: 0.6225944 Vali Loss: 0.3661381 Test Loss: 0.3946372
Validation loss decreased (0.366857 --> 0.366138).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.643176317214966
Epoch: 34, Steps: 63 | Train Loss: 0.6219868 Vali Loss: 0.3675174 Test Loss: 0.3946169
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.8522276878356934
Epoch: 35, Steps: 63 | Train Loss: 0.6228753 Vali Loss: 0.3654940 Test Loss: 0.3945650
Validation loss decreased (0.366138 --> 0.365494).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.694202423095703
Epoch: 36, Steps: 63 | Train Loss: 0.6235947 Vali Loss: 0.3700934 Test Loss: 0.3945426
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.8687117099761963
Epoch: 37, Steps: 63 | Train Loss: 0.6244462 Vali Loss: 0.3672566 Test Loss: 0.3944852
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.271987199783325
Epoch: 38, Steps: 63 | Train Loss: 0.6230360 Vali Loss: 0.3641307 Test Loss: 0.3944533
Validation loss decreased (0.365494 --> 0.364131).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.4654576778411865
Epoch: 39, Steps: 63 | Train Loss: 0.6232585 Vali Loss: 0.3709525 Test Loss: 0.3944251
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.068448305130005
Epoch: 40, Steps: 63 | Train Loss: 0.6218378 Vali Loss: 0.3676930 Test Loss: 0.3943748
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.259307384490967
Epoch: 41, Steps: 63 | Train Loss: 0.6232218 Vali Loss: 0.3684162 Test Loss: 0.3943419
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.752448558807373
Epoch: 42, Steps: 63 | Train Loss: 0.6221742 Vali Loss: 0.3698340 Test Loss: 0.3942836
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.490290641784668
Epoch: 43, Steps: 63 | Train Loss: 0.6213615 Vali Loss: 0.3678717 Test Loss: 0.3943210
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.865811347961426
Epoch: 44, Steps: 63 | Train Loss: 0.6219146 Vali Loss: 0.3693649 Test Loss: 0.3942663
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.3274974822998047
Epoch: 45, Steps: 63 | Train Loss: 0.6218281 Vali Loss: 0.3674494 Test Loss: 0.3942524
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.083653688430786
Epoch: 46, Steps: 63 | Train Loss: 0.6224162 Vali Loss: 0.3657991 Test Loss: 0.3942087
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.280927896499634
Epoch: 47, Steps: 63 | Train Loss: 0.6189490 Vali Loss: 0.3695310 Test Loss: 0.3942106
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.128936529159546
Epoch: 48, Steps: 63 | Train Loss: 0.6219945 Vali Loss: 0.3675536 Test Loss: 0.3941622
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.2781693935394287
Epoch: 49, Steps: 63 | Train Loss: 0.6224129 Vali Loss: 0.3669249 Test Loss: 0.3941363
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.4235146045684814
Epoch: 50, Steps: 63 | Train Loss: 0.6216682 Vali Loss: 0.3683720 Test Loss: 0.3941262
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.968583106994629
Epoch: 51, Steps: 63 | Train Loss: 0.6220936 Vali Loss: 0.3655231 Test Loss: 0.3940978
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.603533983230591
Epoch: 52, Steps: 63 | Train Loss: 0.6220362 Vali Loss: 0.3666184 Test Loss: 0.3940968
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.927614212036133
Epoch: 53, Steps: 63 | Train Loss: 0.6226619 Vali Loss: 0.3668953 Test Loss: 0.3940890
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.184703826904297
Epoch: 54, Steps: 63 | Train Loss: 0.6229857 Vali Loss: 0.3657238 Test Loss: 0.3940669
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.1871252059936523
Epoch: 55, Steps: 63 | Train Loss: 0.6204665 Vali Loss: 0.3690469 Test Loss: 0.3940487
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.2999587059020996
Epoch: 56, Steps: 63 | Train Loss: 0.6201915 Vali Loss: 0.3648253 Test Loss: 0.3940211
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.5340960025787354
Epoch: 57, Steps: 63 | Train Loss: 0.6227938 Vali Loss: 0.3653127 Test Loss: 0.3940240
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.9071009159088135
Epoch: 58, Steps: 63 | Train Loss: 0.6232714 Vali Loss: 0.3673151 Test Loss: 0.3939999
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_336_FITS_ETTh2_ftM_sl180_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.38978952169418335, mae:0.4136907160282135, rse:0.49917706847190857, corr:[0.2620978  0.2671138  0.26403114 0.26248026 0.26286814 0.26183116
 0.2596144  0.258445   0.2580797  0.25687    0.25506106 0.25367168
 0.2528168  0.25158465 0.2503766  0.24970059 0.24921612 0.24815755
 0.24687313 0.2459136  0.245276   0.2444283  0.24297407 0.24108914
 0.23904845 0.23724914 0.23572002 0.23449431 0.23321001 0.23163527
 0.22984344 0.2282641  0.22689874 0.22543056 0.2238323  0.22251342
 0.22151    0.22031641 0.21884823 0.21766707 0.21703632 0.21620452
 0.21477276 0.21337721 0.21255755 0.21157071 0.20957595 0.20677948
 0.20408818 0.20195296 0.19976693 0.1976864  0.1960196  0.19462292
 0.19270313 0.19056213 0.18920381 0.1882405  0.18713719 0.18557706
 0.18450038 0.18400685 0.18382126 0.18319824 0.18261297 0.18232976
 0.18172939 0.18057437 0.17966864 0.17947735 0.17889917 0.17741153
 0.17524537 0.17378452 0.17277513 0.17149445 0.16995873 0.16901611
 0.16855702 0.16797313 0.16742998 0.16692151 0.16672373 0.16653371
 0.1662858  0.16580603 0.1656571  0.16557893 0.16516635 0.164279
 0.16369128 0.16363806 0.1639057  0.16372824 0.16302533 0.16235638
 0.16144446 0.16023113 0.15868218 0.1575917  0.15691191 0.15630342
 0.15570506 0.1552155  0.15528351 0.1553316  0.15530497 0.15501842
 0.15499341 0.15475684 0.15425964 0.15350549 0.15298712 0.15281408
 0.15251005 0.15177515 0.15120693 0.15094416 0.15036078 0.14896253
 0.14701185 0.1454884  0.14425074 0.1430272  0.14146598 0.14017384
 0.13943467 0.13886365 0.13806441 0.13708274 0.13642678 0.13602327
 0.13572769 0.13514017 0.13474603 0.13459092 0.13436177 0.13369654
 0.13301045 0.13270509 0.13271661 0.13237679 0.13139749 0.13003334
 0.12809838 0.12631175 0.12431136 0.122751   0.12168156 0.12101957
 0.12047221 0.11976427 0.11946335 0.11956152 0.11986586 0.11974329
 0.11990961 0.11997853 0.12005004 0.11998589 0.12005464 0.12003148
 0.119775   0.11943788 0.11938838 0.11966973 0.11954267 0.11839644
 0.11642364 0.11509526 0.11408958 0.11318742 0.11192262 0.11058798
 0.10997389 0.10969409 0.10948651 0.10890029 0.10879747 0.10905845
 0.10942027 0.10928818 0.10916867 0.10942273 0.1097671  0.10973935
 0.10964736 0.10969108 0.10993662 0.11018073 0.11020579 0.11003273
 0.10945959 0.10881317 0.10790145 0.10756919 0.10759253 0.10753267
 0.10710707 0.10674845 0.10713865 0.10762898 0.10790777 0.10758381
 0.10767379 0.1079322  0.1079369  0.10762446 0.10757469 0.10812783
 0.1085676  0.10862967 0.10871058 0.10911824 0.10939011 0.10866109
 0.10711133 0.1059351  0.10541158 0.10495003 0.10389502 0.10345316
 0.10382632 0.1045702  0.10470021 0.10457738 0.10489297 0.10506396
 0.10481168 0.10434148 0.10476008 0.10556115 0.10585763 0.10569587
 0.10599872 0.10704484 0.10795105 0.10815284 0.1078926  0.10769188
 0.10691056 0.10583815 0.10467252 0.10449205 0.10441335 0.10367548
 0.10268792 0.10297599 0.10427599 0.10488228 0.10492338 0.10509812
 0.10646276 0.10771649 0.10802905 0.10818046 0.1088485  0.10981315
 0.10999404 0.10992011 0.11054479 0.11154944 0.11207148 0.11185104
 0.11130129 0.11112214 0.11077888 0.11045378 0.11010042 0.11052695
 0.11080201 0.11040681 0.11028563 0.11093443 0.1120873  0.11223856
 0.11224248 0.1126065  0.11376402 0.11435926 0.11426622 0.11421193
 0.11472533 0.11509599 0.11512353 0.11524785 0.11570176 0.11633912
 0.11569701 0.1146732  0.11396987 0.11423367 0.11376334 0.11269563
 0.11214761 0.11259755 0.11360323 0.11352305 0.11381376 0.11515999
 0.11689278 0.11702704 0.11689828 0.11786264 0.11933763 0.1200469
 0.11980867 0.11999827 0.12075354 0.1211444  0.12087748 0.12046614
 0.11979904 0.11901945 0.11756117 0.11699072 0.11696938 0.11674726
 0.115086   0.11441384 0.11603292 0.11758351 0.11755669 0.11706935
 0.11915892 0.12080759 0.12042169 0.11864731 0.11866108 0.11985891
 0.11928488 0.1158714  0.11336011 0.1141792  0.11381675 0.10614439]
