Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=134, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=134, out_features=196, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  23532544.0
params:  26460.0
Trainable parameters:  26460
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.166743516921997
Epoch: 1, Steps: 59 | Train Loss: 0.8363830 Vali Loss: 0.5104020 Test Loss: 0.3918421
Validation loss decreased (inf --> 0.510402).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.1661949157714844
Epoch: 2, Steps: 59 | Train Loss: 0.7029930 Vali Loss: 0.4557406 Test Loss: 0.3709455
Validation loss decreased (0.510402 --> 0.455741).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.2728776931762695
Epoch: 3, Steps: 59 | Train Loss: 0.6689901 Vali Loss: 0.4312020 Test Loss: 0.3654664
Validation loss decreased (0.455741 --> 0.431202).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.2118749618530273
Epoch: 4, Steps: 59 | Train Loss: 0.6547708 Vali Loss: 0.4197089 Test Loss: 0.3629361
Validation loss decreased (0.431202 --> 0.419709).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.086826801300049
Epoch: 5, Steps: 59 | Train Loss: 0.6457542 Vali Loss: 0.4131670 Test Loss: 0.3615666
Validation loss decreased (0.419709 --> 0.413167).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.9939873218536377
Epoch: 6, Steps: 59 | Train Loss: 0.6393170 Vali Loss: 0.4065233 Test Loss: 0.3609001
Validation loss decreased (0.413167 --> 0.406523).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.942516565322876
Epoch: 7, Steps: 59 | Train Loss: 0.6367584 Vali Loss: 0.4032224 Test Loss: 0.3603677
Validation loss decreased (0.406523 --> 0.403222).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.121056079864502
Epoch: 8, Steps: 59 | Train Loss: 0.6335933 Vali Loss: 0.4001245 Test Loss: 0.3600498
Validation loss decreased (0.403222 --> 0.400124).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.1512951850891113
Epoch: 9, Steps: 59 | Train Loss: 0.6320876 Vali Loss: 0.3990048 Test Loss: 0.3597580
Validation loss decreased (0.400124 --> 0.399005).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.190767288208008
Epoch: 10, Steps: 59 | Train Loss: 0.6284726 Vali Loss: 0.3963576 Test Loss: 0.3595134
Validation loss decreased (0.399005 --> 0.396358).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.1493353843688965
Epoch: 11, Steps: 59 | Train Loss: 0.6264711 Vali Loss: 0.3945263 Test Loss: 0.3595027
Validation loss decreased (0.396358 --> 0.394526).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.1410577297210693
Epoch: 12, Steps: 59 | Train Loss: 0.6259463 Vali Loss: 0.3906314 Test Loss: 0.3595001
Validation loss decreased (0.394526 --> 0.390631).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.078721046447754
Epoch: 13, Steps: 59 | Train Loss: 0.6259678 Vali Loss: 0.3910799 Test Loss: 0.3593825
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.086777925491333
Epoch: 14, Steps: 59 | Train Loss: 0.6245454 Vali Loss: 0.3910786 Test Loss: 0.3593249
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.090900421142578
Epoch: 15, Steps: 59 | Train Loss: 0.6227797 Vali Loss: 0.3890278 Test Loss: 0.3592024
Validation loss decreased (0.390631 --> 0.389028).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.469470500946045
Epoch: 16, Steps: 59 | Train Loss: 0.6224670 Vali Loss: 0.3886331 Test Loss: 0.3594050
Validation loss decreased (0.389028 --> 0.388633).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.082750082015991
Epoch: 17, Steps: 59 | Train Loss: 0.6222987 Vali Loss: 0.3892124 Test Loss: 0.3591285
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.9963302612304688
Epoch: 18, Steps: 59 | Train Loss: 0.6213896 Vali Loss: 0.3893594 Test Loss: 0.3589541
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.9187357425689697
Epoch: 19, Steps: 59 | Train Loss: 0.6203804 Vali Loss: 0.3871764 Test Loss: 0.3592281
Validation loss decreased (0.388633 --> 0.387176).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.022610664367676
Epoch: 20, Steps: 59 | Train Loss: 0.6194991 Vali Loss: 0.3886631 Test Loss: 0.3592214
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.0452916622161865
Epoch: 21, Steps: 59 | Train Loss: 0.6198924 Vali Loss: 0.3881105 Test Loss: 0.3591163
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.497145891189575
Epoch: 22, Steps: 59 | Train Loss: 0.6196630 Vali Loss: 0.3879384 Test Loss: 0.3589786
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.0907249450683594
Epoch: 23, Steps: 59 | Train Loss: 0.6182065 Vali Loss: 0.3853472 Test Loss: 0.3589987
Validation loss decreased (0.387176 --> 0.385347).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.2121481895446777
Epoch: 24, Steps: 59 | Train Loss: 0.6188571 Vali Loss: 0.3862863 Test Loss: 0.3590106
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.128781795501709
Epoch: 25, Steps: 59 | Train Loss: 0.6185611 Vali Loss: 0.3867891 Test Loss: 0.3589774
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.4916675090789795
Epoch: 26, Steps: 59 | Train Loss: 0.6174332 Vali Loss: 0.3847220 Test Loss: 0.3589738
Validation loss decreased (0.385347 --> 0.384722).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.0251598358154297
Epoch: 27, Steps: 59 | Train Loss: 0.6192134 Vali Loss: 0.3839281 Test Loss: 0.3590052
Validation loss decreased (0.384722 --> 0.383928).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.2195045948028564
Epoch: 28, Steps: 59 | Train Loss: 0.6164679 Vali Loss: 0.3852602 Test Loss: 0.3590606
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.097508192062378
Epoch: 29, Steps: 59 | Train Loss: 0.6172919 Vali Loss: 0.3838139 Test Loss: 0.3589642
Validation loss decreased (0.383928 --> 0.383814).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.0483529567718506
Epoch: 30, Steps: 59 | Train Loss: 0.6182004 Vali Loss: 0.3856808 Test Loss: 0.3589947
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.2443816661834717
Epoch: 31, Steps: 59 | Train Loss: 0.6178046 Vali Loss: 0.3824041 Test Loss: 0.3589338
Validation loss decreased (0.383814 --> 0.382404).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.9764506816864014
Epoch: 32, Steps: 59 | Train Loss: 0.6179534 Vali Loss: 0.3846247 Test Loss: 0.3589613
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.3540101051330566
Epoch: 33, Steps: 59 | Train Loss: 0.6169549 Vali Loss: 0.3829569 Test Loss: 0.3589660
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.1506547927856445
Epoch: 34, Steps: 59 | Train Loss: 0.6161229 Vali Loss: 0.3824664 Test Loss: 0.3588743
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.4485766887664795
Epoch: 35, Steps: 59 | Train Loss: 0.6153874 Vali Loss: 0.3825556 Test Loss: 0.3589328
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.7483327388763428
Epoch: 36, Steps: 59 | Train Loss: 0.6142749 Vali Loss: 0.3834231 Test Loss: 0.3589469
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.6748504638671875
Epoch: 37, Steps: 59 | Train Loss: 0.6152248 Vali Loss: 0.3829278 Test Loss: 0.3589361
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.535752296447754
Epoch: 38, Steps: 59 | Train Loss: 0.6165886 Vali Loss: 0.3819749 Test Loss: 0.3588831
Validation loss decreased (0.382404 --> 0.381975).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.3097691535949707
Epoch: 39, Steps: 59 | Train Loss: 0.6154481 Vali Loss: 0.3832692 Test Loss: 0.3588713
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.4465813636779785
Epoch: 40, Steps: 59 | Train Loss: 0.6152358 Vali Loss: 0.3808740 Test Loss: 0.3588882
Validation loss decreased (0.381975 --> 0.380874).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.4049296379089355
Epoch: 41, Steps: 59 | Train Loss: 0.6151531 Vali Loss: 0.3809246 Test Loss: 0.3589686
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.57800555229187
Epoch: 42, Steps: 59 | Train Loss: 0.6160006 Vali Loss: 0.3815145 Test Loss: 0.3589212
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.512873888015747
Epoch: 43, Steps: 59 | Train Loss: 0.6167201 Vali Loss: 0.3832916 Test Loss: 0.3589601
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.137451171875
Epoch: 44, Steps: 59 | Train Loss: 0.6145348 Vali Loss: 0.3802255 Test Loss: 0.3589364
Validation loss decreased (0.380874 --> 0.380226).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.761477470397949
Epoch: 45, Steps: 59 | Train Loss: 0.6166649 Vali Loss: 0.3794644 Test Loss: 0.3589251
Validation loss decreased (0.380226 --> 0.379464).  Saving model ...
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.403463363647461
Epoch: 46, Steps: 59 | Train Loss: 0.6160626 Vali Loss: 0.3857858 Test Loss: 0.3589128
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.7624855041503906
Epoch: 47, Steps: 59 | Train Loss: 0.6154560 Vali Loss: 0.3827800 Test Loss: 0.3589612
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.3289637565612793
Epoch: 48, Steps: 59 | Train Loss: 0.6154053 Vali Loss: 0.3814330 Test Loss: 0.3589088
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.1387240886688232
Epoch: 49, Steps: 59 | Train Loss: 0.6148944 Vali Loss: 0.3829941 Test Loss: 0.3589118
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.0148513317108154
Epoch: 50, Steps: 59 | Train Loss: 0.6134991 Vali Loss: 0.3822910 Test Loss: 0.3589102
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.0992093086242676
Epoch: 51, Steps: 59 | Train Loss: 0.6157333 Vali Loss: 0.3834590 Test Loss: 0.3589169
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.3476650714874268
Epoch: 52, Steps: 59 | Train Loss: 0.6161610 Vali Loss: 0.3810821 Test Loss: 0.3589292
EarlyStopping counter: 7 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.10604190826416
Epoch: 53, Steps: 59 | Train Loss: 0.6151519 Vali Loss: 0.3824403 Test Loss: 0.3589505
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.086709976196289
Epoch: 54, Steps: 59 | Train Loss: 0.6139173 Vali Loss: 0.3822269 Test Loss: 0.3589101
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.1041696071624756
Epoch: 55, Steps: 59 | Train Loss: 0.6145715 Vali Loss: 0.3784161 Test Loss: 0.3589258
Validation loss decreased (0.379464 --> 0.378416).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.179051160812378
Epoch: 56, Steps: 59 | Train Loss: 0.6151043 Vali Loss: 0.3812883 Test Loss: 0.3589107
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.158123254776001
Epoch: 57, Steps: 59 | Train Loss: 0.6141406 Vali Loss: 0.3813612 Test Loss: 0.3589357
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 1.9503881931304932
Epoch: 58, Steps: 59 | Train Loss: 0.6151490 Vali Loss: 0.3812755 Test Loss: 0.3589256
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.273322105407715
Epoch: 59, Steps: 59 | Train Loss: 0.6149163 Vali Loss: 0.3816478 Test Loss: 0.3589179
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.317577362060547
Epoch: 60, Steps: 59 | Train Loss: 0.6146908 Vali Loss: 0.3820243 Test Loss: 0.3589492
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.2711563110351562
Epoch: 61, Steps: 59 | Train Loss: 0.6141470 Vali Loss: 0.3810059 Test Loss: 0.3589308
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.0040931701660156
Epoch: 62, Steps: 59 | Train Loss: 0.6136545 Vali Loss: 0.3824266 Test Loss: 0.3589310
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.9800004959106445
Epoch: 63, Steps: 59 | Train Loss: 0.6137177 Vali Loss: 0.3811576 Test Loss: 0.3589295
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.070666551589966
Epoch: 64, Steps: 59 | Train Loss: 0.6150055 Vali Loss: 0.3810973 Test Loss: 0.3589239
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.071779489517212
Epoch: 65, Steps: 59 | Train Loss: 0.6132789 Vali Loss: 0.3803217 Test Loss: 0.3589233
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 3.056658983230591
Epoch: 66, Steps: 59 | Train Loss: 0.6156250 Vali Loss: 0.3794813 Test Loss: 0.3589304
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.1225290298461914
Epoch: 67, Steps: 59 | Train Loss: 0.6140705 Vali Loss: 0.3796732 Test Loss: 0.3589259
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.1727049350738525
Epoch: 68, Steps: 59 | Train Loss: 0.6135358 Vali Loss: 0.3793582 Test Loss: 0.3589273
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 1.9393837451934814
Epoch: 69, Steps: 59 | Train Loss: 0.6142299 Vali Loss: 0.3812608 Test Loss: 0.3589297
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.1390459537506104
Epoch: 70, Steps: 59 | Train Loss: 0.6139336 Vali Loss: 0.3809844 Test Loss: 0.3589290
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.5121712684631348
Epoch: 71, Steps: 59 | Train Loss: 0.6147304 Vali Loss: 0.3816984 Test Loss: 0.3589244
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 2.36673903465271
Epoch: 72, Steps: 59 | Train Loss: 0.6153819 Vali Loss: 0.3817948 Test Loss: 0.3589364
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.3260326385498047
Epoch: 73, Steps: 59 | Train Loss: 0.6147179 Vali Loss: 0.3809862 Test Loss: 0.3589241
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.208700656890869
Epoch: 74, Steps: 59 | Train Loss: 0.6133984 Vali Loss: 0.3824300 Test Loss: 0.3589254
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 2.373810052871704
Epoch: 75, Steps: 59 | Train Loss: 0.6127588 Vali Loss: 0.3817098 Test Loss: 0.3589318
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_336_FITS_ETTh2_ftM_sl720_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.35469499230384827, mae:0.3960169851779938, rse:0.4761755168437958, corr:[0.25817516 0.26419172 0.26225108 0.26049373 0.2610451  0.26189086
 0.26097977 0.25900188 0.25787136 0.25745472 0.25678986 0.2552574
 0.25371245 0.252791   0.25247675 0.2521766  0.25136346 0.250047
 0.24898742 0.24841031 0.24787325 0.24690509 0.24529561 0.24370615
 0.24251257 0.2416816  0.24067865 0.23944473 0.23834027 0.23752868
 0.23679416 0.23584852 0.23473631 0.23370743 0.23297496 0.2323904
 0.23172073 0.23093744 0.23013538 0.22946167 0.22889063 0.22816491
 0.22721417 0.22626662 0.22537237 0.22448581 0.22324832 0.22156826
 0.21981281 0.21833971 0.21707761 0.21581496 0.21451351 0.2130026
 0.21114628 0.20932016 0.20768726 0.20626031 0.20516717 0.2043704
 0.20376857 0.20322378 0.20287752 0.20278624 0.20254281 0.20218311
 0.20153329 0.20088327 0.20035873 0.20003854 0.19951381 0.19864492
 0.19753385 0.19644614 0.19554819 0.1946739  0.19385412 0.19303125
 0.19233993 0.19182552 0.19156756 0.19116153 0.19053394 0.1899763
 0.18978643 0.18977065 0.18952344 0.1889148  0.1881795  0.18773457
 0.18773699 0.1877716  0.1877367  0.18752047 0.18722738 0.18696424
 0.18669158 0.1861862  0.18547897 0.18460153 0.18393162 0.18357356
 0.18351282 0.18324275 0.18280573 0.18224365 0.18193425 0.18177381
 0.18141006 0.18092519 0.18031743 0.17984207 0.17937385 0.17901304
 0.17857654 0.17802109 0.17742272 0.17658576 0.17560922 0.17441149
 0.1731788  0.17214206 0.17142996 0.17088304 0.17019868 0.16939688
 0.16844831 0.16761836 0.16697495 0.166355   0.1656895  0.16486971
 0.1641131  0.16345899 0.16300434 0.16263042 0.16210672 0.16143309
 0.1606867  0.16006403 0.1596425  0.15919949 0.15848425 0.15727735
 0.15564631 0.15405266 0.15275379 0.15178318 0.15096962 0.15023035
 0.1494613  0.14861324 0.14800653 0.14751984 0.14696108 0.14618148
 0.1454441  0.14495061 0.14477484 0.14466691 0.14418156 0.14346902
 0.14275736 0.14233378 0.14229228 0.1423169  0.14200239 0.1410008
 0.13970155 0.13843724 0.13748065 0.13667606 0.1358068  0.13454144
 0.13324332 0.13192886 0.1309546  0.13030663 0.12961213 0.1289414
 0.1283477  0.12794457 0.12770341 0.12752737 0.12703006 0.12660812
 0.1264216  0.12637629 0.12645362 0.1264945  0.12666063 0.12654954
 0.12618662 0.12565826 0.12495198 0.12438897 0.12394992 0.12349962
 0.12300408 0.12239244 0.12195325 0.12172618 0.12165769 0.12145787
 0.12100159 0.12062291 0.1204122  0.12064444 0.12106106 0.12140038
 0.12137004 0.12122497 0.12123896 0.1213849  0.1214169  0.12082311
 0.11963782 0.11828685 0.11728726 0.11699973 0.11697029 0.11704837
 0.11667921 0.11631609 0.11604872 0.11595853 0.11588928 0.11550979
 0.11480523 0.11419646 0.11386818 0.11411665 0.11453012 0.11501572
 0.11539736 0.11577944 0.11611775 0.11636688 0.1163867  0.116109
 0.11559764 0.11500932 0.11449208 0.1142723  0.11400087 0.11354182
 0.11334823 0.11333433 0.11343648 0.11344191 0.11343168 0.11370768
 0.11410911 0.11486126 0.11561407 0.11676371 0.11772585 0.1187015
 0.11915595 0.11927531 0.1194499  0.11976918 0.12071154 0.1215586
 0.12186094 0.12133455 0.12087762 0.1210864  0.12156923 0.1217991
 0.1213994  0.12071256 0.12058415 0.12085252 0.12165248 0.12185061
 0.12139282 0.12078072 0.12077057 0.12152159 0.12235192 0.12247229
 0.12182681 0.12129545 0.12128188 0.12159644 0.12185808 0.12170109
 0.12119322 0.12072313 0.12033188 0.11989534 0.11899739 0.11770836
 0.11702257 0.11716601 0.11757644 0.11727517 0.11646383 0.11573064
 0.1154177  0.11599762 0.11678305 0.11749278 0.11749142 0.11792342
 0.1183126  0.11872136 0.11880343 0.1188809  0.11925242 0.11959236
 0.11956212 0.11843073 0.11715979 0.11643956 0.11643602 0.11639678
 0.11570877 0.11482011 0.11449137 0.11602976 0.11821904 0.11941696
 0.11837187 0.11703812 0.11793672 0.12065606 0.12232278 0.12127002
 0.11904908 0.11860784 0.12069342 0.12206126 0.12094972 0.11678575]
