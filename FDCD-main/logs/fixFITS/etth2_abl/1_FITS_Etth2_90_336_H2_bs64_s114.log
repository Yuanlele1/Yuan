Args in experiment:
Namespace(H_order=2, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=18, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H2_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=18, out_features=85, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  1370880.0
params:  1615.0
Trainable parameters:  1615
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.198026180267334
Epoch: 1, Steps: 64 | Train Loss: 0.9626725 Vali Loss: 0.4972556 Test Loss: 0.5864822
Validation loss decreased (inf --> 0.497256).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.225036859512329
Epoch: 2, Steps: 64 | Train Loss: 0.8390349 Vali Loss: 0.4486203 Test Loss: 0.5255348
Validation loss decreased (0.497256 --> 0.448620).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8834171295166016
Epoch: 3, Steps: 64 | Train Loss: 0.7714908 Vali Loss: 0.4248752 Test Loss: 0.4912470
Validation loss decreased (0.448620 --> 0.424875).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.293043375015259
Epoch: 4, Steps: 64 | Train Loss: 0.7355718 Vali Loss: 0.4100089 Test Loss: 0.4702411
Validation loss decreased (0.424875 --> 0.410009).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 3.2463507652282715
Epoch: 5, Steps: 64 | Train Loss: 0.7113676 Vali Loss: 0.3986073 Test Loss: 0.4568405
Validation loss decreased (0.410009 --> 0.398607).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 3.122394323348999
Epoch: 6, Steps: 64 | Train Loss: 0.6970767 Vali Loss: 0.3917306 Test Loss: 0.4480719
Validation loss decreased (0.398607 --> 0.391731).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.879323959350586
Epoch: 7, Steps: 64 | Train Loss: 0.6861275 Vali Loss: 0.3833767 Test Loss: 0.4420196
Validation loss decreased (0.391731 --> 0.383377).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.0747101306915283
Epoch: 8, Steps: 64 | Train Loss: 0.6780000 Vali Loss: 0.3804040 Test Loss: 0.4378366
Validation loss decreased (0.383377 --> 0.380404).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.9597909450531006
Epoch: 9, Steps: 64 | Train Loss: 0.6742793 Vali Loss: 0.3814155 Test Loss: 0.4348318
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.3987088203430176
Epoch: 10, Steps: 64 | Train Loss: 0.6706713 Vali Loss: 0.3776293 Test Loss: 0.4326814
Validation loss decreased (0.380404 --> 0.377629).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 1.9582159519195557
Epoch: 11, Steps: 64 | Train Loss: 0.6675138 Vali Loss: 0.3772176 Test Loss: 0.4310587
Validation loss decreased (0.377629 --> 0.377218).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 1.8859193325042725
Epoch: 12, Steps: 64 | Train Loss: 0.6644496 Vali Loss: 0.3759181 Test Loss: 0.4297960
Validation loss decreased (0.377218 --> 0.375918).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.5775985717773438
Epoch: 13, Steps: 64 | Train Loss: 0.6623431 Vali Loss: 0.3722390 Test Loss: 0.4288257
Validation loss decreased (0.375918 --> 0.372239).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.093938112258911
Epoch: 14, Steps: 64 | Train Loss: 0.6621483 Vali Loss: 0.3726582 Test Loss: 0.4280913
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.846191167831421
Epoch: 15, Steps: 64 | Train Loss: 0.6606959 Vali Loss: 0.3726049 Test Loss: 0.4275222
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.414344310760498
Epoch: 16, Steps: 64 | Train Loss: 0.6592897 Vali Loss: 0.3736472 Test Loss: 0.4270113
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.424384593963623
Epoch: 17, Steps: 64 | Train Loss: 0.6586663 Vali Loss: 0.3671831 Test Loss: 0.4266650
Validation loss decreased (0.372239 --> 0.367183).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.9178311824798584
Epoch: 18, Steps: 64 | Train Loss: 0.6583636 Vali Loss: 0.3702088 Test Loss: 0.4263113
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.219130754470825
Epoch: 19, Steps: 64 | Train Loss: 0.6567620 Vali Loss: 0.3700303 Test Loss: 0.4259959
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.2777578830718994
Epoch: 20, Steps: 64 | Train Loss: 0.6568459 Vali Loss: 0.3689674 Test Loss: 0.4257579
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.138829469680786
Epoch: 21, Steps: 64 | Train Loss: 0.6578143 Vali Loss: 0.3723880 Test Loss: 0.4255461
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 1.6927599906921387
Epoch: 22, Steps: 64 | Train Loss: 0.6570683 Vali Loss: 0.3687853 Test Loss: 0.4253793
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.347803831100464
Epoch: 23, Steps: 64 | Train Loss: 0.6571493 Vali Loss: 0.3676484 Test Loss: 0.4251723
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.5147509574890137
Epoch: 24, Steps: 64 | Train Loss: 0.6564023 Vali Loss: 0.3695112 Test Loss: 0.4250458
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.8386764526367188
Epoch: 25, Steps: 64 | Train Loss: 0.6565381 Vali Loss: 0.3692876 Test Loss: 0.4249201
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.345097064971924
Epoch: 26, Steps: 64 | Train Loss: 0.6564747 Vali Loss: 0.3687716 Test Loss: 0.4247810
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 1.9125242233276367
Epoch: 27, Steps: 64 | Train Loss: 0.6551773 Vali Loss: 0.3663352 Test Loss: 0.4246925
Validation loss decreased (0.367183 --> 0.366335).  Saving model ...
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.7481086254119873
Epoch: 28, Steps: 64 | Train Loss: 0.6560294 Vali Loss: 0.3669821 Test Loss: 0.4245781
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.8499341011047363
Epoch: 29, Steps: 64 | Train Loss: 0.6553404 Vali Loss: 0.3663076 Test Loss: 0.4244777
Validation loss decreased (0.366335 --> 0.366308).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.971438407897949
Epoch: 30, Steps: 64 | Train Loss: 0.6542810 Vali Loss: 0.3662869 Test Loss: 0.4244049
Validation loss decreased (0.366308 --> 0.366287).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.483055353164673
Epoch: 31, Steps: 64 | Train Loss: 0.6546552 Vali Loss: 0.3699287 Test Loss: 0.4243093
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.9387454986572266
Epoch: 32, Steps: 64 | Train Loss: 0.6526448 Vali Loss: 0.3681884 Test Loss: 0.4242533
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.688616991043091
Epoch: 33, Steps: 64 | Train Loss: 0.6549163 Vali Loss: 0.3677035 Test Loss: 0.4241858
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.1448185443878174
Epoch: 34, Steps: 64 | Train Loss: 0.6542831 Vali Loss: 0.3647452 Test Loss: 0.4241138
Validation loss decreased (0.366287 --> 0.364745).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.6859676837921143
Epoch: 35, Steps: 64 | Train Loss: 0.6531009 Vali Loss: 0.3682473 Test Loss: 0.4240621
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.5770537853240967
Epoch: 36, Steps: 64 | Train Loss: 0.6547175 Vali Loss: 0.3679470 Test Loss: 0.4240129
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.0816705226898193
Epoch: 37, Steps: 64 | Train Loss: 0.6532509 Vali Loss: 0.3685046 Test Loss: 0.4239591
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.0638833045959473
Epoch: 38, Steps: 64 | Train Loss: 0.6536595 Vali Loss: 0.3687999 Test Loss: 0.4239214
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.432053804397583
Epoch: 39, Steps: 64 | Train Loss: 0.6537693 Vali Loss: 0.3637655 Test Loss: 0.4238816
Validation loss decreased (0.364745 --> 0.363766).  Saving model ...
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.0264668464660645
Epoch: 40, Steps: 64 | Train Loss: 0.6534674 Vali Loss: 0.3639587 Test Loss: 0.4238239
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.699627637863159
Epoch: 41, Steps: 64 | Train Loss: 0.6541602 Vali Loss: 0.3696878 Test Loss: 0.4237915
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.026930570602417
Epoch: 42, Steps: 64 | Train Loss: 0.6527304 Vali Loss: 0.3664790 Test Loss: 0.4237522
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.0358376502990723
Epoch: 43, Steps: 64 | Train Loss: 0.6539685 Vali Loss: 0.3670322 Test Loss: 0.4237290
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.9266576766967773
Epoch: 44, Steps: 64 | Train Loss: 0.6519894 Vali Loss: 0.3658276 Test Loss: 0.4236872
EarlyStopping counter: 5 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.7568249702453613
Epoch: 45, Steps: 64 | Train Loss: 0.6527518 Vali Loss: 0.3663002 Test Loss: 0.4236602
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.4120023250579834
Epoch: 46, Steps: 64 | Train Loss: 0.6536868 Vali Loss: 0.3670261 Test Loss: 0.4236327
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.5568878650665283
Epoch: 47, Steps: 64 | Train Loss: 0.6523865 Vali Loss: 0.3657401 Test Loss: 0.4236074
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.7463793754577637
Epoch: 48, Steps: 64 | Train Loss: 0.6529353 Vali Loss: 0.3685040 Test Loss: 0.4235833
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.9928526878356934
Epoch: 49, Steps: 64 | Train Loss: 0.6531128 Vali Loss: 0.3673320 Test Loss: 0.4235617
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.182875394821167
Epoch: 50, Steps: 64 | Train Loss: 0.6521363 Vali Loss: 0.3655843 Test Loss: 0.4235447
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.038804769515991
Epoch: 51, Steps: 64 | Train Loss: 0.6530990 Vali Loss: 0.3674127 Test Loss: 0.4235221
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.779289960861206
Epoch: 52, Steps: 64 | Train Loss: 0.6528105 Vali Loss: 0.3671741 Test Loss: 0.4234986
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.0022785663604736
Epoch: 53, Steps: 64 | Train Loss: 0.6512065 Vali Loss: 0.3676386 Test Loss: 0.4234786
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.119779586791992
Epoch: 54, Steps: 64 | Train Loss: 0.6529348 Vali Loss: 0.3663992 Test Loss: 0.4234567
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.083146095275879
Epoch: 55, Steps: 64 | Train Loss: 0.6524818 Vali Loss: 0.3654750 Test Loss: 0.4234420
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 1.828383445739746
Epoch: 56, Steps: 64 | Train Loss: 0.6531854 Vali Loss: 0.3690955 Test Loss: 0.4234299
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.593647003173828
Epoch: 57, Steps: 64 | Train Loss: 0.6530165 Vali Loss: 0.3673285 Test Loss: 0.4234189
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.0367207527160645
Epoch: 58, Steps: 64 | Train Loss: 0.6534696 Vali Loss: 0.3660916 Test Loss: 0.4233932
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.6240386962890625
Epoch: 59, Steps: 64 | Train Loss: 0.6519820 Vali Loss: 0.3656263 Test Loss: 0.4233840
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H2_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.4191310703754425, mae:0.4264674484729767, rse:0.5176240801811218, corr:[0.2621128  0.2656233  0.264131   0.261178   0.25966933 0.25922793
 0.25822052 0.25630006 0.25487906 0.254149   0.2533351  0.25169802
 0.24993655 0.24879152 0.24833016 0.24788503 0.24696377 0.2457181
 0.2446227  0.24371181 0.24273439 0.2413655  0.23955281 0.23758128
 0.23530382 0.23328747 0.23118342 0.22946829 0.22839357 0.22769336
 0.22684987 0.22533874 0.224104   0.22347514 0.22315557 0.22233666
 0.22099775 0.21986511 0.21912676 0.2186781  0.21820556 0.21746805
 0.21657062 0.21557325 0.21474399 0.21377084 0.21206085 0.2094188
 0.20599538 0.20326665 0.20085329 0.19875807 0.19693989 0.19520883
 0.19370878 0.1915607  0.19017185 0.18933848 0.18893917 0.18821344
 0.18725921 0.18700075 0.1870563  0.1869194  0.18636154 0.18572298
 0.18505056 0.18421072 0.18329318 0.18240128 0.1810925  0.17929642
 0.1768288  0.17511846 0.17357035 0.17225207 0.17135526 0.17105804
 0.17097753 0.1699362  0.16919245 0.16906185 0.16931005 0.16916543
 0.16853532 0.1680406  0.16793717 0.16809511 0.16809307 0.16771981
 0.16708632 0.16619731 0.16568667 0.16531196 0.16446333 0.16287094
 0.16050702 0.15867057 0.15709884 0.15553893 0.15406808 0.15317495
 0.15352637 0.15332007 0.15308525 0.15271609 0.15278482 0.1529112
 0.1526895  0.15244913 0.15193774 0.15149634 0.15113196 0.1508179
 0.15035827 0.14937133 0.14834762 0.14717937 0.14547017 0.14305039
 0.13999566 0.13768204 0.13591589 0.1347855  0.13359809 0.13253796
 0.13193794 0.13128619 0.1310734  0.13090144 0.1307312  0.1303347
 0.1299219  0.12978543 0.12964019 0.12922542 0.12859783 0.1279583
 0.12734063 0.12646985 0.12573242 0.1248461  0.12319072 0.12060156
 0.11710396 0.11439554 0.11226784 0.11092284 0.11005101 0.10937696
 0.10897355 0.10804228 0.10764822 0.10775175 0.10818991 0.10824677
 0.10794999 0.1078991  0.10804946 0.10822532 0.10814814 0.10773911
 0.10728073 0.1067394  0.10652358 0.10617141 0.10508501 0.1029065
 0.09978091 0.09769937 0.0961614  0.09501626 0.09402987 0.09344096
 0.09364949 0.09343316 0.09347907 0.09367929 0.09418044 0.09440789
 0.09435596 0.09450416 0.09474329 0.09514306 0.0953621  0.09546269
 0.09547193 0.09516336 0.09492411 0.09483579 0.09439714 0.09326895
 0.09146661 0.09047438 0.08985717 0.08934605 0.08899546 0.08899576
 0.09004222 0.09101129 0.0918933  0.09226789 0.09248667 0.09266324
 0.09299285 0.0933867  0.09349317 0.09342239 0.09327294 0.09332519
 0.09330543 0.09316567 0.09301739 0.09275331 0.09208255 0.09054562
 0.08822513 0.08634344 0.08479995 0.08379678 0.08318628 0.0835086
 0.08469771 0.08565801 0.08645754 0.08695209 0.08727777 0.08708084
 0.08684503 0.08698733 0.08734608 0.08757871 0.08764411 0.0877829
 0.08811048 0.08852826 0.08874164 0.08858855 0.08771769 0.08620713
 0.08396076 0.08261235 0.0814302  0.08053583 0.08014956 0.08075245
 0.08271001 0.08401228 0.08487664 0.08547343 0.08626604 0.08685293
 0.08723964 0.08807211 0.08880729 0.08951274 0.08983722 0.09027248
 0.09044416 0.09056167 0.09080662 0.09096202 0.0904838  0.08927817
 0.08770398 0.08725749 0.08723765 0.0871306  0.08695604 0.08760095
 0.08926696 0.09044684 0.09143251 0.09200492 0.09243453 0.09236296
 0.09229676 0.09254425 0.09306952 0.0934279  0.09343828 0.09324135
 0.0930428  0.09269433 0.09270196 0.09287059 0.09265707 0.09177317
 0.09022206 0.08941743 0.08899451 0.08864707 0.08837507 0.08825166
 0.08886577 0.08939057 0.09035733 0.09119887 0.09181725 0.09202223
 0.09202106 0.09243104 0.09292575 0.09334726 0.09353871 0.09394995
 0.09426539 0.09403811 0.09408087 0.09433373 0.09448822 0.09391829
 0.09224604 0.09158909 0.0914588  0.09167672 0.09178206 0.09227431
 0.09358699 0.09449697 0.09573736 0.09687829 0.09783217 0.09802768
 0.09791715 0.09793334 0.09811125 0.09816369 0.09811869 0.09842475
 0.09890335 0.09841552 0.09780166 0.09807048 0.0991057  0.09877626]
