Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=74, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=74, out_features=143, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9481472.0
params:  10725.0
Trainable parameters:  10725
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.941312551498413
Epoch: 1, Steps: 62 | Train Loss: 0.8079109 Vali Loss: 0.4735579 Test Loss: 0.4112543
Validation loss decreased (inf --> 0.473558).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 4.239766359329224
Epoch: 2, Steps: 62 | Train Loss: 0.6945027 Vali Loss: 0.4329979 Test Loss: 0.3848378
Validation loss decreased (0.473558 --> 0.432998).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.0462145805358887
Epoch: 3, Steps: 62 | Train Loss: 0.6604377 Vali Loss: 0.4176094 Test Loss: 0.3777323
Validation loss decreased (0.432998 --> 0.417609).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.056706428527832
Epoch: 4, Steps: 62 | Train Loss: 0.6466201 Vali Loss: 0.4071281 Test Loss: 0.3746026
Validation loss decreased (0.417609 --> 0.407128).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.3145875930786133
Epoch: 5, Steps: 62 | Train Loss: 0.6378987 Vali Loss: 0.3991495 Test Loss: 0.3726000
Validation loss decreased (0.407128 --> 0.399150).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.6454522609710693
Epoch: 6, Steps: 62 | Train Loss: 0.6327341 Vali Loss: 0.3950878 Test Loss: 0.3706145
Validation loss decreased (0.399150 --> 0.395088).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.213569402694702
Epoch: 7, Steps: 62 | Train Loss: 0.6286473 Vali Loss: 0.3929915 Test Loss: 0.3693810
Validation loss decreased (0.395088 --> 0.392992).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.6545753479003906
Epoch: 8, Steps: 62 | Train Loss: 0.6254210 Vali Loss: 0.3909219 Test Loss: 0.3681845
Validation loss decreased (0.392992 --> 0.390922).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 4.256846189498901
Epoch: 9, Steps: 62 | Train Loss: 0.6230413 Vali Loss: 0.3897293 Test Loss: 0.3673205
Validation loss decreased (0.390922 --> 0.389729).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.8562073707580566
Epoch: 10, Steps: 62 | Train Loss: 0.6205274 Vali Loss: 0.3871740 Test Loss: 0.3666208
Validation loss decreased (0.389729 --> 0.387174).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.743001699447632
Epoch: 11, Steps: 62 | Train Loss: 0.6191193 Vali Loss: 0.3849796 Test Loss: 0.3661806
Validation loss decreased (0.387174 --> 0.384980).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.614907741546631
Epoch: 12, Steps: 62 | Train Loss: 0.6182631 Vali Loss: 0.3835305 Test Loss: 0.3656309
Validation loss decreased (0.384980 --> 0.383531).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 4.062262535095215
Epoch: 13, Steps: 62 | Train Loss: 0.6165693 Vali Loss: 0.3828800 Test Loss: 0.3652582
Validation loss decreased (0.383531 --> 0.382880).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.7586114406585693
Epoch: 14, Steps: 62 | Train Loss: 0.6164250 Vali Loss: 0.3844822 Test Loss: 0.3651342
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.3193557262420654
Epoch: 15, Steps: 62 | Train Loss: 0.6149306 Vali Loss: 0.3824178 Test Loss: 0.3647431
Validation loss decreased (0.382880 --> 0.382418).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.04905366897583
Epoch: 16, Steps: 62 | Train Loss: 0.6149743 Vali Loss: 0.3804902 Test Loss: 0.3645294
Validation loss decreased (0.382418 --> 0.380490).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.058051347732544
Epoch: 17, Steps: 62 | Train Loss: 0.6138402 Vali Loss: 0.3782467 Test Loss: 0.3645498
Validation loss decreased (0.380490 --> 0.378247).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.3182010650634766
Epoch: 18, Steps: 62 | Train Loss: 0.6138609 Vali Loss: 0.3801544 Test Loss: 0.3642523
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.8616628646850586
Epoch: 19, Steps: 62 | Train Loss: 0.6125106 Vali Loss: 0.3788237 Test Loss: 0.3642554
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.0508546829223633
Epoch: 20, Steps: 62 | Train Loss: 0.6123994 Vali Loss: 0.3783309 Test Loss: 0.3640068
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.7232515811920166
Epoch: 21, Steps: 62 | Train Loss: 0.6126033 Vali Loss: 0.3777441 Test Loss: 0.3639320
Validation loss decreased (0.378247 --> 0.377744).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 3.308253288269043
Epoch: 22, Steps: 62 | Train Loss: 0.6122946 Vali Loss: 0.3795905 Test Loss: 0.3638596
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.228990077972412
Epoch: 23, Steps: 62 | Train Loss: 0.6118586 Vali Loss: 0.3790343 Test Loss: 0.3637535
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.237736701965332
Epoch: 24, Steps: 62 | Train Loss: 0.6118347 Vali Loss: 0.3778434 Test Loss: 0.3637181
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.2205870151519775
Epoch: 25, Steps: 62 | Train Loss: 0.6113906 Vali Loss: 0.3771755 Test Loss: 0.3636056
Validation loss decreased (0.377744 --> 0.377176).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.90983247756958
Epoch: 26, Steps: 62 | Train Loss: 0.6107232 Vali Loss: 0.3783371 Test Loss: 0.3636363
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.5956578254699707
Epoch: 27, Steps: 62 | Train Loss: 0.6108820 Vali Loss: 0.3784024 Test Loss: 0.3636542
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.198690176010132
Epoch: 28, Steps: 62 | Train Loss: 0.6109626 Vali Loss: 0.3768055 Test Loss: 0.3635796
Validation loss decreased (0.377176 --> 0.376806).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.5010907649993896
Epoch: 29, Steps: 62 | Train Loss: 0.6109419 Vali Loss: 0.3760882 Test Loss: 0.3634861
Validation loss decreased (0.376806 --> 0.376088).  Saving model ...
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.5611770153045654
Epoch: 30, Steps: 62 | Train Loss: 0.6101516 Vali Loss: 0.3779824 Test Loss: 0.3635252
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 4.1519389152526855
Epoch: 31, Steps: 62 | Train Loss: 0.6107438 Vali Loss: 0.3757870 Test Loss: 0.3634268
Validation loss decreased (0.376088 --> 0.375787).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.065579414367676
Epoch: 32, Steps: 62 | Train Loss: 0.6103539 Vali Loss: 0.3776129 Test Loss: 0.3634369
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.934694290161133
Epoch: 33, Steps: 62 | Train Loss: 0.6094577 Vali Loss: 0.3754347 Test Loss: 0.3633875
Validation loss decreased (0.375787 --> 0.375435).  Saving model ...
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.8142268657684326
Epoch: 34, Steps: 62 | Train Loss: 0.6101335 Vali Loss: 0.3771603 Test Loss: 0.3633047
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.8417551517486572
Epoch: 35, Steps: 62 | Train Loss: 0.6089783 Vali Loss: 0.3735167 Test Loss: 0.3633427
Validation loss decreased (0.375435 --> 0.373517).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.0906906127929688
Epoch: 36, Steps: 62 | Train Loss: 0.6099330 Vali Loss: 0.3757250 Test Loss: 0.3633824
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.4379093647003174
Epoch: 37, Steps: 62 | Train Loss: 0.6099010 Vali Loss: 0.3758043 Test Loss: 0.3633268
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 5.112133741378784
Epoch: 38, Steps: 62 | Train Loss: 0.6097512 Vali Loss: 0.3770522 Test Loss: 0.3632959
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.7981555461883545
Epoch: 39, Steps: 62 | Train Loss: 0.6098135 Vali Loss: 0.3765805 Test Loss: 0.3632520
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 3.0626847743988037
Epoch: 40, Steps: 62 | Train Loss: 0.6090544 Vali Loss: 0.3780625 Test Loss: 0.3632830
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.7565391063690186
Epoch: 41, Steps: 62 | Train Loss: 0.6092266 Vali Loss: 0.3724021 Test Loss: 0.3632528
Validation loss decreased (0.373517 --> 0.372402).  Saving model ...
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 4.795386791229248
Epoch: 42, Steps: 62 | Train Loss: 0.6097114 Vali Loss: 0.3746616 Test Loss: 0.3632114
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 3.4792675971984863
Epoch: 43, Steps: 62 | Train Loss: 0.6093415 Vali Loss: 0.3757281 Test Loss: 0.3632358
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.8715500831604004
Epoch: 44, Steps: 62 | Train Loss: 0.6085917 Vali Loss: 0.3769127 Test Loss: 0.3632061
EarlyStopping counter: 3 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.3088181018829346
Epoch: 45, Steps: 62 | Train Loss: 0.6088649 Vali Loss: 0.3760726 Test Loss: 0.3632043
EarlyStopping counter: 4 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.5995285511016846
Epoch: 46, Steps: 62 | Train Loss: 0.6088109 Vali Loss: 0.3768157 Test Loss: 0.3632021
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.424636125564575
Epoch: 47, Steps: 62 | Train Loss: 0.6088758 Vali Loss: 0.3763358 Test Loss: 0.3631770
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.325728416442871
Epoch: 48, Steps: 62 | Train Loss: 0.6090353 Vali Loss: 0.3770046 Test Loss: 0.3631912
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.6338441371917725
Epoch: 49, Steps: 62 | Train Loss: 0.6090167 Vali Loss: 0.3756823 Test Loss: 0.3631646
EarlyStopping counter: 8 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.958674430847168
Epoch: 50, Steps: 62 | Train Loss: 0.6092815 Vali Loss: 0.3753487 Test Loss: 0.3631657
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.1464431285858154
Epoch: 51, Steps: 62 | Train Loss: 0.6091772 Vali Loss: 0.3734651 Test Loss: 0.3631593
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.280430555343628
Epoch: 52, Steps: 62 | Train Loss: 0.6088649 Vali Loss: 0.3749207 Test Loss: 0.3631521
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.5158274173736572
Epoch: 53, Steps: 62 | Train Loss: 0.6084395 Vali Loss: 0.3745238 Test Loss: 0.3631694
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.4439351558685303
Epoch: 54, Steps: 62 | Train Loss: 0.6088135 Vali Loss: 0.3744119 Test Loss: 0.3631565
EarlyStopping counter: 13 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.580742359161377
Epoch: 55, Steps: 62 | Train Loss: 0.6078952 Vali Loss: 0.3755862 Test Loss: 0.3631545
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.4112682342529297
Epoch: 56, Steps: 62 | Train Loss: 0.6081962 Vali Loss: 0.3765005 Test Loss: 0.3631328
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.2992630004882812
Epoch: 57, Steps: 62 | Train Loss: 0.6089078 Vali Loss: 0.3734551 Test Loss: 0.3631430
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.1111292839050293
Epoch: 58, Steps: 62 | Train Loss: 0.6087056 Vali Loss: 0.3765927 Test Loss: 0.3631285
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.0168912410736084
Epoch: 59, Steps: 62 | Train Loss: 0.6087952 Vali Loss: 0.3759338 Test Loss: 0.3631192
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.0221755504608154
Epoch: 60, Steps: 62 | Train Loss: 0.6085185 Vali Loss: 0.3768099 Test Loss: 0.3631284
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.255262613296509
Epoch: 61, Steps: 62 | Train Loss: 0.6082569 Vali Loss: 0.3759509 Test Loss: 0.3631111
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3587739169597626, mae:0.3972359001636505, rse:0.47890564799308777, corr:[0.2598895  0.2665668  0.2630907  0.26084834 0.2615237  0.26176003
 0.25975272 0.25771558 0.25698236 0.2563289  0.25465107 0.2524954
 0.25124958 0.25060442 0.24989635 0.24882504 0.24785472 0.2471793
 0.24645065 0.24532606 0.24397047 0.24283008 0.24153478 0.23983148
 0.2377736  0.23610315 0.23500538 0.23422074 0.23340642 0.23235698
 0.23113897 0.22985186 0.22846751 0.22709401 0.2260049  0.22521967
 0.22428118 0.22297697 0.2215727  0.2204622  0.21974142 0.21891183
 0.21757121 0.21599825 0.21457916 0.2134634  0.21193801 0.2096446
 0.20719194 0.20542704 0.20429642 0.20311955 0.20143567 0.19948627
 0.19773744 0.19631559 0.19504194 0.19356425 0.19238093 0.19179672
 0.19158569 0.19123983 0.19065905 0.19004461 0.18951435 0.18901166
 0.18821603 0.1874422  0.18687671 0.18663815 0.18615888 0.18515982
 0.1839443  0.18305083 0.18260664 0.18219806 0.18157965 0.18076311
 0.18010801 0.17975418 0.17956972 0.1791429  0.17877309 0.17866164
 0.17876533 0.1786271  0.17825086 0.17784481 0.17754897 0.17719428
 0.17660342 0.17601    0.17607747 0.17665434 0.17700395 0.17642869
 0.17522404 0.17412932 0.17373876 0.17355092 0.17303114 0.17221583
 0.17174281 0.17160083 0.17169566 0.1714187  0.17098612 0.17085753
 0.17089091 0.17067398 0.16985874 0.1689367  0.16840109 0.16850373
 0.16852558 0.16784106 0.16679272 0.16589168 0.16542064 0.16475204
 0.16339421 0.1618384  0.16101778 0.16096292 0.16088079 0.16025263
 0.15915564 0.15842634 0.15820755 0.15795414 0.15736455 0.15661909
 0.15624884 0.15616846 0.15589778 0.15518376 0.1543993  0.15411049
 0.15413362 0.1542205  0.15394902 0.15325859 0.15250865 0.15163313
 0.15036133 0.14881614 0.14769256 0.14750043 0.14766328 0.14730915
 0.14641966 0.14560579 0.14550063 0.14568746 0.1454328  0.14469033
 0.14415364 0.144237   0.14438859 0.14414881 0.14351776 0.14304446
 0.143152   0.14344777 0.14341415 0.14296286 0.14245717 0.14192355
 0.14131467 0.14030884 0.13916655 0.13849686 0.13818625 0.13761212
 0.13657755 0.13535951 0.13475318 0.13466637 0.13462414 0.13411543
 0.13354293 0.13327746 0.133304   0.1333328  0.13301118 0.13266753
 0.13283993 0.13335377 0.13375233 0.13396798 0.13400424 0.13382512
 0.13349438 0.1328442  0.13214456 0.13183011 0.13187395 0.13187504
 0.1315598  0.13095292 0.13053861 0.13063887 0.1310623  0.13102782
 0.13054694 0.12996472 0.12967674 0.1298642  0.13005753 0.13018869
 0.1303259  0.13063951 0.13086076 0.13069399 0.13038449 0.12990117
 0.12935758 0.12842524 0.12745643 0.12677641 0.12641649 0.1263803
 0.12621416 0.12605758 0.12590873 0.12579224 0.12569511 0.12510903
 0.12411249 0.12321099 0.12291872 0.12326249 0.12355807 0.12364668
 0.12370389 0.12418788 0.12465904 0.12473565 0.12448962 0.1241487
 0.12369253 0.12319867 0.12263227 0.12198409 0.12148909 0.12094299
 0.12057512 0.120407   0.12025253 0.12000684 0.12010989 0.12040474
 0.12058297 0.12085219 0.12125518 0.12229248 0.12329847 0.12396506
 0.12406044 0.12414002 0.12471759 0.1253719  0.12599723 0.12616913
 0.12608267 0.12578827 0.1255687  0.12543854 0.12530269 0.1252859
 0.12525663 0.12528734 0.12516637 0.1249212  0.12522925 0.12531583
 0.12534216 0.1252166  0.12488907 0.12473432 0.12495102 0.12540422
 0.1254808  0.12531942 0.12500738 0.12499803 0.12541106 0.12592885
 0.12543081 0.12414878 0.12301771 0.12244871 0.12179236 0.12090357
 0.11984011 0.11895277 0.11868934 0.11832823 0.11792129 0.11746737
 0.11698609 0.11673121 0.11671278 0.11705536 0.11693026 0.11705811
 0.11694898 0.11707089 0.11732133 0.11709098 0.11721632 0.117181
 0.11665536 0.11511423 0.11325606 0.11193117 0.11114104 0.11066757
 0.10991175 0.10913564 0.10855103 0.10867143 0.10927426 0.10938199
 0.10881385 0.10850082 0.10911229 0.1096153  0.1089337  0.10820541
 0.10845637 0.10900164 0.10765061 0.10590302 0.11032082 0.11631474]
