Args in experiment:
Namespace(H_order=3, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=103, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_720_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=720, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H3_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=103, out_features=206, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19011328.0
params:  21424.0
Trainable parameters:  21424
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.820831775665283
Epoch: 1, Steps: 56 | Train Loss: 0.8669664 Vali Loss: 0.8490837 Test Loss: 0.4840623
Validation loss decreased (inf --> 0.849084).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.530592679977417
Epoch: 2, Steps: 56 | Train Loss: 0.7143706 Vali Loss: 0.7913827 Test Loss: 0.4449357
Validation loss decreased (0.849084 --> 0.791383).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.620924234390259
Epoch: 3, Steps: 56 | Train Loss: 0.6360092 Vali Loss: 0.7549808 Test Loss: 0.4261047
Validation loss decreased (0.791383 --> 0.754981).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.073140859603882
Epoch: 4, Steps: 56 | Train Loss: 0.5916682 Vali Loss: 0.7381286 Test Loss: 0.4163673
Validation loss decreased (0.754981 --> 0.738129).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.5121424198150635
Epoch: 5, Steps: 56 | Train Loss: 0.5649680 Vali Loss: 0.7299042 Test Loss: 0.4111362
Validation loss decreased (0.738129 --> 0.729904).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.947507619857788
Epoch: 6, Steps: 56 | Train Loss: 0.5458948 Vali Loss: 0.7195097 Test Loss: 0.4078510
Validation loss decreased (0.729904 --> 0.719510).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.1189863681793213
Epoch: 7, Steps: 56 | Train Loss: 0.5319762 Vali Loss: 0.7104587 Test Loss: 0.4055275
Validation loss decreased (0.719510 --> 0.710459).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.7859582901000977
Epoch: 8, Steps: 56 | Train Loss: 0.5191520 Vali Loss: 0.7110889 Test Loss: 0.4037226
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.1472434997558594
Epoch: 9, Steps: 56 | Train Loss: 0.5095587 Vali Loss: 0.7075830 Test Loss: 0.4022977
Validation loss decreased (0.710459 --> 0.707583).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.387651205062866
Epoch: 10, Steps: 56 | Train Loss: 0.5020566 Vali Loss: 0.6986713 Test Loss: 0.4009970
Validation loss decreased (0.707583 --> 0.698671).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.7643253803253174
Epoch: 11, Steps: 56 | Train Loss: 0.4946732 Vali Loss: 0.6941553 Test Loss: 0.3998250
Validation loss decreased (0.698671 --> 0.694155).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.9733259677886963
Epoch: 12, Steps: 56 | Train Loss: 0.4887601 Vali Loss: 0.6944191 Test Loss: 0.3988033
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.89406156539917
Epoch: 13, Steps: 56 | Train Loss: 0.4833141 Vali Loss: 0.6884288 Test Loss: 0.3979208
Validation loss decreased (0.694155 --> 0.688429).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 3.111196279525757
Epoch: 14, Steps: 56 | Train Loss: 0.4778452 Vali Loss: 0.6885087 Test Loss: 0.3970335
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.5888006687164307
Epoch: 15, Steps: 56 | Train Loss: 0.4739877 Vali Loss: 0.6853671 Test Loss: 0.3963144
Validation loss decreased (0.688429 --> 0.685367).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.3263449668884277
Epoch: 16, Steps: 56 | Train Loss: 0.4701910 Vali Loss: 0.6866536 Test Loss: 0.3955555
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.4334921836853027
Epoch: 17, Steps: 56 | Train Loss: 0.4667276 Vali Loss: 0.6825746 Test Loss: 0.3948920
Validation loss decreased (0.685367 --> 0.682575).  Saving model ...
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.6565873622894287
Epoch: 18, Steps: 56 | Train Loss: 0.4643991 Vali Loss: 0.6829833 Test Loss: 0.3943565
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.1993191242218018
Epoch: 19, Steps: 56 | Train Loss: 0.4613498 Vali Loss: 0.6782956 Test Loss: 0.3937964
Validation loss decreased (0.682575 --> 0.678296).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.881502389907837
Epoch: 20, Steps: 56 | Train Loss: 0.4587206 Vali Loss: 0.6815988 Test Loss: 0.3932943
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.77951979637146
Epoch: 21, Steps: 56 | Train Loss: 0.4572867 Vali Loss: 0.6823836 Test Loss: 0.3928707
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.3574163913726807
Epoch: 22, Steps: 56 | Train Loss: 0.4548912 Vali Loss: 0.6772152 Test Loss: 0.3924604
Validation loss decreased (0.678296 --> 0.677215).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.9801688194274902
Epoch: 23, Steps: 56 | Train Loss: 0.4533463 Vali Loss: 0.6763779 Test Loss: 0.3920775
Validation loss decreased (0.677215 --> 0.676378).  Saving model ...
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.4490480422973633
Epoch: 24, Steps: 56 | Train Loss: 0.4511804 Vali Loss: 0.6758031 Test Loss: 0.3916847
Validation loss decreased (0.676378 --> 0.675803).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.657306671142578
Epoch: 25, Steps: 56 | Train Loss: 0.4500022 Vali Loss: 0.6697760 Test Loss: 0.3913791
Validation loss decreased (0.675803 --> 0.669776).  Saving model ...
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.6925227642059326
Epoch: 26, Steps: 56 | Train Loss: 0.4490255 Vali Loss: 0.6729757 Test Loss: 0.3911054
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.332127332687378
Epoch: 27, Steps: 56 | Train Loss: 0.4479212 Vali Loss: 0.6715301 Test Loss: 0.3907636
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.0459251403808594
Epoch: 28, Steps: 56 | Train Loss: 0.4465549 Vali Loss: 0.6721248 Test Loss: 0.3905213
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.7396655082702637
Epoch: 29, Steps: 56 | Train Loss: 0.4451981 Vali Loss: 0.6714349 Test Loss: 0.3902954
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.931028127670288
Epoch: 30, Steps: 56 | Train Loss: 0.4439921 Vali Loss: 0.6709193 Test Loss: 0.3900654
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.8740298748016357
Epoch: 31, Steps: 56 | Train Loss: 0.4432312 Vali Loss: 0.6696687 Test Loss: 0.3898822
Validation loss decreased (0.669776 --> 0.669669).  Saving model ...
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.0111806392669678
Epoch: 32, Steps: 56 | Train Loss: 0.4413485 Vali Loss: 0.6690589 Test Loss: 0.3896953
Validation loss decreased (0.669669 --> 0.669059).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 2.499626874923706
Epoch: 33, Steps: 56 | Train Loss: 0.4416862 Vali Loss: 0.6738145 Test Loss: 0.3895122
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.8765459060668945
Epoch: 34, Steps: 56 | Train Loss: 0.4401193 Vali Loss: 0.6683053 Test Loss: 0.3893399
Validation loss decreased (0.669059 --> 0.668305).  Saving model ...
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.9112284183502197
Epoch: 35, Steps: 56 | Train Loss: 0.4404630 Vali Loss: 0.6699761 Test Loss: 0.3891933
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 3.1403963565826416
Epoch: 36, Steps: 56 | Train Loss: 0.4394546 Vali Loss: 0.6718899 Test Loss: 0.3890376
EarlyStopping counter: 2 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.6867387294769287
Epoch: 37, Steps: 56 | Train Loss: 0.4388921 Vali Loss: 0.6682668 Test Loss: 0.3889087
Validation loss decreased (0.668305 --> 0.668267).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.6111884117126465
Epoch: 38, Steps: 56 | Train Loss: 0.4388824 Vali Loss: 0.6641752 Test Loss: 0.3887781
Validation loss decreased (0.668267 --> 0.664175).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.236403703689575
Epoch: 39, Steps: 56 | Train Loss: 0.4372252 Vali Loss: 0.6650243 Test Loss: 0.3886853
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.7962257862091064
Epoch: 40, Steps: 56 | Train Loss: 0.4377395 Vali Loss: 0.6691432 Test Loss: 0.3885630
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 2.705549716949463
Epoch: 41, Steps: 56 | Train Loss: 0.4365236 Vali Loss: 0.6667248 Test Loss: 0.3884670
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.872853994369507
Epoch: 42, Steps: 56 | Train Loss: 0.4364850 Vali Loss: 0.6671932 Test Loss: 0.3883717
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.5707626342773438
Epoch: 43, Steps: 56 | Train Loss: 0.4352996 Vali Loss: 0.6628485 Test Loss: 0.3882920
Validation loss decreased (0.664175 --> 0.662848).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.043741226196289
Epoch: 44, Steps: 56 | Train Loss: 0.4358827 Vali Loss: 0.6679258 Test Loss: 0.3882034
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.088151454925537
Epoch: 45, Steps: 56 | Train Loss: 0.4352283 Vali Loss: 0.6664070 Test Loss: 0.3881277
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.4224748611450195
Epoch: 46, Steps: 56 | Train Loss: 0.4358686 Vali Loss: 0.6654570 Test Loss: 0.3880767
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 3.115494728088379
Epoch: 47, Steps: 56 | Train Loss: 0.4346354 Vali Loss: 0.6666776 Test Loss: 0.3880044
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.9420018196105957
Epoch: 48, Steps: 56 | Train Loss: 0.4351573 Vali Loss: 0.6650242 Test Loss: 0.3879335
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.783781051635742
Epoch: 49, Steps: 56 | Train Loss: 0.4349961 Vali Loss: 0.6655772 Test Loss: 0.3878810
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.8833699226379395
Epoch: 50, Steps: 56 | Train Loss: 0.4340597 Vali Loss: 0.6635004 Test Loss: 0.3878224
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.4624879360198975
Epoch: 51, Steps: 56 | Train Loss: 0.4342087 Vali Loss: 0.6656171 Test Loss: 0.3877648
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.868464708328247
Epoch: 52, Steps: 56 | Train Loss: 0.4338232 Vali Loss: 0.6651103 Test Loss: 0.3877317
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.8871278762817383
Epoch: 53, Steps: 56 | Train Loss: 0.4336318 Vali Loss: 0.6586927 Test Loss: 0.3876839
Validation loss decreased (0.662848 --> 0.658693).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.955397605895996
Epoch: 54, Steps: 56 | Train Loss: 0.4335838 Vali Loss: 0.6634471 Test Loss: 0.3876392
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 2.5445175170898438
Epoch: 55, Steps: 56 | Train Loss: 0.4329063 Vali Loss: 0.6630358 Test Loss: 0.3875994
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.4457998275756836
Epoch: 56, Steps: 56 | Train Loss: 0.4327472 Vali Loss: 0.6659179 Test Loss: 0.3875643
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.657254934310913
Epoch: 57, Steps: 56 | Train Loss: 0.4331100 Vali Loss: 0.6617573 Test Loss: 0.3875293
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.9811224937438965
Epoch: 58, Steps: 56 | Train Loss: 0.4322992 Vali Loss: 0.6651560 Test Loss: 0.3874985
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.894296169281006
Epoch: 59, Steps: 56 | Train Loss: 0.4326378 Vali Loss: 0.6653833 Test Loss: 0.3874668
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.7726268768310547
Epoch: 60, Steps: 56 | Train Loss: 0.4330082 Vali Loss: 0.6610941 Test Loss: 0.3874398
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.8517699241638184
Epoch: 61, Steps: 56 | Train Loss: 0.4327720 Vali Loss: 0.6635119 Test Loss: 0.3874170
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.13157320022583
Epoch: 62, Steps: 56 | Train Loss: 0.4321971 Vali Loss: 0.6632187 Test Loss: 0.3873884
EarlyStopping counter: 9 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.1789793968200684
Epoch: 63, Steps: 56 | Train Loss: 0.4318179 Vali Loss: 0.6588876 Test Loss: 0.3873603
EarlyStopping counter: 10 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.842725992202759
Epoch: 64, Steps: 56 | Train Loss: 0.4321552 Vali Loss: 0.6629825 Test Loss: 0.3873362
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.7956840991973877
Epoch: 65, Steps: 56 | Train Loss: 0.4319383 Vali Loss: 0.6618320 Test Loss: 0.3873196
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.8838865756988525
Epoch: 66, Steps: 56 | Train Loss: 0.4316711 Vali Loss: 0.6616791 Test Loss: 0.3873018
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.710463762283325
Epoch: 67, Steps: 56 | Train Loss: 0.4321937 Vali Loss: 0.6623740 Test Loss: 0.3872820
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.687030553817749
Epoch: 68, Steps: 56 | Train Loss: 0.4315986 Vali Loss: 0.6631103 Test Loss: 0.3872659
EarlyStopping counter: 15 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.973370313644409
Epoch: 69, Steps: 56 | Train Loss: 0.4315674 Vali Loss: 0.6637211 Test Loss: 0.3872522
EarlyStopping counter: 16 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 2.9341466426849365
Epoch: 70, Steps: 56 | Train Loss: 0.4312587 Vali Loss: 0.6599305 Test Loss: 0.3872340
EarlyStopping counter: 17 out of 20
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 2.930337905883789
Epoch: 71, Steps: 56 | Train Loss: 0.4298017 Vali Loss: 0.6669309 Test Loss: 0.3872217
EarlyStopping counter: 18 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.8199996948242188
Epoch: 72, Steps: 56 | Train Loss: 0.4312595 Vali Loss: 0.6636402 Test Loss: 0.3872076
EarlyStopping counter: 19 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.9035534858703613
Epoch: 73, Steps: 56 | Train Loss: 0.4312082 Vali Loss: 0.6569973 Test Loss: 0.3871975
Validation loss decreased (0.658693 --> 0.656997).  Saving model ...
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.422393560409546
Epoch: 74, Steps: 56 | Train Loss: 0.4308090 Vali Loss: 0.6627002 Test Loss: 0.3871851
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.141502618789673
Epoch: 75, Steps: 56 | Train Loss: 0.4303872 Vali Loss: 0.6599214 Test Loss: 0.3871737
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.6224312782287598
Epoch: 76, Steps: 56 | Train Loss: 0.4303164 Vali Loss: 0.6590676 Test Loss: 0.3871593
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 3.003732919692993
Epoch: 77, Steps: 56 | Train Loss: 0.4307040 Vali Loss: 0.6598180 Test Loss: 0.3871533
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.532625436782837
Epoch: 78, Steps: 56 | Train Loss: 0.4301293 Vali Loss: 0.6627597 Test Loss: 0.3871436
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.871967077255249
Epoch: 79, Steps: 56 | Train Loss: 0.4310431 Vali Loss: 0.6619641 Test Loss: 0.3871307
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.652578115463257
Epoch: 80, Steps: 56 | Train Loss: 0.4305934 Vali Loss: 0.6581537 Test Loss: 0.3871239
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.0162971019744873
Epoch: 81, Steps: 56 | Train Loss: 0.4299668 Vali Loss: 0.6580745 Test Loss: 0.3871170
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.1753759384155273
Epoch: 82, Steps: 56 | Train Loss: 0.4307749 Vali Loss: 0.6567093 Test Loss: 0.3871131
Validation loss decreased (0.656997 --> 0.656709).  Saving model ...
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 2.483898878097534
Epoch: 83, Steps: 56 | Train Loss: 0.4301372 Vali Loss: 0.6611906 Test Loss: 0.3871039
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 2.4542787075042725
Epoch: 84, Steps: 56 | Train Loss: 0.4307335 Vali Loss: 0.6611499 Test Loss: 0.3870969
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.038510799407959
Epoch: 85, Steps: 56 | Train Loss: 0.4309057 Vali Loss: 0.6645176 Test Loss: 0.3870907
EarlyStopping counter: 3 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 2.9247970581054688
Epoch: 86, Steps: 56 | Train Loss: 0.4295833 Vali Loss: 0.6599406 Test Loss: 0.3870839
EarlyStopping counter: 4 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.4328722953796387
Epoch: 87, Steps: 56 | Train Loss: 0.4305053 Vali Loss: 0.6637188 Test Loss: 0.3870806
EarlyStopping counter: 5 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 2.6558260917663574
Epoch: 88, Steps: 56 | Train Loss: 0.4302770 Vali Loss: 0.6583211 Test Loss: 0.3870747
EarlyStopping counter: 6 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 3.121293544769287
Epoch: 89, Steps: 56 | Train Loss: 0.4304818 Vali Loss: 0.6593926 Test Loss: 0.3870707
EarlyStopping counter: 7 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 3.5055196285247803
Epoch: 90, Steps: 56 | Train Loss: 0.4302969 Vali Loss: 0.6577433 Test Loss: 0.3870658
EarlyStopping counter: 8 out of 20
Updating learning rate to 5.204402478767869e-06
Epoch: 91 cost time: 3.0440518856048584
Epoch: 91, Steps: 56 | Train Loss: 0.4300181 Vali Loss: 0.6581016 Test Loss: 0.3870609
EarlyStopping counter: 9 out of 20
Updating learning rate to 4.944182354829475e-06
Epoch: 92 cost time: 2.9948956966400146
Epoch: 92, Steps: 56 | Train Loss: 0.4302338 Vali Loss: 0.6606844 Test Loss: 0.3870562
EarlyStopping counter: 10 out of 20
Updating learning rate to 4.696973237088e-06
Epoch: 93 cost time: 2.7779412269592285
Epoch: 93, Steps: 56 | Train Loss: 0.4306769 Vali Loss: 0.6596494 Test Loss: 0.3870552
EarlyStopping counter: 11 out of 20
Updating learning rate to 4.462124575233601e-06
Epoch: 94 cost time: 2.5359389781951904
Epoch: 94, Steps: 56 | Train Loss: 0.4300130 Vali Loss: 0.6600609 Test Loss: 0.3870491
EarlyStopping counter: 12 out of 20
Updating learning rate to 4.239018346471921e-06
Epoch: 95 cost time: 2.7756221294403076
Epoch: 95, Steps: 56 | Train Loss: 0.4304472 Vali Loss: 0.6604995 Test Loss: 0.3870465
EarlyStopping counter: 13 out of 20
Updating learning rate to 4.027067429148324e-06
Epoch: 96 cost time: 2.8096439838409424
Epoch: 96, Steps: 56 | Train Loss: 0.4294834 Vali Loss: 0.6573341 Test Loss: 0.3870436
EarlyStopping counter: 14 out of 20
Updating learning rate to 3.825714057690908e-06
Epoch: 97 cost time: 2.1761064529418945
Epoch: 97, Steps: 56 | Train Loss: 0.4305376 Vali Loss: 0.6605574 Test Loss: 0.3870401
EarlyStopping counter: 15 out of 20
Updating learning rate to 3.6344283548063623e-06
Epoch: 98 cost time: 2.8686511516571045
Epoch: 98, Steps: 56 | Train Loss: 0.4298075 Vali Loss: 0.6606809 Test Loss: 0.3870375
EarlyStopping counter: 16 out of 20
Updating learning rate to 3.452706937066044e-06
Epoch: 99 cost time: 2.7775819301605225
Epoch: 99, Steps: 56 | Train Loss: 0.4302038 Vali Loss: 0.6578882 Test Loss: 0.3870349
EarlyStopping counter: 17 out of 20
Updating learning rate to 3.2800715902127414e-06
Epoch: 100 cost time: 2.5340728759765625
Epoch: 100, Steps: 56 | Train Loss: 0.4303273 Vali Loss: 0.6631353 Test Loss: 0.3870316
EarlyStopping counter: 18 out of 20
Updating learning rate to 3.1160680107021042e-06
train 7201
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=103, out_features=206, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  19011328.0
params:  21424.0
Trainable parameters:  21424
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.888810634613037
Epoch: 1, Steps: 56 | Train Loss: 0.8137283 Vali Loss: 0.6527677 Test Loss: 0.3853911
Validation loss decreased (inf --> 0.652768).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.806065797805786
Epoch: 2, Steps: 56 | Train Loss: 0.8095171 Vali Loss: 0.6513053 Test Loss: 0.3839698
Validation loss decreased (0.652768 --> 0.651305).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 3.6071743965148926
Epoch: 3, Steps: 56 | Train Loss: 0.8048470 Vali Loss: 0.6494157 Test Loss: 0.3830790
Validation loss decreased (0.651305 --> 0.649416).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.6266090869903564
Epoch: 4, Steps: 56 | Train Loss: 0.8033729 Vali Loss: 0.6470724 Test Loss: 0.3824494
Validation loss decreased (0.649416 --> 0.647072).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.1228272914886475
Epoch: 5, Steps: 56 | Train Loss: 0.8025058 Vali Loss: 0.6503955 Test Loss: 0.3820177
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.7910101413726807
Epoch: 6, Steps: 56 | Train Loss: 0.8033683 Vali Loss: 0.6503320 Test Loss: 0.3818637
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 2.4550368785858154
Epoch: 7, Steps: 56 | Train Loss: 0.8027334 Vali Loss: 0.6483397 Test Loss: 0.3816926
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.6996943950653076
Epoch: 8, Steps: 56 | Train Loss: 0.8009005 Vali Loss: 0.6441627 Test Loss: 0.3816506
Validation loss decreased (0.647072 --> 0.644163).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.871073007583618
Epoch: 9, Steps: 56 | Train Loss: 0.8005387 Vali Loss: 0.6508257 Test Loss: 0.3815041
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.956977367401123
Epoch: 10, Steps: 56 | Train Loss: 0.8014601 Vali Loss: 0.6452523 Test Loss: 0.3815407
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.7431933879852295
Epoch: 11, Steps: 56 | Train Loss: 0.8011974 Vali Loss: 0.6439058 Test Loss: 0.3815852
Validation loss decreased (0.644163 --> 0.643906).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.6581809520721436
Epoch: 12, Steps: 56 | Train Loss: 0.8008708 Vali Loss: 0.6418244 Test Loss: 0.3815523
Validation loss decreased (0.643906 --> 0.641824).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.4349114894866943
Epoch: 13, Steps: 56 | Train Loss: 0.7997974 Vali Loss: 0.6438864 Test Loss: 0.3814879
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.6271703243255615
Epoch: 14, Steps: 56 | Train Loss: 0.8003009 Vali Loss: 0.6409307 Test Loss: 0.3814703
Validation loss decreased (0.641824 --> 0.640931).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.6409573554992676
Epoch: 15, Steps: 56 | Train Loss: 0.8000261 Vali Loss: 0.6412393 Test Loss: 0.3814514
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.921865224838257
Epoch: 16, Steps: 56 | Train Loss: 0.8011603 Vali Loss: 0.6413412 Test Loss: 0.3814924
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.7116177082061768
Epoch: 17, Steps: 56 | Train Loss: 0.8010827 Vali Loss: 0.6448385 Test Loss: 0.3814359
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.31827712059021
Epoch: 18, Steps: 56 | Train Loss: 0.8007128 Vali Loss: 0.6436781 Test Loss: 0.3814364
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.95210337638855
Epoch: 19, Steps: 56 | Train Loss: 0.8006396 Vali Loss: 0.6448703 Test Loss: 0.3814814
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.746340274810791
Epoch: 20, Steps: 56 | Train Loss: 0.7995081 Vali Loss: 0.6466693 Test Loss: 0.3815018
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.0299248695373535
Epoch: 21, Steps: 56 | Train Loss: 0.8004605 Vali Loss: 0.6453121 Test Loss: 0.3814959
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.8201794624328613
Epoch: 22, Steps: 56 | Train Loss: 0.8002370 Vali Loss: 0.6457307 Test Loss: 0.3815574
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.733152151107788
Epoch: 23, Steps: 56 | Train Loss: 0.7998972 Vali Loss: 0.6451468 Test Loss: 0.3814875
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.581655263900757
Epoch: 24, Steps: 56 | Train Loss: 0.7980707 Vali Loss: 0.6445164 Test Loss: 0.3814994
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.0599844455718994
Epoch: 25, Steps: 56 | Train Loss: 0.7994392 Vali Loss: 0.6433020 Test Loss: 0.3815163
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.9698729515075684
Epoch: 26, Steps: 56 | Train Loss: 0.8002881 Vali Loss: 0.6418824 Test Loss: 0.3814953
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.728227138519287
Epoch: 27, Steps: 56 | Train Loss: 0.8000490 Vali Loss: 0.6429179 Test Loss: 0.3815034
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.2941408157348633
Epoch: 28, Steps: 56 | Train Loss: 0.7996477 Vali Loss: 0.6373767 Test Loss: 0.3815354
Validation loss decreased (0.640931 --> 0.637377).  Saving model ...
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 2.710491895675659
Epoch: 29, Steps: 56 | Train Loss: 0.7999640 Vali Loss: 0.6410291 Test Loss: 0.3814970
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.9361913204193115
Epoch: 30, Steps: 56 | Train Loss: 0.7991991 Vali Loss: 0.6386241 Test Loss: 0.3815368
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.18990159034729
Epoch: 31, Steps: 56 | Train Loss: 0.7990226 Vali Loss: 0.6431547 Test Loss: 0.3815256
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 2.3861052989959717
Epoch: 32, Steps: 56 | Train Loss: 0.7996154 Vali Loss: 0.6449586 Test Loss: 0.3814987
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.211437702178955
Epoch: 33, Steps: 56 | Train Loss: 0.7989991 Vali Loss: 0.6411109 Test Loss: 0.3815184
EarlyStopping counter: 5 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.034146308898926
Epoch: 34, Steps: 56 | Train Loss: 0.7992346 Vali Loss: 0.6425020 Test Loss: 0.3815362
EarlyStopping counter: 6 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.92690110206604
Epoch: 35, Steps: 56 | Train Loss: 0.7995456 Vali Loss: 0.6415438 Test Loss: 0.3815462
EarlyStopping counter: 7 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.9295597076416016
Epoch: 36, Steps: 56 | Train Loss: 0.8010320 Vali Loss: 0.6403949 Test Loss: 0.3815289
EarlyStopping counter: 8 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.803267478942871
Epoch: 37, Steps: 56 | Train Loss: 0.8012563 Vali Loss: 0.6374162 Test Loss: 0.3815391
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.8496148586273193
Epoch: 38, Steps: 56 | Train Loss: 0.7995116 Vali Loss: 0.6424410 Test Loss: 0.3815261
EarlyStopping counter: 10 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.9529526233673096
Epoch: 39, Steps: 56 | Train Loss: 0.8001438 Vali Loss: 0.6397048 Test Loss: 0.3815342
EarlyStopping counter: 11 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.586735725402832
Epoch: 40, Steps: 56 | Train Loss: 0.7998967 Vali Loss: 0.6458304 Test Loss: 0.3815246
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.439066171646118
Epoch: 41, Steps: 56 | Train Loss: 0.7986457 Vali Loss: 0.6390459 Test Loss: 0.3815439
EarlyStopping counter: 13 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.5442628860473633
Epoch: 42, Steps: 56 | Train Loss: 0.8002472 Vali Loss: 0.6374376 Test Loss: 0.3815393
EarlyStopping counter: 14 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.7185609340667725
Epoch: 43, Steps: 56 | Train Loss: 0.8000946 Vali Loss: 0.6443795 Test Loss: 0.3815275
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 2.642754554748535
Epoch: 44, Steps: 56 | Train Loss: 0.7996114 Vali Loss: 0.6382384 Test Loss: 0.3815302
EarlyStopping counter: 16 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.3476600646972656
Epoch: 45, Steps: 56 | Train Loss: 0.7996981 Vali Loss: 0.6430978 Test Loss: 0.3815355
EarlyStopping counter: 17 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.955282688140869
Epoch: 46, Steps: 56 | Train Loss: 0.7989366 Vali Loss: 0.6390996 Test Loss: 0.3815405
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.478929042816162
Epoch: 47, Steps: 56 | Train Loss: 0.7993423 Vali Loss: 0.6399396 Test Loss: 0.3815245
EarlyStopping counter: 19 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.632749319076538
Epoch: 48, Steps: 56 | Train Loss: 0.7987720 Vali Loss: 0.6390390 Test Loss: 0.3815175
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_720_720_FITS_ETTh2_ftM_sl720_ll48_pl720_H3_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.3801315426826477, mae:0.4237793982028961, rse:0.4928022623062134, corr:[ 2.18537092e-01  2.19987527e-01  2.20058709e-01  2.18886346e-01
  2.17648670e-01  2.16919422e-01  2.16657877e-01  2.16189221e-01
  2.15180084e-01  2.13498384e-01  2.11799279e-01  2.10348204e-01
  2.09393159e-01  2.08826691e-01  2.08239689e-01  2.07401246e-01
  2.06270099e-01  2.05158129e-01  2.04314575e-01  2.03764439e-01
  2.03182101e-01  2.02214241e-01  2.00806364e-01  1.99055806e-01
  1.97462112e-01  1.96323141e-01  1.95691735e-01  1.95149437e-01
  1.94467515e-01  1.93473265e-01  1.92363188e-01  1.91266611e-01
  1.90312535e-01  1.89579919e-01  1.88931420e-01  1.88152790e-01
  1.87215701e-01  1.86316177e-01  1.85678273e-01  1.85185537e-01
  1.84747577e-01  1.84176594e-01  1.83445394e-01  1.82718217e-01
  1.82056844e-01  1.81343555e-01  1.80503204e-01  1.79077163e-01
  1.77375764e-01  1.75773069e-01  1.74566969e-01  1.73658177e-01
  1.73066899e-01  1.72659203e-01  1.72047839e-01  1.71284690e-01
  1.70538515e-01  1.69799685e-01  1.69156522e-01  1.68717891e-01
  1.68552116e-01  1.68428242e-01  1.68405771e-01  1.68599099e-01
  1.68603569e-01  1.68676808e-01  1.68807879e-01  1.68936878e-01
  1.68959409e-01  1.68834105e-01  1.68594971e-01  1.68188974e-01
  1.67711660e-01  1.67147025e-01  1.66707650e-01  1.66251108e-01
  1.66000247e-01  1.65946215e-01  1.65857196e-01  1.65604994e-01
  1.65464118e-01  1.65499449e-01  1.65576994e-01  1.65711567e-01
  1.65903643e-01  1.66111201e-01  1.66205496e-01  1.66147381e-01
  1.65947407e-01  1.65827349e-01  1.65844500e-01  1.65851563e-01
  1.66067824e-01  1.66261554e-01  1.66414067e-01  1.66403830e-01
  1.66240856e-01  1.66047812e-01  1.65818244e-01  1.65575370e-01
  1.65449083e-01  1.65152952e-01  1.64907411e-01  1.64669320e-01
  1.64651662e-01  1.64715394e-01  1.64892703e-01  1.65113792e-01
  1.65035784e-01  1.64663479e-01  1.64237425e-01  1.64032727e-01
  1.63979411e-01  1.64064094e-01  1.64111242e-01  1.63997129e-01
  1.63684800e-01  1.62975535e-01  1.62188262e-01  1.61378890e-01
  1.60635158e-01  1.59914851e-01  1.59205869e-01  1.58560365e-01
  1.57798216e-01  1.57091931e-01  1.56500503e-01  1.56052649e-01
  1.55712634e-01  1.55186176e-01  1.54663846e-01  1.54005527e-01
  1.53476879e-01  1.53032526e-01  1.52682796e-01  1.52428448e-01
  1.51930302e-01  1.51282787e-01  1.50668010e-01  1.50215968e-01
  1.50068343e-01  1.49887294e-01  1.49463817e-01  1.48659289e-01
  1.47398129e-01  1.46232173e-01  1.45429671e-01  1.44955382e-01
  1.44519299e-01  1.44021511e-01  1.43510818e-01  1.42942801e-01
  1.42531589e-01  1.42234996e-01  1.41973391e-01  1.41530707e-01
  1.41007975e-01  1.40480101e-01  1.39978588e-01  1.39575973e-01
  1.39197662e-01  1.39170125e-01  1.39498264e-01  1.39962897e-01
  1.40392169e-01  1.40393391e-01  1.39953226e-01  1.39166355e-01
  1.38360336e-01  1.37755230e-01  1.37485430e-01  1.37370676e-01
  1.37232751e-01  1.36797264e-01  1.36039630e-01  1.35121375e-01
  1.34290561e-01  1.33623809e-01  1.32971555e-01  1.32344022e-01
  1.31856188e-01  1.31589606e-01  1.31664723e-01  1.31920531e-01
  1.32106349e-01  1.32319510e-01  1.32483438e-01  1.32587045e-01
  1.32736981e-01  1.33035049e-01  1.33597553e-01  1.34148151e-01
  1.34437203e-01  1.34549543e-01  1.34492189e-01  1.34457976e-01
  1.34547427e-01  1.34573564e-01  1.34713218e-01  1.34735391e-01
  1.34620354e-01  1.34305403e-01  1.34088501e-01  1.33789226e-01
  1.33608267e-01  1.33732498e-01  1.33997977e-01  1.34319767e-01
  1.34587690e-01  1.34838969e-01  1.35063082e-01  1.35360509e-01
  1.35834888e-01  1.36262372e-01  1.36501774e-01  1.36266097e-01
  1.35719597e-01  1.35048494e-01  1.34436354e-01  1.34209052e-01
  1.34097561e-01  1.34263545e-01  1.34326801e-01  1.34540647e-01
  1.34730950e-01  1.34860575e-01  1.35007814e-01  1.35065600e-01
  1.35216668e-01  1.35400087e-01  1.35744497e-01  1.36305407e-01
  1.36914656e-01  1.37483925e-01  1.37988433e-01  1.38420701e-01
  1.38924345e-01  1.39481351e-01  1.40169472e-01  1.40964404e-01
  1.41682282e-01  1.42040864e-01  1.42087802e-01  1.42104968e-01
  1.42113686e-01  1.42124102e-01  1.42437324e-01  1.42878488e-01
  1.43494263e-01  1.44053668e-01  1.44594252e-01  1.45091504e-01
  1.45633042e-01  1.46360263e-01  1.47163466e-01  1.48148358e-01
  1.48939356e-01  1.49545357e-01  1.49967805e-01  1.50512964e-01
  1.51272252e-01  1.52071312e-01  1.52962998e-01  1.53641656e-01
  1.54105514e-01  1.54339448e-01  1.54555604e-01  1.54966593e-01
  1.55293986e-01  1.55622259e-01  1.55908331e-01  1.56153813e-01
  1.56453535e-01  1.56687185e-01  1.57066062e-01  1.57475635e-01
  1.57856494e-01  1.58427358e-01  1.59016281e-01  1.59612477e-01
  1.60227671e-01  1.60788640e-01  1.61161989e-01  1.61442131e-01
  1.61632001e-01  1.61825210e-01  1.62237942e-01  1.62681401e-01
  1.63035765e-01  1.63172737e-01  1.62968233e-01  1.62806302e-01
  1.62575275e-01  1.62366480e-01  1.62327379e-01  1.62561744e-01
  1.62929669e-01  1.63148835e-01  1.63177639e-01  1.63131058e-01
  1.63014784e-01  1.63219407e-01  1.63794398e-01  1.64507598e-01
  1.64888799e-01  1.65043086e-01  1.64867684e-01  1.64839000e-01
  1.65020436e-01  1.65418014e-01  1.65892497e-01  1.66169584e-01
  1.66305751e-01  1.66160211e-01  1.66058362e-01  1.65897861e-01
  1.65768415e-01  1.65680155e-01  1.65553451e-01  1.65356934e-01
  1.64880097e-01  1.64434254e-01  1.64167821e-01  1.64230242e-01
  1.64476871e-01  1.64771006e-01  1.65289119e-01  1.65683553e-01
  1.65856302e-01  1.65889412e-01  1.65957078e-01  1.66464359e-01
  1.67338297e-01  1.68290958e-01  1.69214383e-01  1.69812277e-01
  1.69894114e-01  1.69778749e-01  1.69839039e-01  1.69960037e-01
  1.70195937e-01  1.70312062e-01  1.70459002e-01  1.70520172e-01
  1.70538351e-01  1.70639604e-01  1.70732513e-01  1.71118498e-01
  1.71619430e-01  1.72186226e-01  1.72769710e-01  1.73387721e-01
  1.73933789e-01  1.74534559e-01  1.75124675e-01  1.75855249e-01
  1.76593870e-01  1.77225858e-01  1.77682504e-01  1.77884772e-01
  1.77935973e-01  1.77935466e-01  1.78103194e-01  1.78350985e-01
  1.78726837e-01  1.79078192e-01  1.79394081e-01  1.79764688e-01
  1.80053726e-01  1.80487782e-01  1.80805877e-01  1.80969551e-01
  1.80876613e-01  1.80710062e-01  1.80647105e-01  1.80704176e-01
  1.80908278e-01  1.81135044e-01  1.81183144e-01  1.81226224e-01
  1.81313038e-01  1.81402445e-01  1.81419924e-01  1.81440577e-01
  1.81417346e-01  1.81320697e-01  1.81298867e-01  1.81448489e-01
  1.81820184e-01  1.82310894e-01  1.82841837e-01  1.83362454e-01
  1.83761746e-01  1.83970988e-01  1.84098259e-01  1.84089959e-01
  1.84131712e-01  1.84033081e-01  1.83953553e-01  1.83956355e-01
  1.83852673e-01  1.83587909e-01  1.83154747e-01  1.82785913e-01
  1.82555437e-01  1.82429135e-01  1.82333559e-01  1.82264477e-01
  1.82105005e-01  1.81910589e-01  1.81725055e-01  1.81495637e-01
  1.81323677e-01  1.81206152e-01  1.81118071e-01  1.81173757e-01
  1.81316867e-01  1.81475177e-01  1.81420624e-01  1.81208134e-01
  1.80655345e-01  1.79850370e-01  1.78870484e-01  1.77832752e-01
  1.76783442e-01  1.75692230e-01  1.74801037e-01  1.73992291e-01
  1.73257545e-01  1.72628686e-01  1.71998516e-01  1.71541438e-01
  1.71122447e-01  1.70661584e-01  1.70307636e-01  1.69868022e-01
  1.69370040e-01  1.68891996e-01  1.68465406e-01  1.68104127e-01
  1.67522535e-01  1.66994989e-01  1.66389197e-01  1.65845454e-01
  1.65230811e-01  1.64837599e-01  1.64549932e-01  1.64404139e-01
  1.64179564e-01  1.63794965e-01  1.63333341e-01  1.63101479e-01
  1.62914902e-01  1.62776917e-01  1.62643775e-01  1.62445039e-01
  1.62191480e-01  1.61826566e-01  1.61399454e-01  1.61184162e-01
  1.61086768e-01  1.60888165e-01  1.60583973e-01  1.60354033e-01
  1.59972847e-01  1.59671277e-01  1.59396976e-01  1.58872083e-01
  1.58263624e-01  1.57517835e-01  1.56944230e-01  1.56496182e-01
  1.56200036e-01  1.56001195e-01  1.55971542e-01  1.55972093e-01
  1.55818775e-01  1.55566350e-01  1.55182317e-01  1.54708549e-01
  1.54063940e-01  1.53322980e-01  1.52681679e-01  1.51796475e-01
  1.50839582e-01  1.49990529e-01  1.49367392e-01  1.48800895e-01
  1.48314208e-01  1.47741556e-01  1.47089750e-01  1.46145642e-01
  1.45034477e-01  1.44071445e-01  1.43562555e-01  1.43429190e-01
  1.43191934e-01  1.42830744e-01  1.42337859e-01  1.41766220e-01
  1.41218007e-01  1.40688881e-01  1.40268832e-01  1.39618456e-01
  1.38606504e-01  1.37532622e-01  1.36617973e-01  1.35814577e-01
  1.35177925e-01  1.34843305e-01  1.34504378e-01  1.34135649e-01
  1.33593827e-01  1.32784963e-01  1.31830379e-01  1.30679429e-01
  1.29655406e-01  1.28888845e-01  1.28513083e-01  1.28229097e-01
  1.27792835e-01  1.27325535e-01  1.26747504e-01  1.26149982e-01
  1.25504196e-01  1.24876462e-01  1.24053679e-01  1.22888505e-01
  1.21318892e-01  1.19767845e-01  1.18610360e-01  1.17522933e-01
  1.16550654e-01  1.15579747e-01  1.14590608e-01  1.13591976e-01
  1.12560250e-01  1.11589976e-01  1.10736556e-01  1.09880775e-01
  1.08923852e-01  1.07814550e-01  1.06711477e-01  1.05672099e-01
  1.04625717e-01  1.03814214e-01  1.02996312e-01  1.02293439e-01
  1.01740465e-01  1.01009592e-01  1.00213021e-01  9.91170704e-02
  9.79142189e-02  9.66982171e-02  9.56114754e-02  9.46343318e-02
  9.37693641e-02  9.30210650e-02  9.22783762e-02  9.13141295e-02
  9.04955491e-02  8.97197723e-02  8.92511532e-02  8.88475552e-02
  8.85130391e-02  8.81611109e-02  8.77606869e-02  8.70814994e-02
  8.64564776e-02  8.58799815e-02  8.52548853e-02  8.45895559e-02
  8.37716684e-02  8.28594491e-02  8.16632286e-02  8.01986903e-02
  7.84647316e-02  7.69877359e-02  7.59034678e-02  7.51480758e-02
  7.45956376e-02  7.39478692e-02  7.30596483e-02  7.19211623e-02
  7.06974790e-02  6.96359575e-02  6.86810836e-02  6.79251105e-02
  6.70988560e-02  6.62343130e-02  6.54965118e-02  6.48674220e-02
  6.44803047e-02  6.42835125e-02  6.39940277e-02  6.36407137e-02
  6.31234050e-02  6.24423437e-02  6.14399388e-02  6.02765530e-02
  5.91405481e-02  5.82283437e-02  5.71919344e-02  5.61417043e-02
  5.47813177e-02  5.33951186e-02  5.19961156e-02  5.07106930e-02
  4.94929142e-02  4.85564061e-02  4.80090268e-02  4.73875739e-02
  4.67224717e-02  4.61323597e-02  4.58468534e-02  4.57941666e-02
  4.57279198e-02  4.58134897e-02  4.57775109e-02  4.56809141e-02
  4.54337001e-02  4.52229008e-02  4.48080897e-02  4.44794260e-02
  4.39833924e-02  4.32013422e-02  4.21584770e-02  4.10058722e-02
  3.99358869e-02  3.91661786e-02  3.85631211e-02  3.79588567e-02
  3.74542661e-02  3.72498855e-02  3.71179916e-02  3.69783267e-02
  3.65381800e-02  3.57642099e-02  3.50365117e-02  3.46576907e-02
  3.42271626e-02  3.39833610e-02  3.38891447e-02  3.36862206e-02
  3.35209630e-02  3.33923064e-02  3.28580812e-02  3.19133811e-02
  3.07489336e-02  2.98046172e-02  2.92027332e-02  2.85599250e-02
  2.79583521e-02  2.72553824e-02  2.65298840e-02  2.60157529e-02
  2.57129110e-02  2.54991744e-02  2.52661388e-02  2.49399301e-02
  2.44791936e-02  2.42417324e-02  2.45334264e-02  2.49666069e-02
  2.52908748e-02  2.52660420e-02  2.50518620e-02  2.49395054e-02
  2.48987209e-02  2.51367427e-02  2.54657958e-02  2.56708786e-02
  2.54824832e-02  2.49751993e-02  2.42565200e-02  2.29673423e-02
  2.16717813e-02  2.04274766e-02  1.98519304e-02  1.94403399e-02
  1.94541812e-02  1.96143724e-02  1.95143018e-02  1.89240538e-02
  1.81958638e-02  1.71859171e-02  1.64997075e-02  1.61882937e-02
  1.61935557e-02  1.65160447e-02  1.70276109e-02  1.70427989e-02
  1.66860987e-02  1.59061104e-02  1.48933213e-02  1.35573195e-02
  1.20666120e-02  1.04958154e-02  9.23350733e-03  7.97487702e-03
  6.87148515e-03  5.61056659e-03  4.46718466e-03  3.11791268e-03
  1.92644109e-03  1.08559744e-03  3.85559601e-04 -1.57901213e-05
 -1.76781250e-04 -3.22390988e-04  3.98962839e-05  4.49790939e-04
  5.45518065e-04  3.75889358e-04 -7.54846842e-05 -2.27843688e-04
  5.65915776e-04  2.41456716e-03  3.19814403e-03  3.69336252e-04]
