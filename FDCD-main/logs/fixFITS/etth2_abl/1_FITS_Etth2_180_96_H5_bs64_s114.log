Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=50, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_180_96', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=96, root_path='./dataset/', seed=114, seq_len=180, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_180_96_FITS_ETTh2_ftM_sl180_ll48_pl96_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8365
val 2785
test 2785
Model(
  (freq_upsampler): Linear(in_features=50, out_features=76, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  3404800.0
params:  3876.0
Trainable parameters:  3876
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.6310911178588867
Epoch: 1, Steps: 65 | Train Loss: 0.5743087 Vali Loss: 0.2653928 Test Loss: 0.3624586
Validation loss decreased (inf --> 0.265393).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 3.102907180786133
Epoch: 2, Steps: 65 | Train Loss: 0.4833407 Vali Loss: 0.2398541 Test Loss: 0.3273279
Validation loss decreased (0.265393 --> 0.239854).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.911708116531372
Epoch: 3, Steps: 65 | Train Loss: 0.4553870 Vali Loss: 0.2278927 Test Loss: 0.3143288
Validation loss decreased (0.239854 --> 0.227893).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.296431541442871
Epoch: 4, Steps: 65 | Train Loss: 0.4416469 Vali Loss: 0.2246567 Test Loss: 0.3081196
Validation loss decreased (0.227893 --> 0.224657).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 2.915214776992798
Epoch: 5, Steps: 65 | Train Loss: 0.4355844 Vali Loss: 0.2218615 Test Loss: 0.3043235
Validation loss decreased (0.224657 --> 0.221861).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 2.5675528049468994
Epoch: 6, Steps: 65 | Train Loss: 0.4317699 Vali Loss: 0.2190313 Test Loss: 0.3017150
Validation loss decreased (0.221861 --> 0.219031).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.8508057594299316
Epoch: 7, Steps: 65 | Train Loss: 0.4278588 Vali Loss: 0.2166017 Test Loss: 0.2998170
Validation loss decreased (0.219031 --> 0.216602).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.4740149974823
Epoch: 8, Steps: 65 | Train Loss: 0.4260833 Vali Loss: 0.2162771 Test Loss: 0.2982695
Validation loss decreased (0.216602 --> 0.216277).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.6406240463256836
Epoch: 9, Steps: 65 | Train Loss: 0.4225756 Vali Loss: 0.2144962 Test Loss: 0.2971703
Validation loss decreased (0.216277 --> 0.214496).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.425508975982666
Epoch: 10, Steps: 65 | Train Loss: 0.4216818 Vali Loss: 0.2146619 Test Loss: 0.2960873
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.413534164428711
Epoch: 11, Steps: 65 | Train Loss: 0.4206827 Vali Loss: 0.2142641 Test Loss: 0.2952797
Validation loss decreased (0.214496 --> 0.214264).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.568464994430542
Epoch: 12, Steps: 65 | Train Loss: 0.4175626 Vali Loss: 0.2129529 Test Loss: 0.2946379
Validation loss decreased (0.214264 --> 0.212953).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 3.363405466079712
Epoch: 13, Steps: 65 | Train Loss: 0.4181899 Vali Loss: 0.2122902 Test Loss: 0.2940434
Validation loss decreased (0.212953 --> 0.212290).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.8859760761260986
Epoch: 14, Steps: 65 | Train Loss: 0.4168447 Vali Loss: 0.2120317 Test Loss: 0.2934856
Validation loss decreased (0.212290 --> 0.212032).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.227161407470703
Epoch: 15, Steps: 65 | Train Loss: 0.4150743 Vali Loss: 0.2124274 Test Loss: 0.2929963
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.3494131565093994
Epoch: 16, Steps: 65 | Train Loss: 0.4143484 Vali Loss: 0.2113006 Test Loss: 0.2926282
Validation loss decreased (0.212032 --> 0.211301).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 2.7103123664855957
Epoch: 17, Steps: 65 | Train Loss: 0.4147982 Vali Loss: 0.2118672 Test Loss: 0.2922142
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.3498916625976562
Epoch: 18, Steps: 65 | Train Loss: 0.4135913 Vali Loss: 0.2109862 Test Loss: 0.2919466
Validation loss decreased (0.211301 --> 0.210986).  Saving model ...
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.1378462314605713
Epoch: 19, Steps: 65 | Train Loss: 0.4133623 Vali Loss: 0.2095106 Test Loss: 0.2916583
Validation loss decreased (0.210986 --> 0.209511).  Saving model ...
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.1297528743743896
Epoch: 20, Steps: 65 | Train Loss: 0.4115109 Vali Loss: 0.2089927 Test Loss: 0.2913649
Validation loss decreased (0.209511 --> 0.208993).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 2.796393394470215
Epoch: 21, Steps: 65 | Train Loss: 0.4119793 Vali Loss: 0.2101544 Test Loss: 0.2911028
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.266671180725098
Epoch: 22, Steps: 65 | Train Loss: 0.4103459 Vali Loss: 0.2104676 Test Loss: 0.2908804
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.444918632507324
Epoch: 23, Steps: 65 | Train Loss: 0.4116075 Vali Loss: 0.2103290 Test Loss: 0.2907044
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 4.4049811363220215
Epoch: 24, Steps: 65 | Train Loss: 0.4107238 Vali Loss: 0.2104192 Test Loss: 0.2905399
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 3.172466278076172
Epoch: 25, Steps: 65 | Train Loss: 0.4111362 Vali Loss: 0.2097549 Test Loss: 0.2903523
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.246485710144043
Epoch: 26, Steps: 65 | Train Loss: 0.4109346 Vali Loss: 0.2087849 Test Loss: 0.2902859
Validation loss decreased (0.208993 --> 0.208785).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.8576955795288086
Epoch: 27, Steps: 65 | Train Loss: 0.4102773 Vali Loss: 0.2088976 Test Loss: 0.2900804
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.1085493564605713
Epoch: 28, Steps: 65 | Train Loss: 0.4099514 Vali Loss: 0.2094029 Test Loss: 0.2899821
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.996145486831665
Epoch: 29, Steps: 65 | Train Loss: 0.4108605 Vali Loss: 0.2093371 Test Loss: 0.2898347
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.7828516960144043
Epoch: 30, Steps: 65 | Train Loss: 0.4099384 Vali Loss: 0.2088713 Test Loss: 0.2897142
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.8649356365203857
Epoch: 31, Steps: 65 | Train Loss: 0.4090313 Vali Loss: 0.2095553 Test Loss: 0.2895782
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.541132926940918
Epoch: 32, Steps: 65 | Train Loss: 0.4088763 Vali Loss: 0.2082441 Test Loss: 0.2894779
Validation loss decreased (0.208785 --> 0.208244).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.7396650314331055
Epoch: 33, Steps: 65 | Train Loss: 0.4085922 Vali Loss: 0.2091179 Test Loss: 0.2894467
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 2.7664172649383545
Epoch: 34, Steps: 65 | Train Loss: 0.4066460 Vali Loss: 0.2093592 Test Loss: 0.2893479
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.323550224304199
Epoch: 35, Steps: 65 | Train Loss: 0.4081131 Vali Loss: 0.2081927 Test Loss: 0.2892774
Validation loss decreased (0.208244 --> 0.208193).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.4289910793304443
Epoch: 36, Steps: 65 | Train Loss: 0.4089326 Vali Loss: 0.2095850 Test Loss: 0.2892376
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 3.732760190963745
Epoch: 37, Steps: 65 | Train Loss: 0.4090363 Vali Loss: 0.2088555 Test Loss: 0.2891213
EarlyStopping counter: 2 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.299132823944092
Epoch: 38, Steps: 65 | Train Loss: 0.4086885 Vali Loss: 0.2088688 Test Loss: 0.2890947
EarlyStopping counter: 3 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.996356964111328
Epoch: 39, Steps: 65 | Train Loss: 0.4084542 Vali Loss: 0.2089570 Test Loss: 0.2890465
EarlyStopping counter: 4 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.196744441986084
Epoch: 40, Steps: 65 | Train Loss: 0.4078685 Vali Loss: 0.2076914 Test Loss: 0.2890049
Validation loss decreased (0.208193 --> 0.207691).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.1774654388427734
Epoch: 41, Steps: 65 | Train Loss: 0.4080468 Vali Loss: 0.2088672 Test Loss: 0.2889383
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 3.611379861831665
Epoch: 42, Steps: 65 | Train Loss: 0.4064534 Vali Loss: 0.2098207 Test Loss: 0.2888668
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.3807144165039062
Epoch: 43, Steps: 65 | Train Loss: 0.4074215 Vali Loss: 0.2066696 Test Loss: 0.2888407
Validation loss decreased (0.207691 --> 0.206670).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.175429105758667
Epoch: 44, Steps: 65 | Train Loss: 0.4080059 Vali Loss: 0.2084692 Test Loss: 0.2888050
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 2.7007131576538086
Epoch: 45, Steps: 65 | Train Loss: 0.4081467 Vali Loss: 0.2088517 Test Loss: 0.2887791
EarlyStopping counter: 2 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.487522840499878
Epoch: 46, Steps: 65 | Train Loss: 0.4059553 Vali Loss: 0.2090696 Test Loss: 0.2887165
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.066276788711548
Epoch: 47, Steps: 65 | Train Loss: 0.4064396 Vali Loss: 0.2088268 Test Loss: 0.2887035
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.3159890174865723
Epoch: 48, Steps: 65 | Train Loss: 0.4074192 Vali Loss: 0.2073767 Test Loss: 0.2886439
EarlyStopping counter: 5 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.2111825942993164
Epoch: 49, Steps: 65 | Train Loss: 0.4067094 Vali Loss: 0.2088195 Test Loss: 0.2886219
EarlyStopping counter: 6 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.0142815113067627
Epoch: 50, Steps: 65 | Train Loss: 0.4078369 Vali Loss: 0.2080822 Test Loss: 0.2886052
EarlyStopping counter: 7 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.4661121368408203
Epoch: 51, Steps: 65 | Train Loss: 0.4073966 Vali Loss: 0.2082672 Test Loss: 0.2885857
EarlyStopping counter: 8 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 3.6326839923858643
Epoch: 52, Steps: 65 | Train Loss: 0.4075523 Vali Loss: 0.2085900 Test Loss: 0.2885704
EarlyStopping counter: 9 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.1116795539855957
Epoch: 53, Steps: 65 | Train Loss: 0.4051711 Vali Loss: 0.2070624 Test Loss: 0.2885281
EarlyStopping counter: 10 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 2.676955461502075
Epoch: 54, Steps: 65 | Train Loss: 0.4063438 Vali Loss: 0.2080062 Test Loss: 0.2885084
EarlyStopping counter: 11 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 4.2976720333099365
Epoch: 55, Steps: 65 | Train Loss: 0.4057009 Vali Loss: 0.2089177 Test Loss: 0.2884810
EarlyStopping counter: 12 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 3.062373399734497
Epoch: 56, Steps: 65 | Train Loss: 0.4065497 Vali Loss: 0.2088363 Test Loss: 0.2884671
EarlyStopping counter: 13 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.977170467376709
Epoch: 57, Steps: 65 | Train Loss: 0.4050174 Vali Loss: 0.2086621 Test Loss: 0.2884461
EarlyStopping counter: 14 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 2.130281448364258
Epoch: 58, Steps: 65 | Train Loss: 0.4062194 Vali Loss: 0.2078873 Test Loss: 0.2884272
EarlyStopping counter: 15 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.1651346683502197
Epoch: 59, Steps: 65 | Train Loss: 0.4056283 Vali Loss: 0.2090058 Test Loss: 0.2883965
EarlyStopping counter: 16 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 3.3055286407470703
Epoch: 60, Steps: 65 | Train Loss: 0.4071414 Vali Loss: 0.2076342 Test Loss: 0.2883891
EarlyStopping counter: 17 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.6910831928253174
Epoch: 61, Steps: 65 | Train Loss: 0.4061855 Vali Loss: 0.2078659 Test Loss: 0.2883849
EarlyStopping counter: 18 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.2777717113494873
Epoch: 62, Steps: 65 | Train Loss: 0.4072876 Vali Loss: 0.2085217 Test Loss: 0.2883610
EarlyStopping counter: 19 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 1.8652632236480713
Epoch: 63, Steps: 65 | Train Loss: 0.4072974 Vali Loss: 0.2084902 Test Loss: 0.2883544
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_180_96_FITS_ETTh2_ftM_sl180_ll48_pl96_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2785
mse:0.2885301411151886, mae:0.34110134840011597, rse:0.43288934230804443, corr:[0.27290526 0.2783478  0.27548188 0.2755089  0.27533576 0.27356812
 0.27259418 0.27192748 0.27058008 0.26914957 0.26789364 0.26631802
 0.26484105 0.26358452 0.26253337 0.26201525 0.26158074 0.26068845
 0.2596377  0.2586314  0.25734913 0.25604132 0.25482005 0.2527901
 0.24968056 0.24726383 0.24590728 0.24431835 0.24210756 0.24061711
 0.23936895 0.2371192  0.23474209 0.23362578 0.23247796 0.2303145
 0.22868212 0.22799268 0.22702703 0.2255869  0.22522385 0.22508986
 0.22389354 0.22235642 0.22176522 0.22106443 0.21944997 0.21736914
 0.21479526 0.21216096 0.20983866 0.20848544 0.20706    0.20508443
 0.20299101 0.20126694 0.19960004 0.19782892 0.19683821 0.19590311
 0.19481884 0.1939842  0.19456024 0.19454896 0.19377762 0.19326681
 0.19286966 0.19179659 0.1911717  0.1912101  0.19039984 0.18864591
 0.1864913  0.18521024 0.18393752 0.1824711  0.18129304 0.1808314
 0.17996898 0.17896612 0.17900409 0.17911583 0.17906189 0.17919551
 0.17967121 0.17946206 0.17964908 0.18014403 0.18036859 0.17956533
 0.17888685 0.1790621  0.18011655 0.1798173  0.18043151 0.18528505]
