Args in experiment:
Namespace(H_order=6, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=34, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_720', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=720, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=2, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H6_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=34, out_features=306, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9321984.0
params:  10710.0
Trainable parameters:  10710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 3.197533369064331
Epoch: 1, Steps: 61 | Train Loss: 1.2830981 Vali Loss: 0.8556917 Test Loss: 0.6874515
Validation loss decreased (inf --> 0.855692).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.054440975189209
Epoch: 2, Steps: 61 | Train Loss: 1.0650644 Vali Loss: 0.7746530 Test Loss: 0.5883635
Validation loss decreased (0.855692 --> 0.774653).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.7511558532714844
Epoch: 3, Steps: 61 | Train Loss: 0.9496719 Vali Loss: 0.7251636 Test Loss: 0.5306595
Validation loss decreased (0.774653 --> 0.725164).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.8837954998016357
Epoch: 4, Steps: 61 | Train Loss: 0.8847884 Vali Loss: 0.6935896 Test Loss: 0.4951343
Validation loss decreased (0.725164 --> 0.693590).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.909977197647095
Epoch: 5, Steps: 61 | Train Loss: 0.8449768 Vali Loss: 0.6741305 Test Loss: 0.4728318
Validation loss decreased (0.693590 --> 0.674130).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.1024651527404785
Epoch: 6, Steps: 61 | Train Loss: 0.8191104 Vali Loss: 0.6685252 Test Loss: 0.4581639
Validation loss decreased (0.674130 --> 0.668525).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.532137155532837
Epoch: 7, Steps: 61 | Train Loss: 0.8029568 Vali Loss: 0.6580722 Test Loss: 0.4485860
Validation loss decreased (0.668525 --> 0.658072).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.893542766571045
Epoch: 8, Steps: 61 | Train Loss: 0.7921931 Vali Loss: 0.6466949 Test Loss: 0.4420501
Validation loss decreased (0.658072 --> 0.646695).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 3.1344611644744873
Epoch: 9, Steps: 61 | Train Loss: 0.7848535 Vali Loss: 0.6415877 Test Loss: 0.4375324
Validation loss decreased (0.646695 --> 0.641588).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.341717004776001
Epoch: 10, Steps: 61 | Train Loss: 0.7798635 Vali Loss: 0.6431655 Test Loss: 0.4343275
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.6293845176696777
Epoch: 11, Steps: 61 | Train Loss: 0.7751668 Vali Loss: 0.6411753 Test Loss: 0.4319866
Validation loss decreased (0.641588 --> 0.641175).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 3.7046194076538086
Epoch: 12, Steps: 61 | Train Loss: 0.7731352 Vali Loss: 0.6402972 Test Loss: 0.4302737
Validation loss decreased (0.641175 --> 0.640297).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.9642016887664795
Epoch: 13, Steps: 61 | Train Loss: 0.7705143 Vali Loss: 0.6356096 Test Loss: 0.4289751
Validation loss decreased (0.640297 --> 0.635610).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.674877882003784
Epoch: 14, Steps: 61 | Train Loss: 0.7690982 Vali Loss: 0.6377903 Test Loss: 0.4279696
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 3.3440017700195312
Epoch: 15, Steps: 61 | Train Loss: 0.7681794 Vali Loss: 0.6301436 Test Loss: 0.4271414
Validation loss decreased (0.635610 --> 0.630144).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 3.6518638134002686
Epoch: 16, Steps: 61 | Train Loss: 0.7666620 Vali Loss: 0.6354974 Test Loss: 0.4264790
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.296874046325684
Epoch: 17, Steps: 61 | Train Loss: 0.7656815 Vali Loss: 0.6317233 Test Loss: 0.4259382
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 3.405784845352173
Epoch: 18, Steps: 61 | Train Loss: 0.7652160 Vali Loss: 0.6339110 Test Loss: 0.4255100
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 3.541886568069458
Epoch: 19, Steps: 61 | Train Loss: 0.7646746 Vali Loss: 0.6361169 Test Loss: 0.4250862
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.743079900741577
Epoch: 20, Steps: 61 | Train Loss: 0.7651145 Vali Loss: 0.6316638 Test Loss: 0.4247728
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.092827796936035
Epoch: 21, Steps: 61 | Train Loss: 0.7637612 Vali Loss: 0.6308419 Test Loss: 0.4244957
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.596178293228149
Epoch: 22, Steps: 61 | Train Loss: 0.7619526 Vali Loss: 0.6291823 Test Loss: 0.4242426
Validation loss decreased (0.630144 --> 0.629182).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.918628215789795
Epoch: 23, Steps: 61 | Train Loss: 0.7624912 Vali Loss: 0.6307089 Test Loss: 0.4240104
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 3.05920672416687
Epoch: 24, Steps: 61 | Train Loss: 0.7623694 Vali Loss: 0.6314464 Test Loss: 0.4238141
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.432420253753662
Epoch: 25, Steps: 61 | Train Loss: 0.7616027 Vali Loss: 0.6325417 Test Loss: 0.4236144
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.272181272506714
Epoch: 26, Steps: 61 | Train Loss: 0.7616518 Vali Loss: 0.6312539 Test Loss: 0.4234517
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.3482847213745117
Epoch: 27, Steps: 61 | Train Loss: 0.7620403 Vali Loss: 0.6301122 Test Loss: 0.4233049
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.884838581085205
Epoch: 28, Steps: 61 | Train Loss: 0.7607380 Vali Loss: 0.6342459 Test Loss: 0.4231657
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.550189733505249
Epoch: 29, Steps: 61 | Train Loss: 0.7612550 Vali Loss: 0.6313670 Test Loss: 0.4230553
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 2.866124391555786
Epoch: 30, Steps: 61 | Train Loss: 0.7616151 Vali Loss: 0.6270177 Test Loss: 0.4229301
Validation loss decreased (0.629182 --> 0.627018).  Saving model ...
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 3.47827410697937
Epoch: 31, Steps: 61 | Train Loss: 0.7611136 Vali Loss: 0.6303782 Test Loss: 0.4228292
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.6254987716674805
Epoch: 32, Steps: 61 | Train Loss: 0.7613634 Vali Loss: 0.6285801 Test Loss: 0.4227379
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 5.045357942581177
Epoch: 33, Steps: 61 | Train Loss: 0.7611946 Vali Loss: 0.6321205 Test Loss: 0.4226428
EarlyStopping counter: 3 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 4.436777114868164
Epoch: 34, Steps: 61 | Train Loss: 0.7612905 Vali Loss: 0.6317217 Test Loss: 0.4225588
EarlyStopping counter: 4 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 3.0875444412231445
Epoch: 35, Steps: 61 | Train Loss: 0.7609595 Vali Loss: 0.6299158 Test Loss: 0.4224850
EarlyStopping counter: 5 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.2496752738952637
Epoch: 36, Steps: 61 | Train Loss: 0.7593891 Vali Loss: 0.6344327 Test Loss: 0.4224189
EarlyStopping counter: 6 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.567079544067383
Epoch: 37, Steps: 61 | Train Loss: 0.7596815 Vali Loss: 0.6301647 Test Loss: 0.4223467
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 2.6802496910095215
Epoch: 38, Steps: 61 | Train Loss: 0.7607843 Vali Loss: 0.6300827 Test Loss: 0.4222864
EarlyStopping counter: 8 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.460622787475586
Epoch: 39, Steps: 61 | Train Loss: 0.7594274 Vali Loss: 0.6312433 Test Loss: 0.4222265
EarlyStopping counter: 9 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 4.083940029144287
Epoch: 40, Steps: 61 | Train Loss: 0.7597818 Vali Loss: 0.6276329 Test Loss: 0.4221675
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 3.1378328800201416
Epoch: 41, Steps: 61 | Train Loss: 0.7584153 Vali Loss: 0.6328706 Test Loss: 0.4221226
EarlyStopping counter: 11 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.9506640434265137
Epoch: 42, Steps: 61 | Train Loss: 0.7604604 Vali Loss: 0.6312899 Test Loss: 0.4220704
EarlyStopping counter: 12 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 4.211986064910889
Epoch: 43, Steps: 61 | Train Loss: 0.7585342 Vali Loss: 0.6302911 Test Loss: 0.4220257
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.3904101848602295
Epoch: 44, Steps: 61 | Train Loss: 0.7590343 Vali Loss: 0.6297497 Test Loss: 0.4219840
EarlyStopping counter: 14 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.24963641166687
Epoch: 45, Steps: 61 | Train Loss: 0.7587251 Vali Loss: 0.6283308 Test Loss: 0.4219469
EarlyStopping counter: 15 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 2.214622735977173
Epoch: 46, Steps: 61 | Train Loss: 0.7592602 Vali Loss: 0.6306058 Test Loss: 0.4219129
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.4507250785827637
Epoch: 47, Steps: 61 | Train Loss: 0.7591551 Vali Loss: 0.6276787 Test Loss: 0.4218706
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 3.102879047393799
Epoch: 48, Steps: 61 | Train Loss: 0.7596484 Vali Loss: 0.6259731 Test Loss: 0.4218385
Validation loss decreased (0.627018 --> 0.625973).  Saving model ...
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 3.2263448238372803
Epoch: 49, Steps: 61 | Train Loss: 0.7597847 Vali Loss: 0.6253279 Test Loss: 0.4218079
Validation loss decreased (0.625973 --> 0.625328).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 3.4974074363708496
Epoch: 50, Steps: 61 | Train Loss: 0.7588741 Vali Loss: 0.6293451 Test Loss: 0.4217762
EarlyStopping counter: 1 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 3.7048935890197754
Epoch: 51, Steps: 61 | Train Loss: 0.7585941 Vali Loss: 0.6295387 Test Loss: 0.4217489
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 4.550351619720459
Epoch: 52, Steps: 61 | Train Loss: 0.7575798 Vali Loss: 0.6283789 Test Loss: 0.4217216
EarlyStopping counter: 3 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 3.9831430912017822
Epoch: 53, Steps: 61 | Train Loss: 0.7585755 Vali Loss: 0.6295860 Test Loss: 0.4216966
EarlyStopping counter: 4 out of 20
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.5366644859313965
Epoch: 54, Steps: 61 | Train Loss: 0.7577297 Vali Loss: 0.6261376 Test Loss: 0.4216735
EarlyStopping counter: 5 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.330310583114624
Epoch: 55, Steps: 61 | Train Loss: 0.7587567 Vali Loss: 0.6256838 Test Loss: 0.4216545
EarlyStopping counter: 6 out of 20
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.6375486850738525
Epoch: 56, Steps: 61 | Train Loss: 0.7597789 Vali Loss: 0.6288222 Test Loss: 0.4216302
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 2.9046201705932617
Epoch: 57, Steps: 61 | Train Loss: 0.7588942 Vali Loss: 0.6291053 Test Loss: 0.4216124
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.208341360092163
Epoch: 58, Steps: 61 | Train Loss: 0.7589683 Vali Loss: 0.6247887 Test Loss: 0.4215906
Validation loss decreased (0.625328 --> 0.624789).  Saving model ...
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 2.477800130844116
Epoch: 59, Steps: 61 | Train Loss: 0.7588823 Vali Loss: 0.6290035 Test Loss: 0.4215710
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.6115705966949463
Epoch: 60, Steps: 61 | Train Loss: 0.7584323 Vali Loss: 0.6255595 Test Loss: 0.4215569
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.9332878589630127
Epoch: 61, Steps: 61 | Train Loss: 0.7581624 Vali Loss: 0.6246304 Test Loss: 0.4215402
Validation loss decreased (0.624789 --> 0.624630).  Saving model ...
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 3.875302791595459
Epoch: 62, Steps: 61 | Train Loss: 0.7577991 Vali Loss: 0.6265658 Test Loss: 0.4215262
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 3.119380235671997
Epoch: 63, Steps: 61 | Train Loss: 0.7591869 Vali Loss: 0.6264230 Test Loss: 0.4215114
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.821672201156616
Epoch: 64, Steps: 61 | Train Loss: 0.7592719 Vali Loss: 0.6289914 Test Loss: 0.4214976
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 3.2362518310546875
Epoch: 65, Steps: 61 | Train Loss: 0.7573489 Vali Loss: 0.6291161 Test Loss: 0.4214850
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.1463623046875
Epoch: 66, Steps: 61 | Train Loss: 0.7581247 Vali Loss: 0.6274109 Test Loss: 0.4214750
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.76792049407959
Epoch: 67, Steps: 61 | Train Loss: 0.7584742 Vali Loss: 0.6294185 Test Loss: 0.4214637
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.7859530448913574
Epoch: 68, Steps: 61 | Train Loss: 0.7581417 Vali Loss: 0.6280357 Test Loss: 0.4214508
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 2.6255717277526855
Epoch: 69, Steps: 61 | Train Loss: 0.7593610 Vali Loss: 0.6314064 Test Loss: 0.4214410
EarlyStopping counter: 8 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 4.57178807258606
Epoch: 70, Steps: 61 | Train Loss: 0.7576196 Vali Loss: 0.6243066 Test Loss: 0.4214296
Validation loss decreased (0.624630 --> 0.624307).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 4.804284334182739
Epoch: 71, Steps: 61 | Train Loss: 0.7591729 Vali Loss: 0.6251608 Test Loss: 0.4214214
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 4.54686975479126
Epoch: 72, Steps: 61 | Train Loss: 0.7591159 Vali Loss: 0.6264555 Test Loss: 0.4214128
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 3.3808765411376953
Epoch: 73, Steps: 61 | Train Loss: 0.7579407 Vali Loss: 0.6278018 Test Loss: 0.4214023
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 3.739049196243286
Epoch: 74, Steps: 61 | Train Loss: 0.7564902 Vali Loss: 0.6303608 Test Loss: 0.4213938
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.036559581756592
Epoch: 75, Steps: 61 | Train Loss: 0.7582108 Vali Loss: 0.6264303 Test Loss: 0.4213879
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 3.265359878540039
Epoch: 76, Steps: 61 | Train Loss: 0.7583191 Vali Loss: 0.6299176 Test Loss: 0.4213801
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.8279595375061035
Epoch: 77, Steps: 61 | Train Loss: 0.7582586 Vali Loss: 0.6261454 Test Loss: 0.4213710
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 2.5241341590881348
Epoch: 78, Steps: 61 | Train Loss: 0.7588292 Vali Loss: 0.6249794 Test Loss: 0.4213643
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.8006091117858887
Epoch: 79, Steps: 61 | Train Loss: 0.7588906 Vali Loss: 0.6267248 Test Loss: 0.4213586
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 2.9757766723632812
Epoch: 80, Steps: 61 | Train Loss: 0.7578277 Vali Loss: 0.6268088 Test Loss: 0.4213532
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 3.366262197494507
Epoch: 81, Steps: 61 | Train Loss: 0.7581038 Vali Loss: 0.6248119 Test Loss: 0.4213469
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 2.732391119003296
Epoch: 82, Steps: 61 | Train Loss: 0.7591077 Vali Loss: 0.6284573 Test Loss: 0.4213414
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 4.1121742725372314
Epoch: 83, Steps: 61 | Train Loss: 0.7594021 Vali Loss: 0.6248754 Test Loss: 0.4213355
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.349717617034912
Epoch: 84, Steps: 61 | Train Loss: 0.7583539 Vali Loss: 0.6309116 Test Loss: 0.4213314
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 3.601468324661255
Epoch: 85, Steps: 61 | Train Loss: 0.7579033 Vali Loss: 0.6253620 Test Loss: 0.4213266
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 3.2782018184661865
Epoch: 86, Steps: 61 | Train Loss: 0.7582814 Vali Loss: 0.6278634 Test Loss: 0.4213218
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 4.066184043884277
Epoch: 87, Steps: 61 | Train Loss: 0.7588669 Vali Loss: 0.6307396 Test Loss: 0.4213172
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 3.4140539169311523
Epoch: 88, Steps: 61 | Train Loss: 0.7586646 Vali Loss: 0.6264044 Test Loss: 0.4213126
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 2.560295820236206
Epoch: 89, Steps: 61 | Train Loss: 0.7589372 Vali Loss: 0.6296152 Test Loss: 0.4213097
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 4.608071804046631
Epoch: 90, Steps: 61 | Train Loss: 0.7577353 Vali Loss: 0.6268694 Test Loss: 0.4213066
EarlyStopping counter: 20 out of 20
Early stopping
train 7831
val 2161
test 2161
Model(
  (freq_upsampler): Linear(in_features=34, out_features=306, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  9321984.0
params:  10710.0
Trainable parameters:  10710
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.296825885772705
Epoch: 1, Steps: 61 | Train Loss: 0.8521670 Vali Loss: 0.6283129 Test Loss: 0.4209293
Validation loss decreased (inf --> 0.628313).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.2081854343414307
Epoch: 2, Steps: 61 | Train Loss: 0.8515549 Vali Loss: 0.6264400 Test Loss: 0.4205798
Validation loss decreased (0.628313 --> 0.626440).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.911630630493164
Epoch: 3, Steps: 61 | Train Loss: 0.8491275 Vali Loss: 0.6253213 Test Loss: 0.4203758
Validation loss decreased (0.626440 --> 0.625321).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 3.0970194339752197
Epoch: 4, Steps: 61 | Train Loss: 0.8497156 Vali Loss: 0.6267033 Test Loss: 0.4202171
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 4.439910888671875
Epoch: 5, Steps: 61 | Train Loss: 0.8481895 Vali Loss: 0.6264272 Test Loss: 0.4200725
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 4.385457754135132
Epoch: 6, Steps: 61 | Train Loss: 0.8471354 Vali Loss: 0.6232505 Test Loss: 0.4199755
Validation loss decreased (0.625321 --> 0.623251).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 3.310652017593384
Epoch: 7, Steps: 61 | Train Loss: 0.8476536 Vali Loss: 0.6230664 Test Loss: 0.4199008
Validation loss decreased (0.623251 --> 0.623066).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 4.054309129714966
Epoch: 8, Steps: 61 | Train Loss: 0.8482046 Vali Loss: 0.6259204 Test Loss: 0.4197700
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 5.6784868240356445
Epoch: 9, Steps: 61 | Train Loss: 0.8475222 Vali Loss: 0.6216986 Test Loss: 0.4197443
Validation loss decreased (0.623066 --> 0.621699).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 3.886312484741211
Epoch: 10, Steps: 61 | Train Loss: 0.8468118 Vali Loss: 0.6239289 Test Loss: 0.4196552
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 3.3292601108551025
Epoch: 11, Steps: 61 | Train Loss: 0.8474221 Vali Loss: 0.6260526 Test Loss: 0.4196172
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.8474888801574707
Epoch: 12, Steps: 61 | Train Loss: 0.8470669 Vali Loss: 0.6229586 Test Loss: 0.4195478
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.8146297931671143
Epoch: 13, Steps: 61 | Train Loss: 0.8461705 Vali Loss: 0.6242050 Test Loss: 0.4195457
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.59063458442688
Epoch: 14, Steps: 61 | Train Loss: 0.8454041 Vali Loss: 0.6213189 Test Loss: 0.4194763
Validation loss decreased (0.621699 --> 0.621319).  Saving model ...
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.711491107940674
Epoch: 15, Steps: 61 | Train Loss: 0.8461571 Vali Loss: 0.6188184 Test Loss: 0.4194586
Validation loss decreased (0.621319 --> 0.618818).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 5.206587076187134
Epoch: 16, Steps: 61 | Train Loss: 0.8463601 Vali Loss: 0.6247241 Test Loss: 0.4194520
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 4.563925266265869
Epoch: 17, Steps: 61 | Train Loss: 0.8463242 Vali Loss: 0.6210508 Test Loss: 0.4193994
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 4.836035966873169
Epoch: 18, Steps: 61 | Train Loss: 0.8457814 Vali Loss: 0.6232188 Test Loss: 0.4193815
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 5.208196401596069
Epoch: 19, Steps: 61 | Train Loss: 0.8466960 Vali Loss: 0.6256096 Test Loss: 0.4193420
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 3.4023022651672363
Epoch: 20, Steps: 61 | Train Loss: 0.8455401 Vali Loss: 0.6228448 Test Loss: 0.4193532
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 4.058562278747559
Epoch: 21, Steps: 61 | Train Loss: 0.8454408 Vali Loss: 0.6239406 Test Loss: 0.4193281
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 4.104271173477173
Epoch: 22, Steps: 61 | Train Loss: 0.8464577 Vali Loss: 0.6274242 Test Loss: 0.4193108
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 3.974778413772583
Epoch: 23, Steps: 61 | Train Loss: 0.8463075 Vali Loss: 0.6213930 Test Loss: 0.4192897
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.5033161640167236
Epoch: 24, Steps: 61 | Train Loss: 0.8440740 Vali Loss: 0.6240309 Test Loss: 0.4192877
EarlyStopping counter: 9 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.474669933319092
Epoch: 25, Steps: 61 | Train Loss: 0.8447631 Vali Loss: 0.6237445 Test Loss: 0.4192559
EarlyStopping counter: 10 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 2.8535375595092773
Epoch: 26, Steps: 61 | Train Loss: 0.8459366 Vali Loss: 0.6216586 Test Loss: 0.4192418
EarlyStopping counter: 11 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 3.1905102729797363
Epoch: 27, Steps: 61 | Train Loss: 0.8454540 Vali Loss: 0.6238560 Test Loss: 0.4192495
EarlyStopping counter: 12 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 2.832111120223999
Epoch: 28, Steps: 61 | Train Loss: 0.8443975 Vali Loss: 0.6235927 Test Loss: 0.4192370
EarlyStopping counter: 13 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.8468387126922607
Epoch: 29, Steps: 61 | Train Loss: 0.8454739 Vali Loss: 0.6251898 Test Loss: 0.4192220
EarlyStopping counter: 14 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.410160541534424
Epoch: 30, Steps: 61 | Train Loss: 0.8448846 Vali Loss: 0.6237758 Test Loss: 0.4192145
EarlyStopping counter: 15 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.753170967102051
Epoch: 31, Steps: 61 | Train Loss: 0.8433064 Vali Loss: 0.6227055 Test Loss: 0.4192045
EarlyStopping counter: 16 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 4.133939266204834
Epoch: 32, Steps: 61 | Train Loss: 0.8448839 Vali Loss: 0.6249053 Test Loss: 0.4191989
EarlyStopping counter: 17 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 4.13457179069519
Epoch: 33, Steps: 61 | Train Loss: 0.8446694 Vali Loss: 0.6268539 Test Loss: 0.4191982
EarlyStopping counter: 18 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.8344271183013916
Epoch: 34, Steps: 61 | Train Loss: 0.8443601 Vali Loss: 0.6250244 Test Loss: 0.4191860
EarlyStopping counter: 19 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 4.592702865600586
Epoch: 35, Steps: 61 | Train Loss: 0.8423160 Vali Loss: 0.6206641 Test Loss: 0.4191762
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_720_FITS_ETTh2_ftM_sl90_ll48_pl720_H6_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4178539216518402, mae:0.4362192749977112, rse:0.516675591468811, corr:[ 0.22112711  0.22068673  0.22025616  0.21943966  0.2176701   0.21707256
  0.21558279  0.21512851  0.21411951  0.21291225  0.21237858  0.2108513
  0.20967433  0.20864818  0.20781027  0.20736806  0.20652655  0.20636773
  0.20559414  0.20435373  0.20375143  0.20276077  0.20203981  0.20042123
  0.19750091  0.19581668  0.194238    0.19356096  0.19281131  0.19154164
  0.19143617  0.19081654  0.18978994  0.18897775  0.18825221  0.18754612
  0.18661037  0.1856358   0.18506658  0.18437876  0.18372716  0.1834591
  0.18322225  0.18290319  0.18243562  0.18157901  0.1813551   0.17950808
  0.17613819  0.17414819  0.17193085  0.17056808  0.16961226  0.16900314
  0.16894342  0.16774753  0.16744696  0.1667868   0.16639048  0.16606341
  0.16548145  0.16516347  0.16501176  0.16528948  0.1654635   0.16572307
  0.16579038  0.16505794  0.16501667  0.1650612   0.16452591  0.16380912
  0.16202235  0.16105945  0.16017525  0.15974197  0.15951666  0.15973458
  0.16027719  0.15956455  0.15930018  0.15905023  0.15883201  0.1586077
  0.15836477  0.15831563  0.15820907  0.15826803  0.15830109  0.15837665
  0.15836295  0.15785317  0.15774675  0.15748693  0.15717101  0.1567646
  0.1553128   0.1541308   0.15269366  0.15195243  0.15157941  0.15082625
  0.15081477  0.15063106  0.15067187  0.1504337   0.15087397  0.15059483
  0.15027685  0.1505281   0.14958636  0.14924069  0.14947842  0.14906037
  0.14890876  0.14864065  0.14831209  0.14788614  0.14720108  0.14590454
  0.14362037  0.14220646  0.14091508  0.14021082  0.1394352   0.13875307
  0.13856453  0.13829933  0.13820414  0.13787262  0.13780671  0.13703185
  0.13630584  0.13576907  0.13483496  0.13438125  0.13394015  0.13344587
  0.13325767  0.13266754  0.13250436  0.13214302  0.13110964  0.12993944
  0.12733255  0.12506777  0.12323815  0.12234985  0.12184668  0.1212516
  0.12086964  0.12042183  0.12078912  0.12043108  0.12039965  0.12030491
  0.11956766  0.11929387  0.11898819  0.11869748  0.11863038  0.11859482
  0.11825256  0.11768522  0.11805411  0.11798684  0.11728217  0.11587594
  0.11336317  0.11174683  0.11057944  0.1104138   0.11017568  0.11000623
  0.11072896  0.11090619  0.11094591  0.11060023  0.11064097  0.10999228
  0.10951372  0.10922712  0.10891714  0.10931014  0.10914491  0.10892341
  0.10937212  0.10972105  0.11003905  0.1103298   0.11008962  0.10954703
  0.10841027  0.10835131  0.10810004  0.10832918  0.10899063  0.10938295
  0.11069887  0.11194824  0.11278788  0.11278801  0.11332908  0.11332987
  0.1127627   0.11231031  0.11223869  0.1126812   0.11256342  0.11281706
  0.11343556  0.11316679  0.11307725  0.11325221  0.11317868  0.11216257
  0.110504    0.10934053  0.10843689  0.1086966   0.10820553  0.10878769
  0.11114325  0.11200804  0.11226404  0.11308197  0.11327012  0.11269697
  0.11273396  0.11255404  0.11275377  0.11374955  0.11390374  0.11372187
  0.11423764  0.11481697  0.11522308  0.1154278   0.11534583  0.11508904
  0.11419663  0.11396836  0.11331259  0.11299461  0.11342397  0.11429887
  0.11567719  0.1161994   0.11712369  0.11759799  0.11784144  0.11808832
  0.11845628  0.11865836  0.11849577  0.11900647  0.11924158  0.11963847
  0.11972602  0.11976925  0.12009337  0.12047581  0.12085349  0.12079895
  0.11961786  0.11929648  0.11953722  0.11998115  0.12022097  0.12158903
  0.12365036  0.1243905   0.1248262   0.12503222  0.125317    0.1249569
  0.12487119  0.12483551  0.12506881  0.12537014  0.12560391  0.12580743
  0.12599556  0.1258225   0.12593581  0.12633194  0.12654053  0.12629403
  0.12575205  0.12544832  0.1248266   0.12467065  0.12498179  0.12532698
  0.12572254  0.12589994  0.12616448  0.12651545  0.12698373  0.12690741
  0.1265827   0.12634116  0.12644023  0.12708892  0.12698948  0.1271359
  0.12781571  0.12755448  0.12751631  0.12770414  0.12768926  0.127239
  0.12585172  0.1252431   0.12483377  0.12502442  0.1252725   0.12545843
  0.12647308  0.12735175  0.12778391  0.1278401   0.12836945  0.1283423
  0.12833369  0.12841119  0.12839414  0.12869817  0.12903981  0.12947677
  0.13021329  0.13047801  0.13051409  0.13083565  0.13118114  0.13138399
  0.13057648  0.13020682  0.12976469  0.13044728  0.1311417   0.13146673
  0.13373977  0.13442384  0.13399124  0.13487773  0.13598694  0.13596578
  0.1359317   0.13594681  0.1360405   0.13635482  0.1367455   0.13695195
  0.13720138  0.13792841  0.13854226  0.13886319  0.13947074  0.13948822
  0.13873112  0.13915494  0.13918187  0.13920198  0.14012052  0.14130571
  0.14366807  0.14569734  0.14700149  0.14828369  0.14866737  0.14881106
  0.1496804   0.15007329  0.15071845  0.15150665  0.15183032  0.15317975
  0.15383305  0.15369283  0.15466273  0.15564199  0.15574996  0.155466
  0.15547544  0.15641324  0.15673372  0.15709037  0.15815824  0.15952614
  0.16243356  0.1643084   0.16466539  0.16509295  0.16620494  0.16679677
  0.16670515  0.16647114  0.16662212  0.16701327  0.1671131   0.16697931
  0.16690475  0.1672877   0.16783006  0.16779405  0.1678557   0.1676263
  0.1668463   0.16707873  0.16676317  0.16722074  0.1678491   0.16788363
  0.16963087  0.170649    0.17048386  0.1709356   0.17128679  0.17115971
  0.1709915   0.17074601  0.17075858  0.1707317   0.17032026  0.17020811
  0.17018285  0.17012653  0.17045785  0.17063949  0.17079212  0.17062898
  0.16991359  0.16961956  0.1694445   0.1693922   0.16939783  0.16951662
  0.17054905  0.17078158  0.17064074  0.17077045  0.17088276  0.17064543
  0.17043623  0.17022143  0.1700007   0.1701926   0.17011167  0.17002223
  0.17002116  0.16994585  0.17008144  0.17009082  0.16999394  0.16978693
  0.16927366  0.1689744   0.1683621   0.16807514  0.16801202  0.1680097
  0.1684091   0.16860767  0.16848502  0.1682172   0.16813444  0.16778724
  0.16749609  0.16723226  0.1666041   0.16653948  0.16662857  0.16623113
  0.16588937  0.16565277  0.1652453   0.16521665  0.1651779   0.16418569
  0.16253677  0.16162007  0.16074033  0.15965989  0.15916917  0.15875599
  0.15820305  0.15779203  0.15730454  0.15630746  0.15602498  0.15550189
  0.15473704  0.15446888  0.15367189  0.15328106  0.15325303  0.15262495
  0.1523926   0.15251012  0.15218459  0.15163353  0.15130219  0.15017001
  0.1479151   0.14611678  0.14477356  0.1432863   0.14220454  0.14192177
  0.14156014  0.14132299  0.14092219  0.14058238  0.14076482  0.13992843
  0.13863881  0.13808092  0.1378442   0.13785392  0.13770582  0.13703664
  0.13654847  0.13603722  0.13533403  0.13503931  0.13450933  0.13315216
  0.13031115  0.12755488  0.12570094  0.12354238  0.12170618  0.12077126
  0.11956839  0.11826172  0.11836308  0.11799379  0.11768341  0.11727842
  0.115998    0.11506978  0.11441188  0.1139768   0.11322705  0.11234542
  0.11177442  0.11129418  0.11035332  0.10955117  0.10886113  0.10654083
  0.10312393  0.10112344  0.09867026  0.09666104  0.09573718  0.09519994
  0.09443346  0.09341108  0.09348519  0.09334994  0.09288019  0.09213541
  0.09142934  0.09095167  0.09003286  0.08877601  0.08867621  0.08896846
  0.08807081  0.0868673   0.08635443  0.08517517  0.08350726  0.08159363
  0.07862374  0.07629267  0.07489215  0.07317284  0.07164437  0.07109108
  0.07078145  0.07020483  0.06989777  0.06936745  0.06911287  0.06860662
  0.06728896  0.06665272  0.06657215  0.06573742  0.06536993  0.06530528
  0.06462096  0.06499319  0.06537299  0.06418283  0.06299911  0.06112559
  0.05831151  0.05607489  0.05420879  0.05324076  0.05181343  0.05064373
  0.05051246  0.05000562  0.04908923  0.04865088  0.04903296  0.04875569
  0.0480157   0.04746895  0.04705773  0.04651734  0.04642844  0.04688586
  0.04650787  0.04659571  0.0461417   0.04423961  0.0440067   0.04335839
  0.03998041  0.03805021  0.03637687  0.03472342  0.03304048  0.03162526
  0.03142885  0.03141569  0.03122755  0.03041265  0.03010871  0.03024671
  0.02955232  0.02828418  0.02726531  0.02660227  0.02658047  0.02617743
  0.02497421  0.02472426  0.0239598   0.02257103  0.02311405  0.02166045
  0.01816322  0.01630241  0.0140841   0.01191403  0.01076859  0.0105491
  0.01016986  0.00890214  0.00833648  0.00804886  0.00803635  0.00802979
  0.00705104  0.00651564  0.00648973  0.00521707  0.00516515  0.00599592
  0.00501675  0.00440487  0.00466659  0.005348    0.00588849  0.00362562
  0.00097333 -0.00107027 -0.004094   -0.00562264 -0.00756037 -0.00888859
 -0.00812147 -0.0089023  -0.00883873 -0.00832511 -0.00865867 -0.00847822
 -0.00893385 -0.01016769 -0.01041108 -0.01045959 -0.00998674 -0.01003321
 -0.01052228 -0.01015379 -0.01065575 -0.01057493 -0.01035027 -0.01280391
 -0.01540395 -0.01798744 -0.02133192 -0.02186638 -0.02293971 -0.0232292
 -0.02198868 -0.02311244 -0.02251477 -0.0211224  -0.02144893 -0.02251264
 -0.02284982 -0.02326406 -0.02361262 -0.02389233 -0.02390362 -0.02500478
 -0.02452308 -0.0235644  -0.02534692 -0.02360149 -0.02481781 -0.02259092]
