Args in experiment:
Namespace(H_order=4, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=26, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_90_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=90, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H4_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 8215
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=26, out_features=123, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  2865408.0
params:  3321.0
Trainable parameters:  3321
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.749830722808838
Epoch: 1, Steps: 64 | Train Loss: 0.9352333 Vali Loss: 0.4863574 Test Loss: 0.5712749
Validation loss decreased (inf --> 0.486357).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 1.8540852069854736
Epoch: 2, Steps: 64 | Train Loss: 0.8110779 Vali Loss: 0.4381549 Test Loss: 0.5107213
Validation loss decreased (0.486357 --> 0.438155).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 1.8352546691894531
Epoch: 3, Steps: 64 | Train Loss: 0.7513331 Vali Loss: 0.4131086 Test Loss: 0.4784479
Validation loss decreased (0.438155 --> 0.413109).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 1.3996057510375977
Epoch: 4, Steps: 64 | Train Loss: 0.7168580 Vali Loss: 0.4004433 Test Loss: 0.4597820
Validation loss decreased (0.413109 --> 0.400443).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 1.5897550582885742
Epoch: 5, Steps: 64 | Train Loss: 0.6970273 Vali Loss: 0.3924547 Test Loss: 0.4485651
Validation loss decreased (0.400443 --> 0.392455).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 1.7239933013916016
Epoch: 6, Steps: 64 | Train Loss: 0.6856083 Vali Loss: 0.3826346 Test Loss: 0.4415843
Validation loss decreased (0.392455 --> 0.382635).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 1.5496752262115479
Epoch: 7, Steps: 64 | Train Loss: 0.6775268 Vali Loss: 0.3802916 Test Loss: 0.4371135
Validation loss decreased (0.382635 --> 0.380292).  Saving model ...
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 2.1785800457000732
Epoch: 8, Steps: 64 | Train Loss: 0.6710417 Vali Loss: 0.3793340 Test Loss: 0.4341489
Validation loss decreased (0.380292 --> 0.379334).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 1.740739345550537
Epoch: 9, Steps: 64 | Train Loss: 0.6688037 Vali Loss: 0.3766946 Test Loss: 0.4321695
Validation loss decreased (0.379334 --> 0.376695).  Saving model ...
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 2.1106462478637695
Epoch: 10, Steps: 64 | Train Loss: 0.6656603 Vali Loss: 0.3745593 Test Loss: 0.4306573
Validation loss decreased (0.376695 --> 0.374559).  Saving model ...
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.0541999340057373
Epoch: 11, Steps: 64 | Train Loss: 0.6647386 Vali Loss: 0.3760033 Test Loss: 0.4295500
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.469198226928711
Epoch: 12, Steps: 64 | Train Loss: 0.6627335 Vali Loss: 0.3723189 Test Loss: 0.4287325
Validation loss decreased (0.374559 --> 0.372319).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.5291078090667725
Epoch: 13, Steps: 64 | Train Loss: 0.6614494 Vali Loss: 0.3681037 Test Loss: 0.4280517
Validation loss decreased (0.372319 --> 0.368104).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 1.6467576026916504
Epoch: 14, Steps: 64 | Train Loss: 0.6603839 Vali Loss: 0.3723996 Test Loss: 0.4274973
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 1.67362380027771
Epoch: 15, Steps: 64 | Train Loss: 0.6603250 Vali Loss: 0.3703766 Test Loss: 0.4270628
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 1.8488099575042725
Epoch: 16, Steps: 64 | Train Loss: 0.6580806 Vali Loss: 0.3712101 Test Loss: 0.4266404
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 1.729520320892334
Epoch: 17, Steps: 64 | Train Loss: 0.6587522 Vali Loss: 0.3689457 Test Loss: 0.4263186
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 1.5834619998931885
Epoch: 18, Steps: 64 | Train Loss: 0.6577383 Vali Loss: 0.3688827 Test Loss: 0.4260634
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 1.613692045211792
Epoch: 19, Steps: 64 | Train Loss: 0.6567045 Vali Loss: 0.3695554 Test Loss: 0.4257795
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.2897884845733643
Epoch: 20, Steps: 64 | Train Loss: 0.6552244 Vali Loss: 0.3688481 Test Loss: 0.4255409
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 1.619680643081665
Epoch: 21, Steps: 64 | Train Loss: 0.6559376 Vali Loss: 0.3678011 Test Loss: 0.4253723
Validation loss decreased (0.368104 --> 0.367801).  Saving model ...
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.349254608154297
Epoch: 22, Steps: 64 | Train Loss: 0.6553066 Vali Loss: 0.3676336 Test Loss: 0.4251294
Validation loss decreased (0.367801 --> 0.367634).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 1.6374008655548096
Epoch: 23, Steps: 64 | Train Loss: 0.6543977 Vali Loss: 0.3682710 Test Loss: 0.4249982
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 1.6387019157409668
Epoch: 24, Steps: 64 | Train Loss: 0.6547644 Vali Loss: 0.3676584 Test Loss: 0.4248614
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.599830150604248
Epoch: 25, Steps: 64 | Train Loss: 0.6528695 Vali Loss: 0.3678035 Test Loss: 0.4246939
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 1.7293429374694824
Epoch: 26, Steps: 64 | Train Loss: 0.6539812 Vali Loss: 0.3652493 Test Loss: 0.4245656
Validation loss decreased (0.367634 --> 0.365249).  Saving model ...
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 2.0412168502807617
Epoch: 27, Steps: 64 | Train Loss: 0.6534977 Vali Loss: 0.3676851 Test Loss: 0.4244554
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 1.8729383945465088
Epoch: 28, Steps: 64 | Train Loss: 0.6540840 Vali Loss: 0.3678840 Test Loss: 0.4243181
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 1.6225717067718506
Epoch: 29, Steps: 64 | Train Loss: 0.6535205 Vali Loss: 0.3671139 Test Loss: 0.4242012
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 1.5834434032440186
Epoch: 30, Steps: 64 | Train Loss: 0.6530437 Vali Loss: 0.3679686 Test Loss: 0.4241459
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 1.8227264881134033
Epoch: 31, Steps: 64 | Train Loss: 0.6535681 Vali Loss: 0.3678836 Test Loss: 0.4240445
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 1.634087324142456
Epoch: 32, Steps: 64 | Train Loss: 0.6527832 Vali Loss: 0.3638933 Test Loss: 0.4239805
Validation loss decreased (0.365249 --> 0.363893).  Saving model ...
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 1.8652698993682861
Epoch: 33, Steps: 64 | Train Loss: 0.6521693 Vali Loss: 0.3661574 Test Loss: 0.4239046
EarlyStopping counter: 1 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 1.5740141868591309
Epoch: 34, Steps: 64 | Train Loss: 0.6529271 Vali Loss: 0.3652805 Test Loss: 0.4238329
EarlyStopping counter: 2 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 1.9682910442352295
Epoch: 35, Steps: 64 | Train Loss: 0.6532062 Vali Loss: 0.3664821 Test Loss: 0.4237594
EarlyStopping counter: 3 out of 20
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 1.7569448947906494
Epoch: 36, Steps: 64 | Train Loss: 0.6525733 Vali Loss: 0.3675731 Test Loss: 0.4237197
EarlyStopping counter: 4 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 1.7047054767608643
Epoch: 37, Steps: 64 | Train Loss: 0.6517217 Vali Loss: 0.3678293 Test Loss: 0.4236540
EarlyStopping counter: 5 out of 20
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 1.496870517730713
Epoch: 38, Steps: 64 | Train Loss: 0.6524979 Vali Loss: 0.3664333 Test Loss: 0.4235826
EarlyStopping counter: 6 out of 20
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 2.06860089302063
Epoch: 39, Steps: 64 | Train Loss: 0.6516773 Vali Loss: 0.3675539 Test Loss: 0.4235284
EarlyStopping counter: 7 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 1.697946548461914
Epoch: 40, Steps: 64 | Train Loss: 0.6515547 Vali Loss: 0.3672163 Test Loss: 0.4235074
EarlyStopping counter: 8 out of 20
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 1.6492209434509277
Epoch: 41, Steps: 64 | Train Loss: 0.6512118 Vali Loss: 0.3670126 Test Loss: 0.4234601
EarlyStopping counter: 9 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 2.2925801277160645
Epoch: 42, Steps: 64 | Train Loss: 0.6520273 Vali Loss: 0.3661447 Test Loss: 0.4234204
EarlyStopping counter: 10 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 2.139437437057495
Epoch: 43, Steps: 64 | Train Loss: 0.6520521 Vali Loss: 0.3641286 Test Loss: 0.4233942
EarlyStopping counter: 11 out of 20
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 1.6457343101501465
Epoch: 44, Steps: 64 | Train Loss: 0.6518359 Vali Loss: 0.3687442 Test Loss: 0.4233403
EarlyStopping counter: 12 out of 20
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 1.7244391441345215
Epoch: 45, Steps: 64 | Train Loss: 0.6513034 Vali Loss: 0.3673776 Test Loss: 0.4233150
EarlyStopping counter: 13 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 1.8560969829559326
Epoch: 46, Steps: 64 | Train Loss: 0.6500213 Vali Loss: 0.3663123 Test Loss: 0.4232861
EarlyStopping counter: 14 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.112133741378784
Epoch: 47, Steps: 64 | Train Loss: 0.6512220 Vali Loss: 0.3665280 Test Loss: 0.4232612
EarlyStopping counter: 15 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 1.5945532321929932
Epoch: 48, Steps: 64 | Train Loss: 0.6521032 Vali Loss: 0.3662189 Test Loss: 0.4232314
EarlyStopping counter: 16 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 1.6684465408325195
Epoch: 49, Steps: 64 | Train Loss: 0.6502617 Vali Loss: 0.3667258 Test Loss: 0.4232112
EarlyStopping counter: 17 out of 20
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 1.5396761894226074
Epoch: 50, Steps: 64 | Train Loss: 0.6507650 Vali Loss: 0.3668633 Test Loss: 0.4231846
EarlyStopping counter: 18 out of 20
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 1.4745640754699707
Epoch: 51, Steps: 64 | Train Loss: 0.6513189 Vali Loss: 0.3652514 Test Loss: 0.4231543
EarlyStopping counter: 19 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 1.6142809391021729
Epoch: 52, Steps: 64 | Train Loss: 0.6509658 Vali Loss: 0.3666740 Test Loss: 0.4231285
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_90_336_FITS_ETTh2_ftM_sl90_ll48_pl336_H4_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.41923731565475464, mae:0.42654016613960266, rse:0.5176897048950195, corr:[0.26271495 0.26572463 0.2625043  0.26162842 0.26091105 0.25890017
 0.25758564 0.2569451  0.25561473 0.254239   0.25363827 0.25253427
 0.2509059  0.24968205 0.24886638 0.24781018 0.24693081 0.24627347
 0.24505179 0.24346851 0.24252446 0.24169005 0.23984405 0.23739946
 0.23496108 0.2328186  0.2306305  0.22940217 0.22836763 0.22698228
 0.22619282 0.22569112 0.22483386 0.2236658  0.22288854 0.22210646
 0.22111447 0.22021936 0.21929622 0.21849425 0.21803068 0.2174323
 0.21635786 0.21527769 0.21453506 0.21331897 0.21139124 0.20905653
 0.20603536 0.20296098 0.20009749 0.19829208 0.19681914 0.19498317
 0.19353427 0.19177888 0.19080047 0.1900664  0.18946008 0.18840848
 0.18750566 0.18743086 0.18713792 0.18665628 0.1864171  0.18606037
 0.18511526 0.18412158 0.18356515 0.18283755 0.18139921 0.17953548
 0.17689133 0.1748492  0.17326713 0.17226087 0.17139754 0.17096083
 0.17093015 0.17012562 0.1696236  0.16958082 0.1696548  0.16914758
 0.1684662  0.16828462 0.1682223  0.16805942 0.16789055 0.16755925
 0.16691548 0.16616094 0.16592196 0.16545352 0.16445994 0.16311558
 0.160914   0.15864144 0.15660949 0.15542139 0.15431558 0.15321784
 0.1534887  0.15343703 0.15338247 0.15315643 0.15331888 0.15309568
 0.15246905 0.15226385 0.15182477 0.15120986 0.15086865 0.1507208
 0.15006746 0.14887843 0.14827673 0.14737192 0.14537565 0.1429409
 0.14031124 0.13801613 0.13569508 0.13439123 0.13339509 0.13253674
 0.13222787 0.13186015 0.1315377  0.13116029 0.13114798 0.13087441
 0.13035156 0.13012566 0.12984698 0.12906075 0.12836853 0.12818348
 0.12760298 0.12628572 0.1256691  0.12530938 0.12363108 0.12065592
 0.11718699 0.11474516 0.11238101 0.11055961 0.10938064 0.10876033
 0.10865127 0.10808469 0.10799559 0.10811778 0.10845259 0.10832009
 0.10777084 0.10775175 0.10800447 0.10785624 0.10752203 0.10768025
 0.1077793  0.10682319 0.10613374 0.1060994  0.10544218 0.10322285
 0.10008475 0.09808913 0.09635249 0.09489881 0.09371244 0.09320815
 0.09359448 0.09352501 0.09381088 0.09413314 0.09440024 0.09432776
 0.09439092 0.09477469 0.09486542 0.09485064 0.09481768 0.09503242
 0.09523352 0.09489509 0.09458688 0.09472261 0.09449478 0.09326906
 0.09131705 0.09036821 0.08950479 0.08875058 0.08848611 0.08867333
 0.08972377 0.09039737 0.09123922 0.09204035 0.09276284 0.09290334
 0.092874   0.09302688 0.09316956 0.09326956 0.09312535 0.09312964
 0.09321838 0.09304902 0.09258804 0.09228858 0.092107   0.09075945
 0.08814415 0.08616168 0.08462355 0.08346868 0.08235599 0.08252496
 0.08395945 0.08494806 0.08580744 0.08673441 0.08745708 0.08723146
 0.08721639 0.08773124 0.08787625 0.08774015 0.08793617 0.08848234
 0.08898482 0.08930977 0.08926252 0.08881155 0.08808777 0.08716588
 0.08490121 0.08268364 0.08077767 0.08027277 0.08025776 0.08063384
 0.082512   0.08378224 0.08474324 0.08584122 0.08735289 0.08802629
 0.08801651 0.08882523 0.08966194 0.09046033 0.09079165 0.09126813
 0.09152089 0.09169172 0.0916186  0.09138346 0.091027   0.09041663
 0.0890269  0.08796933 0.08740655 0.08766237 0.08764242 0.08779366
 0.08952822 0.0911392  0.09207151 0.09231271 0.09294566 0.09317613
 0.09321319 0.09345313 0.09401849 0.09433196 0.0941107  0.09374725
 0.09383096 0.0940109  0.09399589 0.09342674 0.09280026 0.09252601
 0.09156733 0.09030531 0.08898133 0.08865991 0.0886102  0.08796746
 0.0885406  0.08979514 0.09086408 0.09119862 0.0922038  0.09321889
 0.09332667 0.09363604 0.09441953 0.09512889 0.0950388  0.09499519
 0.09529637 0.09560752 0.0961123  0.09557473 0.09470769 0.09442748
 0.09331275 0.09192969 0.09083655 0.09173588 0.09222025 0.09138089
 0.09231565 0.09457051 0.09608161 0.09627317 0.09772084 0.09924462
 0.09928889 0.09914164 0.10012923 0.10080782 0.09977724 0.09817889
 0.09786677 0.09858207 0.09816157 0.09520984 0.09622889 0.09928016]
