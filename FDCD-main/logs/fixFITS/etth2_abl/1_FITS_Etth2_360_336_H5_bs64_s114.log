Args in experiment:
Namespace(H_order=5, ab=2, activation='gelu', aug_data_size=1, aug_method='NA', aug_rate=0.5, base_T=24, batch_size=128, c_out=7, checkpoints='./checkpoints/', cut_freq=90, d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', data_size=1, dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, groups=1, hidden_size=1, in_batch_augmentation=False, in_dataset_augmentation=False, individual=False, is_training=1, itr=1, kernel=5, label_len=48, learning_rate=0.0005, levels=3, loss='mse', lradj='type3', model='FITS', model_id='ETTh2_360_336', moving_avg=25, n_heads=8, num_workers=4, output_attention=False, patience=20, pred_len=336, root_path='./dataset/', seed=114, seq_len=360, stacks=1, target='OT', test_flop=False, test_time_train=False, testset_div=2, train_epochs=100, train_mode=1, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H5_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7945
val 2545
test 2545
Model(
  (freq_upsampler): Linear(in_features=90, out_features=174, bias=True)
)
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
FLOPs:  14031360.0
params:  15834.0
Trainable parameters:  15834
!!!!!!!!!!!!!!learning rate!!!!!!!!!!!!!!!
0.0005
Epoch: 1 cost time: 2.3646650314331055
Epoch: 1, Steps: 62 | Train Loss: 0.8189008 Vali Loss: 0.4818601 Test Loss: 0.4105998
Validation loss decreased (inf --> 0.481860).  Saving model ...
Updating learning rate to 0.0005
Epoch: 2 cost time: 2.712857484817505
Epoch: 2, Steps: 62 | Train Loss: 0.6953149 Vali Loss: 0.4390099 Test Loss: 0.3842955
Validation loss decreased (0.481860 --> 0.439010).  Saving model ...
Updating learning rate to 0.000475
Epoch: 3 cost time: 2.4488978385925293
Epoch: 3, Steps: 62 | Train Loss: 0.6620667 Vali Loss: 0.4193843 Test Loss: 0.3776037
Validation loss decreased (0.439010 --> 0.419384).  Saving model ...
Updating learning rate to 0.00045125
Epoch: 4 cost time: 2.5421972274780273
Epoch: 4, Steps: 62 | Train Loss: 0.6467564 Vali Loss: 0.4098298 Test Loss: 0.3746744
Validation loss decreased (0.419384 --> 0.409830).  Saving model ...
Updating learning rate to 0.0004286875
Epoch: 5 cost time: 5.855839967727661
Epoch: 5, Steps: 62 | Train Loss: 0.6385043 Vali Loss: 0.4004338 Test Loss: 0.3727787
Validation loss decreased (0.409830 --> 0.400434).  Saving model ...
Updating learning rate to 0.00040725312499999993
Epoch: 6 cost time: 5.694612979888916
Epoch: 6, Steps: 62 | Train Loss: 0.6323852 Vali Loss: 0.3955374 Test Loss: 0.3711837
Validation loss decreased (0.400434 --> 0.395537).  Saving model ...
Updating learning rate to 0.0003868904687499999
Epoch: 7 cost time: 4.271807670593262
Epoch: 7, Steps: 62 | Train Loss: 0.6285895 Vali Loss: 0.3956974 Test Loss: 0.3698946
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00036754594531249993
Epoch: 8 cost time: 3.0918080806732178
Epoch: 8, Steps: 62 | Train Loss: 0.6253759 Vali Loss: 0.3863736 Test Loss: 0.3686103
Validation loss decreased (0.395537 --> 0.386374).  Saving model ...
Updating learning rate to 0.00034916864804687486
Epoch: 9 cost time: 2.659245729446411
Epoch: 9, Steps: 62 | Train Loss: 0.6228452 Vali Loss: 0.3892772 Test Loss: 0.3677239
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00033171021564453113
Epoch: 10 cost time: 1.988687515258789
Epoch: 10, Steps: 62 | Train Loss: 0.6207642 Vali Loss: 0.3877292 Test Loss: 0.3671308
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00031512470486230455
Epoch: 11 cost time: 2.396977186203003
Epoch: 11, Steps: 62 | Train Loss: 0.6186885 Vali Loss: 0.3843633 Test Loss: 0.3665701
Validation loss decreased (0.386374 --> 0.384363).  Saving model ...
Updating learning rate to 0.00029936846961918935
Epoch: 12 cost time: 2.547006368637085
Epoch: 12, Steps: 62 | Train Loss: 0.6176537 Vali Loss: 0.3833489 Test Loss: 0.3660305
Validation loss decreased (0.384363 --> 0.383349).  Saving model ...
Updating learning rate to 0.0002844000461382298
Epoch: 13 cost time: 2.243279457092285
Epoch: 13, Steps: 62 | Train Loss: 0.6163223 Vali Loss: 0.3820134 Test Loss: 0.3655656
Validation loss decreased (0.383349 --> 0.382013).  Saving model ...
Updating learning rate to 0.0002701800438313183
Epoch: 14 cost time: 2.262969970703125
Epoch: 14, Steps: 62 | Train Loss: 0.6158227 Vali Loss: 0.3821681 Test Loss: 0.3653215
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0002566710416397524
Epoch: 15 cost time: 2.6680023670196533
Epoch: 15, Steps: 62 | Train Loss: 0.6152753 Vali Loss: 0.3810926 Test Loss: 0.3649229
Validation loss decreased (0.382013 --> 0.381093).  Saving model ...
Updating learning rate to 0.00024383748955776477
Epoch: 16 cost time: 2.6433887481689453
Epoch: 16, Steps: 62 | Train Loss: 0.6144893 Vali Loss: 0.3799509 Test Loss: 0.3648004
Validation loss decreased (0.381093 --> 0.379951).  Saving model ...
Updating learning rate to 0.0002316456150798765
Epoch: 17 cost time: 3.5561463832855225
Epoch: 17, Steps: 62 | Train Loss: 0.6143249 Vali Loss: 0.3814678 Test Loss: 0.3645472
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00022006333432588268
Epoch: 18 cost time: 2.9810192584991455
Epoch: 18, Steps: 62 | Train Loss: 0.6136490 Vali Loss: 0.3802451 Test Loss: 0.3643813
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00020906016760958854
Epoch: 19 cost time: 2.87288498878479
Epoch: 19, Steps: 62 | Train Loss: 0.6124126 Vali Loss: 0.3809583 Test Loss: 0.3642394
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.0001986071592291091
Epoch: 20 cost time: 2.556483268737793
Epoch: 20, Steps: 62 | Train Loss: 0.6127523 Vali Loss: 0.3799114 Test Loss: 0.3641442
Validation loss decreased (0.379951 --> 0.379911).  Saving model ...
Updating learning rate to 0.00018867680126765363
Epoch: 21 cost time: 3.1870927810668945
Epoch: 21, Steps: 62 | Train Loss: 0.6123638 Vali Loss: 0.3814221 Test Loss: 0.3641031
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.00017924296120427094
Epoch: 22 cost time: 2.547476291656494
Epoch: 22, Steps: 62 | Train Loss: 0.6123015 Vali Loss: 0.3780912 Test Loss: 0.3639596
Validation loss decreased (0.379911 --> 0.378091).  Saving model ...
Updating learning rate to 0.0001702808131440574
Epoch: 23 cost time: 2.135871648788452
Epoch: 23, Steps: 62 | Train Loss: 0.6110981 Vali Loss: 0.3790961 Test Loss: 0.3639008
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001617667724868545
Epoch: 24 cost time: 2.74582839012146
Epoch: 24, Steps: 62 | Train Loss: 0.6115670 Vali Loss: 0.3772745 Test Loss: 0.3637788
Validation loss decreased (0.378091 --> 0.377275).  Saving model ...
Updating learning rate to 0.00015367843386251178
Epoch: 25 cost time: 2.217284917831421
Epoch: 25, Steps: 62 | Train Loss: 0.6113492 Vali Loss: 0.3796428 Test Loss: 0.3637092
EarlyStopping counter: 1 out of 20
Updating learning rate to 0.0001459945121693862
Epoch: 26 cost time: 4.346829414367676
Epoch: 26, Steps: 62 | Train Loss: 0.6110816 Vali Loss: 0.3789591 Test Loss: 0.3637007
EarlyStopping counter: 2 out of 20
Updating learning rate to 0.00013869478656091687
Epoch: 27 cost time: 4.703813552856445
Epoch: 27, Steps: 62 | Train Loss: 0.6098217 Vali Loss: 0.3799138 Test Loss: 0.3635698
EarlyStopping counter: 3 out of 20
Updating learning rate to 0.00013176004723287101
Epoch: 28 cost time: 3.5221126079559326
Epoch: 28, Steps: 62 | Train Loss: 0.6103686 Vali Loss: 0.3782308 Test Loss: 0.3635778
EarlyStopping counter: 4 out of 20
Updating learning rate to 0.00012517204487122748
Epoch: 29 cost time: 3.161789655685425
Epoch: 29, Steps: 62 | Train Loss: 0.6103997 Vali Loss: 0.3795217 Test Loss: 0.3636500
EarlyStopping counter: 5 out of 20
Updating learning rate to 0.00011891344262766608
Epoch: 30 cost time: 3.340881824493408
Epoch: 30, Steps: 62 | Train Loss: 0.6102972 Vali Loss: 0.3793624 Test Loss: 0.3634692
EarlyStopping counter: 6 out of 20
Updating learning rate to 0.00011296777049628277
Epoch: 31 cost time: 2.7066826820373535
Epoch: 31, Steps: 62 | Train Loss: 0.6101346 Vali Loss: 0.3775876 Test Loss: 0.3634696
EarlyStopping counter: 7 out of 20
Updating learning rate to 0.00010731938197146864
Epoch: 32 cost time: 3.192437171936035
Epoch: 32, Steps: 62 | Train Loss: 0.6101174 Vali Loss: 0.3783152 Test Loss: 0.3634385
EarlyStopping counter: 8 out of 20
Updating learning rate to 0.00010195341287289519
Epoch: 33 cost time: 3.7921504974365234
Epoch: 33, Steps: 62 | Train Loss: 0.6101198 Vali Loss: 0.3794583 Test Loss: 0.3634091
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.685574222925044e-05
Epoch: 34 cost time: 3.053147077560425
Epoch: 34, Steps: 62 | Train Loss: 0.6098321 Vali Loss: 0.3780335 Test Loss: 0.3633473
EarlyStopping counter: 10 out of 20
Updating learning rate to 9.201295511778792e-05
Epoch: 35 cost time: 2.6012985706329346
Epoch: 35, Steps: 62 | Train Loss: 0.6095651 Vali Loss: 0.3770695 Test Loss: 0.3633499
Validation loss decreased (0.377275 --> 0.377070).  Saving model ...
Updating learning rate to 8.74123073618985e-05
Epoch: 36 cost time: 2.2535030841827393
Epoch: 36, Steps: 62 | Train Loss: 0.6097958 Vali Loss: 0.3777290 Test Loss: 0.3633668
EarlyStopping counter: 1 out of 20
Updating learning rate to 8.304169199380359e-05
Epoch: 37 cost time: 2.763535261154175
Epoch: 37, Steps: 62 | Train Loss: 0.6094423 Vali Loss: 0.3763874 Test Loss: 0.3634106
Validation loss decreased (0.377070 --> 0.376387).  Saving model ...
Updating learning rate to 7.88896073941134e-05
Epoch: 38 cost time: 3.8223090171813965
Epoch: 38, Steps: 62 | Train Loss: 0.6094761 Vali Loss: 0.3762706 Test Loss: 0.3633227
Validation loss decreased (0.376387 --> 0.376271).  Saving model ...
Updating learning rate to 7.494512702440772e-05
Epoch: 39 cost time: 3.5108437538146973
Epoch: 39, Steps: 62 | Train Loss: 0.6080364 Vali Loss: 0.3780519 Test Loss: 0.3633647
EarlyStopping counter: 1 out of 20
Updating learning rate to 7.119787067318733e-05
Epoch: 40 cost time: 2.506669759750366
Epoch: 40, Steps: 62 | Train Loss: 0.6092481 Vali Loss: 0.3758810 Test Loss: 0.3633357
Validation loss decreased (0.376271 --> 0.375881).  Saving model ...
Updating learning rate to 6.763797713952796e-05
Epoch: 41 cost time: 4.698802471160889
Epoch: 41, Steps: 62 | Train Loss: 0.6090497 Vali Loss: 0.3772263 Test Loss: 0.3632801
EarlyStopping counter: 1 out of 20
Updating learning rate to 6.425607828255156e-05
Epoch: 42 cost time: 9.871703386306763
Epoch: 42, Steps: 62 | Train Loss: 0.6080435 Vali Loss: 0.3766163 Test Loss: 0.3632989
EarlyStopping counter: 2 out of 20
Updating learning rate to 6.104327436842398e-05
Epoch: 43 cost time: 5.133376836776733
Epoch: 43, Steps: 62 | Train Loss: 0.6085131 Vali Loss: 0.3756609 Test Loss: 0.3632687
Validation loss decreased (0.375881 --> 0.375661).  Saving model ...
Updating learning rate to 5.799111065000278e-05
Epoch: 44 cost time: 3.341942548751831
Epoch: 44, Steps: 62 | Train Loss: 0.6090370 Vali Loss: 0.3753923 Test Loss: 0.3632504
Validation loss decreased (0.375661 --> 0.375392).  Saving model ...
Updating learning rate to 5.509155511750264e-05
Epoch: 45 cost time: 3.319023370742798
Epoch: 45, Steps: 62 | Train Loss: 0.6087358 Vali Loss: 0.3784598 Test Loss: 0.3632396
EarlyStopping counter: 1 out of 20
Updating learning rate to 5.2336977361627504e-05
Epoch: 46 cost time: 3.7113776206970215
Epoch: 46, Steps: 62 | Train Loss: 0.6091035 Vali Loss: 0.3769926 Test Loss: 0.3632305
EarlyStopping counter: 2 out of 20
Updating learning rate to 4.9720128493546124e-05
Epoch: 47 cost time: 2.647350311279297
Epoch: 47, Steps: 62 | Train Loss: 0.6079897 Vali Loss: 0.3784897 Test Loss: 0.3632333
EarlyStopping counter: 3 out of 20
Updating learning rate to 4.7234122068868816e-05
Epoch: 48 cost time: 2.123817205429077
Epoch: 48, Steps: 62 | Train Loss: 0.6089247 Vali Loss: 0.3756400 Test Loss: 0.3632268
EarlyStopping counter: 4 out of 20
Updating learning rate to 4.487241596542538e-05
Epoch: 49 cost time: 2.7782208919525146
Epoch: 49, Steps: 62 | Train Loss: 0.6080275 Vali Loss: 0.3748630 Test Loss: 0.3632166
Validation loss decreased (0.375392 --> 0.374863).  Saving model ...
Updating learning rate to 4.26287951671541e-05
Epoch: 50 cost time: 2.3542001247406006
Epoch: 50, Steps: 62 | Train Loss: 0.6083233 Vali Loss: 0.3742288 Test Loss: 0.3631745
Validation loss decreased (0.374863 --> 0.374229).  Saving model ...
Updating learning rate to 4.0497355408796396e-05
Epoch: 51 cost time: 2.5220603942871094
Epoch: 51, Steps: 62 | Train Loss: 0.6087095 Vali Loss: 0.3744104 Test Loss: 0.3632177
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.8472487638356575e-05
Epoch: 52 cost time: 2.1691133975982666
Epoch: 52, Steps: 62 | Train Loss: 0.6086710 Vali Loss: 0.3764248 Test Loss: 0.3631842
EarlyStopping counter: 2 out of 20
Updating learning rate to 3.654886325643875e-05
Epoch: 53 cost time: 2.8659427165985107
Epoch: 53, Steps: 62 | Train Loss: 0.6080914 Vali Loss: 0.3737922 Test Loss: 0.3631809
Validation loss decreased (0.374229 --> 0.373792).  Saving model ...
Updating learning rate to 3.47214200936168e-05
Epoch: 54 cost time: 3.578545331954956
Epoch: 54, Steps: 62 | Train Loss: 0.6081406 Vali Loss: 0.3752947 Test Loss: 0.3631780
EarlyStopping counter: 1 out of 20
Updating learning rate to 3.298534908893597e-05
Epoch: 55 cost time: 3.5616188049316406
Epoch: 55, Steps: 62 | Train Loss: 0.6080382 Vali Loss: 0.3728989 Test Loss: 0.3631762
Validation loss decreased (0.373792 --> 0.372899).  Saving model ...
Updating learning rate to 3.1336081634489166e-05
Epoch: 56 cost time: 2.98612642288208
Epoch: 56, Steps: 62 | Train Loss: 0.6085331 Vali Loss: 0.3751009 Test Loss: 0.3631678
EarlyStopping counter: 1 out of 20
Updating learning rate to 2.9769277552764706e-05
Epoch: 57 cost time: 3.1009085178375244
Epoch: 57, Steps: 62 | Train Loss: 0.6078484 Vali Loss: 0.3774807 Test Loss: 0.3631516
EarlyStopping counter: 2 out of 20
Updating learning rate to 2.8280813675126466e-05
Epoch: 58 cost time: 3.4291090965270996
Epoch: 58, Steps: 62 | Train Loss: 0.6083233 Vali Loss: 0.3730829 Test Loss: 0.3631555
EarlyStopping counter: 3 out of 20
Updating learning rate to 2.6866772991370145e-05
Epoch: 59 cost time: 3.490964412689209
Epoch: 59, Steps: 62 | Train Loss: 0.6080066 Vali Loss: 0.3745085 Test Loss: 0.3631622
EarlyStopping counter: 4 out of 20
Updating learning rate to 2.5523434341801633e-05
Epoch: 60 cost time: 2.861532211303711
Epoch: 60, Steps: 62 | Train Loss: 0.6084681 Vali Loss: 0.3741847 Test Loss: 0.3631408
EarlyStopping counter: 5 out of 20
Updating learning rate to 2.4247262624711552e-05
Epoch: 61 cost time: 2.826051712036133
Epoch: 61, Steps: 62 | Train Loss: 0.6084606 Vali Loss: 0.3742454 Test Loss: 0.3631578
EarlyStopping counter: 6 out of 20
Updating learning rate to 2.3034899493475973e-05
Epoch: 62 cost time: 2.538353681564331
Epoch: 62, Steps: 62 | Train Loss: 0.6083523 Vali Loss: 0.3763723 Test Loss: 0.3631341
EarlyStopping counter: 7 out of 20
Updating learning rate to 2.1883154518802173e-05
Epoch: 63 cost time: 2.8926680088043213
Epoch: 63, Steps: 62 | Train Loss: 0.6083645 Vali Loss: 0.3745455 Test Loss: 0.3631616
EarlyStopping counter: 8 out of 20
Updating learning rate to 2.0788996792862066e-05
Epoch: 64 cost time: 2.4778459072113037
Epoch: 64, Steps: 62 | Train Loss: 0.6080661 Vali Loss: 0.3750570 Test Loss: 0.3631393
EarlyStopping counter: 9 out of 20
Updating learning rate to 1.974954695321896e-05
Epoch: 65 cost time: 2.330599546432495
Epoch: 65, Steps: 62 | Train Loss: 0.6075460 Vali Loss: 0.3746563 Test Loss: 0.3631420
EarlyStopping counter: 10 out of 20
Updating learning rate to 1.876206960555801e-05
Epoch: 66 cost time: 2.9280779361724854
Epoch: 66, Steps: 62 | Train Loss: 0.6082652 Vali Loss: 0.3759145 Test Loss: 0.3631452
EarlyStopping counter: 11 out of 20
Updating learning rate to 1.782396612528011e-05
Epoch: 67 cost time: 2.6881728172302246
Epoch: 67, Steps: 62 | Train Loss: 0.6081056 Vali Loss: 0.3760001 Test Loss: 0.3631292
EarlyStopping counter: 12 out of 20
Updating learning rate to 1.6932767819016104e-05
Epoch: 68 cost time: 2.926461935043335
Epoch: 68, Steps: 62 | Train Loss: 0.6079596 Vali Loss: 0.3766032 Test Loss: 0.3631209
EarlyStopping counter: 13 out of 20
Updating learning rate to 1.6086129428065296e-05
Epoch: 69 cost time: 3.873450756072998
Epoch: 69, Steps: 62 | Train Loss: 0.6082241 Vali Loss: 0.3751362 Test Loss: 0.3631251
EarlyStopping counter: 14 out of 20
Updating learning rate to 1.5281822956662033e-05
Epoch: 70 cost time: 3.5282955169677734
Epoch: 70, Steps: 62 | Train Loss: 0.6075463 Vali Loss: 0.3724200 Test Loss: 0.3631203
Validation loss decreased (0.372899 --> 0.372420).  Saving model ...
Updating learning rate to 1.451773180882893e-05
Epoch: 71 cost time: 3.38755202293396
Epoch: 71, Steps: 62 | Train Loss: 0.6075747 Vali Loss: 0.3741438 Test Loss: 0.3631189
EarlyStopping counter: 1 out of 20
Updating learning rate to 1.3791845218387483e-05
Epoch: 72 cost time: 3.9893617630004883
Epoch: 72, Steps: 62 | Train Loss: 0.6080740 Vali Loss: 0.3766017 Test Loss: 0.3631140
EarlyStopping counter: 2 out of 20
Updating learning rate to 1.3102252957468109e-05
Epoch: 73 cost time: 2.738011121749878
Epoch: 73, Steps: 62 | Train Loss: 0.6082713 Vali Loss: 0.3755212 Test Loss: 0.3631128
EarlyStopping counter: 3 out of 20
Updating learning rate to 1.2447140309594702e-05
Epoch: 74 cost time: 2.53865647315979
Epoch: 74, Steps: 62 | Train Loss: 0.6076644 Vali Loss: 0.3742210 Test Loss: 0.3631099
EarlyStopping counter: 4 out of 20
Updating learning rate to 1.1824783294114967e-05
Epoch: 75 cost time: 3.4832651615142822
Epoch: 75, Steps: 62 | Train Loss: 0.6079389 Vali Loss: 0.3751390 Test Loss: 0.3631166
EarlyStopping counter: 5 out of 20
Updating learning rate to 1.1233544129409218e-05
Epoch: 76 cost time: 2.927788019180298
Epoch: 76, Steps: 62 | Train Loss: 0.6081110 Vali Loss: 0.3763875 Test Loss: 0.3631061
EarlyStopping counter: 6 out of 20
Updating learning rate to 1.0671866922938755e-05
Epoch: 77 cost time: 2.741018056869507
Epoch: 77, Steps: 62 | Train Loss: 0.6081989 Vali Loss: 0.3725773 Test Loss: 0.3630977
EarlyStopping counter: 7 out of 20
Updating learning rate to 1.0138273576791817e-05
Epoch: 78 cost time: 3.6921563148498535
Epoch: 78, Steps: 62 | Train Loss: 0.6083568 Vali Loss: 0.3752830 Test Loss: 0.3631065
EarlyStopping counter: 8 out of 20
Updating learning rate to 9.631359897952226e-06
Epoch: 79 cost time: 2.7487590312957764
Epoch: 79, Steps: 62 | Train Loss: 0.6077285 Vali Loss: 0.3730659 Test Loss: 0.3630985
EarlyStopping counter: 9 out of 20
Updating learning rate to 9.149791903054614e-06
Epoch: 80 cost time: 4.112414836883545
Epoch: 80, Steps: 62 | Train Loss: 0.6081355 Vali Loss: 0.3753042 Test Loss: 0.3630990
EarlyStopping counter: 10 out of 20
Updating learning rate to 8.692302307901884e-06
Epoch: 81 cost time: 8.695622682571411
Epoch: 81, Steps: 62 | Train Loss: 0.6081294 Vali Loss: 0.3745663 Test Loss: 0.3630970
EarlyStopping counter: 11 out of 20
Updating learning rate to 8.25768719250679e-06
Epoch: 82 cost time: 3.9278738498687744
Epoch: 82, Steps: 62 | Train Loss: 0.6081268 Vali Loss: 0.3733997 Test Loss: 0.3630982
EarlyStopping counter: 12 out of 20
Updating learning rate to 7.84480283288145e-06
Epoch: 83 cost time: 3.3096587657928467
Epoch: 83, Steps: 62 | Train Loss: 0.6077411 Vali Loss: 0.3759889 Test Loss: 0.3630967
EarlyStopping counter: 13 out of 20
Updating learning rate to 7.452562691237377e-06
Epoch: 84 cost time: 3.012028217315674
Epoch: 84, Steps: 62 | Train Loss: 0.6081068 Vali Loss: 0.3741796 Test Loss: 0.3630927
EarlyStopping counter: 14 out of 20
Updating learning rate to 7.079934556675507e-06
Epoch: 85 cost time: 2.6208431720733643
Epoch: 85, Steps: 62 | Train Loss: 0.6081611 Vali Loss: 0.3752084 Test Loss: 0.3630910
EarlyStopping counter: 15 out of 20
Updating learning rate to 6.725937828841732e-06
Epoch: 86 cost time: 3.004119634628296
Epoch: 86, Steps: 62 | Train Loss: 0.6071080 Vali Loss: 0.3775699 Test Loss: 0.3630888
EarlyStopping counter: 16 out of 20
Updating learning rate to 6.389640937399644e-06
Epoch: 87 cost time: 2.231682300567627
Epoch: 87, Steps: 62 | Train Loss: 0.6083019 Vali Loss: 0.3776501 Test Loss: 0.3630887
EarlyStopping counter: 17 out of 20
Updating learning rate to 6.070158890529662e-06
Epoch: 88 cost time: 3.1764581203460693
Epoch: 88, Steps: 62 | Train Loss: 0.6078898 Vali Loss: 0.3760517 Test Loss: 0.3630883
EarlyStopping counter: 18 out of 20
Updating learning rate to 5.766650946003179e-06
Epoch: 89 cost time: 3.2776341438293457
Epoch: 89, Steps: 62 | Train Loss: 0.6074939 Vali Loss: 0.3736031 Test Loss: 0.3630880
EarlyStopping counter: 19 out of 20
Updating learning rate to 5.47831839870302e-06
Epoch: 90 cost time: 2.8941121101379395
Epoch: 90, Steps: 62 | Train Loss: 0.6081852 Vali Loss: 0.3756422 Test Loss: 0.3630882
EarlyStopping counter: 20 out of 20
Early stopping
>>>>>>>testing : ETTh2_360_336_FITS_ETTh2_ftM_sl360_ll48_pl336_H5_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2545
mse:0.3586523234844208, mae:0.39723584055900574, rse:0.4788244962692261, corr:[0.26052734 0.26643914 0.26233077 0.2625513  0.26299554 0.26058957
 0.25908962 0.25902796 0.25792423 0.2557724  0.2545233  0.25357813
 0.2521516  0.25072545 0.2500801  0.2494547  0.24848263 0.2476515
 0.24698591 0.2458505  0.2444166  0.24352108 0.2424027  0.24045713
 0.2382422  0.23702917 0.23611636 0.23483281 0.23367216 0.23289217
 0.23179142 0.2299867  0.22849488 0.22775707 0.22680148 0.22529964
 0.22407274 0.2234348  0.22241312 0.22086404 0.21987516 0.2193958
 0.2182706  0.2164584  0.21511102 0.21448438 0.2129384  0.21016498
 0.20793119 0.20682001 0.20547283 0.20347904 0.20179552 0.20051914
 0.19867305 0.19651397 0.19520351 0.19436258 0.19315885 0.1917902
 0.19134155 0.1915959  0.19135162 0.19050689 0.19005893 0.18995751
 0.18907179 0.18797247 0.1877136  0.18794023 0.18711038 0.18550746
 0.18470174 0.18462355 0.18386248 0.18254104 0.18196055 0.18184935
 0.18106228 0.17994045 0.17972279 0.1799262  0.17961338 0.1789201
 0.17886683 0.17914066 0.17886584 0.17815995 0.17790553 0.17785123
 0.17721936 0.1765113  0.17690852 0.17766981 0.1774122  0.17626885
 0.17561847 0.17538911 0.17464097 0.17334181 0.1727589  0.17287925
 0.17258586 0.17164895 0.17151839 0.17195524 0.17193596 0.1714293
 0.17114347 0.17112814 0.17052494 0.16965143 0.16928075 0.16940594
 0.16891564 0.16801421 0.1678338  0.16779451 0.1667862  0.1649898
 0.16379993 0.16349714 0.16298456 0.1617882  0.16092426 0.1607917
 0.16018902 0.1590315  0.15839016 0.1583969  0.15807427 0.15717539
 0.15668263 0.1565927  0.15608676 0.1552797  0.15507868 0.1552478
 0.15463546 0.15385689 0.15397324 0.15427536 0.1534568  0.15157948
 0.15016118 0.14952813 0.14866681 0.14758599 0.14711283 0.14713751
 0.14661904 0.14552408 0.14520806 0.1455102  0.14511418 0.14402682
 0.14359435 0.14402784 0.14404623 0.14361858 0.14349975 0.1436679
 0.14336717 0.14278275 0.1430161  0.14371595 0.14343457 0.14191782
 0.14077264 0.14039612 0.13977976 0.13866729 0.13772054 0.13714057
 0.13642374 0.13537262 0.13482505 0.1347088  0.13430893 0.13347879
 0.1331866  0.13334627 0.13318588 0.13284731 0.13282776 0.1330664
 0.13301893 0.13281491 0.13336106 0.13449416 0.13473085 0.13385916
 0.1333072  0.13329712 0.13297072 0.1322075  0.1318016  0.13188814
 0.13164648 0.13081332 0.13043378 0.13081124 0.1309935  0.130308
 0.12980461 0.12983175 0.12972037 0.1294764  0.1294809  0.12998039
 0.13017412 0.12997296 0.13013515 0.13063283 0.13064677 0.12958947
 0.1285166  0.12783028 0.12728873 0.12643114 0.1256888  0.12570982
 0.12578654 0.12565139 0.12540776 0.12531343 0.1251465  0.12442047
 0.12361811 0.12317012 0.12298905 0.12299974 0.12307867 0.12332481
 0.12344866 0.12374526 0.1241891  0.12461286 0.12465475 0.12415055
 0.12342211 0.12289476 0.12234328 0.12158494 0.12107676 0.12056863
 0.12007933 0.11970542 0.11972388 0.12001951 0.12035256 0.12023259
 0.11993614 0.12026235 0.12082098 0.12169913 0.12248064 0.12322496
 0.12352534 0.12364491 0.12421962 0.12493703 0.1255138  0.12541069
 0.12531626 0.12531981 0.12519865 0.12475011 0.12441183 0.12461516
 0.12466772 0.12440155 0.12418605 0.12449433 0.12528764 0.12499597
 0.12445052 0.12441351 0.12454592 0.12452044 0.12450067 0.12470151
 0.124679   0.1246115  0.12452544 0.12470823 0.12497877 0.12518333
 0.12467638 0.12387975 0.12307699 0.12221675 0.12130364 0.12082908
 0.12013815 0.11896935 0.1184266  0.11846197 0.11860434 0.11793383
 0.11676726 0.11631139 0.11653349 0.11698792 0.11684819 0.11718475
 0.11706929 0.116968   0.11733633 0.11749329 0.1175338  0.11669535
 0.11573109 0.11468612 0.11342558 0.11216389 0.11133891 0.11101052
 0.11013753 0.1093896  0.10958029 0.11037566 0.11035889 0.1096092
 0.10942866 0.10961278 0.10917742 0.10891655 0.10966245 0.10998199
 0.10870375 0.10903503 0.110985   0.10958273 0.10972176 0.11903277]
